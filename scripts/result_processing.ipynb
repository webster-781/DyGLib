{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "import pprint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "os.environ[\"http_proxy\"]=\"http://proxy61.iitd.ac.in:3128\"\n",
    "os.environ[\"https_proxy\"]=\"http://proxy61.iitd.ac.in:3128\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_columns = ['new node test average_precision',\n",
    " 'train roc_auc',\n",
    " 'new node val first_3_roc_auc',\n",
    " 'new node test first_3_roc_auc',\n",
    " 'new node test first_3_average_precision',\n",
    " 'new node val average_precision',\n",
    " 'new node val first_1_average_precision',\n",
    " 'new node test first_1_average_precision',\n",
    " 'new node test first_1_roc_auc',\n",
    " 'new node test first_10_roc_auc',\n",
    " 'new node val first_3_average_precision',\n",
    " 'val average_precision',\n",
    " 'val roc_auc',\n",
    " 'new node val roc_auc',\n",
    " 'new node val first_10_average_precision',\n",
    " 'new node test roc_auc',\n",
    " 'train average_precision',\n",
    " 'test roc_auc',\n",
    " 'new node val first_1_roc_auc',\n",
    " 'new node val first_10_roc_auc',\n",
    " 'test average_precision',\n",
    " 'new node test first_10_average_precision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfd2221b1684fbb859a4822e6963142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'new node val first_3_roc_auc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/ayushenv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'new node val first_3_roc_auc'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m tqdm(runs): \n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m track_columns:\n\u001b[0;32m----> 9\u001b[0m         hist \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(hist) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrom \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;241m.\u001b[39msummary[key]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mnanmean(hist)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/ayushenv/lib/python3.10/site-packages/pandas/core/frame.py:3896\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3896\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3898\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/ayushenv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'new node val first_3_roc_auc'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import wandb\n",
    "api = wandb.Api()\n",
    "\n",
    "runs = api.runs(\"fb-graph-proj/fb-graph-proj-dyglib\")\n",
    "\n",
    "for run in tqdm(runs): \n",
    "    for key in track_columns:\n",
    "        hist = run.history(samples = 200, keys = [key])[key]\n",
    "        if type(hist) == float:\n",
    "            print(f'From {run.summary[key]} to {np.nanmean(hist)}')\n",
    "        run.summary[key] = np.nanmean(hist)\n",
    "    run.summary.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import wandb\n",
    "api = wandb.Api()\n",
    "\n",
    "# Project is specified by <entity/project-name>\n",
    "runs = api.runs(\"fb-graph-proj/fb-graph-proj-dyglib\")\n",
    "\n",
    "for run in tqdm(runs): \n",
    "    # .summary contains the output keys/values for metrics like accuracy.\n",
    "    #  We call ._json_dict to omit large files \n",
    "    for key in run.summary.keys():\n",
    "        hist = run.history(samples = 200, keys = [key])[key]\n",
    "        if type(hist) == float:\n",
    "            print(f'From {run.summary[key]} to {np.nanmean(hist)}')\n",
    "        else:\n",
    "            print(hist)\n",
    "        run.summary[key] = np.nanmean(hist)\n",
    "    run.summary.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6307716525833154, 0.74466270894532, 0.6307716525833154)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = track_columns[0]\n",
    "hist = runs[2].history(samples = 200)[col]\n",
    "summary = runs[2].summary[col]\n",
    "hist[199], np.nanmean(hist), summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new node test average_precision',\n",
       " 'train roc_auc',\n",
       " 'new node val first_3_roc_auc',\n",
       " 'new node test first_3_roc_auc',\n",
       " 'new node test first_3_average_precision',\n",
       " 'new node val average_precision',\n",
       " 'new node val first_1_average_precision',\n",
       " 'new node test first_1_average_precision',\n",
       " 'new node test first_1_roc_auc',\n",
       " 'new node test first_10_roc_auc',\n",
       " 'new node val first_3_average_precision',\n",
       " 'val average_precision',\n",
       " 'val roc_auc',\n",
       " 'new node val roc_auc',\n",
       " 'new node val first_10_average_precision',\n",
       " 'new node test roc_auc',\n",
       " 'train average_precision',\n",
       " 'test roc_auc',\n",
       " 'new node val first_1_roc_auc',\n",
       " 'new node val first_10_roc_auc',\n",
       " 'test average_precision',\n",
       " 'new node test first_10_average_precision']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_over_last_five_epochs(run_id):\n",
    "  # Takes wandb run id and returns average of last 5 readings of the following metrics\n",
    "  # new node test average_precision\n",
    "  # test average_precision\n",
    "  # test roc_auc\n",
    "  # new node test roc_auc\n",
    "  api = wandb.Api()\n",
    "  run = api.run(f\"/fb-graph-proj/fb-graph-proj-dyglib/runs/{run_id}\")\n",
    "  index = torch.argmax(torch.tensor(run.history()[\"val average_precision\"].tolist()))\n",
    "  METRICS_LIST = [\"new node test average_precision\", \"test average_precision\", \"test roc_auc\", \"new node test roc_auc\"]\n",
    "  avg_metric = {}\n",
    "  for metric in METRICS_LIST:\n",
    "    avg_metric[metric] = round(float(100 * torch.nanmean(torch.tensor(run.history()[metric].tolist()))), 2)\n",
    "  print(run.config[\"dataset\"])\n",
    "  pprint.pprint(avg_metric)\n",
    "  # return avg_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contacts\n",
      "{'new node test average_precision': 85.31,\n",
      " 'new node test roc_auc': 90.1,\n",
      " 'test average_precision': 80.5,\n",
      " 'test roc_auc': 87.03}\n"
     ]
    }
   ],
   "source": [
    "get_average_over_last_five_epochs(\"p0ft9874\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def vectorized_update_mem_2d(\n",
    "    mem: torch.Tensor, non_zero_counts: torch.Tensor, update: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Update a 2D tensor 'mem' by appending values from 'update' tensor to each 'row' of 'mem'.\n",
    "    This function operates in a queue-like fashion for each row.\n",
    "    If a row has zeros (indicated by non_zero_counts), the first zero is replaced by the corresponding value from 'update'.\n",
    "    If a row is full, it is shifted one place to the left, and the corresponding value from 'update' is added to the end.\n",
    "\n",
    "    Parameters:\n",
    "    mem (torch.Tensor): A 2D tensor of shape (n, seq_len) representing the memory to be updated.\n",
    "    non_zero_counts (torch.Tensor): A 1D tensor of shape (n,) containing the count of non-zero 'elements' in each 'row' of 'mem'.\n",
    "                                    Each element should be an integer.\n",
    "    update (torch.Tensor): A 1D tensor of shape (n,) containing the values to be appended to each 'row' of 'mem'.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The updated memory tensor of shape (n, seq_len).\n",
    "\n",
    "    Note:\n",
    "    - 'n' is the number of sequences, and 'seq_len' is the sequence length.\n",
    "    - This function assumes that 'mem' has been appropriately padded with zeros where necessary.\n",
    "    - The function operates in-place on 'mem', and the updated 'mem' tensor is also returned.\n",
    "    # Example usage\n",
    "    n = 3\n",
    "    seq_len = 5\n",
    "    mem = torch.rand(n, seq_len)  # Example 2D tensor\n",
    "    print(mem)\n",
    "    non_zero_counts = torch.tensor([2, 3, 5])  # Counts of non-zero elements in each row\n",
    "    update = torch.rand(n)  # Update tensor\n",
    "    print(update)\n",
    "    updated_mem = vectorized_update_mem_2d(mem, non_zero_counts, update)\n",
    "    print(updated_mem)\n",
    "    \"\"\"\n",
    "\n",
    "    n, seq_len = mem.shape\n",
    "\n",
    "    # Mask for rows that are full and need shifting\n",
    "    full_rows_mask = non_zero_counts >= seq_len\n",
    "    non_full_rows_mask = non_zero_counts < seq_len\n",
    "\n",
    "    # Shift the full rows\n",
    "    mem[full_rows_mask] = torch.roll(mem[full_rows_mask], -1, dims=1)\n",
    "\n",
    "    # Determine insertion index for each row\n",
    "    insertion_index = non_zero_counts.clone()\n",
    "    insertion_index[full_rows_mask] = seq_len - 1  # If row is full, set index to last position\n",
    "\n",
    "    # Create a mask for insertion points\n",
    "    insertion_mask = torch.zeros(n, seq_len, dtype=torch.bool, device=mem.device)\n",
    "    insertion_mask.scatter_(1, insertion_index.unsqueeze(1), True)\n",
    "\n",
    "    # Insert new values\n",
    "    mem = torch.where(insertion_mask, update.unsqueeze(1), mem)\n",
    "    non_zero_counts[non_full_rows_mask] += 1\n",
    "\n",
    "    return mem, non_zero_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3519, 0.5518, 0.8356, 0.7327, 0.7677],\n",
      "        [0.9330, 0.7643, 0.3385, 0.2019, 0.0988],\n",
      "        [0.8304, 0.8748, 0.6190, 0.1182, 0.4605]])\n",
      "tensor([0.6483, 0.1658, 0.1275])\n",
      "(tensor([[0.3519, 0.5518, 0.6483, 0.7327, 0.7677],\n",
      "        [0.9330, 0.7643, 0.3385, 0.1658, 0.0988],\n",
      "        [0.8748, 0.6190, 0.1182, 0.4605, 0.1275]]), tensor([3, 4, 5]))\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "seq_len = 5\n",
    "mem = torch.rand(n, seq_len)  # Example 2D tensor\n",
    "print(mem)\n",
    "non_zero_counts = torch.tensor([2, 3, 5])  # Counts of non-zero elements in each row\n",
    "update = torch.rand(n)  # Update tensor\n",
    "print(update)\n",
    "updated_mem = vectorized_update_mem_2d(mem, non_zero_counts, update)\n",
    "print(updated_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ayushenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
