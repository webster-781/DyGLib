True
  0%|          | 0/52049 [00:00<?, ?it/s]True
  0%|          | 0/61156 [00:00<?, ?it/s] 11%|█         | 5698/52049 [00:00<00:00, 49901.12it/s]  9%|▉         | 5525/61156 [00:00<00:01, 55242.28it/s] 21%|██        | 10689/52049 [00:00<00:00, 48462.82it/s] 18%|█▊        | 11050/61156 [00:00<00:01, 50005.25it/s] 30%|██▉       | 15531/52049 [00:00<00:00, 47614.76it/s]True
  0%|          | 0/87626 [00:00<?, ?it/s]True
  0%|          | 0/95577 [00:00<?, ?it/s] 26%|██▋       | 16084/61156 [00:00<00:01, 44251.73it/s]  8%|▊         | 7161/87626 [00:00<00:01, 71602.73it/s] 39%|███▉      | 20288/52049 [00:00<00:00, 45202.94it/s]  8%|▊         | 8099/95577 [00:00<00:01, 80982.93it/s] 34%|███▎      | 20576/61156 [00:00<00:00, 44197.24it/s]True
  0%|          | 0/50631 [00:00<?, ?it/s] 48%|████▊     | 24902/52049 [00:00<00:00, 42607.40it/s] 17%|█▋        | 16275/95577 [00:00<00:00, 81438.46it/s]True
  0%|          | 0/140777 [00:00<?, ?it/s] 10%|█         | 5148/50631 [00:00<00:00, 51475.51it/s] 41%|████      | 25036/61156 [00:00<00:00, 41608.44it/s] 16%|█▋        | 14322/87626 [00:00<00:01, 43153.19it/s] 57%|█████▋    | 29543/52049 [00:00<00:00, 42393.54it/s]  4%|▍         | 6009/140777 [00:00<00:02, 60083.18it/s] 21%|██        | 10402/50631 [00:00<00:00, 46823.76it/s] 22%|██▏       | 19257/87626 [00:00<00:01, 44969.61it/s] 49%|████▉     | 30270/61156 [00:00<00:00, 42247.32it/s] 10%|█         | 14163/140777 [00:00<00:01, 72700.31it/s] 66%|██████▋   | 34528/52049 [00:00<00:00, 43102.18it/s] 31%|███       | 15720/50631 [00:00<00:00, 46016.70it/s] 56%|█████▋    | 34516/61156 [00:00<00:00, 40262.84it/s] 16%|█▌        | 22603/140777 [00:00<00:01, 78039.32it/s] 75%|███████▍  | 38944/52049 [00:00<00:00, 42311.67it/s] 28%|██▊       | 24859/87626 [00:00<00:01, 42704.80it/s] 26%|██▌       | 24419/95577 [00:00<00:01, 38231.08it/s] 43%|████▎     | 21558/50631 [00:00<00:00, 45510.80it/s] 65%|██████▍   | 39601/61156 [00:00<00:00, 41970.68it/s] 84%|████████▍ | 43614/52049 [00:00<00:00, 43585.08it/s] 34%|███▎      | 29377/87626 [00:00<00:01, 42650.95it/s] 22%|██▏       | 30407/140777 [00:00<00:01, 61525.57it/s] 31%|███▏      | 29915/95577 [00:00<00:01, 38900.27it/s] 54%|█████▍    | 27244/50631 [00:00<00:00, 49125.70it/s] 72%|███████▏  | 43815/61156 [00:01<00:00, 41527.48it/s] 40%|███▉      | 34619/87626 [00:00<00:01, 45468.21it/s] 92%|█████████▏| 47986/52049 [00:01<00:00, 41955.40it/s] 37%|███▋      | 35278/95577 [00:00<00:01, 39735.31it/s] 79%|███████▉  | 48509/61156 [00:01<00:00, 42563.69it/s]100%|█████████▉| 52048/52049 [00:01<00:00, 43281.65it/s]
 64%|██████▎   | 32214/50631 [00:00<00:00, 44456.88it/s] 45%|████▍     | 39320/87626 [00:00<00:01, 43243.67it/s] 43%|████▎     | 40751/95577 [00:00<00:01, 43337.78it/s] 87%|████████▋ | 53002/61156 [00:01<00:00, 42213.73it/s] 74%|███████▎  | 37227/50631 [00:00<00:00, 46085.36it/s] 26%|██▋       | 37282/140777 [00:00<00:02, 40651.49it/s] 51%|█████     | 44832/87626 [00:01<00:01, 40349.85it/s] 49%|████▉     | 46848/95577 [00:01<00:01, 47481.09it/s] 83%|████████▎ | 42092/50631 [00:00<00:00, 46830.85it/s] 95%|█████████▌| 58380/61156 [00:01<00:00, 42306.27it/s] 32%|███▏      | 45517/140777 [00:00<00:01, 49782.43it/s] 56%|█████▌    | 49248/87626 [00:01<00:00, 41334.71it/s]100%|█████████▉| 61155/61156 [00:01<00:00, 42874.74it/s]
 55%|█████▍    | 52106/95577 [00:01<00:01, 41531.58it/s] 93%|█████████▎| 46838/50631 [00:01<00:00, 42976.68it/s]  0%|          | 0/52049 [00:00<?, ?it/s] 62%|██████▏   | 54049/87626 [00:01<00:00, 43134.98it/s]100%|██████████| 52049/52049 [00:00<00:00, 2025978.65it/s]
100%|█████████▉| 50628/50631 [00:01<00:00, 45233.20it/s]
 60%|█████▉    | 56953/95577 [00:01<00:00, 43224.92it/s] 37%|███▋      | 52126/140777 [00:01<00:02, 41500.56it/s] 67%|██████▋   | 58460/87626 [00:01<00:00, 41606.15it/s] 43%|████▎     | 60350/140777 [00:01<00:01, 49989.01it/s] 64%|██████▍   | 61624/95577 [00:01<00:00, 42937.49it/s]  0%|          | 0/61156 [00:00<?, ?it/s] 72%|███████▏  | 62689/87626 [00:01<00:00, 39830.39it/s]100%|██████████| 61156/61156 [00:00<00:00, 1972249.73it/s]
  0%|          | 0/50631 [00:00<?, ?it/s] 49%|████▉     | 68776/140777 [00:01<00:01, 57849.93it/s] 69%|██████▉   | 66292/95577 [00:01<00:00, 43926.84it/s]100%|██████████| 50631/50631 [00:00<00:00, 1796539.99it/s]
 78%|███████▊  | 68515/87626 [00:01<00:00, 44861.07it/s] 83%|████████▎ | 73092/87626 [00:01<00:00, 44713.54it/s] 74%|███████▍  | 70866/95577 [00:01<00:00, 38333.50it/s] 78%|███████▊  | 74929/95577 [00:01<00:00, 37618.80it/s] 54%|█████▎    | 75601/140777 [00:01<00:01, 40933.10it/s] 89%|████████▊ | 77626/87626 [00:01<00:00, 39825.49it/s] 58%|█████▊    | 81430/140777 [00:01<00:01, 44279.70it/s] 82%|████████▏ | 78845/95577 [00:01<00:00, 36594.87it/s] 93%|█████████▎| 81738/87626 [00:01<00:00, 37367.23it/s] 63%|██████▎   | 88089/140777 [00:01<00:01, 49095.89it/s] 86%|████████▋ | 82607/95577 [00:02<00:00, 32203.37it/s] 98%|█████████▊| 85577/87626 [00:02<00:00, 33836.48it/s]100%|█████████▉| 87625/87626 [00:02<00:00, 40588.64it/s]
 91%|█████████▏| 87295/95577 [00:02<00:00, 35385.32it/s] 97%|█████████▋| 92261/95577 [00:02<00:00, 36571.56it/s] 67%|██████▋   | 93981/140777 [00:02<00:01, 34124.13it/s]100%|█████████▉| 95576/95577 [00:02<00:00, 40926.28it/s]
  0%|          | 0/87626 [00:00<?, ?it/s] 72%|███████▏  | 102027/140777 [00:02<00:01, 31497.07it/s]100%|██████████| 87626/87626 [00:00<00:00, 1928613.47it/s]
  0%|          | 0/95577 [00:00<?, ?it/s] 78%|███████▊  | 110272/140777 [00:02<00:00, 39741.08it/s]100%|██████████| 95577/95577 [00:00<00:00, 1939142.81it/s]
 84%|████████▍ | 118606/140777 [00:02<00:00, 47925.89it/s]wandb: Currently logged in as: singhayush4499 (fb-graph-proj). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: singhayush4499 (fb-graph-proj). Use `wandb login --relogin` to force relogin
 89%|████████▊ | 124850/140777 [00:02<00:00, 40637.36it/s]wandb: Currently logged in as: singhayush4499 (fb-graph-proj). Use `wandb login --relogin` to force relogin
 93%|█████████▎| 130231/140777 [00:03<00:00, 30503.08it/s] 98%|█████████▊| 138469/140777 [00:03<00:00, 38995.64it/s]100%|█████████▉| 140776/140777 [00:03<00:00, 42086.98it/s]
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/ayush/DyGLib/wandb/run-20240407_154344-63ik794v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dygformer-ia-reality-call-old
wandb: ⭐️ View project at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib
wandb: 🚀 View run at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib/runs/63ik794v
INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-reality-call', batch_size=200, model_name='DyGFormer', gpu=0, num_neighbors=20, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=2, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='old', use_ROPe=False, t1_factor_of_t2=1, use_init_method=False, emb_proj=False, init_weights=None, clip=1.0, device='cuda:0', seed=0, save_model_name='DyGFormer_seed0_dygformer-ia-reality-call-old')
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/ayush/DyGLib/wandb/run-20240407_154344-16dgnr0k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dygformer-ia-retweet-pol-old
wandb: ⭐️ View project at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib
wandb: 🚀 View run at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib/runs/16dgnr0k
INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-retweet-pol', batch_size=200, model_name='DyGFormer', gpu=2, num_neighbors=20, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=2, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='old', use_ROPe=False, t1_factor_of_t2=1, use_init_method=False, emb_proj=False, init_weights=None, clip=1.0, device='cuda:2', seed=0, save_model_name='DyGFormer_seed0_dygformer-ia-retweet-pol-old')
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/ayush/DyGLib/wandb/run-20240407_154344-yb34ypf5
wandb: Run `wandb offline` to turn off syncing.
  0%|          | 0/140777 [00:00<?, ?it/s]wandb: Syncing run dygformer-ia-escorts-dynamic-old
wandb: ⭐️ View project at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib
wandb: 🚀 View run at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib/runs/yb34ypf5
INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-escorts-dynamic', batch_size=200, model_name='DyGFormer', gpu=0, num_neighbors=20, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=2, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='old', use_ROPe=False, t1_factor_of_t2=1, use_init_method=False, emb_proj=False, init_weights=None, clip=1.0, device='cuda:0', seed=0, save_model_name='DyGFormer_seed0_dygformer-ia-escorts-dynamic-old')
wandb: Currently logged in as: singhayush4499 (fb-graph-proj). Use `wandb login --relogin` to force relogin
100%|██████████| 140777/140777 [00:00<00:00, 1918640.24it/s]
wandb: Currently logged in as: singhayush4499 (fb-graph-proj). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/ayush/DyGLib/wandb/run-20240407_154345-q7vkjtqm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dygformer-ia-movielens-user2tags-10m-old
wandb: ⭐️ View project at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib
wandb: 🚀 View run at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib/runs/q7vkjtqm
INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-movielens-user2tags-10m', batch_size=200, model_name='DyGFormer', gpu=2, num_neighbors=20, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=2, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='old', use_ROPe=False, t1_factor_of_t2=1, use_init_method=False, emb_proj=False, init_weights=None, clip=1.0, device='cuda:2', seed=0, save_model_name='DyGFormer_seed0_dygformer-ia-movielens-user2tags-10m-old')
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/ayush/DyGLib/wandb/run-20240407_154345-k6dhc836
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dygformer-ia-digg-reply-old
wandb: ⭐️ View project at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib
wandb: 🚀 View run at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib/runs/k6dhc836
INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-digg-reply', batch_size=200, model_name='DyGFormer', gpu=1, num_neighbors=20, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=2, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='old', use_ROPe=False, t1_factor_of_t2=1, use_init_method=False, emb_proj=False, init_weights=None, clip=1.0, device='cuda:1', seed=0, save_model_name='DyGFormer_seed0_dygformer-ia-digg-reply-old')
wandb: Currently logged in as: singhayush4499 (fb-graph-proj). Use `wandb login --relogin` to force relogin
INFO:root:model -> Sequential(
  (0): DyGFormer(
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (memory_bank): MemoryBank(num_nodes=27046, memory_dim=172)
    (neighbor_co_occurrence_encoder): NeighborCooccurrenceEncoder(
      (neighbor_co_occurrence_encode_layer): Sequential(
        (0): Linear(in_features=1, out_features=50, bias=True)
        (1): ReLU()
        (2): Linear(in_features=50, out_features=50, bias=True)
      )
    )
    (projection_layer): ModuleDict(
      (node): Linear(in_features=172, out_features=50, bias=True)
      (edge): Linear(in_features=172, out_features=50, bias=True)
      (time): Linear(in_features=100, out_features=50, bias=True)
      (neighbor_co_occurrence): Linear(in_features=50, out_features=50, bias=True)
    )
    (transformers): ModuleList(
      (0-1): 2 x TransformerEncoder(
        (multi_head_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)
        )
        (dropout): Dropout(p=0.1, inplace=False)
        (linear_layers): ModuleList(
          (0): Linear(in_features=200, out_features=800, bias=True)
          (1): Linear(in_features=800, out_features=200, bias=True)
        )
        (norm_layers): ModuleList(
          (0-1): 2 x LayerNorm((200,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_layer): Linear(in_features=200, out_features=172, bias=True)
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: DyGFormer, #parameters: 4348140 B, 4246.23046875 KB, 4.146709442138672 MB.
The dataset has 52049 interactions, involving 6809 different nodes
The training dataset has 23625 interactions, involving 3838 different nodes
The validation dataset has 7807 interactions, involving 1715 different nodes
The test dataset has 7808 interactions, involving 1937 different nodes
The new node validation dataset has 4011 interactions, involving 1185 different nodes
The new node test dataset has 4611 interactions, involving 1531 different nodes
680 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]INFO:root:model -> Sequential(
  (0): DyGFormer(
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (memory_bank): MemoryBank(num_nodes=10108, memory_dim=172)
    (neighbor_co_occurrence_encoder): NeighborCooccurrenceEncoder(
      (neighbor_co_occurrence_encode_layer): Sequential(
        (0): Linear(in_features=1, out_features=50, bias=True)
        (1): ReLU()
        (2): Linear(in_features=50, out_features=50, bias=True)
      )
    )
    (projection_layer): ModuleDict(
      (node): Linear(in_features=172, out_features=50, bias=True)
      (edge): Linear(in_features=172, out_features=50, bias=True)
      (time): Linear(in_features=100, out_features=50, bias=True)
      (neighbor_co_occurrence): Linear(in_features=50, out_features=50, bias=True)
    )
    (transformers): ModuleList(
      (0-1): 2 x TransformerEncoder(
        (multi_head_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)
        )
        (dropout): Dropout(p=0.1, inplace=False)
        (linear_layers): ModuleList(
          (0): Linear(in_features=200, out_features=800, bias=True)
          (1): Linear(in_features=800, out_features=200, bias=True)
        )
        (norm_layers): ModuleList(
          (0-1): 2 x LayerNorm((200,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_layer): Linear(in_features=200, out_features=172, bias=True)
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: DyGFormer, #parameters: 4348140 B, 4246.23046875 KB, 4.146709442138672 MB.
INFO:root:model -> Sequential(
  (0): DyGFormer(
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (memory_bank): MemoryBank(num_nodes=18471, memory_dim=172)
    (neighbor_co_occurrence_encoder): NeighborCooccurrenceEncoder(
      (neighbor_co_occurrence_encode_layer): Sequential(
        (0): Linear(in_features=1, out_features=50, bias=True)
        (1): ReLU()
        (2): Linear(in_features=50, out_features=50, bias=True)
      )
    )
    (projection_layer): ModuleDict(
      (node): Linear(in_features=172, out_features=50, bias=True)
      (edge): Linear(in_features=172, out_features=50, bias=True)
      (time): Linear(in_features=100, out_features=50, bias=True)
      (neighbor_co_occurrence): Linear(in_features=50, out_features=50, bias=True)
    )
    (transformers): ModuleList(
      (0-1): 2 x TransformerEncoder(
        (multi_head_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)
        )
        (dropout): Dropout(p=0.1, inplace=False)
        (linear_layers): ModuleList(
          (0): Linear(in_features=200, out_features=800, bias=True)
          (1): Linear(in_features=800, out_features=200, bias=True)
        )
        (norm_layers): ModuleList(
          (0-1): 2 x LayerNorm((200,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_layer): Linear(in_features=200, out_features=172, bias=True)
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: DyGFormer, #parameters: 4348140 B, 4246.23046875 KB, 4.146709442138672 MB.
The dataset has 50631 interactions, involving 10106 different nodes
The training dataset has 29100 interactions, involving 7154 different nodes
The validation dataset has 7596 interactions, involving 4118 different nodes
The test dataset has 7577 interactions, involving 4144 different nodes
The new node validation dataset has 3845 interactions, involving 2930 different nodes
The new node test dataset has 4829 interactions, involving 3346 different nodes
1010 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]The dataset has 61156 interactions, involving 18470 different nodes
The training dataset has 30070 interactions, involving 12678 different nodes
The validation dataset has 9173 interactions, involving 5479 different nodes
The test dataset has 9174 interactions, involving 5328 different nodes
The new node validation dataset has 4957 interactions, involving 4196 different nodes
The new node test dataset has 5073 interactions, involving 4153 different nodes
1847 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/ayush/DyGLib/wandb/run-20240407_154347-zrmn8j56
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dygformer-ia-slashdot-reply-dir-old
wandb: ⭐️ View project at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib
wandb: 🚀 View run at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib/runs/zrmn8j56
INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-slashdot-reply-dir', batch_size=200, model_name='DyGFormer', gpu=1, num_neighbors=20, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=2, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='old', use_ROPe=False, t1_factor_of_t2=1, use_init_method=False, emb_proj=False, init_weights=None, clip=1.0, device='cuda:1', seed=0, save_model_name='DyGFormer_seed0_dygformer-ia-slashdot-reply-dir-old')
INFO:root:model -> Sequential(
  (0): DyGFormer(
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (memory_bank): MemoryBank(num_nodes=30400, memory_dim=172)
    (neighbor_co_occurrence_encoder): NeighborCooccurrenceEncoder(
      (neighbor_co_occurrence_encode_layer): Sequential(
        (0): Linear(in_features=1, out_features=50, bias=True)
        (1): ReLU()
        (2): Linear(in_features=50, out_features=50, bias=True)
      )
    )
    (projection_layer): ModuleDict(
      (node): Linear(in_features=172, out_features=50, bias=True)
      (edge): Linear(in_features=172, out_features=50, bias=True)
      (time): Linear(in_features=100, out_features=50, bias=True)
      (neighbor_co_occurrence): Linear(in_features=50, out_features=50, bias=True)
    )
    (transformers): ModuleList(
      (0-1): 2 x TransformerEncoder(
        (multi_head_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)
        )
        (dropout): Dropout(p=0.1, inplace=False)
        (linear_layers): ModuleList(
          (0): Linear(in_features=200, out_features=800, bias=True)
          (1): Linear(in_features=800, out_features=200, bias=True)
        )
        (norm_layers): ModuleList(
          (0-1): 2 x LayerNorm((200,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_layer): Linear(in_features=200, out_features=172, bias=True)
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: DyGFormer, #parameters: 4348140 B, 4246.23046875 KB, 4.146709442138672 MB.
The dataset has 87626 interactions, involving 30398 different nodes
The training dataset has 47297 interactions, involving 21540 different nodes
The validation dataset has 13144 interactions, involving 9241 different nodes
The test dataset has 13144 interactions, involving 9511 different nodes
The new node validation dataset has 7995 interactions, involving 7321 different nodes
The new node test dataset has 8239 interactions, involving 7732 different nodes
3039 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/237 [00:00<?, ?it/s]INFO:root:model -> Sequential(
  (0): DyGFormer(
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (memory_bank): MemoryBank(num_nodes=16530, memory_dim=172)
    (neighbor_co_occurrence_encoder): NeighborCooccurrenceEncoder(
      (neighbor_co_occurrence_encode_layer): Sequential(
        (0): Linear(in_features=1, out_features=50, bias=True)
        (1): ReLU()
        (2): Linear(in_features=50, out_features=50, bias=True)
      )
    )
    (projection_layer): ModuleDict(
      (node): Linear(in_features=172, out_features=50, bias=True)
      (edge): Linear(in_features=172, out_features=50, bias=True)
      (time): Linear(in_features=100, out_features=50, bias=True)
      (neighbor_co_occurrence): Linear(in_features=50, out_features=50, bias=True)
    )
    (transformers): ModuleList(
      (0-1): 2 x TransformerEncoder(
        (multi_head_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)
        )
        (dropout): Dropout(p=0.1, inplace=False)
        (linear_layers): ModuleList(
          (0): Linear(in_features=200, out_features=800, bias=True)
          (1): Linear(in_features=800, out_features=200, bias=True)
        )
        (norm_layers): ModuleList(
          (0-1): 2 x LayerNorm((200,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_layer): Linear(in_features=200, out_features=172, bias=True)
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: DyGFormer, #parameters: 4348140 B, 4246.23046875 KB, 4.146709442138672 MB.
The dataset has 95577 interactions, involving 16527 different nodes
The training dataset has 48189 interactions, involving 10566 different nodes
The validation dataset has 14336 interactions, involving 5209 different nodes
The test dataset has 14337 interactions, involving 5377 different nodes
The new node validation dataset has 6745 interactions, involving 3518 different nodes
The new node test dataset has 7521 interactions, involving 3885 different nodes
1652 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/241 [00:00<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6980577111244202:   0%|                       | 0/119 [00:02<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6980577111244202:   1%|▏              | 1/119 [00:02<04:39,  2.37s/it]Epoch: 1, train for the 1-th batch, train loss: 0.6948155164718628:   0%|                       | 0/151 [00:02<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6948155164718628:   1%|               | 1/151 [00:02<05:59,  2.39s/it]Epoch: 1, train for the 1-th batch, train loss: 0.6978041529655457:   0%|                       | 0/146 [00:02<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6978041529655457:   1%|               | 1/146 [00:02<05:51,  2.43s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6879494190216064:   1%|▏              | 1/119 [00:02<04:39,  2.37s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6879494190216064:   2%|▎              | 2/119 [00:02<02:37,  1.34s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6916537880897522:   1%|               | 1/151 [00:02<05:59,  2.39s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6916537880897522:   1%|▏              | 2/151 [00:02<03:03,  1.23s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6905636191368103:   1%|               | 1/146 [00:03<05:51,  2.43s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6905636191368103:   1%|▏              | 2/146 [00:03<03:14,  1.35s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6740854978561401:   2%|▎              | 2/119 [00:03<02:37,  1.34s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6740854978561401:   3%|▍              | 3/119 [00:03<01:48,  1.07it/s]INFO:root:model -> Sequential(
  (0): DyGFormer(
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (memory_bank): MemoryBank(num_nodes=51085, memory_dim=172)
    (neighbor_co_occurrence_encoder): NeighborCooccurrenceEncoder(
      (neighbor_co_occurrence_encode_layer): Sequential(
        (0): Linear(in_features=1, out_features=50, bias=True)
        (1): ReLU()
        (2): Linear(in_features=50, out_features=50, bias=True)
      )
    )
    (projection_layer): ModuleDict(
      (node): Linear(in_features=172, out_features=50, bias=True)
      (edge): Linear(in_features=172, out_features=50, bias=True)
      (time): Linear(in_features=100, out_features=50, bias=True)
      (neighbor_co_occurrence): Linear(in_features=50, out_features=50, bias=True)
    )
    (transformers): ModuleList(
      (0-1): 2 x TransformerEncoder(
        (multi_head_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)
        )
        (dropout): Dropout(p=0.1, inplace=False)
        (linear_layers): ModuleList(
          (0): Linear(in_features=200, out_features=800, bias=True)
          (1): Linear(in_features=800, out_features=200, bias=True)
        )
        (norm_layers): ModuleList(
          (0-1): 2 x LayerNorm((200,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_layer): Linear(in_features=200, out_features=172, bias=True)
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: DyGFormer, #parameters: 4348140 B, 4246.23046875 KB, 4.146709442138672 MB.
The dataset has 140777 interactions, involving 51083 different nodes
The training dataset has 76599 interactions, involving 34496 different nodes
The validation dataset has 21116 interactions, involving 10542 different nodes
The test dataset has 21117 interactions, involving 10424 different nodes
The new node validation dataset has 15534 interactions, involving 9790 different nodes
The new node test dataset has 16568 interactions, involving 9911 different nodes
5108 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/383 [00:00<?, ?it/s]Epoch: 1, train for the 3-th batch, train loss: 0.6930378675460815:   1%|▏              | 2/151 [00:03<03:03,  1.23s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6930378675460815:   2%|▎              | 3/151 [00:03<02:10,  1.13it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6855253577232361:   2%|▎              | 3/151 [00:03<02:10,  1.13it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6855253577232361:   3%|▍              | 4/151 [00:03<01:29,  1.65it/s]Epoch: 1, train for the 3-th batch, train loss: 0.6842645406723022:   1%|▏              | 2/146 [00:03<03:14,  1.35s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6842645406723022:   2%|▎              | 3/146 [00:03<02:20,  1.02it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6568650603294373:   3%|▍              | 3/119 [00:04<01:48,  1.07it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6568650603294373:   3%|▌              | 4/119 [00:04<01:31,  1.26it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6945436000823975:   0%|                       | 0/237 [00:02<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6945436000823975:   0%|               | 1/237 [00:02<09:04,  2.31s/it]Epoch: 1, train for the 5-th batch, train loss: 0.691232442855835:   3%|▍               | 4/151 [00:03<01:29,  1.65it/s]Epoch: 1, train for the 5-th batch, train loss: 0.691232442855835:   3%|▌               | 5/151 [00:03<01:20,  1.81it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6360085606575012:   3%|▌              | 4/119 [00:04<01:31,  1.26it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6360085606575012:   4%|▋              | 5/119 [00:04<01:12,  1.58it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6860992908477783:   3%|▍              | 5/151 [00:04<01:20,  1.81it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6860992908477783:   4%|▌              | 6/151 [00:04<01:02,  2.32it/s]Epoch: 1, train for the 2-th batch, train loss: 0.6903642416000366:   0%|               | 1/237 [00:02<09:04,  2.31s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6903642416000366:   1%|▏              | 2/237 [00:02<04:46,  1.22s/it]Epoch: 1, train for the 6-th batch, train loss: 0.6165021657943726:   4%|▋              | 5/119 [00:04<01:12,  1.58it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6165021657943726:   5%|▊              | 6/119 [00:04<01:01,  1.82it/s]Epoch: 1, train for the 3-th batch, train loss: 0.6906976699829102:   1%|▏              | 2/237 [00:02<04:46,  1.22s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6906976699829102:   1%|▏              | 3/237 [00:02<02:52,  1.35it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6825008392333984:   2%|▎              | 3/146 [00:04<02:20,  1.02it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6825008392333984:   3%|▍              | 4/146 [00:04<02:12,  1.07it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6942182779312134:   0%|                       | 0/241 [00:03<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6942182779312134:   0%|               | 1/241 [00:03<12:14,  3.06s/it]Epoch: 1, train for the 4-th batch, train loss: 0.6860771179199219:   1%|▏              | 3/237 [00:03<02:52,  1.35it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6860771179199219:   2%|▎              | 4/237 [00:03<02:00,  1.93it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6837608814239502:   2%|▎              | 4/237 [00:03<02:00,  1.93it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6837608814239502:   2%|▎              | 5/237 [00:03<01:32,  2.50it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6900997757911682:   4%|▌              | 6/151 [00:04<01:02,  2.32it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6900997757911682:   5%|▋              | 7/151 [00:04<01:15,  1.92it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6767083406448364:   2%|▎              | 5/237 [00:03<01:32,  2.50it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6767083406448364:   3%|▍              | 6/237 [00:03<01:15,  3.07it/s]Epoch: 1, train for the 5-th batch, train loss: 0.683316171169281:   3%|▍               | 4/146 [00:04<02:12,  1.07it/s]Epoch: 1, train for the 5-th batch, train loss: 0.683316171169281:   3%|▌               | 5/146 [00:04<01:51,  1.27it/s]Epoch: 1, train for the 7-th batch, train loss: 0.5716276168823242:   5%|▊              | 6/119 [00:05<01:01,  1.82it/s]Epoch: 1, train for the 7-th batch, train loss: 0.5716276168823242:   6%|▉              | 7/119 [00:05<01:03,  1.77it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6710661053657532:   3%|▍              | 6/237 [00:03<01:15,  3.07it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6710661053657532:   3%|▍              | 7/237 [00:03<01:03,  3.63it/s]Epoch: 1, train for the 2-th batch, train loss: 0.6797316670417786:   0%|               | 1/241 [00:03<12:14,  3.06s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6797316670417786:   1%|               | 2/241 [00:03<06:22,  1.60s/it]Epoch: 1, train for the 8-th batch, train loss: 0.6921024918556213:   5%|▋              | 7/151 [00:05<01:15,  1.92it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6921024918556213:   5%|▊              | 8/151 [00:05<01:05,  2.17it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6537411212921143:   3%|▍              | 7/237 [00:03<01:03,  3.63it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6537411212921143:   3%|▌              | 8/237 [00:03<01:01,  3.70it/s]Epoch: 1, train for the 9-th batch, train loss: 0.672272801399231:   3%|▌               | 8/237 [00:04<01:01,  3.70it/s]Epoch: 1, train for the 9-th batch, train loss: 0.672272801399231:   4%|▌               | 9/237 [00:04<00:54,  4.20it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6838123202323914:   3%|▌              | 5/146 [00:05<01:51,  1.27it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6838123202323914:   4%|▌              | 6/146 [00:05<01:41,  1.38it/s]Epoch: 1, train for the 8-th batch, train loss: 0.5012788772583008:   6%|▉              | 7/119 [00:05<01:03,  1.77it/s]Epoch: 1, train for the 8-th batch, train loss: 0.5012788772583008:   7%|█              | 8/119 [00:05<01:04,  1.73it/s]Epoch: 1, train for the 3-th batch, train loss: 0.6723028421401978:   1%|               | 2/241 [00:04<06:22,  1.60s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6723028421401978:   1%|▏              | 3/241 [00:04<04:27,  1.13s/it]Epoch: 1, train for the 9-th batch, train loss: 0.6815868020057678:   5%|▊              | 8/151 [00:05<01:05,  2.17it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6815868020057678:   6%|▉              | 9/151 [00:05<01:07,  2.10it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6951258778572083:   0%|                       | 0/383 [00:02<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6951258778572083:   0%|               | 1/383 [00:02<16:03,  2.52s/it]Epoch: 1, train for the 10-th batch, train loss: 0.6170709729194641:   4%|▌             | 9/237 [00:04<00:54,  4.20it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6170709729194641:   4%|▌            | 10/237 [00:04<01:03,  3.58it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6785531640052795:   4%|▌              | 6/146 [00:06<01:41,  1.38it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6785531640052795:   5%|▋              | 7/146 [00:06<01:34,  1.47it/s]Epoch: 1, train for the 9-th batch, train loss: 0.5262176990509033:   7%|█              | 8/119 [00:06<01:04,  1.73it/s]Epoch: 1, train for the 9-th batch, train loss: 0.5262176990509033:   8%|█▏             | 9/119 [00:06<01:04,  1.70it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6824305057525635:   4%|▌            | 10/237 [00:04<01:03,  3.58it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6824305057525635:   5%|▌            | 11/237 [00:04<01:05,  3.46it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6934841871261597:   6%|▊             | 9/151 [00:06<01:07,  2.10it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6934841871261597:   7%|▊            | 10/151 [00:06<01:17,  1.82it/s]Epoch: 1, train for the 4-th batch, train loss: 0.684744119644165:   1%|▏               | 3/241 [00:04<04:27,  1.13s/it]Epoch: 1, train for the 4-th batch, train loss: 0.684744119644165:   2%|▎               | 4/241 [00:04<03:48,  1.04it/s]Epoch: 1, train for the 2-th batch, train loss: 0.692299485206604:   0%|                | 1/383 [00:03<16:03,  2.52s/it]Epoch: 1, train for the 2-th batch, train loss: 0.692299485206604:   1%|                | 2/383 [00:03<09:11,  1.45s/it]Epoch: 1, train for the 8-th batch, train loss: 0.6666402816772461:   5%|▋              | 7/146 [00:06<01:34,  1.47it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6666402816772461:   5%|▊              | 8/146 [00:06<01:26,  1.59it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6305923461914062:   5%|▌            | 11/237 [00:05<01:05,  3.46it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6305923461914062:   5%|▋            | 12/237 [00:05<01:19,  2.85it/s]Epoch: 1, train for the 10-th batch, train loss: 0.48165106773376465:   8%|▉            | 9/119 [00:07<01:04,  1.70it/s]Epoch: 1, train for the 10-th batch, train loss: 0.48165106773376465:   8%|█           | 10/119 [00:07<01:03,  1.71it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6036657691001892:   5%|▋            | 12/237 [00:05<01:19,  2.85it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6036657691001892:   5%|▋            | 13/237 [00:05<01:07,  3.31it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6903359889984131:   7%|▊            | 10/151 [00:06<01:17,  1.82it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6903359889984131:   7%|▉            | 11/151 [00:06<01:18,  1.78it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6694874167442322:   2%|▏              | 4/241 [00:05<03:48,  1.04it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6694874167442322:   2%|▎              | 5/241 [00:05<03:15,  1.21it/s]Epoch: 1, train for the 3-th batch, train loss: 0.6815858483314514:   1%|               | 2/383 [00:03<09:11,  1.45s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6815858483314514:   1%|               | 3/383 [00:03<06:39,  1.05s/it]Epoch: 1, train for the 9-th batch, train loss: 0.6605589389801025:   5%|▊              | 8/146 [00:07<01:26,  1.59it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6605589389801025:   6%|▉              | 9/146 [00:07<01:21,  1.68it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6924699544906616:   1%|               | 3/383 [00:04<06:39,  1.05s/it]Epoch: 1, train for the 4-th batch, train loss: 0.6924699544906616:   1%|▏              | 4/383 [00:04<04:43,  1.34it/s]Epoch: 1, train for the 11-th batch, train loss: 0.47068801522254944:   8%|█           | 10/119 [00:07<01:03,  1.71it/s]Epoch: 1, train for the 11-th batch, train loss: 0.47068801522254944:   9%|█           | 11/119 [00:07<01:02,  1.72it/s]Epoch: 1, train for the 14-th batch, train loss: 0.5847978591918945:   5%|▋            | 13/237 [00:06<01:07,  3.31it/s]Epoch: 1, train for the 14-th batch, train loss: 0.5847978591918945:   6%|▊            | 14/237 [00:06<01:30,  2.46it/s]Epoch: 1, train for the 12-th batch, train loss: 0.692423403263092:   7%|█             | 11/151 [00:07<01:18,  1.78it/s]Epoch: 1, train for the 12-th batch, train loss: 0.692423403263092:   8%|█             | 12/151 [00:07<01:20,  1.73it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6847092509269714:   2%|▎              | 5/241 [00:06<03:15,  1.21it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6847092509269714:   2%|▎              | 6/241 [00:06<02:59,  1.31it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6503831148147583:   6%|▊             | 9/146 [00:07<01:21,  1.68it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6503831148147583:   7%|▉            | 10/146 [00:07<01:19,  1.72it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6846908330917358:   1%|▏              | 4/383 [00:04<04:43,  1.34it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6846908330917358:   1%|▏              | 5/383 [00:04<04:06,  1.54it/s]Epoch: 1, train for the 12-th batch, train loss: 0.4483485817909241:   9%|█▏           | 11/119 [00:08<01:02,  1.72it/s]Epoch: 1, train for the 12-th batch, train loss: 0.4483485817909241:  10%|█▎           | 12/119 [00:08<01:01,  1.73it/s]Epoch: 1, train for the 15-th batch, train loss: 0.5936872959136963:   6%|▊            | 14/237 [00:06<01:30,  2.46it/s]Epoch: 1, train for the 15-th batch, train loss: 0.5936872959136963:   6%|▊            | 15/237 [00:06<01:36,  2.30it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6862902641296387:   8%|█            | 12/151 [00:08<01:20,  1.73it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6862902641296387:   9%|█            | 13/151 [00:08<01:20,  1.71it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6129098534584045:   2%|▎              | 6/241 [00:06<02:59,  1.31it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6129098534584045:   3%|▍              | 7/241 [00:06<02:45,  1.42it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6405233144760132:   7%|▉            | 10/146 [00:08<01:19,  1.72it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6405233144760132:   8%|▉            | 11/146 [00:08<01:15,  1.78it/s]Epoch: 1, train for the 13-th batch, train loss: 0.4598141610622406:  10%|█▎           | 12/119 [00:08<01:01,  1.73it/s]Epoch: 1, train for the 13-th batch, train loss: 0.4598141610622406:  11%|█▍           | 13/119 [00:08<01:01,  1.72it/s]Epoch: 1, train for the 16-th batch, train loss: 0.5886879563331604:   6%|▊            | 15/237 [00:07<01:36,  2.30it/s]Epoch: 1, train for the 16-th batch, train loss: 0.5886879563331604:   7%|▉            | 16/237 [00:07<01:47,  2.06it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6857145428657532:   1%|▏              | 5/383 [00:05<04:06,  1.54it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6857145428657532:   2%|▏              | 6/383 [00:05<04:31,  1.39it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6936030387878418:   9%|█            | 13/151 [00:08<01:20,  1.71it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6936030387878418:   9%|█▏           | 14/151 [00:08<01:20,  1.70it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6639301776885986:   3%|▍              | 7/241 [00:07<02:45,  1.42it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6639301776885986:   3%|▍              | 8/241 [00:07<02:35,  1.50it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6380369663238525:   8%|▉            | 11/146 [00:08<01:15,  1.78it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6380369663238525:   8%|█            | 12/146 [00:08<01:14,  1.80it/s]Epoch: 1, train for the 17-th batch, train loss: 0.5473055243492126:   7%|▉            | 16/237 [00:07<01:47,  2.06it/s]Epoch: 1, train for the 17-th batch, train loss: 0.5473055243492126:   7%|▉            | 17/237 [00:07<01:44,  2.11it/s]Epoch: 1, train for the 14-th batch, train loss: 0.4563499987125397:  11%|█▍           | 13/119 [00:09<01:01,  1.72it/s]Epoch: 1, train for the 14-th batch, train loss: 0.4563499987125397:  12%|█▌           | 14/119 [00:09<01:01,  1.71it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6522231698036194:   3%|▍              | 8/241 [00:07<02:35,  1.50it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6522231698036194:   4%|▌              | 9/241 [00:07<02:13,  1.73it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6792547702789307:   2%|▏              | 6/383 [00:05<04:31,  1.39it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6792547702789307:   2%|▎              | 7/383 [00:05<04:10,  1.50it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6434441804885864:   8%|█            | 12/146 [00:09<01:14,  1.80it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6434441804885864:   9%|█▏           | 13/146 [00:09<01:15,  1.76it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6710469126701355:   4%|▌             | 9/241 [00:08<02:13,  1.73it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6710469126701355:   4%|▌            | 10/241 [00:08<02:01,  1.90it/s]Epoch: 1, train for the 15-th batch, train loss: 0.681691586971283:   9%|█▎            | 14/151 [00:09<01:20,  1.70it/s]Epoch: 1, train for the 15-th batch, train loss: 0.681691586971283:  10%|█▍            | 15/151 [00:09<01:30,  1.51it/s]Epoch: 1, train for the 18-th batch, train loss: 0.5454902648925781:   7%|▉            | 17/237 [00:08<01:44,  2.11it/s]Epoch: 1, train for the 18-th batch, train loss: 0.5454902648925781:   8%|▉            | 18/237 [00:08<01:47,  2.03it/s]Epoch: 1, train for the 15-th batch, train loss: 0.48196548223495483:  12%|█▍          | 14/119 [00:10<01:01,  1.71it/s]Epoch: 1, train for the 15-th batch, train loss: 0.48196548223495483:  13%|█▌          | 15/119 [00:10<01:01,  1.70it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6763172745704651:   2%|▎              | 7/383 [00:06<04:10,  1.50it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6763172745704651:   2%|▎              | 8/383 [00:06<03:50,  1.63it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6306899785995483:   9%|█▏           | 13/146 [00:10<01:15,  1.76it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6306899785995483:  10%|█▏           | 14/146 [00:10<01:22,  1.59it/s]Epoch: 1, train for the 16-th batch, train loss: 0.43304336071014404:  13%|█▌          | 15/119 [00:10<01:01,  1.70it/s]Epoch: 1, train for the 16-th batch, train loss: 0.43304336071014404:  13%|█▌          | 16/119 [00:10<00:56,  1.81it/s]Epoch: 1, train for the 19-th batch, train loss: 0.5316863656044006:   8%|▉            | 18/237 [00:08<01:47,  2.03it/s]Epoch: 1, train for the 19-th batch, train loss: 0.5316863656044006:   8%|█            | 19/237 [00:08<01:50,  1.97it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6141066551208496:   4%|▌            | 10/241 [00:08<02:01,  1.90it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6141066551208496:   5%|▌            | 11/241 [00:08<02:06,  1.82it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6905204653739929:  10%|█▎           | 15/151 [00:10<01:30,  1.51it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6905204653739929:  11%|█▍           | 16/151 [00:10<01:25,  1.57it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6741905212402344:   2%|▎              | 8/383 [00:07<03:50,  1.63it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6741905212402344:   2%|▎              | 9/383 [00:07<03:42,  1.68it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6469950675964355:   8%|█            | 19/237 [00:09<01:50,  1.97it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6469950675964355:   8%|█            | 20/237 [00:09<01:50,  1.96it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6987364292144775:  11%|█▍           | 16/151 [00:10<01:25,  1.57it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6987364292144775:  11%|█▍           | 17/151 [00:10<01:23,  1.60it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6774566769599915:   5%|▌            | 11/241 [00:09<02:06,  1.82it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6774566769599915:   5%|▋            | 12/241 [00:09<02:11,  1.74it/s]Epoch: 1, train for the 17-th batch, train loss: 0.39148902893066406:  13%|█▌          | 16/119 [00:11<00:56,  1.81it/s]Epoch: 1, train for the 17-th batch, train loss: 0.39148902893066406:  14%|█▋          | 17/119 [00:11<00:59,  1.72it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6158357858657837:  10%|█▏           | 14/146 [00:10<01:22,  1.59it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6158357858657837:  10%|█▎           | 15/146 [00:10<01:23,  1.57it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6744644045829773:   2%|▎             | 9/383 [00:07<03:42,  1.68it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6744644045829773:   3%|▎            | 10/383 [00:07<03:36,  1.72it/s]Epoch: 1, train for the 21-th batch, train loss: 0.4651242196559906:   8%|█            | 20/237 [00:09<01:50,  1.96it/s]Epoch: 1, train for the 21-th batch, train loss: 0.4651242196559906:   9%|█▏           | 21/237 [00:09<01:51,  1.93it/s]Epoch: 1, train for the 16-th batch, train loss: 0.5944564342498779:  10%|█▎           | 15/146 [00:11<01:23,  1.57it/s]Epoch: 1, train for the 16-th batch, train loss: 0.5944564342498779:  11%|█▍           | 16/146 [00:11<01:17,  1.68it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6632290482521057:   3%|▎            | 10/383 [00:08<03:36,  1.72it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6632290482521057:   3%|▎            | 11/383 [00:08<03:31,  1.76it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6891447901725769:  11%|█▍           | 17/151 [00:11<01:23,  1.60it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6891447901725769:  12%|█▌           | 18/151 [00:11<01:22,  1.61it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6849333047866821:   5%|▋            | 12/241 [00:09<02:11,  1.74it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6849333047866821:   5%|▋            | 13/241 [00:09<02:14,  1.69it/s]Epoch: 1, train for the 18-th batch, train loss: 0.5034376382827759:  14%|█▊           | 17/119 [00:11<00:59,  1.72it/s]Epoch: 1, train for the 18-th batch, train loss: 0.5034376382827759:  15%|█▉           | 18/119 [00:11<01:00,  1.66it/s]Epoch: 1, train for the 22-th batch, train loss: 0.5100152492523193:   9%|█▏           | 21/237 [00:10<01:51,  1.93it/s]Epoch: 1, train for the 22-th batch, train loss: 0.5100152492523193:   9%|█▏           | 22/237 [00:10<01:53,  1.90it/s]Epoch: 1, train for the 17-th batch, train loss: 0.571052610874176:  11%|█▌            | 16/146 [00:11<01:17,  1.68it/s]Epoch: 1, train for the 17-th batch, train loss: 0.571052610874176:  12%|█▋            | 17/146 [00:11<01:15,  1.72it/s]Epoch: 1, train for the 12-th batch, train loss: 0.63538658618927:   3%|▍              | 11/383 [00:08<03:31,  1.76it/s]Epoch: 1, train for the 12-th batch, train loss: 0.63538658618927:   3%|▍              | 12/383 [00:08<03:28,  1.78it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6427801251411438:   5%|▋            | 13/241 [00:10<02:14,  1.69it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6427801251411438:   6%|▊            | 14/241 [00:10<02:14,  1.69it/s]Epoch: 1, train for the 19-th batch, train loss: 0.37352973222732544:  15%|█▊          | 18/119 [00:12<01:00,  1.66it/s]Epoch: 1, train for the 19-th batch, train loss: 0.37352973222732544:  16%|█▉          | 19/119 [00:12<00:59,  1.68it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6965563297271729:  12%|█▌           | 18/151 [00:12<01:22,  1.61it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6965563297271729:  13%|█▋           | 19/151 [00:12<01:22,  1.60it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6454038619995117:   3%|▍            | 12/383 [00:09<03:28,  1.78it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6454038619995117:   3%|▍            | 13/383 [00:09<03:07,  1.97it/s]Epoch: 1, train for the 23-th batch, train loss: 0.7696651220321655:   9%|█▏           | 22/237 [00:11<01:53,  1.90it/s]Epoch: 1, train for the 23-th batch, train loss: 0.7696651220321655:  10%|█▎           | 23/237 [00:11<02:02,  1.75it/s]Epoch: 1, train for the 18-th batch, train loss: 0.5393552184104919:  12%|█▌           | 17/146 [00:12<01:15,  1.72it/s]Epoch: 1, train for the 18-th batch, train loss: 0.5393552184104919:  12%|█▌           | 18/146 [00:12<01:15,  1.70it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6444258093833923:   6%|▊            | 14/241 [00:11<02:14,  1.69it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6444258093833923:   6%|▊            | 15/241 [00:11<02:13,  1.69it/s]Epoch: 1, train for the 20-th batch, train loss: 0.679124116897583:  13%|█▊            | 19/151 [00:12<01:22,  1.60it/s]Epoch: 1, train for the 20-th batch, train loss: 0.679124116897583:  13%|█▊            | 20/151 [00:12<01:19,  1.64it/s]Epoch: 1, train for the 20-th batch, train loss: 0.48990896344184875:  16%|█▉          | 19/119 [00:13<00:59,  1.68it/s]Epoch: 1, train for the 20-th batch, train loss: 0.48990896344184875:  17%|██          | 20/119 [00:13<00:58,  1.68it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6547918319702148:   3%|▍            | 13/383 [00:09<03:07,  1.97it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6547918319702148:   4%|▍            | 14/383 [00:09<03:13,  1.91it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6400214433670044:   6%|▊            | 15/241 [00:11<02:13,  1.69it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6400214433670044:   7%|▊            | 16/241 [00:11<01:58,  1.90it/s]Epoch: 1, train for the 24-th batch, train loss: 0.5309149622917175:  10%|█▎           | 23/237 [00:11<02:02,  1.75it/s]Epoch: 1, train for the 24-th batch, train loss: 0.5309149622917175:  10%|█▎           | 24/237 [00:11<02:01,  1.76it/s]Epoch: 1, train for the 19-th batch, train loss: 0.5940639972686768:  12%|█▌           | 18/146 [00:13<01:15,  1.70it/s]Epoch: 1, train for the 19-th batch, train loss: 0.5940639972686768:  13%|█▋           | 19/146 [00:13<01:16,  1.67it/s]Epoch: 1, train for the 21-th batch, train loss: 0.4900232255458832:  17%|██▏          | 20/119 [00:13<00:58,  1.68it/s]Epoch: 1, train for the 21-th batch, train loss: 0.4900232255458832:  18%|██▎          | 21/119 [00:13<00:59,  1.66it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6921421885490417:  13%|█▋           | 20/151 [00:13<01:19,  1.64it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6921421885490417:  14%|█▊           | 21/151 [00:13<01:25,  1.53it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6244680881500244:   4%|▍            | 14/383 [00:10<03:13,  1.91it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6244680881500244:   4%|▌            | 15/383 [00:10<03:20,  1.84it/s]Epoch: 1, train for the 17-th batch, train loss: 0.5771465301513672:   7%|▊            | 16/241 [00:12<01:58,  1.90it/s]Epoch: 1, train for the 17-th batch, train loss: 0.5771465301513672:   7%|▉            | 17/241 [00:12<01:54,  1.95it/s]Epoch: 1, train for the 25-th batch, train loss: 0.609135627746582:  10%|█▍            | 24/237 [00:12<02:01,  1.76it/s]Epoch: 1, train for the 25-th batch, train loss: 0.609135627746582:  11%|█▍            | 25/237 [00:12<01:59,  1.77it/s]Epoch: 1, train for the 20-th batch, train loss: 0.5896425843238831:  13%|█▋           | 19/146 [00:13<01:16,  1.67it/s]Epoch: 1, train for the 20-th batch, train loss: 0.5896425843238831:  14%|█▊           | 20/146 [00:13<01:15,  1.67it/s]Epoch: 1, train for the 22-th batch, train loss: 0.4199177920818329:  18%|██▎          | 21/119 [00:14<00:59,  1.66it/s]Epoch: 1, train for the 22-th batch, train loss: 0.4199177920818329:  18%|██▍          | 22/119 [00:14<00:58,  1.67it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6758734583854675:  14%|█▊           | 21/151 [00:13<01:25,  1.53it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6758734583854675:  15%|█▉           | 22/151 [00:13<01:20,  1.61it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6629067659378052:   4%|▌            | 15/383 [00:10<03:20,  1.84it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6629067659378052:   4%|▌            | 16/383 [00:10<03:19,  1.84it/s]Epoch: 1, train for the 18-th batch, train loss: 0.61618971824646:   7%|█              | 17/241 [00:12<01:54,  1.95it/s]Epoch: 1, train for the 18-th batch, train loss: 0.61618971824646:   7%|█              | 18/241 [00:12<01:59,  1.87it/s]Epoch: 1, train for the 26-th batch, train loss: 0.48484304547309875:  11%|█▎          | 25/237 [00:12<01:59,  1.77it/s]Epoch: 1, train for the 26-th batch, train loss: 0.48484304547309875:  11%|█▎          | 26/237 [00:12<01:58,  1.78it/s]Epoch: 1, train for the 21-th batch, train loss: 0.5815294981002808:  14%|█▊           | 20/146 [00:14<01:15,  1.67it/s]Epoch: 1, train for the 21-th batch, train loss: 0.5815294981002808:  14%|█▊           | 21/146 [00:14<01:15,  1.65it/s]Epoch: 1, train for the 23-th batch, train loss: 0.44248244166374207:  18%|██▏         | 22/119 [00:14<00:58,  1.67it/s]Epoch: 1, train for the 23-th batch, train loss: 0.44248244166374207:  19%|██▎         | 23/119 [00:14<00:58,  1.65it/s]Epoch: 1, train for the 23-th batch, train loss: 0.674862265586853:  15%|██            | 22/151 [00:14<01:20,  1.61it/s]Epoch: 1, train for the 23-th batch, train loss: 0.674862265586853:  15%|██▏           | 23/151 [00:14<01:17,  1.66it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6613067388534546:   4%|▌            | 16/383 [00:11<03:19,  1.84it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6613067388534546:   4%|▌            | 17/383 [00:11<03:17,  1.85it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6418958306312561:   7%|▉            | 18/241 [00:13<01:59,  1.87it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6418958306312561:   8%|█            | 19/241 [00:13<02:01,  1.82it/s]Epoch: 1, train for the 27-th batch, train loss: 0.43740686774253845:  11%|█▎          | 26/237 [00:13<01:58,  1.78it/s]Epoch: 1, train for the 27-th batch, train loss: 0.43740686774253845:  11%|█▎          | 27/237 [00:13<01:57,  1.79it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6108453273773193:  14%|█▊           | 21/146 [00:14<01:15,  1.65it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6108453273773193:  15%|█▉           | 22/146 [00:14<01:14,  1.66it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6599794030189514:  15%|█▉           | 23/151 [00:15<01:17,  1.66it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6599794030189514:  16%|██           | 24/151 [00:15<01:15,  1.68it/s]Epoch: 1, train for the 24-th batch, train loss: 0.43012121319770813:  19%|██▎         | 23/119 [00:15<00:58,  1.65it/s]Epoch: 1, train for the 24-th batch, train loss: 0.43012121319770813:  20%|██▍         | 24/119 [00:15<00:57,  1.66it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6542148590087891:   4%|▌            | 17/383 [00:11<03:17,  1.85it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6542148590087891:   5%|▌            | 18/383 [00:11<03:18,  1.83it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6409553289413452:  11%|█▍           | 27/237 [00:13<01:57,  1.79it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6409553289413452:  12%|█▌           | 28/237 [00:13<01:55,  1.81it/s]Epoch: 1, train for the 20-th batch, train loss: 0.5898400545120239:   8%|█            | 19/241 [00:13<02:01,  1.82it/s]Epoch: 1, train for the 20-th batch, train loss: 0.5898400545120239:   8%|█            | 20/241 [00:13<02:03,  1.79it/s]Epoch: 1, train for the 23-th batch, train loss: 0.5879839658737183:  15%|█▉           | 22/146 [00:15<01:14,  1.66it/s]Epoch: 1, train for the 23-th batch, train loss: 0.5879839658737183:  16%|██           | 23/146 [00:15<01:14,  1.64it/s]Epoch: 1, train for the 19-th batch, train loss: 0.5746076703071594:   5%|▌            | 18/383 [00:12<03:18,  1.83it/s]Epoch: 1, train for the 19-th batch, train loss: 0.5746076703071594:   5%|▋            | 19/383 [00:12<03:18,  1.83it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6869698166847229:  16%|██           | 24/151 [00:15<01:15,  1.68it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6869698166847229:  17%|██▏          | 25/151 [00:15<01:15,  1.67it/s]Epoch: 1, train for the 25-th batch, train loss: 0.3902339041233063:  20%|██▌          | 24/119 [00:16<00:57,  1.66it/s]Epoch: 1, train for the 25-th batch, train loss: 0.3902339041233063:  21%|██▋          | 25/119 [00:16<00:57,  1.65it/s]Epoch: 1, train for the 29-th batch, train loss: 0.7564933896064758:  12%|█▌           | 28/237 [00:14<01:55,  1.81it/s]Epoch: 1, train for the 29-th batch, train loss: 0.7564933896064758:  12%|█▌           | 29/237 [00:14<01:55,  1.80it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6435418725013733:   8%|█            | 20/241 [00:14<02:03,  1.79it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6435418725013733:   9%|█▏           | 21/241 [00:14<02:03,  1.78it/s]Epoch: 1, train for the 24-th batch, train loss: 0.5749648809432983:  16%|██           | 23/146 [00:16<01:14,  1.64it/s]Epoch: 1, train for the 24-th batch, train loss: 0.5749648809432983:  16%|██▏          | 24/146 [00:16<01:13,  1.66it/s]Epoch: 1, train for the 20-th batch, train loss: 0.574521541595459:   5%|▋             | 19/383 [00:12<03:18,  1.83it/s]Epoch: 1, train for the 20-th batch, train loss: 0.574521541595459:   5%|▋             | 20/383 [00:12<03:18,  1.83it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6654350757598877:  17%|██▏          | 25/151 [00:16<01:15,  1.67it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6654350757598877:  17%|██▏          | 26/151 [00:16<01:13,  1.71it/s]Epoch: 1, train for the 30-th batch, train loss: 0.8196844458580017:  12%|█▌           | 29/237 [00:14<01:55,  1.80it/s]Epoch: 1, train for the 30-th batch, train loss: 0.8196844458580017:  13%|█▋           | 30/237 [00:14<01:51,  1.86it/s]Epoch: 1, train for the 26-th batch, train loss: 0.3610866069793701:  21%|██▋          | 25/119 [00:16<00:57,  1.65it/s]Epoch: 1, train for the 26-th batch, train loss: 0.3610866069793701:  22%|██▊          | 26/119 [00:16<00:56,  1.66it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6669446229934692:   9%|█▏           | 21/241 [00:14<02:03,  1.78it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6669446229934692:   9%|█▏           | 22/241 [00:14<02:04,  1.76it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6220012307167053:  16%|██▏          | 24/146 [00:16<01:13,  1.66it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6220012307167053:  17%|██▏          | 25/146 [00:16<01:13,  1.65it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6074243187904358:   5%|▋            | 20/383 [00:13<03:18,  1.83it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6074243187904358:   5%|▋            | 21/383 [00:13<03:19,  1.82it/s]Epoch: 1, train for the 31-th batch, train loss: 0.4926285445690155:  13%|█▋           | 30/237 [00:15<01:51,  1.86it/s]Epoch: 1, train for the 31-th batch, train loss: 0.4926285445690155:  13%|█▋           | 31/237 [00:15<01:51,  1.84it/s]Epoch: 1, train for the 27-th batch, train loss: 0.689860463142395:  17%|██▍           | 26/151 [00:16<01:13,  1.71it/s]Epoch: 1, train for the 27-th batch, train loss: 0.689860463142395:  18%|██▌           | 27/151 [00:16<01:13,  1.68it/s]Epoch: 1, train for the 27-th batch, train loss: 0.3946075737476349:  22%|██▊          | 26/119 [00:17<00:56,  1.66it/s]Epoch: 1, train for the 27-th batch, train loss: 0.3946075737476349:  23%|██▉          | 27/119 [00:17<00:55,  1.65it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6583228707313538:   9%|█▏           | 22/241 [00:15<02:04,  1.76it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6583228707313538:  10%|█▏           | 23/241 [00:15<02:06,  1.72it/s]Epoch: 1, train for the 32-th batch, train loss: 0.5493658185005188:  13%|█▋           | 31/237 [00:15<01:51,  1.84it/s]Epoch: 1, train for the 32-th batch, train loss: 0.5493658185005188:  14%|█▊           | 32/237 [00:15<01:42,  1.99it/s]Epoch: 1, train for the 26-th batch, train loss: 0.5575525164604187:  17%|██▏          | 25/146 [00:17<01:13,  1.65it/s]Epoch: 1, train for the 26-th batch, train loss: 0.5575525164604187:  18%|██▎          | 26/146 [00:17<01:13,  1.64it/s]Epoch: 1, train for the 22-th batch, train loss: 0.5988610982894897:   5%|▋            | 21/383 [00:14<03:19,  1.82it/s]Epoch: 1, train for the 22-th batch, train loss: 0.5988610982894897:   6%|▋            | 22/383 [00:14<03:33,  1.69it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6835545301437378:  18%|██▎          | 27/151 [00:17<01:13,  1.68it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6835545301437378:  19%|██▍          | 28/151 [00:17<01:13,  1.67it/s]Epoch: 1, train for the 28-th batch, train loss: 0.4497469365596771:  23%|██▉          | 27/119 [00:17<00:55,  1.65it/s]Epoch: 1, train for the 28-th batch, train loss: 0.4497469365596771:  24%|███          | 28/119 [00:17<00:55,  1.64it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6379090547561646:  10%|█▏           | 23/241 [00:16<02:06,  1.72it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6379090547561646:  10%|█▎           | 24/241 [00:16<02:07,  1.70it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6233513355255127:  14%|█▊           | 32/237 [00:16<01:42,  1.99it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6233513355255127:  14%|█▊           | 33/237 [00:16<01:47,  1.90it/s]Epoch: 1, train for the 27-th batch, train loss: 0.5735313296318054:  18%|██▎          | 26/146 [00:17<01:13,  1.64it/s]Epoch: 1, train for the 27-th batch, train loss: 0.5735313296318054:  18%|██▍          | 27/146 [00:17<01:12,  1.65it/s]Epoch: 1, train for the 23-th batch, train loss: 0.5860945582389832:   6%|▋            | 22/383 [00:14<03:33,  1.69it/s]Epoch: 1, train for the 23-th batch, train loss: 0.5860945582389832:   6%|▊            | 23/383 [00:14<03:31,  1.70it/s]Epoch: 1, train for the 29-th batch, train loss: 0.697003185749054:  19%|██▌           | 28/151 [00:18<01:13,  1.67it/s]Epoch: 1, train for the 29-th batch, train loss: 0.697003185749054:  19%|██▋           | 29/151 [00:18<01:12,  1.68it/s]Epoch: 1, train for the 29-th batch, train loss: 0.36238500475883484:  24%|██▊         | 28/119 [00:18<00:55,  1.64it/s]Epoch: 1, train for the 29-th batch, train loss: 0.36238500475883484:  24%|██▉         | 29/119 [00:18<00:54,  1.65it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6637563705444336:  10%|█▎           | 24/241 [00:16<02:07,  1.70it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6637563705444336:  10%|█▎           | 25/241 [00:16<02:07,  1.69it/s]Epoch: 1, train for the 34-th batch, train loss: 0.4846133589744568:  14%|█▊           | 33/237 [00:16<01:47,  1.90it/s]Epoch: 1, train for the 34-th batch, train loss: 0.4846133589744568:  14%|█▊           | 34/237 [00:16<01:47,  1.89it/s]Epoch: 1, train for the 28-th batch, train loss: 0.5505728721618652:  18%|██▍          | 27/146 [00:18<01:12,  1.65it/s]Epoch: 1, train for the 28-th batch, train loss: 0.5505728721618652:  19%|██▍          | 28/146 [00:18<01:11,  1.65it/s]Epoch: 1, train for the 24-th batch, train loss: 0.5714646577835083:   6%|▊            | 23/383 [00:15<03:31,  1.70it/s]Epoch: 1, train for the 24-th batch, train loss: 0.5714646577835083:   6%|▊            | 24/383 [00:15<03:28,  1.72it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6938517689704895:  19%|██▍          | 29/151 [00:18<01:12,  1.68it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6938517689704895:  20%|██▌          | 30/151 [00:18<01:12,  1.66it/s]Epoch: 1, train for the 30-th batch, train loss: 0.3941304385662079:  24%|███▏         | 29/119 [00:19<00:54,  1.65it/s]Epoch: 1, train for the 30-th batch, train loss: 0.3941304385662079:  25%|███▎         | 30/119 [00:19<00:54,  1.64it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6375066041946411:  10%|█▎           | 25/241 [00:17<02:07,  1.69it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6375066041946411:  11%|█▍           | 26/241 [00:17<02:07,  1.69it/s]Epoch: 1, train for the 35-th batch, train loss: 0.7526130676269531:  14%|█▊           | 34/237 [00:17<01:47,  1.89it/s]Epoch: 1, train for the 35-th batch, train loss: 0.7526130676269531:  15%|█▉           | 35/237 [00:17<01:49,  1.84it/s]Epoch: 1, train for the 25-th batch, train loss: 0.5860704779624939:   6%|▊            | 24/383 [00:15<03:28,  1.72it/s]Epoch: 1, train for the 25-th batch, train loss: 0.5860704779624939:   7%|▊            | 25/383 [00:15<03:27,  1.73it/s]Epoch: 1, train for the 29-th batch, train loss: 0.5879103541374207:  19%|██▍          | 28/146 [00:19<01:11,  1.65it/s]Epoch: 1, train for the 29-th batch, train loss: 0.5879103541374207:  20%|██▌          | 29/146 [00:19<01:11,  1.63it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6881147027015686:  20%|██▌          | 30/151 [00:19<01:12,  1.66it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6881147027015686:  21%|██▋          | 31/151 [00:19<01:11,  1.68it/s]Epoch: 1, train for the 31-th batch, train loss: 0.42023733258247375:  25%|███         | 30/119 [00:19<00:54,  1.64it/s]Epoch: 1, train for the 31-th batch, train loss: 0.42023733258247375:  26%|███▏        | 31/119 [00:19<00:53,  1.63it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6548764109611511:  11%|█▍           | 26/241 [00:17<02:07,  1.69it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6548764109611511:  11%|█▍           | 27/241 [00:17<02:06,  1.69it/s]Epoch: 1, train for the 36-th batch, train loss: 0.5197290778160095:  15%|█▉           | 35/237 [00:18<01:49,  1.84it/s]Epoch: 1, train for the 36-th batch, train loss: 0.5197290778160095:  15%|█▉           | 36/237 [00:18<01:49,  1.84it/s]Epoch: 1, train for the 26-th batch, train loss: 0.5870230793952942:   7%|▊            | 25/383 [00:16<03:27,  1.73it/s]Epoch: 1, train for the 26-th batch, train loss: 0.5870230793952942:   7%|▉            | 26/383 [00:16<03:26,  1.73it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6049604415893555:  20%|██▌          | 29/146 [00:19<01:11,  1.63it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6049604415893555:  21%|██▋          | 30/146 [00:19<01:11,  1.63it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6867684721946716:  21%|██▋          | 31/151 [00:19<01:11,  1.68it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6867684721946716:  21%|██▊          | 32/151 [00:19<01:11,  1.67it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6195367574691772:  11%|█▍           | 27/241 [00:18<02:06,  1.69it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6195367574691772:  12%|█▌           | 28/241 [00:18<02:03,  1.72it/s]Epoch: 1, train for the 32-th batch, train loss: 0.325103759765625:  26%|███▋          | 31/119 [00:20<00:53,  1.63it/s]Epoch: 1, train for the 32-th batch, train loss: 0.325103759765625:  27%|███▊          | 32/119 [00:20<00:53,  1.63it/s]Epoch: 1, train for the 37-th batch, train loss: 0.5982125997543335:  15%|█▉           | 36/237 [00:18<01:49,  1.84it/s]Epoch: 1, train for the 37-th batch, train loss: 0.5982125997543335:  16%|██           | 37/237 [00:18<01:49,  1.83it/s]Epoch: 1, train for the 27-th batch, train loss: 0.5560182332992554:   7%|▉            | 26/383 [00:17<03:26,  1.73it/s]Epoch: 1, train for the 27-th batch, train loss: 0.5560182332992554:   7%|▉            | 27/383 [00:17<03:25,  1.73it/s]Epoch: 1, train for the 31-th batch, train loss: 0.5789117217063904:  21%|██▋          | 30/146 [00:20<01:11,  1.63it/s]Epoch: 1, train for the 31-th batch, train loss: 0.5789117217063904:  21%|██▊          | 31/146 [00:20<01:10,  1.63it/s]Epoch: 1, train for the 33-th batch, train loss: 0.686208188533783:  21%|██▉           | 32/151 [00:20<01:11,  1.67it/s]Epoch: 1, train for the 33-th batch, train loss: 0.686208188533783:  22%|███           | 33/151 [00:20<01:10,  1.68it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6396675109863281:  12%|█▌           | 28/241 [00:19<02:03,  1.72it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6396675109863281:  12%|█▌           | 29/241 [00:19<02:03,  1.72it/s]Epoch: 1, train for the 38-th batch, train loss: 0.4619506597518921:  16%|██           | 37/237 [00:19<01:49,  1.83it/s]Epoch: 1, train for the 38-th batch, train loss: 0.4619506597518921:  16%|██           | 38/237 [00:19<01:48,  1.83it/s]Epoch: 1, train for the 33-th batch, train loss: 0.40964415669441223:  27%|███▏        | 32/119 [00:20<00:53,  1.63it/s]Epoch: 1, train for the 33-th batch, train loss: 0.40964415669441223:  28%|███▎        | 33/119 [00:20<00:52,  1.63it/s]Epoch: 1, train for the 28-th batch, train loss: 0.561103105545044:   7%|▉             | 27/383 [00:17<03:25,  1.73it/s]Epoch: 1, train for the 28-th batch, train loss: 0.561103105545044:   7%|█             | 28/383 [00:17<03:20,  1.77it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6634995937347412:  22%|██▊          | 33/151 [00:20<01:10,  1.68it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6634995937347412:  23%|██▉          | 34/151 [00:20<01:08,  1.71it/s]Epoch: 1, train for the 32-th batch, train loss: 0.5310535430908203:  21%|██▊          | 31/146 [00:21<01:10,  1.63it/s]Epoch: 1, train for the 32-th batch, train loss: 0.5310535430908203:  22%|██▊          | 32/146 [00:21<01:09,  1.65it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6685011386871338:  12%|█▌           | 29/241 [00:19<02:03,  1.72it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6685011386871338:  12%|█▌           | 30/241 [00:19<02:03,  1.70it/s]Epoch: 1, train for the 34-th batch, train loss: 0.34498533606529236:  28%|███▎        | 33/119 [00:21<00:52,  1.63it/s]Epoch: 1, train for the 34-th batch, train loss: 0.34498533606529236:  29%|███▍        | 34/119 [00:21<00:51,  1.64it/s]Epoch: 1, train for the 29-th batch, train loss: 0.5100423097610474:   7%|▉            | 28/383 [00:17<03:20,  1.77it/s]Epoch: 1, train for the 29-th batch, train loss: 0.5100423097610474:   8%|▉            | 29/383 [00:17<02:56,  2.00it/s]Epoch: 1, train for the 39-th batch, train loss: 0.8141787648200989:  16%|██           | 38/237 [00:19<01:48,  1.83it/s]Epoch: 1, train for the 39-th batch, train loss: 0.8141787648200989:  16%|██▏          | 39/237 [00:19<02:01,  1.63it/s]Epoch: 1, train for the 35-th batch, train loss: 0.689276397228241:  23%|███▏          | 34/151 [00:21<01:08,  1.71it/s]Epoch: 1, train for the 35-th batch, train loss: 0.689276397228241:  23%|███▏          | 35/151 [00:21<01:07,  1.71it/s]Epoch: 1, train for the 33-th batch, train loss: 0.5759335160255432:  22%|██▊          | 32/146 [00:21<01:09,  1.65it/s]Epoch: 1, train for the 33-th batch, train loss: 0.5759335160255432:  23%|██▉          | 33/146 [00:21<01:09,  1.63it/s]Epoch: 1, train for the 31-th batch, train loss: 0.5671889185905457:  12%|█▌           | 30/241 [00:20<02:03,  1.70it/s]Epoch: 1, train for the 31-th batch, train loss: 0.5671889185905457:  13%|█▋           | 31/241 [00:20<02:01,  1.72it/s]Epoch: 1, train for the 30-th batch, train loss: 0.5151247978210449:   8%|▉            | 29/383 [00:18<02:56,  2.00it/s]Epoch: 1, train for the 30-th batch, train loss: 0.5151247978210449:   8%|█            | 30/383 [00:18<03:06,  1.89it/s]Epoch: 1, train for the 35-th batch, train loss: 0.34532031416893005:  29%|███▍        | 34/119 [00:22<00:51,  1.64it/s]Epoch: 1, train for the 35-th batch, train loss: 0.34532031416893005:  29%|███▌        | 35/119 [00:22<00:51,  1.63it/s]Epoch: 1, train for the 40-th batch, train loss: 0.5508865118026733:  16%|██▏          | 39/237 [00:20<02:01,  1.63it/s]Epoch: 1, train for the 40-th batch, train loss: 0.5508865118026733:  17%|██▏          | 40/237 [00:20<01:58,  1.66it/s]Epoch: 1, train for the 36-th batch, train loss: 0.676952600479126:  23%|███▏          | 35/151 [00:22<01:07,  1.71it/s]Epoch: 1, train for the 36-th batch, train loss: 0.676952600479126:  24%|███▎          | 36/151 [00:22<01:06,  1.72it/s]Epoch: 1, train for the 34-th batch, train loss: 0.581275224685669:  23%|███▏          | 33/146 [00:22<01:09,  1.63it/s]Epoch: 1, train for the 34-th batch, train loss: 0.581275224685669:  23%|███▎          | 34/146 [00:22<01:08,  1.63it/s]Epoch: 1, train for the 32-th batch, train loss: 0.5170425176620483:  13%|█▋           | 31/241 [00:20<02:01,  1.72it/s]Epoch: 1, train for the 32-th batch, train loss: 0.5170425176620483:  13%|█▋           | 32/241 [00:20<02:02,  1.71it/s]Epoch: 1, train for the 31-th batch, train loss: 0.5574042201042175:   8%|█            | 30/383 [00:19<03:06,  1.89it/s]Epoch: 1, train for the 31-th batch, train loss: 0.5574042201042175:   8%|█            | 31/383 [00:19<03:08,  1.87it/s]Epoch: 1, train for the 36-th batch, train loss: 0.32550132274627686:  29%|███▌        | 35/119 [00:22<00:51,  1.63it/s]Epoch: 1, train for the 36-th batch, train loss: 0.32550132274627686:  30%|███▋        | 36/119 [00:22<00:50,  1.63it/s]Epoch: 1, train for the 41-th batch, train loss: 0.4760403335094452:  17%|██▏          | 40/237 [00:21<01:58,  1.66it/s]Epoch: 1, train for the 41-th batch, train loss: 0.4760403335094452:  17%|██▏          | 41/237 [00:21<01:55,  1.69it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6764170527458191:  24%|███          | 36/151 [00:22<01:06,  1.72it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6764170527458191:  25%|███▏         | 37/151 [00:22<01:05,  1.73it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6389693021774292:  13%|█▋           | 32/241 [00:21<02:02,  1.71it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6389693021774292:  14%|█▊           | 33/241 [00:21<02:00,  1.73it/s]Epoch: 1, train for the 35-th batch, train loss: 0.5646441578865051:  23%|███          | 34/146 [00:22<01:08,  1.63it/s]Epoch: 1, train for the 35-th batch, train loss: 0.5646441578865051:  24%|███          | 35/146 [00:22<01:08,  1.63it/s]Epoch: 1, train for the 32-th batch, train loss: 0.5436447262763977:   8%|█            | 31/383 [00:19<03:08,  1.87it/s]Epoch: 1, train for the 32-th batch, train loss: 0.5436447262763977:   8%|█            | 32/383 [00:19<03:13,  1.82it/s]Epoch: 1, train for the 37-th batch, train loss: 0.37203794717788696:  30%|███▋        | 36/119 [00:23<00:50,  1.63it/s]Epoch: 1, train for the 37-th batch, train loss: 0.37203794717788696:  31%|███▋        | 37/119 [00:23<00:50,  1.63it/s]Epoch: 1, train for the 42-th batch, train loss: 0.7108075022697449:  17%|██▏          | 41/237 [00:21<01:55,  1.69it/s]Epoch: 1, train for the 42-th batch, train loss: 0.7108075022697449:  18%|██▎          | 42/237 [00:21<01:55,  1.69it/s]Epoch: 1, train for the 38-th batch, train loss: 0.6656307578086853:  25%|███▏         | 37/151 [00:23<01:05,  1.73it/s]Epoch: 1, train for the 38-th batch, train loss: 0.6656307578086853:  25%|███▎         | 38/151 [00:23<01:04,  1.74it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6027169823646545:  14%|█▊           | 33/241 [00:21<02:00,  1.73it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6027169823646545:  14%|█▊           | 34/241 [00:21<02:00,  1.72it/s]Epoch: 1, train for the 36-th batch, train loss: 0.5490611791610718:  24%|███          | 35/146 [00:23<01:08,  1.63it/s]Epoch: 1, train for the 36-th batch, train loss: 0.5490611791610718:  25%|███▏         | 36/146 [00:23<01:07,  1.63it/s]Epoch: 1, train for the 33-th batch, train loss: 0.555895209312439:   8%|█▏            | 32/383 [00:20<03:13,  1.82it/s]Epoch: 1, train for the 33-th batch, train loss: 0.555895209312439:   9%|█▏            | 33/383 [00:20<03:17,  1.77it/s]Epoch: 1, train for the 38-th batch, train loss: 0.3242388367652893:  31%|████         | 37/119 [00:24<00:50,  1.63it/s]Epoch: 1, train for the 38-th batch, train loss: 0.3242388367652893:  32%|████▏        | 38/119 [00:24<00:49,  1.63it/s]Epoch: 1, train for the 43-th batch, train loss: 0.699641227722168:  18%|██▍           | 42/237 [00:22<01:55,  1.69it/s]Epoch: 1, train for the 43-th batch, train loss: 0.699641227722168:  18%|██▌           | 43/237 [00:22<01:55,  1.68it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6771952509880066:  25%|███▎         | 38/151 [00:23<01:04,  1.74it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6771952509880066:  26%|███▎         | 39/151 [00:23<01:05,  1.71it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6364625692367554:  14%|█▊           | 34/241 [00:22<02:00,  1.72it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6364625692367554:  15%|█▉           | 35/241 [00:22<02:01,  1.69it/s]Epoch: 1, train for the 34-th batch, train loss: 0.5496918559074402:   9%|█            | 33/383 [00:20<03:17,  1.77it/s]Epoch: 1, train for the 34-th batch, train loss: 0.5496918559074402:   9%|█▏           | 34/383 [00:20<03:15,  1.78it/s]Epoch: 1, train for the 37-th batch, train loss: 0.5848785042762756:  25%|███▏         | 36/146 [00:24<01:07,  1.63it/s]Epoch: 1, train for the 37-th batch, train loss: 0.5848785042762756:  25%|███▎         | 37/146 [00:24<01:06,  1.63it/s]Epoch: 1, train for the 44-th batch, train loss: 0.7372260689735413:  18%|██▎          | 43/237 [00:22<01:55,  1.68it/s]Epoch: 1, train for the 44-th batch, train loss: 0.7372260689735413:  19%|██▍          | 44/237 [00:22<01:53,  1.70it/s]Epoch: 1, train for the 39-th batch, train loss: 0.3792031407356262:  32%|████▏        | 38/119 [00:24<00:49,  1.63it/s]Epoch: 1, train for the 39-th batch, train loss: 0.3792031407356262:  33%|████▎        | 39/119 [00:24<00:49,  1.63it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6745561361312866:  26%|███▎         | 39/151 [00:24<01:05,  1.71it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6745561361312866:  26%|███▍         | 40/151 [00:24<01:05,  1.69it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6318725347518921:  15%|█▉           | 35/241 [00:23<02:01,  1.69it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6318725347518921:  15%|█▉           | 36/241 [00:23<02:02,  1.67it/s]Epoch: 1, train for the 35-th batch, train loss: 0.5767691135406494:   9%|█▏           | 34/383 [00:21<03:15,  1.78it/s]Epoch: 1, train for the 35-th batch, train loss: 0.5767691135406494:   9%|█▏           | 35/383 [00:21<03:19,  1.75it/s]Epoch: 1, train for the 38-th batch, train loss: 0.5749390721321106:  25%|███▎         | 37/146 [00:24<01:06,  1.63it/s]Epoch: 1, train for the 38-th batch, train loss: 0.5749390721321106:  26%|███▍         | 38/146 [00:24<01:06,  1.63it/s]Epoch: 1, train for the 45-th batch, train loss: 0.7169086933135986:  19%|██▍          | 44/237 [00:23<01:53,  1.70it/s]Epoch: 1, train for the 45-th batch, train loss: 0.7169086933135986:  19%|██▍          | 45/237 [00:23<01:52,  1.70it/s]Epoch: 1, train for the 40-th batch, train loss: 0.39350295066833496:  33%|███▉        | 39/119 [00:25<00:49,  1.63it/s]Epoch: 1, train for the 40-th batch, train loss: 0.39350295066833496:  34%|████        | 40/119 [00:25<00:48,  1.63it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6663000583648682:  26%|███▍         | 40/151 [00:25<01:05,  1.69it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6663000583648682:  27%|███▌         | 41/151 [00:25<01:05,  1.67it/s]Epoch: 1, train for the 36-th batch, train loss: 0.49629026651382446:   9%|█           | 35/383 [00:22<03:19,  1.75it/s]Epoch: 1, train for the 36-th batch, train loss: 0.49629026651382446:   9%|█▏          | 36/383 [00:22<03:20,  1.73it/s]Epoch: 1, train for the 37-th batch, train loss: 0.5983443260192871:  15%|█▉           | 36/241 [00:23<02:02,  1.67it/s]Epoch: 1, train for the 37-th batch, train loss: 0.5983443260192871:  15%|█▉           | 37/241 [00:23<02:02,  1.66it/s]Epoch: 1, train for the 39-th batch, train loss: 0.5699172019958496:  26%|███▍         | 38/146 [00:25<01:06,  1.63it/s]Epoch: 1, train for the 39-th batch, train loss: 0.5699172019958496:  27%|███▍         | 39/146 [00:25<01:05,  1.63it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6682418584823608:  19%|██▍          | 45/237 [00:23<01:52,  1.70it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6682418584823608:  19%|██▌          | 46/237 [00:23<01:52,  1.70it/s]Epoch: 1, train for the 41-th batch, train loss: 0.40158811211586:  34%|█████          | 40/119 [00:25<00:48,  1.63it/s]Epoch: 1, train for the 41-th batch, train loss: 0.40158811211586:  34%|█████▏         | 41/119 [00:25<00:47,  1.63it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6535524129867554:  27%|███▌         | 41/151 [00:25<01:05,  1.67it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6535524129867554:  28%|███▌         | 42/151 [00:25<01:05,  1.67it/s]Epoch: 1, train for the 37-th batch, train loss: 0.5140285491943359:   9%|█▏           | 36/383 [00:22<03:20,  1.73it/s]Epoch: 1, train for the 37-th batch, train loss: 0.5140285491943359:  10%|█▎           | 37/383 [00:22<03:21,  1.71it/s]Epoch: 1, train for the 38-th batch, train loss: 0.5414361953735352:  15%|█▉           | 37/241 [00:24<02:02,  1.66it/s]Epoch: 1, train for the 38-th batch, train loss: 0.5414361953735352:  16%|██           | 38/241 [00:24<02:01,  1.67it/s]Epoch: 1, train for the 40-th batch, train loss: 0.5646946430206299:  27%|███▍         | 39/146 [00:25<01:05,  1.63it/s]Epoch: 1, train for the 40-th batch, train loss: 0.5646946430206299:  27%|███▌         | 40/146 [00:25<01:04,  1.63it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6434403657913208:  19%|██▌          | 46/237 [00:24<01:52,  1.70it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6434403657913208:  20%|██▌          | 47/237 [00:24<01:51,  1.70it/s]Epoch: 1, train for the 42-th batch, train loss: 0.40946027636528015:  34%|████▏       | 41/119 [00:26<00:47,  1.63it/s]Epoch: 1, train for the 42-th batch, train loss: 0.40946027636528015:  35%|████▏       | 42/119 [00:26<00:47,  1.63it/s]Epoch: 1, train for the 43-th batch, train loss: 0.665477454662323:  28%|███▉          | 42/151 [00:26<01:05,  1.67it/s]Epoch: 1, train for the 43-th batch, train loss: 0.665477454662323:  28%|███▉          | 43/151 [00:26<01:04,  1.68it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6685835719108582:  16%|██           | 38/241 [00:24<02:01,  1.67it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6685835719108582:  16%|██           | 39/241 [00:24<02:00,  1.67it/s]Epoch: 1, train for the 38-th batch, train loss: 0.5710704922676086:  10%|█▎           | 37/383 [00:23<03:21,  1.71it/s]Epoch: 1, train for the 38-th batch, train loss: 0.5710704922676086:  10%|█▎           | 38/383 [00:23<03:23,  1.69it/s]Epoch: 1, train for the 41-th batch, train loss: 0.5485686659812927:  27%|███▌         | 40/146 [00:26<01:04,  1.63it/s]Epoch: 1, train for the 41-th batch, train loss: 0.5485686659812927:  28%|███▋         | 41/146 [00:26<01:04,  1.63it/s]Epoch: 1, train for the 48-th batch, train loss: 0.591595470905304:  20%|██▊           | 47/237 [00:25<01:51,  1.70it/s]Epoch: 1, train for the 48-th batch, train loss: 0.591595470905304:  20%|██▊           | 48/237 [00:25<01:51,  1.70it/s]Epoch: 1, train for the 43-th batch, train loss: 0.39498862624168396:  35%|████▏       | 42/119 [00:27<00:47,  1.63it/s]Epoch: 1, train for the 43-th batch, train loss: 0.39498862624168396:  36%|████▎       | 43/119 [00:27<00:46,  1.63it/s]Epoch: 1, train for the 44-th batch, train loss: 0.643647313117981:  28%|███▉          | 43/151 [00:26<01:04,  1.68it/s]Epoch: 1, train for the 44-th batch, train loss: 0.643647313117981:  29%|████          | 44/151 [00:26<01:04,  1.65it/s]Epoch: 1, train for the 39-th batch, train loss: 0.5582574605941772:  10%|█▎           | 38/383 [00:23<03:23,  1.69it/s]Epoch: 1, train for the 39-th batch, train loss: 0.5582574605941772:  10%|█▎           | 39/383 [00:23<03:19,  1.72it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6563732624053955:  16%|██           | 39/241 [00:25<02:00,  1.67it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6563732624053955:  17%|██▏          | 40/241 [00:25<02:01,  1.65it/s]Epoch: 1, train for the 42-th batch, train loss: 0.5383710265159607:  28%|███▋         | 41/146 [00:27<01:04,  1.63it/s]Epoch: 1, train for the 42-th batch, train loss: 0.5383710265159607:  29%|███▋         | 42/146 [00:27<01:04,  1.62it/s]Epoch: 1, train for the 49-th batch, train loss: 0.638739824295044:  20%|██▊           | 48/237 [00:25<01:51,  1.70it/s]Epoch: 1, train for the 49-th batch, train loss: 0.638739824295044:  21%|██▉           | 49/237 [00:25<01:49,  1.71it/s]Epoch: 1, train for the 44-th batch, train loss: 0.3467852771282196:  36%|████▋        | 43/119 [00:27<00:46,  1.63it/s]Epoch: 1, train for the 44-th batch, train loss: 0.3467852771282196:  37%|████▊        | 44/119 [00:27<00:46,  1.62it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6179174780845642:  29%|███▊         | 44/151 [00:27<01:04,  1.65it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6179174780845642:  30%|███▊         | 45/151 [00:27<01:03,  1.66it/s]Epoch: 1, train for the 40-th batch, train loss: 0.5343741774559021:  10%|█▎           | 39/383 [00:24<03:19,  1.72it/s]Epoch: 1, train for the 40-th batch, train loss: 0.5343741774559021:  10%|█▎           | 40/383 [00:24<03:20,  1.71it/s]Epoch: 1, train for the 41-th batch, train loss: 0.630254328250885:  17%|██▎           | 40/241 [00:26<02:01,  1.65it/s]Epoch: 1, train for the 41-th batch, train loss: 0.630254328250885:  17%|██▍           | 41/241 [00:26<02:00,  1.66it/s]Epoch: 1, train for the 50-th batch, train loss: 0.5777758955955505:  21%|██▋          | 49/237 [00:26<01:49,  1.71it/s]Epoch: 1, train for the 50-th batch, train loss: 0.5777758955955505:  21%|██▋          | 50/237 [00:26<01:48,  1.72it/s]Epoch: 1, train for the 43-th batch, train loss: 0.5669835805892944:  29%|███▋         | 42/146 [00:27<01:04,  1.62it/s]Epoch: 1, train for the 43-th batch, train loss: 0.5669835805892944:  29%|███▊         | 43/146 [00:27<01:03,  1.62it/s]Epoch: 1, train for the 45-th batch, train loss: 0.30182939767837524:  37%|████▍       | 44/119 [00:28<00:46,  1.62it/s]Epoch: 1, train for the 45-th batch, train loss: 0.30182939767837524:  38%|████▌       | 45/119 [00:28<00:45,  1.62it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6726793646812439:  30%|███▊         | 45/151 [00:28<01:03,  1.66it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6726793646812439:  30%|███▉         | 46/151 [00:28<01:04,  1.64it/s]Epoch: 1, train for the 41-th batch, train loss: 0.47091659903526306:  10%|█▎          | 40/383 [00:24<03:20,  1.71it/s]Epoch: 1, train for the 41-th batch, train loss: 0.47091659903526306:  11%|█▎          | 41/383 [00:24<03:19,  1.71it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6281903982162476:  17%|██▏          | 41/241 [00:26<02:00,  1.66it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6281903982162476:  17%|██▎          | 42/241 [00:26<02:01,  1.64it/s]Epoch: 1, train for the 51-th batch, train loss: 0.5871375799179077:  21%|██▋          | 50/237 [00:26<01:48,  1.72it/s]Epoch: 1, train for the 51-th batch, train loss: 0.5871375799179077:  22%|██▊          | 51/237 [00:26<01:47,  1.73it/s]Epoch: 1, train for the 44-th batch, train loss: 0.5218064785003662:  29%|███▊         | 43/146 [00:28<01:03,  1.62it/s]Epoch: 1, train for the 44-th batch, train loss: 0.5218064785003662:  30%|███▉         | 44/146 [00:28<01:03,  1.62it/s]Epoch: 1, train for the 46-th batch, train loss: 0.3411920964717865:  38%|████▉        | 45/119 [00:28<00:45,  1.62it/s]Epoch: 1, train for the 46-th batch, train loss: 0.3411920964717865:  39%|█████        | 46/119 [00:28<00:45,  1.62it/s]Epoch: 1, train for the 42-th batch, train loss: 0.5174242854118347:  11%|█▍           | 41/383 [00:25<03:19,  1.71it/s]Epoch: 1, train for the 42-th batch, train loss: 0.5174242854118347:  11%|█▍           | 42/383 [00:25<03:14,  1.75it/s]Epoch: 1, train for the 47-th batch, train loss: 0.607958197593689:  30%|████▎         | 46/151 [00:28<01:04,  1.64it/s]Epoch: 1, train for the 47-th batch, train loss: 0.607958197593689:  31%|████▎         | 47/151 [00:28<01:02,  1.65it/s]Epoch: 1, train for the 52-th batch, train loss: 0.7694175839424133:  22%|██▊          | 51/237 [00:27<01:47,  1.73it/s]Epoch: 1, train for the 52-th batch, train loss: 0.7694175839424133:  22%|██▊          | 52/237 [00:27<01:46,  1.73it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6425849199295044:  17%|██▎          | 42/241 [00:27<02:01,  1.64it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6425849199295044:  18%|██▎          | 43/241 [00:27<02:00,  1.65it/s]Epoch: 1, train for the 45-th batch, train loss: 0.558142364025116:  30%|████▏         | 44/146 [00:29<01:03,  1.62it/s]Epoch: 1, train for the 45-th batch, train loss: 0.558142364025116:  31%|████▎         | 45/146 [00:29<01:02,  1.62it/s]Epoch: 1, train for the 47-th batch, train loss: 0.43009230494499207:  39%|████▋       | 46/119 [00:29<00:45,  1.62it/s]Epoch: 1, train for the 47-th batch, train loss: 0.43009230494499207:  39%|████▋       | 47/119 [00:29<00:44,  1.62it/s]Epoch: 1, train for the 43-th batch, train loss: 0.5700652599334717:  11%|█▍           | 42/383 [00:26<03:14,  1.75it/s]Epoch: 1, train for the 43-th batch, train loss: 0.5700652599334717:  11%|█▍           | 43/383 [00:26<03:16,  1.73it/s]Epoch: 1, train for the 48-th batch, train loss: 0.6009483933448792:  31%|████         | 47/151 [00:29<01:02,  1.65it/s]Epoch: 1, train for the 48-th batch, train loss: 0.6009483933448792:  32%|████▏        | 48/151 [00:29<01:02,  1.65it/s]Epoch: 1, train for the 53-th batch, train loss: 0.5463827252388:  22%|███▌            | 52/237 [00:28<01:46,  1.73it/s]Epoch: 1, train for the 53-th batch, train loss: 0.5463827252388:  22%|███▌            | 53/237 [00:28<01:46,  1.73it/s]Epoch: 1, train for the 44-th batch, train loss: 0.6745012998580933:  18%|██▎          | 43/241 [00:28<02:00,  1.65it/s]Epoch: 1, train for the 44-th batch, train loss: 0.6745012998580933:  18%|██▎          | 44/241 [00:28<01:59,  1.64it/s]Epoch: 1, train for the 46-th batch, train loss: 0.5250458121299744:  31%|████         | 45/146 [00:29<01:02,  1.62it/s]Epoch: 1, train for the 46-th batch, train loss: 0.5250458121299744:  32%|████         | 46/146 [00:29<01:01,  1.62it/s]Epoch: 1, train for the 48-th batch, train loss: 0.3841148018836975:  39%|█████▏       | 47/119 [00:30<00:44,  1.62it/s]Epoch: 1, train for the 48-th batch, train loss: 0.3841148018836975:  40%|█████▏       | 48/119 [00:30<00:43,  1.62it/s]Epoch: 1, train for the 44-th batch, train loss: 0.5255688428878784:  11%|█▍           | 43/383 [00:26<03:16,  1.73it/s]Epoch: 1, train for the 44-th batch, train loss: 0.5255688428878784:  11%|█▍           | 44/383 [00:26<03:16,  1.73it/s]Epoch: 1, train for the 49-th batch, train loss: 0.600452721118927:  32%|████▍         | 48/151 [00:29<01:02,  1.65it/s]Epoch: 1, train for the 49-th batch, train loss: 0.600452721118927:  32%|████▌         | 49/151 [00:29<01:02,  1.64it/s]Epoch: 1, train for the 54-th batch, train loss: 0.6004770994186401:  22%|██▉          | 53/237 [00:28<01:46,  1.73it/s]Epoch: 1, train for the 54-th batch, train loss: 0.6004770994186401:  23%|██▉          | 54/237 [00:28<01:45,  1.73it/s]Epoch: 1, train for the 45-th batch, train loss: 0.5844818949699402:  18%|██▎          | 44/241 [00:28<01:59,  1.64it/s]Epoch: 1, train for the 45-th batch, train loss: 0.5844818949699402:  19%|██▍          | 45/241 [00:28<01:58,  1.65it/s]Epoch: 1, train for the 47-th batch, train loss: 0.5513188242912292:  32%|████         | 46/146 [00:30<01:01,  1.62it/s]Epoch: 1, train for the 47-th batch, train loss: 0.5513188242912292:  32%|████▏        | 47/146 [00:30<01:01,  1.62it/s]Epoch: 1, train for the 49-th batch, train loss: 0.40684646368026733:  40%|████▊       | 48/119 [00:30<00:43,  1.62it/s]Epoch: 1, train for the 49-th batch, train loss: 0.40684646368026733:  41%|████▉       | 49/119 [00:30<00:43,  1.62it/s]Epoch: 1, train for the 45-th batch, train loss: 0.5239521861076355:  11%|█▍           | 44/383 [00:27<03:16,  1.73it/s]Epoch: 1, train for the 45-th batch, train loss: 0.5239521861076355:  12%|█▌           | 45/383 [00:27<03:16,  1.72it/s]Epoch: 1, train for the 50-th batch, train loss: 0.5960955023765564:  32%|████▏        | 49/151 [00:30<01:02,  1.64it/s]Epoch: 1, train for the 50-th batch, train loss: 0.5960955023765564:  33%|████▎        | 50/151 [00:30<01:00,  1.66it/s]Epoch: 1, train for the 55-th batch, train loss: 0.5762109756469727:  23%|██▉          | 54/237 [00:29<01:45,  1.73it/s]Epoch: 1, train for the 55-th batch, train loss: 0.5762109756469727:  23%|███          | 55/237 [00:29<01:44,  1.74it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6005685329437256:  19%|██▍          | 45/241 [00:29<01:58,  1.65it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6005685329437256:  19%|██▍          | 46/241 [00:29<01:57,  1.66it/s]Epoch: 1, train for the 48-th batch, train loss: 0.5337307453155518:  32%|████▏        | 47/146 [00:30<01:01,  1.62it/s]Epoch: 1, train for the 48-th batch, train loss: 0.5337307453155518:  33%|████▎        | 48/146 [00:30<01:00,  1.62it/s]Epoch: 1, train for the 50-th batch, train loss: 0.3465745151042938:  41%|█████▎       | 49/119 [00:31<00:43,  1.62it/s]Epoch: 1, train for the 50-th batch, train loss: 0.3465745151042938:  42%|█████▍       | 50/119 [00:31<00:42,  1.62it/s]Epoch: 1, train for the 46-th batch, train loss: 0.5895882844924927:  12%|█▌           | 45/383 [00:27<03:16,  1.72it/s]Epoch: 1, train for the 46-th batch, train loss: 0.5895882844924927:  12%|█▌           | 46/383 [00:27<03:14,  1.73it/s]Epoch: 1, train for the 51-th batch, train loss: 0.6491350531578064:  33%|████▎        | 50/151 [00:31<01:00,  1.66it/s]Epoch: 1, train for the 51-th batch, train loss: 0.6491350531578064:  34%|████▍        | 51/151 [00:31<01:00,  1.64it/s]Epoch: 1, train for the 56-th batch, train loss: 0.5937661528587341:  23%|███          | 55/237 [00:29<01:44,  1.74it/s]Epoch: 1, train for the 56-th batch, train loss: 0.5937661528587341:  24%|███          | 56/237 [00:29<01:44,  1.74it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6373291015625:  19%|███             | 46/241 [00:29<01:57,  1.66it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6373291015625:  20%|███             | 47/241 [00:29<01:57,  1.65it/s]Epoch: 1, train for the 49-th batch, train loss: 0.5647546052932739:  33%|████▎        | 48/146 [00:31<01:00,  1.62it/s]Epoch: 1, train for the 49-th batch, train loss: 0.5647546052932739:  34%|████▎        | 49/146 [00:31<00:59,  1.62it/s]Epoch: 1, train for the 47-th batch, train loss: 0.5011143088340759:  12%|█▌           | 46/383 [00:28<03:14,  1.73it/s]Epoch: 1, train for the 47-th batch, train loss: 0.5011143088340759:  12%|█▌           | 47/383 [00:28<03:11,  1.76it/s]Epoch: 1, train for the 51-th batch, train loss: 0.41588348150253296:  42%|█████       | 50/119 [00:32<00:42,  1.62it/s]Epoch: 1, train for the 51-th batch, train loss: 0.41588348150253296:  43%|█████▏      | 51/119 [00:32<00:41,  1.62it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6397151947021484:  34%|████▍        | 51/151 [00:31<01:00,  1.64it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6397151947021484:  34%|████▍        | 52/151 [00:31<00:59,  1.66it/s]Epoch: 1, train for the 57-th batch, train loss: 0.6638990044593811:  24%|███          | 56/237 [00:30<01:44,  1.74it/s]Epoch: 1, train for the 57-th batch, train loss: 0.6638990044593811:  24%|███▏         | 57/237 [00:30<01:43,  1.74it/s]Epoch: 1, train for the 48-th batch, train loss: 0.598322868347168:  20%|██▋           | 47/241 [00:30<01:57,  1.65it/s]Epoch: 1, train for the 48-th batch, train loss: 0.598322868347168:  20%|██▊           | 48/241 [00:30<01:56,  1.66it/s]Epoch: 1, train for the 50-th batch, train loss: 0.5413948893547058:  34%|████▎        | 49/146 [00:32<00:59,  1.62it/s]Epoch: 1, train for the 50-th batch, train loss: 0.5413948893547058:  34%|████▍        | 50/146 [00:32<00:59,  1.62it/s]Epoch: 1, train for the 48-th batch, train loss: 0.4557918310165405:  12%|█▌           | 47/383 [00:28<03:11,  1.76it/s]Epoch: 1, train for the 48-th batch, train loss: 0.4557918310165405:  13%|█▋           | 48/383 [00:28<03:14,  1.72it/s]Epoch: 1, train for the 52-th batch, train loss: 0.38423898816108704:  43%|█████▏      | 51/119 [00:32<00:41,  1.62it/s]Epoch: 1, train for the 52-th batch, train loss: 0.38423898816108704:  44%|█████▏      | 52/119 [00:32<00:41,  1.62it/s]Epoch: 1, train for the 58-th batch, train loss: 0.5692157745361328:  24%|███▏         | 57/237 [00:30<01:43,  1.74it/s]Epoch: 1, train for the 58-th batch, train loss: 0.5692157745361328:  24%|███▏         | 58/237 [00:30<01:43,  1.73it/s]Epoch: 1, train for the 53-th batch, train loss: 0.6536279320716858:  34%|████▍        | 52/151 [00:32<00:59,  1.66it/s]Epoch: 1, train for the 53-th batch, train loss: 0.6536279320716858:  35%|████▌        | 53/151 [00:32<00:59,  1.65it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6338249444961548:  20%|██▌          | 48/241 [00:31<01:56,  1.66it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6338249444961548:  20%|██▋          | 49/241 [00:31<01:56,  1.65it/s]Epoch: 1, train for the 51-th batch, train loss: 0.5404531955718994:  34%|████▍        | 50/146 [00:32<00:59,  1.62it/s]Epoch: 1, train for the 51-th batch, train loss: 0.5404531955718994:  35%|████▌        | 51/146 [00:32<00:58,  1.62it/s]Epoch: 1, train for the 49-th batch, train loss: 0.536794126033783:  13%|█▊            | 48/383 [00:29<03:14,  1.72it/s]Epoch: 1, train for the 49-th batch, train loss: 0.536794126033783:  13%|█▊            | 49/383 [00:29<03:13,  1.72it/s]Epoch: 1, train for the 53-th batch, train loss: 0.3256394863128662:  44%|█████▋       | 52/119 [00:33<00:41,  1.62it/s]Epoch: 1, train for the 53-th batch, train loss: 0.3256394863128662:  45%|█████▊       | 53/119 [00:33<00:40,  1.62it/s]Epoch: 1, train for the 59-th batch, train loss: 0.6231393814086914:  24%|███▏         | 58/237 [00:31<01:43,  1.73it/s]Epoch: 1, train for the 59-th batch, train loss: 0.6231393814086914:  25%|███▏         | 59/237 [00:31<01:43,  1.73it/s]Epoch: 1, train for the 54-th batch, train loss: 0.5302928686141968:  35%|████▌        | 53/151 [00:32<00:59,  1.65it/s]Epoch: 1, train for the 54-th batch, train loss: 0.5302928686141968:  36%|████▋        | 54/151 [00:32<00:58,  1.66it/s]Epoch: 1, train for the 50-th batch, train loss: 0.644223153591156:  20%|██▊           | 49/241 [00:31<01:56,  1.65it/s]Epoch: 1, train for the 50-th batch, train loss: 0.644223153591156:  21%|██▉           | 50/241 [00:31<01:55,  1.65it/s]Epoch: 1, train for the 52-th batch, train loss: 0.5537013411521912:  35%|████▌        | 51/146 [00:33<00:58,  1.62it/s]Epoch: 1, train for the 52-th batch, train loss: 0.5537013411521912:  36%|████▋        | 52/146 [00:33<00:57,  1.63it/s]Epoch: 1, train for the 50-th batch, train loss: 0.38725745677948:  13%|█▉             | 49/383 [00:30<03:13,  1.72it/s]Epoch: 1, train for the 50-th batch, train loss: 0.38725745677948:  13%|█▉             | 50/383 [00:30<03:13,  1.72it/s]Epoch: 1, train for the 54-th batch, train loss: 0.3165297508239746:  45%|█████▊       | 53/119 [00:33<00:40,  1.62it/s]Epoch: 1, train for the 54-th batch, train loss: 0.3165297508239746:  45%|█████▉       | 54/119 [00:33<00:40,  1.62it/s]Epoch: 1, train for the 60-th batch, train loss: 0.5973591804504395:  25%|███▏         | 59/237 [00:32<01:43,  1.73it/s]Epoch: 1, train for the 60-th batch, train loss: 0.5973591804504395:  25%|███▎         | 60/237 [00:32<01:42,  1.72it/s]Epoch: 1, train for the 55-th batch, train loss: 0.5454431176185608:  36%|████▋        | 54/151 [00:33<00:58,  1.66it/s]Epoch: 1, train for the 55-th batch, train loss: 0.5454431176185608:  36%|████▋        | 55/151 [00:33<00:58,  1.65it/s]Epoch: 1, train for the 51-th batch, train loss: 0.6032277345657349:  21%|██▋          | 50/241 [00:32<01:55,  1.65it/s]Epoch: 1, train for the 51-th batch, train loss: 0.6032277345657349:  21%|██▊          | 51/241 [00:32<01:55,  1.64it/s]Epoch: 1, train for the 51-th batch, train loss: 0.4823167324066162:  13%|█▋           | 50/383 [00:30<03:13,  1.72it/s]Epoch: 1, train for the 51-th batch, train loss: 0.4823167324066162:  13%|█▋           | 51/383 [00:30<03:09,  1.75it/s]Epoch: 1, train for the 53-th batch, train loss: 0.5134820938110352:  36%|████▋        | 52/146 [00:33<00:57,  1.63it/s]Epoch: 1, train for the 53-th batch, train loss: 0.5134820938110352:  36%|████▋        | 53/146 [00:33<00:57,  1.62it/s]Epoch: 1, train for the 61-th batch, train loss: 0.5544661283493042:  25%|███▎         | 60/237 [00:32<01:42,  1.72it/s]Epoch: 1, train for the 61-th batch, train loss: 0.5544661283493042:  26%|███▎         | 61/237 [00:32<01:41,  1.74it/s]Epoch: 1, train for the 55-th batch, train loss: 0.2686459422111511:  45%|█████▉       | 54/119 [00:34<00:40,  1.62it/s]Epoch: 1, train for the 55-th batch, train loss: 0.2686459422111511:  46%|██████       | 55/119 [00:34<00:39,  1.62it/s]Epoch: 1, train for the 56-th batch, train loss: 0.5992596745491028:  36%|████▋        | 55/151 [00:34<00:58,  1.65it/s]Epoch: 1, train for the 56-th batch, train loss: 0.5992596745491028:  37%|████▊        | 56/151 [00:34<00:58,  1.63it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6221281886100769:  21%|██▊          | 51/241 [00:32<01:55,  1.64it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6221281886100769:  22%|██▊          | 52/241 [00:32<01:56,  1.63it/s]Epoch: 1, train for the 52-th batch, train loss: 0.4776827096939087:  13%|█▋           | 51/383 [00:31<03:09,  1.75it/s]Epoch: 1, train for the 52-th batch, train loss: 0.4776827096939087:  14%|█▊           | 52/383 [00:31<03:10,  1.74it/s]Epoch: 1, train for the 54-th batch, train loss: 0.5195145010948181:  36%|████▋        | 53/146 [00:34<00:57,  1.62it/s]Epoch: 1, train for the 54-th batch, train loss: 0.5195145010948181:  37%|████▊        | 54/146 [00:34<00:56,  1.62it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6915022134780884:  26%|███▎         | 61/237 [00:33<01:41,  1.74it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6915022134780884:  26%|███▍         | 62/237 [00:33<01:41,  1.72it/s]Epoch: 1, train for the 56-th batch, train loss: 0.32323676347732544:  46%|█████▌      | 55/119 [00:35<00:39,  1.62it/s]Epoch: 1, train for the 56-th batch, train loss: 0.32323676347732544:  47%|█████▋      | 56/119 [00:35<00:38,  1.62it/s]Epoch: 1, train for the 57-th batch, train loss: 0.5869819521903992:  37%|████▊        | 56/151 [00:34<00:58,  1.63it/s]Epoch: 1, train for the 57-th batch, train loss: 0.5869819521903992:  38%|████▉        | 57/151 [00:34<00:57,  1.63it/s]Epoch: 1, train for the 53-th batch, train loss: 0.44285550713539124:  22%|██▌         | 52/241 [00:33<01:56,  1.63it/s]Epoch: 1, train for the 53-th batch, train loss: 0.44285550713539124:  22%|██▋         | 53/241 [00:33<01:55,  1.63it/s]Epoch: 1, train for the 53-th batch, train loss: 0.5006160736083984:  14%|█▊           | 52/383 [00:31<03:10,  1.74it/s]Epoch: 1, train for the 53-th batch, train loss: 0.5006160736083984:  14%|█▊           | 53/383 [00:31<03:12,  1.72it/s]Epoch: 1, train for the 55-th batch, train loss: 0.5753887295722961:  37%|████▊        | 54/146 [00:35<00:56,  1.62it/s]Epoch: 1, train for the 55-th batch, train loss: 0.5753887295722961:  38%|████▉        | 55/146 [00:35<00:56,  1.62it/s]Epoch: 1, train for the 63-th batch, train loss: 0.7672612071037292:  26%|███▍         | 62/237 [00:33<01:41,  1.72it/s]Epoch: 1, train for the 63-th batch, train loss: 0.7672612071037292:  27%|███▍         | 63/237 [00:33<01:42,  1.70it/s]Epoch: 1, train for the 57-th batch, train loss: 0.27071046829223633:  47%|█████▋      | 56/119 [00:35<00:38,  1.62it/s]Epoch: 1, train for the 57-th batch, train loss: 0.27071046829223633:  48%|█████▋      | 57/119 [00:35<00:38,  1.62it/s]Epoch: 1, train for the 58-th batch, train loss: 0.6280391812324524:  38%|████▉        | 57/151 [00:35<00:57,  1.63it/s]Epoch: 1, train for the 58-th batch, train loss: 0.6280391812324524:  38%|████▉        | 58/151 [00:35<00:57,  1.62it/s]Epoch: 1, train for the 54-th batch, train loss: 0.4845389425754547:  22%|██▊          | 53/241 [00:34<01:55,  1.63it/s]Epoch: 1, train for the 54-th batch, train loss: 0.4845389425754547:  22%|██▉          | 54/241 [00:34<01:55,  1.63it/s]Epoch: 1, train for the 54-th batch, train loss: 0.45149698853492737:  14%|█▋          | 53/383 [00:32<03:12,  1.72it/s]Epoch: 1, train for the 54-th batch, train loss: 0.45149698853492737:  14%|█▋          | 54/383 [00:32<03:15,  1.68it/s]Epoch: 1, train for the 56-th batch, train loss: 0.5093744397163391:  38%|████▉        | 55/146 [00:35<00:56,  1.62it/s]Epoch: 1, train for the 56-th batch, train loss: 0.5093744397163391:  38%|████▉        | 56/146 [00:35<00:55,  1.62it/s]Epoch: 1, train for the 64-th batch, train loss: 0.6280336976051331:  27%|███▍         | 63/237 [00:34<01:42,  1.70it/s]Epoch: 1, train for the 64-th batch, train loss: 0.6280336976051331:  27%|███▌         | 64/237 [00:34<01:42,  1.68it/s]Epoch: 1, train for the 58-th batch, train loss: 0.28688496351242065:  48%|█████▋      | 57/119 [00:36<00:38,  1.62it/s]Epoch: 1, train for the 58-th batch, train loss: 0.28688496351242065:  49%|█████▊      | 58/119 [00:36<00:37,  1.62it/s]Epoch: 1, train for the 59-th batch, train loss: 0.603878378868103:  38%|█████▍        | 58/151 [00:36<00:57,  1.62it/s]Epoch: 1, train for the 59-th batch, train loss: 0.603878378868103:  39%|█████▍        | 59/151 [00:36<00:55,  1.64it/s]Epoch: 1, train for the 55-th batch, train loss: 0.3706980049610138:  22%|██▉          | 54/241 [00:34<01:55,  1.63it/s]Epoch: 1, train for the 55-th batch, train loss: 0.3706980049610138:  23%|██▉          | 55/241 [00:34<01:53,  1.64it/s]Epoch: 1, train for the 55-th batch, train loss: 0.5994876027107239:  14%|█▊           | 54/383 [00:33<03:15,  1.68it/s]Epoch: 1, train for the 55-th batch, train loss: 0.5994876027107239:  14%|█▊           | 55/383 [00:33<03:15,  1.68it/s]Epoch: 1, train for the 57-th batch, train loss: 0.5423517227172852:  38%|████▉        | 56/146 [00:36<00:55,  1.62it/s]Epoch: 1, train for the 57-th batch, train loss: 0.5423517227172852:  39%|█████        | 57/146 [00:36<00:54,  1.62it/s]Epoch: 1, train for the 65-th batch, train loss: 0.6589075922966003:  27%|███▌         | 64/237 [00:35<01:42,  1.68it/s]Epoch: 1, train for the 65-th batch, train loss: 0.6589075922966003:  27%|███▌         | 65/237 [00:35<01:42,  1.68it/s]Epoch: 1, train for the 59-th batch, train loss: 0.32317954301834106:  49%|█████▊      | 58/119 [00:36<00:37,  1.62it/s]Epoch: 1, train for the 59-th batch, train loss: 0.32317954301834106:  50%|█████▉      | 59/119 [00:36<00:36,  1.63it/s]Epoch: 1, train for the 60-th batch, train loss: 0.5855939388275146:  39%|█████        | 59/151 [00:36<00:55,  1.64it/s]Epoch: 1, train for the 60-th batch, train loss: 0.5855939388275146:  40%|█████▏       | 60/151 [00:36<00:56,  1.62it/s]Epoch: 1, train for the 56-th batch, train loss: 0.5081477165222168:  14%|█▊           | 55/383 [00:33<03:15,  1.68it/s]Epoch: 1, train for the 56-th batch, train loss: 0.5081477165222168:  15%|█▉           | 56/383 [00:33<02:57,  1.84it/s]Epoch: 1, train for the 56-th batch, train loss: 0.42642104625701904:  23%|██▋         | 55/241 [00:35<01:53,  1.64it/s]Epoch: 1, train for the 56-th batch, train loss: 0.42642104625701904:  23%|██▊         | 56/241 [00:35<01:54,  1.62it/s]Epoch: 1, train for the 58-th batch, train loss: 0.5667545795440674:  39%|█████        | 57/146 [00:37<00:54,  1.62it/s]Epoch: 1, train for the 58-th batch, train loss: 0.5667545795440674:  40%|█████▏       | 58/146 [00:37<00:54,  1.63it/s]Epoch: 1, train for the 66-th batch, train loss: 0.7130037546157837:  27%|███▌         | 65/237 [00:35<01:42,  1.68it/s]Epoch: 1, train for the 66-th batch, train loss: 0.7130037546157837:  28%|███▌         | 66/237 [00:35<01:44,  1.63it/s]Epoch: 1, train for the 60-th batch, train loss: 0.2914521396160126:  50%|██████▍      | 59/119 [00:37<00:36,  1.63it/s]Epoch: 1, train for the 60-th batch, train loss: 0.2914521396160126:  50%|██████▌      | 60/119 [00:37<00:36,  1.63it/s]Epoch: 1, train for the 61-th batch, train loss: 0.5920272469520569:  40%|█████▏       | 60/151 [00:37<00:56,  1.62it/s]Epoch: 1, train for the 61-th batch, train loss: 0.5920272469520569:  40%|█████▎       | 61/151 [00:37<00:55,  1.62it/s]Epoch: 1, train for the 57-th batch, train loss: 0.4944542646408081:  15%|█▉           | 56/383 [00:34<02:57,  1.84it/s]Epoch: 1, train for the 57-th batch, train loss: 0.4944542646408081:  15%|█▉           | 57/383 [00:34<02:59,  1.81it/s]Epoch: 1, train for the 57-th batch, train loss: 0.4482314884662628:  23%|███          | 56/241 [00:35<01:54,  1.62it/s]Epoch: 1, train for the 57-th batch, train loss: 0.4482314884662628:  24%|███          | 57/241 [00:35<01:53,  1.63it/s]Epoch: 1, train for the 59-th batch, train loss: 0.48775815963745117:  40%|████▊       | 58/146 [00:37<00:54,  1.63it/s]Epoch: 1, train for the 59-th batch, train loss: 0.48775815963745117:  40%|████▊       | 59/146 [00:37<00:53,  1.63it/s]Epoch: 1, train for the 67-th batch, train loss: 0.7066572308540344:  28%|███▌         | 66/237 [00:36<01:44,  1.63it/s]Epoch: 1, train for the 67-th batch, train loss: 0.7066572308540344:  28%|███▋         | 67/237 [00:36<01:44,  1.62it/s]Epoch: 1, train for the 61-th batch, train loss: 0.28033676743507385:  50%|██████      | 60/119 [00:38<00:36,  1.63it/s]Epoch: 1, train for the 61-th batch, train loss: 0.28033676743507385:  51%|██████▏     | 61/119 [00:38<00:35,  1.63it/s]Epoch: 1, train for the 62-th batch, train loss: 0.5850735902786255:  40%|█████▎       | 61/151 [00:37<00:55,  1.62it/s]Epoch: 1, train for the 62-th batch, train loss: 0.5850735902786255:  41%|█████▎       | 62/151 [00:37<00:54,  1.63it/s]Epoch: 1, train for the 58-th batch, train loss: 0.48241126537323:  15%|██▏            | 57/383 [00:34<02:59,  1.81it/s]Epoch: 1, train for the 58-th batch, train loss: 0.48241126537323:  15%|██▎            | 58/383 [00:34<03:02,  1.78it/s]Epoch: 1, train for the 58-th batch, train loss: 0.29497048258781433:  24%|██▊         | 57/241 [00:36<01:53,  1.63it/s]Epoch: 1, train for the 58-th batch, train loss: 0.29497048258781433:  24%|██▉         | 58/241 [00:36<01:52,  1.63it/s]Epoch: 1, train for the 60-th batch, train loss: 0.5265440344810486:  40%|█████▎       | 59/146 [00:38<00:53,  1.63it/s]Epoch: 1, train for the 60-th batch, train loss: 0.5265440344810486:  41%|█████▎       | 60/146 [00:38<00:52,  1.63it/s]Epoch: 1, train for the 68-th batch, train loss: 0.6713253855705261:  28%|███▋         | 67/237 [00:36<01:44,  1.62it/s]Epoch: 1, train for the 68-th batch, train loss: 0.6713253855705261:  29%|███▋         | 68/237 [00:36<01:43,  1.63it/s]Epoch: 1, train for the 62-th batch, train loss: 0.3666527569293976:  51%|██████▋      | 61/119 [00:38<00:35,  1.63it/s]Epoch: 1, train for the 62-th batch, train loss: 0.3666527569293976:  52%|██████▊      | 62/119 [00:38<00:34,  1.63it/s]Epoch: 1, train for the 59-th batch, train loss: 0.47672292590141296:  15%|█▊          | 58/383 [00:35<03:02,  1.78it/s]Epoch: 1, train for the 59-th batch, train loss: 0.47672292590141296:  15%|█▊          | 59/383 [00:35<03:06,  1.74it/s]Epoch: 1, train for the 63-th batch, train loss: 0.5904375314712524:  41%|█████▎       | 62/151 [00:38<00:54,  1.63it/s]Epoch: 1, train for the 63-th batch, train loss: 0.5904375314712524:  42%|█████▍       | 63/151 [00:38<00:54,  1.63it/s]Epoch: 1, train for the 59-th batch, train loss: 0.5524964928627014:  24%|███▏         | 58/241 [00:37<01:52,  1.63it/s]Epoch: 1, train for the 59-th batch, train loss: 0.5524964928627014:  24%|███▏         | 59/241 [00:37<01:51,  1.63it/s]Epoch: 1, train for the 61-th batch, train loss: 0.5550110340118408:  41%|█████▎       | 60/146 [00:38<00:52,  1.63it/s]Epoch: 1, train for the 61-th batch, train loss: 0.5550110340118408:  42%|█████▍       | 61/146 [00:38<00:52,  1.63it/s]Epoch: 1, train for the 69-th batch, train loss: 0.6965430378913879:  29%|███▋         | 68/237 [00:37<01:43,  1.63it/s]Epoch: 1, train for the 69-th batch, train loss: 0.6965430378913879:  29%|███▊         | 69/237 [00:37<01:42,  1.64it/s]Epoch: 1, train for the 63-th batch, train loss: 0.2728634178638458:  52%|██████▊      | 62/119 [00:39<00:34,  1.63it/s]Epoch: 1, train for the 63-th batch, train loss: 0.2728634178638458:  53%|██████▉      | 63/119 [00:39<00:34,  1.63it/s]Epoch: 1, train for the 64-th batch, train loss: 0.4820381700992584:  42%|█████▍       | 63/151 [00:39<00:54,  1.63it/s]Epoch: 1, train for the 64-th batch, train loss: 0.4820381700992584:  42%|█████▌       | 64/151 [00:39<00:52,  1.64it/s]Epoch: 1, train for the 60-th batch, train loss: 0.3945358097553253:  15%|██           | 59/383 [00:35<03:06,  1.74it/s]Epoch: 1, train for the 60-th batch, train loss: 0.3945358097553253:  16%|██           | 60/383 [00:35<03:08,  1.71it/s]Epoch: 1, train for the 60-th batch, train loss: 0.5875296592712402:  24%|███▏         | 59/241 [00:37<01:51,  1.63it/s]Epoch: 1, train for the 60-th batch, train loss: 0.5875296592712402:  25%|███▏         | 60/241 [00:37<01:50,  1.64it/s]Epoch: 1, train for the 62-th batch, train loss: 0.48636117577552795:  42%|█████       | 61/146 [00:39<00:52,  1.63it/s]Epoch: 1, train for the 62-th batch, train loss: 0.48636117577552795:  42%|█████       | 62/146 [00:39<00:51,  1.64it/s]Epoch: 1, train for the 70-th batch, train loss: 0.6987135410308838:  29%|███▊         | 69/237 [00:38<01:42,  1.64it/s]Epoch: 1, train for the 70-th batch, train loss: 0.6987135410308838:  30%|███▊         | 70/237 [00:38<01:41,  1.64it/s]Epoch: 1, train for the 64-th batch, train loss: 0.2587909996509552:  53%|██████▉      | 63/119 [00:40<00:34,  1.63it/s]Epoch: 1, train for the 64-th batch, train loss: 0.2587909996509552:  54%|██████▉      | 64/119 [00:40<00:33,  1.64it/s]Epoch: 1, train for the 61-th batch, train loss: 0.4386274516582489:  16%|██           | 60/383 [00:36<03:08,  1.71it/s]Epoch: 1, train for the 61-th batch, train loss: 0.4386274516582489:  16%|██           | 61/383 [00:36<03:09,  1.70it/s]Epoch: 1, train for the 65-th batch, train loss: 0.4820217788219452:  42%|█████▌       | 64/151 [00:39<00:52,  1.64it/s]Epoch: 1, train for the 65-th batch, train loss: 0.4820217788219452:  43%|█████▌       | 65/151 [00:39<00:52,  1.63it/s]Epoch: 1, train for the 61-th batch, train loss: 0.5698347091674805:  25%|███▏         | 60/241 [00:38<01:50,  1.64it/s]Epoch: 1, train for the 61-th batch, train loss: 0.5698347091674805:  25%|███▎         | 61/241 [00:38<01:50,  1.63it/s]Epoch: 1, train for the 63-th batch, train loss: 0.5103931427001953:  42%|█████▌       | 62/146 [00:40<00:51,  1.64it/s]Epoch: 1, train for the 63-th batch, train loss: 0.5103931427001953:  43%|█████▌       | 63/146 [00:40<00:50,  1.64it/s]Epoch: 1, train for the 71-th batch, train loss: 0.562256932258606:  30%|████▏         | 70/237 [00:38<01:41,  1.64it/s]Epoch: 1, train for the 71-th batch, train loss: 0.562256932258606:  30%|████▏         | 71/237 [00:38<01:39,  1.67it/s]Epoch: 1, train for the 65-th batch, train loss: 0.27461785078048706:  54%|██████▍     | 64/119 [00:40<00:33,  1.64it/s]Epoch: 1, train for the 65-th batch, train loss: 0.27461785078048706:  55%|██████▌     | 65/119 [00:40<00:33,  1.63it/s]Epoch: 1, train for the 62-th batch, train loss: 0.5474171042442322:  16%|██           | 61/383 [00:37<03:09,  1.70it/s]Epoch: 1, train for the 62-th batch, train loss: 0.5474171042442322:  16%|██           | 62/383 [00:37<03:08,  1.70it/s]Epoch: 1, train for the 66-th batch, train loss: 0.49983224272727966:  43%|█████▏      | 65/151 [00:40<00:52,  1.63it/s]Epoch: 1, train for the 66-th batch, train loss: 0.49983224272727966:  44%|█████▏      | 66/151 [00:40<00:51,  1.64it/s]Epoch: 1, train for the 62-th batch, train loss: 0.5609352588653564:  25%|███▎         | 61/241 [00:39<01:50,  1.63it/s]Epoch: 1, train for the 62-th batch, train loss: 0.5609352588653564:  26%|███▎         | 62/241 [00:39<01:49,  1.64it/s]Epoch: 1, train for the 64-th batch, train loss: 0.5616785287857056:  43%|█████▌       | 63/146 [00:40<00:50,  1.64it/s]Epoch: 1, train for the 64-th batch, train loss: 0.5616785287857056:  44%|█████▋       | 64/146 [00:40<00:46,  1.78it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5846981406211853:  30%|███▉         | 71/237 [00:39<01:39,  1.67it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5846981406211853:  30%|███▉         | 72/237 [00:39<01:38,  1.67it/s]Epoch: 1, train for the 63-th batch, train loss: 0.5294923186302185:  16%|██           | 62/383 [00:37<03:08,  1.70it/s]Epoch: 1, train for the 63-th batch, train loss: 0.5294923186302185:  16%|██▏          | 63/383 [00:37<03:09,  1.69it/s]Epoch: 1, train for the 66-th batch, train loss: 0.3638257384300232:  55%|███████      | 65/119 [00:41<00:33,  1.63it/s]Epoch: 1, train for the 66-th batch, train loss: 0.3638257384300232:  55%|███████▏     | 66/119 [00:41<00:33,  1.56it/s]Epoch: 1, train for the 67-th batch, train loss: 0.600007951259613:  44%|██████        | 66/151 [00:40<00:51,  1.64it/s]Epoch: 1, train for the 67-th batch, train loss: 0.600007951259613:  44%|██████▏       | 67/151 [00:40<00:51,  1.62it/s]Epoch: 1, train for the 65-th batch, train loss: 0.5795566439628601:  44%|█████▋       | 64/146 [00:41<00:46,  1.78it/s]Epoch: 1, train for the 65-th batch, train loss: 0.5795566439628601:  45%|█████▊       | 65/146 [00:41<00:45,  1.77it/s]Epoch: 1, train for the 63-th batch, train loss: 0.6992995142936707:  26%|███▎         | 62/241 [00:39<01:49,  1.64it/s]Epoch: 1, train for the 63-th batch, train loss: 0.6992995142936707:  26%|███▍         | 63/241 [00:39<01:49,  1.62it/s]Epoch: 1, train for the 73-th batch, train loss: 0.6860319972038269:  30%|███▉         | 72/237 [00:39<01:38,  1.67it/s]Epoch: 1, train for the 73-th batch, train loss: 0.6860319972038269:  31%|████         | 73/237 [00:39<01:39,  1.66it/s]Epoch: 1, train for the 64-th batch, train loss: 0.5575090050697327:  16%|██▏          | 63/383 [00:38<03:09,  1.69it/s]Epoch: 1, train for the 64-th batch, train loss: 0.5575090050697327:  17%|██▏          | 64/383 [00:38<03:10,  1.68it/s]Epoch: 1, train for the 67-th batch, train loss: 0.41376054286956787:  55%|██████▋     | 66/119 [00:41<00:33,  1.56it/s]Epoch: 1, train for the 67-th batch, train loss: 0.41376054286956787:  56%|██████▊     | 67/119 [00:41<00:32,  1.58it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5556081533432007:  44%|█████▊       | 67/151 [00:41<00:51,  1.62it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5556081533432007:  45%|█████▊       | 68/151 [00:41<00:51,  1.62it/s]Epoch: 1, train for the 66-th batch, train loss: 0.5495853424072266:  45%|█████▊       | 65/146 [00:41<00:45,  1.77it/s]Epoch: 1, train for the 66-th batch, train loss: 0.5495853424072266:  45%|█████▉       | 66/146 [00:41<00:46,  1.73it/s]Epoch: 1, train for the 64-th batch, train loss: 0.3056628108024597:  26%|███▍         | 63/241 [00:40<01:49,  1.62it/s]Epoch: 1, train for the 64-th batch, train loss: 0.3056628108024597:  27%|███▍         | 64/241 [00:40<01:49,  1.62it/s]Epoch: 1, train for the 74-th batch, train loss: 0.6966521143913269:  31%|████         | 73/237 [00:40<01:39,  1.66it/s]Epoch: 1, train for the 74-th batch, train loss: 0.6966521143913269:  31%|████         | 74/237 [00:40<01:38,  1.66it/s]Epoch: 1, train for the 65-th batch, train loss: 0.4484562277793884:  17%|██▏          | 64/383 [00:38<03:10,  1.68it/s]Epoch: 1, train for the 65-th batch, train loss: 0.4484562277793884:  17%|██▏          | 65/383 [00:38<03:09,  1.67it/s]Epoch: 1, train for the 68-th batch, train loss: 0.28515365719795227:  56%|██████▊     | 67/119 [00:42<00:32,  1.58it/s]Epoch: 1, train for the 68-th batch, train loss: 0.28515365719795227:  57%|██████▊     | 68/119 [00:42<00:31,  1.60it/s]Epoch: 1, train for the 69-th batch, train loss: 0.5162144899368286:  45%|█████▊       | 68/151 [00:42<00:51,  1.62it/s]Epoch: 1, train for the 69-th batch, train loss: 0.5162144899368286:  46%|█████▉       | 69/151 [00:42<00:50,  1.63it/s]Epoch: 1, train for the 67-th batch, train loss: 0.5620508790016174:  45%|█████▉       | 66/146 [00:42<00:46,  1.73it/s]Epoch: 1, train for the 67-th batch, train loss: 0.5620508790016174:  46%|█████▉       | 67/146 [00:42<00:46,  1.71it/s]Epoch: 1, train for the 65-th batch, train loss: 0.26709699630737305:  27%|███▏        | 64/241 [00:40<01:49,  1.62it/s]Epoch: 1, train for the 65-th batch, train loss: 0.26709699630737305:  27%|███▏        | 65/241 [00:40<01:48,  1.63it/s]Epoch: 1, train for the 75-th batch, train loss: 0.7152679562568665:  31%|████         | 74/237 [00:41<01:38,  1.66it/s]Epoch: 1, train for the 75-th batch, train loss: 0.7152679562568665:  32%|████         | 75/237 [00:41<01:37,  1.65it/s]Epoch: 1, train for the 66-th batch, train loss: 0.4944336712360382:  17%|██▏          | 65/383 [00:39<03:09,  1.67it/s]Epoch: 1, train for the 66-th batch, train loss: 0.4944336712360382:  17%|██▏          | 66/383 [00:39<03:10,  1.66it/s]Epoch: 1, train for the 69-th batch, train loss: 0.3045632541179657:  57%|███████▍     | 68/119 [00:43<00:31,  1.60it/s]Epoch: 1, train for the 69-th batch, train loss: 0.3045632541179657:  58%|███████▌     | 69/119 [00:43<00:30,  1.62it/s]Epoch: 1, train for the 70-th batch, train loss: 0.5673156380653381:  46%|█████▉       | 69/151 [00:42<00:50,  1.63it/s]Epoch: 1, train for the 70-th batch, train loss: 0.5673156380653381:  46%|██████       | 70/151 [00:42<00:49,  1.62it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5695492625236511:  46%|█████▉       | 67/146 [00:42<00:46,  1.71it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5695492625236511:  47%|██████       | 68/146 [00:42<00:46,  1.69it/s]Epoch: 1, train for the 66-th batch, train loss: 0.22919495403766632:  27%|███▏        | 65/241 [00:41<01:48,  1.63it/s]Epoch: 1, train for the 66-th batch, train loss: 0.22919495403766632:  27%|███▎        | 66/241 [00:41<01:47,  1.63it/s]Epoch: 1, train for the 76-th batch, train loss: 0.68194180727005:  32%|████▋          | 75/237 [00:41<01:37,  1.65it/s]Epoch: 1, train for the 76-th batch, train loss: 0.68194180727005:  32%|████▊          | 76/237 [00:41<01:37,  1.66it/s]Epoch: 1, train for the 67-th batch, train loss: 0.545597493648529:  17%|██▍           | 66/383 [00:40<03:10,  1.66it/s]Epoch: 1, train for the 67-th batch, train loss: 0.545597493648529:  17%|██▍           | 67/383 [00:40<03:08,  1.68it/s]Epoch: 1, train for the 70-th batch, train loss: 0.24391508102416992:  58%|██████▉     | 69/119 [00:43<00:30,  1.62it/s]Epoch: 1, train for the 70-th batch, train loss: 0.24391508102416992:  59%|███████     | 70/119 [00:43<00:30,  1.63it/s]Epoch: 1, train for the 71-th batch, train loss: 0.5340703725814819:  46%|██████       | 70/151 [00:43<00:49,  1.62it/s]Epoch: 1, train for the 71-th batch, train loss: 0.5340703725814819:  47%|██████       | 71/151 [00:43<00:49,  1.63it/s]Epoch: 1, train for the 69-th batch, train loss: 0.5447278022766113:  47%|██████       | 68/146 [00:43<00:46,  1.69it/s]Epoch: 1, train for the 69-th batch, train loss: 0.5447278022766113:  47%|██████▏      | 69/146 [00:43<00:45,  1.68it/s]Epoch: 1, train for the 67-th batch, train loss: 0.4103282392024994:  27%|███▌         | 66/241 [00:42<01:47,  1.63it/s]Epoch: 1, train for the 67-th batch, train loss: 0.4103282392024994:  28%|███▌         | 67/241 [00:42<01:46,  1.63it/s]Epoch: 1, train for the 77-th batch, train loss: 0.5853670239448547:  32%|████▏        | 76/237 [00:42<01:37,  1.66it/s]Epoch: 1, train for the 77-th batch, train loss: 0.5853670239448547:  32%|████▏        | 77/237 [00:42<01:34,  1.69it/s]Epoch: 1, train for the 68-th batch, train loss: 0.48609456419944763:  17%|██          | 67/383 [00:40<03:08,  1.68it/s]Epoch: 1, train for the 68-th batch, train loss: 0.48609456419944763:  18%|██▏         | 68/383 [00:40<03:06,  1.69it/s]Epoch: 1, train for the 71-th batch, train loss: 0.2915186583995819:  59%|███████▋     | 70/119 [00:44<00:30,  1.63it/s]Epoch: 1, train for the 71-th batch, train loss: 0.2915186583995819:  60%|███████▊     | 71/119 [00:44<00:29,  1.64it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5490437746047974:  47%|██████       | 71/151 [00:44<00:49,  1.63it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5490437746047974:  48%|██████▏      | 72/151 [00:44<00:48,  1.63it/s]Epoch: 1, train for the 70-th batch, train loss: 0.6134421229362488:  47%|██████▏      | 69/146 [00:44<00:45,  1.68it/s]Epoch: 1, train for the 70-th batch, train loss: 0.6134421229362488:  48%|██████▏      | 70/146 [00:44<00:45,  1.67it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5389483571052551:  28%|███▌         | 67/241 [00:42<01:46,  1.63it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5389483571052551:  28%|███▋         | 68/241 [00:42<01:46,  1.63it/s]Epoch: 1, train for the 78-th batch, train loss: 0.6510821580886841:  32%|████▏        | 77/237 [00:42<01:34,  1.69it/s]Epoch: 1, train for the 78-th batch, train loss: 0.6510821580886841:  33%|████▎        | 78/237 [00:42<01:34,  1.68it/s]Epoch: 1, train for the 69-th batch, train loss: 0.3870773911476135:  18%|██▎          | 68/383 [00:41<03:06,  1.69it/s]Epoch: 1, train for the 69-th batch, train loss: 0.3870773911476135:  18%|██▎          | 69/383 [00:41<03:06,  1.68it/s]Epoch: 1, train for the 72-th batch, train loss: 0.3156282305717468:  60%|███████▊     | 71/119 [00:44<00:29,  1.64it/s]Epoch: 1, train for the 72-th batch, train loss: 0.3156282305717468:  61%|███████▊     | 72/119 [00:44<00:28,  1.63it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5611563920974731:  48%|██████▏      | 72/151 [00:44<00:48,  1.63it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5611563920974731:  48%|██████▎      | 73/151 [00:44<00:47,  1.63it/s]Epoch: 1, train for the 71-th batch, train loss: 0.5078650712966919:  48%|██████▏      | 70/146 [00:44<00:45,  1.67it/s]Epoch: 1, train for the 71-th batch, train loss: 0.5078650712966919:  49%|██████▎      | 71/146 [00:44<00:45,  1.66it/s]Epoch: 1, train for the 69-th batch, train loss: 0.656063973903656:  28%|███▉          | 68/241 [00:43<01:46,  1.63it/s]Epoch: 1, train for the 69-th batch, train loss: 0.656063973903656:  29%|████          | 69/241 [00:43<01:45,  1.63it/s]Epoch: 1, train for the 79-th batch, train loss: 0.6681938171386719:  33%|████▎        | 78/237 [00:43<01:34,  1.68it/s]Epoch: 1, train for the 79-th batch, train loss: 0.6681938171386719:  33%|████▎        | 79/237 [00:43<01:34,  1.67it/s]Epoch: 1, train for the 70-th batch, train loss: 0.4824679493904114:  18%|██▎          | 69/383 [00:41<03:06,  1.68it/s]Epoch: 1, train for the 70-th batch, train loss: 0.4824679493904114:  18%|██▍          | 70/383 [00:41<03:07,  1.67it/s]Epoch: 1, train for the 73-th batch, train loss: 0.24848267436027527:  61%|███████▎    | 72/119 [00:45<00:28,  1.63it/s]Epoch: 1, train for the 73-th batch, train loss: 0.24848267436027527:  61%|███████▎    | 73/119 [00:45<00:28,  1.64it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5122435688972473:  48%|██████▎      | 73/151 [00:45<00:47,  1.63it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5122435688972473:  49%|██████▎      | 74/151 [00:45<00:47,  1.63it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5511427521705627:  49%|██████▎      | 71/146 [00:45<00:45,  1.66it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5511427521705627:  49%|██████▍      | 72/146 [00:45<00:44,  1.65it/s]Epoch: 1, train for the 70-th batch, train loss: 0.6370307207107544:  29%|███▋         | 69/241 [00:43<01:45,  1.63it/s]Epoch: 1, train for the 70-th batch, train loss: 0.6370307207107544:  29%|███▊         | 70/241 [00:43<01:44,  1.63it/s]Epoch: 1, train for the 80-th batch, train loss: 0.6546323895454407:  33%|████▎        | 79/237 [00:44<01:34,  1.67it/s]Epoch: 1, train for the 80-th batch, train loss: 0.6546323895454407:  34%|████▍        | 80/237 [00:44<01:34,  1.67it/s]Epoch: 1, train for the 71-th batch, train loss: 0.4961622655391693:  18%|██▍          | 70/383 [00:42<03:07,  1.67it/s]Epoch: 1, train for the 71-th batch, train loss: 0.4961622655391693:  19%|██▍          | 71/383 [00:42<03:07,  1.67it/s]Epoch: 1, train for the 74-th batch, train loss: 0.2912473976612091:  61%|███████▉     | 73/119 [00:46<00:28,  1.64it/s]Epoch: 1, train for the 74-th batch, train loss: 0.2912473976612091:  62%|████████     | 74/119 [00:46<00:27,  1.64it/s]Epoch: 1, train for the 75-th batch, train loss: 0.520847499370575:  49%|██████▊       | 74/151 [00:45<00:47,  1.63it/s]Epoch: 1, train for the 75-th batch, train loss: 0.520847499370575:  50%|██████▉       | 75/151 [00:45<00:46,  1.63it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5081154704093933:  49%|██████▍      | 72/146 [00:45<00:44,  1.65it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5081154704093933:  50%|██████▌      | 73/146 [00:45<00:44,  1.65it/s]Epoch: 1, train for the 71-th batch, train loss: 0.6656402945518494:  29%|███▊         | 70/241 [00:44<01:44,  1.63it/s]Epoch: 1, train for the 71-th batch, train loss: 0.6656402945518494:  29%|███▊         | 71/241 [00:44<01:44,  1.63it/s]Epoch: 1, train for the 81-th batch, train loss: 0.6353634595870972:  34%|████▍        | 80/237 [00:44<01:34,  1.67it/s]Epoch: 1, train for the 81-th batch, train loss: 0.6353634595870972:  34%|████▍        | 81/237 [00:44<01:32,  1.68it/s]Epoch: 1, train for the 72-th batch, train loss: 0.48824477195739746:  19%|██▏         | 71/383 [00:43<03:07,  1.67it/s]Epoch: 1, train for the 72-th batch, train loss: 0.48824477195739746:  19%|██▎         | 72/383 [00:43<03:05,  1.68it/s]Epoch: 1, train for the 75-th batch, train loss: 0.29881736636161804:  62%|███████▍    | 74/119 [00:46<00:27,  1.64it/s]Epoch: 1, train for the 75-th batch, train loss: 0.29881736636161804:  63%|███████▌    | 75/119 [00:46<00:26,  1.64it/s]Epoch: 1, train for the 76-th batch, train loss: 0.5776609182357788:  50%|██████▍      | 75/151 [00:46<00:46,  1.63it/s]Epoch: 1, train for the 76-th batch, train loss: 0.5776609182357788:  50%|██████▌      | 76/151 [00:46<00:45,  1.63it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5527483224868774:  50%|██████▌      | 73/146 [00:46<00:44,  1.65it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5527483224868774:  51%|██████▌      | 74/146 [00:46<00:43,  1.64it/s]Epoch: 1, train for the 72-th batch, train loss: 0.6050755977630615:  29%|███▊         | 71/241 [00:45<01:44,  1.63it/s]Epoch: 1, train for the 72-th batch, train loss: 0.6050755977630615:  30%|███▉         | 72/241 [00:45<01:43,  1.63it/s]Epoch: 1, train for the 82-th batch, train loss: 0.6655601263046265:  34%|████▍        | 81/237 [00:45<01:32,  1.68it/s]Epoch: 1, train for the 82-th batch, train loss: 0.6655601263046265:  35%|████▍        | 82/237 [00:45<01:33,  1.66it/s]Epoch: 1, train for the 73-th batch, train loss: 0.41511961817741394:  19%|██▎         | 72/383 [00:43<03:05,  1.68it/s]Epoch: 1, train for the 73-th batch, train loss: 0.41511961817741394:  19%|██▎         | 73/383 [00:43<03:06,  1.66it/s]Epoch: 1, train for the 76-th batch, train loss: 0.29644444584846497:  63%|███████▌    | 75/119 [00:47<00:26,  1.64it/s]Epoch: 1, train for the 76-th batch, train loss: 0.29644444584846497:  64%|███████▋    | 76/119 [00:47<00:26,  1.64it/s]Epoch: 1, train for the 77-th batch, train loss: 0.5365062355995178:  50%|██████▌      | 76/151 [00:47<00:45,  1.63it/s]Epoch: 1, train for the 77-th batch, train loss: 0.5365062355995178:  51%|██████▋      | 77/151 [00:47<00:45,  1.63it/s]Epoch: 1, train for the 75-th batch, train loss: 0.6063512563705444:  51%|██████▌      | 74/146 [00:47<00:43,  1.64it/s]Epoch: 1, train for the 75-th batch, train loss: 0.6063512563705444:  51%|██████▋      | 75/146 [00:47<00:43,  1.64it/s]Epoch: 1, train for the 73-th batch, train loss: 0.559984564781189:  30%|████▏         | 72/241 [00:45<01:43,  1.63it/s]Epoch: 1, train for the 73-th batch, train loss: 0.559984564781189:  30%|████▏         | 73/241 [00:45<01:43,  1.63it/s]Epoch: 1, train for the 83-th batch, train loss: 0.6210310459136963:  35%|████▍        | 82/237 [00:45<01:33,  1.66it/s]Epoch: 1, train for the 83-th batch, train loss: 0.6210310459136963:  35%|████▌        | 83/237 [00:45<01:31,  1.68it/s]Epoch: 1, train for the 74-th batch, train loss: 0.3757459223270416:  19%|██▍          | 73/383 [00:44<03:06,  1.66it/s]Epoch: 1, train for the 74-th batch, train loss: 0.3757459223270416:  19%|██▌          | 74/383 [00:44<03:04,  1.67it/s]Epoch: 1, train for the 77-th batch, train loss: 0.2821987569332123:  64%|████████▎    | 76/119 [00:48<00:26,  1.64it/s]Epoch: 1, train for the 77-th batch, train loss: 0.2821987569332123:  65%|████████▍    | 77/119 [00:48<00:25,  1.63it/s]Epoch: 1, train for the 78-th batch, train loss: 0.551983118057251:  51%|███████▏      | 77/151 [00:47<00:45,  1.63it/s]Epoch: 1, train for the 78-th batch, train loss: 0.551983118057251:  52%|███████▏      | 78/151 [00:47<00:44,  1.63it/s]Epoch: 1, train for the 76-th batch, train loss: 0.5382247567176819:  51%|██████▋      | 75/146 [00:47<00:43,  1.64it/s]Epoch: 1, train for the 76-th batch, train loss: 0.5382247567176819:  52%|██████▊      | 76/146 [00:47<00:42,  1.64it/s]Epoch: 1, train for the 74-th batch, train loss: 0.528678834438324:  30%|████▏         | 73/241 [00:46<01:43,  1.63it/s]Epoch: 1, train for the 74-th batch, train loss: 0.528678834438324:  31%|████▎         | 74/241 [00:46<01:42,  1.63it/s]Epoch: 1, train for the 84-th batch, train loss: 0.6598889827728271:  35%|████▌        | 83/237 [00:46<01:31,  1.68it/s]Epoch: 1, train for the 84-th batch, train loss: 0.6598889827728271:  35%|████▌        | 84/237 [00:46<01:32,  1.66it/s]Epoch: 1, train for the 75-th batch, train loss: 0.3722555935382843:  19%|██▌          | 74/383 [00:44<03:04,  1.67it/s]Epoch: 1, train for the 75-th batch, train loss: 0.3722555935382843:  20%|██▌          | 75/383 [00:44<03:05,  1.66it/s]Epoch: 1, train for the 78-th batch, train loss: 0.2590128779411316:  65%|████████▍    | 77/119 [00:48<00:25,  1.63it/s]Epoch: 1, train for the 78-th batch, train loss: 0.2590128779411316:  66%|████████▌    | 78/119 [00:48<00:25,  1.64it/s]Epoch: 1, train for the 75-th batch, train loss: 0.437710165977478:  31%|████▎         | 74/241 [00:46<01:42,  1.63it/s]Epoch: 1, train for the 75-th batch, train loss: 0.437710165977478:  31%|████▎         | 75/241 [00:46<01:34,  1.76it/s]Epoch: 1, train for the 77-th batch, train loss: 0.5829696655273438:  52%|██████▊      | 76/146 [00:48<00:42,  1.64it/s]Epoch: 1, train for the 77-th batch, train loss: 0.5829696655273438:  53%|██████▊      | 77/146 [00:48<00:42,  1.64it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5717909336090088:  52%|██████▋      | 78/151 [00:48<00:44,  1.63it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5717909336090088:  52%|██████▊      | 79/151 [00:48<00:46,  1.54it/s]Epoch: 1, train for the 85-th batch, train loss: 0.6543498635292053:  35%|████▌        | 84/237 [00:47<01:32,  1.66it/s]Epoch: 1, train for the 85-th batch, train loss: 0.6543498635292053:  36%|████▋        | 85/237 [00:47<01:31,  1.65it/s]Epoch: 1, train for the 76-th batch, train loss: 0.47213515639305115:  20%|██▎         | 75/383 [00:45<03:05,  1.66it/s]Epoch: 1, train for the 76-th batch, train loss: 0.47213515639305115:  20%|██▍         | 76/383 [00:45<03:04,  1.66it/s]Epoch: 1, train for the 79-th batch, train loss: 0.2760102152824402:  66%|████████▌    | 78/119 [00:49<00:25,  1.64it/s]Epoch: 1, train for the 79-th batch, train loss: 0.2760102152824402:  66%|████████▋    | 79/119 [00:49<00:24,  1.63it/s]Epoch: 1, train for the 76-th batch, train loss: 0.17870445549488068:  31%|███▋        | 75/241 [00:47<01:34,  1.76it/s]Epoch: 1, train for the 76-th batch, train loss: 0.17870445549488068:  32%|███▊        | 76/241 [00:47<01:34,  1.75it/s]Epoch: 1, train for the 78-th batch, train loss: 0.567813515663147:  53%|███████▍      | 77/146 [00:49<00:42,  1.64it/s]Epoch: 1, train for the 78-th batch, train loss: 0.567813515663147:  53%|███████▍      | 78/146 [00:49<00:41,  1.63it/s]Epoch: 1, train for the 80-th batch, train loss: 0.54642254114151:  52%|███████▊       | 79/151 [00:49<00:46,  1.54it/s]Epoch: 1, train for the 80-th batch, train loss: 0.54642254114151:  53%|███████▉       | 80/151 [00:49<00:45,  1.57it/s]Epoch: 1, train for the 86-th batch, train loss: 0.6629704236984253:  36%|████▋        | 85/237 [00:47<01:31,  1.65it/s]Epoch: 1, train for the 86-th batch, train loss: 0.6629704236984253:  36%|████▋        | 86/237 [00:47<01:30,  1.66it/s]Epoch: 1, train for the 77-th batch, train loss: 0.48074400424957275:  20%|██▍         | 76/383 [00:46<03:04,  1.66it/s]Epoch: 1, train for the 77-th batch, train loss: 0.48074400424957275:  20%|██▍         | 77/383 [00:46<03:03,  1.67it/s]Epoch: 1, train for the 80-th batch, train loss: 0.33063259720802307:  66%|███████▉    | 79/119 [00:49<00:24,  1.63it/s]Epoch: 1, train for the 80-th batch, train loss: 0.33063259720802307:  67%|████████    | 80/119 [00:49<00:23,  1.63it/s]Epoch: 1, train for the 77-th batch, train loss: 0.4495038390159607:  32%|████         | 76/241 [00:48<01:34,  1.75it/s]Epoch: 1, train for the 77-th batch, train loss: 0.4495038390159607:  32%|████▏        | 77/241 [00:48<01:34,  1.73it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5396396517753601:  53%|██████▉      | 78/146 [00:49<00:41,  1.63it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5396396517753601:  54%|███████      | 79/146 [00:49<00:41,  1.63it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5536023378372192:  53%|██████▉      | 80/151 [00:49<00:45,  1.57it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5536023378372192:  54%|██████▉      | 81/151 [00:49<00:43,  1.60it/s]Epoch: 1, train for the 87-th batch, train loss: 0.6202935576438904:  36%|████▋        | 86/237 [00:48<01:30,  1.66it/s]Epoch: 1, train for the 87-th batch, train loss: 0.6202935576438904:  37%|████▊        | 87/237 [00:48<01:29,  1.67it/s]Epoch: 1, train for the 78-th batch, train loss: 0.4109753966331482:  20%|██▌          | 77/383 [00:46<03:03,  1.67it/s]Epoch: 1, train for the 78-th batch, train loss: 0.4109753966331482:  20%|██▋          | 78/383 [00:46<03:02,  1.67it/s]Epoch: 1, train for the 81-th batch, train loss: 0.25723788142204285:  67%|████████    | 80/119 [00:50<00:23,  1.63it/s]Epoch: 1, train for the 81-th batch, train loss: 0.25723788142204285:  68%|████████▏   | 81/119 [00:50<00:23,  1.63it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5856248140335083:  32%|████▏        | 77/241 [00:48<01:34,  1.73it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5856248140335083:  32%|████▏        | 78/241 [00:48<01:35,  1.70it/s]Epoch: 1, train for the 80-th batch, train loss: 0.5697660446166992:  54%|███████      | 79/146 [00:50<00:41,  1.63it/s]Epoch: 1, train for the 80-th batch, train loss: 0.5697660446166992:  55%|███████      | 80/146 [00:50<00:40,  1.63it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5231526494026184:  54%|██████▉      | 81/151 [00:50<00:43,  1.60it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5231526494026184:  54%|███████      | 82/151 [00:50<00:42,  1.61it/s]Epoch: 1, train for the 88-th batch, train loss: 0.6696545481681824:  37%|████▊        | 87/237 [00:48<01:29,  1.67it/s]Epoch: 1, train for the 88-th batch, train loss: 0.6696545481681824:  37%|████▊        | 88/237 [00:48<01:29,  1.66it/s]Epoch: 1, train for the 79-th batch, train loss: 0.42845821380615234:  20%|██▍         | 78/383 [00:47<03:02,  1.67it/s]Epoch: 1, train for the 79-th batch, train loss: 0.42845821380615234:  21%|██▍         | 79/383 [00:47<03:03,  1.66it/s]Epoch: 1, train for the 82-th batch, train loss: 0.3003880977630615:  68%|████████▊    | 81/119 [00:51<00:23,  1.63it/s]Epoch: 1, train for the 82-th batch, train loss: 0.3003880977630615:  69%|████████▉    | 82/119 [00:51<00:22,  1.64it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5086719393730164:  32%|████▏        | 78/241 [00:49<01:35,  1.70it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5086719393730164:  33%|████▎        | 79/241 [00:49<01:35,  1.69it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5497035384178162:  55%|███████      | 80/146 [00:50<00:40,  1.63it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5497035384178162:  55%|███████▏     | 81/146 [00:50<00:39,  1.64it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5637294054031372:  54%|███████      | 82/151 [00:50<00:42,  1.61it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5637294054031372:  55%|███████▏     | 83/151 [00:50<00:41,  1.62it/s]Epoch: 1, train for the 89-th batch, train loss: 0.6772894263267517:  37%|████▊        | 88/237 [00:49<01:29,  1.66it/s]Epoch: 1, train for the 89-th batch, train loss: 0.6772894263267517:  38%|████▉        | 89/237 [00:49<01:29,  1.66it/s]Epoch: 1, train for the 80-th batch, train loss: 0.38059234619140625:  21%|██▍         | 79/383 [00:47<03:03,  1.66it/s]Epoch: 1, train for the 80-th batch, train loss: 0.38059234619140625:  21%|██▌         | 80/383 [00:47<03:02,  1.66it/s]Epoch: 1, train for the 83-th batch, train loss: 0.3180188238620758:  69%|████████▉    | 82/119 [00:51<00:22,  1.64it/s]Epoch: 1, train for the 83-th batch, train loss: 0.3180188238620758:  70%|█████████    | 83/119 [00:51<00:21,  1.64it/s]Epoch: 1, train for the 80-th batch, train loss: 0.3555556833744049:  33%|████▎        | 79/241 [00:49<01:35,  1.69it/s]Epoch: 1, train for the 80-th batch, train loss: 0.3555556833744049:  33%|████▎        | 80/241 [00:49<01:36,  1.67it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5524091720581055:  55%|███████▏     | 81/146 [00:51<00:39,  1.64it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5524091720581055:  56%|███████▎     | 82/146 [00:51<00:39,  1.64it/s]Epoch: 1, train for the 84-th batch, train loss: 0.6085789203643799:  55%|███████▏     | 83/151 [00:51<00:41,  1.62it/s]Epoch: 1, train for the 84-th batch, train loss: 0.6085789203643799:  56%|███████▏     | 84/151 [00:51<00:41,  1.63it/s]Epoch: 1, train for the 90-th batch, train loss: 0.6767410039901733:  38%|████▉        | 89/237 [00:50<01:29,  1.66it/s]Epoch: 1, train for the 90-th batch, train loss: 0.6767410039901733:  38%|████▉        | 90/237 [00:50<01:28,  1.66it/s]Epoch: 1, train for the 81-th batch, train loss: 0.3322731852531433:  21%|██▋          | 80/383 [00:48<03:02,  1.66it/s]Epoch: 1, train for the 81-th batch, train loss: 0.3322731852531433:  21%|██▋          | 81/383 [00:48<03:02,  1.65it/s]Epoch: 1, train for the 84-th batch, train loss: 0.21372093260288239:  70%|████████▎   | 83/119 [00:52<00:21,  1.64it/s]Epoch: 1, train for the 84-th batch, train loss: 0.21372093260288239:  71%|████████▍   | 84/119 [00:52<00:21,  1.64it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5910130739212036:  56%|███████▏     | 84/151 [00:51<00:41,  1.63it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5910130739212036:  56%|███████▎     | 85/151 [00:51<00:37,  1.74it/s]Epoch: 1, train for the 81-th batch, train loss: 0.4233483672142029:  33%|████▎        | 80/241 [00:50<01:36,  1.67it/s]Epoch: 1, train for the 81-th batch, train loss: 0.4233483672142029:  34%|████▎        | 81/241 [00:50<01:41,  1.57it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5554673075675964:  56%|███████▎     | 82/146 [00:52<00:39,  1.64it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5554673075675964:  57%|███████▍     | 83/146 [00:52<00:38,  1.64it/s]Epoch: 1, train for the 91-th batch, train loss: 0.6202551126480103:  38%|████▉        | 90/237 [00:50<01:28,  1.66it/s]Epoch: 1, train for the 91-th batch, train loss: 0.6202551126480103:  38%|████▉        | 91/237 [00:50<01:27,  1.67it/s]Epoch: 1, train for the 82-th batch, train loss: 0.43763262033462524:  21%|██▌         | 81/383 [00:49<03:02,  1.65it/s]Epoch: 1, train for the 82-th batch, train loss: 0.43763262033462524:  21%|██▌         | 82/383 [00:49<03:00,  1.66it/s]Epoch: 1, train for the 85-th batch, train loss: 0.25089380145072937:  71%|████████▍   | 84/119 [00:52<00:21,  1.64it/s]Epoch: 1, train for the 85-th batch, train loss: 0.25089380145072937:  71%|████████▌   | 85/119 [00:52<00:20,  1.64it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5609741806983948:  56%|███████▎     | 85/151 [00:52<00:37,  1.74it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5609741806983948:  57%|███████▍     | 86/151 [00:52<00:37,  1.73it/s]Epoch: 1, train for the 82-th batch, train loss: 0.4060206115245819:  34%|████▎        | 81/241 [00:51<01:41,  1.57it/s]Epoch: 1, train for the 82-th batch, train loss: 0.4060206115245819:  34%|████▍        | 82/241 [00:51<01:39,  1.59it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5074980854988098:  57%|███████▍     | 83/146 [00:52<00:38,  1.64it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5074980854988098:  58%|███████▍     | 84/146 [00:52<00:37,  1.64it/s]Epoch: 1, train for the 92-th batch, train loss: 0.6736447215080261:  38%|████▉        | 91/237 [00:51<01:27,  1.67it/s]Epoch: 1, train for the 92-th batch, train loss: 0.6736447215080261:  39%|█████        | 92/237 [00:51<01:27,  1.65it/s]Epoch: 1, train for the 83-th batch, train loss: 0.4792076051235199:  21%|██▊          | 82/383 [00:49<03:00,  1.66it/s]Epoch: 1, train for the 83-th batch, train loss: 0.4792076051235199:  22%|██▊          | 83/383 [00:49<03:01,  1.65it/s]Epoch: 1, train for the 87-th batch, train loss: 0.5326992273330688:  57%|███████▍     | 86/151 [00:53<00:37,  1.73it/s]Epoch: 1, train for the 87-th batch, train loss: 0.5326992273330688:  58%|███████▍     | 87/151 [00:53<00:36,  1.73it/s]Epoch: 1, train for the 86-th batch, train loss: 0.27564525604248047:  71%|████████▌   | 85/119 [00:53<00:20,  1.64it/s]Epoch: 1, train for the 86-th batch, train loss: 0.27564525604248047:  72%|████████▋   | 86/119 [00:53<00:20,  1.64it/s]Epoch: 1, train for the 83-th batch, train loss: 0.6476104855537415:  34%|████▍        | 82/241 [00:51<01:39,  1.59it/s]Epoch: 1, train for the 83-th batch, train loss: 0.6476104855537415:  34%|████▍        | 83/241 [00:51<01:38,  1.61it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5380153059959412:  58%|███████▍     | 84/146 [00:53<00:37,  1.64it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5380153059959412:  58%|███████▌     | 85/146 [00:53<00:37,  1.64it/s]Epoch: 1, train for the 93-th batch, train loss: 0.6555181741714478:  39%|█████        | 92/237 [00:51<01:27,  1.65it/s]Epoch: 1, train for the 93-th batch, train loss: 0.6555181741714478:  39%|█████        | 93/237 [00:51<01:27,  1.65it/s]Epoch: 1, train for the 84-th batch, train loss: 0.46896010637283325:  22%|██▌         | 83/383 [00:50<03:01,  1.65it/s]Epoch: 1, train for the 84-th batch, train loss: 0.46896010637283325:  22%|██▋         | 84/383 [00:50<03:01,  1.65it/s]Epoch: 1, train for the 88-th batch, train loss: 0.5852396488189697:  58%|███████▍     | 87/151 [00:53<00:36,  1.73it/s]Epoch: 1, train for the 88-th batch, train loss: 0.5852396488189697:  58%|███████▌     | 88/151 [00:53<00:36,  1.72it/s]Epoch: 1, train for the 87-th batch, train loss: 0.2715040445327759:  72%|█████████▍   | 86/119 [00:54<00:20,  1.64it/s]Epoch: 1, train for the 87-th batch, train loss: 0.2715040445327759:  73%|█████████▌   | 87/119 [00:54<00:19,  1.64it/s]Epoch: 1, train for the 84-th batch, train loss: 0.6367747187614441:  34%|████▍        | 83/241 [00:52<01:38,  1.61it/s]Epoch: 1, train for the 84-th batch, train loss: 0.6367747187614441:  35%|████▌        | 84/241 [00:52<01:36,  1.63it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5547935366630554:  58%|███████▌     | 85/146 [00:53<00:37,  1.64it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5547935366630554:  59%|███████▋     | 86/146 [00:53<00:36,  1.64it/s]Epoch: 1, train for the 94-th batch, train loss: 0.631293535232544:  39%|█████▍        | 93/237 [00:52<01:27,  1.65it/s]Epoch: 1, train for the 94-th batch, train loss: 0.631293535232544:  40%|█████▌        | 94/237 [00:52<01:26,  1.65it/s]Epoch: 1, train for the 85-th batch, train loss: 0.49269017577171326:  22%|██▋         | 84/383 [00:50<03:01,  1.65it/s]Epoch: 1, train for the 85-th batch, train loss: 0.49269017577171326:  22%|██▋         | 85/383 [00:50<03:01,  1.65it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5544889569282532:  58%|███████▌     | 88/151 [00:54<00:36,  1.72it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5544889569282532:  59%|███████▋     | 89/151 [00:54<00:36,  1.69it/s]Epoch: 1, train for the 88-th batch, train loss: 0.31335413455963135:  73%|████████▊   | 87/119 [00:54<00:19,  1.64it/s]Epoch: 1, train for the 88-th batch, train loss: 0.31335413455963135:  74%|████████▊   | 88/119 [00:54<00:18,  1.64it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5612983107566833:  35%|████▌        | 84/241 [00:53<01:36,  1.63it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5612983107566833:  35%|████▌        | 85/241 [00:53<01:35,  1.63it/s]Epoch: 1, train for the 87-th batch, train loss: 0.5553640127182007:  59%|███████▋     | 86/146 [00:54<00:36,  1.64it/s]Epoch: 1, train for the 87-th batch, train loss: 0.5553640127182007:  60%|███████▋     | 87/146 [00:54<00:35,  1.64it/s]Epoch: 1, train for the 95-th batch, train loss: 0.6551216244697571:  40%|█████▏       | 94/237 [00:53<01:26,  1.65it/s]Epoch: 1, train for the 95-th batch, train loss: 0.6551216244697571:  40%|█████▏       | 95/237 [00:53<01:26,  1.64it/s]Epoch: 1, train for the 86-th batch, train loss: 0.36073577404022217:  22%|██▋         | 85/383 [00:51<03:01,  1.65it/s]Epoch: 1, train for the 86-th batch, train loss: 0.36073577404022217:  22%|██▋         | 86/383 [00:51<03:01,  1.64it/s]Epoch: 1, train for the 90-th batch, train loss: 0.561223030090332:  59%|████████▎     | 89/151 [00:54<00:36,  1.69it/s]Epoch: 1, train for the 90-th batch, train loss: 0.561223030090332:  60%|████████▎     | 90/151 [00:54<00:36,  1.67it/s]Epoch: 1, train for the 89-th batch, train loss: 0.2825208902359009:  74%|█████████▌   | 88/119 [00:55<00:18,  1.64it/s]Epoch: 1, train for the 89-th batch, train loss: 0.2825208902359009:  75%|█████████▋   | 89/119 [00:55<00:18,  1.64it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5065774321556091:  35%|████▌        | 85/241 [00:53<01:35,  1.63it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5065774321556091:  36%|████▋        | 86/241 [00:53<01:35,  1.63it/s]Epoch: 1, train for the 88-th batch, train loss: 0.574373185634613:  60%|████████▎     | 87/146 [00:55<00:35,  1.64it/s]Epoch: 1, train for the 88-th batch, train loss: 0.574373185634613:  60%|████████▍     | 88/146 [00:55<00:35,  1.64it/s]Epoch: 1, train for the 96-th batch, train loss: 0.6578809022903442:  40%|█████▏       | 95/237 [00:53<01:26,  1.64it/s]Epoch: 1, train for the 96-th batch, train loss: 0.6578809022903442:  41%|█████▎       | 96/237 [00:53<01:26,  1.64it/s]Epoch: 1, train for the 87-th batch, train loss: 0.4639817178249359:  22%|██▉          | 86/383 [00:52<03:01,  1.64it/s]Epoch: 1, train for the 87-th batch, train loss: 0.4639817178249359:  23%|██▉          | 87/383 [00:52<03:00,  1.64it/s]Epoch: 1, train for the 91-th batch, train loss: 0.4982801079750061:  60%|███████▋     | 90/151 [00:55<00:36,  1.67it/s]Epoch: 1, train for the 91-th batch, train loss: 0.4982801079750061:  60%|███████▊     | 91/151 [00:55<00:36,  1.66it/s]Epoch: 1, train for the 90-th batch, train loss: 0.2776796519756317:  75%|█████████▋   | 89/119 [00:55<00:18,  1.64it/s]Epoch: 1, train for the 90-th batch, train loss: 0.2776796519756317:  76%|█████████▊   | 90/119 [00:55<00:17,  1.64it/s]Epoch: 1, train for the 87-th batch, train loss: 0.554434597492218:  36%|████▉         | 86/241 [00:54<01:35,  1.63it/s]Epoch: 1, train for the 87-th batch, train loss: 0.554434597492218:  36%|█████         | 87/241 [00:54<01:34,  1.63it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5398315787315369:  60%|███████▊     | 88/146 [00:55<00:35,  1.64it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5398315787315369:  61%|███████▉     | 89/146 [00:55<00:34,  1.64it/s]Epoch: 1, train for the 97-th batch, train loss: 0.644175112247467:  41%|█████▋        | 96/237 [00:54<01:26,  1.64it/s]Epoch: 1, train for the 97-th batch, train loss: 0.644175112247467:  41%|█████▋        | 97/237 [00:54<01:25,  1.64it/s]Epoch: 1, train for the 88-th batch, train loss: 0.47907814383506775:  23%|██▋         | 87/383 [00:52<03:00,  1.64it/s]Epoch: 1, train for the 88-th batch, train loss: 0.47907814383506775:  23%|██▊         | 88/383 [00:52<02:59,  1.64it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5555543899536133:  60%|███████▊     | 91/151 [00:56<00:36,  1.66it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5555543899536133:  61%|███████▉     | 92/151 [00:56<00:35,  1.65it/s]Epoch: 1, train for the 91-th batch, train loss: 0.26973482966423035:  76%|█████████   | 90/119 [00:56<00:17,  1.64it/s]Epoch: 1, train for the 91-th batch, train loss: 0.26973482966423035:  76%|█████████▏  | 91/119 [00:56<00:17,  1.64it/s]Epoch: 1, train for the 88-th batch, train loss: 0.5462254881858826:  36%|████▋        | 87/241 [00:54<01:34,  1.63it/s]Epoch: 1, train for the 88-th batch, train loss: 0.5462254881858826:  37%|████▋        | 88/241 [00:54<01:34,  1.63it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5453619956970215:  61%|███████▉     | 89/146 [00:56<00:34,  1.64it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5453619956970215:  62%|████████     | 90/146 [00:56<00:34,  1.64it/s]Epoch: 1, train for the 98-th batch, train loss: 0.607837975025177:  41%|█████▋        | 97/237 [00:54<01:25,  1.64it/s]Epoch: 1, train for the 98-th batch, train loss: 0.607837975025177:  41%|█████▊        | 98/237 [00:54<01:24,  1.65it/s]Epoch: 1, train for the 89-th batch, train loss: 0.47866979241371155:  23%|██▊         | 88/383 [00:53<02:59,  1.64it/s]Epoch: 1, train for the 89-th batch, train loss: 0.47866979241371155:  23%|██▊         | 89/383 [00:53<02:58,  1.65it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5597364902496338:  61%|███████▉     | 92/151 [00:56<00:35,  1.65it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5597364902496338:  62%|████████     | 93/151 [00:56<00:35,  1.64it/s]Epoch: 1, train for the 92-th batch, train loss: 0.29764997959136963:  76%|█████████▏  | 91/119 [00:57<00:17,  1.64it/s]Epoch: 1, train for the 92-th batch, train loss: 0.29764997959136963:  77%|█████████▎  | 92/119 [00:57<00:16,  1.64it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5601844787597656:  62%|████████     | 90/146 [00:56<00:34,  1.64it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5601844787597656:  62%|████████     | 91/146 [00:56<00:33,  1.64it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5794627070426941:  37%|████▋        | 88/241 [00:55<01:34,  1.63it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5794627070426941:  37%|████▊        | 89/241 [00:55<01:33,  1.62it/s]Epoch: 1, train for the 99-th batch, train loss: 0.650122344493866:  41%|█████▊        | 98/237 [00:55<01:24,  1.65it/s]Epoch: 1, train for the 99-th batch, train loss: 0.650122344493866:  42%|█████▊        | 99/237 [00:55<01:24,  1.64it/s]Epoch: 1, train for the 90-th batch, train loss: 0.44144928455352783:  23%|██▊         | 89/383 [00:53<02:58,  1.65it/s]Epoch: 1, train for the 90-th batch, train loss: 0.44144928455352783:  23%|██▊         | 90/383 [00:53<02:58,  1.64it/s]Epoch: 1, train for the 93-th batch, train loss: 0.22217398881912231:  77%|█████████▎  | 92/119 [00:57<00:16,  1.64it/s]Epoch: 1, train for the 93-th batch, train loss: 0.22217398881912231:  78%|█████████▍  | 93/119 [00:57<00:15,  1.64it/s]Epoch: 1, train for the 94-th batch, train loss: 0.5588096380233765:  62%|████████     | 93/151 [00:57<00:35,  1.64it/s]Epoch: 1, train for the 94-th batch, train loss: 0.5588096380233765:  62%|████████     | 94/151 [00:57<00:34,  1.63it/s]Epoch: 1, train for the 91-th batch, train loss: 0.3816399574279785:  23%|███          | 90/383 [00:54<02:58,  1.64it/s]Epoch: 1, train for the 91-th batch, train loss: 0.3816399574279785:  24%|███          | 91/383 [00:54<02:31,  1.92it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5518319010734558:  62%|████████     | 91/146 [00:57<00:33,  1.64it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5518319010734558:  63%|████████▏    | 92/146 [00:57<00:32,  1.64it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5313125848770142:  37%|████▊        | 89/241 [00:56<01:33,  1.62it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5313125848770142:  37%|████▊        | 90/241 [00:56<01:32,  1.62it/s]Epoch: 1, train for the 92-th batch, train loss: 0.3658066987991333:  24%|███          | 91/383 [00:54<02:31,  1.92it/s]Epoch: 1, train for the 92-th batch, train loss: 0.3658066987991333:  24%|███          | 92/383 [00:54<02:14,  2.16it/s]Epoch: 1, train for the 94-th batch, train loss: 0.2694236636161804:  78%|██████████▏  | 93/119 [00:58<00:15,  1.64it/s]Epoch: 1, train for the 94-th batch, train loss: 0.2694236636161804:  79%|██████████▎  | 94/119 [00:58<00:15,  1.64it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5558330416679382:  62%|████████     | 94/151 [00:58<00:34,  1.63it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5558330416679382:  63%|████████▏    | 95/151 [00:58<00:34,  1.63it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5652454495429993:  63%|████████▏    | 92/146 [00:58<00:32,  1.64it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5652454495429993:  64%|████████▎    | 93/146 [00:58<00:32,  1.64it/s]Epoch: 1, train for the 100-th batch, train loss: 0.6633244156837463:  42%|█████       | 99/237 [00:56<01:24,  1.64it/s]Epoch: 1, train for the 100-th batch, train loss: 0.6633244156837463:  42%|████▋      | 100/237 [00:56<01:45,  1.30it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5211815237998962:  24%|███          | 92/383 [00:54<02:14,  2.16it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5211815237998962:  24%|███▏         | 93/383 [00:54<02:03,  2.35it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5268257856369019:  37%|████▊        | 90/241 [00:56<01:32,  1.62it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5268257856369019:  38%|████▉        | 91/241 [00:56<01:32,  1.62it/s]Epoch: 1, train for the 95-th batch, train loss: 0.20982712507247925:  79%|█████████▍  | 94/119 [00:59<00:15,  1.64it/s]Epoch: 1, train for the 95-th batch, train loss: 0.20982712507247925:  80%|█████████▌  | 95/119 [00:59<00:14,  1.64it/s]Epoch: 1, train for the 96-th batch, train loss: 0.5719521045684814:  63%|████████▏    | 95/151 [00:58<00:34,  1.63it/s]Epoch: 1, train for the 96-th batch, train loss: 0.5719521045684814:  64%|████████▎    | 96/151 [00:58<00:33,  1.64it/s]Epoch: 1, train for the 94-th batch, train loss: 0.569231390953064:  64%|████████▉     | 93/146 [00:58<00:32,  1.64it/s]Epoch: 1, train for the 94-th batch, train loss: 0.569231390953064:  64%|█████████     | 94/146 [00:58<00:31,  1.64it/s]Epoch: 1, train for the 92-th batch, train loss: 0.441628098487854:  38%|█████▎        | 91/241 [00:57<01:32,  1.62it/s]Epoch: 1, train for the 92-th batch, train loss: 0.441628098487854:  38%|█████▎        | 92/241 [00:57<01:31,  1.63it/s]Epoch: 1, train for the 94-th batch, train loss: 0.432919979095459:  24%|███▍          | 93/383 [00:55<02:03,  2.35it/s]Epoch: 1, train for the 101-th batch, train loss: 0.6640879511833191:  42%|████▋      | 100/237 [00:57<01:45,  1.30it/s]Epoch: 1, train for the 94-th batch, train loss: 0.432919979095459:  25%|███▍          | 94/383 [00:55<02:22,  2.03it/s]Epoch: 1, train for the 101-th batch, train loss: 0.6640879511833191:  43%|████▋      | 101/237 [00:57<01:39,  1.36it/s]Epoch: 1, train for the 96-th batch, train loss: 0.227436363697052:  80%|███████████▏  | 95/119 [00:59<00:14,  1.64it/s]Epoch: 1, train for the 96-th batch, train loss: 0.227436363697052:  81%|███████████▎  | 96/119 [00:59<00:13,  1.65it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5747807025909424:  64%|████████▎    | 96/151 [00:59<00:33,  1.64it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5747807025909424:  64%|████████▎    | 97/151 [00:59<00:33,  1.63it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5388907194137573:  64%|████████▎    | 94/146 [00:59<00:31,  1.64it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5388907194137573:  65%|████████▍    | 95/146 [00:59<00:30,  1.65it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5609142780303955:  38%|████▉        | 92/241 [00:57<01:31,  1.63it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5609142780303955:  39%|█████        | 93/241 [00:57<01:30,  1.63it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5205327272415161:  25%|███▏         | 94/383 [00:56<02:22,  2.03it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5205327272415161:  25%|███▏         | 95/383 [00:56<02:34,  1.87it/s]Epoch: 1, train for the 102-th batch, train loss: 0.6627642512321472:  43%|████▋      | 101/237 [00:58<01:39,  1.36it/s]Epoch: 1, train for the 102-th batch, train loss: 0.6627642512321472:  43%|████▋      | 102/237 [00:58<01:35,  1.42it/s]Epoch: 1, train for the 97-th batch, train loss: 0.26780271530151367:  81%|█████████▋  | 96/119 [01:00<00:13,  1.65it/s]Epoch: 1, train for the 97-th batch, train loss: 0.26780271530151367:  82%|█████████▊  | 97/119 [01:00<00:13,  1.64it/s]Epoch: 1, train for the 98-th batch, train loss: 0.5951545238494873:  64%|████████▎    | 97/151 [00:59<00:33,  1.63it/s]Epoch: 1, train for the 98-th batch, train loss: 0.5951545238494873:  65%|████████▍    | 98/151 [00:59<00:32,  1.63it/s]Epoch: 1, train for the 96-th batch, train loss: 0.5462328195571899:  65%|████████▍    | 95/146 [00:59<00:30,  1.65it/s]Epoch: 1, train for the 96-th batch, train loss: 0.5462328195571899:  66%|████████▌    | 96/146 [00:59<00:30,  1.64it/s]Epoch: 1, train for the 94-th batch, train loss: 0.42767301201820374:  39%|████▋       | 93/241 [00:58<01:30,  1.63it/s]Epoch: 1, train for the 94-th batch, train loss: 0.42767301201820374:  39%|████▋       | 94/241 [00:58<01:30,  1.63it/s]Epoch: 1, train for the 96-th batch, train loss: 0.42822515964508057:  25%|██▉         | 95/383 [00:56<02:34,  1.87it/s]Epoch: 1, train for the 96-th batch, train loss: 0.42822515964508057:  25%|███         | 96/383 [00:56<02:43,  1.76it/s]Epoch: 1, train for the 103-th batch, train loss: 0.6793173551559448:  43%|████▋      | 102/237 [00:58<01:35,  1.42it/s]Epoch: 1, train for the 103-th batch, train loss: 0.6793173551559448:  43%|████▊      | 103/237 [00:58<01:31,  1.46it/s]Epoch: 1, train for the 98-th batch, train loss: 0.20333507657051086:  82%|█████████▊  | 97/119 [01:00<00:13,  1.64it/s]Epoch: 1, train for the 98-th batch, train loss: 0.20333507657051086:  82%|█████████▉  | 98/119 [01:00<00:12,  1.64it/s]Epoch: 1, train for the 99-th batch, train loss: 0.6069271564483643:  65%|████████▍    | 98/151 [01:00<00:32,  1.63it/s]Epoch: 1, train for the 99-th batch, train loss: 0.6069271564483643:  66%|████████▌    | 99/151 [01:00<00:31,  1.63it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5326497554779053:  66%|████████▌    | 96/146 [01:00<00:30,  1.64it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5326497554779053:  66%|████████▋    | 97/146 [01:00<00:29,  1.64it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5006558299064636:  39%|█████        | 94/241 [00:59<01:30,  1.63it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5006558299064636:  39%|█████        | 95/241 [00:59<01:29,  1.62it/s]Epoch: 1, train for the 104-th batch, train loss: 0.6652086973190308:  43%|████▊      | 103/237 [00:59<01:31,  1.46it/s]Epoch: 1, train for the 97-th batch, train loss: 0.3460426926612854:  25%|███▎         | 96/383 [00:57<02:43,  1.76it/s]Epoch: 1, train for the 104-th batch, train loss: 0.6652086973190308:  44%|████▊      | 104/237 [00:59<01:29,  1.48it/s]Epoch: 1, train for the 97-th batch, train loss: 0.3460426926612854:  25%|███▎         | 97/383 [00:57<02:50,  1.68it/s]Epoch: 1, train for the 99-th batch, train loss: 0.27280497550964355:  82%|█████████▉  | 98/119 [01:01<00:12,  1.64it/s]Epoch: 1, train for the 99-th batch, train loss: 0.27280497550964355:  83%|█████████▉  | 99/119 [01:01<00:12,  1.64it/s]Epoch: 1, train for the 96-th batch, train loss: 0.639660120010376:  39%|█████▌        | 95/241 [00:59<01:29,  1.62it/s]Epoch: 1, train for the 96-th batch, train loss: 0.639660120010376:  40%|█████▌        | 96/241 [00:59<01:21,  1.78it/s]Epoch: 1, train for the 98-th batch, train loss: 0.5659078359603882:  66%|████████▋    | 97/146 [01:01<00:29,  1.64it/s]Epoch: 1, train for the 98-th batch, train loss: 0.5659078359603882:  67%|████████▋    | 98/146 [01:01<00:29,  1.64it/s]Epoch: 1, train for the 97-th batch, train loss: 0.37206733226776123:  40%|████▊       | 96/241 [00:59<01:21,  1.78it/s]Epoch: 1, train for the 97-th batch, train loss: 0.37206733226776123:  40%|████▊       | 97/241 [00:59<01:06,  2.18it/s]Epoch: 1, train for the 98-th batch, train loss: 0.3832363188266754:  25%|███▎         | 97/383 [00:58<02:50,  1.68it/s]Epoch: 1, train for the 105-th batch, train loss: 0.6526023745536804:  44%|████▊      | 104/237 [00:59<01:29,  1.48it/s]Epoch: 1, train for the 98-th batch, train loss: 0.3832363188266754:  26%|███▎         | 98/383 [00:58<02:54,  1.63it/s]Epoch: 1, train for the 105-th batch, train loss: 0.6526023745536804:  44%|████▊      | 105/237 [00:59<01:28,  1.49it/s]Epoch: 1, train for the 100-th batch, train loss: 0.6788607835769653:  66%|███████▊    | 99/151 [01:01<00:31,  1.63it/s]Epoch: 1, train for the 100-th batch, train loss: 0.6788607835769653:  66%|███████▎   | 100/151 [01:01<00:36,  1.40it/s]Epoch: 1, train for the 99-th batch, train loss: 0.5538786053657532:  67%|████████▋    | 98/146 [01:01<00:29,  1.64it/s]Epoch: 1, train for the 99-th batch, train loss: 0.5538786053657532:  68%|████████▊    | 99/146 [01:01<00:25,  1.85it/s]Epoch: 1, train for the 98-th batch, train loss: 0.5529964566230774:  40%|█████▏       | 97/241 [01:00<01:06,  2.18it/s]Epoch: 1, train for the 98-th batch, train loss: 0.5529964566230774:  41%|█████▎       | 98/241 [01:00<01:11,  2.00it/s]Epoch: 1, train for the 100-th batch, train loss: 0.25303566455841064:  83%|█████████▏ | 99/119 [01:02<00:12,  1.64it/s]Epoch: 1, train for the 100-th batch, train loss: 0.25303566455841064:  84%|████████▍ | 100/119 [01:02<00:13,  1.42it/s]Epoch: 1, train for the 99-th batch, train loss: 0.47172465920448303:  26%|███         | 98/383 [00:58<02:54,  1.63it/s]Epoch: 1, train for the 99-th batch, train loss: 0.47172465920448303:  26%|███         | 99/383 [00:58<02:55,  1.62it/s]Epoch: 1, train for the 101-th batch, train loss: 0.6716017127037048:  66%|███████▎   | 100/151 [01:02<00:36,  1.40it/s]Epoch: 1, train for the 101-th batch, train loss: 0.6716017127037048:  67%|███████▎   | 101/151 [01:02<00:34,  1.46it/s]Epoch: 1, train for the 106-th batch, train loss: 0.6513229012489319:  44%|████▊      | 105/237 [01:00<01:28,  1.49it/s]Epoch: 1, train for the 106-th batch, train loss: 0.6513229012489319:  45%|████▉      | 106/237 [01:00<01:26,  1.51it/s]Epoch: 1, train for the 101-th batch, train loss: 0.22066204249858856:  84%|████████▍ | 100/119 [01:02<00:13,  1.42it/s]Epoch: 1, train for the 101-th batch, train loss: 0.22066204249858856:  85%|████████▍ | 101/119 [01:02<00:10,  1.79it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5814502835273743:  68%|████████▏   | 99/146 [01:02<00:25,  1.85it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5814502835273743:  68%|███████▌   | 100/146 [01:02<00:28,  1.63it/s]Epoch: 1, train for the 99-th batch, train loss: 0.3874928653240204:  41%|█████▎       | 98/241 [01:01<01:11,  2.00it/s]Epoch: 1, train for the 99-th batch, train loss: 0.3874928653240204:  41%|█████▎       | 99/241 [01:01<01:16,  1.85it/s]Epoch: 1, train for the 107-th batch, train loss: 0.6111357808113098:  45%|████▉      | 106/237 [01:01<01:26,  1.51it/s]Epoch: 1, train for the 107-th batch, train loss: 0.6111357808113098:  45%|████▉      | 107/237 [01:01<01:19,  1.63it/s]Epoch: 1, train for the 102-th batch, train loss: 0.24347302317619324:  85%|████████▍ | 101/119 [01:03<00:10,  1.79it/s]Epoch: 1, train for the 102-th batch, train loss: 0.24347302317619324:  86%|████████▌ | 102/119 [01:03<00:09,  1.87it/s]Epoch: 1, train for the 102-th batch, train loss: 0.6067577004432678:  67%|███████▎   | 101/151 [01:02<00:34,  1.46it/s]Epoch: 1, train for the 102-th batch, train loss: 0.6067577004432678:  68%|███████▍   | 102/151 [01:02<00:32,  1.50it/s]Epoch: 1, train for the 100-th batch, train loss: 0.4781474173069:  26%|███▉           | 99/383 [00:59<02:55,  1.62it/s]Epoch: 1, train for the 100-th batch, train loss: 0.4781474173069:  26%|███▋          | 100/383 [00:59<03:16,  1.44it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5432303547859192:  68%|███████▌   | 100/146 [01:02<00:28,  1.63it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5432303547859192:  69%|███████▌   | 101/146 [01:02<00:27,  1.64it/s]Epoch: 1, train for the 108-th batch, train loss: 0.650558590888977:  45%|█████▍      | 107/237 [01:01<01:19,  1.63it/s]Epoch: 1, train for the 108-th batch, train loss: 0.650558590888977:  46%|█████▍      | 108/237 [01:01<01:12,  1.79it/s]Epoch: 1, train for the 103-th batch, train loss: 0.6526386141777039:  68%|███████▍   | 102/151 [01:03<00:32,  1.50it/s]Epoch: 1, train for the 103-th batch, train loss: 0.6526386141777039:  68%|███████▌   | 103/151 [01:03<00:28,  1.71it/s]Epoch: 1, train for the 103-th batch, train loss: 0.2938936948776245:  86%|█████████▍ | 102/119 [01:03<00:09,  1.87it/s]Epoch: 1, train for the 103-th batch, train loss: 0.2938936948776245:  87%|█████████▌ | 103/119 [01:03<00:08,  1.81it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5629984736442566:  41%|████▉       | 99/241 [01:01<01:16,  1.85it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5629984736442566:  41%|████▌      | 100/241 [01:01<01:29,  1.58it/s]Epoch: 1, train for the 101-th batch, train loss: 0.49670928716659546:  26%|██▌       | 100/383 [01:00<03:16,  1.44it/s]Epoch: 1, train for the 101-th batch, train loss: 0.49670928716659546:  26%|██▋       | 101/383 [01:00<03:05,  1.52it/s]Epoch: 1, train for the 104-th batch, train loss: 0.6162717938423157:  68%|███████▌   | 103/151 [01:03<00:28,  1.71it/s]Epoch: 1, train for the 104-th batch, train loss: 0.6162717938423157:  69%|███████▌   | 104/151 [01:03<00:25,  1.83it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5439493656158447:  69%|███████▌   | 101/146 [01:03<00:27,  1.64it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5439493656158447:  70%|███████▋   | 102/146 [01:03<00:26,  1.66it/s]Epoch: 1, train for the 109-th batch, train loss: 0.663827657699585:  46%|█████▍      | 108/237 [01:02<01:12,  1.79it/s]Epoch: 1, train for the 109-th batch, train loss: 0.663827657699585:  46%|█████▌      | 109/237 [01:02<01:12,  1.76it/s]Epoch: 1, train for the 104-th batch, train loss: 0.27781614661216736:  87%|████████▋ | 103/119 [01:04<00:08,  1.81it/s]Epoch: 1, train for the 104-th batch, train loss: 0.27781614661216736:  87%|████████▋ | 104/119 [01:04<00:08,  1.77it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5257149934768677:  41%|████▌      | 100/241 [01:02<01:29,  1.58it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5257149934768677:  42%|████▌      | 101/241 [01:02<01:27,  1.59it/s]Epoch: 1, train for the 110-th batch, train loss: 0.6633641123771667:  46%|█████      | 109/237 [01:02<01:12,  1.76it/s]Epoch: 1, train for the 110-th batch, train loss: 0.6633641123771667:  46%|█████      | 110/237 [01:02<01:07,  1.87it/s]Epoch: 1, train for the 105-th batch, train loss: 0.5747430324554443:  69%|███████▌   | 104/151 [01:04<00:25,  1.83it/s]Epoch: 1, train for the 105-th batch, train loss: 0.5747430324554443:  70%|███████▋   | 105/151 [01:04<00:26,  1.76it/s]Epoch: 1, train for the 103-th batch, train loss: 0.5506937503814697:  70%|███████▋   | 102/146 [01:04<00:26,  1.66it/s]Epoch: 1, train for the 103-th batch, train loss: 0.5506937503814697:  71%|███████▊   | 103/146 [01:04<00:25,  1.67it/s]Epoch: 1, train for the 102-th batch, train loss: 0.48572656512260437:  26%|██▋       | 101/383 [01:00<03:05,  1.52it/s]Epoch: 1, train for the 102-th batch, train loss: 0.48572656512260437:  27%|██▋       | 102/383 [01:00<03:07,  1.50it/s]Epoch: 1, train for the 105-th batch, train loss: 0.23796775937080383:  87%|████████▋ | 104/119 [01:04<00:08,  1.77it/s]Epoch: 1, train for the 105-th batch, train loss: 0.23796775937080383:  88%|████████▊ | 105/119 [01:04<00:08,  1.75it/s]Epoch: 1, train for the 102-th batch, train loss: 0.571843147277832:  42%|█████       | 101/241 [01:03<01:27,  1.59it/s]Epoch: 1, train for the 102-th batch, train loss: 0.571843147277832:  42%|█████       | 102/241 [01:03<01:26,  1.60it/s]Epoch: 1, train for the 111-th batch, train loss: 0.6396448612213135:  46%|█████      | 110/237 [01:03<01:07,  1.87it/s]Epoch: 1, train for the 111-th batch, train loss: 0.6396448612213135:  47%|█████▏     | 111/237 [01:03<01:09,  1.82it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5545200109481812:  71%|███████▊   | 103/146 [01:04<00:25,  1.67it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5545200109481812:  71%|███████▊   | 104/146 [01:04<00:25,  1.67it/s]Epoch: 1, train for the 106-th batch, train loss: 0.558684766292572:  70%|████████▎   | 105/151 [01:04<00:26,  1.76it/s]Epoch: 1, train for the 106-th batch, train loss: 0.558684766292572:  70%|████████▍   | 106/151 [01:04<00:26,  1.71it/s]Epoch: 1, train for the 103-th batch, train loss: 0.36466848850250244:  27%|██▋       | 102/383 [01:01<03:07,  1.50it/s]Epoch: 1, train for the 103-th batch, train loss: 0.36466848850250244:  27%|██▋       | 103/383 [01:01<03:01,  1.55it/s]Epoch: 1, train for the 106-th batch, train loss: 0.20751431584358215:  88%|████████▊ | 105/119 [01:05<00:08,  1.75it/s]Epoch: 1, train for the 106-th batch, train loss: 0.20751431584358215:  89%|████████▉ | 106/119 [01:05<00:07,  1.73it/s]Epoch: 1, train for the 103-th batch, train loss: 0.571624219417572:  42%|█████       | 102/241 [01:03<01:26,  1.60it/s]Epoch: 1, train for the 103-th batch, train loss: 0.571624219417572:  43%|█████▏      | 103/241 [01:03<01:25,  1.61it/s]Epoch: 1, train for the 112-th batch, train loss: 0.6607698202133179:  47%|█████▏     | 111/237 [01:03<01:09,  1.82it/s]Epoch: 1, train for the 112-th batch, train loss: 0.6607698202133179:  47%|█████▏     | 112/237 [01:03<01:11,  1.76it/s]Epoch: 1, train for the 105-th batch, train loss: 0.5748763084411621:  71%|███████▊   | 104/146 [01:05<00:25,  1.67it/s]Epoch: 1, train for the 105-th batch, train loss: 0.5748763084411621:  72%|███████▉   | 105/146 [01:05<00:24,  1.68it/s]Epoch: 1, train for the 104-th batch, train loss: 0.4052388370037079:  27%|██▉        | 103/383 [01:02<03:01,  1.55it/s]Epoch: 1, train for the 104-th batch, train loss: 0.4052388370037079:  27%|██▉        | 104/383 [01:02<02:56,  1.58it/s]Epoch: 1, train for the 107-th batch, train loss: 0.5660191774368286:  70%|███████▋   | 106/151 [01:05<00:26,  1.71it/s]Epoch: 1, train for the 107-th batch, train loss: 0.5660191774368286:  71%|███████▊   | 107/151 [01:05<00:26,  1.69it/s]Epoch: 1, train for the 107-th batch, train loss: 0.31932419538497925:  89%|████████▉ | 106/119 [01:06<00:07,  1.73it/s]Epoch: 1, train for the 107-th batch, train loss: 0.31932419538497925:  90%|████████▉ | 107/119 [01:06<00:06,  1.72it/s]Epoch: 1, train for the 113-th batch, train loss: 0.6587952375411987:  47%|█████▏     | 112/237 [01:04<01:11,  1.76it/s]Epoch: 1, train for the 113-th batch, train loss: 0.6587952375411987:  48%|█████▏     | 113/237 [01:04<01:11,  1.74it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5394853353500366:  43%|████▋      | 103/241 [01:04<01:25,  1.61it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5394853353500366:  43%|████▋      | 104/241 [01:04<01:25,  1.61it/s]Epoch: 1, train for the 106-th batch, train loss: 0.576896071434021:  72%|████████▋   | 105/146 [01:05<00:24,  1.68it/s]Epoch: 1, train for the 106-th batch, train loss: 0.576896071434021:  73%|████████▋   | 106/146 [01:05<00:23,  1.68it/s]Epoch: 1, train for the 105-th batch, train loss: 0.3799484074115753:  27%|██▉        | 104/383 [01:02<02:56,  1.58it/s]Epoch: 1, train for the 105-th batch, train loss: 0.3799484074115753:  27%|███        | 105/383 [01:02<02:52,  1.61it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5619155168533325:  71%|███████▊   | 107/151 [01:05<00:26,  1.69it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5619155168533325:  72%|███████▊   | 108/151 [01:05<00:25,  1.67it/s]Epoch: 1, train for the 108-th batch, train loss: 0.20104239881038666:  90%|████████▉ | 107/119 [01:06<00:06,  1.72it/s]Epoch: 1, train for the 108-th batch, train loss: 0.20104239881038666:  91%|█████████ | 108/119 [01:06<00:06,  1.71it/s]Epoch: 1, train for the 114-th batch, train loss: 0.665291428565979:  48%|█████▋      | 113/237 [01:04<01:11,  1.74it/s]Epoch: 1, train for the 114-th batch, train loss: 0.665291428565979:  48%|█████▊      | 114/237 [01:04<01:11,  1.72it/s]Epoch: 1, train for the 105-th batch, train loss: 0.4691578447818756:  43%|████▋      | 104/241 [01:04<01:25,  1.61it/s]Epoch: 1, train for the 105-th batch, train loss: 0.4691578447818756:  44%|████▊      | 105/241 [01:04<01:24,  1.62it/s]Epoch: 1, train for the 107-th batch, train loss: 0.5644872784614563:  73%|███████▉   | 106/146 [01:06<00:23,  1.68it/s]Epoch: 1, train for the 107-th batch, train loss: 0.5644872784614563:  73%|████████   | 107/146 [01:06<00:23,  1.69it/s]Epoch: 1, train for the 106-th batch, train loss: 0.3866669833660126:  27%|███        | 105/383 [01:03<02:52,  1.61it/s]Epoch: 1, train for the 106-th batch, train loss: 0.3866669833660126:  28%|███        | 106/383 [01:03<02:50,  1.63it/s]Epoch: 1, train for the 109-th batch, train loss: 0.5557242631912231:  72%|███████▊   | 108/151 [01:06<00:25,  1.67it/s]Epoch: 1, train for the 109-th batch, train loss: 0.5557242631912231:  72%|███████▉   | 109/151 [01:06<00:25,  1.65it/s]Epoch: 1, train for the 109-th batch, train loss: 0.29978734254837036:  91%|█████████ | 108/119 [01:07<00:06,  1.71it/s]Epoch: 1, train for the 109-th batch, train loss: 0.29978734254837036:  92%|█████████▏| 109/119 [01:07<00:05,  1.71it/s]Epoch: 1, train for the 115-th batch, train loss: 0.6398311257362366:  48%|█████▎     | 114/237 [01:05<01:11,  1.72it/s]Epoch: 1, train for the 115-th batch, train loss: 0.6398311257362366:  49%|█████▎     | 115/237 [01:05<01:11,  1.70it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5709447264671326:  44%|████▊      | 105/241 [01:05<01:24,  1.62it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5709447264671326:  44%|████▊      | 106/241 [01:05<01:23,  1.62it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5583459138870239:  73%|████████   | 107/146 [01:07<00:23,  1.69it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5583459138870239:  74%|████████▏  | 108/146 [01:07<00:22,  1.69it/s]Epoch: 1, train for the 107-th batch, train loss: 0.461673378944397:  28%|███▎        | 106/383 [01:03<02:50,  1.63it/s]Epoch: 1, train for the 107-th batch, train loss: 0.461673378944397:  28%|███▎        | 107/383 [01:03<02:49,  1.63it/s]Epoch: 1, train for the 110-th batch, train loss: 0.5738299489021301:  72%|███████▉   | 109/151 [01:07<00:25,  1.65it/s]Epoch: 1, train for the 110-th batch, train loss: 0.5738299489021301:  73%|████████   | 110/151 [01:07<00:24,  1.64it/s]Epoch: 1, train for the 110-th batch, train loss: 0.23049354553222656:  92%|█████████▏| 109/119 [01:07<00:05,  1.71it/s]Epoch: 1, train for the 110-th batch, train loss: 0.23049354553222656:  92%|█████████▏| 110/119 [01:07<00:05,  1.70it/s]Epoch: 1, train for the 116-th batch, train loss: 0.671409010887146:  49%|█████▊      | 115/237 [01:06<01:11,  1.70it/s]Epoch: 1, train for the 116-th batch, train loss: 0.671409010887146:  49%|█████▊      | 116/237 [01:06<01:11,  1.68it/s]Epoch: 1, train for the 107-th batch, train loss: 0.6047877669334412:  44%|████▊      | 106/241 [01:06<01:23,  1.62it/s]Epoch: 1, train for the 107-th batch, train loss: 0.6047877669334412:  44%|████▉      | 107/241 [01:06<01:22,  1.62it/s]Epoch: 1, train for the 109-th batch, train loss: 0.5404908061027527:  74%|████████▏  | 108/146 [01:07<00:22,  1.69it/s]Epoch: 1, train for the 109-th batch, train loss: 0.5404908061027527:  75%|████████▏  | 109/146 [01:07<00:21,  1.69it/s]Epoch: 1, train for the 108-th batch, train loss: 0.40507078170776367:  28%|██▊       | 107/383 [01:04<02:49,  1.63it/s]Epoch: 1, train for the 108-th batch, train loss: 0.40507078170776367:  28%|██▊       | 108/383 [01:04<02:47,  1.64it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5239588618278503:  73%|████████   | 110/151 [01:07<00:24,  1.64it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5239588618278503:  74%|████████   | 111/151 [01:07<00:24,  1.64it/s]Epoch: 1, train for the 111-th batch, train loss: 0.2546650171279907:  92%|██████████▏| 110/119 [01:08<00:05,  1.70it/s]Epoch: 1, train for the 111-th batch, train loss: 0.2546650171279907:  93%|██████████▎| 111/119 [01:08<00:04,  1.70it/s]Epoch: 1, train for the 117-th batch, train loss: 0.6641297936439514:  49%|█████▍     | 116/237 [01:06<01:11,  1.68it/s]Epoch: 1, train for the 117-th batch, train loss: 0.6641297936439514:  49%|█████▍     | 117/237 [01:06<01:11,  1.67it/s]Epoch: 1, train for the 110-th batch, train loss: 0.5297679901123047:  75%|████████▏  | 109/146 [01:08<00:21,  1.69it/s]Epoch: 1, train for the 110-th batch, train loss: 0.5297679901123047:  75%|████████▎  | 110/146 [01:08<00:21,  1.69it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5180668234825134:  44%|████▉      | 107/241 [01:06<01:22,  1.62it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5180668234825134:  45%|████▉      | 108/241 [01:06<01:22,  1.62it/s]Epoch: 1, train for the 109-th batch, train loss: 0.42292091250419617:  28%|██▊       | 108/383 [01:05<02:47,  1.64it/s]Epoch: 1, train for the 109-th batch, train loss: 0.42292091250419617:  28%|██▊       | 109/383 [01:05<02:47,  1.64it/s]Epoch: 1, train for the 112-th batch, train loss: 0.5166454315185547:  74%|████████   | 111/151 [01:08<00:24,  1.64it/s]Epoch: 1, train for the 112-th batch, train loss: 0.5166454315185547:  74%|████████▏  | 112/151 [01:08<00:23,  1.63it/s]Epoch: 1, train for the 112-th batch, train loss: 0.1941961795091629:  93%|██████████▎| 111/119 [01:08<00:04,  1.70it/s]Epoch: 1, train for the 112-th batch, train loss: 0.1941961795091629:  94%|██████████▎| 112/119 [01:08<00:04,  1.75it/s]Epoch: 1, train for the 118-th batch, train loss: 0.6472952365875244:  49%|█████▍     | 117/237 [01:07<01:11,  1.67it/s]Epoch: 1, train for the 118-th batch, train loss: 0.6472952365875244:  50%|█████▍     | 118/237 [01:07<01:11,  1.66it/s]Epoch: 1, train for the 109-th batch, train loss: 0.7717353701591492:  45%|████▉      | 108/241 [01:07<01:22,  1.62it/s]Epoch: 1, train for the 109-th batch, train loss: 0.7717353701591492:  45%|████▉      | 109/241 [01:07<01:21,  1.62it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5180377960205078:  75%|████████▎  | 110/146 [01:09<00:21,  1.69it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5180377960205078:  76%|████████▎  | 111/146 [01:09<00:21,  1.59it/s]Epoch: 1, train for the 110-th batch, train loss: 0.43044355511665344:  28%|██▊       | 109/383 [01:05<02:47,  1.64it/s]Epoch: 1, train for the 110-th batch, train loss: 0.43044355511665344:  29%|██▊       | 110/383 [01:05<02:46,  1.64it/s]Epoch: 1, train for the 113-th batch, train loss: 0.216364324092865:  94%|███████████▎| 112/119 [01:09<00:04,  1.75it/s]Epoch: 1, train for the 113-th batch, train loss: 0.216364324092865:  95%|███████████▍| 113/119 [01:09<00:03,  1.79it/s]Epoch: 1, train for the 113-th batch, train loss: 0.5489183664321899:  74%|████████▏  | 112/151 [01:09<00:23,  1.63it/s]Epoch: 1, train for the 113-th batch, train loss: 0.5489183664321899:  75%|████████▏  | 113/151 [01:09<00:23,  1.63it/s]Epoch: 1, train for the 119-th batch, train loss: 0.6342328786849976:  50%|█████▍     | 118/237 [01:08<01:11,  1.66it/s]Epoch: 1, train for the 119-th batch, train loss: 0.6342328786849976:  50%|█████▌     | 119/237 [01:08<01:11,  1.66it/s]Epoch: 1, train for the 110-th batch, train loss: 0.6954033970832825:  45%|████▉      | 109/241 [01:08<01:21,  1.62it/s]Epoch: 1, train for the 110-th batch, train loss: 0.6954033970832825:  46%|█████      | 110/241 [01:08<01:20,  1.62it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5320213437080383:  29%|███▏       | 110/383 [01:06<02:46,  1.64it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5320213437080383:  29%|███▏       | 111/383 [01:06<02:45,  1.65it/s]Epoch: 1, train for the 112-th batch, train loss: 0.5707849264144897:  76%|████████▎  | 111/146 [01:09<00:21,  1.59it/s]Epoch: 1, train for the 112-th batch, train loss: 0.5707849264144897:  77%|████████▍  | 112/146 [01:09<00:21,  1.61it/s]Epoch: 1, train for the 114-th batch, train loss: 0.2649153470993042:  95%|██████████▍| 113/119 [01:10<00:03,  1.79it/s]Epoch: 1, train for the 114-th batch, train loss: 0.2649153470993042:  96%|██████████▌| 114/119 [01:10<00:02,  1.76it/s]Epoch: 1, train for the 114-th batch, train loss: 0.5093538761138916:  75%|████████▏  | 113/151 [01:09<00:23,  1.63it/s]Epoch: 1, train for the 114-th batch, train loss: 0.5093538761138916:  75%|████████▎  | 114/151 [01:09<00:22,  1.63it/s]Epoch: 1, train for the 120-th batch, train loss: 0.6376454830169678:  50%|█████▌     | 119/237 [01:08<01:11,  1.66it/s]Epoch: 1, train for the 120-th batch, train loss: 0.6376454830169678:  51%|█████▌     | 120/237 [01:08<01:10,  1.66it/s]Epoch: 1, train for the 111-th batch, train loss: 0.21176530420780182:  46%|████▌     | 110/241 [01:08<01:20,  1.62it/s]Epoch: 1, train for the 111-th batch, train loss: 0.21176530420780182:  46%|████▌     | 111/241 [01:08<01:20,  1.62it/s]Epoch: 1, train for the 112-th batch, train loss: 0.47584936022758484:  29%|██▉       | 111/383 [01:06<02:45,  1.65it/s]Epoch: 1, train for the 112-th batch, train loss: 0.47584936022758484:  29%|██▉       | 112/383 [01:06<02:43,  1.66it/s]Epoch: 1, train for the 113-th batch, train loss: 0.5191116333007812:  77%|████████▍  | 112/146 [01:10<00:21,  1.61it/s]Epoch: 1, train for the 113-th batch, train loss: 0.5191116333007812:  77%|████████▌  | 113/146 [01:10<00:20,  1.60it/s]Epoch: 1, train for the 115-th batch, train loss: 0.23285327851772308:  96%|█████████▌| 114/119 [01:10<00:02,  1.76it/s]Epoch: 1, train for the 115-th batch, train loss: 0.23285327851772308:  97%|█████████▋| 115/119 [01:10<00:02,  1.72it/s]Epoch: 1, train for the 115-th batch, train loss: 0.4935111999511719:  75%|████████▎  | 114/151 [01:10<00:22,  1.63it/s]Epoch: 1, train for the 115-th batch, train loss: 0.4935111999511719:  76%|████████▍  | 115/151 [01:10<00:22,  1.63it/s]Epoch: 1, train for the 121-th batch, train loss: 0.6684908866882324:  51%|█████▌     | 120/237 [01:09<01:10,  1.66it/s]Epoch: 1, train for the 121-th batch, train loss: 0.6684908866882324:  51%|█████▌     | 121/237 [01:09<01:09,  1.67it/s]Epoch: 1, train for the 112-th batch, train loss: 0.2500731348991394:  46%|█████      | 111/241 [01:09<01:20,  1.62it/s]Epoch: 1, train for the 112-th batch, train loss: 0.2500731348991394:  46%|█████      | 112/241 [01:09<01:19,  1.62it/s]Epoch: 1, train for the 113-th batch, train loss: 0.3905843198299408:  29%|███▏       | 112/383 [01:07<02:43,  1.66it/s]Epoch: 1, train for the 113-th batch, train loss: 0.3905843198299408:  30%|███▏       | 113/383 [01:07<02:42,  1.66it/s]Epoch: 1, train for the 114-th batch, train loss: 0.5420665740966797:  77%|████████▌  | 113/146 [01:10<00:20,  1.60it/s]Epoch: 1, train for the 114-th batch, train loss: 0.5420665740966797:  78%|████████▌  | 114/146 [01:10<00:20,  1.58it/s]Epoch: 1, train for the 116-th batch, train loss: 0.16373541951179504:  97%|█████████▋| 115/119 [01:11<00:02,  1.72it/s]Epoch: 1, train for the 116-th batch, train loss: 0.16373541951179504:  97%|█████████▋| 116/119 [01:11<00:01,  1.67it/s]Epoch: 1, train for the 116-th batch, train loss: 0.48110145330429077:  76%|███████▌  | 115/151 [01:10<00:22,  1.63it/s]Epoch: 1, train for the 116-th batch, train loss: 0.48110145330429077:  77%|███████▋  | 116/151 [01:10<00:21,  1.62it/s]Epoch: 1, train for the 122-th batch, train loss: 0.6575559973716736:  51%|█████▌     | 121/237 [01:09<01:09,  1.67it/s]Epoch: 1, train for the 122-th batch, train loss: 0.6575559973716736:  51%|█████▋     | 122/237 [01:09<01:08,  1.67it/s]Epoch: 1, train for the 113-th batch, train loss: 0.24654695391654968:  46%|████▋     | 112/241 [01:09<01:19,  1.62it/s]Epoch: 1, train for the 113-th batch, train loss: 0.24654695391654968:  47%|████▋     | 113/241 [01:09<01:18,  1.62it/s]Epoch: 1, train for the 114-th batch, train loss: 0.42928892374038696:  30%|██▉       | 113/383 [01:08<02:42,  1.66it/s]Epoch: 1, train for the 114-th batch, train loss: 0.42928892374038696:  30%|██▉       | 114/383 [01:08<02:41,  1.67it/s]Epoch: 1, train for the 117-th batch, train loss: 0.5362277626991272:  77%|████████▍  | 116/151 [01:11<00:21,  1.62it/s]Epoch: 1, train for the 117-th batch, train loss: 0.5362277626991272:  77%|████████▌  | 117/151 [01:11<00:20,  1.62it/s]Epoch: 1, train for the 117-th batch, train loss: 0.24916377663612366:  97%|█████████▋| 116/119 [01:11<00:01,  1.67it/s]Epoch: 1, train for the 115-th batch, train loss: 0.5652061104774475:  78%|████████▌  | 114/146 [01:11<00:20,  1.58it/s]Epoch: 1, train for the 117-th batch, train loss: 0.24916377663612366:  98%|█████████▊| 117/119 [01:11<00:01,  1.62it/s]Epoch: 1, train for the 115-th batch, train loss: 0.5652061104774475:  79%|████████▋  | 115/146 [01:11<00:19,  1.56it/s]Epoch: 1, train for the 123-th batch, train loss: 0.6399432420730591:  51%|█████▋     | 122/237 [01:10<01:08,  1.67it/s]Epoch: 1, train for the 123-th batch, train loss: 0.6399432420730591:  52%|█████▋     | 123/237 [01:10<01:08,  1.67it/s]Epoch: 1, train for the 115-th batch, train loss: 0.38053375482559204:  30%|██▉       | 114/383 [01:08<02:41,  1.67it/s]Epoch: 1, train for the 115-th batch, train loss: 0.38053375482559204:  30%|███       | 115/383 [01:08<02:40,  1.67it/s]Epoch: 1, train for the 114-th batch, train loss: 0.2795918881893158:  47%|█████▏     | 113/241 [01:10<01:18,  1.62it/s]Epoch: 1, train for the 114-th batch, train loss: 0.2795918881893158:  47%|█████▏     | 114/241 [01:10<01:18,  1.62it/s]Epoch: 1, train for the 118-th batch, train loss: 0.490953654050827:  77%|█████████▎  | 117/151 [01:12<00:20,  1.62it/s]Epoch: 1, train for the 118-th batch, train loss: 0.490953654050827:  78%|█████████▍  | 118/151 [01:12<00:20,  1.62it/s]Epoch: 1, train for the 116-th batch, train loss: 0.5465396642684937:  79%|████████▋  | 115/146 [01:12<00:19,  1.56it/s]Epoch: 1, train for the 118-th batch, train loss: 0.2134942263364792:  98%|██████████▊| 117/119 [01:12<00:01,  1.62it/s]Epoch: 1, train for the 116-th batch, train loss: 0.5465396642684937:  79%|████████▋  | 116/146 [01:12<00:19,  1.54it/s]Epoch: 1, train for the 118-th batch, train loss: 0.2134942263364792:  99%|██████████▉| 118/119 [01:12<00:00,  1.59it/s]Epoch: 1, train for the 124-th batch, train loss: 0.6402981281280518:  52%|█████▋     | 123/237 [01:11<01:08,  1.67it/s]Epoch: 1, train for the 124-th batch, train loss: 0.6402981281280518:  52%|█████▊     | 124/237 [01:11<01:07,  1.67it/s]Epoch: 1, train for the 117-th batch, train loss: 0.601013720035553:  79%|█████████▌  | 116/146 [01:12<00:19,  1.54it/s]Epoch: 1, train for the 117-th batch, train loss: 0.601013720035553:  80%|█████████▌  | 117/146 [01:12<00:15,  1.91it/s]Epoch: 1, train for the 116-th batch, train loss: 0.47544097900390625:  30%|███       | 115/383 [01:09<02:40,  1.67it/s]Epoch: 1, train for the 116-th batch, train loss: 0.47544097900390625:  30%|███       | 116/383 [01:09<02:40,  1.66it/s]Epoch: 1, train for the 115-th batch, train loss: 0.48076996207237244:  47%|████▋     | 114/241 [01:11<01:18,  1.62it/s]Epoch: 1, train for the 115-th batch, train loss: 0.48076996207237244:  48%|████▊     | 115/241 [01:11<01:17,  1.62it/s]Epoch: 1, train for the 119-th batch, train loss: 0.5425146222114563:  78%|████████▌  | 118/151 [01:12<00:20,  1.62it/s]Epoch: 1, train for the 119-th batch, train loss: 0.5425146222114563:  79%|████████▋  | 119/151 [01:12<00:19,  1.62it/s]Epoch: 1, train for the 119-th batch, train loss: 0.4021523892879486:  99%|██████████▉| 118/119 [01:13<00:00,  1.59it/s]Epoch: 1, train for the 119-th batch, train loss: 0.4021523892879486: 100%|███████████| 119/119 [01:13<00:00,  1.59it/s]Epoch: 1, train for the 119-th batch, train loss: 0.4021523892879486: 100%|███████████| 119/119 [01:13<00:00,  1.62it/s]
  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 1, train for the 118-th batch, train loss: 0.5142315626144409:  80%|████████▊  | 117/146 [01:12<00:15,  1.91it/s]Epoch: 1, train for the 118-th batch, train loss: 0.5142315626144409:  81%|████████▉  | 118/146 [01:12<00:14,  1.95it/s]Epoch: 1, train for the 125-th batch, train loss: 0.628845751285553:  52%|██████▎     | 124/237 [01:11<01:07,  1.67it/s]Epoch: 1, train for the 125-th batch, train loss: 0.628845751285553:  53%|██████▎     | 125/237 [01:11<01:07,  1.66it/s]evaluate for the 1-th batch, evaluate loss: 0.23064732551574707:   0%|                           | 0/40 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.23064732551574707:   2%|▍                  | 1/40 [00:00<00:10,  3.84it/s]Epoch: 1, train for the 117-th batch, train loss: 0.4249379634857178:  30%|███▎       | 116/383 [01:09<02:40,  1.66it/s]Epoch: 1, train for the 117-th batch, train loss: 0.4249379634857178:  31%|███▎       | 117/383 [01:09<02:40,  1.66it/s]Epoch: 1, train for the 116-th batch, train loss: 0.5825105309486389:  48%|█████▏     | 115/241 [01:11<01:17,  1.62it/s]Epoch: 1, train for the 116-th batch, train loss: 0.5825105309486389:  48%|█████▎     | 116/241 [01:11<01:17,  1.62it/s]Epoch: 1, train for the 120-th batch, train loss: 0.6094311475753784:  79%|████████▋  | 119/151 [01:13<00:19,  1.62it/s]Epoch: 1, train for the 120-th batch, train loss: 0.6094311475753784:  79%|████████▋  | 120/151 [01:13<00:19,  1.62it/s]evaluate for the 2-th batch, evaluate loss: 0.2100432962179184:   2%|▌                   | 1/40 [00:00<00:10,  3.84it/s]evaluate for the 2-th batch, evaluate loss: 0.2100432962179184:   5%|█                   | 2/40 [00:00<00:10,  3.63it/s]Epoch: 1, train for the 119-th batch, train loss: 0.5546805262565613:  81%|████████▉  | 118/146 [01:13<00:14,  1.95it/s]Epoch: 1, train for the 119-th batch, train loss: 0.5546805262565613:  82%|████████▉  | 119/146 [01:13<00:14,  1.85it/s]Epoch: 1, train for the 126-th batch, train loss: 0.6364994049072266:  53%|█████▊     | 125/237 [01:12<01:07,  1.66it/s]Epoch: 1, train for the 126-th batch, train loss: 0.6364994049072266:  53%|█████▊     | 126/237 [01:12<01:06,  1.66it/s]evaluate for the 3-th batch, evaluate loss: 0.2742008864879608:   5%|█                   | 2/40 [00:00<00:10,  3.63it/s]evaluate for the 3-th batch, evaluate loss: 0.2742008864879608:   8%|█▌                  | 3/40 [00:00<00:10,  3.52it/s]Epoch: 1, train for the 118-th batch, train loss: 0.42775702476501465:  31%|███       | 117/383 [01:10<02:40,  1.66it/s]Epoch: 1, train for the 118-th batch, train loss: 0.42775702476501465:  31%|███       | 118/383 [01:10<02:40,  1.66it/s]Epoch: 1, train for the 117-th batch, train loss: 0.6319771409034729:  48%|█████▎     | 116/241 [01:12<01:17,  1.62it/s]Epoch: 1, train for the 117-th batch, train loss: 0.6319771409034729:  49%|█████▎     | 117/241 [01:12<01:16,  1.62it/s]evaluate for the 4-th batch, evaluate loss: 0.1872643381357193:   8%|█▌                  | 3/40 [00:01<00:10,  3.52it/s]evaluate for the 4-th batch, evaluate loss: 0.1872643381357193:  10%|██                  | 4/40 [00:01<00:10,  3.59it/s]Epoch: 1, train for the 121-th batch, train loss: 0.520692765712738:  79%|█████████▌  | 120/151 [01:14<00:19,  1.62it/s]Epoch: 1, train for the 121-th batch, train loss: 0.520692765712738:  80%|█████████▌  | 121/151 [01:14<00:18,  1.62it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5447800159454346:  82%|████████▉  | 119/146 [01:14<00:14,  1.85it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5447800159454346:  82%|█████████  | 120/146 [01:14<00:14,  1.79it/s]evaluate for the 5-th batch, evaluate loss: 0.23159350454807281:  10%|█▉                 | 4/40 [00:01<00:10,  3.59it/s]evaluate for the 5-th batch, evaluate loss: 0.23159350454807281:  12%|██▍                | 5/40 [00:01<00:09,  3.50it/s]Epoch: 1, train for the 127-th batch, train loss: 0.6410291194915771:  53%|█████▊     | 126/237 [01:12<01:06,  1.66it/s]Epoch: 1, train for the 127-th batch, train loss: 0.6410291194915771:  54%|█████▉     | 127/237 [01:12<01:06,  1.66it/s]Epoch: 1, train for the 119-th batch, train loss: 0.4255366027355194:  31%|███▍       | 118/383 [01:11<02:40,  1.66it/s]Epoch: 1, train for the 119-th batch, train loss: 0.4255366027355194:  31%|███▍       | 119/383 [01:11<02:39,  1.65it/s]evaluate for the 6-th batch, evaluate loss: 0.2277052104473114:  12%|██▌                 | 5/40 [00:01<00:09,  3.50it/s]evaluate for the 6-th batch, evaluate loss: 0.2277052104473114:  15%|███                 | 6/40 [00:01<00:08,  3.81it/s]Epoch: 1, train for the 118-th batch, train loss: 0.5276384353637695:  49%|█████▎     | 117/241 [01:13<01:16,  1.62it/s]Epoch: 1, train for the 118-th batch, train loss: 0.5276384353637695:  49%|█████▍     | 118/241 [01:13<01:15,  1.62it/s]Epoch: 1, train for the 122-th batch, train loss: 0.5539962649345398:  80%|████████▊  | 121/151 [01:14<00:18,  1.62it/s]Epoch: 1, train for the 122-th batch, train loss: 0.5539962649345398:  81%|████████▉  | 122/151 [01:14<00:17,  1.62it/s]Epoch: 1, train for the 121-th batch, train loss: 0.5336850881576538:  82%|█████████  | 120/146 [01:14<00:14,  1.79it/s]Epoch: 1, train for the 121-th batch, train loss: 0.5336850881576538:  83%|█████████  | 121/146 [01:14<00:14,  1.74it/s]evaluate for the 7-th batch, evaluate loss: 0.1330169439315796:  15%|███                 | 6/40 [00:01<00:08,  3.81it/s]evaluate for the 7-th batch, evaluate loss: 0.1330169439315796:  18%|███▌                | 7/40 [00:01<00:09,  3.59it/s]Epoch: 1, train for the 128-th batch, train loss: 0.6262872815132141:  54%|█████▉     | 127/237 [01:13<01:06,  1.66it/s]Epoch: 1, train for the 128-th batch, train loss: 0.6262872815132141:  54%|█████▉     | 128/237 [01:13<01:06,  1.65it/s]Epoch: 1, train for the 120-th batch, train loss: 0.44906309247016907:  31%|███       | 119/383 [01:11<02:39,  1.65it/s]Epoch: 1, train for the 120-th batch, train loss: 0.44906309247016907:  31%|███▏      | 120/383 [01:11<02:39,  1.65it/s]evaluate for the 8-th batch, evaluate loss: 0.1953289955854416:  18%|███▌                | 7/40 [00:02<00:09,  3.59it/s]evaluate for the 8-th batch, evaluate loss: 0.1953289955854416:  20%|████                | 8/40 [00:02<00:08,  3.67it/s]Epoch: 1, train for the 119-th batch, train loss: 0.3792038559913635:  49%|█████▍     | 118/241 [01:13<01:15,  1.62it/s]Epoch: 1, train for the 119-th batch, train loss: 0.3792038559913635:  49%|█████▍     | 119/241 [01:13<01:15,  1.62it/s]Epoch: 1, train for the 123-th batch, train loss: 0.5679323673248291:  81%|████████▉  | 122/151 [01:15<00:17,  1.62it/s]Epoch: 1, train for the 123-th batch, train loss: 0.5679323673248291:  81%|████████▉  | 123/151 [01:15<00:17,  1.63it/s]Epoch: 1, train for the 122-th batch, train loss: 0.5608445405960083:  83%|█████████  | 121/146 [01:15<00:14,  1.74it/s]Epoch: 1, train for the 122-th batch, train loss: 0.5608445405960083:  84%|█████████▏ | 122/146 [01:15<00:13,  1.71it/s]evaluate for the 9-th batch, evaluate loss: 0.18633921444416046:  20%|███▊               | 8/40 [00:02<00:08,  3.67it/s]evaluate for the 9-th batch, evaluate loss: 0.18633921444416046:  22%|████▎              | 9/40 [00:02<00:08,  3.49it/s]Epoch: 1, train for the 129-th batch, train loss: 0.6397718787193298:  54%|█████▉     | 128/237 [01:14<01:06,  1.65it/s]Epoch: 1, train for the 129-th batch, train loss: 0.6397718787193298:  54%|█████▉     | 129/237 [01:14<01:05,  1.65it/s]evaluate for the 10-th batch, evaluate loss: 0.2530338764190674:  22%|████▎              | 9/40 [00:02<00:08,  3.49it/s]evaluate for the 10-th batch, evaluate loss: 0.2530338764190674:  25%|████▌             | 10/40 [00:02<00:08,  3.56it/s]Epoch: 1, train for the 121-th batch, train loss: 0.48473304510116577:  31%|███▏      | 120/383 [01:12<02:39,  1.65it/s]Epoch: 1, train for the 121-th batch, train loss: 0.48473304510116577:  32%|███▏      | 121/383 [01:12<02:39,  1.65it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5991171598434448:  49%|█████▍     | 119/241 [01:14<01:15,  1.62it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5991171598434448:  50%|█████▍     | 120/241 [01:14<01:14,  1.63it/s]Epoch: 1, train for the 124-th batch, train loss: 0.5152418613433838:  81%|████████▉  | 123/151 [01:15<00:17,  1.63it/s]Epoch: 1, train for the 124-th batch, train loss: 0.5152418613433838:  82%|█████████  | 124/151 [01:15<00:16,  1.64it/s]evaluate for the 11-th batch, evaluate loss: 0.22302097082138062:  25%|████▎            | 10/40 [00:03<00:08,  3.56it/s]evaluate for the 11-th batch, evaluate loss: 0.22302097082138062:  28%|████▋            | 11/40 [00:03<00:08,  3.51it/s]Epoch: 1, train for the 123-th batch, train loss: 0.5754870772361755:  84%|█████████▏ | 122/146 [01:15<00:13,  1.71it/s]Epoch: 1, train for the 123-th batch, train loss: 0.5754870772361755:  84%|█████████▎ | 123/146 [01:15<00:13,  1.70it/s]Epoch: 1, train for the 130-th batch, train loss: 0.631621778011322:  54%|██████▌     | 129/237 [01:14<01:05,  1.65it/s]Epoch: 1, train for the 130-th batch, train loss: 0.631621778011322:  55%|██████▌     | 130/237 [01:14<01:04,  1.65it/s]evaluate for the 12-th batch, evaluate loss: 0.19052325189113617:  28%|████▋            | 11/40 [00:03<00:08,  3.51it/s]evaluate for the 12-th batch, evaluate loss: 0.19052325189113617:  30%|█████            | 12/40 [00:03<00:07,  3.64it/s]Epoch: 1, train for the 122-th batch, train loss: 0.501996636390686:  32%|███▊        | 121/383 [01:12<02:39,  1.65it/s]Epoch: 1, train for the 122-th batch, train loss: 0.501996636390686:  32%|███▊        | 122/383 [01:12<02:37,  1.65it/s]Epoch: 1, train for the 121-th batch, train loss: 0.5676431059837341:  50%|█████▍     | 120/241 [01:14<01:14,  1.63it/s]Epoch: 1, train for the 121-th batch, train loss: 0.5676431059837341:  50%|█████▌     | 121/241 [01:14<01:13,  1.64it/s]Epoch: 1, train for the 125-th batch, train loss: 0.5795260667800903:  82%|█████████  | 124/151 [01:16<00:16,  1.64it/s]Epoch: 1, train for the 125-th batch, train loss: 0.5795260667800903:  83%|█████████  | 125/151 [01:16<00:15,  1.64it/s]evaluate for the 13-th batch, evaluate loss: 0.14111755788326263:  30%|█████            | 12/40 [00:03<00:07,  3.64it/s]evaluate for the 13-th batch, evaluate loss: 0.14111755788326263:  32%|█████▌           | 13/40 [00:03<00:07,  3.58it/s]Epoch: 1, train for the 124-th batch, train loss: 0.5557449460029602:  84%|█████████▎ | 123/146 [01:16<00:13,  1.70it/s]Epoch: 1, train for the 124-th batch, train loss: 0.5557449460029602:  85%|█████████▎ | 124/146 [01:16<00:12,  1.70it/s]Epoch: 1, train for the 121-th batch, train loss: 0.5676431059837341:  50%|█████▌     | 121/241 [01:15<01:14,  1.61it/s]
Traceback (most recent call last):
  File "/home/ayush/DyGLib/train_link_prediction.py", line 552, in <module>
    batch_src_node_embeddings, batch_dst_node_embeddings = model[
  File "/home/ayush/DyGLib/models/DyGFormer.py", line 207, in compute_src_dst_node_temporal_embeddings
    self.memory_bank.node_last_updated_times[src_unique_ids] = torch.from_numpy(node_interact_times[src_latest_indices]).float().to(self.device)
TypeError: expected np.ndarray (got numpy.float64)
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
Epoch: 1, train for the 131-th batch, train loss: 0.6164915561676025:  55%|██████     | 130/237 [01:15<01:04,  1.65it/s]Epoch: 1, train for the 131-th batch, train loss: 0.6164915561676025:  55%|██████     | 131/237 [01:15<01:03,  1.66it/s]evaluate for the 14-th batch, evaluate loss: 0.21891334652900696:  32%|█████▌           | 13/40 [00:03<00:07,  3.58it/s]evaluate for the 14-th batch, evaluate loss: 0.21891334652900696:  35%|█████▉           | 14/40 [00:03<00:07,  3.68it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5413246154785156:  83%|█████████  | 125/151 [01:16<00:15,  1.64it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5413246154785156:  83%|█████████▏ | 126/151 [01:16<00:12,  1.94it/s]Epoch: 1, train for the 123-th batch, train loss: 0.4490412771701813:  32%|███▌       | 122/383 [01:13<02:37,  1.65it/s]Epoch: 1, train for the 123-th batch, train loss: 0.4490412771701813:  32%|███▌       | 123/383 [01:13<02:36,  1.66it/s]Epoch: 1, train for the 127-th batch, train loss: 0.5334773063659668:  83%|█████████▏ | 126/151 [01:16<00:12,  1.94it/s]Epoch: 1, train for the 127-th batch, train loss: 0.5334773063659668:  84%|█████████▎ | 127/151 [01:16<00:10,  2.34it/s]evaluate for the 15-th batch, evaluate loss: 0.2291942536830902:  35%|██████▎           | 14/40 [00:04<00:07,  3.68it/s]evaluate for the 15-th batch, evaluate loss: 0.2291942536830902:  38%|██████▊           | 15/40 [00:04<00:06,  3.63it/s]Epoch: 1, train for the 125-th batch, train loss: 0.5598297119140625:  85%|█████████▎ | 124/146 [01:17<00:12,  1.70it/s]Epoch: 1, train for the 125-th batch, train loss: 0.5598297119140625:  86%|█████████▍ | 125/146 [01:17<00:12,  1.68it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5830013751983643:  84%|█████████▎ | 127/151 [01:17<00:10,  2.34it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5830013751983643:  85%|█████████▎ | 128/151 [01:17<00:08,  2.73it/s]Epoch: 1, train for the 132-th batch, train loss: 0.6292800903320312:  55%|██████     | 131/237 [01:15<01:03,  1.66it/s]Epoch: 1, train for the 132-th batch, train loss: 0.6292800903320312:  56%|██████▏    | 132/237 [01:15<01:03,  1.66it/s]evaluate for the 16-th batch, evaluate loss: 0.2671656608581543:  38%|██████▊           | 15/40 [00:04<00:06,  3.63it/s]evaluate for the 16-th batch, evaluate loss: 0.2671656608581543:  40%|███████▏          | 16/40 [00:04<00:06,  3.56it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5643954873085022:  85%|█████████▎ | 128/151 [01:17<00:08,  2.73it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5643954873085022:  85%|█████████▍ | 129/151 [01:17<00:07,  3.10it/s]Epoch: 1, train for the 124-th batch, train loss: 0.4164496064186096:  32%|███▌       | 123/383 [01:14<02:36,  1.66it/s]Epoch: 1, train for the 124-th batch, train loss: 0.4164496064186096:  32%|███▌       | 124/383 [01:14<02:36,  1.66it/s]evaluate for the 17-th batch, evaluate loss: 0.16945964097976685:  40%|██████▊          | 16/40 [00:04<00:06,  3.56it/s]evaluate for the 17-th batch, evaluate loss: 0.16945964097976685:  42%|███████▏         | 17/40 [00:04<00:06,  3.60it/s]Epoch: 1, train for the 130-th batch, train loss: 0.556576132774353:  85%|██████████▎ | 129/151 [01:17<00:07,  3.10it/s]Epoch: 1, train for the 130-th batch, train loss: 0.556576132774353:  86%|██████████▎ | 130/151 [01:17<00:06,  3.42it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5468718409538269:  86%|█████████▍ | 125/146 [01:17<00:12,  1.68it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5468718409538269:  86%|█████████▍ | 126/146 [01:17<00:11,  1.68it/s]Epoch: 1, train for the 131-th batch, train loss: 0.5020102858543396:  86%|█████████▍ | 130/151 [01:17<00:06,  3.42it/s]Epoch: 1, train for the 131-th batch, train loss: 0.5020102858543396:  87%|█████████▌ | 131/151 [01:17<00:05,  3.69it/s]evaluate for the 18-th batch, evaluate loss: 0.20052823424339294:  42%|███████▏         | 17/40 [00:05<00:06,  3.60it/s]evaluate for the 18-th batch, evaluate loss: 0.20052823424339294:  45%|███████▋         | 18/40 [00:05<00:06,  3.52it/s]Epoch: 1, train for the 133-th batch, train loss: 0.6174601316452026:  56%|██████▏    | 132/237 [01:16<01:03,  1.66it/s]Epoch: 1, train for the 133-th batch, train loss: 0.6174601316452026:  56%|██████▏    | 133/237 [01:16<01:02,  1.67it/s]Epoch: 1, train for the 125-th batch, train loss: 0.4505184590816498:  32%|███▌       | 124/383 [01:14<02:36,  1.66it/s]Epoch: 1, train for the 125-th batch, train loss: 0.4505184590816498:  33%|███▌       | 125/383 [01:14<02:34,  1.66it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5121119022369385:  87%|█████████▌ | 131/151 [01:18<00:05,  3.69it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5121119022369385:  87%|█████████▌ | 132/151 [01:18<00:04,  3.89it/s]evaluate for the 19-th batch, evaluate loss: 0.2333330512046814:  45%|████████          | 18/40 [00:05<00:06,  3.52it/s]evaluate for the 19-th batch, evaluate loss: 0.2333330512046814:  48%|████████▌         | 19/40 [00:05<00:05,  3.68it/s]Epoch: 1, train for the 133-th batch, train loss: 0.5715064406394958:  87%|█████████▌ | 132/151 [01:18<00:04,  3.89it/s]Epoch: 1, train for the 133-th batch, train loss: 0.5715064406394958:  88%|█████████▋ | 133/151 [01:18<00:04,  4.05it/s]Epoch: 1, train for the 127-th batch, train loss: 0.5452864766120911:  86%|█████████▍ | 126/146 [01:18<00:11,  1.68it/s]Epoch: 1, train for the 127-th batch, train loss: 0.5452864766120911:  87%|█████████▌ | 127/146 [01:18<00:11,  1.68it/s]evaluate for the 20-th batch, evaluate loss: 0.1979387402534485:  48%|████████▌         | 19/40 [00:05<00:05,  3.68it/s]evaluate for the 20-th batch, evaluate loss: 0.1979387402534485:  50%|█████████         | 20/40 [00:05<00:05,  3.56it/s]Epoch: 1, train for the 134-th batch, train loss: 0.6271426677703857:  56%|██████▏    | 133/237 [01:17<01:02,  1.67it/s]Epoch: 1, train for the 134-th batch, train loss: 0.6271426677703857:  57%|██████▏    | 134/237 [01:17<01:01,  1.66it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5410131812095642:  88%|█████████▋ | 133/151 [01:18<00:04,  4.05it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5410131812095642:  89%|█████████▊ | 134/151 [01:18<00:04,  4.14it/s]Epoch: 1, train for the 126-th batch, train loss: 0.4162474274635315:  33%|███▌       | 125/383 [01:15<02:34,  1.66it/s]Epoch: 1, train for the 126-th batch, train loss: 0.4162474274635315:  33%|███▌       | 126/383 [01:15<02:34,  1.66it/s]evaluate for the 21-th batch, evaluate loss: 0.15537533164024353:  50%|████████▌        | 20/40 [00:05<00:05,  3.56it/s]evaluate for the 21-th batch, evaluate loss: 0.15537533164024353:  52%|████████▉        | 21/40 [00:05<00:05,  3.72it/s]Epoch: 1, train for the 135-th batch, train loss: 0.5396705865859985:  89%|█████████▊ | 134/151 [01:18<00:04,  4.14it/s]Epoch: 1, train for the 135-th batch, train loss: 0.5396705865859985:  89%|█████████▊ | 135/151 [01:18<00:03,  4.22it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5506001710891724:  87%|█████████▌ | 127/146 [01:18<00:11,  1.68it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5506001710891724:  88%|█████████▋ | 128/146 [01:18<00:10,  1.67it/s]evaluate for the 22-th batch, evaluate loss: 0.1887408345937729:  52%|█████████▍        | 21/40 [00:06<00:05,  3.72it/s]evaluate for the 22-th batch, evaluate loss: 0.1887408345937729:  55%|█████████▉        | 22/40 [00:06<00:05,  3.53it/s]Epoch: 1, train for the 136-th batch, train loss: 0.5731838345527649:  89%|█████████▊ | 135/151 [01:18<00:03,  4.22it/s]Epoch: 1, train for the 136-th batch, train loss: 0.5731838345527649:  90%|█████████▉ | 136/151 [01:18<00:03,  4.28it/s]Epoch: 1, train for the 135-th batch, train loss: 0.6506130695343018:  57%|██████▏    | 134/237 [01:17<01:01,  1.66it/s]Epoch: 1, train for the 135-th batch, train loss: 0.6506130695343018:  57%|██████▎    | 135/237 [01:17<01:01,  1.66it/s]Epoch: 1, train for the 137-th batch, train loss: 0.5915154218673706:  90%|█████████▉ | 136/151 [01:19<00:03,  4.28it/s]Epoch: 1, train for the 137-th batch, train loss: 0.5915154218673706:  91%|█████████▉ | 137/151 [01:19<00:03,  4.33it/s]evaluate for the 23-th batch, evaluate loss: 0.2125435769557953:  55%|█████████▉        | 22/40 [00:06<00:05,  3.53it/s]evaluate for the 23-th batch, evaluate loss: 0.2125435769557953:  57%|██████████▎       | 23/40 [00:06<00:04,  3.57it/s]Epoch: 1, train for the 127-th batch, train loss: 0.4349433183670044:  33%|███▌       | 126/383 [01:16<02:34,  1.66it/s]Epoch: 1, train for the 127-th batch, train loss: 0.4349433183670044:  33%|███▋       | 127/383 [01:16<02:34,  1.66it/s]Epoch: 1, train for the 138-th batch, train loss: 0.6059215664863586:  91%|█████████▉ | 137/151 [01:19<00:03,  4.33it/s]Epoch: 1, train for the 138-th batch, train loss: 0.6059215664863586:  91%|██████████ | 138/151 [01:19<00:02,  4.35it/s]evaluate for the 24-th batch, evaluate loss: 0.18482714891433716:  57%|█████████▊       | 23/40 [00:06<00:04,  3.57it/s]evaluate for the 24-th batch, evaluate loss: 0.18482714891433716:  60%|██████████▏      | 24/40 [00:06<00:04,  3.50it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5835298299789429:  88%|█████████▋ | 128/146 [01:19<00:10,  1.67it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5835298299789429:  88%|█████████▋ | 129/146 [01:19<00:10,  1.67it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5844471454620361:  91%|██████████ | 138/151 [01:19<00:02,  4.35it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5844471454620361:  92%|██████████▏| 139/151 [01:19<00:02,  4.38it/s]Epoch: 1, train for the 136-th batch, train loss: 0.6320670247077942:  57%|██████▎    | 135/237 [01:18<01:01,  1.66it/s]Epoch: 1, train for the 136-th batch, train loss: 0.6320670247077942:  57%|██████▎    | 136/237 [01:18<01:00,  1.66it/s]evaluate for the 25-th batch, evaluate loss: 0.20299559831619263:  60%|██████████▏      | 24/40 [00:06<00:04,  3.50it/s]evaluate for the 25-th batch, evaluate loss: 0.20299559831619263:  62%|██████████▋      | 25/40 [00:06<00:04,  3.63it/s]Epoch: 1, train for the 128-th batch, train loss: 0.494497150182724:  33%|███▉        | 127/383 [01:16<02:34,  1.66it/s]Epoch: 1, train for the 128-th batch, train loss: 0.494497150182724:  33%|████        | 128/383 [01:16<02:33,  1.66it/s]Epoch: 1, train for the 140-th batch, train loss: 0.5303475260734558:  92%|██████████▏| 139/151 [01:19<00:02,  4.38it/s]Epoch: 1, train for the 140-th batch, train loss: 0.5303475260734558:  93%|██████████▏| 140/151 [01:19<00:02,  4.40it/s]evaluate for the 26-th batch, evaluate loss: 0.18712596595287323:  62%|██████████▋      | 25/40 [00:07<00:04,  3.63it/s]evaluate for the 26-th batch, evaluate loss: 0.18712596595287323:  65%|███████████      | 26/40 [00:07<00:03,  3.54it/s]Epoch: 1, train for the 141-th batch, train loss: 0.5243234634399414:  93%|██████████▏| 140/151 [01:20<00:02,  4.40it/s]Epoch: 1, train for the 141-th batch, train loss: 0.5243234634399414:  93%|██████████▎| 141/151 [01:20<00:02,  4.42it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5165437459945679:  88%|█████████▋ | 129/146 [01:20<00:10,  1.67it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5165437459945679:  89%|█████████▊ | 130/146 [01:20<00:09,  1.68it/s]Epoch: 1, train for the 137-th batch, train loss: 0.6547006368637085:  57%|██████▎    | 136/237 [01:18<01:00,  1.66it/s]Epoch: 1, train for the 137-th batch, train loss: 0.6547006368637085:  58%|██████▎    | 137/237 [01:18<01:00,  1.66it/s]Epoch: 1, train for the 142-th batch, train loss: 0.5401747226715088:  93%|██████████▎| 141/151 [01:20<00:02,  4.42it/s]Epoch: 1, train for the 142-th batch, train loss: 0.5401747226715088:  94%|██████████▎| 142/151 [01:20<00:02,  4.40it/s]evaluate for the 27-th batch, evaluate loss: 0.1828160583972931:  65%|███████████▋      | 26/40 [00:07<00:03,  3.54it/s]evaluate for the 27-th batch, evaluate loss: 0.1828160583972931:  68%|████████████▏     | 27/40 [00:07<00:03,  3.64it/s]Epoch: 1, train for the 129-th batch, train loss: 0.4200620949268341:  33%|███▋       | 128/383 [01:17<02:33,  1.66it/s]Epoch: 1, train for the 129-th batch, train loss: 0.4200620949268341:  34%|███▋       | 129/383 [01:17<02:32,  1.66it/s]Epoch: 1, train for the 143-th batch, train loss: 0.4912644326686859:  94%|██████████▎| 142/151 [01:20<00:02,  4.40it/s]Epoch: 1, train for the 143-th batch, train loss: 0.4912644326686859:  95%|██████████▍| 143/151 [01:20<00:01,  4.40it/s]evaluate for the 28-th batch, evaluate loss: 0.16125456988811493:  68%|███████████▍     | 27/40 [00:07<00:03,  3.64it/s]evaluate for the 28-th batch, evaluate loss: 0.16125456988811493:  70%|███████████▉     | 28/40 [00:07<00:03,  3.58it/s]Epoch: 1, train for the 131-th batch, train loss: 0.5357468128204346:  89%|█████████▊ | 130/146 [01:20<00:09,  1.68it/s]Epoch: 1, train for the 131-th batch, train loss: 0.5357468128204346:  90%|█████████▊ | 131/146 [01:20<00:08,  1.68it/s]Epoch: 1, train for the 144-th batch, train loss: 0.5005123019218445:  95%|██████████▍| 143/151 [01:20<00:01,  4.40it/s]Epoch: 1, train for the 144-th batch, train loss: 0.5005123019218445:  95%|██████████▍| 144/151 [01:20<00:01,  4.41it/s]evaluate for the 29-th batch, evaluate loss: 0.18699871003627777:  70%|███████████▉     | 28/40 [00:08<00:03,  3.58it/s]evaluate for the 29-th batch, evaluate loss: 0.18699871003627777:  72%|████████████▎    | 29/40 [00:08<00:03,  3.66it/s]Epoch: 1, train for the 138-th batch, train loss: 0.6737273931503296:  58%|██████▎    | 137/237 [01:19<01:00,  1.66it/s]Epoch: 1, train for the 138-th batch, train loss: 0.6737273931503296:  58%|██████▍    | 138/237 [01:19<00:59,  1.66it/s]Epoch: 1, train for the 145-th batch, train loss: 0.52882981300354:  95%|████████████▍| 144/151 [01:21<00:01,  4.41it/s]Epoch: 1, train for the 145-th batch, train loss: 0.52882981300354:  96%|████████████▍| 145/151 [01:21<00:01,  4.40it/s]Epoch: 1, train for the 130-th batch, train loss: 0.4046138525009155:  34%|███▋       | 129/383 [01:17<02:32,  1.66it/s]Epoch: 1, train for the 130-th batch, train loss: 0.4046138525009155:  34%|███▋       | 130/383 [01:17<02:31,  1.66it/s]evaluate for the 30-th batch, evaluate loss: 0.196762353181839:  72%|█████████████▊     | 29/40 [00:08<00:03,  3.66it/s]evaluate for the 30-th batch, evaluate loss: 0.196762353181839:  75%|██████████████▎    | 30/40 [00:08<00:02,  3.61it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5268933773040771:  96%|██████████▌| 145/151 [01:21<00:01,  4.40it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5268933773040771:  97%|██████████▋| 146/151 [01:21<00:01,  4.41it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5449103116989136:  90%|█████████▊ | 131/146 [01:21<00:08,  1.68it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5449103116989136:  90%|█████████▉ | 132/146 [01:21<00:08,  1.68it/s]Epoch: 1, train for the 147-th batch, train loss: 0.5602653622627258:  97%|██████████▋| 146/151 [01:21<00:01,  4.41it/s]Epoch: 1, train for the 147-th batch, train loss: 0.5602653622627258:  97%|██████████▋| 147/151 [01:21<00:00,  4.41it/s]evaluate for the 31-th batch, evaluate loss: 0.20767474174499512:  75%|████████████▊    | 30/40 [00:08<00:02,  3.61it/s]evaluate for the 31-th batch, evaluate loss: 0.20767474174499512:  78%|█████████████▏   | 31/40 [00:08<00:02,  3.54it/s]Epoch: 1, train for the 139-th batch, train loss: 0.6393333077430725:  58%|██████▍    | 138/237 [01:20<00:59,  1.66it/s]Epoch: 1, train for the 139-th batch, train loss: 0.6393333077430725:  59%|██████▍    | 139/237 [01:20<00:58,  1.67it/s]Epoch: 1, train for the 131-th batch, train loss: 0.4506886303424835:  34%|███▋       | 130/383 [01:18<02:31,  1.66it/s]Epoch: 1, train for the 131-th batch, train loss: 0.4506886303424835:  34%|███▊       | 131/383 [01:18<02:31,  1.67it/s]Epoch: 1, train for the 148-th batch, train loss: 0.5739735960960388:  97%|██████████▋| 147/151 [01:21<00:00,  4.41it/s]Epoch: 1, train for the 148-th batch, train loss: 0.5739735960960388:  98%|██████████▊| 148/151 [01:21<00:00,  4.40it/s]evaluate for the 32-th batch, evaluate loss: 0.18440021574497223:  78%|█████████████▏   | 31/40 [00:08<00:02,  3.54it/s]evaluate for the 32-th batch, evaluate loss: 0.18440021574497223:  80%|█████████████▌   | 32/40 [00:08<00:02,  3.58it/s]Epoch: 1, train for the 149-th batch, train loss: 0.5041552782058716:  98%|██████████▊| 148/151 [01:21<00:00,  4.40it/s]Epoch: 1, train for the 149-th batch, train loss: 0.5041552782058716:  99%|██████████▊| 149/151 [01:21<00:00,  4.41it/s]Epoch: 1, train for the 133-th batch, train loss: 0.5004269480705261:  90%|█████████▉ | 132/146 [01:21<00:08,  1.68it/s]Epoch: 1, train for the 133-th batch, train loss: 0.5004269480705261:  91%|██████████ | 133/146 [01:21<00:07,  1.68it/s]evaluate for the 33-th batch, evaluate loss: 0.16947399079799652:  80%|█████████████▌   | 32/40 [00:09<00:02,  3.58it/s]evaluate for the 33-th batch, evaluate loss: 0.16947399079799652:  82%|██████████████   | 33/40 [00:09<00:01,  3.51it/s]Epoch: 1, train for the 140-th batch, train loss: 0.6419444680213928:  59%|██████▍    | 139/237 [01:20<00:58,  1.67it/s]Epoch: 1, train for the 140-th batch, train loss: 0.6419444680213928:  59%|██████▍    | 140/237 [01:20<00:58,  1.66it/s]Epoch: 1, train for the 150-th batch, train loss: 0.536835789680481:  99%|███████████▊| 149/151 [01:22<00:00,  4.41it/s]Epoch: 1, train for the 150-th batch, train loss: 0.536835789680481:  99%|███████████▉| 150/151 [01:22<00:00,  4.41it/s]Epoch: 1, train for the 132-th batch, train loss: 0.455952912569046:  34%|████        | 131/383 [01:18<02:31,  1.67it/s]Epoch: 1, train for the 132-th batch, train loss: 0.455952912569046:  34%|████▏       | 132/383 [01:18<02:30,  1.67it/s]evaluate for the 34-th batch, evaluate loss: 0.1588951200246811:  82%|██████████████▊   | 33/40 [00:09<00:01,  3.51it/s]evaluate for the 34-th batch, evaluate loss: 0.1588951200246811:  85%|███████████████▎  | 34/40 [00:09<00:01,  3.67it/s]Epoch: 1, train for the 151-th batch, train loss: 0.6129629611968994:  99%|██████████▉| 150/151 [01:22<00:00,  4.41it/s]Epoch: 1, train for the 151-th batch, train loss: 0.6129629611968994: 100%|███████████| 151/151 [01:22<00:00,  4.87it/s]Epoch: 1, train for the 151-th batch, train loss: 0.6129629611968994: 100%|███████████| 151/151 [01:22<00:00,  1.83it/s]
  0%|                                                                                            | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.49275168776512146:   0%|                           | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.49275168776512146:   2%|▍                  | 1/46 [00:00<00:04,  9.44it/s]evaluate for the 2-th batch, evaluate loss: 0.5076330304145813:   2%|▍                   | 1/46 [00:00<00:04,  9.44it/s]evaluate for the 2-th batch, evaluate loss: 0.5076330304145813:   4%|▊                   | 2/46 [00:00<00:04,  9.44it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5300238728523254:  91%|██████████ | 133/146 [01:22<00:07,  1.68it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5300238728523254:  92%|██████████ | 134/146 [01:22<00:07,  1.68it/s]evaluate for the 35-th batch, evaluate loss: 0.19845201075077057:  85%|██████████████▍  | 34/40 [00:09<00:01,  3.67it/s]evaluate for the 35-th batch, evaluate loss: 0.19845201075077057:  88%|██████████████▉  | 35/40 [00:09<00:01,  3.56it/s]evaluate for the 3-th batch, evaluate loss: 0.4764288663864136:   4%|▊                   | 2/46 [00:00<00:04,  9.44it/s]evaluate for the 3-th batch, evaluate loss: 0.4764288663864136:   7%|█▎                  | 3/46 [00:00<00:04,  9.45it/s]Epoch: 1, train for the 141-th batch, train loss: 0.6453839540481567:  59%|██████▍    | 140/237 [01:21<00:58,  1.66it/s]Epoch: 1, train for the 141-th batch, train loss: 0.6453839540481567:  59%|██████▌    | 141/237 [01:21<00:57,  1.67it/s]evaluate for the 4-th batch, evaluate loss: 0.5191822648048401:   7%|█▎                  | 3/46 [00:00<00:04,  9.45it/s]evaluate for the 4-th batch, evaluate loss: 0.5191822648048401:   9%|█▋                  | 4/46 [00:00<00:04,  9.47it/s]evaluate for the 5-th batch, evaluate loss: 0.4709181785583496:   9%|█▋                  | 4/46 [00:00<00:04,  9.47it/s]evaluate for the 5-th batch, evaluate loss: 0.4709181785583496:  11%|██▏                 | 5/46 [00:00<00:04,  9.51it/s]evaluate for the 36-th batch, evaluate loss: 0.2036421000957489:  88%|███████████████▊  | 35/40 [00:09<00:01,  3.56it/s]evaluate for the 36-th batch, evaluate loss: 0.2036421000957489:  90%|████████████████▏ | 36/40 [00:09<00:01,  3.73it/s]Epoch: 1, train for the 133-th batch, train loss: 0.5087316632270813:  34%|███▊       | 132/383 [01:19<02:30,  1.67it/s]Epoch: 1, train for the 133-th batch, train loss: 0.5087316632270813:  35%|███▊       | 133/383 [01:19<02:29,  1.67it/s]evaluate for the 6-th batch, evaluate loss: 0.5563258528709412:  11%|██▏                 | 5/46 [00:00<00:04,  9.51it/s]evaluate for the 6-th batch, evaluate loss: 0.5563258528709412:  13%|██▌                 | 6/46 [00:00<00:04,  9.52it/s]evaluate for the 7-th batch, evaluate loss: 0.5107248425483704:  13%|██▌                 | 6/46 [00:00<00:04,  9.52it/s]evaluate for the 7-th batch, evaluate loss: 0.5107248425483704:  15%|███                 | 7/46 [00:00<00:04,  9.55it/s]Epoch: 1, train for the 135-th batch, train loss: 0.5555877089500427:  92%|██████████ | 134/146 [01:23<00:07,  1.68it/s]Epoch: 1, train for the 135-th batch, train loss: 0.5555877089500427:  92%|██████████▏| 135/146 [01:23<00:06,  1.67it/s]evaluate for the 8-th batch, evaluate loss: 0.5677052736282349:  15%|███                 | 7/46 [00:00<00:04,  9.55it/s]evaluate for the 8-th batch, evaluate loss: 0.5677052736282349:  17%|███▍                | 8/46 [00:00<00:03,  9.55it/s]evaluate for the 37-th batch, evaluate loss: 0.2454044222831726:  90%|████████████████▏ | 36/40 [00:10<00:01,  3.73it/s]evaluate for the 37-th batch, evaluate loss: 0.2454044222831726:  92%|████████████████▋ | 37/40 [00:10<00:00,  3.54it/s]evaluate for the 9-th batch, evaluate loss: 0.5174939632415771:  17%|███▍                | 8/46 [00:00<00:03,  9.55it/s]evaluate for the 9-th batch, evaluate loss: 0.5174939632415771:  20%|███▉                | 9/46 [00:00<00:03,  9.54it/s]Epoch: 1, train for the 142-th batch, train loss: 0.6564537882804871:  59%|██████▌    | 141/237 [01:21<00:57,  1.67it/s]Epoch: 1, train for the 142-th batch, train loss: 0.6564537882804871:  60%|██████▌    | 142/237 [01:21<00:56,  1.68it/s]evaluate for the 10-th batch, evaluate loss: 0.5346540808677673:  20%|███▋               | 9/46 [00:01<00:03,  9.54it/s]evaluate for the 10-th batch, evaluate loss: 0.5346540808677673:  22%|███▉              | 10/46 [00:01<00:03,  9.54it/s]evaluate for the 38-th batch, evaluate loss: 0.21066194772720337:  92%|███████████████▋ | 37/40 [00:10<00:00,  3.54it/s]evaluate for the 38-th batch, evaluate loss: 0.21066194772720337:  95%|████████████████▏| 38/40 [00:10<00:00,  3.59it/s]Epoch: 1, train for the 134-th batch, train loss: 0.4854261577129364:  35%|███▊       | 133/383 [01:20<02:29,  1.67it/s]Epoch: 1, train for the 134-th batch, train loss: 0.4854261577129364:  35%|███▊       | 134/383 [01:20<02:28,  1.67it/s]evaluate for the 11-th batch, evaluate loss: 0.5306474566459656:  22%|███▉              | 10/46 [00:01<00:03,  9.54it/s]evaluate for the 11-th batch, evaluate loss: 0.5306474566459656:  24%|████▎             | 11/46 [00:01<00:03,  9.57it/s]evaluate for the 12-th batch, evaluate loss: 0.46654272079467773:  24%|████             | 11/46 [00:01<00:03,  9.57it/s]evaluate for the 12-th batch, evaluate loss: 0.46654272079467773:  26%|████▍            | 12/46 [00:01<00:03,  9.56it/s]evaluate for the 13-th batch, evaluate loss: 0.5002059936523438:  26%|████▋             | 12/46 [00:01<00:03,  9.56it/s]evaluate for the 13-th batch, evaluate loss: 0.5002059936523438:  28%|█████             | 13/46 [00:01<00:03,  9.56it/s]evaluate for the 39-th batch, evaluate loss: 0.21295452117919922:  95%|████████████████▏| 38/40 [00:10<00:00,  3.59it/s]evaluate for the 39-th batch, evaluate loss: 0.21295452117919922:  98%|████████████████▌| 39/40 [00:10<00:00,  3.53it/s]Epoch: 1, train for the 136-th batch, train loss: 0.5533995032310486:  92%|██████████▏| 135/146 [01:23<00:06,  1.67it/s]Epoch: 1, train for the 136-th batch, train loss: 0.5533995032310486:  93%|██████████▏| 136/146 [01:23<00:05,  1.67it/s]evaluate for the 40-th batch, evaluate loss: 0.0838630273938179:  98%|█████████████████▌| 39/40 [00:10<00:00,  3.53it/s]evaluate for the 40-th batch, evaluate loss: 0.0838630273938179: 100%|██████████████████| 40/40 [00:10<00:00,  3.67it/s]
  0%|                                                                                            | 0/21 [00:00<?, ?it/s]evaluate for the 14-th batch, evaluate loss: 0.5900517106056213:  28%|█████             | 13/46 [00:01<00:03,  9.56it/s]evaluate for the 14-th batch, evaluate loss: 0.5900517106056213:  30%|█████▍            | 14/46 [00:01<00:03,  9.56it/s]evaluate for the 15-th batch, evaluate loss: 0.5476530194282532:  30%|█████▍            | 14/46 [00:01<00:03,  9.56it/s]evaluate for the 15-th batch, evaluate loss: 0.5476530194282532:  33%|█████▊            | 15/46 [00:01<00:03,  9.57it/s]Epoch: 1, train for the 143-th batch, train loss: 0.6343896389007568:  60%|██████▌    | 142/237 [01:22<00:56,  1.68it/s]Epoch: 1, train for the 143-th batch, train loss: 0.6343896389007568:  60%|██████▋    | 143/237 [01:22<00:56,  1.68it/s]evaluate for the 16-th batch, evaluate loss: 0.5764417052268982:  33%|█████▊            | 15/46 [00:01<00:03,  9.57it/s]evaluate for the 16-th batch, evaluate loss: 0.5764417052268982:  35%|██████▎           | 16/46 [00:01<00:03,  9.59it/s]evaluate for the 1-th batch, evaluate loss: 0.2749212086200714:   0%|                            | 0/21 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.2749212086200714:   5%|▉                   | 1/21 [00:00<00:05,  3.57it/s]Epoch: 1, train for the 135-th batch, train loss: 0.44822296500205994:  35%|███▍      | 134/383 [01:20<02:28,  1.67it/s]Epoch: 1, train for the 135-th batch, train loss: 0.44822296500205994:  35%|███▌      | 135/383 [01:20<02:28,  1.68it/s]evaluate for the 17-th batch, evaluate loss: 0.45998626947402954:  35%|█████▉           | 16/46 [00:01<00:03,  9.59it/s]evaluate for the 17-th batch, evaluate loss: 0.45998626947402954:  37%|██████▎          | 17/46 [00:01<00:03,  9.60it/s]evaluate for the 18-th batch, evaluate loss: 0.5087555646896362:  37%|██████▋           | 17/46 [00:01<00:03,  9.60it/s]evaluate for the 18-th batch, evaluate loss: 0.5087555646896362:  39%|███████           | 18/46 [00:01<00:02,  9.59it/s]evaluate for the 19-th batch, evaluate loss: 0.5253972411155701:  39%|███████           | 18/46 [00:01<00:02,  9.59it/s]evaluate for the 19-th batch, evaluate loss: 0.5253972411155701:  41%|███████▍          | 19/46 [00:01<00:02,  9.59it/s]Epoch: 1, train for the 137-th batch, train loss: 0.5507199764251709:  93%|██████████▏| 136/146 [01:24<00:05,  1.67it/s]Epoch: 1, train for the 137-th batch, train loss: 0.5507199764251709:  94%|██████████▎| 137/146 [01:24<00:05,  1.67it/s]evaluate for the 2-th batch, evaluate loss: 0.32997405529022217:   5%|▉                  | 1/21 [00:00<00:05,  3.57it/s]evaluate for the 2-th batch, evaluate loss: 0.32997405529022217:  10%|█▊                 | 2/21 [00:00<00:05,  3.32it/s]evaluate for the 20-th batch, evaluate loss: 0.5352047681808472:  41%|███████▍          | 19/46 [00:02<00:02,  9.59it/s]evaluate for the 20-th batch, evaluate loss: 0.5352047681808472:  43%|███████▊          | 20/46 [00:02<00:02,  9.59it/s]Epoch: 1, train for the 144-th batch, train loss: 0.652765154838562:  60%|███████▏    | 143/237 [01:23<00:56,  1.68it/s]Epoch: 1, train for the 144-th batch, train loss: 0.652765154838562:  61%|███████▎    | 144/237 [01:23<00:55,  1.68it/s]evaluate for the 21-th batch, evaluate loss: 0.5267655253410339:  43%|███████▊          | 20/46 [00:02<00:02,  9.59it/s]evaluate for the 21-th batch, evaluate loss: 0.5267655253410339:  46%|████████▏         | 21/46 [00:02<00:02,  9.59it/s]evaluate for the 22-th batch, evaluate loss: 0.5217025876045227:  46%|████████▏         | 21/46 [00:02<00:02,  9.59it/s]evaluate for the 22-th batch, evaluate loss: 0.5217025876045227:  48%|████████▌         | 22/46 [00:02<00:02,  9.58it/s]evaluate for the 3-th batch, evaluate loss: 0.2933013141155243:  10%|█▉                  | 2/21 [00:00<00:05,  3.32it/s]evaluate for the 3-th batch, evaluate loss: 0.2933013141155243:  14%|██▊                 | 3/21 [00:00<00:05,  3.45it/s]Epoch: 1, train for the 136-th batch, train loss: 0.47166451811790466:  35%|███▌      | 135/383 [01:21<02:28,  1.68it/s]Epoch: 1, train for the 136-th batch, train loss: 0.47166451811790466:  36%|███▌      | 136/383 [01:21<02:26,  1.69it/s]evaluate for the 23-th batch, evaluate loss: 0.4728753864765167:  48%|████████▌         | 22/46 [00:02<00:02,  9.58it/s]evaluate for the 23-th batch, evaluate loss: 0.4728753864765167:  50%|█████████         | 23/46 [00:02<00:02,  9.61it/s]evaluate for the 24-th batch, evaluate loss: 0.4872010350227356:  50%|█████████         | 23/46 [00:02<00:02,  9.61it/s]evaluate for the 24-th batch, evaluate loss: 0.4872010350227356:  52%|█████████▍        | 24/46 [00:02<00:02,  9.62it/s]evaluate for the 25-th batch, evaluate loss: 0.5347689986228943:  52%|█████████▍        | 24/46 [00:02<00:02,  9.62it/s]evaluate for the 25-th batch, evaluate loss: 0.5347689986228943:  54%|█████████▊        | 25/46 [00:02<00:02,  9.61it/s]evaluate for the 4-th batch, evaluate loss: 0.24044978618621826:  14%|██▋                | 3/21 [00:01<00:05,  3.45it/s]evaluate for the 4-th batch, evaluate loss: 0.24044978618621826:  19%|███▌               | 4/21 [00:01<00:05,  3.36it/s]Epoch: 1, train for the 138-th batch, train loss: 0.5770393013954163:  94%|██████████▎| 137/146 [01:24<00:05,  1.67it/s]Epoch: 1, train for the 138-th batch, train loss: 0.5770393013954163:  95%|██████████▍| 138/146 [01:24<00:04,  1.68it/s]evaluate for the 26-th batch, evaluate loss: 0.5669251084327698:  54%|█████████▊        | 25/46 [00:02<00:02,  9.61it/s]evaluate for the 26-th batch, evaluate loss: 0.5669251084327698:  57%|██████████▏       | 26/46 [00:02<00:02,  9.60it/s]Epoch: 1, train for the 145-th batch, train loss: 0.6608322858810425:  61%|██████▋    | 144/237 [01:23<00:55,  1.68it/s]Epoch: 1, train for the 145-th batch, train loss: 0.6608322858810425:  61%|██████▋    | 145/237 [01:23<00:54,  1.68it/s]evaluate for the 27-th batch, evaluate loss: 0.49612563848495483:  57%|█████████▌       | 26/46 [00:02<00:02,  9.60it/s]evaluate for the 27-th batch, evaluate loss: 0.49612563848495483:  59%|█████████▉       | 27/46 [00:02<00:01,  9.62it/s]evaluate for the 5-th batch, evaluate loss: 0.2886250615119934:  19%|███▊                | 4/21 [00:01<00:05,  3.36it/s]evaluate for the 5-th batch, evaluate loss: 0.2886250615119934:  24%|████▊               | 5/21 [00:01<00:04,  3.50it/s]Epoch: 1, train for the 137-th batch, train loss: 0.42654937505722046:  36%|███▌      | 136/383 [01:21<02:26,  1.69it/s]Epoch: 1, train for the 137-th batch, train loss: 0.42654937505722046:  36%|███▌      | 137/383 [01:21<02:25,  1.69it/s]evaluate for the 28-th batch, evaluate loss: 0.5274967551231384:  59%|██████████▌       | 27/46 [00:02<00:01,  9.62it/s]evaluate for the 28-th batch, evaluate loss: 0.5274967551231384:  61%|██████████▉       | 28/46 [00:02<00:01,  9.64it/s]evaluate for the 29-th batch, evaluate loss: 0.4907315969467163:  61%|██████████▉       | 28/46 [00:03<00:01,  9.64it/s]evaluate for the 29-th batch, evaluate loss: 0.4907315969467163:  63%|███████████▎      | 29/46 [00:03<00:01,  9.59it/s]evaluate for the 30-th batch, evaluate loss: 0.5016329884529114:  63%|███████████▎      | 29/46 [00:03<00:01,  9.59it/s]evaluate for the 30-th batch, evaluate loss: 0.5016329884529114:  65%|███████████▋      | 30/46 [00:03<00:01,  9.53it/s]evaluate for the 6-th batch, evaluate loss: 0.3202756345272064:  24%|████▊               | 5/21 [00:01<00:04,  3.50it/s]evaluate for the 6-th batch, evaluate loss: 0.3202756345272064:  29%|█████▋              | 6/21 [00:01<00:04,  3.42it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5791646242141724:  95%|██████████▍| 138/146 [01:25<00:04,  1.68it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5791646242141724:  95%|██████████▍| 139/146 [01:25<00:04,  1.70it/s]evaluate for the 31-th batch, evaluate loss: 0.5188230276107788:  65%|███████████▋      | 30/46 [00:03<00:01,  9.53it/s]evaluate for the 31-th batch, evaluate loss: 0.5188230276107788:  67%|████████████▏     | 31/46 [00:03<00:01,  9.51it/s]evaluate for the 32-th batch, evaluate loss: 0.47778403759002686:  67%|███████████▍     | 31/46 [00:03<00:01,  9.51it/s]evaluate for the 32-th batch, evaluate loss: 0.47778403759002686:  70%|███████████▊     | 32/46 [00:03<00:01,  9.49it/s]Epoch: 1, train for the 146-th batch, train loss: 0.6559495329856873:  61%|██████▋    | 145/237 [01:24<00:54,  1.68it/s]Epoch: 1, train for the 146-th batch, train loss: 0.6559495329856873:  62%|██████▊    | 146/237 [01:24<00:54,  1.68it/s]evaluate for the 7-th batch, evaluate loss: 0.26606279611587524:  29%|█████▍             | 6/21 [00:02<00:04,  3.42it/s]evaluate for the 7-th batch, evaluate loss: 0.26606279611587524:  33%|██████▎            | 7/21 [00:02<00:03,  3.56it/s]evaluate for the 33-th batch, evaluate loss: 0.5012091398239136:  70%|████████████▌     | 32/46 [00:03<00:01,  9.49it/s]evaluate for the 33-th batch, evaluate loss: 0.5012091398239136:  72%|████████████▉     | 33/46 [00:03<00:01,  9.53it/s]Epoch: 1, train for the 138-th batch, train loss: 0.34799924492836:  36%|████▋        | 137/383 [01:22<02:25,  1.69it/s]Epoch: 1, train for the 138-th batch, train loss: 0.34799924492836:  36%|████▋        | 138/383 [01:22<02:25,  1.68it/s]evaluate for the 34-th batch, evaluate loss: 0.4925791919231415:  72%|████████████▉     | 33/46 [00:03<00:01,  9.53it/s]evaluate for the 34-th batch, evaluate loss: 0.4925791919231415:  74%|█████████████▎    | 34/46 [00:03<00:01,  9.55it/s]evaluate for the 35-th batch, evaluate loss: 0.4955904185771942:  74%|█████████████▎    | 34/46 [00:03<00:01,  9.55it/s]evaluate for the 35-th batch, evaluate loss: 0.4955904185771942:  76%|█████████████▋    | 35/46 [00:03<00:01,  9.57it/s]evaluate for the 8-th batch, evaluate loss: 0.34015846252441406:  33%|██████▎            | 7/21 [00:02<00:03,  3.56it/s]evaluate for the 8-th batch, evaluate loss: 0.34015846252441406:  38%|███████▏           | 8/21 [00:02<00:03,  3.50it/s]evaluate for the 36-th batch, evaluate loss: 0.47825437784194946:  76%|████████████▉    | 35/46 [00:03<00:01,  9.57it/s]evaluate for the 36-th batch, evaluate loss: 0.47825437784194946:  78%|█████████████▎   | 36/46 [00:03<00:01,  9.58it/s]Epoch: 1, train for the 140-th batch, train loss: 0.556178092956543:  95%|███████████▍| 139/146 [01:26<00:04,  1.70it/s]Epoch: 1, train for the 140-th batch, train loss: 0.556178092956543:  96%|███████████▌| 140/146 [01:26<00:03,  1.71it/s]evaluate for the 37-th batch, evaluate loss: 0.5155767202377319:  78%|██████████████    | 36/46 [00:03<00:01,  9.58it/s]evaluate for the 37-th batch, evaluate loss: 0.5155767202377319:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.60it/s]Epoch: 1, train for the 147-th batch, train loss: 0.6538119316101074:  62%|██████▊    | 146/237 [01:24<00:54,  1.68it/s]Epoch: 1, train for the 147-th batch, train loss: 0.6538119316101074:  62%|██████▊    | 147/237 [01:24<00:53,  1.68it/s]evaluate for the 38-th batch, evaluate loss: 0.5429049730300903:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.60it/s]evaluate for the 38-th batch, evaluate loss: 0.5429049730300903:  83%|██████████████▊   | 38/46 [00:03<00:00,  9.55it/s]evaluate for the 9-th batch, evaluate loss: 0.29134923219680786:  38%|███████▏           | 8/21 [00:02<00:03,  3.50it/s]evaluate for the 9-th batch, evaluate loss: 0.29134923219680786:  43%|████████▏          | 9/21 [00:02<00:03,  3.61it/s]evaluate for the 39-th batch, evaluate loss: 0.5389946699142456:  83%|██████████████▊   | 38/46 [00:04<00:00,  9.55it/s]evaluate for the 39-th batch, evaluate loss: 0.5389946699142456:  85%|███████████████▎  | 39/46 [00:04<00:00,  9.58it/s]Epoch: 1, train for the 139-th batch, train loss: 0.45661795139312744:  36%|███▌      | 138/383 [01:23<02:25,  1.68it/s]Epoch: 1, train for the 139-th batch, train loss: 0.45661795139312744:  36%|███▋      | 139/383 [01:23<02:24,  1.68it/s]evaluate for the 40-th batch, evaluate loss: 0.4772915542125702:  85%|███████████████▎  | 39/46 [00:04<00:00,  9.58it/s]evaluate for the 40-th batch, evaluate loss: 0.4772915542125702:  87%|███████████████▋  | 40/46 [00:04<00:00,  9.62it/s]evaluate for the 41-th batch, evaluate loss: 0.4924500584602356:  87%|███████████████▋  | 40/46 [00:04<00:00,  9.62it/s]evaluate for the 41-th batch, evaluate loss: 0.4924500584602356:  89%|████████████████  | 41/46 [00:04<00:00,  9.63it/s]evaluate for the 10-th batch, evaluate loss: 0.2963724136352539:  43%|████████▏          | 9/21 [00:02<00:03,  3.61it/s]evaluate for the 10-th batch, evaluate loss: 0.2963724136352539:  48%|████████▌         | 10/21 [00:02<00:03,  3.53it/s]Epoch: 1, train for the 141-th batch, train loss: 0.5662879347801208:  96%|██████████▌| 140/146 [01:26<00:03,  1.71it/s]Epoch: 1, train for the 141-th batch, train loss: 0.5662879347801208:  97%|██████████▌| 141/146 [01:26<00:02,  1.72it/s]evaluate for the 42-th batch, evaluate loss: 0.4591626524925232:  89%|████████████████  | 41/46 [00:04<00:00,  9.63it/s]evaluate for the 42-th batch, evaluate loss: 0.4591626524925232:  91%|████████████████▍ | 42/46 [00:04<00:00,  9.63it/s]evaluate for the 43-th batch, evaluate loss: 0.5373338460922241:  91%|████████████████▍ | 42/46 [00:04<00:00,  9.63it/s]evaluate for the 43-th batch, evaluate loss: 0.5373338460922241:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.63it/s]Epoch: 1, train for the 148-th batch, train loss: 0.6240065693855286:  62%|██████▊    | 147/237 [01:25<00:53,  1.68it/s]Epoch: 1, train for the 148-th batch, train loss: 0.6240065693855286:  62%|██████▊    | 148/237 [01:25<00:52,  1.69it/s]evaluate for the 11-th batch, evaluate loss: 0.22997792065143585:  48%|████████         | 10/21 [00:03<00:03,  3.53it/s]evaluate for the 11-th batch, evaluate loss: 0.22997792065143585:  52%|████████▉        | 11/21 [00:03<00:02,  3.63it/s]evaluate for the 44-th batch, evaluate loss: 0.513821542263031:  93%|█████████████████▊ | 43/46 [00:04<00:00,  9.63it/s]evaluate for the 44-th batch, evaluate loss: 0.513821542263031:  96%|██████████████████▏| 44/46 [00:04<00:00,  9.63it/s]Epoch: 1, train for the 140-th batch, train loss: 0.4413570165634155:  36%|███▉       | 139/383 [01:23<02:24,  1.68it/s]Epoch: 1, train for the 140-th batch, train loss: 0.4413570165634155:  37%|████       | 140/383 [01:23<02:24,  1.68it/s]evaluate for the 45-th batch, evaluate loss: 0.5093238949775696:  96%|█████████████████▏| 44/46 [00:04<00:00,  9.63it/s]evaluate for the 45-th batch, evaluate loss: 0.5093238949775696:  98%|█████████████████▌| 45/46 [00:04<00:00,  9.66it/s]evaluate for the 46-th batch, evaluate loss: 0.5011444687843323:  98%|█████████████████▌| 45/46 [00:04<00:00,  9.66it/s]evaluate for the 46-th batch, evaluate loss: 0.5011444687843323: 100%|██████████████████| 46/46 [00:04<00:00,  9.61it/s]
  0%|                                                                                            | 0/25 [00:00<?, ?it/s]evaluate for the 12-th batch, evaluate loss: 0.324126273393631:  52%|█████████▉         | 11/21 [00:03<00:02,  3.63it/s]evaluate for the 12-th batch, evaluate loss: 0.324126273393631:  57%|██████████▊        | 12/21 [00:03<00:02,  3.55it/s]wandb: 🚀 View run dygformer-ia-movielens-user2tags-10m-old at: https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib/runs/q7vkjtqm
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240407_154345-q7vkjtqm/logs
evaluate for the 1-th batch, evaluate loss: 0.6620042324066162:   0%|                            | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6620042324066162:   4%|▊                   | 1/25 [00:00<00:02,  9.25it/s]Epoch: 1, train for the 142-th batch, train loss: 0.5160008668899536:  97%|██████████▌| 141/146 [01:27<00:02,  1.72it/s]Epoch: 1, train for the 142-th batch, train loss: 0.5160008668899536:  97%|██████████▋| 142/146 [01:27<00:02,  1.72it/s]evaluate for the 2-th batch, evaluate loss: 0.6788581013679504:   4%|▊                   | 1/25 [00:00<00:02,  9.25it/s]evaluate for the 2-th batch, evaluate loss: 0.6788581013679504:   8%|█▌                  | 2/25 [00:00<00:02,  9.20it/s]evaluate for the 13-th batch, evaluate loss: 0.28803595900535583:  57%|█████████▋       | 12/21 [00:03<00:02,  3.55it/s]evaluate for the 13-th batch, evaluate loss: 0.28803595900535583:  62%|██████████▌      | 13/21 [00:03<00:02,  3.64it/s]evaluate for the 3-th batch, evaluate loss: 0.7297466993331909:   8%|█▌                  | 2/25 [00:00<00:02,  9.20it/s]evaluate for the 3-th batch, evaluate loss: 0.7297466993331909:  12%|██▍                 | 3/25 [00:00<00:02,  9.22it/s]Epoch: 1, train for the 149-th batch, train loss: 0.6374567747116089:  62%|██████▊    | 148/237 [01:26<00:52,  1.69it/s]Epoch: 1, train for the 149-th batch, train loss: 0.6374567747116089:  63%|██████▉    | 149/237 [01:26<00:52,  1.68it/s]evaluate for the 4-th batch, evaluate loss: 0.7002291083335876:  12%|██▍                 | 3/25 [00:00<00:02,  9.22it/s]evaluate for the 4-th batch, evaluate loss: 0.7002291083335876:  16%|███▏                | 4/25 [00:00<00:02,  9.23it/s]Epoch: 1, train for the 141-th batch, train loss: 0.47169211506843567:  37%|███▋      | 140/383 [01:24<02:24,  1.68it/s]Epoch: 1, train for the 141-th batch, train loss: 0.47169211506843567:  37%|███▋      | 141/383 [01:24<02:24,  1.68it/s]evaluate for the 5-th batch, evaluate loss: 0.7034460306167603:  16%|███▏                | 4/25 [00:00<00:02,  9.23it/s]evaluate for the 5-th batch, evaluate loss: 0.7034460306167603:  20%|████                | 5/25 [00:00<00:02,  9.22it/s]evaluate for the 14-th batch, evaluate loss: 0.26426878571510315:  62%|██████████▌      | 13/21 [00:03<00:02,  3.64it/s]evaluate for the 14-th batch, evaluate loss: 0.26426878571510315:  67%|███████████▎     | 14/21 [00:03<00:01,  3.56it/s]evaluate for the 6-th batch, evaluate loss: 0.7556292414665222:  20%|████                | 5/25 [00:00<00:02,  9.22it/s]evaluate for the 6-th batch, evaluate loss: 0.7556292414665222:  24%|████▊               | 6/25 [00:00<00:02,  9.23it/s]Epoch: 1, train for the 143-th batch, train loss: 0.5354651808738708:  97%|██████████▋| 142/146 [01:27<00:02,  1.72it/s]Epoch: 1, train for the 143-th batch, train loss: 0.5354651808738708:  98%|██████████▊| 143/146 [01:27<00:01,  1.72it/s]evaluate for the 7-th batch, evaluate loss: 0.7785203456878662:  24%|████▊               | 6/25 [00:00<00:02,  9.23it/s]evaluate for the 7-th batch, evaluate loss: 0.7785203456878662:  28%|█████▌              | 7/25 [00:00<00:01,  9.22it/s]evaluate for the 8-th batch, evaluate loss: 0.7571302056312561:  28%|█████▌              | 7/25 [00:00<00:01,  9.22it/s]evaluate for the 8-th batch, evaluate loss: 0.7571302056312561:  32%|██████▍             | 8/25 [00:00<00:01,  9.23it/s]evaluate for the 15-th batch, evaluate loss: 0.27100425958633423:  67%|███████████▎     | 14/21 [00:04<00:01,  3.56it/s]evaluate for the 15-th batch, evaluate loss: 0.27100425958633423:  71%|████████████▏    | 15/21 [00:04<00:01,  3.62it/s]Epoch: 1, train for the 150-th batch, train loss: 0.6273856163024902:  63%|██████▉    | 149/237 [01:26<00:52,  1.68it/s]Epoch: 1, train for the 150-th batch, train loss: 0.6273856163024902:  63%|██████▉    | 150/237 [01:26<00:51,  1.68it/s]evaluate for the 9-th batch, evaluate loss: 0.7457794547080994:  32%|██████▍             | 8/25 [00:00<00:01,  9.23it/s]evaluate for the 9-th batch, evaluate loss: 0.7457794547080994:  36%|███████▏            | 9/25 [00:00<00:01,  9.25it/s]evaluate for the 10-th batch, evaluate loss: 0.7955576777458191:  36%|██████▊            | 9/25 [00:01<00:01,  9.25it/s]evaluate for the 10-th batch, evaluate loss: 0.7955576777458191:  40%|███████▏          | 10/25 [00:01<00:01,  9.24it/s]Epoch: 1, train for the 142-th batch, train loss: 0.40335264801979065:  37%|███▋      | 141/383 [01:24<02:24,  1.68it/s]Epoch: 1, train for the 142-th batch, train loss: 0.40335264801979065:  37%|███▋      | 142/383 [01:24<02:23,  1.68it/s]evaluate for the 16-th batch, evaluate loss: 0.30204978585243225:  71%|████████████▏    | 15/21 [00:04<00:01,  3.62it/s]evaluate for the 16-th batch, evaluate loss: 0.30204978585243225:  76%|████████████▉    | 16/21 [00:04<00:01,  3.56it/s]evaluate for the 11-th batch, evaluate loss: 0.7721458077430725:  40%|███████▏          | 10/25 [00:01<00:01,  9.24it/s]evaluate for the 11-th batch, evaluate loss: 0.7721458077430725:  44%|███████▉          | 11/25 [00:01<00:01,  9.22it/s]evaluate for the 12-th batch, evaluate loss: 0.7308588027954102:  44%|███████▉          | 11/25 [00:01<00:01,  9.22it/s]evaluate for the 12-th batch, evaluate loss: 0.7308588027954102:  48%|████████▋         | 12/25 [00:01<00:01,  9.11it/s]Epoch: 1, train for the 144-th batch, train loss: 0.5182964205741882:  98%|██████████▊| 143/146 [01:28<00:01,  1.72it/s]Epoch: 1, train for the 144-th batch, train loss: 0.5182964205741882:  99%|██████████▊| 144/146 [01:28<00:01,  1.71it/s]Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ayush/ayushenv/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 260, in check_network_status
    self._loop_check_status(
  File "/home/ayush/ayushenv/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 216, in _loop_check_status
    local_handle = request()
  File "/home/ayush/ayushenv/lib/python3.10/site-packages/wandb/sdk/interface/interface.py", line 795, in deliver_network_status
    return self._deliver_network_status(status)
  File "/home/ayush/ayushenv/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py", line 601, in _deliver_network_status
    return self._deliver_record(record)
  File "/home/ayush/ayushenv/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py", line 560, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home/ayush/ayushenv/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home/ayush/ayushenv/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home/ayush/ayushenv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home/ayush/ayushenv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home/ayush/ayushenv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home/ayush/ayushenv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
evaluate for the 17-th batch, evaluate loss: 0.26692020893096924:  76%|████████████▉    | 16/21 [00:04<00:01,  3.56it/s]evaluate for the 17-th batch, evaluate loss: 0.26692020893096924:  81%|█████████████▊   | 17/21 [00:04<00:01,  3.53it/s]evaluate for the 13-th batch, evaluate loss: 0.7145377397537231:  48%|████████▋         | 12/25 [00:01<00:01,  9.11it/s]evaluate for the 13-th batch, evaluate loss: 0.7145377397537231:  52%|█████████▎        | 13/25 [00:01<00:01,  7.39it/s]Epoch: 1, train for the 151-th batch, train loss: 0.6256138682365417:  63%|██████▉    | 150/237 [01:27<00:51,  1.68it/s]Epoch: 1, train for the 151-th batch, train loss: 0.6256138682365417:  64%|███████    | 151/237 [01:27<00:51,  1.67it/s]evaluate for the 14-th batch, evaluate loss: 0.7955923080444336:  52%|█████████▎        | 13/25 [00:01<00:01,  7.39it/s]evaluate for the 14-th batch, evaluate loss: 0.7955923080444336:  56%|██████████        | 14/25 [00:01<00:01,  7.84it/s]Epoch: 1, train for the 143-th batch, train loss: 0.41050079464912415:  37%|███▋      | 142/383 [01:25<02:23,  1.68it/s]Epoch: 1, train for the 143-th batch, train loss: 0.41050079464912415:  37%|███▋      | 143/383 [01:25<02:23,  1.67it/s]evaluate for the 15-th batch, evaluate loss: 0.7841072082519531:  56%|██████████        | 14/25 [00:01<00:01,  7.84it/s]evaluate for the 15-th batch, evaluate loss: 0.7841072082519531:  60%|██████████▊       | 15/25 [00:01<00:01,  8.19it/s]evaluate for the 18-th batch, evaluate loss: 0.2667507231235504:  81%|██████████████▌   | 17/21 [00:05<00:01,  3.53it/s]evaluate for the 18-th batch, evaluate loss: 0.2667507231235504:  86%|███████████████▍  | 18/21 [00:05<00:00,  3.54it/s]evaluate for the 16-th batch, evaluate loss: 0.6994817852973938:  60%|██████████▊       | 15/25 [00:01<00:01,  8.19it/s]evaluate for the 16-th batch, evaluate loss: 0.6994817852973938:  64%|███████████▌      | 16/25 [00:01<00:01,  8.45it/s]DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
Epoch: 1, train for the 145-th batch, train loss: 0.5528348684310913:  99%|██████████▊| 144/146 [01:29<00:01,  1.71it/s]Epoch: 1, train for the 145-th batch, train loss: 0.5528348684310913:  99%|██████████▉| 145/146 [01:29<00:00,  1.70it/s]evaluate for the 17-th batch, evaluate loss: 0.7102242708206177:  64%|███████████▌      | 16/25 [00:01<00:01,  8.45it/s]evaluate for the 17-th batch, evaluate loss: 0.7102242708206177:  68%|████████████▏     | 17/25 [00:01<00:00,  8.64it/s]evaluate for the 19-th batch, evaluate loss: 0.3511549234390259:  86%|███████████████▍  | 18/21 [00:05<00:00,  3.54it/s]evaluate for the 19-th batch, evaluate loss: 0.3511549234390259:  90%|████████████████▎ | 19/21 [00:05<00:00,  3.53it/s]evaluate for the 18-th batch, evaluate loss: 0.6518028974533081:  68%|████████████▏     | 17/25 [00:02<00:00,  8.64it/s]evaluate for the 18-th batch, evaluate loss: 0.6518028974533081:  72%|████████████▉     | 18/25 [00:02<00:00,  8.76it/s]DEBUG:urllib3.connectionpool:https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/1.1" 200 2
Epoch: 1, train for the 152-th batch, train loss: 0.6456756591796875:  64%|███████    | 151/237 [01:27<00:51,  1.67it/s]Epoch: 1, train for the 152-th batch, train loss: 0.6456756591796875:  64%|███████    | 152/237 [01:27<00:50,  1.67it/s]evaluate for the 19-th batch, evaluate loss: 0.6220406889915466:  72%|████████████▉     | 18/25 [00:02<00:00,  8.76it/s]evaluate for the 19-th batch, evaluate loss: 0.6220406889915466:  76%|█████████████▋    | 19/25 [00:02<00:00,  8.87it/s]evaluate for the 20-th batch, evaluate loss: 0.710690438747406:  76%|██████████████▍    | 19/25 [00:02<00:00,  8.87it/s]evaluate for the 20-th batch, evaluate loss: 0.710690438747406:  80%|███████████████▏   | 20/25 [00:02<00:00,  8.97it/s]Epoch: 1, train for the 144-th batch, train loss: 0.379191517829895:  37%|████▍       | 143/383 [01:26<02:23,  1.67it/s]Epoch: 1, train for the 144-th batch, train loss: 0.379191517829895:  38%|████▌       | 144/383 [01:26<02:23,  1.67it/s]evaluate for the 20-th batch, evaluate loss: 0.3234424591064453:  90%|████████████████▎ | 19/21 [00:05<00:00,  3.53it/s]evaluate for the 20-th batch, evaluate loss: 0.3234424591064453:  95%|█████████████████▏| 20/21 [00:05<00:00,  3.53it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5223271250724792:  99%|██████████▉| 145/146 [01:29<00:00,  1.70it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5223271250724792: 100%|███████████| 146/146 [01:29<00:00,  1.85it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5223271250724792: 100%|███████████| 146/146 [01:29<00:00,  1.63it/s]
  0%|                                                                                            | 0/38 [00:00<?, ?it/s]evaluate for the 21-th batch, evaluate loss: 0.08417027443647385:  95%|████████████████▏| 20/21 [00:05<00:00,  3.53it/s]evaluate for the 21-th batch, evaluate loss: 0.08417027443647385: 100%|█████████████████| 21/21 [00:05<00:00,  3.69it/s]
INFO:root:Epoch: 1, learning rate: 0.0001, train loss: 0.3465
INFO:root:train average_precision, 0.9198
INFO:root:train roc_auc, 0.9142
INFO:root:validate loss: 0.1984
INFO:root:validate average_precision, 0.9748
INFO:root:validate roc_auc, 0.9694
INFO:root:new node validate loss: 0.2816
INFO:root:new node validate first_1_average_precision, 0.8303
INFO:root:new node validate first_1_roc_auc, 0.8210
INFO:root:new node validate first_3_average_precision, 0.9185
INFO:root:new node validate first_3_roc_auc, 0.9065
INFO:root:new node validate first_10_average_precision, 0.9473
INFO:root:new node validate first_10_roc_auc, 0.9395
INFO:root:new node validate average_precision, 0.9497
INFO:root:new node validate roc_auc, 0.9376
INFO:root:save model ./saved_models/DyGFormer/ia-reality-call/DyGFormer_seed0_dygformer-ia-reality-call-old/DyGFormer_seed0_dygformer-ia-reality-call-old.pkl
evaluate for the 21-th batch, evaluate loss: 0.7740710377693176:  80%|██████████████▍   | 20/25 [00:02<00:00,  8.97it/s]evaluate for the 21-th batch, evaluate loss: 0.7740710377693176:  84%|███████████████   | 21/25 [00:02<00:00,  9.03it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]evaluate for the 22-th batch, evaluate loss: 0.6517592668533325:  84%|███████████████   | 21/25 [00:02<00:00,  9.03it/s]evaluate for the 22-th batch, evaluate loss: 0.6517592668533325:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.07it/s]evaluate for the 1-th batch, evaluate loss: 0.497301310300827:   0%|                             | 0/38 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.497301310300827:   3%|▌                    | 1/38 [00:00<00:06,  5.32it/s]evaluate for the 23-th batch, evaluate loss: 0.7104966044425964:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.07it/s]evaluate for the 23-th batch, evaluate loss: 0.7104966044425964:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.07it/s]evaluate for the 24-th batch, evaluate loss: 0.7004691958427429:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.07it/s]evaluate for the 24-th batch, evaluate loss: 0.7004691958427429:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.08it/s]Epoch: 1, train for the 153-th batch, train loss: 0.6439899802207947:  64%|███████    | 152/237 [01:28<00:50,  1.67it/s]Epoch: 1, train for the 153-th batch, train loss: 0.6439899802207947:  65%|███████    | 153/237 [01:28<00:50,  1.67it/s]evaluate for the 2-th batch, evaluate loss: 0.5087199211120605:   3%|▌                   | 1/38 [00:00<00:06,  5.32it/s]evaluate for the 2-th batch, evaluate loss: 0.5087199211120605:   5%|█                   | 2/38 [00:00<00:08,  4.38it/s]evaluate for the 25-th batch, evaluate loss: 0.7461689710617065:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.08it/s]evaluate for the 25-th batch, evaluate loss: 0.7461689710617065: 100%|██████████████████| 25/25 [00:02<00:00,  8.95it/s]
INFO:root:Epoch: 1, learning rate: 0.0001, train loss: 0.5976
INFO:root:train average_precision, 0.7570
INFO:root:train roc_auc, 0.7220
INFO:root:validate loss: 0.5125
INFO:root:validate average_precision, 0.8417
INFO:root:validate roc_auc, 0.8002
INFO:root:new node validate loss: 0.7233
INFO:root:new node validate first_1_average_precision, 0.5844
INFO:root:new node validate first_1_roc_auc, 0.5339
INFO:root:new node validate first_3_average_precision, 0.6662
INFO:root:new node validate first_3_roc_auc, 0.6339
INFO:root:new node validate first_10_average_precision, 0.7344
INFO:root:new node validate first_10_roc_auc, 0.7052
INFO:root:new node validate average_precision, 0.6976
INFO:root:new node validate roc_auc, 0.6490
INFO:root:save model ./saved_models/DyGFormer/ia-retweet-pol/DyGFormer_seed0_dygformer-ia-retweet-pol-old/DyGFormer_seed0_dygformer-ia-retweet-pol-old.pkl
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]Epoch: 1, train for the 145-th batch, train loss: 0.4253474473953247:  38%|████▏      | 144/383 [01:26<02:23,  1.67it/s]Epoch: 1, train for the 145-th batch, train loss: 0.4253474473953247:  38%|████▏      | 145/383 [01:26<02:22,  1.67it/s]Epoch: 2, train for the 1-th batch, train loss: 1.0768935680389404:   0%|                       | 0/119 [00:00<?, ?it/s]Epoch: 2, train for the 1-th batch, train loss: 1.0768935680389404:   1%|▏              | 1/119 [00:00<01:01,  1.92it/s]Epoch: 2, train for the 1-th batch, train loss: 0.7130081057548523:   0%|                       | 0/151 [00:00<?, ?it/s]Epoch: 2, train for the 1-th batch, train loss: 0.7130081057548523:   1%|               | 1/151 [00:00<00:25,  5.83it/s]evaluate for the 3-th batch, evaluate loss: 0.5060051083564758:   5%|█                   | 2/38 [00:00<00:08,  4.38it/s]evaluate for the 3-th batch, evaluate loss: 0.5060051083564758:   8%|█▌                  | 3/38 [00:00<00:08,  3.96it/s]Epoch: 2, train for the 2-th batch, train loss: 0.6820988059043884:   1%|               | 1/151 [00:00<00:25,  5.83it/s]Epoch: 2, train for the 2-th batch, train loss: 0.6820988059043884:   1%|▏              | 2/151 [00:00<00:26,  5.69it/s]evaluate for the 4-th batch, evaluate loss: 0.49572741985321045:   8%|█▌                 | 3/38 [00:00<00:08,  3.96it/s]evaluate for the 4-th batch, evaluate loss: 0.49572741985321045:  11%|██                 | 4/38 [00:00<00:08,  3.85it/s]Epoch: 2, train for the 3-th batch, train loss: 0.6897781491279602:   1%|▏              | 2/151 [00:00<00:26,  5.69it/s]Epoch: 2, train for the 3-th batch, train loss: 0.6897781491279602:   2%|▎              | 3/151 [00:00<00:25,  5.89it/s]Epoch: 1, train for the 154-th batch, train loss: 0.6183940172195435:  65%|███████    | 153/237 [01:29<00:50,  1.67it/s]Epoch: 1, train for the 154-th batch, train loss: 0.6183940172195435:  65%|███████▏   | 154/237 [01:29<00:49,  1.67it/s]Epoch: 1, train for the 146-th batch, train loss: 0.4273151755332947:  38%|████▏      | 145/383 [01:27<02:22,  1.67it/s]Epoch: 1, train for the 146-th batch, train loss: 0.4273151755332947:  38%|████▏      | 146/383 [01:27<02:22,  1.66it/s]Epoch: 2, train for the 2-th batch, train loss: 0.528113603591919:   1%|▏               | 1/119 [00:01<01:01,  1.92it/s]Epoch: 2, train for the 2-th batch, train loss: 0.528113603591919:   2%|▎               | 2/119 [00:01<01:05,  1.77it/s]Epoch: 2, train for the 4-th batch, train loss: 0.6711551547050476:   2%|▎              | 3/151 [00:00<00:25,  5.89it/s]Epoch: 2, train for the 4-th batch, train loss: 0.6711551547050476:   3%|▍              | 4/151 [00:00<00:25,  5.87it/s]evaluate for the 5-th batch, evaluate loss: 0.5229873657226562:  11%|██                  | 4/38 [00:01<00:08,  3.85it/s]evaluate for the 5-th batch, evaluate loss: 0.5229873657226562:  13%|██▋                 | 5/38 [00:01<00:09,  3.66it/s]Epoch: 2, train for the 5-th batch, train loss: 0.6916079521179199:   3%|▍              | 4/151 [00:00<00:25,  5.87it/s]Epoch: 2, train for the 5-th batch, train loss: 0.6916079521179199:   3%|▍              | 5/151 [00:00<00:25,  5.68it/s]evaluate for the 6-th batch, evaluate loss: 0.5157930850982666:  13%|██▋                 | 5/38 [00:01<00:09,  3.66it/s]evaluate for the 6-th batch, evaluate loss: 0.5157930850982666:  16%|███▏                | 6/38 [00:01<00:08,  3.75it/s]Epoch: 2, train for the 6-th batch, train loss: 0.662688136100769:   3%|▌               | 5/151 [00:01<00:25,  5.68it/s]Epoch: 2, train for the 6-th batch, train loss: 0.662688136100769:   4%|▋               | 6/151 [00:01<00:26,  5.53it/s]Epoch: 1, train for the 155-th batch, train loss: 0.6188310980796814:  65%|███████▏   | 154/237 [01:29<00:49,  1.67it/s]Epoch: 1, train for the 155-th batch, train loss: 0.6188310980796814:  65%|███████▏   | 155/237 [01:29<00:49,  1.66it/s]Epoch: 2, train for the 3-th batch, train loss: 0.3569352626800537:   2%|▎              | 2/119 [00:01<01:05,  1.77it/s]Epoch: 2, train for the 3-th batch, train loss: 0.3569352626800537:   3%|▍              | 3/119 [00:01<01:04,  1.79it/s]Epoch: 2, train for the 7-th batch, train loss: 0.5872109532356262:   4%|▌              | 6/151 [00:01<00:26,  5.53it/s]Epoch: 2, train for the 7-th batch, train loss: 0.5872109532356262:   5%|▋              | 7/151 [00:01<00:26,  5.40it/s]Epoch: 1, train for the 147-th batch, train loss: 0.41951102018356323:  38%|███▊      | 146/383 [01:27<02:22,  1.66it/s]Epoch: 1, train for the 147-th batch, train loss: 0.41951102018356323:  38%|███▊      | 147/383 [01:27<02:21,  1.67it/s]evaluate for the 7-th batch, evaluate loss: 0.483905553817749:  16%|███▎                 | 6/38 [00:01<00:08,  3.75it/s]evaluate for the 7-th batch, evaluate loss: 0.483905553817749:  18%|███▊                 | 7/38 [00:01<00:08,  3.63it/s]Epoch: 2, train for the 8-th batch, train loss: 1.2920539379119873:   5%|▋              | 7/151 [00:01<00:26,  5.40it/s]Epoch: 2, train for the 8-th batch, train loss: 1.2920539379119873:   5%|▊              | 8/151 [00:01<00:27,  5.25it/s]evaluate for the 8-th batch, evaluate loss: 0.5532404184341431:  18%|███▋                | 7/38 [00:02<00:08,  3.63it/s]evaluate for the 8-th batch, evaluate loss: 0.5532404184341431:  21%|████▏               | 8/38 [00:02<00:07,  3.77it/s]Epoch: 2, train for the 9-th batch, train loss: 0.6268340349197388:   5%|▊              | 8/151 [00:01<00:27,  5.25it/s]Epoch: 2, train for the 9-th batch, train loss: 0.6268340349197388:   6%|▉              | 9/151 [00:01<00:27,  5.22it/s]Epoch: 1, train for the 156-th batch, train loss: 0.6350892782211304:  65%|███████▏   | 155/237 [01:30<00:49,  1.66it/s]Epoch: 1, train for the 156-th batch, train loss: 0.6350892782211304:  66%|███████▏   | 156/237 [01:30<00:48,  1.67it/s]Epoch: 2, train for the 4-th batch, train loss: 0.36106833815574646:   3%|▎             | 3/119 [00:02<01:04,  1.79it/s]Epoch: 2, train for the 4-th batch, train loss: 0.36106833815574646:   3%|▍             | 4/119 [00:02<01:05,  1.75it/s]Epoch: 2, train for the 10-th batch, train loss: 0.6319122314453125:   6%|▊             | 9/151 [00:01<00:27,  5.22it/s]Epoch: 2, train for the 10-th batch, train loss: 0.6319122314453125:   7%|▊            | 10/151 [00:01<00:27,  5.13it/s]Epoch: 1, train for the 148-th batch, train loss: 0.43546929955482483:  38%|███▊      | 147/383 [01:28<02:21,  1.67it/s]Epoch: 1, train for the 148-th batch, train loss: 0.43546929955482483:  39%|███▊      | 148/383 [01:28<02:20,  1.67it/s]evaluate for the 9-th batch, evaluate loss: 0.5270367860794067:  21%|████▏               | 8/38 [00:02<00:07,  3.77it/s]evaluate for the 9-th batch, evaluate loss: 0.5270367860794067:  24%|████▋               | 9/38 [00:02<00:08,  3.62it/s]Epoch: 2, train for the 11-th batch, train loss: 0.6344408392906189:   7%|▊            | 10/151 [00:02<00:27,  5.13it/s]Epoch: 2, train for the 11-th batch, train loss: 0.6344408392906189:   7%|▉            | 11/151 [00:02<00:27,  5.09it/s]evaluate for the 10-th batch, evaluate loss: 0.5356568694114685:  24%|████▌              | 9/38 [00:02<00:08,  3.62it/s]evaluate for the 10-th batch, evaluate loss: 0.5356568694114685:  26%|████▋             | 10/38 [00:02<00:07,  3.78it/s]evaluate for the 11-th batch, evaluate loss: 0.5298187732696533:  26%|████▋             | 10/38 [00:02<00:07,  3.78it/s]evaluate for the 11-th batch, evaluate loss: 0.5298187732696533:  29%|█████▏            | 11/38 [00:02<00:05,  4.65it/s]Epoch: 2, train for the 12-th batch, train loss: 0.6589069366455078:   7%|▉            | 11/151 [00:02<00:27,  5.09it/s]Epoch: 2, train for the 12-th batch, train loss: 0.6589069366455078:   8%|█            | 12/151 [00:02<00:27,  4.97it/s]Epoch: 1, train for the 157-th batch, train loss: 0.6595168709754944:  66%|███████▏   | 156/237 [01:30<00:48,  1.67it/s]Epoch: 1, train for the 157-th batch, train loss: 0.6595168709754944:  66%|███████▎   | 157/237 [01:30<00:47,  1.68it/s]evaluate for the 12-th batch, evaluate loss: 0.5585169792175293:  29%|█████▏            | 11/38 [00:02<00:05,  4.65it/s]evaluate for the 12-th batch, evaluate loss: 0.5585169792175293:  32%|█████▋            | 12/38 [00:02<00:04,  5.24it/s]Epoch: 1, train for the 149-th batch, train loss: 0.4646722972393036:  39%|████▎      | 148/383 [01:29<02:20,  1.67it/s]Epoch: 1, train for the 149-th batch, train loss: 0.4646722972393036:  39%|████▎      | 149/383 [01:29<02:19,  1.68it/s]Epoch: 2, train for the 13-th batch, train loss: 0.6675036549568176:   8%|█            | 12/151 [00:02<00:27,  4.97it/s]Epoch: 2, train for the 13-th batch, train loss: 0.6675036549568176:   9%|█            | 13/151 [00:02<00:27,  4.97it/s]Epoch: 2, train for the 5-th batch, train loss: 0.3377716839313507:   3%|▌              | 4/119 [00:03<01:05,  1.75it/s]Epoch: 2, train for the 5-th batch, train loss: 0.3377716839313507:   4%|▋              | 5/119 [00:03<01:14,  1.54it/s]evaluate for the 13-th batch, evaluate loss: 0.5643964409828186:  32%|█████▋            | 12/38 [00:03<00:04,  5.24it/s]evaluate for the 13-th batch, evaluate loss: 0.5643964409828186:  34%|██████▏           | 13/38 [00:03<00:05,  4.46it/s]Epoch: 2, train for the 14-th batch, train loss: 0.6531280279159546:   9%|█            | 13/151 [00:02<00:27,  4.97it/s]Epoch: 2, train for the 14-th batch, train loss: 0.6531280279159546:   9%|█▏           | 14/151 [00:02<00:27,  4.96it/s]Epoch: 2, train for the 15-th batch, train loss: 0.6495566368103027:   9%|█▏           | 14/151 [00:02<00:27,  4.96it/s]Epoch: 2, train for the 15-th batch, train loss: 0.6495566368103027:  10%|█▎           | 15/151 [00:02<00:27,  4.93it/s]Epoch: 1, train for the 158-th batch, train loss: 0.6170654296875:  66%|█████████▎    | 157/237 [01:31<00:47,  1.68it/s]Epoch: 1, train for the 158-th batch, train loss: 0.6170654296875:  67%|█████████▎    | 158/237 [01:31<00:47,  1.68it/s]evaluate for the 14-th batch, evaluate loss: 0.5073081254959106:  34%|██████▏           | 13/38 [00:03<00:05,  4.46it/s]evaluate for the 14-th batch, evaluate loss: 0.5073081254959106:  37%|██████▋           | 14/38 [00:03<00:05,  4.34it/s]Epoch: 1, train for the 150-th batch, train loss: 0.5319114327430725:  39%|████▎      | 149/383 [01:29<02:19,  1.68it/s]Epoch: 1, train for the 150-th batch, train loss: 0.5319114327430725:  39%|████▎      | 150/383 [01:29<02:18,  1.68it/s]Epoch: 2, train for the 16-th batch, train loss: 0.6707996726036072:  10%|█▎           | 15/151 [00:03<00:27,  4.93it/s]Epoch: 2, train for the 16-th batch, train loss: 0.6707996726036072:  11%|█▍           | 16/151 [00:03<00:28,  4.81it/s]Epoch: 2, train for the 6-th batch, train loss: 0.284043550491333:   4%|▋               | 5/119 [00:03<01:14,  1.54it/s]Epoch: 2, train for the 6-th batch, train loss: 0.284043550491333:   5%|▊               | 6/119 [00:03<01:11,  1.57it/s]evaluate for the 15-th batch, evaluate loss: 0.5237259268760681:  37%|██████▋           | 14/38 [00:03<00:05,  4.34it/s]evaluate for the 15-th batch, evaluate loss: 0.5237259268760681:  39%|███████           | 15/38 [00:03<00:05,  3.91it/s]Epoch: 2, train for the 17-th batch, train loss: 0.6326928734779358:  11%|█▍           | 16/151 [00:03<00:28,  4.81it/s]Epoch: 2, train for the 17-th batch, train loss: 0.6326928734779358:  11%|█▍           | 17/151 [00:03<00:27,  4.87it/s]evaluate for the 16-th batch, evaluate loss: 0.5306554436683655:  39%|███████           | 15/38 [00:03<00:05,  3.91it/s]evaluate for the 16-th batch, evaluate loss: 0.5306554436683655:  42%|███████▌          | 16/38 [00:03<00:05,  3.85it/s]Epoch: 2, train for the 18-th batch, train loss: 0.6456719636917114:  11%|█▍           | 17/151 [00:03<00:27,  4.87it/s]Epoch: 2, train for the 18-th batch, train loss: 0.6456719636917114:  12%|█▌           | 18/151 [00:03<00:27,  4.85it/s]Epoch: 1, train for the 159-th batch, train loss: 0.6343902945518494:  67%|███████▎   | 158/237 [01:31<00:47,  1.68it/s]Epoch: 1, train for the 159-th batch, train loss: 0.6343902945518494:  67%|███████▍   | 159/237 [01:31<00:46,  1.68it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5072441101074219:  39%|████▎      | 150/383 [01:30<02:18,  1.68it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5072441101074219:  39%|████▎      | 151/383 [01:30<02:18,  1.68it/s]Epoch: 2, train for the 19-th batch, train loss: 0.6316360235214233:  12%|█▌           | 18/151 [00:03<00:27,  4.85it/s]Epoch: 2, train for the 19-th batch, train loss: 0.6316360235214233:  13%|█▋           | 19/151 [00:03<00:27,  4.88it/s]evaluate for the 17-th batch, evaluate loss: 0.505530834197998:  42%|████████           | 16/38 [00:04<00:05,  3.85it/s]evaluate for the 17-th batch, evaluate loss: 0.505530834197998:  45%|████████▌          | 17/38 [00:04<00:05,  3.69it/s]Epoch: 2, train for the 7-th batch, train loss: 0.25614306330680847:   5%|▋             | 6/119 [00:04<01:11,  1.57it/s]Epoch: 2, train for the 7-th batch, train loss: 0.25614306330680847:   6%|▊             | 7/119 [00:04<01:09,  1.61it/s]Epoch: 2, train for the 20-th batch, train loss: 0.6281324625015259:  13%|█▋           | 19/151 [00:03<00:27,  4.88it/s]Epoch: 2, train for the 20-th batch, train loss: 0.6281324625015259:  13%|█▋           | 20/151 [00:03<00:27,  4.83it/s]evaluate for the 18-th batch, evaluate loss: 0.5435345768928528:  45%|████████          | 17/38 [00:04<00:05,  3.69it/s]evaluate for the 18-th batch, evaluate loss: 0.5435345768928528:  47%|████████▌         | 18/38 [00:04<00:05,  3.77it/s]Epoch: 1, train for the 160-th batch, train loss: 0.6423795819282532:  67%|███████▍   | 159/237 [01:32<00:46,  1.68it/s]Epoch: 1, train for the 160-th batch, train loss: 0.6423795819282532:  68%|███████▍   | 160/237 [01:32<00:45,  1.68it/s]Epoch: 2, train for the 21-th batch, train loss: 0.6421166658401489:  13%|█▋           | 20/151 [00:04<00:27,  4.83it/s]Epoch: 2, train for the 21-th batch, train loss: 0.6421166658401489:  14%|█▊           | 21/151 [00:04<00:27,  4.77it/s]Epoch: 1, train for the 152-th batch, train loss: 0.4622504413127899:  39%|████▎      | 151/383 [01:30<02:18,  1.68it/s]Epoch: 1, train for the 152-th batch, train loss: 0.4622504413127899:  40%|████▎      | 152/383 [01:30<02:17,  1.68it/s]Epoch: 2, train for the 22-th batch, train loss: 0.5974284410476685:  14%|█▊           | 21/151 [00:04<00:27,  4.77it/s]Epoch: 2, train for the 22-th batch, train loss: 0.5974284410476685:  15%|█▉           | 22/151 [00:04<00:26,  4.79it/s]evaluate for the 19-th batch, evaluate loss: 0.5181588530540466:  47%|████████▌         | 18/38 [00:04<00:05,  3.77it/s]evaluate for the 19-th batch, evaluate loss: 0.5181588530540466:  50%|█████████         | 19/38 [00:04<00:05,  3.65it/s]Epoch: 2, train for the 8-th batch, train loss: 0.23336465656757355:   6%|▊             | 7/119 [00:04<01:09,  1.61it/s]Epoch: 2, train for the 8-th batch, train loss: 0.23336465656757355:   7%|▉             | 8/119 [00:04<01:07,  1.64it/s]Epoch: 2, train for the 23-th batch, train loss: 0.6116074323654175:  15%|█▉           | 22/151 [00:04<00:26,  4.79it/s]Epoch: 2, train for the 23-th batch, train loss: 0.6116074323654175:  15%|█▉           | 23/151 [00:04<00:26,  4.76it/s]evaluate for the 20-th batch, evaluate loss: 0.5092765688896179:  50%|█████████         | 19/38 [00:05<00:05,  3.65it/s]evaluate for the 20-th batch, evaluate loss: 0.5092765688896179:  53%|█████████▍        | 20/38 [00:05<00:04,  3.73it/s]Epoch: 1, train for the 161-th batch, train loss: 0.6590307354927063:  68%|███████▍   | 160/237 [01:33<00:45,  1.68it/s]Epoch: 1, train for the 161-th batch, train loss: 0.6590307354927063:  68%|███████▍   | 161/237 [01:33<00:45,  1.68it/s]Epoch: 2, train for the 24-th batch, train loss: 0.5807416439056396:  15%|█▉           | 23/151 [00:04<00:26,  4.76it/s]Epoch: 2, train for the 24-th batch, train loss: 0.5807416439056396:  16%|██           | 24/151 [00:04<00:26,  4.76it/s]Epoch: 1, train for the 153-th batch, train loss: 0.5080198049545288:  40%|████▎      | 152/383 [01:31<02:17,  1.68it/s]Epoch: 1, train for the 153-th batch, train loss: 0.5080198049545288:  40%|████▍      | 153/383 [01:31<02:17,  1.67it/s]evaluate for the 21-th batch, evaluate loss: 0.5110970139503479:  53%|█████████▍        | 20/38 [00:05<00:04,  3.73it/s]evaluate for the 21-th batch, evaluate loss: 0.5110970139503479:  55%|█████████▉        | 21/38 [00:05<00:04,  3.64it/s]Epoch: 2, train for the 25-th batch, train loss: 0.6293767094612122:  16%|██           | 24/151 [00:04<00:26,  4.76it/s]Epoch: 2, train for the 25-th batch, train loss: 0.6293767094612122:  17%|██▏          | 25/151 [00:04<00:26,  4.79it/s]Epoch: 2, train for the 9-th batch, train loss: 0.29210516810417175:   7%|▉             | 8/119 [00:05<01:07,  1.64it/s]Epoch: 2, train for the 9-th batch, train loss: 0.29210516810417175:   8%|█             | 9/119 [00:05<01:07,  1.64it/s]evaluate for the 22-th batch, evaluate loss: 0.5289549231529236:  55%|█████████▉        | 21/38 [00:05<00:04,  3.64it/s]evaluate for the 22-th batch, evaluate loss: 0.5289549231529236:  58%|██████████▍       | 22/38 [00:05<00:04,  3.65it/s]Epoch: 2, train for the 26-th batch, train loss: 0.6060714721679688:  17%|██▏          | 25/151 [00:05<00:26,  4.79it/s]Epoch: 2, train for the 26-th batch, train loss: 0.6060714721679688:  17%|██▏          | 26/151 [00:05<00:26,  4.77it/s]Epoch: 1, train for the 154-th batch, train loss: 0.4436797499656677:  40%|████▍      | 153/383 [01:32<02:17,  1.67it/s]Epoch: 1, train for the 154-th batch, train loss: 0.4436797499656677:  40%|████▍      | 154/383 [01:32<02:09,  1.77it/s]Epoch: 2, train for the 27-th batch, train loss: 0.6089052557945251:  17%|██▏          | 26/151 [00:05<00:26,  4.77it/s]Epoch: 2, train for the 27-th batch, train loss: 0.6089052557945251:  18%|██▎          | 27/151 [00:05<00:26,  4.72it/s]Epoch: 1, train for the 162-th batch, train loss: 0.6410276889801025:  68%|███████▍   | 161/237 [01:33<00:45,  1.68it/s]Epoch: 1, train for the 162-th batch, train loss: 0.6410276889801025:  68%|███████▌   | 162/237 [01:33<00:47,  1.58it/s]evaluate for the 23-th batch, evaluate loss: 0.534786581993103:  58%|███████████        | 22/38 [00:05<00:04,  3.65it/s]evaluate for the 23-th batch, evaluate loss: 0.534786581993103:  61%|███████████▌       | 23/38 [00:05<00:04,  3.66it/s]Epoch: 2, train for the 10-th batch, train loss: 0.32255035638809204:   8%|▉            | 9/119 [00:06<01:07,  1.64it/s]Epoch: 2, train for the 10-th batch, train loss: 0.32255035638809204:   8%|█           | 10/119 [00:06<01:06,  1.65it/s]Epoch: 2, train for the 28-th batch, train loss: 0.5750179886817932:  18%|██▎          | 27/151 [00:05<00:26,  4.72it/s]Epoch: 2, train for the 28-th batch, train loss: 0.5750179886817932:  19%|██▍          | 28/151 [00:05<00:26,  4.71it/s]evaluate for the 24-th batch, evaluate loss: 0.5102031826972961:  61%|██████████▉       | 23/38 [00:06<00:04,  3.66it/s]evaluate for the 24-th batch, evaluate loss: 0.5102031826972961:  63%|███████████▎      | 24/38 [00:06<00:03,  3.56it/s]Epoch: 2, train for the 29-th batch, train loss: 0.6249728202819824:  19%|██▍          | 28/151 [00:05<00:26,  4.71it/s]Epoch: 2, train for the 29-th batch, train loss: 0.6249728202819824:  19%|██▍          | 29/151 [00:05<00:25,  4.73it/s]Epoch: 1, train for the 155-th batch, train loss: 0.42312735319137573:  40%|████      | 154/383 [01:32<02:09,  1.77it/s]Epoch: 1, train for the 155-th batch, train loss: 0.42312735319137573:  40%|████      | 155/383 [01:32<02:09,  1.76it/s]evaluate for the 25-th batch, evaluate loss: 0.5250023603439331:  63%|███████████▎      | 24/38 [00:06<00:03,  3.56it/s]evaluate for the 25-th batch, evaluate loss: 0.5250023603439331:  66%|███████████▊      | 25/38 [00:06<00:03,  3.70it/s]Epoch: 1, train for the 163-th batch, train loss: 0.6619467735290527:  68%|███████▌   | 162/237 [01:34<00:47,  1.58it/s]Epoch: 1, train for the 163-th batch, train loss: 0.6619467735290527:  69%|███████▌   | 163/237 [01:34<00:45,  1.61it/s]Epoch: 2, train for the 30-th batch, train loss: 0.6083645820617676:  19%|██▍          | 29/151 [00:06<00:25,  4.73it/s]Epoch: 2, train for the 30-th batch, train loss: 0.6083645820617676:  20%|██▌          | 30/151 [00:06<00:25,  4.72it/s]Epoch: 2, train for the 11-th batch, train loss: 0.2895093262195587:   8%|█            | 10/119 [00:06<01:06,  1.65it/s]Epoch: 2, train for the 11-th batch, train loss: 0.2895093262195587:   9%|█▏           | 11/119 [00:06<01:05,  1.66it/s]Epoch: 2, train for the 31-th batch, train loss: 0.6286011934280396:  20%|██▌          | 30/151 [00:06<00:25,  4.72it/s]Epoch: 2, train for the 31-th batch, train loss: 0.6286011934280396:  21%|██▋          | 31/151 [00:06<00:25,  4.69it/s]evaluate for the 26-th batch, evaluate loss: 0.512224018573761:  66%|████████████▌      | 25/38 [00:06<00:03,  3.70it/s]evaluate for the 26-th batch, evaluate loss: 0.512224018573761:  68%|█████████████      | 26/38 [00:06<00:03,  3.57it/s]Epoch: 2, train for the 32-th batch, train loss: 0.6167573928833008:  21%|██▋          | 31/151 [00:06<00:25,  4.69it/s]Epoch: 2, train for the 32-th batch, train loss: 0.6167573928833008:  21%|██▊          | 32/151 [00:06<00:25,  4.67it/s]Epoch: 1, train for the 156-th batch, train loss: 0.4463006258010864:  40%|████▍      | 155/383 [01:33<02:09,  1.76it/s]Epoch: 1, train for the 156-th batch, train loss: 0.4463006258010864:  41%|████▍      | 156/383 [01:33<02:09,  1.76it/s]evaluate for the 27-th batch, evaluate loss: 0.5444619059562683:  68%|████████████▎     | 26/38 [00:07<00:03,  3.57it/s]evaluate for the 27-th batch, evaluate loss: 0.5444619059562683:  71%|████████████▊     | 27/38 [00:07<00:02,  3.74it/s]Epoch: 1, train for the 164-th batch, train loss: 0.6566685438156128:  69%|███████▌   | 163/237 [01:35<00:45,  1.61it/s]Epoch: 1, train for the 164-th batch, train loss: 0.6566685438156128:  69%|███████▌   | 164/237 [01:35<00:44,  1.63it/s]Epoch: 2, train for the 33-th batch, train loss: 0.577607274055481:  21%|██▉           | 32/151 [00:06<00:25,  4.67it/s]Epoch: 2, train for the 33-th batch, train loss: 0.577607274055481:  22%|███           | 33/151 [00:06<00:25,  4.65it/s]Epoch: 2, train for the 12-th batch, train loss: 0.3195093274116516:   9%|█▏           | 11/119 [00:07<01:05,  1.66it/s]Epoch: 2, train for the 12-th batch, train loss: 0.3195093274116516:  10%|█▎           | 12/119 [00:07<01:04,  1.66it/s]evaluate for the 28-th batch, evaluate loss: 0.554038405418396:  71%|█████████████▌     | 27/38 [00:07<00:02,  3.74it/s]evaluate for the 28-th batch, evaluate loss: 0.554038405418396:  74%|██████████████     | 28/38 [00:07<00:02,  3.56it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5868708491325378:  22%|██▊          | 33/151 [00:06<00:25,  4.65it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5868708491325378:  23%|██▉          | 34/151 [00:06<00:25,  4.63it/s]Epoch: 1, train for the 157-th batch, train loss: 0.44258925318717957:  41%|████      | 156/383 [01:33<02:09,  1.76it/s]Epoch: 1, train for the 157-th batch, train loss: 0.44258925318717957:  41%|████      | 157/383 [01:33<02:08,  1.76it/s]evaluate for the 29-th batch, evaluate loss: 0.5213418006896973:  74%|█████████████▎    | 28/38 [00:07<00:02,  3.56it/s]evaluate for the 29-th batch, evaluate loss: 0.5213418006896973:  76%|█████████████▋    | 29/38 [00:07<00:02,  3.62it/s]Epoch: 2, train for the 35-th batch, train loss: 0.6028078198432922:  23%|██▉          | 34/151 [00:07<00:25,  4.63it/s]Epoch: 2, train for the 35-th batch, train loss: 0.6028078198432922:  23%|███          | 35/151 [00:07<00:24,  4.65it/s]Epoch: 1, train for the 165-th batch, train loss: 0.6170978546142578:  69%|███████▌   | 164/237 [01:35<00:44,  1.63it/s]Epoch: 1, train for the 165-th batch, train loss: 0.6170978546142578:  70%|███████▋   | 165/237 [01:35<00:43,  1.64it/s]Epoch: 2, train for the 36-th batch, train loss: 0.5619522929191589:  23%|███          | 35/151 [00:07<00:24,  4.65it/s]Epoch: 2, train for the 36-th batch, train loss: 0.5619522929191589:  24%|███          | 36/151 [00:07<00:24,  4.66it/s]evaluate for the 30-th batch, evaluate loss: 0.5542017817497253:  76%|█████████████▋    | 29/38 [00:07<00:02,  3.62it/s]evaluate for the 30-th batch, evaluate loss: 0.5542017817497253:  79%|██████████████▏   | 30/38 [00:07<00:02,  3.52it/s]Epoch: 2, train for the 13-th batch, train loss: 0.29071712493896484:  10%|█▏          | 12/119 [00:07<01:04,  1.66it/s]Epoch: 2, train for the 13-th batch, train loss: 0.29071712493896484:  11%|█▎          | 13/119 [00:07<01:03,  1.66it/s]Epoch: 2, train for the 37-th batch, train loss: 0.7720609903335571:  24%|███          | 36/151 [00:07<00:24,  4.66it/s]Epoch: 2, train for the 37-th batch, train loss: 0.7720609903335571:  25%|███▏         | 37/151 [00:07<00:25,  4.54it/s]Epoch: 1, train for the 158-th batch, train loss: 0.48650503158569336:  41%|████      | 157/383 [01:34<02:08,  1.76it/s]Epoch: 1, train for the 158-th batch, train loss: 0.48650503158569336:  41%|████▏     | 158/383 [01:34<02:09,  1.74it/s]evaluate for the 31-th batch, evaluate loss: 0.5410152077674866:  79%|██████████████▏   | 30/38 [00:08<00:02,  3.52it/s]evaluate for the 31-th batch, evaluate loss: 0.5410152077674866:  82%|██████████████▋   | 31/38 [00:08<00:01,  3.63it/s]Epoch: 2, train for the 38-th batch, train loss: 0.582151472568512:  25%|███▍          | 37/151 [00:07<00:25,  4.54it/s]Epoch: 2, train for the 38-th batch, train loss: 0.582151472568512:  25%|███▌          | 38/151 [00:07<00:24,  4.58it/s]Epoch: 1, train for the 166-th batch, train loss: 0.6238702535629272:  70%|███████▋   | 165/237 [01:36<00:43,  1.64it/s]Epoch: 1, train for the 166-th batch, train loss: 0.6238702535629272:  70%|███████▋   | 166/237 [01:36<00:42,  1.66it/s]evaluate for the 32-th batch, evaluate loss: 0.5129774212837219:  82%|██████████████▋   | 31/38 [00:08<00:01,  3.63it/s]evaluate for the 32-th batch, evaluate loss: 0.5129774212837219:  84%|███████████████▏  | 32/38 [00:08<00:01,  3.56it/s]Epoch: 2, train for the 39-th batch, train loss: 0.48193350434303284:  25%|███         | 38/151 [00:07<00:24,  4.58it/s]Epoch: 2, train for the 39-th batch, train loss: 0.48193350434303284:  26%|███         | 39/151 [00:07<00:24,  4.59it/s]Epoch: 2, train for the 14-th batch, train loss: 0.2882196009159088:  11%|█▍           | 13/119 [00:08<01:03,  1.66it/s]Epoch: 2, train for the 14-th batch, train loss: 0.2882196009159088:  12%|█▌           | 14/119 [00:08<01:03,  1.66it/s]Epoch: 2, train for the 40-th batch, train loss: 0.48450514674186707:  26%|███         | 39/151 [00:08<00:24,  4.59it/s]Epoch: 2, train for the 40-th batch, train loss: 0.48450514674186707:  26%|███▏        | 40/151 [00:08<00:23,  4.64it/s]evaluate for the 33-th batch, evaluate loss: 0.5170500874519348:  84%|███████████████▏  | 32/38 [00:08<00:01,  3.56it/s]evaluate for the 33-th batch, evaluate loss: 0.5170500874519348:  87%|███████████████▋  | 33/38 [00:08<00:01,  3.65it/s]Epoch: 1, train for the 159-th batch, train loss: 0.43931296467781067:  41%|████▏     | 158/383 [01:34<02:09,  1.74it/s]Epoch: 1, train for the 159-th batch, train loss: 0.43931296467781067:  42%|████▏     | 159/383 [01:34<02:09,  1.73it/s]Epoch: 1, train for the 167-th batch, train loss: 0.6273441910743713:  70%|███████▋   | 166/237 [01:36<00:42,  1.66it/s]Epoch: 1, train for the 167-th batch, train loss: 0.6273441910743713:  70%|███████▊   | 167/237 [01:36<00:42,  1.67it/s]Epoch: 2, train for the 41-th batch, train loss: 0.5624477863311768:  26%|███▍         | 40/151 [00:08<00:23,  4.64it/s]Epoch: 2, train for the 41-th batch, train loss: 0.5624477863311768:  27%|███▌         | 41/151 [00:08<00:23,  4.61it/s]evaluate for the 34-th batch, evaluate loss: 0.5347498655319214:  87%|███████████████▋  | 33/38 [00:08<00:01,  3.65it/s]evaluate for the 34-th batch, evaluate loss: 0.5347498655319214:  89%|████████████████  | 34/38 [00:08<00:01,  3.59it/s]Epoch: 2, train for the 15-th batch, train loss: 0.28497982025146484:  12%|█▍          | 14/119 [00:09<01:03,  1.66it/s]Epoch: 2, train for the 15-th batch, train loss: 0.28497982025146484:  13%|█▌          | 15/119 [00:09<01:02,  1.66it/s]Epoch: 2, train for the 42-th batch, train loss: 0.491452693939209:  27%|███▊          | 41/151 [00:08<00:23,  4.61it/s]Epoch: 2, train for the 42-th batch, train loss: 0.491452693939209:  28%|███▉          | 42/151 [00:08<00:23,  4.61it/s]evaluate for the 35-th batch, evaluate loss: 0.5582667589187622:  89%|████████████████  | 34/38 [00:09<00:01,  3.59it/s]evaluate for the 35-th batch, evaluate loss: 0.5582667589187622:  92%|████████████████▌ | 35/38 [00:09<00:00,  3.61it/s]Epoch: 1, train for the 160-th batch, train loss: 0.4596092104911804:  42%|████▌      | 159/383 [01:35<02:09,  1.73it/s]Epoch: 1, train for the 160-th batch, train loss: 0.4596092104911804:  42%|████▌      | 160/383 [01:35<02:10,  1.71it/s]Epoch: 2, train for the 43-th batch, train loss: 0.6614884734153748:  28%|███▌         | 42/151 [00:08<00:23,  4.61it/s]Epoch: 2, train for the 43-th batch, train loss: 0.6614884734153748:  28%|███▋         | 43/151 [00:08<00:23,  4.58it/s]Epoch: 1, train for the 168-th batch, train loss: 0.6518089771270752:  70%|███████▊   | 167/237 [01:37<00:42,  1.67it/s]Epoch: 1, train for the 168-th batch, train loss: 0.6518089771270752:  71%|███████▊   | 168/237 [01:37<00:41,  1.67it/s]evaluate for the 36-th batch, evaluate loss: 0.5900996923446655:  92%|████████████████▌ | 35/38 [00:09<00:00,  3.61it/s]evaluate for the 36-th batch, evaluate loss: 0.5900996923446655:  95%|█████████████████ | 36/38 [00:09<00:00,  3.63it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5849372744560242:  28%|███▋         | 43/151 [00:09<00:23,  4.58it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5849372744560242:  29%|███▊         | 44/151 [00:09<00:23,  4.49it/s]Epoch: 2, train for the 16-th batch, train loss: 0.29596149921417236:  13%|█▌          | 15/119 [00:09<01:02,  1.66it/s]Epoch: 2, train for the 16-th batch, train loss: 0.29596149921417236:  13%|█▌          | 16/119 [00:09<01:02,  1.66it/s]Epoch: 2, train for the 45-th batch, train loss: 0.6073259711265564:  29%|███▊         | 44/151 [00:09<00:23,  4.49it/s]Epoch: 2, train for the 45-th batch, train loss: 0.6073259711265564:  30%|███▊         | 45/151 [00:09<00:23,  4.49it/s]evaluate for the 37-th batch, evaluate loss: 0.5180379152297974:  95%|█████████████████ | 36/38 [00:09<00:00,  3.63it/s]evaluate for the 37-th batch, evaluate loss: 0.5180379152297974:  97%|█████████████████▌| 37/38 [00:09<00:00,  3.54it/s]Epoch: 1, train for the 161-th batch, train loss: 0.4279918670654297:  42%|████▌      | 160/383 [01:36<02:10,  1.71it/s]Epoch: 1, train for the 161-th batch, train loss: 0.4279918670654297:  42%|████▌      | 161/383 [01:36<02:10,  1.70it/s]Epoch: 2, train for the 46-th batch, train loss: 0.6597772240638733:  30%|███▊         | 45/151 [00:09<00:23,  4.49it/s]Epoch: 2, train for the 46-th batch, train loss: 0.6597772240638733:  30%|███▉         | 46/151 [00:09<00:23,  4.49it/s]evaluate for the 38-th batch, evaluate loss: 0.5179502367973328:  97%|█████████████████▌| 37/38 [00:10<00:00,  3.54it/s]evaluate for the 38-th batch, evaluate loss: 0.5179502367973328: 100%|██████████████████| 38/38 [00:10<00:00,  3.71it/s]evaluate for the 38-th batch, evaluate loss: 0.5179502367973328: 100%|██████████████████| 38/38 [00:10<00:00,  3.77it/s]
  0%|                                                                                            | 0/20 [00:00<?, ?it/s]Epoch: 1, train for the 169-th batch, train loss: 0.6512863636016846:  71%|███████▊   | 168/237 [01:38<00:41,  1.67it/s]Epoch: 1, train for the 169-th batch, train loss: 0.6512863636016846:  71%|███████▊   | 169/237 [01:38<00:40,  1.66it/s]Epoch: 2, train for the 47-th batch, train loss: 0.6350283622741699:  30%|███▉         | 46/151 [00:09<00:23,  4.49it/s]Epoch: 2, train for the 47-th batch, train loss: 0.6350283622741699:  31%|████         | 47/151 [00:09<00:23,  4.50it/s]Epoch: 2, train for the 17-th batch, train loss: 0.2586651146411896:  13%|█▋           | 16/119 [00:10<01:02,  1.66it/s]Epoch: 2, train for the 17-th batch, train loss: 0.2586651146411896:  14%|█▊           | 17/119 [00:10<01:01,  1.66it/s]evaluate for the 1-th batch, evaluate loss: 0.5788907408714294:   0%|                            | 0/20 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.5788907408714294:   5%|█                   | 1/20 [00:00<00:05,  3.21it/s]Epoch: 2, train for the 48-th batch, train loss: 0.5468471646308899:  31%|████         | 47/151 [00:09<00:23,  4.50it/s]Epoch: 2, train for the 48-th batch, train loss: 0.5468471646308899:  32%|████▏        | 48/151 [00:09<00:22,  4.52it/s]Epoch: 1, train for the 162-th batch, train loss: 0.44882604479789734:  42%|████▏     | 161/383 [01:36<02:10,  1.70it/s]Epoch: 1, train for the 162-th batch, train loss: 0.44882604479789734:  42%|████▏     | 162/383 [01:36<02:10,  1.69it/s]evaluate for the 2-th batch, evaluate loss: 0.6227343082427979:   5%|█                   | 1/20 [00:00<00:05,  3.21it/s]evaluate for the 2-th batch, evaluate loss: 0.6227343082427979:  10%|██                  | 2/20 [00:00<00:04,  3.67it/s]Epoch: 1, train for the 170-th batch, train loss: 0.633118748664856:  71%|████████▌   | 169/237 [01:38<00:40,  1.66it/s]Epoch: 1, train for the 170-th batch, train loss: 0.633118748664856:  72%|████████▌   | 170/237 [01:38<00:40,  1.66it/s]Epoch: 2, train for the 18-th batch, train loss: 0.25407230854034424:  14%|█▋          | 17/119 [00:10<01:01,  1.66it/s]Epoch: 2, train for the 18-th batch, train loss: 0.25407230854034424:  15%|█▊          | 18/119 [00:10<01:00,  1.66it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5963468551635742:  32%|████▏        | 48/151 [00:10<00:22,  4.52it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5963468551635742:  32%|████▏        | 49/151 [00:10<00:29,  3.42it/s]evaluate for the 3-th batch, evaluate loss: 0.6161180138587952:  10%|██                  | 2/20 [00:00<00:04,  3.67it/s]evaluate for the 3-th batch, evaluate loss: 0.6161180138587952:  15%|███                 | 3/20 [00:00<00:05,  3.39it/s]Epoch: 1, train for the 163-th batch, train loss: 0.43668991327285767:  42%|████▏     | 162/383 [01:37<02:10,  1.69it/s]Epoch: 1, train for the 163-th batch, train loss: 0.43668991327285767:  43%|████▎     | 163/383 [01:37<02:10,  1.68it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5381866097450256:  32%|████▏        | 49/151 [00:10<00:29,  3.42it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5381866097450256:  33%|████▎        | 50/151 [00:10<00:27,  3.70it/s]evaluate for the 4-th batch, evaluate loss: 0.5794951319694519:  15%|███                 | 3/20 [00:01<00:05,  3.39it/s]evaluate for the 4-th batch, evaluate loss: 0.5794951319694519:  20%|████                | 4/20 [00:01<00:04,  3.55it/s]Epoch: 1, train for the 171-th batch, train loss: 0.657749593257904:  72%|████████▌   | 170/237 [01:39<00:40,  1.66it/s]Epoch: 1, train for the 171-th batch, train loss: 0.657749593257904:  72%|████████▋   | 171/237 [01:39<00:39,  1.66it/s]Epoch: 2, train for the 51-th batch, train loss: 0.6265084743499756:  33%|████▎        | 50/151 [00:10<00:27,  3.70it/s]Epoch: 2, train for the 51-th batch, train loss: 0.6265084743499756:  34%|████▍        | 51/151 [00:10<00:25,  3.91it/s]Epoch: 2, train for the 19-th batch, train loss: 0.28511059284210205:  15%|█▊          | 18/119 [00:11<01:00,  1.66it/s]Epoch: 2, train for the 19-th batch, train loss: 0.28511059284210205:  16%|█▉          | 19/119 [00:11<01:00,  1.66it/s]evaluate for the 5-th batch, evaluate loss: 0.6046285033226013:  20%|████                | 4/20 [00:01<00:04,  3.55it/s]evaluate for the 5-th batch, evaluate loss: 0.6046285033226013:  25%|█████               | 5/20 [00:01<00:04,  3.36it/s]Epoch: 2, train for the 52-th batch, train loss: 0.4665765166282654:  34%|████▍        | 51/151 [00:11<00:25,  3.91it/s]Epoch: 2, train for the 52-th batch, train loss: 0.4665765166282654:  34%|████▍        | 52/151 [00:11<00:24,  4.11it/s]Epoch: 1, train for the 164-th batch, train loss: 0.46354398131370544:  43%|████▎     | 163/383 [01:37<02:10,  1.68it/s]Epoch: 1, train for the 164-th batch, train loss: 0.46354398131370544:  43%|████▎     | 164/383 [01:37<02:11,  1.66it/s]evaluate for the 6-th batch, evaluate loss: 0.6207066774368286:  25%|█████               | 5/20 [00:01<00:04,  3.36it/s]evaluate for the 6-th batch, evaluate loss: 0.6207066774368286:  30%|██████              | 6/20 [00:01<00:04,  3.47it/s]Epoch: 2, train for the 53-th batch, train loss: 0.8602004051208496:  34%|████▍        | 52/151 [00:11<00:24,  4.11it/s]Epoch: 2, train for the 53-th batch, train loss: 0.8602004051208496:  35%|████▌        | 53/151 [00:11<00:23,  4.20it/s]Epoch: 1, train for the 172-th batch, train loss: 0.6242079138755798:  72%|███████▉   | 171/237 [01:39<00:39,  1.66it/s]Epoch: 1, train for the 172-th batch, train loss: 0.6242079138755798:  73%|███████▉   | 172/237 [01:39<00:39,  1.65it/s]Epoch: 2, train for the 54-th batch, train loss: 0.5894805192947388:  35%|████▌        | 53/151 [00:11<00:23,  4.20it/s]Epoch: 2, train for the 54-th batch, train loss: 0.5894805192947388:  36%|████▋        | 54/151 [00:11<00:22,  4.29it/s]Epoch: 2, train for the 20-th batch, train loss: 0.2946016490459442:  16%|██           | 19/119 [00:12<01:00,  1.66it/s]Epoch: 2, train for the 20-th batch, train loss: 0.2946016490459442:  17%|██▏          | 20/119 [00:12<00:59,  1.66it/s]evaluate for the 7-th batch, evaluate loss: 0.6627763509750366:  30%|██████              | 6/20 [00:02<00:04,  3.47it/s]evaluate for the 7-th batch, evaluate loss: 0.6627763509750366:  35%|███████             | 7/20 [00:02<00:03,  3.33it/s]Epoch: 2, train for the 55-th batch, train loss: 0.6227927207946777:  36%|████▋        | 54/151 [00:11<00:22,  4.29it/s]Epoch: 2, train for the 55-th batch, train loss: 0.6227927207946777:  36%|████▋        | 55/151 [00:11<00:22,  4.34it/s]Epoch: 1, train for the 165-th batch, train loss: 0.39068305492401123:  43%|████▎     | 164/383 [01:38<02:11,  1.66it/s]Epoch: 1, train for the 165-th batch, train loss: 0.39068305492401123:  43%|████▎     | 165/383 [01:38<02:11,  1.66it/s]evaluate for the 8-th batch, evaluate loss: 0.6462762355804443:  35%|███████             | 7/20 [00:02<00:03,  3.33it/s]evaluate for the 8-th batch, evaluate loss: 0.6462762355804443:  40%|████████            | 8/20 [00:02<00:03,  3.43it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5667177438735962:  36%|████▋        | 55/151 [00:11<00:22,  4.34it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5667177438735962:  37%|████▊        | 56/151 [00:11<00:21,  4.42it/s]Epoch: 1, train for the 173-th batch, train loss: 0.6172446012496948:  73%|███████▉   | 172/237 [01:40<00:39,  1.65it/s]Epoch: 1, train for the 173-th batch, train loss: 0.6172446012496948:  73%|████████   | 173/237 [01:40<00:38,  1.65it/s]Epoch: 2, train for the 57-th batch, train loss: 0.6334810256958008:  37%|████▊        | 56/151 [00:12<00:21,  4.42it/s]Epoch: 2, train for the 57-th batch, train loss: 0.6334810256958008:  38%|████▉        | 57/151 [00:12<00:20,  4.51it/s]Epoch: 2, train for the 21-th batch, train loss: 0.2972179353237152:  17%|██▏          | 20/119 [00:12<00:59,  1.66it/s]Epoch: 2, train for the 21-th batch, train loss: 0.2972179353237152:  18%|██▎          | 21/119 [00:12<00:58,  1.67it/s]evaluate for the 9-th batch, evaluate loss: 0.5910258293151855:  40%|████████            | 8/20 [00:02<00:03,  3.43it/s]evaluate for the 9-th batch, evaluate loss: 0.5910258293151855:  45%|█████████           | 9/20 [00:02<00:03,  3.34it/s]Epoch: 2, train for the 58-th batch, train loss: 0.6245000958442688:  38%|████▉        | 57/151 [00:12<00:20,  4.51it/s]Epoch: 2, train for the 58-th batch, train loss: 0.6245000958442688:  38%|████▉        | 58/151 [00:12<00:20,  4.52it/s]Epoch: 1, train for the 166-th batch, train loss: 0.4860081076622009:  43%|████▋      | 165/383 [01:39<02:11,  1.66it/s]Epoch: 1, train for the 166-th batch, train loss: 0.4860081076622009:  43%|████▊      | 166/383 [01:39<02:11,  1.65it/s]evaluate for the 10-th batch, evaluate loss: 0.6090149283409119:  45%|████████▌          | 9/20 [00:02<00:03,  3.34it/s]evaluate for the 10-th batch, evaluate loss: 0.6090149283409119:  50%|█████████         | 10/20 [00:02<00:02,  3.42it/s]Epoch: 1, train for the 174-th batch, train loss: 0.6107714772224426:  73%|████████   | 173/237 [01:41<00:38,  1.65it/s]Epoch: 1, train for the 174-th batch, train loss: 0.6107714772224426:  73%|████████   | 174/237 [01:41<00:38,  1.65it/s]Epoch: 2, train for the 59-th batch, train loss: 0.5907883048057556:  38%|████▉        | 58/151 [00:12<00:20,  4.52it/s]Epoch: 2, train for the 59-th batch, train loss: 0.5907883048057556:  39%|█████        | 59/151 [00:12<00:20,  4.51it/s]Epoch: 2, train for the 22-th batch, train loss: 0.28851062059402466:  18%|██          | 21/119 [00:13<00:58,  1.67it/s]Epoch: 2, train for the 22-th batch, train loss: 0.28851062059402466:  18%|██▏         | 22/119 [00:13<00:58,  1.67it/s]evaluate for the 11-th batch, evaluate loss: 0.5971488356590271:  50%|█████████         | 10/20 [00:03<00:02,  3.42it/s]evaluate for the 11-th batch, evaluate loss: 0.5971488356590271:  55%|█████████▉        | 11/20 [00:03<00:02,  3.34it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5458495020866394:  39%|█████        | 59/151 [00:12<00:20,  4.51it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5458495020866394:  40%|█████▏       | 60/151 [00:12<00:20,  4.53it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5934447646141052:  40%|█████▏       | 60/151 [00:13<00:20,  4.53it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5934447646141052:  40%|█████▎       | 61/151 [00:13<00:19,  4.52it/s]Epoch: 1, train for the 167-th batch, train loss: 0.4522983133792877:  43%|████▊      | 166/383 [01:39<02:11,  1.65it/s]Epoch: 1, train for the 167-th batch, train loss: 0.4522983133792877:  44%|████▊      | 167/383 [01:39<02:11,  1.64it/s]evaluate for the 12-th batch, evaluate loss: 0.6252157688140869:  55%|█████████▉        | 11/20 [00:03<00:02,  3.34it/s]evaluate for the 12-th batch, evaluate loss: 0.6252157688140869:  60%|██████████▊       | 12/20 [00:03<00:02,  3.41it/s]Epoch: 1, train for the 175-th batch, train loss: 0.6298452019691467:  73%|████████   | 174/237 [01:41<00:38,  1.65it/s]Epoch: 1, train for the 175-th batch, train loss: 0.6298452019691467:  74%|████████   | 175/237 [01:41<00:37,  1.64it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5888245105743408:  40%|█████▎       | 61/151 [00:13<00:19,  4.52it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5888245105743408:  41%|█████▎       | 62/151 [00:13<00:19,  4.51it/s]Epoch: 2, train for the 23-th batch, train loss: 0.30765488743782043:  18%|██▏         | 22/119 [00:13<00:58,  1.67it/s]Epoch: 2, train for the 23-th batch, train loss: 0.30765488743782043:  19%|██▎         | 23/119 [00:13<00:57,  1.68it/s]evaluate for the 13-th batch, evaluate loss: 0.6372596621513367:  60%|██████████▊       | 12/20 [00:03<00:02,  3.41it/s]evaluate for the 13-th batch, evaluate loss: 0.6372596621513367:  65%|███████████▋      | 13/20 [00:03<00:02,  3.34it/s]Epoch: 2, train for the 63-th batch, train loss: 0.5755885243415833:  41%|█████▎       | 62/151 [00:13<00:19,  4.51it/s]Epoch: 2, train for the 63-th batch, train loss: 0.5755885243415833:  42%|█████▍       | 63/151 [00:13<00:19,  4.43it/s]evaluate for the 14-th batch, evaluate loss: 0.6229235529899597:  65%|███████████▋      | 13/20 [00:04<00:02,  3.34it/s]evaluate for the 14-th batch, evaluate loss: 0.6229235529899597:  70%|████████████▌     | 14/20 [00:04<00:01,  3.41it/s]Epoch: 1, train for the 168-th batch, train loss: 0.3499821424484253:  44%|████▊      | 167/383 [01:40<02:11,  1.64it/s]Epoch: 1, train for the 168-th batch, train loss: 0.3499821424484253:  44%|████▊      | 168/383 [01:40<02:10,  1.64it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5755895972251892:  42%|█████▍       | 63/151 [00:13<00:19,  4.43it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5755895972251892:  42%|█████▌       | 64/151 [00:13<00:19,  4.37it/s]Epoch: 1, train for the 176-th batch, train loss: 0.6353009939193726:  74%|████████   | 175/237 [01:42<00:37,  1.64it/s]Epoch: 1, train for the 176-th batch, train loss: 0.6353009939193726:  74%|████████▏  | 176/237 [01:42<00:37,  1.64it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5317866206169128:  42%|█████▌       | 64/151 [00:13<00:19,  4.37it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5317866206169128:  43%|█████▌       | 65/151 [00:13<00:19,  4.40it/s]Epoch: 2, train for the 24-th batch, train loss: 0.2628845274448395:  19%|██▌          | 23/119 [00:14<00:57,  1.68it/s]Epoch: 2, train for the 24-th batch, train loss: 0.2628845274448395:  20%|██▌          | 24/119 [00:14<00:56,  1.68it/s]evaluate for the 15-th batch, evaluate loss: 0.6077431440353394:  70%|████████████▌     | 14/20 [00:04<00:01,  3.41it/s]evaluate for the 15-th batch, evaluate loss: 0.6077431440353394:  75%|█████████████▌    | 15/20 [00:04<00:01,  3.34it/s]Epoch: 2, train for the 66-th batch, train loss: 0.5841975212097168:  43%|█████▌       | 65/151 [00:14<00:19,  4.40it/s]Epoch: 2, train for the 66-th batch, train loss: 0.5841975212097168:  44%|█████▋       | 66/151 [00:14<00:19,  4.42it/s]evaluate for the 16-th batch, evaluate loss: 0.5902127027511597:  75%|█████████████▌    | 15/20 [00:04<00:01,  3.34it/s]evaluate for the 16-th batch, evaluate loss: 0.5902127027511597:  80%|██████████████▍   | 16/20 [00:04<00:01,  3.41it/s]Epoch: 1, train for the 169-th batch, train loss: 0.41436251997947693:  44%|████▍     | 168/383 [01:40<02:10,  1.64it/s]Epoch: 1, train for the 169-th batch, train loss: 0.41436251997947693:  44%|████▍     | 169/383 [01:40<02:10,  1.65it/s]Epoch: 2, train for the 67-th batch, train loss: 0.6031311750411987:  44%|█████▋       | 66/151 [00:14<00:19,  4.42it/s]Epoch: 2, train for the 67-th batch, train loss: 0.6031311750411987:  44%|█████▊       | 67/151 [00:14<00:18,  4.44it/s]Epoch: 1, train for the 177-th batch, train loss: 0.6555452346801758:  74%|████████▏  | 176/237 [01:42<00:37,  1.64it/s]Epoch: 1, train for the 177-th batch, train loss: 0.6555452346801758:  75%|████████▏  | 177/237 [01:42<00:36,  1.64it/s]evaluate for the 17-th batch, evaluate loss: 0.5962573289871216:  80%|██████████████▍   | 16/20 [00:05<00:01,  3.41it/s]evaluate for the 17-th batch, evaluate loss: 0.5962573289871216:  85%|███████████████▎  | 17/20 [00:05<00:00,  3.37it/s]Epoch: 2, train for the 25-th batch, train loss: 0.25377586483955383:  20%|██▍         | 24/119 [00:15<00:56,  1.68it/s]Epoch: 2, train for the 25-th batch, train loss: 0.25377586483955383:  21%|██▌         | 25/119 [00:15<00:55,  1.69it/s]Epoch: 2, train for the 68-th batch, train loss: 0.5717475414276123:  44%|█████▊       | 67/151 [00:14<00:18,  4.44it/s]Epoch: 2, train for the 68-th batch, train loss: 0.5717475414276123:  45%|█████▊       | 68/151 [00:14<00:18,  4.42it/s]evaluate for the 18-th batch, evaluate loss: 0.6623435020446777:  85%|███████████████▎  | 17/20 [00:05<00:00,  3.37it/s]evaluate for the 18-th batch, evaluate loss: 0.6623435020446777:  90%|████████████████▏ | 18/20 [00:05<00:00,  3.45it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5487546920776367:  45%|█████▊       | 68/151 [00:14<00:18,  4.42it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5487546920776367:  46%|█████▉       | 69/151 [00:14<00:18,  4.49it/s]Epoch: 1, train for the 170-th batch, train loss: 0.42963966727256775:  44%|████▍     | 169/383 [01:41<02:10,  1.65it/s]Epoch: 1, train for the 170-th batch, train loss: 0.42963966727256775:  44%|████▍     | 170/383 [01:41<02:09,  1.64it/s]Epoch: 1, train for the 178-th batch, train loss: 0.6524593234062195:  75%|████████▏  | 177/237 [01:43<00:36,  1.64it/s]Epoch: 1, train for the 178-th batch, train loss: 0.6524593234062195:  75%|████████▎  | 178/237 [01:43<00:36,  1.64it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5644955039024353:  46%|█████▉       | 69/151 [00:15<00:18,  4.49it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5644955039024353:  46%|██████       | 70/151 [00:15<00:18,  4.48it/s]evaluate for the 19-th batch, evaluate loss: 0.6507173180580139:  90%|████████████████▏ | 18/20 [00:05<00:00,  3.45it/s]evaluate for the 19-th batch, evaluate loss: 0.6507173180580139:  95%|█████████████████ | 19/20 [00:05<00:00,  3.40it/s]Epoch: 2, train for the 26-th batch, train loss: 0.22756117582321167:  21%|██▌         | 25/119 [00:15<00:55,  1.69it/s]Epoch: 2, train for the 26-th batch, train loss: 0.22756117582321167:  22%|██▌         | 26/119 [00:15<00:55,  1.69it/s]evaluate for the 20-th batch, evaluate loss: 0.6349483132362366:  95%|█████████████████ | 19/20 [00:05<00:00,  3.40it/s]evaluate for the 20-th batch, evaluate loss: 0.6349483132362366: 100%|██████████████████| 20/20 [00:05<00:00,  3.53it/s]
INFO:root:Epoch: 1, learning rate: 0.0001, train loss: 0.5649
INFO:root:train average_precision, 0.8086
INFO:root:train roc_auc, 0.7854
INFO:root:validate loss: 0.5269
INFO:root:validate average_precision, 0.8323
INFO:root:validate roc_auc, 0.7954
INFO:root:new node validate loss: 0.6178
INFO:root:new node validate first_1_average_precision, 0.6392
INFO:root:new node validate first_1_roc_auc, 0.5524
INFO:root:new node validate first_3_average_precision, 0.6936
INFO:root:new node validate first_3_roc_auc, 0.6170
INFO:root:new node validate first_10_average_precision, 0.7365
INFO:root:new node validate first_10_roc_auc, 0.6751
INFO:root:new node validate average_precision, 0.7463
INFO:root:new node validate roc_auc, 0.7018
INFO:root:save model ./saved_models/DyGFormer/ia-escorts-dynamic/DyGFormer_seed0_dygformer-ia-escorts-dynamic-old/DyGFormer_seed0_dygformer-ia-escorts-dynamic-old.pkl
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5838508605957031:  46%|██████       | 70/151 [00:15<00:18,  4.48it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5838508605957031:  47%|██████       | 71/151 [00:15<00:17,  4.46it/s]Epoch: 1, train for the 171-th batch, train loss: 0.5250083804130554:  44%|████▉      | 170/383 [01:42<02:09,  1.64it/s]Epoch: 1, train for the 171-th batch, train loss: 0.5250083804130554:  45%|████▉      | 171/383 [01:42<02:08,  1.65it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5232452154159546:  47%|██████       | 71/151 [00:15<00:17,  4.46it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5232452154159546:  48%|██████▏      | 72/151 [00:15<00:17,  4.48it/s]Epoch: 1, train for the 179-th batch, train loss: 0.6279367208480835:  75%|████████▎  | 178/237 [01:44<00:36,  1.64it/s]Epoch: 1, train for the 179-th batch, train loss: 0.6279367208480835:  76%|████████▎  | 179/237 [01:44<00:35,  1.64it/s]Epoch: 2, train for the 27-th batch, train loss: 0.30403631925582886:  22%|██▌         | 26/119 [00:16<00:55,  1.69it/s]Epoch: 2, train for the 27-th batch, train loss: 0.30403631925582886:  23%|██▋         | 27/119 [00:16<00:53,  1.72it/s]Epoch: 2, train for the 1-th batch, train loss: 0.7512427568435669:   0%|                       | 0/146 [00:00<?, ?it/s]Epoch: 2, train for the 1-th batch, train loss: 0.7512427568435669:   1%|               | 1/146 [00:00<01:08,  2.12it/s]Epoch: 2, train for the 73-th batch, train loss: 0.5504478812217712:  48%|██████▏      | 72/151 [00:15<00:17,  4.48it/s]Epoch: 2, train for the 73-th batch, train loss: 0.5504478812217712:  48%|██████▎      | 73/151 [00:15<00:17,  4.46it/s]Epoch: 2, train for the 74-th batch, train loss: 0.5150780081748962:  48%|██████▎      | 73/151 [00:16<00:17,  4.46it/s]Epoch: 2, train for the 74-th batch, train loss: 0.5150780081748962:  49%|██████▎      | 74/151 [00:16<00:17,  4.45it/s]Epoch: 1, train for the 172-th batch, train loss: 0.37277528643608093:  45%|████▍     | 171/383 [01:42<02:08,  1.65it/s]Epoch: 1, train for the 172-th batch, train loss: 0.37277528643608093:  45%|████▍     | 172/383 [01:42<02:08,  1.64it/s]Epoch: 2, train for the 75-th batch, train loss: 0.5262082815170288:  49%|██████▎      | 74/151 [00:16<00:17,  4.45it/s]Epoch: 2, train for the 75-th batch, train loss: 0.5262082815170288:  50%|██████▍      | 75/151 [00:16<00:16,  4.48it/s]Epoch: 1, train for the 180-th batch, train loss: 0.6557378768920898:  76%|████████▎  | 179/237 [01:44<00:35,  1.64it/s]Epoch: 1, train for the 180-th batch, train loss: 0.6557378768920898:  76%|████████▎  | 180/237 [01:44<00:34,  1.64it/s]Epoch: 2, train for the 2-th batch, train loss: 0.5486869812011719:   1%|               | 1/146 [00:01<01:08,  2.12it/s]Epoch: 2, train for the 2-th batch, train loss: 0.5486869812011719:   1%|▏              | 2/146 [00:01<01:15,  1.91it/s]Epoch: 2, train for the 28-th batch, train loss: 0.29272007942199707:  23%|██▋         | 27/119 [00:16<00:53,  1.72it/s]Epoch: 2, train for the 28-th batch, train loss: 0.29272007942199707:  24%|██▊         | 28/119 [00:16<00:54,  1.67it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5696712136268616:  50%|██████▍      | 75/151 [00:16<00:16,  4.48it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5696712136268616:  50%|██████▌      | 76/151 [00:16<00:16,  4.49it/s]Epoch: 2, train for the 77-th batch, train loss: 0.5272977948188782:  50%|██████▌      | 76/151 [00:16<00:16,  4.49it/s]Epoch: 2, train for the 77-th batch, train loss: 0.5272977948188782:  51%|██████▋      | 77/151 [00:16<00:16,  4.50it/s]Epoch: 1, train for the 173-th batch, train loss: 0.4834229052066803:  45%|████▉      | 172/383 [01:43<02:08,  1.64it/s]Epoch: 1, train for the 173-th batch, train loss: 0.4834229052066803:  45%|████▉      | 173/383 [01:43<02:07,  1.65it/s]Epoch: 2, train for the 3-th batch, train loss: 0.4675410985946655:   1%|▏              | 2/146 [00:01<01:15,  1.91it/s]Epoch: 2, train for the 3-th batch, train loss: 0.4675410985946655:   2%|▎              | 3/146 [00:01<01:16,  1.87it/s]Epoch: 2, train for the 78-th batch, train loss: 0.500662088394165:  51%|███████▏      | 77/151 [00:16<00:16,  4.50it/s]Epoch: 2, train for the 78-th batch, train loss: 0.500662088394165:  52%|███████▏      | 78/151 [00:16<00:16,  4.53it/s]Epoch: 1, train for the 181-th batch, train loss: 0.644119381904602:  76%|█████████   | 180/237 [01:45<00:34,  1.64it/s]Epoch: 1, train for the 181-th batch, train loss: 0.644119381904602:  76%|█████████▏  | 181/237 [01:45<00:34,  1.65it/s]Epoch: 2, train for the 29-th batch, train loss: 0.2695397436618805:  24%|███          | 28/119 [00:17<00:54,  1.67it/s]Epoch: 2, train for the 29-th batch, train loss: 0.2695397436618805:  24%|███▏         | 29/119 [00:17<00:53,  1.67it/s]Epoch: 2, train for the 79-th batch, train loss: 0.616608738899231:  52%|███████▏      | 78/151 [00:17<00:16,  4.53it/s]Epoch: 2, train for the 79-th batch, train loss: 0.616608738899231:  52%|███████▎      | 79/151 [00:17<00:15,  4.53it/s]Epoch: 2, train for the 80-th batch, train loss: 0.5570867657661438:  52%|██████▊      | 79/151 [00:17<00:15,  4.53it/s]Epoch: 2, train for the 80-th batch, train loss: 0.5570867657661438:  53%|██████▉      | 80/151 [00:17<00:15,  4.52it/s]Epoch: 1, train for the 174-th batch, train loss: 0.48450523614883423:  45%|████▌     | 173/383 [01:44<02:07,  1.65it/s]Epoch: 1, train for the 174-th batch, train loss: 0.48450523614883423:  45%|████▌     | 174/383 [01:44<02:07,  1.64it/s]Epoch: 2, train for the 4-th batch, train loss: 0.5743270516395569:   2%|▎              | 3/146 [00:02<01:16,  1.87it/s]Epoch: 2, train for the 4-th batch, train loss: 0.5743270516395569:   3%|▍              | 4/146 [00:02<01:18,  1.81it/s]Epoch: 1, train for the 182-th batch, train loss: 0.6434115171432495:  76%|████████▍  | 181/237 [01:45<00:34,  1.65it/s]Epoch: 1, train for the 182-th batch, train loss: 0.6434115171432495:  77%|████████▍  | 182/237 [01:45<00:33,  1.64it/s]Epoch: 2, train for the 81-th batch, train loss: 0.5418787002563477:  53%|██████▉      | 80/151 [00:17<00:15,  4.52it/s]Epoch: 2, train for the 81-th batch, train loss: 0.5418787002563477:  54%|██████▉      | 81/151 [00:17<00:15,  4.52it/s]Epoch: 2, train for the 30-th batch, train loss: 0.2721511721611023:  24%|███▏         | 29/119 [00:18<00:53,  1.67it/s]Epoch: 2, train for the 30-th batch, train loss: 0.2721511721611023:  25%|███▎         | 30/119 [00:18<00:53,  1.66it/s]Epoch: 2, train for the 82-th batch, train loss: 0.5457587838172913:  54%|██████▉      | 81/151 [00:17<00:15,  4.52it/s]Epoch: 2, train for the 82-th batch, train loss: 0.5457587838172913:  54%|███████      | 82/151 [00:17<00:17,  4.05it/s]Epoch: 1, train for the 183-th batch, train loss: 0.6629257798194885:  77%|████████▍  | 182/237 [01:46<00:33,  1.64it/s]Epoch: 1, train for the 183-th batch, train loss: 0.6629257798194885:  77%|████████▍  | 183/237 [01:46<00:30,  1.77it/s]Epoch: 2, train for the 5-th batch, train loss: 0.5252664089202881:   3%|▍              | 4/146 [00:02<01:18,  1.81it/s]Epoch: 2, train for the 5-th batch, train loss: 0.5252664089202881:   3%|▌              | 5/146 [00:02<01:16,  1.84it/s]Epoch: 1, train for the 175-th batch, train loss: 0.3965793251991272:  45%|████▉      | 174/383 [01:44<02:07,  1.64it/s]Epoch: 1, train for the 175-th batch, train loss: 0.3965793251991272:  46%|█████      | 175/383 [01:44<02:13,  1.55it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5738587379455566:  54%|███████      | 82/151 [00:18<00:17,  4.05it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5738587379455566:  55%|███████▏     | 83/151 [00:18<00:16,  4.16it/s]Epoch: 2, train for the 31-th batch, train loss: 0.2505679726600647:  25%|███▎         | 30/119 [00:18<00:53,  1.66it/s]Epoch: 2, train for the 31-th batch, train loss: 0.2505679726600647:  26%|███▍         | 31/119 [00:18<00:52,  1.67it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5788630247116089:  55%|███████▏     | 83/151 [00:18<00:16,  4.16it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5788630247116089:  56%|███████▏     | 84/151 [00:18<00:15,  4.25it/s]Epoch: 2, train for the 6-th batch, train loss: 0.538864254951477:   3%|▌               | 5/146 [00:03<01:16,  1.84it/s]Epoch: 2, train for the 6-th batch, train loss: 0.538864254951477:   4%|▋               | 6/146 [00:03<01:15,  1.84it/s]Epoch: 2, train for the 85-th batch, train loss: 0.5501420497894287:  56%|███████▏     | 84/151 [00:18<00:15,  4.25it/s]Epoch: 2, train for the 85-th batch, train loss: 0.5501420497894287:  56%|███████▎     | 85/151 [00:18<00:15,  4.32it/s]Epoch: 1, train for the 184-th batch, train loss: 0.6409754753112793:  77%|████████▍  | 183/237 [01:47<00:30,  1.77it/s]Epoch: 1, train for the 184-th batch, train loss: 0.6409754753112793:  78%|████████▌  | 184/237 [01:47<00:30,  1.76it/s]Epoch: 1, train for the 176-th batch, train loss: 0.43237462639808655:  46%|████▌     | 175/383 [01:45<02:13,  1.55it/s]Epoch: 1, train for the 176-th batch, train loss: 0.43237462639808655:  46%|████▌     | 176/383 [01:45<02:10,  1.59it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5229760408401489:  56%|███████▎     | 85/151 [00:18<00:15,  4.32it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5229760408401489:  57%|███████▍     | 86/151 [00:18<00:14,  4.37it/s]Epoch: 2, train for the 32-th batch, train loss: 0.22572334110736847:  26%|███▏        | 31/119 [00:19<00:52,  1.67it/s]Epoch: 2, train for the 32-th batch, train loss: 0.22572334110736847:  27%|███▏        | 32/119 [00:19<00:51,  1.68it/s]Epoch: 2, train for the 87-th batch, train loss: 0.5294750928878784:  57%|███████▍     | 86/151 [00:18<00:14,  4.37it/s]Epoch: 2, train for the 87-th batch, train loss: 0.5294750928878784:  58%|███████▍     | 87/151 [00:18<00:14,  4.41it/s]Epoch: 2, train for the 7-th batch, train loss: 0.603022038936615:   4%|▋               | 6/146 [00:03<01:15,  1.84it/s]Epoch: 2, train for the 7-th batch, train loss: 0.603022038936615:   5%|▊               | 7/146 [00:03<01:16,  1.81it/s]Epoch: 1, train for the 185-th batch, train loss: 0.6426064968109131:  78%|████████▌  | 184/237 [01:47<00:30,  1.76it/s]Epoch: 1, train for the 185-th batch, train loss: 0.6426064968109131:  78%|████████▌  | 185/237 [01:47<00:29,  1.75it/s]Epoch: 2, train for the 88-th batch, train loss: 0.5637295842170715:  58%|███████▍     | 87/151 [00:19<00:14,  4.41it/s]Epoch: 2, train for the 88-th batch, train loss: 0.5637295842170715:  58%|███████▌     | 88/151 [00:19<00:14,  4.45it/s]Epoch: 1, train for the 177-th batch, train loss: 0.43264028429985046:  46%|████▌     | 176/383 [01:45<02:10,  1.59it/s]Epoch: 1, train for the 177-th batch, train loss: 0.43264028429985046:  46%|████▌     | 177/383 [01:45<02:07,  1.62it/s]Epoch: 2, train for the 33-th batch, train loss: 0.23324595391750336:  27%|███▏        | 32/119 [00:19<00:51,  1.68it/s]Epoch: 2, train for the 33-th batch, train loss: 0.23324595391750336:  28%|███▎        | 33/119 [00:19<00:51,  1.66it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5282555818557739:  58%|███████▌     | 88/151 [00:19<00:14,  4.45it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5282555818557739:  59%|███████▋     | 89/151 [00:19<00:13,  4.45it/s]Epoch: 2, train for the 8-th batch, train loss: 0.49708136916160583:   5%|▋             | 7/146 [00:04<01:16,  1.81it/s]Epoch: 2, train for the 8-th batch, train loss: 0.49708136916160583:   5%|▊             | 8/146 [00:04<01:16,  1.81it/s]Epoch: 2, train for the 90-th batch, train loss: 0.5227718353271484:  59%|███████▋     | 89/151 [00:19<00:13,  4.45it/s]Epoch: 2, train for the 90-th batch, train loss: 0.5227718353271484:  60%|███████▋     | 90/151 [00:19<00:13,  4.51it/s]Epoch: 1, train for the 186-th batch, train loss: 0.6133451461791992:  78%|████████▌  | 185/237 [01:48<00:29,  1.75it/s]Epoch: 1, train for the 186-th batch, train loss: 0.6133451461791992:  78%|████████▋  | 186/237 [01:48<00:29,  1.74it/s]Epoch: 1, train for the 178-th batch, train loss: 0.4882343113422394:  46%|█████      | 177/383 [01:46<02:07,  1.62it/s]Epoch: 1, train for the 178-th batch, train loss: 0.4882343113422394:  46%|█████      | 178/383 [01:46<02:05,  1.64it/s]Epoch: 2, train for the 91-th batch, train loss: 0.5047364830970764:  60%|███████▋     | 90/151 [00:19<00:13,  4.51it/s]Epoch: 2, train for the 91-th batch, train loss: 0.5047364830970764:  60%|███████▊     | 91/151 [00:19<00:13,  4.51it/s]Epoch: 2, train for the 34-th batch, train loss: 0.24102796614170074:  28%|███▎        | 33/119 [00:20<00:51,  1.66it/s]Epoch: 2, train for the 34-th batch, train loss: 0.24102796614170074:  29%|███▍        | 34/119 [00:20<00:49,  1.71it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5480667948722839:  60%|███████▊     | 91/151 [00:20<00:13,  4.51it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5480667948722839:  61%|███████▉     | 92/151 [00:20<00:13,  4.50it/s]Epoch: 2, train for the 9-th batch, train loss: 0.501017689704895:   5%|▉               | 8/146 [00:04<01:16,  1.81it/s]Epoch: 2, train for the 9-th batch, train loss: 0.501017689704895:   6%|▉               | 9/146 [00:04<01:17,  1.77it/s]Epoch: 1, train for the 187-th batch, train loss: 0.6311617493629456:  78%|████████▋  | 186/237 [01:48<00:29,  1.74it/s]Epoch: 1, train for the 187-th batch, train loss: 0.6311617493629456:  79%|████████▋  | 187/237 [01:48<00:29,  1.72it/s]Epoch: 2, train for the 93-th batch, train loss: 0.5602739453315735:  61%|███████▉     | 92/151 [00:20<00:13,  4.50it/s]Epoch: 2, train for the 93-th batch, train loss: 0.5602739453315735:  62%|████████     | 93/151 [00:20<00:12,  4.48it/s]Epoch: 1, train for the 179-th batch, train loss: 0.40502864122390747:  46%|████▋     | 178/383 [01:47<02:05,  1.64it/s]Epoch: 1, train for the 179-th batch, train loss: 0.40502864122390747:  47%|████▋     | 179/383 [01:47<02:03,  1.65it/s]Epoch: 2, train for the 35-th batch, train loss: 0.21163004636764526:  29%|███▍        | 34/119 [00:20<00:49,  1.71it/s]Epoch: 2, train for the 35-th batch, train loss: 0.21163004636764526:  29%|███▌        | 35/119 [00:20<00:49,  1.70it/s]Epoch: 2, train for the 94-th batch, train loss: 0.5757970809936523:  62%|████████     | 93/151 [00:20<00:12,  4.48it/s]Epoch: 2, train for the 94-th batch, train loss: 0.5757970809936523:  62%|████████     | 94/151 [00:20<00:12,  4.49it/s]Epoch: 2, train for the 95-th batch, train loss: 0.4964739978313446:  62%|████████     | 94/151 [00:20<00:12,  4.49it/s]Epoch: 2, train for the 95-th batch, train loss: 0.4964739978313446:  63%|████████▏    | 95/151 [00:20<00:12,  4.48it/s]Epoch: 2, train for the 10-th batch, train loss: 0.46566691994667053:   6%|▊            | 9/146 [00:05<01:17,  1.77it/s]Epoch: 2, train for the 10-th batch, train loss: 0.46566691994667053:   7%|▊           | 10/146 [00:05<01:17,  1.76it/s]Epoch: 1, train for the 188-th batch, train loss: 0.6517091989517212:  79%|████████▋  | 187/237 [01:49<00:29,  1.72it/s]Epoch: 1, train for the 188-th batch, train loss: 0.6517091989517212:  79%|████████▋  | 188/237 [01:49<00:28,  1.70it/s]Epoch: 2, train for the 96-th batch, train loss: 0.5412791967391968:  63%|████████▏    | 95/151 [00:20<00:12,  4.48it/s]Epoch: 2, train for the 96-th batch, train loss: 0.5412791967391968:  64%|████████▎    | 96/151 [00:20<00:12,  4.50it/s]Epoch: 1, train for the 180-th batch, train loss: 0.46662771701812744:  47%|████▋     | 179/383 [01:47<02:03,  1.65it/s]Epoch: 1, train for the 180-th batch, train loss: 0.46662771701812744:  47%|████▋     | 180/383 [01:47<02:03,  1.65it/s]Epoch: 2, train for the 36-th batch, train loss: 0.23391841351985931:  29%|███▌        | 35/119 [00:21<00:49,  1.70it/s]Epoch: 2, train for the 36-th batch, train loss: 0.23391841351985931:  30%|███▋        | 36/119 [00:21<00:48,  1.71it/s]Epoch: 2, train for the 97-th batch, train loss: 0.5952917337417603:  64%|████████▎    | 96/151 [00:21<00:12,  4.50it/s]Epoch: 2, train for the 97-th batch, train loss: 0.5952917337417603:  64%|████████▎    | 97/151 [00:21<00:12,  4.49it/s]Epoch: 2, train for the 11-th batch, train loss: 0.48489701747894287:   7%|▊           | 10/146 [00:06<01:17,  1.76it/s]Epoch: 2, train for the 11-th batch, train loss: 0.48489701747894287:   8%|▉           | 11/146 [00:06<01:13,  1.83it/s]Epoch: 2, train for the 98-th batch, train loss: 0.578653872013092:  64%|████████▉     | 97/151 [00:21<00:12,  4.49it/s]Epoch: 2, train for the 98-th batch, train loss: 0.578653872013092:  65%|█████████     | 98/151 [00:21<00:11,  4.50it/s]Epoch: 1, train for the 189-th batch, train loss: 0.6381387710571289:  79%|████████▋  | 188/237 [01:49<00:28,  1.70it/s]Epoch: 1, train for the 189-th batch, train loss: 0.6381387710571289:  80%|████████▊  | 189/237 [01:49<00:28,  1.68it/s]Epoch: 2, train for the 99-th batch, train loss: 0.6077597737312317:  65%|████████▍    | 98/151 [00:21<00:11,  4.50it/s]Epoch: 2, train for the 99-th batch, train loss: 0.6077597737312317:  66%|████████▌    | 99/151 [00:21<00:11,  4.52it/s]Epoch: 1, train for the 181-th batch, train loss: 0.44257476925849915:  47%|████▋     | 180/383 [01:48<02:03,  1.65it/s]Epoch: 1, train for the 181-th batch, train loss: 0.44257476925849915:  47%|████▋     | 181/383 [01:48<02:02,  1.65it/s]Epoch: 2, train for the 37-th batch, train loss: 0.2253296822309494:  30%|███▉         | 36/119 [00:22<00:48,  1.71it/s]Epoch: 2, train for the 37-th batch, train loss: 0.2253296822309494:  31%|████         | 37/119 [00:22<00:49,  1.66it/s]Epoch: 2, train for the 100-th batch, train loss: 0.673112154006958:  66%|████████▌    | 99/151 [00:21<00:11,  4.52it/s]Epoch: 2, train for the 100-th batch, train loss: 0.673112154006958:  66%|███████▉    | 100/151 [00:21<00:11,  4.51it/s]Epoch: 2, train for the 12-th batch, train loss: 0.42586249113082886:   8%|▉           | 11/146 [00:06<01:13,  1.83it/s]Epoch: 2, train for the 12-th batch, train loss: 0.42586249113082886:   8%|▉           | 12/146 [00:06<01:15,  1.77it/s]Epoch: 2, train for the 101-th batch, train loss: 0.7010963559150696:  66%|███████▎   | 100/151 [00:22<00:11,  4.51it/s]Epoch: 2, train for the 101-th batch, train loss: 0.7010963559150696:  67%|███████▎   | 101/151 [00:22<00:11,  4.51it/s]Epoch: 1, train for the 190-th batch, train loss: 0.6267684698104858:  80%|████████▊  | 189/237 [01:50<00:28,  1.68it/s]Epoch: 1, train for the 190-th batch, train loss: 0.6267684698104858:  80%|████████▊  | 190/237 [01:50<00:27,  1.68it/s]Epoch: 1, train for the 182-th batch, train loss: 0.5731794834136963:  47%|█████▏     | 181/383 [01:48<02:02,  1.65it/s]Epoch: 1, train for the 182-th batch, train loss: 0.5731794834136963:  48%|█████▏     | 182/383 [01:48<02:01,  1.66it/s]Epoch: 2, train for the 102-th batch, train loss: 0.6058251857757568:  67%|███████▎   | 101/151 [00:22<00:11,  4.51it/s]Epoch: 2, train for the 102-th batch, train loss: 0.6058251857757568:  68%|███████▍   | 102/151 [00:22<00:10,  4.53it/s]Epoch: 2, train for the 38-th batch, train loss: 0.24660366773605347:  31%|███▋        | 37/119 [00:22<00:49,  1.66it/s]Epoch: 2, train for the 38-th batch, train loss: 0.24660366773605347:  32%|███▊        | 38/119 [00:22<00:48,  1.66it/s]Epoch: 2, train for the 13-th batch, train loss: 0.44405174255371094:   8%|▉           | 12/146 [00:07<01:15,  1.77it/s]Epoch: 2, train for the 13-th batch, train loss: 0.44405174255371094:   9%|█           | 13/146 [00:07<01:16,  1.74it/s]Epoch: 2, train for the 103-th batch, train loss: 0.6945213079452515:  68%|███████▍   | 102/151 [00:22<00:10,  4.53it/s]Epoch: 2, train for the 103-th batch, train loss: 0.6945213079452515:  68%|███████▌   | 103/151 [00:22<00:10,  4.51it/s]Epoch: 1, train for the 191-th batch, train loss: 0.6454454660415649:  80%|████████▊  | 190/237 [01:51<00:27,  1.68it/s]Epoch: 1, train for the 191-th batch, train loss: 0.6454454660415649:  81%|████████▊  | 191/237 [01:51<00:27,  1.67it/s]Epoch: 2, train for the 104-th batch, train loss: 0.6054686307907104:  68%|███████▌   | 103/151 [00:22<00:10,  4.51it/s]Epoch: 2, train for the 104-th batch, train loss: 0.6054686307907104:  69%|███████▌   | 104/151 [00:22<00:10,  4.55it/s]Epoch: 1, train for the 183-th batch, train loss: 0.44259893894195557:  48%|████▊     | 182/383 [01:49<02:01,  1.66it/s]Epoch: 1, train for the 183-th batch, train loss: 0.44259893894195557:  48%|████▊     | 183/383 [01:49<02:00,  1.66it/s]Epoch: 2, train for the 39-th batch, train loss: 0.25800275802612305:  32%|███▊        | 38/119 [00:23<00:48,  1.66it/s]Epoch: 2, train for the 39-th batch, train loss: 0.25800275802612305:  33%|███▉        | 39/119 [00:23<00:48,  1.66it/s]Epoch: 2, train for the 105-th batch, train loss: 0.6057426929473877:  69%|███████▌   | 104/151 [00:22<00:10,  4.55it/s]Epoch: 2, train for the 105-th batch, train loss: 0.6057426929473877:  70%|███████▋   | 105/151 [00:22<00:10,  4.54it/s]Epoch: 2, train for the 14-th batch, train loss: 0.4553850591182709:   9%|█▏           | 13/146 [00:07<01:16,  1.74it/s]Epoch: 2, train for the 14-th batch, train loss: 0.4553850591182709:  10%|█▏           | 14/146 [00:07<01:16,  1.72it/s]Epoch: 2, train for the 106-th batch, train loss: 0.5529956817626953:  70%|███████▋   | 105/151 [00:23<00:10,  4.54it/s]Epoch: 2, train for the 106-th batch, train loss: 0.5529956817626953:  70%|███████▋   | 106/151 [00:23<00:09,  4.53it/s]Epoch: 1, train for the 192-th batch, train loss: 0.647939920425415:  81%|█████████▋  | 191/237 [01:51<00:27,  1.67it/s]Epoch: 1, train for the 192-th batch, train loss: 0.647939920425415:  81%|█████████▋  | 192/237 [01:51<00:26,  1.68it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5604755282402039:  70%|███████▋   | 106/151 [00:23<00:09,  4.53it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5604755282402039:  71%|███████▊   | 107/151 [00:23<00:09,  4.53it/s]Epoch: 1, train for the 184-th batch, train loss: 0.4846108853816986:  48%|█████▎     | 183/383 [01:50<02:00,  1.66it/s]Epoch: 1, train for the 184-th batch, train loss: 0.4846108853816986:  48%|█████▎     | 184/383 [01:50<01:59,  1.66it/s]Epoch: 2, train for the 40-th batch, train loss: 0.22143425047397614:  33%|███▉        | 39/119 [00:23<00:48,  1.66it/s]Epoch: 2, train for the 40-th batch, train loss: 0.22143425047397614:  34%|████        | 40/119 [00:23<00:47,  1.66it/s]Epoch: 2, train for the 108-th batch, train loss: 0.5669885277748108:  71%|███████▊   | 107/151 [00:23<00:09,  4.53it/s]Epoch: 2, train for the 108-th batch, train loss: 0.5669885277748108:  72%|███████▊   | 108/151 [00:23<00:09,  4.52it/s]Epoch: 2, train for the 15-th batch, train loss: 0.4428350031375885:  10%|█▏           | 14/146 [00:08<01:16,  1.72it/s]Epoch: 2, train for the 15-th batch, train loss: 0.4428350031375885:  10%|█▎           | 15/146 [00:08<01:17,  1.70it/s]Epoch: 2, train for the 109-th batch, train loss: 0.5258674025535583:  72%|███████▊   | 108/151 [00:23<00:09,  4.52it/s]Epoch: 2, train for the 109-th batch, train loss: 0.5258674025535583:  72%|███████▉   | 109/151 [00:23<00:09,  4.52it/s]Epoch: 1, train for the 193-th batch, train loss: 0.6566066741943359:  81%|████████▉  | 192/237 [01:52<00:26,  1.68it/s]Epoch: 1, train for the 193-th batch, train loss: 0.6566066741943359:  81%|████████▉  | 193/237 [01:52<00:26,  1.68it/s]Epoch: 1, train for the 185-th batch, train loss: 0.4344477653503418:  48%|█████▎     | 184/383 [01:50<01:59,  1.66it/s]Epoch: 1, train for the 185-th batch, train loss: 0.4344477653503418:  48%|█████▎     | 185/383 [01:50<01:57,  1.68it/s]Epoch: 2, train for the 110-th batch, train loss: 0.586007297039032:  72%|████████▋   | 109/151 [00:24<00:09,  4.52it/s]Epoch: 2, train for the 110-th batch, train loss: 0.586007297039032:  73%|████████▋   | 110/151 [00:24<00:09,  4.51it/s]Epoch: 2, train for the 41-th batch, train loss: 0.29880812764167786:  34%|████        | 40/119 [00:24<00:47,  1.66it/s]Epoch: 2, train for the 41-th batch, train loss: 0.29880812764167786:  34%|████▏       | 41/119 [00:24<00:46,  1.66it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5359742641448975:  73%|████████   | 110/151 [00:24<00:09,  4.51it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5359742641448975:  74%|████████   | 111/151 [00:24<00:08,  4.50it/s]Epoch: 2, train for the 16-th batch, train loss: 0.45054930448532104:  10%|█▏          | 15/146 [00:09<01:17,  1.70it/s]Epoch: 2, train for the 16-th batch, train loss: 0.45054930448532104:  11%|█▎          | 16/146 [00:09<01:17,  1.68it/s]Epoch: 1, train for the 194-th batch, train loss: 0.6497159600257874:  81%|████████▉  | 193/237 [01:52<00:26,  1.68it/s]Epoch: 1, train for the 194-th batch, train loss: 0.6497159600257874:  82%|█████████  | 194/237 [01:52<00:25,  1.69it/s]Epoch: 2, train for the 112-th batch, train loss: 0.5327273607254028:  74%|████████   | 111/151 [00:24<00:08,  4.50it/s]Epoch: 2, train for the 112-th batch, train loss: 0.5327273607254028:  74%|████████▏  | 112/151 [00:24<00:08,  4.50it/s]Epoch: 1, train for the 186-th batch, train loss: 0.4542165696620941:  48%|█████▎     | 185/383 [01:51<01:57,  1.68it/s]Epoch: 1, train for the 186-th batch, train loss: 0.4542165696620941:  49%|█████▎     | 186/383 [01:51<01:57,  1.68it/s]Epoch: 2, train for the 113-th batch, train loss: 0.5479158759117126:  74%|████████▏  | 112/151 [00:24<00:08,  4.50it/s]Epoch: 2, train for the 113-th batch, train loss: 0.5479158759117126:  75%|████████▏  | 113/151 [00:24<00:08,  4.49it/s]Epoch: 2, train for the 42-th batch, train loss: 0.27818602323532104:  34%|████▏       | 41/119 [00:25<00:46,  1.66it/s]Epoch: 2, train for the 42-th batch, train loss: 0.27818602323532104:  35%|████▏       | 42/119 [00:25<00:46,  1.65it/s]Epoch: 2, train for the 17-th batch, train loss: 0.45247432589530945:  11%|█▎          | 16/146 [00:09<01:17,  1.68it/s]Epoch: 2, train for the 17-th batch, train loss: 0.45247432589530945:  12%|█▍          | 17/146 [00:09<01:15,  1.71it/s]Epoch: 2, train for the 114-th batch, train loss: 0.5182474255561829:  75%|████████▏  | 113/151 [00:24<00:08,  4.49it/s]Epoch: 2, train for the 114-th batch, train loss: 0.5182474255561829:  75%|████████▎  | 114/151 [00:24<00:08,  4.48it/s]Epoch: 1, train for the 195-th batch, train loss: 0.6418687105178833:  82%|█████████  | 194/237 [01:53<00:25,  1.69it/s]Epoch: 1, train for the 195-th batch, train loss: 0.6418687105178833:  82%|█████████  | 195/237 [01:53<00:24,  1.69it/s]Epoch: 2, train for the 115-th batch, train loss: 0.5203918218612671:  75%|████████▎  | 114/151 [00:25<00:08,  4.48it/s]Epoch: 2, train for the 115-th batch, train loss: 0.5203918218612671:  76%|████████▍  | 115/151 [00:25<00:08,  4.49it/s]Epoch: 1, train for the 187-th batch, train loss: 0.40528854727745056:  49%|████▊     | 186/383 [01:51<01:57,  1.68it/s]Epoch: 1, train for the 187-th batch, train loss: 0.40528854727745056:  49%|████▉     | 187/383 [01:51<01:56,  1.68it/s]Epoch: 2, train for the 43-th batch, train loss: 0.2559485137462616:  35%|████▌        | 42/119 [00:25<00:46,  1.65it/s]Epoch: 2, train for the 43-th batch, train loss: 0.2559485137462616:  36%|████▋        | 43/119 [00:25<00:45,  1.66it/s]Epoch: 2, train for the 116-th batch, train loss: 0.47653111815452576:  76%|███████▌  | 115/151 [00:25<00:08,  4.49it/s]Epoch: 2, train for the 116-th batch, train loss: 0.47653111815452576:  77%|███████▋  | 116/151 [00:25<00:07,  4.49it/s]Epoch: 2, train for the 18-th batch, train loss: 0.40438514947891235:  12%|█▍          | 17/146 [00:10<01:15,  1.71it/s]Epoch: 2, train for the 18-th batch, train loss: 0.40438514947891235:  12%|█▍          | 18/146 [00:10<01:15,  1.70it/s]Epoch: 2, train for the 117-th batch, train loss: 0.5249954462051392:  77%|████████▍  | 116/151 [00:25<00:07,  4.49it/s]Epoch: 2, train for the 117-th batch, train loss: 0.5249954462051392:  77%|████████▌  | 117/151 [00:25<00:07,  4.50it/s]Epoch: 1, train for the 196-th batch, train loss: 0.6431662440299988:  82%|█████████  | 195/237 [01:54<00:24,  1.69it/s]Epoch: 1, train for the 196-th batch, train loss: 0.6431662440299988:  83%|█████████  | 196/237 [01:54<00:24,  1.68it/s]Epoch: 1, train for the 188-th batch, train loss: 0.47925376892089844:  49%|████▉     | 187/383 [01:52<01:56,  1.68it/s]Epoch: 1, train for the 188-th batch, train loss: 0.47925376892089844:  49%|████▉     | 188/383 [01:52<01:56,  1.68it/s]Epoch: 2, train for the 118-th batch, train loss: 0.48773717880249023:  77%|███████▋  | 117/151 [00:25<00:07,  4.50it/s]Epoch: 2, train for the 118-th batch, train loss: 0.48773717880249023:  78%|███████▊  | 118/151 [00:25<00:07,  4.50it/s]Epoch: 2, train for the 44-th batch, train loss: 0.22254733741283417:  36%|████▎       | 43/119 [00:26<00:45,  1.66it/s]Epoch: 2, train for the 44-th batch, train loss: 0.22254733741283417:  37%|████▍       | 44/119 [00:26<00:44,  1.67it/s]Epoch: 2, train for the 19-th batch, train loss: 0.46192023158073425:  12%|█▍          | 18/146 [00:10<01:15,  1.70it/s]Epoch: 2, train for the 19-th batch, train loss: 0.46192023158073425:  13%|█▌          | 19/146 [00:10<01:15,  1.69it/s]Epoch: 2, train for the 119-th batch, train loss: 0.5310244560241699:  78%|████████▌  | 118/151 [00:26<00:07,  4.50it/s]Epoch: 2, train for the 119-th batch, train loss: 0.5310244560241699:  79%|████████▋  | 119/151 [00:26<00:07,  4.51it/s]Epoch: 1, train for the 197-th batch, train loss: 0.6262636184692383:  83%|█████████  | 196/237 [01:54<00:24,  1.68it/s]Epoch: 1, train for the 197-th batch, train loss: 0.6262636184692383:  83%|█████████▏ | 197/237 [01:54<00:23,  1.67it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5854488015174866:  79%|████████▋  | 119/151 [00:26<00:07,  4.51it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5854488015174866:  79%|████████▋  | 120/151 [00:26<00:06,  4.49it/s]Epoch: 1, train for the 189-th batch, train loss: 0.45768290758132935:  49%|████▉     | 188/383 [01:53<01:56,  1.68it/s]Epoch: 1, train for the 189-th batch, train loss: 0.45768290758132935:  49%|████▉     | 189/383 [01:53<01:56,  1.67it/s]Epoch: 2, train for the 121-th batch, train loss: 0.5247524380683899:  79%|████████▋  | 120/151 [00:26<00:06,  4.49it/s]Epoch: 2, train for the 121-th batch, train loss: 0.5247524380683899:  80%|████████▊  | 121/151 [00:26<00:06,  4.49it/s]Epoch: 2, train for the 45-th batch, train loss: 0.19770079851150513:  37%|████▍       | 44/119 [00:26<00:44,  1.67it/s]Epoch: 2, train for the 45-th batch, train loss: 0.19770079851150513:  38%|████▌       | 45/119 [00:26<00:44,  1.68it/s]Epoch: 2, train for the 20-th batch, train loss: 0.43098852038383484:  13%|█▌          | 19/146 [00:11<01:15,  1.69it/s]Epoch: 2, train for the 20-th batch, train loss: 0.43098852038383484:  14%|█▋          | 20/146 [00:11<01:14,  1.68it/s]Epoch: 2, train for the 122-th batch, train loss: 0.5396732091903687:  80%|████████▊  | 121/151 [00:26<00:06,  4.49it/s]Epoch: 2, train for the 122-th batch, train loss: 0.5396732091903687:  81%|████████▉  | 122/151 [00:26<00:06,  4.49it/s]Epoch: 1, train for the 198-th batch, train loss: 0.6360878944396973:  83%|█████████▏ | 197/237 [01:55<00:23,  1.67it/s]Epoch: 1, train for the 198-th batch, train loss: 0.6360878944396973:  84%|█████████▏ | 198/237 [01:55<00:23,  1.67it/s]Epoch: 2, train for the 123-th batch, train loss: 0.5236110091209412:  81%|████████▉  | 122/151 [00:26<00:06,  4.49it/s]Epoch: 2, train for the 123-th batch, train loss: 0.5236110091209412:  81%|████████▉  | 123/151 [00:26<00:06,  4.50it/s]Epoch: 1, train for the 190-th batch, train loss: 0.4246493875980377:  49%|█████▍     | 189/383 [01:53<01:56,  1.67it/s]Epoch: 1, train for the 190-th batch, train loss: 0.4246493875980377:  50%|█████▍     | 190/383 [01:53<01:55,  1.67it/s]Epoch: 2, train for the 46-th batch, train loss: 0.23719185590744019:  38%|████▌       | 45/119 [00:27<00:44,  1.68it/s]Epoch: 2, train for the 46-th batch, train loss: 0.23719185590744019:  39%|████▋       | 46/119 [00:27<00:43,  1.68it/s]Epoch: 2, train for the 124-th batch, train loss: 0.5361710786819458:  81%|████████▉  | 123/151 [00:27<00:06,  4.50it/s]Epoch: 2, train for the 124-th batch, train loss: 0.5361710786819458:  82%|█████████  | 124/151 [00:27<00:06,  4.49it/s]Epoch: 2, train for the 21-th batch, train loss: 0.46932554244995117:  14%|█▋          | 20/146 [00:11<01:14,  1.68it/s]Epoch: 2, train for the 21-th batch, train loss: 0.46932554244995117:  14%|█▋          | 21/146 [00:11<01:14,  1.68it/s]Epoch: 2, train for the 125-th batch, train loss: 0.5439518094062805:  82%|█████████  | 124/151 [00:27<00:06,  4.49it/s]Epoch: 2, train for the 125-th batch, train loss: 0.5439518094062805:  83%|█████████  | 125/151 [00:27<00:05,  4.48it/s]Epoch: 1, train for the 199-th batch, train loss: 0.6233201026916504:  84%|█████████▏ | 198/237 [01:55<00:23,  1.67it/s]Epoch: 1, train for the 199-th batch, train loss: 0.6233201026916504:  84%|█████████▏ | 199/237 [01:55<00:22,  1.67it/s]Epoch: 2, train for the 126-th batch, train loss: 0.5535820126533508:  83%|█████████  | 125/151 [00:27<00:05,  4.48it/s]Epoch: 2, train for the 126-th batch, train loss: 0.5535820126533508:  83%|█████████▏ | 126/151 [00:27<00:05,  4.48it/s]Epoch: 1, train for the 191-th batch, train loss: 0.38797685503959656:  50%|████▉     | 190/383 [01:54<01:55,  1.67it/s]Epoch: 1, train for the 191-th batch, train loss: 0.38797685503959656:  50%|████▉     | 191/383 [01:54<01:55,  1.66it/s]Epoch: 2, train for the 47-th batch, train loss: 0.2789760231971741:  39%|█████        | 46/119 [00:28<00:43,  1.68it/s]Epoch: 2, train for the 47-th batch, train loss: 0.2789760231971741:  39%|█████▏       | 47/119 [00:28<00:42,  1.68it/s]Epoch: 2, train for the 127-th batch, train loss: 0.5464392900466919:  83%|█████████▏ | 126/151 [00:27<00:05,  4.48it/s]Epoch: 2, train for the 127-th batch, train loss: 0.5464392900466919:  84%|█████████▎ | 127/151 [00:27<00:05,  4.48it/s]Epoch: 2, train for the 22-th batch, train loss: 0.4651867151260376:  14%|█▊           | 21/146 [00:12<01:14,  1.68it/s]Epoch: 2, train for the 22-th batch, train loss: 0.4651867151260376:  15%|█▉           | 22/146 [00:12<01:14,  1.67it/s]Epoch: 2, train for the 128-th batch, train loss: 0.6130624413490295:  84%|█████████▎ | 127/151 [00:28<00:05,  4.48it/s]Epoch: 2, train for the 128-th batch, train loss: 0.6130624413490295:  85%|█████████▎ | 128/151 [00:28<00:05,  4.50it/s]Epoch: 1, train for the 192-th batch, train loss: 0.5048694014549255:  50%|█████▍     | 191/383 [01:54<01:55,  1.66it/s]Epoch: 1, train for the 192-th batch, train loss: 0.5048694014549255:  50%|█████▌     | 192/383 [01:54<01:46,  1.80it/s]Epoch: 2, train for the 129-th batch, train loss: 0.5747711062431335:  85%|█████████▎ | 128/151 [00:28<00:05,  4.50it/s]Epoch: 2, train for the 129-th batch, train loss: 0.5747711062431335:  85%|█████████▍ | 129/151 [00:28<00:04,  4.49it/s]Epoch: 1, train for the 193-th batch, train loss: 0.4337674677371979:  50%|█████▌     | 192/383 [01:54<01:46,  1.80it/s]Epoch: 1, train for the 193-th batch, train loss: 0.4337674677371979:  50%|█████▌     | 193/383 [01:54<01:26,  2.19it/s]Epoch: 2, train for the 48-th batch, train loss: 0.27509790658950806:  39%|████▋       | 47/119 [00:28<00:42,  1.68it/s]Epoch: 2, train for the 48-th batch, train loss: 0.27509790658950806:  40%|████▊       | 48/119 [00:28<00:42,  1.68it/s]Epoch: 2, train for the 23-th batch, train loss: 0.4472672641277313:  15%|█▉           | 22/146 [00:13<01:14,  1.67it/s]Epoch: 2, train for the 23-th batch, train loss: 0.4472672641277313:  16%|██           | 23/146 [00:13<01:13,  1.68it/s]Epoch: 1, train for the 200-th batch, train loss: 0.6209627985954285:  84%|█████████▏ | 199/237 [01:56<00:22,  1.67it/s]Epoch: 1, train for the 200-th batch, train loss: 0.6209627985954285:  84%|█████████▎ | 200/237 [01:56<00:26,  1.40it/s]Epoch: 2, train for the 130-th batch, train loss: 0.5501070022583008:  85%|█████████▍ | 129/151 [00:28<00:04,  4.49it/s]Epoch: 2, train for the 130-th batch, train loss: 0.5501070022583008:  86%|█████████▍ | 130/151 [00:28<00:04,  4.49it/s]Epoch: 2, train for the 131-th batch, train loss: 0.533113420009613:  86%|██████████▎ | 130/151 [00:28<00:04,  4.49it/s]Epoch: 2, train for the 131-th batch, train loss: 0.533113420009613:  87%|██████████▍ | 131/151 [00:28<00:04,  4.49it/s]Epoch: 1, train for the 194-th batch, train loss: 0.377775639295578:  50%|██████      | 193/383 [01:55<01:26,  2.19it/s]Epoch: 1, train for the 194-th batch, train loss: 0.377775639295578:  51%|██████      | 194/383 [01:55<01:28,  2.14it/s]Epoch: 2, train for the 49-th batch, train loss: 0.23914527893066406:  40%|████▊       | 48/119 [00:29<00:42,  1.68it/s]Epoch: 2, train for the 49-th batch, train loss: 0.23914527893066406:  41%|████▉       | 49/119 [00:29<00:41,  1.68it/s]Epoch: 2, train for the 132-th batch, train loss: 0.5211650133132935:  87%|█████████▌ | 131/151 [00:28<00:04,  4.49it/s]Epoch: 2, train for the 132-th batch, train loss: 0.5211650133132935:  87%|█████████▌ | 132/151 [00:28<00:04,  4.50it/s]Epoch: 1, train for the 201-th batch, train loss: 0.6268751621246338:  84%|█████████▎ | 200/237 [01:57<00:26,  1.40it/s]Epoch: 1, train for the 201-th batch, train loss: 0.6268751621246338:  85%|█████████▎ | 201/237 [01:57<00:24,  1.50it/s]Epoch: 2, train for the 24-th batch, train loss: 0.45725128054618835:  16%|█▉          | 23/146 [00:13<01:13,  1.68it/s]Epoch: 2, train for the 24-th batch, train loss: 0.45725128054618835:  16%|█▉          | 24/146 [00:13<01:12,  1.68it/s]Epoch: 2, train for the 133-th batch, train loss: 0.5340335369110107:  87%|█████████▌ | 132/151 [00:29<00:04,  4.50it/s]Epoch: 2, train for the 133-th batch, train loss: 0.5340335369110107:  88%|█████████▋ | 133/151 [00:29<00:04,  4.49it/s]Epoch: 1, train for the 195-th batch, train loss: 0.46522870659828186:  51%|█████     | 194/383 [01:56<01:28,  2.14it/s]Epoch: 1, train for the 195-th batch, train loss: 0.46522870659828186:  51%|█████     | 195/383 [01:56<01:34,  2.00it/s]Epoch: 2, train for the 134-th batch, train loss: 0.5194713473320007:  88%|█████████▋ | 133/151 [00:29<00:04,  4.49it/s]Epoch: 2, train for the 134-th batch, train loss: 0.5194713473320007:  89%|█████████▊ | 134/151 [00:29<00:03,  4.49it/s]Epoch: 2, train for the 50-th batch, train loss: 0.2100125104188919:  41%|█████▎       | 49/119 [00:29<00:41,  1.68it/s]Epoch: 2, train for the 50-th batch, train loss: 0.2100125104188919:  42%|█████▍       | 50/119 [00:29<00:41,  1.68it/s]Epoch: 1, train for the 202-th batch, train loss: 0.6193917393684387:  85%|█████████▎ | 201/237 [01:58<00:24,  1.50it/s]Epoch: 1, train for the 202-th batch, train loss: 0.6193917393684387:  85%|█████████▍ | 202/237 [01:58<00:22,  1.57it/s]Epoch: 2, train for the 135-th batch, train loss: 0.5187169909477234:  89%|█████████▊ | 134/151 [00:29<00:03,  4.49it/s]Epoch: 2, train for the 135-th batch, train loss: 0.5187169909477234:  89%|█████████▊ | 135/151 [00:29<00:03,  4.49it/s]Epoch: 2, train for the 25-th batch, train loss: 0.47800958156585693:  16%|█▉          | 24/146 [00:14<01:12,  1.68it/s]Epoch: 2, train for the 25-th batch, train loss: 0.47800958156585693:  17%|██          | 25/146 [00:14<01:11,  1.68it/s]Epoch: 2, train for the 136-th batch, train loss: 0.5732870101928711:  89%|█████████▊ | 135/151 [00:29<00:03,  4.49it/s]Epoch: 2, train for the 136-th batch, train loss: 0.5732870101928711:  90%|█████████▉ | 136/151 [00:29<00:03,  4.49it/s]Epoch: 1, train for the 196-th batch, train loss: 0.4695850610733032:  51%|█████▌     | 195/383 [01:56<01:34,  2.00it/s]Epoch: 1, train for the 196-th batch, train loss: 0.4695850610733032:  51%|█████▋     | 196/383 [01:56<01:38,  1.90it/s]Epoch: 2, train for the 137-th batch, train loss: 0.6071335673332214:  90%|█████████▉ | 136/151 [00:30<00:03,  4.49it/s]Epoch: 2, train for the 137-th batch, train loss: 0.6071335673332214:  91%|█████████▉ | 137/151 [00:30<00:03,  4.49it/s]Epoch: 2, train for the 51-th batch, train loss: 0.2545374929904938:  42%|█████▍       | 50/119 [00:30<00:41,  1.68it/s]Epoch: 2, train for the 51-th batch, train loss: 0.2545374929904938:  43%|█████▌       | 51/119 [00:30<00:40,  1.68it/s]Epoch: 1, train for the 203-th batch, train loss: 0.6170759797096252:  85%|█████████▍ | 202/237 [01:58<00:22,  1.57it/s]Epoch: 1, train for the 203-th batch, train loss: 0.6170759797096252:  86%|█████████▍ | 203/237 [01:58<00:21,  1.62it/s]Epoch: 2, train for the 26-th batch, train loss: 0.4234273433685303:  17%|██▏          | 25/146 [00:14<01:11,  1.68it/s]Epoch: 2, train for the 26-th batch, train loss: 0.4234273433685303:  18%|██▎          | 26/146 [00:14<01:11,  1.68it/s]Epoch: 2, train for the 138-th batch, train loss: 0.6153271794319153:  91%|█████████▉ | 137/151 [00:30<00:03,  4.49it/s]Epoch: 2, train for the 138-th batch, train loss: 0.6153271794319153:  91%|██████████ | 138/151 [00:30<00:02,  4.49it/s]Epoch: 2, train for the 139-th batch, train loss: 0.5723987221717834:  91%|██████████ | 138/151 [00:30<00:02,  4.49it/s]Epoch: 2, train for the 139-th batch, train loss: 0.5723987221717834:  92%|██████████▏| 139/151 [00:30<00:02,  4.49it/s]Epoch: 1, train for the 197-th batch, train loss: 0.5147087574005127:  51%|█████▋     | 196/383 [01:57<01:38,  1.90it/s]Epoch: 1, train for the 197-th batch, train loss: 0.5147087574005127:  51%|█████▋     | 197/383 [01:57<01:43,  1.80it/s]Epoch: 2, train for the 52-th batch, train loss: 0.2897387742996216:  43%|█████▌       | 51/119 [00:31<00:40,  1.68it/s]Epoch: 2, train for the 52-th batch, train loss: 0.2897387742996216:  44%|█████▋       | 52/119 [00:31<00:39,  1.68it/s]Epoch: 2, train for the 140-th batch, train loss: 0.5396009087562561:  92%|██████████▏| 139/151 [00:30<00:02,  4.49it/s]Epoch: 2, train for the 140-th batch, train loss: 0.5396009087562561:  93%|██████████▏| 140/151 [00:30<00:02,  4.48it/s]Epoch: 1, train for the 204-th batch, train loss: 0.636942982673645:  86%|██████████▎ | 203/237 [01:59<00:21,  1.62it/s]Epoch: 1, train for the 204-th batch, train loss: 0.636942982673645:  86%|██████████▎ | 204/237 [01:59<00:20,  1.64it/s]Epoch: 2, train for the 27-th batch, train loss: 0.4602462947368622:  18%|██▎          | 26/146 [00:15<01:11,  1.68it/s]Epoch: 2, train for the 27-th batch, train loss: 0.4602462947368622:  18%|██▍          | 27/146 [00:15<01:11,  1.67it/s]Epoch: 2, train for the 141-th batch, train loss: 0.5468690395355225:  93%|██████████▏| 140/151 [00:30<00:02,  4.48it/s]Epoch: 2, train for the 141-th batch, train loss: 0.5468690395355225:  93%|██████████▎| 141/151 [00:30<00:02,  4.48it/s]Epoch: 2, train for the 142-th batch, train loss: 0.5549386739730835:  93%|██████████▎| 141/151 [00:31<00:02,  4.48it/s]Epoch: 2, train for the 142-th batch, train loss: 0.5549386739730835:  94%|██████████▎| 142/151 [00:31<00:02,  4.48it/s]Epoch: 1, train for the 198-th batch, train loss: 0.3931291103363037:  51%|█████▋     | 197/383 [01:57<01:43,  1.80it/s]Epoch: 1, train for the 198-th batch, train loss: 0.3931291103363037:  52%|█████▋     | 198/383 [01:57<01:46,  1.74it/s]Epoch: 2, train for the 53-th batch, train loss: 0.2019544392824173:  44%|█████▋       | 52/119 [00:31<00:39,  1.68it/s]Epoch: 2, train for the 53-th batch, train loss: 0.2019544392824173:  45%|█████▊       | 53/119 [00:31<00:39,  1.68it/s]Epoch: 1, train for the 205-th batch, train loss: 0.6477611660957336:  86%|█████████▍ | 204/237 [01:59<00:20,  1.64it/s]Epoch: 1, train for the 205-th batch, train loss: 0.6477611660957336:  86%|█████████▌ | 205/237 [01:59<00:19,  1.64it/s]Epoch: 2, train for the 143-th batch, train loss: 0.49394384026527405:  94%|█████████▍| 142/151 [00:31<00:02,  4.48it/s]Epoch: 2, train for the 143-th batch, train loss: 0.49394384026527405:  95%|█████████▍| 143/151 [00:31<00:01,  4.48it/s]Epoch: 2, train for the 28-th batch, train loss: 0.433674693107605:  18%|██▌           | 27/146 [00:16<01:11,  1.67it/s]Epoch: 2, train for the 28-th batch, train loss: 0.433674693107605:  19%|██▋           | 28/146 [00:16<01:10,  1.67it/s]Epoch: 2, train for the 144-th batch, train loss: 0.501608669757843:  95%|███████████▎| 143/151 [00:31<00:01,  4.48it/s]Epoch: 2, train for the 144-th batch, train loss: 0.501608669757843:  95%|███████████▍| 144/151 [00:31<00:01,  4.47it/s]Epoch: 1, train for the 199-th batch, train loss: 0.4144403040409088:  52%|█████▋     | 198/383 [01:58<01:46,  1.74it/s]Epoch: 1, train for the 199-th batch, train loss: 0.4144403040409088:  52%|█████▋     | 199/383 [01:58<01:47,  1.71it/s]Epoch: 2, train for the 145-th batch, train loss: 0.5203890204429626:  95%|██████████▍| 144/151 [00:31<00:01,  4.47it/s]Epoch: 2, train for the 145-th batch, train loss: 0.5203890204429626:  96%|██████████▌| 145/151 [00:31<00:01,  4.47it/s]Epoch: 2, train for the 54-th batch, train loss: 0.21215994656085968:  45%|█████▎      | 53/119 [00:32<00:39,  1.68it/s]Epoch: 2, train for the 54-th batch, train loss: 0.21215994656085968:  45%|█████▍      | 54/119 [00:32<00:38,  1.68it/s]Epoch: 1, train for the 206-th batch, train loss: 0.6354736685752869:  86%|█████████▌ | 205/237 [02:00<00:19,  1.64it/s]Epoch: 1, train for the 206-th batch, train loss: 0.6354736685752869:  87%|█████████▌ | 206/237 [02:00<00:18,  1.64it/s]Epoch: 2, train for the 29-th batch, train loss: 0.43053993582725525:  19%|██▎         | 28/146 [00:16<01:10,  1.67it/s]Epoch: 2, train for the 29-th batch, train loss: 0.43053993582725525:  20%|██▍         | 29/146 [00:16<01:10,  1.67it/s]Epoch: 2, train for the 146-th batch, train loss: 0.5112341046333313:  96%|██████████▌| 145/151 [00:32<00:01,  4.47it/s]Epoch: 2, train for the 146-th batch, train loss: 0.5112341046333313:  97%|██████████▋| 146/151 [00:32<00:01,  4.47it/s]Epoch: 2, train for the 147-th batch, train loss: 0.5654758214950562:  97%|██████████▋| 146/151 [00:32<00:01,  4.47it/s]Epoch: 2, train for the 147-th batch, train loss: 0.5654758214950562:  97%|██████████▋| 147/151 [00:32<00:00,  4.48it/s]Epoch: 1, train for the 207-th batch, train loss: 0.6468603610992432:  87%|█████████▌ | 206/237 [02:00<00:18,  1.64it/s]Epoch: 1, train for the 207-th batch, train loss: 0.6468603610992432:  87%|█████████▌ | 207/237 [02:00<00:17,  1.73it/s]Epoch: 2, train for the 55-th batch, train loss: 0.19161824882030487:  45%|█████▍      | 54/119 [00:32<00:38,  1.68it/s]Epoch: 2, train for the 55-th batch, train loss: 0.19161824882030487:  46%|█████▌      | 55/119 [00:32<00:38,  1.67it/s]Epoch: 2, train for the 148-th batch, train loss: 0.5752593278884888:  97%|██████████▋| 147/151 [00:32<00:00,  4.48it/s]Epoch: 2, train for the 148-th batch, train loss: 0.5752593278884888:  98%|██████████▊| 148/151 [00:32<00:00,  4.49it/s]Epoch: 1, train for the 200-th batch, train loss: 0.47592002153396606:  52%|█████▏    | 199/383 [01:59<01:47,  1.71it/s]Epoch: 1, train for the 200-th batch, train loss: 0.47592002153396606:  52%|█████▏    | 200/383 [01:59<02:00,  1.51it/s]Epoch: 2, train for the 30-th batch, train loss: 0.4460969567298889:  20%|██▌          | 29/146 [00:17<01:10,  1.67it/s]Epoch: 2, train for the 30-th batch, train loss: 0.4460969567298889:  21%|██▋          | 30/146 [00:17<01:09,  1.66it/s]Epoch: 2, train for the 149-th batch, train loss: 0.5467727780342102:  98%|██████████▊| 148/151 [00:32<00:00,  4.49it/s]Epoch: 2, train for the 149-th batch, train loss: 0.5467727780342102:  99%|██████████▊| 149/151 [00:32<00:00,  4.39it/s]Epoch: 1, train for the 208-th batch, train loss: 0.6364738941192627:  87%|█████████▌ | 207/237 [02:01<00:17,  1.73it/s]Epoch: 1, train for the 208-th batch, train loss: 0.6364738941192627:  88%|█████████▋ | 208/237 [02:01<00:14,  1.94it/s]Epoch: 2, train for the 150-th batch, train loss: 0.5070267915725708:  99%|██████████▊| 149/151 [00:32<00:00,  4.39it/s]Epoch: 2, train for the 150-th batch, train loss: 0.5070267915725708:  99%|██████████▉| 150/151 [00:32<00:00,  4.41it/s]Epoch: 2, train for the 56-th batch, train loss: 0.2333136349916458:  46%|██████       | 55/119 [00:33<00:38,  1.67it/s]Epoch: 2, train for the 56-th batch, train loss: 0.2333136349916458:  47%|██████       | 56/119 [00:33<00:37,  1.66it/s]Epoch: 2, train for the 151-th batch, train loss: 0.6048861145973206:  99%|██████████▉| 150/151 [00:33<00:00,  4.41it/s]Epoch: 2, train for the 151-th batch, train loss: 0.6048861145973206: 100%|███████████| 151/151 [00:33<00:00,  4.66it/s]Epoch: 2, train for the 151-th batch, train loss: 0.6048861145973206: 100%|███████████| 151/151 [00:33<00:00,  4.55it/s]
  0%|                                                                                            | 0/46 [00:00<?, ?it/s]Epoch: 2, train for the 31-th batch, train loss: 0.482476145029068:  21%|██▉           | 30/146 [00:17<01:09,  1.66it/s]Epoch: 2, train for the 31-th batch, train loss: 0.482476145029068:  21%|██▉           | 31/146 [00:17<01:09,  1.66it/s]evaluate for the 1-th batch, evaluate loss: 0.49650609493255615:   0%|                           | 0/46 [00:00<?, ?it/s]Epoch: 1, train for the 201-th batch, train loss: 0.4644826352596283:  52%|█████▋     | 200/383 [01:59<02:00,  1.51it/s]evaluate for the 1-th batch, evaluate loss: 0.49650609493255615:   2%|▍                  | 1/46 [00:00<00:04,  9.60it/s]Epoch: 1, train for the 201-th batch, train loss: 0.4644826352596283:  52%|█████▊     | 201/383 [01:59<01:58,  1.54it/s]evaluate for the 2-th batch, evaluate loss: 0.5036754012107849:   2%|▍                   | 1/46 [00:00<00:04,  9.60it/s]evaluate for the 2-th batch, evaluate loss: 0.5036754012107849:   4%|▊                   | 2/46 [00:00<00:04,  9.65it/s]Epoch: 1, train for the 209-th batch, train loss: 0.6304893493652344:  88%|█████████▋ | 208/237 [02:01<00:14,  1.94it/s]Epoch: 1, train for the 209-th batch, train loss: 0.6304893493652344:  88%|█████████▋ | 209/237 [02:01<00:15,  1.85it/s]evaluate for the 3-th batch, evaluate loss: 0.47533097863197327:   4%|▊                  | 2/46 [00:00<00:04,  9.65it/s]evaluate for the 3-th batch, evaluate loss: 0.47533097863197327:   7%|█▏                 | 3/46 [00:00<00:04,  9.67it/s]evaluate for the 4-th batch, evaluate loss: 0.5169950723648071:   7%|█▎                  | 3/46 [00:00<00:04,  9.67it/s]evaluate for the 4-th batch, evaluate loss: 0.5169950723648071:   9%|█▋                  | 4/46 [00:00<00:04,  9.68it/s]evaluate for the 5-th batch, evaluate loss: 0.47486403584480286:   9%|█▋                 | 4/46 [00:00<00:04,  9.68it/s]evaluate for the 5-th batch, evaluate loss: 0.47486403584480286:  11%|██                 | 5/46 [00:00<00:04,  9.61it/s]Epoch: 2, train for the 57-th batch, train loss: 0.18180973827838898:  47%|█████▋      | 56/119 [00:34<00:37,  1.66it/s]Epoch: 2, train for the 57-th batch, train loss: 0.18180973827838898:  48%|█████▋      | 57/119 [00:34<00:37,  1.66it/s]evaluate for the 6-th batch, evaluate loss: 0.5559995770454407:  11%|██▏                 | 5/46 [00:00<00:04,  9.61it/s]evaluate for the 6-th batch, evaluate loss: 0.5559995770454407:  13%|██▌                 | 6/46 [00:00<00:04,  9.63it/s]Epoch: 2, train for the 32-th batch, train loss: 0.4277501702308655:  21%|██▊          | 31/146 [00:18<01:09,  1.66it/s]Epoch: 2, train for the 32-th batch, train loss: 0.4277501702308655:  22%|██▊          | 32/146 [00:18<01:08,  1.66it/s]Epoch: 1, train for the 202-th batch, train loss: 0.5624734163284302:  52%|█████▊     | 201/383 [02:00<01:58,  1.54it/s]Epoch: 1, train for the 202-th batch, train loss: 0.5624734163284302:  53%|█████▊     | 202/383 [02:00<01:56,  1.56it/s]evaluate for the 7-th batch, evaluate loss: 0.5125254988670349:  13%|██▌                 | 6/46 [00:00<00:04,  9.63it/s]evaluate for the 7-th batch, evaluate loss: 0.5125254988670349:  15%|███                 | 7/46 [00:00<00:04,  9.67it/s]evaluate for the 8-th batch, evaluate loss: 0.5664043426513672:  15%|███                 | 7/46 [00:00<00:04,  9.67it/s]evaluate for the 8-th batch, evaluate loss: 0.5664043426513672:  17%|███▍                | 8/46 [00:00<00:03,  9.68it/s]Epoch: 1, train for the 210-th batch, train loss: 0.6500820517539978:  88%|█████████▋ | 209/237 [02:02<00:15,  1.85it/s]Epoch: 1, train for the 210-th batch, train loss: 0.6500820517539978:  89%|█████████▋ | 210/237 [02:02<00:15,  1.78it/s]evaluate for the 9-th batch, evaluate loss: 0.5267839431762695:  17%|███▍                | 8/46 [00:00<00:03,  9.68it/s]evaluate for the 9-th batch, evaluate loss: 0.5267839431762695:  20%|███▉                | 9/46 [00:00<00:03,  9.68it/s]evaluate for the 10-th batch, evaluate loss: 0.5348520278930664:  20%|███▋               | 9/46 [00:01<00:03,  9.68it/s]evaluate for the 10-th batch, evaluate loss: 0.5348520278930664:  22%|███▉              | 10/46 [00:01<00:03,  9.65it/s]evaluate for the 11-th batch, evaluate loss: 0.5277702808380127:  22%|███▉              | 10/46 [00:01<00:03,  9.65it/s]evaluate for the 11-th batch, evaluate loss: 0.5277702808380127:  24%|████▎             | 11/46 [00:01<00:03,  9.66it/s]Epoch: 2, train for the 58-th batch, train loss: 0.19861295819282532:  48%|█████▋      | 57/119 [00:34<00:37,  1.66it/s]Epoch: 2, train for the 58-th batch, train loss: 0.19861295819282532:  49%|█████▊      | 58/119 [00:34<00:36,  1.66it/s]evaluate for the 12-th batch, evaluate loss: 0.47971197962760925:  24%|████             | 11/46 [00:01<00:03,  9.66it/s]evaluate for the 12-th batch, evaluate loss: 0.47971197962760925:  26%|████▍            | 12/46 [00:01<00:03,  9.67it/s]Epoch: 2, train for the 33-th batch, train loss: 0.4664326608181:  22%|███▌            | 32/146 [00:19<01:08,  1.66it/s]Epoch: 2, train for the 33-th batch, train loss: 0.4664326608181:  23%|███▌            | 33/146 [00:19<01:07,  1.66it/s]Epoch: 1, train for the 203-th batch, train loss: 0.4891197085380554:  53%|█████▊     | 202/383 [02:01<01:56,  1.56it/s]Epoch: 1, train for the 203-th batch, train loss: 0.4891197085380554:  53%|█████▊     | 203/383 [02:01<01:53,  1.58it/s]evaluate for the 13-th batch, evaluate loss: 0.5002371668815613:  26%|████▋             | 12/46 [00:01<00:03,  9.67it/s]evaluate for the 13-th batch, evaluate loss: 0.5002371668815613:  28%|█████             | 13/46 [00:01<00:03,  9.70it/s]evaluate for the 14-th batch, evaluate loss: 0.5898544192314148:  28%|█████             | 13/46 [00:01<00:03,  9.70it/s]evaluate for the 14-th batch, evaluate loss: 0.5898544192314148:  30%|█████▍            | 14/46 [00:01<00:03,  9.71it/s]Epoch: 1, train for the 211-th batch, train loss: 0.6246159076690674:  89%|█████████▋ | 210/237 [02:03<00:15,  1.78it/s]Epoch: 1, train for the 211-th batch, train loss: 0.6246159076690674:  89%|█████████▊ | 211/237 [02:03<00:14,  1.74it/s]evaluate for the 15-th batch, evaluate loss: 0.5457618832588196:  30%|█████▍            | 14/46 [00:01<00:03,  9.71it/s]evaluate for the 15-th batch, evaluate loss: 0.5457618832588196:  33%|█████▊            | 15/46 [00:01<00:03,  9.68it/s]evaluate for the 16-th batch, evaluate loss: 0.5750624537467957:  33%|█████▊            | 15/46 [00:01<00:03,  9.68it/s]evaluate for the 16-th batch, evaluate loss: 0.5750624537467957:  35%|██████▎           | 16/46 [00:01<00:03,  9.69it/s]Epoch: 2, train for the 59-th batch, train loss: 0.2205292284488678:  49%|██████▎      | 58/119 [00:35<00:36,  1.66it/s]Epoch: 2, train for the 59-th batch, train loss: 0.2205292284488678:  50%|██████▍      | 59/119 [00:35<00:36,  1.66it/s]evaluate for the 17-th batch, evaluate loss: 0.45920485258102417:  35%|█████▉           | 16/46 [00:01<00:03,  9.69it/s]evaluate for the 17-th batch, evaluate loss: 0.45920485258102417:  37%|██████▎          | 17/46 [00:01<00:02,  9.69it/s]evaluate for the 18-th batch, evaluate loss: 0.5043842196464539:  37%|██████▋           | 17/46 [00:01<00:02,  9.69it/s]evaluate for the 18-th batch, evaluate loss: 0.5043842196464539:  39%|███████           | 18/46 [00:01<00:02,  9.69it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5008594989776611:  23%|██▉          | 33/146 [00:19<01:07,  1.66it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5008594989776611:  23%|███          | 34/146 [00:19<01:07,  1.67it/s]Epoch: 1, train for the 204-th batch, train loss: 0.38335052132606506:  53%|█████▎    | 203/383 [02:01<01:53,  1.58it/s]Epoch: 1, train for the 204-th batch, train loss: 0.38335052132606506:  53%|█████▎    | 204/383 [02:01<01:51,  1.61it/s]evaluate for the 19-th batch, evaluate loss: 0.5233151912689209:  39%|███████           | 18/46 [00:01<00:02,  9.69it/s]evaluate for the 19-th batch, evaluate loss: 0.5233151912689209:  41%|███████▍          | 19/46 [00:01<00:02,  9.67it/s]evaluate for the 20-th batch, evaluate loss: 0.5418071746826172:  41%|███████▍          | 19/46 [00:02<00:02,  9.67it/s]evaluate for the 20-th batch, evaluate loss: 0.5418071746826172:  43%|███████▊          | 20/46 [00:02<00:02,  9.66it/s]Epoch: 1, train for the 212-th batch, train loss: 0.6313814520835876:  89%|█████████▊ | 211/237 [02:03<00:14,  1.74it/s]Epoch: 1, train for the 212-th batch, train loss: 0.6313814520835876:  89%|█████████▊ | 212/237 [02:03<00:14,  1.71it/s]evaluate for the 21-th batch, evaluate loss: 0.5269960165023804:  43%|███████▊          | 20/46 [00:02<00:02,  9.66it/s]evaluate for the 21-th batch, evaluate loss: 0.5269960165023804:  46%|████████▏         | 21/46 [00:02<00:02,  9.65it/s]evaluate for the 22-th batch, evaluate loss: 0.5220211148262024:  46%|████████▏         | 21/46 [00:02<00:02,  9.65it/s]evaluate for the 22-th batch, evaluate loss: 0.5220211148262024:  48%|████████▌         | 22/46 [00:02<00:02,  9.67it/s]Epoch: 2, train for the 60-th batch, train loss: 0.19467785954475403:  50%|█████▉      | 59/119 [00:35<00:36,  1.66it/s]Epoch: 2, train for the 60-th batch, train loss: 0.19467785954475403:  50%|██████      | 60/119 [00:35<00:35,  1.67it/s]evaluate for the 23-th batch, evaluate loss: 0.47338607907295227:  48%|████████▏        | 22/46 [00:02<00:02,  9.67it/s]evaluate for the 23-th batch, evaluate loss: 0.47338607907295227:  50%|████████▌        | 23/46 [00:02<00:02,  9.67it/s]Epoch: 2, train for the 35-th batch, train loss: 0.4383607506752014:  23%|███          | 34/146 [00:20<01:07,  1.67it/s]Epoch: 2, train for the 35-th batch, train loss: 0.4383607506752014:  24%|███          | 35/146 [00:20<01:06,  1.66it/s]evaluate for the 24-th batch, evaluate loss: 0.48737215995788574:  50%|████████▌        | 23/46 [00:02<00:02,  9.67it/s]evaluate for the 24-th batch, evaluate loss: 0.48737215995788574:  52%|████████▊        | 24/46 [00:02<00:02,  9.66it/s]Epoch: 1, train for the 205-th batch, train loss: 0.4359583556652069:  53%|█████▊     | 204/383 [02:02<01:51,  1.61it/s]Epoch: 1, train for the 205-th batch, train loss: 0.4359583556652069:  54%|█████▉     | 205/383 [02:02<01:49,  1.62it/s]evaluate for the 25-th batch, evaluate loss: 0.5363720059394836:  52%|█████████▍        | 24/46 [00:02<00:02,  9.66it/s]evaluate for the 25-th batch, evaluate loss: 0.5363720059394836:  54%|█████████▊        | 25/46 [00:02<00:02,  9.67it/s]Epoch: 1, train for the 213-th batch, train loss: 0.618550181388855:  89%|██████████▋ | 212/237 [02:04<00:14,  1.71it/s]Epoch: 1, train for the 213-th batch, train loss: 0.618550181388855:  90%|██████████▊ | 213/237 [02:04<00:14,  1.70it/s]evaluate for the 26-th batch, evaluate loss: 0.5595417022705078:  54%|█████████▊        | 25/46 [00:02<00:02,  9.67it/s]evaluate for the 26-th batch, evaluate loss: 0.5595417022705078:  57%|██████████▏       | 26/46 [00:02<00:02,  9.66it/s]evaluate for the 27-th batch, evaluate loss: 0.4961452782154083:  57%|██████████▏       | 26/46 [00:02<00:02,  9.66it/s]evaluate for the 27-th batch, evaluate loss: 0.4961452782154083:  59%|██████████▌       | 27/46 [00:02<00:01,  9.65it/s]evaluate for the 28-th batch, evaluate loss: 0.5272606015205383:  59%|██████████▌       | 27/46 [00:02<00:01,  9.65it/s]evaluate for the 28-th batch, evaluate loss: 0.5272606015205383:  61%|██████████▉       | 28/46 [00:02<00:01,  9.62it/s]Epoch: 2, train for the 61-th batch, train loss: 0.20499102771282196:  50%|██████      | 60/119 [00:36<00:35,  1.67it/s]Epoch: 2, train for the 61-th batch, train loss: 0.20499102771282196:  51%|██████▏     | 61/119 [00:36<00:34,  1.67it/s]evaluate for the 29-th batch, evaluate loss: 0.4866233766078949:  61%|██████████▉       | 28/46 [00:03<00:01,  9.62it/s]evaluate for the 29-th batch, evaluate loss: 0.4866233766078949:  63%|███████████▎      | 29/46 [00:03<00:01,  9.58it/s]Epoch: 2, train for the 36-th batch, train loss: 0.44573381543159485:  24%|██▉         | 35/146 [00:20<01:06,  1.66it/s]Epoch: 2, train for the 36-th batch, train loss: 0.44573381543159485:  25%|██▉         | 36/146 [00:20<01:06,  1.66it/s]evaluate for the 30-th batch, evaluate loss: 0.5023390054702759:  63%|███████████▎      | 29/46 [00:03<00:01,  9.58it/s]evaluate for the 30-th batch, evaluate loss: 0.5023390054702759:  65%|███████████▋      | 30/46 [00:03<00:01,  9.59it/s]Epoch: 1, train for the 206-th batch, train loss: 0.3942820727825165:  54%|█████▉     | 205/383 [02:02<01:49,  1.62it/s]Epoch: 1, train for the 206-th batch, train loss: 0.3942820727825165:  54%|█████▉     | 206/383 [02:02<01:48,  1.64it/s]evaluate for the 31-th batch, evaluate loss: 0.516483724117279:  65%|████████████▍      | 30/46 [00:03<00:01,  9.59it/s]evaluate for the 31-th batch, evaluate loss: 0.516483724117279:  67%|████████████▊      | 31/46 [00:03<00:01,  9.59it/s]Epoch: 1, train for the 214-th batch, train loss: 0.6665745973587036:  90%|█████████▉ | 213/237 [02:04<00:14,  1.70it/s]Epoch: 1, train for the 214-th batch, train loss: 0.6665745973587036:  90%|█████████▉ | 214/237 [02:04<00:13,  1.69it/s]evaluate for the 32-th batch, evaluate loss: 0.4749796688556671:  67%|████████████▏     | 31/46 [00:03<00:01,  9.59it/s]evaluate for the 32-th batch, evaluate loss: 0.4749796688556671:  70%|████████████▌     | 32/46 [00:03<00:01,  9.59it/s]evaluate for the 33-th batch, evaluate loss: 0.4967658221721649:  70%|████████████▌     | 32/46 [00:03<00:01,  9.59it/s]evaluate for the 33-th batch, evaluate loss: 0.4967658221721649:  72%|████████████▉     | 33/46 [00:03<00:01,  9.60it/s]evaluate for the 34-th batch, evaluate loss: 0.4928922951221466:  72%|████████████▉     | 33/46 [00:03<00:01,  9.60it/s]evaluate for the 34-th batch, evaluate loss: 0.4928922951221466:  74%|█████████████▎    | 34/46 [00:03<00:01,  9.61it/s]Epoch: 2, train for the 62-th batch, train loss: 0.2695837616920471:  51%|██████▋      | 61/119 [00:37<00:34,  1.67it/s]Epoch: 2, train for the 62-th batch, train loss: 0.2695837616920471:  52%|██████▊      | 62/119 [00:37<00:34,  1.66it/s]evaluate for the 35-th batch, evaluate loss: 0.49194303154945374:  74%|████████████▌    | 34/46 [00:03<00:01,  9.61it/s]evaluate for the 35-th batch, evaluate loss: 0.49194303154945374:  76%|████████████▉    | 35/46 [00:03<00:01,  9.62it/s]Epoch: 2, train for the 37-th batch, train loss: 0.49070116877555847:  25%|██▉         | 36/146 [00:21<01:06,  1.66it/s]Epoch: 2, train for the 37-th batch, train loss: 0.49070116877555847:  25%|███         | 37/146 [00:21<01:05,  1.65it/s]Epoch: 1, train for the 207-th batch, train loss: 0.4457574784755707:  54%|█████▉     | 206/383 [02:03<01:48,  1.64it/s]Epoch: 1, train for the 207-th batch, train loss: 0.4457574784755707:  54%|█████▉     | 207/383 [02:03<01:46,  1.65it/s]evaluate for the 36-th batch, evaluate loss: 0.4743652939796448:  76%|█████████████▋    | 35/46 [00:03<00:01,  9.62it/s]evaluate for the 36-th batch, evaluate loss: 0.4743652939796448:  78%|██████████████    | 36/46 [00:03<00:01,  9.64it/s]evaluate for the 37-th batch, evaluate loss: 0.5083912014961243:  78%|██████████████    | 36/46 [00:03<00:01,  9.64it/s]evaluate for the 37-th batch, evaluate loss: 0.5083912014961243:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.65it/s]Epoch: 1, train for the 215-th batch, train loss: 0.641272246837616:  90%|██████████▊ | 214/237 [02:05<00:13,  1.69it/s]Epoch: 1, train for the 215-th batch, train loss: 0.641272246837616:  91%|██████████▉ | 215/237 [02:05<00:13,  1.69it/s]evaluate for the 38-th batch, evaluate loss: 0.5417581796646118:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.65it/s]evaluate for the 38-th batch, evaluate loss: 0.5417581796646118:  83%|██████████████▊   | 38/46 [00:03<00:00,  9.66it/s]evaluate for the 39-th batch, evaluate loss: 0.5388491749763489:  83%|██████████████▊   | 38/46 [00:04<00:00,  9.66it/s]evaluate for the 39-th batch, evaluate loss: 0.5388491749763489:  85%|███████████████▎  | 39/46 [00:04<00:00,  9.67it/s]evaluate for the 40-th batch, evaluate loss: 0.4689442217350006:  85%|███████████████▎  | 39/46 [00:04<00:00,  9.67it/s]evaluate for the 40-th batch, evaluate loss: 0.4689442217350006:  87%|███████████████▋  | 40/46 [00:04<00:00,  9.66it/s]Epoch: 2, train for the 63-th batch, train loss: 0.19956019520759583:  52%|██████▎     | 62/119 [00:37<00:34,  1.66it/s]Epoch: 2, train for the 63-th batch, train loss: 0.19956019520759583:  53%|██████▎     | 63/119 [00:37<00:33,  1.65it/s]evaluate for the 41-th batch, evaluate loss: 0.48816680908203125:  87%|██████████████▊  | 40/46 [00:04<00:00,  9.66it/s]evaluate for the 41-th batch, evaluate loss: 0.48816680908203125:  89%|███████████████▏ | 41/46 [00:04<00:00,  9.65it/s]Epoch: 2, train for the 38-th batch, train loss: 0.4328790307044983:  25%|███▎         | 37/146 [00:22<01:05,  1.65it/s]Epoch: 2, train for the 38-th batch, train loss: 0.4328790307044983:  26%|███▍         | 38/146 [00:22<01:05,  1.64it/s]Epoch: 1, train for the 208-th batch, train loss: 0.4693143367767334:  54%|█████▉     | 207/383 [02:04<01:46,  1.65it/s]Epoch: 1, train for the 208-th batch, train loss: 0.4693143367767334:  54%|█████▉     | 208/383 [02:04<01:45,  1.66it/s]evaluate for the 42-th batch, evaluate loss: 0.4679299294948578:  89%|████████████████  | 41/46 [00:04<00:00,  9.65it/s]evaluate for the 42-th batch, evaluate loss: 0.4679299294948578:  91%|████████████████▍ | 42/46 [00:04<00:00,  9.65it/s]evaluate for the 43-th batch, evaluate loss: 0.533360481262207:  91%|█████████████████▎ | 42/46 [00:04<00:00,  9.65it/s]evaluate for the 43-th batch, evaluate loss: 0.533360481262207:  93%|█████████████████▊ | 43/46 [00:04<00:00,  9.66it/s]Epoch: 1, train for the 216-th batch, train loss: 0.6251851320266724:  91%|█████████▉ | 215/237 [02:06<00:13,  1.69it/s]Epoch: 1, train for the 216-th batch, train loss: 0.6251851320266724:  91%|██████████ | 216/237 [02:06<00:12,  1.68it/s]evaluate for the 44-th batch, evaluate loss: 0.5141170620918274:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.66it/s]evaluate for the 44-th batch, evaluate loss: 0.5141170620918274:  96%|█████████████████▏| 44/46 [00:04<00:00,  9.67it/s]evaluate for the 45-th batch, evaluate loss: 0.49586257338523865:  96%|████████████████▎| 44/46 [00:04<00:00,  9.67it/s]evaluate for the 45-th batch, evaluate loss: 0.49586257338523865:  98%|████████████████▋| 45/46 [00:04<00:00,  9.67it/s]evaluate for the 46-th batch, evaluate loss: 0.5029076337814331:  98%|█████████████████▌| 45/46 [00:04<00:00,  9.67it/s]evaluate for the 46-th batch, evaluate loss: 0.5029076337814331: 100%|██████████████████| 46/46 [00:04<00:00,  9.68it/s]
  0%|                                                                                            | 0/25 [00:00<?, ?it/s]Epoch: 2, train for the 64-th batch, train loss: 0.15796653926372528:  53%|██████▎     | 63/119 [00:38<00:33,  1.65it/s]Epoch: 2, train for the 64-th batch, train loss: 0.15796653926372528:  54%|██████▍     | 64/119 [00:38<00:33,  1.64it/s]evaluate for the 1-th batch, evaluate loss: 0.6492327451705933:   0%|                            | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6492327451705933:   4%|▊                   | 1/25 [00:00<00:02,  9.28it/s]Epoch: 1, train for the 209-th batch, train loss: 0.4545544385910034:  54%|█████▉     | 208/383 [02:04<01:45,  1.66it/s]Epoch: 1, train for the 209-th batch, train loss: 0.4545544385910034:  55%|██████     | 209/383 [02:04<01:44,  1.66it/s]Epoch: 2, train for the 39-th batch, train loss: 0.49383753538131714:  26%|███         | 38/146 [00:22<01:05,  1.64it/s]Epoch: 2, train for the 39-th batch, train loss: 0.49383753538131714:  27%|███▏        | 39/146 [00:22<01:05,  1.64it/s]evaluate for the 2-th batch, evaluate loss: 0.6593134999275208:   4%|▊                   | 1/25 [00:00<00:02,  9.28it/s]evaluate for the 2-th batch, evaluate loss: 0.6593134999275208:   8%|█▌                  | 2/25 [00:00<00:02,  9.19it/s]Epoch: 1, train for the 217-th batch, train loss: 0.6058597564697266:  91%|██████████ | 216/237 [02:06<00:12,  1.68it/s]Epoch: 1, train for the 217-th batch, train loss: 0.6058597564697266:  92%|██████████ | 217/237 [02:06<00:11,  1.68it/s]evaluate for the 3-th batch, evaluate loss: 0.7033424377441406:   8%|█▌                  | 2/25 [00:00<00:02,  9.19it/s]evaluate for the 3-th batch, evaluate loss: 0.7033424377441406:  12%|██▍                 | 3/25 [00:00<00:02,  9.20it/s]evaluate for the 4-th batch, evaluate loss: 0.6850042343139648:  12%|██▍                 | 3/25 [00:00<00:02,  9.20it/s]evaluate for the 4-th batch, evaluate loss: 0.6850042343139648:  16%|███▏                | 4/25 [00:00<00:02,  9.15it/s]evaluate for the 5-th batch, evaluate loss: 0.6902679204940796:  16%|███▏                | 4/25 [00:00<00:02,  9.15it/s]evaluate for the 5-th batch, evaluate loss: 0.6902679204940796:  20%|████                | 5/25 [00:00<00:02,  9.16it/s]Epoch: 2, train for the 65-th batch, train loss: 0.21482791006565094:  54%|██████▍     | 64/119 [00:39<00:33,  1.64it/s]Epoch: 2, train for the 65-th batch, train loss: 0.21482791006565094:  55%|██████▌     | 65/119 [00:39<00:33,  1.63it/s]evaluate for the 6-th batch, evaluate loss: 0.7268566489219666:  20%|████                | 5/25 [00:00<00:02,  9.16it/s]evaluate for the 6-th batch, evaluate loss: 0.7268566489219666:  24%|████▊               | 6/25 [00:00<00:02,  9.15it/s]evaluate for the 7-th batch, evaluate loss: 0.7528738975524902:  24%|████▊               | 6/25 [00:00<00:02,  9.15it/s]evaluate for the 7-th batch, evaluate loss: 0.7528738975524902:  28%|█████▌              | 7/25 [00:00<00:01,  9.18it/s]Epoch: 1, train for the 210-th batch, train loss: 0.536536455154419:  55%|██████▌     | 209/383 [02:05<01:44,  1.66it/s]Epoch: 1, train for the 210-th batch, train loss: 0.536536455154419:  55%|██████▌     | 210/383 [02:05<01:43,  1.67it/s]Epoch: 2, train for the 40-th batch, train loss: 0.47869908809661865:  27%|███▏        | 39/146 [00:23<01:05,  1.64it/s]Epoch: 2, train for the 40-th batch, train loss: 0.47869908809661865:  27%|███▎        | 40/146 [00:23<01:04,  1.63it/s]evaluate for the 8-th batch, evaluate loss: 0.7316274642944336:  28%|█████▌              | 7/25 [00:00<00:01,  9.18it/s]evaluate for the 8-th batch, evaluate loss: 0.7316274642944336:  32%|██████▍             | 8/25 [00:00<00:01,  9.20it/s]Epoch: 1, train for the 218-th batch, train loss: 0.6379792094230652:  92%|██████████ | 217/237 [02:07<00:11,  1.68it/s]Epoch: 1, train for the 218-th batch, train loss: 0.6379792094230652:  92%|██████████ | 218/237 [02:07<00:11,  1.68it/s]evaluate for the 9-th batch, evaluate loss: 0.7214019894599915:  32%|██████▍             | 8/25 [00:00<00:01,  9.20it/s]evaluate for the 9-th batch, evaluate loss: 0.7214019894599915:  36%|███████▏            | 9/25 [00:00<00:01,  9.21it/s]evaluate for the 10-th batch, evaluate loss: 0.7584512829780579:  36%|██████▊            | 9/25 [00:01<00:01,  9.21it/s]evaluate for the 10-th batch, evaluate loss: 0.7584512829780579:  40%|███████▏          | 10/25 [00:01<00:01,  9.23it/s]evaluate for the 11-th batch, evaluate loss: 0.7440456748008728:  40%|███████▏          | 10/25 [00:01<00:01,  9.23it/s]evaluate for the 11-th batch, evaluate loss: 0.7440456748008728:  44%|███████▉          | 11/25 [00:01<00:01,  9.23it/s]Epoch: 2, train for the 66-th batch, train loss: 0.2436416894197464:  55%|███████      | 65/119 [00:39<00:33,  1.63it/s]Epoch: 2, train for the 66-th batch, train loss: 0.2436416894197464:  55%|███████▏     | 66/119 [00:39<00:32,  1.63it/s]evaluate for the 12-th batch, evaluate loss: 0.7077016830444336:  44%|███████▉          | 11/25 [00:01<00:01,  9.23it/s]evaluate for the 12-th batch, evaluate loss: 0.7077016830444336:  48%|████████▋         | 12/25 [00:01<00:01,  9.25it/s]Epoch: 1, train for the 211-th batch, train loss: 0.45364120602607727:  55%|█████▍    | 210/383 [02:05<01:43,  1.67it/s]Epoch: 1, train for the 211-th batch, train loss: 0.45364120602607727:  55%|█████▌    | 211/383 [02:05<01:43,  1.67it/s]Epoch: 2, train for the 41-th batch, train loss: 0.47848641872406006:  27%|███▎        | 40/146 [00:24<01:04,  1.63it/s]Epoch: 2, train for the 41-th batch, train loss: 0.47848641872406006:  28%|███▎        | 41/146 [00:24<01:04,  1.63it/s]evaluate for the 13-th batch, evaluate loss: 0.6830216646194458:  48%|████████▋         | 12/25 [00:01<00:01,  9.25it/s]evaluate for the 13-th batch, evaluate loss: 0.6830216646194458:  52%|█████████▎        | 13/25 [00:01<00:01,  9.25it/s]evaluate for the 14-th batch, evaluate loss: 0.7698618769645691:  52%|█████████▎        | 13/25 [00:01<00:01,  9.25it/s]evaluate for the 14-th batch, evaluate loss: 0.7698618769645691:  56%|██████████        | 14/25 [00:01<00:01,  9.26it/s]Epoch: 1, train for the 219-th batch, train loss: 0.6324933767318726:  92%|██████████ | 218/237 [02:07<00:11,  1.68it/s]Epoch: 1, train for the 219-th batch, train loss: 0.6324933767318726:  92%|██████████▏| 219/237 [02:07<00:10,  1.67it/s]evaluate for the 15-th batch, evaluate loss: 0.7565732598304749:  56%|██████████        | 14/25 [00:01<00:01,  9.26it/s]evaluate for the 15-th batch, evaluate loss: 0.7565732598304749:  60%|██████████▊       | 15/25 [00:01<00:01,  9.23it/s]evaluate for the 16-th batch, evaluate loss: 0.6740227341651917:  60%|██████████▊       | 15/25 [00:01<00:01,  9.23it/s]evaluate for the 16-th batch, evaluate loss: 0.6740227341651917:  64%|███████████▌      | 16/25 [00:01<00:00,  9.14it/s]evaluate for the 17-th batch, evaluate loss: 0.6848686337471008:  64%|███████████▌      | 16/25 [00:01<00:00,  9.14it/s]evaluate for the 17-th batch, evaluate loss: 0.6848686337471008:  68%|████████████▏     | 17/25 [00:01<00:00,  9.14it/s]Epoch: 2, train for the 67-th batch, train loss: 0.2929612994194031:  55%|███████▏     | 66/119 [00:40<00:32,  1.63it/s]Epoch: 2, train for the 67-th batch, train loss: 0.2929612994194031:  56%|███████▎     | 67/119 [00:40<00:31,  1.63it/s]evaluate for the 18-th batch, evaluate loss: 0.6330443620681763:  68%|████████████▏     | 17/25 [00:01<00:00,  9.14it/s]evaluate for the 18-th batch, evaluate loss: 0.6330443620681763:  72%|████████████▉     | 18/25 [00:01<00:00,  9.18it/s]Epoch: 1, train for the 212-th batch, train loss: 0.48832038044929504:  55%|█████▌    | 211/383 [02:06<01:43,  1.67it/s]Epoch: 1, train for the 212-th batch, train loss: 0.48832038044929504:  55%|█████▌    | 212/383 [02:06<01:42,  1.66it/s]Epoch: 2, train for the 42-th batch, train loss: 0.4676440954208374:  28%|███▋         | 41/146 [00:24<01:04,  1.63it/s]Epoch: 2, train for the 42-th batch, train loss: 0.4676440954208374:  29%|███▋         | 42/146 [00:24<01:03,  1.63it/s]evaluate for the 19-th batch, evaluate loss: 0.6059241890907288:  72%|████████████▉     | 18/25 [00:02<00:00,  9.18it/s]evaluate for the 19-th batch, evaluate loss: 0.6059241890907288:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.20it/s]Epoch: 1, train for the 220-th batch, train loss: 0.6359571814537048:  92%|██████████▏| 219/237 [02:08<00:10,  1.67it/s]Epoch: 1, train for the 220-th batch, train loss: 0.6359571814537048:  93%|██████████▏| 220/237 [02:08<00:10,  1.67it/s]evaluate for the 20-th batch, evaluate loss: 0.6784843802452087:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.20it/s]evaluate for the 20-th batch, evaluate loss: 0.6784843802452087:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.22it/s]evaluate for the 21-th batch, evaluate loss: 0.74332594871521:  80%|████████████████    | 20/25 [00:02<00:00,  9.22it/s]evaluate for the 21-th batch, evaluate loss: 0.74332594871521:  84%|████████████████▊   | 21/25 [00:02<00:00,  9.22it/s]evaluate for the 22-th batch, evaluate loss: 0.6255804896354675:  84%|███████████████   | 21/25 [00:02<00:00,  9.22it/s]evaluate for the 22-th batch, evaluate loss: 0.6255804896354675:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.20it/s]Epoch: 2, train for the 68-th batch, train loss: 0.22430047392845154:  56%|██████▊     | 67/119 [00:40<00:31,  1.63it/s]Epoch: 2, train for the 68-th batch, train loss: 0.22430047392845154:  57%|██████▊     | 68/119 [00:40<00:31,  1.63it/s]evaluate for the 23-th batch, evaluate loss: 0.6835877299308777:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.20it/s]evaluate for the 23-th batch, evaluate loss: 0.6835877299308777:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.18it/s]Epoch: 1, train for the 213-th batch, train loss: 0.6142861247062683:  55%|██████     | 212/383 [02:07<01:42,  1.66it/s]Epoch: 1, train for the 213-th batch, train loss: 0.6142861247062683:  56%|██████     | 213/383 [02:07<01:42,  1.66it/s]evaluate for the 24-th batch, evaluate loss: 0.6729019284248352:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.18it/s]evaluate for the 24-th batch, evaluate loss: 0.6729019284248352:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.17it/s]Epoch: 2, train for the 43-th batch, train loss: 0.4746200442314148:  29%|███▋         | 42/146 [00:25<01:03,  1.63it/s]Epoch: 2, train for the 43-th batch, train loss: 0.4746200442314148:  29%|███▊         | 43/146 [00:25<01:03,  1.63it/s]evaluate for the 25-th batch, evaluate loss: 0.7194833159446716:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.17it/s]evaluate for the 25-th batch, evaluate loss: 0.7194833159446716: 100%|██████████████████| 25/25 [00:02<00:00,  9.27it/s]
INFO:root:Epoch: 2, learning rate: 0.0001, train loss: 0.5847
INFO:root:train average_precision, 0.7908
INFO:root:train roc_auc, 0.7527
INFO:root:validate loss: 0.5117
INFO:root:validate average_precision, 0.8391
INFO:root:validate roc_auc, 0.7987
INFO:root:new node validate loss: 0.6984
INFO:root:new node validate first_1_average_precision, 0.5817
INFO:root:new node validate first_1_roc_auc, 0.5314
INFO:root:new node validate first_3_average_precision, 0.6634
INFO:root:new node validate first_3_roc_auc, 0.6300
INFO:root:new node validate first_10_average_precision, 0.7348
INFO:root:new node validate first_10_roc_auc, 0.7042
INFO:root:new node validate average_precision, 0.6984
INFO:root:new node validate roc_auc, 0.6504
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]Epoch: 1, train for the 221-th batch, train loss: 0.646248459815979:  93%|███████████▏| 220/237 [02:09<00:10,  1.67it/s]Epoch: 1, train for the 221-th batch, train loss: 0.646248459815979:  93%|███████████▏| 221/237 [02:09<00:09,  1.66it/s]Epoch: 3, train for the 1-th batch, train loss: 0.7094239592552185:   0%|                       | 0/151 [00:00<?, ?it/s]Epoch: 3, train for the 1-th batch, train loss: 0.7094239592552185:   1%|               | 1/151 [00:00<00:25,  5.85it/s]Epoch: 3, train for the 2-th batch, train loss: 0.6794213056564331:   1%|               | 1/151 [00:00<00:25,  5.85it/s]Epoch: 3, train for the 2-th batch, train loss: 0.6794213056564331:   1%|▏              | 2/151 [00:00<00:26,  5.73it/s]Epoch: 2, train for the 69-th batch, train loss: 0.20899036526679993:  57%|██████▊     | 68/119 [00:41<00:31,  1.63it/s]Epoch: 2, train for the 69-th batch, train loss: 0.20899036526679993:  58%|██████▉     | 69/119 [00:41<00:30,  1.63it/s]Epoch: 1, train for the 214-th batch, train loss: 0.4587063491344452:  56%|██████     | 213/383 [02:07<01:42,  1.66it/s]Epoch: 1, train for the 214-th batch, train loss: 0.4587063491344452:  56%|██████▏    | 214/383 [02:07<01:41,  1.66it/s]Epoch: 3, train for the 3-th batch, train loss: 0.6968092918395996:   1%|▏              | 2/151 [00:00<00:26,  5.73it/s]Epoch: 3, train for the 3-th batch, train loss: 0.6968092918395996:   2%|▎              | 3/151 [00:00<00:24,  5.92it/s]Epoch: 2, train for the 44-th batch, train loss: 0.4482329487800598:  29%|███▊         | 43/146 [00:25<01:03,  1.63it/s]Epoch: 2, train for the 44-th batch, train loss: 0.4482329487800598:  30%|███▉         | 44/146 [00:25<01:02,  1.63it/s]Epoch: 1, train for the 222-th batch, train loss: 0.6182076334953308:  93%|██████████▎| 221/237 [02:09<00:09,  1.66it/s]Epoch: 1, train for the 222-th batch, train loss: 0.6182076334953308:  94%|██████████▎| 222/237 [02:09<00:08,  1.67it/s]Epoch: 3, train for the 4-th batch, train loss: 0.6789181232452393:   2%|▎              | 3/151 [00:00<00:24,  5.92it/s]Epoch: 3, train for the 4-th batch, train loss: 0.6789181232452393:   3%|▍              | 4/151 [00:00<00:24,  5.89it/s]Epoch: 3, train for the 5-th batch, train loss: 0.6441906094551086:   3%|▍              | 4/151 [00:00<00:24,  5.89it/s]Epoch: 3, train for the 5-th batch, train loss: 0.6441906094551086:   3%|▍              | 5/151 [00:00<00:25,  5.71it/s]Epoch: 2, train for the 70-th batch, train loss: 0.19827671349048615:  58%|██████▉     | 69/119 [00:42<00:30,  1.63it/s]Epoch: 2, train for the 70-th batch, train loss: 0.19827671349048615:  59%|███████     | 70/119 [00:42<00:30,  1.63it/s]Epoch: 3, train for the 6-th batch, train loss: 0.649868905544281:   3%|▌               | 5/151 [00:01<00:25,  5.71it/s]Epoch: 3, train for the 6-th batch, train loss: 0.649868905544281:   4%|▋               | 6/151 [00:01<00:25,  5.59it/s]Epoch: 1, train for the 215-th batch, train loss: 0.47007614374160767:  56%|█████▌    | 214/383 [02:08<01:41,  1.66it/s]Epoch: 1, train for the 215-th batch, train loss: 0.47007614374160767:  56%|█████▌    | 215/383 [02:08<01:41,  1.66it/s]Epoch: 2, train for the 45-th batch, train loss: 0.43904101848602295:  30%|███▌        | 44/146 [00:26<01:02,  1.63it/s]Epoch: 2, train for the 45-th batch, train loss: 0.43904101848602295:  31%|███▋        | 45/146 [00:26<01:01,  1.63it/s]Epoch: 1, train for the 223-th batch, train loss: 0.626699686050415:  94%|███████████▏| 222/237 [02:10<00:08,  1.67it/s]Epoch: 1, train for the 223-th batch, train loss: 0.626699686050415:  94%|███████████▎| 223/237 [02:10<00:08,  1.66it/s]Epoch: 3, train for the 7-th batch, train loss: 0.63572758436203:   4%|▋                | 6/151 [00:01<00:25,  5.59it/s]Epoch: 3, train for the 7-th batch, train loss: 0.63572758436203:   5%|▊                | 7/151 [00:01<00:26,  5.44it/s]Epoch: 3, train for the 8-th batch, train loss: 0.6520843505859375:   5%|▋              | 7/151 [00:01<00:26,  5.44it/s]Epoch: 3, train for the 8-th batch, train loss: 0.6520843505859375:   5%|▊              | 8/151 [00:01<00:26,  5.38it/s]Epoch: 2, train for the 71-th batch, train loss: 0.25423282384872437:  59%|███████     | 70/119 [00:42<00:30,  1.63it/s]Epoch: 2, train for the 71-th batch, train loss: 0.25423282384872437:  60%|███████▏    | 71/119 [00:42<00:29,  1.63it/s]Epoch: 3, train for the 9-th batch, train loss: 0.6504451036453247:   5%|▊              | 8/151 [00:01<00:26,  5.38it/s]Epoch: 3, train for the 9-th batch, train loss: 0.6504451036453247:   6%|▉              | 9/151 [00:01<00:26,  5.32it/s]Epoch: 1, train for the 216-th batch, train loss: 0.457325279712677:  56%|██████▋     | 215/383 [02:08<01:41,  1.66it/s]Epoch: 1, train for the 216-th batch, train loss: 0.457325279712677:  56%|██████▊     | 216/383 [02:08<01:40,  1.66it/s]Epoch: 2, train for the 46-th batch, train loss: 0.4784173369407654:  31%|████         | 45/146 [00:27<01:01,  1.63it/s]Epoch: 2, train for the 46-th batch, train loss: 0.4784173369407654:  32%|████         | 46/146 [00:27<01:01,  1.63it/s]Epoch: 1, train for the 224-th batch, train loss: 0.6626644730567932:  94%|██████████▎| 223/237 [02:10<00:08,  1.66it/s]Epoch: 1, train for the 224-th batch, train loss: 0.6626644730567932:  95%|██████████▍| 224/237 [02:10<00:07,  1.66it/s]Epoch: 3, train for the 10-th batch, train loss: 0.6371229290962219:   6%|▊             | 9/151 [00:01<00:26,  5.32it/s]Epoch: 3, train for the 10-th batch, train loss: 0.6371229290962219:   7%|▊            | 10/151 [00:01<00:26,  5.23it/s]Epoch: 3, train for the 11-th batch, train loss: 0.6211919784545898:   7%|▊            | 10/151 [00:02<00:26,  5.23it/s]Epoch: 3, train for the 11-th batch, train loss: 0.6211919784545898:   7%|▉            | 11/151 [00:02<00:27,  5.16it/s]Epoch: 2, train for the 72-th batch, train loss: 0.28172117471694946:  60%|███████▏    | 71/119 [00:43<00:29,  1.63it/s]Epoch: 2, train for the 72-th batch, train loss: 0.28172117471694946:  61%|███████▎    | 72/119 [00:43<00:28,  1.63it/s]Epoch: 3, train for the 12-th batch, train loss: 0.6604557633399963:   7%|▉            | 11/151 [00:02<00:27,  5.16it/s]Epoch: 3, train for the 12-th batch, train loss: 0.6604557633399963:   8%|█            | 12/151 [00:02<00:27,  5.03it/s]Epoch: 1, train for the 217-th batch, train loss: 0.46104690432548523:  56%|█████▋    | 216/383 [02:09<01:40,  1.66it/s]Epoch: 1, train for the 217-th batch, train loss: 0.46104690432548523:  57%|█████▋    | 217/383 [02:09<01:39,  1.66it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5097257494926453:  32%|████         | 46/146 [00:27<01:01,  1.63it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5097257494926453:  32%|████▏        | 47/146 [00:27<01:00,  1.63it/s]Epoch: 1, train for the 225-th batch, train loss: 0.6593806147575378:  95%|██████████▍| 224/237 [02:11<00:07,  1.66it/s]Epoch: 1, train for the 225-th batch, train loss: 0.6593806147575378:  95%|██████████▍| 225/237 [02:11<00:07,  1.66it/s]Epoch: 3, train for the 13-th batch, train loss: 0.6097881197929382:   8%|█            | 12/151 [00:02<00:27,  5.03it/s]Epoch: 3, train for the 13-th batch, train loss: 0.6097881197929382:   9%|█            | 13/151 [00:02<00:27,  5.03it/s]Epoch: 3, train for the 14-th batch, train loss: 0.5775954127311707:   9%|█            | 13/151 [00:02<00:27,  5.03it/s]Epoch: 3, train for the 14-th batch, train loss: 0.5775954127311707:   9%|█▏           | 14/151 [00:02<00:27,  5.02it/s]Epoch: 3, train for the 15-th batch, train loss: 0.5017734169960022:   9%|█▏           | 14/151 [00:02<00:27,  5.02it/s]Epoch: 3, train for the 15-th batch, train loss: 0.5017734169960022:  10%|█▎           | 15/151 [00:02<00:27,  4.98it/s]Epoch: 2, train for the 73-th batch, train loss: 0.209959477186203:  61%|████████▍     | 72/119 [00:43<00:28,  1.63it/s]Epoch: 2, train for the 73-th batch, train loss: 0.209959477186203:  61%|████████▌     | 73/119 [00:43<00:28,  1.63it/s]Epoch: 1, train for the 218-th batch, train loss: 0.5444836020469666:  57%|██████▏    | 217/383 [02:10<01:39,  1.66it/s]Epoch: 1, train for the 218-th batch, train loss: 0.5444836020469666:  57%|██████▎    | 218/383 [02:10<01:39,  1.66it/s]Epoch: 2, train for the 48-th batch, train loss: 0.50727379322052:  32%|████▊          | 47/146 [00:28<01:00,  1.63it/s]Epoch: 2, train for the 48-th batch, train loss: 0.50727379322052:  33%|████▉          | 48/146 [00:28<01:00,  1.63it/s]Epoch: 1, train for the 226-th batch, train loss: 0.6603898406028748:  95%|██████████▍| 225/237 [02:12<00:07,  1.66it/s]Epoch: 1, train for the 226-th batch, train loss: 0.6603898406028748:  95%|██████████▍| 226/237 [02:12<00:06,  1.66it/s]Epoch: 3, train for the 16-th batch, train loss: 0.505158543586731:  10%|█▍            | 15/151 [00:03<00:27,  4.98it/s]Epoch: 3, train for the 16-th batch, train loss: 0.505158543586731:  11%|█▍            | 16/151 [00:03<00:27,  4.98it/s]Epoch: 3, train for the 17-th batch, train loss: 0.797747790813446:  11%|█▍            | 16/151 [00:03<00:27,  4.98it/s]Epoch: 3, train for the 17-th batch, train loss: 0.797747790813446:  11%|█▌            | 17/151 [00:03<00:27,  4.91it/s]Epoch: 3, train for the 18-th batch, train loss: 0.5836736559867859:  11%|█▍           | 17/151 [00:03<00:27,  4.91it/s]Epoch: 3, train for the 18-th batch, train loss: 0.5836736559867859:  12%|█▌           | 18/151 [00:03<00:27,  4.91it/s]Epoch: 2, train for the 74-th batch, train loss: 0.2538200616836548:  61%|███████▉     | 73/119 [00:44<00:28,  1.63it/s]Epoch: 2, train for the 74-th batch, train loss: 0.2538200616836548:  62%|████████     | 74/119 [00:44<00:27,  1.63it/s]Epoch: 1, train for the 219-th batch, train loss: 0.4118642807006836:  57%|██████▎    | 218/383 [02:10<01:39,  1.66it/s]Epoch: 1, train for the 219-th batch, train loss: 0.4118642807006836:  57%|██████▎    | 219/383 [02:10<01:38,  1.66it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5005999803543091:  33%|████▎        | 48/146 [00:28<01:00,  1.63it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5005999803543091:  34%|████▎        | 49/146 [00:28<00:59,  1.63it/s]Epoch: 1, train for the 227-th batch, train loss: 0.6389539241790771:  95%|██████████▍| 226/237 [02:12<00:06,  1.66it/s]Epoch: 1, train for the 227-th batch, train loss: 0.6389539241790771:  96%|██████████▌| 227/237 [02:12<00:06,  1.66it/s]Epoch: 3, train for the 19-th batch, train loss: 0.6229547262191772:  12%|█▌           | 18/151 [00:03<00:27,  4.91it/s]Epoch: 3, train for the 19-th batch, train loss: 0.6229547262191772:  13%|█▋           | 19/151 [00:03<00:27,  4.88it/s]Epoch: 3, train for the 20-th batch, train loss: 0.6098905801773071:  13%|█▋           | 19/151 [00:03<00:27,  4.88it/s]Epoch: 3, train for the 20-th batch, train loss: 0.6098905801773071:  13%|█▋           | 20/151 [00:03<00:27,  4.85it/s]Epoch: 2, train for the 75-th batch, train loss: 0.20446975529193878:  62%|███████▍    | 74/119 [00:45<00:27,  1.63it/s]Epoch: 2, train for the 75-th batch, train loss: 0.20446975529193878:  63%|███████▌    | 75/119 [00:45<00:27,  1.63it/s]Epoch: 3, train for the 21-th batch, train loss: 0.6309109926223755:  13%|█▋           | 20/151 [00:04<00:27,  4.85it/s]Epoch: 3, train for the 21-th batch, train loss: 0.6309109926223755:  14%|█▊           | 21/151 [00:04<00:26,  4.84it/s]Epoch: 1, train for the 220-th batch, train loss: 0.450834184885025:  57%|██████▊     | 219/383 [02:11<01:38,  1.66it/s]Epoch: 1, train for the 220-th batch, train loss: 0.450834184885025:  57%|██████▉     | 220/383 [02:11<01:38,  1.65it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5187555551528931:  34%|████▎        | 49/146 [00:29<00:59,  1.63it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5187555551528931:  34%|████▍        | 50/146 [00:29<00:59,  1.62it/s]Epoch: 1, train for the 228-th batch, train loss: 0.6183539628982544:  96%|██████████▌| 227/237 [02:13<00:06,  1.66it/s]Epoch: 1, train for the 228-th batch, train loss: 0.6183539628982544:  96%|██████████▌| 228/237 [02:13<00:05,  1.65it/s]Epoch: 3, train for the 22-th batch, train loss: 0.6051177382469177:  14%|█▊           | 21/151 [00:04<00:26,  4.84it/s]Epoch: 3, train for the 22-th batch, train loss: 0.6051177382469177:  15%|█▉           | 22/151 [00:04<00:26,  4.84it/s]Epoch: 3, train for the 23-th batch, train loss: 0.6317709684371948:  15%|█▉           | 22/151 [00:04<00:26,  4.84it/s]Epoch: 3, train for the 23-th batch, train loss: 0.6317709684371948:  15%|█▉           | 23/151 [00:04<00:26,  4.76it/s]Epoch: 2, train for the 76-th batch, train loss: 0.23635661602020264:  63%|███████▌    | 75/119 [00:45<00:27,  1.63it/s]Epoch: 2, train for the 76-th batch, train loss: 0.23635661602020264:  64%|███████▋    | 76/119 [00:45<00:26,  1.63it/s]Epoch: 1, train for the 221-th batch, train loss: 0.42777514457702637:  57%|█████▋    | 220/383 [02:12<01:38,  1.65it/s]Epoch: 1, train for the 221-th batch, train loss: 0.42777514457702637:  58%|█████▊    | 221/383 [02:12<01:38,  1.65it/s]Epoch: 3, train for the 24-th batch, train loss: 0.6125968098640442:  15%|█▉           | 23/151 [00:04<00:26,  4.76it/s]Epoch: 3, train for the 24-th batch, train loss: 0.6125968098640442:  16%|██           | 24/151 [00:04<00:27,  4.67it/s]Epoch: 2, train for the 51-th batch, train loss: 0.48287323117256165:  34%|████        | 50/146 [00:30<00:59,  1.62it/s]Epoch: 2, train for the 51-th batch, train loss: 0.48287323117256165:  35%|████▏       | 51/146 [00:30<00:58,  1.63it/s]Epoch: 1, train for the 229-th batch, train loss: 0.6744926571846008:  96%|██████████▌| 228/237 [02:13<00:05,  1.65it/s]Epoch: 1, train for the 229-th batch, train loss: 0.6744926571846008:  97%|██████████▋| 229/237 [02:13<00:04,  1.65it/s]Epoch: 3, train for the 25-th batch, train loss: 0.6711428165435791:  16%|██           | 24/151 [00:04<00:27,  4.67it/s]Epoch: 3, train for the 25-th batch, train loss: 0.6711428165435791:  17%|██▏          | 25/151 [00:04<00:26,  4.68it/s]Epoch: 3, train for the 26-th batch, train loss: 0.6021116375923157:  17%|██▏          | 25/151 [00:05<00:26,  4.68it/s]Epoch: 3, train for the 26-th batch, train loss: 0.6021116375923157:  17%|██▏          | 26/151 [00:05<00:26,  4.69it/s]Epoch: 2, train for the 77-th batch, train loss: 0.23964841663837433:  64%|███████▋    | 76/119 [00:46<00:26,  1.63it/s]Epoch: 2, train for the 77-th batch, train loss: 0.23964841663837433:  65%|███████▊    | 77/119 [00:46<00:25,  1.63it/s]Epoch: 1, train for the 222-th batch, train loss: 0.4788293242454529:  58%|██████▎    | 221/383 [02:12<01:38,  1.65it/s]Epoch: 1, train for the 222-th batch, train loss: 0.4788293242454529:  58%|██████▍    | 222/383 [02:12<01:37,  1.65it/s]Epoch: 3, train for the 27-th batch, train loss: 0.6111553311347961:  17%|██▏          | 26/151 [00:05<00:26,  4.69it/s]Epoch: 3, train for the 27-th batch, train loss: 0.6111553311347961:  18%|██▎          | 27/151 [00:05<00:26,  4.64it/s]Epoch: 2, train for the 52-th batch, train loss: 0.5122402310371399:  35%|████▌        | 51/146 [00:30<00:58,  1.63it/s]Epoch: 2, train for the 52-th batch, train loss: 0.5122402310371399:  36%|████▋        | 52/146 [00:30<00:57,  1.63it/s]Epoch: 1, train for the 230-th batch, train loss: 0.6474679708480835:  97%|██████████▋| 229/237 [02:14<00:04,  1.65it/s]Epoch: 1, train for the 230-th batch, train loss: 0.6474679708480835:  97%|██████████▋| 230/237 [02:14<00:04,  1.65it/s]Epoch: 3, train for the 28-th batch, train loss: 0.5394532084465027:  18%|██▎          | 27/151 [00:05<00:26,  4.64it/s]Epoch: 3, train for the 28-th batch, train loss: 0.5394532084465027:  19%|██▍          | 28/151 [00:05<00:26,  4.66it/s]Epoch: 3, train for the 29-th batch, train loss: 0.600174605846405:  19%|██▌           | 28/151 [00:05<00:26,  4.66it/s]Epoch: 3, train for the 29-th batch, train loss: 0.600174605846405:  19%|██▋           | 29/151 [00:05<00:29,  4.19it/s]Epoch: 1, train for the 223-th batch, train loss: 0.42646360397338867:  58%|█████▊    | 222/383 [02:13<01:37,  1.65it/s]Epoch: 1, train for the 223-th batch, train loss: 0.42646360397338867:  58%|█████▊    | 223/383 [02:13<01:36,  1.65it/s]Epoch: 2, train for the 78-th batch, train loss: 0.2238330841064453:  65%|████████▍    | 77/119 [00:46<00:25,  1.63it/s]Epoch: 2, train for the 78-th batch, train loss: 0.2238330841064453:  66%|████████▌    | 78/119 [00:46<00:25,  1.63it/s]Epoch: 1, train for the 231-th batch, train loss: 0.6384487748146057:  97%|██████████▋| 230/237 [02:15<00:04,  1.65it/s]Epoch: 1, train for the 231-th batch, train loss: 0.6384487748146057:  97%|██████████▋| 231/237 [02:15<00:03,  1.65it/s]Epoch: 2, train for the 53-th batch, train loss: 0.4751347005367279:  36%|████▋        | 52/146 [00:31<00:57,  1.63it/s]Epoch: 2, train for the 53-th batch, train loss: 0.4751347005367279:  36%|████▋        | 53/146 [00:31<00:57,  1.63it/s]Epoch: 3, train for the 30-th batch, train loss: 0.6032084226608276:  19%|██▍          | 29/151 [00:06<00:29,  4.19it/s]Epoch: 3, train for the 30-th batch, train loss: 0.6032084226608276:  20%|██▌          | 30/151 [00:06<00:28,  4.32it/s]Epoch: 3, train for the 31-th batch, train loss: 0.7945178151130676:  20%|██▌          | 30/151 [00:06<00:28,  4.32it/s]Epoch: 3, train for the 31-th batch, train loss: 0.7945178151130676:  21%|██▋          | 31/151 [00:06<00:27,  4.37it/s]Epoch: 2, train for the 79-th batch, train loss: 0.2219165414571762:  66%|████████▌    | 78/119 [00:47<00:25,  1.63it/s]Epoch: 2, train for the 79-th batch, train loss: 0.2219165414571762:  66%|████████▋    | 79/119 [00:47<00:23,  1.73it/s]Epoch: 1, train for the 224-th batch, train loss: 0.4722113013267517:  58%|██████▍    | 223/383 [02:13<01:36,  1.65it/s]Epoch: 1, train for the 224-th batch, train loss: 0.4722113013267517:  58%|██████▍    | 224/383 [02:13<01:35,  1.66it/s]Epoch: 3, train for the 32-th batch, train loss: 0.6531233191490173:  21%|██▋          | 31/151 [00:06<00:27,  4.37it/s]Epoch: 3, train for the 32-th batch, train loss: 0.6531233191490173:  21%|██▊          | 32/151 [00:06<00:26,  4.43it/s]Epoch: 1, train for the 232-th batch, train loss: 0.656183123588562:  97%|███████████▋| 231/237 [02:15<00:03,  1.65it/s]Epoch: 1, train for the 232-th batch, train loss: 0.656183123588562:  98%|███████████▋| 232/237 [02:15<00:03,  1.66it/s]Epoch: 3, train for the 33-th batch, train loss: 0.4664824604988098:  21%|██▊          | 32/151 [00:06<00:26,  4.43it/s]Epoch: 3, train for the 33-th batch, train loss: 0.4664824604988098:  22%|██▊          | 33/151 [00:06<00:25,  4.54it/s]Epoch: 2, train for the 80-th batch, train loss: 0.28491052985191345:  66%|███████▉    | 79/119 [00:47<00:23,  1.73it/s]Epoch: 2, train for the 80-th batch, train loss: 0.28491052985191345:  67%|████████    | 80/119 [00:47<00:20,  1.93it/s]Epoch: 3, train for the 34-th batch, train loss: 0.5106661319732666:  22%|██▊          | 33/151 [00:06<00:25,  4.54it/s]Epoch: 3, train for the 34-th batch, train loss: 0.5106661319732666:  23%|██▉          | 34/151 [00:06<00:25,  4.60it/s]Epoch: 1, train for the 225-th batch, train loss: 0.4598838686943054:  58%|██████▍    | 224/383 [02:14<01:35,  1.66it/s]Epoch: 1, train for the 225-th batch, train loss: 0.4598838686943054:  59%|██████▍    | 225/383 [02:14<01:34,  1.66it/s]Epoch: 3, train for the 35-th batch, train loss: 0.6044737696647644:  23%|██▉          | 34/151 [00:07<00:25,  4.60it/s]Epoch: 3, train for the 35-th batch, train loss: 0.6044737696647644:  23%|███          | 35/151 [00:07<00:25,  4.58it/s]Epoch: 2, train for the 54-th batch, train loss: 0.498203843832016:  36%|█████         | 53/146 [00:32<00:57,  1.63it/s]Epoch: 2, train for the 54-th batch, train loss: 0.498203843832016:  37%|█████▏        | 54/146 [00:32<01:11,  1.29it/s]Epoch: 1, train for the 233-th batch, train loss: 0.6452046036720276:  98%|██████████▊| 232/237 [02:16<00:03,  1.66it/s]Epoch: 1, train for the 233-th batch, train loss: 0.6452046036720276:  98%|██████████▊| 233/237 [02:16<00:02,  1.67it/s]Epoch: 3, train for the 36-th batch, train loss: 0.574064314365387:  23%|███▏          | 35/151 [00:07<00:25,  4.58it/s]Epoch: 3, train for the 36-th batch, train loss: 0.574064314365387:  24%|███▎          | 36/151 [00:07<00:24,  4.63it/s]Epoch: 3, train for the 37-th batch, train loss: 0.6677794456481934:  24%|███          | 36/151 [00:07<00:24,  4.63it/s]Epoch: 3, train for the 37-th batch, train loss: 0.6677794456481934:  25%|███▏         | 37/151 [00:07<00:24,  4.60it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5144347548484802:  37%|████▊        | 54/146 [00:32<01:11,  1.29it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5144347548484802:  38%|████▉        | 55/146 [00:33<01:00,  1.50it/s]Epoch: 2, train for the 81-th batch, train loss: 0.23359587788581848:  67%|████████    | 80/119 [00:48<00:20,  1.93it/s]Epoch: 2, train for the 81-th batch, train loss: 0.23359587788581848:  68%|████████▏   | 81/119 [00:48<00:23,  1.61it/s]Epoch: 1, train for the 226-th batch, train loss: 0.4689965844154358:  59%|██████▍    | 225/383 [02:15<01:34,  1.66it/s]Epoch: 1, train for the 226-th batch, train loss: 0.4689965844154358:  59%|██████▍    | 226/383 [02:15<01:34,  1.67it/s]Epoch: 3, train for the 38-th batch, train loss: 0.5935081839561462:  25%|███▏         | 37/151 [00:07<00:24,  4.60it/s]Epoch: 3, train for the 38-th batch, train loss: 0.5935081839561462:  25%|███▎         | 38/151 [00:07<00:24,  4.60it/s]Epoch: 1, train for the 234-th batch, train loss: 0.6036962270736694:  98%|██████████▊| 233/237 [02:16<00:02,  1.67it/s]Epoch: 1, train for the 234-th batch, train loss: 0.6036962270736694:  99%|██████████▊| 234/237 [02:16<00:01,  1.67it/s]Epoch: 3, train for the 39-th batch, train loss: 0.6378785371780396:  25%|███▎         | 38/151 [00:08<00:24,  4.60it/s]Epoch: 3, train for the 39-th batch, train loss: 0.6378785371780396:  26%|███▎         | 39/151 [00:08<00:24,  4.57it/s]Epoch: 3, train for the 40-th batch, train loss: 0.487965852022171:  26%|███▌          | 39/151 [00:08<00:24,  4.57it/s]Epoch: 3, train for the 40-th batch, train loss: 0.487965852022171:  26%|███▋          | 40/151 [00:08<00:24,  4.62it/s]Epoch: 2, train for the 82-th batch, train loss: 0.250365674495697:  68%|█████████▌    | 81/119 [00:49<00:23,  1.61it/s]Epoch: 1, train for the 227-th batch, train loss: 0.4831371307373047:  59%|██████▍    | 226/383 [02:15<01:34,  1.67it/s]Epoch: 2, train for the 56-th batch, train loss: 0.4966587722301483:  38%|████▉        | 55/146 [00:33<01:00,  1.50it/s]Epoch: 1, train for the 227-th batch, train loss: 0.4831371307373047:  59%|██████▌    | 227/383 [02:15<01:33,  1.67it/s]Epoch: 2, train for the 82-th batch, train loss: 0.250365674495697:  69%|█████████▋    | 82/119 [00:49<00:23,  1.59it/s]Epoch: 2, train for the 56-th batch, train loss: 0.4966587722301483:  38%|████▉        | 56/146 [00:33<00:59,  1.50it/s]Epoch: 1, train for the 235-th batch, train loss: 0.6255925297737122:  99%|██████████▊| 234/237 [02:17<00:01,  1.67it/s]Epoch: 1, train for the 235-th batch, train loss: 0.6255925297737122:  99%|██████████▉| 235/237 [02:17<00:01,  1.67it/s]Epoch: 3, train for the 41-th batch, train loss: 0.6399861574172974:  26%|███▍         | 40/151 [00:08<00:24,  4.62it/s]Epoch: 3, train for the 41-th batch, train loss: 0.6399861574172974:  27%|███▌         | 41/151 [00:08<00:23,  4.58it/s]Epoch: 3, train for the 42-th batch, train loss: 0.5442606210708618:  27%|███▌         | 41/151 [00:08<00:23,  4.58it/s]Epoch: 3, train for the 42-th batch, train loss: 0.5442606210708618:  28%|███▌         | 42/151 [00:08<00:23,  4.57it/s]Epoch: 1, train for the 228-th batch, train loss: 0.4477181136608124:  59%|██████▌    | 227/383 [02:16<01:33,  1.67it/s]Epoch: 1, train for the 228-th batch, train loss: 0.4477181136608124:  60%|██████▌    | 228/383 [02:16<01:33,  1.66it/s]Epoch: 3, train for the 43-th batch, train loss: 0.49851295351982117:  28%|███▎        | 42/151 [00:08<00:23,  4.57it/s]Epoch: 3, train for the 43-th batch, train loss: 0.49851295351982117:  28%|███▍        | 43/151 [00:08<00:23,  4.61it/s]Epoch: 2, train for the 57-th batch, train loss: 0.4964520037174225:  38%|████▉        | 56/146 [00:34<00:59,  1.50it/s]Epoch: 2, train for the 57-th batch, train loss: 0.4964520037174225:  39%|█████        | 57/146 [00:34<00:59,  1.50it/s]Epoch: 2, train for the 83-th batch, train loss: 0.22702036798000336:  69%|████████▎   | 82/119 [00:50<00:23,  1.59it/s]Epoch: 2, train for the 83-th batch, train loss: 0.22702036798000336:  70%|████████▎   | 83/119 [00:50<00:23,  1.56it/s]Epoch: 1, train for the 236-th batch, train loss: 0.6176771521568298:  99%|██████████▉| 235/237 [02:18<00:01,  1.67it/s]Epoch: 1, train for the 236-th batch, train loss: 0.6176771521568298: 100%|██████████▉| 236/237 [02:18<00:00,  1.66it/s]Epoch: 3, train for the 44-th batch, train loss: 0.6220638155937195:  28%|███▋         | 43/151 [00:09<00:23,  4.61it/s]Epoch: 3, train for the 44-th batch, train loss: 0.6220638155937195:  29%|███▊         | 44/151 [00:09<00:23,  4.56it/s]Epoch: 3, train for the 45-th batch, train loss: 0.5052990913391113:  29%|███▊         | 44/151 [00:09<00:23,  4.56it/s]Epoch: 3, train for the 45-th batch, train loss: 0.5052990913391113:  30%|███▊         | 45/151 [00:09<00:23,  4.55it/s]Epoch: 1, train for the 237-th batch, train loss: 0.6771914958953857: 100%|██████████▉| 236/237 [02:18<00:00,  1.66it/s]Epoch: 1, train for the 237-th batch, train loss: 0.6771914958953857: 100%|███████████| 237/237 [02:18<00:00,  1.79it/s]Epoch: 1, train for the 237-th batch, train loss: 0.6771914958953857: 100%|███████████| 237/237 [02:18<00:00,  1.71it/s]
  0%|                                                                                            | 0/66 [00:00<?, ?it/s]Epoch: 1, train for the 229-th batch, train loss: 0.4049087166786194:  60%|██████▌    | 228/383 [02:16<01:33,  1.66it/s]Epoch: 1, train for the 229-th batch, train loss: 0.4049087166786194:  60%|██████▌    | 229/383 [02:16<01:33,  1.64it/s]Epoch: 3, train for the 46-th batch, train loss: 0.5901234149932861:  30%|███▊         | 45/151 [00:09<00:23,  4.55it/s]Epoch: 3, train for the 46-th batch, train loss: 0.5901234149932861:  30%|███▉         | 46/151 [00:09<00:23,  4.55it/s]Epoch: 2, train for the 58-th batch, train loss: 0.48867732286453247:  39%|████▋       | 57/146 [00:34<00:59,  1.50it/s]Epoch: 2, train for the 58-th batch, train loss: 0.48867732286453247:  40%|████▊       | 58/146 [00:34<00:58,  1.50it/s]Epoch: 2, train for the 84-th batch, train loss: 0.19962620735168457:  70%|████████▎   | 83/119 [00:50<00:23,  1.56it/s]Epoch: 2, train for the 84-th batch, train loss: 0.19962620735168457:  71%|████████▍   | 84/119 [00:50<00:22,  1.55it/s]evaluate for the 1-th batch, evaluate loss: 0.5960100889205933:   0%|                            | 0/66 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.5960100889205933:   2%|▎                   | 1/66 [00:00<00:16,  4.04it/s]Epoch: 3, train for the 47-th batch, train loss: 0.6095356345176697:  30%|███▉         | 46/151 [00:09<00:23,  4.55it/s]Epoch: 3, train for the 47-th batch, train loss: 0.6095356345176697:  31%|████         | 47/151 [00:09<00:22,  4.53it/s]Epoch: 3, train for the 48-th batch, train loss: 0.5677658915519714:  31%|████         | 47/151 [00:10<00:22,  4.53it/s]Epoch: 3, train for the 48-th batch, train loss: 0.5677658915519714:  32%|████▏        | 48/151 [00:10<00:22,  4.52it/s]evaluate for the 2-th batch, evaluate loss: 0.5737980604171753:   2%|▎                   | 1/66 [00:00<00:16,  4.04it/s]evaluate for the 2-th batch, evaluate loss: 0.5737980604171753:   3%|▌                   | 2/66 [00:00<00:17,  3.69it/s]Epoch: 1, train for the 230-th batch, train loss: 0.4946724474430084:  60%|██████▌    | 229/383 [02:17<01:33,  1.64it/s]Epoch: 1, train for the 230-th batch, train loss: 0.4946724474430084:  60%|██████▌    | 230/383 [02:17<01:31,  1.66it/s]Epoch: 3, train for the 49-th batch, train loss: 0.5075666904449463:  32%|████▏        | 48/151 [00:10<00:22,  4.52it/s]Epoch: 3, train for the 49-th batch, train loss: 0.5075666904449463:  32%|████▏        | 49/151 [00:10<00:22,  4.55it/s]Epoch: 2, train for the 59-th batch, train loss: 0.48685145378112793:  40%|████▊       | 58/146 [00:35<00:58,  1.50it/s]Epoch: 2, train for the 59-th batch, train loss: 0.48685145378112793:  40%|████▊       | 59/146 [00:35<00:57,  1.51it/s]Epoch: 2, train for the 85-th batch, train loss: 0.23544223606586456:  71%|████████▍   | 84/119 [00:51<00:22,  1.55it/s]Epoch: 2, train for the 85-th batch, train loss: 0.23544223606586456:  71%|████████▌   | 85/119 [00:51<00:22,  1.54it/s]evaluate for the 3-th batch, evaluate loss: 0.6153387427330017:   3%|▌                   | 2/66 [00:00<00:17,  3.69it/s]evaluate for the 3-th batch, evaluate loss: 0.6153387427330017:   5%|▉                   | 3/66 [00:00<00:16,  3.81it/s]Epoch: 3, train for the 50-th batch, train loss: 0.4773360788822174:  32%|████▏        | 49/151 [00:10<00:22,  4.55it/s]Epoch: 3, train for the 50-th batch, train loss: 0.4773360788822174:  33%|████▎        | 50/151 [00:10<00:22,  4.58it/s]evaluate for the 4-th batch, evaluate loss: 0.6094675064086914:   5%|▉                   | 3/66 [00:01<00:16,  3.81it/s]evaluate for the 4-th batch, evaluate loss: 0.6094675064086914:   6%|█▏                  | 4/66 [00:01<00:16,  3.72it/s]Epoch: 3, train for the 51-th batch, train loss: 0.7491073608398438:  33%|████▎        | 50/151 [00:10<00:22,  4.58it/s]Epoch: 3, train for the 51-th batch, train loss: 0.7491073608398438:  34%|████▍        | 51/151 [00:10<00:21,  4.55it/s]Epoch: 1, train for the 231-th batch, train loss: 0.5038891434669495:  60%|██████▌    | 230/383 [02:18<01:31,  1.66it/s]Epoch: 1, train for the 231-th batch, train loss: 0.5038891434669495:  60%|██████▋    | 231/383 [02:18<01:31,  1.66it/s]evaluate for the 5-th batch, evaluate loss: 0.6261422038078308:   6%|█▏                  | 4/66 [00:01<00:16,  3.72it/s]evaluate for the 5-th batch, evaluate loss: 0.6261422038078308:   8%|█▌                  | 5/66 [00:01<00:16,  3.67it/s]Epoch: 3, train for the 52-th batch, train loss: 0.654841959476471:  34%|████▋         | 51/151 [00:10<00:21,  4.55it/s]Epoch: 3, train for the 52-th batch, train loss: 0.654841959476471:  34%|████▊         | 52/151 [00:10<00:21,  4.54it/s]Epoch: 2, train for the 86-th batch, train loss: 0.2591996192932129:  71%|█████████▎   | 85/119 [00:52<00:22,  1.54it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5148246884346008:  40%|█████▎       | 59/146 [00:36<00:57,  1.51it/s]Epoch: 2, train for the 86-th batch, train loss: 0.2591996192932129:  72%|█████████▍   | 86/119 [00:52<00:21,  1.53it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5148246884346008:  41%|█████▎       | 60/146 [00:36<00:57,  1.51it/s]Epoch: 3, train for the 53-th batch, train loss: 0.6384904980659485:  34%|████▍        | 52/151 [00:11<00:21,  4.54it/s]Epoch: 3, train for the 53-th batch, train loss: 0.6384904980659485:  35%|████▌        | 53/151 [00:11<00:21,  4.52it/s]evaluate for the 6-th batch, evaluate loss: 0.6565144062042236:   8%|█▌                  | 5/66 [00:01<00:16,  3.67it/s]evaluate for the 6-th batch, evaluate loss: 0.6565144062042236:   9%|█▊                  | 6/66 [00:01<00:16,  3.69it/s]Epoch: 1, train for the 232-th batch, train loss: 0.4108956456184387:  60%|██████▋    | 231/383 [02:18<01:31,  1.66it/s]Epoch: 1, train for the 232-th batch, train loss: 0.4108956456184387:  61%|██████▋    | 232/383 [02:18<01:30,  1.67it/s]Epoch: 3, train for the 54-th batch, train loss: 0.589972198009491:  35%|████▉         | 53/151 [00:11<00:21,  4.52it/s]Epoch: 3, train for the 54-th batch, train loss: 0.589972198009491:  36%|█████         | 54/151 [00:11<00:21,  4.52it/s]evaluate for the 7-th batch, evaluate loss: 0.62650066614151:   9%|██                    | 6/66 [00:01<00:16,  3.69it/s]evaluate for the 7-th batch, evaluate loss: 0.62650066614151:  11%|██▎                   | 7/66 [00:01<00:16,  3.59it/s]Epoch: 3, train for the 55-th batch, train loss: 0.6309651732444763:  36%|████▋        | 54/151 [00:11<00:21,  4.52it/s]Epoch: 3, train for the 55-th batch, train loss: 0.6309651732444763:  36%|████▋        | 55/151 [00:11<00:21,  4.50it/s]Epoch: 2, train for the 87-th batch, train loss: 0.24513667821884155:  72%|████████▋   | 86/119 [00:52<00:21,  1.53it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5372567772865295:  41%|█████▎       | 60/146 [00:36<00:57,  1.51it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5372567772865295:  42%|█████▍       | 61/146 [00:36<00:56,  1.51it/s]Epoch: 2, train for the 87-th batch, train loss: 0.24513667821884155:  73%|████████▊   | 87/119 [00:52<00:21,  1.52it/s]evaluate for the 8-th batch, evaluate loss: 0.6308931708335876:  11%|██                  | 7/66 [00:02<00:16,  3.59it/s]evaluate for the 8-th batch, evaluate loss: 0.6308931708335876:  12%|██▍                 | 8/66 [00:02<00:15,  3.73it/s]Epoch: 3, train for the 56-th batch, train loss: 0.5151059627532959:  36%|████▋        | 55/151 [00:11<00:21,  4.50it/s]Epoch: 3, train for the 56-th batch, train loss: 0.5151059627532959:  37%|████▊        | 56/151 [00:11<00:21,  4.50it/s]Epoch: 1, train for the 233-th batch, train loss: 0.4551540017127991:  61%|██████▋    | 232/383 [02:19<01:30,  1.67it/s]Epoch: 1, train for the 233-th batch, train loss: 0.4551540017127991:  61%|██████▋    | 233/383 [02:19<01:29,  1.68it/s]evaluate for the 9-th batch, evaluate loss: 0.5673800706863403:  12%|██▍                 | 8/66 [00:02<00:15,  3.73it/s]evaluate for the 9-th batch, evaluate loss: 0.5673800706863403:  14%|██▋                 | 9/66 [00:02<00:15,  3.60it/s]Epoch: 3, train for the 57-th batch, train loss: 0.5554136633872986:  37%|████▊        | 56/151 [00:12<00:21,  4.50it/s]Epoch: 3, train for the 57-th batch, train loss: 0.5554136633872986:  38%|████▉        | 57/151 [00:12<00:20,  4.50it/s]evaluate for the 10-th batch, evaluate loss: 0.5857633352279663:  14%|██▌                | 9/66 [00:02<00:15,  3.60it/s]evaluate for the 10-th batch, evaluate loss: 0.5857633352279663:  15%|██▋               | 10/66 [00:02<00:14,  3.79it/s]Epoch: 3, train for the 58-th batch, train loss: 0.5268005132675171:  38%|████▉        | 57/151 [00:12<00:20,  4.50it/s]Epoch: 3, train for the 58-th batch, train loss: 0.5268005132675171:  38%|████▉        | 58/151 [00:12<00:20,  4.51it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5047734379768372:  42%|█████▍       | 61/146 [00:37<00:56,  1.51it/s]Epoch: 2, train for the 88-th batch, train loss: 0.26003018021583557:  73%|████████▊   | 87/119 [00:53<00:21,  1.52it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5047734379768372:  42%|█████▌       | 62/146 [00:37<00:55,  1.51it/s]Epoch: 2, train for the 88-th batch, train loss: 0.26003018021583557:  74%|████████▊   | 88/119 [00:53<00:20,  1.52it/s]Epoch: 3, train for the 59-th batch, train loss: 0.5440744161605835:  38%|████▉        | 58/151 [00:12<00:20,  4.51it/s]Epoch: 3, train for the 59-th batch, train loss: 0.5440744161605835:  39%|█████        | 59/151 [00:12<00:20,  4.50it/s]Epoch: 1, train for the 234-th batch, train loss: 0.46141549944877625:  61%|██████    | 233/383 [02:19<01:29,  1.68it/s]Epoch: 1, train for the 234-th batch, train loss: 0.46141549944877625:  61%|██████    | 234/383 [02:19<01:28,  1.68it/s]evaluate for the 11-th batch, evaluate loss: 0.58896404504776:  15%|███                 | 10/66 [00:02<00:14,  3.79it/s]evaluate for the 11-th batch, evaluate loss: 0.58896404504776:  17%|███▎                | 11/66 [00:02<00:15,  3.60it/s]Epoch: 3, train for the 60-th batch, train loss: 0.5270827412605286:  39%|█████        | 59/151 [00:12<00:20,  4.50it/s]Epoch: 3, train for the 60-th batch, train loss: 0.5270827412605286:  40%|█████▏       | 60/151 [00:12<00:20,  4.48it/s]evaluate for the 12-th batch, evaluate loss: 0.6274974346160889:  17%|███               | 11/66 [00:03<00:15,  3.60it/s]evaluate for the 12-th batch, evaluate loss: 0.6274974346160889:  18%|███▎              | 12/66 [00:03<00:14,  3.68it/s]Epoch: 3, train for the 61-th batch, train loss: 0.554394543170929:  40%|█████▌        | 60/151 [00:12<00:20,  4.48it/s]Epoch: 3, train for the 61-th batch, train loss: 0.554394543170929:  40%|█████▋        | 61/151 [00:12<00:20,  4.49it/s]Epoch: 2, train for the 63-th batch, train loss: 0.5332750082015991:  42%|█████▌       | 62/146 [00:38<00:55,  1.51it/s]Epoch: 2, train for the 89-th batch, train loss: 0.25101402401924133:  74%|████████▊   | 88/119 [00:54<00:20,  1.52it/s]Epoch: 2, train for the 63-th batch, train loss: 0.5332750082015991:  43%|█████▌       | 63/146 [00:38<00:55,  1.51it/s]Epoch: 2, train for the 89-th batch, train loss: 0.25101402401924133:  75%|████████▉   | 89/119 [00:54<00:19,  1.51it/s]evaluate for the 13-th batch, evaluate loss: 0.6007863283157349:  18%|███▎              | 12/66 [00:03<00:14,  3.68it/s]evaluate for the 13-th batch, evaluate loss: 0.6007863283157349:  20%|███▌              | 13/66 [00:03<00:14,  3.58it/s]Epoch: 1, train for the 235-th batch, train loss: 0.42562949657440186:  61%|██████    | 234/383 [02:20<01:28,  1.68it/s]Epoch: 1, train for the 235-th batch, train loss: 0.42562949657440186:  61%|██████▏   | 235/383 [02:20<01:27,  1.68it/s]Epoch: 3, train for the 62-th batch, train loss: 0.5844323039054871:  40%|█████▎       | 61/151 [00:13<00:20,  4.49it/s]Epoch: 3, train for the 62-th batch, train loss: 0.5844323039054871:  41%|█████▎       | 62/151 [00:13<00:19,  4.50it/s]evaluate for the 14-th batch, evaluate loss: 0.6030161380767822:  20%|███▌              | 13/66 [00:03<00:14,  3.58it/s]evaluate for the 14-th batch, evaluate loss: 0.6030161380767822:  21%|███▊              | 14/66 [00:03<00:14,  3.69it/s]Epoch: 3, train for the 63-th batch, train loss: 0.5404861569404602:  41%|█████▎       | 62/151 [00:13<00:19,  4.50it/s]Epoch: 3, train for the 63-th batch, train loss: 0.5404861569404602:  42%|█████▍       | 63/151 [00:13<00:19,  4.50it/s]Epoch: 3, train for the 64-th batch, train loss: 0.5196181535720825:  42%|█████▍       | 63/151 [00:13<00:19,  4.50it/s]Epoch: 3, train for the 64-th batch, train loss: 0.5196181535720825:  42%|█████▌       | 64/151 [00:13<00:19,  4.50it/s]evaluate for the 15-th batch, evaluate loss: 0.6368400454521179:  21%|███▊              | 14/66 [00:04<00:14,  3.69it/s]evaluate for the 15-th batch, evaluate loss: 0.6368400454521179:  23%|████              | 15/66 [00:04<00:14,  3.62it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5385093688964844:  43%|█████▌       | 63/146 [00:38<00:55,  1.51it/s]Epoch: 2, train for the 90-th batch, train loss: 0.23455871641635895:  75%|████████▉   | 89/119 [00:54<00:19,  1.51it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5385093688964844:  44%|█████▋       | 64/146 [00:38<00:54,  1.50it/s]Epoch: 2, train for the 90-th batch, train loss: 0.23455871641635895:  76%|█████████   | 90/119 [00:54<00:19,  1.51it/s]Epoch: 1, train for the 236-th batch, train loss: 0.45968881249427795:  61%|██████▏   | 235/383 [02:20<01:27,  1.68it/s]Epoch: 1, train for the 236-th batch, train loss: 0.45968881249427795:  62%|██████▏   | 236/383 [02:20<01:26,  1.69it/s]Epoch: 3, train for the 65-th batch, train loss: 0.42017462849617004:  42%|█████       | 64/151 [00:13<00:19,  4.50it/s]Epoch: 3, train for the 65-th batch, train loss: 0.42017462849617004:  43%|█████▏      | 65/151 [00:13<00:18,  4.57it/s]evaluate for the 16-th batch, evaluate loss: 0.5687358975410461:  23%|████              | 15/66 [00:04<00:14,  3.62it/s]evaluate for the 16-th batch, evaluate loss: 0.5687358975410461:  24%|████▎             | 16/66 [00:04<00:13,  3.72it/s]Epoch: 3, train for the 66-th batch, train loss: 0.5286760330200195:  43%|█████▌       | 65/151 [00:14<00:18,  4.57it/s]Epoch: 3, train for the 66-th batch, train loss: 0.5286760330200195:  44%|█████▋       | 66/151 [00:14<00:18,  4.55it/s]evaluate for the 17-th batch, evaluate loss: 0.6074304580688477:  24%|████▎             | 16/66 [00:04<00:13,  3.72it/s]evaluate for the 17-th batch, evaluate loss: 0.6074304580688477:  26%|████▋             | 17/66 [00:04<00:13,  3.66it/s]Epoch: 3, train for the 67-th batch, train loss: 0.5832027792930603:  44%|█████▋       | 66/151 [00:14<00:18,  4.55it/s]Epoch: 3, train for the 67-th batch, train loss: 0.5832027792930603:  44%|█████▊       | 67/151 [00:14<00:18,  4.54it/s]Epoch: 1, train for the 237-th batch, train loss: 0.4590214490890503:  62%|██████▊    | 236/383 [02:21<01:26,  1.69it/s]Epoch: 1, train for the 237-th batch, train loss: 0.4590214490890503:  62%|██████▊    | 237/383 [02:21<01:26,  1.69it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5546578168869019:  44%|█████▋       | 64/146 [00:39<00:54,  1.50it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5546578168869019:  45%|█████▊       | 65/146 [00:39<00:53,  1.51it/s]Epoch: 2, train for the 91-th batch, train loss: 0.22764936089515686:  76%|█████████   | 90/119 [00:55<00:19,  1.51it/s]Epoch: 2, train for the 91-th batch, train loss: 0.22764936089515686:  76%|█████████▏  | 91/119 [00:55<00:18,  1.51it/s]evaluate for the 18-th batch, evaluate loss: 0.6173961758613586:  26%|████▋             | 17/66 [00:04<00:13,  3.66it/s]evaluate for the 18-th batch, evaluate loss: 0.6173961758613586:  27%|████▉             | 18/66 [00:04<00:13,  3.68it/s]Epoch: 3, train for the 68-th batch, train loss: 0.5428814888000488:  44%|█████▊       | 67/151 [00:14<00:18,  4.54it/s]Epoch: 3, train for the 68-th batch, train loss: 0.5428814888000488:  45%|█████▊       | 68/151 [00:14<00:18,  4.53it/s]Epoch: 3, train for the 69-th batch, train loss: 0.57850182056427:  45%|██████▊        | 68/151 [00:14<00:18,  4.53it/s]Epoch: 3, train for the 69-th batch, train loss: 0.57850182056427:  46%|██████▊        | 69/151 [00:14<00:18,  4.52it/s]evaluate for the 19-th batch, evaluate loss: 0.6044793725013733:  27%|████▉             | 18/66 [00:05<00:13,  3.68it/s]evaluate for the 19-th batch, evaluate loss: 0.6044793725013733:  29%|█████▏            | 19/66 [00:05<00:12,  3.68it/s]Epoch: 1, train for the 238-th batch, train loss: 0.44264495372772217:  62%|██████▏   | 237/383 [02:22<01:26,  1.69it/s]Epoch: 1, train for the 238-th batch, train loss: 0.44264495372772217:  62%|██████▏   | 238/383 [02:22<01:26,  1.69it/s]Epoch: 3, train for the 70-th batch, train loss: 0.5452312231063843:  46%|█████▉       | 69/151 [00:14<00:18,  4.52it/s]Epoch: 3, train for the 70-th batch, train loss: 0.5452312231063843:  46%|██████       | 70/151 [00:14<00:17,  4.50it/s]Epoch: 2, train for the 92-th batch, train loss: 0.28543660044670105:  76%|█████████▏  | 91/119 [00:56<00:18,  1.51it/s]Epoch: 2, train for the 92-th batch, train loss: 0.28543660044670105:  77%|█████████▎  | 92/119 [00:56<00:17,  1.52it/s]Epoch: 2, train for the 66-th batch, train loss: 0.530056893825531:  45%|██████▏       | 65/146 [00:40<00:53,  1.51it/s]Epoch: 2, train for the 66-th batch, train loss: 0.530056893825531:  45%|██████▎       | 66/146 [00:40<00:52,  1.51it/s]evaluate for the 20-th batch, evaluate loss: 0.6427825689315796:  29%|█████▏            | 19/66 [00:05<00:12,  3.68it/s]evaluate for the 20-th batch, evaluate loss: 0.6427825689315796:  30%|█████▍            | 20/66 [00:05<00:12,  3.60it/s]Epoch: 3, train for the 71-th batch, train loss: 0.5691151022911072:  46%|██████       | 70/151 [00:15<00:17,  4.50it/s]Epoch: 3, train for the 71-th batch, train loss: 0.5691151022911072:  47%|██████       | 71/151 [00:15<00:17,  4.50it/s]evaluate for the 21-th batch, evaluate loss: 0.6346361637115479:  30%|█████▍            | 20/66 [00:05<00:12,  3.60it/s]evaluate for the 21-th batch, evaluate loss: 0.6346361637115479:  32%|█████▋            | 21/66 [00:05<00:12,  3.72it/s]Epoch: 3, train for the 72-th batch, train loss: 0.474517285823822:  47%|██████▌       | 71/151 [00:15<00:17,  4.50it/s]Epoch: 3, train for the 72-th batch, train loss: 0.474517285823822:  48%|██████▋       | 72/151 [00:15<00:17,  4.54it/s]Epoch: 1, train for the 239-th batch, train loss: 0.45517459511756897:  62%|██████▏   | 238/383 [02:22<01:26,  1.69it/s]Epoch: 1, train for the 239-th batch, train loss: 0.45517459511756897:  62%|██████▏   | 239/383 [02:22<01:24,  1.70it/s]evaluate for the 22-th batch, evaluate loss: 0.6260539889335632:  32%|█████▋            | 21/66 [00:05<00:12,  3.72it/s]evaluate for the 22-th batch, evaluate loss: 0.6260539889335632:  33%|██████            | 22/66 [00:05<00:12,  3.61it/s]Epoch: 3, train for the 73-th batch, train loss: 0.5675845742225647:  48%|██████▏      | 72/151 [00:15<00:17,  4.54it/s]Epoch: 3, train for the 73-th batch, train loss: 0.5675845742225647:  48%|██████▎      | 73/151 [00:15<00:17,  4.52it/s]Epoch: 2, train for the 93-th batch, train loss: 0.20876619219779968:  77%|█████████▎  | 92/119 [00:56<00:17,  1.52it/s]Epoch: 2, train for the 93-th batch, train loss: 0.20876619219779968:  78%|█████████▍  | 93/119 [00:56<00:17,  1.53it/s]Epoch: 2, train for the 67-th batch, train loss: 0.5633720755577087:  45%|█████▉       | 66/146 [00:40<00:52,  1.51it/s]Epoch: 2, train for the 67-th batch, train loss: 0.5633720755577087:  46%|█████▉       | 67/146 [00:40<00:51,  1.52it/s]evaluate for the 23-th batch, evaluate loss: 0.6382364630699158:  33%|██████            | 22/66 [00:06<00:12,  3.61it/s]evaluate for the 23-th batch, evaluate loss: 0.6382364630699158:  35%|██████▎           | 23/66 [00:06<00:11,  3.79it/s]Epoch: 3, train for the 74-th batch, train loss: 0.4643218219280243:  48%|██████▎      | 73/151 [00:15<00:17,  4.52it/s]Epoch: 3, train for the 74-th batch, train loss: 0.4643218219280243:  49%|██████▎      | 74/151 [00:15<00:16,  4.58it/s]Epoch: 3, train for the 75-th batch, train loss: 0.4530022442340851:  49%|██████▎      | 74/151 [00:15<00:16,  4.58it/s]Epoch: 3, train for the 75-th batch, train loss: 0.4530022442340851:  50%|██████▍      | 75/151 [00:15<00:16,  4.60it/s]Epoch: 1, train for the 240-th batch, train loss: 0.489852637052536:  62%|███████▍    | 239/383 [02:23<01:24,  1.70it/s]Epoch: 1, train for the 240-th batch, train loss: 0.489852637052536:  63%|███████▌    | 240/383 [02:23<01:24,  1.69it/s]evaluate for the 24-th batch, evaluate loss: 0.6362771391868591:  35%|██████▎           | 23/66 [00:06<00:11,  3.79it/s]evaluate for the 24-th batch, evaluate loss: 0.6362771391868591:  36%|██████▌           | 24/66 [00:06<00:11,  3.61it/s]Epoch: 3, train for the 76-th batch, train loss: 0.5677220821380615:  50%|██████▍      | 75/151 [00:16<00:16,  4.60it/s]Epoch: 3, train for the 76-th batch, train loss: 0.5677220821380615:  50%|██████▌      | 76/151 [00:16<00:16,  4.56it/s]Epoch: 2, train for the 94-th batch, train loss: 0.24014659225940704:  78%|█████████▍  | 93/119 [00:57<00:17,  1.53it/s]Epoch: 2, train for the 94-th batch, train loss: 0.24014659225940704:  79%|█████████▍  | 94/119 [00:57<00:16,  1.53it/s]Epoch: 2, train for the 68-th batch, train loss: 0.5525598526000977:  46%|█████▉       | 67/146 [00:41<00:51,  1.52it/s]Epoch: 2, train for the 68-th batch, train loss: 0.5525598526000977:  47%|██████       | 68/146 [00:41<00:51,  1.53it/s]evaluate for the 25-th batch, evaluate loss: 0.6578611731529236:  36%|██████▌           | 24/66 [00:06<00:11,  3.61it/s]evaluate for the 25-th batch, evaluate loss: 0.6578611731529236:  38%|██████▊           | 25/66 [00:06<00:11,  3.68it/s]Epoch: 3, train for the 77-th batch, train loss: 0.4883913993835449:  50%|██████▌      | 76/151 [00:16<00:16,  4.56it/s]Epoch: 3, train for the 77-th batch, train loss: 0.4883913993835449:  51%|██████▋      | 77/151 [00:16<00:16,  4.58it/s]Epoch: 1, train for the 241-th batch, train loss: 0.493081659078598:  63%|███████▌    | 240/383 [02:23<01:24,  1.69it/s]Epoch: 1, train for the 241-th batch, train loss: 0.493081659078598:  63%|███████▌    | 241/383 [02:23<01:24,  1.69it/s]evaluate for the 26-th batch, evaluate loss: 0.6197276711463928:  38%|██████▊           | 25/66 [00:07<00:11,  3.68it/s]evaluate for the 26-th batch, evaluate loss: 0.6197276711463928:  39%|███████           | 26/66 [00:07<00:11,  3.52it/s]Epoch: 3, train for the 78-th batch, train loss: 0.5352678298950195:  51%|██████▋      | 77/151 [00:16<00:16,  4.58it/s]Epoch: 3, train for the 78-th batch, train loss: 0.5352678298950195:  52%|██████▋      | 78/151 [00:16<00:16,  4.56it/s]Epoch: 3, train for the 79-th batch, train loss: 0.5844718217849731:  52%|██████▋      | 78/151 [00:16<00:16,  4.56it/s]Epoch: 3, train for the 79-th batch, train loss: 0.5844718217849731:  52%|██████▊      | 79/151 [00:16<00:15,  4.55it/s]Epoch: 2, train for the 95-th batch, train loss: 0.19356444478034973:  79%|█████████▍  | 94/119 [00:57<00:16,  1.53it/s]Epoch: 2, train for the 95-th batch, train loss: 0.19356444478034973:  80%|█████████▌  | 95/119 [00:57<00:15,  1.53it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5400322079658508:  47%|██████       | 68/146 [00:42<00:51,  1.53it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5400322079658508:  47%|██████▏      | 69/146 [00:42<00:50,  1.53it/s]evaluate for the 27-th batch, evaluate loss: 0.6334802508354187:  39%|███████           | 26/66 [00:07<00:11,  3.52it/s]evaluate for the 27-th batch, evaluate loss: 0.6334802508354187:  41%|███████▎          | 27/66 [00:07<00:10,  3.60it/s]Epoch: 3, train for the 80-th batch, train loss: 0.5276718735694885:  52%|██████▊      | 79/151 [00:17<00:15,  4.55it/s]Epoch: 3, train for the 80-th batch, train loss: 0.5276718735694885:  53%|██████▉      | 80/151 [00:17<00:15,  4.53it/s]evaluate for the 28-th batch, evaluate loss: 0.6310122013092041:  41%|███████▎          | 27/66 [00:07<00:10,  3.60it/s]evaluate for the 28-th batch, evaluate loss: 0.6310122013092041:  42%|███████▋          | 28/66 [00:07<00:10,  3.54it/s]Epoch: 1, train for the 242-th batch, train loss: 0.5106199383735657:  63%|██████▉    | 241/383 [02:24<01:24,  1.69it/s]Epoch: 1, train for the 242-th batch, train loss: 0.5106199383735657:  63%|██████▉    | 242/383 [02:24<01:23,  1.70it/s]Epoch: 3, train for the 81-th batch, train loss: 0.5379113554954529:  53%|██████▉      | 80/151 [00:17<00:15,  4.53it/s]Epoch: 3, train for the 81-th batch, train loss: 0.5379113554954529:  54%|██████▉      | 81/151 [00:17<00:15,  4.52it/s]evaluate for the 29-th batch, evaluate loss: 0.6180460453033447:  42%|███████▋          | 28/66 [00:07<00:10,  3.54it/s]evaluate for the 29-th batch, evaluate loss: 0.6180460453033447:  44%|███████▉          | 29/66 [00:07<00:10,  3.67it/s]Epoch: 2, train for the 96-th batch, train loss: 0.21663224697113037:  80%|█████████▌  | 95/119 [00:58<00:15,  1.53it/s]Epoch: 2, train for the 96-th batch, train loss: 0.21663224697113037:  81%|█████████▋  | 96/119 [00:58<00:14,  1.54it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5633696913719177:  47%|██████▏      | 69/146 [00:42<00:50,  1.53it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5633696913719177:  48%|██████▏      | 70/146 [00:42<00:49,  1.54it/s]Epoch: 3, train for the 82-th batch, train loss: 0.5599192976951599:  54%|██████▉      | 81/151 [00:17<00:15,  4.52it/s]Epoch: 3, train for the 82-th batch, train loss: 0.5599192976951599:  54%|███████      | 82/151 [00:17<00:15,  4.52it/s]evaluate for the 30-th batch, evaluate loss: 0.6272570490837097:  44%|███████▉          | 29/66 [00:08<00:10,  3.67it/s]evaluate for the 30-th batch, evaluate loss: 0.6272570490837097:  45%|████████▏         | 30/66 [00:08<00:10,  3.60it/s]Epoch: 3, train for the 83-th batch, train loss: 0.57167649269104:  54%|████████▏      | 82/151 [00:17<00:15,  4.52it/s]Epoch: 3, train for the 83-th batch, train loss: 0.57167649269104:  55%|████████▏      | 83/151 [00:17<00:15,  4.51it/s]Epoch: 1, train for the 243-th batch, train loss: 0.47416526079177856:  63%|██████▎   | 242/383 [02:25<01:23,  1.70it/s]Epoch: 1, train for the 243-th batch, train loss: 0.47416526079177856:  63%|██████▎   | 243/383 [02:25<01:22,  1.71it/s]Epoch: 3, train for the 84-th batch, train loss: 0.5836851596832275:  55%|███████▏     | 83/151 [00:17<00:15,  4.51it/s]Epoch: 3, train for the 84-th batch, train loss: 0.5836851596832275:  56%|███████▏     | 84/151 [00:17<00:14,  4.50it/s]evaluate for the 31-th batch, evaluate loss: 0.6301016807556152:  45%|████████▏         | 30/66 [00:08<00:10,  3.60it/s]evaluate for the 31-th batch, evaluate loss: 0.6301016807556152:  47%|████████▍         | 31/66 [00:08<00:09,  3.70it/s]Epoch: 2, train for the 97-th batch, train loss: 0.2506767809391022:  81%|██████████▍  | 96/119 [00:59<00:14,  1.54it/s]Epoch: 2, train for the 97-th batch, train loss: 0.2506767809391022:  82%|██████████▌  | 97/119 [00:59<00:14,  1.53it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5518028140068054:  48%|██████▏      | 70/146 [00:43<00:49,  1.54it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5518028140068054:  49%|██████▎      | 71/146 [00:43<00:48,  1.53it/s]Epoch: 3, train for the 85-th batch, train loss: 0.5622137188911438:  56%|███████▏     | 84/151 [00:18<00:14,  4.50it/s]Epoch: 3, train for the 85-th batch, train loss: 0.5622137188911438:  56%|███████▎     | 85/151 [00:18<00:14,  4.50it/s]evaluate for the 32-th batch, evaluate loss: 0.615580141544342:  47%|████████▉          | 31/66 [00:08<00:09,  3.70it/s]evaluate for the 32-th batch, evaluate loss: 0.615580141544342:  48%|█████████▏         | 32/66 [00:08<00:09,  3.64it/s]Epoch: 1, train for the 244-th batch, train loss: 0.3916128873825073:  63%|██████▉    | 243/383 [02:25<01:22,  1.71it/s]Epoch: 1, train for the 244-th batch, train loss: 0.3916128873825073:  64%|███████    | 244/383 [02:25<01:21,  1.70it/s]Epoch: 3, train for the 86-th batch, train loss: 0.544160783290863:  56%|███████▉      | 85/151 [00:18<00:14,  4.50it/s]Epoch: 3, train for the 86-th batch, train loss: 0.544160783290863:  57%|███████▉      | 86/151 [00:18<00:14,  4.49it/s]evaluate for the 33-th batch, evaluate loss: 0.6176404356956482:  48%|████████▋         | 32/66 [00:09<00:09,  3.64it/s]evaluate for the 33-th batch, evaluate loss: 0.6176404356956482:  50%|█████████         | 33/66 [00:09<00:09,  3.66it/s]Epoch: 3, train for the 87-th batch, train loss: 0.5639997720718384:  57%|███████▍     | 86/151 [00:18<00:14,  4.49it/s]Epoch: 3, train for the 87-th batch, train loss: 0.5639997720718384:  58%|███████▍     | 87/151 [00:18<00:14,  4.48it/s]evaluate for the 34-th batch, evaluate loss: 0.6096981167793274:  50%|█████████         | 33/66 [00:09<00:09,  3.66it/s]evaluate for the 34-th batch, evaluate loss: 0.6096981167793274:  52%|█████████▎        | 34/66 [00:09<00:08,  3.68it/s]Epoch: 2, train for the 98-th batch, train loss: 0.1967332810163498:  82%|██████████▌  | 97/119 [00:59<00:14,  1.53it/s]Epoch: 2, train for the 98-th batch, train loss: 0.1967332810163498:  82%|██████████▋  | 98/119 [00:59<00:13,  1.53it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5289677977561951:  49%|██████▎      | 71/146 [00:44<00:48,  1.53it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5289677977561951:  49%|██████▍      | 72/146 [00:44<00:48,  1.53it/s]Epoch: 3, train for the 88-th batch, train loss: 0.5537593960762024:  58%|███████▍     | 87/151 [00:18<00:14,  4.48it/s]Epoch: 3, train for the 88-th batch, train loss: 0.5537593960762024:  58%|███████▌     | 88/151 [00:18<00:13,  4.50it/s]Epoch: 1, train for the 245-th batch, train loss: 0.4560592472553253:  64%|███████    | 244/383 [02:26<01:21,  1.70it/s]Epoch: 1, train for the 245-th batch, train loss: 0.4560592472553253:  64%|███████    | 245/383 [02:26<01:21,  1.69it/s]Epoch: 3, train for the 89-th batch, train loss: 0.533600389957428:  58%|████████▏     | 88/151 [00:19<00:13,  4.50it/s]Epoch: 3, train for the 89-th batch, train loss: 0.533600389957428:  59%|████████▎     | 89/151 [00:19<00:13,  4.50it/s]evaluate for the 35-th batch, evaluate loss: 0.6172204613685608:  52%|█████████▎        | 34/66 [00:09<00:08,  3.68it/s]evaluate for the 35-th batch, evaluate loss: 0.6172204613685608:  53%|█████████▌        | 35/66 [00:09<00:08,  3.59it/s]Epoch: 3, train for the 90-th batch, train loss: 0.5324240922927856:  59%|███████▋     | 89/151 [00:19<00:13,  4.50it/s]Epoch: 3, train for the 90-th batch, train loss: 0.5324240922927856:  60%|███████▋     | 90/151 [00:19<00:13,  4.50it/s]evaluate for the 36-th batch, evaluate loss: 0.6508740186691284:  53%|█████████▌        | 35/66 [00:09<00:08,  3.59it/s]evaluate for the 36-th batch, evaluate loss: 0.6508740186691284:  55%|█████████▊        | 36/66 [00:09<00:08,  3.72it/s]Epoch: 2, train for the 99-th batch, train loss: 0.23278243839740753:  82%|█████████▉  | 98/119 [01:00<00:13,  1.53it/s]Epoch: 2, train for the 99-th batch, train loss: 0.23278243839740753:  83%|█████████▉  | 99/119 [01:00<00:13,  1.53it/s]Epoch: 2, train for the 73-th batch, train loss: 0.5439650416374207:  49%|██████▍      | 72/146 [00:44<00:48,  1.53it/s]Epoch: 2, train for the 73-th batch, train loss: 0.5439650416374207:  50%|██████▌      | 73/146 [00:44<00:47,  1.53it/s]Epoch: 3, train for the 91-th batch, train loss: 0.4989891052246094:  60%|███████▋     | 90/151 [00:19<00:13,  4.50it/s]Epoch: 3, train for the 91-th batch, train loss: 0.4989891052246094:  60%|███████▊     | 91/151 [00:19<00:13,  4.51it/s]Epoch: 1, train for the 246-th batch, train loss: 0.49070897698402405:  64%|██████▍   | 245/383 [02:26<01:21,  1.69it/s]Epoch: 1, train for the 246-th batch, train loss: 0.49070897698402405:  64%|██████▍   | 246/383 [02:26<01:20,  1.70it/s]evaluate for the 37-th batch, evaluate loss: 0.6591811776161194:  55%|█████████▊        | 36/66 [00:10<00:08,  3.72it/s]evaluate for the 37-th batch, evaluate loss: 0.6591811776161194:  56%|██████████        | 37/66 [00:10<00:08,  3.60it/s]Epoch: 3, train for the 92-th batch, train loss: 0.5845511555671692:  60%|███████▊     | 91/151 [00:19<00:13,  4.51it/s]Epoch: 3, train for the 92-th batch, train loss: 0.5845511555671692:  61%|███████▉     | 92/151 [00:19<00:13,  4.51it/s]evaluate for the 38-th batch, evaluate loss: 0.6199987530708313:  56%|██████████        | 37/66 [00:10<00:08,  3.60it/s]evaluate for the 38-th batch, evaluate loss: 0.6199987530708313:  58%|██████████▎       | 38/66 [00:10<00:07,  3.78it/s]Epoch: 3, train for the 93-th batch, train loss: 0.5334386229515076:  61%|███████▉     | 92/151 [00:19<00:13,  4.51it/s]Epoch: 3, train for the 93-th batch, train loss: 0.5334386229515076:  62%|████████     | 93/151 [00:19<00:12,  4.52it/s]Epoch: 2, train for the 74-th batch, train loss: 0.5324751734733582:  50%|██████▌      | 73/146 [00:45<00:47,  1.53it/s]Epoch: 2, train for the 74-th batch, train loss: 0.5324751734733582:  51%|██████▌      | 74/146 [00:45<00:43,  1.64it/s]Epoch: 1, train for the 247-th batch, train loss: 0.4118674099445343:  64%|███████    | 246/383 [02:27<01:20,  1.70it/s]Epoch: 1, train for the 247-th batch, train loss: 0.4118674099445343:  64%|███████    | 247/383 [02:27<01:20,  1.69it/s]Epoch: 2, train for the 100-th batch, train loss: 0.229971781373024:  83%|██████████▊  | 99/119 [01:01<00:13,  1.53it/s]Epoch: 2, train for the 100-th batch, train loss: 0.229971781373024:  84%|██████████  | 100/119 [01:01<00:12,  1.51it/s]evaluate for the 39-th batch, evaluate loss: 0.615814745426178:  58%|██████████▉        | 38/66 [00:10<00:07,  3.78it/s]evaluate for the 39-th batch, evaluate loss: 0.615814745426178:  59%|███████████▏       | 39/66 [00:10<00:07,  3.60it/s]Epoch: 3, train for the 94-th batch, train loss: 0.5509316325187683:  62%|████████     | 93/151 [00:20<00:12,  4.52it/s]Epoch: 3, train for the 94-th batch, train loss: 0.5509316325187683:  62%|████████     | 94/151 [00:20<00:12,  4.49it/s]Epoch: 3, train for the 95-th batch, train loss: 0.5332457423210144:  62%|████████     | 94/151 [00:20<00:12,  4.49it/s]Epoch: 3, train for the 95-th batch, train loss: 0.5332457423210144:  63%|████████▏    | 95/151 [00:20<00:12,  4.50it/s]evaluate for the 40-th batch, evaluate loss: 0.6232169270515442:  59%|██████████▋       | 39/66 [00:10<00:07,  3.60it/s]evaluate for the 40-th batch, evaluate loss: 0.6232169270515442:  61%|██████████▉       | 40/66 [00:10<00:07,  3.68it/s]Epoch: 2, train for the 75-th batch, train loss: 0.5529530048370361:  51%|██████▌      | 74/146 [00:45<00:43,  1.64it/s]Epoch: 2, train for the 75-th batch, train loss: 0.5529530048370361:  51%|██████▋      | 75/146 [00:45<00:43,  1.62it/s]Epoch: 3, train for the 96-th batch, train loss: 0.5570382475852966:  63%|████████▏    | 95/151 [00:20<00:12,  4.50it/s]Epoch: 3, train for the 96-th batch, train loss: 0.5570382475852966:  64%|████████▎    | 96/151 [00:20<00:12,  4.49it/s]Epoch: 1, train for the 248-th batch, train loss: 0.4126279354095459:  64%|███████    | 247/383 [02:28<01:20,  1.69it/s]Epoch: 1, train for the 248-th batch, train loss: 0.4126279354095459:  65%|███████    | 248/383 [02:28<01:19,  1.69it/s]evaluate for the 41-th batch, evaluate loss: 0.5857915282249451:  61%|██████████▉       | 40/66 [00:11<00:07,  3.68it/s]evaluate for the 41-th batch, evaluate loss: 0.5857915282249451:  62%|███████████▏      | 41/66 [00:11<00:07,  3.52it/s]Epoch: 2, train for the 101-th batch, train loss: 0.20074063539505005:  84%|████████▍ | 100/119 [01:01<00:12,  1.51it/s]Epoch: 2, train for the 101-th batch, train loss: 0.20074063539505005:  85%|████████▍ | 101/119 [01:01<00:11,  1.54it/s]Epoch: 3, train for the 97-th batch, train loss: 0.6086395382881165:  64%|████████▎    | 96/151 [00:20<00:12,  4.49it/s]Epoch: 3, train for the 97-th batch, train loss: 0.6086395382881165:  64%|████████▎    | 97/151 [00:20<00:12,  4.49it/s]evaluate for the 42-th batch, evaluate loss: 0.6199360489845276:  62%|███████████▏      | 41/66 [00:11<00:07,  3.52it/s]evaluate for the 42-th batch, evaluate loss: 0.6199360489845276:  64%|███████████▍      | 42/66 [00:11<00:06,  3.57it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5551945567131042:  51%|██████▋      | 75/146 [00:46<00:43,  1.62it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5551945567131042:  52%|██████▊      | 76/146 [00:46<00:43,  1.61it/s]evaluate for the 43-th batch, evaluate loss: 0.591704249382019:  64%|████████████       | 42/66 [00:11<00:06,  3.57it/s]evaluate for the 43-th batch, evaluate loss: 0.591704249382019:  65%|████████████▍      | 43/66 [00:11<00:06,  3.52it/s]Epoch: 3, train for the 98-th batch, train loss: 0.589057207107544:  64%|████████▉     | 97/151 [00:21<00:12,  4.49it/s]Epoch: 3, train for the 98-th batch, train loss: 0.589057207107544:  65%|█████████     | 98/151 [00:21<00:15,  3.36it/s]Epoch: 1, train for the 249-th batch, train loss: 0.4997948408126831:  65%|███████    | 248/383 [02:28<01:19,  1.69it/s]Epoch: 1, train for the 249-th batch, train loss: 0.4997948408126831:  65%|███████▏   | 249/383 [02:28<01:18,  1.70it/s]Epoch: 2, train for the 102-th batch, train loss: 0.20365206897258759:  85%|████████▍ | 101/119 [01:02<00:11,  1.54it/s]Epoch: 2, train for the 102-th batch, train loss: 0.20365206897258759:  86%|████████▌ | 102/119 [01:02<00:10,  1.55it/s]Epoch: 3, train for the 99-th batch, train loss: 0.6386563181877136:  65%|████████▍    | 98/151 [00:21<00:15,  3.36it/s]Epoch: 3, train for the 99-th batch, train loss: 0.6386563181877136:  66%|████████▌    | 99/151 [00:21<00:14,  3.62it/s]evaluate for the 44-th batch, evaluate loss: 0.5811989307403564:  65%|███████████▋      | 43/66 [00:12<00:06,  3.52it/s]evaluate for the 44-th batch, evaluate loss: 0.5811989307403564:  67%|████████████      | 44/66 [00:12<00:06,  3.65it/s]Epoch: 3, train for the 100-th batch, train loss: 0.7064111232757568:  66%|███████▊    | 99/151 [00:21<00:14,  3.62it/s]Epoch: 3, train for the 100-th batch, train loss: 0.7064111232757568:  66%|███████▎   | 100/151 [00:21<00:13,  3.84it/s]evaluate for the 45-th batch, evaluate loss: 0.6287529468536377:  67%|████████████      | 44/66 [00:12<00:06,  3.65it/s]evaluate for the 45-th batch, evaluate loss: 0.6287529468536377:  68%|████████████▎     | 45/66 [00:12<00:05,  3.60it/s]Epoch: 2, train for the 77-th batch, train loss: 0.5674751996994019:  52%|██████▊      | 76/146 [00:47<00:43,  1.61it/s]Epoch: 2, train for the 77-th batch, train loss: 0.5674751996994019:  53%|██████▊      | 77/146 [00:47<00:42,  1.61it/s]Epoch: 1, train for the 250-th batch, train loss: 0.482424795627594:  65%|███████▊    | 249/383 [02:29<01:18,  1.70it/s]Epoch: 1, train for the 250-th batch, train loss: 0.482424795627594:  65%|███████▊    | 250/383 [02:29<01:18,  1.70it/s]Epoch: 3, train for the 101-th batch, train loss: 0.6691094636917114:  66%|███████▎   | 100/151 [00:22<00:13,  3.84it/s]Epoch: 3, train for the 101-th batch, train loss: 0.6691094636917114:  67%|███████▎   | 101/151 [00:22<00:12,  4.02it/s]Epoch: 2, train for the 103-th batch, train loss: 0.2406945377588272:  86%|█████████▍ | 102/119 [01:03<00:10,  1.55it/s]Epoch: 2, train for the 103-th batch, train loss: 0.2406945377588272:  87%|█████████▌ | 103/119 [01:03<00:10,  1.57it/s]evaluate for the 46-th batch, evaluate loss: 0.6347307562828064:  68%|████████████▎     | 45/66 [00:12<00:05,  3.60it/s]evaluate for the 46-th batch, evaluate loss: 0.6347307562828064:  70%|████████████▌     | 46/66 [00:12<00:05,  3.70it/s]Epoch: 3, train for the 102-th batch, train loss: 0.6295356750488281:  67%|███████▎   | 101/151 [00:22<00:12,  4.02it/s]Epoch: 3, train for the 102-th batch, train loss: 0.6295356750488281:  68%|███████▍   | 102/151 [00:22<00:11,  4.15it/s]evaluate for the 47-th batch, evaluate loss: 0.638435423374176:  70%|█████████████▏     | 46/66 [00:12<00:05,  3.70it/s]evaluate for the 47-th batch, evaluate loss: 0.638435423374176:  71%|█████████████▌     | 47/66 [00:12<00:05,  3.64it/s]Epoch: 3, train for the 103-th batch, train loss: 0.6528352499008179:  68%|███████▍   | 102/151 [00:22<00:11,  4.15it/s]Epoch: 3, train for the 103-th batch, train loss: 0.6528352499008179:  68%|███████▌   | 103/151 [00:22<00:11,  4.25it/s]Epoch: 2, train for the 78-th batch, train loss: 0.5450331568717957:  53%|██████▊      | 77/146 [00:47<00:42,  1.61it/s]Epoch: 2, train for the 78-th batch, train loss: 0.5450331568717957:  53%|██████▉      | 78/146 [00:47<00:42,  1.61it/s]Epoch: 1, train for the 251-th batch, train loss: 0.47043997049331665:  65%|██████▌   | 250/383 [02:29<01:18,  1.70it/s]Epoch: 1, train for the 251-th batch, train loss: 0.47043997049331665:  66%|██████▌   | 251/383 [02:29<01:17,  1.69it/s]Epoch: 2, train for the 104-th batch, train loss: 0.2725820541381836:  87%|█████████▌ | 103/119 [01:03<00:10,  1.57it/s]Epoch: 2, train for the 104-th batch, train loss: 0.2725820541381836:  87%|█████████▌ | 104/119 [01:03<00:09,  1.59it/s]evaluate for the 48-th batch, evaluate loss: 0.6342331171035767:  71%|████████████▊     | 47/66 [00:13<00:05,  3.64it/s]evaluate for the 48-th batch, evaluate loss: 0.6342331171035767:  73%|█████████████     | 48/66 [00:13<00:04,  3.66it/s]Epoch: 3, train for the 104-th batch, train loss: 0.642929196357727:  68%|████████▏   | 103/151 [00:22<00:11,  4.25it/s]Epoch: 3, train for the 104-th batch, train loss: 0.642929196357727:  69%|████████▎   | 104/151 [00:22<00:10,  4.31it/s]Epoch: 3, train for the 105-th batch, train loss: 0.5843676328659058:  69%|███████▌   | 104/151 [00:22<00:10,  4.31it/s]Epoch: 3, train for the 105-th batch, train loss: 0.5843676328659058:  70%|███████▋   | 105/151 [00:22<00:10,  4.36it/s]evaluate for the 49-th batch, evaluate loss: 0.6531112790107727:  73%|█████████████     | 48/66 [00:13<00:04,  3.66it/s]evaluate for the 49-th batch, evaluate loss: 0.6531112790107727:  74%|█████████████▎    | 49/66 [00:13<00:04,  3.68it/s]Epoch: 1, train for the 252-th batch, train loss: 0.41704925894737244:  66%|██████▌   | 251/383 [02:30<01:17,  1.69it/s]Epoch: 1, train for the 252-th batch, train loss: 0.41704925894737244:  66%|██████▌   | 252/383 [02:30<01:17,  1.70it/s]Epoch: 2, train for the 79-th batch, train loss: 0.5685860514640808:  53%|██████▉      | 78/146 [00:48<00:42,  1.61it/s]Epoch: 2, train for the 79-th batch, train loss: 0.5685860514640808:  54%|███████      | 79/146 [00:48<00:41,  1.62it/s]Epoch: 3, train for the 106-th batch, train loss: 0.5229341387748718:  70%|███████▋   | 105/151 [00:23<00:10,  4.36it/s]Epoch: 3, train for the 106-th batch, train loss: 0.5229341387748718:  70%|███████▋   | 106/151 [00:23<00:10,  4.40it/s]evaluate for the 50-th batch, evaluate loss: 0.6389980316162109:  74%|█████████████▎    | 49/66 [00:13<00:04,  3.68it/s]evaluate for the 50-th batch, evaluate loss: 0.6389980316162109:  76%|█████████████▋    | 50/66 [00:13<00:04,  3.58it/s]Epoch: 2, train for the 105-th batch, train loss: 0.19461490213871002:  87%|████████▋ | 104/119 [01:04<00:09,  1.59it/s]Epoch: 2, train for the 105-th batch, train loss: 0.19461490213871002:  88%|████████▊ | 105/119 [01:04<00:08,  1.60it/s]Epoch: 3, train for the 107-th batch, train loss: 0.537260890007019:  70%|████████▍   | 106/151 [00:23<00:10,  4.40it/s]Epoch: 3, train for the 107-th batch, train loss: 0.537260890007019:  71%|████████▌   | 107/151 [00:23<00:09,  4.45it/s]evaluate for the 51-th batch, evaluate loss: 0.6054742336273193:  76%|█████████████▋    | 50/66 [00:13<00:04,  3.58it/s]evaluate for the 51-th batch, evaluate loss: 0.6054742336273193:  77%|█████████████▉    | 51/66 [00:13<00:04,  3.72it/s]Epoch: 3, train for the 108-th batch, train loss: 0.5772083401679993:  71%|███████▊   | 107/151 [00:23<00:09,  4.45it/s]Epoch: 3, train for the 108-th batch, train loss: 0.5772083401679993:  72%|███████▊   | 108/151 [00:23<00:09,  4.48it/s]Epoch: 1, train for the 253-th batch, train loss: 0.4144112169742584:  66%|███████▏   | 252/383 [02:31<01:17,  1.70it/s]Epoch: 1, train for the 253-th batch, train loss: 0.4144112169742584:  66%|███████▎   | 253/383 [02:31<01:16,  1.70it/s]Epoch: 2, train for the 80-th batch, train loss: 0.552466094493866:  54%|███████▌      | 79/146 [00:49<00:41,  1.62it/s]Epoch: 2, train for the 80-th batch, train loss: 0.552466094493866:  55%|███████▋      | 80/146 [00:49<00:40,  1.62it/s]evaluate for the 52-th batch, evaluate loss: 0.6266338229179382:  77%|█████████████▉    | 51/66 [00:14<00:04,  3.72it/s]evaluate for the 52-th batch, evaluate loss: 0.6266338229179382:  79%|██████████████▏   | 52/66 [00:14<00:03,  3.60it/s]Epoch: 3, train for the 109-th batch, train loss: 0.5497662425041199:  72%|███████▊   | 108/151 [00:23<00:09,  4.48it/s]Epoch: 3, train for the 109-th batch, train loss: 0.5497662425041199:  72%|███████▉   | 109/151 [00:23<00:09,  4.49it/s]Epoch: 2, train for the 106-th batch, train loss: 0.17716872692108154:  88%|████████▊ | 105/119 [01:04<00:08,  1.60it/s]Epoch: 2, train for the 106-th batch, train loss: 0.17716872692108154:  89%|████████▉ | 106/119 [01:04<00:08,  1.61it/s]evaluate for the 53-th batch, evaluate loss: 0.6205588579177856:  79%|██████████████▏   | 52/66 [00:14<00:03,  3.60it/s]evaluate for the 53-th batch, evaluate loss: 0.6205588579177856:  80%|██████████████▍   | 53/66 [00:14<00:03,  3.79it/s]Epoch: 3, train for the 110-th batch, train loss: 0.561950147151947:  72%|████████▋   | 109/151 [00:24<00:09,  4.49it/s]Epoch: 3, train for the 110-th batch, train loss: 0.561950147151947:  73%|████████▋   | 110/151 [00:24<00:09,  4.50it/s]Epoch: 3, train for the 111-th batch, train loss: 0.5306478142738342:  73%|████████   | 110/151 [00:24<00:09,  4.50it/s]Epoch: 3, train for the 111-th batch, train loss: 0.5306478142738342:  74%|████████   | 111/151 [00:24<00:08,  4.51it/s]Epoch: 1, train for the 254-th batch, train loss: 0.48172178864479065:  66%|██████▌   | 253/383 [02:31<01:16,  1.70it/s]Epoch: 1, train for the 254-th batch, train loss: 0.48172178864479065:  66%|██████▋   | 254/383 [02:31<01:16,  1.69it/s]evaluate for the 54-th batch, evaluate loss: 0.6539950370788574:  80%|██████████████▍   | 53/66 [00:14<00:03,  3.79it/s]evaluate for the 54-th batch, evaluate loss: 0.6539950370788574:  82%|██████████████▋   | 54/66 [00:14<00:03,  3.61it/s]Epoch: 2, train for the 81-th batch, train loss: 0.5412101745605469:  55%|███████      | 80/146 [00:49<00:40,  1.62it/s]Epoch: 2, train for the 81-th batch, train loss: 0.5412101745605469:  55%|███████▏     | 81/146 [00:49<00:40,  1.62it/s]Epoch: 3, train for the 112-th batch, train loss: 0.5164563655853271:  74%|████████   | 111/151 [00:24<00:08,  4.51it/s]Epoch: 3, train for the 112-th batch, train loss: 0.5164563655853271:  74%|████████▏  | 112/151 [00:24<00:08,  4.52it/s]Epoch: 2, train for the 107-th batch, train loss: 0.27247145771980286:  89%|████████▉ | 106/119 [01:05<00:08,  1.61it/s]Epoch: 2, train for the 107-th batch, train loss: 0.27247145771980286:  90%|████████▉ | 107/119 [01:05<00:07,  1.61it/s]evaluate for the 55-th batch, evaluate loss: 0.6057828068733215:  82%|██████████████▋   | 54/66 [00:15<00:03,  3.61it/s]evaluate for the 55-th batch, evaluate loss: 0.6057828068733215:  83%|███████████████   | 55/66 [00:15<00:02,  3.70it/s]Epoch: 3, train for the 113-th batch, train loss: 0.5715799331665039:  74%|████████▏  | 112/151 [00:24<00:08,  4.52it/s]Epoch: 3, train for the 113-th batch, train loss: 0.5715799331665039:  75%|████████▏  | 113/151 [00:24<00:08,  4.51it/s]Epoch: 1, train for the 255-th batch, train loss: 0.4204513728618622:  66%|███████▎   | 254/383 [02:32<01:16,  1.69it/s]Epoch: 1, train for the 255-th batch, train loss: 0.4204513728618622:  67%|███████▎   | 255/383 [02:32<01:15,  1.69it/s]evaluate for the 56-th batch, evaluate loss: 0.6322654485702515:  83%|███████████████   | 55/66 [00:15<00:02,  3.70it/s]evaluate for the 56-th batch, evaluate loss: 0.6322654485702515:  85%|███████████████▎  | 56/66 [00:15<00:02,  3.53it/s]Epoch: 3, train for the 114-th batch, train loss: 0.5221043825149536:  75%|████████▏  | 113/151 [00:24<00:08,  4.51it/s]Epoch: 3, train for the 114-th batch, train loss: 0.5221043825149536:  75%|████████▎  | 114/151 [00:24<00:08,  4.51it/s]Epoch: 2, train for the 82-th batch, train loss: 0.5164986252784729:  55%|███████▏     | 81/146 [00:50<00:40,  1.62it/s]Epoch: 2, train for the 82-th batch, train loss: 0.5164986252784729:  56%|███████▎     | 82/146 [00:50<00:39,  1.63it/s]Epoch: 2, train for the 108-th batch, train loss: 0.1835554838180542:  90%|█████████▉ | 107/119 [01:06<00:07,  1.61it/s]Epoch: 2, train for the 108-th batch, train loss: 0.1835554838180542:  91%|█████████▉ | 108/119 [01:06<00:06,  1.62it/s]Epoch: 3, train for the 115-th batch, train loss: 0.5152234435081482:  75%|████████▎  | 114/151 [00:25<00:08,  4.51it/s]Epoch: 3, train for the 115-th batch, train loss: 0.5152234435081482:  76%|████████▍  | 115/151 [00:25<00:08,  4.48it/s]evaluate for the 57-th batch, evaluate loss: 0.6281871795654297:  85%|███████████████▎  | 56/66 [00:15<00:02,  3.53it/s]evaluate for the 57-th batch, evaluate loss: 0.6281871795654297:  86%|███████████████▌  | 57/66 [00:15<00:02,  3.60it/s]Epoch: 3, train for the 116-th batch, train loss: 0.4842150807380676:  76%|████████▍  | 115/151 [00:25<00:08,  4.48it/s]Epoch: 3, train for the 116-th batch, train loss: 0.4842150807380676:  77%|████████▍  | 116/151 [00:25<00:07,  4.49it/s]evaluate for the 58-th batch, evaluate loss: 0.6572505831718445:  86%|███████████████▌  | 57/66 [00:15<00:02,  3.60it/s]evaluate for the 58-th batch, evaluate loss: 0.6572505831718445:  88%|███████████████▊  | 58/66 [00:15<00:02,  3.55it/s]Epoch: 1, train for the 256-th batch, train loss: 0.45158371329307556:  67%|██████▋   | 255/383 [02:32<01:15,  1.69it/s]Epoch: 1, train for the 256-th batch, train loss: 0.45158371329307556:  67%|██████▋   | 256/383 [02:32<01:14,  1.70it/s]Epoch: 3, train for the 117-th batch, train loss: 0.5240156650543213:  77%|████████▍  | 116/151 [00:25<00:07,  4.49it/s]Epoch: 3, train for the 117-th batch, train loss: 0.5240156650543213:  77%|████████▌  | 117/151 [00:25<00:07,  4.48it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5488351583480835:  56%|███████▎     | 82/146 [00:50<00:39,  1.63it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5488351583480835:  57%|███████▍     | 83/146 [00:50<00:38,  1.63it/s]evaluate for the 59-th batch, evaluate loss: 0.6365580558776855:  88%|███████████████▊  | 58/66 [00:16<00:02,  3.55it/s]evaluate for the 59-th batch, evaluate loss: 0.6365580558776855:  89%|████████████████  | 59/66 [00:16<00:01,  3.67it/s]Epoch: 2, train for the 109-th batch, train loss: 0.26644766330718994:  91%|█████████ | 108/119 [01:06<00:06,  1.62it/s]Epoch: 2, train for the 109-th batch, train loss: 0.26644766330718994:  92%|█████████▏| 109/119 [01:06<00:06,  1.62it/s]Epoch: 3, train for the 118-th batch, train loss: 0.47311529517173767:  77%|███████▋  | 117/151 [00:25<00:07,  4.48it/s]Epoch: 3, train for the 118-th batch, train loss: 0.47311529517173767:  78%|███████▊  | 118/151 [00:25<00:07,  4.50it/s]evaluate for the 60-th batch, evaluate loss: 0.647450864315033:  89%|████████████████▉  | 59/66 [00:16<00:01,  3.67it/s]evaluate for the 60-th batch, evaluate loss: 0.647450864315033:  91%|█████████████████▎ | 60/66 [00:16<00:01,  3.61it/s]Epoch: 3, train for the 119-th batch, train loss: 0.5297630429267883:  78%|████████▌  | 118/151 [00:26<00:07,  4.50it/s]Epoch: 3, train for the 119-th batch, train loss: 0.5297630429267883:  79%|████████▋  | 119/151 [00:26<00:07,  4.51it/s]Epoch: 1, train for the 257-th batch, train loss: 0.48719337582588196:  67%|██████▋   | 256/383 [02:33<01:14,  1.70it/s]Epoch: 1, train for the 257-th batch, train loss: 0.48719337582588196:  67%|██████▋   | 257/383 [02:33<01:13,  1.70it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5085275173187256:  57%|███████▍     | 83/146 [00:51<00:38,  1.63it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5085275173187256:  58%|███████▍     | 84/146 [00:51<00:38,  1.63it/s]Epoch: 3, train for the 120-th batch, train loss: 0.576426088809967:  79%|█████████▍  | 119/151 [00:26<00:07,  4.51it/s]Epoch: 3, train for the 120-th batch, train loss: 0.576426088809967:  79%|█████████▌  | 120/151 [00:26<00:06,  4.51it/s]evaluate for the 61-th batch, evaluate loss: 0.6449081301689148:  91%|████████████████▎ | 60/66 [00:16<00:01,  3.61it/s]evaluate for the 61-th batch, evaluate loss: 0.6449081301689148:  92%|████████████████▋ | 61/66 [00:16<00:01,  3.70it/s]Epoch: 2, train for the 110-th batch, train loss: 0.20400884747505188:  92%|█████████▏| 109/119 [01:07<00:06,  1.62it/s]Epoch: 2, train for the 110-th batch, train loss: 0.20400884747505188:  92%|█████████▏| 110/119 [01:07<00:05,  1.62it/s]Epoch: 3, train for the 121-th batch, train loss: 0.4974289536476135:  79%|████████▋  | 120/151 [00:26<00:06,  4.51it/s]Epoch: 3, train for the 121-th batch, train loss: 0.4974289536476135:  80%|████████▊  | 121/151 [00:26<00:06,  4.50it/s]evaluate for the 62-th batch, evaluate loss: 0.6707138419151306:  92%|████████████████▋ | 61/66 [00:16<00:01,  3.70it/s]evaluate for the 62-th batch, evaluate loss: 0.6707138419151306:  94%|████████████████▉ | 62/66 [00:16<00:01,  3.64it/s]Epoch: 1, train for the 258-th batch, train loss: 0.45128142833709717:  67%|██████▋   | 257/383 [02:33<01:13,  1.70it/s]Epoch: 1, train for the 258-th batch, train loss: 0.45128142833709717:  67%|██████▋   | 258/383 [02:33<01:13,  1.70it/s]Epoch: 3, train for the 122-th batch, train loss: 0.5534385442733765:  80%|████████▊  | 121/151 [00:26<00:06,  4.50it/s]Epoch: 3, train for the 122-th batch, train loss: 0.5534385442733765:  81%|████████▉  | 122/151 [00:26<00:06,  4.51it/s]evaluate for the 63-th batch, evaluate loss: 0.6476220488548279:  94%|████████████████▉ | 62/66 [00:17<00:01,  3.64it/s]evaluate for the 63-th batch, evaluate loss: 0.6476220488548279:  95%|█████████████████▏| 63/66 [00:17<00:00,  3.66it/s]Epoch: 2, train for the 85-th batch, train loss: 0.5089406371116638:  58%|███████▍     | 84/146 [00:52<00:38,  1.63it/s]Epoch: 2, train for the 85-th batch, train loss: 0.5089406371116638:  58%|███████▌     | 85/146 [00:52<00:37,  1.63it/s]Epoch: 3, train for the 123-th batch, train loss: 0.542106568813324:  81%|█████████▋  | 122/151 [00:26<00:06,  4.51it/s]Epoch: 3, train for the 123-th batch, train loss: 0.542106568813324:  81%|█████████▊  | 123/151 [00:26<00:06,  4.50it/s]Epoch: 2, train for the 111-th batch, train loss: 0.24367626011371613:  92%|█████████▏| 110/119 [01:08<00:05,  1.62it/s]Epoch: 2, train for the 111-th batch, train loss: 0.24367626011371613:  93%|█████████▎| 111/119 [01:08<00:04,  1.62it/s]evaluate for the 64-th batch, evaluate loss: 0.6342328190803528:  95%|█████████████████▏| 63/66 [00:17<00:00,  3.66it/s]evaluate for the 64-th batch, evaluate loss: 0.6342328190803528:  97%|█████████████████▍| 64/66 [00:17<00:00,  3.67it/s]Epoch: 3, train for the 124-th batch, train loss: 0.5196303725242615:  81%|████████▉  | 123/151 [00:27<00:06,  4.50it/s]Epoch: 3, train for the 124-th batch, train loss: 0.5196303725242615:  82%|█████████  | 124/151 [00:27<00:05,  4.51it/s]Epoch: 1, train for the 259-th batch, train loss: 0.43863150477409363:  67%|██████▋   | 258/383 [02:34<01:13,  1.70it/s]Epoch: 1, train for the 259-th batch, train loss: 0.43863150477409363:  68%|██████▊   | 259/383 [02:34<01:13,  1.69it/s]evaluate for the 65-th batch, evaluate loss: 0.6035159826278687:  97%|█████████████████▍| 64/66 [00:17<00:00,  3.67it/s]evaluate for the 65-th batch, evaluate loss: 0.6035159826278687:  98%|█████████████████▋| 65/66 [00:17<00:00,  3.59it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5321539044380188:  58%|███████▌     | 85/146 [00:52<00:37,  1.63it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5321539044380188:  59%|███████▋     | 86/146 [00:52<00:36,  1.63it/s]Epoch: 3, train for the 125-th batch, train loss: 0.5437985062599182:  82%|█████████  | 124/151 [00:27<00:05,  4.51it/s]Epoch: 3, train for the 125-th batch, train loss: 0.5437985062599182:  83%|█████████  | 125/151 [00:27<00:06,  4.03it/s]evaluate for the 66-th batch, evaluate loss: 0.6533422470092773:  98%|█████████████████▋| 65/66 [00:18<00:00,  3.59it/s]evaluate for the 66-th batch, evaluate loss: 0.6533422470092773: 100%|██████████████████| 66/66 [00:18<00:00,  4.06it/s]evaluate for the 66-th batch, evaluate loss: 0.6533422470092773: 100%|██████████████████| 66/66 [00:18<00:00,  3.67it/s]
  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 2, train for the 112-th batch, train loss: 0.15751780569553375:  93%|█████████▎| 111/119 [01:08<00:04,  1.62it/s]Epoch: 2, train for the 112-th batch, train loss: 0.15751780569553375:  94%|█████████▍| 112/119 [01:08<00:04,  1.63it/s]Epoch: 3, train for the 126-th batch, train loss: 0.5268632173538208:  83%|█████████  | 125/151 [00:27<00:06,  4.03it/s]Epoch: 3, train for the 126-th batch, train loss: 0.5268632173538208:  83%|█████████▏ | 126/151 [00:27<00:06,  4.16it/s]Epoch: 1, train for the 260-th batch, train loss: 0.45191049575805664:  68%|██████▊   | 259/383 [02:35<01:13,  1.69it/s]Epoch: 1, train for the 260-th batch, train loss: 0.45191049575805664:  68%|██████▊   | 260/383 [02:35<01:12,  1.69it/s]evaluate for the 1-th batch, evaluate loss: 0.6501606106758118:   0%|                            | 0/40 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6501606106758118:   2%|▌                   | 1/40 [00:00<00:12,  3.12it/s]Epoch: 3, train for the 127-th batch, train loss: 0.5335097312927246:  83%|█████████▏ | 126/151 [00:27<00:06,  4.16it/s]Epoch: 3, train for the 127-th batch, train loss: 0.5335097312927246:  84%|█████████▎ | 127/151 [00:27<00:05,  4.26it/s]Epoch: 2, train for the 87-th batch, train loss: 0.5374862551689148:  59%|███████▋     | 86/146 [00:53<00:36,  1.63it/s]Epoch: 2, train for the 87-th batch, train loss: 0.5374862551689148:  60%|███████▋     | 87/146 [00:53<00:36,  1.63it/s]Epoch: 3, train for the 128-th batch, train loss: 0.5904370546340942:  84%|█████████▎ | 127/151 [00:28<00:05,  4.26it/s]Epoch: 3, train for the 128-th batch, train loss: 0.5904370546340942:  85%|█████████▎ | 128/151 [00:28<00:05,  4.34it/s]evaluate for the 2-th batch, evaluate loss: 0.6905074119567871:   2%|▌                   | 1/40 [00:00<00:12,  3.12it/s]evaluate for the 2-th batch, evaluate loss: 0.6905074119567871:   5%|█                   | 2/40 [00:00<00:10,  3.48it/s]Epoch: 2, train for the 113-th batch, train loss: 0.21219146251678467:  94%|█████████▍| 112/119 [01:09<00:04,  1.63it/s]Epoch: 2, train for the 113-th batch, train loss: 0.21219146251678467:  95%|█████████▍| 113/119 [01:09<00:03,  1.63it/s]Epoch: 3, train for the 129-th batch, train loss: 0.5641237497329712:  85%|█████████▎ | 128/151 [00:28<00:05,  4.34it/s]Epoch: 3, train for the 129-th batch, train loss: 0.5641237497329712:  85%|█████████▍ | 129/151 [00:28<00:05,  4.38it/s]Epoch: 1, train for the 261-th batch, train loss: 0.45689550042152405:  68%|██████▊   | 260/383 [02:35<01:12,  1.69it/s]Epoch: 1, train for the 261-th batch, train loss: 0.45689550042152405:  68%|██████▊   | 261/383 [02:35<01:11,  1.69it/s]evaluate for the 3-th batch, evaluate loss: 0.6793650388717651:   5%|█                   | 2/40 [00:00<00:10,  3.48it/s]evaluate for the 3-th batch, evaluate loss: 0.6793650388717651:   8%|█▌                  | 3/40 [00:00<00:11,  3.30it/s]Epoch: 3, train for the 130-th batch, train loss: 0.5511273145675659:  85%|█████████▍ | 129/151 [00:28<00:05,  4.38it/s]Epoch: 3, train for the 130-th batch, train loss: 0.5511273145675659:  86%|█████████▍ | 130/151 [00:28<00:04,  4.39it/s]Epoch: 2, train for the 88-th batch, train loss: 0.5551570057868958:  60%|███████▋     | 87/146 [00:54<00:36,  1.63it/s]Epoch: 2, train for the 88-th batch, train loss: 0.5551570057868958:  60%|███████▊     | 88/146 [00:54<00:35,  1.63it/s]evaluate for the 4-th batch, evaluate loss: 0.6901878118515015:   8%|█▌                  | 3/40 [00:01<00:11,  3.30it/s]evaluate for the 4-th batch, evaluate loss: 0.6901878118515015:  10%|██                  | 4/40 [00:01<00:10,  3.47it/s]Epoch: 3, train for the 131-th batch, train loss: 0.5184441804885864:  86%|█████████▍ | 130/151 [00:28<00:04,  4.39it/s]Epoch: 3, train for the 131-th batch, train loss: 0.5184441804885864:  87%|█████████▌ | 131/151 [00:28<00:04,  4.43it/s]Epoch: 2, train for the 114-th batch, train loss: 0.20091229677200317:  95%|█████████▍| 113/119 [01:09<00:03,  1.63it/s]Epoch: 2, train for the 114-th batch, train loss: 0.20091229677200317:  96%|█████████▌| 114/119 [01:09<00:03,  1.63it/s]Epoch: 3, train for the 132-th batch, train loss: 0.5384201407432556:  87%|█████████▌ | 131/151 [00:28<00:04,  4.43it/s]Epoch: 3, train for the 132-th batch, train loss: 0.5384201407432556:  87%|█████████▌ | 132/151 [00:28<00:04,  4.45it/s]Epoch: 1, train for the 262-th batch, train loss: 0.4343230426311493:  68%|███████▍   | 261/383 [02:36<01:11,  1.69it/s]Epoch: 1, train for the 262-th batch, train loss: 0.4343230426311493:  68%|███████▌   | 262/383 [02:36<01:11,  1.70it/s]evaluate for the 5-th batch, evaluate loss: 0.660112738609314:  10%|██                   | 4/40 [00:01<00:10,  3.47it/s]evaluate for the 5-th batch, evaluate loss: 0.660112738609314:  12%|██▋                  | 5/40 [00:01<00:10,  3.33it/s]Epoch: 3, train for the 133-th batch, train loss: 0.5680675506591797:  87%|█████████▌ | 132/151 [00:29<00:04,  4.45it/s]Epoch: 3, train for the 133-th batch, train loss: 0.5680675506591797:  88%|█████████▋ | 133/151 [00:29<00:04,  4.46it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5214332938194275:  60%|███████▊     | 88/146 [00:54<00:35,  1.63it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5214332938194275:  61%|███████▉     | 89/146 [00:54<00:35,  1.62it/s]evaluate for the 6-th batch, evaluate loss: 0.668984591960907:  12%|██▋                  | 5/40 [00:01<00:10,  3.33it/s]evaluate for the 6-th batch, evaluate loss: 0.668984591960907:  15%|███▏                 | 6/40 [00:01<00:09,  3.48it/s]Epoch: 2, train for the 115-th batch, train loss: 0.21454757452011108:  96%|█████████▌| 114/119 [01:10<00:03,  1.63it/s]Epoch: 2, train for the 115-th batch, train loss: 0.21454757452011108:  97%|█████████▋| 115/119 [01:10<00:02,  1.62it/s]Epoch: 3, train for the 134-th batch, train loss: 0.5380297899246216:  88%|█████████▋ | 133/151 [00:29<00:04,  4.46it/s]Epoch: 3, train for the 134-th batch, train loss: 0.5380297899246216:  89%|█████████▊ | 134/151 [00:29<00:03,  4.49it/s]Epoch: 1, train for the 263-th batch, train loss: 0.45160382986068726:  68%|██████▊   | 262/383 [02:36<01:11,  1.70it/s]Epoch: 1, train for the 263-th batch, train loss: 0.45160382986068726:  69%|██████▊   | 263/383 [02:36<01:10,  1.70it/s]evaluate for the 7-th batch, evaluate loss: 0.6725471615791321:  15%|███                 | 6/40 [00:02<00:09,  3.48it/s]evaluate for the 7-th batch, evaluate loss: 0.6725471615791321:  18%|███▌                | 7/40 [00:02<00:09,  3.35it/s]Epoch: 3, train for the 135-th batch, train loss: 0.5482211112976074:  89%|█████████▊ | 134/151 [00:29<00:03,  4.49it/s]Epoch: 3, train for the 135-th batch, train loss: 0.5482211112976074:  89%|█████████▊ | 135/151 [00:29<00:03,  4.49it/s]evaluate for the 8-th batch, evaluate loss: 0.7221328616142273:  18%|███▌                | 7/40 [00:02<00:09,  3.35it/s]evaluate for the 8-th batch, evaluate loss: 0.7221328616142273:  20%|████                | 8/40 [00:02<00:09,  3.48it/s]Epoch: 3, train for the 136-th batch, train loss: 0.5679287314414978:  89%|█████████▊ | 135/151 [00:29<00:03,  4.49it/s]Epoch: 3, train for the 136-th batch, train loss: 0.5679287314414978:  90%|█████████▉ | 136/151 [00:29<00:03,  4.50it/s]Epoch: 2, train for the 90-th batch, train loss: 0.5394578576087952:  61%|███████▉     | 89/146 [00:55<00:35,  1.62it/s]Epoch: 2, train for the 90-th batch, train loss: 0.5394578576087952:  62%|████████     | 90/146 [00:55<00:34,  1.63it/s]Epoch: 2, train for the 116-th batch, train loss: 0.14939025044441223:  97%|█████████▋| 115/119 [01:11<00:02,  1.62it/s]Epoch: 2, train for the 116-th batch, train loss: 0.14939025044441223:  97%|█████████▋| 116/119 [01:11<00:01,  1.63it/s]Epoch: 3, train for the 137-th batch, train loss: 0.6095282435417175:  90%|█████████▉ | 136/151 [00:30<00:03,  4.50it/s]Epoch: 3, train for the 137-th batch, train loss: 0.6095282435417175:  91%|█████████▉ | 137/151 [00:30<00:03,  4.51it/s]Epoch: 1, train for the 264-th batch, train loss: 0.39694228768348694:  69%|██████▊   | 263/383 [02:37<01:10,  1.70it/s]Epoch: 1, train for the 264-th batch, train loss: 0.39694228768348694:  69%|██████▉   | 264/383 [02:37<01:09,  1.70it/s]evaluate for the 9-th batch, evaluate loss: 0.6979979276657104:  20%|████                | 8/40 [00:02<00:09,  3.48it/s]evaluate for the 9-th batch, evaluate loss: 0.6979979276657104:  22%|████▌               | 9/40 [00:02<00:09,  3.35it/s]Epoch: 3, train for the 138-th batch, train loss: 0.6411195397377014:  91%|█████████▉ | 137/151 [00:30<00:03,  4.51it/s]Epoch: 3, train for the 138-th batch, train loss: 0.6411195397377014:  91%|██████████ | 138/151 [00:30<00:02,  4.52it/s]evaluate for the 10-th batch, evaluate loss: 0.6757792234420776:  22%|████▎              | 9/40 [00:02<00:09,  3.35it/s]evaluate for the 10-th batch, evaluate loss: 0.6757792234420776:  25%|████▌             | 10/40 [00:02<00:08,  3.48it/s]Epoch: 2, train for the 91-th batch, train loss: 0.5468841195106506:  62%|████████     | 90/146 [00:55<00:34,  1.63it/s]Epoch: 2, train for the 91-th batch, train loss: 0.5468841195106506:  62%|████████     | 91/146 [00:55<00:33,  1.63it/s]Epoch: 3, train for the 139-th batch, train loss: 0.5801528692245483:  91%|██████████ | 138/151 [00:30<00:02,  4.52it/s]Epoch: 3, train for the 139-th batch, train loss: 0.5801528692245483:  92%|██████████▏| 139/151 [00:30<00:02,  4.52it/s]Epoch: 2, train for the 117-th batch, train loss: 0.2420836240053177:  97%|██████████▋| 116/119 [01:11<00:01,  1.63it/s]Epoch: 2, train for the 117-th batch, train loss: 0.2420836240053177:  98%|██████████▊| 117/119 [01:11<00:01,  1.63it/s]Epoch: 3, train for the 140-th batch, train loss: 0.5414353609085083:  92%|██████████▏| 139/151 [00:30<00:02,  4.52it/s]Epoch: 3, train for the 140-th batch, train loss: 0.5414353609085083:  93%|██████████▏| 140/151 [00:30<00:02,  4.52it/s]Epoch: 1, train for the 265-th batch, train loss: 0.4177212715148926:  69%|███████▌   | 264/383 [02:38<01:09,  1.70it/s]Epoch: 1, train for the 265-th batch, train loss: 0.4177212715148926:  69%|███████▌   | 265/383 [02:38<01:09,  1.71it/s]evaluate for the 11-th batch, evaluate loss: 0.6870884299278259:  25%|████▌             | 10/40 [00:03<00:08,  3.48it/s]evaluate for the 11-th batch, evaluate loss: 0.6870884299278259:  28%|████▉             | 11/40 [00:03<00:08,  3.36it/s]Epoch: 3, train for the 141-th batch, train loss: 0.5610084533691406:  93%|██████████▏| 140/151 [00:30<00:02,  4.52it/s]Epoch: 3, train for the 141-th batch, train loss: 0.5610084533691406:  93%|██████████▎| 141/151 [00:30<00:02,  4.52it/s]evaluate for the 12-th batch, evaluate loss: 0.7546588182449341:  28%|████▉             | 11/40 [00:03<00:08,  3.36it/s]evaluate for the 12-th batch, evaluate loss: 0.7546588182449341:  30%|█████▍            | 12/40 [00:03<00:08,  3.48it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5377980470657349:  62%|████████     | 91/146 [00:56<00:33,  1.63it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5377980470657349:  63%|████████▏    | 92/146 [00:56<00:33,  1.63it/s]Epoch: 3, train for the 142-th batch, train loss: 0.5592098832130432:  93%|██████████▎| 141/151 [00:31<00:02,  4.52it/s]Epoch: 3, train for the 142-th batch, train loss: 0.5592098832130432:  94%|██████████▎| 142/151 [00:31<00:01,  4.52it/s]Epoch: 2, train for the 118-th batch, train loss: 0.1617928445339203:  98%|██████████▊| 117/119 [01:12<00:01,  1.63it/s]Epoch: 2, train for the 118-th batch, train loss: 0.1617928445339203:  99%|██████████▉| 118/119 [01:12<00:00,  1.63it/s]Epoch: 1, train for the 266-th batch, train loss: 0.4874163568019867:  69%|███████▌   | 265/383 [02:38<01:09,  1.71it/s]Epoch: 1, train for the 266-th batch, train loss: 0.4874163568019867:  69%|███████▋   | 266/383 [02:38<01:08,  1.71it/s]evaluate for the 13-th batch, evaluate loss: 0.6861581206321716:  30%|█████▍            | 12/40 [00:03<00:08,  3.48it/s]evaluate for the 13-th batch, evaluate loss: 0.6861581206321716:  32%|█████▊            | 13/40 [00:03<00:08,  3.36it/s]Epoch: 3, train for the 143-th batch, train loss: 0.48608842492103577:  94%|█████████▍| 142/151 [00:31<00:01,  4.52it/s]Epoch: 3, train for the 143-th batch, train loss: 0.48608842492103577:  95%|█████████▍| 143/151 [00:31<00:01,  4.51it/s]Epoch: 2, train for the 119-th batch, train loss: 0.34885793924331665:  99%|█████████▉| 118/119 [01:12<00:00,  1.63it/s]Epoch: 2, train for the 119-th batch, train loss: 0.34885793924331665: 100%|██████████| 119/119 [01:12<00:00,  1.87it/s]Epoch: 2, train for the 119-th batch, train loss: 0.34885793924331665: 100%|██████████| 119/119 [01:12<00:00,  1.64it/s]
  0%|                                                                                            | 0/40 [00:00<?, ?it/s]evaluate for the 14-th batch, evaluate loss: 0.7384240627288818:  32%|█████▊            | 13/40 [00:04<00:08,  3.36it/s]evaluate for the 14-th batch, evaluate loss: 0.7384240627288818:  35%|██████▎           | 14/40 [00:04<00:07,  3.49it/s]Epoch: 3, train for the 144-th batch, train loss: 0.5011921525001526:  95%|██████████▍| 143/151 [00:31<00:01,  4.51it/s]Epoch: 3, train for the 144-th batch, train loss: 0.5011921525001526:  95%|██████████▍| 144/151 [00:31<00:01,  4.50it/s]Epoch: 2, train for the 93-th batch, train loss: 0.5629650950431824:  63%|████████▏    | 92/146 [00:57<00:33,  1.63it/s]Epoch: 2, train for the 93-th batch, train loss: 0.5629650950431824:  64%|████████▎    | 93/146 [00:57<00:32,  1.63it/s]Epoch: 3, train for the 145-th batch, train loss: 0.5304157733917236:  95%|██████████▍| 144/151 [00:31<00:01,  4.50it/s]Epoch: 3, train for the 145-th batch, train loss: 0.5304157733917236:  96%|██████████▌| 145/151 [00:31<00:01,  4.49it/s]evaluate for the 1-th batch, evaluate loss: 0.21822340786457062:   0%|                           | 0/40 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.21822340786457062:   2%|▍                  | 1/40 [00:00<00:10,  3.75it/s]Epoch: 1, train for the 267-th batch, train loss: 0.44333121180534363:  69%|██████▉   | 266/383 [02:39<01:08,  1.71it/s]Epoch: 1, train for the 267-th batch, train loss: 0.44333121180534363:  70%|██████▉   | 267/383 [02:39<01:07,  1.71it/s]evaluate for the 15-th batch, evaluate loss: 0.7424823045730591:  35%|██████▎           | 14/40 [00:04<00:07,  3.49it/s]evaluate for the 15-th batch, evaluate loss: 0.7424823045730591:  38%|██████▊           | 15/40 [00:04<00:07,  3.36it/s]Epoch: 3, train for the 146-th batch, train loss: 0.5381642580032349:  96%|██████████▌| 145/151 [00:32<00:01,  4.49it/s]Epoch: 3, train for the 146-th batch, train loss: 0.5381642580032349:  97%|██████████▋| 146/151 [00:32<00:01,  4.49it/s]evaluate for the 2-th batch, evaluate loss: 0.21414382755756378:   2%|▍                  | 1/40 [00:00<00:10,  3.75it/s]evaluate for the 2-th batch, evaluate loss: 0.21414382755756378:   5%|▉                  | 2/40 [00:00<00:10,  3.59it/s]evaluate for the 16-th batch, evaluate loss: 0.7021421790122986:  38%|██████▊           | 15/40 [00:04<00:07,  3.36it/s]evaluate for the 16-th batch, evaluate loss: 0.7021421790122986:  40%|███████▏          | 16/40 [00:04<00:06,  3.48it/s]Epoch: 3, train for the 147-th batch, train loss: 0.5500537157058716:  97%|██████████▋| 146/151 [00:32<00:01,  4.49it/s]Epoch: 3, train for the 147-th batch, train loss: 0.5500537157058716:  97%|██████████▋| 147/151 [00:32<00:00,  4.51it/s]Epoch: 2, train for the 94-th batch, train loss: 0.5564274191856384:  64%|████████▎    | 93/146 [00:57<00:32,  1.63it/s]Epoch: 2, train for the 94-th batch, train loss: 0.5564274191856384:  64%|████████▎    | 94/146 [00:57<00:31,  1.64it/s]evaluate for the 3-th batch, evaluate loss: 0.27028828859329224:   5%|▉                  | 2/40 [00:00<00:10,  3.59it/s]evaluate for the 3-th batch, evaluate loss: 0.27028828859329224:   8%|█▍                 | 3/40 [00:00<00:10,  3.50it/s]Epoch: 1, train for the 268-th batch, train loss: 0.462736576795578:  70%|████████▎   | 267/383 [02:39<01:07,  1.71it/s]Epoch: 1, train for the 268-th batch, train loss: 0.462736576795578:  70%|████████▍   | 268/383 [02:39<01:07,  1.71it/s]evaluate for the 17-th batch, evaluate loss: 0.6887591481208801:  40%|███████▏          | 16/40 [00:04<00:06,  3.48it/s]evaluate for the 17-th batch, evaluate loss: 0.6887591481208801:  42%|███████▋          | 17/40 [00:04<00:06,  3.37it/s]Epoch: 3, train for the 148-th batch, train loss: 0.5783067941665649:  97%|██████████▋| 147/151 [00:32<00:00,  4.51it/s]Epoch: 3, train for the 148-th batch, train loss: 0.5783067941665649:  98%|██████████▊| 148/151 [00:32<00:00,  4.51it/s]evaluate for the 4-th batch, evaluate loss: 0.18911151587963104:   8%|█▍                 | 3/40 [00:01<00:10,  3.50it/s]evaluate for the 4-th batch, evaluate loss: 0.18911151587963104:  10%|█▉                 | 4/40 [00:01<00:10,  3.55it/s]Epoch: 3, train for the 149-th batch, train loss: 0.5024486184120178:  98%|██████████▊| 148/151 [00:32<00:00,  4.51it/s]Epoch: 3, train for the 149-th batch, train loss: 0.5024486184120178:  99%|██████████▊| 149/151 [00:32<00:00,  4.51it/s]evaluate for the 18-th batch, evaluate loss: 0.7061928510665894:  42%|███████▋          | 17/40 [00:05<00:06,  3.37it/s]evaluate for the 18-th batch, evaluate loss: 0.7061928510665894:  45%|████████          | 18/40 [00:05<00:06,  3.49it/s]Epoch: 2, train for the 95-th batch, train loss: 0.5433570146560669:  64%|████████▎    | 94/146 [00:58<00:31,  1.64it/s]Epoch: 2, train for the 95-th batch, train loss: 0.5433570146560669:  65%|████████▍    | 95/146 [00:58<00:30,  1.65it/s]Epoch: 3, train for the 150-th batch, train loss: 0.5338996052742004:  99%|██████████▊| 149/151 [00:32<00:00,  4.51it/s]Epoch: 3, train for the 150-th batch, train loss: 0.5338996052742004:  99%|██████████▉| 150/151 [00:32<00:00,  4.51it/s]evaluate for the 5-th batch, evaluate loss: 0.22806712985038757:  10%|█▉                 | 4/40 [00:01<00:10,  3.55it/s]evaluate for the 5-th batch, evaluate loss: 0.22806712985038757:  12%|██▍                | 5/40 [00:01<00:10,  3.47it/s]Epoch: 1, train for the 269-th batch, train loss: 0.5135437846183777:  70%|███████▋   | 268/383 [02:40<01:07,  1.71it/s]Epoch: 1, train for the 269-th batch, train loss: 0.5135437846183777:  70%|███████▋   | 269/383 [02:40<01:06,  1.71it/s]evaluate for the 19-th batch, evaluate loss: 0.7141207456588745:  45%|████████          | 18/40 [00:05<00:06,  3.49it/s]evaluate for the 19-th batch, evaluate loss: 0.7141207456588745:  48%|████████▌         | 19/40 [00:05<00:06,  3.36it/s]Epoch: 3, train for the 151-th batch, train loss: 0.593741774559021:  99%|███████████▉| 150/151 [00:33<00:00,  4.51it/s]Epoch: 3, train for the 151-th batch, train loss: 0.593741774559021: 100%|████████████| 151/151 [00:33<00:00,  4.99it/s]Epoch: 3, train for the 151-th batch, train loss: 0.593741774559021: 100%|████████████| 151/151 [00:33<00:00,  4.56it/s]
  0%|                                                                                            | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.4999570846557617:   0%|                            | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.4999570846557617:   2%|▍                   | 1/46 [00:00<00:04,  9.68it/s]evaluate for the 6-th batch, evaluate loss: 0.22824963927268982:  12%|██▍                | 5/40 [00:01<00:10,  3.47it/s]evaluate for the 6-th batch, evaluate loss: 0.22824963927268982:  15%|██▊                | 6/40 [00:01<00:09,  3.64it/s]evaluate for the 2-th batch, evaluate loss: 0.5037221312522888:   2%|▍                   | 1/46 [00:00<00:04,  9.68it/s]evaluate for the 2-th batch, evaluate loss: 0.5037221312522888:   4%|▊                   | 2/46 [00:00<00:04,  9.66it/s]evaluate for the 20-th batch, evaluate loss: 0.7487505674362183:  48%|████████▌         | 19/40 [00:05<00:06,  3.36it/s]evaluate for the 20-th batch, evaluate loss: 0.7487505674362183:  50%|█████████         | 20/40 [00:05<00:05,  3.48it/s]evaluate for the 3-th batch, evaluate loss: 0.47740066051483154:   4%|▊                  | 2/46 [00:00<00:04,  9.66it/s]evaluate for the 3-th batch, evaluate loss: 0.47740066051483154:   7%|█▏                 | 3/46 [00:00<00:04,  9.67it/s]Epoch: 2, train for the 96-th batch, train loss: 0.5744115114212036:  65%|████████▍    | 95/146 [00:58<00:30,  1.65it/s]Epoch: 2, train for the 96-th batch, train loss: 0.5744115114212036:  66%|████████▌    | 96/146 [00:58<00:30,  1.66it/s]evaluate for the 4-th batch, evaluate loss: 0.5142965316772461:   7%|█▎                  | 3/46 [00:00<00:04,  9.67it/s]evaluate for the 4-th batch, evaluate loss: 0.5142965316772461:   9%|█▋                  | 4/46 [00:00<00:04,  9.65it/s]evaluate for the 7-th batch, evaluate loss: 0.13723812997341156:  15%|██▊                | 6/40 [00:01<00:09,  3.64it/s]evaluate for the 7-th batch, evaluate loss: 0.13723812997341156:  18%|███▎               | 7/40 [00:01<00:09,  3.52it/s]evaluate for the 5-th batch, evaluate loss: 0.4774286150932312:   9%|█▋                  | 4/46 [00:00<00:04,  9.65it/s]evaluate for the 5-th batch, evaluate loss: 0.4774286150932312:  11%|██▏                 | 5/46 [00:00<00:04,  9.66it/s]Epoch: 1, train for the 270-th batch, train loss: 0.43161046504974365:  70%|███████   | 269/383 [02:40<01:06,  1.71it/s]Epoch: 1, train for the 270-th batch, train loss: 0.43161046504974365:  70%|███████   | 270/383 [02:40<01:06,  1.71it/s]evaluate for the 21-th batch, evaluate loss: 0.7404423356056213:  50%|█████████         | 20/40 [00:06<00:05,  3.48it/s]evaluate for the 21-th batch, evaluate loss: 0.7404423356056213:  52%|█████████▍        | 21/40 [00:06<00:05,  3.36it/s]evaluate for the 6-th batch, evaluate loss: 0.5556043386459351:  11%|██▏                 | 5/46 [00:00<00:04,  9.66it/s]evaluate for the 6-th batch, evaluate loss: 0.5556043386459351:  13%|██▌                 | 6/46 [00:00<00:04,  9.68it/s]evaluate for the 8-th batch, evaluate loss: 0.19411292672157288:  18%|███▎               | 7/40 [00:02<00:09,  3.52it/s]evaluate for the 8-th batch, evaluate loss: 0.19411292672157288:  20%|███▊               | 8/40 [00:02<00:08,  3.70it/s]evaluate for the 7-th batch, evaluate loss: 0.5077745318412781:  13%|██▌                 | 6/46 [00:00<00:04,  9.68it/s]evaluate for the 7-th batch, evaluate loss: 0.5077745318412781:  15%|███                 | 7/46 [00:00<00:04,  9.70it/s]evaluate for the 22-th batch, evaluate loss: 0.7569625973701477:  52%|█████████▍        | 21/40 [00:06<00:05,  3.36it/s]evaluate for the 22-th batch, evaluate loss: 0.7569625973701477:  55%|█████████▉        | 22/40 [00:06<00:04,  3.64it/s]evaluate for the 8-th batch, evaluate loss: 0.5687304735183716:  15%|███                 | 7/46 [00:00<00:04,  9.70it/s]evaluate for the 8-th batch, evaluate loss: 0.5687304735183716:  17%|███▍                | 8/46 [00:00<00:03,  9.70it/s]evaluate for the 9-th batch, evaluate loss: 0.528755247592926:  17%|███▋                 | 8/46 [00:00<00:03,  9.70it/s]evaluate for the 9-th batch, evaluate loss: 0.528755247592926:  20%|████                 | 9/46 [00:00<00:03,  9.69it/s]evaluate for the 23-th batch, evaluate loss: 0.6772903203964233:  55%|█████████▉        | 22/40 [00:06<00:04,  3.64it/s]evaluate for the 23-th batch, evaluate loss: 0.6772903203964233:  57%|██████████▎       | 23/40 [00:06<00:04,  4.11it/s]Epoch: 2, train for the 97-th batch, train loss: 0.5133180618286133:  66%|████████▌    | 96/146 [00:59<00:30,  1.66it/s]Epoch: 2, train for the 97-th batch, train loss: 0.5133180618286133:  66%|████████▋    | 97/146 [00:59<00:29,  1.65it/s]evaluate for the 9-th batch, evaluate loss: 0.18947143852710724:  20%|███▊               | 8/40 [00:02<00:08,  3.70it/s]evaluate for the 9-th batch, evaluate loss: 0.18947143852710724:  22%|████▎              | 9/40 [00:02<00:08,  3.51it/s]evaluate for the 10-th batch, evaluate loss: 0.5367319583892822:  20%|███▋               | 9/46 [00:01<00:03,  9.69it/s]evaluate for the 10-th batch, evaluate loss: 0.5367319583892822:  22%|███▉              | 10/46 [00:01<00:03,  9.67it/s]evaluate for the 11-th batch, evaluate loss: 0.5268539190292358:  22%|███▉              | 10/46 [00:01<00:03,  9.67it/s]evaluate for the 11-th batch, evaluate loss: 0.5268539190292358:  24%|████▎             | 11/46 [00:01<00:03,  9.67it/s]Epoch: 1, train for the 271-th batch, train loss: 0.41099610924720764:  70%|███████   | 270/383 [02:41<01:06,  1.71it/s]Epoch: 1, train for the 271-th batch, train loss: 0.41099610924720764:  71%|███████   | 271/383 [02:41<01:08,  1.64it/s]evaluate for the 12-th batch, evaluate loss: 0.47913292050361633:  24%|████             | 11/46 [00:01<00:03,  9.67it/s]evaluate for the 12-th batch, evaluate loss: 0.47913292050361633:  26%|████▍            | 12/46 [00:01<00:03,  9.68it/s]evaluate for the 24-th batch, evaluate loss: 0.6976215839385986:  57%|██████████▎       | 23/40 [00:06<00:04,  4.11it/s]evaluate for the 24-th batch, evaluate loss: 0.6976215839385986:  60%|██████████▊       | 24/40 [00:06<00:04,  3.81it/s]evaluate for the 10-th batch, evaluate loss: 0.2624216675758362:  22%|████▎              | 9/40 [00:02<00:08,  3.51it/s]evaluate for the 10-th batch, evaluate loss: 0.2624216675758362:  25%|████▌             | 10/40 [00:02<00:08,  3.58it/s]evaluate for the 13-th batch, evaluate loss: 0.5013717412948608:  26%|████▋             | 12/46 [00:01<00:03,  9.68it/s]evaluate for the 13-th batch, evaluate loss: 0.5013717412948608:  28%|█████             | 13/46 [00:01<00:03,  9.67it/s]evaluate for the 14-th batch, evaluate loss: 0.5871855020523071:  28%|█████             | 13/46 [00:01<00:03,  9.67it/s]evaluate for the 14-th batch, evaluate loss: 0.5871855020523071:  30%|█████▍            | 14/46 [00:01<00:03,  9.66it/s]evaluate for the 25-th batch, evaluate loss: 0.6899584531784058:  60%|██████████▊       | 24/40 [00:07<00:04,  3.81it/s]evaluate for the 25-th batch, evaluate loss: 0.6899584531784058:  62%|███████████▎      | 25/40 [00:07<00:03,  3.88it/s]evaluate for the 15-th batch, evaluate loss: 0.5462518334388733:  30%|█████▍            | 14/46 [00:01<00:03,  9.66it/s]evaluate for the 15-th batch, evaluate loss: 0.5462518334388733:  33%|█████▊            | 15/46 [00:01<00:03,  9.66it/s]Epoch: 2, train for the 98-th batch, train loss: 0.5382723212242126:  66%|████████▋    | 97/146 [01:00<00:29,  1.65it/s]Epoch: 2, train for the 98-th batch, train loss: 0.5382723212242126:  67%|████████▋    | 98/146 [01:00<00:29,  1.65it/s]evaluate for the 11-th batch, evaluate loss: 0.20446445047855377:  25%|████▎            | 10/40 [00:03<00:08,  3.58it/s]evaluate for the 11-th batch, evaluate loss: 0.20446445047855377:  28%|████▋            | 11/40 [00:03<00:08,  3.43it/s]evaluate for the 16-th batch, evaluate loss: 0.5748245716094971:  33%|█████▊            | 15/46 [00:01<00:03,  9.66it/s]evaluate for the 16-th batch, evaluate loss: 0.5748245716094971:  35%|██████▎           | 16/46 [00:01<00:03,  9.68it/s]evaluate for the 17-th batch, evaluate loss: 0.4621056318283081:  35%|██████▎           | 16/46 [00:01<00:03,  9.68it/s]evaluate for the 17-th batch, evaluate loss: 0.4621056318283081:  37%|██████▋           | 17/46 [00:01<00:02,  9.68it/s]Epoch: 1, train for the 272-th batch, train loss: 0.32771730422973633:  71%|███████   | 271/383 [02:42<01:08,  1.64it/s]Epoch: 1, train for the 272-th batch, train loss: 0.32771730422973633:  71%|███████   | 272/383 [02:42<01:06,  1.66it/s]evaluate for the 26-th batch, evaluate loss: 0.6971210837364197:  62%|███████████▎      | 25/40 [00:07<00:03,  3.88it/s]evaluate for the 26-th batch, evaluate loss: 0.6971210837364197:  65%|███████████▋      | 26/40 [00:07<00:03,  3.63it/s]evaluate for the 18-th batch, evaluate loss: 0.50522381067276:  37%|███████▍            | 17/46 [00:01<00:02,  9.68it/s]evaluate for the 18-th batch, evaluate loss: 0.50522381067276:  39%|███████▊            | 18/46 [00:01<00:02,  9.69it/s]evaluate for the 12-th batch, evaluate loss: 0.18791590631008148:  28%|████▋            | 11/40 [00:03<00:08,  3.43it/s]evaluate for the 12-th batch, evaluate loss: 0.18791590631008148:  30%|█████            | 12/40 [00:03<00:07,  3.50it/s]evaluate for the 19-th batch, evaluate loss: 0.5226737260818481:  39%|███████           | 18/46 [00:01<00:02,  9.69it/s]evaluate for the 19-th batch, evaluate loss: 0.5226737260818481:  41%|███████▍          | 19/46 [00:01<00:02,  9.69it/s]evaluate for the 20-th batch, evaluate loss: 0.5410938262939453:  41%|███████▍          | 19/46 [00:02<00:02,  9.69it/s]evaluate for the 20-th batch, evaluate loss: 0.5410938262939453:  43%|███████▊          | 20/46 [00:02<00:02,  9.69it/s]evaluate for the 27-th batch, evaluate loss: 0.7145980000495911:  65%|███████████▋      | 26/40 [00:07<00:03,  3.63it/s]evaluate for the 27-th batch, evaluate loss: 0.7145980000495911:  68%|████████████▏     | 27/40 [00:07<00:03,  3.69it/s]evaluate for the 21-th batch, evaluate loss: 0.525192379951477:  43%|████████▎          | 20/46 [00:02<00:02,  9.69it/s]evaluate for the 21-th batch, evaluate loss: 0.525192379951477:  46%|████████▋          | 21/46 [00:02<00:02,  9.68it/s]evaluate for the 13-th batch, evaluate loss: 0.1465252786874771:  30%|█████▍            | 12/40 [00:03<00:07,  3.50it/s]evaluate for the 13-th batch, evaluate loss: 0.1465252786874771:  32%|█████▊            | 13/40 [00:03<00:07,  3.46it/s]Epoch: 2, train for the 99-th batch, train loss: 0.5299726128578186:  67%|████████▋    | 98/146 [01:00<00:29,  1.65it/s]Epoch: 2, train for the 99-th batch, train loss: 0.5299726128578186:  68%|████████▊    | 99/146 [01:00<00:28,  1.66it/s]evaluate for the 22-th batch, evaluate loss: 0.5196298360824585:  46%|████████▏         | 21/46 [00:02<00:02,  9.68it/s]evaluate for the 22-th batch, evaluate loss: 0.5196298360824585:  48%|████████▌         | 22/46 [00:02<00:02,  9.67it/s]evaluate for the 23-th batch, evaluate loss: 0.4774586260318756:  48%|████████▌         | 22/46 [00:02<00:02,  9.67it/s]evaluate for the 23-th batch, evaluate loss: 0.4774586260318756:  50%|█████████         | 23/46 [00:02<00:02,  9.69it/s]Epoch: 1, train for the 273-th batch, train loss: 0.40516966581344604:  71%|███████   | 272/383 [02:42<01:06,  1.66it/s]Epoch: 1, train for the 273-th batch, train loss: 0.40516966581344604:  71%|███████▏  | 273/383 [02:42<01:05,  1.67it/s]evaluate for the 28-th batch, evaluate loss: 0.7155759930610657:  68%|████████████▏     | 27/40 [00:08<00:03,  3.69it/s]evaluate for the 28-th batch, evaluate loss: 0.7155759930610657:  70%|████████████▌     | 28/40 [00:08<00:03,  3.50it/s]evaluate for the 14-th batch, evaluate loss: 0.23277166485786438:  32%|█████▌           | 13/40 [00:03<00:07,  3.46it/s]evaluate for the 14-th batch, evaluate loss: 0.23277166485786438:  35%|█████▉           | 14/40 [00:03<00:07,  3.59it/s]evaluate for the 24-th batch, evaluate loss: 0.48743414878845215:  50%|████████▌        | 23/46 [00:02<00:02,  9.69it/s]evaluate for the 24-th batch, evaluate loss: 0.48743414878845215:  52%|████████▊        | 24/46 [00:02<00:02,  9.69it/s]evaluate for the 25-th batch, evaluate loss: 0.5384401679039001:  52%|█████████▍        | 24/46 [00:02<00:02,  9.69it/s]evaluate for the 25-th batch, evaluate loss: 0.5384401679039001:  54%|█████████▊        | 25/46 [00:02<00:02,  9.69it/s]evaluate for the 29-th batch, evaluate loss: 0.7441105246543884:  70%|████████████▌     | 28/40 [00:08<00:03,  3.50it/s]evaluate for the 29-th batch, evaluate loss: 0.7441105246543884:  72%|█████████████     | 29/40 [00:08<00:03,  3.58it/s]evaluate for the 26-th batch, evaluate loss: 0.5578125715255737:  54%|█████████▊        | 25/46 [00:02<00:02,  9.69it/s]evaluate for the 26-th batch, evaluate loss: 0.5578125715255737:  57%|██████████▏       | 26/46 [00:02<00:02,  9.67it/s]evaluate for the 15-th batch, evaluate loss: 0.24170973896980286:  35%|█████▉           | 14/40 [00:04<00:07,  3.59it/s]evaluate for the 15-th batch, evaluate loss: 0.24170973896980286:  38%|██████▍          | 15/40 [00:04<00:07,  3.54it/s]evaluate for the 27-th batch, evaluate loss: 0.49961456656455994:  57%|█████████▌       | 26/46 [00:02<00:02,  9.67it/s]evaluate for the 27-th batch, evaluate loss: 0.49961456656455994:  59%|█████████▉       | 27/46 [00:02<00:01,  9.66it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5740655660629272:  68%|████████▏   | 99/146 [01:01<00:28,  1.66it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5740655660629272:  68%|███████▌   | 100/146 [01:01<00:27,  1.66it/s]evaluate for the 28-th batch, evaluate loss: 0.5248517394065857:  59%|██████████▌       | 27/46 [00:02<00:01,  9.66it/s]evaluate for the 28-th batch, evaluate loss: 0.5248517394065857:  61%|██████████▉       | 28/46 [00:02<00:01,  9.66it/s]evaluate for the 16-th batch, evaluate loss: 0.26212531328201294:  38%|██████▍          | 15/40 [00:04<00:07,  3.54it/s]evaluate for the 16-th batch, evaluate loss: 0.26212531328201294:  40%|██████▊          | 16/40 [00:04<00:06,  3.63it/s]Epoch: 1, train for the 274-th batch, train loss: 0.420064240694046:  71%|████████▌   | 273/383 [02:43<01:05,  1.67it/s]Epoch: 1, train for the 274-th batch, train loss: 0.420064240694046:  72%|████████▌   | 274/383 [02:43<01:04,  1.68it/s]evaluate for the 30-th batch, evaluate loss: 0.7385278344154358:  72%|█████████████     | 29/40 [00:08<00:03,  3.58it/s]evaluate for the 30-th batch, evaluate loss: 0.7385278344154358:  75%|█████████████▌    | 30/40 [00:08<00:02,  3.43it/s]evaluate for the 29-th batch, evaluate loss: 0.49023541808128357:  61%|██████████▎      | 28/46 [00:02<00:01,  9.66it/s]evaluate for the 29-th batch, evaluate loss: 0.49023541808128357:  63%|██████████▋      | 29/46 [00:02<00:01,  9.65it/s]evaluate for the 30-th batch, evaluate loss: 0.4995056092739105:  63%|███████████▎      | 29/46 [00:03<00:01,  9.65it/s]evaluate for the 30-th batch, evaluate loss: 0.4995056092739105:  65%|███████████▋      | 30/46 [00:03<00:01,  9.64it/s]evaluate for the 31-th batch, evaluate loss: 0.5192168354988098:  65%|███████████▋      | 30/46 [00:03<00:01,  9.64it/s]evaluate for the 31-th batch, evaluate loss: 0.5192168354988098:  67%|████████████▏     | 31/46 [00:03<00:01,  9.63it/s]evaluate for the 31-th batch, evaluate loss: 0.697073221206665:  75%|██████████████▎    | 30/40 [00:08<00:02,  3.43it/s]evaluate for the 31-th batch, evaluate loss: 0.697073221206665:  78%|██████████████▋    | 31/40 [00:08<00:02,  3.52it/s]evaluate for the 17-th batch, evaluate loss: 0.1666249781847:  40%|████████▍            | 16/40 [00:04<00:06,  3.63it/s]evaluate for the 17-th batch, evaluate loss: 0.1666249781847:  42%|████████▉            | 17/40 [00:04<00:06,  3.60it/s]evaluate for the 32-th batch, evaluate loss: 0.4786738455295563:  67%|████████████▏     | 31/46 [00:03<00:01,  9.63it/s]evaluate for the 32-th batch, evaluate loss: 0.4786738455295563:  70%|████████████▌     | 32/46 [00:03<00:01,  9.67it/s]evaluate for the 33-th batch, evaluate loss: 0.49571487307548523:  70%|███████████▊     | 32/46 [00:03<00:01,  9.67it/s]evaluate for the 33-th batch, evaluate loss: 0.49571487307548523:  72%|████████████▏    | 33/46 [00:03<00:01,  9.65it/s]Epoch: 2, train for the 101-th batch, train loss: 0.5072907209396362:  68%|███████▌   | 100/146 [01:01<00:27,  1.66it/s]Epoch: 2, train for the 101-th batch, train loss: 0.5072907209396362:  69%|███████▌   | 101/146 [01:01<00:27,  1.65it/s]evaluate for the 34-th batch, evaluate loss: 0.4903780221939087:  72%|████████████▉     | 33/46 [00:03<00:01,  9.65it/s]evaluate for the 34-th batch, evaluate loss: 0.4903780221939087:  74%|█████████████▎    | 34/46 [00:03<00:01,  9.66it/s]evaluate for the 18-th batch, evaluate loss: 0.1902974247932434:  42%|███████▋          | 17/40 [00:05<00:06,  3.60it/s]evaluate for the 18-th batch, evaluate loss: 0.1902974247932434:  45%|████████          | 18/40 [00:05<00:06,  3.55it/s]Epoch: 1, train for the 275-th batch, train loss: 0.4952557384967804:  72%|███████▊   | 274/383 [02:44<01:04,  1.68it/s]Epoch: 1, train for the 275-th batch, train loss: 0.4952557384967804:  72%|███████▉   | 275/383 [02:44<01:03,  1.69it/s]evaluate for the 32-th batch, evaluate loss: 0.7330322265625:  78%|████████████████▎    | 31/40 [00:09<00:02,  3.52it/s]evaluate for the 32-th batch, evaluate loss: 0.7330322265625:  80%|████████████████▊    | 32/40 [00:09<00:02,  3.42it/s]evaluate for the 35-th batch, evaluate loss: 0.4875521659851074:  74%|█████████████▎    | 34/46 [00:03<00:01,  9.66it/s]evaluate for the 35-th batch, evaluate loss: 0.4875521659851074:  76%|█████████████▋    | 35/46 [00:03<00:01,  9.67it/s]evaluate for the 36-th batch, evaluate loss: 0.4744811952114105:  76%|█████████████▋    | 35/46 [00:03<00:01,  9.67it/s]evaluate for the 36-th batch, evaluate loss: 0.4744811952114105:  78%|██████████████    | 36/46 [00:03<00:01,  9.69it/s]evaluate for the 37-th batch, evaluate loss: 0.504217267036438:  78%|██████████████▊    | 36/46 [00:03<00:01,  9.69it/s]evaluate for the 37-th batch, evaluate loss: 0.504217267036438:  80%|███████████████▎   | 37/46 [00:03<00:00,  9.67it/s]evaluate for the 19-th batch, evaluate loss: 0.23525430262088776:  45%|███████▋         | 18/40 [00:05<00:06,  3.55it/s]evaluate for the 19-th batch, evaluate loss: 0.23525430262088776:  48%|████████         | 19/40 [00:05<00:05,  3.58it/s]evaluate for the 33-th batch, evaluate loss: 0.7369599938392639:  80%|██████████████▍   | 32/40 [00:09<00:02,  3.42it/s]evaluate for the 33-th batch, evaluate loss: 0.7369599938392639:  82%|██████████████▊   | 33/40 [00:09<00:02,  3.49it/s]evaluate for the 38-th batch, evaluate loss: 0.5382490754127502:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.67it/s]evaluate for the 38-th batch, evaluate loss: 0.5382490754127502:  83%|██████████████▊   | 38/46 [00:03<00:00,  9.68it/s]Epoch: 2, train for the 102-th batch, train loss: 0.5487228035926819:  69%|███████▌   | 101/146 [01:02<00:27,  1.65it/s]Epoch: 2, train for the 102-th batch, train loss: 0.5487228035926819:  70%|███████▋   | 102/146 [01:02<00:26,  1.66it/s]evaluate for the 39-th batch, evaluate loss: 0.5385787487030029:  83%|██████████████▊   | 38/46 [00:04<00:00,  9.68it/s]evaluate for the 39-th batch, evaluate loss: 0.5385787487030029:  85%|███████████████▎  | 39/46 [00:04<00:00,  9.68it/s]evaluate for the 20-th batch, evaluate loss: 0.2064323127269745:  48%|████████▌         | 19/40 [00:05<00:05,  3.58it/s]evaluate for the 20-th batch, evaluate loss: 0.2064323127269745:  50%|█████████         | 20/40 [00:05<00:05,  3.50it/s]evaluate for the 40-th batch, evaluate loss: 0.4691208600997925:  85%|███████████████▎  | 39/46 [00:04<00:00,  9.68it/s]evaluate for the 40-th batch, evaluate loss: 0.4691208600997925:  87%|███████████████▋  | 40/46 [00:04<00:00,  9.70it/s]Epoch: 1, train for the 276-th batch, train loss: 0.594261884689331:  72%|████████▌   | 275/383 [02:44<01:03,  1.69it/s]Epoch: 1, train for the 276-th batch, train loss: 0.594261884689331:  72%|████████▋   | 276/383 [02:44<01:02,  1.70it/s]evaluate for the 34-th batch, evaluate loss: 0.7419023513793945:  82%|██████████████▊   | 33/40 [00:09<00:02,  3.49it/s]evaluate for the 34-th batch, evaluate loss: 0.7419023513793945:  85%|███████████████▎  | 34/40 [00:09<00:01,  3.42it/s]evaluate for the 41-th batch, evaluate loss: 0.48614394664764404:  87%|██████████████▊  | 40/46 [00:04<00:00,  9.70it/s]evaluate for the 41-th batch, evaluate loss: 0.48614394664764404:  89%|███████████████▏ | 41/46 [00:04<00:00,  9.70it/s]evaluate for the 42-th batch, evaluate loss: 0.4721680283546448:  89%|████████████████  | 41/46 [00:04<00:00,  9.70it/s]evaluate for the 42-th batch, evaluate loss: 0.4721680283546448:  91%|████████████████▍ | 42/46 [00:04<00:00,  9.69it/s]evaluate for the 21-th batch, evaluate loss: 0.16275626420974731:  50%|████████▌        | 20/40 [00:05<00:05,  3.50it/s]evaluate for the 21-th batch, evaluate loss: 0.16275626420974731:  52%|████████▉        | 21/40 [00:05<00:05,  3.64it/s]evaluate for the 35-th batch, evaluate loss: 0.7306737899780273:  85%|███████████████▎  | 34/40 [00:10<00:01,  3.42it/s]evaluate for the 35-th batch, evaluate loss: 0.7306737899780273:  88%|███████████████▊  | 35/40 [00:10<00:01,  3.48it/s]evaluate for the 43-th batch, evaluate loss: 0.5326748490333557:  91%|████████████████▍ | 42/46 [00:04<00:00,  9.69it/s]evaluate for the 43-th batch, evaluate loss: 0.5326748490333557:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.68it/s]evaluate for the 44-th batch, evaluate loss: 0.5131417512893677:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.68it/s]evaluate for the 44-th batch, evaluate loss: 0.5131417512893677:  96%|█████████████████▏| 44/46 [00:04<00:00,  9.69it/s]Epoch: 2, train for the 103-th batch, train loss: 0.5633029341697693:  70%|███████▋   | 102/146 [01:03<00:26,  1.66it/s]Epoch: 2, train for the 103-th batch, train loss: 0.5633029341697693:  71%|███████▊   | 103/146 [01:03<00:25,  1.66it/s]evaluate for the 45-th batch, evaluate loss: 0.4984124004840851:  96%|█████████████████▏| 44/46 [00:04<00:00,  9.69it/s]evaluate for the 45-th batch, evaluate loss: 0.4984124004840851:  98%|█████████████████▌| 45/46 [00:04<00:00,  9.68it/s]evaluate for the 22-th batch, evaluate loss: 0.19669732451438904:  52%|████████▉        | 21/40 [00:06<00:05,  3.64it/s]evaluate for the 22-th batch, evaluate loss: 0.19669732451438904:  55%|█████████▎       | 22/40 [00:06<00:05,  3.53it/s]Epoch: 1, train for the 277-th batch, train loss: 0.40800929069519043:  72%|███████▏  | 276/383 [02:45<01:02,  1.70it/s]Epoch: 1, train for the 277-th batch, train loss: 0.40800929069519043:  72%|███████▏  | 277/383 [02:45<01:02,  1.70it/s]evaluate for the 36-th batch, evaluate loss: 0.7671370506286621:  88%|███████████████▊  | 35/40 [00:10<00:01,  3.48it/s]evaluate for the 36-th batch, evaluate loss: 0.7671370506286621:  90%|████████████████▏ | 36/40 [00:10<00:01,  3.41it/s]evaluate for the 46-th batch, evaluate loss: 0.505986213684082:  98%|██████████████████▌| 45/46 [00:04<00:00,  9.68it/s]evaluate for the 46-th batch, evaluate loss: 0.505986213684082: 100%|███████████████████| 46/46 [00:04<00:00,  9.70it/s]
  0%|                                                                                            | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6424040198326111:   0%|                            | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6424040198326111:   4%|▊                   | 1/25 [00:00<00:02,  9.21it/s]evaluate for the 23-th batch, evaluate loss: 0.20207500457763672:  55%|█████████▎       | 22/40 [00:06<00:05,  3.53it/s]evaluate for the 23-th batch, evaluate loss: 0.20207500457763672:  57%|█████████▊       | 23/40 [00:06<00:04,  3.70it/s]evaluate for the 2-th batch, evaluate loss: 0.6503733396530151:   4%|▊                   | 1/25 [00:00<00:02,  9.21it/s]evaluate for the 2-th batch, evaluate loss: 0.6503733396530151:   8%|█▌                  | 2/25 [00:00<00:02,  9.21it/s]evaluate for the 37-th batch, evaluate loss: 0.757095992565155:  90%|█████████████████  | 36/40 [00:10<00:01,  3.41it/s]evaluate for the 37-th batch, evaluate loss: 0.757095992565155:  92%|█████████████████▌ | 37/40 [00:10<00:00,  3.48it/s]evaluate for the 3-th batch, evaluate loss: 0.6921543478965759:   8%|█▌                  | 2/25 [00:00<00:02,  9.21it/s]evaluate for the 3-th batch, evaluate loss: 0.6921543478965759:  12%|██▍                 | 3/25 [00:00<00:02,  9.22it/s]evaluate for the 4-th batch, evaluate loss: 0.6768110394477844:  12%|██▍                 | 3/25 [00:00<00:02,  9.22it/s]evaluate for the 4-th batch, evaluate loss: 0.6768110394477844:  16%|███▏                | 4/25 [00:00<00:02,  9.26it/s]Epoch: 2, train for the 104-th batch, train loss: 0.5544350147247314:  71%|███████▊   | 103/146 [01:03<00:25,  1.66it/s]Epoch: 2, train for the 104-th batch, train loss: 0.5544350147247314:  71%|███████▊   | 104/146 [01:03<00:25,  1.65it/s]evaluate for the 24-th batch, evaluate loss: 0.2062104493379593:  57%|██████████▎       | 23/40 [00:06<00:04,  3.70it/s]evaluate for the 24-th batch, evaluate loss: 0.2062104493379593:  60%|██████████▊       | 24/40 [00:06<00:04,  3.51it/s]evaluate for the 5-th batch, evaluate loss: 0.6828794479370117:  16%|███▏                | 4/25 [00:00<00:02,  9.26it/s]evaluate for the 5-th batch, evaluate loss: 0.6828794479370117:  20%|████                | 5/25 [00:00<00:02,  9.25it/s]Epoch: 1, train for the 278-th batch, train loss: 0.4394257366657257:  72%|███████▉   | 277/383 [02:45<01:02,  1.70it/s]Epoch: 1, train for the 278-th batch, train loss: 0.4394257366657257:  73%|███████▉   | 278/383 [02:45<01:01,  1.71it/s]evaluate for the 38-th batch, evaluate loss: 0.7876235842704773:  92%|████████████████▋ | 37/40 [00:10<00:00,  3.48it/s]evaluate for the 38-th batch, evaluate loss: 0.7876235842704773:  95%|█████████████████ | 38/40 [00:10<00:00,  3.41it/s]evaluate for the 6-th batch, evaluate loss: 0.7163934111595154:  20%|████                | 5/25 [00:00<00:02,  9.25it/s]evaluate for the 6-th batch, evaluate loss: 0.7163934111595154:  24%|████▊               | 6/25 [00:00<00:02,  9.25it/s]evaluate for the 7-th batch, evaluate loss: 0.7372510433197021:  24%|████▊               | 6/25 [00:00<00:02,  9.25it/s]evaluate for the 7-th batch, evaluate loss: 0.7372510433197021:  28%|█████▌              | 7/25 [00:00<00:01,  9.24it/s]evaluate for the 25-th batch, evaluate loss: 0.2112850546836853:  60%|██████████▊       | 24/40 [00:07<00:04,  3.51it/s]evaluate for the 25-th batch, evaluate loss: 0.2112850546836853:  62%|███████████▎      | 25/40 [00:07<00:04,  3.57it/s]evaluate for the 39-th batch, evaluate loss: 0.7502346634864807:  95%|█████████████████ | 38/40 [00:11<00:00,  3.41it/s]evaluate for the 39-th batch, evaluate loss: 0.7502346634864807:  98%|█████████████████▌| 39/40 [00:11<00:00,  3.49it/s]evaluate for the 8-th batch, evaluate loss: 0.7226890921592712:  28%|█████▌              | 7/25 [00:00<00:01,  9.24it/s]evaluate for the 8-th batch, evaluate loss: 0.7226890921592712:  32%|██████▍             | 8/25 [00:00<00:01,  9.27it/s]evaluate for the 9-th batch, evaluate loss: 0.7109686136245728:  32%|██████▍             | 8/25 [00:00<00:01,  9.27it/s]evaluate for the 9-th batch, evaluate loss: 0.7109686136245728:  36%|███████▏            | 9/25 [00:00<00:01,  9.29it/s]evaluate for the 26-th batch, evaluate loss: 0.1764029860496521:  62%|███████████▎      | 25/40 [00:07<00:04,  3.57it/s]evaluate for the 26-th batch, evaluate loss: 0.1764029860496521:  65%|███████████▋      | 26/40 [00:07<00:04,  3.49it/s]evaluate for the 10-th batch, evaluate loss: 0.7405233383178711:  36%|██████▊            | 9/25 [00:01<00:01,  9.29it/s]evaluate for the 10-th batch, evaluate loss: 0.7405233383178711:  40%|███████▏          | 10/25 [00:01<00:01,  9.28it/s]Epoch: 2, train for the 105-th batch, train loss: 0.585369884967804:  71%|████████▌   | 104/146 [01:04<00:25,  1.65it/s]Epoch: 2, train for the 105-th batch, train loss: 0.585369884967804:  72%|████████▋   | 105/146 [01:04<00:24,  1.65it/s]evaluate for the 40-th batch, evaluate loss: 0.7496784329414368:  98%|█████████████████▌| 39/40 [00:11<00:00,  3.49it/s]evaluate for the 40-th batch, evaluate loss: 0.7496784329414368: 100%|██████████████████| 40/40 [00:11<00:00,  3.46it/s]evaluate for the 40-th batch, evaluate loss: 0.7496784329414368: 100%|██████████████████| 40/40 [00:11<00:00,  3.48it/s]
Epoch: 1, train for the 279-th batch, train loss: 0.4630012512207031:  73%|███████▉   | 278/383 [02:46<01:01,  1.71it/s]Epoch: 1, train for the 279-th batch, train loss: 0.4630012512207031:  73%|████████   | 279/383 [02:46<01:00,  1.72it/s]INFO:root:Epoch: 1, learning rate: 0.0001, train loss: 0.6382
INFO:root:train average_precision, 0.7104
INFO:root:train roc_auc, 0.6978
INFO:root:validate loss: 0.6234
INFO:root:validate average_precision, 0.7200
INFO:root:validate roc_auc, 0.7072
INFO:root:new node validate loss: 0.7150
INFO:root:new node validate first_1_average_precision, 0.5820
INFO:root:new node validate first_1_roc_auc, 0.5675
INFO:root:new node validate first_3_average_precision, 0.5862
INFO:root:new node validate first_3_roc_auc, 0.5854
INFO:root:new node validate first_10_average_precision, 0.6106
INFO:root:new node validate first_10_roc_auc, 0.6201
INFO:root:new node validate average_precision, 0.5851
INFO:root:new node validate roc_auc, 0.5888
INFO:root:save model ./saved_models/DyGFormer/ia-digg-reply/DyGFormer_seed0_dygformer-ia-digg-reply-old/DyGFormer_seed0_dygformer-ia-digg-reply-old.pkl
evaluate for the 11-th batch, evaluate loss: 0.7329469323158264:  40%|███████▏          | 10/25 [00:01<00:01,  9.28it/s]evaluate for the 11-th batch, evaluate loss: 0.7329469323158264:  44%|███████▉          | 11/25 [00:01<00:01,  9.29it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/237 [00:00<?, ?it/s]evaluate for the 12-th batch, evaluate loss: 0.6975404024124146:  44%|███████▉          | 11/25 [00:01<00:01,  9.29it/s]evaluate for the 12-th batch, evaluate loss: 0.6975404024124146:  48%|████████▋         | 12/25 [00:01<00:01,  9.28it/s]evaluate for the 27-th batch, evaluate loss: 0.17693601548671722:  65%|███████████      | 26/40 [00:07<00:04,  3.49it/s]evaluate for the 27-th batch, evaluate loss: 0.17693601548671722:  68%|███████████▍     | 27/40 [00:07<00:03,  3.62it/s]evaluate for the 13-th batch, evaluate loss: 0.6705387830734253:  48%|████████▋         | 12/25 [00:01<00:01,  9.28it/s]evaluate for the 13-th batch, evaluate loss: 0.6705387830734253:  52%|█████████▎        | 13/25 [00:01<00:01,  9.28it/s]evaluate for the 14-th batch, evaluate loss: 0.7574986219406128:  52%|█████████▎        | 13/25 [00:01<00:01,  9.28it/s]evaluate for the 14-th batch, evaluate loss: 0.7574986219406128:  56%|██████████        | 14/25 [00:01<00:01,  9.28it/s]evaluate for the 28-th batch, evaluate loss: 0.1678794026374817:  68%|████████████▏     | 27/40 [00:07<00:03,  3.62it/s]evaluate for the 28-th batch, evaluate loss: 0.1678794026374817:  70%|████████████▌     | 28/40 [00:07<00:03,  3.55it/s]evaluate for the 15-th batch, evaluate loss: 0.7329214215278625:  56%|██████████        | 14/25 [00:01<00:01,  9.28it/s]evaluate for the 15-th batch, evaluate loss: 0.7329214215278625:  60%|██████████▊       | 15/25 [00:01<00:01,  9.27it/s]Epoch: 2, train for the 1-th batch, train loss: 0.7062296867370605:   0%|                       | 0/237 [00:00<?, ?it/s]Epoch: 2, train for the 1-th batch, train loss: 0.7062296867370605:   0%|               | 1/237 [00:00<01:42,  2.30it/s]Epoch: 1, train for the 280-th batch, train loss: 0.4104475677013397:  73%|████████   | 279/383 [02:46<01:00,  1.72it/s]Epoch: 1, train for the 280-th batch, train loss: 0.4104475677013397:  73%|████████   | 280/383 [02:46<00:57,  1.79it/s]Epoch: 2, train for the 106-th batch, train loss: 0.5359596610069275:  72%|███████▉   | 105/146 [01:04<00:24,  1.65it/s]Epoch: 2, train for the 106-th batch, train loss: 0.5359596610069275:  73%|███████▉   | 106/146 [01:04<00:24,  1.66it/s]evaluate for the 16-th batch, evaluate loss: 0.6631099581718445:  60%|██████████▊       | 15/25 [00:01<00:01,  9.27it/s]evaluate for the 16-th batch, evaluate loss: 0.6631099581718445:  64%|███████████▌      | 16/25 [00:01<00:00,  9.26it/s]evaluate for the 17-th batch, evaluate loss: 0.6676614284515381:  64%|███████████▌      | 16/25 [00:01<00:00,  9.26it/s]evaluate for the 17-th batch, evaluate loss: 0.6676614284515381:  68%|████████████▏     | 17/25 [00:01<00:00,  9.29it/s]evaluate for the 29-th batch, evaluate loss: 0.18971210718154907:  70%|███████████▉     | 28/40 [00:08<00:03,  3.55it/s]evaluate for the 29-th batch, evaluate loss: 0.18971210718154907:  72%|████████████▎    | 29/40 [00:08<00:03,  3.64it/s]evaluate for the 18-th batch, evaluate loss: 0.6260393261909485:  68%|████████████▏     | 17/25 [00:01<00:00,  9.29it/s]evaluate for the 18-th batch, evaluate loss: 0.6260393261909485:  72%|████████████▉     | 18/25 [00:01<00:00,  9.27it/s]Epoch: 2, train for the 2-th batch, train loss: 0.7036944627761841:   0%|               | 1/237 [00:00<01:42,  2.30it/s]Epoch: 2, train for the 2-th batch, train loss: 0.7036944627761841:   1%|▏              | 2/237 [00:00<01:38,  2.39it/s]evaluate for the 19-th batch, evaluate loss: 0.5971830487251282:  72%|████████████▉     | 18/25 [00:02<00:00,  9.27it/s]evaluate for the 19-th batch, evaluate loss: 0.5971830487251282:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.25it/s]evaluate for the 20-th batch, evaluate loss: 0.663491427898407:  76%|██████████████▍    | 19/25 [00:02<00:00,  9.25it/s]evaluate for the 20-th batch, evaluate loss: 0.663491427898407:  80%|███████████████▏   | 20/25 [00:02<00:00,  9.25it/s]evaluate for the 30-th batch, evaluate loss: 0.20077325403690338:  72%|████████████▎    | 29/40 [00:08<00:03,  3.64it/s]evaluate for the 30-th batch, evaluate loss: 0.20077325403690338:  75%|████████████▊    | 30/40 [00:08<00:02,  3.58it/s]Epoch: 1, train for the 281-th batch, train loss: 0.5050845146179199:  73%|████████   | 280/383 [02:47<00:57,  1.79it/s]Epoch: 1, train for the 281-th batch, train loss: 0.5050845146179199:  73%|████████   | 281/383 [02:47<00:57,  1.78it/s]evaluate for the 21-th batch, evaluate loss: 0.7282600402832031:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.25it/s]evaluate for the 21-th batch, evaluate loss: 0.7282600402832031:  84%|███████████████   | 21/25 [00:02<00:00,  9.27it/s]Epoch: 2, train for the 107-th batch, train loss: 0.567869246006012:  73%|████████▋   | 106/146 [01:05<00:24,  1.66it/s]Epoch: 2, train for the 107-th batch, train loss: 0.567869246006012:  73%|████████▊   | 107/146 [01:05<00:23,  1.66it/s]evaluate for the 22-th batch, evaluate loss: 0.6153441071510315:  84%|███████████████   | 21/25 [00:02<00:00,  9.27it/s]evaluate for the 22-th batch, evaluate loss: 0.6153441071510315:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.25it/s]evaluate for the 31-th batch, evaluate loss: 0.22074158489704132:  75%|████████████▊    | 30/40 [00:08<00:02,  3.58it/s]evaluate for the 31-th batch, evaluate loss: 0.22074158489704132:  78%|█████████████▏   | 31/40 [00:08<00:02,  3.61it/s]evaluate for the 23-th batch, evaluate loss: 0.6714907884597778:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.25it/s]evaluate for the 23-th batch, evaluate loss: 0.6714907884597778:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.22it/s]Epoch: 2, train for the 3-th batch, train loss: 0.6938827633857727:   1%|▏              | 2/237 [00:01<01:38,  2.39it/s]Epoch: 2, train for the 3-th batch, train loss: 0.6938827633857727:   1%|▏              | 3/237 [00:01<01:41,  2.32it/s]evaluate for the 24-th batch, evaluate loss: 0.6628310084342957:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.22it/s]evaluate for the 24-th batch, evaluate loss: 0.6628310084342957:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.23it/s]evaluate for the 25-th batch, evaluate loss: 0.7107195854187012:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.23it/s]evaluate for the 25-th batch, evaluate loss: 0.7107195854187012: 100%|██████████████████| 25/25 [00:02<00:00,  9.33it/s]
INFO:root:Epoch: 3, learning rate: 0.0001, train loss: 0.5727
INFO:root:train average_precision, 0.8086
INFO:root:train roc_auc, 0.7701
INFO:root:validate loss: 0.5118
INFO:root:validate average_precision, 0.8400
INFO:root:validate roc_auc, 0.8002
INFO:root:new node validate loss: 0.6868
INFO:root:new node validate first_1_average_precision, 0.5882
INFO:root:new node validate first_1_roc_auc, 0.5343
INFO:root:new node validate first_3_average_precision, 0.6695
INFO:root:new node validate first_3_roc_auc, 0.6313
INFO:root:new node validate first_10_average_precision, 0.7394
INFO:root:new node validate first_10_roc_auc, 0.7057
INFO:root:new node validate average_precision, 0.7024
INFO:root:new node validate roc_auc, 0.6530
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]evaluate for the 32-th batch, evaluate loss: 0.2151743322610855:  78%|█████████████▉    | 31/40 [00:08<00:02,  3.61it/s]evaluate for the 32-th batch, evaluate loss: 0.2151743322610855:  80%|██████████████▍   | 32/40 [00:08<00:02,  3.63it/s]Epoch: 1, train for the 282-th batch, train loss: 0.5080880522727966:  73%|████████   | 281/383 [02:47<00:57,  1.78it/s]Epoch: 1, train for the 282-th batch, train loss: 0.5080880522727966:  74%|████████   | 282/383 [02:47<00:57,  1.77it/s]Epoch: 4, train for the 1-th batch, train loss: 0.72342449426651:   0%|                         | 0/151 [00:00<?, ?it/s]Epoch: 4, train for the 1-th batch, train loss: 0.72342449426651:   1%|                 | 1/151 [00:00<00:25,  5.93it/s]Epoch: 2, train for the 108-th batch, train loss: 0.5592626333236694:  73%|████████   | 107/146 [01:06<00:23,  1.66it/s]Epoch: 2, train for the 108-th batch, train loss: 0.5592626333236694:  74%|████████▏  | 108/146 [01:06<00:22,  1.65it/s]Epoch: 2, train for the 4-th batch, train loss: 0.6724996566772461:   1%|▏              | 3/237 [00:01<01:41,  2.32it/s]Epoch: 2, train for the 4-th batch, train loss: 0.6724996566772461:   2%|▎              | 4/237 [00:01<01:42,  2.27it/s]evaluate for the 33-th batch, evaluate loss: 0.17563003301620483:  80%|█████████████▌   | 32/40 [00:09<00:02,  3.63it/s]evaluate for the 33-th batch, evaluate loss: 0.17563003301620483:  82%|██████████████   | 33/40 [00:09<00:01,  3.53it/s]Epoch: 4, train for the 2-th batch, train loss: 0.7069180011749268:   1%|               | 1/151 [00:00<00:25,  5.93it/s]Epoch: 4, train for the 2-th batch, train loss: 0.7069180011749268:   1%|▏              | 2/151 [00:00<00:25,  5.75it/s]Epoch: 4, train for the 3-th batch, train loss: 0.6839564442634583:   1%|▏              | 2/151 [00:00<00:25,  5.75it/s]Epoch: 4, train for the 3-th batch, train loss: 0.6839564442634583:   2%|▎              | 3/151 [00:00<00:24,  5.94it/s]evaluate for the 34-th batch, evaluate loss: 0.17747217416763306:  82%|██████████████   | 33/40 [00:09<00:01,  3.53it/s]evaluate for the 34-th batch, evaluate loss: 0.17747217416763306:  85%|██████████████▍  | 34/40 [00:09<00:01,  3.67it/s]Epoch: 4, train for the 4-th batch, train loss: 0.6809641122817993:   2%|▎              | 3/151 [00:00<00:24,  5.94it/s]Epoch: 4, train for the 4-th batch, train loss: 0.6809641122817993:   3%|▍              | 4/151 [00:00<00:24,  5.89it/s]Epoch: 1, train for the 283-th batch, train loss: 0.44047582149505615:  74%|███████▎  | 282/383 [02:48<00:57,  1.77it/s]Epoch: 1, train for the 283-th batch, train loss: 0.44047582149505615:  74%|███████▍  | 283/383 [02:48<00:57,  1.73it/s]Epoch: 2, train for the 5-th batch, train loss: 0.6801349520683289:   2%|▎              | 4/237 [00:02<01:42,  2.27it/s]Epoch: 2, train for the 5-th batch, train loss: 0.6801349520683289:   2%|▎              | 5/237 [00:02<01:45,  2.19it/s]Epoch: 2, train for the 109-th batch, train loss: 0.5541684627532959:  74%|████████▏  | 108/146 [01:06<00:22,  1.65it/s]Epoch: 2, train for the 109-th batch, train loss: 0.5541684627532959:  75%|████████▏  | 109/146 [01:06<00:22,  1.66it/s]Epoch: 4, train for the 5-th batch, train loss: 0.65006422996521:   3%|▍                | 4/151 [00:00<00:24,  5.89it/s]Epoch: 4, train for the 5-th batch, train loss: 0.65006422996521:   3%|▌                | 5/151 [00:00<00:25,  5.67it/s]evaluate for the 35-th batch, evaluate loss: 0.20196643471717834:  85%|██████████████▍  | 34/40 [00:09<00:01,  3.67it/s]evaluate for the 35-th batch, evaluate loss: 0.20196643471717834:  88%|██████████████▉  | 35/40 [00:09<00:01,  3.55it/s]Epoch: 4, train for the 6-th batch, train loss: 0.6560676693916321:   3%|▍              | 5/151 [00:01<00:25,  5.67it/s]Epoch: 4, train for the 6-th batch, train loss: 0.6560676693916321:   4%|▌              | 6/151 [00:01<00:26,  5.53it/s]evaluate for the 36-th batch, evaluate loss: 0.2010618895292282:  88%|███████████████▊  | 35/40 [00:10<00:01,  3.55it/s]evaluate for the 36-th batch, evaluate loss: 0.2010618895292282:  90%|████████████████▏ | 36/40 [00:10<00:01,  3.71it/s]Epoch: 2, train for the 6-th batch, train loss: 0.6627505421638489:   2%|▎              | 5/237 [00:02<01:45,  2.19it/s]Epoch: 2, train for the 6-th batch, train loss: 0.6627505421638489:   3%|▍              | 6/237 [00:02<01:47,  2.14it/s]Epoch: 4, train for the 7-th batch, train loss: 0.6109572649002075:   4%|▌              | 6/151 [00:01<00:26,  5.53it/s]Epoch: 4, train for the 7-th batch, train loss: 0.6109572649002075:   5%|▋              | 7/151 [00:01<00:26,  5.41it/s]Epoch: 1, train for the 284-th batch, train loss: 0.5016999840736389:  74%|████████▏  | 283/383 [02:49<00:57,  1.73it/s]Epoch: 1, train for the 284-th batch, train loss: 0.5016999840736389:  74%|████████▏  | 284/383 [02:49<00:58,  1.70it/s]Epoch: 2, train for the 110-th batch, train loss: 0.5230112671852112:  75%|████████▏  | 109/146 [01:07<00:22,  1.66it/s]Epoch: 2, train for the 110-th batch, train loss: 0.5230112671852112:  75%|████████▎  | 110/146 [01:07<00:21,  1.66it/s]evaluate for the 37-th batch, evaluate loss: 0.2383546084165573:  90%|████████████████▏ | 36/40 [00:10<00:01,  3.71it/s]evaluate for the 37-th batch, evaluate loss: 0.2383546084165573:  92%|████████████████▋ | 37/40 [00:10<00:00,  3.53it/s]Epoch: 4, train for the 8-th batch, train loss: 0.6460191011428833:   5%|▋              | 7/151 [00:01<00:26,  5.41it/s]Epoch: 4, train for the 8-th batch, train loss: 0.6460191011428833:   5%|▊              | 8/151 [00:01<00:26,  5.36it/s]Epoch: 4, train for the 9-th batch, train loss: 0.6588835120201111:   5%|▊              | 8/151 [00:01<00:26,  5.36it/s]Epoch: 4, train for the 9-th batch, train loss: 0.6588835120201111:   6%|▉              | 9/151 [00:01<00:27,  5.21it/s]Epoch: 2, train for the 7-th batch, train loss: 0.6495254635810852:   3%|▍              | 6/237 [00:03<01:47,  2.14it/s]Epoch: 2, train for the 7-th batch, train loss: 0.6495254635810852:   3%|▍              | 7/237 [00:03<01:45,  2.19it/s]evaluate for the 38-th batch, evaluate loss: 0.20159177482128143:  92%|███████████████▋ | 37/40 [00:10<00:00,  3.53it/s]evaluate for the 38-th batch, evaluate loss: 0.20159177482128143:  95%|████████████████▏| 38/40 [00:10<00:00,  3.61it/s]Epoch: 4, train for the 10-th batch, train loss: 0.6310718655586243:   6%|▊             | 9/151 [00:01<00:27,  5.21it/s]Epoch: 4, train for the 10-th batch, train loss: 0.6310718655586243:   7%|▊            | 10/151 [00:01<00:27,  5.07it/s]Epoch: 1, train for the 285-th batch, train loss: 0.4919804632663727:  74%|████████▏  | 284/383 [02:49<00:58,  1.70it/s]Epoch: 1, train for the 285-th batch, train loss: 0.4919804632663727:  74%|████████▏  | 285/383 [02:49<00:57,  1.71it/s]evaluate for the 39-th batch, evaluate loss: 0.22114355862140656:  95%|████████████████▏| 38/40 [00:10<00:00,  3.61it/s]evaluate for the 39-th batch, evaluate loss: 0.22114355862140656:  98%|████████████████▌| 39/40 [00:10<00:00,  3.51it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5230221152305603:  75%|████████▎  | 110/146 [01:07<00:21,  1.66it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5230221152305603:  76%|████████▎  | 111/146 [01:07<00:21,  1.66it/s]evaluate for the 40-th batch, evaluate loss: 0.06282178312540054:  98%|████████████████▌| 39/40 [00:10<00:00,  3.51it/s]evaluate for the 40-th batch, evaluate loss: 0.06282178312540054: 100%|█████████████████| 40/40 [00:10<00:00,  3.65it/s]
  0%|                                                                                            | 0/21 [00:00<?, ?it/s]Epoch: 4, train for the 11-th batch, train loss: 0.562343418598175:   7%|▉             | 10/151 [00:02<00:27,  5.07it/s]Epoch: 4, train for the 11-th batch, train loss: 0.562343418598175:   7%|█             | 11/151 [00:02<00:27,  5.08it/s]Epoch: 2, train for the 8-th batch, train loss: 0.6181167960166931:   3%|▍              | 7/237 [00:03<01:45,  2.19it/s]Epoch: 2, train for the 8-th batch, train loss: 0.6181167960166931:   3%|▌              | 8/237 [00:03<01:43,  2.21it/s]Epoch: 4, train for the 12-th batch, train loss: 0.5654683709144592:   7%|▉            | 11/151 [00:02<00:27,  5.08it/s]Epoch: 4, train for the 12-th batch, train loss: 0.5654683709144592:   8%|█            | 12/151 [00:02<00:27,  5.07it/s]evaluate for the 1-th batch, evaluate loss: 0.27067339420318604:   0%|                           | 0/21 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.27067339420318604:   5%|▉                  | 1/21 [00:00<00:05,  3.66it/s]Epoch: 4, train for the 13-th batch, train loss: 0.5874636769294739:   8%|█            | 12/151 [00:02<00:27,  5.07it/s]Epoch: 4, train for the 13-th batch, train loss: 0.5874636769294739:   9%|█            | 13/151 [00:02<00:27,  5.05it/s]Epoch: 1, train for the 286-th batch, train loss: 0.43368521332740784:  74%|███████▍  | 285/383 [02:50<00:57,  1.71it/s]Epoch: 1, train for the 286-th batch, train loss: 0.43368521332740784:  75%|███████▍  | 286/383 [02:50<00:56,  1.72it/s]Epoch: 2, train for the 9-th batch, train loss: 0.6064472198486328:   3%|▌              | 8/237 [00:04<01:43,  2.21it/s]Epoch: 2, train for the 9-th batch, train loss: 0.6064472198486328:   4%|▌              | 9/237 [00:04<01:44,  2.18it/s]Epoch: 2, train for the 112-th batch, train loss: 0.5759171843528748:  76%|████████▎  | 111/146 [01:08<00:21,  1.66it/s]Epoch: 2, train for the 112-th batch, train loss: 0.5759171843528748:  77%|████████▍  | 112/146 [01:08<00:20,  1.66it/s]evaluate for the 2-th batch, evaluate loss: 0.312319815158844:   5%|█                    | 1/21 [00:00<00:05,  3.66it/s]evaluate for the 2-th batch, evaluate loss: 0.312319815158844:  10%|██                   | 2/21 [00:00<00:05,  3.35it/s]Epoch: 4, train for the 14-th batch, train loss: 0.5575361847877502:   9%|█            | 13/151 [00:02<00:27,  5.05it/s]Epoch: 4, train for the 14-th batch, train loss: 0.5575361847877502:   9%|█▏           | 14/151 [00:02<00:27,  5.05it/s]Epoch: 4, train for the 15-th batch, train loss: 0.4811318814754486:   9%|█▏           | 14/151 [00:02<00:27,  5.05it/s]Epoch: 4, train for the 15-th batch, train loss: 0.4811318814754486:  10%|█▎           | 15/151 [00:02<00:27,  4.99it/s]evaluate for the 3-th batch, evaluate loss: 0.28534042835235596:  10%|█▊                 | 2/21 [00:00<00:05,  3.35it/s]evaluate for the 3-th batch, evaluate loss: 0.28534042835235596:  14%|██▋                | 3/21 [00:00<00:05,  3.46it/s]Epoch: 2, train for the 10-th batch, train loss: 0.5579248070716858:   4%|▌             | 9/237 [00:04<01:44,  2.18it/s]Epoch: 2, train for the 10-th batch, train loss: 0.5579248070716858:   4%|▌            | 10/237 [00:04<01:44,  2.17it/s]Epoch: 4, train for the 16-th batch, train loss: 0.675086259841919:  10%|█▍            | 15/151 [00:03<00:27,  4.99it/s]Epoch: 4, train for the 16-th batch, train loss: 0.675086259841919:  11%|█▍            | 16/151 [00:03<00:27,  4.95it/s]Epoch: 1, train for the 287-th batch, train loss: 0.46229779720306396:  75%|███████▍  | 286/383 [02:50<00:56,  1.72it/s]Epoch: 1, train for the 287-th batch, train loss: 0.46229779720306396:  75%|███████▍  | 287/383 [02:50<00:56,  1.71it/s]Epoch: 2, train for the 113-th batch, train loss: 0.508841872215271:  77%|█████████▏  | 112/146 [01:09<00:20,  1.66it/s]Epoch: 2, train for the 113-th batch, train loss: 0.508841872215271:  77%|█████████▎  | 113/146 [01:09<00:19,  1.67it/s]evaluate for the 4-th batch, evaluate loss: 0.23473522067070007:  14%|██▋                | 3/21 [00:01<00:05,  3.46it/s]evaluate for the 4-th batch, evaluate loss: 0.23473522067070007:  19%|███▌               | 4/21 [00:01<00:05,  3.35it/s]Epoch: 4, train for the 17-th batch, train loss: 0.5729843378067017:  11%|█▍           | 16/151 [00:03<00:27,  4.95it/s]Epoch: 4, train for the 17-th batch, train loss: 0.5729843378067017:  11%|█▍           | 17/151 [00:03<00:26,  4.96it/s]Epoch: 4, train for the 18-th batch, train loss: 0.6099372506141663:  11%|█▍           | 17/151 [00:03<00:26,  4.96it/s]Epoch: 4, train for the 18-th batch, train loss: 0.6099372506141663:  12%|█▌           | 18/151 [00:03<00:26,  4.97it/s]evaluate for the 5-th batch, evaluate loss: 0.27350011467933655:  19%|███▌               | 4/21 [00:01<00:05,  3.35it/s]evaluate for the 5-th batch, evaluate loss: 0.27350011467933655:  24%|████▌              | 5/21 [00:01<00:04,  3.42it/s]Epoch: 2, train for the 11-th batch, train loss: 0.6282025575637817:   4%|▌            | 10/237 [00:04<01:44,  2.17it/s]Epoch: 2, train for the 11-th batch, train loss: 0.6282025575637817:   5%|▌            | 11/237 [00:04<01:43,  2.18it/s]Epoch: 1, train for the 288-th batch, train loss: 0.52060467004776:  75%|█████████▋   | 287/383 [02:51<00:56,  1.71it/s]Epoch: 1, train for the 288-th batch, train loss: 0.52060467004776:  75%|█████████▊   | 288/383 [02:51<00:55,  1.71it/s]Epoch: 4, train for the 19-th batch, train loss: 0.6289276480674744:  12%|█▌           | 18/151 [00:03<00:26,  4.97it/s]Epoch: 4, train for the 19-th batch, train loss: 0.6289276480674744:  13%|█▋           | 19/151 [00:03<00:26,  4.95it/s]evaluate for the 6-th batch, evaluate loss: 0.31243881583213806:  24%|████▌              | 5/21 [00:01<00:04,  3.42it/s]evaluate for the 6-th batch, evaluate loss: 0.31243881583213806:  29%|█████▍             | 6/21 [00:01<00:04,  3.36it/s]Epoch: 2, train for the 114-th batch, train loss: 0.5558634400367737:  77%|████████▌  | 113/146 [01:09<00:19,  1.67it/s]Epoch: 2, train for the 114-th batch, train loss: 0.5558634400367737:  78%|████████▌  | 114/146 [01:09<00:19,  1.68it/s]Epoch: 4, train for the 20-th batch, train loss: 0.6098507642745972:  13%|█▋           | 19/151 [00:03<00:26,  4.95it/s]Epoch: 4, train for the 20-th batch, train loss: 0.6098507642745972:  13%|█▋           | 20/151 [00:03<00:26,  4.90it/s]Epoch: 2, train for the 12-th batch, train loss: 0.5350990891456604:   5%|▌            | 11/237 [00:05<01:43,  2.18it/s]Epoch: 2, train for the 12-th batch, train loss: 0.5350990891456604:   5%|▋            | 12/237 [00:05<01:45,  2.13it/s]evaluate for the 7-th batch, evaluate loss: 0.2618457078933716:  29%|█████▋              | 6/21 [00:02<00:04,  3.36it/s]evaluate for the 7-th batch, evaluate loss: 0.2618457078933716:  33%|██████▋             | 7/21 [00:02<00:04,  3.48it/s]Epoch: 4, train for the 21-th batch, train loss: 0.638016939163208:  13%|█▊            | 20/151 [00:04<00:26,  4.90it/s]Epoch: 4, train for the 21-th batch, train loss: 0.638016939163208:  14%|█▉            | 21/151 [00:04<00:26,  4.88it/s]Epoch: 1, train for the 289-th batch, train loss: 0.5140067934989929:  75%|████████▎  | 288/383 [02:52<00:55,  1.71it/s]Epoch: 1, train for the 289-th batch, train loss: 0.5140067934989929:  75%|████████▎  | 289/383 [02:52<00:55,  1.70it/s]Epoch: 4, train for the 22-th batch, train loss: 0.6069257259368896:  14%|█▊           | 21/151 [00:04<00:26,  4.88it/s]Epoch: 4, train for the 22-th batch, train loss: 0.6069257259368896:  15%|█▉           | 22/151 [00:04<00:26,  4.83it/s]evaluate for the 8-th batch, evaluate loss: 0.32496127486228943:  33%|██████▎            | 7/21 [00:02<00:04,  3.48it/s]evaluate for the 8-th batch, evaluate loss: 0.32496127486228943:  38%|███████▏           | 8/21 [00:02<00:03,  3.41it/s]Epoch: 2, train for the 115-th batch, train loss: 0.5491042733192444:  78%|████████▌  | 114/146 [01:10<00:19,  1.68it/s]Epoch: 2, train for the 115-th batch, train loss: 0.5491042733192444:  79%|████████▋  | 115/146 [01:10<00:18,  1.69it/s]Epoch: 2, train for the 13-th batch, train loss: 0.5206552743911743:   5%|▋            | 12/237 [00:05<01:45,  2.13it/s]Epoch: 2, train for the 13-th batch, train loss: 0.5206552743911743:   5%|▋            | 13/237 [00:05<01:45,  2.12it/s]Epoch: 4, train for the 23-th batch, train loss: 0.6238359808921814:  15%|█▉           | 22/151 [00:04<00:26,  4.83it/s]Epoch: 4, train for the 23-th batch, train loss: 0.6238359808921814:  15%|█▉           | 23/151 [00:04<00:26,  4.77it/s]evaluate for the 9-th batch, evaluate loss: 0.2586239278316498:  38%|███████▌            | 8/21 [00:02<00:03,  3.41it/s]evaluate for the 9-th batch, evaluate loss: 0.2586239278316498:  43%|████████▌           | 9/21 [00:02<00:03,  3.54it/s]Epoch: 4, train for the 24-th batch, train loss: 0.5943076610565186:  15%|█▉           | 23/151 [00:04<00:26,  4.77it/s]Epoch: 4, train for the 24-th batch, train loss: 0.5943076610565186:  16%|██           | 24/151 [00:04<00:26,  4.74it/s]Epoch: 1, train for the 290-th batch, train loss: 0.41612982749938965:  75%|███████▌  | 289/383 [02:52<00:55,  1.70it/s]Epoch: 1, train for the 290-th batch, train loss: 0.41612982749938965:  76%|███████▌  | 290/383 [02:52<00:53,  1.75it/s]Epoch: 4, train for the 25-th batch, train loss: 0.638976514339447:  16%|██▏           | 24/151 [00:04<00:26,  4.74it/s]Epoch: 4, train for the 25-th batch, train loss: 0.638976514339447:  17%|██▎           | 25/151 [00:04<00:26,  4.75it/s]evaluate for the 10-th batch, evaluate loss: 0.2870998680591583:  43%|████████▏          | 9/21 [00:02<00:03,  3.54it/s]evaluate for the 10-th batch, evaluate loss: 0.2870998680591583:  48%|████████▌         | 10/21 [00:02<00:03,  3.46it/s]Epoch: 2, train for the 116-th batch, train loss: 0.523434579372406:  79%|█████████▍  | 115/146 [01:10<00:18,  1.69it/s]Epoch: 2, train for the 116-th batch, train loss: 0.523434579372406:  79%|█████████▌  | 116/146 [01:10<00:17,  1.71it/s]Epoch: 2, train for the 14-th batch, train loss: 0.4942263066768646:   5%|▋            | 13/237 [00:06<01:45,  2.12it/s]Epoch: 2, train for the 14-th batch, train loss: 0.4942263066768646:   6%|▊            | 14/237 [00:06<01:46,  2.10it/s]Epoch: 4, train for the 26-th batch, train loss: 0.5851349830627441:  17%|██▏          | 25/151 [00:05<00:26,  4.75it/s]Epoch: 4, train for the 26-th batch, train loss: 0.5851349830627441:  17%|██▏          | 26/151 [00:05<00:26,  4.74it/s]evaluate for the 11-th batch, evaluate loss: 0.2224145531654358:  48%|████████▌         | 10/21 [00:03<00:03,  3.46it/s]evaluate for the 11-th batch, evaluate loss: 0.2224145531654358:  52%|█████████▍        | 11/21 [00:03<00:02,  3.57it/s]Epoch: 1, train for the 291-th batch, train loss: 0.42725974321365356:  76%|███████▌  | 290/383 [02:53<00:53,  1.75it/s]Epoch: 1, train for the 291-th batch, train loss: 0.42725974321365356:  76%|███████▌  | 291/383 [02:53<00:50,  1.81it/s]Epoch: 4, train for the 27-th batch, train loss: 0.5468201041221619:  17%|██▏          | 26/151 [00:05<00:26,  4.74it/s]Epoch: 4, train for the 27-th batch, train loss: 0.5468201041221619:  18%|██▎          | 27/151 [00:05<00:26,  4.75it/s]evaluate for the 12-th batch, evaluate loss: 0.3072042465209961:  52%|█████████▍        | 11/21 [00:03<00:02,  3.57it/s]evaluate for the 12-th batch, evaluate loss: 0.3072042465209961:  57%|██████████▎       | 12/21 [00:03<00:02,  3.49it/s]Epoch: 2, train for the 117-th batch, train loss: 0.5891230702400208:  79%|████████▋  | 116/146 [01:11<00:17,  1.71it/s]Epoch: 2, train for the 117-th batch, train loss: 0.5891230702400208:  80%|████████▊  | 117/146 [01:11<00:16,  1.72it/s]Epoch: 2, train for the 15-th batch, train loss: 0.534922182559967:   6%|▊             | 14/237 [00:07<01:46,  2.10it/s]Epoch: 2, train for the 15-th batch, train loss: 0.534922182559967:   6%|▉             | 15/237 [00:07<01:52,  1.97it/s]Epoch: 4, train for the 28-th batch, train loss: 0.5025076270103455:  18%|██▎          | 27/151 [00:05<00:26,  4.75it/s]Epoch: 4, train for the 28-th batch, train loss: 0.5025076270103455:  19%|██▍          | 28/151 [00:05<00:25,  4.77it/s]evaluate for the 13-th batch, evaluate loss: 0.28950828313827515:  57%|█████████▋       | 12/21 [00:03<00:02,  3.49it/s]evaluate for the 13-th batch, evaluate loss: 0.28950828313827515:  62%|██████████▌      | 13/21 [00:03<00:02,  3.58it/s]Epoch: 4, train for the 29-th batch, train loss: 0.5250827670097351:  19%|██▍          | 28/151 [00:05<00:25,  4.77it/s]Epoch: 4, train for the 29-th batch, train loss: 0.5250827670097351:  19%|██▍          | 29/151 [00:05<00:25,  4.79it/s]Epoch: 1, train for the 292-th batch, train loss: 0.48840776085853577:  76%|███████▌  | 291/383 [02:53<00:50,  1.81it/s]Epoch: 1, train for the 292-th batch, train loss: 0.48840776085853577:  76%|███████▌  | 292/383 [02:53<00:49,  1.82it/s]Epoch: 4, train for the 30-th batch, train loss: 0.6777216196060181:  19%|██▍          | 29/151 [00:05<00:25,  4.79it/s]Epoch: 4, train for the 30-th batch, train loss: 0.6777216196060181:  20%|██▌          | 30/151 [00:05<00:25,  4.74it/s]evaluate for the 14-th batch, evaluate loss: 0.25179606676101685:  62%|██████████▌      | 13/21 [00:04<00:02,  3.58it/s]evaluate for the 14-th batch, evaluate loss: 0.25179606676101685:  67%|███████████▎     | 14/21 [00:04<00:02,  3.50it/s]Epoch: 2, train for the 16-th batch, train loss: 0.4682222604751587:   6%|▊            | 15/237 [00:07<01:52,  1.97it/s]Epoch: 2, train for the 16-th batch, train loss: 0.4682222604751587:   7%|▉            | 16/237 [00:07<01:52,  1.96it/s]Epoch: 2, train for the 118-th batch, train loss: 0.5208677053451538:  80%|████████▊  | 117/146 [01:12<00:16,  1.72it/s]Epoch: 2, train for the 118-th batch, train loss: 0.5208677053451538:  81%|████████▉  | 118/146 [01:12<00:16,  1.73it/s]Epoch: 4, train for the 31-th batch, train loss: 0.5677297115325928:  20%|██▌          | 30/151 [00:06<00:25,  4.74it/s]Epoch: 4, train for the 31-th batch, train loss: 0.5677297115325928:  21%|██▋          | 31/151 [00:06<00:25,  4.73it/s]evaluate for the 15-th batch, evaluate loss: 0.24320775270462036:  67%|███████████▎     | 14/21 [00:04<00:02,  3.50it/s]evaluate for the 15-th batch, evaluate loss: 0.24320775270462036:  71%|████████████▏    | 15/21 [00:04<00:01,  3.58it/s]Epoch: 1, train for the 293-th batch, train loss: 0.46525049209594727:  76%|███████▌  | 292/383 [02:54<00:49,  1.82it/s]Epoch: 1, train for the 293-th batch, train loss: 0.46525049209594727:  77%|███████▋  | 293/383 [02:54<00:49,  1.83it/s]Epoch: 4, train for the 32-th batch, train loss: 0.5848017334938049:  21%|██▋          | 31/151 [00:06<00:25,  4.73it/s]Epoch: 4, train for the 32-th batch, train loss: 0.5848017334938049:  21%|██▊          | 32/151 [00:06<00:25,  4.74it/s]Epoch: 2, train for the 17-th batch, train loss: 0.40924346446990967:   7%|▊           | 16/237 [00:08<01:52,  1.96it/s]Epoch: 2, train for the 17-th batch, train loss: 0.40924346446990967:   7%|▊           | 17/237 [00:08<01:52,  1.96it/s]evaluate for the 16-th batch, evaluate loss: 0.28124454617500305:  71%|████████████▏    | 15/21 [00:04<00:01,  3.58it/s]evaluate for the 16-th batch, evaluate loss: 0.28124454617500305:  76%|████████████▉    | 16/21 [00:04<00:01,  3.51it/s]Epoch: 4, train for the 33-th batch, train loss: 0.8261183500289917:  21%|██▊          | 32/151 [00:06<00:25,  4.74it/s]Epoch: 4, train for the 33-th batch, train loss: 0.8261183500289917:  22%|██▊          | 33/151 [00:06<00:25,  4.67it/s]Epoch: 2, train for the 119-th batch, train loss: 0.5508058667182922:  81%|████████▉  | 118/146 [01:12<00:16,  1.73it/s]Epoch: 2, train for the 119-th batch, train loss: 0.5508058667182922:  82%|████████▉  | 119/146 [01:12<00:15,  1.73it/s]Epoch: 4, train for the 34-th batch, train loss: 0.47470197081565857:  22%|██▌         | 33/151 [00:06<00:25,  4.67it/s]Epoch: 4, train for the 34-th batch, train loss: 0.47470197081565857:  23%|██▋         | 34/151 [00:06<00:24,  4.70it/s]evaluate for the 17-th batch, evaluate loss: 0.28851860761642456:  76%|████████████▉    | 16/21 [00:04<00:01,  3.51it/s]evaluate for the 17-th batch, evaluate loss: 0.28851860761642456:  81%|█████████████▊   | 17/21 [00:04<00:01,  3.59it/s]Epoch: 1, train for the 294-th batch, train loss: 0.4096810817718506:  77%|████████▍  | 293/383 [02:54<00:49,  1.83it/s]Epoch: 1, train for the 294-th batch, train loss: 0.4096810817718506:  77%|████████▍  | 294/383 [02:54<00:50,  1.77it/s]Epoch: 4, train for the 35-th batch, train loss: 0.5528069138526917:  23%|██▉          | 34/151 [00:07<00:24,  4.70it/s]Epoch: 4, train for the 35-th batch, train loss: 0.5528069138526917:  23%|███          | 35/151 [00:07<00:24,  4.71it/s]Epoch: 2, train for the 18-th batch, train loss: 0.4647155702114105:   7%|▉            | 17/237 [00:08<01:52,  1.96it/s]Epoch: 2, train for the 18-th batch, train loss: 0.4647155702114105:   8%|▉            | 18/237 [00:08<01:55,  1.90it/s]evaluate for the 18-th batch, evaluate loss: 0.2500365078449249:  81%|██████████████▌   | 17/21 [00:05<00:01,  3.59it/s]evaluate for the 18-th batch, evaluate loss: 0.2500365078449249:  86%|███████████████▍  | 18/21 [00:05<00:00,  3.52it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5265798568725586:  82%|████████▉  | 119/146 [01:13<00:15,  1.73it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5265798568725586:  82%|█████████  | 120/146 [01:13<00:15,  1.73it/s]Epoch: 4, train for the 36-th batch, train loss: 0.6912722587585449:  23%|███          | 35/151 [00:07<00:24,  4.71it/s]Epoch: 4, train for the 36-th batch, train loss: 0.6912722587585449:  24%|███          | 36/151 [00:07<00:24,  4.64it/s]evaluate for the 19-th batch, evaluate loss: 0.33401787281036377:  86%|██████████████▌  | 18/21 [00:05<00:00,  3.52it/s]evaluate for the 19-th batch, evaluate loss: 0.33401787281036377:  90%|███████████████▍ | 19/21 [00:05<00:00,  3.60it/s]Epoch: 4, train for the 37-th batch, train loss: 0.6743573546409607:  24%|███          | 36/151 [00:07<00:24,  4.64it/s]Epoch: 4, train for the 37-th batch, train loss: 0.6743573546409607:  25%|███▏         | 37/151 [00:07<00:24,  4.67it/s]Epoch: 1, train for the 295-th batch, train loss: 0.5094248056411743:  77%|████████▍  | 294/383 [02:55<00:50,  1.77it/s]Epoch: 1, train for the 295-th batch, train loss: 0.5094248056411743:  77%|████████▍  | 295/383 [02:55<00:50,  1.74it/s]Epoch: 4, train for the 38-th batch, train loss: 0.6199474334716797:  25%|███▏         | 37/151 [00:07<00:24,  4.67it/s]Epoch: 4, train for the 38-th batch, train loss: 0.6199474334716797:  25%|███▎         | 38/151 [00:07<00:23,  4.73it/s]Epoch: 2, train for the 19-th batch, train loss: 0.4791496694087982:   8%|▉            | 18/237 [00:09<01:55,  1.90it/s]Epoch: 2, train for the 19-th batch, train loss: 0.4791496694087982:   8%|█            | 19/237 [00:09<01:57,  1.86it/s]evaluate for the 20-th batch, evaluate loss: 0.3196762502193451:  90%|████████████████▎ | 19/21 [00:05<00:00,  3.60it/s]evaluate for the 20-th batch, evaluate loss: 0.3196762502193451:  95%|█████████████████▏| 20/21 [00:05<00:00,  3.53it/s]evaluate for the 21-th batch, evaluate loss: 0.07870126515626907:  95%|████████████████▏| 20/21 [00:05<00:00,  3.53it/s]evaluate for the 21-th batch, evaluate loss: 0.07870126515626907: 100%|█████████████████| 21/21 [00:05<00:00,  3.65it/s]
INFO:root:Epoch: 2, learning rate: 0.0001, train loss: 0.2534
INFO:root:train average_precision, 0.9602
INFO:root:train roc_auc, 0.9459
INFO:root:validate loss: 0.2003
INFO:root:validate average_precision, 0.9763
INFO:root:validate roc_auc, 0.9713
INFO:root:new node validate loss: 0.2709
INFO:root:new node validate first_1_average_precision, 0.8596
INFO:root:new node validate first_1_roc_auc, 0.8609
INFO:root:new node validate first_3_average_precision, 0.9295
INFO:root:new node validate first_3_roc_auc, 0.9247
INFO:root:new node validate first_10_average_precision, 0.9544
INFO:root:new node validate first_10_roc_auc, 0.9497
INFO:root:new node validate average_precision, 0.9560
INFO:root:new node validate roc_auc, 0.9477
INFO:root:save model ./saved_models/DyGFormer/ia-reality-call/DyGFormer_seed0_dygformer-ia-reality-call-old/DyGFormer_seed0_dygformer-ia-reality-call-old.pkl
Epoch: 2, train for the 121-th batch, train loss: 0.555009663105011:  82%|█████████▊  | 120/146 [01:13<00:15,  1.73it/s]Epoch: 2, train for the 121-th batch, train loss: 0.555009663105011:  83%|█████████▉  | 121/146 [01:13<00:14,  1.76it/s]Epoch: 4, train for the 39-th batch, train loss: 0.5659614205360413:  25%|███▎         | 38/151 [00:07<00:23,  4.73it/s]Epoch: 4, train for the 39-th batch, train loss: 0.5659614205360413:  26%|███▎         | 39/151 [00:07<00:23,  4.75it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]Epoch: 2, train for the 122-th batch, train loss: 0.5541402101516724:  83%|█████████  | 121/146 [01:13<00:14,  1.76it/s]Epoch: 2, train for the 122-th batch, train loss: 0.5541402101516724:  84%|█████████▏ | 122/146 [01:13<00:11,  2.06it/s]Epoch: 4, train for the 40-th batch, train loss: 0.6073325276374817:  26%|███▎         | 39/151 [00:08<00:23,  4.75it/s]Epoch: 4, train for the 40-th batch, train loss: 0.6073325276374817:  26%|███▍         | 40/151 [00:08<00:23,  4.67it/s]Epoch: 1, train for the 296-th batch, train loss: 0.36472538113594055:  77%|███████▋  | 295/383 [02:56<00:50,  1.74it/s]Epoch: 1, train for the 296-th batch, train loss: 0.36472538113594055:  77%|███████▋  | 296/383 [02:56<00:50,  1.73it/s]Epoch: 2, train for the 20-th batch, train loss: 0.6790798902511597:   8%|█            | 19/237 [00:09<01:57,  1.86it/s]Epoch: 2, train for the 20-th batch, train loss: 0.6790798902511597:   8%|█            | 20/237 [00:09<01:56,  1.87it/s]Epoch: 4, train for the 41-th batch, train loss: 0.4999901056289673:  26%|███▍         | 40/151 [00:08<00:23,  4.67it/s]Epoch: 4, train for the 41-th batch, train loss: 0.4999901056289673:  27%|███▌         | 41/151 [00:08<00:23,  4.69it/s]Epoch: 3, train for the 1-th batch, train loss: 1.0263084173202515:   0%|                       | 0/119 [00:00<?, ?it/s]Epoch: 3, train for the 1-th batch, train loss: 1.0263084173202515:   1%|▏              | 1/119 [00:00<00:59,  1.98it/s]Epoch: 4, train for the 42-th batch, train loss: 0.6577724814414978:  27%|███▌         | 41/151 [00:08<00:23,  4.69it/s]Epoch: 4, train for the 42-th batch, train loss: 0.6577724814414978:  28%|███▌         | 42/151 [00:08<00:23,  4.63it/s]Epoch: 2, train for the 123-th batch, train loss: 0.5766240358352661:  84%|█████████▏ | 122/146 [01:14<00:11,  2.06it/s]Epoch: 2, train for the 123-th batch, train loss: 0.5766240358352661:  84%|█████████▎ | 123/146 [01:14<00:11,  1.95it/s]Epoch: 4, train for the 43-th batch, train loss: 0.6670960783958435:  28%|███▌         | 42/151 [00:08<00:23,  4.63it/s]Epoch: 4, train for the 43-th batch, train loss: 0.6670960783958435:  28%|███▋         | 43/151 [00:08<00:23,  4.59it/s]Epoch: 2, train for the 21-th batch, train loss: 0.4022139310836792:   8%|█            | 20/237 [00:10<01:56,  1.87it/s]Epoch: 2, train for the 21-th batch, train loss: 0.4022139310836792:   9%|█▏           | 21/237 [00:10<01:56,  1.86it/s]Epoch: 1, train for the 297-th batch, train loss: 0.5267012715339661:  77%|████████▌  | 296/383 [02:56<00:50,  1.73it/s]Epoch: 1, train for the 297-th batch, train loss: 0.5267012715339661:  78%|████████▌  | 297/383 [02:56<00:50,  1.70it/s]Epoch: 4, train for the 44-th batch, train loss: 0.6465234160423279:  28%|███▋         | 43/151 [00:08<00:23,  4.59it/s]Epoch: 4, train for the 44-th batch, train loss: 0.6465234160423279:  29%|███▊         | 44/151 [00:08<00:23,  4.56it/s]Epoch: 3, train for the 2-th batch, train loss: 0.5817826986312866:   1%|▏              | 1/119 [00:01<00:59,  1.98it/s]Epoch: 3, train for the 2-th batch, train loss: 0.5817826986312866:   2%|▎              | 2/119 [00:01<01:06,  1.76it/s]Epoch: 4, train for the 45-th batch, train loss: 0.5081238150596619:  29%|███▊         | 44/151 [00:09<00:23,  4.56it/s]Epoch: 4, train for the 45-th batch, train loss: 0.5081238150596619:  30%|███▊         | 45/151 [00:09<00:23,  4.60it/s]Epoch: 2, train for the 124-th batch, train loss: 0.5826079249382019:  84%|█████████▎ | 123/146 [01:15<00:11,  1.95it/s]Epoch: 2, train for the 124-th batch, train loss: 0.5826079249382019:  85%|█████████▎ | 124/146 [01:15<00:11,  1.85it/s]Epoch: 2, train for the 22-th batch, train loss: 0.6097155213356018:   9%|█▏           | 21/237 [00:10<01:56,  1.86it/s]Epoch: 2, train for the 22-th batch, train loss: 0.6097155213356018:   9%|█▏           | 22/237 [00:10<02:00,  1.79it/s]Epoch: 1, train for the 298-th batch, train loss: 0.48616889119148254:  78%|███████▊  | 297/383 [02:57<00:50,  1.70it/s]Epoch: 1, train for the 298-th batch, train loss: 0.48616889119148254:  78%|███████▊  | 298/383 [02:57<00:51,  1.66it/s]Epoch: 4, train for the 46-th batch, train loss: 0.6287741661071777:  30%|███▊         | 45/151 [00:09<00:23,  4.60it/s]Epoch: 4, train for the 46-th batch, train loss: 0.6287741661071777:  30%|███▉         | 46/151 [00:09<00:22,  4.57it/s]Epoch: 4, train for the 47-th batch, train loss: 0.5235351920127869:  30%|███▉         | 46/151 [00:09<00:22,  4.57it/s]Epoch: 4, train for the 47-th batch, train loss: 0.5235351920127869:  31%|████         | 47/151 [00:09<00:22,  4.52it/s]Epoch: 3, train for the 3-th batch, train loss: 0.35402050614356995:   2%|▏             | 2/119 [00:01<01:06,  1.76it/s]Epoch: 3, train for the 3-th batch, train loss: 0.35402050614356995:   3%|▎             | 3/119 [00:01<01:07,  1.72it/s]Epoch: 4, train for the 48-th batch, train loss: 0.5114226341247559:  31%|████         | 47/151 [00:09<00:22,  4.52it/s]Epoch: 4, train for the 48-th batch, train loss: 0.5114226341247559:  32%|████▏        | 48/151 [00:09<00:22,  4.57it/s]Epoch: 2, train for the 125-th batch, train loss: 0.530606210231781:  85%|██████████▏ | 124/146 [01:15<00:11,  1.85it/s]Epoch: 2, train for the 125-th batch, train loss: 0.530606210231781:  86%|██████████▎ | 125/146 [01:15<00:11,  1.78it/s]Epoch: 2, train for the 23-th batch, train loss: 0.5372657179832458:   9%|█▏           | 22/237 [00:11<02:00,  1.79it/s]Epoch: 2, train for the 23-th batch, train loss: 0.5372657179832458:  10%|█▎           | 23/237 [00:11<02:01,  1.75it/s]Epoch: 1, train for the 299-th batch, train loss: 0.45156851410865784:  78%|███████▊  | 298/383 [02:57<00:51,  1.66it/s]Epoch: 1, train for the 299-th batch, train loss: 0.45156851410865784:  78%|███████▊  | 299/383 [02:57<00:50,  1.66it/s]Epoch: 4, train for the 49-th batch, train loss: 0.5868821740150452:  32%|████▏        | 48/151 [00:10<00:22,  4.57it/s]Epoch: 4, train for the 49-th batch, train loss: 0.5868821740150452:  32%|████▏        | 49/151 [00:10<00:22,  4.56it/s]Epoch: 4, train for the 50-th batch, train loss: 0.5826972723007202:  32%|████▏        | 49/151 [00:10<00:22,  4.56it/s]Epoch: 4, train for the 50-th batch, train loss: 0.5826972723007202:  33%|████▎        | 50/151 [00:10<00:22,  4.55it/s]Epoch: 3, train for the 4-th batch, train loss: 0.2796168327331543:   3%|▍              | 3/119 [00:02<01:07,  1.72it/s]Epoch: 3, train for the 4-th batch, train loss: 0.2796168327331543:   3%|▌              | 4/119 [00:02<01:08,  1.68it/s]Epoch: 2, train for the 24-th batch, train loss: 0.5481138229370117:  10%|█▎           | 23/237 [00:11<02:01,  1.75it/s]Epoch: 2, train for the 24-th batch, train loss: 0.5481138229370117:  10%|█▎           | 24/237 [00:11<01:48,  1.96it/s]Epoch: 2, train for the 126-th batch, train loss: 0.5570869445800781:  86%|█████████▍ | 125/146 [01:16<00:11,  1.78it/s]Epoch: 2, train for the 126-th batch, train loss: 0.5570869445800781:  86%|█████████▍ | 126/146 [01:16<00:11,  1.74it/s]Epoch: 4, train for the 51-th batch, train loss: 0.6259390711784363:  33%|████▎        | 50/151 [00:10<00:22,  4.55it/s]Epoch: 4, train for the 51-th batch, train loss: 0.6259390711784363:  34%|████▍        | 51/151 [00:10<00:22,  4.54it/s]Epoch: 2, train for the 25-th batch, train loss: 0.540087103843689:  10%|█▍            | 24/237 [00:12<01:48,  1.96it/s]Epoch: 2, train for the 25-th batch, train loss: 0.540087103843689:  11%|█▍            | 25/237 [00:12<01:38,  2.16it/s]Epoch: 4, train for the 52-th batch, train loss: 0.5710257291793823:  34%|████▍        | 51/151 [00:10<00:22,  4.54it/s]Epoch: 4, train for the 52-th batch, train loss: 0.5710257291793823:  34%|████▍        | 52/151 [00:10<00:21,  4.57it/s]Epoch: 2, train for the 26-th batch, train loss: 0.3870968818664551:  11%|█▎           | 25/237 [00:12<01:38,  2.16it/s]Epoch: 2, train for the 26-th batch, train loss: 0.3870968818664551:  11%|█▍           | 26/237 [00:12<01:21,  2.58it/s]Epoch: 3, train for the 5-th batch, train loss: 0.34575438499450684:   3%|▍             | 4/119 [00:02<01:08,  1.68it/s]Epoch: 3, train for the 5-th batch, train loss: 0.34575438499450684:   4%|▌             | 5/119 [00:02<01:08,  1.66it/s]Epoch: 4, train for the 53-th batch, train loss: 0.6412264704704285:  34%|████▍        | 52/151 [00:10<00:21,  4.57it/s]Epoch: 4, train for the 53-th batch, train loss: 0.6412264704704285:  35%|████▌        | 53/151 [00:10<00:21,  4.55it/s]Epoch: 1, train for the 300-th batch, train loss: 0.4318479299545288:  78%|████████▌  | 299/383 [02:58<00:50,  1.66it/s]Epoch: 1, train for the 300-th batch, train loss: 0.4318479299545288:  78%|████████▌  | 300/383 [02:58<01:01,  1.35it/s]Epoch: 2, train for the 127-th batch, train loss: 0.5388432741165161:  86%|█████████▍ | 126/146 [01:17<00:11,  1.74it/s]Epoch: 2, train for the 127-th batch, train loss: 0.5388432741165161:  87%|█████████▌ | 127/146 [01:17<00:11,  1.70it/s]Epoch: 4, train for the 54-th batch, train loss: 0.566084623336792:  35%|████▉         | 53/151 [00:11<00:21,  4.55it/s]Epoch: 4, train for the 54-th batch, train loss: 0.566084623336792:  36%|█████         | 54/151 [00:11<00:21,  4.54it/s]Epoch: 2, train for the 27-th batch, train loss: 0.5528348684310913:  11%|█▍           | 26/237 [00:12<01:21,  2.58it/s]Epoch: 2, train for the 27-th batch, train loss: 0.5528348684310913:  11%|█▍           | 27/237 [00:12<01:26,  2.44it/s]Epoch: 4, train for the 55-th batch, train loss: 0.5791340470314026:  36%|████▋        | 54/151 [00:11<00:21,  4.54it/s]Epoch: 4, train for the 55-th batch, train loss: 0.5791340470314026:  36%|████▋        | 55/151 [00:11<00:21,  4.52it/s]Epoch: 3, train for the 6-th batch, train loss: 0.29911643266677856:   4%|▌             | 5/119 [00:03<01:08,  1.66it/s]Epoch: 3, train for the 6-th batch, train loss: 0.29911643266677856:   5%|▋             | 6/119 [00:03<01:09,  1.63it/s]Epoch: 4, train for the 56-th batch, train loss: 0.4766770899295807:  36%|████▋        | 55/151 [00:11<00:21,  4.52it/s]Epoch: 4, train for the 56-th batch, train loss: 0.4766770899295807:  37%|████▊        | 56/151 [00:11<00:20,  4.53it/s]Epoch: 1, train for the 301-th batch, train loss: 0.38077864050865173:  78%|███████▊  | 300/383 [02:59<01:01,  1.35it/s]Epoch: 1, train for the 301-th batch, train loss: 0.38077864050865173:  79%|███████▊  | 301/383 [02:59<00:57,  1.44it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5514386892318726:  87%|█████████▌ | 127/146 [01:17<00:11,  1.70it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5514386892318726:  88%|█████████▋ | 128/146 [01:17<00:10,  1.67it/s]Epoch: 4, train for the 57-th batch, train loss: 0.5549471974372864:  37%|████▊        | 56/151 [00:11<00:20,  4.53it/s]Epoch: 4, train for the 57-th batch, train loss: 0.5549471974372864:  38%|████▉        | 57/151 [00:11<00:20,  4.51it/s]Epoch: 2, train for the 28-th batch, train loss: 0.5607104301452637:  11%|█▍           | 27/237 [00:13<01:26,  2.44it/s]Epoch: 2, train for the 28-th batch, train loss: 0.5607104301452637:  12%|█▌           | 28/237 [00:13<01:34,  2.21it/s]Epoch: 4, train for the 58-th batch, train loss: 0.5605182647705078:  38%|████▉        | 57/151 [00:12<00:20,  4.51it/s]Epoch: 4, train for the 58-th batch, train loss: 0.5605182647705078:  38%|████▉        | 58/151 [00:12<00:20,  4.52it/s]Epoch: 3, train for the 7-th batch, train loss: 0.3109055161476135:   5%|▊              | 6/119 [00:04<01:09,  1.63it/s]Epoch: 3, train for the 7-th batch, train loss: 0.3109055161476135:   6%|▉              | 7/119 [00:04<01:08,  1.63it/s]Epoch: 1, train for the 302-th batch, train loss: 0.4081301689147949:  79%|████████▋  | 301/383 [03:00<00:57,  1.44it/s]Epoch: 1, train for the 302-th batch, train loss: 0.4081301689147949:  79%|████████▋  | 302/383 [03:00<00:52,  1.53it/s]Epoch: 4, train for the 59-th batch, train loss: 0.5400352478027344:  38%|████▉        | 58/151 [00:12<00:20,  4.52it/s]Epoch: 4, train for the 59-th batch, train loss: 0.5400352478027344:  39%|█████        | 59/151 [00:12<00:20,  4.52it/s]Epoch: 2, train for the 129-th batch, train loss: 0.5645602345466614:  88%|█████████▋ | 128/146 [01:18<00:10,  1.67it/s]Epoch: 2, train for the 129-th batch, train loss: 0.5645602345466614:  88%|█████████▋ | 129/146 [01:18<00:10,  1.65it/s]Epoch: 2, train for the 29-th batch, train loss: 0.3931255638599396:  12%|█▌           | 28/237 [00:13<01:34,  2.21it/s]Epoch: 2, train for the 29-th batch, train loss: 0.3931255638599396:  12%|█▌           | 29/237 [00:13<01:41,  2.06it/s]Epoch: 4, train for the 60-th batch, train loss: 0.5275034308433533:  39%|█████        | 59/151 [00:12<00:20,  4.52it/s]Epoch: 4, train for the 60-th batch, train loss: 0.5275034308433533:  40%|█████▏       | 60/151 [00:12<00:20,  4.52it/s]Epoch: 1, train for the 303-th batch, train loss: 0.4490353763103485:  79%|████████▋  | 302/383 [03:00<00:52,  1.53it/s]Epoch: 1, train for the 303-th batch, train loss: 0.4490353763103485:  79%|████████▋  | 303/383 [03:00<00:50,  1.59it/s]Epoch: 3, train for the 8-th batch, train loss: 0.2636885643005371:   6%|▉              | 7/119 [00:04<01:08,  1.63it/s]Epoch: 3, train for the 8-th batch, train loss: 0.2636885643005371:   7%|█              | 8/119 [00:04<01:08,  1.62it/s]Epoch: 4, train for the 61-th batch, train loss: 0.5497598648071289:  40%|█████▏       | 60/151 [00:12<00:20,  4.52it/s]Epoch: 4, train for the 61-th batch, train loss: 0.5497598648071289:  40%|█████▎       | 61/151 [00:12<00:22,  4.06it/s]Epoch: 2, train for the 130-th batch, train loss: 0.505469799041748:  88%|██████████▌ | 129/146 [01:18<00:10,  1.65it/s]Epoch: 2, train for the 130-th batch, train loss: 0.505469799041748:  89%|██████████▋ | 130/146 [01:18<00:09,  1.64it/s]Epoch: 2, train for the 30-th batch, train loss: 0.9975123405456543:  12%|█▌           | 29/237 [00:14<01:41,  2.06it/s]Epoch: 2, train for the 30-th batch, train loss: 0.9975123405456543:  13%|█▋           | 30/237 [00:14<01:40,  2.06it/s]Epoch: 4, train for the 62-th batch, train loss: 0.5877243876457214:  40%|█████▎       | 61/151 [00:13<00:22,  4.06it/s]Epoch: 4, train for the 62-th batch, train loss: 0.5877243876457214:  41%|█████▎       | 62/151 [00:13<00:21,  4.18it/s]Epoch: 4, train for the 63-th batch, train loss: 0.5560874342918396:  41%|█████▎       | 62/151 [00:13<00:21,  4.18it/s]Epoch: 4, train for the 63-th batch, train loss: 0.5560874342918396:  42%|█████▍       | 63/151 [00:13<00:20,  4.27it/s]Epoch: 3, train for the 9-th batch, train loss: 0.27857455611228943:   7%|▉             | 8/119 [00:05<01:08,  1.62it/s]Epoch: 3, train for the 9-th batch, train loss: 0.27857455611228943:   8%|█             | 9/119 [00:05<01:07,  1.62it/s]Epoch: 1, train for the 304-th batch, train loss: 0.4885454773902893:  79%|████████▋  | 303/383 [03:01<00:50,  1.59it/s]Epoch: 1, train for the 304-th batch, train loss: 0.4885454773902893:  79%|████████▋  | 304/383 [03:01<00:49,  1.59it/s]Epoch: 4, train for the 64-th batch, train loss: 0.5267944931983948:  42%|█████▍       | 63/151 [00:13<00:20,  4.27it/s]Epoch: 4, train for the 64-th batch, train loss: 0.5267944931983948:  42%|█████▌       | 64/151 [00:13<00:20,  4.34it/s]Epoch: 2, train for the 31-th batch, train loss: 0.7334977388381958:  13%|█▋           | 30/237 [00:15<01:40,  2.06it/s]Epoch: 2, train for the 31-th batch, train loss: 0.7334977388381958:  13%|█▋           | 31/237 [00:15<01:47,  1.91it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5396323800086975:  89%|█████████▊ | 130/146 [01:19<00:09,  1.64it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5396323800086975:  90%|█████████▊ | 131/146 [01:19<00:09,  1.64it/s]Epoch: 4, train for the 65-th batch, train loss: 0.45829641819000244:  42%|█████       | 64/151 [00:13<00:20,  4.34it/s]Epoch: 4, train for the 65-th batch, train loss: 0.45829641819000244:  43%|█████▏      | 65/151 [00:13<00:19,  4.39it/s]Epoch: 4, train for the 66-th batch, train loss: 0.5250089168548584:  43%|█████▌       | 65/151 [00:13<00:19,  4.39it/s]Epoch: 4, train for the 66-th batch, train loss: 0.5250089168548584:  44%|█████▋       | 66/151 [00:13<00:19,  4.42it/s]Epoch: 1, train for the 305-th batch, train loss: 0.5385846495628357:  79%|████████▋  | 304/383 [03:01<00:49,  1.59it/s]Epoch: 1, train for the 305-th batch, train loss: 0.5385846495628357:  80%|████████▊  | 305/383 [03:01<00:48,  1.61it/s]Epoch: 3, train for the 10-th batch, train loss: 0.2665415406227112:   8%|█             | 9/119 [00:06<01:07,  1.62it/s]Epoch: 3, train for the 10-th batch, train loss: 0.2665415406227112:   8%|█            | 10/119 [00:06<01:07,  1.62it/s]Epoch: 2, train for the 32-th batch, train loss: 0.4390985369682312:  13%|█▋           | 31/237 [00:15<01:47,  1.91it/s]Epoch: 2, train for the 32-th batch, train loss: 0.4390985369682312:  14%|█▊           | 32/237 [00:15<01:50,  1.86it/s]Epoch: 4, train for the 67-th batch, train loss: 0.5917561650276184:  44%|█████▋       | 66/151 [00:14<00:19,  4.42it/s]Epoch: 4, train for the 67-th batch, train loss: 0.5917561650276184:  44%|█████▊       | 67/151 [00:14<00:18,  4.44it/s]Epoch: 2, train for the 132-th batch, train loss: 0.5257858037948608:  90%|█████████▊ | 131/146 [01:20<00:09,  1.64it/s]Epoch: 2, train for the 132-th batch, train loss: 0.5257858037948608:  90%|█████████▉ | 132/146 [01:20<00:08,  1.63it/s]Epoch: 4, train for the 68-th batch, train loss: 0.5300526022911072:  44%|█████▊       | 67/151 [00:14<00:18,  4.44it/s]Epoch: 4, train for the 68-th batch, train loss: 0.5300526022911072:  45%|█████▊       | 68/151 [00:14<00:18,  4.45it/s]Epoch: 1, train for the 306-th batch, train loss: 0.39757201075553894:  80%|███████▉  | 305/383 [03:02<00:48,  1.61it/s]Epoch: 1, train for the 306-th batch, train loss: 0.39757201075553894:  80%|███████▉  | 306/383 [03:02<00:46,  1.66it/s]Epoch: 4, train for the 69-th batch, train loss: 0.5219094753265381:  45%|█████▊       | 68/151 [00:14<00:18,  4.45it/s]Epoch: 4, train for the 69-th batch, train loss: 0.5219094753265381:  46%|█████▉       | 69/151 [00:14<00:18,  4.44it/s]Epoch: 3, train for the 11-th batch, train loss: 0.2698093056678772:   8%|█            | 10/119 [00:06<01:07,  1.62it/s]Epoch: 3, train for the 11-th batch, train loss: 0.2698093056678772:   9%|█▏           | 11/119 [00:06<01:06,  1.62it/s]Epoch: 2, train for the 33-th batch, train loss: 0.7138540744781494:  14%|█▊           | 32/237 [00:16<01:50,  1.86it/s]Epoch: 2, train for the 33-th batch, train loss: 0.7138540744781494:  14%|█▊           | 33/237 [00:16<01:52,  1.81it/s]Epoch: 2, train for the 133-th batch, train loss: 0.5576827526092529:  90%|█████████▉ | 132/146 [01:20<00:08,  1.63it/s]Epoch: 2, train for the 133-th batch, train loss: 0.5576827526092529:  91%|██████████ | 133/146 [01:20<00:07,  1.63it/s]Epoch: 4, train for the 70-th batch, train loss: 0.5378525257110596:  46%|█████▉       | 69/151 [00:14<00:18,  4.44it/s]Epoch: 4, train for the 70-th batch, train loss: 0.5378525257110596:  46%|██████       | 70/151 [00:14<00:18,  4.43it/s]Epoch: 4, train for the 71-th batch, train loss: 0.5625415444374084:  46%|██████       | 70/151 [00:15<00:18,  4.43it/s]Epoch: 4, train for the 71-th batch, train loss: 0.5625415444374084:  47%|██████       | 71/151 [00:15<00:18,  4.44it/s]Epoch: 1, train for the 307-th batch, train loss: 0.46546217799186707:  80%|███████▉  | 306/383 [03:03<00:46,  1.66it/s]Epoch: 1, train for the 307-th batch, train loss: 0.46546217799186707:  80%|████████  | 307/383 [03:03<00:45,  1.66it/s]Epoch: 3, train for the 12-th batch, train loss: 0.29438507556915283:   9%|█           | 11/119 [00:07<01:06,  1.62it/s]Epoch: 3, train for the 12-th batch, train loss: 0.29438507556915283:  10%|█▏          | 12/119 [00:07<01:05,  1.62it/s]Epoch: 4, train for the 72-th batch, train loss: 0.5206237435340881:  47%|██████       | 71/151 [00:15<00:18,  4.44it/s]Epoch: 4, train for the 72-th batch, train loss: 0.5206237435340881:  48%|██████▏      | 72/151 [00:15<00:17,  4.45it/s]Epoch: 2, train for the 34-th batch, train loss: 0.4258604645729065:  14%|█▊           | 33/237 [00:16<01:52,  1.81it/s]Epoch: 2, train for the 34-th batch, train loss: 0.4258604645729065:  14%|█▊           | 34/237 [00:16<01:52,  1.81it/s]Epoch: 2, train for the 134-th batch, train loss: 0.5016212463378906:  91%|██████████ | 133/146 [01:21<00:07,  1.63it/s]Epoch: 2, train for the 134-th batch, train loss: 0.5016212463378906:  92%|██████████ | 134/146 [01:21<00:07,  1.63it/s]Epoch: 4, train for the 73-th batch, train loss: 0.5722276568412781:  48%|██████▏      | 72/151 [00:15<00:17,  4.45it/s]Epoch: 4, train for the 73-th batch, train loss: 0.5722276568412781:  48%|██████▎      | 73/151 [00:15<00:17,  4.46it/s]Epoch: 4, train for the 74-th batch, train loss: 0.5056352019309998:  48%|██████▎      | 73/151 [00:15<00:17,  4.46it/s]Epoch: 4, train for the 74-th batch, train loss: 0.5056352019309998:  49%|██████▎      | 74/151 [00:15<00:17,  4.48it/s]Epoch: 3, train for the 13-th batch, train loss: 0.263538658618927:  10%|█▍            | 12/119 [00:07<01:05,  1.62it/s]Epoch: 3, train for the 13-th batch, train loss: 0.263538658618927:  11%|█▌            | 13/119 [00:07<01:01,  1.73it/s]Epoch: 1, train for the 308-th batch, train loss: 0.442112535238266:  80%|█████████▌  | 307/383 [03:03<00:45,  1.66it/s]Epoch: 1, train for the 308-th batch, train loss: 0.442112535238266:  80%|█████████▋  | 308/383 [03:03<00:44,  1.67it/s]Epoch: 2, train for the 35-th batch, train loss: 0.44690433144569397:  14%|█▋          | 34/237 [00:17<01:52,  1.81it/s]Epoch: 2, train for the 35-th batch, train loss: 0.44690433144569397:  15%|█▊          | 35/237 [00:17<01:52,  1.80it/s]Epoch: 4, train for the 75-th batch, train loss: 0.5643393397331238:  49%|██████▎      | 74/151 [00:15<00:17,  4.48it/s]Epoch: 4, train for the 75-th batch, train loss: 0.5643393397331238:  50%|██████▍      | 75/151 [00:15<00:16,  4.49it/s]Epoch: 2, train for the 135-th batch, train loss: 0.5652352571487427:  92%|██████████ | 134/146 [01:22<00:07,  1.63it/s]Epoch: 2, train for the 135-th batch, train loss: 0.5652352571487427:  92%|██████████▏| 135/146 [01:22<00:06,  1.58it/s]Epoch: 4, train for the 76-th batch, train loss: 0.5619809627532959:  50%|██████▍      | 75/151 [00:16<00:16,  4.49it/s]Epoch: 4, train for the 76-th batch, train loss: 0.5619809627532959:  50%|██████▌      | 76/151 [00:16<00:16,  4.48it/s]Epoch: 3, train for the 14-th batch, train loss: 0.2824205756187439:  11%|█▍           | 13/119 [00:08<01:01,  1.73it/s]Epoch: 3, train for the 14-th batch, train loss: 0.2824205756187439:  12%|█▌           | 14/119 [00:08<00:58,  1.79it/s]Epoch: 1, train for the 309-th batch, train loss: 0.49077674746513367:  80%|████████  | 308/383 [03:04<00:44,  1.67it/s]Epoch: 1, train for the 309-th batch, train loss: 0.49077674746513367:  81%|████████  | 309/383 [03:04<00:44,  1.68it/s]Epoch: 4, train for the 77-th batch, train loss: 0.5536168217658997:  50%|██████▌      | 76/151 [00:16<00:16,  4.48it/s]Epoch: 4, train for the 77-th batch, train loss: 0.5536168217658997:  51%|██████▋      | 77/151 [00:16<00:16,  4.49it/s]Epoch: 2, train for the 36-th batch, train loss: 0.4722353219985962:  15%|█▉           | 35/237 [00:17<01:52,  1.80it/s]Epoch: 2, train for the 36-th batch, train loss: 0.4722353219985962:  15%|█▉           | 36/237 [00:17<01:52,  1.79it/s]Epoch: 4, train for the 78-th batch, train loss: 0.5015766024589539:  51%|██████▋      | 77/151 [00:16<00:16,  4.49it/s]Epoch: 4, train for the 78-th batch, train loss: 0.5015766024589539:  52%|██████▋      | 78/151 [00:16<00:16,  4.50it/s]Epoch: 2, train for the 136-th batch, train loss: 0.5294469594955444:  92%|██████████▏| 135/146 [01:22<00:06,  1.58it/s]Epoch: 2, train for the 136-th batch, train loss: 0.5294469594955444:  93%|██████████▏| 136/146 [01:22<00:06,  1.57it/s]Epoch: 4, train for the 79-th batch, train loss: 0.6056339740753174:  52%|██████▋      | 78/151 [00:16<00:16,  4.50it/s]Epoch: 4, train for the 79-th batch, train loss: 0.6056339740753174:  52%|██████▊      | 79/151 [00:16<00:15,  4.50it/s]Epoch: 3, train for the 15-th batch, train loss: 0.2775406539440155:  12%|█▌           | 14/119 [00:08<00:58,  1.79it/s]Epoch: 3, train for the 15-th batch, train loss: 0.2775406539440155:  13%|█▋           | 15/119 [00:08<01:00,  1.72it/s]Epoch: 2, train for the 37-th batch, train loss: 0.4192948043346405:  15%|█▉           | 36/237 [00:18<01:52,  1.79it/s]Epoch: 2, train for the 37-th batch, train loss: 0.4192948043346405:  16%|██           | 37/237 [00:18<01:51,  1.79it/s]Epoch: 1, train for the 310-th batch, train loss: 0.5043827891349792:  81%|████████▊  | 309/383 [03:04<00:44,  1.68it/s]Epoch: 1, train for the 310-th batch, train loss: 0.5043827891349792:  81%|████████▉  | 310/383 [03:04<00:43,  1.67it/s]Epoch: 4, train for the 80-th batch, train loss: 0.5506815314292908:  52%|██████▊      | 79/151 [00:17<00:15,  4.50it/s]Epoch: 4, train for the 80-th batch, train loss: 0.5506815314292908:  53%|██████▉      | 80/151 [00:17<00:15,  4.47it/s]Epoch: 4, train for the 81-th batch, train loss: 0.5442835688591003:  53%|██████▉      | 80/151 [00:17<00:15,  4.47it/s]Epoch: 4, train for the 81-th batch, train loss: 0.5442835688591003:  54%|██████▉      | 81/151 [00:17<00:15,  4.46it/s]Epoch: 2, train for the 137-th batch, train loss: 0.5495821833610535:  93%|██████████▏| 136/146 [01:23<00:06,  1.57it/s]Epoch: 2, train for the 137-th batch, train loss: 0.5495821833610535:  94%|██████████▎| 137/146 [01:23<00:05,  1.59it/s]Epoch: 4, train for the 82-th batch, train loss: 0.5607929229736328:  54%|██████▉      | 81/151 [00:17<00:15,  4.46it/s]Epoch: 4, train for the 82-th batch, train loss: 0.5607929229736328:  54%|███████      | 82/151 [00:17<00:15,  4.46it/s]Epoch: 3, train for the 16-th batch, train loss: 0.3143206238746643:  13%|█▋           | 15/119 [00:09<01:00,  1.72it/s]Epoch: 3, train for the 16-th batch, train loss: 0.3143206238746643:  13%|█▋           | 16/119 [00:09<01:00,  1.69it/s]Epoch: 2, train for the 38-th batch, train loss: 0.48153918981552124:  16%|█▊          | 37/237 [00:19<01:51,  1.79it/s]Epoch: 2, train for the 38-th batch, train loss: 0.48153918981552124:  16%|█▉          | 38/237 [00:19<01:54,  1.74it/s]Epoch: 1, train for the 311-th batch, train loss: 0.47169986367225647:  81%|████████  | 310/383 [03:05<00:43,  1.67it/s]Epoch: 1, train for the 311-th batch, train loss: 0.47169986367225647:  81%|████████  | 311/383 [03:05<00:43,  1.64it/s]Epoch: 4, train for the 83-th batch, train loss: 0.5471341609954834:  54%|███████      | 82/151 [00:17<00:15,  4.46it/s]Epoch: 4, train for the 83-th batch, train loss: 0.5471341609954834:  55%|███████▏     | 83/151 [00:17<00:15,  4.47it/s]Epoch: 4, train for the 84-th batch, train loss: 0.5708555579185486:  55%|███████▏     | 83/151 [00:17<00:15,  4.47it/s]Epoch: 4, train for the 84-th batch, train loss: 0.5708555579185486:  56%|███████▏     | 84/151 [00:17<00:15,  4.47it/s]Epoch: 2, train for the 138-th batch, train loss: 0.5669370889663696:  94%|██████████▎| 137/146 [01:23<00:05,  1.59it/s]Epoch: 2, train for the 138-th batch, train loss: 0.5669370889663696:  95%|██████████▍| 138/146 [01:23<00:05,  1.60it/s]Epoch: 3, train for the 17-th batch, train loss: 0.25124335289001465:  13%|█▌          | 16/119 [00:10<01:00,  1.69it/s]Epoch: 3, train for the 17-th batch, train loss: 0.25124335289001465:  14%|█▋          | 17/119 [00:10<01:01,  1.67it/s]Epoch: 4, train for the 85-th batch, train loss: 0.48969534039497375:  56%|██████▋     | 84/151 [00:18<00:15,  4.47it/s]Epoch: 4, train for the 85-th batch, train loss: 0.48969534039497375:  56%|██████▊     | 85/151 [00:18<00:14,  4.51it/s]Epoch: 2, train for the 39-th batch, train loss: 0.7071797251701355:  16%|██           | 38/237 [00:19<01:54,  1.74it/s]Epoch: 2, train for the 39-th batch, train loss: 0.7071797251701355:  16%|██▏          | 39/237 [00:19<01:57,  1.68it/s]Epoch: 1, train for the 312-th batch, train loss: 0.3987274765968323:  81%|████████▉  | 311/383 [03:06<00:43,  1.64it/s]Epoch: 1, train for the 312-th batch, train loss: 0.3987274765968323:  81%|████████▉  | 312/383 [03:06<00:43,  1.63it/s]Epoch: 4, train for the 86-th batch, train loss: 0.5158771276473999:  56%|███████▎     | 85/151 [00:18<00:14,  4.51it/s]Epoch: 4, train for the 86-th batch, train loss: 0.5158771276473999:  57%|███████▍     | 86/151 [00:18<00:14,  4.50it/s]Epoch: 2, train for the 139-th batch, train loss: 0.5838420987129211:  95%|██████████▍| 138/146 [01:24<00:05,  1.60it/s]Epoch: 2, train for the 139-th batch, train loss: 0.5838420987129211:  95%|██████████▍| 139/146 [01:24<00:04,  1.61it/s]Epoch: 4, train for the 87-th batch, train loss: 0.5373746156692505:  57%|███████▍     | 86/151 [00:18<00:14,  4.50it/s]Epoch: 4, train for the 87-th batch, train loss: 0.5373746156692505:  58%|███████▍     | 87/151 [00:18<00:14,  4.50it/s]Epoch: 3, train for the 18-th batch, train loss: 0.2732754051685333:  14%|█▊           | 17/119 [00:10<01:01,  1.67it/s]Epoch: 3, train for the 18-th batch, train loss: 0.2732754051685333:  15%|█▉           | 18/119 [00:10<01:00,  1.66it/s]Epoch: 2, train for the 40-th batch, train loss: 0.41848036646842957:  16%|█▉          | 39/237 [00:20<01:57,  1.68it/s]Epoch: 2, train for the 40-th batch, train loss: 0.41848036646842957:  17%|██          | 40/237 [00:20<01:58,  1.67it/s]Epoch: 4, train for the 88-th batch, train loss: 0.5742626786231995:  58%|███████▍     | 87/151 [00:18<00:14,  4.50it/s]Epoch: 4, train for the 88-th batch, train loss: 0.5742626786231995:  58%|███████▌     | 88/151 [00:18<00:14,  4.49it/s]Epoch: 1, train for the 313-th batch, train loss: 0.47915297746658325:  81%|████████▏ | 312/383 [03:06<00:43,  1.63it/s]Epoch: 1, train for the 313-th batch, train loss: 0.47915297746658325:  82%|████████▏ | 313/383 [03:06<00:43,  1.62it/s]Epoch: 4, train for the 89-th batch, train loss: 0.5255454778671265:  58%|███████▌     | 88/151 [00:19<00:14,  4.49it/s]Epoch: 4, train for the 89-th batch, train loss: 0.5255454778671265:  59%|███████▋     | 89/151 [00:19<00:13,  4.49it/s]Epoch: 2, train for the 140-th batch, train loss: 0.5284906029701233:  95%|██████████▍| 139/146 [01:25<00:04,  1.61it/s]Epoch: 2, train for the 140-th batch, train loss: 0.5284906029701233:  96%|██████████▌| 140/146 [01:25<00:03,  1.61it/s]Epoch: 4, train for the 90-th batch, train loss: 0.572089672088623:  59%|████████▎     | 89/151 [00:19<00:13,  4.49it/s]Epoch: 4, train for the 90-th batch, train loss: 0.572089672088623:  60%|████████▎     | 90/151 [00:19<00:13,  4.47it/s]Epoch: 3, train for the 19-th batch, train loss: 0.24137085676193237:  15%|█▊          | 18/119 [00:11<01:00,  1.66it/s]Epoch: 3, train for the 19-th batch, train loss: 0.24137085676193237:  16%|█▉          | 19/119 [00:11<01:00,  1.65it/s]Epoch: 2, train for the 41-th batch, train loss: 0.5400756597518921:  17%|██▏          | 40/237 [00:20<01:58,  1.67it/s]Epoch: 2, train for the 41-th batch, train loss: 0.5400756597518921:  17%|██▏          | 41/237 [00:20<01:59,  1.64it/s]Epoch: 1, train for the 314-th batch, train loss: 0.4551236629486084:  82%|████████▉  | 313/383 [03:07<00:43,  1.62it/s]Epoch: 1, train for the 314-th batch, train loss: 0.4551236629486084:  82%|█████████  | 314/383 [03:07<00:42,  1.62it/s]Epoch: 4, train for the 91-th batch, train loss: 0.5059587955474854:  60%|███████▋     | 90/151 [00:19<00:13,  4.47it/s]Epoch: 4, train for the 91-th batch, train loss: 0.5059587955474854:  60%|███████▊     | 91/151 [00:19<00:13,  4.47it/s]Epoch: 4, train for the 92-th batch, train loss: 0.5332525372505188:  60%|███████▊     | 91/151 [00:19<00:13,  4.47it/s]Epoch: 4, train for the 92-th batch, train loss: 0.5332525372505188:  61%|███████▉     | 92/151 [00:19<00:13,  4.48it/s]Epoch: 2, train for the 141-th batch, train loss: 0.547227680683136:  96%|███████████▌| 140/146 [01:25<00:03,  1.61it/s]Epoch: 2, train for the 141-th batch, train loss: 0.547227680683136:  97%|███████████▌| 141/146 [01:25<00:03,  1.62it/s]Epoch: 4, train for the 93-th batch, train loss: 0.5364142656326294:  61%|███████▉     | 92/151 [00:19<00:13,  4.48it/s]Epoch: 4, train for the 93-th batch, train loss: 0.5364142656326294:  62%|████████     | 93/151 [00:19<00:12,  4.47it/s]Epoch: 3, train for the 20-th batch, train loss: 0.25955015420913696:  16%|█▉          | 19/119 [00:11<01:00,  1.65it/s]Epoch: 3, train for the 20-th batch, train loss: 0.25955015420913696:  17%|██          | 20/119 [00:11<01:00,  1.64it/s]Epoch: 2, train for the 42-th batch, train loss: 0.4716460406780243:  17%|██▏          | 41/237 [00:21<01:59,  1.64it/s]Epoch: 2, train for the 42-th batch, train loss: 0.4716460406780243:  18%|██▎          | 42/237 [00:21<01:59,  1.63it/s]Epoch: 1, train for the 315-th batch, train loss: 0.5169564485549927:  82%|█████████  | 314/383 [03:07<00:42,  1.62it/s]Epoch: 1, train for the 315-th batch, train loss: 0.5169564485549927:  82%|█████████  | 315/383 [03:07<00:42,  1.61it/s]Epoch: 4, train for the 94-th batch, train loss: 0.5697992444038391:  62%|████████     | 93/151 [00:20<00:12,  4.47it/s]Epoch: 4, train for the 94-th batch, train loss: 0.5697992444038391:  62%|████████     | 94/151 [00:20<00:12,  4.47it/s]Epoch: 4, train for the 95-th batch, train loss: 0.5133155584335327:  62%|████████     | 94/151 [00:20<00:12,  4.47it/s]Epoch: 4, train for the 95-th batch, train loss: 0.5133155584335327:  63%|████████▏    | 95/151 [00:20<00:12,  4.47it/s]Epoch: 2, train for the 142-th batch, train loss: 0.49777689576148987:  97%|█████████▋| 141/146 [01:26<00:03,  1.62it/s]Epoch: 2, train for the 142-th batch, train loss: 0.49777689576148987:  97%|█████████▋| 142/146 [01:26<00:02,  1.62it/s]Epoch: 3, train for the 21-th batch, train loss: 0.3031370937824249:  17%|██▏          | 20/119 [00:12<01:00,  1.64it/s]Epoch: 3, train for the 21-th batch, train loss: 0.3031370937824249:  18%|██▎          | 21/119 [00:12<00:59,  1.64it/s]Epoch: 4, train for the 96-th batch, train loss: 0.5477195978164673:  63%|████████▏    | 95/151 [00:20<00:12,  4.47it/s]Epoch: 4, train for the 96-th batch, train loss: 0.5477195978164673:  64%|████████▎    | 96/151 [00:20<00:12,  4.47it/s]Epoch: 2, train for the 43-th batch, train loss: 0.5660572648048401:  18%|██▎          | 42/237 [00:22<01:59,  1.63it/s]Epoch: 2, train for the 43-th batch, train loss: 0.5660572648048401:  18%|██▎          | 43/237 [00:22<01:59,  1.62it/s]Epoch: 1, train for the 316-th batch, train loss: 0.4157215654850006:  82%|█████████  | 315/383 [03:08<00:42,  1.61it/s]Epoch: 1, train for the 316-th batch, train loss: 0.4157215654850006:  83%|█████████  | 316/383 [03:08<00:41,  1.61it/s]Epoch: 4, train for the 97-th batch, train loss: 0.5931718349456787:  64%|████████▎    | 96/151 [00:20<00:12,  4.47it/s]Epoch: 4, train for the 97-th batch, train loss: 0.5931718349456787:  64%|████████▎    | 97/151 [00:20<00:12,  4.47it/s]Epoch: 2, train for the 143-th batch, train loss: 0.5369139909744263:  97%|██████████▋| 142/146 [01:26<00:02,  1.62it/s]Epoch: 2, train for the 143-th batch, train loss: 0.5369139909744263:  98%|██████████▊| 143/146 [01:26<00:01,  1.63it/s]Epoch: 4, train for the 98-th batch, train loss: 0.6076224446296692:  64%|████████▎    | 97/151 [00:21<00:12,  4.47it/s]Epoch: 4, train for the 98-th batch, train loss: 0.6076224446296692:  65%|████████▍    | 98/151 [00:21<00:11,  4.48it/s]Epoch: 3, train for the 22-th batch, train loss: 0.24697299301624298:  18%|██          | 21/119 [00:13<00:59,  1.64it/s]Epoch: 3, train for the 22-th batch, train loss: 0.24697299301624298:  18%|██▏         | 22/119 [00:13<00:59,  1.64it/s]Epoch: 4, train for the 99-th batch, train loss: 0.6277747750282288:  65%|████████▍    | 98/151 [00:21<00:11,  4.48it/s]Epoch: 4, train for the 99-th batch, train loss: 0.6277747750282288:  66%|████████▌    | 99/151 [00:21<00:11,  4.46it/s]Epoch: 2, train for the 44-th batch, train loss: 0.43126487731933594:  18%|██▏         | 43/237 [00:22<01:59,  1.62it/s]Epoch: 2, train for the 44-th batch, train loss: 0.43126487731933594:  19%|██▏         | 44/237 [00:22<01:59,  1.62it/s]Epoch: 1, train for the 317-th batch, train loss: 0.4500122666358948:  83%|█████████  | 316/383 [03:09<00:41,  1.61it/s]Epoch: 1, train for the 317-th batch, train loss: 0.4500122666358948:  83%|█████████  | 317/383 [03:09<00:41,  1.60it/s]Epoch: 4, train for the 100-th batch, train loss: 0.663620114326477:  66%|████████▌    | 99/151 [00:21<00:11,  4.46it/s]Epoch: 4, train for the 100-th batch, train loss: 0.663620114326477:  66%|███████▉    | 100/151 [00:21<00:11,  4.47it/s]Epoch: 2, train for the 144-th batch, train loss: 0.5284724235534668:  98%|██████████▊| 143/146 [01:27<00:01,  1.63it/s]Epoch: 2, train for the 144-th batch, train loss: 0.5284724235534668:  99%|██████████▊| 144/146 [01:27<00:01,  1.63it/s]Epoch: 4, train for the 101-th batch, train loss: 0.6733339428901672:  66%|███████▎   | 100/151 [00:21<00:11,  4.47it/s]Epoch: 4, train for the 101-th batch, train loss: 0.6733339428901672:  67%|███████▎   | 101/151 [00:21<00:11,  4.44it/s]Epoch: 3, train for the 23-th batch, train loss: 0.27760234475135803:  18%|██▏         | 22/119 [00:13<00:59,  1.64it/s]Epoch: 3, train for the 23-th batch, train loss: 0.27760234475135803:  19%|██▎         | 23/119 [00:13<00:58,  1.64it/s]Epoch: 4, train for the 102-th batch, train loss: 0.6124280691146851:  67%|███████▎   | 101/151 [00:21<00:11,  4.44it/s]Epoch: 4, train for the 102-th batch, train loss: 0.6124280691146851:  68%|███████▍   | 102/151 [00:21<00:11,  4.42it/s]Epoch: 2, train for the 45-th batch, train loss: 0.7868635654449463:  19%|██▍          | 44/237 [00:23<01:59,  1.62it/s]Epoch: 2, train for the 45-th batch, train loss: 0.7868635654449463:  19%|██▍          | 45/237 [00:23<02:00,  1.59it/s]Epoch: 1, train for the 318-th batch, train loss: 0.45219019055366516:  83%|████████▎ | 317/383 [03:09<00:41,  1.60it/s]Epoch: 1, train for the 318-th batch, train loss: 0.45219019055366516:  83%|████████▎ | 318/383 [03:09<00:40,  1.60it/s]Epoch: 4, train for the 103-th batch, train loss: 0.6566113829612732:  68%|███████▍   | 102/151 [00:22<00:11,  4.42it/s]Epoch: 4, train for the 103-th batch, train loss: 0.6566113829612732:  68%|███████▌   | 103/151 [00:22<00:10,  4.41it/s]Epoch: 2, train for the 145-th batch, train loss: 0.5531599521636963:  99%|██████████▊| 144/146 [01:28<00:01,  1.63it/s]Epoch: 2, train for the 145-th batch, train loss: 0.5531599521636963:  99%|██████████▉| 145/146 [01:28<00:00,  1.63it/s]Epoch: 3, train for the 24-th batch, train loss: 0.24911029636859894:  19%|██▎         | 23/119 [00:14<00:58,  1.64it/s]Epoch: 3, train for the 24-th batch, train loss: 0.24911029636859894:  20%|██▍         | 24/119 [00:14<00:56,  1.67it/s]Epoch: 4, train for the 104-th batch, train loss: 0.6275526285171509:  68%|███████▌   | 103/151 [00:22<00:10,  4.41it/s]Epoch: 4, train for the 104-th batch, train loss: 0.6275526285171509:  69%|███████▌   | 104/151 [00:22<00:10,  4.41it/s]Epoch: 2, train for the 46-th batch, train loss: 0.523244321346283:  19%|██▋           | 45/237 [00:24<02:00,  1.59it/s]Epoch: 2, train for the 46-th batch, train loss: 0.523244321346283:  19%|██▋           | 46/237 [00:24<02:00,  1.59it/s]Epoch: 1, train for the 319-th batch, train loss: 0.5144520401954651:  83%|█████████▏ | 318/383 [03:10<00:40,  1.60it/s]Epoch: 1, train for the 319-th batch, train loss: 0.5144520401954651:  83%|█████████▏ | 319/383 [03:10<00:40,  1.59it/s]Epoch: 4, train for the 105-th batch, train loss: 0.5727642178535461:  69%|███████▌   | 104/151 [00:22<00:10,  4.41it/s]Epoch: 4, train for the 105-th batch, train loss: 0.5727642178535461:  70%|███████▋   | 105/151 [00:22<00:10,  4.41it/s]Epoch: 2, train for the 146-th batch, train loss: 0.4907778799533844:  99%|██████████▉| 145/146 [01:28<00:00,  1.63it/s]Epoch: 2, train for the 146-th batch, train loss: 0.4907778799533844: 100%|███████████| 146/146 [01:28<00:00,  1.81it/s]Epoch: 2, train for the 146-th batch, train loss: 0.4907778799533844: 100%|███████████| 146/146 [01:28<00:00,  1.65it/s]
  0%|                                                                                            | 0/38 [00:00<?, ?it/s]Epoch: 4, train for the 106-th batch, train loss: 0.5410957932472229:  70%|███████▋   | 105/151 [00:22<00:10,  4.41it/s]Epoch: 4, train for the 106-th batch, train loss: 0.5410957932472229:  70%|███████▋   | 106/151 [00:22<00:10,  4.43it/s]Epoch: 3, train for the 25-th batch, train loss: 0.25202852487564087:  20%|██▍         | 24/119 [00:14<00:56,  1.67it/s]Epoch: 3, train for the 25-th batch, train loss: 0.25202852487564087:  21%|██▌         | 25/119 [00:14<00:56,  1.67it/s]evaluate for the 1-th batch, evaluate loss: 0.4929411709308624:   0%|                            | 0/38 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.4929411709308624:   3%|▌                   | 1/38 [00:00<00:11,  3.16it/s]Epoch: 4, train for the 107-th batch, train loss: 0.5317744612693787:  70%|███████▋   | 106/151 [00:23<00:10,  4.43it/s]Epoch: 4, train for the 107-th batch, train loss: 0.5317744612693787:  71%|███████▊   | 107/151 [00:23<00:09,  4.43it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5521107316017151:  19%|██▌          | 46/237 [00:24<02:00,  1.59it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5521107316017151:  20%|██▌          | 47/237 [00:24<01:59,  1.58it/s]Epoch: 1, train for the 320-th batch, train loss: 0.5690593123435974:  83%|█████████▏ | 319/383 [03:11<00:40,  1.59it/s]Epoch: 1, train for the 320-th batch, train loss: 0.5690593123435974:  84%|█████████▏ | 320/383 [03:11<00:39,  1.59it/s]evaluate for the 2-th batch, evaluate loss: 0.5044340491294861:   3%|▌                   | 1/38 [00:00<00:11,  3.16it/s]evaluate for the 2-th batch, evaluate loss: 0.5044340491294861:   5%|█                   | 2/38 [00:00<00:10,  3.47it/s]Epoch: 4, train for the 108-th batch, train loss: 0.5523474216461182:  71%|███████▊   | 107/151 [00:23<00:09,  4.43it/s]Epoch: 4, train for the 108-th batch, train loss: 0.5523474216461182:  72%|███████▊   | 108/151 [00:23<00:09,  4.44it/s]Epoch: 4, train for the 109-th batch, train loss: 0.5250566005706787:  72%|███████▊   | 108/151 [00:23<00:09,  4.44it/s]Epoch: 4, train for the 109-th batch, train loss: 0.5250566005706787:  72%|███████▉   | 109/151 [00:23<00:09,  4.44it/s]evaluate for the 3-th batch, evaluate loss: 0.5003705620765686:   5%|█                   | 2/38 [00:00<00:10,  3.47it/s]evaluate for the 3-th batch, evaluate loss: 0.5003705620765686:   8%|█▌                  | 3/38 [00:00<00:10,  3.41it/s]Epoch: 3, train for the 26-th batch, train loss: 0.2331034541130066:  21%|██▋          | 25/119 [00:15<00:56,  1.67it/s]Epoch: 3, train for the 26-th batch, train loss: 0.2331034541130066:  22%|██▊          | 26/119 [00:15<00:55,  1.67it/s]Epoch: 4, train for the 110-th batch, train loss: 0.556587278842926:  72%|████████▋   | 109/151 [00:23<00:09,  4.44it/s]Epoch: 4, train for the 110-th batch, train loss: 0.556587278842926:  73%|████████▋   | 110/151 [00:23<00:09,  4.43it/s]evaluate for the 4-th batch, evaluate loss: 0.4905754327774048:   8%|█▌                  | 3/38 [00:01<00:10,  3.41it/s]evaluate for the 4-th batch, evaluate loss: 0.4905754327774048:  11%|██                  | 4/38 [00:01<00:09,  3.59it/s]Epoch: 2, train for the 48-th batch, train loss: 0.5784472823143005:  20%|██▌          | 47/237 [00:25<01:59,  1.58it/s]Epoch: 2, train for the 48-th batch, train loss: 0.5784472823143005:  20%|██▋          | 48/237 [00:25<01:59,  1.58it/s]Epoch: 1, train for the 321-th batch, train loss: 0.5999305844306946:  84%|█████████▏ | 320/383 [03:11<00:39,  1.59it/s]Epoch: 1, train for the 321-th batch, train loss: 0.5999305844306946:  84%|█████████▏ | 321/383 [03:11<00:39,  1.58it/s]Epoch: 4, train for the 111-th batch, train loss: 0.5255950689315796:  73%|████████   | 110/151 [00:24<00:09,  4.43it/s]Epoch: 4, train for the 111-th batch, train loss: 0.5255950689315796:  74%|████████   | 111/151 [00:24<00:09,  4.41it/s]evaluate for the 5-th batch, evaluate loss: 0.5214248299598694:  11%|██                  | 4/38 [00:01<00:09,  3.59it/s]evaluate for the 5-th batch, evaluate loss: 0.5214248299598694:  13%|██▋                 | 5/38 [00:01<00:09,  3.52it/s]Epoch: 3, train for the 27-th batch, train loss: 0.27822577953338623:  22%|██▌         | 26/119 [00:16<00:55,  1.67it/s]Epoch: 3, train for the 27-th batch, train loss: 0.27822577953338623:  23%|██▋         | 27/119 [00:16<00:55,  1.67it/s]Epoch: 4, train for the 112-th batch, train loss: 0.5190277099609375:  74%|████████   | 111/151 [00:24<00:09,  4.41it/s]Epoch: 4, train for the 112-th batch, train loss: 0.5190277099609375:  74%|████████▏  | 112/151 [00:24<00:08,  4.41it/s]evaluate for the 6-th batch, evaluate loss: 0.5089399814605713:  13%|██▋                 | 5/38 [00:01<00:09,  3.52it/s]evaluate for the 6-th batch, evaluate loss: 0.5089399814605713:  16%|███▏                | 6/38 [00:01<00:08,  3.63it/s]Epoch: 4, train for the 113-th batch, train loss: 0.5902971029281616:  74%|████████▏  | 112/151 [00:24<00:08,  4.41it/s]Epoch: 4, train for the 113-th batch, train loss: 0.5902971029281616:  75%|████████▏  | 113/151 [00:24<00:08,  4.41it/s]Epoch: 2, train for the 49-th batch, train loss: 0.6497617363929749:  20%|██▋          | 48/237 [00:26<01:59,  1.58it/s]Epoch: 1, train for the 322-th batch, train loss: 0.5382148623466492:  84%|█████████▏ | 321/383 [03:12<00:39,  1.58it/s]Epoch: 2, train for the 49-th batch, train loss: 0.6497617363929749:  21%|██▋          | 49/237 [00:26<02:00,  1.57it/s]Epoch: 1, train for the 322-th batch, train loss: 0.5382148623466492:  84%|█████████▏ | 322/383 [03:12<00:38,  1.57it/s]evaluate for the 7-th batch, evaluate loss: 0.4767056107521057:  16%|███▏                | 6/38 [00:01<00:08,  3.63it/s]evaluate for the 7-th batch, evaluate loss: 0.4767056107521057:  18%|███▋                | 7/38 [00:01<00:08,  3.58it/s]Epoch: 4, train for the 114-th batch, train loss: 0.5116281509399414:  75%|████████▏  | 113/151 [00:24<00:08,  4.41it/s]Epoch: 4, train for the 114-th batch, train loss: 0.5116281509399414:  75%|████████▎  | 114/151 [00:24<00:08,  4.40it/s]Epoch: 3, train for the 28-th batch, train loss: 0.2711326777935028:  23%|██▉          | 27/119 [00:16<00:55,  1.67it/s]Epoch: 3, train for the 28-th batch, train loss: 0.2711326777935028:  24%|███          | 28/119 [00:16<00:54,  1.66it/s]Epoch: 4, train for the 115-th batch, train loss: 0.499699205160141:  75%|█████████   | 114/151 [00:24<00:08,  4.40it/s]Epoch: 4, train for the 115-th batch, train loss: 0.499699205160141:  76%|█████████▏  | 115/151 [00:24<00:08,  4.41it/s]evaluate for the 8-th batch, evaluate loss: 0.5536839365959167:  18%|███▋                | 7/38 [00:02<00:08,  3.58it/s]evaluate for the 8-th batch, evaluate loss: 0.5536839365959167:  21%|████▏               | 8/38 [00:02<00:08,  3.60it/s]Epoch: 4, train for the 116-th batch, train loss: 0.4584658741950989:  76%|████████▍  | 115/151 [00:25<00:08,  4.41it/s]Epoch: 4, train for the 116-th batch, train loss: 0.4584658741950989:  77%|████████▍  | 116/151 [00:25<00:07,  4.41it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5764092803001404:  21%|██▋          | 49/237 [00:26<02:00,  1.57it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5764092803001404:  21%|██▋          | 50/237 [00:26<01:58,  1.57it/s]Epoch: 1, train for the 323-th batch, train loss: 0.4534599184989929:  84%|█████████▏ | 322/383 [03:13<00:38,  1.57it/s]Epoch: 1, train for the 323-th batch, train loss: 0.4534599184989929:  84%|█████████▎ | 323/383 [03:13<00:38,  1.57it/s]evaluate for the 9-th batch, evaluate loss: 0.5203055739402771:  21%|████▏               | 8/38 [00:02<00:08,  3.60it/s]evaluate for the 9-th batch, evaluate loss: 0.5203055739402771:  24%|████▋               | 9/38 [00:02<00:08,  3.61it/s]Epoch: 4, train for the 117-th batch, train loss: 0.5309931039810181:  77%|████████▍  | 116/151 [00:25<00:07,  4.41it/s]Epoch: 4, train for the 117-th batch, train loss: 0.5309931039810181:  77%|████████▌  | 117/151 [00:25<00:07,  4.42it/s]Epoch: 3, train for the 29-th batch, train loss: 0.23160222172737122:  24%|██▊         | 28/119 [00:17<00:54,  1.66it/s]Epoch: 3, train for the 29-th batch, train loss: 0.23160222172737122:  24%|██▉         | 29/119 [00:17<00:54,  1.66it/s]evaluate for the 10-th batch, evaluate loss: 0.5299228429794312:  24%|████▌              | 9/38 [00:02<00:08,  3.61it/s]evaluate for the 10-th batch, evaluate loss: 0.5299228429794312:  26%|████▋             | 10/38 [00:02<00:07,  3.52it/s]Epoch: 4, train for the 118-th batch, train loss: 0.4906429052352905:  77%|████████▌  | 117/151 [00:25<00:07,  4.42it/s]Epoch: 4, train for the 118-th batch, train loss: 0.4906429052352905:  78%|████████▌  | 118/151 [00:25<00:07,  4.43it/s]evaluate for the 11-th batch, evaluate loss: 0.5204091668128967:  26%|████▋             | 10/38 [00:03<00:07,  3.52it/s]evaluate for the 11-th batch, evaluate loss: 0.5204091668128967:  29%|█████▏            | 11/38 [00:03<00:07,  3.65it/s]Epoch: 2, train for the 51-th batch, train loss: 0.5925113558769226:  21%|██▋          | 50/237 [00:27<01:58,  1.57it/s]Epoch: 2, train for the 51-th batch, train loss: 0.5925113558769226:  22%|██▊          | 51/237 [00:27<01:58,  1.57it/s]Epoch: 1, train for the 324-th batch, train loss: 0.386421263217926:  84%|██████████  | 323/383 [03:13<00:38,  1.57it/s]Epoch: 1, train for the 324-th batch, train loss: 0.386421263217926:  85%|██████████▏ | 324/383 [03:13<00:37,  1.57it/s]Epoch: 4, train for the 119-th batch, train loss: 0.514726996421814:  78%|█████████▍  | 118/151 [00:25<00:07,  4.43it/s]Epoch: 4, train for the 119-th batch, train loss: 0.514726996421814:  79%|█████████▍  | 119/151 [00:25<00:07,  4.43it/s]Epoch: 3, train for the 30-th batch, train loss: 0.2390853315591812:  24%|███▏         | 29/119 [00:17<00:54,  1.66it/s]Epoch: 3, train for the 30-th batch, train loss: 0.2390853315591812:  25%|███▎         | 30/119 [00:17<00:53,  1.67it/s]Epoch: 4, train for the 120-th batch, train loss: 0.6095834374427795:  79%|████████▋  | 119/151 [00:26<00:07,  4.43it/s]Epoch: 4, train for the 120-th batch, train loss: 0.6095834374427795:  79%|████████▋  | 120/151 [00:26<00:06,  4.43it/s]evaluate for the 12-th batch, evaluate loss: 0.5582813024520874:  29%|█████▏            | 11/38 [00:03<00:07,  3.65it/s]evaluate for the 12-th batch, evaluate loss: 0.5582813024520874:  32%|█████▋            | 12/38 [00:03<00:07,  3.54it/s]Epoch: 4, train for the 121-th batch, train loss: 0.5025098919868469:  79%|████████▋  | 120/151 [00:26<00:06,  4.43it/s]Epoch: 4, train for the 121-th batch, train loss: 0.5025098919868469:  80%|████████▊  | 121/151 [00:26<00:06,  4.43it/s]evaluate for the 13-th batch, evaluate loss: 0.5615404844284058:  32%|█████▋            | 12/38 [00:03<00:07,  3.54it/s]evaluate for the 13-th batch, evaluate loss: 0.5615404844284058:  34%|██████▏           | 13/38 [00:03<00:06,  3.72it/s]Epoch: 1, train for the 325-th batch, train loss: 0.4294426739215851:  85%|█████████▎ | 324/383 [03:14<00:37,  1.57it/s]Epoch: 2, train for the 52-th batch, train loss: 0.6496583223342896:  22%|██▊          | 51/237 [00:27<01:58,  1.57it/s]Epoch: 1, train for the 325-th batch, train loss: 0.4294426739215851:  85%|█████████▎ | 325/383 [03:14<00:37,  1.56it/s]Epoch: 2, train for the 52-th batch, train loss: 0.6496583223342896:  22%|██▊          | 52/237 [00:27<01:58,  1.56it/s]Epoch: 4, train for the 122-th batch, train loss: 0.5565534830093384:  80%|████████▊  | 121/151 [00:26<00:06,  4.43it/s]Epoch: 4, train for the 122-th batch, train loss: 0.5565534830093384:  81%|████████▉  | 122/151 [00:26<00:06,  4.42it/s]Epoch: 3, train for the 31-th batch, train loss: 0.21913011372089386:  25%|███         | 30/119 [00:18<00:53,  1.67it/s]Epoch: 3, train for the 31-th batch, train loss: 0.21913011372089386:  26%|███▏        | 31/119 [00:18<00:53,  1.66it/s]evaluate for the 14-th batch, evaluate loss: 0.5009059906005859:  34%|██████▏           | 13/38 [00:03<00:06,  3.72it/s]evaluate for the 14-th batch, evaluate loss: 0.5009059906005859:  37%|██████▋           | 14/38 [00:03<00:06,  3.53it/s]Epoch: 4, train for the 123-th batch, train loss: 0.5324825048446655:  81%|████████▉  | 122/151 [00:26<00:06,  4.42it/s]Epoch: 4, train for the 123-th batch, train loss: 0.5324825048446655:  81%|████████▉  | 123/151 [00:26<00:06,  4.43it/s]evaluate for the 15-th batch, evaluate loss: 0.5167515277862549:  37%|██████▋           | 14/38 [00:04<00:06,  3.53it/s]evaluate for the 15-th batch, evaluate loss: 0.5167515277862549:  39%|███████           | 15/38 [00:04<00:06,  3.60it/s]Epoch: 4, train for the 124-th batch, train loss: 0.5831758379936218:  81%|████████▉  | 123/151 [00:26<00:06,  4.43it/s]Epoch: 4, train for the 124-th batch, train loss: 0.5831758379936218:  82%|█████████  | 124/151 [00:26<00:06,  4.40it/s]Epoch: 2, train for the 53-th batch, train loss: 0.6346309781074524:  22%|██▊          | 52/237 [00:28<01:58,  1.56it/s]Epoch: 1, train for the 326-th batch, train loss: 0.4665580689907074:  85%|█████████▎ | 325/383 [03:14<00:37,  1.56it/s]Epoch: 2, train for the 53-th batch, train loss: 0.6346309781074524:  22%|██▉          | 53/237 [00:28<01:58,  1.56it/s]Epoch: 1, train for the 326-th batch, train loss: 0.4665580689907074:  85%|█████████▎ | 326/383 [03:14<00:36,  1.56it/s]Epoch: 4, train for the 125-th batch, train loss: 0.5486758947372437:  82%|█████████  | 124/151 [00:27<00:06,  4.40it/s]Epoch: 4, train for the 125-th batch, train loss: 0.5486758947372437:  83%|█████████  | 125/151 [00:27<00:05,  4.38it/s]Epoch: 3, train for the 32-th batch, train loss: 0.23369979858398438:  26%|███▏        | 31/119 [00:19<00:53,  1.66it/s]Epoch: 3, train for the 32-th batch, train loss: 0.23369979858398438:  27%|███▏        | 32/119 [00:19<00:52,  1.66it/s]evaluate for the 16-th batch, evaluate loss: 0.53416508436203:  39%|███████▉            | 15/38 [00:04<00:06,  3.60it/s]evaluate for the 16-th batch, evaluate loss: 0.53416508436203:  42%|████████▍           | 16/38 [00:04<00:06,  3.45it/s]Epoch: 4, train for the 126-th batch, train loss: 0.5308196544647217:  83%|█████████  | 125/151 [00:27<00:05,  4.38it/s]Epoch: 4, train for the 126-th batch, train loss: 0.5308196544647217:  83%|█████████▏ | 126/151 [00:27<00:05,  4.37it/s]evaluate for the 17-th batch, evaluate loss: 0.5023101568222046:  42%|███████▌          | 16/38 [00:04<00:06,  3.45it/s]evaluate for the 17-th batch, evaluate loss: 0.5023101568222046:  45%|████████          | 17/38 [00:04<00:05,  3.51it/s]Epoch: 4, train for the 127-th batch, train loss: 0.5468469262123108:  83%|█████████▏ | 126/151 [00:27<00:05,  4.37it/s]Epoch: 4, train for the 127-th batch, train loss: 0.5468469262123108:  84%|█████████▎ | 127/151 [00:27<00:05,  4.38it/s]Epoch: 2, train for the 54-th batch, train loss: 0.4720824658870697:  22%|██▉          | 53/237 [00:29<01:58,  1.56it/s]Epoch: 2, train for the 54-th batch, train loss: 0.4720824658870697:  23%|██▉          | 54/237 [00:29<01:57,  1.56it/s]Epoch: 1, train for the 327-th batch, train loss: 0.5217206478118896:  85%|█████████▎ | 326/383 [03:15<00:36,  1.56it/s]Epoch: 1, train for the 327-th batch, train loss: 0.5217206478118896:  85%|█████████▍ | 327/383 [03:15<00:35,  1.56it/s]evaluate for the 18-th batch, evaluate loss: 0.5402712821960449:  45%|████████          | 17/38 [00:05<00:05,  3.51it/s]evaluate for the 18-th batch, evaluate loss: 0.5402712821960449:  47%|████████▌         | 18/38 [00:05<00:05,  3.48it/s]Epoch: 3, train for the 33-th batch, train loss: 0.2627562880516052:  27%|███▍         | 32/119 [00:19<00:52,  1.66it/s]Epoch: 3, train for the 33-th batch, train loss: 0.2627562880516052:  28%|███▌         | 33/119 [00:19<00:51,  1.66it/s]Epoch: 4, train for the 128-th batch, train loss: 0.5782771706581116:  84%|█████████▎ | 127/151 [00:27<00:05,  4.38it/s]Epoch: 4, train for the 128-th batch, train loss: 0.5782771706581116:  85%|█████████▎ | 128/151 [00:27<00:05,  4.40it/s]evaluate for the 19-th batch, evaluate loss: 0.5135130882263184:  47%|████████▌         | 18/38 [00:05<00:05,  3.48it/s]evaluate for the 19-th batch, evaluate loss: 0.5135130882263184:  50%|█████████         | 19/38 [00:05<00:05,  3.60it/s]Epoch: 4, train for the 129-th batch, train loss: 0.5511271357536316:  85%|█████████▎ | 128/151 [00:28<00:05,  4.40it/s]Epoch: 4, train for the 129-th batch, train loss: 0.5511271357536316:  85%|█████████▍ | 129/151 [00:28<00:04,  4.41it/s]evaluate for the 20-th batch, evaluate loss: 0.5002481937408447:  50%|█████████         | 19/38 [00:05<00:05,  3.60it/s]evaluate for the 20-th batch, evaluate loss: 0.5002481937408447:  53%|█████████▍        | 20/38 [00:05<00:05,  3.56it/s]Epoch: 4, train for the 130-th batch, train loss: 0.5823072791099548:  85%|█████████▍ | 129/151 [00:28<00:04,  4.41it/s]Epoch: 4, train for the 130-th batch, train loss: 0.5823072791099548:  86%|█████████▍ | 130/151 [00:28<00:04,  4.38it/s]Epoch: 2, train for the 55-th batch, train loss: 0.4748757481575012:  23%|██▉          | 54/237 [00:29<01:57,  1.56it/s]Epoch: 2, train for the 55-th batch, train loss: 0.4748757481575012:  23%|███          | 55/237 [00:29<01:55,  1.58it/s]Epoch: 1, train for the 328-th batch, train loss: 0.5205866098403931:  85%|█████████▍ | 327/383 [03:16<00:35,  1.56it/s]Epoch: 1, train for the 328-th batch, train loss: 0.5205866098403931:  86%|█████████▍ | 328/383 [03:16<00:35,  1.57it/s]Epoch: 3, train for the 34-th batch, train loss: 0.2576231062412262:  28%|███▌         | 33/119 [00:20<00:51,  1.66it/s]Epoch: 3, train for the 34-th batch, train loss: 0.2576231062412262:  29%|███▋         | 34/119 [00:20<00:51,  1.66it/s]Epoch: 4, train for the 131-th batch, train loss: 0.5301355123519897:  86%|█████████▍ | 130/151 [00:28<00:04,  4.38it/s]Epoch: 4, train for the 131-th batch, train loss: 0.5301355123519897:  87%|█████████▌ | 131/151 [00:28<00:04,  4.38it/s]evaluate for the 21-th batch, evaluate loss: 0.5119953155517578:  53%|█████████▍        | 20/38 [00:05<00:05,  3.56it/s]evaluate for the 21-th batch, evaluate loss: 0.5119953155517578:  55%|█████████▉        | 21/38 [00:05<00:04,  3.63it/s]Epoch: 4, train for the 132-th batch, train loss: 0.5348234176635742:  87%|█████████▌ | 131/151 [00:28<00:04,  4.38it/s]Epoch: 4, train for the 132-th batch, train loss: 0.5348234176635742:  87%|█████████▌ | 132/151 [00:28<00:04,  4.41it/s]evaluate for the 22-th batch, evaluate loss: 0.5297800302505493:  55%|█████████▉        | 21/38 [00:06<00:04,  3.63it/s]evaluate for the 22-th batch, evaluate loss: 0.5297800302505493:  58%|██████████▍       | 22/38 [00:06<00:04,  3.59it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5599279403686523:  23%|███          | 55/237 [00:30<01:55,  1.58it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5599279403686523:  24%|███          | 56/237 [00:30<01:55,  1.57it/s]Epoch: 1, train for the 329-th batch, train loss: 0.44099509716033936:  86%|████████▌ | 328/383 [03:16<00:35,  1.57it/s]Epoch: 1, train for the 329-th batch, train loss: 0.44099509716033936:  86%|████████▌ | 329/383 [03:16<00:34,  1.57it/s]Epoch: 4, train for the 133-th batch, train loss: 0.5531322956085205:  87%|█████████▌ | 132/151 [00:29<00:04,  4.41it/s]Epoch: 4, train for the 133-th batch, train loss: 0.5531322956085205:  88%|█████████▋ | 133/151 [00:29<00:04,  4.44it/s]Epoch: 3, train for the 35-th batch, train loss: 0.18500077724456787:  29%|███▍        | 34/119 [00:21<00:51,  1.66it/s]Epoch: 3, train for the 35-th batch, train loss: 0.18500077724456787:  29%|███▌        | 35/119 [00:21<00:50,  1.66it/s]evaluate for the 23-th batch, evaluate loss: 0.525071382522583:  58%|███████████        | 22/38 [00:06<00:04,  3.59it/s]evaluate for the 23-th batch, evaluate loss: 0.525071382522583:  61%|███████████▌       | 23/38 [00:06<00:04,  3.54it/s]Epoch: 4, train for the 134-th batch, train loss: 0.5510439872741699:  88%|█████████▋ | 133/151 [00:29<00:04,  4.44it/s]Epoch: 4, train for the 134-th batch, train loss: 0.5510439872741699:  89%|█████████▊ | 134/151 [00:29<00:03,  4.45it/s]evaluate for the 24-th batch, evaluate loss: 0.5125852227210999:  61%|██████████▉       | 23/38 [00:06<00:04,  3.54it/s]evaluate for the 24-th batch, evaluate loss: 0.5125852227210999:  63%|███████████▎      | 24/38 [00:06<00:03,  3.56it/s]Epoch: 4, train for the 135-th batch, train loss: 0.5505297780036926:  89%|█████████▊ | 134/151 [00:29<00:03,  4.45it/s]Epoch: 4, train for the 135-th batch, train loss: 0.5505297780036926:  89%|█████████▊ | 135/151 [00:29<00:03,  4.47it/s]Epoch: 3, train for the 36-th batch, train loss: 0.2057214379310608:  29%|███▊         | 35/119 [00:21<00:50,  1.66it/s]Epoch: 3, train for the 36-th batch, train loss: 0.2057214379310608:  30%|███▉         | 36/119 [00:21<00:50,  1.66it/s]Epoch: 2, train for the 57-th batch, train loss: 0.47568202018737793:  24%|██▊         | 56/237 [00:31<01:55,  1.57it/s]Epoch: 2, train for the 57-th batch, train loss: 0.47568202018737793:  24%|██▉         | 57/237 [00:31<01:54,  1.58it/s]Epoch: 1, train for the 330-th batch, train loss: 0.44238561391830444:  86%|████████▌ | 329/383 [03:17<00:34,  1.57it/s]Epoch: 1, train for the 330-th batch, train loss: 0.44238561391830444:  86%|████████▌ | 330/383 [03:17<00:33,  1.58it/s]Epoch: 4, train for the 136-th batch, train loss: 0.5553163290023804:  89%|█████████▊ | 135/151 [00:29<00:03,  4.47it/s]Epoch: 4, train for the 136-th batch, train loss: 0.5553163290023804:  90%|█████████▉ | 136/151 [00:29<00:03,  4.47it/s]evaluate for the 25-th batch, evaluate loss: 0.5148359537124634:  63%|███████████▎      | 24/38 [00:07<00:03,  3.56it/s]evaluate for the 25-th batch, evaluate loss: 0.5148359537124634:  66%|███████████▊      | 25/38 [00:07<00:03,  3.49it/s]Epoch: 4, train for the 137-th batch, train loss: 0.5955829620361328:  90%|█████████▉ | 136/151 [00:29<00:03,  4.47it/s]Epoch: 4, train for the 137-th batch, train loss: 0.5955829620361328:  91%|█████████▉ | 137/151 [00:29<00:03,  4.48it/s]evaluate for the 26-th batch, evaluate loss: 0.5167114734649658:  66%|███████████▊      | 25/38 [00:07<00:03,  3.49it/s]evaluate for the 26-th batch, evaluate loss: 0.5167114734649658:  68%|████████████▎     | 26/38 [00:07<00:03,  3.65it/s]Epoch: 4, train for the 138-th batch, train loss: 0.6228914260864258:  91%|█████████▉ | 137/151 [00:30<00:03,  4.48it/s]Epoch: 4, train for the 138-th batch, train loss: 0.6228914260864258:  91%|██████████ | 138/151 [00:30<00:02,  4.49it/s]Epoch: 3, train for the 37-th batch, train loss: 0.25987789034843445:  30%|███▋        | 36/119 [00:22<00:50,  1.66it/s]Epoch: 3, train for the 37-th batch, train loss: 0.25987789034843445:  31%|███▋        | 37/119 [00:22<00:49,  1.66it/s]evaluate for the 27-th batch, evaluate loss: 0.5433624982833862:  68%|████████████▎     | 26/38 [00:07<00:03,  3.65it/s]evaluate for the 27-th batch, evaluate loss: 0.5433624982833862:  71%|████████████▊     | 27/38 [00:07<00:03,  3.54it/s]Epoch: 1, train for the 331-th batch, train loss: 0.4714483618736267:  86%|█████████▍ | 330/383 [03:18<00:33,  1.58it/s]Epoch: 1, train for the 331-th batch, train loss: 0.4714483618736267:  86%|█████████▌ | 331/383 [03:18<00:33,  1.57it/s]Epoch: 2, train for the 58-th batch, train loss: 0.6689483523368835:  24%|███▏         | 57/237 [00:31<01:54,  1.58it/s]Epoch: 2, train for the 58-th batch, train loss: 0.6689483523368835:  24%|███▏         | 58/237 [00:31<01:54,  1.57it/s]Epoch: 4, train for the 139-th batch, train loss: 0.5735911726951599:  91%|██████████ | 138/151 [00:30<00:02,  4.49it/s]Epoch: 4, train for the 139-th batch, train loss: 0.5735911726951599:  92%|██████████▏| 139/151 [00:30<00:02,  4.51it/s]evaluate for the 28-th batch, evaluate loss: 0.551498532295227:  71%|█████████████▌     | 27/38 [00:07<00:03,  3.54it/s]evaluate for the 28-th batch, evaluate loss: 0.551498532295227:  74%|██████████████     | 28/38 [00:07<00:02,  3.70it/s]Epoch: 4, train for the 140-th batch, train loss: 0.4989491403102875:  92%|██████████▏| 139/151 [00:30<00:02,  4.51it/s]Epoch: 4, train for the 140-th batch, train loss: 0.4989491403102875:  93%|██████████▏| 140/151 [00:30<00:02,  4.51it/s]Epoch: 4, train for the 141-th batch, train loss: 0.5512169599533081:  93%|██████████▏| 140/151 [00:30<00:02,  4.51it/s]Epoch: 4, train for the 141-th batch, train loss: 0.5512169599533081:  93%|██████████▎| 141/151 [00:30<00:02,  4.50it/s]Epoch: 3, train for the 38-th batch, train loss: 0.2375601977109909:  31%|████         | 37/119 [00:22<00:49,  1.66it/s]Epoch: 3, train for the 38-th batch, train loss: 0.2375601977109909:  32%|████▏        | 38/119 [00:22<00:49,  1.65it/s]evaluate for the 29-th batch, evaluate loss: 0.5222151279449463:  74%|█████████████▎    | 28/38 [00:08<00:02,  3.70it/s]evaluate for the 29-th batch, evaluate loss: 0.5222151279449463:  76%|█████████████▋    | 29/38 [00:08<00:02,  3.51it/s]Epoch: 1, train for the 332-th batch, train loss: 0.4889809191226959:  86%|█████████▌ | 331/383 [03:18<00:33,  1.57it/s]Epoch: 1, train for the 332-th batch, train loss: 0.4889809191226959:  87%|█████████▌ | 332/383 [03:18<00:32,  1.56it/s]Epoch: 2, train for the 59-th batch, train loss: 0.6953228712081909:  24%|███▏         | 58/237 [00:32<01:54,  1.57it/s]Epoch: 2, train for the 59-th batch, train loss: 0.6953228712081909:  25%|███▏         | 59/237 [00:32<01:54,  1.56it/s]Epoch: 4, train for the 142-th batch, train loss: 0.5605131387710571:  93%|██████████▎| 141/151 [00:31<00:02,  4.50it/s]Epoch: 4, train for the 142-th batch, train loss: 0.5605131387710571:  94%|██████████▎| 142/151 [00:31<00:02,  4.50it/s]evaluate for the 30-th batch, evaluate loss: 0.5537874698638916:  76%|█████████████▋    | 29/38 [00:08<00:02,  3.51it/s]evaluate for the 30-th batch, evaluate loss: 0.5537874698638916:  79%|██████████████▏   | 30/38 [00:08<00:02,  3.57it/s]Epoch: 4, train for the 143-th batch, train loss: 0.49700629711151123:  94%|█████████▍| 142/151 [00:31<00:02,  4.50it/s]Epoch: 4, train for the 143-th batch, train loss: 0.49700629711151123:  95%|█████████▍| 143/151 [00:31<00:01,  4.47it/s]evaluate for the 31-th batch, evaluate loss: 0.537586510181427:  79%|███████████████    | 30/38 [00:08<00:02,  3.57it/s]evaluate for the 31-th batch, evaluate loss: 0.537586510181427:  82%|███████████████▌   | 31/38 [00:08<00:02,  3.49it/s]Epoch: 3, train for the 39-th batch, train loss: 0.22723647952079773:  32%|███▊        | 38/119 [00:23<00:49,  1.65it/s]Epoch: 3, train for the 39-th batch, train loss: 0.22723647952079773:  33%|███▉        | 39/119 [00:23<00:48,  1.66it/s]Epoch: 4, train for the 144-th batch, train loss: 0.4982169270515442:  95%|██████████▍| 143/151 [00:31<00:01,  4.47it/s]Epoch: 4, train for the 144-th batch, train loss: 0.4982169270515442:  95%|██████████▍| 144/151 [00:31<00:01,  4.48it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5363931655883789:  25%|███▏         | 59/237 [00:33<01:54,  1.56it/s]Epoch: 1, train for the 333-th batch, train loss: 0.4710444509983063:  87%|█████████▌ | 332/383 [03:19<00:32,  1.56it/s]Epoch: 1, train for the 333-th batch, train loss: 0.4710444509983063:  87%|█████████▌ | 333/383 [03:19<00:32,  1.56it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5363931655883789:  25%|███▎         | 60/237 [00:33<01:53,  1.56it/s]evaluate for the 32-th batch, evaluate loss: 0.5077950954437256:  82%|██████████████▋   | 31/38 [00:08<00:02,  3.49it/s]evaluate for the 32-th batch, evaluate loss: 0.5077950954437256:  84%|███████████████▏  | 32/38 [00:08<00:01,  3.62it/s]Epoch: 4, train for the 145-th batch, train loss: 0.5181227922439575:  95%|██████████▍| 144/151 [00:31<00:01,  4.48it/s]Epoch: 4, train for the 145-th batch, train loss: 0.5181227922439575:  96%|██████████▌| 145/151 [00:31<00:01,  4.48it/s]Epoch: 4, train for the 146-th batch, train loss: 0.5527381300926208:  96%|██████████▌| 145/151 [00:31<00:01,  4.48it/s]Epoch: 4, train for the 146-th batch, train loss: 0.5527381300926208:  97%|██████████▋| 146/151 [00:31<00:01,  4.48it/s]evaluate for the 33-th batch, evaluate loss: 0.5093103647232056:  84%|███████████████▏  | 32/38 [00:09<00:01,  3.62it/s]evaluate for the 33-th batch, evaluate loss: 0.5093103647232056:  87%|███████████████▋  | 33/38 [00:09<00:01,  3.55it/s]Epoch: 3, train for the 40-th batch, train loss: 0.24363917112350464:  33%|███▉        | 39/119 [00:24<00:48,  1.66it/s]Epoch: 3, train for the 40-th batch, train loss: 0.24363917112350464:  34%|████        | 40/119 [00:24<00:47,  1.66it/s]evaluate for the 34-th batch, evaluate loss: 0.5337457060813904:  87%|███████████████▋  | 33/38 [00:09<00:01,  3.55it/s]evaluate for the 34-th batch, evaluate loss: 0.5337457060813904:  89%|████████████████  | 34/38 [00:09<00:01,  3.65it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5182295441627502:  25%|███▎         | 60/237 [00:33<01:53,  1.56it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5182295441627502:  26%|███▎         | 61/237 [00:33<01:52,  1.56it/s]Epoch: 1, train for the 334-th batch, train loss: 0.5071823596954346:  87%|█████████▌ | 333/383 [03:20<00:32,  1.56it/s]Epoch: 1, train for the 334-th batch, train loss: 0.5071823596954346:  87%|█████████▌ | 334/383 [03:20<00:31,  1.56it/s]Epoch: 4, train for the 147-th batch, train loss: 0.5675049424171448:  97%|██████████▋| 146/151 [00:32<00:01,  4.48it/s]Epoch: 4, train for the 147-th batch, train loss: 0.5675049424171448:  97%|██████████▋| 147/151 [00:32<00:01,  3.60it/s]evaluate for the 35-th batch, evaluate loss: 0.560782253742218:  89%|█████████████████  | 34/38 [00:09<00:01,  3.65it/s]evaluate for the 35-th batch, evaluate loss: 0.560782253742218:  92%|█████████████████▌ | 35/38 [00:09<00:00,  3.60it/s]Epoch: 4, train for the 148-th batch, train loss: 0.5678777098655701:  97%|██████████▋| 147/151 [00:32<00:01,  3.60it/s]Epoch: 4, train for the 148-th batch, train loss: 0.5678777098655701:  98%|██████████▊| 148/151 [00:32<00:00,  3.83it/s]Epoch: 3, train for the 41-th batch, train loss: 0.2546543776988983:  34%|████▎        | 40/119 [00:24<00:47,  1.66it/s]Epoch: 3, train for the 41-th batch, train loss: 0.2546543776988983:  34%|████▍        | 41/119 [00:24<00:47,  1.66it/s]Epoch: 4, train for the 149-th batch, train loss: 0.5290225744247437:  98%|██████████▊| 148/151 [00:32<00:00,  3.83it/s]Epoch: 4, train for the 149-th batch, train loss: 0.5290225744247437:  99%|██████████▊| 149/151 [00:32<00:00,  4.01it/s]evaluate for the 36-th batch, evaluate loss: 0.5804898142814636:  92%|████████████████▌ | 35/38 [00:10<00:00,  3.60it/s]evaluate for the 36-th batch, evaluate loss: 0.5804898142814636:  95%|█████████████████ | 36/38 [00:10<00:00,  3.60it/s]Epoch: 2, train for the 62-th batch, train loss: 0.4190015196800232:  26%|███▎         | 61/237 [00:34<01:52,  1.56it/s]Epoch: 2, train for the 62-th batch, train loss: 0.4190015196800232:  26%|███▍         | 62/237 [00:34<01:51,  1.58it/s]Epoch: 1, train for the 335-th batch, train loss: 0.5226516127586365:  87%|█████████▌ | 334/383 [03:20<00:31,  1.56it/s]Epoch: 1, train for the 335-th batch, train loss: 0.5226516127586365:  87%|█████████▌ | 335/383 [03:20<00:30,  1.56it/s]Epoch: 4, train for the 150-th batch, train loss: 0.5016671419143677:  99%|██████████▊| 149/151 [00:32<00:00,  4.01it/s]Epoch: 4, train for the 150-th batch, train loss: 0.5016671419143677:  99%|██████████▉| 150/151 [00:32<00:00,  4.14it/s]evaluate for the 37-th batch, evaluate loss: 0.5186165571212769:  95%|█████████████████ | 36/38 [00:10<00:00,  3.60it/s]evaluate for the 37-th batch, evaluate loss: 0.5186165571212769:  97%|█████████████████▌| 37/38 [00:10<00:00,  3.61it/s]Epoch: 4, train for the 151-th batch, train loss: 0.6100975871086121:  99%|██████████▉| 150/151 [00:33<00:00,  4.14it/s]Epoch: 4, train for the 151-th batch, train loss: 0.6100975871086121: 100%|███████████| 151/151 [00:33<00:00,  4.65it/s]Epoch: 4, train for the 151-th batch, train loss: 0.6100975871086121: 100%|███████████| 151/151 [00:33<00:00,  4.56it/s]
  0%|                                                                                            | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.4933125674724579:   0%|                            | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.4933125674724579:   2%|▍                   | 1/46 [00:00<00:04,  9.67it/s]Epoch: 3, train for the 42-th batch, train loss: 0.2518240809440613:  34%|████▍        | 41/119 [00:25<00:47,  1.66it/s]Epoch: 3, train for the 42-th batch, train loss: 0.2518240809440613:  35%|████▌        | 42/119 [00:25<00:46,  1.66it/s]evaluate for the 2-th batch, evaluate loss: 0.5039770603179932:   2%|▍                   | 1/46 [00:00<00:04,  9.67it/s]evaluate for the 2-th batch, evaluate loss: 0.5039770603179932:   4%|▊                   | 2/46 [00:00<00:04,  9.62it/s]evaluate for the 38-th batch, evaluate loss: 0.517068088054657:  97%|██████████████████▌| 37/38 [00:10<00:00,  3.61it/s]evaluate for the 38-th batch, evaluate loss: 0.517068088054657: 100%|███████████████████| 38/38 [00:10<00:00,  3.54it/s]evaluate for the 38-th batch, evaluate loss: 0.517068088054657: 100%|███████████████████| 38/38 [00:10<00:00,  3.56it/s]
  0%|                                                                                            | 0/20 [00:00<?, ?it/s]Epoch: 1, train for the 336-th batch, train loss: 0.4827224314212799:  87%|█████████▌ | 335/383 [03:21<00:30,  1.56it/s]Epoch: 1, train for the 336-th batch, train loss: 0.4827224314212799:  88%|█████████▋ | 336/383 [03:21<00:28,  1.65it/s]evaluate for the 3-th batch, evaluate loss: 0.47593268752098083:   4%|▊                  | 2/46 [00:00<00:04,  9.62it/s]evaluate for the 3-th batch, evaluate loss: 0.47593268752098083:   7%|█▏                 | 3/46 [00:00<00:04,  9.61it/s]evaluate for the 4-th batch, evaluate loss: 0.5146555304527283:   7%|█▎                  | 3/46 [00:00<00:04,  9.61it/s]evaluate for the 4-th batch, evaluate loss: 0.5146555304527283:   9%|█▋                  | 4/46 [00:00<00:04,  9.61it/s]Epoch: 1, train for the 337-th batch, train loss: 0.5426499247550964:  88%|█████████▋ | 336/383 [03:21<00:28,  1.65it/s]Epoch: 1, train for the 337-th batch, train loss: 0.5426499247550964:  88%|█████████▋ | 337/383 [03:21<00:22,  2.04it/s]evaluate for the 1-th batch, evaluate loss: 0.5740106701850891:   0%|                            | 0/20 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.5740106701850891:   5%|█                   | 1/20 [00:00<00:04,  3.91it/s]evaluate for the 5-th batch, evaluate loss: 0.4755100607872009:   9%|█▋                  | 4/46 [00:00<00:04,  9.61it/s]evaluate for the 5-th batch, evaluate loss: 0.4755100607872009:  11%|██▏                 | 5/46 [00:00<00:04,  9.61it/s]Epoch: 2, train for the 63-th batch, train loss: 0.5488224625587463:  26%|███▍         | 62/237 [00:35<01:51,  1.58it/s]Epoch: 2, train for the 63-th batch, train loss: 0.5488224625587463:  27%|███▍         | 63/237 [00:35<02:04,  1.40it/s]evaluate for the 6-th batch, evaluate loss: 0.5556238293647766:  11%|██▏                 | 5/46 [00:00<00:04,  9.61it/s]evaluate for the 6-th batch, evaluate loss: 0.5556238293647766:  13%|██▌                 | 6/46 [00:00<00:04,  9.60it/s]Epoch: 3, train for the 43-th batch, train loss: 0.25801360607147217:  35%|████▏       | 42/119 [00:25<00:46,  1.66it/s]Epoch: 3, train for the 43-th batch, train loss: 0.25801360607147217:  36%|████▎       | 43/119 [00:25<00:45,  1.68it/s]evaluate for the 7-th batch, evaluate loss: 0.5049911141395569:  13%|██▌                 | 6/46 [00:00<00:04,  9.60it/s]evaluate for the 7-th batch, evaluate loss: 0.5049911141395569:  15%|███                 | 7/46 [00:00<00:04,  9.62it/s]evaluate for the 2-th batch, evaluate loss: 0.6250505447387695:   5%|█                   | 1/20 [00:00<00:04,  3.91it/s]evaluate for the 2-th batch, evaluate loss: 0.6250505447387695:  10%|██                  | 2/20 [00:00<00:05,  3.49it/s]evaluate for the 8-th batch, evaluate loss: 0.5672540664672852:  15%|███                 | 7/46 [00:00<00:04,  9.62it/s]evaluate for the 8-th batch, evaluate loss: 0.5672540664672852:  17%|███▍                | 8/46 [00:00<00:03,  9.61it/s]evaluate for the 9-th batch, evaluate loss: 0.5231480002403259:  17%|███▍                | 8/46 [00:00<00:03,  9.61it/s]evaluate for the 9-th batch, evaluate loss: 0.5231480002403259:  20%|███▉                | 9/46 [00:00<00:03,  9.62it/s]Epoch: 1, train for the 338-th batch, train loss: 0.415850967168808:  88%|██████████▌ | 337/383 [03:21<00:22,  2.04it/s]Epoch: 1, train for the 338-th batch, train loss: 0.415850967168808:  88%|██████████▌ | 338/383 [03:21<00:22,  2.02it/s]evaluate for the 10-th batch, evaluate loss: 0.5378377437591553:  20%|███▋               | 9/46 [00:01<00:03,  9.62it/s]evaluate for the 10-th batch, evaluate loss: 0.5378377437591553:  22%|███▉              | 10/46 [00:01<00:03,  9.59it/s]evaluate for the 3-th batch, evaluate loss: 0.606069028377533:  10%|██                   | 2/20 [00:00<00:05,  3.49it/s]evaluate for the 3-th batch, evaluate loss: 0.606069028377533:  15%|███▏                 | 3/20 [00:00<00:04,  3.66it/s]evaluate for the 11-th batch, evaluate loss: 0.5248579382896423:  22%|███▉              | 10/46 [00:01<00:03,  9.59it/s]evaluate for the 11-th batch, evaluate loss: 0.5248579382896423:  24%|████▎             | 11/46 [00:01<00:03,  9.59it/s]Epoch: 2, train for the 64-th batch, train loss: 0.6727500557899475:  27%|███▍         | 63/237 [00:35<02:04,  1.40it/s]Epoch: 2, train for the 64-th batch, train loss: 0.6727500557899475:  27%|███▌         | 64/237 [00:35<01:55,  1.49it/s]evaluate for the 12-th batch, evaluate loss: 0.4715937674045563:  24%|████▎             | 11/46 [00:01<00:03,  9.59it/s]evaluate for the 12-th batch, evaluate loss: 0.4715937674045563:  26%|████▋             | 12/46 [00:01<00:03,  9.59it/s]Epoch: 3, train for the 44-th batch, train loss: 0.2326127141714096:  36%|████▋        | 43/119 [00:26<00:45,  1.68it/s]Epoch: 3, train for the 44-th batch, train loss: 0.2326127141714096:  37%|████▊        | 44/119 [00:26<00:44,  1.70it/s]evaluate for the 4-th batch, evaluate loss: 0.5783980488777161:  15%|███                 | 3/20 [00:01<00:04,  3.66it/s]evaluate for the 4-th batch, evaluate loss: 0.5783980488777161:  20%|████                | 4/20 [00:01<00:04,  3.48it/s]evaluate for the 13-th batch, evaluate loss: 0.49777448177337646:  26%|████▍            | 12/46 [00:01<00:03,  9.59it/s]evaluate for the 13-th batch, evaluate loss: 0.49777448177337646:  28%|████▊            | 13/46 [00:01<00:03,  9.61it/s]evaluate for the 14-th batch, evaluate loss: 0.5936693549156189:  28%|█████             | 13/46 [00:01<00:03,  9.61it/s]evaluate for the 14-th batch, evaluate loss: 0.5936693549156189:  30%|█████▍            | 14/46 [00:01<00:03,  9.56it/s]evaluate for the 15-th batch, evaluate loss: 0.5498532056808472:  30%|█████▍            | 14/46 [00:01<00:03,  9.56it/s]evaluate for the 15-th batch, evaluate loss: 0.5498532056808472:  33%|█████▊            | 15/46 [00:01<00:03,  9.60it/s]Epoch: 1, train for the 339-th batch, train loss: 0.44337373971939087:  88%|████████▊ | 338/383 [03:22<00:22,  2.02it/s]Epoch: 1, train for the 339-th batch, train loss: 0.44337373971939087:  89%|████████▊ | 339/383 [03:22<00:23,  1.89it/s]evaluate for the 5-th batch, evaluate loss: 0.600957989692688:  20%|████▏                | 4/20 [00:01<00:04,  3.48it/s]evaluate for the 5-th batch, evaluate loss: 0.600957989692688:  25%|█████▎               | 5/20 [00:01<00:04,  3.63it/s]evaluate for the 16-th batch, evaluate loss: 0.577333390712738:  33%|██████▏            | 15/46 [00:01<00:03,  9.60it/s]evaluate for the 16-th batch, evaluate loss: 0.577333390712738:  35%|██████▌            | 16/46 [00:01<00:03,  9.65it/s]Epoch: 2, train for the 65-th batch, train loss: 0.6657754182815552:  27%|███▌         | 64/237 [00:36<01:55,  1.49it/s]Epoch: 2, train for the 65-th batch, train loss: 0.6657754182815552:  27%|███▌         | 65/237 [00:36<01:50,  1.55it/s]evaluate for the 17-th batch, evaluate loss: 0.45493242144584656:  35%|█████▉           | 16/46 [00:01<00:03,  9.65it/s]evaluate for the 17-th batch, evaluate loss: 0.45493242144584656:  37%|██████▎          | 17/46 [00:01<00:03,  9.65it/s]Epoch: 3, train for the 45-th batch, train loss: 0.19731801748275757:  37%|████▍       | 44/119 [00:26<00:44,  1.70it/s]Epoch: 3, train for the 45-th batch, train loss: 0.19731801748275757:  38%|████▌       | 45/119 [00:26<00:43,  1.71it/s]evaluate for the 18-th batch, evaluate loss: 0.5051013827323914:  37%|██████▋           | 17/46 [00:01<00:03,  9.65it/s]evaluate for the 18-th batch, evaluate loss: 0.5051013827323914:  39%|███████           | 18/46 [00:01<00:02,  9.65it/s]evaluate for the 6-th batch, evaluate loss: 0.6095791459083557:  25%|█████               | 5/20 [00:01<00:04,  3.63it/s]evaluate for the 6-th batch, evaluate loss: 0.6095791459083557:  30%|██████              | 6/20 [00:01<00:04,  3.48it/s]evaluate for the 19-th batch, evaluate loss: 0.5219577550888062:  39%|███████           | 18/46 [00:01<00:02,  9.65it/s]evaluate for the 19-th batch, evaluate loss: 0.5219577550888062:  41%|███████▍          | 19/46 [00:01<00:02,  9.64it/s]evaluate for the 20-th batch, evaluate loss: 0.5421518683433533:  41%|███████▍          | 19/46 [00:02<00:02,  9.64it/s]evaluate for the 20-th batch, evaluate loss: 0.5421518683433533:  43%|███████▊          | 20/46 [00:02<00:02,  9.63it/s]evaluate for the 7-th batch, evaluate loss: 0.6590059995651245:  30%|██████              | 6/20 [00:01<00:04,  3.48it/s]evaluate for the 7-th batch, evaluate loss: 0.6590059995651245:  35%|███████             | 7/20 [00:01<00:03,  3.64it/s]evaluate for the 21-th batch, evaluate loss: 0.52576744556427:  43%|████████▋           | 20/46 [00:02<00:02,  9.63it/s]evaluate for the 21-th batch, evaluate loss: 0.52576744556427:  46%|█████████▏          | 21/46 [00:02<00:02,  9.62it/s]Epoch: 1, train for the 340-th batch, train loss: 0.5064040422439575:  89%|█████████▋ | 339/383 [03:23<00:23,  1.89it/s]Epoch: 1, train for the 340-th batch, train loss: 0.5064040422439575:  89%|█████████▊ | 340/383 [03:23<00:23,  1.79it/s]evaluate for the 22-th batch, evaluate loss: 0.5195450186729431:  46%|████████▏         | 21/46 [00:02<00:02,  9.62it/s]evaluate for the 22-th batch, evaluate loss: 0.5195450186729431:  48%|████████▌         | 22/46 [00:02<00:02,  9.62it/s]Epoch: 2, train for the 66-th batch, train loss: 0.549639105796814:  27%|███▊          | 65/237 [00:36<01:50,  1.55it/s]Epoch: 2, train for the 66-th batch, train loss: 0.549639105796814:  28%|███▉          | 66/237 [00:36<01:47,  1.59it/s]evaluate for the 23-th batch, evaluate loss: 0.4730869233608246:  48%|████████▌         | 22/46 [00:02<00:02,  9.62it/s]evaluate for the 23-th batch, evaluate loss: 0.4730869233608246:  50%|█████████         | 23/46 [00:02<00:02,  9.64it/s]Epoch: 3, train for the 46-th batch, train loss: 0.23724299669265747:  38%|████▌       | 45/119 [00:27<00:43,  1.71it/s]Epoch: 3, train for the 46-th batch, train loss: 0.23724299669265747:  39%|████▋       | 46/119 [00:27<00:42,  1.71it/s]evaluate for the 8-th batch, evaluate loss: 0.6441378593444824:  35%|███████             | 7/20 [00:02<00:03,  3.64it/s]evaluate for the 8-th batch, evaluate loss: 0.6441378593444824:  40%|████████            | 8/20 [00:02<00:03,  3.49it/s]evaluate for the 24-th batch, evaluate loss: 0.4870597720146179:  50%|█████████         | 23/46 [00:02<00:02,  9.64it/s]evaluate for the 24-th batch, evaluate loss: 0.4870597720146179:  52%|█████████▍        | 24/46 [00:02<00:02,  9.66it/s]evaluate for the 25-th batch, evaluate loss: 0.5372552871704102:  52%|█████████▍        | 24/46 [00:02<00:02,  9.66it/s]evaluate for the 25-th batch, evaluate loss: 0.5372552871704102:  54%|█████████▊        | 25/46 [00:02<00:02,  9.67it/s]evaluate for the 26-th batch, evaluate loss: 0.5617791414260864:  54%|█████████▊        | 25/46 [00:02<00:02,  9.67it/s]evaluate for the 26-th batch, evaluate loss: 0.5617791414260864:  57%|██████████▏       | 26/46 [00:02<00:02,  9.69it/s]evaluate for the 9-th batch, evaluate loss: 0.5909506678581238:  40%|████████            | 8/20 [00:02<00:03,  3.49it/s]evaluate for the 9-th batch, evaluate loss: 0.5909506678581238:  45%|█████████           | 9/20 [00:02<00:03,  3.65it/s]evaluate for the 27-th batch, evaluate loss: 0.49689623713493347:  57%|█████████▌       | 26/46 [00:02<00:02,  9.69it/s]evaluate for the 27-th batch, evaluate loss: 0.49689623713493347:  59%|█████████▉       | 27/46 [00:02<00:01,  9.69it/s]Epoch: 1, train for the 341-th batch, train loss: 0.44258397817611694:  89%|████████▉ | 340/383 [03:23<00:23,  1.79it/s]Epoch: 1, train for the 341-th batch, train loss: 0.44258397817611694:  89%|████████▉ | 341/383 [03:23<00:23,  1.75it/s]evaluate for the 28-th batch, evaluate loss: 0.5235949158668518:  59%|██████████▌       | 27/46 [00:02<00:01,  9.69it/s]evaluate for the 28-th batch, evaluate loss: 0.5235949158668518:  61%|██████████▉       | 28/46 [00:02<00:01,  9.55it/s]Epoch: 2, train for the 67-th batch, train loss: 0.6470739841461182:  28%|███▌         | 66/237 [00:37<01:47,  1.59it/s]Epoch: 2, train for the 67-th batch, train loss: 0.6470739841461182:  28%|███▋         | 67/237 [00:37<01:45,  1.61it/s]Epoch: 3, train for the 47-th batch, train loss: 0.26281940937042236:  39%|████▋       | 46/119 [00:28<00:42,  1.71it/s]Epoch: 3, train for the 47-th batch, train loss: 0.26281940937042236:  39%|████▋       | 47/119 [00:28<00:42,  1.70it/s]evaluate for the 29-th batch, evaluate loss: 0.4894382357597351:  61%|██████████▉       | 28/46 [00:03<00:01,  9.55it/s]evaluate for the 29-th batch, evaluate loss: 0.4894382357597351:  63%|███████████▎      | 29/46 [00:03<00:01,  9.49it/s]evaluate for the 10-th batch, evaluate loss: 0.6021806001663208:  45%|████████▌          | 9/20 [00:02<00:03,  3.65it/s]evaluate for the 10-th batch, evaluate loss: 0.6021806001663208:  50%|█████████         | 10/20 [00:02<00:02,  3.46it/s]evaluate for the 30-th batch, evaluate loss: 0.4991171061992645:  63%|███████████▎      | 29/46 [00:03<00:01,  9.49it/s]evaluate for the 30-th batch, evaluate loss: 0.4991171061992645:  65%|███████████▋      | 30/46 [00:03<00:01,  9.52it/s]evaluate for the 31-th batch, evaluate loss: 0.5171542763710022:  65%|███████████▋      | 30/46 [00:03<00:01,  9.52it/s]evaluate for the 31-th batch, evaluate loss: 0.5171542763710022:  67%|████████████▏     | 31/46 [00:03<00:01,  9.54it/s]evaluate for the 11-th batch, evaluate loss: 0.5955247282981873:  50%|█████████         | 10/20 [00:03<00:02,  3.46it/s]evaluate for the 11-th batch, evaluate loss: 0.5955247282981873:  55%|█████████▉        | 11/20 [00:03<00:02,  3.56it/s]evaluate for the 32-th batch, evaluate loss: 0.47497525811195374:  67%|███████████▍     | 31/46 [00:03<00:01,  9.54it/s]evaluate for the 32-th batch, evaluate loss: 0.47497525811195374:  70%|███████████▊     | 32/46 [00:03<00:01,  9.56it/s]Epoch: 1, train for the 342-th batch, train loss: 0.46448272466659546:  89%|████████▉ | 341/383 [03:24<00:23,  1.75it/s]Epoch: 1, train for the 342-th batch, train loss: 0.46448272466659546:  89%|████████▉ | 342/383 [03:24<00:23,  1.72it/s]evaluate for the 33-th batch, evaluate loss: 0.49418729543685913:  70%|███████████▊     | 32/46 [00:03<00:01,  9.56it/s]evaluate for the 33-th batch, evaluate loss: 0.49418729543685913:  72%|████████████▏    | 33/46 [00:03<00:01,  9.57it/s]evaluate for the 34-th batch, evaluate loss: 0.48861292004585266:  72%|████████████▏    | 33/46 [00:03<00:01,  9.57it/s]evaluate for the 34-th batch, evaluate loss: 0.48861292004585266:  74%|████████████▌    | 34/46 [00:03<00:01,  9.60it/s]Epoch: 2, train for the 68-th batch, train loss: 0.5248612761497498:  28%|███▋         | 67/237 [00:38<01:45,  1.61it/s]Epoch: 2, train for the 68-th batch, train loss: 0.5248612761497498:  29%|███▋         | 68/237 [00:38<01:43,  1.63it/s]Epoch: 3, train for the 48-th batch, train loss: 0.24766162037849426:  39%|████▋       | 47/119 [00:28<00:42,  1.70it/s]Epoch: 3, train for the 48-th batch, train loss: 0.24766162037849426:  40%|████▊       | 48/119 [00:28<00:42,  1.69it/s]evaluate for the 12-th batch, evaluate loss: 0.6202980279922485:  55%|█████████▉        | 11/20 [00:03<00:02,  3.56it/s]evaluate for the 12-th batch, evaluate loss: 0.6202980279922485:  60%|██████████▊       | 12/20 [00:03<00:02,  3.40it/s]evaluate for the 35-th batch, evaluate loss: 0.4855547249317169:  74%|█████████████▎    | 34/46 [00:03<00:01,  9.60it/s]evaluate for the 35-th batch, evaluate loss: 0.4855547249317169:  76%|█████████████▋    | 35/46 [00:03<00:01,  9.64it/s]evaluate for the 36-th batch, evaluate loss: 0.4728228747844696:  76%|█████████████▋    | 35/46 [00:03<00:01,  9.64it/s]evaluate for the 36-th batch, evaluate loss: 0.4728228747844696:  78%|██████████████    | 36/46 [00:03<00:01,  9.67it/s]evaluate for the 37-th batch, evaluate loss: 0.5035519003868103:  78%|██████████████    | 36/46 [00:03<00:01,  9.67it/s]evaluate for the 37-th batch, evaluate loss: 0.5035519003868103:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.66it/s]evaluate for the 13-th batch, evaluate loss: 0.6265377402305603:  60%|██████████▊       | 12/20 [00:03<00:02,  3.40it/s]evaluate for the 13-th batch, evaluate loss: 0.6265377402305603:  65%|███████████▋      | 13/20 [00:03<00:02,  3.49it/s]evaluate for the 38-th batch, evaluate loss: 0.5396168231964111:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.66it/s]evaluate for the 38-th batch, evaluate loss: 0.5396168231964111:  83%|██████████████▊   | 38/46 [00:03<00:00,  9.67it/s]Epoch: 1, train for the 343-th batch, train loss: 0.4272879660129547:  89%|█████████▊ | 342/383 [03:24<00:23,  1.72it/s]Epoch: 1, train for the 343-th batch, train loss: 0.4272879660129547:  90%|█████████▊ | 343/383 [03:24<00:23,  1.72it/s]evaluate for the 39-th batch, evaluate loss: 0.5344197154045105:  83%|██████████████▊   | 38/46 [00:04<00:00,  9.67it/s]evaluate for the 39-th batch, evaluate loss: 0.5344197154045105:  85%|███████████████▎  | 39/46 [00:04<00:00,  9.66it/s]Epoch: 2, train for the 69-th batch, train loss: 0.6102908849716187:  29%|███▋         | 68/237 [00:38<01:43,  1.63it/s]Epoch: 2, train for the 69-th batch, train loss: 0.6102908849716187:  29%|███▊         | 69/237 [00:38<01:42,  1.64it/s]evaluate for the 40-th batch, evaluate loss: 0.4704228341579437:  85%|███████████████▎  | 39/46 [00:04<00:00,  9.66it/s]evaluate for the 40-th batch, evaluate loss: 0.4704228341579437:  87%|███████████████▋  | 40/46 [00:04<00:00,  9.69it/s]Epoch: 3, train for the 49-th batch, train loss: 0.26000121235847473:  40%|████▊       | 48/119 [00:29<00:42,  1.69it/s]Epoch: 3, train for the 49-th batch, train loss: 0.26000121235847473:  41%|████▉       | 49/119 [00:29<00:41,  1.68it/s]evaluate for the 14-th batch, evaluate loss: 0.6193292737007141:  65%|███████████▋      | 13/20 [00:04<00:02,  3.49it/s]evaluate for the 14-th batch, evaluate loss: 0.6193292737007141:  70%|████████████▌     | 14/20 [00:04<00:01,  3.35it/s]evaluate for the 41-th batch, evaluate loss: 0.4815238416194916:  87%|███████████████▋  | 40/46 [00:04<00:00,  9.69it/s]evaluate for the 41-th batch, evaluate loss: 0.4815238416194916:  89%|████████████████  | 41/46 [00:04<00:00,  9.72it/s]evaluate for the 42-th batch, evaluate loss: 0.47154685854911804:  89%|███████████████▏ | 41/46 [00:04<00:00,  9.72it/s]evaluate for the 42-th batch, evaluate loss: 0.47154685854911804:  91%|███████████████▌ | 42/46 [00:04<00:00,  9.71it/s]evaluate for the 43-th batch, evaluate loss: 0.5316970348358154:  91%|████████████████▍ | 42/46 [00:04<00:00,  9.71it/s]evaluate for the 43-th batch, evaluate loss: 0.5316970348358154:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.68it/s]evaluate for the 15-th batch, evaluate loss: 0.6087644696235657:  70%|████████████▌     | 14/20 [00:04<00:01,  3.35it/s]evaluate for the 15-th batch, evaluate loss: 0.6087644696235657:  75%|█████████████▌    | 15/20 [00:04<00:01,  3.44it/s]evaluate for the 44-th batch, evaluate loss: 0.5166797637939453:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.68it/s]evaluate for the 44-th batch, evaluate loss: 0.5166797637939453:  96%|█████████████████▏| 44/46 [00:04<00:00,  9.67it/s]Epoch: 1, train for the 344-th batch, train loss: 0.4654204547405243:  90%|█████████▊ | 343/383 [03:25<00:23,  1.72it/s]Epoch: 1, train for the 344-th batch, train loss: 0.4654204547405243:  90%|█████████▉ | 344/383 [03:25<00:22,  1.70it/s]evaluate for the 45-th batch, evaluate loss: 0.49374353885650635:  96%|████████████████▎| 44/46 [00:04<00:00,  9.67it/s]evaluate for the 45-th batch, evaluate loss: 0.49374353885650635:  98%|████████████████▋| 45/46 [00:04<00:00,  9.66it/s]Epoch: 2, train for the 70-th batch, train loss: 0.6538391709327698:  29%|███▊         | 69/237 [00:39<01:42,  1.64it/s]Epoch: 2, train for the 70-th batch, train loss: 0.6538391709327698:  30%|███▊         | 70/237 [00:39<01:41,  1.64it/s]evaluate for the 46-th batch, evaluate loss: 0.5070192813873291:  98%|█████████████████▌| 45/46 [00:04<00:00,  9.66it/s]evaluate for the 46-th batch, evaluate loss: 0.5070192813873291: 100%|██████████████████| 46/46 [00:04<00:00,  9.65it/s]
  0%|                                                                                            | 0/25 [00:00<?, ?it/s]Epoch: 3, train for the 50-th batch, train loss: 0.21359321475028992:  41%|████▉       | 49/119 [00:29<00:41,  1.68it/s]Epoch: 3, train for the 50-th batch, train loss: 0.21359321475028992:  42%|█████       | 50/119 [00:29<00:41,  1.68it/s]evaluate for the 16-th batch, evaluate loss: 0.584507405757904:  75%|██████████████▎    | 15/20 [00:04<00:01,  3.44it/s]evaluate for the 16-th batch, evaluate loss: 0.584507405757904:  80%|███████████████▏   | 16/20 [00:04<00:01,  3.35it/s]evaluate for the 1-th batch, evaluate loss: 0.6511933207511902:   0%|                            | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6511933207511902:   4%|▊                   | 1/25 [00:00<00:02,  9.19it/s]evaluate for the 2-th batch, evaluate loss: 0.6604070663452148:   4%|▊                   | 1/25 [00:00<00:02,  9.19it/s]evaluate for the 2-th batch, evaluate loss: 0.6604070663452148:   8%|█▌                  | 2/25 [00:00<00:02,  9.18it/s]evaluate for the 3-th batch, evaluate loss: 0.7078707218170166:   8%|█▌                  | 2/25 [00:00<00:02,  9.18it/s]evaluate for the 3-th batch, evaluate loss: 0.7078707218170166:  12%|██▍                 | 3/25 [00:00<00:02,  9.17it/s]evaluate for the 17-th batch, evaluate loss: 0.5896479487419128:  80%|██████████████▍   | 16/20 [00:04<00:01,  3.35it/s]evaluate for the 17-th batch, evaluate loss: 0.5896479487419128:  85%|███████████████▎  | 17/20 [00:04<00:00,  3.42it/s]evaluate for the 4-th batch, evaluate loss: 0.6862327456474304:  12%|██▍                 | 3/25 [00:00<00:02,  9.17it/s]evaluate for the 4-th batch, evaluate loss: 0.6862327456474304:  16%|███▏                | 4/25 [00:00<00:02,  9.17it/s]Epoch: 1, train for the 345-th batch, train loss: 0.39006340503692627:  90%|████████▉ | 344/383 [03:26<00:22,  1.70it/s]Epoch: 1, train for the 345-th batch, train loss: 0.39006340503692627:  90%|█████████ | 345/383 [03:26<00:22,  1.68it/s]evaluate for the 5-th batch, evaluate loss: 0.6853207349777222:  16%|███▏                | 4/25 [00:00<00:02,  9.17it/s]evaluate for the 5-th batch, evaluate loss: 0.6853207349777222:  20%|████                | 5/25 [00:00<00:02,  9.05it/s]Epoch: 2, train for the 71-th batch, train loss: 0.6451836824417114:  30%|███▊         | 70/237 [00:39<01:41,  1.64it/s]Epoch: 2, train for the 71-th batch, train loss: 0.6451836824417114:  30%|███▉         | 71/237 [00:39<01:41,  1.64it/s]Epoch: 3, train for the 51-th batch, train loss: 0.24814473092556:  42%|██████▎        | 50/119 [00:30<00:41,  1.68it/s]Epoch: 3, train for the 51-th batch, train loss: 0.24814473092556:  43%|██████▍        | 51/119 [00:30<00:40,  1.68it/s]evaluate for the 18-th batch, evaluate loss: 0.6552683711051941:  85%|███████████████▎  | 17/20 [00:05<00:00,  3.42it/s]evaluate for the 18-th batch, evaluate loss: 0.6552683711051941:  90%|████████████████▏ | 18/20 [00:05<00:00,  3.34it/s]evaluate for the 6-th batch, evaluate loss: 0.7275148034095764:  20%|████                | 5/25 [00:00<00:02,  9.05it/s]evaluate for the 6-th batch, evaluate loss: 0.7275148034095764:  24%|████▊               | 6/25 [00:00<00:02,  9.07it/s]evaluate for the 7-th batch, evaluate loss: 0.7559671401977539:  24%|████▊               | 6/25 [00:00<00:02,  9.07it/s]evaluate for the 7-th batch, evaluate loss: 0.7559671401977539:  28%|█████▌              | 7/25 [00:00<00:01,  9.12it/s]evaluate for the 8-th batch, evaluate loss: 0.7379761338233948:  28%|█████▌              | 7/25 [00:00<00:01,  9.12it/s]evaluate for the 8-th batch, evaluate loss: 0.7379761338233948:  32%|██████▍             | 8/25 [00:00<00:01,  9.14it/s]evaluate for the 19-th batch, evaluate loss: 0.6450814008712769:  90%|████████████████▏ | 18/20 [00:05<00:00,  3.34it/s]evaluate for the 19-th batch, evaluate loss: 0.6450814008712769:  95%|█████████████████ | 19/20 [00:05<00:00,  3.41it/s]evaluate for the 9-th batch, evaluate loss: 0.7196980714797974:  32%|██████▍             | 8/25 [00:00<00:01,  9.14it/s]evaluate for the 9-th batch, evaluate loss: 0.7196980714797974:  36%|███████▏            | 9/25 [00:00<00:01,  9.17it/s]evaluate for the 20-th batch, evaluate loss: 0.6266980767250061:  95%|█████████████████ | 19/20 [00:05<00:00,  3.41it/s]evaluate for the 20-th batch, evaluate loss: 0.6266980767250061: 100%|██████████████████| 20/20 [00:05<00:00,  3.61it/s]
INFO:root:Epoch: 2, learning rate: 0.0001, train loss: 0.5180
INFO:root:train average_precision, 0.8445
INFO:root:train roc_auc, 0.8188
INFO:root:validate loss: 0.5236
INFO:root:validate average_precision, 0.8372
INFO:root:validate roc_auc, 0.8037
INFO:root:new node validate loss: 0.6131
INFO:root:new node validate first_1_average_precision, 0.6393
INFO:root:new node validate first_1_roc_auc, 0.5666
INFO:root:new node validate first_3_average_precision, 0.6961
INFO:root:new node validate first_3_roc_auc, 0.6259
INFO:root:new node validate first_10_average_precision, 0.7420
INFO:root:new node validate first_10_roc_auc, 0.6864
INFO:root:new node validate average_precision, 0.7477
INFO:root:new node validate roc_auc, 0.7073
INFO:root:save model ./saved_models/DyGFormer/ia-escorts-dynamic/DyGFormer_seed0_dygformer-ia-escorts-dynamic-old/DyGFormer_seed0_dygformer-ia-escorts-dynamic-old.pkl
Epoch: 1, train for the 346-th batch, train loss: 0.41329750418663025:  90%|█████████ | 345/383 [03:26<00:22,  1.68it/s]Epoch: 1, train for the 346-th batch, train loss: 0.41329750418663025:  90%|█████████ | 346/383 [03:26<00:22,  1.67it/s]Epoch: 3, train for the 52-th batch, train loss: 0.31712669134140015:  43%|█████▏      | 51/119 [00:30<00:40,  1.68it/s]Epoch: 3, train for the 52-th batch, train loss: 0.31712669134140015:  44%|█████▏      | 52/119 [00:30<00:36,  1.84it/s]evaluate for the 10-th batch, evaluate loss: 0.7515293955802917:  36%|██████▊            | 9/25 [00:01<00:01,  9.17it/s]evaluate for the 10-th batch, evaluate loss: 0.7515293955802917:  40%|███████▏          | 10/25 [00:01<00:01,  9.17it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]evaluate for the 11-th batch, evaluate loss: 0.7485888004302979:  40%|███████▏          | 10/25 [00:01<00:01,  9.17it/s]evaluate for the 11-th batch, evaluate loss: 0.7485888004302979:  44%|███████▉          | 11/25 [00:01<00:01,  9.18it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5502275824546814:  30%|███▉         | 71/237 [00:40<01:41,  1.64it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5502275824546814:  30%|███▉         | 72/237 [00:40<01:40,  1.64it/s]evaluate for the 12-th batch, evaluate loss: 0.7079198956489563:  44%|███████▉          | 11/25 [00:01<00:01,  9.18it/s]evaluate for the 12-th batch, evaluate loss: 0.7079198956489563:  48%|████████▋         | 12/25 [00:01<00:01,  9.19it/s]evaluate for the 13-th batch, evaluate loss: 0.6828417778015137:  48%|████████▋         | 12/25 [00:01<00:01,  9.19it/s]evaluate for the 13-th batch, evaluate loss: 0.6828417778015137:  52%|█████████▎        | 13/25 [00:01<00:01,  9.18it/s]evaluate for the 14-th batch, evaluate loss: 0.7683204412460327:  52%|█████████▎        | 13/25 [00:01<00:01,  9.18it/s]evaluate for the 14-th batch, evaluate loss: 0.7683204412460327:  56%|██████████        | 14/25 [00:01<00:01,  9.19it/s]Epoch: 3, train for the 1-th batch, train loss: 0.9144450426101685:   0%|                       | 0/146 [00:00<?, ?it/s]Epoch: 3, train for the 1-th batch, train loss: 0.9144450426101685:   1%|               | 1/146 [00:00<01:05,  2.20it/s]Epoch: 3, train for the 53-th batch, train loss: 0.2362537384033203:  44%|█████▋       | 52/119 [00:31<00:36,  1.84it/s]Epoch: 3, train for the 53-th batch, train loss: 0.2362537384033203:  45%|█████▊       | 53/119 [00:31<00:35,  1.88it/s]evaluate for the 15-th batch, evaluate loss: 0.7473173141479492:  56%|██████████        | 14/25 [00:01<00:01,  9.19it/s]evaluate for the 15-th batch, evaluate loss: 0.7473173141479492:  60%|██████████▊       | 15/25 [00:01<00:01,  9.19it/s]Epoch: 1, train for the 347-th batch, train loss: 0.4181678295135498:  90%|█████████▉ | 346/383 [03:27<00:22,  1.67it/s]Epoch: 1, train for the 347-th batch, train loss: 0.4181678295135498:  91%|█████████▉ | 347/383 [03:27<00:21,  1.67it/s]evaluate for the 16-th batch, evaluate loss: 0.6747013926506042:  60%|██████████▊       | 15/25 [00:01<00:01,  9.19it/s]evaluate for the 16-th batch, evaluate loss: 0.6747013926506042:  64%|███████████▌      | 16/25 [00:01<00:00,  9.19it/s]Epoch: 2, train for the 73-th batch, train loss: 0.5933863520622253:  30%|███▉         | 72/237 [00:41<01:40,  1.64it/s]Epoch: 2, train for the 73-th batch, train loss: 0.5933863520622253:  31%|████         | 73/237 [00:41<01:39,  1.65it/s]evaluate for the 17-th batch, evaluate loss: 0.6805731654167175:  64%|███████████▌      | 16/25 [00:01<00:00,  9.19it/s]evaluate for the 17-th batch, evaluate loss: 0.6805731654167175:  68%|████████████▏     | 17/25 [00:01<00:00,  9.20it/s]evaluate for the 18-th batch, evaluate loss: 0.638915479183197:  68%|████████████▉      | 17/25 [00:01<00:00,  9.20it/s]evaluate for the 18-th batch, evaluate loss: 0.638915479183197:  72%|█████████████▋     | 18/25 [00:01<00:00,  9.19it/s]evaluate for the 19-th batch, evaluate loss: 0.6008503437042236:  72%|████████████▉     | 18/25 [00:02<00:00,  9.19it/s]evaluate for the 19-th batch, evaluate loss: 0.6008503437042236:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.19it/s]Epoch: 3, train for the 2-th batch, train loss: 0.49421370029449463:   1%|              | 1/146 [00:01<01:05,  2.20it/s]Epoch: 3, train for the 2-th batch, train loss: 0.49421370029449463:   1%|▏             | 2/146 [00:01<01:14,  1.92it/s]evaluate for the 20-th batch, evaluate loss: 0.6734976172447205:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.19it/s]evaluate for the 20-th batch, evaluate loss: 0.6734976172447205:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.21it/s]Epoch: 3, train for the 54-th batch, train loss: 0.1977597177028656:  45%|█████▊       | 53/119 [00:32<00:35,  1.88it/s]Epoch: 3, train for the 54-th batch, train loss: 0.1977597177028656:  45%|█████▉       | 54/119 [00:32<00:36,  1.79it/s]Epoch: 1, train for the 348-th batch, train loss: 0.5124548077583313:  91%|█████████▉ | 347/383 [03:28<00:21,  1.67it/s]Epoch: 1, train for the 348-th batch, train loss: 0.5124548077583313:  91%|█████████▉ | 348/383 [03:28<00:21,  1.66it/s]evaluate for the 21-th batch, evaluate loss: 0.7426493167877197:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.21it/s]evaluate for the 21-th batch, evaluate loss: 0.7426493167877197:  84%|███████████████   | 21/25 [00:02<00:00,  9.22it/s]evaluate for the 22-th batch, evaluate loss: 0.6276172995567322:  84%|███████████████   | 21/25 [00:02<00:00,  9.22it/s]evaluate for the 22-th batch, evaluate loss: 0.6276172995567322:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.22it/s]Epoch: 2, train for the 74-th batch, train loss: 0.6459236145019531:  31%|████         | 73/237 [00:41<01:39,  1.65it/s]Epoch: 2, train for the 74-th batch, train loss: 0.6459236145019531:  31%|████         | 74/237 [00:41<01:38,  1.65it/s]evaluate for the 23-th batch, evaluate loss: 0.685965359210968:  88%|████████████████▋  | 22/25 [00:02<00:00,  9.22it/s]evaluate for the 23-th batch, evaluate loss: 0.685965359210968:  92%|█████████████████▍ | 23/25 [00:02<00:00,  9.21it/s]evaluate for the 24-th batch, evaluate loss: 0.6734952330589294:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.21it/s]evaluate for the 24-th batch, evaluate loss: 0.6734952330589294:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.21it/s]evaluate for the 25-th batch, evaluate loss: 0.7241163849830627:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.21it/s]evaluate for the 25-th batch, evaluate loss: 0.7241163849830627: 100%|██████████████████| 25/25 [00:02<00:00,  9.25it/s]
INFO:root:Epoch: 4, learning rate: 0.0001, train loss: 0.5709
INFO:root:train average_precision, 0.8127
INFO:root:train roc_auc, 0.7755
INFO:root:validate loss: 0.5106
INFO:root:validate average_precision, 0.8401
INFO:root:validate roc_auc, 0.8003
INFO:root:new node validate loss: 0.6984
INFO:root:new node validate first_1_average_precision, 0.5884
Epoch: 3, train for the 3-th batch, train loss: 0.41008076071739197:   1%|▏             | 2/146 [00:01<01:14,  1.92it/s]INFO:root:new node validate first_1_roc_auc, 0.5333
Epoch: 3, train for the 3-th batch, train loss: 0.41008076071739197:   2%|▎             | 3/146 [00:01<01:18,  1.83it/s]INFO:root:new node validate first_3_average_precision, 0.6724
INFO:root:new node validate first_3_roc_auc, 0.6325
INFO:root:new node validate first_10_average_precision, 0.7416
INFO:root:new node validate first_10_roc_auc, 0.7070
INFO:root:new node validate average_precision, 0.7036
INFO:root:new node validate roc_auc, 0.6535
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]Epoch: 3, train for the 55-th batch, train loss: 0.19389350712299347:  45%|█████▍      | 54/119 [00:32<00:36,  1.79it/s]Epoch: 3, train for the 55-th batch, train loss: 0.19389350712299347:  46%|█████▌      | 55/119 [00:32<00:36,  1.75it/s]Epoch: 1, train for the 349-th batch, train loss: 0.48910775780677795:  91%|█████████ | 348/383 [03:28<00:21,  1.66it/s]Epoch: 1, train for the 349-th batch, train loss: 0.48910775780677795:  91%|█████████ | 349/383 [03:28<00:20,  1.66it/s]Epoch: 5, train for the 1-th batch, train loss: 0.8063349723815918:   0%|                       | 0/151 [00:00<?, ?it/s]Epoch: 5, train for the 1-th batch, train loss: 0.8063349723815918:   1%|               | 1/151 [00:00<00:25,  5.79it/s]Epoch: 2, train for the 75-th batch, train loss: 0.5828941464424133:  31%|████         | 74/237 [00:42<01:38,  1.65it/s]Epoch: 2, train for the 75-th batch, train loss: 0.5828941464424133:  32%|████         | 75/237 [00:42<01:38,  1.65it/s]Epoch: 5, train for the 2-th batch, train loss: 0.7010599970817566:   1%|               | 1/151 [00:00<00:25,  5.79it/s]Epoch: 5, train for the 2-th batch, train loss: 0.7010599970817566:   1%|▏              | 2/151 [00:00<00:26,  5.66it/s]Epoch: 5, train for the 3-th batch, train loss: 0.6721569895744324:   1%|▏              | 2/151 [00:00<00:26,  5.66it/s]Epoch: 5, train for the 3-th batch, train loss: 0.6721569895744324:   2%|▎              | 3/151 [00:00<00:25,  5.84it/s]Epoch: 3, train for the 4-th batch, train loss: 0.802603006362915:   2%|▎               | 3/146 [00:02<01:18,  1.83it/s]Epoch: 3, train for the 4-th batch, train loss: 0.802603006362915:   3%|▍               | 4/146 [00:02<01:20,  1.77it/s]Epoch: 3, train for the 56-th batch, train loss: 0.22447596490383148:  46%|█████▌      | 55/119 [00:33<00:36,  1.75it/s]Epoch: 3, train for the 56-th batch, train loss: 0.22447596490383148:  47%|█████▋      | 56/119 [00:33<00:36,  1.73it/s]Epoch: 5, train for the 4-th batch, train loss: 0.5923252701759338:   2%|▎              | 3/151 [00:00<00:25,  5.84it/s]Epoch: 5, train for the 4-th batch, train loss: 0.5923252701759338:   3%|▍              | 4/151 [00:00<00:24,  5.90it/s]Epoch: 1, train for the 350-th batch, train loss: 0.5044927597045898:  91%|██████████ | 349/383 [03:29<00:20,  1.66it/s]Epoch: 1, train for the 350-th batch, train loss: 0.5044927597045898:  91%|██████████ | 350/383 [03:29<00:19,  1.66it/s]Epoch: 5, train for the 5-th batch, train loss: 0.5408111214637756:   3%|▍              | 4/151 [00:00<00:24,  5.90it/s]Epoch: 5, train for the 5-th batch, train loss: 0.5408111214637756:   3%|▍              | 5/151 [00:00<00:25,  5.70it/s]Epoch: 2, train for the 76-th batch, train loss: 0.6280240416526794:  32%|████         | 75/237 [00:43<01:38,  1.65it/s]Epoch: 2, train for the 76-th batch, train loss: 0.6280240416526794:  32%|████▏        | 76/237 [00:43<01:37,  1.66it/s]Epoch: 5, train for the 6-th batch, train loss: 0.6092106699943542:   3%|▍              | 5/151 [00:01<00:25,  5.70it/s]Epoch: 5, train for the 6-th batch, train loss: 0.6092106699943542:   4%|▌              | 6/151 [00:01<00:25,  5.58it/s]Epoch: 3, train for the 5-th batch, train loss: 0.49686434864997864:   3%|▍             | 4/146 [00:02<01:20,  1.77it/s]Epoch: 3, train for the 5-th batch, train loss: 0.49686434864997864:   3%|▍             | 5/146 [00:02<01:18,  1.80it/s]Epoch: 5, train for the 7-th batch, train loss: 0.6424806118011475:   4%|▌              | 6/151 [00:01<00:25,  5.58it/s]Epoch: 5, train for the 7-th batch, train loss: 0.6424806118011475:   5%|▋              | 7/151 [00:01<00:26,  5.38it/s]Epoch: 3, train for the 57-th batch, train loss: 0.1740988940000534:  47%|██████       | 56/119 [00:33<00:36,  1.73it/s]Epoch: 3, train for the 57-th batch, train loss: 0.1740988940000534:  48%|██████▏      | 57/119 [00:33<00:36,  1.71it/s]Epoch: 2, train for the 77-th batch, train loss: 0.6216647028923035:  32%|████▏        | 76/237 [00:43<01:37,  1.66it/s]Epoch: 2, train for the 77-th batch, train loss: 0.6216647028923035:  32%|████▏        | 77/237 [00:43<01:31,  1.75it/s]Epoch: 5, train for the 8-th batch, train loss: 0.6441075205802917:   5%|▋              | 7/151 [00:01<00:26,  5.38it/s]Epoch: 5, train for the 8-th batch, train loss: 0.6441075205802917:   5%|▊              | 8/151 [00:01<00:26,  5.35it/s]Epoch: 1, train for the 351-th batch, train loss: 0.49876531958580017:  91%|█████████▏| 350/383 [03:29<00:19,  1.66it/s]Epoch: 1, train for the 351-th batch, train loss: 0.49876531958580017:  92%|█████████▏| 351/383 [03:29<00:20,  1.57it/s]Epoch: 5, train for the 9-th batch, train loss: 0.60735684633255:   5%|▉                | 8/151 [00:01<00:26,  5.35it/s]Epoch: 5, train for the 9-th batch, train loss: 0.60735684633255:   6%|█                | 9/151 [00:01<00:30,  4.66it/s]Epoch: 3, train for the 6-th batch, train loss: 0.4760033190250397:   3%|▌              | 5/146 [00:03<01:18,  1.80it/s]Epoch: 3, train for the 6-th batch, train loss: 0.4760033190250397:   4%|▌              | 6/146 [00:03<01:19,  1.76it/s]Epoch: 3, train for the 58-th batch, train loss: 0.19208304584026337:  48%|█████▋      | 57/119 [00:34<00:36,  1.71it/s]Epoch: 3, train for the 58-th batch, train loss: 0.19208304584026337:  49%|█████▊      | 58/119 [00:34<00:36,  1.69it/s]Epoch: 5, train for the 10-th batch, train loss: 0.5145645141601562:   6%|▊             | 9/151 [00:01<00:30,  4.66it/s]Epoch: 5, train for the 10-th batch, train loss: 0.5145645141601562:   7%|▊            | 10/151 [00:01<00:29,  4.77it/s]Epoch: 2, train for the 78-th batch, train loss: 0.5401420593261719:  32%|████▏        | 77/237 [00:44<01:31,  1.75it/s]Epoch: 2, train for the 78-th batch, train loss: 0.5401420593261719:  33%|████▎        | 78/237 [00:44<01:31,  1.74it/s]Epoch: 1, train for the 352-th batch, train loss: 0.39106521010398865:  92%|█████████▏| 351/383 [03:30<00:20,  1.57it/s]Epoch: 1, train for the 352-th batch, train loss: 0.39106521010398865:  92%|█████████▏| 352/383 [03:30<00:19,  1.61it/s]Epoch: 5, train for the 11-th batch, train loss: 0.5385767817497253:   7%|▊            | 10/151 [00:02<00:29,  4.77it/s]Epoch: 5, train for the 11-th batch, train loss: 0.5385767817497253:   7%|▉            | 11/151 [00:02<00:28,  4.85it/s]Epoch: 5, train for the 12-th batch, train loss: 0.47113296389579773:   7%|▊           | 11/151 [00:02<00:28,  4.85it/s]Epoch: 5, train for the 12-th batch, train loss: 0.47113296389579773:   8%|▉           | 12/151 [00:02<00:28,  4.91it/s]Epoch: 3, train for the 7-th batch, train loss: 0.4971044957637787:   4%|▌              | 6/146 [00:03<01:19,  1.76it/s]Epoch: 3, train for the 7-th batch, train loss: 0.4971044957637787:   5%|▋              | 7/146 [00:03<01:21,  1.70it/s]Epoch: 3, train for the 59-th batch, train loss: 0.23437638580799103:  49%|█████▊      | 58/119 [00:35<00:36,  1.69it/s]Epoch: 3, train for the 59-th batch, train loss: 0.23437638580799103:  50%|█████▉      | 59/119 [00:35<00:36,  1.66it/s]Epoch: 5, train for the 13-th batch, train loss: 1.0392155647277832:   8%|█            | 12/151 [00:02<00:28,  4.91it/s]Epoch: 5, train for the 13-th batch, train loss: 1.0392155647277832:   9%|█            | 13/151 [00:02<00:28,  4.89it/s]Epoch: 2, train for the 79-th batch, train loss: 0.6366094946861267:  33%|████▎        | 78/237 [00:44<01:31,  1.74it/s]Epoch: 2, train for the 79-th batch, train loss: 0.6366094946861267:  33%|████▎        | 79/237 [00:44<01:32,  1.71it/s]Epoch: 1, train for the 353-th batch, train loss: 0.4880330562591553:  92%|██████████ | 352/383 [03:31<00:19,  1.61it/s]Epoch: 1, train for the 353-th batch, train loss: 0.4880330562591553:  92%|██████████▏| 353/383 [03:31<00:18,  1.63it/s]Epoch: 5, train for the 14-th batch, train loss: 1.1008267402648926:   9%|█            | 13/151 [00:02<00:28,  4.89it/s]Epoch: 5, train for the 14-th batch, train loss: 1.1008267402648926:   9%|█▏           | 14/151 [00:02<00:28,  4.79it/s]Epoch: 3, train for the 60-th batch, train loss: 0.2039872258901596:  50%|██████▍      | 59/119 [00:35<00:36,  1.66it/s]Epoch: 3, train for the 60-th batch, train loss: 0.2039872258901596:  50%|██████▌      | 60/119 [00:35<00:30,  1.93it/s]Epoch: 5, train for the 15-th batch, train loss: 0.47854289412498474:   9%|█           | 14/151 [00:02<00:28,  4.79it/s]Epoch: 5, train for the 15-th batch, train loss: 0.47854289412498474:  10%|█▏          | 15/151 [00:02<00:28,  4.82it/s]Epoch: 5, train for the 16-th batch, train loss: 0.5560542941093445:  10%|█▎           | 15/151 [00:03<00:28,  4.82it/s]Epoch: 5, train for the 16-th batch, train loss: 0.5560542941093445:  11%|█▍           | 16/151 [00:03<00:27,  4.85it/s]Epoch: 3, train for the 61-th batch, train loss: 0.2018139660358429:  50%|██████▌      | 60/119 [00:35<00:30,  1.93it/s]Epoch: 3, train for the 61-th batch, train loss: 0.2018139660358429:  51%|██████▋      | 61/119 [00:35<00:27,  2.14it/s]Epoch: 2, train for the 80-th batch, train loss: 0.6153662800788879:  33%|████▎        | 79/237 [00:45<01:32,  1.71it/s]Epoch: 2, train for the 80-th batch, train loss: 0.6153662800788879:  34%|████▍        | 80/237 [00:45<01:32,  1.69it/s]Epoch: 1, train for the 354-th batch, train loss: 0.4154678285121918:  92%|██████████▏| 353/383 [03:31<00:18,  1.63it/s]Epoch: 1, train for the 354-th batch, train loss: 0.4154678285121918:  92%|██████████▏| 354/383 [03:31<00:17,  1.65it/s]Epoch: 5, train for the 17-th batch, train loss: 0.6489061117172241:  11%|█▍           | 16/151 [00:03<00:27,  4.85it/s]Epoch: 5, train for the 17-th batch, train loss: 0.6489061117172241:  11%|█▍           | 17/151 [00:03<00:27,  4.86it/s]Epoch: 3, train for the 8-th batch, train loss: 0.447710245847702:   5%|▊               | 7/146 [00:05<01:21,  1.70it/s]Epoch: 3, train for the 8-th batch, train loss: 0.447710245847702:   5%|▉               | 8/146 [00:05<01:41,  1.35it/s]Epoch: 5, train for the 18-th batch, train loss: 0.6719504594802856:  11%|█▍           | 17/151 [00:03<00:27,  4.86it/s]Epoch: 5, train for the 18-th batch, train loss: 0.6719504594802856:  12%|█▌           | 18/151 [00:03<00:27,  4.79it/s]Epoch: 5, train for the 19-th batch, train loss: 0.5838894248008728:  12%|█▌           | 18/151 [00:03<00:27,  4.79it/s]Epoch: 5, train for the 19-th batch, train loss: 0.5838894248008728:  13%|█▋           | 19/151 [00:03<00:27,  4.84it/s]Epoch: 3, train for the 9-th batch, train loss: 0.4491426646709442:   5%|▊              | 8/146 [00:05<01:41,  1.35it/s]Epoch: 3, train for the 9-th batch, train loss: 0.4491426646709442:   6%|▉              | 9/146 [00:05<01:25,  1.60it/s]Epoch: 2, train for the 81-th batch, train loss: 0.6138314008712769:  34%|████▍        | 80/237 [00:45<01:32,  1.69it/s]Epoch: 2, train for the 81-th batch, train loss: 0.6138314008712769:  34%|████▍        | 81/237 [00:45<01:34,  1.66it/s]Epoch: 1, train for the 355-th batch, train loss: 0.5518984794616699:  92%|██████████▏| 354/383 [03:32<00:17,  1.65it/s]Epoch: 1, train for the 355-th batch, train loss: 0.5518984794616699:  93%|██████████▏| 355/383 [03:32<00:16,  1.65it/s]Epoch: 3, train for the 62-th batch, train loss: 0.2909295856952667:  51%|██████▋      | 61/119 [00:36<00:27,  2.14it/s]Epoch: 3, train for the 62-th batch, train loss: 0.2909295856952667:  52%|██████▊      | 62/119 [00:36<00:30,  1.86it/s]Epoch: 5, train for the 20-th batch, train loss: 0.649606466293335:  13%|█▊            | 19/151 [00:03<00:27,  4.84it/s]Epoch: 5, train for the 20-th batch, train loss: 0.649606466293335:  13%|█▊            | 20/151 [00:03<00:27,  4.76it/s]Epoch: 3, train for the 10-th batch, train loss: 0.46454083919525146:   6%|▊            | 9/146 [00:05<01:25,  1.60it/s]Epoch: 3, train for the 10-th batch, train loss: 0.46454083919525146:   7%|▊           | 10/146 [00:05<01:15,  1.80it/s]Epoch: 5, train for the 21-th batch, train loss: 0.6257946491241455:  13%|█▋           | 20/151 [00:04<00:27,  4.76it/s]Epoch: 5, train for the 21-th batch, train loss: 0.6257946491241455:  14%|█▊           | 21/151 [00:04<00:27,  4.77it/s]Epoch: 5, train for the 22-th batch, train loss: 0.66368567943573:  14%|██             | 21/151 [00:04<00:27,  4.77it/s]Epoch: 5, train for the 22-th batch, train loss: 0.66368567943573:  15%|██▏            | 22/151 [00:04<00:27,  4.69it/s]Epoch: 2, train for the 82-th batch, train loss: 0.627117931842804:  34%|████▊         | 81/237 [00:46<01:34,  1.66it/s]Epoch: 2, train for the 82-th batch, train loss: 0.627117931842804:  35%|████▊         | 82/237 [00:46<01:35,  1.62it/s]Epoch: 1, train for the 356-th batch, train loss: 0.43606650829315186:  93%|█████████▎| 355/383 [03:32<00:16,  1.65it/s]Epoch: 1, train for the 356-th batch, train loss: 0.43606650829315186:  93%|█████████▎| 356/383 [03:32<00:16,  1.63it/s]Epoch: 3, train for the 63-th batch, train loss: 0.20581214129924774:  52%|██████▎     | 62/119 [00:37<00:30,  1.86it/s]Epoch: 3, train for the 63-th batch, train loss: 0.20581214129924774:  53%|██████▎     | 63/119 [00:37<00:31,  1.79it/s]Epoch: 5, train for the 23-th batch, train loss: 0.6662256717681885:  15%|█▉           | 22/151 [00:04<00:27,  4.69it/s]Epoch: 5, train for the 23-th batch, train loss: 0.6662256717681885:  15%|█▉           | 23/151 [00:04<00:27,  4.62it/s]Epoch: 3, train for the 11-th batch, train loss: 0.4556073546409607:   7%|▉            | 10/146 [00:06<01:15,  1.80it/s]Epoch: 3, train for the 11-th batch, train loss: 0.4556073546409607:   8%|▉            | 11/146 [00:06<01:13,  1.84it/s]Epoch: 5, train for the 24-th batch, train loss: 0.6458971500396729:  15%|█▉           | 23/151 [00:04<00:27,  4.62it/s]Epoch: 5, train for the 24-th batch, train loss: 0.6458971500396729:  16%|██           | 24/151 [00:04<00:27,  4.60it/s]Epoch: 3, train for the 64-th batch, train loss: 0.19357357919216156:  53%|██████▎     | 63/119 [00:37<00:31,  1.79it/s]Epoch: 3, train for the 64-th batch, train loss: 0.19357357919216156:  54%|██████▍     | 64/119 [00:37<00:30,  1.79it/s]Epoch: 5, train for the 25-th batch, train loss: 0.6959721446037292:  16%|██           | 24/151 [00:05<00:27,  4.60it/s]Epoch: 5, train for the 25-th batch, train loss: 0.6959721446037292:  17%|██▏          | 25/151 [00:05<00:27,  4.57it/s]Epoch: 2, train for the 83-th batch, train loss: 0.559657633304596:  35%|████▊         | 82/237 [00:47<01:35,  1.62it/s]Epoch: 2, train for the 83-th batch, train loss: 0.559657633304596:  35%|████▉         | 83/237 [00:47<01:35,  1.62it/s]Epoch: 1, train for the 357-th batch, train loss: 0.4857546389102936:  93%|██████████▏| 356/383 [03:33<00:16,  1.63it/s]Epoch: 1, train for the 357-th batch, train loss: 0.4857546389102936:  93%|██████████▎| 357/383 [03:33<00:16,  1.61it/s]Epoch: 3, train for the 12-th batch, train loss: 0.4424070119857788:   8%|▉            | 11/146 [00:06<01:13,  1.84it/s]Epoch: 3, train for the 12-th batch, train loss: 0.4424070119857788:   8%|█            | 12/146 [00:06<01:12,  1.84it/s]Epoch: 5, train for the 26-th batch, train loss: 0.6465919613838196:  17%|██▏          | 25/151 [00:05<00:27,  4.57it/s]Epoch: 5, train for the 26-th batch, train loss: 0.6465919613838196:  17%|██▏          | 26/151 [00:05<00:27,  4.61it/s]Epoch: 5, train for the 27-th batch, train loss: 0.6038344502449036:  17%|██▏          | 26/151 [00:05<00:27,  4.61it/s]Epoch: 5, train for the 27-th batch, train loss: 0.6038344502449036:  18%|██▎          | 27/151 [00:05<00:26,  4.67it/s]Epoch: 3, train for the 65-th batch, train loss: 0.2090339958667755:  54%|██████▉      | 64/119 [00:38<00:30,  1.79it/s]Epoch: 3, train for the 65-th batch, train loss: 0.2090339958667755:  55%|███████      | 65/119 [00:38<00:31,  1.74it/s]Epoch: 5, train for the 28-th batch, train loss: 0.6619421243667603:  18%|██▎          | 27/151 [00:05<00:26,  4.67it/s]Epoch: 5, train for the 28-th batch, train loss: 0.6619421243667603:  19%|██▍          | 28/151 [00:05<00:26,  4.61it/s]Epoch: 2, train for the 84-th batch, train loss: 0.6046171188354492:  35%|████▌        | 83/237 [00:47<01:35,  1.62it/s]Epoch: 2, train for the 84-th batch, train loss: 0.6046171188354492:  35%|████▌        | 84/237 [00:47<01:35,  1.60it/s]Epoch: 1, train for the 358-th batch, train loss: 0.5351436138153076:  93%|██████████▎| 357/383 [03:34<00:16,  1.61it/s]Epoch: 1, train for the 358-th batch, train loss: 0.5351436138153076:  93%|██████████▎| 358/383 [03:34<00:15,  1.60it/s]Epoch: 3, train for the 13-th batch, train loss: 0.4920113980770111:   8%|█            | 12/146 [00:07<01:12,  1.84it/s]Epoch: 3, train for the 13-th batch, train loss: 0.4920113980770111:   9%|█▏           | 13/146 [00:07<01:14,  1.79it/s]Epoch: 5, train for the 29-th batch, train loss: 0.6769145131111145:  19%|██▍          | 28/151 [00:05<00:26,  4.61it/s]Epoch: 5, train for the 29-th batch, train loss: 0.6769145131111145:  19%|██▍          | 29/151 [00:05<00:26,  4.64it/s]Epoch: 5, train for the 30-th batch, train loss: 0.5445498824119568:  19%|██▍          | 29/151 [00:06<00:26,  4.64it/s]Epoch: 5, train for the 30-th batch, train loss: 0.5445498824119568:  20%|██▌          | 30/151 [00:06<00:25,  4.71it/s]Epoch: 3, train for the 66-th batch, train loss: 0.24562059342861176:  55%|██████▌     | 65/119 [00:38<00:31,  1.74it/s]Epoch: 3, train for the 66-th batch, train loss: 0.24562059342861176:  55%|██████▋     | 66/119 [00:38<00:31,  1.69it/s]Epoch: 5, train for the 31-th batch, train loss: 0.6792906522750854:  20%|██▌          | 30/151 [00:06<00:25,  4.71it/s]Epoch: 5, train for the 31-th batch, train loss: 0.6792906522750854:  21%|██▋          | 31/151 [00:06<00:25,  4.66it/s]Epoch: 1, train for the 359-th batch, train loss: 0.4872004985809326:  93%|██████████▎| 358/383 [03:34<00:15,  1.60it/s]Epoch: 1, train for the 359-th batch, train loss: 0.4872004985809326:  94%|██████████▎| 359/383 [03:34<00:15,  1.58it/s]Epoch: 2, train for the 85-th batch, train loss: 0.5957767367362976:  35%|████▌        | 84/237 [00:48<01:35,  1.60it/s]Epoch: 2, train for the 85-th batch, train loss: 0.5957767367362976:  36%|████▋        | 85/237 [00:48<01:36,  1.58it/s]Epoch: 3, train for the 14-th batch, train loss: 0.4397372007369995:   9%|█▏           | 13/146 [00:08<01:14,  1.79it/s]Epoch: 3, train for the 14-th batch, train loss: 0.4397372007369995:  10%|█▏           | 14/146 [00:08<01:15,  1.74it/s]Epoch: 5, train for the 32-th batch, train loss: 0.6440173983573914:  21%|██▋          | 31/151 [00:06<00:25,  4.66it/s]Epoch: 5, train for the 32-th batch, train loss: 0.6440173983573914:  21%|██▊          | 32/151 [00:06<00:25,  4.65it/s]Epoch: 5, train for the 33-th batch, train loss: 0.6156799793243408:  21%|██▊          | 32/151 [00:06<00:25,  4.65it/s]Epoch: 5, train for the 33-th batch, train loss: 0.6156799793243408:  22%|██▊          | 33/151 [00:06<00:25,  4.66it/s]Epoch: 3, train for the 67-th batch, train loss: 0.2973986268043518:  55%|███████▏     | 66/119 [00:39<00:31,  1.69it/s]Epoch: 3, train for the 67-th batch, train loss: 0.2973986268043518:  56%|███████▎     | 67/119 [00:39<00:31,  1.67it/s]Epoch: 5, train for the 34-th batch, train loss: 0.6588795185089111:  22%|██▊          | 33/151 [00:07<00:25,  4.66it/s]Epoch: 5, train for the 34-th batch, train loss: 0.6588795185089111:  23%|██▉          | 34/151 [00:07<00:25,  4.61it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5470253229141235:  36%|████▋        | 85/237 [00:49<01:36,  1.58it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5470253229141235:  36%|████▋        | 86/237 [00:49<01:35,  1.58it/s]Epoch: 1, train for the 360-th batch, train loss: 0.4765060842037201:  94%|██████████▎| 359/383 [03:35<00:15,  1.58it/s]Epoch: 1, train for the 360-th batch, train loss: 0.4765060842037201:  94%|██████████▎| 360/383 [03:35<00:14,  1.58it/s]Epoch: 3, train for the 15-th batch, train loss: 0.44823721051216125:  10%|█▏          | 14/146 [00:08<01:15,  1.74it/s]Epoch: 3, train for the 15-th batch, train loss: 0.44823721051216125:  10%|█▏          | 15/146 [00:08<01:16,  1.71it/s]Epoch: 5, train for the 35-th batch, train loss: 0.6695380806922913:  23%|██▉          | 34/151 [00:07<00:25,  4.61it/s]Epoch: 5, train for the 35-th batch, train loss: 0.6695380806922913:  23%|███          | 35/151 [00:07<00:25,  4.57it/s]Epoch: 5, train for the 36-th batch, train loss: 0.6153182983398438:  23%|███          | 35/151 [00:07<00:25,  4.57it/s]Epoch: 5, train for the 36-th batch, train loss: 0.6153182983398438:  24%|███          | 36/151 [00:07<00:24,  4.61it/s]Epoch: 3, train for the 68-th batch, train loss: 0.17534275352954865:  56%|██████▊     | 67/119 [00:40<00:31,  1.67it/s]Epoch: 3, train for the 68-th batch, train loss: 0.17534275352954865:  57%|██████▊     | 68/119 [00:40<00:30,  1.65it/s]Epoch: 5, train for the 37-th batch, train loss: 0.6277203559875488:  24%|███          | 36/151 [00:07<00:24,  4.61it/s]Epoch: 5, train for the 37-th batch, train loss: 0.6277203559875488:  25%|███▏         | 37/151 [00:07<00:24,  4.62it/s]Epoch: 3, train for the 16-th batch, train loss: 0.4417891204357147:  10%|█▎           | 15/146 [00:09<01:16,  1.71it/s]Epoch: 3, train for the 16-th batch, train loss: 0.4417891204357147:  11%|█▍           | 16/146 [00:09<01:17,  1.68it/s]Epoch: 2, train for the 87-th batch, train loss: 0.6389755606651306:  36%|████▋        | 86/237 [00:49<01:35,  1.58it/s]Epoch: 2, train for the 87-th batch, train loss: 0.6389755606651306:  37%|████▊        | 87/237 [00:49<01:35,  1.57it/s]Epoch: 1, train for the 361-th batch, train loss: 0.4796343743801117:  94%|██████████▎| 360/383 [03:36<00:14,  1.58it/s]Epoch: 1, train for the 361-th batch, train loss: 0.4796343743801117:  94%|██████████▎| 361/383 [03:36<00:14,  1.57it/s]Epoch: 5, train for the 38-th batch, train loss: 0.6668328642845154:  25%|███▏         | 37/151 [00:07<00:24,  4.62it/s]Epoch: 5, train for the 38-th batch, train loss: 0.6668328642845154:  25%|███▎         | 38/151 [00:07<00:24,  4.59it/s]Epoch: 5, train for the 39-th batch, train loss: 0.5924208760261536:  25%|███▎         | 38/151 [00:08<00:24,  4.59it/s]Epoch: 5, train for the 39-th batch, train loss: 0.5924208760261536:  26%|███▎         | 39/151 [00:08<00:24,  4.62it/s]Epoch: 3, train for the 69-th batch, train loss: 0.19978030025959015:  57%|██████▊     | 68/119 [00:40<00:30,  1.65it/s]Epoch: 3, train for the 69-th batch, train loss: 0.19978030025959015:  58%|██████▉     | 69/119 [00:40<00:30,  1.65it/s]Epoch: 3, train for the 17-th batch, train loss: 0.4188756048679352:  11%|█▍           | 16/146 [00:09<01:17,  1.68it/s]Epoch: 3, train for the 17-th batch, train loss: 0.4188756048679352:  12%|█▌           | 17/146 [00:09<01:17,  1.68it/s]Epoch: 5, train for the 40-th batch, train loss: 0.636347770690918:  26%|███▌          | 39/151 [00:08<00:24,  4.62it/s]Epoch: 5, train for the 40-th batch, train loss: 0.636347770690918:  26%|███▋          | 40/151 [00:08<00:24,  4.59it/s]Epoch: 2, train for the 88-th batch, train loss: 0.6536144614219666:  37%|████▊        | 87/237 [00:50<01:35,  1.57it/s]Epoch: 1, train for the 362-th batch, train loss: 0.4402681589126587:  94%|██████████▎| 361/383 [03:36<00:14,  1.57it/s]Epoch: 2, train for the 88-th batch, train loss: 0.6536144614219666:  37%|████▊        | 88/237 [00:50<01:35,  1.56it/s]Epoch: 1, train for the 362-th batch, train loss: 0.4402681589126587:  95%|██████████▍| 362/383 [03:36<00:13,  1.56it/s]Epoch: 5, train for the 41-th batch, train loss: 0.6420441269874573:  26%|███▍         | 40/151 [00:08<00:24,  4.59it/s]Epoch: 5, train for the 41-th batch, train loss: 0.6420441269874573:  27%|███▌         | 41/151 [00:08<00:24,  4.57it/s]Epoch: 3, train for the 70-th batch, train loss: 0.15738508105278015:  58%|██████▉     | 69/119 [00:41<00:30,  1.65it/s]Epoch: 3, train for the 70-th batch, train loss: 0.15738508105278015:  59%|███████     | 70/119 [00:41<00:29,  1.66it/s]Epoch: 5, train for the 42-th batch, train loss: 0.5629555583000183:  27%|███▌         | 41/151 [00:08<00:24,  4.57it/s]Epoch: 5, train for the 42-th batch, train loss: 0.5629555583000183:  28%|███▌         | 42/151 [00:08<00:23,  4.59it/s]Epoch: 3, train for the 18-th batch, train loss: 0.456927627325058:  12%|█▋            | 17/146 [00:10<01:17,  1.68it/s]Epoch: 3, train for the 18-th batch, train loss: 0.456927627325058:  12%|█▋            | 18/146 [00:10<01:15,  1.69it/s]Epoch: 5, train for the 43-th batch, train loss: 0.5941481590270996:  28%|███▌         | 42/151 [00:08<00:23,  4.59it/s]Epoch: 5, train for the 43-th batch, train loss: 0.5941481590270996:  28%|███▋         | 43/151 [00:08<00:23,  4.61it/s]Epoch: 2, train for the 89-th batch, train loss: 0.594940185546875:  37%|█████▏        | 88/237 [00:51<01:35,  1.56it/s]Epoch: 2, train for the 89-th batch, train loss: 0.594940185546875:  38%|█████▎        | 89/237 [00:51<01:34,  1.56it/s]Epoch: 1, train for the 363-th batch, train loss: 0.49373388290405273:  95%|█████████▍| 362/383 [03:37<00:13,  1.56it/s]Epoch: 1, train for the 363-th batch, train loss: 0.49373388290405273:  95%|█████████▍| 363/383 [03:37<00:12,  1.56it/s]Epoch: 5, train for the 44-th batch, train loss: 0.5687458515167236:  28%|███▋         | 43/151 [00:09<00:23,  4.61it/s]Epoch: 5, train for the 44-th batch, train loss: 0.5687458515167236:  29%|███▊         | 44/151 [00:09<00:23,  4.61it/s]Epoch: 3, train for the 71-th batch, train loss: 0.2389330118894577:  59%|███████▋     | 70/119 [00:41<00:29,  1.66it/s]Epoch: 3, train for the 71-th batch, train loss: 0.2389330118894577:  60%|███████▊     | 71/119 [00:41<00:28,  1.67it/s]Epoch: 5, train for the 45-th batch, train loss: 0.48205384612083435:  29%|███▍        | 44/151 [00:09<00:23,  4.61it/s]Epoch: 5, train for the 45-th batch, train loss: 0.48205384612083435:  30%|███▌        | 45/151 [00:09<00:22,  4.65it/s]Epoch: 3, train for the 19-th batch, train loss: 0.45987555384635925:  12%|█▍          | 18/146 [00:11<01:15,  1.69it/s]Epoch: 3, train for the 19-th batch, train loss: 0.45987555384635925:  13%|█▌          | 19/146 [00:11<01:15,  1.68it/s]Epoch: 5, train for the 46-th batch, train loss: 0.5907419323921204:  30%|███▊         | 45/151 [00:09<00:22,  4.65it/s]Epoch: 5, train for the 46-th batch, train loss: 0.5907419323921204:  30%|███▉         | 46/151 [00:09<00:22,  4.64it/s]Epoch: 2, train for the 90-th batch, train loss: 0.6258482933044434:  38%|████▉        | 89/237 [00:51<01:34,  1.56it/s]Epoch: 2, train for the 90-th batch, train loss: 0.6258482933044434:  38%|████▉        | 90/237 [00:51<01:34,  1.55it/s]Epoch: 1, train for the 364-th batch, train loss: 0.46040910482406616:  95%|█████████▍| 363/383 [03:38<00:12,  1.56it/s]Epoch: 1, train for the 364-th batch, train loss: 0.46040910482406616:  95%|█████████▌| 364/383 [03:38<00:12,  1.55it/s]Epoch: 5, train for the 47-th batch, train loss: 0.6535688638687134:  30%|███▉         | 46/151 [00:09<00:22,  4.64it/s]Epoch: 5, train for the 47-th batch, train loss: 0.6535688638687134:  31%|████         | 47/151 [00:09<00:22,  4.60it/s]Epoch: 3, train for the 72-th batch, train loss: 0.20818011462688446:  60%|███████▏    | 71/119 [00:42<00:28,  1.67it/s]Epoch: 3, train for the 72-th batch, train loss: 0.20818011462688446:  61%|███████▎    | 72/119 [00:42<00:28,  1.67it/s]Epoch: 3, train for the 20-th batch, train loss: 0.4709226191043854:  13%|█▋           | 19/146 [00:11<01:15,  1.68it/s]Epoch: 3, train for the 20-th batch, train loss: 0.4709226191043854:  14%|█▊           | 20/146 [00:11<01:15,  1.68it/s]Epoch: 5, train for the 48-th batch, train loss: 0.5864728689193726:  31%|████         | 47/151 [00:10<00:22,  4.60it/s]Epoch: 5, train for the 48-th batch, train loss: 0.5864728689193726:  32%|████▏        | 48/151 [00:10<00:22,  4.57it/s]Epoch: 2, train for the 91-th batch, train loss: 0.5772338509559631:  38%|████▉        | 90/237 [00:52<01:34,  1.55it/s]Epoch: 2, train for the 91-th batch, train loss: 0.5772338509559631:  38%|████▉        | 91/237 [00:52<01:33,  1.56it/s]Epoch: 1, train for the 365-th batch, train loss: 0.4629380702972412:  95%|██████████▍| 364/383 [03:38<00:12,  1.55it/s]Epoch: 1, train for the 365-th batch, train loss: 0.4629380702972412:  95%|██████████▍| 365/383 [03:38<00:11,  1.55it/s]Epoch: 5, train for the 49-th batch, train loss: 0.6115158200263977:  32%|████▏        | 48/151 [00:10<00:22,  4.57it/s]Epoch: 5, train for the 49-th batch, train loss: 0.6115158200263977:  32%|████▏        | 49/151 [00:10<00:22,  4.56it/s]Epoch: 5, train for the 50-th batch, train loss: 0.5663663148880005:  32%|████▏        | 49/151 [00:10<00:22,  4.56it/s]Epoch: 5, train for the 50-th batch, train loss: 0.5663663148880005:  33%|████▎        | 50/151 [00:10<00:22,  4.55it/s]Epoch: 3, train for the 73-th batch, train loss: 0.2081257551908493:  61%|███████▊     | 72/119 [00:43<00:28,  1.67it/s]Epoch: 3, train for the 73-th batch, train loss: 0.2081257551908493:  61%|███████▉     | 73/119 [00:43<00:27,  1.67it/s]Epoch: 3, train for the 21-th batch, train loss: 0.4407985508441925:  14%|█▊           | 20/146 [00:12<01:15,  1.68it/s]Epoch: 3, train for the 21-th batch, train loss: 0.4407985508441925:  14%|█▊           | 21/146 [00:12<01:14,  1.68it/s]Epoch: 5, train for the 51-th batch, train loss: 0.6402316689491272:  33%|████▎        | 50/151 [00:10<00:22,  4.55it/s]Epoch: 5, train for the 51-th batch, train loss: 0.6402316689491272:  34%|████▍        | 51/151 [00:10<00:22,  4.54it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5926901698112488:  38%|████▉        | 91/237 [00:53<01:33,  1.56it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5926901698112488:  39%|█████        | 92/237 [00:53<01:33,  1.56it/s]Epoch: 1, train for the 366-th batch, train loss: 0.48194169998168945:  95%|█████████▌| 365/383 [03:39<00:11,  1.55it/s]Epoch: 1, train for the 366-th batch, train loss: 0.48194169998168945:  96%|█████████▌| 366/383 [03:39<00:10,  1.56it/s]Epoch: 5, train for the 52-th batch, train loss: 0.6240357756614685:  34%|████▍        | 51/151 [00:10<00:22,  4.54it/s]Epoch: 5, train for the 52-th batch, train loss: 0.6240357756614685:  34%|████▍        | 52/151 [00:10<00:21,  4.53it/s]Epoch: 3, train for the 74-th batch, train loss: 0.250253289937973:  61%|████████▌     | 73/119 [00:43<00:27,  1.67it/s]Epoch: 3, train for the 74-th batch, train loss: 0.250253289937973:  62%|████████▋     | 74/119 [00:43<00:26,  1.68it/s]Epoch: 5, train for the 53-th batch, train loss: 0.6244707107543945:  34%|████▍        | 52/151 [00:11<00:21,  4.53it/s]Epoch: 5, train for the 53-th batch, train loss: 0.6244707107543945:  35%|████▌        | 53/151 [00:11<00:21,  4.52it/s]Epoch: 3, train for the 22-th batch, train loss: 0.46934065222740173:  14%|█▋          | 21/146 [00:12<01:14,  1.68it/s]Epoch: 3, train for the 22-th batch, train loss: 0.46934065222740173:  15%|█▊          | 22/146 [00:12<01:13,  1.69it/s]Epoch: 5, train for the 54-th batch, train loss: 0.5886271595954895:  35%|████▌        | 53/151 [00:11<00:21,  4.52it/s]Epoch: 5, train for the 54-th batch, train loss: 0.5886271595954895:  36%|████▋        | 54/151 [00:11<00:21,  4.51it/s]Epoch: 2, train for the 93-th batch, train loss: 0.6185140609741211:  39%|█████        | 92/237 [00:53<01:33,  1.56it/s]Epoch: 2, train for the 93-th batch, train loss: 0.6185140609741211:  39%|█████        | 93/237 [00:53<01:32,  1.55it/s]Epoch: 1, train for the 367-th batch, train loss: 0.43129271268844604:  96%|█████████▌| 366/383 [03:40<00:10,  1.56it/s]Epoch: 1, train for the 367-th batch, train loss: 0.43129271268844604:  96%|█████████▌| 367/383 [03:40<00:10,  1.55it/s]Epoch: 5, train for the 55-th batch, train loss: 0.49979934096336365:  36%|████▎       | 54/151 [00:11<00:21,  4.51it/s]Epoch: 5, train for the 55-th batch, train loss: 0.49979934096336365:  36%|████▎       | 55/151 [00:11<00:21,  4.54it/s]Epoch: 3, train for the 75-th batch, train loss: 0.20402821898460388:  62%|███████▍    | 74/119 [00:44<00:26,  1.68it/s]Epoch: 3, train for the 75-th batch, train loss: 0.20402821898460388:  63%|███████▌    | 75/119 [00:44<00:26,  1.68it/s]Epoch: 3, train for the 23-th batch, train loss: 0.47633102536201477:  15%|█▊          | 22/146 [00:13<01:13,  1.69it/s]Epoch: 3, train for the 23-th batch, train loss: 0.47633102536201477:  16%|█▉          | 23/146 [00:13<01:12,  1.69it/s]Epoch: 5, train for the 56-th batch, train loss: 0.5040673613548279:  36%|████▋        | 55/151 [00:11<00:21,  4.54it/s]Epoch: 5, train for the 56-th batch, train loss: 0.5040673613548279:  37%|████▊        | 56/151 [00:11<00:21,  4.52it/s]Epoch: 5, train for the 57-th batch, train loss: 0.5579683780670166:  37%|████▊        | 56/151 [00:12<00:21,  4.52it/s]Epoch: 5, train for the 57-th batch, train loss: 0.5579683780670166:  38%|████▉        | 57/151 [00:12<00:20,  4.51it/s]Epoch: 2, train for the 94-th batch, train loss: 0.6404098272323608:  39%|█████        | 93/237 [00:54<01:32,  1.55it/s]Epoch: 2, train for the 94-th batch, train loss: 0.6404098272323608:  40%|█████▏       | 94/237 [00:54<01:32,  1.55it/s]Epoch: 1, train for the 368-th batch, train loss: 0.4981699287891388:  96%|██████████▌| 367/383 [03:40<00:10,  1.55it/s]Epoch: 1, train for the 368-th batch, train loss: 0.4981699287891388:  96%|██████████▌| 368/383 [03:40<00:09,  1.55it/s]Epoch: 5, train for the 58-th batch, train loss: 0.5505694150924683:  38%|████▉        | 57/151 [00:12<00:20,  4.51it/s]Epoch: 5, train for the 58-th batch, train loss: 0.5505694150924683:  38%|████▉        | 58/151 [00:12<00:20,  4.52it/s]Epoch: 3, train for the 76-th batch, train loss: 0.24156272411346436:  63%|███████▌    | 75/119 [00:44<00:26,  1.68it/s]Epoch: 3, train for the 76-th batch, train loss: 0.24156272411346436:  64%|███████▋    | 76/119 [00:44<00:25,  1.69it/s]Epoch: 3, train for the 24-th batch, train loss: 0.44787710905075073:  16%|█▉          | 23/146 [00:14<01:12,  1.69it/s]Epoch: 3, train for the 24-th batch, train loss: 0.44787710905075073:  16%|█▉          | 24/146 [00:14<01:12,  1.68it/s]Epoch: 5, train for the 59-th batch, train loss: 0.5261707901954651:  38%|████▉        | 58/151 [00:12<00:20,  4.52it/s]Epoch: 5, train for the 59-th batch, train loss: 0.5261707901954651:  39%|█████        | 59/151 [00:12<00:20,  4.51it/s]Epoch: 5, train for the 60-th batch, train loss: 0.5325796604156494:  39%|█████        | 59/151 [00:12<00:20,  4.51it/s]Epoch: 5, train for the 60-th batch, train loss: 0.5325796604156494:  40%|█████▏       | 60/151 [00:12<00:20,  4.48it/s]Epoch: 2, train for the 95-th batch, train loss: 0.5384455919265747:  40%|█████▏       | 94/237 [00:54<01:32,  1.55it/s]Epoch: 2, train for the 95-th batch, train loss: 0.5384455919265747:  40%|█████▏       | 95/237 [00:54<01:30,  1.56it/s]Epoch: 1, train for the 369-th batch, train loss: 0.44229385256767273:  96%|█████████▌| 368/383 [03:41<00:09,  1.55it/s]Epoch: 1, train for the 369-th batch, train loss: 0.44229385256767273:  96%|█████████▋| 369/383 [03:41<00:09,  1.55it/s]Epoch: 3, train for the 25-th batch, train loss: 0.5038928985595703:  16%|██▏          | 24/146 [00:14<01:12,  1.68it/s]Epoch: 3, train for the 25-th batch, train loss: 0.5038928985595703:  17%|██▏          | 25/146 [00:14<01:08,  1.78it/s]Epoch: 5, train for the 61-th batch, train loss: 0.5502630472183228:  40%|█████▏       | 60/151 [00:12<00:20,  4.48it/s]Epoch: 5, train for the 61-th batch, train loss: 0.5502630472183228:  40%|█████▎       | 61/151 [00:12<00:20,  4.47it/s]Epoch: 3, train for the 77-th batch, train loss: 0.1963375061750412:  64%|████████▎    | 76/119 [00:45<00:25,  1.69it/s]Epoch: 3, train for the 77-th batch, train loss: 0.1963375061750412:  65%|████████▍    | 77/119 [00:45<00:26,  1.58it/s]Epoch: 5, train for the 62-th batch, train loss: 0.580120325088501:  40%|█████▋        | 61/151 [00:13<00:20,  4.47it/s]Epoch: 5, train for the 62-th batch, train loss: 0.580120325088501:  41%|█████▋        | 62/151 [00:13<00:19,  4.47it/s]Epoch: 5, train for the 63-th batch, train loss: 0.5350656509399414:  41%|█████▎       | 62/151 [00:13<00:19,  4.47it/s]Epoch: 5, train for the 63-th batch, train loss: 0.5350656509399414:  42%|█████▍       | 63/151 [00:13<00:19,  4.47it/s]Epoch: 3, train for the 26-th batch, train loss: 0.4077593982219696:  17%|██▏          | 25/146 [00:15<01:08,  1.78it/s]Epoch: 3, train for the 26-th batch, train loss: 0.4077593982219696:  18%|██▎          | 26/146 [00:15<01:07,  1.79it/s]Epoch: 2, train for the 96-th batch, train loss: 0.5741881728172302:  40%|█████▏       | 95/237 [00:55<01:30,  1.56it/s]Epoch: 2, train for the 96-th batch, train loss: 0.5741881728172302:  41%|█████▎       | 96/237 [00:55<01:30,  1.56it/s]Epoch: 1, train for the 370-th batch, train loss: 0.459471195936203:  96%|███████████▌| 369/383 [03:41<00:09,  1.55it/s]Epoch: 1, train for the 370-th batch, train loss: 0.459471195936203:  97%|███████████▌| 370/383 [03:41<00:08,  1.57it/s]Epoch: 3, train for the 78-th batch, train loss: 0.20974288880825043:  65%|███████▊    | 77/119 [00:46<00:26,  1.58it/s]Epoch: 3, train for the 78-th batch, train loss: 0.20974288880825043:  66%|███████▊    | 78/119 [00:46<00:25,  1.60it/s]Epoch: 5, train for the 64-th batch, train loss: 0.5177678465843201:  42%|█████▍       | 63/151 [00:13<00:19,  4.47it/s]Epoch: 5, train for the 64-th batch, train loss: 0.5177678465843201:  42%|█████▌       | 64/151 [00:13<00:19,  4.46it/s]Epoch: 5, train for the 65-th batch, train loss: 0.475930780172348:  42%|█████▉        | 64/151 [00:13<00:19,  4.46it/s]Epoch: 5, train for the 65-th batch, train loss: 0.475930780172348:  43%|██████        | 65/151 [00:13<00:19,  4.46it/s]Epoch: 3, train for the 27-th batch, train loss: 0.44234901666641235:  18%|██▏         | 26/146 [00:15<01:07,  1.79it/s]Epoch: 3, train for the 27-th batch, train loss: 0.44234901666641235:  18%|██▏         | 27/146 [00:15<01:07,  1.76it/s]Epoch: 5, train for the 66-th batch, train loss: 0.5375961065292358:  43%|█████▌       | 65/151 [00:14<00:19,  4.46it/s]Epoch: 5, train for the 66-th batch, train loss: 0.5375961065292358:  44%|█████▋       | 66/151 [00:14<00:19,  4.45it/s]Epoch: 1, train for the 371-th batch, train loss: 0.46228766441345215:  97%|█████████▋| 370/383 [03:42<00:08,  1.57it/s]Epoch: 1, train for the 371-th batch, train loss: 0.46228766441345215:  97%|█████████▋| 371/383 [03:42<00:07,  1.56it/s]Epoch: 2, train for the 97-th batch, train loss: 0.6181402206420898:  41%|█████▎       | 96/237 [00:56<01:30,  1.56it/s]Epoch: 2, train for the 97-th batch, train loss: 0.6181402206420898:  41%|█████▎       | 97/237 [00:56<01:30,  1.55it/s]Epoch: 3, train for the 79-th batch, train loss: 0.2010628879070282:  66%|████████▌    | 78/119 [00:46<00:25,  1.60it/s]Epoch: 3, train for the 79-th batch, train loss: 0.2010628879070282:  66%|████████▋    | 79/119 [00:46<00:24,  1.62it/s]Epoch: 5, train for the 67-th batch, train loss: 0.5630831122398376:  44%|█████▋       | 66/151 [00:14<00:19,  4.45it/s]Epoch: 5, train for the 67-th batch, train loss: 0.5630831122398376:  44%|█████▊       | 67/151 [00:14<00:19,  4.40it/s]Epoch: 5, train for the 68-th batch, train loss: 0.5702035427093506:  44%|█████▊       | 67/151 [00:14<00:19,  4.40it/s]Epoch: 5, train for the 68-th batch, train loss: 0.5702035427093506:  45%|█████▊       | 68/151 [00:14<00:18,  4.42it/s]Epoch: 3, train for the 28-th batch, train loss: 0.45896780490875244:  18%|██▏         | 27/146 [00:16<01:07,  1.76it/s]Epoch: 3, train for the 28-th batch, train loss: 0.45896780490875244:  19%|██▎         | 28/146 [00:16<01:09,  1.71it/s]Epoch: 5, train for the 69-th batch, train loss: 0.4496525526046753:  45%|█████▊       | 68/151 [00:14<00:18,  4.42it/s]Epoch: 5, train for the 69-th batch, train loss: 0.4496525526046753:  46%|█████▉       | 69/151 [00:14<00:18,  4.46it/s]Epoch: 1, train for the 372-th batch, train loss: 0.5302001237869263:  97%|██████████▋| 371/383 [03:43<00:07,  1.56it/s]Epoch: 1, train for the 372-th batch, train loss: 0.5302001237869263:  97%|██████████▋| 372/383 [03:43<00:07,  1.55it/s]Epoch: 2, train for the 98-th batch, train loss: 0.6033536195755005:  41%|█████▎       | 97/237 [00:56<01:30,  1.55it/s]Epoch: 2, train for the 98-th batch, train loss: 0.6033536195755005:  41%|█████▍       | 98/237 [00:56<01:29,  1.55it/s]Epoch: 3, train for the 80-th batch, train loss: 0.2352510392665863:  66%|████████▋    | 79/119 [00:47<00:24,  1.62it/s]Epoch: 3, train for the 80-th batch, train loss: 0.2352510392665863:  67%|████████▋    | 80/119 [00:47<00:24,  1.62it/s]Epoch: 5, train for the 70-th batch, train loss: 0.5456570386886597:  46%|█████▉       | 69/151 [00:14<00:18,  4.46it/s]Epoch: 5, train for the 70-th batch, train loss: 0.5456570386886597:  46%|██████       | 70/151 [00:14<00:18,  4.47it/s]Epoch: 5, train for the 71-th batch, train loss: 0.5490459203720093:  46%|██████       | 70/151 [00:15<00:18,  4.47it/s]Epoch: 5, train for the 71-th batch, train loss: 0.5490459203720093:  47%|██████       | 71/151 [00:15<00:17,  4.46it/s]Epoch: 3, train for the 29-th batch, train loss: 0.40873000025749207:  19%|██▎         | 28/146 [00:16<01:09,  1.71it/s]Epoch: 3, train for the 29-th batch, train loss: 0.40873000025749207:  20%|██▍         | 29/146 [00:16<01:09,  1.68it/s]Epoch: 5, train for the 72-th batch, train loss: 0.5476804971694946:  47%|██████       | 71/151 [00:15<00:17,  4.46it/s]Epoch: 5, train for the 72-th batch, train loss: 0.5476804971694946:  48%|██████▏      | 72/151 [00:15<00:17,  4.46it/s]Epoch: 1, train for the 373-th batch, train loss: 0.501507043838501:  97%|███████████▋| 372/383 [03:43<00:07,  1.55it/s]Epoch: 1, train for the 373-th batch, train loss: 0.501507043838501:  97%|███████████▋| 373/383 [03:43<00:06,  1.55it/s]Epoch: 2, train for the 99-th batch, train loss: 0.615336000919342:  41%|█████▊        | 98/237 [00:57<01:29,  1.55it/s]Epoch: 2, train for the 99-th batch, train loss: 0.615336000919342:  42%|█████▊        | 99/237 [00:57<01:29,  1.55it/s]Epoch: 3, train for the 81-th batch, train loss: 0.21560348570346832:  67%|████████    | 80/119 [00:48<00:24,  1.62it/s]Epoch: 3, train for the 81-th batch, train loss: 0.21560348570346832:  68%|████████▏   | 81/119 [00:48<00:23,  1.62it/s]Epoch: 5, train for the 73-th batch, train loss: 0.5555395483970642:  48%|██████▏      | 72/151 [00:15<00:17,  4.46it/s]Epoch: 5, train for the 73-th batch, train loss: 0.5555395483970642:  48%|██████▎      | 73/151 [00:15<00:17,  4.45it/s]Epoch: 5, train for the 74-th batch, train loss: 0.5309422612190247:  48%|██████▎      | 73/151 [00:15<00:17,  4.45it/s]Epoch: 5, train for the 74-th batch, train loss: 0.5309422612190247:  49%|██████▎      | 74/151 [00:15<00:17,  4.44it/s]Epoch: 3, train for the 30-th batch, train loss: 0.4441121816635132:  20%|██▌          | 29/146 [00:17<01:09,  1.68it/s]Epoch: 3, train for the 30-th batch, train loss: 0.4441121816635132:  21%|██▋          | 30/146 [00:17<01:09,  1.66it/s]Epoch: 1, train for the 374-th batch, train loss: 0.4545760154724121:  97%|██████████▋| 373/383 [03:44<00:06,  1.55it/s]Epoch: 1, train for the 374-th batch, train loss: 0.4545760154724121:  98%|██████████▋| 374/383 [03:44<00:05,  1.55it/s]Epoch: 2, train for the 100-th batch, train loss: 0.6123038530349731:  42%|█████       | 99/237 [00:58<01:29,  1.55it/s]Epoch: 2, train for the 100-th batch, train loss: 0.6123038530349731:  42%|████▋      | 100/237 [00:58<01:28,  1.55it/s]Epoch: 3, train for the 82-th batch, train loss: 0.224139004945755:  68%|█████████▌    | 81/119 [00:48<00:23,  1.62it/s]Epoch: 3, train for the 82-th batch, train loss: 0.224139004945755:  69%|█████████▋    | 82/119 [00:48<00:22,  1.62it/s]Epoch: 5, train for the 75-th batch, train loss: 0.5317457318305969:  49%|██████▎      | 74/151 [00:16<00:17,  4.44it/s]Epoch: 5, train for the 75-th batch, train loss: 0.5317457318305969:  50%|██████▍      | 75/151 [00:16<00:17,  4.42it/s]Epoch: 5, train for the 76-th batch, train loss: 0.5817712545394897:  50%|██████▍      | 75/151 [00:16<00:17,  4.42it/s]Epoch: 5, train for the 76-th batch, train loss: 0.5817712545394897:  50%|██████▌      | 76/151 [00:16<00:16,  4.42it/s]Epoch: 3, train for the 31-th batch, train loss: 0.5090962648391724:  21%|██▋          | 30/146 [00:18<01:09,  1.66it/s]Epoch: 3, train for the 31-th batch, train loss: 0.5090962648391724:  21%|██▊          | 31/146 [00:18<01:09,  1.64it/s]Epoch: 5, train for the 77-th batch, train loss: 0.5582309365272522:  50%|██████▌      | 76/151 [00:16<00:16,  4.42it/s]Epoch: 5, train for the 77-th batch, train loss: 0.5582309365272522:  51%|██████▋      | 77/151 [00:16<00:16,  4.42it/s]Epoch: 3, train for the 83-th batch, train loss: 0.20464523136615753:  69%|████████▎   | 82/119 [00:49<00:22,  1.62it/s]Epoch: 3, train for the 83-th batch, train loss: 0.20464523136615753:  70%|████████▎   | 83/119 [00:49<00:22,  1.61it/s]Epoch: 1, train for the 375-th batch, train loss: 0.542044997215271:  98%|███████████▋| 374/383 [03:45<00:05,  1.55it/s]Epoch: 1, train for the 375-th batch, train loss: 0.542044997215271:  98%|███████████▋| 375/383 [03:45<00:05,  1.55it/s]Epoch: 2, train for the 101-th batch, train loss: 0.6114944815635681:  42%|████▋      | 100/237 [00:58<01:28,  1.55it/s]Epoch: 2, train for the 101-th batch, train loss: 0.6114944815635681:  43%|████▋      | 101/237 [00:58<01:27,  1.55it/s]Epoch: 5, train for the 78-th batch, train loss: 0.4611901640892029:  51%|██████▋      | 77/151 [00:16<00:16,  4.42it/s]Epoch: 5, train for the 78-th batch, train loss: 0.4611901640892029:  52%|██████▋      | 78/151 [00:16<00:16,  4.46it/s]Epoch: 5, train for the 79-th batch, train loss: 0.5397284030914307:  52%|██████▋      | 78/151 [00:16<00:16,  4.46it/s]Epoch: 5, train for the 79-th batch, train loss: 0.5397284030914307:  52%|██████▊      | 79/151 [00:16<00:16,  4.48it/s]Epoch: 3, train for the 32-th batch, train loss: 0.4480535089969635:  21%|██▊          | 31/146 [00:18<01:09,  1.64it/s]Epoch: 3, train for the 32-th batch, train loss: 0.4480535089969635:  22%|██▊          | 32/146 [00:18<01:09,  1.64it/s]Epoch: 5, train for the 80-th batch, train loss: 0.48242777585983276:  52%|██████▎     | 79/151 [00:17<00:16,  4.48it/s]Epoch: 5, train for the 80-th batch, train loss: 0.48242777585983276:  53%|██████▎     | 80/151 [00:17<00:15,  4.48it/s]Epoch: 3, train for the 84-th batch, train loss: 0.17909717559814453:  70%|████████▎   | 83/119 [00:49<00:22,  1.61it/s]Epoch: 3, train for the 84-th batch, train loss: 0.17909717559814453:  71%|████████▍   | 84/119 [00:49<00:21,  1.62it/s]Epoch: 1, train for the 376-th batch, train loss: 0.40239474177360535:  98%|█████████▊| 375/383 [03:45<00:05,  1.55it/s]Epoch: 1, train for the 376-th batch, train loss: 0.40239474177360535:  98%|█████████▊| 376/383 [03:45<00:04,  1.55it/s]Epoch: 2, train for the 102-th batch, train loss: 0.6313026547431946:  43%|████▋      | 101/237 [00:59<01:27,  1.55it/s]Epoch: 2, train for the 102-th batch, train loss: 0.6313026547431946:  43%|████▋      | 102/237 [00:59<01:27,  1.55it/s]Epoch: 5, train for the 81-th batch, train loss: 0.5725383162498474:  53%|██████▉      | 80/151 [00:17<00:15,  4.48it/s]Epoch: 5, train for the 81-th batch, train loss: 0.5725383162498474:  54%|██████▉      | 81/151 [00:17<00:15,  4.45it/s]Epoch: 5, train for the 82-th batch, train loss: 0.5766823291778564:  54%|██████▉      | 81/151 [00:17<00:15,  4.45it/s]Epoch: 5, train for the 82-th batch, train loss: 0.5766823291778564:  54%|███████      | 82/151 [00:17<00:15,  4.45it/s]Epoch: 3, train for the 33-th batch, train loss: 0.4610614776611328:  22%|██▊          | 32/146 [00:19<01:09,  1.64it/s]Epoch: 3, train for the 33-th batch, train loss: 0.4610614776611328:  23%|██▉          | 33/146 [00:19<01:08,  1.64it/s]Epoch: 5, train for the 83-th batch, train loss: 0.5549904704093933:  54%|███████      | 82/151 [00:17<00:15,  4.45it/s]Epoch: 5, train for the 83-th batch, train loss: 0.5549904704093933:  55%|███████▏     | 83/151 [00:17<00:15,  4.45it/s]Epoch: 3, train for the 85-th batch, train loss: 0.22384455800056458:  71%|████████▍   | 84/119 [00:50<00:21,  1.62it/s]Epoch: 3, train for the 85-th batch, train loss: 0.22384455800056458:  71%|████████▌   | 85/119 [00:50<00:20,  1.63it/s]Epoch: 1, train for the 377-th batch, train loss: 0.5117421746253967:  98%|██████████▊| 376/383 [03:46<00:04,  1.55it/s]Epoch: 1, train for the 377-th batch, train loss: 0.5117421746253967:  98%|██████████▊| 377/383 [03:46<00:03,  1.55it/s]Epoch: 2, train for the 103-th batch, train loss: 0.6522729992866516:  43%|████▋      | 102/237 [01:00<01:27,  1.55it/s]Epoch: 2, train for the 103-th batch, train loss: 0.6522729992866516:  43%|████▊      | 103/237 [01:00<01:26,  1.55it/s]Epoch: 5, train for the 84-th batch, train loss: 0.5582338571548462:  55%|███████▏     | 83/151 [00:18<00:15,  4.45it/s]Epoch: 5, train for the 84-th batch, train loss: 0.5582338571548462:  56%|███████▏     | 84/151 [00:18<00:16,  4.01it/s]Epoch: 3, train for the 34-th batch, train loss: 0.4896676242351532:  23%|██▉          | 33/146 [00:19<01:08,  1.64it/s]Epoch: 3, train for the 34-th batch, train loss: 0.4896676242351532:  23%|███          | 34/146 [00:19<01:08,  1.63it/s]Epoch: 5, train for the 85-th batch, train loss: 0.5832934379577637:  56%|███████▏     | 84/151 [00:18<00:16,  4.01it/s]Epoch: 5, train for the 85-th batch, train loss: 0.5832934379577637:  56%|███████▎     | 85/151 [00:18<00:15,  4.13it/s]Epoch: 3, train for the 86-th batch, train loss: 0.2461421936750412:  71%|█████████▎   | 85/119 [00:51<00:20,  1.63it/s]Epoch: 3, train for the 86-th batch, train loss: 0.2461421936750412:  72%|█████████▍   | 86/119 [00:51<00:20,  1.62it/s]Epoch: 5, train for the 86-th batch, train loss: 0.5445817708969116:  56%|███████▎     | 85/151 [00:18<00:15,  4.13it/s]Epoch: 5, train for the 86-th batch, train loss: 0.5445817708969116:  57%|███████▍     | 86/151 [00:18<00:15,  4.22it/s]Epoch: 1, train for the 378-th batch, train loss: 0.49925535917282104:  98%|█████████▊| 377/383 [03:47<00:03,  1.55it/s]Epoch: 1, train for the 378-th batch, train loss: 0.49925535917282104:  99%|█████████▊| 378/383 [03:47<00:03,  1.55it/s]Epoch: 2, train for the 104-th batch, train loss: 0.6464868187904358:  43%|████▊      | 103/237 [01:00<01:26,  1.55it/s]Epoch: 2, train for the 104-th batch, train loss: 0.6464868187904358:  44%|████▊      | 104/237 [01:00<01:25,  1.55it/s]Epoch: 5, train for the 87-th batch, train loss: 0.5772501230239868:  57%|███████▍     | 86/151 [00:18<00:15,  4.22it/s]Epoch: 5, train for the 87-th batch, train loss: 0.5772501230239868:  58%|███████▍     | 87/151 [00:18<00:14,  4.29it/s]Epoch: 3, train for the 35-th batch, train loss: 0.42159533500671387:  23%|██▊         | 34/146 [00:20<01:08,  1.63it/s]Epoch: 3, train for the 35-th batch, train loss: 0.42159533500671387:  24%|██▉         | 35/146 [00:20<01:08,  1.63it/s]Epoch: 5, train for the 88-th batch, train loss: 0.5978838801383972:  58%|███████▍     | 87/151 [00:19<00:14,  4.29it/s]Epoch: 5, train for the 88-th batch, train loss: 0.5978838801383972:  58%|███████▌     | 88/151 [00:19<00:14,  4.35it/s]Epoch: 3, train for the 87-th batch, train loss: 0.2050977349281311:  72%|█████████▍   | 86/119 [00:51<00:20,  1.62it/s]Epoch: 3, train for the 87-th batch, train loss: 0.2050977349281311:  73%|█████████▌   | 87/119 [00:51<00:19,  1.62it/s]Epoch: 1, train for the 379-th batch, train loss: 0.4436939060688019:  99%|██████████▊| 378/383 [03:47<00:03,  1.55it/s]Epoch: 2, train for the 105-th batch, train loss: 0.6366056203842163:  44%|████▊      | 104/237 [01:01<01:25,  1.55it/s]Epoch: 1, train for the 379-th batch, train loss: 0.4436939060688019:  99%|██████████▉| 379/383 [03:47<00:02,  1.54it/s]Epoch: 2, train for the 105-th batch, train loss: 0.6366056203842163:  44%|████▊      | 105/237 [01:01<01:25,  1.54it/s]Epoch: 5, train for the 89-th batch, train loss: 0.5667089819908142:  58%|███████▌     | 88/151 [00:19<00:14,  4.35it/s]Epoch: 5, train for the 89-th batch, train loss: 0.5667089819908142:  59%|███████▋     | 89/151 [00:19<00:14,  4.38it/s]Epoch: 5, train for the 90-th batch, train loss: 0.5527040958404541:  59%|███████▋     | 89/151 [00:19<00:14,  4.38it/s]Epoch: 5, train for the 90-th batch, train loss: 0.5527040958404541:  60%|███████▋     | 90/151 [00:19<00:13,  4.41it/s]Epoch: 3, train for the 36-th batch, train loss: 0.4457123577594757:  24%|███          | 35/146 [00:21<01:08,  1.63it/s]Epoch: 3, train for the 36-th batch, train loss: 0.4457123577594757:  25%|███▏         | 36/146 [00:21<01:07,  1.64it/s]Epoch: 3, train for the 88-th batch, train loss: 0.24017558991909027:  73%|████████▊   | 87/119 [00:52<00:19,  1.62it/s]Epoch: 3, train for the 88-th batch, train loss: 0.24017558991909027:  74%|████████▊   | 88/119 [00:52<00:18,  1.63it/s]Epoch: 5, train for the 91-th batch, train loss: 0.47803789377212524:  60%|███████▏    | 90/151 [00:19<00:13,  4.41it/s]Epoch: 5, train for the 91-th batch, train loss: 0.47803789377212524:  60%|███████▏    | 91/151 [00:19<00:13,  4.42it/s]Epoch: 1, train for the 380-th batch, train loss: 0.4591071903705597:  99%|██████████▉| 379/383 [03:48<00:02,  1.54it/s]Epoch: 1, train for the 380-th batch, train loss: 0.4591071903705597:  99%|██████████▉| 380/383 [03:48<00:01,  1.54it/s]Epoch: 2, train for the 106-th batch, train loss: 0.6512544751167297:  44%|████▊      | 105/237 [01:02<01:25,  1.54it/s]Epoch: 2, train for the 106-th batch, train loss: 0.6512544751167297:  45%|████▉      | 106/237 [01:02<01:24,  1.54it/s]Epoch: 5, train for the 92-th batch, train loss: 0.5542978644371033:  60%|███████▊     | 91/151 [00:19<00:13,  4.42it/s]Epoch: 5, train for the 92-th batch, train loss: 0.5542978644371033:  61%|███████▉     | 92/151 [00:19<00:13,  4.43it/s]Epoch: 5, train for the 93-th batch, train loss: 0.5191379189491272:  61%|███████▉     | 92/151 [00:20<00:13,  4.43it/s]Epoch: 5, train for the 93-th batch, train loss: 0.5191379189491272:  62%|████████     | 93/151 [00:20<00:13,  4.42it/s]Epoch: 3, train for the 37-th batch, train loss: 0.46451112627983093:  25%|██▉         | 36/146 [00:21<01:07,  1.64it/s]Epoch: 3, train for the 37-th batch, train loss: 0.46451112627983093:  25%|███         | 37/146 [00:21<01:07,  1.63it/s]Epoch: 3, train for the 89-th batch, train loss: 0.2245555818080902:  74%|█████████▌   | 88/119 [00:53<00:18,  1.63it/s]Epoch: 3, train for the 89-th batch, train loss: 0.2245555818080902:  75%|█████████▋   | 89/119 [00:53<00:18,  1.62it/s]Epoch: 5, train for the 94-th batch, train loss: 0.5409719944000244:  62%|████████     | 93/151 [00:20<00:13,  4.42it/s]Epoch: 5, train for the 94-th batch, train loss: 0.5409719944000244:  62%|████████     | 94/151 [00:20<00:12,  4.43it/s]Epoch: 1, train for the 381-th batch, train loss: 0.5384240746498108:  99%|██████████▉| 380/383 [03:49<00:01,  1.54it/s]Epoch: 1, train for the 381-th batch, train loss: 0.5384240746498108:  99%|██████████▉| 381/383 [03:49<00:01,  1.54it/s]Epoch: 2, train for the 107-th batch, train loss: 0.6129014492034912:  45%|████▉      | 106/237 [01:02<01:24,  1.54it/s]Epoch: 2, train for the 107-th batch, train loss: 0.6129014492034912:  45%|████▉      | 107/237 [01:02<01:24,  1.54it/s]Epoch: 5, train for the 95-th batch, train loss: 0.527359127998352:  62%|████████▋     | 94/151 [00:20<00:12,  4.43it/s]Epoch: 5, train for the 95-th batch, train loss: 0.527359127998352:  63%|████████▊     | 95/151 [00:20<00:12,  4.44it/s]Epoch: 3, train for the 38-th batch, train loss: 0.4921959340572357:  25%|███▎         | 37/146 [00:22<01:07,  1.63it/s]Epoch: 3, train for the 38-th batch, train loss: 0.4921959340572357:  26%|███▍         | 38/146 [00:22<01:06,  1.63it/s]Epoch: 3, train for the 90-th batch, train loss: 0.22822611033916473:  75%|████████▉   | 89/119 [00:53<00:18,  1.62it/s]Epoch: 3, train for the 90-th batch, train loss: 0.22822611033916473:  76%|█████████   | 90/119 [00:53<00:17,  1.63it/s]Epoch: 5, train for the 96-th batch, train loss: 0.5654782652854919:  63%|████████▏    | 95/151 [00:21<00:12,  4.44it/s]Epoch: 5, train for the 96-th batch, train loss: 0.5654782652854919:  64%|████████▎    | 96/151 [00:21<00:15,  3.59it/s]Epoch: 1, train for the 382-th batch, train loss: 0.4812859296798706:  99%|██████████▉| 381/383 [03:49<00:01,  1.54it/s]Epoch: 1, train for the 382-th batch, train loss: 0.4812859296798706: 100%|██████████▉| 382/383 [03:49<00:00,  1.55it/s]Epoch: 2, train for the 108-th batch, train loss: 0.6411329507827759:  45%|████▉      | 107/237 [01:03<01:24,  1.54it/s]Epoch: 2, train for the 108-th batch, train loss: 0.6411329507827759:  46%|█████      | 108/237 [01:03<01:23,  1.54it/s]Epoch: 5, train for the 97-th batch, train loss: 0.614059329032898:  64%|████████▉     | 96/151 [00:21<00:15,  3.59it/s]Epoch: 5, train for the 97-th batch, train loss: 0.614059329032898:  64%|████████▉     | 97/151 [00:21<00:14,  3.82it/s]Epoch: 3, train for the 39-th batch, train loss: 0.4945649206638336:  26%|███▍         | 38/146 [00:23<01:06,  1.63it/s]Epoch: 3, train for the 39-th batch, train loss: 0.4945649206638336:  27%|███▍         | 39/146 [00:23<01:05,  1.63it/s]Epoch: 5, train for the 98-th batch, train loss: 0.5969569683074951:  64%|████████▎    | 97/151 [00:21<00:14,  3.82it/s]Epoch: 5, train for the 98-th batch, train loss: 0.5969569683074951:  65%|████████▍    | 98/151 [00:21<00:13,  4.00it/s]Epoch: 3, train for the 91-th batch, train loss: 0.20956777036190033:  76%|█████████   | 90/119 [00:54<00:17,  1.63it/s]Epoch: 3, train for the 91-th batch, train loss: 0.20956777036190033:  76%|█████████▏  | 91/119 [00:54<00:17,  1.63it/s]Epoch: 5, train for the 99-th batch, train loss: 0.6339300870895386:  65%|████████▍    | 98/151 [00:21<00:13,  4.00it/s]Epoch: 5, train for the 99-th batch, train loss: 0.6339300870895386:  66%|████████▌    | 99/151 [00:21<00:12,  4.13it/s]Epoch: 1, train for the 383-th batch, train loss: 0.4264429807662964: 100%|██████████▉| 382/383 [03:50<00:00,  1.55it/s]Epoch: 1, train for the 383-th batch, train loss: 0.4264429807662964: 100%|███████████| 383/383 [03:50<00:00,  1.55it/s]Epoch: 1, train for the 383-th batch, train loss: 0.4264429807662964: 100%|███████████| 383/383 [03:50<00:00,  1.66it/s]
  0%|                                                                                           | 0/106 [00:00<?, ?it/s]Epoch: 2, train for the 109-th batch, train loss: 0.6494658589363098:  46%|█████      | 108/237 [01:03<01:23,  1.54it/s]Epoch: 2, train for the 109-th batch, train loss: 0.6494658589363098:  46%|█████      | 109/237 [01:03<01:22,  1.55it/s]Epoch: 5, train for the 100-th batch, train loss: 0.592860996723175:  66%|████████▌    | 99/151 [00:21<00:12,  4.13it/s]Epoch: 5, train for the 100-th batch, train loss: 0.592860996723175:  66%|███████▉    | 100/151 [00:21<00:11,  4.27it/s]Epoch: 3, train for the 40-th batch, train loss: 0.45436781644821167:  27%|███▏        | 39/146 [00:23<01:05,  1.63it/s]Epoch: 3, train for the 40-th batch, train loss: 0.45436781644821167:  27%|███▎        | 40/146 [00:23<01:04,  1.63it/s]evaluate for the 1-th batch, evaluate loss: 0.4836607575416565:   0%|                           | 0/106 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.4836607575416565:   1%|▏                  | 1/106 [00:00<00:27,  3.86it/s]Epoch: 5, train for the 101-th batch, train loss: 0.6643470525741577:  66%|███████▎   | 100/151 [00:22<00:11,  4.27it/s]Epoch: 5, train for the 101-th batch, train loss: 0.6643470525741577:  67%|███████▎   | 101/151 [00:22<00:11,  4.32it/s]Epoch: 3, train for the 92-th batch, train loss: 0.24546591937541962:  76%|█████████▏  | 91/119 [00:54<00:17,  1.63it/s]Epoch: 3, train for the 92-th batch, train loss: 0.24546591937541962:  77%|█████████▎  | 92/119 [00:54<00:16,  1.63it/s]Epoch: 5, train for the 102-th batch, train loss: 0.6438162326812744:  67%|███████▎   | 101/151 [00:22<00:11,  4.32it/s]Epoch: 5, train for the 102-th batch, train loss: 0.6438162326812744:  68%|███████▍   | 102/151 [00:22<00:11,  4.36it/s]evaluate for the 2-th batch, evaluate loss: 0.5763620734214783:   1%|▏                  | 1/106 [00:00<00:27,  3.86it/s]evaluate for the 2-th batch, evaluate loss: 0.5763620734214783:   2%|▎                  | 2/106 [00:00<00:28,  3.64it/s]Epoch: 2, train for the 110-th batch, train loss: 0.5620336532592773:  46%|█████      | 109/237 [01:04<01:22,  1.55it/s]Epoch: 2, train for the 110-th batch, train loss: 0.5620336532592773:  46%|█████      | 110/237 [01:04<01:19,  1.60it/s]Epoch: 5, train for the 103-th batch, train loss: 0.6287583112716675:  68%|███████▍   | 102/151 [00:22<00:11,  4.36it/s]Epoch: 5, train for the 103-th batch, train loss: 0.6287583112716675:  68%|███████▌   | 103/151 [00:22<00:10,  4.39it/s]Epoch: 3, train for the 41-th batch, train loss: 0.45848196744918823:  27%|███▎        | 40/146 [00:24<01:04,  1.63it/s]Epoch: 3, train for the 41-th batch, train loss: 0.45848196744918823:  28%|███▎        | 41/146 [00:24<01:04,  1.63it/s]evaluate for the 3-th batch, evaluate loss: 0.5069991946220398:   2%|▎                  | 2/106 [00:00<00:28,  3.64it/s]evaluate for the 3-th batch, evaluate loss: 0.5069991946220398:   3%|▌                  | 3/106 [00:00<00:27,  3.80it/s]Epoch: 3, train for the 93-th batch, train loss: 0.17797961831092834:  77%|█████████▎  | 92/119 [00:55<00:16,  1.63it/s]Epoch: 3, train for the 93-th batch, train loss: 0.17797961831092834:  78%|█████████▍  | 93/119 [00:55<00:15,  1.63it/s]Epoch: 5, train for the 104-th batch, train loss: 0.6026522517204285:  68%|███████▌   | 103/151 [00:22<00:10,  4.39it/s]Epoch: 5, train for the 104-th batch, train loss: 0.6026522517204285:  69%|███████▌   | 104/151 [00:22<00:10,  4.39it/s]evaluate for the 4-th batch, evaluate loss: 0.517155647277832:   3%|▌                   | 3/106 [00:01<00:27,  3.80it/s]evaluate for the 4-th batch, evaluate loss: 0.517155647277832:   4%|▊                   | 4/106 [00:01<00:27,  3.68it/s]Epoch: 2, train for the 111-th batch, train loss: 0.6349105834960938:  46%|█████      | 110/237 [01:05<01:19,  1.60it/s]Epoch: 2, train for the 111-th batch, train loss: 0.6349105834960938:  47%|█████▏     | 111/237 [01:05<01:17,  1.62it/s]Epoch: 5, train for the 105-th batch, train loss: 0.5386431813240051:  69%|███████▌   | 104/151 [00:23<00:10,  4.39it/s]Epoch: 5, train for the 105-th batch, train loss: 0.5386431813240051:  70%|███████▋   | 105/151 [00:23<00:10,  4.40it/s]evaluate for the 5-th batch, evaluate loss: 0.5747489929199219:   4%|▋                  | 4/106 [00:01<00:27,  3.68it/s]evaluate for the 5-th batch, evaluate loss: 0.5747489929199219:   5%|▉                  | 5/106 [00:01<00:26,  3.76it/s]Epoch: 3, train for the 42-th batch, train loss: 0.45972156524658203:  28%|███▎        | 41/146 [00:24<01:04,  1.63it/s]Epoch: 3, train for the 42-th batch, train loss: 0.45972156524658203:  29%|███▍        | 42/146 [00:24<01:03,  1.63it/s]Epoch: 5, train for the 106-th batch, train loss: 0.5499452948570251:  70%|███████▋   | 105/151 [00:23<00:10,  4.40it/s]Epoch: 5, train for the 106-th batch, train loss: 0.5499452948570251:  70%|███████▋   | 106/151 [00:23<00:10,  4.42it/s]Epoch: 3, train for the 94-th batch, train loss: 0.21521113812923431:  78%|█████████▍  | 93/119 [00:56<00:15,  1.63it/s]Epoch: 3, train for the 94-th batch, train loss: 0.21521113812923431:  79%|█████████▍  | 94/119 [00:56<00:15,  1.63it/s]evaluate for the 6-th batch, evaluate loss: 0.5702685713768005:   5%|▉                  | 5/106 [00:01<00:26,  3.76it/s]evaluate for the 6-th batch, evaluate loss: 0.5702685713768005:   6%|█                  | 6/106 [00:01<00:27,  3.69it/s]Epoch: 5, train for the 107-th batch, train loss: 0.5410867929458618:  70%|███████▋   | 106/151 [00:23<00:10,  4.42it/s]Epoch: 5, train for the 107-th batch, train loss: 0.5410867929458618:  71%|███████▊   | 107/151 [00:23<00:09,  4.44it/s]Epoch: 2, train for the 112-th batch, train loss: 0.6392813324928284:  47%|█████▏     | 111/237 [01:05<01:17,  1.62it/s]Epoch: 2, train for the 112-th batch, train loss: 0.6392813324928284:  47%|█████▏     | 112/237 [01:05<01:16,  1.63it/s]Epoch: 5, train for the 108-th batch, train loss: 0.5688050985336304:  71%|███████▊   | 107/151 [00:23<00:09,  4.44it/s]Epoch: 5, train for the 108-th batch, train loss: 0.5688050985336304:  72%|███████▊   | 108/151 [00:23<00:09,  4.42it/s]evaluate for the 7-th batch, evaluate loss: 0.5179072618484497:   6%|█                  | 6/106 [00:01<00:27,  3.69it/s]evaluate for the 7-th batch, evaluate loss: 0.5179072618484497:   7%|█▎                 | 7/106 [00:01<00:27,  3.59it/s]Epoch: 3, train for the 43-th batch, train loss: 0.5270032286643982:  29%|███▋         | 42/146 [00:25<01:03,  1.63it/s]Epoch: 3, train for the 43-th batch, train loss: 0.5270032286643982:  29%|███▊         | 43/146 [00:25<01:03,  1.63it/s]Epoch: 5, train for the 109-th batch, train loss: 0.5330758690834045:  72%|███████▊   | 108/151 [00:23<00:09,  4.42it/s]Epoch: 5, train for the 109-th batch, train loss: 0.5330758690834045:  72%|███████▉   | 109/151 [00:23<00:09,  4.43it/s]evaluate for the 8-th batch, evaluate loss: 0.4007939100265503:   7%|█▎                 | 7/106 [00:02<00:27,  3.59it/s]evaluate for the 8-th batch, evaluate loss: 0.4007939100265503:   8%|█▍                 | 8/106 [00:02<00:26,  3.69it/s]Epoch: 3, train for the 95-th batch, train loss: 0.1802314668893814:  79%|██████████▎  | 94/119 [00:56<00:15,  1.63it/s]Epoch: 3, train for the 95-th batch, train loss: 0.1802314668893814:  80%|██████████▍  | 95/119 [00:56<00:14,  1.63it/s]Epoch: 5, train for the 110-th batch, train loss: 0.5699358582496643:  72%|███████▉   | 109/151 [00:24<00:09,  4.43it/s]Epoch: 5, train for the 110-th batch, train loss: 0.5699358582496643:  73%|████████   | 110/151 [00:24<00:09,  4.40it/s]Epoch: 2, train for the 113-th batch, train loss: 0.6628586649894714:  47%|█████▏     | 112/237 [01:06<01:16,  1.63it/s]Epoch: 2, train for the 113-th batch, train loss: 0.6628586649894714:  48%|█████▏     | 113/237 [01:06<01:15,  1.64it/s]evaluate for the 9-th batch, evaluate loss: 0.579037606716156:   8%|█▌                  | 8/106 [00:02<00:26,  3.69it/s]evaluate for the 9-th batch, evaluate loss: 0.579037606716156:   8%|█▋                  | 9/106 [00:02<00:27,  3.59it/s]Epoch: 5, train for the 111-th batch, train loss: 0.511106550693512:  73%|████████▋   | 110/151 [00:24<00:09,  4.40it/s]Epoch: 5, train for the 111-th batch, train loss: 0.511106550693512:  74%|████████▊   | 111/151 [00:24<00:09,  4.36it/s]Epoch: 3, train for the 44-th batch, train loss: 0.4404538869857788:  29%|███▊         | 43/146 [00:26<01:03,  1.63it/s]Epoch: 3, train for the 44-th batch, train loss: 0.4404538869857788:  30%|███▉         | 44/146 [00:26<01:02,  1.63it/s]evaluate for the 10-th batch, evaluate loss: 0.4452308416366577:   8%|█▌                | 9/106 [00:02<00:27,  3.59it/s]evaluate for the 10-th batch, evaluate loss: 0.4452308416366577:   9%|█▌               | 10/106 [00:02<00:25,  3.78it/s]Epoch: 3, train for the 96-th batch, train loss: 0.20337635278701782:  80%|█████████▌  | 95/119 [00:57<00:14,  1.63it/s]Epoch: 3, train for the 96-th batch, train loss: 0.20337635278701782:  81%|█████████▋  | 96/119 [00:57<00:14,  1.63it/s]Epoch: 5, train for the 112-th batch, train loss: 0.5500665307044983:  74%|████████   | 111/151 [00:24<00:09,  4.36it/s]Epoch: 5, train for the 112-th batch, train loss: 0.5500665307044983:  74%|████████▏  | 112/151 [00:24<00:08,  4.38it/s]Epoch: 2, train for the 114-th batch, train loss: 0.6555306911468506:  48%|█████▏     | 113/237 [01:06<01:15,  1.64it/s]Epoch: 2, train for the 114-th batch, train loss: 0.6555306911468506:  48%|█████▎     | 114/237 [01:06<01:14,  1.65it/s]evaluate for the 11-th batch, evaluate loss: 0.42811161279678345:   9%|█▌              | 10/106 [00:03<00:25,  3.78it/s]evaluate for the 11-th batch, evaluate loss: 0.42811161279678345:  10%|█▋              | 11/106 [00:03<00:26,  3.60it/s]Epoch: 5, train for the 113-th batch, train loss: 0.5432955026626587:  74%|████████▏  | 112/151 [00:24<00:08,  4.38it/s]Epoch: 5, train for the 113-th batch, train loss: 0.5432955026626587:  75%|████████▏  | 113/151 [00:24<00:08,  4.40it/s]evaluate for the 12-th batch, evaluate loss: 0.44196227192878723:  10%|█▋              | 11/106 [00:03<00:26,  3.60it/s]evaluate for the 12-th batch, evaluate loss: 0.44196227192878723:  11%|█▊              | 12/106 [00:03<00:24,  3.79it/s]Epoch: 5, train for the 114-th batch, train loss: 0.5317469835281372:  75%|████████▏  | 113/151 [00:25<00:08,  4.40it/s]Epoch: 5, train for the 114-th batch, train loss: 0.5317469835281372:  75%|████████▎  | 114/151 [00:25<00:08,  4.41it/s]Epoch: 3, train for the 45-th batch, train loss: 0.4443529546260834:  30%|███▉         | 44/146 [00:26<01:02,  1.63it/s]Epoch: 3, train for the 45-th batch, train loss: 0.4443529546260834:  31%|████         | 45/146 [00:26<01:02,  1.63it/s]evaluate for the 13-th batch, evaluate loss: 0.456962913274765:  11%|██                | 12/106 [00:03<00:24,  3.79it/s]evaluate for the 13-th batch, evaluate loss: 0.456962913274765:  12%|██▏               | 13/106 [00:03<00:21,  4.37it/s]Epoch: 3, train for the 97-th batch, train loss: 0.2170044630765915:  81%|██████████▍  | 96/119 [00:57<00:14,  1.63it/s]Epoch: 3, train for the 97-th batch, train loss: 0.2170044630765915:  82%|██████████▌  | 97/119 [00:57<00:13,  1.63it/s]Epoch: 5, train for the 115-th batch, train loss: 0.521078884601593:  75%|█████████   | 114/151 [00:25<00:08,  4.41it/s]Epoch: 5, train for the 115-th batch, train loss: 0.521078884601593:  76%|█████████▏  | 115/151 [00:25<00:08,  4.41it/s]Epoch: 2, train for the 115-th batch, train loss: 0.5680000185966492:  48%|█████▎     | 114/237 [01:07<01:14,  1.65it/s]Epoch: 2, train for the 115-th batch, train loss: 0.5680000185966492:  49%|█████▎     | 115/237 [01:07<01:16,  1.60it/s]evaluate for the 14-th batch, evaluate loss: 0.4285750985145569:  12%|██               | 13/106 [00:03<00:21,  4.37it/s]evaluate for the 14-th batch, evaluate loss: 0.4285750985145569:  13%|██▏              | 14/106 [00:03<00:23,  3.98it/s]Epoch: 5, train for the 116-th batch, train loss: 0.49450239539146423:  76%|███████▌  | 115/151 [00:25<00:08,  4.41it/s]Epoch: 5, train for the 116-th batch, train loss: 0.49450239539146423:  77%|███████▋  | 116/151 [00:25<00:07,  4.39it/s]Epoch: 3, train for the 46-th batch, train loss: 0.47079572081565857:  31%|███▋        | 45/146 [00:27<01:02,  1.63it/s]Epoch: 3, train for the 46-th batch, train loss: 0.47079572081565857:  32%|███▊        | 46/146 [00:27<01:01,  1.63it/s]Epoch: 5, train for the 117-th batch, train loss: 0.5188891291618347:  77%|████████▍  | 116/151 [00:25<00:07,  4.39it/s]Epoch: 5, train for the 117-th batch, train loss: 0.5188891291618347:  77%|████████▌  | 117/151 [00:25<00:07,  4.40it/s]evaluate for the 15-th batch, evaluate loss: 0.5851491689682007:  13%|██▏              | 14/106 [00:03<00:23,  3.98it/s]evaluate for the 15-th batch, evaluate loss: 0.5851491689682007:  14%|██▍              | 15/106 [00:03<00:22,  3.99it/s]Epoch: 3, train for the 98-th batch, train loss: 0.18186427652835846:  82%|█████████▊  | 97/119 [00:58<00:13,  1.63it/s]Epoch: 3, train for the 98-th batch, train loss: 0.18186427652835846:  82%|█████████▉  | 98/119 [00:58<00:12,  1.63it/s]Epoch: 5, train for the 118-th batch, train loss: 0.46183130145072937:  77%|███████▋  | 117/151 [00:26<00:07,  4.40it/s]Epoch: 5, train for the 118-th batch, train loss: 0.46183130145072937:  78%|███████▊  | 118/151 [00:26<00:07,  4.42it/s]Epoch: 2, train for the 116-th batch, train loss: 0.6386627554893494:  49%|█████▎     | 115/237 [01:08<01:16,  1.60it/s]Epoch: 2, train for the 116-th batch, train loss: 0.6386627554893494:  49%|█████▍     | 116/237 [01:08<01:14,  1.62it/s]evaluate for the 16-th batch, evaluate loss: 0.5018467307090759:  14%|██▍              | 15/106 [00:04<00:22,  3.99it/s]evaluate for the 16-th batch, evaluate loss: 0.5018467307090759:  15%|██▌              | 16/106 [00:04<00:24,  3.71it/s]Epoch: 5, train for the 119-th batch, train loss: 0.548331081867218:  78%|█████████▍  | 118/151 [00:26<00:07,  4.42it/s]Epoch: 5, train for the 119-th batch, train loss: 0.548331081867218:  79%|█████████▍  | 119/151 [00:26<00:07,  4.44it/s]Epoch: 3, train for the 47-th batch, train loss: 0.521608293056488:  32%|████▍         | 46/146 [00:27<01:01,  1.63it/s]Epoch: 3, train for the 47-th batch, train loss: 0.521608293056488:  32%|████▌         | 47/146 [00:27<01:00,  1.63it/s]evaluate for the 17-th batch, evaluate loss: 0.46807101368904114:  15%|██▍             | 16/106 [00:04<00:24,  3.71it/s]evaluate for the 17-th batch, evaluate loss: 0.46807101368904114:  16%|██▌             | 17/106 [00:04<00:23,  3.72it/s]Epoch: 5, train for the 120-th batch, train loss: 0.5917993783950806:  79%|████████▋  | 119/151 [00:26<00:07,  4.44it/s]Epoch: 5, train for the 120-th batch, train loss: 0.5917993783950806:  79%|████████▋  | 120/151 [00:26<00:06,  4.44it/s]Epoch: 3, train for the 99-th batch, train loss: 0.20293094217777252:  82%|█████████▉  | 98/119 [00:59<00:12,  1.63it/s]Epoch: 3, train for the 99-th batch, train loss: 0.20293094217777252:  83%|█████████▉  | 99/119 [00:59<00:12,  1.64it/s]evaluate for the 18-th batch, evaluate loss: 0.429611474275589:  16%|██▉               | 17/106 [00:04<00:23,  3.72it/s]evaluate for the 18-th batch, evaluate loss: 0.429611474275589:  17%|███               | 18/106 [00:04<00:24,  3.62it/s]Epoch: 5, train for the 121-th batch, train loss: 0.4970075190067291:  79%|████████▋  | 120/151 [00:26<00:06,  4.44it/s]Epoch: 5, train for the 121-th batch, train loss: 0.4970075190067291:  80%|████████▊  | 121/151 [00:26<00:06,  4.44it/s]Epoch: 2, train for the 117-th batch, train loss: 0.624994695186615:  49%|█████▊      | 116/237 [01:08<01:14,  1.62it/s]Epoch: 2, train for the 117-th batch, train loss: 0.624994695186615:  49%|█████▉      | 117/237 [01:08<01:13,  1.64it/s]Epoch: 5, train for the 122-th batch, train loss: 0.548957109451294:  80%|█████████▌  | 121/151 [00:26<00:06,  4.44it/s]Epoch: 5, train for the 122-th batch, train loss: 0.548957109451294:  81%|█████████▋  | 122/151 [00:26<00:06,  4.45it/s]evaluate for the 19-th batch, evaluate loss: 0.39719608426094055:  17%|██▋             | 18/106 [00:05<00:24,  3.62it/s]evaluate for the 19-th batch, evaluate loss: 0.39719608426094055:  18%|██▊             | 19/106 [00:05<00:23,  3.72it/s]Epoch: 3, train for the 48-th batch, train loss: 0.4754546284675598:  32%|████▏        | 47/146 [00:28<01:00,  1.63it/s]Epoch: 3, train for the 48-th batch, train loss: 0.4754546284675598:  33%|████▎        | 48/146 [00:28<00:59,  1.64it/s]Epoch: 3, train for the 100-th batch, train loss: 0.23528607189655304:  83%|█████████▏ | 99/119 [00:59<00:12,  1.64it/s]Epoch: 3, train for the 100-th batch, train loss: 0.23528607189655304:  84%|████████▍ | 100/119 [00:59<00:11,  1.64it/s]Epoch: 5, train for the 123-th batch, train loss: 0.5582964420318604:  81%|████████▉  | 122/151 [00:27<00:06,  4.45it/s]Epoch: 5, train for the 123-th batch, train loss: 0.5582964420318604:  81%|████████▉  | 123/151 [00:27<00:06,  4.44it/s]evaluate for the 20-th batch, evaluate loss: 0.43975546956062317:  18%|██▊             | 19/106 [00:05<00:23,  3.72it/s]evaluate for the 20-th batch, evaluate loss: 0.43975546956062317:  19%|███             | 20/106 [00:05<00:23,  3.65it/s]Epoch: 2, train for the 118-th batch, train loss: 0.6055484414100647:  49%|█████▍     | 117/237 [01:09<01:13,  1.64it/s]Epoch: 2, train for the 118-th batch, train loss: 0.6055484414100647:  50%|█████▍     | 118/237 [01:09<01:11,  1.65it/s]Epoch: 5, train for the 124-th batch, train loss: 0.5081019997596741:  81%|████████▉  | 123/151 [00:27<00:06,  4.44it/s]Epoch: 5, train for the 124-th batch, train loss: 0.5081019997596741:  82%|█████████  | 124/151 [00:27<00:06,  4.43it/s]evaluate for the 21-th batch, evaluate loss: 0.35489019751548767:  19%|███             | 20/106 [00:05<00:23,  3.65it/s]evaluate for the 21-th batch, evaluate loss: 0.35489019751548767:  20%|███▏            | 21/106 [00:05<00:22,  3.72it/s]Epoch: 3, train for the 49-th batch, train loss: 0.5005770921707153:  33%|████▎        | 48/146 [00:29<00:59,  1.64it/s]Epoch: 3, train for the 49-th batch, train loss: 0.5005770921707153:  34%|████▎        | 49/146 [00:29<00:59,  1.64it/s]Epoch: 5, train for the 125-th batch, train loss: 0.5648608207702637:  82%|█████████  | 124/151 [00:27<00:06,  4.43it/s]Epoch: 5, train for the 125-th batch, train loss: 0.5648608207702637:  83%|█████████  | 125/151 [00:27<00:05,  4.44it/s]Epoch: 3, train for the 101-th batch, train loss: 0.17445281147956848:  84%|████████▍ | 100/119 [01:00<00:11,  1.64it/s]Epoch: 3, train for the 101-th batch, train loss: 0.17445281147956848:  85%|████████▍ | 101/119 [01:00<00:10,  1.64it/s]evaluate for the 22-th batch, evaluate loss: 0.4052923619747162:  20%|███▎             | 21/106 [00:05<00:22,  3.72it/s]evaluate for the 22-th batch, evaluate loss: 0.4052923619747162:  21%|███▌             | 22/106 [00:05<00:22,  3.67it/s]Epoch: 5, train for the 126-th batch, train loss: 0.5620180368423462:  83%|█████████  | 125/151 [00:27<00:05,  4.44it/s]Epoch: 5, train for the 126-th batch, train loss: 0.5620180368423462:  83%|█████████▏ | 126/151 [00:27<00:05,  4.45it/s]Epoch: 2, train for the 119-th batch, train loss: 0.6182594299316406:  50%|█████▍     | 118/237 [01:10<01:11,  1.65it/s]Epoch: 2, train for the 119-th batch, train loss: 0.6182594299316406:  50%|█████▌     | 119/237 [01:10<01:11,  1.66it/s]Epoch: 5, train for the 127-th batch, train loss: 0.5850859880447388:  83%|█████████▏ | 126/151 [00:28<00:05,  4.45it/s]Epoch: 5, train for the 127-th batch, train loss: 0.5850859880447388:  84%|█████████▎ | 127/151 [00:28<00:05,  4.46it/s]evaluate for the 23-th batch, evaluate loss: 0.42621320486068726:  21%|███▎            | 22/106 [00:06<00:22,  3.67it/s]evaluate for the 23-th batch, evaluate loss: 0.42621320486068726:  22%|███▍            | 23/106 [00:06<00:22,  3.61it/s]Epoch: 3, train for the 50-th batch, train loss: 0.47432371973991394:  34%|████        | 49/146 [00:29<00:59,  1.64it/s]Epoch: 3, train for the 50-th batch, train loss: 0.47432371973991394:  34%|████        | 50/146 [00:29<00:58,  1.63it/s]Epoch: 5, train for the 128-th batch, train loss: 0.6028928756713867:  84%|█████████▎ | 127/151 [00:28<00:05,  4.46it/s]Epoch: 5, train for the 128-th batch, train loss: 0.6028928756713867:  85%|█████████▎ | 128/151 [00:28<00:05,  4.47it/s]evaluate for the 24-th batch, evaluate loss: 0.4254836440086365:  22%|███▋             | 23/106 [00:06<00:22,  3.61it/s]evaluate for the 24-th batch, evaluate loss: 0.4254836440086365:  23%|███▊             | 24/106 [00:06<00:22,  3.64it/s]Epoch: 3, train for the 102-th batch, train loss: 0.2024887353181839:  85%|█████████▎ | 101/119 [01:00<00:10,  1.64it/s]Epoch: 3, train for the 102-th batch, train loss: 0.2024887353181839:  86%|█████████▍ | 102/119 [01:00<00:10,  1.63it/s]Epoch: 5, train for the 129-th batch, train loss: 0.5733388662338257:  85%|█████████▎ | 128/151 [00:28<00:05,  4.47it/s]Epoch: 5, train for the 129-th batch, train loss: 0.5733388662338257:  85%|█████████▍ | 129/151 [00:28<00:04,  4.48it/s]Epoch: 2, train for the 120-th batch, train loss: 0.6171842217445374:  50%|█████▌     | 119/237 [01:10<01:11,  1.66it/s]Epoch: 2, train for the 120-th batch, train loss: 0.6171842217445374:  51%|█████▌     | 120/237 [01:10<01:10,  1.67it/s]evaluate for the 25-th batch, evaluate loss: 0.48028692603111267:  23%|███▌            | 24/106 [00:06<00:22,  3.64it/s]evaluate for the 25-th batch, evaluate loss: 0.48028692603111267:  24%|███▊            | 25/106 [00:06<00:22,  3.56it/s]Epoch: 5, train for the 130-th batch, train loss: 0.5574657917022705:  85%|█████████▍ | 129/151 [00:28<00:04,  4.48it/s]Epoch: 5, train for the 130-th batch, train loss: 0.5574657917022705:  86%|█████████▍ | 130/151 [00:28<00:04,  4.49it/s]Epoch: 3, train for the 51-th batch, train loss: 0.4928745627403259:  34%|████▍        | 50/146 [00:30<00:58,  1.63it/s]Epoch: 3, train for the 51-th batch, train loss: 0.4928745627403259:  35%|████▌        | 51/146 [00:30<00:58,  1.64it/s]evaluate for the 26-th batch, evaluate loss: 0.44566071033477783:  24%|███▊            | 25/106 [00:06<00:22,  3.56it/s]evaluate for the 26-th batch, evaluate loss: 0.44566071033477783:  25%|███▉            | 26/106 [00:06<00:21,  3.70it/s]Epoch: 5, train for the 131-th batch, train loss: 0.5151940584182739:  86%|█████████▍ | 130/151 [00:28<00:04,  4.49it/s]Epoch: 5, train for the 131-th batch, train loss: 0.5151940584182739:  87%|█████████▌ | 131/151 [00:28<00:04,  4.49it/s]Epoch: 3, train for the 103-th batch, train loss: 0.21356742084026337:  86%|████████▌ | 102/119 [01:01<00:10,  1.63it/s]Epoch: 3, train for the 103-th batch, train loss: 0.21356742084026337:  87%|████████▋ | 103/119 [01:01<00:09,  1.64it/s]Epoch: 2, train for the 121-th batch, train loss: 0.6414564847946167:  51%|█████▌     | 120/237 [01:11<01:10,  1.67it/s]Epoch: 2, train for the 121-th batch, train loss: 0.6414564847946167:  51%|█████▌     | 121/237 [01:11<01:09,  1.68it/s]Epoch: 5, train for the 132-th batch, train loss: 0.5474349856376648:  87%|█████████▌ | 131/151 [00:29<00:04,  4.49it/s]Epoch: 5, train for the 132-th batch, train loss: 0.5474349856376648:  87%|█████████▌ | 132/151 [00:29<00:04,  4.48it/s]evaluate for the 27-th batch, evaluate loss: 0.34123408794403076:  25%|███▉            | 26/106 [00:07<00:21,  3.70it/s]evaluate for the 27-th batch, evaluate loss: 0.34123408794403076:  25%|████            | 27/106 [00:07<00:22,  3.59it/s]Epoch: 5, train for the 133-th batch, train loss: 0.5349389314651489:  87%|█████████▌ | 132/151 [00:29<00:04,  4.48it/s]Epoch: 5, train for the 133-th batch, train loss: 0.5349389314651489:  88%|█████████▋ | 133/151 [00:29<00:04,  4.48it/s]Epoch: 3, train for the 52-th batch, train loss: 0.4760192334651947:  35%|████▌        | 51/146 [00:31<00:58,  1.64it/s]Epoch: 3, train for the 52-th batch, train loss: 0.4760192334651947:  36%|████▋        | 52/146 [00:31<00:57,  1.64it/s]evaluate for the 28-th batch, evaluate loss: 0.3629249036312103:  25%|████▎            | 27/106 [00:07<00:22,  3.59it/s]evaluate for the 28-th batch, evaluate loss: 0.3629249036312103:  26%|████▍            | 28/106 [00:07<00:20,  3.76it/s]Epoch: 3, train for the 104-th batch, train loss: 0.24729909002780914:  87%|████████▋ | 103/119 [01:02<00:09,  1.64it/s]Epoch: 3, train for the 104-th batch, train loss: 0.24729909002780914:  87%|████████▋ | 104/119 [01:02<00:09,  1.64it/s]Epoch: 5, train for the 134-th batch, train loss: 0.5479248762130737:  88%|█████████▋ | 133/151 [00:29<00:04,  4.48it/s]Epoch: 5, train for the 134-th batch, train loss: 0.5479248762130737:  89%|█████████▊ | 134/151 [00:29<00:03,  4.47it/s]Epoch: 2, train for the 122-th batch, train loss: 0.6429073214530945:  51%|█████▌     | 121/237 [01:11<01:09,  1.68it/s]Epoch: 2, train for the 122-th batch, train loss: 0.6429073214530945:  51%|█████▋     | 122/237 [01:11<01:09,  1.66it/s]evaluate for the 29-th batch, evaluate loss: 0.3810649812221527:  26%|████▍            | 28/106 [00:07<00:20,  3.76it/s]evaluate for the 29-th batch, evaluate loss: 0.3810649812221527:  27%|████▋            | 29/106 [00:07<00:21,  3.56it/s]Epoch: 5, train for the 135-th batch, train loss: 0.5641912221908569:  89%|█████████▊ | 134/151 [00:29<00:03,  4.47it/s]Epoch: 5, train for the 135-th batch, train loss: 0.5641912221908569:  89%|█████████▊ | 135/151 [00:29<00:03,  4.48it/s]evaluate for the 30-th batch, evaluate loss: 0.4225166141986847:  27%|████▋            | 29/106 [00:08<00:21,  3.56it/s]evaluate for the 30-th batch, evaluate loss: 0.4225166141986847:  28%|████▊            | 30/106 [00:08<00:20,  3.63it/s]Epoch: 3, train for the 53-th batch, train loss: 0.4435044527053833:  36%|████▋        | 52/146 [00:31<00:57,  1.64it/s]Epoch: 3, train for the 53-th batch, train loss: 0.4435044527053833:  36%|████▋        | 53/146 [00:31<00:56,  1.64it/s]Epoch: 5, train for the 136-th batch, train loss: 0.5698314309120178:  89%|█████████▊ | 135/151 [00:30<00:03,  4.48it/s]Epoch: 5, train for the 136-th batch, train loss: 0.5698314309120178:  90%|█████████▉ | 136/151 [00:30<00:03,  4.48it/s]Epoch: 3, train for the 105-th batch, train loss: 0.21951615810394287:  87%|████████▋ | 104/119 [01:02<00:09,  1.64it/s]Epoch: 3, train for the 105-th batch, train loss: 0.21951615810394287:  88%|████████▊ | 105/119 [01:02<00:08,  1.64it/s]evaluate for the 31-th batch, evaluate loss: 0.34824782609939575:  28%|████▌           | 30/106 [00:08<00:20,  3.63it/s]evaluate for the 31-th batch, evaluate loss: 0.34824782609939575:  29%|████▋           | 31/106 [00:08<00:21,  3.57it/s]Epoch: 5, train for the 137-th batch, train loss: 0.5936991572380066:  90%|█████████▉ | 136/151 [00:30<00:03,  4.48it/s]Epoch: 5, train for the 137-th batch, train loss: 0.5936991572380066:  91%|█████████▉ | 137/151 [00:30<00:03,  4.49it/s]Epoch: 2, train for the 123-th batch, train loss: 0.634835958480835:  51%|██████▏     | 122/237 [01:12<01:09,  1.66it/s]Epoch: 2, train for the 123-th batch, train loss: 0.634835958480835:  52%|██████▏     | 123/237 [01:12<01:08,  1.67it/s]Epoch: 5, train for the 138-th batch, train loss: 0.5934377908706665:  91%|█████████▉ | 137/151 [00:30<00:03,  4.49it/s]Epoch: 5, train for the 138-th batch, train loss: 0.5934377908706665:  91%|██████████ | 138/151 [00:30<00:02,  4.50it/s]evaluate for the 32-th batch, evaluate loss: 0.4720381796360016:  29%|████▉            | 31/106 [00:08<00:21,  3.57it/s]evaluate for the 32-th batch, evaluate loss: 0.4720381796360016:  30%|█████▏           | 32/106 [00:08<00:20,  3.69it/s]Epoch: 3, train for the 54-th batch, train loss: 0.47931089997291565:  36%|████▎       | 53/146 [00:32<00:56,  1.64it/s]Epoch: 3, train for the 54-th batch, train loss: 0.47931089997291565:  37%|████▍       | 54/146 [00:32<00:56,  1.63it/s]Epoch: 5, train for the 139-th batch, train loss: 0.5927960276603699:  91%|██████████ | 138/151 [00:30<00:02,  4.50it/s]Epoch: 5, train for the 139-th batch, train loss: 0.5927960276603699:  92%|██████████▏| 139/151 [00:30<00:02,  4.49it/s]Epoch: 3, train for the 106-th batch, train loss: 0.1568608582019806:  88%|█████████▋ | 105/119 [01:03<00:08,  1.64it/s]Epoch: 3, train for the 106-th batch, train loss: 0.1568608582019806:  89%|█████████▊ | 106/119 [01:03<00:07,  1.63it/s]evaluate for the 33-th batch, evaluate loss: 0.47142547369003296:  30%|████▊           | 32/106 [00:08<00:20,  3.69it/s]evaluate for the 33-th batch, evaluate loss: 0.47142547369003296:  31%|████▉           | 33/106 [00:08<00:20,  3.63it/s]Epoch: 2, train for the 124-th batch, train loss: 0.60968017578125:  52%|██████▋      | 123/237 [01:12<01:08,  1.67it/s]Epoch: 2, train for the 124-th batch, train loss: 0.60968017578125:  52%|██████▊      | 124/237 [01:12<01:07,  1.68it/s]Epoch: 5, train for the 140-th batch, train loss: 0.536057710647583:  92%|███████████ | 139/151 [00:30<00:02,  4.49it/s]Epoch: 5, train for the 140-th batch, train loss: 0.536057710647583:  93%|███████████▏| 140/151 [00:30<00:02,  4.49it/s]evaluate for the 34-th batch, evaluate loss: 0.4876357614994049:  31%|█████▎           | 33/106 [00:09<00:20,  3.63it/s]evaluate for the 34-th batch, evaluate loss: 0.4876357614994049:  32%|█████▍           | 34/106 [00:09<00:19,  3.70it/s]Epoch: 5, train for the 141-th batch, train loss: 0.5543941259384155:  93%|██████████▏| 140/151 [00:31<00:02,  4.49it/s]Epoch: 5, train for the 141-th batch, train loss: 0.5543941259384155:  93%|██████████▎| 141/151 [00:31<00:02,  4.49it/s]Epoch: 3, train for the 55-th batch, train loss: 0.5422194600105286:  37%|████▊        | 54/146 [00:32<00:56,  1.63it/s]Epoch: 3, train for the 55-th batch, train loss: 0.5422194600105286:  38%|████▉        | 55/146 [00:32<00:55,  1.63it/s]evaluate for the 35-th batch, evaluate loss: 0.43951216340065:  32%|██████             | 34/106 [00:09<00:19,  3.70it/s]evaluate for the 35-th batch, evaluate loss: 0.43951216340065:  33%|██████▎            | 35/106 [00:09<00:19,  3.67it/s]Epoch: 5, train for the 142-th batch, train loss: 0.5512474179267883:  93%|██████████▎| 141/151 [00:31<00:02,  4.49it/s]Epoch: 5, train for the 142-th batch, train loss: 0.5512474179267883:  94%|██████████▎| 142/151 [00:31<00:02,  4.49it/s]Epoch: 3, train for the 107-th batch, train loss: 0.2908392548561096:  89%|█████████▊ | 106/119 [01:04<00:07,  1.63it/s]Epoch: 3, train for the 107-th batch, train loss: 0.2908392548561096:  90%|█████████▉ | 107/119 [01:04<00:07,  1.63it/s]Epoch: 2, train for the 125-th batch, train loss: 0.5658215880393982:  52%|█████▊     | 124/237 [01:13<01:07,  1.68it/s]Epoch: 2, train for the 125-th batch, train loss: 0.5658215880393982:  53%|█████▊     | 125/237 [01:13<01:06,  1.69it/s]Epoch: 5, train for the 143-th batch, train loss: 0.48314788937568665:  94%|█████████▍| 142/151 [00:31<00:02,  4.49it/s]Epoch: 5, train for the 143-th batch, train loss: 0.48314788937568665:  95%|█████████▍| 143/151 [00:31<00:01,  4.48it/s]evaluate for the 36-th batch, evaluate loss: 0.45016273856163025:  33%|█████▎          | 35/106 [00:09<00:19,  3.67it/s]evaluate for the 36-th batch, evaluate loss: 0.45016273856163025:  34%|█████▍          | 36/106 [00:09<00:19,  3.65it/s]Epoch: 5, train for the 144-th batch, train loss: 0.5009782314300537:  95%|██████████▍| 143/151 [00:31<00:01,  4.48it/s]Epoch: 5, train for the 144-th batch, train loss: 0.5009782314300537:  95%|██████████▍| 144/151 [00:31<00:01,  4.48it/s]Epoch: 3, train for the 56-th batch, train loss: 0.4930229187011719:  38%|████▉        | 55/146 [00:33<00:55,  1.63it/s]Epoch: 3, train for the 56-th batch, train loss: 0.4930229187011719:  38%|████▉        | 56/146 [00:33<00:55,  1.63it/s]evaluate for the 37-th batch, evaluate loss: 0.5399797558784485:  34%|█████▊           | 36/106 [00:10<00:19,  3.65it/s]evaluate for the 37-th batch, evaluate loss: 0.5399797558784485:  35%|█████▉           | 37/106 [00:10<00:18,  3.66it/s]Epoch: 3, train for the 108-th batch, train loss: 0.17336317896842957:  90%|████████▉ | 107/119 [01:04<00:07,  1.63it/s]Epoch: 3, train for the 108-th batch, train loss: 0.17336317896842957:  91%|█████████ | 108/119 [01:04<00:06,  1.63it/s]Epoch: 5, train for the 145-th batch, train loss: 0.5121170282363892:  95%|██████████▍| 144/151 [00:32<00:01,  4.48it/s]Epoch: 5, train for the 145-th batch, train loss: 0.5121170282363892:  96%|██████████▌| 145/151 [00:32<00:01,  4.48it/s]Epoch: 2, train for the 126-th batch, train loss: 0.6460685729980469:  53%|█████▊     | 125/237 [01:14<01:06,  1.69it/s]Epoch: 2, train for the 126-th batch, train loss: 0.6460685729980469:  53%|█████▊     | 126/237 [01:14<01:05,  1.68it/s]evaluate for the 38-th batch, evaluate loss: 0.5512751936912537:  35%|█████▉           | 37/106 [00:10<00:18,  3.66it/s]evaluate for the 38-th batch, evaluate loss: 0.5512751936912537:  36%|██████           | 38/106 [00:10<00:19,  3.57it/s]Epoch: 5, train for the 146-th batch, train loss: 0.507787823677063:  96%|███████████▌| 145/151 [00:32<00:01,  4.48it/s]Epoch: 5, train for the 146-th batch, train loss: 0.507787823677063:  97%|███████████▌| 146/151 [00:32<00:01,  4.48it/s]evaluate for the 39-th batch, evaluate loss: 0.42756232619285583:  36%|█████▋          | 38/106 [00:10<00:19,  3.57it/s]evaluate for the 39-th batch, evaluate loss: 0.42756232619285583:  37%|█████▉          | 39/106 [00:10<00:17,  3.76it/s]Epoch: 3, train for the 57-th batch, train loss: 0.507565438747406:  38%|█████▎        | 56/146 [00:34<00:55,  1.63it/s]Epoch: 3, train for the 57-th batch, train loss: 0.507565438747406:  39%|█████▍        | 57/146 [00:34<00:54,  1.63it/s]Epoch: 5, train for the 147-th batch, train loss: 0.5497716665267944:  97%|██████████▋| 146/151 [00:32<00:01,  4.48it/s]Epoch: 5, train for the 147-th batch, train loss: 0.5497716665267944:  97%|██████████▋| 147/151 [00:32<00:00,  4.49it/s]Epoch: 3, train for the 109-th batch, train loss: 0.22933439910411835:  91%|█████████ | 108/119 [01:05<00:06,  1.63it/s]Epoch: 3, train for the 109-th batch, train loss: 0.22933439910411835:  92%|█████████▏| 109/119 [01:05<00:06,  1.63it/s]Epoch: 2, train for the 127-th batch, train loss: 0.6475871205329895:  53%|█████▊     | 126/237 [01:14<01:05,  1.68it/s]Epoch: 2, train for the 127-th batch, train loss: 0.6475871205329895:  54%|█████▉     | 127/237 [01:14<01:05,  1.67it/s]evaluate for the 40-th batch, evaluate loss: 0.4430580139160156:  37%|██████▎          | 39/106 [00:10<00:17,  3.76it/s]evaluate for the 40-th batch, evaluate loss: 0.4430580139160156:  38%|██████▍          | 40/106 [00:10<00:18,  3.64it/s]Epoch: 5, train for the 148-th batch, train loss: 0.5902250409126282:  97%|██████████▋| 147/151 [00:32<00:00,  4.49it/s]Epoch: 5, train for the 148-th batch, train loss: 0.5902250409126282:  98%|██████████▊| 148/151 [00:32<00:00,  4.50it/s]Epoch: 5, train for the 149-th batch, train loss: 0.5165214538574219:  98%|██████████▊| 148/151 [00:32<00:00,  4.50it/s]Epoch: 5, train for the 149-th batch, train loss: 0.5165214538574219:  99%|██████████▊| 149/151 [00:32<00:00,  4.49it/s]evaluate for the 41-th batch, evaluate loss: 0.4526692032814026:  38%|██████▍          | 40/106 [00:11<00:18,  3.64it/s]evaluate for the 41-th batch, evaluate loss: 0.4526692032814026:  39%|██████▌          | 41/106 [00:11<00:17,  3.74it/s]Epoch: 3, train for the 58-th batch, train loss: 0.5086889266967773:  39%|█████        | 57/146 [00:34<00:54,  1.63it/s]Epoch: 3, train for the 58-th batch, train loss: 0.5086889266967773:  40%|█████▏       | 58/146 [00:34<00:54,  1.63it/s]Epoch: 5, train for the 150-th batch, train loss: 0.5421341061592102:  99%|██████████▊| 149/151 [00:33<00:00,  4.49it/s]Epoch: 5, train for the 150-th batch, train loss: 0.5421341061592102:  99%|██████████▉| 150/151 [00:33<00:00,  4.49it/s]Epoch: 3, train for the 110-th batch, train loss: 0.18609756231307983:  92%|█████████▏| 109/119 [01:05<00:06,  1.63it/s]Epoch: 3, train for the 110-th batch, train loss: 0.18609756231307983:  92%|█████████▏| 110/119 [01:05<00:05,  1.63it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5761772990226746:  54%|█████▉     | 127/237 [01:15<01:05,  1.67it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5761772990226746:  54%|█████▉     | 128/237 [01:15<01:05,  1.68it/s]evaluate for the 42-th batch, evaluate loss: 0.39638644456863403:  39%|██████▏         | 41/106 [00:11<00:17,  3.74it/s]evaluate for the 42-th batch, evaluate loss: 0.39638644456863403:  40%|██████▎         | 42/106 [00:11<00:17,  3.56it/s]Epoch: 5, train for the 151-th batch, train loss: 0.5963274240493774:  99%|██████████▉| 150/151 [00:33<00:00,  4.49it/s]Epoch: 5, train for the 151-th batch, train loss: 0.5963274240493774: 100%|███████████| 151/151 [00:33<00:00,  4.96it/s]Epoch: 5, train for the 151-th batch, train loss: 0.5963274240493774: 100%|███████████| 151/151 [00:33<00:00,  4.53it/s]
  0%|                                                                                            | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.4966942369937897:   0%|                            | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.4966942369937897:   2%|▍                   | 1/46 [00:00<00:04,  9.57it/s]evaluate for the 2-th batch, evaluate loss: 0.5072488188743591:   2%|▍                   | 1/46 [00:00<00:04,  9.57it/s]evaluate for the 2-th batch, evaluate loss: 0.5072488188743591:   4%|▊                   | 2/46 [00:00<00:04,  9.59it/s]evaluate for the 43-th batch, evaluate loss: 0.35373517870903015:  40%|██████▎         | 42/106 [00:11<00:17,  3.56it/s]evaluate for the 43-th batch, evaluate loss: 0.35373517870903015:  41%|██████▍         | 43/106 [00:11<00:17,  3.61it/s]evaluate for the 3-th batch, evaluate loss: 0.48294273018836975:   4%|▊                  | 2/46 [00:00<00:04,  9.59it/s]evaluate for the 3-th batch, evaluate loss: 0.48294273018836975:   7%|█▏                 | 3/46 [00:00<00:04,  9.56it/s]Epoch: 3, train for the 59-th batch, train loss: 0.4498946964740753:  40%|█████▏       | 58/146 [00:35<00:54,  1.63it/s]Epoch: 3, train for the 59-th batch, train loss: 0.4498946964740753:  40%|█████▎       | 59/146 [00:35<00:53,  1.63it/s]evaluate for the 4-th batch, evaluate loss: 0.5138859748840332:   7%|█▎                  | 3/46 [00:00<00:04,  9.56it/s]evaluate for the 4-th batch, evaluate loss: 0.5138859748840332:   9%|█▋                  | 4/46 [00:00<00:04,  9.58it/s]evaluate for the 44-th batch, evaluate loss: 0.44349223375320435:  41%|██████▍         | 43/106 [00:11<00:17,  3.61it/s]evaluate for the 44-th batch, evaluate loss: 0.44349223375320435:  42%|██████▋         | 44/106 [00:11<00:17,  3.55it/s]evaluate for the 5-th batch, evaluate loss: 0.4778328537940979:   9%|█▋                  | 4/46 [00:00<00:04,  9.58it/s]evaluate for the 5-th batch, evaluate loss: 0.4778328537940979:  11%|██▏                 | 5/46 [00:00<00:04,  9.57it/s]Epoch: 3, train for the 111-th batch, train loss: 0.22836415469646454:  92%|█████████▏| 110/119 [01:06<00:05,  1.63it/s]Epoch: 3, train for the 111-th batch, train loss: 0.22836415469646454:  93%|█████████▎| 111/119 [01:06<00:04,  1.63it/s]Epoch: 2, train for the 129-th batch, train loss: 0.6240137219429016:  54%|█████▉     | 128/237 [01:15<01:05,  1.68it/s]Epoch: 2, train for the 129-th batch, train loss: 0.6240137219429016:  54%|█████▉     | 129/237 [01:15<01:04,  1.68it/s]evaluate for the 6-th batch, evaluate loss: 0.5508747696876526:  11%|██▏                 | 5/46 [00:00<00:04,  9.57it/s]evaluate for the 6-th batch, evaluate loss: 0.5508747696876526:  13%|██▌                 | 6/46 [00:00<00:04,  9.58it/s]evaluate for the 7-th batch, evaluate loss: 0.4871149957180023:  13%|██▌                 | 6/46 [00:00<00:04,  9.58it/s]evaluate for the 7-th batch, evaluate loss: 0.4871149957180023:  15%|███                 | 7/46 [00:00<00:04,  9.60it/s]evaluate for the 45-th batch, evaluate loss: 0.4686613976955414:  42%|███████          | 44/106 [00:12<00:17,  3.55it/s]evaluate for the 45-th batch, evaluate loss: 0.4686613976955414:  42%|███████▏         | 45/106 [00:12<00:16,  3.68it/s]evaluate for the 8-th batch, evaluate loss: 0.568996787071228:  15%|███▏                 | 7/46 [00:00<00:04,  9.60it/s]evaluate for the 8-th batch, evaluate loss: 0.568996787071228:  17%|███▋                 | 8/46 [00:00<00:03,  9.60it/s]evaluate for the 9-th batch, evaluate loss: 0.5254374146461487:  17%|███▍                | 8/46 [00:00<00:03,  9.60it/s]evaluate for the 9-th batch, evaluate loss: 0.5254374146461487:  20%|███▉                | 9/46 [00:00<00:03,  9.61it/s]Epoch: 3, train for the 60-th batch, train loss: 0.5230604410171509:  40%|█████▎       | 59/146 [00:35<00:53,  1.63it/s]Epoch: 3, train for the 60-th batch, train loss: 0.5230604410171509:  41%|█████▎       | 60/146 [00:35<00:52,  1.63it/s]evaluate for the 10-th batch, evaluate loss: 0.5333166718482971:  20%|███▋               | 9/46 [00:01<00:03,  9.61it/s]evaluate for the 10-th batch, evaluate loss: 0.5333166718482971:  22%|███▉              | 10/46 [00:01<00:03,  9.60it/s]evaluate for the 46-th batch, evaluate loss: 0.4560180604457855:  42%|███████▏         | 45/106 [00:12<00:16,  3.68it/s]evaluate for the 46-th batch, evaluate loss: 0.4560180604457855:  43%|███████▍         | 46/106 [00:12<00:16,  3.62it/s]Epoch: 2, train for the 130-th batch, train loss: 0.6047244668006897:  54%|█████▉     | 129/237 [01:16<01:04,  1.68it/s]Epoch: 2, train for the 130-th batch, train loss: 0.6047244668006897:  55%|██████     | 130/237 [01:16<01:03,  1.68it/s]Epoch: 3, train for the 112-th batch, train loss: 0.15341414511203766:  93%|█████████▎| 111/119 [01:07<00:04,  1.63it/s]Epoch: 3, train for the 112-th batch, train loss: 0.15341414511203766:  94%|█████████▍| 112/119 [01:07<00:04,  1.63it/s]evaluate for the 11-th batch, evaluate loss: 0.5276584625244141:  22%|███▉              | 10/46 [00:01<00:03,  9.60it/s]evaluate for the 11-th batch, evaluate loss: 0.5276584625244141:  24%|████▎             | 11/46 [00:01<00:03,  9.61it/s]evaluate for the 12-th batch, evaluate loss: 0.477948397397995:  24%|████▌              | 11/46 [00:01<00:03,  9.61it/s]evaluate for the 12-th batch, evaluate loss: 0.477948397397995:  26%|████▉              | 12/46 [00:01<00:03,  9.62it/s]evaluate for the 47-th batch, evaluate loss: 0.36390045285224915:  43%|██████▉         | 46/106 [00:12<00:16,  3.62it/s]evaluate for the 47-th batch, evaluate loss: 0.36390045285224915:  44%|███████         | 47/106 [00:12<00:15,  3.72it/s]evaluate for the 13-th batch, evaluate loss: 0.4994139075279236:  26%|████▋             | 12/46 [00:01<00:03,  9.62it/s]evaluate for the 13-th batch, evaluate loss: 0.4994139075279236:  28%|█████             | 13/46 [00:01<00:03,  9.65it/s]evaluate for the 14-th batch, evaluate loss: 0.5852485299110413:  28%|█████             | 13/46 [00:01<00:03,  9.65it/s]evaluate for the 14-th batch, evaluate loss: 0.5852485299110413:  30%|█████▍            | 14/46 [00:01<00:03,  9.65it/s]evaluate for the 15-th batch, evaluate loss: 0.5504641532897949:  30%|█████▍            | 14/46 [00:01<00:03,  9.65it/s]evaluate for the 15-th batch, evaluate loss: 0.5504641532897949:  33%|█████▊            | 15/46 [00:01<00:03,  9.63it/s]evaluate for the 48-th batch, evaluate loss: 0.4871758222579956:  44%|███████▌         | 47/106 [00:13<00:15,  3.72it/s]evaluate for the 48-th batch, evaluate loss: 0.4871758222579956:  45%|███████▋         | 48/106 [00:13<00:15,  3.64it/s]Epoch: 3, train for the 61-th batch, train loss: 0.5279560685157776:  41%|█████▎       | 60/146 [00:36<00:52,  1.63it/s]Epoch: 3, train for the 61-th batch, train loss: 0.5279560685157776:  42%|█████▍       | 61/146 [00:36<00:52,  1.63it/s]evaluate for the 16-th batch, evaluate loss: 0.5721473693847656:  33%|█████▊            | 15/46 [00:01<00:03,  9.63it/s]evaluate for the 16-th batch, evaluate loss: 0.5721473693847656:  35%|██████▎           | 16/46 [00:01<00:03,  9.63it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5874301791191101:  55%|██████     | 130/237 [01:17<01:03,  1.68it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5874301791191101:  55%|██████     | 131/237 [01:17<01:03,  1.67it/s]Epoch: 3, train for the 113-th batch, train loss: 0.18626873195171356:  94%|█████████▍| 112/119 [01:07<00:04,  1.63it/s]Epoch: 3, train for the 113-th batch, train loss: 0.18626873195171356:  95%|█████████▍| 113/119 [01:07<00:03,  1.63it/s]evaluate for the 17-th batch, evaluate loss: 0.4553390443325043:  35%|██████▎           | 16/46 [00:01<00:03,  9.63it/s]evaluate for the 17-th batch, evaluate loss: 0.4553390443325043:  37%|██████▋           | 17/46 [00:01<00:03,  9.63it/s]evaluate for the 49-th batch, evaluate loss: 0.4738162159919739:  45%|███████▋         | 48/106 [00:13<00:15,  3.64it/s]evaluate for the 49-th batch, evaluate loss: 0.4738162159919739:  46%|███████▊         | 49/106 [00:13<00:15,  3.59it/s]evaluate for the 18-th batch, evaluate loss: 0.5046002864837646:  37%|██████▋           | 17/46 [00:01<00:03,  9.63it/s]evaluate for the 18-th batch, evaluate loss: 0.5046002864837646:  39%|███████           | 18/46 [00:01<00:02,  9.57it/s]evaluate for the 19-th batch, evaluate loss: 0.5251949429512024:  39%|███████           | 18/46 [00:01<00:02,  9.57it/s]evaluate for the 19-th batch, evaluate loss: 0.5251949429512024:  41%|███████▍          | 19/46 [00:01<00:02,  9.57it/s]evaluate for the 20-th batch, evaluate loss: 0.5348638892173767:  41%|███████▍          | 19/46 [00:02<00:02,  9.57it/s]evaluate for the 20-th batch, evaluate loss: 0.5348638892173767:  43%|███████▊          | 20/46 [00:02<00:02,  9.57it/s]evaluate for the 50-th batch, evaluate loss: 0.4051746129989624:  46%|███████▊         | 49/106 [00:13<00:15,  3.59it/s]evaluate for the 50-th batch, evaluate loss: 0.4051746129989624:  47%|████████         | 50/106 [00:13<00:15,  3.62it/s]evaluate for the 21-th batch, evaluate loss: 0.5304017663002014:  43%|███████▊          | 20/46 [00:02<00:02,  9.57it/s]evaluate for the 21-th batch, evaluate loss: 0.5304017663002014:  46%|████████▏         | 21/46 [00:02<00:02,  9.57it/s]Epoch: 3, train for the 62-th batch, train loss: 0.4926263689994812:  42%|█████▍       | 61/146 [00:37<00:52,  1.63it/s]Epoch: 3, train for the 62-th batch, train loss: 0.4926263689994812:  42%|█████▌       | 62/146 [00:37<00:51,  1.63it/s]evaluate for the 22-th batch, evaluate loss: 0.5208452939987183:  46%|████████▏         | 21/46 [00:02<00:02,  9.57it/s]evaluate for the 22-th batch, evaluate loss: 0.5208452939987183:  48%|████████▌         | 22/46 [00:02<00:02,  9.58it/s]Epoch: 2, train for the 132-th batch, train loss: 0.6179702877998352:  55%|██████     | 131/237 [01:17<01:03,  1.67it/s]Epoch: 2, train for the 132-th batch, train loss: 0.6179702877998352:  56%|██████▏    | 132/237 [01:17<01:02,  1.67it/s]Epoch: 3, train for the 114-th batch, train loss: 0.19693411886692047:  95%|█████████▍| 113/119 [01:08<00:03,  1.63it/s]Epoch: 3, train for the 114-th batch, train loss: 0.19693411886692047:  96%|█████████▌| 114/119 [01:08<00:03,  1.63it/s]evaluate for the 23-th batch, evaluate loss: 0.4744422733783722:  48%|████████▌         | 22/46 [00:02<00:02,  9.58it/s]evaluate for the 23-th batch, evaluate loss: 0.4744422733783722:  50%|█████████         | 23/46 [00:02<00:02,  9.60it/s]evaluate for the 51-th batch, evaluate loss: 0.49776431918144226:  47%|███████▌        | 50/106 [00:13<00:15,  3.62it/s]evaluate for the 51-th batch, evaluate loss: 0.49776431918144226:  48%|███████▋        | 51/106 [00:13<00:15,  3.55it/s]evaluate for the 24-th batch, evaluate loss: 0.4852641224861145:  50%|█████████         | 23/46 [00:02<00:02,  9.60it/s]evaluate for the 24-th batch, evaluate loss: 0.4852641224861145:  52%|█████████▍        | 24/46 [00:02<00:02,  9.59it/s]evaluate for the 25-th batch, evaluate loss: 0.5348256826400757:  52%|█████████▍        | 24/46 [00:02<00:02,  9.59it/s]evaluate for the 25-th batch, evaluate loss: 0.5348256826400757:  54%|█████████▊        | 25/46 [00:02<00:02,  9.58it/s]evaluate for the 52-th batch, evaluate loss: 0.4425140917301178:  48%|████████▏        | 51/106 [00:14<00:15,  3.55it/s]evaluate for the 52-th batch, evaluate loss: 0.4425140917301178:  49%|████████▎        | 52/106 [00:14<00:14,  3.70it/s]evaluate for the 26-th batch, evaluate loss: 0.5578882694244385:  54%|█████████▊        | 25/46 [00:02<00:02,  9.58it/s]evaluate for the 26-th batch, evaluate loss: 0.5578882694244385:  57%|██████████▏       | 26/46 [00:02<00:02,  9.57it/s]evaluate for the 27-th batch, evaluate loss: 0.4994911849498749:  57%|██████████▏       | 26/46 [00:02<00:02,  9.57it/s]evaluate for the 27-th batch, evaluate loss: 0.4994911849498749:  59%|██████████▌       | 27/46 [00:02<00:01,  9.60it/s]Epoch: 3, train for the 63-th batch, train loss: 0.495959997177124:  42%|█████▉        | 62/146 [00:37<00:51,  1.63it/s]Epoch: 3, train for the 63-th batch, train loss: 0.495959997177124:  43%|██████        | 63/146 [00:37<00:51,  1.62it/s]evaluate for the 28-th batch, evaluate loss: 0.526935875415802:  59%|███████████▏       | 27/46 [00:02<00:01,  9.60it/s]evaluate for the 28-th batch, evaluate loss: 0.526935875415802:  61%|███████████▌       | 28/46 [00:02<00:01,  9.58it/s]Epoch: 2, train for the 133-th batch, train loss: 0.6198210120201111:  56%|██████▏    | 132/237 [01:18<01:02,  1.67it/s]Epoch: 2, train for the 133-th batch, train loss: 0.6198210120201111:  56%|██████▏    | 133/237 [01:18<01:01,  1.68it/s]evaluate for the 53-th batch, evaluate loss: 0.5639916062355042:  49%|████████▎        | 52/106 [00:14<00:14,  3.70it/s]evaluate for the 53-th batch, evaluate loss: 0.5639916062355042:  50%|████████▌        | 53/106 [00:14<00:14,  3.58it/s]Epoch: 3, train for the 115-th batch, train loss: 0.18656164407730103:  96%|█████████▌| 114/119 [01:08<00:03,  1.63it/s]Epoch: 3, train for the 115-th batch, train loss: 0.18656164407730103:  97%|█████████▋| 115/119 [01:08<00:02,  1.62it/s]evaluate for the 29-th batch, evaluate loss: 0.4933473765850067:  61%|██████████▉       | 28/46 [00:03<00:01,  9.58it/s]evaluate for the 29-th batch, evaluate loss: 0.4933473765850067:  63%|███████████▎      | 29/46 [00:03<00:01,  9.56it/s]evaluate for the 30-th batch, evaluate loss: 0.49846771359443665:  63%|██████████▋      | 29/46 [00:03<00:01,  9.56it/s]evaluate for the 30-th batch, evaluate loss: 0.49846771359443665:  65%|███████████      | 30/46 [00:03<00:01,  9.49it/s]evaluate for the 54-th batch, evaluate loss: 0.40424421429634094:  50%|████████        | 53/106 [00:14<00:14,  3.58it/s]evaluate for the 54-th batch, evaluate loss: 0.40424421429634094:  51%|████████▏       | 54/106 [00:14<00:13,  3.75it/s]evaluate for the 31-th batch, evaluate loss: 0.5171859264373779:  65%|███████████▋      | 30/46 [00:03<00:01,  9.49it/s]evaluate for the 31-th batch, evaluate loss: 0.5171859264373779:  67%|████████████▏     | 31/46 [00:03<00:01,  9.55it/s]evaluate for the 32-th batch, evaluate loss: 0.4787464439868927:  67%|████████████▏     | 31/46 [00:03<00:01,  9.55it/s]evaluate for the 32-th batch, evaluate loss: 0.4787464439868927:  70%|████████████▌     | 32/46 [00:03<00:01,  9.57it/s]evaluate for the 33-th batch, evaluate loss: 0.5027819871902466:  70%|████████████▌     | 32/46 [00:03<00:01,  9.57it/s]evaluate for the 33-th batch, evaluate loss: 0.5027819871902466:  72%|████████████▉     | 33/46 [00:03<00:01,  9.57it/s]Epoch: 3, train for the 64-th batch, train loss: 0.5382006168365479:  43%|█████▌       | 63/146 [00:38<00:51,  1.62it/s]Epoch: 3, train for the 64-th batch, train loss: 0.5382006168365479:  44%|█████▋       | 64/146 [00:38<00:50,  1.62it/s]Epoch: 2, train for the 134-th batch, train loss: 0.6262673139572144:  56%|██████▏    | 133/237 [01:18<01:01,  1.68it/s]Epoch: 2, train for the 134-th batch, train loss: 0.6262673139572144:  57%|██████▏    | 134/237 [01:18<01:01,  1.67it/s]evaluate for the 55-th batch, evaluate loss: 0.3691107928752899:  51%|████████▋        | 54/106 [00:14<00:13,  3.75it/s]evaluate for the 55-th batch, evaluate loss: 0.3691107928752899:  52%|████████▊        | 55/106 [00:14<00:14,  3.56it/s]evaluate for the 34-th batch, evaluate loss: 0.49129295349121094:  72%|████████████▏    | 33/46 [00:03<00:01,  9.57it/s]evaluate for the 34-th batch, evaluate loss: 0.49129295349121094:  74%|████████████▌    | 34/46 [00:03<00:01,  9.58it/s]Epoch: 3, train for the 116-th batch, train loss: 0.14650845527648926:  97%|█████████▋| 115/119 [01:09<00:02,  1.62it/s]Epoch: 3, train for the 116-th batch, train loss: 0.14650845527648926:  97%|█████████▋| 116/119 [01:09<00:01,  1.62it/s]evaluate for the 35-th batch, evaluate loss: 0.49203506112098694:  74%|████████████▌    | 34/46 [00:03<00:01,  9.58it/s]evaluate for the 35-th batch, evaluate loss: 0.49203506112098694:  76%|████████████▉    | 35/46 [00:03<00:01,  9.59it/s]evaluate for the 36-th batch, evaluate loss: 0.47630685567855835:  76%|████████████▉    | 35/46 [00:03<00:01,  9.59it/s]evaluate for the 36-th batch, evaluate loss: 0.47630685567855835:  78%|█████████████▎   | 36/46 [00:03<00:01,  9.61it/s]evaluate for the 56-th batch, evaluate loss: 0.4607308804988861:  52%|████████▊        | 55/106 [00:15<00:14,  3.56it/s]evaluate for the 56-th batch, evaluate loss: 0.4607308804988861:  53%|████████▉        | 56/106 [00:15<00:13,  3.62it/s]evaluate for the 37-th batch, evaluate loss: 0.5041649341583252:  78%|██████████████    | 36/46 [00:03<00:01,  9.61it/s]evaluate for the 37-th batch, evaluate loss: 0.5041649341583252:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.56it/s]evaluate for the 38-th batch, evaluate loss: 0.5379012227058411:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.56it/s]evaluate for the 38-th batch, evaluate loss: 0.5379012227058411:  83%|██████████████▊   | 38/46 [00:03<00:00,  9.58it/s]Epoch: 3, train for the 65-th batch, train loss: 0.5611308217048645:  44%|█████▋       | 64/146 [00:39<00:50,  1.62it/s]Epoch: 3, train for the 65-th batch, train loss: 0.5611308217048645:  45%|█████▊       | 65/146 [00:39<00:49,  1.62it/s]evaluate for the 39-th batch, evaluate loss: 0.5388643741607666:  83%|██████████████▊   | 38/46 [00:04<00:00,  9.58it/s]evaluate for the 39-th batch, evaluate loss: 0.5388643741607666:  85%|███████████████▎  | 39/46 [00:04<00:00,  9.58it/s]evaluate for the 57-th batch, evaluate loss: 0.4182246923446655:  53%|████████▉        | 56/106 [00:15<00:13,  3.62it/s]evaluate for the 57-th batch, evaluate loss: 0.4182246923446655:  54%|█████████▏       | 57/106 [00:15<00:13,  3.55it/s]Epoch: 2, train for the 135-th batch, train loss: 0.6558049917221069:  57%|██████▏    | 134/237 [01:19<01:01,  1.67it/s]Epoch: 2, train for the 135-th batch, train loss: 0.6558049917221069:  57%|██████▎    | 135/237 [01:19<01:00,  1.68it/s]evaluate for the 40-th batch, evaluate loss: 0.47568878531455994:  85%|██████████████▍  | 39/46 [00:04<00:00,  9.58it/s]evaluate for the 40-th batch, evaluate loss: 0.47568878531455994:  87%|██████████████▊  | 40/46 [00:04<00:00,  9.58it/s]Epoch: 3, train for the 117-th batch, train loss: 0.2008756548166275:  97%|██████████▋| 116/119 [01:10<00:01,  1.62it/s]Epoch: 3, train for the 117-th batch, train loss: 0.2008756548166275:  98%|██████████▊| 117/119 [01:10<00:01,  1.62it/s]evaluate for the 41-th batch, evaluate loss: 0.48557737469673157:  87%|██████████████▊  | 40/46 [00:04<00:00,  9.58it/s]evaluate for the 41-th batch, evaluate loss: 0.48557737469673157:  89%|███████████████▏ | 41/46 [00:04<00:00,  9.60it/s]evaluate for the 58-th batch, evaluate loss: 0.42630836367607117:  54%|████████▌       | 57/106 [00:15<00:13,  3.55it/s]evaluate for the 58-th batch, evaluate loss: 0.42630836367607117:  55%|████████▊       | 58/106 [00:15<00:13,  3.66it/s]evaluate for the 42-th batch, evaluate loss: 0.4675937592983246:  89%|████████████████  | 41/46 [00:04<00:00,  9.60it/s]evaluate for the 42-th batch, evaluate loss: 0.4675937592983246:  91%|████████████████▍ | 42/46 [00:04<00:00,  9.58it/s]evaluate for the 43-th batch, evaluate loss: 0.5361969470977783:  91%|████████████████▍ | 42/46 [00:04<00:00,  9.58it/s]evaluate for the 43-th batch, evaluate loss: 0.5361969470977783:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.59it/s]evaluate for the 44-th batch, evaluate loss: 0.5153629779815674:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.59it/s]evaluate for the 44-th batch, evaluate loss: 0.5153629779815674:  96%|█████████████████▏| 44/46 [00:04<00:00,  9.60it/s]evaluate for the 59-th batch, evaluate loss: 0.45733246207237244:  55%|████████▊       | 58/106 [00:16<00:13,  3.66it/s]evaluate for the 59-th batch, evaluate loss: 0.45733246207237244:  56%|████████▉       | 59/106 [00:16<00:13,  3.60it/s]Epoch: 3, train for the 66-th batch, train loss: 0.55722975730896:  45%|██████▋        | 65/146 [00:39<00:49,  1.62it/s]Epoch: 3, train for the 66-th batch, train loss: 0.55722975730896:  45%|██████▊        | 66/146 [00:39<00:49,  1.62it/s]Epoch: 2, train for the 136-th batch, train loss: 0.5956171751022339:  57%|██████▎    | 135/237 [01:20<01:00,  1.68it/s]evaluate for the 45-th batch, evaluate loss: 0.5015548467636108:  96%|█████████████████▏| 44/46 [00:04<00:00,  9.60it/s]Epoch: 2, train for the 136-th batch, train loss: 0.5956171751022339:  57%|██████▎    | 136/237 [01:20<00:59,  1.69it/s]evaluate for the 45-th batch, evaluate loss: 0.5015548467636108:  98%|█████████████████▌| 45/46 [00:04<00:00,  9.62it/s]evaluate for the 46-th batch, evaluate loss: 0.509050190448761:  98%|██████████████████▌| 45/46 [00:04<00:00,  9.62it/s]evaluate for the 46-th batch, evaluate loss: 0.509050190448761: 100%|███████████████████| 46/46 [00:04<00:00,  9.62it/s]
  0%|                                                                                            | 0/25 [00:00<?, ?it/s]Epoch: 3, train for the 118-th batch, train loss: 0.1708701252937317:  98%|██████████▊| 117/119 [01:10<00:01,  1.62it/s]Epoch: 3, train for the 118-th batch, train loss: 0.1708701252937317:  99%|██████████▉| 118/119 [01:10<00:00,  1.62it/s]evaluate for the 60-th batch, evaluate loss: 0.45335423946380615:  56%|████████▉       | 59/106 [00:16<00:13,  3.60it/s]evaluate for the 60-th batch, evaluate loss: 0.45335423946380615:  57%|█████████       | 60/106 [00:16<00:12,  3.69it/s]evaluate for the 1-th batch, evaluate loss: 0.6454906463623047:   0%|                            | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6454906463623047:   4%|▊                   | 1/25 [00:00<00:02,  9.13it/s]evaluate for the 2-th batch, evaluate loss: 0.6549923419952393:   4%|▊                   | 1/25 [00:00<00:02,  9.13it/s]evaluate for the 2-th batch, evaluate loss: 0.6549923419952393:   8%|█▌                  | 2/25 [00:00<00:02,  9.18it/s]Epoch: 3, train for the 67-th batch, train loss: 0.5370488166809082:  45%|█████▉       | 66/146 [00:39<00:49,  1.62it/s]Epoch: 3, train for the 67-th batch, train loss: 0.5370488166809082:  46%|█████▉       | 67/146 [00:39<00:42,  1.86it/s]evaluate for the 3-th batch, evaluate loss: 0.6991605162620544:   8%|█▌                  | 2/25 [00:00<00:02,  9.18it/s]evaluate for the 3-th batch, evaluate loss: 0.6991605162620544:  12%|██▍                 | 3/25 [00:00<00:02,  9.19it/s]evaluate for the 61-th batch, evaluate loss: 0.4809926748275757:  57%|█████████▌       | 60/106 [00:16<00:12,  3.69it/s]evaluate for the 61-th batch, evaluate loss: 0.4809926748275757:  58%|█████████▊       | 61/106 [00:16<00:12,  3.64it/s]evaluate for the 4-th batch, evaluate loss: 0.6837394833564758:  12%|██▍                 | 3/25 [00:00<00:02,  9.19it/s]evaluate for the 4-th batch, evaluate loss: 0.6837394833564758:  16%|███▏                | 4/25 [00:00<00:02,  9.17it/s]Epoch: 3, train for the 119-th batch, train loss: 0.36777448654174805:  99%|█████████▉| 118/119 [01:11<00:00,  1.62it/s]Epoch: 3, train for the 119-th batch, train loss: 0.36777448654174805: 100%|██████████| 119/119 [01:11<00:00,  1.76it/s]Epoch: 3, train for the 119-th batch, train loss: 0.36777448654174805: 100%|██████████| 119/119 [01:11<00:00,  1.67it/s]
  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 2, train for the 137-th batch, train loss: 0.6579425930976868:  57%|██████▎    | 136/237 [01:20<00:59,  1.69it/s]Epoch: 2, train for the 137-th batch, train loss: 0.6579425930976868:  58%|██████▎    | 137/237 [01:20<00:59,  1.68it/s]evaluate for the 5-th batch, evaluate loss: 0.685452938079834:  16%|███▎                 | 4/25 [00:00<00:02,  9.17it/s]evaluate for the 5-th batch, evaluate loss: 0.685452938079834:  20%|████▏                | 5/25 [00:00<00:02,  9.16it/s]evaluate for the 6-th batch, evaluate loss: 0.7307963371276855:  20%|████                | 5/25 [00:00<00:02,  9.16it/s]evaluate for the 6-th batch, evaluate loss: 0.7307963371276855:  24%|████▊               | 6/25 [00:00<00:02,  9.16it/s]evaluate for the 62-th batch, evaluate loss: 0.4907713234424591:  58%|█████████▊       | 61/106 [00:16<00:12,  3.64it/s]evaluate for the 62-th batch, evaluate loss: 0.4907713234424591:  58%|█████████▉       | 62/106 [00:16<00:12,  3.66it/s]evaluate for the 7-th batch, evaluate loss: 0.7507951855659485:  24%|████▊               | 6/25 [00:00<00:02,  9.16it/s]evaluate for the 7-th batch, evaluate loss: 0.7507951855659485:  28%|█████▌              | 7/25 [00:00<00:01,  9.18it/s]evaluate for the 1-th batch, evaluate loss: 0.20672541856765747:   0%|                           | 0/40 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.20672541856765747:   2%|▍                  | 1/40 [00:00<00:11,  3.39it/s]Epoch: 3, train for the 68-th batch, train loss: 0.5491249561309814:  46%|█████▉       | 67/146 [00:40<00:42,  1.86it/s]Epoch: 3, train for the 68-th batch, train loss: 0.5491249561309814:  47%|██████       | 68/146 [00:40<00:43,  1.81it/s]evaluate for the 8-th batch, evaluate loss: 0.7333505153656006:  28%|█████▌              | 7/25 [00:00<00:01,  9.18it/s]evaluate for the 8-th batch, evaluate loss: 0.7333505153656006:  32%|██████▍             | 8/25 [00:00<00:01,  9.19it/s]evaluate for the 63-th batch, evaluate loss: 0.4811684191226959:  58%|█████████▉       | 62/106 [00:17<00:12,  3.66it/s]evaluate for the 63-th batch, evaluate loss: 0.4811684191226959:  59%|██████████       | 63/106 [00:17<00:11,  3.67it/s]evaluate for the 9-th batch, evaluate loss: 0.7134738564491272:  32%|██████▍             | 8/25 [00:00<00:01,  9.19it/s]evaluate for the 9-th batch, evaluate loss: 0.7134738564491272:  36%|███████▏            | 9/25 [00:00<00:01,  9.21it/s]evaluate for the 2-th batch, evaluate loss: 0.2051803171634674:   2%|▌                   | 1/40 [00:00<00:11,  3.39it/s]evaluate for the 2-th batch, evaluate loss: 0.2051803171634674:   5%|█                   | 2/40 [00:00<00:10,  3.69it/s]evaluate for the 10-th batch, evaluate loss: 0.755686342716217:  36%|███████▏            | 9/25 [00:01<00:01,  9.21it/s]evaluate for the 10-th batch, evaluate loss: 0.755686342716217:  40%|███████▌           | 10/25 [00:01<00:01,  9.20it/s]Epoch: 2, train for the 138-th batch, train loss: 0.6724542379379272:  58%|██████▎    | 137/237 [01:21<00:59,  1.68it/s]Epoch: 2, train for the 138-th batch, train loss: 0.6724542379379272:  58%|██████▍    | 138/237 [01:21<00:58,  1.68it/s]evaluate for the 11-th batch, evaluate loss: 0.7451044917106628:  40%|███████▏          | 10/25 [00:01<00:01,  9.20it/s]evaluate for the 11-th batch, evaluate loss: 0.7451044917106628:  44%|███████▉          | 11/25 [00:01<00:01,  9.21it/s]evaluate for the 64-th batch, evaluate loss: 0.4308241605758667:  59%|██████████       | 63/106 [00:17<00:11,  3.67it/s]evaluate for the 64-th batch, evaluate loss: 0.4308241605758667:  60%|██████████▎      | 64/106 [00:17<00:11,  3.58it/s]evaluate for the 12-th batch, evaluate loss: 0.7067801356315613:  44%|███████▉          | 11/25 [00:01<00:01,  9.21it/s]evaluate for the 12-th batch, evaluate loss: 0.7067801356315613:  48%|████████▋         | 12/25 [00:01<00:01,  9.22it/s]evaluate for the 3-th batch, evaluate loss: 0.2629553973674774:   5%|█                   | 2/40 [00:00<00:10,  3.69it/s]evaluate for the 3-th batch, evaluate loss: 0.2629553973674774:   8%|█▌                  | 3/40 [00:00<00:10,  3.55it/s]evaluate for the 13-th batch, evaluate loss: 0.6813226938247681:  48%|████████▋         | 12/25 [00:01<00:01,  9.22it/s]evaluate for the 13-th batch, evaluate loss: 0.6813226938247681:  52%|█████████▎        | 13/25 [00:01<00:01,  9.22it/s]Epoch: 3, train for the 69-th batch, train loss: 0.5106036067008972:  47%|██████       | 68/146 [00:41<00:43,  1.81it/s]Epoch: 3, train for the 69-th batch, train loss: 0.5106036067008972:  47%|██████▏      | 69/146 [00:41<00:43,  1.77it/s]evaluate for the 65-th batch, evaluate loss: 0.5283054709434509:  60%|██████████▎      | 64/106 [00:17<00:11,  3.58it/s]evaluate for the 65-th batch, evaluate loss: 0.5283054709434509:  61%|██████████▍      | 65/106 [00:17<00:11,  3.71it/s]evaluate for the 14-th batch, evaluate loss: 0.7619601488113403:  52%|█████████▎        | 13/25 [00:01<00:01,  9.22it/s]evaluate for the 14-th batch, evaluate loss: 0.7619601488113403:  56%|██████████        | 14/25 [00:01<00:01,  9.20it/s]evaluate for the 4-th batch, evaluate loss: 0.1794768124818802:   8%|█▌                  | 3/40 [00:01<00:10,  3.55it/s]evaluate for the 4-th batch, evaluate loss: 0.1794768124818802:  10%|██                  | 4/40 [00:01<00:09,  3.67it/s]evaluate for the 15-th batch, evaluate loss: 0.7524849772453308:  56%|██████████        | 14/25 [00:01<00:01,  9.20it/s]evaluate for the 15-th batch, evaluate loss: 0.7524849772453308:  60%|██████████▊       | 15/25 [00:01<00:01,  9.19it/s]Epoch: 2, train for the 139-th batch, train loss: 0.6346923112869263:  58%|██████▍    | 138/237 [01:21<00:58,  1.68it/s]Epoch: 2, train for the 139-th batch, train loss: 0.6346923112869263:  59%|██████▍    | 139/237 [01:21<00:57,  1.69it/s]evaluate for the 16-th batch, evaluate loss: 0.6725114583969116:  60%|██████████▊       | 15/25 [00:01<00:01,  9.19it/s]evaluate for the 16-th batch, evaluate loss: 0.6725114583969116:  64%|███████████▌      | 16/25 [00:01<00:00,  9.19it/s]evaluate for the 66-th batch, evaluate loss: 0.4304979741573334:  61%|██████████▍      | 65/106 [00:17<00:11,  3.71it/s]evaluate for the 66-th batch, evaluate loss: 0.4304979741573334:  62%|██████████▌      | 66/106 [00:17<00:11,  3.59it/s]evaluate for the 17-th batch, evaluate loss: 0.6746996641159058:  64%|███████████▌      | 16/25 [00:01<00:00,  9.19it/s]evaluate for the 17-th batch, evaluate loss: 0.6746996641159058:  68%|████████████▏     | 17/25 [00:01<00:00,  9.18it/s]evaluate for the 5-th batch, evaluate loss: 0.22684603929519653:  10%|█▉                 | 4/40 [00:01<00:09,  3.67it/s]evaluate for the 5-th batch, evaluate loss: 0.22684603929519653:  12%|██▍                | 5/40 [00:01<00:09,  3.61it/s]evaluate for the 18-th batch, evaluate loss: 0.6333367228507996:  68%|████████████▏     | 17/25 [00:01<00:00,  9.18it/s]evaluate for the 18-th batch, evaluate loss: 0.6333367228507996:  72%|████████████▉     | 18/25 [00:01<00:00,  9.17it/s]evaluate for the 67-th batch, evaluate loss: 0.4134197533130646:  62%|██████████▌      | 66/106 [00:18<00:11,  3.59it/s]evaluate for the 67-th batch, evaluate loss: 0.4134197533130646:  63%|██████████▋      | 67/106 [00:18<00:10,  3.77it/s]Epoch: 3, train for the 70-th batch, train loss: 0.5517824292182922:  47%|██████▏      | 69/146 [00:41<00:43,  1.77it/s]Epoch: 3, train for the 70-th batch, train loss: 0.5517824292182922:  48%|██████▏      | 70/146 [00:41<00:44,  1.73it/s]evaluate for the 19-th batch, evaluate loss: 0.6032366156578064:  72%|████████████▉     | 18/25 [00:02<00:00,  9.17it/s]evaluate for the 19-th batch, evaluate loss: 0.6032366156578064:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.19it/s]evaluate for the 20-th batch, evaluate loss: 0.6751726865768433:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.19it/s]evaluate for the 20-th batch, evaluate loss: 0.6751726865768433:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.20it/s]evaluate for the 6-th batch, evaluate loss: 0.2204051911830902:  12%|██▌                 | 5/40 [00:01<00:09,  3.61it/s]evaluate for the 6-th batch, evaluate loss: 0.2204051911830902:  15%|███                 | 6/40 [00:01<00:09,  3.55it/s]evaluate for the 21-th batch, evaluate loss: 0.7400426864624023:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.20it/s]evaluate for the 21-th batch, evaluate loss: 0.7400426864624023:  84%|███████████████   | 21/25 [00:02<00:00,  9.17it/s]Epoch: 2, train for the 140-th batch, train loss: 0.6304936408996582:  59%|██████▍    | 139/237 [01:22<00:57,  1.69it/s]Epoch: 2, train for the 140-th batch, train loss: 0.6304936408996582:  59%|██████▍    | 140/237 [01:22<00:57,  1.68it/s]evaluate for the 68-th batch, evaluate loss: 0.46547168493270874:  63%|██████████      | 67/106 [00:18<00:10,  3.77it/s]evaluate for the 68-th batch, evaluate loss: 0.46547168493270874:  64%|██████████▎     | 68/106 [00:18<00:10,  3.59it/s]evaluate for the 22-th batch, evaluate loss: 0.625677764415741:  84%|███████████████▉   | 21/25 [00:02<00:00,  9.17it/s]evaluate for the 22-th batch, evaluate loss: 0.625677764415741:  88%|████████████████▋  | 22/25 [00:02<00:00,  9.16it/s]evaluate for the 7-th batch, evaluate loss: 0.13212458789348602:  15%|██▊                | 6/40 [00:01<00:09,  3.55it/s]evaluate for the 7-th batch, evaluate loss: 0.13212458789348602:  18%|███▎               | 7/40 [00:01<00:09,  3.57it/s]evaluate for the 23-th batch, evaluate loss: 0.6825655698776245:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.16it/s]evaluate for the 23-th batch, evaluate loss: 0.6825655698776245:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.15it/s]evaluate for the 69-th batch, evaluate loss: 0.4208203852176666:  64%|██████████▉      | 68/106 [00:18<00:10,  3.59it/s]evaluate for the 69-th batch, evaluate loss: 0.4208203852176666:  65%|███████████      | 69/106 [00:18<00:10,  3.65it/s]evaluate for the 24-th batch, evaluate loss: 0.6752806901931763:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.15it/s]evaluate for the 24-th batch, evaluate loss: 0.6752806901931763:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.16it/s]Epoch: 3, train for the 71-th batch, train loss: 0.5296096205711365:  48%|██████▏      | 70/146 [00:42<00:44,  1.73it/s]Epoch: 3, train for the 71-th batch, train loss: 0.5296096205711365:  49%|██████▎      | 71/146 [00:42<00:43,  1.70it/s]evaluate for the 25-th batch, evaluate loss: 0.7204267382621765:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.16it/s]evaluate for the 25-th batch, evaluate loss: 0.7204267382621765: 100%|██████████████████| 25/25 [00:02<00:00,  9.25it/s]
INFO:root:Epoch: 5, learning rate: 0.0001, train loss: 0.5798
INFO:root:train average_precision, 0.8037
INFO:root:train roc_auc, 0.7671
INFO:root:validate loss: 0.5115
INFO:root:validate average_precision, 0.8396
INFO:root:validate roc_auc, 0.8005
INFO:root:new node validate loss: 0.6961
INFO:root:new node validate first_1_average_precision, 0.5742
INFO:root:new node validate first_1_roc_auc, 0.5329
INFO:root:new node validate first_3_average_precision, 0.6599
INFO:root:new node validate first_3_roc_auc, 0.6314
INFO:root:new node validate first_10_average_precision, 0.7343
INFO:root:new node validate first_10_roc_auc, 0.7058
INFO:root:new node validate average_precision, 0.6976
INFO:root:new node validate roc_auc, 0.6504
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]evaluate for the 8-th batch, evaluate loss: 0.17922934889793396:  18%|███▎               | 7/40 [00:02<00:09,  3.57it/s]evaluate for the 8-th batch, evaluate loss: 0.17922934889793396:  20%|███▊               | 8/40 [00:02<00:09,  3.49it/s]Epoch: 6, train for the 1-th batch, train loss: 0.8358616232872009:   0%|                       | 0/151 [00:00<?, ?it/s]Epoch: 6, train for the 1-th batch, train loss: 0.8358616232872009:   1%|               | 1/151 [00:00<00:26,  5.71it/s]evaluate for the 70-th batch, evaluate loss: 0.3962905704975128:  65%|███████████      | 69/106 [00:19<00:10,  3.65it/s]Epoch: 2, train for the 141-th batch, train loss: 0.6418558955192566:  59%|██████▍    | 140/237 [01:23<00:57,  1.68it/s]evaluate for the 70-th batch, evaluate loss: 0.3962905704975128:  66%|███████████▏     | 70/106 [00:19<00:10,  3.50it/s]Epoch: 2, train for the 141-th batch, train loss: 0.6418558955192566:  59%|██████▌    | 141/237 [01:23<00:57,  1.68it/s]evaluate for the 9-th batch, evaluate loss: 0.1822647601366043:  20%|████                | 8/40 [00:02<00:09,  3.49it/s]evaluate for the 9-th batch, evaluate loss: 0.1822647601366043:  22%|████▌               | 9/40 [00:02<00:08,  3.65it/s]Epoch: 6, train for the 2-th batch, train loss: 0.8157498240470886:   1%|               | 1/151 [00:00<00:26,  5.71it/s]Epoch: 6, train for the 2-th batch, train loss: 0.8157498240470886:   1%|▏              | 2/151 [00:00<00:26,  5.63it/s]evaluate for the 71-th batch, evaluate loss: 0.41135597229003906:  66%|██████████▌     | 70/106 [00:19<00:10,  3.50it/s]evaluate for the 71-th batch, evaluate loss: 0.41135597229003906:  67%|██████████▋     | 71/106 [00:19<00:09,  3.61it/s]Epoch: 6, train for the 3-th batch, train loss: 0.7035568952560425:   1%|▏              | 2/151 [00:00<00:26,  5.63it/s]Epoch: 6, train for the 3-th batch, train loss: 0.7035568952560425:   2%|▎              | 3/151 [00:00<00:25,  5.83it/s]Epoch: 3, train for the 72-th batch, train loss: 0.5295438170433044:  49%|██████▎      | 71/146 [00:42<00:43,  1.70it/s]Epoch: 3, train for the 72-th batch, train loss: 0.5295438170433044:  49%|██████▍      | 72/146 [00:42<00:43,  1.70it/s]evaluate for the 10-th batch, evaluate loss: 0.25586238503456116:  22%|████              | 9/40 [00:02<00:08,  3.65it/s]evaluate for the 10-th batch, evaluate loss: 0.25586238503456116:  25%|████▎            | 10/40 [00:02<00:08,  3.53it/s]Epoch: 6, train for the 4-th batch, train loss: 0.5839841365814209:   2%|▎              | 3/151 [00:00<00:25,  5.83it/s]Epoch: 6, train for the 4-th batch, train loss: 0.5839841365814209:   3%|▍              | 4/151 [00:00<00:25,  5.85it/s]evaluate for the 72-th batch, evaluate loss: 0.39813655614852905:  67%|██████████▋     | 71/106 [00:19<00:09,  3.61it/s]evaluate for the 72-th batch, evaluate loss: 0.39813655614852905:  68%|██████████▊     | 72/106 [00:19<00:09,  3.55it/s]Epoch: 2, train for the 142-th batch, train loss: 0.6397655606269836:  59%|██████▌    | 141/237 [01:23<00:57,  1.68it/s]Epoch: 2, train for the 142-th batch, train loss: 0.6397655606269836:  60%|██████▌    | 142/237 [01:23<00:56,  1.68it/s]evaluate for the 11-th batch, evaluate loss: 0.19197070598602295:  25%|████▎            | 10/40 [00:03<00:08,  3.53it/s]evaluate for the 11-th batch, evaluate loss: 0.19197070598602295:  28%|████▋            | 11/40 [00:03<00:07,  3.70it/s]Epoch: 6, train for the 5-th batch, train loss: 0.5316376090049744:   3%|▍              | 4/151 [00:00<00:25,  5.85it/s]Epoch: 6, train for the 5-th batch, train loss: 0.5316376090049744:   3%|▍              | 5/151 [00:00<00:25,  5.66it/s]evaluate for the 73-th batch, evaluate loss: 0.4339447319507599:  68%|███████████▌     | 72/106 [00:19<00:09,  3.55it/s]evaluate for the 73-th batch, evaluate loss: 0.4339447319507599:  69%|███████████▋     | 73/106 [00:19<00:09,  3.66it/s]Epoch: 6, train for the 6-th batch, train loss: 0.5269514918327332:   3%|▍              | 5/151 [00:01<00:25,  5.66it/s]Epoch: 6, train for the 6-th batch, train loss: 0.5269514918327332:   4%|▌              | 6/151 [00:01<00:26,  5.53it/s]Epoch: 3, train for the 73-th batch, train loss: 0.5226049423217773:  49%|██████▍      | 72/146 [00:43<00:43,  1.70it/s]Epoch: 3, train for the 73-th batch, train loss: 0.5226049423217773:  50%|██████▌      | 73/146 [00:43<00:43,  1.68it/s]evaluate for the 12-th batch, evaluate loss: 0.182830348610878:  28%|█████▏             | 11/40 [00:03<00:07,  3.70it/s]evaluate for the 12-th batch, evaluate loss: 0.182830348610878:  30%|█████▋             | 12/40 [00:03<00:07,  3.52it/s]Epoch: 6, train for the 7-th batch, train loss: 0.4207504689693451:   4%|▌              | 6/151 [00:01<00:26,  5.53it/s]Epoch: 6, train for the 7-th batch, train loss: 0.4207504689693451:   5%|▋              | 7/151 [00:01<00:26,  5.40it/s]evaluate for the 74-th batch, evaluate loss: 0.5007648468017578:  69%|███████████▋     | 73/106 [00:20<00:09,  3.66it/s]evaluate for the 74-th batch, evaluate loss: 0.5007648468017578:  70%|███████████▊     | 74/106 [00:20<00:08,  3.61it/s]Epoch: 2, train for the 143-th batch, train loss: 0.6216050386428833:  60%|██████▌    | 142/237 [01:24<00:56,  1.68it/s]Epoch: 2, train for the 143-th batch, train loss: 0.6216050386428833:  60%|██████▋    | 143/237 [01:24<00:55,  1.68it/s]evaluate for the 13-th batch, evaluate loss: 0.14671370387077332:  30%|█████            | 12/40 [00:03<00:07,  3.52it/s]evaluate for the 13-th batch, evaluate loss: 0.14671370387077332:  32%|█████▌           | 13/40 [00:03<00:07,  3.60it/s]Epoch: 6, train for the 8-th batch, train loss: 0.5674858093261719:   5%|▋              | 7/151 [00:01<00:26,  5.40it/s]Epoch: 6, train for the 8-th batch, train loss: 0.5674858093261719:   5%|▊              | 8/151 [00:01<00:26,  5.33it/s]evaluate for the 75-th batch, evaluate loss: 0.42487913370132446:  70%|███████████▏    | 74/106 [00:20<00:08,  3.61it/s]evaluate for the 75-th batch, evaluate loss: 0.42487913370132446:  71%|███████████▎    | 75/106 [00:20<00:08,  3.68it/s]Epoch: 6, train for the 9-th batch, train loss: 0.6418246030807495:   5%|▊              | 8/151 [00:01<00:26,  5.33it/s]Epoch: 6, train for the 9-th batch, train loss: 0.6418246030807495:   6%|▉              | 9/151 [00:01<00:27,  5.25it/s]Epoch: 3, train for the 74-th batch, train loss: 0.5406834483146667:  50%|██████▌      | 73/146 [00:44<00:43,  1.68it/s]Epoch: 3, train for the 74-th batch, train loss: 0.5406834483146667:  51%|██████▌      | 74/146 [00:44<00:43,  1.67it/s]evaluate for the 14-th batch, evaluate loss: 0.22977924346923828:  32%|█████▌           | 13/40 [00:03<00:07,  3.60it/s]evaluate for the 14-th batch, evaluate loss: 0.22977924346923828:  35%|█████▉           | 14/40 [00:03<00:07,  3.44it/s]evaluate for the 76-th batch, evaluate loss: 0.47906866669654846:  71%|███████████▎    | 75/106 [00:20<00:08,  3.68it/s]evaluate for the 76-th batch, evaluate loss: 0.47906866669654846:  72%|███████████▍    | 76/106 [00:20<00:08,  3.63it/s]Epoch: 6, train for the 10-th batch, train loss: 0.6221417784690857:   6%|▊             | 9/151 [00:01<00:27,  5.25it/s]Epoch: 6, train for the 10-th batch, train loss: 0.6221417784690857:   7%|▊            | 10/151 [00:01<00:27,  5.14it/s]Epoch: 2, train for the 144-th batch, train loss: 0.6287863850593567:  60%|██████▋    | 143/237 [01:24<00:55,  1.68it/s]Epoch: 2, train for the 144-th batch, train loss: 0.6287863850593567:  61%|██████▋    | 144/237 [01:24<00:55,  1.68it/s]evaluate for the 15-th batch, evaluate loss: 0.22190377116203308:  35%|█████▉           | 14/40 [00:04<00:07,  3.44it/s]evaluate for the 15-th batch, evaluate loss: 0.22190377116203308:  38%|██████▍          | 15/40 [00:04<00:07,  3.50it/s]Epoch: 6, train for the 11-th batch, train loss: 0.4757097661495209:   7%|▊            | 10/151 [00:02<00:27,  5.14it/s]Epoch: 6, train for the 11-th batch, train loss: 0.4757097661495209:   7%|▉            | 11/151 [00:02<00:27,  5.11it/s]evaluate for the 77-th batch, evaluate loss: 0.4288037419319153:  72%|████████████▏    | 76/106 [00:21<00:08,  3.63it/s]evaluate for the 77-th batch, evaluate loss: 0.4288037419319153:  73%|████████████▎    | 77/106 [00:21<00:08,  3.57it/s]Epoch: 6, train for the 12-th batch, train loss: 0.4877612888813019:   7%|▉            | 11/151 [00:02<00:27,  5.11it/s]Epoch: 6, train for the 12-th batch, train loss: 0.4877612888813019:   8%|█            | 12/151 [00:02<00:27,  5.07it/s]evaluate for the 16-th batch, evaluate loss: 0.24939411878585815:  38%|██████▍          | 15/40 [00:04<00:07,  3.50it/s]evaluate for the 16-th batch, evaluate loss: 0.24939411878585815:  40%|██████▊          | 16/40 [00:04<00:06,  3.46it/s]Epoch: 3, train for the 75-th batch, train loss: 0.5538648962974548:  51%|██████▌      | 74/146 [00:44<00:43,  1.67it/s]Epoch: 3, train for the 75-th batch, train loss: 0.5538648962974548:  51%|██████▋      | 75/146 [00:44<00:42,  1.68it/s]evaluate for the 78-th batch, evaluate loss: 0.5023861527442932:  73%|████████████▎    | 77/106 [00:21<00:08,  3.57it/s]evaluate for the 78-th batch, evaluate loss: 0.5023861527442932:  74%|████████████▌    | 78/106 [00:21<00:07,  3.60it/s]Epoch: 6, train for the 13-th batch, train loss: 0.5155210494995117:   8%|█            | 12/151 [00:02<00:27,  5.07it/s]Epoch: 6, train for the 13-th batch, train loss: 0.5155210494995117:   9%|█            | 13/151 [00:02<00:27,  5.02it/s]Epoch: 2, train for the 145-th batch, train loss: 0.6630436778068542:  61%|██████▋    | 144/237 [01:25<00:55,  1.68it/s]Epoch: 2, train for the 145-th batch, train loss: 0.6630436778068542:  61%|██████▋    | 145/237 [01:25<00:54,  1.68it/s]evaluate for the 17-th batch, evaluate loss: 0.15586696565151215:  40%|██████▊          | 16/40 [00:04<00:06,  3.46it/s]evaluate for the 17-th batch, evaluate loss: 0.15586696565151215:  42%|███████▏         | 17/40 [00:04<00:06,  3.59it/s]Epoch: 6, train for the 14-th batch, train loss: 0.6936520338058472:   9%|█            | 13/151 [00:02<00:27,  5.02it/s]Epoch: 6, train for the 14-th batch, train loss: 0.6936520338058472:   9%|█▏           | 14/151 [00:02<00:27,  4.93it/s]evaluate for the 79-th batch, evaluate loss: 0.49468889832496643:  74%|███████████▊    | 78/106 [00:21<00:07,  3.60it/s]evaluate for the 79-th batch, evaluate loss: 0.49468889832496643:  75%|███████████▉    | 79/106 [00:21<00:07,  3.54it/s]evaluate for the 18-th batch, evaluate loss: 0.17681416869163513:  42%|███████▏         | 17/40 [00:05<00:06,  3.59it/s]evaluate for the 18-th batch, evaluate loss: 0.17681416869163513:  45%|███████▋         | 18/40 [00:05<00:06,  3.53it/s]Epoch: 6, train for the 15-th batch, train loss: 0.3901541829109192:   9%|█▏           | 14/151 [00:02<00:27,  4.93it/s]Epoch: 6, train for the 15-th batch, train loss: 0.3901541829109192:  10%|█▎           | 15/151 [00:02<00:27,  4.90it/s]evaluate for the 80-th batch, evaluate loss: 0.49215757846832275:  75%|███████████▉    | 79/106 [00:21<00:07,  3.54it/s]evaluate for the 80-th batch, evaluate loss: 0.49215757846832275:  75%|████████████    | 80/106 [00:21<00:07,  3.67it/s]Epoch: 3, train for the 76-th batch, train loss: 0.5184667110443115:  51%|██████▋      | 75/146 [00:45<00:42,  1.68it/s]Epoch: 3, train for the 76-th batch, train loss: 0.5184667110443115:  52%|██████▊      | 76/146 [00:45<00:41,  1.68it/s]Epoch: 6, train for the 16-th batch, train loss: 0.4467989206314087:  10%|█▎           | 15/151 [00:03<00:27,  4.90it/s]Epoch: 6, train for the 16-th batch, train loss: 0.4467989206314087:  11%|█▍           | 16/151 [00:03<00:27,  4.90it/s]evaluate for the 19-th batch, evaluate loss: 0.2180279940366745:  45%|████████          | 18/40 [00:05<00:06,  3.53it/s]evaluate for the 19-th batch, evaluate loss: 0.2180279940366745:  48%|████████▌         | 19/40 [00:05<00:05,  3.63it/s]Epoch: 2, train for the 146-th batch, train loss: 0.6120921969413757:  61%|██████▋    | 145/237 [01:26<00:54,  1.68it/s]Epoch: 2, train for the 146-th batch, train loss: 0.6120921969413757:  62%|██████▊    | 146/237 [01:26<00:53,  1.69it/s]evaluate for the 81-th batch, evaluate loss: 0.48555946350097656:  75%|████████████    | 80/106 [00:22<00:07,  3.67it/s]evaluate for the 81-th batch, evaluate loss: 0.48555946350097656:  76%|████████████▏   | 81/106 [00:22<00:07,  3.57it/s]Epoch: 6, train for the 17-th batch, train loss: 0.6881301999092102:  11%|█▍           | 16/151 [00:03<00:27,  4.90it/s]Epoch: 6, train for the 17-th batch, train loss: 0.6881301999092102:  11%|█▍           | 17/151 [00:03<00:27,  4.84it/s]evaluate for the 20-th batch, evaluate loss: 0.19726011157035828:  48%|████████         | 19/40 [00:05<00:05,  3.63it/s]evaluate for the 20-th batch, evaluate loss: 0.19726011157035828:  50%|████████▌        | 20/40 [00:05<00:05,  3.58it/s]evaluate for the 82-th batch, evaluate loss: 0.4497329592704773:  76%|████████████▉    | 81/106 [00:22<00:07,  3.57it/s]evaluate for the 82-th batch, evaluate loss: 0.4497329592704773:  77%|█████████████▏   | 82/106 [00:22<00:06,  3.73it/s]Epoch: 6, train for the 18-th batch, train loss: 0.5386488437652588:  11%|█▍           | 17/151 [00:03<00:27,  4.84it/s]Epoch: 6, train for the 18-th batch, train loss: 0.5386488437652588:  12%|█▌           | 18/151 [00:03<00:27,  4.85it/s]Epoch: 3, train for the 77-th batch, train loss: 0.5621175765991211:  52%|██████▊      | 76/146 [00:45<00:41,  1.68it/s]Epoch: 3, train for the 77-th batch, train loss: 0.5621175765991211:  53%|██████▊      | 77/146 [00:45<00:41,  1.67it/s]evaluate for the 21-th batch, evaluate loss: 0.14918681979179382:  50%|████████▌        | 20/40 [00:05<00:05,  3.58it/s]evaluate for the 21-th batch, evaluate loss: 0.14918681979179382:  52%|████████▉        | 21/40 [00:05<00:05,  3.60it/s]Epoch: 6, train for the 19-th batch, train loss: 0.6285412311553955:  12%|█▌           | 18/151 [00:03<00:27,  4.85it/s]Epoch: 6, train for the 19-th batch, train loss: 0.6285412311553955:  13%|█▋           | 19/151 [00:03<00:27,  4.79it/s]Epoch: 2, train for the 147-th batch, train loss: 0.6657489538192749:  62%|██████▊    | 146/237 [01:26<00:53,  1.69it/s]Epoch: 2, train for the 147-th batch, train loss: 0.6657489538192749:  62%|██████▊    | 147/237 [01:26<00:53,  1.68it/s]evaluate for the 83-th batch, evaluate loss: 0.42698150873184204:  77%|████████████▍   | 82/106 [00:22<00:06,  3.73it/s]evaluate for the 83-th batch, evaluate loss: 0.42698150873184204:  78%|████████████▌   | 83/106 [00:22<00:06,  3.56it/s]Epoch: 6, train for the 20-th batch, train loss: 0.46063750982284546:  13%|█▌          | 19/151 [00:03<00:27,  4.79it/s]Epoch: 6, train for the 20-th batch, train loss: 0.46063750982284546:  13%|█▌          | 20/151 [00:03<00:27,  4.78it/s]evaluate for the 22-th batch, evaluate loss: 0.1844531148672104:  52%|█████████▍        | 21/40 [00:06<00:05,  3.60it/s]evaluate for the 22-th batch, evaluate loss: 0.1844531148672104:  55%|█████████▉        | 22/40 [00:06<00:04,  3.61it/s]evaluate for the 84-th batch, evaluate loss: 0.5060462951660156:  78%|█████████████▎   | 83/106 [00:22<00:06,  3.56it/s]evaluate for the 84-th batch, evaluate loss: 0.5060462951660156:  79%|█████████████▍   | 84/106 [00:22<00:06,  3.62it/s]Epoch: 6, train for the 21-th batch, train loss: 0.5907551646232605:  13%|█▋           | 20/151 [00:04<00:27,  4.78it/s]Epoch: 6, train for the 21-th batch, train loss: 0.5907551646232605:  14%|█▊           | 21/151 [00:04<00:27,  4.72it/s]Epoch: 3, train for the 78-th batch, train loss: 0.540803074836731:  53%|███████▍      | 77/146 [00:46<00:41,  1.67it/s]Epoch: 3, train for the 78-th batch, train loss: 0.540803074836731:  53%|███████▍      | 78/146 [00:46<00:40,  1.66it/s]evaluate for the 23-th batch, evaluate loss: 0.1887264996767044:  55%|█████████▉        | 22/40 [00:06<00:04,  3.61it/s]evaluate for the 23-th batch, evaluate loss: 0.1887264996767044:  57%|██████████▎       | 23/40 [00:06<00:04,  3.52it/s]evaluate for the 85-th batch, evaluate loss: 0.5095576047897339:  79%|█████████████▍   | 84/106 [00:23<00:06,  3.62it/s]evaluate for the 85-th batch, evaluate loss: 0.5095576047897339:  80%|█████████████▋   | 85/106 [00:23<00:06,  3.49it/s]Epoch: 2, train for the 148-th batch, train loss: 0.6124087572097778:  62%|██████▊    | 147/237 [01:27<00:53,  1.68it/s]Epoch: 2, train for the 148-th batch, train loss: 0.6124087572097778:  62%|██████▊    | 148/237 [01:27<00:53,  1.68it/s]Epoch: 6, train for the 22-th batch, train loss: 0.5507595539093018:  14%|█▊           | 21/151 [00:04<00:27,  4.72it/s]Epoch: 6, train for the 22-th batch, train loss: 0.5507595539093018:  15%|█▉           | 22/151 [00:04<00:27,  4.69it/s]evaluate for the 24-th batch, evaluate loss: 0.1990227997303009:  57%|██████████▎       | 23/40 [00:06<00:04,  3.52it/s]evaluate for the 24-th batch, evaluate loss: 0.1990227997303009:  60%|██████████▊       | 24/40 [00:06<00:04,  3.67it/s]Epoch: 6, train for the 23-th batch, train loss: 0.4599321782588959:  15%|█▉           | 22/151 [00:04<00:27,  4.69it/s]Epoch: 6, train for the 23-th batch, train loss: 0.4599321782588959:  15%|█▉           | 23/151 [00:04<00:27,  4.70it/s]evaluate for the 86-th batch, evaluate loss: 0.4628792405128479:  80%|█████████████▋   | 85/106 [00:23<00:06,  3.49it/s]evaluate for the 86-th batch, evaluate loss: 0.4628792405128479:  81%|█████████████▊   | 86/106 [00:23<00:05,  3.60it/s]Epoch: 3, train for the 79-th batch, train loss: 0.539194643497467:  53%|███████▍      | 78/146 [00:47<00:40,  1.66it/s]Epoch: 3, train for the 79-th batch, train loss: 0.539194643497467:  54%|███████▌      | 79/146 [00:47<00:40,  1.67it/s]Epoch: 6, train for the 24-th batch, train loss: 0.3668399453163147:  15%|█▉           | 23/151 [00:04<00:27,  4.70it/s]Epoch: 6, train for the 24-th batch, train loss: 0.3668399453163147:  16%|██           | 24/151 [00:04<00:26,  4.72it/s]evaluate for the 25-th batch, evaluate loss: 0.1993836760520935:  60%|██████████▊       | 24/40 [00:07<00:04,  3.67it/s]evaluate for the 25-th batch, evaluate loss: 0.1993836760520935:  62%|███████████▎      | 25/40 [00:07<00:04,  3.54it/s]evaluate for the 87-th batch, evaluate loss: 0.4990804195404053:  81%|█████████████▊   | 86/106 [00:23<00:05,  3.60it/s]evaluate for the 87-th batch, evaluate loss: 0.4990804195404053:  82%|█████████████▉   | 87/106 [00:23<00:05,  3.54it/s]Epoch: 2, train for the 149-th batch, train loss: 0.6249890327453613:  62%|██████▊    | 148/237 [01:27<00:53,  1.68it/s]Epoch: 2, train for the 149-th batch, train loss: 0.6249890327453613:  63%|██████▉    | 149/237 [01:27<00:52,  1.68it/s]Epoch: 6, train for the 25-th batch, train loss: 0.4597524106502533:  16%|██           | 24/151 [00:04<00:26,  4.72it/s]Epoch: 6, train for the 25-th batch, train loss: 0.4597524106502533:  17%|██▏          | 25/151 [00:04<00:26,  4.75it/s]evaluate for the 26-th batch, evaluate loss: 0.16212232410907745:  62%|██████████▋      | 25/40 [00:07<00:04,  3.54it/s]evaluate for the 26-th batch, evaluate loss: 0.16212232410907745:  65%|███████████      | 26/40 [00:07<00:03,  3.72it/s]evaluate for the 88-th batch, evaluate loss: 0.5364714860916138:  82%|█████████████▉   | 87/106 [00:24<00:05,  3.54it/s]evaluate for the 88-th batch, evaluate loss: 0.5364714860916138:  83%|██████████████   | 88/106 [00:24<00:04,  3.66it/s]Epoch: 6, train for the 26-th batch, train loss: 0.8702365756034851:  17%|██▏          | 25/151 [00:05<00:26,  4.75it/s]Epoch: 6, train for the 26-th batch, train loss: 0.8702365756034851:  17%|██▏          | 26/151 [00:05<00:26,  4.68it/s]Epoch: 3, train for the 80-th batch, train loss: 0.5314502120018005:  54%|███████      | 79/146 [00:47<00:40,  1.67it/s]Epoch: 3, train for the 80-th batch, train loss: 0.5314502120018005:  55%|███████      | 80/146 [00:47<00:39,  1.66it/s]evaluate for the 27-th batch, evaluate loss: 0.166213259100914:  65%|████████████▎      | 26/40 [00:07<00:03,  3.72it/s]evaluate for the 27-th batch, evaluate loss: 0.166213259100914:  68%|████████████▊      | 27/40 [00:07<00:03,  3.54it/s]Epoch: 6, train for the 27-th batch, train loss: 0.7800459861755371:  17%|██▏          | 26/151 [00:05<00:26,  4.68it/s]Epoch: 6, train for the 27-th batch, train loss: 0.7800459861755371:  18%|██▎          | 27/151 [00:05<00:26,  4.63it/s]evaluate for the 89-th batch, evaluate loss: 0.4592898488044739:  83%|██████████████   | 88/106 [00:24<00:04,  3.66it/s]evaluate for the 89-th batch, evaluate loss: 0.4592898488044739:  84%|██████████████▎  | 89/106 [00:24<00:04,  3.60it/s]Epoch: 2, train for the 150-th batch, train loss: 0.5963846445083618:  63%|██████▉    | 149/237 [01:28<00:52,  1.68it/s]Epoch: 2, train for the 150-th batch, train loss: 0.5963846445083618:  63%|██████▉    | 150/237 [01:28<00:51,  1.68it/s]evaluate for the 28-th batch, evaluate loss: 0.1653180867433548:  68%|████████████▏     | 27/40 [00:07<00:03,  3.54it/s]evaluate for the 28-th batch, evaluate loss: 0.1653180867433548:  70%|████████████▌     | 28/40 [00:07<00:03,  3.60it/s]Epoch: 6, train for the 28-th batch, train loss: 0.6519042253494263:  18%|██▎          | 27/151 [00:05<00:26,  4.63it/s]Epoch: 6, train for the 28-th batch, train loss: 0.6519042253494263:  19%|██▍          | 28/151 [00:05<00:26,  4.59it/s]evaluate for the 90-th batch, evaluate loss: 0.3997158706188202:  84%|██████████████▎  | 89/106 [00:24<00:04,  3.60it/s]evaluate for the 90-th batch, evaluate loss: 0.3997158706188202:  85%|██████████████▍  | 90/106 [00:24<00:04,  3.68it/s]Epoch: 6, train for the 29-th batch, train loss: 0.7384265065193176:  19%|██▍          | 28/151 [00:05<00:26,  4.59it/s]Epoch: 6, train for the 29-th batch, train loss: 0.7384265065193176:  19%|██▍          | 29/151 [00:05<00:26,  4.57it/s]evaluate for the 29-th batch, evaluate loss: 0.17871949076652527:  70%|███████████▉     | 28/40 [00:08<00:03,  3.60it/s]evaluate for the 29-th batch, evaluate loss: 0.17871949076652527:  72%|████████████▎    | 29/40 [00:08<00:03,  3.51it/s]Epoch: 3, train for the 81-th batch, train loss: 0.5222936272621155:  55%|███████      | 80/146 [00:48<00:39,  1.66it/s]Epoch: 3, train for the 81-th batch, train loss: 0.5222936272621155:  55%|███████▏     | 81/146 [00:48<00:39,  1.66it/s]evaluate for the 91-th batch, evaluate loss: 0.46817198395729065:  85%|█████████████▌  | 90/106 [00:24<00:04,  3.68it/s]evaluate for the 91-th batch, evaluate loss: 0.46817198395729065:  86%|█████████████▋  | 91/106 [00:24<00:04,  3.63it/s]Epoch: 2, train for the 151-th batch, train loss: 0.6091448068618774:  63%|██████▉    | 150/237 [01:29<00:51,  1.68it/s]Epoch: 2, train for the 151-th batch, train loss: 0.6091448068618774:  64%|███████    | 151/237 [01:29<00:51,  1.68it/s]Epoch: 6, train for the 30-th batch, train loss: 0.7596221566200256:  19%|██▍          | 29/151 [00:06<00:26,  4.57it/s]Epoch: 6, train for the 30-th batch, train loss: 0.7596221566200256:  20%|██▌          | 30/151 [00:06<00:29,  4.14it/s]evaluate for the 30-th batch, evaluate loss: 0.20274505019187927:  72%|████████████▎    | 29/40 [00:08<00:03,  3.51it/s]evaluate for the 30-th batch, evaluate loss: 0.20274505019187927:  75%|████████████▊    | 30/40 [00:08<00:02,  3.63it/s]evaluate for the 92-th batch, evaluate loss: 0.4749058485031128:  86%|██████████████▌  | 91/106 [00:25<00:04,  3.63it/s]evaluate for the 92-th batch, evaluate loss: 0.4749058485031128:  87%|██████████████▊  | 92/106 [00:25<00:03,  3.57it/s]Epoch: 6, train for the 31-th batch, train loss: 0.6973440647125244:  20%|██▌          | 30/151 [00:06<00:29,  4.14it/s]Epoch: 6, train for the 31-th batch, train loss: 0.6973440647125244:  21%|██▋          | 31/151 [00:06<00:27,  4.31it/s]evaluate for the 31-th batch, evaluate loss: 0.21176448464393616:  75%|████████████▊    | 30/40 [00:08<00:02,  3.63it/s]evaluate for the 31-th batch, evaluate loss: 0.21176448464393616:  78%|█████████████▏   | 31/40 [00:08<00:02,  3.56it/s]evaluate for the 93-th batch, evaluate loss: 0.416035532951355:  87%|███████████████▌  | 92/106 [00:25<00:03,  3.57it/s]evaluate for the 93-th batch, evaluate loss: 0.416035532951355:  88%|███████████████▊  | 93/106 [00:25<00:03,  3.60it/s]Epoch: 3, train for the 82-th batch, train loss: 0.5105558633804321:  55%|███████▏     | 81/146 [00:48<00:39,  1.66it/s]Epoch: 3, train for the 82-th batch, train loss: 0.5105558633804321:  56%|███████▎     | 82/146 [00:48<00:38,  1.66it/s]Epoch: 6, train for the 32-th batch, train loss: 0.7074314951896667:  21%|██▋          | 31/151 [00:06<00:27,  4.31it/s]Epoch: 6, train for the 32-th batch, train loss: 0.7074314951896667:  21%|██▊          | 32/151 [00:06<00:27,  4.37it/s]Epoch: 2, train for the 152-th batch, train loss: 0.6428694725036621:  64%|███████    | 151/237 [01:29<00:51,  1.68it/s]Epoch: 2, train for the 152-th batch, train loss: 0.6428694725036621:  64%|███████    | 152/237 [01:29<00:50,  1.68it/s]evaluate for the 32-th batch, evaluate loss: 0.20352914929389954:  78%|█████████████▏   | 31/40 [00:08<00:02,  3.56it/s]evaluate for the 32-th batch, evaluate loss: 0.20352914929389954:  80%|█████████████▌   | 32/40 [00:08<00:02,  3.65it/s]Epoch: 6, train for the 33-th batch, train loss: 0.6074790954589844:  21%|██▊          | 32/151 [00:06<00:27,  4.37it/s]Epoch: 6, train for the 33-th batch, train loss: 0.6074790954589844:  22%|██▊          | 33/151 [00:06<00:26,  4.46it/s]evaluate for the 94-th batch, evaluate loss: 0.45581260323524475:  88%|██████████████  | 93/106 [00:25<00:03,  3.60it/s]evaluate for the 94-th batch, evaluate loss: 0.45581260323524475:  89%|██████████████▏ | 94/106 [00:25<00:03,  3.53it/s]Epoch: 6, train for the 34-th batch, train loss: 0.4922252297401428:  22%|██▊          | 33/151 [00:07<00:26,  4.46it/s]Epoch: 6, train for the 34-th batch, train loss: 0.4922252297401428:  23%|██▉          | 34/151 [00:07<00:25,  4.53it/s]evaluate for the 33-th batch, evaluate loss: 0.1697569638490677:  80%|██████████████▍   | 32/40 [00:09<00:02,  3.65it/s]evaluate for the 33-th batch, evaluate loss: 0.1697569638490677:  82%|██████████████▊   | 33/40 [00:09<00:01,  3.61it/s]evaluate for the 95-th batch, evaluate loss: 0.38722744584083557:  89%|██████████████▏ | 94/106 [00:26<00:03,  3.53it/s]evaluate for the 95-th batch, evaluate loss: 0.38722744584083557:  90%|██████████████▎ | 95/106 [00:26<00:02,  3.68it/s]Epoch: 3, train for the 83-th batch, train loss: 0.5256810784339905:  56%|███████▎     | 82/146 [00:49<00:38,  1.66it/s]Epoch: 3, train for the 83-th batch, train loss: 0.5256810784339905:  57%|███████▍     | 83/146 [00:49<00:38,  1.65it/s]Epoch: 6, train for the 35-th batch, train loss: 0.5419597625732422:  23%|██▉          | 34/151 [00:07<00:25,  4.53it/s]Epoch: 6, train for the 35-th batch, train loss: 0.5419597625732422:  23%|███          | 35/151 [00:07<00:25,  4.58it/s]evaluate for the 34-th batch, evaluate loss: 0.16920247673988342:  82%|██████████████   | 33/40 [00:09<00:01,  3.61it/s]evaluate for the 34-th batch, evaluate loss: 0.16920247673988342:  85%|██████████████▍  | 34/40 [00:09<00:01,  3.59it/s]Epoch: 2, train for the 153-th batch, train loss: 0.632254958152771:  64%|███████▋    | 152/237 [01:30<00:50,  1.68it/s]Epoch: 2, train for the 153-th batch, train loss: 0.632254958152771:  65%|███████▋    | 153/237 [01:30<00:49,  1.69it/s]evaluate for the 96-th batch, evaluate loss: 0.42079412937164307:  90%|██████████████▎ | 95/106 [00:26<00:02,  3.68it/s]evaluate for the 96-th batch, evaluate loss: 0.42079412937164307:  91%|██████████████▍ | 96/106 [00:26<00:02,  3.58it/s]Epoch: 6, train for the 36-th batch, train loss: 0.6997895836830139:  23%|███          | 35/151 [00:07<00:25,  4.58it/s]Epoch: 6, train for the 36-th batch, train loss: 0.6997895836830139:  24%|███          | 36/151 [00:07<00:25,  4.57it/s]evaluate for the 35-th batch, evaluate loss: 0.19126182794570923:  85%|██████████████▍  | 34/40 [00:09<00:01,  3.59it/s]evaluate for the 35-th batch, evaluate loss: 0.19126182794570923:  88%|██████████████▉  | 35/40 [00:09<00:01,  4.16it/s]evaluate for the 97-th batch, evaluate loss: 0.43725526332855225:  91%|██████████████▍ | 96/106 [00:26<00:02,  3.58it/s]evaluate for the 97-th batch, evaluate loss: 0.43725526332855225:  92%|██████████████▋ | 97/106 [00:26<00:02,  3.75it/s]Epoch: 6, train for the 37-th batch, train loss: 0.5797380208969116:  24%|███          | 36/151 [00:07<00:25,  4.57it/s]Epoch: 6, train for the 37-th batch, train loss: 0.5797380208969116:  25%|███▏         | 37/151 [00:07<00:24,  4.61it/s]evaluate for the 36-th batch, evaluate loss: 0.19551238417625427:  88%|██████████████▉  | 35/40 [00:09<00:01,  4.16it/s]evaluate for the 36-th batch, evaluate loss: 0.19551238417625427:  90%|███████████████▎ | 36/40 [00:09<00:01,  3.95it/s]Epoch: 3, train for the 84-th batch, train loss: 0.4884604811668396:  57%|███████▍     | 83/146 [00:50<00:38,  1.65it/s]Epoch: 3, train for the 84-th batch, train loss: 0.4884604811668396:  58%|███████▍     | 84/146 [00:50<00:39,  1.58it/s]Epoch: 6, train for the 38-th batch, train loss: 0.7446286082267761:  25%|███▏         | 37/151 [00:07<00:24,  4.61it/s]Epoch: 6, train for the 38-th batch, train loss: 0.7446286082267761:  25%|███▎         | 38/151 [00:07<00:24,  4.58it/s]Epoch: 2, train for the 154-th batch, train loss: 0.6228090524673462:  65%|███████    | 153/237 [01:30<00:49,  1.69it/s]Epoch: 2, train for the 154-th batch, train loss: 0.6228090524673462:  65%|███████▏   | 154/237 [01:30<00:49,  1.68it/s]evaluate for the 98-th batch, evaluate loss: 0.47046977281570435:  92%|██████████████▋ | 97/106 [00:26<00:02,  3.75it/s]evaluate for the 98-th batch, evaluate loss: 0.47046977281570435:  92%|██████████████▊ | 98/106 [00:26<00:02,  3.58it/s]evaluate for the 37-th batch, evaluate loss: 0.230093315243721:  90%|█████████████████  | 36/40 [00:10<00:01,  3.95it/s]evaluate for the 37-th batch, evaluate loss: 0.230093315243721:  92%|█████████████████▌ | 37/40 [00:10<00:00,  3.87it/s]Epoch: 6, train for the 39-th batch, train loss: 0.7175008058547974:  25%|███▎         | 38/151 [00:08<00:24,  4.58it/s]Epoch: 6, train for the 39-th batch, train loss: 0.7175008058547974:  26%|███▎         | 39/151 [00:08<00:24,  4.56it/s]evaluate for the 99-th batch, evaluate loss: 0.4142993688583374:  92%|███████████████▋ | 98/106 [00:27<00:02,  3.58it/s]evaluate for the 99-th batch, evaluate loss: 0.4142993688583374:  93%|███████████████▉ | 99/106 [00:27<00:01,  3.65it/s]evaluate for the 38-th batch, evaluate loss: 0.18785914778709412:  92%|███████████████▋ | 37/40 [00:10<00:00,  3.87it/s]evaluate for the 38-th batch, evaluate loss: 0.18785914778709412:  95%|████████████████▏| 38/40 [00:10<00:00,  3.73it/s]Epoch: 6, train for the 40-th batch, train loss: 0.5774906277656555:  26%|███▎         | 39/151 [00:08<00:24,  4.56it/s]Epoch: 6, train for the 40-th batch, train loss: 0.5774906277656555:  26%|███▍         | 40/151 [00:08<00:24,  4.59it/s]Epoch: 3, train for the 85-th batch, train loss: 0.5284631252288818:  58%|███████▍     | 84/146 [00:50<00:39,  1.58it/s]Epoch: 3, train for the 85-th batch, train loss: 0.5284631252288818:  58%|███████▌     | 85/146 [00:50<00:38,  1.60it/s]evaluate for the 100-th batch, evaluate loss: 0.39796364307403564:  93%|██████████████ | 99/106 [00:27<00:01,  3.65it/s]evaluate for the 100-th batch, evaluate loss: 0.39796364307403564:  94%|█████████████▏| 100/106 [00:27<00:01,  3.58it/s]Epoch: 2, train for the 155-th batch, train loss: 0.6134730577468872:  65%|███████▏   | 154/237 [01:31<00:49,  1.68it/s]Epoch: 2, train for the 155-th batch, train loss: 0.6134730577468872:  65%|███████▏   | 155/237 [01:31<00:48,  1.68it/s]Epoch: 6, train for the 41-th batch, train loss: 0.6566233038902283:  26%|███▍         | 40/151 [00:08<00:24,  4.59it/s]Epoch: 6, train for the 41-th batch, train loss: 0.6566233038902283:  27%|███▌         | 41/151 [00:08<00:24,  4.58it/s]evaluate for the 39-th batch, evaluate loss: 0.21134744584560394:  95%|████████████████▏| 38/40 [00:10<00:00,  3.73it/s]evaluate for the 39-th batch, evaluate loss: 0.21134744584560394:  98%|████████████████▌| 39/40 [00:10<00:00,  3.60it/s]evaluate for the 40-th batch, evaluate loss: 0.05238942801952362:  98%|████████████████▌| 39/40 [00:10<00:00,  3.60it/s]evaluate for the 40-th batch, evaluate loss: 0.05238942801952362: 100%|█████████████████| 40/40 [00:10<00:00,  3.70it/s]
  0%|                                                                                            | 0/21 [00:00<?, ?it/s]evaluate for the 101-th batch, evaluate loss: 0.4781001806259155:  94%|██████████████▏| 100/106 [00:27<00:01,  3.58it/s]evaluate for the 101-th batch, evaluate loss: 0.4781001806259155:  95%|██████████████▎| 101/106 [00:27<00:01,  3.69it/s]Epoch: 6, train for the 42-th batch, train loss: 0.6274572014808655:  27%|███▌         | 41/151 [00:08<00:24,  4.58it/s]Epoch: 6, train for the 42-th batch, train loss: 0.6274572014808655:  28%|███▌         | 42/151 [00:08<00:23,  4.56it/s]evaluate for the 1-th batch, evaluate loss: 0.251865029335022:   0%|                             | 0/21 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.251865029335022:   5%|█                    | 1/21 [00:00<00:05,  3.58it/s]Epoch: 6, train for the 43-th batch, train loss: 0.6302452087402344:  28%|███▌         | 42/151 [00:08<00:23,  4.56it/s]Epoch: 6, train for the 43-th batch, train loss: 0.6302452087402344:  28%|███▋         | 43/151 [00:08<00:23,  4.55it/s]evaluate for the 102-th batch, evaluate loss: 0.4370664060115814:  95%|██████████████▎| 101/106 [00:27<00:01,  3.69it/s]evaluate for the 102-th batch, evaluate loss: 0.4370664060115814:  96%|██████████████▍| 102/106 [00:27<00:01,  3.63it/s]Epoch: 3, train for the 86-th batch, train loss: 0.5264970064163208:  58%|███████▌     | 85/146 [00:51<00:38,  1.60it/s]Epoch: 3, train for the 86-th batch, train loss: 0.5264970064163208:  59%|███████▋     | 86/146 [00:51<00:36,  1.62it/s]Epoch: 2, train for the 156-th batch, train loss: 0.6404474973678589:  65%|███████▏   | 155/237 [01:32<00:48,  1.68it/s]Epoch: 2, train for the 156-th batch, train loss: 0.6404474973678589:  66%|███████▏   | 156/237 [01:32<00:48,  1.68it/s]evaluate for the 2-th batch, evaluate loss: 0.2844001352787018:   5%|▉                   | 1/21 [00:00<00:05,  3.58it/s]evaluate for the 2-th batch, evaluate loss: 0.2844001352787018:  10%|█▉                  | 2/21 [00:00<00:05,  3.40it/s]Epoch: 6, train for the 44-th batch, train loss: 0.5841809511184692:  28%|███▋         | 43/151 [00:09<00:23,  4.55it/s]Epoch: 6, train for the 44-th batch, train loss: 0.5841809511184692:  29%|███▊         | 44/151 [00:09<00:23,  4.52it/s]evaluate for the 103-th batch, evaluate loss: 0.5440924763679504:  96%|██████████████▍| 102/106 [00:28<00:01,  3.63it/s]evaluate for the 103-th batch, evaluate loss: 0.5440924763679504:  97%|██████████████▌| 103/106 [00:28<00:00,  3.72it/s]evaluate for the 3-th batch, evaluate loss: 0.2762531042098999:  10%|█▉                  | 2/21 [00:00<00:05,  3.40it/s]evaluate for the 3-th batch, evaluate loss: 0.2762531042098999:  14%|██▊                 | 3/21 [00:00<00:05,  3.50it/s]Epoch: 6, train for the 45-th batch, train loss: 0.5953251719474792:  29%|███▊         | 44/151 [00:09<00:23,  4.52it/s]Epoch: 6, train for the 45-th batch, train loss: 0.5953251719474792:  30%|███▊         | 45/151 [00:09<00:25,  4.17it/s]evaluate for the 104-th batch, evaluate loss: 0.4657135605812073:  97%|██████████████▌| 103/106 [00:28<00:00,  3.72it/s]evaluate for the 104-th batch, evaluate loss: 0.4657135605812073:  98%|██████████████▋| 104/106 [00:28<00:00,  3.68it/s]Epoch: 3, train for the 87-th batch, train loss: 0.5190659761428833:  59%|███████▋     | 86/146 [00:52<00:36,  1.62it/s]Epoch: 3, train for the 87-th batch, train loss: 0.5190659761428833:  60%|███████▋     | 87/146 [00:52<00:35,  1.64it/s]Epoch: 2, train for the 157-th batch, train loss: 0.6426933407783508:  66%|███████▏   | 156/237 [01:32<00:48,  1.68it/s]Epoch: 2, train for the 157-th batch, train loss: 0.6426933407783508:  66%|███████▎   | 157/237 [01:32<00:47,  1.68it/s]Epoch: 6, train for the 46-th batch, train loss: 0.5704799294471741:  30%|███▊         | 45/151 [00:09<00:25,  4.17it/s]Epoch: 6, train for the 46-th batch, train loss: 0.5704799294471741:  30%|███▉         | 46/151 [00:09<00:24,  4.34it/s]evaluate for the 4-th batch, evaluate loss: 0.22131262719631195:  14%|██▋                | 3/21 [00:01<00:05,  3.50it/s]evaluate for the 4-th batch, evaluate loss: 0.22131262719631195:  19%|███▌               | 4/21 [00:01<00:04,  3.41it/s]evaluate for the 105-th batch, evaluate loss: 0.48574110865592957:  98%|█████████████▋| 104/106 [00:28<00:00,  3.68it/s]evaluate for the 105-th batch, evaluate loss: 0.48574110865592957:  99%|█████████████▊| 105/106 [00:28<00:00,  3.61it/s]Epoch: 6, train for the 47-th batch, train loss: 0.6410195231437683:  30%|███▉         | 46/151 [00:09<00:24,  4.34it/s]Epoch: 6, train for the 47-th batch, train loss: 0.6410195231437683:  31%|████         | 47/151 [00:09<00:23,  4.38it/s]evaluate for the 106-th batch, evaluate loss: 0.6092822551727295:  99%|██████████████▊| 105/106 [00:28<00:00,  3.61it/s]evaluate for the 106-th batch, evaluate loss: 0.6092822551727295: 100%|███████████████| 106/106 [00:28<00:00,  4.17it/s]evaluate for the 106-th batch, evaluate loss: 0.6092822551727295: 100%|███████████████| 106/106 [00:28<00:00,  3.66it/s]
  0%|                                                                                            | 0/78 [00:00<?, ?it/s]evaluate for the 5-th batch, evaluate loss: 0.25031939148902893:  19%|███▌               | 4/21 [00:01<00:04,  3.41it/s]evaluate for the 5-th batch, evaluate loss: 0.25031939148902893:  24%|████▌              | 5/21 [00:01<00:04,  3.54it/s]Epoch: 6, train for the 48-th batch, train loss: 0.5610323548316956:  31%|████         | 47/151 [00:10<00:23,  4.38it/s]Epoch: 6, train for the 48-th batch, train loss: 0.5610323548316956:  32%|████▏        | 48/151 [00:10<00:23,  4.44it/s]Epoch: 3, train for the 88-th batch, train loss: 0.5461830496788025:  60%|███████▋     | 87/146 [00:52<00:35,  1.64it/s]Epoch: 3, train for the 88-th batch, train loss: 0.5461830496788025:  60%|███████▊     | 88/146 [00:52<00:34,  1.66it/s]Epoch: 2, train for the 158-th batch, train loss: 0.617662787437439:  66%|███████▉    | 157/237 [01:33<00:47,  1.68it/s]Epoch: 2, train for the 158-th batch, train loss: 0.617662787437439:  67%|████████    | 158/237 [01:33<00:47,  1.67it/s]evaluate for the 1-th batch, evaluate loss: 0.7141587138175964:   0%|                            | 0/78 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.7141587138175964:   1%|▎                   | 1/78 [00:00<00:24,  3.12it/s]evaluate for the 6-th batch, evaluate loss: 0.29709765315055847:  24%|████▌              | 5/21 [00:01<00:04,  3.54it/s]evaluate for the 6-th batch, evaluate loss: 0.29709765315055847:  29%|█████▍             | 6/21 [00:01<00:04,  3.44it/s]Epoch: 6, train for the 49-th batch, train loss: 0.5635454654693604:  32%|████▏        | 48/151 [00:10<00:23,  4.44it/s]Epoch: 6, train for the 49-th batch, train loss: 0.5635454654693604:  32%|████▏        | 49/151 [00:10<00:22,  4.48it/s]evaluate for the 2-th batch, evaluate loss: 0.7058321237564087:   1%|▎                   | 1/78 [00:00<00:24,  3.12it/s]evaluate for the 2-th batch, evaluate loss: 0.7058321237564087:   3%|▌                   | 2/78 [00:00<00:22,  3.45it/s]Epoch: 6, train for the 50-th batch, train loss: 0.5919438004493713:  32%|████▏        | 49/151 [00:10<00:22,  4.48it/s]Epoch: 6, train for the 50-th batch, train loss: 0.5919438004493713:  33%|████▎        | 50/151 [00:10<00:22,  4.49it/s]evaluate for the 7-th batch, evaluate loss: 0.2482871562242508:  29%|█████▋              | 6/21 [00:01<00:04,  3.44it/s]evaluate for the 7-th batch, evaluate loss: 0.2482871562242508:  33%|██████▋             | 7/21 [00:01<00:03,  3.62it/s]Epoch: 6, train for the 51-th batch, train loss: 0.6053063273429871:  33%|████▎        | 50/151 [00:10<00:22,  4.49it/s]Epoch: 6, train for the 51-th batch, train loss: 0.6053063273429871:  34%|████▍        | 51/151 [00:10<00:22,  4.50it/s]Epoch: 3, train for the 89-th batch, train loss: 0.5169089436531067:  60%|███████▊     | 88/146 [00:53<00:34,  1.66it/s]Epoch: 3, train for the 89-th batch, train loss: 0.5169089436531067:  61%|███████▉     | 89/146 [00:53<00:33,  1.68it/s]Epoch: 2, train for the 159-th batch, train loss: 0.6420392990112305:  67%|███████▎   | 158/237 [01:33<00:47,  1.67it/s]Epoch: 2, train for the 159-th batch, train loss: 0.6420392990112305:  67%|███████▍   | 159/237 [01:33<00:46,  1.68it/s]evaluate for the 3-th batch, evaluate loss: 0.6726095676422119:   3%|▌                   | 2/78 [00:00<00:22,  3.45it/s]evaluate for the 3-th batch, evaluate loss: 0.6726095676422119:   4%|▊                   | 3/78 [00:00<00:22,  3.34it/s]evaluate for the 8-th batch, evaluate loss: 0.3036464750766754:  33%|██████▋             | 7/21 [00:02<00:03,  3.62it/s]evaluate for the 8-th batch, evaluate loss: 0.3036464750766754:  38%|███████▌            | 8/21 [00:02<00:03,  3.49it/s]Epoch: 6, train for the 52-th batch, train loss: 0.6243345737457275:  34%|████▍        | 51/151 [00:11<00:22,  4.50it/s]Epoch: 6, train for the 52-th batch, train loss: 0.6243345737457275:  34%|████▍        | 52/151 [00:11<00:21,  4.50it/s]evaluate for the 9-th batch, evaluate loss: 0.23815184831619263:  38%|███████▏           | 8/21 [00:02<00:03,  3.49it/s]evaluate for the 9-th batch, evaluate loss: 0.23815184831619263:  43%|████████▏          | 9/21 [00:02<00:03,  3.67it/s]evaluate for the 4-th batch, evaluate loss: 0.7112196087837219:   4%|▊                   | 3/78 [00:01<00:22,  3.34it/s]evaluate for the 4-th batch, evaluate loss: 0.7112196087837219:   5%|█                   | 4/78 [00:01<00:21,  3.48it/s]Epoch: 6, train for the 53-th batch, train loss: 0.5961445569992065:  34%|████▍        | 52/151 [00:11<00:21,  4.50it/s]Epoch: 6, train for the 53-th batch, train loss: 0.5961445569992065:  35%|████▌        | 53/151 [00:11<00:21,  4.52it/s]Epoch: 3, train for the 90-th batch, train loss: 0.5386825203895569:  61%|███████▉     | 89/146 [00:53<00:33,  1.68it/s]Epoch: 3, train for the 90-th batch, train loss: 0.5386825203895569:  62%|████████     | 90/146 [00:53<00:33,  1.68it/s]evaluate for the 5-th batch, evaluate loss: 0.6402202248573303:   5%|█                   | 4/78 [00:01<00:21,  3.48it/s]evaluate for the 5-th batch, evaluate loss: 0.6402202248573303:   6%|█▎                  | 5/78 [00:01<00:21,  3.43it/s]evaluate for the 10-th batch, evaluate loss: 0.2664676308631897:  43%|████████▏          | 9/21 [00:02<00:03,  3.67it/s]evaluate for the 10-th batch, evaluate loss: 0.2664676308631897:  48%|████████▌         | 10/21 [00:02<00:03,  3.50it/s]Epoch: 6, train for the 54-th batch, train loss: 0.6012757420539856:  35%|████▌        | 53/151 [00:11<00:21,  4.52it/s]Epoch: 6, train for the 54-th batch, train loss: 0.6012757420539856:  36%|████▋        | 54/151 [00:11<00:21,  4.51it/s]Epoch: 2, train for the 160-th batch, train loss: 0.6409543752670288:  67%|███████▍   | 159/237 [01:34<00:46,  1.68it/s]Epoch: 2, train for the 160-th batch, train loss: 0.6409543752670288:  68%|███████▍   | 160/237 [01:34<00:45,  1.69it/s]Epoch: 6, train for the 55-th batch, train loss: 0.5943111777305603:  36%|████▋        | 54/151 [00:11<00:21,  4.51it/s]Epoch: 6, train for the 55-th batch, train loss: 0.5943111777305603:  36%|████▋        | 55/151 [00:11<00:21,  4.51it/s]evaluate for the 6-th batch, evaluate loss: 0.5838579535484314:   6%|█▎                  | 5/78 [00:01<00:21,  3.43it/s]evaluate for the 6-th batch, evaluate loss: 0.5838579535484314:   8%|█▌                  | 6/78 [00:01<00:20,  3.58it/s]evaluate for the 11-th batch, evaluate loss: 0.20031827688217163:  48%|████████         | 10/21 [00:03<00:03,  3.50it/s]evaluate for the 11-th batch, evaluate loss: 0.20031827688217163:  52%|████████▉        | 11/21 [00:03<00:02,  3.64it/s]Epoch: 6, train for the 56-th batch, train loss: 0.533972442150116:  36%|█████         | 55/151 [00:11<00:21,  4.51it/s]Epoch: 6, train for the 56-th batch, train loss: 0.533972442150116:  37%|█████▏        | 56/151 [00:11<00:21,  4.50it/s]evaluate for the 7-th batch, evaluate loss: 0.7107241749763489:   8%|█▌                  | 6/78 [00:02<00:20,  3.58it/s]evaluate for the 7-th batch, evaluate loss: 0.7107241749763489:   9%|█▊                  | 7/78 [00:02<00:20,  3.51it/s]Epoch: 3, train for the 91-th batch, train loss: 0.5512366890907288:  62%|████████     | 90/146 [00:54<00:33,  1.68it/s]Epoch: 3, train for the 91-th batch, train loss: 0.5512366890907288:  62%|████████     | 91/146 [00:54<00:32,  1.67it/s]evaluate for the 12-th batch, evaluate loss: 0.2856915593147278:  52%|█████████▍        | 11/21 [00:03<00:02,  3.64it/s]evaluate for the 12-th batch, evaluate loss: 0.2856915593147278:  57%|██████████▎       | 12/21 [00:03<00:02,  3.46it/s]Epoch: 2, train for the 161-th batch, train loss: 0.6530612111091614:  68%|███████▍   | 160/237 [01:34<00:45,  1.69it/s]Epoch: 2, train for the 161-th batch, train loss: 0.6530612111091614:  68%|███████▍   | 161/237 [01:34<00:44,  1.71it/s]Epoch: 6, train for the 57-th batch, train loss: 0.5522921681404114:  37%|████▊        | 56/151 [00:12<00:21,  4.50it/s]Epoch: 6, train for the 57-th batch, train loss: 0.5522921681404114:  38%|████▉        | 57/151 [00:12<00:20,  4.51it/s]evaluate for the 8-th batch, evaluate loss: 0.5569910407066345:   9%|█▊                  | 7/78 [00:02<00:20,  3.51it/s]evaluate for the 8-th batch, evaluate loss: 0.5569910407066345:  10%|██                  | 8/78 [00:02<00:19,  3.64it/s]evaluate for the 13-th batch, evaluate loss: 0.2680559456348419:  57%|██████████▎       | 12/21 [00:03<00:02,  3.46it/s]evaluate for the 13-th batch, evaluate loss: 0.2680559456348419:  62%|███████████▏      | 13/21 [00:03<00:02,  3.53it/s]Epoch: 6, train for the 58-th batch, train loss: 0.44759511947631836:  38%|████▌       | 57/151 [00:12<00:20,  4.51it/s]Epoch: 6, train for the 58-th batch, train loss: 0.44759511947631836:  38%|████▌       | 58/151 [00:12<00:20,  4.58it/s]evaluate for the 9-th batch, evaluate loss: 0.5354806184768677:  10%|██                  | 8/78 [00:02<00:19,  3.64it/s]evaluate for the 9-th batch, evaluate loss: 0.5354806184768677:  12%|██▎                 | 9/78 [00:02<00:19,  3.57it/s]Epoch: 6, train for the 59-th batch, train loss: 0.49914443492889404:  38%|████▌       | 58/151 [00:12<00:20,  4.58it/s]Epoch: 6, train for the 59-th batch, train loss: 0.49914443492889404:  39%|████▋       | 59/151 [00:12<00:20,  4.60it/s]Epoch: 2, train for the 162-th batch, train loss: 0.6220177412033081:  68%|███████▍   | 161/237 [01:35<00:44,  1.71it/s]Epoch: 2, train for the 162-th batch, train loss: 0.6220177412033081:  68%|███████▌   | 162/237 [01:35<00:43,  1.72it/s]Epoch: 3, train for the 92-th batch, train loss: 0.5618744492530823:  62%|████████     | 91/146 [00:55<00:32,  1.67it/s]Epoch: 3, train for the 92-th batch, train loss: 0.5618744492530823:  63%|████████▏    | 92/146 [00:55<00:32,  1.67it/s]evaluate for the 14-th batch, evaluate loss: 0.23364819586277008:  62%|██████████▌      | 13/21 [00:04<00:02,  3.53it/s]evaluate for the 14-th batch, evaluate loss: 0.23364819586277008:  67%|███████████▎     | 14/21 [00:04<00:02,  3.38it/s]Epoch: 6, train for the 60-th batch, train loss: 0.4927864074707031:  39%|█████        | 59/151 [00:12<00:20,  4.60it/s]Epoch: 6, train for the 60-th batch, train loss: 0.4927864074707031:  40%|█████▏       | 60/151 [00:12<00:19,  4.59it/s]evaluate for the 10-th batch, evaluate loss: 0.619895339012146:  12%|██▎                 | 9/78 [00:02<00:19,  3.57it/s]evaluate for the 10-th batch, evaluate loss: 0.619895339012146:  13%|██▍                | 10/78 [00:02<00:18,  3.65it/s]evaluate for the 15-th batch, evaluate loss: 0.22152091562747955:  67%|███████████▎     | 14/21 [00:04<00:02,  3.38it/s]evaluate for the 15-th batch, evaluate loss: 0.22152091562747955:  71%|████████████▏    | 15/21 [00:04<00:01,  3.46it/s]evaluate for the 11-th batch, evaluate loss: 0.6723179221153259:  13%|██▎               | 10/78 [00:02<00:18,  3.65it/s]evaluate for the 11-th batch, evaluate loss: 0.6723179221153259:  14%|██▌               | 11/78 [00:02<00:16,  4.14it/s]Epoch: 6, train for the 61-th batch, train loss: 0.5613639950752258:  40%|█████▏       | 60/151 [00:13<00:19,  4.59it/s]Epoch: 6, train for the 61-th batch, train loss: 0.5613639950752258:  40%|█████▎       | 61/151 [00:13<00:19,  4.57it/s]evaluate for the 12-th batch, evaluate loss: 0.6452618837356567:  14%|██▌               | 11/78 [00:03<00:16,  4.14it/s]evaluate for the 12-th batch, evaluate loss: 0.6452618837356567:  15%|██▊               | 12/78 [00:03<00:13,  4.98it/s]evaluate for the 13-th batch, evaluate loss: 0.7111340165138245:  15%|██▊               | 12/78 [00:03<00:13,  4.98it/s]evaluate for the 13-th batch, evaluate loss: 0.7111340165138245:  17%|███               | 13/78 [00:03<00:11,  5.68it/s]Epoch: 3, train for the 93-th batch, train loss: 0.5467138886451721:  63%|████████▏    | 92/146 [00:55<00:32,  1.67it/s]Epoch: 3, train for the 93-th batch, train loss: 0.5467138886451721:  64%|████████▎    | 93/146 [00:55<00:31,  1.67it/s]evaluate for the 16-th batch, evaluate loss: 0.2663585841655731:  71%|████████████▊     | 15/21 [00:04<00:01,  3.46it/s]evaluate for the 16-th batch, evaluate loss: 0.2663585841655731:  76%|█████████████▋    | 16/21 [00:04<00:01,  3.38it/s]Epoch: 6, train for the 62-th batch, train loss: 0.5818125009536743:  40%|█████▎       | 61/151 [00:13<00:19,  4.57it/s]Epoch: 6, train for the 62-th batch, train loss: 0.5818125009536743:  41%|█████▎       | 62/151 [00:13<00:19,  4.55it/s]evaluate for the 14-th batch, evaluate loss: 0.5249667167663574:  17%|███               | 13/78 [00:03<00:11,  5.68it/s]evaluate for the 14-th batch, evaluate loss: 0.5249667167663574:  18%|███▏              | 14/78 [00:03<00:12,  5.24it/s]Epoch: 6, train for the 63-th batch, train loss: 0.544944167137146:  41%|█████▋        | 62/151 [00:13<00:19,  4.55it/s]Epoch: 6, train for the 63-th batch, train loss: 0.544944167137146:  42%|█████▊        | 63/151 [00:13<00:19,  4.53it/s]evaluate for the 17-th batch, evaluate loss: 0.2791437804698944:  76%|█████████████▋    | 16/21 [00:04<00:01,  3.38it/s]evaluate for the 17-th batch, evaluate loss: 0.2791437804698944:  81%|██████████████▌   | 17/21 [00:04<00:01,  3.46it/s]evaluate for the 15-th batch, evaluate loss: 0.5735944509506226:  18%|███▏              | 14/78 [00:03<00:12,  5.24it/s]evaluate for the 15-th batch, evaluate loss: 0.5735944509506226:  19%|███▍              | 15/78 [00:03<00:10,  6.03it/s]evaluate for the 16-th batch, evaluate loss: 0.5639626979827881:  19%|███▍              | 15/78 [00:03<00:10,  6.03it/s]evaluate for the 16-th batch, evaluate loss: 0.5639626979827881:  21%|███▋              | 16/78 [00:03<00:09,  6.72it/s]Epoch: 6, train for the 64-th batch, train loss: 0.5109453201293945:  42%|█████▍       | 63/151 [00:13<00:19,  4.53it/s]Epoch: 6, train for the 64-th batch, train loss: 0.5109453201293945:  42%|█████▌       | 64/151 [00:13<00:19,  4.53it/s]Epoch: 2, train for the 163-th batch, train loss: 0.669422447681427:  68%|████████▏   | 162/237 [01:36<00:43,  1.72it/s]Epoch: 2, train for the 163-th batch, train loss: 0.669422447681427:  69%|████████▎   | 163/237 [01:36<00:56,  1.32it/s]evaluate for the 18-th batch, evaluate loss: 0.2425130009651184:  81%|██████████████▌   | 17/21 [00:05<00:01,  3.46it/s]evaluate for the 18-th batch, evaluate loss: 0.2425130009651184:  86%|███████████████▍  | 18/21 [00:05<00:00,  3.41it/s]Epoch: 3, train for the 94-th batch, train loss: 0.5200538635253906:  64%|████████▎    | 93/146 [00:56<00:31,  1.67it/s]Epoch: 3, train for the 94-th batch, train loss: 0.5200538635253906:  64%|████████▎    | 94/146 [00:56<00:30,  1.69it/s]Epoch: 6, train for the 65-th batch, train loss: 0.489020973443985:  42%|█████▉        | 64/151 [00:13<00:19,  4.53it/s]Epoch: 6, train for the 65-th batch, train loss: 0.489020973443985:  43%|██████        | 65/151 [00:13<00:19,  4.52it/s]evaluate for the 17-th batch, evaluate loss: 0.5363888144493103:  21%|███▋              | 16/78 [00:03<00:09,  6.72it/s]evaluate for the 17-th batch, evaluate loss: 0.5363888144493103:  22%|███▉              | 17/78 [00:03<00:11,  5.34it/s]evaluate for the 19-th batch, evaluate loss: 0.3087209165096283:  86%|███████████████▍  | 18/21 [00:05<00:00,  3.41it/s]evaluate for the 19-th batch, evaluate loss: 0.3087209165096283:  90%|████████████████▎ | 19/21 [00:05<00:00,  3.54it/s]Epoch: 6, train for the 66-th batch, train loss: 0.5863843560218811:  43%|█████▌       | 65/151 [00:14<00:19,  4.52it/s]Epoch: 6, train for the 66-th batch, train loss: 0.5863843560218811:  44%|█████▋       | 66/151 [00:14<00:18,  4.50it/s]evaluate for the 18-th batch, evaluate loss: 0.5645773410797119:  22%|███▉              | 17/78 [00:04<00:11,  5.34it/s]evaluate for the 18-th batch, evaluate loss: 0.5645773410797119:  23%|████▏             | 18/78 [00:04<00:12,  4.68it/s]Epoch: 6, train for the 67-th batch, train loss: 0.5604920983314514:  44%|█████▋       | 66/151 [00:14<00:18,  4.50it/s]Epoch: 6, train for the 67-th batch, train loss: 0.5604920983314514:  44%|█████▊       | 67/151 [00:14<00:18,  4.50it/s]evaluate for the 20-th batch, evaluate loss: 0.3082975447177887:  90%|████████████████▎ | 19/21 [00:05<00:00,  3.54it/s]evaluate for the 20-th batch, evaluate loss: 0.3082975447177887:  95%|█████████████████▏| 20/21 [00:05<00:00,  3.47it/s]Epoch: 2, train for the 164-th batch, train loss: 0.6446762681007385:  69%|███████▌   | 163/237 [01:37<00:56,  1.32it/s]Epoch: 2, train for the 164-th batch, train loss: 0.6446762681007385:  69%|███████▌   | 164/237 [01:37<00:51,  1.42it/s]Epoch: 3, train for the 95-th batch, train loss: 0.5621975064277649:  64%|████████▎    | 94/146 [00:56<00:30,  1.69it/s]Epoch: 3, train for the 95-th batch, train loss: 0.5621975064277649:  65%|████████▍    | 95/146 [00:56<00:30,  1.69it/s]evaluate for the 21-th batch, evaluate loss: 0.07411565631628036:  95%|████████████████▏| 20/21 [00:05<00:00,  3.47it/s]evaluate for the 21-th batch, evaluate loss: 0.07411565631628036: 100%|█████████████████| 21/21 [00:05<00:00,  3.64it/s]
INFO:root:Epoch: 3, learning rate: 0.0001, train loss: 0.2424
INFO:root:train average_precision, 0.9626
INFO:root:train roc_auc, 0.9491
INFO:root:validate loss: 0.1910
INFO:root:validate average_precision, 0.9781
INFO:root:validate roc_auc, 0.9730
INFO:root:new node validate loss: 0.2536
INFO:root:new node validate first_1_average_precision, 0.8765
INFO:root:new node validate first_1_roc_auc, 0.8803
INFO:root:new node validate first_3_average_precision, 0.9378
INFO:root:new node validate first_3_roc_auc, 0.9358
INFO:root:new node validate first_10_average_precision, 0.9594
INFO:root:new node validate first_10_roc_auc, 0.9565
INFO:root:new node validate average_precision, 0.9612
INFO:root:new node validate roc_auc, 0.9548
INFO:root:save model ./saved_models/DyGFormer/ia-reality-call/DyGFormer_seed0_dygformer-ia-reality-call-old/DyGFormer_seed0_dygformer-ia-reality-call-old.pkl
evaluate for the 19-th batch, evaluate loss: 0.6290993690490723:  23%|████▏             | 18/78 [00:04<00:12,  4.68it/s]evaluate for the 19-th batch, evaluate loss: 0.6290993690490723:  24%|████▍             | 19/78 [00:04<00:14,  4.19it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]Epoch: 6, train for the 68-th batch, train loss: 0.5245742797851562:  44%|█████▊       | 67/151 [00:14<00:18,  4.50it/s]Epoch: 6, train for the 68-th batch, train loss: 0.5245742797851562:  45%|█████▊       | 68/151 [00:14<00:18,  4.50it/s]evaluate for the 20-th batch, evaluate loss: 0.5011739730834961:  24%|████▍             | 19/78 [00:04<00:14,  4.19it/s]evaluate for the 20-th batch, evaluate loss: 0.5011739730834961:  26%|████▌             | 20/78 [00:04<00:14,  4.06it/s]Epoch: 3, train for the 96-th batch, train loss: 0.5434417724609375:  65%|████████▍    | 95/146 [00:57<00:30,  1.69it/s]Epoch: 3, train for the 96-th batch, train loss: 0.5434417724609375:  66%|████████▌    | 96/146 [00:57<00:26,  1.88it/s]Epoch: 6, train for the 69-th batch, train loss: 0.5487582683563232:  45%|█████▊       | 68/151 [00:14<00:18,  4.50it/s]Epoch: 6, train for the 69-th batch, train loss: 0.5487582683563232:  46%|█████▉       | 69/151 [00:14<00:18,  4.50it/s]Epoch: 2, train for the 165-th batch, train loss: 0.6468372941017151:  69%|███████▌   | 164/237 [01:37<00:51,  1.42it/s]Epoch: 2, train for the 165-th batch, train loss: 0.6468372941017151:  70%|███████▋   | 165/237 [01:37<00:48,  1.49it/s]Epoch: 6, train for the 70-th batch, train loss: 0.5617200136184692:  46%|█████▉       | 69/151 [00:15<00:18,  4.50it/s]Epoch: 6, train for the 70-th batch, train loss: 0.5617200136184692:  46%|██████       | 70/151 [00:15<00:18,  4.49it/s]Epoch: 4, train for the 1-th batch, train loss: 1.011335015296936:   0%|                        | 0/119 [00:00<?, ?it/s]Epoch: 4, train for the 1-th batch, train loss: 1.011335015296936:   1%|▏               | 1/119 [00:00<01:01,  1.91it/s]evaluate for the 21-th batch, evaluate loss: 0.5486595034599304:  26%|████▌             | 20/78 [00:05<00:14,  4.06it/s]evaluate for the 21-th batch, evaluate loss: 0.5486595034599304:  27%|████▊             | 21/78 [00:05<00:14,  3.81it/s]Epoch: 6, train for the 71-th batch, train loss: 0.5666671395301819:  46%|██████       | 70/151 [00:15<00:18,  4.49it/s]Epoch: 6, train for the 71-th batch, train loss: 0.5666671395301819:  47%|██████       | 71/151 [00:15<00:17,  4.49it/s]evaluate for the 22-th batch, evaluate loss: 0.5669267773628235:  27%|████▊             | 21/78 [00:05<00:14,  3.81it/s]evaluate for the 22-th batch, evaluate loss: 0.5669267773628235:  28%|█████             | 22/78 [00:05<00:14,  3.84it/s]Epoch: 3, train for the 97-th batch, train loss: 0.5231696367263794:  66%|████████▌    | 96/146 [00:57<00:26,  1.88it/s]Epoch: 3, train for the 97-th batch, train loss: 0.5231696367263794:  66%|████████▋    | 97/146 [00:57<00:26,  1.83it/s]Epoch: 6, train for the 72-th batch, train loss: 0.5206542611122131:  47%|██████       | 71/151 [00:15<00:17,  4.49it/s]Epoch: 6, train for the 72-th batch, train loss: 0.5206542611122131:  48%|██████▏      | 72/151 [00:15<00:17,  4.49it/s]Epoch: 2, train for the 166-th batch, train loss: 0.5953176617622375:  70%|███████▋   | 165/237 [01:38<00:48,  1.49it/s]Epoch: 2, train for the 166-th batch, train loss: 0.5953176617622375:  70%|███████▋   | 166/237 [01:38<00:45,  1.56it/s]evaluate for the 23-th batch, evaluate loss: 0.5242347717285156:  28%|█████             | 22/78 [00:05<00:14,  3.84it/s]evaluate for the 23-th batch, evaluate loss: 0.5242347717285156:  29%|█████▎            | 23/78 [00:05<00:14,  3.67it/s]Epoch: 4, train for the 2-th batch, train loss: 0.5715336203575134:   1%|▏              | 1/119 [00:01<01:01,  1.91it/s]Epoch: 4, train for the 2-th batch, train loss: 0.5715336203575134:   2%|▎              | 2/119 [00:01<01:05,  1.78it/s]Epoch: 6, train for the 73-th batch, train loss: 0.5192809104919434:  48%|██████▏      | 72/151 [00:15<00:17,  4.49it/s]Epoch: 6, train for the 73-th batch, train loss: 0.5192809104919434:  48%|██████▎      | 73/151 [00:15<00:17,  4.49it/s]evaluate for the 24-th batch, evaluate loss: 0.5577327609062195:  29%|█████▎            | 23/78 [00:05<00:14,  3.67it/s]evaluate for the 24-th batch, evaluate loss: 0.5577327609062195:  31%|█████▌            | 24/78 [00:05<00:14,  3.77it/s]Epoch: 6, train for the 74-th batch, train loss: 0.5494160056114197:  48%|██████▎      | 73/151 [00:15<00:17,  4.49it/s]Epoch: 6, train for the 74-th batch, train loss: 0.5494160056114197:  49%|██████▎      | 74/151 [00:15<00:17,  4.48it/s]Epoch: 3, train for the 98-th batch, train loss: 0.5279392600059509:  66%|████████▋    | 97/146 [00:58<00:26,  1.83it/s]Epoch: 3, train for the 98-th batch, train loss: 0.5279392600059509:  67%|████████▋    | 98/146 [00:58<00:26,  1.79it/s]Epoch: 2, train for the 167-th batch, train loss: 0.6182190775871277:  70%|███████▋   | 166/237 [01:39<00:45,  1.56it/s]Epoch: 2, train for the 167-th batch, train loss: 0.6182190775871277:  70%|███████▊   | 167/237 [01:39<00:43,  1.61it/s]Epoch: 6, train for the 75-th batch, train loss: 0.47774702310562134:  49%|█████▉      | 74/151 [00:16<00:17,  4.48it/s]Epoch: 6, train for the 75-th batch, train loss: 0.47774702310562134:  50%|█████▉      | 75/151 [00:16<00:16,  4.52it/s]evaluate for the 25-th batch, evaluate loss: 0.6049649715423584:  31%|█████▌            | 24/78 [00:06<00:14,  3.77it/s]evaluate for the 25-th batch, evaluate loss: 0.6049649715423584:  32%|█████▊            | 25/78 [00:06<00:14,  3.61it/s]Epoch: 4, train for the 3-th batch, train loss: 0.36271944642066956:   2%|▏             | 2/119 [00:01<01:05,  1.78it/s]Epoch: 4, train for the 3-th batch, train loss: 0.36271944642066956:   3%|▎             | 3/119 [00:01<01:06,  1.73it/s]Epoch: 6, train for the 76-th batch, train loss: 0.5811799764633179:  50%|██████▍      | 75/151 [00:16<00:16,  4.52it/s]Epoch: 6, train for the 76-th batch, train loss: 0.5811799764633179:  50%|██████▌      | 76/151 [00:16<00:16,  4.51it/s]evaluate for the 26-th batch, evaluate loss: 0.5715172290802002:  32%|█████▊            | 25/78 [00:06<00:14,  3.61it/s]evaluate for the 26-th batch, evaluate loss: 0.5715172290802002:  33%|██████            | 26/78 [00:06<00:13,  3.76it/s]Epoch: 3, train for the 99-th batch, train loss: 0.52791827917099:  67%|██████████     | 98/146 [00:58<00:26,  1.79it/s]Epoch: 3, train for the 99-th batch, train loss: 0.52791827917099:  68%|██████████▏    | 99/146 [00:58<00:26,  1.75it/s]Epoch: 6, train for the 77-th batch, train loss: 0.5492347478866577:  50%|██████▌      | 76/151 [00:16<00:16,  4.51it/s]Epoch: 6, train for the 77-th batch, train loss: 0.5492347478866577:  51%|██████▋      | 77/151 [00:16<00:16,  4.51it/s]Epoch: 2, train for the 168-th batch, train loss: 0.644915759563446:  70%|████████▍   | 167/237 [01:39<00:43,  1.61it/s]Epoch: 2, train for the 168-th batch, train loss: 0.644915759563446:  71%|████████▌   | 168/237 [01:39<00:42,  1.64it/s]evaluate for the 27-th batch, evaluate loss: 0.6470767259597778:  33%|██████            | 26/78 [00:06<00:13,  3.76it/s]evaluate for the 27-th batch, evaluate loss: 0.6470767259597778:  35%|██████▏           | 27/78 [00:06<00:14,  3.58it/s]Epoch: 6, train for the 78-th batch, train loss: 0.5200634598731995:  51%|██████▋      | 77/151 [00:16<00:16,  4.51it/s]Epoch: 6, train for the 78-th batch, train loss: 0.5200634598731995:  52%|██████▋      | 78/151 [00:16<00:16,  4.51it/s]Epoch: 4, train for the 4-th batch, train loss: 0.2863938510417938:   3%|▍              | 3/119 [00:02<01:06,  1.73it/s]Epoch: 4, train for the 4-th batch, train loss: 0.2863938510417938:   3%|▌              | 4/119 [00:02<01:07,  1.71it/s]evaluate for the 28-th batch, evaluate loss: 0.7166783809661865:  35%|██████▏           | 27/78 [00:06<00:14,  3.58it/s]evaluate for the 28-th batch, evaluate loss: 0.7166783809661865:  36%|██████▍           | 28/78 [00:06<00:13,  3.70it/s]Epoch: 6, train for the 79-th batch, train loss: 0.6018291115760803:  52%|██████▋      | 78/151 [00:17<00:16,  4.51it/s]Epoch: 6, train for the 79-th batch, train loss: 0.6018291115760803:  52%|██████▊      | 79/151 [00:17<00:15,  4.52it/s]Epoch: 3, train for the 100-th batch, train loss: 0.593192458152771:  68%|████████▊    | 99/146 [00:59<00:26,  1.75it/s]Epoch: 3, train for the 100-th batch, train loss: 0.593192458152771:  68%|████████▏   | 100/146 [00:59<00:26,  1.73it/s]Epoch: 6, train for the 80-th batch, train loss: 0.5515962839126587:  52%|██████▊      | 79/151 [00:17<00:15,  4.52it/s]Epoch: 6, train for the 80-th batch, train loss: 0.5515962839126587:  53%|██████▉      | 80/151 [00:17<00:15,  4.51it/s]Epoch: 2, train for the 169-th batch, train loss: 0.6414659023284912:  71%|███████▊   | 168/237 [01:40<00:42,  1.64it/s]Epoch: 2, train for the 169-th batch, train loss: 0.6414659023284912:  71%|███████▊   | 169/237 [01:40<00:41,  1.65it/s]evaluate for the 29-th batch, evaluate loss: 0.5729480385780334:  36%|██████▍           | 28/78 [00:07<00:13,  3.70it/s]evaluate for the 29-th batch, evaluate loss: 0.5729480385780334:  37%|██████▋           | 29/78 [00:07<00:13,  3.51it/s]Epoch: 4, train for the 5-th batch, train loss: 0.34489768743515015:   3%|▍             | 4/119 [00:02<01:07,  1.71it/s]Epoch: 4, train for the 5-th batch, train loss: 0.34489768743515015:   4%|▌             | 5/119 [00:02<01:07,  1.70it/s]Epoch: 6, train for the 81-th batch, train loss: 0.5610537528991699:  53%|██████▉      | 80/151 [00:17<00:15,  4.51it/s]Epoch: 6, train for the 81-th batch, train loss: 0.5610537528991699:  54%|██████▉      | 81/151 [00:17<00:15,  4.51it/s]evaluate for the 30-th batch, evaluate loss: 0.6048826575279236:  37%|██████▋           | 29/78 [00:07<00:13,  3.51it/s]evaluate for the 30-th batch, evaluate loss: 0.6048826575279236:  38%|██████▉           | 30/78 [00:07<00:13,  3.58it/s]Epoch: 6, train for the 82-th batch, train loss: 0.5227447748184204:  54%|██████▉      | 81/151 [00:17<00:15,  4.51it/s]Epoch: 6, train for the 82-th batch, train loss: 0.5227447748184204:  54%|███████      | 82/151 [00:17<00:15,  4.53it/s]Epoch: 3, train for the 101-th batch, train loss: 0.5429918766021729:  68%|███████▌   | 100/146 [01:00<00:26,  1.73it/s]Epoch: 3, train for the 101-th batch, train loss: 0.5429918766021729:  69%|███████▌   | 101/146 [01:00<00:26,  1.71it/s]Epoch: 2, train for the 170-th batch, train loss: 0.6457840800285339:  71%|███████▊   | 169/237 [01:40<00:41,  1.65it/s]Epoch: 2, train for the 170-th batch, train loss: 0.6457840800285339:  72%|███████▉   | 170/237 [01:40<00:40,  1.66it/s]evaluate for the 31-th batch, evaluate loss: 0.47171729803085327:  38%|██████▌          | 30/78 [00:07<00:13,  3.58it/s]evaluate for the 31-th batch, evaluate loss: 0.47171729803085327:  40%|██████▊          | 31/78 [00:07<00:13,  3.43it/s]Epoch: 6, train for the 83-th batch, train loss: 0.5131000280380249:  54%|███████      | 82/151 [00:17<00:15,  4.53it/s]Epoch: 6, train for the 83-th batch, train loss: 0.5131000280380249:  55%|███████▏     | 83/151 [00:17<00:15,  4.52it/s]Epoch: 4, train for the 6-th batch, train loss: 0.271904855966568:   4%|▋               | 5/119 [00:03<01:07,  1.70it/s]Epoch: 4, train for the 6-th batch, train loss: 0.271904855966568:   5%|▊               | 6/119 [00:03<01:06,  1.69it/s]Epoch: 6, train for the 84-th batch, train loss: 0.585298478603363:  55%|███████▋      | 83/151 [00:18<00:15,  4.52it/s]Epoch: 6, train for the 84-th batch, train loss: 0.585298478603363:  56%|███████▊      | 84/151 [00:18<00:14,  4.51it/s]evaluate for the 32-th batch, evaluate loss: 0.5487505197525024:  40%|███████▏          | 31/78 [00:08<00:13,  3.43it/s]evaluate for the 32-th batch, evaluate loss: 0.5487505197525024:  41%|███████▍          | 32/78 [00:08<00:13,  3.51it/s]Epoch: 6, train for the 85-th batch, train loss: 0.5543290376663208:  56%|███████▏     | 84/151 [00:18<00:14,  4.51it/s]Epoch: 6, train for the 85-th batch, train loss: 0.5543290376663208:  56%|███████▎     | 85/151 [00:18<00:14,  4.50it/s]Epoch: 3, train for the 102-th batch, train loss: 0.5486446022987366:  69%|███████▌   | 101/146 [01:00<00:26,  1.71it/s]Epoch: 3, train for the 102-th batch, train loss: 0.5486446022987366:  70%|███████▋   | 102/146 [01:00<00:25,  1.70it/s]Epoch: 2, train for the 171-th batch, train loss: 0.6557233929634094:  72%|███████▉   | 170/237 [01:41<00:40,  1.66it/s]Epoch: 2, train for the 171-th batch, train loss: 0.6557233929634094:  72%|███████▉   | 171/237 [01:41<00:39,  1.67it/s]evaluate for the 33-th batch, evaluate loss: 0.6009758710861206:  41%|███████▍          | 32/78 [00:08<00:13,  3.51it/s]evaluate for the 33-th batch, evaluate loss: 0.6009758710861206:  42%|███████▌          | 33/78 [00:08<00:13,  3.42it/s]Epoch: 6, train for the 86-th batch, train loss: 0.5284239053726196:  56%|███████▎     | 85/151 [00:18<00:14,  4.50it/s]Epoch: 6, train for the 86-th batch, train loss: 0.5284239053726196:  57%|███████▍     | 86/151 [00:18<00:14,  4.52it/s]Epoch: 4, train for the 7-th batch, train loss: 0.2641967236995697:   5%|▊              | 6/119 [00:04<01:06,  1.69it/s]Epoch: 4, train for the 7-th batch, train loss: 0.2641967236995697:   6%|▉              | 7/119 [00:04<01:06,  1.69it/s]evaluate for the 34-th batch, evaluate loss: 0.5202884674072266:  42%|███████▌          | 33/78 [00:08<00:13,  3.42it/s]evaluate for the 34-th batch, evaluate loss: 0.5202884674072266:  44%|███████▊          | 34/78 [00:08<00:12,  3.49it/s]Epoch: 6, train for the 87-th batch, train loss: 0.5636542439460754:  57%|███████▍     | 86/151 [00:18<00:14,  4.52it/s]Epoch: 6, train for the 87-th batch, train loss: 0.5636542439460754:  58%|███████▍     | 87/151 [00:18<00:14,  4.51it/s]Epoch: 3, train for the 103-th batch, train loss: 0.5430423617362976:  70%|███████▋   | 102/146 [01:01<00:25,  1.70it/s]Epoch: 3, train for the 103-th batch, train loss: 0.5430423617362976:  71%|███████▊   | 103/146 [01:01<00:25,  1.70it/s]Epoch: 6, train for the 88-th batch, train loss: 0.5868982076644897:  58%|███████▍     | 87/151 [00:18<00:14,  4.51it/s]Epoch: 6, train for the 88-th batch, train loss: 0.5868982076644897:  58%|███████▌     | 88/151 [00:18<00:13,  4.51it/s]evaluate for the 35-th batch, evaluate loss: 0.6122720837593079:  44%|███████▊          | 34/78 [00:09<00:12,  3.49it/s]evaluate for the 35-th batch, evaluate loss: 0.6122720837593079:  45%|████████          | 35/78 [00:09<00:12,  3.43it/s]Epoch: 2, train for the 172-th batch, train loss: 0.5942901372909546:  72%|███████▉   | 171/237 [01:41<00:39,  1.67it/s]Epoch: 2, train for the 172-th batch, train loss: 0.5942901372909546:  73%|███████▉   | 172/237 [01:41<00:38,  1.69it/s]Epoch: 6, train for the 89-th batch, train loss: 0.558345377445221:  58%|████████▏     | 88/151 [00:19<00:13,  4.51it/s]Epoch: 6, train for the 89-th batch, train loss: 0.558345377445221:  59%|████████▎     | 89/151 [00:19<00:13,  4.51it/s]Epoch: 4, train for the 8-th batch, train loss: 0.255071222782135:   6%|▉               | 7/119 [00:04<01:06,  1.69it/s]Epoch: 4, train for the 8-th batch, train loss: 0.255071222782135:   7%|█               | 8/119 [00:04<01:05,  1.68it/s]evaluate for the 36-th batch, evaluate loss: 0.5844802260398865:  45%|████████          | 35/78 [00:09<00:12,  3.43it/s]evaluate for the 36-th batch, evaluate loss: 0.5844802260398865:  46%|████████▎         | 36/78 [00:09<00:11,  3.56it/s]Epoch: 6, train for the 90-th batch, train loss: 0.5581825375556946:  59%|███████▋     | 89/151 [00:19<00:13,  4.51it/s]Epoch: 6, train for the 90-th batch, train loss: 0.5581825375556946:  60%|███████▋     | 90/151 [00:19<00:13,  4.50it/s]Epoch: 3, train for the 104-th batch, train loss: 0.5511707663536072:  71%|███████▊   | 103/146 [01:01<00:25,  1.70it/s]Epoch: 3, train for the 104-th batch, train loss: 0.5511707663536072:  71%|███████▊   | 104/146 [01:01<00:24,  1.69it/s]evaluate for the 37-th batch, evaluate loss: 0.6327670812606812:  46%|████████▎         | 36/78 [00:09<00:11,  3.56it/s]evaluate for the 37-th batch, evaluate loss: 0.6327670812606812:  47%|████████▌         | 37/78 [00:09<00:11,  3.51it/s]Epoch: 2, train for the 173-th batch, train loss: 0.6228219270706177:  73%|███████▉   | 172/237 [01:42<00:38,  1.69it/s]Epoch: 2, train for the 173-th batch, train loss: 0.6228219270706177:  73%|████████   | 173/237 [01:42<00:37,  1.71it/s]Epoch: 6, train for the 91-th batch, train loss: 0.47230997681617737:  60%|███████▏    | 90/151 [00:19<00:13,  4.50it/s]Epoch: 6, train for the 91-th batch, train loss: 0.47230997681617737:  60%|███████▏    | 91/151 [00:19<00:13,  4.50it/s]Epoch: 4, train for the 9-th batch, train loss: 0.2533807158470154:   7%|█              | 8/119 [00:05<01:05,  1.68it/s]Epoch: 4, train for the 9-th batch, train loss: 0.2533807158470154:   8%|█▏             | 9/119 [00:05<01:04,  1.70it/s]evaluate for the 38-th batch, evaluate loss: 0.5474126935005188:  47%|████████▌         | 37/78 [00:09<00:11,  3.51it/s]evaluate for the 38-th batch, evaluate loss: 0.5474126935005188:  49%|████████▊         | 38/78 [00:09<00:11,  3.63it/s]Epoch: 6, train for the 92-th batch, train loss: 0.5671693682670593:  60%|███████▊     | 91/151 [00:19<00:13,  4.50it/s]Epoch: 6, train for the 92-th batch, train loss: 0.5671693682670593:  61%|███████▉     | 92/151 [00:19<00:13,  4.51it/s]Epoch: 6, train for the 93-th batch, train loss: 0.5434667468070984:  61%|███████▉     | 92/151 [00:20<00:13,  4.51it/s]Epoch: 6, train for the 93-th batch, train loss: 0.5434667468070984:  62%|████████     | 93/151 [00:20<00:12,  4.50it/s]Epoch: 3, train for the 105-th batch, train loss: 0.5729460716247559:  71%|███████▊   | 104/146 [01:02<00:24,  1.69it/s]Epoch: 3, train for the 105-th batch, train loss: 0.5729460716247559:  72%|███████▉   | 105/146 [01:02<00:24,  1.70it/s]evaluate for the 39-th batch, evaluate loss: 0.6242749094963074:  49%|████████▊         | 38/78 [00:10<00:11,  3.63it/s]evaluate for the 39-th batch, evaluate loss: 0.6242749094963074:  50%|█████████         | 39/78 [00:10<00:10,  3.55it/s]Epoch: 2, train for the 174-th batch, train loss: 0.6168275475502014:  73%|████████   | 173/237 [01:43<00:37,  1.71it/s]Epoch: 2, train for the 174-th batch, train loss: 0.6168275475502014:  73%|████████   | 174/237 [01:43<00:36,  1.72it/s]Epoch: 6, train for the 94-th batch, train loss: 0.5429759621620178:  62%|████████     | 93/151 [00:20<00:12,  4.50it/s]Epoch: 6, train for the 94-th batch, train loss: 0.5429759621620178:  62%|████████     | 94/151 [00:20<00:12,  4.50it/s]evaluate for the 40-th batch, evaluate loss: 0.5529171824455261:  50%|█████████         | 39/78 [00:10<00:10,  3.55it/s]evaluate for the 40-th batch, evaluate loss: 0.5529171824455261:  51%|█████████▏        | 40/78 [00:10<00:10,  3.65it/s]Epoch: 4, train for the 10-th batch, train loss: 0.267459511756897:   8%|█▏             | 9/119 [00:05<01:04,  1.70it/s]Epoch: 4, train for the 10-th batch, train loss: 0.267459511756897:   8%|█▏            | 10/119 [00:05<01:04,  1.69it/s]Epoch: 6, train for the 95-th batch, train loss: 0.5050509572029114:  62%|████████     | 94/151 [00:20<00:12,  4.50it/s]Epoch: 6, train for the 95-th batch, train loss: 0.5050509572029114:  63%|████████▏    | 95/151 [00:20<00:12,  4.49it/s]evaluate for the 41-th batch, evaluate loss: 0.5772349238395691:  51%|█████████▏        | 40/78 [00:10<00:10,  3.65it/s]evaluate for the 41-th batch, evaluate loss: 0.5772349238395691:  53%|█████████▍        | 41/78 [00:10<00:10,  3.58it/s]Epoch: 3, train for the 106-th batch, train loss: 0.5493218898773193:  72%|███████▉   | 105/146 [01:03<00:24,  1.70it/s]Epoch: 3, train for the 106-th batch, train loss: 0.5493218898773193:  73%|███████▉   | 106/146 [01:03<00:23,  1.70it/s]Epoch: 2, train for the 175-th batch, train loss: 0.6308819651603699:  73%|████████   | 174/237 [01:43<00:36,  1.72it/s]Epoch: 2, train for the 175-th batch, train loss: 0.6308819651603699:  74%|████████   | 175/237 [01:43<00:35,  1.73it/s]Epoch: 6, train for the 96-th batch, train loss: 0.5488696694374084:  63%|████████▏    | 95/151 [00:20<00:12,  4.49it/s]Epoch: 6, train for the 96-th batch, train loss: 0.5488696694374084:  64%|████████▎    | 96/151 [00:20<00:12,  4.50it/s]evaluate for the 42-th batch, evaluate loss: 0.5292142033576965:  53%|█████████▍        | 41/78 [00:10<00:10,  3.58it/s]evaluate for the 42-th batch, evaluate loss: 0.5292142033576965:  54%|█████████▋        | 42/78 [00:10<00:09,  3.65it/s]Epoch: 6, train for the 97-th batch, train loss: 0.6033251285552979:  64%|████████▎    | 96/151 [00:20<00:12,  4.50it/s]Epoch: 6, train for the 97-th batch, train loss: 0.6033251285552979:  64%|████████▎    | 97/151 [00:20<00:11,  4.50it/s]Epoch: 4, train for the 11-th batch, train loss: 0.23229137063026428:   8%|█           | 10/119 [00:06<01:04,  1.69it/s]Epoch: 4, train for the 11-th batch, train loss: 0.23229137063026428:   9%|█           | 11/119 [00:06<01:04,  1.69it/s]Epoch: 6, train for the 98-th batch, train loss: 0.6049941182136536:  64%|████████▎    | 97/151 [00:21<00:11,  4.50it/s]Epoch: 6, train for the 98-th batch, train loss: 0.6049941182136536:  65%|████████▍    | 98/151 [00:21<00:11,  4.50it/s]evaluate for the 43-th batch, evaluate loss: 0.6162384748458862:  54%|█████████▋        | 42/78 [00:11<00:09,  3.65it/s]evaluate for the 43-th batch, evaluate loss: 0.6162384748458862:  55%|█████████▉        | 43/78 [00:11<00:09,  3.59it/s]Epoch: 3, train for the 107-th batch, train loss: 0.5538106560707092:  73%|███████▉   | 106/146 [01:03<00:23,  1.70it/s]Epoch: 3, train for the 107-th batch, train loss: 0.5538106560707092:  73%|████████   | 107/146 [01:03<00:23,  1.69it/s]Epoch: 2, train for the 176-th batch, train loss: 0.6091403961181641:  74%|████████   | 175/237 [01:44<00:35,  1.73it/s]Epoch: 2, train for the 176-th batch, train loss: 0.6091403961181641:  74%|████████▏  | 176/237 [01:44<00:35,  1.72it/s]Epoch: 6, train for the 99-th batch, train loss: 0.6144881844520569:  65%|████████▍    | 98/151 [00:21<00:11,  4.50it/s]Epoch: 6, train for the 99-th batch, train loss: 0.6144881844520569:  66%|████████▌    | 99/151 [00:21<00:11,  4.50it/s]evaluate for the 44-th batch, evaluate loss: 0.5178300142288208:  55%|█████████▉        | 43/78 [00:11<00:09,  3.59it/s]evaluate for the 44-th batch, evaluate loss: 0.5178300142288208:  56%|██████████▏       | 44/78 [00:11<00:09,  3.63it/s]Epoch: 4, train for the 12-th batch, train loss: 0.2644868493080139:   9%|█▏           | 11/119 [00:07<01:04,  1.69it/s]Epoch: 4, train for the 12-th batch, train loss: 0.2644868493080139:  10%|█▎           | 12/119 [00:07<01:02,  1.70it/s]Epoch: 6, train for the 100-th batch, train loss: 0.6960086822509766:  66%|███████▊    | 99/151 [00:21<00:11,  4.50it/s]Epoch: 6, train for the 100-th batch, train loss: 0.6960086822509766:  66%|███████▎   | 100/151 [00:21<00:11,  4.50it/s]evaluate for the 45-th batch, evaluate loss: 0.596619725227356:  56%|██████████▋        | 44/78 [00:11<00:09,  3.63it/s]evaluate for the 45-th batch, evaluate loss: 0.596619725227356:  58%|██████████▉        | 45/78 [00:11<00:09,  3.58it/s]Epoch: 6, train for the 101-th batch, train loss: 0.6854325532913208:  66%|███████▎   | 100/151 [00:21<00:11,  4.50it/s]Epoch: 6, train for the 101-th batch, train loss: 0.6854325532913208:  67%|███████▎   | 101/151 [00:21<00:11,  4.50it/s]Epoch: 2, train for the 177-th batch, train loss: 0.6354536414146423:  74%|████████▏  | 176/237 [01:44<00:35,  1.72it/s]Epoch: 2, train for the 177-th batch, train loss: 0.6354536414146423:  75%|████████▏  | 177/237 [01:44<00:34,  1.72it/s]Epoch: 4, train for the 13-th batch, train loss: 0.23927447199821472:  10%|█▏          | 12/119 [00:07<01:02,  1.70it/s]Epoch: 4, train for the 13-th batch, train loss: 0.23927447199821472:  11%|█▎          | 13/119 [00:07<00:55,  1.92it/s]evaluate for the 46-th batch, evaluate loss: 0.6437830924987793:  58%|██████████▍       | 45/78 [00:12<00:09,  3.58it/s]evaluate for the 46-th batch, evaluate loss: 0.6437830924987793:  59%|██████████▌       | 46/78 [00:12<00:08,  3.56it/s]Epoch: 6, train for the 102-th batch, train loss: 0.6261202692985535:  67%|███████▎   | 101/151 [00:22<00:11,  4.50it/s]Epoch: 6, train for the 102-th batch, train loss: 0.6261202692985535:  68%|███████▍   | 102/151 [00:22<00:10,  4.50it/s]Epoch: 3, train for the 108-th batch, train loss: 0.5683935284614563:  73%|████████   | 107/146 [01:04<00:23,  1.69it/s]Epoch: 3, train for the 108-th batch, train loss: 0.5683935284614563:  74%|████████▏  | 108/146 [01:04<00:25,  1.46it/s]Epoch: 4, train for the 14-th batch, train loss: 0.2691487967967987:  11%|█▍           | 13/119 [00:07<00:55,  1.92it/s]Epoch: 4, train for the 14-th batch, train loss: 0.2691487967967987:  12%|█▌           | 14/119 [00:07<00:47,  2.23it/s]evaluate for the 47-th batch, evaluate loss: 0.5001330375671387:  59%|██████████▌       | 46/78 [00:12<00:08,  3.56it/s]evaluate for the 47-th batch, evaluate loss: 0.5001330375671387:  60%|██████████▊       | 47/78 [00:12<00:08,  3.58it/s]Epoch: 6, train for the 103-th batch, train loss: 0.6612755656242371:  68%|███████▍   | 102/151 [00:22<00:10,  4.50it/s]Epoch: 6, train for the 103-th batch, train loss: 0.6612755656242371:  68%|███████▌   | 103/151 [00:22<00:11,  4.04it/s]Epoch: 2, train for the 178-th batch, train loss: 0.6269588470458984:  75%|████████▏  | 177/237 [01:45<00:34,  1.72it/s]Epoch: 2, train for the 178-th batch, train loss: 0.6269588470458984:  75%|████████▎  | 178/237 [01:45<00:34,  1.71it/s]Epoch: 6, train for the 104-th batch, train loss: 0.6199169754981995:  68%|███████▌   | 103/151 [00:22<00:11,  4.04it/s]Epoch: 6, train for the 104-th batch, train loss: 0.6199169754981995:  69%|███████▌   | 104/151 [00:22<00:11,  4.16it/s]evaluate for the 48-th batch, evaluate loss: 0.6295813918113708:  60%|██████████▊       | 47/78 [00:12<00:08,  3.58it/s]evaluate for the 48-th batch, evaluate loss: 0.6295813918113708:  62%|███████████       | 48/78 [00:12<00:08,  3.51it/s]Epoch: 3, train for the 109-th batch, train loss: 0.538139283657074:  74%|████████▉   | 108/146 [01:05<00:25,  1.46it/s]Epoch: 3, train for the 109-th batch, train loss: 0.538139283657074:  75%|████████▉   | 109/146 [01:05<00:24,  1.49it/s]Epoch: 4, train for the 15-th batch, train loss: 0.2537033259868622:  12%|█▌           | 14/119 [00:08<00:47,  2.23it/s]Epoch: 4, train for the 15-th batch, train loss: 0.2537033259868622:  13%|█▋           | 15/119 [00:08<00:51,  2.00it/s]Epoch: 6, train for the 105-th batch, train loss: 0.5623820424079895:  69%|███████▌   | 104/151 [00:22<00:11,  4.16it/s]Epoch: 6, train for the 105-th batch, train loss: 0.5623820424079895:  70%|███████▋   | 105/151 [00:22<00:10,  4.25it/s]evaluate for the 49-th batch, evaluate loss: 0.5614817142486572:  62%|███████████       | 48/78 [00:12<00:08,  3.51it/s]evaluate for the 49-th batch, evaluate loss: 0.5614817142486572:  63%|███████████▎      | 49/78 [00:12<00:08,  3.58it/s]Epoch: 6, train for the 106-th batch, train loss: 0.5795506834983826:  70%|███████▋   | 105/151 [00:23<00:10,  4.25it/s]Epoch: 6, train for the 106-th batch, train loss: 0.5795506834983826:  70%|███████▋   | 106/151 [00:23<00:10,  4.32it/s]Epoch: 2, train for the 179-th batch, train loss: 0.6210229396820068:  75%|████████▎  | 178/237 [01:46<00:34,  1.71it/s]Epoch: 2, train for the 179-th batch, train loss: 0.6210229396820068:  76%|████████▎  | 179/237 [01:46<00:33,  1.71it/s]evaluate for the 50-th batch, evaluate loss: 0.5684861540794373:  63%|███████████▎      | 49/78 [00:13<00:08,  3.58it/s]evaluate for the 50-th batch, evaluate loss: 0.5684861540794373:  64%|███████████▌      | 50/78 [00:13<00:08,  3.50it/s]Epoch: 6, train for the 107-th batch, train loss: 0.5587302446365356:  70%|███████▋   | 106/151 [00:23<00:10,  4.32it/s]Epoch: 6, train for the 107-th batch, train loss: 0.5587302446365356:  71%|███████▊   | 107/151 [00:23<00:10,  4.37it/s]evaluate for the 51-th batch, evaluate loss: 0.5475521087646484:  64%|███████████▌      | 50/78 [00:13<00:08,  3.50it/s]evaluate for the 51-th batch, evaluate loss: 0.5475521087646484:  65%|███████████▊      | 51/78 [00:13<00:07,  3.63it/s]Epoch: 3, train for the 110-th batch, train loss: 0.5194705724716187:  75%|████████▏  | 109/146 [01:05<00:24,  1.49it/s]Epoch: 3, train for the 110-th batch, train loss: 0.5194705724716187:  75%|████████▎  | 110/146 [01:05<00:24,  1.50it/s]Epoch: 4, train for the 16-th batch, train loss: 0.2585963308811188:  13%|█▋           | 15/119 [00:08<00:51,  2.00it/s]Epoch: 4, train for the 16-th batch, train loss: 0.2585963308811188:  13%|█▋           | 16/119 [00:08<00:56,  1.83it/s]Epoch: 6, train for the 108-th batch, train loss: 0.5524606108665466:  71%|███████▊   | 107/151 [00:23<00:10,  4.37it/s]Epoch: 6, train for the 108-th batch, train loss: 0.5524606108665466:  72%|███████▊   | 108/151 [00:23<00:09,  4.39it/s]Epoch: 2, train for the 180-th batch, train loss: 0.6435887217521667:  76%|████████▎  | 179/237 [01:46<00:33,  1.71it/s]Epoch: 2, train for the 180-th batch, train loss: 0.6435887217521667:  76%|████████▎  | 180/237 [01:46<00:33,  1.72it/s]Epoch: 6, train for the 109-th batch, train loss: 0.5140378475189209:  72%|███████▊   | 108/151 [00:23<00:09,  4.39it/s]Epoch: 6, train for the 109-th batch, train loss: 0.5140378475189209:  72%|███████▉   | 109/151 [00:23<00:09,  4.46it/s]evaluate for the 52-th batch, evaluate loss: 0.5624969005584717:  65%|███████████▊      | 51/78 [00:13<00:07,  3.63it/s]evaluate for the 52-th batch, evaluate loss: 0.5624969005584717:  67%|████████████      | 52/78 [00:13<00:07,  3.53it/s]Epoch: 6, train for the 110-th batch, train loss: 0.5383121967315674:  72%|███████▉   | 109/151 [00:23<00:09,  4.46it/s]Epoch: 6, train for the 110-th batch, train loss: 0.5383121967315674:  73%|████████   | 110/151 [00:23<00:09,  4.49it/s]evaluate for the 53-th batch, evaluate loss: 0.5040524005889893:  67%|████████████      | 52/78 [00:14<00:07,  3.53it/s]evaluate for the 53-th batch, evaluate loss: 0.5040524005889893:  68%|████████████▏     | 53/78 [00:14<00:06,  3.67it/s]Epoch: 4, train for the 17-th batch, train loss: 0.2505457103252411:  13%|█▋           | 16/119 [00:09<00:56,  1.83it/s]Epoch: 3, train for the 111-th batch, train loss: 0.5330948233604431:  75%|████████▎  | 110/146 [01:06<00:24,  1.50it/s]Epoch: 4, train for the 17-th batch, train loss: 0.2505457103252411:  14%|█▊           | 17/119 [00:09<00:59,  1.72it/s]Epoch: 3, train for the 111-th batch, train loss: 0.5330948233604431:  76%|████████▎  | 111/146 [01:06<00:23,  1.50it/s]Epoch: 6, train for the 111-th batch, train loss: 0.5350314378738403:  73%|████████   | 110/151 [00:24<00:09,  4.49it/s]Epoch: 6, train for the 111-th batch, train loss: 0.5350314378738403:  74%|████████   | 111/151 [00:24<00:08,  4.49it/s]Epoch: 2, train for the 181-th batch, train loss: 0.6401557922363281:  76%|████████▎  | 180/237 [01:47<00:33,  1.72it/s]Epoch: 2, train for the 181-th batch, train loss: 0.6401557922363281:  76%|████████▍  | 181/237 [01:47<00:32,  1.73it/s]evaluate for the 54-th batch, evaluate loss: 0.5920605659484863:  68%|████████████▏     | 53/78 [00:14<00:06,  3.67it/s]evaluate for the 54-th batch, evaluate loss: 0.5920605659484863:  69%|████████████▍     | 54/78 [00:14<00:06,  3.55it/s]Epoch: 6, train for the 112-th batch, train loss: 0.520531177520752:  74%|████████▊   | 111/151 [00:24<00:08,  4.49it/s]Epoch: 6, train for the 112-th batch, train loss: 0.520531177520752:  74%|████████▉   | 112/151 [00:24<00:08,  4.49it/s]evaluate for the 55-th batch, evaluate loss: 0.5645197629928589:  69%|████████████▍     | 54/78 [00:14<00:06,  3.55it/s]evaluate for the 55-th batch, evaluate loss: 0.5645197629928589:  71%|████████████▋     | 55/78 [00:14<00:06,  3.71it/s]Epoch: 6, train for the 113-th batch, train loss: 0.5788733959197998:  74%|████████▏  | 112/151 [00:24<00:08,  4.49it/s]Epoch: 6, train for the 113-th batch, train loss: 0.5788733959197998:  75%|████████▏  | 113/151 [00:24<00:08,  4.50it/s]Epoch: 3, train for the 112-th batch, train loss: 0.5808293223381042:  76%|████████▎  | 111/146 [01:07<00:23,  1.50it/s]Epoch: 4, train for the 18-th batch, train loss: 0.24968650937080383:  14%|█▋          | 17/119 [00:10<00:59,  1.72it/s]Epoch: 3, train for the 112-th batch, train loss: 0.5808293223381042:  77%|████████▍  | 112/146 [01:07<00:22,  1.50it/s]Epoch: 4, train for the 18-th batch, train loss: 0.24968650937080383:  15%|█▊          | 18/119 [00:10<01:01,  1.65it/s]Epoch: 2, train for the 182-th batch, train loss: 0.6372510194778442:  76%|████████▍  | 181/237 [01:47<00:32,  1.73it/s]Epoch: 2, train for the 182-th batch, train loss: 0.6372510194778442:  77%|████████▍  | 182/237 [01:47<00:31,  1.72it/s]Epoch: 6, train for the 114-th batch, train loss: 0.5043500661849976:  75%|████████▏  | 113/151 [00:24<00:08,  4.50it/s]Epoch: 6, train for the 114-th batch, train loss: 0.5043500661849976:  75%|████████▎  | 114/151 [00:24<00:08,  4.49it/s]evaluate for the 56-th batch, evaluate loss: 0.6094062924385071:  71%|████████████▋     | 55/78 [00:14<00:06,  3.71it/s]evaluate for the 56-th batch, evaluate loss: 0.6094062924385071:  72%|████████████▉     | 56/78 [00:14<00:06,  3.54it/s]Epoch: 6, train for the 115-th batch, train loss: 0.5218633413314819:  75%|████████▎  | 114/151 [00:25<00:08,  4.49it/s]Epoch: 6, train for the 115-th batch, train loss: 0.5218633413314819:  76%|████████▍  | 115/151 [00:25<00:07,  4.51it/s]evaluate for the 57-th batch, evaluate loss: 0.5827354788780212:  72%|████████████▉     | 56/78 [00:15<00:06,  3.54it/s]evaluate for the 57-th batch, evaluate loss: 0.5827354788780212:  73%|█████████████▏    | 57/78 [00:15<00:05,  3.67it/s]Epoch: 6, train for the 116-th batch, train loss: 0.5007709264755249:  76%|████████▍  | 115/151 [00:25<00:07,  4.51it/s]Epoch: 6, train for the 116-th batch, train loss: 0.5007709264755249:  77%|████████▍  | 116/151 [00:25<00:07,  4.50it/s]Epoch: 2, train for the 183-th batch, train loss: 0.6400485038757324:  77%|████████▍  | 182/237 [01:48<00:31,  1.72it/s]Epoch: 2, train for the 183-th batch, train loss: 0.6400485038757324:  77%|████████▍  | 183/237 [01:48<00:31,  1.71it/s]evaluate for the 58-th batch, evaluate loss: 0.6520971655845642:  73%|█████████████▏    | 57/78 [00:15<00:05,  3.67it/s]evaluate for the 58-th batch, evaluate loss: 0.6520971655845642:  74%|█████████████▍    | 58/78 [00:15<00:05,  3.48it/s]Epoch: 3, train for the 113-th batch, train loss: 0.4987197816371918:  77%|████████▍  | 112/146 [01:07<00:22,  1.50it/s]Epoch: 3, train for the 113-th batch, train loss: 0.4987197816371918:  77%|████████▌  | 113/146 [01:07<00:21,  1.50it/s]Epoch: 4, train for the 19-th batch, train loss: 0.20423224568367004:  15%|█▊          | 18/119 [00:10<01:01,  1.65it/s]Epoch: 4, train for the 19-th batch, train loss: 0.20423224568367004:  16%|█▉          | 19/119 [00:10<01:02,  1.60it/s]Epoch: 6, train for the 117-th batch, train loss: 0.5096353888511658:  77%|████████▍  | 116/151 [00:25<00:07,  4.50it/s]Epoch: 6, train for the 117-th batch, train loss: 0.5096353888511658:  77%|████████▌  | 117/151 [00:25<00:07,  4.51it/s]evaluate for the 59-th batch, evaluate loss: 0.5819271206855774:  74%|█████████████▍    | 58/78 [00:15<00:05,  3.48it/s]evaluate for the 59-th batch, evaluate loss: 0.5819271206855774:  76%|█████████████▌    | 59/78 [00:15<00:05,  3.57it/s]Epoch: 6, train for the 118-th batch, train loss: 0.48284682631492615:  77%|███████▋  | 117/151 [00:25<00:07,  4.51it/s]Epoch: 6, train for the 118-th batch, train loss: 0.48284682631492615:  78%|███████▊  | 118/151 [00:25<00:07,  4.52it/s]Epoch: 6, train for the 119-th batch, train loss: 0.5393533706665039:  78%|████████▌  | 118/151 [00:25<00:07,  4.52it/s]Epoch: 6, train for the 119-th batch, train loss: 0.5393533706665039:  79%|████████▋  | 119/151 [00:25<00:07,  4.51it/s]Epoch: 2, train for the 184-th batch, train loss: 0.6249008178710938:  77%|████████▍  | 183/237 [01:48<00:31,  1.71it/s]Epoch: 2, train for the 184-th batch, train loss: 0.6249008178710938:  78%|████████▌  | 184/237 [01:48<00:31,  1.70it/s]evaluate for the 60-th batch, evaluate loss: 0.6047583818435669:  76%|█████████████▌    | 59/78 [00:16<00:05,  3.57it/s]evaluate for the 60-th batch, evaluate loss: 0.6047583818435669:  77%|█████████████▊    | 60/78 [00:16<00:05,  3.41it/s]Epoch: 4, train for the 20-th batch, train loss: 0.23994769155979156:  16%|█▉          | 19/119 [00:11<01:02,  1.60it/s]Epoch: 3, train for the 114-th batch, train loss: 0.5411034822463989:  77%|████████▌  | 113/146 [01:08<00:21,  1.50it/s]Epoch: 4, train for the 20-th batch, train loss: 0.23994769155979156:  17%|██          | 20/119 [00:11<01:03,  1.57it/s]Epoch: 3, train for the 114-th batch, train loss: 0.5411034822463989:  78%|████████▌  | 114/146 [01:08<00:21,  1.50it/s]Epoch: 6, train for the 120-th batch, train loss: 0.5912202596664429:  79%|████████▋  | 119/151 [00:26<00:07,  4.51it/s]Epoch: 6, train for the 120-th batch, train loss: 0.5912202596664429:  79%|████████▋  | 120/151 [00:26<00:06,  4.49it/s]evaluate for the 61-th batch, evaluate loss: 0.5716018080711365:  77%|█████████████▊    | 60/78 [00:16<00:05,  3.41it/s]evaluate for the 61-th batch, evaluate loss: 0.5716018080711365:  78%|██████████████    | 61/78 [00:16<00:04,  3.51it/s]Epoch: 6, train for the 121-th batch, train loss: 0.5130835771560669:  79%|████████▋  | 120/151 [00:26<00:06,  4.49it/s]Epoch: 6, train for the 121-th batch, train loss: 0.5130835771560669:  80%|████████▊  | 121/151 [00:26<00:06,  4.50it/s]Epoch: 2, train for the 185-th batch, train loss: 0.598278284072876:  78%|█████████▎  | 184/237 [01:49<00:31,  1.70it/s]Epoch: 2, train for the 185-th batch, train loss: 0.598278284072876:  78%|█████████▎  | 185/237 [01:49<00:30,  1.70it/s]evaluate for the 62-th batch, evaluate loss: 0.6617388725280762:  78%|██████████████    | 61/78 [00:16<00:04,  3.51it/s]evaluate for the 62-th batch, evaluate loss: 0.6617388725280762:  79%|██████████████▎   | 62/78 [00:16<00:04,  3.38it/s]Epoch: 6, train for the 122-th batch, train loss: 0.5795429348945618:  80%|████████▊  | 121/151 [00:26<00:06,  4.50it/s]Epoch: 6, train for the 122-th batch, train loss: 0.5795429348945618:  81%|████████▉  | 122/151 [00:26<00:06,  4.50it/s]Epoch: 3, train for the 115-th batch, train loss: 0.5417851805686951:  78%|████████▌  | 114/146 [01:09<00:21,  1.50it/s]Epoch: 4, train for the 21-th batch, train loss: 0.2879404127597809:  17%|██▏          | 20/119 [00:12<01:03,  1.57it/s]Epoch: 3, train for the 115-th batch, train loss: 0.5417851805686951:  79%|████████▋  | 115/146 [01:09<00:20,  1.50it/s]Epoch: 4, train for the 21-th batch, train loss: 0.2879404127597809:  18%|██▎          | 21/119 [00:12<01:03,  1.55it/s]Epoch: 6, train for the 123-th batch, train loss: 0.5392364263534546:  81%|████████▉  | 122/151 [00:26<00:06,  4.50it/s]Epoch: 6, train for the 123-th batch, train loss: 0.5392364263534546:  81%|████████▉  | 123/151 [00:26<00:06,  4.49it/s]evaluate for the 63-th batch, evaluate loss: 0.6097846031188965:  79%|██████████████▎   | 62/78 [00:16<00:04,  3.38it/s]evaluate for the 63-th batch, evaluate loss: 0.6097846031188965:  81%|██████████████▌   | 63/78 [00:16<00:04,  3.48it/s]Epoch: 6, train for the 124-th batch, train loss: 0.5668496489524841:  81%|████████▉  | 123/151 [00:27<00:06,  4.49it/s]Epoch: 6, train for the 124-th batch, train loss: 0.5668496489524841:  82%|█████████  | 124/151 [00:27<00:06,  4.49it/s]Epoch: 2, train for the 186-th batch, train loss: 0.6233484745025635:  78%|████████▌  | 185/237 [01:50<00:30,  1.70it/s]Epoch: 2, train for the 186-th batch, train loss: 0.6233484745025635:  78%|████████▋  | 186/237 [01:50<00:29,  1.70it/s]evaluate for the 64-th batch, evaluate loss: 0.5707083940505981:  81%|██████████████▌   | 63/78 [00:17<00:04,  3.48it/s]evaluate for the 64-th batch, evaluate loss: 0.5707083940505981:  82%|██████████████▊   | 64/78 [00:17<00:04,  3.39it/s]Epoch: 6, train for the 125-th batch, train loss: 0.5371420979499817:  82%|█████████  | 124/151 [00:27<00:06,  4.49it/s]Epoch: 6, train for the 125-th batch, train loss: 0.5371420979499817:  83%|█████████  | 125/151 [00:27<00:05,  4.49it/s]evaluate for the 65-th batch, evaluate loss: 0.5589181780815125:  82%|██████████████▊   | 64/78 [00:17<00:04,  3.39it/s]evaluate for the 65-th batch, evaluate loss: 0.5589181780815125:  83%|███████████████   | 65/78 [00:17<00:03,  3.48it/s]Epoch: 4, train for the 22-th batch, train loss: 0.2820846140384674:  18%|██▎          | 21/119 [00:12<01:03,  1.55it/s]Epoch: 4, train for the 22-th batch, train loss: 0.2820846140384674:  18%|██▍          | 22/119 [00:12<01:03,  1.54it/s]Epoch: 3, train for the 116-th batch, train loss: 0.5241100192070007:  79%|████████▋  | 115/146 [01:09<00:20,  1.50it/s]Epoch: 3, train for the 116-th batch, train loss: 0.5241100192070007:  79%|████████▋  | 116/146 [01:09<00:19,  1.51it/s]Epoch: 6, train for the 126-th batch, train loss: 0.5356104969978333:  83%|█████████  | 125/151 [00:27<00:05,  4.49it/s]Epoch: 6, train for the 126-th batch, train loss: 0.5356104969978333:  83%|█████████▏ | 126/151 [00:27<00:05,  4.49it/s]Epoch: 6, train for the 127-th batch, train loss: 0.5483993291854858:  83%|█████████▏ | 126/151 [00:27<00:05,  4.49it/s]Epoch: 6, train for the 127-th batch, train loss: 0.5483993291854858:  84%|█████████▎ | 127/151 [00:27<00:05,  4.50it/s]evaluate for the 66-th batch, evaluate loss: 0.6054717898368835:  83%|███████████████   | 65/78 [00:17<00:03,  3.48it/s]evaluate for the 66-th batch, evaluate loss: 0.6054717898368835:  85%|███████████████▏  | 66/78 [00:17<00:03,  3.44it/s]Epoch: 2, train for the 187-th batch, train loss: 0.6399497389793396:  78%|████████▋  | 186/237 [01:50<00:29,  1.70it/s]Epoch: 2, train for the 187-th batch, train loss: 0.6399497389793396:  79%|████████▋  | 187/237 [01:50<00:29,  1.71it/s]Epoch: 6, train for the 128-th batch, train loss: 0.5751203894615173:  84%|█████████▎ | 127/151 [00:27<00:05,  4.50it/s]Epoch: 6, train for the 128-th batch, train loss: 0.5751203894615173:  85%|█████████▎ | 128/151 [00:27<00:05,  4.51it/s]evaluate for the 67-th batch, evaluate loss: 0.5918492674827576:  85%|███████████████▏  | 66/78 [00:18<00:03,  3.44it/s]evaluate for the 67-th batch, evaluate loss: 0.5918492674827576:  86%|███████████████▍  | 67/78 [00:18<00:03,  3.55it/s]Epoch: 4, train for the 23-th batch, train loss: 0.28315475583076477:  18%|██▏         | 22/119 [00:13<01:03,  1.54it/s]Epoch: 4, train for the 23-th batch, train loss: 0.28315475583076477:  19%|██▎         | 23/119 [00:13<01:02,  1.54it/s]Epoch: 3, train for the 117-th batch, train loss: 0.5811639428138733:  79%|████████▋  | 116/146 [01:10<00:19,  1.51it/s]Epoch: 3, train for the 117-th batch, train loss: 0.5811639428138733:  80%|████████▊  | 117/146 [01:10<00:19,  1.51it/s]Epoch: 6, train for the 129-th batch, train loss: 0.5736715793609619:  85%|█████████▎ | 128/151 [00:28<00:05,  4.51it/s]Epoch: 6, train for the 129-th batch, train loss: 0.5736715793609619:  85%|█████████▍ | 129/151 [00:28<00:04,  4.50it/s]evaluate for the 68-th batch, evaluate loss: 0.5681066513061523:  86%|███████████████▍  | 67/78 [00:18<00:03,  3.55it/s]evaluate for the 68-th batch, evaluate loss: 0.5681066513061523:  87%|███████████████▋  | 68/78 [00:18<00:02,  3.49it/s]Epoch: 2, train for the 188-th batch, train loss: 0.6396947503089905:  79%|████████▋  | 187/237 [01:51<00:29,  1.71it/s]Epoch: 2, train for the 188-th batch, train loss: 0.6396947503089905:  79%|████████▋  | 188/237 [01:51<00:28,  1.72it/s]Epoch: 6, train for the 130-th batch, train loss: 0.5240451097488403:  85%|█████████▍ | 129/151 [00:28<00:04,  4.50it/s]Epoch: 6, train for the 130-th batch, train loss: 0.5240451097488403:  86%|█████████▍ | 130/151 [00:28<00:04,  4.50it/s]evaluate for the 69-th batch, evaluate loss: 0.5111441016197205:  87%|███████████████▋  | 68/78 [00:18<00:02,  3.49it/s]evaluate for the 69-th batch, evaluate loss: 0.5111441016197205:  88%|███████████████▉  | 69/78 [00:18<00:02,  3.61it/s]Epoch: 6, train for the 131-th batch, train loss: 0.5314313173294067:  86%|█████████▍ | 130/151 [00:28<00:04,  4.50it/s]Epoch: 6, train for the 131-th batch, train loss: 0.5314313173294067:  87%|█████████▌ | 131/151 [00:28<00:04,  4.50it/s]Epoch: 4, train for the 24-th batch, train loss: 0.210474893450737:  19%|██▋           | 23/119 [00:14<01:02,  1.54it/s]Epoch: 4, train for the 24-th batch, train loss: 0.210474893450737:  20%|██▊           | 24/119 [00:14<01:01,  1.54it/s]Epoch: 3, train for the 118-th batch, train loss: 0.4882464110851288:  80%|████████▊  | 117/146 [01:11<00:19,  1.51it/s]Epoch: 3, train for the 118-th batch, train loss: 0.4882464110851288:  81%|████████▉  | 118/146 [01:11<00:18,  1.52it/s]Epoch: 6, train for the 132-th batch, train loss: 0.521754801273346:  87%|██████████▍ | 131/151 [00:28<00:04,  4.50it/s]Epoch: 6, train for the 132-th batch, train loss: 0.521754801273346:  87%|██████████▍ | 132/151 [00:28<00:04,  4.50it/s]evaluate for the 70-th batch, evaluate loss: 0.5246274471282959:  88%|███████████████▉  | 69/78 [00:18<00:02,  3.61it/s]evaluate for the 70-th batch, evaluate loss: 0.5246274471282959:  90%|████████████████▏ | 70/78 [00:18<00:02,  3.54it/s]Epoch: 2, train for the 189-th batch, train loss: 0.6376453042030334:  79%|████████▋  | 188/237 [01:51<00:28,  1.72it/s]Epoch: 2, train for the 189-th batch, train loss: 0.6376453042030334:  80%|████████▊  | 189/237 [01:51<00:27,  1.73it/s]Epoch: 6, train for the 133-th batch, train loss: 0.5462561845779419:  87%|█████████▌ | 132/151 [00:29<00:04,  4.50it/s]Epoch: 6, train for the 133-th batch, train loss: 0.5462561845779419:  88%|█████████▋ | 133/151 [00:29<00:04,  4.50it/s]evaluate for the 71-th batch, evaluate loss: 0.5055340528488159:  90%|████████████████▏ | 70/78 [00:19<00:02,  3.54it/s]evaluate for the 71-th batch, evaluate loss: 0.5055340528488159:  91%|████████████████▍ | 71/78 [00:19<00:01,  3.64it/s]Epoch: 6, train for the 134-th batch, train loss: 0.5245645642280579:  88%|█████████▋ | 133/151 [00:29<00:04,  4.50it/s]Epoch: 6, train for the 134-th batch, train loss: 0.5245645642280579:  89%|█████████▊ | 134/151 [00:29<00:03,  4.49it/s]evaluate for the 72-th batch, evaluate loss: 0.5961700081825256:  91%|████████████████▍ | 71/78 [00:19<00:01,  3.64it/s]evaluate for the 72-th batch, evaluate loss: 0.5961700081825256:  92%|████████████████▌ | 72/78 [00:19<00:01,  3.57it/s]Epoch: 3, train for the 119-th batch, train loss: 0.5521650314331055:  81%|████████▉  | 118/146 [01:11<00:18,  1.52it/s]Epoch: 3, train for the 119-th batch, train loss: 0.5521650314331055:  82%|████████▉  | 119/146 [01:11<00:17,  1.52it/s]Epoch: 4, train for the 25-th batch, train loss: 0.25223302841186523:  20%|██▍         | 24/119 [00:14<01:01,  1.54it/s]Epoch: 4, train for the 25-th batch, train loss: 0.25223302841186523:  21%|██▌         | 25/119 [00:14<01:01,  1.53it/s]Epoch: 2, train for the 190-th batch, train loss: 0.648571789264679:  80%|█████████▌  | 189/237 [01:52<00:27,  1.73it/s]Epoch: 2, train for the 190-th batch, train loss: 0.648571789264679:  80%|█████████▌  | 190/237 [01:52<00:27,  1.74it/s]Epoch: 6, train for the 135-th batch, train loss: 0.552817702293396:  89%|██████████▋ | 134/151 [00:29<00:03,  4.49it/s]Epoch: 6, train for the 135-th batch, train loss: 0.552817702293396:  89%|██████████▋ | 135/151 [00:29<00:03,  4.48it/s]evaluate for the 73-th batch, evaluate loss: 0.49195295572280884:  92%|███████████████▋ | 72/78 [00:19<00:01,  3.57it/s]evaluate for the 73-th batch, evaluate loss: 0.49195295572280884:  94%|███████████████▉ | 73/78 [00:19<00:01,  3.66it/s]Epoch: 6, train for the 136-th batch, train loss: 0.5463452935218811:  89%|█████████▊ | 135/151 [00:29<00:03,  4.48it/s]Epoch: 6, train for the 136-th batch, train loss: 0.5463452935218811:  90%|█████████▉ | 136/151 [00:29<00:03,  4.48it/s]Epoch: 6, train for the 137-th batch, train loss: 0.5854135155677795:  90%|█████████▉ | 136/151 [00:29<00:03,  4.48it/s]Epoch: 6, train for the 137-th batch, train loss: 0.5854135155677795:  91%|█████████▉ | 137/151 [00:29<00:03,  4.49it/s]evaluate for the 74-th batch, evaluate loss: 0.5585253834724426:  94%|████████████████▊ | 73/78 [00:19<00:01,  3.66it/s]evaluate for the 74-th batch, evaluate loss: 0.5585253834724426:  95%|█████████████████ | 74/78 [00:19<00:01,  3.60it/s]Epoch: 2, train for the 191-th batch, train loss: 0.6331019997596741:  80%|████████▊  | 190/237 [01:52<00:27,  1.74it/s]Epoch: 2, train for the 191-th batch, train loss: 0.6331019997596741:  81%|████████▊  | 191/237 [01:52<00:26,  1.74it/s]Epoch: 3, train for the 120-th batch, train loss: 0.5275528430938721:  82%|████████▉  | 119/146 [01:12<00:17,  1.52it/s]Epoch: 3, train for the 120-th batch, train loss: 0.5275528430938721:  82%|█████████  | 120/146 [01:12<00:17,  1.52it/s]Epoch: 4, train for the 26-th batch, train loss: 0.24112579226493835:  21%|██▌         | 25/119 [00:15<01:01,  1.53it/s]Epoch: 4, train for the 26-th batch, train loss: 0.24112579226493835:  22%|██▌         | 26/119 [00:15<01:00,  1.53it/s]Epoch: 6, train for the 138-th batch, train loss: 0.5934752225875854:  91%|█████████▉ | 137/151 [00:30<00:03,  4.49it/s]Epoch: 6, train for the 138-th batch, train loss: 0.5934752225875854:  91%|██████████ | 138/151 [00:30<00:02,  4.49it/s]evaluate for the 75-th batch, evaluate loss: 0.5482022166252136:  95%|█████████████████ | 74/78 [00:20<00:01,  3.60it/s]evaluate for the 75-th batch, evaluate loss: 0.5482022166252136:  96%|█████████████████▎| 75/78 [00:20<00:00,  3.66it/s]Epoch: 6, train for the 139-th batch, train loss: 0.5923972725868225:  91%|██████████ | 138/151 [00:30<00:02,  4.49it/s]Epoch: 6, train for the 139-th batch, train loss: 0.5923972725868225:  92%|██████████▏| 139/151 [00:30<00:02,  4.49it/s]evaluate for the 76-th batch, evaluate loss: 0.5955978035926819:  96%|█████████████████▎| 75/78 [00:20<00:00,  3.66it/s]evaluate for the 76-th batch, evaluate loss: 0.5955978035926819:  97%|█████████████████▌| 76/78 [00:20<00:00,  3.61it/s]Epoch: 6, train for the 140-th batch, train loss: 0.5224190354347229:  92%|██████████▏| 139/151 [00:30<00:02,  4.49it/s]Epoch: 6, train for the 140-th batch, train loss: 0.5224190354347229:  93%|██████████▏| 140/151 [00:30<00:02,  4.49it/s]Epoch: 2, train for the 192-th batch, train loss: 0.6407462358474731:  81%|████████▊  | 191/237 [01:53<00:26,  1.74it/s]Epoch: 2, train for the 192-th batch, train loss: 0.6407462358474731:  81%|████████▉  | 192/237 [01:53<00:26,  1.72it/s]Epoch: 3, train for the 121-th batch, train loss: 0.5466644763946533:  82%|█████████  | 120/146 [01:13<00:17,  1.52it/s]Epoch: 3, train for the 121-th batch, train loss: 0.5466644763946533:  83%|█████████  | 121/146 [01:13<00:16,  1.52it/s]Epoch: 4, train for the 27-th batch, train loss: 0.2424805760383606:  22%|██▊          | 26/119 [00:16<01:00,  1.53it/s]Epoch: 4, train for the 27-th batch, train loss: 0.2424805760383606:  23%|██▉          | 27/119 [00:16<01:00,  1.53it/s]evaluate for the 77-th batch, evaluate loss: 0.5701210498809814:  97%|█████████████████▌| 76/78 [00:20<00:00,  3.61it/s]evaluate for the 77-th batch, evaluate loss: 0.5701210498809814:  99%|█████████████████▊| 77/78 [00:20<00:00,  3.58it/s]Epoch: 6, train for the 141-th batch, train loss: 0.5526809096336365:  93%|██████████▏| 140/151 [00:30<00:02,  4.49it/s]Epoch: 6, train for the 141-th batch, train loss: 0.5526809096336365:  93%|██████████▎| 141/151 [00:30<00:02,  4.49it/s]evaluate for the 78-th batch, evaluate loss: 0.6705151200294495:  99%|█████████████████▊| 77/78 [00:20<00:00,  3.58it/s]evaluate for the 78-th batch, evaluate loss: 0.6705151200294495: 100%|██████████████████| 78/78 [00:20<00:00,  4.03it/s]evaluate for the 78-th batch, evaluate loss: 0.6705151200294495: 100%|██████████████████| 78/78 [00:20<00:00,  3.72it/s]
INFO:root:Epoch: 1, learning rate: 0.0001, train loss: 0.4745
INFO:root:train average_precision, 0.8680
INFO:root:train roc_auc, 0.8524
INFO:root:validate loss: 0.4568
INFO:root:validate average_precision, 0.8812
INFO:root:validate roc_auc, 0.8582
INFO:root:new node validate loss: 0.5877
INFO:root:new node validate first_1_average_precision, 0.8027
INFO:root:new node validate first_1_roc_auc, 0.7658
INFO:root:new node validate first_3_average_precision, 0.7831
INFO:root:new node validate first_3_roc_auc, 0.7392
INFO:root:new node validate first_10_average_precision, 0.8011
INFO:root:new node validate first_10_roc_auc, 0.7620
INFO:root:new node validate average_precision, 0.8121
INFO:root:new node validate roc_auc, 0.7708
INFO:root:save model ./saved_models/DyGFormer/ia-slashdot-reply-dir/DyGFormer_seed0_dygformer-ia-slashdot-reply-dir-old/DyGFormer_seed0_dygformer-ia-slashdot-reply-dir-old.pkl
Epoch: 2, train for the 193-th batch, train loss: 0.6352604627609253:  81%|████████▉  | 192/237 [01:53<00:26,  1.72it/s]Epoch: 2, train for the 193-th batch, train loss: 0.6352604627609253:  81%|████████▉  | 193/237 [01:53<00:23,  1.89it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/383 [00:00<?, ?it/s]Epoch: 6, train for the 142-th batch, train loss: 0.570364773273468:  93%|███████████▏| 141/151 [00:31<00:02,  4.49it/s]Epoch: 6, train for the 142-th batch, train loss: 0.570364773273468:  94%|███████████▎| 142/151 [00:31<00:02,  4.47it/s]Epoch: 6, train for the 143-th batch, train loss: 0.47714391350746155:  94%|█████████▍| 142/151 [00:31<00:02,  4.47it/s]Epoch: 6, train for the 143-th batch, train loss: 0.47714391350746155:  95%|█████████▍| 143/151 [00:31<00:01,  4.48it/s]Epoch: 4, train for the 28-th batch, train loss: 0.2685908079147339:  23%|██▉          | 27/119 [00:16<01:00,  1.53it/s]Epoch: 4, train for the 28-th batch, train loss: 0.2685908079147339:  24%|███          | 28/119 [00:16<00:59,  1.52it/s]Epoch: 3, train for the 122-th batch, train loss: 0.5556237697601318:  83%|█████████  | 121/146 [01:13<00:16,  1.52it/s]Epoch: 3, train for the 122-th batch, train loss: 0.5556237697601318:  84%|█████████▏ | 122/146 [01:13<00:15,  1.52it/s]Epoch: 6, train for the 144-th batch, train loss: 0.5085639357566833:  95%|██████████▍| 143/151 [00:31<00:01,  4.48it/s]Epoch: 6, train for the 144-th batch, train loss: 0.5085639357566833:  95%|██████████▍| 144/151 [00:31<00:01,  4.47it/s]Epoch: 2, train for the 1-th batch, train loss: 1.899232029914856:   0%|                        | 0/383 [00:00<?, ?it/s]Epoch: 2, train for the 1-th batch, train loss: 1.899232029914856:   0%|                | 1/383 [00:00<03:04,  2.07it/s]Epoch: 2, train for the 194-th batch, train loss: 0.6456178426742554:  81%|████████▉  | 193/237 [01:54<00:23,  1.89it/s]Epoch: 2, train for the 194-th batch, train loss: 0.6456178426742554:  82%|█████████  | 194/237 [01:54<00:23,  1.83it/s]Epoch: 6, train for the 145-th batch, train loss: 0.5100725293159485:  95%|██████████▍| 144/151 [00:31<00:01,  4.47it/s]Epoch: 6, train for the 145-th batch, train loss: 0.5100725293159485:  96%|██████████▌| 145/151 [00:31<00:01,  4.37it/s]Epoch: 2, train for the 2-th batch, train loss: 0.8827351927757263:   0%|               | 1/383 [00:00<03:04,  2.07it/s]Epoch: 2, train for the 2-th batch, train loss: 0.8827351927757263:   1%|               | 2/383 [00:00<02:43,  2.33it/s]Epoch: 6, train for the 146-th batch, train loss: 0.5331224799156189:  96%|██████████▌| 145/151 [00:31<00:01,  4.37it/s]Epoch: 6, train for the 146-th batch, train loss: 0.5331224799156189:  97%|██████████▋| 146/151 [00:31<00:01,  4.41it/s]Epoch: 4, train for the 29-th batch, train loss: 0.2396988421678543:  24%|███          | 28/119 [00:17<00:59,  1.52it/s]Epoch: 4, train for the 29-th batch, train loss: 0.2396988421678543:  24%|███▏         | 29/119 [00:17<00:59,  1.52it/s]Epoch: 3, train for the 123-th batch, train loss: 0.5571525692939758:  84%|█████████▏ | 122/146 [01:14<00:15,  1.52it/s]Epoch: 3, train for the 123-th batch, train loss: 0.5571525692939758:  84%|█████████▎ | 123/146 [01:14<00:15,  1.52it/s]Epoch: 6, train for the 147-th batch, train loss: 0.5447327494621277:  97%|██████████▋| 146/151 [00:32<00:01,  4.41it/s]Epoch: 6, train for the 147-th batch, train loss: 0.5447327494621277:  97%|██████████▋| 147/151 [00:32<00:00,  4.44it/s]Epoch: 2, train for the 195-th batch, train loss: 0.6286730170249939:  82%|█████████  | 194/237 [01:55<00:23,  1.83it/s]Epoch: 2, train for the 195-th batch, train loss: 0.6286730170249939:  82%|█████████  | 195/237 [01:55<00:24,  1.71it/s]Epoch: 2, train for the 3-th batch, train loss: 0.5445688366889954:   1%|               | 2/383 [00:01<02:43,  2.33it/s]Epoch: 2, train for the 3-th batch, train loss: 0.5445688366889954:   1%|               | 3/383 [00:01<02:43,  2.33it/s]Epoch: 6, train for the 148-th batch, train loss: 0.5567644834518433:  97%|██████████▋| 147/151 [00:32<00:00,  4.44it/s]Epoch: 6, train for the 148-th batch, train loss: 0.5567644834518433:  98%|██████████▊| 148/151 [00:32<00:00,  4.45it/s]Epoch: 6, train for the 149-th batch, train loss: 0.5147538781166077:  98%|██████████▊| 148/151 [00:32<00:00,  4.45it/s]Epoch: 6, train for the 149-th batch, train loss: 0.5147538781166077:  99%|██████████▊| 149/151 [00:32<00:00,  4.46it/s]Epoch: 4, train for the 30-th batch, train loss: 0.22252143919467926:  24%|██▉         | 29/119 [00:18<00:59,  1.52it/s]Epoch: 4, train for the 30-th batch, train loss: 0.22252143919467926:  25%|███         | 30/119 [00:18<00:58,  1.52it/s]Epoch: 3, train for the 124-th batch, train loss: 0.5762265920639038:  84%|█████████▎ | 123/146 [01:15<00:15,  1.52it/s]Epoch: 3, train for the 124-th batch, train loss: 0.5762265920639038:  85%|█████████▎ | 124/146 [01:15<00:14,  1.52it/s]Epoch: 2, train for the 4-th batch, train loss: 0.5736916661262512:   1%|               | 3/383 [00:01<02:43,  2.33it/s]Epoch: 6, train for the 150-th batch, train loss: 0.5223680734634399:  99%|██████████▊| 149/151 [00:32<00:00,  4.46it/s]Epoch: 2, train for the 4-th batch, train loss: 0.5736916661262512:   1%|▏              | 4/383 [00:01<02:54,  2.18it/s]Epoch: 6, train for the 150-th batch, train loss: 0.5223680734634399:  99%|██████████▉| 150/151 [00:32<00:00,  4.47it/s]Epoch: 2, train for the 196-th batch, train loss: 0.6470005512237549:  82%|█████████  | 195/237 [01:55<00:24,  1.71it/s]Epoch: 2, train for the 196-th batch, train loss: 0.6470005512237549:  83%|█████████  | 196/237 [01:55<00:24,  1.69it/s]Epoch: 6, train for the 151-th batch, train loss: 0.6149094104766846:  99%|██████████▉| 150/151 [00:33<00:00,  4.47it/s]Epoch: 6, train for the 151-th batch, train loss: 0.6149094104766846: 100%|███████████| 151/151 [00:33<00:00,  4.94it/s]Epoch: 6, train for the 151-th batch, train loss: 0.6149094104766846: 100%|███████████| 151/151 [00:33<00:00,  4.57it/s]
  0%|                                                                                            | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.4917040169239044:   0%|                            | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.4917040169239044:   2%|▍                   | 1/46 [00:00<00:04,  9.63it/s]evaluate for the 2-th batch, evaluate loss: 0.5023396015167236:   2%|▍                   | 1/46 [00:00<00:04,  9.63it/s]evaluate for the 2-th batch, evaluate loss: 0.5023396015167236:   4%|▊                   | 2/46 [00:00<00:04,  9.58it/s]evaluate for the 3-th batch, evaluate loss: 0.4782521426677704:   4%|▊                   | 2/46 [00:00<00:04,  9.58it/s]evaluate for the 3-th batch, evaluate loss: 0.4782521426677704:   7%|█▎                  | 3/46 [00:00<00:04,  9.61it/s]Epoch: 2, train for the 5-th batch, train loss: 0.490308940410614:   1%|▏               | 4/383 [00:02<02:54,  2.18it/s]Epoch: 2, train for the 5-th batch, train loss: 0.490308940410614:   1%|▏               | 5/383 [00:02<02:56,  2.14it/s]Epoch: 4, train for the 31-th batch, train loss: 0.2229972928762436:  25%|███▎         | 30/119 [00:18<00:58,  1.52it/s]Epoch: 3, train for the 125-th batch, train loss: 0.5163113474845886:  85%|█████████▎ | 124/146 [01:15<00:14,  1.52it/s]Epoch: 4, train for the 31-th batch, train loss: 0.2229972928762436:  26%|███▍         | 31/119 [00:18<00:57,  1.52it/s]Epoch: 3, train for the 125-th batch, train loss: 0.5163113474845886:  86%|█████████▍ | 125/146 [01:15<00:13,  1.52it/s]evaluate for the 4-th batch, evaluate loss: 0.5110099911689758:   7%|█▎                  | 3/46 [00:00<00:04,  9.61it/s]evaluate for the 4-th batch, evaluate loss: 0.5110099911689758:   9%|█▋                  | 4/46 [00:00<00:04,  9.60it/s]Epoch: 2, train for the 197-th batch, train loss: 0.6233645677566528:  83%|█████████  | 196/237 [01:56<00:24,  1.69it/s]Epoch: 2, train for the 197-th batch, train loss: 0.6233645677566528:  83%|█████████▏ | 197/237 [01:56<00:23,  1.69it/s]evaluate for the 5-th batch, evaluate loss: 0.4739817678928375:   9%|█▋                  | 4/46 [00:00<00:04,  9.60it/s]evaluate for the 5-th batch, evaluate loss: 0.4739817678928375:  11%|██▏                 | 5/46 [00:00<00:04,  9.61it/s]evaluate for the 6-th batch, evaluate loss: 0.5494403839111328:  11%|██▏                 | 5/46 [00:00<00:04,  9.61it/s]evaluate for the 6-th batch, evaluate loss: 0.5494403839111328:  13%|██▌                 | 6/46 [00:00<00:04,  9.60it/s]evaluate for the 7-th batch, evaluate loss: 0.49293744564056396:  13%|██▍                | 6/46 [00:00<00:04,  9.60it/s]evaluate for the 7-th batch, evaluate loss: 0.49293744564056396:  15%|██▉                | 7/46 [00:00<00:04,  9.63it/s]evaluate for the 8-th batch, evaluate loss: 0.5589777827262878:  15%|███                 | 7/46 [00:00<00:04,  9.63it/s]evaluate for the 8-th batch, evaluate loss: 0.5589777827262878:  17%|███▍                | 8/46 [00:00<00:03,  9.62it/s]evaluate for the 9-th batch, evaluate loss: 0.52278733253479:  17%|███▊                  | 8/46 [00:00<00:03,  9.62it/s]evaluate for the 9-th batch, evaluate loss: 0.52278733253479:  20%|████▎                 | 9/46 [00:00<00:03,  9.64it/s]Epoch: 2, train for the 6-th batch, train loss: 0.5761803984642029:   1%|▏              | 5/383 [00:02<02:56,  2.14it/s]Epoch: 2, train for the 6-th batch, train loss: 0.5761803984642029:   2%|▏              | 6/383 [00:02<03:14,  1.94it/s]Epoch: 4, train for the 32-th batch, train loss: 0.20370852947235107:  26%|███▏        | 31/119 [00:19<00:57,  1.52it/s]Epoch: 3, train for the 126-th batch, train loss: 0.55837482213974:  86%|███████████▏ | 125/146 [01:16<00:13,  1.52it/s]Epoch: 4, train for the 32-th batch, train loss: 0.20370852947235107:  27%|███▏        | 32/119 [00:19<00:57,  1.52it/s]Epoch: 3, train for the 126-th batch, train loss: 0.55837482213974:  86%|███████████▏ | 126/146 [01:16<00:13,  1.51it/s]evaluate for the 10-th batch, evaluate loss: 0.5374880433082581:  20%|███▋               | 9/46 [00:01<00:03,  9.64it/s]evaluate for the 10-th batch, evaluate loss: 0.5374880433082581:  22%|███▉              | 10/46 [00:01<00:03,  9.65it/s]Epoch: 2, train for the 198-th batch, train loss: 0.6316990852355957:  83%|█████████▏ | 197/237 [01:57<00:23,  1.69it/s]Epoch: 2, train for the 198-th batch, train loss: 0.6316990852355957:  84%|█████████▏ | 198/237 [01:57<00:23,  1.67it/s]evaluate for the 11-th batch, evaluate loss: 0.5228952169418335:  22%|███▉              | 10/46 [00:01<00:03,  9.65it/s]evaluate for the 11-th batch, evaluate loss: 0.5228952169418335:  24%|████▎             | 11/46 [00:01<00:03,  9.65it/s]evaluate for the 12-th batch, evaluate loss: 0.4740166664123535:  24%|████▎             | 11/46 [00:01<00:03,  9.65it/s]evaluate for the 12-th batch, evaluate loss: 0.4740166664123535:  26%|████▋             | 12/46 [00:01<00:03,  9.64it/s]evaluate for the 13-th batch, evaluate loss: 0.49846351146698:  26%|█████▏              | 12/46 [00:01<00:03,  9.64it/s]evaluate for the 13-th batch, evaluate loss: 0.49846351146698:  28%|█████▋              | 13/46 [00:01<00:03,  9.65it/s]Epoch: 2, train for the 7-th batch, train loss: 0.4722205698490143:   2%|▏              | 6/383 [00:03<03:14,  1.94it/s]Epoch: 2, train for the 7-th batch, train loss: 0.4722205698490143:   2%|▎              | 7/383 [00:03<03:10,  1.97it/s]evaluate for the 14-th batch, evaluate loss: 0.5927610397338867:  28%|█████             | 13/46 [00:01<00:03,  9.65it/s]evaluate for the 14-th batch, evaluate loss: 0.5927610397338867:  30%|█████▍            | 14/46 [00:01<00:03,  9.66it/s]evaluate for the 15-th batch, evaluate loss: 0.5430821776390076:  30%|█████▍            | 14/46 [00:01<00:03,  9.66it/s]evaluate for the 15-th batch, evaluate loss: 0.5430821776390076:  33%|█████▊            | 15/46 [00:01<00:03,  9.65it/s]evaluate for the 16-th batch, evaluate loss: 0.5744473338127136:  33%|█████▊            | 15/46 [00:01<00:03,  9.65it/s]evaluate for the 16-th batch, evaluate loss: 0.5744473338127136:  35%|██████▎           | 16/46 [00:01<00:03,  9.66it/s]Epoch: 4, train for the 33-th batch, train loss: 0.2267046868801117:  27%|███▍         | 32/119 [00:20<00:57,  1.52it/s]Epoch: 4, train for the 33-th batch, train loss: 0.2267046868801117:  28%|███▌         | 33/119 [00:20<00:56,  1.51it/s]Epoch: 3, train for the 127-th batch, train loss: 0.535878598690033:  86%|██████████▎ | 126/146 [01:17<00:13,  1.51it/s]Epoch: 3, train for the 127-th batch, train loss: 0.535878598690033:  87%|██████████▍ | 127/146 [01:17<00:12,  1.51it/s]Epoch: 2, train for the 199-th batch, train loss: 0.6471422910690308:  84%|█████████▏ | 198/237 [01:57<00:23,  1.67it/s]Epoch: 2, train for the 199-th batch, train loss: 0.6471422910690308:  84%|█████████▏ | 199/237 [01:57<00:22,  1.67it/s]evaluate for the 17-th batch, evaluate loss: 0.45409414172172546:  35%|█████▉           | 16/46 [00:01<00:03,  9.66it/s]evaluate for the 17-th batch, evaluate loss: 0.45409414172172546:  37%|██████▎          | 17/46 [00:01<00:03,  9.64it/s]evaluate for the 18-th batch, evaluate loss: 0.5018938183784485:  37%|██████▋           | 17/46 [00:01<00:03,  9.64it/s]evaluate for the 18-th batch, evaluate loss: 0.5018938183784485:  39%|███████           | 18/46 [00:01<00:02,  9.63it/s]Epoch: 2, train for the 8-th batch, train loss: 0.5326822400093079:   2%|▎              | 7/383 [00:03<03:10,  1.97it/s]Epoch: 2, train for the 8-th batch, train loss: 0.5326822400093079:   2%|▎              | 8/383 [00:03<03:11,  1.96it/s]evaluate for the 19-th batch, evaluate loss: 0.5208260416984558:  39%|███████           | 18/46 [00:01<00:02,  9.63it/s]evaluate for the 19-th batch, evaluate loss: 0.5208260416984558:  41%|███████▍          | 19/46 [00:01<00:02,  9.62it/s]evaluate for the 20-th batch, evaluate loss: 0.5349189043045044:  41%|███████▍          | 19/46 [00:02<00:02,  9.62it/s]evaluate for the 20-th batch, evaluate loss: 0.5349189043045044:  43%|███████▊          | 20/46 [00:02<00:02,  9.61it/s]evaluate for the 21-th batch, evaluate loss: 0.5248581171035767:  43%|███████▊          | 20/46 [00:02<00:02,  9.61it/s]evaluate for the 21-th batch, evaluate loss: 0.5248581171035767:  46%|████████▏         | 21/46 [00:02<00:02,  9.60it/s]Epoch: 2, train for the 200-th batch, train loss: 0.6140346527099609:  84%|█████████▏ | 199/237 [01:58<00:22,  1.67it/s]Epoch: 2, train for the 200-th batch, train loss: 0.6140346527099609:  84%|█████████▎ | 200/237 [01:58<00:21,  1.71it/s]evaluate for the 22-th batch, evaluate loss: 0.5188192129135132:  46%|████████▏         | 21/46 [00:02<00:02,  9.60it/s]evaluate for the 22-th batch, evaluate loss: 0.5188192129135132:  48%|████████▌         | 22/46 [00:02<00:02,  9.62it/s]Epoch: 4, train for the 34-th batch, train loss: 0.2277849316596985:  28%|███▌         | 33/119 [00:20<00:56,  1.51it/s]Epoch: 3, train for the 128-th batch, train loss: 0.5474372506141663:  87%|█████████▌ | 127/146 [01:17<00:12,  1.51it/s]Epoch: 4, train for the 34-th batch, train loss: 0.2277849316596985:  29%|███▋         | 34/119 [00:20<00:56,  1.51it/s]Epoch: 3, train for the 128-th batch, train loss: 0.5474372506141663:  88%|█████████▋ | 128/146 [01:17<00:11,  1.51it/s]evaluate for the 23-th batch, evaluate loss: 0.46977731585502625:  48%|████████▏        | 22/46 [00:02<00:02,  9.62it/s]evaluate for the 23-th batch, evaluate loss: 0.46977731585502625:  50%|████████▌        | 23/46 [00:02<00:02,  9.62it/s]evaluate for the 24-th batch, evaluate loss: 0.4821282923221588:  50%|█████████         | 23/46 [00:02<00:02,  9.62it/s]evaluate for the 24-th batch, evaluate loss: 0.4821282923221588:  52%|█████████▍        | 24/46 [00:02<00:02,  9.62it/s]Epoch: 2, train for the 9-th batch, train loss: 0.5369641780853271:   2%|▎              | 8/383 [00:04<03:11,  1.96it/s]Epoch: 2, train for the 9-th batch, train loss: 0.5369641780853271:   2%|▎              | 9/383 [00:04<03:21,  1.86it/s]evaluate for the 25-th batch, evaluate loss: 0.5357421040534973:  52%|█████████▍        | 24/46 [00:02<00:02,  9.62it/s]evaluate for the 25-th batch, evaluate loss: 0.5357421040534973:  54%|█████████▊        | 25/46 [00:02<00:02,  9.52it/s]evaluate for the 26-th batch, evaluate loss: 0.5568054914474487:  54%|█████████▊        | 25/46 [00:02<00:02,  9.52it/s]evaluate for the 26-th batch, evaluate loss: 0.5568054914474487:  57%|██████████▏       | 26/46 [00:02<00:02,  9.55it/s]evaluate for the 27-th batch, evaluate loss: 0.4957372844219208:  57%|██████████▏       | 26/46 [00:02<00:02,  9.55it/s]evaluate for the 27-th batch, evaluate loss: 0.4957372844219208:  59%|██████████▌       | 27/46 [00:02<00:01,  9.58it/s]Epoch: 2, train for the 201-th batch, train loss: 0.6295921802520752:  84%|█████████▎ | 200/237 [01:58<00:21,  1.71it/s]Epoch: 2, train for the 201-th batch, train loss: 0.6295921802520752:  85%|█████████▎ | 201/237 [01:58<00:21,  1.70it/s]Epoch: 3, train for the 129-th batch, train loss: 0.5605897307395935:  88%|█████████▋ | 128/146 [01:18<00:11,  1.51it/s]Epoch: 3, train for the 129-th batch, train loss: 0.5605897307395935:  88%|█████████▋ | 129/146 [01:18<00:10,  1.63it/s]evaluate for the 28-th batch, evaluate loss: 0.52125084400177:  59%|███████████▋        | 27/46 [00:02<00:01,  9.58it/s]evaluate for the 28-th batch, evaluate loss: 0.52125084400177:  61%|████████████▏       | 28/46 [00:02<00:01,  9.58it/s]evaluate for the 29-th batch, evaluate loss: 0.48677048087120056:  61%|██████████▎      | 28/46 [00:03<00:01,  9.58it/s]evaluate for the 29-th batch, evaluate loss: 0.48677048087120056:  63%|██████████▋      | 29/46 [00:03<00:01,  9.57it/s]Epoch: 4, train for the 35-th batch, train loss: 0.1797991394996643:  29%|███▋         | 34/119 [00:21<00:56,  1.51it/s]Epoch: 4, train for the 35-th batch, train loss: 0.1797991394996643:  29%|███▊         | 35/119 [00:21<00:56,  1.50it/s]Epoch: 2, train for the 10-th batch, train loss: 0.5077416300773621:   2%|▎             | 9/383 [00:05<03:21,  1.86it/s]Epoch: 2, train for the 10-th batch, train loss: 0.5077416300773621:   3%|▎            | 10/383 [00:05<03:23,  1.84it/s]evaluate for the 30-th batch, evaluate loss: 0.4966163635253906:  63%|███████████▎      | 29/46 [00:03<00:01,  9.57it/s]evaluate for the 30-th batch, evaluate loss: 0.4966163635253906:  65%|███████████▋      | 30/46 [00:03<00:01,  9.56it/s]evaluate for the 31-th batch, evaluate loss: 0.5192798376083374:  65%|███████████▋      | 30/46 [00:03<00:01,  9.56it/s]evaluate for the 31-th batch, evaluate loss: 0.5192798376083374:  67%|████████████▏     | 31/46 [00:03<00:01,  9.59it/s]evaluate for the 32-th batch, evaluate loss: 0.47466373443603516:  67%|███████████▍     | 31/46 [00:03<00:01,  9.59it/s]evaluate for the 32-th batch, evaluate loss: 0.47466373443603516:  70%|███████████▊     | 32/46 [00:03<00:01,  9.61it/s]Epoch: 2, train for the 202-th batch, train loss: 0.6265050172805786:  85%|█████████▎ | 201/237 [01:59<00:21,  1.70it/s]Epoch: 2, train for the 202-th batch, train loss: 0.6265050172805786:  85%|█████████▍ | 202/237 [01:59<00:20,  1.73it/s]evaluate for the 33-th batch, evaluate loss: 0.4967428743839264:  70%|████████████▌     | 32/46 [00:03<00:01,  9.61it/s]evaluate for the 33-th batch, evaluate loss: 0.4967428743839264:  72%|████████████▉     | 33/46 [00:03<00:01,  9.62it/s]Epoch: 3, train for the 130-th batch, train loss: 0.513945996761322:  88%|██████████▌ | 129/146 [01:18<00:10,  1.63it/s]Epoch: 3, train for the 130-th batch, train loss: 0.513945996761322:  89%|██████████▋ | 130/146 [01:18<00:09,  1.61it/s]evaluate for the 34-th batch, evaluate loss: 0.48656710982322693:  72%|████████████▏    | 33/46 [00:03<00:01,  9.62it/s]evaluate for the 34-th batch, evaluate loss: 0.48656710982322693:  74%|████████████▌    | 34/46 [00:03<00:01,  9.62it/s]evaluate for the 35-th batch, evaluate loss: 0.48374906182289124:  74%|████████████▌    | 34/46 [00:03<00:01,  9.62it/s]evaluate for the 35-th batch, evaluate loss: 0.48374906182289124:  76%|████████████▉    | 35/46 [00:03<00:01,  9.62it/s]Epoch: 4, train for the 36-th batch, train loss: 0.22222073376178741:  29%|███▌        | 35/119 [00:22<00:56,  1.50it/s]Epoch: 4, train for the 36-th batch, train loss: 0.22222073376178741:  30%|███▋        | 36/119 [00:22<00:54,  1.53it/s]Epoch: 2, train for the 11-th batch, train loss: 0.6249498724937439:   3%|▎            | 10/383 [00:05<03:23,  1.84it/s]Epoch: 2, train for the 11-th batch, train loss: 0.6249498724937439:   3%|▎            | 11/383 [00:05<03:23,  1.83it/s]evaluate for the 36-th batch, evaluate loss: 0.47368988394737244:  76%|████████████▉    | 35/46 [00:03<00:01,  9.62it/s]evaluate for the 36-th batch, evaluate loss: 0.47368988394737244:  78%|█████████████▎   | 36/46 [00:03<00:01,  9.63it/s]evaluate for the 37-th batch, evaluate loss: 0.5046224594116211:  78%|██████████████    | 36/46 [00:03<00:01,  9.63it/s]evaluate for the 37-th batch, evaluate loss: 0.5046224594116211:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.63it/s]evaluate for the 38-th batch, evaluate loss: 0.5368988513946533:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.63it/s]evaluate for the 38-th batch, evaluate loss: 0.5368988513946533:  83%|██████████████▊   | 38/46 [00:03<00:00,  9.63it/s]Epoch: 2, train for the 203-th batch, train loss: 0.6259145736694336:  85%|█████████▍ | 202/237 [01:59<00:20,  1.73it/s]Epoch: 2, train for the 203-th batch, train loss: 0.6259145736694336:  86%|█████████▍ | 203/237 [01:59<00:19,  1.75it/s]evaluate for the 39-th batch, evaluate loss: 0.5355420112609863:  83%|██████████████▊   | 38/46 [00:04<00:00,  9.63it/s]evaluate for the 39-th batch, evaluate loss: 0.5355420112609863:  85%|███████████████▎  | 39/46 [00:04<00:00,  9.62it/s]Epoch: 3, train for the 131-th batch, train loss: 0.5591951608657837:  89%|█████████▊ | 130/146 [01:19<00:09,  1.61it/s]Epoch: 3, train for the 131-th batch, train loss: 0.5591951608657837:  90%|█████████▊ | 131/146 [01:19<00:09,  1.60it/s]evaluate for the 40-th batch, evaluate loss: 0.4679586589336395:  85%|███████████████▎  | 39/46 [00:04<00:00,  9.62it/s]evaluate for the 40-th batch, evaluate loss: 0.4679586589336395:  87%|███████████████▋  | 40/46 [00:04<00:00,  9.62it/s]Epoch: 2, train for the 12-th batch, train loss: 0.39325523376464844:   3%|▎           | 11/383 [00:06<03:23,  1.83it/s]Epoch: 2, train for the 12-th batch, train loss: 0.39325523376464844:   3%|▍           | 12/383 [00:06<03:24,  1.81it/s]evaluate for the 41-th batch, evaluate loss: 0.48600926995277405:  87%|██████████████▊  | 40/46 [00:04<00:00,  9.62it/s]evaluate for the 41-th batch, evaluate loss: 0.48600926995277405:  89%|███████████████▏ | 41/46 [00:04<00:00,  9.63it/s]Epoch: 4, train for the 37-th batch, train loss: 0.210621640086174:  30%|████▏         | 36/119 [00:22<00:54,  1.53it/s]Epoch: 4, train for the 37-th batch, train loss: 0.210621640086174:  31%|████▎         | 37/119 [00:22<00:52,  1.55it/s]evaluate for the 42-th batch, evaluate loss: 0.46906933188438416:  89%|███████████████▏ | 41/46 [00:04<00:00,  9.63it/s]evaluate for the 42-th batch, evaluate loss: 0.46906933188438416:  91%|███████████████▌ | 42/46 [00:04<00:00,  9.62it/s]evaluate for the 43-th batch, evaluate loss: 0.5342525839805603:  91%|████████████████▍ | 42/46 [00:04<00:00,  9.62it/s]evaluate for the 43-th batch, evaluate loss: 0.5342525839805603:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.61it/s]Epoch: 2, train for the 204-th batch, train loss: 0.6209064722061157:  86%|█████████▍ | 203/237 [02:00<00:19,  1.75it/s]Epoch: 2, train for the 204-th batch, train loss: 0.6209064722061157:  86%|█████████▍ | 204/237 [02:00<00:18,  1.75it/s]evaluate for the 44-th batch, evaluate loss: 0.5139521956443787:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.61it/s]evaluate for the 44-th batch, evaluate loss: 0.5139521956443787:  96%|█████████████████▏| 44/46 [00:04<00:00,  9.62it/s]evaluate for the 45-th batch, evaluate loss: 0.4946226477622986:  96%|█████████████████▏| 44/46 [00:04<00:00,  9.62it/s]evaluate for the 45-th batch, evaluate loss: 0.4946226477622986:  98%|█████████████████▌| 45/46 [00:04<00:00,  9.63it/s]Epoch: 3, train for the 132-th batch, train loss: 0.5476700663566589:  90%|█████████▊ | 131/146 [01:20<00:09,  1.60it/s]Epoch: 3, train for the 132-th batch, train loss: 0.5476700663566589:  90%|█████████▉ | 132/146 [01:20<00:08,  1.61it/s]Epoch: 2, train for the 13-th batch, train loss: 0.3462173640727997:   3%|▍            | 12/383 [00:06<03:24,  1.81it/s]Epoch: 2, train for the 13-th batch, train loss: 0.3462173640727997:   3%|▍            | 13/383 [00:06<03:21,  1.84it/s]evaluate for the 46-th batch, evaluate loss: 0.5040595531463623:  98%|█████████████████▌| 45/46 [00:04<00:00,  9.63it/s]evaluate for the 46-th batch, evaluate loss: 0.5040595531463623: 100%|██████████████████| 46/46 [00:04<00:00,  9.64it/s]
  0%|                                                                                            | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6460286378860474:   0%|                            | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6460286378860474:   4%|▊                   | 1/25 [00:00<00:02,  9.20it/s]Epoch: 4, train for the 38-th batch, train loss: 0.19769535958766937:  31%|███▋        | 37/119 [00:23<00:52,  1.55it/s]Epoch: 4, train for the 38-th batch, train loss: 0.19769535958766937:  32%|███▊        | 38/119 [00:23<00:51,  1.57it/s]evaluate for the 2-th batch, evaluate loss: 0.6538599133491516:   4%|▊                   | 1/25 [00:00<00:02,  9.20it/s]evaluate for the 2-th batch, evaluate loss: 0.6538599133491516:   8%|█▌                  | 2/25 [00:00<00:02,  9.18it/s]evaluate for the 3-th batch, evaluate loss: 0.701153576374054:   8%|█▋                   | 2/25 [00:00<00:02,  9.18it/s]evaluate for the 3-th batch, evaluate loss: 0.701153576374054:  12%|██▌                  | 3/25 [00:00<00:02,  9.18it/s]Epoch: 2, train for the 205-th batch, train loss: 0.6399515867233276:  86%|█████████▍ | 204/237 [02:01<00:18,  1.75it/s]Epoch: 2, train for the 205-th batch, train loss: 0.6399515867233276:  86%|█████████▌ | 205/237 [02:01<00:18,  1.74it/s]evaluate for the 4-th batch, evaluate loss: 0.6815197467803955:  12%|██▍                 | 3/25 [00:00<00:02,  9.18it/s]evaluate for the 4-th batch, evaluate loss: 0.6815197467803955:  16%|███▏                | 4/25 [00:00<00:02,  9.21it/s]Epoch: 2, train for the 14-th batch, train loss: 0.35700544714927673:   3%|▍           | 13/383 [00:07<03:21,  1.84it/s]Epoch: 2, train for the 14-th batch, train loss: 0.35700544714927673:   4%|▍           | 14/383 [00:07<03:20,  1.84it/s]evaluate for the 5-th batch, evaluate loss: 0.6746137738227844:  16%|███▏                | 4/25 [00:00<00:02,  9.21it/s]evaluate for the 5-th batch, evaluate loss: 0.6746137738227844:  20%|████                | 5/25 [00:00<00:02,  9.14it/s]Epoch: 3, train for the 133-th batch, train loss: 0.5109657049179077:  90%|█████████▉ | 132/146 [01:20<00:08,  1.61it/s]Epoch: 3, train for the 133-th batch, train loss: 0.5109657049179077:  91%|██████████ | 133/146 [01:20<00:08,  1.61it/s]evaluate for the 6-th batch, evaluate loss: 0.7300045490264893:  20%|████                | 5/25 [00:00<00:02,  9.14it/s]evaluate for the 6-th batch, evaluate loss: 0.7300045490264893:  24%|████▊               | 6/25 [00:00<00:02,  9.14it/s]Epoch: 4, train for the 39-th batch, train loss: 0.2150806337594986:  32%|████▏        | 38/119 [00:24<00:51,  1.57it/s]Epoch: 4, train for the 39-th batch, train loss: 0.2150806337594986:  33%|████▎        | 39/119 [00:24<00:50,  1.59it/s]evaluate for the 7-th batch, evaluate loss: 0.7519909739494324:  24%|████▊               | 6/25 [00:00<00:02,  9.14it/s]evaluate for the 7-th batch, evaluate loss: 0.7519909739494324:  28%|█████▌              | 7/25 [00:00<00:01,  9.15it/s]evaluate for the 8-th batch, evaluate loss: 0.7320202589035034:  28%|█████▌              | 7/25 [00:00<00:01,  9.15it/s]evaluate for the 8-th batch, evaluate loss: 0.7320202589035034:  32%|██████▍             | 8/25 [00:00<00:01,  9.18it/s]Epoch: 2, train for the 206-th batch, train loss: 0.6492411494255066:  86%|█████████▌ | 205/237 [02:01<00:18,  1.74it/s]Epoch: 2, train for the 206-th batch, train loss: 0.6492411494255066:  87%|█████████▌ | 206/237 [02:01<00:18,  1.69it/s]evaluate for the 9-th batch, evaluate loss: 0.7152875065803528:  32%|██████▍             | 8/25 [00:00<00:01,  9.18it/s]evaluate for the 9-th batch, evaluate loss: 0.7152875065803528:  36%|███████▏            | 9/25 [00:00<00:01,  9.19it/s]evaluate for the 10-th batch, evaluate loss: 0.7528071403503418:  36%|██████▊            | 9/25 [00:01<00:01,  9.19it/s]evaluate for the 10-th batch, evaluate loss: 0.7528071403503418:  40%|███████▏          | 10/25 [00:01<00:01,  9.18it/s]Epoch: 2, train for the 15-th batch, train loss: 0.4808768630027771:   4%|▍            | 14/383 [00:07<03:20,  1.84it/s]Epoch: 2, train for the 15-th batch, train loss: 0.4808768630027771:   4%|▌            | 15/383 [00:07<03:26,  1.79it/s]Epoch: 3, train for the 134-th batch, train loss: 0.5196734666824341:  91%|██████████ | 133/146 [01:21<00:08,  1.61it/s]Epoch: 3, train for the 134-th batch, train loss: 0.5196734666824341:  92%|██████████ | 134/146 [01:21<00:07,  1.62it/s]evaluate for the 11-th batch, evaluate loss: 0.7430437803268433:  40%|███████▏          | 10/25 [00:01<00:01,  9.18it/s]evaluate for the 11-th batch, evaluate loss: 0.7430437803268433:  44%|███████▉          | 11/25 [00:01<00:01,  9.18it/s]evaluate for the 12-th batch, evaluate loss: 0.7083979845046997:  44%|███████▉          | 11/25 [00:01<00:01,  9.18it/s]evaluate for the 12-th batch, evaluate loss: 0.7083979845046997:  48%|████████▋         | 12/25 [00:01<00:01,  9.18it/s]Epoch: 4, train for the 40-th batch, train loss: 0.20242391526699066:  33%|███▉        | 39/119 [00:24<00:50,  1.59it/s]Epoch: 4, train for the 40-th batch, train loss: 0.20242391526699066:  34%|████        | 40/119 [00:24<00:49,  1.61it/s]evaluate for the 13-th batch, evaluate loss: 0.6756765842437744:  48%|████████▋         | 12/25 [00:01<00:01,  9.18it/s]evaluate for the 13-th batch, evaluate loss: 0.6756765842437744:  52%|█████████▎        | 13/25 [00:01<00:01,  9.19it/s]evaluate for the 14-th batch, evaluate loss: 0.7652816772460938:  52%|█████████▎        | 13/25 [00:01<00:01,  9.19it/s]evaluate for the 14-th batch, evaluate loss: 0.7652816772460938:  56%|██████████        | 14/25 [00:01<00:01,  9.19it/s]Epoch: 2, train for the 207-th batch, train loss: 0.6553217768669128:  87%|█████████▌ | 206/237 [02:02<00:18,  1.69it/s]Epoch: 2, train for the 207-th batch, train loss: 0.6553217768669128:  87%|█████████▌ | 207/237 [02:02<00:17,  1.70it/s]evaluate for the 15-th batch, evaluate loss: 0.7476280331611633:  56%|██████████        | 14/25 [00:01<00:01,  9.19it/s]evaluate for the 15-th batch, evaluate loss: 0.7476280331611633:  60%|██████████▊       | 15/25 [00:01<00:01,  9.17it/s]Epoch: 2, train for the 16-th batch, train loss: 0.4282824695110321:   4%|▌            | 15/383 [00:08<03:26,  1.79it/s]Epoch: 2, train for the 16-th batch, train loss: 0.4282824695110321:   4%|▌            | 16/383 [00:08<03:28,  1.76it/s]evaluate for the 16-th batch, evaluate loss: 0.6710084676742554:  60%|██████████▊       | 15/25 [00:01<00:01,  9.17it/s]evaluate for the 16-th batch, evaluate loss: 0.6710084676742554:  64%|███████████▌      | 16/25 [00:01<00:00,  9.17it/s]Epoch: 3, train for the 135-th batch, train loss: 0.5494834184646606:  92%|██████████ | 134/146 [01:22<00:07,  1.62it/s]Epoch: 3, train for the 135-th batch, train loss: 0.5494834184646606:  92%|██████████▏| 135/146 [01:22<00:06,  1.63it/s]evaluate for the 17-th batch, evaluate loss: 0.6741817593574524:  64%|███████████▌      | 16/25 [00:01<00:00,  9.17it/s]evaluate for the 17-th batch, evaluate loss: 0.6741817593574524:  68%|████████████▏     | 17/25 [00:01<00:00,  9.18it/s]Epoch: 4, train for the 41-th batch, train loss: 0.29266682267189026:  34%|████        | 40/119 [00:25<00:49,  1.61it/s]Epoch: 4, train for the 41-th batch, train loss: 0.29266682267189026:  34%|████▏       | 41/119 [00:25<00:48,  1.62it/s]evaluate for the 18-th batch, evaluate loss: 0.6325916647911072:  68%|████████████▏     | 17/25 [00:01<00:00,  9.18it/s]evaluate for the 18-th batch, evaluate loss: 0.6325916647911072:  72%|████████████▉     | 18/25 [00:01<00:00,  9.18it/s]evaluate for the 19-th batch, evaluate loss: 0.6028860807418823:  72%|████████████▉     | 18/25 [00:02<00:00,  9.18it/s]evaluate for the 19-th batch, evaluate loss: 0.6028860807418823:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.18it/s]Epoch: 2, train for the 208-th batch, train loss: 0.653725802898407:  87%|██████████▍ | 207/237 [02:02<00:17,  1.70it/s]Epoch: 2, train for the 208-th batch, train loss: 0.653725802898407:  88%|██████████▌ | 208/237 [02:02<00:17,  1.70it/s]evaluate for the 20-th batch, evaluate loss: 0.6776581406593323:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.18it/s]evaluate for the 20-th batch, evaluate loss: 0.6776581406593323:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.16it/s]evaluate for the 21-th batch, evaluate loss: 0.7398554682731628:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.16it/s]evaluate for the 21-th batch, evaluate loss: 0.7398554682731628:  84%|███████████████   | 21/25 [00:02<00:00,  9.17it/s]evaluate for the 22-th batch, evaluate loss: 0.6166855096817017:  84%|███████████████   | 21/25 [00:02<00:00,  9.17it/s]evaluate for the 22-th batch, evaluate loss: 0.6166855096817017:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.18it/s]Epoch: 3, train for the 136-th batch, train loss: 0.5204230546951294:  92%|██████████▏| 135/146 [01:22<00:06,  1.63it/s]Epoch: 3, train for the 136-th batch, train loss: 0.5204230546951294:  93%|██████████▏| 136/146 [01:22<00:06,  1.63it/s]Epoch: 2, train for the 17-th batch, train loss: 0.5378152132034302:   4%|▌            | 16/383 [00:09<03:28,  1.76it/s]Epoch: 2, train for the 17-th batch, train loss: 0.5378152132034302:   4%|▌            | 17/383 [00:09<03:51,  1.58it/s]evaluate for the 23-th batch, evaluate loss: 0.6837050318717957:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.18it/s]evaluate for the 23-th batch, evaluate loss: 0.6837050318717957:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.15it/s]Epoch: 2, train for the 209-th batch, train loss: 0.6265959739685059:  88%|█████████▋ | 208/237 [02:03<00:17,  1.70it/s]Epoch: 2, train for the 209-th batch, train loss: 0.6265959739685059:  88%|█████████▋ | 209/237 [02:03<00:14,  1.89it/s]Epoch: 4, train for the 42-th batch, train loss: 0.2230670154094696:  34%|████▍        | 41/119 [00:25<00:48,  1.62it/s]Epoch: 4, train for the 42-th batch, train loss: 0.2230670154094696:  35%|████▌        | 42/119 [00:25<00:47,  1.62it/s]evaluate for the 24-th batch, evaluate loss: 0.6714723110198975:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.15it/s]evaluate for the 24-th batch, evaluate loss: 0.6714723110198975:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.15it/s]evaluate for the 25-th batch, evaluate loss: 0.7180777788162231:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.15it/s]evaluate for the 25-th batch, evaluate loss: 0.7180777788162231: 100%|██████████████████| 25/25 [00:02<00:00,  9.24it/s]
INFO:root:Epoch: 6, learning rate: 0.0001, train loss: 0.5694
INFO:root:train average_precision, 0.8132
INFO:root:train roc_auc, 0.7776
INFO:root:validate loss: 0.5088
INFO:root:validate average_precision, 0.8415
INFO:root:validate roc_auc, 0.8016
INFO:root:new node validate loss: 0.6947
INFO:root:new node validate first_1_average_precision, 0.5929
INFO:root:new node validate first_1_roc_auc, 0.5391
INFO:root:new node validate first_3_average_precision, 0.6746
INFO:root:new node validate first_3_roc_auc, 0.6360
INFO:root:new node validate first_10_average_precision, 0.7430
INFO:root:new node validate first_10_roc_auc, 0.7087
INFO:root:new node validate average_precision, 0.7044
INFO:root:new node validate roc_auc, 0.6532
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]Epoch: 7, train for the 1-th batch, train loss: 0.896902322769165:   0%|                        | 0/151 [00:00<?, ?it/s]Epoch: 7, train for the 1-th batch, train loss: 0.896902322769165:   1%|                | 1/151 [00:00<00:26,  5.72it/s]Epoch: 3, train for the 137-th batch, train loss: 0.5318776369094849:  93%|██████████▏| 136/146 [01:23<00:06,  1.63it/s]Epoch: 3, train for the 137-th batch, train loss: 0.5318776369094849:  94%|██████████▎| 137/146 [01:23<00:05,  1.63it/s]Epoch: 2, train for the 18-th batch, train loss: 0.5779209136962891:   4%|▌            | 17/383 [00:09<03:51,  1.58it/s]Epoch: 2, train for the 18-th batch, train loss: 0.5779209136962891:   5%|▌            | 18/383 [00:09<03:43,  1.63it/s]Epoch: 7, train for the 2-th batch, train loss: 0.9059398174285889:   1%|               | 1/151 [00:00<00:26,  5.72it/s]Epoch: 7, train for the 2-th batch, train loss: 0.9059398174285889:   1%|▏              | 2/151 [00:00<00:26,  5.63it/s]Epoch: 2, train for the 210-th batch, train loss: 0.6306766271591187:  88%|█████████▋ | 209/237 [02:03<00:14,  1.89it/s]Epoch: 2, train for the 210-th batch, train loss: 0.6306766271591187:  89%|█████████▋ | 210/237 [02:03<00:14,  1.83it/s]Epoch: 7, train for the 3-th batch, train loss: 0.6768496036529541:   1%|▏              | 2/151 [00:00<00:26,  5.63it/s]Epoch: 7, train for the 3-th batch, train loss: 0.6768496036529541:   2%|▎              | 3/151 [00:00<00:25,  5.83it/s]Epoch: 3, train for the 138-th batch, train loss: 0.560748279094696:  94%|███████████▎| 137/146 [01:23<00:05,  1.63it/s]Epoch: 3, train for the 138-th batch, train loss: 0.560748279094696:  95%|███████████▎| 138/146 [01:23<00:04,  1.99it/s]Epoch: 7, train for the 4-th batch, train loss: 0.5214964151382446:   2%|▎              | 3/151 [00:00<00:25,  5.83it/s]Epoch: 7, train for the 4-th batch, train loss: 0.5214964151382446:   3%|▍              | 4/151 [00:00<00:24,  5.89it/s]Epoch: 4, train for the 43-th batch, train loss: 0.22091302275657654:  35%|████▏       | 42/119 [00:26<00:47,  1.62it/s]Epoch: 4, train for the 43-th batch, train loss: 0.22091302275657654:  36%|████▎       | 43/119 [00:26<00:52,  1.46it/s]Epoch: 7, train for the 5-th batch, train loss: 0.5774043798446655:   3%|▍              | 4/151 [00:00<00:24,  5.89it/s]Epoch: 7, train for the 5-th batch, train loss: 0.5774043798446655:   3%|▍              | 5/151 [00:00<00:25,  5.68it/s]Epoch: 2, train for the 19-th batch, train loss: 0.4037937521934509:   5%|▌            | 18/383 [00:10<03:43,  1.63it/s]Epoch: 2, train for the 19-th batch, train loss: 0.4037937521934509:   5%|▋            | 19/383 [00:10<03:36,  1.68it/s]Epoch: 2, train for the 211-th batch, train loss: 0.6306918859481812:  89%|█████████▋ | 210/237 [02:04<00:14,  1.83it/s]Epoch: 2, train for the 211-th batch, train loss: 0.6306918859481812:  89%|█████████▊ | 211/237 [02:04<00:14,  1.78it/s]Epoch: 7, train for the 6-th batch, train loss: 0.5722010135650635:   3%|▍              | 5/151 [00:01<00:25,  5.68it/s]Epoch: 7, train for the 6-th batch, train loss: 0.5722010135650635:   4%|▌              | 6/151 [00:01<00:26,  5.54it/s]Epoch: 3, train for the 139-th batch, train loss: 0.552327573299408:  95%|███████████▎| 138/146 [01:24<00:04,  1.99it/s]Epoch: 3, train for the 139-th batch, train loss: 0.552327573299408:  95%|███████████▍| 139/146 [01:24<00:03,  1.87it/s]Epoch: 7, train for the 7-th batch, train loss: 0.6305458545684814:   4%|▌              | 6/151 [00:01<00:26,  5.54it/s]Epoch: 7, train for the 7-th batch, train loss: 0.6305458545684814:   5%|▋              | 7/151 [00:01<00:26,  5.35it/s]Epoch: 4, train for the 44-th batch, train loss: 0.2178204357624054:  36%|████▋        | 43/119 [00:27<00:52,  1.46it/s]Epoch: 4, train for the 44-th batch, train loss: 0.2178204357624054:  37%|████▊        | 44/119 [00:27<00:49,  1.50it/s]Epoch: 2, train for the 20-th batch, train loss: 0.4160936176776886:   5%|▋            | 19/383 [00:10<03:36,  1.68it/s]Epoch: 2, train for the 20-th batch, train loss: 0.4160936176776886:   5%|▋            | 20/383 [00:10<03:31,  1.72it/s]Epoch: 7, train for the 8-th batch, train loss: 0.6418484449386597:   5%|▋              | 7/151 [00:01<00:26,  5.35it/s]Epoch: 7, train for the 8-th batch, train loss: 0.6418484449386597:   5%|▊              | 8/151 [00:01<00:26,  5.32it/s]Epoch: 2, train for the 212-th batch, train loss: 0.6317723989486694:  89%|█████████▊ | 211/237 [02:05<00:14,  1.78it/s]Epoch: 2, train for the 212-th batch, train loss: 0.6317723989486694:  89%|█████████▊ | 212/237 [02:05<00:14,  1.75it/s]Epoch: 7, train for the 9-th batch, train loss: 0.5861416459083557:   5%|▊              | 8/151 [00:01<00:26,  5.32it/s]Epoch: 7, train for the 9-th batch, train loss: 0.5861416459083557:   6%|▉              | 9/151 [00:01<00:26,  5.27it/s]Epoch: 3, train for the 140-th batch, train loss: 0.5256170034408569:  95%|██████████▍| 139/146 [01:24<00:03,  1.87it/s]Epoch: 3, train for the 140-th batch, train loss: 0.5256170034408569:  96%|██████████▌| 140/146 [01:24<00:03,  1.79it/s]Epoch: 7, train for the 10-th batch, train loss: 0.5126258134841919:   6%|▊             | 9/151 [00:01<00:26,  5.27it/s]Epoch: 7, train for the 10-th batch, train loss: 0.5126258134841919:   7%|▊            | 10/151 [00:01<00:27,  5.19it/s]Epoch: 4, train for the 45-th batch, train loss: 0.16992591321468353:  37%|████▍       | 44/119 [00:27<00:49,  1.50it/s]Epoch: 4, train for the 45-th batch, train loss: 0.16992591321468353:  38%|████▌       | 45/119 [00:27<00:48,  1.54it/s]Epoch: 2, train for the 21-th batch, train loss: 0.4251028001308441:   5%|▋            | 20/383 [00:11<03:31,  1.72it/s]Epoch: 2, train for the 21-th batch, train loss: 0.4251028001308441:   5%|▋            | 21/383 [00:11<03:31,  1.71it/s]Epoch: 7, train for the 11-th batch, train loss: 0.6138842701911926:   7%|▊            | 10/151 [00:02<00:27,  5.19it/s]Epoch: 7, train for the 11-th batch, train loss: 0.6138842701911926:   7%|▉            | 11/151 [00:02<00:27,  5.08it/s]Epoch: 2, train for the 213-th batch, train loss: 0.6358612775802612:  89%|█████████▊ | 212/237 [02:05<00:14,  1.75it/s]Epoch: 2, train for the 213-th batch, train loss: 0.6358612775802612:  90%|█████████▉ | 213/237 [02:05<00:13,  1.73it/s]Epoch: 7, train for the 12-th batch, train loss: 0.8765214085578918:   7%|▉            | 11/151 [00:02<00:27,  5.08it/s]Epoch: 7, train for the 12-th batch, train loss: 0.8765214085578918:   8%|█            | 12/151 [00:02<00:28,  4.93it/s]Epoch: 3, train for the 141-th batch, train loss: 0.5658207535743713:  96%|██████████▌| 140/146 [01:25<00:03,  1.79it/s]Epoch: 3, train for the 141-th batch, train loss: 0.5658207535743713:  97%|██████████▌| 141/146 [01:25<00:02,  1.74it/s]Epoch: 7, train for the 13-th batch, train loss: 0.555233895778656:   8%|█             | 12/151 [00:02<00:28,  4.93it/s]Epoch: 7, train for the 13-th batch, train loss: 0.555233895778656:   9%|█▏            | 13/151 [00:02<00:27,  4.94it/s]Epoch: 4, train for the 46-th batch, train loss: 0.20765052735805511:  38%|████▌       | 45/119 [00:28<00:48,  1.54it/s]Epoch: 4, train for the 46-th batch, train loss: 0.20765052735805511:  39%|████▋       | 46/119 [00:28<00:46,  1.56it/s]Epoch: 2, train for the 22-th batch, train loss: 0.40215545892715454:   5%|▋           | 21/383 [00:12<03:31,  1.71it/s]Epoch: 2, train for the 22-th batch, train loss: 0.40215545892715454:   6%|▋           | 22/383 [00:12<03:31,  1.71it/s]Epoch: 7, train for the 14-th batch, train loss: 0.5717292428016663:   9%|█            | 13/151 [00:02<00:27,  4.94it/s]Epoch: 7, train for the 14-th batch, train loss: 0.5717292428016663:   9%|█▏           | 14/151 [00:02<00:27,  4.94it/s]Epoch: 2, train for the 214-th batch, train loss: 0.6567818522453308:  90%|█████████▉ | 213/237 [02:06<00:13,  1.73it/s]Epoch: 2, train for the 214-th batch, train loss: 0.6567818522453308:  90%|█████████▉ | 214/237 [02:06<00:13,  1.71it/s]Epoch: 7, train for the 15-th batch, train loss: 0.44305965304374695:   9%|█           | 14/151 [00:02<00:27,  4.94it/s]Epoch: 7, train for the 15-th batch, train loss: 0.44305965304374695:  10%|█▏          | 15/151 [00:02<00:27,  4.91it/s]Epoch: 3, train for the 142-th batch, train loss: 0.5126825571060181:  97%|██████████▌| 141/146 [01:25<00:02,  1.74it/s]Epoch: 3, train for the 142-th batch, train loss: 0.5126825571060181:  97%|██████████▋| 142/146 [01:25<00:02,  1.71it/s]Epoch: 7, train for the 16-th batch, train loss: 0.656165599822998:  10%|█▍            | 15/151 [00:03<00:27,  4.91it/s]Epoch: 7, train for the 16-th batch, train loss: 0.656165599822998:  11%|█▍            | 16/151 [00:03<00:27,  4.85it/s]Epoch: 4, train for the 47-th batch, train loss: 0.24187564849853516:  39%|████▋       | 46/119 [00:29<00:46,  1.56it/s]Epoch: 4, train for the 47-th batch, train loss: 0.24187564849853516:  39%|████▋       | 47/119 [00:29<00:45,  1.59it/s]Epoch: 2, train for the 23-th batch, train loss: 0.497003972530365:   6%|▊             | 22/383 [00:12<03:31,  1.71it/s]Epoch: 2, train for the 23-th batch, train loss: 0.497003972530365:   6%|▊             | 23/383 [00:12<03:31,  1.70it/s]Epoch: 7, train for the 17-th batch, train loss: 0.4580010175704956:  11%|█▍           | 16/151 [00:03<00:27,  4.85it/s]Epoch: 7, train for the 17-th batch, train loss: 0.4580010175704956:  11%|█▍           | 17/151 [00:03<00:27,  4.92it/s]Epoch: 2, train for the 215-th batch, train loss: 0.6143062710762024:  90%|█████████▉ | 214/237 [02:06<00:13,  1.71it/s]Epoch: 2, train for the 215-th batch, train loss: 0.6143062710762024:  91%|█████████▉ | 215/237 [02:06<00:12,  1.71it/s]Epoch: 7, train for the 18-th batch, train loss: 0.8369605541229248:  11%|█▍           | 17/151 [00:03<00:27,  4.92it/s]Epoch: 7, train for the 18-th batch, train loss: 0.8369605541229248:  12%|█▌           | 18/151 [00:03<00:27,  4.83it/s]Epoch: 3, train for the 143-th batch, train loss: 0.5090785026550293:  97%|██████████▋| 142/146 [01:26<00:02,  1.71it/s]Epoch: 3, train for the 143-th batch, train loss: 0.5090785026550293:  98%|██████████▊| 143/146 [01:26<00:01,  1.69it/s]Epoch: 7, train for the 19-th batch, train loss: 0.7465843558311462:  12%|█▌           | 18/151 [00:03<00:27,  4.83it/s]Epoch: 7, train for the 19-th batch, train loss: 0.7465843558311462:  13%|█▋           | 19/151 [00:03<00:27,  4.76it/s]Epoch: 4, train for the 48-th batch, train loss: 0.25703904032707214:  39%|████▋       | 47/119 [00:29<00:45,  1.59it/s]Epoch: 4, train for the 48-th batch, train loss: 0.25703904032707214:  40%|████▊       | 48/119 [00:29<00:44,  1.61it/s]Epoch: 2, train for the 24-th batch, train loss: 0.41073834896087646:   6%|▋           | 23/383 [00:13<03:31,  1.70it/s]Epoch: 2, train for the 24-th batch, train loss: 0.41073834896087646:   6%|▊           | 24/383 [00:13<03:35,  1.67it/s]Epoch: 7, train for the 20-th batch, train loss: 0.42506060004234314:  13%|█▌          | 19/151 [00:03<00:27,  4.76it/s]Epoch: 7, train for the 20-th batch, train loss: 0.42506060004234314:  13%|█▌          | 20/151 [00:03<00:27,  4.79it/s]Epoch: 2, train for the 216-th batch, train loss: 0.6343877911567688:  91%|█████████▉ | 215/237 [02:07<00:12,  1.71it/s]Epoch: 2, train for the 216-th batch, train loss: 0.6343877911567688:  91%|██████████ | 216/237 [02:07<00:12,  1.68it/s]Epoch: 7, train for the 21-th batch, train loss: 0.53711998462677:  13%|█▉             | 20/151 [00:04<00:27,  4.79it/s]Epoch: 7, train for the 21-th batch, train loss: 0.53711998462677:  14%|██             | 21/151 [00:04<00:27,  4.79it/s]Epoch: 3, train for the 144-th batch, train loss: 0.5042073130607605:  98%|██████████▊| 143/146 [01:27<00:01,  1.69it/s]Epoch: 3, train for the 144-th batch, train loss: 0.5042073130607605:  99%|██████████▊| 144/146 [01:27<00:01,  1.67it/s]Epoch: 7, train for the 22-th batch, train loss: 0.5355298519134521:  14%|█▊           | 21/151 [00:04<00:27,  4.79it/s]Epoch: 7, train for the 22-th batch, train loss: 0.5355298519134521:  15%|█▉           | 22/151 [00:04<00:26,  4.80it/s]Epoch: 4, train for the 49-th batch, train loss: 0.21483184397220612:  40%|████▊       | 48/119 [00:30<00:44,  1.61it/s]Epoch: 4, train for the 49-th batch, train loss: 0.21483184397220612:  41%|████▉       | 49/119 [00:30<00:43,  1.62it/s]Epoch: 2, train for the 25-th batch, train loss: 0.4272726774215698:   6%|▊            | 24/383 [00:13<03:35,  1.67it/s]Epoch: 2, train for the 25-th batch, train loss: 0.4272726774215698:   7%|▊            | 25/383 [00:13<03:33,  1.68it/s]Epoch: 7, train for the 23-th batch, train loss: 0.45022881031036377:  15%|█▋          | 22/151 [00:04<00:26,  4.80it/s]Epoch: 7, train for the 23-th batch, train loss: 0.45022881031036377:  15%|█▊          | 23/151 [00:04<00:26,  4.81it/s]Epoch: 2, train for the 217-th batch, train loss: 0.6367286443710327:  91%|██████████ | 216/237 [02:08<00:12,  1.68it/s]Epoch: 2, train for the 217-th batch, train loss: 0.6367286443710327:  92%|██████████ | 217/237 [02:08<00:11,  1.68it/s]Epoch: 7, train for the 24-th batch, train loss: 0.6074694395065308:  15%|█▉           | 23/151 [00:04<00:26,  4.81it/s]Epoch: 7, train for the 24-th batch, train loss: 0.6074694395065308:  16%|██           | 24/151 [00:04<00:26,  4.72it/s]Epoch: 3, train for the 145-th batch, train loss: 0.5626682043075562:  99%|██████████▊| 144/146 [01:27<00:01,  1.67it/s]Epoch: 3, train for the 145-th batch, train loss: 0.5626682043075562:  99%|██████████▉| 145/146 [01:27<00:00,  1.67it/s]Epoch: 4, train for the 50-th batch, train loss: 0.1865585297346115:  41%|█████▎       | 49/119 [00:30<00:43,  1.62it/s]Epoch: 4, train for the 50-th batch, train loss: 0.1865585297346115:  42%|█████▍       | 50/119 [00:30<00:41,  1.67it/s]Epoch: 7, train for the 25-th batch, train loss: 0.6322848200798035:  16%|██           | 24/151 [00:04<00:26,  4.72it/s]Epoch: 7, train for the 25-th batch, train loss: 0.6322848200798035:  17%|██▏          | 25/151 [00:04<00:26,  4.68it/s]Epoch: 2, train for the 26-th batch, train loss: 0.4604119062423706:   7%|▊            | 25/383 [00:14<03:33,  1.68it/s]Epoch: 2, train for the 26-th batch, train loss: 0.4604119062423706:   7%|▉            | 26/383 [00:14<03:31,  1.69it/s]Epoch: 2, train for the 218-th batch, train loss: 0.6541370749473572:  92%|██████████ | 217/237 [02:08<00:11,  1.68it/s]Epoch: 2, train for the 218-th batch, train loss: 0.6541370749473572:  92%|██████████ | 218/237 [02:08<00:11,  1.69it/s]Epoch: 7, train for the 26-th batch, train loss: 0.5378040671348572:  17%|██▏          | 25/151 [00:05<00:26,  4.68it/s]Epoch: 7, train for the 26-th batch, train loss: 0.5378040671348572:  17%|██▏          | 26/151 [00:05<00:26,  4.68it/s]Epoch: 3, train for the 146-th batch, train loss: 0.483944296836853:  99%|███████████▉| 145/146 [01:28<00:00,  1.67it/s]Epoch: 3, train for the 146-th batch, train loss: 0.483944296836853: 100%|████████████| 146/146 [01:28<00:00,  1.84it/s]Epoch: 3, train for the 146-th batch, train loss: 0.483944296836853: 100%|████████████| 146/146 [01:28<00:00,  1.66it/s]
  0%|                                                                                            | 0/38 [00:00<?, ?it/s]Epoch: 7, train for the 27-th batch, train loss: 0.5218405723571777:  17%|██▏          | 26/151 [00:05<00:26,  4.68it/s]Epoch: 7, train for the 27-th batch, train loss: 0.5218405723571777:  18%|██▎          | 27/151 [00:05<00:26,  4.69it/s]Epoch: 4, train for the 51-th batch, train loss: 0.2301396131515503:  42%|█████▍       | 50/119 [00:31<00:41,  1.67it/s]Epoch: 4, train for the 51-th batch, train loss: 0.2301396131515503:  43%|█████▌       | 51/119 [00:31<00:40,  1.67it/s]evaluate for the 1-th batch, evaluate loss: 0.49128684401512146:   0%|                           | 0/38 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.49128684401512146:   3%|▌                  | 1/38 [00:00<00:11,  3.21it/s]Epoch: 7, train for the 28-th batch, train loss: 0.6226581335067749:  18%|██▎          | 27/151 [00:05<00:26,  4.69it/s]Epoch: 7, train for the 28-th batch, train loss: 0.6226581335067749:  19%|██▍          | 28/151 [00:05<00:26,  4.65it/s]Epoch: 2, train for the 27-th batch, train loss: 0.39125892519950867:   7%|▊           | 26/383 [00:15<03:31,  1.69it/s]Epoch: 2, train for the 27-th batch, train loss: 0.39125892519950867:   7%|▊           | 27/383 [00:15<03:34,  1.66it/s]Epoch: 2, train for the 219-th batch, train loss: 0.6511842012405396:  92%|██████████ | 218/237 [02:09<00:11,  1.69it/s]Epoch: 2, train for the 219-th batch, train loss: 0.6511842012405396:  92%|██████████▏| 219/237 [02:09<00:10,  1.67it/s]evaluate for the 2-th batch, evaluate loss: 0.5061742663383484:   3%|▌                   | 1/38 [00:00<00:11,  3.21it/s]evaluate for the 2-th batch, evaluate loss: 0.5061742663383484:   5%|█                   | 2/38 [00:00<00:10,  3.57it/s]Epoch: 7, train for the 29-th batch, train loss: 0.6964647769927979:  19%|██▍          | 28/151 [00:05<00:26,  4.65it/s]Epoch: 7, train for the 29-th batch, train loss: 0.6964647769927979:  19%|██▍          | 29/151 [00:05<00:26,  4.61it/s]Epoch: 7, train for the 30-th batch, train loss: 0.6595728397369385:  19%|██▍          | 29/151 [00:06<00:26,  4.61it/s]Epoch: 7, train for the 30-th batch, train loss: 0.6595728397369385:  20%|██▌          | 30/151 [00:06<00:26,  4.58it/s]Epoch: 4, train for the 52-th batch, train loss: 0.2883760929107666:  43%|█████▌       | 51/119 [00:32<00:40,  1.67it/s]Epoch: 4, train for the 52-th batch, train loss: 0.2883760929107666:  44%|█████▋       | 52/119 [00:32<00:40,  1.67it/s]evaluate for the 3-th batch, evaluate loss: 0.501501739025116:   5%|█                    | 2/38 [00:00<00:10,  3.57it/s]evaluate for the 3-th batch, evaluate loss: 0.501501739025116:   8%|█▋                   | 3/38 [00:00<00:10,  3.34it/s]Epoch: 2, train for the 28-th batch, train loss: 0.43787917494773865:   7%|▊           | 27/383 [00:15<03:34,  1.66it/s]Epoch: 2, train for the 28-th batch, train loss: 0.43787917494773865:   7%|▉           | 28/383 [00:15<03:34,  1.65it/s]Epoch: 7, train for the 31-th batch, train loss: 0.49472999572753906:  20%|██▍         | 30/151 [00:06<00:26,  4.58it/s]Epoch: 7, train for the 31-th batch, train loss: 0.49472999572753906:  21%|██▍         | 31/151 [00:06<00:25,  4.65it/s]evaluate for the 4-th batch, evaluate loss: 0.4875252842903137:   8%|█▌                  | 3/38 [00:01<00:10,  3.34it/s]evaluate for the 4-th batch, evaluate loss: 0.4875252842903137:  11%|██                  | 4/38 [00:01<00:09,  3.47it/s]Epoch: 2, train for the 220-th batch, train loss: 0.6391171216964722:  92%|██████████▏| 219/237 [02:09<00:10,  1.67it/s]Epoch: 2, train for the 220-th batch, train loss: 0.6391171216964722:  93%|██████████▏| 220/237 [02:09<00:10,  1.66it/s]Epoch: 7, train for the 32-th batch, train loss: 0.5287266969680786:  21%|██▋          | 31/151 [00:06<00:25,  4.65it/s]Epoch: 7, train for the 32-th batch, train loss: 0.5287266969680786:  21%|██▊          | 32/151 [00:06<00:25,  4.65it/s]evaluate for the 5-th batch, evaluate loss: 0.519146203994751:  11%|██▏                  | 4/38 [00:01<00:09,  3.47it/s]evaluate for the 5-th batch, evaluate loss: 0.519146203994751:  13%|██▊                  | 5/38 [00:01<00:09,  3.43it/s]Epoch: 4, train for the 53-th batch, train loss: 0.1797005981206894:  44%|█████▋       | 52/119 [00:32<00:40,  1.67it/s]Epoch: 4, train for the 53-th batch, train loss: 0.1797005981206894:  45%|█████▊       | 53/119 [00:32<00:39,  1.67it/s]Epoch: 7, train for the 33-th batch, train loss: 0.6491746306419373:  21%|██▊          | 32/151 [00:06<00:25,  4.65it/s]Epoch: 7, train for the 33-th batch, train loss: 0.6491746306419373:  22%|██▊          | 33/151 [00:06<00:25,  4.61it/s]Epoch: 2, train for the 29-th batch, train loss: 0.3431117832660675:   7%|▉            | 28/383 [00:16<03:34,  1.65it/s]Epoch: 2, train for the 29-th batch, train loss: 0.3431117832660675:   8%|▉            | 29/383 [00:16<03:34,  1.65it/s]Epoch: 7, train for the 34-th batch, train loss: 0.4881356656551361:  22%|██▊          | 33/151 [00:06<00:25,  4.61it/s]Epoch: 7, train for the 34-th batch, train loss: 0.4881356656551361:  23%|██▉          | 34/151 [00:06<00:25,  4.63it/s]evaluate for the 6-th batch, evaluate loss: 0.5101116299629211:  13%|██▋                 | 5/38 [00:01<00:09,  3.43it/s]evaluate for the 6-th batch, evaluate loss: 0.5101116299629211:  16%|███▏                | 6/38 [00:01<00:08,  3.58it/s]Epoch: 2, train for the 221-th batch, train loss: 0.636501669883728:  93%|███████████▏| 220/237 [02:10<00:10,  1.66it/s]Epoch: 2, train for the 221-th batch, train loss: 0.636501669883728:  93%|███████████▏| 221/237 [02:10<00:09,  1.66it/s]Epoch: 7, train for the 35-th batch, train loss: 0.5088026523590088:  23%|██▉          | 34/151 [00:07<00:25,  4.63it/s]Epoch: 7, train for the 35-th batch, train loss: 0.5088026523590088:  23%|███          | 35/151 [00:07<00:24,  4.65it/s]evaluate for the 7-th batch, evaluate loss: 0.4742693305015564:  16%|███▏                | 6/38 [00:02<00:08,  3.58it/s]evaluate for the 7-th batch, evaluate loss: 0.4742693305015564:  18%|███▋                | 7/38 [00:02<00:08,  3.52it/s]Epoch: 4, train for the 54-th batch, train loss: 0.19857746362686157:  45%|█████▎      | 53/119 [00:33<00:39,  1.67it/s]Epoch: 4, train for the 54-th batch, train loss: 0.19857746362686157:  45%|█████▍      | 54/119 [00:33<00:38,  1.67it/s]Epoch: 7, train for the 36-th batch, train loss: 0.4542061388492584:  23%|███          | 35/151 [00:07<00:24,  4.65it/s]Epoch: 7, train for the 36-th batch, train loss: 0.4542061388492584:  24%|███          | 36/151 [00:07<00:24,  4.68it/s]Epoch: 2, train for the 30-th batch, train loss: 0.39210620522499084:   8%|▉           | 29/383 [00:16<03:34,  1.65it/s]Epoch: 2, train for the 30-th batch, train loss: 0.39210620522499084:   8%|▉           | 30/383 [00:16<03:30,  1.68it/s]evaluate for the 8-th batch, evaluate loss: 0.5532340407371521:  18%|███▋                | 7/38 [00:02<00:08,  3.52it/s]evaluate for the 8-th batch, evaluate loss: 0.5532340407371521:  21%|████▏               | 8/38 [00:02<00:08,  3.63it/s]Epoch: 7, train for the 37-th batch, train loss: 0.6354787945747375:  24%|███          | 36/151 [00:07<00:24,  4.68it/s]Epoch: 7, train for the 37-th batch, train loss: 0.6354787945747375:  25%|███▏         | 37/151 [00:07<00:24,  4.57it/s]Epoch: 2, train for the 222-th batch, train loss: 0.634809136390686:  93%|███████████▏| 221/237 [02:11<00:09,  1.66it/s]Epoch: 2, train for the 222-th batch, train loss: 0.634809136390686:  94%|███████████▏| 222/237 [02:11<00:08,  1.67it/s]evaluate for the 9-th batch, evaluate loss: 0.5211204290390015:  21%|████▏               | 8/38 [00:02<00:08,  3.63it/s]evaluate for the 9-th batch, evaluate loss: 0.5211204290390015:  24%|████▋               | 9/38 [00:02<00:08,  3.56it/s]Epoch: 7, train for the 38-th batch, train loss: 0.6277484893798828:  25%|███▏         | 37/151 [00:07<00:24,  4.57it/s]Epoch: 7, train for the 38-th batch, train loss: 0.6277484893798828:  25%|███▎         | 38/151 [00:07<00:24,  4.56it/s]Epoch: 4, train for the 55-th batch, train loss: 0.19694072008132935:  45%|█████▍      | 54/119 [00:33<00:38,  1.67it/s]Epoch: 4, train for the 55-th batch, train loss: 0.19694072008132935:  46%|█████▌      | 55/119 [00:33<00:38,  1.66it/s]Epoch: 2, train for the 31-th batch, train loss: 0.42218175530433655:   8%|▉           | 30/383 [00:17<03:30,  1.68it/s]Epoch: 2, train for the 31-th batch, train loss: 0.42218175530433655:   8%|▉           | 31/383 [00:17<03:29,  1.68it/s]Epoch: 7, train for the 39-th batch, train loss: 0.6681292653083801:  25%|███▎         | 38/151 [00:08<00:24,  4.56it/s]Epoch: 7, train for the 39-th batch, train loss: 0.6681292653083801:  26%|███▎         | 39/151 [00:08<00:24,  4.53it/s]evaluate for the 10-th batch, evaluate loss: 0.5310935378074646:  24%|████▌              | 9/38 [00:02<00:08,  3.56it/s]evaluate for the 10-th batch, evaluate loss: 0.5310935378074646:  26%|████▋             | 10/38 [00:02<00:07,  3.59it/s]Epoch: 2, train for the 223-th batch, train loss: 0.6223616600036621:  94%|██████████▎| 222/237 [02:11<00:08,  1.67it/s]Epoch: 2, train for the 223-th batch, train loss: 0.6223616600036621:  94%|██████████▎| 223/237 [02:11<00:08,  1.67it/s]Epoch: 7, train for the 40-th batch, train loss: 0.6215008497238159:  26%|███▎         | 39/151 [00:08<00:24,  4.53it/s]Epoch: 7, train for the 40-th batch, train loss: 0.6215008497238159:  26%|███▍         | 40/151 [00:08<00:24,  4.49it/s]evaluate for the 11-th batch, evaluate loss: 0.5159673094749451:  26%|████▋             | 10/38 [00:03<00:07,  3.59it/s]evaluate for the 11-th batch, evaluate loss: 0.5159673094749451:  29%|█████▏            | 11/38 [00:03<00:07,  3.61it/s]Epoch: 7, train for the 41-th batch, train loss: 0.43190595507621765:  26%|███▏        | 40/151 [00:08<00:24,  4.49it/s]Epoch: 7, train for the 41-th batch, train loss: 0.43190595507621765:  27%|███▎        | 41/151 [00:08<00:24,  4.55it/s]Epoch: 4, train for the 56-th batch, train loss: 0.21699675917625427:  46%|█████▌      | 55/119 [00:34<00:38,  1.66it/s]Epoch: 4, train for the 56-th batch, train loss: 0.21699675917625427:  47%|█████▋      | 56/119 [00:34<00:37,  1.66it/s]evaluate for the 12-th batch, evaluate loss: 0.5616092681884766:  29%|█████▏            | 11/38 [00:03<00:07,  3.61it/s]evaluate for the 12-th batch, evaluate loss: 0.5616092681884766:  32%|█████▋            | 12/38 [00:03<00:07,  3.52it/s]Epoch: 2, train for the 32-th batch, train loss: 0.3613232970237732:   8%|█            | 31/383 [00:18<03:29,  1.68it/s]Epoch: 2, train for the 32-th batch, train loss: 0.3613232970237732:   8%|█            | 32/383 [00:18<03:33,  1.65it/s]Epoch: 7, train for the 42-th batch, train loss: 0.4441387951374054:  27%|███▌         | 41/151 [00:08<00:24,  4.55it/s]Epoch: 7, train for the 42-th batch, train loss: 0.4441387951374054:  28%|███▌         | 42/151 [00:08<00:23,  4.58it/s]Epoch: 2, train for the 224-th batch, train loss: 0.665360152721405:  94%|███████████▎| 223/237 [02:12<00:08,  1.67it/s]Epoch: 2, train for the 224-th batch, train loss: 0.665360152721405:  95%|███████████▎| 224/237 [02:12<00:07,  1.65it/s]evaluate for the 13-th batch, evaluate loss: 0.5672274827957153:  32%|█████▋            | 12/38 [00:03<00:07,  3.52it/s]evaluate for the 13-th batch, evaluate loss: 0.5672274827957153:  34%|██████▏           | 13/38 [00:03<00:06,  3.66it/s]Epoch: 7, train for the 43-th batch, train loss: 0.640515148639679:  28%|███▉          | 42/151 [00:08<00:23,  4.58it/s]Epoch: 7, train for the 43-th batch, train loss: 0.640515148639679:  28%|███▉          | 43/151 [00:08<00:23,  4.55it/s]Epoch: 4, train for the 57-th batch, train loss: 0.18301041424274445:  47%|█████▋      | 56/119 [00:35<00:37,  1.66it/s]Epoch: 4, train for the 57-th batch, train loss: 0.18301041424274445:  48%|█████▋      | 57/119 [00:35<00:37,  1.67it/s]Epoch: 7, train for the 44-th batch, train loss: 0.5973165035247803:  28%|███▋         | 43/151 [00:09<00:23,  4.55it/s]Epoch: 7, train for the 44-th batch, train loss: 0.5973165035247803:  29%|███▊         | 44/151 [00:09<00:23,  4.52it/s]evaluate for the 14-th batch, evaluate loss: 0.49633726477622986:  34%|█████▊           | 13/38 [00:03<00:06,  3.66it/s]evaluate for the 14-th batch, evaluate loss: 0.49633726477622986:  37%|██████▎          | 14/38 [00:03<00:06,  3.54it/s]Epoch: 2, train for the 33-th batch, train loss: 0.36450594663619995:   8%|█           | 32/383 [00:18<03:33,  1.65it/s]Epoch: 2, train for the 33-th batch, train loss: 0.36450594663619995:   9%|█           | 33/383 [00:18<03:32,  1.65it/s]Epoch: 7, train for the 45-th batch, train loss: 0.48018720746040344:  29%|███▍        | 44/151 [00:09<00:23,  4.52it/s]Epoch: 7, train for the 45-th batch, train loss: 0.48018720746040344:  30%|███▌        | 45/151 [00:09<00:23,  4.55it/s]Epoch: 2, train for the 225-th batch, train loss: 0.6308228373527527:  95%|██████████▍| 224/237 [02:12<00:07,  1.65it/s]Epoch: 2, train for the 225-th batch, train loss: 0.6308228373527527:  95%|██████████▍| 225/237 [02:12<00:07,  1.65it/s]evaluate for the 15-th batch, evaluate loss: 0.5161657333374023:  37%|██████▋           | 14/38 [00:04<00:06,  3.54it/s]evaluate for the 15-th batch, evaluate loss: 0.5161657333374023:  39%|███████           | 15/38 [00:04<00:06,  3.72it/s]Epoch: 7, train for the 46-th batch, train loss: 0.5278732776641846:  30%|███▊         | 45/151 [00:09<00:23,  4.55it/s]Epoch: 7, train for the 46-th batch, train loss: 0.5278732776641846:  30%|███▉         | 46/151 [00:09<00:22,  4.57it/s]Epoch: 4, train for the 58-th batch, train loss: 0.16033941507339478:  48%|█████▋      | 57/119 [00:35<00:37,  1.67it/s]Epoch: 4, train for the 58-th batch, train loss: 0.16033941507339478:  49%|█████▊      | 58/119 [00:35<00:36,  1.66it/s]evaluate for the 16-th batch, evaluate loss: 0.5363300442695618:  39%|███████           | 15/38 [00:04<00:06,  3.72it/s]evaluate for the 16-th batch, evaluate loss: 0.5363300442695618:  42%|███████▌          | 16/38 [00:04<00:06,  3.54it/s]Epoch: 7, train for the 47-th batch, train loss: 0.6306895017623901:  30%|███▉         | 46/151 [00:09<00:22,  4.57it/s]Epoch: 7, train for the 47-th batch, train loss: 0.6306895017623901:  31%|████         | 47/151 [00:09<00:22,  4.55it/s]Epoch: 2, train for the 34-th batch, train loss: 0.3242350220680237:   9%|█            | 33/383 [00:19<03:32,  1.65it/s]Epoch: 2, train for the 34-th batch, train loss: 0.3242350220680237:   9%|█▏           | 34/383 [00:19<03:29,  1.66it/s]Epoch: 2, train for the 226-th batch, train loss: 0.6217182874679565:  95%|██████████▍| 225/237 [02:13<00:07,  1.65it/s]Epoch: 2, train for the 226-th batch, train loss: 0.6217182874679565:  95%|██████████▍| 226/237 [02:13<00:06,  1.66it/s]Epoch: 7, train for the 48-th batch, train loss: 0.5560770034790039:  31%|████         | 47/151 [00:10<00:22,  4.55it/s]Epoch: 7, train for the 48-th batch, train loss: 0.5560770034790039:  32%|████▏        | 48/151 [00:10<00:22,  4.54it/s]evaluate for the 17-th batch, evaluate loss: 0.5039528012275696:  42%|███████▌          | 16/38 [00:04<00:06,  3.54it/s]evaluate for the 17-th batch, evaluate loss: 0.5039528012275696:  45%|████████          | 17/38 [00:04<00:05,  3.59it/s]Epoch: 7, train for the 49-th batch, train loss: 0.37724438309669495:  32%|███▊        | 48/151 [00:10<00:22,  4.54it/s]Epoch: 7, train for the 49-th batch, train loss: 0.37724438309669495:  32%|███▉        | 49/151 [00:10<00:24,  4.15it/s]evaluate for the 18-th batch, evaluate loss: 0.5464376211166382:  45%|████████          | 17/38 [00:05<00:05,  3.59it/s]evaluate for the 18-th batch, evaluate loss: 0.5464376211166382:  47%|████████▌         | 18/38 [00:05<00:05,  3.45it/s]Epoch: 4, train for the 59-th batch, train loss: 0.19204092025756836:  49%|█████▊      | 58/119 [00:36<00:36,  1.66it/s]Epoch: 4, train for the 59-th batch, train loss: 0.19204092025756836:  50%|█████▉      | 59/119 [00:36<00:36,  1.66it/s]Epoch: 2, train for the 35-th batch, train loss: 0.38291963934898376:   9%|█           | 34/383 [00:19<03:29,  1.66it/s]Epoch: 2, train for the 35-th batch, train loss: 0.38291963934898376:   9%|█           | 35/383 [00:19<03:30,  1.65it/s]Epoch: 7, train for the 50-th batch, train loss: 0.5746884346008301:  32%|████▏        | 49/151 [00:10<00:24,  4.15it/s]Epoch: 7, train for the 50-th batch, train loss: 0.5746884346008301:  33%|████▎        | 50/151 [00:10<00:23,  4.24it/s]evaluate for the 19-th batch, evaluate loss: 0.513202965259552:  47%|█████████          | 18/38 [00:05<00:05,  3.45it/s]evaluate for the 19-th batch, evaluate loss: 0.513202965259552:  50%|█████████▌         | 19/38 [00:05<00:05,  3.55it/s]Epoch: 2, train for the 227-th batch, train loss: 0.6312156915664673:  95%|██████████▍| 226/237 [02:14<00:06,  1.66it/s]Epoch: 2, train for the 227-th batch, train loss: 0.6312156915664673:  96%|██████████▌| 227/237 [02:14<00:06,  1.65it/s]Epoch: 7, train for the 51-th batch, train loss: 0.5105346441268921:  33%|████▎        | 50/151 [00:10<00:23,  4.24it/s]Epoch: 7, train for the 51-th batch, train loss: 0.5105346441268921:  34%|████▍        | 51/151 [00:10<00:23,  4.33it/s]evaluate for the 20-th batch, evaluate loss: 0.49815428256988525:  50%|████████▌        | 19/38 [00:05<00:05,  3.55it/s]evaluate for the 20-th batch, evaluate loss: 0.49815428256988525:  53%|████████▉        | 20/38 [00:05<00:05,  3.50it/s]Epoch: 4, train for the 60-th batch, train loss: 0.17018592357635498:  50%|█████▉      | 59/119 [00:36<00:36,  1.66it/s]Epoch: 4, train for the 60-th batch, train loss: 0.17018592357635498:  50%|██████      | 60/119 [00:36<00:35,  1.66it/s]Epoch: 7, train for the 52-th batch, train loss: 0.5593889355659485:  34%|████▍        | 51/151 [00:10<00:23,  4.33it/s]Epoch: 7, train for the 52-th batch, train loss: 0.5593889355659485:  34%|████▍        | 52/151 [00:10<00:22,  4.40it/s]Epoch: 2, train for the 36-th batch, train loss: 0.38695448637008667:   9%|█           | 35/383 [00:20<03:30,  1.65it/s]Epoch: 2, train for the 36-th batch, train loss: 0.38695448637008667:   9%|█▏          | 36/383 [00:20<03:25,  1.69it/s]evaluate for the 21-th batch, evaluate loss: 0.5115828514099121:  53%|█████████▍        | 20/38 [00:05<00:05,  3.50it/s]evaluate for the 21-th batch, evaluate loss: 0.5115828514099121:  55%|█████████▉        | 21/38 [00:05<00:04,  3.60it/s]Epoch: 7, train for the 53-th batch, train loss: 0.6326536536216736:  34%|████▍        | 52/151 [00:11<00:22,  4.40it/s]Epoch: 7, train for the 53-th batch, train loss: 0.6326536536216736:  35%|████▌        | 53/151 [00:11<00:22,  4.43it/s]Epoch: 2, train for the 228-th batch, train loss: 0.6378802061080933:  96%|██████████▌| 227/237 [02:14<00:06,  1.65it/s]Epoch: 2, train for the 228-th batch, train loss: 0.6378802061080933:  96%|██████████▌| 228/237 [02:14<00:05,  1.67it/s]Epoch: 7, train for the 54-th batch, train loss: 0.5544145703315735:  35%|████▌        | 53/151 [00:11<00:22,  4.43it/s]Epoch: 7, train for the 54-th batch, train loss: 0.5544145703315735:  36%|████▋        | 54/151 [00:11<00:21,  4.43it/s]evaluate for the 22-th batch, evaluate loss: 0.5309423804283142:  55%|█████████▉        | 21/38 [00:06<00:04,  3.60it/s]evaluate for the 22-th batch, evaluate loss: 0.5309423804283142:  58%|██████████▍       | 22/38 [00:06<00:04,  3.56it/s]Epoch: 4, train for the 61-th batch, train loss: 0.16413567960262299:  50%|██████      | 60/119 [00:37<00:35,  1.66it/s]Epoch: 4, train for the 61-th batch, train loss: 0.16413567960262299:  51%|██████▏     | 61/119 [00:37<00:34,  1.66it/s]Epoch: 7, train for the 55-th batch, train loss: 0.553597092628479:  36%|█████         | 54/151 [00:11<00:21,  4.43it/s]Epoch: 7, train for the 55-th batch, train loss: 0.553597092628479:  36%|█████         | 55/151 [00:11<00:21,  4.45it/s]Epoch: 2, train for the 37-th batch, train loss: 0.3569990396499634:   9%|█▏           | 36/383 [00:21<03:25,  1.69it/s]Epoch: 2, train for the 37-th batch, train loss: 0.3569990396499634:  10%|█▎           | 37/383 [00:21<03:28,  1.66it/s]evaluate for the 23-th batch, evaluate loss: 0.5319471955299377:  58%|██████████▍       | 22/38 [00:06<00:04,  3.56it/s]evaluate for the 23-th batch, evaluate loss: 0.5319471955299377:  61%|██████████▉       | 23/38 [00:06<00:04,  3.64it/s]Epoch: 2, train for the 229-th batch, train loss: 0.6697344779968262:  96%|██████████▌| 228/237 [02:15<00:05,  1.67it/s]Epoch: 2, train for the 229-th batch, train loss: 0.6697344779968262:  97%|██████████▋| 229/237 [02:15<00:04,  1.66it/s]Epoch: 7, train for the 56-th batch, train loss: 0.5016897320747375:  36%|████▋        | 55/151 [00:11<00:21,  4.45it/s]Epoch: 7, train for the 56-th batch, train loss: 0.5016897320747375:  37%|████▊        | 56/151 [00:11<00:21,  4.47it/s]evaluate for the 24-th batch, evaluate loss: 0.5100874900817871:  61%|██████████▉       | 23/38 [00:06<00:04,  3.64it/s]evaluate for the 24-th batch, evaluate loss: 0.5100874900817871:  63%|███████████▎      | 24/38 [00:06<00:03,  3.59it/s]Epoch: 7, train for the 57-th batch, train loss: 0.5305665135383606:  37%|████▊        | 56/151 [00:12<00:21,  4.47it/s]Epoch: 7, train for the 57-th batch, train loss: 0.5305665135383606:  38%|████▉        | 57/151 [00:12<00:21,  4.47it/s]Epoch: 4, train for the 62-th batch, train loss: 0.23916827142238617:  51%|██████▏     | 61/119 [00:38<00:34,  1.66it/s]Epoch: 4, train for the 62-th batch, train loss: 0.23916827142238617:  52%|██████▎     | 62/119 [00:38<00:34,  1.66it/s]Epoch: 2, train for the 38-th batch, train loss: 0.41743841767311096:  10%|█▏          | 37/383 [00:21<03:28,  1.66it/s]Epoch: 2, train for the 38-th batch, train loss: 0.41743841767311096:  10%|█▏          | 38/383 [00:21<03:29,  1.65it/s]evaluate for the 25-th batch, evaluate loss: 0.525175154209137:  63%|████████████       | 24/38 [00:07<00:03,  3.59it/s]evaluate for the 25-th batch, evaluate loss: 0.525175154209137:  66%|████████████▌      | 25/38 [00:07<00:03,  3.53it/s]Epoch: 7, train for the 58-th batch, train loss: 0.5713092088699341:  38%|████▉        | 57/151 [00:12<00:21,  4.47it/s]Epoch: 7, train for the 58-th batch, train loss: 0.5713092088699341:  38%|████▉        | 58/151 [00:12<00:20,  4.48it/s]Epoch: 2, train for the 230-th batch, train loss: 0.6496325731277466:  97%|██████████▋| 229/237 [02:15<00:04,  1.66it/s]Epoch: 2, train for the 230-th batch, train loss: 0.6496325731277466:  97%|██████████▋| 230/237 [02:15<00:04,  1.65it/s]Epoch: 7, train for the 59-th batch, train loss: 0.5359058976173401:  38%|████▉        | 58/151 [00:12<00:20,  4.48it/s]Epoch: 7, train for the 59-th batch, train loss: 0.5359058976173401:  39%|█████        | 59/151 [00:12<00:20,  4.49it/s]evaluate for the 26-th batch, evaluate loss: 0.5144684314727783:  66%|███████████▊      | 25/38 [00:07<00:03,  3.53it/s]evaluate for the 26-th batch, evaluate loss: 0.5144684314727783:  68%|████████████▎     | 26/38 [00:07<00:03,  3.56it/s]Epoch: 4, train for the 63-th batch, train loss: 0.16592267155647278:  52%|██████▎     | 62/119 [00:38<00:34,  1.66it/s]Epoch: 4, train for the 63-th batch, train loss: 0.16592267155647278:  53%|██████▎     | 63/119 [00:38<00:33,  1.66it/s]Epoch: 7, train for the 60-th batch, train loss: 0.5027502179145813:  39%|█████        | 59/151 [00:12<00:20,  4.49it/s]Epoch: 7, train for the 60-th batch, train loss: 0.5027502179145813:  40%|█████▏       | 60/151 [00:12<00:20,  4.49it/s]Epoch: 2, train for the 39-th batch, train loss: 0.41477298736572266:  10%|█▏          | 38/383 [00:22<03:29,  1.65it/s]Epoch: 2, train for the 39-th batch, train loss: 0.41477298736572266:  10%|█▏          | 39/383 [00:22<03:25,  1.67it/s]evaluate for the 27-th batch, evaluate loss: 0.5428933501243591:  68%|████████████▎     | 26/38 [00:07<00:03,  3.56it/s]evaluate for the 27-th batch, evaluate loss: 0.5428933501243591:  71%|████████████▊     | 27/38 [00:07<00:03,  3.49it/s]Epoch: 7, train for the 61-th batch, train loss: 0.5264890789985657:  40%|█████▏       | 60/151 [00:12<00:20,  4.49it/s]Epoch: 7, train for the 61-th batch, train loss: 0.5264890789985657:  40%|█████▎       | 61/151 [00:12<00:20,  4.49it/s]Epoch: 2, train for the 231-th batch, train loss: 0.6406852602958679:  97%|██████████▋| 230/237 [02:16<00:04,  1.65it/s]Epoch: 2, train for the 231-th batch, train loss: 0.6406852602958679:  97%|██████████▋| 231/237 [02:16<00:03,  1.66it/s]evaluate for the 28-th batch, evaluate loss: 0.555963933467865:  71%|█████████████▌     | 27/38 [00:07<00:03,  3.49it/s]evaluate for the 28-th batch, evaluate loss: 0.555963933467865:  74%|██████████████     | 28/38 [00:07<00:02,  3.64it/s]Epoch: 7, train for the 62-th batch, train loss: 0.4626804292201996:  40%|█████▎       | 61/151 [00:13<00:20,  4.49it/s]Epoch: 7, train for the 62-th batch, train loss: 0.4626804292201996:  41%|█████▎       | 62/151 [00:13<00:19,  4.55it/s]Epoch: 4, train for the 64-th batch, train loss: 0.1370762437582016:  53%|██████▉      | 63/119 [00:39<00:33,  1.66it/s]Epoch: 4, train for the 64-th batch, train loss: 0.1370762437582016:  54%|██████▉      | 64/119 [00:39<00:33,  1.67it/s]Epoch: 7, train for the 63-th batch, train loss: 0.5874300599098206:  41%|█████▎       | 62/151 [00:13<00:19,  4.55it/s]Epoch: 7, train for the 63-th batch, train loss: 0.5874300599098206:  42%|█████▍       | 63/151 [00:13<00:19,  4.54it/s]evaluate for the 29-th batch, evaluate loss: 0.5238736867904663:  74%|█████████████▎    | 28/38 [00:08<00:02,  3.64it/s]evaluate for the 29-th batch, evaluate loss: 0.5238736867904663:  76%|█████████████▋    | 29/38 [00:08<00:02,  3.52it/s]Epoch: 2, train for the 40-th batch, train loss: 0.41845908761024475:  10%|█▏          | 39/383 [00:22<03:25,  1.67it/s]Epoch: 2, train for the 40-th batch, train loss: 0.41845908761024475:  10%|█▎          | 40/383 [00:22<03:23,  1.69it/s]Epoch: 2, train for the 232-th batch, train loss: 0.636337161064148:  97%|███████████▋| 231/237 [02:17<00:03,  1.66it/s]Epoch: 2, train for the 232-th batch, train loss: 0.636337161064148:  98%|███████████▋| 232/237 [02:17<00:02,  1.68it/s]Epoch: 7, train for the 64-th batch, train loss: 0.562862753868103:  42%|█████▊        | 63/151 [00:13<00:19,  4.54it/s]Epoch: 7, train for the 64-th batch, train loss: 0.562862753868103:  42%|█████▉        | 64/151 [00:13<00:19,  4.51it/s]evaluate for the 30-th batch, evaluate loss: 0.556342601776123:  76%|██████████████▌    | 29/38 [00:08<00:02,  3.52it/s]evaluate for the 30-th batch, evaluate loss: 0.556342601776123:  79%|███████████████    | 30/38 [00:08<00:02,  3.70it/s]Epoch: 7, train for the 65-th batch, train loss: 0.5002148747444153:  42%|█████▌       | 64/151 [00:13<00:19,  4.51it/s]Epoch: 7, train for the 65-th batch, train loss: 0.5002148747444153:  43%|█████▌       | 65/151 [00:13<00:19,  4.51it/s]Epoch: 4, train for the 65-th batch, train loss: 0.1820078194141388:  54%|██████▉      | 64/119 [00:39<00:33,  1.67it/s]Epoch: 4, train for the 65-th batch, train loss: 0.1820078194141388:  55%|███████      | 65/119 [00:39<00:32,  1.66it/s]evaluate for the 31-th batch, evaluate loss: 0.5419726371765137:  79%|██████████████▏   | 30/38 [00:08<00:02,  3.70it/s]evaluate for the 31-th batch, evaluate loss: 0.5419726371765137:  82%|██████████████▋   | 31/38 [00:08<00:01,  3.52it/s]Epoch: 2, train for the 41-th batch, train loss: 0.3424551784992218:  10%|█▎           | 40/383 [00:23<03:23,  1.69it/s]Epoch: 2, train for the 41-th batch, train loss: 0.3424551784992218:  11%|█▍           | 41/383 [00:23<03:26,  1.66it/s]Epoch: 7, train for the 66-th batch, train loss: 0.5228920578956604:  43%|█████▌       | 65/151 [00:14<00:19,  4.51it/s]Epoch: 7, train for the 66-th batch, train loss: 0.5228920578956604:  44%|█████▋       | 66/151 [00:14<00:18,  4.51it/s]Epoch: 2, train for the 233-th batch, train loss: 0.6436401009559631:  98%|██████████▊| 232/237 [02:17<00:02,  1.68it/s]Epoch: 2, train for the 233-th batch, train loss: 0.6436401009559631:  98%|██████████▊| 233/237 [02:17<00:02,  1.66it/s]evaluate for the 32-th batch, evaluate loss: 0.5031785368919373:  82%|██████████████▋   | 31/38 [00:08<00:01,  3.52it/s]evaluate for the 32-th batch, evaluate loss: 0.5031785368919373:  84%|███████████████▏  | 32/38 [00:08<00:01,  3.58it/s]Epoch: 7, train for the 67-th batch, train loss: 0.5904677510261536:  44%|█████▋       | 66/151 [00:14<00:18,  4.51it/s]Epoch: 7, train for the 67-th batch, train loss: 0.5904677510261536:  44%|█████▊       | 67/151 [00:14<00:18,  4.51it/s]Epoch: 7, train for the 68-th batch, train loss: 0.5772306323051453:  44%|█████▊       | 67/151 [00:14<00:18,  4.51it/s]Epoch: 7, train for the 68-th batch, train loss: 0.5772306323051453:  45%|█████▊       | 68/151 [00:14<00:18,  4.51it/s]evaluate for the 33-th batch, evaluate loss: 0.5109952688217163:  84%|███████████████▏  | 32/38 [00:09<00:01,  3.58it/s]evaluate for the 33-th batch, evaluate loss: 0.5109952688217163:  87%|███████████████▋  | 33/38 [00:09<00:01,  3.48it/s]Epoch: 4, train for the 66-th batch, train loss: 0.20382952690124512:  55%|██████▌     | 65/119 [00:40<00:32,  1.66it/s]Epoch: 4, train for the 66-th batch, train loss: 0.20382952690124512:  55%|██████▋     | 66/119 [00:40<00:31,  1.66it/s]Epoch: 2, train for the 42-th batch, train loss: 0.35122641921043396:  11%|█▎          | 41/383 [00:24<03:26,  1.66it/s]Epoch: 2, train for the 42-th batch, train loss: 0.35122641921043396:  11%|█▎          | 42/383 [00:24<03:24,  1.67it/s]Epoch: 7, train for the 69-th batch, train loss: 0.5847585201263428:  45%|█████▊       | 68/151 [00:14<00:18,  4.51it/s]Epoch: 7, train for the 69-th batch, train loss: 0.5847585201263428:  46%|█████▉       | 69/151 [00:14<00:18,  4.51it/s]evaluate for the 34-th batch, evaluate loss: 0.5301939249038696:  87%|███████████████▋  | 33/38 [00:09<00:01,  3.48it/s]evaluate for the 34-th batch, evaluate loss: 0.5301939249038696:  89%|████████████████  | 34/38 [00:09<00:01,  3.60it/s]Epoch: 2, train for the 234-th batch, train loss: 0.6183058619499207:  98%|██████████▊| 233/237 [02:18<00:02,  1.66it/s]Epoch: 2, train for the 234-th batch, train loss: 0.6183058619499207:  99%|██████████▊| 234/237 [02:18<00:01,  1.67it/s]Epoch: 7, train for the 70-th batch, train loss: 0.579235851764679:  46%|██████▍       | 69/151 [00:14<00:18,  4.51it/s]Epoch: 7, train for the 70-th batch, train loss: 0.579235851764679:  46%|██████▍       | 70/151 [00:14<00:17,  4.50it/s]evaluate for the 35-th batch, evaluate loss: 0.5671180486679077:  89%|████████████████  | 34/38 [00:09<00:01,  3.60it/s]evaluate for the 35-th batch, evaluate loss: 0.5671180486679077:  92%|████████████████▌ | 35/38 [00:09<00:00,  3.54it/s]Epoch: 4, train for the 67-th batch, train loss: 0.2606906592845917:  55%|███████▏     | 66/119 [00:41<00:31,  1.66it/s]Epoch: 4, train for the 67-th batch, train loss: 0.2606906592845917:  56%|███████▎     | 67/119 [00:41<00:31,  1.67it/s]Epoch: 7, train for the 71-th batch, train loss: 0.5548076033592224:  46%|██████       | 70/151 [00:15<00:17,  4.50it/s]Epoch: 7, train for the 71-th batch, train loss: 0.5548076033592224:  47%|██████       | 71/151 [00:15<00:17,  4.50it/s]Epoch: 2, train for the 43-th batch, train loss: 0.3418843150138855:  11%|█▍           | 42/383 [00:24<03:24,  1.67it/s]Epoch: 2, train for the 43-th batch, train loss: 0.3418843150138855:  11%|█▍           | 43/383 [00:24<03:26,  1.64it/s]evaluate for the 36-th batch, evaluate loss: 0.5860036611557007:  92%|████████████████▌ | 35/38 [00:10<00:00,  3.54it/s]evaluate for the 36-th batch, evaluate loss: 0.5860036611557007:  95%|█████████████████ | 36/38 [00:10<00:00,  3.63it/s]Epoch: 7, train for the 72-th batch, train loss: 0.5493561625480652:  47%|██████       | 71/151 [00:15<00:17,  4.50it/s]Epoch: 7, train for the 72-th batch, train loss: 0.5493561625480652:  48%|██████▏      | 72/151 [00:15<00:17,  4.51it/s]Epoch: 2, train for the 235-th batch, train loss: 0.6118702292442322:  99%|██████████▊| 234/237 [02:18<00:01,  1.67it/s]Epoch: 2, train for the 235-th batch, train loss: 0.6118702292442322:  99%|██████████▉| 235/237 [02:18<00:01,  1.65it/s]Epoch: 7, train for the 73-th batch, train loss: 0.5664030313491821:  48%|██████▏      | 72/151 [00:15<00:17,  4.51it/s]Epoch: 7, train for the 73-th batch, train loss: 0.5664030313491821:  48%|██████▎      | 73/151 [00:15<00:17,  4.50it/s]evaluate for the 37-th batch, evaluate loss: 0.5207936763763428:  95%|█████████████████ | 36/38 [00:10<00:00,  3.63it/s]evaluate for the 37-th batch, evaluate loss: 0.5207936763763428:  97%|█████████████████▌| 37/38 [00:10<00:00,  3.59it/s]Epoch: 4, train for the 68-th batch, train loss: 0.15801358222961426:  56%|██████▊     | 67/119 [00:41<00:31,  1.67it/s]Epoch: 4, train for the 68-th batch, train loss: 0.15801358222961426:  57%|██████▊     | 68/119 [00:41<00:30,  1.66it/s]Epoch: 7, train for the 74-th batch, train loss: 0.4807755947113037:  48%|██████▎      | 73/151 [00:15<00:17,  4.50it/s]Epoch: 7, train for the 74-th batch, train loss: 0.4807755947113037:  49%|██████▎      | 74/151 [00:15<00:17,  4.51it/s]Epoch: 2, train for the 44-th batch, train loss: 0.34508636593818665:  11%|█▎          | 43/383 [00:25<03:26,  1.64it/s]Epoch: 2, train for the 44-th batch, train loss: 0.34508636593818665:  11%|█▍          | 44/383 [00:25<03:26,  1.64it/s]evaluate for the 38-th batch, evaluate loss: 0.5240541100502014:  97%|█████████████████▌| 37/38 [00:10<00:00,  3.59it/s]evaluate for the 38-th batch, evaluate loss: 0.5240541100502014: 100%|██████████████████| 38/38 [00:10<00:00,  3.63it/s]evaluate for the 38-th batch, evaluate loss: 0.5240541100502014: 100%|██████████████████| 38/38 [00:10<00:00,  3.56it/s]
  0%|                                                                                            | 0/20 [00:00<?, ?it/s]Epoch: 2, train for the 236-th batch, train loss: 0.6057153344154358:  99%|██████████▉| 235/237 [02:19<00:01,  1.65it/s]Epoch: 2, train for the 236-th batch, train loss: 0.6057153344154358: 100%|██████████▉| 236/237 [02:19<00:00,  1.65it/s]Epoch: 7, train for the 75-th batch, train loss: 0.5124257802963257:  49%|██████▎      | 74/151 [00:16<00:17,  4.51it/s]Epoch: 7, train for the 75-th batch, train loss: 0.5124257802963257:  50%|██████▍      | 75/151 [00:16<00:16,  4.51it/s]evaluate for the 1-th batch, evaluate loss: 0.5735658407211304:   0%|                            | 0/20 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.5735658407211304:   5%|█                   | 1/20 [00:00<00:05,  3.55it/s]Epoch: 7, train for the 76-th batch, train loss: 0.5710657238960266:  50%|██████▍      | 75/151 [00:16<00:16,  4.51it/s]Epoch: 7, train for the 76-th batch, train loss: 0.5710657238960266:  50%|██████▌      | 76/151 [00:16<00:16,  4.51it/s]Epoch: 4, train for the 69-th batch, train loss: 0.194793701171875:  57%|████████      | 68/119 [00:42<00:30,  1.66it/s]Epoch: 4, train for the 69-th batch, train loss: 0.194793701171875:  58%|████████      | 69/119 [00:42<00:30,  1.66it/s]evaluate for the 2-th batch, evaluate loss: 0.6254224181175232:   5%|█                   | 1/20 [00:00<00:05,  3.55it/s]evaluate for the 2-th batch, evaluate loss: 0.6254224181175232:  10%|██                  | 2/20 [00:00<00:05,  3.39it/s]Epoch: 7, train for the 77-th batch, train loss: 0.43199077248573303:  50%|██████      | 76/151 [00:16<00:16,  4.51it/s]Epoch: 7, train for the 77-th batch, train loss: 0.43199077248573303:  51%|██████      | 77/151 [00:16<00:16,  4.54it/s]Epoch: 2, train for the 45-th batch, train loss: 0.3675389289855957:  11%|█▍           | 44/383 [00:25<03:26,  1.64it/s]Epoch: 2, train for the 45-th batch, train loss: 0.3675389289855957:  12%|█▌           | 45/383 [00:25<03:28,  1.62it/s]Epoch: 2, train for the 237-th batch, train loss: 0.6278513669967651: 100%|██████████▉| 236/237 [02:19<00:00,  1.65it/s]Epoch: 2, train for the 237-th batch, train loss: 0.6278513669967651: 100%|███████████| 237/237 [02:19<00:00,  1.76it/s]Epoch: 2, train for the 237-th batch, train loss: 0.6278513669967651: 100%|███████████| 237/237 [02:19<00:00,  1.69it/s]
  0%|                                                                                            | 0/66 [00:00<?, ?it/s]Epoch: 7, train for the 78-th batch, train loss: 0.47654086351394653:  51%|██████      | 77/151 [00:16<00:16,  4.54it/s]Epoch: 7, train for the 78-th batch, train loss: 0.47654086351394653:  52%|██████▏     | 78/151 [00:16<00:16,  4.54it/s]evaluate for the 3-th batch, evaluate loss: 0.6085286140441895:  10%|██                  | 2/20 [00:00<00:05,  3.39it/s]evaluate for the 3-th batch, evaluate loss: 0.6085286140441895:  15%|███                 | 3/20 [00:00<00:04,  3.49it/s]evaluate for the 1-th batch, evaluate loss: 0.5802068710327148:   0%|                            | 0/66 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.5802068710327148:   2%|▎                   | 1/66 [00:00<00:17,  3.69it/s]Epoch: 4, train for the 70-th batch, train loss: 0.1434430480003357:  58%|███████▌     | 69/119 [00:42<00:30,  1.66it/s]Epoch: 4, train for the 70-th batch, train loss: 0.1434430480003357:  59%|███████▋     | 70/119 [00:42<00:29,  1.68it/s]Epoch: 7, train for the 79-th batch, train loss: 0.5934455394744873:  52%|██████▋      | 78/151 [00:16<00:16,  4.54it/s]Epoch: 7, train for the 79-th batch, train loss: 0.5934455394744873:  52%|██████▊      | 79/151 [00:16<00:15,  4.53it/s]evaluate for the 4-th batch, evaluate loss: 0.5768239498138428:  15%|███                 | 3/20 [00:01<00:04,  3.49it/s]evaluate for the 4-th batch, evaluate loss: 0.5768239498138428:  20%|████                | 4/20 [00:01<00:04,  3.41it/s]evaluate for the 2-th batch, evaluate loss: 0.5685538649559021:   2%|▎                   | 1/66 [00:00<00:17,  3.69it/s]evaluate for the 2-th batch, evaluate loss: 0.5685538649559021:   3%|▌                   | 2/66 [00:00<00:18,  3.54it/s]Epoch: 2, train for the 46-th batch, train loss: 0.4301358759403229:  12%|█▌           | 45/383 [00:26<03:28,  1.62it/s]Epoch: 2, train for the 46-th batch, train loss: 0.4301358759403229:  12%|█▌           | 46/383 [00:26<03:22,  1.67it/s]Epoch: 7, train for the 80-th batch, train loss: 0.5379031300544739:  52%|██████▊      | 79/151 [00:17<00:15,  4.53it/s]Epoch: 7, train for the 80-th batch, train loss: 0.5379031300544739:  53%|██████▉      | 80/151 [00:17<00:15,  4.52it/s]evaluate for the 5-th batch, evaluate loss: 0.601863443851471:  20%|████▏                | 4/20 [00:01<00:04,  3.41it/s]evaluate for the 5-th batch, evaluate loss: 0.601863443851471:  25%|█████▎               | 5/20 [00:01<00:04,  3.52it/s]evaluate for the 3-th batch, evaluate loss: 0.5971462726593018:   3%|▌                   | 2/66 [00:00<00:18,  3.54it/s]evaluate for the 3-th batch, evaluate loss: 0.5971462726593018:   5%|▉                   | 3/66 [00:00<00:17,  3.67it/s]Epoch: 7, train for the 81-th batch, train loss: 0.45551785826683044:  53%|██████▎     | 80/151 [00:17<00:15,  4.52it/s]Epoch: 7, train for the 81-th batch, train loss: 0.45551785826683044:  54%|██████▍     | 81/151 [00:17<00:15,  4.57it/s]Epoch: 4, train for the 71-th batch, train loss: 0.2107311189174652:  59%|███████▋     | 70/119 [00:43<00:29,  1.68it/s]Epoch: 4, train for the 71-th batch, train loss: 0.2107311189174652:  60%|███████▊     | 71/119 [00:43<00:28,  1.68it/s]Epoch: 7, train for the 82-th batch, train loss: 0.5790315270423889:  54%|██████▉      | 81/151 [00:17<00:15,  4.57it/s]Epoch: 7, train for the 82-th batch, train loss: 0.5790315270423889:  54%|███████      | 82/151 [00:17<00:15,  4.55it/s]evaluate for the 4-th batch, evaluate loss: 0.5989423990249634:   5%|▉                   | 3/66 [00:01<00:17,  3.67it/s]evaluate for the 4-th batch, evaluate loss: 0.5989423990249634:   6%|█▏                  | 4/66 [00:01<00:17,  3.59it/s]evaluate for the 6-th batch, evaluate loss: 0.6209030151367188:  25%|█████               | 5/20 [00:01<00:04,  3.52it/s]evaluate for the 6-th batch, evaluate loss: 0.6209030151367188:  30%|██████              | 6/20 [00:01<00:04,  3.43it/s]Epoch: 2, train for the 47-th batch, train loss: 0.37951818108558655:  12%|█▍          | 46/383 [00:27<03:22,  1.67it/s]Epoch: 2, train for the 47-th batch, train loss: 0.37951818108558655:  12%|█▍          | 47/383 [00:27<03:19,  1.68it/s]Epoch: 7, train for the 83-th batch, train loss: 0.536668598651886:  54%|███████▌      | 82/151 [00:17<00:15,  4.55it/s]Epoch: 7, train for the 83-th batch, train loss: 0.536668598651886:  55%|███████▋      | 83/151 [00:17<00:14,  4.54it/s]evaluate for the 5-th batch, evaluate loss: 0.607975423336029:   6%|█▎                   | 4/66 [00:01<00:17,  3.59it/s]evaluate for the 5-th batch, evaluate loss: 0.607975423336029:   8%|█▌                   | 5/66 [00:01<00:15,  3.96it/s]evaluate for the 7-th batch, evaluate loss: 0.6693464517593384:  30%|██████              | 6/20 [00:01<00:04,  3.43it/s]evaluate for the 7-th batch, evaluate loss: 0.6693464517593384:  35%|███████             | 7/20 [00:01<00:03,  3.58it/s]evaluate for the 6-th batch, evaluate loss: 0.6365244388580322:   8%|█▌                  | 5/66 [00:01<00:15,  3.96it/s]evaluate for the 6-th batch, evaluate loss: 0.6365244388580322:   9%|█▊                  | 6/66 [00:01<00:13,  4.41it/s]Epoch: 7, train for the 84-th batch, train loss: 0.5793948173522949:  55%|███████▏     | 83/151 [00:18<00:14,  4.54it/s]Epoch: 7, train for the 84-th batch, train loss: 0.5793948173522949:  56%|███████▏     | 84/151 [00:18<00:14,  4.52it/s]Epoch: 4, train for the 72-th batch, train loss: 0.24227029085159302:  60%|███████▏    | 71/119 [00:44<00:28,  1.68it/s]Epoch: 4, train for the 72-th batch, train loss: 0.24227029085159302:  61%|███████▎    | 72/119 [00:44<00:27,  1.70it/s]evaluate for the 8-th batch, evaluate loss: 0.6498982906341553:  35%|███████             | 7/20 [00:02<00:03,  3.58it/s]evaluate for the 8-th batch, evaluate loss: 0.6498982906341553:  40%|████████            | 8/20 [00:02<00:03,  3.46it/s]Epoch: 7, train for the 85-th batch, train loss: 0.5657641291618347:  56%|███████▏     | 84/151 [00:18<00:14,  4.52it/s]Epoch: 7, train for the 85-th batch, train loss: 0.5657641291618347:  56%|███████▎     | 85/151 [00:18<00:14,  4.51it/s]evaluate for the 7-th batch, evaluate loss: 0.606231153011322:   9%|█▉                   | 6/66 [00:01<00:13,  4.41it/s]evaluate for the 7-th batch, evaluate loss: 0.606231153011322:  11%|██▏                  | 7/66 [00:01<00:14,  4.04it/s]Epoch: 2, train for the 48-th batch, train loss: 0.33727195858955383:  12%|█▍          | 47/383 [00:27<03:19,  1.68it/s]Epoch: 2, train for the 48-th batch, train loss: 0.33727195858955383:  13%|█▌          | 48/383 [00:27<03:27,  1.61it/s]evaluate for the 9-th batch, evaluate loss: 0.5963169932365417:  40%|████████            | 8/20 [00:02<00:03,  3.46it/s]evaluate for the 9-th batch, evaluate loss: 0.5963169932365417:  45%|█████████           | 9/20 [00:02<00:03,  3.61it/s]Epoch: 7, train for the 86-th batch, train loss: 0.5564897656440735:  56%|███████▎     | 85/151 [00:18<00:14,  4.51it/s]Epoch: 7, train for the 86-th batch, train loss: 0.5564897656440735:  57%|███████▍     | 86/151 [00:18<00:14,  4.51it/s]evaluate for the 8-th batch, evaluate loss: 0.6030063629150391:  11%|██                  | 7/66 [00:02<00:14,  4.04it/s]evaluate for the 8-th batch, evaluate loss: 0.6030063629150391:  12%|██▍                 | 8/66 [00:02<00:14,  4.02it/s]Epoch: 4, train for the 73-th batch, train loss: 0.20841743052005768:  61%|███████▎    | 72/119 [00:44<00:27,  1.70it/s]Epoch: 4, train for the 73-th batch, train loss: 0.20841743052005768:  61%|███████▎    | 73/119 [00:44<00:26,  1.71it/s]Epoch: 7, train for the 87-th batch, train loss: 0.5448179244995117:  57%|███████▍     | 86/151 [00:18<00:14,  4.51it/s]Epoch: 7, train for the 87-th batch, train loss: 0.5448179244995117:  58%|███████▍     | 87/151 [00:18<00:14,  4.51it/s]evaluate for the 10-th batch, evaluate loss: 0.6063566207885742:  45%|████████▌          | 9/20 [00:02<00:03,  3.61it/s]evaluate for the 10-th batch, evaluate loss: 0.6063566207885742:  50%|█████████         | 10/20 [00:02<00:02,  3.49it/s]evaluate for the 9-th batch, evaluate loss: 0.557195782661438:  12%|██▌                  | 8/66 [00:02<00:14,  4.02it/s]evaluate for the 9-th batch, evaluate loss: 0.557195782661438:  14%|██▊                  | 9/66 [00:02<00:14,  3.84it/s]Epoch: 7, train for the 88-th batch, train loss: 0.5342723727226257:  58%|███████▍     | 87/151 [00:18<00:14,  4.51it/s]Epoch: 7, train for the 88-th batch, train loss: 0.5342723727226257:  58%|███████▌     | 88/151 [00:18<00:13,  4.55it/s]Epoch: 2, train for the 49-th batch, train loss: 0.3600766658782959:  13%|█▋           | 48/383 [00:28<03:27,  1.61it/s]Epoch: 2, train for the 49-th batch, train loss: 0.3600766658782959:  13%|█▋           | 49/383 [00:28<03:24,  1.64it/s]evaluate for the 11-th batch, evaluate loss: 0.5996338725090027:  50%|█████████         | 10/20 [00:03<00:02,  3.49it/s]evaluate for the 11-th batch, evaluate loss: 0.5996338725090027:  55%|█████████▉        | 11/20 [00:03<00:02,  3.63it/s]evaluate for the 10-th batch, evaluate loss: 0.571110188961029:  14%|██▋                 | 9/66 [00:02<00:14,  3.84it/s]evaluate for the 10-th batch, evaluate loss: 0.571110188961029:  15%|██▉                | 10/66 [00:02<00:14,  3.85it/s]Epoch: 7, train for the 89-th batch, train loss: 0.5275087356567383:  58%|███████▌     | 88/151 [00:19<00:13,  4.55it/s]Epoch: 7, train for the 89-th batch, train loss: 0.5275087356567383:  59%|███████▋     | 89/151 [00:19<00:13,  4.55it/s]Epoch: 4, train for the 74-th batch, train loss: 0.24561981856822968:  61%|███████▎    | 73/119 [00:45<00:26,  1.71it/s]Epoch: 4, train for the 74-th batch, train loss: 0.24561981856822968:  62%|███████▍    | 74/119 [00:45<00:26,  1.71it/s]evaluate for the 12-th batch, evaluate loss: 0.6269767880439758:  55%|█████████▉        | 11/20 [00:03<00:02,  3.63it/s]evaluate for the 12-th batch, evaluate loss: 0.6269767880439758:  60%|██████████▊       | 12/20 [00:03<00:02,  3.50it/s]Epoch: 7, train for the 90-th batch, train loss: 0.5701954364776611:  59%|███████▋     | 89/151 [00:19<00:13,  4.55it/s]Epoch: 7, train for the 90-th batch, train loss: 0.5701954364776611:  60%|███████▋     | 90/151 [00:19<00:13,  4.52it/s]evaluate for the 11-th batch, evaluate loss: 0.571783185005188:  15%|██▉                | 10/66 [00:02<00:14,  3.85it/s]evaluate for the 11-th batch, evaluate loss: 0.571783185005188:  17%|███▏               | 11/66 [00:02<00:14,  3.75it/s]Epoch: 2, train for the 50-th batch, train loss: 0.342492014169693:  13%|█▊            | 49/383 [00:28<03:24,  1.64it/s]Epoch: 2, train for the 50-th batch, train loss: 0.342492014169693:  13%|█▊            | 50/383 [00:28<03:22,  1.65it/s]evaluate for the 13-th batch, evaluate loss: 0.6356645226478577:  60%|██████████▊       | 12/20 [00:03<00:02,  3.50it/s]evaluate for the 13-th batch, evaluate loss: 0.6356645226478577:  65%|███████████▋      | 13/20 [00:03<00:01,  3.65it/s]Epoch: 7, train for the 91-th batch, train loss: 0.5148223042488098:  60%|███████▋     | 90/151 [00:19<00:13,  4.52it/s]Epoch: 7, train for the 91-th batch, train loss: 0.5148223042488098:  60%|███████▊     | 91/151 [00:19<00:13,  4.52it/s]evaluate for the 12-th batch, evaluate loss: 0.61306232213974:  17%|███▎                | 11/66 [00:03<00:14,  3.75it/s]evaluate for the 12-th batch, evaluate loss: 0.61306232213974:  18%|███▋                | 12/66 [00:03<00:14,  3.65it/s]Epoch: 7, train for the 92-th batch, train loss: 0.5794300436973572:  60%|███████▊     | 91/151 [00:19<00:13,  4.52it/s]Epoch: 7, train for the 92-th batch, train loss: 0.5794300436973572:  61%|███████▉     | 92/151 [00:19<00:13,  4.51it/s]Epoch: 4, train for the 75-th batch, train loss: 0.18895156681537628:  62%|███████▍    | 74/119 [00:45<00:26,  1.71it/s]Epoch: 4, train for the 75-th batch, train loss: 0.18895156681537628:  63%|███████▌    | 75/119 [00:45<00:25,  1.71it/s]evaluate for the 14-th batch, evaluate loss: 0.6269077062606812:  65%|███████████▋      | 13/20 [00:03<00:01,  3.65it/s]evaluate for the 14-th batch, evaluate loss: 0.6269077062606812:  70%|████████████▌     | 14/20 [00:03<00:01,  3.48it/s]evaluate for the 13-th batch, evaluate loss: 0.5943566560745239:  18%|███▎              | 12/66 [00:03<00:14,  3.65it/s]evaluate for the 13-th batch, evaluate loss: 0.5943566560745239:  20%|███▌              | 13/66 [00:03<00:14,  3.67it/s]Epoch: 7, train for the 93-th batch, train loss: 0.5255032181739807:  61%|███████▉     | 92/151 [00:20<00:13,  4.51it/s]Epoch: 7, train for the 93-th batch, train loss: 0.5255032181739807:  62%|████████     | 93/151 [00:20<00:12,  4.51it/s]Epoch: 2, train for the 51-th batch, train loss: 0.3448318541049957:  13%|█▋           | 50/383 [00:29<03:22,  1.65it/s]Epoch: 2, train for the 51-th batch, train loss: 0.3448318541049957:  13%|█▋           | 51/383 [00:29<03:19,  1.66it/s]evaluate for the 15-th batch, evaluate loss: 0.6126051545143127:  70%|████████████▌     | 14/20 [00:04<00:01,  3.48it/s]evaluate for the 15-th batch, evaluate loss: 0.6126051545143127:  75%|█████████████▌    | 15/20 [00:04<00:01,  3.61it/s]evaluate for the 14-th batch, evaluate loss: 0.5913551449775696:  20%|███▌              | 13/66 [00:03<00:14,  3.67it/s]evaluate for the 14-th batch, evaluate loss: 0.5913551449775696:  21%|███▊              | 14/66 [00:03<00:14,  3.58it/s]Epoch: 4, train for the 76-th batch, train loss: 0.22241896390914917:  63%|███████▌    | 75/119 [00:46<00:25,  1.71it/s]Epoch: 4, train for the 76-th batch, train loss: 0.22241896390914917:  64%|███████▋    | 76/119 [00:46<00:25,  1.69it/s]evaluate for the 16-th batch, evaluate loss: 0.5914228558540344:  75%|█████████████▌    | 15/20 [00:04<00:01,  3.61it/s]evaluate for the 16-th batch, evaluate loss: 0.5914228558540344:  80%|██████████████▍   | 16/20 [00:04<00:01,  3.43it/s]evaluate for the 15-th batch, evaluate loss: 0.6250423192977905:  21%|███▊              | 14/66 [00:03<00:14,  3.58it/s]evaluate for the 15-th batch, evaluate loss: 0.6250423192977905:  23%|████              | 15/66 [00:03<00:13,  3.72it/s]Epoch: 7, train for the 94-th batch, train loss: 0.5671740770339966:  62%|████████     | 93/151 [00:20<00:12,  4.51it/s]Epoch: 7, train for the 94-th batch, train loss: 0.5671740770339966:  62%|████████     | 94/151 [00:20<00:16,  3.44it/s]Epoch: 2, train for the 52-th batch, train loss: 0.422491192817688:  13%|█▊            | 51/383 [00:30<03:19,  1.66it/s]Epoch: 2, train for the 52-th batch, train loss: 0.422491192817688:  14%|█▉            | 52/383 [00:30<03:17,  1.68it/s]Epoch: 7, train for the 95-th batch, train loss: 0.5265015959739685:  62%|████████     | 94/151 [00:20<00:16,  3.44it/s]Epoch: 7, train for the 95-th batch, train loss: 0.5265015959739685:  63%|████████▏    | 95/151 [00:20<00:15,  3.69it/s]evaluate for the 17-th batch, evaluate loss: 0.5962410569190979:  80%|██████████████▍   | 16/20 [00:04<00:01,  3.43it/s]evaluate for the 17-th batch, evaluate loss: 0.5962410569190979:  85%|███████████████▎  | 17/20 [00:04<00:00,  3.52it/s]evaluate for the 16-th batch, evaluate loss: 0.5631127953529358:  23%|████              | 15/66 [00:04<00:13,  3.72it/s]evaluate for the 16-th batch, evaluate loss: 0.5631127953529358:  24%|████▎             | 16/66 [00:04<00:13,  3.60it/s]Epoch: 7, train for the 96-th batch, train loss: 0.5356101393699646:  63%|████████▏    | 95/151 [00:20<00:15,  3.69it/s]Epoch: 7, train for the 96-th batch, train loss: 0.5356101393699646:  64%|████████▎    | 96/151 [00:20<00:14,  3.90it/s]evaluate for the 17-th batch, evaluate loss: 0.5954213738441467:  24%|████▎             | 16/66 [00:04<00:13,  3.60it/s]evaluate for the 17-th batch, evaluate loss: 0.5954213738441467:  26%|████▋             | 17/66 [00:04<00:13,  3.77it/s]Epoch: 4, train for the 77-th batch, train loss: 0.1761450320482254:  64%|████████▎    | 76/119 [00:47<00:25,  1.69it/s]Epoch: 4, train for the 77-th batch, train loss: 0.1761450320482254:  65%|████████▍    | 77/119 [00:47<00:24,  1.68it/s]evaluate for the 18-th batch, evaluate loss: 0.6634194850921631:  85%|███████████████▎  | 17/20 [00:05<00:00,  3.52it/s]evaluate for the 18-th batch, evaluate loss: 0.6634194850921631:  90%|████████████████▏ | 18/20 [00:05<00:00,  3.36it/s]Epoch: 7, train for the 97-th batch, train loss: 0.6041719913482666:  64%|████████▎    | 96/151 [00:21<00:14,  3.90it/s]Epoch: 7, train for the 97-th batch, train loss: 0.6041719913482666:  64%|████████▎    | 97/151 [00:21<00:13,  4.06it/s]Epoch: 2, train for the 53-th batch, train loss: 0.38623732328414917:  14%|█▋          | 52/383 [00:30<03:17,  1.68it/s]Epoch: 2, train for the 53-th batch, train loss: 0.38623732328414917:  14%|█▋          | 53/383 [00:30<03:14,  1.69it/s]evaluate for the 18-th batch, evaluate loss: 0.5982422232627869:  26%|████▋             | 17/66 [00:04<00:13,  3.77it/s]evaluate for the 18-th batch, evaluate loss: 0.5982422232627869:  27%|████▉             | 18/66 [00:04<00:13,  3.60it/s]evaluate for the 19-th batch, evaluate loss: 0.6590092182159424:  90%|████████████████▏ | 18/20 [00:05<00:00,  3.36it/s]evaluate for the 19-th batch, evaluate loss: 0.6590092182159424:  95%|█████████████████ | 19/20 [00:05<00:00,  3.46it/s]Epoch: 7, train for the 98-th batch, train loss: 0.6612671613693237:  64%|████████▎    | 97/151 [00:21<00:13,  4.06it/s]Epoch: 7, train for the 98-th batch, train loss: 0.6612671613693237:  65%|████████▍    | 98/151 [00:21<00:12,  4.19it/s]evaluate for the 20-th batch, evaluate loss: 0.6357789039611816:  95%|█████████████████ | 19/20 [00:05<00:00,  3.46it/s]evaluate for the 20-th batch, evaluate loss: 0.6357789039611816: 100%|██████████████████| 20/20 [00:05<00:00,  3.63it/s]
INFO:root:Epoch: 3, learning rate: 0.0001, train loss: 0.5141
INFO:root:train average_precision, 0.8478
INFO:root:train roc_auc, 0.8225
INFO:root:validate loss: 0.5247
INFO:root:validate average_precision, 0.8380
INFO:root:validate roc_auc, 0.8043
INFO:root:new node validate loss: 0.6188
INFO:root:new node validate first_1_average_precision, 0.6447
INFO:root:new node validate first_1_roc_auc, 0.5640
INFO:root:new node validate first_3_average_precision, 0.6974
INFO:root:new node validate first_3_roc_auc, 0.6258
INFO:root:new node validate first_10_average_precision, 0.7410
INFO:root:new node validate first_10_roc_auc, 0.6847
INFO:root:new node validate average_precision, 0.7489
INFO:root:new node validate roc_auc, 0.7079
INFO:root:save model ./saved_models/DyGFormer/ia-escorts-dynamic/DyGFormer_seed0_dygformer-ia-escorts-dynamic-old/DyGFormer_seed0_dygformer-ia-escorts-dynamic-old.pkl
Epoch: 4, train for the 78-th batch, train loss: 0.20552843809127808:  65%|███████▊    | 77/119 [00:47<00:24,  1.68it/s]Epoch: 4, train for the 78-th batch, train loss: 0.20552843809127808:  66%|███████▊    | 78/119 [00:47<00:22,  1.84it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]evaluate for the 19-th batch, evaluate loss: 0.6050870418548584:  27%|████▉             | 18/66 [00:05<00:13,  3.60it/s]evaluate for the 19-th batch, evaluate loss: 0.6050870418548584:  29%|█████▏            | 19/66 [00:05<00:12,  3.72it/s]Epoch: 7, train for the 99-th batch, train loss: 0.6212784647941589:  65%|████████▍    | 98/151 [00:21<00:12,  4.19it/s]Epoch: 7, train for the 99-th batch, train loss: 0.6212784647941589:  66%|████████▌    | 99/151 [00:21<00:12,  4.28it/s]Epoch: 7, train for the 100-th batch, train loss: 0.6938439607620239:  66%|███████▊    | 99/151 [00:21<00:12,  4.28it/s]Epoch: 7, train for the 100-th batch, train loss: 0.6938439607620239:  66%|███████▎   | 100/151 [00:21<00:11,  4.34it/s]Epoch: 2, train for the 54-th batch, train loss: 0.4336673617362976:  14%|█▊           | 53/383 [00:31<03:14,  1.69it/s]Epoch: 2, train for the 54-th batch, train loss: 0.4336673617362976:  14%|█▊           | 54/383 [00:31<03:15,  1.69it/s]evaluate for the 20-th batch, evaluate loss: 0.6111348867416382:  29%|█████▏            | 19/66 [00:05<00:12,  3.72it/s]evaluate for the 20-th batch, evaluate loss: 0.6111348867416382:  30%|█████▍            | 20/66 [00:05<00:13,  3.53it/s]Epoch: 4, train for the 79-th batch, train loss: 0.18386024236679077:  66%|███████▊    | 78/119 [00:47<00:22,  1.84it/s]Epoch: 4, train for the 79-th batch, train loss: 0.18386024236679077:  66%|███████▉    | 79/119 [00:47<00:21,  1.88it/s]Epoch: 4, train for the 1-th batch, train loss: 0.8586580753326416:   0%|                       | 0/146 [00:00<?, ?it/s]Epoch: 4, train for the 1-th batch, train loss: 0.8586580753326416:   1%|               | 1/146 [00:00<01:06,  2.18it/s]Epoch: 7, train for the 101-th batch, train loss: 0.681329607963562:  66%|███████▉    | 100/151 [00:22<00:11,  4.34it/s]Epoch: 7, train for the 101-th batch, train loss: 0.681329607963562:  67%|████████    | 101/151 [00:22<00:11,  4.38it/s]evaluate for the 21-th batch, evaluate loss: 0.6212618350982666:  30%|█████▍            | 20/66 [00:05<00:13,  3.53it/s]evaluate for the 21-th batch, evaluate loss: 0.6212618350982666:  32%|█████▋            | 21/66 [00:05<00:12,  3.60it/s]Epoch: 7, train for the 102-th batch, train loss: 0.6318262815475464:  67%|███████▎   | 101/151 [00:22<00:11,  4.38it/s]Epoch: 7, train for the 102-th batch, train loss: 0.6318262815475464:  68%|███████▍   | 102/151 [00:22<00:11,  4.43it/s]evaluate for the 22-th batch, evaluate loss: 0.6073793172836304:  32%|█████▋            | 21/66 [00:05<00:12,  3.60it/s]evaluate for the 22-th batch, evaluate loss: 0.6073793172836304:  33%|██████            | 22/66 [00:05<00:12,  3.54it/s]Epoch: 2, train for the 55-th batch, train loss: 0.43262216448783875:  14%|█▋          | 54/383 [00:31<03:15,  1.69it/s]Epoch: 2, train for the 55-th batch, train loss: 0.43262216448783875:  14%|█▋          | 55/383 [00:31<03:13,  1.69it/s]Epoch: 7, train for the 103-th batch, train loss: 0.6145207285881042:  68%|███████▍   | 102/151 [00:22<00:11,  4.43it/s]Epoch: 7, train for the 103-th batch, train loss: 0.6145207285881042:  68%|███████▌   | 103/151 [00:22<00:10,  4.45it/s]Epoch: 4, train for the 2-th batch, train loss: 0.5035630464553833:   1%|               | 1/146 [00:01<01:06,  2.18it/s]Epoch: 4, train for the 2-th batch, train loss: 0.5035630464553833:   1%|▏              | 2/146 [00:01<01:14,  1.92it/s]Epoch: 4, train for the 80-th batch, train loss: 0.21886581182479858:  66%|███████▉    | 79/119 [00:48<00:21,  1.88it/s]Epoch: 4, train for the 80-th batch, train loss: 0.21886581182479858:  67%|████████    | 80/119 [00:48<00:21,  1.79it/s]evaluate for the 23-th batch, evaluate loss: 0.6226993799209595:  33%|██████            | 22/66 [00:06<00:12,  3.54it/s]evaluate for the 23-th batch, evaluate loss: 0.6226993799209595:  35%|██████▎           | 23/66 [00:06<00:11,  3.67it/s]Epoch: 7, train for the 104-th batch, train loss: 0.6387932300567627:  68%|███████▌   | 103/151 [00:22<00:10,  4.45it/s]Epoch: 7, train for the 104-th batch, train loss: 0.6387932300567627:  69%|███████▌   | 104/151 [00:22<00:10,  4.46it/s]Epoch: 7, train for the 105-th batch, train loss: 0.5745716691017151:  69%|███████▌   | 104/151 [00:22<00:10,  4.46it/s]Epoch: 7, train for the 105-th batch, train loss: 0.5745716691017151:  70%|███████▋   | 105/151 [00:22<00:10,  4.47it/s]evaluate for the 24-th batch, evaluate loss: 0.6168741583824158:  35%|██████▎           | 23/66 [00:06<00:11,  3.67it/s]evaluate for the 24-th batch, evaluate loss: 0.6168741583824158:  36%|██████▌           | 24/66 [00:06<00:11,  3.60it/s]Epoch: 2, train for the 56-th batch, train loss: 0.367489755153656:  14%|██            | 55/383 [00:32<03:13,  1.69it/s]Epoch: 2, train for the 56-th batch, train loss: 0.367489755153656:  15%|██            | 56/383 [00:32<03:13,  1.69it/s]Epoch: 4, train for the 3-th batch, train loss: 0.3580764830112457:   1%|▏              | 2/146 [00:01<01:14,  1.92it/s]Epoch: 4, train for the 3-th batch, train loss: 0.3580764830112457:   2%|▎              | 3/146 [00:01<01:17,  1.85it/s]Epoch: 7, train for the 106-th batch, train loss: 0.5649083256721497:  70%|███████▋   | 105/151 [00:23<00:10,  4.47it/s]Epoch: 7, train for the 106-th batch, train loss: 0.5649083256721497:  70%|███████▋   | 106/151 [00:23<00:10,  4.48it/s]Epoch: 4, train for the 81-th batch, train loss: 0.20516356825828552:  67%|████████    | 80/119 [00:49<00:21,  1.79it/s]Epoch: 4, train for the 81-th batch, train loss: 0.20516356825828552:  68%|████████▏   | 81/119 [00:49<00:21,  1.76it/s]evaluate for the 25-th batch, evaluate loss: 0.6397942900657654:  36%|██████▌           | 24/66 [00:06<00:11,  3.60it/s]evaluate for the 25-th batch, evaluate loss: 0.6397942900657654:  38%|██████▊           | 25/66 [00:06<00:11,  3.70it/s]Epoch: 7, train for the 107-th batch, train loss: 0.5515279769897461:  70%|███████▋   | 106/151 [00:23<00:10,  4.48it/s]Epoch: 7, train for the 107-th batch, train loss: 0.5515279769897461:  71%|███████▊   | 107/151 [00:23<00:09,  4.50it/s]evaluate for the 26-th batch, evaluate loss: 0.5983504056930542:  38%|██████▊           | 25/66 [00:06<00:11,  3.70it/s]evaluate for the 26-th batch, evaluate loss: 0.5983504056930542:  39%|███████           | 26/66 [00:06<00:10,  3.66it/s]Epoch: 7, train for the 108-th batch, train loss: 0.47680121660232544:  71%|███████   | 107/151 [00:23<00:09,  4.50it/s]Epoch: 7, train for the 108-th batch, train loss: 0.47680121660232544:  72%|███████▏  | 108/151 [00:23<00:09,  4.55it/s]Epoch: 2, train for the 57-th batch, train loss: 0.3412924110889435:  15%|█▉           | 56/383 [00:33<03:13,  1.69it/s]Epoch: 2, train for the 57-th batch, train loss: 0.3412924110889435:  15%|█▉           | 57/383 [00:33<03:13,  1.68it/s]Epoch: 4, train for the 4-th batch, train loss: 0.441540390253067:   2%|▎               | 3/146 [00:02<01:17,  1.85it/s]Epoch: 4, train for the 4-th batch, train loss: 0.441540390253067:   3%|▍               | 4/146 [00:02<01:19,  1.79it/s]Epoch: 4, train for the 82-th batch, train loss: 0.22137431800365448:  68%|████████▏   | 81/119 [00:49<00:21,  1.76it/s]Epoch: 4, train for the 82-th batch, train loss: 0.22137431800365448:  69%|████████▎   | 82/119 [00:49<00:21,  1.73it/s]evaluate for the 27-th batch, evaluate loss: 0.6103643774986267:  39%|███████           | 26/66 [00:07<00:10,  3.66it/s]evaluate for the 27-th batch, evaluate loss: 0.6103643774986267:  41%|███████▎          | 27/66 [00:07<00:10,  3.61it/s]Epoch: 7, train for the 109-th batch, train loss: 0.545293927192688:  72%|████████▌   | 108/151 [00:23<00:09,  4.55it/s]Epoch: 7, train for the 109-th batch, train loss: 0.545293927192688:  72%|████████▋   | 109/151 [00:23<00:09,  4.54it/s]Epoch: 7, train for the 110-th batch, train loss: 0.5622231364250183:  72%|███████▉   | 109/151 [00:24<00:09,  4.54it/s]Epoch: 7, train for the 110-th batch, train loss: 0.5622231364250183:  73%|████████   | 110/151 [00:24<00:09,  4.53it/s]evaluate for the 28-th batch, evaluate loss: 0.6101703643798828:  41%|███████▎          | 27/66 [00:07<00:10,  3.61it/s]evaluate for the 28-th batch, evaluate loss: 0.6101703643798828:  42%|███████▋          | 28/66 [00:07<00:10,  3.64it/s]Epoch: 4, train for the 5-th batch, train loss: 0.4702196717262268:   3%|▍              | 4/146 [00:02<01:19,  1.79it/s]Epoch: 4, train for the 5-th batch, train loss: 0.4702196717262268:   3%|▌              | 5/146 [00:02<01:18,  1.81it/s]Epoch: 2, train for the 58-th batch, train loss: 0.38153988122940063:  15%|█▊          | 57/383 [00:33<03:13,  1.68it/s]Epoch: 2, train for the 58-th batch, train loss: 0.38153988122940063:  15%|█▊          | 58/383 [00:33<03:13,  1.68it/s]Epoch: 7, train for the 111-th batch, train loss: 0.5174536108970642:  73%|████████   | 110/151 [00:24<00:09,  4.53it/s]Epoch: 7, train for the 111-th batch, train loss: 0.5174536108970642:  74%|████████   | 111/151 [00:24<00:08,  4.52it/s]evaluate for the 29-th batch, evaluate loss: 0.6036088466644287:  42%|███████▋          | 28/66 [00:07<00:10,  3.64it/s]evaluate for the 29-th batch, evaluate loss: 0.6036088466644287:  44%|███████▉          | 29/66 [00:07<00:10,  3.56it/s]Epoch: 4, train for the 83-th batch, train loss: 0.19554023444652557:  69%|████████▎   | 82/119 [00:50<00:21,  1.73it/s]Epoch: 4, train for the 83-th batch, train loss: 0.19554023444652557:  70%|████████▎   | 83/119 [00:50<00:21,  1.71it/s]Epoch: 7, train for the 112-th batch, train loss: 0.5515449047088623:  74%|████████   | 111/151 [00:24<00:08,  4.52it/s]Epoch: 7, train for the 112-th batch, train loss: 0.5515449047088623:  74%|████████▏  | 112/151 [00:24<00:08,  4.52it/s]evaluate for the 30-th batch, evaluate loss: 0.611512303352356:  44%|████████▎          | 29/66 [00:08<00:10,  3.56it/s]evaluate for the 30-th batch, evaluate loss: 0.611512303352356:  45%|████████▋          | 30/66 [00:08<00:09,  3.70it/s]Epoch: 7, train for the 113-th batch, train loss: 0.5583229660987854:  74%|████████▏  | 112/151 [00:24<00:08,  4.52it/s]Epoch: 7, train for the 113-th batch, train loss: 0.5583229660987854:  75%|████████▏  | 113/151 [00:24<00:08,  4.51it/s]Epoch: 2, train for the 59-th batch, train loss: 0.32780975103378296:  15%|█▊          | 58/383 [00:34<03:13,  1.68it/s]Epoch: 2, train for the 59-th batch, train loss: 0.32780975103378296:  15%|█▊          | 59/383 [00:34<03:10,  1.70it/s]Epoch: 4, train for the 6-th batch, train loss: 0.521422803401947:   3%|▌               | 5/146 [00:03<01:18,  1.81it/s]Epoch: 4, train for the 6-th batch, train loss: 0.521422803401947:   4%|▋               | 6/146 [00:03<01:20,  1.74it/s]evaluate for the 31-th batch, evaluate loss: 0.6172860860824585:  45%|████████▏         | 30/66 [00:08<00:09,  3.70it/s]evaluate for the 31-th batch, evaluate loss: 0.6172860860824585:  47%|████████▍         | 31/66 [00:08<00:09,  3.58it/s]Epoch: 7, train for the 114-th batch, train loss: 0.5150535702705383:  75%|████████▏  | 113/151 [00:24<00:08,  4.51it/s]Epoch: 7, train for the 114-th batch, train loss: 0.5150535702705383:  75%|████████▎  | 114/151 [00:24<00:08,  4.50it/s]Epoch: 4, train for the 84-th batch, train loss: 0.15242548286914825:  70%|████████▎   | 83/119 [00:50<00:21,  1.71it/s]Epoch: 4, train for the 84-th batch, train loss: 0.15242548286914825:  71%|████████▍   | 84/119 [00:50<00:20,  1.69it/s]evaluate for the 32-th batch, evaluate loss: 0.6082212924957275:  47%|████████▍         | 31/66 [00:08<00:09,  3.58it/s]evaluate for the 32-th batch, evaluate loss: 0.6082212924957275:  48%|████████▋         | 32/66 [00:08<00:09,  3.77it/s]Epoch: 7, train for the 115-th batch, train loss: 0.5167539715766907:  75%|████████▎  | 114/151 [00:25<00:08,  4.50it/s]Epoch: 7, train for the 115-th batch, train loss: 0.5167539715766907:  76%|████████▍  | 115/151 [00:25<00:07,  4.50it/s]Epoch: 7, train for the 116-th batch, train loss: 0.4889218509197235:  76%|████████▍  | 115/151 [00:25<00:07,  4.50it/s]Epoch: 7, train for the 116-th batch, train loss: 0.4889218509197235:  77%|████████▍  | 116/151 [00:25<00:07,  4.51it/s]Epoch: 2, train for the 60-th batch, train loss: 0.31605610251426697:  15%|█▊          | 59/383 [00:34<03:10,  1.70it/s]Epoch: 2, train for the 60-th batch, train loss: 0.31605610251426697:  16%|█▉          | 60/383 [00:34<03:10,  1.69it/s]evaluate for the 33-th batch, evaluate loss: 0.601367712020874:  48%|█████████▏         | 32/66 [00:08<00:09,  3.77it/s]evaluate for the 33-th batch, evaluate loss: 0.601367712020874:  50%|█████████▌         | 33/66 [00:08<00:09,  3.59it/s]Epoch: 4, train for the 7-th batch, train loss: 0.46981802582740784:   4%|▌             | 6/146 [00:03<01:20,  1.74it/s]Epoch: 4, train for the 7-th batch, train loss: 0.46981802582740784:   5%|▋             | 7/146 [00:03<01:20,  1.72it/s]Epoch: 7, train for the 117-th batch, train loss: 0.5321733951568604:  77%|████████▍  | 116/151 [00:25<00:07,  4.51it/s]Epoch: 7, train for the 117-th batch, train loss: 0.5321733951568604:  77%|████████▌  | 117/151 [00:25<00:07,  4.51it/s]Epoch: 4, train for the 85-th batch, train loss: 0.20284613966941833:  71%|████████▍   | 84/119 [00:51<00:20,  1.69it/s]Epoch: 4, train for the 85-th batch, train loss: 0.20284613966941833:  71%|████████▌   | 85/119 [00:51<00:20,  1.68it/s]evaluate for the 34-th batch, evaluate loss: 0.6065391898155212:  50%|█████████         | 33/66 [00:09<00:09,  3.59it/s]evaluate for the 34-th batch, evaluate loss: 0.6065391898155212:  52%|█████████▎        | 34/66 [00:09<00:08,  3.69it/s]Epoch: 7, train for the 118-th batch, train loss: 0.4876055121421814:  77%|████████▌  | 117/151 [00:25<00:07,  4.51it/s]Epoch: 7, train for the 118-th batch, train loss: 0.4876055121421814:  78%|████████▌  | 118/151 [00:25<00:07,  4.52it/s]Epoch: 4, train for the 8-th batch, train loss: 0.4002237915992737:   5%|▋              | 7/146 [00:04<01:20,  1.72it/s]Epoch: 4, train for the 8-th batch, train loss: 0.4002237915992737:   5%|▊              | 8/146 [00:04<01:16,  1.79it/s]Epoch: 2, train for the 61-th batch, train loss: 0.34282413125038147:  16%|█▉          | 60/383 [00:35<03:10,  1.69it/s]Epoch: 2, train for the 61-th batch, train loss: 0.34282413125038147:  16%|█▉          | 61/383 [00:35<03:10,  1.69it/s]evaluate for the 35-th batch, evaluate loss: 0.6106106042861938:  52%|█████████▎        | 34/66 [00:09<00:08,  3.69it/s]evaluate for the 35-th batch, evaluate loss: 0.6106106042861938:  53%|█████████▌        | 35/66 [00:09<00:08,  3.52it/s]Epoch: 7, train for the 119-th batch, train loss: 0.5134342908859253:  78%|████████▌  | 118/151 [00:26<00:07,  4.52it/s]Epoch: 7, train for the 119-th batch, train loss: 0.5134342908859253:  79%|████████▋  | 119/151 [00:26<00:07,  4.51it/s]Epoch: 4, train for the 86-th batch, train loss: 0.2307392954826355:  71%|█████████▎   | 85/119 [00:52<00:20,  1.68it/s]Epoch: 4, train for the 86-th batch, train loss: 0.2307392954826355:  72%|█████████▍   | 86/119 [00:52<00:19,  1.68it/s]Epoch: 7, train for the 120-th batch, train loss: 0.5912599563598633:  79%|████████▋  | 119/151 [00:26<00:07,  4.51it/s]Epoch: 7, train for the 120-th batch, train loss: 0.5912599563598633:  79%|████████▋  | 120/151 [00:26<00:06,  4.52it/s]evaluate for the 36-th batch, evaluate loss: 0.6452671885490417:  53%|█████████▌        | 35/66 [00:09<00:08,  3.52it/s]evaluate for the 36-th batch, evaluate loss: 0.6452671885490417:  55%|█████████▊        | 36/66 [00:09<00:08,  3.57it/s]Epoch: 7, train for the 121-th batch, train loss: 0.5151094198226929:  79%|████████▋  | 120/151 [00:26<00:06,  4.52it/s]Epoch: 7, train for the 121-th batch, train loss: 0.5151094198226929:  80%|████████▊  | 121/151 [00:26<00:06,  4.51it/s]Epoch: 4, train for the 9-th batch, train loss: 0.4023331105709076:   5%|▊              | 8/146 [00:04<01:16,  1.79it/s]Epoch: 4, train for the 9-th batch, train loss: 0.4023331105709076:   6%|▉              | 9/146 [00:04<01:15,  1.80it/s]evaluate for the 37-th batch, evaluate loss: 0.6425378322601318:  55%|█████████▊        | 36/66 [00:10<00:08,  3.57it/s]evaluate for the 37-th batch, evaluate loss: 0.6425378322601318:  56%|██████████        | 37/66 [00:10<00:08,  3.53it/s]Epoch: 2, train for the 62-th batch, train loss: 0.41594916582107544:  16%|█▉          | 61/383 [00:36<03:10,  1.69it/s]Epoch: 2, train for the 62-th batch, train loss: 0.41594916582107544:  16%|█▉          | 62/383 [00:36<03:07,  1.71it/s]Epoch: 7, train for the 122-th batch, train loss: 0.5497742891311646:  80%|████████▊  | 121/151 [00:26<00:06,  4.51it/s]Epoch: 7, train for the 122-th batch, train loss: 0.5497742891311646:  81%|████████▉  | 122/151 [00:26<00:07,  4.06it/s]Epoch: 4, train for the 87-th batch, train loss: 0.21596460044384003:  72%|████████▋   | 86/119 [00:52<00:19,  1.68it/s]Epoch: 4, train for the 87-th batch, train loss: 0.21596460044384003:  73%|████████▊   | 87/119 [00:52<00:19,  1.67it/s]evaluate for the 38-th batch, evaluate loss: 0.5928440093994141:  56%|██████████        | 37/66 [00:10<00:08,  3.53it/s]evaluate for the 38-th batch, evaluate loss: 0.5928440093994141:  58%|██████████▎       | 38/66 [00:10<00:07,  3.64it/s]Epoch: 7, train for the 123-th batch, train loss: 0.5451659560203552:  81%|████████▉  | 122/151 [00:27<00:07,  4.06it/s]Epoch: 7, train for the 123-th batch, train loss: 0.5451659560203552:  81%|████████▉  | 123/151 [00:27<00:06,  4.17it/s]Epoch: 4, train for the 10-th batch, train loss: 0.4591468572616577:   6%|▊             | 9/146 [00:05<01:15,  1.80it/s]Epoch: 4, train for the 10-th batch, train loss: 0.4591468572616577:   7%|▉            | 10/146 [00:05<01:16,  1.78it/s]evaluate for the 39-th batch, evaluate loss: 0.593786358833313:  58%|██████████▉        | 38/66 [00:10<00:07,  3.64it/s]evaluate for the 39-th batch, evaluate loss: 0.593786358833313:  59%|███████████▏       | 39/66 [00:10<00:07,  3.58it/s]Epoch: 2, train for the 63-th batch, train loss: 0.35892149806022644:  16%|█▉          | 62/383 [00:36<03:07,  1.71it/s]Epoch: 2, train for the 63-th batch, train loss: 0.35892149806022644:  16%|█▉          | 63/383 [00:36<03:07,  1.71it/s]Epoch: 7, train for the 124-th batch, train loss: 0.5150264501571655:  81%|████████▉  | 123/151 [00:27<00:06,  4.17it/s]Epoch: 7, train for the 124-th batch, train loss: 0.5150264501571655:  82%|█████████  | 124/151 [00:27<00:06,  4.26it/s]Epoch: 4, train for the 88-th batch, train loss: 0.23685817420482635:  73%|████████▊   | 87/119 [00:53<00:19,  1.67it/s]Epoch: 4, train for the 88-th batch, train loss: 0.23685817420482635:  74%|████████▊   | 88/119 [00:53<00:18,  1.69it/s]evaluate for the 40-th batch, evaluate loss: 0.6180576682090759:  59%|██████████▋       | 39/66 [00:10<00:07,  3.58it/s]evaluate for the 40-th batch, evaluate loss: 0.6180576682090759:  61%|██████████▉       | 40/66 [00:10<00:07,  3.68it/s]Epoch: 7, train for the 125-th batch, train loss: 0.5448541045188904:  82%|█████████  | 124/151 [00:27<00:06,  4.26it/s]Epoch: 7, train for the 125-th batch, train loss: 0.5448541045188904:  83%|█████████  | 125/151 [00:27<00:06,  4.32it/s]Epoch: 4, train for the 11-th batch, train loss: 0.477472722530365:   7%|▉             | 10/146 [00:06<01:16,  1.78it/s]Epoch: 4, train for the 11-th batch, train loss: 0.477472722530365:   8%|█             | 11/146 [00:06<01:14,  1.82it/s]evaluate for the 41-th batch, evaluate loss: 0.5833770036697388:  61%|██████████▉       | 40/66 [00:11<00:07,  3.68it/s]evaluate for the 41-th batch, evaluate loss: 0.5833770036697388:  62%|███████████▏      | 41/66 [00:11<00:06,  3.63it/s]Epoch: 7, train for the 126-th batch, train loss: 0.5356313586235046:  83%|█████████  | 125/151 [00:27<00:06,  4.32it/s]Epoch: 7, train for the 126-th batch, train loss: 0.5356313586235046:  83%|█████████▏ | 126/151 [00:27<00:05,  4.37it/s]Epoch: 2, train for the 64-th batch, train loss: 0.36236572265625:  16%|██▍            | 63/383 [00:37<03:07,  1.71it/s]Epoch: 2, train for the 64-th batch, train loss: 0.36236572265625:  17%|██▌            | 64/383 [00:37<03:07,  1.70it/s]Epoch: 7, train for the 127-th batch, train loss: 0.5220065712928772:  83%|█████████▏ | 126/151 [00:27<00:05,  4.37it/s]Epoch: 7, train for the 127-th batch, train loss: 0.5220065712928772:  84%|█████████▎ | 127/151 [00:27<00:05,  4.40it/s]evaluate for the 42-th batch, evaluate loss: 0.6129559874534607:  62%|███████████▏      | 41/66 [00:11<00:06,  3.63it/s]evaluate for the 42-th batch, evaluate loss: 0.6129559874534607:  64%|███████████▍      | 42/66 [00:11<00:06,  3.68it/s]Epoch: 4, train for the 89-th batch, train loss: 0.21507501602172852:  74%|████████▊   | 88/119 [00:53<00:18,  1.69it/s]Epoch: 4, train for the 89-th batch, train loss: 0.21507501602172852:  75%|████████▉   | 89/119 [00:53<00:17,  1.70it/s]Epoch: 7, train for the 128-th batch, train loss: 0.5878468155860901:  84%|█████████▎ | 127/151 [00:28<00:05,  4.40it/s]Epoch: 7, train for the 128-th batch, train loss: 0.5878468155860901:  85%|█████████▎ | 128/151 [00:28<00:05,  4.43it/s]Epoch: 4, train for the 12-th batch, train loss: 0.4600314199924469:   8%|▉            | 11/146 [00:06<01:14,  1.82it/s]Epoch: 4, train for the 12-th batch, train loss: 0.4600314199924469:   8%|█            | 12/146 [00:06<01:12,  1.84it/s]evaluate for the 43-th batch, evaluate loss: 0.5908363461494446:  64%|███████████▍      | 42/66 [00:11<00:06,  3.68it/s]evaluate for the 43-th batch, evaluate loss: 0.5908363461494446:  65%|███████████▋      | 43/66 [00:11<00:06,  3.62it/s]Epoch: 7, train for the 129-th batch, train loss: 0.5548955202102661:  85%|█████████▎ | 128/151 [00:28<00:05,  4.43it/s]Epoch: 7, train for the 129-th batch, train loss: 0.5548955202102661:  85%|█████████▍ | 129/151 [00:28<00:04,  4.45it/s]Epoch: 2, train for the 65-th batch, train loss: 0.3485666811466217:  17%|██▏          | 64/383 [00:37<03:07,  1.70it/s]Epoch: 2, train for the 65-th batch, train loss: 0.3485666811466217:  17%|██▏          | 65/383 [00:37<03:07,  1.69it/s]evaluate for the 44-th batch, evaluate loss: 0.5674653649330139:  65%|███████████▋      | 43/66 [00:11<00:06,  3.62it/s]evaluate for the 44-th batch, evaluate loss: 0.5674653649330139:  67%|████████████      | 44/66 [00:11<00:06,  3.57it/s]Epoch: 7, train for the 130-th batch, train loss: 0.537950873374939:  85%|██████████▎ | 129/151 [00:28<00:04,  4.45it/s]Epoch: 7, train for the 130-th batch, train loss: 0.537950873374939:  86%|██████████▎ | 130/151 [00:28<00:04,  4.47it/s]Epoch: 4, train for the 90-th batch, train loss: 0.22289562225341797:  75%|████████▉   | 89/119 [00:54<00:17,  1.70it/s]Epoch: 4, train for the 90-th batch, train loss: 0.22289562225341797:  76%|█████████   | 90/119 [00:54<00:17,  1.66it/s]Epoch: 4, train for the 13-th batch, train loss: 0.4455892741680145:   8%|█            | 12/146 [00:07<01:12,  1.84it/s]Epoch: 4, train for the 13-th batch, train loss: 0.4455892741680145:   9%|█▏           | 13/146 [00:07<01:15,  1.77it/s]evaluate for the 45-th batch, evaluate loss: 0.6188060641288757:  67%|████████████      | 44/66 [00:12<00:06,  3.57it/s]evaluate for the 45-th batch, evaluate loss: 0.6188060641288757:  68%|████████████▎     | 45/66 [00:12<00:05,  3.60it/s]Epoch: 7, train for the 131-th batch, train loss: 0.5420171022415161:  86%|█████████▍ | 130/151 [00:28<00:04,  4.47it/s]Epoch: 7, train for the 131-th batch, train loss: 0.5420171022415161:  87%|█████████▌ | 131/151 [00:28<00:04,  4.48it/s]Epoch: 2, train for the 66-th batch, train loss: 0.38548359274864197:  17%|██          | 65/383 [00:38<03:07,  1.69it/s]Epoch: 2, train for the 66-th batch, train loss: 0.38548359274864197:  17%|██          | 66/383 [00:38<03:07,  1.69it/s]Epoch: 7, train for the 132-th batch, train loss: 0.5045310854911804:  87%|█████████▌ | 131/151 [00:29<00:04,  4.48it/s]Epoch: 7, train for the 132-th batch, train loss: 0.5045310854911804:  87%|█████████▌ | 132/151 [00:29<00:04,  4.50it/s]evaluate for the 46-th batch, evaluate loss: 0.6229280829429626:  68%|████████████▎     | 45/66 [00:12<00:05,  3.60it/s]evaluate for the 46-th batch, evaluate loss: 0.6229280829429626:  70%|████████████▌     | 46/66 [00:12<00:05,  3.53it/s]Epoch: 4, train for the 91-th batch, train loss: 0.21600480377674103:  76%|█████████   | 90/119 [00:55<00:17,  1.66it/s]Epoch: 4, train for the 91-th batch, train loss: 0.21600480377674103:  76%|█████████▏  | 91/119 [00:55<00:16,  1.65it/s]Epoch: 7, train for the 133-th batch, train loss: 0.5345124006271362:  87%|█████████▌ | 132/151 [00:29<00:04,  4.50it/s]Epoch: 7, train for the 133-th batch, train loss: 0.5345124006271362:  88%|█████████▋ | 133/151 [00:29<00:04,  4.50it/s]evaluate for the 47-th batch, evaluate loss: 0.6370535492897034:  70%|████████████▌     | 46/66 [00:12<00:05,  3.53it/s]evaluate for the 47-th batch, evaluate loss: 0.6370535492897034:  71%|████████████▊     | 47/66 [00:12<00:05,  3.69it/s]Epoch: 4, train for the 14-th batch, train loss: 0.42217910289764404:   9%|█           | 13/146 [00:07<01:15,  1.77it/s]Epoch: 4, train for the 14-th batch, train loss: 0.42217910289764404:  10%|█▏          | 14/146 [00:07<01:16,  1.72it/s]Epoch: 7, train for the 134-th batch, train loss: 0.5321957468986511:  88%|█████████▋ | 133/151 [00:29<00:04,  4.50it/s]Epoch: 7, train for the 134-th batch, train loss: 0.5321957468986511:  89%|█████████▊ | 134/151 [00:29<00:03,  4.49it/s]Epoch: 2, train for the 67-th batch, train loss: 0.39700964093208313:  17%|██          | 66/383 [00:39<03:07,  1.69it/s]Epoch: 2, train for the 67-th batch, train loss: 0.39700964093208313:  17%|██          | 67/383 [00:39<03:06,  1.69it/s]evaluate for the 48-th batch, evaluate loss: 0.6189689636230469:  71%|████████████▊     | 47/66 [00:13<00:05,  3.69it/s]evaluate for the 48-th batch, evaluate loss: 0.6189689636230469:  73%|█████████████     | 48/66 [00:13<00:05,  3.57it/s]Epoch: 7, train for the 135-th batch, train loss: 0.5699909925460815:  89%|█████████▊ | 134/151 [00:29<00:03,  4.49it/s]Epoch: 7, train for the 135-th batch, train loss: 0.5699909925460815:  89%|█████████▊ | 135/151 [00:29<00:03,  4.49it/s]Epoch: 4, train for the 92-th batch, train loss: 0.24169261753559113:  76%|█████████▏  | 91/119 [00:55<00:16,  1.65it/s]Epoch: 4, train for the 92-th batch, train loss: 0.24169261753559113:  77%|█████████▎  | 92/119 [00:55<00:16,  1.65it/s]evaluate for the 49-th batch, evaluate loss: 0.6385253667831421:  73%|█████████████     | 48/66 [00:13<00:05,  3.57it/s]evaluate for the 49-th batch, evaluate loss: 0.6385253667831421:  74%|█████████████▎    | 49/66 [00:13<00:04,  3.75it/s]Epoch: 7, train for the 136-th batch, train loss: 0.5309647917747498:  89%|█████████▊ | 135/151 [00:29<00:03,  4.49it/s]Epoch: 7, train for the 136-th batch, train loss: 0.5309647917747498:  90%|█████████▉ | 136/151 [00:29<00:03,  4.50it/s]Epoch: 4, train for the 15-th batch, train loss: 0.4380514323711395:  10%|█▏           | 14/146 [00:08<01:16,  1.72it/s]Epoch: 4, train for the 15-th batch, train loss: 0.4380514323711395:  10%|█▎           | 15/146 [00:08<01:17,  1.69it/s]Epoch: 7, train for the 137-th batch, train loss: 0.6179013848304749:  90%|█████████▉ | 136/151 [00:30<00:03,  4.50it/s]Epoch: 7, train for the 137-th batch, train loss: 0.6179013848304749:  91%|█████████▉ | 137/151 [00:30<00:03,  4.50it/s]Epoch: 2, train for the 68-th batch, train loss: 0.3885085880756378:  17%|██▎          | 67/383 [00:39<03:06,  1.69it/s]Epoch: 2, train for the 68-th batch, train loss: 0.3885085880756378:  18%|██▎          | 68/383 [00:39<03:07,  1.68it/s]evaluate for the 50-th batch, evaluate loss: 0.6347172260284424:  74%|█████████████▎    | 49/66 [00:13<00:04,  3.75it/s]evaluate for the 50-th batch, evaluate loss: 0.6347172260284424:  76%|█████████████▋    | 50/66 [00:13<00:04,  3.56it/s]Epoch: 7, train for the 138-th batch, train loss: 0.6078237295150757:  91%|█████████▉ | 137/151 [00:30<00:03,  4.50it/s]Epoch: 7, train for the 138-th batch, train loss: 0.6078237295150757:  91%|██████████ | 138/151 [00:30<00:02,  4.51it/s]evaluate for the 51-th batch, evaluate loss: 0.5946061611175537:  76%|█████████████▋    | 50/66 [00:13<00:04,  3.56it/s]evaluate for the 51-th batch, evaluate loss: 0.5946061611175537:  77%|█████████████▉    | 51/66 [00:13<00:04,  3.62it/s]Epoch: 4, train for the 93-th batch, train loss: 0.19435228407382965:  77%|█████████▎  | 92/119 [00:56<00:16,  1.65it/s]Epoch: 4, train for the 93-th batch, train loss: 0.19435228407382965:  78%|█████████▍  | 93/119 [00:56<00:15,  1.64it/s]Epoch: 7, train for the 139-th batch, train loss: 0.5549378991127014:  91%|██████████ | 138/151 [00:30<00:02,  4.51it/s]Epoch: 7, train for the 139-th batch, train loss: 0.5549378991127014:  92%|██████████▏| 139/151 [00:30<00:02,  4.50it/s]Epoch: 4, train for the 16-th batch, train loss: 0.44425344467163086:  10%|█▏          | 15/146 [00:09<01:17,  1.69it/s]Epoch: 4, train for the 16-th batch, train loss: 0.44425344467163086:  11%|█▎          | 16/146 [00:09<01:17,  1.67it/s]Epoch: 2, train for the 69-th batch, train loss: 0.3664892613887787:  18%|██▎          | 68/383 [00:40<03:07,  1.68it/s]Epoch: 2, train for the 69-th batch, train loss: 0.3664892613887787:  18%|██▎          | 69/383 [00:40<03:04,  1.71it/s]evaluate for the 52-th batch, evaluate loss: 0.6117455363273621:  77%|█████████████▉    | 51/66 [00:14<00:04,  3.62it/s]evaluate for the 52-th batch, evaluate loss: 0.6117455363273621:  79%|██████████████▏   | 52/66 [00:14<00:03,  3.55it/s]Epoch: 7, train for the 140-th batch, train loss: 0.49711042642593384:  92%|█████████▏| 139/151 [00:30<00:02,  4.50it/s]Epoch: 7, train for the 140-th batch, train loss: 0.49711042642593384:  93%|█████████▎| 140/151 [00:30<00:02,  4.50it/s]evaluate for the 53-th batch, evaluate loss: 0.6172146797180176:  79%|██████████████▏   | 52/66 [00:14<00:03,  3.55it/s]evaluate for the 53-th batch, evaluate loss: 0.6172146797180176:  80%|██████████████▍   | 53/66 [00:14<00:03,  3.61it/s]Epoch: 7, train for the 141-th batch, train loss: 0.5754202604293823:  93%|██████████▏| 140/151 [00:31<00:02,  4.50it/s]Epoch: 7, train for the 141-th batch, train loss: 0.5754202604293823:  93%|██████████▎| 141/151 [00:31<00:02,  4.51it/s]Epoch: 4, train for the 94-th batch, train loss: 0.21028466522693634:  78%|█████████▍  | 93/119 [00:57<00:15,  1.64it/s]Epoch: 4, train for the 94-th batch, train loss: 0.21028466522693634:  79%|█████████▍  | 94/119 [00:57<00:15,  1.64it/s]Epoch: 4, train for the 17-th batch, train loss: 0.4287310838699341:  11%|█▍           | 16/146 [00:09<01:17,  1.67it/s]Epoch: 4, train for the 17-th batch, train loss: 0.4287310838699341:  12%|█▌           | 17/146 [00:09<01:16,  1.70it/s]Epoch: 7, train for the 142-th batch, train loss: 0.5519264340400696:  93%|██████████▎| 141/151 [00:31<00:02,  4.51it/s]Epoch: 7, train for the 142-th batch, train loss: 0.5519264340400696:  94%|██████████▎| 142/151 [00:31<00:01,  4.51it/s]evaluate for the 54-th batch, evaluate loss: 0.6463330984115601:  80%|██████████████▍   | 53/66 [00:14<00:03,  3.61it/s]evaluate for the 54-th batch, evaluate loss: 0.6463330984115601:  82%|██████████████▋   | 54/66 [00:14<00:03,  3.54it/s]Epoch: 2, train for the 70-th batch, train loss: 0.4199337661266327:  18%|██▎          | 69/383 [00:40<03:04,  1.71it/s]Epoch: 2, train for the 70-th batch, train loss: 0.4199337661266327:  18%|██▍          | 70/383 [00:40<03:03,  1.71it/s]Epoch: 7, train for the 143-th batch, train loss: 0.49482467770576477:  94%|█████████▍| 142/151 [00:31<00:01,  4.51it/s]Epoch: 7, train for the 143-th batch, train loss: 0.49482467770576477:  95%|█████████▍| 143/151 [00:31<00:01,  4.51it/s]evaluate for the 55-th batch, evaluate loss: 0.6024413704872131:  82%|██████████████▋   | 54/66 [00:15<00:03,  3.54it/s]evaluate for the 55-th batch, evaluate loss: 0.6024413704872131:  83%|███████████████   | 55/66 [00:15<00:03,  3.65it/s]Epoch: 4, train for the 95-th batch, train loss: 0.17470355331897736:  79%|█████████▍  | 94/119 [00:57<00:15,  1.64it/s]Epoch: 4, train for the 95-th batch, train loss: 0.17470355331897736:  80%|█████████▌  | 95/119 [00:57<00:14,  1.66it/s]Epoch: 7, train for the 144-th batch, train loss: 0.503396213054657:  95%|███████████▎| 143/151 [00:31<00:01,  4.51it/s]Epoch: 7, train for the 144-th batch, train loss: 0.503396213054657:  95%|███████████▍| 144/151 [00:31<00:01,  4.51it/s]Epoch: 4, train for the 18-th batch, train loss: 0.41648799180984497:  12%|█▍          | 17/146 [00:10<01:16,  1.70it/s]Epoch: 4, train for the 18-th batch, train loss: 0.41648799180984497:  12%|█▍          | 18/146 [00:10<01:15,  1.71it/s]evaluate for the 56-th batch, evaluate loss: 0.6215518116950989:  83%|███████████████   | 55/66 [00:15<00:03,  3.65it/s]evaluate for the 56-th batch, evaluate loss: 0.6215518116950989:  85%|███████████████▎  | 56/66 [00:15<00:02,  3.59it/s]Epoch: 2, train for the 71-th batch, train loss: 0.4154394865036011:  18%|██▍          | 70/383 [00:41<03:03,  1.71it/s]Epoch: 2, train for the 71-th batch, train loss: 0.4154394865036011:  19%|██▍          | 71/383 [00:41<03:02,  1.71it/s]Epoch: 7, train for the 145-th batch, train loss: 0.5273479223251343:  95%|██████████▍| 144/151 [00:31<00:01,  4.51it/s]Epoch: 7, train for the 145-th batch, train loss: 0.5273479223251343:  96%|██████████▌| 145/151 [00:31<00:01,  4.49it/s]evaluate for the 57-th batch, evaluate loss: 0.610492467880249:  85%|████████████████   | 56/66 [00:15<00:02,  3.59it/s]evaluate for the 57-th batch, evaluate loss: 0.610492467880249:  86%|████████████████▍  | 57/66 [00:15<00:02,  3.69it/s]Epoch: 7, train for the 146-th batch, train loss: 0.5307777523994446:  96%|██████████▌| 145/151 [00:32<00:01,  4.49it/s]Epoch: 7, train for the 146-th batch, train loss: 0.5307777523994446:  97%|██████████▋| 146/151 [00:32<00:01,  4.49it/s]Epoch: 4, train for the 96-th batch, train loss: 0.22080844640731812:  80%|█████████▌  | 95/119 [00:58<00:14,  1.66it/s]Epoch: 4, train for the 96-th batch, train loss: 0.22080844640731812:  81%|█████████▋  | 96/119 [00:58<00:13,  1.67it/s]Epoch: 7, train for the 147-th batch, train loss: 0.5627864003181458:  97%|██████████▋| 146/151 [00:32<00:01,  4.49it/s]Epoch: 7, train for the 147-th batch, train loss: 0.5627864003181458:  97%|██████████▋| 147/151 [00:32<00:00,  4.50it/s]Epoch: 4, train for the 19-th batch, train loss: 0.46673956513404846:  12%|█▍          | 18/146 [00:10<01:15,  1.71it/s]Epoch: 4, train for the 19-th batch, train loss: 0.46673956513404846:  13%|█▌          | 19/146 [00:10<01:15,  1.69it/s]evaluate for the 58-th batch, evaluate loss: 0.6449255347251892:  86%|███████████████▌  | 57/66 [00:15<00:02,  3.69it/s]evaluate for the 58-th batch, evaluate loss: 0.6449255347251892:  88%|███████████████▊  | 58/66 [00:15<00:02,  3.65it/s]Epoch: 2, train for the 72-th batch, train loss: 0.3934379518032074:  19%|██▍          | 71/383 [00:41<03:02,  1.71it/s]Epoch: 2, train for the 72-th batch, train loss: 0.3934379518032074:  19%|██▍          | 72/383 [00:41<03:03,  1.69it/s]Epoch: 7, train for the 148-th batch, train loss: 0.5624650716781616:  97%|██████████▋| 147/151 [00:32<00:00,  4.50it/s]Epoch: 7, train for the 148-th batch, train loss: 0.5624650716781616:  98%|██████████▊| 148/151 [00:32<00:00,  4.51it/s]evaluate for the 59-th batch, evaluate loss: 0.6291396021842957:  88%|███████████████▊  | 58/66 [00:16<00:02,  3.65it/s]evaluate for the 59-th batch, evaluate loss: 0.6291396021842957:  89%|████████████████  | 59/66 [00:16<00:01,  3.66it/s]Epoch: 7, train for the 149-th batch, train loss: 0.5037927627563477:  98%|██████████▊| 148/151 [00:32<00:00,  4.51it/s]Epoch: 7, train for the 149-th batch, train loss: 0.5037927627563477:  99%|██████████▊| 149/151 [00:32<00:00,  4.50it/s]Epoch: 4, train for the 97-th batch, train loss: 0.20047500729560852:  81%|█████████▋  | 96/119 [00:58<00:13,  1.67it/s]Epoch: 4, train for the 97-th batch, train loss: 0.20047500729560852:  82%|█████████▊  | 97/119 [00:58<00:13,  1.69it/s]evaluate for the 60-th batch, evaluate loss: 0.6314328908920288:  89%|████████████████  | 59/66 [00:16<00:01,  3.66it/s]evaluate for the 60-th batch, evaluate loss: 0.6314328908920288:  91%|████████████████▎ | 60/66 [00:16<00:01,  3.67it/s]Epoch: 4, train for the 20-th batch, train loss: 0.416142076253891:  13%|█▊            | 19/146 [00:11<01:15,  1.69it/s]Epoch: 4, train for the 20-th batch, train loss: 0.416142076253891:  14%|█▉            | 20/146 [00:11<01:15,  1.68it/s]Epoch: 7, train for the 150-th batch, train loss: 0.5189504623413086:  99%|██████████▊| 149/151 [00:33<00:00,  4.50it/s]Epoch: 7, train for the 150-th batch, train loss: 0.5189504623413086:  99%|██████████▉| 150/151 [00:33<00:00,  4.50it/s]Epoch: 2, train for the 73-th batch, train loss: 0.3757696747779846:  19%|██▍          | 72/383 [00:42<03:03,  1.69it/s]Epoch: 2, train for the 73-th batch, train loss: 0.3757696747779846:  19%|██▍          | 73/383 [00:42<03:03,  1.69it/s]Epoch: 7, train for the 151-th batch, train loss: 0.6284126043319702:  99%|██████████▉| 150/151 [00:33<00:00,  4.50it/s]Epoch: 7, train for the 151-th batch, train loss: 0.6284126043319702: 100%|███████████| 151/151 [00:33<00:00,  4.98it/s]Epoch: 7, train for the 151-th batch, train loss: 0.6284126043319702: 100%|███████████| 151/151 [00:33<00:00,  4.55it/s]
  0%|                                                                                            | 0/46 [00:00<?, ?it/s]evaluate for the 61-th batch, evaluate loss: 0.6233711242675781:  91%|████████████████▎ | 60/66 [00:16<00:01,  3.67it/s]evaluate for the 61-th batch, evaluate loss: 0.6233711242675781:  92%|████████████████▋ | 61/66 [00:16<00:01,  3.58it/s]evaluate for the 1-th batch, evaluate loss: 0.49732065200805664:   0%|                           | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.49732065200805664:   2%|▍                  | 1/46 [00:00<00:04,  9.64it/s]Epoch: 4, train for the 98-th batch, train loss: 0.15356846153736115:  82%|█████████▊  | 97/119 [00:59<00:13,  1.69it/s]Epoch: 4, train for the 98-th batch, train loss: 0.15356846153736115:  82%|█████████▉  | 98/119 [00:59<00:11,  1.77it/s]evaluate for the 2-th batch, evaluate loss: 0.5053041577339172:   2%|▍                   | 1/46 [00:00<00:04,  9.64it/s]evaluate for the 2-th batch, evaluate loss: 0.5053041577339172:   4%|▊                   | 2/46 [00:00<00:04,  9.66it/s]evaluate for the 62-th batch, evaluate loss: 0.65311199426651:  92%|██████████████████▍ | 61/66 [00:16<00:01,  3.58it/s]evaluate for the 62-th batch, evaluate loss: 0.65311199426651:  94%|██████████████████▊ | 62/66 [00:16<00:01,  3.69it/s]evaluate for the 3-th batch, evaluate loss: 0.48103320598602295:   4%|▊                  | 2/46 [00:00<00:04,  9.66it/s]evaluate for the 3-th batch, evaluate loss: 0.48103320598602295:   7%|█▏                 | 3/46 [00:00<00:04,  9.64it/s]evaluate for the 4-th batch, evaluate loss: 0.5096086859703064:   7%|█▎                  | 3/46 [00:00<00:04,  9.64it/s]evaluate for the 4-th batch, evaluate loss: 0.5096086859703064:   9%|█▋                  | 4/46 [00:00<00:04,  9.61it/s]Epoch: 2, train for the 74-th batch, train loss: 0.3373311460018158:  19%|██▍          | 73/383 [00:43<03:03,  1.69it/s]Epoch: 2, train for the 74-th batch, train loss: 0.3373311460018158:  19%|██▌          | 74/383 [00:43<03:00,  1.71it/s]Epoch: 4, train for the 21-th batch, train loss: 0.4131714105606079:  14%|█▊           | 20/146 [00:12<01:15,  1.68it/s]Epoch: 4, train for the 21-th batch, train loss: 0.4131714105606079:  14%|█▊           | 21/146 [00:12<01:19,  1.58it/s]evaluate for the 5-th batch, evaluate loss: 0.48073282837867737:   9%|█▋                 | 4/46 [00:00<00:04,  9.61it/s]evaluate for the 5-th batch, evaluate loss: 0.48073282837867737:  11%|██                 | 5/46 [00:00<00:04,  9.62it/s]evaluate for the 63-th batch, evaluate loss: 0.6267318725585938:  94%|████████████████▉ | 62/66 [00:17<00:01,  3.69it/s]evaluate for the 63-th batch, evaluate loss: 0.6267318725585938:  95%|█████████████████▏| 63/66 [00:17<00:00,  3.59it/s]evaluate for the 6-th batch, evaluate loss: 0.5540515184402466:  11%|██▏                 | 5/46 [00:00<00:04,  9.62it/s]evaluate for the 6-th batch, evaluate loss: 0.5540515184402466:  13%|██▌                 | 6/46 [00:00<00:04,  9.58it/s]Epoch: 4, train for the 99-th batch, train loss: 0.1882629543542862:  82%|██████████▋  | 98/119 [00:59<00:11,  1.77it/s]Epoch: 4, train for the 99-th batch, train loss: 0.1882629543542862:  83%|██████████▊  | 99/119 [00:59<00:11,  1.80it/s]evaluate for the 7-th batch, evaluate loss: 0.48473992943763733:  13%|██▍                | 6/46 [00:00<00:04,  9.58it/s]evaluate for the 7-th batch, evaluate loss: 0.48473992943763733:  15%|██▉                | 7/46 [00:00<00:04,  9.63it/s]evaluate for the 64-th batch, evaluate loss: 0.6192750334739685:  95%|█████████████████▏| 63/66 [00:17<00:00,  3.59it/s]evaluate for the 64-th batch, evaluate loss: 0.6192750334739685:  97%|█████████████████▍| 64/66 [00:17<00:00,  3.77it/s]evaluate for the 8-th batch, evaluate loss: 0.558140218257904:  15%|███▏                 | 7/46 [00:00<00:04,  9.63it/s]evaluate for the 8-th batch, evaluate loss: 0.558140218257904:  17%|███▋                 | 8/46 [00:00<00:03,  9.62it/s]evaluate for the 9-th batch, evaluate loss: 0.5228838324546814:  17%|███▍                | 8/46 [00:00<00:03,  9.62it/s]evaluate for the 9-th batch, evaluate loss: 0.5228838324546814:  20%|███▉                | 9/46 [00:00<00:03,  9.61it/s]evaluate for the 10-th batch, evaluate loss: 0.536449670791626:  20%|███▉                | 9/46 [00:01<00:03,  9.61it/s]evaluate for the 10-th batch, evaluate loss: 0.536449670791626:  22%|████▏              | 10/46 [00:01<00:03,  9.64it/s]Epoch: 2, train for the 75-th batch, train loss: 0.3276348114013672:  19%|██▌          | 74/383 [00:43<03:00,  1.71it/s]Epoch: 2, train for the 75-th batch, train loss: 0.3276348114013672:  20%|██▌          | 75/383 [00:43<03:01,  1.70it/s]evaluate for the 65-th batch, evaluate loss: 0.5922101736068726:  97%|█████████████████▍| 64/66 [00:17<00:00,  3.77it/s]evaluate for the 65-th batch, evaluate loss: 0.5922101736068726:  98%|█████████████████▋| 65/66 [00:17<00:00,  3.60it/s]Epoch: 4, train for the 22-th batch, train loss: 0.4156896770000458:  14%|█▊           | 21/146 [00:12<01:19,  1.58it/s]evaluate for the 11-th batch, evaluate loss: 0.5223332047462463:  22%|███▉              | 10/46 [00:01<00:03,  9.64it/s]evaluate for the 11-th batch, evaluate loss: 0.5223332047462463:  24%|████▎             | 11/46 [00:01<00:03,  9.65it/s]Epoch: 4, train for the 22-th batch, train loss: 0.4156896770000458:  15%|█▉           | 22/146 [00:12<01:18,  1.59it/s]evaluate for the 12-th batch, evaluate loss: 0.4766906499862671:  24%|████▎             | 11/46 [00:01<00:03,  9.65it/s]evaluate for the 12-th batch, evaluate loss: 0.4766906499862671:  26%|████▋             | 12/46 [00:01<00:03,  9.66it/s]Epoch: 4, train for the 100-th batch, train loss: 0.19626601040363312:  83%|█████████▏ | 99/119 [01:00<00:11,  1.80it/s]Epoch: 4, train for the 100-th batch, train loss: 0.19626601040363312:  84%|████████▍ | 100/119 [01:00<00:10,  1.74it/s]evaluate for the 66-th batch, evaluate loss: 0.6429362893104553:  98%|█████████████████▋| 65/66 [00:17<00:00,  3.60it/s]evaluate for the 66-th batch, evaluate loss: 0.6429362893104553: 100%|██████████████████| 66/66 [00:17<00:00,  3.96it/s]evaluate for the 66-th batch, evaluate loss: 0.6429362893104553: 100%|██████████████████| 66/66 [00:17<00:00,  3.67it/s]
  0%|                                                                                            | 0/40 [00:00<?, ?it/s]evaluate for the 13-th batch, evaluate loss: 0.4958322048187256:  26%|████▋             | 12/46 [00:01<00:03,  9.66it/s]evaluate for the 13-th batch, evaluate loss: 0.4958322048187256:  28%|█████             | 13/46 [00:01<00:03,  9.67it/s]evaluate for the 14-th batch, evaluate loss: 0.5922990441322327:  28%|█████             | 13/46 [00:01<00:03,  9.67it/s]evaluate for the 14-th batch, evaluate loss: 0.5922990441322327:  30%|█████▍            | 14/46 [00:01<00:03,  9.69it/s]evaluate for the 15-th batch, evaluate loss: 0.5420989990234375:  30%|█████▍            | 14/46 [00:01<00:03,  9.69it/s]evaluate for the 15-th batch, evaluate loss: 0.5420989990234375:  33%|█████▊            | 15/46 [00:01<00:03,  9.69it/s]evaluate for the 1-th batch, evaluate loss: 0.6585372090339661:   0%|                            | 0/40 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6585372090339661:   2%|▌                   | 1/40 [00:00<00:11,  3.37it/s]evaluate for the 16-th batch, evaluate loss: 0.5753204822540283:  33%|█████▊            | 15/46 [00:01<00:03,  9.69it/s]evaluate for the 16-th batch, evaluate loss: 0.5753204822540283:  35%|██████▎           | 16/46 [00:01<00:03,  9.69it/s]Epoch: 2, train for the 76-th batch, train loss: 0.39523985981941223:  20%|██▎         | 75/383 [00:44<03:01,  1.70it/s]Epoch: 2, train for the 76-th batch, train loss: 0.39523985981941223:  20%|██▍         | 76/383 [00:44<02:58,  1.72it/s]Epoch: 4, train for the 23-th batch, train loss: 0.4808129072189331:  15%|█▉           | 22/146 [00:13<01:18,  1.59it/s]Epoch: 4, train for the 23-th batch, train loss: 0.4808129072189331:  16%|██           | 23/146 [00:13<01:17,  1.60it/s]evaluate for the 17-th batch, evaluate loss: 0.45658743381500244:  35%|█████▉           | 16/46 [00:01<00:03,  9.69it/s]evaluate for the 17-th batch, evaluate loss: 0.45658743381500244:  37%|██████▎          | 17/46 [00:01<00:02,  9.69it/s]evaluate for the 18-th batch, evaluate loss: 0.498553991317749:  37%|███████            | 17/46 [00:01<00:02,  9.69it/s]evaluate for the 18-th batch, evaluate loss: 0.498553991317749:  39%|███████▍           | 18/46 [00:01<00:02,  9.67it/s]evaluate for the 2-th batch, evaluate loss: 0.6920275688171387:   2%|▌                   | 1/40 [00:00<00:11,  3.37it/s]evaluate for the 2-th batch, evaluate loss: 0.6920275688171387:   5%|█                   | 2/40 [00:00<00:10,  3.61it/s]Epoch: 4, train for the 101-th batch, train loss: 0.16945227980613708:  84%|████████▍ | 100/119 [01:01<00:10,  1.74it/s]Epoch: 4, train for the 101-th batch, train loss: 0.16945227980613708:  85%|████████▍ | 101/119 [01:01<00:10,  1.70it/s]evaluate for the 19-th batch, evaluate loss: 0.5205174684524536:  39%|███████           | 18/46 [00:01<00:02,  9.67it/s]evaluate for the 19-th batch, evaluate loss: 0.5205174684524536:  41%|███████▍          | 19/46 [00:01<00:02,  9.70it/s]evaluate for the 20-th batch, evaluate loss: 0.5373771786689758:  41%|███████▍          | 19/46 [00:02<00:02,  9.70it/s]evaluate for the 20-th batch, evaluate loss: 0.5373771786689758:  43%|███████▊          | 20/46 [00:02<00:02,  9.69it/s]evaluate for the 21-th batch, evaluate loss: 0.528357744216919:  43%|████████▎          | 20/46 [00:02<00:02,  9.69it/s]evaluate for the 21-th batch, evaluate loss: 0.528357744216919:  46%|████████▋          | 21/46 [00:02<00:02,  9.69it/s]evaluate for the 3-th batch, evaluate loss: 0.6654506325721741:   5%|█                   | 2/40 [00:00<00:10,  3.61it/s]evaluate for the 3-th batch, evaluate loss: 0.6654506325721741:   8%|█▌                  | 3/40 [00:00<00:10,  3.52it/s]Epoch: 2, train for the 77-th batch, train loss: 0.4064493775367737:  20%|██▌          | 76/383 [00:44<02:58,  1.72it/s]Epoch: 2, train for the 77-th batch, train loss: 0.4064493775367737:  20%|██▌          | 77/383 [00:44<02:54,  1.76it/s]evaluate for the 22-th batch, evaluate loss: 0.520790159702301:  46%|████████▋          | 21/46 [00:02<00:02,  9.69it/s]evaluate for the 22-th batch, evaluate loss: 0.520790159702301:  48%|█████████          | 22/46 [00:02<00:02,  9.70it/s]Epoch: 4, train for the 24-th batch, train loss: 0.473397821187973:  16%|██▏           | 23/146 [00:14<01:17,  1.60it/s]Epoch: 4, train for the 24-th batch, train loss: 0.473397821187973:  16%|██▎           | 24/146 [00:14<01:15,  1.61it/s]evaluate for the 23-th batch, evaluate loss: 0.47421911358833313:  48%|████████▏        | 22/46 [00:02<00:02,  9.70it/s]evaluate for the 23-th batch, evaluate loss: 0.47421911358833313:  50%|████████▌        | 23/46 [00:02<00:02,  9.69it/s]evaluate for the 4-th batch, evaluate loss: 0.6898933053016663:   8%|█▌                  | 3/40 [00:01<00:10,  3.52it/s]evaluate for the 4-th batch, evaluate loss: 0.6898933053016663:  10%|██                  | 4/40 [00:01<00:09,  3.65it/s]evaluate for the 24-th batch, evaluate loss: 0.47978582978248596:  50%|████████▌        | 23/46 [00:02<00:02,  9.69it/s]evaluate for the 24-th batch, evaluate loss: 0.47978582978248596:  52%|████████▊        | 24/46 [00:02<00:02,  9.69it/s]Epoch: 4, train for the 102-th batch, train loss: 0.19427847862243652:  85%|████████▍ | 101/119 [01:01<00:10,  1.70it/s]Epoch: 4, train for the 102-th batch, train loss: 0.19427847862243652:  86%|████████▌ | 102/119 [01:01<00:10,  1.68it/s]evaluate for the 25-th batch, evaluate loss: 0.539922833442688:  52%|█████████▉         | 24/46 [00:02<00:02,  9.69it/s]evaluate for the 25-th batch, evaluate loss: 0.539922833442688:  54%|██████████▎        | 25/46 [00:02<00:02,  9.69it/s]evaluate for the 26-th batch, evaluate loss: 0.5568791627883911:  54%|█████████▊        | 25/46 [00:02<00:02,  9.69it/s]evaluate for the 26-th batch, evaluate loss: 0.5568791627883911:  57%|██████████▏       | 26/46 [00:02<00:02,  9.67it/s]evaluate for the 5-th batch, evaluate loss: 0.6595360636711121:  10%|██                  | 4/40 [00:01<00:09,  3.65it/s]evaluate for the 5-th batch, evaluate loss: 0.6595360636711121:  12%|██▌                 | 5/40 [00:01<00:09,  3.55it/s]Epoch: 2, train for the 78-th batch, train loss: 0.33410459756851196:  20%|██▍         | 77/383 [00:45<02:54,  1.76it/s]Epoch: 2, train for the 78-th batch, train loss: 0.33410459756851196:  20%|██▍         | 78/383 [00:45<02:52,  1.76it/s]evaluate for the 27-th batch, evaluate loss: 0.5007874369621277:  57%|██████████▏       | 26/46 [00:02<00:02,  9.67it/s]evaluate for the 27-th batch, evaluate loss: 0.5007874369621277:  59%|██████████▌       | 27/46 [00:02<00:01,  9.68it/s]evaluate for the 28-th batch, evaluate loss: 0.5255374908447266:  59%|██████████▌       | 27/46 [00:02<00:01,  9.68it/s]evaluate for the 28-th batch, evaluate loss: 0.5255374908447266:  61%|██████████▉       | 28/46 [00:02<00:01,  9.69it/s]Epoch: 4, train for the 25-th batch, train loss: 0.47663551568984985:  16%|█▉          | 24/146 [00:14<01:15,  1.61it/s]Epoch: 4, train for the 25-th batch, train loss: 0.47663551568984985:  17%|██          | 25/146 [00:14<01:14,  1.62it/s]evaluate for the 6-th batch, evaluate loss: 0.687351644039154:  12%|██▋                  | 5/40 [00:01<00:09,  3.55it/s]evaluate for the 6-th batch, evaluate loss: 0.687351644039154:  15%|███▏                 | 6/40 [00:01<00:09,  3.65it/s]evaluate for the 29-th batch, evaluate loss: 0.4930102527141571:  61%|██████████▉       | 28/46 [00:02<00:01,  9.69it/s]evaluate for the 29-th batch, evaluate loss: 0.4930102527141571:  63%|███████████▎      | 29/46 [00:03<00:01,  9.68it/s]evaluate for the 30-th batch, evaluate loss: 0.49808773398399353:  63%|██████████▋      | 29/46 [00:03<00:01,  9.68it/s]evaluate for the 30-th batch, evaluate loss: 0.49808773398399353:  65%|███████████      | 30/46 [00:03<00:01,  9.64it/s]Epoch: 4, train for the 103-th batch, train loss: 0.1960119903087616:  86%|█████████▍ | 102/119 [01:02<00:10,  1.68it/s]Epoch: 4, train for the 103-th batch, train loss: 0.1960119903087616:  87%|█████████▌ | 103/119 [01:02<00:09,  1.67it/s]evaluate for the 31-th batch, evaluate loss: 0.5185464024543762:  65%|███████████▋      | 30/46 [00:03<00:01,  9.64it/s]evaluate for the 31-th batch, evaluate loss: 0.5185464024543762:  67%|████████████▏     | 31/46 [00:03<00:01,  9.63it/s]evaluate for the 7-th batch, evaluate loss: 0.6668354868888855:  15%|███                 | 6/40 [00:01<00:09,  3.65it/s]evaluate for the 7-th batch, evaluate loss: 0.6668354868888855:  18%|███▌                | 7/40 [00:01<00:09,  3.55it/s]evaluate for the 32-th batch, evaluate loss: 0.47739002108573914:  67%|███████████▍     | 31/46 [00:03<00:01,  9.63it/s]evaluate for the 32-th batch, evaluate loss: 0.47739002108573914:  70%|███████████▊     | 32/46 [00:03<00:01,  9.66it/s]Epoch: 2, train for the 79-th batch, train loss: 0.36331626772880554:  20%|██▍         | 78/383 [00:45<02:52,  1.76it/s]Epoch: 2, train for the 79-th batch, train loss: 0.36331626772880554:  21%|██▍         | 79/383 [00:45<02:51,  1.77it/s]evaluate for the 33-th batch, evaluate loss: 0.49995672702789307:  70%|███████████▊     | 32/46 [00:03<00:01,  9.66it/s]evaluate for the 33-th batch, evaluate loss: 0.49995672702789307:  72%|████████████▏    | 33/46 [00:03<00:01,  9.68it/s]evaluate for the 34-th batch, evaluate loss: 0.4875078499317169:  72%|████████████▉     | 33/46 [00:03<00:01,  9.68it/s]evaluate for the 34-th batch, evaluate loss: 0.4875078499317169:  74%|█████████████▎    | 34/46 [00:03<00:01,  9.69it/s]evaluate for the 8-th batch, evaluate loss: 0.7228937149047852:  18%|███▌                | 7/40 [00:02<00:09,  3.55it/s]evaluate for the 8-th batch, evaluate loss: 0.7228937149047852:  20%|████                | 8/40 [00:02<00:08,  3.65it/s]Epoch: 4, train for the 26-th batch, train loss: 0.44200775027275085:  17%|██          | 25/146 [00:15<01:14,  1.62it/s]Epoch: 4, train for the 26-th batch, train loss: 0.44200775027275085:  18%|██▏         | 26/146 [00:15<01:14,  1.61it/s]evaluate for the 35-th batch, evaluate loss: 0.48322927951812744:  74%|████████████▌    | 34/46 [00:03<00:01,  9.69it/s]evaluate for the 35-th batch, evaluate loss: 0.48322927951812744:  76%|████████████▉    | 35/46 [00:03<00:01,  9.68it/s]evaluate for the 36-th batch, evaluate loss: 0.4739581048488617:  76%|█████████████▋    | 35/46 [00:03<00:01,  9.68it/s]evaluate for the 36-th batch, evaluate loss: 0.4739581048488617:  78%|██████████████    | 36/46 [00:03<00:01,  9.67it/s]Epoch: 4, train for the 104-th batch, train loss: 0.20854777097702026:  87%|████████▋ | 103/119 [01:02<00:09,  1.67it/s]Epoch: 4, train for the 104-th batch, train loss: 0.20854777097702026:  87%|████████▋ | 104/119 [01:02<00:09,  1.65it/s]evaluate for the 37-th batch, evaluate loss: 0.5065125226974487:  78%|██████████████    | 36/46 [00:03<00:01,  9.67it/s]evaluate for the 37-th batch, evaluate loss: 0.5065125226974487:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.68it/s]evaluate for the 9-th batch, evaluate loss: 0.7006609439849854:  20%|████                | 8/40 [00:02<00:08,  3.65it/s]evaluate for the 9-th batch, evaluate loss: 0.7006609439849854:  22%|████▌               | 9/40 [00:02<00:08,  3.56it/s]Epoch: 2, train for the 80-th batch, train loss: 0.3237028121948242:  21%|██▋          | 79/383 [00:46<02:51,  1.77it/s]Epoch: 2, train for the 80-th batch, train loss: 0.3237028121948242:  21%|██▋          | 80/383 [00:46<02:50,  1.78it/s]evaluate for the 38-th batch, evaluate loss: 0.5375051498413086:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.68it/s]evaluate for the 38-th batch, evaluate loss: 0.5375051498413086:  83%|██████████████▊   | 38/46 [00:03<00:00,  9.67it/s]evaluate for the 39-th batch, evaluate loss: 0.5377498865127563:  83%|██████████████▊   | 38/46 [00:04<00:00,  9.67it/s]evaluate for the 39-th batch, evaluate loss: 0.5377498865127563:  85%|███████████████▎  | 39/46 [00:04<00:00,  9.63it/s]evaluate for the 10-th batch, evaluate loss: 0.6822639107704163:  22%|████▎              | 9/40 [00:02<00:08,  3.56it/s]evaluate for the 10-th batch, evaluate loss: 0.6822639107704163:  25%|████▌             | 10/40 [00:02<00:08,  3.64it/s]evaluate for the 40-th batch, evaluate loss: 0.4711638391017914:  85%|███████████████▎  | 39/46 [00:04<00:00,  9.63it/s]evaluate for the 40-th batch, evaluate loss: 0.4711638391017914:  87%|███████████████▋  | 40/46 [00:04<00:00,  9.60it/s]Epoch: 4, train for the 27-th batch, train loss: 0.43484488129615784:  18%|██▏         | 26/146 [00:15<01:14,  1.61it/s]Epoch: 4, train for the 27-th batch, train loss: 0.43484488129615784:  18%|██▏         | 27/146 [00:15<01:13,  1.62it/s]evaluate for the 41-th batch, evaluate loss: 0.48416799306869507:  87%|██████████████▊  | 40/46 [00:04<00:00,  9.60it/s]evaluate for the 41-th batch, evaluate loss: 0.48416799306869507:  89%|███████████████▏ | 41/46 [00:04<00:00,  9.58it/s]evaluate for the 42-th batch, evaluate loss: 0.4711700677871704:  89%|████████████████  | 41/46 [00:04<00:00,  9.58it/s]evaluate for the 42-th batch, evaluate loss: 0.4711700677871704:  91%|████████████████▍ | 42/46 [00:04<00:00,  9.59it/s]Epoch: 4, train for the 105-th batch, train loss: 0.1962122917175293:  87%|█████████▌ | 104/119 [01:03<00:09,  1.65it/s]Epoch: 4, train for the 105-th batch, train loss: 0.1962122917175293:  88%|█████████▋ | 105/119 [01:03<00:08,  1.64it/s]evaluate for the 11-th batch, evaluate loss: 0.6897291541099548:  25%|████▌             | 10/40 [00:03<00:08,  3.64it/s]evaluate for the 11-th batch, evaluate loss: 0.6897291541099548:  28%|████▉             | 11/40 [00:03<00:08,  3.56it/s]Epoch: 2, train for the 81-th batch, train loss: 0.3436644673347473:  21%|██▋          | 80/383 [00:47<02:50,  1.78it/s]Epoch: 2, train for the 81-th batch, train loss: 0.3436644673347473:  21%|██▋          | 81/383 [00:47<02:50,  1.78it/s]evaluate for the 43-th batch, evaluate loss: 0.5368310809135437:  91%|████████████████▍ | 42/46 [00:04<00:00,  9.59it/s]evaluate for the 43-th batch, evaluate loss: 0.5368310809135437:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.56it/s]evaluate for the 44-th batch, evaluate loss: 0.5144601464271545:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.56it/s]evaluate for the 44-th batch, evaluate loss: 0.5144601464271545:  96%|█████████████████▏| 44/46 [00:04<00:00,  9.57it/s]evaluate for the 12-th batch, evaluate loss: 0.7387049198150635:  28%|████▉             | 11/40 [00:03<00:08,  3.56it/s]evaluate for the 12-th batch, evaluate loss: 0.7387049198150635:  30%|█████▍            | 12/40 [00:03<00:08,  3.48it/s]Epoch: 4, train for the 28-th batch, train loss: 0.4224504232406616:  18%|██▍          | 27/146 [00:16<01:13,  1.62it/s]Epoch: 4, train for the 28-th batch, train loss: 0.4224504232406616:  19%|██▍          | 28/146 [00:16<01:09,  1.70it/s]evaluate for the 45-th batch, evaluate loss: 0.49408891797065735:  96%|████████████████▎| 44/46 [00:04<00:00,  9.57it/s]evaluate for the 45-th batch, evaluate loss: 0.49408891797065735:  98%|████████████████▋| 45/46 [00:04<00:00,  7.67it/s]evaluate for the 46-th batch, evaluate loss: 0.5065615773200989:  98%|█████████████████▌| 45/46 [00:04<00:00,  7.67it/s]evaluate for the 46-th batch, evaluate loss: 0.5065615773200989: 100%|██████████████████| 46/46 [00:04<00:00,  9.50it/s]
  0%|                                                                                            | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.646152913570404:   0%|                             | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.646152913570404:   4%|▊                    | 1/25 [00:00<00:02,  9.08it/s]evaluate for the 13-th batch, evaluate loss: 0.682766854763031:  30%|█████▋             | 12/40 [00:03<00:08,  3.48it/s]evaluate for the 13-th batch, evaluate loss: 0.682766854763031:  32%|██████▏            | 13/40 [00:03<00:07,  3.62it/s]Epoch: 4, train for the 106-th batch, train loss: 0.16985249519348145:  88%|████████▊ | 105/119 [01:04<00:08,  1.64it/s]Epoch: 4, train for the 106-th batch, train loss: 0.16985249519348145:  89%|████████▉ | 106/119 [01:04<00:08,  1.61it/s]evaluate for the 2-th batch, evaluate loss: 0.6523617506027222:   4%|▊                   | 1/25 [00:00<00:02,  9.08it/s]evaluate for the 2-th batch, evaluate loss: 0.6523617506027222:   8%|█▌                  | 2/25 [00:00<00:02,  9.02it/s]Epoch: 2, train for the 82-th batch, train loss: 0.3961170017719269:  21%|██▋          | 81/383 [00:47<02:50,  1.78it/s]Epoch: 2, train for the 82-th batch, train loss: 0.3961170017719269:  21%|██▊          | 82/383 [00:47<02:57,  1.69it/s]evaluate for the 3-th batch, evaluate loss: 0.6980051398277283:   8%|█▌                  | 2/25 [00:00<00:02,  9.02it/s]evaluate for the 3-th batch, evaluate loss: 0.6980051398277283:  12%|██▍                 | 3/25 [00:00<00:02,  9.06it/s]evaluate for the 14-th batch, evaluate loss: 0.726978600025177:  32%|██████▏            | 13/40 [00:03<00:07,  3.62it/s]evaluate for the 14-th batch, evaluate loss: 0.726978600025177:  35%|██████▋            | 14/40 [00:03<00:07,  3.55it/s]Epoch: 4, train for the 29-th batch, train loss: 0.41050368547439575:  19%|██▎         | 28/146 [00:16<01:09,  1.70it/s]Epoch: 4, train for the 29-th batch, train loss: 0.41050368547439575:  20%|██▍         | 29/146 [00:16<01:07,  1.75it/s]evaluate for the 4-th batch, evaluate loss: 0.6817978024482727:  12%|██▍                 | 3/25 [00:00<00:02,  9.06it/s]evaluate for the 4-th batch, evaluate loss: 0.6817978024482727:  16%|███▏                | 4/25 [00:00<00:02,  9.13it/s]evaluate for the 5-th batch, evaluate loss: 0.6705873608589172:  16%|███▏                | 4/25 [00:00<00:02,  9.13it/s]evaluate for the 5-th batch, evaluate loss: 0.6705873608589172:  20%|████                | 5/25 [00:00<00:02,  9.11it/s]evaluate for the 6-th batch, evaluate loss: 0.7202742099761963:  20%|████                | 5/25 [00:00<00:02,  9.11it/s]evaluate for the 6-th batch, evaluate loss: 0.7202742099761963:  24%|████▊               | 6/25 [00:00<00:02,  9.10it/s]evaluate for the 15-th batch, evaluate loss: 0.7409733533859253:  35%|██████▎           | 14/40 [00:04<00:07,  3.55it/s]evaluate for the 15-th batch, evaluate loss: 0.7409733533859253:  38%|██████▊           | 15/40 [00:04<00:07,  3.53it/s]evaluate for the 7-th batch, evaluate loss: 0.7447347640991211:  24%|████▊               | 6/25 [00:00<00:02,  9.10it/s]evaluate for the 7-th batch, evaluate loss: 0.7447347640991211:  28%|█████▌              | 7/25 [00:00<00:01,  9.07it/s]Epoch: 4, train for the 107-th batch, train loss: 0.2652101516723633:  89%|█████████▊ | 106/119 [01:04<00:08,  1.61it/s]Epoch: 4, train for the 107-th batch, train loss: 0.2652101516723633:  90%|█████████▉ | 107/119 [01:04<00:07,  1.62it/s]Epoch: 2, train for the 83-th batch, train loss: 0.38121140003204346:  21%|██▌         | 82/383 [00:48<02:57,  1.69it/s]Epoch: 2, train for the 83-th batch, train loss: 0.38121140003204346:  22%|██▌         | 83/383 [00:48<02:57,  1.69it/s]evaluate for the 8-th batch, evaluate loss: 0.7239303588867188:  28%|█████▌              | 7/25 [00:00<00:01,  9.07it/s]evaluate for the 8-th batch, evaluate loss: 0.7239303588867188:  32%|██████▍             | 8/25 [00:00<00:01,  9.09it/s]Epoch: 4, train for the 30-th batch, train loss: 0.4631645977497101:  20%|██▌          | 29/146 [00:17<01:07,  1.75it/s]Epoch: 4, train for the 30-th batch, train loss: 0.4631645977497101:  21%|██▋          | 30/146 [00:17<01:05,  1.77it/s]evaluate for the 16-th batch, evaluate loss: 0.6951370239257812:  38%|██████▊           | 15/40 [00:04<00:07,  3.53it/s]evaluate for the 16-th batch, evaluate loss: 0.6951370239257812:  40%|███████▏          | 16/40 [00:04<00:06,  3.47it/s]evaluate for the 9-th batch, evaluate loss: 0.7101089954376221:  32%|██████▍             | 8/25 [00:00<00:01,  9.09it/s]evaluate for the 9-th batch, evaluate loss: 0.7101089954376221:  36%|███████▏            | 9/25 [00:00<00:01,  9.07it/s]evaluate for the 10-th batch, evaluate loss: 0.7456496357917786:  36%|██████▊            | 9/25 [00:01<00:01,  9.07it/s]evaluate for the 10-th batch, evaluate loss: 0.7456496357917786:  40%|███████▏          | 10/25 [00:01<00:01,  9.07it/s]evaluate for the 11-th batch, evaluate loss: 0.7377901077270508:  40%|███████▏          | 10/25 [00:01<00:01,  9.07it/s]evaluate for the 11-th batch, evaluate loss: 0.7377901077270508:  44%|███████▉          | 11/25 [00:01<00:01,  9.07it/s]evaluate for the 17-th batch, evaluate loss: 0.6859719753265381:  40%|███████▏          | 16/40 [00:04<00:06,  3.47it/s]evaluate for the 17-th batch, evaluate loss: 0.6859719753265381:  42%|███████▋          | 17/40 [00:04<00:06,  3.52it/s]evaluate for the 12-th batch, evaluate loss: 0.7016288042068481:  44%|███████▉          | 11/25 [00:01<00:01,  9.07it/s]evaluate for the 12-th batch, evaluate loss: 0.7016288042068481:  48%|████████▋         | 12/25 [00:01<00:01,  9.10it/s]Epoch: 4, train for the 108-th batch, train loss: 0.14887002110481262:  90%|████████▉ | 107/119 [01:05<00:07,  1.62it/s]Epoch: 4, train for the 108-th batch, train loss: 0.14887002110481262:  91%|█████████ | 108/119 [01:05<00:06,  1.61it/s]Epoch: 2, train for the 84-th batch, train loss: 0.44691169261932373:  22%|██▌         | 83/383 [00:48<02:57,  1.69it/s]Epoch: 2, train for the 84-th batch, train loss: 0.44691169261932373:  22%|██▋         | 84/383 [00:48<02:55,  1.70it/s]evaluate for the 13-th batch, evaluate loss: 0.6730589270591736:  48%|████████▋         | 12/25 [00:01<00:01,  9.10it/s]evaluate for the 13-th batch, evaluate loss: 0.6730589270591736:  52%|█████████▎        | 13/25 [00:01<00:01,  9.12it/s]evaluate for the 14-th batch, evaluate loss: 0.7579111456871033:  52%|█████████▎        | 13/25 [00:01<00:01,  9.12it/s]evaluate for the 14-th batch, evaluate loss: 0.7579111456871033:  56%|██████████        | 14/25 [00:01<00:01,  9.10it/s]evaluate for the 18-th batch, evaluate loss: 0.7046685814857483:  42%|███████▋          | 17/40 [00:05<00:06,  3.52it/s]evaluate for the 18-th batch, evaluate loss: 0.7046685814857483:  45%|████████          | 18/40 [00:05<00:06,  3.44it/s]Epoch: 4, train for the 31-th batch, train loss: 0.49660858511924744:  21%|██▍         | 30/146 [00:18<01:05,  1.77it/s]Epoch: 4, train for the 31-th batch, train loss: 0.49660858511924744:  21%|██▌         | 31/146 [00:18<01:06,  1.72it/s]evaluate for the 15-th batch, evaluate loss: 0.7411669492721558:  56%|██████████        | 14/25 [00:01<00:01,  9.10it/s]evaluate for the 15-th batch, evaluate loss: 0.7411669492721558:  60%|██████████▊       | 15/25 [00:01<00:01,  6.18it/s]Epoch: 4, train for the 109-th batch, train loss: 0.21427184343338013:  91%|█████████ | 108/119 [01:05<00:06,  1.61it/s]Epoch: 4, train for the 109-th batch, train loss: 0.21427184343338013:  92%|█████████▏| 109/119 [01:05<00:05,  1.78it/s]evaluate for the 19-th batch, evaluate loss: 0.7067059278488159:  45%|████████          | 18/40 [00:05<00:06,  3.44it/s]evaluate for the 19-th batch, evaluate loss: 0.7067059278488159:  48%|████████▌         | 19/40 [00:05<00:06,  3.32it/s]evaluate for the 16-th batch, evaluate loss: 0.6690028309822083:  60%|██████████▊       | 15/25 [00:01<00:01,  6.18it/s]evaluate for the 16-th batch, evaluate loss: 0.6690028309822083:  64%|███████████▌      | 16/25 [00:01<00:01,  6.76it/s]Epoch: 2, train for the 85-th batch, train loss: 0.39671459794044495:  22%|██▋         | 84/383 [00:49<02:55,  1.70it/s]Epoch: 2, train for the 85-th batch, train loss: 0.39671459794044495:  22%|██▋         | 85/383 [00:49<02:58,  1.67it/s]evaluate for the 17-th batch, evaluate loss: 0.6706832051277161:  64%|███████████▌      | 16/25 [00:02<00:01,  6.76it/s]evaluate for the 17-th batch, evaluate loss: 0.6706832051277161:  68%|████████████▏     | 17/25 [00:02<00:01,  7.29it/s]evaluate for the 18-th batch, evaluate loss: 0.6268498301506042:  68%|████████████▏     | 17/25 [00:02<00:01,  7.29it/s]evaluate for the 18-th batch, evaluate loss: 0.6268498301506042:  72%|████████████▉     | 18/25 [00:02<00:00,  7.76it/s]evaluate for the 20-th batch, evaluate loss: 0.7489473819732666:  48%|████████▌         | 19/40 [00:05<00:06,  3.32it/s]evaluate for the 20-th batch, evaluate loss: 0.7489473819732666:  50%|█████████         | 20/40 [00:05<00:06,  3.33it/s]evaluate for the 19-th batch, evaluate loss: 0.597320556640625:  72%|█████████████▋     | 18/25 [00:02<00:00,  7.76it/s]evaluate for the 19-th batch, evaluate loss: 0.597320556640625:  76%|██████████████▍    | 19/25 [00:02<00:00,  8.16it/s]Epoch: 4, train for the 32-th batch, train loss: 0.4260379672050476:  21%|██▊          | 31/146 [00:18<01:06,  1.72it/s]Epoch: 4, train for the 32-th batch, train loss: 0.4260379672050476:  22%|██▊          | 32/146 [00:18<01:11,  1.60it/s]usage: Interface for the link prediction task [-h]
                                              [--dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}]
                                              [--batch_size BATCH_SIZE]
                                              [--model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}]
                                              [--gpu GPU]
                                              [--num_neighbors NUM_NEIGHBORS]
                                              [--sample_neighbor_strategy {uniform,recent,time_interval_aware}]
                                              [--time_scaling_factor TIME_SCALING_FACTOR]
                                              [--num_walk_heads NUM_WALK_HEADS]
                                              [--num_heads NUM_HEADS]
                                              [--num_layers NUM_LAYERS]
                                              [--walk_length WALK_LENGTH]
                                              [--time_gap TIME_GAP]
                                              [--time_feat_dim TIME_FEAT_DIM]
                                              [--position_feat_dim POSITION_FEAT_DIM]
                                              [--edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}]
                                              [--time_window_mode {fixed_proportion,repeat_interval}]
                                              [--patch_size PATCH_SIZE]
                                              [--channel_embedding_dim CHANNEL_EMBEDDING_DIM]
                                              [--max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH]
                                              [--learning_rate LEARNING_RATE]
                                              [--dropout DROPOUT]
                                              [--num_epochs NUM_EPOCHS]
                                              [--optimizer {SGD,Adam,RMSprop,AdamW}]
                                              [--weight_decay WEIGHT_DECAY]
                                              [--patience PATIENCE]
                                              [--val_ratio VAL_RATIO]
                                              [--test_ratio TEST_RATIO]
                                              [--num_runs NUM_RUNS]
                                              [--test_interval_epochs TEST_INTERVAL_EPOCHS]
                                              [--negative_sample_strategy {random,historical,inductive}]
                                              [--load_best_configs]
                                              [--use_wandb USE_WANDB]
                                              [--use_ROPe]
                                              [--t1_factor_of_t2 T1_FACTOR_OF_T2]
                                              [--use_init_method] [--emb_proj]
                                              [--init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}]
                                              [--clip CLIP]
Interface for the link prediction task: error: argument --dataset_name: expected one argument
True
usage: Interface for the link prediction task [-h]
                                              [--dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}]
                                              [--batch_size BATCH_SIZE]
                                              [--model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}]
                                              [--gpu GPU]
                                              [--num_neighbors NUM_NEIGHBORS]
                                              [--sample_neighbor_strategy {uniform,recent,time_interval_aware}]
                                              [--time_scaling_factor TIME_SCALING_FACTOR]
                                              [--num_walk_heads NUM_WALK_HEADS]
                                              [--num_heads NUM_HEADS]
                                              [--num_layers NUM_LAYERS]
                                              [--walk_length WALK_LENGTH]
                                              [--time_gap TIME_GAP]
                                              [--time_feat_dim TIME_FEAT_DIM]
                                              [--position_feat_dim POSITION_FEAT_DIM]
                                              [--edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}]
                                              [--time_window_mode {fixed_proportion,repeat_interval}]
                                              [--patch_size PATCH_SIZE]
                                              [--channel_embedding_dim CHANNEL_EMBEDDING_DIM]
                                              [--max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH]
                                              [--learning_rate LEARNING_RATE]
                                              [--dropout DROPOUT]
                                              [--num_epochs NUM_EPOCHS]
                                              [--optimizer {SGD,Adam,RMSprop,AdamW}]
                                              [--weight_decay WEIGHT_DECAY]
                                              [--patience PATIENCE]
                                              [--val_ratio VAL_RATIO]
                                              [--test_ratio TEST_RATIO]
                                              [--num_runs NUM_RUNS]
                                              [--test_interval_epochs TEST_INTERVAL_EPOCHS]
                                              [--negative_sample_strategy {random,historical,inductive}]
                                              [--load_best_configs]
                                              [--use_wandb USE_WANDB]
                                              [--use_ROPe]
                                              [--t1_factor_of_t2 T1_FACTOR_OF_T2]
                                              [--use_init_method] [--emb_proj]
                                              [--init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}]
                                              [--clip CLIP]

options:
  -h, --help            show this help message and exit
  --dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}
                        dataset to be used
  --batch_size BATCH_SIZE
                        batch size
  --model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}
                        name of the model, note that EdgeBank is only
                        applicable for evaluation
  --gpu GPU             number of gpu to use
  --num_neighbors NUM_NEIGHBORS
                        number of neighbors to sample for each node
  --sample_neighbor_strategy {uniform,recent,time_interval_aware}
                        how to sample historical neighbors
  --time_scaling_factor TIME_SCALING_FACTOR
                        the hyperparameter that controls the sampling
                        preference with time interval, a large
                        time_scaling_factor tends to sample more on recent
                        links, 0.0 corresponds to uniform sampling, it works
                        when sample_neighbor_strategy == time_interval_aware
  --num_walk_heads NUM_WALK_HEADS
                        number of heads used for the attention in walk encoder
  --num_heads NUM_HEADS
                        number of heads used in attention layer
  --num_layers NUM_LAYERS
                        number of model layers
  --walk_length WALK_LENGTH
                        length of each random walk
  --time_gap TIME_GAP   time gap for neighbors to compute node features
  --time_feat_dim TIME_FEAT_DIM
                        dimension of the time embedding
  --position_feat_dim POSITION_FEAT_DIM
                        dimension of the position embedding
  --edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}
                        how memory of EdgeBank works
  --time_window_mode {fixed_proportion,repeat_interval}
                        how to select the time window size for time window
                        memory
  --patch_size PATCH_SIZE
                        patch size
  --channel_embedding_dim CHANNEL_EMBEDDING_DIM
                        dimension of each channel embedding
  --max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH
                        maximal length of the input sequence of each node
  --learning_rate LEARNING_RATE
                        learning rate
  --dropout DROPOUT     dropout rate
  --num_epochs NUM_EPOCHS
                        number of epochs
  --optimizer {SGD,Adam,RMSprop,AdamW}
                        name of optimizer
  --weight_decay WEIGHT_DECAY
                        weight decay
  --patience PATIENCE   patience for early stopping
  --val_ratio VAL_RATIO
                        ratio of validation set
  --test_ratio TEST_RATIO
                        ratio of test set
  --num_runs NUM_RUNS   number of runs
  --test_interval_epochs TEST_INTERVAL_EPOCHS
                        how many epochs to perform testing once
  --negative_sample_strategy {random,historical,inductive}
                        strategy for the negative edge sampling
  --load_best_configs   whether to load the best configurations
  --use_wandb USE_WANDB
                        do you want to track this run using wandb? If arg is
                        `no`, then don`t track. Else, track using wandb and
                        name the run `run_name_dataset`
  --use_ROPe            Use ROPe embeddings for DecoLP
  --t1_factor_of_t2 T1_FACTOR_OF_T2
                        t1 factor of t2 when t2 is 0.04 of total time
  --use_init_method     Use new init method
  --emb_proj            Use embedding projection before weighted average
  --init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}
  --clip CLIP           clip val for grad of time-transformation operation
usage: Interface for the link prediction task [-h]
                                              [--dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}]
                                              [--batch_size BATCH_SIZE]
                                              [--model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}]
                                              [--gpu GPU]
                                              [--num_neighbors NUM_NEIGHBORS]
                                              [--sample_neighbor_strategy {uniform,recent,time_interval_aware}]
                                              [--time_scaling_factor TIME_SCALING_FACTOR]
                                              [--num_walk_heads NUM_WALK_HEADS]
                                              [--num_heads NUM_HEADS]
                                              [--num_layers NUM_LAYERS]
                                              [--walk_length WALK_LENGTH]
                                              [--time_gap TIME_GAP]
                                              [--time_feat_dim TIME_FEAT_DIM]
                                              [--position_feat_dim POSITION_FEAT_DIM]
                                              [--edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}]
                                              [--time_window_mode {fixed_proportion,repeat_interval}]
                                              [--patch_size PATCH_SIZE]
                                              [--channel_embedding_dim CHANNEL_EMBEDDING_DIM]
                                              [--max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH]
                                              [--learning_rate LEARNING_RATE]
                                              [--dropout DROPOUT]
                                              [--num_epochs NUM_EPOCHS]
                                              [--optimizer {SGD,Adam,RMSprop,AdamW}]
                                              [--weight_decay WEIGHT_DECAY]
                                              [--patience PATIENCE]
                                              [--val_ratio VAL_RATIO]
                                              [--test_ratio TEST_RATIO]
                                              [--num_runs NUM_RUNS]
                                              [--test_interval_epochs TEST_INTERVAL_EPOCHS]
                                              [--negative_sample_strategy {random,historical,inductive}]
                                              [--load_best_configs]
                                              [--use_wandb USE_WANDB]
                                              [--use_ROPe]
                                              [--t1_factor_of_t2 T1_FACTOR_OF_T2]
                                              [--use_init_method] [--emb_proj]
                                              [--init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}]
                                              [--clip CLIP]
Interface for the link prediction task: error: argument --dataset_name: expected one argument
True
usage: Interface for the link prediction task [-h]
                                              [--dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}]
                                              [--batch_size BATCH_SIZE]
                                              [--model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}]
                                              [--gpu GPU]
                                              [--num_neighbors NUM_NEIGHBORS]
                                              [--sample_neighbor_strategy {uniform,recent,time_interval_aware}]
                                              [--time_scaling_factor TIME_SCALING_FACTOR]
                                              [--num_walk_heads NUM_WALK_HEADS]
                                              [--num_heads NUM_HEADS]
                                              [--num_layers NUM_LAYERS]
                                              [--walk_length WALK_LENGTH]
                                              [--time_gap TIME_GAP]
                                              [--time_feat_dim TIME_FEAT_DIM]
                                              [--position_feat_dim POSITION_FEAT_DIM]
                                              [--edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}]
                                              [--time_window_mode {fixed_proportion,repeat_interval}]
                                              [--patch_size PATCH_SIZE]
                                              [--channel_embedding_dim CHANNEL_EMBEDDING_DIM]
                                              [--max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH]
                                              [--learning_rate LEARNING_RATE]
                                              [--dropout DROPOUT]
                                              [--num_epochs NUM_EPOCHS]
                                              [--optimizer {SGD,Adam,RMSprop,AdamW}]
                                              [--weight_decay WEIGHT_DECAY]
                                              [--patience PATIENCE]
                                              [--val_ratio VAL_RATIO]
                                              [--test_ratio TEST_RATIO]
                                              [--num_runs NUM_RUNS]
                                              [--test_interval_epochs TEST_INTERVAL_EPOCHS]
                                              [--negative_sample_strategy {random,historical,inductive}]
                                              [--load_best_configs]
                                              [--use_wandb USE_WANDB]
                                              [--use_ROPe]
                                              [--t1_factor_of_t2 T1_FACTOR_OF_T2]
                                              [--use_init_method] [--emb_proj]
                                              [--init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}]
                                              [--clip CLIP]

options:
  -h, --help            show this help message and exit
  --dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}
                        dataset to be used
  --batch_size BATCH_SIZE
                        batch size
  --model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}
                        name of the model, note that EdgeBank is only
                        applicable for evaluation
  --gpu GPU             number of gpu to use
  --num_neighbors NUM_NEIGHBORS
                        number of neighbors to sample for each node
  --sample_neighbor_strategy {uniform,recent,time_interval_aware}
                        how to sample historical neighbors
  --time_scaling_factor TIME_SCALING_FACTOR
                        the hyperparameter that controls the sampling
                        preference with time interval, a large
                        time_scaling_factor tends to sample more on recent
                        links, 0.0 corresponds to uniform sampling, it works
                        when sample_neighbor_strategy == time_interval_aware
  --num_walk_heads NUM_WALK_HEADS
                        number of heads used for the attention in walk encoder
  --num_heads NUM_HEADS
                        number of heads used in attention layer
  --num_layers NUM_LAYERS
                        number of model layers
  --walk_length WALK_LENGTH
                        length of each random walk
  --time_gap TIME_GAP   time gap for neighbors to compute node features
  --time_feat_dim TIME_FEAT_DIM
                        dimension of the time embedding
  --position_feat_dim POSITION_FEAT_DIM
                        dimension of the position embedding
  --edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}
                        how memory of EdgeBank works
  --time_window_mode {fixed_proportion,repeat_interval}
                        how to select the time window size for time window
                        memory
  --patch_size PATCH_SIZE
                        patch size
  --channel_embedding_dim CHANNEL_EMBEDDING_DIM
                        dimension of each channel embedding
  --max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH
                        maximal length of the input sequence of each node
  --learning_rate LEARNING_RATE
                        learning rate
  --dropout DROPOUT     dropout rate
  --num_epochs NUM_EPOCHS
                        number of epochs
  --optimizer {SGD,Adam,RMSprop,AdamW}
                        name of optimizer
  --weight_decay WEIGHT_DECAY
                        weight decay
  --patience PATIENCE   patience for early stopping
  --val_ratio VAL_RATIO
                        ratio of validation set
  --test_ratio TEST_RATIO
                        ratio of test set
  --num_runs NUM_RUNS   number of runs
  --test_interval_epochs TEST_INTERVAL_EPOCHS
                        how many epochs to perform testing once
  --negative_sample_strategy {random,historical,inductive}
                        strategy for the negative edge sampling
  --load_best_configs   whether to load the best configurations
  --use_wandb USE_WANDB
                        do you want to track this run using wandb? If arg is
                        `no`, then don`t track. Else, track using wandb and
                        name the run `run_name_dataset`
  --use_ROPe            Use ROPe embeddings for DecoLP
  --t1_factor_of_t2 T1_FACTOR_OF_T2
                        t1 factor of t2 when t2 is 0.04 of total time
  --use_init_method     Use new init method
  --emb_proj            Use embedding projection before weighted average
  --init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}
  --clip CLIP           clip val for grad of time-transformation operation
evaluate for the 20-th batch, evaluate loss: 0.6687446236610413:  76%|█████████████▋    | 19/25 [00:02<00:00,  8.16it/s]evaluate for the 20-th batch, evaluate loss: 0.6687446236610413:  80%|██████████████▍   | 20/25 [00:02<00:00,  8.41it/s]usage: Interface for the link prediction task [-h]
                                              [--dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}]
                                              [--batch_size BATCH_SIZE]
                                              [--model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}]
                                              [--gpu GPU]
                                              [--num_neighbors NUM_NEIGHBORS]
                                              [--sample_neighbor_strategy {uniform,recent,time_interval_aware}]
                                              [--time_scaling_factor TIME_SCALING_FACTOR]
                                              [--num_walk_heads NUM_WALK_HEADS]
                                              [--num_heads NUM_HEADS]
                                              [--num_layers NUM_LAYERS]
                                              [--walk_length WALK_LENGTH]
                                              [--time_gap TIME_GAP]
                                              [--time_feat_dim TIME_FEAT_DIM]
                                              [--position_feat_dim POSITION_FEAT_DIM]
                                              [--edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}]
                                              [--time_window_mode {fixed_proportion,repeat_interval}]
                                              [--patch_size PATCH_SIZE]
                                              [--channel_embedding_dim CHANNEL_EMBEDDING_DIM]
                                              [--max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH]
                                              [--learning_rate LEARNING_RATE]
                                              [--dropout DROPOUT]
                                              [--num_epochs NUM_EPOCHS]
                                              [--optimizer {SGD,Adam,RMSprop,AdamW}]
                                              [--weight_decay WEIGHT_DECAY]
                                              [--patience PATIENCE]
                                              [--val_ratio VAL_RATIO]
                                              [--test_ratio TEST_RATIO]
                                              [--num_runs NUM_RUNS]
                                              [--test_interval_epochs TEST_INTERVAL_EPOCHS]
                                              [--negative_sample_strategy {random,historical,inductive}]
                                              [--load_best_configs]
                                              [--use_wandb USE_WANDB]
                                              [--use_ROPe]
                                              [--t1_factor_of_t2 T1_FACTOR_OF_T2]
                                              [--use_init_method] [--emb_proj]
                                              [--init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}]
                                              [--clip CLIP]
Interface for the link prediction task: error: argument --dataset_name: expected one argument
True
usage: Interface for the link prediction task [-h]
                                              [--dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}]
                                              [--batch_size BATCH_SIZE]
                                              [--model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}]
                                              [--gpu GPU]
                                              [--num_neighbors NUM_NEIGHBORS]
                                              [--sample_neighbor_strategy {uniform,recent,time_interval_aware}]
                                              [--time_scaling_factor TIME_SCALING_FACTOR]
                                              [--num_walk_heads NUM_WALK_HEADS]
                                              [--num_heads NUM_HEADS]
                                              [--num_layers NUM_LAYERS]
                                              [--walk_length WALK_LENGTH]
                                              [--time_gap TIME_GAP]
                                              [--time_feat_dim TIME_FEAT_DIM]
                                              [--position_feat_dim POSITION_FEAT_DIM]
                                              [--edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}]
                                              [--time_window_mode {fixed_proportion,repeat_interval}]
                                              [--patch_size PATCH_SIZE]
                                              [--channel_embedding_dim CHANNEL_EMBEDDING_DIM]
                                              [--max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH]
                                              [--learning_rate LEARNING_RATE]
                                              [--dropout DROPOUT]
                                              [--num_epochs NUM_EPOCHS]
                                              [--optimizer {SGD,Adam,RMSprop,AdamW}]
                                              [--weight_decay WEIGHT_DECAY]
                                              [--patience PATIENCE]
                                              [--val_ratio VAL_RATIO]
                                              [--test_ratio TEST_RATIO]
                                              [--num_runs NUM_RUNS]
                                              [--test_interval_epochs TEST_INTERVAL_EPOCHS]
                                              [--negative_sample_strategy {random,historical,inductive}]
                                              [--load_best_configs]
                                              [--use_wandb USE_WANDB]
                                              [--use_ROPe]
                                              [--t1_factor_of_t2 T1_FACTOR_OF_T2]
                                              [--use_init_method] [--emb_proj]
                                              [--init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}]
                                              [--clip CLIP]

options:
  -h, --help            show this help message and exit
  --dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}
                        dataset to be used
  --batch_size BATCH_SIZE
                        batch size
  --model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}
                        name of the model, note that EdgeBank is only
                        applicable for evaluation
  --gpu GPU             number of gpu to use
  --num_neighbors NUM_NEIGHBORS
                        number of neighbors to sample for each node
  --sample_neighbor_strategy {uniform,recent,time_interval_aware}
                        how to sample historical neighbors
  --time_scaling_factor TIME_SCALING_FACTOR
                        the hyperparameter that controls the sampling
                        preference with time interval, a large
                        time_scaling_factor tends to sample more on recent
                        links, 0.0 corresponds to uniform sampling, it works
                        when sample_neighbor_strategy == time_interval_aware
  --num_walk_heads NUM_WALK_HEADS
                        number of heads used for the attention in walk encoder
  --num_heads NUM_HEADS
                        number of heads used in attention layer
  --num_layers NUM_LAYERS
                        number of model layers
  --walk_length WALK_LENGTH
                        length of each random walk
  --time_gap TIME_GAP   time gap for neighbors to compute node features
  --time_feat_dim TIME_FEAT_DIM
                        dimension of the time embedding
  --position_feat_dim POSITION_FEAT_DIM
                        dimension of the position embedding
  --edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}
                        how memory of EdgeBank works
  --time_window_mode {fixed_proportion,repeat_interval}
                        how to select the time window size for time window
                        memory
  --patch_size PATCH_SIZE
                        patch size
  --channel_embedding_dim CHANNEL_EMBEDDING_DIM
                        dimension of each channel embedding
  --max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH
                        maximal length of the input sequence of each node
  --learning_rate LEARNING_RATE
                        learning rate
  --dropout DROPOUT     dropout rate
  --num_epochs NUM_EPOCHS
                        number of epochs
  --optimizer {SGD,Adam,RMSprop,AdamW}
                        name of optimizer
  --weight_decay WEIGHT_DECAY
                        weight decay
  --patience PATIENCE   patience for early stopping
  --val_ratio VAL_RATIO
                        ratio of validation set
  --test_ratio TEST_RATIO
                        ratio of test set
  --num_runs NUM_RUNS   number of runs
  --test_interval_epochs TEST_INTERVAL_EPOCHS
                        how many epochs to perform testing once
  --negative_sample_strategy {random,historical,inductive}
                        strategy for the negative edge sampling
  --load_best_configs   whether to load the best configurations
  --use_wandb USE_WANDB
                        do you want to track this run using wandb? If arg is
                        `no`, then don`t track. Else, track using wandb and
                        name the run `run_name_dataset`
  --use_ROPe            Use ROPe embeddings for DecoLP
  --t1_factor_of_t2 T1_FACTOR_OF_T2
                        t1 factor of t2 when t2 is 0.04 of total time
  --use_init_method     Use new init method
  --emb_proj            Use embedding projection before weighted average
  --init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}
  --clip CLIP           clip val for grad of time-transformation operation
Epoch: 4, train for the 110-th batch, train loss: 0.18515576422214508:  92%|█████████▏| 109/119 [01:06<00:05,  1.78it/s]Epoch: 4, train for the 110-th batch, train loss: 0.18515576422214508:  92%|█████████▏| 110/119 [01:06<00:05,  1.77it/s]usage: Interface for the link prediction task [-h]
                                              [--dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}]
                                              [--batch_size BATCH_SIZE]
                                              [--model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}]
                                              [--gpu GPU]
                                              [--num_neighbors NUM_NEIGHBORS]
                                              [--sample_neighbor_strategy {uniform,recent,time_interval_aware}]
                                              [--time_scaling_factor TIME_SCALING_FACTOR]
                                              [--num_walk_heads NUM_WALK_HEADS]
                                              [--num_heads NUM_HEADS]
                                              [--num_layers NUM_LAYERS]
                                              [--walk_length WALK_LENGTH]
                                              [--time_gap TIME_GAP]
                                              [--time_feat_dim TIME_FEAT_DIM]
                                              [--position_feat_dim POSITION_FEAT_DIM]
                                              [--edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}]
                                              [--time_window_mode {fixed_proportion,repeat_interval}]
                                              [--patch_size PATCH_SIZE]
                                              [--channel_embedding_dim CHANNEL_EMBEDDING_DIM]
                                              [--max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH]
                                              [--learning_rate LEARNING_RATE]
                                              [--dropout DROPOUT]
                                              [--num_epochs NUM_EPOCHS]
                                              [--optimizer {SGD,Adam,RMSprop,AdamW}]
                                              [--weight_decay WEIGHT_DECAY]
                                              [--patience PATIENCE]
                                              [--val_ratio VAL_RATIO]
                                              [--test_ratio TEST_RATIO]
                                              [--num_runs NUM_RUNS]
                                              [--test_interval_epochs TEST_INTERVAL_EPOCHS]
                                              [--negative_sample_strategy {random,historical,inductive}]
                                              [--load_best_configs]
                                              [--use_wandb USE_WANDB]
                                              [--use_ROPe]
                                              [--t1_factor_of_t2 T1_FACTOR_OF_T2]
                                              [--use_init_method] [--emb_proj]
                                              [--init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}]
                                              [--clip CLIP]
Interface for the link prediction task: error: argument --dataset_name: expected one argument
True
usage: Interface for the link prediction task [-h]
                                              [--dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}]
                                              [--batch_size BATCH_SIZE]
                                              [--model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}]
                                              [--gpu GPU]
                                              [--num_neighbors NUM_NEIGHBORS]
                                              [--sample_neighbor_strategy {uniform,recent,time_interval_aware}]
                                              [--time_scaling_factor TIME_SCALING_FACTOR]
                                              [--num_walk_heads NUM_WALK_HEADS]
                                              [--num_heads NUM_HEADS]
                                              [--num_layers NUM_LAYERS]
                                              [--walk_length WALK_LENGTH]
                                              [--time_gap TIME_GAP]
                                              [--time_feat_dim TIME_FEAT_DIM]
                                              [--position_feat_dim POSITION_FEAT_DIM]
                                              [--edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}]
                                              [--time_window_mode {fixed_proportion,repeat_interval}]
                                              [--patch_size PATCH_SIZE]
                                              [--channel_embedding_dim CHANNEL_EMBEDDING_DIM]
                                              [--max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH]
                                              [--learning_rate LEARNING_RATE]
                                              [--dropout DROPOUT]
                                              [--num_epochs NUM_EPOCHS]
                                              [--optimizer {SGD,Adam,RMSprop,AdamW}]
                                              [--weight_decay WEIGHT_DECAY]
                                              [--patience PATIENCE]
                                              [--val_ratio VAL_RATIO]
                                              [--test_ratio TEST_RATIO]
                                              [--num_runs NUM_RUNS]
                                              [--test_interval_epochs TEST_INTERVAL_EPOCHS]
                                              [--negative_sample_strategy {random,historical,inductive}]
                                              [--load_best_configs]
                                              [--use_wandb USE_WANDB]
                                              [--use_ROPe]
                                              [--t1_factor_of_t2 T1_FACTOR_OF_T2]
                                              [--use_init_method] [--emb_proj]
                                              [--init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}]
                                              [--clip CLIP]

options:
  -h, --help            show this help message and exit
  --dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}
                        dataset to be used
  --batch_size BATCH_SIZE
                        batch size
  --model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}
                        name of the model, note that EdgeBank is only
                        applicable for evaluation
  --gpu GPU             number of gpu to use
  --num_neighbors NUM_NEIGHBORS
                        number of neighbors to sample for each node
  --sample_neighbor_strategy {uniform,recent,time_interval_aware}
                        how to sample historical neighbors
  --time_scaling_factor TIME_SCALING_FACTOR
                        the hyperparameter that controls the sampling
                        preference with time interval, a large
                        time_scaling_factor tends to sample more on recent
                        links, 0.0 corresponds to uniform sampling, it works
                        when sample_neighbor_strategy == time_interval_aware
  --num_walk_heads NUM_WALK_HEADS
                        number of heads used for the attention in walk encoder
  --num_heads NUM_HEADS
                        number of heads used in attention layer
  --num_layers NUM_LAYERS
                        number of model layers
  --walk_length WALK_LENGTH
                        length of each random walk
  --time_gap TIME_GAP   time gap for neighbors to compute node features
  --time_feat_dim TIME_FEAT_DIM
                        dimension of the time embedding
  --position_feat_dim POSITION_FEAT_DIM
                        dimension of the position embedding
  --edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}
                        how memory of EdgeBank works
  --time_window_mode {fixed_proportion,repeat_interval}
                        how to select the time window size for time window
                        memory
  --patch_size PATCH_SIZE
                        patch size
  --channel_embedding_dim CHANNEL_EMBEDDING_DIM
                        dimension of each channel embedding
  --max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH
                        maximal length of the input sequence of each node
  --learning_rate LEARNING_RATE
                        learning rate
  --dropout DROPOUT     dropout rate
  --num_epochs NUM_EPOCHS
                        number of epochs
  --optimizer {SGD,Adam,RMSprop,AdamW}
                        name of optimizer
  --weight_decay WEIGHT_DECAY
                        weight decay
  --patience PATIENCE   patience for early stopping
  --val_ratio VAL_RATIO
                        ratio of validation set
  --test_ratio TEST_RATIO
                        ratio of test set
  --num_runs NUM_RUNS   number of runs
  --test_interval_epochs TEST_INTERVAL_EPOCHS
                        how many epochs to perform testing once
  --negative_sample_strategy {random,historical,inductive}
                        strategy for the negative edge sampling
  --load_best_configs   whether to load the best configurations
  --use_wandb USE_WANDB
                        do you want to track this run using wandb? If arg is
                        `no`, then don`t track. Else, track using wandb and
                        name the run `run_name_dataset`
  --use_ROPe            Use ROPe embeddings for DecoLP
  --t1_factor_of_t2 T1_FACTOR_OF_T2
                        t1 factor of t2 when t2 is 0.04 of total time
  --use_init_method     Use new init method
  --emb_proj            Use embedding projection before weighted average
  --init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}
  --clip CLIP           clip val for grad of time-transformation operation
usage: Interface for the link prediction task [-h]
                                              [--dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}]
                                              [--batch_size BATCH_SIZE]
                                              [--model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}]
                                              [--gpu GPU]
                                              [--num_neighbors NUM_NEIGHBORS]
                                              [--sample_neighbor_strategy {uniform,recent,time_interval_aware}]
                                              [--time_scaling_factor TIME_SCALING_FACTOR]
                                              [--num_walk_heads NUM_WALK_HEADS]
                                              [--num_heads NUM_HEADS]
                                              [--num_layers NUM_LAYERS]
                                              [--walk_length WALK_LENGTH]
                                              [--time_gap TIME_GAP]
                                              [--time_feat_dim TIME_FEAT_DIM]
                                              [--position_feat_dim POSITION_FEAT_DIM]
                                              [--edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}]
                                              [--time_window_mode {fixed_proportion,repeat_interval}]
                                              [--patch_size PATCH_SIZE]
                                              [--channel_embedding_dim CHANNEL_EMBEDDING_DIM]
                                              [--max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH]
                                              [--learning_rate LEARNING_RATE]
                                              [--dropout DROPOUT]
                                              [--num_epochs NUM_EPOCHS]
                                              [--optimizer {SGD,Adam,RMSprop,AdamW}]
                                              [--weight_decay WEIGHT_DECAY]
                                              [--patience PATIENCE]
                                              [--val_ratio VAL_RATIO]
                                              [--test_ratio TEST_RATIO]
                                              [--num_runs NUM_RUNS]
                                              [--test_interval_epochs TEST_INTERVAL_EPOCHS]
                                              [--negative_sample_strategy {random,historical,inductive}]
                                              [--load_best_configs]
                                              [--use_wandb USE_WANDB]
                                              [--use_ROPe]
                                              [--t1_factor_of_t2 T1_FACTOR_OF_T2]
                                              [--use_init_method] [--emb_proj]
                                              [--init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}]
                                              [--clip CLIP]
Interface for the link prediction task: error: argument --dataset_name: expected one argument
True
usage: Interface for the link prediction task [-h]
                                              [--dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}]
                                              [--batch_size BATCH_SIZE]
                                              [--model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}]
                                              [--gpu GPU]
                                              [--num_neighbors NUM_NEIGHBORS]
                                              [--sample_neighbor_strategy {uniform,recent,time_interval_aware}]
                                              [--time_scaling_factor TIME_SCALING_FACTOR]
                                              [--num_walk_heads NUM_WALK_HEADS]
                                              [--num_heads NUM_HEADS]
                                              [--num_layers NUM_LAYERS]
                                              [--walk_length WALK_LENGTH]
                                              [--time_gap TIME_GAP]
                                              [--time_feat_dim TIME_FEAT_DIM]
                                              [--position_feat_dim POSITION_FEAT_DIM]
                                              [--edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}]
                                              [--time_window_mode {fixed_proportion,repeat_interval}]
                                              [--patch_size PATCH_SIZE]
                                              [--channel_embedding_dim CHANNEL_EMBEDDING_DIM]
                                              [--max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH]
                                              [--learning_rate LEARNING_RATE]
                                              [--dropout DROPOUT]
                                              [--num_epochs NUM_EPOCHS]
                                              [--optimizer {SGD,Adam,RMSprop,AdamW}]
                                              [--weight_decay WEIGHT_DECAY]
                                              [--patience PATIENCE]
                                              [--val_ratio VAL_RATIO]
                                              [--test_ratio TEST_RATIO]
                                              [--num_runs NUM_RUNS]
                                              [--test_interval_epochs TEST_INTERVAL_EPOCHS]
                                              [--negative_sample_strategy {random,historical,inductive}]
                                              [--load_best_configs]
                                              [--use_wandb USE_WANDB]
                                              [--use_ROPe]
                                              [--t1_factor_of_t2 T1_FACTOR_OF_T2]
                                              [--use_init_method] [--emb_proj]
                                              [--init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}]
                                              [--clip CLIP]

options:
  -h, --help            show this help message and exit
  --dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}
                        dataset to be used
  --batch_size BATCH_SIZE
                        batch size
  --model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}
                        name of the model, note that EdgeBank is only
                        applicable for evaluation
  --gpu GPU             number of gpu to use
  --num_neighbors NUM_NEIGHBORS
                        number of neighbors to sample for each node
  --sample_neighbor_strategy {uniform,recent,time_interval_aware}
                        how to sample historical neighbors
  --time_scaling_factor TIME_SCALING_FACTOR
                        the hyperparameter that controls the sampling
                        preference with time interval, a large
                        time_scaling_factor tends to sample more on recent
                        links, 0.0 corresponds to uniform sampling, it works
                        when sample_neighbor_strategy == time_interval_aware
  --num_walk_heads NUM_WALK_HEADS
                        number of heads used for the attention in walk encoder
  --num_heads NUM_HEADS
                        number of heads used in attention layer
  --num_layers NUM_LAYERS
                        number of model layers
  --walk_length WALK_LENGTH
                        length of each random walk
  --time_gap TIME_GAP   time gap for neighbors to compute node features
  --time_feat_dim TIME_FEAT_DIM
                        dimension of the time embedding
  --position_feat_dim POSITION_FEAT_DIM
                        dimension of the position embedding
  --edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}
                        how memory of EdgeBank works
  --time_window_mode {fixed_proportion,repeat_interval}
                        how to select the time window size for time window
                        memory
  --patch_size PATCH_SIZE
                        patch size
  --channel_embedding_dim CHANNEL_EMBEDDING_DIM
                        dimension of each channel embedding
  --max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH
                        maximal length of the input sequence of each node
  --learning_rate LEARNING_RATE
                        learning rate
  --dropout DROPOUT     dropout rate
  --num_epochs NUM_EPOCHS
                        number of epochs
  --optimizer {SGD,Adam,RMSprop,AdamW}
                        name of optimizer
  --weight_decay WEIGHT_DECAY
                        weight decay
  --patience PATIENCE   patience for early stopping
  --val_ratio VAL_RATIO
                        ratio of validation set
  --test_ratio TEST_RATIO
                        ratio of test set
  --num_runs NUM_RUNS   number of runs
  --test_interval_epochs TEST_INTERVAL_EPOCHS
                        how many epochs to perform testing once
  --negative_sample_strategy {random,historical,inductive}
                        strategy for the negative edge sampling
  --load_best_configs   whether to load the best configurations
  --use_wandb USE_WANDB
                        do you want to track this run using wandb? If arg is
                        `no`, then don`t track. Else, track using wandb and
                        name the run `run_name_dataset`
  --use_ROPe            Use ROPe embeddings for DecoLP
  --t1_factor_of_t2 T1_FACTOR_OF_T2
                        t1 factor of t2 when t2 is 0.04 of total time
  --use_init_method     Use new init method
  --emb_proj            Use embedding projection before weighted average
  --init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}
  --clip CLIP           clip val for grad of time-transformation operation
evaluate for the 21-th batch, evaluate loss: 0.7385876178741455:  50%|█████████         | 20/40 [00:05<00:06,  3.33it/s]evaluate for the 21-th batch, evaluate loss: 0.7385876178741455:  52%|█████████▍        | 21/40 [00:05<00:05,  3.41it/s]evaluate for the 21-th batch, evaluate loss: 0.7332119345664978:  80%|██████████████▍   | 20/25 [00:02<00:00,  8.41it/s]evaluate for the 21-th batch, evaluate loss: 0.7332119345664978:  84%|███████████████   | 21/25 [00:02<00:00,  8.58it/s]True
  0%|          | 0/95577 [00:00<?, ?it/s]evaluate for the 22-th batch, evaluate loss: 0.6120067834854126:  84%|███████████████   | 21/25 [00:02<00:00,  8.58it/s]evaluate for the 22-th batch, evaluate loss: 0.6120067834854126:  88%|███████████████▊  | 22/25 [00:02<00:00,  8.71it/s]  9%|▊         | 8142/95577 [00:00<00:01, 81411.54it/s]Epoch: 2, train for the 86-th batch, train loss: 0.3538834750652313:  22%|██▉          | 85/383 [00:50<02:58,  1.67it/s]Epoch: 2, train for the 86-th batch, train loss: 0.3538834750652313:  22%|██▉          | 86/383 [00:50<02:56,  1.69it/s] 17%|█▋        | 16284/95577 [00:00<00:00, 80324.67it/s]evaluate for the 23-th batch, evaluate loss: 0.6760984659194946:  88%|███████████████▊  | 22/25 [00:02<00:00,  8.71it/s]evaluate for the 23-th batch, evaluate loss: 0.6760984659194946:  92%|████████████████▌ | 23/25 [00:02<00:00,  8.80it/s]evaluate for the 22-th batch, evaluate loss: 0.7580581307411194:  52%|█████████▍        | 21/40 [00:06<00:05,  3.41it/s]evaluate for the 22-th batch, evaluate loss: 0.7580581307411194:  55%|█████████▉        | 22/40 [00:06<00:05,  3.39it/s]evaluate for the 24-th batch, evaluate loss: 0.66316819190979:  92%|██████████████████▍ | 23/25 [00:02<00:00,  8.80it/s]evaluate for the 24-th batch, evaluate loss: 0.66316819190979:  96%|███████████████████▏| 24/25 [00:02<00:00,  8.89it/s]Epoch: 4, train for the 33-th batch, train loss: 0.48039576411247253:  22%|██▋         | 32/146 [00:19<01:11,  1.60it/s]Epoch: 4, train for the 33-th batch, train loss: 0.48039576411247253:  23%|██▋         | 33/146 [00:19<01:09,  1.63it/s]evaluate for the 25-th batch, evaluate loss: 0.7118353247642517:  96%|█████████████████▎| 24/25 [00:02<00:00,  8.89it/s]evaluate for the 25-th batch, evaluate loss: 0.7118353247642517: 100%|██████████████████| 25/25 [00:02<00:00,  8.59it/s]
INFO:root:Epoch: 7, learning rate: 0.0001, train loss: 0.5624
INFO:root:train average_precision, 0.8209
INFO:root:train roc_auc, 0.7854
INFO:root:validate loss: 0.5101
INFO:root:validate average_precision, 0.8413
INFO:root:validate roc_auc, 0.8018
INFO:root:new node validate loss: 0.6894
INFO:root:new node validate first_1_average_precision, 0.5912
INFO:root:new node validate first_1_roc_auc, 0.5404
INFO:root:new node validate first_3_average_precision, 0.6740
INFO:root:new node validate first_3_roc_auc, 0.6364
INFO:root:new node validate first_10_average_precision, 0.7435
INFO:root:new node validate first_10_roc_auc, 0.7091
INFO:root:new node validate average_precision, 0.7054
INFO:root:new node validate roc_auc, 0.6543
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]Epoch: 4, train for the 111-th batch, train loss: 0.21970008313655853:  92%|█████████▏| 110/119 [01:07<00:05,  1.77it/s]Epoch: 4, train for the 111-th batch, train loss: 0.21970008313655853:  93%|█████████▎| 111/119 [01:07<00:04,  1.73it/s]evaluate for the 23-th batch, evaluate loss: 0.6690834164619446:  55%|█████████▉        | 22/40 [00:06<00:05,  3.39it/s]evaluate for the 23-th batch, evaluate loss: 0.6690834164619446:  57%|██████████▎       | 23/40 [00:06<00:04,  3.46it/s] 25%|██▌       | 24318/95577 [00:00<00:01, 37529.11it/s]Epoch: 8, train for the 1-th batch, train loss: 1.235783576965332:   0%|                        | 0/151 [00:00<?, ?it/s]Epoch: 8, train for the 1-th batch, train loss: 1.235783576965332:   1%|                | 1/151 [00:00<00:25,  5.91it/s] 31%|███       | 29727/95577 [00:00<00:01, 38338.48it/s]Epoch: 2, train for the 87-th batch, train loss: 0.4334733188152313:  22%|██▉          | 86/383 [00:50<02:56,  1.69it/s]Epoch: 2, train for the 87-th batch, train loss: 0.4334733188152313:  23%|██▉          | 87/383 [00:50<02:53,  1.71it/s]Epoch: 8, train for the 2-th batch, train loss: 1.184002161026001:   1%|                | 1/151 [00:00<00:25,  5.91it/s]Epoch: 8, train for the 2-th batch, train loss: 1.184002161026001:   1%|▏               | 2/151 [00:00<00:26,  5.73it/s] 37%|███▋      | 35278/95577 [00:00<00:01, 39520.04it/s]evaluate for the 24-th batch, evaluate loss: 0.7079583406448364:  57%|██████████▎       | 23/40 [00:06<00:04,  3.46it/s]evaluate for the 24-th batch, evaluate loss: 0.7079583406448364:  60%|██████████▊       | 24/40 [00:06<00:04,  3.42it/s] 43%|████▎     | 40720/95577 [00:00<00:01, 43073.68it/s]Epoch: 8, train for the 3-th batch, train loss: 0.5963554978370667:   1%|▏              | 2/151 [00:00<00:26,  5.73it/s]Epoch: 8, train for the 3-th batch, train loss: 0.5963554978370667:   2%|▎              | 3/151 [00:00<00:25,  5.90it/s]Epoch: 4, train for the 34-th batch, train loss: 0.47646796703338623:  23%|██▋         | 33/146 [00:19<01:09,  1.63it/s]Epoch: 4, train for the 34-th batch, train loss: 0.47646796703338623:  23%|██▊         | 34/146 [00:19<01:07,  1.66it/s] 49%|████▉     | 46848/95577 [00:01<00:01, 47317.12it/s]Epoch: 8, train for the 4-th batch, train loss: 0.4331991374492645:   2%|▎              | 3/151 [00:00<00:25,  5.90it/s]Epoch: 8, train for the 4-th batch, train loss: 0.4331991374492645:   3%|▍              | 4/151 [00:00<00:24,  5.92it/s]evaluate for the 25-th batch, evaluate loss: 0.6765657663345337:  60%|██████████▊       | 24/40 [00:07<00:04,  3.42it/s]evaluate for the 25-th batch, evaluate loss: 0.6765657663345337:  62%|███████████▎      | 25/40 [00:07<00:04,  3.45it/s]Epoch: 4, train for the 112-th batch, train loss: 0.15113294124603271:  93%|█████████▎| 111/119 [01:07<00:04,  1.73it/s]Epoch: 4, train for the 112-th batch, train loss: 0.15113294124603271:  94%|█████████▍| 112/119 [01:07<00:04,  1.70it/s] 55%|█████▍    | 52097/95577 [00:01<00:01, 41416.78it/s]Epoch: 8, train for the 5-th batch, train loss: 0.48595038056373596:   3%|▎             | 4/151 [00:00<00:24,  5.92it/s]Epoch: 8, train for the 5-th batch, train loss: 0.48595038056373596:   3%|▍             | 5/151 [00:00<00:25,  5.68it/s] 59%|█████▉    | 56850/95577 [00:01<00:00, 42904.07it/s]Epoch: 2, train for the 88-th batch, train loss: 0.3903573453426361:  23%|██▉          | 87/383 [00:51<02:53,  1.71it/s]Epoch: 2, train for the 88-th batch, train loss: 0.3903573453426361:  23%|██▉          | 88/383 [00:51<02:53,  1.70it/s] 64%|██████▍   | 61486/95577 [00:01<00:00, 42640.62it/s]evaluate for the 26-th batch, evaluate loss: 0.7031226754188538:  62%|███████████▎      | 25/40 [00:07<00:04,  3.45it/s]evaluate for the 26-th batch, evaluate loss: 0.7031226754188538:  65%|███████████▋      | 26/40 [00:07<00:04,  3.41it/s]Epoch: 8, train for the 6-th batch, train loss: 0.5165380239486694:   3%|▍              | 5/151 [00:01<00:25,  5.68it/s]Epoch: 8, train for the 6-th batch, train loss: 0.5165380239486694:   4%|▌              | 6/151 [00:01<00:26,  5.52it/s] 69%|██████▉   | 66084/95577 [00:01<00:00, 43521.81it/s]Epoch: 4, train for the 35-th batch, train loss: 0.3952648937702179:  23%|███          | 34/146 [00:20<01:07,  1.66it/s]Epoch: 4, train for the 35-th batch, train loss: 0.3952648937702179:  24%|███          | 35/146 [00:20<01:07,  1.66it/s] 74%|███████▍  | 70614/95577 [00:01<00:00, 37963.32it/s]Epoch: 8, train for the 7-th batch, train loss: 0.626430094242096:   4%|▋               | 6/151 [00:01<00:26,  5.52it/s]Epoch: 8, train for the 7-th batch, train loss: 0.626430094242096:   5%|▋               | 7/151 [00:01<00:27,  5.32it/s]evaluate for the 27-th batch, evaluate loss: 0.7169023156166077:  65%|███████████▋      | 26/40 [00:07<00:04,  3.41it/s]evaluate for the 27-th batch, evaluate loss: 0.7169023156166077:  68%|████████████▏     | 27/40 [00:07<00:03,  3.49it/s]Epoch: 4, train for the 113-th batch, train loss: 0.2036576271057129:  94%|██████████▎| 112/119 [01:08<00:04,  1.70it/s]Epoch: 4, train for the 113-th batch, train loss: 0.2036576271057129:  95%|██████████▍| 113/119 [01:08<00:03,  1.68it/s] 80%|████████  | 76618/95577 [00:01<00:00, 43534.36it/s]Epoch: 2, train for the 89-th batch, train loss: 0.4242141544818878:  23%|██▉          | 88/383 [00:51<02:53,  1.70it/s]Epoch: 2, train for the 89-th batch, train loss: 0.4242141544818878:  23%|███          | 89/383 [00:51<02:51,  1.71it/s]Epoch: 8, train for the 8-th batch, train loss: 0.7623773217201233:   5%|▋              | 7/151 [00:01<00:27,  5.32it/s]Epoch: 8, train for the 8-th batch, train loss: 0.7623773217201233:   5%|▊              | 8/151 [00:01<00:27,  5.24it/s] 85%|████████▌ | 81242/95577 [00:01<00:00, 41075.23it/s] 90%|████████▉ | 85550/95577 [00:01<00:00, 41146.23it/s]evaluate for the 28-th batch, evaluate loss: 0.7208130955696106:  68%|████████████▏     | 27/40 [00:08<00:03,  3.49it/s]evaluate for the 28-th batch, evaluate loss: 0.7208130955696106:  70%|████████████▌     | 28/40 [00:08<00:03,  3.44it/s]Epoch: 8, train for the 9-th batch, train loss: 0.6296619176864624:   5%|▊              | 8/151 [00:01<00:27,  5.24it/s]Epoch: 8, train for the 9-th batch, train loss: 0.6296619176864624:   6%|▉              | 9/151 [00:01<00:27,  5.17it/s] 94%|█████████▍| 89806/95577 [00:02<00:00, 39498.53it/s]Epoch: 4, train for the 36-th batch, train loss: 0.4374963343143463:  24%|███          | 35/146 [00:21<01:07,  1.66it/s]Epoch: 4, train for the 36-th batch, train loss: 0.4374963343143463:  25%|███▏         | 36/146 [00:21<01:06,  1.65it/s]100%|█████████▉| 95576/95577 [00:02<00:00, 42728.80it/s]
evaluate for the 29-th batch, evaluate loss: 0.7374173402786255:  70%|████████████▌     | 28/40 [00:08<00:03,  3.44it/s]evaluate for the 29-th batch, evaluate loss: 0.7374173402786255:  72%|█████████████     | 29/40 [00:08<00:03,  3.52it/s]Epoch: 8, train for the 10-th batch, train loss: 0.6072841286659241:   6%|▊             | 9/151 [00:01<00:27,  5.17it/s]Epoch: 8, train for the 10-th batch, train loss: 0.6072841286659241:   7%|▊            | 10/151 [00:01<00:27,  5.09it/s]Epoch: 4, train for the 114-th batch, train loss: 0.17182452976703644:  95%|█████████▍| 113/119 [01:08<00:03,  1.68it/s]Epoch: 4, train for the 114-th batch, train loss: 0.17182452976703644:  96%|█████████▌| 114/119 [01:08<00:02,  1.67it/s]Epoch: 2, train for the 90-th batch, train loss: 0.4280639588832855:  23%|███          | 89/383 [00:52<02:51,  1.71it/s]Epoch: 2, train for the 90-th batch, train loss: 0.4280639588832855:  23%|███          | 90/383 [00:52<02:49,  1.73it/s]  0%|          | 0/95577 [00:00<?, ?it/s]Epoch: 8, train for the 11-th batch, train loss: 0.639055073261261:   7%|▉             | 10/151 [00:02<00:27,  5.09it/s]Epoch: 8, train for the 11-th batch, train loss: 0.639055073261261:   7%|█             | 11/151 [00:02<00:27,  5.03it/s]100%|██████████| 95577/95577 [00:00<00:00, 1979747.12it/s]
evaluate for the 30-th batch, evaluate loss: 0.7308485507965088:  72%|█████████████     | 29/40 [00:08<00:03,  3.52it/s]evaluate for the 30-th batch, evaluate loss: 0.7308485507965088:  75%|█████████████▌    | 30/40 [00:08<00:02,  3.46it/s]Epoch: 8, train for the 12-th batch, train loss: 0.5292758345603943:   7%|▉            | 11/151 [00:02<00:27,  5.03it/s]Epoch: 8, train for the 12-th batch, train loss: 0.5292758345603943:   8%|█            | 12/151 [00:02<00:27,  5.00it/s]Epoch: 4, train for the 37-th batch, train loss: 0.49182870984077454:  25%|██▉         | 36/146 [00:21<01:06,  1.65it/s]Epoch: 4, train for the 37-th batch, train loss: 0.49182870984077454:  25%|███         | 37/146 [00:21<01:06,  1.64it/s]evaluate for the 31-th batch, evaluate loss: 0.6969465613365173:  75%|█████████████▌    | 30/40 [00:08<00:02,  3.46it/s]evaluate for the 31-th batch, evaluate loss: 0.6969465613365173:  78%|█████████████▉    | 31/40 [00:08<00:02,  3.54it/s]Epoch: 8, train for the 13-th batch, train loss: 0.7108158469200134:   8%|█            | 12/151 [00:02<00:27,  5.00it/s]Epoch: 8, train for the 13-th batch, train loss: 0.7108158469200134:   9%|█            | 13/151 [00:02<00:28,  4.87it/s]Epoch: 4, train for the 115-th batch, train loss: 0.18284566700458527:  96%|█████████▌| 114/119 [01:09<00:02,  1.67it/s]Epoch: 4, train for the 115-th batch, train loss: 0.18284566700458527:  97%|█████████▋| 115/119 [01:09<00:02,  1.65it/s]Epoch: 2, train for the 91-th batch, train loss: 0.3764283359050751:  23%|███          | 90/383 [00:52<02:49,  1.73it/s]Epoch: 2, train for the 91-th batch, train loss: 0.3764283359050751:  24%|███          | 91/383 [00:52<02:49,  1.73it/s]Epoch: 8, train for the 14-th batch, train loss: 0.6991133689880371:   9%|█            | 13/151 [00:02<00:28,  4.87it/s]Epoch: 8, train for the 14-th batch, train loss: 0.6991133689880371:   9%|█▏           | 14/151 [00:02<00:29,  4.63it/s]evaluate for the 32-th batch, evaluate loss: 0.730079174041748:  78%|██████████████▋    | 31/40 [00:09<00:02,  3.54it/s]evaluate for the 32-th batch, evaluate loss: 0.730079174041748:  80%|███████████████▏   | 32/40 [00:09<00:02,  3.46it/s]Epoch: 8, train for the 15-th batch, train loss: 0.49403655529022217:   9%|█           | 14/151 [00:02<00:29,  4.63it/s]Epoch: 8, train for the 15-th batch, train loss: 0.49403655529022217:  10%|█▏          | 15/151 [00:02<00:29,  4.65it/s]evaluate for the 33-th batch, evaluate loss: 0.7441952228546143:  80%|██████████████▍   | 32/40 [00:09<00:02,  3.46it/s]evaluate for the 33-th batch, evaluate loss: 0.7441952228546143:  82%|██████████████▊   | 33/40 [00:09<00:01,  3.57it/s]Epoch: 4, train for the 38-th batch, train loss: 0.4631757140159607:  25%|███▎         | 37/146 [00:22<01:06,  1.64it/s]Epoch: 4, train for the 38-th batch, train loss: 0.4631757140159607:  26%|███▍         | 38/146 [00:22<01:05,  1.64it/s]Epoch: 8, train for the 16-th batch, train loss: 0.671025812625885:  10%|█▍            | 15/151 [00:03<00:29,  4.65it/s]Epoch: 8, train for the 16-th batch, train loss: 0.671025812625885:  11%|█▍            | 16/151 [00:03<00:29,  4.65it/s]Epoch: 4, train for the 116-th batch, train loss: 0.12713854014873505:  97%|█████████▋| 115/119 [01:10<00:02,  1.65it/s]Epoch: 4, train for the 116-th batch, train loss: 0.12713854014873505:  97%|█████████▋| 116/119 [01:10<00:01,  1.65it/s]Epoch: 2, train for the 92-th batch, train loss: 0.4063294529914856:  24%|███          | 91/383 [00:53<02:49,  1.73it/s]Epoch: 2, train for the 92-th batch, train loss: 0.4063294529914856:  24%|███          | 92/383 [00:53<02:48,  1.73it/s]evaluate for the 34-th batch, evaluate loss: 0.7409170269966125:  82%|██████████████▊   | 33/40 [00:09<00:01,  3.57it/s]evaluate for the 34-th batch, evaluate loss: 0.7409170269966125:  85%|███████████████▎  | 34/40 [00:09<00:01,  3.48it/s]Epoch: 8, train for the 17-th batch, train loss: 0.6233527064323425:  11%|█▍           | 16/151 [00:03<00:29,  4.65it/s]Epoch: 8, train for the 17-th batch, train loss: 0.6233527064323425:  11%|█▍           | 17/151 [00:03<00:28,  4.71it/s]evaluate for the 35-th batch, evaluate loss: 0.7345835566520691:  85%|███████████████▎  | 34/40 [00:09<00:01,  3.48it/s]evaluate for the 35-th batch, evaluate loss: 0.7345835566520691:  88%|███████████████▊  | 35/40 [00:09<00:01,  3.61it/s]Epoch: 8, train for the 18-th batch, train loss: 0.6052563190460205:  11%|█▍           | 17/151 [00:03<00:28,  4.71it/s]Epoch: 8, train for the 18-th batch, train loss: 0.6052563190460205:  12%|█▌           | 18/151 [00:03<00:28,  4.74it/s]Epoch: 4, train for the 39-th batch, train loss: 0.4588213264942169:  26%|███▍         | 38/146 [00:23<01:05,  1.64it/s]Epoch: 4, train for the 39-th batch, train loss: 0.4588213264942169:  27%|███▍         | 39/146 [00:23<01:05,  1.64it/s]Epoch: 2, train for the 93-th batch, train loss: 0.44303810596466064:  24%|██▉         | 92/383 [00:54<02:48,  1.73it/s]Epoch: 2, train for the 93-th batch, train loss: 0.44303810596466064:  24%|██▉         | 93/383 [00:54<02:46,  1.74it/s]Epoch: 8, train for the 19-th batch, train loss: 0.5921581983566284:  12%|█▌           | 18/151 [00:03<00:28,  4.74it/s]Epoch: 8, train for the 19-th batch, train loss: 0.5921581983566284:  13%|█▋           | 19/151 [00:03<00:27,  4.75it/s]Epoch: 4, train for the 117-th batch, train loss: 0.19325515627861023:  97%|█████████▋| 116/119 [01:10<00:01,  1.65it/s]Epoch: 4, train for the 117-th batch, train loss: 0.19325515627861023:  98%|█████████▊| 117/119 [01:10<00:01,  1.64it/s]wandb: Currently logged in as: singhayush4499 (fb-graph-proj). Use `wandb login --relogin` to force relogin
evaluate for the 36-th batch, evaluate loss: 0.7638171315193176:  88%|███████████████▊  | 35/40 [00:10<00:01,  3.61it/s]evaluate for the 36-th batch, evaluate loss: 0.7638171315193176:  90%|████████████████▏ | 36/40 [00:10<00:01,  3.50it/s]Epoch: 8, train for the 20-th batch, train loss: 0.6284375190734863:  13%|█▋           | 19/151 [00:03<00:27,  4.75it/s]Epoch: 8, train for the 20-th batch, train loss: 0.6284375190734863:  13%|█▋           | 20/151 [00:03<00:27,  4.69it/s]evaluate for the 37-th batch, evaluate loss: 0.760221540927887:  90%|█████████████████  | 36/40 [00:10<00:01,  3.50it/s]evaluate for the 37-th batch, evaluate loss: 0.760221540927887:  92%|█████████████████▌ | 37/40 [00:10<00:00,  3.62it/s]Epoch: 8, train for the 21-th batch, train loss: 0.6889083981513977:  13%|█▋           | 20/151 [00:04<00:27,  4.69it/s]Epoch: 8, train for the 21-th batch, train loss: 0.6889083981513977:  14%|█▊           | 21/151 [00:04<00:28,  4.63it/s]Epoch: 4, train for the 40-th batch, train loss: 0.4840116798877716:  27%|███▍         | 39/146 [00:23<01:05,  1.64it/s]Epoch: 4, train for the 40-th batch, train loss: 0.4840116798877716:  27%|███▌         | 40/146 [00:23<01:04,  1.63it/s]Epoch: 2, train for the 94-th batch, train loss: 0.3830271065235138:  24%|███▏         | 93/383 [00:54<02:46,  1.74it/s]Epoch: 2, train for the 94-th batch, train loss: 0.3830271065235138:  25%|███▏         | 94/383 [00:54<02:45,  1.75it/s]Epoch: 4, train for the 118-th batch, train loss: 0.14116303622722626:  98%|█████████▊| 117/119 [01:11<00:01,  1.64it/s]Epoch: 4, train for the 118-th batch, train loss: 0.14116303622722626:  99%|█████████▉| 118/119 [01:11<00:00,  1.64it/s]evaluate for the 38-th batch, evaluate loss: 0.7833439707756042:  92%|████████████████▋ | 37/40 [00:10<00:00,  3.62it/s]evaluate for the 38-th batch, evaluate loss: 0.7833439707756042:  95%|█████████████████ | 38/40 [00:10<00:00,  3.52it/s]Epoch: 8, train for the 22-th batch, train loss: 0.6012153625488281:  14%|█▊           | 21/151 [00:04<00:28,  4.63it/s]Epoch: 8, train for the 22-th batch, train loss: 0.6012153625488281:  15%|█▉           | 22/151 [00:04<00:27,  4.62it/s]Epoch: 8, train for the 23-th batch, train loss: 0.40879908204078674:  15%|█▋          | 22/151 [00:04<00:27,  4.62it/s]Epoch: 8, train for the 23-th batch, train loss: 0.40879908204078674:  15%|█▊          | 23/151 [00:04<00:27,  4.67it/s]evaluate for the 39-th batch, evaluate loss: 0.7545095682144165:  95%|█████████████████ | 38/40 [00:11<00:00,  3.52it/s]evaluate for the 39-th batch, evaluate loss: 0.7545095682144165:  98%|█████████████████▌| 39/40 [00:11<00:00,  3.65it/s]Epoch: 4, train for the 119-th batch, train loss: 0.2799699306488037:  99%|██████████▉| 118/119 [01:11<00:00,  1.64it/s]Epoch: 4, train for the 119-th batch, train loss: 0.2799699306488037: 100%|███████████| 119/119 [01:11<00:00,  1.88it/s]Epoch: 4, train for the 119-th batch, train loss: 0.2799699306488037: 100%|███████████| 119/119 [01:11<00:00,  1.66it/s]
  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 4, train for the 41-th batch, train loss: 0.443173348903656:  27%|███▊          | 40/146 [00:24<01:04,  1.63it/s]Epoch: 4, train for the 41-th batch, train loss: 0.443173348903656:  28%|███▉          | 41/146 [00:24<01:04,  1.63it/s]Epoch: 8, train for the 24-th batch, train loss: 0.5256891250610352:  15%|█▉           | 23/151 [00:04<00:27,  4.67it/s]Epoch: 8, train for the 24-th batch, train loss: 0.5256891250610352:  16%|██           | 24/151 [00:04<00:27,  4.66it/s]Epoch: 2, train for the 95-th batch, train loss: 0.4354395866394043:  25%|███▏         | 94/383 [00:55<02:45,  1.75it/s]Epoch: 2, train for the 95-th batch, train loss: 0.4354395866394043:  25%|███▏         | 95/383 [00:55<02:43,  1.76it/s]evaluate for the 40-th batch, evaluate loss: 0.7404881119728088:  98%|█████████████████▌| 39/40 [00:11<00:00,  3.65it/s]evaluate for the 40-th batch, evaluate loss: 0.7404881119728088: 100%|██████████████████| 40/40 [00:11<00:00,  3.56it/s]evaluate for the 40-th batch, evaluate loss: 0.7404881119728088: 100%|██████████████████| 40/40 [00:11<00:00,  3.51it/s]
INFO:root:Epoch: 2, learning rate: 0.0001, train loss: 0.6093
INFO:root:train average_precision, 0.7429
INFO:root:train roc_auc, 0.7258
INFO:root:validate loss: 0.6101
INFO:root:validate average_precision, 0.7370
INFO:root:validate roc_auc, 0.7244
INFO:root:new node validate loss: 0.7139
INFO:root:new node validate first_1_average_precision, 0.6037
INFO:root:new node validate first_1_roc_auc, 0.5838
INFO:root:new node validate first_3_average_precision, 0.6021
INFO:root:new node validate first_3_roc_auc, 0.5937
INFO:root:new node validate first_10_average_precision, 0.6250
INFO:root:new node validate first_10_roc_auc, 0.6265
INFO:root:new node validate average_precision, 0.5968
INFO:root:new node validate roc_auc, 0.5892
INFO:root:save model ./saved_models/DyGFormer/ia-digg-reply/DyGFormer_seed0_dygformer-ia-digg-reply-old/DyGFormer_seed0_dygformer-ia-digg-reply-old.pkl
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/ayush/DyGLib/wandb/run-20240407_154926-x1mz772x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dygformer-ia-movielens-user2tags-10m-old
wandb: ⭐️ View project at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib
wandb: 🚀 View run at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib/runs/x1mz772x
INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-movielens-user2tags-10m', batch_size=200, model_name='DyGFormer', gpu=2, num_neighbors=20, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=2, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='old', use_ROPe=False, t1_factor_of_t2=1, use_init_method=False, emb_proj=False, init_weights=None, clip=1.0, device='cuda:2', seed=0, save_model_name='DyGFormer_seed0_dygformer-ia-movielens-user2tags-10m-old')
evaluate for the 1-th batch, evaluate loss: 0.1550361067056656:   0%|                            | 0/40 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.1550361067056656:   2%|▌                   | 1/40 [00:00<00:10,  3.68it/s]Epoch: 8, train for the 25-th batch, train loss: 0.4619298577308655:  16%|██           | 24/151 [00:05<00:27,  4.66it/s]Epoch: 8, train for the 25-th batch, train loss: 0.4619298577308655:  17%|██▏          | 25/151 [00:05<00:26,  4.73it/s]Epoch: 2, train for the 96-th batch, train loss: 0.3707401156425476:  25%|███▏         | 95/383 [00:55<02:43,  1.76it/s]Epoch: 2, train for the 96-th batch, train loss: 0.3707401156425476:  25%|███▎         | 96/383 [00:55<02:18,  2.07it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/237 [00:00<?, ?it/s]Epoch: 8, train for the 26-th batch, train loss: 0.6636397242546082:  17%|██▏          | 25/151 [00:05<00:26,  4.73it/s]Epoch: 8, train for the 26-th batch, train loss: 0.6636397242546082:  17%|██▏          | 26/151 [00:05<00:26,  4.66it/s]evaluate for the 2-th batch, evaluate loss: 0.1848866194486618:   2%|▌                   | 1/40 [00:00<00:10,  3.68it/s]evaluate for the 2-th batch, evaluate loss: 0.1848866194486618:   5%|█                   | 2/40 [00:00<00:10,  3.54it/s]Epoch: 4, train for the 42-th batch, train loss: 0.5067417025566101:  28%|███▋         | 41/146 [00:24<01:04,  1.63it/s]Epoch: 4, train for the 42-th batch, train loss: 0.5067417025566101:  29%|███▋         | 42/146 [00:24<01:03,  1.63it/s]Epoch: 8, train for the 27-th batch, train loss: 0.5003005862236023:  17%|██▏          | 26/151 [00:05<00:26,  4.66it/s]Epoch: 8, train for the 27-th batch, train loss: 0.5003005862236023:  18%|██▎          | 27/151 [00:05<00:26,  4.68it/s]Epoch: 3, train for the 1-th batch, train loss: 0.6475376486778259:   0%|                       | 0/237 [00:00<?, ?it/s]Epoch: 3, train for the 1-th batch, train loss: 0.6475376486778259:   0%|               | 1/237 [00:00<01:43,  2.28it/s]evaluate for the 3-th batch, evaluate loss: 0.21846206486225128:   5%|▉                  | 2/40 [00:00<00:10,  3.54it/s]evaluate for the 3-th batch, evaluate loss: 0.21846206486225128:   8%|█▍                 | 3/40 [00:00<00:10,  3.44it/s]Epoch: 8, train for the 28-th batch, train loss: 0.5601637363433838:  18%|██▎          | 27/151 [00:05<00:26,  4.68it/s]Epoch: 8, train for the 28-th batch, train loss: 0.5601637363433838:  19%|██▍          | 28/151 [00:05<00:26,  4.66it/s]Epoch: 2, train for the 97-th batch, train loss: 0.31534838676452637:  25%|███         | 96/383 [00:56<02:18,  2.07it/s]Epoch: 2, train for the 97-th batch, train loss: 0.31534838676452637:  25%|███         | 97/383 [00:56<02:27,  1.93it/s]evaluate for the 4-th batch, evaluate loss: 0.14561325311660767:   8%|█▍                 | 3/40 [00:01<00:10,  3.44it/s]evaluate for the 4-th batch, evaluate loss: 0.14561325311660767:  10%|█▉                 | 4/40 [00:01<00:10,  3.55it/s]Epoch: 8, train for the 29-th batch, train loss: 0.5313150882720947:  19%|██▍          | 28/151 [00:05<00:26,  4.66it/s]Epoch: 8, train for the 29-th batch, train loss: 0.5313150882720947:  19%|██▍          | 29/151 [00:05<00:26,  4.68it/s]Epoch: 3, train for the 2-th batch, train loss: 0.6162329912185669:   0%|               | 1/237 [00:00<01:43,  2.28it/s]Epoch: 3, train for the 2-th batch, train loss: 0.6162329912185669:   1%|▏              | 2/237 [00:00<01:46,  2.21it/s]Epoch: 4, train for the 43-th batch, train loss: 0.4673001766204834:  29%|███▋         | 42/146 [00:25<01:03,  1.63it/s]Epoch: 4, train for the 43-th batch, train loss: 0.4673001766204834:  29%|███▊         | 43/146 [00:25<01:02,  1.64it/s]Epoch: 8, train for the 30-th batch, train loss: 0.6897828578948975:  19%|██▍          | 29/151 [00:06<00:26,  4.68it/s]Epoch: 8, train for the 30-th batch, train loss: 0.6897828578948975:  20%|██▌          | 30/151 [00:06<00:26,  4.55it/s]evaluate for the 5-th batch, evaluate loss: 0.1884070783853531:  10%|██                  | 4/40 [00:01<00:10,  3.55it/s]evaluate for the 5-th batch, evaluate loss: 0.1884070783853531:  12%|██▌                 | 5/40 [00:01<00:10,  3.46it/s]Epoch: 2, train for the 98-th batch, train loss: 0.32992422580718994:  25%|███         | 97/383 [00:56<02:27,  1.93it/s]Epoch: 2, train for the 98-th batch, train loss: 0.32992422580718994:  26%|███         | 98/383 [00:56<02:30,  1.89it/s]Epoch: 8, train for the 31-th batch, train loss: 0.674152672290802:  20%|██▊           | 30/151 [00:06<00:26,  4.55it/s]Epoch: 8, train for the 31-th batch, train loss: 0.674152672290802:  21%|██▊           | 31/151 [00:06<00:26,  4.55it/s]evaluate for the 6-th batch, evaluate loss: 0.18348661065101624:  12%|██▍                | 5/40 [00:01<00:10,  3.46it/s]evaluate for the 6-th batch, evaluate loss: 0.18348661065101624:  15%|██▊                | 6/40 [00:01<00:09,  3.64it/s]Epoch: 3, train for the 3-th batch, train loss: 0.6020482778549194:   1%|▏              | 2/237 [00:01<01:46,  2.21it/s]Epoch: 3, train for the 3-th batch, train loss: 0.6020482778549194:   1%|▏              | 3/237 [00:01<01:43,  2.25it/s]Epoch: 8, train for the 32-th batch, train loss: 0.6454067230224609:  21%|██▋          | 31/151 [00:06<00:26,  4.55it/s]Epoch: 8, train for the 32-th batch, train loss: 0.6454067230224609:  21%|██▊          | 32/151 [00:06<00:26,  4.45it/s]Epoch: 4, train for the 44-th batch, train loss: 0.4294727146625519:  29%|███▊         | 43/146 [00:26<01:02,  1.64it/s]Epoch: 4, train for the 44-th batch, train loss: 0.4294727146625519:  30%|███▉         | 44/146 [00:26<01:01,  1.65it/s]evaluate for the 7-th batch, evaluate loss: 0.1129809319972992:  15%|███                 | 6/40 [00:01<00:09,  3.64it/s]evaluate for the 7-th batch, evaluate loss: 0.1129809319972992:  18%|███▌                | 7/40 [00:01<00:09,  3.51it/s]Epoch: 8, train for the 33-th batch, train loss: 0.6286042928695679:  21%|██▊          | 32/151 [00:06<00:26,  4.45it/s]Epoch: 8, train for the 33-th batch, train loss: 0.6286042928695679:  22%|██▊          | 33/151 [00:06<00:26,  4.46it/s]Epoch: 2, train for the 99-th batch, train loss: 0.35125091671943665:  26%|███         | 98/383 [00:57<02:30,  1.89it/s]Epoch: 2, train for the 99-th batch, train loss: 0.35125091671943665:  26%|███         | 99/383 [00:57<02:36,  1.82it/s]Epoch: 3, train for the 4-th batch, train loss: 0.5881263613700867:   1%|▏              | 3/237 [00:01<01:43,  2.25it/s]Epoch: 3, train for the 4-th batch, train loss: 0.5881263613700867:   2%|▎              | 4/237 [00:01<01:45,  2.21it/s]evaluate for the 8-th batch, evaluate loss: 0.14473384618759155:  18%|███▎               | 7/40 [00:02<00:09,  3.51it/s]evaluate for the 8-th batch, evaluate loss: 0.14473384618759155:  20%|███▊               | 8/40 [00:02<00:08,  3.69it/s]Epoch: 8, train for the 34-th batch, train loss: 0.5224402546882629:  22%|██▊          | 33/151 [00:07<00:26,  4.46it/s]Epoch: 8, train for the 34-th batch, train loss: 0.5224402546882629:  23%|██▉          | 34/151 [00:07<00:26,  4.47it/s]Epoch: 4, train for the 45-th batch, train loss: 0.4766145944595337:  30%|███▉         | 44/146 [00:26<01:01,  1.65it/s]Epoch: 4, train for the 45-th batch, train loss: 0.4766145944595337:  31%|████         | 45/146 [00:26<01:01,  1.65it/s]Epoch: 8, train for the 35-th batch, train loss: 0.6752382516860962:  23%|██▉          | 34/151 [00:07<00:26,  4.47it/s]Epoch: 8, train for the 35-th batch, train loss: 0.6752382516860962:  23%|███          | 35/151 [00:07<00:25,  4.49it/s]evaluate for the 9-th batch, evaluate loss: 0.1769387274980545:  20%|████                | 8/40 [00:02<00:08,  3.69it/s]evaluate for the 9-th batch, evaluate loss: 0.1769387274980545:  22%|████▌               | 9/40 [00:02<00:08,  3.50it/s]Epoch: 3, train for the 5-th batch, train loss: 0.6529501080513:   2%|▎                 | 4/237 [00:02<01:45,  2.21it/s]Epoch: 3, train for the 5-th batch, train loss: 0.6529501080513:   2%|▍                 | 5/237 [00:02<01:47,  2.16it/s]Epoch: 8, train for the 36-th batch, train loss: 0.5897781848907471:  23%|███          | 35/151 [00:07<00:25,  4.49it/s]Epoch: 8, train for the 36-th batch, train loss: 0.5897781848907471:  24%|███          | 36/151 [00:07<00:25,  4.52it/s]Epoch: 2, train for the 100-th batch, train loss: 0.3930710554122925:  26%|███         | 99/383 [00:57<02:36,  1.82it/s]Epoch: 2, train for the 100-th batch, train loss: 0.3930710554122925:  26%|██▊        | 100/383 [00:57<02:40,  1.76it/s]evaluate for the 10-th batch, evaluate loss: 0.2111877053976059:  22%|████▎              | 9/40 [00:02<00:08,  3.50it/s]evaluate for the 10-th batch, evaluate loss: 0.2111877053976059:  25%|████▌             | 10/40 [00:02<00:08,  3.56it/s]Epoch: 8, train for the 37-th batch, train loss: 0.5720292925834656:  24%|███          | 36/151 [00:07<00:25,  4.52it/s]Epoch: 8, train for the 37-th batch, train loss: 0.5720292925834656:  25%|███▏         | 37/151 [00:07<00:24,  4.58it/s]evaluate for the 11-th batch, evaluate loss: 0.18611203134059906:  25%|████▎            | 10/40 [00:03<00:08,  3.56it/s]evaluate for the 11-th batch, evaluate loss: 0.18611203134059906:  28%|████▋            | 11/40 [00:03<00:08,  3.49it/s]INFO:root:model -> Sequential(
  (0): DyGFormer(
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (memory_bank): MemoryBank(num_nodes=16530, memory_dim=172)
    (neighbor_co_occurrence_encoder): NeighborCooccurrenceEncoder(
      (neighbor_co_occurrence_encode_layer): Sequential(
        (0): Linear(in_features=1, out_features=50, bias=True)
        (1): ReLU()
        (2): Linear(in_features=50, out_features=50, bias=True)
      )
    )
    (projection_layer): ModuleDict(
      (node): Linear(in_features=172, out_features=50, bias=True)
      (edge): Linear(in_features=172, out_features=50, bias=True)
      (time): Linear(in_features=100, out_features=50, bias=True)
      (neighbor_co_occurrence): Linear(in_features=50, out_features=50, bias=True)
    )
    (transformers): ModuleList(
      (0-1): 2 x TransformerEncoder(
        (multi_head_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)
        )
        (dropout): Dropout(p=0.1, inplace=False)
        (linear_layers): ModuleList(
          (0): Linear(in_features=200, out_features=800, bias=True)
          (1): Linear(in_features=800, out_features=200, bias=True)
        )
        (norm_layers): ModuleList(
          (0-1): 2 x LayerNorm((200,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_layer): Linear(in_features=200, out_features=172, bias=True)
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: DyGFormer, #parameters: 4348140 B, 4246.23046875 KB, 4.146709442138672 MB.
The dataset has 95577 interactions, involving 16527 different nodes
The training dataset has 48189 interactions, involving 10566 different nodes
The validation dataset has 14336 interactions, involving 5209 different nodes
The test dataset has 14337 interactions, involving 5377 different nodes
The new node validation dataset has 6745 interactions, involving 3518 different nodes
The new node test dataset has 7521 interactions, involving 3885 different nodes
1652 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/241 [00:00<?, ?it/s]Epoch: 3, train for the 6-th batch, train loss: 0.5897024273872375:   2%|▎              | 5/237 [00:02<01:47,  2.16it/s]Epoch: 3, train for the 6-th batch, train loss: 0.5897024273872375:   3%|▍              | 6/237 [00:02<01:44,  2.20it/s]Epoch: 4, train for the 46-th batch, train loss: 0.47053954005241394:  31%|███▋        | 45/146 [00:27<01:01,  1.65it/s]Epoch: 4, train for the 46-th batch, train loss: 0.47053954005241394:  32%|███▊        | 46/146 [00:27<01:00,  1.65it/s]Epoch: 8, train for the 38-th batch, train loss: 0.662608802318573:  25%|███▍          | 37/151 [00:08<00:24,  4.58it/s]Epoch: 8, train for the 38-th batch, train loss: 0.662608802318573:  25%|███▌          | 38/151 [00:08<00:28,  3.90it/s]Epoch: 2, train for the 101-th batch, train loss: 0.4002765417098999:  26%|██▊        | 100/383 [00:58<02:40,  1.76it/s]Epoch: 2, train for the 101-th batch, train loss: 0.4002765417098999:  26%|██▉        | 101/383 [00:58<02:40,  1.76it/s]evaluate for the 12-th batch, evaluate loss: 0.16014327108860016:  28%|████▋            | 11/40 [00:03<00:08,  3.49it/s]evaluate for the 12-th batch, evaluate loss: 0.16014327108860016:  30%|█████            | 12/40 [00:03<00:07,  3.62it/s]Epoch: 8, train for the 39-th batch, train loss: 0.6598870754241943:  25%|███▎         | 38/151 [00:08<00:28,  3.90it/s]Epoch: 8, train for the 39-th batch, train loss: 0.6598870754241943:  26%|███▎         | 39/151 [00:08<00:27,  4.01it/s]Epoch: 3, train for the 7-th batch, train loss: 0.5466263294219971:   3%|▍              | 6/237 [00:03<01:44,  2.20it/s]Epoch: 3, train for the 7-th batch, train loss: 0.5466263294219971:   3%|▍              | 7/237 [00:03<01:45,  2.18it/s]evaluate for the 13-th batch, evaluate loss: 0.12449067831039429:  30%|█████            | 12/40 [00:03<00:07,  3.62it/s]evaluate for the 13-th batch, evaluate loss: 0.12449067831039429:  32%|█████▌           | 13/40 [00:03<00:07,  3.54it/s]Epoch: 4, train for the 47-th batch, train loss: 0.5012192726135254:  32%|████         | 46/146 [00:27<01:00,  1.65it/s]Epoch: 4, train for the 47-th batch, train loss: 0.5012192726135254:  32%|████▏        | 47/146 [00:27<00:59,  1.66it/s]Epoch: 8, train for the 40-th batch, train loss: 0.6524683833122253:  26%|███▎         | 39/151 [00:08<00:27,  4.01it/s]Epoch: 8, train for the 40-th batch, train loss: 0.6524683833122253:  26%|███▍         | 40/151 [00:08<00:27,  4.06it/s]Epoch: 2, train for the 102-th batch, train loss: 0.38908958435058594:  26%|██▋       | 101/383 [00:59<02:40,  1.76it/s]Epoch: 2, train for the 102-th batch, train loss: 0.38908958435058594:  27%|██▋       | 102/383 [00:59<02:39,  1.76it/s]evaluate for the 14-th batch, evaluate loss: 0.17769554257392883:  32%|█████▌           | 13/40 [00:03<00:07,  3.54it/s]evaluate for the 14-th batch, evaluate loss: 0.17769554257392883:  35%|█████▉           | 14/40 [00:03<00:07,  3.64it/s]Epoch: 8, train for the 41-th batch, train loss: 0.46480247378349304:  26%|███▏        | 40/151 [00:08<00:27,  4.06it/s]Epoch: 8, train for the 41-th batch, train loss: 0.46480247378349304:  27%|███▎        | 41/151 [00:08<00:26,  4.22it/s]Epoch: 3, train for the 8-th batch, train loss: 0.557995080947876:   3%|▍               | 7/237 [00:03<01:45,  2.18it/s]Epoch: 3, train for the 8-th batch, train loss: 0.557995080947876:   3%|▌               | 8/237 [00:03<01:45,  2.18it/s]evaluate for the 15-th batch, evaluate loss: 0.19553431868553162:  35%|█████▉           | 14/40 [00:04<00:07,  3.64it/s]evaluate for the 15-th batch, evaluate loss: 0.19553431868553162:  38%|██████▍          | 15/40 [00:04<00:06,  3.57it/s]Epoch: 8, train for the 42-th batch, train loss: 0.45345741510391235:  27%|███▎        | 41/151 [00:08<00:26,  4.22it/s]Epoch: 8, train for the 42-th batch, train loss: 0.45345741510391235:  28%|███▎        | 42/151 [00:08<00:25,  4.28it/s]Epoch: 4, train for the 48-th batch, train loss: 0.46728453040122986:  32%|███▊        | 47/146 [00:28<00:59,  1.66it/s]Epoch: 4, train for the 48-th batch, train loss: 0.46728453040122986:  33%|███▉        | 48/146 [00:28<00:59,  1.66it/s]evaluate for the 16-th batch, evaluate loss: 0.20815031230449677:  38%|██████▍          | 15/40 [00:04<00:06,  3.57it/s]evaluate for the 16-th batch, evaluate loss: 0.20815031230449677:  40%|██████▊          | 16/40 [00:04<00:06,  3.60it/s]Epoch: 2, train for the 103-th batch, train loss: 0.3674135208129883:  27%|██▉        | 102/383 [00:59<02:39,  1.76it/s]Epoch: 2, train for the 103-th batch, train loss: 0.3674135208129883:  27%|██▉        | 103/383 [00:59<02:42,  1.72it/s]Epoch: 8, train for the 43-th batch, train loss: 0.6395318508148193:  28%|███▌         | 42/151 [00:09<00:25,  4.28it/s]Epoch: 8, train for the 43-th batch, train loss: 0.6395318508148193:  28%|███▋         | 43/151 [00:09<00:27,  3.94it/s]Epoch: 3, train for the 9-th batch, train loss: 0.5543549060821533:   3%|▌              | 8/237 [00:04<01:45,  2.18it/s]Epoch: 3, train for the 9-th batch, train loss: 0.5543549060821533:   4%|▌              | 9/237 [00:04<01:45,  2.15it/s]Epoch: 8, train for the 44-th batch, train loss: 0.5927674174308777:  28%|███▋         | 43/151 [00:09<00:27,  3.94it/s]Epoch: 8, train for the 44-th batch, train loss: 0.5927674174308777:  29%|███▊         | 44/151 [00:09<00:26,  4.05it/s]evaluate for the 17-th batch, evaluate loss: 0.14240078628063202:  40%|██████▊          | 16/40 [00:04<00:06,  3.60it/s]evaluate for the 17-th batch, evaluate loss: 0.14240078628063202:  42%|███████▏         | 17/40 [00:04<00:06,  3.60it/s]Epoch: 4, train for the 49-th batch, train loss: 0.47104811668395996:  33%|███▉        | 48/146 [00:29<00:59,  1.66it/s]Epoch: 4, train for the 49-th batch, train loss: 0.47104811668395996:  34%|████        | 49/146 [00:29<00:58,  1.65it/s]Epoch: 8, train for the 45-th batch, train loss: 0.47701290249824524:  29%|███▍        | 44/151 [00:09<00:26,  4.05it/s]Epoch: 8, train for the 45-th batch, train loss: 0.47701290249824524:  30%|███▌        | 45/151 [00:09<00:25,  4.18it/s]Epoch: 3, train for the 10-th batch, train loss: 0.5281701683998108:   4%|▌             | 9/237 [00:04<01:45,  2.15it/s]Epoch: 3, train for the 10-th batch, train loss: 0.5281701683998108:   4%|▌            | 10/237 [00:04<01:48,  2.09it/s]evaluate for the 18-th batch, evaluate loss: 0.16079169511795044:  42%|███████▏         | 17/40 [00:05<00:06,  3.60it/s]evaluate for the 18-th batch, evaluate loss: 0.16079169511795044:  45%|███████▋         | 18/40 [00:05<00:06,  3.51it/s]Epoch: 2, train for the 104-th batch, train loss: 0.3603377342224121:  27%|██▉        | 103/383 [01:00<02:42,  1.72it/s]Epoch: 2, train for the 104-th batch, train loss: 0.3603377342224121:  27%|██▉        | 104/383 [01:00<02:45,  1.69it/s]Epoch: 8, train for the 46-th batch, train loss: 0.6158153414726257:  30%|███▊         | 45/151 [00:09<00:25,  4.18it/s]Epoch: 8, train for the 46-th batch, train loss: 0.6158153414726257:  30%|███▉         | 46/151 [00:09<00:24,  4.23it/s]evaluate for the 19-th batch, evaluate loss: 0.1757139414548874:  45%|████████          | 18/40 [00:05<00:06,  3.51it/s]evaluate for the 19-th batch, evaluate loss: 0.1757139414548874:  48%|████████▌         | 19/40 [00:05<00:05,  3.65it/s]Epoch: 3, train for the 11-th batch, train loss: 0.5637770891189575:   4%|▌            | 10/237 [00:05<01:48,  2.09it/s]Epoch: 3, train for the 11-th batch, train loss: 0.5637770891189575:   5%|▌            | 11/237 [00:05<01:45,  2.14it/s]Epoch: 4, train for the 50-th batch, train loss: 0.5308567881584167:  34%|████▎        | 49/146 [00:29<00:58,  1.65it/s]Epoch: 4, train for the 50-th batch, train loss: 0.5308567881584167:  34%|████▍        | 50/146 [00:29<00:57,  1.66it/s]Epoch: 8, train for the 47-th batch, train loss: 0.6004140377044678:  30%|███▉         | 46/151 [00:10<00:24,  4.23it/s]Epoch: 8, train for the 47-th batch, train loss: 0.6004140377044678:  31%|████         | 47/151 [00:10<00:28,  3.68it/s]evaluate for the 20-th batch, evaluate loss: 0.1627206951379776:  48%|████████▌         | 19/40 [00:05<00:05,  3.65it/s]evaluate for the 20-th batch, evaluate loss: 0.1627206951379776:  50%|█████████         | 20/40 [00:05<00:05,  3.54it/s]Epoch: 2, train for the 105-th batch, train loss: 0.31985199451446533:  27%|██▋       | 104/383 [01:00<02:45,  1.69it/s]Epoch: 2, train for the 105-th batch, train loss: 0.31985199451446533:  27%|██▋       | 105/383 [01:00<02:42,  1.72it/s]Epoch: 8, train for the 48-th batch, train loss: 0.5645322799682617:  31%|████         | 47/151 [00:10<00:28,  3.68it/s]Epoch: 8, train for the 48-th batch, train loss: 0.5645322799682617:  32%|████▏        | 48/151 [00:10<00:26,  3.89it/s]evaluate for the 21-th batch, evaluate loss: 0.10412588715553284:  50%|████████▌        | 20/40 [00:05<00:05,  3.54it/s]evaluate for the 21-th batch, evaluate loss: 0.10412588715553284:  52%|████████▉        | 21/40 [00:05<00:05,  3.71it/s]Epoch: 3, train for the 12-th batch, train loss: 0.5735805630683899:   5%|▌            | 11/237 [00:05<01:45,  2.14it/s]Epoch: 3, train for the 12-th batch, train loss: 0.5735805630683899:   5%|▋            | 12/237 [00:05<01:47,  2.09it/s]Epoch: 8, train for the 49-th batch, train loss: 0.5817932486534119:  32%|████▏        | 48/151 [00:10<00:26,  3.89it/s]Epoch: 8, train for the 49-th batch, train loss: 0.5817932486534119:  32%|████▏        | 49/151 [00:10<00:29,  3.48it/s]Epoch: 4, train for the 51-th batch, train loss: 0.5077806115150452:  34%|████▍        | 50/146 [00:30<00:57,  1.66it/s]Epoch: 4, train for the 51-th batch, train loss: 0.5077806115150452:  35%|████▌        | 51/146 [00:30<00:57,  1.65it/s]evaluate for the 22-th batch, evaluate loss: 0.15605662763118744:  52%|████████▉        | 21/40 [00:06<00:05,  3.71it/s]evaluate for the 22-th batch, evaluate loss: 0.15605662763118744:  55%|█████████▎       | 22/40 [00:06<00:05,  3.53it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6942182779312134:   0%|                       | 0/241 [00:03<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6942182779312134:   0%|               | 1/241 [00:03<12:37,  3.16s/it]Epoch: 2, train for the 106-th batch, train loss: 0.38254621624946594:  27%|██▋       | 105/383 [01:01<02:42,  1.72it/s]Epoch: 2, train for the 106-th batch, train loss: 0.38254621624946594:  28%|██▊       | 106/383 [01:01<02:42,  1.70it/s]evaluate for the 23-th batch, evaluate loss: 0.16789089143276215:  55%|█████████▎       | 22/40 [00:06<00:05,  3.53it/s]evaluate for the 23-th batch, evaluate loss: 0.16789089143276215:  57%|█████████▊       | 23/40 [00:06<00:04,  3.58it/s]Epoch: 3, train for the 13-th batch, train loss: 0.4352302551269531:   5%|▋            | 12/237 [00:06<01:47,  2.09it/s]Epoch: 3, train for the 13-th batch, train loss: 0.4352302551269531:   5%|▋            | 13/237 [00:06<01:47,  2.08it/s]Epoch: 8, train for the 50-th batch, train loss: 0.5715696811676025:  32%|████▏        | 49/151 [00:11<00:29,  3.48it/s]Epoch: 8, train for the 50-th batch, train loss: 0.5715696811676025:  33%|████▎        | 50/151 [00:11<00:32,  3.06it/s]evaluate for the 24-th batch, evaluate loss: 0.15351030230522156:  57%|█████████▊       | 23/40 [00:06<00:04,  3.58it/s]evaluate for the 24-th batch, evaluate loss: 0.15351030230522156:  60%|██████████▏      | 24/40 [00:06<00:04,  3.45it/s]Epoch: 4, train for the 52-th batch, train loss: 0.4820631444454193:  35%|████▌        | 51/146 [00:30<00:57,  1.65it/s]Epoch: 4, train for the 52-th batch, train loss: 0.4820631444454193:  36%|████▋        | 52/146 [00:30<00:56,  1.65it/s]Epoch: 2, train for the 107-th batch, train loss: 0.45230573415756226:  28%|██▊       | 106/383 [01:02<02:42,  1.70it/s]Epoch: 2, train for the 107-th batch, train loss: 0.45230573415756226:  28%|██▊       | 107/383 [01:02<02:43,  1.69it/s]Epoch: 3, train for the 14-th batch, train loss: 0.545740008354187:   5%|▊             | 13/237 [00:06<01:47,  2.08it/s]Epoch: 3, train for the 14-th batch, train loss: 0.545740008354187:   6%|▊             | 14/237 [00:06<01:50,  2.01it/s]evaluate for the 25-th batch, evaluate loss: 0.1583048701286316:  60%|██████████▊       | 24/40 [00:07<00:04,  3.45it/s]evaluate for the 25-th batch, evaluate loss: 0.1583048701286316:  62%|███████████▎      | 25/40 [00:07<00:04,  3.54it/s]Epoch: 8, train for the 51-th batch, train loss: 0.6232602596282959:  33%|████▎        | 50/151 [00:11<00:32,  3.06it/s]Epoch: 8, train for the 51-th batch, train loss: 0.6232602596282959:  34%|████▍        | 51/151 [00:11<00:37,  2.70it/s]Epoch: 1, train for the 2-th batch, train loss: 0.6797316670417786:   0%|               | 1/241 [00:04<12:37,  3.16s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6797316670417786:   1%|               | 2/241 [00:04<07:21,  1.85s/it]evaluate for the 26-th batch, evaluate loss: 0.13745327293872833:  62%|██████████▋      | 25/40 [00:07<00:04,  3.54it/s]evaluate for the 26-th batch, evaluate loss: 0.13745327293872833:  65%|███████████      | 26/40 [00:07<00:04,  3.49it/s]Epoch: 4, train for the 53-th batch, train loss: 0.43568849563598633:  36%|████▎       | 52/146 [00:31<00:56,  1.65it/s]Epoch: 4, train for the 53-th batch, train loss: 0.43568849563598633:  36%|████▎       | 53/146 [00:31<00:56,  1.65it/s]Epoch: 8, train for the 52-th batch, train loss: 0.6120750904083252:  34%|████▍        | 51/151 [00:12<00:37,  2.70it/s]Epoch: 8, train for the 52-th batch, train loss: 0.6120750904083252:  34%|████▍        | 52/151 [00:12<00:36,  2.74it/s]Epoch: 2, train for the 108-th batch, train loss: 0.3642824590206146:  28%|███        | 107/383 [01:02<02:43,  1.69it/s]Epoch: 2, train for the 108-th batch, train loss: 0.3642824590206146:  28%|███        | 108/383 [01:02<02:43,  1.68it/s]Epoch: 3, train for the 15-th batch, train loss: 0.5261895060539246:   6%|▊            | 14/237 [00:07<01:50,  2.01it/s]Epoch: 3, train for the 15-th batch, train loss: 0.5261895060539246:   6%|▊            | 15/237 [00:07<01:50,  2.00it/s]evaluate for the 27-th batch, evaluate loss: 0.15058533847332:  65%|█████████████       | 26/40 [00:07<00:04,  3.49it/s]evaluate for the 27-th batch, evaluate loss: 0.15058533847332:  68%|█████████████▌      | 27/40 [00:07<00:03,  3.61it/s]Epoch: 1, train for the 3-th batch, train loss: 0.6723028421401978:   1%|               | 2/241 [00:04<07:21,  1.85s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6723028421401978:   1%|▏              | 3/241 [00:04<05:01,  1.27s/it]evaluate for the 28-th batch, evaluate loss: 0.11839107424020767:  68%|███████████▍     | 27/40 [00:07<00:03,  3.61it/s]evaluate for the 28-th batch, evaluate loss: 0.11839107424020767:  70%|███████████▉     | 28/40 [00:07<00:03,  3.56it/s]Epoch: 8, train for the 53-th batch, train loss: 0.5523266792297363:  34%|████▍        | 52/151 [00:12<00:36,  2.74it/s]Epoch: 8, train for the 53-th batch, train loss: 0.5523266792297363:  35%|████▌        | 53/151 [00:12<00:41,  2.35it/s]Epoch: 4, train for the 54-th batch, train loss: 0.5027680993080139:  36%|████▋        | 53/146 [00:32<00:56,  1.65it/s]Epoch: 4, train for the 54-th batch, train loss: 0.5027680993080139:  37%|████▊        | 54/146 [00:32<00:55,  1.66it/s]Epoch: 3, train for the 16-th batch, train loss: 0.5813302993774414:   6%|▊            | 15/237 [00:07<01:50,  2.00it/s]Epoch: 3, train for the 16-th batch, train loss: 0.5813302993774414:   7%|▉            | 16/237 [00:07<01:54,  1.93it/s]evaluate for the 29-th batch, evaluate loss: 0.16572672128677368:  70%|███████████▉     | 28/40 [00:08<00:03,  3.56it/s]evaluate for the 29-th batch, evaluate loss: 0.16572672128677368:  72%|████████████▎    | 29/40 [00:08<00:03,  3.63it/s]Epoch: 2, train for the 109-th batch, train loss: 0.3475845158100128:  28%|███        | 108/383 [01:03<02:43,  1.68it/s]Epoch: 2, train for the 109-th batch, train loss: 0.3475845158100128:  28%|███▏       | 109/383 [01:03<02:45,  1.65it/s]evaluate for the 30-th batch, evaluate loss: 0.19056178629398346:  72%|████████████▎    | 29/40 [00:08<00:03,  3.63it/s]evaluate for the 30-th batch, evaluate loss: 0.19056178629398346:  75%|████████████▊    | 30/40 [00:08<00:02,  3.58it/s]Epoch: 1, train for the 4-th batch, train loss: 0.684744119644165:   1%|▏               | 3/241 [00:05<05:01,  1.27s/it]Epoch: 1, train for the 4-th batch, train loss: 0.684744119644165:   2%|▎               | 4/241 [00:05<04:00,  1.02s/it]Epoch: 4, train for the 55-th batch, train loss: 0.515134871006012:  37%|█████▏        | 54/146 [00:32<00:55,  1.66it/s]Epoch: 4, train for the 55-th batch, train loss: 0.515134871006012:  38%|█████▎        | 55/146 [00:32<00:54,  1.66it/s]Epoch: 8, train for the 54-th batch, train loss: 0.4904133081436157:  35%|████▌        | 53/151 [00:13<00:41,  2.35it/s]Epoch: 8, train for the 54-th batch, train loss: 0.4904133081436157:  36%|████▋        | 54/151 [00:13<00:46,  2.07it/s]Epoch: 3, train for the 17-th batch, train loss: 0.5211791396141052:   7%|▉            | 16/237 [00:08<01:54,  1.93it/s]Epoch: 3, train for the 17-th batch, train loss: 0.5211791396141052:   7%|▉            | 17/237 [00:08<01:55,  1.90it/s]evaluate for the 31-th batch, evaluate loss: 0.16692736744880676:  75%|████████████▊    | 30/40 [00:08<00:02,  3.58it/s]evaluate for the 31-th batch, evaluate loss: 0.16692736744880676:  78%|█████████████▏   | 31/40 [00:08<00:02,  3.51it/s]Epoch: 2, train for the 110-th batch, train loss: 0.40832334756851196:  28%|██▊       | 109/383 [01:03<02:45,  1.65it/s]Epoch: 2, train for the 110-th batch, train loss: 0.40832334756851196:  29%|██▊       | 110/383 [01:03<02:43,  1.66it/s]evaluate for the 32-th batch, evaluate loss: 0.14838619530200958:  78%|█████████████▏   | 31/40 [00:08<00:02,  3.51it/s]evaluate for the 32-th batch, evaluate loss: 0.14838619530200958:  80%|█████████████▌   | 32/40 [00:08<00:02,  3.56it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6694874167442322:   2%|▏              | 4/241 [00:05<04:00,  1.02s/it]Epoch: 1, train for the 5-th batch, train loss: 0.6694874167442322:   2%|▎              | 5/241 [00:05<03:21,  1.17it/s]Epoch: 3, train for the 18-th batch, train loss: 0.362934947013855:   7%|█             | 17/237 [00:08<01:55,  1.90it/s]Epoch: 3, train for the 18-th batch, train loss: 0.362934947013855:   8%|█             | 18/237 [00:08<01:54,  1.92it/s]Epoch: 8, train for the 55-th batch, train loss: 0.5822204351425171:  36%|████▋        | 54/151 [00:13<00:46,  2.07it/s]Epoch: 8, train for the 55-th batch, train loss: 0.5822204351425171:  36%|████▋        | 55/151 [00:13<00:49,  1.96it/s]Epoch: 4, train for the 56-th batch, train loss: 0.5049037933349609:  38%|████▉        | 55/146 [00:33<00:54,  1.66it/s]Epoch: 4, train for the 56-th batch, train loss: 0.5049037933349609:  38%|████▉        | 56/146 [00:33<00:54,  1.66it/s]evaluate for the 33-th batch, evaluate loss: 0.14108318090438843:  80%|█████████████▌   | 32/40 [00:09<00:02,  3.56it/s]evaluate for the 33-th batch, evaluate loss: 0.14108318090438843:  82%|██████████████   | 33/40 [00:09<00:02,  3.49it/s]Epoch: 2, train for the 111-th batch, train loss: 0.47490713000297546:  29%|██▊       | 110/383 [01:04<02:43,  1.66it/s]Epoch: 2, train for the 111-th batch, train loss: 0.47490713000297546:  29%|██▉       | 111/383 [01:04<02:41,  1.69it/s]evaluate for the 34-th batch, evaluate loss: 0.12169977277517319:  82%|██████████████   | 33/40 [00:09<00:02,  3.49it/s]evaluate for the 34-th batch, evaluate loss: 0.12169977277517319:  85%|██████████████▍  | 34/40 [00:09<00:01,  3.63it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6847092509269714:   2%|▎              | 5/241 [00:06<03:21,  1.17it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6847092509269714:   2%|▎              | 6/241 [00:06<03:02,  1.29it/s]Epoch: 3, train for the 19-th batch, train loss: 0.3830093443393707:   8%|▉            | 18/237 [00:09<01:54,  1.92it/s]Epoch: 3, train for the 19-th batch, train loss: 0.3830093443393707:   8%|█            | 19/237 [00:09<01:56,  1.87it/s]Epoch: 4, train for the 57-th batch, train loss: 0.49775832891464233:  38%|████▌       | 56/146 [00:33<00:54,  1.66it/s]Epoch: 4, train for the 57-th batch, train loss: 0.49775832891464233:  39%|████▋       | 57/146 [00:33<00:53,  1.67it/s]Epoch: 8, train for the 56-th batch, train loss: 0.5138243436813354:  36%|████▋        | 55/151 [00:14<00:49,  1.96it/s]Epoch: 8, train for the 56-th batch, train loss: 0.5138243436813354:  37%|████▊        | 56/151 [00:14<00:51,  1.83it/s]evaluate for the 35-th batch, evaluate loss: 0.1644975244998932:  85%|███████████████▎  | 34/40 [00:09<00:01,  3.63it/s]evaluate for the 35-th batch, evaluate loss: 0.1644975244998932:  88%|███████████████▊  | 35/40 [00:09<00:01,  3.52it/s]Epoch: 2, train for the 112-th batch, train loss: 0.458607017993927:  29%|███▍        | 111/383 [01:04<02:41,  1.69it/s]Epoch: 2, train for the 112-th batch, train loss: 0.458607017993927:  29%|███▌        | 112/383 [01:04<02:40,  1.69it/s]evaluate for the 36-th batch, evaluate loss: 0.16551044583320618:  88%|██████████████▉  | 35/40 [00:10<00:01,  3.52it/s]evaluate for the 36-th batch, evaluate loss: 0.16551044583320618:  90%|███████████████▎ | 36/40 [00:10<00:01,  3.69it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6129098534584045:   2%|▎              | 6/241 [00:07<03:02,  1.29it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6129098534584045:   3%|▍              | 7/241 [00:07<02:47,  1.40it/s]Epoch: 3, train for the 20-th batch, train loss: 0.6471312642097473:   8%|█            | 19/237 [00:09<01:56,  1.87it/s]Epoch: 3, train for the 20-th batch, train loss: 0.6471312642097473:   8%|█            | 20/237 [00:09<01:55,  1.87it/s]Epoch: 8, train for the 57-th batch, train loss: 0.49393245577812195:  37%|████▍       | 56/151 [00:15<00:51,  1.83it/s]Epoch: 8, train for the 57-th batch, train loss: 0.49393245577812195:  38%|████▌       | 57/151 [00:15<00:52,  1.79it/s]Epoch: 4, train for the 58-th batch, train loss: 0.5144073367118835:  39%|█████        | 57/146 [00:34<00:53,  1.67it/s]Epoch: 4, train for the 58-th batch, train loss: 0.5144073367118835:  40%|█████▏       | 58/146 [00:34<00:53,  1.66it/s]evaluate for the 37-th batch, evaluate loss: 0.20728091895580292:  90%|███████████████▎ | 36/40 [00:10<00:01,  3.69it/s]evaluate for the 37-th batch, evaluate loss: 0.20728091895580292:  92%|███████████████▋ | 37/40 [00:10<00:00,  3.52it/s]Epoch: 2, train for the 113-th batch, train loss: 0.36451467871665955:  29%|██▉       | 112/383 [01:05<02:40,  1.69it/s]Epoch: 2, train for the 113-th batch, train loss: 0.36451467871665955:  30%|██▉       | 113/383 [01:05<02:40,  1.69it/s]evaluate for the 38-th batch, evaluate loss: 0.15431737899780273:  92%|███████████████▋ | 37/40 [00:10<00:00,  3.52it/s]evaluate for the 38-th batch, evaluate loss: 0.15431737899780273:  95%|████████████████▏| 38/40 [00:10<00:00,  3.58it/s]Epoch: 3, train for the 21-th batch, train loss: 0.44387495517730713:   8%|█           | 20/237 [00:10<01:55,  1.87it/s]Epoch: 3, train for the 21-th batch, train loss: 0.44387495517730713:   9%|█           | 21/237 [00:10<01:55,  1.87it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6639301776885986:   3%|▍              | 7/241 [00:07<02:47,  1.40it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6639301776885986:   3%|▍              | 8/241 [00:07<02:36,  1.49it/s]Epoch: 8, train for the 58-th batch, train loss: 0.5564799904823303:  38%|████▉        | 57/151 [00:15<00:52,  1.79it/s]Epoch: 8, train for the 58-th batch, train loss: 0.5564799904823303:  38%|████▉        | 58/151 [00:15<00:53,  1.75it/s]Epoch: 4, train for the 59-th batch, train loss: 0.4930656850337982:  40%|█████▏       | 58/146 [00:35<00:53,  1.66it/s]Epoch: 4, train for the 59-th batch, train loss: 0.4930656850337982:  40%|█████▎       | 59/146 [00:35<00:52,  1.66it/s]evaluate for the 39-th batch, evaluate loss: 0.1661299616098404:  95%|█████████████████ | 38/40 [00:10<00:00,  3.58it/s]evaluate for the 39-th batch, evaluate loss: 0.1661299616098404:  98%|█████████████████▌| 39/40 [00:10<00:00,  3.44it/s]evaluate for the 40-th batch, evaluate loss: 0.058703504502773285:  98%|███████████████▌| 39/40 [00:11<00:00,  3.44it/s]evaluate for the 40-th batch, evaluate loss: 0.058703504502773285: 100%|████████████████| 40/40 [00:11<00:00,  3.64it/s]
  0%|                                                                                            | 0/21 [00:00<?, ?it/s]Epoch: 2, train for the 114-th batch, train loss: 0.3711737096309662:  30%|███▏       | 113/383 [01:06<02:40,  1.69it/s]Epoch: 2, train for the 114-th batch, train loss: 0.3711737096309662:  30%|███▎       | 114/383 [01:06<02:39,  1.69it/s]evaluate for the 1-th batch, evaluate loss: 0.22039006650447845:   0%|                           | 0/21 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.22039006650447845:   5%|▉                  | 1/21 [00:00<00:05,  3.78it/s]Epoch: 3, train for the 22-th batch, train loss: 0.36991751194000244:   9%|█           | 21/237 [00:10<01:55,  1.87it/s]Epoch: 3, train for the 22-th batch, train loss: 0.36991751194000244:   9%|█           | 22/237 [00:10<01:56,  1.85it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6522231698036194:   3%|▍              | 8/241 [00:08<02:36,  1.49it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6522231698036194:   4%|▌              | 9/241 [00:08<02:29,  1.55it/s]Epoch: 8, train for the 59-th batch, train loss: 0.5438446402549744:  38%|████▉        | 58/151 [00:16<00:53,  1.75it/s]Epoch: 8, train for the 59-th batch, train loss: 0.5438446402549744:  39%|█████        | 59/151 [00:16<00:52,  1.74it/s]Epoch: 4, train for the 60-th batch, train loss: 0.48508596420288086:  40%|████▊       | 59/146 [00:35<00:52,  1.66it/s]Epoch: 4, train for the 60-th batch, train loss: 0.48508596420288086:  41%|████▉       | 60/146 [00:35<00:51,  1.66it/s]evaluate for the 2-th batch, evaluate loss: 0.253074586391449:   5%|█                    | 1/21 [00:00<00:05,  3.78it/s]evaluate for the 2-th batch, evaluate loss: 0.253074586391449:  10%|██                   | 2/21 [00:00<00:05,  3.33it/s]Epoch: 2, train for the 115-th batch, train loss: 0.33413875102996826:  30%|██▉       | 114/383 [01:06<02:39,  1.69it/s]Epoch: 2, train for the 115-th batch, train loss: 0.33413875102996826:  30%|███       | 115/383 [01:06<02:35,  1.72it/s]evaluate for the 3-th batch, evaluate loss: 0.24510562419891357:  10%|█▊                 | 2/21 [00:00<00:05,  3.33it/s]evaluate for the 3-th batch, evaluate loss: 0.24510562419891357:  14%|██▋                | 3/21 [00:00<00:05,  3.50it/s]Epoch: 3, train for the 23-th batch, train loss: 0.40343162417411804:   9%|█           | 22/237 [00:11<01:56,  1.85it/s]Epoch: 3, train for the 23-th batch, train loss: 0.40343162417411804:  10%|█▏          | 23/237 [00:11<01:56,  1.84it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6710469126701355:   4%|▌             | 9/241 [00:08<02:29,  1.55it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6710469126701355:   4%|▌            | 10/241 [00:08<02:27,  1.56it/s]Epoch: 8, train for the 60-th batch, train loss: 0.49538129568099976:  39%|████▋       | 59/151 [00:16<00:52,  1.74it/s]Epoch: 8, train for the 60-th batch, train loss: 0.49538129568099976:  40%|████▊       | 60/151 [00:16<00:53,  1.71it/s]Epoch: 4, train for the 61-th batch, train loss: 0.537154495716095:  41%|█████▊        | 60/146 [00:36<00:51,  1.66it/s]Epoch: 4, train for the 61-th batch, train loss: 0.537154495716095:  42%|█████▊        | 61/146 [00:36<00:51,  1.66it/s]Epoch: 2, train for the 116-th batch, train loss: 0.4160788655281067:  30%|███▎       | 115/383 [01:07<02:35,  1.72it/s]Epoch: 2, train for the 116-th batch, train loss: 0.4160788655281067:  30%|███▎       | 116/383 [01:07<02:32,  1.75it/s]evaluate for the 4-th batch, evaluate loss: 0.20822882652282715:  14%|██▋                | 3/21 [00:01<00:05,  3.50it/s]evaluate for the 4-th batch, evaluate loss: 0.20822882652282715:  19%|███▌               | 4/21 [00:01<00:05,  3.31it/s]Epoch: 3, train for the 24-th batch, train loss: 0.5180590152740479:  10%|█▎           | 23/237 [00:12<01:56,  1.84it/s]Epoch: 3, train for the 24-th batch, train loss: 0.5180590152740479:  10%|█▎           | 24/237 [00:12<01:57,  1.81it/s]evaluate for the 5-th batch, evaluate loss: 0.22460559010505676:  19%|███▌               | 4/21 [00:01<00:05,  3.31it/s]evaluate for the 5-th batch, evaluate loss: 0.22460559010505676:  24%|████▌              | 5/21 [00:01<00:04,  3.44it/s]evaluate for the 6-th batch, evaluate loss: 0.26130902767181396:  24%|████▌              | 5/21 [00:01<00:04,  3.44it/s]evaluate for the 6-th batch, evaluate loss: 0.26130902767181396:  29%|█████▍             | 6/21 [00:01<00:03,  4.30it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6141066551208496:   4%|▌            | 10/241 [00:09<02:27,  1.56it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6141066551208496:   5%|▌            | 11/241 [00:09<02:26,  1.57it/s]evaluate for the 7-th batch, evaluate loss: 0.23585231602191925:  29%|█████▍             | 6/21 [00:01<00:03,  4.30it/s]evaluate for the 7-th batch, evaluate loss: 0.23585231602191925:  33%|██████▎            | 7/21 [00:01<00:02,  5.21it/s]Epoch: 3, train for the 25-th batch, train loss: 0.3840351700782776:  10%|█▎           | 24/237 [00:12<01:57,  1.81it/s]Epoch: 3, train for the 25-th batch, train loss: 0.3840351700782776:  11%|█▎           | 25/237 [00:12<01:37,  2.17it/s]Epoch: 8, train for the 61-th batch, train loss: 0.5338456034660339:  40%|█████▏       | 60/151 [00:17<00:53,  1.71it/s]Epoch: 8, train for the 61-th batch, train loss: 0.5338456034660339:  40%|█████▎       | 61/151 [00:17<00:53,  1.68it/s]evaluate for the 8-th batch, evaluate loss: 0.2841297388076782:  33%|██████▋             | 7/21 [00:01<00:02,  5.21it/s]evaluate for the 8-th batch, evaluate loss: 0.2841297388076782:  38%|███████▌            | 8/21 [00:01<00:02,  5.21it/s]evaluate for the 9-th batch, evaluate loss: 0.22550347447395325:  38%|███████▏           | 8/21 [00:02<00:02,  5.21it/s]evaluate for the 9-th batch, evaluate loss: 0.22550347447395325:  43%|████████▏          | 9/21 [00:02<00:02,  5.02it/s]evaluate for the 10-th batch, evaluate loss: 0.22728152573108673:  43%|███████▋          | 9/21 [00:02<00:02,  5.02it/s]evaluate for the 10-th batch, evaluate loss: 0.22728152573108673:  48%|████████         | 10/21 [00:02<00:01,  5.82it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6774566769599915:   5%|▌            | 11/241 [00:10<02:26,  1.57it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6774566769599915:   5%|▋            | 12/241 [00:10<02:23,  1.59it/s]Epoch: 4, train for the 62-th batch, train loss: 0.48458579182624817:  42%|█████       | 61/146 [00:37<00:51,  1.66it/s]Epoch: 4, train for the 62-th batch, train loss: 0.48458579182624817:  42%|█████       | 62/146 [00:37<01:02,  1.35it/s]Epoch: 2, train for the 117-th batch, train loss: 0.4126523435115814:  30%|███▎       | 116/383 [01:08<02:32,  1.75it/s]Epoch: 2, train for the 117-th batch, train loss: 0.4126523435115814:  31%|███▎       | 117/383 [01:08<03:19,  1.33it/s]Epoch: 3, train for the 26-th batch, train loss: 0.42268407344818115:  11%|█▎          | 25/237 [00:12<01:37,  2.17it/s]Epoch: 3, train for the 26-th batch, train loss: 0.42268407344818115:  11%|█▎          | 26/237 [00:12<01:51,  1.90it/s]Epoch: 8, train for the 62-th batch, train loss: 0.5828866362571716:  40%|█████▎       | 61/151 [00:18<00:53,  1.68it/s]Epoch: 8, train for the 62-th batch, train loss: 0.5828866362571716:  41%|█████▎       | 62/151 [00:18<00:53,  1.67it/s]evaluate for the 11-th batch, evaluate loss: 0.15565632283687592:  48%|████████         | 10/21 [00:02<00:01,  5.82it/s]evaluate for the 11-th batch, evaluate loss: 0.15565632283687592:  52%|████████▉        | 11/21 [00:02<00:01,  5.35it/s]evaluate for the 12-th batch, evaluate loss: 0.2754967510700226:  52%|█████████▍        | 11/21 [00:02<00:01,  5.35it/s]evaluate for the 12-th batch, evaluate loss: 0.2754967510700226:  57%|██████████▎       | 12/21 [00:02<00:01,  4.59it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6849333047866821:   5%|▋            | 12/241 [00:10<02:23,  1.59it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6849333047866821:   5%|▋            | 13/241 [00:10<02:21,  1.61it/s]Epoch: 4, train for the 63-th batch, train loss: 0.5076954960823059:  42%|█████▌       | 62/146 [00:37<01:02,  1.35it/s]Epoch: 4, train for the 63-th batch, train loss: 0.5076954960823059:  43%|█████▌       | 63/146 [00:37<00:58,  1.43it/s]Epoch: 3, train for the 27-th batch, train loss: 0.34271901845932007:  11%|█▎          | 26/237 [00:13<01:51,  1.90it/s]Epoch: 3, train for the 27-th batch, train loss: 0.34271901845932007:  11%|█▎          | 27/237 [00:13<01:54,  1.83it/s]Epoch: 8, train for the 63-th batch, train loss: 0.4940319359302521:  41%|█████▎       | 62/151 [00:18<00:53,  1.67it/s]Epoch: 8, train for the 63-th batch, train loss: 0.4940319359302521:  42%|█████▍       | 63/151 [00:18<00:52,  1.68it/s]Epoch: 2, train for the 118-th batch, train loss: 0.42172399163246155:  31%|███       | 117/383 [01:09<03:19,  1.33it/s]Epoch: 2, train for the 118-th batch, train loss: 0.42172399163246155:  31%|███       | 118/383 [01:09<03:10,  1.39it/s]evaluate for the 13-th batch, evaluate loss: 0.23406024277210236:  57%|█████████▋       | 12/21 [00:03<00:01,  4.59it/s]evaluate for the 13-th batch, evaluate loss: 0.23406024277210236:  62%|██████████▌      | 13/21 [00:03<00:01,  4.17it/s]evaluate for the 14-th batch, evaluate loss: 0.2002091407775879:  62%|███████████▏      | 13/21 [00:03<00:01,  4.17it/s]evaluate for the 14-th batch, evaluate loss: 0.2002091407775879:  67%|████████████      | 14/21 [00:03<00:01,  3.89it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6427801251411438:   5%|▋            | 13/241 [00:11<02:21,  1.61it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6427801251411438:   6%|▊            | 14/241 [00:11<02:15,  1.67it/s]Epoch: 4, train for the 64-th batch, train loss: 0.5679277181625366:  43%|█████▌       | 63/146 [00:38<00:58,  1.43it/s]Epoch: 4, train for the 64-th batch, train loss: 0.5679277181625366:  44%|█████▋       | 64/146 [00:38<00:55,  1.49it/s]Epoch: 8, train for the 64-th batch, train loss: 0.5618752837181091:  42%|█████▍       | 63/151 [00:19<00:52,  1.68it/s]Epoch: 8, train for the 64-th batch, train loss: 0.5618752837181091:  42%|█████▌       | 64/151 [00:19<00:51,  1.70it/s]Epoch: 3, train for the 28-th batch, train loss: 0.5186635255813599:  11%|█▍           | 27/237 [00:14<01:54,  1.83it/s]Epoch: 3, train for the 28-th batch, train loss: 0.5186635255813599:  12%|█▌           | 28/237 [00:14<01:56,  1.79it/s]Epoch: 2, train for the 119-th batch, train loss: 0.43725329637527466:  31%|███       | 118/383 [01:09<03:10,  1.39it/s]Epoch: 2, train for the 119-th batch, train loss: 0.43725329637527466:  31%|███       | 119/383 [01:09<02:59,  1.47it/s]evaluate for the 15-th batch, evaluate loss: 0.20050399005413055:  67%|███████████▎     | 14/21 [00:03<00:01,  3.89it/s]evaluate for the 15-th batch, evaluate loss: 0.20050399005413055:  71%|████████████▏    | 15/21 [00:03<00:01,  3.70it/s]evaluate for the 16-th batch, evaluate loss: 0.23827658593654633:  71%|████████████▏    | 15/21 [00:03<00:01,  3.70it/s]evaluate for the 16-th batch, evaluate loss: 0.23827658593654633:  76%|████████████▉    | 16/21 [00:03<00:01,  3.66it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6444258093833923:   6%|▊            | 14/241 [00:11<02:15,  1.67it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6444258093833923:   6%|▊            | 15/241 [00:11<02:14,  1.68it/s]Epoch: 4, train for the 65-th batch, train loss: 0.5418139696121216:  44%|█████▋       | 64/146 [00:39<00:55,  1.49it/s]Epoch: 4, train for the 65-th batch, train loss: 0.5418139696121216:  45%|█████▊       | 65/146 [00:39<00:52,  1.54it/s]Epoch: 8, train for the 65-th batch, train loss: 0.40246880054473877:  42%|█████       | 64/151 [00:19<00:51,  1.70it/s]Epoch: 8, train for the 65-th batch, train loss: 0.40246880054473877:  43%|█████▏      | 65/151 [00:19<00:50,  1.70it/s]Epoch: 3, train for the 29-th batch, train loss: 0.42536115646362305:  12%|█▍          | 28/237 [00:14<01:56,  1.79it/s]Epoch: 3, train for the 29-th batch, train loss: 0.42536115646362305:  12%|█▍          | 29/237 [00:14<01:59,  1.75it/s]evaluate for the 17-th batch, evaluate loss: 0.24429196119308472:  76%|████████████▉    | 16/21 [00:04<00:01,  3.66it/s]evaluate for the 17-th batch, evaluate loss: 0.24429196119308472:  81%|█████████████▊   | 17/21 [00:04<00:01,  3.53it/s]Epoch: 2, train for the 120-th batch, train loss: 0.426837295293808:  31%|███▋        | 119/383 [01:10<02:59,  1.47it/s]Epoch: 2, train for the 120-th batch, train loss: 0.426837295293808:  31%|███▊        | 120/383 [01:10<02:53,  1.52it/s]evaluate for the 18-th batch, evaluate loss: 0.21890480816364288:  81%|█████████████▊   | 17/21 [00:04<00:01,  3.53it/s]evaluate for the 18-th batch, evaluate loss: 0.21890480816364288:  86%|██████████████▌  | 18/21 [00:04<00:00,  3.59it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6400214433670044:   6%|▊            | 15/241 [00:12<02:14,  1.68it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6400214433670044:   7%|▊            | 16/241 [00:12<02:12,  1.69it/s]Epoch: 4, train for the 66-th batch, train loss: 0.5694078207015991:  45%|█████▊       | 65/146 [00:39<00:52,  1.54it/s]Epoch: 4, train for the 66-th batch, train loss: 0.5694078207015991:  45%|█████▉       | 66/146 [00:39<00:50,  1.58it/s]Epoch: 3, train for the 30-th batch, train loss: 1.2401065826416016:  12%|█▌           | 29/237 [00:15<01:59,  1.75it/s]Epoch: 3, train for the 30-th batch, train loss: 1.2401065826416016:  13%|█▋           | 30/237 [00:15<01:53,  1.83it/s]Epoch: 8, train for the 66-th batch, train loss: 0.4209565222263336:  43%|█████▌       | 65/151 [00:20<00:50,  1.70it/s]Epoch: 8, train for the 66-th batch, train loss: 0.4209565222263336:  44%|█████▋       | 66/151 [00:20<00:49,  1.71it/s]evaluate for the 19-th batch, evaluate loss: 0.2738195061683655:  86%|███████████████▍  | 18/21 [00:04<00:00,  3.59it/s]evaluate for the 19-th batch, evaluate loss: 0.2738195061683655:  90%|████████████████▎ | 19/21 [00:04<00:00,  3.48it/s]Epoch: 2, train for the 121-th batch, train loss: 0.4137530028820038:  31%|███▍       | 120/383 [01:10<02:53,  1.52it/s]Epoch: 2, train for the 121-th batch, train loss: 0.4137530028820038:  32%|███▍       | 121/383 [01:10<02:47,  1.56it/s]evaluate for the 20-th batch, evaluate loss: 0.24896903336048126:  90%|███████████████▍ | 19/21 [00:05<00:00,  3.48it/s]evaluate for the 20-th batch, evaluate loss: 0.24896903336048126:  95%|████████████████▏| 20/21 [00:05<00:00,  3.61it/s]evaluate for the 21-th batch, evaluate loss: 0.11545978486537933:  95%|████████████████▏| 20/21 [00:05<00:00,  3.61it/s]evaluate for the 21-th batch, evaluate loss: 0.11545978486537933: 100%|█████████████████| 21/21 [00:05<00:00,  4.16it/s]
INFO:root:Epoch: 4, learning rate: 0.0001, train loss: 0.2261
INFO:root:train average_precision, 0.9659
INFO:root:train roc_auc, 0.9535
INFO:root:validate loss: 0.1603
INFO:root:validate average_precision, 0.9825
INFO:root:validate roc_auc, 0.9787
INFO:root:new node validate loss: 0.2281
INFO:root:new node validate first_1_average_precision, 0.8968
INFO:root:new node validate first_1_roc_auc, 0.8995
INFO:root:new node validate first_3_average_precision, 0.9486
INFO:root:new node validate first_3_roc_auc, 0.9464
INFO:root:new node validate first_10_average_precision, 0.9666
INFO:root:new node validate first_10_roc_auc, 0.9641
INFO:root:new node validate average_precision, 0.9663
INFO:root:new node validate roc_auc, 0.9613
INFO:root:save model ./saved_models/DyGFormer/ia-reality-call/DyGFormer_seed0_dygformer-ia-reality-call-old/DyGFormer_seed0_dygformer-ia-reality-call-old.pkl
Epoch: 4, train for the 67-th batch, train loss: 0.548173189163208:  45%|██████▎       | 66/146 [00:40<00:50,  1.58it/s]Epoch: 4, train for the 67-th batch, train loss: 0.548173189163208:  46%|██████▍       | 67/146 [00:40<00:45,  1.72it/s]Epoch: 1, train for the 17-th batch, train loss: 0.5771465301513672:   7%|▊            | 16/241 [00:13<02:12,  1.69it/s]Epoch: 1, train for the 17-th batch, train loss: 0.5771465301513672:   7%|▉            | 17/241 [00:13<02:13,  1.68it/s]Epoch: 3, train for the 31-th batch, train loss: 0.8998367190361023:  13%|█▋           | 30/237 [00:15<01:53,  1.83it/s]Epoch: 3, train for the 31-th batch, train loss: 0.8998367190361023:  13%|█▋           | 31/237 [00:15<01:53,  1.81it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]Epoch: 8, train for the 67-th batch, train loss: 0.6069679856300354:  44%|█████▋       | 66/151 [00:21<00:49,  1.71it/s]Epoch: 8, train for the 67-th batch, train loss: 0.6069679856300354:  44%|█████▊       | 67/151 [00:21<00:49,  1.69it/s]Epoch: 2, train for the 122-th batch, train loss: 0.4294597804546356:  32%|███▍       | 121/383 [01:11<02:47,  1.56it/s]Epoch: 2, train for the 122-th batch, train loss: 0.4294597804546356:  32%|███▌       | 122/383 [01:11<02:42,  1.61it/s]Epoch: 4, train for the 68-th batch, train loss: 0.5088591575622559:  46%|█████▉       | 67/146 [00:40<00:45,  1.72it/s]Epoch: 4, train for the 68-th batch, train loss: 0.5088591575622559:  47%|██████       | 68/146 [00:40<00:41,  1.88it/s]Epoch: 5, train for the 1-th batch, train loss: 1.0030646324157715:   0%|                       | 0/119 [00:00<?, ?it/s]Epoch: 5, train for the 1-th batch, train loss: 1.0030646324157715:   1%|▏              | 1/119 [00:00<01:00,  1.94it/s]Epoch: 1, train for the 18-th batch, train loss: 0.61618971824646:   7%|█              | 17/241 [00:13<02:13,  1.68it/s]Epoch: 1, train for the 18-th batch, train loss: 0.61618971824646:   7%|█              | 18/241 [00:13<02:13,  1.67it/s]Epoch: 3, train for the 32-th batch, train loss: 0.8036128878593445:  13%|█▋           | 31/237 [00:16<01:53,  1.81it/s]Epoch: 3, train for the 32-th batch, train loss: 0.8036128878593445:  14%|█▊           | 32/237 [00:16<01:56,  1.76it/s]Epoch: 8, train for the 68-th batch, train loss: 0.6042457818984985:  44%|█████▊       | 67/151 [00:21<00:49,  1.69it/s]Epoch: 8, train for the 68-th batch, train loss: 0.6042457818984985:  45%|█████▊       | 68/151 [00:21<00:49,  1.68it/s]Epoch: 2, train for the 123-th batch, train loss: 0.41809672117233276:  32%|███▏      | 122/383 [01:12<02:42,  1.61it/s]Epoch: 2, train for the 123-th batch, train loss: 0.41809672117233276:  32%|███▏      | 123/383 [01:12<02:40,  1.62it/s]Epoch: 4, train for the 69-th batch, train loss: 0.5569412708282471:  47%|██████       | 68/146 [00:41<00:41,  1.88it/s]Epoch: 4, train for the 69-th batch, train loss: 0.5569412708282471:  47%|██████▏      | 69/146 [00:41<00:42,  1.83it/s]Epoch: 5, train for the 2-th batch, train loss: 0.52361661195755:   1%|▏                | 1/119 [00:01<01:00,  1.94it/s]Epoch: 5, train for the 2-th batch, train loss: 0.52361661195755:   2%|▎                | 2/119 [00:01<01:01,  1.91it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6418958306312561:   7%|▉            | 18/241 [00:14<02:13,  1.67it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6418958306312561:   8%|█            | 19/241 [00:14<02:15,  1.64it/s]Epoch: 3, train for the 33-th batch, train loss: 0.8206336498260498:  14%|█▊           | 32/237 [00:17<01:56,  1.76it/s]Epoch: 3, train for the 33-th batch, train loss: 0.8206336498260498:  14%|█▊           | 33/237 [00:17<01:57,  1.73it/s]Epoch: 8, train for the 69-th batch, train loss: 0.5658828616142273:  45%|█████▊       | 68/151 [00:22<00:49,  1.68it/s]Epoch: 8, train for the 69-th batch, train loss: 0.5658828616142273:  46%|█████▉       | 69/151 [00:22<00:49,  1.65it/s]Epoch: 2, train for the 124-th batch, train loss: 0.40567922592163086:  32%|███▏      | 123/383 [01:12<02:40,  1.62it/s]Epoch: 2, train for the 124-th batch, train loss: 0.40567922592163086:  32%|███▏      | 124/383 [01:12<02:38,  1.63it/s]Epoch: 4, train for the 70-th batch, train loss: 0.5486314296722412:  47%|██████▏      | 69/146 [00:41<00:42,  1.83it/s]Epoch: 4, train for the 70-th batch, train loss: 0.5486314296722412:  48%|██████▏      | 70/146 [00:41<00:43,  1.75it/s]Epoch: 5, train for the 3-th batch, train loss: 0.3505743741989136:   2%|▎              | 2/119 [00:01<01:01,  1.91it/s]Epoch: 5, train for the 3-th batch, train loss: 0.3505743741989136:   3%|▍              | 3/119 [00:01<01:05,  1.76it/s]Epoch: 1, train for the 20-th batch, train loss: 0.5898400545120239:   8%|█            | 19/241 [00:14<02:15,  1.64it/s]Epoch: 1, train for the 20-th batch, train loss: 0.5898400545120239:   8%|█            | 20/241 [00:14<02:13,  1.66it/s]Epoch: 3, train for the 34-th batch, train loss: 0.8594149351119995:  14%|█▊           | 33/237 [00:17<01:57,  1.73it/s]Epoch: 3, train for the 34-th batch, train loss: 0.8594149351119995:  14%|█▊           | 34/237 [00:17<01:56,  1.74it/s]Epoch: 8, train for the 70-th batch, train loss: 0.402893602848053:  46%|██████▍       | 69/151 [00:22<00:49,  1.65it/s]Epoch: 8, train for the 70-th batch, train loss: 0.402893602848053:  46%|██████▍       | 70/151 [00:22<00:48,  1.67it/s]Epoch: 2, train for the 125-th batch, train loss: 0.3996283710002899:  32%|███▌       | 124/383 [01:13<02:38,  1.63it/s]Epoch: 2, train for the 125-th batch, train loss: 0.3996283710002899:  33%|███▌       | 125/383 [01:13<02:36,  1.65it/s]Epoch: 4, train for the 71-th batch, train loss: 0.5750020742416382:  48%|██████▏      | 70/146 [00:42<00:43,  1.75it/s]Epoch: 4, train for the 71-th batch, train loss: 0.5750020742416382:  49%|██████▎      | 71/146 [00:42<00:43,  1.72it/s]Epoch: 5, train for the 4-th batch, train loss: 0.2818227708339691:   3%|▍              | 3/119 [00:02<01:05,  1.76it/s]Epoch: 5, train for the 4-th batch, train loss: 0.2818227708339691:   3%|▌              | 4/119 [00:02<01:05,  1.75it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6435418725013733:   8%|█            | 20/241 [00:15<02:13,  1.66it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6435418725013733:   9%|█▏           | 21/241 [00:15<02:10,  1.69it/s]Epoch: 3, train for the 35-th batch, train loss: 0.5278711915016174:  14%|█▊           | 34/237 [00:18<01:56,  1.74it/s]Epoch: 3, train for the 35-th batch, train loss: 0.5278711915016174:  15%|█▉           | 35/237 [00:18<01:57,  1.72it/s]Epoch: 8, train for the 71-th batch, train loss: 0.41478416323661804:  46%|█████▌      | 70/151 [00:23<00:48,  1.67it/s]Epoch: 8, train for the 71-th batch, train loss: 0.41478416323661804:  47%|█████▋      | 71/151 [00:23<00:47,  1.69it/s]Epoch: 2, train for the 126-th batch, train loss: 0.3290879726409912:  33%|███▌       | 125/383 [01:13<02:36,  1.65it/s]Epoch: 2, train for the 126-th batch, train loss: 0.3290879726409912:  33%|███▌       | 126/383 [01:13<02:34,  1.66it/s]Epoch: 4, train for the 72-th batch, train loss: 0.5574097037315369:  49%|██████▎      | 71/146 [00:43<00:43,  1.72it/s]Epoch: 4, train for the 72-th batch, train loss: 0.5574097037315369:  49%|██████▍      | 72/146 [00:43<00:43,  1.71it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6669446229934692:   9%|█▏           | 21/241 [00:15<02:10,  1.69it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6669446229934692:   9%|█▏           | 22/241 [00:15<01:59,  1.83it/s]Epoch: 5, train for the 5-th batch, train loss: 0.3446069359779358:   3%|▌              | 4/119 [00:02<01:05,  1.75it/s]Epoch: 5, train for the 5-th batch, train loss: 0.3446069359779358:   4%|▋              | 5/119 [00:02<01:05,  1.74it/s]Epoch: 3, train for the 36-th batch, train loss: 0.431927353143692:  15%|██            | 35/237 [00:18<01:57,  1.72it/s]Epoch: 3, train for the 36-th batch, train loss: 0.431927353143692:  15%|██▏           | 36/237 [00:18<01:57,  1.71it/s]Epoch: 2, train for the 127-th batch, train loss: 0.397296667098999:  33%|███▉        | 126/383 [01:14<02:34,  1.66it/s]Epoch: 2, train for the 127-th batch, train loss: 0.397296667098999:  33%|███▉        | 127/383 [01:14<02:33,  1.67it/s]Epoch: 8, train for the 72-th batch, train loss: 0.5487366914749146:  47%|██████       | 71/151 [00:24<00:47,  1.69it/s]Epoch: 8, train for the 72-th batch, train loss: 0.5487366914749146:  48%|██████▏      | 72/151 [00:24<00:49,  1.60it/s]Epoch: 4, train for the 73-th batch, train loss: 0.514191210269928:  49%|██████▉       | 72/146 [00:43<00:43,  1.71it/s]Epoch: 4, train for the 73-th batch, train loss: 0.514191210269928:  50%|███████       | 73/146 [00:43<00:42,  1.70it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6583228707313538:   9%|█▏           | 22/241 [00:16<01:59,  1.83it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6583228707313538:  10%|█▏           | 23/241 [00:16<02:01,  1.80it/s]Epoch: 5, train for the 6-th batch, train loss: 0.3145845830440521:   4%|▋              | 5/119 [00:03<01:05,  1.74it/s]Epoch: 5, train for the 6-th batch, train loss: 0.3145845830440521:   5%|▊              | 6/119 [00:03<01:05,  1.72it/s]Epoch: 3, train for the 37-th batch, train loss: 0.6632575988769531:  15%|█▉           | 36/237 [00:19<01:57,  1.71it/s]Epoch: 3, train for the 37-th batch, train loss: 0.6632575988769531:  16%|██           | 37/237 [00:19<01:57,  1.70it/s]Epoch: 2, train for the 128-th batch, train loss: 0.47127649188041687:  33%|███▎      | 127/383 [01:15<02:33,  1.67it/s]Epoch: 2, train for the 128-th batch, train loss: 0.47127649188041687:  33%|███▎      | 128/383 [01:15<02:31,  1.68it/s]Epoch: 8, train for the 73-th batch, train loss: 0.5697678327560425:  48%|██████▏      | 72/151 [00:24<00:49,  1.60it/s]Epoch: 8, train for the 73-th batch, train loss: 0.5697678327560425:  48%|██████▎      | 73/151 [00:24<00:48,  1.61it/s]Epoch: 4, train for the 74-th batch, train loss: 0.5298152565956116:  50%|██████▌      | 73/146 [00:44<00:42,  1.70it/s]Epoch: 4, train for the 74-th batch, train loss: 0.5298152565956116:  51%|██████▌      | 74/146 [00:44<00:42,  1.70it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6379090547561646:  10%|█▏           | 23/241 [00:17<02:01,  1.80it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6379090547561646:  10%|█▎           | 24/241 [00:17<02:04,  1.74it/s]Epoch: 5, train for the 7-th batch, train loss: 0.2780069410800934:   5%|▊              | 6/119 [00:04<01:05,  1.72it/s]Epoch: 5, train for the 7-th batch, train loss: 0.2780069410800934:   6%|▉              | 7/119 [00:04<01:06,  1.70it/s]Epoch: 3, train for the 38-th batch, train loss: 0.921552836894989:  16%|██▏           | 37/237 [00:19<01:57,  1.70it/s]Epoch: 3, train for the 38-th batch, train loss: 0.921552836894989:  16%|██▏           | 38/237 [00:19<01:59,  1.67it/s]Epoch: 2, train for the 129-th batch, train loss: 0.3784670829772949:  33%|███▋       | 128/383 [01:15<02:31,  1.68it/s]Epoch: 2, train for the 129-th batch, train loss: 0.3784670829772949:  34%|███▋       | 129/383 [01:15<02:32,  1.66it/s]Epoch: 4, train for the 75-th batch, train loss: 0.5571550130844116:  51%|██████▌      | 74/146 [00:44<00:42,  1.70it/s]Epoch: 4, train for the 75-th batch, train loss: 0.5571550130844116:  51%|██████▋      | 75/146 [00:44<00:38,  1.84it/s]Epoch: 8, train for the 74-th batch, train loss: 0.5050140023231506:  48%|██████▎      | 73/151 [00:25<00:48,  1.61it/s]Epoch: 8, train for the 74-th batch, train loss: 0.5050140023231506:  49%|██████▎      | 74/151 [00:25<00:47,  1.61it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6637563705444336:  10%|█▎           | 24/241 [00:17<02:04,  1.74it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6637563705444336:  10%|█▎           | 25/241 [00:17<02:07,  1.70it/s]Epoch: 5, train for the 8-th batch, train loss: 0.22410835325717926:   6%|▊             | 7/119 [00:04<01:06,  1.70it/s]Epoch: 5, train for the 8-th batch, train loss: 0.22410835325717926:   7%|▉             | 8/119 [00:04<01:05,  1.69it/s]Epoch: 3, train for the 39-th batch, train loss: 0.7246803045272827:  16%|██           | 38/237 [00:20<01:59,  1.67it/s]Epoch: 3, train for the 39-th batch, train loss: 0.7246803045272827:  16%|██▏          | 39/237 [00:20<01:58,  1.67it/s]Epoch: 4, train for the 76-th batch, train loss: 0.5274609327316284:  51%|██████▋      | 75/146 [00:45<00:38,  1.84it/s]Epoch: 4, train for the 76-th batch, train loss: 0.5274609327316284:  52%|██████▊      | 76/146 [00:45<00:37,  1.85it/s]Epoch: 2, train for the 130-th batch, train loss: 0.3799571096897125:  34%|███▋       | 129/383 [01:16<02:32,  1.66it/s]Epoch: 2, train for the 130-th batch, train loss: 0.3799571096897125:  34%|███▋       | 130/383 [01:16<02:32,  1.66it/s]Epoch: 8, train for the 75-th batch, train loss: 0.5324048399925232:  49%|██████▎      | 74/151 [00:26<00:47,  1.61it/s]Epoch: 8, train for the 75-th batch, train loss: 0.5324048399925232:  50%|██████▍      | 75/151 [00:26<00:47,  1.61it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6375066041946411:  10%|█▎           | 25/241 [00:18<02:07,  1.70it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6375066041946411:  11%|█▍           | 26/241 [00:18<02:06,  1.70it/s]Epoch: 5, train for the 9-th batch, train loss: 0.2502320408821106:   7%|█              | 8/119 [00:05<01:05,  1.69it/s]Epoch: 5, train for the 9-th batch, train loss: 0.2502320408821106:   8%|█▏             | 9/119 [00:05<01:04,  1.70it/s]Epoch: 3, train for the 40-th batch, train loss: 0.4806846082210541:  16%|██▏          | 39/237 [00:21<01:58,  1.67it/s]Epoch: 3, train for the 40-th batch, train loss: 0.4806846082210541:  17%|██▏          | 40/237 [00:21<01:57,  1.68it/s]Epoch: 4, train for the 77-th batch, train loss: 0.574112057685852:  52%|███████▎      | 76/146 [00:45<00:37,  1.85it/s]Epoch: 4, train for the 77-th batch, train loss: 0.574112057685852:  53%|███████▍      | 77/146 [00:45<00:37,  1.82it/s]Epoch: 2, train for the 131-th batch, train loss: 0.41340309381484985:  34%|███▍      | 130/383 [01:16<02:32,  1.66it/s]Epoch: 2, train for the 131-th batch, train loss: 0.41340309381484985:  34%|███▍      | 131/383 [01:16<02:30,  1.67it/s]Epoch: 8, train for the 76-th batch, train loss: 0.5034845471382141:  50%|██████▍      | 75/151 [00:26<00:47,  1.61it/s]Epoch: 8, train for the 76-th batch, train loss: 0.5034845471382141:  50%|██████▌      | 76/151 [00:26<00:45,  1.65it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6548764109611511:  11%|█▍           | 26/241 [00:18<02:06,  1.70it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6548764109611511:  11%|█▍           | 27/241 [00:18<02:06,  1.69it/s]Epoch: 5, train for the 10-th batch, train loss: 0.24734267592430115:   8%|▉            | 9/119 [00:05<01:04,  1.70it/s]Epoch: 5, train for the 10-th batch, train loss: 0.24734267592430115:   8%|█           | 10/119 [00:05<01:04,  1.69it/s]Epoch: 3, train for the 41-th batch, train loss: 0.4778595268726349:  17%|██▏          | 40/237 [00:21<01:57,  1.68it/s]Epoch: 3, train for the 41-th batch, train loss: 0.4778595268726349:  17%|██▏          | 41/237 [00:21<01:56,  1.68it/s]Epoch: 4, train for the 78-th batch, train loss: 0.5404669642448425:  53%|██████▊      | 77/146 [00:46<00:37,  1.82it/s]Epoch: 4, train for the 78-th batch, train loss: 0.5404669642448425:  53%|██████▉      | 78/146 [00:46<00:37,  1.80it/s]Epoch: 2, train for the 132-th batch, train loss: 0.43978339433670044:  34%|███▍      | 131/383 [01:17<02:30,  1.67it/s]Epoch: 2, train for the 132-th batch, train loss: 0.43978339433670044:  34%|███▍      | 132/383 [01:17<02:29,  1.68it/s]Epoch: 8, train for the 77-th batch, train loss: 0.5077367424964905:  50%|██████▌      | 76/151 [00:27<00:45,  1.65it/s]Epoch: 8, train for the 77-th batch, train loss: 0.5077367424964905:  51%|██████▋      | 77/151 [00:27<00:44,  1.65it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6195367574691772:  11%|█▍           | 27/241 [00:19<02:06,  1.69it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6195367574691772:  12%|█▌           | 28/241 [00:19<02:04,  1.71it/s]Epoch: 5, train for the 11-th batch, train loss: 0.23939283192157745:   8%|█           | 10/119 [00:06<01:04,  1.69it/s]Epoch: 5, train for the 11-th batch, train loss: 0.23939283192157745:   9%|█           | 11/119 [00:06<01:04,  1.68it/s]Epoch: 3, train for the 42-th batch, train loss: 0.5993607640266418:  17%|██▏          | 41/237 [00:22<01:56,  1.68it/s]Epoch: 3, train for the 42-th batch, train loss: 0.5993607640266418:  18%|██▎          | 42/237 [00:22<01:57,  1.65it/s]Epoch: 4, train for the 79-th batch, train loss: 0.546516478061676:  53%|███████▍      | 78/146 [00:46<00:37,  1.80it/s]Epoch: 4, train for the 79-th batch, train loss: 0.546516478061676:  54%|███████▌      | 79/146 [00:46<00:37,  1.77it/s]Epoch: 2, train for the 133-th batch, train loss: 0.4729892611503601:  34%|███▊       | 132/383 [01:18<02:29,  1.68it/s]Epoch: 2, train for the 133-th batch, train loss: 0.4729892611503601:  35%|███▊       | 133/383 [01:18<02:30,  1.66it/s]Epoch: 8, train for the 78-th batch, train loss: 0.4831249415874481:  51%|██████▋      | 77/151 [00:27<00:44,  1.65it/s]Epoch: 8, train for the 78-th batch, train loss: 0.4831249415874481:  52%|██████▋      | 78/151 [00:27<00:43,  1.67it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6396675109863281:  12%|█▌           | 28/241 [00:20<02:04,  1.71it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6396675109863281:  12%|█▌           | 29/241 [00:20<02:03,  1.71it/s]Epoch: 5, train for the 12-th batch, train loss: 0.2621914744377136:   9%|█▏           | 11/119 [00:07<01:04,  1.68it/s]Epoch: 5, train for the 12-th batch, train loss: 0.2621914744377136:  10%|█▎           | 12/119 [00:07<01:04,  1.66it/s]Epoch: 3, train for the 43-th batch, train loss: 0.6371462941169739:  18%|██▎          | 42/237 [00:22<01:57,  1.65it/s]Epoch: 3, train for the 43-th batch, train loss: 0.6371462941169739:  18%|██▎          | 43/237 [00:22<01:56,  1.67it/s]Epoch: 4, train for the 80-th batch, train loss: 0.5665002465248108:  54%|███████      | 79/146 [00:47<00:37,  1.77it/s]Epoch: 4, train for the 80-th batch, train loss: 0.5665002465248108:  55%|███████      | 80/146 [00:47<00:37,  1.74it/s]Epoch: 2, train for the 134-th batch, train loss: 0.4554908573627472:  35%|███▊       | 133/383 [01:18<02:30,  1.66it/s]Epoch: 2, train for the 134-th batch, train loss: 0.4554908573627472:  35%|███▊       | 134/383 [01:18<02:29,  1.67it/s]Epoch: 8, train for the 79-th batch, train loss: 0.5869783759117126:  52%|██████▋      | 78/151 [00:28<00:43,  1.67it/s]Epoch: 8, train for the 79-th batch, train loss: 0.5869783759117126:  52%|██████▊      | 79/151 [00:28<00:42,  1.68it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6685011386871338:  12%|█▌           | 29/241 [00:20<02:03,  1.71it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6685011386871338:  12%|█▌           | 30/241 [00:20<02:04,  1.69it/s]Epoch: 5, train for the 13-th batch, train loss: 0.2495836615562439:  10%|█▎           | 12/119 [00:07<01:04,  1.66it/s]Epoch: 5, train for the 13-th batch, train loss: 0.2495836615562439:  11%|█▍           | 13/119 [00:07<01:04,  1.64it/s]Epoch: 3, train for the 44-th batch, train loss: 0.59228515625:  18%|███▎              | 43/237 [00:23<01:56,  1.67it/s]Epoch: 3, train for the 44-th batch, train loss: 0.59228515625:  19%|███▎              | 44/237 [00:23<01:54,  1.69it/s]Epoch: 4, train for the 81-th batch, train loss: 0.529215931892395:  55%|███████▋      | 80/146 [00:48<00:37,  1.74it/s]Epoch: 4, train for the 81-th batch, train loss: 0.529215931892395:  55%|███████▊      | 81/146 [00:48<00:38,  1.70it/s]Epoch: 2, train for the 135-th batch, train loss: 0.4603201150894165:  35%|███▊       | 134/383 [01:19<02:29,  1.67it/s]Epoch: 2, train for the 135-th batch, train loss: 0.4603201150894165:  35%|███▉       | 135/383 [01:19<02:28,  1.67it/s]Epoch: 8, train for the 80-th batch, train loss: 0.5619154572486877:  52%|██████▊      | 79/151 [00:28<00:42,  1.68it/s]Epoch: 8, train for the 80-th batch, train loss: 0.5619154572486877:  53%|██████▉      | 80/151 [00:28<00:42,  1.68it/s]Epoch: 1, train for the 31-th batch, train loss: 0.5671889185905457:  12%|█▌           | 30/241 [00:21<02:04,  1.69it/s]Epoch: 1, train for the 31-th batch, train loss: 0.5671889185905457:  13%|█▋           | 31/241 [00:21<02:02,  1.71it/s]Epoch: 5, train for the 14-th batch, train loss: 0.2567880153656006:  11%|█▍           | 13/119 [00:08<01:04,  1.64it/s]Epoch: 5, train for the 14-th batch, train loss: 0.2567880153656006:  12%|█▌           | 14/119 [00:08<01:04,  1.63it/s]Epoch: 3, train for the 45-th batch, train loss: 0.6167836785316467:  19%|██▍          | 44/237 [00:24<01:54,  1.69it/s]Epoch: 3, train for the 45-th batch, train loss: 0.6167836785316467:  19%|██▍          | 45/237 [00:24<01:54,  1.67it/s]Epoch: 4, train for the 82-th batch, train loss: 0.5251210331916809:  55%|███████▏     | 81/146 [00:48<00:38,  1.70it/s]Epoch: 4, train for the 82-th batch, train loss: 0.5251210331916809:  56%|███████▎     | 82/146 [00:48<00:38,  1.67it/s]Epoch: 2, train for the 136-th batch, train loss: 0.4509866237640381:  35%|███▉       | 135/383 [01:19<02:28,  1.67it/s]Epoch: 2, train for the 136-th batch, train loss: 0.4509866237640381:  36%|███▉       | 136/383 [01:19<02:27,  1.67it/s]Epoch: 8, train for the 81-th batch, train loss: 0.5511903166770935:  53%|██████▉      | 80/151 [00:29<00:42,  1.68it/s]Epoch: 8, train for the 81-th batch, train loss: 0.5511903166770935:  54%|██████▉      | 81/151 [00:29<00:41,  1.69it/s]Epoch: 1, train for the 32-th batch, train loss: 0.5170425176620483:  13%|█▋           | 31/241 [00:21<02:02,  1.71it/s]Epoch: 1, train for the 32-th batch, train loss: 0.5170425176620483:  13%|█▋           | 32/241 [00:21<02:03,  1.69it/s]Epoch: 5, train for the 15-th batch, train loss: 0.22819511592388153:  12%|█▍          | 14/119 [00:08<01:04,  1.63it/s]Epoch: 5, train for the 15-th batch, train loss: 0.22819511592388153:  13%|█▌          | 15/119 [00:08<01:04,  1.62it/s]Epoch: 3, train for the 46-th batch, train loss: 0.5741530060768127:  19%|██▍          | 45/237 [00:24<01:54,  1.67it/s]Epoch: 3, train for the 46-th batch, train loss: 0.5741530060768127:  19%|██▌          | 46/237 [00:24<01:54,  1.67it/s]Epoch: 4, train for the 83-th batch, train loss: 0.5171907544136047:  56%|███████▎     | 82/146 [00:49<00:38,  1.67it/s]Epoch: 4, train for the 83-th batch, train loss: 0.5171907544136047:  57%|███████▍     | 83/146 [00:49<00:38,  1.65it/s]Epoch: 2, train for the 137-th batch, train loss: 0.3666664659976959:  36%|███▉       | 136/383 [01:20<02:27,  1.67it/s]Epoch: 2, train for the 137-th batch, train loss: 0.3666664659976959:  36%|███▉       | 137/383 [01:20<02:27,  1.67it/s]Epoch: 8, train for the 82-th batch, train loss: 0.5363739728927612:  54%|██████▉      | 81/151 [00:30<00:41,  1.69it/s]Epoch: 8, train for the 82-th batch, train loss: 0.5363739728927612:  54%|███████      | 82/151 [00:30<00:40,  1.69it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6389693021774292:  13%|█▋           | 32/241 [00:22<02:03,  1.69it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6389693021774292:  14%|█▊           | 33/241 [00:22<02:01,  1.71it/s]Epoch: 5, train for the 16-th batch, train loss: 0.2544862926006317:  13%|█▋           | 15/119 [00:09<01:04,  1.62it/s]Epoch: 5, train for the 16-th batch, train loss: 0.2544862926006317:  13%|█▋           | 16/119 [00:09<01:03,  1.62it/s]Epoch: 3, train for the 47-th batch, train loss: 0.589398205280304:  19%|██▋           | 46/237 [00:25<01:54,  1.67it/s]Epoch: 3, train for the 47-th batch, train loss: 0.589398205280304:  20%|██▊           | 47/237 [00:25<01:54,  1.67it/s]Epoch: 4, train for the 84-th batch, train loss: 0.48190757632255554:  57%|██████▊     | 83/146 [00:50<00:38,  1.65it/s]Epoch: 4, train for the 84-th batch, train loss: 0.48190757632255554:  58%|██████▉     | 84/146 [00:50<00:37,  1.64it/s]Epoch: 2, train for the 138-th batch, train loss: 0.33240848779678345:  36%|███▌      | 137/383 [01:21<02:27,  1.67it/s]Epoch: 2, train for the 138-th batch, train loss: 0.33240848779678345:  36%|███▌      | 138/383 [01:21<02:27,  1.67it/s]Epoch: 8, train for the 83-th batch, train loss: 0.5632304549217224:  54%|███████      | 82/151 [00:30<00:40,  1.69it/s]Epoch: 8, train for the 83-th batch, train loss: 0.5632304549217224:  55%|███████▏     | 83/151 [00:30<00:40,  1.70it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6027169823646545:  14%|█▊           | 33/241 [00:22<02:01,  1.71it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6027169823646545:  14%|█▊           | 34/241 [00:22<02:02,  1.69it/s]Epoch: 5, train for the 17-th batch, train loss: 0.22027108073234558:  13%|█▌          | 16/119 [00:10<01:03,  1.62it/s]Epoch: 5, train for the 17-th batch, train loss: 0.22027108073234558:  14%|█▋          | 17/119 [00:10<01:03,  1.62it/s]Epoch: 3, train for the 48-th batch, train loss: 0.5604234933853149:  20%|██▌          | 47/237 [00:25<01:54,  1.67it/s]Epoch: 3, train for the 48-th batch, train loss: 0.5604234933853149:  20%|██▋          | 48/237 [00:25<01:53,  1.67it/s]Epoch: 4, train for the 85-th batch, train loss: 0.5310952067375183:  58%|███████▍     | 84/146 [00:50<00:37,  1.64it/s]Epoch: 4, train for the 85-th batch, train loss: 0.5310952067375183:  58%|███████▌     | 85/146 [00:50<00:37,  1.63it/s]Epoch: 2, train for the 139-th batch, train loss: 0.41900449991226196:  36%|███▌      | 138/383 [01:21<02:27,  1.67it/s]Epoch: 2, train for the 139-th batch, train loss: 0.41900449991226196:  36%|███▋      | 139/383 [01:21<02:26,  1.67it/s]Epoch: 8, train for the 84-th batch, train loss: 0.6105543971061707:  55%|███████▏     | 83/151 [00:31<00:40,  1.70it/s]Epoch: 8, train for the 84-th batch, train loss: 0.6105543971061707:  56%|███████▏     | 84/151 [00:31<00:39,  1.70it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6364625692367554:  14%|█▊           | 34/241 [00:23<02:02,  1.69it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6364625692367554:  15%|█▉           | 35/241 [00:23<02:02,  1.68it/s]Epoch: 5, train for the 18-th batch, train loss: 0.2278205007314682:  14%|█▊           | 17/119 [00:10<01:03,  1.62it/s]Epoch: 5, train for the 18-th batch, train loss: 0.2278205007314682:  15%|█▉           | 18/119 [00:10<01:02,  1.62it/s]Epoch: 3, train for the 49-th batch, train loss: 0.6187517642974854:  20%|██▋          | 48/237 [00:26<01:53,  1.67it/s]Epoch: 3, train for the 49-th batch, train loss: 0.6187517642974854:  21%|██▋          | 49/237 [00:26<01:54,  1.65it/s]Epoch: 4, train for the 86-th batch, train loss: 0.5303764343261719:  58%|███████▌     | 85/146 [00:51<00:37,  1.63it/s]Epoch: 4, train for the 86-th batch, train loss: 0.5303764343261719:  59%|███████▋     | 86/146 [00:51<00:36,  1.63it/s]Epoch: 2, train for the 140-th batch, train loss: 0.4283185601234436:  36%|███▉       | 139/383 [01:22<02:26,  1.67it/s]Epoch: 2, train for the 140-th batch, train loss: 0.4283185601234436:  37%|████       | 140/383 [01:22<02:27,  1.65it/s]Epoch: 8, train for the 85-th batch, train loss: 0.5725804567337036:  56%|███████▏     | 84/151 [00:31<00:39,  1.70it/s]Epoch: 8, train for the 85-th batch, train loss: 0.5725804567337036:  56%|███████▎     | 85/151 [00:31<00:39,  1.68it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6318725347518921:  15%|█▉           | 35/241 [00:24<02:02,  1.68it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6318725347518921:  15%|█▉           | 36/241 [00:24<02:02,  1.67it/s]Epoch: 5, train for the 19-th batch, train loss: 0.19576673209667206:  15%|█▊          | 18/119 [00:11<01:02,  1.62it/s]Epoch: 5, train for the 19-th batch, train loss: 0.19576673209667206:  16%|█▉          | 19/119 [00:11<01:01,  1.62it/s]Epoch: 3, train for the 50-th batch, train loss: 0.5624523162841797:  21%|██▋          | 49/237 [00:27<01:54,  1.65it/s]Epoch: 3, train for the 50-th batch, train loss: 0.5624523162841797:  21%|██▋          | 50/237 [00:27<01:52,  1.67it/s]Epoch: 4, train for the 87-th batch, train loss: 0.538540244102478:  59%|████████▏     | 86/146 [00:51<00:36,  1.63it/s]Epoch: 4, train for the 87-th batch, train loss: 0.538540244102478:  60%|████████▎     | 87/146 [00:51<00:36,  1.63it/s]Epoch: 2, train for the 141-th batch, train loss: 0.48025190830230713:  37%|███▋      | 140/383 [01:22<02:27,  1.65it/s]Epoch: 2, train for the 141-th batch, train loss: 0.48025190830230713:  37%|███▋      | 141/383 [01:22<02:25,  1.66it/s]Epoch: 8, train for the 86-th batch, train loss: 0.5091323256492615:  56%|███████▎     | 85/151 [00:32<00:39,  1.68it/s]Epoch: 8, train for the 86-th batch, train loss: 0.5091323256492615:  57%|███████▍     | 86/151 [00:32<00:38,  1.67it/s]Epoch: 1, train for the 37-th batch, train loss: 0.5983443260192871:  15%|█▉           | 36/241 [00:24<02:02,  1.67it/s]Epoch: 1, train for the 37-th batch, train loss: 0.5983443260192871:  15%|█▉           | 37/241 [00:24<02:02,  1.66it/s]Epoch: 3, train for the 51-th batch, train loss: 0.580035924911499:  21%|██▉           | 50/237 [00:27<01:52,  1.67it/s]Epoch: 3, train for the 51-th batch, train loss: 0.580035924911499:  22%|███           | 51/237 [00:27<01:51,  1.67it/s]Epoch: 5, train for the 20-th batch, train loss: 0.27085569500923157:  16%|█▉          | 19/119 [00:11<01:01,  1.62it/s]Epoch: 5, train for the 20-th batch, train loss: 0.27085569500923157:  17%|██          | 20/119 [00:11<01:01,  1.62it/s]Epoch: 2, train for the 142-th batch, train loss: 0.39649614691734314:  37%|███▋      | 141/383 [01:23<02:25,  1.66it/s]Epoch: 2, train for the 142-th batch, train loss: 0.39649614691734314:  37%|███▋      | 142/383 [01:23<02:24,  1.67it/s]Epoch: 4, train for the 88-th batch, train loss: 0.564270555973053:  60%|████████▎     | 87/146 [00:52<00:36,  1.63it/s]Epoch: 4, train for the 88-th batch, train loss: 0.564270555973053:  60%|████████▍     | 88/146 [00:52<00:35,  1.63it/s]Epoch: 8, train for the 87-th batch, train loss: 0.567755401134491:  57%|███████▉      | 86/151 [00:33<00:38,  1.67it/s]Epoch: 8, train for the 87-th batch, train loss: 0.567755401134491:  58%|████████      | 87/151 [00:33<00:38,  1.66it/s]Epoch: 1, train for the 38-th batch, train loss: 0.5414361953735352:  15%|█▉           | 37/241 [00:25<02:02,  1.66it/s]Epoch: 1, train for the 38-th batch, train loss: 0.5414361953735352:  16%|██           | 38/241 [00:25<02:02,  1.66it/s]Epoch: 5, train for the 21-th batch, train loss: 0.2829212546348572:  17%|██▏          | 20/119 [00:12<01:01,  1.62it/s]Epoch: 5, train for the 21-th batch, train loss: 0.2829212546348572:  18%|██▎          | 21/119 [00:12<01:00,  1.62it/s]Epoch: 2, train for the 143-th batch, train loss: 0.42791298031806946:  37%|███▋      | 142/383 [01:23<02:24,  1.67it/s]Epoch: 2, train for the 143-th batch, train loss: 0.42791298031806946:  37%|███▋      | 143/383 [01:23<02:17,  1.74it/s]Epoch: 3, train for the 52-th batch, train loss: 0.6349206566810608:  22%|██▊          | 51/237 [00:28<01:51,  1.67it/s]Epoch: 3, train for the 52-th batch, train loss: 0.6349206566810608:  22%|██▊          | 52/237 [00:28<01:57,  1.58it/s]Epoch: 4, train for the 89-th batch, train loss: 0.51767897605896:  60%|█████████      | 88/146 [00:53<00:35,  1.63it/s]Epoch: 4, train for the 89-th batch, train loss: 0.51767897605896:  61%|█████████▏     | 89/146 [00:53<00:35,  1.63it/s]Epoch: 8, train for the 88-th batch, train loss: 0.6031416654586792:  58%|███████▍     | 87/151 [00:33<00:38,  1.66it/s]Epoch: 8, train for the 88-th batch, train loss: 0.6031416654586792:  58%|███████▌     | 88/151 [00:33<00:37,  1.67it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6685835719108582:  16%|██           | 38/241 [00:25<02:02,  1.66it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6685835719108582:  16%|██           | 39/241 [00:25<02:01,  1.67it/s]Epoch: 3, train for the 53-th batch, train loss: 0.582535982131958:  22%|███           | 52/237 [00:28<01:57,  1.58it/s]Epoch: 3, train for the 53-th batch, train loss: 0.582535982131958:  22%|███▏          | 53/237 [00:28<01:44,  1.77it/s]Epoch: 5, train for the 22-th batch, train loss: 0.2392786741256714:  18%|██▎          | 21/119 [00:13<01:00,  1.62it/s]Epoch: 5, train for the 22-th batch, train loss: 0.2392786741256714:  18%|██▍          | 22/119 [00:13<00:59,  1.62it/s]Epoch: 2, train for the 144-th batch, train loss: 0.4124304950237274:  37%|████       | 143/383 [01:24<02:17,  1.74it/s]Epoch: 2, train for the 144-th batch, train loss: 0.4124304950237274:  38%|████▏      | 144/383 [01:24<02:22,  1.67it/s]Epoch: 8, train for the 89-th batch, train loss: 0.5500932335853577:  58%|███████▌     | 88/151 [00:34<00:37,  1.67it/s]Epoch: 8, train for the 89-th batch, train loss: 0.5500932335853577:  59%|███████▋     | 89/151 [00:34<00:37,  1.67it/s]Epoch: 4, train for the 90-th batch, train loss: 0.5546790361404419:  61%|███████▉     | 89/146 [00:53<00:35,  1.63it/s]Epoch: 4, train for the 90-th batch, train loss: 0.5546790361404419:  62%|████████     | 90/146 [00:53<00:34,  1.63it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6563732624053955:  16%|██           | 39/241 [00:26<02:01,  1.67it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6563732624053955:  17%|██▏          | 40/241 [00:26<02:00,  1.67it/s]Epoch: 3, train for the 54-th batch, train loss: 0.536491334438324:  22%|███▏          | 53/237 [00:29<01:44,  1.77it/s]Epoch: 3, train for the 54-th batch, train loss: 0.536491334438324:  23%|███▏          | 54/237 [00:29<01:42,  1.78it/s]Epoch: 5, train for the 23-th batch, train loss: 0.22506242990493774:  18%|██▏         | 22/119 [00:13<00:59,  1.62it/s]Epoch: 5, train for the 23-th batch, train loss: 0.22506242990493774:  19%|██▎         | 23/119 [00:13<00:59,  1.62it/s]Epoch: 2, train for the 145-th batch, train loss: 0.4166078567504883:  38%|████▏      | 144/383 [01:25<02:22,  1.67it/s]Epoch: 2, train for the 145-th batch, train loss: 0.4166078567504883:  38%|████▏      | 145/383 [01:25<02:21,  1.69it/s]Epoch: 8, train for the 90-th batch, train loss: 0.5315594673156738:  59%|███████▋     | 89/151 [00:34<00:37,  1.67it/s]Epoch: 8, train for the 90-th batch, train loss: 0.5315594673156738:  60%|███████▋     | 90/151 [00:34<00:36,  1.66it/s]Epoch: 4, train for the 91-th batch, train loss: 0.5417643785476685:  62%|████████     | 90/146 [00:54<00:34,  1.63it/s]Epoch: 4, train for the 91-th batch, train loss: 0.5417643785476685:  62%|████████     | 91/146 [00:54<00:33,  1.63it/s]Epoch: 1, train for the 41-th batch, train loss: 0.630254328250885:  17%|██▎           | 40/241 [00:27<02:00,  1.67it/s]Epoch: 1, train for the 41-th batch, train loss: 0.630254328250885:  17%|██▍           | 41/241 [00:27<02:00,  1.66it/s]Epoch: 3, train for the 55-th batch, train loss: 0.5475706458091736:  23%|██▉          | 54/237 [00:29<01:42,  1.78it/s]Epoch: 3, train for the 55-th batch, train loss: 0.5475706458091736:  23%|███          | 55/237 [00:29<01:41,  1.79it/s]Epoch: 4, train for the 92-th batch, train loss: 0.54656583070755:  62%|█████████▎     | 91/146 [00:54<00:33,  1.63it/s]Epoch: 4, train for the 92-th batch, train loss: 0.54656583070755:  63%|█████████▍     | 92/146 [00:54<00:30,  1.76it/s]Epoch: 2, train for the 146-th batch, train loss: 0.4005821943283081:  38%|████▏      | 145/383 [01:25<02:21,  1.69it/s]Epoch: 2, train for the 146-th batch, train loss: 0.4005821943283081:  38%|████▏      | 146/383 [01:25<02:19,  1.70it/s]Epoch: 8, train for the 91-th batch, train loss: 0.4149324297904968:  60%|███████▋     | 90/151 [00:35<00:36,  1.66it/s]Epoch: 8, train for the 91-th batch, train loss: 0.4149324297904968:  60%|███████▊     | 91/151 [00:35<00:36,  1.66it/s]Epoch: 4, train for the 93-th batch, train loss: 0.5430005192756653:  63%|████████▏    | 92/146 [00:55<00:30,  1.76it/s]Epoch: 4, train for the 93-th batch, train loss: 0.5430005192756653:  64%|████████▎    | 93/146 [00:55<00:24,  2.16it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6281903982162476:  17%|██▏          | 41/241 [00:27<02:00,  1.66it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6281903982162476:  17%|██▎          | 42/241 [00:27<02:00,  1.66it/s]Epoch: 3, train for the 56-th batch, train loss: 0.6176865696907043:  23%|███          | 55/237 [00:30<01:41,  1.79it/s]Epoch: 3, train for the 56-th batch, train loss: 0.6176865696907043:  24%|███          | 56/237 [00:30<01:41,  1.78it/s]Epoch: 5, train for the 24-th batch, train loss: 0.23459938168525696:  19%|██▎         | 23/119 [00:14<00:59,  1.62it/s]Epoch: 5, train for the 24-th batch, train loss: 0.23459938168525696:  20%|██▍         | 24/119 [00:14<01:08,  1.39it/s]Epoch: 2, train for the 147-th batch, train loss: 0.3828411102294922:  38%|████▏      | 146/383 [01:26<02:19,  1.70it/s]Epoch: 2, train for the 147-th batch, train loss: 0.3828411102294922:  38%|████▏      | 147/383 [01:26<02:19,  1.69it/s]Epoch: 8, train for the 92-th batch, train loss: 0.5430038571357727:  60%|███████▊     | 91/151 [00:36<00:36,  1.66it/s]Epoch: 8, train for the 92-th batch, train loss: 0.5430038571357727:  61%|███████▉     | 92/151 [00:36<00:35,  1.65it/s]Epoch: 3, train for the 57-th batch, train loss: 0.5191430449485779:  24%|███          | 56/237 [00:31<01:41,  1.78it/s]Epoch: 3, train for the 57-th batch, train loss: 0.5191430449485779:  24%|███▏         | 57/237 [00:31<01:41,  1.78it/s]Epoch: 5, train for the 25-th batch, train loss: 0.2547505497932434:  20%|██▌          | 24/119 [00:15<01:08,  1.39it/s]Epoch: 5, train for the 25-th batch, train loss: 0.2547505497932434:  21%|██▋          | 25/119 [00:15<01:02,  1.49it/s]Epoch: 4, train for the 94-th batch, train loss: 0.5743390321731567:  64%|████████▎    | 93/146 [00:55<00:24,  2.16it/s]Epoch: 4, train for the 94-th batch, train loss: 0.5743390321731567:  64%|████████▎    | 94/146 [00:55<00:27,  1.88it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6425849199295044:  17%|██▎          | 42/241 [00:28<02:00,  1.66it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6425849199295044:  18%|██▎          | 43/241 [00:28<02:00,  1.64it/s]Epoch: 2, train for the 148-th batch, train loss: 0.442625492811203:  38%|████▌       | 147/383 [01:26<02:19,  1.69it/s]Epoch: 2, train for the 148-th batch, train loss: 0.442625492811203:  39%|████▋       | 148/383 [01:26<02:17,  1.71it/s]Epoch: 8, train for the 93-th batch, train loss: 0.5309157967567444:  61%|███████▉     | 92/151 [00:36<00:35,  1.65it/s]Epoch: 8, train for the 93-th batch, train loss: 0.5309157967567444:  62%|████████     | 93/151 [00:36<00:35,  1.64it/s]Epoch: 3, train for the 58-th batch, train loss: 0.5319445729255676:  24%|███▏         | 57/237 [00:31<01:41,  1.78it/s]Epoch: 3, train for the 58-th batch, train loss: 0.5319445729255676:  24%|███▏         | 58/237 [00:31<01:40,  1.78it/s]Epoch: 1, train for the 44-th batch, train loss: 0.6745012998580933:  18%|██▎          | 43/241 [00:29<02:00,  1.64it/s]Epoch: 1, train for the 44-th batch, train loss: 0.6745012998580933:  18%|██▎          | 44/241 [00:29<02:00,  1.64it/s]Epoch: 5, train for the 26-th batch, train loss: 0.2026456743478775:  21%|██▋          | 25/119 [00:15<01:02,  1.49it/s]Epoch: 5, train for the 26-th batch, train loss: 0.2026456743478775:  22%|██▊          | 26/119 [00:15<01:01,  1.50it/s]Epoch: 4, train for the 95-th batch, train loss: 0.527496874332428:  64%|█████████     | 94/146 [00:56<00:27,  1.88it/s]Epoch: 4, train for the 95-th batch, train loss: 0.527496874332428:  65%|█████████     | 95/146 [00:56<00:28,  1.76it/s]Epoch: 2, train for the 149-th batch, train loss: 0.4477737247943878:  39%|████▎      | 148/383 [01:27<02:17,  1.71it/s]Epoch: 2, train for the 149-th batch, train loss: 0.4477737247943878:  39%|████▎      | 149/383 [01:27<02:15,  1.73it/s]Epoch: 8, train for the 94-th batch, train loss: 0.5588404536247253:  62%|████████     | 93/151 [00:37<00:35,  1.64it/s]Epoch: 8, train for the 94-th batch, train loss: 0.5588404536247253:  62%|████████     | 94/151 [00:37<00:34,  1.64it/s]Epoch: 3, train for the 59-th batch, train loss: 0.6320998072624207:  24%|███▏         | 58/237 [00:32<01:40,  1.78it/s]Epoch: 3, train for the 59-th batch, train loss: 0.6320998072624207:  25%|███▏         | 59/237 [00:32<01:41,  1.75it/s]Epoch: 1, train for the 45-th batch, train loss: 0.5844818949699402:  18%|██▎          | 44/241 [00:29<02:00,  1.64it/s]Epoch: 1, train for the 45-th batch, train loss: 0.5844818949699402:  19%|██▍          | 45/241 [00:29<01:58,  1.66it/s]Epoch: 5, train for the 27-th batch, train loss: 0.2650851905345917:  22%|██▊          | 26/119 [00:16<01:01,  1.50it/s]Epoch: 4, train for the 96-th batch, train loss: 0.5379139184951782:  65%|████████▍    | 95/146 [00:57<00:28,  1.76it/s]Epoch: 5, train for the 27-th batch, train loss: 0.2650851905345917:  23%|██▉          | 27/119 [00:16<01:01,  1.50it/s]Epoch: 4, train for the 96-th batch, train loss: 0.5379139184951782:  66%|████████▌    | 96/146 [00:57<00:29,  1.68it/s]Epoch: 2, train for the 150-th batch, train loss: 0.5147615671157837:  39%|████▎      | 149/383 [01:28<02:15,  1.73it/s]Epoch: 2, train for the 150-th batch, train loss: 0.5147615671157837:  39%|████▎      | 150/383 [01:28<02:15,  1.72it/s]Epoch: 8, train for the 95-th batch, train loss: 0.5260549187660217:  62%|████████     | 94/151 [00:37<00:34,  1.64it/s]Epoch: 8, train for the 95-th batch, train loss: 0.5260549187660217:  63%|████████▏    | 95/151 [00:37<00:33,  1.66it/s]Epoch: 3, train for the 60-th batch, train loss: 0.48651614785194397:  25%|██▉         | 59/237 [00:32<01:41,  1.75it/s]Epoch: 3, train for the 60-th batch, train loss: 0.48651614785194397:  25%|███         | 60/237 [00:32<01:40,  1.76it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6005685329437256:  19%|██▍          | 45/241 [00:30<01:58,  1.66it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6005685329437256:  19%|██▍          | 46/241 [00:30<01:57,  1.66it/s]Epoch: 5, train for the 28-th batch, train loss: 0.2519955635070801:  23%|██▉          | 27/119 [00:17<01:01,  1.50it/s]Epoch: 4, train for the 97-th batch, train loss: 0.5267021059989929:  66%|████████▌    | 96/146 [00:57<00:29,  1.68it/s]Epoch: 5, train for the 28-th batch, train loss: 0.2519955635070801:  24%|███          | 28/119 [00:17<01:00,  1.50it/s]Epoch: 4, train for the 97-th batch, train loss: 0.5267021059989929:  66%|████████▋    | 97/146 [00:57<00:30,  1.62it/s]Epoch: 2, train for the 151-th batch, train loss: 0.45795831084251404:  39%|███▉      | 150/383 [01:28<02:15,  1.72it/s]Epoch: 2, train for the 151-th batch, train loss: 0.45795831084251404:  39%|███▉      | 151/383 [01:28<02:13,  1.74it/s]Epoch: 8, train for the 96-th batch, train loss: 0.5422243475914001:  63%|████████▏    | 95/151 [00:38<00:33,  1.66it/s]Epoch: 8, train for the 96-th batch, train loss: 0.5422243475914001:  64%|████████▎    | 96/151 [00:38<00:33,  1.66it/s]Epoch: 3, train for the 61-th batch, train loss: 0.571831464767456:  25%|███▌          | 60/237 [00:33<01:40,  1.76it/s]Epoch: 3, train for the 61-th batch, train loss: 0.571831464767456:  26%|███▌          | 61/237 [00:33<01:40,  1.74it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6373291015625:  19%|███             | 46/241 [00:30<01:57,  1.66it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6373291015625:  20%|███             | 47/241 [00:30<01:56,  1.67it/s]Epoch: 2, train for the 152-th batch, train loss: 0.46496641635894775:  39%|███▉      | 151/383 [01:29<02:13,  1.74it/s]Epoch: 2, train for the 152-th batch, train loss: 0.46496641635894775:  40%|███▉      | 152/383 [01:29<02:13,  1.73it/s]Epoch: 5, train for the 29-th batch, train loss: 0.2610713541507721:  24%|███          | 28/119 [00:17<01:00,  1.50it/s]Epoch: 5, train for the 29-th batch, train loss: 0.2610713541507721:  24%|███▏         | 29/119 [00:17<00:59,  1.51it/s]Epoch: 4, train for the 98-th batch, train loss: 0.5570581555366516:  66%|████████▋    | 97/146 [00:58<00:30,  1.62it/s]Epoch: 4, train for the 98-th batch, train loss: 0.5570581555366516:  67%|████████▋    | 98/146 [00:58<00:30,  1.59it/s]Epoch: 8, train for the 97-th batch, train loss: 0.5806763172149658:  64%|████████▎    | 96/151 [00:39<00:33,  1.66it/s]Epoch: 8, train for the 97-th batch, train loss: 0.5806763172149658:  64%|████████▎    | 97/151 [00:39<00:32,  1.67it/s]Epoch: 3, train for the 62-th batch, train loss: 0.5871424078941345:  26%|███▎         | 61/237 [00:34<01:40,  1.74it/s]Epoch: 3, train for the 62-th batch, train loss: 0.5871424078941345:  26%|███▍         | 62/237 [00:34<01:41,  1.73it/s]Epoch: 1, train for the 48-th batch, train loss: 0.598322868347168:  20%|██▋           | 47/241 [00:31<01:56,  1.67it/s]Epoch: 1, train for the 48-th batch, train loss: 0.598322868347168:  20%|██▊           | 48/241 [00:31<01:55,  1.67it/s]Epoch: 2, train for the 153-th batch, train loss: 0.4697936177253723:  40%|████▎      | 152/383 [01:29<02:13,  1.73it/s]Epoch: 2, train for the 153-th batch, train loss: 0.4697936177253723:  40%|████▍      | 153/383 [01:29<02:13,  1.72it/s]Epoch: 5, train for the 30-th batch, train loss: 0.20562362670898438:  24%|██▉         | 29/119 [00:18<00:59,  1.51it/s]Epoch: 5, train for the 30-th batch, train loss: 0.20562362670898438:  25%|███         | 30/119 [00:18<00:58,  1.51it/s]Epoch: 4, train for the 99-th batch, train loss: 0.5504313707351685:  67%|████████▋    | 98/146 [00:58<00:30,  1.59it/s]Epoch: 4, train for the 99-th batch, train loss: 0.5504313707351685:  68%|████████▊    | 99/146 [00:58<00:29,  1.57it/s]Epoch: 8, train for the 98-th batch, train loss: 0.5285564064979553:  64%|████████▎    | 97/151 [00:39<00:32,  1.67it/s]Epoch: 8, train for the 98-th batch, train loss: 0.5285564064979553:  65%|████████▍    | 98/151 [00:39<00:31,  1.69it/s]Epoch: 3, train for the 63-th batch, train loss: 0.5892980098724365:  26%|███▍         | 62/237 [00:34<01:41,  1.73it/s]Epoch: 3, train for the 63-th batch, train loss: 0.5892980098724365:  27%|███▍         | 63/237 [00:34<01:40,  1.73it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6338249444961548:  20%|██▌          | 48/241 [00:32<01:55,  1.67it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6338249444961548:  20%|██▋          | 49/241 [00:32<01:54,  1.67it/s]Epoch: 2, train for the 154-th batch, train loss: 0.43992799520492554:  40%|███▉      | 153/383 [01:30<02:13,  1.72it/s]Epoch: 2, train for the 154-th batch, train loss: 0.43992799520492554:  40%|████      | 154/383 [01:30<02:13,  1.72it/s]Epoch: 5, train for the 31-th batch, train loss: 0.237907275557518:  25%|███▌          | 30/119 [00:19<00:58,  1.51it/s]Epoch: 5, train for the 31-th batch, train loss: 0.237907275557518:  26%|███▋          | 31/119 [00:19<00:57,  1.52it/s]Epoch: 4, train for the 100-th batch, train loss: 0.5622206926345825:  68%|████████▏   | 99/146 [00:59<00:29,  1.57it/s]Epoch: 4, train for the 100-th batch, train loss: 0.5622206926345825:  68%|███████▌   | 100/146 [00:59<00:29,  1.56it/s]Epoch: 3, train for the 64-th batch, train loss: 0.49861258268356323:  27%|███▏        | 63/237 [00:35<01:40,  1.73it/s]Epoch: 3, train for the 64-th batch, train loss: 0.49861258268356323:  27%|███▏        | 64/237 [00:35<01:39,  1.74it/s]Epoch: 8, train for the 99-th batch, train loss: 0.63629150390625:  65%|█████████▋     | 98/151 [00:40<00:31,  1.69it/s]Epoch: 8, train for the 99-th batch, train loss: 0.63629150390625:  66%|█████████▊     | 99/151 [00:40<00:31,  1.66it/s]Epoch: 1, train for the 50-th batch, train loss: 0.644223153591156:  20%|██▊           | 49/241 [00:32<01:54,  1.67it/s]Epoch: 1, train for the 50-th batch, train loss: 0.644223153591156:  21%|██▉           | 50/241 [00:32<01:55,  1.65it/s]Epoch: 2, train for the 155-th batch, train loss: 0.3613698184490204:  40%|████▍      | 154/383 [01:30<02:13,  1.72it/s]Epoch: 2, train for the 155-th batch, train loss: 0.3613698184490204:  40%|████▍      | 155/383 [01:30<02:11,  1.73it/s]Epoch: 3, train for the 65-th batch, train loss: 0.6263632774353027:  27%|███▌         | 64/237 [00:35<01:39,  1.74it/s]Epoch: 3, train for the 65-th batch, train loss: 0.6263632774353027:  27%|███▌         | 65/237 [00:35<01:39,  1.72it/s]Epoch: 5, train for the 32-th batch, train loss: 0.20568767189979553:  26%|███▏        | 31/119 [00:19<00:57,  1.52it/s]Epoch: 4, train for the 101-th batch, train loss: 0.5204628705978394:  68%|███████▌   | 100/146 [01:00<00:29,  1.56it/s]Epoch: 5, train for the 32-th batch, train loss: 0.20568767189979553:  27%|███▏        | 32/119 [00:19<00:57,  1.52it/s]Epoch: 4, train for the 101-th batch, train loss: 0.5204628705978394:  69%|███████▌   | 101/146 [01:00<00:29,  1.54it/s]Epoch: 8, train for the 100-th batch, train loss: 0.6746984720230103:  66%|███████▊    | 99/151 [00:40<00:31,  1.66it/s]Epoch: 8, train for the 100-th batch, train loss: 0.6746984720230103:  66%|███████▎   | 100/151 [00:40<00:30,  1.65it/s]Epoch: 1, train for the 51-th batch, train loss: 0.6032277345657349:  21%|██▋          | 50/241 [00:33<01:55,  1.65it/s]Epoch: 1, train for the 51-th batch, train loss: 0.6032277345657349:  21%|██▊          | 51/241 [00:33<01:55,  1.64it/s]Epoch: 2, train for the 156-th batch, train loss: 0.400317907333374:  40%|████▊       | 155/383 [01:31<02:11,  1.73it/s]Epoch: 2, train for the 156-th batch, train loss: 0.400317907333374:  41%|████▉       | 156/383 [01:31<02:11,  1.72it/s]Epoch: 3, train for the 66-th batch, train loss: 0.5419290065765381:  27%|███▌         | 65/237 [00:36<01:39,  1.72it/s]Epoch: 3, train for the 66-th batch, train loss: 0.5419290065765381:  28%|███▌         | 66/237 [00:36<01:38,  1.73it/s]Epoch: 5, train for the 33-th batch, train loss: 0.1949857473373413:  27%|███▍         | 32/119 [00:20<00:57,  1.52it/s]Epoch: 4, train for the 102-th batch, train loss: 0.5306541919708252:  69%|███████▌   | 101/146 [01:00<00:29,  1.54it/s]Epoch: 5, train for the 33-th batch, train loss: 0.1949857473373413:  28%|███▌         | 33/119 [00:20<00:56,  1.51it/s]Epoch: 4, train for the 102-th batch, train loss: 0.5306541919708252:  70%|███████▋   | 102/146 [01:00<00:28,  1.53it/s]Epoch: 8, train for the 101-th batch, train loss: 0.652361273765564:  66%|███████▉    | 100/151 [00:41<00:30,  1.65it/s]Epoch: 8, train for the 101-th batch, train loss: 0.652361273765564:  67%|████████    | 101/151 [00:41<00:30,  1.64it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6221281886100769:  21%|██▊          | 51/241 [00:33<01:55,  1.64it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6221281886100769:  22%|██▊          | 52/241 [00:33<01:55,  1.64it/s]Epoch: 2, train for the 157-th batch, train loss: 0.414822518825531:  41%|████▉       | 156/383 [01:32<02:11,  1.72it/s]Epoch: 2, train for the 157-th batch, train loss: 0.414822518825531:  41%|████▉       | 157/383 [01:32<02:10,  1.73it/s]Epoch: 3, train for the 67-th batch, train loss: 0.44152572751045227:  28%|███▎        | 66/237 [00:36<01:38,  1.73it/s]Epoch: 3, train for the 67-th batch, train loss: 0.44152572751045227:  28%|███▍        | 67/237 [00:36<01:37,  1.75it/s]Epoch: 8, train for the 102-th batch, train loss: 0.6301655173301697:  67%|███████▎   | 101/151 [00:42<00:30,  1.64it/s]Epoch: 8, train for the 102-th batch, train loss: 0.6301655173301697:  68%|███████▍   | 102/151 [00:42<00:29,  1.64it/s]Epoch: 5, train for the 34-th batch, train loss: 0.24594177305698395:  28%|███▎        | 33/119 [00:21<00:56,  1.51it/s]Epoch: 5, train for the 34-th batch, train loss: 0.24594177305698395:  29%|███▍        | 34/119 [00:21<00:56,  1.51it/s]Epoch: 4, train for the 103-th batch, train loss: 0.5391530990600586:  70%|███████▋   | 102/146 [01:01<00:28,  1.53it/s]Epoch: 4, train for the 103-th batch, train loss: 0.5391530990600586:  71%|███████▊   | 103/146 [01:01<00:28,  1.53it/s]Epoch: 2, train for the 158-th batch, train loss: 0.4722205102443695:  41%|████▌      | 157/383 [01:32<02:10,  1.73it/s]Epoch: 2, train for the 158-th batch, train loss: 0.4722205102443695:  41%|████▌      | 158/383 [01:32<02:09,  1.74it/s]Epoch: 1, train for the 53-th batch, train loss: 0.44285550713539124:  22%|██▌         | 52/241 [00:34<01:55,  1.64it/s]Epoch: 1, train for the 53-th batch, train loss: 0.44285550713539124:  22%|██▋         | 53/241 [00:34<01:54,  1.64it/s]Epoch: 3, train for the 68-th batch, train loss: 0.6144127249717712:  28%|███▋         | 67/237 [00:37<01:37,  1.75it/s]Epoch: 3, train for the 68-th batch, train loss: 0.6144127249717712:  29%|███▋         | 68/237 [00:37<01:38,  1.72it/s]Epoch: 8, train for the 103-th batch, train loss: 0.6393927931785583:  68%|███████▍   | 102/151 [00:42<00:29,  1.64it/s]Epoch: 8, train for the 103-th batch, train loss: 0.6393927931785583:  68%|███████▌   | 103/151 [00:42<00:29,  1.64it/s]Epoch: 5, train for the 35-th batch, train loss: 0.1760096251964569:  29%|███▋         | 34/119 [00:21<00:56,  1.51it/s]Epoch: 5, train for the 35-th batch, train loss: 0.1760096251964569:  29%|███▊         | 35/119 [00:21<00:55,  1.51it/s]Epoch: 4, train for the 104-th batch, train loss: 0.5566233396530151:  71%|███████▊   | 103/146 [01:02<00:28,  1.53it/s]Epoch: 4, train for the 104-th batch, train loss: 0.5566233396530151:  71%|███████▊   | 104/146 [01:02<00:27,  1.52it/s]Epoch: 2, train for the 159-th batch, train loss: 0.39537760615348816:  41%|████▏     | 158/383 [01:33<02:09,  1.74it/s]Epoch: 2, train for the 159-th batch, train loss: 0.39537760615348816:  42%|████▏     | 159/383 [01:33<02:09,  1.72it/s]Epoch: 1, train for the 54-th batch, train loss: 0.4845389425754547:  22%|██▊          | 53/241 [00:35<01:54,  1.64it/s]Epoch: 1, train for the 54-th batch, train loss: 0.4845389425754547:  22%|██▉          | 54/241 [00:35<01:54,  1.64it/s]Epoch: 3, train for the 69-th batch, train loss: 0.6161717772483826:  29%|███▋         | 68/237 [00:38<01:38,  1.72it/s]Epoch: 3, train for the 69-th batch, train loss: 0.6161717772483826:  29%|███▊         | 69/237 [00:38<01:37,  1.72it/s]Epoch: 8, train for the 104-th batch, train loss: 0.645386815071106:  68%|████████▏   | 103/151 [00:43<00:29,  1.64it/s]Epoch: 8, train for the 104-th batch, train loss: 0.645386815071106:  69%|████████▎   | 104/151 [00:43<00:28,  1.63it/s]Epoch: 2, train for the 160-th batch, train loss: 0.4285651445388794:  42%|████▌      | 159/383 [01:33<02:09,  1.72it/s]Epoch: 2, train for the 160-th batch, train loss: 0.4285651445388794:  42%|████▌      | 160/383 [01:33<02:09,  1.72it/s]Epoch: 5, train for the 36-th batch, train loss: 0.1819181591272354:  29%|███▊         | 35/119 [00:22<00:55,  1.51it/s]Epoch: 5, train for the 36-th batch, train loss: 0.1819181591272354:  30%|███▉         | 36/119 [00:22<00:54,  1.51it/s]Epoch: 4, train for the 105-th batch, train loss: 0.5648325085639954:  71%|███████▊   | 104/146 [01:02<00:27,  1.52it/s]Epoch: 4, train for the 105-th batch, train loss: 0.5648325085639954:  72%|███████▉   | 105/146 [01:02<00:27,  1.52it/s]Epoch: 1, train for the 55-th batch, train loss: 0.3706980049610138:  22%|██▉          | 54/241 [00:35<01:54,  1.64it/s]Epoch: 1, train for the 55-th batch, train loss: 0.3706980049610138:  23%|██▉          | 55/241 [00:35<01:53,  1.64it/s]Epoch: 3, train for the 70-th batch, train loss: 0.6193270087242126:  29%|███▊         | 69/237 [00:38<01:37,  1.72it/s]Epoch: 3, train for the 70-th batch, train loss: 0.6193270087242126:  30%|███▊         | 70/237 [00:38<01:37,  1.72it/s]Epoch: 8, train for the 105-th batch, train loss: 0.49930688738822937:  69%|██████▉   | 104/151 [00:44<00:28,  1.63it/s]Epoch: 8, train for the 105-th batch, train loss: 0.49930688738822937:  70%|██████▉   | 105/151 [00:44<00:28,  1.64it/s]Epoch: 2, train for the 161-th batch, train loss: 0.3994084894657135:  42%|████▌      | 160/383 [01:34<02:09,  1.72it/s]Epoch: 2, train for the 161-th batch, train loss: 0.3994084894657135:  42%|████▌      | 161/383 [01:34<02:09,  1.72it/s]Epoch: 1, train for the 56-th batch, train loss: 0.42642104625701904:  23%|██▋         | 55/241 [00:36<01:53,  1.64it/s]Epoch: 1, train for the 56-th batch, train loss: 0.42642104625701904:  23%|██▊         | 56/241 [00:36<01:52,  1.64it/s]Epoch: 5, train for the 37-th batch, train loss: 0.17559146881103516:  30%|███▋        | 36/119 [00:23<00:54,  1.51it/s]Epoch: 4, train for the 106-th batch, train loss: 0.5306916832923889:  72%|███████▉   | 105/146 [01:03<00:27,  1.52it/s]Epoch: 5, train for the 37-th batch, train loss: 0.17559146881103516:  31%|███▋        | 37/119 [00:23<00:54,  1.51it/s]Epoch: 4, train for the 106-th batch, train loss: 0.5306916832923889:  73%|███████▉   | 106/146 [01:03<00:26,  1.52it/s]Epoch: 3, train for the 71-th batch, train loss: 0.5730857849121094:  30%|███▊         | 70/237 [00:39<01:37,  1.72it/s]Epoch: 3, train for the 71-th batch, train loss: 0.5730857849121094:  30%|███▉         | 71/237 [00:39<01:36,  1.72it/s]Epoch: 8, train for the 106-th batch, train loss: 0.5366195440292358:  70%|███████▋   | 105/151 [00:44<00:28,  1.64it/s]Epoch: 8, train for the 106-th batch, train loss: 0.5366195440292358:  70%|███████▋   | 106/151 [00:44<00:27,  1.63it/s]Epoch: 2, train for the 162-th batch, train loss: 0.4472757577896118:  42%|████▌      | 161/383 [01:35<02:09,  1.72it/s]Epoch: 2, train for the 162-th batch, train loss: 0.4472757577896118:  42%|████▋      | 162/383 [01:35<02:08,  1.72it/s]Epoch: 1, train for the 57-th batch, train loss: 0.4482314884662628:  23%|███          | 56/241 [00:36<01:52,  1.64it/s]Epoch: 1, train for the 57-th batch, train loss: 0.4482314884662628:  24%|███          | 57/241 [00:36<01:52,  1.63it/s]Epoch: 4, train for the 107-th batch, train loss: 0.5765726566314697:  73%|███████▉   | 106/146 [01:04<00:26,  1.52it/s]Epoch: 5, train for the 38-th batch, train loss: 0.17389720678329468:  31%|███▋        | 37/119 [00:23<00:54,  1.51it/s]Epoch: 4, train for the 107-th batch, train loss: 0.5765726566314697:  73%|████████   | 107/146 [01:04<00:25,  1.51it/s]Epoch: 5, train for the 38-th batch, train loss: 0.17389720678329468:  32%|███▊        | 38/119 [00:23<00:53,  1.51it/s]Epoch: 3, train for the 72-th batch, train loss: 0.6226484179496765:  30%|███▉         | 71/237 [00:39<01:36,  1.72it/s]Epoch: 3, train for the 72-th batch, train loss: 0.6226484179496765:  30%|███▉         | 72/237 [00:39<01:36,  1.71it/s]Epoch: 8, train for the 107-th batch, train loss: 0.5422699451446533:  70%|███████▋   | 106/151 [00:45<00:27,  1.63it/s]Epoch: 8, train for the 107-th batch, train loss: 0.5422699451446533:  71%|███████▊   | 107/151 [00:45<00:26,  1.63it/s]Epoch: 2, train for the 163-th batch, train loss: 0.4248931407928467:  42%|████▋      | 162/383 [01:35<02:08,  1.72it/s]Epoch: 2, train for the 163-th batch, train loss: 0.4248931407928467:  43%|████▋      | 163/383 [01:35<02:08,  1.71it/s]Epoch: 1, train for the 58-th batch, train loss: 0.29497048258781433:  24%|██▊         | 57/241 [00:37<01:52,  1.63it/s]Epoch: 1, train for the 58-th batch, train loss: 0.29497048258781433:  24%|██▉         | 58/241 [00:37<01:52,  1.63it/s]Epoch: 4, train for the 108-th batch, train loss: 0.5391775965690613:  73%|████████   | 107/146 [01:04<00:25,  1.51it/s]Epoch: 4, train for the 108-th batch, train loss: 0.5391775965690613:  74%|████████▏  | 108/146 [01:04<00:25,  1.51it/s]Epoch: 5, train for the 39-th batch, train loss: 0.23702359199523926:  32%|███▊        | 38/119 [00:24<00:53,  1.51it/s]Epoch: 5, train for the 39-th batch, train loss: 0.23702359199523926:  33%|███▉        | 39/119 [00:24<00:53,  1.51it/s]Epoch: 3, train for the 73-th batch, train loss: 0.6362993121147156:  30%|███▉         | 72/237 [00:40<01:36,  1.71it/s]Epoch: 3, train for the 73-th batch, train loss: 0.6362993121147156:  31%|████         | 73/237 [00:40<01:35,  1.71it/s]Epoch: 2, train for the 164-th batch, train loss: 0.40521910786628723:  43%|████▎     | 163/383 [01:36<02:08,  1.71it/s]Epoch: 2, train for the 164-th batch, train loss: 0.40521910786628723:  43%|████▎     | 164/383 [01:36<02:07,  1.71it/s]Epoch: 8, train for the 108-th batch, train loss: 0.5581651329994202:  71%|███████▊   | 107/151 [00:45<00:26,  1.63it/s]Epoch: 8, train for the 108-th batch, train loss: 0.5581651329994202:  72%|███████▊   | 108/151 [00:45<00:26,  1.63it/s]Epoch: 1, train for the 59-th batch, train loss: 0.5524964928627014:  24%|███▏         | 58/241 [00:38<01:52,  1.63it/s]Epoch: 1, train for the 59-th batch, train loss: 0.5524964928627014:  24%|███▏         | 59/241 [00:38<01:51,  1.64it/s]Epoch: 3, train for the 74-th batch, train loss: 0.55826336145401:  31%|████▌          | 73/237 [00:40<01:35,  1.71it/s]Epoch: 3, train for the 74-th batch, train loss: 0.55826336145401:  31%|████▋          | 74/237 [00:40<01:34,  1.72it/s]Epoch: 4, train for the 109-th batch, train loss: 0.5303364992141724:  74%|████████▏  | 108/146 [01:05<00:25,  1.51it/s]Epoch: 4, train for the 109-th batch, train loss: 0.5303364992141724:  75%|████████▏  | 109/146 [01:05<00:24,  1.51it/s]Epoch: 5, train for the 40-th batch, train loss: 0.15965859591960907:  33%|███▉        | 39/119 [00:25<00:53,  1.51it/s]Epoch: 5, train for the 40-th batch, train loss: 0.15965859591960907:  34%|████        | 40/119 [00:25<00:52,  1.51it/s]Epoch: 2, train for the 165-th batch, train loss: 0.3776489794254303:  43%|████▋      | 164/383 [01:36<02:07,  1.71it/s]Epoch: 2, train for the 165-th batch, train loss: 0.3776489794254303:  43%|████▋      | 165/383 [01:36<02:06,  1.72it/s]Epoch: 8, train for the 109-th batch, train loss: 0.5318618416786194:  72%|███████▊   | 108/151 [00:46<00:26,  1.63it/s]Epoch: 8, train for the 109-th batch, train loss: 0.5318618416786194:  72%|███████▉   | 109/151 [00:46<00:25,  1.63it/s]Epoch: 1, train for the 60-th batch, train loss: 0.5875296592712402:  24%|███▏         | 59/241 [00:38<01:51,  1.64it/s]Epoch: 1, train for the 60-th batch, train loss: 0.5875296592712402:  25%|███▏         | 60/241 [00:38<01:50,  1.63it/s]Epoch: 3, train for the 75-th batch, train loss: 0.5200375914573669:  31%|████         | 74/237 [00:41<01:34,  1.72it/s]Epoch: 3, train for the 75-th batch, train loss: 0.5200375914573669:  32%|████         | 75/237 [00:41<01:33,  1.72it/s]Epoch: 5, train for the 41-th batch, train loss: 0.2520262598991394:  34%|████▎        | 40/119 [00:25<00:52,  1.51it/s]Epoch: 4, train for the 110-th batch, train loss: 0.49007648229599:  75%|█████████▋   | 109/146 [01:06<00:24,  1.51it/s]Epoch: 5, train for the 41-th batch, train loss: 0.2520262598991394:  34%|████▍        | 41/119 [00:25<00:51,  1.51it/s]Epoch: 4, train for the 110-th batch, train loss: 0.49007648229599:  75%|█████████▊   | 110/146 [01:06<00:23,  1.51it/s]Epoch: 2, train for the 166-th batch, train loss: 0.5029680132865906:  43%|████▋      | 165/383 [01:37<02:06,  1.72it/s]Epoch: 2, train for the 166-th batch, train loss: 0.5029680132865906:  43%|████▊      | 166/383 [01:37<02:06,  1.72it/s]Epoch: 8, train for the 110-th batch, train loss: 0.5700092315673828:  72%|███████▉   | 109/151 [00:47<00:25,  1.63it/s]Epoch: 8, train for the 110-th batch, train loss: 0.5700092315673828:  73%|████████   | 110/151 [00:47<00:25,  1.63it/s]Epoch: 1, train for the 61-th batch, train loss: 0.5698347091674805:  25%|███▏         | 60/241 [00:39<01:50,  1.63it/s]Epoch: 1, train for the 61-th batch, train loss: 0.5698347091674805:  25%|███▎         | 61/241 [00:39<01:50,  1.63it/s]Epoch: 3, train for the 76-th batch, train loss: 0.5775498151779175:  32%|████         | 75/237 [00:42<01:33,  1.72it/s]Epoch: 3, train for the 76-th batch, train loss: 0.5775498151779175:  32%|████▏        | 76/237 [00:42<01:33,  1.72it/s]Epoch: 4, train for the 111-th batch, train loss: 0.5210083723068237:  75%|████████▎  | 110/146 [01:06<00:23,  1.51it/s]Epoch: 5, train for the 42-th batch, train loss: 0.2607610523700714:  34%|████▍        | 41/119 [00:26<00:51,  1.51it/s]Epoch: 4, train for the 111-th batch, train loss: 0.5210083723068237:  76%|████████▎  | 111/146 [01:06<00:23,  1.51it/s]Epoch: 5, train for the 42-th batch, train loss: 0.2607610523700714:  35%|████▌        | 42/119 [00:26<00:51,  1.51it/s]Epoch: 2, train for the 167-th batch, train loss: 0.4072798490524292:  43%|████▊      | 166/383 [01:37<02:06,  1.72it/s]Epoch: 2, train for the 167-th batch, train loss: 0.4072798490524292:  44%|████▊      | 167/383 [01:37<02:05,  1.72it/s]Epoch: 8, train for the 111-th batch, train loss: 0.5181735754013062:  73%|████████   | 110/151 [00:47<00:25,  1.63it/s]Epoch: 8, train for the 111-th batch, train loss: 0.5181735754013062:  74%|████████   | 111/151 [00:47<00:24,  1.63it/s]Epoch: 1, train for the 62-th batch, train loss: 0.5609352588653564:  25%|███▎         | 61/241 [00:39<01:50,  1.63it/s]Epoch: 1, train for the 62-th batch, train loss: 0.5609352588653564:  26%|███▎         | 62/241 [00:39<01:49,  1.63it/s]Epoch: 3, train for the 77-th batch, train loss: 0.5880031585693359:  32%|████▏        | 76/237 [00:42<01:33,  1.72it/s]Epoch: 3, train for the 77-th batch, train loss: 0.5880031585693359:  32%|████▏        | 77/237 [00:42<01:33,  1.72it/s]Epoch: 2, train for the 168-th batch, train loss: 0.3431693911552429:  44%|████▊      | 167/383 [01:38<02:05,  1.72it/s]Epoch: 2, train for the 168-th batch, train loss: 0.3431693911552429:  44%|████▊      | 168/383 [01:38<02:05,  1.72it/s]Epoch: 4, train for the 112-th batch, train loss: 0.573202908039093:  76%|█████████   | 111/146 [01:07<00:23,  1.51it/s]Epoch: 5, train for the 43-th batch, train loss: 0.22428731620311737:  35%|████▏       | 42/119 [00:27<00:51,  1.51it/s]Epoch: 4, train for the 112-th batch, train loss: 0.573202908039093:  77%|█████████▏  | 112/146 [01:07<00:22,  1.51it/s]Epoch: 5, train for the 43-th batch, train loss: 0.22428731620311737:  36%|████▎       | 43/119 [00:27<00:50,  1.51it/s]Epoch: 8, train for the 112-th batch, train loss: 0.5319845676422119:  74%|████████   | 111/151 [00:48<00:24,  1.63it/s]Epoch: 8, train for the 112-th batch, train loss: 0.5319845676422119:  74%|████████▏  | 112/151 [00:48<00:23,  1.63it/s]Epoch: 3, train for the 78-th batch, train loss: 0.6310626864433289:  32%|████▏        | 77/237 [00:43<01:33,  1.72it/s]Epoch: 3, train for the 78-th batch, train loss: 0.6310626864433289:  33%|████▎        | 78/237 [00:43<01:32,  1.71it/s]Epoch: 1, train for the 63-th batch, train loss: 0.6992995142936707:  26%|███▎         | 62/241 [00:40<01:49,  1.63it/s]Epoch: 1, train for the 63-th batch, train loss: 0.6992995142936707:  26%|███▍         | 63/241 [00:40<01:49,  1.63it/s]Epoch: 2, train for the 169-th batch, train loss: 0.42517244815826416:  44%|████▍     | 168/383 [01:39<02:05,  1.72it/s]Epoch: 2, train for the 169-th batch, train loss: 0.42517244815826416:  44%|████▍     | 169/383 [01:39<02:05,  1.71it/s]Epoch: 5, train for the 44-th batch, train loss: 0.19120150804519653:  36%|████▎       | 43/119 [00:27<00:50,  1.51it/s]Epoch: 4, train for the 113-th batch, train loss: 0.4730979800224304:  77%|████████▍  | 112/146 [01:08<00:22,  1.51it/s]Epoch: 5, train for the 44-th batch, train loss: 0.19120150804519653:  37%|████▍       | 44/119 [00:27<00:49,  1.51it/s]Epoch: 4, train for the 113-th batch, train loss: 0.4730979800224304:  77%|████████▌  | 113/146 [01:08<00:21,  1.51it/s]Epoch: 8, train for the 113-th batch, train loss: 0.5521324872970581:  74%|████████▏  | 112/151 [00:48<00:23,  1.63it/s]Epoch: 8, train for the 113-th batch, train loss: 0.5521324872970581:  75%|████████▏  | 113/151 [00:48<00:23,  1.63it/s]Epoch: 3, train for the 79-th batch, train loss: 0.5847727656364441:  33%|████▎        | 78/237 [00:43<01:32,  1.71it/s]Epoch: 3, train for the 79-th batch, train loss: 0.5847727656364441:  33%|████▎        | 79/237 [00:43<01:32,  1.71it/s]Epoch: 1, train for the 64-th batch, train loss: 0.3056628108024597:  26%|███▍         | 63/241 [00:41<01:49,  1.63it/s]Epoch: 1, train for the 64-th batch, train loss: 0.3056628108024597:  27%|███▍         | 64/241 [00:41<01:48,  1.63it/s]Epoch: 2, train for the 170-th batch, train loss: 0.410476416349411:  44%|█████▎      | 169/383 [01:39<02:05,  1.71it/s]Epoch: 2, train for the 170-th batch, train loss: 0.410476416349411:  44%|█████▎      | 170/383 [01:39<02:04,  1.71it/s]Epoch: 5, train for the 45-th batch, train loss: 0.17569944262504578:  37%|████▍       | 44/119 [00:28<00:49,  1.51it/s]Epoch: 4, train for the 114-th batch, train loss: 0.5414114594459534:  77%|████████▌  | 113/146 [01:08<00:21,  1.51it/s]Epoch: 5, train for the 45-th batch, train loss: 0.17569944262504578:  38%|████▌       | 45/119 [00:28<00:49,  1.51it/s]Epoch: 4, train for the 114-th batch, train loss: 0.5414114594459534:  78%|████████▌  | 114/146 [01:08<00:21,  1.51it/s]Epoch: 8, train for the 114-th batch, train loss: 0.5004534125328064:  75%|████████▏  | 113/151 [00:49<00:23,  1.63it/s]Epoch: 8, train for the 114-th batch, train loss: 0.5004534125328064:  75%|████████▎  | 114/151 [00:49<00:22,  1.63it/s]Epoch: 3, train for the 80-th batch, train loss: 0.5814558863639832:  33%|████▎        | 79/237 [00:44<01:32,  1.71it/s]Epoch: 3, train for the 80-th batch, train loss: 0.5814558863639832:  34%|████▍        | 80/237 [00:44<01:31,  1.71it/s]Epoch: 1, train for the 65-th batch, train loss: 0.26709699630737305:  27%|███▏        | 64/241 [00:41<01:48,  1.63it/s]Epoch: 1, train for the 65-th batch, train loss: 0.26709699630737305:  27%|███▏        | 65/241 [00:41<01:47,  1.63it/s]Epoch: 2, train for the 171-th batch, train loss: 0.5175445675849915:  44%|████▉      | 170/383 [01:40<02:04,  1.71it/s]Epoch: 2, train for the 171-th batch, train loss: 0.5175445675849915:  45%|████▉      | 171/383 [01:40<02:03,  1.71it/s]Epoch: 8, train for the 115-th batch, train loss: 0.5219486951828003:  75%|████████▎  | 114/151 [00:50<00:22,  1.63it/s]Epoch: 8, train for the 115-th batch, train loss: 0.5219486951828003:  76%|████████▍  | 115/151 [00:50<00:22,  1.63it/s]Epoch: 4, train for the 115-th batch, train loss: 0.5159199237823486:  78%|████████▌  | 114/146 [01:09<00:21,  1.51it/s]Epoch: 4, train for the 115-th batch, train loss: 0.5159199237823486:  79%|████████▋  | 115/146 [01:09<00:20,  1.51it/s]Epoch: 5, train for the 46-th batch, train loss: 0.2232562005519867:  38%|████▉        | 45/119 [00:29<00:49,  1.51it/s]Epoch: 5, train for the 46-th batch, train loss: 0.2232562005519867:  39%|█████        | 46/119 [00:29<00:48,  1.50it/s]Epoch: 3, train for the 81-th batch, train loss: 0.6139412522315979:  34%|████▍        | 80/237 [00:45<01:31,  1.71it/s]Epoch: 3, train for the 81-th batch, train loss: 0.6139412522315979:  34%|████▍        | 81/237 [00:45<01:31,  1.71it/s]Epoch: 1, train for the 66-th batch, train loss: 0.22919495403766632:  27%|███▏        | 65/241 [00:42<01:47,  1.63it/s]Epoch: 1, train for the 66-th batch, train loss: 0.22919495403766632:  27%|███▎        | 66/241 [00:42<01:47,  1.63it/s]Epoch: 2, train for the 172-th batch, train loss: 0.34756651520729065:  45%|████▍     | 171/383 [01:40<02:03,  1.71it/s]Epoch: 2, train for the 172-th batch, train loss: 0.34756651520729065:  45%|████▍     | 172/383 [01:40<02:03,  1.71it/s]Epoch: 8, train for the 116-th batch, train loss: 0.4909577965736389:  76%|████████▍  | 115/151 [00:50<00:22,  1.63it/s]Epoch: 8, train for the 116-th batch, train loss: 0.4909577965736389:  77%|████████▍  | 116/151 [00:50<00:21,  1.63it/s]Epoch: 3, train for the 82-th batch, train loss: 0.5501070022583008:  34%|████▍        | 81/237 [00:45<01:31,  1.71it/s]Epoch: 3, train for the 82-th batch, train loss: 0.5501070022583008:  35%|████▍        | 82/237 [00:45<01:29,  1.72it/s]Epoch: 4, train for the 116-th batch, train loss: 0.5186899304389954:  79%|████████▋  | 115/146 [01:10<00:20,  1.51it/s]Epoch: 5, train for the 47-th batch, train loss: 0.24278277158737183:  39%|████▋       | 46/119 [00:29<00:48,  1.50it/s]Epoch: 4, train for the 116-th batch, train loss: 0.5186899304389954:  79%|████████▋  | 116/146 [01:10<00:19,  1.51it/s]Epoch: 5, train for the 47-th batch, train loss: 0.24278277158737183:  39%|████▋       | 47/119 [00:29<00:47,  1.51it/s]Epoch: 1, train for the 67-th batch, train loss: 0.4103282392024994:  27%|███▌         | 66/241 [00:43<01:47,  1.63it/s]Epoch: 1, train for the 67-th batch, train loss: 0.4103282392024994:  28%|███▌         | 67/241 [00:43<01:46,  1.63it/s]Epoch: 2, train for the 173-th batch, train loss: 0.38860514760017395:  45%|████▍     | 172/383 [01:41<02:03,  1.71it/s]Epoch: 2, train for the 173-th batch, train loss: 0.38860514760017395:  45%|████▌     | 173/383 [01:41<02:02,  1.72it/s]Epoch: 8, train for the 117-th batch, train loss: 0.5367417931556702:  77%|████████▍  | 116/151 [00:51<00:21,  1.63it/s]Epoch: 8, train for the 117-th batch, train loss: 0.5367417931556702:  77%|████████▌  | 117/151 [00:51<00:20,  1.63it/s]Epoch: 3, train for the 83-th batch, train loss: 0.6390219926834106:  35%|████▍        | 82/237 [00:46<01:29,  1.72it/s]Epoch: 3, train for the 83-th batch, train loss: 0.6390219926834106:  35%|████▌        | 83/237 [00:46<01:29,  1.71it/s]Epoch: 4, train for the 117-th batch, train loss: 0.5548804402351379:  79%|████████▋  | 116/146 [01:10<00:19,  1.51it/s]Epoch: 5, train for the 48-th batch, train loss: 0.2629311680793762:  39%|█████▏       | 47/119 [00:30<00:47,  1.51it/s]Epoch: 4, train for the 117-th batch, train loss: 0.5548804402351379:  80%|████████▊  | 117/146 [01:10<00:19,  1.50it/s]Epoch: 5, train for the 48-th batch, train loss: 0.2629311680793762:  40%|█████▏       | 48/119 [00:30<00:47,  1.50it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5389483571052551:  28%|███▌         | 67/241 [00:43<01:46,  1.63it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5389483571052551:  28%|███▋         | 68/241 [00:43<01:46,  1.63it/s]Epoch: 2, train for the 174-th batch, train loss: 0.46045249700546265:  45%|████▌     | 173/383 [01:42<02:02,  1.72it/s]Epoch: 2, train for the 174-th batch, train loss: 0.46045249700546265:  45%|████▌     | 174/383 [01:42<02:01,  1.72it/s]Epoch: 3, train for the 84-th batch, train loss: 0.497161865234375:  35%|████▉         | 83/237 [00:46<01:29,  1.71it/s]Epoch: 3, train for the 84-th batch, train loss: 0.497161865234375:  35%|████▉         | 84/237 [00:46<01:28,  1.72it/s]Epoch: 8, train for the 118-th batch, train loss: 0.49031347036361694:  77%|███████▋  | 117/151 [00:52<00:20,  1.63it/s]Epoch: 8, train for the 118-th batch, train loss: 0.49031347036361694:  78%|███████▊  | 118/151 [00:52<00:20,  1.63it/s]Epoch: 1, train for the 69-th batch, train loss: 0.656063973903656:  28%|███▉          | 68/241 [00:44<01:46,  1.63it/s]Epoch: 1, train for the 69-th batch, train loss: 0.656063973903656:  29%|████          | 69/241 [00:44<01:45,  1.63it/s]Epoch: 4, train for the 118-th batch, train loss: 0.5086535811424255:  80%|████████▊  | 117/146 [01:11<00:19,  1.50it/s]Epoch: 4, train for the 118-th batch, train loss: 0.5086535811424255:  81%|████████▉  | 118/146 [01:11<00:18,  1.51it/s]Epoch: 5, train for the 49-th batch, train loss: 0.21128255128860474:  40%|████▊       | 48/119 [00:31<00:47,  1.50it/s]Epoch: 5, train for the 49-th batch, train loss: 0.21128255128860474:  41%|████▉       | 49/119 [00:31<00:46,  1.51it/s]Epoch: 2, train for the 175-th batch, train loss: 0.4279652237892151:  45%|████▉      | 174/383 [01:42<02:01,  1.72it/s]Epoch: 2, train for the 175-th batch, train loss: 0.4279652237892151:  46%|█████      | 175/383 [01:42<02:00,  1.72it/s]Epoch: 3, train for the 85-th batch, train loss: 0.6465233564376831:  35%|████▌        | 84/237 [00:47<01:28,  1.72it/s]Epoch: 3, train for the 85-th batch, train loss: 0.6465233564376831:  36%|████▋        | 85/237 [00:47<01:28,  1.72it/s]Epoch: 8, train for the 119-th batch, train loss: 0.5038198232650757:  78%|████████▌  | 118/151 [00:52<00:20,  1.63it/s]Epoch: 8, train for the 119-th batch, train loss: 0.5038198232650757:  79%|████████▋  | 119/151 [00:52<00:19,  1.64it/s]Epoch: 1, train for the 70-th batch, train loss: 0.6370307207107544:  29%|███▋         | 69/241 [00:44<01:45,  1.63it/s]Epoch: 1, train for the 70-th batch, train loss: 0.6370307207107544:  29%|███▊         | 70/241 [00:44<01:44,  1.63it/s]Epoch: 2, train for the 176-th batch, train loss: 0.4391765594482422:  46%|█████      | 175/383 [01:43<02:00,  1.72it/s]Epoch: 2, train for the 176-th batch, train loss: 0.4391765594482422:  46%|█████      | 176/383 [01:43<02:00,  1.72it/s]Epoch: 4, train for the 119-th batch, train loss: 0.5476696491241455:  81%|████████▉  | 118/146 [01:12<00:18,  1.51it/s]Epoch: 4, train for the 119-th batch, train loss: 0.5476696491241455:  82%|████████▉  | 119/146 [01:12<00:17,  1.51it/s]Epoch: 5, train for the 50-th batch, train loss: 0.20548436045646667:  41%|████▉       | 49/119 [00:31<00:46,  1.51it/s]Epoch: 5, train for the 50-th batch, train loss: 0.20548436045646667:  42%|█████       | 50/119 [00:31<00:45,  1.51it/s]Epoch: 3, train for the 86-th batch, train loss: 0.4394671618938446:  36%|████▋        | 85/237 [00:47<01:28,  1.72it/s]Epoch: 3, train for the 86-th batch, train loss: 0.4394671618938446:  36%|████▋        | 86/237 [00:47<01:27,  1.73it/s]Epoch: 8, train for the 120-th batch, train loss: 0.6051765084266663:  79%|████████▋  | 119/151 [00:53<00:19,  1.64it/s]Epoch: 8, train for the 120-th batch, train loss: 0.6051765084266663:  79%|████████▋  | 120/151 [00:53<00:18,  1.64it/s]Epoch: 1, train for the 71-th batch, train loss: 0.6656402945518494:  29%|███▊         | 70/241 [00:45<01:44,  1.63it/s]Epoch: 1, train for the 71-th batch, train loss: 0.6656402945518494:  29%|███▊         | 71/241 [00:45<01:43,  1.63it/s]Epoch: 2, train for the 177-th batch, train loss: 0.4274941086769104:  46%|█████      | 176/383 [01:43<02:00,  1.72it/s]Epoch: 2, train for the 177-th batch, train loss: 0.4274941086769104:  46%|█████      | 177/383 [01:43<01:59,  1.73it/s]Epoch: 4, train for the 120-th batch, train loss: 0.5161315202713013:  82%|████████▉  | 119/146 [01:12<00:17,  1.51it/s]Epoch: 5, train for the 51-th batch, train loss: 0.22873133420944214:  42%|█████       | 50/119 [00:32<00:45,  1.51it/s]Epoch: 4, train for the 120-th batch, train loss: 0.5161315202713013:  82%|█████████  | 120/146 [01:12<00:17,  1.51it/s]Epoch: 5, train for the 51-th batch, train loss: 0.22873133420944214:  43%|█████▏      | 51/119 [00:32<00:45,  1.51it/s]Epoch: 3, train for the 87-th batch, train loss: 0.5305225849151611:  36%|████▋        | 86/237 [00:48<01:27,  1.73it/s]Epoch: 3, train for the 87-th batch, train loss: 0.5305225849151611:  37%|████▊        | 87/237 [00:48<01:26,  1.73it/s]Epoch: 8, train for the 121-th batch, train loss: 0.4958440363407135:  79%|████████▋  | 120/151 [00:53<00:18,  1.64it/s]Epoch: 8, train for the 121-th batch, train loss: 0.4958440363407135:  80%|████████▊  | 121/151 [00:53<00:18,  1.63it/s]Epoch: 1, train for the 72-th batch, train loss: 0.6050755977630615:  29%|███▊         | 71/241 [00:46<01:43,  1.63it/s]Epoch: 1, train for the 72-th batch, train loss: 0.6050755977630615:  30%|███▉         | 72/241 [00:46<01:43,  1.63it/s]Epoch: 2, train for the 178-th batch, train loss: 0.48559844493865967:  46%|████▌     | 177/383 [01:44<01:59,  1.73it/s]Epoch: 2, train for the 178-th batch, train loss: 0.48559844493865967:  46%|████▋     | 178/383 [01:44<01:58,  1.73it/s]Epoch: 5, train for the 52-th batch, train loss: 0.25296032428741455:  43%|█████▏      | 51/119 [00:33<00:45,  1.51it/s]Epoch: 4, train for the 121-th batch, train loss: 0.522355318069458:  82%|█████████▊  | 120/146 [01:13<00:17,  1.51it/s]Epoch: 4, train for the 121-th batch, train loss: 0.522355318069458:  83%|█████████▉  | 121/146 [01:13<00:16,  1.51it/s]Epoch: 5, train for the 52-th batch, train loss: 0.25296032428741455:  44%|█████▏      | 52/119 [00:33<00:44,  1.51it/s]Epoch: 3, train for the 88-th batch, train loss: 0.6662797331809998:  37%|████▊        | 87/237 [00:49<01:26,  1.73it/s]Epoch: 3, train for the 88-th batch, train loss: 0.6662797331809998:  37%|████▊        | 88/237 [00:49<01:26,  1.72it/s]Epoch: 8, train for the 122-th batch, train loss: 0.5420016646385193:  80%|████████▊  | 121/151 [00:54<00:18,  1.63it/s]Epoch: 8, train for the 122-th batch, train loss: 0.5420016646385193:  81%|████████▉  | 122/151 [00:54<00:17,  1.63it/s]Epoch: 2, train for the 179-th batch, train loss: 0.41422951221466064:  46%|████▋     | 178/383 [01:44<01:58,  1.73it/s]Epoch: 2, train for the 179-th batch, train loss: 0.41422951221466064:  47%|████▋     | 179/383 [01:44<01:58,  1.72it/s]Epoch: 1, train for the 73-th batch, train loss: 0.559984564781189:  30%|████▏         | 72/241 [00:46<01:43,  1.63it/s]Epoch: 1, train for the 73-th batch, train loss: 0.559984564781189:  30%|████▏         | 73/241 [00:46<01:43,  1.63it/s]Epoch: 4, train for the 122-th batch, train loss: 0.594300389289856:  83%|█████████▉  | 121/146 [01:14<00:16,  1.51it/s]Epoch: 5, train for the 53-th batch, train loss: 0.1813763976097107:  44%|█████▋       | 52/119 [00:33<00:44,  1.51it/s]Epoch: 4, train for the 122-th batch, train loss: 0.594300389289856:  84%|██████████  | 122/146 [01:14<00:15,  1.51it/s]Epoch: 5, train for the 53-th batch, train loss: 0.1813763976097107:  45%|█████▊       | 53/119 [00:33<00:43,  1.51it/s]Epoch: 3, train for the 89-th batch, train loss: 0.518760085105896:  37%|█████▏        | 88/237 [00:49<01:26,  1.72it/s]Epoch: 3, train for the 89-th batch, train loss: 0.518760085105896:  38%|█████▎        | 89/237 [00:49<01:25,  1.73it/s]Epoch: 8, train for the 123-th batch, train loss: 0.5873907804489136:  81%|████████▉  | 122/151 [00:55<00:17,  1.63it/s]Epoch: 8, train for the 123-th batch, train loss: 0.5873907804489136:  81%|████████▉  | 123/151 [00:55<00:17,  1.63it/s]Epoch: 2, train for the 180-th batch, train loss: 0.43813079595565796:  47%|████▋     | 179/383 [01:45<01:58,  1.72it/s]Epoch: 2, train for the 180-th batch, train loss: 0.43813079595565796:  47%|████▋     | 180/383 [01:45<01:57,  1.73it/s]Epoch: 1, train for the 74-th batch, train loss: 0.528678834438324:  30%|████▏         | 73/241 [00:47<01:43,  1.63it/s]Epoch: 1, train for the 74-th batch, train loss: 0.528678834438324:  31%|████▎         | 74/241 [00:47<01:42,  1.63it/s]Epoch: 3, train for the 90-th batch, train loss: 0.658489465713501:  38%|█████▎        | 89/237 [00:50<01:25,  1.73it/s]Epoch: 3, train for the 90-th batch, train loss: 0.658489465713501:  38%|█████▎        | 90/237 [00:50<01:25,  1.72it/s]Epoch: 5, train for the 54-th batch, train loss: 0.2068762481212616:  45%|█████▊       | 53/119 [00:34<00:43,  1.51it/s]Epoch: 5, train for the 54-th batch, train loss: 0.2068762481212616:  45%|█████▉       | 54/119 [00:34<00:43,  1.50it/s]Epoch: 4, train for the 123-th batch, train loss: 0.5761330127716064:  84%|█████████▏ | 122/146 [01:14<00:15,  1.51it/s]Epoch: 4, train for the 123-th batch, train loss: 0.5761330127716064:  84%|█████████▎ | 123/146 [01:14<00:15,  1.50it/s]Epoch: 8, train for the 124-th batch, train loss: 0.5435999035835266:  81%|████████▉  | 123/151 [00:55<00:17,  1.63it/s]Epoch: 8, train for the 124-th batch, train loss: 0.5435999035835266:  82%|█████████  | 124/151 [00:55<00:16,  1.63it/s]Epoch: 2, train for the 181-th batch, train loss: 0.41237714886665344:  47%|████▋     | 180/383 [01:46<01:57,  1.73it/s]Epoch: 2, train for the 181-th batch, train loss: 0.41237714886665344:  47%|████▋     | 181/383 [01:46<01:57,  1.72it/s]Epoch: 1, train for the 75-th batch, train loss: 0.437710165977478:  31%|████▎         | 74/241 [00:47<01:42,  1.63it/s]Epoch: 1, train for the 75-th batch, train loss: 0.437710165977478:  31%|████▎         | 75/241 [00:47<01:41,  1.63it/s]Epoch: 3, train for the 91-th batch, train loss: 0.6575465202331543:  38%|████▉        | 90/237 [00:50<01:25,  1.72it/s]Epoch: 3, train for the 91-th batch, train loss: 0.6575465202331543:  38%|████▉        | 91/237 [00:50<01:25,  1.71it/s]Epoch: 5, train for the 55-th batch, train loss: 0.18617776036262512:  45%|█████▍      | 54/119 [00:35<00:43,  1.50it/s]Epoch: 4, train for the 124-th batch, train loss: 0.57356858253479:  84%|██████████▉  | 123/146 [01:15<00:15,  1.50it/s]Epoch: 5, train for the 55-th batch, train loss: 0.18617776036262512:  46%|█████▌      | 55/119 [00:35<00:42,  1.51it/s]Epoch: 4, train for the 124-th batch, train loss: 0.57356858253479:  85%|███████████  | 124/146 [01:15<00:14,  1.51it/s]Epoch: 8, train for the 125-th batch, train loss: 0.54265296459198:  82%|██████████▋  | 124/151 [00:56<00:16,  1.63it/s]Epoch: 8, train for the 125-th batch, train loss: 0.54265296459198:  83%|██████████▊  | 125/151 [00:56<00:15,  1.63it/s]Epoch: 2, train for the 182-th batch, train loss: 0.5121706128120422:  47%|█████▏     | 181/383 [01:46<01:57,  1.72it/s]Epoch: 2, train for the 182-th batch, train loss: 0.5121706128120422:  48%|█████▏     | 182/383 [01:46<01:57,  1.72it/s]Epoch: 1, train for the 76-th batch, train loss: 0.17870445549488068:  31%|███▋        | 75/241 [00:48<01:41,  1.63it/s]Epoch: 1, train for the 76-th batch, train loss: 0.17870445549488068:  32%|███▊        | 76/241 [00:48<01:41,  1.63it/s]Epoch: 3, train for the 92-th batch, train loss: 0.6374539136886597:  38%|████▉        | 91/237 [00:51<01:25,  1.71it/s]Epoch: 3, train for the 92-th batch, train loss: 0.6374539136886597:  39%|█████        | 92/237 [00:51<01:24,  1.71it/s]Epoch: 4, train for the 125-th batch, train loss: 0.510202944278717:  85%|██████████▏ | 124/146 [01:16<00:14,  1.51it/s]Epoch: 4, train for the 125-th batch, train loss: 0.510202944278717:  86%|██████████▎ | 125/146 [01:16<00:13,  1.51it/s]Epoch: 5, train for the 56-th batch, train loss: 0.20193450152873993:  46%|█████▌      | 55/119 [00:35<00:42,  1.51it/s]Epoch: 5, train for the 56-th batch, train loss: 0.20193450152873993:  47%|█████▋      | 56/119 [00:35<00:41,  1.51it/s]Epoch: 8, train for the 126-th batch, train loss: 0.5441868305206299:  83%|█████████  | 125/151 [00:56<00:15,  1.63it/s]Epoch: 8, train for the 126-th batch, train loss: 0.5441868305206299:  83%|█████████▏ | 126/151 [00:56<00:15,  1.63it/s]Epoch: 2, train for the 183-th batch, train loss: 0.4147506654262543:  48%|█████▏     | 182/383 [01:47<01:57,  1.72it/s]Epoch: 2, train for the 183-th batch, train loss: 0.4147506654262543:  48%|█████▎     | 183/383 [01:47<01:56,  1.71it/s]Epoch: 1, train for the 77-th batch, train loss: 0.4495038390159607:  32%|████         | 76/241 [00:49<01:41,  1.63it/s]Epoch: 1, train for the 77-th batch, train loss: 0.4495038390159607:  32%|████▏        | 77/241 [00:49<01:40,  1.63it/s]Epoch: 3, train for the 93-th batch, train loss: 0.6193431615829468:  39%|█████        | 92/237 [00:52<01:24,  1.71it/s]Epoch: 3, train for the 93-th batch, train loss: 0.6193431615829468:  39%|█████        | 93/237 [00:52<01:24,  1.71it/s]Epoch: 2, train for the 184-th batch, train loss: 0.44907379150390625:  48%|████▊     | 183/383 [01:47<01:56,  1.71it/s]Epoch: 2, train for the 184-th batch, train loss: 0.44907379150390625:  48%|████▊     | 184/383 [01:47<01:56,  1.71it/s]Epoch: 4, train for the 126-th batch, train loss: 0.5414406657218933:  86%|█████████▍ | 125/146 [01:16<00:13,  1.51it/s]Epoch: 5, train for the 57-th batch, train loss: 0.15390504896640778:  47%|█████▋      | 56/119 [00:36<00:41,  1.51it/s]Epoch: 4, train for the 126-th batch, train loss: 0.5414406657218933:  86%|█████████▍ | 126/146 [01:16<00:13,  1.51it/s]Epoch: 5, train for the 57-th batch, train loss: 0.15390504896640778:  48%|█████▋      | 57/119 [00:36<00:41,  1.51it/s]Epoch: 8, train for the 127-th batch, train loss: 0.5546902418136597:  83%|█████████▏ | 126/151 [00:57<00:15,  1.63it/s]Epoch: 8, train for the 127-th batch, train loss: 0.5546902418136597:  84%|█████████▎ | 127/151 [00:57<00:14,  1.63it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5856248140335083:  32%|████▏        | 77/241 [00:49<01:40,  1.63it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5856248140335083:  32%|████▏        | 78/241 [00:49<01:39,  1.63it/s]Epoch: 3, train for the 94-th batch, train loss: 0.6563072204589844:  39%|█████        | 93/237 [00:52<01:24,  1.71it/s]Epoch: 3, train for the 94-th batch, train loss: 0.6563072204589844:  40%|█████▏       | 94/237 [00:52<01:23,  1.71it/s]Epoch: 2, train for the 185-th batch, train loss: 0.4030076563358307:  48%|█████▎     | 184/383 [01:48<01:56,  1.71it/s]Epoch: 2, train for the 185-th batch, train loss: 0.4030076563358307:  48%|█████▎     | 185/383 [01:48<01:55,  1.71it/s]Epoch: 8, train for the 128-th batch, train loss: 0.5675373077392578:  84%|█████████▎ | 127/151 [00:58<00:14,  1.63it/s]Epoch: 8, train for the 128-th batch, train loss: 0.5675373077392578:  85%|█████████▎ | 128/151 [00:58<00:14,  1.63it/s]Epoch: 4, train for the 127-th batch, train loss: 0.5427432060241699:  86%|█████████▍ | 126/146 [01:17<00:13,  1.51it/s]Epoch: 5, train for the 58-th batch, train loss: 0.16848182678222656:  48%|█████▋      | 57/119 [00:37<00:41,  1.51it/s]Epoch: 4, train for the 127-th batch, train loss: 0.5427432060241699:  87%|█████████▌ | 127/146 [01:17<00:12,  1.51it/s]Epoch: 5, train for the 58-th batch, train loss: 0.16848182678222656:  49%|█████▊      | 58/119 [00:37<00:40,  1.51it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5086719393730164:  32%|████▏        | 78/241 [00:50<01:39,  1.63it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5086719393730164:  33%|████▎        | 79/241 [00:50<01:39,  1.63it/s]Epoch: 3, train for the 95-th batch, train loss: 0.48189714550971985:  40%|████▊       | 94/237 [00:53<01:23,  1.71it/s]Epoch: 3, train for the 95-th batch, train loss: 0.48189714550971985:  40%|████▊       | 95/237 [00:53<01:22,  1.72it/s]Epoch: 2, train for the 186-th batch, train loss: 0.40737485885620117:  48%|████▊     | 185/383 [01:49<01:55,  1.71it/s]Epoch: 2, train for the 186-th batch, train loss: 0.40737485885620117:  49%|████▊     | 186/383 [01:49<01:54,  1.71it/s]Epoch: 8, train for the 129-th batch, train loss: 0.5629724264144897:  85%|█████████▎ | 128/151 [00:58<00:14,  1.63it/s]Epoch: 8, train for the 129-th batch, train loss: 0.5629724264144897:  85%|█████████▍ | 129/151 [00:58<00:13,  1.63it/s]Epoch: 4, train for the 128-th batch, train loss: 0.5344733595848083:  87%|█████████▌ | 127/146 [01:18<00:12,  1.51it/s]Epoch: 5, train for the 59-th batch, train loss: 0.1822563111782074:  49%|██████▎      | 58/119 [00:37<00:40,  1.51it/s]Epoch: 4, train for the 128-th batch, train loss: 0.5344733595848083:  88%|█████████▋ | 128/146 [01:18<00:11,  1.50it/s]Epoch: 5, train for the 59-th batch, train loss: 0.1822563111782074:  50%|██████▍      | 59/119 [00:37<00:39,  1.50it/s]Epoch: 1, train for the 80-th batch, train loss: 0.3555556833744049:  33%|████▎        | 79/241 [00:51<01:39,  1.63it/s]Epoch: 1, train for the 80-th batch, train loss: 0.3555556833744049:  33%|████▎        | 80/241 [00:51<01:38,  1.63it/s]Epoch: 3, train for the 96-th batch, train loss: 0.6695751547813416:  40%|█████▏       | 95/237 [00:53<01:22,  1.72it/s]Epoch: 3, train for the 96-th batch, train loss: 0.6695751547813416:  41%|█████▎       | 96/237 [00:53<01:22,  1.71it/s]Epoch: 2, train for the 187-th batch, train loss: 0.4029158055782318:  49%|█████▎     | 186/383 [01:49<01:54,  1.71it/s]Epoch: 2, train for the 187-th batch, train loss: 0.4029158055782318:  49%|█████▎     | 187/383 [01:49<01:54,  1.71it/s]Epoch: 8, train for the 130-th batch, train loss: 0.547390878200531:  85%|██████████▎ | 129/151 [00:59<00:13,  1.63it/s]Epoch: 8, train for the 130-th batch, train loss: 0.547390878200531:  86%|██████████▎ | 130/151 [00:59<00:12,  1.63it/s]Epoch: 4, train for the 129-th batch, train loss: 0.5629355907440186:  88%|█████████▋ | 128/146 [01:18<00:11,  1.50it/s]Epoch: 5, train for the 60-th batch, train loss: 0.14974313974380493:  50%|█████▉      | 59/119 [00:38<00:39,  1.50it/s]Epoch: 4, train for the 129-th batch, train loss: 0.5629355907440186:  88%|█████████▋ | 129/146 [01:18<00:11,  1.50it/s]Epoch: 5, train for the 60-th batch, train loss: 0.14974313974380493:  50%|██████      | 60/119 [00:38<00:39,  1.50it/s]Epoch: 1, train for the 81-th batch, train loss: 0.4233483672142029:  33%|████▎        | 80/241 [00:51<01:38,  1.63it/s]Epoch: 1, train for the 81-th batch, train loss: 0.4233483672142029:  34%|████▎        | 81/241 [00:51<01:38,  1.63it/s]Epoch: 3, train for the 97-th batch, train loss: 0.644690215587616:  41%|█████▋        | 96/237 [00:54<01:22,  1.71it/s]Epoch: 3, train for the 97-th batch, train loss: 0.644690215587616:  41%|█████▋        | 97/237 [00:54<01:22,  1.71it/s]Epoch: 2, train for the 188-th batch, train loss: 0.4623195230960846:  49%|█████▎     | 187/383 [01:50<01:54,  1.71it/s]Epoch: 2, train for the 188-th batch, train loss: 0.4623195230960846:  49%|█████▍     | 188/383 [01:50<01:53,  1.71it/s]Epoch: 8, train for the 131-th batch, train loss: 0.5372165441513062:  86%|█████████▍ | 130/151 [00:59<00:12,  1.63it/s]Epoch: 8, train for the 131-th batch, train loss: 0.5372165441513062:  87%|█████████▌ | 131/151 [00:59<00:12,  1.63it/s]Epoch: 1, train for the 82-th batch, train loss: 0.4060206115245819:  34%|████▎        | 81/241 [00:52<01:38,  1.63it/s]Epoch: 1, train for the 82-th batch, train loss: 0.4060206115245819:  34%|████▍        | 82/241 [00:52<01:37,  1.63it/s]Epoch: 3, train for the 98-th batch, train loss: 0.6136492490768433:  41%|█████▎       | 97/237 [00:54<01:22,  1.71it/s]Epoch: 3, train for the 98-th batch, train loss: 0.6136492490768433:  41%|█████▍       | 98/237 [00:54<01:21,  1.71it/s]Epoch: 4, train for the 130-th batch, train loss: 0.4850141406059265:  88%|█████████▋ | 129/146 [01:19<00:11,  1.50it/s]Epoch: 5, train for the 61-th batch, train loss: 0.1609755903482437:  50%|██████▌      | 60/119 [00:39<00:39,  1.50it/s]Epoch: 4, train for the 130-th batch, train loss: 0.4850141406059265:  89%|█████████▊ | 130/146 [01:19<00:10,  1.50it/s]Epoch: 5, train for the 61-th batch, train loss: 0.1609755903482437:  51%|██████▋      | 61/119 [00:39<00:38,  1.50it/s]Epoch: 2, train for the 189-th batch, train loss: 0.4278689920902252:  49%|█████▍     | 188/383 [01:50<01:53,  1.71it/s]Epoch: 2, train for the 189-th batch, train loss: 0.4278689920902252:  49%|█████▍     | 189/383 [01:50<01:53,  1.71it/s]Epoch: 8, train for the 132-th batch, train loss: 0.5240331888198853:  87%|█████████▌ | 131/151 [01:00<00:12,  1.63it/s]Epoch: 8, train for the 132-th batch, train loss: 0.5240331888198853:  87%|█████████▌ | 132/151 [01:00<00:11,  1.63it/s]Epoch: 3, train for the 99-th batch, train loss: 0.6193631887435913:  41%|█████▍       | 98/237 [00:55<01:21,  1.71it/s]Epoch: 3, train for the 99-th batch, train loss: 0.6193631887435913:  42%|█████▍       | 99/237 [00:55<01:20,  1.71it/s]Epoch: 1, train for the 83-th batch, train loss: 0.6476104855537415:  34%|████▍        | 82/241 [00:52<01:37,  1.63it/s]Epoch: 1, train for the 83-th batch, train loss: 0.6476104855537415:  34%|████▍        | 83/241 [00:52<01:36,  1.63it/s]Epoch: 4, train for the 131-th batch, train loss: 0.5398278832435608:  89%|█████████▊ | 130/146 [01:20<00:10,  1.50it/s]Epoch: 5, train for the 62-th batch, train loss: 0.2543763816356659:  51%|██████▋      | 61/119 [00:39<00:38,  1.50it/s]Epoch: 4, train for the 131-th batch, train loss: 0.5398278832435608:  90%|█████████▊ | 131/146 [01:20<00:09,  1.50it/s]Epoch: 5, train for the 62-th batch, train loss: 0.2543763816356659:  52%|██████▊      | 62/119 [00:39<00:37,  1.50it/s]Epoch: 2, train for the 190-th batch, train loss: 0.44267040491104126:  49%|████▉     | 189/383 [01:51<01:53,  1.71it/s]Epoch: 2, train for the 190-th batch, train loss: 0.44267040491104126:  50%|████▉     | 190/383 [01:51<01:52,  1.71it/s]Epoch: 8, train for the 133-th batch, train loss: 0.5334426760673523:  87%|█████████▌ | 132/151 [01:01<00:11,  1.63it/s]Epoch: 8, train for the 133-th batch, train loss: 0.5334426760673523:  88%|█████████▋ | 133/151 [01:01<00:11,  1.63it/s]Epoch: 3, train for the 100-th batch, train loss: 0.637898325920105:  42%|█████▍       | 99/237 [00:56<01:20,  1.71it/s]Epoch: 3, train for the 100-th batch, train loss: 0.637898325920105:  42%|█████       | 100/237 [00:56<01:20,  1.71it/s]Epoch: 1, train for the 84-th batch, train loss: 0.6367747187614441:  34%|████▍        | 83/241 [00:53<01:36,  1.63it/s]Epoch: 1, train for the 84-th batch, train loss: 0.6367747187614441:  35%|████▌        | 84/241 [00:53<01:36,  1.63it/s]Epoch: 4, train for the 132-th batch, train loss: 0.5322925448417664:  90%|█████████▊ | 131/146 [01:20<00:09,  1.50it/s]Epoch: 4, train for the 132-th batch, train loss: 0.5322925448417664:  90%|█████████▉ | 132/146 [01:20<00:09,  1.50it/s]Epoch: 5, train for the 63-th batch, train loss: 0.14479196071624756:  52%|██████▎     | 62/119 [00:40<00:37,  1.50it/s]Epoch: 5, train for the 63-th batch, train loss: 0.14479196071624756:  53%|██████▎     | 63/119 [00:40<00:37,  1.50it/s]Epoch: 2, train for the 191-th batch, train loss: 0.37561720609664917:  50%|████▉     | 190/383 [01:51<01:52,  1.71it/s]Epoch: 2, train for the 191-th batch, train loss: 0.37561720609664917:  50%|████▉     | 191/383 [01:51<01:52,  1.71it/s]Epoch: 8, train for the 134-th batch, train loss: 0.5708169341087341:  88%|█████████▋ | 133/151 [01:01<00:11,  1.63it/s]Epoch: 8, train for the 134-th batch, train loss: 0.5708169341087341:  89%|█████████▊ | 134/151 [01:01<00:10,  1.63it/s]Epoch: 3, train for the 101-th batch, train loss: 0.6334644556045532:  42%|████▋      | 100/237 [00:56<01:20,  1.71it/s]Epoch: 3, train for the 101-th batch, train loss: 0.6334644556045532:  43%|████▋      | 101/237 [00:56<01:19,  1.71it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5612983107566833:  35%|████▌        | 84/241 [00:54<01:36,  1.63it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5612983107566833:  35%|████▌        | 85/241 [00:54<01:35,  1.63it/s]Epoch: 4, train for the 133-th batch, train loss: 0.521559476852417:  90%|██████████▊ | 132/146 [01:21<00:09,  1.50it/s]Epoch: 5, train for the 64-th batch, train loss: 0.15042047202587128:  53%|██████▎     | 63/119 [00:41<00:37,  1.50it/s]Epoch: 4, train for the 133-th batch, train loss: 0.521559476852417:  91%|██████████▉ | 133/146 [01:21<00:08,  1.50it/s]Epoch: 5, train for the 64-th batch, train loss: 0.15042047202587128:  54%|██████▍     | 64/119 [00:41<00:36,  1.50it/s]Epoch: 2, train for the 192-th batch, train loss: 0.47250694036483765:  50%|████▉     | 191/383 [01:52<01:52,  1.71it/s]Epoch: 2, train for the 192-th batch, train loss: 0.47250694036483765:  50%|█████     | 192/383 [01:52<01:51,  1.71it/s]Epoch: 8, train for the 135-th batch, train loss: 0.5129796266555786:  89%|█████████▊ | 134/151 [01:02<00:10,  1.63it/s]Epoch: 8, train for the 135-th batch, train loss: 0.5129796266555786:  89%|█████████▊ | 135/151 [01:02<00:09,  1.63it/s]Epoch: 3, train for the 102-th batch, train loss: 0.6517015099525452:  43%|████▋      | 101/237 [00:57<01:19,  1.71it/s]Epoch: 3, train for the 102-th batch, train loss: 0.6517015099525452:  43%|████▋      | 102/237 [00:57<01:19,  1.71it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5065774321556091:  35%|████▌        | 85/241 [00:54<01:35,  1.63it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5065774321556091:  36%|████▋        | 86/241 [00:54<01:34,  1.63it/s]Epoch: 2, train for the 193-th batch, train loss: 0.41436567902565:  50%|██████▌      | 192/383 [01:53<01:51,  1.71it/s]Epoch: 2, train for the 193-th batch, train loss: 0.41436567902565:  50%|██████▌      | 193/383 [01:53<01:51,  1.70it/s]Epoch: 5, train for the 65-th batch, train loss: 0.16760554909706116:  54%|██████▍     | 64/119 [00:41<00:36,  1.50it/s]Epoch: 4, train for the 134-th batch, train loss: 0.5096166133880615:  91%|██████████ | 133/146 [01:22<00:08,  1.50it/s]Epoch: 5, train for the 65-th batch, train loss: 0.16760554909706116:  55%|██████▌     | 65/119 [00:41<00:35,  1.50it/s]Epoch: 4, train for the 134-th batch, train loss: 0.5096166133880615:  92%|██████████ | 134/146 [01:22<00:07,  1.50it/s]Epoch: 8, train for the 136-th batch, train loss: 0.5884920954704285:  89%|█████████▊ | 135/151 [01:03<00:09,  1.63it/s]Epoch: 8, train for the 136-th batch, train loss: 0.5884920954704285:  90%|█████████▉ | 136/151 [01:03<00:09,  1.63it/s]Epoch: 3, train for the 103-th batch, train loss: 0.6014562845230103:  43%|████▋      | 102/237 [00:57<01:19,  1.71it/s]Epoch: 3, train for the 103-th batch, train loss: 0.6014562845230103:  43%|████▊      | 103/237 [00:57<01:18,  1.71it/s]Epoch: 1, train for the 87-th batch, train loss: 0.554434597492218:  36%|████▉         | 86/241 [00:55<01:34,  1.63it/s]Epoch: 1, train for the 87-th batch, train loss: 0.554434597492218:  36%|█████         | 87/241 [00:55<01:34,  1.63it/s]Epoch: 2, train for the 194-th batch, train loss: 0.3644583523273468:  50%|█████▌     | 193/383 [01:53<01:51,  1.70it/s]Epoch: 2, train for the 194-th batch, train loss: 0.3644583523273468:  51%|█████▌     | 194/383 [01:53<01:50,  1.71it/s]Epoch: 4, train for the 135-th batch, train loss: 0.535149872303009:  92%|███████████ | 134/146 [01:22<00:07,  1.50it/s]Epoch: 4, train for the 135-th batch, train loss: 0.535149872303009:  92%|███████████ | 135/146 [01:22<00:07,  1.50it/s]Epoch: 5, train for the 66-th batch, train loss: 0.20358459651470184:  55%|██████▌     | 65/119 [00:42<00:35,  1.50it/s]Epoch: 5, train for the 66-th batch, train loss: 0.20358459651470184:  55%|██████▋     | 66/119 [00:42<00:35,  1.50it/s]Epoch: 3, train for the 104-th batch, train loss: 0.6196738481521606:  43%|████▊      | 103/237 [00:58<01:18,  1.71it/s]Epoch: 3, train for the 104-th batch, train loss: 0.6196738481521606:  44%|████▊      | 104/237 [00:58<01:17,  1.71it/s]Epoch: 8, train for the 137-th batch, train loss: 0.6073558330535889:  90%|█████████▉ | 136/151 [01:03<00:09,  1.63it/s]Epoch: 8, train for the 137-th batch, train loss: 0.6073558330535889:  91%|█████████▉ | 137/151 [01:03<00:08,  1.63it/s]Epoch: 1, train for the 88-th batch, train loss: 0.5462254881858826:  36%|████▋        | 87/241 [00:55<01:34,  1.63it/s]Epoch: 1, train for the 88-th batch, train loss: 0.5462254881858826:  37%|████▋        | 88/241 [00:55<01:33,  1.63it/s]Epoch: 2, train for the 195-th batch, train loss: 0.47202685475349426:  51%|█████     | 194/383 [01:54<01:50,  1.71it/s]Epoch: 2, train for the 195-th batch, train loss: 0.47202685475349426:  51%|█████     | 195/383 [01:54<01:50,  1.71it/s]Epoch: 8, train for the 138-th batch, train loss: 0.6129162311553955:  91%|█████████▉ | 137/151 [01:04<00:08,  1.63it/s]Epoch: 8, train for the 138-th batch, train loss: 0.6129162311553955:  91%|██████████ | 138/151 [01:04<00:07,  1.79it/s]Epoch: 4, train for the 136-th batch, train loss: 0.5208323001861572:  92%|██████████▏| 135/146 [01:23<00:07,  1.50it/s]Epoch: 5, train for the 67-th batch, train loss: 0.24374908208847046:  55%|██████▋     | 66/119 [00:43<00:35,  1.50it/s]Epoch: 4, train for the 136-th batch, train loss: 0.5208323001861572:  93%|██████████▏| 136/146 [01:23<00:06,  1.51it/s]Epoch: 5, train for the 67-th batch, train loss: 0.24374908208847046:  56%|██████▊     | 67/119 [00:43<00:34,  1.51it/s]Epoch: 3, train for the 105-th batch, train loss: 0.6618975400924683:  44%|████▊      | 104/237 [00:59<01:17,  1.71it/s]Epoch: 3, train for the 105-th batch, train loss: 0.6618975400924683:  44%|████▊      | 105/237 [00:59<01:17,  1.71it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5794627070426941:  37%|████▋        | 88/241 [00:56<01:33,  1.63it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5794627070426941:  37%|████▊        | 89/241 [00:56<01:36,  1.57it/s]Epoch: 2, train for the 196-th batch, train loss: 0.5170430541038513:  51%|█████▌     | 195/383 [01:54<01:50,  1.71it/s]Epoch: 2, train for the 196-th batch, train loss: 0.5170430541038513:  51%|█████▋     | 196/383 [01:54<01:49,  1.71it/s]Epoch: 8, train for the 139-th batch, train loss: 0.6065512895584106:  91%|██████████ | 138/151 [01:04<00:07,  1.79it/s]Epoch: 8, train for the 139-th batch, train loss: 0.6065512895584106:  92%|██████████▏| 139/151 [01:04<00:06,  1.78it/s]Epoch: 4, train for the 137-th batch, train loss: 0.535651445388794:  93%|███████████▏| 136/146 [01:24<00:06,  1.51it/s]Epoch: 5, train for the 68-th batch, train loss: 0.15751370787620544:  56%|██████▊     | 67/119 [00:43<00:34,  1.51it/s]Epoch: 4, train for the 137-th batch, train loss: 0.535651445388794:  94%|███████████▎| 137/146 [01:24<00:05,  1.50it/s]Epoch: 5, train for the 68-th batch, train loss: 0.15751370787620544:  57%|██████▊     | 68/119 [00:43<00:33,  1.50it/s]Epoch: 3, train for the 106-th batch, train loss: 0.6664541363716125:  44%|████▊      | 105/237 [00:59<01:17,  1.71it/s]Epoch: 3, train for the 106-th batch, train loss: 0.6664541363716125:  45%|████▉      | 106/237 [00:59<01:16,  1.70it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5313125848770142:  37%|████▊        | 89/241 [00:57<01:36,  1.57it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5313125848770142:  37%|████▊        | 90/241 [00:57<01:35,  1.59it/s]Epoch: 2, train for the 197-th batch, train loss: 0.46923545002937317:  51%|█████     | 196/383 [01:55<01:49,  1.71it/s]Epoch: 2, train for the 197-th batch, train loss: 0.46923545002937317:  51%|█████▏    | 197/383 [01:55<01:48,  1.71it/s]Epoch: 8, train for the 140-th batch, train loss: 0.5220893621444702:  92%|██████████▏| 139/151 [01:05<00:06,  1.78it/s]Epoch: 8, train for the 140-th batch, train loss: 0.5220893621444702:  93%|██████████▏| 140/151 [01:05<00:06,  1.73it/s]Epoch: 3, train for the 107-th batch, train loss: 0.6096516847610474:  45%|████▉      | 106/237 [01:00<01:16,  1.70it/s]Epoch: 3, train for the 107-th batch, train loss: 0.6096516847610474:  45%|████▉      | 107/237 [01:00<01:16,  1.71it/s]Epoch: 4, train for the 138-th batch, train loss: 0.5746189951896667:  94%|██████████▎| 137/146 [01:24<00:05,  1.50it/s]Epoch: 4, train for the 138-th batch, train loss: 0.5746189951896667:  95%|██████████▍| 138/146 [01:24<00:05,  1.50it/s]Epoch: 5, train for the 69-th batch, train loss: 0.14706313610076904:  57%|██████▊     | 68/119 [00:44<00:33,  1.50it/s]Epoch: 5, train for the 69-th batch, train loss: 0.14706313610076904:  58%|██████▉     | 69/119 [00:44<00:33,  1.50it/s]Epoch: 2, train for the 198-th batch, train loss: 0.4134746491909027:  51%|█████▋     | 197/383 [01:56<01:48,  1.71it/s]Epoch: 2, train for the 198-th batch, train loss: 0.4134746491909027:  52%|█████▋     | 198/383 [01:56<01:48,  1.71it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5268257856369019:  37%|████▊        | 90/241 [00:57<01:35,  1.59it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5268257856369019:  38%|████▉        | 91/241 [00:57<01:33,  1.60it/s]Epoch: 8, train for the 141-th batch, train loss: 0.5512693524360657:  93%|██████████▏| 140/151 [01:05<00:06,  1.73it/s]Epoch: 8, train for the 141-th batch, train loss: 0.5512693524360657:  93%|██████████▎| 141/151 [01:05<00:05,  1.69it/s]Epoch: 3, train for the 108-th batch, train loss: 0.6370031833648682:  45%|████▉      | 107/237 [01:00<01:16,  1.71it/s]Epoch: 3, train for the 108-th batch, train loss: 0.6370031833648682:  46%|█████      | 108/237 [01:00<01:15,  1.70it/s]Epoch: 4, train for the 139-th batch, train loss: 0.555227518081665:  95%|███████████▎| 138/146 [01:25<00:05,  1.50it/s]Epoch: 4, train for the 139-th batch, train loss: 0.555227518081665:  95%|███████████▍| 139/146 [01:25<00:04,  1.51it/s]Epoch: 5, train for the 70-th batch, train loss: 0.14019744098186493:  58%|██████▉     | 69/119 [00:45<00:33,  1.50it/s]Epoch: 5, train for the 70-th batch, train loss: 0.14019744098186493:  59%|███████     | 70/119 [00:45<00:32,  1.51it/s]Epoch: 2, train for the 199-th batch, train loss: 0.3951822817325592:  52%|█████▋     | 198/383 [01:56<01:48,  1.71it/s]Epoch: 2, train for the 199-th batch, train loss: 0.3951822817325592:  52%|█████▋     | 199/383 [01:56<01:47,  1.71it/s]Epoch: 1, train for the 92-th batch, train loss: 0.441628098487854:  38%|█████▎        | 91/241 [00:58<01:33,  1.60it/s]Epoch: 1, train for the 92-th batch, train loss: 0.441628098487854:  38%|█████▎        | 92/241 [00:58<01:32,  1.61it/s]Epoch: 8, train for the 142-th batch, train loss: 0.5637151002883911:  93%|██████████▎| 141/151 [01:06<00:05,  1.69it/s]Epoch: 8, train for the 142-th batch, train loss: 0.5637151002883911:  94%|██████████▎| 142/151 [01:06<00:05,  1.68it/s]Epoch: 3, train for the 109-th batch, train loss: 0.6553930640220642:  46%|█████      | 108/237 [01:01<01:15,  1.70it/s]Epoch: 3, train for the 109-th batch, train loss: 0.6553930640220642:  46%|█████      | 109/237 [01:01<01:14,  1.71it/s]Epoch: 4, train for the 140-th batch, train loss: 0.5331084132194519:  95%|██████████▍| 139/146 [01:26<00:04,  1.51it/s]Epoch: 5, train for the 71-th batch, train loss: 0.19693033397197723:  59%|███████     | 70/119 [00:45<00:32,  1.51it/s]Epoch: 4, train for the 140-th batch, train loss: 0.5331084132194519:  96%|██████████▌| 140/146 [01:26<00:03,  1.51it/s]Epoch: 5, train for the 71-th batch, train loss: 0.19693033397197723:  60%|███████▏    | 71/119 [00:45<00:31,  1.51it/s]Epoch: 2, train for the 200-th batch, train loss: 0.427645742893219:  52%|██████▏     | 199/383 [01:57<01:47,  1.71it/s]Epoch: 2, train for the 200-th batch, train loss: 0.427645742893219:  52%|██████▎     | 200/383 [01:57<01:47,  1.71it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5609142780303955:  38%|████▉        | 92/241 [00:59<01:32,  1.61it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5609142780303955:  39%|█████        | 93/241 [00:59<01:31,  1.62it/s]Epoch: 3, train for the 110-th batch, train loss: 0.6424595713615417:  46%|█████      | 109/237 [01:02<01:14,  1.71it/s]Epoch: 3, train for the 110-th batch, train loss: 0.6424595713615417:  46%|█████      | 110/237 [01:02<01:14,  1.71it/s]Epoch: 1, train for the 94-th batch, train loss: 0.42767301201820374:  39%|████▋       | 93/241 [00:59<01:31,  1.62it/s]Epoch: 1, train for the 94-th batch, train loss: 0.42767301201820374:  39%|████▋       | 94/241 [00:59<01:18,  1.88it/s]Epoch: 8, train for the 143-th batch, train loss: 0.48857975006103516:  94%|█████████▍| 142/151 [01:07<00:05,  1.68it/s]Epoch: 8, train for the 143-th batch, train loss: 0.48857975006103516:  95%|█████████▍| 143/151 [01:07<00:05,  1.49it/s]Epoch: 2, train for the 201-th batch, train loss: 0.4814520478248596:  52%|█████▋     | 200/383 [01:57<01:47,  1.71it/s]Epoch: 2, train for the 201-th batch, train loss: 0.4814520478248596:  52%|█████▊     | 201/383 [01:57<01:46,  1.71it/s]Epoch: 4, train for the 141-th batch, train loss: 0.5463907122612:  96%|█████████████▍| 140/146 [01:26<00:03,  1.51it/s]Epoch: 5, train for the 72-th batch, train loss: 0.19325891137123108:  60%|███████▏    | 71/119 [00:46<00:31,  1.51it/s]Epoch: 4, train for the 141-th batch, train loss: 0.5463907122612:  97%|█████████████▌| 141/146 [01:26<00:03,  1.51it/s]Epoch: 5, train for the 72-th batch, train loss: 0.19325891137123108:  61%|███████▎    | 72/119 [00:46<00:31,  1.51it/s]Epoch: 3, train for the 111-th batch, train loss: 0.641444981098175:  46%|█████▌      | 110/237 [01:02<01:14,  1.71it/s]Epoch: 3, train for the 111-th batch, train loss: 0.641444981098175:  47%|█████▌      | 111/237 [01:02<01:13,  1.71it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5006558299064636:  39%|█████        | 94/241 [01:00<01:18,  1.88it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5006558299064636:  39%|█████        | 95/241 [01:00<01:21,  1.80it/s]Epoch: 8, train for the 144-th batch, train loss: 0.4965365529060364:  95%|██████████▍| 143/151 [01:07<00:05,  1.49it/s]Epoch: 8, train for the 144-th batch, train loss: 0.4965365529060364:  95%|██████████▍| 144/151 [01:07<00:04,  1.54it/s]Epoch: 2, train for the 202-th batch, train loss: 0.5401307344436646:  52%|█████▊     | 201/383 [01:58<01:46,  1.71it/s]Epoch: 2, train for the 202-th batch, train loss: 0.5401307344436646:  53%|█████▊     | 202/383 [01:58<01:45,  1.71it/s]Epoch: 4, train for the 142-th batch, train loss: 0.49658095836639404:  97%|█████████▋| 141/146 [01:27<00:03,  1.51it/s]Epoch: 4, train for the 142-th batch, train loss: 0.49658095836639404:  97%|█████████▋| 142/146 [01:27<00:02,  1.51it/s]Epoch: 5, train for the 73-th batch, train loss: 0.16678905487060547:  61%|███████▎    | 72/119 [00:47<00:31,  1.51it/s]Epoch: 5, train for the 73-th batch, train loss: 0.16678905487060547:  61%|███████▎    | 73/119 [00:47<00:30,  1.51it/s]Epoch: 3, train for the 112-th batch, train loss: 0.6444902420043945:  47%|█████▏     | 111/237 [01:03<01:13,  1.71it/s]Epoch: 3, train for the 112-th batch, train loss: 0.6444902420043945:  47%|█████▏     | 112/237 [01:03<01:13,  1.71it/s]Epoch: 1, train for the 96-th batch, train loss: 0.639660120010376:  39%|█████▌        | 95/241 [01:00<01:21,  1.80it/s]Epoch: 1, train for the 96-th batch, train loss: 0.639660120010376:  40%|█████▌        | 96/241 [01:00<01:23,  1.74it/s]Epoch: 8, train for the 145-th batch, train loss: 0.5222598910331726:  95%|██████████▍| 144/151 [01:08<00:04,  1.54it/s]Epoch: 8, train for the 145-th batch, train loss: 0.5222598910331726:  96%|██████████▌| 145/151 [01:08<00:03,  1.58it/s]Epoch: 2, train for the 203-th batch, train loss: 0.4791690707206726:  53%|█████▊     | 202/383 [01:58<01:45,  1.71it/s]Epoch: 2, train for the 203-th batch, train loss: 0.4791690707206726:  53%|█████▊     | 203/383 [01:58<01:45,  1.71it/s]Epoch: 4, train for the 143-th batch, train loss: 0.5379791259765625:  97%|██████████▋| 142/146 [01:28<00:02,  1.51it/s]Epoch: 4, train for the 143-th batch, train loss: 0.5379791259765625:  98%|██████████▊| 143/146 [01:28<00:01,  1.51it/s]Epoch: 5, train for the 74-th batch, train loss: 0.2085823267698288:  61%|███████▉     | 73/119 [00:47<00:30,  1.51it/s]Epoch: 5, train for the 74-th batch, train loss: 0.2085823267698288:  62%|████████     | 74/119 [00:47<00:29,  1.51it/s]Epoch: 3, train for the 113-th batch, train loss: 0.6581071615219116:  47%|█████▏     | 112/237 [01:03<01:13,  1.71it/s]Epoch: 3, train for the 113-th batch, train loss: 0.6581071615219116:  48%|█████▏     | 113/237 [01:03<01:12,  1.71it/s]Epoch: 1, train for the 97-th batch, train loss: 0.37206733226776123:  40%|████▊       | 96/241 [01:01<01:23,  1.74it/s]Epoch: 8, train for the 146-th batch, train loss: 0.5222513675689697:  96%|██████████▌| 145/151 [01:09<00:03,  1.58it/s]Epoch: 8, train for the 146-th batch, train loss: 0.5222513675689697:  97%|██████████▋| 146/151 [01:09<00:03,  1.57it/s]Epoch: 1, train for the 97-th batch, train loss: 0.37206733226776123:  40%|████▊       | 97/241 [01:01<01:26,  1.67it/s]Epoch: 2, train for the 204-th batch, train loss: 0.3664596378803253:  53%|█████▊     | 203/383 [01:59<01:45,  1.71it/s]Epoch: 2, train for the 204-th batch, train loss: 0.3664596378803253:  53%|█████▊     | 204/383 [01:59<01:44,  1.71it/s]Epoch: 4, train for the 144-th batch, train loss: 0.5266123414039612:  98%|██████████▊| 143/146 [01:28<00:01,  1.51it/s]Epoch: 5, train for the 75-th batch, train loss: 0.17223216593265533:  62%|███████▍    | 74/119 [00:48<00:29,  1.51it/s]Epoch: 4, train for the 144-th batch, train loss: 0.5266123414039612:  99%|██████████▊| 144/146 [01:28<00:01,  1.51it/s]Epoch: 5, train for the 75-th batch, train loss: 0.17223216593265533:  63%|███████▌    | 75/119 [00:48<00:29,  1.51it/s]Epoch: 3, train for the 114-th batch, train loss: 0.597571611404419:  48%|█████▋      | 113/237 [01:04<01:12,  1.71it/s]Epoch: 3, train for the 114-th batch, train loss: 0.597571611404419:  48%|█████▊      | 114/237 [01:04<01:11,  1.71it/s]Epoch: 2, train for the 205-th batch, train loss: 0.4155336022377014:  53%|█████▊     | 204/383 [02:00<01:44,  1.71it/s]Epoch: 2, train for the 205-th batch, train loss: 0.4155336022377014:  54%|█████▉     | 205/383 [02:00<01:43,  1.71it/s]Epoch: 8, train for the 147-th batch, train loss: 0.5791850090026855:  97%|██████████▋| 146/151 [01:09<00:03,  1.57it/s]Epoch: 8, train for the 147-th batch, train loss: 0.5791850090026855:  97%|██████████▋| 147/151 [01:09<00:02,  1.56it/s]Epoch: 1, train for the 98-th batch, train loss: 0.5529964566230774:  40%|█████▏       | 97/241 [01:01<01:26,  1.67it/s]Epoch: 1, train for the 98-th batch, train loss: 0.5529964566230774:  41%|█████▎       | 98/241 [01:01<01:28,  1.62it/s]Epoch: 3, train for the 115-th batch, train loss: 0.6250129342079163:  48%|█████▎     | 114/237 [01:04<01:11,  1.71it/s]Epoch: 3, train for the 115-th batch, train loss: 0.6250129342079163:  49%|█████▎     | 115/237 [01:04<01:11,  1.71it/s]Epoch: 4, train for the 145-th batch, train loss: 0.5359317660331726:  99%|██████████▊| 144/146 [01:29<00:01,  1.51it/s]Epoch: 5, train for the 76-th batch, train loss: 0.19762049615383148:  63%|███████▌    | 75/119 [00:49<00:29,  1.51it/s]Epoch: 4, train for the 145-th batch, train loss: 0.5359317660331726:  99%|██████████▉| 145/146 [01:29<00:00,  1.51it/s]Epoch: 5, train for the 76-th batch, train loss: 0.19762049615383148:  64%|███████▋    | 76/119 [00:49<00:28,  1.51it/s]Epoch: 2, train for the 206-th batch, train loss: 0.406412810087204:  54%|██████▍     | 205/383 [02:00<01:43,  1.71it/s]Epoch: 2, train for the 206-th batch, train loss: 0.406412810087204:  54%|██████▍     | 206/383 [02:00<01:43,  1.71it/s]Epoch: 8, train for the 148-th batch, train loss: 0.5789205431938171:  97%|██████████▋| 147/151 [01:10<00:02,  1.56it/s]Epoch: 1, train for the 99-th batch, train loss: 0.3874928653240204:  41%|█████▎       | 98/241 [01:02<01:28,  1.62it/s]Epoch: 8, train for the 148-th batch, train loss: 0.5789205431938171:  98%|██████████▊| 148/151 [01:10<00:01,  1.55it/s]Epoch: 1, train for the 99-th batch, train loss: 0.3874928653240204:  41%|█████▎       | 99/241 [01:02<01:29,  1.59it/s]Epoch: 4, train for the 146-th batch, train loss: 0.49093204736709595:  99%|█████████▉| 145/146 [01:29<00:00,  1.51it/s]Epoch: 4, train for the 146-th batch, train loss: 0.49093204736709595: 100%|██████████| 146/146 [01:29<00:00,  1.68it/s]Epoch: 4, train for the 146-th batch, train loss: 0.49093204736709595: 100%|██████████| 146/146 [01:29<00:00,  1.62it/s]
  0%|                                                                                            | 0/38 [00:00<?, ?it/s]Epoch: 3, train for the 116-th batch, train loss: 0.6383247375488281:  49%|█████▎     | 115/237 [01:05<01:11,  1.71it/s]Epoch: 3, train for the 116-th batch, train loss: 0.6383247375488281:  49%|█████▍     | 116/237 [01:05<01:10,  1.71it/s]Epoch: 5, train for the 77-th batch, train loss: 0.159076988697052:  64%|████████▉     | 76/119 [00:49<00:28,  1.51it/s]Epoch: 5, train for the 77-th batch, train loss: 0.159076988697052:  65%|█████████     | 77/119 [00:49<00:26,  1.56it/s]evaluate for the 1-th batch, evaluate loss: 0.489555686712265:   0%|                             | 0/38 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.489555686712265:   3%|▌                    | 1/38 [00:00<00:10,  3.43it/s]Epoch: 8, train for the 149-th batch, train loss: 0.5106534957885742:  98%|██████████▊| 148/151 [01:10<00:01,  1.55it/s]Epoch: 8, train for the 149-th batch, train loss: 0.5106534957885742:  99%|██████████▊| 149/151 [01:10<00:01,  1.74it/s]Epoch: 2, train for the 207-th batch, train loss: 0.4713514745235443:  54%|█████▉     | 206/383 [02:01<01:43,  1.71it/s]Epoch: 2, train for the 207-th batch, train loss: 0.4713514745235443:  54%|█████▉     | 207/383 [02:01<01:43,  1.71it/s]evaluate for the 2-th batch, evaluate loss: 0.5022109746932983:   3%|▌                   | 1/38 [00:00<00:10,  3.43it/s]evaluate for the 2-th batch, evaluate loss: 0.5022109746932983:   5%|█                   | 2/38 [00:00<00:10,  3.55it/s]Epoch: 3, train for the 117-th batch, train loss: 0.6217701435089111:  49%|█████▍     | 116/237 [01:06<01:10,  1.71it/s]Epoch: 3, train for the 117-th batch, train loss: 0.6217701435089111:  49%|█████▍     | 117/237 [01:06<01:10,  1.70it/s]Epoch: 8, train for the 150-th batch, train loss: 0.5116481781005859:  99%|██████████▊| 149/151 [01:11<00:01,  1.74it/s]Epoch: 8, train for the 150-th batch, train loss: 0.5116481781005859:  99%|██████████▉| 150/151 [01:11<00:00,  1.95it/s]Epoch: 5, train for the 78-th batch, train loss: 0.16753382980823517:  65%|███████▊    | 77/119 [00:50<00:26,  1.56it/s]Epoch: 5, train for the 78-th batch, train loss: 0.16753382980823517:  66%|███████▊    | 78/119 [00:50<00:25,  1.59it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5629984736442566:  41%|████▉       | 99/241 [01:03<01:29,  1.59it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5629984736442566:  41%|████▌      | 100/241 [01:03<01:41,  1.39it/s]evaluate for the 3-th batch, evaluate loss: 0.49660009145736694:   5%|█                  | 2/38 [00:00<00:10,  3.55it/s]evaluate for the 3-th batch, evaluate loss: 0.49660009145736694:   8%|█▌                 | 3/38 [00:00<00:10,  3.45it/s]Epoch: 8, train for the 151-th batch, train loss: 0.5949017405509949:  99%|██████████▉| 150/151 [01:11<00:00,  1.95it/s]Epoch: 8, train for the 151-th batch, train loss: 0.5949017405509949: 100%|███████████| 151/151 [01:11<00:00,  2.33it/s]Epoch: 8, train for the 151-th batch, train loss: 0.5949017405509949: 100%|███████████| 151/151 [01:11<00:00,  2.11it/s]
  0%|                                                                                            | 0/46 [00:00<?, ?it/s]Epoch: 2, train for the 208-th batch, train loss: 0.44753023982048035:  54%|█████▍    | 207/383 [02:01<01:43,  1.71it/s]Epoch: 2, train for the 208-th batch, train loss: 0.44753023982048035:  54%|█████▍    | 208/383 [02:01<01:42,  1.70it/s]evaluate for the 4-th batch, evaluate loss: 0.4842927157878876:   8%|█▌                  | 3/38 [00:01<00:10,  3.45it/s]evaluate for the 4-th batch, evaluate loss: 0.4842927157878876:  11%|██                  | 4/38 [00:01<00:09,  3.66it/s]evaluate for the 1-th batch, evaluate loss: 0.4890347421169281:   0%|                            | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.4890347421169281:   2%|▍                   | 1/46 [00:00<00:10,  4.21it/s]Epoch: 3, train for the 118-th batch, train loss: 0.5467219948768616:  49%|█████▍     | 117/237 [01:06<01:10,  1.70it/s]Epoch: 3, train for the 118-th batch, train loss: 0.5467219948768616:  50%|█████▍     | 118/237 [01:06<01:09,  1.71it/s]Epoch: 5, train for the 79-th batch, train loss: 0.18673931062221527:  66%|███████▊    | 78/119 [00:50<00:25,  1.59it/s]Epoch: 5, train for the 79-th batch, train loss: 0.18673931062221527:  66%|███████▉    | 79/119 [00:50<00:24,  1.62it/s]evaluate for the 5-th batch, evaluate loss: 0.511013925075531:  11%|██▏                  | 4/38 [00:01<00:09,  3.66it/s]evaluate for the 5-th batch, evaluate loss: 0.511013925075531:  13%|██▊                  | 5/38 [00:01<00:09,  3.51it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5257149934768677:  41%|████▌      | 100/241 [01:04<01:41,  1.39it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5257149934768677:  42%|████▌      | 101/241 [01:04<01:35,  1.46it/s]evaluate for the 2-th batch, evaluate loss: 0.5021215081214905:   2%|▍                   | 1/46 [00:00<00:10,  4.21it/s]evaluate for the 2-th batch, evaluate loss: 0.5021215081214905:   4%|▊                   | 2/46 [00:00<00:12,  3.56it/s]Epoch: 2, train for the 209-th batch, train loss: 0.42666611075401306:  54%|█████▍    | 208/383 [02:02<01:42,  1.70it/s]Epoch: 2, train for the 209-th batch, train loss: 0.42666611075401306:  55%|█████▍    | 209/383 [02:02<01:41,  1.71it/s]evaluate for the 6-th batch, evaluate loss: 0.49551311135292053:  13%|██▌                | 5/38 [00:01<00:09,  3.51it/s]evaluate for the 6-th batch, evaluate loss: 0.49551311135292053:  16%|███                | 6/38 [00:01<00:08,  3.72it/s]evaluate for the 3-th batch, evaluate loss: 0.4806458055973053:   4%|▊                   | 2/46 [00:00<00:12,  3.56it/s]evaluate for the 3-th batch, evaluate loss: 0.4806458055973053:   7%|█▎                  | 3/46 [00:00<00:11,  3.70it/s]Epoch: 3, train for the 119-th batch, train loss: 0.641638994216919:  50%|█████▉      | 118/237 [01:07<01:09,  1.71it/s]Epoch: 3, train for the 119-th batch, train loss: 0.641638994216919:  50%|██████      | 119/237 [01:07<01:09,  1.71it/s]Epoch: 5, train for the 80-th batch, train loss: 0.19048453867435455:  66%|███████▉    | 79/119 [00:51<00:24,  1.62it/s]Epoch: 5, train for the 80-th batch, train loss: 0.19048453867435455:  67%|████████    | 80/119 [00:51<00:24,  1.62it/s]evaluate for the 7-th batch, evaluate loss: 0.47102880477905273:  16%|███                | 6/38 [00:01<00:08,  3.72it/s]evaluate for the 7-th batch, evaluate loss: 0.47102880477905273:  18%|███▌               | 7/38 [00:01<00:08,  3.51it/s]Epoch: 1, train for the 102-th batch, train loss: 0.571843147277832:  42%|█████       | 101/241 [01:04<01:35,  1.46it/s]Epoch: 1, train for the 102-th batch, train loss: 0.571843147277832:  42%|█████       | 102/241 [01:04<01:31,  1.52it/s]evaluate for the 4-th batch, evaluate loss: 0.5081490874290466:   7%|█▎                  | 3/46 [00:01<00:11,  3.70it/s]evaluate for the 4-th batch, evaluate loss: 0.5081490874290466:   9%|█▋                  | 4/46 [00:01<00:12,  3.45it/s]evaluate for the 8-th batch, evaluate loss: 0.5482409000396729:  18%|███▋                | 7/38 [00:02<00:08,  3.51it/s]evaluate for the 8-th batch, evaluate loss: 0.5482409000396729:  21%|████▏               | 8/38 [00:02<00:07,  3.97it/s]Epoch: 2, train for the 210-th batch, train loss: 0.4575197994709015:  55%|██████     | 209/383 [02:03<01:41,  1.71it/s]Epoch: 2, train for the 210-th batch, train loss: 0.4575197994709015:  55%|██████     | 210/383 [02:03<01:41,  1.71it/s]evaluate for the 5-th batch, evaluate loss: 0.4804588854312897:   9%|█▋                  | 4/46 [00:01<00:12,  3.45it/s]evaluate for the 5-th batch, evaluate loss: 0.4804588854312897:  11%|██▏                 | 5/46 [00:01<00:11,  3.55it/s]evaluate for the 9-th batch, evaluate loss: 0.5165699124336243:  21%|████▏               | 8/38 [00:02<00:07,  3.97it/s]evaluate for the 9-th batch, evaluate loss: 0.5165699124336243:  24%|████▋               | 9/38 [00:02<00:06,  4.23it/s]Epoch: 3, train for the 120-th batch, train loss: 0.6214403510093689:  50%|█████▌     | 119/237 [01:07<01:09,  1.71it/s]Epoch: 3, train for the 120-th batch, train loss: 0.6214403510093689:  51%|█████▌     | 120/237 [01:07<01:08,  1.70it/s]evaluate for the 6-th batch, evaluate loss: 0.5490301251411438:  11%|██▏                 | 5/46 [00:01<00:11,  3.55it/s]evaluate for the 6-th batch, evaluate loss: 0.5490301251411438:  13%|██▌                 | 6/46 [00:01<00:11,  3.47it/s]Epoch: 1, train for the 103-th batch, train loss: 0.571624219417572:  42%|█████       | 102/241 [01:05<01:31,  1.52it/s]Epoch: 1, train for the 103-th batch, train loss: 0.571624219417572:  43%|█████▏      | 103/241 [01:05<01:28,  1.56it/s]Epoch: 5, train for the 81-th batch, train loss: 0.1839759349822998:  67%|████████▋    | 80/119 [00:52<00:24,  1.62it/s]Epoch: 5, train for the 81-th batch, train loss: 0.1839759349822998:  68%|████████▊    | 81/119 [00:52<00:24,  1.55it/s]evaluate for the 10-th batch, evaluate loss: 0.5260923504829407:  24%|████▌              | 9/38 [00:02<00:06,  4.23it/s]evaluate for the 10-th batch, evaluate loss: 0.5260923504829407:  26%|████▋             | 10/38 [00:02<00:07,  3.82it/s]Epoch: 2, train for the 211-th batch, train loss: 0.4203166961669922:  55%|██████     | 210/383 [02:03<01:41,  1.71it/s]Epoch: 2, train for the 211-th batch, train loss: 0.4203166961669922:  55%|██████     | 211/383 [02:03<01:40,  1.70it/s]evaluate for the 7-th batch, evaluate loss: 0.47188735008239746:  13%|██▍                | 6/46 [00:01<00:11,  3.47it/s]evaluate for the 7-th batch, evaluate loss: 0.47188735008239746:  15%|██▉                | 7/46 [00:01<00:10,  3.61it/s]evaluate for the 11-th batch, evaluate loss: 0.5110307931900024:  26%|████▋             | 10/38 [00:02<00:07,  3.82it/s]evaluate for the 11-th batch, evaluate loss: 0.5110307931900024:  29%|█████▏            | 11/38 [00:02<00:07,  3.78it/s]Epoch: 3, train for the 121-th batch, train loss: 0.6614465713500977:  51%|█████▌     | 120/237 [01:08<01:08,  1.70it/s]Epoch: 3, train for the 121-th batch, train loss: 0.6614465713500977:  51%|█████▌     | 121/237 [01:08<01:08,  1.70it/s]evaluate for the 8-th batch, evaluate loss: 0.5578949451446533:  15%|███                 | 7/46 [00:02<00:10,  3.61it/s]evaluate for the 8-th batch, evaluate loss: 0.5578949451446533:  17%|███▍                | 8/46 [00:02<00:10,  3.54it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5394853353500366:  43%|████▋      | 103/241 [01:05<01:28,  1.56it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5394853353500366:  43%|████▋      | 104/241 [01:05<01:25,  1.60it/s]evaluate for the 12-th batch, evaluate loss: 0.5545746684074402:  29%|█████▏            | 11/38 [00:03<00:07,  3.78it/s]evaluate for the 12-th batch, evaluate loss: 0.5545746684074402:  32%|█████▋            | 12/38 [00:03<00:07,  3.64it/s]Epoch: 5, train for the 82-th batch, train loss: 0.22667621076107025:  68%|████████▏   | 81/119 [00:52<00:24,  1.55it/s]Epoch: 5, train for the 82-th batch, train loss: 0.22667621076107025:  69%|████████▎   | 82/119 [00:52<00:23,  1.59it/s]Epoch: 2, train for the 212-th batch, train loss: 0.47108909487724304:  55%|█████▌    | 211/383 [02:04<01:40,  1.70it/s]Epoch: 2, train for the 212-th batch, train loss: 0.47108909487724304:  55%|█████▌    | 212/383 [02:04<01:40,  1.70it/s]evaluate for the 9-th batch, evaluate loss: 0.5233809947967529:  17%|███▍                | 8/46 [00:02<00:10,  3.54it/s]evaluate for the 9-th batch, evaluate loss: 0.5233809947967529:  20%|███▉                | 9/46 [00:02<00:10,  3.64it/s]evaluate for the 13-th batch, evaluate loss: 0.5608696937561035:  32%|█████▋            | 12/38 [00:03<00:07,  3.64it/s]evaluate for the 13-th batch, evaluate loss: 0.5608696937561035:  34%|██████▏           | 13/38 [00:03<00:06,  3.73it/s]Epoch: 3, train for the 122-th batch, train loss: 0.6192575693130493:  51%|█████▌     | 121/237 [01:09<01:08,  1.70it/s]Epoch: 3, train for the 122-th batch, train loss: 0.6192575693130493:  51%|█████▋     | 122/237 [01:09<01:07,  1.71it/s]evaluate for the 10-th batch, evaluate loss: 0.5361636281013489:  20%|███▋               | 9/46 [00:02<00:10,  3.64it/s]evaluate for the 10-th batch, evaluate loss: 0.5361636281013489:  22%|███▉              | 10/46 [00:02<00:10,  3.59it/s]evaluate for the 14-th batch, evaluate loss: 0.4925086796283722:  34%|██████▏           | 13/38 [00:03<00:06,  3.73it/s]evaluate for the 14-th batch, evaluate loss: 0.4925086796283722:  37%|██████▋           | 14/38 [00:03<00:06,  3.63it/s]Epoch: 1, train for the 105-th batch, train loss: 0.4691578447818756:  43%|████▋      | 104/241 [01:06<01:25,  1.60it/s]Epoch: 1, train for the 105-th batch, train loss: 0.4691578447818756:  44%|████▊      | 105/241 [01:06<01:24,  1.61it/s]Epoch: 5, train for the 83-th batch, train loss: 0.17379161715507507:  69%|████████▎   | 82/119 [00:53<00:23,  1.59it/s]Epoch: 5, train for the 83-th batch, train loss: 0.17379161715507507:  70%|████████▎   | 83/119 [00:53<00:22,  1.61it/s]Epoch: 2, train for the 213-th batch, train loss: 0.5837743282318115:  55%|██████     | 212/383 [02:04<01:40,  1.70it/s]Epoch: 2, train for the 213-th batch, train loss: 0.5837743282318115:  56%|██████     | 213/383 [02:04<01:39,  1.71it/s]evaluate for the 11-th batch, evaluate loss: 0.5219781398773193:  22%|███▉              | 10/46 [00:03<00:10,  3.59it/s]evaluate for the 11-th batch, evaluate loss: 0.5219781398773193:  24%|████▎             | 11/46 [00:03<00:09,  3.62it/s]evaluate for the 15-th batch, evaluate loss: 0.5130533576011658:  37%|██████▋           | 14/38 [00:04<00:06,  3.63it/s]evaluate for the 15-th batch, evaluate loss: 0.5130533576011658:  39%|███████           | 15/38 [00:04<00:06,  3.71it/s]Epoch: 3, train for the 123-th batch, train loss: 0.6163954734802246:  51%|█████▋     | 122/237 [01:09<01:07,  1.71it/s]Epoch: 3, train for the 123-th batch, train loss: 0.6163954734802246:  52%|█████▋     | 123/237 [01:09<01:06,  1.71it/s]evaluate for the 12-th batch, evaluate loss: 0.4731994867324829:  24%|████▎             | 11/46 [00:03<00:09,  3.62it/s]evaluate for the 12-th batch, evaluate loss: 0.4731994867324829:  26%|████▋             | 12/46 [00:03<00:09,  3.63it/s]evaluate for the 16-th batch, evaluate loss: 0.5324702858924866:  39%|███████           | 15/38 [00:04<00:06,  3.71it/s]evaluate for the 16-th batch, evaluate loss: 0.5324702858924866:  42%|███████▌          | 16/38 [00:04<00:06,  3.66it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5709447264671326:  44%|████▊      | 105/241 [01:07<01:24,  1.61it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5709447264671326:  44%|████▊      | 106/241 [01:07<01:23,  1.63it/s]Epoch: 5, train for the 84-th batch, train loss: 0.1577986180782318:  70%|█████████    | 83/119 [00:54<00:22,  1.61it/s]Epoch: 5, train for the 84-th batch, train loss: 0.1577986180782318:  71%|█████████▏   | 84/119 [00:54<00:21,  1.62it/s]Epoch: 2, train for the 214-th batch, train loss: 0.4197324216365814:  56%|██████     | 213/383 [02:05<01:39,  1.71it/s]Epoch: 2, train for the 214-th batch, train loss: 0.4197324216365814:  56%|██████▏    | 214/383 [02:05<01:39,  1.71it/s]evaluate for the 13-th batch, evaluate loss: 0.49619051814079285:  26%|████▍            | 12/46 [00:03<00:09,  3.63it/s]evaluate for the 13-th batch, evaluate loss: 0.49619051814079285:  28%|████▊            | 13/46 [00:03<00:09,  3.54it/s]evaluate for the 17-th batch, evaluate loss: 0.5000070333480835:  42%|███████▌          | 16/38 [00:04<00:06,  3.66it/s]evaluate for the 17-th batch, evaluate loss: 0.5000070333480835:  45%|████████          | 17/38 [00:04<00:05,  3.59it/s]Epoch: 3, train for the 124-th batch, train loss: 0.6086162328720093:  52%|█████▋     | 123/237 [01:10<01:06,  1.71it/s]Epoch: 3, train for the 124-th batch, train loss: 0.6086162328720093:  52%|█████▊     | 124/237 [01:10<01:06,  1.70it/s]evaluate for the 14-th batch, evaluate loss: 0.5879196524620056:  28%|█████             | 13/46 [00:03<00:09,  3.54it/s]evaluate for the 14-th batch, evaluate loss: 0.5879196524620056:  30%|█████▍            | 14/46 [00:03<00:08,  3.67it/s]evaluate for the 18-th batch, evaluate loss: 0.5403603911399841:  45%|████████          | 17/38 [00:04<00:05,  3.59it/s]evaluate for the 18-th batch, evaluate loss: 0.5403603911399841:  47%|████████▌         | 18/38 [00:04<00:05,  3.61it/s]Epoch: 1, train for the 107-th batch, train loss: 0.6047877669334412:  44%|████▊      | 106/241 [01:07<01:23,  1.63it/s]Epoch: 1, train for the 107-th batch, train loss: 0.6047877669334412:  44%|████▉      | 107/241 [01:07<01:21,  1.65it/s]Epoch: 5, train for the 85-th batch, train loss: 0.18433338403701782:  71%|████████▍   | 84/119 [00:54<00:21,  1.62it/s]Epoch: 5, train for the 85-th batch, train loss: 0.18433338403701782:  71%|████████▌   | 85/119 [00:54<00:20,  1.63it/s]Epoch: 2, train for the 215-th batch, train loss: 0.4248307943344116:  56%|██████▏    | 214/383 [02:06<01:39,  1.71it/s]Epoch: 2, train for the 215-th batch, train loss: 0.4248307943344116:  56%|██████▏    | 215/383 [02:06<01:38,  1.71it/s]evaluate for the 15-th batch, evaluate loss: 0.5422077775001526:  30%|█████▍            | 14/46 [00:04<00:08,  3.67it/s]evaluate for the 15-th batch, evaluate loss: 0.5422077775001526:  33%|█████▊            | 15/46 [00:04<00:08,  3.55it/s]evaluate for the 19-th batch, evaluate loss: 0.5081213116645813:  47%|████████▌         | 18/38 [00:05<00:05,  3.61it/s]evaluate for the 19-th batch, evaluate loss: 0.5081213116645813:  50%|█████████         | 19/38 [00:05<00:05,  3.52it/s]evaluate for the 16-th batch, evaluate loss: 0.5697125792503357:  33%|█████▊            | 15/46 [00:04<00:08,  3.55it/s]evaluate for the 16-th batch, evaluate loss: 0.5697125792503357:  35%|██████▎           | 16/46 [00:04<00:08,  3.73it/s]Epoch: 3, train for the 125-th batch, train loss: 0.6060100197792053:  52%|█████▊     | 124/237 [01:10<01:06,  1.70it/s]Epoch: 3, train for the 125-th batch, train loss: 0.6060100197792053:  53%|█████▊     | 125/237 [01:10<01:05,  1.71it/s]evaluate for the 20-th batch, evaluate loss: 0.49491363763809204:  50%|████████▌        | 19/38 [00:05<00:05,  3.52it/s]evaluate for the 20-th batch, evaluate loss: 0.49491363763809204:  53%|████████▉        | 20/38 [00:05<00:04,  3.66it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5180668234825134:  44%|████▉      | 107/241 [01:08<01:21,  1.65it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5180668234825134:  45%|████▉      | 108/241 [01:08<01:20,  1.65it/s]evaluate for the 17-th batch, evaluate loss: 0.4532533884048462:  35%|██████▎           | 16/46 [00:04<00:08,  3.73it/s]evaluate for the 17-th batch, evaluate loss: 0.4532533884048462:  37%|██████▋           | 17/46 [00:04<00:08,  3.55it/s]Epoch: 5, train for the 86-th batch, train loss: 0.19584427773952484:  71%|████████▌   | 85/119 [00:55<00:20,  1.63it/s]Epoch: 5, train for the 86-th batch, train loss: 0.19584427773952484:  72%|████████▋   | 86/119 [00:55<00:20,  1.64it/s]Epoch: 2, train for the 216-th batch, train loss: 0.46407201886177063:  56%|█████▌    | 215/383 [02:06<01:38,  1.71it/s]Epoch: 2, train for the 216-th batch, train loss: 0.46407201886177063:  56%|█████▋    | 216/383 [02:06<01:37,  1.71it/s]evaluate for the 21-th batch, evaluate loss: 0.510454535484314:  53%|██████████         | 20/38 [00:05<00:04,  3.66it/s]evaluate for the 21-th batch, evaluate loss: 0.510454535484314:  55%|██████████▌        | 21/38 [00:05<00:04,  3.54it/s]evaluate for the 18-th batch, evaluate loss: 0.49951866269111633:  37%|██████▎          | 17/46 [00:04<00:08,  3.55it/s]evaluate for the 18-th batch, evaluate loss: 0.49951866269111633:  39%|██████▋          | 18/46 [00:04<00:07,  3.65it/s]evaluate for the 22-th batch, evaluate loss: 0.5227952599525452:  55%|█████████▉        | 21/38 [00:05<00:04,  3.54it/s]evaluate for the 22-th batch, evaluate loss: 0.5227952599525452:  58%|██████████▍       | 22/38 [00:05<00:04,  3.71it/s]Epoch: 1, train for the 109-th batch, train loss: 0.7717353701591492:  45%|████▉      | 108/241 [01:08<01:20,  1.65it/s]Epoch: 1, train for the 109-th batch, train loss: 0.7717353701591492:  45%|████▉      | 109/241 [01:08<01:19,  1.65it/s]evaluate for the 19-th batch, evaluate loss: 0.5194593071937561:  39%|███████           | 18/46 [00:05<00:07,  3.65it/s]evaluate for the 19-th batch, evaluate loss: 0.5194593071937561:  41%|███████▍          | 19/46 [00:05<00:07,  3.47it/s]Epoch: 5, train for the 87-th batch, train loss: 0.20128265023231506:  72%|████████▋   | 86/119 [00:55<00:20,  1.64it/s]Epoch: 5, train for the 87-th batch, train loss: 0.20128265023231506:  73%|████████▊   | 87/119 [00:55<00:19,  1.64it/s]evaluate for the 23-th batch, evaluate loss: 0.5246317982673645:  58%|██████████▍       | 22/38 [00:06<00:04,  3.71it/s]evaluate for the 23-th batch, evaluate loss: 0.5246317982673645:  61%|██████████▉       | 23/38 [00:06<00:04,  3.52it/s]evaluate for the 20-th batch, evaluate loss: 0.5328651070594788:  41%|███████▍          | 19/46 [00:05<00:07,  3.47it/s]evaluate for the 20-th batch, evaluate loss: 0.5328651070594788:  43%|███████▊          | 20/46 [00:05<00:07,  3.53it/s]evaluate for the 24-th batch, evaluate loss: 0.5060593485832214:  61%|██████████▉       | 23/38 [00:06<00:04,  3.52it/s]evaluate for the 24-th batch, evaluate loss: 0.5060593485832214:  63%|███████████▎      | 24/38 [00:06<00:03,  3.56it/s]Epoch: 3, train for the 126-th batch, train loss: 0.6319434642791748:  53%|█████▊     | 125/237 [01:12<01:05,  1.71it/s]Epoch: 2, train for the 217-th batch, train loss: 0.4632090628147125:  56%|██████▏    | 216/383 [02:07<01:37,  1.71it/s]Epoch: 3, train for the 126-th batch, train loss: 0.6319434642791748:  53%|█████▊     | 126/237 [01:12<01:26,  1.29it/s]Epoch: 2, train for the 217-th batch, train loss: 0.4632090628147125:  57%|██████▏    | 217/383 [02:07<01:53,  1.46it/s]evaluate for the 21-th batch, evaluate loss: 0.5246065258979797:  43%|███████▊          | 20/46 [00:05<00:07,  3.53it/s]evaluate for the 21-th batch, evaluate loss: 0.5246065258979797:  46%|████████▏         | 21/46 [00:05<00:07,  3.48it/s]Epoch: 1, train for the 110-th batch, train loss: 0.6954033970832825:  45%|████▉      | 109/241 [01:09<01:19,  1.65it/s]Epoch: 1, train for the 110-th batch, train loss: 0.6954033970832825:  46%|█████      | 110/241 [01:09<01:18,  1.66it/s]evaluate for the 25-th batch, evaluate loss: 0.5124388337135315:  63%|███████████▎      | 24/38 [00:06<00:03,  3.56it/s]evaluate for the 25-th batch, evaluate loss: 0.5124388337135315:  66%|███████████▊      | 25/38 [00:06<00:03,  3.50it/s]Epoch: 5, train for the 88-th batch, train loss: 0.24471378326416016:  73%|████████▊   | 87/119 [00:56<00:19,  1.64it/s]Epoch: 5, train for the 88-th batch, train loss: 0.24471378326416016:  74%|████████▊   | 88/119 [00:56<00:18,  1.65it/s]evaluate for the 22-th batch, evaluate loss: 0.5206094980239868:  46%|████████▏         | 21/46 [00:06<00:07,  3.48it/s]evaluate for the 22-th batch, evaluate loss: 0.5206094980239868:  48%|████████▌         | 22/46 [00:06<00:06,  3.61it/s]evaluate for the 26-th batch, evaluate loss: 0.5175399780273438:  66%|███████████▊      | 25/38 [00:07<00:03,  3.50it/s]evaluate for the 26-th batch, evaluate loss: 0.5175399780273438:  68%|████████████▎     | 26/38 [00:07<00:03,  3.63it/s]Epoch: 3, train for the 127-th batch, train loss: 0.5760425329208374:  53%|█████▊     | 126/237 [01:12<01:26,  1.29it/s]Epoch: 3, train for the 127-th batch, train loss: 0.5760425329208374:  54%|█████▉     | 127/237 [01:12<01:20,  1.37it/s]Epoch: 2, train for the 218-th batch, train loss: 0.5008417963981628:  57%|██████▏    | 217/383 [02:08<01:53,  1.46it/s]Epoch: 2, train for the 218-th batch, train loss: 0.5008417963981628:  57%|██████▎    | 218/383 [02:08<01:51,  1.49it/s]evaluate for the 23-th batch, evaluate loss: 0.47403717041015625:  48%|████████▏        | 22/46 [00:06<00:06,  3.61it/s]evaluate for the 23-th batch, evaluate loss: 0.47403717041015625:  50%|████████▌        | 23/46 [00:06<00:06,  3.55it/s]evaluate for the 27-th batch, evaluate loss: 0.5356712341308594:  68%|████████████▎     | 26/38 [00:07<00:03,  3.63it/s]evaluate for the 27-th batch, evaluate loss: 0.5356712341308594:  71%|████████████▊     | 27/38 [00:07<00:03,  3.57it/s]Epoch: 1, train for the 111-th batch, train loss: 0.21176530420780182:  46%|████▌     | 110/241 [01:10<01:18,  1.66it/s]Epoch: 1, train for the 111-th batch, train loss: 0.21176530420780182:  46%|████▌     | 111/241 [01:10<01:17,  1.67it/s]Epoch: 5, train for the 89-th batch, train loss: 0.20737062394618988:  74%|████████▊   | 88/119 [00:57<00:18,  1.65it/s]Epoch: 5, train for the 89-th batch, train loss: 0.20737062394618988:  75%|████████▉   | 89/119 [00:57<00:18,  1.66it/s]evaluate for the 24-th batch, evaluate loss: 0.48207929730415344:  50%|████████▌        | 23/46 [00:06<00:06,  3.55it/s]evaluate for the 24-th batch, evaluate loss: 0.48207929730415344:  52%|████████▊        | 24/46 [00:06<00:06,  3.64it/s]evaluate for the 28-th batch, evaluate loss: 0.5463184714317322:  71%|████████████▊     | 27/38 [00:07<00:03,  3.57it/s]evaluate for the 28-th batch, evaluate loss: 0.5463184714317322:  74%|█████████████▎    | 28/38 [00:07<00:02,  3.66it/s]Epoch: 3, train for the 128-th batch, train loss: 0.5998785495758057:  54%|█████▉     | 127/237 [01:13<01:20,  1.37it/s]Epoch: 3, train for the 128-th batch, train loss: 0.5998785495758057:  54%|█████▉     | 128/237 [01:13<01:17,  1.41it/s]Epoch: 2, train for the 219-th batch, train loss: 0.36868348717689514:  57%|█████▋    | 218/383 [02:08<01:51,  1.49it/s]Epoch: 2, train for the 219-th batch, train loss: 0.36868348717689514:  57%|█████▋    | 219/383 [02:08<01:48,  1.51it/s]evaluate for the 25-th batch, evaluate loss: 0.5320939421653748:  52%|█████████▍        | 24/46 [00:06<00:06,  3.64it/s]evaluate for the 25-th batch, evaluate loss: 0.5320939421653748:  54%|█████████▊        | 25/46 [00:06<00:05,  3.59it/s]evaluate for the 29-th batch, evaluate loss: 0.5191283226013184:  74%|█████████████▎    | 28/38 [00:07<00:02,  3.66it/s]evaluate for the 29-th batch, evaluate loss: 0.5191283226013184:  76%|█████████████▋    | 29/38 [00:07<00:02,  3.62it/s]Epoch: 1, train for the 112-th batch, train loss: 0.2500731348991394:  46%|█████      | 111/241 [01:10<01:17,  1.67it/s]Epoch: 1, train for the 112-th batch, train loss: 0.2500731348991394:  46%|█████      | 112/241 [01:10<01:17,  1.67it/s]Epoch: 5, train for the 90-th batch, train loss: 0.2055395096540451:  75%|█████████▋   | 89/119 [00:57<00:18,  1.66it/s]Epoch: 5, train for the 90-th batch, train loss: 0.2055395096540451:  76%|█████████▊   | 90/119 [00:57<00:17,  1.65it/s]evaluate for the 26-th batch, evaluate loss: 0.5518975853919983:  54%|█████████▊        | 25/46 [00:07<00:05,  3.59it/s]evaluate for the 26-th batch, evaluate loss: 0.5518975853919983:  57%|██████████▏       | 26/46 [00:07<00:05,  3.61it/s]evaluate for the 30-th batch, evaluate loss: 0.5529048442840576:  76%|█████████████▋    | 29/38 [00:08<00:02,  3.62it/s]evaluate for the 30-th batch, evaluate loss: 0.5529048442840576:  79%|██████████████▏   | 30/38 [00:08<00:02,  3.57it/s]evaluate for the 27-th batch, evaluate loss: 0.49772346019744873:  57%|█████████▌       | 26/46 [00:07<00:05,  3.61it/s]evaluate for the 27-th batch, evaluate loss: 0.49772346019744873:  59%|█████████▉       | 27/46 [00:07<00:05,  3.63it/s]evaluate for the 31-th batch, evaluate loss: 0.5358881950378418:  79%|██████████████▏   | 30/38 [00:08<00:02,  3.57it/s]evaluate for the 31-th batch, evaluate loss: 0.5358881950378418:  82%|██████████████▋   | 31/38 [00:08<00:01,  3.60it/s]Epoch: 3, train for the 129-th batch, train loss: 0.6256887912750244:  54%|█████▉     | 128/237 [01:13<01:17,  1.41it/s]Epoch: 2, train for the 220-th batch, train loss: 0.42791998386383057:  57%|█████▋    | 219/383 [02:09<01:48,  1.51it/s]Epoch: 3, train for the 129-th batch, train loss: 0.6256887912750244:  54%|█████▉     | 129/237 [01:13<01:14,  1.45it/s]Epoch: 2, train for the 220-th batch, train loss: 0.42791998386383057:  57%|█████▋    | 220/383 [02:09<01:47,  1.51it/s]Epoch: 1, train for the 113-th batch, train loss: 0.24654695391654968:  46%|████▋     | 112/241 [01:11<01:17,  1.67it/s]Epoch: 1, train for the 113-th batch, train loss: 0.24654695391654968:  47%|████▋     | 113/241 [01:11<01:16,  1.66it/s]Epoch: 5, train for the 91-th batch, train loss: 0.2060241401195526:  76%|█████████▊   | 90/119 [00:58<00:17,  1.65it/s]Epoch: 5, train for the 91-th batch, train loss: 0.2060241401195526:  76%|█████████▉   | 91/119 [00:58<00:16,  1.65it/s]evaluate for the 28-th batch, evaluate loss: 0.5212500691413879:  59%|██████████▌       | 27/46 [00:07<00:05,  3.63it/s]evaluate for the 28-th batch, evaluate loss: 0.5212500691413879:  61%|██████████▉       | 28/46 [00:07<00:05,  3.54it/s]evaluate for the 32-th batch, evaluate loss: 0.4999290406703949:  82%|██████████████▋   | 31/38 [00:08<00:01,  3.60it/s]evaluate for the 32-th batch, evaluate loss: 0.4999290406703949:  84%|███████████████▏  | 32/38 [00:08<00:01,  3.52it/s]evaluate for the 29-th batch, evaluate loss: 0.49275991320610046:  61%|██████████▎      | 28/46 [00:08<00:05,  3.54it/s]evaluate for the 29-th batch, evaluate loss: 0.49275991320610046:  63%|██████████▋      | 29/46 [00:08<00:04,  3.67it/s]evaluate for the 33-th batch, evaluate loss: 0.5066108703613281:  84%|███████████████▏  | 32/38 [00:09<00:01,  3.52it/s]evaluate for the 33-th batch, evaluate loss: 0.5066108703613281:  87%|███████████████▋  | 33/38 [00:09<00:01,  3.67it/s]Epoch: 3, train for the 130-th batch, train loss: 0.6349023580551147:  54%|█████▉     | 129/237 [01:14<01:14,  1.45it/s]Epoch: 2, train for the 221-th batch, train loss: 0.45594462752342224:  57%|█████▋    | 220/383 [02:10<01:47,  1.51it/s]Epoch: 3, train for the 130-th batch, train loss: 0.6349023580551147:  55%|██████     | 130/237 [01:14<01:12,  1.47it/s]Epoch: 2, train for the 221-th batch, train loss: 0.45594462752342224:  58%|█████▊    | 221/383 [02:10<01:46,  1.52it/s]Epoch: 1, train for the 114-th batch, train loss: 0.2795918881893158:  47%|█████▏     | 113/241 [01:11<01:16,  1.66it/s]Epoch: 1, train for the 114-th batch, train loss: 0.2795918881893158:  47%|█████▏     | 114/241 [01:11<01:15,  1.68it/s]evaluate for the 30-th batch, evaluate loss: 0.49280261993408203:  63%|██████████▋      | 29/46 [00:08<00:04,  3.67it/s]evaluate for the 30-th batch, evaluate loss: 0.49280261993408203:  65%|███████████      | 30/46 [00:08<00:04,  3.55it/s]Epoch: 5, train for the 92-th batch, train loss: 0.22315114736557007:  76%|█████████▏  | 91/119 [00:58<00:16,  1.65it/s]Epoch: 5, train for the 92-th batch, train loss: 0.22315114736557007:  77%|█████████▎  | 92/119 [00:58<00:16,  1.65it/s]evaluate for the 34-th batch, evaluate loss: 0.5262246131896973:  87%|███████████████▋  | 33/38 [00:09<00:01,  3.67it/s]evaluate for the 34-th batch, evaluate loss: 0.5262246131896973:  89%|████████████████  | 34/38 [00:09<00:01,  3.55it/s]evaluate for the 31-th batch, evaluate loss: 0.5215449333190918:  65%|███████████▋      | 30/46 [00:08<00:04,  3.55it/s]evaluate for the 31-th batch, evaluate loss: 0.5215449333190918:  67%|████████████▏     | 31/46 [00:08<00:04,  3.73it/s]evaluate for the 35-th batch, evaluate loss: 0.5622265338897705:  89%|████████████████  | 34/38 [00:09<00:01,  3.55it/s]evaluate for the 35-th batch, evaluate loss: 0.5622265338897705:  92%|████████████████▌ | 35/38 [00:09<00:00,  3.69it/s]Epoch: 1, train for the 115-th batch, train loss: 0.48076996207237244:  47%|████▋     | 114/241 [01:12<01:15,  1.68it/s]Epoch: 1, train for the 115-th batch, train loss: 0.48076996207237244:  48%|████▊     | 115/241 [01:12<01:15,  1.67it/s]Epoch: 3, train for the 131-th batch, train loss: 0.6086617112159729:  55%|██████     | 130/237 [01:15<01:12,  1.47it/s]Epoch: 2, train for the 222-th batch, train loss: 0.4444864094257355:  58%|██████▎    | 221/383 [02:10<01:46,  1.52it/s]Epoch: 3, train for the 131-th batch, train loss: 0.6086617112159729:  55%|██████     | 131/237 [01:15<01:11,  1.49it/s]Epoch: 2, train for the 222-th batch, train loss: 0.4444864094257355:  58%|██████▍    | 222/383 [02:10<01:45,  1.52it/s]evaluate for the 32-th batch, evaluate loss: 0.4789360761642456:  67%|████████████▏     | 31/46 [00:08<00:04,  3.73it/s]evaluate for the 32-th batch, evaluate loss: 0.4789360761642456:  70%|████████████▌     | 32/46 [00:08<00:03,  3.56it/s]Epoch: 5, train for the 93-th batch, train loss: 0.1718876212835312:  77%|██████████   | 92/119 [00:59<00:16,  1.65it/s]Epoch: 5, train for the 93-th batch, train loss: 0.1718876212835312:  78%|██████████▏  | 93/119 [00:59<00:15,  1.65it/s]evaluate for the 36-th batch, evaluate loss: 0.5708332061767578:  92%|████████████████▌ | 35/38 [00:09<00:00,  3.69it/s]evaluate for the 36-th batch, evaluate loss: 0.5708332061767578:  95%|█████████████████ | 36/38 [00:09<00:00,  3.50it/s]evaluate for the 33-th batch, evaluate loss: 0.4982965886592865:  70%|████████████▌     | 32/46 [00:09<00:03,  3.56it/s]evaluate for the 33-th batch, evaluate loss: 0.4982965886592865:  72%|████████████▉     | 33/46 [00:09<00:03,  3.64it/s]evaluate for the 37-th batch, evaluate loss: 0.5128292441368103:  95%|█████████████████ | 36/38 [00:10<00:00,  3.50it/s]evaluate for the 37-th batch, evaluate loss: 0.5128292441368103:  97%|█████████████████▌| 37/38 [00:10<00:00,  3.56it/s]Epoch: 2, train for the 223-th batch, train loss: 0.38099759817123413:  58%|█████▊    | 222/383 [02:11<01:45,  1.52it/s]Epoch: 2, train for the 223-th batch, train loss: 0.38099759817123413:  58%|█████▊    | 223/383 [02:11<01:34,  1.69it/s]Epoch: 1, train for the 116-th batch, train loss: 0.5825105309486389:  48%|█████▏     | 115/241 [01:13<01:15,  1.67it/s]Epoch: 1, train for the 116-th batch, train loss: 0.5825105309486389:  48%|█████▎     | 116/241 [01:13<01:15,  1.67it/s]evaluate for the 34-th batch, evaluate loss: 0.4845978915691376:  72%|████████████▉     | 33/46 [00:09<00:03,  3.64it/s]evaluate for the 34-th batch, evaluate loss: 0.4845978915691376:  74%|█████████████▎    | 34/46 [00:09<00:03,  3.47it/s]evaluate for the 38-th batch, evaluate loss: 0.5160085558891296:  97%|█████████████████▌| 37/38 [00:10<00:00,  3.56it/s]evaluate for the 38-th batch, evaluate loss: 0.5160085558891296: 100%|██████████████████| 38/38 [00:10<00:00,  3.52it/s]evaluate for the 38-th batch, evaluate loss: 0.5160085558891296: 100%|██████████████████| 38/38 [00:10<00:00,  3.62it/s]
  0%|                                                                                            | 0/20 [00:00<?, ?it/s]Epoch: 3, train for the 132-th batch, train loss: 0.5889010429382324:  55%|██████     | 131/237 [01:15<01:11,  1.49it/s]Epoch: 3, train for the 132-th batch, train loss: 0.5889010429382324:  56%|██████▏    | 132/237 [01:15<01:09,  1.51it/s]Epoch: 5, train for the 94-th batch, train loss: 0.18609946966171265:  78%|█████████▍  | 93/119 [01:00<00:15,  1.65it/s]Epoch: 5, train for the 94-th batch, train loss: 0.18609946966171265:  79%|█████████▍  | 94/119 [01:00<00:15,  1.66it/s]evaluate for the 35-th batch, evaluate loss: 0.4843935966491699:  74%|█████████████▎    | 34/46 [00:09<00:03,  3.47it/s]evaluate for the 35-th batch, evaluate loss: 0.4843935966491699:  76%|█████████████▋    | 35/46 [00:09<00:03,  3.52it/s]evaluate for the 1-th batch, evaluate loss: 0.5675995349884033:   0%|                            | 0/20 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.5675995349884033:   5%|█                   | 1/20 [00:00<00:04,  3.82it/s]evaluate for the 36-th batch, evaluate loss: 0.46998998522758484:  76%|████████████▉    | 35/46 [00:10<00:03,  3.52it/s]evaluate for the 36-th batch, evaluate loss: 0.46998998522758484:  78%|█████████████▎   | 36/46 [00:10<00:02,  3.47it/s]Epoch: 1, train for the 117-th batch, train loss: 0.6319771409034729:  48%|█████▎     | 116/241 [01:13<01:15,  1.67it/s]Epoch: 1, train for the 117-th batch, train loss: 0.6319771409034729:  49%|█████▎     | 117/241 [01:13<01:14,  1.67it/s]Epoch: 3, train for the 133-th batch, train loss: 0.6124032735824585:  56%|██████▏    | 132/237 [01:16<01:09,  1.51it/s]Epoch: 3, train for the 133-th batch, train loss: 0.6124032735824585:  56%|██████▏    | 133/237 [01:16<01:05,  1.60it/s]Epoch: 2, train for the 224-th batch, train loss: 0.5075553059577942:  58%|██████▍    | 223/383 [02:11<01:34,  1.69it/s]Epoch: 2, train for the 224-th batch, train loss: 0.5075553059577942:  58%|██████▍    | 224/383 [02:11<01:41,  1.57it/s]evaluate for the 2-th batch, evaluate loss: 0.6206934452056885:   5%|█                   | 1/20 [00:00<00:04,  3.82it/s]evaluate for the 2-th batch, evaluate loss: 0.6206934452056885:  10%|██                  | 2/20 [00:00<00:05,  3.51it/s]Epoch: 5, train for the 95-th batch, train loss: 0.16647720336914062:  79%|█████████▍  | 94/119 [01:00<00:15,  1.66it/s]Epoch: 5, train for the 95-th batch, train loss: 0.16647720336914062:  80%|█████████▌  | 95/119 [01:00<00:14,  1.68it/s]evaluate for the 37-th batch, evaluate loss: 0.5067912340164185:  78%|██████████████    | 36/46 [00:10<00:02,  3.47it/s]evaluate for the 37-th batch, evaluate loss: 0.5067912340164185:  80%|██████████████▍   | 37/46 [00:10<00:02,  3.60it/s]evaluate for the 3-th batch, evaluate loss: 0.5945709347724915:  10%|██                  | 2/20 [00:00<00:05,  3.51it/s]evaluate for the 3-th batch, evaluate loss: 0.5945709347724915:  15%|███                 | 3/20 [00:00<00:04,  3.64it/s]evaluate for the 38-th batch, evaluate loss: 0.5365462899208069:  80%|██████████████▍   | 37/46 [00:10<00:02,  3.60it/s]evaluate for the 38-th batch, evaluate loss: 0.5365462899208069:  83%|██████████████▊   | 38/46 [00:10<00:02,  3.53it/s]Epoch: 1, train for the 118-th batch, train loss: 0.5276384353637695:  49%|█████▎     | 117/241 [01:14<01:14,  1.67it/s]Epoch: 1, train for the 118-th batch, train loss: 0.5276384353637695:  49%|█████▍     | 118/241 [01:14<01:13,  1.68it/s]evaluate for the 4-th batch, evaluate loss: 0.5691437125205994:  15%|███                 | 3/20 [00:01<00:04,  3.64it/s]evaluate for the 4-th batch, evaluate loss: 0.5691437125205994:  20%|████                | 4/20 [00:01<00:04,  3.51it/s]Epoch: 5, train for the 96-th batch, train loss: 0.17534632980823517:  80%|█████████▌  | 95/119 [01:01<00:14,  1.68it/s]Epoch: 5, train for the 96-th batch, train loss: 0.17534632980823517:  81%|█████████▋  | 96/119 [01:01<00:13,  1.70it/s]Epoch: 3, train for the 134-th batch, train loss: 0.64186692237854:  56%|███████▎     | 133/237 [01:17<01:05,  1.60it/s]Epoch: 3, train for the 134-th batch, train loss: 0.64186692237854:  57%|███████▎     | 134/237 [01:17<01:05,  1.58it/s]Epoch: 2, train for the 225-th batch, train loss: 0.4447212219238281:  58%|██████▍    | 224/383 [02:12<01:41,  1.57it/s]Epoch: 2, train for the 225-th batch, train loss: 0.4447212219238281:  59%|██████▍    | 225/383 [02:12<01:41,  1.56it/s]evaluate for the 39-th batch, evaluate loss: 0.5332440137863159:  83%|██████████████▊   | 38/46 [00:10<00:02,  3.53it/s]evaluate for the 39-th batch, evaluate loss: 0.5332440137863159:  85%|███████████████▎  | 39/46 [00:10<00:01,  3.63it/s]evaluate for the 5-th batch, evaluate loss: 0.5947696566581726:  20%|████                | 4/20 [00:01<00:04,  3.51it/s]evaluate for the 5-th batch, evaluate loss: 0.5947696566581726:  25%|█████               | 5/20 [00:01<00:04,  3.62it/s]evaluate for the 40-th batch, evaluate loss: 0.469919353723526:  85%|████████████████   | 39/46 [00:11<00:01,  3.63it/s]evaluate for the 40-th batch, evaluate loss: 0.469919353723526:  87%|████████████████▌  | 40/46 [00:11<00:01,  3.58it/s]evaluate for the 6-th batch, evaluate loss: 0.6067410111427307:  25%|█████               | 5/20 [00:01<00:04,  3.62it/s]evaluate for the 6-th batch, evaluate loss: 0.6067410111427307:  30%|██████              | 6/20 [00:01<00:03,  3.52it/s]Epoch: 1, train for the 119-th batch, train loss: 0.3792038559913635:  49%|█████▍     | 118/241 [01:14<01:13,  1.68it/s]Epoch: 1, train for the 119-th batch, train loss: 0.3792038559913635:  49%|█████▍     | 119/241 [01:14<01:12,  1.67it/s]Epoch: 5, train for the 97-th batch, train loss: 0.17774111032485962:  81%|█████████▋  | 96/119 [01:01<00:13,  1.70it/s]Epoch: 5, train for the 97-th batch, train loss: 0.17774111032485962:  82%|█████████▊  | 97/119 [01:01<00:12,  1.71it/s]Epoch: 3, train for the 135-th batch, train loss: 0.6671481728553772:  57%|██████▏    | 134/237 [01:17<01:05,  1.58it/s]Epoch: 3, train for the 135-th batch, train loss: 0.6671481728553772:  57%|██████▎    | 135/237 [01:17<01:05,  1.57it/s]Epoch: 2, train for the 226-th batch, train loss: 0.4789748787879944:  59%|██████▍    | 225/383 [02:13<01:41,  1.56it/s]Epoch: 2, train for the 226-th batch, train loss: 0.4789748787879944:  59%|██████▍    | 226/383 [02:13<01:40,  1.56it/s]evaluate for the 41-th batch, evaluate loss: 0.48531246185302734:  87%|██████████████▊  | 40/46 [00:11<00:01,  3.58it/s]evaluate for the 41-th batch, evaluate loss: 0.48531246185302734:  89%|███████████████▏ | 41/46 [00:11<00:01,  3.62it/s]evaluate for the 7-th batch, evaluate loss: 0.662094235420227:  30%|██████▎              | 6/20 [00:01<00:03,  3.52it/s]evaluate for the 7-th batch, evaluate loss: 0.662094235420227:  35%|███████▎             | 7/20 [00:01<00:03,  3.61it/s]evaluate for the 42-th batch, evaluate loss: 0.4687684178352356:  89%|████████████████  | 41/46 [00:11<00:01,  3.62it/s]evaluate for the 42-th batch, evaluate loss: 0.4687684178352356:  91%|████████████████▍ | 42/46 [00:11<00:01,  3.57it/s]evaluate for the 8-th batch, evaluate loss: 0.6486866474151611:  35%|███████             | 7/20 [00:02<00:03,  3.61it/s]evaluate for the 8-th batch, evaluate loss: 0.6486866474151611:  40%|████████            | 8/20 [00:02<00:03,  3.54it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5991171598434448:  49%|█████▍     | 119/241 [01:15<01:12,  1.67it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5991171598434448:  50%|█████▍     | 120/241 [01:15<01:12,  1.67it/s]Epoch: 5, train for the 98-th batch, train loss: 0.15719150006771088:  82%|█████████▊  | 97/119 [01:02<00:12,  1.71it/s]Epoch: 5, train for the 98-th batch, train loss: 0.15719150006771088:  82%|█████████▉  | 98/119 [01:02<00:12,  1.71it/s]evaluate for the 43-th batch, evaluate loss: 0.5321569442749023:  91%|████████████████▍ | 42/46 [00:12<00:01,  3.57it/s]evaluate for the 43-th batch, evaluate loss: 0.5321569442749023:  93%|████████████████▊ | 43/46 [00:12<00:00,  3.51it/s]Epoch: 3, train for the 136-th batch, train loss: 0.644668698310852:  57%|██████▊     | 135/237 [01:18<01:05,  1.57it/s]Epoch: 3, train for the 136-th batch, train loss: 0.644668698310852:  57%|██████▉     | 136/237 [01:18<01:04,  1.56it/s]Epoch: 2, train for the 227-th batch, train loss: 0.4970647394657135:  59%|██████▍    | 226/383 [02:13<01:40,  1.56it/s]Epoch: 2, train for the 227-th batch, train loss: 0.4970647394657135:  59%|██████▌    | 227/383 [02:13<01:40,  1.55it/s]evaluate for the 9-th batch, evaluate loss: 0.5872102975845337:  40%|████████            | 8/20 [00:02<00:03,  3.54it/s]evaluate for the 9-th batch, evaluate loss: 0.5872102975845337:  45%|█████████           | 9/20 [00:02<00:03,  3.60it/s]evaluate for the 44-th batch, evaluate loss: 0.5155866146087646:  93%|████████████████▊ | 43/46 [00:12<00:00,  3.51it/s]evaluate for the 44-th batch, evaluate loss: 0.5155866146087646:  96%|█████████████████▏| 44/46 [00:12<00:00,  3.55it/s]evaluate for the 10-th batch, evaluate loss: 0.6071609258651733:  45%|████████▌          | 9/20 [00:02<00:03,  3.60it/s]evaluate for the 10-th batch, evaluate loss: 0.6071609258651733:  50%|█████████         | 10/20 [00:02<00:02,  3.56it/s]Epoch: 1, train for the 121-th batch, train loss: 0.5676431059837341:  50%|█████▍     | 120/241 [01:16<01:12,  1.67it/s]Epoch: 1, train for the 121-th batch, train loss: 0.5676431059837341:  50%|█████▌     | 121/241 [01:16<01:11,  1.67it/s]Epoch: 5, train for the 99-th batch, train loss: 0.19311992824077606:  82%|█████████▉  | 98/119 [01:03<00:12,  1.71it/s]Epoch: 5, train for the 99-th batch, train loss: 0.19311992824077606:  83%|█████████▉  | 99/119 [01:03<00:11,  1.70it/s]evaluate for the 45-th batch, evaluate loss: 0.49581435322761536:  96%|████████████████▎| 44/46 [00:12<00:00,  3.55it/s]evaluate for the 45-th batch, evaluate loss: 0.49581435322761536:  98%|████████████████▋| 45/46 [00:12<00:00,  3.49it/s]evaluate for the 11-th batch, evaluate loss: 0.5952782034873962:  50%|█████████         | 10/20 [00:03<00:02,  3.56it/s]evaluate for the 11-th batch, evaluate loss: 0.5952782034873962:  55%|█████████▉        | 11/20 [00:03<00:02,  3.53it/s]Epoch: 3, train for the 137-th batch, train loss: 0.6788600087165833:  57%|██████▎    | 136/237 [01:19<01:04,  1.56it/s]Epoch: 3, train for the 137-th batch, train loss: 0.6788600087165833:  58%|██████▎    | 137/237 [01:19<01:04,  1.55it/s]Epoch: 2, train for the 228-th batch, train loss: 0.40070632100105286:  59%|█████▉    | 227/383 [02:14<01:40,  1.55it/s]Epoch: 2, train for the 228-th batch, train loss: 0.40070632100105286:  60%|█████▉    | 228/383 [02:14<01:40,  1.55it/s]Epoch: 1, train for the 121-th batch, train loss: 0.5676431059837341:  50%|█████▌     | 121/241 [01:16<01:15,  1.58it/s]
Traceback (most recent call last):
  File "/home/ayush/DyGLib/train_link_prediction.py", line 552, in <module>
    batch_src_node_embeddings, batch_dst_node_embeddings = model[
  File "/home/ayush/DyGLib/models/DyGFormer.py", line 207, in compute_src_dst_node_temporal_embeddings
    self.memory_bank.node_last_updated_times[src_unique_ids] = torch.from_numpy(node_interact_times[src_latest_indices]).float().to(self.device)
TypeError: expected np.ndarray (got numpy.float64)
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
evaluate for the 46-th batch, evaluate loss: 0.5097312331199646:  98%|█████████████████▌| 45/46 [00:12<00:00,  3.49it/s]evaluate for the 46-th batch, evaluate loss: 0.5097312331199646: 100%|██████████████████| 46/46 [00:12<00:00,  3.86it/s]evaluate for the 46-th batch, evaluate loss: 0.5097312331199646: 100%|██████████████████| 46/46 [00:12<00:00,  3.60it/s]
  0%|                                                                                            | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6380549073219299:   0%|                            | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6380549073219299:   4%|▊                   | 1/25 [00:00<00:02,  9.13it/s]evaluate for the 12-th batch, evaluate loss: 0.6222485303878784:  55%|█████████▉        | 11/20 [00:03<00:02,  3.53it/s]evaluate for the 12-th batch, evaluate loss: 0.6222485303878784:  60%|██████████▊       | 12/20 [00:03<00:02,  3.47it/s]evaluate for the 2-th batch, evaluate loss: 0.6538370847702026:   4%|▊                   | 1/25 [00:00<00:02,  9.13it/s]evaluate for the 2-th batch, evaluate loss: 0.6538370847702026:   8%|█▌                  | 2/25 [00:00<00:02,  9.16it/s]Epoch: 5, train for the 100-th batch, train loss: 0.16904784739017487:  83%|█████████▏ | 99/119 [01:03<00:11,  1.70it/s]Epoch: 5, train for the 100-th batch, train loss: 0.16904784739017487:  84%|████████▍ | 100/119 [01:03<00:11,  1.69it/s]evaluate for the 3-th batch, evaluate loss: 0.6923218965530396:   8%|█▌                  | 2/25 [00:00<00:02,  9.16it/s]evaluate for the 3-th batch, evaluate loss: 0.6923218965530396:  12%|██▍                 | 3/25 [00:00<00:02,  9.16it/s]evaluate for the 4-th batch, evaluate loss: 0.6759135127067566:  12%|██▍                 | 3/25 [00:00<00:02,  9.16it/s]evaluate for the 4-th batch, evaluate loss: 0.6759135127067566:  16%|███▏                | 4/25 [00:00<00:02,  9.18it/s]evaluate for the 13-th batch, evaluate loss: 0.6311143636703491:  60%|██████████▊       | 12/20 [00:03<00:02,  3.47it/s]evaluate for the 13-th batch, evaluate loss: 0.6311143636703491:  65%|███████████▋      | 13/20 [00:03<00:02,  3.44it/s]evaluate for the 5-th batch, evaluate loss: 0.6728008985519409:  16%|███▏                | 4/25 [00:00<00:02,  9.18it/s]evaluate for the 5-th batch, evaluate loss: 0.6728008985519409:  20%|████                | 5/25 [00:00<00:02,  9.20it/s]Epoch: 2, train for the 229-th batch, train loss: 0.38806289434432983:  60%|█████▉    | 228/383 [02:15<01:40,  1.55it/s]Epoch: 3, train for the 138-th batch, train loss: 0.6473989486694336:  58%|██████▎    | 137/237 [01:19<01:04,  1.55it/s]Epoch: 2, train for the 229-th batch, train loss: 0.38806289434432983:  60%|█████▉    | 229/383 [02:15<01:39,  1.54it/s]Epoch: 3, train for the 138-th batch, train loss: 0.6473989486694336:  58%|██████▍    | 138/237 [01:19<01:03,  1.55it/s]evaluate for the 6-th batch, evaluate loss: 0.7196520566940308:  20%|████                | 5/25 [00:00<00:02,  9.20it/s]evaluate for the 6-th batch, evaluate loss: 0.7196520566940308:  24%|████▊               | 6/25 [00:00<00:02,  9.22it/s]evaluate for the 14-th batch, evaluate loss: 0.618266761302948:  65%|████████████▎      | 13/20 [00:03<00:02,  3.44it/s]evaluate for the 14-th batch, evaluate loss: 0.618266761302948:  70%|█████████████▎     | 14/20 [00:03<00:01,  3.46it/s]evaluate for the 7-th batch, evaluate loss: 0.7425660490989685:  24%|████▊               | 6/25 [00:00<00:02,  9.22it/s]evaluate for the 7-th batch, evaluate loss: 0.7425660490989685:  28%|█████▌              | 7/25 [00:00<00:01,  9.21it/s]evaluate for the 8-th batch, evaluate loss: 0.7250405550003052:  28%|█████▌              | 7/25 [00:00<00:01,  9.21it/s]evaluate for the 8-th batch, evaluate loss: 0.7250405550003052:  32%|██████▍             | 8/25 [00:00<00:01,  9.21it/s]Epoch: 5, train for the 101-th batch, train loss: 0.14491954445838928:  84%|████████▍ | 100/119 [01:04<00:11,  1.69it/s]Epoch: 5, train for the 101-th batch, train loss: 0.14491954445838928:  85%|████████▍ | 101/119 [01:04<00:10,  1.68it/s]evaluate for the 9-th batch, evaluate loss: 0.7053958773612976:  32%|██████▍             | 8/25 [00:00<00:01,  9.21it/s]evaluate for the 9-th batch, evaluate loss: 0.7053958773612976:  36%|███████▏            | 9/25 [00:00<00:01,  9.21it/s]evaluate for the 15-th batch, evaluate loss: 0.6106488108634949:  70%|████████████▌     | 14/20 [00:04<00:01,  3.46it/s]evaluate for the 15-th batch, evaluate loss: 0.6106488108634949:  75%|█████████████▌    | 15/20 [00:04<00:01,  3.40it/s]evaluate for the 10-th batch, evaluate loss: 0.7343890070915222:  36%|██████▊            | 9/25 [00:01<00:01,  9.21it/s]evaluate for the 10-th batch, evaluate loss: 0.7343890070915222:  40%|███████▏          | 10/25 [00:01<00:01,  9.20it/s]evaluate for the 11-th batch, evaluate loss: 0.7343663573265076:  40%|███████▏          | 10/25 [00:01<00:01,  9.20it/s]evaluate for the 11-th batch, evaluate loss: 0.7343663573265076:  44%|███████▉          | 11/25 [00:01<00:01,  9.19it/s]Epoch: 3, train for the 139-th batch, train loss: 0.6256880164146423:  58%|██████▍    | 138/237 [01:20<01:03,  1.55it/s]Epoch: 2, train for the 230-th batch, train loss: 0.4529130160808563:  60%|██████▌    | 229/383 [02:15<01:39,  1.54it/s]Epoch: 3, train for the 139-th batch, train loss: 0.6256880164146423:  59%|██████▍    | 139/237 [01:20<01:03,  1.54it/s]Epoch: 2, train for the 230-th batch, train loss: 0.4529130160808563:  60%|██████▌    | 230/383 [02:15<01:39,  1.54it/s]evaluate for the 12-th batch, evaluate loss: 0.7014621496200562:  44%|███████▉          | 11/25 [00:01<00:01,  9.19it/s]evaluate for the 12-th batch, evaluate loss: 0.7014621496200562:  48%|████████▋         | 12/25 [00:01<00:01,  9.16it/s]evaluate for the 16-th batch, evaluate loss: 0.5932073593139648:  75%|█████████████▌    | 15/20 [00:04<00:01,  3.40it/s]evaluate for the 16-th batch, evaluate loss: 0.5932073593139648:  80%|██████████████▍   | 16/20 [00:04<00:01,  3.45it/s]evaluate for the 13-th batch, evaluate loss: 0.6674633622169495:  48%|████████▋         | 12/25 [00:01<00:01,  9.16it/s]evaluate for the 13-th batch, evaluate loss: 0.6674633622169495:  52%|█████████▎        | 13/25 [00:01<00:01,  9.11it/s]Epoch: 5, train for the 102-th batch, train loss: 0.19427257776260376:  85%|████████▍ | 101/119 [01:04<00:10,  1.68it/s]Epoch: 5, train for the 102-th batch, train loss: 0.19427257776260376:  86%|████████▌ | 102/119 [01:04<00:10,  1.69it/s]evaluate for the 14-th batch, evaluate loss: 0.7519804835319519:  52%|█████████▎        | 13/25 [00:01<00:01,  9.11it/s]evaluate for the 14-th batch, evaluate loss: 0.7519804835319519:  56%|██████████        | 14/25 [00:01<00:01,  9.12it/s]evaluate for the 17-th batch, evaluate loss: 0.590857744216919:  80%|███████████████▏   | 16/20 [00:04<00:01,  3.45it/s]evaluate for the 17-th batch, evaluate loss: 0.590857744216919:  85%|████████████████▏  | 17/20 [00:04<00:00,  3.40it/s]evaluate for the 15-th batch, evaluate loss: 0.740458607673645:  56%|██████████▋        | 14/25 [00:01<00:01,  9.12it/s]evaluate for the 15-th batch, evaluate loss: 0.740458607673645:  60%|███████████▍       | 15/25 [00:01<00:01,  9.14it/s]evaluate for the 16-th batch, evaluate loss: 0.6706472039222717:  60%|██████████▊       | 15/25 [00:01<00:01,  9.14it/s]evaluate for the 16-th batch, evaluate loss: 0.6706472039222717:  64%|███████████▌      | 16/25 [00:01<00:00,  9.15it/s]evaluate for the 17-th batch, evaluate loss: 0.6681990027427673:  64%|███████████▌      | 16/25 [00:01<00:00,  9.15it/s]evaluate for the 17-th batch, evaluate loss: 0.6681990027427673:  68%|████████████▏     | 17/25 [00:01<00:00,  9.12it/s]Epoch: 3, train for the 140-th batch, train loss: 0.6227820515632629:  59%|██████▍    | 139/237 [01:20<01:03,  1.54it/s]Epoch: 2, train for the 231-th batch, train loss: 0.508265495300293:  60%|███████▏    | 230/383 [02:16<01:39,  1.54it/s]Epoch: 3, train for the 140-th batch, train loss: 0.6227820515632629:  59%|██████▍    | 140/237 [01:20<01:03,  1.54it/s]Epoch: 2, train for the 231-th batch, train loss: 0.508265495300293:  60%|███████▏    | 231/383 [02:16<01:39,  1.53it/s]evaluate for the 18-th batch, evaluate loss: 0.6525723338127136:  85%|███████████████▎  | 17/20 [00:05<00:00,  3.40it/s]evaluate for the 18-th batch, evaluate loss: 0.6525723338127136:  90%|████████████████▏ | 18/20 [00:05<00:00,  3.49it/s]evaluate for the 18-th batch, evaluate loss: 0.6305036544799805:  68%|████████████▏     | 17/25 [00:01<00:00,  9.12it/s]evaluate for the 18-th batch, evaluate loss: 0.6305036544799805:  72%|████████████▉     | 18/25 [00:01<00:00,  9.11it/s]wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)evaluate for the 19-th batch, evaluate loss: 0.591827929019928:  72%|█████████████▋     | 18/25 [00:02<00:00,  9.11it/s]evaluate for the 19-th batch, evaluate loss: 0.591827929019928:  76%|██████████████▍    | 19/25 [00:02<00:00,  9.13it/s]Epoch: 5, train for the 103-th batch, train loss: 0.19801822304725647:  86%|████████▌ | 102/119 [01:05<00:10,  1.69it/s]Epoch: 5, train for the 103-th batch, train loss: 0.19801822304725647:  87%|████████▋ | 103/119 [01:05<00:09,  1.69it/s]evaluate for the 20-th batch, evaluate loss: 0.6639235615730286:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.13it/s]evaluate for the 20-th batch, evaluate loss: 0.6639235615730286:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.13it/s]evaluate for the 19-th batch, evaluate loss: 0.6510792374610901:  90%|████████████████▏ | 18/20 [00:05<00:00,  3.49it/s]evaluate for the 19-th batch, evaluate loss: 0.6510792374610901:  95%|█████████████████ | 19/20 [00:05<00:00,  3.42it/s]evaluate for the 20-th batch, evaluate loss: 0.6296536326408386:  95%|█████████████████ | 19/20 [00:05<00:00,  3.42it/s]evaluate for the 20-th batch, evaluate loss: 0.6296536326408386: 100%|██████████████████| 20/20 [00:05<00:00,  3.64it/s]
INFO:root:Epoch: 4, learning rate: 0.0001, train loss: 0.5086
INFO:root:train average_precision, 0.8486
INFO:root:train roc_auc, 0.8239
INFO:root:validate loss: 0.5191
INFO:root:validate average_precision, 0.8395
INFO:root:validate roc_auc, 0.8047
INFO:root:new node validate loss: 0.6127
INFO:root:new node validate first_1_average_precision, 0.6486
INFO:root:new node validate first_1_roc_auc, 0.5720
INFO:root:new node validate first_3_average_precision, 0.7004
INFO:root:new node validate first_3_roc_auc, 0.6336
INFO:root:new node validate first_10_average_precision, 0.7456
INFO:root:new node validate first_10_roc_auc, 0.6919
INFO:root:new node validate average_precision, 0.7523
INFO:root:new node validate roc_auc, 0.7116
INFO:root:save model ./saved_models/DyGFormer/ia-escorts-dynamic/DyGFormer_seed0_dygformer-ia-escorts-dynamic-old/DyGFormer_seed0_dygformer-ia-escorts-dynamic-old.pkl
evaluate for the 21-th batch, evaluate loss: 0.7303845882415771:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.13it/s]evaluate for the 21-th batch, evaluate loss: 0.7303845882415771:  84%|███████████████   | 21/25 [00:02<00:00,  9.13it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]evaluate for the 22-th batch, evaluate loss: 0.6099632382392883:  84%|███████████████   | 21/25 [00:02<00:00,  9.13it/s]evaluate for the 22-th batch, evaluate loss: 0.6099632382392883:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.15it/s]Epoch: 5, train for the 104-th batch, train loss: 0.22103890776634216:  87%|████████▋ | 103/119 [01:05<00:09,  1.69it/s]Epoch: 5, train for the 104-th batch, train loss: 0.22103890776634216:  87%|████████▋ | 104/119 [01:05<00:08,  1.84it/s]evaluate for the 23-th batch, evaluate loss: 0.6690984964370728:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.15it/s]evaluate for the 23-th batch, evaluate loss: 0.6690984964370728:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.15it/s]Epoch: 2, train for the 232-th batch, train loss: 0.3897804915904999:  60%|██████▋    | 231/383 [02:17<01:39,  1.53it/s]Epoch: 3, train for the 141-th batch, train loss: 0.6583965420722961:  59%|██████▍    | 140/237 [01:21<01:03,  1.54it/s]Epoch: 2, train for the 232-th batch, train loss: 0.3897804915904999:  61%|██████▋    | 232/383 [02:17<01:38,  1.53it/s]Epoch: 3, train for the 141-th batch, train loss: 0.6583965420722961:  59%|██████▌    | 141/237 [01:21<01:02,  1.53it/s]evaluate for the 24-th batch, evaluate loss: 0.6657528281211853:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.15it/s]evaluate for the 24-th batch, evaluate loss: 0.6657528281211853:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.15it/s]evaluate for the 25-th batch, evaluate loss: 0.7118499875068665:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.15it/s]evaluate for the 25-th batch, evaluate loss: 0.7118499875068665: 100%|██████████████████| 25/25 [00:02<00:00,  9.23it/s]
INFO:root:Epoch: 8, learning rate: 0.0001, train loss: 0.5683
INFO:root:train average_precision, 0.8164
INFO:root:train roc_auc, 0.7791
INFO:root:validate loss: 0.5082
INFO:root:validate average_precision, 0.8423
INFO:root:validate roc_auc, 0.8018
INFO:root:new node validate loss: 0.6867
INFO:root:new node validate first_1_average_precision, 0.5976
INFO:root:new node validate first_1_roc_auc, 0.5419
INFO:root:new node validate first_3_average_precision, 0.6794
INFO:root:new node validate first_3_roc_auc, 0.6395
INFO:root:new node validate first_10_average_precision, 0.7461
INFO:root:new node validate first_10_roc_auc, 0.7107
INFO:root:new node validate average_precision, 0.7063
INFO:root:new node validate roc_auc, 0.6534
INFO:root:save model ./saved_models/DyGFormer/ia-retweet-pol/DyGFormer_seed0_dygformer-ia-retweet-pol-old/DyGFormer_seed0_dygformer-ia-retweet-pol-old.pkl
Epoch: 5, train for the 1-th batch, train loss: 0.8488538861274719:   0%|                       | 0/146 [00:00<?, ?it/s]Epoch: 5, train for the 1-th batch, train loss: 0.8488538861274719:   1%|               | 1/146 [00:00<01:02,  2.30it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]wandb: \ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)Epoch: 9, train for the 1-th batch, train loss: 0.9076766967773438:   0%|                       | 0/151 [00:00<?, ?it/s]Epoch: 9, train for the 1-th batch, train loss: 0.9076766967773438:   1%|               | 1/151 [00:00<00:25,  5.84it/s]Epoch: 5, train for the 105-th batch, train loss: 0.17392821609973907:  87%|████████▋ | 104/119 [01:06<00:08,  1.84it/s]Epoch: 5, train for the 105-th batch, train loss: 0.17392821609973907:  88%|████████▊ | 105/119 [01:06<00:07,  1.78it/s]Epoch: 3, train for the 142-th batch, train loss: 0.6261353492736816:  59%|██████▌    | 141/237 [01:22<01:02,  1.53it/s]Epoch: 3, train for the 142-th batch, train loss: 0.6261353492736816:  60%|██████▌    | 142/237 [01:22<01:01,  1.54it/s]Epoch: 2, train for the 233-th batch, train loss: 0.4605545401573181:  61%|██████▋    | 232/383 [02:17<01:38,  1.53it/s]Epoch: 2, train for the 233-th batch, train loss: 0.4605545401573181:  61%|██████▋    | 233/383 [02:17<01:37,  1.53it/s]Epoch: 9, train for the 2-th batch, train loss: 0.91593337059021:   1%|                 | 1/151 [00:00<00:25,  5.84it/s]Epoch: 9, train for the 2-th batch, train loss: 0.91593337059021:   1%|▏                | 2/151 [00:00<00:33,  4.46it/s]Epoch: 5, train for the 2-th batch, train loss: 0.4920673668384552:   1%|               | 1/146 [00:00<01:02,  2.30it/s]Epoch: 5, train for the 2-th batch, train loss: 0.4920673668384552:   1%|▏              | 2/146 [00:00<01:10,  2.04it/s]Epoch: 9, train for the 3-th batch, train loss: 0.5471580624580383:   1%|▏              | 2/151 [00:00<00:33,  4.46it/s]Epoch: 9, train for the 3-th batch, train loss: 0.5471580624580383:   2%|▎              | 3/151 [00:00<00:29,  5.07it/s]Epoch: 9, train for the 4-th batch, train loss: 0.6145764589309692:   2%|▎              | 3/151 [00:00<00:29,  5.07it/s]Epoch: 9, train for the 4-th batch, train loss: 0.6145764589309692:   3%|▍              | 4/151 [00:00<00:27,  5.35it/s]Epoch: 5, train for the 106-th batch, train loss: 0.1561105102300644:  88%|█████████▋ | 105/119 [01:07<00:07,  1.78it/s]Epoch: 5, train for the 106-th batch, train loss: 0.1561105102300644:  89%|█████████▊ | 106/119 [01:07<00:07,  1.78it/s]Epoch: 3, train for the 143-th batch, train loss: 0.6389658451080322:  60%|██████▌    | 142/237 [01:22<01:01,  1.54it/s]Epoch: 3, train for the 143-th batch, train loss: 0.6389658451080322:  60%|██████▋    | 143/237 [01:22<01:01,  1.54it/s]Epoch: 2, train for the 234-th batch, train loss: 0.4357093572616577:  61%|██████▋    | 233/383 [02:18<01:37,  1.53it/s]Epoch: 2, train for the 234-th batch, train loss: 0.4357093572616577:  61%|██████▋    | 234/383 [02:18<01:36,  1.54it/s]Epoch: 9, train for the 5-th batch, train loss: 0.5706453323364258:   3%|▍              | 4/151 [00:00<00:27,  5.35it/s]Epoch: 9, train for the 5-th batch, train loss: 0.5706453323364258:   3%|▍              | 5/151 [00:00<00:27,  5.29it/s]Epoch: 5, train for the 3-th batch, train loss: 0.3686783015727997:   1%|▏              | 2/146 [00:01<01:10,  2.04it/s]Epoch: 5, train for the 3-th batch, train loss: 0.3686783015727997:   2%|▎              | 3/146 [00:01<01:13,  1.95it/s]wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)Epoch: 9, train for the 6-th batch, train loss: 0.5416818261146545:   3%|▍              | 5/151 [00:01<00:27,  5.29it/s]Epoch: 9, train for the 6-th batch, train loss: 0.5416818261146545:   4%|▌              | 6/151 [00:01<00:27,  5.30it/s]Epoch: 9, train for the 7-th batch, train loss: 0.5653759837150574:   4%|▌              | 6/151 [00:01<00:27,  5.30it/s]Epoch: 9, train for the 7-th batch, train loss: 0.5653759837150574:   5%|▋              | 7/151 [00:01<00:27,  5.21it/s]Epoch: 5, train for the 107-th batch, train loss: 0.21426694095134735:  89%|████████▉ | 106/119 [01:07<00:07,  1.78it/s]Epoch: 5, train for the 107-th batch, train loss: 0.21426694095134735:  90%|████████▉ | 107/119 [01:07<00:06,  1.73it/s]Epoch: 9, train for the 8-th batch, train loss: 0.6536024212837219:   5%|▋              | 7/151 [00:01<00:27,  5.21it/s]Epoch: 9, train for the 8-th batch, train loss: 0.6536024212837219:   5%|▊              | 8/151 [00:01<00:27,  5.21it/s]Epoch: 5, train for the 4-th batch, train loss: 0.37198299169540405:   2%|▎             | 3/146 [00:02<01:13,  1.95it/s]Epoch: 5, train for the 4-th batch, train loss: 0.37198299169540405:   3%|▍             | 4/146 [00:02<01:16,  1.85it/s]Epoch: 3, train for the 144-th batch, train loss: 0.6609959006309509:  60%|██████▋    | 143/237 [01:23<01:01,  1.54it/s]Epoch: 3, train for the 144-th batch, train loss: 0.6609959006309509:  61%|██████▋    | 144/237 [01:23<01:00,  1.54it/s]Epoch: 2, train for the 235-th batch, train loss: 0.3947112262248993:  61%|██████▋    | 234/383 [02:19<01:36,  1.54it/s]Epoch: 2, train for the 235-th batch, train loss: 0.3947112262248993:  61%|██████▋    | 235/383 [02:19<01:36,  1.54it/s]Epoch: 9, train for the 9-th batch, train loss: 0.6751116514205933:   5%|▊              | 8/151 [00:01<00:27,  5.21it/s]Epoch: 9, train for the 9-th batch, train loss: 0.6751116514205933:   6%|▉              | 9/151 [00:01<00:27,  5.13it/s]Epoch: 9, train for the 10-th batch, train loss: 0.5767051577568054:   6%|▊             | 9/151 [00:01<00:27,  5.13it/s]Epoch: 9, train for the 10-th batch, train loss: 0.5767051577568054:   7%|▊            | 10/151 [00:01<00:28,  5.03it/s]Epoch: 5, train for the 108-th batch, train loss: 0.1264510452747345:  90%|█████████▉ | 107/119 [01:08<00:06,  1.73it/s]Epoch: 5, train for the 108-th batch, train loss: 0.1264510452747345:  91%|█████████▉ | 108/119 [01:08<00:06,  1.68it/s]wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)Epoch: 9, train for the 11-th batch, train loss: 0.4456264078617096:   7%|▊            | 10/151 [00:02<00:28,  5.03it/s]Epoch: 9, train for the 11-th batch, train loss: 0.4456264078617096:   7%|▉            | 11/151 [00:02<00:27,  5.04it/s]Epoch: 5, train for the 5-th batch, train loss: 0.35306739807128906:   3%|▍             | 4/146 [00:02<01:16,  1.85it/s]Epoch: 5, train for the 5-th batch, train loss: 0.35306739807128906:   3%|▍             | 5/146 [00:02<01:17,  1.82it/s]Epoch: 3, train for the 145-th batch, train loss: 0.6820368766784668:  61%|██████▋    | 144/237 [01:24<01:00,  1.54it/s]Epoch: 2, train for the 236-th batch, train loss: 0.45491132140159607:  61%|██████▏   | 235/383 [02:19<01:36,  1.54it/s]Epoch: 3, train for the 145-th batch, train loss: 0.6820368766784668:  61%|██████▋    | 145/237 [01:24<00:59,  1.54it/s]Epoch: 2, train for the 236-th batch, train loss: 0.45491132140159607:  62%|██████▏   | 236/383 [02:19<01:35,  1.54it/s]Epoch: 9, train for the 12-th batch, train loss: 0.5361630916595459:   7%|▉            | 11/151 [00:02<00:27,  5.04it/s]Epoch: 9, train for the 12-th batch, train loss: 0.5361630916595459:   8%|█            | 12/151 [00:02<00:27,  5.02it/s]Epoch: 9, train for the 13-th batch, train loss: 0.6463817358016968:   8%|█            | 12/151 [00:02<00:27,  5.02it/s]Epoch: 9, train for the 13-th batch, train loss: 0.6463817358016968:   9%|█            | 13/151 [00:02<00:27,  4.98it/s]Epoch: 5, train for the 109-th batch, train loss: 0.20370303094387054:  91%|█████████ | 108/119 [01:08<00:06,  1.68it/s]Epoch: 5, train for the 109-th batch, train loss: 0.20370303094387054:  92%|█████████▏| 109/119 [01:08<00:05,  1.68it/s]Epoch: 5, train for the 6-th batch, train loss: 0.565760612487793:   3%|▌               | 5/146 [00:03<01:17,  1.82it/s]Epoch: 5, train for the 6-th batch, train loss: 0.565760612487793:   4%|▋               | 6/146 [00:03<01:18,  1.77it/s]Epoch: 9, train for the 14-th batch, train loss: 0.6254188418388367:   9%|█            | 13/151 [00:02<00:27,  4.98it/s]Epoch: 9, train for the 14-th batch, train loss: 0.6254188418388367:   9%|█▏           | 14/151 [00:02<00:27,  4.92it/s]Epoch: 3, train for the 146-th batch, train loss: 0.642309844493866:  61%|███████▎    | 145/237 [01:24<00:59,  1.54it/s]Epoch: 2, train for the 237-th batch, train loss: 0.4285086393356323:  62%|██████▊    | 236/383 [02:20<01:35,  1.54it/s]Epoch: 3, train for the 146-th batch, train loss: 0.642309844493866:  62%|███████▍    | 146/237 [01:24<00:59,  1.53it/s]Epoch: 2, train for the 237-th batch, train loss: 0.4285086393356323:  62%|██████▊    | 237/383 [02:20<01:35,  1.53it/s]Epoch: 9, train for the 15-th batch, train loss: 0.47656843066215515:   9%|█           | 14/151 [00:02<00:27,  4.92it/s]Epoch: 9, train for the 15-th batch, train loss: 0.47656843066215515:  10%|█▏          | 15/151 [00:02<00:27,  4.86it/s]wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)Epoch: 9, train for the 16-th batch, train loss: 0.7124686241149902:  10%|█▎           | 15/151 [00:03<00:27,  4.86it/s]Epoch: 9, train for the 16-th batch, train loss: 0.7124686241149902:  11%|█▍           | 16/151 [00:03<00:28,  4.72it/s]Epoch: 5, train for the 110-th batch, train loss: 0.17706653475761414:  92%|█████████▏| 109/119 [01:09<00:05,  1.68it/s]Epoch: 5, train for the 110-th batch, train loss: 0.17706653475761414:  92%|█████████▏| 110/119 [01:09<00:05,  1.68it/s]wandb: \ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)Epoch: 5, train for the 7-th batch, train loss: 0.5136732459068298:   4%|▌              | 6/146 [00:03<01:18,  1.77it/s]Epoch: 5, train for the 7-th batch, train loss: 0.5136732459068298:   5%|▋              | 7/146 [00:03<01:18,  1.76it/s]Epoch: 9, train for the 17-th batch, train loss: 0.6132264733314514:  11%|█▍           | 16/151 [00:03<00:28,  4.72it/s]Epoch: 9, train for the 17-th batch, train loss: 0.6132264733314514:  11%|█▍           | 17/151 [00:03<00:28,  4.74it/s]Epoch: 3, train for the 147-th batch, train loss: 0.6532671451568604:  62%|██████▊    | 146/237 [01:25<00:59,  1.53it/s]Epoch: 2, train for the 238-th batch, train loss: 0.4289705455303192:  62%|██████▊    | 237/383 [02:21<01:35,  1.53it/s]Epoch: 3, train for the 147-th batch, train loss: 0.6532671451568604:  62%|██████▊    | 147/237 [01:25<00:58,  1.53it/s]Epoch: 2, train for the 238-th batch, train loss: 0.4289705455303192:  62%|██████▊    | 238/383 [02:21<01:34,  1.53it/s]wandb: 🚀 View run dygformer-ia-movielens-user2tags-10m-old at: https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib/runs/x1mz772x
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240407_154926-x1mz772x/logs
Epoch: 9, train for the 18-th batch, train loss: 0.5189138054847717:  11%|█▍           | 17/151 [00:03<00:28,  4.74it/s]Epoch: 9, train for the 18-th batch, train loss: 0.5189138054847717:  12%|█▌           | 18/151 [00:03<00:27,  4.76it/s]Epoch: 9, train for the 19-th batch, train loss: 0.5317588448524475:  12%|█▌           | 18/151 [00:03<00:27,  4.76it/s]Epoch: 9, train for the 19-th batch, train loss: 0.5317588448524475:  13%|█▋           | 19/151 [00:03<00:27,  4.75it/s]Epoch: 5, train for the 8-th batch, train loss: 0.3937737047672272:   5%|▋              | 7/146 [00:04<01:18,  1.76it/s]Epoch: 5, train for the 8-th batch, train loss: 0.3937737047672272:   5%|▊              | 8/146 [00:04<01:16,  1.81it/s]Epoch: 5, train for the 111-th batch, train loss: 0.18023723363876343:  92%|█████████▏| 110/119 [01:10<00:05,  1.68it/s]Epoch: 5, train for the 111-th batch, train loss: 0.18023723363876343:  93%|█████████▎| 111/119 [01:10<00:04,  1.67it/s]Epoch: 9, train for the 20-th batch, train loss: 0.4351551830768585:  13%|█▋           | 19/151 [00:04<00:27,  4.75it/s]Epoch: 9, train for the 20-th batch, train loss: 0.4351551830768585:  13%|█▋           | 20/151 [00:04<00:27,  4.73it/s]Epoch: 3, train for the 148-th batch, train loss: 0.6284716129302979:  62%|██████▊    | 147/237 [01:26<00:58,  1.53it/s]Epoch: 2, train for the 239-th batch, train loss: 0.4521591067314148:  62%|██████▊    | 238/383 [02:21<01:34,  1.53it/s]Epoch: 3, train for the 148-th batch, train loss: 0.6284716129302979:  62%|██████▊    | 148/237 [01:26<00:58,  1.53it/s]Epoch: 2, train for the 239-th batch, train loss: 0.4521591067314148:  62%|██████▊    | 239/383 [02:21<01:34,  1.53it/s]Epoch: 9, train for the 21-th batch, train loss: 0.45120131969451904:  13%|█▌          | 20/151 [00:04<00:27,  4.73it/s]Epoch: 9, train for the 21-th batch, train loss: 0.45120131969451904:  14%|█▋          | 21/151 [00:04<00:27,  4.71it/s]Epoch: 9, train for the 22-th batch, train loss: 0.5365421175956726:  14%|█▊           | 21/151 [00:04<00:27,  4.71it/s]Epoch: 9, train for the 22-th batch, train loss: 0.5365421175956726:  15%|█▉           | 22/151 [00:04<00:27,  4.71it/s]Epoch: 5, train for the 9-th batch, train loss: 0.41784340143203735:   5%|▊             | 8/146 [00:04<01:16,  1.81it/s]Epoch: 5, train for the 9-th batch, train loss: 0.41784340143203735:   6%|▊             | 9/146 [00:04<01:19,  1.72it/s]Epoch: 5, train for the 112-th batch, train loss: 0.12115516513586044:  93%|█████████▎| 111/119 [01:10<00:04,  1.67it/s]Epoch: 5, train for the 112-th batch, train loss: 0.12115516513586044:  94%|█████████▍| 112/119 [01:10<00:04,  1.63it/s]Epoch: 9, train for the 23-th batch, train loss: 0.8394847512245178:  15%|█▉           | 22/151 [00:04<00:27,  4.71it/s]Epoch: 9, train for the 23-th batch, train loss: 0.8394847512245178:  15%|█▉           | 23/151 [00:04<00:27,  4.64it/s]Epoch: 3, train for the 149-th batch, train loss: 0.6242226958274841:  62%|██████▊    | 148/237 [01:26<00:58,  1.53it/s]Epoch: 2, train for the 240-th batch, train loss: 0.4991760849952698:  62%|██████▊    | 239/383 [02:22<01:34,  1.53it/s]Epoch: 3, train for the 149-th batch, train loss: 0.6242226958274841:  63%|██████▉    | 149/237 [01:26<00:57,  1.53it/s]Epoch: 2, train for the 240-th batch, train loss: 0.4991760849952698:  63%|██████▉    | 240/383 [02:22<01:33,  1.53it/s]Epoch: 9, train for the 24-th batch, train loss: 0.26383253931999207:  15%|█▊          | 23/151 [00:04<00:27,  4.64it/s]Epoch: 9, train for the 24-th batch, train loss: 0.26383253931999207:  16%|█▉          | 24/151 [00:04<00:27,  4.69it/s]Epoch: 5, train for the 10-th batch, train loss: 0.4347569942474365:   6%|▊             | 9/146 [00:05<01:19,  1.72it/s]Epoch: 5, train for the 10-th batch, train loss: 0.4347569942474365:   7%|▉            | 10/146 [00:05<01:20,  1.69it/s]Epoch: 9, train for the 25-th batch, train loss: 0.4884589910507202:  16%|██           | 24/151 [00:05<00:27,  4.69it/s]Epoch: 9, train for the 25-th batch, train loss: 0.4884589910507202:  17%|██▏          | 25/151 [00:05<00:26,  4.70it/s]Epoch: 5, train for the 113-th batch, train loss: 0.1642075479030609:  94%|██████████▎| 112/119 [01:11<00:04,  1.63it/s]Epoch: 5, train for the 113-th batch, train loss: 0.1642075479030609:  95%|██████████▍| 113/119 [01:11<00:03,  1.62it/s]Epoch: 9, train for the 26-th batch, train loss: 0.6496661305427551:  17%|██▏          | 25/151 [00:05<00:26,  4.70it/s]Epoch: 9, train for the 26-th batch, train loss: 0.6496661305427551:  17%|██▏          | 26/151 [00:05<00:27,  4.59it/s]DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
Epoch: 3, train for the 150-th batch, train loss: 0.6058847904205322:  63%|██████▉    | 149/237 [01:27<00:57,  1.53it/s]Epoch: 2, train for the 241-th batch, train loss: 0.4888938069343567:  63%|██████▉    | 240/383 [02:23<01:33,  1.53it/s]Epoch: 3, train for the 150-th batch, train loss: 0.6058847904205322:  63%|██████▉    | 150/237 [01:27<00:56,  1.53it/s]Epoch: 2, train for the 241-th batch, train loss: 0.4888938069343567:  63%|██████▉    | 241/383 [02:23<01:32,  1.53it/s]Epoch: 9, train for the 27-th batch, train loss: 0.5004709362983704:  17%|██▏          | 26/151 [00:05<00:27,  4.59it/s]Epoch: 9, train for the 27-th batch, train loss: 0.5004709362983704:  18%|██▎          | 27/151 [00:05<00:26,  4.61it/s]DEBUG:urllib3.connectionpool:https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/1.1" 200 2
Epoch: 5, train for the 11-th batch, train loss: 0.46039608120918274:   7%|▊           | 10/146 [00:06<01:20,  1.69it/s]Epoch: 5, train for the 11-th batch, train loss: 0.46039608120918274:   8%|▉           | 11/146 [00:06<01:16,  1.76it/s]Epoch: 5, train for the 114-th batch, train loss: 0.17969608306884766:  95%|█████████▍| 113/119 [01:11<00:03,  1.62it/s]Epoch: 5, train for the 114-th batch, train loss: 0.17969608306884766:  96%|█████████▌| 114/119 [01:11<00:03,  1.63it/s]Epoch: 9, train for the 28-th batch, train loss: 0.4863961637020111:  18%|██▎          | 27/151 [00:05<00:26,  4.61it/s]Epoch: 9, train for the 28-th batch, train loss: 0.4863961637020111:  19%|██▍          | 28/151 [00:05<00:26,  4.64it/s]Epoch: 9, train for the 29-th batch, train loss: 0.6698468923568726:  19%|██▍          | 28/151 [00:05<00:26,  4.64it/s]Epoch: 9, train for the 29-th batch, train loss: 0.6698468923568726:  19%|██▍          | 29/151 [00:05<00:26,  4.59it/s]Epoch: 5, train for the 12-th batch, train loss: 0.4574165940284729:   8%|▉            | 11/146 [00:06<01:16,  1.76it/s]Epoch: 5, train for the 12-th batch, train loss: 0.4574165940284729:   8%|█            | 12/146 [00:06<01:14,  1.80it/s]Epoch: 2, train for the 242-th batch, train loss: 0.5185528993606567:  63%|██████▉    | 241/383 [02:23<01:32,  1.53it/s]Epoch: 3, train for the 151-th batch, train loss: 0.630143940448761:  63%|███████▌    | 150/237 [01:28<00:56,  1.53it/s]Epoch: 2, train for the 242-th batch, train loss: 0.5185528993606567:  63%|██████▉    | 242/383 [02:23<01:32,  1.53it/s]Epoch: 3, train for the 151-th batch, train loss: 0.630143940448761:  64%|███████▋    | 151/237 [01:28<00:56,  1.53it/s]Epoch: 9, train for the 30-th batch, train loss: 0.47573867440223694:  19%|██▎         | 29/151 [00:06<00:26,  4.59it/s]Epoch: 9, train for the 30-th batch, train loss: 0.47573867440223694:  20%|██▍         | 30/151 [00:06<00:26,  4.61it/s]Epoch: 5, train for the 115-th batch, train loss: 0.17505145072937012:  96%|█████████▌| 114/119 [01:12<00:03,  1.63it/s]Epoch: 5, train for the 115-th batch, train loss: 0.17505145072937012:  97%|█████████▋| 115/119 [01:12<00:02,  1.66it/s]Epoch: 9, train for the 31-th batch, train loss: 0.46568217873573303:  20%|██▍         | 30/151 [00:06<00:26,  4.61it/s]Epoch: 9, train for the 31-th batch, train loss: 0.46568217873573303:  21%|██▍         | 31/151 [00:06<00:25,  4.63it/s]Epoch: 9, train for the 32-th batch, train loss: 0.640736997127533:  21%|██▊           | 31/151 [00:06<00:25,  4.63it/s]Epoch: 9, train for the 32-th batch, train loss: 0.640736997127533:  21%|██▉           | 32/151 [00:06<00:25,  4.59it/s]Epoch: 5, train for the 13-th batch, train loss: 0.44955921173095703:   8%|▉           | 12/146 [00:07<01:14,  1.80it/s]Epoch: 5, train for the 13-th batch, train loss: 0.44955921173095703:   9%|█           | 13/146 [00:07<01:15,  1.76it/s]Epoch: 3, train for the 152-th batch, train loss: 0.6373120546340942:  64%|███████    | 151/237 [01:28<00:56,  1.53it/s]Epoch: 2, train for the 243-th batch, train loss: 0.4465266466140747:  63%|██████▉    | 242/383 [02:24<01:32,  1.53it/s]Epoch: 3, train for the 152-th batch, train loss: 0.6373120546340942:  64%|███████    | 152/237 [01:28<00:55,  1.53it/s]Epoch: 2, train for the 243-th batch, train loss: 0.4465266466140747:  63%|██████▉    | 243/383 [02:24<01:31,  1.53it/s]Epoch: 9, train for the 33-th batch, train loss: 0.5249669551849365:  21%|██▊          | 32/151 [00:06<00:25,  4.59it/s]Epoch: 9, train for the 33-th batch, train loss: 0.5249669551849365:  22%|██▊          | 33/151 [00:06<00:25,  4.57it/s]Epoch: 5, train for the 116-th batch, train loss: 0.12651677429676056:  97%|█████████▋| 115/119 [01:13<00:02,  1.66it/s]Epoch: 5, train for the 116-th batch, train loss: 0.12651677429676056:  97%|█████████▋| 116/119 [01:13<00:01,  1.66it/s]Epoch: 9, train for the 34-th batch, train loss: 0.5738259553909302:  22%|██▊          | 33/151 [00:07<00:25,  4.57it/s]Epoch: 9, train for the 34-th batch, train loss: 0.5738259553909302:  23%|██▉          | 34/151 [00:07<00:25,  4.56it/s]Epoch: 9, train for the 35-th batch, train loss: 0.6645553708076477:  23%|██▉          | 34/151 [00:07<00:25,  4.56it/s]Epoch: 9, train for the 35-th batch, train loss: 0.6645553708076477:  23%|███          | 35/151 [00:07<00:25,  4.53it/s]Epoch: 5, train for the 14-th batch, train loss: 0.44667235016822815:   9%|█           | 13/146 [00:07<01:15,  1.76it/s]Epoch: 5, train for the 14-th batch, train loss: 0.44667235016822815:  10%|█▏          | 14/146 [00:07<01:16,  1.71it/s]Epoch: 3, train for the 153-th batch, train loss: 0.6348676681518555:  64%|███████    | 152/237 [01:29<00:55,  1.53it/s]Epoch: 3, train for the 153-th batch, train loss: 0.6348676681518555:  65%|███████    | 153/237 [01:29<00:55,  1.53it/s]Epoch: 2, train for the 244-th batch, train loss: 0.3662168085575104:  63%|██████▉    | 243/383 [02:25<01:31,  1.53it/s]Epoch: 2, train for the 244-th batch, train loss: 0.3662168085575104:  64%|███████    | 244/383 [02:25<01:31,  1.53it/s]Epoch: 5, train for the 117-th batch, train loss: 0.2035665214061737:  97%|██████████▋| 116/119 [01:13<00:01,  1.66it/s]Epoch: 5, train for the 117-th batch, train loss: 0.2035665214061737:  98%|██████████▊| 117/119 [01:13<00:01,  1.65it/s]Epoch: 9, train for the 36-th batch, train loss: 0.6779841184616089:  23%|███          | 35/151 [00:07<00:25,  4.53it/s]Epoch: 9, train for the 36-th batch, train loss: 0.6779841184616089:  24%|███          | 36/151 [00:07<00:25,  4.51it/s]Epoch: 9, train for the 37-th batch, train loss: 0.7047508358955383:  24%|███          | 36/151 [00:07<00:25,  4.51it/s]Epoch: 9, train for the 37-th batch, train loss: 0.7047508358955383:  25%|███▏         | 37/151 [00:07<00:25,  4.47it/s]Epoch: 9, train for the 38-th batch, train loss: 0.6627215147018433:  25%|███▏         | 37/151 [00:07<00:25,  4.47it/s]Epoch: 9, train for the 38-th batch, train loss: 0.6627215147018433:  25%|███▎         | 38/151 [00:07<00:25,  4.46it/s]Epoch: 5, train for the 15-th batch, train loss: 0.42346617579460144:  10%|█▏          | 14/146 [00:08<01:16,  1.71it/s]Epoch: 5, train for the 15-th batch, train loss: 0.42346617579460144:  10%|█▏          | 15/146 [00:08<01:18,  1.67it/s]Epoch: 3, train for the 154-th batch, train loss: 0.6234979629516602:  65%|███████    | 153/237 [01:30<00:55,  1.53it/s]Epoch: 2, train for the 245-th batch, train loss: 0.4186473786830902:  64%|███████    | 244/383 [02:25<01:31,  1.53it/s]Epoch: 3, train for the 154-th batch, train loss: 0.6234979629516602:  65%|███████▏   | 154/237 [01:30<00:54,  1.53it/s]Epoch: 2, train for the 245-th batch, train loss: 0.4186473786830902:  64%|███████    | 245/383 [02:25<01:30,  1.53it/s]Epoch: 5, train for the 118-th batch, train loss: 0.14365577697753906:  98%|█████████▊| 117/119 [01:14<00:01,  1.65it/s]Epoch: 5, train for the 118-th batch, train loss: 0.14365577697753906:  99%|█████████▉| 118/119 [01:14<00:00,  1.63it/s]Epoch: 9, train for the 39-th batch, train loss: 0.6407456994056702:  25%|███▎         | 38/151 [00:08<00:25,  4.46it/s]Epoch: 9, train for the 39-th batch, train loss: 0.6407456994056702:  26%|███▎         | 39/151 [00:08<00:25,  4.46it/s]Epoch: 9, train for the 40-th batch, train loss: 0.3836197555065155:  26%|███▎         | 39/151 [00:08<00:25,  4.46it/s]Epoch: 9, train for the 40-th batch, train loss: 0.3836197555065155:  26%|███▍         | 40/151 [00:08<00:24,  4.51it/s]Epoch: 5, train for the 119-th batch, train loss: 0.20284827053546906:  99%|█████████▉| 118/119 [01:14<00:00,  1.63it/s]Epoch: 5, train for the 119-th batch, train loss: 0.20284827053546906: 100%|██████████| 119/119 [01:14<00:00,  1.91it/s]Epoch: 5, train for the 119-th batch, train loss: 0.20284827053546906: 100%|██████████| 119/119 [01:14<00:00,  1.59it/s]
  0%|                                                                                            | 0/40 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.1434697061777115:   0%|                            | 0/40 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.1434697061777115:   2%|▌                   | 1/40 [00:00<00:04,  9.64it/s]Epoch: 9, train for the 41-th batch, train loss: 0.5655331015586853:  26%|███▍         | 40/151 [00:08<00:24,  4.51it/s]Epoch: 9, train for the 41-th batch, train loss: 0.5655331015586853:  27%|███▌         | 41/151 [00:08<00:24,  4.49it/s]evaluate for the 2-th batch, evaluate loss: 0.18172651529312134:   2%|▍                  | 1/40 [00:00<00:04,  9.64it/s]evaluate for the 2-th batch, evaluate loss: 0.18172651529312134:   5%|▉                  | 2/40 [00:00<00:05,  6.54it/s]Epoch: 3, train for the 155-th batch, train loss: 0.6223604679107666:  65%|███████▏   | 154/237 [01:30<00:54,  1.53it/s]Epoch: 3, train for the 155-th batch, train loss: 0.6223604679107666:  65%|███████▏   | 155/237 [01:30<00:53,  1.53it/s]Epoch: 2, train for the 246-th batch, train loss: 0.4700062870979309:  64%|███████    | 245/383 [02:26<01:30,  1.53it/s]Epoch: 2, train for the 246-th batch, train loss: 0.4700062870979309:  64%|███████    | 246/383 [02:26<01:29,  1.53it/s]Epoch: 5, train for the 16-th batch, train loss: 0.4609611928462982:  10%|█▎           | 15/146 [00:09<01:18,  1.67it/s]Epoch: 5, train for the 16-th batch, train loss: 0.4609611928462982:  11%|█▍           | 16/146 [00:09<01:26,  1.50it/s]Epoch: 9, train for the 42-th batch, train loss: 0.6131707429885864:  27%|███▌         | 41/151 [00:08<00:24,  4.49it/s]Epoch: 9, train for the 42-th batch, train loss: 0.6131707429885864:  28%|███▌         | 42/151 [00:08<00:24,  4.48it/s]evaluate for the 3-th batch, evaluate loss: 0.21259738504886627:   5%|▉                  | 2/40 [00:00<00:05,  6.54it/s]evaluate for the 3-th batch, evaluate loss: 0.21259738504886627:   8%|█▍                 | 3/40 [00:00<00:07,  4.99it/s]Epoch: 9, train for the 43-th batch, train loss: 0.6091345548629761:  28%|███▌         | 42/151 [00:09<00:24,  4.48it/s]Epoch: 9, train for the 43-th batch, train loss: 0.6091345548629761:  28%|███▋         | 43/151 [00:09<00:24,  4.47it/s]evaluate for the 4-th batch, evaluate loss: 0.13964895904064178:   8%|█▍                 | 3/40 [00:00<00:07,  4.99it/s]evaluate for the 4-th batch, evaluate loss: 0.13964895904064178:  10%|█▉                 | 4/40 [00:00<00:08,  4.24it/s]Epoch: 9, train for the 44-th batch, train loss: 0.46781301498413086:  28%|███▍        | 43/151 [00:09<00:24,  4.47it/s]Epoch: 9, train for the 44-th batch, train loss: 0.46781301498413086:  29%|███▍        | 44/151 [00:09<00:23,  4.49it/s]Epoch: 5, train for the 17-th batch, train loss: 0.43376708030700684:  11%|█▎          | 16/146 [00:09<01:26,  1.50it/s]Epoch: 5, train for the 17-th batch, train loss: 0.43376708030700684:  12%|█▍          | 17/146 [00:09<01:22,  1.57it/s]Epoch: 3, train for the 156-th batch, train loss: 0.6425727605819702:  65%|███████▏   | 155/237 [01:31<00:53,  1.53it/s]Epoch: 3, train for the 156-th batch, train loss: 0.6425727605819702:  66%|███████▏   | 156/237 [01:31<00:52,  1.53it/s]Epoch: 2, train for the 247-th batch, train loss: 0.40936407446861267:  64%|██████▍   | 246/383 [02:26<01:29,  1.53it/s]Epoch: 2, train for the 247-th batch, train loss: 0.40936407446861267:  64%|██████▍   | 247/383 [02:26<01:28,  1.53it/s]Epoch: 9, train for the 45-th batch, train loss: 0.5916880369186401:  29%|███▊         | 44/151 [00:09<00:23,  4.49it/s]Epoch: 9, train for the 45-th batch, train loss: 0.5916880369186401:  30%|███▊         | 45/151 [00:09<00:23,  4.47it/s]evaluate for the 5-th batch, evaluate loss: 0.18483571708202362:  10%|█▉                 | 4/40 [00:01<00:08,  4.24it/s]evaluate for the 5-th batch, evaluate loss: 0.18483571708202362:  12%|██▍                | 5/40 [00:01<00:08,  4.06it/s]Epoch: 9, train for the 46-th batch, train loss: 0.3776646852493286:  30%|███▊         | 45/151 [00:09<00:23,  4.47it/s]Epoch: 9, train for the 46-th batch, train loss: 0.3776646852493286:  30%|███▉         | 46/151 [00:09<00:23,  4.53it/s]evaluate for the 6-th batch, evaluate loss: 0.1740707904100418:  12%|██▌                 | 5/40 [00:01<00:08,  4.06it/s]evaluate for the 6-th batch, evaluate loss: 0.1740707904100418:  15%|███                 | 6/40 [00:01<00:08,  3.83it/s]Epoch: 5, train for the 18-th batch, train loss: 0.39678552746772766:  12%|█▍          | 17/146 [00:10<01:22,  1.57it/s]Epoch: 5, train for the 18-th batch, train loss: 0.39678552746772766:  12%|█▍          | 18/146 [00:10<01:19,  1.62it/s]Epoch: 9, train for the 47-th batch, train loss: 0.6234239935874939:  30%|███▉         | 46/151 [00:09<00:23,  4.53it/s]Epoch: 9, train for the 47-th batch, train loss: 0.6234239935874939:  31%|████         | 47/151 [00:09<00:23,  4.49it/s]Epoch: 3, train for the 157-th batch, train loss: 0.6490829586982727:  66%|███████▏   | 156/237 [01:32<00:52,  1.53it/s]Epoch: 2, train for the 248-th batch, train loss: 0.4114264249801636:  64%|███████    | 247/383 [02:27<01:28,  1.53it/s]Epoch: 3, train for the 157-th batch, train loss: 0.6490829586982727:  66%|███████▎   | 157/237 [01:32<00:52,  1.53it/s]Epoch: 2, train for the 248-th batch, train loss: 0.4114264249801636:  65%|███████    | 248/383 [02:27<01:28,  1.53it/s]evaluate for the 7-th batch, evaluate loss: 0.11621861159801483:  15%|██▊                | 6/40 [00:01<00:08,  3.83it/s]evaluate for the 7-th batch, evaluate loss: 0.11621861159801483:  18%|███▎               | 7/40 [00:01<00:08,  3.81it/s]Epoch: 9, train for the 48-th batch, train loss: 0.40128087997436523:  31%|███▋        | 47/151 [00:10<00:23,  4.49it/s]Epoch: 9, train for the 48-th batch, train loss: 0.40128087997436523:  32%|███▊        | 48/151 [00:10<00:22,  4.54it/s]evaluate for the 8-th batch, evaluate loss: 0.13734762370586395:  18%|███▎               | 7/40 [00:01<00:08,  3.81it/s]evaluate for the 8-th batch, evaluate loss: 0.13734762370586395:  20%|███▊               | 8/40 [00:01<00:08,  3.71it/s]Epoch: 9, train for the 49-th batch, train loss: 0.5791882872581482:  32%|████▏        | 48/151 [00:10<00:22,  4.54it/s]Epoch: 9, train for the 49-th batch, train loss: 0.5791882872581482:  32%|████▏        | 49/151 [00:10<00:22,  4.50it/s]Epoch: 5, train for the 19-th batch, train loss: 0.4750034511089325:  12%|█▌           | 18/146 [00:11<01:19,  1.62it/s]Epoch: 5, train for the 19-th batch, train loss: 0.4750034511089325:  13%|█▋           | 19/146 [00:11<01:17,  1.65it/s]Epoch: 9, train for the 50-th batch, train loss: 0.5782350301742554:  32%|████▏        | 49/151 [00:10<00:22,  4.50it/s]Epoch: 9, train for the 50-th batch, train loss: 0.5782350301742554:  33%|████▎        | 50/151 [00:10<00:22,  4.47it/s]evaluate for the 9-th batch, evaluate loss: 0.18375177681446075:  20%|███▊               | 8/40 [00:02<00:08,  3.71it/s]evaluate for the 9-th batch, evaluate loss: 0.18375177681446075:  22%|████▎              | 9/40 [00:02<00:08,  3.67it/s]Epoch: 3, train for the 158-th batch, train loss: 0.6220178008079529:  66%|███████▎   | 157/237 [01:32<00:52,  1.53it/s]Epoch: 2, train for the 249-th batch, train loss: 0.474922776222229:  65%|███████▊    | 248/383 [02:28<01:28,  1.53it/s]Epoch: 3, train for the 158-th batch, train loss: 0.6220178008079529:  67%|███████▎   | 158/237 [01:32<00:51,  1.53it/s]Epoch: 2, train for the 249-th batch, train loss: 0.474922776222229:  65%|███████▊    | 249/383 [02:28<01:27,  1.53it/s]Epoch: 9, train for the 51-th batch, train loss: 0.6121054291725159:  33%|████▎        | 50/151 [00:10<00:22,  4.47it/s]Epoch: 9, train for the 51-th batch, train loss: 0.6121054291725159:  34%|████▍        | 51/151 [00:10<00:22,  4.47it/s]evaluate for the 10-th batch, evaluate loss: 0.2085423618555069:  22%|████▎              | 9/40 [00:02<00:08,  3.67it/s]evaluate for the 10-th batch, evaluate loss: 0.2085423618555069:  25%|████▌             | 10/40 [00:02<00:08,  3.65it/s]Epoch: 9, train for the 52-th batch, train loss: 0.6047401428222656:  34%|████▍        | 51/151 [00:11<00:22,  4.47it/s]Epoch: 9, train for the 52-th batch, train loss: 0.6047401428222656:  34%|████▍        | 52/151 [00:11<00:22,  4.46it/s]Epoch: 5, train for the 20-th batch, train loss: 0.4484948217868805:  13%|█▋           | 19/146 [00:11<01:17,  1.65it/s]Epoch: 5, train for the 20-th batch, train loss: 0.4484948217868805:  14%|█▊           | 20/146 [00:11<01:16,  1.65it/s]evaluate for the 11-th batch, evaluate loss: 0.19243206083774567:  25%|████▎            | 10/40 [00:02<00:08,  3.65it/s]evaluate for the 11-th batch, evaluate loss: 0.19243206083774567:  28%|████▋            | 11/40 [00:02<00:08,  3.55it/s]Epoch: 9, train for the 53-th batch, train loss: 0.6123101115226746:  34%|████▍        | 52/151 [00:11<00:22,  4.46it/s]Epoch: 9, train for the 53-th batch, train loss: 0.6123101115226746:  35%|████▌        | 53/151 [00:11<00:22,  4.45it/s]Epoch: 3, train for the 159-th batch, train loss: 0.6338238716125488:  67%|███████▎   | 158/237 [01:33<00:51,  1.53it/s]Epoch: 3, train for the 159-th batch, train loss: 0.6338238716125488:  67%|███████▍   | 159/237 [01:33<00:50,  1.53it/s]Epoch: 2, train for the 250-th batch, train loss: 0.4875037372112274:  65%|███████▏   | 249/383 [02:28<01:27,  1.53it/s]Epoch: 2, train for the 250-th batch, train loss: 0.4875037372112274:  65%|███████▏   | 250/383 [02:28<01:26,  1.53it/s]evaluate for the 12-th batch, evaluate loss: 0.1559930443763733:  28%|████▉             | 11/40 [00:03<00:08,  3.55it/s]evaluate for the 12-th batch, evaluate loss: 0.1559930443763733:  30%|█████▍            | 12/40 [00:03<00:07,  3.69it/s]Epoch: 9, train for the 54-th batch, train loss: 0.5768105387687683:  35%|████▌        | 53/151 [00:11<00:22,  4.45it/s]Epoch: 9, train for the 54-th batch, train loss: 0.5768105387687683:  36%|████▋        | 54/151 [00:11<00:21,  4.44it/s]Epoch: 5, train for the 21-th batch, train loss: 0.45848700404167175:  14%|█▋          | 20/146 [00:12<01:16,  1.65it/s]Epoch: 5, train for the 21-th batch, train loss: 0.45848700404167175:  14%|█▋          | 21/146 [00:12<01:15,  1.66it/s]Epoch: 9, train for the 55-th batch, train loss: 0.5652515888214111:  36%|████▋        | 54/151 [00:11<00:21,  4.44it/s]Epoch: 9, train for the 55-th batch, train loss: 0.5652515888214111:  36%|████▋        | 55/151 [00:11<00:21,  4.44it/s]evaluate for the 13-th batch, evaluate loss: 0.12157157808542252:  30%|█████            | 12/40 [00:03<00:07,  3.69it/s]evaluate for the 13-th batch, evaluate loss: 0.12157157808542252:  32%|█████▌           | 13/40 [00:03<00:07,  3.56it/s]Epoch: 9, train for the 56-th batch, train loss: 0.47890135645866394:  36%|████▎       | 55/151 [00:12<00:21,  4.44it/s]Epoch: 9, train for the 56-th batch, train loss: 0.47890135645866394:  37%|████▍       | 56/151 [00:12<00:21,  4.44it/s]Epoch: 3, train for the 160-th batch, train loss: 0.6376662850379944:  67%|███████▍   | 159/237 [01:34<00:50,  1.53it/s]Epoch: 3, train for the 160-th batch, train loss: 0.6376662850379944:  68%|███████▍   | 160/237 [01:34<00:50,  1.53it/s]Epoch: 2, train for the 251-th batch, train loss: 0.489092618227005:  65%|███████▊    | 250/383 [02:29<01:26,  1.53it/s]Epoch: 2, train for the 251-th batch, train loss: 0.489092618227005:  66%|███████▊    | 251/383 [02:29<01:26,  1.53it/s]evaluate for the 14-th batch, evaluate loss: 0.16639308631420135:  32%|█████▌           | 13/40 [00:03<00:07,  3.56it/s]evaluate for the 14-th batch, evaluate loss: 0.16639308631420135:  35%|█████▉           | 14/40 [00:03<00:06,  3.72it/s]Epoch: 9, train for the 57-th batch, train loss: 0.5307884812355042:  37%|████▊        | 56/151 [00:12<00:21,  4.44it/s]Epoch: 9, train for the 57-th batch, train loss: 0.5307884812355042:  38%|████▉        | 57/151 [00:12<00:21,  4.41it/s]Epoch: 5, train for the 22-th batch, train loss: 0.44281744956970215:  14%|█▋          | 21/146 [00:12<01:15,  1.66it/s]Epoch: 5, train for the 22-th batch, train loss: 0.44281744956970215:  15%|█▊          | 22/146 [00:12<01:15,  1.65it/s]evaluate for the 15-th batch, evaluate loss: 0.190301313996315:  35%|██████▋            | 14/40 [00:03<00:06,  3.72it/s]evaluate for the 15-th batch, evaluate loss: 0.190301313996315:  38%|███████▏           | 15/40 [00:03<00:07,  3.54it/s]Epoch: 9, train for the 58-th batch, train loss: 0.5321800112724304:  38%|████▉        | 57/151 [00:12<00:21,  4.41it/s]Epoch: 9, train for the 58-th batch, train loss: 0.5321800112724304:  38%|████▉        | 58/151 [00:12<00:21,  4.41it/s]evaluate for the 16-th batch, evaluate loss: 0.1995563805103302:  38%|██████▊           | 15/40 [00:04<00:07,  3.54it/s]evaluate for the 16-th batch, evaluate loss: 0.1995563805103302:  40%|███████▏          | 16/40 [00:04<00:06,  3.59it/s]Epoch: 9, train for the 59-th batch, train loss: 0.5214323997497559:  38%|████▉        | 58/151 [00:12<00:21,  4.41it/s]Epoch: 9, train for the 59-th batch, train loss: 0.5214323997497559:  39%|█████        | 59/151 [00:12<00:20,  4.43it/s]Epoch: 3, train for the 161-th batch, train loss: 0.644274115562439:  68%|████████    | 160/237 [01:34<00:50,  1.53it/s]Epoch: 3, train for the 161-th batch, train loss: 0.644274115562439:  68%|████████▏   | 161/237 [01:34<00:49,  1.54it/s]Epoch: 2, train for the 252-th batch, train loss: 0.3965400755405426:  66%|███████▏   | 251/383 [02:30<01:26,  1.53it/s]Epoch: 2, train for the 252-th batch, train loss: 0.3965400755405426:  66%|███████▏   | 252/383 [02:30<01:25,  1.54it/s]Epoch: 9, train for the 60-th batch, train loss: 0.525181233882904:  39%|█████▍        | 59/151 [00:12<00:20,  4.43it/s]Epoch: 9, train for the 60-th batch, train loss: 0.525181233882904:  40%|█████▌        | 60/151 [00:12<00:20,  4.42it/s]Epoch: 5, train for the 23-th batch, train loss: 0.44709861278533936:  15%|█▊          | 22/146 [00:13<01:15,  1.65it/s]Epoch: 5, train for the 23-th batch, train loss: 0.44709861278533936:  16%|█▉          | 23/146 [00:13<01:14,  1.65it/s]evaluate for the 17-th batch, evaluate loss: 0.14173945784568787:  40%|██████▊          | 16/40 [00:04<00:06,  3.59it/s]evaluate for the 17-th batch, evaluate loss: 0.14173945784568787:  42%|███████▏         | 17/40 [00:04<00:06,  3.45it/s]Epoch: 9, train for the 61-th batch, train loss: 0.5193507075309753:  40%|█████▏       | 60/151 [00:13<00:20,  4.42it/s]Epoch: 9, train for the 61-th batch, train loss: 0.5193507075309753:  40%|█████▎       | 61/151 [00:13<00:20,  4.43it/s]evaluate for the 18-th batch, evaluate loss: 0.1614178866147995:  42%|███████▋          | 17/40 [00:04<00:06,  3.45it/s]evaluate for the 18-th batch, evaluate loss: 0.1614178866147995:  45%|████████          | 18/40 [00:04<00:06,  3.51it/s]Epoch: 9, train for the 62-th batch, train loss: 0.4185386300086975:  40%|█████▎       | 61/151 [00:13<00:20,  4.43it/s]Epoch: 9, train for the 62-th batch, train loss: 0.4185386300086975:  41%|█████▎       | 62/151 [00:13<00:19,  4.51it/s]Epoch: 3, train for the 162-th batch, train loss: 0.6456271409988403:  68%|███████▍   | 161/237 [01:35<00:49,  1.54it/s]Epoch: 3, train for the 162-th batch, train loss: 0.6456271409988403:  68%|███████▌   | 162/237 [01:35<00:48,  1.54it/s]Epoch: 2, train for the 253-th batch, train loss: 0.38154223561286926:  66%|██████▌   | 252/383 [02:30<01:25,  1.54it/s]Epoch: 2, train for the 253-th batch, train loss: 0.38154223561286926:  66%|██████▌   | 253/383 [02:30<01:24,  1.54it/s]evaluate for the 19-th batch, evaluate loss: 0.16694790124893188:  45%|███████▋         | 18/40 [00:05<00:06,  3.51it/s]evaluate for the 19-th batch, evaluate loss: 0.16694790124893188:  48%|████████         | 19/40 [00:05<00:06,  3.46it/s]Epoch: 5, train for the 24-th batch, train loss: 0.436540812253952:  16%|██▏           | 23/146 [00:14<01:14,  1.65it/s]Epoch: 5, train for the 24-th batch, train loss: 0.436540812253952:  16%|██▎           | 24/146 [00:14<01:13,  1.66it/s]Epoch: 9, train for the 63-th batch, train loss: 0.524692952632904:  41%|█████▋        | 62/151 [00:13<00:19,  4.51it/s]Epoch: 9, train for the 63-th batch, train loss: 0.524692952632904:  42%|█████▊        | 63/151 [00:13<00:19,  4.50it/s]evaluate for the 20-th batch, evaluate loss: 0.15559960901737213:  48%|████████         | 19/40 [00:05<00:06,  3.46it/s]evaluate for the 20-th batch, evaluate loss: 0.15559960901737213:  50%|████████▌        | 20/40 [00:05<00:05,  3.59it/s]Epoch: 9, train for the 64-th batch, train loss: 0.5295727252960205:  42%|█████▍       | 63/151 [00:13<00:19,  4.50it/s]Epoch: 9, train for the 64-th batch, train loss: 0.5295727252960205:  42%|█████▌       | 64/151 [00:13<00:19,  4.50it/s]Epoch: 3, train for the 163-th batch, train loss: 0.6216128468513489:  68%|███████▌   | 162/237 [01:35<00:48,  1.54it/s]Epoch: 3, train for the 163-th batch, train loss: 0.6216128468513489:  69%|███████▌   | 163/237 [01:35<00:48,  1.54it/s]Epoch: 2, train for the 254-th batch, train loss: 0.4534555673599243:  66%|███████▎   | 253/383 [02:31<01:24,  1.54it/s]Epoch: 2, train for the 254-th batch, train loss: 0.4534555673599243:  66%|███████▎   | 254/383 [02:31<01:23,  1.54it/s]Epoch: 9, train for the 65-th batch, train loss: 0.5068939328193665:  42%|█████▌       | 64/151 [00:14<00:19,  4.50it/s]Epoch: 9, train for the 65-th batch, train loss: 0.5068939328193665:  43%|█████▌       | 65/151 [00:14<00:19,  4.50it/s]evaluate for the 21-th batch, evaluate loss: 0.09705747663974762:  50%|████████▌        | 20/40 [00:05<00:05,  3.59it/s]evaluate for the 21-th batch, evaluate loss: 0.09705747663974762:  52%|████████▉        | 21/40 [00:05<00:05,  3.53it/s]Epoch: 5, train for the 25-th batch, train loss: 0.45149123668670654:  16%|█▉          | 24/146 [00:14<01:13,  1.66it/s]Epoch: 5, train for the 25-th batch, train loss: 0.45149123668670654:  17%|██          | 25/146 [00:14<01:12,  1.67it/s]Epoch: 9, train for the 66-th batch, train loss: 0.4490104615688324:  43%|█████▌       | 65/151 [00:14<00:19,  4.50it/s]Epoch: 9, train for the 66-th batch, train loss: 0.4490104615688324:  44%|█████▋       | 66/151 [00:14<00:18,  4.54it/s]evaluate for the 22-th batch, evaluate loss: 0.15708979964256287:  52%|████████▉        | 21/40 [00:05<00:05,  3.53it/s]evaluate for the 22-th batch, evaluate loss: 0.15708979964256287:  55%|█████████▎       | 22/40 [00:05<00:04,  3.62it/s]Epoch: 9, train for the 67-th batch, train loss: 0.5778714418411255:  44%|█████▋       | 66/151 [00:14<00:18,  4.54it/s]Epoch: 9, train for the 67-th batch, train loss: 0.5778714418411255:  44%|█████▊       | 67/151 [00:14<00:18,  4.53it/s]evaluate for the 23-th batch, evaluate loss: 0.1638978123664856:  55%|█████████▉        | 22/40 [00:06<00:04,  3.62it/s]evaluate for the 23-th batch, evaluate loss: 0.1638978123664856:  57%|██████████▎       | 23/40 [00:06<00:04,  3.58it/s]Epoch: 3, train for the 164-th batch, train loss: 0.6478034853935242:  69%|███████▌   | 163/237 [01:36<00:48,  1.54it/s]Epoch: 3, train for the 164-th batch, train loss: 0.6478034853935242:  69%|███████▌   | 164/237 [01:36<00:47,  1.54it/s]Epoch: 2, train for the 255-th batch, train loss: 0.4184395968914032:  66%|███████▎   | 254/383 [02:32<01:23,  1.54it/s]Epoch: 2, train for the 255-th batch, train loss: 0.4184395968914032:  67%|███████▎   | 255/383 [02:32<01:22,  1.54it/s]Epoch: 9, train for the 68-th batch, train loss: 0.5482102036476135:  44%|█████▊       | 67/151 [00:14<00:18,  4.53it/s]Epoch: 9, train for the 68-th batch, train loss: 0.5482102036476135:  45%|█████▊       | 68/151 [00:14<00:18,  4.52it/s]Epoch: 5, train for the 26-th batch, train loss: 0.39574363827705383:  17%|██          | 25/146 [00:15<01:12,  1.67it/s]Epoch: 5, train for the 26-th batch, train loss: 0.39574363827705383:  18%|██▏         | 26/146 [00:15<01:11,  1.68it/s]Epoch: 9, train for the 69-th batch, train loss: 0.5577878355979919:  45%|█████▊       | 68/151 [00:14<00:18,  4.52it/s]Epoch: 9, train for the 69-th batch, train loss: 0.5577878355979919:  46%|█████▉       | 69/151 [00:14<00:18,  4.52it/s]evaluate for the 24-th batch, evaluate loss: 0.15071231126785278:  57%|█████████▊       | 23/40 [00:06<00:04,  3.58it/s]evaluate for the 24-th batch, evaluate loss: 0.15071231126785278:  60%|██████████▏      | 24/40 [00:06<00:04,  3.63it/s]Epoch: 9, train for the 70-th batch, train loss: 0.5564550161361694:  46%|█████▉       | 69/151 [00:15<00:18,  4.52it/s]Epoch: 9, train for the 70-th batch, train loss: 0.5564550161361694:  46%|██████       | 70/151 [00:15<00:17,  4.51it/s]evaluate for the 25-th batch, evaluate loss: 0.15364734828472137:  60%|██████████▏      | 24/40 [00:06<00:04,  3.63it/s]evaluate for the 25-th batch, evaluate loss: 0.15364734828472137:  62%|██████████▋      | 25/40 [00:06<00:04,  3.57it/s]Epoch: 3, train for the 165-th batch, train loss: 0.6336808800697327:  69%|███████▌   | 164/237 [01:37<00:47,  1.54it/s]Epoch: 3, train for the 165-th batch, train loss: 0.6336808800697327:  70%|███████▋   | 165/237 [01:37<00:46,  1.54it/s]Epoch: 2, train for the 256-th batch, train loss: 0.4432174563407898:  67%|███████▎   | 255/383 [02:32<01:22,  1.54it/s]Epoch: 2, train for the 256-th batch, train loss: 0.4432174563407898:  67%|███████▎   | 256/383 [02:32<01:22,  1.54it/s]Epoch: 9, train for the 71-th batch, train loss: 0.50846928358078:  46%|██████▉        | 70/151 [00:15<00:17,  4.51it/s]Epoch: 9, train for the 71-th batch, train loss: 0.50846928358078:  47%|███████        | 71/151 [00:15<00:17,  4.55it/s]Epoch: 5, train for the 27-th batch, train loss: 0.4406994581222534:  18%|██▎          | 26/146 [00:15<01:11,  1.68it/s]Epoch: 5, train for the 27-th batch, train loss: 0.4406994581222534:  18%|██▍          | 27/146 [00:15<01:11,  1.67it/s]evaluate for the 26-th batch, evaluate loss: 0.1336321085691452:  62%|███████████▎      | 25/40 [00:07<00:04,  3.57it/s]evaluate for the 26-th batch, evaluate loss: 0.1336321085691452:  65%|███████████▋      | 26/40 [00:07<00:03,  3.50it/s]Epoch: 9, train for the 72-th batch, train loss: 0.4706849157810211:  47%|██████       | 71/151 [00:15<00:17,  4.55it/s]Epoch: 9, train for the 72-th batch, train loss: 0.4706849157810211:  48%|██████▏      | 72/151 [00:15<00:17,  4.56it/s]evaluate for the 27-th batch, evaluate loss: 0.15136006474494934:  65%|███████████      | 26/40 [00:07<00:03,  3.50it/s]evaluate for the 27-th batch, evaluate loss: 0.15136006474494934:  68%|███████████▍     | 27/40 [00:07<00:03,  3.54it/s]Epoch: 9, train for the 73-th batch, train loss: 0.531645655632019:  48%|██████▋       | 72/151 [00:15<00:17,  4.56it/s]Epoch: 9, train for the 73-th batch, train loss: 0.531645655632019:  48%|██████▊       | 73/151 [00:15<00:17,  4.54it/s]Epoch: 5, train for the 28-th batch, train loss: 0.43445873260498047:  18%|██▏         | 27/146 [00:16<01:11,  1.67it/s]Epoch: 5, train for the 28-th batch, train loss: 0.43445873260498047:  19%|██▎         | 28/146 [00:16<01:10,  1.67it/s]Epoch: 3, train for the 166-th batch, train loss: 0.6032906770706177:  70%|███████▋   | 165/237 [01:37<00:46,  1.54it/s]Epoch: 3, train for the 166-th batch, train loss: 0.6032906770706177:  70%|███████▋   | 166/237 [01:37<00:46,  1.54it/s]Epoch: 2, train for the 257-th batch, train loss: 0.5057531595230103:  67%|███████▎   | 256/383 [02:33<01:22,  1.54it/s]Epoch: 2, train for the 257-th batch, train loss: 0.5057531595230103:  67%|███████▍   | 257/383 [02:33<01:21,  1.54it/s]Epoch: 9, train for the 74-th batch, train loss: 0.5223198533058167:  48%|██████▎      | 73/151 [00:16<00:17,  4.54it/s]Epoch: 9, train for the 74-th batch, train loss: 0.5223198533058167:  49%|██████▎      | 74/151 [00:16<00:17,  4.53it/s]evaluate for the 28-th batch, evaluate loss: 0.12076772749423981:  68%|███████████▍     | 27/40 [00:07<00:03,  3.54it/s]evaluate for the 28-th batch, evaluate loss: 0.12076772749423981:  70%|███████████▉     | 28/40 [00:07<00:03,  3.48it/s]Epoch: 9, train for the 75-th batch, train loss: 0.5346510410308838:  49%|██████▎      | 74/151 [00:16<00:17,  4.53it/s]Epoch: 9, train for the 75-th batch, train loss: 0.5346510410308838:  50%|██████▍      | 75/151 [00:16<00:16,  4.52it/s]evaluate for the 29-th batch, evaluate loss: 0.16834889352321625:  70%|███████████▉     | 28/40 [00:07<00:03,  3.48it/s]evaluate for the 29-th batch, evaluate loss: 0.16834889352321625:  72%|████████████▎    | 29/40 [00:07<00:03,  3.62it/s]Epoch: 9, train for the 76-th batch, train loss: 0.39264941215515137:  50%|█████▉      | 75/151 [00:16<00:16,  4.52it/s]Epoch: 9, train for the 76-th batch, train loss: 0.39264941215515137:  50%|██████      | 76/151 [00:16<00:16,  4.58it/s]Epoch: 5, train for the 29-th batch, train loss: 0.4557742476463318:  19%|██▍          | 28/146 [00:17<01:10,  1.67it/s]Epoch: 5, train for the 29-th batch, train loss: 0.4557742476463318:  20%|██▌          | 29/146 [00:17<01:09,  1.68it/s]Epoch: 3, train for the 167-th batch, train loss: 0.6304107904434204:  70%|███████▋   | 166/237 [01:38<00:46,  1.54it/s]Epoch: 2, train for the 258-th batch, train loss: 0.4499462842941284:  67%|███████▍   | 257/383 [02:34<01:21,  1.54it/s]Epoch: 3, train for the 167-th batch, train loss: 0.6304107904434204:  70%|███████▊   | 167/237 [01:38<00:45,  1.54it/s]Epoch: 2, train for the 258-th batch, train loss: 0.4499462842941284:  67%|███████▍   | 258/383 [02:34<01:21,  1.54it/s]evaluate for the 30-th batch, evaluate loss: 0.18011817336082458:  72%|████████████▎    | 29/40 [00:08<00:03,  3.62it/s]evaluate for the 30-th batch, evaluate loss: 0.18011817336082458:  75%|████████████▊    | 30/40 [00:08<00:02,  3.52it/s]Epoch: 9, train for the 77-th batch, train loss: 0.5551550984382629:  50%|██████▌      | 76/151 [00:16<00:16,  4.58it/s]Epoch: 9, train for the 77-th batch, train loss: 0.5551550984382629:  51%|██████▋      | 77/151 [00:16<00:16,  4.56it/s]evaluate for the 31-th batch, evaluate loss: 0.17165042459964752:  75%|████████████▊    | 30/40 [00:08<00:02,  3.52it/s]evaluate for the 31-th batch, evaluate loss: 0.17165042459964752:  78%|█████████████▏   | 31/40 [00:08<00:02,  3.71it/s]Epoch: 9, train for the 78-th batch, train loss: 0.5398215055465698:  51%|██████▋      | 77/151 [00:16<00:16,  4.56it/s]Epoch: 9, train for the 78-th batch, train loss: 0.5398215055465698:  52%|██████▋      | 78/151 [00:16<00:16,  4.53it/s]Epoch: 5, train for the 30-th batch, train loss: 0.4392595589160919:  20%|██▌          | 29/146 [00:17<01:09,  1.68it/s]Epoch: 5, train for the 30-th batch, train loss: 0.4392595589160919:  21%|██▋          | 30/146 [00:17<01:09,  1.67it/s]evaluate for the 32-th batch, evaluate loss: 0.13815729320049286:  78%|█████████████▏   | 31/40 [00:08<00:02,  3.71it/s]evaluate for the 32-th batch, evaluate loss: 0.13815729320049286:  80%|█████████████▌   | 32/40 [00:08<00:02,  3.53it/s]Epoch: 9, train for the 79-th batch, train loss: 0.5446762442588806:  52%|██████▋      | 78/151 [00:17<00:16,  4.53it/s]Epoch: 9, train for the 79-th batch, train loss: 0.5446762442588806:  52%|██████▊      | 79/151 [00:17<00:17,  4.08it/s]Epoch: 3, train for the 168-th batch, train loss: 0.6356604099273682:  70%|███████▊   | 167/237 [01:39<00:45,  1.54it/s]Epoch: 2, train for the 259-th batch, train loss: 0.43465614318847656:  67%|██████▋   | 258/383 [02:34<01:21,  1.54it/s]Epoch: 3, train for the 168-th batch, train loss: 0.6356604099273682:  71%|███████▊   | 168/237 [01:39<00:44,  1.54it/s]Epoch: 2, train for the 259-th batch, train loss: 0.43465614318847656:  68%|██████▊   | 259/383 [02:34<01:20,  1.54it/s]Epoch: 9, train for the 80-th batch, train loss: 0.5825155377388:  52%|████████▎       | 79/151 [00:17<00:17,  4.08it/s]Epoch: 9, train for the 80-th batch, train loss: 0.5825155377388:  53%|████████▍       | 80/151 [00:17<00:16,  4.19it/s]evaluate for the 33-th batch, evaluate loss: 0.13753549754619598:  80%|█████████████▌   | 32/40 [00:08<00:02,  3.53it/s]evaluate for the 33-th batch, evaluate loss: 0.13753549754619598:  82%|██████████████   | 33/40 [00:08<00:01,  3.60it/s]Epoch: 9, train for the 81-th batch, train loss: 0.5992531180381775:  53%|██████▉      | 80/151 [00:17<00:16,  4.19it/s]Epoch: 9, train for the 81-th batch, train loss: 0.5992531180381775:  54%|██████▉      | 81/151 [00:17<00:16,  4.27it/s]Epoch: 5, train for the 31-th batch, train loss: 0.4647373557090759:  21%|██▋          | 30/146 [00:18<01:09,  1.67it/s]Epoch: 5, train for the 31-th batch, train loss: 0.4647373557090759:  21%|██▊          | 31/146 [00:18<01:09,  1.66it/s]evaluate for the 34-th batch, evaluate loss: 0.11724384874105453:  82%|██████████████   | 33/40 [00:09<00:01,  3.60it/s]evaluate for the 34-th batch, evaluate loss: 0.11724384874105453:  85%|██████████████▍  | 34/40 [00:09<00:01,  3.45it/s]Epoch: 9, train for the 82-th batch, train loss: 0.5572390556335449:  54%|██████▉      | 81/151 [00:17<00:16,  4.27it/s]Epoch: 9, train for the 82-th batch, train loss: 0.5572390556335449:  54%|███████      | 82/151 [00:17<00:15,  4.34it/s]Epoch: 3, train for the 169-th batch, train loss: 0.6383269429206848:  71%|███████▊   | 168/237 [01:39<00:44,  1.54it/s]Epoch: 2, train for the 260-th batch, train loss: 0.4439360797405243:  68%|███████▍   | 259/383 [02:35<01:20,  1.54it/s]Epoch: 3, train for the 169-th batch, train loss: 0.6383269429206848:  71%|███████▊   | 169/237 [01:39<00:44,  1.53it/s]Epoch: 2, train for the 260-th batch, train loss: 0.4439360797405243:  68%|███████▍   | 260/383 [02:35<01:20,  1.53it/s]evaluate for the 35-th batch, evaluate loss: 0.16519583761692047:  85%|██████████████▍  | 34/40 [00:09<00:01,  3.45it/s]evaluate for the 35-th batch, evaluate loss: 0.16519583761692047:  88%|██████████████▉  | 35/40 [00:09<00:01,  3.50it/s]Epoch: 9, train for the 83-th batch, train loss: 0.5213859677314758:  54%|███████      | 82/151 [00:18<00:15,  4.34it/s]Epoch: 9, train for the 83-th batch, train loss: 0.5213859677314758:  55%|███████▏     | 83/151 [00:18<00:15,  4.38it/s]Epoch: 9, train for the 84-th batch, train loss: 0.5830040574073792:  55%|███████▏     | 83/151 [00:18<00:15,  4.38it/s]Epoch: 9, train for the 84-th batch, train loss: 0.5830040574073792:  56%|███████▏     | 84/151 [00:18<00:15,  4.42it/s]evaluate for the 36-th batch, evaluate loss: 0.15589764714241028:  88%|██████████████▉  | 35/40 [00:09<00:01,  3.50it/s]evaluate for the 36-th batch, evaluate loss: 0.15589764714241028:  90%|███████████████▎ | 36/40 [00:09<00:01,  3.46it/s]Epoch: 5, train for the 32-th batch, train loss: 0.43639037013053894:  21%|██▌         | 31/146 [00:18<01:09,  1.66it/s]Epoch: 5, train for the 32-th batch, train loss: 0.43639037013053894:  22%|██▋         | 32/146 [00:18<01:07,  1.68it/s]Epoch: 9, train for the 85-th batch, train loss: 0.5555559992790222:  56%|███████▏     | 84/151 [00:18<00:15,  4.42it/s]Epoch: 9, train for the 85-th batch, train loss: 0.5555559992790222:  56%|███████▎     | 85/151 [00:18<00:14,  4.44it/s]Epoch: 3, train for the 170-th batch, train loss: 0.6488853096961975:  71%|███████▊   | 169/237 [01:40<00:44,  1.53it/s]Epoch: 2, train for the 261-th batch, train loss: 0.43432989716529846:  68%|██████▊   | 260/383 [02:36<01:20,  1.53it/s]Epoch: 3, train for the 170-th batch, train loss: 0.6488853096961975:  72%|███████▉   | 170/237 [01:40<00:43,  1.53it/s]Epoch: 2, train for the 261-th batch, train loss: 0.43432989716529846:  68%|██████▊   | 261/383 [02:36<01:19,  1.53it/s]evaluate for the 37-th batch, evaluate loss: 0.2092789262533188:  90%|████████████████▏ | 36/40 [00:10<00:01,  3.46it/s]evaluate for the 37-th batch, evaluate loss: 0.2092789262533188:  92%|████████████████▋ | 37/40 [00:10<00:00,  3.58it/s]Epoch: 9, train for the 86-th batch, train loss: 0.5488818287849426:  56%|███████▎     | 85/151 [00:18<00:14,  4.44it/s]Epoch: 9, train for the 86-th batch, train loss: 0.5488818287849426:  57%|███████▍     | 86/151 [00:18<00:14,  4.46it/s]evaluate for the 38-th batch, evaluate loss: 0.148042231798172:  92%|█████████████████▌ | 37/40 [00:10<00:00,  3.58it/s]evaluate for the 38-th batch, evaluate loss: 0.148042231798172:  95%|██████████████████ | 38/40 [00:10<00:00,  3.53it/s]Epoch: 5, train for the 33-th batch, train loss: 0.47887754440307617:  22%|██▋         | 32/146 [00:19<01:07,  1.68it/s]Epoch: 5, train for the 33-th batch, train loss: 0.47887754440307617:  23%|██▋         | 33/146 [00:19<01:06,  1.69it/s]Epoch: 9, train for the 87-th batch, train loss: 0.5377835631370544:  57%|███████▍     | 86/151 [00:18<00:14,  4.46it/s]Epoch: 9, train for the 87-th batch, train loss: 0.5377835631370544:  58%|███████▍     | 87/151 [00:18<00:14,  4.47it/s]evaluate for the 39-th batch, evaluate loss: 0.16470462083816528:  95%|████████████████▏| 38/40 [00:10<00:00,  3.53it/s]evaluate for the 39-th batch, evaluate loss: 0.16470462083816528:  98%|████████████████▌| 39/40 [00:10<00:00,  3.63it/s]evaluate for the 40-th batch, evaluate loss: 0.06824970245361328:  98%|████████████████▌| 39/40 [00:10<00:00,  3.63it/s]evaluate for the 40-th batch, evaluate loss: 0.06824970245361328: 100%|█████████████████| 40/40 [00:10<00:00,  3.75it/s]
  0%|                                                                                            | 0/21 [00:00<?, ?it/s]Epoch: 9, train for the 88-th batch, train loss: 0.5874790549278259:  58%|███████▍     | 87/151 [00:19<00:14,  4.47it/s]Epoch: 9, train for the 88-th batch, train loss: 0.5874790549278259:  58%|███████▌     | 88/151 [00:19<00:14,  4.48it/s]Epoch: 3, train for the 171-th batch, train loss: 0.6384881138801575:  72%|███████▉   | 170/237 [01:41<00:43,  1.53it/s]Epoch: 2, train for the 262-th batch, train loss: 0.4473519027233124:  68%|███████▍   | 261/383 [02:36<01:19,  1.53it/s]Epoch: 3, train for the 171-th batch, train loss: 0.6384881138801575:  72%|███████▉   | 171/237 [01:41<00:43,  1.53it/s]Epoch: 2, train for the 262-th batch, train loss: 0.4473519027233124:  68%|███████▌   | 262/383 [02:36<01:19,  1.53it/s]Epoch: 9, train for the 89-th batch, train loss: 0.5485743880271912:  58%|███████▌     | 88/151 [00:19<00:14,  4.48it/s]Epoch: 9, train for the 89-th batch, train loss: 0.5485743880271912:  59%|███████▋     | 89/151 [00:19<00:13,  4.49it/s]evaluate for the 1-th batch, evaluate loss: 0.22263337671756744:   0%|                           | 0/21 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.22263337671756744:   5%|▉                  | 1/21 [00:00<00:06,  3.31it/s]Epoch: 5, train for the 34-th batch, train loss: 0.4423317611217499:  23%|██▉          | 33/146 [00:19<01:06,  1.69it/s]Epoch: 5, train for the 34-th batch, train loss: 0.4423317611217499:  23%|███          | 34/146 [00:19<01:05,  1.70it/s]Epoch: 9, train for the 90-th batch, train loss: 0.5477113127708435:  59%|███████▋     | 89/151 [00:19<00:13,  4.49it/s]Epoch: 9, train for the 90-th batch, train loss: 0.5477113127708435:  60%|███████▋     | 90/151 [00:19<00:13,  4.48it/s]evaluate for the 2-th batch, evaluate loss: 0.2627999186515808:   5%|▉                   | 1/21 [00:00<00:06,  3.31it/s]evaluate for the 2-th batch, evaluate loss: 0.2627999186515808:  10%|█▉                  | 2/21 [00:00<00:05,  3.59it/s]Epoch: 9, train for the 91-th batch, train loss: 0.5003871917724609:  60%|███████▋     | 90/151 [00:19<00:13,  4.48it/s]Epoch: 9, train for the 91-th batch, train loss: 0.5003871917724609:  60%|███████▊     | 91/151 [00:19<00:13,  4.49it/s]Epoch: 3, train for the 172-th batch, train loss: 0.6064073443412781:  72%|███████▉   | 171/237 [01:41<00:43,  1.53it/s]Epoch: 2, train for the 263-th batch, train loss: 0.5189255475997925:  68%|███████▌   | 262/383 [02:37<01:19,  1.53it/s]Epoch: 3, train for the 172-th batch, train loss: 0.6064073443412781:  73%|███████▉   | 172/237 [01:41<00:42,  1.53it/s]Epoch: 2, train for the 263-th batch, train loss: 0.5189255475997925:  69%|███████▌   | 263/383 [02:37<01:18,  1.53it/s]evaluate for the 3-th batch, evaluate loss: 0.2550426721572876:  10%|█▉                  | 2/21 [00:00<00:05,  3.59it/s]evaluate for the 3-th batch, evaluate loss: 0.2550426721572876:  14%|██▊                 | 3/21 [00:00<00:05,  3.46it/s]Epoch: 5, train for the 35-th batch, train loss: 0.4475908577442169:  23%|███          | 34/146 [00:20<01:05,  1.70it/s]Epoch: 5, train for the 35-th batch, train loss: 0.4475908577442169:  24%|███          | 35/146 [00:20<01:04,  1.72it/s]Epoch: 9, train for the 92-th batch, train loss: 0.5558177828788757:  60%|███████▊     | 91/151 [00:20<00:13,  4.49it/s]Epoch: 9, train for the 92-th batch, train loss: 0.5558177828788757:  61%|███████▉     | 92/151 [00:20<00:14,  4.17it/s]evaluate for the 4-th batch, evaluate loss: 0.2242342084646225:  14%|██▊                 | 3/21 [00:01<00:05,  3.46it/s]evaluate for the 4-th batch, evaluate loss: 0.2242342084646225:  19%|███▊                | 4/21 [00:01<00:04,  3.59it/s]Epoch: 9, train for the 93-th batch, train loss: 0.5243489742279053:  61%|███████▉     | 92/151 [00:20<00:14,  4.17it/s]Epoch: 9, train for the 93-th batch, train loss: 0.5243489742279053:  62%|████████     | 93/151 [00:20<00:13,  4.26it/s]Epoch: 3, train for the 173-th batch, train loss: 0.6317738890647888:  73%|███████▉   | 172/237 [01:42<00:42,  1.53it/s]Epoch: 2, train for the 264-th batch, train loss: 0.40381452441215515:  69%|██████▊   | 263/383 [02:38<01:18,  1.53it/s]Epoch: 3, train for the 173-th batch, train loss: 0.6317738890647888:  73%|████████   | 173/237 [01:42<00:41,  1.53it/s]Epoch: 2, train for the 264-th batch, train loss: 0.40381452441215515:  69%|██████▉   | 264/383 [02:38<01:17,  1.53it/s]Epoch: 9, train for the 94-th batch, train loss: 0.5424424409866333:  62%|████████     | 93/151 [00:20<00:13,  4.26it/s]Epoch: 9, train for the 94-th batch, train loss: 0.5424424409866333:  62%|████████     | 94/151 [00:20<00:13,  4.32it/s]evaluate for the 5-th batch, evaluate loss: 0.243874654173851:  19%|████                 | 4/21 [00:01<00:04,  3.59it/s]evaluate for the 5-th batch, evaluate loss: 0.243874654173851:  24%|█████                | 5/21 [00:01<00:04,  3.49it/s]Epoch: 5, train for the 36-th batch, train loss: 0.44721660017967224:  24%|██▉         | 35/146 [00:21<01:04,  1.72it/s]Epoch: 5, train for the 36-th batch, train loss: 0.44721660017967224:  25%|██▉         | 36/146 [00:21<01:03,  1.72it/s]Epoch: 9, train for the 95-th batch, train loss: 0.4728132486343384:  62%|████████     | 94/151 [00:20<00:13,  4.32it/s]Epoch: 9, train for the 95-th batch, train loss: 0.4728132486343384:  63%|████████▏    | 95/151 [00:20<00:12,  4.39it/s]evaluate for the 6-th batch, evaluate loss: 0.2567193806171417:  24%|████▊               | 5/21 [00:01<00:04,  3.49it/s]evaluate for the 6-th batch, evaluate loss: 0.2567193806171417:  29%|█████▋              | 6/21 [00:01<00:04,  3.58it/s]Epoch: 9, train for the 96-th batch, train loss: 0.5778582692146301:  63%|████████▏    | 95/151 [00:21<00:12,  4.39it/s]Epoch: 9, train for the 96-th batch, train loss: 0.5778582692146301:  64%|████████▎    | 96/151 [00:21<00:12,  4.42it/s]evaluate for the 7-th batch, evaluate loss: 0.2482561469078064:  29%|█████▋              | 6/21 [00:01<00:04,  3.58it/s]evaluate for the 7-th batch, evaluate loss: 0.2482561469078064:  33%|██████▋             | 7/21 [00:01<00:03,  3.51it/s]Epoch: 3, train for the 174-th batch, train loss: 0.6137997508049011:  73%|████████   | 173/237 [01:43<00:41,  1.53it/s]Epoch: 2, train for the 265-th batch, train loss: 0.4047265648841858:  69%|███████▌   | 264/383 [02:38<01:17,  1.53it/s]Epoch: 3, train for the 174-th batch, train loss: 0.6137997508049011:  73%|████████   | 174/237 [01:43<00:41,  1.53it/s]Epoch: 2, train for the 265-th batch, train loss: 0.4047265648841858:  69%|███████▌   | 265/383 [02:38<01:17,  1.53it/s]Epoch: 5, train for the 37-th batch, train loss: 0.475458025932312:  25%|███▍          | 36/146 [00:21<01:03,  1.72it/s]Epoch: 5, train for the 37-th batch, train loss: 0.475458025932312:  25%|███▌          | 37/146 [00:21<01:02,  1.74it/s]Epoch: 9, train for the 97-th batch, train loss: 0.6207136511802673:  64%|████████▎    | 96/151 [00:21<00:12,  4.42it/s]Epoch: 9, train for the 97-th batch, train loss: 0.6207136511802673:  64%|████████▎    | 97/151 [00:21<00:12,  4.45it/s]evaluate for the 8-th batch, evaluate loss: 0.2988438308238983:  33%|██████▋             | 7/21 [00:02<00:03,  3.51it/s]evaluate for the 8-th batch, evaluate loss: 0.2988438308238983:  38%|███████▌            | 8/21 [00:02<00:03,  3.60it/s]Epoch: 9, train for the 98-th batch, train loss: 0.6189162135124207:  64%|████████▎    | 97/151 [00:21<00:12,  4.45it/s]Epoch: 9, train for the 98-th batch, train loss: 0.6189162135124207:  65%|████████▍    | 98/151 [00:21<00:11,  4.46it/s]Epoch: 9, train for the 99-th batch, train loss: 0.6280319094657898:  65%|████████▍    | 98/151 [00:21<00:11,  4.46it/s]Epoch: 9, train for the 99-th batch, train loss: 0.6280319094657898:  66%|████████▌    | 99/151 [00:21<00:11,  4.48it/s]evaluate for the 9-th batch, evaluate loss: 0.24250701069831848:  38%|███████▏           | 8/21 [00:02<00:03,  3.60it/s]evaluate for the 9-th batch, evaluate loss: 0.24250701069831848:  43%|████████▏          | 9/21 [00:02<00:03,  3.52it/s]Epoch: 5, train for the 38-th batch, train loss: 0.4622401297092438:  25%|███▎         | 37/146 [00:22<01:02,  1.74it/s]Epoch: 5, train for the 38-th batch, train loss: 0.4622401297092438:  26%|███▍         | 38/146 [00:22<01:02,  1.74it/s]Epoch: 3, train for the 175-th batch, train loss: 0.6524015665054321:  73%|████████   | 174/237 [01:43<00:41,  1.53it/s]Epoch: 3, train for the 175-th batch, train loss: 0.6524015665054321:  74%|████████   | 175/237 [01:43<00:40,  1.53it/s]Epoch: 2, train for the 266-th batch, train loss: 0.5047122836112976:  69%|███████▌   | 265/383 [02:39<01:17,  1.53it/s]Epoch: 2, train for the 266-th batch, train loss: 0.5047122836112976:  69%|███████▋   | 266/383 [02:39<01:16,  1.53it/s]Epoch: 9, train for the 100-th batch, train loss: 0.70151686668396:  66%|█████████▏    | 99/151 [00:21<00:11,  4.48it/s]Epoch: 9, train for the 100-th batch, train loss: 0.70151686668396:  66%|████████▌    | 100/151 [00:21<00:11,  4.48it/s]evaluate for the 10-th batch, evaluate loss: 0.23251678049564362:  43%|███████▋          | 9/21 [00:02<00:03,  3.52it/s]evaluate for the 10-th batch, evaluate loss: 0.23251678049564362:  48%|████████         | 10/21 [00:02<00:02,  3.70it/s]Epoch: 9, train for the 101-th batch, train loss: 0.664297342300415:  66%|███████▉    | 100/151 [00:22<00:11,  4.48it/s]Epoch: 9, train for the 101-th batch, train loss: 0.664297342300415:  67%|████████    | 101/151 [00:22<00:11,  4.49it/s]evaluate for the 11-th batch, evaluate loss: 0.1617242395877838:  48%|████████▌         | 10/21 [00:02<00:02,  3.70it/s]evaluate for the 11-th batch, evaluate loss: 0.1617242395877838:  52%|█████████▍        | 11/21 [00:02<00:02,  4.08it/s]Epoch: 9, train for the 102-th batch, train loss: 0.6221501231193542:  67%|███████▎   | 101/151 [00:22<00:11,  4.49it/s]Epoch: 9, train for the 102-th batch, train loss: 0.6221501231193542:  68%|███████▍   | 102/151 [00:22<00:10,  4.49it/s]evaluate for the 12-th batch, evaluate loss: 0.30642902851104736:  52%|████████▉        | 11/21 [00:03<00:02,  4.08it/s]evaluate for the 12-th batch, evaluate loss: 0.30642902851104736:  57%|█████████▋       | 12/21 [00:03<00:02,  3.81it/s]Epoch: 5, train for the 39-th batch, train loss: 0.4764823317527771:  26%|███▍         | 38/146 [00:22<01:02,  1.74it/s]Epoch: 5, train for the 39-th batch, train loss: 0.4764823317527771:  27%|███▍         | 39/146 [00:22<01:04,  1.66it/s]Epoch: 2, train for the 267-th batch, train loss: 0.4488067030906677:  69%|███████▋   | 266/383 [02:40<01:16,  1.53it/s]Epoch: 3, train for the 176-th batch, train loss: 0.621442973613739:  74%|████████▊   | 175/237 [01:44<00:40,  1.53it/s]Epoch: 2, train for the 267-th batch, train loss: 0.4488067030906677:  70%|███████▋   | 267/383 [02:40<01:15,  1.53it/s]Epoch: 3, train for the 176-th batch, train loss: 0.621442973613739:  74%|████████▉   | 176/237 [01:44<00:39,  1.53it/s]Epoch: 9, train for the 103-th batch, train loss: 0.6421580910682678:  68%|███████▍   | 102/151 [00:22<00:10,  4.49it/s]Epoch: 9, train for the 103-th batch, train loss: 0.6421580910682678:  68%|███████▌   | 103/151 [00:22<00:10,  4.40it/s]evaluate for the 13-th batch, evaluate loss: 0.24809785187244415:  57%|█████████▋       | 12/21 [00:03<00:02,  3.81it/s]evaluate for the 13-th batch, evaluate loss: 0.24809785187244415:  62%|██████████▌      | 13/21 [00:03<00:02,  3.82it/s]Epoch: 9, train for the 104-th batch, train loss: 0.6395981311798096:  68%|███████▌   | 103/151 [00:22<00:10,  4.40it/s]Epoch: 9, train for the 104-th batch, train loss: 0.6395981311798096:  69%|███████▌   | 104/151 [00:22<00:10,  4.40it/s]evaluate for the 14-th batch, evaluate loss: 0.20777903497219086:  62%|██████████▌      | 13/21 [00:03<00:02,  3.82it/s]evaluate for the 14-th batch, evaluate loss: 0.20777903497219086:  67%|███████████▎     | 14/21 [00:03<00:01,  3.65it/s]Epoch: 5, train for the 40-th batch, train loss: 0.4439338147640228:  27%|███▍         | 39/146 [00:23<01:04,  1.66it/s]Epoch: 5, train for the 40-th batch, train loss: 0.4439338147640228:  27%|███▌         | 40/146 [00:23<01:03,  1.68it/s]Epoch: 9, train for the 105-th batch, train loss: 0.5769307017326355:  69%|███████▌   | 104/151 [00:23<00:10,  4.40it/s]Epoch: 9, train for the 105-th batch, train loss: 0.5769307017326355:  70%|███████▋   | 105/151 [00:23<00:10,  4.37it/s]Epoch: 2, train for the 268-th batch, train loss: 0.4599744379520416:  70%|███████▋   | 267/383 [02:40<01:15,  1.53it/s]Epoch: 2, train for the 268-th batch, train loss: 0.4599744379520416:  70%|███████▋   | 268/383 [02:40<01:14,  1.54it/s]Epoch: 3, train for the 177-th batch, train loss: 0.65423184633255:  74%|█████████▋   | 176/237 [01:45<00:39,  1.53it/s]Epoch: 3, train for the 177-th batch, train loss: 0.65423184633255:  75%|█████████▋   | 177/237 [01:45<00:39,  1.53it/s]evaluate for the 15-th batch, evaluate loss: 0.2055617868900299:  67%|████████████      | 14/21 [00:04<00:01,  3.65it/s]evaluate for the 15-th batch, evaluate loss: 0.2055617868900299:  71%|████████████▊     | 15/21 [00:04<00:01,  3.71it/s]Epoch: 9, train for the 106-th batch, train loss: 0.5498248934745789:  70%|███████▋   | 105/151 [00:23<00:10,  4.37it/s]Epoch: 9, train for the 106-th batch, train loss: 0.5498248934745789:  70%|███████▋   | 106/151 [00:23<00:10,  4.39it/s]Epoch: 9, train for the 107-th batch, train loss: 0.5431613922119141:  70%|███████▋   | 106/151 [00:23<00:10,  4.39it/s]Epoch: 9, train for the 107-th batch, train loss: 0.5431613922119141:  71%|███████▊   | 107/151 [00:23<00:09,  4.40it/s]evaluate for the 16-th batch, evaluate loss: 0.25183749198913574:  71%|████████████▏    | 15/21 [00:04<00:01,  3.71it/s]evaluate for the 16-th batch, evaluate loss: 0.25183749198913574:  76%|████████████▉    | 16/21 [00:04<00:01,  3.58it/s]Epoch: 5, train for the 41-th batch, train loss: 0.4592665135860443:  27%|███▌         | 40/146 [00:24<01:03,  1.68it/s]Epoch: 5, train for the 41-th batch, train loss: 0.4592665135860443:  28%|███▋         | 41/146 [00:24<01:01,  1.70it/s]Epoch: 9, train for the 108-th batch, train loss: 0.5790389180183411:  71%|███████▊   | 107/151 [00:23<00:09,  4.40it/s]Epoch: 9, train for the 108-th batch, train loss: 0.5790389180183411:  72%|███████▊   | 108/151 [00:23<00:09,  4.39it/s]Epoch: 2, train for the 269-th batch, train loss: 0.5104362964630127:  70%|███████▋   | 268/383 [02:41<01:14,  1.54it/s]Epoch: 2, train for the 269-th batch, train loss: 0.5104362964630127:  70%|███████▋   | 269/383 [02:41<01:14,  1.54it/s]Epoch: 3, train for the 178-th batch, train loss: 0.620940625667572:  75%|████████▉   | 177/237 [01:45<00:39,  1.53it/s]Epoch: 3, train for the 178-th batch, train loss: 0.620940625667572:  75%|█████████   | 178/237 [01:45<00:38,  1.54it/s]evaluate for the 17-th batch, evaluate loss: 0.2410092055797577:  76%|█████████████▋    | 16/21 [00:04<00:01,  3.58it/s]evaluate for the 17-th batch, evaluate loss: 0.2410092055797577:  81%|██████████████▌   | 17/21 [00:04<00:01,  3.65it/s]Epoch: 9, train for the 109-th batch, train loss: 0.5584330558776855:  72%|███████▊   | 108/151 [00:23<00:09,  4.39it/s]Epoch: 9, train for the 109-th batch, train loss: 0.5584330558776855:  72%|███████▉   | 109/151 [00:23<00:09,  4.40it/s]evaluate for the 18-th batch, evaluate loss: 0.2280937135219574:  81%|██████████████▌   | 17/21 [00:04<00:01,  3.65it/s]evaluate for the 18-th batch, evaluate loss: 0.2280937135219574:  86%|███████████████▍  | 18/21 [00:04<00:00,  3.55it/s]Epoch: 5, train for the 42-th batch, train loss: 0.5070586800575256:  28%|███▋         | 41/146 [00:24<01:01,  1.70it/s]Epoch: 5, train for the 42-th batch, train loss: 0.5070586800575256:  29%|███▋         | 42/146 [00:24<01:00,  1.71it/s]Epoch: 9, train for the 110-th batch, train loss: 0.562973141670227:  72%|████████▋   | 109/151 [00:24<00:09,  4.40it/s]Epoch: 9, train for the 110-th batch, train loss: 0.562973141670227:  73%|████████▋   | 110/151 [00:24<00:09,  4.40it/s]evaluate for the 19-th batch, evaluate loss: 0.2900207042694092:  86%|███████████████▍  | 18/21 [00:05<00:00,  3.55it/s]evaluate for the 19-th batch, evaluate loss: 0.2900207042694092:  90%|████████████████▎ | 19/21 [00:05<00:00,  3.63it/s]Epoch: 9, train for the 111-th batch, train loss: 0.5164052248001099:  73%|████████   | 110/151 [00:24<00:09,  4.40it/s]Epoch: 9, train for the 111-th batch, train loss: 0.5164052248001099:  74%|████████   | 111/151 [00:24<00:09,  4.40it/s]Epoch: 2, train for the 270-th batch, train loss: 0.4228270649909973:  70%|███████▋   | 269/383 [02:41<01:14,  1.54it/s]Epoch: 2, train for the 270-th batch, train loss: 0.4228270649909973:  70%|███████▊   | 270/383 [02:41<01:13,  1.54it/s]Epoch: 3, train for the 179-th batch, train loss: 0.6636958122253418:  75%|████████▎  | 178/237 [01:46<00:38,  1.54it/s]Epoch: 3, train for the 179-th batch, train loss: 0.6636958122253418:  76%|████████▎  | 179/237 [01:46<00:37,  1.54it/s]Epoch: 9, train for the 112-th batch, train loss: 0.5171989798545837:  74%|████████   | 111/151 [00:24<00:09,  4.40it/s]Epoch: 9, train for the 112-th batch, train loss: 0.5171989798545837:  74%|████████▏  | 112/151 [00:24<00:08,  4.41it/s]evaluate for the 20-th batch, evaluate loss: 0.25741374492645264:  90%|███████████████▍ | 19/21 [00:05<00:00,  3.63it/s]evaluate for the 20-th batch, evaluate loss: 0.25741374492645264:  95%|████████████████▏| 20/21 [00:05<00:00,  3.55it/s]evaluate for the 21-th batch, evaluate loss: 0.1437613070011139:  95%|█████████████████▏| 20/21 [00:05<00:00,  3.55it/s]evaluate for the 21-th batch, evaluate loss: 0.1437613070011139: 100%|██████████████████| 21/21 [00:05<00:00,  3.77it/s]
INFO:root:Epoch: 5, learning rate: 0.0001, train loss: 0.2145
INFO:root:train average_precision, 0.9691
INFO:root:train roc_auc, 0.9587
INFO:root:validate loss: 0.1572
INFO:root:validate average_precision, 0.9851
INFO:root:validate roc_auc, 0.9820
INFO:root:new node validate loss: 0.2395
INFO:root:new node validate first_1_average_precision, 0.9061
INFO:root:new node validate first_1_roc_auc, 0.9042
INFO:root:new node validate first_3_average_precision, 0.9500
INFO:root:new node validate first_3_roc_auc, 0.9468
Epoch: 5, train for the 43-th batch, train loss: 0.46236494183540344:  29%|███▍        | 42/146 [00:25<01:00,  1.71it/s]INFO:root:new node validate first_10_average_precision, 0.9681
INFO:root:new node validate first_10_roc_auc, 0.9647
Epoch: 5, train for the 43-th batch, train loss: 0.46236494183540344:  29%|███▌        | 43/146 [00:25<00:59,  1.73it/s]INFO:root:new node validate average_precision, 0.9674
INFO:root:new node validate roc_auc, 0.9623
INFO:root:save model ./saved_models/DyGFormer/ia-reality-call/DyGFormer_seed0_dygformer-ia-reality-call-old/DyGFormer_seed0_dygformer-ia-reality-call-old.pkl
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]Epoch: 9, train for the 113-th batch, train loss: 0.5630783438682556:  74%|████████▏  | 112/151 [00:24<00:08,  4.41it/s]Epoch: 9, train for the 113-th batch, train loss: 0.5630783438682556:  75%|████████▏  | 113/151 [00:24<00:08,  4.39it/s]Epoch: 2, train for the 271-th batch, train loss: 0.44519850611686707:  70%|███████   | 270/383 [02:42<01:13,  1.54it/s]Epoch: 2, train for the 271-th batch, train loss: 0.44519850611686707:  71%|███████   | 271/383 [02:42<01:12,  1.55it/s]Epoch: 3, train for the 180-th batch, train loss: 0.6285149455070496:  76%|████████▎  | 179/237 [01:47<00:37,  1.54it/s]Epoch: 3, train for the 180-th batch, train loss: 0.6285149455070496:  76%|████████▎  | 180/237 [01:47<00:36,  1.55it/s]Epoch: 9, train for the 114-th batch, train loss: 0.527823269367218:  75%|████████▉   | 113/151 [00:25<00:08,  4.39it/s]Epoch: 9, train for the 114-th batch, train loss: 0.527823269367218:  75%|█████████   | 114/151 [00:25<00:08,  4.40it/s]Epoch: 5, train for the 44-th batch, train loss: 0.4704330265522003:  29%|███▊         | 43/146 [00:25<00:59,  1.73it/s]Epoch: 5, train for the 44-th batch, train loss: 0.4704330265522003:  30%|███▉         | 44/146 [00:25<00:53,  1.90it/s]Epoch: 6, train for the 1-th batch, train loss: 1.0336575508117676:   0%|                       | 0/119 [00:00<?, ?it/s]Epoch: 6, train for the 1-th batch, train loss: 1.0336575508117676:   1%|▏              | 1/119 [00:00<00:58,  2.02it/s]Epoch: 9, train for the 115-th batch, train loss: 0.5367102026939392:  75%|████████▎  | 114/151 [00:25<00:08,  4.40it/s]Epoch: 9, train for the 115-th batch, train loss: 0.5367102026939392:  76%|████████▍  | 115/151 [00:25<00:08,  4.41it/s]Epoch: 9, train for the 116-th batch, train loss: 0.5143539905548096:  76%|████████▍  | 115/151 [00:25<00:08,  4.41it/s]Epoch: 9, train for the 116-th batch, train loss: 0.5143539905548096:  77%|████████▍  | 116/151 [00:25<00:07,  4.42it/s]Epoch: 2, train for the 272-th batch, train loss: 0.3285983204841614:  71%|███████▊   | 271/383 [02:43<01:12,  1.55it/s]Epoch: 2, train for the 272-th batch, train loss: 0.3285983204841614:  71%|███████▊   | 272/383 [02:43<01:11,  1.55it/s]Epoch: 3, train for the 181-th batch, train loss: 0.6448906660079956:  76%|████████▎  | 180/237 [01:47<00:36,  1.55it/s]Epoch: 3, train for the 181-th batch, train loss: 0.6448906660079956:  76%|████████▍  | 181/237 [01:47<00:36,  1.55it/s]Epoch: 9, train for the 117-th batch, train loss: 0.47298744320869446:  77%|███████▋  | 116/151 [00:25<00:07,  4.42it/s]Epoch: 9, train for the 117-th batch, train loss: 0.47298744320869446:  77%|███████▋  | 117/151 [00:25<00:07,  4.42it/s]Epoch: 5, train for the 45-th batch, train loss: 0.45034903287887573:  30%|███▌        | 44/146 [00:26<00:53,  1.90it/s]Epoch: 5, train for the 45-th batch, train loss: 0.45034903287887573:  31%|███▋        | 45/146 [00:26<00:56,  1.78it/s]Epoch: 6, train for the 2-th batch, train loss: 0.513148307800293:   1%|▏               | 1/119 [00:01<00:58,  2.02it/s]Epoch: 6, train for the 2-th batch, train loss: 0.513148307800293:   2%|▎               | 2/119 [00:01<01:06,  1.77it/s]Epoch: 9, train for the 118-th batch, train loss: 0.5033663511276245:  77%|████████▌  | 117/151 [00:25<00:07,  4.42it/s]Epoch: 9, train for the 118-th batch, train loss: 0.5033663511276245:  78%|████████▌  | 118/151 [00:25<00:07,  4.42it/s]Epoch: 9, train for the 119-th batch, train loss: 0.5451027750968933:  78%|████████▌  | 118/151 [00:26<00:07,  4.42it/s]Epoch: 9, train for the 119-th batch, train loss: 0.5451027750968933:  79%|████████▋  | 119/151 [00:26<00:07,  4.42it/s]Epoch: 2, train for the 273-th batch, train loss: 0.4086690843105316:  71%|███████▊   | 272/383 [02:43<01:11,  1.55it/s]Epoch: 2, train for the 273-th batch, train loss: 0.4086690843105316:  71%|███████▊   | 273/383 [02:43<01:11,  1.55it/s]Epoch: 3, train for the 182-th batch, train loss: 0.6328442692756653:  76%|████████▍  | 181/237 [01:48<00:36,  1.55it/s]Epoch: 3, train for the 182-th batch, train loss: 0.6328442692756653:  77%|████████▍  | 182/237 [01:48<00:35,  1.55it/s]Epoch: 5, train for the 46-th batch, train loss: 0.4479641318321228:  31%|████         | 45/146 [00:26<00:56,  1.78it/s]Epoch: 5, train for the 46-th batch, train loss: 0.4479641318321228:  32%|████         | 46/146 [00:26<00:57,  1.73it/s]Epoch: 9, train for the 120-th batch, train loss: 0.5616269707679749:  79%|████████▋  | 119/151 [00:26<00:07,  4.42it/s]Epoch: 9, train for the 120-th batch, train loss: 0.5616269707679749:  79%|████████▋  | 120/151 [00:26<00:07,  4.40it/s]Epoch: 6, train for the 3-th batch, train loss: 0.3262340724468231:   2%|▎              | 2/119 [00:01<01:06,  1.77it/s]Epoch: 6, train for the 3-th batch, train loss: 0.3262340724468231:   3%|▍              | 3/119 [00:01<01:07,  1.72it/s]Epoch: 9, train for the 121-th batch, train loss: 0.49040481448173523:  79%|███████▉  | 120/151 [00:26<00:07,  4.40it/s]Epoch: 9, train for the 121-th batch, train loss: 0.49040481448173523:  80%|████████  | 121/151 [00:26<00:06,  4.41it/s]Epoch: 9, train for the 122-th batch, train loss: 0.5434585213661194:  80%|████████▊  | 121/151 [00:26<00:06,  4.41it/s]Epoch: 9, train for the 122-th batch, train loss: 0.5434585213661194:  81%|████████▉  | 122/151 [00:26<00:06,  4.36it/s]Epoch: 5, train for the 47-th batch, train loss: 0.5068445205688477:  32%|████         | 46/146 [00:27<00:57,  1.73it/s]Epoch: 5, train for the 47-th batch, train loss: 0.5068445205688477:  32%|████▏        | 47/146 [00:27<00:57,  1.72it/s]Epoch: 2, train for the 274-th batch, train loss: 0.4134982228279114:  71%|███████▊   | 273/383 [02:44<01:11,  1.55it/s]Epoch: 2, train for the 274-th batch, train loss: 0.4134982228279114:  72%|███████▊   | 274/383 [02:44<01:10,  1.55it/s]Epoch: 3, train for the 183-th batch, train loss: 0.638763427734375:  77%|█████████▏  | 182/237 [01:49<00:35,  1.55it/s]Epoch: 3, train for the 183-th batch, train loss: 0.638763427734375:  77%|█████████▎  | 183/237 [01:49<00:34,  1.55it/s]Epoch: 6, train for the 4-th batch, train loss: 0.2885488271713257:   3%|▍              | 3/119 [00:02<01:07,  1.72it/s]Epoch: 6, train for the 4-th batch, train loss: 0.2885488271713257:   3%|▌              | 4/119 [00:02<01:07,  1.69it/s]Epoch: 9, train for the 123-th batch, train loss: 0.5555122494697571:  81%|████████▉  | 122/151 [00:27<00:06,  4.36it/s]Epoch: 9, train for the 123-th batch, train loss: 0.5555122494697571:  81%|████████▉  | 123/151 [00:27<00:06,  4.38it/s]Epoch: 9, train for the 124-th batch, train loss: 0.5438478589057922:  81%|████████▉  | 123/151 [00:27<00:06,  4.38it/s]Epoch: 9, train for the 124-th batch, train loss: 0.5438478589057922:  82%|█████████  | 124/151 [00:27<00:06,  4.40it/s]Epoch: 5, train for the 48-th batch, train loss: 0.4694460928440094:  32%|████▏        | 47/146 [00:27<00:57,  1.72it/s]Epoch: 5, train for the 48-th batch, train loss: 0.4694460928440094:  33%|████▎        | 48/146 [00:27<00:54,  1.80it/s]Epoch: 9, train for the 125-th batch, train loss: 0.5279756188392639:  82%|█████████  | 124/151 [00:27<00:06,  4.40it/s]Epoch: 9, train for the 125-th batch, train loss: 0.5279756188392639:  83%|█████████  | 125/151 [00:27<00:05,  4.40it/s]Epoch: 2, train for the 275-th batch, train loss: 0.44902631640434265:  72%|███████▏  | 274/383 [02:45<01:10,  1.55it/s]Epoch: 2, train for the 275-th batch, train loss: 0.44902631640434265:  72%|███████▏  | 275/383 [02:45<01:09,  1.55it/s]Epoch: 3, train for the 184-th batch, train loss: 0.6161500215530396:  77%|████████▍  | 183/237 [01:49<00:34,  1.55it/s]Epoch: 3, train for the 184-th batch, train loss: 0.6161500215530396:  78%|████████▌  | 184/237 [01:49<00:34,  1.55it/s]Epoch: 9, train for the 126-th batch, train loss: 0.5368847250938416:  83%|█████████  | 125/151 [00:27<00:05,  4.40it/s]Epoch: 9, train for the 126-th batch, train loss: 0.5368847250938416:  83%|█████████▏ | 126/151 [00:27<00:05,  4.40it/s]Epoch: 5, train for the 49-th batch, train loss: 0.4761730432510376:  33%|████▎        | 48/146 [00:28<00:54,  1.80it/s]Epoch: 5, train for the 49-th batch, train loss: 0.4761730432510376:  34%|████▎        | 49/146 [00:28<00:48,  1.99it/s]Epoch: 9, train for the 127-th batch, train loss: 0.561286449432373:  83%|██████████  | 126/151 [00:28<00:05,  4.40it/s]Epoch: 9, train for the 127-th batch, train loss: 0.561286449432373:  84%|██████████  | 127/151 [00:28<00:05,  4.39it/s]Epoch: 5, train for the 50-th batch, train loss: 0.5104473829269409:  34%|████▎        | 49/146 [00:28<00:48,  1.99it/s]Epoch: 5, train for the 50-th batch, train loss: 0.5104473829269409:  34%|████▍        | 50/146 [00:28<00:40,  2.39it/s]Epoch: 6, train for the 5-th batch, train loss: 0.3826653063297272:   3%|▌              | 4/119 [00:03<01:07,  1.69it/s]Epoch: 6, train for the 5-th batch, train loss: 0.3826653063297272:   4%|▋              | 5/119 [00:03<01:29,  1.27it/s]Epoch: 9, train for the 128-th batch, train loss: 0.588455080986023:  84%|██████████  | 127/151 [00:28<00:05,  4.39it/s]Epoch: 9, train for the 128-th batch, train loss: 0.588455080986023:  85%|██████████▏ | 128/151 [00:28<00:05,  4.40it/s]Epoch: 2, train for the 276-th batch, train loss: 0.535000205039978:  72%|████████▌   | 275/383 [02:45<01:09,  1.55it/s]Epoch: 2, train for the 276-th batch, train loss: 0.535000205039978:  72%|████████▋   | 276/383 [02:45<01:08,  1.55it/s]Epoch: 3, train for the 185-th batch, train loss: 0.600196123123169:  78%|█████████▎  | 184/237 [01:50<00:34,  1.55it/s]Epoch: 3, train for the 185-th batch, train loss: 0.600196123123169:  78%|█████████▎  | 185/237 [01:50<00:33,  1.55it/s]Epoch: 9, train for the 129-th batch, train loss: 0.5571483373641968:  85%|█████████▎ | 128/151 [00:28<00:05,  4.40it/s]Epoch: 9, train for the 129-th batch, train loss: 0.5571483373641968:  85%|█████████▍ | 129/151 [00:28<00:04,  4.40it/s]Epoch: 5, train for the 51-th batch, train loss: 0.47263872623443604:  34%|████        | 50/146 [00:29<00:40,  2.39it/s]Epoch: 5, train for the 51-th batch, train loss: 0.47263872623443604:  35%|████▏       | 51/146 [00:29<00:41,  2.27it/s]Epoch: 9, train for the 130-th batch, train loss: 0.5508570671081543:  85%|█████████▍ | 129/151 [00:28<00:04,  4.40it/s]Epoch: 9, train for the 130-th batch, train loss: 0.5508570671081543:  86%|█████████▍ | 130/151 [00:28<00:04,  4.42it/s]Epoch: 6, train for the 6-th batch, train loss: 0.2822200059890747:   4%|▋              | 5/119 [00:04<01:29,  1.27it/s]Epoch: 6, train for the 6-th batch, train loss: 0.2822200059890747:   5%|▊              | 6/119 [00:04<01:20,  1.40it/s]Epoch: 2, train for the 277-th batch, train loss: 0.40342214703559875:  72%|███████▏  | 276/383 [02:46<01:08,  1.55it/s]Epoch: 2, train for the 277-th batch, train loss: 0.40342214703559875:  72%|███████▏  | 277/383 [02:46<01:08,  1.55it/s]Epoch: 3, train for the 186-th batch, train loss: 0.6064489483833313:  78%|████████▌  | 185/237 [01:50<00:33,  1.55it/s]Epoch: 3, train for the 186-th batch, train loss: 0.6064489483833313:  78%|████████▋  | 186/237 [01:50<00:32,  1.55it/s]Epoch: 9, train for the 131-th batch, train loss: 0.5328212380409241:  86%|█████████▍ | 130/151 [00:28<00:04,  4.42it/s]Epoch: 9, train for the 131-th batch, train loss: 0.5328212380409241:  87%|█████████▌ | 131/151 [00:28<00:04,  4.42it/s]Epoch: 5, train for the 52-th batch, train loss: 0.48821285367012024:  35%|████▏       | 51/146 [00:29<00:41,  2.27it/s]Epoch: 5, train for the 52-th batch, train loss: 0.48821285367012024:  36%|████▎       | 52/146 [00:29<00:45,  2.08it/s]Epoch: 9, train for the 132-th batch, train loss: 0.5137149095535278:  87%|█████████▌ | 131/151 [00:29<00:04,  4.42it/s]Epoch: 9, train for the 132-th batch, train loss: 0.5137149095535278:  87%|█████████▌ | 132/151 [00:29<00:04,  4.43it/s]Epoch: 9, train for the 133-th batch, train loss: 0.5624104738235474:  87%|█████████▌ | 132/151 [00:29<00:04,  4.43it/s]Epoch: 9, train for the 133-th batch, train loss: 0.5624104738235474:  88%|█████████▋ | 133/151 [00:29<00:04,  4.42it/s]Epoch: 6, train for the 7-th batch, train loss: 0.26686498522758484:   5%|▋             | 6/119 [00:04<01:20,  1.40it/s]Epoch: 6, train for the 7-th batch, train loss: 0.26686498522758484:   6%|▊             | 7/119 [00:04<01:15,  1.48it/s]Epoch: 2, train for the 278-th batch, train loss: 0.41869017481803894:  72%|███████▏  | 277/383 [02:47<01:08,  1.55it/s]Epoch: 2, train for the 278-th batch, train loss: 0.41869017481803894:  73%|███████▎  | 278/383 [02:47<01:07,  1.55it/s]Epoch: 3, train for the 187-th batch, train loss: 0.6451817154884338:  78%|████████▋  | 186/237 [01:51<00:32,  1.55it/s]Epoch: 3, train for the 187-th batch, train loss: 0.6451817154884338:  79%|████████▋  | 187/237 [01:51<00:32,  1.55it/s]Epoch: 9, train for the 134-th batch, train loss: 0.5437409281730652:  88%|█████████▋ | 133/151 [00:29<00:04,  4.42it/s]Epoch: 9, train for the 134-th batch, train loss: 0.5437409281730652:  89%|█████████▊ | 134/151 [00:29<00:03,  4.43it/s]Epoch: 5, train for the 53-th batch, train loss: 0.4531227648258209:  36%|████▋        | 52/146 [00:30<00:45,  2.08it/s]Epoch: 5, train for the 53-th batch, train loss: 0.4531227648258209:  36%|████▋        | 53/146 [00:30<00:47,  1.95it/s]Epoch: 9, train for the 135-th batch, train loss: 0.5175915956497192:  89%|█████████▊ | 134/151 [00:29<00:03,  4.43it/s]Epoch: 9, train for the 135-th batch, train loss: 0.5175915956497192:  89%|█████████▊ | 135/151 [00:29<00:03,  4.42it/s]Epoch: 6, train for the 8-th batch, train loss: 0.19128337502479553:   6%|▊             | 7/119 [00:05<01:15,  1.48it/s]Epoch: 6, train for the 8-th batch, train loss: 0.19128337502479553:   7%|▉             | 8/119 [00:05<01:11,  1.56it/s]Epoch: 9, train for the 136-th batch, train loss: 0.5576696395874023:  89%|█████████▊ | 135/151 [00:30<00:03,  4.42it/s]Epoch: 9, train for the 136-th batch, train loss: 0.5576696395874023:  90%|█████████▉ | 136/151 [00:30<00:03,  4.43it/s]Epoch: 2, train for the 279-th batch, train loss: 0.45656818151474:  73%|█████████▍   | 278/383 [02:47<01:07,  1.55it/s]Epoch: 2, train for the 279-th batch, train loss: 0.45656818151474:  73%|█████████▍   | 279/383 [02:47<01:07,  1.55it/s]Epoch: 3, train for the 188-th batch, train loss: 0.6308908462524414:  79%|████████▋  | 187/237 [01:52<00:32,  1.55it/s]Epoch: 3, train for the 188-th batch, train loss: 0.6308908462524414:  79%|████████▋  | 188/237 [01:52<00:31,  1.55it/s]Epoch: 9, train for the 137-th batch, train loss: 0.5819024443626404:  90%|█████████▉ | 136/151 [00:30<00:03,  4.43it/s]Epoch: 9, train for the 137-th batch, train loss: 0.5819024443626404:  91%|█████████▉ | 137/151 [00:30<00:03,  4.42it/s]Epoch: 5, train for the 54-th batch, train loss: 0.46838700771331787:  36%|████▎       | 53/146 [00:30<00:47,  1.95it/s]Epoch: 5, train for the 54-th batch, train loss: 0.46838700771331787:  37%|████▍       | 54/146 [00:30<00:49,  1.88it/s]Epoch: 9, train for the 138-th batch, train loss: 0.6199191212654114:  91%|█████████▉ | 137/151 [00:30<00:03,  4.42it/s]Epoch: 9, train for the 138-th batch, train loss: 0.6199191212654114:  91%|██████████ | 138/151 [00:30<00:02,  4.43it/s]Epoch: 6, train for the 9-th batch, train loss: 0.25742003321647644:   7%|▉             | 8/119 [00:05<01:11,  1.56it/s]Epoch: 6, train for the 9-th batch, train loss: 0.25742003321647644:   8%|█             | 9/119 [00:05<01:09,  1.59it/s]Epoch: 9, train for the 139-th batch, train loss: 0.5458791851997375:  91%|██████████ | 138/151 [00:30<00:02,  4.43it/s]Epoch: 9, train for the 139-th batch, train loss: 0.5458791851997375:  92%|██████████▏| 139/151 [00:30<00:02,  4.43it/s]Epoch: 2, train for the 280-th batch, train loss: 0.4120877683162689:  73%|████████   | 279/383 [02:48<01:07,  1.55it/s]Epoch: 2, train for the 280-th batch, train loss: 0.4120877683162689:  73%|████████   | 280/383 [02:48<01:06,  1.55it/s]Epoch: 3, train for the 189-th batch, train loss: 0.6493874192237854:  79%|████████▋  | 188/237 [01:52<00:31,  1.55it/s]Epoch: 3, train for the 189-th batch, train loss: 0.6493874192237854:  80%|████████▊  | 189/237 [01:52<00:30,  1.55it/s]Epoch: 5, train for the 55-th batch, train loss: 0.5229964852333069:  37%|████▊        | 54/146 [00:31<00:49,  1.88it/s]Epoch: 5, train for the 55-th batch, train loss: 0.5229964852333069:  38%|████▉        | 55/146 [00:31<00:50,  1.81it/s]Epoch: 9, train for the 140-th batch, train loss: 0.5260205864906311:  92%|██████████▏| 139/151 [00:30<00:02,  4.43it/s]Epoch: 9, train for the 140-th batch, train loss: 0.5260205864906311:  93%|██████████▏| 140/151 [00:30<00:02,  4.42it/s]Epoch: 6, train for the 10-th batch, train loss: 0.26184454560279846:   8%|▉            | 9/119 [00:06<01:09,  1.59it/s]Epoch: 6, train for the 10-th batch, train loss: 0.26184454560279846:   8%|█           | 10/119 [00:06<01:07,  1.62it/s]Epoch: 9, train for the 141-th batch, train loss: 0.5540788173675537:  93%|██████████▏| 140/151 [00:31<00:02,  4.42it/s]Epoch: 9, train for the 141-th batch, train loss: 0.5540788173675537:  93%|██████████▎| 141/151 [00:31<00:02,  4.42it/s]Epoch: 9, train for the 142-th batch, train loss: 0.5649453401565552:  93%|██████████▎| 141/151 [00:31<00:02,  4.42it/s]Epoch: 9, train for the 142-th batch, train loss: 0.5649453401565552:  94%|██████████▎| 142/151 [00:31<00:02,  4.40it/s]Epoch: 5, train for the 56-th batch, train loss: 0.5002277493476868:  38%|████▉        | 55/146 [00:31<00:50,  1.81it/s]Epoch: 5, train for the 56-th batch, train loss: 0.5002277493476868:  38%|████▉        | 56/146 [00:31<00:50,  1.77it/s]Epoch: 2, train for the 281-th batch, train loss: 0.4515770375728607:  73%|████████   | 280/383 [02:49<01:06,  1.55it/s]Epoch: 2, train for the 281-th batch, train loss: 0.4515770375728607:  73%|████████   | 281/383 [02:49<01:05,  1.55it/s]Epoch: 3, train for the 190-th batch, train loss: 0.612824559211731:  80%|█████████▌  | 189/237 [01:53<00:30,  1.55it/s]Epoch: 3, train for the 190-th batch, train loss: 0.612824559211731:  80%|█████████▌  | 190/237 [01:53<00:30,  1.55it/s]Epoch: 9, train for the 143-th batch, train loss: 0.48717430233955383:  94%|█████████▍| 142/151 [00:31<00:02,  4.40it/s]Epoch: 9, train for the 143-th batch, train loss: 0.48717430233955383:  95%|█████████▍| 143/151 [00:31<00:01,  4.41it/s]Epoch: 6, train for the 11-th batch, train loss: 0.24488432705402374:   8%|█           | 10/119 [00:06<01:07,  1.62it/s]Epoch: 6, train for the 11-th batch, train loss: 0.24488432705402374:   9%|█           | 11/119 [00:06<01:06,  1.64it/s]Epoch: 9, train for the 144-th batch, train loss: 0.5044762492179871:  95%|██████████▍| 143/151 [00:31<00:01,  4.41it/s]Epoch: 9, train for the 144-th batch, train loss: 0.5044762492179871:  95%|██████████▍| 144/151 [00:31<00:01,  4.40it/s]Epoch: 5, train for the 57-th batch, train loss: 0.4936010241508484:  38%|████▉        | 56/146 [00:32<00:50,  1.77it/s]Epoch: 5, train for the 57-th batch, train loss: 0.4936010241508484:  39%|█████        | 57/146 [00:32<00:51,  1.74it/s]Epoch: 9, train for the 145-th batch, train loss: 0.5397484302520752:  95%|██████████▍| 144/151 [00:32<00:01,  4.40it/s]Epoch: 9, train for the 145-th batch, train loss: 0.5397484302520752:  96%|██████████▌| 145/151 [00:32<00:01,  4.41it/s]Epoch: 2, train for the 282-th batch, train loss: 0.5361554622650146:  73%|████████   | 281/383 [02:49<01:05,  1.55it/s]Epoch: 2, train for the 282-th batch, train loss: 0.5361554622650146:  74%|████████   | 282/383 [02:49<01:05,  1.55it/s]Epoch: 3, train for the 191-th batch, train loss: 0.6333845257759094:  80%|████████▊  | 190/237 [01:54<00:30,  1.55it/s]Epoch: 3, train for the 191-th batch, train loss: 0.6333845257759094:  81%|████████▊  | 191/237 [01:54<00:29,  1.55it/s]Epoch: 9, train for the 146-th batch, train loss: 0.5280239582061768:  96%|██████████▌| 145/151 [00:32<00:01,  4.41it/s]Epoch: 9, train for the 146-th batch, train loss: 0.5280239582061768:  97%|██████████▋| 146/151 [00:32<00:01,  4.43it/s]Epoch: 6, train for the 12-th batch, train loss: 0.2665400505065918:   9%|█▏           | 11/119 [00:07<01:06,  1.64it/s]Epoch: 6, train for the 12-th batch, train loss: 0.2665400505065918:  10%|█▎           | 12/119 [00:07<01:04,  1.65it/s]Epoch: 9, train for the 147-th batch, train loss: 0.578574538230896:  97%|███████████▌| 146/151 [00:32<00:01,  4.43it/s]Epoch: 9, train for the 147-th batch, train loss: 0.578574538230896:  97%|███████████▋| 147/151 [00:32<00:00,  4.44it/s]Epoch: 5, train for the 58-th batch, train loss: 0.4736766815185547:  39%|█████        | 57/146 [00:33<00:51,  1.74it/s]Epoch: 5, train for the 58-th batch, train loss: 0.4736766815185547:  40%|█████▏       | 58/146 [00:33<00:51,  1.72it/s]Epoch: 9, train for the 148-th batch, train loss: 0.5712785124778748:  97%|██████████▋| 147/151 [00:32<00:00,  4.44it/s]Epoch: 9, train for the 148-th batch, train loss: 0.5712785124778748:  98%|██████████▊| 148/151 [00:32<00:00,  4.43it/s]Epoch: 2, train for the 283-th batch, train loss: 0.45767539739608765:  74%|███████▎  | 282/383 [02:50<01:05,  1.55it/s]Epoch: 2, train for the 283-th batch, train loss: 0.45767539739608765:  74%|███████▍  | 283/383 [02:50<01:04,  1.55it/s]Epoch: 3, train for the 192-th batch, train loss: 0.6470924019813538:  81%|████████▊  | 191/237 [01:54<00:29,  1.55it/s]Epoch: 3, train for the 192-th batch, train loss: 0.6470924019813538:  81%|████████▉  | 192/237 [01:54<00:28,  1.55it/s]Epoch: 6, train for the 13-th batch, train loss: 0.2523503601551056:  10%|█▎           | 12/119 [00:08<01:04,  1.65it/s]Epoch: 6, train for the 13-th batch, train loss: 0.2523503601551056:  11%|█▍           | 13/119 [00:08<01:03,  1.66it/s]Epoch: 9, train for the 149-th batch, train loss: 0.54086834192276:  98%|████████████▋| 148/151 [00:33<00:00,  4.43it/s]Epoch: 9, train for the 149-th batch, train loss: 0.54086834192276:  99%|████████████▊| 149/151 [00:33<00:00,  4.43it/s]Epoch: 9, train for the 150-th batch, train loss: 0.5084695816040039:  99%|██████████▊| 149/151 [00:33<00:00,  4.43it/s]Epoch: 9, train for the 150-th batch, train loss: 0.5084695816040039:  99%|██████████▉| 150/151 [00:33<00:00,  4.41it/s]Epoch: 5, train for the 59-th batch, train loss: 0.4445453882217407:  40%|█████▏       | 58/146 [00:33<00:51,  1.72it/s]Epoch: 5, train for the 59-th batch, train loss: 0.4445453882217407:  40%|█████▎       | 59/146 [00:33<00:50,  1.71it/s]Epoch: 9, train for the 151-th batch, train loss: 0.5630402565002441:  99%|██████████▉| 150/151 [00:33<00:00,  4.41it/s]Epoch: 9, train for the 151-th batch, train loss: 0.5630402565002441: 100%|███████████| 151/151 [00:33<00:00,  4.91it/s]Epoch: 9, train for the 151-th batch, train loss: 0.5630402565002441: 100%|███████████| 151/151 [00:33<00:00,  4.52it/s]
  0%|                                                                                            | 0/46 [00:00<?, ?it/s]Epoch: 2, train for the 284-th batch, train loss: 0.5178144574165344:  74%|████████▏  | 283/383 [02:50<01:04,  1.55it/s]Epoch: 2, train for the 284-th batch, train loss: 0.5178144574165344:  74%|████████▏  | 284/383 [02:50<01:03,  1.55it/s]Epoch: 3, train for the 193-th batch, train loss: 0.6286696195602417:  81%|████████▉  | 192/237 [01:55<00:28,  1.55it/s]Epoch: 3, train for the 193-th batch, train loss: 0.6286696195602417:  81%|████████▉  | 193/237 [01:55<00:28,  1.55it/s]evaluate for the 1-th batch, evaluate loss: 0.4968923330307007:   0%|                            | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.4968923330307007:   2%|▍                   | 1/46 [00:00<00:04,  9.61it/s]Epoch: 6, train for the 14-th batch, train loss: 0.31535035371780396:  11%|█▎          | 13/119 [00:08<01:03,  1.66it/s]Epoch: 6, train for the 14-th batch, train loss: 0.31535035371780396:  12%|█▍          | 14/119 [00:08<01:03,  1.67it/s]evaluate for the 2-th batch, evaluate loss: 0.5056158900260925:   2%|▍                   | 1/46 [00:00<00:04,  9.61it/s]evaluate for the 2-th batch, evaluate loss: 0.5056158900260925:   4%|▊                   | 2/46 [00:00<00:04,  9.54it/s]evaluate for the 3-th batch, evaluate loss: 0.4821087121963501:   4%|▊                   | 2/46 [00:00<00:04,  9.54it/s]evaluate for the 3-th batch, evaluate loss: 0.4821087121963501:   7%|█▎                  | 3/46 [00:00<00:04,  9.59it/s]evaluate for the 4-th batch, evaluate loss: 0.5092910528182983:   7%|█▎                  | 3/46 [00:00<00:04,  9.59it/s]evaluate for the 4-th batch, evaluate loss: 0.5092910528182983:   9%|█▋                  | 4/46 [00:00<00:04,  9.65it/s]Epoch: 5, train for the 60-th batch, train loss: 0.4984787404537201:  40%|█████▎       | 59/146 [00:34<00:50,  1.71it/s]Epoch: 5, train for the 60-th batch, train loss: 0.4984787404537201:  41%|█████▎       | 60/146 [00:34<00:50,  1.70it/s]evaluate for the 5-th batch, evaluate loss: 0.4780774712562561:   9%|█▋                  | 4/46 [00:00<00:04,  9.65it/s]evaluate for the 5-th batch, evaluate loss: 0.4780774712562561:  11%|██▏                 | 5/46 [00:00<00:04,  9.65it/s]evaluate for the 6-th batch, evaluate loss: 0.5533167719841003:  11%|██▏                 | 5/46 [00:00<00:04,  9.65it/s]evaluate for the 6-th batch, evaluate loss: 0.5533167719841003:  13%|██▌                 | 6/46 [00:00<00:04,  9.66it/s]Epoch: 2, train for the 285-th batch, train loss: 0.5009158253669739:  74%|████████▏  | 284/383 [02:51<01:03,  1.55it/s]Epoch: 2, train for the 285-th batch, train loss: 0.5009158253669739:  74%|████████▏  | 285/383 [02:51<01:03,  1.55it/s]Epoch: 3, train for the 194-th batch, train loss: 0.6274123787879944:  81%|████████▉  | 193/237 [01:56<00:28,  1.55it/s]Epoch: 3, train for the 194-th batch, train loss: 0.6274123787879944:  82%|█████████  | 194/237 [01:56<00:27,  1.55it/s]evaluate for the 7-th batch, evaluate loss: 0.4738549590110779:  13%|██▌                 | 6/46 [00:00<00:04,  9.66it/s]evaluate for the 7-th batch, evaluate loss: 0.4738549590110779:  15%|███                 | 7/46 [00:00<00:04,  9.71it/s]Epoch: 6, train for the 15-th batch, train loss: 0.28539130091667175:  12%|█▍          | 14/119 [00:09<01:03,  1.67it/s]Epoch: 6, train for the 15-th batch, train loss: 0.28539130091667175:  13%|█▌          | 15/119 [00:09<01:02,  1.67it/s]evaluate for the 8-th batch, evaluate loss: 0.5570334792137146:  15%|███                 | 7/46 [00:00<00:04,  9.71it/s]evaluate for the 8-th batch, evaluate loss: 0.5570334792137146:  17%|███▍                | 8/46 [00:00<00:03,  9.73it/s]evaluate for the 9-th batch, evaluate loss: 0.5244718194007874:  17%|███▍                | 8/46 [00:00<00:03,  9.73it/s]evaluate for the 9-th batch, evaluate loss: 0.5244718194007874:  20%|███▉                | 9/46 [00:00<00:03,  9.75it/s]evaluate for the 10-th batch, evaluate loss: 0.533519446849823:  20%|███▉                | 9/46 [00:01<00:03,  9.75it/s]evaluate for the 10-th batch, evaluate loss: 0.533519446849823:  22%|████▏              | 10/46 [00:01<00:03,  9.74it/s]Epoch: 5, train for the 61-th batch, train loss: 0.535197913646698:  41%|█████▊        | 60/146 [00:34<00:50,  1.70it/s]Epoch: 5, train for the 61-th batch, train loss: 0.535197913646698:  42%|█████▊        | 61/146 [00:34<00:50,  1.69it/s]evaluate for the 11-th batch, evaluate loss: 0.5233103632926941:  22%|███▉              | 10/46 [00:01<00:03,  9.74it/s]evaluate for the 11-th batch, evaluate loss: 0.5233103632926941:  24%|████▎             | 11/46 [00:01<00:03,  9.72it/s]evaluate for the 12-th batch, evaluate loss: 0.4756195843219757:  24%|████▎             | 11/46 [00:01<00:03,  9.72it/s]evaluate for the 12-th batch, evaluate loss: 0.4756195843219757:  26%|████▋             | 12/46 [00:01<00:03,  9.69it/s]evaluate for the 13-th batch, evaluate loss: 0.49361273646354675:  26%|████▍            | 12/46 [00:01<00:03,  9.69it/s]evaluate for the 13-th batch, evaluate loss: 0.49361273646354675:  28%|████▊            | 13/46 [00:01<00:03,  9.67it/s]Epoch: 2, train for the 286-th batch, train loss: 0.42537832260131836:  74%|███████▍  | 285/383 [02:52<01:03,  1.55it/s]Epoch: 2, train for the 286-th batch, train loss: 0.42537832260131836:  75%|███████▍  | 286/383 [02:52<01:02,  1.55it/s]Epoch: 3, train for the 195-th batch, train loss: 0.6209433078765869:  82%|█████████  | 194/237 [01:56<00:27,  1.55it/s]Epoch: 3, train for the 195-th batch, train loss: 0.6209433078765869:  82%|█████████  | 195/237 [01:56<00:27,  1.55it/s]Epoch: 6, train for the 16-th batch, train loss: 0.2880619466304779:  13%|█▋           | 15/119 [00:09<01:02,  1.67it/s]Epoch: 6, train for the 16-th batch, train loss: 0.2880619466304779:  13%|█▋           | 16/119 [00:09<01:01,  1.67it/s]evaluate for the 14-th batch, evaluate loss: 0.5869647264480591:  28%|█████             | 13/46 [00:01<00:03,  9.67it/s]evaluate for the 14-th batch, evaluate loss: 0.5869647264480591:  30%|█████▍            | 14/46 [00:01<00:03,  9.69it/s]evaluate for the 15-th batch, evaluate loss: 0.5419259667396545:  30%|█████▍            | 14/46 [00:01<00:03,  9.69it/s]evaluate for the 15-th batch, evaluate loss: 0.5419259667396545:  33%|█████▊            | 15/46 [00:01<00:03,  9.71it/s]evaluate for the 16-th batch, evaluate loss: 0.5712355971336365:  33%|█████▊            | 15/46 [00:01<00:03,  9.71it/s]evaluate for the 16-th batch, evaluate loss: 0.5712355971336365:  35%|██████▎           | 16/46 [00:01<00:03,  9.72it/s]Epoch: 5, train for the 62-th batch, train loss: 0.4822295904159546:  42%|█████▍       | 61/146 [00:35<00:50,  1.69it/s]Epoch: 5, train for the 62-th batch, train loss: 0.4822295904159546:  42%|█████▌       | 62/146 [00:35<00:49,  1.69it/s]evaluate for the 17-th batch, evaluate loss: 0.45460227131843567:  35%|█████▉           | 16/46 [00:01<00:03,  9.72it/s]evaluate for the 17-th batch, evaluate loss: 0.45460227131843567:  37%|██████▎          | 17/46 [00:01<00:02,  9.72it/s]evaluate for the 18-th batch, evaluate loss: 0.5001066327095032:  37%|██████▋           | 17/46 [00:01<00:02,  9.72it/s]evaluate for the 18-th batch, evaluate loss: 0.5001066327095032:  39%|███████           | 18/46 [00:01<00:02,  9.69it/s]evaluate for the 19-th batch, evaluate loss: 0.5184928178787231:  39%|███████           | 18/46 [00:01<00:02,  9.69it/s]evaluate for the 19-th batch, evaluate loss: 0.5184928178787231:  41%|███████▍          | 19/46 [00:01<00:02,  9.68it/s]Epoch: 6, train for the 17-th batch, train loss: 0.22384940087795258:  13%|█▌          | 16/119 [00:10<01:01,  1.67it/s]Epoch: 6, train for the 17-th batch, train loss: 0.22384940087795258:  14%|█▋          | 17/119 [00:10<01:00,  1.67it/s]Epoch: 2, train for the 287-th batch, train loss: 0.4386071562767029:  75%|████████▏  | 286/383 [02:52<01:02,  1.55it/s]Epoch: 2, train for the 287-th batch, train loss: 0.4386071562767029:  75%|████████▏  | 287/383 [02:52<01:01,  1.55it/s]Epoch: 3, train for the 196-th batch, train loss: 0.6266610622406006:  82%|█████████  | 195/237 [01:57<00:27,  1.55it/s]Epoch: 3, train for the 196-th batch, train loss: 0.6266610622406006:  83%|█████████  | 196/237 [01:57<00:26,  1.55it/s]evaluate for the 20-th batch, evaluate loss: 0.538223385810852:  41%|███████▊           | 19/46 [00:02<00:02,  9.68it/s]evaluate for the 20-th batch, evaluate loss: 0.538223385810852:  43%|████████▎          | 20/46 [00:02<00:02,  9.68it/s]evaluate for the 21-th batch, evaluate loss: 0.5255571603775024:  43%|███████▊          | 20/46 [00:02<00:02,  9.68it/s]evaluate for the 21-th batch, evaluate loss: 0.5255571603775024:  46%|████████▏         | 21/46 [00:02<00:02,  9.71it/s]evaluate for the 22-th batch, evaluate loss: 0.5222765803337097:  46%|████████▏         | 21/46 [00:02<00:02,  9.71it/s]evaluate for the 22-th batch, evaluate loss: 0.5222765803337097:  48%|████████▌         | 22/46 [00:02<00:02,  9.73it/s]Epoch: 5, train for the 63-th batch, train loss: 0.5252612233161926:  42%|█████▌       | 62/146 [00:36<00:49,  1.69it/s]Epoch: 5, train for the 63-th batch, train loss: 0.5252612233161926:  43%|█████▌       | 63/146 [00:36<00:49,  1.69it/s]evaluate for the 23-th batch, evaluate loss: 0.47426626086235046:  48%|████████▏        | 22/46 [00:02<00:02,  9.73it/s]evaluate for the 23-th batch, evaluate loss: 0.47426626086235046:  50%|████████▌        | 23/46 [00:02<00:02,  9.74it/s]evaluate for the 24-th batch, evaluate loss: 0.4849798083305359:  50%|█████████         | 23/46 [00:02<00:02,  9.74it/s]evaluate for the 24-th batch, evaluate loss: 0.4849798083305359:  52%|█████████▍        | 24/46 [00:02<00:02,  9.71it/s]Epoch: 6, train for the 18-th batch, train loss: 0.23751108348369598:  14%|█▋          | 17/119 [00:11<01:00,  1.67it/s]Epoch: 6, train for the 18-th batch, train loss: 0.23751108348369598:  15%|█▊          | 18/119 [00:11<01:00,  1.68it/s]evaluate for the 25-th batch, evaluate loss: 0.53709876537323:  52%|██████████▍         | 24/46 [00:02<00:02,  9.71it/s]evaluate for the 25-th batch, evaluate loss: 0.53709876537323:  54%|██████████▊         | 25/46 [00:02<00:02,  9.70it/s]Epoch: 2, train for the 288-th batch, train loss: 0.5030108690261841:  75%|████████▏  | 287/383 [02:53<01:01,  1.55it/s]Epoch: 2, train for the 288-th batch, train loss: 0.5030108690261841:  75%|████████▎  | 288/383 [02:53<01:00,  1.56it/s]Epoch: 3, train for the 197-th batch, train loss: 0.6456802487373352:  83%|█████████  | 196/237 [01:58<00:26,  1.55it/s]Epoch: 3, train for the 197-th batch, train loss: 0.6456802487373352:  83%|█████████▏ | 197/237 [01:58<00:25,  1.56it/s]evaluate for the 26-th batch, evaluate loss: 0.557774543762207:  54%|██████████▎        | 25/46 [00:02<00:02,  9.70it/s]evaluate for the 26-th batch, evaluate loss: 0.557774543762207:  57%|██████████▋        | 26/46 [00:02<00:02,  9.72it/s]evaluate for the 27-th batch, evaluate loss: 0.495584636926651:  57%|██████████▋        | 26/46 [00:02<00:02,  9.72it/s]evaluate for the 27-th batch, evaluate loss: 0.495584636926651:  59%|███████████▏       | 27/46 [00:02<00:01,  9.73it/s]Epoch: 5, train for the 64-th batch, train loss: 0.5484365820884705:  43%|█████▌       | 63/146 [00:36<00:49,  1.69it/s]Epoch: 5, train for the 64-th batch, train loss: 0.5484365820884705:  44%|█████▋       | 64/146 [00:36<00:48,  1.68it/s]evaluate for the 28-th batch, evaluate loss: 0.5204172134399414:  59%|██████████▌       | 27/46 [00:02<00:01,  9.73it/s]evaluate for the 28-th batch, evaluate loss: 0.5204172134399414:  61%|██████████▉       | 28/46 [00:02<00:01,  9.75it/s]evaluate for the 29-th batch, evaluate loss: 0.49244216084480286:  61%|██████████▎      | 28/46 [00:02<00:01,  9.75it/s]evaluate for the 29-th batch, evaluate loss: 0.49244216084480286:  63%|██████████▋      | 29/46 [00:02<00:01,  9.75it/s]evaluate for the 30-th batch, evaluate loss: 0.49414974451065063:  63%|██████████▋      | 29/46 [00:03<00:01,  9.75it/s]evaluate for the 30-th batch, evaluate loss: 0.49414974451065063:  65%|███████████      | 30/46 [00:03<00:01,  9.73it/s]Epoch: 6, train for the 19-th batch, train loss: 0.19187206029891968:  15%|█▊          | 18/119 [00:11<01:00,  1.68it/s]Epoch: 6, train for the 19-th batch, train loss: 0.19187206029891968:  16%|█▉          | 19/119 [00:11<00:59,  1.68it/s]evaluate for the 31-th batch, evaluate loss: 0.5181886553764343:  65%|███████████▋      | 30/46 [00:03<00:01,  9.73it/s]evaluate for the 31-th batch, evaluate loss: 0.5181886553764343:  67%|████████████▏     | 31/46 [00:03<00:01,  9.71it/s]Epoch: 2, train for the 289-th batch, train loss: 0.4549742341041565:  75%|████████▎  | 288/383 [02:54<01:00,  1.56it/s]Epoch: 2, train for the 289-th batch, train loss: 0.4549742341041565:  75%|████████▎  | 289/383 [02:54<01:00,  1.56it/s]Epoch: 3, train for the 198-th batch, train loss: 0.6124317049980164:  83%|█████████▏ | 197/237 [01:58<00:25,  1.56it/s]Epoch: 3, train for the 198-th batch, train loss: 0.6124317049980164:  84%|█████████▏ | 198/237 [01:58<00:25,  1.56it/s]evaluate for the 32-th batch, evaluate loss: 0.47983527183532715:  67%|███████████▍     | 31/46 [00:03<00:01,  9.71it/s]evaluate for the 32-th batch, evaluate loss: 0.47983527183532715:  70%|███████████▊     | 32/46 [00:03<00:01,  9.73it/s]evaluate for the 33-th batch, evaluate loss: 0.5011562705039978:  70%|████████████▌     | 32/46 [00:03<00:01,  9.73it/s]evaluate for the 33-th batch, evaluate loss: 0.5011562705039978:  72%|████████████▉     | 33/46 [00:03<00:01,  9.75it/s]Epoch: 5, train for the 65-th batch, train loss: 0.5333028435707092:  44%|█████▋       | 64/146 [00:37<00:48,  1.68it/s]Epoch: 5, train for the 65-th batch, train loss: 0.5333028435707092:  45%|█████▊       | 65/146 [00:37<00:48,  1.68it/s]evaluate for the 34-th batch, evaluate loss: 0.48529279232025146:  72%|████████████▏    | 33/46 [00:03<00:01,  9.75it/s]evaluate for the 34-th batch, evaluate loss: 0.48529279232025146:  74%|████████████▌    | 34/46 [00:03<00:01,  9.72it/s]evaluate for the 35-th batch, evaluate loss: 0.483591228723526:  74%|██████████████     | 34/46 [00:03<00:01,  9.72it/s]evaluate for the 35-th batch, evaluate loss: 0.483591228723526:  76%|██████████████▍    | 35/46 [00:03<00:01,  9.71it/s]evaluate for the 36-th batch, evaluate loss: 0.4710114300251007:  76%|█████████████▋    | 35/46 [00:03<00:01,  9.71it/s]evaluate for the 36-th batch, evaluate loss: 0.4710114300251007:  78%|██████████████    | 36/46 [00:03<00:01,  9.71it/s]Epoch: 6, train for the 20-th batch, train loss: 0.22747081518173218:  16%|█▉          | 19/119 [00:12<00:59,  1.68it/s]Epoch: 6, train for the 20-th batch, train loss: 0.22747081518173218:  17%|██          | 20/119 [00:12<00:59,  1.68it/s]evaluate for the 37-th batch, evaluate loss: 0.5040323734283447:  78%|██████████████    | 36/46 [00:03<00:01,  9.71it/s]evaluate for the 37-th batch, evaluate loss: 0.5040323734283447:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.71it/s]evaluate for the 38-th batch, evaluate loss: 0.5362054109573364:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.71it/s]evaluate for the 38-th batch, evaluate loss: 0.5362054109573364:  83%|██████████████▊   | 38/46 [00:03<00:00,  9.73it/s]Epoch: 2, train for the 290-th batch, train loss: 0.39059263467788696:  75%|███████▌  | 289/383 [02:54<01:00,  1.56it/s]Epoch: 2, train for the 290-th batch, train loss: 0.39059263467788696:  76%|███████▌  | 290/383 [02:54<00:59,  1.55it/s]Epoch: 3, train for the 199-th batch, train loss: 0.6125712990760803:  84%|█████████▏ | 198/237 [01:59<00:25,  1.56it/s]Epoch: 3, train for the 199-th batch, train loss: 0.6125712990760803:  84%|█████████▏ | 199/237 [01:59<00:24,  1.55it/s]evaluate for the 39-th batch, evaluate loss: 0.5354655385017395:  83%|██████████████▊   | 38/46 [00:04<00:00,  9.73it/s]evaluate for the 39-th batch, evaluate loss: 0.5354655385017395:  85%|███████████████▎  | 39/46 [00:04<00:00,  9.72it/s]Epoch: 5, train for the 66-th batch, train loss: 0.5435141921043396:  45%|█████▊       | 65/146 [00:37<00:48,  1.68it/s]Epoch: 5, train for the 66-th batch, train loss: 0.5435141921043396:  45%|█████▉       | 66/146 [00:37<00:47,  1.67it/s]evaluate for the 40-th batch, evaluate loss: 0.4693544805049896:  85%|███████████████▎  | 39/46 [00:04<00:00,  9.72it/s]evaluate for the 40-th batch, evaluate loss: 0.4693544805049896:  87%|███████████████▋  | 40/46 [00:04<00:00,  9.70it/s]evaluate for the 41-th batch, evaluate loss: 0.4808898866176605:  87%|███████████████▋  | 40/46 [00:04<00:00,  9.70it/s]evaluate for the 41-th batch, evaluate loss: 0.4808898866176605:  89%|████████████████  | 41/46 [00:04<00:00,  9.72it/s]evaluate for the 42-th batch, evaluate loss: 0.47153404355049133:  89%|███████████████▏ | 41/46 [00:04<00:00,  9.72it/s]evaluate for the 42-th batch, evaluate loss: 0.47153404355049133:  91%|███████████████▌ | 42/46 [00:04<00:00,  9.71it/s]Epoch: 6, train for the 21-th batch, train loss: 0.2721403241157532:  17%|██▏          | 20/119 [00:12<00:59,  1.68it/s]Epoch: 6, train for the 21-th batch, train loss: 0.2721403241157532:  18%|██▎          | 21/119 [00:12<00:58,  1.67it/s]evaluate for the 43-th batch, evaluate loss: 0.5324633121490479:  91%|████████████████▍ | 42/46 [00:04<00:00,  9.71it/s]evaluate for the 43-th batch, evaluate loss: 0.5324633121490479:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.72it/s]evaluate for the 44-th batch, evaluate loss: 0.5131507515907288:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.72it/s]evaluate for the 44-th batch, evaluate loss: 0.5131507515907288:  96%|█████████████████▏| 44/46 [00:04<00:00,  9.70it/s]Epoch: 2, train for the 291-th batch, train loss: 0.3848799467086792:  76%|████████▎  | 290/383 [02:55<00:59,  1.55it/s]Epoch: 2, train for the 291-th batch, train loss: 0.3848799467086792:  76%|████████▎  | 291/383 [02:55<00:59,  1.55it/s]Epoch: 3, train for the 200-th batch, train loss: 0.6121595501899719:  84%|█████████▏ | 199/237 [01:59<00:24,  1.55it/s]Epoch: 3, train for the 200-th batch, train loss: 0.6121595501899719:  84%|█████████▎ | 200/237 [01:59<00:23,  1.55it/s]evaluate for the 45-th batch, evaluate loss: 0.49411487579345703:  96%|████████████████▎| 44/46 [00:04<00:00,  9.70it/s]evaluate for the 45-th batch, evaluate loss: 0.49411487579345703:  98%|████████████████▋| 45/46 [00:04<00:00,  9.72it/s]Epoch: 5, train for the 67-th batch, train loss: 0.5584043860435486:  45%|█████▉       | 66/146 [00:38<00:47,  1.67it/s]Epoch: 5, train for the 67-th batch, train loss: 0.5584043860435486:  46%|█████▉       | 67/146 [00:38<00:47,  1.68it/s]evaluate for the 46-th batch, evaluate loss: 0.5078297853469849:  98%|█████████████████▌| 45/46 [00:04<00:00,  9.72it/s]evaluate for the 46-th batch, evaluate loss: 0.5078297853469849: 100%|██████████████████| 46/46 [00:04<00:00,  9.73it/s]
  0%|                                                                                            | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6401763558387756:   0%|                            | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6401763558387756:   4%|▊                   | 1/25 [00:00<00:02,  9.27it/s]evaluate for the 2-th batch, evaluate loss: 0.6494573950767517:   4%|▊                   | 1/25 [00:00<00:02,  9.27it/s]evaluate for the 2-th batch, evaluate loss: 0.6494573950767517:   8%|█▌                  | 2/25 [00:00<00:02,  9.26it/s]Epoch: 6, train for the 22-th batch, train loss: 0.2580532729625702:  18%|██▎          | 21/119 [00:13<00:58,  1.67it/s]Epoch: 6, train for the 22-th batch, train loss: 0.2580532729625702:  18%|██▍          | 22/119 [00:13<00:57,  1.67it/s]evaluate for the 3-th batch, evaluate loss: 0.6869058012962341:   8%|█▌                  | 2/25 [00:00<00:02,  9.26it/s]evaluate for the 3-th batch, evaluate loss: 0.6869058012962341:  12%|██▍                 | 3/25 [00:00<00:02,  9.28it/s]evaluate for the 4-th batch, evaluate loss: 0.6731177568435669:  12%|██▍                 | 3/25 [00:00<00:02,  9.28it/s]evaluate for the 4-th batch, evaluate loss: 0.6731177568435669:  16%|███▏                | 4/25 [00:00<00:02,  9.27it/s]Epoch: 2, train for the 292-th batch, train loss: 0.456460177898407:  76%|█████████   | 291/383 [02:56<00:59,  1.55it/s]Epoch: 2, train for the 292-th batch, train loss: 0.456460177898407:  76%|█████████▏  | 292/383 [02:56<00:58,  1.55it/s]Epoch: 3, train for the 201-th batch, train loss: 0.6174578070640564:  84%|█████████▎ | 200/237 [02:00<00:23,  1.55it/s]Epoch: 3, train for the 201-th batch, train loss: 0.6174578070640564:  85%|█████████▎ | 201/237 [02:00<00:23,  1.55it/s]Epoch: 5, train for the 68-th batch, train loss: 0.5665863156318665:  46%|█████▉       | 67/146 [00:39<00:47,  1.68it/s]Epoch: 5, train for the 68-th batch, train loss: 0.5665863156318665:  47%|██████       | 68/146 [00:39<00:46,  1.68it/s]evaluate for the 5-th batch, evaluate loss: 0.6711445450782776:  16%|███▏                | 4/25 [00:00<00:02,  9.27it/s]evaluate for the 5-th batch, evaluate loss: 0.6711445450782776:  20%|████                | 5/25 [00:00<00:02,  9.27it/s]evaluate for the 6-th batch, evaluate loss: 0.7152621150016785:  20%|████                | 5/25 [00:00<00:02,  9.27it/s]evaluate for the 6-th batch, evaluate loss: 0.7152621150016785:  24%|████▊               | 6/25 [00:00<00:02,  9.27it/s]evaluate for the 7-th batch, evaluate loss: 0.7383034229278564:  24%|████▊               | 6/25 [00:00<00:02,  9.27it/s]evaluate for the 7-th batch, evaluate loss: 0.7383034229278564:  28%|█████▌              | 7/25 [00:00<00:01,  9.26it/s]Epoch: 6, train for the 23-th batch, train loss: 0.25606533885002136:  18%|██▏         | 22/119 [00:14<00:57,  1.67it/s]Epoch: 6, train for the 23-th batch, train loss: 0.25606533885002136:  19%|██▎         | 23/119 [00:14<00:57,  1.67it/s]evaluate for the 8-th batch, evaluate loss: 0.7215746641159058:  28%|█████▌              | 7/25 [00:00<00:01,  9.26it/s]evaluate for the 8-th batch, evaluate loss: 0.7215746641159058:  32%|██████▍             | 8/25 [00:00<00:01,  9.29it/s]evaluate for the 9-th batch, evaluate loss: 0.7018250823020935:  32%|██████▍             | 8/25 [00:00<00:01,  9.29it/s]evaluate for the 9-th batch, evaluate loss: 0.7018250823020935:  36%|███████▏            | 9/25 [00:00<00:01,  9.31it/s]evaluate for the 10-th batch, evaluate loss: 0.7352492213249207:  36%|██████▊            | 9/25 [00:01<00:01,  9.31it/s]evaluate for the 10-th batch, evaluate loss: 0.7352492213249207:  40%|███████▏          | 10/25 [00:01<00:01,  9.30it/s]Epoch: 2, train for the 293-th batch, train loss: 0.45391741394996643:  76%|███████▌  | 292/383 [02:56<00:58,  1.55it/s]Epoch: 2, train for the 293-th batch, train loss: 0.45391741394996643:  77%|███████▋  | 293/383 [02:56<00:58,  1.55it/s]Epoch: 5, train for the 69-th batch, train loss: 0.5311928987503052:  47%|██████       | 68/146 [00:39<00:46,  1.68it/s]Epoch: 5, train for the 69-th batch, train loss: 0.5311928987503052:  47%|██████▏      | 69/146 [00:39<00:45,  1.68it/s]Epoch: 3, train for the 202-th batch, train loss: 0.6211771965026855:  85%|█████████▎ | 201/237 [02:01<00:23,  1.55it/s]Epoch: 3, train for the 202-th batch, train loss: 0.6211771965026855:  85%|█████████▍ | 202/237 [02:01<00:22,  1.55it/s]evaluate for the 11-th batch, evaluate loss: 0.7294374704360962:  40%|███████▏          | 10/25 [00:01<00:01,  9.30it/s]evaluate for the 11-th batch, evaluate loss: 0.7294374704360962:  44%|███████▉          | 11/25 [00:01<00:01,  9.29it/s]evaluate for the 12-th batch, evaluate loss: 0.6982449293136597:  44%|███████▉          | 11/25 [00:01<00:01,  9.29it/s]evaluate for the 12-th batch, evaluate loss: 0.6982449293136597:  48%|████████▋         | 12/25 [00:01<00:01,  9.31it/s]evaluate for the 13-th batch, evaluate loss: 0.6643334627151489:  48%|████████▋         | 12/25 [00:01<00:01,  9.31it/s]evaluate for the 13-th batch, evaluate loss: 0.6643334627151489:  52%|█████████▎        | 13/25 [00:01<00:01,  9.30it/s]Epoch: 6, train for the 24-th batch, train loss: 0.20484530925750732:  19%|██▎         | 23/119 [00:14<00:57,  1.67it/s]Epoch: 6, train for the 24-th batch, train loss: 0.20484530925750732:  20%|██▍         | 24/119 [00:14<00:56,  1.68it/s]evaluate for the 14-th batch, evaluate loss: 0.7495403289794922:  52%|█████████▎        | 13/25 [00:01<00:01,  9.30it/s]evaluate for the 14-th batch, evaluate loss: 0.7495403289794922:  56%|██████████        | 14/25 [00:01<00:01,  9.29it/s]evaluate for the 15-th batch, evaluate loss: 0.7230299711227417:  56%|██████████        | 14/25 [00:01<00:01,  9.29it/s]evaluate for the 15-th batch, evaluate loss: 0.7230299711227417:  60%|██████████▊       | 15/25 [00:01<00:01,  9.26it/s]evaluate for the 16-th batch, evaluate loss: 0.6593629121780396:  60%|██████████▊       | 15/25 [00:01<00:01,  9.26it/s]evaluate for the 16-th batch, evaluate loss: 0.6593629121780396:  64%|███████████▌      | 16/25 [00:01<00:00,  9.25it/s]Epoch: 5, train for the 70-th batch, train loss: 0.5304603576660156:  47%|██████▏      | 69/146 [00:40<00:45,  1.68it/s]Epoch: 5, train for the 70-th batch, train loss: 0.5304603576660156:  48%|██████▏      | 70/146 [00:40<00:45,  1.68it/s]Epoch: 2, train for the 294-th batch, train loss: 0.395527720451355:  77%|█████████▏  | 293/383 [02:57<00:58,  1.55it/s]Epoch: 2, train for the 294-th batch, train loss: 0.395527720451355:  77%|█████████▏  | 294/383 [02:57<00:57,  1.55it/s]Epoch: 3, train for the 203-th batch, train loss: 0.6427669525146484:  85%|█████████▍ | 202/237 [02:01<00:22,  1.55it/s]Epoch: 3, train for the 203-th batch, train loss: 0.6427669525146484:  86%|█████████▍ | 203/237 [02:01<00:21,  1.55it/s]evaluate for the 17-th batch, evaluate loss: 0.657684862613678:  64%|████████████▏      | 16/25 [00:01<00:00,  9.25it/s]evaluate for the 17-th batch, evaluate loss: 0.657684862613678:  68%|████████████▉      | 17/25 [00:01<00:00,  9.25it/s]evaluate for the 18-th batch, evaluate loss: 0.6248313188552856:  68%|████████████▏     | 17/25 [00:01<00:00,  9.25it/s]evaluate for the 18-th batch, evaluate loss: 0.6248313188552856:  72%|████████████▉     | 18/25 [00:01<00:00,  9.26it/s]Epoch: 6, train for the 25-th batch, train loss: 0.22650234401226044:  20%|██▍         | 24/119 [00:15<00:56,  1.68it/s]Epoch: 6, train for the 25-th batch, train loss: 0.22650234401226044:  21%|██▌         | 25/119 [00:15<00:56,  1.68it/s]evaluate for the 19-th batch, evaluate loss: 0.5935584902763367:  72%|████████████▉     | 18/25 [00:02<00:00,  9.26it/s]evaluate for the 19-th batch, evaluate loss: 0.5935584902763367:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.26it/s]evaluate for the 20-th batch, evaluate loss: 0.658563494682312:  76%|██████████████▍    | 19/25 [00:02<00:00,  9.26it/s]evaluate for the 20-th batch, evaluate loss: 0.658563494682312:  80%|███████████████▏   | 20/25 [00:02<00:00,  9.25it/s]evaluate for the 21-th batch, evaluate loss: 0.7241773009300232:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.25it/s]evaluate for the 21-th batch, evaluate loss: 0.7241773009300232:  84%|███████████████   | 21/25 [00:02<00:00,  9.27it/s]Epoch: 5, train for the 71-th batch, train loss: 0.5377792119979858:  48%|██████▏      | 70/146 [00:40<00:45,  1.68it/s]Epoch: 5, train for the 71-th batch, train loss: 0.5377792119979858:  49%|██████▎      | 71/146 [00:40<00:44,  1.68it/s]evaluate for the 22-th batch, evaluate loss: 0.6084567904472351:  84%|███████████████   | 21/25 [00:02<00:00,  9.27it/s]evaluate for the 22-th batch, evaluate loss: 0.6084567904472351:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.27it/s]Epoch: 3, train for the 204-th batch, train loss: 0.6315466165542603:  86%|█████████▍ | 203/237 [02:02<00:21,  1.55it/s]Epoch: 2, train for the 295-th batch, train loss: 0.4248553514480591:  77%|████████▍  | 294/383 [02:58<00:57,  1.55it/s]Epoch: 3, train for the 204-th batch, train loss: 0.6315466165542603:  86%|█████████▍ | 204/237 [02:02<00:21,  1.54it/s]Epoch: 2, train for the 295-th batch, train loss: 0.4248553514480591:  77%|████████▍  | 295/383 [02:58<00:57,  1.54it/s]evaluate for the 23-th batch, evaluate loss: 0.6633740067481995:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.27it/s]evaluate for the 23-th batch, evaluate loss: 0.6633740067481995:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.26it/s]evaluate for the 24-th batch, evaluate loss: 0.6604312062263489:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.26it/s]evaluate for the 24-th batch, evaluate loss: 0.6604312062263489:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.25it/s]Epoch: 6, train for the 26-th batch, train loss: 0.24059316515922546:  21%|██▌         | 25/119 [00:15<00:56,  1.68it/s]Epoch: 6, train for the 26-th batch, train loss: 0.24059316515922546:  22%|██▌         | 26/119 [00:15<00:55,  1.68it/s]evaluate for the 25-th batch, evaluate loss: 0.7054286003112793:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.25it/s]evaluate for the 25-th batch, evaluate loss: 0.7054286003112793: 100%|██████████████████| 25/25 [00:02<00:00,  9.34it/s]
INFO:root:Epoch: 9, learning rate: 0.0001, train loss: 0.5574
INFO:root:train average_precision, 0.8240
INFO:root:train roc_auc, 0.7890
INFO:root:validate loss: 0.5088
INFO:root:validate average_precision, 0.8428
INFO:root:validate roc_auc, 0.8038
INFO:root:new node validate loss: 0.6821
INFO:root:new node validate first_1_average_precision, 0.5931
INFO:root:new node validate first_1_roc_auc, 0.5436
INFO:root:new node validate first_3_average_precision, 0.6757
INFO:root:new node validate first_3_roc_auc, 0.6372
INFO:root:new node validate first_10_average_precision, 0.7455
INFO:root:new node validate first_10_roc_auc, 0.7102
INFO:root:new node validate average_precision, 0.7079
INFO:root:new node validate roc_auc, 0.6573
INFO:root:save model ./saved_models/DyGFormer/ia-retweet-pol/DyGFormer_seed0_dygformer-ia-retweet-pol-old/DyGFormer_seed0_dygformer-ia-retweet-pol-old.pkl
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]Epoch: 5, train for the 72-th batch, train loss: 0.5354355573654175:  49%|██████▎      | 71/146 [00:41<00:44,  1.68it/s]Epoch: 5, train for the 72-th batch, train loss: 0.5354355573654175:  49%|██████▍      | 72/146 [00:41<00:44,  1.68it/s]Epoch: 2, train for the 296-th batch, train loss: 0.3880693018436432:  77%|████████▍  | 295/383 [02:58<00:57,  1.54it/s]Epoch: 2, train for the 296-th batch, train loss: 0.3880693018436432:  77%|████████▌  | 296/383 [02:58<00:52,  1.65it/s]Epoch: 10, train for the 1-th batch, train loss: 1.2161527872085571:   0%|                      | 0/151 [00:00<?, ?it/s]Epoch: 10, train for the 1-th batch, train loss: 1.2161527872085571:   1%|              | 1/151 [00:00<00:26,  5.64it/s]Epoch: 3, train for the 205-th batch, train loss: 0.6314611434936523:  86%|█████████▍ | 204/237 [02:03<00:21,  1.54it/s]Epoch: 3, train for the 205-th batch, train loss: 0.6314611434936523:  86%|█████████▌ | 205/237 [02:03<00:21,  1.52it/s]Epoch: 10, train for the 2-th batch, train loss: 1.1895713806152344:   1%|              | 1/151 [00:00<00:26,  5.64it/s]Epoch: 10, train for the 2-th batch, train loss: 1.1895713806152344:   1%|▏             | 2/151 [00:00<00:26,  5.66it/s]Epoch: 6, train for the 27-th batch, train loss: 0.19960488379001617:  22%|██▌         | 26/119 [00:16<00:55,  1.68it/s]Epoch: 6, train for the 27-th batch, train loss: 0.19960488379001617:  23%|██▋         | 27/119 [00:16<00:54,  1.68it/s]Epoch: 10, train for the 3-th batch, train loss: 0.7138411402702332:   1%|▏             | 2/151 [00:00<00:26,  5.66it/s]Epoch: 10, train for the 3-th batch, train loss: 0.7138411402702332:   2%|▎             | 3/151 [00:00<00:25,  5.80it/s]Epoch: 10, train for the 4-th batch, train loss: 0.5877492427825928:   2%|▎             | 3/151 [00:00<00:25,  5.80it/s]Epoch: 10, train for the 4-th batch, train loss: 0.5877492427825928:   3%|▎             | 4/151 [00:00<00:25,  5.81it/s]Epoch: 5, train for the 73-th batch, train loss: 0.5219363570213318:  49%|██████▍      | 72/146 [00:42<00:44,  1.68it/s]Epoch: 5, train for the 73-th batch, train loss: 0.5219363570213318:  50%|██████▌      | 73/146 [00:42<00:43,  1.68it/s]Epoch: 3, train for the 206-th batch, train loss: 0.6363940834999084:  86%|█████████▌ | 205/237 [02:03<00:21,  1.52it/s]Epoch: 3, train for the 206-th batch, train loss: 0.6363940834999084:  87%|█████████▌ | 206/237 [02:03<00:18,  1.68it/s]Epoch: 10, train for the 5-th batch, train loss: 0.4665312170982361:   3%|▎             | 4/151 [00:00<00:25,  5.81it/s]Epoch: 10, train for the 5-th batch, train loss: 0.4665312170982361:   3%|▍             | 5/151 [00:00<00:25,  5.64it/s]Epoch: 2, train for the 297-th batch, train loss: 0.5319724678993225:  77%|████████▌  | 296/383 [02:59<00:52,  1.65it/s]Epoch: 2, train for the 297-th batch, train loss: 0.5319724678993225:  78%|████████▌  | 297/383 [02:59<00:55,  1.54it/s]Epoch: 6, train for the 28-th batch, train loss: 0.24339988827705383:  23%|██▋         | 27/119 [00:17<00:54,  1.68it/s]Epoch: 6, train for the 28-th batch, train loss: 0.24339988827705383:  24%|██▊         | 28/119 [00:17<00:54,  1.68it/s]Epoch: 10, train for the 6-th batch, train loss: 0.43317121267318726:   3%|▍            | 5/151 [00:01<00:25,  5.64it/s]Epoch: 10, train for the 6-th batch, train loss: 0.43317121267318726:   4%|▌            | 6/151 [00:01<00:26,  5.54it/s]Epoch: 10, train for the 7-th batch, train loss: 0.3440227508544922:   4%|▌             | 6/151 [00:01<00:26,  5.54it/s]Epoch: 10, train for the 7-th batch, train loss: 0.3440227508544922:   5%|▋             | 7/151 [00:01<00:26,  5.44it/s]Epoch: 5, train for the 74-th batch, train loss: 0.5491890907287598:  50%|██████▌      | 73/146 [00:42<00:43,  1.68it/s]Epoch: 5, train for the 74-th batch, train loss: 0.5491890907287598:  51%|██████▌      | 74/146 [00:42<00:42,  1.68it/s]Epoch: 3, train for the 207-th batch, train loss: 0.6541273593902588:  87%|█████████▌ | 206/237 [02:04<00:18,  1.68it/s]Epoch: 3, train for the 207-th batch, train loss: 0.6541273593902588:  87%|█████████▌ | 207/237 [02:04<00:17,  1.69it/s]Epoch: 10, train for the 8-th batch, train loss: 0.6377713680267334:   5%|▋             | 7/151 [00:01<00:26,  5.44it/s]Epoch: 10, train for the 8-th batch, train loss: 0.6377713680267334:   5%|▋             | 8/151 [00:01<00:26,  5.41it/s]Epoch: 2, train for the 298-th batch, train loss: 0.4503566026687622:  78%|████████▌  | 297/383 [02:59<00:55,  1.54it/s]Epoch: 2, train for the 298-th batch, train loss: 0.4503566026687622:  78%|████████▌  | 298/383 [02:59<00:53,  1.58it/s]Epoch: 6, train for the 29-th batch, train loss: 0.21821944415569305:  24%|██▊         | 28/119 [00:17<00:54,  1.68it/s]Epoch: 6, train for the 29-th batch, train loss: 0.21821944415569305:  24%|██▉         | 29/119 [00:17<00:53,  1.68it/s]Epoch: 10, train for the 9-th batch, train loss: 0.5642385482788086:   5%|▋             | 8/151 [00:01<00:26,  5.41it/s]Epoch: 10, train for the 9-th batch, train loss: 0.5642385482788086:   6%|▊             | 9/151 [00:01<00:26,  5.33it/s]Epoch: 10, train for the 10-th batch, train loss: 0.42840030789375305:   6%|▋           | 9/151 [00:01<00:26,  5.33it/s]Epoch: 10, train for the 10-th batch, train loss: 0.42840030789375305:   7%|▋          | 10/151 [00:01<00:26,  5.23it/s]Epoch: 5, train for the 75-th batch, train loss: 0.569956362247467:  51%|███████       | 74/146 [00:43<00:42,  1.68it/s]Epoch: 5, train for the 75-th batch, train loss: 0.569956362247467:  51%|███████▏      | 75/146 [00:43<00:42,  1.68it/s]Epoch: 3, train for the 208-th batch, train loss: 0.6498770117759705:  87%|█████████▌ | 207/237 [02:04<00:17,  1.69it/s]Epoch: 3, train for the 208-th batch, train loss: 0.6498770117759705:  88%|█████████▋ | 208/237 [02:04<00:17,  1.69it/s]Epoch: 10, train for the 11-th batch, train loss: 0.6086009740829468:   7%|▊           | 10/151 [00:02<00:26,  5.23it/s]Epoch: 10, train for the 11-th batch, train loss: 0.6086009740829468:   7%|▊           | 11/151 [00:02<00:27,  5.15it/s]Epoch: 2, train for the 299-th batch, train loss: 0.4873170256614685:  78%|████████▌  | 298/383 [03:00<00:53,  1.58it/s]Epoch: 2, train for the 299-th batch, train loss: 0.4873170256614685:  78%|████████▌  | 299/383 [03:00<00:52,  1.61it/s]Epoch: 6, train for the 30-th batch, train loss: 0.21069148182868958:  24%|██▉         | 29/119 [00:18<00:53,  1.68it/s]Epoch: 6, train for the 30-th batch, train loss: 0.21069148182868958:  25%|███         | 30/119 [00:18<00:52,  1.68it/s]Epoch: 10, train for the 12-th batch, train loss: 0.465818852186203:   7%|▉            | 11/151 [00:02<00:27,  5.15it/s]Epoch: 10, train for the 12-th batch, train loss: 0.465818852186203:   8%|█            | 12/151 [00:02<00:27,  5.11it/s]Epoch: 10, train for the 13-th batch, train loss: 0.7047247886657715:   8%|▉           | 12/151 [00:02<00:27,  5.11it/s]Epoch: 10, train for the 13-th batch, train loss: 0.7047247886657715:   9%|█           | 13/151 [00:02<00:27,  5.02it/s]Epoch: 5, train for the 76-th batch, train loss: 0.540458619594574:  51%|███████▏      | 75/146 [00:43<00:42,  1.68it/s]Epoch: 5, train for the 76-th batch, train loss: 0.540458619594574:  52%|███████▎      | 76/146 [00:43<00:41,  1.68it/s]Epoch: 3, train for the 209-th batch, train loss: 0.6306761503219604:  88%|█████████▋ | 208/237 [02:05<00:17,  1.69it/s]Epoch: 3, train for the 209-th batch, train loss: 0.6306761503219604:  88%|█████████▋ | 209/237 [02:05<00:16,  1.69it/s]Epoch: 10, train for the 14-th batch, train loss: 0.768259584903717:   9%|█            | 13/151 [00:02<00:27,  5.02it/s]Epoch: 10, train for the 14-th batch, train loss: 0.768259584903717:   9%|█▏           | 14/151 [00:02<00:27,  4.94it/s]Epoch: 2, train for the 300-th batch, train loss: 0.422132670879364:  78%|█████████▎  | 299/383 [03:01<00:52,  1.61it/s]Epoch: 2, train for the 300-th batch, train loss: 0.422132670879364:  78%|█████████▍  | 300/383 [03:01<00:51,  1.63it/s]Epoch: 6, train for the 31-th batch, train loss: 0.22017072141170502:  25%|███         | 30/119 [00:18<00:52,  1.68it/s]Epoch: 6, train for the 31-th batch, train loss: 0.22017072141170502:  26%|███▏        | 31/119 [00:18<00:52,  1.68it/s]Epoch: 10, train for the 15-th batch, train loss: 0.7208404541015625:   9%|█           | 14/151 [00:02<00:27,  4.94it/s]Epoch: 10, train for the 15-th batch, train loss: 0.7208404541015625:  10%|█▏          | 15/151 [00:02<00:28,  4.83it/s]Epoch: 10, train for the 16-th batch, train loss: 0.5454990863800049:  10%|█▏          | 15/151 [00:03<00:28,  4.83it/s]Epoch: 10, train for the 16-th batch, train loss: 0.5454990863800049:  11%|█▎          | 16/151 [00:03<00:27,  4.84it/s]Epoch: 5, train for the 77-th batch, train loss: 0.5520306825637817:  52%|██████▊      | 76/146 [00:44<00:41,  1.68it/s]Epoch: 5, train for the 77-th batch, train loss: 0.5520306825637817:  53%|██████▊      | 77/146 [00:44<00:41,  1.68it/s]Epoch: 3, train for the 210-th batch, train loss: 0.6258945465087891:  88%|█████████▋ | 209/237 [02:06<00:16,  1.69it/s]Epoch: 3, train for the 210-th batch, train loss: 0.6258945465087891:  89%|█████████▋ | 210/237 [02:06<00:16,  1.68it/s]Epoch: 10, train for the 17-th batch, train loss: 0.49893683195114136:  11%|█▏         | 16/151 [00:03<00:27,  4.84it/s]Epoch: 10, train for the 17-th batch, train loss: 0.49893683195114136:  11%|█▏         | 17/151 [00:03<00:27,  4.91it/s]Epoch: 2, train for the 301-th batch, train loss: 0.412078857421875:  78%|█████████▍  | 300/383 [03:01<00:51,  1.63it/s]Epoch: 2, train for the 301-th batch, train loss: 0.412078857421875:  79%|█████████▍  | 301/383 [03:01<00:50,  1.63it/s]Epoch: 6, train for the 32-th batch, train loss: 0.21551793813705444:  26%|███▏        | 31/119 [00:19<00:52,  1.68it/s]Epoch: 6, train for the 32-th batch, train loss: 0.21551793813705444:  27%|███▏        | 32/119 [00:19<00:51,  1.68it/s]Epoch: 10, train for the 18-th batch, train loss: 0.6413546800613403:  11%|█▎          | 17/151 [00:03<00:27,  4.91it/s]Epoch: 10, train for the 18-th batch, train loss: 0.6413546800613403:  12%|█▍          | 18/151 [00:03<00:27,  4.88it/s]Epoch: 10, train for the 19-th batch, train loss: 0.6954283118247986:  12%|█▍          | 18/151 [00:03<00:27,  4.88it/s]Epoch: 10, train for the 19-th batch, train loss: 0.6954283118247986:  13%|█▌          | 19/151 [00:03<00:27,  4.79it/s]Epoch: 5, train for the 78-th batch, train loss: 0.5445464849472046:  53%|██████▊      | 77/146 [00:45<00:41,  1.68it/s]Epoch: 5, train for the 78-th batch, train loss: 0.5445464849472046:  53%|██████▉      | 78/146 [00:45<00:40,  1.68it/s]Epoch: 3, train for the 211-th batch, train loss: 0.640735924243927:  89%|██████████▋ | 210/237 [02:06<00:16,  1.68it/s]Epoch: 3, train for the 211-th batch, train loss: 0.640735924243927:  89%|██████████▋ | 211/237 [02:06<00:15,  1.67it/s]Epoch: 2, train for the 302-th batch, train loss: 0.422099232673645:  79%|█████████▍  | 301/383 [03:02<00:50,  1.63it/s]Epoch: 2, train for the 302-th batch, train loss: 0.422099232673645:  79%|█████████▍  | 302/383 [03:02<00:49,  1.64it/s]Epoch: 10, train for the 20-th batch, train loss: 0.6405828595161438:  13%|█▌          | 19/151 [00:03<00:27,  4.79it/s]Epoch: 10, train for the 20-th batch, train loss: 0.6405828595161438:  13%|█▌          | 20/151 [00:03<00:27,  4.72it/s]Epoch: 6, train for the 33-th batch, train loss: 0.19302453100681305:  27%|███▏        | 32/119 [00:20<00:51,  1.68it/s]Epoch: 6, train for the 33-th batch, train loss: 0.19302453100681305:  28%|███▎        | 33/119 [00:20<00:51,  1.68it/s]Epoch: 10, train for the 21-th batch, train loss: 0.6367111206054688:  13%|█▌          | 20/151 [00:04<00:27,  4.72it/s]Epoch: 10, train for the 21-th batch, train loss: 0.6367111206054688:  14%|█▋          | 21/151 [00:04<00:27,  4.68it/s]Epoch: 5, train for the 79-th batch, train loss: 0.5556175708770752:  53%|██████▉      | 78/146 [00:45<00:40,  1.68it/s]Epoch: 5, train for the 79-th batch, train loss: 0.5556175708770752:  54%|███████      | 79/146 [00:45<00:39,  1.68it/s]Epoch: 10, train for the 22-th batch, train loss: 0.4810100495815277:  14%|█▋          | 21/151 [00:04<00:27,  4.68it/s]Epoch: 10, train for the 22-th batch, train loss: 0.4810100495815277:  15%|█▋          | 22/151 [00:04<00:27,  4.76it/s]Epoch: 3, train for the 212-th batch, train loss: 0.6178253889083862:  89%|█████████▊ | 211/237 [02:07<00:15,  1.67it/s]Epoch: 3, train for the 212-th batch, train loss: 0.6178253889083862:  89%|█████████▊ | 212/237 [02:07<00:15,  1.66it/s]Epoch: 2, train for the 303-th batch, train loss: 0.4557129442691803:  79%|████████▋  | 302/383 [03:02<00:49,  1.64it/s]Epoch: 2, train for the 303-th batch, train loss: 0.4557129442691803:  79%|████████▋  | 303/383 [03:02<00:48,  1.64it/s]Epoch: 10, train for the 23-th batch, train loss: 0.5776742696762085:  15%|█▋          | 22/151 [00:04<00:27,  4.76it/s]Epoch: 10, train for the 23-th batch, train loss: 0.5776742696762085:  15%|█▊          | 23/151 [00:04<00:27,  4.74it/s]Epoch: 6, train for the 34-th batch, train loss: 0.20052741467952728:  28%|███▎        | 33/119 [00:20<00:51,  1.68it/s]Epoch: 6, train for the 34-th batch, train loss: 0.20052741467952728:  29%|███▍        | 34/119 [00:20<00:50,  1.68it/s]Epoch: 10, train for the 24-th batch, train loss: 0.6207883954048157:  15%|█▊          | 23/151 [00:04<00:27,  4.74it/s]Epoch: 10, train for the 24-th batch, train loss: 0.6207883954048157:  16%|█▉          | 24/151 [00:04<00:27,  4.68it/s]Epoch: 5, train for the 80-th batch, train loss: 0.5536455512046814:  54%|███████      | 79/146 [00:46<00:39,  1.68it/s]Epoch: 5, train for the 80-th batch, train loss: 0.5536455512046814:  55%|███████      | 80/146 [00:46<00:39,  1.68it/s]Epoch: 3, train for the 213-th batch, train loss: 0.6175046563148499:  89%|█████████▊ | 212/237 [02:07<00:15,  1.66it/s]Epoch: 3, train for the 213-th batch, train loss: 0.6175046563148499:  90%|█████████▉ | 213/237 [02:07<00:14,  1.66it/s]Epoch: 10, train for the 25-th batch, train loss: 0.5620622038841248:  16%|█▉          | 24/151 [00:04<00:27,  4.68it/s]Epoch: 10, train for the 25-th batch, train loss: 0.5620622038841248:  17%|█▉          | 25/151 [00:04<00:26,  4.72it/s]Epoch: 2, train for the 304-th batch, train loss: 0.45949164032936096:  79%|███████▉  | 303/383 [03:03<00:48,  1.64it/s]Epoch: 2, train for the 304-th batch, train loss: 0.45949164032936096:  79%|███████▉  | 304/383 [03:03<00:47,  1.65it/s]Epoch: 10, train for the 26-th batch, train loss: 0.5570963025093079:  17%|█▉          | 25/151 [00:05<00:26,  4.72it/s]Epoch: 10, train for the 26-th batch, train loss: 0.5570963025093079:  17%|██          | 26/151 [00:05<00:26,  4.71it/s]Epoch: 6, train for the 35-th batch, train loss: 0.15943385660648346:  29%|███▍        | 34/119 [00:21<00:50,  1.68it/s]Epoch: 6, train for the 35-th batch, train loss: 0.15943385660648346:  29%|███▌        | 35/119 [00:21<00:50,  1.68it/s]Epoch: 10, train for the 27-th batch, train loss: 0.5532678365707397:  17%|██          | 26/151 [00:05<00:26,  4.71it/s]Epoch: 10, train for the 27-th batch, train loss: 0.5532678365707397:  18%|██▏         | 27/151 [00:05<00:26,  4.69it/s]Epoch: 5, train for the 81-th batch, train loss: 0.5244548916816711:  55%|███████      | 80/146 [00:46<00:39,  1.68it/s]Epoch: 5, train for the 81-th batch, train loss: 0.5244548916816711:  55%|███████▏     | 81/146 [00:46<00:38,  1.68it/s]Epoch: 3, train for the 214-th batch, train loss: 0.6532473564147949:  90%|█████████▉ | 213/237 [02:08<00:14,  1.66it/s]Epoch: 3, train for the 214-th batch, train loss: 0.6532473564147949:  90%|█████████▉ | 214/237 [02:08<00:13,  1.67it/s]Epoch: 10, train for the 28-th batch, train loss: 0.4095660448074341:  18%|██▏         | 27/151 [00:05<00:26,  4.69it/s]Epoch: 10, train for the 28-th batch, train loss: 0.4095660448074341:  19%|██▏         | 28/151 [00:05<00:26,  4.73it/s]Epoch: 2, train for the 305-th batch, train loss: 0.5289897322654724:  79%|████████▋  | 304/383 [03:04<00:47,  1.65it/s]Epoch: 2, train for the 305-th batch, train loss: 0.5289897322654724:  80%|████████▊  | 305/383 [03:04<00:47,  1.66it/s]Epoch: 6, train for the 36-th batch, train loss: 0.18810521066188812:  29%|███▌        | 35/119 [00:21<00:50,  1.68it/s]Epoch: 6, train for the 36-th batch, train loss: 0.18810521066188812:  30%|███▋        | 36/119 [00:21<00:49,  1.68it/s]Epoch: 10, train for the 29-th batch, train loss: 0.7263274788856506:  19%|██▏         | 28/151 [00:05<00:26,  4.73it/s]Epoch: 10, train for the 29-th batch, train loss: 0.7263274788856506:  19%|██▎         | 29/151 [00:05<00:29,  4.18it/s]Epoch: 5, train for the 82-th batch, train loss: 0.5231713056564331:  55%|███████▏     | 81/146 [00:47<00:38,  1.68it/s]Epoch: 5, train for the 82-th batch, train loss: 0.5231713056564331:  56%|███████▎     | 82/146 [00:47<00:38,  1.68it/s]Epoch: 10, train for the 30-th batch, train loss: 0.45907625555992126:  19%|██         | 29/151 [00:06<00:29,  4.18it/s]Epoch: 10, train for the 30-th batch, train loss: 0.45907625555992126:  20%|██▏        | 30/151 [00:06<00:27,  4.34it/s]Epoch: 3, train for the 215-th batch, train loss: 0.6285179257392883:  90%|█████████▉ | 214/237 [02:09<00:13,  1.67it/s]Epoch: 3, train for the 215-th batch, train loss: 0.6285179257392883:  91%|█████████▉ | 215/237 [02:09<00:13,  1.67it/s]Epoch: 2, train for the 306-th batch, train loss: 0.3798292875289917:  80%|████████▊  | 305/383 [03:04<00:47,  1.66it/s]Epoch: 2, train for the 306-th batch, train loss: 0.3798292875289917:  80%|████████▊  | 306/383 [03:04<00:46,  1.66it/s]Epoch: 10, train for the 31-th batch, train loss: 0.7701278328895569:  20%|██▍         | 30/151 [00:06<00:27,  4.34it/s]Epoch: 10, train for the 31-th batch, train loss: 0.7701278328895569:  21%|██▍         | 31/151 [00:06<00:27,  4.39it/s]Epoch: 6, train for the 37-th batch, train loss: 0.1946905106306076:  30%|███▉         | 36/119 [00:22<00:49,  1.68it/s]Epoch: 6, train for the 37-th batch, train loss: 0.1946905106306076:  31%|████         | 37/119 [00:22<00:48,  1.68it/s]Epoch: 10, train for the 32-th batch, train loss: 0.7077252268791199:  21%|██▍         | 31/151 [00:06<00:27,  4.39it/s]Epoch: 10, train for the 32-th batch, train loss: 0.7077252268791199:  21%|██▌         | 32/151 [00:06<00:26,  4.42it/s]Epoch: 5, train for the 83-th batch, train loss: 0.5192772746086121:  56%|███████▎     | 82/146 [00:48<00:38,  1.68it/s]Epoch: 5, train for the 83-th batch, train loss: 0.5192772746086121:  57%|███████▍     | 83/146 [00:48<00:37,  1.67it/s]Epoch: 3, train for the 216-th batch, train loss: 0.6166572570800781:  91%|█████████▉ | 215/237 [02:09<00:13,  1.67it/s]Epoch: 3, train for the 216-th batch, train loss: 0.6166572570800781:  91%|██████████ | 216/237 [02:09<00:12,  1.67it/s]Epoch: 10, train for the 33-th batch, train loss: 0.5834916830062866:  21%|██▌         | 32/151 [00:06<00:26,  4.42it/s]Epoch: 10, train for the 33-th batch, train loss: 0.5834916830062866:  22%|██▌         | 33/151 [00:06<00:26,  4.46it/s]Epoch: 2, train for the 307-th batch, train loss: 0.42555609345436096:  80%|███████▉  | 306/383 [03:05<00:46,  1.66it/s]Epoch: 2, train for the 307-th batch, train loss: 0.42555609345436096:  80%|████████  | 307/383 [03:05<00:45,  1.66it/s]Epoch: 6, train for the 38-th batch, train loss: 0.172639399766922:  31%|████▎         | 37/119 [00:23<00:48,  1.68it/s]Epoch: 6, train for the 38-th batch, train loss: 0.172639399766922:  32%|████▍         | 38/119 [00:23<00:48,  1.67it/s]Epoch: 10, train for the 34-th batch, train loss: 0.5407877564430237:  22%|██▌         | 33/151 [00:07<00:26,  4.46it/s]Epoch: 10, train for the 34-th batch, train loss: 0.5407877564430237:  23%|██▋         | 34/151 [00:07<00:25,  4.52it/s]Epoch: 10, train for the 35-th batch, train loss: 0.6588874459266663:  23%|██▋         | 34/151 [00:07<00:25,  4.52it/s]Epoch: 10, train for the 35-th batch, train loss: 0.6588874459266663:  23%|██▊         | 35/151 [00:07<00:25,  4.52it/s]Epoch: 5, train for the 84-th batch, train loss: 0.4843977987766266:  57%|███████▍     | 83/146 [00:48<00:37,  1.67it/s]Epoch: 5, train for the 84-th batch, train loss: 0.4843977987766266:  58%|███████▍     | 84/146 [00:48<00:37,  1.67it/s]Epoch: 3, train for the 217-th batch, train loss: 0.6122795343399048:  91%|██████████ | 216/237 [02:10<00:12,  1.67it/s]Epoch: 3, train for the 217-th batch, train loss: 0.6122795343399048:  92%|██████████ | 217/237 [02:10<00:12,  1.67it/s]Epoch: 10, train for the 36-th batch, train loss: 0.6809976100921631:  23%|██▊         | 35/151 [00:07<00:25,  4.52it/s]Epoch: 10, train for the 36-th batch, train loss: 0.6809976100921631:  24%|██▊         | 36/151 [00:07<00:25,  4.53it/s]Epoch: 2, train for the 308-th batch, train loss: 0.41726556420326233:  80%|████████  | 307/383 [03:05<00:45,  1.66it/s]Epoch: 2, train for the 308-th batch, train loss: 0.41726556420326233:  80%|████████  | 308/383 [03:05<00:45,  1.66it/s]Epoch: 6, train for the 39-th batch, train loss: 0.20923195779323578:  32%|███▊        | 38/119 [00:23<00:48,  1.67it/s]Epoch: 6, train for the 39-th batch, train loss: 0.20923195779323578:  33%|███▉        | 39/119 [00:23<00:47,  1.67it/s]Epoch: 10, train for the 37-th batch, train loss: 0.7084675431251526:  24%|██▊         | 36/151 [00:07<00:25,  4.53it/s]Epoch: 10, train for the 37-th batch, train loss: 0.7084675431251526:  25%|██▉         | 37/151 [00:07<00:25,  4.51it/s]Epoch: 5, train for the 85-th batch, train loss: 0.5199706554412842:  58%|███████▍     | 84/146 [00:49<00:37,  1.67it/s]Epoch: 5, train for the 85-th batch, train loss: 0.5199706554412842:  58%|███████▌     | 85/146 [00:49<00:36,  1.68it/s]Epoch: 10, train for the 38-th batch, train loss: 0.6539784073829651:  25%|██▉         | 37/151 [00:07<00:25,  4.51it/s]Epoch: 10, train for the 38-th batch, train loss: 0.6539784073829651:  25%|███         | 38/151 [00:07<00:24,  4.53it/s]Epoch: 3, train for the 218-th batch, train loss: 0.6454986929893494:  92%|██████████ | 217/237 [02:10<00:12,  1.67it/s]Epoch: 3, train for the 218-th batch, train loss: 0.6454986929893494:  92%|██████████ | 218/237 [02:10<00:11,  1.66it/s]Epoch: 10, train for the 39-th batch, train loss: 0.6562691926956177:  25%|███         | 38/151 [00:08<00:24,  4.53it/s]Epoch: 10, train for the 39-th batch, train loss: 0.6562691926956177:  26%|███         | 39/151 [00:08<00:24,  4.52it/s]Epoch: 2, train for the 309-th batch, train loss: 0.4781198799610138:  80%|████████▊  | 308/383 [03:06<00:45,  1.66it/s]Epoch: 2, train for the 309-th batch, train loss: 0.4781198799610138:  81%|████████▊  | 309/383 [03:06<00:44,  1.66it/s]Epoch: 6, train for the 40-th batch, train loss: 0.1626749038696289:  33%|████▎        | 39/119 [00:24<00:47,  1.67it/s]Epoch: 6, train for the 40-th batch, train loss: 0.1626749038696289:  34%|████▎        | 40/119 [00:24<00:47,  1.67it/s]Epoch: 10, train for the 40-th batch, train loss: 0.5746001601219177:  26%|███         | 39/151 [00:08<00:24,  4.52it/s]Epoch: 10, train for the 40-th batch, train loss: 0.5746001601219177:  26%|███▏        | 40/151 [00:08<00:24,  4.55it/s]Epoch: 5, train for the 86-th batch, train loss: 0.5190730690956116:  58%|███████▌     | 85/146 [00:49<00:36,  1.68it/s]Epoch: 5, train for the 86-th batch, train loss: 0.5190730690956116:  59%|███████▋     | 86/146 [00:49<00:35,  1.67it/s]Epoch: 3, train for the 219-th batch, train loss: 0.6156657934188843:  92%|██████████ | 218/237 [02:11<00:11,  1.66it/s]Epoch: 3, train for the 219-th batch, train loss: 0.6156657934188843:  92%|██████████▏| 219/237 [02:11<00:10,  1.66it/s]Epoch: 10, train for the 41-th batch, train loss: 0.5580615401268005:  26%|███▏        | 40/151 [00:08<00:24,  4.55it/s]Epoch: 10, train for the 41-th batch, train loss: 0.5580615401268005:  27%|███▎        | 41/151 [00:08<00:25,  4.31it/s]Epoch: 2, train for the 310-th batch, train loss: 0.5100589394569397:  81%|████████▊  | 309/383 [03:07<00:44,  1.66it/s]Epoch: 2, train for the 310-th batch, train loss: 0.5100589394569397:  81%|████████▉  | 310/383 [03:07<00:44,  1.65it/s]Epoch: 6, train for the 41-th batch, train loss: 0.26076382398605347:  34%|████        | 40/119 [00:24<00:47,  1.67it/s]Epoch: 6, train for the 41-th batch, train loss: 0.26076382398605347:  34%|████▏       | 41/119 [00:24<00:46,  1.68it/s]Epoch: 10, train for the 42-th batch, train loss: 0.502284824848175:  27%|███▌         | 41/151 [00:08<00:25,  4.31it/s]Epoch: 10, train for the 42-th batch, train loss: 0.502284824848175:  28%|███▌         | 42/151 [00:08<00:24,  4.41it/s]Epoch: 10, train for the 43-th batch, train loss: 0.6354551911354065:  28%|███▎        | 42/151 [00:09<00:24,  4.41it/s]Epoch: 10, train for the 43-th batch, train loss: 0.6354551911354065:  28%|███▍        | 43/151 [00:09<00:24,  4.44it/s]Epoch: 5, train for the 87-th batch, train loss: 0.5183513164520264:  59%|███████▋     | 86/146 [00:50<00:35,  1.67it/s]Epoch: 5, train for the 87-th batch, train loss: 0.5183513164520264:  60%|███████▋     | 87/146 [00:50<00:35,  1.68it/s]Epoch: 3, train for the 220-th batch, train loss: 0.6444258689880371:  92%|██████████▏| 219/237 [02:12<00:10,  1.66it/s]Epoch: 3, train for the 220-th batch, train loss: 0.6444258689880371:  93%|██████████▏| 220/237 [02:12<00:10,  1.66it/s]Epoch: 10, train for the 44-th batch, train loss: 0.5916101932525635:  28%|███▍        | 43/151 [00:09<00:24,  4.44it/s]Epoch: 10, train for the 44-th batch, train loss: 0.5916101932525635:  29%|███▍        | 44/151 [00:09<00:23,  4.46it/s]Epoch: 2, train for the 311-th batch, train loss: 0.47736725211143494:  81%|████████  | 310/383 [03:07<00:44,  1.65it/s]Epoch: 2, train for the 311-th batch, train loss: 0.47736725211143494:  81%|████████  | 311/383 [03:07<00:43,  1.66it/s]Epoch: 6, train for the 42-th batch, train loss: 0.2387329787015915:  34%|████▍        | 41/119 [00:25<00:46,  1.68it/s]Epoch: 6, train for the 42-th batch, train loss: 0.2387329787015915:  35%|████▌        | 42/119 [00:25<00:45,  1.68it/s]Epoch: 10, train for the 45-th batch, train loss: 0.6217631697654724:  29%|███▍        | 44/151 [00:09<00:23,  4.46it/s]Epoch: 10, train for the 45-th batch, train loss: 0.6217631697654724:  30%|███▌        | 45/151 [00:09<00:23,  4.47it/s]Epoch: 5, train for the 88-th batch, train loss: 0.5417687296867371:  60%|███████▋     | 87/146 [00:51<00:35,  1.68it/s]Epoch: 5, train for the 88-th batch, train loss: 0.5417687296867371:  60%|███████▊     | 88/146 [00:51<00:34,  1.68it/s]Epoch: 10, train for the 46-th batch, train loss: 0.6303927302360535:  30%|███▌        | 45/151 [00:09<00:23,  4.47it/s]Epoch: 10, train for the 46-th batch, train loss: 0.6303927302360535:  30%|███▋        | 46/151 [00:09<00:23,  4.47it/s]Epoch: 3, train for the 221-th batch, train loss: 0.6396631598472595:  93%|██████████▏| 220/237 [02:12<00:10,  1.66it/s]Epoch: 3, train for the 221-th batch, train loss: 0.6396631598472595:  93%|██████████▎| 221/237 [02:12<00:09,  1.66it/s]Epoch: 10, train for the 47-th batch, train loss: 0.6367564797401428:  30%|███▋        | 46/151 [00:09<00:23,  4.47it/s]Epoch: 10, train for the 47-th batch, train loss: 0.6367564797401428:  31%|███▋        | 47/151 [00:09<00:23,  4.47it/s]Epoch: 2, train for the 312-th batch, train loss: 0.3699460029602051:  81%|████████▉  | 311/383 [03:08<00:43,  1.66it/s]Epoch: 2, train for the 312-th batch, train loss: 0.3699460029602051:  81%|████████▉  | 312/383 [03:08<00:42,  1.67it/s]Epoch: 6, train for the 43-th batch, train loss: 0.21233566105365753:  35%|████▏       | 42/119 [00:26<00:45,  1.68it/s]Epoch: 6, train for the 43-th batch, train loss: 0.21233566105365753:  36%|████▎       | 43/119 [00:26<00:45,  1.67it/s]Epoch: 10, train for the 48-th batch, train loss: 0.586941659450531:  31%|████         | 47/151 [00:10<00:23,  4.47it/s]Epoch: 10, train for the 48-th batch, train loss: 0.586941659450531:  32%|████▏        | 48/151 [00:10<00:23,  4.46it/s]Epoch: 5, train for the 89-th batch, train loss: 0.5074366927146912:  60%|███████▊     | 88/146 [00:51<00:34,  1.68it/s]Epoch: 5, train for the 89-th batch, train loss: 0.5074366927146912:  61%|███████▉     | 89/146 [00:51<00:34,  1.67it/s]Epoch: 10, train for the 49-th batch, train loss: 0.6015022993087769:  32%|███▊        | 48/151 [00:10<00:23,  4.46it/s]Epoch: 10, train for the 49-th batch, train loss: 0.6015022993087769:  32%|███▉        | 49/151 [00:10<00:22,  4.47it/s]Epoch: 3, train for the 222-th batch, train loss: 0.6440670490264893:  93%|██████████▎| 221/237 [02:13<00:09,  1.66it/s]Epoch: 3, train for the 222-th batch, train loss: 0.6440670490264893:  94%|██████████▎| 222/237 [02:13<00:08,  1.67it/s]Epoch: 2, train for the 313-th batch, train loss: 0.4392431080341339:  81%|████████▉  | 312/383 [03:08<00:42,  1.67it/s]Epoch: 2, train for the 313-th batch, train loss: 0.4392431080341339:  82%|████████▉  | 313/383 [03:08<00:41,  1.67it/s]Epoch: 6, train for the 44-th batch, train loss: 0.20338311791419983:  36%|████▎       | 43/119 [00:26<00:45,  1.67it/s]Epoch: 6, train for the 44-th batch, train loss: 0.20338311791419983:  37%|████▍       | 44/119 [00:26<00:44,  1.67it/s]Epoch: 10, train for the 50-th batch, train loss: 0.523301362991333:  32%|████▏        | 49/151 [00:10<00:22,  4.47it/s]Epoch: 10, train for the 50-th batch, train loss: 0.523301362991333:  33%|████▎        | 50/151 [00:10<00:22,  4.50it/s]Epoch: 10, train for the 51-th batch, train loss: 0.6112668514251709:  33%|███▉        | 50/151 [00:10<00:22,  4.50it/s]Epoch: 10, train for the 51-th batch, train loss: 0.6112668514251709:  34%|████        | 51/151 [00:10<00:22,  4.49it/s]Epoch: 5, train for the 90-th batch, train loss: 0.5491829514503479:  61%|███████▉     | 89/146 [00:52<00:34,  1.67it/s]Epoch: 5, train for the 90-th batch, train loss: 0.5491829514503479:  62%|████████     | 90/146 [00:52<00:33,  1.67it/s]Epoch: 3, train for the 223-th batch, train loss: 0.6352871656417847:  94%|██████████▎| 222/237 [02:13<00:08,  1.67it/s]Epoch: 3, train for the 223-th batch, train loss: 0.6352871656417847:  94%|██████████▎| 223/237 [02:13<00:08,  1.67it/s]Epoch: 10, train for the 52-th batch, train loss: 0.603740930557251:  34%|████▍        | 51/151 [00:11<00:22,  4.49it/s]Epoch: 10, train for the 52-th batch, train loss: 0.603740930557251:  34%|████▍        | 52/151 [00:11<00:21,  4.50it/s]Epoch: 2, train for the 314-th batch, train loss: 0.4183179438114166:  82%|████████▉  | 313/383 [03:09<00:41,  1.67it/s]Epoch: 2, train for the 314-th batch, train loss: 0.4183179438114166:  82%|█████████  | 314/383 [03:09<00:41,  1.67it/s]Epoch: 6, train for the 45-th batch, train loss: 0.16706159710884094:  37%|████▍       | 44/119 [00:27<00:44,  1.67it/s]Epoch: 6, train for the 45-th batch, train loss: 0.16706159710884094:  38%|████▌       | 45/119 [00:27<00:44,  1.68it/s]Epoch: 10, train for the 53-th batch, train loss: 0.6264375448226929:  34%|████▏       | 52/151 [00:11<00:21,  4.50it/s]Epoch: 10, train for the 53-th batch, train loss: 0.6264375448226929:  35%|████▏       | 53/151 [00:11<00:21,  4.50it/s]Epoch: 5, train for the 91-th batch, train loss: 0.5451739430427551:  62%|████████     | 90/146 [00:52<00:33,  1.67it/s]Epoch: 5, train for the 91-th batch, train loss: 0.5451739430427551:  62%|████████     | 91/146 [00:52<00:32,  1.67it/s]Epoch: 10, train for the 54-th batch, train loss: 0.5655224919319153:  35%|████▏       | 53/151 [00:11<00:21,  4.50it/s]Epoch: 10, train for the 54-th batch, train loss: 0.5655224919319153:  36%|████▎       | 54/151 [00:11<00:21,  4.50it/s]Epoch: 3, train for the 224-th batch, train loss: 0.6431146264076233:  94%|██████████▎| 223/237 [02:14<00:08,  1.67it/s]Epoch: 3, train for the 224-th batch, train loss: 0.6431146264076233:  95%|██████████▍| 224/237 [02:14<00:07,  1.67it/s]Epoch: 10, train for the 55-th batch, train loss: 0.5900472402572632:  36%|████▎       | 54/151 [00:11<00:21,  4.50it/s]Epoch: 10, train for the 55-th batch, train loss: 0.5900472402572632:  36%|████▎       | 55/151 [00:11<00:21,  4.50it/s]Epoch: 2, train for the 315-th batch, train loss: 0.4935447871685028:  82%|█████████  | 314/383 [03:10<00:41,  1.67it/s]Epoch: 2, train for the 315-th batch, train loss: 0.4935447871685028:  82%|█████████  | 315/383 [03:10<00:40,  1.66it/s]Epoch: 6, train for the 46-th batch, train loss: 0.2200792133808136:  38%|████▉        | 45/119 [00:27<00:44,  1.68it/s]Epoch: 6, train for the 46-th batch, train loss: 0.2200792133808136:  39%|█████        | 46/119 [00:27<00:43,  1.67it/s]Epoch: 10, train for the 56-th batch, train loss: 0.46974754333496094:  36%|████       | 55/151 [00:11<00:21,  4.50it/s]Epoch: 10, train for the 56-th batch, train loss: 0.46974754333496094:  37%|████       | 56/151 [00:11<00:21,  4.42it/s]Epoch: 5, train for the 92-th batch, train loss: 0.5699245929718018:  62%|████████     | 91/146 [00:53<00:32,  1.67it/s]Epoch: 5, train for the 92-th batch, train loss: 0.5699245929718018:  63%|████████▏    | 92/146 [00:53<00:32,  1.67it/s]Epoch: 10, train for the 57-th batch, train loss: 0.5386119484901428:  37%|████▍       | 56/151 [00:12<00:21,  4.42it/s]Epoch: 10, train for the 57-th batch, train loss: 0.5386119484901428:  38%|████▌       | 57/151 [00:12<00:21,  4.45it/s]Epoch: 3, train for the 225-th batch, train loss: 0.6362891793251038:  95%|██████████▍| 224/237 [02:15<00:07,  1.67it/s]Epoch: 3, train for the 225-th batch, train loss: 0.6362891793251038:  95%|██████████▍| 225/237 [02:15<00:07,  1.65it/s]Epoch: 2, train for the 316-th batch, train loss: 0.4168057441711426:  82%|█████████  | 315/383 [03:10<00:40,  1.66it/s]Epoch: 2, train for the 316-th batch, train loss: 0.4168057441711426:  83%|█████████  | 316/383 [03:10<00:40,  1.65it/s]Epoch: 6, train for the 47-th batch, train loss: 0.2588042914867401:  39%|█████        | 46/119 [00:28<00:43,  1.67it/s]Epoch: 6, train for the 47-th batch, train loss: 0.2588042914867401:  39%|█████▏       | 47/119 [00:28<00:43,  1.67it/s]Epoch: 10, train for the 58-th batch, train loss: 0.5476775765419006:  38%|████▌       | 57/151 [00:12<00:21,  4.45it/s]Epoch: 10, train for the 58-th batch, train loss: 0.5476775765419006:  38%|████▌       | 58/151 [00:12<00:20,  4.46it/s]Epoch: 10, train for the 59-th batch, train loss: 0.5078986883163452:  38%|████▌       | 58/151 [00:12<00:20,  4.46it/s]Epoch: 10, train for the 59-th batch, train loss: 0.5078986883163452:  39%|████▋       | 59/151 [00:12<00:20,  4.48it/s]Epoch: 5, train for the 93-th batch, train loss: 0.5425001382827759:  63%|████████▏    | 92/146 [00:54<00:32,  1.67it/s]Epoch: 5, train for the 93-th batch, train loss: 0.5425001382827759:  64%|████████▎    | 93/146 [00:54<00:31,  1.67it/s]Epoch: 10, train for the 60-th batch, train loss: 0.5193180441856384:  39%|████▋       | 59/151 [00:12<00:20,  4.48it/s]Epoch: 10, train for the 60-th batch, train loss: 0.5193180441856384:  40%|████▊       | 60/151 [00:12<00:20,  4.49it/s]Epoch: 6, train for the 48-th batch, train loss: 0.22596564888954163:  39%|████▋       | 47/119 [00:29<00:43,  1.67it/s]Epoch: 6, train for the 48-th batch, train loss: 0.22596564888954163:  40%|████▊       | 48/119 [00:29<00:42,  1.67it/s]Epoch: 10, train for the 61-th batch, train loss: 0.5382689237594604:  40%|████▊       | 60/151 [00:13<00:20,  4.49it/s]Epoch: 10, train for the 61-th batch, train loss: 0.5382689237594604:  40%|████▊       | 61/151 [00:13<00:20,  4.50it/s]Epoch: 10, train for the 62-th batch, train loss: 0.5442553758621216:  40%|████▊       | 61/151 [00:13<00:20,  4.50it/s]Epoch: 10, train for the 62-th batch, train loss: 0.5442553758621216:  41%|████▉       | 62/151 [00:13<00:19,  4.52it/s]Epoch: 5, train for the 94-th batch, train loss: 0.556575357913971:  64%|████████▉     | 93/146 [00:54<00:31,  1.67it/s]Epoch: 5, train for the 94-th batch, train loss: 0.556575357913971:  64%|█████████     | 94/146 [00:54<00:31,  1.67it/s]Epoch: 3, train for the 226-th batch, train loss: 0.6427241563796997:  95%|██████████▍| 225/237 [02:16<00:07,  1.65it/s]Epoch: 3, train for the 226-th batch, train loss: 0.6427241563796997:  95%|██████████▍| 226/237 [02:16<00:08,  1.27it/s]Epoch: 2, train for the 317-th batch, train loss: 0.42154043912887573:  83%|████████▎ | 316/383 [03:11<00:40,  1.65it/s]Epoch: 2, train for the 317-th batch, train loss: 0.42154043912887573:  83%|████████▎ | 317/383 [03:11<00:49,  1.34it/s]Epoch: 10, train for the 63-th batch, train loss: 0.5523548722267151:  41%|████▉       | 62/151 [00:13<00:19,  4.52it/s]Epoch: 10, train for the 63-th batch, train loss: 0.5523548722267151:  42%|█████       | 63/151 [00:13<00:19,  4.52it/s]Epoch: 6, train for the 49-th batch, train loss: 0.21630170941352844:  40%|████▊       | 48/119 [00:29<00:42,  1.67it/s]Epoch: 6, train for the 49-th batch, train loss: 0.21630170941352844:  41%|████▉       | 49/119 [00:29<00:41,  1.68it/s]Epoch: 10, train for the 64-th batch, train loss: 0.541307806968689:  42%|█████▍       | 63/151 [00:13<00:19,  4.52it/s]Epoch: 10, train for the 64-th batch, train loss: 0.541307806968689:  42%|█████▌       | 64/151 [00:13<00:19,  4.50it/s]Epoch: 5, train for the 95-th batch, train loss: 0.5445541143417358:  64%|████████▎    | 94/146 [00:55<00:31,  1.67it/s]Epoch: 5, train for the 95-th batch, train loss: 0.5445541143417358:  65%|████████▍    | 95/146 [00:55<00:30,  1.67it/s]Epoch: 10, train for the 65-th batch, train loss: 0.43845823407173157:  42%|████▋      | 64/151 [00:13<00:19,  4.50it/s]Epoch: 10, train for the 65-th batch, train loss: 0.43845823407173157:  43%|████▋      | 65/151 [00:13<00:18,  4.54it/s]Epoch: 3, train for the 227-th batch, train loss: 0.6577197313308716:  95%|██████████▍| 226/237 [02:16<00:08,  1.27it/s]Epoch: 3, train for the 227-th batch, train loss: 0.6577197313308716:  96%|██████████▌| 227/237 [02:16<00:07,  1.34it/s]Epoch: 2, train for the 318-th batch, train loss: 0.4189653694629669:  83%|█████████  | 317/383 [03:12<00:49,  1.34it/s]Epoch: 2, train for the 318-th batch, train loss: 0.4189653694629669:  83%|█████████▏ | 318/383 [03:12<00:46,  1.40it/s]Epoch: 10, train for the 66-th batch, train loss: 0.5720695853233337:  43%|█████▏      | 65/151 [00:14<00:18,  4.54it/s]Epoch: 10, train for the 66-th batch, train loss: 0.5720695853233337:  44%|█████▏      | 66/151 [00:14<00:18,  4.51it/s]Epoch: 6, train for the 50-th batch, train loss: 0.19944292306900024:  41%|████▉       | 49/119 [00:30<00:41,  1.68it/s]Epoch: 6, train for the 50-th batch, train loss: 0.19944292306900024:  42%|█████       | 50/119 [00:30<00:43,  1.59it/s]Epoch: 10, train for the 67-th batch, train loss: 0.5627407431602478:  44%|█████▏      | 66/151 [00:14<00:18,  4.51it/s]Epoch: 10, train for the 67-th batch, train loss: 0.5627407431602478:  44%|█████▎      | 67/151 [00:14<00:18,  4.51it/s]Epoch: 5, train for the 96-th batch, train loss: 0.5730656981468201:  65%|████████▍    | 95/146 [00:55<00:30,  1.67it/s]Epoch: 5, train for the 96-th batch, train loss: 0.5730656981468201:  66%|████████▌    | 96/146 [00:55<00:28,  1.75it/s]Epoch: 10, train for the 68-th batch, train loss: 0.5494480729103088:  44%|█████▎      | 67/151 [00:14<00:18,  4.51it/s]Epoch: 10, train for the 68-th batch, train loss: 0.5494480729103088:  45%|█████▍      | 68/151 [00:14<00:18,  4.49it/s]Epoch: 3, train for the 228-th batch, train loss: 0.6429154872894287:  96%|██████████▌| 227/237 [02:17<00:07,  1.34it/s]Epoch: 2, train for the 319-th batch, train loss: 0.4906054139137268:  83%|█████████▏ | 318/383 [03:13<00:46,  1.40it/s]Epoch: 3, train for the 228-th batch, train loss: 0.6429154872894287:  96%|██████████▌| 228/237 [02:17<00:06,  1.39it/s]Epoch: 2, train for the 319-th batch, train loss: 0.4906054139137268:  83%|█████████▏ | 319/383 [03:13<00:44,  1.43it/s]Epoch: 10, train for the 69-th batch, train loss: 0.5176858305931091:  45%|█████▍      | 68/151 [00:14<00:18,  4.49it/s]Epoch: 10, train for the 69-th batch, train loss: 0.5176858305931091:  46%|█████▍      | 69/151 [00:14<00:18,  4.52it/s]Epoch: 6, train for the 51-th batch, train loss: 0.21208655834197998:  42%|█████       | 50/119 [00:30<00:43,  1.59it/s]Epoch: 6, train for the 51-th batch, train loss: 0.21208655834197998:  43%|█████▏      | 51/119 [00:30<00:41,  1.62it/s]Epoch: 5, train for the 97-th batch, train loss: 0.5139645934104919:  66%|████████▌    | 96/146 [00:56<00:28,  1.75it/s]Epoch: 5, train for the 97-th batch, train loss: 0.5139645934104919:  66%|████████▋    | 97/146 [00:56<00:28,  1.73it/s]Epoch: 10, train for the 70-th batch, train loss: 0.461556077003479:  46%|█████▉       | 69/151 [00:15<00:18,  4.52it/s]Epoch: 10, train for the 70-th batch, train loss: 0.461556077003479:  46%|██████       | 70/151 [00:15<00:17,  4.57it/s]Epoch: 10, train for the 71-th batch, train loss: 0.5452515482902527:  46%|█████▌      | 70/151 [00:15<00:17,  4.57it/s]Epoch: 10, train for the 71-th batch, train loss: 0.5452515482902527:  47%|█████▋      | 71/151 [00:15<00:17,  4.55it/s]Epoch: 3, train for the 229-th batch, train loss: 0.672045111656189:  96%|███████████▌| 228/237 [02:18<00:06,  1.39it/s]Epoch: 2, train for the 320-th batch, train loss: 0.5531591773033142:  83%|█████████▏ | 319/383 [03:13<00:44,  1.43it/s]Epoch: 3, train for the 229-th batch, train loss: 0.672045111656189:  97%|███████████▌| 229/237 [02:18<00:05,  1.43it/s]Epoch: 2, train for the 320-th batch, train loss: 0.5531591773033142:  84%|█████████▏ | 320/383 [03:13<00:43,  1.46it/s]Epoch: 6, train for the 52-th batch, train loss: 0.2905241847038269:  43%|█████▌       | 51/119 [00:31<00:41,  1.62it/s]Epoch: 6, train for the 52-th batch, train loss: 0.2905241847038269:  44%|█████▋       | 52/119 [00:31<00:40,  1.64it/s]Epoch: 10, train for the 72-th batch, train loss: 0.5201128125190735:  47%|█████▋      | 71/151 [00:15<00:17,  4.55it/s]Epoch: 10, train for the 72-th batch, train loss: 0.5201128125190735:  48%|█████▋      | 72/151 [00:15<00:17,  4.53it/s]Epoch: 5, train for the 98-th batch, train loss: 0.5670085549354553:  66%|████████▋    | 97/146 [00:56<00:28,  1.73it/s]Epoch: 5, train for the 98-th batch, train loss: 0.5670085549354553:  67%|████████▋    | 98/146 [00:56<00:28,  1.71it/s]Epoch: 10, train for the 73-th batch, train loss: 0.5434970259666443:  48%|█████▋      | 72/151 [00:15<00:17,  4.53it/s]Epoch: 10, train for the 73-th batch, train loss: 0.5434970259666443:  48%|█████▊      | 73/151 [00:15<00:17,  4.53it/s]Epoch: 10, train for the 74-th batch, train loss: 0.5682975649833679:  48%|█████▊      | 73/151 [00:15<00:17,  4.53it/s]Epoch: 10, train for the 74-th batch, train loss: 0.5682975649833679:  49%|█████▉      | 74/151 [00:15<00:17,  4.53it/s]Epoch: 3, train for the 230-th batch, train loss: 0.6316553354263306:  97%|██████████▋| 229/237 [02:18<00:05,  1.43it/s]Epoch: 2, train for the 321-th batch, train loss: 0.6006602644920349:  84%|█████████▏ | 320/383 [03:14<00:43,  1.46it/s]Epoch: 3, train for the 230-th batch, train loss: 0.6316553354263306:  97%|██████████▋| 230/237 [02:18<00:04,  1.46it/s]Epoch: 2, train for the 321-th batch, train loss: 0.6006602644920349:  84%|█████████▏ | 321/383 [03:14<00:41,  1.48it/s]Epoch: 6, train for the 53-th batch, train loss: 0.17463000118732452:  44%|█████▏      | 52/119 [00:32<00:40,  1.64it/s]Epoch: 6, train for the 53-th batch, train loss: 0.17463000118732452:  45%|█████▎      | 53/119 [00:32<00:39,  1.66it/s]Epoch: 10, train for the 75-th batch, train loss: 0.5781406760215759:  49%|█████▉      | 74/151 [00:16<00:17,  4.53it/s]Epoch: 10, train for the 75-th batch, train loss: 0.5781406760215759:  50%|█████▉      | 75/151 [00:16<00:16,  4.52it/s]Epoch: 5, train for the 99-th batch, train loss: 0.5507782697677612:  67%|████████▋    | 98/146 [00:57<00:28,  1.71it/s]Epoch: 5, train for the 99-th batch, train loss: 0.5507782697677612:  68%|████████▊    | 99/146 [00:57<00:27,  1.69it/s]Epoch: 10, train for the 76-th batch, train loss: 0.5632343888282776:  50%|█████▉      | 75/151 [00:16<00:16,  4.52it/s]Epoch: 10, train for the 76-th batch, train loss: 0.5632343888282776:  50%|██████      | 76/151 [00:16<00:16,  4.53it/s]Epoch: 10, train for the 77-th batch, train loss: 0.5156652927398682:  50%|██████      | 76/151 [00:16<00:16,  4.53it/s]Epoch: 10, train for the 77-th batch, train loss: 0.5156652927398682:  51%|██████      | 77/151 [00:16<00:16,  4.51it/s]Epoch: 6, train for the 54-th batch, train loss: 0.16089242696762085:  45%|█████▎      | 53/119 [00:32<00:39,  1.66it/s]Epoch: 6, train for the 54-th batch, train loss: 0.16089242696762085:  45%|█████▍      | 54/119 [00:32<00:39,  1.66it/s]Epoch: 2, train for the 322-th batch, train loss: 0.5346019864082336:  84%|█████████▏ | 321/383 [03:15<00:41,  1.48it/s]Epoch: 3, train for the 231-th batch, train loss: 0.63352370262146:  97%|████████████▌| 230/237 [02:19<00:04,  1.46it/s]Epoch: 2, train for the 322-th batch, train loss: 0.5346019864082336:  84%|█████████▏ | 322/383 [03:15<00:40,  1.49it/s]Epoch: 3, train for the 231-th batch, train loss: 0.63352370262146:  97%|████████████▋| 231/237 [02:19<00:04,  1.47it/s]Epoch: 5, train for the 100-th batch, train loss: 0.5629540681838989:  68%|████████▏   | 99/146 [00:58<00:27,  1.69it/s]Epoch: 5, train for the 100-th batch, train loss: 0.5629540681838989:  68%|███████▌   | 100/146 [00:58<00:27,  1.68it/s]Epoch: 10, train for the 78-th batch, train loss: 0.4732770323753357:  51%|██████      | 77/151 [00:16<00:16,  4.51it/s]Epoch: 10, train for the 78-th batch, train loss: 0.4732770323753357:  52%|██████▏     | 78/151 [00:16<00:16,  4.55it/s]Epoch: 10, train for the 79-th batch, train loss: 0.5755196809768677:  52%|██████▏     | 78/151 [00:17<00:16,  4.55it/s]Epoch: 10, train for the 79-th batch, train loss: 0.5755196809768677:  52%|██████▎     | 79/151 [00:17<00:15,  4.51it/s]Epoch: 6, train for the 55-th batch, train loss: 0.17856162786483765:  45%|█████▍      | 54/119 [00:33<00:39,  1.66it/s]Epoch: 6, train for the 55-th batch, train loss: 0.17856162786483765:  46%|█████▌      | 55/119 [00:33<00:38,  1.67it/s]Epoch: 10, train for the 80-th batch, train loss: 0.543097734451294:  52%|██████▊      | 79/151 [00:17<00:15,  4.51it/s]Epoch: 10, train for the 80-th batch, train loss: 0.543097734451294:  53%|██████▉      | 80/151 [00:17<00:15,  4.52it/s]Epoch: 2, train for the 323-th batch, train loss: 0.5109024047851562:  84%|█████████▏ | 322/383 [03:15<00:40,  1.49it/s]Epoch: 2, train for the 323-th batch, train loss: 0.5109024047851562:  84%|█████████▎ | 323/383 [03:15<00:39,  1.50it/s]Epoch: 3, train for the 232-th batch, train loss: 0.6397269368171692:  97%|██████████▋| 231/237 [02:20<00:04,  1.47it/s]Epoch: 3, train for the 232-th batch, train loss: 0.6397269368171692:  98%|██████████▊| 232/237 [02:20<00:03,  1.49it/s]Epoch: 5, train for the 101-th batch, train loss: 0.531399667263031:  68%|████████▏   | 100/146 [00:58<00:27,  1.68it/s]Epoch: 5, train for the 101-th batch, train loss: 0.531399667263031:  69%|████████▎   | 101/146 [00:58<00:26,  1.67it/s]Epoch: 10, train for the 81-th batch, train loss: 0.5438930988311768:  53%|██████▎     | 80/151 [00:17<00:15,  4.52it/s]Epoch: 10, train for the 81-th batch, train loss: 0.5438930988311768:  54%|██████▍     | 81/151 [00:17<00:15,  4.53it/s]Epoch: 10, train for the 82-th batch, train loss: 0.5432809591293335:  54%|██████▍     | 81/151 [00:17<00:15,  4.53it/s]Epoch: 10, train for the 82-th batch, train loss: 0.5432809591293335:  54%|██████▌     | 82/151 [00:17<00:15,  4.52it/s]Epoch: 6, train for the 56-th batch, train loss: 0.21599265933036804:  46%|█████▌      | 55/119 [00:33<00:38,  1.67it/s]Epoch: 6, train for the 56-th batch, train loss: 0.21599265933036804:  47%|█████▋      | 56/119 [00:33<00:38,  1.66it/s]Epoch: 10, train for the 83-th batch, train loss: 0.5307080149650574:  54%|██████▌     | 82/151 [00:17<00:15,  4.52it/s]Epoch: 10, train for the 83-th batch, train loss: 0.5307080149650574:  55%|██████▌     | 83/151 [00:17<00:15,  4.38it/s]Epoch: 2, train for the 324-th batch, train loss: 0.4179750382900238:  84%|█████████▎ | 323/383 [03:16<00:39,  1.50it/s]Epoch: 2, train for the 324-th batch, train loss: 0.4179750382900238:  85%|█████████▎ | 324/383 [03:16<00:38,  1.52it/s]Epoch: 3, train for the 233-th batch, train loss: 0.6454664468765259:  98%|██████████▊| 232/237 [02:20<00:03,  1.49it/s]Epoch: 3, train for the 233-th batch, train loss: 0.6454664468765259:  98%|██████████▊| 233/237 [02:20<00:02,  1.51it/s]Epoch: 5, train for the 102-th batch, train loss: 0.5338681936264038:  69%|███████▌   | 101/146 [00:59<00:26,  1.67it/s]Epoch: 5, train for the 102-th batch, train loss: 0.5338681936264038:  70%|███████▋   | 102/146 [00:59<00:26,  1.66it/s]Epoch: 10, train for the 84-th batch, train loss: 0.586710512638092:  55%|███████▏     | 83/151 [00:18<00:15,  4.38it/s]Epoch: 10, train for the 84-th batch, train loss: 0.586710512638092:  56%|███████▏     | 84/151 [00:18<00:15,  4.42it/s]Epoch: 10, train for the 85-th batch, train loss: 0.5625277161598206:  56%|██████▋     | 84/151 [00:18<00:15,  4.42it/s]Epoch: 10, train for the 85-th batch, train loss: 0.5625277161598206:  56%|██████▊     | 85/151 [00:18<00:14,  4.46it/s]Epoch: 6, train for the 57-th batch, train loss: 0.15271300077438354:  47%|█████▋      | 56/119 [00:34<00:38,  1.66it/s]Epoch: 6, train for the 57-th batch, train loss: 0.15271300077438354:  48%|█████▋      | 57/119 [00:34<00:37,  1.65it/s]Epoch: 10, train for the 86-th batch, train loss: 0.5574314594268799:  56%|██████▊     | 85/151 [00:18<00:14,  4.46it/s]Epoch: 10, train for the 86-th batch, train loss: 0.5574314594268799:  57%|██████▊     | 86/151 [00:18<00:14,  4.47it/s]Epoch: 5, train for the 103-th batch, train loss: 0.5429748296737671:  70%|███████▋   | 102/146 [01:00<00:26,  1.66it/s]Epoch: 5, train for the 103-th batch, train loss: 0.5429748296737671:  71%|███████▊   | 103/146 [01:00<00:26,  1.65it/s]Epoch: 2, train for the 325-th batch, train loss: 0.4217168390750885:  85%|█████████▎ | 324/383 [03:17<00:38,  1.52it/s]Epoch: 2, train for the 325-th batch, train loss: 0.4217168390750885:  85%|█████████▎ | 325/383 [03:17<00:38,  1.52it/s]Epoch: 3, train for the 234-th batch, train loss: 0.6430650353431702:  98%|██████████▊| 233/237 [02:21<00:02,  1.51it/s]Epoch: 3, train for the 234-th batch, train loss: 0.6430650353431702:  99%|██████████▊| 234/237 [02:21<00:01,  1.52it/s]Epoch: 10, train for the 87-th batch, train loss: 0.5504701733589172:  57%|██████▊     | 86/151 [00:18<00:14,  4.47it/s]Epoch: 10, train for the 87-th batch, train loss: 0.5504701733589172:  58%|██████▉     | 87/151 [00:18<00:14,  4.49it/s]Epoch: 10, train for the 88-th batch, train loss: 0.5978884100914001:  58%|██████▉     | 87/151 [00:19<00:14,  4.49it/s]Epoch: 10, train for the 88-th batch, train loss: 0.5978884100914001:  58%|██████▉     | 88/151 [00:19<00:14,  4.50it/s]Epoch: 6, train for the 58-th batch, train loss: 0.1582712084054947:  48%|██████▏      | 57/119 [00:35<00:37,  1.65it/s]Epoch: 6, train for the 58-th batch, train loss: 0.1582712084054947:  49%|██████▎      | 58/119 [00:35<00:37,  1.64it/s]Epoch: 5, train for the 104-th batch, train loss: 0.5442790389060974:  71%|███████▊   | 103/146 [01:00<00:26,  1.65it/s]Epoch: 5, train for the 104-th batch, train loss: 0.5442790389060974:  71%|███████▊   | 104/146 [01:00<00:25,  1.64it/s]Epoch: 10, train for the 89-th batch, train loss: 0.5578311085700989:  58%|██████▉     | 88/151 [00:19<00:14,  4.50it/s]Epoch: 10, train for the 89-th batch, train loss: 0.5578311085700989:  59%|███████     | 89/151 [00:19<00:13,  4.50it/s]Epoch: 2, train for the 326-th batch, train loss: 0.4570213854312897:  85%|█████████▎ | 325/383 [03:17<00:38,  1.52it/s]Epoch: 2, train for the 326-th batch, train loss: 0.4570213854312897:  85%|█████████▎ | 326/383 [03:17<00:37,  1.53it/s]Epoch: 3, train for the 235-th batch, train loss: 0.6217081546783447:  99%|██████████▊| 234/237 [02:22<00:01,  1.52it/s]Epoch: 3, train for the 235-th batch, train loss: 0.6217081546783447:  99%|██████████▉| 235/237 [02:22<00:01,  1.53it/s]Epoch: 10, train for the 90-th batch, train loss: 0.562450647354126:  59%|███████▋     | 89/151 [00:19<00:13,  4.50it/s]Epoch: 10, train for the 90-th batch, train loss: 0.562450647354126:  60%|███████▋     | 90/151 [00:19<00:13,  4.49it/s]Epoch: 6, train for the 59-th batch, train loss: 0.16602440178394318:  49%|█████▊      | 58/119 [00:35<00:37,  1.64it/s]Epoch: 6, train for the 59-th batch, train loss: 0.16602440178394318:  50%|█████▉      | 59/119 [00:35<00:36,  1.64it/s]Epoch: 10, train for the 91-th batch, train loss: 0.507421612739563:  60%|███████▋     | 90/151 [00:19<00:13,  4.49it/s]Epoch: 10, train for the 91-th batch, train loss: 0.507421612739563:  60%|███████▊     | 91/151 [00:19<00:13,  4.39it/s]Epoch: 5, train for the 105-th batch, train loss: 0.5550804138183594:  71%|███████▊   | 104/146 [01:01<00:25,  1.64it/s]Epoch: 5, train for the 105-th batch, train loss: 0.5550804138183594:  72%|███████▉   | 105/146 [01:01<00:24,  1.64it/s]Epoch: 2, train for the 327-th batch, train loss: 0.5103839039802551:  85%|█████████▎ | 326/383 [03:18<00:37,  1.53it/s]Epoch: 2, train for the 327-th batch, train loss: 0.5103839039802551:  85%|█████████▍ | 327/383 [03:18<00:36,  1.54it/s]Epoch: 3, train for the 236-th batch, train loss: 0.6127132773399353:  99%|██████████▉| 235/237 [02:22<00:01,  1.53it/s]Epoch: 3, train for the 236-th batch, train loss: 0.6127132773399353: 100%|██████████▉| 236/237 [02:22<00:00,  1.54it/s]Epoch: 10, train for the 92-th batch, train loss: 0.5791190266609192:  60%|███████▏    | 91/151 [00:19<00:13,  4.39it/s]Epoch: 10, train for the 92-th batch, train loss: 0.5791190266609192:  61%|███████▎    | 92/151 [00:19<00:13,  4.43it/s]Epoch: 10, train for the 93-th batch, train loss: 0.5437617301940918:  61%|███████▎    | 92/151 [00:20<00:13,  4.43it/s]Epoch: 10, train for the 93-th batch, train loss: 0.5437617301940918:  62%|███████▍    | 93/151 [00:20<00:13,  4.46it/s]Epoch: 6, train for the 60-th batch, train loss: 0.14737722277641296:  50%|█████▉      | 59/119 [00:36<00:36,  1.64it/s]Epoch: 6, train for the 60-th batch, train loss: 0.14737722277641296:  50%|██████      | 60/119 [00:36<00:36,  1.64it/s]Epoch: 3, train for the 237-th batch, train loss: 0.6093150973320007: 100%|██████████▉| 236/237 [02:23<00:00,  1.54it/s]Epoch: 3, train for the 237-th batch, train loss: 0.6093150973320007: 100%|███████████| 237/237 [02:23<00:00,  1.71it/s]Epoch: 3, train for the 237-th batch, train loss: 0.6093150973320007: 100%|███████████| 237/237 [02:23<00:00,  1.65it/s]
  0%|                                                                                            | 0/66 [00:00<?, ?it/s]Epoch: 10, train for the 94-th batch, train loss: 0.5477098226547241:  62%|███████▍    | 93/151 [00:20<00:13,  4.46it/s]Epoch: 10, train for the 94-th batch, train loss: 0.5477098226547241:  62%|███████▍    | 94/151 [00:20<00:12,  4.46it/s]Epoch: 5, train for the 106-th batch, train loss: 0.5644242167472839:  72%|███████▉   | 105/146 [01:01<00:24,  1.64it/s]Epoch: 5, train for the 106-th batch, train loss: 0.5644242167472839:  73%|███████▉   | 106/146 [01:01<00:24,  1.64it/s]Epoch: 2, train for the 328-th batch, train loss: 0.4938078820705414:  85%|█████████▍ | 327/383 [03:18<00:36,  1.54it/s]Epoch: 2, train for the 328-th batch, train loss: 0.4938078820705414:  86%|█████████▍ | 328/383 [03:18<00:34,  1.59it/s]Epoch: 10, train for the 95-th batch, train loss: 0.49999499320983887:  62%|██████▊    | 94/151 [00:20<00:12,  4.46it/s]Epoch: 10, train for the 95-th batch, train loss: 0.49999499320983887:  63%|██████▉    | 95/151 [00:20<00:12,  4.48it/s]evaluate for the 1-th batch, evaluate loss: 0.5933167934417725:   0%|                            | 0/66 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.5933167934417725:   2%|▎                   | 1/66 [00:00<00:18,  3.49it/s]Epoch: 10, train for the 96-th batch, train loss: 0.5293764472007751:  63%|███████▌    | 95/151 [00:20<00:12,  4.48it/s]Epoch: 10, train for the 96-th batch, train loss: 0.5293764472007751:  64%|███████▋    | 96/151 [00:20<00:12,  4.49it/s]evaluate for the 2-th batch, evaluate loss: 0.5884469747543335:   2%|▎                   | 1/66 [00:00<00:18,  3.49it/s]evaluate for the 2-th batch, evaluate loss: 0.5884469747543335:   3%|▌                   | 2/66 [00:00<00:17,  3.61it/s]Epoch: 6, train for the 61-th batch, train loss: 0.1327730119228363:  50%|██████▌      | 60/119 [00:36<00:36,  1.64it/s]Epoch: 6, train for the 61-th batch, train loss: 0.1327730119228363:  51%|██████▋      | 61/119 [00:36<00:35,  1.63it/s]Epoch: 10, train for the 97-th batch, train loss: 0.5816112160682678:  64%|███████▋    | 96/151 [00:21<00:12,  4.49it/s]Epoch: 10, train for the 97-th batch, train loss: 0.5816112160682678:  64%|███████▋    | 97/151 [00:21<00:12,  4.49it/s]Epoch: 5, train for the 107-th batch, train loss: 0.5364115238189697:  73%|███████▉   | 106/146 [01:02<00:24,  1.64it/s]Epoch: 5, train for the 107-th batch, train loss: 0.5364115238189697:  73%|████████   | 107/146 [01:02<00:23,  1.63it/s]Epoch: 2, train for the 329-th batch, train loss: 0.46410855650901794:  86%|████████▌ | 328/383 [03:19<00:34,  1.59it/s]Epoch: 2, train for the 329-th batch, train loss: 0.46410855650901794:  86%|████████▌ | 329/383 [03:19<00:33,  1.62it/s]evaluate for the 3-th batch, evaluate loss: 0.6110725402832031:   3%|▌                   | 2/66 [00:00<00:17,  3.61it/s]evaluate for the 3-th batch, evaluate loss: 0.6110725402832031:   5%|▉                   | 3/66 [00:00<00:17,  3.52it/s]Epoch: 10, train for the 98-th batch, train loss: 0.6183501482009888:  64%|███████▋    | 97/151 [00:21<00:12,  4.49it/s]Epoch: 10, train for the 98-th batch, train loss: 0.6183501482009888:  65%|███████▊    | 98/151 [00:21<00:11,  4.51it/s]evaluate for the 4-th batch, evaluate loss: 0.6064578294754028:   5%|▉                   | 3/66 [00:01<00:17,  3.52it/s]evaluate for the 4-th batch, evaluate loss: 0.6064578294754028:   6%|█▏                  | 4/66 [00:01<00:16,  3.76it/s]Epoch: 10, train for the 99-th batch, train loss: 0.6668263077735901:  65%|███████▊    | 98/151 [00:21<00:11,  4.51it/s]Epoch: 10, train for the 99-th batch, train loss: 0.6668263077735901:  66%|███████▊    | 99/151 [00:21<00:11,  4.51it/s]Epoch: 6, train for the 62-th batch, train loss: 0.26491448283195496:  51%|██████▏     | 61/119 [00:37<00:35,  1.63it/s]Epoch: 6, train for the 62-th batch, train loss: 0.26491448283195496:  52%|██████▎     | 62/119 [00:37<00:34,  1.63it/s]Epoch: 2, train for the 330-th batch, train loss: 0.4469102621078491:  86%|█████████▍ | 329/383 [03:20<00:33,  1.62it/s]Epoch: 5, train for the 108-th batch, train loss: 0.5531759858131409:  73%|████████   | 107/146 [01:03<00:23,  1.63it/s]Epoch: 2, train for the 330-th batch, train loss: 0.4469102621078491:  86%|█████████▍ | 330/383 [03:20<00:32,  1.63it/s]Epoch: 5, train for the 108-th batch, train loss: 0.5531759858131409:  74%|████████▏  | 108/146 [01:03<00:23,  1.63it/s]Epoch: 10, train for the 100-th batch, train loss: 0.6987887620925903:  66%|███████▏   | 99/151 [00:21<00:11,  4.51it/s]Epoch: 10, train for the 100-th batch, train loss: 0.6987887620925903:  66%|██████▌   | 100/151 [00:21<00:11,  4.51it/s]evaluate for the 5-th batch, evaluate loss: 0.6254639029502869:   6%|█▏                  | 4/66 [00:01<00:16,  3.76it/s]evaluate for the 5-th batch, evaluate loss: 0.6254639029502869:   8%|█▌                  | 5/66 [00:01<00:16,  3.60it/s]Epoch: 10, train for the 101-th batch, train loss: 0.6603135466575623:  66%|██████▌   | 100/151 [00:21<00:11,  4.51it/s]Epoch: 10, train for the 101-th batch, train loss: 0.6603135466575623:  67%|██████▋   | 101/151 [00:21<00:11,  4.52it/s]evaluate for the 6-th batch, evaluate loss: 0.641062319278717:   8%|█▌                   | 5/66 [00:01<00:16,  3.60it/s]evaluate for the 6-th batch, evaluate loss: 0.641062319278717:   9%|█▉                   | 6/66 [00:01<00:15,  3.76it/s]Epoch: 6, train for the 63-th batch, train loss: 0.1544133722782135:  52%|██████▊      | 62/119 [00:38<00:34,  1.63it/s]Epoch: 6, train for the 63-th batch, train loss: 0.1544133722782135:  53%|██████▉      | 63/119 [00:38<00:34,  1.63it/s]Epoch: 10, train for the 102-th batch, train loss: 0.6254921555519104:  67%|██████▋   | 101/151 [00:22<00:11,  4.52it/s]Epoch: 10, train for the 102-th batch, train loss: 0.6254921555519104:  68%|██████▊   | 102/151 [00:22<00:10,  4.52it/s]Epoch: 2, train for the 331-th batch, train loss: 0.4797077775001526:  86%|█████████▍ | 330/383 [03:20<00:32,  1.63it/s]Epoch: 2, train for the 331-th batch, train loss: 0.4797077775001526:  86%|█████████▌ | 331/383 [03:20<00:31,  1.64it/s]evaluate for the 7-th batch, evaluate loss: 0.6184831857681274:   9%|█▊                  | 6/66 [00:01<00:15,  3.76it/s]evaluate for the 7-th batch, evaluate loss: 0.6184831857681274:  11%|██                  | 7/66 [00:01<00:16,  3.55it/s]Epoch: 5, train for the 109-th batch, train loss: 0.5170154571533203:  74%|████████▏  | 108/146 [01:03<00:23,  1.63it/s]Epoch: 5, train for the 109-th batch, train loss: 0.5170154571533203:  75%|████████▏  | 109/146 [01:03<00:22,  1.62it/s]Epoch: 10, train for the 103-th batch, train loss: 0.6499806642532349:  68%|██████▊   | 102/151 [00:22<00:10,  4.52it/s]Epoch: 10, train for the 103-th batch, train loss: 0.6499806642532349:  68%|██████▊   | 103/151 [00:22<00:10,  4.52it/s]evaluate for the 8-th batch, evaluate loss: 0.615958571434021:  11%|██▏                  | 7/66 [00:02<00:16,  3.55it/s]evaluate for the 8-th batch, evaluate loss: 0.615958571434021:  12%|██▌                  | 8/66 [00:02<00:16,  3.61it/s]Epoch: 10, train for the 104-th batch, train loss: 0.6333445906639099:  68%|██████▊   | 103/151 [00:22<00:10,  4.52it/s]Epoch: 10, train for the 104-th batch, train loss: 0.6333445906639099:  69%|██████▉   | 104/151 [00:22<00:10,  4.52it/s]Epoch: 6, train for the 64-th batch, train loss: 0.13617563247680664:  53%|██████▎     | 63/119 [00:38<00:34,  1.63it/s]Epoch: 6, train for the 64-th batch, train loss: 0.13617563247680664:  54%|██████▍     | 64/119 [00:38<00:33,  1.62it/s]Epoch: 10, train for the 105-th batch, train loss: 0.554469108581543:  69%|███████▌   | 104/151 [00:22<00:10,  4.52it/s]Epoch: 10, train for the 105-th batch, train loss: 0.554469108581543:  70%|███████▋   | 105/151 [00:22<00:10,  4.52it/s]evaluate for the 9-th batch, evaluate loss: 0.5715649724006653:  12%|██▍                 | 8/66 [00:02<00:16,  3.61it/s]evaluate for the 9-th batch, evaluate loss: 0.5715649724006653:  14%|██▋                 | 9/66 [00:02<00:16,  3.55it/s]Epoch: 2, train for the 332-th batch, train loss: 0.4935712218284607:  86%|█████████▌ | 331/383 [03:21<00:31,  1.64it/s]Epoch: 2, train for the 332-th batch, train loss: 0.4935712218284607:  87%|█████████▌ | 332/383 [03:21<00:30,  1.66it/s]Epoch: 5, train for the 110-th batch, train loss: 0.5067657232284546:  75%|████████▏  | 109/146 [01:04<00:22,  1.62it/s]Epoch: 5, train for the 110-th batch, train loss: 0.5067657232284546:  75%|████████▎  | 110/146 [01:04<00:22,  1.62it/s]Epoch: 10, train for the 106-th batch, train loss: 0.5571024417877197:  70%|██████▉   | 105/151 [00:23<00:10,  4.52it/s]Epoch: 10, train for the 106-th batch, train loss: 0.5571024417877197:  70%|███████   | 106/151 [00:23<00:10,  4.49it/s]evaluate for the 10-th batch, evaluate loss: 0.5861400365829468:  14%|██▌                | 9/66 [00:02<00:16,  3.55it/s]evaluate for the 10-th batch, evaluate loss: 0.5861400365829468:  15%|██▋               | 10/66 [00:02<00:15,  3.68it/s]Epoch: 10, train for the 107-th batch, train loss: 0.5526403188705444:  70%|███████   | 106/151 [00:23<00:10,  4.49it/s]Epoch: 10, train for the 107-th batch, train loss: 0.5526403188705444:  71%|███████   | 107/151 [00:23<00:10,  4.39it/s]evaluate for the 11-th batch, evaluate loss: 0.5839862823486328:  15%|██▋               | 10/66 [00:03<00:15,  3.68it/s]evaluate for the 11-th batch, evaluate loss: 0.5839862823486328:  17%|███               | 11/66 [00:03<00:15,  3.62it/s]Epoch: 6, train for the 65-th batch, train loss: 0.17737771570682526:  54%|██████▍     | 64/119 [00:39<00:33,  1.62it/s]Epoch: 6, train for the 65-th batch, train loss: 0.17737771570682526:  55%|██████▌     | 65/119 [00:39<00:33,  1.61it/s]Epoch: 2, train for the 333-th batch, train loss: 0.4843675196170807:  87%|█████████▌ | 332/383 [03:21<00:30,  1.66it/s]Epoch: 2, train for the 333-th batch, train loss: 0.4843675196170807:  87%|█████████▌ | 333/383 [03:21<00:30,  1.66it/s]Epoch: 10, train for the 108-th batch, train loss: 0.5537849068641663:  71%|███████   | 107/151 [00:23<00:10,  4.39it/s]Epoch: 10, train for the 108-th batch, train loss: 0.5537849068641663:  72%|███████▏  | 108/151 [00:23<00:09,  4.42it/s]Epoch: 5, train for the 111-th batch, train loss: 0.5102580785751343:  75%|████████▎  | 110/146 [01:04<00:22,  1.62it/s]Epoch: 5, train for the 111-th batch, train loss: 0.5102580785751343:  76%|████████▎  | 111/146 [01:04<00:21,  1.61it/s]evaluate for the 12-th batch, evaluate loss: 0.6160162091255188:  17%|███               | 11/66 [00:03<00:15,  3.62it/s]evaluate for the 12-th batch, evaluate loss: 0.6160162091255188:  18%|███▎              | 12/66 [00:03<00:14,  3.71it/s]Epoch: 10, train for the 109-th batch, train loss: 0.5372228026390076:  72%|███████▏  | 108/151 [00:23<00:09,  4.42it/s]Epoch: 10, train for the 109-th batch, train loss: 0.5372228026390076:  72%|███████▏  | 109/151 [00:23<00:10,  4.00it/s]evaluate for the 13-th batch, evaluate loss: 0.5955967307090759:  18%|███▎              | 12/66 [00:03<00:14,  3.71it/s]evaluate for the 13-th batch, evaluate loss: 0.5955967307090759:  20%|███▌              | 13/66 [00:03<00:14,  3.65it/s]Epoch: 6, train for the 66-th batch, train loss: 0.2198864370584488:  55%|███████      | 65/119 [00:40<00:33,  1.61it/s]Epoch: 6, train for the 66-th batch, train loss: 0.2198864370584488:  55%|███████▏     | 66/119 [00:40<00:32,  1.61it/s]Epoch: 10, train for the 110-th batch, train loss: 0.5666739344596863:  72%|███████▏  | 109/151 [00:24<00:10,  4.00it/s]Epoch: 10, train for the 110-th batch, train loss: 0.5666739344596863:  73%|███████▎  | 110/151 [00:24<00:09,  4.14it/s]Epoch: 2, train for the 334-th batch, train loss: 0.4883134365081787:  87%|█████████▌ | 333/383 [03:22<00:30,  1.66it/s]Epoch: 2, train for the 334-th batch, train loss: 0.4883134365081787:  87%|█████████▌ | 334/383 [03:22<00:29,  1.66it/s]Epoch: 5, train for the 112-th batch, train loss: 0.5411576628684998:  76%|████████▎  | 111/146 [01:05<00:21,  1.61it/s]Epoch: 5, train for the 112-th batch, train loss: 0.5411576628684998:  77%|████████▍  | 112/146 [01:05<00:21,  1.61it/s]evaluate for the 14-th batch, evaluate loss: 0.5969994068145752:  20%|███▌              | 13/66 [00:03<00:14,  3.65it/s]evaluate for the 14-th batch, evaluate loss: 0.5969994068145752:  21%|███▊              | 14/66 [00:03<00:14,  3.58it/s]Epoch: 10, train for the 111-th batch, train loss: 0.5457696318626404:  73%|███████▎  | 110/151 [00:24<00:09,  4.14it/s]Epoch: 10, train for the 111-th batch, train loss: 0.5457696318626404:  74%|███████▎  | 111/151 [00:24<00:09,  4.17it/s]Epoch: 10, train for the 112-th batch, train loss: 0.46139800548553467:  74%|██████▌  | 111/151 [00:24<00:09,  4.17it/s]Epoch: 10, train for the 112-th batch, train loss: 0.46139800548553467:  74%|██████▋  | 112/151 [00:24<00:09,  4.30it/s]evaluate for the 15-th batch, evaluate loss: 0.6287974119186401:  21%|███▊              | 14/66 [00:04<00:14,  3.58it/s]evaluate for the 15-th batch, evaluate loss: 0.6287974119186401:  23%|████              | 15/66 [00:04<00:14,  3.62it/s]Epoch: 6, train for the 67-th batch, train loss: 0.24651923775672913:  55%|██████▋     | 66/119 [00:40<00:32,  1.61it/s]Epoch: 6, train for the 67-th batch, train loss: 0.24651923775672913:  56%|██████▊     | 67/119 [00:40<00:32,  1.61it/s]Epoch: 2, train for the 335-th batch, train loss: 0.5329893231391907:  87%|█████████▌ | 334/383 [03:23<00:29,  1.66it/s]Epoch: 2, train for the 335-th batch, train loss: 0.5329893231391907:  87%|█████████▌ | 335/383 [03:23<00:28,  1.67it/s]Epoch: 10, train for the 113-th batch, train loss: 0.5778500437736511:  74%|███████▍  | 112/151 [00:24<00:09,  4.30it/s]Epoch: 10, train for the 113-th batch, train loss: 0.5778500437736511:  75%|███████▍  | 113/151 [00:24<00:08,  4.32it/s]evaluate for the 16-th batch, evaluate loss: 0.5618499517440796:  23%|████              | 15/66 [00:04<00:14,  3.62it/s]evaluate for the 16-th batch, evaluate loss: 0.5618499517440796:  24%|████▎             | 16/66 [00:04<00:14,  3.54it/s]Epoch: 5, train for the 113-th batch, train loss: 0.4896526634693146:  77%|████████▍  | 112/146 [01:06<00:21,  1.61it/s]Epoch: 5, train for the 113-th batch, train loss: 0.4896526634693146:  77%|████████▌  | 113/146 [01:06<00:20,  1.61it/s]Epoch: 10, train for the 114-th batch, train loss: 0.5191757678985596:  75%|███████▍  | 113/151 [00:24<00:08,  4.32it/s]Epoch: 10, train for the 114-th batch, train loss: 0.5191757678985596:  75%|███████▌  | 114/151 [00:24<00:08,  4.35it/s]evaluate for the 17-th batch, evaluate loss: 0.6035099625587463:  24%|████▎             | 16/66 [00:04<00:14,  3.54it/s]evaluate for the 17-th batch, evaluate loss: 0.6035099625587463:  26%|████▋             | 17/66 [00:04<00:13,  3.72it/s]Epoch: 10, train for the 115-th batch, train loss: 0.53225177526474:  75%|█████████   | 114/151 [00:25<00:08,  4.35it/s]Epoch: 10, train for the 115-th batch, train loss: 0.53225177526474:  76%|█████████▏  | 115/151 [00:25<00:08,  4.33it/s]Epoch: 6, train for the 68-th batch, train loss: 0.13063684105873108:  56%|██████▊     | 67/119 [00:41<00:32,  1.61it/s]Epoch: 6, train for the 68-th batch, train loss: 0.13063684105873108:  57%|██████▊     | 68/119 [00:41<00:31,  1.62it/s]Epoch: 2, train for the 336-th batch, train loss: 0.4905930161476135:  87%|█████████▌ | 335/383 [03:23<00:28,  1.67it/s]Epoch: 2, train for the 336-th batch, train loss: 0.4905930161476135:  88%|█████████▋ | 336/383 [03:23<00:28,  1.67it/s]evaluate for the 18-th batch, evaluate loss: 0.6114057898521423:  26%|████▋             | 17/66 [00:04<00:13,  3.72it/s]evaluate for the 18-th batch, evaluate loss: 0.6114057898521423:  27%|████▉             | 18/66 [00:04<00:13,  3.59it/s]Epoch: 10, train for the 116-th batch, train loss: 0.4921237826347351:  76%|███████▌  | 115/151 [00:25<00:08,  4.33it/s]Epoch: 10, train for the 116-th batch, train loss: 0.4921237826347351:  77%|███████▋  | 116/151 [00:25<00:08,  4.35it/s]Epoch: 5, train for the 114-th batch, train loss: 0.5777998566627502:  77%|████████▌  | 113/146 [01:06<00:20,  1.61it/s]Epoch: 5, train for the 114-th batch, train loss: 0.5777998566627502:  78%|████████▌  | 114/146 [01:06<00:19,  1.61it/s]evaluate for the 19-th batch, evaluate loss: 0.6059784293174744:  27%|████▉             | 18/66 [00:05<00:13,  3.59it/s]evaluate for the 19-th batch, evaluate loss: 0.6059784293174744:  29%|█████▏            | 19/66 [00:05<00:12,  3.73it/s]Epoch: 10, train for the 117-th batch, train loss: 0.5267869234085083:  77%|███████▋  | 116/151 [00:25<00:08,  4.35it/s]Epoch: 10, train for the 117-th batch, train loss: 0.5267869234085083:  77%|███████▋  | 117/151 [00:25<00:07,  4.35it/s]Epoch: 10, train for the 118-th batch, train loss: 0.4944392740726471:  77%|███████▋  | 117/151 [00:25<00:07,  4.35it/s]Epoch: 10, train for the 118-th batch, train loss: 0.4944392740726471:  78%|███████▊  | 118/151 [00:25<00:07,  4.36it/s]Epoch: 2, train for the 337-th batch, train loss: 0.5429372787475586:  88%|█████████▋ | 336/383 [03:24<00:28,  1.67it/s]Epoch: 2, train for the 337-th batch, train loss: 0.5429372787475586:  88%|█████████▋ | 337/383 [03:24<00:27,  1.66it/s]Epoch: 6, train for the 69-th batch, train loss: 0.15672174096107483:  57%|██████▊     | 68/119 [00:41<00:31,  1.62it/s]Epoch: 6, train for the 69-th batch, train loss: 0.15672174096107483:  58%|██████▉     | 69/119 [00:41<00:30,  1.61it/s]evaluate for the 20-th batch, evaluate loss: 0.6220618486404419:  29%|█████▏            | 19/66 [00:05<00:12,  3.73it/s]evaluate for the 20-th batch, evaluate loss: 0.6220618486404419:  30%|█████▍            | 20/66 [00:05<00:12,  3.55it/s]Epoch: 5, train for the 115-th batch, train loss: 0.5379958152770996:  78%|████████▌  | 114/146 [01:07<00:19,  1.61it/s]Epoch: 5, train for the 115-th batch, train loss: 0.5379958152770996:  79%|████████▋  | 115/146 [01:07<00:19,  1.61it/s]Epoch: 10, train for the 119-th batch, train loss: 0.5063450336456299:  78%|███████▊  | 118/151 [00:26<00:07,  4.36it/s]Epoch: 10, train for the 119-th batch, train loss: 0.5063450336456299:  79%|███████▉  | 119/151 [00:26<00:07,  4.34it/s]evaluate for the 21-th batch, evaluate loss: 0.6304594874382019:  30%|█████▍            | 20/66 [00:05<00:12,  3.55it/s]evaluate for the 21-th batch, evaluate loss: 0.6304594874382019:  32%|█████▋            | 21/66 [00:05<00:12,  3.60it/s]Epoch: 10, train for the 120-th batch, train loss: 0.5905230641365051:  79%|███████▉  | 119/151 [00:26<00:07,  4.34it/s]Epoch: 10, train for the 120-th batch, train loss: 0.5905230641365051:  79%|███████▉  | 120/151 [00:26<00:07,  3.93it/s]Epoch: 6, train for the 70-th batch, train loss: 0.16540181636810303:  58%|██████▉     | 69/119 [00:42<00:30,  1.61it/s]Epoch: 6, train for the 70-th batch, train loss: 0.16540181636810303:  59%|███████     | 70/119 [00:42<00:29,  1.69it/s]evaluate for the 22-th batch, evaluate loss: 0.6177196502685547:  32%|█████▋            | 21/66 [00:06<00:12,  3.60it/s]evaluate for the 22-th batch, evaluate loss: 0.6177196502685547:  33%|██████            | 22/66 [00:06<00:12,  3.54it/s]Epoch: 2, train for the 338-th batch, train loss: 0.46298763155937195:  88%|████████▊ | 337/383 [03:24<00:27,  1.66it/s]Epoch: 2, train for the 338-th batch, train loss: 0.46298763155937195:  88%|████████▊ | 338/383 [03:24<00:26,  1.68it/s]Epoch: 10, train for the 121-th batch, train loss: 0.49805715680122375:  79%|███████▏ | 120/151 [00:26<00:07,  3.93it/s]Epoch: 10, train for the 121-th batch, train loss: 0.49805715680122375:  80%|███████▏ | 121/151 [00:26<00:07,  3.97it/s]evaluate for the 23-th batch, evaluate loss: 0.6359213590621948:  33%|██████            | 22/66 [00:06<00:12,  3.54it/s]evaluate for the 23-th batch, evaluate loss: 0.6359213590621948:  35%|██████▎           | 23/66 [00:06<00:11,  3.66it/s]Epoch: 6, train for the 71-th batch, train loss: 0.1903776079416275:  59%|███████▋     | 70/119 [00:42<00:29,  1.69it/s]Epoch: 6, train for the 71-th batch, train loss: 0.1903776079416275:  60%|███████▊     | 71/119 [00:42<00:25,  1.91it/s]Epoch: 10, train for the 122-th batch, train loss: 0.5501924157142639:  80%|████████  | 121/151 [00:26<00:07,  3.97it/s]Epoch: 10, train for the 122-th batch, train loss: 0.5501924157142639:  81%|████████  | 122/151 [00:26<00:07,  4.13it/s]Epoch: 5, train for the 116-th batch, train loss: 0.5174726247787476:  79%|████████▋  | 115/146 [01:08<00:19,  1.61it/s]Epoch: 5, train for the 116-th batch, train loss: 0.5174726247787476:  79%|████████▋  | 116/146 [01:08<00:21,  1.42it/s]evaluate for the 24-th batch, evaluate loss: 0.6271624565124512:  35%|██████▎           | 23/66 [00:06<00:11,  3.66it/s]evaluate for the 24-th batch, evaluate loss: 0.6271624565124512:  36%|██████▌           | 24/66 [00:06<00:11,  3.61it/s]Epoch: 2, train for the 339-th batch, train loss: 0.5178308486938477:  88%|█████████▋ | 338/383 [03:25<00:26,  1.68it/s]Epoch: 2, train for the 339-th batch, train loss: 0.5178308486938477:  89%|█████████▋ | 339/383 [03:25<00:26,  1.68it/s]Epoch: 10, train for the 123-th batch, train loss: 0.5378121137619019:  81%|████████  | 122/151 [00:27<00:07,  4.13it/s]Epoch: 10, train for the 123-th batch, train loss: 0.5378121137619019:  81%|████████▏ | 123/151 [00:27<00:06,  4.16it/s]Epoch: 6, train for the 72-th batch, train loss: 0.20834949612617493:  60%|███████▏    | 71/119 [00:43<00:25,  1.91it/s]Epoch: 6, train for the 72-th batch, train loss: 0.20834949612617493:  61%|███████▎    | 72/119 [00:43<00:22,  2.07it/s]evaluate for the 25-th batch, evaluate loss: 0.6488401889801025:  36%|██████▌           | 24/66 [00:06<00:11,  3.61it/s]evaluate for the 25-th batch, evaluate loss: 0.6488401889801025:  38%|██████▊           | 25/66 [00:06<00:11,  3.69it/s]Epoch: 10, train for the 124-th batch, train loss: 0.5395250916481018:  81%|████████▏ | 123/151 [00:27<00:06,  4.16it/s]Epoch: 10, train for the 124-th batch, train loss: 0.5395250916481018:  82%|████████▏ | 124/151 [00:27<00:06,  4.26it/s]evaluate for the 26-th batch, evaluate loss: 0.6128216981887817:  38%|██████▊           | 25/66 [00:07<00:11,  3.69it/s]evaluate for the 26-th batch, evaluate loss: 0.6128216981887817:  39%|███████           | 26/66 [00:07<00:10,  3.66it/s]Epoch: 5, train for the 117-th batch, train loss: 0.594618558883667:  79%|█████████▌  | 116/146 [01:08<00:21,  1.42it/s]Epoch: 5, train for the 117-th batch, train loss: 0.594618558883667:  80%|█████████▌  | 117/146 [01:08<00:19,  1.49it/s]Epoch: 10, train for the 125-th batch, train loss: 0.5467398762702942:  82%|████████▏ | 124/151 [00:27<00:06,  4.26it/s]Epoch: 10, train for the 125-th batch, train loss: 0.5467398762702942:  83%|████████▎ | 125/151 [00:27<00:06,  4.23it/s]Epoch: 2, train for the 340-th batch, train loss: 0.521277129650116:  89%|██████████▌ | 339/383 [03:26<00:26,  1.68it/s]Epoch: 2, train for the 340-th batch, train loss: 0.521277129650116:  89%|██████████▋ | 340/383 [03:26<00:25,  1.67it/s]Epoch: 6, train for the 73-th batch, train loss: 0.17053504288196564:  61%|███████▎    | 72/119 [00:43<00:22,  2.07it/s]Epoch: 6, train for the 73-th batch, train loss: 0.17053504288196564:  61%|███████▎    | 73/119 [00:43<00:23,  1.96it/s]Epoch: 10, train for the 126-th batch, train loss: 0.5158755779266357:  83%|████████▎ | 125/151 [00:27<00:06,  4.23it/s]Epoch: 10, train for the 126-th batch, train loss: 0.5158755779266357:  83%|████████▎ | 126/151 [00:27<00:05,  4.32it/s]evaluate for the 27-th batch, evaluate loss: 0.621477484703064:  39%|███████▍           | 26/66 [00:07<00:10,  3.66it/s]evaluate for the 27-th batch, evaluate loss: 0.621477484703064:  41%|███████▊           | 27/66 [00:07<00:10,  3.60it/s]Epoch: 10, train for the 127-th batch, train loss: 0.5747849345207214:  83%|████████▎ | 126/151 [00:28<00:05,  4.32it/s]Epoch: 10, train for the 127-th batch, train loss: 0.5747849345207214:  84%|████████▍ | 127/151 [00:28<00:05,  4.32it/s]evaluate for the 28-th batch, evaluate loss: 0.6259539723396301:  41%|███████▎          | 27/66 [00:07<00:10,  3.60it/s]evaluate for the 28-th batch, evaluate loss: 0.6259539723396301:  42%|███████▋          | 28/66 [00:07<00:10,  3.63it/s]Epoch: 5, train for the 118-th batch, train loss: 0.4755666255950928:  80%|████████▊  | 117/146 [01:09<00:19,  1.49it/s]Epoch: 5, train for the 118-th batch, train loss: 0.4755666255950928:  81%|████████▉  | 118/146 [01:09<00:18,  1.52it/s]Epoch: 10, train for the 128-th batch, train loss: 0.5970908403396606:  84%|████████▍ | 127/151 [00:28<00:05,  4.32it/s]Epoch: 10, train for the 128-th batch, train loss: 0.5970908403396606:  85%|████████▍ | 128/151 [00:28<00:05,  4.38it/s]Epoch: 2, train for the 341-th batch, train loss: 0.4475243389606476:  89%|█████████▊ | 340/383 [03:26<00:25,  1.67it/s]Epoch: 2, train for the 341-th batch, train loss: 0.4475243389606476:  89%|█████████▊ | 341/383 [03:26<00:25,  1.67it/s]Epoch: 6, train for the 74-th batch, train loss: 0.2159218043088913:  61%|███████▉     | 73/119 [00:44<00:23,  1.96it/s]Epoch: 6, train for the 74-th batch, train loss: 0.2159218043088913:  62%|████████     | 74/119 [00:44<00:24,  1.87it/s]evaluate for the 29-th batch, evaluate loss: 0.6180944442749023:  42%|███████▋          | 28/66 [00:08<00:10,  3.63it/s]evaluate for the 29-th batch, evaluate loss: 0.6180944442749023:  44%|███████▉          | 29/66 [00:08<00:10,  3.55it/s]Epoch: 10, train for the 129-th batch, train loss: 0.5588024258613586:  85%|████████▍ | 128/151 [00:28<00:05,  4.38it/s]Epoch: 10, train for the 129-th batch, train loss: 0.5588024258613586:  85%|████████▌ | 129/151 [00:28<00:05,  4.04it/s]evaluate for the 30-th batch, evaluate loss: 0.6244007349014282:  44%|███████▉          | 29/66 [00:08<00:10,  3.55it/s]evaluate for the 30-th batch, evaluate loss: 0.6244007349014282:  45%|████████▏         | 30/66 [00:08<00:09,  3.69it/s]Epoch: 10, train for the 130-th batch, train loss: 0.5486144423484802:  85%|████████▌ | 129/151 [00:28<00:05,  4.04it/s]Epoch: 10, train for the 130-th batch, train loss: 0.5486144423484802:  86%|████████▌ | 130/151 [00:28<00:05,  4.19it/s]Epoch: 5, train for the 119-th batch, train loss: 0.5656615495681763:  81%|████████▉  | 118/146 [01:10<00:18,  1.52it/s]Epoch: 5, train for the 119-th batch, train loss: 0.5656615495681763:  82%|████████▉  | 119/146 [01:10<00:17,  1.53it/s]Epoch: 2, train for the 342-th batch, train loss: 0.44356265664100647:  89%|████████▉ | 341/383 [03:27<00:25,  1.67it/s]Epoch: 2, train for the 342-th batch, train loss: 0.44356265664100647:  89%|████████▉ | 342/383 [03:27<00:24,  1.68it/s]evaluate for the 31-th batch, evaluate loss: 0.6188420057296753:  45%|████████▏         | 30/66 [00:08<00:09,  3.69it/s]evaluate for the 31-th batch, evaluate loss: 0.6188420057296753:  47%|████████▍         | 31/66 [00:08<00:09,  3.58it/s]Epoch: 6, train for the 75-th batch, train loss: 0.17737895250320435:  62%|███████▍    | 74/119 [00:45<00:24,  1.87it/s]Epoch: 6, train for the 75-th batch, train loss: 0.17737895250320435:  63%|███████▌    | 75/119 [00:45<00:24,  1.77it/s]Epoch: 10, train for the 131-th batch, train loss: 0.5124961137771606:  86%|████████▌ | 130/151 [00:29<00:05,  4.19it/s]Epoch: 10, train for the 131-th batch, train loss: 0.5124961137771606:  87%|████████▋ | 131/151 [00:29<00:05,  3.61it/s]evaluate for the 32-th batch, evaluate loss: 0.6197140216827393:  47%|████████▍         | 31/66 [00:08<00:09,  3.58it/s]evaluate for the 32-th batch, evaluate loss: 0.6197140216827393:  48%|████████▋         | 32/66 [00:08<00:09,  3.75it/s]Epoch: 5, train for the 120-th batch, train loss: 0.5302741527557373:  82%|████████▉  | 119/146 [01:10<00:17,  1.53it/s]Epoch: 5, train for the 120-th batch, train loss: 0.5302741527557373:  82%|█████████  | 120/146 [01:10<00:16,  1.56it/s]Epoch: 2, train for the 343-th batch, train loss: 0.4243350028991699:  89%|█████████▊ | 342/383 [03:27<00:24,  1.68it/s]Epoch: 2, train for the 343-th batch, train loss: 0.4243350028991699:  90%|█████████▊ | 343/383 [03:27<00:23,  1.67it/s]evaluate for the 33-th batch, evaluate loss: 0.6168779134750366:  48%|████████▋         | 32/66 [00:09<00:09,  3.75it/s]evaluate for the 33-th batch, evaluate loss: 0.6168779134750366:  50%|█████████         | 33/66 [00:09<00:09,  3.56it/s]Epoch: 10, train for the 132-th batch, train loss: 0.5411377549171448:  87%|████████▋ | 131/151 [00:29<00:05,  3.61it/s]Epoch: 10, train for the 132-th batch, train loss: 0.5411377549171448:  87%|████████▋ | 132/151 [00:29<00:06,  3.14it/s]Epoch: 6, train for the 76-th batch, train loss: 0.19803962111473083:  63%|███████▌    | 75/119 [00:45<00:24,  1.77it/s]Epoch: 6, train for the 76-th batch, train loss: 0.19803962111473083:  64%|███████▋    | 76/119 [00:45<00:24,  1.73it/s]evaluate for the 34-th batch, evaluate loss: 0.611673891544342:  50%|█████████▌         | 33/66 [00:09<00:09,  3.56it/s]evaluate for the 34-th batch, evaluate loss: 0.611673891544342:  52%|█████████▊         | 34/66 [00:09<00:08,  3.62it/s]Epoch: 10, train for the 133-th batch, train loss: 0.5387004613876343:  87%|████████▋ | 132/151 [00:29<00:06,  3.14it/s]Epoch: 10, train for the 133-th batch, train loss: 0.5387004613876343:  88%|████████▊ | 133/151 [00:29<00:06,  2.81it/s]evaluate for the 35-th batch, evaluate loss: 0.6188021302223206:  52%|█████████▎        | 34/66 [00:09<00:08,  3.62it/s]evaluate for the 35-th batch, evaluate loss: 0.6188021302223206:  53%|█████████▌        | 35/66 [00:09<00:08,  3.55it/s]Epoch: 5, train for the 121-th batch, train loss: 0.5524981021881104:  82%|█████████  | 120/146 [01:11<00:16,  1.56it/s]Epoch: 5, train for the 121-th batch, train loss: 0.5524981021881104:  83%|█████████  | 121/146 [01:11<00:15,  1.58it/s]Epoch: 2, train for the 344-th batch, train loss: 0.5084742903709412:  90%|█████████▊ | 343/383 [03:28<00:23,  1.67it/s]Epoch: 2, train for the 344-th batch, train loss: 0.5084742903709412:  90%|█████████▉ | 344/383 [03:28<00:23,  1.68it/s]Epoch: 6, train for the 77-th batch, train loss: 0.16960559785366058:  64%|███████▋    | 76/119 [00:46<00:24,  1.73it/s]Epoch: 6, train for the 77-th batch, train loss: 0.16960559785366058:  65%|███████▊    | 77/119 [00:46<00:24,  1.71it/s]evaluate for the 36-th batch, evaluate loss: 0.6395984292030334:  53%|█████████▌        | 35/66 [00:09<00:08,  3.55it/s]evaluate for the 36-th batch, evaluate loss: 0.6395984292030334:  55%|█████████▊        | 36/66 [00:09<00:08,  3.67it/s]Epoch: 10, train for the 134-th batch, train loss: 0.5610955953598022:  88%|████████▊ | 133/151 [00:30<00:06,  2.81it/s]Epoch: 10, train for the 134-th batch, train loss: 0.5610955953598022:  89%|████████▊ | 134/151 [00:30<00:06,  2.80it/s]evaluate for the 37-th batch, evaluate loss: 0.6501380801200867:  55%|█████████▊        | 36/66 [00:10<00:08,  3.67it/s]evaluate for the 37-th batch, evaluate loss: 0.6501380801200867:  56%|██████████        | 37/66 [00:10<00:08,  3.60it/s]Epoch: 5, train for the 122-th batch, train loss: 0.5523282289505005:  83%|█████████  | 121/146 [01:12<00:15,  1.58it/s]Epoch: 5, train for the 122-th batch, train loss: 0.5523282289505005:  84%|█████████▏ | 122/146 [01:12<00:14,  1.61it/s]Epoch: 2, train for the 345-th batch, train loss: 0.38854286074638367:  90%|████████▉ | 344/383 [03:29<00:23,  1.68it/s]Epoch: 2, train for the 345-th batch, train loss: 0.38854286074638367:  90%|█████████ | 345/383 [03:29<00:22,  1.69it/s]Epoch: 6, train for the 78-th batch, train loss: 0.1912350356578827:  65%|████████▍    | 77/119 [00:46<00:24,  1.71it/s]Epoch: 6, train for the 78-th batch, train loss: 0.1912350356578827:  66%|████████▌    | 78/119 [00:46<00:24,  1.69it/s]evaluate for the 38-th batch, evaluate loss: 0.6112762093544006:  56%|██████████        | 37/66 [00:10<00:08,  3.60it/s]evaluate for the 38-th batch, evaluate loss: 0.6112762093544006:  58%|██████████▎       | 38/66 [00:10<00:07,  3.70it/s]Epoch: 10, train for the 135-th batch, train loss: 0.5230824947357178:  89%|████████▊ | 134/151 [00:30<00:06,  2.80it/s]Epoch: 10, train for the 135-th batch, train loss: 0.5230824947357178:  89%|████████▉ | 135/151 [00:30<00:06,  2.36it/s]evaluate for the 39-th batch, evaluate loss: 0.6075767874717712:  58%|██████████▎       | 38/66 [00:10<00:07,  3.70it/s]evaluate for the 39-th batch, evaluate loss: 0.6075767874717712:  59%|██████████▋       | 39/66 [00:10<00:07,  3.68it/s]Epoch: 5, train for the 123-th batch, train loss: 0.5636774301528931:  84%|█████████▏ | 122/146 [01:12<00:14,  1.61it/s]Epoch: 5, train for the 123-th batch, train loss: 0.5636774301528931:  84%|█████████▎ | 123/146 [01:12<00:14,  1.62it/s]Epoch: 2, train for the 346-th batch, train loss: 0.4387883245944977:  90%|█████████▉ | 345/383 [03:29<00:22,  1.69it/s]Epoch: 2, train for the 346-th batch, train loss: 0.4387883245944977:  90%|█████████▉ | 346/383 [03:29<00:22,  1.67it/s]evaluate for the 40-th batch, evaluate loss: 0.6139083504676819:  59%|██████████▋       | 39/66 [00:11<00:07,  3.68it/s]evaluate for the 40-th batch, evaluate loss: 0.6139083504676819:  61%|██████████▉       | 40/66 [00:11<00:07,  3.62it/s]Epoch: 6, train for the 79-th batch, train loss: 0.19275933504104614:  66%|███████▊    | 78/119 [00:47<00:24,  1.69it/s]Epoch: 6, train for the 79-th batch, train loss: 0.19275933504104614:  66%|███████▉    | 79/119 [00:47<00:23,  1.67it/s]Epoch: 10, train for the 136-th batch, train loss: 0.562653124332428:  89%|█████████▊ | 135/151 [00:31<00:06,  2.36it/s]Epoch: 10, train for the 136-th batch, train loss: 0.562653124332428:  90%|█████████▉ | 136/151 [00:31<00:07,  2.09it/s]evaluate for the 41-th batch, evaluate loss: 0.5769096612930298:  61%|██████████▉       | 40/66 [00:11<00:07,  3.62it/s]evaluate for the 41-th batch, evaluate loss: 0.5769096612930298:  62%|███████████▏      | 41/66 [00:11<00:06,  3.64it/s]Epoch: 2, train for the 347-th batch, train loss: 0.40064293146133423:  90%|█████████ | 346/383 [03:30<00:22,  1.67it/s]Epoch: 2, train for the 347-th batch, train loss: 0.40064293146133423:  91%|█████████ | 347/383 [03:30<00:21,  1.67it/s]Epoch: 5, train for the 124-th batch, train loss: 0.5895837545394897:  84%|█████████▎ | 123/146 [01:13<00:14,  1.62it/s]Epoch: 5, train for the 124-th batch, train loss: 0.5895837545394897:  85%|█████████▎ | 124/146 [01:13<00:13,  1.63it/s]evaluate for the 42-th batch, evaluate loss: 0.6252756714820862:  62%|███████████▏      | 41/66 [00:11<00:06,  3.64it/s]evaluate for the 42-th batch, evaluate loss: 0.6252756714820862:  64%|███████████▍      | 42/66 [00:11<00:06,  3.57it/s]Epoch: 6, train for the 80-th batch, train loss: 0.1962500661611557:  66%|████████▋    | 79/119 [00:48<00:23,  1.67it/s]Epoch: 6, train for the 80-th batch, train loss: 0.1962500661611557:  67%|████████▋    | 80/119 [00:48<00:23,  1.67it/s]Epoch: 10, train for the 137-th batch, train loss: 0.610546350479126:  90%|█████████▉ | 136/151 [00:32<00:07,  2.09it/s]Epoch: 10, train for the 137-th batch, train loss: 0.610546350479126:  91%|█████████▉ | 137/151 [00:32<00:06,  2.01it/s]evaluate for the 43-th batch, evaluate loss: 0.5884279012680054:  64%|███████████▍      | 42/66 [00:11<00:06,  3.57it/s]evaluate for the 43-th batch, evaluate loss: 0.5884279012680054:  65%|███████████▋      | 43/66 [00:11<00:06,  3.73it/s]Epoch: 2, train for the 348-th batch, train loss: 0.49085554480552673:  91%|█████████ | 347/383 [03:30<00:21,  1.67it/s]Epoch: 2, train for the 348-th batch, train loss: 0.49085554480552673:  91%|█████████ | 348/383 [03:30<00:20,  1.67it/s]Epoch: 5, train for the 125-th batch, train loss: 0.522885262966156:  85%|██████████▏ | 124/146 [01:13<00:13,  1.63it/s]Epoch: 5, train for the 125-th batch, train loss: 0.522885262966156:  86%|██████████▎ | 125/146 [01:13<00:12,  1.64it/s]evaluate for the 44-th batch, evaluate loss: 0.5776766538619995:  65%|███████████▋      | 43/66 [00:12<00:06,  3.73it/s]evaluate for the 44-th batch, evaluate loss: 0.5776766538619995:  67%|████████████      | 44/66 [00:12<00:06,  3.60it/s]Epoch: 6, train for the 81-th batch, train loss: 0.1907350867986679:  67%|████████▋    | 80/119 [00:48<00:23,  1.67it/s]Epoch: 6, train for the 81-th batch, train loss: 0.1907350867986679:  68%|████████▊    | 81/119 [00:48<00:22,  1.67it/s]Epoch: 10, train for the 138-th batch, train loss: 0.5905413627624512:  91%|█████████ | 137/151 [00:32<00:06,  2.01it/s]Epoch: 10, train for the 138-th batch, train loss: 0.5905413627624512:  91%|█████████▏| 138/151 [00:32<00:06,  1.90it/s]evaluate for the 45-th batch, evaluate loss: 0.6270634531974792:  67%|████████████      | 44/66 [00:12<00:06,  3.60it/s]evaluate for the 45-th batch, evaluate loss: 0.6270634531974792:  68%|████████████▎     | 45/66 [00:12<00:05,  3.74it/s]Epoch: 5, train for the 126-th batch, train loss: 0.5560922622680664:  86%|█████████▍ | 125/146 [01:14<00:12,  1.64it/s]Epoch: 5, train for the 126-th batch, train loss: 0.5560922622680664:  86%|█████████▍ | 126/146 [01:14<00:12,  1.65it/s]Epoch: 2, train for the 349-th batch, train loss: 0.49836549162864685:  91%|█████████ | 348/383 [03:31<00:20,  1.67it/s]Epoch: 2, train for the 349-th batch, train loss: 0.49836549162864685:  91%|█████████ | 349/383 [03:31<00:20,  1.67it/s]evaluate for the 46-th batch, evaluate loss: 0.6298108696937561:  68%|████████████▎     | 45/66 [00:12<00:05,  3.74it/s]evaluate for the 46-th batch, evaluate loss: 0.6298108696937561:  70%|████████████▌     | 46/66 [00:12<00:05,  3.55it/s]Epoch: 6, train for the 82-th batch, train loss: 0.187329962849617:  68%|█████████▌    | 81/119 [00:49<00:22,  1.67it/s]Epoch: 6, train for the 82-th batch, train loss: 0.187329962849617:  69%|█████████▋    | 82/119 [00:49<00:22,  1.67it/s]Epoch: 10, train for the 139-th batch, train loss: 0.5525133013725281:  91%|█████████▏| 138/151 [00:33<00:06,  1.90it/s]Epoch: 10, train for the 139-th batch, train loss: 0.5525133013725281:  92%|█████████▏| 139/151 [00:33<00:06,  1.80it/s]evaluate for the 47-th batch, evaluate loss: 0.6285667419433594:  70%|████████████▌     | 46/66 [00:12<00:05,  3.55it/s]evaluate for the 47-th batch, evaluate loss: 0.6285667419433594:  71%|████████████▊     | 47/66 [00:12<00:05,  3.61it/s]evaluate for the 48-th batch, evaluate loss: 0.6194329857826233:  71%|████████████▊     | 47/66 [00:13<00:05,  3.61it/s]evaluate for the 48-th batch, evaluate loss: 0.6194329857826233:  73%|█████████████     | 48/66 [00:13<00:05,  3.56it/s]Epoch: 2, train for the 350-th batch, train loss: 0.4315697252750397:  91%|██████████ | 349/383 [03:32<00:20,  1.67it/s]Epoch: 2, train for the 350-th batch, train loss: 0.4315697252750397:  91%|██████████ | 350/383 [03:32<00:19,  1.67it/s]Epoch: 5, train for the 127-th batch, train loss: 0.5504985451698303:  86%|█████████▍ | 126/146 [01:15<00:12,  1.65it/s]Epoch: 5, train for the 127-th batch, train loss: 0.5504985451698303:  87%|█████████▌ | 127/146 [01:15<00:11,  1.65it/s]Epoch: 6, train for the 83-th batch, train loss: 0.17991778254508972:  69%|████████▎   | 82/119 [00:49<00:22,  1.67it/s]Epoch: 6, train for the 83-th batch, train loss: 0.17991778254508972:  70%|████████▎   | 83/119 [00:49<00:21,  1.67it/s]evaluate for the 49-th batch, evaluate loss: 0.6431860327720642:  73%|█████████████     | 48/66 [00:13<00:05,  3.56it/s]evaluate for the 49-th batch, evaluate loss: 0.6431860327720642:  74%|█████████████▎    | 49/66 [00:13<00:04,  3.68it/s]Epoch: 10, train for the 140-th batch, train loss: 0.5185955166816711:  92%|█████████▏| 139/151 [00:33<00:06,  1.80it/s]Epoch: 10, train for the 140-th batch, train loss: 0.5185955166816711:  93%|█████████▎| 140/151 [00:33<00:06,  1.73it/s]evaluate for the 50-th batch, evaluate loss: 0.6408107876777649:  74%|█████████████▎    | 49/66 [00:13<00:04,  3.68it/s]evaluate for the 50-th batch, evaluate loss: 0.6408107876777649:  76%|█████████████▋    | 50/66 [00:13<00:04,  3.62it/s]Epoch: 2, train for the 351-th batch, train loss: 0.452595591545105:  91%|██████████▉ | 350/383 [03:32<00:19,  1.67it/s]Epoch: 2, train for the 351-th batch, train loss: 0.452595591545105:  92%|██████████▉ | 351/383 [03:32<00:19,  1.68it/s]Epoch: 5, train for the 128-th batch, train loss: 0.5341029763221741:  87%|█████████▌ | 127/146 [01:15<00:11,  1.65it/s]Epoch: 5, train for the 128-th batch, train loss: 0.5341029763221741:  88%|█████████▋ | 128/146 [01:15<00:10,  1.66it/s]Epoch: 6, train for the 84-th batch, train loss: 0.162500262260437:  70%|█████████▊    | 83/119 [00:50<00:21,  1.67it/s]Epoch: 6, train for the 84-th batch, train loss: 0.162500262260437:  71%|█████████▉    | 84/119 [00:50<00:20,  1.67it/s]evaluate for the 51-th batch, evaluate loss: 0.6016543507575989:  76%|█████████████▋    | 50/66 [00:14<00:04,  3.62it/s]evaluate for the 51-th batch, evaluate loss: 0.6016543507575989:  77%|█████████████▉    | 51/66 [00:14<00:04,  3.70it/s]evaluate for the 52-th batch, evaluate loss: 0.6198064684867859:  77%|█████████████▉    | 51/66 [00:14<00:04,  3.70it/s]evaluate for the 52-th batch, evaluate loss: 0.6198064684867859:  79%|██████████████▏   | 52/66 [00:14<00:03,  3.65it/s]Epoch: 10, train for the 141-th batch, train loss: 0.5798466205596924:  93%|█████████▎| 140/151 [00:34<00:06,  1.73it/s]Epoch: 10, train for the 141-th batch, train loss: 0.5798466205596924:  93%|█████████▎| 141/151 [00:34<00:06,  1.50it/s]Epoch: 2, train for the 352-th batch, train loss: 0.3803911507129669:  92%|██████████ | 351/383 [03:33<00:19,  1.68it/s]Epoch: 2, train for the 352-th batch, train loss: 0.3803911507129669:  92%|██████████ | 352/383 [03:33<00:18,  1.67it/s]Epoch: 5, train for the 129-th batch, train loss: 0.5737744569778442:  88%|█████████▋ | 128/146 [01:16<00:10,  1.66it/s]Epoch: 5, train for the 129-th batch, train loss: 0.5737744569778442:  88%|█████████▋ | 129/146 [01:16<00:10,  1.66it/s]evaluate for the 53-th batch, evaluate loss: 0.6217458844184875:  79%|██████████████▏   | 52/66 [00:14<00:03,  3.65it/s]evaluate for the 53-th batch, evaluate loss: 0.6217458844184875:  80%|██████████████▍   | 53/66 [00:14<00:03,  3.58it/s]Epoch: 6, train for the 85-th batch, train loss: 0.17578542232513428:  71%|████████▍   | 84/119 [00:51<00:20,  1.67it/s]Epoch: 6, train for the 85-th batch, train loss: 0.17578542232513428:  71%|████████▌   | 85/119 [00:51<00:20,  1.67it/s]evaluate for the 54-th batch, evaluate loss: 0.652142345905304:  80%|███████████████▎   | 53/66 [00:14<00:03,  3.58it/s]evaluate for the 54-th batch, evaluate loss: 0.652142345905304:  82%|███████████████▌   | 54/66 [00:14<00:03,  3.62it/s]Epoch: 10, train for the 142-th batch, train loss: 0.5555112361907959:  93%|█████████▎| 141/151 [00:35<00:06,  1.50it/s]Epoch: 10, train for the 142-th batch, train loss: 0.5555112361907959:  94%|█████████▍| 142/151 [00:35<00:05,  1.56it/s]Epoch: 2, train for the 353-th batch, train loss: 0.45981788635253906:  92%|█████████▏| 352/383 [03:33<00:18,  1.67it/s]Epoch: 2, train for the 353-th batch, train loss: 0.45981788635253906:  92%|█████████▏| 353/383 [03:33<00:17,  1.68it/s]Epoch: 5, train for the 130-th batch, train loss: 0.521919846534729:  88%|██████████▌ | 129/146 [01:16<00:10,  1.66it/s]Epoch: 5, train for the 130-th batch, train loss: 0.521919846534729:  89%|██████████▋ | 130/146 [01:16<00:09,  1.66it/s]evaluate for the 55-th batch, evaluate loss: 0.615315854549408:  82%|███████████████▌   | 54/66 [00:15<00:03,  3.62it/s]evaluate for the 55-th batch, evaluate loss: 0.615315854549408:  83%|███████████████▊   | 55/66 [00:15<00:03,  3.54it/s]Epoch: 6, train for the 86-th batch, train loss: 0.19286313652992249:  71%|████████▌   | 85/119 [00:51<00:20,  1.67it/s]Epoch: 6, train for the 86-th batch, train loss: 0.19286313652992249:  72%|████████▋   | 86/119 [00:51<00:19,  1.67it/s]evaluate for the 56-th batch, evaluate loss: 0.6278352737426758:  83%|███████████████   | 55/66 [00:15<00:03,  3.54it/s]evaluate for the 56-th batch, evaluate loss: 0.6278352737426758:  85%|███████████████▎  | 56/66 [00:15<00:02,  3.70it/s]Epoch: 10, train for the 143-th batch, train loss: 0.472655713558197:  94%|██████████▎| 142/151 [00:35<00:05,  1.56it/s]Epoch: 10, train for the 143-th batch, train loss: 0.472655713558197:  95%|██████████▍| 143/151 [00:35<00:04,  1.62it/s]Epoch: 2, train for the 354-th batch, train loss: 0.3604895770549774:  92%|██████████▏| 353/383 [03:34<00:17,  1.68it/s]Epoch: 2, train for the 354-th batch, train loss: 0.3604895770549774:  92%|██████████▏| 354/383 [03:34<00:17,  1.68it/s]Epoch: 5, train for the 131-th batch, train loss: 0.5594583749771118:  89%|█████████▊ | 130/146 [01:17<00:09,  1.66it/s]Epoch: 5, train for the 131-th batch, train loss: 0.5594583749771118:  90%|█████████▊ | 131/146 [01:17<00:09,  1.66it/s]evaluate for the 57-th batch, evaluate loss: 0.6050394177436829:  85%|███████████████▎  | 56/66 [00:15<00:02,  3.70it/s]evaluate for the 57-th batch, evaluate loss: 0.6050394177436829:  86%|███████████████▌  | 57/66 [00:15<00:02,  3.58it/s]Epoch: 6, train for the 87-th batch, train loss: 0.16878154873847961:  72%|████████▋   | 86/119 [00:52<00:19,  1.67it/s]Epoch: 6, train for the 87-th batch, train loss: 0.16878154873847961:  73%|████████▊   | 87/119 [00:52<00:19,  1.67it/s]evaluate for the 58-th batch, evaluate loss: 0.6517866253852844:  86%|███████████████▌  | 57/66 [00:15<00:02,  3.58it/s]evaluate for the 58-th batch, evaluate loss: 0.6517866253852844:  88%|███████████████▊  | 58/66 [00:15<00:02,  3.74it/s]Epoch: 10, train for the 144-th batch, train loss: 0.5070332288742065:  95%|█████████▍| 143/151 [00:36<00:04,  1.62it/s]Epoch: 10, train for the 144-th batch, train loss: 0.5070332288742065:  95%|█████████▌| 144/151 [00:36<00:04,  1.70it/s]Epoch: 2, train for the 355-th batch, train loss: 0.5458164811134338:  92%|██████████▏| 354/383 [03:35<00:17,  1.68it/s]Epoch: 2, train for the 355-th batch, train loss: 0.5458164811134338:  93%|██████████▏| 355/383 [03:35<00:16,  1.67it/s]evaluate for the 59-th batch, evaluate loss: 0.6307867765426636:  88%|███████████████▊  | 58/66 [00:16<00:02,  3.74it/s]evaluate for the 59-th batch, evaluate loss: 0.6307867765426636:  89%|████████████████  | 59/66 [00:16<00:01,  3.56it/s]Epoch: 5, train for the 132-th batch, train loss: 0.5280389189720154:  90%|█████████▊ | 131/146 [01:18<00:09,  1.66it/s]Epoch: 5, train for the 132-th batch, train loss: 0.5280389189720154:  90%|█████████▉ | 132/146 [01:18<00:08,  1.66it/s]Epoch: 6, train for the 88-th batch, train loss: 0.23563620448112488:  73%|████████▊   | 87/119 [00:52<00:19,  1.67it/s]Epoch: 6, train for the 88-th batch, train loss: 0.23563620448112488:  74%|████████▊   | 88/119 [00:52<00:18,  1.67it/s]evaluate for the 60-th batch, evaluate loss: 0.6377363204956055:  89%|████████████████  | 59/66 [00:16<00:01,  3.56it/s]evaluate for the 60-th batch, evaluate loss: 0.6377363204956055:  91%|████████████████▎ | 60/66 [00:16<00:01,  3.61it/s]Epoch: 10, train for the 145-th batch, train loss: 0.5355382561683655:  95%|█████████▌| 144/151 [00:37<00:04,  1.70it/s]Epoch: 10, train for the 145-th batch, train loss: 0.5355382561683655:  96%|█████████▌| 145/151 [00:37<00:03,  1.67it/s]evaluate for the 61-th batch, evaluate loss: 0.6456155776977539:  91%|████████████████▎ | 60/66 [00:16<00:01,  3.61it/s]evaluate for the 61-th batch, evaluate loss: 0.6456155776977539:  92%|████████████████▋ | 61/66 [00:16<00:01,  3.55it/s]Epoch: 2, train for the 356-th batch, train loss: 0.35791173577308655:  93%|█████████▎| 355/383 [03:35<00:16,  1.67it/s]Epoch: 2, train for the 356-th batch, train loss: 0.35791173577308655:  93%|█████████▎| 356/383 [03:35<00:16,  1.68it/s]Epoch: 5, train for the 133-th batch, train loss: 0.5221682786941528:  90%|█████████▉ | 132/146 [01:18<00:08,  1.66it/s]Epoch: 5, train for the 133-th batch, train loss: 0.5221682786941528:  91%|██████████ | 133/146 [01:18<00:07,  1.66it/s]Epoch: 6, train for the 89-th batch, train loss: 0.19451870024204254:  74%|████████▊   | 88/119 [00:53<00:18,  1.67it/s]Epoch: 6, train for the 89-th batch, train loss: 0.19451870024204254:  75%|████████▉   | 89/119 [00:53<00:18,  1.67it/s]evaluate for the 62-th batch, evaluate loss: 0.6622330546379089:  92%|████████████████▋ | 61/66 [00:17<00:01,  3.55it/s]evaluate for the 62-th batch, evaluate loss: 0.6622330546379089:  94%|████████████████▉ | 62/66 [00:17<00:01,  3.69it/s]Epoch: 10, train for the 146-th batch, train loss: 0.5518566966056824:  96%|█████████▌| 145/151 [00:37<00:03,  1.67it/s]Epoch: 10, train for the 146-th batch, train loss: 0.5518566966056824:  97%|█████████▋| 146/151 [00:37<00:02,  1.70it/s]evaluate for the 63-th batch, evaluate loss: 0.6308634281158447:  94%|████████████████▉ | 62/66 [00:17<00:01,  3.69it/s]evaluate for the 63-th batch, evaluate loss: 0.6308634281158447:  95%|█████████████████▏| 63/66 [00:17<00:00,  3.64it/s]Epoch: 2, train for the 357-th batch, train loss: 0.48087605834007263:  93%|█████████▎| 356/383 [03:36<00:16,  1.68it/s]Epoch: 2, train for the 357-th batch, train loss: 0.48087605834007263:  93%|█████████▎| 357/383 [03:36<00:15,  1.68it/s]Epoch: 5, train for the 134-th batch, train loss: 0.5203966498374939:  91%|██████████ | 133/146 [01:19<00:07,  1.66it/s]Epoch: 5, train for the 134-th batch, train loss: 0.5203966498374939:  92%|██████████ | 134/146 [01:19<00:07,  1.67it/s]evaluate for the 64-th batch, evaluate loss: 0.6224268078804016:  95%|█████████████████▏| 63/66 [00:17<00:00,  3.64it/s]evaluate for the 64-th batch, evaluate loss: 0.6224268078804016:  97%|█████████████████▍| 64/66 [00:17<00:00,  3.72it/s]Epoch: 6, train for the 90-th batch, train loss: 0.1906110793352127:  75%|█████████▋   | 89/119 [00:54<00:18,  1.67it/s]Epoch: 6, train for the 90-th batch, train loss: 0.1906110793352127:  76%|█████████▊   | 90/119 [00:54<00:17,  1.67it/s]Epoch: 10, train for the 147-th batch, train loss: 0.5604180693626404:  97%|█████████▋| 146/151 [00:38<00:02,  1.70it/s]Epoch: 10, train for the 147-th batch, train loss: 0.5604180693626404:  97%|█████████▋| 147/151 [00:38<00:02,  1.71it/s]evaluate for the 65-th batch, evaluate loss: 0.600460410118103:  97%|██████████████████▍| 64/66 [00:17<00:00,  3.72it/s]evaluate for the 65-th batch, evaluate loss: 0.600460410118103:  98%|██████████████████▋| 65/66 [00:17<00:00,  3.66it/s]Epoch: 2, train for the 358-th batch, train loss: 0.5478823781013489:  93%|██████████▎| 357/383 [03:36<00:15,  1.68it/s]Epoch: 2, train for the 358-th batch, train loss: 0.5478823781013489:  93%|██████████▎| 358/383 [03:36<00:14,  1.67it/s]Epoch: 6, train for the 91-th batch, train loss: 0.16991494596004486:  76%|█████████   | 90/119 [00:54<00:17,  1.67it/s]Epoch: 6, train for the 91-th batch, train loss: 0.16991494596004486:  76%|█████████▏  | 91/119 [00:54<00:15,  1.77it/s]evaluate for the 66-th batch, evaluate loss: 0.6436198949813843:  98%|█████████████████▋| 65/66 [00:18<00:00,  3.66it/s]evaluate for the 66-th batch, evaluate loss: 0.6436198949813843: 100%|██████████████████| 66/66 [00:18<00:00,  3.90it/s]evaluate for the 66-th batch, evaluate loss: 0.6436198949813843: 100%|██████████████████| 66/66 [00:18<00:00,  3.64it/s]
  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 5, train for the 135-th batch, train loss: 0.5227535367012024:  92%|██████████ | 134/146 [01:19<00:07,  1.67it/s]Epoch: 5, train for the 135-th batch, train loss: 0.5227535367012024:  92%|██████████▏| 135/146 [01:19<00:07,  1.56it/s]evaluate for the 1-th batch, evaluate loss: 0.6705091595649719:   0%|                            | 0/40 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6705091595649719:   2%|▌                   | 1/40 [00:00<00:09,  4.11it/s]Epoch: 10, train for the 148-th batch, train loss: 0.5705066919326782:  97%|█████████▋| 147/151 [00:38<00:02,  1.71it/s]Epoch: 10, train for the 148-th batch, train loss: 0.5705066919326782:  98%|█████████▊| 148/151 [00:38<00:01,  1.73it/s]Epoch: 2, train for the 359-th batch, train loss: 0.45713481307029724:  93%|█████████▎| 358/383 [03:37<00:14,  1.67it/s]Epoch: 2, train for the 359-th batch, train loss: 0.45713481307029724:  94%|█████████▎| 359/383 [03:37<00:14,  1.67it/s]evaluate for the 2-th batch, evaluate loss: 0.68937087059021:   2%|▌                     | 1/40 [00:00<00:09,  4.11it/s]evaluate for the 2-th batch, evaluate loss: 0.68937087059021:   5%|█                     | 2/40 [00:00<00:10,  3.50it/s]Epoch: 6, train for the 92-th batch, train loss: 0.21802851557731628:  76%|█████████▏  | 91/119 [00:55<00:15,  1.77it/s]Epoch: 6, train for the 92-th batch, train loss: 0.21802851557731628:  77%|█████████▎  | 92/119 [00:55<00:15,  1.75it/s]Epoch: 5, train for the 136-th batch, train loss: 0.5386649370193481:  92%|██████████▏| 135/146 [01:20<00:07,  1.56it/s]Epoch: 5, train for the 136-th batch, train loss: 0.5386649370193481:  93%|██████████▏| 136/146 [01:20<00:06,  1.59it/s]evaluate for the 3-th batch, evaluate loss: 0.6824235320091248:   5%|█                   | 2/40 [00:00<00:10,  3.50it/s]evaluate for the 3-th batch, evaluate loss: 0.6824235320091248:   8%|█▌                  | 3/40 [00:00<00:10,  3.65it/s]Epoch: 10, train for the 149-th batch, train loss: 0.5080130100250244:  98%|█████████▊| 148/151 [00:39<00:01,  1.73it/s]Epoch: 10, train for the 149-th batch, train loss: 0.5080130100250244:  99%|█████████▊| 149/151 [00:39<00:01,  1.76it/s]Epoch: 2, train for the 360-th batch, train loss: 0.4310608506202698:  94%|██████████▎| 359/383 [03:38<00:14,  1.67it/s]Epoch: 2, train for the 360-th batch, train loss: 0.4310608506202698:  94%|██████████▎| 360/383 [03:38<00:13,  1.67it/s]evaluate for the 4-th batch, evaluate loss: 0.6993538737297058:   8%|█▌                  | 3/40 [00:01<00:10,  3.65it/s]evaluate for the 4-th batch, evaluate loss: 0.6993538737297058:  10%|██                  | 4/40 [00:01<00:10,  3.42it/s]Epoch: 6, train for the 93-th batch, train loss: 0.1677677482366562:  77%|██████████   | 92/119 [00:55<00:15,  1.75it/s]Epoch: 6, train for the 93-th batch, train loss: 0.1677677482366562:  78%|██████████▏  | 93/119 [00:55<00:14,  1.74it/s]Epoch: 5, train for the 137-th batch, train loss: 0.5379740595817566:  93%|██████████▏| 136/146 [01:21<00:06,  1.59it/s]Epoch: 5, train for the 137-th batch, train loss: 0.5379740595817566:  94%|██████████▎| 137/146 [01:21<00:05,  1.61it/s]evaluate for the 5-th batch, evaluate loss: 0.6676327586174011:  10%|██                  | 4/40 [00:01<00:10,  3.42it/s]evaluate for the 5-th batch, evaluate loss: 0.6676327586174011:  12%|██▌                 | 5/40 [00:01<00:09,  3.52it/s]Epoch: 10, train for the 150-th batch, train loss: 0.5293580889701843:  99%|█████████▊| 149/151 [00:39<00:01,  1.76it/s]Epoch: 10, train for the 150-th batch, train loss: 0.5293580889701843:  99%|█████████▉| 150/151 [00:39<00:00,  1.72it/s]Epoch: 2, train for the 361-th batch, train loss: 0.4611766040325165:  94%|██████████▎| 360/383 [03:38<00:13,  1.67it/s]Epoch: 2, train for the 361-th batch, train loss: 0.4611766040325165:  94%|██████████▎| 361/383 [03:38<00:13,  1.68it/s]evaluate for the 6-th batch, evaluate loss: 0.6885927319526672:  12%|██▌                 | 5/40 [00:01<00:09,  3.52it/s]evaluate for the 6-th batch, evaluate loss: 0.6885927319526672:  15%|███                 | 6/40 [00:01<00:10,  3.39it/s]Epoch: 6, train for the 94-th batch, train loss: 0.1787203550338745:  78%|██████████▏  | 93/119 [00:56<00:14,  1.74it/s]Epoch: 6, train for the 94-th batch, train loss: 0.1787203550338745:  79%|██████████▎  | 94/119 [00:56<00:14,  1.71it/s]Epoch: 10, train for the 151-th batch, train loss: 0.5806106328964233:  99%|█████████▉| 150/151 [00:40<00:00,  1.72it/s]Epoch: 10, train for the 151-th batch, train loss: 0.5806106328964233: 100%|██████████| 151/151 [00:40<00:00,  1.96it/s]Epoch: 10, train for the 151-th batch, train loss: 0.5806106328964233: 100%|██████████| 151/151 [00:40<00:00,  3.75it/s]
  0%|                                                                                            | 0/46 [00:00<?, ?it/s]Epoch: 5, train for the 138-th batch, train loss: 0.5456982851028442:  94%|██████████▎| 137/146 [01:21<00:05,  1.61it/s]Epoch: 5, train for the 138-th batch, train loss: 0.5456982851028442:  95%|██████████▍| 138/146 [01:21<00:04,  1.61it/s]evaluate for the 7-th batch, evaluate loss: 0.6764656901359558:  15%|███                 | 6/40 [00:01<00:10,  3.39it/s]evaluate for the 7-th batch, evaluate loss: 0.6764656901359558:  18%|███▌                | 7/40 [00:01<00:09,  3.48it/s]evaluate for the 1-th batch, evaluate loss: 0.48856955766677856:   0%|                           | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.48856955766677856:   2%|▍                  | 1/46 [00:00<00:14,  3.19it/s]Epoch: 2, train for the 362-th batch, train loss: 0.4112902879714966:  94%|██████████▎| 361/383 [03:39<00:13,  1.68it/s]Epoch: 2, train for the 362-th batch, train loss: 0.4112902879714966:  95%|██████████▍| 362/383 [03:39<00:12,  1.69it/s]evaluate for the 8-th batch, evaluate loss: 0.7219287157058716:  18%|███▌                | 7/40 [00:02<00:09,  3.48it/s]evaluate for the 8-th batch, evaluate loss: 0.7219287157058716:  20%|████                | 8/40 [00:02<00:09,  3.39it/s]evaluate for the 2-th batch, evaluate loss: 0.5013492703437805:   2%|▍                   | 1/46 [00:00<00:14,  3.19it/s]evaluate for the 2-th batch, evaluate loss: 0.5013492703437805:   4%|▊                   | 2/46 [00:00<00:12,  3.58it/s]Epoch: 6, train for the 95-th batch, train loss: 0.17441943287849426:  79%|█████████▍  | 94/119 [00:56<00:14,  1.71it/s]Epoch: 6, train for the 95-th batch, train loss: 0.17441943287849426:  80%|█████████▌  | 95/119 [00:56<00:14,  1.68it/s]Epoch: 5, train for the 139-th batch, train loss: 0.5601140856742859:  95%|██████████▍| 138/146 [01:22<00:04,  1.61it/s]Epoch: 5, train for the 139-th batch, train loss: 0.5601140856742859:  95%|██████████▍| 139/146 [01:22<00:04,  1.62it/s]evaluate for the 9-th batch, evaluate loss: 0.6874615550041199:  20%|████                | 8/40 [00:02<00:09,  3.39it/s]evaluate for the 9-th batch, evaluate loss: 0.6874615550041199:  22%|████▌               | 9/40 [00:02<00:08,  3.47it/s]evaluate for the 3-th batch, evaluate loss: 0.4797888398170471:   4%|▊                   | 2/46 [00:00<00:12,  3.58it/s]evaluate for the 3-th batch, evaluate loss: 0.4797888398170471:   7%|█▎                  | 3/46 [00:00<00:11,  3.65it/s]Epoch: 2, train for the 363-th batch, train loss: 0.48234865069389343:  95%|█████████▍| 362/383 [03:39<00:12,  1.69it/s]Epoch: 2, train for the 363-th batch, train loss: 0.48234865069389343:  95%|█████████▍| 363/383 [03:39<00:11,  1.70it/s]evaluate for the 10-th batch, evaluate loss: 0.6851354241371155:  22%|████▎              | 9/40 [00:02<00:08,  3.47it/s]evaluate for the 10-th batch, evaluate loss: 0.6851354241371155:  25%|████▌             | 10/40 [00:02<00:08,  3.40it/s]evaluate for the 4-th batch, evaluate loss: 0.5100887417793274:   7%|█▎                  | 3/46 [00:01<00:11,  3.65it/s]evaluate for the 4-th batch, evaluate loss: 0.5100887417793274:   9%|█▋                  | 4/46 [00:01<00:11,  3.81it/s]Epoch: 6, train for the 96-th batch, train loss: 0.18804959952831268:  80%|█████████▌  | 95/119 [00:57<00:14,  1.68it/s]Epoch: 6, train for the 96-th batch, train loss: 0.18804959952831268:  81%|█████████▋  | 96/119 [00:57<00:13,  1.66it/s]evaluate for the 5-th batch, evaluate loss: 0.4817308783531189:   9%|█▋                  | 4/46 [00:01<00:11,  3.81it/s]evaluate for the 5-th batch, evaluate loss: 0.4817308783531189:  11%|██▏                 | 5/46 [00:01<00:10,  3.80it/s]evaluate for the 11-th batch, evaluate loss: 0.6927117705345154:  25%|████▌             | 10/40 [00:03<00:08,  3.40it/s]evaluate for the 11-th batch, evaluate loss: 0.6927117705345154:  28%|████▉             | 11/40 [00:03<00:08,  3.48it/s]Epoch: 5, train for the 140-th batch, train loss: 0.5339784622192383:  95%|██████████▍| 139/146 [01:23<00:04,  1.62it/s]Epoch: 5, train for the 140-th batch, train loss: 0.5339784622192383:  96%|██████████▌| 140/146 [01:23<00:03,  1.62it/s]evaluate for the 6-th batch, evaluate loss: 0.5485027432441711:  11%|██▏                 | 5/46 [00:01<00:10,  3.80it/s]evaluate for the 6-th batch, evaluate loss: 0.5485027432441711:  13%|██▌                 | 6/46 [00:01<00:10,  3.79it/s]evaluate for the 12-th batch, evaluate loss: 0.7388758063316345:  28%|████▉             | 11/40 [00:03<00:08,  3.48it/s]evaluate for the 12-th batch, evaluate loss: 0.7388758063316345:  30%|█████▍            | 12/40 [00:03<00:08,  3.44it/s]Epoch: 2, train for the 364-th batch, train loss: 0.4263854920864105:  95%|██████████▍| 363/383 [03:40<00:11,  1.70it/s]Epoch: 2, train for the 364-th batch, train loss: 0.4263854920864105:  95%|██████████▍| 364/383 [03:40<00:11,  1.71it/s]Epoch: 6, train for the 97-th batch, train loss: 0.16535325348377228:  81%|█████████▋  | 96/119 [00:58<00:13,  1.66it/s]Epoch: 6, train for the 97-th batch, train loss: 0.16535325348377228:  82%|█████████▊  | 97/119 [00:58<00:13,  1.65it/s]evaluate for the 7-th batch, evaluate loss: 0.47589877247810364:  13%|██▍                | 6/46 [00:01<00:10,  3.79it/s]evaluate for the 7-th batch, evaluate loss: 0.47589877247810364:  15%|██▉                | 7/46 [00:01<00:10,  3.89it/s]evaluate for the 13-th batch, evaluate loss: 0.6947553157806396:  30%|█████▍            | 12/40 [00:03<00:08,  3.44it/s]evaluate for the 13-th batch, evaluate loss: 0.6947553157806396:  32%|█████▊            | 13/40 [00:03<00:07,  3.56it/s]Epoch: 5, train for the 141-th batch, train loss: 0.5666487216949463:  96%|██████████▌| 140/146 [01:23<00:03,  1.62it/s]Epoch: 5, train for the 141-th batch, train loss: 0.5666487216949463:  97%|██████████▌| 141/146 [01:23<00:03,  1.62it/s]evaluate for the 8-th batch, evaluate loss: 0.5579399466514587:  15%|███                 | 7/46 [00:02<00:10,  3.89it/s]evaluate for the 8-th batch, evaluate loss: 0.5579399466514587:  17%|███▍                | 8/46 [00:02<00:10,  3.68it/s]evaluate for the 14-th batch, evaluate loss: 0.7281556725502014:  32%|█████▊            | 13/40 [00:04<00:07,  3.56it/s]evaluate for the 14-th batch, evaluate loss: 0.7281556725502014:  35%|██████▎           | 14/40 [00:04<00:07,  3.49it/s]Epoch: 2, train for the 365-th batch, train loss: 0.417602002620697:  95%|███████████▍| 364/383 [03:40<00:11,  1.71it/s]Epoch: 2, train for the 365-th batch, train loss: 0.417602002620697:  95%|███████████▍| 365/383 [03:40<00:10,  1.72it/s]evaluate for the 9-th batch, evaluate loss: 0.5246984958648682:  17%|███▍                | 8/46 [00:02<00:10,  3.68it/s]evaluate for the 9-th batch, evaluate loss: 0.5246984958648682:  20%|███▉                | 9/46 [00:02<00:09,  3.86it/s]Epoch: 6, train for the 98-th batch, train loss: 0.12026920169591904:  82%|█████████▊  | 97/119 [00:58<00:13,  1.65it/s]Epoch: 6, train for the 98-th batch, train loss: 0.12026920169591904:  82%|█████████▉  | 98/119 [00:58<00:12,  1.64it/s]evaluate for the 15-th batch, evaluate loss: 0.73834228515625:  35%|███████             | 14/40 [00:04<00:07,  3.49it/s]evaluate for the 15-th batch, evaluate loss: 0.73834228515625:  38%|███████▌            | 15/40 [00:04<00:06,  3.60it/s]Epoch: 5, train for the 142-th batch, train loss: 0.488971084356308:  97%|███████████▌| 141/146 [01:24<00:03,  1.62it/s]Epoch: 5, train for the 142-th batch, train loss: 0.488971084356308:  97%|███████████▋| 142/146 [01:24<00:02,  1.62it/s]evaluate for the 10-th batch, evaluate loss: 0.531922459602356:  20%|███▉                | 9/46 [00:02<00:09,  3.86it/s]evaluate for the 10-th batch, evaluate loss: 0.531922459602356:  22%|████▏              | 10/46 [00:02<00:09,  3.69it/s]evaluate for the 16-th batch, evaluate loss: 0.6988523602485657:  38%|██████▊           | 15/40 [00:04<00:06,  3.60it/s]evaluate for the 16-th batch, evaluate loss: 0.6988523602485657:  40%|███████▏          | 16/40 [00:04<00:06,  3.52it/s]Epoch: 2, train for the 366-th batch, train loss: 0.47029757499694824:  95%|█████████▌| 365/383 [03:41<00:10,  1.72it/s]Epoch: 2, train for the 366-th batch, train loss: 0.47029757499694824:  96%|█████████▌| 366/383 [03:41<00:09,  1.73it/s]evaluate for the 11-th batch, evaluate loss: 0.5202842354774475:  22%|███▉              | 10/46 [00:02<00:09,  3.69it/s]evaluate for the 11-th batch, evaluate loss: 0.5202842354774475:  24%|████▎             | 11/46 [00:02<00:09,  3.87it/s]evaluate for the 17-th batch, evaluate loss: 0.6897493600845337:  40%|███████▏          | 16/40 [00:04<00:06,  3.52it/s]evaluate for the 17-th batch, evaluate loss: 0.6897493600845337:  42%|███████▋          | 17/40 [00:04<00:06,  3.62it/s]Epoch: 6, train for the 99-th batch, train loss: 0.1742965131998062:  82%|██████████▋  | 98/119 [00:59<00:12,  1.64it/s]Epoch: 6, train for the 99-th batch, train loss: 0.1742965131998062:  83%|██████████▊  | 99/119 [00:59<00:12,  1.64it/s]evaluate for the 12-th batch, evaluate loss: 0.47510993480682373:  24%|████             | 11/46 [00:03<00:09,  3.87it/s]evaluate for the 12-th batch, evaluate loss: 0.47510993480682373:  26%|████▍            | 12/46 [00:03<00:08,  3.84it/s]Epoch: 5, train for the 143-th batch, train loss: 0.5420715808868408:  97%|██████████▋| 142/146 [01:24<00:02,  1.62it/s]Epoch: 5, train for the 143-th batch, train loss: 0.5420715808868408:  98%|██████████▊| 143/146 [01:24<00:01,  1.62it/s]evaluate for the 18-th batch, evaluate loss: 0.7026166319847107:  42%|███████▋          | 17/40 [00:05<00:06,  3.62it/s]evaluate for the 18-th batch, evaluate loss: 0.7026166319847107:  45%|████████          | 18/40 [00:05<00:06,  3.53it/s]Epoch: 2, train for the 367-th batch, train loss: 0.43129098415374756:  96%|█████████▌| 366/383 [03:42<00:09,  1.73it/s]Epoch: 2, train for the 367-th batch, train loss: 0.43129098415374756:  96%|█████████▌| 367/383 [03:42<00:09,  1.75it/s]evaluate for the 13-th batch, evaluate loss: 0.49293139576911926:  26%|████▍            | 12/46 [00:03<00:08,  3.84it/s]evaluate for the 13-th batch, evaluate loss: 0.49293139576911926:  28%|████▊            | 13/46 [00:03<00:08,  3.69it/s]evaluate for the 19-th batch, evaluate loss: 0.7075942754745483:  45%|████████          | 18/40 [00:05<00:06,  3.53it/s]evaluate for the 19-th batch, evaluate loss: 0.7075942754745483:  48%|████████▌         | 19/40 [00:05<00:05,  3.62it/s]Epoch: 6, train for the 100-th batch, train loss: 0.1969916671514511:  83%|█████████▉  | 99/119 [01:00<00:12,  1.64it/s]Epoch: 6, train for the 100-th batch, train loss: 0.1969916671514511:  84%|█████████▏ | 100/119 [01:00<00:11,  1.63it/s]evaluate for the 14-th batch, evaluate loss: 0.5873731970787048:  28%|█████             | 13/46 [00:03<00:08,  3.69it/s]evaluate for the 14-th batch, evaluate loss: 0.5873731970787048:  30%|█████▍            | 14/46 [00:03<00:08,  3.95it/s]Epoch: 5, train for the 144-th batch, train loss: 0.5333117842674255:  98%|██████████▊| 143/146 [01:25<00:01,  1.62it/s]Epoch: 5, train for the 144-th batch, train loss: 0.5333117842674255:  99%|██████████▊| 144/146 [01:25<00:01,  1.62it/s]evaluate for the 20-th batch, evaluate loss: 0.7445193529129028:  48%|████████▌         | 19/40 [00:05<00:05,  3.62it/s]evaluate for the 20-th batch, evaluate loss: 0.7445193529129028:  50%|█████████         | 20/40 [00:05<00:05,  3.55it/s]Epoch: 2, train for the 368-th batch, train loss: 0.4945153594017029:  96%|██████████▌| 367/383 [03:42<00:09,  1.75it/s]Epoch: 2, train for the 368-th batch, train loss: 0.4945153594017029:  96%|██████████▌| 368/383 [03:42<00:08,  1.75it/s]evaluate for the 15-th batch, evaluate loss: 0.5392181873321533:  30%|█████▍            | 14/46 [00:04<00:08,  3.95it/s]evaluate for the 15-th batch, evaluate loss: 0.5392181873321533:  33%|█████▊            | 15/46 [00:04<00:08,  3.69it/s]evaluate for the 21-th batch, evaluate loss: 0.7280470132827759:  50%|█████████         | 20/40 [00:05<00:05,  3.55it/s]evaluate for the 21-th batch, evaluate loss: 0.7280470132827759:  52%|█████████▍        | 21/40 [00:05<00:05,  3.64it/s]evaluate for the 16-th batch, evaluate loss: 0.5703349113464355:  33%|█████▊            | 15/46 [00:04<00:08,  3.69it/s]evaluate for the 16-th batch, evaluate loss: 0.5703349113464355:  35%|██████▎           | 16/46 [00:04<00:07,  3.77it/s]Epoch: 6, train for the 101-th batch, train loss: 0.16446882486343384:  84%|████████▍ | 100/119 [01:00<00:11,  1.63it/s]Epoch: 6, train for the 101-th batch, train loss: 0.16446882486343384:  85%|████████▍ | 101/119 [01:00<00:11,  1.63it/s]Epoch: 5, train for the 145-th batch, train loss: 0.5581749081611633:  99%|██████████▊| 144/146 [01:26<00:01,  1.62it/s]Epoch: 5, train for the 145-th batch, train loss: 0.5581749081611633:  99%|██████████▉| 145/146 [01:26<00:00,  1.63it/s]evaluate for the 22-th batch, evaluate loss: 0.7515391111373901:  52%|█████████▍        | 21/40 [00:06<00:05,  3.64it/s]evaluate for the 22-th batch, evaluate loss: 0.7515391111373901:  55%|█████████▉        | 22/40 [00:06<00:05,  3.56it/s]Epoch: 2, train for the 369-th batch, train loss: 0.38522040843963623:  96%|█████████▌| 368/383 [03:43<00:08,  1.75it/s]Epoch: 2, train for the 369-th batch, train loss: 0.38522040843963623:  96%|█████████▋| 369/383 [03:43<00:07,  1.75it/s]evaluate for the 17-th batch, evaluate loss: 0.45159900188446045:  35%|█████▉           | 16/46 [00:04<00:07,  3.77it/s]evaluate for the 17-th batch, evaluate loss: 0.45159900188446045:  37%|██████▎          | 17/46 [00:04<00:07,  3.76it/s]evaluate for the 23-th batch, evaluate loss: 0.6671295166015625:  55%|█████████▉        | 22/40 [00:06<00:05,  3.56it/s]evaluate for the 23-th batch, evaluate loss: 0.6671295166015625:  57%|██████████▎       | 23/40 [00:06<00:04,  3.64it/s]evaluate for the 18-th batch, evaluate loss: 0.4981755018234253:  37%|██████▋           | 17/46 [00:04<00:07,  3.76it/s]evaluate for the 18-th batch, evaluate loss: 0.4981755018234253:  39%|███████           | 18/46 [00:04<00:07,  3.89it/s]Epoch: 5, train for the 146-th batch, train loss: 0.48582136631011963:  99%|█████████▉| 145/146 [01:26<00:00,  1.63it/s]Epoch: 6, train for the 102-th batch, train loss: 0.1879388988018036:  85%|█████████▎ | 101/119 [01:01<00:11,  1.63it/s]Epoch: 5, train for the 146-th batch, train loss: 0.48582136631011963: 100%|██████████| 146/146 [01:26<00:00,  1.73it/s]Epoch: 6, train for the 102-th batch, train loss: 0.1879388988018036:  86%|█████████▍ | 102/119 [01:01<00:10,  1.61it/s]Epoch: 5, train for the 146-th batch, train loss: 0.48582136631011963: 100%|██████████| 146/146 [01:26<00:00,  1.69it/s]
  0%|                                                                                            | 0/38 [00:00<?, ?it/s]evaluate for the 24-th batch, evaluate loss: 0.6962423324584961:  57%|██████████▎       | 23/40 [00:06<00:04,  3.64it/s]evaluate for the 24-th batch, evaluate loss: 0.6962423324584961:  60%|██████████▊       | 24/40 [00:06<00:04,  3.56it/s]evaluate for the 19-th batch, evaluate loss: 0.5180179476737976:  39%|███████           | 18/46 [00:05<00:07,  3.89it/s]evaluate for the 19-th batch, evaluate loss: 0.5180179476737976:  41%|███████▍          | 19/46 [00:05<00:06,  3.86it/s]Epoch: 2, train for the 370-th batch, train loss: 0.42189672589302063:  96%|█████████▋| 369/383 [03:43<00:07,  1.75it/s]Epoch: 2, train for the 370-th batch, train loss: 0.42189672589302063:  97%|█████████▋| 370/383 [03:43<00:07,  1.75it/s]evaluate for the 1-th batch, evaluate loss: 0.4880143105983734:   0%|                            | 0/38 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.4880143105983734:   3%|▌                   | 1/38 [00:00<00:10,  3.69it/s]evaluate for the 25-th batch, evaluate loss: 0.6795675754547119:  60%|██████████▊       | 24/40 [00:07<00:04,  3.56it/s]evaluate for the 25-th batch, evaluate loss: 0.6795675754547119:  62%|███████████▎      | 25/40 [00:07<00:04,  3.64it/s]evaluate for the 20-th batch, evaluate loss: 0.5350674986839294:  41%|███████▍          | 19/46 [00:05<00:06,  3.86it/s]evaluate for the 20-th batch, evaluate loss: 0.5350674986839294:  43%|███████▊          | 20/46 [00:05<00:07,  3.69it/s]evaluate for the 2-th batch, evaluate loss: 0.4994470179080963:   3%|▌                   | 1/38 [00:00<00:10,  3.69it/s]evaluate for the 2-th batch, evaluate loss: 0.4994470179080963:   5%|█                   | 2/38 [00:00<00:10,  3.50it/s]Epoch: 6, train for the 103-th batch, train loss: 0.1882868856191635:  86%|█████████▍ | 102/119 [01:01<00:10,  1.61it/s]Epoch: 6, train for the 103-th batch, train loss: 0.1882868856191635:  87%|█████████▌ | 103/119 [01:01<00:09,  1.63it/s]evaluate for the 21-th batch, evaluate loss: 0.5268754363059998:  43%|███████▊          | 20/46 [00:05<00:07,  3.69it/s]evaluate for the 21-th batch, evaluate loss: 0.5268754363059998:  46%|████████▏         | 21/46 [00:05<00:06,  3.96it/s]evaluate for the 26-th batch, evaluate loss: 0.684317946434021:  62%|███████████▉       | 25/40 [00:07<00:04,  3.64it/s]evaluate for the 26-th batch, evaluate loss: 0.684317946434021:  65%|████████████▎      | 26/40 [00:07<00:03,  3.57it/s]Epoch: 2, train for the 371-th batch, train loss: 0.4037579298019409:  97%|██████████▋| 370/383 [03:44<00:07,  1.75it/s]Epoch: 2, train for the 371-th batch, train loss: 0.4037579298019409:  97%|██████████▋| 371/383 [03:44<00:06,  1.75it/s]evaluate for the 3-th batch, evaluate loss: 0.4992958605289459:   5%|█                   | 2/38 [00:00<00:10,  3.50it/s]evaluate for the 3-th batch, evaluate loss: 0.4992958605289459:   8%|█▌                  | 3/38 [00:00<00:09,  3.68it/s]evaluate for the 27-th batch, evaluate loss: 0.6978869438171387:  65%|███████████▋      | 26/40 [00:07<00:03,  3.57it/s]evaluate for the 27-th batch, evaluate loss: 0.6978869438171387:  68%|████████████▏     | 27/40 [00:07<00:03,  3.62it/s]evaluate for the 22-th batch, evaluate loss: 0.5206244587898254:  46%|████████▏         | 21/46 [00:05<00:06,  3.96it/s]evaluate for the 22-th batch, evaluate loss: 0.5206244587898254:  48%|████████▌         | 22/46 [00:05<00:06,  3.71it/s]evaluate for the 4-th batch, evaluate loss: 0.48720961809158325:   8%|█▌                 | 3/38 [00:01<00:09,  3.68it/s]evaluate for the 4-th batch, evaluate loss: 0.48720961809158325:  11%|██                 | 4/38 [00:01<00:09,  3.58it/s]evaluate for the 23-th batch, evaluate loss: 0.47107017040252686:  48%|████████▏        | 22/46 [00:06<00:06,  3.71it/s]evaluate for the 23-th batch, evaluate loss: 0.47107017040252686:  50%|████████▌        | 23/46 [00:06<00:06,  3.82it/s]evaluate for the 28-th batch, evaluate loss: 0.7091477513313293:  68%|████████████▏     | 27/40 [00:07<00:03,  3.62it/s]evaluate for the 28-th batch, evaluate loss: 0.7091477513313293:  70%|████████████▌     | 28/40 [00:07<00:03,  3.56it/s]Epoch: 6, train for the 104-th batch, train loss: 0.22004550695419312:  87%|████████▋ | 103/119 [01:02<00:09,  1.63it/s]Epoch: 6, train for the 104-th batch, train loss: 0.22004550695419312:  87%|████████▋ | 104/119 [01:02<00:09,  1.64it/s]Epoch: 2, train for the 372-th batch, train loss: 0.49754324555397034:  97%|█████████▋| 371/383 [03:44<00:06,  1.75it/s]Epoch: 2, train for the 372-th batch, train loss: 0.49754324555397034:  97%|█████████▋| 372/383 [03:44<00:06,  1.73it/s]evaluate for the 5-th batch, evaluate loss: 0.5165220499038696:  11%|██                  | 4/38 [00:01<00:09,  3.58it/s]evaluate for the 5-th batch, evaluate loss: 0.5165220499038696:  13%|██▋                 | 5/38 [00:01<00:08,  3.68it/s]evaluate for the 24-th batch, evaluate loss: 0.47996291518211365:  50%|████████▌        | 23/46 [00:06<00:06,  3.82it/s]evaluate for the 24-th batch, evaluate loss: 0.47996291518211365:  52%|████████▊        | 24/46 [00:06<00:05,  3.79it/s]evaluate for the 29-th batch, evaluate loss: 0.7291151285171509:  70%|████████████▌     | 28/40 [00:08<00:03,  3.56it/s]evaluate for the 29-th batch, evaluate loss: 0.7291151285171509:  72%|█████████████     | 29/40 [00:08<00:03,  3.54it/s]evaluate for the 6-th batch, evaluate loss: 0.5000499486923218:  13%|██▋                 | 5/38 [00:01<00:08,  3.68it/s]evaluate for the 6-th batch, evaluate loss: 0.5000499486923218:  16%|███▏                | 6/38 [00:01<00:07,  4.15it/s]evaluate for the 7-th batch, evaluate loss: 0.4694227874279022:  16%|███▏                | 6/38 [00:01<00:07,  4.15it/s]evaluate for the 7-th batch, evaluate loss: 0.4694227874279022:  18%|███▋                | 7/38 [00:01<00:06,  5.13it/s]evaluate for the 25-th batch, evaluate loss: 0.5330031514167786:  52%|█████████▍        | 24/46 [00:06<00:05,  3.79it/s]evaluate for the 25-th batch, evaluate loss: 0.5330031514167786:  54%|█████████▊        | 25/46 [00:06<00:05,  3.93it/s]evaluate for the 30-th batch, evaluate loss: 0.7244188785552979:  72%|█████████████     | 29/40 [00:08<00:03,  3.54it/s]evaluate for the 30-th batch, evaluate loss: 0.7244188785552979:  75%|█████████████▌    | 30/40 [00:08<00:02,  3.50it/s]evaluate for the 8-th batch, evaluate loss: 0.5436769723892212:  18%|███▋                | 7/38 [00:01<00:06,  5.13it/s]evaluate for the 8-th batch, evaluate loss: 0.5436769723892212:  21%|████▏               | 8/38 [00:01<00:06,  4.82it/s]Epoch: 2, train for the 373-th batch, train loss: 0.5138760805130005:  97%|██████████▋| 372/383 [03:45<00:06,  1.73it/s]Epoch: 2, train for the 373-th batch, train loss: 0.5138760805130005:  97%|██████████▋| 373/383 [03:45<00:05,  1.71it/s]evaluate for the 26-th batch, evaluate loss: 0.5531957745552063:  54%|█████████▊        | 25/46 [00:06<00:05,  3.93it/s]evaluate for the 26-th batch, evaluate loss: 0.5531957745552063:  57%|██████████▏       | 26/46 [00:06<00:05,  3.88it/s]evaluate for the 9-th batch, evaluate loss: 0.5122680068016052:  21%|████▏               | 8/38 [00:01<00:06,  4.82it/s]evaluate for the 9-th batch, evaluate loss: 0.5122680068016052:  24%|████▋               | 9/38 [00:01<00:05,  5.75it/s]evaluate for the 31-th batch, evaluate loss: 0.690264880657196:  75%|██████████████▎    | 30/40 [00:08<00:02,  3.50it/s]evaluate for the 31-th batch, evaluate loss: 0.690264880657196:  78%|██████████████▋    | 31/40 [00:08<00:02,  3.45it/s]evaluate for the 10-th batch, evaluate loss: 0.5308541059494019:  24%|████▌              | 9/38 [00:02<00:05,  5.75it/s]evaluate for the 10-th batch, evaluate loss: 0.5308541059494019:  26%|████▋             | 10/38 [00:02<00:04,  6.52it/s]evaluate for the 27-th batch, evaluate loss: 0.4973562955856323:  57%|██████████▏       | 26/46 [00:07<00:05,  3.88it/s]evaluate for the 27-th batch, evaluate loss: 0.4973562955856323:  59%|██████████▌       | 27/46 [00:07<00:05,  3.70it/s]Epoch: 6, train for the 105-th batch, train loss: 0.18686647713184357:  87%|████████▋ | 104/119 [01:03<00:09,  1.64it/s]Epoch: 6, train for the 105-th batch, train loss: 0.18686647713184357:  88%|████████▊ | 105/119 [01:03<00:10,  1.34it/s]evaluate for the 11-th batch, evaluate loss: 0.5144185423851013:  26%|████▋             | 10/38 [00:02<00:04,  6.52it/s]evaluate for the 11-th batch, evaluate loss: 0.5144185423851013:  29%|█████▏            | 11/38 [00:02<00:04,  5.53it/s]evaluate for the 32-th batch, evaluate loss: 0.7303736805915833:  78%|█████████████▉    | 31/40 [00:09<00:02,  3.45it/s]evaluate for the 32-th batch, evaluate loss: 0.7303736805915833:  80%|██████████████▍   | 32/40 [00:09<00:02,  3.42it/s]evaluate for the 28-th batch, evaluate loss: 0.5179795622825623:  59%|██████████▌       | 27/46 [00:07<00:05,  3.70it/s]evaluate for the 28-th batch, evaluate loss: 0.5179795622825623:  61%|██████████▉       | 28/46 [00:07<00:04,  3.99it/s]Epoch: 2, train for the 374-th batch, train loss: 0.46873870491981506:  97%|█████████▋| 373/383 [03:46<00:05,  1.71it/s]Epoch: 2, train for the 374-th batch, train loss: 0.46873870491981506:  98%|█████████▊| 374/383 [03:46<00:05,  1.71it/s]evaluate for the 12-th batch, evaluate loss: 0.557929277420044:  29%|█████▌             | 11/38 [00:02<00:04,  5.53it/s]evaluate for the 12-th batch, evaluate loss: 0.557929277420044:  32%|██████             | 12/38 [00:02<00:05,  5.08it/s]evaluate for the 33-th batch, evaluate loss: 0.725837230682373:  80%|███████████████▏   | 32/40 [00:09<00:02,  3.42it/s]evaluate for the 33-th batch, evaluate loss: 0.725837230682373:  82%|███████████████▋   | 33/40 [00:09<00:02,  3.43it/s]evaluate for the 29-th batch, evaluate loss: 0.4917317032814026:  61%|██████████▉       | 28/46 [00:07<00:04,  3.99it/s]evaluate for the 29-th batch, evaluate loss: 0.4917317032814026:  63%|███████████▎      | 29/46 [00:07<00:04,  3.72it/s]Epoch: 6, train for the 106-th batch, train loss: 0.1354745328426361:  88%|█████████▋ | 105/119 [01:04<00:10,  1.34it/s]Epoch: 6, train for the 106-th batch, train loss: 0.1354745328426361:  89%|█████████▊ | 106/119 [01:04<00:09,  1.42it/s]evaluate for the 13-th batch, evaluate loss: 0.5612013936042786:  32%|█████▋            | 12/38 [00:02<00:05,  5.08it/s]evaluate for the 13-th batch, evaluate loss: 0.5612013936042786:  34%|██████▏           | 13/38 [00:02<00:05,  4.31it/s]evaluate for the 34-th batch, evaluate loss: 0.7211228013038635:  82%|██████████████▊   | 33/40 [00:09<00:02,  3.43it/s]evaluate for the 34-th batch, evaluate loss: 0.7211228013038635:  85%|███████████████▎  | 34/40 [00:09<00:01,  3.48it/s]evaluate for the 30-th batch, evaluate loss: 0.49184921383857727:  63%|██████████▋      | 29/46 [00:07<00:04,  3.72it/s]evaluate for the 30-th batch, evaluate loss: 0.49184921383857727:  65%|███████████      | 30/46 [00:07<00:04,  3.81it/s]Epoch: 2, train for the 375-th batch, train loss: 0.5171123147010803:  98%|██████████▋| 374/383 [03:46<00:05,  1.71it/s]Epoch: 2, train for the 375-th batch, train loss: 0.5171123147010803:  98%|██████████▊| 375/383 [03:46<00:04,  1.71it/s]evaluate for the 14-th batch, evaluate loss: 0.5002070665359497:  34%|██████▏           | 13/38 [00:03<00:05,  4.31it/s]evaluate for the 14-th batch, evaluate loss: 0.5002070665359497:  37%|██████▋           | 14/38 [00:03<00:05,  4.12it/s]evaluate for the 35-th batch, evaluate loss: 0.7192422151565552:  85%|███████████████▎  | 34/40 [00:09<00:01,  3.48it/s]evaluate for the 35-th batch, evaluate loss: 0.7192422151565552:  88%|███████████████▊  | 35/40 [00:09<00:01,  3.42it/s]evaluate for the 31-th batch, evaluate loss: 0.5212337970733643:  65%|███████████▋      | 30/46 [00:08<00:04,  3.81it/s]evaluate for the 31-th batch, evaluate loss: 0.5212337970733643:  67%|████████████▏     | 31/46 [00:08<00:03,  3.78it/s]evaluate for the 15-th batch, evaluate loss: 0.5128127336502075:  37%|██████▋           | 14/38 [00:03<00:05,  4.12it/s]evaluate for the 15-th batch, evaluate loss: 0.5128127336502075:  39%|███████           | 15/38 [00:03<00:06,  3.78it/s]Epoch: 6, train for the 107-th batch, train loss: 0.22951149940490723:  89%|████████▉ | 106/119 [01:04<00:09,  1.42it/s]Epoch: 6, train for the 107-th batch, train loss: 0.22951149940490723:  90%|████████▉ | 107/119 [01:04<00:08,  1.48it/s]evaluate for the 36-th batch, evaluate loss: 0.7538033127784729:  88%|███████████████▊  | 35/40 [00:10<00:01,  3.42it/s]evaluate for the 36-th batch, evaluate loss: 0.7538033127784729:  90%|████████████████▏ | 36/40 [00:10<00:01,  3.52it/s]evaluate for the 32-th batch, evaluate loss: 0.47979840636253357:  67%|███████████▍     | 31/46 [00:08<00:03,  3.78it/s]evaluate for the 32-th batch, evaluate loss: 0.47979840636253357:  70%|███████████▊     | 32/46 [00:08<00:03,  3.87it/s]Epoch: 2, train for the 376-th batch, train loss: 0.40261512994766235:  98%|█████████▊| 375/383 [03:47<00:04,  1.71it/s]Epoch: 2, train for the 376-th batch, train loss: 0.40261512994766235:  98%|█████████▊| 376/383 [03:47<00:04,  1.70it/s]evaluate for the 16-th batch, evaluate loss: 0.5347185134887695:  39%|███████           | 15/38 [00:03<00:06,  3.78it/s]evaluate for the 16-th batch, evaluate loss: 0.5347185134887695:  42%|███████▌          | 16/38 [00:03<00:05,  3.79it/s]evaluate for the 33-th batch, evaluate loss: 0.4947667717933655:  70%|████████████▌     | 32/46 [00:08<00:03,  3.87it/s]evaluate for the 33-th batch, evaluate loss: 0.4947667717933655:  72%|████████████▉     | 33/46 [00:08<00:03,  3.84it/s]evaluate for the 37-th batch, evaluate loss: 0.7522308230400085:  90%|████████████████▏ | 36/40 [00:10<00:01,  3.52it/s]evaluate for the 37-th batch, evaluate loss: 0.7522308230400085:  92%|████████████████▋ | 37/40 [00:10<00:00,  3.45it/s]evaluate for the 17-th batch, evaluate loss: 0.502274751663208:  42%|████████           | 16/38 [00:04<00:05,  3.79it/s]evaluate for the 17-th batch, evaluate loss: 0.502274751663208:  45%|████████▌          | 17/38 [00:04<00:05,  3.67it/s]evaluate for the 38-th batch, evaluate loss: 0.7715873718261719:  92%|████████████████▋ | 37/40 [00:10<00:00,  3.45it/s]evaluate for the 38-th batch, evaluate loss: 0.7715873718261719:  95%|█████████████████ | 38/40 [00:10<00:00,  3.58it/s]evaluate for the 34-th batch, evaluate loss: 0.4784911274909973:  72%|████████████▉     | 33/46 [00:08<00:03,  3.84it/s]evaluate for the 34-th batch, evaluate loss: 0.4784911274909973:  74%|█████████████▎    | 34/46 [00:08<00:03,  3.71it/s]Epoch: 6, train for the 108-th batch, train loss: 0.11521705240011215:  90%|████████▉ | 107/119 [01:05<00:08,  1.48it/s]Epoch: 6, train for the 108-th batch, train loss: 0.11521705240011215:  91%|█████████ | 108/119 [01:05<00:07,  1.53it/s]Epoch: 2, train for the 377-th batch, train loss: 0.494812548160553:  98%|███████████▊| 376/383 [03:47<00:04,  1.70it/s]Epoch: 2, train for the 377-th batch, train loss: 0.494812548160553:  98%|███████████▊| 377/383 [03:47<00:03,  1.72it/s]evaluate for the 35-th batch, evaluate loss: 0.4804624915122986:  74%|█████████████▎    | 34/46 [00:09<00:03,  3.71it/s]evaluate for the 35-th batch, evaluate loss: 0.4804624915122986:  76%|█████████████▋    | 35/46 [00:09<00:02,  3.95it/s]evaluate for the 18-th batch, evaluate loss: 0.5399385690689087:  45%|████████          | 17/38 [00:04<00:05,  3.67it/s]evaluate for the 18-th batch, evaluate loss: 0.5399385690689087:  47%|████████▌         | 18/38 [00:04<00:05,  3.74it/s]evaluate for the 39-th batch, evaluate loss: 0.7376304268836975:  95%|█████████████████ | 38/40 [00:11<00:00,  3.58it/s]evaluate for the 39-th batch, evaluate loss: 0.7376304268836975:  98%|█████████████████▌| 39/40 [00:11<00:00,  3.49it/s]evaluate for the 19-th batch, evaluate loss: 0.5066910982131958:  47%|████████▌         | 18/38 [00:04<00:05,  3.74it/s]evaluate for the 19-th batch, evaluate loss: 0.5066910982131958:  50%|█████████         | 19/38 [00:04<00:05,  3.65it/s]evaluate for the 36-th batch, evaluate loss: 0.4707522392272949:  76%|█████████████▋    | 35/46 [00:09<00:02,  3.95it/s]evaluate for the 36-th batch, evaluate loss: 0.4707522392272949:  78%|██████████████    | 36/46 [00:09<00:02,  3.69it/s]evaluate for the 40-th batch, evaluate loss: 0.7348877787590027:  98%|█████████████████▌| 39/40 [00:11<00:00,  3.49it/s]evaluate for the 40-th batch, evaluate loss: 0.7348877787590027: 100%|██████████████████| 40/40 [00:11<00:00,  3.66it/s]evaluate for the 40-th batch, evaluate loss: 0.7348877787590027: 100%|██████████████████| 40/40 [00:11<00:00,  3.53it/s]
INFO:root:Epoch: 3, learning rate: 0.0001, train loss: 0.6114
INFO:root:train average_precision, 0.7511
INFO:root:train roc_auc, 0.7377
INFO:root:validate loss: 0.6179
INFO:root:validate average_precision, 0.7237
INFO:root:validate roc_auc, 0.7098
INFO:root:new node validate loss: 0.7102
INFO:root:new node validate first_1_average_precision, 0.5860
INFO:root:new node validate first_1_roc_auc, 0.5634
INFO:root:new node validate first_3_average_precision, 0.5897
INFO:root:new node validate first_3_roc_auc, 0.5804
INFO:root:new node validate first_10_average_precision, 0.6127
INFO:root:new node validate first_10_roc_auc, 0.6145
INFO:root:new node validate average_precision, 0.5830
INFO:root:new node validate roc_auc, 0.5751
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/237 [00:00<?, ?it/s]Epoch: 6, train for the 109-th batch, train loss: 0.2123383730649948:  91%|█████████▉ | 108/119 [01:05<00:07,  1.53it/s]Epoch: 6, train for the 109-th batch, train loss: 0.2123383730649948:  92%|██████████ | 109/119 [01:05<00:06,  1.57it/s]Epoch: 2, train for the 378-th batch, train loss: 0.5126321911811829:  98%|██████████▊| 377/383 [03:48<00:03,  1.72it/s]Epoch: 2, train for the 378-th batch, train loss: 0.5126321911811829:  99%|██████████▊| 378/383 [03:48<00:02,  1.74it/s]evaluate for the 20-th batch, evaluate loss: 0.4997026026248932:  50%|█████████         | 19/38 [00:04<00:05,  3.65it/s]evaluate for the 20-th batch, evaluate loss: 0.4997026026248932:  53%|█████████▍        | 20/38 [00:04<00:04,  3.69it/s]evaluate for the 37-th batch, evaluate loss: 0.5019860863685608:  78%|██████████████    | 36/46 [00:09<00:02,  3.69it/s]evaluate for the 37-th batch, evaluate loss: 0.5019860863685608:  80%|██████████████▍   | 37/46 [00:09<00:02,  3.79it/s]Epoch: 4, train for the 1-th batch, train loss: 1.0595976114273071:   0%|                       | 0/237 [00:00<?, ?it/s]Epoch: 4, train for the 1-th batch, train loss: 1.0595976114273071:   0%|               | 1/237 [00:00<01:37,  2.43it/s]evaluate for the 38-th batch, evaluate loss: 0.535685658454895:  80%|███████████████▎   | 37/46 [00:10<00:02,  3.79it/s]evaluate for the 38-th batch, evaluate loss: 0.535685658454895:  83%|███████████████▋   | 38/46 [00:10<00:02,  3.76it/s]evaluate for the 21-th batch, evaluate loss: 0.5047889351844788:  53%|█████████▍        | 20/38 [00:05<00:04,  3.69it/s]evaluate for the 21-th batch, evaluate loss: 0.5047889351844788:  55%|█████████▉        | 21/38 [00:05<00:04,  3.61it/s]Epoch: 6, train for the 110-th batch, train loss: 0.1824287325143814:  92%|██████████ | 109/119 [01:06<00:06,  1.57it/s]Epoch: 6, train for the 110-th batch, train loss: 0.1824287325143814:  92%|██████████▏| 110/119 [01:06<00:05,  1.60it/s]evaluate for the 39-th batch, evaluate loss: 0.530636727809906:  83%|███████████████▋   | 38/46 [00:10<00:02,  3.76it/s]evaluate for the 39-th batch, evaluate loss: 0.530636727809906:  85%|████████████████   | 39/46 [00:10<00:01,  3.89it/s]Epoch: 2, train for the 379-th batch, train loss: 0.40752050280570984:  99%|█████████▊| 378/383 [03:48<00:02,  1.74it/s]Epoch: 2, train for the 379-th batch, train loss: 0.40752050280570984:  99%|█████████▉| 379/383 [03:48<00:02,  1.75it/s]evaluate for the 22-th batch, evaluate loss: 0.5277067422866821:  55%|█████████▉        | 21/38 [00:05<00:04,  3.61it/s]evaluate for the 22-th batch, evaluate loss: 0.5277067422866821:  58%|██████████▍       | 22/38 [00:05<00:04,  3.54it/s]Epoch: 4, train for the 2-th batch, train loss: 0.7500345706939697:   0%|               | 1/237 [00:00<01:37,  2.43it/s]Epoch: 4, train for the 2-th batch, train loss: 0.7500345706939697:   1%|▏              | 2/237 [00:00<01:43,  2.28it/s]evaluate for the 40-th batch, evaluate loss: 0.46766507625579834:  85%|██████████████▍  | 39/46 [00:10<00:01,  3.89it/s]evaluate for the 40-th batch, evaluate loss: 0.46766507625579834:  87%|██████████████▊  | 40/46 [00:10<00:01,  3.86it/s]evaluate for the 23-th batch, evaluate loss: 0.5235413908958435:  58%|██████████▍       | 22/38 [00:05<00:04,  3.54it/s]evaluate for the 23-th batch, evaluate loss: 0.5235413908958435:  61%|██████████▉       | 23/38 [00:05<00:04,  3.58it/s]Epoch: 6, train for the 111-th batch, train loss: 0.18737109005451202:  92%|█████████▏| 110/119 [01:07<00:05,  1.60it/s]Epoch: 6, train for the 111-th batch, train loss: 0.18737109005451202:  93%|█████████▎| 111/119 [01:07<00:04,  1.61it/s]Epoch: 4, train for the 3-th batch, train loss: 0.6426076889038086:   1%|▏              | 2/237 [00:01<01:43,  2.28it/s]Epoch: 4, train for the 3-th batch, train loss: 0.6426076889038086:   1%|▏              | 3/237 [00:01<01:41,  2.31it/s]evaluate for the 41-th batch, evaluate loss: 0.48415809869766235:  87%|██████████████▊  | 40/46 [00:10<00:01,  3.86it/s]evaluate for the 41-th batch, evaluate loss: 0.48415809869766235:  89%|███████████████▏ | 41/46 [00:10<00:01,  3.69it/s]Epoch: 2, train for the 380-th batch, train loss: 0.4258548617362976:  99%|██████████▉| 379/383 [03:49<00:02,  1.75it/s]Epoch: 2, train for the 380-th batch, train loss: 0.4258548617362976:  99%|██████████▉| 380/383 [03:49<00:01,  1.72it/s]evaluate for the 24-th batch, evaluate loss: 0.5156362056732178:  61%|██████████▉       | 23/38 [00:05<00:04,  3.58it/s]evaluate for the 24-th batch, evaluate loss: 0.5156362056732178:  63%|███████████▎      | 24/38 [00:05<00:03,  3.50it/s]evaluate for the 42-th batch, evaluate loss: 0.47006532549858093:  89%|███████████████▏ | 41/46 [00:11<00:01,  3.69it/s]evaluate for the 42-th batch, evaluate loss: 0.47006532549858093:  91%|███████████████▌ | 42/46 [00:11<00:01,  3.98it/s]evaluate for the 25-th batch, evaluate loss: 0.5169095993041992:  63%|███████████▎      | 24/38 [00:06<00:03,  3.50it/s]evaluate for the 25-th batch, evaluate loss: 0.5169095993041992:  66%|███████████▊      | 25/38 [00:06<00:03,  3.65it/s]Epoch: 4, train for the 4-th batch, train loss: 0.527928352355957:   1%|▏               | 3/237 [00:01<01:41,  2.31it/s]Epoch: 4, train for the 4-th batch, train loss: 0.527928352355957:   2%|▎               | 4/237 [00:01<01:39,  2.35it/s]evaluate for the 43-th batch, evaluate loss: 0.53041672706604:  91%|██████████████████▎ | 42/46 [00:11<00:01,  3.98it/s]evaluate for the 43-th batch, evaluate loss: 0.53041672706604:  93%|██████████████████▋ | 43/46 [00:11<00:00,  3.69it/s]Epoch: 6, train for the 112-th batch, train loss: 0.12488462030887604:  93%|█████████▎| 111/119 [01:07<00:04,  1.61it/s]Epoch: 6, train for the 112-th batch, train loss: 0.12488462030887604:  94%|█████████▍| 112/119 [01:07<00:04,  1.63it/s]evaluate for the 26-th batch, evaluate loss: 0.5181781649589539:  66%|███████████▊      | 25/38 [00:06<00:03,  3.65it/s]evaluate for the 26-th batch, evaluate loss: 0.5181781649589539:  68%|████████████▎     | 26/38 [00:06<00:03,  3.54it/s]Epoch: 2, train for the 381-th batch, train loss: 0.5096328258514404:  99%|██████████▉| 380/383 [03:50<00:01,  1.72it/s]Epoch: 2, train for the 381-th batch, train loss: 0.5096328258514404:  99%|██████████▉| 381/383 [03:50<00:01,  1.73it/s]evaluate for the 44-th batch, evaluate loss: 0.5131248831748962:  93%|████████████████▊ | 43/46 [00:11<00:00,  3.69it/s]evaluate for the 44-th batch, evaluate loss: 0.5131248831748962:  96%|█████████████████▏| 44/46 [00:11<00:00,  3.78it/s]evaluate for the 27-th batch, evaluate loss: 0.545315682888031:  68%|█████████████      | 26/38 [00:06<00:03,  3.54it/s]evaluate for the 27-th batch, evaluate loss: 0.545315682888031:  71%|█████████████▌     | 27/38 [00:06<00:02,  3.71it/s]Epoch: 4, train for the 5-th batch, train loss: 0.6610884070396423:   2%|▎              | 4/237 [00:02<01:39,  2.35it/s]Epoch: 4, train for the 5-th batch, train loss: 0.6610884070396423:   2%|▎              | 5/237 [00:02<01:42,  2.26it/s]evaluate for the 45-th batch, evaluate loss: 0.4980977475643158:  96%|█████████████████▏| 44/46 [00:11<00:00,  3.78it/s]evaluate for the 45-th batch, evaluate loss: 0.4980977475643158:  98%|█████████████████▌| 45/46 [00:11<00:00,  3.79it/s]Epoch: 6, train for the 113-th batch, train loss: 0.1599826216697693:  94%|██████████▎| 112/119 [01:08<00:04,  1.63it/s]Epoch: 6, train for the 113-th batch, train loss: 0.1599826216697693:  95%|██████████▍| 113/119 [01:08<00:03,  1.63it/s]evaluate for the 28-th batch, evaluate loss: 0.5467156767845154:  71%|████████████▊     | 27/38 [00:07<00:02,  3.71it/s]evaluate for the 28-th batch, evaluate loss: 0.5467156767845154:  74%|█████████████▎    | 28/38 [00:07<00:02,  3.52it/s]Epoch: 2, train for the 382-th batch, train loss: 0.4865398406982422:  99%|██████████▉| 381/383 [03:50<00:01,  1.73it/s]Epoch: 2, train for the 382-th batch, train loss: 0.4865398406982422: 100%|██████████▉| 382/383 [03:50<00:00,  1.73it/s]evaluate for the 46-th batch, evaluate loss: 0.5046038627624512:  98%|█████████████████▌| 45/46 [00:12<00:00,  3.79it/s]evaluate for the 46-th batch, evaluate loss: 0.5046038627624512: 100%|██████████████████| 46/46 [00:12<00:00,  3.85it/s]evaluate for the 46-th batch, evaluate loss: 0.5046038627624512: 100%|██████████████████| 46/46 [00:12<00:00,  3.80it/s]
  0%|                                                                                            | 0/25 [00:00<?, ?it/s]Epoch: 4, train for the 6-th batch, train loss: 0.5999966859817505:   2%|▎              | 5/237 [00:02<01:42,  2.26it/s]Epoch: 4, train for the 6-th batch, train loss: 0.5999966859817505:   3%|▍              | 6/237 [00:02<01:44,  2.21it/s]evaluate for the 29-th batch, evaluate loss: 0.5136460661888123:  74%|█████████████▎    | 28/38 [00:07<00:02,  3.52it/s]evaluate for the 29-th batch, evaluate loss: 0.5136460661888123:  76%|█████████████▋    | 29/38 [00:07<00:02,  3.56it/s]evaluate for the 1-th batch, evaluate loss: 0.6379397511482239:   0%|                            | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6379397511482239:   4%|▊                   | 1/25 [00:00<00:05,  4.80it/s]evaluate for the 30-th batch, evaluate loss: 0.553087055683136:  76%|██████████████▌    | 29/38 [00:07<00:02,  3.56it/s]evaluate for the 30-th batch, evaluate loss: 0.553087055683136:  79%|███████████████    | 30/38 [00:07<00:02,  3.49it/s]Epoch: 6, train for the 114-th batch, train loss: 0.18374843895435333:  95%|█████████▍| 113/119 [01:08<00:03,  1.63it/s]Epoch: 6, train for the 114-th batch, train loss: 0.18374843895435333:  96%|█████████▌| 114/119 [01:08<00:03,  1.64it/s]evaluate for the 2-th batch, evaluate loss: 0.6498391628265381:   4%|▊                   | 1/25 [00:00<00:05,  4.80it/s]evaluate for the 2-th batch, evaluate loss: 0.6498391628265381:   8%|█▌                  | 2/25 [00:00<00:06,  3.64it/s]Epoch: 2, train for the 383-th batch, train loss: 0.42417141795158386: 100%|█████████▉| 382/383 [03:51<00:00,  1.73it/s]Epoch: 2, train for the 383-th batch, train loss: 0.42417141795158386: 100%|██████████| 383/383 [03:51<00:00,  1.71it/s]Epoch: 2, train for the 383-th batch, train loss: 0.42417141795158386: 100%|██████████| 383/383 [03:51<00:00,  1.66it/s]
  0%|                                                                                           | 0/106 [00:00<?, ?it/s]Epoch: 4, train for the 7-th batch, train loss: 0.693528950214386:   3%|▍               | 6/237 [00:03<01:44,  2.21it/s]Epoch: 4, train for the 7-th batch, train loss: 0.693528950214386:   3%|▍               | 7/237 [00:03<01:46,  2.17it/s]evaluate for the 31-th batch, evaluate loss: 0.5345773696899414:  79%|██████████████▏   | 30/38 [00:07<00:02,  3.49it/s]evaluate for the 31-th batch, evaluate loss: 0.5345773696899414:  82%|██████████████▋   | 31/38 [00:07<00:01,  3.62it/s]evaluate for the 3-th batch, evaluate loss: 0.687256395816803:   8%|█▋                   | 2/25 [00:00<00:06,  3.64it/s]evaluate for the 3-th batch, evaluate loss: 0.687256395816803:  12%|██▌                  | 3/25 [00:00<00:05,  3.76it/s]evaluate for the 1-th batch, evaluate loss: 0.45620197057724:   0%|                             | 0/106 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.45620197057724:   1%|▏                    | 1/106 [00:00<00:27,  3.76it/s]Epoch: 4, train for the 8-th batch, train loss: 0.6520876884460449:   3%|▍              | 7/237 [00:03<01:46,  2.17it/s]Epoch: 4, train for the 8-th batch, train loss: 0.6520876884460449:   3%|▌              | 8/237 [00:03<01:46,  2.16it/s]evaluate for the 32-th batch, evaluate loss: 0.5007030963897705:  82%|██████████████▋   | 31/38 [00:08<00:01,  3.62it/s]evaluate for the 32-th batch, evaluate loss: 0.5007030963897705:  84%|███████████████▏  | 32/38 [00:08<00:01,  3.56it/s]evaluate for the 4-th batch, evaluate loss: 0.6737335324287415:  12%|██▍                 | 3/25 [00:01<00:05,  3.76it/s]evaluate for the 4-th batch, evaluate loss: 0.6737335324287415:  16%|███▏                | 4/25 [00:01<00:05,  3.68it/s]evaluate for the 2-th batch, evaluate loss: 0.5562803149223328:   1%|▏                  | 1/106 [00:00<00:27,  3.76it/s]evaluate for the 2-th batch, evaluate loss: 0.5562803149223328:   2%|▎                  | 2/106 [00:00<00:28,  3.67it/s]Epoch: 6, train for the 115-th batch, train loss: 0.16946105659008026:  96%|█████████▌| 114/119 [01:09<00:03,  1.64it/s]Epoch: 6, train for the 115-th batch, train loss: 0.16946105659008026:  97%|█████████▋| 115/119 [01:09<00:02,  1.65it/s]evaluate for the 33-th batch, evaluate loss: 0.4983687400817871:  84%|███████████████▏  | 32/38 [00:08<00:01,  3.56it/s]evaluate for the 33-th batch, evaluate loss: 0.4983687400817871:  87%|███████████████▋  | 33/38 [00:08<00:01,  3.66it/s]evaluate for the 5-th batch, evaluate loss: 0.6685881018638611:  16%|███▏                | 4/25 [00:01<00:05,  3.68it/s]evaluate for the 5-th batch, evaluate loss: 0.6685881018638611:  20%|████                | 5/25 [00:01<00:05,  3.85it/s]evaluate for the 3-th batch, evaluate loss: 0.5088987350463867:   2%|▎                  | 2/106 [00:00<00:28,  3.67it/s]evaluate for the 3-th batch, evaluate loss: 0.5088987350463867:   3%|▌                  | 3/106 [00:00<00:27,  3.76it/s]Epoch: 4, train for the 9-th batch, train loss: 0.5889038443565369:   3%|▌              | 8/237 [00:04<01:46,  2.16it/s]Epoch: 4, train for the 9-th batch, train loss: 0.5889038443565369:   4%|▌              | 9/237 [00:04<01:45,  2.16it/s]evaluate for the 34-th batch, evaluate loss: 0.523678183555603:  87%|████████████████▌  | 33/38 [00:08<00:01,  3.66it/s]evaluate for the 34-th batch, evaluate loss: 0.523678183555603:  89%|█████████████████  | 34/38 [00:08<00:01,  3.63it/s]evaluate for the 6-th batch, evaluate loss: 0.7172367572784424:  20%|████                | 5/25 [00:01<00:05,  3.85it/s]evaluate for the 6-th batch, evaluate loss: 0.7172367572784424:  24%|████▊               | 6/25 [00:01<00:05,  3.80it/s]evaluate for the 4-th batch, evaluate loss: 0.4686215817928314:   3%|▌                  | 3/106 [00:01<00:27,  3.76it/s]evaluate for the 4-th batch, evaluate loss: 0.4686215817928314:   4%|▋                  | 4/106 [00:01<00:27,  3.65it/s]Epoch: 6, train for the 116-th batch, train loss: 0.12276075035333633:  97%|█████████▋| 115/119 [01:10<00:02,  1.65it/s]Epoch: 6, train for the 116-th batch, train loss: 0.12276075035333633:  97%|█████████▋| 116/119 [01:10<00:01,  1.65it/s]evaluate for the 35-th batch, evaluate loss: 0.5565432906150818:  89%|████████████████  | 34/38 [00:09<00:01,  3.63it/s]evaluate for the 35-th batch, evaluate loss: 0.5565432906150818:  92%|████████████████▌ | 35/38 [00:09<00:00,  3.57it/s]evaluate for the 7-th batch, evaluate loss: 0.7407780289649963:  24%|████▊               | 6/25 [00:01<00:05,  3.80it/s]evaluate for the 7-th batch, evaluate loss: 0.7407780289649963:  28%|█████▌              | 7/25 [00:01<00:04,  3.67it/s]evaluate for the 5-th batch, evaluate loss: 0.5674569606781006:   4%|▋                  | 4/106 [00:01<00:27,  3.65it/s]evaluate for the 5-th batch, evaluate loss: 0.5674569606781006:   5%|▉                  | 5/106 [00:01<00:28,  3.58it/s]Epoch: 4, train for the 10-th batch, train loss: 0.6063057780265808:   4%|▌             | 9/237 [00:04<01:45,  2.16it/s]Epoch: 4, train for the 10-th batch, train loss: 0.6063057780265808:   4%|▌            | 10/237 [00:04<01:46,  2.13it/s]evaluate for the 8-th batch, evaluate loss: 0.7237960696220398:  28%|█████▌              | 7/25 [00:02<00:04,  3.67it/s]evaluate for the 8-th batch, evaluate loss: 0.7237960696220398:  32%|██████▍             | 8/25 [00:02<00:04,  3.82it/s]evaluate for the 36-th batch, evaluate loss: 0.5803476572036743:  92%|████████████████▌ | 35/38 [00:09<00:00,  3.57it/s]evaluate for the 36-th batch, evaluate loss: 0.5803476572036743:  95%|█████████████████ | 36/38 [00:09<00:00,  3.59it/s]evaluate for the 6-th batch, evaluate loss: 0.5671928524971008:   5%|▉                  | 5/106 [00:01<00:28,  3.58it/s]evaluate for the 6-th batch, evaluate loss: 0.5671928524971008:   6%|█                  | 6/106 [00:01<00:27,  3.64it/s]Epoch: 6, train for the 117-th batch, train loss: 0.17858031392097473:  97%|█████████▋| 116/119 [01:10<00:01,  1.65it/s]Epoch: 6, train for the 117-th batch, train loss: 0.17858031392097473:  98%|█████████▊| 117/119 [01:10<00:01,  1.65it/s]evaluate for the 9-th batch, evaluate loss: 0.6995813846588135:  32%|██████▍             | 8/25 [00:02<00:04,  3.82it/s]evaluate for the 9-th batch, evaluate loss: 0.6995813846588135:  36%|███████▏            | 9/25 [00:02<00:04,  3.62it/s]evaluate for the 37-th batch, evaluate loss: 0.5124430060386658:  95%|█████████████████ | 36/38 [00:09<00:00,  3.59it/s]evaluate for the 37-th batch, evaluate loss: 0.5124430060386658:  97%|█████████████████▌| 37/38 [00:09<00:00,  3.51it/s]Epoch: 4, train for the 11-th batch, train loss: 0.5690892934799194:   4%|▌            | 10/237 [00:05<01:46,  2.13it/s]Epoch: 4, train for the 11-th batch, train loss: 0.5690892934799194:   5%|▌            | 11/237 [00:05<01:46,  2.12it/s]evaluate for the 7-th batch, evaluate loss: 0.5274724960327148:   6%|█                  | 6/106 [00:01<00:27,  3.64it/s]evaluate for the 7-th batch, evaluate loss: 0.5274724960327148:   7%|█▎                 | 7/106 [00:01<00:27,  3.62it/s]evaluate for the 10-th batch, evaluate loss: 0.7307923436164856:  36%|██████▊            | 9/25 [00:02<00:04,  3.62it/s]evaluate for the 10-th batch, evaluate loss: 0.7307923436164856:  40%|███████▏          | 10/25 [00:02<00:03,  3.81it/s]evaluate for the 38-th batch, evaluate loss: 0.5153219699859619:  97%|█████████████████▌| 37/38 [00:09<00:00,  3.51it/s]evaluate for the 38-th batch, evaluate loss: 0.5153219699859619: 100%|██████████████████| 38/38 [00:09<00:00,  3.70it/s]evaluate for the 38-th batch, evaluate loss: 0.5153219699859619: 100%|██████████████████| 38/38 [00:09<00:00,  3.86it/s]
  0%|                                                                                            | 0/20 [00:00<?, ?it/s]evaluate for the 8-th batch, evaluate loss: 0.3873544931411743:   7%|█▎                 | 7/106 [00:02<00:27,  3.62it/s]evaluate for the 8-th batch, evaluate loss: 0.3873544931411743:   8%|█▍                 | 8/106 [00:02<00:26,  3.68it/s]Epoch: 6, train for the 118-th batch, train loss: 0.14205966889858246:  98%|█████████▊| 117/119 [01:11<00:01,  1.65it/s]Epoch: 6, train for the 118-th batch, train loss: 0.14205966889858246:  99%|█████████▉| 118/119 [01:11<00:00,  1.65it/s]Epoch: 4, train for the 12-th batch, train loss: 0.5441221594810486:   5%|▌            | 11/237 [00:05<01:46,  2.12it/s]Epoch: 4, train for the 12-th batch, train loss: 0.5441221594810486:   5%|▋            | 12/237 [00:05<01:48,  2.07it/s]evaluate for the 1-th batch, evaluate loss: 0.5730416178703308:   0%|                            | 0/20 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.5730416178703308:   5%|█                   | 1/20 [00:00<00:05,  3.34it/s]evaluate for the 11-th batch, evaluate loss: 0.7346369624137878:  40%|███████▏          | 10/25 [00:02<00:03,  3.81it/s]evaluate for the 11-th batch, evaluate loss: 0.7346369624137878:  44%|███████▉          | 11/25 [00:02<00:03,  3.56it/s]evaluate for the 9-th batch, evaluate loss: 0.5516711473464966:   8%|█▍                 | 8/106 [00:02<00:26,  3.68it/s]evaluate for the 9-th batch, evaluate loss: 0.5516711473464966:   8%|█▌                 | 9/106 [00:02<00:27,  3.59it/s]evaluate for the 12-th batch, evaluate loss: 0.6970244646072388:  44%|███████▉          | 11/25 [00:03<00:03,  3.56it/s]evaluate for the 12-th batch, evaluate loss: 0.6970244646072388:  48%|████████▋         | 12/25 [00:03<00:03,  3.68it/s]evaluate for the 2-th batch, evaluate loss: 0.6198701858520508:   5%|█                   | 1/20 [00:00<00:05,  3.34it/s]evaluate for the 2-th batch, evaluate loss: 0.6198701858520508:  10%|██                  | 2/20 [00:00<00:05,  3.52it/s]Epoch: 6, train for the 119-th batch, train loss: 0.19114051759243011:  99%|█████████▉| 118/119 [01:11<00:00,  1.65it/s]Epoch: 6, train for the 119-th batch, train loss: 0.19114051759243011: 100%|██████████| 119/119 [01:11<00:00,  1.90it/s]Epoch: 6, train for the 119-th batch, train loss: 0.19114051759243011: 100%|██████████| 119/119 [01:11<00:00,  1.66it/s]
  0%|                                                                                            | 0/40 [00:00<?, ?it/s]evaluate for the 10-th batch, evaluate loss: 0.44647693634033203:   8%|█▍               | 9/106 [00:02<00:27,  3.59it/s]evaluate for the 10-th batch, evaluate loss: 0.44647693634033203:   9%|█▌              | 10/106 [00:02<00:25,  3.72it/s]Epoch: 4, train for the 13-th batch, train loss: 0.5826789140701294:   5%|▋            | 12/237 [00:06<01:48,  2.07it/s]Epoch: 4, train for the 13-th batch, train loss: 0.5826789140701294:   5%|▋            | 13/237 [00:06<01:49,  2.05it/s]evaluate for the 13-th batch, evaluate loss: 0.6631867289543152:  48%|████████▋         | 12/25 [00:03<00:03,  3.68it/s]evaluate for the 13-th batch, evaluate loss: 0.6631867289543152:  52%|█████████▎        | 13/25 [00:03<00:03,  3.70it/s]evaluate for the 3-th batch, evaluate loss: 0.5961703062057495:  10%|██                  | 2/20 [00:00<00:05,  3.52it/s]evaluate for the 3-th batch, evaluate loss: 0.5961703062057495:  15%|███                 | 3/20 [00:00<00:04,  3.53it/s]evaluate for the 1-th batch, evaluate loss: 0.13538503646850586:   0%|                           | 0/40 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.13538503646850586:   2%|▍                  | 1/40 [00:00<00:10,  3.69it/s]evaluate for the 11-th batch, evaluate loss: 0.4210766851902008:   9%|█▌               | 10/106 [00:03<00:25,  3.72it/s]evaluate for the 11-th batch, evaluate loss: 0.4210766851902008:  10%|█▊               | 11/106 [00:03<00:26,  3.63it/s]evaluate for the 14-th batch, evaluate loss: 0.7464183568954468:  52%|█████████▎        | 13/25 [00:03<00:03,  3.70it/s]evaluate for the 14-th batch, evaluate loss: 0.7464183568954468:  56%|██████████        | 14/25 [00:03<00:02,  3.76it/s]evaluate for the 4-th batch, evaluate loss: 0.5696330666542053:  15%|███                 | 3/20 [00:01<00:04,  3.53it/s]evaluate for the 4-th batch, evaluate loss: 0.5696330666542053:  20%|████                | 4/20 [00:01<00:04,  3.54it/s]evaluate for the 12-th batch, evaluate loss: 0.42971038818359375:  10%|█▋              | 11/106 [00:03<00:26,  3.63it/s]evaluate for the 12-th batch, evaluate loss: 0.42971038818359375:  11%|█▊              | 12/106 [00:03<00:25,  3.73it/s]evaluate for the 2-th batch, evaluate loss: 0.16873693466186523:   2%|▍                  | 1/40 [00:00<00:10,  3.69it/s]evaluate for the 2-th batch, evaluate loss: 0.16873693466186523:   5%|▉                  | 2/40 [00:00<00:10,  3.65it/s]Epoch: 4, train for the 14-th batch, train loss: 0.6111156940460205:   5%|▋            | 13/237 [00:06<01:49,  2.05it/s]Epoch: 4, train for the 14-th batch, train loss: 0.6111156940460205:   6%|▊            | 14/237 [00:06<01:50,  2.01it/s]evaluate for the 15-th batch, evaluate loss: 0.7295025587081909:  56%|██████████        | 14/25 [00:04<00:02,  3.76it/s]evaluate for the 15-th batch, evaluate loss: 0.7295025587081909:  60%|██████████▊       | 15/25 [00:04<00:02,  3.76it/s]evaluate for the 5-th batch, evaluate loss: 0.597767174243927:  20%|████▏                | 4/20 [00:01<00:04,  3.54it/s]evaluate for the 5-th batch, evaluate loss: 0.597767174243927:  25%|█████▎               | 5/20 [00:01<00:04,  3.54it/s]evaluate for the 3-th batch, evaluate loss: 0.20609961450099945:   5%|▉                  | 2/40 [00:00<00:10,  3.65it/s]evaluate for the 3-th batch, evaluate loss: 0.20609961450099945:   8%|█▍                 | 3/40 [00:00<00:10,  3.63it/s]evaluate for the 13-th batch, evaluate loss: 0.45291221141815186:  11%|█▊              | 12/106 [00:03<00:25,  3.73it/s]evaluate for the 13-th batch, evaluate loss: 0.45291221141815186:  12%|█▉              | 13/106 [00:03<00:25,  3.65it/s]evaluate for the 16-th batch, evaluate loss: 0.6620350480079651:  60%|██████████▊       | 15/25 [00:04<00:02,  3.76it/s]evaluate for the 16-th batch, evaluate loss: 0.6620350480079651:  64%|███████████▌      | 16/25 [00:04<00:02,  3.63it/s]evaluate for the 14-th batch, evaluate loss: 0.42858415842056274:  12%|█▉              | 13/106 [00:03<00:25,  3.65it/s]evaluate for the 14-th batch, evaluate loss: 0.42858415842056274:  13%|██              | 14/106 [00:03<00:24,  3.72it/s]evaluate for the 4-th batch, evaluate loss: 0.13819798827171326:   8%|█▍                 | 3/40 [00:01<00:10,  3.63it/s]evaluate for the 4-th batch, evaluate loss: 0.13819798827171326:  10%|█▉                 | 4/40 [00:01<00:10,  3.56it/s]evaluate for the 6-th batch, evaluate loss: 0.6158899664878845:  25%|█████               | 5/20 [00:01<00:04,  3.54it/s]evaluate for the 6-th batch, evaluate loss: 0.6158899664878845:  30%|██████              | 6/20 [00:01<00:04,  3.48it/s]Epoch: 4, train for the 15-th batch, train loss: 0.5665965676307678:   6%|▊            | 14/237 [00:07<01:50,  2.01it/s]Epoch: 4, train for the 15-th batch, train loss: 0.5665965676307678:   6%|▊            | 15/237 [00:07<01:51,  1.99it/s]evaluate for the 17-th batch, evaluate loss: 0.6623560190200806:  64%|███████████▌      | 16/25 [00:04<00:02,  3.63it/s]evaluate for the 17-th batch, evaluate loss: 0.6623560190200806:  68%|████████████▏     | 17/25 [00:04<00:02,  3.80it/s]evaluate for the 15-th batch, evaluate loss: 0.5571339130401611:  13%|██▏              | 14/106 [00:04<00:24,  3.72it/s]evaluate for the 15-th batch, evaluate loss: 0.5571339130401611:  14%|██▍              | 15/106 [00:04<00:24,  3.68it/s]evaluate for the 5-th batch, evaluate loss: 0.17535367608070374:  10%|█▉                 | 4/40 [00:01<00:10,  3.56it/s]evaluate for the 5-th batch, evaluate loss: 0.17535367608070374:  12%|██▍                | 5/40 [00:01<00:09,  3.59it/s]evaluate for the 7-th batch, evaluate loss: 0.6561033725738525:  30%|██████              | 6/20 [00:01<00:04,  3.48it/s]evaluate for the 7-th batch, evaluate loss: 0.6561033725738525:  35%|███████             | 7/20 [00:01<00:03,  3.49it/s]evaluate for the 18-th batch, evaluate loss: 0.6295467019081116:  68%|████████████▏     | 17/25 [00:04<00:02,  3.80it/s]evaluate for the 18-th batch, evaluate loss: 0.6295467019081116:  72%|████████████▉     | 18/25 [00:04<00:01,  3.60it/s]evaluate for the 16-th batch, evaluate loss: 0.4891766309738159:  14%|██▍              | 15/106 [00:04<00:24,  3.68it/s]evaluate for the 16-th batch, evaluate loss: 0.4891766309738159:  15%|██▌              | 16/106 [00:04<00:24,  3.65it/s]evaluate for the 6-th batch, evaluate loss: 0.169204980134964:  12%|██▋                  | 5/40 [00:01<00:09,  3.59it/s]evaluate for the 6-th batch, evaluate loss: 0.169204980134964:  15%|███▏                 | 6/40 [00:01<00:09,  3.63it/s]evaluate for the 8-th batch, evaluate loss: 0.6450643539428711:  35%|███████             | 7/20 [00:02<00:03,  3.49it/s]evaluate for the 8-th batch, evaluate loss: 0.6450643539428711:  40%|████████            | 8/20 [00:02<00:03,  3.51it/s]Epoch: 4, train for the 16-th batch, train loss: 0.536091685295105:   6%|▉             | 15/237 [00:07<01:51,  1.99it/s]Epoch: 4, train for the 16-th batch, train loss: 0.536091685295105:   7%|▉             | 16/237 [00:07<01:52,  1.97it/s]evaluate for the 19-th batch, evaluate loss: 0.5898529887199402:  72%|████████████▉     | 18/25 [00:05<00:01,  3.60it/s]evaluate for the 19-th batch, evaluate loss: 0.5898529887199402:  76%|█████████████▋    | 19/25 [00:05<00:01,  3.77it/s]evaluate for the 17-th batch, evaluate loss: 0.45920658111572266:  15%|██▍             | 16/106 [00:04<00:24,  3.65it/s]evaluate for the 17-th batch, evaluate loss: 0.45920658111572266:  16%|██▌             | 17/106 [00:04<00:24,  3.69it/s]evaluate for the 7-th batch, evaluate loss: 0.11091596633195877:  15%|██▊                | 6/40 [00:01<00:09,  3.63it/s]evaluate for the 7-th batch, evaluate loss: 0.11091596633195877:  18%|███▎               | 7/40 [00:01<00:08,  3.67it/s]evaluate for the 9-th batch, evaluate loss: 0.5907071828842163:  40%|████████            | 8/20 [00:02<00:03,  3.51it/s]evaluate for the 9-th batch, evaluate loss: 0.5907071828842163:  45%|█████████           | 9/20 [00:02<00:03,  3.52it/s]evaluate for the 20-th batch, evaluate loss: 0.6584615707397461:  76%|█████████████▋    | 19/25 [00:05<00:01,  3.77it/s]evaluate for the 20-th batch, evaluate loss: 0.6584615707397461:  80%|██████████████▍   | 20/25 [00:05<00:01,  3.58it/s]evaluate for the 8-th batch, evaluate loss: 0.12699680030345917:  18%|███▎               | 7/40 [00:02<00:08,  3.67it/s]evaluate for the 8-th batch, evaluate loss: 0.12699680030345917:  20%|███▊               | 8/40 [00:02<00:08,  3.69it/s]evaluate for the 18-th batch, evaluate loss: 0.42732492089271545:  16%|██▌             | 17/106 [00:04<00:24,  3.69it/s]evaluate for the 18-th batch, evaluate loss: 0.42732492089271545:  17%|██▋             | 18/106 [00:04<00:24,  3.65it/s]evaluate for the 10-th batch, evaluate loss: 0.5963560342788696:  45%|████████▌          | 9/20 [00:02<00:03,  3.52it/s]evaluate for the 10-th batch, evaluate loss: 0.5963560342788696:  50%|█████████         | 10/20 [00:02<00:02,  3.53it/s]Epoch: 4, train for the 17-th batch, train loss: 0.5475279688835144:   7%|▉            | 16/237 [00:08<01:52,  1.97it/s]Epoch: 4, train for the 17-th batch, train loss: 0.5475279688835144:   7%|▉            | 17/237 [00:08<01:54,  1.92it/s]evaluate for the 21-th batch, evaluate loss: 0.727671205997467:  80%|███████████████▏   | 20/25 [00:05<00:01,  3.58it/s]evaluate for the 21-th batch, evaluate loss: 0.727671205997467:  84%|███████████████▉   | 21/25 [00:05<00:01,  3.68it/s]evaluate for the 19-th batch, evaluate loss: 0.3593415319919586:  17%|██▉              | 18/106 [00:05<00:24,  3.65it/s]evaluate for the 19-th batch, evaluate loss: 0.3593415319919586:  18%|███              | 19/106 [00:05<00:23,  3.69it/s]evaluate for the 9-th batch, evaluate loss: 0.17458830773830414:  20%|███▊               | 8/40 [00:02<00:08,  3.69it/s]evaluate for the 9-th batch, evaluate loss: 0.17458830773830414:  22%|████▎              | 9/40 [00:02<00:08,  3.69it/s]evaluate for the 11-th batch, evaluate loss: 0.5942386984825134:  50%|█████████         | 10/20 [00:03<00:02,  3.53it/s]evaluate for the 11-th batch, evaluate loss: 0.5942386984825134:  55%|█████████▉        | 11/20 [00:03<00:02,  3.53it/s]evaluate for the 22-th batch, evaluate loss: 0.604162871837616:  84%|███████████████▉   | 21/25 [00:05<00:01,  3.68it/s]evaluate for the 22-th batch, evaluate loss: 0.604162871837616:  88%|████████████████▋  | 22/25 [00:05<00:00,  3.67it/s]evaluate for the 10-th batch, evaluate loss: 0.1979740709066391:  22%|████▎              | 9/40 [00:02<00:08,  3.69it/s]evaluate for the 10-th batch, evaluate loss: 0.1979740709066391:  25%|████▌             | 10/40 [00:02<00:08,  3.70it/s]evaluate for the 20-th batch, evaluate loss: 0.41291290521621704:  18%|██▊             | 19/106 [00:05<00:23,  3.69it/s]evaluate for the 20-th batch, evaluate loss: 0.41291290521621704:  19%|███             | 20/106 [00:05<00:23,  3.64it/s]evaluate for the 12-th batch, evaluate loss: 0.6226913928985596:  55%|█████████▉        | 11/20 [00:03<00:02,  3.53it/s]evaluate for the 12-th batch, evaluate loss: 0.6226913928985596:  60%|██████████▊       | 12/20 [00:03<00:02,  3.54it/s]Epoch: 4, train for the 18-th batch, train loss: 0.5348796248435974:   7%|▉            | 17/237 [00:08<01:54,  1.92it/s]Epoch: 4, train for the 18-th batch, train loss: 0.5348796248435974:   8%|▉            | 18/237 [00:08<01:56,  1.88it/s]evaluate for the 23-th batch, evaluate loss: 0.6654587984085083:  88%|███████████████▊  | 22/25 [00:06<00:00,  3.67it/s]evaluate for the 23-th batch, evaluate loss: 0.6654587984085083:  92%|████████████████▌ | 23/25 [00:06<00:00,  3.79it/s]evaluate for the 11-th batch, evaluate loss: 0.1698603332042694:  25%|████▌             | 10/40 [00:02<00:08,  3.70it/s]evaluate for the 11-th batch, evaluate loss: 0.1698603332042694:  28%|████▉             | 11/40 [00:02<00:07,  3.70it/s]evaluate for the 21-th batch, evaluate loss: 0.3314242362976074:  19%|███▏             | 20/106 [00:05<00:23,  3.64it/s]evaluate for the 21-th batch, evaluate loss: 0.3314242362976074:  20%|███▎             | 21/106 [00:05<00:23,  3.65it/s]evaluate for the 13-th batch, evaluate loss: 0.6422343254089355:  60%|██████████▊       | 12/20 [00:03<00:02,  3.54it/s]evaluate for the 13-th batch, evaluate loss: 0.6422343254089355:  65%|███████████▋      | 13/20 [00:03<00:01,  3.54it/s]evaluate for the 24-th batch, evaluate loss: 0.6619218587875366:  92%|████████████████▌ | 23/25 [00:06<00:00,  3.79it/s]evaluate for the 24-th batch, evaluate loss: 0.6619218587875366:  96%|█████████████████▎| 24/25 [00:06<00:00,  3.76it/s]evaluate for the 12-th batch, evaluate loss: 0.1503642052412033:  28%|████▉             | 11/40 [00:03<00:07,  3.70it/s]evaluate for the 12-th batch, evaluate loss: 0.1503642052412033:  30%|█████▍            | 12/40 [00:03<00:07,  3.71it/s]evaluate for the 22-th batch, evaluate loss: 0.4080522954463959:  20%|███▎             | 21/106 [00:06<00:23,  3.65it/s]evaluate for the 22-th batch, evaluate loss: 0.4080522954463959:  21%|███▌             | 22/106 [00:06<00:23,  3.61it/s]evaluate for the 14-th batch, evaluate loss: 0.6151302456855774:  65%|███████████▋      | 13/20 [00:03<00:01,  3.54it/s]evaluate for the 14-th batch, evaluate loss: 0.6151302456855774:  70%|████████████▌     | 14/20 [00:03<00:01,  3.54it/s]Epoch: 4, train for the 19-th batch, train loss: 0.4762231707572937:   8%|▉            | 18/237 [00:09<01:56,  1.88it/s]Epoch: 4, train for the 19-th batch, train loss: 0.4762231707572937:   8%|█            | 19/237 [00:09<01:56,  1.86it/s]evaluate for the 25-th batch, evaluate loss: 0.7089424729347229:  96%|█████████████████▎| 24/25 [00:06<00:00,  3.76it/s]evaluate for the 25-th batch, evaluate loss: 0.7089424729347229: 100%|██████████████████| 25/25 [00:06<00:00,  3.85it/s]evaluate for the 25-th batch, evaluate loss: 0.7089424729347229: 100%|██████████████████| 25/25 [00:06<00:00,  3.74it/s]
INFO:root:Epoch: 10, learning rate: 0.0001, train loss: 0.5735
INFO:root:train average_precision, 0.8082
INFO:root:train roc_auc, 0.7696
INFO:root:validate loss: 0.5073
INFO:root:validate average_precision, 0.8432
INFO:root:validate roc_auc, 0.8028
INFO:root:new node validate loss: 0.6828
INFO:root:new node validate first_1_average_precision, 0.5965
INFO:root:new node validate first_1_roc_auc, 0.5414
INFO:root:new node validate first_3_average_precision, 0.6808
INFO:root:new node validate first_3_roc_auc, 0.6397
INFO:root:new node validate first_10_average_precision, 0.7481
INFO:root:new node validate first_10_roc_auc, 0.7121
INFO:root:new node validate average_precision, 0.7086
INFO:root:new node validate roc_auc, 0.6546
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                            | 0/46 [00:00<?, ?it/s]evaluate for the 13-th batch, evaluate loss: 0.11977241188287735:  30%|█████            | 12/40 [00:03<00:07,  3.71it/s]evaluate for the 13-th batch, evaluate loss: 0.11977241188287735:  32%|█████▌           | 13/40 [00:03<00:07,  3.71it/s]evaluate for the 23-th batch, evaluate loss: 0.42669203877449036:  21%|███▎            | 22/106 [00:06<00:23,  3.61it/s]evaluate for the 23-th batch, evaluate loss: 0.42669203877449036:  22%|███▍            | 23/106 [00:06<00:22,  3.65it/s]evaluate for the 1-th batch, evaluate loss: 0.5164243578910828:   0%|                            | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.5164243578910828:   2%|▍                   | 1/46 [00:00<00:09,  4.93it/s]evaluate for the 15-th batch, evaluate loss: 0.6088207364082336:  70%|████████████▌     | 14/20 [00:04<00:01,  3.54it/s]evaluate for the 15-th batch, evaluate loss: 0.6088207364082336:  75%|█████████████▌    | 15/20 [00:04<00:01,  3.55it/s]evaluate for the 14-th batch, evaluate loss: 0.15947072207927704:  32%|█████▌           | 13/40 [00:03<00:07,  3.71it/s]evaluate for the 14-th batch, evaluate loss: 0.15947072207927704:  35%|█████▉           | 14/40 [00:03<00:07,  3.69it/s]evaluate for the 24-th batch, evaluate loss: 0.3755419850349426:  22%|███▋             | 23/106 [00:06<00:22,  3.65it/s]evaluate for the 24-th batch, evaluate loss: 0.3755419850349426:  23%|███▊             | 24/106 [00:06<00:22,  3.60it/s]evaluate for the 16-th batch, evaluate loss: 0.5863321423530579:  75%|█████████████▌    | 15/20 [00:04<00:01,  3.55it/s]evaluate for the 16-th batch, evaluate loss: 0.5863321423530579:  80%|██████████████▍   | 16/20 [00:04<00:01,  3.54it/s]Epoch: 4, train for the 20-th batch, train loss: 0.6522822380065918:   8%|█            | 19/237 [00:09<01:56,  1.86it/s]Epoch: 4, train for the 20-th batch, train loss: 0.6522822380065918:   8%|█            | 20/237 [00:09<01:57,  1.84it/s]evaluate for the 2-th batch, evaluate loss: 0.5323327779769897:   2%|▍                   | 1/46 [00:00<00:09,  4.93it/s]evaluate for the 2-th batch, evaluate loss: 0.5323327779769897:   4%|▊                   | 2/46 [00:00<00:11,  3.69it/s]evaluate for the 15-th batch, evaluate loss: 0.18346671760082245:  35%|█████▉           | 14/40 [00:04<00:07,  3.69it/s]evaluate for the 15-th batch, evaluate loss: 0.18346671760082245:  38%|██████▍          | 15/40 [00:04<00:06,  3.66it/s]evaluate for the 25-th batch, evaluate loss: 0.47669845819473267:  23%|███▌            | 24/106 [00:06<00:22,  3.60it/s]evaluate for the 25-th batch, evaluate loss: 0.47669845819473267:  24%|███▊            | 25/106 [00:06<00:22,  3.64it/s]evaluate for the 3-th batch, evaluate loss: 0.5584384799003601:   4%|▊                   | 2/46 [00:00<00:11,  3.69it/s]evaluate for the 3-th batch, evaluate loss: 0.5584384799003601:   7%|█▎                  | 3/46 [00:00<00:11,  3.85it/s]evaluate for the 17-th batch, evaluate loss: 0.5854244828224182:  80%|██████████████▍   | 16/20 [00:04<00:01,  3.54it/s]evaluate for the 17-th batch, evaluate loss: 0.5854244828224182:  85%|███████████████▎  | 17/20 [00:04<00:00,  3.54it/s]evaluate for the 16-th batch, evaluate loss: 0.19895023107528687:  38%|██████▍          | 15/40 [00:04<00:06,  3.66it/s]evaluate for the 16-th batch, evaluate loss: 0.19895023107528687:  40%|██████▊          | 16/40 [00:04<00:06,  3.65it/s]evaluate for the 26-th batch, evaluate loss: 0.43864935636520386:  24%|███▊            | 25/106 [00:07<00:22,  3.64it/s]evaluate for the 26-th batch, evaluate loss: 0.43864935636520386:  25%|███▉            | 26/106 [00:07<00:22,  3.61it/s]evaluate for the 4-th batch, evaluate loss: 0.5482338070869446:   7%|█▎                  | 3/46 [00:01<00:11,  3.85it/s]evaluate for the 4-th batch, evaluate loss: 0.5482338070869446:   9%|█▋                  | 4/46 [00:01<00:11,  3.82it/s]Epoch: 4, train for the 21-th batch, train loss: 0.5144824385643005:   8%|█            | 20/237 [00:10<01:57,  1.84it/s]Epoch: 4, train for the 21-th batch, train loss: 0.5144824385643005:   9%|█▏           | 21/237 [00:10<01:58,  1.83it/s]evaluate for the 18-th batch, evaluate loss: 0.652225911617279:  85%|████████████████▏  | 17/20 [00:05<00:00,  3.54it/s]evaluate for the 18-th batch, evaluate loss: 0.652225911617279:  90%|█████████████████  | 18/20 [00:05<00:00,  3.50it/s]evaluate for the 17-th batch, evaluate loss: 0.13047970831394196:  40%|██████▊          | 16/40 [00:04<00:06,  3.65it/s]evaluate for the 17-th batch, evaluate loss: 0.13047970831394196:  42%|███████▏         | 17/40 [00:04<00:06,  3.68it/s]evaluate for the 27-th batch, evaluate loss: 0.322487473487854:  25%|████▍             | 26/106 [00:07<00:22,  3.61it/s]evaluate for the 27-th batch, evaluate loss: 0.322487473487854:  25%|████▌             | 27/106 [00:07<00:21,  3.63it/s]evaluate for the 5-th batch, evaluate loss: 0.5179421901702881:   9%|█▋                  | 4/46 [00:01<00:11,  3.82it/s]evaluate for the 5-th batch, evaluate loss: 0.5179421901702881:  11%|██▏                 | 5/46 [00:01<00:10,  3.85it/s]evaluate for the 19-th batch, evaluate loss: 0.6492599844932556:  90%|████████████████▏ | 18/20 [00:05<00:00,  3.50it/s]evaluate for the 19-th batch, evaluate loss: 0.6492599844932556:  95%|█████████████████ | 19/20 [00:05<00:00,  3.51it/s]evaluate for the 20-th batch, evaluate loss: 0.629719614982605:  95%|██████████████████ | 19/20 [00:05<00:00,  3.51it/s]evaluate for the 20-th batch, evaluate loss: 0.629719614982605: 100%|███████████████████| 20/20 [00:05<00:00,  3.65it/s]
INFO:root:Epoch: 5, learning rate: 0.0001, train loss: 0.5076
INFO:root:train average_precision, 0.8489
evaluate for the 18-th batch, evaluate loss: 0.14475779235363007:  42%|███████▏         | 17/40 [00:04<00:06,  3.68it/s]INFO:root:train roc_auc, 0.8247
evaluate for the 18-th batch, evaluate loss: 0.14475779235363007:  45%|███████▋         | 18/40 [00:04<00:05,  3.72it/s]INFO:root:validate loss: 0.5201
INFO:root:validate average_precision, 0.8412
INFO:root:validate roc_auc, 0.8061
INFO:root:new node validate loss: 0.6123
INFO:root:new node validate first_1_average_precision, 0.6520
INFO:root:new node validate first_1_roc_auc, 0.5776
INFO:root:new node validate first_3_average_precision, 0.7039
INFO:root:new node validate first_3_roc_auc, 0.6375
INFO:root:new node validate first_10_average_precision, 0.7471
INFO:root:new node validate first_10_roc_auc, 0.6928
INFO:root:new node validate average_precision, 0.7530
INFO:root:new node validate roc_auc, 0.7122
INFO:root:save model ./saved_models/DyGFormer/ia-escorts-dynamic/DyGFormer_seed0_dygformer-ia-escorts-dynamic-old/DyGFormer_seed0_dygformer-ia-escorts-dynamic-old.pkl
evaluate for the 28-th batch, evaluate loss: 0.33273476362228394:  25%|████            | 27/106 [00:07<00:21,  3.63it/s]evaluate for the 28-th batch, evaluate loss: 0.33273476362228394:  26%|████▏           | 28/106 [00:07<00:21,  3.60it/s]evaluate for the 6-th batch, evaluate loss: 0.4779565632343292:  11%|██▏                 | 5/46 [00:01<00:10,  3.85it/s]evaluate for the 6-th batch, evaluate loss: 0.4779565632343292:  13%|██▌                 | 6/46 [00:01<00:10,  3.93it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]evaluate for the 19-th batch, evaluate loss: 0.15120798349380493:  45%|███████▋         | 18/40 [00:04<00:05,  3.72it/s]evaluate for the 19-th batch, evaluate loss: 0.15120798349380493:  48%|████████         | 19/40 [00:04<00:04,  4.56it/s]Epoch: 4, train for the 22-th batch, train loss: 0.450545996427536:   9%|█▏            | 21/237 [00:10<01:58,  1.83it/s]Epoch: 4, train for the 22-th batch, train loss: 0.450545996427536:   9%|█▎            | 22/237 [00:10<01:57,  1.83it/s]evaluate for the 29-th batch, evaluate loss: 0.3790648281574249:  26%|████▍            | 28/106 [00:07<00:21,  3.60it/s]evaluate for the 29-th batch, evaluate loss: 0.3790648281574249:  27%|████▋            | 29/106 [00:07<00:21,  3.63it/s]evaluate for the 20-th batch, evaluate loss: 0.15007279813289642:  48%|████████         | 19/40 [00:05<00:04,  4.56it/s]evaluate for the 20-th batch, evaluate loss: 0.15007279813289642:  50%|████████▌        | 20/40 [00:05<00:04,  4.25it/s]evaluate for the 7-th batch, evaluate loss: 0.5388860106468201:  13%|██▌                 | 6/46 [00:01<00:10,  3.93it/s]evaluate for the 7-th batch, evaluate loss: 0.5388860106468201:  15%|███                 | 7/46 [00:01<00:10,  3.69it/s]Epoch: 6, train for the 1-th batch, train loss: 0.8439362645149231:   0%|                       | 0/146 [00:00<?, ?it/s]Epoch: 6, train for the 1-th batch, train loss: 0.8439362645149231:   1%|               | 1/146 [00:00<01:05,  2.21it/s]evaluate for the 8-th batch, evaluate loss: 0.5151957869529724:  15%|███                 | 7/46 [00:02<00:10,  3.69it/s]evaluate for the 8-th batch, evaluate loss: 0.5151957869529724:  17%|███▍                | 8/46 [00:02<00:09,  3.89it/s]evaluate for the 30-th batch, evaluate loss: 0.4231794774532318:  27%|████▋            | 29/106 [00:08<00:21,  3.63it/s]evaluate for the 30-th batch, evaluate loss: 0.4231794774532318:  28%|████▊            | 30/106 [00:08<00:21,  3.60it/s]evaluate for the 21-th batch, evaluate loss: 0.08998506516218185:  50%|████████▌        | 20/40 [00:05<00:04,  4.25it/s]evaluate for the 21-th batch, evaluate loss: 0.08998506516218185:  52%|████████▉        | 21/40 [00:05<00:04,  3.99it/s]Epoch: 4, train for the 23-th batch, train loss: 0.43711239099502563:   9%|█           | 22/237 [00:11<01:57,  1.83it/s]Epoch: 4, train for the 23-th batch, train loss: 0.43711239099502563:  10%|█▏          | 23/237 [00:11<01:56,  1.83it/s]evaluate for the 31-th batch, evaluate loss: 0.3630933165550232:  28%|████▊            | 30/106 [00:08<00:21,  3.60it/s]evaluate for the 31-th batch, evaluate loss: 0.3630933165550232:  29%|████▉            | 31/106 [00:08<00:20,  3.65it/s]evaluate for the 9-th batch, evaluate loss: 0.522927463054657:  17%|███▋                 | 8/46 [00:02<00:09,  3.89it/s]evaluate for the 9-th batch, evaluate loss: 0.522927463054657:  20%|████                 | 9/46 [00:02<00:10,  3.69it/s]evaluate for the 22-th batch, evaluate loss: 0.14639343321323395:  52%|████████▉        | 21/40 [00:05<00:04,  3.99it/s]evaluate for the 22-th batch, evaluate loss: 0.14639343321323395:  55%|█████████▎       | 22/40 [00:05<00:04,  3.93it/s]Epoch: 6, train for the 2-th batch, train loss: 0.5081892609596252:   1%|               | 1/146 [00:00<01:05,  2.21it/s]Epoch: 6, train for the 2-th batch, train loss: 0.5081892609596252:   1%|▏              | 2/146 [00:00<01:09,  2.07it/s]evaluate for the 10-th batch, evaluate loss: 0.5211113095283508:  20%|███▋               | 9/46 [00:02<00:10,  3.69it/s]evaluate for the 10-th batch, evaluate loss: 0.5211113095283508:  22%|███▉              | 10/46 [00:02<00:09,  3.86it/s]evaluate for the 32-th batch, evaluate loss: 0.4622335433959961:  29%|████▉            | 31/106 [00:08<00:20,  3.65it/s]evaluate for the 32-th batch, evaluate loss: 0.4622335433959961:  30%|█████▏           | 32/106 [00:08<00:20,  3.61it/s]evaluate for the 23-th batch, evaluate loss: 0.14736035466194153:  55%|█████████▎       | 22/40 [00:06<00:04,  3.93it/s]evaluate for the 23-th batch, evaluate loss: 0.14736035466194153:  57%|█████████▊       | 23/40 [00:06<00:04,  3.82it/s]Epoch: 4, train for the 24-th batch, train loss: 0.4375367760658264:  10%|█▎           | 23/237 [00:12<01:56,  1.83it/s]Epoch: 4, train for the 24-th batch, train loss: 0.4375367760658264:  10%|█▎           | 24/237 [00:12<01:57,  1.82it/s]evaluate for the 11-th batch, evaluate loss: 0.5239439010620117:  22%|███▉              | 10/46 [00:02<00:09,  3.86it/s]evaluate for the 11-th batch, evaluate loss: 0.5239439010620117:  24%|████▎             | 11/46 [00:02<00:09,  3.85it/s]evaluate for the 33-th batch, evaluate loss: 0.47670698165893555:  30%|████▊           | 32/106 [00:09<00:20,  3.61it/s]evaluate for the 33-th batch, evaluate loss: 0.47670698165893555:  31%|████▉           | 33/106 [00:09<00:20,  3.64it/s]evaluate for the 24-th batch, evaluate loss: 0.1448339819908142:  57%|██████████▎       | 23/40 [00:06<00:04,  3.82it/s]evaluate for the 24-th batch, evaluate loss: 0.1448339819908142:  60%|██████████▊       | 24/40 [00:06<00:04,  3.77it/s]Epoch: 6, train for the 3-th batch, train loss: 0.4220532774925232:   1%|▏              | 2/146 [00:01<01:09,  2.07it/s]Epoch: 6, train for the 3-th batch, train loss: 0.4220532774925232:   2%|▎              | 3/146 [00:01<01:16,  1.88it/s]evaluate for the 12-th batch, evaluate loss: 0.496846079826355:  24%|████▌              | 11/46 [00:03<00:09,  3.85it/s]evaluate for the 12-th batch, evaluate loss: 0.496846079826355:  26%|████▉              | 12/46 [00:03<00:09,  3.71it/s]evaluate for the 34-th batch, evaluate loss: 0.46633583307266235:  31%|████▉           | 33/106 [00:09<00:20,  3.64it/s]evaluate for the 34-th batch, evaluate loss: 0.46633583307266235:  32%|█████▏          | 34/106 [00:09<00:19,  3.61it/s]evaluate for the 25-th batch, evaluate loss: 0.14719873666763306:  60%|██████████▏      | 24/40 [00:06<00:04,  3.77it/s]evaluate for the 25-th batch, evaluate loss: 0.14719873666763306:  62%|██████████▋      | 25/40 [00:06<00:04,  3.63it/s]Epoch: 4, train for the 25-th batch, train loss: 0.4668458104133606:  10%|█▎           | 24/237 [00:12<01:57,  1.82it/s]Epoch: 4, train for the 25-th batch, train loss: 0.4668458104133606:  11%|█▎           | 25/237 [00:12<01:57,  1.81it/s]evaluate for the 13-th batch, evaluate loss: 0.4881574809551239:  26%|████▋             | 12/46 [00:03<00:09,  3.71it/s]evaluate for the 13-th batch, evaluate loss: 0.4881574809551239:  28%|█████             | 13/46 [00:03<00:08,  3.96it/s]evaluate for the 35-th batch, evaluate loss: 0.4173465967178345:  32%|█████▍           | 34/106 [00:09<00:19,  3.61it/s]evaluate for the 35-th batch, evaluate loss: 0.4173465967178345:  33%|█████▌           | 35/106 [00:09<00:19,  3.63it/s]evaluate for the 26-th batch, evaluate loss: 0.12664122879505157:  62%|██████████▋      | 25/40 [00:06<00:04,  3.63it/s]evaluate for the 26-th batch, evaluate loss: 0.12664122879505157:  65%|███████████      | 26/40 [00:06<00:03,  3.73it/s]evaluate for the 14-th batch, evaluate loss: 0.4873800575733185:  28%|█████             | 13/46 [00:03<00:08,  3.96it/s]evaluate for the 14-th batch, evaluate loss: 0.4873800575733185:  30%|█████▍            | 14/46 [00:03<00:08,  3.70it/s]Epoch: 6, train for the 4-th batch, train loss: 0.39162880182266235:   2%|▎             | 3/146 [00:02<01:16,  1.88it/s]Epoch: 6, train for the 4-th batch, train loss: 0.39162880182266235:   3%|▍             | 4/146 [00:02<01:18,  1.80it/s]evaluate for the 36-th batch, evaluate loss: 0.4556259512901306:  33%|█████▌           | 35/106 [00:09<00:19,  3.63it/s]evaluate for the 36-th batch, evaluate loss: 0.4556259512901306:  34%|█████▊           | 36/106 [00:09<00:19,  3.59it/s]evaluate for the 27-th batch, evaluate loss: 0.14162607491016388:  65%|███████████      | 26/40 [00:07<00:03,  3.73it/s]evaluate for the 27-th batch, evaluate loss: 0.14162607491016388:  68%|███████████▍     | 27/40 [00:07<00:03,  3.60it/s]Epoch: 4, train for the 26-th batch, train loss: 0.5053088665008545:  11%|█▎           | 25/237 [00:13<01:57,  1.81it/s]Epoch: 4, train for the 26-th batch, train loss: 0.5053088665008545:  11%|█▍           | 26/237 [00:13<01:57,  1.80it/s]evaluate for the 15-th batch, evaluate loss: 0.5136695504188538:  30%|█████▍            | 14/46 [00:03<00:08,  3.70it/s]evaluate for the 15-th batch, evaluate loss: 0.5136695504188538:  33%|█████▊            | 15/46 [00:03<00:08,  3.80it/s]evaluate for the 37-th batch, evaluate loss: 0.52683025598526:  34%|██████▍            | 36/106 [00:10<00:19,  3.59it/s]evaluate for the 37-th batch, evaluate loss: 0.52683025598526:  35%|██████▋            | 37/106 [00:10<00:19,  3.61it/s]evaluate for the 28-th batch, evaluate loss: 0.11382198333740234:  68%|███████████▍     | 27/40 [00:07<00:03,  3.60it/s]evaluate for the 28-th batch, evaluate loss: 0.11382198333740234:  70%|███████████▉     | 28/40 [00:07<00:03,  3.70it/s]evaluate for the 16-th batch, evaluate loss: 0.5037059783935547:  33%|█████▊            | 15/46 [00:04<00:08,  3.80it/s]evaluate for the 16-th batch, evaluate loss: 0.5037059783935547:  35%|██████▎           | 16/46 [00:04<00:07,  3.78it/s]Epoch: 6, train for the 5-th batch, train loss: 0.4530787169933319:   3%|▍              | 4/146 [00:02<01:18,  1.80it/s]Epoch: 6, train for the 5-th batch, train loss: 0.4530787169933319:   3%|▌              | 5/146 [00:02<01:18,  1.80it/s]evaluate for the 38-th batch, evaluate loss: 0.5370995998382568:  35%|█████▉           | 37/106 [00:10<00:19,  3.61it/s]evaluate for the 38-th batch, evaluate loss: 0.5370995998382568:  36%|██████           | 38/106 [00:10<00:19,  3.56it/s]evaluate for the 29-th batch, evaluate loss: 0.16357216238975525:  70%|███████████▉     | 28/40 [00:07<00:03,  3.70it/s]evaluate for the 29-th batch, evaluate loss: 0.16357216238975525:  72%|████████████▎    | 29/40 [00:07<00:03,  3.58it/s]Epoch: 4, train for the 27-th batch, train loss: 0.46854695677757263:  11%|█▎          | 26/237 [00:13<01:57,  1.80it/s]Epoch: 4, train for the 27-th batch, train loss: 0.46854695677757263:  11%|█▎          | 27/237 [00:13<01:57,  1.79it/s]evaluate for the 17-th batch, evaluate loss: 0.4518275260925293:  35%|██████▎           | 16/46 [00:04<00:07,  3.78it/s]evaluate for the 17-th batch, evaluate loss: 0.4518275260925293:  37%|██████▋           | 17/46 [00:04<00:07,  3.88it/s]evaluate for the 39-th batch, evaluate loss: 0.40681779384613037:  36%|█████▋          | 38/106 [00:10<00:19,  3.56it/s]evaluate for the 39-th batch, evaluate loss: 0.40681779384613037:  37%|█████▉          | 39/106 [00:10<00:18,  3.59it/s]evaluate for the 30-th batch, evaluate loss: 0.1756613552570343:  72%|█████████████     | 29/40 [00:08<00:03,  3.58it/s]evaluate for the 30-th batch, evaluate loss: 0.1756613552570343:  75%|█████████████▌    | 30/40 [00:08<00:02,  3.71it/s]evaluate for the 18-th batch, evaluate loss: 0.4729780852794647:  37%|██████▋           | 17/46 [00:04<00:07,  3.88it/s]evaluate for the 18-th batch, evaluate loss: 0.4729780852794647:  39%|███████           | 18/46 [00:04<00:07,  3.94it/s]Epoch: 6, train for the 6-th batch, train loss: 0.40644514560699463:   3%|▍             | 5/146 [00:03<01:18,  1.80it/s]Epoch: 6, train for the 6-th batch, train loss: 0.40644514560699463:   4%|▌             | 6/146 [00:03<01:18,  1.78it/s]evaluate for the 40-th batch, evaluate loss: 0.4420825242996216:  37%|██████▎          | 39/106 [00:11<00:18,  3.59it/s]evaluate for the 40-th batch, evaluate loss: 0.4420825242996216:  38%|██████▍          | 40/106 [00:11<00:18,  3.55it/s]evaluate for the 31-th batch, evaluate loss: 0.1614929884672165:  75%|█████████████▌    | 30/40 [00:08<00:02,  3.71it/s]evaluate for the 31-th batch, evaluate loss: 0.1614929884672165:  78%|█████████████▉    | 31/40 [00:08<00:02,  3.57it/s]Epoch: 4, train for the 28-th batch, train loss: 0.6029506325721741:  11%|█▍           | 27/237 [00:14<01:57,  1.79it/s]Epoch: 4, train for the 28-th batch, train loss: 0.6029506325721741:  12%|█▌           | 28/237 [00:14<01:56,  1.79it/s]evaluate for the 19-th batch, evaluate loss: 0.4890832304954529:  39%|███████           | 18/46 [00:04<00:07,  3.94it/s]evaluate for the 19-th batch, evaluate loss: 0.4890832304954529:  41%|███████▍          | 19/46 [00:04<00:07,  3.73it/s]evaluate for the 41-th batch, evaluate loss: 0.45721280574798584:  38%|██████          | 40/106 [00:11<00:18,  3.55it/s]evaluate for the 41-th batch, evaluate loss: 0.45721280574798584:  39%|██████▏         | 41/106 [00:11<00:18,  3.59it/s]evaluate for the 32-th batch, evaluate loss: 0.13493917882442474:  78%|█████████████▏   | 31/40 [00:08<00:02,  3.57it/s]evaluate for the 32-th batch, evaluate loss: 0.13493917882442474:  80%|█████████████▌   | 32/40 [00:08<00:02,  3.71it/s]evaluate for the 20-th batch, evaluate loss: 0.5267630219459534:  41%|███████▍          | 19/46 [00:05<00:07,  3.73it/s]evaluate for the 20-th batch, evaluate loss: 0.5267630219459534:  43%|███████▊          | 20/46 [00:05<00:06,  3.92it/s]Epoch: 6, train for the 7-th batch, train loss: 0.39494451880455017:   4%|▌             | 6/146 [00:03<01:18,  1.78it/s]Epoch: 6, train for the 7-th batch, train loss: 0.39494451880455017:   5%|▋             | 7/146 [00:03<01:19,  1.76it/s]evaluate for the 42-th batch, evaluate loss: 0.38584503531455994:  39%|██████▏         | 41/106 [00:11<00:18,  3.59it/s]evaluate for the 42-th batch, evaluate loss: 0.38584503531455994:  40%|██████▎         | 42/106 [00:11<00:17,  3.56it/s]evaluate for the 33-th batch, evaluate loss: 0.14035964012145996:  80%|█████████████▌   | 32/40 [00:08<00:02,  3.71it/s]evaluate for the 33-th batch, evaluate loss: 0.14035964012145996:  82%|██████████████   | 33/40 [00:08<00:01,  3.55it/s]evaluate for the 21-th batch, evaluate loss: 0.506642758846283:  43%|████████▎          | 20/46 [00:05<00:06,  3.92it/s]evaluate for the 21-th batch, evaluate loss: 0.506642758846283:  46%|████████▋          | 21/46 [00:05<00:06,  3.72it/s]Epoch: 4, train for the 29-th batch, train loss: 0.47666653990745544:  12%|█▍          | 28/237 [00:14<01:56,  1.79it/s]Epoch: 4, train for the 29-th batch, train loss: 0.47666653990745544:  12%|█▍          | 29/237 [00:14<01:56,  1.78it/s]evaluate for the 43-th batch, evaluate loss: 0.33666694164276123:  40%|██████▎         | 42/106 [00:11<00:17,  3.56it/s]evaluate for the 43-th batch, evaluate loss: 0.33666694164276123:  41%|██████▍         | 43/106 [00:11<00:17,  3.57it/s]evaluate for the 34-th batch, evaluate loss: 0.11120792478322983:  82%|██████████████   | 33/40 [00:09<00:01,  3.55it/s]evaluate for the 34-th batch, evaluate loss: 0.11120792478322983:  85%|██████████████▍  | 34/40 [00:09<00:01,  3.66it/s]evaluate for the 22-th batch, evaluate loss: 0.49491187930107117:  46%|███████▊         | 21/46 [00:05<00:06,  3.72it/s]evaluate for the 22-th batch, evaluate loss: 0.49491187930107117:  48%|████████▏        | 22/46 [00:05<00:06,  3.88it/s]Epoch: 6, train for the 8-th batch, train loss: 0.3930920362472534:   5%|▋              | 7/146 [00:04<01:19,  1.76it/s]Epoch: 6, train for the 8-th batch, train loss: 0.3930920362472534:   5%|▊              | 8/146 [00:04<01:17,  1.78it/s]evaluate for the 44-th batch, evaluate loss: 0.4153406322002411:  41%|██████▉          | 43/106 [00:12<00:17,  3.57it/s]evaluate for the 44-th batch, evaluate loss: 0.4153406322002411:  42%|███████          | 44/106 [00:12<00:17,  3.54it/s]evaluate for the 23-th batch, evaluate loss: 0.5141726732254028:  48%|████████▌         | 22/46 [00:05<00:06,  3.88it/s]evaluate for the 23-th batch, evaluate loss: 0.5141726732254028:  50%|█████████         | 23/46 [00:05<00:05,  3.86it/s]evaluate for the 35-th batch, evaluate loss: 0.155632883310318:  85%|████████████████▏  | 34/40 [00:09<00:01,  3.66it/s]evaluate for the 35-th batch, evaluate loss: 0.155632883310318:  88%|████████████████▋  | 35/40 [00:09<00:01,  3.53it/s]Epoch: 4, train for the 30-th batch, train loss: 0.690159261226654:  12%|█▋            | 29/237 [00:15<01:56,  1.78it/s]Epoch: 4, train for the 30-th batch, train loss: 0.690159261226654:  13%|█▊            | 30/237 [00:15<01:53,  1.83it/s]evaluate for the 45-th batch, evaluate loss: 0.45291757583618164:  42%|██████▋         | 44/106 [00:12<00:17,  3.54it/s]evaluate for the 45-th batch, evaluate loss: 0.45291757583618164:  42%|██████▊         | 45/106 [00:12<00:16,  3.62it/s]evaluate for the 36-th batch, evaluate loss: 0.14937268197536469:  88%|██████████████▉  | 35/40 [00:09<00:01,  3.53it/s]evaluate for the 36-th batch, evaluate loss: 0.14937268197536469:  90%|███████████████▎ | 36/40 [00:09<00:01,  3.67it/s]evaluate for the 24-th batch, evaluate loss: 0.5135449767112732:  50%|█████████         | 23/46 [00:06<00:05,  3.86it/s]evaluate for the 24-th batch, evaluate loss: 0.5135449767112732:  52%|█████████▍        | 24/46 [00:06<00:05,  3.70it/s]evaluate for the 25-th batch, evaluate loss: 0.5237743854522705:  52%|█████████▍        | 24/46 [00:06<00:05,  3.70it/s]evaluate for the 25-th batch, evaluate loss: 0.5237743854522705:  54%|█████████▊        | 25/46 [00:06<00:05,  3.96it/s]Epoch: 6, train for the 9-th batch, train loss: 0.3479713499546051:   5%|▊              | 8/146 [00:04<01:17,  1.78it/s]Epoch: 6, train for the 9-th batch, train loss: 0.3479713499546051:   6%|▉              | 9/146 [00:04<01:17,  1.77it/s]evaluate for the 46-th batch, evaluate loss: 0.47308191657066345:  42%|██████▊         | 45/106 [00:12<00:16,  3.62it/s]evaluate for the 46-th batch, evaluate loss: 0.47308191657066345:  43%|██████▉         | 46/106 [00:12<00:16,  3.59it/s]evaluate for the 37-th batch, evaluate loss: 0.20155403017997742:  90%|███████████████▎ | 36/40 [00:10<00:01,  3.67it/s]evaluate for the 37-th batch, evaluate loss: 0.20155403017997742:  92%|███████████████▋ | 37/40 [00:10<00:00,  3.52it/s]Epoch: 4, train for the 31-th batch, train loss: 0.4357776939868927:  13%|█▋           | 30/237 [00:15<01:53,  1.83it/s]Epoch: 4, train for the 31-th batch, train loss: 0.4357776939868927:  13%|█▋           | 31/237 [00:15<01:52,  1.82it/s]evaluate for the 47-th batch, evaluate loss: 0.35156598687171936:  43%|██████▉         | 46/106 [00:12<00:16,  3.59it/s]evaluate for the 47-th batch, evaluate loss: 0.35156598687171936:  44%|███████         | 47/106 [00:12<00:16,  3.67it/s]evaluate for the 38-th batch, evaluate loss: 0.13694819808006287:  92%|███████████████▋ | 37/40 [00:10<00:00,  3.52it/s]evaluate for the 38-th batch, evaluate loss: 0.13694819808006287:  95%|████████████████▏| 38/40 [00:10<00:00,  3.63it/s]evaluate for the 26-th batch, evaluate loss: 0.5330073237419128:  54%|█████████▊        | 25/46 [00:06<00:05,  3.96it/s]evaluate for the 26-th batch, evaluate loss: 0.5330073237419128:  57%|██████████▏       | 26/46 [00:06<00:05,  3.70it/s]evaluate for the 48-th batch, evaluate loss: 0.4662342667579651:  44%|███████▌         | 47/106 [00:13<00:16,  3.67it/s]evaluate for the 48-th batch, evaluate loss: 0.4662342667579651:  45%|███████▋         | 48/106 [00:13<00:15,  3.64it/s]Epoch: 6, train for the 10-th batch, train loss: 0.3687978982925415:   6%|▊             | 9/146 [00:05<01:17,  1.77it/s]Epoch: 6, train for the 10-th batch, train loss: 0.3687978982925415:   7%|▉            | 10/146 [00:05<01:15,  1.79it/s]evaluate for the 27-th batch, evaluate loss: 0.4888201355934143:  57%|██████████▏       | 26/46 [00:07<00:05,  3.70it/s]evaluate for the 27-th batch, evaluate loss: 0.4888201355934143:  59%|██████████▌       | 27/46 [00:07<00:05,  3.80it/s]evaluate for the 39-th batch, evaluate loss: 0.15037497878074646:  95%|████████████████▏| 38/40 [00:10<00:00,  3.63it/s]evaluate for the 39-th batch, evaluate loss: 0.15037497878074646:  98%|████████████████▌| 39/40 [00:10<00:00,  3.51it/s]evaluate for the 40-th batch, evaluate loss: 0.054980237036943436:  98%|███████████████▌| 39/40 [00:10<00:00,  3.51it/s]evaluate for the 40-th batch, evaluate loss: 0.054980237036943436: 100%|████████████████| 40/40 [00:10<00:00,  3.78it/s]
  0%|                                                                                            | 0/21 [00:00<?, ?it/s]Epoch: 4, train for the 32-th batch, train loss: 0.6632260084152222:  13%|█▋           | 31/237 [00:16<01:52,  1.82it/s]Epoch: 4, train for the 32-th batch, train loss: 0.6632260084152222:  14%|█▊           | 32/237 [00:16<01:55,  1.78it/s]evaluate for the 28-th batch, evaluate loss: 0.47505760192871094:  59%|█████████▉       | 27/46 [00:07<00:05,  3.80it/s]evaluate for the 28-th batch, evaluate loss: 0.47505760192871094:  61%|██████████▎      | 28/46 [00:07<00:04,  3.80it/s]evaluate for the 49-th batch, evaluate loss: 0.48848700523376465:  45%|███████▏        | 48/106 [00:13<00:15,  3.64it/s]evaluate for the 49-th batch, evaluate loss: 0.48848700523376465:  46%|███████▍        | 49/106 [00:13<00:15,  3.62it/s]evaluate for the 1-th batch, evaluate loss: 0.20716525614261627:   0%|                           | 0/21 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.20716525614261627:   5%|▉                  | 1/21 [00:00<00:05,  3.78it/s]Epoch: 6, train for the 11-th batch, train loss: 0.4194256067276001:   7%|▉            | 10/146 [00:06<01:15,  1.79it/s]Epoch: 6, train for the 11-th batch, train loss: 0.4194256067276001:   8%|▉            | 11/146 [00:06<01:13,  1.84it/s]evaluate for the 29-th batch, evaluate loss: 0.5352681875228882:  61%|██████████▉       | 28/46 [00:07<00:04,  3.80it/s]evaluate for the 29-th batch, evaluate loss: 0.5352681875228882:  63%|███████████▎      | 29/46 [00:07<00:04,  3.87it/s]evaluate for the 50-th batch, evaluate loss: 0.39565229415893555:  46%|███████▍        | 49/106 [00:13<00:15,  3.62it/s]evaluate for the 50-th batch, evaluate loss: 0.39565229415893555:  47%|███████▌        | 50/106 [00:13<00:15,  3.67it/s]evaluate for the 2-th batch, evaluate loss: 0.23410269618034363:   5%|▉                  | 1/21 [00:00<00:05,  3.78it/s]evaluate for the 2-th batch, evaluate loss: 0.23410269618034363:  10%|█▊                 | 2/21 [00:00<00:05,  3.49it/s]Epoch: 4, train for the 33-th batch, train loss: 0.3973720371723175:  14%|█▊           | 32/237 [00:17<01:55,  1.78it/s]Epoch: 4, train for the 33-th batch, train loss: 0.3973720371723175:  14%|█▊           | 33/237 [00:17<01:54,  1.77it/s]evaluate for the 30-th batch, evaluate loss: 0.48742690682411194:  63%|██████████▋      | 29/46 [00:07<00:04,  3.87it/s]evaluate for the 30-th batch, evaluate loss: 0.48742690682411194:  65%|███████████      | 30/46 [00:07<00:04,  3.94it/s]evaluate for the 51-th batch, evaluate loss: 0.49103742837905884:  47%|███████▌        | 50/106 [00:14<00:15,  3.67it/s]evaluate for the 51-th batch, evaluate loss: 0.49103742837905884:  48%|███████▋        | 51/106 [00:14<00:15,  3.58it/s]evaluate for the 3-th batch, evaluate loss: 0.24297793209552765:  10%|█▊                 | 2/21 [00:00<00:05,  3.49it/s]evaluate for the 3-th batch, evaluate loss: 0.24297793209552765:  14%|██▋                | 3/21 [00:00<00:05,  3.53it/s]evaluate for the 31-th batch, evaluate loss: 0.5378192663192749:  65%|███████████▋      | 30/46 [00:08<00:04,  3.94it/s]evaluate for the 31-th batch, evaluate loss: 0.5378192663192749:  67%|████████████▏     | 31/46 [00:08<00:04,  3.72it/s]Epoch: 6, train for the 12-th batch, train loss: 0.4229530692100525:   8%|▉            | 11/146 [00:06<01:13,  1.84it/s]Epoch: 6, train for the 12-th batch, train loss: 0.4229530692100525:   8%|█            | 12/146 [00:06<01:14,  1.81it/s]evaluate for the 52-th batch, evaluate loss: 0.43643468618392944:  48%|███████▋        | 51/106 [00:14<00:15,  3.58it/s]evaluate for the 52-th batch, evaluate loss: 0.43643468618392944:  49%|███████▊        | 52/106 [00:14<00:14,  3.64it/s]evaluate for the 4-th batch, evaluate loss: 0.2087869644165039:  14%|██▊                 | 3/21 [00:01<00:05,  3.53it/s]evaluate for the 4-th batch, evaluate loss: 0.2087869644165039:  19%|███▊                | 4/21 [00:01<00:04,  3.42it/s]Epoch: 4, train for the 34-th batch, train loss: 0.3903886377811432:  14%|█▊           | 33/237 [00:17<01:54,  1.77it/s]Epoch: 4, train for the 34-th batch, train loss: 0.3903886377811432:  14%|█▊           | 34/237 [00:17<01:53,  1.78it/s]evaluate for the 32-th batch, evaluate loss: 0.5596917867660522:  67%|████████████▏     | 31/46 [00:08<00:04,  3.72it/s]evaluate for the 32-th batch, evaluate loss: 0.5596917867660522:  70%|████████████▌     | 32/46 [00:08<00:03,  3.89it/s]evaluate for the 53-th batch, evaluate loss: 0.545409083366394:  49%|████████▊         | 52/106 [00:14<00:14,  3.64it/s]evaluate for the 53-th batch, evaluate loss: 0.545409083366394:  50%|█████████         | 53/106 [00:14<00:14,  3.57it/s]evaluate for the 5-th batch, evaluate loss: 0.21098698675632477:  19%|███▌               | 4/21 [00:01<00:04,  3.42it/s]evaluate for the 5-th batch, evaluate loss: 0.21098698675632477:  24%|████▌              | 5/21 [00:01<00:04,  3.38it/s]evaluate for the 33-th batch, evaluate loss: 0.5188100337982178:  70%|████████████▌     | 32/46 [00:08<00:03,  3.89it/s]evaluate for the 33-th batch, evaluate loss: 0.5188100337982178:  72%|████████████▉     | 33/46 [00:08<00:03,  3.71it/s]evaluate for the 54-th batch, evaluate loss: 0.3831140697002411:  50%|████████▌        | 53/106 [00:14<00:14,  3.57it/s]evaluate for the 54-th batch, evaluate loss: 0.3831140697002411:  51%|████████▋        | 54/106 [00:14<00:14,  3.61it/s]Epoch: 6, train for the 13-th batch, train loss: 0.3955196440219879:   8%|█            | 12/146 [00:07<01:14,  1.81it/s]Epoch: 6, train for the 13-th batch, train loss: 0.3955196440219879:   9%|█▏           | 13/146 [00:07<01:14,  1.78it/s]evaluate for the 34-th batch, evaluate loss: 0.47582679986953735:  72%|████████████▏    | 33/46 [00:08<00:03,  3.71it/s]evaluate for the 34-th batch, evaluate loss: 0.47582679986953735:  74%|████████████▌    | 34/46 [00:08<00:03,  3.90it/s]Epoch: 4, train for the 35-th batch, train loss: 0.41231077909469604:  14%|█▋          | 34/237 [00:18<01:53,  1.78it/s]Epoch: 4, train for the 35-th batch, train loss: 0.41231077909469604:  15%|█▊          | 35/237 [00:18<01:53,  1.78it/s]evaluate for the 6-th batch, evaluate loss: 0.24246519804000854:  24%|████▌              | 5/21 [00:01<00:04,  3.38it/s]evaluate for the 6-th batch, evaluate loss: 0.24246519804000854:  29%|█████▍             | 6/21 [00:01<00:04,  3.42it/s]evaluate for the 55-th batch, evaluate loss: 0.3424428403377533:  51%|████████▋        | 54/106 [00:15<00:14,  3.61it/s]evaluate for the 55-th batch, evaluate loss: 0.3424428403377533:  52%|████████▊        | 55/106 [00:15<00:14,  3.55it/s]evaluate for the 35-th batch, evaluate loss: 0.53839111328125:  74%|██████████████▊     | 34/46 [00:09<00:03,  3.90it/s]evaluate for the 35-th batch, evaluate loss: 0.53839111328125:  76%|███████████████▏    | 35/46 [00:09<00:02,  3.86it/s]evaluate for the 7-th batch, evaluate loss: 0.23457446694374084:  29%|█████▍             | 6/21 [00:02<00:04,  3.42it/s]evaluate for the 7-th batch, evaluate loss: 0.23457446694374084:  33%|██████▎            | 7/21 [00:02<00:04,  3.39it/s]evaluate for the 56-th batch, evaluate loss: 0.46524810791015625:  52%|████████▎       | 55/106 [00:15<00:14,  3.55it/s]evaluate for the 56-th batch, evaluate loss: 0.46524810791015625:  53%|████████▍       | 56/106 [00:15<00:13,  3.61it/s]Epoch: 6, train for the 14-th batch, train loss: 0.40869542956352234:   9%|█           | 13/146 [00:07<01:14,  1.78it/s]Epoch: 6, train for the 14-th batch, train loss: 0.40869542956352234:  10%|█▏          | 14/146 [00:07<01:15,  1.74it/s]evaluate for the 36-th batch, evaluate loss: 0.4862384796142578:  76%|█████████████▋    | 35/46 [00:09<00:02,  3.86it/s]evaluate for the 36-th batch, evaluate loss: 0.4862384796142578:  78%|██████████████    | 36/46 [00:09<00:02,  3.69it/s]Epoch: 4, train for the 36-th batch, train loss: 0.40291595458984375:  15%|█▊          | 35/237 [00:18<01:53,  1.78it/s]Epoch: 4, train for the 36-th batch, train loss: 0.40291595458984375:  15%|█▊          | 36/237 [00:18<01:52,  1.78it/s]evaluate for the 8-th batch, evaluate loss: 0.2773418724536896:  33%|██████▋             | 7/21 [00:02<00:04,  3.39it/s]evaluate for the 8-th batch, evaluate loss: 0.2773418724536896:  38%|███████▌            | 8/21 [00:02<00:03,  3.39it/s]evaluate for the 57-th batch, evaluate loss: 0.41711539030075073:  53%|████████▍       | 56/106 [00:15<00:13,  3.61it/s]evaluate for the 57-th batch, evaluate loss: 0.41711539030075073:  54%|████████▌       | 57/106 [00:15<00:13,  3.55it/s]evaluate for the 37-th batch, evaluate loss: 0.5149237513542175:  78%|██████████████    | 36/46 [00:09<00:02,  3.69it/s]evaluate for the 37-th batch, evaluate loss: 0.5149237513542175:  80%|██████████████▍   | 37/46 [00:09<00:02,  3.96it/s]evaluate for the 9-th batch, evaluate loss: 0.21151305735111237:  38%|███████▏           | 8/21 [00:02<00:03,  3.39it/s]evaluate for the 9-th batch, evaluate loss: 0.21151305735111237:  43%|████████▏          | 9/21 [00:02<00:03,  3.37it/s]evaluate for the 58-th batch, evaluate loss: 0.4357464611530304:  54%|█████████▏       | 57/106 [00:15<00:13,  3.55it/s]evaluate for the 58-th batch, evaluate loss: 0.4357464611530304:  55%|█████████▎       | 58/106 [00:15<00:13,  3.63it/s]Epoch: 6, train for the 15-th batch, train loss: 0.41996505856513977:  10%|█▏          | 14/146 [00:08<01:15,  1.74it/s]Epoch: 6, train for the 15-th batch, train loss: 0.41996505856513977:  10%|█▏          | 15/146 [00:08<01:14,  1.75it/s]evaluate for the 38-th batch, evaluate loss: 0.48064836859703064:  80%|█████████████▋   | 37/46 [00:09<00:02,  3.96it/s]evaluate for the 38-th batch, evaluate loss: 0.48064836859703064:  83%|██████████████   | 38/46 [00:09<00:02,  3.69it/s]Epoch: 4, train for the 37-th batch, train loss: 0.33007580041885376:  15%|█▊          | 36/237 [00:19<01:52,  1.78it/s]Epoch: 4, train for the 37-th batch, train loss: 0.33007580041885376:  16%|█▊          | 37/237 [00:19<01:51,  1.80it/s]evaluate for the 10-th batch, evaluate loss: 0.2135283648967743:  43%|████████▏          | 9/21 [00:02<00:03,  3.37it/s]evaluate for the 10-th batch, evaluate loss: 0.2135283648967743:  48%|████████▌         | 10/21 [00:02<00:03,  3.44it/s]evaluate for the 59-th batch, evaluate loss: 0.4361763596534729:  55%|█████████▎       | 58/106 [00:16<00:13,  3.63it/s]evaluate for the 59-th batch, evaluate loss: 0.4361763596534729:  56%|█████████▍       | 59/106 [00:16<00:13,  3.58it/s]evaluate for the 39-th batch, evaluate loss: 0.49257954955101013:  83%|██████████████   | 38/46 [00:10<00:02,  3.69it/s]evaluate for the 39-th batch, evaluate loss: 0.49257954955101013:  85%|██████████████▍  | 39/46 [00:10<00:01,  3.79it/s]evaluate for the 11-th batch, evaluate loss: 0.14393962919712067:  48%|████████         | 10/21 [00:03<00:03,  3.44it/s]evaluate for the 11-th batch, evaluate loss: 0.14393962919712067:  52%|████████▉        | 11/21 [00:03<00:02,  3.43it/s]evaluate for the 60-th batch, evaluate loss: 0.4429265260696411:  56%|█████████▍       | 59/106 [00:16<00:13,  3.58it/s]evaluate for the 60-th batch, evaluate loss: 0.4429265260696411:  57%|█████████▌       | 60/106 [00:16<00:12,  3.61it/s]evaluate for the 40-th batch, evaluate loss: 0.47669264674186707:  85%|██████████████▍  | 39/46 [00:10<00:01,  3.79it/s]evaluate for the 40-th batch, evaluate loss: 0.47669264674186707:  87%|██████████████▊  | 40/46 [00:10<00:01,  3.78it/s]Epoch: 6, train for the 16-th batch, train loss: 0.40753450989723206:  10%|█▏          | 15/146 [00:08<01:14,  1.75it/s]Epoch: 6, train for the 16-th batch, train loss: 0.40753450989723206:  11%|█▎          | 16/146 [00:08<01:15,  1.73it/s]Epoch: 4, train for the 38-th batch, train loss: 0.3845582604408264:  16%|██           | 37/237 [00:19<01:51,  1.80it/s]Epoch: 4, train for the 38-th batch, train loss: 0.3845582604408264:  16%|██           | 38/237 [00:19<01:51,  1.79it/s]evaluate for the 12-th batch, evaluate loss: 0.27225181460380554:  52%|████████▉        | 11/21 [00:03<00:02,  3.43it/s]evaluate for the 12-th batch, evaluate loss: 0.27225181460380554:  57%|█████████▋       | 12/21 [00:03<00:02,  3.43it/s]evaluate for the 61-th batch, evaluate loss: 0.49692413210868835:  57%|█████████       | 60/106 [00:16<00:12,  3.61it/s]evaluate for the 61-th batch, evaluate loss: 0.49692413210868835:  58%|█████████▏      | 61/106 [00:16<00:12,  3.56it/s]evaluate for the 41-th batch, evaluate loss: 0.5378094911575317:  87%|███████████████▋  | 40/46 [00:10<00:01,  3.78it/s]evaluate for the 41-th batch, evaluate loss: 0.5378094911575317:  89%|████████████████  | 41/46 [00:10<00:01,  3.86it/s]evaluate for the 13-th batch, evaluate loss: 0.22607623040676117:  57%|█████████▋       | 12/21 [00:03<00:02,  3.43it/s]evaluate for the 13-th batch, evaluate loss: 0.22607623040676117:  62%|██████████▌      | 13/21 [00:03<00:02,  3.42it/s]evaluate for the 62-th batch, evaluate loss: 0.4590457081794739:  58%|█████████▊       | 61/106 [00:17<00:12,  3.56it/s]evaluate for the 62-th batch, evaluate loss: 0.4590457081794739:  58%|█████████▉       | 62/106 [00:17<00:12,  3.61it/s]evaluate for the 42-th batch, evaluate loss: 0.4979962110519409:  89%|████████████████  | 41/46 [00:10<00:01,  3.86it/s]evaluate for the 42-th batch, evaluate loss: 0.4979962110519409:  91%|████████████████▍ | 42/46 [00:10<00:01,  3.93it/s]Epoch: 6, train for the 17-th batch, train loss: 0.4214048385620117:  11%|█▍           | 16/146 [00:09<01:15,  1.73it/s]Epoch: 6, train for the 17-th batch, train loss: 0.4214048385620117:  12%|█▌           | 17/146 [00:09<01:13,  1.75it/s]Epoch: 4, train for the 39-th batch, train loss: 0.744327962398529:  16%|██▏           | 38/237 [00:20<01:51,  1.79it/s]Epoch: 4, train for the 39-th batch, train loss: 0.744327962398529:  16%|██▎           | 39/237 [00:20<01:52,  1.76it/s]evaluate for the 14-th batch, evaluate loss: 0.1910308450460434:  62%|███████████▏      | 13/21 [00:04<00:02,  3.42it/s]evaluate for the 14-th batch, evaluate loss: 0.1910308450460434:  67%|████████████      | 14/21 [00:04<00:02,  3.49it/s]evaluate for the 63-th batch, evaluate loss: 0.46708860993385315:  58%|█████████▎      | 62/106 [00:17<00:12,  3.61it/s]evaluate for the 63-th batch, evaluate loss: 0.46708860993385315:  59%|█████████▌      | 63/106 [00:17<00:12,  3.53it/s]evaluate for the 43-th batch, evaluate loss: 0.44368597865104675:  91%|███████████████▌ | 42/46 [00:11<00:01,  3.93it/s]evaluate for the 43-th batch, evaluate loss: 0.44368597865104675:  93%|███████████████▉ | 43/46 [00:11<00:00,  3.72it/s]evaluate for the 44-th batch, evaluate loss: 0.5067737102508545:  93%|████████████████▊ | 43/46 [00:11<00:00,  3.72it/s]evaluate for the 44-th batch, evaluate loss: 0.5067737102508545:  96%|█████████████████▏| 44/46 [00:11<00:00,  3.89it/s]evaluate for the 64-th batch, evaluate loss: 0.4086163341999054:  59%|██████████       | 63/106 [00:17<00:12,  3.53it/s]evaluate for the 64-th batch, evaluate loss: 0.4086163341999054:  60%|██████████▎      | 64/106 [00:17<00:11,  3.67it/s]evaluate for the 15-th batch, evaluate loss: 0.1861831694841385:  67%|████████████      | 14/21 [00:04<00:02,  3.49it/s]evaluate for the 15-th batch, evaluate loss: 0.1861831694841385:  71%|████████████▊     | 15/21 [00:04<00:01,  3.46it/s]Epoch: 6, train for the 18-th batch, train loss: 0.412062406539917:  12%|█▋            | 17/146 [00:10<01:13,  1.75it/s]Epoch: 6, train for the 18-th batch, train loss: 0.412062406539917:  12%|█▋            | 18/146 [00:10<01:14,  1.73it/s]Epoch: 4, train for the 40-th batch, train loss: 0.5191250443458557:  16%|██▏          | 39/237 [00:20<01:52,  1.76it/s]Epoch: 4, train for the 40-th batch, train loss: 0.5191250443458557:  17%|██▏          | 40/237 [00:20<01:51,  1.76it/s]evaluate for the 65-th batch, evaluate loss: 0.5113419890403748:  60%|██████████▎      | 64/106 [00:17<00:11,  3.67it/s]evaluate for the 65-th batch, evaluate loss: 0.5113419890403748:  61%|██████████▍      | 65/106 [00:17<00:11,  3.58it/s]evaluate for the 16-th batch, evaluate loss: 0.2326483428478241:  71%|████████████▊     | 15/21 [00:04<00:01,  3.46it/s]evaluate for the 45-th batch, evaluate loss: 0.5350384712219238:  96%|█████████████████▏| 44/46 [00:11<00:00,  3.89it/s]evaluate for the 16-th batch, evaluate loss: 0.2326483428478241:  76%|█████████████▋    | 16/21 [00:04<00:01,  3.47it/s]evaluate for the 45-th batch, evaluate loss: 0.5350384712219238:  98%|█████████████████▌| 45/46 [00:11<00:00,  3.71it/s]evaluate for the 46-th batch, evaluate loss: 0.5760369300842285:  98%|█████████████████▌| 45/46 [00:11<00:00,  3.71it/s]evaluate for the 46-th batch, evaluate loss: 0.5760369300842285: 100%|██████████████████| 46/46 [00:11<00:00,  4.04it/s]evaluate for the 46-th batch, evaluate loss: 0.5760369300842285: 100%|██████████████████| 46/46 [00:11<00:00,  3.84it/s]
  0%|                                                                                            | 0/26 [00:00<?, ?it/s]evaluate for the 66-th batch, evaluate loss: 0.424954354763031:  61%|███████████       | 65/106 [00:18<00:11,  3.58it/s]evaluate for the 66-th batch, evaluate loss: 0.424954354763031:  62%|███████████▏      | 66/106 [00:18<00:10,  3.72it/s]evaluate for the 17-th batch, evaluate loss: 0.22888323664665222:  76%|████████████▉    | 16/21 [00:04<00:01,  3.47it/s]evaluate for the 17-th batch, evaluate loss: 0.22888323664665222:  81%|█████████████▊   | 17/21 [00:04<00:01,  3.42it/s]Epoch: 6, train for the 19-th batch, train loss: 0.40720248222351074:  12%|█▍          | 18/146 [00:10<01:14,  1.73it/s]Epoch: 6, train for the 19-th batch, train loss: 0.40720248222351074:  13%|█▌          | 19/146 [00:10<01:13,  1.73it/s]evaluate for the 1-th batch, evaluate loss: 0.666134238243103:   0%|                             | 0/26 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.666134238243103:   4%|▊                    | 1/26 [00:00<00:06,  3.70it/s]Epoch: 4, train for the 41-th batch, train loss: 0.5606942176818848:  17%|██▏          | 40/237 [00:21<01:51,  1.76it/s]Epoch: 4, train for the 41-th batch, train loss: 0.5606942176818848:  17%|██▏          | 41/237 [00:21<01:51,  1.76it/s]evaluate for the 67-th batch, evaluate loss: 0.40909543633461:  62%|███████████▊       | 66/106 [00:18<00:10,  3.72it/s]evaluate for the 67-th batch, evaluate loss: 0.40909543633461:  63%|████████████       | 67/106 [00:18<00:10,  3.60it/s]evaluate for the 18-th batch, evaluate loss: 0.20906436443328857:  81%|█████████████▊   | 17/21 [00:05<00:01,  3.42it/s]evaluate for the 18-th batch, evaluate loss: 0.20906436443328857:  86%|██████████████▌  | 18/21 [00:05<00:00,  3.47it/s]evaluate for the 2-th batch, evaluate loss: 0.7769165635108948:   4%|▊                   | 1/26 [00:00<00:06,  3.70it/s]evaluate for the 2-th batch, evaluate loss: 0.7769165635108948:   8%|█▌                  | 2/26 [00:00<00:06,  3.45it/s]evaluate for the 68-th batch, evaluate loss: 0.47658953070640564:  63%|██████████      | 67/106 [00:18<00:10,  3.60it/s]evaluate for the 68-th batch, evaluate loss: 0.47658953070640564:  64%|██████████▎     | 68/106 [00:18<00:10,  3.74it/s]evaluate for the 19-th batch, evaluate loss: 0.26640579104423523:  86%|██████████████▌  | 18/21 [00:05<00:00,  3.47it/s]evaluate for the 19-th batch, evaluate loss: 0.26640579104423523:  90%|███████████████▍ | 19/21 [00:05<00:00,  3.44it/s]evaluate for the 3-th batch, evaluate loss: 0.6870236992835999:   8%|█▌                  | 2/26 [00:00<00:06,  3.45it/s]evaluate for the 3-th batch, evaluate loss: 0.6870236992835999:  12%|██▎                 | 3/26 [00:00<00:05,  3.89it/s]Epoch: 6, train for the 20-th batch, train loss: 0.41479241847991943:  13%|█▌          | 19/146 [00:11<01:13,  1.73it/s]Epoch: 6, train for the 20-th batch, train loss: 0.41479241847991943:  14%|█▋          | 20/146 [00:11<01:13,  1.72it/s]Epoch: 4, train for the 42-th batch, train loss: 0.5481643676757812:  17%|██▏          | 41/237 [00:22<01:51,  1.76it/s]Epoch: 4, train for the 42-th batch, train loss: 0.5481643676757812:  18%|██▎          | 42/237 [00:22<01:51,  1.75it/s]evaluate for the 69-th batch, evaluate loss: 0.43782997131347656:  64%|██████████▎     | 68/106 [00:19<00:10,  3.74it/s]evaluate for the 69-th batch, evaluate loss: 0.43782997131347656:  65%|██████████▍     | 69/106 [00:19<00:10,  3.58it/s]evaluate for the 20-th batch, evaluate loss: 0.23652338981628418:  90%|███████████████▍ | 19/21 [00:05<00:00,  3.44it/s]evaluate for the 20-th batch, evaluate loss: 0.23652338981628418:  95%|████████████████▏| 20/21 [00:05<00:00,  3.43it/s]evaluate for the 21-th batch, evaluate loss: 0.11171690374612808:  95%|████████████████▏| 20/21 [00:05<00:00,  3.43it/s]evaluate for the 21-th batch, evaluate loss: 0.11171690374612808: 100%|█████████████████| 21/21 [00:05<00:00,  3.59it/s]
INFO:root:Epoch: 6, learning rate: 0.0001, train loss: 0.2122
INFO:root:train average_precision, 0.9708
INFO:root:train roc_auc, 0.9608
INFO:root:validate loss: 0.1489
INFO:root:validate average_precision, 0.9860
INFO:root:validate roc_auc, 0.9833
INFO:root:new node validate loss: 0.2185
INFO:root:new node validate first_1_average_precision, 0.9165
INFO:root:new node validate first_1_roc_auc, 0.9216
INFO:root:new node validate first_3_average_precision, 0.9530
INFO:root:new node validate first_3_roc_auc, 0.9535
INFO:root:new node validate first_10_average_precision, 0.9697
INFO:root:new node validate first_10_roc_auc, 0.9682
INFO:root:new node validate average_precision, 0.9709
INFO:root:new node validate roc_auc, 0.9677
INFO:root:save model ./saved_models/DyGFormer/ia-reality-call/DyGFormer_seed0_dygformer-ia-reality-call-old/DyGFormer_seed0_dygformer-ia-reality-call-old.pkl
evaluate for the 4-th batch, evaluate loss: 0.6907169818878174:  12%|██▎                 | 3/26 [00:01<00:05,  3.89it/s]evaluate for the 4-th batch, evaluate loss: 0.6907169818878174:  15%|███                 | 4/26 [00:01<00:06,  3.56it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]evaluate for the 70-th batch, evaluate loss: 0.39200538396835327:  65%|██████████▍     | 69/106 [00:19<00:10,  3.58it/s]evaluate for the 70-th batch, evaluate loss: 0.39200538396835327:  66%|██████████▌     | 70/106 [00:19<00:09,  3.68it/s]Epoch: 6, train for the 21-th batch, train loss: 0.4454798102378845:  14%|█▊           | 20/146 [00:11<01:13,  1.72it/s]Epoch: 6, train for the 21-th batch, train loss: 0.4454798102378845:  14%|█▊           | 21/146 [00:11<01:03,  1.97it/s]evaluate for the 5-th batch, evaluate loss: 0.718900740146637:  15%|███▏                 | 4/26 [00:01<00:06,  3.56it/s]evaluate for the 5-th batch, evaluate loss: 0.718900740146637:  19%|████                 | 5/26 [00:01<00:05,  3.74it/s]Epoch: 4, train for the 43-th batch, train loss: 0.38287651538848877:  18%|██▏         | 42/237 [00:22<01:51,  1.75it/s]Epoch: 4, train for the 43-th batch, train loss: 0.38287651538848877:  18%|██▏         | 43/237 [00:22<01:50,  1.75it/s]evaluate for the 71-th batch, evaluate loss: 0.4018395245075226:  66%|███████████▏     | 70/106 [00:19<00:09,  3.68it/s]evaluate for the 71-th batch, evaluate loss: 0.4018395245075226:  67%|███████████▍     | 71/106 [00:19<00:09,  3.54it/s]evaluate for the 6-th batch, evaluate loss: 0.6539630889892578:  19%|███▊                | 5/26 [00:01<00:05,  3.74it/s]evaluate for the 6-th batch, evaluate loss: 0.6539630889892578:  23%|████▌               | 6/26 [00:01<00:05,  3.64it/s]Epoch: 7, train for the 1-th batch, train loss: 1.0752501487731934:   0%|                       | 0/119 [00:00<?, ?it/s]Epoch: 7, train for the 1-th batch, train loss: 1.0752501487731934:   1%|▏              | 1/119 [00:00<01:06,  1.79it/s]evaluate for the 72-th batch, evaluate loss: 0.3712151348590851:  67%|███████████▍     | 71/106 [00:19<00:09,  3.54it/s]evaluate for the 72-th batch, evaluate loss: 0.3712151348590851:  68%|███████████▌     | 72/106 [00:19<00:09,  3.64it/s]Epoch: 6, train for the 22-th batch, train loss: 0.4055398404598236:  14%|█▊           | 21/146 [00:12<01:03,  1.97it/s]Epoch: 6, train for the 22-th batch, train loss: 0.4055398404598236:  15%|█▉           | 22/146 [00:12<01:06,  1.85it/s]evaluate for the 7-th batch, evaluate loss: 0.6796427965164185:  23%|████▌               | 6/26 [00:01<00:05,  3.64it/s]evaluate for the 7-th batch, evaluate loss: 0.6796427965164185:  27%|█████▍              | 7/26 [00:01<00:04,  3.82it/s]Epoch: 4, train for the 44-th batch, train loss: 0.36118876934051514:  18%|██▏         | 43/237 [00:23<01:50,  1.75it/s]Epoch: 4, train for the 44-th batch, train loss: 0.36118876934051514:  19%|██▏         | 44/237 [00:23<01:50,  1.75it/s]evaluate for the 73-th batch, evaluate loss: 0.425586074590683:  68%|████████████▏     | 72/106 [00:20<00:09,  3.64it/s]evaluate for the 73-th batch, evaluate loss: 0.425586074590683:  69%|████████████▍     | 73/106 [00:20<00:09,  3.51it/s]evaluate for the 8-th batch, evaluate loss: 0.6944060921669006:  27%|█████▍              | 7/26 [00:02<00:04,  3.82it/s]evaluate for the 8-th batch, evaluate loss: 0.6944060921669006:  31%|██████▏             | 8/26 [00:02<00:04,  3.80it/s]evaluate for the 74-th batch, evaluate loss: 0.5012474060058594:  69%|███████████▋     | 73/106 [00:20<00:09,  3.51it/s]evaluate for the 74-th batch, evaluate loss: 0.5012474060058594:  70%|███████████▊     | 74/106 [00:20<00:08,  3.61it/s]Epoch: 7, train for the 2-th batch, train loss: 0.5134037137031555:   1%|▏              | 1/119 [00:01<01:06,  1.79it/s]Epoch: 7, train for the 2-th batch, train loss: 0.5134037137031555:   2%|▎              | 2/119 [00:01<01:08,  1.71it/s]Epoch: 6, train for the 23-th batch, train loss: 0.43623077869415283:  15%|█▊          | 22/146 [00:12<01:06,  1.85it/s]Epoch: 6, train for the 23-th batch, train loss: 0.43623077869415283:  16%|█▉          | 23/146 [00:12<01:08,  1.80it/s]evaluate for the 9-th batch, evaluate loss: 0.6684240698814392:  31%|██████▏             | 8/26 [00:02<00:04,  3.80it/s]evaluate for the 9-th batch, evaluate loss: 0.6684240698814392:  35%|██████▉             | 9/26 [00:02<00:04,  3.69it/s]Epoch: 4, train for the 45-th batch, train loss: 0.3370248079299927:  19%|██▍          | 44/237 [00:23<01:50,  1.75it/s]Epoch: 4, train for the 45-th batch, train loss: 0.3370248079299927:  19%|██▍          | 45/237 [00:23<01:48,  1.76it/s]evaluate for the 75-th batch, evaluate loss: 0.4154317378997803:  70%|███████████▊     | 74/106 [00:20<00:08,  3.61it/s]evaluate for the 75-th batch, evaluate loss: 0.4154317378997803:  71%|████████████     | 75/106 [00:20<00:08,  3.50it/s]evaluate for the 10-th batch, evaluate loss: 0.6104385852813721:  35%|██████▌            | 9/26 [00:02<00:04,  3.69it/s]evaluate for the 10-th batch, evaluate loss: 0.6104385852813721:  38%|██████▉           | 10/26 [00:02<00:04,  3.83it/s]evaluate for the 76-th batch, evaluate loss: 0.4799247980117798:  71%|████████████     | 75/106 [00:20<00:08,  3.50it/s]evaluate for the 76-th batch, evaluate loss: 0.4799247980117798:  72%|████████████▏    | 76/106 [00:20<00:08,  3.61it/s]Epoch: 7, train for the 3-th batch, train loss: 0.3137969970703125:   2%|▎              | 2/119 [00:01<01:08,  1.71it/s]Epoch: 7, train for the 3-th batch, train loss: 0.3137969970703125:   3%|▍              | 3/119 [00:01<01:08,  1.69it/s]Epoch: 6, train for the 24-th batch, train loss: 0.4491044282913208:  16%|██           | 23/146 [00:13<01:08,  1.80it/s]Epoch: 6, train for the 24-th batch, train loss: 0.4491044282913208:  16%|██▏          | 24/146 [00:13<01:09,  1.76it/s]evaluate for the 11-th batch, evaluate loss: 0.6846166849136353:  38%|██████▉           | 10/26 [00:02<00:04,  3.83it/s]evaluate for the 11-th batch, evaluate loss: 0.6846166849136353:  42%|███████▌          | 11/26 [00:02<00:04,  3.62it/s]Epoch: 4, train for the 46-th batch, train loss: 0.8915479779243469:  19%|██▍          | 45/237 [00:24<01:48,  1.76it/s]Epoch: 4, train for the 46-th batch, train loss: 0.8915479779243469:  19%|██▌          | 46/237 [00:24<01:49,  1.75it/s]evaluate for the 77-th batch, evaluate loss: 0.43508073687553406:  72%|███████████▍    | 76/106 [00:21<00:08,  3.61it/s]evaluate for the 77-th batch, evaluate loss: 0.43508073687553406:  73%|███████████▌    | 77/106 [00:21<00:08,  3.48it/s]evaluate for the 12-th batch, evaluate loss: 0.71950763463974:  42%|████████▍           | 11/26 [00:03<00:04,  3.62it/s]evaluate for the 12-th batch, evaluate loss: 0.71950763463974:  46%|█████████▏          | 12/26 [00:03<00:03,  3.82it/s]evaluate for the 78-th batch, evaluate loss: 0.45630618929862976:  73%|███████████▌    | 77/106 [00:21<00:08,  3.48it/s]evaluate for the 78-th batch, evaluate loss: 0.45630618929862976:  74%|███████████▊    | 78/106 [00:21<00:07,  3.57it/s]Epoch: 6, train for the 25-th batch, train loss: 0.47723159193992615:  16%|█▉          | 24/146 [00:13<01:09,  1.76it/s]Epoch: 6, train for the 25-th batch, train loss: 0.47723159193992615:  17%|██          | 25/146 [00:13<01:05,  1.84it/s]evaluate for the 13-th batch, evaluate loss: 0.7204330563545227:  46%|████████▎         | 12/26 [00:03<00:03,  3.82it/s]evaluate for the 13-th batch, evaluate loss: 0.7204330563545227:  50%|█████████         | 13/26 [00:03<00:03,  3.60it/s]Epoch: 7, train for the 4-th batch, train loss: 0.2875702679157257:   3%|▍              | 3/119 [00:02<01:08,  1.69it/s]Epoch: 7, train for the 4-th batch, train loss: 0.2875702679157257:   3%|▌              | 4/119 [00:02<01:12,  1.59it/s]evaluate for the 79-th batch, evaluate loss: 0.4860910177230835:  74%|████████████▌    | 78/106 [00:21<00:07,  3.57it/s]evaluate for the 79-th batch, evaluate loss: 0.4860910177230835:  75%|████████████▋    | 79/106 [00:21<00:07,  3.51it/s]Epoch: 4, train for the 47-th batch, train loss: 0.7165095210075378:  19%|██▌          | 46/237 [00:25<01:49,  1.75it/s]Epoch: 4, train for the 47-th batch, train loss: 0.7165095210075378:  20%|██▌          | 47/237 [00:25<01:49,  1.73it/s]evaluate for the 14-th batch, evaluate loss: 0.6256032586097717:  50%|█████████         | 13/26 [00:03<00:03,  3.60it/s]evaluate for the 14-th batch, evaluate loss: 0.6256032586097717:  54%|█████████▋        | 14/26 [00:03<00:03,  3.69it/s]evaluate for the 80-th batch, evaluate loss: 0.4853784441947937:  75%|████████████▋    | 79/106 [00:22<00:07,  3.51it/s]evaluate for the 80-th batch, evaluate loss: 0.4853784441947937:  75%|████████████▊    | 80/106 [00:22<00:07,  3.61it/s]Epoch: 6, train for the 26-th batch, train loss: 0.3695518672466278:  17%|██▏          | 25/146 [00:14<01:05,  1.84it/s]Epoch: 6, train for the 26-th batch, train loss: 0.3695518672466278:  18%|██▎          | 26/146 [00:14<01:07,  1.78it/s]evaluate for the 15-th batch, evaluate loss: 0.706775426864624:  54%|██████████▏        | 14/26 [00:04<00:03,  3.69it/s]evaluate for the 15-th batch, evaluate loss: 0.706775426864624:  58%|██████████▉        | 15/26 [00:04<00:02,  3.69it/s]Epoch: 7, train for the 5-th batch, train loss: 0.3244647979736328:   3%|▌              | 4/119 [00:03<01:12,  1.59it/s]Epoch: 7, train for the 5-th batch, train loss: 0.3244647979736328:   4%|▋              | 5/119 [00:03<01:11,  1.60it/s]evaluate for the 81-th batch, evaluate loss: 0.49670055508613586:  75%|████████████    | 80/106 [00:22<00:07,  3.61it/s]evaluate for the 81-th batch, evaluate loss: 0.49670055508613586:  76%|████████████▏   | 81/106 [00:22<00:07,  3.57it/s]Epoch: 4, train for the 48-th batch, train loss: 0.7673214673995972:  20%|██▌          | 47/237 [00:25<01:49,  1.73it/s]Epoch: 4, train for the 48-th batch, train loss: 0.7673214673995972:  20%|██▋          | 48/237 [00:25<01:48,  1.75it/s]evaluate for the 16-th batch, evaluate loss: 0.7271479368209839:  58%|██████████▍       | 15/26 [00:04<00:02,  3.69it/s]evaluate for the 16-th batch, evaluate loss: 0.7271479368209839:  62%|███████████       | 16/26 [00:04<00:02,  3.79it/s]evaluate for the 82-th batch, evaluate loss: 0.4592099189758301:  76%|████████████▉    | 81/106 [00:22<00:07,  3.57it/s]evaluate for the 82-th batch, evaluate loss: 0.4592099189758301:  77%|█████████████▏   | 82/106 [00:22<00:06,  3.66it/s]evaluate for the 17-th batch, evaluate loss: 0.681797444820404:  62%|███████████▋       | 16/26 [00:04<00:02,  3.79it/s]evaluate for the 17-th batch, evaluate loss: 0.681797444820404:  65%|████████████▍      | 17/26 [00:04<00:02,  3.77it/s]Epoch: 6, train for the 27-th batch, train loss: 0.421928733587265:  18%|██▍           | 26/146 [00:15<01:07,  1.78it/s]Epoch: 6, train for the 27-th batch, train loss: 0.421928733587265:  18%|██▌           | 27/146 [00:15<01:08,  1.73it/s]Epoch: 7, train for the 6-th batch, train loss: 0.2791144847869873:   4%|▋              | 5/119 [00:03<01:11,  1.60it/s]Epoch: 7, train for the 6-th batch, train loss: 0.2791144847869873:   5%|▊              | 6/119 [00:03<01:08,  1.64it/s]evaluate for the 83-th batch, evaluate loss: 0.39794203639030457:  77%|████████████▍   | 82/106 [00:22<00:06,  3.66it/s]evaluate for the 83-th batch, evaluate loss: 0.39794203639030457:  78%|████████████▌   | 83/106 [00:22<00:06,  3.60it/s]Epoch: 4, train for the 49-th batch, train loss: 0.9474694728851318:  20%|██▋          | 48/237 [00:26<01:48,  1.75it/s]Epoch: 4, train for the 49-th batch, train loss: 0.9474694728851318:  21%|██▋          | 49/237 [00:26<01:46,  1.76it/s]evaluate for the 18-th batch, evaluate loss: 0.7165113687515259:  65%|███████████▊      | 17/26 [00:04<00:02,  3.77it/s]evaluate for the 18-th batch, evaluate loss: 0.7165113687515259:  69%|████████████▍     | 18/26 [00:04<00:02,  3.61it/s]evaluate for the 84-th batch, evaluate loss: 0.49559783935546875:  78%|████████████▌   | 83/106 [00:23<00:06,  3.60it/s]evaluate for the 84-th batch, evaluate loss: 0.49559783935546875:  79%|████████████▋   | 84/106 [00:23<00:05,  3.70it/s]evaluate for the 19-th batch, evaluate loss: 0.6271828413009644:  69%|████████████▍     | 18/26 [00:05<00:02,  3.61it/s]evaluate for the 19-th batch, evaluate loss: 0.6271828413009644:  73%|█████████████▏    | 19/26 [00:05<00:01,  3.90it/s]Epoch: 6, train for the 28-th batch, train loss: 0.4328189790248871:  18%|██▍          | 27/146 [00:15<01:08,  1.73it/s]Epoch: 6, train for the 28-th batch, train loss: 0.4328189790248871:  19%|██▍          | 28/146 [00:15<01:08,  1.72it/s]evaluate for the 85-th batch, evaluate loss: 0.5088256597518921:  79%|█████████████▍   | 84/106 [00:23<00:05,  3.70it/s]evaluate for the 85-th batch, evaluate loss: 0.5088256597518921:  80%|█████████████▋   | 85/106 [00:23<00:05,  3.63it/s]Epoch: 7, train for the 7-th batch, train loss: 0.2747262418270111:   5%|▊              | 6/119 [00:04<01:08,  1.64it/s]Epoch: 7, train for the 7-th batch, train loss: 0.2747262418270111:   6%|▉              | 7/119 [00:04<01:08,  1.65it/s]evaluate for the 20-th batch, evaluate loss: 0.6463244557380676:  73%|█████████████▏    | 19/26 [00:05<00:01,  3.90it/s]evaluate for the 20-th batch, evaluate loss: 0.6463244557380676:  77%|█████████████▊    | 20/26 [00:05<00:01,  3.62it/s]Epoch: 4, train for the 50-th batch, train loss: 1.0200060606002808:  21%|██▋          | 49/237 [00:26<01:46,  1.76it/s]Epoch: 4, train for the 50-th batch, train loss: 1.0200060606002808:  21%|██▋          | 50/237 [00:26<01:47,  1.74it/s]evaluate for the 86-th batch, evaluate loss: 0.465389609336853:  80%|██████████████▍   | 85/106 [00:23<00:05,  3.63it/s]evaluate for the 86-th batch, evaluate loss: 0.465389609336853:  81%|██████████████▌   | 86/106 [00:23<00:05,  3.69it/s]evaluate for the 21-th batch, evaluate loss: 0.6779587268829346:  77%|█████████████▊    | 20/26 [00:05<00:01,  3.62it/s]evaluate for the 21-th batch, evaluate loss: 0.6779587268829346:  81%|██████████████▌   | 21/26 [00:05<00:01,  3.71it/s]Epoch: 6, train for the 29-th batch, train loss: 0.3957716226577759:  19%|██▍          | 28/146 [00:16<01:08,  1.72it/s]Epoch: 6, train for the 29-th batch, train loss: 0.3957716226577759:  20%|██▌          | 29/146 [00:16<01:08,  1.72it/s]evaluate for the 87-th batch, evaluate loss: 0.4605073928833008:  81%|█████████████▊   | 86/106 [00:24<00:05,  3.69it/s]evaluate for the 87-th batch, evaluate loss: 0.4605073928833008:  82%|█████████████▉   | 87/106 [00:24<00:05,  3.64it/s]evaluate for the 22-th batch, evaluate loss: 0.7109091281890869:  81%|██████████████▌   | 21/26 [00:05<00:01,  3.71it/s]evaluate for the 22-th batch, evaluate loss: 0.7109091281890869:  85%|███████████████▏  | 22/26 [00:05<00:01,  3.66it/s]Epoch: 7, train for the 8-th batch, train loss: 0.21097087860107422:   6%|▊             | 7/119 [00:04<01:08,  1.65it/s]Epoch: 7, train for the 8-th batch, train loss: 0.21097087860107422:   7%|▉             | 8/119 [00:04<01:07,  1.65it/s]Epoch: 4, train for the 51-th batch, train loss: 0.933269202709198:  21%|██▉           | 50/237 [00:27<01:47,  1.74it/s]Epoch: 4, train for the 51-th batch, train loss: 0.933269202709198:  22%|███           | 51/237 [00:27<01:47,  1.73it/s]evaluate for the 88-th batch, evaluate loss: 0.5208258032798767:  82%|█████████████▉   | 87/106 [00:24<00:05,  3.64it/s]evaluate for the 88-th batch, evaluate loss: 0.5208258032798767:  83%|██████████████   | 88/106 [00:24<00:04,  3.62it/s]evaluate for the 23-th batch, evaluate loss: 0.6868777275085449:  85%|███████████████▏  | 22/26 [00:06<00:01,  3.66it/s]evaluate for the 23-th batch, evaluate loss: 0.6868777275085449:  88%|███████████████▉  | 23/26 [00:06<00:00,  3.80it/s]evaluate for the 89-th batch, evaluate loss: 0.4682559072971344:  83%|██████████████   | 88/106 [00:24<00:04,  3.62it/s]evaluate for the 89-th batch, evaluate loss: 0.4682559072971344:  84%|██████████████▎  | 89/106 [00:24<00:04,  3.64it/s]evaluate for the 24-th batch, evaluate loss: 0.6955044269561768:  88%|███████████████▉  | 23/26 [00:06<00:00,  3.80it/s]evaluate for the 24-th batch, evaluate loss: 0.6955044269561768:  92%|████████████████▌ | 24/26 [00:06<00:00,  3.77it/s]Epoch: 6, train for the 30-th batch, train loss: 0.43868187069892883:  20%|██▍         | 29/146 [00:16<01:08,  1.72it/s]Epoch: 6, train for the 30-th batch, train loss: 0.43868187069892883:  21%|██▍         | 30/146 [00:16<01:08,  1.69it/s]Epoch: 4, train for the 52-th batch, train loss: 0.8229894638061523:  22%|██▊          | 51/237 [00:27<01:47,  1.73it/s]Epoch: 4, train for the 52-th batch, train loss: 0.8229894638061523:  22%|██▊          | 52/237 [00:27<01:46,  1.74it/s]Epoch: 7, train for the 9-th batch, train loss: 0.23528441786766052:   7%|▉             | 8/119 [00:05<01:07,  1.65it/s]Epoch: 7, train for the 9-th batch, train loss: 0.23528441786766052:   8%|█             | 9/119 [00:05<01:06,  1.65it/s]evaluate for the 90-th batch, evaluate loss: 0.3726845383644104:  84%|██████████████▎  | 89/106 [00:24<00:04,  3.64it/s]evaluate for the 90-th batch, evaluate loss: 0.3726845383644104:  85%|██████████████▍  | 90/106 [00:24<00:04,  3.57it/s]evaluate for the 25-th batch, evaluate loss: 0.730060875415802:  92%|█████████████████▌ | 24/26 [00:06<00:00,  3.77it/s]evaluate for the 25-th batch, evaluate loss: 0.730060875415802:  96%|██████████████████▎| 25/26 [00:06<00:00,  3.67it/s]evaluate for the 26-th batch, evaluate loss: 0.7120772004127502:  96%|█████████████████▎| 25/26 [00:06<00:00,  3.67it/s]evaluate for the 26-th batch, evaluate loss: 0.7120772004127502: 100%|██████████████████| 26/26 [00:06<00:00,  4.51it/s]evaluate for the 26-th batch, evaluate loss: 0.7120772004127502: 100%|██████████████████| 26/26 [00:06<00:00,  3.80it/s]
INFO:root:test loss: 0.5099
INFO:root:test average_precision, 0.8426
INFO:root:test roc_auc, 0.8018
INFO:root:new node test loss: 0.6891
INFO:root:new node test first_1_average_precision, 0.4869
INFO:root:new node test first_1_roc_auc, 0.4169
INFO:root:new node test first_3_average_precision, 0.5664
INFO:root:new node test first_3_roc_auc, 0.5177
INFO:root:new node test first_10_average_precision, 0.6855
INFO:root:new node test first_10_roc_auc, 0.6443
INFO:root:new node test average_precision, 0.7082
INFO:root:new node test roc_auc, 0.6551
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]evaluate for the 91-th batch, evaluate loss: 0.4206297695636749:  85%|██████████████▍  | 90/106 [00:25<00:04,  3.57it/s]evaluate for the 91-th batch, evaluate loss: 0.4206297695636749:  86%|██████████████▌  | 91/106 [00:25<00:04,  3.62it/s]Epoch: 6, train for the 31-th batch, train loss: 0.48938995599746704:  21%|██▍         | 30/146 [00:17<01:08,  1.69it/s]Epoch: 6, train for the 31-th batch, train loss: 0.48938995599746704:  21%|██▌         | 31/146 [00:17<01:08,  1.68it/s]Epoch: 4, train for the 53-th batch, train loss: 0.7018489837646484:  22%|██▊          | 52/237 [00:28<01:46,  1.74it/s]Epoch: 4, train for the 53-th batch, train loss: 0.7018489837646484:  22%|██▉          | 53/237 [00:28<01:45,  1.74it/s]Epoch: 7, train for the 10-th batch, train loss: 0.2548263370990753:   8%|█             | 9/119 [00:06<01:06,  1.65it/s]Epoch: 7, train for the 10-th batch, train loss: 0.2548263370990753:   8%|█            | 10/119 [00:06<01:05,  1.66it/s]evaluate for the 92-th batch, evaluate loss: 0.4731089174747467:  86%|██████████████▌  | 91/106 [00:25<00:04,  3.62it/s]evaluate for the 92-th batch, evaluate loss: 0.4731089174747467:  87%|██████████████▊  | 92/106 [00:25<00:03,  3.56it/s]Epoch: 11, train for the 1-th batch, train loss: 0.9036789536476135:   0%|                      | 0/151 [00:00<?, ?it/s]Epoch: 11, train for the 1-th batch, train loss: 0.9036789536476135:   1%|              | 1/151 [00:00<01:07,  2.21it/s]evaluate for the 93-th batch, evaluate loss: 0.41338714957237244:  87%|█████████████▉  | 92/106 [00:25<00:03,  3.56it/s]evaluate for the 93-th batch, evaluate loss: 0.41338714957237244:  88%|██████████████  | 93/106 [00:25<00:03,  3.68it/s]Epoch: 6, train for the 32-th batch, train loss: 0.41816726326942444:  21%|██▌         | 31/146 [00:18<01:08,  1.68it/s]Epoch: 6, train for the 32-th batch, train loss: 0.41816726326942444:  22%|██▋         | 32/146 [00:18<01:07,  1.68it/s]Epoch: 4, train for the 54-th batch, train loss: 0.6414377689361572:  22%|██▉          | 53/237 [00:29<01:45,  1.74it/s]Epoch: 4, train for the 54-th batch, train loss: 0.6414377689361572:  23%|██▉          | 54/237 [00:29<01:45,  1.74it/s]Epoch: 11, train for the 2-th batch, train loss: 0.8723548650741577:   1%|              | 1/151 [00:00<01:07,  2.21it/s]Epoch: 11, train for the 2-th batch, train loss: 0.8723548650741577:   1%|▏             | 2/151 [00:00<01:07,  2.21it/s]Epoch: 7, train for the 11-th batch, train loss: 0.21003295481204987:   8%|█           | 10/119 [00:06<01:05,  1.66it/s]Epoch: 7, train for the 11-th batch, train loss: 0.21003295481204987:   9%|█           | 11/119 [00:06<01:05,  1.66it/s]evaluate for the 94-th batch, evaluate loss: 0.4532229006290436:  88%|██████████████▉  | 93/106 [00:25<00:03,  3.68it/s]evaluate for the 94-th batch, evaluate loss: 0.4532229006290436:  89%|███████████████  | 94/106 [00:25<00:03,  3.59it/s]evaluate for the 95-th batch, evaluate loss: 0.3682701289653778:  89%|███████████████  | 94/106 [00:26<00:03,  3.59it/s]evaluate for the 95-th batch, evaluate loss: 0.3682701289653778:  90%|███████████████▏ | 95/106 [00:26<00:02,  3.74it/s]Epoch: 11, train for the 3-th batch, train loss: 0.5473014116287231:   1%|▏             | 2/151 [00:01<01:07,  2.21it/s]Epoch: 11, train for the 3-th batch, train loss: 0.5473014116287231:   2%|▎             | 3/151 [00:01<01:03,  2.33it/s]Epoch: 6, train for the 33-th batch, train loss: 0.449491411447525:  22%|███           | 32/146 [00:18<01:07,  1.68it/s]Epoch: 6, train for the 33-th batch, train loss: 0.449491411447525:  23%|███▏          | 33/146 [00:18<01:07,  1.67it/s]Epoch: 4, train for the 55-th batch, train loss: 0.657383918762207:  23%|███▏          | 54/237 [00:29<01:45,  1.74it/s]Epoch: 4, train for the 55-th batch, train loss: 0.657383918762207:  23%|███▏          | 55/237 [00:29<01:45,  1.72it/s]evaluate for the 96-th batch, evaluate loss: 0.4303636848926544:  90%|███████████████▏ | 95/106 [00:26<00:02,  3.74it/s]evaluate for the 96-th batch, evaluate loss: 0.4303636848926544:  91%|███████████████▍ | 96/106 [00:26<00:02,  3.59it/s]Epoch: 7, train for the 12-th batch, train loss: 0.2680039405822754:   9%|█▏           | 11/119 [00:07<01:05,  1.66it/s]Epoch: 7, train for the 12-th batch, train loss: 0.2680039405822754:  10%|█▎           | 12/119 [00:07<01:04,  1.65it/s]Epoch: 11, train for the 4-th batch, train loss: 0.5815147757530212:   2%|▎             | 3/151 [00:01<01:03,  2.33it/s]Epoch: 11, train for the 4-th batch, train loss: 0.5815147757530212:   3%|▎             | 4/151 [00:01<01:04,  2.27it/s]evaluate for the 97-th batch, evaluate loss: 0.4145471453666687:  91%|███████████████▍ | 96/106 [00:26<00:02,  3.59it/s]evaluate for the 97-th batch, evaluate loss: 0.4145471453666687:  92%|███████████████▌ | 97/106 [00:26<00:02,  3.72it/s]Epoch: 6, train for the 34-th batch, train loss: 0.44112730026245117:  23%|██▋         | 33/146 [00:19<01:07,  1.67it/s]Epoch: 6, train for the 34-th batch, train loss: 0.44112730026245117:  23%|██▊         | 34/146 [00:19<01:07,  1.66it/s]Epoch: 4, train for the 56-th batch, train loss: 0.6361917853355408:  23%|███          | 55/237 [00:30<01:45,  1.72it/s]Epoch: 4, train for the 56-th batch, train loss: 0.6361917853355408:  24%|███          | 56/237 [00:30<01:46,  1.70it/s]evaluate for the 98-th batch, evaluate loss: 0.46754440665245056:  92%|██████████████▋ | 97/106 [00:27<00:02,  3.72it/s]evaluate for the 98-th batch, evaluate loss: 0.46754440665245056:  92%|██████████████▊ | 98/106 [00:27<00:02,  3.53it/s]Epoch: 7, train for the 13-th batch, train loss: 0.20551957190036774:  10%|█▏          | 12/119 [00:07<01:04,  1.65it/s]Epoch: 7, train for the 13-th batch, train loss: 0.20551957190036774:  11%|█▎          | 13/119 [00:07<01:04,  1.65it/s]Epoch: 11, train for the 5-th batch, train loss: 0.56264728307724:   3%|▍               | 4/151 [00:02<01:04,  2.27it/s]Epoch: 11, train for the 5-th batch, train loss: 0.56264728307724:   3%|▌               | 5/151 [00:02<01:07,  2.18it/s]evaluate for the 99-th batch, evaluate loss: 0.41966840624809265:  92%|██████████████▊ | 98/106 [00:27<00:02,  3.53it/s]evaluate for the 99-th batch, evaluate loss: 0.41966840624809265:  93%|██████████████▉ | 99/106 [00:27<00:01,  3.59it/s]Epoch: 6, train for the 35-th batch, train loss: 0.43724584579467773:  23%|██▊         | 34/146 [00:19<01:07,  1.66it/s]Epoch: 6, train for the 35-th batch, train loss: 0.43724584579467773:  24%|██▉         | 35/146 [00:19<01:06,  1.66it/s]evaluate for the 100-th batch, evaluate loss: 0.39488354325294495:  93%|██████████████ | 99/106 [00:27<00:01,  3.59it/s]evaluate for the 100-th batch, evaluate loss: 0.39488354325294495:  94%|█████████████▏| 100/106 [00:27<00:01,  3.53it/s]Epoch: 4, train for the 57-th batch, train loss: 0.6500454545021057:  24%|███          | 56/237 [00:30<01:46,  1.70it/s]Epoch: 4, train for the 57-th batch, train loss: 0.6500454545021057:  24%|███▏         | 57/237 [00:30<01:45,  1.70it/s]Epoch: 11, train for the 6-th batch, train loss: 0.587449848651886:   3%|▍              | 5/151 [00:02<01:07,  2.18it/s]Epoch: 11, train for the 6-th batch, train loss: 0.587449848651886:   4%|▌              | 6/151 [00:02<01:06,  2.17it/s]Epoch: 7, train for the 14-th batch, train loss: 0.24674411118030548:  11%|█▎          | 13/119 [00:08<01:04,  1.65it/s]Epoch: 7, train for the 14-th batch, train loss: 0.24674411118030548:  12%|█▍          | 14/119 [00:08<01:03,  1.65it/s]evaluate for the 101-th batch, evaluate loss: 0.43793052434921265:  94%|█████████████▏| 100/106 [00:27<00:01,  3.53it/s]evaluate for the 101-th batch, evaluate loss: 0.43793052434921265:  95%|█████████████▎| 101/106 [00:27<00:01,  3.65it/s]evaluate for the 102-th batch, evaluate loss: 0.4088306427001953:  95%|██████████████▎| 101/106 [00:28<00:01,  3.65it/s]evaluate for the 102-th batch, evaluate loss: 0.4088306427001953:  96%|██████████████▍| 102/106 [00:28<00:01,  3.59it/s]Epoch: 11, train for the 7-th batch, train loss: 0.6223374605178833:   4%|▌             | 6/151 [00:03<01:06,  2.17it/s]Epoch: 11, train for the 7-th batch, train loss: 0.6223374605178833:   5%|▋             | 7/151 [00:03<01:08,  2.11it/s]Epoch: 6, train for the 36-th batch, train loss: 0.4160504937171936:  24%|███          | 35/146 [00:20<01:06,  1.66it/s]Epoch: 6, train for the 36-th batch, train loss: 0.4160504937171936:  25%|███▏         | 36/146 [00:20<01:07,  1.64it/s]Epoch: 4, train for the 58-th batch, train loss: 0.6433102488517761:  24%|███▏         | 57/237 [00:31<01:45,  1.70it/s]Epoch: 4, train for the 58-th batch, train loss: 0.6433102488517761:  24%|███▏         | 58/237 [00:31<01:44,  1.71it/s]Epoch: 7, train for the 15-th batch, train loss: 0.2516430914402008:  12%|█▌           | 14/119 [00:09<01:03,  1.65it/s]Epoch: 7, train for the 15-th batch, train loss: 0.2516430914402008:  13%|█▋           | 15/119 [00:09<01:03,  1.63it/s]evaluate for the 103-th batch, evaluate loss: 0.5518326163291931:  96%|██████████████▍| 102/106 [00:28<00:01,  3.59it/s]evaluate for the 103-th batch, evaluate loss: 0.5518326163291931:  97%|██████████████▌| 103/106 [00:28<00:00,  3.67it/s]Epoch: 11, train for the 8-th batch, train loss: 0.6448149681091309:   5%|▋             | 7/151 [00:03<01:08,  2.11it/s]Epoch: 11, train for the 8-th batch, train loss: 0.6448149681091309:   5%|▋             | 8/151 [00:03<01:08,  2.08it/s]evaluate for the 104-th batch, evaluate loss: 0.42893072962760925:  97%|█████████████▌| 103/106 [00:28<00:00,  3.67it/s]evaluate for the 104-th batch, evaluate loss: 0.42893072962760925:  98%|█████████████▋| 104/106 [00:28<00:00,  3.63it/s]Epoch: 4, train for the 59-th batch, train loss: 0.6339260935783386:  24%|███▏         | 58/237 [00:31<01:44,  1.71it/s]Epoch: 4, train for the 59-th batch, train loss: 0.6339260935783386:  25%|███▏         | 59/237 [00:31<01:43,  1.72it/s]Epoch: 6, train for the 37-th batch, train loss: 0.4757663309574127:  25%|███▏         | 36/146 [00:21<01:07,  1.64it/s]Epoch: 6, train for the 37-th batch, train loss: 0.4757663309574127:  25%|███▎         | 37/146 [00:21<01:06,  1.63it/s]evaluate for the 105-th batch, evaluate loss: 0.4767381250858307:  98%|██████████████▋| 104/106 [00:29<00:00,  3.63it/s]evaluate for the 105-th batch, evaluate loss: 0.4767381250858307:  99%|██████████████▊| 105/106 [00:29<00:00,  3.70it/s]Epoch: 7, train for the 16-th batch, train loss: 0.25684675574302673:  13%|█▌          | 15/119 [00:09<01:03,  1.63it/s]Epoch: 7, train for the 16-th batch, train loss: 0.25684675574302673:  13%|█▌          | 16/119 [00:09<01:03,  1.63it/s]evaluate for the 106-th batch, evaluate loss: 0.5761008262634277:  99%|██████████████▊| 105/106 [00:29<00:00,  3.70it/s]evaluate for the 106-th batch, evaluate loss: 0.5761008262634277: 100%|███████████████| 106/106 [00:29<00:00,  4.29it/s]evaluate for the 106-th batch, evaluate loss: 0.5761008262634277: 100%|███████████████| 106/106 [00:29<00:00,  3.64it/s]
  0%|                                                                                            | 0/78 [00:00<?, ?it/s]Epoch: 11, train for the 9-th batch, train loss: 0.6586819291114807:   5%|▋             | 8/151 [00:04<01:08,  2.08it/s]Epoch: 11, train for the 9-th batch, train loss: 0.6586819291114807:   6%|▊             | 9/151 [00:04<01:11,  1.97it/s]Epoch: 4, train for the 60-th batch, train loss: 0.6917160749435425:  25%|███▏         | 59/237 [00:32<01:43,  1.72it/s]Epoch: 4, train for the 60-th batch, train loss: 0.6917160749435425:  25%|███▎         | 60/237 [00:32<01:44,  1.70it/s]evaluate for the 1-th batch, evaluate loss: 0.6690879464149475:   0%|                            | 0/78 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6690879464149475:   1%|▎                   | 1/78 [00:00<00:24,  3.17it/s]Epoch: 6, train for the 38-th batch, train loss: 0.441976398229599:  25%|███▌          | 37/146 [00:21<01:06,  1.63it/s]Epoch: 6, train for the 38-th batch, train loss: 0.441976398229599:  26%|███▋          | 38/146 [00:21<01:06,  1.63it/s]Epoch: 7, train for the 17-th batch, train loss: 0.22893820703029633:  13%|█▌          | 16/119 [00:10<01:03,  1.63it/s]Epoch: 7, train for the 17-th batch, train loss: 0.22893820703029633:  14%|█▋          | 17/119 [00:10<01:02,  1.63it/s]evaluate for the 2-th batch, evaluate loss: 0.697419285774231:   1%|▎                    | 1/78 [00:00<00:24,  3.17it/s]evaluate for the 2-th batch, evaluate loss: 0.697419285774231:   3%|▌                    | 2/78 [00:00<00:21,  3.53it/s]Epoch: 11, train for the 10-th batch, train loss: 0.5164870619773865:   6%|▊            | 9/151 [00:04<01:11,  1.97it/s]Epoch: 11, train for the 10-th batch, train loss: 0.5164870619773865:   7%|▊           | 10/151 [00:04<01:11,  1.96it/s]Epoch: 4, train for the 61-th batch, train loss: 0.5427390933036804:  25%|███▎         | 60/237 [00:33<01:44,  1.70it/s]Epoch: 4, train for the 61-th batch, train loss: 0.5427390933036804:  26%|███▎         | 61/237 [00:33<01:41,  1.73it/s]evaluate for the 3-th batch, evaluate loss: 0.6114563345909119:   3%|▌                   | 2/78 [00:00<00:21,  3.53it/s]evaluate for the 3-th batch, evaluate loss: 0.6114563345909119:   4%|▊                   | 3/78 [00:00<00:22,  3.39it/s]Epoch: 6, train for the 39-th batch, train loss: 0.487496554851532:  26%|███▋          | 38/146 [00:22<01:06,  1.63it/s]Epoch: 6, train for the 39-th batch, train loss: 0.487496554851532:  27%|███▋          | 39/146 [00:22<01:05,  1.63it/s]Epoch: 11, train for the 11-th batch, train loss: 0.5467444658279419:   7%|▊           | 10/151 [00:05<01:11,  1.96it/s]Epoch: 11, train for the 11-th batch, train loss: 0.5467444658279419:   7%|▊           | 11/151 [00:05<01:03,  2.21it/s]Epoch: 7, train for the 18-th batch, train loss: 0.2216677963733673:  14%|█▊           | 17/119 [00:10<01:02,  1.63it/s]Epoch: 7, train for the 18-th batch, train loss: 0.2216677963733673:  15%|█▉           | 18/119 [00:10<01:02,  1.63it/s]evaluate for the 4-th batch, evaluate loss: 0.6917274594306946:   4%|▊                   | 3/78 [00:01<00:22,  3.39it/s]evaluate for the 4-th batch, evaluate loss: 0.6917274594306946:   5%|█                   | 4/78 [00:01<00:20,  3.59it/s]Epoch: 4, train for the 62-th batch, train loss: 0.6237817406654358:  26%|███▎         | 61/237 [00:33<01:41,  1.73it/s]Epoch: 4, train for the 62-th batch, train loss: 0.6237817406654358:  26%|███▍         | 62/237 [00:33<01:41,  1.72it/s]evaluate for the 5-th batch, evaluate loss: 0.6390575766563416:   5%|█                   | 4/78 [00:01<00:20,  3.59it/s]evaluate for the 5-th batch, evaluate loss: 0.6390575766563416:   6%|█▎                  | 5/78 [00:01<00:21,  3.42it/s]Epoch: 11, train for the 12-th batch, train loss: 0.5432963967323303:   7%|▊           | 11/151 [00:05<01:03,  2.21it/s]Epoch: 11, train for the 12-th batch, train loss: 0.5432963967323303:   8%|▉           | 12/151 [00:05<01:06,  2.09it/s]Epoch: 6, train for the 40-th batch, train loss: 0.451342910528183:  27%|███▋          | 39/146 [00:23<01:05,  1.63it/s]Epoch: 6, train for the 40-th batch, train loss: 0.451342910528183:  27%|███▊          | 40/146 [00:23<01:05,  1.63it/s]evaluate for the 6-th batch, evaluate loss: 0.5633103251457214:   6%|█▎                  | 5/78 [00:01<00:21,  3.42it/s]evaluate for the 6-th batch, evaluate loss: 0.5633103251457214:   8%|█▌                  | 6/78 [00:01<00:20,  3.55it/s]Epoch: 7, train for the 19-th batch, train loss: 0.17981235682964325:  15%|█▊          | 18/119 [00:11<01:02,  1.63it/s]Epoch: 7, train for the 19-th batch, train loss: 0.17981235682964325:  16%|█▉          | 19/119 [00:11<01:01,  1.63it/s]Epoch: 4, train for the 63-th batch, train loss: 0.655037522315979:  26%|███▋          | 62/237 [00:34<01:41,  1.72it/s]Epoch: 4, train for the 63-th batch, train loss: 0.655037522315979:  27%|███▋          | 63/237 [00:34<01:41,  1.71it/s]evaluate for the 7-th batch, evaluate loss: 0.6787217855453491:   8%|█▌                  | 6/78 [00:02<00:20,  3.55it/s]evaluate for the 7-th batch, evaluate loss: 0.6787217855453491:   9%|█▊                  | 7/78 [00:02<00:20,  3.39it/s]Epoch: 11, train for the 13-th batch, train loss: 0.673937201499939:   8%|█            | 12/151 [00:06<01:06,  2.09it/s]Epoch: 11, train for the 13-th batch, train loss: 0.673937201499939:   9%|█            | 13/151 [00:06<01:08,  2.02it/s]Epoch: 6, train for the 41-th batch, train loss: 0.4603760540485382:  27%|███▌         | 40/146 [00:23<01:05,  1.63it/s]Epoch: 6, train for the 41-th batch, train loss: 0.4603760540485382:  28%|███▋         | 41/146 [00:23<01:04,  1.63it/s]evaluate for the 8-th batch, evaluate loss: 0.545161247253418:   9%|█▉                   | 7/78 [00:02<00:20,  3.39it/s]evaluate for the 8-th batch, evaluate loss: 0.545161247253418:  10%|██▏                  | 8/78 [00:02<00:19,  3.50it/s]Epoch: 7, train for the 20-th batch, train loss: 0.232797771692276:  16%|██▏           | 19/119 [00:12<01:01,  1.63it/s]Epoch: 7, train for the 20-th batch, train loss: 0.232797771692276:  17%|██▎           | 20/119 [00:12<01:00,  1.63it/s]Epoch: 11, train for the 14-th batch, train loss: 0.5089849829673767:   9%|█           | 13/151 [00:06<01:08,  2.02it/s]Epoch: 11, train for the 14-th batch, train loss: 0.5089849829673767:   9%|█           | 14/151 [00:06<01:06,  2.05it/s]Epoch: 4, train for the 64-th batch, train loss: 0.6231937408447266:  27%|███▍         | 63/237 [00:34<01:41,  1.71it/s]Epoch: 4, train for the 64-th batch, train loss: 0.6231937408447266:  27%|███▌         | 64/237 [00:34<01:41,  1.71it/s]evaluate for the 9-th batch, evaluate loss: 0.5002498626708984:  10%|██                  | 8/78 [00:02<00:19,  3.50it/s]evaluate for the 9-th batch, evaluate loss: 0.5002498626708984:  12%|██▎                 | 9/78 [00:02<00:20,  3.36it/s]Epoch: 6, train for the 42-th batch, train loss: 0.5012343525886536:  28%|███▋         | 41/146 [00:24<01:04,  1.63it/s]Epoch: 6, train for the 42-th batch, train loss: 0.5012343525886536:  29%|███▋         | 42/146 [00:24<01:03,  1.63it/s]evaluate for the 10-th batch, evaluate loss: 0.5919374227523804:  12%|██▏                | 9/78 [00:02<00:20,  3.36it/s]evaluate for the 10-th batch, evaluate loss: 0.5919374227523804:  13%|██▎               | 10/78 [00:02<00:19,  3.48it/s]Epoch: 7, train for the 21-th batch, train loss: 0.26651737093925476:  17%|██          | 20/119 [00:12<01:00,  1.63it/s]Epoch: 7, train for the 21-th batch, train loss: 0.26651737093925476:  18%|██          | 21/119 [00:12<01:00,  1.63it/s]Epoch: 11, train for the 15-th batch, train loss: 0.49231860041618347:   9%|█          | 14/151 [00:07<01:06,  2.05it/s]Epoch: 11, train for the 15-th batch, train loss: 0.49231860041618347:  10%|█          | 15/151 [00:07<01:07,  2.00it/s]Epoch: 4, train for the 65-th batch, train loss: 0.6167728900909424:  27%|███▌         | 64/237 [00:35<01:41,  1.71it/s]Epoch: 4, train for the 65-th batch, train loss: 0.6167728900909424:  27%|███▌         | 65/237 [00:35<01:40,  1.71it/s]evaluate for the 11-th batch, evaluate loss: 0.6487392783164978:  13%|██▎               | 10/78 [00:03<00:19,  3.48it/s]evaluate for the 11-th batch, evaluate loss: 0.6487392783164978:  14%|██▌               | 11/78 [00:03<00:20,  3.35it/s]Epoch: 6, train for the 43-th batch, train loss: 0.49201518297195435:  29%|███▍        | 42/146 [00:24<01:03,  1.63it/s]Epoch: 6, train for the 43-th batch, train loss: 0.49201518297195435:  29%|███▌        | 43/146 [00:24<01:02,  1.64it/s]evaluate for the 12-th batch, evaluate loss: 0.6293678879737854:  14%|██▌               | 11/78 [00:03<00:20,  3.35it/s]evaluate for the 12-th batch, evaluate loss: 0.6293678879737854:  15%|██▊               | 12/78 [00:03<00:19,  3.46it/s]Epoch: 7, train for the 22-th batch, train loss: 0.25739267468452454:  18%|██          | 21/119 [00:13<01:00,  1.63it/s]Epoch: 7, train for the 22-th batch, train loss: 0.25739267468452454:  18%|██▏         | 22/119 [00:13<00:59,  1.64it/s]Epoch: 11, train for the 16-th batch, train loss: 0.6869950890541077:  10%|█▏          | 15/151 [00:07<01:07,  2.00it/s]Epoch: 11, train for the 16-th batch, train loss: 0.6869950890541077:  11%|█▎          | 16/151 [00:07<01:09,  1.94it/s]Epoch: 4, train for the 66-th batch, train loss: 0.6064621210098267:  27%|███▌         | 65/237 [00:36<01:40,  1.71it/s]Epoch: 4, train for the 66-th batch, train loss: 0.6064621210098267:  28%|███▌         | 66/237 [00:36<01:39,  1.72it/s]evaluate for the 13-th batch, evaluate loss: 0.6972792148590088:  15%|██▊               | 12/78 [00:03<00:19,  3.46it/s]evaluate for the 13-th batch, evaluate loss: 0.6972792148590088:  17%|███               | 13/78 [00:03<00:19,  3.38it/s]Epoch: 6, train for the 44-th batch, train loss: 0.4361880421638489:  29%|███▊         | 43/146 [00:25<01:02,  1.64it/s]Epoch: 6, train for the 44-th batch, train loss: 0.4361880421638489:  30%|███▉         | 44/146 [00:25<01:02,  1.63it/s]evaluate for the 14-th batch, evaluate loss: 0.4985577166080475:  17%|███               | 13/78 [00:04<00:19,  3.38it/s]evaluate for the 14-th batch, evaluate loss: 0.4985577166080475:  18%|███▏              | 14/78 [00:04<00:18,  3.50it/s]Epoch: 11, train for the 17-th batch, train loss: 0.5380990505218506:  11%|█▎          | 16/151 [00:08<01:09,  1.94it/s]Epoch: 11, train for the 17-th batch, train loss: 0.5380990505218506:  11%|█▎          | 17/151 [00:08<01:07,  1.99it/s]Epoch: 7, train for the 23-th batch, train loss: 0.24528191983699799:  18%|██▏         | 22/119 [00:14<00:59,  1.64it/s]Epoch: 7, train for the 23-th batch, train loss: 0.24528191983699799:  19%|██▎         | 23/119 [00:14<00:58,  1.63it/s]Epoch: 4, train for the 67-th batch, train loss: 0.6162934899330139:  28%|███▌         | 66/237 [00:36<01:39,  1.72it/s]Epoch: 4, train for the 67-th batch, train loss: 0.6162934899330139:  28%|███▋         | 67/237 [00:36<01:39,  1.71it/s]evaluate for the 15-th batch, evaluate loss: 0.5558837652206421:  18%|███▏              | 14/78 [00:04<00:18,  3.50it/s]evaluate for the 15-th batch, evaluate loss: 0.5558837652206421:  19%|███▍              | 15/78 [00:04<00:18,  3.37it/s]Epoch: 6, train for the 45-th batch, train loss: 0.4506968855857849:  30%|███▉         | 44/146 [00:26<01:02,  1.63it/s]Epoch: 6, train for the 45-th batch, train loss: 0.4506968855857849:  31%|████         | 45/146 [00:26<01:01,  1.63it/s]evaluate for the 16-th batch, evaluate loss: 0.5249910354614258:  19%|███▍              | 15/78 [00:04<00:18,  3.37it/s]evaluate for the 16-th batch, evaluate loss: 0.5249910354614258:  21%|███▋              | 16/78 [00:04<00:17,  3.49it/s]Epoch: 11, train for the 18-th batch, train loss: 0.686092734336853:  11%|█▍           | 17/151 [00:08<01:07,  1.99it/s]Epoch: 11, train for the 18-th batch, train loss: 0.686092734336853:  12%|█▌           | 18/151 [00:08<01:10,  1.88it/s]Epoch: 7, train for the 24-th batch, train loss: 0.1873418539762497:  19%|██▌          | 23/119 [00:14<00:58,  1.63it/s]Epoch: 7, train for the 24-th batch, train loss: 0.1873418539762497:  20%|██▌          | 24/119 [00:14<00:58,  1.63it/s]Epoch: 4, train for the 68-th batch, train loss: 0.5863280892372131:  28%|███▋         | 67/237 [00:37<01:39,  1.71it/s]Epoch: 4, train for the 68-th batch, train loss: 0.5863280892372131:  29%|███▋         | 68/237 [00:37<01:38,  1.71it/s]evaluate for the 17-th batch, evaluate loss: 0.5424684286117554:  21%|███▋              | 16/78 [00:04<00:17,  3.49it/s]evaluate for the 17-th batch, evaluate loss: 0.5424684286117554:  22%|███▉              | 17/78 [00:04<00:18,  3.37it/s]Epoch: 11, train for the 19-th batch, train loss: 0.419980525970459:  12%|█▌           | 18/151 [00:09<01:10,  1.88it/s]Epoch: 11, train for the 19-th batch, train loss: 0.419980525970459:  13%|█▋           | 19/151 [00:09<01:09,  1.90it/s]evaluate for the 18-th batch, evaluate loss: 0.5130863785743713:  22%|███▉              | 17/78 [00:05<00:18,  3.37it/s]evaluate for the 18-th batch, evaluate loss: 0.5130863785743713:  23%|████▏             | 18/78 [00:05<00:17,  3.48it/s]Epoch: 6, train for the 46-th batch, train loss: 0.4509944021701813:  31%|████         | 45/146 [00:26<01:01,  1.63it/s]Epoch: 6, train for the 46-th batch, train loss: 0.4509944021701813:  32%|████         | 46/146 [00:26<01:01,  1.63it/s]evaluate for the 19-th batch, evaluate loss: 0.6280282735824585:  23%|████▏             | 18/78 [00:05<00:17,  3.48it/s]evaluate for the 19-th batch, evaluate loss: 0.6280282735824585:  24%|████▍             | 19/78 [00:05<00:14,  4.04it/s]Epoch: 7, train for the 25-th batch, train loss: 0.21393203735351562:  20%|██▍         | 24/119 [00:15<00:58,  1.63it/s]Epoch: 7, train for the 25-th batch, train loss: 0.21393203735351562:  21%|██▌         | 25/119 [00:15<00:57,  1.63it/s]Epoch: 4, train for the 69-th batch, train loss: 0.5885261297225952:  29%|███▋         | 68/237 [00:37<01:38,  1.71it/s]Epoch: 4, train for the 69-th batch, train loss: 0.5885261297225952:  29%|███▊         | 69/237 [00:37<01:40,  1.66it/s]Epoch: 11, train for the 20-th batch, train loss: 0.5098381042480469:  13%|█▌          | 19/151 [00:09<01:09,  1.90it/s]Epoch: 11, train for the 20-th batch, train loss: 0.5098381042480469:  13%|█▌          | 20/151 [00:09<01:07,  1.95it/s]evaluate for the 20-th batch, evaluate loss: 0.45703673362731934:  24%|████▏            | 19/78 [00:05<00:14,  4.04it/s]evaluate for the 20-th batch, evaluate loss: 0.45703673362731934:  26%|████▎            | 20/78 [00:05<00:15,  3.79it/s]Epoch: 6, train for the 47-th batch, train loss: 0.5107513070106506:  32%|████         | 46/146 [00:27<01:01,  1.63it/s]Epoch: 6, train for the 47-th batch, train loss: 0.5107513070106506:  32%|████▏        | 47/146 [00:27<01:00,  1.63it/s]evaluate for the 21-th batch, evaluate loss: 0.5081027746200562:  26%|████▌             | 20/78 [00:05<00:15,  3.79it/s]evaluate for the 21-th batch, evaluate loss: 0.5081027746200562:  27%|████▊             | 21/78 [00:05<00:14,  3.85it/s]Epoch: 7, train for the 26-th batch, train loss: 0.22344012558460236:  21%|██▌         | 25/119 [00:15<00:57,  1.63it/s]Epoch: 7, train for the 26-th batch, train loss: 0.22344012558460236:  22%|██▌         | 26/119 [00:15<00:56,  1.63it/s]Epoch: 4, train for the 70-th batch, train loss: 0.628246009349823:  29%|████          | 69/237 [00:38<01:40,  1.66it/s]Epoch: 4, train for the 70-th batch, train loss: 0.628246009349823:  30%|████▏         | 70/237 [00:38<01:38,  1.69it/s]Epoch: 11, train for the 21-th batch, train loss: 0.3862963020801544:  13%|█▌          | 20/151 [00:10<01:07,  1.95it/s]Epoch: 11, train for the 21-th batch, train loss: 0.3862963020801544:  14%|█▋          | 21/151 [00:10<01:06,  1.96it/s]evaluate for the 22-th batch, evaluate loss: 0.5582943558692932:  27%|████▊             | 21/78 [00:06<00:14,  3.85it/s]evaluate for the 22-th batch, evaluate loss: 0.5582943558692932:  28%|█████             | 22/78 [00:06<00:15,  3.66it/s]Epoch: 6, train for the 48-th batch, train loss: 0.45753178000450134:  32%|███▊        | 47/146 [00:27<01:00,  1.63it/s]Epoch: 6, train for the 48-th batch, train loss: 0.45753178000450134:  33%|███▉        | 48/146 [00:27<00:59,  1.63it/s]evaluate for the 23-th batch, evaluate loss: 0.49826085567474365:  28%|████▊            | 22/78 [00:06<00:15,  3.66it/s]evaluate for the 23-th batch, evaluate loss: 0.49826085567474365:  29%|█████            | 23/78 [00:06<00:14,  3.75it/s]Epoch: 7, train for the 27-th batch, train loss: 0.23063704371452332:  22%|██▌         | 26/119 [00:16<00:56,  1.63it/s]Epoch: 7, train for the 27-th batch, train loss: 0.23063704371452332:  23%|██▋         | 27/119 [00:16<00:56,  1.63it/s]Epoch: 11, train for the 22-th batch, train loss: 0.37657445669174194:  14%|█▌         | 21/151 [00:10<01:06,  1.96it/s]Epoch: 11, train for the 22-th batch, train loss: 0.37657445669174194:  15%|█▌         | 22/151 [00:10<01:06,  1.93it/s]Epoch: 4, train for the 71-th batch, train loss: 0.5545567870140076:  30%|███▊         | 70/237 [00:38<01:38,  1.69it/s]Epoch: 4, train for the 71-th batch, train loss: 0.5545567870140076:  30%|███▉         | 71/237 [00:38<01:36,  1.72it/s]evaluate for the 24-th batch, evaluate loss: 0.5679191946983337:  29%|█████▎            | 23/78 [00:06<00:14,  3.75it/s]evaluate for the 24-th batch, evaluate loss: 0.5679191946983337:  31%|█████▌            | 24/78 [00:06<00:15,  3.59it/s]evaluate for the 25-th batch, evaluate loss: 0.5790964961051941:  31%|█████▌            | 24/78 [00:07<00:15,  3.59it/s]evaluate for the 25-th batch, evaluate loss: 0.5790964961051941:  32%|█████▊            | 25/78 [00:07<00:14,  3.74it/s]Epoch: 6, train for the 49-th batch, train loss: 0.5042821168899536:  33%|████▎        | 48/146 [00:28<00:59,  1.63it/s]Epoch: 6, train for the 49-th batch, train loss: 0.5042821168899536:  34%|████▎        | 49/146 [00:28<00:59,  1.63it/s]Epoch: 7, train for the 28-th batch, train loss: 0.26104575395584106:  23%|██▋         | 27/119 [00:17<00:56,  1.63it/s]Epoch: 7, train for the 28-th batch, train loss: 0.26104575395584106:  24%|██▊         | 28/119 [00:17<00:55,  1.63it/s]Epoch: 11, train for the 23-th batch, train loss: 0.934110164642334:  15%|█▉           | 22/151 [00:11<01:06,  1.93it/s]Epoch: 11, train for the 23-th batch, train loss: 0.934110164642334:  15%|█▉           | 23/151 [00:11<01:07,  1.89it/s]Epoch: 4, train for the 72-th batch, train loss: 0.6000688672065735:  30%|███▉         | 71/237 [00:39<01:36,  1.72it/s]Epoch: 4, train for the 72-th batch, train loss: 0.6000688672065735:  30%|███▉         | 72/237 [00:39<01:36,  1.72it/s]evaluate for the 26-th batch, evaluate loss: 0.5585394501686096:  32%|█████▊            | 25/78 [00:07<00:14,  3.74it/s]evaluate for the 26-th batch, evaluate loss: 0.5585394501686096:  33%|██████            | 26/78 [00:07<00:14,  3.58it/s]evaluate for the 27-th batch, evaluate loss: 0.6430585980415344:  33%|██████            | 26/78 [00:07<00:14,  3.58it/s]evaluate for the 27-th batch, evaluate loss: 0.6430585980415344:  35%|██████▏           | 27/78 [00:07<00:13,  3.70it/s]Epoch: 6, train for the 50-th batch, train loss: 0.4850504696369171:  34%|████▎        | 49/146 [00:29<00:59,  1.63it/s]Epoch: 6, train for the 50-th batch, train loss: 0.4850504696369171:  34%|████▍        | 50/146 [00:29<00:58,  1.63it/s]Epoch: 11, train for the 24-th batch, train loss: 0.4776093065738678:  15%|█▊          | 23/151 [00:11<01:07,  1.89it/s]Epoch: 11, train for the 24-th batch, train loss: 0.4776093065738678:  16%|█▉          | 24/151 [00:11<01:08,  1.86it/s]Epoch: 7, train for the 29-th batch, train loss: 0.17361290752887726:  24%|██▊         | 28/119 [00:17<00:55,  1.63it/s]Epoch: 7, train for the 29-th batch, train loss: 0.17361290752887726:  24%|██▉         | 29/119 [00:17<00:55,  1.63it/s]Epoch: 4, train for the 73-th batch, train loss: 0.6316938400268555:  30%|███▉         | 72/237 [00:40<01:36,  1.72it/s]Epoch: 4, train for the 73-th batch, train loss: 0.6316938400268555:  31%|████         | 73/237 [00:40<01:36,  1.70it/s]evaluate for the 28-th batch, evaluate loss: 0.6988746523857117:  35%|██████▏           | 27/78 [00:07<00:13,  3.70it/s]evaluate for the 28-th batch, evaluate loss: 0.6988746523857117:  36%|██████▍           | 28/78 [00:07<00:14,  3.52it/s]evaluate for the 29-th batch, evaluate loss: 0.5344475507736206:  36%|██████▍           | 28/78 [00:08<00:14,  3.52it/s]evaluate for the 29-th batch, evaluate loss: 0.5344475507736206:  37%|██████▋           | 29/78 [00:08<00:13,  3.59it/s]Epoch: 6, train for the 51-th batch, train loss: 0.45546725392341614:  34%|████        | 50/146 [00:29<00:58,  1.63it/s]Epoch: 6, train for the 51-th batch, train loss: 0.45546725392341614:  35%|████▏       | 51/146 [00:29<00:58,  1.63it/s]Epoch: 11, train for the 25-th batch, train loss: 0.5221463441848755:  16%|█▉          | 24/151 [00:12<01:08,  1.86it/s]Epoch: 11, train for the 25-th batch, train loss: 0.5221463441848755:  17%|█▉          | 25/151 [00:12<01:08,  1.83it/s]Epoch: 7, train for the 30-th batch, train loss: 0.18202301859855652:  24%|██▉         | 29/119 [00:18<00:55,  1.63it/s]Epoch: 7, train for the 30-th batch, train loss: 0.18202301859855652:  25%|███         | 30/119 [00:18<00:54,  1.63it/s]Epoch: 4, train for the 74-th batch, train loss: 0.6445116996765137:  31%|████         | 73/237 [00:40<01:36,  1.70it/s]Epoch: 4, train for the 74-th batch, train loss: 0.6445116996765137:  31%|████         | 74/237 [00:40<01:36,  1.70it/s]evaluate for the 30-th batch, evaluate loss: 0.6045486330986023:  37%|██████▋           | 29/78 [00:08<00:13,  3.59it/s]evaluate for the 30-th batch, evaluate loss: 0.6045486330986023:  38%|██████▉           | 30/78 [00:08<00:13,  3.43it/s]evaluate for the 31-th batch, evaluate loss: 0.4540666937828064:  38%|██████▉           | 30/78 [00:08<00:13,  3.43it/s]evaluate for the 31-th batch, evaluate loss: 0.4540666937828064:  40%|███████▏          | 31/78 [00:08<00:13,  3.51it/s]Epoch: 6, train for the 52-th batch, train loss: 0.47669917345046997:  35%|████▏       | 51/146 [00:30<00:58,  1.63it/s]Epoch: 6, train for the 52-th batch, train loss: 0.47669917345046997:  36%|████▎       | 52/146 [00:30<00:57,  1.62it/s]Epoch: 11, train for the 26-th batch, train loss: 0.6952406167984009:  17%|█▉          | 25/151 [00:13<01:08,  1.83it/s]Epoch: 11, train for the 26-th batch, train loss: 0.6952406167984009:  17%|██          | 26/151 [00:13<01:09,  1.80it/s]Epoch: 4, train for the 75-th batch, train loss: 0.5479289293289185:  31%|████         | 74/237 [00:41<01:36,  1.70it/s]Epoch: 4, train for the 75-th batch, train loss: 0.5479289293289185:  32%|████         | 75/237 [00:41<01:35,  1.70it/s]evaluate for the 32-th batch, evaluate loss: 0.5415214896202087:  40%|███████▏          | 31/78 [00:09<00:13,  3.51it/s]evaluate for the 32-th batch, evaluate loss: 0.5415214896202087:  41%|███████▍          | 32/78 [00:09<00:13,  3.41it/s]Epoch: 7, train for the 31-th batch, train loss: 0.19231165945529938:  25%|███         | 30/119 [00:18<00:54,  1.63it/s]Epoch: 7, train for the 31-th batch, train loss: 0.19231165945529938:  26%|███▏        | 31/119 [00:18<00:54,  1.62it/s]evaluate for the 33-th batch, evaluate loss: 0.587455689907074:  41%|███████▊           | 32/78 [00:09<00:13,  3.41it/s]evaluate for the 33-th batch, evaluate loss: 0.587455689907074:  42%|████████           | 33/78 [00:09<00:12,  3.50it/s]Epoch: 11, train for the 27-th batch, train loss: 0.5634763836860657:  17%|██          | 26/151 [00:13<01:09,  1.80it/s]Epoch: 11, train for the 27-th batch, train loss: 0.5634763836860657:  18%|██▏         | 27/151 [00:13<01:06,  1.86it/s]Epoch: 6, train for the 53-th batch, train loss: 0.4714985191822052:  36%|████▋        | 52/146 [00:30<00:57,  1.62it/s]Epoch: 6, train for the 53-th batch, train loss: 0.4714985191822052:  36%|████▋        | 53/146 [00:30<00:57,  1.62it/s]Epoch: 4, train for the 76-th batch, train loss: 0.6273865103721619:  32%|████         | 75/237 [00:41<01:35,  1.70it/s]Epoch: 4, train for the 76-th batch, train loss: 0.6273865103721619:  32%|████▏        | 76/237 [00:41<01:34,  1.70it/s]evaluate for the 34-th batch, evaluate loss: 0.4975287616252899:  42%|███████▌          | 33/78 [00:09<00:12,  3.50it/s]evaluate for the 34-th batch, evaluate loss: 0.4975287616252899:  44%|███████▊          | 34/78 [00:09<00:12,  3.40it/s]Epoch: 7, train for the 32-th batch, train loss: 0.16953249275684357:  26%|███▏        | 31/119 [00:19<00:54,  1.62it/s]Epoch: 7, train for the 32-th batch, train loss: 0.16953249275684357:  27%|███▏        | 32/119 [00:19<00:53,  1.62it/s]evaluate for the 35-th batch, evaluate loss: 0.5775851607322693:  44%|███████▊          | 34/78 [00:09<00:12,  3.40it/s]evaluate for the 35-th batch, evaluate loss: 0.5775851607322693:  45%|████████          | 35/78 [00:09<00:12,  3.47it/s]Epoch: 11, train for the 28-th batch, train loss: 0.5212854146957397:  18%|██▏         | 27/151 [00:14<01:06,  1.86it/s]Epoch: 11, train for the 28-th batch, train loss: 0.5212854146957397:  19%|██▏         | 28/151 [00:14<01:05,  1.87it/s]Epoch: 6, train for the 54-th batch, train loss: 0.4607047140598297:  36%|████▋        | 53/146 [00:31<00:57,  1.62it/s]Epoch: 6, train for the 54-th batch, train loss: 0.4607047140598297:  37%|████▊        | 54/146 [00:31<00:56,  1.63it/s]Epoch: 4, train for the 77-th batch, train loss: 0.6227957010269165:  32%|████▏        | 76/237 [00:42<01:34,  1.70it/s]Epoch: 4, train for the 77-th batch, train loss: 0.6227957010269165:  32%|████▏        | 77/237 [00:42<01:33,  1.70it/s]evaluate for the 36-th batch, evaluate loss: 0.5963318943977356:  45%|████████          | 35/78 [00:10<00:12,  3.47it/s]evaluate for the 36-th batch, evaluate loss: 0.5963318943977356:  46%|████████▎         | 36/78 [00:10<00:12,  3.40it/s]Epoch: 7, train for the 33-th batch, train loss: 0.19031858444213867:  27%|███▏        | 32/119 [00:20<00:53,  1.62it/s]Epoch: 7, train for the 33-th batch, train loss: 0.19031858444213867:  28%|███▎        | 33/119 [00:20<00:52,  1.63it/s]evaluate for the 37-th batch, evaluate loss: 0.6131020784378052:  46%|████████▎         | 36/78 [00:10<00:12,  3.40it/s]evaluate for the 37-th batch, evaluate loss: 0.6131020784378052:  47%|████████▌         | 37/78 [00:10<00:11,  3.45it/s]Epoch: 11, train for the 29-th batch, train loss: 0.6730372309684753:  19%|██▏         | 28/151 [00:14<01:05,  1.87it/s]Epoch: 11, train for the 29-th batch, train loss: 0.6730372309684753:  19%|██▎         | 29/151 [00:14<01:06,  1.82it/s]Epoch: 6, train for the 55-th batch, train loss: 0.523892343044281:  37%|█████▏        | 54/146 [00:32<00:56,  1.63it/s]Epoch: 6, train for the 55-th batch, train loss: 0.523892343044281:  38%|█████▎        | 55/146 [00:32<00:56,  1.62it/s]Epoch: 4, train for the 78-th batch, train loss: 0.565666913986206:  32%|████▌         | 77/237 [00:43<01:33,  1.70it/s]Epoch: 4, train for the 78-th batch, train loss: 0.565666913986206:  33%|████▌         | 78/237 [00:43<01:32,  1.71it/s]evaluate for the 38-th batch, evaluate loss: 0.5179709792137146:  47%|████████▌         | 37/78 [00:10<00:11,  3.45it/s]evaluate for the 38-th batch, evaluate loss: 0.5179709792137146:  49%|████████▊         | 38/78 [00:10<00:11,  3.40it/s]Epoch: 7, train for the 34-th batch, train loss: 0.19231824576854706:  28%|███▎        | 33/119 [00:20<00:52,  1.63it/s]Epoch: 7, train for the 34-th batch, train loss: 0.19231824576854706:  29%|███▍        | 34/119 [00:20<00:52,  1.63it/s]evaluate for the 39-th batch, evaluate loss: 0.6032426953315735:  49%|████████▊         | 38/78 [00:11<00:11,  3.40it/s]evaluate for the 39-th batch, evaluate loss: 0.6032426953315735:  50%|█████████         | 39/78 [00:11<00:11,  3.48it/s]Epoch: 11, train for the 30-th batch, train loss: 0.6474887132644653:  19%|██▎         | 29/151 [00:15<01:06,  1.82it/s]Epoch: 11, train for the 30-th batch, train loss: 0.6474887132644653:  20%|██▍         | 30/151 [00:15<01:06,  1.82it/s]Epoch: 6, train for the 56-th batch, train loss: 0.4918423593044281:  38%|████▉        | 55/146 [00:32<00:56,  1.62it/s]Epoch: 6, train for the 56-th batch, train loss: 0.4918423593044281:  38%|████▉        | 56/146 [00:32<00:55,  1.63it/s]Epoch: 4, train for the 79-th batch, train loss: 0.6126078367233276:  33%|████▎        | 78/237 [00:43<01:32,  1.71it/s]Epoch: 4, train for the 79-th batch, train loss: 0.6126078367233276:  33%|████▎        | 79/237 [00:43<01:32,  1.71it/s]evaluate for the 40-th batch, evaluate loss: 0.5309564471244812:  50%|█████████         | 39/78 [00:11<00:11,  3.48it/s]evaluate for the 40-th batch, evaluate loss: 0.5309564471244812:  51%|█████████▏        | 40/78 [00:11<00:11,  3.39it/s]Epoch: 7, train for the 35-th batch, train loss: 0.1490594446659088:  29%|███▋         | 34/119 [00:21<00:52,  1.63it/s]Epoch: 7, train for the 35-th batch, train loss: 0.1490594446659088:  29%|███▊         | 35/119 [00:21<00:51,  1.63it/s]Epoch: 11, train for the 31-th batch, train loss: 0.6571423411369324:  20%|██▍         | 30/151 [00:15<01:06,  1.82it/s]Epoch: 11, train for the 31-th batch, train loss: 0.6571423411369324:  21%|██▍         | 31/151 [00:15<01:05,  1.84it/s]evaluate for the 41-th batch, evaluate loss: 0.5473634004592896:  51%|█████████▏        | 40/78 [00:11<00:11,  3.39it/s]evaluate for the 41-th batch, evaluate loss: 0.5473634004592896:  53%|█████████▍        | 41/78 [00:11<00:10,  3.47it/s]Epoch: 4, train for the 80-th batch, train loss: 0.5664735436439514:  33%|████▎        | 79/237 [00:44<01:32,  1.71it/s]Epoch: 4, train for the 80-th batch, train loss: 0.5664735436439514:  34%|████▍        | 80/237 [00:44<01:31,  1.72it/s]evaluate for the 42-th batch, evaluate loss: 0.5249535441398621:  53%|█████████▍        | 41/78 [00:11<00:10,  3.47it/s]evaluate for the 42-th batch, evaluate loss: 0.5249535441398621:  54%|█████████▋        | 42/78 [00:11<00:10,  3.39it/s]Epoch: 6, train for the 57-th batch, train loss: 0.5183894038200378:  38%|████▉        | 56/146 [00:33<00:55,  1.63it/s]Epoch: 6, train for the 57-th batch, train loss: 0.5183894038200378:  39%|█████        | 57/146 [00:33<00:54,  1.63it/s]Epoch: 7, train for the 36-th batch, train loss: 0.19012434780597687:  29%|███▌        | 35/119 [00:22<00:51,  1.63it/s]Epoch: 7, train for the 36-th batch, train loss: 0.19012434780597687:  30%|███▋        | 36/119 [00:22<00:51,  1.63it/s]Epoch: 11, train for the 32-th batch, train loss: 0.5346869230270386:  21%|██▍         | 31/151 [00:16<01:05,  1.84it/s]Epoch: 11, train for the 32-th batch, train loss: 0.5346869230270386:  21%|██▌         | 32/151 [00:16<01:05,  1.80it/s]evaluate for the 43-th batch, evaluate loss: 0.6018072366714478:  54%|█████████▋        | 42/78 [00:12<00:10,  3.39it/s]evaluate for the 43-th batch, evaluate loss: 0.6018072366714478:  55%|█████████▉        | 43/78 [00:12<00:10,  3.50it/s]Epoch: 4, train for the 81-th batch, train loss: 0.6142706274986267:  34%|████▍        | 80/237 [00:44<01:31,  1.72it/s]Epoch: 4, train for the 81-th batch, train loss: 0.6142706274986267:  34%|████▍        | 81/237 [00:44<01:31,  1.71it/s]evaluate for the 44-th batch, evaluate loss: 0.4779156446456909:  55%|█████████▉        | 43/78 [00:12<00:10,  3.50it/s]evaluate for the 44-th batch, evaluate loss: 0.4779156446456909:  56%|██████████▏       | 44/78 [00:12<00:10,  3.36it/s]Epoch: 6, train for the 58-th batch, train loss: 0.4962139427661896:  39%|█████        | 57/146 [00:34<00:54,  1.63it/s]Epoch: 6, train for the 58-th batch, train loss: 0.4962139427661896:  40%|█████▏       | 58/146 [00:34<00:54,  1.63it/s]Epoch: 7, train for the 37-th batch, train loss: 0.1575174480676651:  30%|███▉         | 36/119 [00:22<00:51,  1.63it/s]Epoch: 7, train for the 37-th batch, train loss: 0.1575174480676651:  31%|████         | 37/119 [00:22<00:50,  1.63it/s]Epoch: 11, train for the 33-th batch, train loss: 0.6339008212089539:  21%|██▌         | 32/151 [00:16<01:05,  1.80it/s]Epoch: 11, train for the 33-th batch, train loss: 0.6339008212089539:  22%|██▌         | 33/151 [00:16<01:05,  1.80it/s]evaluate for the 45-th batch, evaluate loss: 0.594946026802063:  56%|██████████▋        | 44/78 [00:12<00:10,  3.36it/s]evaluate for the 45-th batch, evaluate loss: 0.594946026802063:  58%|██████████▉        | 45/78 [00:12<00:09,  3.47it/s]Epoch: 4, train for the 82-th batch, train loss: 0.6272136569023132:  34%|████▍        | 81/237 [00:45<01:31,  1.71it/s]Epoch: 4, train for the 82-th batch, train loss: 0.6272136569023132:  35%|████▍        | 82/237 [00:45<01:30,  1.71it/s]evaluate for the 46-th batch, evaluate loss: 0.6229912638664246:  58%|██████████▍       | 45/78 [00:13<00:09,  3.47it/s]evaluate for the 46-th batch, evaluate loss: 0.6229912638664246:  59%|██████████▌       | 46/78 [00:13<00:09,  3.35it/s]Epoch: 6, train for the 59-th batch, train loss: 0.4354493319988251:  40%|█████▏       | 58/146 [00:34<00:54,  1.63it/s]Epoch: 6, train for the 59-th batch, train loss: 0.4354493319988251:  40%|█████▎       | 59/146 [00:34<00:53,  1.63it/s]Epoch: 7, train for the 38-th batch, train loss: 0.15341602265834808:  31%|███▋        | 37/119 [00:23<00:50,  1.63it/s]Epoch: 7, train for the 38-th batch, train loss: 0.15341602265834808:  32%|███▊        | 38/119 [00:23<00:49,  1.63it/s]Epoch: 11, train for the 34-th batch, train loss: 0.5195823311805725:  22%|██▌         | 33/151 [00:17<01:05,  1.80it/s]Epoch: 11, train for the 34-th batch, train loss: 0.5195823311805725:  23%|██▋         | 34/151 [00:17<01:06,  1.75it/s]evaluate for the 47-th batch, evaluate loss: 0.48103442788124084:  59%|██████████       | 46/78 [00:13<00:09,  3.35it/s]evaluate for the 47-th batch, evaluate loss: 0.48103442788124084:  60%|██████████▏      | 47/78 [00:13<00:08,  3.46it/s]Epoch: 4, train for the 83-th batch, train loss: 0.6086015701293945:  35%|████▍        | 82/237 [00:46<01:30,  1.71it/s]Epoch: 4, train for the 83-th batch, train loss: 0.6086015701293945:  35%|████▌        | 83/237 [00:46<01:30,  1.71it/s]evaluate for the 48-th batch, evaluate loss: 0.6261003017425537:  60%|██████████▊       | 47/78 [00:13<00:08,  3.46it/s]evaluate for the 48-th batch, evaluate loss: 0.6261003017425537:  62%|███████████       | 48/78 [00:13<00:08,  3.35it/s]Epoch: 6, train for the 60-th batch, train loss: 0.5083272457122803:  40%|█████▎       | 59/146 [00:35<00:53,  1.63it/s]Epoch: 6, train for the 60-th batch, train loss: 0.5083272457122803:  41%|█████▎       | 60/146 [00:35<00:52,  1.63it/s]Epoch: 11, train for the 35-th batch, train loss: 0.6587505340576172:  23%|██▋         | 34/151 [00:18<01:06,  1.75it/s]Epoch: 11, train for the 35-th batch, train loss: 0.6587505340576172:  23%|██▊         | 35/151 [00:18<01:04,  1.81it/s]Epoch: 7, train for the 39-th batch, train loss: 0.21586965024471283:  32%|███▊        | 38/119 [00:23<00:49,  1.63it/s]Epoch: 7, train for the 39-th batch, train loss: 0.21586965024471283:  33%|███▉        | 39/119 [00:23<00:49,  1.63it/s]evaluate for the 49-th batch, evaluate loss: 0.543037474155426:  62%|███████████▋       | 48/78 [00:14<00:08,  3.35it/s]evaluate for the 49-th batch, evaluate loss: 0.543037474155426:  63%|███████████▉       | 49/78 [00:14<00:08,  3.47it/s]Epoch: 4, train for the 84-th batch, train loss: 0.5948200821876526:  35%|████▌        | 83/237 [00:46<01:30,  1.71it/s]Epoch: 4, train for the 84-th batch, train loss: 0.5948200821876526:  35%|████▌        | 84/237 [00:46<01:29,  1.70it/s]evaluate for the 50-th batch, evaluate loss: 0.5701637864112854:  63%|███████████▎      | 49/78 [00:14<00:08,  3.47it/s]evaluate for the 50-th batch, evaluate loss: 0.5701637864112854:  64%|███████████▌      | 50/78 [00:14<00:08,  3.34it/s]Epoch: 6, train for the 61-th batch, train loss: 0.549413800239563:  41%|█████▊        | 60/146 [00:35<00:52,  1.63it/s]Epoch: 6, train for the 61-th batch, train loss: 0.549413800239563:  42%|█████▊        | 61/146 [00:35<00:52,  1.63it/s]Epoch: 11, train for the 36-th batch, train loss: 0.6701891422271729:  23%|██▊         | 35/151 [00:18<01:04,  1.81it/s]Epoch: 11, train for the 36-th batch, train loss: 0.6701891422271729:  24%|██▊         | 36/151 [00:18<01:04,  1.79it/s]evaluate for the 51-th batch, evaluate loss: 0.553198516368866:  64%|████████████▏      | 50/78 [00:14<00:08,  3.34it/s]evaluate for the 51-th batch, evaluate loss: 0.553198516368866:  65%|████████████▍      | 51/78 [00:14<00:07,  3.47it/s]Epoch: 7, train for the 40-th batch, train loss: 0.19139882922172546:  33%|███▉        | 39/119 [00:24<00:49,  1.63it/s]Epoch: 7, train for the 40-th batch, train loss: 0.19139882922172546:  34%|████        | 40/119 [00:24<00:48,  1.63it/s]Epoch: 4, train for the 85-th batch, train loss: 0.5739730596542358:  35%|████▌        | 84/237 [00:47<01:29,  1.70it/s]Epoch: 4, train for the 85-th batch, train loss: 0.5739730596542358:  36%|████▋        | 85/237 [00:47<01:28,  1.72it/s]evaluate for the 52-th batch, evaluate loss: 0.5523774027824402:  65%|███████████▊      | 51/78 [00:14<00:07,  3.47it/s]evaluate for the 52-th batch, evaluate loss: 0.5523774027824402:  67%|████████████      | 52/78 [00:14<00:07,  3.37it/s]Epoch: 11, train for the 37-th batch, train loss: 0.644416332244873:  24%|███          | 36/151 [00:19<01:04,  1.79it/s]Epoch: 11, train for the 37-th batch, train loss: 0.644416332244873:  25%|███▏         | 37/151 [00:19<01:02,  1.81it/s]Epoch: 6, train for the 62-th batch, train loss: 0.47172579169273376:  42%|█████       | 61/146 [00:36<00:52,  1.63it/s]Epoch: 6, train for the 62-th batch, train loss: 0.47172579169273376:  42%|█████       | 62/146 [00:36<00:51,  1.63it/s]evaluate for the 53-th batch, evaluate loss: 0.47572600841522217:  67%|███████████▎     | 52/78 [00:15<00:07,  3.37it/s]evaluate for the 53-th batch, evaluate loss: 0.47572600841522217:  68%|███████████▌     | 53/78 [00:15<00:07,  3.48it/s]Epoch: 7, train for the 41-th batch, train loss: 0.2339422106742859:  34%|████▎        | 40/119 [00:25<00:48,  1.63it/s]Epoch: 7, train for the 41-th batch, train loss: 0.2339422106742859:  34%|████▍        | 41/119 [00:25<00:47,  1.63it/s]Epoch: 4, train for the 86-th batch, train loss: 0.5569651126861572:  36%|████▋        | 85/237 [00:47<01:28,  1.72it/s]Epoch: 4, train for the 86-th batch, train loss: 0.5569651126861572:  36%|████▋        | 86/237 [00:47<01:27,  1.73it/s]evaluate for the 54-th batch, evaluate loss: 0.5593185424804688:  68%|████████████▏     | 53/78 [00:15<00:07,  3.48it/s]evaluate for the 54-th batch, evaluate loss: 0.5593185424804688:  69%|████████████▍     | 54/78 [00:15<00:07,  3.39it/s]Epoch: 11, train for the 38-th batch, train loss: 0.4381476640701294:  25%|██▉         | 37/151 [00:19<01:02,  1.81it/s]Epoch: 11, train for the 38-th batch, train loss: 0.4381476640701294:  25%|███         | 38/151 [00:19<01:01,  1.83it/s]Epoch: 6, train for the 63-th batch, train loss: 0.49837812781333923:  42%|█████       | 62/146 [00:37<00:51,  1.63it/s]Epoch: 6, train for the 63-th batch, train loss: 0.49837812781333923:  43%|█████▏      | 63/146 [00:37<00:51,  1.63it/s]evaluate for the 55-th batch, evaluate loss: 0.566234827041626:  69%|█████████████▏     | 54/78 [00:15<00:07,  3.39it/s]evaluate for the 55-th batch, evaluate loss: 0.566234827041626:  71%|█████████████▍     | 55/78 [00:15<00:06,  3.52it/s]Epoch: 7, train for the 42-th batch, train loss: 0.23238666355609894:  34%|████▏       | 41/119 [00:25<00:47,  1.63it/s]Epoch: 7, train for the 42-th batch, train loss: 0.23238666355609894:  35%|████▏       | 42/119 [00:25<00:47,  1.63it/s]Epoch: 4, train for the 87-th batch, train loss: 0.6306389570236206:  36%|████▋        | 86/237 [00:48<01:27,  1.73it/s]Epoch: 4, train for the 87-th batch, train loss: 0.6306389570236206:  37%|████▊        | 87/237 [00:48<01:27,  1.71it/s]evaluate for the 56-th batch, evaluate loss: 0.6003385186195374:  71%|████████████▋     | 55/78 [00:16<00:06,  3.52it/s]evaluate for the 56-th batch, evaluate loss: 0.6003385186195374:  72%|████████████▉     | 56/78 [00:16<00:06,  3.39it/s]Epoch: 11, train for the 39-th batch, train loss: 0.47722601890563965:  25%|██▊        | 38/151 [00:20<01:01,  1.83it/s]Epoch: 11, train for the 39-th batch, train loss: 0.47722601890563965:  26%|██▊        | 39/151 [00:20<01:01,  1.83it/s]Epoch: 6, train for the 64-th batch, train loss: 0.5308775305747986:  43%|█████▌       | 63/146 [00:37<00:51,  1.63it/s]Epoch: 6, train for the 64-th batch, train loss: 0.5308775305747986:  44%|█████▋       | 64/146 [00:37<00:50,  1.63it/s]evaluate for the 57-th batch, evaluate loss: 0.5437083840370178:  72%|████████████▉     | 56/78 [00:16<00:06,  3.39it/s]evaluate for the 57-th batch, evaluate loss: 0.5437083840370178:  73%|█████████████▏    | 57/78 [00:16<00:06,  3.49it/s]Epoch: 7, train for the 43-th batch, train loss: 0.2224579155445099:  35%|████▌        | 42/119 [00:26<00:47,  1.63it/s]Epoch: 7, train for the 43-th batch, train loss: 0.2224579155445099:  36%|████▋        | 43/119 [00:26<00:46,  1.63it/s]Epoch: 4, train for the 88-th batch, train loss: 0.6586887836456299:  37%|████▊        | 87/237 [00:48<01:27,  1.71it/s]Epoch: 4, train for the 88-th batch, train loss: 0.6586887836456299:  37%|████▊        | 88/237 [00:48<01:27,  1.71it/s]evaluate for the 58-th batch, evaluate loss: 0.6469900608062744:  73%|█████████████▏    | 57/78 [00:16<00:06,  3.49it/s]evaluate for the 58-th batch, evaluate loss: 0.6469900608062744:  74%|█████████████▍    | 58/78 [00:16<00:05,  3.37it/s]Epoch: 11, train for the 40-th batch, train loss: 0.49866175651550293:  26%|██▊        | 39/151 [00:20<01:01,  1.83it/s]Epoch: 11, train for the 40-th batch, train loss: 0.49866175651550293:  26%|██▉        | 40/151 [00:20<01:02,  1.77it/s]Epoch: 6, train for the 65-th batch, train loss: 0.5524523258209229:  44%|█████▋       | 64/146 [00:38<00:50,  1.63it/s]Epoch: 6, train for the 65-th batch, train loss: 0.5524523258209229:  45%|█████▊       | 65/146 [00:38<00:49,  1.63it/s]evaluate for the 59-th batch, evaluate loss: 0.5971764326095581:  74%|█████████████▍    | 58/78 [00:16<00:05,  3.37it/s]evaluate for the 59-th batch, evaluate loss: 0.5971764326095581:  76%|█████████████▌    | 59/78 [00:16<00:05,  3.47it/s]Epoch: 7, train for the 44-th batch, train loss: 0.17258037626743317:  36%|████▎       | 43/119 [00:26<00:46,  1.63it/s]Epoch: 7, train for the 44-th batch, train loss: 0.17258037626743317:  37%|████▍       | 44/119 [00:26<00:46,  1.63it/s]evaluate for the 60-th batch, evaluate loss: 0.6351016163825989:  76%|█████████████▌    | 59/78 [00:17<00:05,  3.47it/s]evaluate for the 60-th batch, evaluate loss: 0.6351016163825989:  77%|█████████████▊    | 60/78 [00:17<00:04,  3.73it/s]Epoch: 11, train for the 41-th batch, train loss: 0.5298461318016052:  26%|███▏        | 40/151 [00:21<01:02,  1.77it/s]evaluate for the 61-th batch, evaluate loss: 0.5701212286949158:  77%|█████████████▊    | 60/78 [00:17<00:04,  3.73it/s]Epoch: 11, train for the 41-th batch, train loss: 0.5298461318016052:  27%|███▎        | 41/151 [00:21<01:02,  1.76it/s]evaluate for the 61-th batch, evaluate loss: 0.5701212286949158:  78%|██████████████    | 61/78 [00:17<00:03,  4.52it/s]evaluate for the 62-th batch, evaluate loss: 0.6606703996658325:  78%|██████████████    | 61/78 [00:17<00:03,  4.52it/s]evaluate for the 62-th batch, evaluate loss: 0.6606703996658325:  79%|██████████████▎   | 62/78 [00:17<00:03,  4.56it/s]Epoch: 4, train for the 89-th batch, train loss: 0.6555863618850708:  37%|████▊        | 88/237 [00:49<01:27,  1.71it/s]Epoch: 4, train for the 89-th batch, train loss: 0.6555863618850708:  38%|████▉        | 89/237 [00:49<01:37,  1.52it/s]Epoch: 6, train for the 66-th batch, train loss: 0.5121268630027771:  45%|█████▊       | 65/146 [00:38<00:49,  1.63it/s]Epoch: 6, train for the 66-th batch, train loss: 0.5121268630027771:  45%|█████▉       | 66/146 [00:38<00:48,  1.63it/s]Epoch: 7, train for the 45-th batch, train loss: 0.1860133707523346:  37%|████▊        | 44/119 [00:27<00:46,  1.63it/s]Epoch: 7, train for the 45-th batch, train loss: 0.1860133707523346:  38%|████▉        | 45/119 [00:27<00:45,  1.63it/s]evaluate for the 63-th batch, evaluate loss: 0.5903229117393494:  79%|██████████████▎   | 62/78 [00:17<00:03,  4.56it/s]evaluate for the 63-th batch, evaluate loss: 0.5903229117393494:  81%|██████████████▌   | 63/78 [00:17<00:03,  4.32it/s]Epoch: 11, train for the 42-th batch, train loss: 0.6644285917282104:  27%|███▎        | 41/151 [00:21<01:02,  1.76it/s]Epoch: 11, train for the 42-th batch, train loss: 0.6644285917282104:  28%|███▎        | 42/151 [00:21<01:01,  1.77it/s]evaluate for the 64-th batch, evaluate loss: 0.5472598075866699:  81%|██████████████▌   | 63/78 [00:18<00:03,  4.32it/s]evaluate for the 64-th batch, evaluate loss: 0.5472598075866699:  82%|██████████████▊   | 64/78 [00:18<00:03,  3.98it/s]Epoch: 4, train for the 90-th batch, train loss: 0.5680062174797058:  38%|████▉        | 89/237 [00:50<01:37,  1.52it/s]Epoch: 4, train for the 90-th batch, train loss: 0.5680062174797058:  38%|████▉        | 90/237 [00:50<01:32,  1.59it/s]Epoch: 6, train for the 67-th batch, train loss: 0.5366135835647583:  45%|█████▉       | 66/146 [00:39<00:48,  1.63it/s]Epoch: 6, train for the 67-th batch, train loss: 0.5366135835647583:  46%|█████▉       | 67/146 [00:39<00:48,  1.63it/s]Epoch: 7, train for the 46-th batch, train loss: 0.22011198103427887:  38%|████▌       | 45/119 [00:28<00:45,  1.63it/s]Epoch: 7, train for the 46-th batch, train loss: 0.22011198103427887:  39%|████▋       | 46/119 [00:28<00:44,  1.63it/s]evaluate for the 65-th batch, evaluate loss: 0.5664392709732056:  82%|██████████████▊   | 64/78 [00:18<00:03,  3.98it/s]evaluate for the 65-th batch, evaluate loss: 0.5664392709732056:  83%|███████████████   | 65/78 [00:18<00:03,  3.93it/s]Epoch: 11, train for the 43-th batch, train loss: 0.38292673230171204:  28%|███        | 42/151 [00:22<01:01,  1.77it/s]Epoch: 11, train for the 43-th batch, train loss: 0.38292673230171204:  28%|███▏       | 43/151 [00:22<01:01,  1.75it/s]evaluate for the 66-th batch, evaluate loss: 0.5503908395767212:  83%|███████████████   | 65/78 [00:18<00:03,  3.93it/s]evaluate for the 66-th batch, evaluate loss: 0.5503908395767212:  85%|███████████████▏  | 66/78 [00:18<00:03,  3.74it/s]Epoch: 4, train for the 91-th batch, train loss: 0.5761359930038452:  38%|████▉        | 90/237 [00:50<01:32,  1.59it/s]Epoch: 4, train for the 91-th batch, train loss: 0.5761359930038452:  38%|████▉        | 91/237 [00:50<01:29,  1.64it/s]Epoch: 6, train for the 68-th batch, train loss: 0.5215086936950684:  46%|█████▉       | 67/146 [00:40<00:48,  1.63it/s]Epoch: 6, train for the 68-th batch, train loss: 0.5215086936950684:  47%|██████       | 68/146 [00:40<00:47,  1.63it/s]evaluate for the 67-th batch, evaluate loss: 0.5835700035095215:  85%|███████████████▏  | 66/78 [00:18<00:03,  3.74it/s]evaluate for the 67-th batch, evaluate loss: 0.5835700035095215:  86%|███████████████▍  | 67/78 [00:18<00:02,  3.78it/s]Epoch: 7, train for the 47-th batch, train loss: 0.29372093081474304:  39%|████▋       | 46/119 [00:28<00:44,  1.63it/s]Epoch: 7, train for the 47-th batch, train loss: 0.29372093081474304:  39%|████▋       | 47/119 [00:28<00:44,  1.63it/s]Epoch: 11, train for the 44-th batch, train loss: 0.6584290862083435:  28%|███▍        | 43/151 [00:23<01:01,  1.75it/s]Epoch: 11, train for the 44-th batch, train loss: 0.6584290862083435:  29%|███▍        | 44/151 [00:23<00:59,  1.81it/s]evaluate for the 68-th batch, evaluate loss: 0.5615370273590088:  86%|███████████████▍  | 67/78 [00:19<00:02,  3.78it/s]evaluate for the 68-th batch, evaluate loss: 0.5615370273590088:  87%|███████████████▋  | 68/78 [00:19<00:02,  3.64it/s]Epoch: 4, train for the 92-th batch, train loss: 0.6091559529304504:  38%|████▉        | 91/237 [00:51<01:29,  1.64it/s]Epoch: 4, train for the 92-th batch, train loss: 0.6091559529304504:  39%|█████        | 92/237 [00:51<01:26,  1.67it/s]Epoch: 6, train for the 69-th batch, train loss: 0.5517768263816833:  47%|██████       | 68/146 [00:40<00:47,  1.63it/s]Epoch: 6, train for the 69-th batch, train loss: 0.5517768263816833:  47%|██████▏      | 69/146 [00:40<00:47,  1.63it/s]evaluate for the 69-th batch, evaluate loss: 0.5307624936103821:  87%|███████████████▋  | 68/78 [00:19<00:02,  3.64it/s]evaluate for the 69-th batch, evaluate loss: 0.5307624936103821:  88%|███████████████▉  | 69/78 [00:19<00:02,  3.70it/s]Epoch: 11, train for the 45-th batch, train loss: 0.6570753455162048:  29%|███▍        | 44/151 [00:23<00:59,  1.81it/s]Epoch: 11, train for the 45-th batch, train loss: 0.6570753455162048:  30%|███▌        | 45/151 [00:23<00:59,  1.79it/s]Epoch: 7, train for the 48-th batch, train loss: 0.23768261075019836:  39%|████▋       | 47/119 [00:29<00:44,  1.63it/s]Epoch: 7, train for the 48-th batch, train loss: 0.23768261075019836:  40%|████▊       | 48/119 [00:29<00:43,  1.63it/s]evaluate for the 70-th batch, evaluate loss: 0.5227973461151123:  88%|███████████████▉  | 69/78 [00:19<00:02,  3.70it/s]evaluate for the 70-th batch, evaluate loss: 0.5227973461151123:  90%|████████████████▏ | 70/78 [00:19<00:02,  3.60it/s]Epoch: 4, train for the 93-th batch, train loss: 0.5569342970848083:  39%|█████        | 92/237 [00:52<01:26,  1.67it/s]Epoch: 4, train for the 93-th batch, train loss: 0.5569342970848083:  39%|█████        | 93/237 [00:52<01:24,  1.71it/s]Epoch: 7, train for the 49-th batch, train loss: 0.20435434579849243:  40%|████▊       | 48/119 [00:29<00:43,  1.63it/s]Epoch: 7, train for the 49-th batch, train loss: 0.20435434579849243:  41%|████▉       | 49/119 [00:29<00:38,  1.82it/s]evaluate for the 71-th batch, evaluate loss: 0.47623810172080994:  90%|███████████████▎ | 70/78 [00:19<00:02,  3.60it/s]evaluate for the 71-th batch, evaluate loss: 0.47623810172080994:  91%|███████████████▍ | 71/78 [00:19<00:01,  3.67it/s]Epoch: 11, train for the 46-th batch, train loss: 0.6690026521682739:  30%|███▌        | 45/151 [00:24<00:59,  1.79it/s]Epoch: 11, train for the 46-th batch, train loss: 0.6690026521682739:  30%|███▋        | 46/151 [00:24<00:58,  1.81it/s]Epoch: 7, train for the 50-th batch, train loss: 0.1792466640472412:  41%|█████▎       | 49/119 [00:30<00:38,  1.82it/s]Epoch: 7, train for the 50-th batch, train loss: 0.1792466640472412:  42%|█████▍       | 50/119 [00:30<00:33,  2.08it/s]evaluate for the 72-th batch, evaluate loss: 0.6034739017486572:  91%|████████████████▍ | 71/78 [00:20<00:01,  3.67it/s]evaluate for the 72-th batch, evaluate loss: 0.6034739017486572:  92%|████████████████▌ | 72/78 [00:20<00:01,  3.57it/s]Epoch: 6, train for the 70-th batch, train loss: 0.5467973351478577:  47%|██████▏      | 69/146 [00:41<00:47,  1.63it/s]Epoch: 6, train for the 70-th batch, train loss: 0.5467973351478577:  48%|██████▏      | 70/146 [00:41<00:53,  1.42it/s]Epoch: 4, train for the 94-th batch, train loss: 0.6286869049072266:  39%|█████        | 93/237 [00:52<01:24,  1.71it/s]Epoch: 4, train for the 94-th batch, train loss: 0.6286869049072266:  40%|█████▏       | 94/237 [00:52<01:22,  1.73it/s]evaluate for the 73-th batch, evaluate loss: 0.4910908341407776:  92%|████████████████▌ | 72/78 [00:20<00:01,  3.57it/s]evaluate for the 73-th batch, evaluate loss: 0.4910908341407776:  94%|████████████████▊ | 73/78 [00:20<00:01,  3.65it/s]Epoch: 11, train for the 47-th batch, train loss: 0.630982518196106:  30%|███▉         | 46/151 [00:24<00:58,  1.81it/s]Epoch: 11, train for the 47-th batch, train loss: 0.630982518196106:  31%|████         | 47/151 [00:24<00:58,  1.78it/s]evaluate for the 74-th batch, evaluate loss: 0.5419232845306396:  94%|████████████████▊ | 73/78 [00:20<00:01,  3.65it/s]evaluate for the 74-th batch, evaluate loss: 0.5419232845306396:  95%|█████████████████ | 74/78 [00:20<00:01,  3.57it/s]Epoch: 4, train for the 95-th batch, train loss: 0.5394418835639954:  40%|█████▏       | 94/237 [00:53<01:22,  1.73it/s]Epoch: 4, train for the 95-th batch, train loss: 0.5394418835639954:  40%|█████▏       | 95/237 [00:53<01:21,  1.75it/s]Epoch: 7, train for the 51-th batch, train loss: 0.23288238048553467:  42%|█████       | 50/119 [00:30<00:33,  2.08it/s]Epoch: 7, train for the 51-th batch, train loss: 0.23288238048553467:  43%|█████▏      | 51/119 [00:30<00:35,  1.89it/s]Epoch: 6, train for the 71-th batch, train loss: 0.5699623823165894:  48%|██████▏      | 70/146 [00:42<00:53,  1.42it/s]Epoch: 6, train for the 71-th batch, train loss: 0.5699623823165894:  49%|██████▎      | 71/146 [00:42<00:50,  1.48it/s]evaluate for the 75-th batch, evaluate loss: 0.5362910628318787:  95%|█████████████████ | 74/78 [00:21<00:01,  3.57it/s]evaluate for the 75-th batch, evaluate loss: 0.5362910628318787:  96%|█████████████████▎| 75/78 [00:21<00:00,  3.65it/s]Epoch: 11, train for the 48-th batch, train loss: 0.5284069180488586:  31%|███▋        | 47/151 [00:25<00:58,  1.78it/s]Epoch: 11, train for the 48-th batch, train loss: 0.5284069180488586:  32%|███▊        | 48/151 [00:25<00:57,  1.80it/s]evaluate for the 76-th batch, evaluate loss: 0.5938456058502197:  96%|█████████████████▎| 75/78 [00:21<00:00,  3.65it/s]evaluate for the 76-th batch, evaluate loss: 0.5938456058502197:  97%|█████████████████▌| 76/78 [00:21<00:00,  3.56it/s]Epoch: 4, train for the 96-th batch, train loss: 0.5680140852928162:  40%|█████▏       | 95/237 [00:53<01:21,  1.75it/s]Epoch: 4, train for the 96-th batch, train loss: 0.5680140852928162:  41%|█████▎       | 96/237 [00:53<01:19,  1.77it/s]Epoch: 7, train for the 52-th batch, train loss: 0.26619547605514526:  43%|█████▏      | 51/119 [00:31<00:35,  1.89it/s]Epoch: 7, train for the 52-th batch, train loss: 0.26619547605514526:  44%|█████▏      | 52/119 [00:31<00:38,  1.76it/s]Epoch: 6, train for the 72-th batch, train loss: 0.5429829955101013:  49%|██████▎      | 71/146 [00:42<00:50,  1.48it/s]Epoch: 6, train for the 72-th batch, train loss: 0.5429829955101013:  49%|██████▍      | 72/146 [00:42<00:49,  1.49it/s]evaluate for the 77-th batch, evaluate loss: 0.5604317784309387:  97%|█████████████████▌| 76/78 [00:21<00:00,  3.56it/s]evaluate for the 77-th batch, evaluate loss: 0.5604317784309387:  99%|█████████████████▊| 77/78 [00:21<00:00,  3.66it/s]Epoch: 11, train for the 49-th batch, train loss: 0.5518940091133118:  32%|███▊        | 48/151 [00:25<00:57,  1.80it/s]Epoch: 11, train for the 49-th batch, train loss: 0.5518940091133118:  32%|███▉        | 49/151 [00:25<00:56,  1.79it/s]evaluate for the 78-th batch, evaluate loss: 0.6635372638702393:  99%|█████████████████▊| 77/78 [00:21<00:00,  3.66it/s]evaluate for the 78-th batch, evaluate loss: 0.6635372638702393: 100%|██████████████████| 78/78 [00:21<00:00,  4.00it/s]evaluate for the 78-th batch, evaluate loss: 0.6635372638702393: 100%|██████████████████| 78/78 [00:21<00:00,  3.57it/s]
INFO:root:Epoch: 2, learning rate: 0.0001, train loss: 0.4365
INFO:root:train average_precision, 0.8920
INFO:root:train roc_auc, 0.8749
INFO:root:validate loss: 0.4461
INFO:root:validate average_precision, 0.8878
INFO:root:validate roc_auc, 0.8639
INFO:root:new node validate loss: 0.5721
INFO:root:new node validate first_1_average_precision, 0.8118
INFO:root:new node validate first_1_roc_auc, 0.7764
INFO:root:new node validate first_3_average_precision, 0.7941
INFO:root:new node validate first_3_roc_auc, 0.7512
INFO:root:new node validate first_10_average_precision, 0.8120
INFO:root:new node validate first_10_roc_auc, 0.7725
INFO:root:new node validate average_precision, 0.8217
INFO:root:new node validate roc_auc, 0.7809
INFO:root:save model ./saved_models/DyGFormer/ia-slashdot-reply-dir/DyGFormer_seed0_dygformer-ia-slashdot-reply-dir-old/DyGFormer_seed0_dygformer-ia-slashdot-reply-dir-old.pkl
Epoch: 4, train for the 97-th batch, train loss: 0.6166210174560547:  41%|█████▎       | 96/237 [00:54<01:19,  1.77it/s]Epoch: 4, train for the 97-th batch, train loss: 0.6166210174560547:  41%|█████▎       | 97/237 [00:54<01:15,  1.86it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/383 [00:00<?, ?it/s]Epoch: 7, train for the 53-th batch, train loss: 0.1714460700750351:  44%|█████▋       | 52/119 [00:32<00:38,  1.76it/s]Epoch: 7, train for the 53-th batch, train loss: 0.1714460700750351:  45%|█████▊       | 53/119 [00:32<00:39,  1.68it/s]Epoch: 6, train for the 73-th batch, train loss: 0.5456587672233582:  49%|██████▍      | 72/146 [00:43<00:49,  1.49it/s]Epoch: 6, train for the 73-th batch, train loss: 0.5456587672233582:  50%|██████▌      | 73/146 [00:43<00:48,  1.50it/s]Epoch: 11, train for the 50-th batch, train loss: 0.6023966073989868:  32%|███▉        | 49/151 [00:26<00:56,  1.79it/s]Epoch: 11, train for the 50-th batch, train loss: 0.6023966073989868:  33%|███▉        | 50/151 [00:26<00:56,  1.78it/s]Epoch: 4, train for the 98-th batch, train loss: 0.59248948097229:  41%|██████▏        | 97/237 [00:54<01:15,  1.86it/s]Epoch: 4, train for the 98-th batch, train loss: 0.59248948097229:  41%|██████▏        | 98/237 [00:54<01:10,  1.97it/s]Epoch: 3, train for the 1-th batch, train loss: 0.5597233176231384:   0%|                       | 0/383 [00:00<?, ?it/s]Epoch: 3, train for the 1-th batch, train loss: 0.5597233176231384:   0%|               | 1/383 [00:00<02:59,  2.12it/s]Epoch: 11, train for the 51-th batch, train loss: 0.620367705821991:  33%|████▎        | 50/151 [00:26<00:56,  1.78it/s]Epoch: 11, train for the 51-th batch, train loss: 0.620367705821991:  34%|████▍        | 51/151 [00:26<00:48,  2.05it/s]Epoch: 11, train for the 52-th batch, train loss: 0.5705323815345764:  34%|████        | 51/151 [00:26<00:48,  2.05it/s]Epoch: 11, train for the 52-th batch, train loss: 0.5705323815345764:  34%|████▏       | 52/151 [00:26<00:40,  2.46it/s]Epoch: 7, train for the 54-th batch, train loss: 0.1760973334312439:  45%|█████▊       | 53/119 [00:32<00:39,  1.68it/s]Epoch: 7, train for the 54-th batch, train loss: 0.1760973334312439:  45%|█████▉       | 54/119 [00:32<00:39,  1.63it/s]Epoch: 6, train for the 74-th batch, train loss: 0.5530921220779419:  50%|██████▌      | 73/146 [00:44<00:48,  1.50it/s]Epoch: 6, train for the 74-th batch, train loss: 0.5530921220779419:  51%|██████▌      | 74/146 [00:44<00:47,  1.50it/s]Epoch: 4, train for the 99-th batch, train loss: 0.6327850222587585:  41%|█████▍       | 98/237 [00:55<01:10,  1.97it/s]Epoch: 4, train for the 99-th batch, train loss: 0.6327850222587585:  42%|█████▍       | 99/237 [00:55<01:14,  1.86it/s]Epoch: 3, train for the 2-th batch, train loss: 0.5300165414810181:   0%|               | 1/383 [00:00<02:59,  2.12it/s]Epoch: 3, train for the 2-th batch, train loss: 0.5300165414810181:   1%|               | 2/383 [00:00<03:04,  2.06it/s]Epoch: 11, train for the 53-th batch, train loss: 0.6226146817207336:  34%|████▏       | 52/151 [00:27<00:40,  2.46it/s]Epoch: 11, train for the 53-th batch, train loss: 0.6226146817207336:  35%|████▏       | 53/151 [00:27<00:34,  2.85it/s]Epoch: 11, train for the 54-th batch, train loss: 0.5033171772956848:  35%|████▏       | 53/151 [00:27<00:34,  2.85it/s]Epoch: 11, train for the 54-th batch, train loss: 0.5033171772956848:  36%|████▎       | 54/151 [00:27<00:30,  3.22it/s]Epoch: 7, train for the 55-th batch, train loss: 0.16812559962272644:  45%|█████▍      | 54/119 [00:33<00:39,  1.63it/s]Epoch: 7, train for the 55-th batch, train loss: 0.16812559962272644:  46%|█████▌      | 55/119 [00:33<00:36,  1.76it/s]Epoch: 11, train for the 55-th batch, train loss: 0.5150203108787537:  36%|████▎       | 54/151 [00:27<00:30,  3.22it/s]Epoch: 11, train for the 55-th batch, train loss: 0.5150203108787537:  36%|████▎       | 55/151 [00:27<00:27,  3.54it/s]Epoch: 3, train for the 3-th batch, train loss: 0.4166609048843384:   1%|               | 2/383 [00:01<03:04,  2.06it/s]Epoch: 3, train for the 3-th batch, train loss: 0.4166609048843384:   1%|               | 3/383 [00:01<03:05,  2.05it/s]Epoch: 6, train for the 75-th batch, train loss: 0.5542829036712646:  51%|██████▌      | 74/146 [00:44<00:47,  1.50it/s]Epoch: 6, train for the 75-th batch, train loss: 0.5542829036712646:  51%|██████▋      | 75/146 [00:44<00:47,  1.51it/s]Epoch: 4, train for the 100-th batch, train loss: 0.6415640115737915:  42%|█████       | 99/237 [00:55<01:14,  1.86it/s]Epoch: 4, train for the 100-th batch, train loss: 0.6415640115737915:  42%|████▋      | 100/237 [00:55<01:16,  1.78it/s]Epoch: 11, train for the 56-th batch, train loss: 0.512272298336029:  36%|████▋        | 55/151 [00:27<00:27,  3.54it/s]Epoch: 11, train for the 56-th batch, train loss: 0.512272298336029:  37%|████▊        | 56/151 [00:27<00:25,  3.79it/s]Epoch: 11, train for the 57-th batch, train loss: 0.5476812124252319:  37%|████▍       | 56/151 [00:28<00:25,  3.79it/s]Epoch: 11, train for the 57-th batch, train loss: 0.5476812124252319:  38%|████▌       | 57/151 [00:28<00:23,  3.97it/s]Epoch: 7, train for the 56-th batch, train loss: 0.2087399959564209:  46%|██████       | 55/119 [00:33<00:36,  1.76it/s]Epoch: 7, train for the 56-th batch, train loss: 0.2087399959564209:  47%|██████       | 56/119 [00:33<00:36,  1.71it/s]Epoch: 3, train for the 4-th batch, train loss: 0.3155461549758911:   1%|               | 3/383 [00:01<03:05,  2.05it/s]Epoch: 3, train for the 4-th batch, train loss: 0.3155461549758911:   1%|▏              | 4/383 [00:01<03:02,  2.08it/s]Epoch: 4, train for the 101-th batch, train loss: 0.5634942650794983:  42%|████▋      | 100/237 [00:56<01:16,  1.78it/s]Epoch: 4, train for the 101-th batch, train loss: 0.5634942650794983:  43%|████▋      | 101/237 [00:56<01:16,  1.78it/s]Epoch: 6, train for the 76-th batch, train loss: 0.5560286641120911:  51%|██████▋      | 75/146 [00:45<00:47,  1.51it/s]Epoch: 6, train for the 76-th batch, train loss: 0.5560286641120911:  52%|██████▊      | 76/146 [00:45<00:44,  1.56it/s]Epoch: 11, train for the 58-th batch, train loss: 0.556688129901886:  38%|████▉        | 57/151 [00:28<00:23,  3.97it/s]Epoch: 11, train for the 58-th batch, train loss: 0.556688129901886:  38%|████▉        | 58/151 [00:28<00:22,  4.12it/s]Epoch: 11, train for the 59-th batch, train loss: 0.5426435470581055:  38%|████▌       | 58/151 [00:28<00:22,  4.12it/s]Epoch: 11, train for the 59-th batch, train loss: 0.5426435470581055:  39%|████▋       | 59/151 [00:28<00:21,  4.24it/s]Epoch: 3, train for the 5-th batch, train loss: 0.366590291261673:   1%|▏               | 4/383 [00:02<03:02,  2.08it/s]Epoch: 3, train for the 5-th batch, train loss: 0.366590291261673:   1%|▏               | 5/383 [00:02<03:08,  2.01it/s]Epoch: 11, train for the 60-th batch, train loss: 0.5299713015556335:  39%|████▋       | 59/151 [00:28<00:21,  4.24it/s]Epoch: 11, train for the 60-th batch, train loss: 0.5299713015556335:  40%|████▊       | 60/151 [00:28<00:21,  4.31it/s]Epoch: 7, train for the 57-th batch, train loss: 0.1628507822751999:  47%|██████       | 56/119 [00:34<00:36,  1.71it/s]Epoch: 7, train for the 57-th batch, train loss: 0.1628507822751999:  48%|██████▏      | 57/119 [00:34<00:37,  1.66it/s]Epoch: 4, train for the 102-th batch, train loss: 0.6030049920082092:  43%|████▋      | 101/237 [00:56<01:16,  1.78it/s]Epoch: 4, train for the 102-th batch, train loss: 0.6030049920082092:  43%|████▋      | 102/237 [00:56<01:16,  1.77it/s]Epoch: 6, train for the 77-th batch, train loss: 0.5473406910896301:  52%|██████▊      | 76/146 [00:46<00:44,  1.56it/s]Epoch: 6, train for the 77-th batch, train loss: 0.5473406910896301:  53%|██████▊      | 77/146 [00:46<00:43,  1.57it/s]Epoch: 11, train for the 61-th batch, train loss: 0.5287107825279236:  40%|████▊       | 60/151 [00:28<00:21,  4.31it/s]Epoch: 11, train for the 61-th batch, train loss: 0.5287107825279236:  40%|████▊       | 61/151 [00:28<00:20,  4.35it/s]Epoch: 11, train for the 62-th batch, train loss: 0.5205569863319397:  40%|████▊       | 61/151 [00:29<00:20,  4.35it/s]Epoch: 11, train for the 62-th batch, train loss: 0.5205569863319397:  41%|████▉       | 62/151 [00:29<00:20,  4.41it/s]Epoch: 3, train for the 6-th batch, train loss: 0.40184926986694336:   1%|▏             | 5/383 [00:03<03:08,  2.01it/s]Epoch: 3, train for the 6-th batch, train loss: 0.40184926986694336:   2%|▏             | 6/383 [00:03<03:15,  1.93it/s]Epoch: 7, train for the 58-th batch, train loss: 0.17355266213417053:  48%|█████▋      | 57/119 [00:35<00:37,  1.66it/s]Epoch: 7, train for the 58-th batch, train loss: 0.17355266213417053:  49%|█████▊      | 58/119 [00:35<00:36,  1.65it/s]Epoch: 11, train for the 63-th batch, train loss: 0.5719285607337952:  41%|████▉       | 62/151 [00:29<00:20,  4.41it/s]Epoch: 11, train for the 63-th batch, train loss: 0.5719285607337952:  42%|█████       | 63/151 [00:29<00:19,  4.43it/s]Epoch: 4, train for the 103-th batch, train loss: 0.6276438236236572:  43%|████▋      | 102/237 [00:57<01:16,  1.77it/s]Epoch: 4, train for the 103-th batch, train loss: 0.6276438236236572:  43%|████▊      | 103/237 [00:57<01:16,  1.75it/s]Epoch: 6, train for the 78-th batch, train loss: 0.5296015739440918:  53%|██████▊      | 77/146 [00:46<00:43,  1.57it/s]Epoch: 6, train for the 78-th batch, train loss: 0.5296015739440918:  53%|██████▉      | 78/146 [00:46<00:42,  1.58it/s]Epoch: 11, train for the 64-th batch, train loss: 0.5278961658477783:  42%|█████       | 63/151 [00:29<00:19,  4.43it/s]Epoch: 11, train for the 64-th batch, train loss: 0.5278961658477783:  42%|█████       | 64/151 [00:29<00:19,  4.45it/s]Epoch: 3, train for the 7-th batch, train loss: 0.3974243104457855:   2%|▏              | 6/383 [00:03<03:15,  1.93it/s]Epoch: 3, train for the 7-th batch, train loss: 0.3974243104457855:   2%|▎              | 7/383 [00:03<03:11,  1.96it/s]Epoch: 11, train for the 65-th batch, train loss: 0.4923569858074188:  42%|█████       | 64/151 [00:29<00:19,  4.45it/s]Epoch: 11, train for the 65-th batch, train loss: 0.4923569858074188:  43%|█████▏      | 65/151 [00:29<00:19,  4.46it/s]Epoch: 7, train for the 59-th batch, train loss: 0.1862485408782959:  49%|██████▎      | 58/119 [00:35<00:36,  1.65it/s]Epoch: 7, train for the 59-th batch, train loss: 0.1862485408782959:  50%|██████▍      | 59/119 [00:35<00:36,  1.64it/s]Epoch: 4, train for the 104-th batch, train loss: 0.6171754598617554:  43%|████▊      | 103/237 [00:58<01:16,  1.75it/s]Epoch: 4, train for the 104-th batch, train loss: 0.6171754598617554:  44%|████▊      | 104/237 [00:58<01:15,  1.76it/s]Epoch: 11, train for the 66-th batch, train loss: 0.5643975138664246:  43%|█████▏      | 65/151 [00:30<00:19,  4.46it/s]Epoch: 11, train for the 66-th batch, train loss: 0.5643975138664246:  44%|█████▏      | 66/151 [00:30<00:19,  4.46it/s]Epoch: 6, train for the 79-th batch, train loss: 0.5421391725540161:  53%|██████▉      | 78/146 [00:47<00:42,  1.58it/s]Epoch: 6, train for the 79-th batch, train loss: 0.5421391725540161:  54%|███████      | 79/146 [00:47<00:41,  1.60it/s]Epoch: 3, train for the 8-th batch, train loss: 0.33053889870643616:   2%|▎             | 7/383 [00:04<03:11,  1.96it/s]Epoch: 3, train for the 8-th batch, train loss: 0.33053889870643616:   2%|▎             | 8/383 [00:04<03:13,  1.94it/s]Epoch: 11, train for the 67-th batch, train loss: 0.5619935393333435:  44%|█████▏      | 66/151 [00:30<00:19,  4.46it/s]Epoch: 11, train for the 67-th batch, train loss: 0.5619935393333435:  44%|█████▎      | 67/151 [00:30<00:18,  4.47it/s]Epoch: 11, train for the 68-th batch, train loss: 0.5434154868125916:  44%|█████▎      | 67/151 [00:30<00:18,  4.47it/s]Epoch: 11, train for the 68-th batch, train loss: 0.5434154868125916:  45%|█████▍      | 68/151 [00:30<00:18,  4.47it/s]Epoch: 4, train for the 105-th batch, train loss: 0.6459263563156128:  44%|████▊      | 104/237 [00:58<01:15,  1.76it/s]Epoch: 4, train for the 105-th batch, train loss: 0.6459263563156128:  44%|████▊      | 105/237 [00:58<01:14,  1.77it/s]Epoch: 7, train for the 60-th batch, train loss: 0.14647050201892853:  50%|█████▉      | 59/119 [00:36<00:36,  1.64it/s]Epoch: 7, train for the 60-th batch, train loss: 0.14647050201892853:  50%|██████      | 60/119 [00:36<00:36,  1.64it/s]Epoch: 6, train for the 80-th batch, train loss: 0.5555657744407654:  54%|███████      | 79/146 [00:48<00:41,  1.60it/s]Epoch: 6, train for the 80-th batch, train loss: 0.5555657744407654:  55%|███████      | 80/146 [00:48<00:41,  1.61it/s]Epoch: 11, train for the 69-th batch, train loss: 0.5391530990600586:  45%|█████▍      | 68/151 [00:30<00:18,  4.47it/s]Epoch: 11, train for the 69-th batch, train loss: 0.5391530990600586:  46%|█████▍      | 69/151 [00:30<00:18,  4.48it/s]Epoch: 3, train for the 9-th batch, train loss: 0.34508568048477173:   2%|▎             | 8/383 [00:04<03:13,  1.94it/s]Epoch: 3, train for the 9-th batch, train loss: 0.34508568048477173:   2%|▎             | 9/383 [00:04<03:14,  1.92it/s]Epoch: 11, train for the 70-th batch, train loss: 0.5503256916999817:  46%|█████▍      | 69/151 [00:30<00:18,  4.48it/s]Epoch: 11, train for the 70-th batch, train loss: 0.5503256916999817:  46%|█████▌      | 70/151 [00:30<00:18,  4.49it/s]Epoch: 4, train for the 106-th batch, train loss: 0.6192327737808228:  44%|████▊      | 105/237 [00:59<01:14,  1.77it/s]Epoch: 4, train for the 106-th batch, train loss: 0.6192327737808228:  45%|████▉      | 106/237 [00:59<01:15,  1.73it/s]Epoch: 11, train for the 71-th batch, train loss: 0.4533654749393463:  46%|█████▌      | 70/151 [00:31<00:18,  4.49it/s]Epoch: 11, train for the 71-th batch, train loss: 0.4533654749393463:  47%|█████▋      | 71/151 [00:31<00:17,  4.53it/s]Epoch: 7, train for the 61-th batch, train loss: 0.1445178985595703:  50%|██████▌      | 60/119 [00:36<00:36,  1.64it/s]Epoch: 7, train for the 61-th batch, train loss: 0.1445178985595703:  51%|██████▋      | 61/119 [00:36<00:35,  1.64it/s]Epoch: 3, train for the 10-th batch, train loss: 0.3553997576236725:   2%|▎             | 9/383 [00:05<03:14,  1.92it/s]Epoch: 3, train for the 10-th batch, train loss: 0.3553997576236725:   3%|▎            | 10/383 [00:05<03:18,  1.88it/s]Epoch: 6, train for the 81-th batch, train loss: 0.5055195689201355:  55%|███████      | 80/146 [00:48<00:41,  1.61it/s]Epoch: 6, train for the 81-th batch, train loss: 0.5055195689201355:  55%|███████▏     | 81/146 [00:48<00:40,  1.61it/s]Epoch: 11, train for the 72-th batch, train loss: 0.5394216775894165:  47%|█████▋      | 71/151 [00:31<00:17,  4.53it/s]Epoch: 11, train for the 72-th batch, train loss: 0.5394216775894165:  48%|█████▋      | 72/151 [00:31<00:17,  4.51it/s]Epoch: 11, train for the 73-th batch, train loss: 0.5441513061523438:  48%|█████▋      | 72/151 [00:31<00:17,  4.51it/s]Epoch: 11, train for the 73-th batch, train loss: 0.5441513061523438:  48%|█████▊      | 73/151 [00:31<00:17,  4.50it/s]Epoch: 4, train for the 107-th batch, train loss: 0.6336027979850769:  45%|████▉      | 106/237 [00:59<01:15,  1.73it/s]Epoch: 4, train for the 107-th batch, train loss: 0.6336027979850769:  45%|████▉      | 107/237 [00:59<01:16,  1.69it/s]Epoch: 7, train for the 62-th batch, train loss: 0.24589498341083527:  51%|██████▏     | 61/119 [00:37<00:35,  1.64it/s]Epoch: 7, train for the 62-th batch, train loss: 0.24589498341083527:  52%|██████▎     | 62/119 [00:37<00:34,  1.63it/s]Epoch: 11, train for the 74-th batch, train loss: 0.5472880601882935:  48%|█████▊      | 73/151 [00:31<00:17,  4.50it/s]Epoch: 11, train for the 74-th batch, train loss: 0.5472880601882935:  49%|█████▉      | 74/151 [00:31<00:17,  4.49it/s]Epoch: 3, train for the 11-th batch, train loss: 0.35009732842445374:   3%|▎           | 10/383 [00:05<03:18,  1.88it/s]Epoch: 3, train for the 11-th batch, train loss: 0.35009732842445374:   3%|▎           | 11/383 [00:05<03:24,  1.82it/s]Epoch: 6, train for the 82-th batch, train loss: 0.5253133177757263:  55%|███████▏     | 81/146 [00:49<00:40,  1.61it/s]Epoch: 6, train for the 82-th batch, train loss: 0.5253133177757263:  56%|███████▎     | 82/146 [00:49<00:39,  1.62it/s]Epoch: 11, train for the 75-th batch, train loss: 0.5681691765785217:  49%|█████▉      | 74/151 [00:32<00:17,  4.49it/s]Epoch: 11, train for the 75-th batch, train loss: 0.5681691765785217:  50%|█████▉      | 75/151 [00:32<00:16,  4.48it/s]Epoch: 4, train for the 108-th batch, train loss: 0.5984529256820679:  45%|████▉      | 107/237 [01:00<01:16,  1.69it/s]Epoch: 4, train for the 108-th batch, train loss: 0.5984529256820679:  46%|█████      | 108/237 [01:00<01:10,  1.83it/s]Epoch: 11, train for the 76-th batch, train loss: 0.4733635485172272:  50%|█████▉      | 75/151 [00:32<00:16,  4.48it/s]Epoch: 11, train for the 76-th batch, train loss: 0.4733635485172272:  50%|██████      | 76/151 [00:32<00:16,  4.51it/s]Epoch: 7, train for the 63-th batch, train loss: 0.17387887835502625:  52%|██████▎     | 62/119 [00:38<00:34,  1.63it/s]Epoch: 7, train for the 63-th batch, train loss: 0.17387887835502625:  53%|██████▎     | 63/119 [00:38<00:34,  1.63it/s]Epoch: 3, train for the 12-th batch, train loss: 0.2711056172847748:   3%|▎            | 11/383 [00:06<03:24,  1.82it/s]Epoch: 3, train for the 12-th batch, train loss: 0.2711056172847748:   3%|▍            | 12/383 [00:06<03:30,  1.77it/s]Epoch: 11, train for the 77-th batch, train loss: 0.5142761468887329:  50%|██████      | 76/151 [00:32<00:16,  4.51it/s]Epoch: 11, train for the 77-th batch, train loss: 0.5142761468887329:  51%|██████      | 77/151 [00:32<00:16,  4.48it/s]Epoch: 6, train for the 83-th batch, train loss: 0.5271962881088257:  56%|███████▎     | 82/146 [00:49<00:39,  1.62it/s]Epoch: 6, train for the 83-th batch, train loss: 0.5271962881088257:  57%|███████▍     | 83/146 [00:49<00:38,  1.62it/s]Epoch: 4, train for the 109-th batch, train loss: 0.6667776107788086:  46%|█████      | 108/237 [01:00<01:10,  1.83it/s]Epoch: 4, train for the 109-th batch, train loss: 0.6667776107788086:  46%|█████      | 109/237 [01:00<01:08,  1.86it/s]Epoch: 11, train for the 78-th batch, train loss: 0.49934160709381104:  51%|█████▌     | 77/151 [00:32<00:16,  4.48it/s]Epoch: 11, train for the 78-th batch, train loss: 0.49934160709381104:  52%|█████▋     | 78/151 [00:32<00:16,  4.49it/s]Epoch: 11, train for the 79-th batch, train loss: 0.5898612141609192:  52%|██████▏     | 78/151 [00:32<00:16,  4.49it/s]Epoch: 11, train for the 79-th batch, train loss: 0.5898612141609192:  52%|██████▎     | 79/151 [00:32<00:16,  4.49it/s]Epoch: 3, train for the 13-th batch, train loss: 0.3035551905632019:   3%|▍            | 12/383 [00:06<03:30,  1.77it/s]Epoch: 3, train for the 13-th batch, train loss: 0.3035551905632019:   3%|▍            | 13/383 [00:06<03:24,  1.81it/s]Epoch: 7, train for the 64-th batch, train loss: 0.1631283164024353:  53%|██████▉      | 63/119 [00:38<00:34,  1.63it/s]Epoch: 7, train for the 64-th batch, train loss: 0.1631283164024353:  54%|██████▉      | 64/119 [00:38<00:33,  1.63it/s]Epoch: 11, train for the 80-th batch, train loss: 0.5400924682617188:  52%|██████▎     | 79/151 [00:33<00:16,  4.49it/s]Epoch: 11, train for the 80-th batch, train loss: 0.5400924682617188:  53%|██████▎     | 80/151 [00:33<00:15,  4.49it/s]Epoch: 6, train for the 84-th batch, train loss: 0.4891660213470459:  57%|███████▍     | 83/146 [00:50<00:38,  1.62it/s]Epoch: 6, train for the 84-th batch, train loss: 0.4891660213470459:  58%|███████▍     | 84/146 [00:50<00:38,  1.62it/s]Epoch: 4, train for the 110-th batch, train loss: 0.6209651827812195:  46%|█████      | 109/237 [01:01<01:08,  1.86it/s]Epoch: 4, train for the 110-th batch, train loss: 0.6209651827812195:  46%|█████      | 110/237 [01:01<01:10,  1.81it/s]Epoch: 11, train for the 81-th batch, train loss: 0.4649173617362976:  53%|██████▎     | 80/151 [00:33<00:15,  4.49it/s]Epoch: 11, train for the 81-th batch, train loss: 0.4649173617362976:  54%|██████▍     | 81/151 [00:33<00:15,  4.53it/s]Epoch: 3, train for the 14-th batch, train loss: 0.30981773138046265:   3%|▍           | 13/383 [00:07<03:24,  1.81it/s]Epoch: 3, train for the 14-th batch, train loss: 0.30981773138046265:   4%|▍           | 14/383 [00:07<03:27,  1.78it/s]Epoch: 11, train for the 82-th batch, train loss: 0.571366012096405:  54%|██████▉      | 81/151 [00:33<00:15,  4.53it/s]Epoch: 11, train for the 82-th batch, train loss: 0.571366012096405:  54%|███████      | 82/151 [00:33<00:15,  4.53it/s]Epoch: 7, train for the 65-th batch, train loss: 0.17566777765750885:  54%|██████▍     | 64/119 [00:39<00:33,  1.63it/s]Epoch: 7, train for the 65-th batch, train loss: 0.17566777765750885:  55%|██████▌     | 65/119 [00:39<00:33,  1.63it/s]Epoch: 6, train for the 85-th batch, train loss: 0.49897170066833496:  58%|██████▉     | 84/146 [00:51<00:38,  1.62it/s]Epoch: 6, train for the 85-th batch, train loss: 0.49897170066833496:  58%|██████▉     | 85/146 [00:51<00:37,  1.62it/s]Epoch: 11, train for the 83-th batch, train loss: 0.5520737171173096:  54%|██████▌     | 82/151 [00:33<00:15,  4.53it/s]Epoch: 11, train for the 83-th batch, train loss: 0.5520737171173096:  55%|██████▌     | 83/151 [00:33<00:15,  4.52it/s]Epoch: 4, train for the 111-th batch, train loss: 0.5993676781654358:  46%|█████      | 110/237 [01:02<01:10,  1.81it/s]Epoch: 4, train for the 111-th batch, train loss: 0.5993676781654358:  47%|█████▏     | 111/237 [01:02<01:10,  1.79it/s]Epoch: 11, train for the 84-th batch, train loss: 0.5863805413246155:  55%|██████▌     | 83/151 [00:34<00:15,  4.52it/s]Epoch: 11, train for the 84-th batch, train loss: 0.5863805413246155:  56%|██████▋     | 84/151 [00:34<00:14,  4.52it/s]Epoch: 3, train for the 15-th batch, train loss: 0.33414480090141296:   4%|▍           | 14/383 [00:07<03:27,  1.78it/s]Epoch: 3, train for the 15-th batch, train loss: 0.33414480090141296:   4%|▍           | 15/383 [00:07<03:28,  1.77it/s]Epoch: 7, train for the 66-th batch, train loss: 0.20833365619182587:  55%|██████▌     | 65/119 [00:39<00:33,  1.63it/s]Epoch: 7, train for the 66-th batch, train loss: 0.20833365619182587:  55%|██████▋     | 66/119 [00:39<00:32,  1.63it/s]Epoch: 11, train for the 85-th batch, train loss: 0.47333791851997375:  56%|██████     | 84/151 [00:34<00:14,  4.52it/s]Epoch: 11, train for the 85-th batch, train loss: 0.47333791851997375:  56%|██████▏    | 85/151 [00:34<00:14,  4.53it/s]Epoch: 6, train for the 86-th batch, train loss: 0.5394653081893921:  58%|███████▌     | 85/146 [00:51<00:37,  1.62it/s]Epoch: 6, train for the 86-th batch, train loss: 0.5394653081893921:  59%|███████▋     | 86/146 [00:51<00:37,  1.62it/s]Epoch: 4, train for the 112-th batch, train loss: 0.6210264563560486:  47%|█████▏     | 111/237 [01:02<01:10,  1.79it/s]Epoch: 4, train for the 112-th batch, train loss: 0.6210264563560486:  47%|█████▏     | 112/237 [01:02<01:10,  1.77it/s]Epoch: 11, train for the 86-th batch, train loss: 0.5674465298652649:  56%|██████▊     | 85/151 [00:34<00:14,  4.53it/s]Epoch: 11, train for the 86-th batch, train loss: 0.5674465298652649:  57%|██████▊     | 86/151 [00:34<00:14,  4.51it/s]Epoch: 3, train for the 16-th batch, train loss: 0.37940964102745056:   4%|▍           | 15/383 [00:08<03:28,  1.77it/s]Epoch: 11, train for the 87-th batch, train loss: 0.5468776822090149:  57%|██████▊     | 86/151 [00:34<00:14,  4.51it/s]Epoch: 11, train for the 87-th batch, train loss: 0.5468776822090149:  58%|██████▉     | 87/151 [00:34<00:14,  4.51it/s]Epoch: 3, train for the 16-th batch, train loss: 0.37940964102745056:   4%|▌           | 16/383 [00:08<03:26,  1.78it/s]Epoch: 7, train for the 67-th batch, train loss: 0.26610612869262695:  55%|██████▋     | 66/119 [00:40<00:32,  1.63it/s]Epoch: 7, train for the 67-th batch, train loss: 0.26610612869262695:  56%|██████▊     | 67/119 [00:40<00:32,  1.62it/s]Epoch: 11, train for the 88-th batch, train loss: 0.5848961472511292:  58%|██████▉     | 87/151 [00:34<00:14,  4.51it/s]Epoch: 11, train for the 88-th batch, train loss: 0.5848961472511292:  58%|██████▉     | 88/151 [00:34<00:13,  4.51it/s]Epoch: 4, train for the 113-th batch, train loss: 0.6723260879516602:  47%|█████▏     | 112/237 [01:03<01:10,  1.77it/s]Epoch: 4, train for the 113-th batch, train loss: 0.6723260879516602:  48%|█████▏     | 113/237 [01:03<01:10,  1.77it/s]Epoch: 6, train for the 87-th batch, train loss: 0.5286166667938232:  59%|███████▋     | 86/146 [00:52<00:37,  1.62it/s]Epoch: 6, train for the 87-th batch, train loss: 0.5286166667938232:  60%|███████▋     | 87/146 [00:52<00:36,  1.61it/s]Epoch: 11, train for the 89-th batch, train loss: 0.5623793601989746:  58%|██████▉     | 88/151 [00:35<00:13,  4.51it/s]Epoch: 11, train for the 89-th batch, train loss: 0.5623793601989746:  59%|███████     | 89/151 [00:35<00:13,  4.49it/s]Epoch: 3, train for the 17-th batch, train loss: 0.43321916460990906:   4%|▌           | 16/383 [00:09<03:26,  1.78it/s]Epoch: 3, train for the 17-th batch, train loss: 0.43321916460990906:   4%|▌           | 17/383 [00:09<03:24,  1.79it/s]Epoch: 7, train for the 68-th batch, train loss: 0.13547740876674652:  56%|██████▊     | 67/119 [00:41<00:32,  1.62it/s]Epoch: 7, train for the 68-th batch, train loss: 0.13547740876674652:  57%|██████▊     | 68/119 [00:41<00:31,  1.61it/s]Epoch: 4, train for the 114-th batch, train loss: 0.640189528465271:  48%|█████▋      | 113/237 [01:03<01:10,  1.77it/s]Epoch: 4, train for the 114-th batch, train loss: 0.640189528465271:  48%|█████▊      | 114/237 [01:03<01:09,  1.78it/s]Epoch: 6, train for the 88-th batch, train loss: 0.5856820344924927:  60%|███████▋     | 87/146 [00:52<00:36,  1.61it/s]Epoch: 6, train for the 88-th batch, train loss: 0.5856820344924927:  60%|███████▊     | 88/146 [00:52<00:35,  1.61it/s]Epoch: 11, train for the 90-th batch, train loss: 0.5582098364830017:  59%|███████     | 89/151 [00:35<00:13,  4.49it/s]Epoch: 11, train for the 90-th batch, train loss: 0.5582098364830017:  60%|███████▏    | 90/151 [00:35<00:18,  3.25it/s]Epoch: 3, train for the 18-th batch, train loss: 0.3639450669288635:   4%|▌            | 17/383 [00:09<03:24,  1.79it/s]Epoch: 3, train for the 18-th batch, train loss: 0.3639450669288635:   5%|▌            | 18/383 [00:09<03:25,  1.78it/s]Epoch: 11, train for the 91-th batch, train loss: 0.5057067275047302:  60%|███████▏    | 90/151 [00:35<00:18,  3.25it/s]Epoch: 11, train for the 91-th batch, train loss: 0.5057067275047302:  60%|███████▏    | 91/151 [00:35<00:16,  3.54it/s]Epoch: 11, train for the 92-th batch, train loss: 0.5645743012428284:  60%|███████▏    | 91/151 [00:36<00:16,  3.54it/s]Epoch: 11, train for the 92-th batch, train loss: 0.5645743012428284:  61%|███████▎    | 92/151 [00:36<00:15,  3.78it/s]Epoch: 7, train for the 69-th batch, train loss: 0.15552671253681183:  57%|██████▊     | 68/119 [00:41<00:31,  1.61it/s]Epoch: 7, train for the 69-th batch, train loss: 0.15552671253681183:  58%|██████▉     | 69/119 [00:41<00:31,  1.61it/s]Epoch: 4, train for the 115-th batch, train loss: 0.6123426556587219:  48%|█████▎     | 114/237 [01:04<01:09,  1.78it/s]Epoch: 4, train for the 115-th batch, train loss: 0.6123426556587219:  49%|█████▎     | 115/237 [01:04<01:08,  1.78it/s]Epoch: 6, train for the 89-th batch, train loss: 0.5143526196479797:  60%|███████▊     | 88/146 [00:53<00:35,  1.61it/s]Epoch: 6, train for the 89-th batch, train loss: 0.5143526196479797:  61%|███████▉     | 89/146 [00:53<00:35,  1.61it/s]Epoch: 11, train for the 93-th batch, train loss: 0.5218283534049988:  61%|███████▎    | 92/151 [00:36<00:15,  3.78it/s]Epoch: 11, train for the 93-th batch, train loss: 0.5218283534049988:  62%|███████▍    | 93/151 [00:36<00:14,  3.98it/s]Epoch: 3, train for the 19-th batch, train loss: 0.2560451328754425:   5%|▌            | 18/383 [00:10<03:25,  1.78it/s]Epoch: 3, train for the 19-th batch, train loss: 0.2560451328754425:   5%|▋            | 19/383 [00:10<03:24,  1.78it/s]Epoch: 11, train for the 94-th batch, train loss: 0.5354171395301819:  62%|███████▍    | 93/151 [00:36<00:14,  3.98it/s]Epoch: 11, train for the 94-th batch, train loss: 0.5354171395301819:  62%|███████▍    | 94/151 [00:36<00:13,  4.12it/s]Epoch: 4, train for the 116-th batch, train loss: 0.633285403251648:  49%|█████▊      | 115/237 [01:04<01:08,  1.78it/s]Epoch: 4, train for the 116-th batch, train loss: 0.633285403251648:  49%|█████▊      | 116/237 [01:04<01:08,  1.78it/s]Epoch: 7, train for the 70-th batch, train loss: 0.12451305240392685:  58%|██████▉     | 69/119 [00:42<00:31,  1.61it/s]Epoch: 7, train for the 70-th batch, train loss: 0.12451305240392685:  59%|███████     | 70/119 [00:42<00:30,  1.61it/s]Epoch: 11, train for the 95-th batch, train loss: 0.49843963980674744:  62%|██████▊    | 94/151 [00:36<00:13,  4.12it/s]Epoch: 11, train for the 95-th batch, train loss: 0.49843963980674744:  63%|██████▉    | 95/151 [00:36<00:13,  4.21it/s]Epoch: 6, train for the 90-th batch, train loss: 0.549166738986969:  61%|████████▌     | 89/146 [00:54<00:35,  1.61it/s]Epoch: 6, train for the 90-th batch, train loss: 0.549166738986969:  62%|████████▋     | 90/146 [00:54<00:34,  1.61it/s]Epoch: 3, train for the 20-th batch, train loss: 0.35378217697143555:   5%|▌           | 19/383 [00:10<03:24,  1.78it/s]Epoch: 3, train for the 20-th batch, train loss: 0.35378217697143555:   5%|▋           | 20/383 [00:10<03:23,  1.78it/s]Epoch: 11, train for the 96-th batch, train loss: 0.5439704656600952:  63%|███████▌    | 95/151 [00:37<00:13,  4.21it/s]Epoch: 11, train for the 96-th batch, train loss: 0.5439704656600952:  64%|███████▋    | 96/151 [00:37<00:12,  4.26it/s]Epoch: 11, train for the 97-th batch, train loss: 0.5911247134208679:  64%|███████▋    | 96/151 [00:37<00:12,  4.26it/s]Epoch: 11, train for the 97-th batch, train loss: 0.5911247134208679:  64%|███████▋    | 97/151 [00:37<00:12,  4.32it/s]Epoch: 4, train for the 117-th batch, train loss: 0.6465423703193665:  49%|█████▍     | 116/237 [01:05<01:08,  1.78it/s]Epoch: 4, train for the 117-th batch, train loss: 0.6465423703193665:  49%|█████▍     | 117/237 [01:05<01:07,  1.78it/s]Epoch: 7, train for the 71-th batch, train loss: 0.22309504449367523:  59%|███████     | 70/119 [00:43<00:30,  1.61it/s]Epoch: 7, train for the 71-th batch, train loss: 0.22309504449367523:  60%|███████▏    | 71/119 [00:43<00:29,  1.61it/s]Epoch: 11, train for the 98-th batch, train loss: 0.6229377388954163:  64%|███████▋    | 97/151 [00:37<00:12,  4.32it/s]Epoch: 11, train for the 98-th batch, train loss: 0.6229377388954163:  65%|███████▊    | 98/151 [00:37<00:12,  4.35it/s]Epoch: 3, train for the 21-th batch, train loss: 0.312797874212265:   5%|▋             | 20/383 [00:11<03:23,  1.78it/s]Epoch: 3, train for the 21-th batch, train loss: 0.312797874212265:   5%|▊             | 21/383 [00:11<03:24,  1.77it/s]Epoch: 6, train for the 91-th batch, train loss: 0.5578042268753052:  62%|████████     | 90/146 [00:54<00:34,  1.61it/s]Epoch: 6, train for the 91-th batch, train loss: 0.5578042268753052:  62%|████████     | 91/146 [00:54<00:34,  1.60it/s]Epoch: 11, train for the 99-th batch, train loss: 0.6430778503417969:  65%|███████▊    | 98/151 [00:37<00:12,  4.35it/s]Epoch: 11, train for the 99-th batch, train loss: 0.6430778503417969:  66%|███████▊    | 99/151 [00:37<00:11,  4.39it/s]Epoch: 4, train for the 118-th batch, train loss: 0.6147767901420593:  49%|█████▍     | 117/237 [01:05<01:07,  1.78it/s]Epoch: 4, train for the 118-th batch, train loss: 0.6147767901420593:  50%|█████▍     | 118/237 [01:05<01:07,  1.77it/s]Epoch: 11, train for the 100-th batch, train loss: 0.6970489621162415:  66%|███████▏   | 99/151 [00:37<00:11,  4.39it/s]Epoch: 11, train for the 100-th batch, train loss: 0.6970489621162415:  66%|██████▌   | 100/151 [00:37<00:11,  4.40it/s]Epoch: 7, train for the 72-th batch, train loss: 0.19655364751815796:  60%|███████▏    | 71/119 [00:43<00:29,  1.61it/s]Epoch: 7, train for the 72-th batch, train loss: 0.19655364751815796:  61%|███████▎    | 72/119 [00:43<00:29,  1.60it/s]Epoch: 3, train for the 22-th batch, train loss: 0.2723841667175293:   5%|▋            | 21/383 [00:11<03:24,  1.77it/s]Epoch: 3, train for the 22-th batch, train loss: 0.2723841667175293:   6%|▋            | 22/383 [00:11<03:26,  1.75it/s]Epoch: 11, train for the 101-th batch, train loss: 0.6844701766967773:  66%|██████▌   | 100/151 [00:38<00:11,  4.40it/s]Epoch: 11, train for the 101-th batch, train loss: 0.6844701766967773:  67%|██████▋   | 101/151 [00:38<00:11,  4.41it/s]Epoch: 6, train for the 92-th batch, train loss: 0.5613493919372559:  62%|████████     | 91/146 [00:55<00:34,  1.60it/s]Epoch: 6, train for the 92-th batch, train loss: 0.5613493919372559:  63%|████████▏    | 92/146 [00:55<00:33,  1.60it/s]Epoch: 11, train for the 102-th batch, train loss: 0.620448887348175:  67%|███████▎   | 101/151 [00:38<00:11,  4.41it/s]Epoch: 11, train for the 102-th batch, train loss: 0.620448887348175:  68%|███████▍   | 102/151 [00:38<00:11,  4.42it/s]Epoch: 4, train for the 119-th batch, train loss: 0.6332264542579651:  50%|█████▍     | 118/237 [01:06<01:07,  1.77it/s]Epoch: 4, train for the 119-th batch, train loss: 0.6332264542579651:  50%|█████▌     | 119/237 [01:06<01:07,  1.76it/s]Epoch: 11, train for the 103-th batch, train loss: 0.605592668056488:  68%|███████▍   | 102/151 [00:38<00:11,  4.42it/s]Epoch: 11, train for the 103-th batch, train loss: 0.605592668056488:  68%|███████▌   | 103/151 [00:38<00:10,  4.43it/s]Epoch: 7, train for the 73-th batch, train loss: 0.18532466888427734:  61%|███████▎    | 72/119 [00:44<00:29,  1.60it/s]Epoch: 7, train for the 73-th batch, train loss: 0.18532466888427734:  61%|███████▎    | 73/119 [00:44<00:28,  1.61it/s]Epoch: 3, train for the 23-th batch, train loss: 0.38391897082328796:   6%|▋           | 22/383 [00:12<03:26,  1.75it/s]Epoch: 3, train for the 23-th batch, train loss: 0.38391897082328796:   6%|▋           | 23/383 [00:12<03:25,  1.75it/s]Epoch: 6, train for the 93-th batch, train loss: 0.5517702698707581:  63%|████████▏    | 92/146 [00:56<00:33,  1.60it/s]Epoch: 6, train for the 93-th batch, train loss: 0.5517702698707581:  64%|████████▎    | 93/146 [00:56<00:32,  1.61it/s]Epoch: 11, train for the 104-th batch, train loss: 0.6289675235748291:  68%|██████▊   | 103/151 [00:38<00:10,  4.43it/s]Epoch: 11, train for the 104-th batch, train loss: 0.6289675235748291:  69%|██████▉   | 104/151 [00:38<00:10,  4.43it/s]Epoch: 4, train for the 120-th batch, train loss: 0.6349356174468994:  50%|█████▌     | 119/237 [01:07<01:07,  1.76it/s]Epoch: 4, train for the 120-th batch, train loss: 0.6349356174468994:  51%|█████▌     | 120/237 [01:07<01:06,  1.75it/s]Epoch: 11, train for the 105-th batch, train loss: 0.5632272958755493:  69%|██████▉   | 104/151 [00:39<00:10,  4.43it/s]Epoch: 11, train for the 105-th batch, train loss: 0.5632272958755493:  70%|██████▉   | 105/151 [00:39<00:10,  4.44it/s]Epoch: 7, train for the 74-th batch, train loss: 0.19536449015140533:  61%|███████▎    | 73/119 [00:44<00:28,  1.61it/s]Epoch: 7, train for the 74-th batch, train loss: 0.19536449015140533:  62%|███████▍    | 74/119 [00:44<00:27,  1.61it/s]Epoch: 3, train for the 24-th batch, train loss: 0.31347861886024475:   6%|▋           | 23/383 [00:13<03:25,  1.75it/s]Epoch: 3, train for the 24-th batch, train loss: 0.31347861886024475:   6%|▊           | 24/383 [00:13<03:24,  1.76it/s]Epoch: 11, train for the 106-th batch, train loss: 0.5494768619537354:  70%|██████▉   | 105/151 [00:39<00:10,  4.44it/s]Epoch: 11, train for the 106-th batch, train loss: 0.5494768619537354:  70%|███████   | 106/151 [00:39<00:10,  4.43it/s]Epoch: 6, train for the 94-th batch, train loss: 0.5311927199363708:  64%|████████▎    | 93/146 [00:56<00:32,  1.61it/s]Epoch: 6, train for the 94-th batch, train loss: 0.5311927199363708:  64%|████████▎    | 94/146 [00:56<00:32,  1.61it/s]Epoch: 11, train for the 107-th batch, train loss: 0.543048083782196:  70%|███████▋   | 106/151 [00:39<00:10,  4.43it/s]Epoch: 11, train for the 107-th batch, train loss: 0.543048083782196:  71%|███████▊   | 107/151 [00:39<00:09,  4.42it/s]Epoch: 4, train for the 121-th batch, train loss: 0.6290469169616699:  51%|█████▌     | 120/237 [01:07<01:06,  1.75it/s]Epoch: 4, train for the 121-th batch, train loss: 0.6290469169616699:  51%|█████▌     | 121/237 [01:07<01:05,  1.76it/s]Epoch: 11, train for the 108-th batch, train loss: 0.5753175020217896:  71%|███████   | 107/151 [00:39<00:09,  4.42it/s]Epoch: 11, train for the 108-th batch, train loss: 0.5753175020217896:  72%|███████▏  | 108/151 [00:39<00:09,  4.43it/s]Epoch: 3, train for the 25-th batch, train loss: 0.3093116283416748:   6%|▊            | 24/383 [00:13<03:24,  1.76it/s]Epoch: 3, train for the 25-th batch, train loss: 0.3093116283416748:   7%|▊            | 25/383 [00:13<03:25,  1.74it/s]Epoch: 7, train for the 75-th batch, train loss: 0.16825753450393677:  62%|███████▍    | 74/119 [00:45<00:27,  1.61it/s]Epoch: 7, train for the 75-th batch, train loss: 0.16825753450393677:  63%|███████▌    | 75/119 [00:45<00:27,  1.62it/s]Epoch: 11, train for the 109-th batch, train loss: 0.5427914261817932:  72%|███████▏  | 108/151 [00:39<00:09,  4.43it/s]Epoch: 11, train for the 109-th batch, train loss: 0.5427914261817932:  72%|███████▏  | 109/151 [00:39<00:09,  4.44it/s]Epoch: 6, train for the 95-th batch, train loss: 0.5080600380897522:  64%|████████▎    | 94/146 [00:57<00:32,  1.61it/s]Epoch: 6, train for the 95-th batch, train loss: 0.5080600380897522:  65%|████████▍    | 95/146 [00:57<00:31,  1.61it/s]Epoch: 4, train for the 122-th batch, train loss: 0.6520958542823792:  51%|█████▌     | 121/237 [01:08<01:05,  1.76it/s]Epoch: 4, train for the 122-th batch, train loss: 0.6520958542823792:  51%|█████▋     | 122/237 [01:08<01:05,  1.75it/s]Epoch: 11, train for the 110-th batch, train loss: 0.5683398842811584:  72%|███████▏  | 109/151 [00:40<00:09,  4.44it/s]Epoch: 11, train for the 110-th batch, train loss: 0.5683398842811584:  73%|███████▎  | 110/151 [00:40<00:09,  4.43it/s]Epoch: 11, train for the 111-th batch, train loss: 0.5135381817817688:  73%|███████▎  | 110/151 [00:40<00:09,  4.43it/s]Epoch: 11, train for the 111-th batch, train loss: 0.5135381817817688:  74%|███████▎  | 111/151 [00:40<00:09,  4.44it/s]Epoch: 3, train for the 26-th batch, train loss: 0.30658265948295593:   7%|▊           | 25/383 [00:14<03:25,  1.74it/s]Epoch: 3, train for the 26-th batch, train loss: 0.30658265948295593:   7%|▊           | 26/383 [00:14<03:26,  1.73it/s]Epoch: 7, train for the 76-th batch, train loss: 0.22310571372509003:  63%|███████▌    | 75/119 [00:46<00:27,  1.62it/s]Epoch: 7, train for the 76-th batch, train loss: 0.22310571372509003:  64%|███████▋    | 76/119 [00:46<00:26,  1.61it/s]Epoch: 11, train for the 112-th batch, train loss: 0.5199674367904663:  74%|███████▎  | 111/151 [00:40<00:09,  4.44it/s]Epoch: 11, train for the 112-th batch, train loss: 0.5199674367904663:  74%|███████▍  | 112/151 [00:40<00:08,  4.41it/s]Epoch: 6, train for the 96-th batch, train loss: 0.5361039042472839:  65%|████████▍    | 95/146 [00:57<00:31,  1.61it/s]Epoch: 6, train for the 96-th batch, train loss: 0.5361039042472839:  66%|████████▌    | 96/146 [00:57<00:31,  1.61it/s]Epoch: 4, train for the 123-th batch, train loss: 0.6412158608436584:  51%|█████▋     | 122/237 [01:08<01:05,  1.75it/s]Epoch: 4, train for the 123-th batch, train loss: 0.6412158608436584:  52%|█████▋     | 123/237 [01:08<01:05,  1.73it/s]Epoch: 11, train for the 113-th batch, train loss: 0.5639339089393616:  74%|███████▍  | 112/151 [00:40<00:08,  4.41it/s]Epoch: 11, train for the 113-th batch, train loss: 0.5639339089393616:  75%|███████▍  | 113/151 [00:40<00:08,  4.41it/s]Epoch: 3, train for the 27-th batch, train loss: 0.28011390566825867:   7%|▊           | 26/383 [00:14<03:26,  1.73it/s]Epoch: 3, train for the 27-th batch, train loss: 0.28011390566825867:   7%|▊           | 27/383 [00:14<03:26,  1.72it/s]Epoch: 11, train for the 114-th batch, train loss: 0.505757749080658:  75%|████████▏  | 113/151 [00:41<00:08,  4.41it/s]Epoch: 11, train for the 114-th batch, train loss: 0.505757749080658:  75%|████████▎  | 114/151 [00:41<00:08,  4.42it/s]Epoch: 7, train for the 77-th batch, train loss: 0.15256182849407196:  64%|███████▋    | 76/119 [00:46<00:26,  1.61it/s]Epoch: 7, train for the 77-th batch, train loss: 0.15256182849407196:  65%|███████▊    | 77/119 [00:46<00:25,  1.62it/s]Epoch: 6, train for the 97-th batch, train loss: 0.5326751470565796:  66%|████████▌    | 96/146 [00:58<00:31,  1.61it/s]Epoch: 6, train for the 97-th batch, train loss: 0.5326751470565796:  66%|████████▋    | 97/146 [00:58<00:30,  1.62it/s]Epoch: 11, train for the 115-th batch, train loss: 0.5049337148666382:  75%|███████▌  | 114/151 [00:41<00:08,  4.42it/s]Epoch: 11, train for the 115-th batch, train loss: 0.5049337148666382:  76%|███████▌  | 115/151 [00:41<00:08,  4.43it/s]Epoch: 4, train for the 124-th batch, train loss: 0.6142286658287048:  52%|█████▋     | 123/237 [01:09<01:05,  1.73it/s]Epoch: 4, train for the 124-th batch, train loss: 0.6142286658287048:  52%|█████▊     | 124/237 [01:09<01:05,  1.72it/s]Epoch: 11, train for the 116-th batch, train loss: 0.5121886134147644:  76%|███████▌  | 115/151 [00:41<00:08,  4.43it/s]Epoch: 11, train for the 116-th batch, train loss: 0.5121886134147644:  77%|███████▋  | 116/151 [00:41<00:07,  4.44it/s]Epoch: 3, train for the 28-th batch, train loss: 0.3311302065849304:   7%|▉            | 27/383 [00:15<03:26,  1.72it/s]Epoch: 3, train for the 28-th batch, train loss: 0.3311302065849304:   7%|▉            | 28/383 [00:15<03:26,  1.72it/s]Epoch: 7, train for the 78-th batch, train loss: 0.18170973658561707:  65%|███████▊    | 77/119 [00:47<00:25,  1.62it/s]Epoch: 7, train for the 78-th batch, train loss: 0.18170973658561707:  66%|███████▊    | 78/119 [00:47<00:25,  1.61it/s]Epoch: 11, train for the 117-th batch, train loss: 0.5159473419189453:  77%|███████▋  | 116/151 [00:41<00:07,  4.44it/s]Epoch: 11, train for the 117-th batch, train loss: 0.5159473419189453:  77%|███████▋  | 117/151 [00:41<00:07,  4.44it/s]Epoch: 6, train for the 98-th batch, train loss: 0.5598261952400208:  66%|████████▋    | 97/146 [00:59<00:30,  1.62it/s]Epoch: 6, train for the 98-th batch, train loss: 0.5598261952400208:  67%|████████▋    | 98/146 [00:59<00:29,  1.61it/s]Epoch: 4, train for the 125-th batch, train loss: 0.6275397539138794:  52%|█████▊     | 124/237 [01:10<01:05,  1.72it/s]Epoch: 4, train for the 125-th batch, train loss: 0.6275397539138794:  53%|█████▊     | 125/237 [01:10<01:05,  1.72it/s]Epoch: 11, train for the 118-th batch, train loss: 0.4884602725505829:  77%|███████▋  | 117/151 [00:41<00:07,  4.44it/s]Epoch: 11, train for the 118-th batch, train loss: 0.4884602725505829:  78%|███████▊  | 118/151 [00:41<00:07,  4.45it/s]Epoch: 3, train for the 29-th batch, train loss: 0.33712470531463623:   7%|▉           | 28/383 [00:16<03:26,  1.72it/s]Epoch: 3, train for the 29-th batch, train loss: 0.33712470531463623:   8%|▉           | 29/383 [00:16<03:24,  1.73it/s]Epoch: 11, train for the 119-th batch, train loss: 0.5128983855247498:  78%|███████▊  | 118/151 [00:42<00:07,  4.45it/s]Epoch: 11, train for the 119-th batch, train loss: 0.5128983855247498:  79%|███████▉  | 119/151 [00:42<00:07,  4.47it/s]Epoch: 7, train for the 79-th batch, train loss: 0.18273033201694489:  66%|███████▊    | 78/119 [00:48<00:25,  1.61it/s]Epoch: 7, train for the 79-th batch, train loss: 0.18273033201694489:  66%|███████▉    | 79/119 [00:48<00:24,  1.62it/s]Epoch: 11, train for the 120-th batch, train loss: 0.5692415237426758:  79%|███████▉  | 119/151 [00:42<00:07,  4.47it/s]Epoch: 11, train for the 120-th batch, train loss: 0.5692415237426758:  79%|███████▉  | 120/151 [00:42<00:07,  4.42it/s]Epoch: 4, train for the 126-th batch, train loss: 0.6317123174667358:  53%|█████▊     | 125/237 [01:10<01:05,  1.72it/s]Epoch: 4, train for the 126-th batch, train loss: 0.6317123174667358:  53%|█████▊     | 126/237 [01:10<01:04,  1.73it/s]Epoch: 6, train for the 99-th batch, train loss: 0.5366159677505493:  67%|████████▋    | 98/146 [00:59<00:29,  1.61it/s]Epoch: 6, train for the 99-th batch, train loss: 0.5366159677505493:  68%|████████▊    | 99/146 [00:59<00:29,  1.62it/s]Epoch: 11, train for the 121-th batch, train loss: 0.46809515357017517:  79%|███████▏ | 120/151 [00:42<00:07,  4.42it/s]Epoch: 11, train for the 121-th batch, train loss: 0.46809515357017517:  80%|███████▏ | 121/151 [00:42<00:06,  4.43it/s]Epoch: 3, train for the 30-th batch, train loss: 0.28402620553970337:   8%|▉           | 29/383 [00:16<03:24,  1.73it/s]Epoch: 3, train for the 30-th batch, train loss: 0.28402620553970337:   8%|▉           | 30/383 [00:16<03:23,  1.73it/s]Epoch: 11, train for the 122-th batch, train loss: 0.5524564981460571:  80%|████████  | 121/151 [00:42<00:06,  4.43it/s]Epoch: 11, train for the 122-th batch, train loss: 0.5524564981460571:  81%|████████  | 122/151 [00:42<00:06,  4.44it/s]Epoch: 7, train for the 80-th batch, train loss: 0.17705519497394562:  66%|███████▉    | 79/119 [00:48<00:24,  1.62it/s]Epoch: 7, train for the 80-th batch, train loss: 0.17705519497394562:  67%|████████    | 80/119 [00:48<00:24,  1.61it/s]Epoch: 4, train for the 127-th batch, train loss: 0.6427106261253357:  53%|█████▊     | 126/237 [01:11<01:04,  1.73it/s]Epoch: 4, train for the 127-th batch, train loss: 0.6427106261253357:  54%|█████▉     | 127/237 [01:11<01:03,  1.73it/s]Epoch: 11, train for the 123-th batch, train loss: 0.5571392774581909:  81%|████████  | 122/151 [00:43<00:06,  4.44it/s]Epoch: 11, train for the 123-th batch, train loss: 0.5571392774581909:  81%|████████▏ | 123/151 [00:43<00:06,  4.47it/s]Epoch: 6, train for the 100-th batch, train loss: 0.5698623657226562:  68%|████████▏   | 99/146 [01:00<00:29,  1.62it/s]Epoch: 6, train for the 100-th batch, train loss: 0.5698623657226562:  68%|███████▌   | 100/146 [01:00<00:28,  1.61it/s]Epoch: 11, train for the 124-th batch, train loss: 0.5339668989181519:  81%|████████▏ | 123/151 [00:43<00:06,  4.47it/s]Epoch: 11, train for the 124-th batch, train loss: 0.5339668989181519:  82%|████████▏ | 124/151 [00:43<00:06,  4.48it/s]Epoch: 3, train for the 31-th batch, train loss: 0.3882513642311096:   8%|█            | 30/383 [00:17<03:23,  1.73it/s]Epoch: 3, train for the 31-th batch, train loss: 0.3882513642311096:   8%|█            | 31/383 [00:17<03:23,  1.73it/s]Epoch: 11, train for the 125-th batch, train loss: 0.5581855773925781:  82%|████████▏ | 124/151 [00:43<00:06,  4.48it/s]Epoch: 11, train for the 125-th batch, train loss: 0.5581855773925781:  83%|████████▎ | 125/151 [00:43<00:05,  4.49it/s]Epoch: 7, train for the 81-th batch, train loss: 0.22467350959777832:  67%|████████    | 80/119 [00:49<00:24,  1.61it/s]Epoch: 7, train for the 81-th batch, train loss: 0.22467350959777832:  68%|████████▏   | 81/119 [00:49<00:23,  1.61it/s]Epoch: 4, train for the 128-th batch, train loss: 0.6092427372932434:  54%|█████▉     | 127/237 [01:11<01:03,  1.73it/s]Epoch: 4, train for the 128-th batch, train loss: 0.6092427372932434:  54%|█████▉     | 128/237 [01:11<01:02,  1.73it/s]Epoch: 6, train for the 101-th batch, train loss: 0.5246481895446777:  68%|███████▌   | 100/146 [01:01<00:28,  1.61it/s]Epoch: 6, train for the 101-th batch, train loss: 0.5246481895446777:  69%|███████▌   | 101/146 [01:01<00:27,  1.61it/s]Epoch: 11, train for the 126-th batch, train loss: 0.5370667576789856:  83%|████████▎ | 125/151 [00:43<00:05,  4.49it/s]Epoch: 11, train for the 126-th batch, train loss: 0.5370667576789856:  83%|████████▎ | 126/151 [00:43<00:05,  4.50it/s]Epoch: 3, train for the 32-th batch, train loss: 0.2512841820716858:   8%|█            | 31/383 [00:17<03:23,  1.73it/s]Epoch: 3, train for the 32-th batch, train loss: 0.2512841820716858:   8%|█            | 32/383 [00:17<03:23,  1.72it/s]Epoch: 11, train for the 127-th batch, train loss: 0.5679413080215454:  83%|████████▎ | 126/151 [00:43<00:05,  4.50it/s]Epoch: 11, train for the 127-th batch, train loss: 0.5679413080215454:  84%|████████▍ | 127/151 [00:43<00:05,  4.50it/s]Epoch: 11, train for the 128-th batch, train loss: 0.6000379920005798:  84%|████████▍ | 127/151 [00:44<00:05,  4.50it/s]Epoch: 7, train for the 82-th batch, train loss: 0.1980597823858261:  68%|████████▊    | 81/119 [00:49<00:23,  1.61it/s]Epoch: 11, train for the 128-th batch, train loss: 0.6000379920005798:  85%|████████▍ | 128/151 [00:44<00:05,  4.51it/s]Epoch: 7, train for the 82-th batch, train loss: 0.1980597823858261:  69%|████████▉    | 82/119 [00:49<00:23,  1.61it/s]Epoch: 4, train for the 129-th batch, train loss: 0.6451979875564575:  54%|█████▉     | 128/237 [01:12<01:02,  1.73it/s]Epoch: 4, train for the 129-th batch, train loss: 0.6451979875564575:  54%|█████▉     | 129/237 [01:12<01:02,  1.72it/s]Epoch: 6, train for the 102-th batch, train loss: 0.5539072155952454:  69%|███████▌   | 101/146 [01:01<00:27,  1.61it/s]Epoch: 6, train for the 102-th batch, train loss: 0.5539072155952454:  70%|███████▋   | 102/146 [01:01<00:27,  1.61it/s]Epoch: 11, train for the 129-th batch, train loss: 0.5295812487602234:  85%|████████▍ | 128/151 [00:44<00:05,  4.51it/s]Epoch: 11, train for the 129-th batch, train loss: 0.5295812487602234:  85%|████████▌ | 129/151 [00:44<00:04,  4.52it/s]Epoch: 3, train for the 33-th batch, train loss: 0.30882951617240906:   8%|█           | 32/383 [00:18<03:23,  1.72it/s]Epoch: 3, train for the 33-th batch, train loss: 0.30882951617240906:   9%|█           | 33/383 [00:18<03:21,  1.74it/s]Epoch: 11, train for the 130-th batch, train loss: 0.5564408898353577:  85%|████████▌ | 129/151 [00:44<00:04,  4.52it/s]Epoch: 11, train for the 130-th batch, train loss: 0.5564408898353577:  86%|████████▌ | 130/151 [00:44<00:04,  4.49it/s]Epoch: 4, train for the 130-th batch, train loss: 0.6421617865562439:  54%|█████▉     | 129/237 [01:12<01:02,  1.72it/s]Epoch: 4, train for the 130-th batch, train loss: 0.6421617865562439:  55%|██████     | 130/237 [01:12<01:01,  1.73it/s]Epoch: 7, train for the 83-th batch, train loss: 0.16580535471439362:  69%|████████▎   | 82/119 [00:50<00:23,  1.61it/s]Epoch: 7, train for the 83-th batch, train loss: 0.16580535471439362:  70%|████████▎   | 83/119 [00:50<00:22,  1.61it/s]Epoch: 11, train for the 131-th batch, train loss: 0.5408084988594055:  86%|████████▌ | 130/151 [00:44<00:04,  4.49it/s]Epoch: 11, train for the 131-th batch, train loss: 0.5408084988594055:  87%|████████▋ | 131/151 [00:44<00:04,  4.48it/s]Epoch: 6, train for the 103-th batch, train loss: 0.5399913191795349:  70%|███████▋   | 102/146 [01:02<00:27,  1.61it/s]Epoch: 6, train for the 103-th batch, train loss: 0.5399913191795349:  71%|███████▊   | 103/146 [01:02<00:26,  1.61it/s]Epoch: 4, train for the 131-th batch, train loss: 0.5960023403167725:  55%|██████     | 130/237 [01:13<01:01,  1.73it/s]Epoch: 4, train for the 131-th batch, train loss: 0.5960023403167725:  55%|██████     | 131/237 [01:13<00:52,  2.02it/s]Epoch: 11, train for the 132-th batch, train loss: 0.5134521126747131:  87%|████████▋ | 131/151 [00:45<00:04,  4.48it/s]Epoch: 11, train for the 132-th batch, train loss: 0.5134521126747131:  87%|████████▋ | 132/151 [00:45<00:04,  4.49it/s]Epoch: 3, train for the 34-th batch, train loss: 0.26983121037483215:   9%|█           | 33/383 [00:19<03:21,  1.74it/s]Epoch: 3, train for the 34-th batch, train loss: 0.26983121037483215:   9%|█           | 34/383 [00:19<03:42,  1.57it/s]Epoch: 11, train for the 133-th batch, train loss: 0.5582967400550842:  87%|████████▋ | 132/151 [00:45<00:04,  4.49it/s]Epoch: 11, train for the 133-th batch, train loss: 0.5582967400550842:  88%|████████▊ | 133/151 [00:45<00:04,  4.50it/s]Epoch: 6, train for the 104-th batch, train loss: 0.552329957485199:  71%|████████▍   | 103/146 [01:02<00:26,  1.61it/s]Epoch: 6, train for the 104-th batch, train loss: 0.552329957485199:  71%|████████▌   | 104/146 [01:02<00:24,  1.68it/s]Epoch: 11, train for the 134-th batch, train loss: 0.539598286151886:  88%|█████████▋ | 133/151 [00:45<00:04,  4.50it/s]Epoch: 11, train for the 134-th batch, train loss: 0.539598286151886:  89%|█████████▊ | 134/151 [00:45<00:03,  4.50it/s]Epoch: 7, train for the 84-th batch, train loss: 0.14902172982692719:  70%|████████▎   | 83/119 [00:51<00:22,  1.61it/s]Epoch: 7, train for the 84-th batch, train loss: 0.14902172982692719:  71%|████████▍   | 84/119 [00:51<00:22,  1.53it/s]Epoch: 4, train for the 132-th batch, train loss: 0.6273533701896667:  55%|██████     | 131/237 [01:13<00:52,  2.02it/s]Epoch: 4, train for the 132-th batch, train loss: 0.6273533701896667:  56%|██████▏    | 132/237 [01:13<00:51,  2.03it/s]Epoch: 11, train for the 135-th batch, train loss: 0.5656133890151978:  89%|████████▊ | 134/151 [00:45<00:03,  4.50it/s]Epoch: 11, train for the 135-th batch, train loss: 0.5656133890151978:  89%|████████▉ | 135/151 [00:45<00:03,  4.51it/s]Epoch: 3, train for the 35-th batch, train loss: 0.35565802454948425:   9%|█           | 34/383 [00:19<03:42,  1.57it/s]Epoch: 3, train for the 35-th batch, train loss: 0.35565802454948425:   9%|█           | 35/383 [00:19<03:36,  1.61it/s]Epoch: 11, train for the 136-th batch, train loss: 0.5737694501876831:  89%|████████▉ | 135/151 [00:46<00:03,  4.51it/s]Epoch: 11, train for the 136-th batch, train loss: 0.5737694501876831:  90%|█████████ | 136/151 [00:46<00:03,  4.04it/s]Epoch: 6, train for the 105-th batch, train loss: 0.5665187835693359:  71%|███████▊   | 104/146 [01:03<00:24,  1.68it/s]Epoch: 6, train for the 105-th batch, train loss: 0.5665187835693359:  72%|███████▉   | 105/146 [01:03<00:24,  1.66it/s]Epoch: 4, train for the 133-th batch, train loss: 0.6001971364021301:  56%|██████▏    | 132/237 [01:14<00:51,  2.03it/s]Epoch: 4, train for the 133-th batch, train loss: 0.6001971364021301:  56%|██████▏    | 133/237 [01:14<00:53,  1.93it/s]Epoch: 7, train for the 85-th batch, train loss: 0.18157440423965454:  71%|████████▍   | 84/119 [00:51<00:22,  1.53it/s]Epoch: 7, train for the 85-th batch, train loss: 0.18157440423965454:  71%|████████▌   | 85/119 [00:51<00:21,  1.56it/s]Epoch: 11, train for the 137-th batch, train loss: 0.5940585732460022:  90%|█████████ | 136/151 [00:46<00:03,  4.04it/s]Epoch: 11, train for the 137-th batch, train loss: 0.5940585732460022:  91%|█████████ | 137/151 [00:46<00:03,  4.16it/s]Epoch: 3, train for the 36-th batch, train loss: 0.3678213357925415:   9%|█▏           | 35/383 [00:20<03:36,  1.61it/s]Epoch: 3, train for the 36-th batch, train loss: 0.3678213357925415:   9%|█▏           | 36/383 [00:20<03:27,  1.67it/s]Epoch: 11, train for the 138-th batch, train loss: 0.6218746900558472:  91%|█████████ | 137/151 [00:46<00:03,  4.16it/s]Epoch: 11, train for the 138-th batch, train loss: 0.6218746900558472:  91%|█████████▏| 138/151 [00:46<00:03,  4.25it/s]Epoch: 6, train for the 106-th batch, train loss: 0.5478429794311523:  72%|███████▉   | 105/146 [01:03<00:24,  1.66it/s]Epoch: 6, train for the 106-th batch, train loss: 0.5478429794311523:  73%|███████▉   | 106/146 [01:03<00:21,  1.84it/s]Epoch: 4, train for the 134-th batch, train loss: 0.6290701627731323:  56%|██████▏    | 133/237 [01:14<00:53,  1.93it/s]Epoch: 4, train for the 134-th batch, train loss: 0.6290701627731323:  57%|██████▏    | 134/237 [01:14<00:54,  1.89it/s]Epoch: 11, train for the 139-th batch, train loss: 0.5768333673477173:  91%|█████████▏| 138/151 [00:46<00:03,  4.25it/s]Epoch: 11, train for the 139-th batch, train loss: 0.5768333673477173:  92%|█████████▏| 139/151 [00:46<00:02,  4.33it/s]Epoch: 6, train for the 107-th batch, train loss: 0.5145060420036316:  73%|███████▉   | 106/146 [01:04<00:21,  1.84it/s]Epoch: 6, train for the 107-th batch, train loss: 0.5145060420036316:  73%|████████   | 107/146 [01:04<00:18,  2.05it/s]Epoch: 11, train for the 140-th batch, train loss: 0.5120301842689514:  92%|█████████▏| 139/151 [00:46<00:02,  4.33it/s]Epoch: 11, train for the 140-th batch, train loss: 0.5120301842689514:  93%|█████████▎| 140/151 [00:46<00:02,  4.36it/s]Epoch: 3, train for the 37-th batch, train loss: 0.34717246890068054:   9%|█▏          | 36/383 [00:20<03:27,  1.67it/s]Epoch: 3, train for the 37-th batch, train loss: 0.34717246890068054:  10%|█▏          | 37/383 [00:20<03:27,  1.67it/s]Epoch: 7, train for the 86-th batch, train loss: 0.1869312971830368:  71%|█████████▎   | 85/119 [00:52<00:21,  1.56it/s]Epoch: 7, train for the 86-th batch, train loss: 0.1869312971830368:  72%|█████████▍   | 86/119 [00:52<00:24,  1.34it/s]Epoch: 11, train for the 141-th batch, train loss: 0.4996756315231323:  93%|█████████▎| 140/151 [00:47<00:02,  4.36it/s]Epoch: 11, train for the 141-th batch, train loss: 0.4996756315231323:  93%|█████████▎| 141/151 [00:47<00:02,  4.41it/s]Epoch: 4, train for the 135-th batch, train loss: 0.6470181345939636:  57%|██████▏    | 134/237 [01:15<00:54,  1.89it/s]Epoch: 4, train for the 135-th batch, train loss: 0.6470181345939636:  57%|██████▎    | 135/237 [01:15<00:55,  1.83it/s]Epoch: 6, train for the 108-th batch, train loss: 0.5558143258094788:  73%|████████   | 107/146 [01:04<00:18,  2.05it/s]Epoch: 6, train for the 108-th batch, train loss: 0.5558143258094788:  74%|████████▏  | 108/146 [01:04<00:17,  2.13it/s]Epoch: 11, train for the 142-th batch, train loss: 0.5885097980499268:  93%|█████████▎| 141/151 [00:47<00:02,  4.41it/s]Epoch: 11, train for the 142-th batch, train loss: 0.5885097980499268:  94%|█████████▍| 142/151 [00:47<00:02,  4.44it/s]Epoch: 3, train for the 38-th batch, train loss: 0.33138594031333923:  10%|█▏          | 37/383 [00:21<03:27,  1.67it/s]Epoch: 3, train for the 38-th batch, train loss: 0.33138594031333923:  10%|█▏          | 38/383 [00:21<03:26,  1.67it/s]Epoch: 11, train for the 143-th batch, train loss: 0.5041049718856812:  94%|█████████▍| 142/151 [00:47<00:02,  4.44it/s]Epoch: 11, train for the 143-th batch, train loss: 0.5041049718856812:  95%|█████████▍| 143/151 [00:47<00:01,  4.45it/s]Epoch: 7, train for the 87-th batch, train loss: 0.16880303621292114:  72%|████████▋   | 86/119 [00:53<00:24,  1.34it/s]Epoch: 7, train for the 87-th batch, train loss: 0.16880303621292114:  73%|████████▊   | 87/119 [00:53<00:22,  1.40it/s]Epoch: 11, train for the 144-th batch, train loss: 0.5452015399932861:  95%|█████████▍| 143/151 [00:47<00:01,  4.45it/s]Epoch: 11, train for the 144-th batch, train loss: 0.5452015399932861:  95%|█████████▌| 144/151 [00:47<00:01,  4.46it/s]Epoch: 4, train for the 136-th batch, train loss: 0.6282432079315186:  57%|██████▎    | 135/237 [01:16<00:55,  1.83it/s]Epoch: 4, train for the 136-th batch, train loss: 0.6282432079315186:  57%|██████▎    | 136/237 [01:16<00:56,  1.79it/s]Epoch: 6, train for the 109-th batch, train loss: 0.5424260497093201:  74%|████████▏  | 108/146 [01:05<00:17,  2.13it/s]Epoch: 6, train for the 109-th batch, train loss: 0.5424260497093201:  75%|████████▏  | 109/146 [01:05<00:19,  1.94it/s]Epoch: 11, train for the 145-th batch, train loss: 0.5054836273193359:  95%|█████████▌| 144/151 [00:48<00:01,  4.46it/s]Epoch: 11, train for the 145-th batch, train loss: 0.5054836273193359:  96%|█████████▌| 145/151 [00:48<00:01,  4.46it/s]Epoch: 3, train for the 39-th batch, train loss: 0.32666686177253723:  10%|█▏          | 38/383 [00:21<03:26,  1.67it/s]Epoch: 3, train for the 39-th batch, train loss: 0.32666686177253723:  10%|█▏          | 39/383 [00:21<03:22,  1.70it/s]Epoch: 11, train for the 146-th batch, train loss: 0.5284238457679749:  96%|█████████▌| 145/151 [00:48<00:01,  4.46it/s]Epoch: 11, train for the 146-th batch, train loss: 0.5284238457679749:  97%|█████████▋| 146/151 [00:48<00:01,  4.47it/s]Epoch: 7, train for the 88-th batch, train loss: 0.23355166614055634:  73%|████████▊   | 87/119 [00:54<00:22,  1.40it/s]Epoch: 7, train for the 88-th batch, train loss: 0.23355166614055634:  74%|████████▊   | 88/119 [00:54<00:21,  1.46it/s]Epoch: 4, train for the 137-th batch, train loss: 0.6721026301383972:  57%|██████▎    | 136/237 [01:16<00:56,  1.79it/s]Epoch: 4, train for the 137-th batch, train loss: 0.6721026301383972:  58%|██████▎    | 137/237 [01:16<00:56,  1.78it/s]Epoch: 11, train for the 147-th batch, train loss: 0.5457722544670105:  97%|█████████▋| 146/151 [00:48<00:01,  4.47it/s]Epoch: 11, train for the 147-th batch, train loss: 0.5457722544670105:  97%|█████████▋| 147/151 [00:48<00:00,  4.49it/s]Epoch: 6, train for the 110-th batch, train loss: 0.5221999287605286:  75%|████████▏  | 109/146 [01:05<00:19,  1.94it/s]Epoch: 6, train for the 110-th batch, train loss: 0.5221999287605286:  75%|████████▎  | 110/146 [01:05<00:19,  1.83it/s]Epoch: 3, train for the 40-th batch, train loss: 0.31131845712661743:  10%|█▏          | 39/383 [00:22<03:22,  1.70it/s]Epoch: 3, train for the 40-th batch, train loss: 0.31131845712661743:  10%|█▎          | 40/383 [00:22<03:21,  1.70it/s]Epoch: 11, train for the 148-th batch, train loss: 0.591504693031311:  97%|██████████▋| 147/151 [00:48<00:00,  4.49it/s]Epoch: 11, train for the 148-th batch, train loss: 0.591504693031311:  98%|██████████▊| 148/151 [00:48<00:00,  4.49it/s]Epoch: 11, train for the 149-th batch, train loss: 0.5141902565956116:  98%|█████████▊| 148/151 [00:48<00:00,  4.49it/s]Epoch: 11, train for the 149-th batch, train loss: 0.5141902565956116:  99%|█████████▊| 149/151 [00:48<00:00,  4.49it/s]Epoch: 7, train for the 89-th batch, train loss: 0.19491703808307648:  74%|████████▊   | 88/119 [00:54<00:21,  1.46it/s]Epoch: 7, train for the 89-th batch, train loss: 0.19491703808307648:  75%|████████▉   | 89/119 [00:54<00:19,  1.51it/s]Epoch: 4, train for the 138-th batch, train loss: 0.646494448184967:  58%|██████▉     | 137/237 [01:17<00:56,  1.78it/s]Epoch: 4, train for the 138-th batch, train loss: 0.646494448184967:  58%|██████▉     | 138/237 [01:17<00:56,  1.76it/s]Epoch: 6, train for the 111-th batch, train loss: 0.5102627873420715:  75%|████████▎  | 110/146 [01:06<00:19,  1.83it/s]Epoch: 6, train for the 111-th batch, train loss: 0.5102627873420715:  76%|████████▎  | 111/146 [01:06<00:19,  1.76it/s]Epoch: 11, train for the 150-th batch, train loss: 0.5131114721298218:  99%|█████████▊| 149/151 [00:49<00:00,  4.49it/s]Epoch: 11, train for the 150-th batch, train loss: 0.5131114721298218:  99%|█████████▉| 150/151 [00:49<00:00,  4.49it/s]Epoch: 3, train for the 41-th batch, train loss: 0.24889075756072998:  10%|█▎          | 40/383 [00:23<03:21,  1.70it/s]Epoch: 3, train for the 41-th batch, train loss: 0.24889075756072998:  11%|█▎          | 41/383 [00:23<03:20,  1.70it/s]Epoch: 11, train for the 151-th batch, train loss: 0.6137269139289856:  99%|█████████▉| 150/151 [00:49<00:00,  4.49it/s]Epoch: 11, train for the 151-th batch, train loss: 0.6137269139289856: 100%|██████████| 151/151 [00:49<00:00,  4.96it/s]Epoch: 11, train for the 151-th batch, train loss: 0.6137269139289856: 100%|██████████| 151/151 [00:49<00:00,  3.06it/s]
  0%|                                                                                            | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.49736902117729187:   0%|                           | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.49736902117729187:   2%|▍                  | 1/46 [00:00<00:04,  9.49it/s]evaluate for the 2-th batch, evaluate loss: 0.5035524368286133:   2%|▍                   | 1/46 [00:00<00:04,  9.49it/s]evaluate for the 2-th batch, evaluate loss: 0.5035524368286133:   4%|▊                   | 2/46 [00:00<00:04,  9.49it/s]Epoch: 4, train for the 139-th batch, train loss: 0.6520770192146301:  58%|██████▍    | 138/237 [01:17<00:56,  1.76it/s]Epoch: 4, train for the 139-th batch, train loss: 0.6520770192146301:  59%|██████▍    | 139/237 [01:17<00:56,  1.74it/s]Epoch: 7, train for the 90-th batch, train loss: 0.1793367713689804:  75%|█████████▋   | 89/119 [00:55<00:19,  1.51it/s]Epoch: 7, train for the 90-th batch, train loss: 0.1793367713689804:  76%|█████████▊   | 90/119 [00:55<00:18,  1.55it/s]evaluate for the 3-th batch, evaluate loss: 0.480953574180603:   4%|▉                    | 2/46 [00:00<00:04,  9.49it/s]evaluate for the 3-th batch, evaluate loss: 0.480953574180603:   7%|█▎                   | 3/46 [00:00<00:04,  9.52it/s]evaluate for the 4-th batch, evaluate loss: 0.5093647837638855:   7%|█▎                  | 3/46 [00:00<00:04,  9.52it/s]evaluate for the 4-th batch, evaluate loss: 0.5093647837638855:   9%|█▋                  | 4/46 [00:00<00:04,  9.56it/s]Epoch: 6, train for the 112-th batch, train loss: 0.5757657885551453:  76%|████████▎  | 111/146 [01:07<00:19,  1.76it/s]Epoch: 6, train for the 112-th batch, train loss: 0.5757657885551453:  77%|████████▍  | 112/146 [01:07<00:19,  1.73it/s]evaluate for the 5-th batch, evaluate loss: 0.4776284694671631:   9%|█▋                  | 4/46 [00:00<00:04,  9.56it/s]evaluate for the 5-th batch, evaluate loss: 0.4776284694671631:  11%|██▏                 | 5/46 [00:00<00:04,  9.61it/s]Epoch: 3, train for the 42-th batch, train loss: 0.2872161865234375:  11%|█▍           | 41/383 [00:23<03:20,  1.70it/s]Epoch: 3, train for the 42-th batch, train loss: 0.2872161865234375:  11%|█▍           | 42/383 [00:23<03:19,  1.71it/s]evaluate for the 6-th batch, evaluate loss: 0.5543252229690552:  11%|██▏                 | 5/46 [00:00<00:04,  9.61it/s]evaluate for the 6-th batch, evaluate loss: 0.5543252229690552:  13%|██▌                 | 6/46 [00:00<00:04,  9.60it/s]evaluate for the 7-th batch, evaluate loss: 0.4795542061328888:  13%|██▌                 | 6/46 [00:00<00:04,  9.60it/s]evaluate for the 7-th batch, evaluate loss: 0.4795542061328888:  15%|███                 | 7/46 [00:00<00:04,  9.63it/s]evaluate for the 8-th batch, evaluate loss: 0.5627749562263489:  15%|███                 | 7/46 [00:00<00:04,  9.63it/s]evaluate for the 8-th batch, evaluate loss: 0.5627749562263489:  17%|███▍                | 8/46 [00:00<00:03,  9.62it/s]Epoch: 4, train for the 140-th batch, train loss: 0.6074868440628052:  59%|██████▍    | 139/237 [01:18<00:56,  1.74it/s]Epoch: 4, train for the 140-th batch, train loss: 0.6074868440628052:  59%|██████▍    | 140/237 [01:18<00:55,  1.73it/s]Epoch: 7, train for the 91-th batch, train loss: 0.20682784914970398:  76%|█████████   | 90/119 [00:55<00:18,  1.55it/s]Epoch: 7, train for the 91-th batch, train loss: 0.20682784914970398:  76%|█████████▏  | 91/119 [00:55<00:17,  1.57it/s]evaluate for the 9-th batch, evaluate loss: 0.5255520939826965:  17%|███▍                | 8/46 [00:00<00:03,  9.62it/s]evaluate for the 9-th batch, evaluate loss: 0.5255520939826965:  20%|███▉                | 9/46 [00:00<00:03,  9.64it/s]evaluate for the 10-th batch, evaluate loss: 0.534035861492157:  20%|███▉                | 9/46 [00:01<00:03,  9.64it/s]evaluate for the 10-th batch, evaluate loss: 0.534035861492157:  22%|████▏              | 10/46 [00:01<00:03,  9.61it/s]Epoch: 6, train for the 113-th batch, train loss: 0.5113220810890198:  77%|████████▍  | 112/146 [01:07<00:19,  1.73it/s]Epoch: 6, train for the 113-th batch, train loss: 0.5113220810890198:  77%|████████▌  | 113/146 [01:07<00:19,  1.69it/s]evaluate for the 11-th batch, evaluate loss: 0.5240165591239929:  22%|███▉              | 10/46 [00:01<00:03,  9.61it/s]evaluate for the 11-th batch, evaluate loss: 0.5240165591239929:  24%|████▎             | 11/46 [00:01<00:03,  9.62it/s]Epoch: 3, train for the 43-th batch, train loss: 0.29732611775398254:  11%|█▎          | 42/383 [00:24<03:19,  1.71it/s]Epoch: 3, train for the 43-th batch, train loss: 0.29732611775398254:  11%|█▎          | 43/383 [00:24<03:19,  1.71it/s]evaluate for the 12-th batch, evaluate loss: 0.4747738242149353:  24%|████▎             | 11/46 [00:01<00:03,  9.62it/s]evaluate for the 12-th batch, evaluate loss: 0.4747738242149353:  26%|████▋             | 12/46 [00:01<00:03,  9.63it/s]evaluate for the 13-th batch, evaluate loss: 0.49597272276878357:  26%|████▍            | 12/46 [00:01<00:03,  9.63it/s]evaluate for the 13-th batch, evaluate loss: 0.49597272276878357:  28%|████▊            | 13/46 [00:01<00:03,  9.63it/s]Epoch: 4, train for the 141-th batch, train loss: 0.6486321091651917:  59%|██████▍    | 140/237 [01:18<00:55,  1.73it/s]Epoch: 4, train for the 141-th batch, train loss: 0.6486321091651917:  59%|██████▌    | 141/237 [01:18<00:55,  1.73it/s]evaluate for the 14-th batch, evaluate loss: 0.5913254022598267:  28%|█████             | 13/46 [00:01<00:03,  9.63it/s]evaluate for the 14-th batch, evaluate loss: 0.5913254022598267:  30%|█████▍            | 14/46 [00:01<00:03,  9.63it/s]Epoch: 7, train for the 92-th batch, train loss: 0.23180022835731506:  76%|█████████▏  | 91/119 [00:56<00:17,  1.57it/s]Epoch: 7, train for the 92-th batch, train loss: 0.23180022835731506:  77%|█████████▎  | 92/119 [00:56<00:17,  1.58it/s]evaluate for the 15-th batch, evaluate loss: 0.5458327531814575:  30%|█████▍            | 14/46 [00:01<00:03,  9.63it/s]evaluate for the 15-th batch, evaluate loss: 0.5458327531814575:  33%|█████▊            | 15/46 [00:01<00:03,  9.61it/s]Epoch: 3, train for the 44-th batch, train loss: 0.2943700850009918:  11%|█▍           | 43/383 [00:24<03:19,  1.71it/s]Epoch: 3, train for the 44-th batch, train loss: 0.2943700850009918:  11%|█▍           | 44/383 [00:24<03:07,  1.81it/s]evaluate for the 16-th batch, evaluate loss: 0.5729280114173889:  33%|█████▊            | 15/46 [00:01<00:03,  9.61it/s]evaluate for the 16-th batch, evaluate loss: 0.5729280114173889:  35%|██████▎           | 16/46 [00:01<00:03,  9.62it/s]Epoch: 6, train for the 114-th batch, train loss: 0.54298996925354:  77%|██████████   | 113/146 [01:08<00:19,  1.69it/s]Epoch: 6, train for the 114-th batch, train loss: 0.54298996925354:  78%|██████████▏  | 114/146 [01:08<00:19,  1.67it/s]evaluate for the 17-th batch, evaluate loss: 0.45362335443496704:  35%|█████▉           | 16/46 [00:01<00:03,  9.62it/s]evaluate for the 17-th batch, evaluate loss: 0.45362335443496704:  37%|██████▎          | 17/46 [00:01<00:03,  9.61it/s]evaluate for the 18-th batch, evaluate loss: 0.5004033446311951:  37%|██████▋           | 17/46 [00:01<00:03,  9.61it/s]evaluate for the 18-th batch, evaluate loss: 0.5004033446311951:  39%|███████           | 18/46 [00:01<00:02,  9.63it/s]evaluate for the 19-th batch, evaluate loss: 0.5238673090934753:  39%|███████           | 18/46 [00:01<00:02,  9.63it/s]evaluate for the 19-th batch, evaluate loss: 0.5238673090934753:  41%|███████▍          | 19/46 [00:01<00:02,  9.62it/s]evaluate for the 20-th batch, evaluate loss: 0.5402323603630066:  41%|███████▍          | 19/46 [00:02<00:02,  9.62it/s]evaluate for the 20-th batch, evaluate loss: 0.5402323603630066:  43%|███████▊          | 20/46 [00:02<00:02,  9.61it/s]Epoch: 7, train for the 93-th batch, train loss: 0.16748081147670746:  77%|█████████▎  | 92/119 [00:57<00:17,  1.58it/s]Epoch: 7, train for the 93-th batch, train loss: 0.16748081147670746:  78%|█████████▍  | 93/119 [00:57<00:16,  1.60it/s]Epoch: 3, train for the 45-th batch, train loss: 0.29378849267959595:  11%|█▍          | 44/383 [00:25<03:07,  1.81it/s]Epoch: 3, train for the 45-th batch, train loss: 0.29378849267959595:  12%|█▍          | 45/383 [00:25<03:04,  1.83it/s]Epoch: 4, train for the 142-th batch, train loss: 0.6206332445144653:  59%|██████▌    | 141/237 [01:19<00:55,  1.73it/s]Epoch: 4, train for the 142-th batch, train loss: 0.6206332445144653:  60%|██████▌    | 142/237 [01:19<00:58,  1.61it/s]evaluate for the 21-th batch, evaluate loss: 0.5262683033943176:  43%|███████▊          | 20/46 [00:02<00:02,  9.61it/s]evaluate for the 21-th batch, evaluate loss: 0.5262683033943176:  46%|████████▏         | 21/46 [00:02<00:02,  9.60it/s]evaluate for the 22-th batch, evaluate loss: 0.5243880748748779:  46%|████████▏         | 21/46 [00:02<00:02,  9.60it/s]evaluate for the 22-th batch, evaluate loss: 0.5243880748748779:  48%|████████▌         | 22/46 [00:02<00:02,  9.57it/s]Epoch: 6, train for the 115-th batch, train loss: 0.5258942246437073:  78%|████████▌  | 114/146 [01:08<00:19,  1.67it/s]Epoch: 6, train for the 115-th batch, train loss: 0.5258942246437073:  79%|████████▋  | 115/146 [01:08<00:18,  1.66it/s]evaluate for the 23-th batch, evaluate loss: 0.472663938999176:  48%|█████████          | 22/46 [00:02<00:02,  9.57it/s]evaluate for the 23-th batch, evaluate loss: 0.472663938999176:  50%|█████████▌         | 23/46 [00:02<00:02,  9.58it/s]evaluate for the 24-th batch, evaluate loss: 0.4830824136734009:  50%|█████████         | 23/46 [00:02<00:02,  9.58it/s]evaluate for the 24-th batch, evaluate loss: 0.4830824136734009:  52%|█████████▍        | 24/46 [00:02<00:02,  9.57it/s]evaluate for the 25-th batch, evaluate loss: 0.5379988551139832:  52%|█████████▍        | 24/46 [00:02<00:02,  9.57it/s]evaluate for the 25-th batch, evaluate loss: 0.5379988551139832:  54%|█████████▊        | 25/46 [00:02<00:02,  9.57it/s]evaluate for the 26-th batch, evaluate loss: 0.5583627223968506:  54%|█████████▊        | 25/46 [00:02<00:02,  9.57it/s]evaluate for the 26-th batch, evaluate loss: 0.5583627223968506:  57%|██████████▏       | 26/46 [00:02<00:02,  9.56it/s]Epoch: 7, train for the 94-th batch, train loss: 0.19006608426570892:  78%|█████████▍  | 93/119 [00:57<00:16,  1.60it/s]Epoch: 7, train for the 94-th batch, train loss: 0.19006608426570892:  79%|█████████▍  | 94/119 [00:57<00:15,  1.61it/s]Epoch: 3, train for the 46-th batch, train loss: 0.3648086190223694:  12%|█▌           | 45/383 [00:25<03:04,  1.83it/s]Epoch: 3, train for the 46-th batch, train loss: 0.3648086190223694:  12%|█▌           | 46/383 [00:25<03:14,  1.73it/s]Epoch: 4, train for the 143-th batch, train loss: 0.6228867173194885:  60%|██████▌    | 142/237 [01:20<00:58,  1.61it/s]Epoch: 4, train for the 143-th batch, train loss: 0.6228867173194885:  60%|██████▋    | 143/237 [01:20<00:58,  1.59it/s]evaluate for the 27-th batch, evaluate loss: 0.4953237771987915:  57%|██████████▏       | 26/46 [00:02<00:02,  9.56it/s]evaluate for the 27-th batch, evaluate loss: 0.4953237771987915:  59%|██████████▌       | 27/46 [00:02<00:01,  9.55it/s]Epoch: 6, train for the 116-th batch, train loss: 0.5346664190292358:  79%|████████▋  | 115/146 [01:09<00:18,  1.66it/s]Epoch: 6, train for the 116-th batch, train loss: 0.5346664190292358:  79%|████████▋  | 116/146 [01:09<00:18,  1.65it/s]evaluate for the 28-th batch, evaluate loss: 0.522432267665863:  59%|███████████▏       | 27/46 [00:02<00:01,  9.55it/s]evaluate for the 28-th batch, evaluate loss: 0.522432267665863:  61%|███████████▌       | 28/46 [00:02<00:01,  9.55it/s]evaluate for the 29-th batch, evaluate loss: 0.49162036180496216:  61%|██████████▎      | 28/46 [00:03<00:01,  9.55it/s]evaluate for the 29-th batch, evaluate loss: 0.49162036180496216:  63%|██████████▋      | 29/46 [00:03<00:01,  9.55it/s]evaluate for the 30-th batch, evaluate loss: 0.4944821894168854:  63%|███████████▎      | 29/46 [00:03<00:01,  9.55it/s]evaluate for the 30-th batch, evaluate loss: 0.4944821894168854:  65%|███████████▋      | 30/46 [00:03<00:01,  9.55it/s]evaluate for the 31-th batch, evaluate loss: 0.5169506669044495:  65%|███████████▋      | 30/46 [00:03<00:01,  9.55it/s]evaluate for the 31-th batch, evaluate loss: 0.5169506669044495:  67%|████████████▏     | 31/46 [00:03<00:01,  9.54it/s]evaluate for the 32-th batch, evaluate loss: 0.4792655408382416:  67%|████████████▏     | 31/46 [00:03<00:01,  9.54it/s]evaluate for the 32-th batch, evaluate loss: 0.4792655408382416:  70%|████████████▌     | 32/46 [00:03<00:01,  9.54it/s]Epoch: 7, train for the 95-th batch, train loss: 0.15778860449790955:  79%|█████████▍  | 94/119 [00:58<00:15,  1.61it/s]Epoch: 7, train for the 95-th batch, train loss: 0.15778860449790955:  80%|█████████▌  | 95/119 [00:58<00:14,  1.62it/s]evaluate for the 33-th batch, evaluate loss: 0.49826779961586:  70%|█████████████▉      | 32/46 [00:03<00:01,  9.54it/s]evaluate for the 33-th batch, evaluate loss: 0.49826779961586:  72%|██████████████▎     | 33/46 [00:03<00:01,  9.56it/s]Epoch: 3, train for the 47-th batch, train loss: 0.3141729533672333:  12%|█▌           | 46/383 [00:26<03:14,  1.73it/s]Epoch: 4, train for the 144-th batch, train loss: 0.6580538153648376:  60%|██████▋    | 143/237 [01:20<00:58,  1.59it/s]Epoch: 3, train for the 47-th batch, train loss: 0.3141729533672333:  12%|█▌           | 47/383 [00:26<03:21,  1.67it/s]Epoch: 4, train for the 144-th batch, train loss: 0.6580538153648376:  61%|██████▋    | 144/237 [01:20<00:59,  1.57it/s]Epoch: 6, train for the 117-th batch, train loss: 0.6039225459098816:  79%|████████▋  | 116/146 [01:10<00:18,  1.65it/s]Epoch: 6, train for the 117-th batch, train loss: 0.6039225459098816:  80%|████████▊  | 117/146 [01:10<00:17,  1.65it/s]evaluate for the 34-th batch, evaluate loss: 0.4837058186531067:  72%|████████████▉     | 33/46 [00:03<00:01,  9.56it/s]evaluate for the 34-th batch, evaluate loss: 0.4837058186531067:  74%|█████████████▎    | 34/46 [00:03<00:01,  9.55it/s]evaluate for the 35-th batch, evaluate loss: 0.4842822849750519:  74%|█████████████▎    | 34/46 [00:03<00:01,  9.55it/s]evaluate for the 35-th batch, evaluate loss: 0.4842822849750519:  76%|█████████████▋    | 35/46 [00:03<00:01,  9.54it/s]evaluate for the 36-th batch, evaluate loss: 0.46916723251342773:  76%|████████████▉    | 35/46 [00:03<00:01,  9.54it/s]evaluate for the 36-th batch, evaluate loss: 0.46916723251342773:  78%|█████████████▎   | 36/46 [00:03<00:01,  9.55it/s]evaluate for the 37-th batch, evaluate loss: 0.5049074292182922:  78%|██████████████    | 36/46 [00:03<00:01,  9.55it/s]evaluate for the 37-th batch, evaluate loss: 0.5049074292182922:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.56it/s]evaluate for the 38-th batch, evaluate loss: 0.5340776443481445:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.56it/s]evaluate for the 38-th batch, evaluate loss: 0.5340776443481445:  83%|██████████████▊   | 38/46 [00:03<00:00,  9.56it/s]Epoch: 7, train for the 96-th batch, train loss: 0.15744952857494354:  80%|█████████▌  | 95/119 [00:59<00:14,  1.62it/s]Epoch: 7, train for the 96-th batch, train loss: 0.15744952857494354:  81%|█████████▋  | 96/119 [00:59<00:14,  1.62it/s]evaluate for the 39-th batch, evaluate loss: 0.5360320210456848:  83%|██████████████▊   | 38/46 [00:04<00:00,  9.56it/s]evaluate for the 39-th batch, evaluate loss: 0.5360320210456848:  85%|███████████████▎  | 39/46 [00:04<00:00,  9.57it/s]Epoch: 3, train for the 48-th batch, train loss: 0.3147844970226288:  12%|█▌           | 47/383 [00:27<03:21,  1.67it/s]Epoch: 3, train for the 48-th batch, train loss: 0.3147844970226288:  13%|█▋           | 48/383 [00:27<03:25,  1.63it/s]Epoch: 4, train for the 145-th batch, train loss: 0.6606216430664062:  61%|██████▋    | 144/237 [01:21<00:59,  1.57it/s]Epoch: 4, train for the 145-th batch, train loss: 0.6606216430664062:  61%|██████▋    | 145/237 [01:21<00:58,  1.57it/s]Epoch: 6, train for the 118-th batch, train loss: 0.5017471313476562:  80%|████████▊  | 117/146 [01:10<00:17,  1.65it/s]Epoch: 6, train for the 118-th batch, train loss: 0.5017471313476562:  81%|████████▉  | 118/146 [01:10<00:17,  1.64it/s]evaluate for the 40-th batch, evaluate loss: 0.4677775502204895:  85%|███████████████▎  | 39/46 [00:04<00:00,  9.57it/s]evaluate for the 40-th batch, evaluate loss: 0.4677775502204895:  87%|███████████████▋  | 40/46 [00:04<00:00,  9.57it/s]evaluate for the 41-th batch, evaluate loss: 0.481138676404953:  87%|████████████████▌  | 40/46 [00:04<00:00,  9.57it/s]evaluate for the 41-th batch, evaluate loss: 0.481138676404953:  89%|████████████████▉  | 41/46 [00:04<00:00,  9.61it/s]evaluate for the 42-th batch, evaluate loss: 0.4713168740272522:  89%|████████████████  | 41/46 [00:04<00:00,  9.61it/s]evaluate for the 42-th batch, evaluate loss: 0.4713168740272522:  91%|████████████████▍ | 42/46 [00:04<00:00,  9.60it/s]evaluate for the 43-th batch, evaluate loss: 0.5342237949371338:  91%|████████████████▍ | 42/46 [00:04<00:00,  9.60it/s]evaluate for the 43-th batch, evaluate loss: 0.5342237949371338:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.58it/s]evaluate for the 44-th batch, evaluate loss: 0.5134689211845398:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.58it/s]evaluate for the 44-th batch, evaluate loss: 0.5134689211845398:  96%|█████████████████▏| 44/46 [00:04<00:00,  9.58it/s]Epoch: 7, train for the 97-th batch, train loss: 0.18002061545848846:  81%|█████████▋  | 96/119 [00:59<00:14,  1.62it/s]Epoch: 7, train for the 97-th batch, train loss: 0.18002061545848846:  82%|█████████▊  | 97/119 [00:59<00:13,  1.62it/s]evaluate for the 45-th batch, evaluate loss: 0.4920002520084381:  96%|█████████████████▏| 44/46 [00:04<00:00,  9.58it/s]evaluate for the 45-th batch, evaluate loss: 0.4920002520084381:  98%|█████████████████▌| 45/46 [00:04<00:00,  9.57it/s]Epoch: 3, train for the 49-th batch, train loss: 0.31204748153686523:  13%|█▌          | 48/383 [00:27<03:25,  1.63it/s]Epoch: 3, train for the 49-th batch, train loss: 0.31204748153686523:  13%|█▌          | 49/383 [00:27<03:25,  1.62it/s]Epoch: 4, train for the 146-th batch, train loss: 0.6251271367073059:  61%|██████▋    | 145/237 [01:22<00:58,  1.57it/s]Epoch: 4, train for the 146-th batch, train loss: 0.6251271367073059:  62%|██████▊    | 146/237 [01:22<00:58,  1.57it/s]Epoch: 6, train for the 119-th batch, train loss: 0.555902361869812:  81%|█████████▋  | 118/146 [01:11<00:17,  1.64it/s]Epoch: 6, train for the 119-th batch, train loss: 0.555902361869812:  82%|█████████▊  | 119/146 [01:11<00:16,  1.64it/s]evaluate for the 46-th batch, evaluate loss: 0.5095559358596802:  98%|█████████████████▌| 45/46 [00:04<00:00,  9.57it/s]evaluate for the 46-th batch, evaluate loss: 0.5095559358596802: 100%|██████████████████| 46/46 [00:04<00:00,  9.61it/s]
  0%|                                                                                            | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6423516869544983:   0%|                            | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6423516869544983:   4%|▊                   | 1/25 [00:00<00:02,  9.17it/s]evaluate for the 2-th batch, evaluate loss: 0.6519111394882202:   4%|▊                   | 1/25 [00:00<00:02,  9.17it/s]evaluate for the 2-th batch, evaluate loss: 0.6519111394882202:   8%|█▌                  | 2/25 [00:00<00:02,  9.16it/s]evaluate for the 3-th batch, evaluate loss: 0.689156711101532:   8%|█▋                   | 2/25 [00:00<00:02,  9.16it/s]evaluate for the 3-th batch, evaluate loss: 0.689156711101532:  12%|██▌                  | 3/25 [00:00<00:02,  9.13it/s]Epoch: 7, train for the 98-th batch, train loss: 0.1198171079158783:  82%|██████████▌  | 97/119 [01:00<00:13,  1.62it/s]Epoch: 7, train for the 98-th batch, train loss: 0.1198171079158783:  82%|██████████▋  | 98/119 [01:00<00:12,  1.62it/s]evaluate for the 4-th batch, evaluate loss: 0.6790621876716614:  12%|██▍                 | 3/25 [00:00<00:02,  9.13it/s]evaluate for the 4-th batch, evaluate loss: 0.6790621876716614:  16%|███▏                | 4/25 [00:00<00:02,  9.13it/s]evaluate for the 5-th batch, evaluate loss: 0.6728254556655884:  16%|███▏                | 4/25 [00:00<00:02,  9.13it/s]evaluate for the 5-th batch, evaluate loss: 0.6728254556655884:  20%|████                | 5/25 [00:00<00:02,  9.13it/s]Epoch: 6, train for the 120-th batch, train loss: 0.5200473666191101:  82%|████████▉  | 119/146 [01:12<00:16,  1.64it/s]Epoch: 6, train for the 120-th batch, train loss: 0.5200473666191101:  82%|█████████  | 120/146 [01:12<00:15,  1.63it/s]Epoch: 3, train for the 50-th batch, train loss: 0.30736520886421204:  13%|█▌          | 49/383 [00:28<03:25,  1.62it/s]Epoch: 3, train for the 50-th batch, train loss: 0.30736520886421204:  13%|█▌          | 50/383 [00:28<03:28,  1.60it/s]Epoch: 4, train for the 147-th batch, train loss: 0.660935640335083:  62%|███████▍    | 146/237 [01:22<00:58,  1.57it/s]Epoch: 4, train for the 147-th batch, train loss: 0.660935640335083:  62%|███████▍    | 147/237 [01:22<00:57,  1.57it/s]evaluate for the 6-th batch, evaluate loss: 0.7214105129241943:  20%|████                | 5/25 [00:00<00:02,  9.13it/s]evaluate for the 6-th batch, evaluate loss: 0.7214105129241943:  24%|████▊               | 6/25 [00:00<00:02,  9.14it/s]evaluate for the 7-th batch, evaluate loss: 0.7423109412193298:  24%|████▊               | 6/25 [00:00<00:02,  9.14it/s]evaluate for the 7-th batch, evaluate loss: 0.7423109412193298:  28%|█████▌              | 7/25 [00:00<00:01,  9.14it/s]evaluate for the 8-th batch, evaluate loss: 0.7228460311889648:  28%|█████▌              | 7/25 [00:00<00:01,  9.14it/s]evaluate for the 8-th batch, evaluate loss: 0.7228460311889648:  32%|██████▍             | 8/25 [00:00<00:01,  9.13it/s]evaluate for the 9-th batch, evaluate loss: 0.7081214785575867:  32%|██████▍             | 8/25 [00:00<00:01,  9.13it/s]evaluate for the 9-th batch, evaluate loss: 0.7081214785575867:  36%|███████▏            | 9/25 [00:00<00:01,  9.11it/s]Epoch: 7, train for the 99-th batch, train loss: 0.19633716344833374:  82%|█████████▉  | 98/119 [01:00<00:12,  1.62it/s]Epoch: 7, train for the 99-th batch, train loss: 0.19633716344833374:  83%|█████████▉  | 99/119 [01:00<00:12,  1.63it/s]evaluate for the 10-th batch, evaluate loss: 0.7413464188575745:  36%|██████▊            | 9/25 [00:01<00:01,  9.11it/s]evaluate for the 10-th batch, evaluate loss: 0.7413464188575745:  40%|███████▏          | 10/25 [00:01<00:01,  9.13it/s]Epoch: 6, train for the 121-th batch, train loss: 0.5328800678253174:  82%|█████████  | 120/146 [01:12<00:15,  1.63it/s]Epoch: 6, train for the 121-th batch, train loss: 0.5328800678253174:  83%|█████████  | 121/146 [01:12<00:15,  1.63it/s]evaluate for the 11-th batch, evaluate loss: 0.7325417399406433:  40%|███████▏          | 10/25 [00:01<00:01,  9.13it/s]evaluate for the 11-th batch, evaluate loss: 0.7325417399406433:  44%|███████▉          | 11/25 [00:01<00:01,  9.13it/s]Epoch: 3, train for the 51-th batch, train loss: 0.31790831685066223:  13%|█▌          | 50/383 [00:29<03:28,  1.60it/s]Epoch: 3, train for the 51-th batch, train loss: 0.31790831685066223:  13%|█▌          | 51/383 [00:29<03:28,  1.59it/s]Epoch: 4, train for the 148-th batch, train loss: 0.6171549558639526:  62%|██████▊    | 147/237 [01:23<00:57,  1.57it/s]Epoch: 4, train for the 148-th batch, train loss: 0.6171549558639526:  62%|██████▊    | 148/237 [01:23<00:56,  1.57it/s]evaluate for the 12-th batch, evaluate loss: 0.7010607123374939:  44%|███████▉          | 11/25 [00:01<00:01,  9.13it/s]evaluate for the 12-th batch, evaluate loss: 0.7010607123374939:  48%|████████▋         | 12/25 [00:01<00:01,  9.13it/s]evaluate for the 13-th batch, evaluate loss: 0.6647277474403381:  48%|████████▋         | 12/25 [00:01<00:01,  9.13it/s]evaluate for the 13-th batch, evaluate loss: 0.6647277474403381:  52%|█████████▎        | 13/25 [00:01<00:01,  9.12it/s]evaluate for the 14-th batch, evaluate loss: 0.7554283142089844:  52%|█████████▎        | 13/25 [00:01<00:01,  9.12it/s]evaluate for the 14-th batch, evaluate loss: 0.7554283142089844:  56%|██████████        | 14/25 [00:01<00:01,  9.12it/s]evaluate for the 15-th batch, evaluate loss: 0.7297415137290955:  56%|██████████        | 14/25 [00:01<00:01,  9.12it/s]evaluate for the 15-th batch, evaluate loss: 0.7297415137290955:  60%|██████████▊       | 15/25 [00:01<00:01,  9.12it/s]Epoch: 7, train for the 100-th batch, train loss: 0.18082545697689056:  83%|█████████▏ | 99/119 [01:01<00:12,  1.63it/s]Epoch: 7, train for the 100-th batch, train loss: 0.18082545697689056:  84%|████████▍ | 100/119 [01:01<00:11,  1.62it/s]evaluate for the 16-th batch, evaluate loss: 0.6651051044464111:  60%|██████████▊       | 15/25 [00:01<00:01,  9.12it/s]evaluate for the 16-th batch, evaluate loss: 0.6651051044464111:  64%|███████████▌      | 16/25 [00:01<00:00,  9.15it/s]Epoch: 6, train for the 122-th batch, train loss: 0.5321702361106873:  83%|█████████  | 121/146 [01:13<00:15,  1.63it/s]Epoch: 6, train for the 122-th batch, train loss: 0.5321702361106873:  84%|█████████▏ | 122/146 [01:13<00:14,  1.63it/s]Epoch: 3, train for the 52-th batch, train loss: 0.3077269494533539:  13%|█▋           | 51/383 [00:29<03:28,  1.59it/s]Epoch: 3, train for the 52-th batch, train loss: 0.3077269494533539:  14%|█▊           | 52/383 [00:29<03:29,  1.58it/s]evaluate for the 17-th batch, evaluate loss: 0.6616911292076111:  64%|███████████▌      | 16/25 [00:01<00:00,  9.15it/s]evaluate for the 17-th batch, evaluate loss: 0.6616911292076111:  68%|████████████▏     | 17/25 [00:01<00:00,  9.17it/s]Epoch: 4, train for the 149-th batch, train loss: 0.6131569743156433:  62%|██████▊    | 148/237 [01:24<00:56,  1.57it/s]Epoch: 4, train for the 149-th batch, train loss: 0.6131569743156433:  63%|██████▉    | 149/237 [01:24<00:56,  1.57it/s]evaluate for the 18-th batch, evaluate loss: 0.6286778450012207:  68%|████████████▏     | 17/25 [00:01<00:00,  9.17it/s]evaluate for the 18-th batch, evaluate loss: 0.6286778450012207:  72%|████████████▉     | 18/25 [00:01<00:00,  9.15it/s]evaluate for the 19-th batch, evaluate loss: 0.5963990092277527:  72%|████████████▉     | 18/25 [00:02<00:00,  9.15it/s]evaluate for the 19-th batch, evaluate loss: 0.5963990092277527:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.15it/s]evaluate for the 20-th batch, evaluate loss: 0.6592637300491333:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.15it/s]evaluate for the 20-th batch, evaluate loss: 0.6592637300491333:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.14it/s]Epoch: 7, train for the 101-th batch, train loss: 0.14553691446781158:  84%|████████▍ | 100/119 [01:02<00:11,  1.62it/s]Epoch: 7, train for the 101-th batch, train loss: 0.14553691446781158:  85%|████████▍ | 101/119 [01:02<00:11,  1.63it/s]evaluate for the 21-th batch, evaluate loss: 0.7319871187210083:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.14it/s]evaluate for the 21-th batch, evaluate loss: 0.7319871187210083:  84%|███████████████   | 21/25 [00:02<00:00,  9.14it/s]evaluate for the 22-th batch, evaluate loss: 0.6061971783638:  84%|█████████████████▋   | 21/25 [00:02<00:00,  9.14it/s]evaluate for the 22-th batch, evaluate loss: 0.6061971783638:  88%|██████████████████▍  | 22/25 [00:02<00:00,  9.12it/s]Epoch: 6, train for the 123-th batch, train loss: 0.5408933162689209:  84%|█████████▏ | 122/146 [01:13<00:14,  1.63it/s]Epoch: 6, train for the 123-th batch, train loss: 0.5408933162689209:  84%|█████████▎ | 123/146 [01:13<00:14,  1.63it/s]Epoch: 3, train for the 53-th batch, train loss: 0.3633989691734314:  14%|█▊           | 52/383 [00:30<03:29,  1.58it/s]Epoch: 4, train for the 150-th batch, train loss: 0.6300563812255859:  63%|██████▉    | 149/237 [01:24<00:56,  1.57it/s]Epoch: 3, train for the 53-th batch, train loss: 0.3633989691734314:  14%|█▊           | 53/383 [00:30<03:30,  1.57it/s]Epoch: 4, train for the 150-th batch, train loss: 0.6300563812255859:  63%|██████▉    | 150/237 [01:24<00:55,  1.56it/s]evaluate for the 23-th batch, evaluate loss: 0.6669091582298279:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.12it/s]evaluate for the 23-th batch, evaluate loss: 0.6669091582298279:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.14it/s]evaluate for the 24-th batch, evaluate loss: 0.6583102345466614:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.14it/s]evaluate for the 24-th batch, evaluate loss: 0.6583102345466614:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.12it/s]evaluate for the 25-th batch, evaluate loss: 0.7100942730903625:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.12it/s]evaluate for the 25-th batch, evaluate loss: 0.7100942730903625: 100%|██████████████████| 25/25 [00:02<00:00,  9.20it/s]
INFO:root:Epoch: 11, learning rate: 0.0001, train loss: 0.5628
INFO:root:train average_precision, 0.8161
INFO:root:train roc_auc, 0.7802
INFO:root:validate loss: 0.5094
INFO:root:validate average_precision, 0.8425
INFO:root:validate roc_auc, 0.8033
INFO:root:new node validate loss: 0.6856
INFO:root:new node validate first_1_average_precision, 0.5921
INFO:root:new node validate first_1_roc_auc, 0.5412
INFO:root:new node validate first_3_average_precision, 0.6764
INFO:root:new node validate first_3_roc_auc, 0.6370
INFO:root:new node validate first_10_average_precision, 0.7456
INFO:root:new node validate first_10_roc_auc, 0.7104
INFO:root:new node validate average_precision, 0.7082
INFO:root:new node validate roc_auc, 0.6561
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]Epoch: 7, train for the 102-th batch, train loss: 0.18856491148471832:  85%|████████▍ | 101/119 [01:02<00:11,  1.63it/s]Epoch: 7, train for the 102-th batch, train loss: 0.18856491148471832:  86%|████████▌ | 102/119 [01:02<00:10,  1.63it/s]Epoch: 12, train for the 1-th batch, train loss: 1.2837826013565063:   0%|                      | 0/151 [00:00<?, ?it/s]Epoch: 12, train for the 1-th batch, train loss: 1.2837826013565063:   1%|              | 1/151 [00:00<00:25,  5.88it/s]Epoch: 6, train for the 124-th batch, train loss: 0.5737929940223694:  84%|█████████▎ | 123/146 [01:14<00:14,  1.63it/s]Epoch: 6, train for the 124-th batch, train loss: 0.5737929940223694:  85%|█████████▎ | 124/146 [01:14<00:13,  1.63it/s]Epoch: 12, train for the 2-th batch, train loss: 1.2767335176467896:   1%|              | 1/151 [00:00<00:25,  5.88it/s]Epoch: 12, train for the 2-th batch, train loss: 1.2767335176467896:   1%|▏             | 2/151 [00:00<00:26,  5.72it/s]Epoch: 3, train for the 54-th batch, train loss: 0.30708155035972595:  14%|█▋          | 53/383 [00:31<03:30,  1.57it/s]Epoch: 3, train for the 54-th batch, train loss: 0.30708155035972595:  14%|█▋          | 54/383 [00:31<03:31,  1.56it/s]Epoch: 4, train for the 151-th batch, train loss: 0.6032653450965881:  63%|██████▉    | 150/237 [01:25<00:55,  1.56it/s]Epoch: 4, train for the 151-th batch, train loss: 0.6032653450965881:  64%|███████    | 151/237 [01:25<00:55,  1.55it/s]Epoch: 12, train for the 3-th batch, train loss: 0.6570873856544495:   1%|▏             | 2/151 [00:00<00:26,  5.72it/s]Epoch: 12, train for the 3-th batch, train loss: 0.6570873856544495:   2%|▎             | 3/151 [00:00<00:25,  5.89it/s]Epoch: 12, train for the 4-th batch, train loss: 0.42230135202407837:   2%|▎            | 3/151 [00:00<00:25,  5.89it/s]Epoch: 12, train for the 4-th batch, train loss: 0.42230135202407837:   3%|▎            | 4/151 [00:00<00:24,  5.93it/s]Epoch: 7, train for the 103-th batch, train loss: 0.17637504637241364:  86%|████████▌ | 102/119 [01:03<00:10,  1.63it/s]Epoch: 7, train for the 103-th batch, train loss: 0.17637504637241364:  87%|████████▋ | 103/119 [01:03<00:09,  1.63it/s]Epoch: 12, train for the 5-th batch, train loss: 0.6309295892715454:   3%|▎             | 4/151 [00:00<00:24,  5.93it/s]Epoch: 12, train for the 5-th batch, train loss: 0.6309295892715454:   3%|▍             | 5/151 [00:00<00:25,  5.67it/s]Epoch: 6, train for the 125-th batch, train loss: 0.514360249042511:  85%|██████████▏ | 124/146 [01:15<00:13,  1.63it/s]Epoch: 6, train for the 125-th batch, train loss: 0.514360249042511:  86%|██████████▎ | 125/146 [01:15<00:12,  1.63it/s]Epoch: 12, train for the 6-th batch, train loss: 0.6698466539382935:   3%|▍             | 5/151 [00:01<00:25,  5.67it/s]Epoch: 12, train for the 6-th batch, train loss: 0.6698466539382935:   4%|▌             | 6/151 [00:01<00:26,  5.50it/s]Epoch: 3, train for the 55-th batch, train loss: 0.359027236700058:  14%|█▉            | 54/383 [00:31<03:31,  1.56it/s]Epoch: 4, train for the 152-th batch, train loss: 0.6262446641921997:  64%|███████    | 151/237 [01:26<00:55,  1.55it/s]Epoch: 3, train for the 55-th batch, train loss: 0.359027236700058:  14%|██            | 55/383 [00:31<03:31,  1.55it/s]Epoch: 4, train for the 152-th batch, train loss: 0.6262446641921997:  64%|███████    | 152/237 [01:26<00:55,  1.55it/s]Epoch: 12, train for the 7-th batch, train loss: 0.4617127776145935:   4%|▌             | 6/151 [00:01<00:26,  5.50it/s]Epoch: 12, train for the 7-th batch, train loss: 0.4617127776145935:   5%|▋             | 7/151 [00:01<00:26,  5.40it/s]Epoch: 7, train for the 104-th batch, train loss: 0.22511450946331024:  87%|████████▋ | 103/119 [01:03<00:09,  1.63it/s]Epoch: 7, train for the 104-th batch, train loss: 0.22511450946331024:  87%|████████▋ | 104/119 [01:03<00:09,  1.63it/s]Epoch: 12, train for the 8-th batch, train loss: 0.6803736686706543:   5%|▋             | 7/151 [00:01<00:26,  5.40it/s]Epoch: 12, train for the 8-th batch, train loss: 0.6803736686706543:   5%|▋             | 8/151 [00:01<00:26,  5.36it/s]Epoch: 6, train for the 126-th batch, train loss: 0.549606442451477:  86%|██████████▎ | 125/146 [01:15<00:12,  1.63it/s]Epoch: 6, train for the 126-th batch, train loss: 0.549606442451477:  86%|██████████▎ | 126/146 [01:15<00:12,  1.63it/s]Epoch: 12, train for the 9-th batch, train loss: 0.6533116102218628:   5%|▋             | 8/151 [00:01<00:26,  5.36it/s]Epoch: 12, train for the 9-th batch, train loss: 0.6533116102218628:   6%|▊             | 9/151 [00:01<00:27,  5.21it/s]Epoch: 3, train for the 56-th batch, train loss: 0.29613253474235535:  14%|█▋          | 55/383 [00:32<03:31,  1.55it/s]Epoch: 4, train for the 153-th batch, train loss: 0.6653417944908142:  64%|███████    | 152/237 [01:26<00:55,  1.55it/s]Epoch: 3, train for the 56-th batch, train loss: 0.29613253474235535:  15%|█▊          | 56/383 [00:32<03:31,  1.54it/s]Epoch: 4, train for the 153-th batch, train loss: 0.6653417944908142:  65%|███████    | 153/237 [01:26<00:54,  1.54it/s]Epoch: 12, train for the 10-th batch, train loss: 0.5030106902122498:   6%|▊            | 9/151 [00:01<00:27,  5.21it/s]Epoch: 12, train for the 10-th batch, train loss: 0.5030106902122498:   7%|▊           | 10/151 [00:01<00:27,  5.12it/s]Epoch: 7, train for the 105-th batch, train loss: 0.154788076877594:  87%|██████████▍ | 104/119 [01:04<00:09,  1.63it/s]Epoch: 7, train for the 105-th batch, train loss: 0.154788076877594:  88%|██████████▌ | 105/119 [01:04<00:08,  1.63it/s]Epoch: 12, train for the 11-th batch, train loss: 0.6427160501480103:   7%|▊           | 10/151 [00:02<00:27,  5.12it/s]Epoch: 12, train for the 11-th batch, train loss: 0.6427160501480103:   7%|▊           | 11/151 [00:02<00:27,  5.03it/s]Epoch: 6, train for the 127-th batch, train loss: 0.5463595390319824:  86%|█████████▍ | 126/146 [01:16<00:12,  1.63it/s]Epoch: 6, train for the 127-th batch, train loss: 0.5463595390319824:  87%|█████████▌ | 127/146 [01:16<00:11,  1.63it/s]Epoch: 12, train for the 12-th batch, train loss: 0.6173796057701111:   7%|▊           | 11/151 [00:02<00:27,  5.03it/s]Epoch: 12, train for the 12-th batch, train loss: 0.6173796057701111:   8%|▉           | 12/151 [00:02<00:27,  5.00it/s]Epoch: 3, train for the 57-th batch, train loss: 0.3054324984550476:  15%|█▉           | 56/383 [00:33<03:31,  1.54it/s]Epoch: 3, train for the 57-th batch, train loss: 0.3054324984550476:  15%|█▉           | 57/383 [00:33<03:28,  1.57it/s]Epoch: 4, train for the 154-th batch, train loss: 0.6232551336288452:  65%|███████    | 153/237 [01:27<00:54,  1.54it/s]Epoch: 4, train for the 154-th batch, train loss: 0.6232551336288452:  65%|███████▏   | 154/237 [01:27<00:53,  1.55it/s]Epoch: 12, train for the 13-th batch, train loss: 0.6443184018135071:   8%|▉           | 12/151 [00:02<00:27,  5.00it/s]Epoch: 12, train for the 13-th batch, train loss: 0.6443184018135071:   9%|█           | 13/151 [00:02<00:27,  4.94it/s]Epoch: 7, train for the 106-th batch, train loss: 0.14761890470981598:  88%|████████▊ | 105/119 [01:05<00:08,  1.63it/s]Epoch: 7, train for the 106-th batch, train loss: 0.14761890470981598:  89%|████████▉ | 106/119 [01:05<00:07,  1.63it/s]Epoch: 12, train for the 14-th batch, train loss: 0.5691601037979126:   9%|█           | 13/151 [00:02<00:27,  4.94it/s]Epoch: 12, train for the 14-th batch, train loss: 0.5691601037979126:   9%|█           | 14/151 [00:02<00:27,  4.92it/s]Epoch: 6, train for the 128-th batch, train loss: 0.5352473855018616:  87%|█████████▌ | 127/146 [01:16<00:11,  1.63it/s]Epoch: 6, train for the 128-th batch, train loss: 0.5352473855018616:  88%|█████████▋ | 128/146 [01:16<00:11,  1.63it/s]Epoch: 12, train for the 15-th batch, train loss: 0.5842798352241516:   9%|█           | 14/151 [00:02<00:27,  4.92it/s]Epoch: 12, train for the 15-th batch, train loss: 0.5842798352241516:  10%|█▏          | 15/151 [00:02<00:28,  4.84it/s]Epoch: 3, train for the 58-th batch, train loss: 0.3849090039730072:  15%|█▉           | 57/383 [00:33<03:28,  1.57it/s]Epoch: 3, train for the 58-th batch, train loss: 0.3849090039730072:  15%|█▉           | 58/383 [00:33<03:23,  1.59it/s]Epoch: 4, train for the 155-th batch, train loss: 0.6274588704109192:  65%|███████▏   | 154/237 [01:27<00:53,  1.55it/s]Epoch: 4, train for the 155-th batch, train loss: 0.6274588704109192:  65%|███████▏   | 155/237 [01:27<00:52,  1.57it/s]Epoch: 12, train for the 16-th batch, train loss: 0.6544609069824219:  10%|█▏          | 15/151 [00:03<00:28,  4.84it/s]Epoch: 12, train for the 16-th batch, train loss: 0.6544609069824219:  11%|█▎          | 16/151 [00:03<00:28,  4.80it/s]Epoch: 7, train for the 107-th batch, train loss: 0.21324566006660461:  89%|████████▉ | 106/119 [01:05<00:07,  1.63it/s]Epoch: 7, train for the 107-th batch, train loss: 0.21324566006660461:  90%|████████▉ | 107/119 [01:05<00:07,  1.63it/s]Epoch: 12, train for the 17-th batch, train loss: 0.5314936637878418:  11%|█▎          | 16/151 [00:03<00:28,  4.80it/s]Epoch: 12, train for the 17-th batch, train loss: 0.5314936637878418:  11%|█▎          | 17/151 [00:03<00:27,  4.86it/s]Epoch: 6, train for the 129-th batch, train loss: 0.5735658407211304:  88%|█████████▋ | 128/146 [01:17<00:11,  1.63it/s]Epoch: 6, train for the 129-th batch, train loss: 0.5735658407211304:  88%|█████████▋ | 129/146 [01:17<00:10,  1.63it/s]Epoch: 12, train for the 18-th batch, train loss: 0.6419795751571655:  11%|█▎          | 17/151 [00:03<00:27,  4.86it/s]Epoch: 12, train for the 18-th batch, train loss: 0.6419795751571655:  12%|█▍          | 18/151 [00:03<00:27,  4.84it/s]Epoch: 3, train for the 59-th batch, train loss: 0.31459513306617737:  15%|█▊          | 58/383 [00:34<03:23,  1.59it/s]Epoch: 3, train for the 59-th batch, train loss: 0.31459513306617737:  15%|█▊          | 59/383 [00:34<03:24,  1.59it/s]Epoch: 4, train for the 156-th batch, train loss: 0.6361413598060608:  65%|███████▏   | 155/237 [01:28<00:52,  1.57it/s]Epoch: 4, train for the 156-th batch, train loss: 0.6361413598060608:  66%|███████▏   | 156/237 [01:28<00:50,  1.60it/s]Epoch: 12, train for the 19-th batch, train loss: 0.5397942066192627:  12%|█▍          | 18/151 [00:03<00:27,  4.84it/s]Epoch: 12, train for the 19-th batch, train loss: 0.5397942066192627:  13%|█▌          | 19/151 [00:03<00:27,  4.85it/s]Epoch: 7, train for the 108-th batch, train loss: 0.16446852684020996:  90%|████████▉ | 107/119 [01:06<00:07,  1.63it/s]Epoch: 7, train for the 108-th batch, train loss: 0.16446852684020996:  91%|█████████ | 108/119 [01:06<00:06,  1.63it/s]Epoch: 12, train for the 20-th batch, train loss: 0.447506308555603:  13%|█▋           | 19/151 [00:03<00:27,  4.85it/s]Epoch: 12, train for the 20-th batch, train loss: 0.447506308555603:  13%|█▋           | 20/151 [00:03<00:27,  4.82it/s]Epoch: 6, train for the 130-th batch, train loss: 0.5141310691833496:  88%|█████████▋ | 129/146 [01:18<00:10,  1.63it/s]Epoch: 6, train for the 130-th batch, train loss: 0.5141310691833496:  89%|█████████▊ | 130/146 [01:18<00:09,  1.63it/s]Epoch: 12, train for the 21-th batch, train loss: 0.594300389289856:  13%|█▋           | 20/151 [00:04<00:27,  4.82it/s]Epoch: 12, train for the 21-th batch, train loss: 0.594300389289856:  14%|█▊           | 21/151 [00:04<00:27,  4.76it/s]Epoch: 3, train for the 60-th batch, train loss: 0.31219810247421265:  15%|█▊          | 59/383 [00:34<03:24,  1.59it/s]Epoch: 3, train for the 60-th batch, train loss: 0.31219810247421265:  16%|█▉          | 60/383 [00:34<03:23,  1.59it/s]Epoch: 4, train for the 157-th batch, train loss: 0.6521641612052917:  66%|███████▏   | 156/237 [01:29<00:50,  1.60it/s]Epoch: 4, train for the 157-th batch, train loss: 0.6521641612052917:  66%|███████▎   | 157/237 [01:29<00:50,  1.59it/s]Epoch: 12, train for the 22-th batch, train loss: 0.5080097317695618:  14%|█▋          | 21/151 [00:04<00:27,  4.76it/s]Epoch: 12, train for the 22-th batch, train loss: 0.5080097317695618:  15%|█▋          | 22/151 [00:04<00:27,  4.75it/s]Epoch: 7, train for the 109-th batch, train loss: 0.2060818076133728:  91%|█████████▉ | 108/119 [01:07<00:06,  1.63it/s]Epoch: 7, train for the 109-th batch, train loss: 0.2060818076133728:  92%|██████████ | 109/119 [01:07<00:06,  1.63it/s]Epoch: 12, train for the 23-th batch, train loss: 0.4429936110973358:  15%|█▋          | 22/151 [00:04<00:27,  4.75it/s]Epoch: 12, train for the 23-th batch, train loss: 0.4429936110973358:  15%|█▊          | 23/151 [00:04<00:26,  4.75it/s]Epoch: 6, train for the 131-th batch, train loss: 0.5222667455673218:  89%|█████████▊ | 130/146 [01:18<00:09,  1.63it/s]Epoch: 6, train for the 131-th batch, train loss: 0.5222667455673218:  90%|█████████▊ | 131/146 [01:18<00:09,  1.63it/s]Epoch: 12, train for the 24-th batch, train loss: 0.48222190141677856:  15%|█▋         | 23/151 [00:04<00:26,  4.75it/s]Epoch: 12, train for the 24-th batch, train loss: 0.48222190141677856:  16%|█▋         | 24/151 [00:04<00:26,  4.73it/s]Epoch: 4, train for the 158-th batch, train loss: 0.6031187772750854:  66%|███████▎   | 157/237 [01:29<00:50,  1.59it/s]Epoch: 4, train for the 158-th batch, train loss: 0.6031187772750854:  67%|███████▎   | 158/237 [01:29<00:49,  1.59it/s]Epoch: 3, train for the 61-th batch, train loss: 0.3365887999534607:  16%|██           | 60/383 [00:35<03:23,  1.59it/s]Epoch: 3, train for the 61-th batch, train loss: 0.3365887999534607:  16%|██           | 61/383 [00:35<03:24,  1.58it/s]Epoch: 12, train for the 25-th batch, train loss: 0.5184651613235474:  16%|█▉          | 24/151 [00:04<00:26,  4.73it/s]Epoch: 12, train for the 25-th batch, train loss: 0.5184651613235474:  17%|█▉          | 25/151 [00:04<00:26,  4.73it/s]Epoch: 7, train for the 110-th batch, train loss: 0.16994675993919373:  92%|█████████▏| 109/119 [01:07<00:06,  1.63it/s]Epoch: 7, train for the 110-th batch, train loss: 0.16994675993919373:  92%|█████████▏| 110/119 [01:07<00:05,  1.63it/s]Epoch: 12, train for the 26-th batch, train loss: 0.49621710181236267:  17%|█▊         | 25/151 [00:05<00:26,  4.73it/s]Epoch: 12, train for the 26-th batch, train loss: 0.49621710181236267:  17%|█▉         | 26/151 [00:05<00:26,  4.70it/s]Epoch: 6, train for the 132-th batch, train loss: 0.5394428372383118:  90%|█████████▊ | 131/146 [01:19<00:09,  1.63it/s]Epoch: 6, train for the 132-th batch, train loss: 0.5394428372383118:  90%|█████████▉ | 132/146 [01:19<00:08,  1.63it/s]Epoch: 12, train for the 27-th batch, train loss: 0.701901376247406:  17%|██▏          | 26/151 [00:05<00:26,  4.70it/s]Epoch: 12, train for the 27-th batch, train loss: 0.701901376247406:  18%|██▎          | 27/151 [00:05<00:26,  4.65it/s]Epoch: 4, train for the 159-th batch, train loss: 0.6161442995071411:  67%|███████▎   | 158/237 [01:30<00:49,  1.59it/s]Epoch: 4, train for the 159-th batch, train loss: 0.6161442995071411:  67%|███████▍   | 159/237 [01:30<00:49,  1.58it/s]Epoch: 3, train for the 62-th batch, train loss: 0.36183199286460876:  16%|█▉          | 61/383 [00:36<03:24,  1.58it/s]Epoch: 3, train for the 62-th batch, train loss: 0.36183199286460876:  16%|█▉          | 62/383 [00:36<03:23,  1.58it/s]Epoch: 12, train for the 28-th batch, train loss: 0.5227864980697632:  18%|██▏         | 27/151 [00:05<00:26,  4.65it/s]Epoch: 12, train for the 28-th batch, train loss: 0.5227864980697632:  19%|██▏         | 28/151 [00:05<00:26,  4.66it/s]Epoch: 7, train for the 111-th batch, train loss: 0.23057754337787628:  92%|█████████▏| 110/119 [01:08<00:05,  1.63it/s]Epoch: 7, train for the 111-th batch, train loss: 0.23057754337787628:  93%|█████████▎| 111/119 [01:08<00:04,  1.63it/s]Epoch: 6, train for the 133-th batch, train loss: 0.5307307839393616:  90%|█████████▉ | 132/146 [01:19<00:08,  1.63it/s]Epoch: 6, train for the 133-th batch, train loss: 0.5307307839393616:  91%|██████████ | 133/146 [01:19<00:07,  1.63it/s]Epoch: 12, train for the 29-th batch, train loss: 0.6735095977783203:  19%|██▏         | 28/151 [00:05<00:26,  4.66it/s]Epoch: 12, train for the 29-th batch, train loss: 0.6735095977783203:  19%|██▎         | 29/151 [00:05<00:26,  4.60it/s]Epoch: 12, train for the 30-th batch, train loss: 0.6275743842124939:  19%|██▎         | 29/151 [00:06<00:26,  4.60it/s]Epoch: 12, train for the 30-th batch, train loss: 0.6275743842124939:  20%|██▍         | 30/151 [00:06<00:26,  4.59it/s]Epoch: 4, train for the 160-th batch, train loss: 0.6362563967704773:  67%|███████▍   | 159/237 [01:31<00:49,  1.58it/s]Epoch: 4, train for the 160-th batch, train loss: 0.6362563967704773:  68%|███████▍   | 160/237 [01:31<00:48,  1.57it/s]Epoch: 3, train for the 63-th batch, train loss: 0.34203317761421204:  16%|█▉          | 62/383 [00:36<03:23,  1.58it/s]Epoch: 3, train for the 63-th batch, train loss: 0.34203317761421204:  16%|█▉          | 63/383 [00:36<03:23,  1.57it/s]Epoch: 12, train for the 31-th batch, train loss: 0.6095311045646667:  20%|██▍         | 30/151 [00:06<00:26,  4.59it/s]Epoch: 12, train for the 31-th batch, train loss: 0.6095311045646667:  21%|██▍         | 31/151 [00:06<00:26,  4.60it/s]Epoch: 7, train for the 112-th batch, train loss: 0.11883723735809326:  93%|█████████▎| 111/119 [01:08<00:04,  1.63it/s]Epoch: 7, train for the 112-th batch, train loss: 0.11883723735809326:  94%|█████████▍| 112/119 [01:08<00:04,  1.63it/s]Epoch: 6, train for the 134-th batch, train loss: 0.503921627998352:  91%|██████████▉ | 133/146 [01:20<00:07,  1.63it/s]Epoch: 6, train for the 134-th batch, train loss: 0.503921627998352:  92%|███████████ | 134/146 [01:20<00:07,  1.63it/s]Epoch: 12, train for the 32-th batch, train loss: 0.5477127432823181:  21%|██▍         | 31/151 [00:06<00:26,  4.60it/s]Epoch: 12, train for the 32-th batch, train loss: 0.5477127432823181:  21%|██▌         | 32/151 [00:06<00:25,  4.60it/s]Epoch: 12, train for the 33-th batch, train loss: 0.6641567945480347:  21%|██▌         | 32/151 [00:06<00:25,  4.60it/s]Epoch: 12, train for the 33-th batch, train loss: 0.6641567945480347:  22%|██▌         | 33/151 [00:06<00:25,  4.58it/s]Epoch: 4, train for the 161-th batch, train loss: 0.653908371925354:  68%|████████    | 160/237 [01:31<00:48,  1.57it/s]Epoch: 4, train for the 161-th batch, train loss: 0.653908371925354:  68%|████████▏   | 161/237 [01:31<00:48,  1.57it/s]Epoch: 3, train for the 64-th batch, train loss: 0.3299404978752136:  16%|██▏          | 63/383 [00:37<03:23,  1.57it/s]Epoch: 3, train for the 64-th batch, train loss: 0.3299404978752136:  17%|██▏          | 64/383 [00:37<03:23,  1.57it/s]Epoch: 7, train for the 113-th batch, train loss: 0.16069616377353668:  94%|█████████▍| 112/119 [01:09<00:04,  1.63it/s]Epoch: 7, train for the 113-th batch, train loss: 0.16069616377353668:  95%|█████████▍| 113/119 [01:09<00:03,  1.63it/s]Epoch: 12, train for the 34-th batch, train loss: 0.6733149290084839:  22%|██▌         | 33/151 [00:06<00:25,  4.58it/s]Epoch: 12, train for the 34-th batch, train loss: 0.6733149290084839:  23%|██▋         | 34/151 [00:06<00:25,  4.55it/s]Epoch: 6, train for the 135-th batch, train loss: 0.5229413509368896:  92%|██████████ | 134/146 [01:21<00:07,  1.63it/s]Epoch: 6, train for the 135-th batch, train loss: 0.5229413509368896:  92%|██████████▏| 135/146 [01:21<00:06,  1.63it/s]Epoch: 12, train for the 35-th batch, train loss: 0.65291827917099:  23%|███▏          | 34/151 [00:07<00:25,  4.55it/s]Epoch: 12, train for the 35-th batch, train loss: 0.65291827917099:  23%|███▏          | 35/151 [00:07<00:25,  4.53it/s]Epoch: 12, train for the 36-th batch, train loss: 0.6411947011947632:  23%|██▊         | 35/151 [00:07<00:25,  4.53it/s]Epoch: 12, train for the 36-th batch, train loss: 0.6411947011947632:  24%|██▊         | 36/151 [00:07<00:25,  4.55it/s]Epoch: 4, train for the 162-th batch, train loss: 0.6469694375991821:  68%|███████▍   | 161/237 [01:32<00:48,  1.57it/s]Epoch: 4, train for the 162-th batch, train loss: 0.6469694375991821:  68%|███████▌   | 162/237 [01:32<00:47,  1.57it/s]Epoch: 3, train for the 65-th batch, train loss: 0.3379021883010864:  17%|██▏          | 64/383 [00:38<03:23,  1.57it/s]Epoch: 3, train for the 65-th batch, train loss: 0.3379021883010864:  17%|██▏          | 65/383 [00:38<03:23,  1.56it/s]Epoch: 7, train for the 114-th batch, train loss: 0.18289592862129211:  95%|█████████▍| 113/119 [01:10<00:03,  1.63it/s]Epoch: 7, train for the 114-th batch, train loss: 0.18289592862129211:  96%|█████████▌| 114/119 [01:10<00:03,  1.63it/s]Epoch: 12, train for the 37-th batch, train loss: 0.6470174193382263:  24%|██▊         | 36/151 [00:07<00:25,  4.55it/s]Epoch: 12, train for the 37-th batch, train loss: 0.6470174193382263:  25%|██▉         | 37/151 [00:07<00:25,  4.53it/s]Epoch: 6, train for the 136-th batch, train loss: 0.5236232876777649:  92%|██████████▏| 135/146 [01:21<00:06,  1.63it/s]Epoch: 6, train for the 136-th batch, train loss: 0.5236232876777649:  93%|██████████▏| 136/146 [01:21<00:06,  1.63it/s]Epoch: 12, train for the 38-th batch, train loss: 0.6552510857582092:  25%|██▉         | 37/151 [00:07<00:25,  4.53it/s]Epoch: 12, train for the 38-th batch, train loss: 0.6552510857582092:  25%|███         | 38/151 [00:07<00:24,  4.53it/s]Epoch: 4, train for the 163-th batch, train loss: 0.6461716890335083:  68%|███████▌   | 162/237 [01:33<00:47,  1.57it/s]Epoch: 4, train for the 163-th batch, train loss: 0.6461716890335083:  69%|███████▌   | 163/237 [01:33<00:47,  1.56it/s]Epoch: 3, train for the 66-th batch, train loss: 0.3346617519855499:  17%|██▏          | 65/383 [00:38<03:23,  1.56it/s]Epoch: 3, train for the 66-th batch, train loss: 0.3346617519855499:  17%|██▏          | 66/383 [00:38<03:22,  1.56it/s]Epoch: 7, train for the 115-th batch, train loss: 0.16287605464458466:  96%|█████████▌| 114/119 [01:10<00:03,  1.63it/s]Epoch: 7, train for the 115-th batch, train loss: 0.16287605464458466:  97%|█████████▋| 115/119 [01:10<00:02,  1.63it/s]Epoch: 12, train for the 39-th batch, train loss: 0.6316736936569214:  25%|███         | 38/151 [00:08<00:24,  4.53it/s]Epoch: 12, train for the 39-th batch, train loss: 0.6316736936569214:  26%|███         | 39/151 [00:08<00:28,  3.93it/s]Epoch: 6, train for the 137-th batch, train loss: 0.5466108322143555:  93%|██████████▏| 136/146 [01:22<00:06,  1.63it/s]Epoch: 6, train for the 137-th batch, train loss: 0.5466108322143555:  94%|██████████▎| 137/146 [01:22<00:05,  1.63it/s]Epoch: 12, train for the 40-th batch, train loss: 0.5912890434265137:  26%|███         | 39/151 [00:08<00:28,  3.93it/s]Epoch: 12, train for the 40-th batch, train loss: 0.5912890434265137:  26%|███▏        | 40/151 [00:08<00:27,  4.09it/s]Epoch: 12, train for the 41-th batch, train loss: 0.5668993592262268:  26%|███▏        | 40/151 [00:08<00:27,  4.09it/s]Epoch: 12, train for the 41-th batch, train loss: 0.5668993592262268:  27%|███▎        | 41/151 [00:08<00:25,  4.24it/s]Epoch: 3, train for the 67-th batch, train loss: 0.3885784447193146:  17%|██▏          | 66/383 [00:39<03:22,  1.56it/s]Epoch: 3, train for the 67-th batch, train loss: 0.3885784447193146:  17%|██▎          | 67/383 [00:39<03:20,  1.57it/s]Epoch: 4, train for the 164-th batch, train loss: 0.6381940245628357:  69%|███████▌   | 163/237 [01:33<00:47,  1.56it/s]Epoch: 4, train for the 164-th batch, train loss: 0.6381940245628357:  69%|███████▌   | 164/237 [01:33<00:46,  1.56it/s]Epoch: 7, train for the 116-th batch, train loss: 0.1184089332818985:  97%|██████████▋| 115/119 [01:11<00:02,  1.63it/s]Epoch: 7, train for the 116-th batch, train loss: 0.1184089332818985:  97%|██████████▋| 116/119 [01:11<00:01,  1.63it/s]Epoch: 12, train for the 42-th batch, train loss: 0.5998045206069946:  27%|███▎        | 41/151 [00:08<00:25,  4.24it/s]Epoch: 12, train for the 42-th batch, train loss: 0.5998045206069946:  28%|███▎        | 42/151 [00:08<00:25,  4.30it/s]Epoch: 6, train for the 138-th batch, train loss: 0.5388496518135071:  94%|██████████▎| 137/146 [01:23<00:05,  1.63it/s]Epoch: 6, train for the 138-th batch, train loss: 0.5388496518135071:  95%|██████████▍| 138/146 [01:23<00:04,  1.63it/s]Epoch: 12, train for the 43-th batch, train loss: 0.6147047877311707:  28%|███▎        | 42/151 [00:09<00:25,  4.30it/s]Epoch: 12, train for the 43-th batch, train loss: 0.6147047877311707:  28%|███▍        | 43/151 [00:09<00:24,  4.35it/s]Epoch: 12, train for the 44-th batch, train loss: 0.5574847459793091:  28%|███▍        | 43/151 [00:09<00:24,  4.35it/s]Epoch: 12, train for the 44-th batch, train loss: 0.5574847459793091:  29%|███▍        | 44/151 [00:09<00:24,  4.40it/s]Epoch: 3, train for the 68-th batch, train loss: 0.3401261866092682:  17%|██▎          | 67/383 [00:40<03:20,  1.57it/s]Epoch: 3, train for the 68-th batch, train loss: 0.3401261866092682:  18%|██▎          | 68/383 [00:40<03:21,  1.56it/s]Epoch: 4, train for the 165-th batch, train loss: 0.633507251739502:  69%|████████▎   | 164/237 [01:34<00:46,  1.56it/s]Epoch: 4, train for the 165-th batch, train loss: 0.633507251739502:  70%|████████▎   | 165/237 [01:34<00:45,  1.57it/s]Epoch: 7, train for the 117-th batch, train loss: 0.18057021498680115:  97%|█████████▋| 116/119 [01:11<00:01,  1.63it/s]Epoch: 7, train for the 117-th batch, train loss: 0.18057021498680115:  98%|█████████▊| 117/119 [01:11<00:01,  1.63it/s]Epoch: 12, train for the 45-th batch, train loss: 0.5498703718185425:  29%|███▍        | 44/151 [00:09<00:24,  4.40it/s]Epoch: 12, train for the 45-th batch, train loss: 0.5498703718185425:  30%|███▌        | 45/151 [00:09<00:23,  4.44it/s]Epoch: 6, train for the 139-th batch, train loss: 0.5755846500396729:  95%|██████████▍| 138/146 [01:23<00:04,  1.63it/s]Epoch: 6, train for the 139-th batch, train loss: 0.5755846500396729:  95%|██████████▍| 139/146 [01:23<00:04,  1.63it/s]Epoch: 12, train for the 46-th batch, train loss: 0.5549173355102539:  30%|███▌        | 45/151 [00:09<00:23,  4.44it/s]Epoch: 12, train for the 46-th batch, train loss: 0.5549173355102539:  30%|███▋        | 46/151 [00:09<00:23,  4.50it/s]Epoch: 12, train for the 47-th batch, train loss: 0.6359541416168213:  30%|███▋        | 46/151 [00:09<00:23,  4.50it/s]Epoch: 12, train for the 47-th batch, train loss: 0.6359541416168213:  31%|███▋        | 47/151 [00:09<00:23,  4.49it/s]Epoch: 7, train for the 118-th batch, train loss: 0.12704820930957794:  98%|█████████▊| 117/119 [01:12<00:01,  1.63it/s]Epoch: 7, train for the 118-th batch, train loss: 0.12704820930957794:  99%|█████████▉| 118/119 [01:12<00:00,  1.63it/s]Epoch: 3, train for the 69-th batch, train loss: 0.30741095542907715:  18%|██▏         | 68/383 [00:40<03:21,  1.56it/s]Epoch: 4, train for the 166-th batch, train loss: 0.6186199188232422:  70%|███████▋   | 165/237 [01:34<00:45,  1.57it/s]Epoch: 3, train for the 69-th batch, train loss: 0.30741095542907715:  18%|██▏         | 69/383 [00:40<03:21,  1.56it/s]Epoch: 4, train for the 166-th batch, train loss: 0.6186199188232422:  70%|███████▋   | 166/237 [01:34<00:45,  1.56it/s]Epoch: 6, train for the 140-th batch, train loss: 0.5172192454338074:  95%|██████████▍| 139/146 [01:24<00:04,  1.63it/s]Epoch: 6, train for the 140-th batch, train loss: 0.5172192454338074:  96%|██████████▌| 140/146 [01:24<00:03,  1.68it/s]Epoch: 12, train for the 48-th batch, train loss: 0.5708319544792175:  31%|███▋        | 47/151 [00:10<00:23,  4.49it/s]Epoch: 12, train for the 48-th batch, train loss: 0.5708319544792175:  32%|███▊        | 48/151 [00:10<00:22,  4.50it/s]Epoch: 7, train for the 119-th batch, train loss: 0.1982017308473587:  99%|██████████▉| 118/119 [01:12<00:00,  1.63it/s]Epoch: 7, train for the 119-th batch, train loss: 0.1982017308473587: 100%|███████████| 119/119 [01:12<00:00,  1.89it/s]Epoch: 7, train for the 119-th batch, train loss: 0.1982017308473587: 100%|███████████| 119/119 [01:12<00:00,  1.63it/s]
  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 12, train for the 49-th batch, train loss: 0.5950508117675781:  32%|███▊        | 48/151 [00:10<00:22,  4.50it/s]Epoch: 12, train for the 49-th batch, train loss: 0.5950508117675781:  32%|███▉        | 49/151 [00:10<00:22,  4.51it/s]Epoch: 12, train for the 50-th batch, train loss: 0.47506093978881836:  32%|███▌       | 49/151 [00:10<00:22,  4.51it/s]Epoch: 12, train for the 50-th batch, train loss: 0.47506093978881836:  33%|███▋       | 50/151 [00:10<00:22,  4.55it/s]evaluate for the 1-th batch, evaluate loss: 0.14225907623767853:   0%|                           | 0/40 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.14225907623767853:   2%|▍                  | 1/40 [00:00<00:11,  3.40it/s]Epoch: 3, train for the 70-th batch, train loss: 0.39075881242752075:  18%|██▏         | 69/383 [00:41<03:21,  1.56it/s]Epoch: 4, train for the 167-th batch, train loss: 0.6057139039039612:  70%|███████▋   | 166/237 [01:35<00:45,  1.56it/s]Epoch: 3, train for the 70-th batch, train loss: 0.39075881242752075:  18%|██▏         | 70/383 [00:41<03:22,  1.55it/s]Epoch: 4, train for the 167-th batch, train loss: 0.6057139039039612:  70%|███████▊   | 167/237 [01:35<00:45,  1.55it/s]Epoch: 6, train for the 141-th batch, train loss: 0.5400056838989258:  96%|██████████▌| 140/146 [01:24<00:03,  1.68it/s]Epoch: 6, train for the 141-th batch, train loss: 0.5400056838989258:  97%|██████████▌| 141/146 [01:24<00:02,  1.67it/s]Epoch: 12, train for the 51-th batch, train loss: 0.6173931956291199:  33%|███▉        | 50/151 [00:10<00:22,  4.55it/s]Epoch: 12, train for the 51-th batch, train loss: 0.6173931956291199:  34%|████        | 51/151 [00:10<00:22,  4.53it/s]evaluate for the 2-th batch, evaluate loss: 0.17334730923175812:   2%|▍                  | 1/40 [00:00<00:11,  3.40it/s]evaluate for the 2-th batch, evaluate loss: 0.17334730923175812:   5%|▉                  | 2/40 [00:00<00:10,  3.68it/s]Epoch: 12, train for the 52-th batch, train loss: 0.6129646897315979:  34%|████        | 51/151 [00:11<00:22,  4.53it/s]Epoch: 12, train for the 52-th batch, train loss: 0.6129646897315979:  34%|████▏       | 52/151 [00:11<00:21,  4.52it/s]evaluate for the 3-th batch, evaluate loss: 0.21226021647453308:   5%|▉                  | 2/40 [00:00<00:10,  3.68it/s]evaluate for the 3-th batch, evaluate loss: 0.21226021647453308:   8%|█▍                 | 3/40 [00:00<00:10,  3.60it/s]Epoch: 12, train for the 53-th batch, train loss: 0.47123652696609497:  34%|███▊       | 52/151 [00:11<00:21,  4.52it/s]Epoch: 12, train for the 53-th batch, train loss: 0.47123652696609497:  35%|███▊       | 53/151 [00:11<00:21,  4.58it/s]Epoch: 6, train for the 142-th batch, train loss: 0.4873565435409546:  97%|██████████▌| 141/146 [01:25<00:02,  1.67it/s]Epoch: 6, train for the 142-th batch, train loss: 0.4873565435409546:  97%|██████████▋| 142/146 [01:25<00:02,  1.66it/s]Epoch: 3, train for the 71-th batch, train loss: 0.3357193171977997:  18%|██▍          | 70/383 [00:41<03:22,  1.55it/s]Epoch: 4, train for the 168-th batch, train loss: 0.6445234417915344:  70%|███████▊   | 167/237 [01:36<00:45,  1.55it/s]Epoch: 3, train for the 71-th batch, train loss: 0.3357193171977997:  19%|██▍          | 71/383 [00:41<03:22,  1.54it/s]Epoch: 4, train for the 168-th batch, train loss: 0.6445234417915344:  71%|███████▊   | 168/237 [01:36<00:44,  1.54it/s]evaluate for the 4-th batch, evaluate loss: 0.13423292338848114:   8%|█▍                 | 3/40 [00:01<00:10,  3.60it/s]evaluate for the 4-th batch, evaluate loss: 0.13423292338848114:  10%|█▉                 | 4/40 [00:01<00:10,  3.53it/s]Epoch: 12, train for the 54-th batch, train loss: 0.5672693252563477:  35%|████▏       | 53/151 [00:11<00:21,  4.58it/s]Epoch: 12, train for the 54-th batch, train loss: 0.5672693252563477:  36%|████▎       | 54/151 [00:11<00:21,  4.57it/s]Epoch: 12, train for the 55-th batch, train loss: 0.5362874269485474:  36%|████▎       | 54/151 [00:11<00:21,  4.57it/s]Epoch: 12, train for the 55-th batch, train loss: 0.5362874269485474:  36%|████▎       | 55/151 [00:11<00:21,  4.56it/s]evaluate for the 5-th batch, evaluate loss: 0.17394280433654785:  10%|█▉                 | 4/40 [00:01<00:10,  3.53it/s]evaluate for the 5-th batch, evaluate loss: 0.17394280433654785:  12%|██▍                | 5/40 [00:01<00:09,  3.57it/s]Epoch: 6, train for the 143-th batch, train loss: 0.5224896669387817:  97%|██████████▋| 142/146 [01:26<00:02,  1.66it/s]Epoch: 6, train for the 143-th batch, train loss: 0.5224896669387817:  98%|██████████▊| 143/146 [01:26<00:01,  1.66it/s]Epoch: 12, train for the 56-th batch, train loss: 0.4742848873138428:  36%|████▎       | 55/151 [00:11<00:21,  4.56it/s]Epoch: 12, train for the 56-th batch, train loss: 0.4742848873138428:  37%|████▍       | 56/151 [00:11<00:20,  4.55it/s]Epoch: 3, train for the 72-th batch, train loss: 0.30315926671028137:  19%|██▏         | 71/383 [00:42<03:22,  1.54it/s]Epoch: 3, train for the 72-th batch, train loss: 0.30315926671028137:  19%|██▎         | 72/383 [00:42<03:21,  1.54it/s]Epoch: 4, train for the 169-th batch, train loss: 0.6570366621017456:  71%|███████▊   | 168/237 [01:36<00:44,  1.54it/s]Epoch: 4, train for the 169-th batch, train loss: 0.6570366621017456:  71%|███████▊   | 169/237 [01:36<00:44,  1.54it/s]evaluate for the 6-th batch, evaluate loss: 0.17351695895195007:  12%|██▍                | 5/40 [00:01<00:09,  3.57it/s]evaluate for the 6-th batch, evaluate loss: 0.17351695895195007:  15%|██▊                | 6/40 [00:01<00:09,  3.48it/s]Epoch: 12, train for the 57-th batch, train loss: 0.5250429511070251:  37%|████▍       | 56/151 [00:12<00:20,  4.55it/s]Epoch: 12, train for the 57-th batch, train loss: 0.5250429511070251:  38%|████▌       | 57/151 [00:12<00:20,  4.54it/s]evaluate for the 7-th batch, evaluate loss: 0.11097653955221176:  15%|██▊                | 6/40 [00:01<00:09,  3.48it/s]evaluate for the 7-th batch, evaluate loss: 0.11097653955221176:  18%|███▎               | 7/40 [00:01<00:09,  3.66it/s]Epoch: 12, train for the 58-th batch, train loss: 0.4843735098838806:  38%|████▌       | 57/151 [00:12<00:20,  4.54it/s]Epoch: 12, train for the 58-th batch, train loss: 0.4843735098838806:  38%|████▌       | 58/151 [00:12<00:20,  4.55it/s]Epoch: 6, train for the 144-th batch, train loss: 0.5153768658638:  98%|█████████████▋| 143/146 [01:26<00:01,  1.66it/s]Epoch: 6, train for the 144-th batch, train loss: 0.5153768658638:  99%|█████████████▊| 144/146 [01:26<00:01,  1.66it/s]evaluate for the 8-th batch, evaluate loss: 0.1320718675851822:  18%|███▌                | 7/40 [00:02<00:09,  3.66it/s]evaluate for the 8-th batch, evaluate loss: 0.1320718675851822:  20%|████                | 8/40 [00:02<00:09,  3.53it/s]Epoch: 12, train for the 59-th batch, train loss: 0.5242816805839539:  38%|████▌       | 58/151 [00:12<00:20,  4.55it/s]Epoch: 12, train for the 59-th batch, train loss: 0.5242816805839539:  39%|████▋       | 59/151 [00:12<00:20,  4.54it/s]Epoch: 3, train for the 73-th batch, train loss: 0.3291473984718323:  19%|██▍          | 72/383 [00:43<03:21,  1.54it/s]Epoch: 4, train for the 170-th batch, train loss: 0.6509184241294861:  71%|███████▊   | 169/237 [01:37<00:44,  1.54it/s]Epoch: 3, train for the 73-th batch, train loss: 0.3291473984718323:  19%|██▍          | 73/383 [00:43<03:21,  1.54it/s]Epoch: 4, train for the 170-th batch, train loss: 0.6509184241294861:  72%|███████▉   | 170/237 [01:37<00:43,  1.54it/s]Epoch: 12, train for the 60-th batch, train loss: 0.5126442909240723:  39%|████▋       | 59/151 [00:12<00:20,  4.54it/s]Epoch: 12, train for the 60-th batch, train loss: 0.5126442909240723:  40%|████▊       | 60/151 [00:12<00:20,  4.52it/s]evaluate for the 9-th batch, evaluate loss: 0.16803108155727386:  20%|███▊               | 8/40 [00:02<00:09,  3.53it/s]evaluate for the 9-th batch, evaluate loss: 0.16803108155727386:  22%|████▎              | 9/40 [00:02<00:08,  3.71it/s]Epoch: 12, train for the 61-th batch, train loss: 0.45413297414779663:  40%|████▎      | 60/151 [00:13<00:20,  4.52it/s]Epoch: 12, train for the 61-th batch, train loss: 0.45413297414779663:  40%|████▍      | 61/151 [00:13<00:19,  4.54it/s]Epoch: 6, train for the 145-th batch, train loss: 0.5010827779769897:  99%|██████████▊| 144/146 [01:27<00:01,  1.66it/s]Epoch: 6, train for the 145-th batch, train loss: 0.5010827779769897:  99%|██████████▉| 145/146 [01:27<00:00,  1.65it/s]evaluate for the 10-th batch, evaluate loss: 0.19550712406635284:  22%|████              | 9/40 [00:02<00:08,  3.71it/s]evaluate for the 10-th batch, evaluate loss: 0.19550712406635284:  25%|████▎            | 10/40 [00:02<00:08,  3.51it/s]Epoch: 12, train for the 62-th batch, train loss: 0.6048712730407715:  40%|████▊       | 61/151 [00:13<00:19,  4.54it/s]Epoch: 12, train for the 62-th batch, train loss: 0.6048712730407715:  41%|████▉       | 62/151 [00:13<00:19,  4.51it/s]Epoch: 4, train for the 171-th batch, train loss: 0.6376237869262695:  72%|███████▉   | 170/237 [01:38<00:43,  1.54it/s]Epoch: 3, train for the 74-th batch, train loss: 0.24973048269748688:  19%|██▎         | 73/383 [00:43<03:21,  1.54it/s]Epoch: 4, train for the 171-th batch, train loss: 0.6376237869262695:  72%|███████▉   | 171/237 [01:38<00:42,  1.54it/s]Epoch: 3, train for the 74-th batch, train loss: 0.24973048269748688:  19%|██▎         | 74/383 [00:43<03:21,  1.54it/s]evaluate for the 11-th batch, evaluate loss: 0.170878067612648:  25%|████▊              | 10/40 [00:03<00:08,  3.51it/s]evaluate for the 11-th batch, evaluate loss: 0.170878067612648:  28%|█████▏             | 11/40 [00:03<00:07,  3.70it/s]Epoch: 12, train for the 63-th batch, train loss: 0.4632897973060608:  41%|████▉       | 62/151 [00:13<00:19,  4.51it/s]Epoch: 12, train for the 63-th batch, train loss: 0.4632897973060608:  42%|█████       | 63/151 [00:13<00:19,  4.53it/s]Epoch: 6, train for the 146-th batch, train loss: 0.48658984899520874:  99%|█████████▉| 145/146 [01:27<00:00,  1.65it/s]Epoch: 6, train for the 146-th batch, train loss: 0.48658984899520874: 100%|██████████| 146/146 [01:27<00:00,  1.78it/s]Epoch: 6, train for the 146-th batch, train loss: 0.48658984899520874: 100%|██████████| 146/146 [01:27<00:00,  1.66it/s]
  0%|                                                                                            | 0/38 [00:00<?, ?it/s]evaluate for the 12-th batch, evaluate loss: 0.1469171941280365:  28%|████▉             | 11/40 [00:03<00:07,  3.70it/s]evaluate for the 12-th batch, evaluate loss: 0.1469171941280365:  30%|█████▍            | 12/40 [00:03<00:07,  3.61it/s]Epoch: 12, train for the 64-th batch, train loss: 0.5288271307945251:  42%|█████       | 63/151 [00:13<00:19,  4.53it/s]Epoch: 12, train for the 64-th batch, train loss: 0.5288271307945251:  42%|█████       | 64/151 [00:13<00:19,  4.51it/s]evaluate for the 1-th batch, evaluate loss: 0.47951796650886536:   0%|                           | 0/38 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.47951796650886536:   3%|▌                  | 1/38 [00:00<00:10,  3.61it/s]Epoch: 3, train for the 75-th batch, train loss: 0.304140567779541:  19%|██▋           | 74/383 [00:44<03:21,  1.54it/s]Epoch: 3, train for the 75-th batch, train loss: 0.304140567779541:  20%|██▋           | 75/383 [00:44<03:18,  1.55it/s]Epoch: 12, train for the 65-th batch, train loss: 0.4952165186405182:  42%|█████       | 64/151 [00:13<00:19,  4.51it/s]Epoch: 12, train for the 65-th batch, train loss: 0.4952165186405182:  43%|█████▏      | 65/151 [00:13<00:19,  4.52it/s]Epoch: 4, train for the 172-th batch, train loss: 0.6173407435417175:  72%|███████▉   | 171/237 [01:38<00:42,  1.54it/s]Epoch: 4, train for the 172-th batch, train loss: 0.6173407435417175:  73%|███████▉   | 172/237 [01:38<00:42,  1.54it/s]evaluate for the 13-th batch, evaluate loss: 0.12399766594171524:  30%|█████            | 12/40 [00:03<00:07,  3.61it/s]evaluate for the 13-th batch, evaluate loss: 0.12399766594171524:  32%|█████▌           | 13/40 [00:03<00:07,  3.61it/s]evaluate for the 2-th batch, evaluate loss: 0.49120405316352844:   3%|▌                  | 1/38 [00:00<00:10,  3.61it/s]evaluate for the 2-th batch, evaluate loss: 0.49120405316352844:   5%|█                  | 2/38 [00:00<00:09,  3.63it/s]Epoch: 12, train for the 66-th batch, train loss: 0.550767719745636:  43%|█████▌       | 65/151 [00:14<00:19,  4.52it/s]Epoch: 12, train for the 66-th batch, train loss: 0.550767719745636:  44%|█████▋       | 66/151 [00:14<00:18,  4.49it/s]evaluate for the 14-th batch, evaluate loss: 0.1643509864807129:  32%|█████▊            | 13/40 [00:03<00:07,  3.61it/s]evaluate for the 14-th batch, evaluate loss: 0.1643509864807129:  35%|██████▎           | 14/40 [00:03<00:07,  3.61it/s]Epoch: 12, train for the 67-th batch, train loss: 0.5725021958351135:  44%|█████▏      | 66/151 [00:14<00:18,  4.49it/s]Epoch: 12, train for the 67-th batch, train loss: 0.5725021958351135:  44%|█████▎      | 67/151 [00:14<00:18,  4.49it/s]evaluate for the 3-th batch, evaluate loss: 0.48459890484809875:   5%|█                  | 2/38 [00:00<00:09,  3.63it/s]evaluate for the 3-th batch, evaluate loss: 0.48459890484809875:   8%|█▌                 | 3/38 [00:00<00:09,  3.62it/s]evaluate for the 15-th batch, evaluate loss: 0.18452221155166626:  35%|█████▉           | 14/40 [00:04<00:07,  3.61it/s]evaluate for the 15-th batch, evaluate loss: 0.18452221155166626:  38%|██████▍          | 15/40 [00:04<00:06,  3.61it/s]Epoch: 3, train for the 76-th batch, train loss: 0.3336523473262787:  20%|██▌          | 75/383 [00:45<03:18,  1.55it/s]Epoch: 3, train for the 76-th batch, train loss: 0.3336523473262787:  20%|██▌          | 76/383 [00:45<03:17,  1.55it/s]Epoch: 4, train for the 173-th batch, train loss: 0.6303163170814514:  73%|███████▉   | 172/237 [01:39<00:42,  1.54it/s]Epoch: 4, train for the 173-th batch, train loss: 0.6303163170814514:  73%|████████   | 173/237 [01:39<00:41,  1.55it/s]Epoch: 12, train for the 68-th batch, train loss: 0.5539039373397827:  44%|█████▎      | 67/151 [00:14<00:18,  4.49it/s]Epoch: 12, train for the 68-th batch, train loss: 0.5539039373397827:  45%|█████▍      | 68/151 [00:14<00:18,  4.50it/s]evaluate for the 4-th batch, evaluate loss: 0.4743577539920807:   8%|█▌                  | 3/38 [00:01<00:09,  3.62it/s]evaluate for the 4-th batch, evaluate loss: 0.4743577539920807:  11%|██                  | 4/38 [00:01<00:09,  3.62it/s]evaluate for the 16-th batch, evaluate loss: 0.19618350267410278:  38%|██████▍          | 15/40 [00:04<00:06,  3.61it/s]evaluate for the 16-th batch, evaluate loss: 0.19618350267410278:  40%|██████▊          | 16/40 [00:04<00:06,  3.62it/s]Epoch: 12, train for the 69-th batch, train loss: 0.5656011700630188:  45%|█████▍      | 68/151 [00:14<00:18,  4.50it/s]Epoch: 12, train for the 69-th batch, train loss: 0.5656011700630188:  46%|█████▍      | 69/151 [00:14<00:18,  4.50it/s]evaluate for the 5-th batch, evaluate loss: 0.49682366847991943:  11%|██                 | 4/38 [00:01<00:09,  3.62it/s]evaluate for the 5-th batch, evaluate loss: 0.49682366847991943:  13%|██▌                | 5/38 [00:01<00:09,  3.62it/s]Epoch: 12, train for the 70-th batch, train loss: 0.5538889765739441:  46%|█████▍      | 69/151 [00:15<00:18,  4.50it/s]Epoch: 12, train for the 70-th batch, train loss: 0.5538889765739441:  46%|█████▌      | 70/151 [00:15<00:18,  4.49it/s]evaluate for the 17-th batch, evaluate loss: 0.1313452124595642:  40%|███████▏          | 16/40 [00:04<00:06,  3.62it/s]evaluate for the 17-th batch, evaluate loss: 0.1313452124595642:  42%|███████▋          | 17/40 [00:04<00:06,  3.61it/s]Epoch: 3, train for the 77-th batch, train loss: 0.3324697017669678:  20%|██▌          | 76/383 [00:45<03:17,  1.55it/s]Epoch: 3, train for the 77-th batch, train loss: 0.3324697017669678:  20%|██▌          | 77/383 [00:45<03:17,  1.55it/s]Epoch: 4, train for the 174-th batch, train loss: 0.6011353731155396:  73%|████████   | 173/237 [01:40<00:41,  1.55it/s]Epoch: 4, train for the 174-th batch, train loss: 0.6011353731155396:  73%|████████   | 174/237 [01:40<00:40,  1.55it/s]evaluate for the 6-th batch, evaluate loss: 0.48933038115501404:  13%|██▌                | 5/38 [00:01<00:09,  3.62it/s]evaluate for the 6-th batch, evaluate loss: 0.48933038115501404:  16%|███                | 6/38 [00:01<00:08,  3.62it/s]Epoch: 12, train for the 71-th batch, train loss: 0.542503297328949:  46%|██████       | 70/151 [00:15<00:18,  4.49it/s]Epoch: 12, train for the 71-th batch, train loss: 0.542503297328949:  47%|██████       | 71/151 [00:15<00:17,  4.49it/s]evaluate for the 18-th batch, evaluate loss: 0.1449359804391861:  42%|███████▋          | 17/40 [00:05<00:06,  3.61it/s]evaluate for the 18-th batch, evaluate loss: 0.1449359804391861:  45%|████████          | 18/40 [00:05<00:06,  3.61it/s]Epoch: 12, train for the 72-th batch, train loss: 0.5252311825752258:  47%|█████▋      | 71/151 [00:15<00:17,  4.49it/s]Epoch: 12, train for the 72-th batch, train loss: 0.5252311825752258:  48%|█████▋      | 72/151 [00:15<00:17,  4.49it/s]evaluate for the 7-th batch, evaluate loss: 0.4539588689804077:  16%|███▏                | 6/38 [00:01<00:08,  3.62it/s]evaluate for the 7-th batch, evaluate loss: 0.4539588689804077:  18%|███▋                | 7/38 [00:01<00:08,  3.62it/s]evaluate for the 19-th batch, evaluate loss: 0.15382489562034607:  45%|███████▋         | 18/40 [00:05<00:06,  3.61it/s]evaluate for the 19-th batch, evaluate loss: 0.15382489562034607:  48%|████████         | 19/40 [00:05<00:05,  3.62it/s]Epoch: 12, train for the 73-th batch, train loss: 0.5384504795074463:  48%|█████▋      | 72/151 [00:15<00:17,  4.49it/s]Epoch: 12, train for the 73-th batch, train loss: 0.5384504795074463:  48%|█████▊      | 73/151 [00:15<00:17,  4.46it/s]evaluate for the 8-th batch, evaluate loss: 0.5400049686431885:  18%|███▋                | 7/38 [00:02<00:08,  3.62it/s]evaluate for the 8-th batch, evaluate loss: 0.5400049686431885:  21%|████▏               | 8/38 [00:02<00:08,  3.61it/s]Epoch: 3, train for the 78-th batch, train loss: 0.3494113087654114:  20%|██▌          | 77/383 [00:46<03:17,  1.55it/s]Epoch: 3, train for the 78-th batch, train loss: 0.3494113087654114:  20%|██▋          | 78/383 [00:46<03:15,  1.56it/s]Epoch: 4, train for the 175-th batch, train loss: 0.6446332931518555:  73%|████████   | 174/237 [01:40<00:40,  1.55it/s]Epoch: 4, train for the 175-th batch, train loss: 0.6446332931518555:  74%|████████   | 175/237 [01:40<00:39,  1.56it/s]evaluate for the 20-th batch, evaluate loss: 0.15124450623989105:  48%|████████         | 19/40 [00:05<00:05,  3.62it/s]evaluate for the 20-th batch, evaluate loss: 0.15124450623989105:  50%|████████▌        | 20/40 [00:05<00:05,  3.62it/s]Epoch: 12, train for the 74-th batch, train loss: 0.489423006772995:  48%|██████▎      | 73/151 [00:16<00:17,  4.46it/s]Epoch: 12, train for the 74-th batch, train loss: 0.489423006772995:  49%|██████▎      | 74/151 [00:16<00:19,  4.01it/s]evaluate for the 9-th batch, evaluate loss: 0.5062121152877808:  21%|████▏               | 8/38 [00:02<00:08,  3.61it/s]evaluate for the 9-th batch, evaluate loss: 0.5062121152877808:  24%|████▋               | 9/38 [00:02<00:08,  3.62it/s]evaluate for the 21-th batch, evaluate loss: 0.08868923038244247:  50%|████████▌        | 20/40 [00:05<00:05,  3.62it/s]evaluate for the 21-th batch, evaluate loss: 0.08868923038244247:  52%|████████▉        | 21/40 [00:05<00:05,  3.62it/s]Epoch: 12, train for the 75-th batch, train loss: 0.5534233450889587:  49%|█████▉      | 74/151 [00:16<00:19,  4.01it/s]Epoch: 12, train for the 75-th batch, train loss: 0.5534233450889587:  50%|█████▉      | 75/151 [00:16<00:18,  4.14it/s]evaluate for the 10-th batch, evaluate loss: 0.5148802995681763:  24%|████▌              | 9/38 [00:02<00:08,  3.62it/s]evaluate for the 10-th batch, evaluate loss: 0.5148802995681763:  26%|████▋             | 10/38 [00:02<00:07,  3.62it/s]evaluate for the 22-th batch, evaluate loss: 0.15068265795707703:  52%|████████▉        | 21/40 [00:06<00:05,  3.62it/s]evaluate for the 22-th batch, evaluate loss: 0.15068265795707703:  55%|█████████▎       | 22/40 [00:06<00:04,  3.62it/s]Epoch: 12, train for the 76-th batch, train loss: 0.5625709295272827:  50%|█████▉      | 75/151 [00:16<00:18,  4.14it/s]Epoch: 12, train for the 76-th batch, train loss: 0.5625709295272827:  50%|██████      | 76/151 [00:16<00:17,  4.23it/s]Epoch: 3, train for the 79-th batch, train loss: 0.3112640976905823:  20%|██▋          | 78/383 [00:47<03:15,  1.56it/s]Epoch: 3, train for the 79-th batch, train loss: 0.3112640976905823:  21%|██▋          | 79/383 [00:47<03:15,  1.55it/s]Epoch: 4, train for the 176-th batch, train loss: 0.6332224011421204:  74%|████████   | 175/237 [01:41<00:39,  1.56it/s]Epoch: 4, train for the 176-th batch, train loss: 0.6332224011421204:  74%|████████▏  | 176/237 [01:41<00:39,  1.56it/s]evaluate for the 11-th batch, evaluate loss: 0.5076786875724792:  26%|████▋             | 10/38 [00:03<00:07,  3.62it/s]evaluate for the 11-th batch, evaluate loss: 0.5076786875724792:  29%|█████▏            | 11/38 [00:03<00:07,  3.62it/s]Epoch: 12, train for the 77-th batch, train loss: 0.497528076171875:  50%|██████▌      | 76/151 [00:16<00:17,  4.23it/s]Epoch: 12, train for the 77-th batch, train loss: 0.497528076171875:  51%|██████▋      | 77/151 [00:16<00:17,  4.33it/s]evaluate for the 23-th batch, evaluate loss: 0.15629921853542328:  55%|█████████▎       | 22/40 [00:06<00:04,  3.62it/s]evaluate for the 23-th batch, evaluate loss: 0.15629921853542328:  57%|█████████▊       | 23/40 [00:06<00:04,  3.62it/s]evaluate for the 12-th batch, evaluate loss: 0.5534970760345459:  29%|█████▏            | 11/38 [00:03<00:07,  3.62it/s]evaluate for the 12-th batch, evaluate loss: 0.5534970760345459:  32%|█████▋            | 12/38 [00:03<00:07,  3.62it/s]Epoch: 12, train for the 78-th batch, train loss: 0.4607858955860138:  51%|██████      | 77/151 [00:16<00:17,  4.33it/s]Epoch: 12, train for the 78-th batch, train loss: 0.4607858955860138:  52%|██████▏     | 78/151 [00:16<00:16,  4.41it/s]evaluate for the 24-th batch, evaluate loss: 0.14755435287952423:  57%|█████████▊       | 23/40 [00:06<00:04,  3.62it/s]evaluate for the 24-th batch, evaluate loss: 0.14755435287952423:  60%|██████████▏      | 24/40 [00:06<00:04,  3.62it/s]Epoch: 12, train for the 79-th batch, train loss: 0.6124972105026245:  52%|██████▏     | 78/151 [00:17<00:16,  4.41it/s]Epoch: 12, train for the 79-th batch, train loss: 0.6124972105026245:  52%|██████▎     | 79/151 [00:17<00:16,  4.43it/s]Epoch: 3, train for the 80-th batch, train loss: 0.23714210093021393:  21%|██▍         | 79/383 [00:47<03:15,  1.55it/s]Epoch: 3, train for the 80-th batch, train loss: 0.23714210093021393:  21%|██▌         | 80/383 [00:47<03:15,  1.55it/s]Epoch: 4, train for the 177-th batch, train loss: 0.6457450985908508:  74%|████████▏  | 176/237 [01:42<00:39,  1.56it/s]Epoch: 4, train for the 177-th batch, train loss: 0.6457450985908508:  75%|████████▏  | 177/237 [01:42<00:38,  1.55it/s]evaluate for the 13-th batch, evaluate loss: 0.5399911999702454:  32%|█████▋            | 12/38 [00:03<00:07,  3.62it/s]evaluate for the 13-th batch, evaluate loss: 0.5399911999702454:  34%|██████▏           | 13/38 [00:03<00:06,  3.62it/s]evaluate for the 25-th batch, evaluate loss: 0.14859838783740997:  60%|██████████▏      | 24/40 [00:06<00:04,  3.62it/s]evaluate for the 25-th batch, evaluate loss: 0.14859838783740997:  62%|██████████▋      | 25/40 [00:06<00:04,  3.61it/s]Epoch: 12, train for the 80-th batch, train loss: 0.3838363587856293:  52%|██████▎     | 79/151 [00:17<00:16,  4.43it/s]Epoch: 12, train for the 80-th batch, train loss: 0.3838363587856293:  53%|██████▎     | 80/151 [00:17<00:15,  4.50it/s]evaluate for the 14-th batch, evaluate loss: 0.47466665506362915:  34%|█████▊           | 13/38 [00:03<00:06,  3.62it/s]evaluate for the 14-th batch, evaluate loss: 0.47466665506362915:  37%|██████▎          | 14/38 [00:03<00:06,  3.63it/s]evaluate for the 26-th batch, evaluate loss: 0.12979936599731445:  62%|██████████▋      | 25/40 [00:07<00:04,  3.61it/s]evaluate for the 26-th batch, evaluate loss: 0.12979936599731445:  65%|███████████      | 26/40 [00:07<00:03,  3.61it/s]Epoch: 12, train for the 81-th batch, train loss: 0.5631277561187744:  53%|██████▎     | 80/151 [00:17<00:15,  4.50it/s]Epoch: 12, train for the 81-th batch, train loss: 0.5631277561187744:  54%|██████▍     | 81/151 [00:17<00:15,  4.50it/s]evaluate for the 15-th batch, evaluate loss: 0.5049023032188416:  37%|██████▋           | 14/38 [00:04<00:06,  3.63it/s]evaluate for the 15-th batch, evaluate loss: 0.5049023032188416:  39%|███████           | 15/38 [00:04<00:06,  3.63it/s]Epoch: 3, train for the 81-th batch, train loss: 0.3068920969963074:  21%|██▋          | 80/383 [00:48<03:15,  1.55it/s]Epoch: 3, train for the 81-th batch, train loss: 0.3068920969963074:  21%|██▋          | 81/383 [00:48<03:15,  1.55it/s]Epoch: 4, train for the 178-th batch, train loss: 0.6303539276123047:  75%|████████▏  | 177/237 [01:42<00:38,  1.55it/s]Epoch: 4, train for the 178-th batch, train loss: 0.6303539276123047:  75%|████████▎  | 178/237 [01:42<00:38,  1.55it/s]Epoch: 12, train for the 82-th batch, train loss: 0.5756760835647583:  54%|██████▍     | 81/151 [00:17<00:15,  4.50it/s]Epoch: 12, train for the 82-th batch, train loss: 0.5756760835647583:  54%|██████▌     | 82/151 [00:17<00:15,  4.47it/s]evaluate for the 27-th batch, evaluate loss: 0.14365220069885254:  65%|███████████      | 26/40 [00:07<00:03,  3.61it/s]evaluate for the 27-th batch, evaluate loss: 0.14365220069885254:  68%|███████████▍     | 27/40 [00:07<00:03,  3.61it/s]evaluate for the 16-th batch, evaluate loss: 0.5235844850540161:  39%|███████           | 15/38 [00:04<00:06,  3.63it/s]evaluate for the 16-th batch, evaluate loss: 0.5235844850540161:  42%|███████▌          | 16/38 [00:04<00:06,  3.63it/s]Epoch: 12, train for the 83-th batch, train loss: 0.5533726215362549:  54%|██████▌     | 82/151 [00:17<00:15,  4.47it/s]Epoch: 12, train for the 83-th batch, train loss: 0.5533726215362549:  55%|██████▌     | 83/151 [00:17<00:15,  4.47it/s]evaluate for the 28-th batch, evaluate loss: 0.11687183380126953:  68%|███████████▍     | 27/40 [00:07<00:03,  3.61it/s]evaluate for the 28-th batch, evaluate loss: 0.11687183380126953:  70%|███████████▉     | 28/40 [00:07<00:03,  3.61it/s]Epoch: 12, train for the 84-th batch, train loss: 0.6023808717727661:  55%|██████▌     | 83/151 [00:18<00:15,  4.47it/s]Epoch: 12, train for the 84-th batch, train loss: 0.6023808717727661:  56%|██████▋     | 84/151 [00:18<00:14,  4.48it/s]evaluate for the 17-th batch, evaluate loss: 0.48943030834198:  42%|████████▍           | 16/38 [00:04<00:06,  3.63it/s]evaluate for the 17-th batch, evaluate loss: 0.48943030834198:  45%|████████▉           | 17/38 [00:04<00:05,  3.63it/s]evaluate for the 29-th batch, evaluate loss: 0.16512081027030945:  70%|███████████▉     | 28/40 [00:08<00:03,  3.61it/s]evaluate for the 29-th batch, evaluate loss: 0.16512081027030945:  72%|████████████▎    | 29/40 [00:08<00:03,  3.62it/s]Epoch: 3, train for the 82-th batch, train loss: 0.34769406914711:  21%|███▏           | 81/383 [00:49<03:15,  1.55it/s]Epoch: 3, train for the 82-th batch, train loss: 0.34769406914711:  21%|███▏           | 82/383 [00:49<03:14,  1.55it/s]Epoch: 4, train for the 179-th batch, train loss: 0.6314263343811035:  75%|████████▎  | 178/237 [01:43<00:38,  1.55it/s]Epoch: 4, train for the 179-th batch, train loss: 0.6314263343811035:  76%|████████▎  | 179/237 [01:43<00:37,  1.55it/s]Epoch: 12, train for the 85-th batch, train loss: 0.5616841912269592:  56%|██████▋     | 84/151 [00:18<00:14,  4.48it/s]Epoch: 12, train for the 85-th batch, train loss: 0.5616841912269592:  56%|██████▊     | 85/151 [00:18<00:14,  4.48it/s]evaluate for the 18-th batch, evaluate loss: 0.5373410582542419:  45%|████████          | 17/38 [00:04<00:05,  3.63it/s]evaluate for the 18-th batch, evaluate loss: 0.5373410582542419:  47%|████████▌         | 18/38 [00:04<00:05,  3.63it/s]evaluate for the 30-th batch, evaluate loss: 0.16632165014743805:  72%|████████████▎    | 29/40 [00:08<00:03,  3.62it/s]evaluate for the 30-th batch, evaluate loss: 0.16632165014743805:  75%|████████████▊    | 30/40 [00:08<00:02,  3.61it/s]Epoch: 12, train for the 86-th batch, train loss: 0.5435649752616882:  56%|██████▊     | 85/151 [00:18<00:14,  4.48it/s]Epoch: 12, train for the 86-th batch, train loss: 0.5435649752616882:  57%|██████▊     | 86/151 [00:18<00:14,  4.46it/s]evaluate for the 19-th batch, evaluate loss: 0.5020358562469482:  47%|████████▌         | 18/38 [00:05<00:05,  3.63it/s]evaluate for the 19-th batch, evaluate loss: 0.5020358562469482:  50%|█████████         | 19/38 [00:05<00:05,  3.63it/s]Epoch: 12, train for the 87-th batch, train loss: 0.5599713921546936:  57%|██████▊     | 86/151 [00:18<00:14,  4.46it/s]Epoch: 12, train for the 87-th batch, train loss: 0.5599713921546936:  58%|██████▉     | 87/151 [00:18<00:14,  4.48it/s]evaluate for the 31-th batch, evaluate loss: 0.17228129506111145:  75%|████████████▊    | 30/40 [00:08<00:02,  3.61it/s]evaluate for the 31-th batch, evaluate loss: 0.17228129506111145:  78%|█████████████▏   | 31/40 [00:08<00:02,  3.62it/s]Epoch: 3, train for the 83-th batch, train loss: 0.38361358642578125:  21%|██▌         | 82/383 [00:49<03:14,  1.55it/s]Epoch: 3, train for the 83-th batch, train loss: 0.38361358642578125:  22%|██▌         | 83/383 [00:49<03:14,  1.54it/s]Epoch: 4, train for the 180-th batch, train loss: 0.6457485556602478:  76%|████████▎  | 179/237 [01:44<00:37,  1.55it/s]Epoch: 4, train for the 180-th batch, train loss: 0.6457485556602478:  76%|████████▎  | 180/237 [01:44<00:36,  1.54it/s]evaluate for the 20-th batch, evaluate loss: 0.4846135675907135:  50%|█████████         | 19/38 [00:05<00:05,  3.63it/s]evaluate for the 20-th batch, evaluate loss: 0.4846135675907135:  53%|█████████▍        | 20/38 [00:05<00:04,  3.62it/s]Epoch: 12, train for the 88-th batch, train loss: 0.6030625104904175:  58%|██████▉     | 87/151 [00:19<00:14,  4.48it/s]Epoch: 12, train for the 88-th batch, train loss: 0.6030625104904175:  58%|██████▉     | 88/151 [00:19<00:14,  4.49it/s]evaluate for the 32-th batch, evaluate loss: 0.13095951080322266:  78%|█████████████▏   | 31/40 [00:08<00:02,  3.62it/s]evaluate for the 32-th batch, evaluate loss: 0.13095951080322266:  80%|█████████████▌   | 32/40 [00:08<00:02,  3.61it/s]Epoch: 12, train for the 89-th batch, train loss: 0.55939781665802:  58%|████████▏     | 88/151 [00:19<00:14,  4.49it/s]Epoch: 12, train for the 89-th batch, train loss: 0.55939781665802:  59%|████████▎     | 89/151 [00:19<00:13,  4.50it/s]evaluate for the 21-th batch, evaluate loss: 0.4948926568031311:  53%|█████████▍        | 20/38 [00:05<00:04,  3.62it/s]evaluate for the 21-th batch, evaluate loss: 0.4948926568031311:  55%|█████████▉        | 21/38 [00:05<00:04,  3.62it/s]evaluate for the 33-th batch, evaluate loss: 0.14280995726585388:  80%|█████████████▌   | 32/40 [00:09<00:02,  3.61it/s]evaluate for the 33-th batch, evaluate loss: 0.14280995726585388:  82%|██████████████   | 33/40 [00:09<00:01,  3.61it/s]Epoch: 12, train for the 90-th batch, train loss: 0.5606949925422668:  59%|███████     | 89/151 [00:19<00:13,  4.50it/s]Epoch: 12, train for the 90-th batch, train loss: 0.5606949925422668:  60%|███████▏    | 90/151 [00:19<00:13,  4.49it/s]evaluate for the 22-th batch, evaluate loss: 0.5199306607246399:  55%|█████████▉        | 21/38 [00:06<00:04,  3.62it/s]evaluate for the 22-th batch, evaluate loss: 0.5199306607246399:  58%|██████████▍       | 22/38 [00:06<00:04,  3.62it/s]Epoch: 3, train for the 84-th batch, train loss: 0.36842843890190125:  22%|██▌         | 83/383 [00:50<03:14,  1.54it/s]Epoch: 3, train for the 84-th batch, train loss: 0.36842843890190125:  22%|██▋         | 84/383 [00:50<03:13,  1.54it/s]Epoch: 4, train for the 181-th batch, train loss: 0.6594156622886658:  76%|████████▎  | 180/237 [01:44<00:36,  1.54it/s]Epoch: 4, train for the 181-th batch, train loss: 0.6594156622886658:  76%|████████▍  | 181/237 [01:44<00:36,  1.54it/s]evaluate for the 34-th batch, evaluate loss: 0.10778390616178513:  82%|██████████████   | 33/40 [00:09<00:01,  3.61it/s]evaluate for the 34-th batch, evaluate loss: 0.10778390616178513:  85%|██████████████▍  | 34/40 [00:09<00:01,  3.61it/s]Epoch: 12, train for the 91-th batch, train loss: 0.4902404248714447:  60%|███████▏    | 90/151 [00:19<00:13,  4.49it/s]Epoch: 12, train for the 91-th batch, train loss: 0.4902404248714447:  60%|███████▏    | 91/151 [00:19<00:13,  4.49it/s]evaluate for the 23-th batch, evaluate loss: 0.5220463275909424:  58%|██████████▍       | 22/38 [00:06<00:04,  3.62it/s]evaluate for the 23-th batch, evaluate loss: 0.5220463275909424:  61%|██████████▉       | 23/38 [00:06<00:04,  3.63it/s]Epoch: 12, train for the 92-th batch, train loss: 0.559186577796936:  60%|███████▊     | 91/151 [00:19<00:13,  4.49it/s]Epoch: 12, train for the 92-th batch, train loss: 0.559186577796936:  61%|███████▉     | 92/151 [00:19<00:13,  4.49it/s]evaluate for the 35-th batch, evaluate loss: 0.15232989192008972:  85%|██████████████▍  | 34/40 [00:09<00:01,  3.61it/s]evaluate for the 35-th batch, evaluate loss: 0.15232989192008972:  88%|██████████████▉  | 35/40 [00:09<00:01,  3.61it/s]evaluate for the 24-th batch, evaluate loss: 0.49548956751823425:  61%|██████████▎      | 23/38 [00:06<00:04,  3.63it/s]evaluate for the 24-th batch, evaluate loss: 0.49548956751823425:  63%|██████████▋      | 24/38 [00:06<00:03,  3.62it/s]Epoch: 12, train for the 93-th batch, train loss: 0.5065100193023682:  61%|███████▎    | 92/151 [00:20<00:13,  4.49it/s]Epoch: 12, train for the 93-th batch, train loss: 0.5065100193023682:  62%|███████▍    | 93/151 [00:20<00:12,  4.49it/s]evaluate for the 36-th batch, evaluate loss: 0.14556998014450073:  88%|██████████████▉  | 35/40 [00:09<00:01,  3.61it/s]evaluate for the 36-th batch, evaluate loss: 0.14556998014450073:  90%|███████████████▎ | 36/40 [00:09<00:01,  3.62it/s]Epoch: 3, train for the 85-th batch, train loss: 0.39128515124320984:  22%|██▋         | 84/383 [00:51<03:13,  1.54it/s]Epoch: 3, train for the 85-th batch, train loss: 0.39128515124320984:  22%|██▋         | 85/383 [00:51<03:11,  1.56it/s]Epoch: 4, train for the 182-th batch, train loss: 0.6498814225196838:  76%|████████▍  | 181/237 [01:45<00:36,  1.54it/s]Epoch: 4, train for the 182-th batch, train loss: 0.6498814225196838:  77%|████████▍  | 182/237 [01:45<00:35,  1.55it/s]Epoch: 12, train for the 94-th batch, train loss: 0.5424531698226929:  62%|███████▍    | 93/151 [00:20<00:12,  4.49it/s]Epoch: 12, train for the 94-th batch, train loss: 0.5424531698226929:  62%|███████▍    | 94/151 [00:20<00:12,  4.48it/s]evaluate for the 25-th batch, evaluate loss: 0.4999048411846161:  63%|███████████▎      | 24/38 [00:06<00:03,  3.62it/s]evaluate for the 25-th batch, evaluate loss: 0.4999048411846161:  66%|███████████▊      | 25/38 [00:06<00:03,  3.62it/s]evaluate for the 37-th batch, evaluate loss: 0.20604819059371948:  90%|███████████████▎ | 36/40 [00:10<00:01,  3.62it/s]evaluate for the 37-th batch, evaluate loss: 0.20604819059371948:  92%|███████████████▋ | 37/40 [00:10<00:00,  3.62it/s]Epoch: 12, train for the 95-th batch, train loss: 0.5171679258346558:  62%|███████▍    | 94/151 [00:20<00:12,  4.48it/s]Epoch: 12, train for the 95-th batch, train loss: 0.5171679258346558:  63%|███████▌    | 95/151 [00:20<00:12,  4.48it/s]evaluate for the 26-th batch, evaluate loss: 0.50454181432724:  66%|█████████████▏      | 25/38 [00:07<00:03,  3.62it/s]evaluate for the 26-th batch, evaluate loss: 0.50454181432724:  68%|█████████████▋      | 26/38 [00:07<00:03,  3.62it/s]evaluate for the 38-th batch, evaluate loss: 0.13564009964466095:  92%|███████████████▋ | 37/40 [00:10<00:00,  3.62it/s]evaluate for the 38-th batch, evaluate loss: 0.13564009964466095:  95%|████████████████▏| 38/40 [00:10<00:00,  3.62it/s]Epoch: 12, train for the 96-th batch, train loss: 0.5409867167472839:  63%|███████▌    | 95/151 [00:20<00:12,  4.48it/s]Epoch: 12, train for the 96-th batch, train loss: 0.5409867167472839:  64%|███████▋    | 96/151 [00:20<00:12,  4.48it/s]Epoch: 3, train for the 86-th batch, train loss: 0.3284832835197449:  22%|██▉          | 85/383 [00:51<03:11,  1.56it/s]Epoch: 4, train for the 183-th batch, train loss: 0.643321692943573:  77%|█████████▏  | 182/237 [01:45<00:35,  1.55it/s]Epoch: 3, train for the 86-th batch, train loss: 0.3284832835197449:  22%|██▉          | 86/383 [00:51<03:11,  1.55it/s]Epoch: 4, train for the 183-th batch, train loss: 0.643321692943573:  77%|█████████▎  | 183/237 [01:45<00:34,  1.55it/s]evaluate for the 27-th batch, evaluate loss: 0.5319567918777466:  68%|████████████▎     | 26/38 [00:07<00:03,  3.62it/s]evaluate for the 27-th batch, evaluate loss: 0.5319567918777466:  71%|████████████▊     | 27/38 [00:07<00:03,  3.62it/s]Epoch: 12, train for the 97-th batch, train loss: 0.5985112190246582:  64%|███████▋    | 96/151 [00:21<00:12,  4.48it/s]Epoch: 12, train for the 97-th batch, train loss: 0.5985112190246582:  64%|███████▋    | 97/151 [00:21<00:12,  4.48it/s]evaluate for the 39-th batch, evaluate loss: 0.15356923639774323:  95%|████████████████▏| 38/40 [00:10<00:00,  3.62it/s]evaluate for the 39-th batch, evaluate loss: 0.15356923639774323:  98%|████████████████▌| 39/40 [00:10<00:00,  3.62it/s]evaluate for the 40-th batch, evaluate loss: 0.06034257262945175:  98%|████████████████▌| 39/40 [00:10<00:00,  3.62it/s]evaluate for the 40-th batch, evaluate loss: 0.06034257262945175: 100%|█████████████████| 40/40 [00:10<00:00,  3.69it/s]
  0%|                                                                                            | 0/21 [00:00<?, ?it/s]evaluate for the 28-th batch, evaluate loss: 0.5419740080833435:  71%|████████████▊     | 27/38 [00:07<00:03,  3.62it/s]evaluate for the 28-th batch, evaluate loss: 0.5419740080833435:  74%|█████████████▎    | 28/38 [00:07<00:02,  3.62it/s]Epoch: 12, train for the 98-th batch, train loss: 0.6102206110954285:  64%|███████▋    | 97/151 [00:21<00:12,  4.48it/s]Epoch: 12, train for the 98-th batch, train loss: 0.6102206110954285:  65%|███████▊    | 98/151 [00:21<00:11,  4.48it/s]evaluate for the 1-th batch, evaluate loss: 0.21635375916957855:   0%|                           | 0/21 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.21635375916957855:   5%|▉                  | 1/21 [00:00<00:05,  3.50it/s]Epoch: 12, train for the 99-th batch, train loss: 0.6170512437820435:  65%|███████▊    | 98/151 [00:21<00:11,  4.48it/s]Epoch: 12, train for the 99-th batch, train loss: 0.6170512437820435:  66%|███████▊    | 99/151 [00:21<00:11,  4.48it/s]evaluate for the 29-th batch, evaluate loss: 0.5087930560112:  74%|███████████████▍     | 28/38 [00:08<00:02,  3.62it/s]evaluate for the 29-th batch, evaluate loss: 0.5087930560112:  76%|████████████████     | 29/38 [00:08<00:02,  3.63it/s]Epoch: 3, train for the 87-th batch, train loss: 0.3899073600769043:  22%|██▉          | 86/383 [00:52<03:11,  1.55it/s]Epoch: 4, train for the 184-th batch, train loss: 0.6159095764160156:  77%|████████▍  | 183/237 [01:46<00:34,  1.55it/s]Epoch: 3, train for the 87-th batch, train loss: 0.3899073600769043:  23%|██▉          | 87/383 [00:52<03:11,  1.54it/s]Epoch: 4, train for the 184-th batch, train loss: 0.6159095764160156:  78%|████████▌  | 184/237 [01:46<00:34,  1.55it/s]evaluate for the 2-th batch, evaluate loss: 0.24868203699588776:   5%|▉                  | 1/21 [00:00<00:05,  3.50it/s]evaluate for the 2-th batch, evaluate loss: 0.24868203699588776:  10%|█▊                 | 2/21 [00:00<00:05,  3.46it/s]Epoch: 12, train for the 100-th batch, train loss: 0.6471933126449585:  66%|███████▏   | 99/151 [00:21<00:11,  4.48it/s]Epoch: 12, train for the 100-th batch, train loss: 0.6471933126449585:  66%|██████▌   | 100/151 [00:21<00:11,  4.48it/s]evaluate for the 30-th batch, evaluate loss: 0.5392991304397583:  76%|█████████████▋    | 29/38 [00:08<00:02,  3.63it/s]evaluate for the 30-th batch, evaluate loss: 0.5392991304397583:  79%|██████████████▏   | 30/38 [00:08<00:02,  3.67it/s]Epoch: 12, train for the 101-th batch, train loss: 0.6751376390457153:  66%|██████▌   | 100/151 [00:22<00:11,  4.48it/s]Epoch: 12, train for the 101-th batch, train loss: 0.6751376390457153:  67%|██████▋   | 101/151 [00:22<00:11,  4.48it/s]evaluate for the 3-th batch, evaluate loss: 0.25019848346710205:  10%|█▊                 | 2/21 [00:00<00:05,  3.46it/s]evaluate for the 3-th batch, evaluate loss: 0.25019848346710205:  14%|██▋                | 3/21 [00:00<00:05,  3.49it/s]evaluate for the 31-th batch, evaluate loss: 0.5288220643997192:  79%|██████████████▏   | 30/38 [00:08<00:02,  3.67it/s]evaluate for the 31-th batch, evaluate loss: 0.5288220643997192:  82%|██████████████▋   | 31/38 [00:08<00:01,  3.69it/s]Epoch: 12, train for the 102-th batch, train loss: 0.6251601576805115:  67%|██████▋   | 101/151 [00:22<00:11,  4.48it/s]Epoch: 12, train for the 102-th batch, train loss: 0.6251601576805115:  68%|██████▊   | 102/151 [00:22<00:10,  4.48it/s]evaluate for the 4-th batch, evaluate loss: 0.21333588659763336:  14%|██▋                | 3/21 [00:01<00:05,  3.49it/s]evaluate for the 4-th batch, evaluate loss: 0.21333588659763336:  19%|███▌               | 4/21 [00:01<00:04,  3.51it/s]Epoch: 3, train for the 88-th batch, train loss: 0.3863692879676819:  23%|██▉          | 87/383 [00:52<03:11,  1.54it/s]Epoch: 4, train for the 185-th batch, train loss: 0.5955288410186768:  78%|████████▌  | 184/237 [01:47<00:34,  1.55it/s]Epoch: 3, train for the 88-th batch, train loss: 0.3863692879676819:  23%|██▉          | 88/383 [00:52<03:11,  1.54it/s]Epoch: 4, train for the 185-th batch, train loss: 0.5955288410186768:  78%|████████▌  | 185/237 [01:47<00:33,  1.54it/s]evaluate for the 32-th batch, evaluate loss: 0.4938606917858124:  82%|██████████████▋   | 31/38 [00:08<00:01,  3.69it/s]evaluate for the 32-th batch, evaluate loss: 0.4938606917858124:  84%|███████████████▏  | 32/38 [00:08<00:01,  3.70it/s]Epoch: 12, train for the 103-th batch, train loss: 0.6419603824615479:  68%|██████▊   | 102/151 [00:22<00:10,  4.48it/s]Epoch: 12, train for the 103-th batch, train loss: 0.6419603824615479:  68%|██████▊   | 103/151 [00:22<00:10,  4.48it/s]evaluate for the 5-th batch, evaluate loss: 0.22392214834690094:  19%|███▌               | 4/21 [00:01<00:04,  3.51it/s]evaluate for the 5-th batch, evaluate loss: 0.22392214834690094:  24%|████▌              | 5/21 [00:01<00:04,  3.52it/s]evaluate for the 33-th batch, evaluate loss: 0.4858235716819763:  84%|███████████████▏  | 32/38 [00:09<00:01,  3.70it/s]evaluate for the 33-th batch, evaluate loss: 0.4858235716819763:  87%|███████████████▋  | 33/38 [00:09<00:01,  3.71it/s]Epoch: 12, train for the 104-th batch, train loss: 0.5822371244430542:  68%|██████▊   | 103/151 [00:22<00:10,  4.48it/s]Epoch: 12, train for the 104-th batch, train loss: 0.5822371244430542:  69%|██████▉   | 104/151 [00:22<00:10,  4.49it/s]evaluate for the 6-th batch, evaluate loss: 0.24116100370883942:  24%|████▌              | 5/21 [00:01<00:04,  3.52it/s]evaluate for the 6-th batch, evaluate loss: 0.24116100370883942:  29%|█████▍             | 6/21 [00:01<00:04,  3.53it/s]Epoch: 12, train for the 105-th batch, train loss: 0.5495632290840149:  69%|██████▉   | 104/151 [00:22<00:10,  4.49it/s]Epoch: 12, train for the 105-th batch, train loss: 0.5495632290840149:  70%|██████▉   | 105/151 [00:22<00:10,  4.49it/s]evaluate for the 34-th batch, evaluate loss: 0.5215301513671875:  87%|███████████████▋  | 33/38 [00:09<00:01,  3.71it/s]evaluate for the 34-th batch, evaluate loss: 0.5215301513671875:  89%|████████████████  | 34/38 [00:09<00:01,  3.71it/s]Epoch: 3, train for the 89-th batch, train loss: 0.41529831290245056:  23%|██▊         | 88/383 [00:53<03:11,  1.54it/s]Epoch: 4, train for the 186-th batch, train loss: 0.6174758672714233:  78%|████████▌  | 185/237 [01:47<00:33,  1.54it/s]Epoch: 4, train for the 186-th batch, train loss: 0.6174758672714233:  78%|████████▋  | 186/237 [01:47<00:33,  1.54it/s]Epoch: 3, train for the 89-th batch, train loss: 0.41529831290245056:  23%|██▊         | 89/383 [00:53<03:11,  1.54it/s]Epoch: 12, train for the 106-th batch, train loss: 0.5350889563560486:  70%|██████▉   | 105/151 [00:23<00:10,  4.49it/s]Epoch: 12, train for the 106-th batch, train loss: 0.5350889563560486:  70%|███████   | 106/151 [00:23<00:10,  4.49it/s]evaluate for the 7-th batch, evaluate loss: 0.23908595740795135:  29%|█████▍             | 6/21 [00:01<00:04,  3.53it/s]evaluate for the 7-th batch, evaluate loss: 0.23908595740795135:  33%|██████▎            | 7/21 [00:01<00:03,  3.53it/s]evaluate for the 35-th batch, evaluate loss: 0.544333815574646:  89%|█████████████████  | 34/38 [00:09<00:01,  3.71it/s]evaluate for the 35-th batch, evaluate loss: 0.544333815574646:  92%|█████████████████▌ | 35/38 [00:09<00:00,  3.73it/s]Epoch: 12, train for the 107-th batch, train loss: 0.564433753490448:  70%|███████▋   | 106/151 [00:23<00:10,  4.49it/s]Epoch: 12, train for the 107-th batch, train loss: 0.564433753490448:  71%|███████▊   | 107/151 [00:23<00:09,  4.48it/s]evaluate for the 8-th batch, evaluate loss: 0.29301953315734863:  33%|██████▎            | 7/21 [00:02<00:03,  3.53it/s]evaluate for the 8-th batch, evaluate loss: 0.29301953315734863:  38%|███████▏           | 8/21 [00:02<00:03,  3.53it/s]evaluate for the 36-th batch, evaluate loss: 0.5664330720901489:  92%|████████████████▌ | 35/38 [00:09<00:00,  3.73it/s]evaluate for the 36-th batch, evaluate loss: 0.5664330720901489:  95%|█████████████████ | 36/38 [00:09<00:00,  3.72it/s]Epoch: 12, train for the 108-th batch, train loss: 0.5495807528495789:  71%|███████   | 107/151 [00:23<00:09,  4.48it/s]Epoch: 12, train for the 108-th batch, train loss: 0.5495807528495789:  72%|███████▏  | 108/151 [00:23<00:09,  4.48it/s]Epoch: 4, train for the 187-th batch, train loss: 0.6313778758049011:  78%|████████▋  | 186/237 [01:48<00:33,  1.54it/s]Epoch: 4, train for the 187-th batch, train loss: 0.6313778758049011:  79%|████████▋  | 187/237 [01:48<00:32,  1.54it/s]Epoch: 3, train for the 90-th batch, train loss: 0.34686893224716187:  23%|██▊         | 89/383 [00:54<03:11,  1.54it/s]Epoch: 3, train for the 90-th batch, train loss: 0.34686893224716187:  23%|██▊         | 90/383 [00:54<03:10,  1.54it/s]evaluate for the 9-th batch, evaluate loss: 0.21706141531467438:  38%|███████▏           | 8/21 [00:02<00:03,  3.53it/s]evaluate for the 9-th batch, evaluate loss: 0.21706141531467438:  43%|████████▏          | 9/21 [00:02<00:03,  3.54it/s]evaluate for the 37-th batch, evaluate loss: 0.4913143813610077:  95%|█████████████████ | 36/38 [00:10<00:00,  3.72it/s]evaluate for the 37-th batch, evaluate loss: 0.4913143813610077:  97%|█████████████████▌| 37/38 [00:10<00:00,  3.72it/s]Epoch: 12, train for the 109-th batch, train loss: 0.5448668003082275:  72%|███████▏  | 108/151 [00:23<00:09,  4.48it/s]Epoch: 12, train for the 109-th batch, train loss: 0.5448668003082275:  72%|███████▏  | 109/151 [00:23<00:09,  4.48it/s]evaluate for the 10-th batch, evaluate loss: 0.21862705051898956:  43%|███████▋          | 9/21 [00:02<00:03,  3.54it/s]evaluate for the 38-th batch, evaluate loss: 0.5033716559410095:  97%|█████████████████▌| 37/38 [00:10<00:00,  3.72it/s]evaluate for the 10-th batch, evaluate loss: 0.21862705051898956:  48%|████████         | 10/21 [00:02<00:03,  3.52it/s]evaluate for the 38-th batch, evaluate loss: 0.5033716559410095: 100%|██████████████████| 38/38 [00:10<00:00,  3.69it/s]evaluate for the 38-th batch, evaluate loss: 0.5033716559410095: 100%|██████████████████| 38/38 [00:10<00:00,  3.65it/s]
  0%|                                                                                            | 0/20 [00:00<?, ?it/s]Epoch: 12, train for the 110-th batch, train loss: 0.5573608875274658:  72%|███████▏  | 109/151 [00:24<00:09,  4.48it/s]Epoch: 12, train for the 110-th batch, train loss: 0.5573608875274658:  73%|███████▎  | 110/151 [00:24<00:09,  4.48it/s]Epoch: 4, train for the 188-th batch, train loss: 0.6353791952133179:  79%|████████▋  | 187/237 [01:49<00:32,  1.54it/s]Epoch: 4, train for the 188-th batch, train loss: 0.6353791952133179:  79%|████████▋  | 188/237 [01:49<00:31,  1.55it/s]Epoch: 12, train for the 111-th batch, train loss: 0.5198538899421692:  73%|███████▎  | 110/151 [00:24<00:09,  4.48it/s]Epoch: 12, train for the 111-th batch, train loss: 0.5198538899421692:  74%|███████▎  | 111/151 [00:24<00:08,  4.49it/s]Epoch: 3, train for the 91-th batch, train loss: 0.2871706187725067:  23%|███          | 90/383 [00:54<03:10,  1.54it/s]Epoch: 3, train for the 91-th batch, train loss: 0.2871706187725067:  24%|███          | 91/383 [00:54<03:08,  1.55it/s]evaluate for the 11-th batch, evaluate loss: 0.15628688037395477:  48%|████████         | 10/21 [00:03<00:03,  3.52it/s]evaluate for the 11-th batch, evaluate loss: 0.15628688037395477:  52%|████████▉        | 11/21 [00:03<00:02,  3.51it/s]evaluate for the 1-th batch, evaluate loss: 0.5753114819526672:   0%|                            | 0/20 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.5753114819526672:   5%|█                   | 1/20 [00:00<00:05,  3.45it/s]Epoch: 12, train for the 112-th batch, train loss: 0.5155300498008728:  74%|███████▎  | 111/151 [00:24<00:08,  4.49it/s]Epoch: 12, train for the 112-th batch, train loss: 0.5155300498008728:  74%|███████▍  | 112/151 [00:24<00:08,  4.49it/s]evaluate for the 12-th batch, evaluate loss: 0.27052831649780273:  52%|████████▉        | 11/21 [00:03<00:02,  3.51it/s]evaluate for the 12-th batch, evaluate loss: 0.27052831649780273:  57%|█████████▋       | 12/21 [00:03<00:02,  3.51it/s]evaluate for the 2-th batch, evaluate loss: 0.6236918568611145:   5%|█                   | 1/20 [00:00<00:05,  3.45it/s]evaluate for the 2-th batch, evaluate loss: 0.6236918568611145:  10%|██                  | 2/20 [00:00<00:05,  3.47it/s]Epoch: 3, train for the 92-th batch, train loss: 0.3448215425014496:  24%|███          | 91/383 [00:55<03:08,  1.55it/s]Epoch: 3, train for the 92-th batch, train loss: 0.3448215425014496:  24%|███          | 92/383 [00:55<02:45,  1.75it/s]Epoch: 12, train for the 113-th batch, train loss: 0.557824432849884:  74%|████████▏  | 112/151 [00:24<00:08,  4.49it/s]Epoch: 12, train for the 113-th batch, train loss: 0.557824432849884:  75%|████████▏  | 113/151 [00:24<00:08,  4.47it/s]evaluate for the 13-th batch, evaluate loss: 0.2269870638847351:  57%|██████████▎       | 12/21 [00:03<00:02,  3.51it/s]evaluate for the 13-th batch, evaluate loss: 0.2269870638847351:  62%|███████████▏      | 13/21 [00:03<00:02,  3.52it/s]evaluate for the 3-th batch, evaluate loss: 0.5974147915840149:  10%|██                  | 2/20 [00:00<00:05,  3.47it/s]evaluate for the 3-th batch, evaluate loss: 0.5974147915840149:  15%|███                 | 3/20 [00:00<00:04,  3.50it/s]Epoch: 12, train for the 114-th batch, train loss: 0.5165886282920837:  75%|███████▍  | 113/151 [00:24<00:08,  4.47it/s]Epoch: 12, train for the 114-th batch, train loss: 0.5165886282920837:  75%|███████▌  | 114/151 [00:24<00:08,  4.46it/s]Epoch: 3, train for the 93-th batch, train loss: 0.3753654658794403:  24%|███          | 92/383 [00:55<02:45,  1.75it/s]Epoch: 3, train for the 93-th batch, train loss: 0.3753654658794403:  24%|███▏         | 93/383 [00:55<02:25,  1.99it/s]evaluate for the 14-th batch, evaluate loss: 0.1991824358701706:  62%|███████████▏      | 13/21 [00:03<00:02,  3.52it/s]evaluate for the 14-th batch, evaluate loss: 0.1991824358701706:  67%|████████████      | 14/21 [00:03<00:01,  3.53it/s]evaluate for the 4-th batch, evaluate loss: 0.5811508297920227:  15%|███                 | 3/20 [00:01<00:04,  3.50it/s]evaluate for the 4-th batch, evaluate loss: 0.5811508297920227:  20%|████                | 4/20 [00:01<00:04,  3.52it/s]Epoch: 12, train for the 115-th batch, train loss: 0.5033007860183716:  75%|███████▌  | 114/151 [00:25<00:08,  4.46it/s]Epoch: 12, train for the 115-th batch, train loss: 0.5033007860183716:  76%|███████▌  | 115/151 [00:25<00:08,  4.47it/s]Epoch: 3, train for the 94-th batch, train loss: 0.3612197935581207:  24%|███▏         | 93/383 [00:55<02:25,  1.99it/s]Epoch: 3, train for the 94-th batch, train loss: 0.3612197935581207:  25%|███▏         | 94/383 [00:55<02:00,  2.39it/s]Epoch: 4, train for the 189-th batch, train loss: 0.6651769876480103:  79%|████████▋  | 188/237 [01:50<00:31,  1.55it/s]Epoch: 4, train for the 189-th batch, train loss: 0.6651769876480103:  80%|████████▊  | 189/237 [01:50<00:37,  1.29it/s]Epoch: 12, train for the 116-th batch, train loss: 0.4900582432746887:  76%|███████▌  | 115/151 [00:25<00:08,  4.47it/s]Epoch: 12, train for the 116-th batch, train loss: 0.4900582432746887:  77%|███████▋  | 116/151 [00:25<00:07,  4.47it/s]evaluate for the 15-th batch, evaluate loss: 0.19304980337619781:  67%|███████████▎     | 14/21 [00:04<00:01,  3.53it/s]evaluate for the 15-th batch, evaluate loss: 0.19304980337619781:  71%|████████████▏    | 15/21 [00:04<00:01,  3.53it/s]evaluate for the 5-th batch, evaluate loss: 0.600157618522644:  20%|████▏                | 4/20 [00:01<00:04,  3.52it/s]evaluate for the 5-th batch, evaluate loss: 0.600157618522644:  25%|█████▎               | 5/20 [00:01<00:04,  3.52it/s]Epoch: 12, train for the 117-th batch, train loss: 0.51005619764328:  77%|█████████▏  | 116/151 [00:25<00:07,  4.47it/s]Epoch: 12, train for the 117-th batch, train loss: 0.51005619764328:  77%|█████████▎  | 117/151 [00:25<00:07,  4.48it/s]evaluate for the 16-th batch, evaluate loss: 0.23585891723632812:  71%|████████████▏    | 15/21 [00:04<00:01,  3.53it/s]evaluate for the 16-th batch, evaluate loss: 0.23585891723632812:  76%|████████████▉    | 16/21 [00:04<00:01,  3.53it/s]evaluate for the 6-th batch, evaluate loss: 0.6341634392738342:  25%|█████               | 5/20 [00:01<00:04,  3.52it/s]evaluate for the 6-th batch, evaluate loss: 0.6341634392738342:  30%|██████              | 6/20 [00:01<00:03,  3.53it/s]Epoch: 3, train for the 95-th batch, train loss: 0.3801933228969574:  25%|███▏         | 94/383 [00:56<02:00,  2.39it/s]Epoch: 3, train for the 95-th batch, train loss: 0.3801933228969574:  25%|███▏         | 95/383 [00:56<02:11,  2.19it/s]Epoch: 12, train for the 118-th batch, train loss: 0.4633287787437439:  77%|███████▋  | 117/151 [00:25<00:07,  4.48it/s]Epoch: 12, train for the 118-th batch, train loss: 0.4633287787437439:  78%|███████▊  | 118/151 [00:25<00:07,  4.46it/s]Epoch: 4, train for the 190-th batch, train loss: 0.6245269775390625:  80%|████████▊  | 189/237 [01:50<00:37,  1.29it/s]Epoch: 4, train for the 190-th batch, train loss: 0.6245269775390625:  80%|████████▊  | 190/237 [01:50<00:33,  1.39it/s]evaluate for the 17-th batch, evaluate loss: 0.22901085019111633:  76%|████████████▉    | 16/21 [00:04<00:01,  3.53it/s]evaluate for the 17-th batch, evaluate loss: 0.22901085019111633:  81%|█████████████▊   | 17/21 [00:04<00:01,  3.54it/s]evaluate for the 7-th batch, evaluate loss: 0.6841528415679932:  30%|██████              | 6/20 [00:01<00:03,  3.53it/s]evaluate for the 7-th batch, evaluate loss: 0.6841528415679932:  35%|███████             | 7/20 [00:01<00:03,  3.53it/s]Epoch: 12, train for the 119-th batch, train loss: 0.5182705521583557:  78%|███████▊  | 118/151 [00:26<00:07,  4.46it/s]Epoch: 12, train for the 119-th batch, train loss: 0.5182705521583557:  79%|███████▉  | 119/151 [00:26<00:07,  4.48it/s]Epoch: 12, train for the 120-th batch, train loss: 0.5877889394760132:  79%|███████▉  | 119/151 [00:26<00:07,  4.48it/s]Epoch: 12, train for the 120-th batch, train loss: 0.5877889394760132:  79%|███████▉  | 120/151 [00:26<00:06,  4.49it/s]evaluate for the 18-th batch, evaluate loss: 0.20877549052238464:  81%|█████████████▊   | 17/21 [00:05<00:01,  3.54it/s]evaluate for the 18-th batch, evaluate loss: 0.20877549052238464:  86%|██████████████▌  | 18/21 [00:05<00:00,  3.53it/s]evaluate for the 8-th batch, evaluate loss: 0.6694474816322327:  35%|███████             | 7/20 [00:02<00:03,  3.53it/s]evaluate for the 8-th batch, evaluate loss: 0.6694474816322327:  40%|████████            | 8/20 [00:02<00:03,  3.53it/s]Epoch: 3, train for the 96-th batch, train loss: 0.34247100353240967:  25%|██▉         | 95/383 [00:57<02:11,  2.19it/s]Epoch: 3, train for the 96-th batch, train loss: 0.34247100353240967:  25%|███         | 96/383 [00:57<02:24,  1.98it/s]Epoch: 12, train for the 121-th batch, train loss: 0.49307578802108765:  79%|███████▏ | 120/151 [00:26<00:06,  4.49it/s]Epoch: 12, train for the 121-th batch, train loss: 0.49307578802108765:  80%|███████▏ | 121/151 [00:26<00:06,  4.49it/s]Epoch: 4, train for the 191-th batch, train loss: 0.6362656950950623:  80%|████████▊  | 190/237 [01:51<00:33,  1.39it/s]Epoch: 4, train for the 191-th batch, train loss: 0.6362656950950623:  81%|████████▊  | 191/237 [01:51<00:31,  1.46it/s]evaluate for the 19-th batch, evaluate loss: 0.27112290263175964:  86%|██████████████▌  | 18/21 [00:05<00:00,  3.53it/s]evaluate for the 19-th batch, evaluate loss: 0.27112290263175964:  90%|███████████████▍ | 19/21 [00:05<00:00,  3.54it/s]evaluate for the 9-th batch, evaluate loss: 0.6051931977272034:  40%|████████            | 8/20 [00:02<00:03,  3.53it/s]evaluate for the 9-th batch, evaluate loss: 0.6051931977272034:  45%|█████████           | 9/20 [00:02<00:03,  3.54it/s]Epoch: 12, train for the 122-th batch, train loss: 0.5602508187294006:  80%|████████  | 121/151 [00:26<00:06,  4.49it/s]Epoch: 12, train for the 122-th batch, train loss: 0.5602508187294006:  81%|████████  | 122/151 [00:26<00:06,  4.48it/s]evaluate for the 20-th batch, evaluate loss: 0.24714279174804688:  90%|███████████████▍ | 19/21 [00:05<00:00,  3.54it/s]evaluate for the 20-th batch, evaluate loss: 0.24714279174804688:  95%|████████████████▏| 20/21 [00:05<00:00,  3.54it/s]evaluate for the 10-th batch, evaluate loss: 0.6143931746482849:  45%|████████▌          | 9/20 [00:02<00:03,  3.54it/s]evaluate for the 10-th batch, evaluate loss: 0.6143931746482849:  50%|█████████         | 10/20 [00:02<00:02,  3.53it/s]evaluate for the 21-th batch, evaluate loss: 0.1268717348575592:  95%|█████████████████▏| 20/21 [00:05<00:00,  3.54it/s]evaluate for the 21-th batch, evaluate loss: 0.1268717348575592: 100%|██████████████████| 21/21 [00:05<00:00,  3.69it/s]
INFO:root:Epoch: 7, learning rate: 0.0001, train loss: 0.2085
INFO:root:train average_precision, 0.9720
INFO:root:train roc_auc, 0.9628
INFO:root:validate loss: 0.1501
INFO:root:validate average_precision, 0.9863
INFO:root:validate roc_auc, 0.9838
INFO:root:new node validate loss: 0.2251
INFO:root:new node validate first_1_average_precision, 0.9164
INFO:root:new node validate first_1_roc_auc, 0.9173
INFO:root:new node validate first_3_average_precision, 0.9532
INFO:root:new node validate first_3_roc_auc, 0.9529
INFO:root:new node validate first_10_average_precision, 0.9693
INFO:root:new node validate first_10_roc_auc, 0.9679
INFO:root:new node validate average_precision, 0.9697
INFO:root:new node validate roc_auc, 0.9662
INFO:root:save model ./saved_models/DyGFormer/ia-reality-call/DyGFormer_seed0_dygformer-ia-reality-call-old/DyGFormer_seed0_dygformer-ia-reality-call-old.pkl
Epoch: 12, train for the 123-th batch, train loss: 0.5333255529403687:  81%|████████  | 122/151 [00:26<00:06,  4.48it/s]Epoch: 12, train for the 123-th batch, train loss: 0.5333255529403687:  81%|████████▏ | 123/151 [00:26<00:06,  4.46it/s]evaluate for the 11-th batch, evaluate loss: 0.6162693500518799:  50%|█████████         | 10/20 [00:02<00:02,  3.53it/s]evaluate for the 11-th batch, evaluate loss: 0.6162693500518799:  55%|█████████▉        | 11/20 [00:02<00:02,  4.33it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]Epoch: 3, train for the 97-th batch, train loss: 0.3054582178592682:  25%|███▎         | 96/383 [00:57<02:24,  1.98it/s]Epoch: 3, train for the 97-th batch, train loss: 0.3054582178592682:  25%|███▎         | 97/383 [00:57<02:33,  1.86it/s]Epoch: 4, train for the 192-th batch, train loss: 0.6451076865196228:  81%|████████▊  | 191/237 [01:52<00:31,  1.46it/s]Epoch: 4, train for the 192-th batch, train loss: 0.6451076865196228:  81%|████████▉  | 192/237 [01:52<00:29,  1.50it/s]Epoch: 12, train for the 124-th batch, train loss: 0.517497718334198:  81%|████████▉  | 123/151 [00:27<00:06,  4.46it/s]Epoch: 12, train for the 124-th batch, train loss: 0.517497718334198:  82%|█████████  | 124/151 [00:27<00:06,  4.47it/s]evaluate for the 12-th batch, evaluate loss: 0.6397212147712708:  55%|█████████▉        | 11/20 [00:03<00:02,  4.33it/s]evaluate for the 12-th batch, evaluate loss: 0.6397212147712708:  60%|██████████▊       | 12/20 [00:03<00:01,  4.27it/s]Epoch: 12, train for the 125-th batch, train loss: 0.5505995154380798:  82%|████████▏ | 124/151 [00:27<00:06,  4.47it/s]Epoch: 12, train for the 125-th batch, train loss: 0.5505995154380798:  83%|████████▎ | 125/151 [00:27<00:05,  4.48it/s]Epoch: 8, train for the 1-th batch, train loss: 0.9955790638923645:   0%|                       | 0/119 [00:00<?, ?it/s]Epoch: 8, train for the 1-th batch, train loss: 0.9955790638923645:   1%|▏              | 1/119 [00:00<01:00,  1.95it/s]evaluate for the 13-th batch, evaluate loss: 0.6489280462265015:  60%|██████████▊       | 12/20 [00:03<00:01,  4.27it/s]evaluate for the 13-th batch, evaluate loss: 0.6489280462265015:  65%|███████████▋      | 13/20 [00:03<00:01,  3.98it/s]Epoch: 3, train for the 98-th batch, train loss: 0.2883470356464386:  25%|███▎         | 97/383 [00:58<02:33,  1.86it/s]Epoch: 3, train for the 98-th batch, train loss: 0.2883470356464386:  26%|███▎         | 98/383 [00:58<02:39,  1.79it/s]Epoch: 12, train for the 126-th batch, train loss: 0.5547481179237366:  83%|████████▎ | 125/151 [00:27<00:05,  4.48it/s]Epoch: 12, train for the 126-th batch, train loss: 0.5547481179237366:  83%|████████▎ | 126/151 [00:27<00:05,  4.47it/s]Epoch: 4, train for the 193-th batch, train loss: 0.6319414377212524:  81%|████████▉  | 192/237 [01:52<00:29,  1.50it/s]Epoch: 4, train for the 193-th batch, train loss: 0.6319414377212524:  81%|████████▉  | 193/237 [01:52<00:28,  1.54it/s]evaluate for the 14-th batch, evaluate loss: 0.6412807106971741:  65%|███████████▋      | 13/20 [00:03<00:01,  3.98it/s]evaluate for the 14-th batch, evaluate loss: 0.6412807106971741:  70%|████████████▌     | 14/20 [00:03<00:01,  3.86it/s]Epoch: 12, train for the 127-th batch, train loss: 0.5258643627166748:  83%|████████▎ | 126/151 [00:27<00:05,  4.47it/s]Epoch: 12, train for the 127-th batch, train loss: 0.5258643627166748:  84%|████████▍ | 127/151 [00:27<00:05,  4.48it/s]Epoch: 12, train for the 128-th batch, train loss: 0.5764032006263733:  84%|████████▍ | 127/151 [00:28<00:05,  4.48it/s]Epoch: 12, train for the 128-th batch, train loss: 0.5764032006263733:  85%|████████▍ | 128/151 [00:28<00:05,  4.49it/s]Epoch: 8, train for the 2-th batch, train loss: 0.5258904099464417:   1%|▏              | 1/119 [00:01<01:00,  1.95it/s]Epoch: 8, train for the 2-th batch, train loss: 0.5258904099464417:   2%|▎              | 2/119 [00:01<01:04,  1.81it/s]evaluate for the 15-th batch, evaluate loss: 0.6312485337257385:  70%|████████████▌     | 14/20 [00:04<00:01,  3.86it/s]evaluate for the 15-th batch, evaluate loss: 0.6312485337257385:  75%|█████████████▌    | 15/20 [00:04<00:01,  3.64it/s]Epoch: 3, train for the 99-th batch, train loss: 0.31616395711898804:  26%|███         | 98/383 [00:58<02:39,  1.79it/s]Epoch: 3, train for the 99-th batch, train loss: 0.31616395711898804:  26%|███         | 99/383 [00:58<02:43,  1.74it/s]Epoch: 12, train for the 129-th batch, train loss: 0.5842452645301819:  85%|████████▍ | 128/151 [00:28<00:05,  4.49it/s]Epoch: 12, train for the 129-th batch, train loss: 0.5842452645301819:  85%|████████▌ | 129/151 [00:28<00:04,  4.48it/s]evaluate for the 16-th batch, evaluate loss: 0.6059414744377136:  75%|█████████████▌    | 15/20 [00:04<00:01,  3.64it/s]evaluate for the 16-th batch, evaluate loss: 0.6059414744377136:  80%|██████████████▍   | 16/20 [00:04<00:01,  3.66it/s]Epoch: 4, train for the 194-th batch, train loss: 0.6312371492385864:  81%|████████▉  | 193/237 [01:53<00:28,  1.54it/s]Epoch: 4, train for the 194-th batch, train loss: 0.6312371492385864:  82%|█████████  | 194/237 [01:53<00:27,  1.57it/s]Epoch: 12, train for the 130-th batch, train loss: 0.526995062828064:  85%|█████████▍ | 129/151 [00:28<00:04,  4.48it/s]Epoch: 12, train for the 130-th batch, train loss: 0.526995062828064:  86%|█████████▍ | 130/151 [00:28<00:04,  4.47it/s]Epoch: 8, train for the 3-th batch, train loss: 0.3332120478153229:   2%|▎              | 2/119 [00:01<01:04,  1.81it/s]Epoch: 8, train for the 3-th batch, train loss: 0.3332120478153229:   3%|▍              | 3/119 [00:01<01:06,  1.75it/s]evaluate for the 17-th batch, evaluate loss: 0.6071982979774475:  80%|██████████████▍   | 16/20 [00:04<00:01,  3.66it/s]evaluate for the 17-th batch, evaluate loss: 0.6071982979774475:  85%|███████████████▎  | 17/20 [00:04<00:00,  3.48it/s]Epoch: 12, train for the 131-th batch, train loss: 0.5135274529457092:  86%|████████▌ | 130/151 [00:28<00:04,  4.47it/s]Epoch: 12, train for the 131-th batch, train loss: 0.5135274529457092:  87%|████████▋ | 131/151 [00:28<00:04,  4.46it/s]Epoch: 3, train for the 100-th batch, train loss: 0.3892597258090973:  26%|███         | 99/383 [00:59<02:43,  1.74it/s]Epoch: 3, train for the 100-th batch, train loss: 0.3892597258090973:  26%|██▊        | 100/383 [00:59<02:44,  1.72it/s]evaluate for the 18-th batch, evaluate loss: 0.6812684535980225:  85%|███████████████▎  | 17/20 [00:04<00:00,  3.48it/s]evaluate for the 18-th batch, evaluate loss: 0.6812684535980225:  90%|████████████████▏ | 18/20 [00:04<00:00,  3.52it/s]Epoch: 12, train for the 132-th batch, train loss: 0.5363124012947083:  87%|████████▋ | 131/151 [00:28<00:04,  4.46it/s]Epoch: 12, train for the 132-th batch, train loss: 0.5363124012947083:  87%|████████▋ | 132/151 [00:28<00:04,  4.46it/s]Epoch: 4, train for the 195-th batch, train loss: 0.6163548231124878:  82%|█████████  | 194/237 [01:53<00:27,  1.57it/s]Epoch: 4, train for the 195-th batch, train loss: 0.6163548231124878:  82%|█████████  | 195/237 [01:53<00:26,  1.59it/s]Epoch: 12, train for the 133-th batch, train loss: 0.5532515048980713:  87%|████████▋ | 132/151 [00:29<00:04,  4.46it/s]Epoch: 12, train for the 133-th batch, train loss: 0.5532515048980713:  88%|████████▊ | 133/151 [00:29<00:04,  4.45it/s]Epoch: 8, train for the 4-th batch, train loss: 0.28969305753707886:   3%|▎             | 3/119 [00:02<01:06,  1.75it/s]Epoch: 8, train for the 4-th batch, train loss: 0.28969305753707886:   3%|▍             | 4/119 [00:02<01:06,  1.72it/s]evaluate for the 19-th batch, evaluate loss: 0.6811218857765198:  90%|████████████████▏ | 18/20 [00:05<00:00,  3.52it/s]evaluate for the 19-th batch, evaluate loss: 0.6811218857765198:  95%|█████████████████ | 19/20 [00:05<00:00,  3.41it/s]evaluate for the 20-th batch, evaluate loss: 0.6478332281112671:  95%|█████████████████ | 19/20 [00:05<00:00,  3.41it/s]evaluate for the 20-th batch, evaluate loss: 0.6478332281112671: 100%|██████████████████| 20/20 [00:05<00:00,  3.75it/s]
INFO:root:Epoch: 6, learning rate: 0.0001, train loss: 0.5006
INFO:root:train average_precision, 0.8529
INFO:root:train roc_auc, 0.8277
INFO:root:validate loss: 0.5090
INFO:root:validate average_precision, 0.8448
INFO:root:validate roc_auc, 0.8114
INFO:root:new node validate loss: 0.6293
INFO:root:new node validate first_1_average_precision, 0.6464
INFO:root:new node validate first_1_roc_auc, 0.5606
INFO:root:new node validate first_3_average_precision, 0.7016
INFO:root:new node validate first_3_roc_auc, 0.6281
INFO:root:new node validate first_10_average_precision, 0.7456
INFO:root:new node validate first_10_roc_auc, 0.6896
INFO:root:new node validate average_precision, 0.7534
INFO:root:new node validate roc_auc, 0.7122
INFO:root:save model ./saved_models/DyGFormer/ia-escorts-dynamic/DyGFormer_seed0_dygformer-ia-escorts-dynamic-old/DyGFormer_seed0_dygformer-ia-escorts-dynamic-old.pkl
Epoch: 12, train for the 134-th batch, train loss: 0.5490556955337524:  88%|████████▊ | 133/151 [00:29<00:04,  4.45it/s]Epoch: 12, train for the 134-th batch, train loss: 0.5490556955337524:  89%|████████▊ | 134/151 [00:29<00:03,  4.44it/s]Epoch: 3, train for the 101-th batch, train loss: 0.35850462317466736:  26%|██▌       | 100/383 [01:00<02:44,  1.72it/s]Epoch: 3, train for the 101-th batch, train loss: 0.35850462317466736:  26%|██▋       | 101/383 [01:00<02:47,  1.69it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]Epoch: 4, train for the 196-th batch, train loss: 0.6347596049308777:  82%|█████████  | 195/237 [01:54<00:26,  1.59it/s]Epoch: 4, train for the 196-th batch, train loss: 0.6347596049308777:  83%|█████████  | 196/237 [01:54<00:25,  1.60it/s]Epoch: 12, train for the 135-th batch, train loss: 0.5584619045257568:  89%|████████▊ | 134/151 [00:29<00:03,  4.44it/s]Epoch: 12, train for the 135-th batch, train loss: 0.5584619045257568:  89%|████████▉ | 135/151 [00:29<00:03,  4.45it/s]Epoch: 8, train for the 5-th batch, train loss: 0.3336181640625:   3%|▌                 | 4/119 [00:02<01:06,  1.72it/s]Epoch: 8, train for the 5-th batch, train loss: 0.3336181640625:   4%|▊                 | 5/119 [00:02<01:01,  1.86it/s]Epoch: 12, train for the 136-th batch, train loss: 0.5664680600166321:  89%|████████▉ | 135/151 [00:29<00:03,  4.45it/s]Epoch: 12, train for the 136-th batch, train loss: 0.5664680600166321:  90%|█████████ | 136/151 [00:29<00:03,  4.45it/s]Epoch: 7, train for the 1-th batch, train loss: 0.743110716342926:   0%|                        | 0/146 [00:00<?, ?it/s]Epoch: 7, train for the 1-th batch, train loss: 0.743110716342926:   1%|                | 1/146 [00:00<01:07,  2.16it/s]Epoch: 3, train for the 102-th batch, train loss: 0.3694896101951599:  26%|██▉        | 101/383 [01:00<02:47,  1.69it/s]Epoch: 3, train for the 102-th batch, train loss: 0.3694896101951599:  27%|██▉        | 102/383 [01:00<02:46,  1.68it/s]Epoch: 12, train for the 137-th batch, train loss: 0.5750863552093506:  90%|█████████ | 136/151 [00:30<00:03,  4.45it/s]Epoch: 12, train for the 137-th batch, train loss: 0.5750863552093506:  91%|█████████ | 137/151 [00:30<00:03,  4.47it/s]Epoch: 4, train for the 197-th batch, train loss: 0.6426783800125122:  83%|█████████  | 196/237 [01:55<00:25,  1.60it/s]Epoch: 4, train for the 197-th batch, train loss: 0.6426783800125122:  83%|█████████▏ | 197/237 [01:55<00:24,  1.62it/s]Epoch: 8, train for the 6-th batch, train loss: 0.2767283320426941:   4%|▋              | 5/119 [00:03<01:01,  1.86it/s]Epoch: 8, train for the 6-th batch, train loss: 0.2767283320426941:   5%|▊              | 6/119 [00:03<01:01,  1.82it/s]Epoch: 12, train for the 138-th batch, train loss: 0.6045348048210144:  91%|█████████ | 137/151 [00:30<00:03,  4.47it/s]Epoch: 12, train for the 138-th batch, train loss: 0.6045348048210144:  91%|█████████▏| 138/151 [00:30<00:02,  4.46it/s]Epoch: 7, train for the 2-th batch, train loss: 0.4970737099647522:   1%|               | 1/146 [00:00<01:07,  2.16it/s]Epoch: 7, train for the 2-th batch, train loss: 0.4970737099647522:   1%|▏              | 2/146 [00:00<01:10,  2.03it/s]Epoch: 12, train for the 139-th batch, train loss: 0.5408939123153687:  91%|█████████▏| 138/151 [00:30<00:02,  4.46it/s]Epoch: 12, train for the 139-th batch, train loss: 0.5408939123153687:  92%|█████████▏| 139/151 [00:30<00:02,  4.35it/s]Epoch: 3, train for the 103-th batch, train loss: 0.3290007710456848:  27%|██▉        | 102/383 [01:01<02:46,  1.68it/s]Epoch: 3, train for the 103-th batch, train loss: 0.3290007710456848:  27%|██▉        | 103/383 [01:01<02:49,  1.66it/s]Epoch: 8, train for the 7-th batch, train loss: 0.26098528504371643:   5%|▋             | 6/119 [00:03<01:01,  1.82it/s]Epoch: 8, train for the 7-th batch, train loss: 0.26098528504371643:   6%|▊             | 7/119 [00:03<00:57,  1.94it/s]Epoch: 12, train for the 140-th batch, train loss: 0.5182278156280518:  92%|█████████▏| 139/151 [00:30<00:02,  4.35it/s]Epoch: 12, train for the 140-th batch, train loss: 0.5182278156280518:  93%|█████████▎| 140/151 [00:30<00:02,  4.38it/s]Epoch: 4, train for the 198-th batch, train loss: 0.6341667771339417:  83%|█████████▏ | 197/237 [01:55<00:24,  1.62it/s]Epoch: 4, train for the 198-th batch, train loss: 0.6341667771339417:  84%|█████████▏ | 198/237 [01:55<00:24,  1.62it/s]Epoch: 12, train for the 141-th batch, train loss: 0.5669727325439453:  93%|█████████▎| 140/151 [00:30<00:02,  4.38it/s]Epoch: 12, train for the 141-th batch, train loss: 0.5669727325439453:  93%|█████████▎| 141/151 [00:30<00:02,  4.42it/s]Epoch: 7, train for the 3-th batch, train loss: 0.3780537247657776:   1%|▏              | 2/146 [00:01<01:10,  2.03it/s]Epoch: 7, train for the 3-th batch, train loss: 0.3780537247657776:   2%|▎              | 3/146 [00:01<01:17,  1.85it/s]Epoch: 12, train for the 142-th batch, train loss: 0.5625630617141724:  93%|█████████▎| 141/151 [00:31<00:02,  4.42it/s]Epoch: 12, train for the 142-th batch, train loss: 0.5625630617141724:  94%|█████████▍| 142/151 [00:31<00:02,  4.44it/s]Epoch: 3, train for the 104-th batch, train loss: 0.33669859170913696:  27%|██▋       | 103/383 [01:01<02:49,  1.66it/s]Epoch: 3, train for the 104-th batch, train loss: 0.33669859170913696:  27%|██▋       | 104/383 [01:01<02:49,  1.65it/s]Epoch: 8, train for the 8-th batch, train loss: 0.20235978066921234:   6%|▊             | 7/119 [00:04<00:57,  1.94it/s]Epoch: 8, train for the 8-th batch, train loss: 0.20235978066921234:   7%|▉             | 8/119 [00:04<01:00,  1.84it/s]Epoch: 4, train for the 199-th batch, train loss: 0.6145015358924866:  84%|█████████▏ | 198/237 [01:56<00:24,  1.62it/s]Epoch: 4, train for the 199-th batch, train loss: 0.6145015358924866:  84%|█████████▏ | 199/237 [01:56<00:23,  1.62it/s]Epoch: 12, train for the 143-th batch, train loss: 0.49495798349380493:  94%|████████▍| 142/151 [00:31<00:02,  4.44it/s]Epoch: 12, train for the 143-th batch, train loss: 0.49495798349380493:  95%|████████▌| 143/151 [00:31<00:01,  4.44it/s]Epoch: 7, train for the 4-th batch, train loss: 0.3339173495769501:   2%|▎              | 3/146 [00:02<01:17,  1.85it/s]Epoch: 7, train for the 4-th batch, train loss: 0.3339173495769501:   3%|▍              | 4/146 [00:02<01:18,  1.81it/s]Epoch: 12, train for the 144-th batch, train loss: 0.49606555700302124:  95%|████████▌| 143/151 [00:31<00:01,  4.44it/s]Epoch: 12, train for the 144-th batch, train loss: 0.49606555700302124:  95%|████████▌| 144/151 [00:31<00:01,  4.44it/s]Epoch: 3, train for the 105-th batch, train loss: 0.3058803379535675:  27%|██▉        | 104/383 [01:02<02:49,  1.65it/s]Epoch: 3, train for the 105-th batch, train loss: 0.3058803379535675:  27%|███        | 105/383 [01:02<02:49,  1.64it/s]Epoch: 12, train for the 145-th batch, train loss: 0.5509737133979797:  95%|█████████▌| 144/151 [00:31<00:01,  4.44it/s]Epoch: 12, train for the 145-th batch, train loss: 0.5509737133979797:  96%|█████████▌| 145/151 [00:31<00:01,  4.44it/s]Epoch: 8, train for the 9-th batch, train loss: 0.24677173793315887:   7%|▉             | 8/119 [00:04<01:00,  1.84it/s]Epoch: 8, train for the 9-th batch, train loss: 0.24677173793315887:   8%|█             | 9/119 [00:04<01:00,  1.81it/s]Epoch: 4, train for the 200-th batch, train loss: 0.6304248571395874:  84%|█████████▏ | 199/237 [01:57<00:23,  1.62it/s]Epoch: 4, train for the 200-th batch, train loss: 0.6304248571395874:  84%|█████████▎ | 200/237 [01:57<00:22,  1.62it/s]Epoch: 12, train for the 146-th batch, train loss: 0.5290302634239197:  96%|█████████▌| 145/151 [00:32<00:01,  4.44it/s]Epoch: 12, train for the 146-th batch, train loss: 0.5290302634239197:  97%|█████████▋| 146/151 [00:32<00:01,  4.45it/s]Epoch: 7, train for the 5-th batch, train loss: 0.39232000708580017:   3%|▍             | 4/146 [00:02<01:18,  1.81it/s]Epoch: 7, train for the 5-th batch, train loss: 0.39232000708580017:   3%|▍             | 5/146 [00:02<01:19,  1.78it/s]Epoch: 12, train for the 147-th batch, train loss: 0.549200713634491:  97%|██████████▋| 146/151 [00:32<00:01,  4.45it/s]Epoch: 12, train for the 147-th batch, train loss: 0.549200713634491:  97%|██████████▋| 147/151 [00:32<00:00,  4.01it/s]Epoch: 8, train for the 10-th batch, train loss: 0.24721242487430573:   8%|▉            | 9/119 [00:05<01:00,  1.81it/s]Epoch: 8, train for the 10-th batch, train loss: 0.24721242487430573:   8%|█           | 10/119 [00:05<01:01,  1.79it/s]Epoch: 4, train for the 201-th batch, train loss: 0.6359235048294067:  84%|█████████▎ | 200/237 [01:57<00:22,  1.62it/s]Epoch: 4, train for the 201-th batch, train loss: 0.6359235048294067:  85%|█████████▎ | 201/237 [01:57<00:20,  1.74it/s]Epoch: 3, train for the 106-th batch, train loss: 0.3288728892803192:  27%|███        | 105/383 [01:03<02:49,  1.64it/s]Epoch: 3, train for the 106-th batch, train loss: 0.3288728892803192:  28%|███        | 106/383 [01:03<02:58,  1.55it/s]Epoch: 12, train for the 148-th batch, train loss: 0.5624958872795105:  97%|█████████▋| 147/151 [00:32<00:00,  4.01it/s]Epoch: 12, train for the 148-th batch, train loss: 0.5624958872795105:  98%|█████████▊| 148/151 [00:32<00:00,  4.14it/s]Epoch: 7, train for the 6-th batch, train loss: 0.46234557032585144:   3%|▍             | 5/146 [00:03<01:19,  1.78it/s]Epoch: 7, train for the 6-th batch, train loss: 0.46234557032585144:   4%|▌             | 6/146 [00:03<01:19,  1.77it/s]Epoch: 12, train for the 149-th batch, train loss: 0.4977191388607025:  98%|█████████▊| 148/151 [00:32<00:00,  4.14it/s]Epoch: 12, train for the 149-th batch, train loss: 0.4977191388607025:  99%|█████████▊| 149/151 [00:32<00:00,  4.21it/s]Epoch: 8, train for the 11-th batch, train loss: 0.1969296932220459:   8%|█            | 10/119 [00:06<01:01,  1.79it/s]Epoch: 8, train for the 11-th batch, train loss: 0.1969296932220459:   9%|█▏           | 11/119 [00:06<01:00,  1.77it/s]Epoch: 12, train for the 150-th batch, train loss: 0.5227358937263489:  99%|█████████▊| 149/151 [00:33<00:00,  4.21it/s]Epoch: 12, train for the 150-th batch, train loss: 0.5227358937263489:  99%|█████████▉| 150/151 [00:33<00:00,  4.27it/s]Epoch: 4, train for the 202-th batch, train loss: 0.6099909543991089:  85%|█████████▎ | 201/237 [01:58<00:20,  1.74it/s]Epoch: 4, train for the 202-th batch, train loss: 0.6099909543991089:  85%|█████████▍ | 202/237 [01:58<00:20,  1.74it/s]Epoch: 3, train for the 107-th batch, train loss: 0.3928567171096802:  28%|███        | 106/383 [01:03<02:58,  1.55it/s]Epoch: 3, train for the 107-th batch, train loss: 0.3928567171096802:  28%|███        | 107/383 [01:03<02:54,  1.59it/s]Epoch: 12, train for the 151-th batch, train loss: 0.5851081609725952:  99%|█████████▉| 150/151 [00:33<00:00,  4.27it/s]Epoch: 12, train for the 151-th batch, train loss: 0.5851081609725952: 100%|██████████| 151/151 [00:33<00:00,  4.74it/s]Epoch: 12, train for the 151-th batch, train loss: 0.5851081609725952: 100%|██████████| 151/151 [00:33<00:00,  4.54it/s]
  0%|                                                                                            | 0/46 [00:00<?, ?it/s]Epoch: 7, train for the 7-th batch, train loss: 0.5280249714851379:   4%|▌              | 6/146 [00:03<01:19,  1.77it/s]Epoch: 7, train for the 7-th batch, train loss: 0.5280249714851379:   5%|▋              | 7/146 [00:03<01:19,  1.75it/s]evaluate for the 1-th batch, evaluate loss: 0.49168768525123596:   0%|                           | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.49168768525123596:   2%|▍                  | 1/46 [00:00<00:04,  9.65it/s]evaluate for the 2-th batch, evaluate loss: 0.5025251507759094:   2%|▍                   | 1/46 [00:00<00:04,  9.65it/s]evaluate for the 2-th batch, evaluate loss: 0.5025251507759094:   4%|▊                   | 2/46 [00:00<00:04,  9.62it/s]evaluate for the 3-th batch, evaluate loss: 0.4810297191143036:   4%|▊                   | 2/46 [00:00<00:04,  9.62it/s]evaluate for the 3-th batch, evaluate loss: 0.4810297191143036:   7%|█▎                  | 3/46 [00:00<00:04,  9.63it/s]Epoch: 8, train for the 12-th batch, train loss: 0.2703700661659241:   9%|█▏           | 11/119 [00:06<01:00,  1.77it/s]Epoch: 8, train for the 12-th batch, train loss: 0.2703700661659241:  10%|█▎           | 12/119 [00:06<01:01,  1.75it/s]evaluate for the 4-th batch, evaluate loss: 0.5067736506462097:   7%|█▎                  | 3/46 [00:00<00:04,  9.63it/s]evaluate for the 4-th batch, evaluate loss: 0.5067736506462097:   9%|█▋                  | 4/46 [00:00<00:04,  9.62it/s]Epoch: 4, train for the 203-th batch, train loss: 0.6340732574462891:  85%|█████████▍ | 202/237 [01:58<00:20,  1.74it/s]Epoch: 4, train for the 203-th batch, train loss: 0.6340732574462891:  86%|█████████▍ | 203/237 [01:58<00:19,  1.74it/s]evaluate for the 5-th batch, evaluate loss: 0.4777902066707611:   9%|█▋                  | 4/46 [00:00<00:04,  9.62it/s]evaluate for the 5-th batch, evaluate loss: 0.4777902066707611:  11%|██▏                 | 5/46 [00:00<00:04,  9.64it/s]Epoch: 3, train for the 108-th batch, train loss: 0.29886335134506226:  28%|██▊       | 107/383 [01:04<02:54,  1.59it/s]Epoch: 3, train for the 108-th batch, train loss: 0.29886335134506226:  28%|██▊       | 108/383 [01:04<02:50,  1.61it/s]evaluate for the 6-th batch, evaluate loss: 0.5476545691490173:  11%|██▏                 | 5/46 [00:00<00:04,  9.64it/s]evaluate for the 6-th batch, evaluate loss: 0.5476545691490173:  13%|██▌                 | 6/46 [00:00<00:04,  9.64it/s]Epoch: 7, train for the 8-th batch, train loss: 0.3951978087425232:   5%|▋              | 7/146 [00:04<01:19,  1.75it/s]Epoch: 7, train for the 8-th batch, train loss: 0.3951978087425232:   5%|▊              | 8/146 [00:04<01:18,  1.76it/s]evaluate for the 7-th batch, evaluate loss: 0.4720951020717621:  13%|██▌                 | 6/46 [00:00<00:04,  9.64it/s]evaluate for the 7-th batch, evaluate loss: 0.4720951020717621:  15%|███                 | 7/46 [00:00<00:04,  9.67it/s]evaluate for the 8-th batch, evaluate loss: 0.5584392547607422:  15%|███                 | 7/46 [00:00<00:04,  9.67it/s]evaluate for the 8-th batch, evaluate loss: 0.5584392547607422:  17%|███▍                | 8/46 [00:00<00:03,  9.67it/s]evaluate for the 9-th batch, evaluate loss: 0.5241955518722534:  17%|███▍                | 8/46 [00:00<00:03,  9.67it/s]evaluate for the 9-th batch, evaluate loss: 0.5241955518722534:  20%|███▉                | 9/46 [00:00<00:03,  9.62it/s]Epoch: 8, train for the 13-th batch, train loss: 0.2153615951538086:  10%|█▎           | 12/119 [00:07<01:01,  1.75it/s]Epoch: 8, train for the 13-th batch, train loss: 0.2153615951538086:  11%|█▍           | 13/119 [00:07<01:00,  1.76it/s]Epoch: 4, train for the 204-th batch, train loss: 0.6218656301498413:  86%|█████████▍ | 203/237 [01:59<00:19,  1.74it/s]Epoch: 4, train for the 204-th batch, train loss: 0.6218656301498413:  86%|█████████▍ | 204/237 [01:59<00:19,  1.73it/s]evaluate for the 10-th batch, evaluate loss: 0.5309624671936035:  20%|███▋               | 9/46 [00:01<00:03,  9.62it/s]evaluate for the 10-th batch, evaluate loss: 0.5309624671936035:  22%|███▉              | 10/46 [00:01<00:03,  9.63it/s]evaluate for the 11-th batch, evaluate loss: 0.5221104621887207:  22%|███▉              | 10/46 [00:01<00:03,  9.63it/s]evaluate for the 11-th batch, evaluate loss: 0.5221104621887207:  24%|████▎             | 11/46 [00:01<00:03,  9.65it/s]Epoch: 3, train for the 109-th batch, train loss: 0.34064990282058716:  28%|██▊       | 108/383 [01:05<02:50,  1.61it/s]Epoch: 3, train for the 109-th batch, train loss: 0.34064990282058716:  28%|██▊       | 109/383 [01:05<02:48,  1.63it/s]evaluate for the 12-th batch, evaluate loss: 0.4714491069316864:  24%|████▎             | 11/46 [00:01<00:03,  9.65it/s]evaluate for the 12-th batch, evaluate loss: 0.4714491069316864:  26%|████▋             | 12/46 [00:01<00:03,  9.66it/s]Epoch: 7, train for the 9-th batch, train loss: 0.3667535185813904:   5%|▊              | 8/146 [00:05<01:18,  1.76it/s]Epoch: 7, train for the 9-th batch, train loss: 0.3667535185813904:   6%|▉              | 9/146 [00:05<01:18,  1.73it/s]evaluate for the 13-th batch, evaluate loss: 0.49257680773735046:  26%|████▍            | 12/46 [00:01<00:03,  9.66it/s]evaluate for the 13-th batch, evaluate loss: 0.49257680773735046:  28%|████▊            | 13/46 [00:01<00:03,  9.65it/s]evaluate for the 14-th batch, evaluate loss: 0.5863156318664551:  28%|█████             | 13/46 [00:01<00:03,  9.65it/s]evaluate for the 14-th batch, evaluate loss: 0.5863156318664551:  30%|█████▍            | 14/46 [00:01<00:03,  9.65it/s]Epoch: 8, train for the 14-th batch, train loss: 0.23769177496433258:  11%|█▎          | 13/119 [00:07<01:00,  1.76it/s]Epoch: 8, train for the 14-th batch, train loss: 0.23769177496433258:  12%|█▍          | 14/119 [00:07<01:00,  1.74it/s]evaluate for the 15-th batch, evaluate loss: 0.5404362082481384:  30%|█████▍            | 14/46 [00:01<00:03,  9.65it/s]evaluate for the 15-th batch, evaluate loss: 0.5404362082481384:  33%|█████▊            | 15/46 [00:01<00:03,  9.65it/s]Epoch: 4, train for the 205-th batch, train loss: 0.620670735836029:  86%|██████████▎ | 204/237 [01:59<00:19,  1.73it/s]Epoch: 4, train for the 205-th batch, train loss: 0.620670735836029:  86%|██████████▍ | 205/237 [01:59<00:18,  1.71it/s]evaluate for the 16-th batch, evaluate loss: 0.5683602094650269:  33%|█████▊            | 15/46 [00:01<00:03,  9.65it/s]evaluate for the 16-th batch, evaluate loss: 0.5683602094650269:  35%|██████▎           | 16/46 [00:01<00:03,  9.68it/s]evaluate for the 17-th batch, evaluate loss: 0.44874662160873413:  35%|█████▉           | 16/46 [00:01<00:03,  9.68it/s]evaluate for the 17-th batch, evaluate loss: 0.44874662160873413:  37%|██████▎          | 17/46 [00:01<00:02,  9.69it/s]Epoch: 3, train for the 110-th batch, train loss: 0.3648509979248047:  28%|███▏       | 109/383 [01:05<02:48,  1.63it/s]Epoch: 3, train for the 110-th batch, train loss: 0.3648509979248047:  29%|███▏       | 110/383 [01:05<02:47,  1.63it/s]Epoch: 7, train for the 10-th batch, train loss: 0.43588024377822876:   6%|▊            | 9/146 [00:05<01:18,  1.73it/s]Epoch: 7, train for the 10-th batch, train loss: 0.43588024377822876:   7%|▊           | 10/146 [00:05<01:17,  1.76it/s]evaluate for the 18-th batch, evaluate loss: 0.50080806016922:  37%|███████▍            | 17/46 [00:01<00:02,  9.69it/s]evaluate for the 18-th batch, evaluate loss: 0.50080806016922:  39%|███████▊            | 18/46 [00:01<00:02,  9.71it/s]evaluate for the 19-th batch, evaluate loss: 0.5184543132781982:  39%|███████           | 18/46 [00:01<00:02,  9.71it/s]evaluate for the 19-th batch, evaluate loss: 0.5184543132781982:  41%|███████▍          | 19/46 [00:01<00:02,  9.72it/s]evaluate for the 20-th batch, evaluate loss: 0.5338712930679321:  41%|███████▍          | 19/46 [00:02<00:02,  9.72it/s]evaluate for the 20-th batch, evaluate loss: 0.5338712930679321:  43%|███████▊          | 20/46 [00:02<00:02,  9.70it/s]Epoch: 8, train for the 15-th batch, train loss: 0.19550685584545135:  12%|█▍          | 14/119 [00:08<01:00,  1.74it/s]Epoch: 8, train for the 15-th batch, train loss: 0.19550685584545135:  13%|█▌          | 15/119 [00:08<00:59,  1.75it/s]evaluate for the 21-th batch, evaluate loss: 0.5264435410499573:  43%|███████▊          | 20/46 [00:02<00:02,  9.70it/s]evaluate for the 21-th batch, evaluate loss: 0.5264435410499573:  46%|████████▏         | 21/46 [00:02<00:02,  9.71it/s]Epoch: 4, train for the 206-th batch, train loss: 0.6236915588378906:  86%|█████████▌ | 205/237 [02:00<00:18,  1.71it/s]Epoch: 4, train for the 206-th batch, train loss: 0.6236915588378906:  87%|█████████▌ | 206/237 [02:00<00:18,  1.69it/s]evaluate for the 22-th batch, evaluate loss: 0.5203563570976257:  46%|████████▏         | 21/46 [00:02<00:02,  9.71it/s]evaluate for the 22-th batch, evaluate loss: 0.5203563570976257:  48%|████████▌         | 22/46 [00:02<00:02,  9.72it/s]Epoch: 7, train for the 11-th batch, train loss: 0.4715510308742523:   7%|▉            | 10/146 [00:06<01:17,  1.76it/s]Epoch: 7, train for the 11-th batch, train loss: 0.4715510308742523:   8%|▉            | 11/146 [00:06<01:14,  1.80it/s]Epoch: 3, train for the 111-th batch, train loss: 0.432655394077301:  29%|███▍        | 110/383 [01:06<02:47,  1.63it/s]Epoch: 3, train for the 111-th batch, train loss: 0.432655394077301:  29%|███▍        | 111/383 [01:06<02:45,  1.64it/s]evaluate for the 23-th batch, evaluate loss: 0.47250238060951233:  48%|████████▏        | 22/46 [00:02<00:02,  9.72it/s]evaluate for the 23-th batch, evaluate loss: 0.47250238060951233:  50%|████████▌        | 23/46 [00:02<00:02,  9.73it/s]evaluate for the 24-th batch, evaluate loss: 0.47796303033828735:  50%|████████▌        | 23/46 [00:02<00:02,  9.73it/s]evaluate for the 24-th batch, evaluate loss: 0.47796303033828735:  52%|████████▊        | 24/46 [00:02<00:02,  9.72it/s]evaluate for the 25-th batch, evaluate loss: 0.5339409112930298:  52%|█████████▍        | 24/46 [00:02<00:02,  9.72it/s]evaluate for the 25-th batch, evaluate loss: 0.5339409112930298:  54%|█████████▊        | 25/46 [00:02<00:02,  9.67it/s]evaluate for the 26-th batch, evaluate loss: 0.5499948859214783:  54%|█████████▊        | 25/46 [00:02<00:02,  9.67it/s]evaluate for the 26-th batch, evaluate loss: 0.5499948859214783:  57%|██████████▏       | 26/46 [00:02<00:02,  9.65it/s]Epoch: 8, train for the 16-th batch, train loss: 0.23974891006946564:  13%|█▌          | 15/119 [00:09<00:59,  1.75it/s]Epoch: 8, train for the 16-th batch, train loss: 0.23974891006946564:  13%|█▌          | 16/119 [00:09<01:00,  1.71it/s]evaluate for the 27-th batch, evaluate loss: 0.49281346797943115:  57%|█████████▌       | 26/46 [00:02<00:02,  9.65it/s]evaluate for the 27-th batch, evaluate loss: 0.49281346797943115:  59%|█████████▉       | 27/46 [00:02<00:01,  9.64it/s]Epoch: 4, train for the 207-th batch, train loss: 0.6457263231277466:  87%|█████████▌ | 206/237 [02:01<00:18,  1.69it/s]Epoch: 4, train for the 207-th batch, train loss: 0.6457263231277466:  87%|█████████▌ | 207/237 [02:01<00:17,  1.68it/s]Epoch: 7, train for the 12-th batch, train loss: 0.44965028762817383:   8%|▉           | 11/146 [00:06<01:14,  1.80it/s]Epoch: 7, train for the 12-th batch, train loss: 0.44965028762817383:   8%|▉           | 12/146 [00:06<01:14,  1.79it/s]evaluate for the 28-th batch, evaluate loss: 0.517776608467102:  59%|███████████▏       | 27/46 [00:02<00:01,  9.64it/s]evaluate for the 28-th batch, evaluate loss: 0.517776608467102:  61%|███████████▌       | 28/46 [00:02<00:01,  9.68it/s]Epoch: 3, train for the 112-th batch, train loss: 0.39568907022476196:  29%|██▉       | 111/383 [01:06<02:45,  1.64it/s]Epoch: 3, train for the 112-th batch, train loss: 0.39568907022476196:  29%|██▉       | 112/383 [01:06<02:45,  1.64it/s]evaluate for the 29-th batch, evaluate loss: 0.4885868430137634:  61%|██████████▉       | 28/46 [00:02<00:01,  9.68it/s]evaluate for the 29-th batch, evaluate loss: 0.4885868430137634:  63%|███████████▎      | 29/46 [00:02<00:01,  9.68it/s]evaluate for the 30-th batch, evaluate loss: 0.4908958673477173:  63%|███████████▎      | 29/46 [00:03<00:01,  9.68it/s]evaluate for the 30-th batch, evaluate loss: 0.4908958673477173:  65%|███████████▋      | 30/46 [00:03<00:01,  9.68it/s]evaluate for the 31-th batch, evaluate loss: 0.5250131487846375:  65%|███████████▋      | 30/46 [00:03<00:01,  9.68it/s]evaluate for the 31-th batch, evaluate loss: 0.5250131487846375:  67%|████████████▏     | 31/46 [00:03<00:01,  9.67it/s]evaluate for the 32-th batch, evaluate loss: 0.4797162115573883:  67%|████████████▏     | 31/46 [00:03<00:01,  9.67it/s]evaluate for the 32-th batch, evaluate loss: 0.4797162115573883:  70%|████████████▌     | 32/46 [00:03<00:01,  9.67it/s]Epoch: 8, train for the 17-th batch, train loss: 0.22532838582992554:  13%|█▌          | 16/119 [00:09<01:00,  1.71it/s]Epoch: 8, train for the 17-th batch, train loss: 0.22532838582992554:  14%|█▋          | 17/119 [00:09<01:00,  1.67it/s]evaluate for the 33-th batch, evaluate loss: 0.4938621520996094:  70%|████████████▌     | 32/46 [00:03<00:01,  9.67it/s]evaluate for the 33-th batch, evaluate loss: 0.4938621520996094:  72%|████████████▉     | 33/46 [00:03<00:01,  9.70it/s]Epoch: 4, train for the 208-th batch, train loss: 0.6412338018417358:  87%|█████████▌ | 207/237 [02:01<00:17,  1.68it/s]Epoch: 4, train for the 208-th batch, train loss: 0.6412338018417358:  88%|█████████▋ | 208/237 [02:01<00:17,  1.67it/s]Epoch: 7, train for the 13-th batch, train loss: 0.4638065695762634:   8%|█            | 12/146 [00:07<01:14,  1.79it/s]Epoch: 7, train for the 13-th batch, train loss: 0.4638065695762634:   9%|█▏           | 13/146 [00:07<01:16,  1.74it/s]evaluate for the 34-th batch, evaluate loss: 0.4786953628063202:  72%|████████████▉     | 33/46 [00:03<00:01,  9.70it/s]evaluate for the 34-th batch, evaluate loss: 0.4786953628063202:  74%|█████████████▎    | 34/46 [00:03<00:01,  9.72it/s]Epoch: 3, train for the 113-th batch, train loss: 0.32342299818992615:  29%|██▉       | 112/383 [01:07<02:45,  1.64it/s]Epoch: 3, train for the 113-th batch, train loss: 0.32342299818992615:  30%|██▉       | 113/383 [01:07<02:44,  1.64it/s]evaluate for the 35-th batch, evaluate loss: 0.48037391901016235:  74%|████████████▌    | 34/46 [00:03<00:01,  9.72it/s]evaluate for the 35-th batch, evaluate loss: 0.48037391901016235:  76%|████████████▉    | 35/46 [00:03<00:01,  9.70it/s]evaluate for the 36-th batch, evaluate loss: 0.4734629690647125:  76%|█████████████▋    | 35/46 [00:03<00:01,  9.70it/s]evaluate for the 36-th batch, evaluate loss: 0.4734629690647125:  78%|██████████████    | 36/46 [00:03<00:01,  9.69it/s]evaluate for the 37-th batch, evaluate loss: 0.5045556426048279:  78%|██████████████    | 36/46 [00:03<00:01,  9.69it/s]evaluate for the 37-th batch, evaluate loss: 0.5045556426048279:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.69it/s]evaluate for the 38-th batch, evaluate loss: 0.5321136116981506:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.69it/s]evaluate for the 38-th batch, evaluate loss: 0.5321136116981506:  83%|██████████████▊   | 38/46 [00:03<00:00,  9.71it/s]Epoch: 8, train for the 18-th batch, train loss: 0.23164504766464233:  14%|█▋          | 17/119 [00:10<01:00,  1.67it/s]Epoch: 8, train for the 18-th batch, train loss: 0.23164504766464233:  15%|█▊          | 18/119 [00:10<01:00,  1.67it/s]evaluate for the 39-th batch, evaluate loss: 0.5270661115646362:  83%|██████████████▊   | 38/46 [00:04<00:00,  9.71it/s]evaluate for the 39-th batch, evaluate loss: 0.5270661115646362:  85%|███████████████▎  | 39/46 [00:04<00:00,  9.71it/s]Epoch: 4, train for the 209-th batch, train loss: 0.6142076849937439:  88%|█████████▋ | 208/237 [02:02<00:17,  1.67it/s]Epoch: 4, train for the 209-th batch, train loss: 0.6142076849937439:  88%|█████████▋ | 209/237 [02:02<00:16,  1.66it/s]Epoch: 7, train for the 14-th batch, train loss: 0.4202482998371124:   9%|█▏           | 13/146 [00:07<01:16,  1.74it/s]Epoch: 7, train for the 14-th batch, train loss: 0.4202482998371124:  10%|█▏           | 14/146 [00:07<01:16,  1.73it/s]evaluate for the 40-th batch, evaluate loss: 0.4691934883594513:  85%|███████████████▎  | 39/46 [00:04<00:00,  9.71it/s]evaluate for the 40-th batch, evaluate loss: 0.4691934883594513:  87%|███████████████▋  | 40/46 [00:04<00:00,  9.69it/s]Epoch: 3, train for the 114-th batch, train loss: 0.3487195372581482:  30%|███▏       | 113/383 [01:08<02:44,  1.64it/s]Epoch: 3, train for the 114-th batch, train loss: 0.3487195372581482:  30%|███▎       | 114/383 [01:08<02:43,  1.65it/s]evaluate for the 41-th batch, evaluate loss: 0.4843054711818695:  87%|███████████████▋  | 40/46 [00:04<00:00,  9.69it/s]evaluate for the 41-th batch, evaluate loss: 0.4843054711818695:  89%|████████████████  | 41/46 [00:04<00:00,  9.69it/s]evaluate for the 42-th batch, evaluate loss: 0.47231245040893555:  89%|███████████████▏ | 41/46 [00:04<00:00,  9.69it/s]evaluate for the 42-th batch, evaluate loss: 0.47231245040893555:  91%|███████████████▌ | 42/46 [00:04<00:00,  9.68it/s]evaluate for the 43-th batch, evaluate loss: 0.5304094552993774:  91%|████████████████▍ | 42/46 [00:04<00:00,  9.68it/s]evaluate for the 43-th batch, evaluate loss: 0.5304094552993774:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.68it/s]Epoch: 8, train for the 19-th batch, train loss: 0.18087752163410187:  15%|█▊          | 18/119 [00:10<01:00,  1.67it/s]Epoch: 8, train for the 19-th batch, train loss: 0.18087752163410187:  16%|█▉          | 19/119 [00:10<00:59,  1.68it/s]evaluate for the 44-th batch, evaluate loss: 0.5145827531814575:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.68it/s]evaluate for the 44-th batch, evaluate loss: 0.5145827531814575:  96%|█████████████████▏| 44/46 [00:04<00:00,  9.69it/s]Epoch: 4, train for the 210-th batch, train loss: 0.6389598846435547:  88%|█████████▋ | 209/237 [02:02<00:16,  1.66it/s]Epoch: 4, train for the 210-th batch, train loss: 0.6389598846435547:  89%|█████████▋ | 210/237 [02:02<00:16,  1.67it/s]evaluate for the 45-th batch, evaluate loss: 0.49474799633026123:  96%|████████████████▎| 44/46 [00:04<00:00,  9.69it/s]evaluate for the 45-th batch, evaluate loss: 0.49474799633026123:  98%|████████████████▋| 45/46 [00:04<00:00,  9.68it/s]Epoch: 7, train for the 15-th batch, train loss: 0.4250602722167969:  10%|█▏           | 14/146 [00:08<01:16,  1.73it/s]Epoch: 7, train for the 15-th batch, train loss: 0.4250602722167969:  10%|█▎           | 15/146 [00:08<01:16,  1.71it/s]evaluate for the 46-th batch, evaluate loss: 0.5065903067588806:  98%|█████████████████▌| 45/46 [00:04<00:00,  9.68it/s]evaluate for the 46-th batch, evaluate loss: 0.5065903067588806: 100%|██████████████████| 46/46 [00:04<00:00,  9.70it/s]
  0%|                                                                                            | 0/25 [00:00<?, ?it/s]Epoch: 3, train for the 115-th batch, train loss: 0.34049826860427856:  30%|██▉       | 114/383 [01:08<02:43,  1.65it/s]Epoch: 3, train for the 115-th batch, train loss: 0.34049826860427856:  30%|███       | 115/383 [01:08<02:42,  1.65it/s]evaluate for the 1-th batch, evaluate loss: 0.6334718465805054:   0%|                            | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6334718465805054:   4%|▊                   | 1/25 [00:00<00:02,  9.26it/s]evaluate for the 2-th batch, evaluate loss: 0.6455667614936829:   4%|▊                   | 1/25 [00:00<00:02,  9.26it/s]evaluate for the 2-th batch, evaluate loss: 0.6455667614936829:   8%|█▌                  | 2/25 [00:00<00:02,  9.23it/s]evaluate for the 3-th batch, evaluate loss: 0.6839587092399597:   8%|█▌                  | 2/25 [00:00<00:02,  9.23it/s]evaluate for the 3-th batch, evaluate loss: 0.6839587092399597:  12%|██▍                 | 3/25 [00:00<00:02,  9.25it/s]Epoch: 8, train for the 20-th batch, train loss: 0.22860468924045563:  16%|█▉          | 19/119 [00:11<00:59,  1.68it/s]Epoch: 8, train for the 20-th batch, train loss: 0.22860468924045563:  17%|██          | 20/119 [00:11<00:58,  1.68it/s]evaluate for the 4-th batch, evaluate loss: 0.665420413017273:  12%|██▌                  | 3/25 [00:00<00:02,  9.25it/s]evaluate for the 4-th batch, evaluate loss: 0.665420413017273:  16%|███▎                 | 4/25 [00:00<00:02,  9.30it/s]Epoch: 4, train for the 211-th batch, train loss: 0.6251592636108398:  89%|█████████▋ | 210/237 [02:03<00:16,  1.67it/s]Epoch: 4, train for the 211-th batch, train loss: 0.6251592636108398:  89%|█████████▊ | 211/237 [02:03<00:15,  1.67it/s]evaluate for the 5-th batch, evaluate loss: 0.6715150475502014:  16%|███▏                | 4/25 [00:00<00:02,  9.30it/s]evaluate for the 5-th batch, evaluate loss: 0.6715150475502014:  20%|████                | 5/25 [00:00<00:02,  9.30it/s]Epoch: 7, train for the 16-th batch, train loss: 0.43277183175086975:  10%|█▏          | 15/146 [00:09<01:16,  1.71it/s]Epoch: 7, train for the 16-th batch, train loss: 0.43277183175086975:  11%|█▎          | 16/146 [00:09<01:16,  1.69it/s]Epoch: 3, train for the 116-th batch, train loss: 0.3774096667766571:  30%|███▎       | 115/383 [01:09<02:42,  1.65it/s]Epoch: 3, train for the 116-th batch, train loss: 0.3774096667766571:  30%|███▎       | 116/383 [01:09<02:41,  1.66it/s]evaluate for the 6-th batch, evaluate loss: 0.7138590812683105:  20%|████                | 5/25 [00:00<00:02,  9.30it/s]evaluate for the 6-th batch, evaluate loss: 0.7138590812683105:  24%|████▊               | 6/25 [00:00<00:02,  9.28it/s]evaluate for the 7-th batch, evaluate loss: 0.72812819480896:  24%|█████▎                | 6/25 [00:00<00:02,  9.28it/s]evaluate for the 7-th batch, evaluate loss: 0.72812819480896:  28%|██████▏               | 7/25 [00:00<00:01,  9.26it/s]evaluate for the 8-th batch, evaluate loss: 0.713634192943573:  28%|█████▉               | 7/25 [00:00<00:01,  9.26it/s]evaluate for the 8-th batch, evaluate loss: 0.713634192943573:  32%|██████▋              | 8/25 [00:00<00:01,  9.24it/s]evaluate for the 9-th batch, evaluate loss: 0.6873068809509277:  32%|██████▍             | 8/25 [00:00<00:01,  9.24it/s]evaluate for the 9-th batch, evaluate loss: 0.6873068809509277:  36%|███████▏            | 9/25 [00:00<00:01,  9.29it/s]Epoch: 8, train for the 21-th batch, train loss: 0.25471025705337524:  17%|██          | 20/119 [00:12<00:58,  1.68it/s]Epoch: 8, train for the 21-th batch, train loss: 0.25471025705337524:  18%|██          | 21/119 [00:12<00:58,  1.67it/s]evaluate for the 10-th batch, evaluate loss: 0.7252033948898315:  36%|██████▊            | 9/25 [00:01<00:01,  9.29it/s]evaluate for the 10-th batch, evaluate loss: 0.7252033948898315:  40%|███████▏          | 10/25 [00:01<00:01,  9.30it/s]Epoch: 4, train for the 212-th batch, train loss: 0.6240171194076538:  89%|█████████▊ | 211/237 [02:04<00:15,  1.67it/s]Epoch: 4, train for the 212-th batch, train loss: 0.6240171194076538:  89%|█████████▊ | 212/237 [02:04<00:15,  1.66it/s]Epoch: 7, train for the 17-th batch, train loss: 0.41477933526039124:  11%|█▎          | 16/146 [00:09<01:16,  1.69it/s]Epoch: 7, train for the 17-th batch, train loss: 0.41477933526039124:  12%|█▍          | 17/146 [00:09<01:14,  1.73it/s]evaluate for the 11-th batch, evaluate loss: 0.7291254997253418:  40%|███████▏          | 10/25 [00:01<00:01,  9.30it/s]evaluate for the 11-th batch, evaluate loss: 0.7291254997253418:  44%|███████▉          | 11/25 [00:01<00:01,  9.28it/s]Epoch: 3, train for the 117-th batch, train loss: 0.34765714406967163:  30%|███       | 116/383 [01:09<02:41,  1.66it/s]Epoch: 3, train for the 117-th batch, train loss: 0.34765714406967163:  31%|███       | 117/383 [01:09<02:40,  1.65it/s]evaluate for the 12-th batch, evaluate loss: 0.6951575875282288:  44%|███████▉          | 11/25 [00:01<00:01,  9.28it/s]evaluate for the 12-th batch, evaluate loss: 0.6951575875282288:  48%|████████▋         | 12/25 [00:01<00:01,  9.29it/s]evaluate for the 13-th batch, evaluate loss: 0.6547676920890808:  48%|████████▋         | 12/25 [00:01<00:01,  9.29it/s]evaluate for the 13-th batch, evaluate loss: 0.6547676920890808:  52%|█████████▎        | 13/25 [00:01<00:01,  9.29it/s]evaluate for the 14-th batch, evaluate loss: 0.7434080243110657:  52%|█████████▎        | 13/25 [00:01<00:01,  9.29it/s]evaluate for the 14-th batch, evaluate loss: 0.7434080243110657:  56%|██████████        | 14/25 [00:01<00:01,  9.22it/s]Epoch: 8, train for the 22-th batch, train loss: 0.23943065106868744:  18%|██          | 21/119 [00:12<00:58,  1.67it/s]Epoch: 8, train for the 22-th batch, train loss: 0.23943065106868744:  18%|██▏         | 22/119 [00:12<00:57,  1.68it/s]evaluate for the 15-th batch, evaluate loss: 0.7226722836494446:  56%|██████████        | 14/25 [00:01<00:01,  9.22it/s]evaluate for the 15-th batch, evaluate loss: 0.7226722836494446:  60%|██████████▊       | 15/25 [00:01<00:01,  9.23it/s]Epoch: 7, train for the 18-th batch, train loss: 0.3840905427932739:  12%|█▌           | 17/146 [00:10<01:14,  1.73it/s]Epoch: 7, train for the 18-th batch, train loss: 0.3840905427932739:  12%|█▌           | 18/146 [00:10<01:14,  1.71it/s]Epoch: 4, train for the 213-th batch, train loss: 0.6078768968582153:  89%|█████████▊ | 212/237 [02:04<00:15,  1.66it/s]Epoch: 4, train for the 213-th batch, train loss: 0.6078768968582153:  90%|█████████▉ | 213/237 [02:04<00:14,  1.66it/s]evaluate for the 16-th batch, evaluate loss: 0.6567492485046387:  60%|██████████▊       | 15/25 [00:01<00:01,  9.23it/s]evaluate for the 16-th batch, evaluate loss: 0.6567492485046387:  64%|███████████▌      | 16/25 [00:01<00:00,  9.23it/s]evaluate for the 17-th batch, evaluate loss: 0.6588250398635864:  64%|███████████▌      | 16/25 [00:01<00:00,  9.23it/s]evaluate for the 17-th batch, evaluate loss: 0.6588250398635864:  68%|████████████▏     | 17/25 [00:01<00:00,  9.23it/s]Epoch: 3, train for the 118-th batch, train loss: 0.3447028696537018:  31%|███▎       | 117/383 [01:10<02:40,  1.65it/s]Epoch: 3, train for the 118-th batch, train loss: 0.3447028696537018:  31%|███▍       | 118/383 [01:10<02:40,  1.65it/s]evaluate for the 18-th batch, evaluate loss: 0.6264472603797913:  68%|████████████▏     | 17/25 [00:01<00:00,  9.23it/s]evaluate for the 18-th batch, evaluate loss: 0.6264472603797913:  72%|████████████▉     | 18/25 [00:01<00:00,  9.22it/s]evaluate for the 19-th batch, evaluate loss: 0.5872106552124023:  72%|████████████▉     | 18/25 [00:02<00:00,  9.22it/s]evaluate for the 19-th batch, evaluate loss: 0.5872106552124023:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.23it/s]evaluate for the 20-th batch, evaluate loss: 0.650015652179718:  76%|██████████████▍    | 19/25 [00:02<00:00,  9.23it/s]evaluate for the 20-th batch, evaluate loss: 0.650015652179718:  80%|███████████████▏   | 20/25 [00:02<00:00,  9.26it/s]Epoch: 8, train for the 23-th batch, train loss: 0.264523983001709:  18%|██▌           | 22/119 [00:13<00:57,  1.68it/s]Epoch: 8, train for the 23-th batch, train loss: 0.264523983001709:  19%|██▋           | 23/119 [00:13<00:56,  1.69it/s]evaluate for the 21-th batch, evaluate loss: 0.7178941965103149:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.26it/s]evaluate for the 21-th batch, evaluate loss: 0.7178941965103149:  84%|███████████████   | 21/25 [00:02<00:00,  9.26it/s]Epoch: 7, train for the 19-th batch, train loss: 0.47840842604637146:  12%|█▍          | 18/146 [00:10<01:14,  1.71it/s]Epoch: 7, train for the 19-th batch, train loss: 0.47840842604637146:  13%|█▌          | 19/146 [00:10<01:14,  1.70it/s]Epoch: 4, train for the 214-th batch, train loss: 0.6621073484420776:  90%|█████████▉ | 213/237 [02:05<00:14,  1.66it/s]Epoch: 4, train for the 214-th batch, train loss: 0.6621073484420776:  90%|█████████▉ | 214/237 [02:05<00:13,  1.65it/s]evaluate for the 22-th batch, evaluate loss: 0.6006236672401428:  84%|███████████████   | 21/25 [00:02<00:00,  9.26it/s]evaluate for the 22-th batch, evaluate loss: 0.6006236672401428:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.24it/s]Epoch: 3, train for the 119-th batch, train loss: 0.39159467816352844:  31%|███       | 118/383 [01:11<02:40,  1.65it/s]Epoch: 3, train for the 119-th batch, train loss: 0.39159467816352844:  31%|███       | 119/383 [01:11<02:39,  1.65it/s]evaluate for the 23-th batch, evaluate loss: 0.6576887369155884:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.24it/s]evaluate for the 23-th batch, evaluate loss: 0.6576887369155884:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.23it/s]evaluate for the 24-th batch, evaluate loss: 0.6554650664329529:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.23it/s]evaluate for the 24-th batch, evaluate loss: 0.6554650664329529:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.22it/s]evaluate for the 25-th batch, evaluate loss: 0.700286865234375:  96%|██████████████████▏| 24/25 [00:02<00:00,  9.22it/s]evaluate for the 25-th batch, evaluate loss: 0.700286865234375: 100%|███████████████████| 25/25 [00:02<00:00,  9.32it/s]
INFO:root:Epoch: 12, learning rate: 0.0001, train loss: 0.5677
INFO:root:train average_precision, 0.8194
INFO:root:train roc_auc, 0.7837
INFO:root:validate loss: 0.5068
INFO:root:validate average_precision, 0.8432
INFO:root:validate roc_auc, 0.8033
INFO:root:new node validate loss: 0.6770
INFO:root:new node validate first_1_average_precision, 0.5958
INFO:root:new node validate first_1_roc_auc, 0.5428
INFO:root:new node validate first_3_average_precision, 0.6815
INFO:root:new node validate first_3_roc_auc, 0.6407
INFO:root:new node validate first_10_average_precision, 0.7486
INFO:root:new node validate first_10_roc_auc, 0.7127
INFO:root:new node validate average_precision, 0.7087
INFO:root:new node validate roc_auc, 0.6562
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]Epoch: 8, train for the 24-th batch, train loss: 0.20730996131896973:  19%|██▎         | 23/119 [00:13<00:56,  1.69it/s]Epoch: 8, train for the 24-th batch, train loss: 0.20730996131896973:  20%|██▍         | 24/119 [00:13<00:56,  1.69it/s]Epoch: 13, train for the 1-th batch, train loss: 0.7383100986480713:   0%|                      | 0/151 [00:00<?, ?it/s]Epoch: 13, train for the 1-th batch, train loss: 0.7383100986480713:   1%|              | 1/151 [00:00<00:25,  5.86it/s]Epoch: 7, train for the 20-th batch, train loss: 0.4821862578392029:  13%|█▋           | 19/146 [00:11<01:14,  1.70it/s]Epoch: 7, train for the 20-th batch, train loss: 0.4821862578392029:  14%|█▊           | 20/146 [00:11<01:13,  1.71it/s]Epoch: 4, train for the 215-th batch, train loss: 0.603961706161499:  90%|██████████▊ | 214/237 [02:05<00:13,  1.65it/s]Epoch: 4, train for the 215-th batch, train loss: 0.603961706161499:  91%|██████████▉ | 215/237 [02:05<00:13,  1.65it/s]Epoch: 13, train for the 2-th batch, train loss: 0.7780002355575562:   1%|              | 1/151 [00:00<00:25,  5.86it/s]Epoch: 13, train for the 2-th batch, train loss: 0.7780002355575562:   1%|▏             | 2/151 [00:00<00:26,  5.72it/s]Epoch: 3, train for the 120-th batch, train loss: 0.39630699157714844:  31%|███       | 119/383 [01:11<02:39,  1.65it/s]Epoch: 3, train for the 120-th batch, train loss: 0.39630699157714844:  31%|███▏      | 120/383 [01:11<02:39,  1.65it/s]Epoch: 13, train for the 3-th batch, train loss: 0.5063785910606384:   1%|▏             | 2/151 [00:00<00:26,  5.72it/s]Epoch: 13, train for the 3-th batch, train loss: 0.5063785910606384:   2%|▎             | 3/151 [00:00<00:25,  5.89it/s]Epoch: 8, train for the 25-th batch, train loss: 0.2673361599445343:  20%|██▌          | 24/119 [00:14<00:56,  1.69it/s]Epoch: 8, train for the 25-th batch, train loss: 0.2673361599445343:  21%|██▋          | 25/119 [00:14<00:55,  1.69it/s]Epoch: 13, train for the 4-th batch, train loss: 0.574826717376709:   2%|▎              | 3/151 [00:00<00:25,  5.89it/s]Epoch: 13, train for the 4-th batch, train loss: 0.574826717376709:   3%|▍              | 4/151 [00:00<00:24,  5.90it/s]Epoch: 7, train for the 21-th batch, train loss: 0.41571375727653503:  14%|█▋          | 20/146 [00:12<01:13,  1.71it/s]Epoch: 7, train for the 21-th batch, train loss: 0.41571375727653503:  14%|█▋          | 21/146 [00:12<01:13,  1.69it/s]Epoch: 4, train for the 216-th batch, train loss: 0.6206808090209961:  91%|█████████▉ | 215/237 [02:06<00:13,  1.65it/s]Epoch: 4, train for the 216-th batch, train loss: 0.6206808090209961:  91%|██████████ | 216/237 [02:06<00:12,  1.65it/s]Epoch: 13, train for the 5-th batch, train loss: 0.6489318013191223:   3%|▎             | 4/151 [00:00<00:24,  5.90it/s]Epoch: 13, train for the 5-th batch, train loss: 0.6489318013191223:   3%|▍             | 5/151 [00:00<00:25,  5.65it/s]Epoch: 3, train for the 121-th batch, train loss: 0.4267849624156952:  31%|███▍       | 120/383 [01:12<02:39,  1.65it/s]Epoch: 3, train for the 121-th batch, train loss: 0.4267849624156952:  32%|███▍       | 121/383 [01:12<02:38,  1.65it/s]Epoch: 13, train for the 6-th batch, train loss: 0.5766082406044006:   3%|▍             | 5/151 [00:01<00:25,  5.65it/s]Epoch: 13, train for the 6-th batch, train loss: 0.5766082406044006:   4%|▌             | 6/151 [00:01<00:26,  5.52it/s]Epoch: 13, train for the 7-th batch, train loss: 0.5948572158813477:   4%|▌             | 6/151 [00:01<00:26,  5.52it/s]Epoch: 13, train for the 7-th batch, train loss: 0.5948572158813477:   5%|▋             | 7/151 [00:01<00:26,  5.41it/s]Epoch: 8, train for the 26-th batch, train loss: 0.1747705638408661:  21%|██▋          | 25/119 [00:14<00:55,  1.69it/s]Epoch: 8, train for the 26-th batch, train loss: 0.1747705638408661:  22%|██▊          | 26/119 [00:14<00:54,  1.70it/s]Epoch: 7, train for the 22-th batch, train loss: 0.4467979371547699:  14%|█▊           | 21/146 [00:12<01:13,  1.69it/s]Epoch: 7, train for the 22-th batch, train loss: 0.4467979371547699:  15%|█▉           | 22/146 [00:12<01:13,  1.68it/s]Epoch: 13, train for the 8-th batch, train loss: 0.618668258190155:   5%|▋              | 7/151 [00:01<00:26,  5.41it/s]Epoch: 13, train for the 8-th batch, train loss: 0.618668258190155:   5%|▊              | 8/151 [00:01<00:26,  5.39it/s]Epoch: 4, train for the 217-th batch, train loss: 0.6221494674682617:  91%|██████████ | 216/237 [02:07<00:12,  1.65it/s]Epoch: 4, train for the 217-th batch, train loss: 0.6221494674682617:  92%|██████████ | 217/237 [02:07<00:12,  1.65it/s]Epoch: 3, train for the 122-th batch, train loss: 0.46275991201400757:  32%|███▏      | 121/383 [01:12<02:38,  1.65it/s]Epoch: 3, train for the 122-th batch, train loss: 0.46275991201400757:  32%|███▏      | 122/383 [01:12<02:38,  1.65it/s]Epoch: 13, train for the 9-th batch, train loss: 0.5387763977050781:   5%|▋             | 8/151 [00:01<00:26,  5.39it/s]Epoch: 13, train for the 9-th batch, train loss: 0.5387763977050781:   6%|▊             | 9/151 [00:01<00:26,  5.33it/s]Epoch: 13, train for the 10-th batch, train loss: 0.5832038521766663:   6%|▊            | 9/151 [00:01<00:26,  5.33it/s]Epoch: 13, train for the 10-th batch, train loss: 0.5832038521766663:   7%|▊           | 10/151 [00:01<00:26,  5.23it/s]Epoch: 8, train for the 27-th batch, train loss: 0.2042710781097412:  22%|██▊          | 26/119 [00:15<00:54,  1.70it/s]Epoch: 8, train for the 27-th batch, train loss: 0.2042710781097412:  23%|██▉          | 27/119 [00:15<00:54,  1.69it/s]Epoch: 7, train for the 23-th batch, train loss: 0.4623321294784546:  15%|█▉           | 22/146 [00:13<01:13,  1.68it/s]Epoch: 7, train for the 23-th batch, train loss: 0.4623321294784546:  16%|██           | 23/146 [00:13<01:12,  1.69it/s]Epoch: 13, train for the 11-th batch, train loss: 0.5229358077049255:   7%|▊           | 10/151 [00:02<00:26,  5.23it/s]Epoch: 13, train for the 11-th batch, train loss: 0.5229358077049255:   7%|▊           | 11/151 [00:02<00:27,  5.18it/s]Epoch: 4, train for the 218-th batch, train loss: 0.6321499347686768:  92%|██████████ | 217/237 [02:07<00:12,  1.65it/s]Epoch: 4, train for the 218-th batch, train loss: 0.6321499347686768:  92%|██████████ | 218/237 [02:07<00:11,  1.65it/s]Epoch: 3, train for the 123-th batch, train loss: 0.35214704275131226:  32%|███▏      | 122/383 [01:13<02:38,  1.65it/s]Epoch: 3, train for the 123-th batch, train loss: 0.35214704275131226:  32%|███▏      | 123/383 [01:13<02:37,  1.65it/s]Epoch: 13, train for the 12-th batch, train loss: 0.5486403703689575:   7%|▊           | 11/151 [00:02<00:27,  5.18it/s]Epoch: 13, train for the 12-th batch, train loss: 0.5486403703689575:   8%|▉           | 12/151 [00:02<00:27,  5.14it/s]Epoch: 8, train for the 28-th batch, train loss: 0.2225518822669983:  23%|██▉          | 27/119 [00:16<00:54,  1.69it/s]Epoch: 8, train for the 28-th batch, train loss: 0.2225518822669983:  24%|███          | 28/119 [00:16<00:50,  1.81it/s]Epoch: 13, train for the 13-th batch, train loss: 0.6290738582611084:   8%|▉           | 12/151 [00:02<00:27,  5.14it/s]Epoch: 13, train for the 13-th batch, train loss: 0.6290738582611084:   9%|█           | 13/151 [00:02<00:27,  5.10it/s]Epoch: 13, train for the 14-th batch, train loss: 0.6116470098495483:   9%|█           | 13/151 [00:02<00:27,  5.10it/s]Epoch: 13, train for the 14-th batch, train loss: 0.6116470098495483:   9%|█           | 14/151 [00:02<00:27,  5.06it/s]Epoch: 4, train for the 219-th batch, train loss: 0.6181517839431763:  92%|██████████ | 218/237 [02:08<00:11,  1.65it/s]Epoch: 4, train for the 219-th batch, train loss: 0.6181517839431763:  92%|██████████▏| 219/237 [02:08<00:10,  1.65it/s]Epoch: 8, train for the 29-th batch, train loss: 0.23361270129680634:  24%|██▊         | 28/119 [00:16<00:50,  1.81it/s]Epoch: 8, train for the 29-th batch, train loss: 0.23361270129680634:  24%|██▉         | 29/119 [00:16<00:44,  2.03it/s]Epoch: 3, train for the 124-th batch, train loss: 0.4145217835903168:  32%|███▌       | 123/383 [01:14<02:37,  1.65it/s]Epoch: 3, train for the 124-th batch, train loss: 0.4145217835903168:  32%|███▌       | 124/383 [01:14<02:36,  1.65it/s]Epoch: 13, train for the 15-th batch, train loss: 0.47650155425071716:   9%|█          | 14/151 [00:02<00:27,  5.06it/s]Epoch: 13, train for the 15-th batch, train loss: 0.47650155425071716:  10%|█          | 15/151 [00:02<00:27,  5.02it/s]Epoch: 8, train for the 30-th batch, train loss: 0.2010004222393036:  24%|███▏         | 29/119 [00:16<00:44,  2.03it/s]Epoch: 8, train for the 30-th batch, train loss: 0.2010004222393036:  25%|███▎         | 30/119 [00:16<00:37,  2.40it/s]Epoch: 13, train for the 16-th batch, train loss: 0.49972403049468994:  10%|█          | 15/151 [00:03<00:27,  5.02it/s]Epoch: 13, train for the 16-th batch, train loss: 0.49972403049468994:  11%|█▏         | 16/151 [00:03<00:26,  5.01it/s]Epoch: 7, train for the 24-th batch, train loss: 0.4551849365234375:  16%|██           | 23/146 [00:14<01:12,  1.69it/s]Epoch: 7, train for the 24-th batch, train loss: 0.4551849365234375:  16%|██▏          | 24/146 [00:14<01:32,  1.32it/s]Epoch: 13, train for the 17-th batch, train loss: 0.6914433836936951:  11%|█▎          | 16/151 [00:03<00:26,  5.01it/s]Epoch: 13, train for the 17-th batch, train loss: 0.6914433836936951:  11%|█▎          | 17/151 [00:03<00:27,  4.93it/s]Epoch: 4, train for the 220-th batch, train loss: 0.639076292514801:  92%|███████████ | 219/237 [02:08<00:10,  1.65it/s]Epoch: 4, train for the 220-th batch, train loss: 0.639076292514801:  93%|███████████▏| 220/237 [02:08<00:10,  1.65it/s]Epoch: 3, train for the 125-th batch, train loss: 0.3744550347328186:  32%|███▌       | 124/383 [01:14<02:36,  1.65it/s]Epoch: 3, train for the 125-th batch, train loss: 0.3744550347328186:  33%|███▌       | 125/383 [01:14<02:36,  1.65it/s]Epoch: 8, train for the 31-th batch, train loss: 0.192778542637825:  25%|███▌          | 30/119 [00:17<00:37,  2.40it/s]Epoch: 8, train for the 31-th batch, train loss: 0.192778542637825:  26%|███▋          | 31/119 [00:17<00:39,  2.24it/s]Epoch: 13, train for the 18-th batch, train loss: 0.7047877311706543:  11%|█▎          | 17/151 [00:03<00:27,  4.93it/s]Epoch: 13, train for the 18-th batch, train loss: 0.7047877311706543:  12%|█▍          | 18/151 [00:03<00:27,  4.84it/s]Epoch: 13, train for the 19-th batch, train loss: 0.6647699475288391:  12%|█▍          | 18/151 [00:03<00:27,  4.84it/s]Epoch: 13, train for the 19-th batch, train loss: 0.6647699475288391:  13%|█▌          | 19/151 [00:03<00:27,  4.79it/s]Epoch: 7, train for the 25-th batch, train loss: 0.49522703886032104:  16%|█▉          | 24/146 [00:14<01:32,  1.32it/s]Epoch: 7, train for the 25-th batch, train loss: 0.49522703886032104:  17%|██          | 25/146 [00:14<01:25,  1.42it/s]Epoch: 4, train for the 221-th batch, train loss: 0.6375928521156311:  93%|██████████▏| 220/237 [02:09<00:10,  1.65it/s]Epoch: 4, train for the 221-th batch, train loss: 0.6375928521156311:  93%|██████████▎| 221/237 [02:09<00:09,  1.65it/s]Epoch: 13, train for the 20-th batch, train loss: 0.3616009056568146:  13%|█▌          | 19/151 [00:03<00:27,  4.79it/s]Epoch: 13, train for the 20-th batch, train loss: 0.3616009056568146:  13%|█▌          | 20/151 [00:03<00:27,  4.83it/s]Epoch: 8, train for the 32-th batch, train loss: 0.20122365653514862:  26%|███▏        | 31/119 [00:17<00:39,  2.24it/s]Epoch: 8, train for the 32-th batch, train loss: 0.20122365653514862:  27%|███▏        | 32/119 [00:17<00:42,  2.06it/s]Epoch: 3, train for the 126-th batch, train loss: 0.3345929980278015:  33%|███▌       | 125/383 [01:15<02:36,  1.65it/s]Epoch: 3, train for the 126-th batch, train loss: 0.3345929980278015:  33%|███▌       | 126/383 [01:15<02:36,  1.65it/s]Epoch: 13, train for the 21-th batch, train loss: 0.5766698122024536:  13%|█▌          | 20/151 [00:04<00:27,  4.83it/s]Epoch: 13, train for the 21-th batch, train loss: 0.5766698122024536:  14%|█▋          | 21/151 [00:04<00:27,  4.80it/s]Epoch: 7, train for the 26-th batch, train loss: 0.445382297039032:  17%|██▍           | 25/146 [00:15<01:25,  1.42it/s]Epoch: 7, train for the 26-th batch, train loss: 0.445382297039032:  18%|██▍           | 26/146 [00:15<01:19,  1.50it/s]Epoch: 13, train for the 22-th batch, train loss: 0.5411847829818726:  14%|█▋          | 21/151 [00:04<00:27,  4.80it/s]Epoch: 13, train for the 22-th batch, train loss: 0.5411847829818726:  15%|█▋          | 22/151 [00:04<00:26,  4.80it/s]Epoch: 4, train for the 222-th batch, train loss: 0.638022780418396:  93%|███████████▏| 221/237 [02:10<00:09,  1.65it/s]Epoch: 4, train for the 222-th batch, train loss: 0.638022780418396:  94%|███████████▏| 222/237 [02:10<00:09,  1.65it/s]Epoch: 13, train for the 23-th batch, train loss: 0.4612063765525818:  15%|█▋          | 22/151 [00:04<00:26,  4.80it/s]Epoch: 13, train for the 23-th batch, train loss: 0.4612063765525818:  15%|█▊          | 23/151 [00:04<00:26,  4.80it/s]Epoch: 8, train for the 33-th batch, train loss: 0.21463660895824432:  27%|███▏        | 32/119 [00:18<00:42,  2.06it/s]Epoch: 8, train for the 33-th batch, train loss: 0.21463660895824432:  28%|███▎        | 33/119 [00:18<00:44,  1.94it/s]Epoch: 3, train for the 127-th batch, train loss: 0.40893399715423584:  33%|███▎      | 126/383 [01:15<02:36,  1.65it/s]Epoch: 3, train for the 127-th batch, train loss: 0.40893399715423584:  33%|███▎      | 127/383 [01:15<02:35,  1.65it/s]Epoch: 13, train for the 24-th batch, train loss: 0.3622981905937195:  15%|█▊          | 23/151 [00:04<00:26,  4.80it/s]Epoch: 13, train for the 24-th batch, train loss: 0.3622981905937195:  16%|█▉          | 24/151 [00:04<00:26,  4.80it/s]Epoch: 7, train for the 27-th batch, train loss: 0.4660944938659668:  18%|██▎          | 26/146 [00:16<01:19,  1.50it/s]Epoch: 7, train for the 27-th batch, train loss: 0.4660944938659668:  18%|██▍          | 27/146 [00:16<01:16,  1.55it/s]Epoch: 13, train for the 25-th batch, train loss: 0.7487702965736389:  16%|█▉          | 24/151 [00:04<00:26,  4.80it/s]Epoch: 13, train for the 25-th batch, train loss: 0.7487702965736389:  17%|█▉          | 25/151 [00:04<00:26,  4.76it/s]Epoch: 4, train for the 223-th batch, train loss: 0.6200256943702698:  94%|██████████▎| 222/237 [02:10<00:09,  1.65it/s]Epoch: 4, train for the 223-th batch, train loss: 0.6200256943702698:  94%|██████████▎| 223/237 [02:10<00:08,  1.65it/s]Epoch: 13, train for the 26-th batch, train loss: 0.7603669762611389:  17%|█▉          | 25/151 [00:05<00:26,  4.76it/s]Epoch: 13, train for the 26-th batch, train loss: 0.7603669762611389:  17%|██          | 26/151 [00:05<00:26,  4.71it/s]Epoch: 8, train for the 34-th batch, train loss: 0.17834551632404327:  28%|███▎        | 33/119 [00:18<00:44,  1.94it/s]Epoch: 8, train for the 34-th batch, train loss: 0.17834551632404327:  29%|███▍        | 34/119 [00:18<00:45,  1.86it/s]Epoch: 3, train for the 128-th batch, train loss: 0.4122185707092285:  33%|███▋       | 127/383 [01:16<02:35,  1.65it/s]Epoch: 3, train for the 128-th batch, train loss: 0.4122185707092285:  33%|███▋       | 128/383 [01:16<02:34,  1.65it/s]Epoch: 13, train for the 27-th batch, train loss: 0.6915996670722961:  17%|██          | 26/151 [00:05<00:26,  4.71it/s]Epoch: 13, train for the 27-th batch, train loss: 0.6915996670722961:  18%|██▏         | 27/151 [00:05<00:26,  4.66it/s]Epoch: 7, train for the 28-th batch, train loss: 0.48392778635025024:  18%|██▏         | 27/146 [00:16<01:16,  1.55it/s]Epoch: 7, train for the 28-th batch, train loss: 0.48392778635025024:  19%|██▎         | 28/146 [00:16<01:14,  1.58it/s]Epoch: 13, train for the 28-th batch, train loss: 0.5403812527656555:  18%|██▏         | 27/151 [00:05<00:26,  4.66it/s]Epoch: 13, train for the 28-th batch, train loss: 0.5403812527656555:  19%|██▏         | 28/151 [00:05<00:26,  4.68it/s]Epoch: 4, train for the 224-th batch, train loss: 0.6642981767654419:  94%|██████████▎| 223/237 [02:11<00:08,  1.65it/s]Epoch: 4, train for the 224-th batch, train loss: 0.6642981767654419:  95%|██████████▍| 224/237 [02:11<00:07,  1.65it/s]Epoch: 8, train for the 35-th batch, train loss: 0.1652112603187561:  29%|███▋         | 34/119 [00:19<00:45,  1.86it/s]Epoch: 8, train for the 35-th batch, train loss: 0.1652112603187561:  29%|███▊         | 35/119 [00:19<00:46,  1.80it/s]Epoch: 13, train for the 29-th batch, train loss: 0.5523094534873962:  19%|██▏         | 28/151 [00:05<00:26,  4.68it/s]Epoch: 13, train for the 29-th batch, train loss: 0.5523094534873962:  19%|██▎         | 29/151 [00:05<00:25,  4.71it/s]Epoch: 3, train for the 129-th batch, train loss: 0.3943214416503906:  33%|███▋       | 128/383 [01:17<02:34,  1.65it/s]Epoch: 3, train for the 129-th batch, train loss: 0.3943214416503906:  34%|███▋       | 129/383 [01:17<02:33,  1.65it/s]Epoch: 13, train for the 30-th batch, train loss: 0.6109780669212341:  19%|██▎         | 29/151 [00:05<00:25,  4.71it/s]Epoch: 13, train for the 30-th batch, train loss: 0.6109780669212341:  20%|██▍         | 30/151 [00:05<00:25,  4.68it/s]Epoch: 7, train for the 29-th batch, train loss: 0.43056923151016235:  19%|██▎         | 28/146 [00:17<01:14,  1.58it/s]Epoch: 7, train for the 29-th batch, train loss: 0.43056923151016235:  20%|██▍         | 29/146 [00:17<01:12,  1.61it/s]Epoch: 13, train for the 31-th batch, train loss: 0.5740394592285156:  20%|██▍         | 30/151 [00:06<00:25,  4.68it/s]Epoch: 13, train for the 31-th batch, train loss: 0.5740394592285156:  21%|██▍         | 31/151 [00:06<00:25,  4.70it/s]Epoch: 4, train for the 225-th batch, train loss: 0.6439361572265625:  95%|██████████▍| 224/237 [02:11<00:07,  1.65it/s]Epoch: 4, train for the 225-th batch, train loss: 0.6439361572265625:  95%|██████████▍| 225/237 [02:11<00:07,  1.65it/s]Epoch: 8, train for the 36-th batch, train loss: 0.19461937248706818:  29%|███▌        | 35/119 [00:20<00:46,  1.80it/s]Epoch: 8, train for the 36-th batch, train loss: 0.19461937248706818:  30%|███▋        | 36/119 [00:20<00:47,  1.76it/s]Epoch: 13, train for the 32-th batch, train loss: 0.5681942701339722:  21%|██▍         | 31/151 [00:06<00:25,  4.70it/s]Epoch: 13, train for the 32-th batch, train loss: 0.5681942701339722:  21%|██▌         | 32/151 [00:06<00:25,  4.71it/s]Epoch: 3, train for the 130-th batch, train loss: 0.33944326639175415:  34%|███▎      | 129/383 [01:17<02:33,  1.65it/s]Epoch: 3, train for the 130-th batch, train loss: 0.33944326639175415:  34%|███▍      | 130/383 [01:17<02:33,  1.65it/s]Epoch: 13, train for the 33-th batch, train loss: 0.5280399322509766:  21%|██▌         | 32/151 [00:06<00:25,  4.71it/s]Epoch: 13, train for the 33-th batch, train loss: 0.5280399322509766:  22%|██▌         | 33/151 [00:06<00:24,  4.73it/s]Epoch: 7, train for the 30-th batch, train loss: 0.42420637607574463:  20%|██▍         | 29/146 [00:17<01:12,  1.61it/s]Epoch: 7, train for the 30-th batch, train loss: 0.42420637607574463:  21%|██▍         | 30/146 [00:17<01:11,  1.63it/s]Epoch: 13, train for the 34-th batch, train loss: 0.6273912191390991:  22%|██▌         | 33/151 [00:06<00:24,  4.73it/s]Epoch: 13, train for the 34-th batch, train loss: 0.6273912191390991:  23%|██▋         | 34/151 [00:06<00:24,  4.69it/s]Epoch: 4, train for the 226-th batch, train loss: 0.6350059509277344:  95%|██████████▍| 225/237 [02:12<00:07,  1.65it/s]Epoch: 4, train for the 226-th batch, train loss: 0.6350059509277344:  95%|██████████▍| 226/237 [02:12<00:06,  1.65it/s]Epoch: 8, train for the 37-th batch, train loss: 0.16175518929958344:  30%|███▋        | 36/119 [00:20<00:47,  1.76it/s]Epoch: 8, train for the 37-th batch, train loss: 0.16175518929958344:  31%|███▋        | 37/119 [00:20<00:47,  1.74it/s]Epoch: 3, train for the 131-th batch, train loss: 0.3849141597747803:  34%|███▋       | 130/383 [01:18<02:33,  1.65it/s]Epoch: 3, train for the 131-th batch, train loss: 0.3849141597747803:  34%|███▊       | 131/383 [01:18<02:32,  1.65it/s]Epoch: 13, train for the 35-th batch, train loss: 0.6677746176719666:  23%|██▋         | 34/151 [00:07<00:24,  4.69it/s]Epoch: 13, train for the 35-th batch, train loss: 0.6677746176719666:  23%|██▊         | 35/151 [00:07<00:24,  4.65it/s]Epoch: 7, train for the 31-th batch, train loss: 0.4912837743759155:  21%|██▋          | 30/146 [00:18<01:11,  1.63it/s]Epoch: 7, train for the 31-th batch, train loss: 0.4912837743759155:  21%|██▊          | 31/146 [00:18<01:05,  1.75it/s]Epoch: 13, train for the 36-th batch, train loss: 0.5344184637069702:  23%|██▊         | 35/151 [00:07<00:24,  4.65it/s]Epoch: 13, train for the 36-th batch, train loss: 0.5344184637069702:  24%|██▊         | 36/151 [00:07<00:24,  4.70it/s]Epoch: 13, train for the 37-th batch, train loss: 0.6390967965126038:  24%|██▊         | 36/151 [00:07<00:24,  4.70it/s]Epoch: 13, train for the 37-th batch, train loss: 0.6390967965126038:  25%|██▉         | 37/151 [00:07<00:24,  4.69it/s]Epoch: 4, train for the 227-th batch, train loss: 0.6346067786216736:  95%|██████████▍| 226/237 [02:13<00:06,  1.65it/s]Epoch: 4, train for the 227-th batch, train loss: 0.6346067786216736:  96%|██████████▌| 227/237 [02:13<00:06,  1.65it/s]Epoch: 3, train for the 132-th batch, train loss: 0.427486777305603:  34%|████        | 131/383 [01:19<02:32,  1.65it/s]Epoch: 3, train for the 132-th batch, train loss: 0.427486777305603:  34%|████▏       | 132/383 [01:19<02:31,  1.65it/s]Epoch: 8, train for the 38-th batch, train loss: 0.1339954286813736:  31%|████         | 37/119 [00:21<00:47,  1.74it/s]Epoch: 8, train for the 38-th batch, train loss: 0.1339954286813736:  32%|████▏        | 38/119 [00:21<00:49,  1.63it/s]Epoch: 7, train for the 32-th batch, train loss: 0.4343559145927429:  21%|██▊          | 31/146 [00:18<01:05,  1.75it/s]Epoch: 7, train for the 32-th batch, train loss: 0.4343559145927429:  22%|██▊          | 32/146 [00:18<01:04,  1.77it/s]Epoch: 13, train for the 38-th batch, train loss: 0.5017575621604919:  25%|██▉         | 37/151 [00:07<00:24,  4.69it/s]Epoch: 13, train for the 38-th batch, train loss: 0.5017575621604919:  25%|███         | 38/151 [00:07<00:23,  4.72it/s]Epoch: 13, train for the 39-th batch, train loss: 0.6547306776046753:  25%|███         | 38/151 [00:07<00:23,  4.72it/s]Epoch: 13, train for the 39-th batch, train loss: 0.6547306776046753:  26%|███         | 39/151 [00:07<00:23,  4.67it/s]Epoch: 4, train for the 228-th batch, train loss: 0.627372145652771:  96%|███████████▍| 227/237 [02:13<00:06,  1.65it/s]Epoch: 4, train for the 228-th batch, train loss: 0.627372145652771:  96%|███████████▌| 228/237 [02:13<00:05,  1.65it/s]Epoch: 13, train for the 40-th batch, train loss: 0.5870808959007263:  26%|███         | 39/151 [00:08<00:23,  4.67it/s]Epoch: 13, train for the 40-th batch, train loss: 0.5870808959007263:  26%|███▏        | 40/151 [00:08<00:23,  4.64it/s]Epoch: 3, train for the 133-th batch, train loss: 0.4480706751346588:  34%|███▊       | 132/383 [01:19<02:31,  1.65it/s]Epoch: 3, train for the 133-th batch, train loss: 0.4480706751346588:  35%|███▊       | 133/383 [01:19<02:31,  1.65it/s]Epoch: 7, train for the 33-th batch, train loss: 0.4585183262825012:  22%|██▊          | 32/146 [00:19<01:04,  1.77it/s]Epoch: 8, train for the 39-th batch, train loss: 0.213449165225029:  32%|████▍         | 38/119 [00:22<00:49,  1.63it/s]Epoch: 7, train for the 33-th batch, train loss: 0.4585183262825012:  23%|██▉          | 33/146 [00:19<01:06,  1.71it/s]Epoch: 8, train for the 39-th batch, train loss: 0.213449165225029:  33%|████▌         | 39/119 [00:22<00:50,  1.59it/s]Epoch: 13, train for the 41-th batch, train loss: 0.5613124370574951:  26%|███▏        | 40/151 [00:08<00:23,  4.64it/s]Epoch: 13, train for the 41-th batch, train loss: 0.5613124370574951:  27%|███▎        | 41/151 [00:08<00:23,  4.65it/s]Epoch: 13, train for the 42-th batch, train loss: 0.6253355741500854:  27%|███▎        | 41/151 [00:08<00:23,  4.65it/s]Epoch: 13, train for the 42-th batch, train loss: 0.6253355741500854:  28%|███▎        | 42/151 [00:08<00:23,  4.61it/s]Epoch: 4, train for the 229-th batch, train loss: 0.6546088457107544:  96%|██████████▌| 228/237 [02:14<00:05,  1.65it/s]Epoch: 4, train for the 229-th batch, train loss: 0.6546088457107544:  97%|██████████▋| 229/237 [02:14<00:04,  1.75it/s]Epoch: 13, train for the 43-th batch, train loss: 0.6071909666061401:  28%|███▎        | 42/151 [00:08<00:23,  4.61it/s]Epoch: 13, train for the 43-th batch, train loss: 0.6071909666061401:  28%|███▍        | 43/151 [00:08<00:23,  4.61it/s]Epoch: 8, train for the 40-th batch, train loss: 0.16115964949131012:  33%|███▉        | 39/119 [00:22<00:50,  1.59it/s]Epoch: 7, train for the 34-th batch, train loss: 0.4452858567237854:  23%|██▉          | 33/146 [00:20<01:06,  1.71it/s]Epoch: 8, train for the 40-th batch, train loss: 0.16115964949131012:  34%|████        | 40/119 [00:22<00:50,  1.57it/s]Epoch: 7, train for the 34-th batch, train loss: 0.4452858567237854:  23%|███          | 34/146 [00:20<01:08,  1.64it/s]Epoch: 13, train for the 44-th batch, train loss: 0.6144368052482605:  28%|███▍        | 43/151 [00:09<00:23,  4.61it/s]Epoch: 13, train for the 44-th batch, train loss: 0.6144368052482605:  29%|███▍        | 44/151 [00:09<00:23,  4.58it/s]Epoch: 4, train for the 230-th batch, train loss: 0.6242950558662415:  97%|██████████▋| 229/237 [02:14<00:04,  1.75it/s]Epoch: 4, train for the 230-th batch, train loss: 0.6242950558662415:  97%|██████████▋| 230/237 [02:14<00:03,  1.90it/s]Epoch: 3, train for the 134-th batch, train loss: 0.4262230694293976:  35%|███▊       | 133/383 [01:20<02:31,  1.65it/s]Epoch: 3, train for the 134-th batch, train loss: 0.4262230694293976:  35%|███▊       | 134/383 [01:20<02:54,  1.43it/s]Epoch: 13, train for the 45-th batch, train loss: 0.45741522312164307:  29%|███▏       | 44/151 [00:09<00:23,  4.58it/s]Epoch: 13, train for the 45-th batch, train loss: 0.45741522312164307:  30%|███▎       | 45/151 [00:09<00:22,  4.64it/s]Epoch: 4, train for the 231-th batch, train loss: 0.6434580087661743:  97%|██████████▋| 230/237 [02:15<00:03,  1.90it/s]Epoch: 4, train for the 231-th batch, train loss: 0.6434580087661743:  97%|██████████▋| 231/237 [02:15<00:02,  2.05it/s]Epoch: 13, train for the 46-th batch, train loss: 0.6039173603057861:  30%|███▌        | 45/151 [00:09<00:22,  4.64it/s]Epoch: 13, train for the 46-th batch, train loss: 0.6039173603057861:  30%|███▋        | 46/151 [00:09<00:22,  4.62it/s]Epoch: 8, train for the 41-th batch, train loss: 0.23059134185314178:  34%|████        | 40/119 [00:23<00:50,  1.57it/s]Epoch: 7, train for the 35-th batch, train loss: 0.41189277172088623:  23%|██▊         | 34/146 [00:20<01:08,  1.64it/s]Epoch: 8, train for the 41-th batch, train loss: 0.23059134185314178:  34%|████▏       | 41/119 [00:23<00:50,  1.55it/s]Epoch: 7, train for the 35-th batch, train loss: 0.41189277172088623:  24%|██▉         | 35/146 [00:20<01:09,  1.60it/s]Epoch: 13, train for the 47-th batch, train loss: 0.6410775780677795:  30%|███▋        | 46/151 [00:09<00:22,  4.62it/s]Epoch: 13, train for the 47-th batch, train loss: 0.6410775780677795:  31%|███▋        | 47/151 [00:09<00:22,  4.60it/s]Epoch: 3, train for the 135-th batch, train loss: 0.3637467622756958:  35%|███▊       | 134/383 [01:21<02:54,  1.43it/s]Epoch: 3, train for the 135-th batch, train loss: 0.3637467622756958:  35%|███▉       | 135/383 [01:21<02:45,  1.50it/s]Epoch: 13, train for the 48-th batch, train loss: 0.6108821630477905:  31%|███▋        | 47/151 [00:09<00:22,  4.60it/s]Epoch: 13, train for the 48-th batch, train loss: 0.6108821630477905:  32%|███▊        | 48/151 [00:09<00:22,  4.58it/s]Epoch: 4, train for the 232-th batch, train loss: 0.6169958114624023:  97%|██████████▋| 231/237 [02:15<00:02,  2.05it/s]Epoch: 4, train for the 232-th batch, train loss: 0.6169958114624023:  98%|██████████▊| 232/237 [02:15<00:02,  1.96it/s]Epoch: 13, train for the 49-th batch, train loss: 0.5333803296089172:  32%|███▊        | 48/151 [00:10<00:22,  4.58it/s]Epoch: 13, train for the 49-th batch, train loss: 0.5333803296089172:  32%|███▉        | 49/151 [00:10<00:22,  4.58it/s]Epoch: 13, train for the 50-th batch, train loss: 0.5008043646812439:  32%|███▉        | 49/151 [00:10<00:22,  4.58it/s]Epoch: 13, train for the 50-th batch, train loss: 0.5008043646812439:  33%|███▉        | 50/151 [00:10<00:21,  4.59it/s]Epoch: 8, train for the 42-th batch, train loss: 0.2541419565677643:  34%|████▍        | 41/119 [00:24<00:50,  1.55it/s]Epoch: 8, train for the 42-th batch, train loss: 0.2541419565677643:  35%|████▌        | 42/119 [00:24<00:50,  1.54it/s]Epoch: 7, train for the 36-th batch, train loss: 0.4336635172367096:  24%|███          | 35/146 [00:21<01:09,  1.60it/s]Epoch: 7, train for the 36-th batch, train loss: 0.4336635172367096:  25%|███▏         | 36/146 [00:21<01:10,  1.57it/s]Epoch: 3, train for the 136-th batch, train loss: 0.40228530764579773:  35%|███▌      | 135/383 [01:21<02:45,  1.50it/s]Epoch: 3, train for the 136-th batch, train loss: 0.40228530764579773:  36%|███▌      | 136/383 [01:21<02:40,  1.54it/s]Epoch: 13, train for the 51-th batch, train loss: 0.38275641202926636:  33%|███▋       | 50/151 [00:10<00:21,  4.59it/s]Epoch: 13, train for the 51-th batch, train loss: 0.38275641202926636:  34%|███▋       | 51/151 [00:10<00:21,  4.65it/s]Epoch: 4, train for the 233-th batch, train loss: 0.6377206444740295:  98%|██████████▊| 232/237 [02:16<00:02,  1.96it/s]Epoch: 4, train for the 233-th batch, train loss: 0.6377206444740295:  98%|██████████▊| 233/237 [02:16<00:02,  1.89it/s]Epoch: 13, train for the 52-th batch, train loss: 0.6177646517753601:  34%|████        | 51/151 [00:10<00:21,  4.65it/s]Epoch: 13, train for the 52-th batch, train loss: 0.6177646517753601:  34%|████▏       | 52/151 [00:10<00:21,  4.62it/s]Epoch: 13, train for the 53-th batch, train loss: 0.6190935373306274:  34%|████▏       | 52/151 [00:10<00:21,  4.62it/s]Epoch: 13, train for the 53-th batch, train loss: 0.6190935373306274:  35%|████▏       | 53/151 [00:10<00:21,  4.59it/s]Epoch: 8, train for the 43-th batch, train loss: 0.19893778860569:  35%|█████▎         | 42/119 [00:24<00:50,  1.54it/s]Epoch: 7, train for the 37-th batch, train loss: 0.4918437898159027:  25%|███▏         | 36/146 [00:22<01:10,  1.57it/s]Epoch: 8, train for the 43-th batch, train loss: 0.19893778860569:  36%|█████▍         | 43/119 [00:24<00:49,  1.53it/s]Epoch: 7, train for the 37-th batch, train loss: 0.4918437898159027:  25%|███▎         | 37/146 [00:22<01:10,  1.55it/s]Epoch: 3, train for the 137-th batch, train loss: 0.3527311682701111:  36%|███▉       | 136/383 [01:22<02:40,  1.54it/s]Epoch: 3, train for the 137-th batch, train loss: 0.3527311682701111:  36%|███▉       | 137/383 [01:22<02:38,  1.55it/s]Epoch: 4, train for the 234-th batch, train loss: 0.6120339632034302:  98%|██████████▊| 233/237 [02:16<00:02,  1.89it/s]Epoch: 4, train for the 234-th batch, train loss: 0.6120339632034302:  99%|██████████▊| 234/237 [02:16<00:01,  1.80it/s]Epoch: 13, train for the 54-th batch, train loss: 0.5794051885604858:  35%|████▏       | 53/151 [00:11<00:21,  4.59it/s]Epoch: 13, train for the 54-th batch, train loss: 0.5794051885604858:  36%|████▎       | 54/151 [00:11<00:21,  4.58it/s]Epoch: 13, train for the 55-th batch, train loss: 0.4287458658218384:  36%|████▎       | 54/151 [00:11<00:21,  4.58it/s]Epoch: 13, train for the 55-th batch, train loss: 0.4287458658218384:  36%|████▎       | 55/151 [00:11<00:20,  4.60it/s]Epoch: 13, train for the 56-th batch, train loss: 0.5374587178230286:  36%|████▎       | 55/151 [00:11<00:20,  4.60it/s]Epoch: 13, train for the 56-th batch, train loss: 0.5374587178230286:  37%|████▍       | 56/151 [00:11<00:20,  4.57it/s]Epoch: 3, train for the 138-th batch, train loss: 0.3175840973854065:  36%|███▉       | 137/383 [01:22<02:38,  1.55it/s]Epoch: 3, train for the 138-th batch, train loss: 0.3175840973854065:  36%|███▉       | 138/383 [01:22<02:36,  1.57it/s]Epoch: 8, train for the 44-th batch, train loss: 0.17860722541809082:  36%|████▎       | 43/119 [00:25<00:49,  1.53it/s]Epoch: 7, train for the 38-th batch, train loss: 0.47052329778671265:  25%|███         | 37/146 [00:22<01:10,  1.55it/s]Epoch: 8, train for the 44-th batch, train loss: 0.17860722541809082:  37%|████▍       | 44/119 [00:25<00:49,  1.52it/s]Epoch: 7, train for the 38-th batch, train loss: 0.47052329778671265:  26%|███         | 38/146 [00:22<01:10,  1.54it/s]Epoch: 4, train for the 235-th batch, train loss: 0.640084981918335:  99%|███████████▊| 234/237 [02:17<00:01,  1.80it/s]Epoch: 4, train for the 235-th batch, train loss: 0.640084981918335:  99%|███████████▉| 235/237 [02:17<00:01,  1.74it/s]Epoch: 13, train for the 57-th batch, train loss: 0.5377878546714783:  37%|████▍       | 56/151 [00:11<00:20,  4.57it/s]Epoch: 13, train for the 57-th batch, train loss: 0.5377878546714783:  38%|████▌       | 57/151 [00:11<00:20,  4.57it/s]Epoch: 13, train for the 58-th batch, train loss: 0.5423194766044617:  38%|████▌       | 57/151 [00:12<00:20,  4.57it/s]Epoch: 13, train for the 58-th batch, train loss: 0.5423194766044617:  38%|████▌       | 58/151 [00:12<00:20,  4.56it/s]Epoch: 3, train for the 139-th batch, train loss: 0.38007986545562744:  36%|███▌      | 138/383 [01:23<02:36,  1.57it/s]Epoch: 3, train for the 139-th batch, train loss: 0.38007986545562744:  36%|███▋      | 139/383 [01:23<02:33,  1.59it/s]Epoch: 13, train for the 59-th batch, train loss: 0.4856896996498108:  38%|████▌       | 58/151 [00:12<00:20,  4.56it/s]Epoch: 13, train for the 59-th batch, train loss: 0.4856896996498108:  39%|████▋       | 59/151 [00:12<00:20,  4.57it/s]Epoch: 8, train for the 45-th batch, train loss: 0.19085191190242767:  37%|████▍       | 44/119 [00:26<00:49,  1.52it/s]Epoch: 7, train for the 39-th batch, train loss: 0.453550785779953:  26%|███▋          | 38/146 [00:23<01:10,  1.54it/s]Epoch: 8, train for the 45-th batch, train loss: 0.19085191190242767:  38%|████▌       | 45/119 [00:26<00:48,  1.51it/s]Epoch: 7, train for the 39-th batch, train loss: 0.453550785779953:  27%|███▋          | 39/146 [00:23<01:10,  1.53it/s]Epoch: 4, train for the 236-th batch, train loss: 0.6022505760192871:  99%|██████████▉| 235/237 [02:18<00:01,  1.74it/s]Epoch: 4, train for the 236-th batch, train loss: 0.6022505760192871: 100%|██████████▉| 236/237 [02:18<00:00,  1.71it/s]Epoch: 13, train for the 60-th batch, train loss: 0.48985666036605835:  39%|████▎      | 59/151 [00:12<00:20,  4.57it/s]Epoch: 13, train for the 60-th batch, train loss: 0.48985666036605835:  40%|████▎      | 60/151 [00:12<00:19,  4.58it/s]Epoch: 13, train for the 61-th batch, train loss: 0.4594328999519348:  40%|████▊       | 60/151 [00:12<00:19,  4.58it/s]Epoch: 13, train for the 61-th batch, train loss: 0.4594328999519348:  40%|████▊       | 61/151 [00:12<00:19,  4.60it/s]Epoch: 3, train for the 140-th batch, train loss: 0.3695842921733856:  36%|███▉       | 139/383 [01:24<02:33,  1.59it/s]Epoch: 3, train for the 140-th batch, train loss: 0.3695842921733856:  37%|████       | 140/383 [01:24<02:32,  1.59it/s]Epoch: 4, train for the 237-th batch, train loss: 0.6132937073707581: 100%|██████████▉| 236/237 [02:18<00:00,  1.71it/s]Epoch: 4, train for the 237-th batch, train loss: 0.6132937073707581: 100%|███████████| 237/237 [02:18<00:00,  1.81it/s]Epoch: 4, train for the 237-th batch, train loss: 0.6132937073707581: 100%|███████████| 237/237 [02:18<00:00,  1.71it/s]
  0%|                                                                                            | 0/66 [00:00<?, ?it/s]Epoch: 13, train for the 62-th batch, train loss: 0.598956286907196:  40%|█████▎       | 61/151 [00:12<00:19,  4.60it/s]Epoch: 13, train for the 62-th batch, train loss: 0.598956286907196:  41%|█████▎       | 62/151 [00:12<00:19,  4.58it/s]Epoch: 8, train for the 46-th batch, train loss: 0.20957623422145844:  38%|████▌       | 45/119 [00:26<00:48,  1.51it/s]Epoch: 7, train for the 40-th batch, train loss: 0.47557204961776733:  27%|███▏        | 39/146 [00:24<01:10,  1.53it/s]Epoch: 8, train for the 46-th batch, train loss: 0.20957623422145844:  39%|████▋       | 46/119 [00:26<00:48,  1.51it/s]Epoch: 7, train for the 40-th batch, train loss: 0.47557204961776733:  27%|███▎        | 40/146 [00:24<01:09,  1.52it/s]Epoch: 13, train for the 63-th batch, train loss: 0.37946394085884094:  41%|████▌      | 62/151 [00:13<00:19,  4.58it/s]Epoch: 13, train for the 63-th batch, train loss: 0.37946394085884094:  42%|████▌      | 63/151 [00:13<00:19,  4.62it/s]evaluate for the 1-th batch, evaluate loss: 0.5905746221542358:   0%|                            | 0/66 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.5905746221542358:   2%|▎                   | 1/66 [00:00<00:17,  3.70it/s]Epoch: 13, train for the 64-th batch, train loss: 0.5302967429161072:  42%|█████       | 63/151 [00:13<00:19,  4.62it/s]Epoch: 13, train for the 64-th batch, train loss: 0.5302967429161072:  42%|█████       | 64/151 [00:13<00:18,  4.59it/s]evaluate for the 2-th batch, evaluate loss: 0.5760457515716553:   2%|▎                   | 1/66 [00:00<00:17,  3.70it/s]evaluate for the 2-th batch, evaluate loss: 0.5760457515716553:   3%|▌                   | 2/66 [00:00<00:18,  3.53it/s]Epoch: 3, train for the 141-th batch, train loss: 0.4320950210094452:  37%|████       | 140/383 [01:24<02:32,  1.59it/s]Epoch: 3, train for the 141-th batch, train loss: 0.4320950210094452:  37%|████       | 141/383 [01:24<02:29,  1.62it/s]Epoch: 13, train for the 65-th batch, train loss: 0.5078474879264832:  42%|█████       | 64/151 [00:13<00:18,  4.59it/s]Epoch: 13, train for the 65-th batch, train loss: 0.5078474879264832:  43%|█████▏      | 65/151 [00:13<00:18,  4.57it/s]Epoch: 8, train for the 47-th batch, train loss: 0.2564264237880707:  39%|█████        | 46/119 [00:27<00:48,  1.51it/s]Epoch: 7, train for the 41-th batch, train loss: 0.434640109539032:  27%|███▊          | 40/146 [00:24<01:09,  1.52it/s]Epoch: 8, train for the 47-th batch, train loss: 0.2564264237880707:  39%|█████▏       | 47/119 [00:27<00:47,  1.51it/s]Epoch: 7, train for the 41-th batch, train loss: 0.434640109539032:  28%|███▉          | 41/146 [00:24<01:09,  1.51it/s]evaluate for the 3-th batch, evaluate loss: 0.6114316582679749:   3%|▌                   | 2/66 [00:00<00:18,  3.53it/s]evaluate for the 3-th batch, evaluate loss: 0.6114316582679749:   5%|▉                   | 3/66 [00:00<00:16,  3.72it/s]Epoch: 13, train for the 66-th batch, train loss: 0.5479986667633057:  43%|█████▏      | 65/151 [00:13<00:18,  4.57it/s]Epoch: 13, train for the 66-th batch, train loss: 0.5479986667633057:  44%|█████▏      | 66/151 [00:13<00:18,  4.54it/s]evaluate for the 4-th batch, evaluate loss: 0.609861433506012:   5%|▉                    | 3/66 [00:01<00:16,  3.72it/s]evaluate for the 4-th batch, evaluate loss: 0.609861433506012:   6%|█▎                   | 4/66 [00:01<00:17,  3.62it/s]Epoch: 13, train for the 67-th batch, train loss: 0.5697211623191833:  44%|█████▏      | 66/151 [00:14<00:18,  4.54it/s]Epoch: 13, train for the 67-th batch, train loss: 0.5697211623191833:  44%|█████▎      | 67/151 [00:14<00:18,  4.53it/s]Epoch: 3, train for the 142-th batch, train loss: 0.3374152183532715:  37%|████       | 141/383 [01:25<02:29,  1.62it/s]Epoch: 3, train for the 142-th batch, train loss: 0.3374152183532715:  37%|████       | 142/383 [01:25<02:27,  1.64it/s]evaluate for the 5-th batch, evaluate loss: 0.6284876465797424:   6%|█▏                  | 4/66 [00:01<00:17,  3.62it/s]evaluate for the 5-th batch, evaluate loss: 0.6284876465797424:   8%|█▌                  | 5/66 [00:01<00:16,  3.71it/s]Epoch: 13, train for the 68-th batch, train loss: 0.4909500777721405:  44%|█████▎      | 67/151 [00:14<00:18,  4.53it/s]Epoch: 13, train for the 68-th batch, train loss: 0.4909500777721405:  45%|█████▍      | 68/151 [00:14<00:18,  4.56it/s]Epoch: 8, train for the 48-th batch, train loss: 0.2260095477104187:  39%|█████▏       | 47/119 [00:28<00:47,  1.51it/s]Epoch: 8, train for the 48-th batch, train loss: 0.2260095477104187:  40%|█████▏       | 48/119 [00:28<00:47,  1.51it/s]Epoch: 7, train for the 42-th batch, train loss: 0.45281317830085754:  28%|███▎        | 41/146 [00:25<01:09,  1.51it/s]Epoch: 7, train for the 42-th batch, train loss: 0.45281317830085754:  29%|███▍        | 42/146 [00:25<01:08,  1.51it/s]Epoch: 13, train for the 69-th batch, train loss: 0.5444667935371399:  45%|█████▍      | 68/151 [00:14<00:18,  4.56it/s]Epoch: 13, train for the 69-th batch, train loss: 0.5444667935371399:  46%|█████▍      | 69/151 [00:14<00:17,  4.56it/s]evaluate for the 6-th batch, evaluate loss: 0.654181182384491:   8%|█▌                   | 5/66 [00:01<00:16,  3.71it/s]evaluate for the 6-th batch, evaluate loss: 0.654181182384491:   9%|█▉                   | 6/66 [00:01<00:16,  3.66it/s]Epoch: 3, train for the 143-th batch, train loss: 0.3579753339290619:  37%|████       | 142/383 [01:26<02:27,  1.64it/s]Epoch: 3, train for the 143-th batch, train loss: 0.3579753339290619:  37%|████       | 143/383 [01:26<02:25,  1.64it/s]Epoch: 13, train for the 70-th batch, train loss: 0.5433043837547302:  46%|█████▍      | 69/151 [00:14<00:17,  4.56it/s]Epoch: 13, train for the 70-th batch, train loss: 0.5433043837547302:  46%|█████▌      | 70/151 [00:14<00:17,  4.56it/s]evaluate for the 7-th batch, evaluate loss: 0.6118497848510742:   9%|█▊                  | 6/66 [00:01<00:16,  3.66it/s]evaluate for the 7-th batch, evaluate loss: 0.6118497848510742:  11%|██                  | 7/66 [00:01<00:16,  3.60it/s]Epoch: 13, train for the 71-th batch, train loss: 0.5368685126304626:  46%|█████▌      | 70/151 [00:14<00:17,  4.56it/s]Epoch: 13, train for the 71-th batch, train loss: 0.5368685126304626:  47%|█████▋      | 71/151 [00:14<00:17,  4.55it/s]Epoch: 8, train for the 49-th batch, train loss: 0.20795848965644836:  40%|████▊       | 48/119 [00:28<00:47,  1.51it/s]Epoch: 7, train for the 43-th batch, train loss: 0.45585569739341736:  29%|███▍        | 42/146 [00:26<01:08,  1.51it/s]Epoch: 8, train for the 49-th batch, train loss: 0.20795848965644836:  41%|████▉       | 49/119 [00:28<00:46,  1.51it/s]Epoch: 7, train for the 43-th batch, train loss: 0.45585569739341736:  29%|███▌        | 43/146 [00:26<01:08,  1.51it/s]evaluate for the 8-th batch, evaluate loss: 0.6128476858139038:  11%|██                  | 7/66 [00:02<00:16,  3.60it/s]evaluate for the 8-th batch, evaluate loss: 0.6128476858139038:  12%|██▍                 | 8/66 [00:02<00:16,  3.62it/s]Epoch: 13, train for the 72-th batch, train loss: 0.5253661870956421:  47%|█████▋      | 71/151 [00:15<00:17,  4.55it/s]Epoch: 13, train for the 72-th batch, train loss: 0.5253661870956421:  48%|█████▋      | 72/151 [00:15<00:17,  4.55it/s]Epoch: 3, train for the 144-th batch, train loss: 0.33651861548423767:  37%|███▋      | 143/383 [01:26<02:25,  1.64it/s]Epoch: 3, train for the 144-th batch, train loss: 0.33651861548423767:  38%|███▊      | 144/383 [01:26<02:24,  1.65it/s]Epoch: 13, train for the 73-th batch, train loss: 0.5338228344917297:  48%|█████▋      | 72/151 [00:15<00:17,  4.55it/s]Epoch: 13, train for the 73-th batch, train loss: 0.5338228344917297:  48%|█████▊      | 73/151 [00:15<00:17,  4.54it/s]evaluate for the 9-th batch, evaluate loss: 0.5601341724395752:  12%|██▍                 | 8/66 [00:02<00:16,  3.62it/s]evaluate for the 9-th batch, evaluate loss: 0.5601341724395752:  14%|██▋                 | 9/66 [00:02<00:16,  3.55it/s]Epoch: 13, train for the 74-th batch, train loss: 0.5216339826583862:  48%|█████▊      | 73/151 [00:15<00:17,  4.54it/s]Epoch: 13, train for the 74-th batch, train loss: 0.5216339826583862:  49%|█████▉      | 74/151 [00:15<00:16,  4.54it/s]evaluate for the 10-th batch, evaluate loss: 0.575222909450531:  14%|██▋                 | 9/66 [00:02<00:16,  3.55it/s]evaluate for the 10-th batch, evaluate loss: 0.575222909450531:  15%|██▉                | 10/66 [00:02<00:15,  3.69it/s]Epoch: 8, train for the 50-th batch, train loss: 0.1804889291524887:  41%|█████▎       | 49/119 [00:29<00:46,  1.51it/s]Epoch: 8, train for the 50-th batch, train loss: 0.1804889291524887:  42%|█████▍       | 50/119 [00:29<00:45,  1.51it/s]Epoch: 7, train for the 44-th batch, train loss: 0.4540027379989624:  29%|███▊         | 43/146 [00:26<01:08,  1.51it/s]Epoch: 7, train for the 44-th batch, train loss: 0.4540027379989624:  30%|███▉         | 44/146 [00:26<01:07,  1.51it/s]Epoch: 13, train for the 75-th batch, train loss: 0.5582970380783081:  49%|█████▉      | 74/151 [00:15<00:16,  4.54it/s]Epoch: 13, train for the 75-th batch, train loss: 0.5582970380783081:  50%|█████▉      | 75/151 [00:15<00:16,  4.53it/s]Epoch: 3, train for the 145-th batch, train loss: 0.37802016735076904:  38%|███▊      | 144/383 [01:27<02:24,  1.65it/s]Epoch: 3, train for the 145-th batch, train loss: 0.37802016735076904:  38%|███▊      | 145/383 [01:27<02:22,  1.67it/s]evaluate for the 11-th batch, evaluate loss: 0.5814149975776672:  15%|██▋               | 10/66 [00:03<00:15,  3.69it/s]evaluate for the 11-th batch, evaluate loss: 0.5814149975776672:  17%|███               | 11/66 [00:03<00:15,  3.57it/s]Epoch: 13, train for the 76-th batch, train loss: 0.47909027338027954:  50%|█████▍     | 75/151 [00:16<00:16,  4.53it/s]Epoch: 13, train for the 76-th batch, train loss: 0.47909027338027954:  50%|█████▌     | 76/151 [00:16<00:16,  4.57it/s]evaluate for the 12-th batch, evaluate loss: 0.6223918199539185:  17%|███               | 11/66 [00:03<00:15,  3.57it/s]evaluate for the 12-th batch, evaluate loss: 0.6223918199539185:  18%|███▎              | 12/66 [00:03<00:14,  3.75it/s]Epoch: 13, train for the 77-th batch, train loss: 0.5260800123214722:  50%|██████      | 76/151 [00:16<00:16,  4.57it/s]Epoch: 13, train for the 77-th batch, train loss: 0.5260800123214722:  51%|██████      | 77/151 [00:16<00:16,  4.55it/s]Epoch: 8, train for the 51-th batch, train loss: 0.2210676372051239:  42%|█████▍       | 50/119 [00:30<00:45,  1.51it/s]Epoch: 8, train for the 51-th batch, train loss: 0.2210676372051239:  43%|█████▌       | 51/119 [00:30<00:45,  1.51it/s]Epoch: 7, train for the 45-th batch, train loss: 0.4591320753097534:  30%|███▉         | 44/146 [00:27<01:07,  1.51it/s]Epoch: 7, train for the 45-th batch, train loss: 0.4591320753097534:  31%|████         | 45/146 [00:27<01:06,  1.51it/s]Epoch: 3, train for the 146-th batch, train loss: 0.32359862327575684:  38%|███▊      | 145/383 [01:27<02:22,  1.67it/s]Epoch: 3, train for the 146-th batch, train loss: 0.32359862327575684:  38%|███▊      | 146/383 [01:27<02:22,  1.67it/s]evaluate for the 13-th batch, evaluate loss: 0.5987287759780884:  18%|███▎              | 12/66 [00:03<00:14,  3.75it/s]evaluate for the 13-th batch, evaluate loss: 0.5987287759780884:  20%|███▌              | 13/66 [00:03<00:14,  3.57it/s]Epoch: 13, train for the 78-th batch, train loss: 0.5290181636810303:  51%|██████      | 77/151 [00:16<00:16,  4.55it/s]Epoch: 13, train for the 78-th batch, train loss: 0.5290181636810303:  52%|██████▏     | 78/151 [00:16<00:17,  4.09it/s]evaluate for the 14-th batch, evaluate loss: 0.5983235836029053:  20%|███▌              | 13/66 [00:03<00:14,  3.57it/s]evaluate for the 14-th batch, evaluate loss: 0.5983235836029053:  21%|███▊              | 14/66 [00:03<00:14,  3.65it/s]Epoch: 13, train for the 79-th batch, train loss: 0.6142396926879883:  52%|██████▏     | 78/151 [00:16<00:17,  4.09it/s]Epoch: 13, train for the 79-th batch, train loss: 0.6142396926879883:  52%|██████▎     | 79/151 [00:16<00:17,  4.20it/s]Epoch: 8, train for the 52-th batch, train loss: 0.2518670856952667:  43%|█████▌       | 51/119 [00:30<00:45,  1.51it/s]Epoch: 7, train for the 46-th batch, train loss: 0.4263402819633484:  31%|████         | 45/146 [00:28<01:06,  1.51it/s]Epoch: 8, train for the 52-th batch, train loss: 0.2518670856952667:  44%|█████▋       | 52/119 [00:30<00:44,  1.51it/s]Epoch: 7, train for the 46-th batch, train loss: 0.4263402819633484:  32%|████         | 46/146 [00:28<01:06,  1.51it/s]Epoch: 13, train for the 80-th batch, train loss: 0.5190852880477905:  52%|██████▎     | 79/151 [00:16<00:17,  4.20it/s]Epoch: 13, train for the 80-th batch, train loss: 0.5190852880477905:  53%|██████▎     | 80/151 [00:16<00:16,  4.31it/s]evaluate for the 15-th batch, evaluate loss: 0.6308256387710571:  21%|███▊              | 14/66 [00:04<00:14,  3.65it/s]evaluate for the 15-th batch, evaluate loss: 0.6308256387710571:  23%|████              | 15/66 [00:04<00:14,  3.54it/s]Epoch: 3, train for the 147-th batch, train loss: 0.3168412446975708:  38%|████▏      | 146/383 [01:28<02:22,  1.67it/s]Epoch: 3, train for the 147-th batch, train loss: 0.3168412446975708:  38%|████▏      | 147/383 [01:28<02:21,  1.67it/s]Epoch: 13, train for the 81-th batch, train loss: 0.5627474188804626:  53%|██████▎     | 80/151 [00:17<00:16,  4.31it/s]Epoch: 13, train for the 81-th batch, train loss: 0.5627474188804626:  54%|██████▍     | 81/151 [00:17<00:16,  4.37it/s]evaluate for the 16-th batch, evaluate loss: 0.5709864497184753:  23%|████              | 15/66 [00:04<00:14,  3.54it/s]evaluate for the 16-th batch, evaluate loss: 0.5709864497184753:  24%|████▎             | 16/66 [00:04<00:13,  3.66it/s]Epoch: 13, train for the 82-th batch, train loss: 0.552410900592804:  54%|██████▉      | 81/151 [00:17<00:16,  4.37it/s]Epoch: 13, train for the 82-th batch, train loss: 0.552410900592804:  54%|███████      | 82/151 [00:17<00:15,  4.42it/s]evaluate for the 17-th batch, evaluate loss: 0.6073514223098755:  24%|████▎             | 16/66 [00:04<00:13,  3.66it/s]evaluate for the 17-th batch, evaluate loss: 0.6073514223098755:  26%|████▋             | 17/66 [00:04<00:13,  3.59it/s]Epoch: 3, train for the 148-th batch, train loss: 0.37384361028671265:  38%|███▊      | 147/383 [01:28<02:21,  1.67it/s]Epoch: 3, train for the 148-th batch, train loss: 0.37384361028671265:  39%|███▊      | 148/383 [01:28<02:19,  1.68it/s]Epoch: 8, train for the 53-th batch, train loss: 0.19152355194091797:  44%|█████▏      | 52/119 [00:31<00:44,  1.51it/s]Epoch: 7, train for the 47-th batch, train loss: 0.4923649728298187:  32%|████         | 46/146 [00:28<01:06,  1.51it/s]Epoch: 8, train for the 53-th batch, train loss: 0.19152355194091797:  45%|█████▎      | 53/119 [00:31<00:43,  1.51it/s]Epoch: 7, train for the 47-th batch, train loss: 0.4923649728298187:  32%|████▏        | 47/146 [00:28<01:05,  1.51it/s]Epoch: 13, train for the 83-th batch, train loss: 0.5540552139282227:  54%|██████▌     | 82/151 [00:17<00:15,  4.42it/s]Epoch: 13, train for the 83-th batch, train loss: 0.5540552139282227:  55%|██████▌     | 83/151 [00:17<00:15,  4.45it/s]evaluate for the 18-th batch, evaluate loss: 0.6084779500961304:  26%|████▋             | 17/66 [00:04<00:13,  3.59it/s]evaluate for the 18-th batch, evaluate loss: 0.6084779500961304:  27%|████▉             | 18/66 [00:04<00:13,  3.68it/s]Epoch: 13, train for the 84-th batch, train loss: 0.5831139087677002:  55%|██████▌     | 83/151 [00:17<00:15,  4.45it/s]Epoch: 13, train for the 84-th batch, train loss: 0.5831139087677002:  56%|██████▋     | 84/151 [00:17<00:14,  4.48it/s]Epoch: 13, train for the 85-th batch, train loss: 0.5762379169464111:  56%|██████▋     | 84/151 [00:18<00:14,  4.48it/s]Epoch: 13, train for the 85-th batch, train loss: 0.5762379169464111:  56%|██████▊     | 85/151 [00:18<00:14,  4.48it/s]evaluate for the 19-th batch, evaluate loss: 0.6117678880691528:  27%|████▉             | 18/66 [00:05<00:13,  3.68it/s]evaluate for the 19-th batch, evaluate loss: 0.6117678880691528:  29%|█████▏            | 19/66 [00:05<00:12,  3.64it/s]Epoch: 3, train for the 149-th batch, train loss: 0.42837682366371155:  39%|███▊      | 148/383 [01:29<02:19,  1.68it/s]Epoch: 3, train for the 149-th batch, train loss: 0.42837682366371155:  39%|███▉      | 149/383 [01:29<02:19,  1.67it/s]Epoch: 8, train for the 54-th batch, train loss: 0.16053001582622528:  45%|█████▎      | 53/119 [00:31<00:43,  1.51it/s]Epoch: 7, train for the 48-th batch, train loss: 0.45627081394195557:  32%|███▊        | 47/146 [00:29<01:05,  1.51it/s]Epoch: 8, train for the 54-th batch, train loss: 0.16053001582622528:  45%|█████▍      | 54/119 [00:31<00:43,  1.51it/s]Epoch: 7, train for the 48-th batch, train loss: 0.45627081394195557:  33%|███▉        | 48/146 [00:29<01:05,  1.51it/s]Epoch: 13, train for the 86-th batch, train loss: 0.5470420122146606:  56%|██████▊     | 85/151 [00:18<00:14,  4.48it/s]Epoch: 13, train for the 86-th batch, train loss: 0.5470420122146606:  57%|██████▊     | 86/151 [00:18<00:14,  4.49it/s]evaluate for the 20-th batch, evaluate loss: 0.6221948266029358:  29%|█████▏            | 19/66 [00:05<00:12,  3.64it/s]evaluate for the 20-th batch, evaluate loss: 0.6221948266029358:  30%|█████▍            | 20/66 [00:05<00:12,  3.66it/s]Epoch: 13, train for the 87-th batch, train loss: 0.5388035178184509:  57%|██████▊     | 86/151 [00:18<00:14,  4.49it/s]Epoch: 13, train for the 87-th batch, train loss: 0.5388035178184509:  58%|██████▉     | 87/151 [00:18<00:14,  4.50it/s]evaluate for the 21-th batch, evaluate loss: 0.6285555362701416:  30%|█████▍            | 20/66 [00:05<00:12,  3.66it/s]evaluate for the 21-th batch, evaluate loss: 0.6285555362701416:  32%|█████▋            | 21/66 [00:05<00:12,  3.67it/s]Epoch: 13, train for the 88-th batch, train loss: 0.5078263878822327:  58%|██████▉     | 87/151 [00:18<00:14,  4.50it/s]Epoch: 13, train for the 88-th batch, train loss: 0.5078263878822327:  58%|██████▉     | 88/151 [00:18<00:14,  4.44it/s]Epoch: 3, train for the 150-th batch, train loss: 0.47751182317733765:  39%|███▉      | 149/383 [01:30<02:19,  1.67it/s]Epoch: 3, train for the 150-th batch, train loss: 0.47751182317733765:  39%|███▉      | 150/383 [01:30<02:19,  1.67it/s]evaluate for the 22-th batch, evaluate loss: 0.6199507117271423:  32%|█████▋            | 21/66 [00:06<00:12,  3.67it/s]evaluate for the 22-th batch, evaluate loss: 0.6199507117271423:  33%|██████            | 22/66 [00:06<00:12,  3.58it/s]Epoch: 8, train for the 55-th batch, train loss: 0.16983570158481598:  45%|█████▍      | 54/119 [00:32<00:43,  1.51it/s]Epoch: 8, train for the 55-th batch, train loss: 0.16983570158481598:  46%|█████▌      | 55/119 [00:32<00:42,  1.51it/s]Epoch: 7, train for the 49-th batch, train loss: 0.48488524556159973:  33%|███▉        | 48/146 [00:30<01:05,  1.51it/s]Epoch: 7, train for the 49-th batch, train loss: 0.48488524556159973:  34%|████        | 49/146 [00:30<01:04,  1.51it/s]Epoch: 13, train for the 89-th batch, train loss: 0.5571765303611755:  58%|██████▉     | 88/151 [00:18<00:14,  4.44it/s]Epoch: 13, train for the 89-th batch, train loss: 0.5571765303611755:  59%|███████     | 89/151 [00:18<00:13,  4.47it/s]evaluate for the 23-th batch, evaluate loss: 0.6380295157432556:  33%|██████            | 22/66 [00:06<00:12,  3.58it/s]evaluate for the 23-th batch, evaluate loss: 0.6380295157432556:  35%|██████▎           | 23/66 [00:06<00:11,  3.71it/s]Epoch: 13, train for the 90-th batch, train loss: 0.552735447883606:  59%|███████▋     | 89/151 [00:19<00:13,  4.47it/s]Epoch: 13, train for the 90-th batch, train loss: 0.552735447883606:  60%|███████▋     | 90/151 [00:19<00:13,  4.49it/s]Epoch: 3, train for the 151-th batch, train loss: 0.48580512404441833:  39%|███▉      | 150/383 [01:30<02:19,  1.67it/s]Epoch: 13, train for the 91-th batch, train loss: 0.5057520270347595:  60%|███████▏    | 90/151 [00:19<00:13,  4.49it/s]Epoch: 3, train for the 151-th batch, train loss: 0.48580512404441833:  39%|███▉      | 151/383 [01:30<02:17,  1.68it/s]Epoch: 13, train for the 91-th batch, train loss: 0.5057520270347595:  60%|███████▏    | 91/151 [00:19<00:13,  4.49it/s]evaluate for the 24-th batch, evaluate loss: 0.6296536922454834:  35%|██████▎           | 23/66 [00:06<00:11,  3.71it/s]evaluate for the 24-th batch, evaluate loss: 0.6296536922454834:  36%|██████▌           | 24/66 [00:06<00:11,  3.59it/s]Epoch: 7, train for the 50-th batch, train loss: 0.5134455561637878:  34%|████▎        | 49/146 [00:30<01:04,  1.51it/s]Epoch: 8, train for the 56-th batch, train loss: 0.2024381011724472:  46%|██████       | 55/119 [00:33<00:42,  1.51it/s]Epoch: 7, train for the 50-th batch, train loss: 0.5134455561637878:  34%|████▍        | 50/146 [00:30<01:03,  1.51it/s]Epoch: 8, train for the 56-th batch, train loss: 0.2024381011724472:  47%|██████       | 56/119 [00:33<00:41,  1.51it/s]Epoch: 13, train for the 92-th batch, train loss: 0.5128322243690491:  60%|███████▏    | 91/151 [00:19<00:13,  4.49it/s]Epoch: 13, train for the 92-th batch, train loss: 0.5128322243690491:  61%|███████▎    | 92/151 [00:19<00:13,  4.51it/s]evaluate for the 25-th batch, evaluate loss: 0.6473321318626404:  36%|██████▌           | 24/66 [00:06<00:11,  3.59it/s]evaluate for the 25-th batch, evaluate loss: 0.6473321318626404:  38%|██████▊           | 25/66 [00:06<00:10,  3.76it/s]Epoch: 13, train for the 93-th batch, train loss: 0.5146551728248596:  61%|███████▎    | 92/151 [00:19<00:13,  4.51it/s]Epoch: 13, train for the 93-th batch, train loss: 0.5146551728248596:  62%|███████▍    | 93/151 [00:19<00:12,  4.52it/s]Epoch: 3, train for the 152-th batch, train loss: 0.42948341369628906:  39%|███▉      | 151/383 [01:31<02:17,  1.68it/s]Epoch: 3, train for the 152-th batch, train loss: 0.42948341369628906:  40%|███▉      | 152/383 [01:31<02:17,  1.67it/s]evaluate for the 26-th batch, evaluate loss: 0.6098523139953613:  38%|██████▊           | 25/66 [00:07<00:10,  3.76it/s]evaluate for the 26-th batch, evaluate loss: 0.6098523139953613:  39%|███████           | 26/66 [00:07<00:11,  3.58it/s]Epoch: 13, train for the 94-th batch, train loss: 0.560529887676239:  62%|████████     | 93/151 [00:20<00:12,  4.52it/s]Epoch: 13, train for the 94-th batch, train loss: 0.560529887676239:  62%|████████     | 94/151 [00:20<00:12,  4.50it/s]Epoch: 7, train for the 51-th batch, train loss: 0.46409091353416443:  34%|████        | 50/146 [00:31<01:03,  1.51it/s]Epoch: 8, train for the 57-th batch, train loss: 0.13264119625091553:  47%|█████▋      | 56/119 [00:33<00:41,  1.51it/s]Epoch: 7, train for the 51-th batch, train loss: 0.46409091353416443:  35%|████▏       | 51/146 [00:31<01:03,  1.51it/s]Epoch: 8, train for the 57-th batch, train loss: 0.13264119625091553:  48%|█████▋      | 57/119 [00:33<00:41,  1.51it/s]evaluate for the 27-th batch, evaluate loss: 0.6245414018630981:  39%|███████           | 26/66 [00:07<00:11,  3.58it/s]Epoch: 13, train for the 95-th batch, train loss: 0.5365514755249023:  62%|███████▍    | 94/151 [00:20<00:12,  4.50it/s]evaluate for the 27-th batch, evaluate loss: 0.6245414018630981:  41%|███████▎          | 27/66 [00:07<00:10,  3.65it/s]Epoch: 13, train for the 95-th batch, train loss: 0.5365514755249023:  63%|███████▌    | 95/151 [00:20<00:12,  4.51it/s]Epoch: 13, train for the 96-th batch, train loss: 0.5448091626167297:  63%|███████▌    | 95/151 [00:20<00:12,  4.51it/s]Epoch: 13, train for the 96-th batch, train loss: 0.5448091626167297:  64%|███████▋    | 96/151 [00:20<00:12,  4.50it/s]evaluate for the 28-th batch, evaluate loss: 0.6180877685546875:  41%|███████▎          | 27/66 [00:07<00:10,  3.65it/s]evaluate for the 28-th batch, evaluate loss: 0.6180877685546875:  42%|███████▋          | 28/66 [00:07<00:10,  3.55it/s]Epoch: 3, train for the 153-th batch, train loss: 0.47090598940849304:  40%|███▉      | 152/383 [01:31<02:17,  1.67it/s]Epoch: 3, train for the 153-th batch, train loss: 0.47090598940849304:  40%|███▉      | 153/383 [01:31<02:17,  1.68it/s]Epoch: 13, train for the 97-th batch, train loss: 0.5805601477622986:  64%|███████▋    | 96/151 [00:20<00:12,  4.50it/s]Epoch: 13, train for the 97-th batch, train loss: 0.5805601477622986:  64%|███████▋    | 97/151 [00:20<00:11,  4.51it/s]evaluate for the 29-th batch, evaluate loss: 0.6207471489906311:  42%|███████▋          | 28/66 [00:07<00:10,  3.55it/s]evaluate for the 29-th batch, evaluate loss: 0.6207471489906311:  44%|███████▉          | 29/66 [00:07<00:10,  3.67it/s]Epoch: 8, train for the 58-th batch, train loss: 0.16999131441116333:  48%|█████▋      | 57/119 [00:34<00:41,  1.51it/s]Epoch: 7, train for the 52-th batch, train loss: 0.4714484214782715:  35%|████▌        | 51/146 [00:32<01:03,  1.51it/s]Epoch: 8, train for the 58-th batch, train loss: 0.16999131441116333:  49%|█████▊      | 58/119 [00:34<00:40,  1.50it/s]Epoch: 7, train for the 52-th batch, train loss: 0.4714484214782715:  36%|████▋        | 52/146 [00:32<01:02,  1.50it/s]Epoch: 13, train for the 98-th batch, train loss: 0.6202839612960815:  64%|███████▋    | 97/151 [00:20<00:11,  4.51it/s]Epoch: 13, train for the 98-th batch, train loss: 0.6202839612960815:  65%|███████▊    | 98/151 [00:20<00:11,  4.50it/s]evaluate for the 30-th batch, evaluate loss: 0.6182315349578857:  44%|███████▉          | 29/66 [00:08<00:10,  3.67it/s]evaluate for the 30-th batch, evaluate loss: 0.6182315349578857:  45%|████████▏         | 30/66 [00:08<00:10,  3.60it/s]Epoch: 13, train for the 99-th batch, train loss: 0.6206333041191101:  65%|███████▊    | 98/151 [00:21<00:11,  4.50it/s]Epoch: 13, train for the 99-th batch, train loss: 0.6206333041191101:  66%|███████▊    | 99/151 [00:21<00:11,  4.51it/s]Epoch: 3, train for the 154-th batch, train loss: 0.3812063932418823:  40%|████▍      | 153/383 [01:32<02:17,  1.68it/s]Epoch: 3, train for the 154-th batch, train loss: 0.3812063932418823:  40%|████▍      | 154/383 [01:32<02:15,  1.68it/s]evaluate for the 31-th batch, evaluate loss: 0.6189745664596558:  45%|████████▏         | 30/66 [00:08<00:10,  3.60it/s]evaluate for the 31-th batch, evaluate loss: 0.6189745664596558:  47%|████████▍         | 31/66 [00:08<00:09,  3.69it/s]Epoch: 13, train for the 100-th batch, train loss: 0.6759148836135864:  66%|███████▏   | 99/151 [00:21<00:11,  4.51it/s]Epoch: 13, train for the 100-th batch, train loss: 0.6759148836135864:  66%|██████▌   | 100/151 [00:21<00:11,  4.52it/s]Epoch: 8, train for the 59-th batch, train loss: 0.1722993105649948:  49%|██████▎      | 58/119 [00:35<00:40,  1.50it/s]Epoch: 8, train for the 59-th batch, train loss: 0.1722993105649948:  50%|██████▍      | 59/119 [00:35<00:39,  1.50it/s]Epoch: 7, train for the 53-th batch, train loss: 0.4492364525794983:  36%|████▋        | 52/146 [00:32<01:02,  1.50it/s]Epoch: 7, train for the 53-th batch, train loss: 0.4492364525794983:  36%|████▋        | 53/146 [00:32<01:01,  1.50it/s]Epoch: 13, train for the 101-th batch, train loss: 0.6625778079032898:  66%|██████▌   | 100/151 [00:21<00:11,  4.52it/s]Epoch: 13, train for the 101-th batch, train loss: 0.6625778079032898:  67%|██████▋   | 101/151 [00:21<00:11,  4.54it/s]evaluate for the 32-th batch, evaluate loss: 0.6131653189659119:  47%|████████▍         | 31/66 [00:08<00:09,  3.69it/s]evaluate for the 32-th batch, evaluate loss: 0.6131653189659119:  48%|████████▋         | 32/66 [00:08<00:09,  3.63it/s]Epoch: 3, train for the 155-th batch, train loss: 0.3139628767967224:  40%|████▍      | 154/383 [01:33<02:15,  1.68it/s]Epoch: 3, train for the 155-th batch, train loss: 0.3139628767967224:  40%|████▍      | 155/383 [01:33<02:16,  1.68it/s]Epoch: 13, train for the 102-th batch, train loss: 0.627217710018158:  67%|███████▎   | 101/151 [00:21<00:11,  4.54it/s]Epoch: 13, train for the 102-th batch, train loss: 0.627217710018158:  68%|███████▍   | 102/151 [00:21<00:10,  4.54it/s]evaluate for the 33-th batch, evaluate loss: 0.6151204705238342:  48%|████████▋         | 32/66 [00:09<00:09,  3.63it/s]evaluate for the 33-th batch, evaluate loss: 0.6151204705238342:  50%|█████████         | 33/66 [00:09<00:09,  3.65it/s]Epoch: 13, train for the 103-th batch, train loss: 0.6316221952438354:  68%|██████▊   | 102/151 [00:22<00:10,  4.54it/s]Epoch: 13, train for the 103-th batch, train loss: 0.6316221952438354:  68%|██████▊   | 103/151 [00:22<00:10,  4.54it/s]evaluate for the 34-th batch, evaluate loss: 0.6190153360366821:  50%|█████████         | 33/66 [00:09<00:09,  3.65it/s]evaluate for the 34-th batch, evaluate loss: 0.6190153360366821:  52%|█████████▎        | 34/66 [00:09<00:08,  3.66it/s]Epoch: 8, train for the 60-th batch, train loss: 0.15900371968746185:  50%|█████▉      | 59/119 [00:35<00:39,  1.50it/s]Epoch: 8, train for the 60-th batch, train loss: 0.15900371968746185:  50%|██████      | 60/119 [00:35<00:39,  1.51it/s]Epoch: 7, train for the 54-th batch, train loss: 0.4633735418319702:  36%|████▋        | 53/146 [00:33<01:01,  1.50it/s]Epoch: 7, train for the 54-th batch, train loss: 0.4633735418319702:  37%|████▊        | 54/146 [00:33<01:01,  1.51it/s]Epoch: 13, train for the 104-th batch, train loss: 0.6342187523841858:  68%|██████▊   | 103/151 [00:22<00:10,  4.54it/s]Epoch: 13, train for the 104-th batch, train loss: 0.6342187523841858:  69%|██████▉   | 104/151 [00:22<00:10,  4.54it/s]Epoch: 3, train for the 156-th batch, train loss: 0.3201729953289032:  40%|████▍      | 155/383 [01:33<02:16,  1.68it/s]Epoch: 3, train for the 156-th batch, train loss: 0.3201729953289032:  41%|████▍      | 156/383 [01:33<02:15,  1.68it/s]Epoch: 13, train for the 105-th batch, train loss: 0.5602999925613403:  69%|██████▉   | 104/151 [00:22<00:10,  4.54it/s]Epoch: 13, train for the 105-th batch, train loss: 0.5602999925613403:  70%|██████▉   | 105/151 [00:22<00:10,  4.51it/s]evaluate for the 35-th batch, evaluate loss: 0.624644935131073:  52%|█████████▊         | 34/66 [00:09<00:08,  3.66it/s]evaluate for the 35-th batch, evaluate loss: 0.624644935131073:  53%|██████████         | 35/66 [00:09<00:08,  3.58it/s]Epoch: 13, train for the 106-th batch, train loss: 0.5479804277420044:  70%|██████▉   | 105/151 [00:22<00:10,  4.51it/s]Epoch: 13, train for the 106-th batch, train loss: 0.5479804277420044:  70%|███████   | 106/151 [00:22<00:09,  4.50it/s]evaluate for the 36-th batch, evaluate loss: 0.6427225470542908:  53%|█████████▌        | 35/66 [00:09<00:08,  3.58it/s]evaluate for the 36-th batch, evaluate loss: 0.6427225470542908:  55%|█████████▊        | 36/66 [00:09<00:08,  3.71it/s]Epoch: 8, train for the 61-th batch, train loss: 0.14573292434215546:  50%|██████      | 60/119 [00:36<00:39,  1.51it/s]Epoch: 8, train for the 61-th batch, train loss: 0.14573292434215546:  51%|██████▏     | 61/119 [00:36<00:38,  1.51it/s]Epoch: 7, train for the 55-th batch, train loss: 0.49750959873199463:  37%|████▍       | 54/146 [00:34<01:01,  1.51it/s]Epoch: 7, train for the 55-th batch, train loss: 0.49750959873199463:  38%|████▌       | 55/146 [00:34<01:00,  1.51it/s]Epoch: 13, train for the 107-th batch, train loss: 0.5599725246429443:  70%|███████   | 106/151 [00:22<00:09,  4.50it/s]Epoch: 13, train for the 107-th batch, train loss: 0.5599725246429443:  71%|███████   | 107/151 [00:22<00:09,  4.51it/s]Epoch: 3, train for the 157-th batch, train loss: 0.3759284019470215:  41%|████▍      | 156/383 [01:34<02:15,  1.68it/s]Epoch: 3, train for the 157-th batch, train loss: 0.3759284019470215:  41%|████▌      | 157/383 [01:34<02:14,  1.68it/s]evaluate for the 37-th batch, evaluate loss: 0.6612527370452881:  55%|█████████▊        | 36/66 [00:10<00:08,  3.71it/s]evaluate for the 37-th batch, evaluate loss: 0.6612527370452881:  56%|██████████        | 37/66 [00:10<00:08,  3.59it/s]Epoch: 13, train for the 108-th batch, train loss: 0.5505990982055664:  71%|███████   | 107/151 [00:23<00:09,  4.51it/s]Epoch: 13, train for the 108-th batch, train loss: 0.5505990982055664:  72%|███████▏  | 108/151 [00:23<00:09,  4.52it/s]evaluate for the 38-th batch, evaluate loss: 0.6149525046348572:  56%|██████████        | 37/66 [00:10<00:08,  3.59it/s]evaluate for the 38-th batch, evaluate loss: 0.6149525046348572:  58%|██████████▎       | 38/66 [00:10<00:07,  3.76it/s]Epoch: 13, train for the 109-th batch, train loss: 0.5501102805137634:  72%|███████▏  | 108/151 [00:23<00:09,  4.52it/s]Epoch: 13, train for the 109-th batch, train loss: 0.5501102805137634:  72%|███████▏  | 109/151 [00:23<00:09,  4.52it/s]Epoch: 3, train for the 158-th batch, train loss: 0.4445086717605591:  41%|████▌      | 157/383 [01:34<02:14,  1.68it/s]Epoch: 3, train for the 158-th batch, train loss: 0.4445086717605591:  41%|████▌      | 158/383 [01:34<02:14,  1.68it/s]Epoch: 8, train for the 62-th batch, train loss: 0.24625509977340698:  51%|██████▏     | 61/119 [00:37<00:38,  1.51it/s]Epoch: 7, train for the 56-th batch, train loss: 0.501365602016449:  38%|█████▎        | 55/146 [00:34<01:00,  1.51it/s]Epoch: 8, train for the 62-th batch, train loss: 0.24625509977340698:  52%|██████▎     | 62/119 [00:37<00:37,  1.51it/s]Epoch: 7, train for the 56-th batch, train loss: 0.501365602016449:  38%|█████▎        | 56/146 [00:34<00:59,  1.51it/s]evaluate for the 39-th batch, evaluate loss: 0.6088210344314575:  58%|██████████▎       | 38/66 [00:10<00:07,  3.76it/s]evaluate for the 39-th batch, evaluate loss: 0.6088210344314575:  59%|██████████▋       | 39/66 [00:10<00:07,  3.58it/s]Epoch: 13, train for the 110-th batch, train loss: 0.5502393841743469:  72%|███████▏  | 109/151 [00:23<00:09,  4.52it/s]Epoch: 13, train for the 110-th batch, train loss: 0.5502393841743469:  73%|███████▎  | 110/151 [00:23<00:09,  4.53it/s]Epoch: 13, train for the 111-th batch, train loss: 0.5313388109207153:  73%|███████▎  | 110/151 [00:23<00:09,  4.53it/s]Epoch: 13, train for the 111-th batch, train loss: 0.5313388109207153:  74%|███████▎  | 111/151 [00:23<00:08,  4.53it/s]evaluate for the 40-th batch, evaluate loss: 0.6183916926383972:  59%|██████████▋       | 39/66 [00:10<00:07,  3.58it/s]evaluate for the 40-th batch, evaluate loss: 0.6183916926383972:  61%|██████████▉       | 40/66 [00:10<00:07,  3.64it/s]Epoch: 13, train for the 112-th batch, train loss: 0.5244747996330261:  74%|███████▎  | 111/151 [00:24<00:08,  4.53it/s]Epoch: 13, train for the 112-th batch, train loss: 0.5244747996330261:  74%|███████▍  | 112/151 [00:24<00:08,  4.53it/s]evaluate for the 41-th batch, evaluate loss: 0.5946587920188904:  61%|██████████▉       | 40/66 [00:11<00:07,  3.64it/s]evaluate for the 41-th batch, evaluate loss: 0.5946587920188904:  62%|███████████▏      | 41/66 [00:11<00:07,  3.53it/s]Epoch: 3, train for the 159-th batch, train loss: 0.3735809922218323:  41%|████▌      | 158/383 [01:35<02:14,  1.68it/s]Epoch: 3, train for the 159-th batch, train loss: 0.3735809922218323:  42%|████▌      | 159/383 [01:35<02:13,  1.67it/s]Epoch: 8, train for the 63-th batch, train loss: 0.15720045566558838:  52%|██████▎     | 62/119 [00:37<00:37,  1.51it/s]Epoch: 7, train for the 57-th batch, train loss: 0.49862825870513916:  38%|████▌       | 56/146 [00:35<00:59,  1.51it/s]Epoch: 8, train for the 63-th batch, train loss: 0.15720045566558838:  53%|██████▎     | 63/119 [00:37<00:37,  1.51it/s]Epoch: 7, train for the 57-th batch, train loss: 0.49862825870513916:  39%|████▋       | 57/146 [00:35<00:58,  1.51it/s]Epoch: 13, train for the 113-th batch, train loss: 0.5717018246650696:  74%|███████▍  | 112/151 [00:24<00:08,  4.53it/s]Epoch: 13, train for the 113-th batch, train loss: 0.5717018246650696:  75%|███████▍  | 113/151 [00:24<00:08,  4.54it/s]evaluate for the 42-th batch, evaluate loss: 0.6266530156135559:  62%|███████████▏      | 41/66 [00:11<00:07,  3.53it/s]evaluate for the 42-th batch, evaluate loss: 0.6266530156135559:  64%|███████████▍      | 42/66 [00:11<00:06,  3.65it/s]Epoch: 13, train for the 114-th batch, train loss: 0.5179855227470398:  75%|███████▍  | 113/151 [00:24<00:08,  4.54it/s]Epoch: 13, train for the 114-th batch, train loss: 0.5179855227470398:  75%|███████▌  | 114/151 [00:24<00:08,  4.54it/s]evaluate for the 43-th batch, evaluate loss: 0.5962970852851868:  64%|███████████▍      | 42/66 [00:11<00:06,  3.65it/s]evaluate for the 43-th batch, evaluate loss: 0.5962970852851868:  65%|███████████▋      | 43/66 [00:11<00:06,  3.59it/s]Epoch: 13, train for the 115-th batch, train loss: 0.5034153461456299:  75%|███████▌  | 114/151 [00:24<00:08,  4.54it/s]Epoch: 13, train for the 115-th batch, train loss: 0.5034153461456299:  76%|███████▌  | 115/151 [00:24<00:07,  4.52it/s]Epoch: 3, train for the 160-th batch, train loss: 0.39357829093933105:  42%|████▏     | 159/383 [01:36<02:13,  1.67it/s]Epoch: 3, train for the 160-th batch, train loss: 0.39357829093933105:  42%|████▏     | 160/383 [01:36<02:12,  1.68it/s]Epoch: 7, train for the 58-th batch, train loss: 0.4841518700122833:  39%|█████        | 57/146 [00:36<00:58,  1.51it/s]Epoch: 7, train for the 58-th batch, train loss: 0.4841518700122833:  40%|█████▏       | 58/146 [00:36<00:58,  1.52it/s]Epoch: 8, train for the 64-th batch, train loss: 0.1585584580898285:  53%|██████▉      | 63/119 [00:38<00:37,  1.51it/s]Epoch: 8, train for the 64-th batch, train loss: 0.1585584580898285:  54%|██████▉      | 64/119 [00:38<00:36,  1.52it/s]Epoch: 13, train for the 116-th batch, train loss: 0.5151320695877075:  76%|███████▌  | 115/151 [00:24<00:07,  4.52it/s]Epoch: 13, train for the 116-th batch, train loss: 0.5151320695877075:  77%|███████▋  | 116/151 [00:24<00:07,  4.52it/s]evaluate for the 44-th batch, evaluate loss: 0.5817711353302002:  65%|███████████▋      | 43/66 [00:12<00:06,  3.59it/s]evaluate for the 44-th batch, evaluate loss: 0.5817711353302002:  67%|████████████      | 44/66 [00:12<00:05,  3.67it/s]Epoch: 13, train for the 117-th batch, train loss: 0.5065674781799316:  77%|███████▋  | 116/151 [00:25<00:07,  4.52it/s]Epoch: 13, train for the 117-th batch, train loss: 0.5065674781799316:  77%|███████▋  | 117/151 [00:25<00:07,  4.51it/s]evaluate for the 45-th batch, evaluate loss: 0.6258984804153442:  67%|████████████      | 44/66 [00:12<00:05,  3.67it/s]evaluate for the 45-th batch, evaluate loss: 0.6258984804153442:  68%|████████████▎     | 45/66 [00:12<00:05,  3.63it/s]Epoch: 3, train for the 161-th batch, train loss: 0.37685835361480713:  42%|████▏     | 160/383 [01:36<02:12,  1.68it/s]Epoch: 3, train for the 161-th batch, train loss: 0.37685835361480713:  42%|████▏     | 161/383 [01:36<02:12,  1.67it/s]Epoch: 13, train for the 118-th batch, train loss: 0.4972747564315796:  77%|███████▋  | 117/151 [00:25<00:07,  4.51it/s]Epoch: 13, train for the 118-th batch, train loss: 0.4972747564315796:  78%|███████▊  | 118/151 [00:25<00:07,  4.51it/s]evaluate for the 46-th batch, evaluate loss: 0.6388717293739319:  68%|████████████▎     | 45/66 [00:12<00:05,  3.63it/s]evaluate for the 46-th batch, evaluate loss: 0.6388717293739319:  70%|████████████▌     | 46/66 [00:12<00:05,  3.65it/s]Epoch: 7, train for the 59-th batch, train loss: 0.45572420954704285:  40%|████▊       | 58/146 [00:36<00:58,  1.52it/s]Epoch: 7, train for the 59-th batch, train loss: 0.45572420954704285:  40%|████▊       | 59/146 [00:36<00:57,  1.52it/s]Epoch: 8, train for the 65-th batch, train loss: 0.15432128310203552:  54%|██████▍     | 64/119 [00:39<00:36,  1.52it/s]Epoch: 8, train for the 65-th batch, train loss: 0.15432128310203552:  55%|██████▌     | 65/119 [00:39<00:35,  1.52it/s]Epoch: 13, train for the 119-th batch, train loss: 0.5167378783226013:  78%|███████▊  | 118/151 [00:25<00:07,  4.51it/s]Epoch: 13, train for the 119-th batch, train loss: 0.5167378783226013:  79%|███████▉  | 119/151 [00:25<00:07,  4.51it/s]evaluate for the 47-th batch, evaluate loss: 0.6485452651977539:  70%|████████████▌     | 46/66 [00:12<00:05,  3.65it/s]evaluate for the 47-th batch, evaluate loss: 0.6485452651977539:  71%|████████████▊     | 47/66 [00:12<00:05,  3.67it/s]Epoch: 13, train for the 120-th batch, train loss: 0.5794384479522705:  79%|███████▉  | 119/151 [00:25<00:07,  4.51it/s]Epoch: 13, train for the 120-th batch, train loss: 0.5794384479522705:  79%|███████▉  | 120/151 [00:25<00:06,  4.52it/s]Epoch: 3, train for the 162-th batch, train loss: 0.38355979323387146:  42%|████▏     | 161/383 [01:37<02:12,  1.67it/s]Epoch: 3, train for the 162-th batch, train loss: 0.38355979323387146:  42%|████▏     | 162/383 [01:37<02:11,  1.67it/s]Epoch: 13, train for the 121-th batch, train loss: 0.506197452545166:  79%|████████▋  | 120/151 [00:26<00:06,  4.52it/s]Epoch: 13, train for the 121-th batch, train loss: 0.506197452545166:  80%|████████▊  | 121/151 [00:26<00:06,  4.51it/s]evaluate for the 48-th batch, evaluate loss: 0.6358620524406433:  71%|████████████▊     | 47/66 [00:13<00:05,  3.67it/s]evaluate for the 48-th batch, evaluate loss: 0.6358620524406433:  73%|█████████████     | 48/66 [00:13<00:05,  3.58it/s]Epoch: 8, train for the 66-th batch, train loss: 0.19180987775325775:  55%|██████▌     | 65/119 [00:39<00:35,  1.52it/s]Epoch: 7, train for the 60-th batch, train loss: 0.5044915080070496:  40%|█████▎       | 59/146 [00:37<00:57,  1.52it/s]Epoch: 8, train for the 66-th batch, train loss: 0.19180987775325775:  55%|██████▋     | 66/119 [00:39<00:35,  1.51it/s]Epoch: 7, train for the 60-th batch, train loss: 0.5044915080070496:  41%|█████▎       | 60/146 [00:37<00:56,  1.51it/s]Epoch: 13, train for the 122-th batch, train loss: 0.563016414642334:  80%|████████▊  | 121/151 [00:26<00:06,  4.51it/s]Epoch: 13, train for the 122-th batch, train loss: 0.563016414642334:  81%|████████▉  | 122/151 [00:26<00:06,  4.50it/s]evaluate for the 49-th batch, evaluate loss: 0.6466168761253357:  73%|█████████████     | 48/66 [00:13<00:05,  3.58it/s]evaluate for the 49-th batch, evaluate loss: 0.6466168761253357:  74%|█████████████▎    | 49/66 [00:13<00:04,  3.71it/s]Epoch: 13, train for the 123-th batch, train loss: 0.5387622117996216:  81%|████████  | 122/151 [00:26<00:06,  4.50it/s]Epoch: 13, train for the 123-th batch, train loss: 0.5387622117996216:  81%|████████▏ | 123/151 [00:26<00:06,  4.50it/s]Epoch: 3, train for the 163-th batch, train loss: 0.3638743460178375:  42%|████▋      | 162/383 [01:37<02:11,  1.67it/s]Epoch: 3, train for the 163-th batch, train loss: 0.3638743460178375:  43%|████▋      | 163/383 [01:37<02:10,  1.68it/s]evaluate for the 50-th batch, evaluate loss: 0.6378194093704224:  74%|█████████████▎    | 49/66 [00:13<00:04,  3.71it/s]evaluate for the 50-th batch, evaluate loss: 0.6378194093704224:  76%|█████████████▋    | 50/66 [00:13<00:04,  3.59it/s]Epoch: 7, train for the 61-th batch, train loss: 0.5157169699668884:  41%|█████▎       | 60/146 [00:37<00:56,  1.51it/s]Epoch: 7, train for the 61-th batch, train loss: 0.5157169699668884:  42%|█████▍       | 61/146 [00:37<00:49,  1.71it/s]Epoch: 13, train for the 124-th batch, train loss: 0.5305942296981812:  81%|████████▏ | 123/151 [00:26<00:06,  4.50it/s]Epoch: 13, train for the 124-th batch, train loss: 0.5305942296981812:  82%|████████▏ | 124/151 [00:26<00:05,  4.52it/s]evaluate for the 51-th batch, evaluate loss: 0.6070292592048645:  76%|█████████████▋    | 50/66 [00:13<00:04,  3.59it/s]evaluate for the 51-th batch, evaluate loss: 0.6070292592048645:  77%|█████████████▉    | 51/66 [00:14<00:03,  3.76it/s]Epoch: 13, train for the 125-th batch, train loss: 0.5426737070083618:  82%|████████▏ | 124/151 [00:26<00:05,  4.52it/s]Epoch: 13, train for the 125-th batch, train loss: 0.5426737070083618:  83%|████████▎ | 125/151 [00:26<00:05,  4.51it/s]Epoch: 7, train for the 62-th batch, train loss: 0.48671820759773254:  42%|█████       | 61/146 [00:38<00:49,  1.71it/s]Epoch: 7, train for the 62-th batch, train loss: 0.48671820759773254:  42%|█████       | 62/146 [00:38<00:42,  1.98it/s]Epoch: 3, train for the 164-th batch, train loss: 0.410502552986145:  43%|█████       | 163/383 [01:38<02:10,  1.68it/s]Epoch: 3, train for the 164-th batch, train loss: 0.410502552986145:  43%|█████▏      | 164/383 [01:38<02:09,  1.69it/s]Epoch: 13, train for the 126-th batch, train loss: 0.5599302649497986:  83%|████████▎ | 125/151 [00:27<00:05,  4.51it/s]Epoch: 13, train for the 126-th batch, train loss: 0.5599302649497986:  83%|████████▎ | 126/151 [00:27<00:05,  4.52it/s]evaluate for the 52-th batch, evaluate loss: 0.6165490746498108:  77%|█████████████▉    | 51/66 [00:14<00:03,  3.76it/s]evaluate for the 52-th batch, evaluate loss: 0.6165490746498108:  79%|██████████████▏   | 52/66 [00:14<00:03,  3.59it/s]Epoch: 8, train for the 67-th batch, train loss: 0.24068038165569305:  55%|██████▋     | 66/119 [00:40<00:35,  1.51it/s]Epoch: 8, train for the 67-th batch, train loss: 0.24068038165569305:  56%|██████▊     | 67/119 [00:40<00:38,  1.34it/s]Epoch: 7, train for the 63-th batch, train loss: 0.5057862401008606:  42%|█████▌       | 62/146 [00:38<00:42,  1.98it/s]Epoch: 7, train for the 63-th batch, train loss: 0.5057862401008606:  43%|█████▌       | 63/146 [00:38<00:39,  2.11it/s]Epoch: 13, train for the 127-th batch, train loss: 0.5451256632804871:  83%|████████▎ | 126/151 [00:27<00:05,  4.52it/s]Epoch: 13, train for the 127-th batch, train loss: 0.5451256632804871:  84%|████████▍ | 127/151 [00:27<00:05,  4.52it/s]evaluate for the 53-th batch, evaluate loss: 0.6219232678413391:  79%|██████████████▏   | 52/66 [00:14<00:03,  3.59it/s]evaluate for the 53-th batch, evaluate loss: 0.6219232678413391:  80%|██████████████▍   | 53/66 [00:14<00:03,  3.69it/s]Epoch: 13, train for the 128-th batch, train loss: 0.6041358709335327:  84%|████████▍ | 127/151 [00:27<00:05,  4.52it/s]Epoch: 13, train for the 128-th batch, train loss: 0.6041358709335327:  85%|████████▍ | 128/151 [00:27<00:05,  4.52it/s]Epoch: 3, train for the 165-th batch, train loss: 0.31078222393989563:  43%|████▎     | 164/383 [01:39<02:09,  1.69it/s]Epoch: 3, train for the 165-th batch, train loss: 0.31078222393989563:  43%|████▎     | 165/383 [01:39<02:09,  1.69it/s]evaluate for the 54-th batch, evaluate loss: 0.6570098996162415:  80%|██████████████▍   | 53/66 [00:14<00:03,  3.69it/s]evaluate for the 54-th batch, evaluate loss: 0.6570098996162415:  82%|██████████████▋   | 54/66 [00:14<00:03,  3.51it/s]Epoch: 13, train for the 129-th batch, train loss: 0.5450296998023987:  85%|████████▍ | 128/151 [00:27<00:05,  4.52it/s]Epoch: 13, train for the 129-th batch, train loss: 0.5450296998023987:  85%|████████▌ | 129/151 [00:27<00:04,  4.52it/s]Epoch: 8, train for the 68-th batch, train loss: 0.11714538186788559:  56%|██████▊     | 67/119 [00:41<00:38,  1.34it/s]Epoch: 8, train for the 68-th batch, train loss: 0.11714538186788559:  57%|██████▊     | 68/119 [00:41<00:36,  1.40it/s]Epoch: 7, train for the 64-th batch, train loss: 0.513635516166687:  43%|██████        | 63/146 [00:39<00:39,  2.11it/s]Epoch: 7, train for the 64-th batch, train loss: 0.513635516166687:  44%|██████▏       | 64/146 [00:39<00:42,  1.93it/s]evaluate for the 55-th batch, evaluate loss: 0.6083031892776489:  82%|██████████████▋   | 54/66 [00:15<00:03,  3.51it/s]evaluate for the 55-th batch, evaluate loss: 0.6083031892776489:  83%|███████████████   | 55/66 [00:15<00:03,  3.59it/s]Epoch: 13, train for the 130-th batch, train loss: 0.5379599928855896:  85%|████████▌ | 129/151 [00:28<00:04,  4.52it/s]Epoch: 13, train for the 130-th batch, train loss: 0.5379599928855896:  86%|████████▌ | 130/151 [00:28<00:04,  4.53it/s]Epoch: 13, train for the 131-th batch, train loss: 0.5177308917045593:  86%|████████▌ | 130/151 [00:28<00:04,  4.53it/s]Epoch: 13, train for the 131-th batch, train loss: 0.5177308917045593:  87%|████████▋ | 131/151 [00:28<00:04,  4.53it/s]evaluate for the 56-th batch, evaluate loss: 0.6273640394210815:  83%|███████████████   | 55/66 [00:15<00:03,  3.59it/s]evaluate for the 56-th batch, evaluate loss: 0.6273640394210815:  85%|███████████████▎  | 56/66 [00:15<00:02,  3.52it/s]Epoch: 3, train for the 166-th batch, train loss: 0.4541357457637787:  43%|████▋      | 165/383 [01:39<02:09,  1.69it/s]Epoch: 3, train for the 166-th batch, train loss: 0.4541357457637787:  43%|████▊      | 166/383 [01:39<02:08,  1.69it/s]Epoch: 8, train for the 69-th batch, train loss: 0.14732587337493896:  57%|██████▊     | 68/119 [00:42<00:36,  1.40it/s]Epoch: 8, train for the 69-th batch, train loss: 0.14732587337493896:  58%|██████▉     | 69/119 [00:42<00:34,  1.45it/s]Epoch: 13, train for the 132-th batch, train loss: 0.5199947357177734:  87%|████████▋ | 131/151 [00:28<00:04,  4.53it/s]Epoch: 13, train for the 132-th batch, train loss: 0.5199947357177734:  87%|████████▋ | 132/151 [00:28<00:04,  4.51it/s]evaluate for the 57-th batch, evaluate loss: 0.611074686050415:  85%|████████████████   | 56/66 [00:15<00:02,  3.52it/s]evaluate for the 57-th batch, evaluate loss: 0.611074686050415:  86%|████████████████▍  | 57/66 [00:15<00:02,  3.65it/s]Epoch: 7, train for the 65-th batch, train loss: 0.5240193605422974:  44%|█████▋       | 64/146 [00:39<00:42,  1.93it/s]Epoch: 7, train for the 65-th batch, train loss: 0.5240193605422974:  45%|█████▊       | 65/146 [00:39<00:44,  1.82it/s]Epoch: 13, train for the 133-th batch, train loss: 0.5370697379112244:  87%|████████▋ | 132/151 [00:28<00:04,  4.51it/s]Epoch: 13, train for the 133-th batch, train loss: 0.5370697379112244:  88%|████████▊ | 133/151 [00:28<00:03,  4.52it/s]evaluate for the 58-th batch, evaluate loss: 0.6493301391601562:  86%|███████████████▌  | 57/66 [00:15<00:02,  3.65it/s]evaluate for the 58-th batch, evaluate loss: 0.6493301391601562:  88%|███████████████▊  | 58/66 [00:15<00:02,  3.58it/s]Epoch: 3, train for the 167-th batch, train loss: 0.35975927114486694:  43%|████▎     | 166/383 [01:40<02:08,  1.69it/s]Epoch: 3, train for the 167-th batch, train loss: 0.35975927114486694:  44%|████▎     | 167/383 [01:40<02:07,  1.70it/s]Epoch: 13, train for the 134-th batch, train loss: 0.5281806588172913:  88%|████████▊ | 133/151 [00:28<00:03,  4.52it/s]Epoch: 13, train for the 134-th batch, train loss: 0.5281806588172913:  89%|████████▊ | 134/151 [00:28<00:03,  4.52it/s]Epoch: 8, train for the 70-th batch, train loss: 0.1385882943868637:  58%|███████▌     | 69/119 [00:42<00:34,  1.45it/s]Epoch: 8, train for the 70-th batch, train loss: 0.1385882943868637:  59%|███████▋     | 70/119 [00:42<00:32,  1.50it/s]evaluate for the 59-th batch, evaluate loss: 0.6343385577201843:  88%|███████████████▊  | 58/66 [00:16<00:02,  3.58it/s]evaluate for the 59-th batch, evaluate loss: 0.6343385577201843:  89%|████████████████  | 59/66 [00:16<00:01,  3.67it/s]Epoch: 13, train for the 135-th batch, train loss: 0.5556532144546509:  89%|████████▊ | 134/151 [00:29<00:03,  4.52it/s]Epoch: 13, train for the 135-th batch, train loss: 0.5556532144546509:  89%|████████▉ | 135/151 [00:29<00:03,  4.52it/s]Epoch: 7, train for the 66-th batch, train loss: 0.5349828004837036:  45%|█████▊       | 65/146 [00:40<00:44,  1.82it/s]Epoch: 7, train for the 66-th batch, train loss: 0.5349828004837036:  45%|█████▉       | 66/146 [00:40<00:45,  1.76it/s]Epoch: 13, train for the 136-th batch, train loss: 0.5586249232292175:  89%|████████▉ | 135/151 [00:29<00:03,  4.52it/s]Epoch: 13, train for the 136-th batch, train loss: 0.5586249232292175:  90%|█████████ | 136/151 [00:29<00:03,  4.52it/s]evaluate for the 60-th batch, evaluate loss: 0.6428703665733337:  89%|████████████████  | 59/66 [00:16<00:01,  3.67it/s]evaluate for the 60-th batch, evaluate loss: 0.6428703665733337:  91%|████████████████▎ | 60/66 [00:16<00:01,  3.62it/s]Epoch: 3, train for the 168-th batch, train loss: 0.32794371247291565:  44%|████▎     | 167/383 [01:40<02:07,  1.70it/s]Epoch: 3, train for the 168-th batch, train loss: 0.32794371247291565:  44%|████▍     | 168/383 [01:40<02:07,  1.68it/s]Epoch: 13, train for the 137-th batch, train loss: 0.6361454725265503:  90%|█████████ | 136/151 [00:29<00:03,  4.52it/s]Epoch: 13, train for the 137-th batch, train loss: 0.6361454725265503:  91%|█████████ | 137/151 [00:29<00:03,  4.53it/s]evaluate for the 61-th batch, evaluate loss: 0.6372269988059998:  91%|████████████████▎ | 60/66 [00:16<00:01,  3.62it/s]evaluate for the 61-th batch, evaluate loss: 0.6372269988059998:  92%|████████████████▋ | 61/66 [00:16<00:01,  3.64it/s]Epoch: 8, train for the 71-th batch, train loss: 0.18835674226284027:  59%|███████     | 70/119 [00:43<00:32,  1.50it/s]Epoch: 8, train for the 71-th batch, train loss: 0.18835674226284027:  60%|███████▏    | 71/119 [00:43<00:31,  1.53it/s]Epoch: 13, train for the 138-th batch, train loss: 0.5947689414024353:  91%|█████████ | 137/151 [00:29<00:03,  4.53it/s]Epoch: 13, train for the 138-th batch, train loss: 0.5947689414024353:  91%|█████████▏| 138/151 [00:29<00:02,  4.53it/s]Epoch: 7, train for the 67-th batch, train loss: 0.5402347445487976:  45%|█████▉       | 66/146 [00:41<00:45,  1.76it/s]Epoch: 7, train for the 67-th batch, train loss: 0.5402347445487976:  46%|█████▉       | 67/146 [00:41<00:46,  1.71it/s]evaluate for the 62-th batch, evaluate loss: 0.6662641167640686:  92%|████████████████▋ | 61/66 [00:17<00:01,  3.64it/s]evaluate for the 62-th batch, evaluate loss: 0.6662641167640686:  94%|████████████████▉ | 62/66 [00:17<00:01,  3.66it/s]Epoch: 13, train for the 139-th batch, train loss: 0.5788474082946777:  91%|█████████▏| 138/151 [00:30<00:02,  4.53it/s]Epoch: 13, train for the 139-th batch, train loss: 0.5788474082946777:  92%|█████████▏| 139/151 [00:30<00:02,  4.53it/s]Epoch: 3, train for the 169-th batch, train loss: 0.33409637212753296:  44%|████▍     | 168/383 [01:41<02:07,  1.68it/s]Epoch: 3, train for the 169-th batch, train loss: 0.33409637212753296:  44%|████▍     | 169/383 [01:41<02:07,  1.68it/s]evaluate for the 63-th batch, evaluate loss: 0.6313705444335938:  94%|████████████████▉ | 62/66 [00:17<00:01,  3.66it/s]evaluate for the 63-th batch, evaluate loss: 0.6313705444335938:  95%|█████████████████▏| 63/66 [00:17<00:00,  3.57it/s]Epoch: 13, train for the 140-th batch, train loss: 0.5053413510322571:  92%|█████████▏| 139/151 [00:30<00:02,  4.53it/s]Epoch: 13, train for the 140-th batch, train loss: 0.5053413510322571:  93%|█████████▎| 140/151 [00:30<00:02,  4.54it/s]Epoch: 8, train for the 72-th batch, train loss: 0.19244666397571564:  60%|███████▏    | 71/119 [00:43<00:31,  1.53it/s]Epoch: 8, train for the 72-th batch, train loss: 0.19244666397571564:  61%|███████▎    | 72/119 [00:43<00:30,  1.56it/s]Epoch: 7, train for the 68-th batch, train loss: 0.5109021663665771:  46%|█████▉       | 67/146 [00:41<00:46,  1.71it/s]Epoch: 7, train for the 68-th batch, train loss: 0.5109021663665771:  47%|██████       | 68/146 [00:41<00:46,  1.69it/s]Epoch: 13, train for the 141-th batch, train loss: 0.5439488887786865:  93%|█████████▎| 140/151 [00:30<00:02,  4.54it/s]Epoch: 13, train for the 141-th batch, train loss: 0.5439488887786865:  93%|█████████▎| 141/151 [00:30<00:02,  4.52it/s]evaluate for the 64-th batch, evaluate loss: 0.6224836707115173:  95%|█████████████████▏| 63/66 [00:17<00:00,  3.57it/s]evaluate for the 64-th batch, evaluate loss: 0.6224836707115173:  97%|█████████████████▍| 64/66 [00:17<00:00,  3.70it/s]Epoch: 13, train for the 142-th batch, train loss: 0.5573509931564331:  93%|█████████▎| 141/151 [00:30<00:02,  4.52it/s]Epoch: 13, train for the 142-th batch, train loss: 0.5573509931564331:  94%|█████████▍| 142/151 [00:30<00:01,  4.52it/s]Epoch: 3, train for the 170-th batch, train loss: 0.3698084056377411:  44%|████▊      | 169/383 [01:42<02:07,  1.68it/s]Epoch: 3, train for the 170-th batch, train loss: 0.3698084056377411:  44%|████▉      | 170/383 [01:42<02:05,  1.69it/s]evaluate for the 65-th batch, evaluate loss: 0.5954669713973999:  97%|█████████████████▍| 64/66 [00:17<00:00,  3.70it/s]evaluate for the 65-th batch, evaluate loss: 0.5954669713973999:  98%|█████████████████▋| 65/66 [00:17<00:00,  3.58it/s]Epoch: 8, train for the 73-th batch, train loss: 0.16092059016227722:  61%|███████▎    | 72/119 [00:44<00:30,  1.56it/s]Epoch: 8, train for the 73-th batch, train loss: 0.16092059016227722:  61%|███████▎    | 73/119 [00:44<00:29,  1.58it/s]Epoch: 13, train for the 143-th batch, train loss: 0.49089956283569336:  94%|████████▍| 142/151 [00:30<00:01,  4.52it/s]Epoch: 13, train for the 143-th batch, train loss: 0.49089956283569336:  95%|████████▌| 143/151 [00:30<00:01,  4.51it/s]evaluate for the 66-th batch, evaluate loss: 0.6448522210121155:  98%|█████████████████▋| 65/66 [00:18<00:00,  3.58it/s]evaluate for the 66-th batch, evaluate loss: 0.6448522210121155: 100%|██████████████████| 66/66 [00:18<00:00,  3.99it/s]evaluate for the 66-th batch, evaluate loss: 0.6448522210121155: 100%|██████████████████| 66/66 [00:18<00:00,  3.65it/s]
  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 7, train for the 69-th batch, train loss: 0.5306684970855713:  47%|██████       | 68/146 [00:42<00:46,  1.69it/s]Epoch: 7, train for the 69-th batch, train loss: 0.5306684970855713:  47%|██████▏      | 69/146 [00:42<00:46,  1.67it/s]Epoch: 13, train for the 144-th batch, train loss: 0.5204474329948425:  95%|█████████▍| 143/151 [00:31<00:01,  4.51it/s]Epoch: 13, train for the 144-th batch, train loss: 0.5204474329948425:  95%|█████████▌| 144/151 [00:31<00:01,  4.52it/s]evaluate for the 1-th batch, evaluate loss: 0.6648808717727661:   0%|                            | 0/40 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6648808717727661:   2%|▌                   | 1/40 [00:00<00:11,  3.32it/s]Epoch: 3, train for the 171-th batch, train loss: 0.5096165537834167:  44%|████▉      | 170/383 [01:42<02:05,  1.69it/s]Epoch: 3, train for the 171-th batch, train loss: 0.5096165537834167:  45%|████▉      | 171/383 [01:42<02:03,  1.71it/s]Epoch: 13, train for the 145-th batch, train loss: 0.5351302027702332:  95%|█████████▌| 144/151 [00:31<00:01,  4.52it/s]Epoch: 13, train for the 145-th batch, train loss: 0.5351302027702332:  96%|█████████▌| 145/151 [00:31<00:01,  4.52it/s]evaluate for the 2-th batch, evaluate loss: 0.7030913233757019:   2%|▌                   | 1/40 [00:00<00:11,  3.32it/s]evaluate for the 2-th batch, evaluate loss: 0.7030913233757019:   5%|█                   | 2/40 [00:00<00:10,  3.63it/s]Epoch: 8, train for the 74-th batch, train loss: 0.25226858258247375:  61%|███████▎    | 73/119 [00:45<00:29,  1.58it/s]Epoch: 8, train for the 74-th batch, train loss: 0.25226858258247375:  62%|███████▍    | 74/119 [00:45<00:28,  1.59it/s]Epoch: 13, train for the 146-th batch, train loss: 0.533237874507904:  96%|██████████▌| 145/151 [00:31<00:01,  4.52it/s]Epoch: 13, train for the 146-th batch, train loss: 0.533237874507904:  97%|██████████▋| 146/151 [00:31<00:01,  4.53it/s]Epoch: 7, train for the 70-th batch, train loss: 0.528958261013031:  47%|██████▌       | 69/146 [00:42<00:46,  1.67it/s]Epoch: 7, train for the 70-th batch, train loss: 0.528958261013031:  48%|██████▋       | 70/146 [00:42<00:45,  1.66it/s]Epoch: 13, train for the 147-th batch, train loss: 0.5646300315856934:  97%|█████████▋| 146/151 [00:31<00:01,  4.53it/s]Epoch: 13, train for the 147-th batch, train loss: 0.5646300315856934:  97%|█████████▋| 147/151 [00:31<00:00,  4.52it/s]evaluate for the 3-th batch, evaluate loss: 0.6817512512207031:   5%|█                   | 2/40 [00:00<00:10,  3.63it/s]evaluate for the 3-th batch, evaluate loss: 0.6817512512207031:   8%|█▌                  | 3/40 [00:00<00:10,  3.50it/s]Epoch: 3, train for the 172-th batch, train loss: 0.30048853158950806:  45%|████▍     | 171/383 [01:43<02:03,  1.71it/s]Epoch: 3, train for the 172-th batch, train loss: 0.30048853158950806:  45%|████▍     | 172/383 [01:43<02:02,  1.72it/s]Epoch: 13, train for the 148-th batch, train loss: 0.5996701121330261:  97%|█████████▋| 147/151 [00:32<00:00,  4.52it/s]Epoch: 13, train for the 148-th batch, train loss: 0.5996701121330261:  98%|█████████▊| 148/151 [00:32<00:00,  4.52it/s]evaluate for the 4-th batch, evaluate loss: 0.6898036599159241:   8%|█▌                  | 3/40 [00:01<00:10,  3.50it/s]evaluate for the 4-th batch, evaluate loss: 0.6898036599159241:  10%|██                  | 4/40 [00:01<00:09,  3.63it/s]Epoch: 8, train for the 75-th batch, train loss: 0.16182838380336761:  62%|███████▍    | 74/119 [00:45<00:28,  1.59it/s]Epoch: 8, train for the 75-th batch, train loss: 0.16182838380336761:  63%|███████▌    | 75/119 [00:45<00:27,  1.60it/s]Epoch: 13, train for the 149-th batch, train loss: 0.48343947529792786:  98%|████████▊| 148/151 [00:32<00:00,  4.52it/s]Epoch: 13, train for the 149-th batch, train loss: 0.48343947529792786:  99%|████████▉| 149/151 [00:32<00:00,  4.53it/s]Epoch: 7, train for the 71-th batch, train loss: 0.5355169773101807:  48%|██████▏      | 70/146 [00:43<00:45,  1.66it/s]Epoch: 7, train for the 71-th batch, train loss: 0.5355169773101807:  49%|██████▎      | 71/146 [00:43<00:45,  1.65it/s]evaluate for the 5-th batch, evaluate loss: 0.6563873291015625:  10%|██                  | 4/40 [00:01<00:09,  3.63it/s]evaluate for the 5-th batch, evaluate loss: 0.6563873291015625:  12%|██▌                 | 5/40 [00:01<00:09,  3.53it/s]Epoch: 3, train for the 173-th batch, train loss: 0.375852108001709:  45%|█████▍      | 172/383 [01:43<02:02,  1.72it/s]Epoch: 3, train for the 173-th batch, train loss: 0.375852108001709:  45%|█████▍      | 173/383 [01:43<02:01,  1.73it/s]Epoch: 13, train for the 150-th batch, train loss: 0.5157586336135864:  99%|█████████▊| 149/151 [00:32<00:00,  4.53it/s]Epoch: 13, train for the 150-th batch, train loss: 0.5157586336135864:  99%|█████████▉| 150/151 [00:32<00:00,  4.53it/s]Epoch: 13, train for the 151-th batch, train loss: 0.5950272083282471:  99%|█████████▉| 150/151 [00:32<00:00,  4.53it/s]Epoch: 13, train for the 151-th batch, train loss: 0.5950272083282471: 100%|██████████| 151/151 [00:32<00:00,  5.01it/s]Epoch: 13, train for the 151-th batch, train loss: 0.5950272083282471: 100%|██████████| 151/151 [00:32<00:00,  4.63it/s]
  0%|                                                                                            | 0/46 [00:00<?, ?it/s]evaluate for the 6-th batch, evaluate loss: 0.6801682710647583:  12%|██▌                 | 5/40 [00:01<00:09,  3.53it/s]evaluate for the 6-th batch, evaluate loss: 0.6801682710647583:  15%|███                 | 6/40 [00:01<00:09,  3.64it/s]evaluate for the 1-th batch, evaluate loss: 0.49444928765296936:   0%|                           | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.49444928765296936:   2%|▍                  | 1/46 [00:00<00:04,  9.73it/s]Epoch: 8, train for the 76-th batch, train loss: 0.2327556610107422:  63%|████████▏    | 75/119 [00:46<00:27,  1.60it/s]Epoch: 8, train for the 76-th batch, train loss: 0.2327556610107422:  64%|████████▎    | 76/119 [00:46<00:26,  1.61it/s]evaluate for the 2-th batch, evaluate loss: 0.504787802696228:   2%|▍                    | 1/46 [00:00<00:04,  9.73it/s]evaluate for the 2-th batch, evaluate loss: 0.504787802696228:   4%|▉                    | 2/46 [00:00<00:04,  9.69it/s]Epoch: 7, train for the 72-th batch, train loss: 0.5314754843711853:  49%|██████▎      | 71/146 [00:44<00:45,  1.65it/s]Epoch: 7, train for the 72-th batch, train loss: 0.5314754843711853:  49%|██████▍      | 72/146 [00:44<00:45,  1.64it/s]evaluate for the 3-th batch, evaluate loss: 0.48301032185554504:   4%|▊                  | 2/46 [00:00<00:04,  9.69it/s]evaluate for the 3-th batch, evaluate loss: 0.48301032185554504:   7%|█▏                 | 3/46 [00:00<00:04,  9.67it/s]evaluate for the 7-th batch, evaluate loss: 0.6753976345062256:  15%|███                 | 6/40 [00:01<00:09,  3.64it/s]evaluate for the 7-th batch, evaluate loss: 0.6753976345062256:  18%|███▌                | 7/40 [00:01<00:09,  3.55it/s]Epoch: 3, train for the 174-th batch, train loss: 0.46084561944007874:  45%|████▌     | 173/383 [01:44<02:01,  1.73it/s]Epoch: 3, train for the 174-th batch, train loss: 0.46084561944007874:  45%|████▌     | 174/383 [01:44<01:59,  1.74it/s]evaluate for the 4-th batch, evaluate loss: 0.5116044878959656:   7%|█▎                  | 3/46 [00:00<00:04,  9.67it/s]evaluate for the 4-th batch, evaluate loss: 0.5116044878959656:   9%|█▋                  | 4/46 [00:00<00:04,  9.69it/s]evaluate for the 5-th batch, evaluate loss: 0.4781326949596405:   9%|█▋                  | 4/46 [00:00<00:04,  9.69it/s]evaluate for the 5-th batch, evaluate loss: 0.4781326949596405:  11%|██▏                 | 5/46 [00:00<00:04,  9.71it/s]evaluate for the 8-th batch, evaluate loss: 0.7327006459236145:  18%|███▌                | 7/40 [00:02<00:09,  3.55it/s]evaluate for the 8-th batch, evaluate loss: 0.7327006459236145:  20%|████                | 8/40 [00:02<00:08,  3.64it/s]evaluate for the 6-th batch, evaluate loss: 0.550690233707428:  11%|██▎                  | 5/46 [00:00<00:04,  9.71it/s]evaluate for the 6-th batch, evaluate loss: 0.550690233707428:  13%|██▋                  | 6/46 [00:00<00:04,  9.69it/s]evaluate for the 7-th batch, evaluate loss: 0.4694267809391022:  13%|██▌                 | 6/46 [00:00<00:04,  9.69it/s]evaluate for the 7-th batch, evaluate loss: 0.4694267809391022:  15%|███                 | 7/46 [00:00<00:04,  9.73it/s]Epoch: 8, train for the 77-th batch, train loss: 0.14100612699985504:  64%|███████▋    | 76/119 [00:47<00:26,  1.61it/s]Epoch: 8, train for the 77-th batch, train loss: 0.14100612699985504:  65%|███████▊    | 77/119 [00:47<00:26,  1.61it/s]evaluate for the 8-th batch, evaluate loss: 0.5586224794387817:  15%|███                 | 7/46 [00:00<00:04,  9.73it/s]evaluate for the 8-th batch, evaluate loss: 0.5586224794387817:  17%|███▍                | 8/46 [00:00<00:03,  9.74it/s]evaluate for the 9-th batch, evaluate loss: 0.6991351842880249:  20%|████                | 8/40 [00:02<00:08,  3.64it/s]evaluate for the 9-th batch, evaluate loss: 0.6991351842880249:  22%|████▌               | 9/40 [00:02<00:08,  3.55it/s]Epoch: 7, train for the 73-th batch, train loss: 0.4998801350593567:  49%|██████▍      | 72/146 [00:44<00:45,  1.64it/s]Epoch: 7, train for the 73-th batch, train loss: 0.4998801350593567:  50%|██████▌      | 73/146 [00:44<00:44,  1.63it/s]evaluate for the 9-th batch, evaluate loss: 0.5269739627838135:  17%|███▍                | 8/46 [00:00<00:03,  9.74it/s]evaluate for the 9-th batch, evaluate loss: 0.5269739627838135:  20%|███▉                | 9/46 [00:00<00:03,  9.73it/s]Epoch: 3, train for the 175-th batch, train loss: 0.35066497325897217:  45%|████▌     | 174/383 [01:44<01:59,  1.74it/s]Epoch: 3, train for the 175-th batch, train loss: 0.35066497325897217:  46%|████▌     | 175/383 [01:44<01:58,  1.75it/s]evaluate for the 10-th batch, evaluate loss: 0.5331605672836304:  20%|███▋               | 9/46 [00:01<00:03,  9.73it/s]evaluate for the 10-th batch, evaluate loss: 0.5331605672836304:  22%|███▉              | 10/46 [00:01<00:03,  9.70it/s]evaluate for the 11-th batch, evaluate loss: 0.525185227394104:  22%|████▏              | 10/46 [00:01<00:03,  9.70it/s]evaluate for the 11-th batch, evaluate loss: 0.525185227394104:  24%|████▌              | 11/46 [00:01<00:03,  9.73it/s]evaluate for the 10-th batch, evaluate loss: 0.6900051832199097:  22%|████▎              | 9/40 [00:02<00:08,  3.55it/s]evaluate for the 10-th batch, evaluate loss: 0.6900051832199097:  25%|████▌             | 10/40 [00:02<00:08,  3.64it/s]evaluate for the 12-th batch, evaluate loss: 0.47443312406539917:  24%|████             | 11/46 [00:01<00:03,  9.73it/s]evaluate for the 12-th batch, evaluate loss: 0.47443312406539917:  26%|████▍            | 12/46 [00:01<00:03,  9.72it/s]evaluate for the 13-th batch, evaluate loss: 0.4975331425666809:  26%|████▋             | 12/46 [00:01<00:03,  9.72it/s]evaluate for the 13-th batch, evaluate loss: 0.4975331425666809:  28%|█████             | 13/46 [00:01<00:03,  9.71it/s]Epoch: 8, train for the 78-th batch, train loss: 0.1985441893339157:  65%|████████▍    | 77/119 [00:47<00:26,  1.61it/s]Epoch: 8, train for the 78-th batch, train loss: 0.1985441893339157:  66%|████████▌    | 78/119 [00:47<00:25,  1.62it/s]evaluate for the 11-th batch, evaluate loss: 0.6958796381950378:  25%|████▌             | 10/40 [00:03<00:08,  3.64it/s]evaluate for the 11-th batch, evaluate loss: 0.6958796381950378:  28%|████▉             | 11/40 [00:03<00:08,  3.56it/s]evaluate for the 14-th batch, evaluate loss: 0.5897395014762878:  28%|█████             | 13/46 [00:01<00:03,  9.71it/s]evaluate for the 14-th batch, evaluate loss: 0.5897395014762878:  30%|█████▍            | 14/46 [00:01<00:03,  9.71it/s]Epoch: 3, train for the 176-th batch, train loss: 0.39450162649154663:  46%|████▌     | 175/383 [01:45<01:58,  1.75it/s]Epoch: 3, train for the 176-th batch, train loss: 0.39450162649154663:  46%|████▌     | 176/383 [01:45<01:58,  1.75it/s]Epoch: 7, train for the 74-th batch, train loss: 0.5277429223060608:  50%|██████▌      | 73/146 [00:45<00:44,  1.63it/s]Epoch: 7, train for the 74-th batch, train loss: 0.5277429223060608:  51%|██████▌      | 74/146 [00:45<00:44,  1.63it/s]evaluate for the 15-th batch, evaluate loss: 0.5415763258934021:  30%|█████▍            | 14/46 [00:01<00:03,  9.71it/s]evaluate for the 15-th batch, evaluate loss: 0.5415763258934021:  33%|█████▊            | 15/46 [00:01<00:03,  9.70it/s]evaluate for the 16-th batch, evaluate loss: 0.5725858807563782:  33%|█████▊            | 15/46 [00:01<00:03,  9.70it/s]evaluate for the 16-th batch, evaluate loss: 0.5725858807563782:  35%|██████▎           | 16/46 [00:01<00:03,  9.71it/s]evaluate for the 12-th batch, evaluate loss: 0.7479559183120728:  28%|████▉             | 11/40 [00:03<00:08,  3.56it/s]evaluate for the 12-th batch, evaluate loss: 0.7479559183120728:  30%|█████▍            | 12/40 [00:03<00:07,  3.64it/s]evaluate for the 17-th batch, evaluate loss: 0.4511112570762634:  35%|██████▎           | 16/46 [00:01<00:03,  9.71it/s]evaluate for the 17-th batch, evaluate loss: 0.4511112570762634:  37%|██████▋           | 17/46 [00:01<00:02,  9.69it/s]evaluate for the 18-th batch, evaluate loss: 0.504638671875:  37%|████████▏             | 17/46 [00:01<00:02,  9.69it/s]evaluate for the 18-th batch, evaluate loss: 0.504638671875:  39%|████████▌             | 18/46 [00:01<00:02,  9.68it/s]evaluate for the 19-th batch, evaluate loss: 0.5205036401748657:  39%|███████           | 18/46 [00:01<00:02,  9.68it/s]evaluate for the 19-th batch, evaluate loss: 0.5205036401748657:  41%|███████▍          | 19/46 [00:01<00:02,  9.68it/s]evaluate for the 13-th batch, evaluate loss: 0.6880039572715759:  30%|█████▍            | 12/40 [00:03<00:07,  3.64it/s]evaluate for the 13-th batch, evaluate loss: 0.6880039572715759:  32%|█████▊            | 13/40 [00:03<00:07,  3.57it/s]Epoch: 8, train for the 79-th batch, train loss: 0.15028172731399536:  66%|███████▊    | 78/119 [00:48<00:25,  1.62it/s]Epoch: 8, train for the 79-th batch, train loss: 0.15028172731399536:  66%|███████▉    | 79/119 [00:48<00:24,  1.62it/s]evaluate for the 20-th batch, evaluate loss: 0.536305844783783:  41%|███████▊           | 19/46 [00:02<00:02,  9.68it/s]evaluate for the 20-th batch, evaluate loss: 0.536305844783783:  43%|████████▎          | 20/46 [00:02<00:02,  9.63it/s]Epoch: 3, train for the 177-th batch, train loss: 0.3710600435733795:  46%|█████      | 176/383 [01:46<01:58,  1.75it/s]Epoch: 3, train for the 177-th batch, train loss: 0.3710600435733795:  46%|█████      | 177/383 [01:46<01:58,  1.74it/s]Epoch: 7, train for the 75-th batch, train loss: 0.5444128513336182:  51%|██████▌      | 74/146 [00:45<00:44,  1.63it/s]Epoch: 7, train for the 75-th batch, train loss: 0.5444128513336182:  51%|██████▋      | 75/146 [00:45<00:43,  1.63it/s]evaluate for the 21-th batch, evaluate loss: 0.5294961333274841:  43%|███████▊          | 20/46 [00:02<00:02,  9.63it/s]evaluate for the 21-th batch, evaluate loss: 0.5294961333274841:  46%|████████▏         | 21/46 [00:02<00:02,  9.65it/s]evaluate for the 14-th batch, evaluate loss: 0.7400999069213867:  32%|█████▊            | 13/40 [00:03<00:07,  3.57it/s]evaluate for the 14-th batch, evaluate loss: 0.7400999069213867:  35%|██████▎           | 14/40 [00:03<00:07,  3.63it/s]evaluate for the 22-th batch, evaluate loss: 0.5208602547645569:  46%|████████▏         | 21/46 [00:02<00:02,  9.65it/s]evaluate for the 22-th batch, evaluate loss: 0.5208602547645569:  48%|████████▌         | 22/46 [00:02<00:02,  9.66it/s]evaluate for the 23-th batch, evaluate loss: 0.47523969411849976:  48%|████████▏        | 22/46 [00:02<00:02,  9.66it/s]evaluate for the 23-th batch, evaluate loss: 0.47523969411849976:  50%|████████▌        | 23/46 [00:02<00:02,  9.66it/s]evaluate for the 24-th batch, evaluate loss: 0.48188918828964233:  50%|████████▌        | 23/46 [00:02<00:02,  9.66it/s]evaluate for the 24-th batch, evaluate loss: 0.48188918828964233:  52%|████████▊        | 24/46 [00:02<00:02,  9.66it/s]evaluate for the 15-th batch, evaluate loss: 0.7476188540458679:  35%|██████▎           | 14/40 [00:04<00:07,  3.63it/s]evaluate for the 15-th batch, evaluate loss: 0.7476188540458679:  38%|██████▊           | 15/40 [00:04<00:06,  3.59it/s]evaluate for the 25-th batch, evaluate loss: 0.5359803438186646:  52%|█████████▍        | 24/46 [00:02<00:02,  9.66it/s]evaluate for the 25-th batch, evaluate loss: 0.5359803438186646:  54%|█████████▊        | 25/46 [00:02<00:02,  9.66it/s]Epoch: 8, train for the 80-th batch, train loss: 0.18571247160434723:  66%|███████▉    | 79/119 [00:48<00:24,  1.62it/s]Epoch: 8, train for the 80-th batch, train loss: 0.18571247160434723:  67%|████████    | 80/119 [00:48<00:24,  1.62it/s]Epoch: 3, train for the 178-th batch, train loss: 0.4599074423313141:  46%|█████      | 177/383 [01:46<01:58,  1.74it/s]Epoch: 3, train for the 178-th batch, train loss: 0.4599074423313141:  46%|█████      | 178/383 [01:46<01:58,  1.73it/s]evaluate for the 26-th batch, evaluate loss: 0.5563391447067261:  54%|█████████▊        | 25/46 [00:02<00:02,  9.66it/s]evaluate for the 26-th batch, evaluate loss: 0.5563391447067261:  57%|██████████▏       | 26/46 [00:02<00:02,  9.70it/s]Epoch: 7, train for the 76-th batch, train loss: 0.5505975484848022:  51%|██████▋      | 75/146 [00:46<00:43,  1.63it/s]Epoch: 7, train for the 76-th batch, train loss: 0.5505975484848022:  52%|██████▊      | 76/146 [00:46<00:43,  1.63it/s]evaluate for the 27-th batch, evaluate loss: 0.49483129382133484:  57%|█████████▌       | 26/46 [00:02<00:02,  9.70it/s]evaluate for the 27-th batch, evaluate loss: 0.49483129382133484:  59%|█████████▉       | 27/46 [00:02<00:01,  9.71it/s]evaluate for the 16-th batch, evaluate loss: 0.6896197199821472:  38%|██████▊           | 15/40 [00:04<00:06,  3.59it/s]evaluate for the 16-th batch, evaluate loss: 0.6896197199821472:  40%|███████▏          | 16/40 [00:04<00:06,  3.56it/s]evaluate for the 28-th batch, evaluate loss: 0.5224626660346985:  59%|██████████▌       | 27/46 [00:02<00:01,  9.71it/s]evaluate for the 28-th batch, evaluate loss: 0.5224626660346985:  61%|██████████▉       | 28/46 [00:02<00:01,  9.70it/s]evaluate for the 29-th batch, evaluate loss: 0.49176138639450073:  61%|██████████▎      | 28/46 [00:02<00:01,  9.70it/s]evaluate for the 29-th batch, evaluate loss: 0.49176138639450073:  63%|██████████▋      | 29/46 [00:02<00:01,  9.70it/s]evaluate for the 30-th batch, evaluate loss: 0.49198102951049805:  63%|██████████▋      | 29/46 [00:03<00:01,  9.70it/s]evaluate for the 30-th batch, evaluate loss: 0.49198102951049805:  65%|███████████      | 30/46 [00:03<00:01,  9.69it/s]evaluate for the 17-th batch, evaluate loss: 0.6906839609146118:  40%|███████▏          | 16/40 [00:04<00:06,  3.56it/s]evaluate for the 17-th batch, evaluate loss: 0.6906839609146118:  42%|███████▋          | 17/40 [00:04<00:06,  3.53it/s]evaluate for the 31-th batch, evaluate loss: 0.5201165080070496:  65%|███████████▋      | 30/46 [00:03<00:01,  9.69it/s]evaluate for the 31-th batch, evaluate loss: 0.5201165080070496:  67%|████████████▏     | 31/46 [00:03<00:01,  9.67it/s]Epoch: 8, train for the 81-th batch, train loss: 0.18109704554080963:  67%|████████    | 80/119 [00:49<00:24,  1.62it/s]Epoch: 8, train for the 81-th batch, train loss: 0.18109704554080963:  68%|████████▏   | 81/119 [00:49<00:23,  1.62it/s]Epoch: 3, train for the 179-th batch, train loss: 0.3283539414405823:  46%|█████      | 178/383 [01:47<01:58,  1.73it/s]Epoch: 3, train for the 179-th batch, train loss: 0.3283539414405823:  47%|█████▏     | 179/383 [01:47<01:58,  1.72it/s]evaluate for the 32-th batch, evaluate loss: 0.48050636053085327:  67%|███████████▍     | 31/46 [00:03<00:01,  9.67it/s]evaluate for the 32-th batch, evaluate loss: 0.48050636053085327:  70%|███████████▊     | 32/46 [00:03<00:01,  9.66it/s]Epoch: 7, train for the 77-th batch, train loss: 0.5727556347846985:  52%|██████▊      | 76/146 [00:47<00:43,  1.63it/s]Epoch: 7, train for the 77-th batch, train loss: 0.5727556347846985:  53%|██████▊      | 77/146 [00:47<00:42,  1.63it/s]evaluate for the 33-th batch, evaluate loss: 0.49928683042526245:  70%|███████████▊     | 32/46 [00:03<00:01,  9.66it/s]evaluate for the 33-th batch, evaluate loss: 0.49928683042526245:  72%|████████████▏    | 33/46 [00:03<00:01,  9.64it/s]evaluate for the 18-th batch, evaluate loss: 0.7052375078201294:  42%|███████▋          | 17/40 [00:05<00:06,  3.53it/s]evaluate for the 18-th batch, evaluate loss: 0.7052375078201294:  45%|████████          | 18/40 [00:05<00:06,  3.48it/s]evaluate for the 34-th batch, evaluate loss: 0.48361936211586:  72%|██████████████▎     | 33/46 [00:03<00:01,  9.64it/s]evaluate for the 34-th batch, evaluate loss: 0.48361936211586:  74%|██████████████▊     | 34/46 [00:03<00:01,  9.65it/s]evaluate for the 35-th batch, evaluate loss: 0.4820188283920288:  74%|█████████████▎    | 34/46 [00:03<00:01,  9.65it/s]evaluate for the 35-th batch, evaluate loss: 0.4820188283920288:  76%|█████████████▋    | 35/46 [00:03<00:01,  9.68it/s]evaluate for the 19-th batch, evaluate loss: 0.708805501461029:  45%|████████▌          | 18/40 [00:05<00:06,  3.48it/s]evaluate for the 19-th batch, evaluate loss: 0.708805501461029:  48%|█████████          | 19/40 [00:05<00:06,  3.50it/s]evaluate for the 36-th batch, evaluate loss: 0.4700636863708496:  76%|█████████████▋    | 35/46 [00:03<00:01,  9.68it/s]evaluate for the 36-th batch, evaluate loss: 0.4700636863708496:  78%|██████████████    | 36/46 [00:03<00:01,  9.69it/s]evaluate for the 37-th batch, evaluate loss: 0.5035409927368164:  78%|██████████████    | 36/46 [00:03<00:01,  9.69it/s]evaluate for the 37-th batch, evaluate loss: 0.5035409927368164:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.70it/s]Epoch: 8, train for the 82-th batch, train loss: 0.18318483233451843:  68%|████████▏   | 81/119 [00:50<00:23,  1.62it/s]Epoch: 8, train for the 82-th batch, train loss: 0.18318483233451843:  69%|████████▎   | 82/119 [00:50<00:22,  1.62it/s]Epoch: 3, train for the 180-th batch, train loss: 0.43177542090415955:  47%|████▋     | 179/383 [01:47<01:58,  1.72it/s]Epoch: 3, train for the 180-th batch, train loss: 0.43177542090415955:  47%|████▋     | 180/383 [01:47<01:58,  1.72it/s]evaluate for the 38-th batch, evaluate loss: 0.5343317985534668:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.70it/s]evaluate for the 38-th batch, evaluate loss: 0.5343317985534668:  83%|██████████████▊   | 38/46 [00:03<00:00,  9.72it/s]evaluate for the 20-th batch, evaluate loss: 0.7506433129310608:  48%|████████▌         | 19/40 [00:05<00:06,  3.50it/s]evaluate for the 20-th batch, evaluate loss: 0.7506433129310608:  50%|█████████         | 20/40 [00:05<00:05,  3.45it/s]Epoch: 7, train for the 78-th batch, train loss: 0.5491653680801392:  53%|██████▊      | 77/146 [00:47<00:42,  1.63it/s]Epoch: 7, train for the 78-th batch, train loss: 0.5491653680801392:  53%|██████▉      | 78/146 [00:47<00:41,  1.63it/s]evaluate for the 39-th batch, evaluate loss: 0.536174476146698:  83%|███████████████▋   | 38/46 [00:04<00:00,  9.72it/s]evaluate for the 39-th batch, evaluate loss: 0.536174476146698:  85%|████████████████   | 39/46 [00:04<00:00,  9.73it/s]evaluate for the 40-th batch, evaluate loss: 0.46717894077301025:  85%|██████████████▍  | 39/46 [00:04<00:00,  9.73it/s]evaluate for the 40-th batch, evaluate loss: 0.46717894077301025:  87%|██████████████▊  | 40/46 [00:04<00:00,  9.70it/s]evaluate for the 41-th batch, evaluate loss: 0.4798726439476013:  87%|███████████████▋  | 40/46 [00:04<00:00,  9.70it/s]evaluate for the 41-th batch, evaluate loss: 0.4798726439476013:  89%|████████████████  | 41/46 [00:04<00:00,  9.70it/s]evaluate for the 21-th batch, evaluate loss: 0.7413665652275085:  50%|█████████         | 20/40 [00:05<00:05,  3.45it/s]evaluate for the 21-th batch, evaluate loss: 0.7413665652275085:  52%|█████████▍        | 21/40 [00:05<00:05,  3.51it/s]evaluate for the 42-th batch, evaluate loss: 0.4727911949157715:  89%|████████████████  | 41/46 [00:04<00:00,  9.70it/s]evaluate for the 42-th batch, evaluate loss: 0.4727911949157715:  91%|████████████████▍ | 42/46 [00:04<00:00,  9.70it/s]Epoch: 3, train for the 181-th batch, train loss: 0.32528090476989746:  47%|████▋     | 180/383 [01:48<01:58,  1.72it/s]Epoch: 3, train for the 181-th batch, train loss: 0.32528090476989746:  47%|████▋     | 181/383 [01:48<01:57,  1.72it/s]evaluate for the 43-th batch, evaluate loss: 0.5341485738754272:  91%|████████████████▍ | 42/46 [00:04<00:00,  9.70it/s]evaluate for the 43-th batch, evaluate loss: 0.5341485738754272:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.71it/s]Epoch: 8, train for the 83-th batch, train loss: 0.19966495037078857:  69%|████████▎   | 82/119 [00:50<00:22,  1.62it/s]Epoch: 8, train for the 83-th batch, train loss: 0.19966495037078857:  70%|████████▎   | 83/119 [00:50<00:22,  1.62it/s]evaluate for the 44-th batch, evaluate loss: 0.5131531357765198:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.71it/s]evaluate for the 44-th batch, evaluate loss: 0.5131531357765198:  96%|█████████████████▏| 44/46 [00:04<00:00,  9.71it/s]evaluate for the 22-th batch, evaluate loss: 0.7710020542144775:  52%|█████████▍        | 21/40 [00:06<00:05,  3.51it/s]evaluate for the 22-th batch, evaluate loss: 0.7710020542144775:  55%|█████████▉        | 22/40 [00:06<00:05,  3.46it/s]Epoch: 7, train for the 79-th batch, train loss: 0.5460041761398315:  53%|██████▉      | 78/146 [00:48<00:41,  1.63it/s]Epoch: 7, train for the 79-th batch, train loss: 0.5460041761398315:  54%|███████      | 79/146 [00:48<00:41,  1.62it/s]evaluate for the 45-th batch, evaluate loss: 0.4939013421535492:  96%|█████████████████▏| 44/46 [00:04<00:00,  9.71it/s]evaluate for the 45-th batch, evaluate loss: 0.4939013421535492:  98%|█████████████████▌| 45/46 [00:04<00:00,  9.66it/s]evaluate for the 46-th batch, evaluate loss: 0.51199871301651:  98%|███████████████████▌| 45/46 [00:04<00:00,  9.66it/s]evaluate for the 46-th batch, evaluate loss: 0.51199871301651: 100%|████████████████████| 46/46 [00:04<00:00,  9.71it/s]
  0%|                                                                                            | 0/25 [00:00<?, ?it/s]evaluate for the 23-th batch, evaluate loss: 0.6784180998802185:  55%|█████████▉        | 22/40 [00:06<00:05,  3.46it/s]evaluate for the 23-th batch, evaluate loss: 0.6784180998802185:  57%|██████████▎       | 23/40 [00:06<00:04,  3.55it/s]evaluate for the 1-th batch, evaluate loss: 0.6361876130104065:   0%|                            | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6361876130104065:   4%|▊                   | 1/25 [00:00<00:02,  9.15it/s]evaluate for the 2-th batch, evaluate loss: 0.6482452154159546:   4%|▊                   | 1/25 [00:00<00:02,  9.15it/s]evaluate for the 2-th batch, evaluate loss: 0.6482452154159546:   8%|█▌                  | 2/25 [00:00<00:02,  9.20it/s]Epoch: 3, train for the 182-th batch, train loss: 0.5168647766113281:  47%|█████▏     | 181/383 [01:48<01:57,  1.72it/s]Epoch: 3, train for the 182-th batch, train loss: 0.5168647766113281:  48%|█████▏     | 182/383 [01:48<01:57,  1.71it/s]evaluate for the 3-th batch, evaluate loss: 0.6865555644035339:   8%|█▌                  | 2/25 [00:00<00:02,  9.20it/s]evaluate for the 3-th batch, evaluate loss: 0.6865555644035339:  12%|██▍                 | 3/25 [00:00<00:02,  9.20it/s]Epoch: 8, train for the 84-th batch, train loss: 0.1642075628042221:  70%|█████████    | 83/119 [00:51<00:22,  1.62it/s]Epoch: 8, train for the 84-th batch, train loss: 0.1642075628042221:  71%|█████████▏   | 84/119 [00:51<00:21,  1.62it/s]evaluate for the 24-th batch, evaluate loss: 0.7089242935180664:  57%|██████████▎       | 23/40 [00:06<00:04,  3.55it/s]evaluate for the 24-th batch, evaluate loss: 0.7089242935180664:  60%|██████████▊       | 24/40 [00:06<00:04,  3.47it/s]evaluate for the 4-th batch, evaluate loss: 0.6737606525421143:  12%|██▍                 | 3/25 [00:00<00:02,  9.20it/s]evaluate for the 4-th batch, evaluate loss: 0.6737606525421143:  16%|███▏                | 4/25 [00:00<00:02,  9.25it/s]Epoch: 7, train for the 80-th batch, train loss: 0.5366394519805908:  54%|███████      | 79/146 [00:49<00:41,  1.62it/s]Epoch: 7, train for the 80-th batch, train loss: 0.5366394519805908:  55%|███████      | 80/146 [00:49<00:40,  1.62it/s]evaluate for the 5-th batch, evaluate loss: 0.6730329990386963:  16%|███▏                | 4/25 [00:00<00:02,  9.25it/s]evaluate for the 5-th batch, evaluate loss: 0.6730329990386963:  20%|████                | 5/25 [00:00<00:02,  9.22it/s]evaluate for the 25-th batch, evaluate loss: 0.6941253542900085:  60%|██████████▊       | 24/40 [00:07<00:04,  3.47it/s]evaluate for the 25-th batch, evaluate loss: 0.6941253542900085:  62%|███████████▎      | 25/40 [00:07<00:04,  3.61it/s]evaluate for the 6-th batch, evaluate loss: 0.7124378681182861:  20%|████                | 5/25 [00:00<00:02,  9.22it/s]evaluate for the 6-th batch, evaluate loss: 0.7124378681182861:  24%|████▊               | 6/25 [00:00<00:02,  9.24it/s]evaluate for the 7-th batch, evaluate loss: 0.7386704683303833:  24%|████▊               | 6/25 [00:00<00:02,  9.24it/s]evaluate for the 7-th batch, evaluate loss: 0.7386704683303833:  28%|█████▌              | 7/25 [00:00<00:01,  9.25it/s]Epoch: 3, train for the 183-th batch, train loss: 0.37932631373405457:  48%|████▊     | 182/383 [01:49<01:57,  1.71it/s]Epoch: 3, train for the 183-th batch, train loss: 0.37932631373405457:  48%|████▊     | 183/383 [01:49<01:55,  1.72it/s]evaluate for the 8-th batch, evaluate loss: 0.7164134979248047:  28%|█████▌              | 7/25 [00:00<00:01,  9.25it/s]evaluate for the 8-th batch, evaluate loss: 0.7164134979248047:  32%|██████▍             | 8/25 [00:00<00:01,  9.25it/s]evaluate for the 26-th batch, evaluate loss: 0.7055054903030396:  62%|███████████▎      | 25/40 [00:07<00:04,  3.61it/s]evaluate for the 26-th batch, evaluate loss: 0.7055054903030396:  65%|███████████▋      | 26/40 [00:07<00:03,  3.51it/s]Epoch: 8, train for the 85-th batch, train loss: 0.17964677512645721:  71%|████████▍   | 84/119 [00:52<00:21,  1.62it/s]Epoch: 8, train for the 85-th batch, train loss: 0.17964677512645721:  71%|████████▌   | 85/119 [00:52<00:20,  1.62it/s]evaluate for the 9-th batch, evaluate loss: 0.6978088021278381:  32%|██████▍             | 8/25 [00:00<00:01,  9.25it/s]evaluate for the 9-th batch, evaluate loss: 0.6978088021278381:  36%|███████▏            | 9/25 [00:00<00:01,  9.23it/s]evaluate for the 10-th batch, evaluate loss: 0.7303044199943542:  36%|██████▊            | 9/25 [00:01<00:01,  9.23it/s]evaluate for the 10-th batch, evaluate loss: 0.7303044199943542:  40%|███████▏          | 10/25 [00:01<00:01,  9.22it/s]Epoch: 7, train for the 81-th batch, train loss: 0.5146558284759521:  55%|███████      | 80/146 [00:49<00:40,  1.62it/s]Epoch: 7, train for the 81-th batch, train loss: 0.5146558284759521:  55%|███████▏     | 81/146 [00:49<00:40,  1.62it/s]evaluate for the 11-th batch, evaluate loss: 0.7255721688270569:  40%|███████▏          | 10/25 [00:01<00:01,  9.22it/s]evaluate for the 11-th batch, evaluate loss: 0.7255721688270569:  44%|███████▉          | 11/25 [00:01<00:01,  9.22it/s]evaluate for the 27-th batch, evaluate loss: 0.7154743671417236:  65%|███████████▋      | 26/40 [00:07<00:03,  3.51it/s]evaluate for the 27-th batch, evaluate loss: 0.7154743671417236:  68%|████████████▏     | 27/40 [00:07<00:03,  3.66it/s]evaluate for the 12-th batch, evaluate loss: 0.6956413388252258:  44%|███████▉          | 11/25 [00:01<00:01,  9.22it/s]evaluate for the 12-th batch, evaluate loss: 0.6956413388252258:  48%|████████▋         | 12/25 [00:01<00:01,  9.23it/s]evaluate for the 13-th batch, evaluate loss: 0.6602029204368591:  48%|████████▋         | 12/25 [00:01<00:01,  9.23it/s]evaluate for the 13-th batch, evaluate loss: 0.6602029204368591:  52%|█████████▎        | 13/25 [00:01<00:01,  9.24it/s]Epoch: 3, train for the 184-th batch, train loss: 0.441485732793808:  48%|█████▋      | 183/383 [01:50<01:55,  1.72it/s]Epoch: 3, train for the 184-th batch, train loss: 0.441485732793808:  48%|█████▊      | 184/383 [01:50<01:55,  1.73it/s]evaluate for the 28-th batch, evaluate loss: 0.7223799228668213:  68%|████████████▏     | 27/40 [00:07<00:03,  3.66it/s]evaluate for the 28-th batch, evaluate loss: 0.7223799228668213:  70%|████████████▌     | 28/40 [00:07<00:03,  3.54it/s]evaluate for the 14-th batch, evaluate loss: 0.7452653050422668:  52%|█████████▎        | 13/25 [00:01<00:01,  9.24it/s]evaluate for the 14-th batch, evaluate loss: 0.7452653050422668:  56%|██████████        | 14/25 [00:01<00:01,  9.24it/s]Epoch: 8, train for the 86-th batch, train loss: 0.16707012057304382:  71%|████████▌   | 85/119 [00:52<00:20,  1.62it/s]Epoch: 8, train for the 86-th batch, train loss: 0.16707012057304382:  72%|████████▋   | 86/119 [00:52<00:20,  1.61it/s]evaluate for the 15-th batch, evaluate loss: 0.7232306003570557:  56%|██████████        | 14/25 [00:01<00:01,  9.24it/s]evaluate for the 15-th batch, evaluate loss: 0.7232306003570557:  60%|██████████▊       | 15/25 [00:01<00:01,  9.25it/s]evaluate for the 16-th batch, evaluate loss: 0.6616592407226562:  60%|██████████▊       | 15/25 [00:01<00:01,  9.25it/s]evaluate for the 16-th batch, evaluate loss: 0.6616592407226562:  64%|███████████▌      | 16/25 [00:01<00:00,  9.23it/s]evaluate for the 29-th batch, evaluate loss: 0.7471156120300293:  70%|████████████▌     | 28/40 [00:08<00:03,  3.54it/s]evaluate for the 29-th batch, evaluate loss: 0.7471156120300293:  72%|█████████████     | 29/40 [00:08<00:02,  3.70it/s]Epoch: 7, train for the 82-th batch, train loss: 0.5263634920120239:  55%|███████▏     | 81/146 [00:50<00:40,  1.62it/s]Epoch: 7, train for the 82-th batch, train loss: 0.5263634920120239:  56%|███████▎     | 82/146 [00:50<00:39,  1.61it/s]evaluate for the 17-th batch, evaluate loss: 0.656227707862854:  64%|████████████▏      | 16/25 [00:01<00:00,  9.23it/s]evaluate for the 17-th batch, evaluate loss: 0.656227707862854:  68%|████████████▉      | 17/25 [00:01<00:00,  9.24it/s]evaluate for the 18-th batch, evaluate loss: 0.6268365383148193:  68%|████████████▏     | 17/25 [00:01<00:00,  9.24it/s]evaluate for the 18-th batch, evaluate loss: 0.6268365383148193:  72%|████████████▉     | 18/25 [00:01<00:00,  9.24it/s]Epoch: 3, train for the 185-th batch, train loss: 0.37829142808914185:  48%|████▊     | 184/383 [01:50<01:55,  1.73it/s]Epoch: 3, train for the 185-th batch, train loss: 0.37829142808914185:  48%|████▊     | 185/383 [01:50<01:54,  1.73it/s]evaluate for the 30-th batch, evaluate loss: 0.7309865951538086:  72%|█████████████     | 29/40 [00:08<00:02,  3.70it/s]evaluate for the 30-th batch, evaluate loss: 0.7309865951538086:  75%|█████████████▌    | 30/40 [00:08<00:02,  3.54it/s]evaluate for the 19-th batch, evaluate loss: 0.5912694334983826:  72%|████████████▉     | 18/25 [00:02<00:00,  9.24it/s]evaluate for the 19-th batch, evaluate loss: 0.5912694334983826:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.25it/s]evaluate for the 20-th batch, evaluate loss: 0.6560211181640625:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.25it/s]evaluate for the 20-th batch, evaluate loss: 0.6560211181640625:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.24it/s]Epoch: 8, train for the 87-th batch, train loss: 0.15603885054588318:  72%|████████▋   | 86/119 [00:53<00:20,  1.61it/s]Epoch: 8, train for the 87-th batch, train loss: 0.15603885054588318:  73%|████████▊   | 87/119 [00:53<00:19,  1.61it/s]evaluate for the 21-th batch, evaluate loss: 0.723578929901123:  80%|███████████████▏   | 20/25 [00:02<00:00,  9.24it/s]evaluate for the 21-th batch, evaluate loss: 0.723578929901123:  84%|███████████████▉   | 21/25 [00:02<00:00,  9.23it/s]evaluate for the 31-th batch, evaluate loss: 0.6941350102424622:  75%|█████████████▌    | 30/40 [00:08<00:02,  3.54it/s]evaluate for the 31-th batch, evaluate loss: 0.6941350102424622:  78%|█████████████▉    | 31/40 [00:08<00:02,  3.69it/s]Epoch: 7, train for the 83-th batch, train loss: 0.5211846828460693:  56%|███████▎     | 82/146 [00:50<00:39,  1.61it/s]Epoch: 7, train for the 83-th batch, train loss: 0.5211846828460693:  57%|███████▍     | 83/146 [00:50<00:39,  1.61it/s]evaluate for the 22-th batch, evaluate loss: 0.6000388264656067:  84%|███████████████   | 21/25 [00:02<00:00,  9.23it/s]evaluate for the 22-th batch, evaluate loss: 0.6000388264656067:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.26it/s]evaluate for the 32-th batch, evaluate loss: 0.7394373416900635:  78%|█████████████▉    | 31/40 [00:08<00:02,  3.69it/s]evaluate for the 32-th batch, evaluate loss: 0.7394373416900635:  80%|██████████████▍   | 32/40 [00:08<00:01,  4.22it/s]evaluate for the 23-th batch, evaluate loss: 0.6608955264091492:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.26it/s]evaluate for the 23-th batch, evaluate loss: 0.6608955264091492:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.26it/s]evaluate for the 24-th batch, evaluate loss: 0.6563133001327515:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.26it/s]evaluate for the 24-th batch, evaluate loss: 0.6563133001327515:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.19it/s]Epoch: 3, train for the 186-th batch, train loss: 0.39201870560646057:  48%|████▊     | 185/383 [01:51<01:54,  1.73it/s]Epoch: 3, train for the 186-th batch, train loss: 0.39201870560646057:  49%|████▊     | 186/383 [01:51<01:58,  1.66it/s]evaluate for the 25-th batch, evaluate loss: 0.7045803666114807:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.19it/s]evaluate for the 25-th batch, evaluate loss: 0.7045803666114807: 100%|██████████████████| 25/25 [00:02<00:00,  9.30it/s]
INFO:root:Epoch: 13, learning rate: 0.0001, train loss: 0.5588
INFO:root:train average_precision, 0.8225
INFO:root:train roc_auc, 0.7871
INFO:root:validate loss: 0.5089
INFO:root:validate average_precision, 0.8426
INFO:root:validate roc_auc, 0.8035
INFO:root:new node validate loss: 0.6800
INFO:root:new node validate first_1_average_precision, 0.5938
INFO:root:new node validate first_1_roc_auc, 0.5417
INFO:root:new node validate first_3_average_precision, 0.6774
INFO:root:new node validate first_3_roc_auc, 0.6369
INFO:root:new node validate first_10_average_precision, 0.7463
INFO:root:new node validate first_10_roc_auc, 0.7102
INFO:root:new node validate average_precision, 0.7093
INFO:root:new node validate roc_auc, 0.6568
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]Epoch: 8, train for the 88-th batch, train loss: 0.23670348525047302:  73%|████████▊   | 87/119 [00:53<00:19,  1.61it/s]Epoch: 8, train for the 88-th batch, train loss: 0.23670348525047302:  74%|████████▊   | 88/119 [00:53<00:18,  1.72it/s]evaluate for the 33-th batch, evaluate loss: 0.7328075766563416:  80%|██████████████▍   | 32/40 [00:09<00:01,  4.22it/s]evaluate for the 33-th batch, evaluate loss: 0.7328075766563416:  82%|██████████████▊   | 33/40 [00:09<00:01,  3.90it/s]Epoch: 14, train for the 1-th batch, train loss: 1.4648592472076416:   0%|                      | 0/151 [00:00<?, ?it/s]Epoch: 14, train for the 1-th batch, train loss: 1.4648592472076416:   1%|              | 1/151 [00:00<00:26,  5.61it/s]evaluate for the 34-th batch, evaluate loss: 0.7461420893669128:  82%|██████████████▊   | 33/40 [00:09<00:01,  3.90it/s]evaluate for the 34-th batch, evaluate loss: 0.7461420893669128:  85%|███████████████▎  | 34/40 [00:09<00:01,  3.95it/s]Epoch: 7, train for the 84-th batch, train loss: 0.5106421709060669:  57%|███████▍     | 83/146 [00:51<00:39,  1.61it/s]Epoch: 7, train for the 84-th batch, train loss: 0.5106421709060669:  58%|███████▍     | 84/146 [00:51<00:39,  1.56it/s]Epoch: 14, train for the 2-th batch, train loss: 1.4511321783065796:   1%|              | 1/151 [00:00<00:26,  5.61it/s]Epoch: 14, train for the 2-th batch, train loss: 1.4511321783065796:   1%|▏             | 2/151 [00:00<00:26,  5.54it/s]Epoch: 8, train for the 89-th batch, train loss: 0.20965667068958282:  74%|████████▊   | 88/119 [00:54<00:18,  1.72it/s]Epoch: 8, train for the 89-th batch, train loss: 0.20965667068958282:  75%|████████▉   | 89/119 [00:54<00:16,  1.77it/s]Epoch: 14, train for the 3-th batch, train loss: 0.6035869717597961:   1%|▏             | 2/151 [00:00<00:26,  5.54it/s]Epoch: 14, train for the 3-th batch, train loss: 0.6035869717597961:   2%|▎             | 3/151 [00:00<00:25,  5.79it/s]Epoch: 3, train for the 187-th batch, train loss: 0.3753783702850342:  49%|█████▎     | 186/383 [01:51<01:58,  1.66it/s]Epoch: 3, train for the 187-th batch, train loss: 0.3753783702850342:  49%|█████▎     | 187/383 [01:51<01:56,  1.68it/s]evaluate for the 35-th batch, evaluate loss: 0.7274067401885986:  85%|███████████████▎  | 34/40 [00:09<00:01,  3.95it/s]evaluate for the 35-th batch, evaluate loss: 0.7274067401885986:  88%|███████████████▊  | 35/40 [00:09<00:01,  3.71it/s]Epoch: 14, train for the 4-th batch, train loss: 0.4405190050601959:   2%|▎             | 3/151 [00:00<00:25,  5.79it/s]Epoch: 14, train for the 4-th batch, train loss: 0.4405190050601959:   3%|▎             | 4/151 [00:00<00:25,  5.70it/s]evaluate for the 36-th batch, evaluate loss: 0.7661073803901672:  88%|███████████████▊  | 35/40 [00:09<00:01,  3.71it/s]evaluate for the 36-th batch, evaluate loss: 0.7661073803901672:  90%|████████████████▏ | 36/40 [00:09<00:01,  3.86it/s]Epoch: 14, train for the 5-th batch, train loss: 0.5211288928985596:   3%|▎             | 4/151 [00:00<00:25,  5.70it/s]Epoch: 14, train for the 5-th batch, train loss: 0.5211288928985596:   3%|▍             | 5/151 [00:00<00:26,  5.55it/s]Epoch: 7, train for the 85-th batch, train loss: 0.508465051651001:  58%|████████      | 84/146 [00:52<00:39,  1.56it/s]Epoch: 7, train for the 85-th batch, train loss: 0.508465051651001:  58%|████████▏     | 85/146 [00:52<00:39,  1.56it/s]Epoch: 14, train for the 6-th batch, train loss: 0.4867313802242279:   3%|▍             | 5/151 [00:01<00:26,  5.55it/s]Epoch: 14, train for the 6-th batch, train loss: 0.4867313802242279:   4%|▌             | 6/151 [00:01<00:26,  5.46it/s]Epoch: 8, train for the 90-th batch, train loss: 0.19033488631248474:  75%|████████▉   | 89/119 [00:54<00:16,  1.77it/s]Epoch: 3, train for the 188-th batch, train loss: 0.4356238842010498:  49%|█████▎     | 187/383 [01:52<01:56,  1.68it/s]Epoch: 8, train for the 90-th batch, train loss: 0.19033488631248474:  76%|█████████   | 90/119 [00:54<00:16,  1.71it/s]Epoch: 3, train for the 188-th batch, train loss: 0.4356238842010498:  49%|█████▍     | 188/383 [01:52<01:56,  1.67it/s]evaluate for the 37-th batch, evaluate loss: 0.7614030838012695:  90%|████████████████▏ | 36/40 [00:10<00:01,  3.86it/s]evaluate for the 37-th batch, evaluate loss: 0.7614030838012695:  92%|████████████████▋ | 37/40 [00:10<00:00,  3.60it/s]Epoch: 14, train for the 7-th batch, train loss: 0.658056914806366:   4%|▌              | 6/151 [00:01<00:26,  5.46it/s]Epoch: 14, train for the 7-th batch, train loss: 0.658056914806366:   5%|▋              | 7/151 [00:01<00:27,  5.27it/s]evaluate for the 38-th batch, evaluate loss: 0.7903764247894287:  92%|████████████████▋ | 37/40 [00:10<00:00,  3.60it/s]evaluate for the 38-th batch, evaluate loss: 0.7903764247894287:  95%|█████████████████ | 38/40 [00:10<00:00,  3.67it/s]Epoch: 14, train for the 8-th batch, train loss: 0.6793361306190491:   5%|▋             | 7/151 [00:01<00:27,  5.27it/s]Epoch: 14, train for the 8-th batch, train loss: 0.6793361306190491:   5%|▋             | 8/151 [00:01<00:27,  5.28it/s]Epoch: 7, train for the 86-th batch, train loss: 0.5146386027336121:  58%|███████▌     | 85/146 [00:52<00:39,  1.56it/s]Epoch: 7, train for the 86-th batch, train loss: 0.5146386027336121:  59%|███████▋     | 86/146 [00:52<00:37,  1.58it/s]Epoch: 14, train for the 9-th batch, train loss: 0.5889927744865417:   5%|▋             | 8/151 [00:01<00:27,  5.28it/s]Epoch: 14, train for the 9-th batch, train loss: 0.5889927744865417:   6%|▊             | 9/151 [00:01<00:27,  5.24it/s]Epoch: 3, train for the 189-th batch, train loss: 0.44366446137428284:  49%|████▉     | 188/383 [01:53<01:56,  1.67it/s]Epoch: 3, train for the 189-th batch, train loss: 0.44366446137428284:  49%|████▉     | 189/383 [01:53<01:55,  1.68it/s]evaluate for the 39-th batch, evaluate loss: 0.7497807145118713:  95%|█████████████████ | 38/40 [00:10<00:00,  3.67it/s]evaluate for the 39-th batch, evaluate loss: 0.7497807145118713:  98%|█████████████████▌| 39/40 [00:10<00:00,  3.52it/s]Epoch: 8, train for the 91-th batch, train loss: 0.17390410602092743:  76%|█████████   | 90/119 [00:55<00:16,  1.71it/s]Epoch: 8, train for the 91-th batch, train loss: 0.17390410602092743:  76%|█████████▏  | 91/119 [00:55<00:16,  1.69it/s]Epoch: 14, train for the 10-th batch, train loss: 0.5562528967857361:   6%|▊            | 9/151 [00:01<00:27,  5.24it/s]Epoch: 14, train for the 10-th batch, train loss: 0.5562528967857361:   7%|▊           | 10/151 [00:01<00:27,  5.08it/s]evaluate for the 40-th batch, evaluate loss: 0.7477360963821411:  98%|█████████████████▌| 39/40 [00:11<00:00,  3.52it/s]evaluate for the 40-th batch, evaluate loss: 0.7477360963821411: 100%|██████████████████| 40/40 [00:11<00:00,  3.61it/s]evaluate for the 40-th batch, evaluate loss: 0.7477360963821411: 100%|██████████████████| 40/40 [00:11<00:00,  3.61it/s]
INFO:root:Epoch: 4, learning rate: 0.0001, train loss: 0.6170
INFO:root:train average_precision, 0.7551
INFO:root:train roc_auc, 0.7388
INFO:root:validate loss: 0.6194
INFO:root:validate average_precision, 0.7217
INFO:root:validate roc_auc, 0.7058
INFO:root:new node validate loss: 0.7177
INFO:root:new node validate first_1_average_precision, 0.5854
INFO:root:new node validate first_1_roc_auc, 0.5679
INFO:root:new node validate first_3_average_precision, 0.5860
INFO:root:new node validate first_3_roc_auc, 0.5802
INFO:root:new node validate first_10_average_precision, 0.6107
INFO:root:new node validate first_10_roc_auc, 0.6141
INFO:root:new node validate average_precision, 0.5833
INFO:root:new node validate roc_auc, 0.5759
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/237 [00:00<?, ?it/s]Epoch: 14, train for the 11-th batch, train loss: 0.5659904479980469:   7%|▊           | 10/151 [00:02<00:27,  5.08it/s]Epoch: 14, train for the 11-th batch, train loss: 0.5659904479980469:   7%|▊           | 11/151 [00:02<00:28,  5.00it/s]Epoch: 7, train for the 87-th batch, train loss: 0.5375689268112183:  59%|███████▋     | 86/146 [00:53<00:37,  1.58it/s]Epoch: 7, train for the 87-th batch, train loss: 0.5375689268112183:  60%|███████▋     | 87/146 [00:53<00:36,  1.60it/s]Epoch: 3, train for the 190-th batch, train loss: 0.43004071712493896:  49%|████▉     | 189/383 [01:53<01:55,  1.68it/s]Epoch: 3, train for the 190-th batch, train loss: 0.43004071712493896:  50%|████▉     | 190/383 [01:53<01:50,  1.75it/s]Epoch: 14, train for the 12-th batch, train loss: 0.5415728092193604:   7%|▊           | 11/151 [00:02<00:28,  5.00it/s]Epoch: 14, train for the 12-th batch, train loss: 0.5415728092193604:   8%|▉           | 12/151 [00:02<00:28,  4.95it/s]Epoch: 8, train for the 92-th batch, train loss: 0.2354346662759781:  76%|█████████▉   | 91/119 [00:56<00:16,  1.69it/s]Epoch: 8, train for the 92-th batch, train loss: 0.2354346662759781:  77%|██████████   | 92/119 [00:56<00:16,  1.67it/s]Epoch: 5, train for the 1-th batch, train loss: 0.6914833188056946:   0%|                       | 0/237 [00:00<?, ?it/s]Epoch: 5, train for the 1-th batch, train loss: 0.6914833188056946:   0%|               | 1/237 [00:00<01:56,  2.03it/s]Epoch: 14, train for the 13-th batch, train loss: 0.7353816032409668:   8%|▉           | 12/151 [00:02<00:28,  4.95it/s]Epoch: 14, train for the 13-th batch, train loss: 0.7353816032409668:   9%|█           | 13/151 [00:02<00:28,  4.87it/s]Epoch: 14, train for the 14-th batch, train loss: 0.5487363338470459:   9%|█           | 13/151 [00:02<00:28,  4.87it/s]Epoch: 14, train for the 14-th batch, train loss: 0.5487363338470459:   9%|█           | 14/151 [00:02<00:28,  4.88it/s]Epoch: 3, train for the 191-th batch, train loss: 0.3345838189125061:  50%|█████▍     | 190/383 [01:54<01:50,  1.75it/s]Epoch: 3, train for the 191-th batch, train loss: 0.3345838189125061:  50%|█████▍     | 191/383 [01:54<01:45,  1.82it/s]Epoch: 7, train for the 88-th batch, train loss: 0.5551759004592896:  60%|███████▋     | 87/146 [00:54<00:36,  1.60it/s]Epoch: 7, train for the 88-th batch, train loss: 0.5551759004592896:  60%|███████▊     | 88/146 [00:54<00:36,  1.61it/s]Epoch: 14, train for the 15-th batch, train loss: 0.6470542550086975:   9%|█           | 14/151 [00:02<00:28,  4.88it/s]Epoch: 14, train for the 15-th batch, train loss: 0.6470542550086975:  10%|█▏          | 15/151 [00:02<00:28,  4.80it/s]Epoch: 5, train for the 2-th batch, train loss: 0.6855501532554626:   0%|               | 1/237 [00:00<01:56,  2.03it/s]Epoch: 5, train for the 2-th batch, train loss: 0.6855501532554626:   1%|▏              | 2/237 [00:00<01:49,  2.15it/s]Epoch: 8, train for the 93-th batch, train loss: 0.14991909265518188:  77%|█████████▎  | 92/119 [00:56<00:16,  1.67it/s]Epoch: 8, train for the 93-th batch, train loss: 0.14991909265518188:  78%|█████████▍  | 93/119 [00:56<00:15,  1.66it/s]Epoch: 14, train for the 16-th batch, train loss: 0.4660419225692749:  10%|█▏          | 15/151 [00:03<00:28,  4.80it/s]Epoch: 14, train for the 16-th batch, train loss: 0.4660419225692749:  11%|█▎          | 16/151 [00:03<00:28,  4.79it/s]Epoch: 14, train for the 17-th batch, train loss: 0.5429003834724426:  11%|█▎          | 16/151 [00:03<00:28,  4.79it/s]Epoch: 14, train for the 17-th batch, train loss: 0.5429003834724426:  11%|█▎          | 17/151 [00:03<00:27,  4.85it/s]Epoch: 3, train for the 192-th batch, train loss: 0.4437468945980072:  50%|█████▍     | 191/383 [01:54<01:45,  1.82it/s]Epoch: 3, train for the 192-th batch, train loss: 0.4437468945980072:  50%|█████▌     | 192/383 [01:54<01:47,  1.77it/s]Epoch: 5, train for the 3-th batch, train loss: 0.6794562935829163:   1%|▏              | 2/237 [00:01<01:49,  2.15it/s]Epoch: 5, train for the 3-th batch, train loss: 0.6794562935829163:   1%|▏              | 3/237 [00:01<01:47,  2.18it/s]Epoch: 7, train for the 89-th batch, train loss: 0.5295965671539307:  60%|███████▊     | 88/146 [00:54<00:36,  1.61it/s]Epoch: 7, train for the 89-th batch, train loss: 0.5295965671539307:  61%|███████▉     | 89/146 [00:54<00:35,  1.62it/s]Epoch: 14, train for the 18-th batch, train loss: 0.5105312466621399:  11%|█▎          | 17/151 [00:03<00:27,  4.85it/s]Epoch: 14, train for the 18-th batch, train loss: 0.5105312466621399:  12%|█▍          | 18/151 [00:03<00:27,  4.89it/s]Epoch: 8, train for the 94-th batch, train loss: 0.1747552454471588:  78%|██████████▏  | 93/119 [00:57<00:15,  1.66it/s]Epoch: 8, train for the 94-th batch, train loss: 0.1747552454471588:  79%|██████████▎  | 94/119 [00:57<00:15,  1.66it/s]Epoch: 14, train for the 19-th batch, train loss: 0.49581512808799744:  12%|█▎         | 18/151 [00:03<00:27,  4.89it/s]Epoch: 14, train for the 19-th batch, train loss: 0.49581512808799744:  13%|█▍         | 19/151 [00:03<00:27,  4.88it/s]Epoch: 5, train for the 4-th batch, train loss: 0.6565168499946594:   1%|▏              | 3/237 [00:01<01:47,  2.18it/s]Epoch: 5, train for the 4-th batch, train loss: 0.6565168499946594:   2%|▎              | 4/237 [00:01<01:46,  2.19it/s]Epoch: 3, train for the 193-th batch, train loss: 0.4542110860347748:  50%|█████▌     | 192/383 [01:55<01:47,  1.77it/s]Epoch: 3, train for the 193-th batch, train loss: 0.4542110860347748:  50%|█████▌     | 193/383 [01:55<01:49,  1.74it/s]Epoch: 14, train for the 20-th batch, train loss: 0.5350494384765625:  13%|█▌          | 19/151 [00:03<00:27,  4.88it/s]Epoch: 14, train for the 20-th batch, train loss: 0.5350494384765625:  13%|█▌          | 20/151 [00:03<00:27,  4.82it/s]Epoch: 7, train for the 90-th batch, train loss: 0.5447373986244202:  61%|███████▉     | 89/146 [00:55<00:35,  1.62it/s]Epoch: 7, train for the 90-th batch, train loss: 0.5447373986244202:  62%|████████     | 90/146 [00:55<00:34,  1.63it/s]Epoch: 14, train for the 21-th batch, train loss: 0.4524787664413452:  13%|█▌          | 20/151 [00:04<00:27,  4.82it/s]Epoch: 14, train for the 21-th batch, train loss: 0.4524787664413452:  14%|█▋          | 21/151 [00:04<00:27,  4.78it/s]Epoch: 8, train for the 95-th batch, train loss: 0.12030181288719177:  79%|█████████▍  | 94/119 [00:57<00:15,  1.66it/s]Epoch: 8, train for the 95-th batch, train loss: 0.12030181288719177:  80%|█████████▌  | 95/119 [00:57<00:14,  1.65it/s]Epoch: 5, train for the 5-th batch, train loss: 0.651608407497406:   2%|▎               | 4/237 [00:02<01:46,  2.19it/s]Epoch: 5, train for the 5-th batch, train loss: 0.651608407497406:   2%|▎               | 5/237 [00:02<01:44,  2.21it/s]Epoch: 14, train for the 22-th batch, train loss: 0.3587252199649811:  14%|█▋          | 21/151 [00:04<00:27,  4.78it/s]Epoch: 14, train for the 22-th batch, train loss: 0.3587252199649811:  15%|█▋          | 22/151 [00:04<00:27,  4.78it/s]Epoch: 3, train for the 194-th batch, train loss: 0.34046950936317444:  50%|█████     | 193/383 [01:55<01:49,  1.74it/s]Epoch: 3, train for the 194-th batch, train loss: 0.34046950936317444:  51%|█████     | 194/383 [01:55<01:48,  1.74it/s]Epoch: 14, train for the 23-th batch, train loss: 0.3998452425003052:  15%|█▋          | 22/151 [00:04<00:27,  4.78it/s]Epoch: 14, train for the 23-th batch, train loss: 0.3998452425003052:  15%|█▊          | 23/151 [00:04<00:26,  4.76it/s]Epoch: 7, train for the 91-th batch, train loss: 0.5280875563621521:  62%|████████     | 90/146 [00:55<00:34,  1.63it/s]Epoch: 7, train for the 91-th batch, train loss: 0.5280875563621521:  62%|████████     | 91/146 [00:55<00:33,  1.62it/s]Epoch: 5, train for the 6-th batch, train loss: 0.6339877843856812:   2%|▎              | 5/237 [00:02<01:44,  2.21it/s]Epoch: 5, train for the 6-th batch, train loss: 0.6339877843856812:   3%|▍              | 6/237 [00:02<01:44,  2.20it/s]Epoch: 14, train for the 24-th batch, train loss: 0.8367257714271545:  15%|█▊          | 23/151 [00:04<00:26,  4.76it/s]Epoch: 14, train for the 24-th batch, train loss: 0.8367257714271545:  16%|█▉          | 24/151 [00:04<00:27,  4.65it/s]Epoch: 8, train for the 96-th batch, train loss: 0.20796820521354675:  80%|█████████▌  | 95/119 [00:58<00:14,  1.65it/s]Epoch: 8, train for the 96-th batch, train loss: 0.20796820521354675:  81%|█████████▋  | 96/119 [00:58<00:14,  1.64it/s]Epoch: 14, train for the 25-th batch, train loss: 0.7755892276763916:  16%|█▉          | 24/151 [00:05<00:27,  4.65it/s]Epoch: 14, train for the 25-th batch, train loss: 0.7755892276763916:  17%|█▉          | 25/151 [00:05<00:27,  4.57it/s]Epoch: 3, train for the 195-th batch, train loss: 0.4814504086971283:  51%|█████▌     | 194/383 [01:56<01:48,  1.74it/s]Epoch: 3, train for the 195-th batch, train loss: 0.4814504086971283:  51%|█████▌     | 195/383 [01:56<01:48,  1.73it/s]Epoch: 5, train for the 7-th batch, train loss: 0.6422898769378662:   3%|▍              | 6/237 [00:03<01:44,  2.20it/s]Epoch: 5, train for the 7-th batch, train loss: 0.6422898769378662:   3%|▍              | 7/237 [00:03<01:46,  2.17it/s]Epoch: 14, train for the 26-th batch, train loss: 0.5238147377967834:  17%|█▉          | 25/151 [00:05<00:27,  4.57it/s]Epoch: 14, train for the 26-th batch, train loss: 0.5238147377967834:  17%|██          | 26/151 [00:05<00:27,  4.53it/s]Epoch: 7, train for the 92-th batch, train loss: 0.5617599487304688:  62%|████████     | 91/146 [00:56<00:33,  1.62it/s]Epoch: 7, train for the 92-th batch, train loss: 0.5617599487304688:  63%|████████▏    | 92/146 [00:56<00:33,  1.62it/s]Epoch: 8, train for the 97-th batch, train loss: 0.17894256114959717:  81%|█████████▋  | 96/119 [00:59<00:14,  1.64it/s]Epoch: 8, train for the 97-th batch, train loss: 0.17894256114959717:  82%|█████████▊  | 97/119 [00:59<00:13,  1.63it/s]Epoch: 14, train for the 27-th batch, train loss: 0.44952690601348877:  17%|█▉         | 26/151 [00:05<00:27,  4.53it/s]Epoch: 14, train for the 27-th batch, train loss: 0.44952690601348877:  18%|█▉         | 27/151 [00:05<00:27,  4.59it/s]Epoch: 5, train for the 8-th batch, train loss: 0.6282572150230408:   3%|▍              | 7/237 [00:03<01:46,  2.17it/s]Epoch: 5, train for the 8-th batch, train loss: 0.6282572150230408:   3%|▌              | 8/237 [00:03<01:45,  2.17it/s]Epoch: 14, train for the 28-th batch, train loss: 0.6338896751403809:  18%|██▏         | 27/151 [00:05<00:27,  4.59it/s]Epoch: 14, train for the 28-th batch, train loss: 0.6338896751403809:  19%|██▏         | 28/151 [00:05<00:26,  4.56it/s]Epoch: 3, train for the 196-th batch, train loss: 0.4891138970851898:  51%|█████▌     | 195/383 [01:57<01:48,  1.73it/s]Epoch: 3, train for the 196-th batch, train loss: 0.4891138970851898:  51%|█████▋     | 196/383 [01:57<01:49,  1.71it/s]Epoch: 7, train for the 93-th batch, train loss: 0.554533839225769:  63%|████████▊     | 92/146 [00:57<00:33,  1.62it/s]Epoch: 7, train for the 93-th batch, train loss: 0.554533839225769:  64%|████████▉     | 93/146 [00:57<00:32,  1.62it/s]Epoch: 14, train for the 29-th batch, train loss: 0.6586318016052246:  19%|██▏         | 28/151 [00:06<00:26,  4.56it/s]Epoch: 14, train for the 29-th batch, train loss: 0.6586318016052246:  19%|██▎         | 29/151 [00:06<00:30,  4.02it/s]Epoch: 8, train for the 98-th batch, train loss: 0.1232268288731575:  82%|██████████▌  | 97/119 [00:59<00:13,  1.63it/s]Epoch: 8, train for the 98-th batch, train loss: 0.1232268288731575:  82%|██████████▋  | 98/119 [00:59<00:12,  1.63it/s]Epoch: 5, train for the 9-th batch, train loss: 0.6038733720779419:   3%|▌              | 8/237 [00:04<01:45,  2.17it/s]Epoch: 5, train for the 9-th batch, train loss: 0.6038733720779419:   4%|▌              | 9/237 [00:04<01:42,  2.23it/s]Epoch: 14, train for the 30-th batch, train loss: 0.5486488342285156:  19%|██▎         | 29/151 [00:06<00:30,  4.02it/s]Epoch: 14, train for the 30-th batch, train loss: 0.5486488342285156:  20%|██▍         | 30/151 [00:06<00:28,  4.20it/s]Epoch: 3, train for the 197-th batch, train loss: 0.5031023025512695:  51%|█████▋     | 196/383 [01:57<01:49,  1.71it/s]Epoch: 3, train for the 197-th batch, train loss: 0.5031023025512695:  51%|█████▋     | 197/383 [01:57<01:47,  1.72it/s]Epoch: 14, train for the 31-th batch, train loss: 0.5325710773468018:  20%|██▍         | 30/151 [00:06<00:28,  4.20it/s]Epoch: 14, train for the 31-th batch, train loss: 0.5325710773468018:  21%|██▍         | 31/151 [00:06<00:27,  4.33it/s]Epoch: 7, train for the 94-th batch, train loss: 0.5279162526130676:  64%|████████▎    | 93/146 [00:57<00:32,  1.62it/s]Epoch: 7, train for the 94-th batch, train loss: 0.5279162526130676:  64%|████████▎    | 94/146 [00:57<00:32,  1.62it/s]Epoch: 5, train for the 10-th batch, train loss: 0.6264403462409973:   4%|▌             | 9/237 [00:04<01:42,  2.23it/s]Epoch: 5, train for the 10-th batch, train loss: 0.6264403462409973:   4%|▌            | 10/237 [00:04<01:43,  2.19it/s]Epoch: 8, train for the 99-th batch, train loss: 0.17659391462802887:  82%|█████████▉  | 98/119 [01:00<00:12,  1.63it/s]Epoch: 14, train for the 32-th batch, train loss: 0.640886127948761:  21%|██▋          | 31/151 [00:06<00:27,  4.33it/s]Epoch: 8, train for the 99-th batch, train loss: 0.17659391462802887:  83%|█████████▉  | 99/119 [01:00<00:12,  1.63it/s]Epoch: 14, train for the 32-th batch, train loss: 0.640886127948761:  21%|██▊          | 32/151 [00:06<00:27,  4.36it/s]Epoch: 3, train for the 198-th batch, train loss: 0.3571557402610779:  51%|█████▋     | 197/383 [01:58<01:47,  1.72it/s]Epoch: 3, train for the 198-th batch, train loss: 0.3571557402610779:  52%|█████▋     | 198/383 [01:58<01:49,  1.70it/s]Epoch: 14, train for the 33-th batch, train loss: 0.6310300827026367:  21%|██▌         | 32/151 [00:06<00:27,  4.36it/s]Epoch: 14, train for the 33-th batch, train loss: 0.6310300827026367:  22%|██▌         | 33/151 [00:06<00:26,  4.40it/s]Epoch: 5, train for the 11-th batch, train loss: 0.6220102310180664:   4%|▌            | 10/237 [00:05<01:43,  2.19it/s]Epoch: 5, train for the 11-th batch, train loss: 0.6220102310180664:   5%|▌            | 11/237 [00:05<01:45,  2.14it/s]Epoch: 14, train for the 34-th batch, train loss: 0.5493742823600769:  22%|██▌         | 33/151 [00:07<00:26,  4.40it/s]Epoch: 14, train for the 34-th batch, train loss: 0.5493742823600769:  23%|██▋         | 34/151 [00:07<00:26,  4.46it/s]Epoch: 7, train for the 95-th batch, train loss: 0.5326599478721619:  64%|████████▎    | 94/146 [00:58<00:32,  1.62it/s]Epoch: 7, train for the 95-th batch, train loss: 0.5326599478721619:  65%|████████▍    | 95/146 [00:58<00:31,  1.62it/s]Epoch: 8, train for the 100-th batch, train loss: 0.17015226185321808:  83%|█████████▏ | 99/119 [01:01<00:12,  1.63it/s]Epoch: 8, train for the 100-th batch, train loss: 0.17015226185321808:  84%|████████▍ | 100/119 [01:01<00:11,  1.63it/s]Epoch: 14, train for the 35-th batch, train loss: 0.43154242634773254:  23%|██▍        | 34/151 [00:07<00:26,  4.46it/s]Epoch: 14, train for the 35-th batch, train loss: 0.43154242634773254:  23%|██▌        | 35/151 [00:07<00:25,  4.54it/s]Epoch: 3, train for the 199-th batch, train loss: 0.37386465072631836:  52%|█████▏    | 198/383 [01:58<01:49,  1.70it/s]Epoch: 3, train for the 199-th batch, train loss: 0.37386465072631836:  52%|█████▏    | 199/383 [01:58<01:47,  1.71it/s]Epoch: 14, train for the 36-th batch, train loss: 0.681576669216156:  23%|███          | 35/151 [00:07<00:25,  4.54it/s]Epoch: 14, train for the 36-th batch, train loss: 0.681576669216156:  24%|███          | 36/151 [00:07<00:25,  4.52it/s]Epoch: 5, train for the 12-th batch, train loss: 0.5571049451828003:   5%|▌            | 11/237 [00:05<01:45,  2.14it/s]Epoch: 5, train for the 12-th batch, train loss: 0.5571049451828003:   5%|▋            | 12/237 [00:05<01:48,  2.07it/s]Epoch: 7, train for the 96-th batch, train loss: 0.557953417301178:  65%|█████████     | 95/146 [00:59<00:31,  1.62it/s]Epoch: 7, train for the 96-th batch, train loss: 0.557953417301178:  66%|█████████▏    | 96/146 [00:59<00:30,  1.63it/s]Epoch: 8, train for the 101-th batch, train loss: 0.15912772715091705:  84%|████████▍ | 100/119 [01:01<00:11,  1.63it/s]Epoch: 8, train for the 101-th batch, train loss: 0.15912772715091705:  85%|████████▍ | 101/119 [01:01<00:11,  1.63it/s]Epoch: 14, train for the 37-th batch, train loss: 0.45634573698043823:  24%|██▌        | 36/151 [00:07<00:25,  4.52it/s]Epoch: 14, train for the 37-th batch, train loss: 0.45634573698043823:  25%|██▋        | 37/151 [00:07<00:32,  3.47it/s]Epoch: 3, train for the 200-th batch, train loss: 0.40827372670173645:  52%|█████▏    | 199/383 [01:59<01:47,  1.71it/s]Epoch: 3, train for the 200-th batch, train loss: 0.40827372670173645:  52%|█████▏    | 200/383 [01:59<01:47,  1.70it/s]Epoch: 5, train for the 13-th batch, train loss: 0.5661531686782837:   5%|▋            | 12/237 [00:06<01:48,  2.07it/s]Epoch: 5, train for the 13-th batch, train loss: 0.5661531686782837:   5%|▋            | 13/237 [00:06<01:48,  2.07it/s]Epoch: 14, train for the 38-th batch, train loss: 0.5630473494529724:  25%|██▉         | 37/151 [00:08<00:32,  3.47it/s]Epoch: 14, train for the 38-th batch, train loss: 0.5630473494529724:  25%|███         | 38/151 [00:08<00:30,  3.75it/s]Epoch: 7, train for the 97-th batch, train loss: 0.5020623803138733:  66%|████████▌    | 96/146 [00:59<00:30,  1.63it/s]Epoch: 7, train for the 97-th batch, train loss: 0.5020623803138733:  66%|████████▋    | 97/146 [00:59<00:30,  1.63it/s]Epoch: 14, train for the 39-th batch, train loss: 0.6606023907661438:  25%|███         | 38/151 [00:08<00:30,  3.75it/s]Epoch: 14, train for the 39-th batch, train loss: 0.6606023907661438:  26%|███         | 39/151 [00:08<00:28,  3.96it/s]Epoch: 8, train for the 102-th batch, train loss: 0.19010299444198608:  85%|████████▍ | 101/119 [01:02<00:11,  1.63it/s]Epoch: 8, train for the 102-th batch, train loss: 0.19010299444198608:  86%|████████▌ | 102/119 [01:02<00:10,  1.63it/s]Epoch: 5, train for the 14-th batch, train loss: 0.48102515935897827:   5%|▋           | 13/237 [00:06<01:48,  2.07it/s]Epoch: 5, train for the 14-th batch, train loss: 0.48102515935897827:   6%|▋           | 14/237 [00:06<01:51,  2.00it/s]Epoch: 14, train for the 40-th batch, train loss: 0.5841309428215027:  26%|███         | 39/151 [00:08<00:28,  3.96it/s]Epoch: 14, train for the 40-th batch, train loss: 0.5841309428215027:  26%|███▏        | 40/151 [00:08<00:26,  4.12it/s]Epoch: 3, train for the 201-th batch, train loss: 0.47296473383903503:  52%|█████▏    | 200/383 [02:00<01:47,  1.70it/s]Epoch: 3, train for the 201-th batch, train loss: 0.47296473383903503:  52%|█████▏    | 201/383 [02:00<01:48,  1.67it/s]Epoch: 14, train for the 41-th batch, train loss: 0.6567603945732117:  26%|███▏        | 40/151 [00:08<00:26,  4.12it/s]Epoch: 14, train for the 41-th batch, train loss: 0.6567603945732117:  27%|███▎        | 41/151 [00:08<00:26,  4.22it/s]Epoch: 7, train for the 98-th batch, train loss: 0.5324472188949585:  66%|████████▋    | 97/146 [01:00<00:30,  1.63it/s]Epoch: 7, train for the 98-th batch, train loss: 0.5324472188949585:  67%|████████▋    | 98/146 [01:00<00:29,  1.62it/s]Epoch: 5, train for the 15-th batch, train loss: 0.5483587384223938:   6%|▊            | 14/237 [00:07<01:51,  2.00it/s]Epoch: 5, train for the 15-th batch, train loss: 0.5483587384223938:   6%|▊            | 15/237 [00:07<01:48,  2.04it/s]Epoch: 14, train for the 42-th batch, train loss: 0.6176099181175232:  27%|███▎        | 41/151 [00:09<00:26,  4.22it/s]Epoch: 14, train for the 42-th batch, train loss: 0.6176099181175232:  28%|███▎        | 42/151 [00:09<00:25,  4.29it/s]Epoch: 8, train for the 103-th batch, train loss: 0.17417314648628235:  86%|████████▌ | 102/119 [01:02<00:10,  1.63it/s]Epoch: 8, train for the 103-th batch, train loss: 0.17417314648628235:  87%|████████▋ | 103/119 [01:02<00:09,  1.62it/s]Epoch: 3, train for the 202-th batch, train loss: 0.5299396514892578:  52%|█████▊     | 201/383 [02:00<01:48,  1.67it/s]Epoch: 3, train for the 202-th batch, train loss: 0.5299396514892578:  53%|█████▊     | 202/383 [02:00<01:47,  1.69it/s]Epoch: 14, train for the 43-th batch, train loss: 0.5695511698722839:  28%|███▎        | 42/151 [00:09<00:25,  4.29it/s]Epoch: 14, train for the 43-th batch, train loss: 0.5695511698722839:  28%|███▍        | 43/151 [00:09<00:24,  4.37it/s]Epoch: 14, train for the 44-th batch, train loss: 0.567709743976593:  28%|███▋         | 43/151 [00:09<00:24,  4.37it/s]Epoch: 14, train for the 44-th batch, train loss: 0.567709743976593:  29%|███▊         | 44/151 [00:09<00:24,  4.41it/s]Epoch: 7, train for the 99-th batch, train loss: 0.5466991066932678:  67%|████████▋    | 98/146 [01:00<00:29,  1.62it/s]Epoch: 7, train for the 99-th batch, train loss: 0.5466991066932678:  68%|████████▊    | 99/146 [01:00<00:29,  1.62it/s]Epoch: 5, train for the 16-th batch, train loss: 0.5000300407409668:   6%|▊            | 15/237 [00:07<01:48,  2.04it/s]Epoch: 5, train for the 16-th batch, train loss: 0.5000300407409668:   7%|▉            | 16/237 [00:07<01:49,  2.01it/s]Epoch: 8, train for the 104-th batch, train loss: 0.20633544027805328:  87%|████████▋ | 103/119 [01:03<00:09,  1.62it/s]Epoch: 8, train for the 104-th batch, train loss: 0.20633544027805328:  87%|████████▋ | 104/119 [01:03<00:09,  1.62it/s]Epoch: 14, train for the 45-th batch, train loss: 0.5823174715042114:  29%|███▍        | 44/151 [00:09<00:24,  4.41it/s]Epoch: 14, train for the 45-th batch, train loss: 0.5823174715042114:  30%|███▌        | 45/151 [00:09<00:24,  4.42it/s]Epoch: 3, train for the 203-th batch, train loss: 0.4217405617237091:  53%|█████▊     | 202/383 [02:01<01:47,  1.69it/s]Epoch: 3, train for the 203-th batch, train loss: 0.4217405617237091:  53%|█████▊     | 203/383 [02:01<01:47,  1.68it/s]Epoch: 14, train for the 46-th batch, train loss: 0.5921086072921753:  30%|███▌        | 45/151 [00:09<00:24,  4.42it/s]Epoch: 14, train for the 46-th batch, train loss: 0.5921086072921753:  30%|███▋        | 46/151 [00:09<00:23,  4.45it/s]Epoch: 5, train for the 17-th batch, train loss: 0.6133224964141846:   7%|▉            | 16/237 [00:08<01:49,  2.01it/s]Epoch: 5, train for the 17-th batch, train loss: 0.6133224964141846:   7%|▉            | 17/237 [00:08<01:52,  1.96it/s]Epoch: 14, train for the 47-th batch, train loss: 0.6210293173789978:  30%|███▋        | 46/151 [00:10<00:23,  4.45it/s]Epoch: 14, train for the 47-th batch, train loss: 0.6210293173789978:  31%|███▋        | 47/151 [00:10<00:23,  4.46it/s]Epoch: 7, train for the 100-th batch, train loss: 0.585536003112793:  68%|████████▊    | 99/146 [01:01<00:29,  1.62it/s]Epoch: 7, train for the 100-th batch, train loss: 0.585536003112793:  68%|████████▏   | 100/146 [01:01<00:28,  1.62it/s]Epoch: 8, train for the 105-th batch, train loss: 0.17289823293685913:  87%|████████▋ | 104/119 [01:04<00:09,  1.62it/s]Epoch: 8, train for the 105-th batch, train loss: 0.17289823293685913:  88%|████████▊ | 105/119 [01:04<00:08,  1.62it/s]Epoch: 14, train for the 48-th batch, train loss: 0.494252473115921:  31%|████         | 47/151 [00:10<00:23,  4.46it/s]Epoch: 14, train for the 48-th batch, train loss: 0.494252473115921:  32%|████▏        | 48/151 [00:10<00:22,  4.53it/s]Epoch: 3, train for the 204-th batch, train loss: 0.3725309669971466:  53%|█████▊     | 203/383 [02:01<01:47,  1.68it/s]Epoch: 3, train for the 204-th batch, train loss: 0.3725309669971466:  53%|█████▊     | 204/383 [02:01<01:45,  1.69it/s]Epoch: 14, train for the 49-th batch, train loss: 0.578971803188324:  32%|████▏        | 48/151 [00:10<00:22,  4.53it/s]Epoch: 14, train for the 49-th batch, train loss: 0.578971803188324:  32%|████▏        | 49/151 [00:10<00:22,  4.54it/s]Epoch: 5, train for the 18-th batch, train loss: 0.5128533244132996:   7%|▉            | 17/237 [00:08<01:52,  1.96it/s]Epoch: 5, train for the 18-th batch, train loss: 0.5128533244132996:   8%|▉            | 18/237 [00:08<01:54,  1.92it/s]Epoch: 7, train for the 101-th batch, train loss: 0.51717209815979:  68%|████████▉    | 100/146 [01:02<00:28,  1.62it/s]Epoch: 7, train for the 101-th batch, train loss: 0.51717209815979:  69%|████████▉    | 101/146 [01:02<00:27,  1.62it/s]Epoch: 14, train for the 50-th batch, train loss: 0.595064640045166:  32%|████▏        | 49/151 [00:10<00:22,  4.54it/s]Epoch: 14, train for the 50-th batch, train loss: 0.595064640045166:  33%|████▎        | 50/151 [00:10<00:22,  4.53it/s]Epoch: 8, train for the 106-th batch, train loss: 0.1747313290834427:  88%|█████████▋ | 105/119 [01:04<00:08,  1.62it/s]Epoch: 8, train for the 106-th batch, train loss: 0.1747313290834427:  89%|█████████▊ | 106/119 [01:04<00:08,  1.62it/s]Epoch: 3, train for the 205-th batch, train loss: 0.377686083316803:  53%|██████▍     | 204/383 [02:02<01:45,  1.69it/s]Epoch: 3, train for the 205-th batch, train loss: 0.377686083316803:  54%|██████▍     | 205/383 [02:02<01:43,  1.72it/s]Epoch: 14, train for the 51-th batch, train loss: 0.6159719228744507:  33%|███▉        | 50/151 [00:11<00:22,  4.53it/s]Epoch: 14, train for the 51-th batch, train loss: 0.6159719228744507:  34%|████        | 51/151 [00:11<00:22,  4.52it/s]Epoch: 5, train for the 19-th batch, train loss: 0.4162426292896271:   8%|▉            | 18/237 [00:09<01:54,  1.92it/s]Epoch: 5, train for the 19-th batch, train loss: 0.4162426292896271:   8%|█            | 19/237 [00:09<01:54,  1.90it/s]Epoch: 14, train for the 52-th batch, train loss: 0.6150961518287659:  34%|████        | 51/151 [00:11<00:22,  4.52it/s]Epoch: 14, train for the 52-th batch, train loss: 0.6150961518287659:  34%|████▏       | 52/151 [00:11<00:21,  4.53it/s]Epoch: 7, train for the 102-th batch, train loss: 0.5482811331748962:  69%|███████▌   | 101/146 [01:02<00:27,  1.62it/s]Epoch: 7, train for the 102-th batch, train loss: 0.5482811331748962:  70%|███████▋   | 102/146 [01:02<00:27,  1.62it/s]Epoch: 14, train for the 53-th batch, train loss: 0.6238709092140198:  34%|████▏       | 52/151 [00:11<00:21,  4.53it/s]Epoch: 14, train for the 53-th batch, train loss: 0.6238709092140198:  35%|████▏       | 53/151 [00:11<00:21,  4.51it/s]Epoch: 3, train for the 206-th batch, train loss: 0.3876761496067047:  54%|█████▉     | 205/383 [02:02<01:43,  1.72it/s]Epoch: 3, train for the 206-th batch, train loss: 0.3876761496067047:  54%|█████▉     | 206/383 [02:02<01:42,  1.72it/s]Epoch: 8, train for the 107-th batch, train loss: 0.20146092772483826:  89%|████████▉ | 106/119 [01:05<00:08,  1.62it/s]Epoch: 8, train for the 107-th batch, train loss: 0.20146092772483826:  90%|████████▉ | 107/119 [01:05<00:07,  1.62it/s]Epoch: 14, train for the 54-th batch, train loss: 0.5799073576927185:  35%|████▏       | 53/151 [00:11<00:21,  4.51it/s]Epoch: 14, train for the 54-th batch, train loss: 0.5799073576927185:  36%|████▎       | 54/151 [00:11<00:21,  4.50it/s]Epoch: 5, train for the 20-th batch, train loss: 0.6705717444419861:   8%|█            | 19/237 [00:09<01:54,  1.90it/s]Epoch: 5, train for the 20-th batch, train loss: 0.6705717444419861:   8%|█            | 20/237 [00:09<01:53,  1.91it/s]Epoch: 14, train for the 55-th batch, train loss: 0.5884518623352051:  36%|████▎       | 54/151 [00:11<00:21,  4.50it/s]Epoch: 14, train for the 55-th batch, train loss: 0.5884518623352051:  36%|████▎       | 55/151 [00:11<00:21,  4.49it/s]Epoch: 7, train for the 103-th batch, train loss: 0.5711429119110107:  70%|███████▋   | 102/146 [01:03<00:27,  1.62it/s]Epoch: 7, train for the 103-th batch, train loss: 0.5711429119110107:  71%|███████▊   | 103/146 [01:03<00:26,  1.61it/s]Epoch: 14, train for the 56-th batch, train loss: 0.5508073568344116:  36%|████▎       | 55/151 [00:12<00:21,  4.49it/s]Epoch: 14, train for the 56-th batch, train loss: 0.5508073568344116:  37%|████▍       | 56/151 [00:12<00:21,  4.49it/s]Epoch: 3, train for the 207-th batch, train loss: 0.41729921102523804:  54%|█████▍    | 206/383 [02:03<01:42,  1.72it/s]Epoch: 3, train for the 207-th batch, train loss: 0.41729921102523804:  54%|█████▍    | 207/383 [02:03<01:44,  1.68it/s]Epoch: 8, train for the 108-th batch, train loss: 0.12986145913600922:  90%|████████▉ | 107/119 [01:05<00:07,  1.62it/s]Epoch: 8, train for the 108-th batch, train loss: 0.12986145913600922:  91%|█████████ | 108/119 [01:05<00:06,  1.61it/s]Epoch: 5, train for the 21-th batch, train loss: 0.4441812038421631:   8%|█            | 20/237 [00:10<01:53,  1.91it/s]Epoch: 5, train for the 21-th batch, train loss: 0.4441812038421631:   9%|█▏           | 21/237 [00:10<01:56,  1.86it/s]Epoch: 14, train for the 57-th batch, train loss: 0.5417815446853638:  37%|████▍       | 56/151 [00:12<00:21,  4.49it/s]Epoch: 14, train for the 57-th batch, train loss: 0.5417815446853638:  38%|████▌       | 57/151 [00:12<00:20,  4.48it/s]Epoch: 14, train for the 58-th batch, train loss: 0.5402175784111023:  38%|████▌       | 57/151 [00:12<00:20,  4.48it/s]Epoch: 14, train for the 58-th batch, train loss: 0.5402175784111023:  38%|████▌       | 58/151 [00:12<00:20,  4.47it/s]Epoch: 7, train for the 104-th batch, train loss: 0.5582871437072754:  71%|███████▊   | 103/146 [01:03<00:26,  1.61it/s]Epoch: 7, train for the 104-th batch, train loss: 0.5582871437072754:  71%|███████▊   | 104/146 [01:03<00:25,  1.62it/s]Epoch: 3, train for the 208-th batch, train loss: 0.4495784640312195:  54%|█████▉     | 207/383 [02:04<01:44,  1.68it/s]Epoch: 3, train for the 208-th batch, train loss: 0.4495784640312195:  54%|█████▉     | 208/383 [02:04<01:43,  1.69it/s]Epoch: 8, train for the 109-th batch, train loss: 0.1994444876909256:  91%|█████████▉ | 108/119 [01:06<00:06,  1.61it/s]Epoch: 8, train for the 109-th batch, train loss: 0.1994444876909256:  92%|██████████ | 109/119 [01:06<00:06,  1.62it/s]Epoch: 14, train for the 59-th batch, train loss: 0.5575809478759766:  38%|████▌       | 58/151 [00:12<00:20,  4.47it/s]Epoch: 14, train for the 59-th batch, train loss: 0.5575809478759766:  39%|████▋       | 59/151 [00:12<00:20,  4.47it/s]Epoch: 5, train for the 22-th batch, train loss: 0.5386561155319214:   9%|█▏           | 21/237 [00:10<01:56,  1.86it/s]Epoch: 5, train for the 22-th batch, train loss: 0.5386561155319214:   9%|█▏           | 22/237 [00:10<01:57,  1.84it/s]Epoch: 14, train for the 60-th batch, train loss: 0.5383865833282471:  39%|████▋       | 59/151 [00:13<00:20,  4.47it/s]Epoch: 14, train for the 60-th batch, train loss: 0.5383865833282471:  40%|████▊       | 60/151 [00:13<00:20,  4.48it/s]Epoch: 14, train for the 61-th batch, train loss: 0.5371853113174438:  40%|████▊       | 60/151 [00:13<00:20,  4.48it/s]Epoch: 14, train for the 61-th batch, train loss: 0.5371853113174438:  40%|████▊       | 61/151 [00:13<00:20,  4.48it/s]Epoch: 7, train for the 105-th batch, train loss: 0.5483998656272888:  71%|███████▊   | 104/146 [01:04<00:25,  1.62it/s]Epoch: 7, train for the 105-th batch, train loss: 0.5483998656272888:  72%|███████▉   | 105/146 [01:04<00:25,  1.62it/s]Epoch: 3, train for the 209-th batch, train loss: 0.4252142310142517:  54%|█████▉     | 208/383 [02:04<01:43,  1.69it/s]Epoch: 3, train for the 209-th batch, train loss: 0.4252142310142517:  55%|██████     | 209/383 [02:04<01:43,  1.68it/s]Epoch: 5, train for the 23-th batch, train loss: 0.4671339988708496:   9%|█▏           | 22/237 [00:11<01:57,  1.84it/s]Epoch: 5, train for the 23-th batch, train loss: 0.4671339988708496:  10%|█▎           | 23/237 [00:11<01:56,  1.84it/s]Epoch: 8, train for the 110-th batch, train loss: 0.16939015686511993:  92%|█████████▏| 109/119 [01:07<00:06,  1.62it/s]Epoch: 8, train for the 110-th batch, train loss: 0.16939015686511993:  92%|█████████▏| 110/119 [01:07<00:05,  1.62it/s]Epoch: 14, train for the 62-th batch, train loss: 0.505454957485199:  40%|█████▎       | 61/151 [00:13<00:20,  4.48it/s]Epoch: 14, train for the 62-th batch, train loss: 0.505454957485199:  41%|█████▎       | 62/151 [00:13<00:19,  4.54it/s]Epoch: 14, train for the 63-th batch, train loss: 0.5461180806159973:  41%|████▉       | 62/151 [00:13<00:19,  4.54it/s]Epoch: 14, train for the 63-th batch, train loss: 0.5461180806159973:  42%|█████       | 63/151 [00:13<00:19,  4.53it/s]Epoch: 7, train for the 106-th batch, train loss: 0.5363874435424805:  72%|███████▉   | 105/146 [01:05<00:25,  1.62it/s]Epoch: 7, train for the 106-th batch, train loss: 0.5363874435424805:  73%|███████▉   | 106/146 [01:05<00:24,  1.62it/s]Epoch: 14, train for the 64-th batch, train loss: 0.549860417842865:  42%|█████▍       | 63/151 [00:13<00:19,  4.53it/s]Epoch: 14, train for the 64-th batch, train loss: 0.549860417842865:  42%|█████▌       | 64/151 [00:13<00:19,  4.51it/s]Epoch: 5, train for the 24-th batch, train loss: 0.4692572057247162:  10%|█▎           | 23/237 [00:12<01:56,  1.84it/s]Epoch: 3, train for the 210-th batch, train loss: 0.45109647512435913:  55%|█████▍    | 209/383 [02:05<01:43,  1.68it/s]Epoch: 3, train for the 210-th batch, train loss: 0.45109647512435913:  55%|█████▍    | 210/383 [02:05<01:44,  1.66it/s]Epoch: 5, train for the 24-th batch, train loss: 0.4692572057247162:  10%|█▎           | 24/237 [00:12<01:59,  1.79it/s]Epoch: 8, train for the 111-th batch, train loss: 0.1763879805803299:  92%|██████████▏| 110/119 [01:07<00:05,  1.62it/s]Epoch: 8, train for the 111-th batch, train loss: 0.1763879805803299:  93%|██████████▎| 111/119 [01:07<00:04,  1.62it/s]Epoch: 14, train for the 65-th batch, train loss: 0.49226516485214233:  42%|████▋      | 64/151 [00:14<00:19,  4.51it/s]Epoch: 14, train for the 65-th batch, train loss: 0.49226516485214233:  43%|████▋      | 65/151 [00:14<00:19,  4.49it/s]Epoch: 14, train for the 66-th batch, train loss: 0.5258522629737854:  43%|█████▏      | 65/151 [00:14<00:19,  4.49it/s]Epoch: 14, train for the 66-th batch, train loss: 0.5258522629737854:  44%|█████▏      | 66/151 [00:14<00:18,  4.48it/s]Epoch: 7, train for the 107-th batch, train loss: 0.5816656351089478:  73%|███████▉   | 106/146 [01:05<00:24,  1.62it/s]Epoch: 7, train for the 107-th batch, train loss: 0.5816656351089478:  73%|████████   | 107/146 [01:05<00:24,  1.62it/s]Epoch: 5, train for the 25-th batch, train loss: 0.45695310831069946:  10%|█▏          | 24/237 [00:12<01:59,  1.79it/s]Epoch: 5, train for the 25-th batch, train loss: 0.45695310831069946:  11%|█▎          | 25/237 [00:12<02:02,  1.73it/s]Epoch: 14, train for the 67-th batch, train loss: 0.5289033055305481:  44%|█████▏      | 66/151 [00:14<00:18,  4.48it/s]Epoch: 14, train for the 67-th batch, train loss: 0.5289033055305481:  44%|█████▎      | 67/151 [00:14<00:18,  4.49it/s]Epoch: 3, train for the 211-th batch, train loss: 0.3813816010951996:  55%|██████     | 210/383 [02:06<01:44,  1.66it/s]Epoch: 3, train for the 211-th batch, train loss: 0.3813816010951996:  55%|██████     | 211/383 [02:06<01:45,  1.63it/s]Epoch: 8, train for the 112-th batch, train loss: 0.11258942633867264:  93%|█████████▎| 111/119 [01:08<00:04,  1.62it/s]Epoch: 8, train for the 112-th batch, train loss: 0.11258942633867264:  94%|█████████▍| 112/119 [01:08<00:04,  1.62it/s]Epoch: 14, train for the 68-th batch, train loss: 0.48253825306892395:  44%|████▉      | 67/151 [00:14<00:18,  4.49it/s]Epoch: 14, train for the 68-th batch, train loss: 0.48253825306892395:  45%|████▉      | 68/151 [00:14<00:18,  4.53it/s]Epoch: 14, train for the 69-th batch, train loss: 0.5488840937614441:  45%|█████▍      | 68/151 [00:15<00:18,  4.53it/s]Epoch: 14, train for the 69-th batch, train loss: 0.5488840937614441:  46%|█████▍      | 69/151 [00:15<00:18,  4.53it/s]Epoch: 7, train for the 108-th batch, train loss: 0.5327947735786438:  73%|████████   | 107/146 [01:06<00:24,  1.62it/s]Epoch: 7, train for the 108-th batch, train loss: 0.5327947735786438:  74%|████████▏  | 108/146 [01:06<00:23,  1.62it/s]Epoch: 5, train for the 26-th batch, train loss: 0.4593474566936493:  11%|█▎           | 25/237 [00:13<02:02,  1.73it/s]Epoch: 5, train for the 26-th batch, train loss: 0.4593474566936493:  11%|█▍           | 26/237 [00:13<02:04,  1.70it/s]Epoch: 3, train for the 212-th batch, train loss: 0.4559054970741272:  55%|██████     | 211/383 [02:06<01:45,  1.63it/s]Epoch: 3, train for the 212-th batch, train loss: 0.4559054970741272:  55%|██████     | 212/383 [02:06<01:45,  1.63it/s]Epoch: 14, train for the 70-th batch, train loss: 0.562282383441925:  46%|█████▉       | 69/151 [00:15<00:18,  4.53it/s]Epoch: 14, train for the 70-th batch, train loss: 0.562282383441925:  46%|██████       | 70/151 [00:15<00:17,  4.52it/s]Epoch: 8, train for the 113-th batch, train loss: 0.1779070943593979:  94%|██████████▎| 112/119 [01:09<00:04,  1.62it/s]Epoch: 8, train for the 113-th batch, train loss: 0.1779070943593979:  95%|██████████▍| 113/119 [01:09<00:03,  1.62it/s]Epoch: 14, train for the 71-th batch, train loss: 0.5838966965675354:  46%|█████▌      | 70/151 [00:15<00:17,  4.52it/s]Epoch: 14, train for the 71-th batch, train loss: 0.5838966965675354:  47%|█████▋      | 71/151 [00:15<00:17,  4.50it/s]Epoch: 14, train for the 72-th batch, train loss: 0.5110120177268982:  47%|█████▋      | 71/151 [00:15<00:17,  4.50it/s]Epoch: 14, train for the 72-th batch, train loss: 0.5110120177268982:  48%|█████▋      | 72/151 [00:15<00:17,  4.50it/s]Epoch: 7, train for the 109-th batch, train loss: 0.5246880054473877:  74%|████████▏  | 108/146 [01:07<00:23,  1.62it/s]Epoch: 7, train for the 109-th batch, train loss: 0.5246880054473877:  75%|████████▏  | 109/146 [01:07<00:22,  1.62it/s]Epoch: 5, train for the 27-th batch, train loss: 0.4075956642627716:  11%|█▍           | 26/237 [00:13<02:04,  1.70it/s]Epoch: 5, train for the 27-th batch, train loss: 0.4075956642627716:  11%|█▍           | 27/237 [00:13<02:04,  1.69it/s]Epoch: 3, train for the 213-th batch, train loss: 0.5067672729492188:  55%|██████     | 212/383 [02:07<01:45,  1.63it/s]Epoch: 3, train for the 213-th batch, train loss: 0.5067672729492188:  56%|██████     | 213/383 [02:07<01:44,  1.63it/s]Epoch: 8, train for the 114-th batch, train loss: 0.208513081073761:  95%|███████████▍| 113/119 [01:09<00:03,  1.62it/s]Epoch: 8, train for the 114-th batch, train loss: 0.208513081073761:  96%|███████████▍| 114/119 [01:09<00:03,  1.63it/s]Epoch: 14, train for the 73-th batch, train loss: 0.5329341888427734:  48%|█████▋      | 72/151 [00:15<00:17,  4.50it/s]Epoch: 14, train for the 73-th batch, train loss: 0.5329341888427734:  48%|█████▊      | 73/151 [00:15<00:17,  4.50it/s]Epoch: 14, train for the 74-th batch, train loss: 0.5474877953529358:  48%|█████▊      | 73/151 [00:16<00:17,  4.50it/s]Epoch: 14, train for the 74-th batch, train loss: 0.5474877953529358:  49%|█████▉      | 74/151 [00:16<00:17,  4.50it/s]Epoch: 7, train for the 110-th batch, train loss: 0.5112295150756836:  75%|████████▏  | 109/146 [01:07<00:22,  1.62it/s]Epoch: 7, train for the 110-th batch, train loss: 0.5112295150756836:  75%|████████▎  | 110/146 [01:07<00:22,  1.62it/s]Epoch: 14, train for the 75-th batch, train loss: 0.5199050307273865:  49%|█████▉      | 74/151 [00:16<00:17,  4.50it/s]Epoch: 14, train for the 75-th batch, train loss: 0.5199050307273865:  50%|█████▉      | 75/151 [00:16<00:16,  4.48it/s]Epoch: 5, train for the 28-th batch, train loss: 0.5760214328765869:  11%|█▍           | 27/237 [00:14<02:04,  1.69it/s]Epoch: 5, train for the 28-th batch, train loss: 0.5760214328765869:  12%|█▌           | 28/237 [00:14<02:03,  1.70it/s]Epoch: 3, train for the 214-th batch, train loss: 0.40978458523750305:  56%|█████▌    | 213/383 [02:07<01:44,  1.63it/s]Epoch: 3, train for the 214-th batch, train loss: 0.40978458523750305:  56%|█████▌    | 214/383 [02:07<01:43,  1.64it/s]Epoch: 8, train for the 115-th batch, train loss: 0.16690826416015625:  96%|█████████▌| 114/119 [01:10<00:03,  1.63it/s]Epoch: 8, train for the 115-th batch, train loss: 0.16690826416015625:  97%|█████████▋| 115/119 [01:10<00:02,  1.63it/s]Epoch: 14, train for the 76-th batch, train loss: 0.5518420934677124:  50%|█████▉      | 75/151 [00:16<00:16,  4.48it/s]Epoch: 14, train for the 76-th batch, train loss: 0.5518420934677124:  50%|██████      | 76/151 [00:16<00:16,  4.49it/s]Epoch: 14, train for the 77-th batch, train loss: 0.5450368523597717:  50%|██████      | 76/151 [00:16<00:16,  4.49it/s]Epoch: 14, train for the 77-th batch, train loss: 0.5450368523597717:  51%|██████      | 77/151 [00:16<00:16,  4.47it/s]Epoch: 7, train for the 111-th batch, train loss: 0.5136789679527283:  75%|████████▎  | 110/146 [01:08<00:22,  1.62it/s]Epoch: 7, train for the 111-th batch, train loss: 0.5136789679527283:  76%|████████▎  | 111/146 [01:08<00:21,  1.63it/s]Epoch: 5, train for the 29-th batch, train loss: 0.5194476246833801:  12%|█▌           | 28/237 [00:15<02:03,  1.70it/s]Epoch: 5, train for the 29-th batch, train loss: 0.5194476246833801:  12%|█▌           | 29/237 [00:15<02:02,  1.70it/s]Epoch: 3, train for the 215-th batch, train loss: 0.36428317427635193:  56%|█████▌    | 214/383 [02:08<01:43,  1.64it/s]Epoch: 3, train for the 215-th batch, train loss: 0.36428317427635193:  56%|█████▌    | 215/383 [02:08<01:41,  1.66it/s]Epoch: 14, train for the 78-th batch, train loss: 0.49497419595718384:  51%|█████▌     | 77/151 [00:17<00:16,  4.47it/s]Epoch: 14, train for the 78-th batch, train loss: 0.49497419595718384:  52%|█████▋     | 78/151 [00:17<00:16,  4.46it/s]Epoch: 8, train for the 116-th batch, train loss: 0.11514964699745178:  97%|█████████▋| 115/119 [01:10<00:02,  1.63it/s]Epoch: 8, train for the 116-th batch, train loss: 0.11514964699745178:  97%|█████████▋| 116/119 [01:10<00:01,  1.63it/s]Epoch: 14, train for the 79-th batch, train loss: 0.5735751390457153:  52%|██████▏     | 78/151 [00:17<00:16,  4.46it/s]Epoch: 14, train for the 79-th batch, train loss: 0.5735751390457153:  52%|██████▎     | 79/151 [00:17<00:16,  4.48it/s]Epoch: 5, train for the 30-th batch, train loss: 0.683407723903656:  12%|█▋            | 29/237 [00:15<02:02,  1.70it/s]Epoch: 5, train for the 30-th batch, train loss: 0.683407723903656:  13%|█▊            | 30/237 [00:15<01:56,  1.78it/s]Epoch: 14, train for the 80-th batch, train loss: 0.5718254446983337:  52%|██████▎     | 79/151 [00:17<00:16,  4.48it/s]Epoch: 14, train for the 80-th batch, train loss: 0.5718254446983337:  53%|██████▎     | 80/151 [00:17<00:15,  4.48it/s]Epoch: 7, train for the 112-th batch, train loss: 0.5786495804786682:  76%|████████▎  | 111/146 [01:08<00:21,  1.63it/s]Epoch: 7, train for the 112-th batch, train loss: 0.5786495804786682:  77%|████████▍  | 112/146 [01:08<00:20,  1.63it/s]Epoch: 3, train for the 216-th batch, train loss: 0.39653345942497253:  56%|█████▌    | 215/383 [02:09<01:41,  1.66it/s]Epoch: 3, train for the 216-th batch, train loss: 0.39653345942497253:  56%|█████▋    | 216/383 [02:09<01:40,  1.67it/s]Epoch: 14, train for the 81-th batch, train loss: 0.5734254717826843:  53%|██████▎     | 80/151 [00:17<00:15,  4.48it/s]Epoch: 14, train for the 81-th batch, train loss: 0.5734254717826843:  54%|██████▍     | 81/151 [00:17<00:15,  4.47it/s]Epoch: 8, train for the 117-th batch, train loss: 0.18176791071891785:  97%|█████████▋| 116/119 [01:11<00:01,  1.63it/s]Epoch: 8, train for the 117-th batch, train loss: 0.18176791071891785:  98%|█████████▊| 117/119 [01:11<00:01,  1.63it/s]Epoch: 14, train for the 82-th batch, train loss: 0.5892446041107178:  54%|██████▍     | 81/151 [00:17<00:15,  4.47it/s]Epoch: 14, train for the 82-th batch, train loss: 0.5892446041107178:  54%|██████▌     | 82/151 [00:17<00:15,  4.48it/s]Epoch: 5, train for the 31-th batch, train loss: 0.29701361060142517:  13%|█▌          | 30/237 [00:16<01:56,  1.78it/s]Epoch: 5, train for the 31-th batch, train loss: 0.29701361060142517:  13%|█▌          | 31/237 [00:16<01:54,  1.80it/s]Epoch: 14, train for the 83-th batch, train loss: 0.5504692196846008:  54%|██████▌     | 82/151 [00:18<00:15,  4.48it/s]Epoch: 14, train for the 83-th batch, train loss: 0.5504692196846008:  55%|██████▌     | 83/151 [00:18<00:15,  4.49it/s]Epoch: 3, train for the 217-th batch, train loss: 0.429385781288147:  56%|██████▊     | 216/383 [02:09<01:40,  1.67it/s]Epoch: 3, train for the 217-th batch, train loss: 0.429385781288147:  57%|██████▊     | 217/383 [02:09<01:37,  1.69it/s]Epoch: 7, train for the 113-th batch, train loss: 0.5086034536361694:  77%|████████▍  | 112/146 [01:09<00:20,  1.63it/s]Epoch: 7, train for the 113-th batch, train loss: 0.5086034536361694:  77%|████████▌  | 113/146 [01:09<00:20,  1.62it/s]Epoch: 8, train for the 118-th batch, train loss: 0.1432751566171646:  98%|██████████▊| 117/119 [01:12<00:01,  1.63it/s]Epoch: 8, train for the 118-th batch, train loss: 0.1432751566171646:  99%|██████████▉| 118/119 [01:12<00:00,  1.62it/s]Epoch: 14, train for the 84-th batch, train loss: 0.587696373462677:  55%|███████▏     | 83/151 [00:18<00:15,  4.49it/s]Epoch: 14, train for the 84-th batch, train loss: 0.587696373462677:  56%|███████▏     | 84/151 [00:18<00:14,  4.48it/s]Epoch: 14, train for the 85-th batch, train loss: 0.5529614686965942:  56%|██████▋     | 84/151 [00:18<00:14,  4.48it/s]Epoch: 14, train for the 85-th batch, train loss: 0.5529614686965942:  56%|██████▊     | 85/151 [00:18<00:14,  4.47it/s]Epoch: 5, train for the 32-th batch, train loss: 0.45011430978775024:  13%|█▌          | 31/237 [00:16<01:54,  1.80it/s]Epoch: 5, train for the 32-th batch, train loss: 0.45011430978775024:  14%|█▌          | 32/237 [00:16<01:56,  1.76it/s]Epoch: 8, train for the 119-th batch, train loss: 0.15894053876399994:  99%|█████████▉| 118/119 [01:12<00:00,  1.62it/s]Epoch: 8, train for the 119-th batch, train loss: 0.15894053876399994: 100%|██████████| 119/119 [01:12<00:00,  1.87it/s]Epoch: 8, train for the 119-th batch, train loss: 0.15894053876399994: 100%|██████████| 119/119 [01:12<00:00,  1.64it/s]
  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 3, train for the 218-th batch, train loss: 0.42510855197906494:  57%|█████▋    | 217/383 [02:10<01:37,  1.69it/s]Epoch: 3, train for the 218-th batch, train loss: 0.42510855197906494:  57%|█████▋    | 218/383 [02:10<01:37,  1.70it/s]Epoch: 7, train for the 114-th batch, train loss: 0.5409094095230103:  77%|████████▌  | 113/146 [01:10<00:20,  1.62it/s]Epoch: 7, train for the 114-th batch, train loss: 0.5409094095230103:  78%|████████▌  | 114/146 [01:10<00:19,  1.62it/s]Epoch: 14, train for the 86-th batch, train loss: 0.5463627576828003:  56%|██████▊     | 85/151 [00:18<00:14,  4.47it/s]Epoch: 14, train for the 86-th batch, train loss: 0.5463627576828003:  57%|██████▊     | 86/151 [00:18<00:14,  4.48it/s]evaluate for the 1-th batch, evaluate loss: 0.13753879070281982:   0%|                           | 0/40 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.13753879070281982:   2%|▍                  | 1/40 [00:00<00:10,  3.66it/s]Epoch: 14, train for the 87-th batch, train loss: 0.521902322769165:  57%|███████▍     | 86/151 [00:19<00:14,  4.48it/s]Epoch: 14, train for the 87-th batch, train loss: 0.521902322769165:  58%|███████▍     | 87/151 [00:19<00:14,  4.48it/s]Epoch: 5, train for the 33-th batch, train loss: 0.4201318919658661:  14%|█▊           | 32/237 [00:17<01:56,  1.76it/s]Epoch: 5, train for the 33-th batch, train loss: 0.4201318919658661:  14%|█▊           | 33/237 [00:17<01:57,  1.74it/s]evaluate for the 2-th batch, evaluate loss: 0.16512081027030945:   2%|▍                  | 1/40 [00:00<00:10,  3.66it/s]evaluate for the 2-th batch, evaluate loss: 0.16512081027030945:   5%|▉                  | 2/40 [00:00<00:10,  3.65it/s]Epoch: 14, train for the 88-th batch, train loss: 0.5863393545150757:  58%|██████▉     | 87/151 [00:19<00:14,  4.48it/s]Epoch: 14, train for the 88-th batch, train loss: 0.5863393545150757:  58%|██████▉     | 88/151 [00:19<00:14,  4.49it/s]Epoch: 3, train for the 219-th batch, train loss: 0.31262049078941345:  57%|█████▋    | 218/383 [02:10<01:37,  1.70it/s]Epoch: 3, train for the 219-th batch, train loss: 0.31262049078941345:  57%|█████▋    | 219/383 [02:10<01:37,  1.69it/s]Epoch: 7, train for the 115-th batch, train loss: 0.5454272031784058:  78%|████████▌  | 114/146 [01:10<00:19,  1.62it/s]Epoch: 7, train for the 115-th batch, train loss: 0.5454272031784058:  79%|████████▋  | 115/146 [01:10<00:19,  1.63it/s]Epoch: 14, train for the 89-th batch, train loss: 0.569208562374115:  58%|███████▌     | 88/151 [00:19<00:14,  4.49it/s]Epoch: 14, train for the 89-th batch, train loss: 0.569208562374115:  59%|███████▋     | 89/151 [00:19<00:13,  4.45it/s]evaluate for the 3-th batch, evaluate loss: 0.1976396143436432:   5%|█                   | 2/40 [00:00<00:10,  3.65it/s]evaluate for the 3-th batch, evaluate loss: 0.1976396143436432:   8%|█▌                  | 3/40 [00:00<00:10,  3.50it/s]Epoch: 14, train for the 90-th batch, train loss: 0.5462662577629089:  59%|███████     | 89/151 [00:19<00:13,  4.45it/s]Epoch: 14, train for the 90-th batch, train loss: 0.5462662577629089:  60%|███████▏    | 90/151 [00:19<00:13,  4.44it/s]evaluate for the 4-th batch, evaluate loss: 0.14115102589130402:   8%|█▍                 | 3/40 [00:01<00:10,  3.50it/s]evaluate for the 4-th batch, evaluate loss: 0.14115102589130402:  10%|█▉                 | 4/40 [00:01<00:09,  3.69it/s]Epoch: 5, train for the 34-th batch, train loss: 0.34960609674453735:  14%|█▋          | 33/237 [00:17<01:57,  1.74it/s]Epoch: 5, train for the 34-th batch, train loss: 0.34960609674453735:  14%|█▋          | 34/237 [00:17<01:57,  1.73it/s]Epoch: 14, train for the 91-th batch, train loss: 0.5000686645507812:  60%|███████▏    | 90/151 [00:20<00:13,  4.44it/s]Epoch: 14, train for the 91-th batch, train loss: 0.5000686645507812:  60%|███████▏    | 91/151 [00:20<00:13,  4.43it/s]Epoch: 3, train for the 220-th batch, train loss: 0.34515535831451416:  57%|█████▋    | 219/383 [02:11<01:37,  1.69it/s]Epoch: 3, train for the 220-th batch, train loss: 0.34515535831451416:  57%|█████▋    | 220/383 [02:11<01:36,  1.69it/s]Epoch: 7, train for the 116-th batch, train loss: 0.5367597937583923:  79%|████████▋  | 115/146 [01:11<00:19,  1.63it/s]Epoch: 7, train for the 116-th batch, train loss: 0.5367597937583923:  79%|████████▋  | 116/146 [01:11<00:18,  1.64it/s]evaluate for the 5-th batch, evaluate loss: 0.17280662059783936:  10%|█▉                 | 4/40 [00:01<00:09,  3.69it/s]evaluate for the 5-th batch, evaluate loss: 0.17280662059783936:  12%|██▍                | 5/40 [00:01<00:09,  3.54it/s]Epoch: 14, train for the 92-th batch, train loss: 0.5400483012199402:  60%|███████▏    | 91/151 [00:20<00:13,  4.43it/s]Epoch: 14, train for the 92-th batch, train loss: 0.5400483012199402:  61%|███████▎    | 92/151 [00:20<00:13,  4.41it/s]evaluate for the 6-th batch, evaluate loss: 0.17702704668045044:  12%|██▍                | 5/40 [00:01<00:09,  3.54it/s]evaluate for the 6-th batch, evaluate loss: 0.17702704668045044:  15%|██▊                | 6/40 [00:01<00:09,  3.73it/s]Epoch: 5, train for the 35-th batch, train loss: 0.3633778393268585:  14%|█▊           | 34/237 [00:18<01:57,  1.73it/s]Epoch: 5, train for the 35-th batch, train loss: 0.3633778393268585:  15%|█▉           | 35/237 [00:18<01:57,  1.72it/s]Epoch: 14, train for the 93-th batch, train loss: 0.5050587058067322:  61%|███████▎    | 92/151 [00:20<00:13,  4.41it/s]Epoch: 14, train for the 93-th batch, train loss: 0.5050587058067322:  62%|███████▍    | 93/151 [00:20<00:13,  4.43it/s]Epoch: 3, train for the 221-th batch, train loss: 0.4455903172492981:  57%|██████▎    | 220/383 [02:11<01:36,  1.69it/s]Epoch: 3, train for the 221-th batch, train loss: 0.4455903172492981:  58%|██████▎    | 221/383 [02:11<01:35,  1.69it/s]Epoch: 7, train for the 117-th batch, train loss: 0.5946391820907593:  79%|████████▋  | 116/146 [01:11<00:18,  1.64it/s]Epoch: 7, train for the 117-th batch, train loss: 0.5946391820907593:  80%|████████▊  | 117/146 [01:11<00:17,  1.64it/s]Epoch: 14, train for the 94-th batch, train loss: 0.5264277458190918:  62%|███████▍    | 93/151 [00:20<00:13,  4.43it/s]Epoch: 14, train for the 94-th batch, train loss: 0.5264277458190918:  62%|███████▍    | 94/151 [00:20<00:12,  4.44it/s]evaluate for the 7-th batch, evaluate loss: 0.11525201797485352:  15%|██▊                | 6/40 [00:01<00:09,  3.73it/s]evaluate for the 7-th batch, evaluate loss: 0.11525201797485352:  18%|███▎               | 7/40 [00:01<00:09,  3.53it/s]Epoch: 14, train for the 95-th batch, train loss: 0.5080305337905884:  62%|███████▍    | 94/151 [00:20<00:12,  4.44it/s]Epoch: 14, train for the 95-th batch, train loss: 0.5080305337905884:  63%|███████▌    | 95/151 [00:20<00:12,  4.44it/s]evaluate for the 8-th batch, evaluate loss: 0.13046495616436005:  18%|███▎               | 7/40 [00:02<00:09,  3.53it/s]evaluate for the 8-th batch, evaluate loss: 0.13046495616436005:  20%|███▊               | 8/40 [00:02<00:08,  3.60it/s]Epoch: 5, train for the 36-th batch, train loss: 0.3305003345012665:  15%|█▉           | 35/237 [00:19<01:57,  1.72it/s]Epoch: 5, train for the 36-th batch, train loss: 0.3305003345012665:  15%|█▉           | 36/237 [00:19<01:57,  1.72it/s]Epoch: 14, train for the 96-th batch, train loss: 0.5545152425765991:  63%|███████▌    | 95/151 [00:21<00:12,  4.44it/s]Epoch: 14, train for the 96-th batch, train loss: 0.5545152425765991:  64%|███████▋    | 96/151 [00:21<00:12,  4.45it/s]Epoch: 3, train for the 222-th batch, train loss: 0.46357402205467224:  58%|█████▊    | 221/383 [02:12<01:35,  1.69it/s]Epoch: 3, train for the 222-th batch, train loss: 0.46357402205467224:  58%|█████▊    | 222/383 [02:12<01:35,  1.69it/s]evaluate for the 9-th batch, evaluate loss: 0.17182666063308716:  20%|███▊               | 8/40 [00:02<00:08,  3.60it/s]evaluate for the 9-th batch, evaluate loss: 0.17182666063308716:  22%|████▎              | 9/40 [00:02<00:08,  3.51it/s]Epoch: 7, train for the 118-th batch, train loss: 0.5011485815048218:  80%|████████▊  | 117/146 [01:12<00:17,  1.64it/s]Epoch: 7, train for the 118-th batch, train loss: 0.5011485815048218:  81%|████████▉  | 118/146 [01:12<00:17,  1.65it/s]Epoch: 14, train for the 97-th batch, train loss: 0.5952937602996826:  64%|███████▋    | 96/151 [00:21<00:12,  4.45it/s]Epoch: 14, train for the 97-th batch, train loss: 0.5952937602996826:  64%|███████▋    | 97/151 [00:21<00:12,  4.46it/s]evaluate for the 10-th batch, evaluate loss: 0.19257193803787231:  22%|████              | 9/40 [00:02<00:08,  3.51it/s]evaluate for the 10-th batch, evaluate loss: 0.19257193803787231:  25%|████▎            | 10/40 [00:02<00:08,  3.64it/s]Epoch: 14, train for the 98-th batch, train loss: 0.6144087314605713:  64%|███████▋    | 97/151 [00:21<00:12,  4.46it/s]Epoch: 14, train for the 98-th batch, train loss: 0.6144087314605713:  65%|███████▊    | 98/151 [00:21<00:11,  4.46it/s]Epoch: 5, train for the 37-th batch, train loss: 0.5304383039474487:  15%|█▉           | 36/237 [00:19<01:57,  1.72it/s]Epoch: 5, train for the 37-th batch, train loss: 0.5304383039474487:  16%|██           | 37/237 [00:19<01:57,  1.70it/s]Epoch: 3, train for the 223-th batch, train loss: 0.35646894574165344:  58%|█████▊    | 222/383 [02:13<01:35,  1.69it/s]Epoch: 3, train for the 223-th batch, train loss: 0.35646894574165344:  58%|█████▊    | 223/383 [02:13<01:34,  1.69it/s]Epoch: 14, train for the 99-th batch, train loss: 0.6196078658103943:  65%|███████▊    | 98/151 [00:21<00:11,  4.46it/s]Epoch: 14, train for the 99-th batch, train loss: 0.6196078658103943:  66%|███████▊    | 99/151 [00:21<00:11,  4.48it/s]evaluate for the 11-th batch, evaluate loss: 0.16553302109241486:  25%|████▎            | 10/40 [00:03<00:08,  3.64it/s]evaluate for the 11-th batch, evaluate loss: 0.16553302109241486:  28%|████▋            | 11/40 [00:03<00:08,  3.56it/s]Epoch: 7, train for the 119-th batch, train loss: 0.5375949144363403:  81%|████████▉  | 118/146 [01:13<00:17,  1.65it/s]Epoch: 7, train for the 119-th batch, train loss: 0.5375949144363403:  82%|████████▉  | 119/146 [01:13<00:16,  1.66it/s]Epoch: 14, train for the 100-th batch, train loss: 0.6539595127105713:  66%|███████▏   | 99/151 [00:22<00:11,  4.48it/s]Epoch: 14, train for the 100-th batch, train loss: 0.6539595127105713:  66%|██████▌   | 100/151 [00:22<00:11,  4.47it/s]evaluate for the 12-th batch, evaluate loss: 0.14559242129325867:  28%|████▋            | 11/40 [00:03<00:08,  3.56it/s]evaluate for the 12-th batch, evaluate loss: 0.14559242129325867:  30%|█████            | 12/40 [00:03<00:07,  3.65it/s]Epoch: 5, train for the 38-th batch, train loss: 0.3081149160861969:  16%|██           | 37/237 [00:20<01:57,  1.70it/s]Epoch: 5, train for the 38-th batch, train loss: 0.3081149160861969:  16%|██           | 38/237 [00:20<01:57,  1.70it/s]Epoch: 14, train for the 101-th batch, train loss: 0.6573841571807861:  66%|██████▌   | 100/151 [00:22<00:11,  4.47it/s]Epoch: 14, train for the 101-th batch, train loss: 0.6573841571807861:  67%|██████▋   | 101/151 [00:22<00:11,  4.47it/s]evaluate for the 13-th batch, evaluate loss: 0.12623447179794312:  30%|█████            | 12/40 [00:03<00:07,  3.65it/s]evaluate for the 13-th batch, evaluate loss: 0.12623447179794312:  32%|█████▌           | 13/40 [00:03<00:07,  3.62it/s]Epoch: 3, train for the 224-th batch, train loss: 0.4842670261859894:  58%|██████▍    | 223/383 [02:13<01:34,  1.69it/s]Epoch: 3, train for the 224-th batch, train loss: 0.4842670261859894:  58%|██████▍    | 224/383 [02:13<01:34,  1.69it/s]Epoch: 14, train for the 102-th batch, train loss: 0.597804844379425:  67%|███████▎   | 101/151 [00:22<00:11,  4.47it/s]Epoch: 14, train for the 102-th batch, train loss: 0.597804844379425:  68%|███████▍   | 102/151 [00:22<00:10,  4.48it/s]Epoch: 7, train for the 120-th batch, train loss: 0.5172156691551208:  82%|████████▉  | 119/146 [01:13<00:16,  1.66it/s]Epoch: 7, train for the 120-th batch, train loss: 0.5172156691551208:  82%|█████████  | 120/146 [01:13<00:15,  1.65it/s]evaluate for the 14-th batch, evaluate loss: 0.1648176908493042:  32%|█████▊            | 13/40 [00:03<00:07,  3.62it/s]evaluate for the 14-th batch, evaluate loss: 0.1648176908493042:  35%|██████▎           | 14/40 [00:03<00:07,  3.56it/s]Epoch: 14, train for the 103-th batch, train loss: 0.6180371642112732:  68%|██████▊   | 102/151 [00:22<00:10,  4.48it/s]Epoch: 14, train for the 103-th batch, train loss: 0.6180371642112732:  68%|██████▊   | 103/151 [00:22<00:10,  4.43it/s]Epoch: 5, train for the 39-th batch, train loss: 0.5696766376495361:  16%|██           | 38/237 [00:20<01:57,  1.70it/s]Epoch: 5, train for the 39-th batch, train loss: 0.5696766376495361:  16%|██▏          | 39/237 [00:20<01:58,  1.67it/s]evaluate for the 15-th batch, evaluate loss: 0.18044663965702057:  35%|█████▉           | 14/40 [00:04<00:07,  3.56it/s]evaluate for the 15-th batch, evaluate loss: 0.18044663965702057:  38%|██████▍          | 15/40 [00:04<00:06,  3.59it/s]Epoch: 14, train for the 104-th batch, train loss: 0.6200546622276306:  68%|██████▊   | 103/151 [00:22<00:10,  4.43it/s]Epoch: 14, train for the 104-th batch, train loss: 0.6200546622276306:  69%|██████▉   | 104/151 [00:22<00:10,  4.42it/s]Epoch: 3, train for the 225-th batch, train loss: 0.42037275433540344:  58%|█████▊    | 224/383 [02:14<01:34,  1.69it/s]Epoch: 3, train for the 225-th batch, train loss: 0.42037275433540344:  59%|█████▊    | 225/383 [02:14<01:34,  1.67it/s]Epoch: 7, train for the 121-th batch, train loss: 0.5177146196365356:  82%|█████████  | 120/146 [01:14<00:15,  1.65it/s]Epoch: 7, train for the 121-th batch, train loss: 0.5177146196365356:  83%|█████████  | 121/146 [01:14<00:15,  1.65it/s]Epoch: 14, train for the 105-th batch, train loss: 0.5486214756965637:  69%|██████▉   | 104/151 [00:23<00:10,  4.42it/s]Epoch: 14, train for the 105-th batch, train loss: 0.5486214756965637:  70%|██████▉   | 105/151 [00:23<00:10,  4.42it/s]evaluate for the 16-th batch, evaluate loss: 0.19487254321575165:  38%|██████▍          | 15/40 [00:04<00:06,  3.59it/s]evaluate for the 16-th batch, evaluate loss: 0.19487254321575165:  40%|██████▊          | 16/40 [00:04<00:06,  3.51it/s]Epoch: 14, train for the 106-th batch, train loss: 0.5135700702667236:  70%|██████▉   | 105/151 [00:23<00:10,  4.42it/s]Epoch: 14, train for the 106-th batch, train loss: 0.5135700702667236:  70%|███████   | 106/151 [00:23<00:10,  4.40it/s]Epoch: 5, train for the 40-th batch, train loss: 0.5118479132652283:  16%|██▏          | 39/237 [00:21<01:58,  1.67it/s]Epoch: 5, train for the 40-th batch, train loss: 0.5118479132652283:  17%|██▏          | 40/237 [00:21<01:57,  1.68it/s]evaluate for the 17-th batch, evaluate loss: 0.12716235220432281:  40%|██████▊          | 16/40 [00:04<00:06,  3.51it/s]evaluate for the 17-th batch, evaluate loss: 0.12716235220432281:  42%|███████▏         | 17/40 [00:04<00:06,  3.67it/s]Epoch: 3, train for the 226-th batch, train loss: 0.47090670466423035:  59%|█████▊    | 225/383 [02:14<01:34,  1.67it/s]Epoch: 3, train for the 226-th batch, train loss: 0.47090670466423035:  59%|█████▉    | 226/383 [02:14<01:33,  1.67it/s]Epoch: 14, train for the 107-th batch, train loss: 0.5675665736198425:  70%|███████   | 106/151 [00:23<00:10,  4.40it/s]Epoch: 14, train for the 107-th batch, train loss: 0.5675665736198425:  71%|███████   | 107/151 [00:23<00:09,  4.43it/s]Epoch: 7, train for the 122-th batch, train loss: 0.5472398400306702:  83%|█████████  | 121/146 [01:14<00:15,  1.65it/s]Epoch: 7, train for the 122-th batch, train loss: 0.5472398400306702:  84%|█████████▏ | 122/146 [01:14<00:14,  1.65it/s]evaluate for the 18-th batch, evaluate loss: 0.14736893773078918:  42%|███████▏         | 17/40 [00:05<00:06,  3.67it/s]evaluate for the 18-th batch, evaluate loss: 0.14736893773078918:  45%|███████▋         | 18/40 [00:05<00:06,  3.55it/s]Epoch: 14, train for the 108-th batch, train loss: 0.5510253310203552:  71%|███████   | 107/151 [00:23<00:09,  4.43it/s]Epoch: 14, train for the 108-th batch, train loss: 0.5510253310203552:  72%|███████▏  | 108/151 [00:23<00:09,  4.46it/s]evaluate for the 19-th batch, evaluate loss: 0.14628468453884125:  45%|███████▋         | 18/40 [00:05<00:06,  3.55it/s]evaluate for the 19-th batch, evaluate loss: 0.14628468453884125:  48%|████████         | 19/40 [00:05<00:05,  3.69it/s]Epoch: 5, train for the 41-th batch, train loss: 0.36857879161834717:  17%|██          | 40/237 [00:22<01:57,  1.68it/s]Epoch: 5, train for the 41-th batch, train loss: 0.36857879161834717:  17%|██          | 41/237 [00:22<01:56,  1.68it/s]Epoch: 14, train for the 109-th batch, train loss: 0.5255858302116394:  72%|███████▏  | 108/151 [00:24<00:09,  4.46it/s]Epoch: 14, train for the 109-th batch, train loss: 0.5255858302116394:  72%|███████▏  | 109/151 [00:24<00:10,  4.01it/s]Epoch: 3, train for the 227-th batch, train loss: 0.4764961898326874:  59%|██████▍    | 226/383 [02:15<01:33,  1.67it/s]Epoch: 3, train for the 227-th batch, train loss: 0.4764961898326874:  59%|██████▌    | 227/383 [02:15<01:32,  1.68it/s]Epoch: 7, train for the 123-th batch, train loss: 0.5543997287750244:  84%|█████████▏ | 122/146 [01:15<00:14,  1.65it/s]Epoch: 7, train for the 123-th batch, train loss: 0.5543997287750244:  84%|█████████▎ | 123/146 [01:15<00:13,  1.65it/s]evaluate for the 20-th batch, evaluate loss: 0.1556645929813385:  48%|████████▌         | 19/40 [00:05<00:05,  3.69it/s]evaluate for the 20-th batch, evaluate loss: 0.1556645929813385:  50%|█████████         | 20/40 [00:05<00:05,  3.51it/s]Epoch: 14, train for the 110-th batch, train loss: 0.5799734592437744:  72%|███████▏  | 109/151 [00:24<00:10,  4.01it/s]Epoch: 14, train for the 110-th batch, train loss: 0.5799734592437744:  73%|███████▎  | 110/151 [00:24<00:09,  4.14it/s]Epoch: 14, train for the 111-th batch, train loss: 0.5264909863471985:  73%|███████▎  | 110/151 [00:24<00:09,  4.14it/s]Epoch: 14, train for the 111-th batch, train loss: 0.5264909863471985:  74%|███████▎  | 111/151 [00:24<00:09,  4.24it/s]evaluate for the 21-th batch, evaluate loss: 0.09469980001449585:  50%|████████▌        | 20/40 [00:05<00:05,  3.51it/s]evaluate for the 21-th batch, evaluate loss: 0.09469980001449585:  52%|████████▉        | 21/40 [00:05<00:05,  3.56it/s]Epoch: 5, train for the 42-th batch, train loss: 0.9178942441940308:  17%|██▏          | 41/237 [00:22<01:56,  1.68it/s]Epoch: 5, train for the 42-th batch, train loss: 0.9178942441940308:  18%|██▎          | 42/237 [00:22<01:57,  1.65it/s]Epoch: 3, train for the 228-th batch, train loss: 0.3826653063297272:  59%|██████▌    | 227/383 [02:16<01:32,  1.68it/s]Epoch: 3, train for the 228-th batch, train loss: 0.3826653063297272:  60%|██████▌    | 228/383 [02:16<01:33,  1.66it/s]Epoch: 14, train for the 112-th batch, train loss: 0.512596070766449:  74%|████████   | 111/151 [00:24<00:09,  4.24it/s]Epoch: 14, train for the 112-th batch, train loss: 0.512596070766449:  74%|████████▏  | 112/151 [00:24<00:09,  4.32it/s]evaluate for the 22-th batch, evaluate loss: 0.14816705882549286:  52%|████████▉        | 21/40 [00:06<00:05,  3.56it/s]evaluate for the 22-th batch, evaluate loss: 0.14816705882549286:  55%|█████████▎       | 22/40 [00:06<00:04,  3.82it/s]evaluate for the 23-th batch, evaluate loss: 0.1414933055639267:  55%|█████████▉        | 22/40 [00:06<00:04,  3.82it/s]evaluate for the 23-th batch, evaluate loss: 0.1414933055639267:  57%|██████████▎       | 23/40 [00:06<00:03,  4.68it/s]Epoch: 14, train for the 113-th batch, train loss: 0.5589434504508972:  74%|███████▍  | 112/151 [00:25<00:09,  4.32it/s]Epoch: 14, train for the 113-th batch, train loss: 0.5589434504508972:  75%|███████▍  | 113/151 [00:25<00:08,  4.37it/s]Epoch: 7, train for the 124-th batch, train loss: 0.5707699060440063:  84%|█████████▎ | 123/146 [01:16<00:13,  1.65it/s]Epoch: 7, train for the 124-th batch, train loss: 0.5707699060440063:  85%|█████████▎ | 124/146 [01:16<00:14,  1.50it/s]evaluate for the 24-th batch, evaluate loss: 0.14621533453464508:  57%|█████████▊       | 23/40 [00:06<00:03,  4.68it/s]evaluate for the 24-th batch, evaluate loss: 0.14621533453464508:  60%|██████████▏      | 24/40 [00:06<00:03,  4.46it/s]Epoch: 5, train for the 43-th batch, train loss: 0.33320918679237366:  18%|██▏         | 42/237 [00:23<01:57,  1.65it/s]Epoch: 5, train for the 43-th batch, train loss: 0.33320918679237366:  18%|██▏         | 43/237 [00:23<01:56,  1.67it/s]Epoch: 14, train for the 114-th batch, train loss: 0.4994271695613861:  75%|███████▍  | 113/151 [00:25<00:08,  4.37it/s]Epoch: 14, train for the 114-th batch, train loss: 0.4994271695613861:  75%|███████▌  | 114/151 [00:25<00:08,  4.41it/s]Epoch: 3, train for the 229-th batch, train loss: 0.3477291166782379:  60%|██████▌    | 228/383 [02:16<01:33,  1.66it/s]Epoch: 3, train for the 229-th batch, train loss: 0.3477291166782379:  60%|██████▌    | 229/383 [02:16<01:32,  1.67it/s]evaluate for the 25-th batch, evaluate loss: 0.13864751160144806:  60%|██████████▏      | 24/40 [00:06<00:03,  4.46it/s]evaluate for the 25-th batch, evaluate loss: 0.13864751160144806:  62%|██████████▋      | 25/40 [00:06<00:03,  4.29it/s]Epoch: 14, train for the 115-th batch, train loss: 0.4983038306236267:  75%|███████▌  | 114/151 [00:25<00:08,  4.41it/s]Epoch: 14, train for the 115-th batch, train loss: 0.4983038306236267:  76%|███████▌  | 115/151 [00:25<00:08,  4.44it/s]Epoch: 14, train for the 116-th batch, train loss: 0.49615126848220825:  76%|██████▊  | 115/151 [00:25<00:08,  4.44it/s]Epoch: 14, train for the 116-th batch, train loss: 0.49615126848220825:  77%|██████▉  | 116/151 [00:25<00:07,  4.45it/s]Epoch: 7, train for the 125-th batch, train loss: 0.5321936011314392:  85%|█████████▎ | 124/146 [01:16<00:14,  1.50it/s]Epoch: 7, train for the 125-th batch, train loss: 0.5321936011314392:  86%|█████████▍ | 125/146 [01:16<00:13,  1.54it/s]evaluate for the 26-th batch, evaluate loss: 0.12334974110126495:  62%|██████████▋      | 25/40 [00:06<00:03,  4.29it/s]evaluate for the 26-th batch, evaluate loss: 0.12334974110126495:  65%|███████████      | 26/40 [00:06<00:03,  3.85it/s]Epoch: 5, train for the 44-th batch, train loss: 0.6897746324539185:  18%|██▎          | 43/237 [00:23<01:56,  1.67it/s]Epoch: 5, train for the 44-th batch, train loss: 0.6897746324539185:  19%|██▍          | 44/237 [00:23<01:57,  1.65it/s]Epoch: 14, train for the 117-th batch, train loss: 0.51044100522995:  77%|█████████▏  | 116/151 [00:25<00:07,  4.45it/s]Epoch: 14, train for the 117-th batch, train loss: 0.51044100522995:  77%|█████████▎  | 117/151 [00:25<00:07,  4.45it/s]evaluate for the 27-th batch, evaluate loss: 0.14513440430164337:  65%|███████████      | 26/40 [00:07<00:03,  3.85it/s]evaluate for the 27-th batch, evaluate loss: 0.14513440430164337:  68%|███████████▍     | 27/40 [00:07<00:03,  3.80it/s]Epoch: 3, train for the 230-th batch, train loss: 0.44613900780677795:  60%|█████▉    | 229/383 [02:17<01:32,  1.67it/s]Epoch: 3, train for the 230-th batch, train loss: 0.44613900780677795:  60%|██████    | 230/383 [02:17<01:32,  1.65it/s]Epoch: 14, train for the 118-th batch, train loss: 0.49364104866981506:  77%|██████▉  | 117/151 [00:26<00:07,  4.45it/s]Epoch: 14, train for the 118-th batch, train loss: 0.49364104866981506:  78%|███████  | 118/151 [00:26<00:07,  4.46it/s]evaluate for the 28-th batch, evaluate loss: 0.11826484650373459:  68%|███████████▍     | 27/40 [00:07<00:03,  3.80it/s]evaluate for the 28-th batch, evaluate loss: 0.11826484650373459:  70%|███████████▉     | 28/40 [00:07<00:03,  3.66it/s]Epoch: 7, train for the 126-th batch, train loss: 0.5346046090126038:  86%|█████████▍ | 125/146 [01:17<00:13,  1.54it/s]Epoch: 7, train for the 126-th batch, train loss: 0.5346046090126038:  86%|█████████▍ | 126/146 [01:17<00:12,  1.58it/s]Epoch: 14, train for the 119-th batch, train loss: 0.5357877016067505:  78%|███████▊  | 118/151 [00:26<00:07,  4.46it/s]Epoch: 14, train for the 119-th batch, train loss: 0.5357877016067505:  79%|███████▉  | 119/151 [00:26<00:07,  4.47it/s]Epoch: 5, train for the 45-th batch, train loss: 0.5321889519691467:  19%|██▍          | 44/237 [00:24<01:57,  1.65it/s]Epoch: 5, train for the 45-th batch, train loss: 0.5321889519691467:  19%|██▍          | 45/237 [00:24<01:55,  1.66it/s]evaluate for the 29-th batch, evaluate loss: 0.16116081178188324:  70%|███████████▉     | 28/40 [00:07<00:03,  3.66it/s]evaluate for the 29-th batch, evaluate loss: 0.16116081178188324:  72%|████████████▎    | 29/40 [00:07<00:02,  3.73it/s]Epoch: 14, train for the 120-th batch, train loss: 0.5896272659301758:  79%|███████▉  | 119/151 [00:26<00:07,  4.47it/s]Epoch: 14, train for the 120-th batch, train loss: 0.5896272659301758:  79%|███████▉  | 120/151 [00:26<00:06,  4.47it/s]Epoch: 3, train for the 231-th batch, train loss: 0.4704556167125702:  60%|██████▌    | 230/383 [02:17<01:32,  1.65it/s]Epoch: 3, train for the 231-th batch, train loss: 0.4704556167125702:  60%|██████▋    | 231/383 [02:17<01:31,  1.66it/s]Epoch: 14, train for the 121-th batch, train loss: 0.48515447974205017:  79%|███████▏ | 120/151 [00:26<00:06,  4.47it/s]Epoch: 14, train for the 121-th batch, train loss: 0.48515447974205017:  80%|███████▏ | 121/151 [00:26<00:06,  4.48it/s]evaluate for the 30-th batch, evaluate loss: 0.1665155440568924:  72%|█████████████     | 29/40 [00:08<00:02,  3.73it/s]evaluate for the 30-th batch, evaluate loss: 0.1665155440568924:  75%|█████████████▌    | 30/40 [00:08<00:02,  3.63it/s]Epoch: 7, train for the 127-th batch, train loss: 0.532542884349823:  86%|██████████▎ | 126/146 [01:18<00:12,  1.58it/s]Epoch: 7, train for the 127-th batch, train loss: 0.532542884349823:  87%|██████████▍ | 127/146 [01:18<00:11,  1.60it/s]Epoch: 5, train for the 46-th batch, train loss: 0.3523687720298767:  19%|██▍          | 45/237 [00:25<01:55,  1.66it/s]Epoch: 5, train for the 46-th batch, train loss: 0.3523687720298767:  19%|██▌          | 46/237 [00:25<01:54,  1.67it/s]Epoch: 14, train for the 122-th batch, train loss: 0.5771381855010986:  80%|████████  | 121/151 [00:27<00:06,  4.48it/s]Epoch: 14, train for the 122-th batch, train loss: 0.5771381855010986:  81%|████████  | 122/151 [00:27<00:06,  4.49it/s]evaluate for the 31-th batch, evaluate loss: 0.16739894449710846:  75%|████████████▊    | 30/40 [00:08<00:02,  3.63it/s]evaluate for the 31-th batch, evaluate loss: 0.16739894449710846:  78%|█████████████▏   | 31/40 [00:08<00:02,  3.70it/s]Epoch: 3, train for the 232-th batch, train loss: 0.33349335193634033:  60%|██████    | 231/383 [02:18<01:31,  1.66it/s]Epoch: 3, train for the 232-th batch, train loss: 0.33349335193634033:  61%|██████    | 232/383 [02:18<01:30,  1.67it/s]Epoch: 14, train for the 123-th batch, train loss: 0.5505125522613525:  81%|████████  | 122/151 [00:27<00:06,  4.49it/s]Epoch: 14, train for the 123-th batch, train loss: 0.5505125522613525:  81%|████████▏ | 123/151 [00:27<00:06,  4.50it/s]evaluate for the 32-th batch, evaluate loss: 0.13237009942531586:  78%|█████████████▏   | 31/40 [00:08<00:02,  3.70it/s]evaluate for the 32-th batch, evaluate loss: 0.13237009942531586:  80%|█████████████▌   | 32/40 [00:08<00:02,  3.65it/s]Epoch: 14, train for the 124-th batch, train loss: 0.5012058019638062:  81%|████████▏ | 123/151 [00:27<00:06,  4.50it/s]Epoch: 14, train for the 124-th batch, train loss: 0.5012058019638062:  82%|████████▏ | 124/151 [00:27<00:06,  4.49it/s]Epoch: 7, train for the 128-th batch, train loss: 0.5263898968696594:  87%|█████████▌ | 127/146 [01:18<00:11,  1.60it/s]Epoch: 7, train for the 128-th batch, train loss: 0.5263898968696594:  88%|█████████▋ | 128/146 [01:18<00:11,  1.61it/s]Epoch: 5, train for the 47-th batch, train loss: 1.00884211063385:  19%|██▉            | 46/237 [00:25<01:54,  1.67it/s]Epoch: 5, train for the 47-th batch, train loss: 1.00884211063385:  20%|██▉            | 47/237 [00:25<01:55,  1.65it/s]evaluate for the 33-th batch, evaluate loss: 0.14470653235912323:  80%|█████████████▌   | 32/40 [00:08<00:02,  3.65it/s]evaluate for the 33-th batch, evaluate loss: 0.14470653235912323:  82%|██████████████   | 33/40 [00:08<00:01,  3.59it/s]Epoch: 14, train for the 125-th batch, train loss: 0.5232049226760864:  82%|████████▏ | 124/151 [00:27<00:06,  4.49it/s]Epoch: 14, train for the 125-th batch, train loss: 0.5232049226760864:  83%|████████▎ | 125/151 [00:27<00:05,  4.49it/s]Epoch: 3, train for the 233-th batch, train loss: 0.4420172870159149:  61%|██████▋    | 232/383 [02:19<01:30,  1.67it/s]Epoch: 3, train for the 233-th batch, train loss: 0.4420172870159149:  61%|██████▋    | 233/383 [02:19<01:30,  1.65it/s]Epoch: 14, train for the 126-th batch, train loss: 0.5221008658409119:  83%|████████▎ | 125/151 [00:27<00:05,  4.49it/s]Epoch: 14, train for the 126-th batch, train loss: 0.5221008658409119:  83%|████████▎ | 126/151 [00:27<00:05,  4.48it/s]evaluate for the 34-th batch, evaluate loss: 0.11094331741333008:  82%|██████████████   | 33/40 [00:09<00:01,  3.59it/s]evaluate for the 34-th batch, evaluate loss: 0.11094331741333008:  85%|██████████████▍  | 34/40 [00:09<00:01,  3.60it/s]Epoch: 7, train for the 129-th batch, train loss: 0.5383319854736328:  88%|█████████▋ | 128/146 [01:19<00:11,  1.61it/s]Epoch: 7, train for the 129-th batch, train loss: 0.5383319854736328:  88%|█████████▋ | 129/146 [01:19<00:10,  1.62it/s]Epoch: 14, train for the 127-th batch, train loss: 0.5557957887649536:  83%|████████▎ | 126/151 [00:28<00:05,  4.48it/s]Epoch: 14, train for the 127-th batch, train loss: 0.5557957887649536:  84%|████████▍ | 127/151 [00:28<00:05,  4.48it/s]Epoch: 5, train for the 48-th batch, train loss: 0.3429235517978668:  20%|██▌          | 47/237 [00:26<01:55,  1.65it/s]Epoch: 5, train for the 48-th batch, train loss: 0.3429235517978668:  20%|██▋          | 48/237 [00:26<01:49,  1.73it/s]evaluate for the 35-th batch, evaluate loss: 0.15456438064575195:  85%|██████████████▍  | 34/40 [00:09<00:01,  3.60it/s]evaluate for the 35-th batch, evaluate loss: 0.15456438064575195:  88%|██████████████▉  | 35/40 [00:09<00:01,  3.52it/s]Epoch: 14, train for the 128-th batch, train loss: 0.6125835180282593:  84%|████████▍ | 127/151 [00:28<00:05,  4.48it/s]Epoch: 14, train for the 128-th batch, train loss: 0.6125835180282593:  85%|████████▍ | 128/151 [00:28<00:05,  4.50it/s]evaluate for the 36-th batch, evaluate loss: 0.14651615917682648:  88%|██████████████▉  | 35/40 [00:09<00:01,  3.52it/s]evaluate for the 36-th batch, evaluate loss: 0.14651615917682648:  90%|███████████████▎ | 36/40 [00:09<00:01,  3.66it/s]Epoch: 5, train for the 49-th batch, train loss: 0.44603338837623596:  20%|██▍         | 48/237 [00:26<01:49,  1.73it/s]Epoch: 5, train for the 49-th batch, train loss: 0.44603338837623596:  21%|██▍         | 49/237 [00:26<01:39,  1.90it/s]Epoch: 14, train for the 129-th batch, train loss: 0.5855323672294617:  85%|████████▍ | 128/151 [00:28<00:05,  4.50it/s]Epoch: 14, train for the 129-th batch, train loss: 0.5855323672294617:  85%|████████▌ | 129/151 [00:28<00:04,  4.51it/s]Epoch: 7, train for the 130-th batch, train loss: 0.5218412280082703:  88%|█████████▋ | 129/146 [01:19<00:10,  1.62it/s]Epoch: 7, train for the 130-th batch, train loss: 0.5218412280082703:  89%|█████████▊ | 130/146 [01:19<00:09,  1.64it/s]Epoch: 3, train for the 234-th batch, train loss: 0.4187149405479431:  61%|██████▋    | 233/383 [02:20<01:30,  1.65it/s]Epoch: 3, train for the 234-th batch, train loss: 0.4187149405479431:  61%|██████▋    | 234/383 [02:20<01:44,  1.42it/s]evaluate for the 37-th batch, evaluate loss: 0.19871856272220612:  90%|███████████████▎ | 36/40 [00:10<00:01,  3.66it/s]evaluate for the 37-th batch, evaluate loss: 0.19871856272220612:  92%|███████████████▋ | 37/40 [00:10<00:00,  3.54it/s]Epoch: 14, train for the 130-th batch, train loss: 0.5203277468681335:  85%|████████▌ | 129/151 [00:28<00:04,  4.51it/s]Epoch: 14, train for the 130-th batch, train loss: 0.5203277468681335:  86%|████████▌ | 130/151 [00:28<00:04,  4.50it/s]Epoch: 5, train for the 50-th batch, train loss: 0.6586407423019409:  21%|██▋          | 49/237 [00:26<01:39,  1.90it/s]Epoch: 5, train for the 50-th batch, train loss: 0.6586407423019409:  21%|██▋          | 50/237 [00:26<01:30,  2.07it/s]Epoch: 14, train for the 131-th batch, train loss: 0.5188500285148621:  86%|████████▌ | 130/151 [00:29<00:04,  4.50it/s]Epoch: 14, train for the 131-th batch, train loss: 0.5188500285148621:  87%|████████▋ | 131/151 [00:29<00:04,  4.49it/s]evaluate for the 38-th batch, evaluate loss: 0.13098978996276855:  92%|███████████████▋ | 37/40 [00:10<00:00,  3.54it/s]evaluate for the 38-th batch, evaluate loss: 0.13098978996276855:  95%|████████████████▏| 38/40 [00:10<00:00,  3.71it/s]Epoch: 14, train for the 132-th batch, train loss: 0.5413885116577148:  87%|████████▋ | 131/151 [00:29<00:04,  4.49it/s]Epoch: 14, train for the 132-th batch, train loss: 0.5413885116577148:  87%|████████▋ | 132/151 [00:29<00:04,  4.50it/s]Epoch: 7, train for the 131-th batch, train loss: 0.5486662983894348:  89%|█████████▊ | 130/146 [01:20<00:09,  1.64it/s]Epoch: 7, train for the 131-th batch, train loss: 0.5486662983894348:  90%|█████████▊ | 131/146 [01:20<00:09,  1.63it/s]Epoch: 3, train for the 235-th batch, train loss: 0.36872443556785583:  61%|██████    | 234/383 [02:20<01:44,  1.42it/s]Epoch: 3, train for the 235-th batch, train loss: 0.36872443556785583:  61%|██████▏   | 235/383 [02:20<01:39,  1.48it/s]evaluate for the 39-th batch, evaluate loss: 0.15238061547279358:  95%|████████████████▏| 38/40 [00:10<00:00,  3.71it/s]evaluate for the 39-th batch, evaluate loss: 0.15238061547279358:  98%|████████████████▌| 39/40 [00:10<00:00,  3.51it/s]evaluate for the 40-th batch, evaluate loss: 0.05494912341237068:  98%|████████████████▌| 39/40 [00:10<00:00,  3.51it/s]evaluate for the 40-th batch, evaluate loss: 0.05494912341237068: 100%|█████████████████| 40/40 [00:10<00:00,  3.75it/s]
  0%|                                                                                            | 0/21 [00:00<?, ?it/s]Epoch: 14, train for the 133-th batch, train loss: 0.5217871069908142:  87%|████████▋ | 132/151 [00:29<00:04,  4.50it/s]Epoch: 14, train for the 133-th batch, train loss: 0.5217871069908142:  88%|████████▊ | 133/151 [00:29<00:03,  4.50it/s]Epoch: 5, train for the 51-th batch, train loss: 0.7043894529342651:  21%|██▋          | 50/237 [00:27<01:30,  2.07it/s]Epoch: 5, train for the 51-th batch, train loss: 0.7043894529342651:  22%|██▊          | 51/237 [00:27<01:35,  1.96it/s]evaluate for the 1-th batch, evaluate loss: 0.21098649501800537:   0%|                           | 0/21 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.21098649501800537:   5%|▉                  | 1/21 [00:00<00:05,  3.90it/s]Epoch: 14, train for the 134-th batch, train loss: 0.5465909838676453:  88%|████████▊ | 133/151 [00:29<00:03,  4.50it/s]Epoch: 14, train for the 134-th batch, train loss: 0.5465909838676453:  89%|████████▊ | 134/151 [00:29<00:03,  4.50it/s]Epoch: 3, train for the 236-th batch, train loss: 0.4186217486858368:  61%|██████▋    | 235/383 [02:21<01:39,  1.48it/s]Epoch: 3, train for the 236-th batch, train loss: 0.4186217486858368:  62%|██████▊    | 236/383 [02:21<01:33,  1.58it/s]Epoch: 14, train for the 135-th batch, train loss: 0.5280589461326599:  89%|████████▊ | 134/151 [00:29<00:03,  4.50it/s]Epoch: 14, train for the 135-th batch, train loss: 0.5280589461326599:  89%|████████▉ | 135/151 [00:29<00:03,  4.49it/s]Epoch: 7, train for the 132-th batch, train loss: 0.5181773900985718:  90%|█████████▊ | 131/146 [01:21<00:09,  1.63it/s]Epoch: 7, train for the 132-th batch, train loss: 0.5181773900985718:  90%|█████████▉ | 132/146 [01:21<00:08,  1.64it/s]evaluate for the 2-th batch, evaluate loss: 0.23045746982097626:   5%|▉                  | 1/21 [00:00<00:05,  3.90it/s]evaluate for the 2-th batch, evaluate loss: 0.23045746982097626:  10%|█▊                 | 2/21 [00:00<00:05,  3.38it/s]Epoch: 3, train for the 237-th batch, train loss: 0.43831130862236023:  62%|██████▏   | 236/383 [02:21<01:33,  1.58it/s]Epoch: 3, train for the 237-th batch, train loss: 0.43831130862236023:  62%|██████▏   | 237/383 [02:21<01:14,  1.96it/s]Epoch: 14, train for the 136-th batch, train loss: 0.5665080547332764:  89%|████████▉ | 135/151 [00:30<00:03,  4.49it/s]Epoch: 14, train for the 136-th batch, train loss: 0.5665080547332764:  90%|█████████ | 136/151 [00:30<00:03,  4.49it/s]evaluate for the 3-th batch, evaluate loss: 0.24341337382793427:  10%|█▊                 | 2/21 [00:00<00:05,  3.38it/s]evaluate for the 3-th batch, evaluate loss: 0.24341337382793427:  14%|██▋                | 3/21 [00:00<00:05,  3.52it/s]Epoch: 3, train for the 238-th batch, train loss: 0.37751808762550354:  62%|██████▏   | 237/383 [02:21<01:14,  1.96it/s]Epoch: 3, train for the 238-th batch, train loss: 0.37751808762550354:  62%|██████▏   | 238/383 [02:21<01:09,  2.09it/s]Epoch: 7, train for the 133-th batch, train loss: 0.5131027102470398:  90%|█████████▉ | 132/146 [01:21<00:08,  1.64it/s]Epoch: 7, train for the 133-th batch, train loss: 0.5131027102470398:  91%|██████████ | 133/146 [01:21<00:07,  1.65it/s]evaluate for the 4-th batch, evaluate loss: 0.21083371341228485:  14%|██▋                | 3/21 [00:01<00:05,  3.52it/s]evaluate for the 4-th batch, evaluate loss: 0.21083371341228485:  19%|███▌               | 4/21 [00:01<00:05,  3.32it/s]Epoch: 5, train for the 52-th batch, train loss: 0.7001737952232361:  22%|██▊          | 51/237 [00:28<01:35,  1.96it/s]Epoch: 5, train for the 52-th batch, train loss: 0.7001737952232361:  22%|██▊          | 52/237 [00:28<02:06,  1.46it/s]Epoch: 14, train for the 137-th batch, train loss: 0.5884267091751099:  90%|█████████ | 136/151 [00:30<00:03,  4.49it/s]Epoch: 14, train for the 137-th batch, train loss: 0.5884267091751099:  91%|█████████ | 137/151 [00:30<00:04,  3.24it/s]evaluate for the 5-th batch, evaluate loss: 0.22145619988441467:  19%|███▌               | 4/21 [00:01<00:05,  3.32it/s]evaluate for the 5-th batch, evaluate loss: 0.22145619988441467:  24%|████▌              | 5/21 [00:01<00:04,  3.42it/s]Epoch: 14, train for the 138-th batch, train loss: 0.6336493492126465:  91%|█████████ | 137/151 [00:30<00:04,  3.24it/s]Epoch: 14, train for the 138-th batch, train loss: 0.6336493492126465:  91%|█████████▏| 138/151 [00:30<00:03,  3.54it/s]Epoch: 3, train for the 239-th batch, train loss: 0.4487588107585907:  62%|██████▊    | 238/383 [02:22<01:09,  2.09it/s]Epoch: 3, train for the 239-th batch, train loss: 0.4487588107585907:  62%|██████▊    | 239/383 [02:22<01:07,  2.12it/s]Epoch: 14, train for the 139-th batch, train loss: 0.5793373584747314:  91%|█████████▏| 138/151 [00:31<00:03,  3.54it/s]Epoch: 14, train for the 139-th batch, train loss: 0.5793373584747314:  92%|█████████▏| 139/151 [00:31<00:03,  3.78it/s]Epoch: 5, train for the 53-th batch, train loss: 0.46186167001724243:  22%|██▋         | 52/237 [00:29<02:06,  1.46it/s]Epoch: 5, train for the 53-th batch, train loss: 0.46186167001724243:  22%|██▋         | 53/237 [00:29<01:58,  1.56it/s]Epoch: 7, train for the 134-th batch, train loss: 0.5240205526351929:  91%|██████████ | 133/146 [01:22<00:07,  1.65it/s]Epoch: 7, train for the 134-th batch, train loss: 0.5240205526351929:  92%|██████████ | 134/146 [01:22<00:07,  1.65it/s]evaluate for the 6-th batch, evaluate loss: 0.22752033174037933:  24%|████▌              | 5/21 [00:01<00:04,  3.42it/s]evaluate for the 6-th batch, evaluate loss: 0.22752033174037933:  29%|█████▍             | 6/21 [00:01<00:04,  3.28it/s]Epoch: 14, train for the 140-th batch, train loss: 0.5286261439323425:  92%|█████████▏| 139/151 [00:31<00:03,  3.78it/s]Epoch: 14, train for the 140-th batch, train loss: 0.5286261439323425:  93%|█████████▎| 140/151 [00:31<00:02,  3.97it/s]evaluate for the 7-th batch, evaluate loss: 0.2352178692817688:  29%|█████▋              | 6/21 [00:02<00:04,  3.28it/s]evaluate for the 7-th batch, evaluate loss: 0.2352178692817688:  33%|██████▋             | 7/21 [00:02<00:04,  3.41it/s]Epoch: 14, train for the 141-th batch, train loss: 0.5543358325958252:  93%|█████████▎| 140/151 [00:31<00:02,  3.97it/s]Epoch: 14, train for the 141-th batch, train loss: 0.5543358325958252:  93%|█████████▎| 141/151 [00:31<00:02,  4.12it/s]Epoch: 3, train for the 240-th batch, train loss: 0.4968228042125702:  62%|██████▊    | 239/383 [02:22<01:07,  2.12it/s]Epoch: 3, train for the 240-th batch, train loss: 0.4968228042125702:  63%|██████▉    | 240/383 [02:22<01:14,  1.93it/s]Epoch: 5, train for the 54-th batch, train loss: 0.2549843490123749:  22%|██▉          | 53/237 [00:29<01:58,  1.56it/s]Epoch: 5, train for the 54-th batch, train loss: 0.2549843490123749:  23%|██▉          | 54/237 [00:29<01:54,  1.60it/s]Epoch: 7, train for the 135-th batch, train loss: 0.5526599884033203:  92%|██████████ | 134/146 [01:23<00:07,  1.65it/s]Epoch: 7, train for the 135-th batch, train loss: 0.5526599884033203:  92%|██████████▏| 135/146 [01:23<00:06,  1.66it/s]Epoch: 14, train for the 142-th batch, train loss: 0.5720784664154053:  93%|█████████▎| 141/151 [00:31<00:02,  4.12it/s]Epoch: 14, train for the 142-th batch, train loss: 0.5720784664154053:  94%|█████████▍| 142/151 [00:31<00:02,  4.22it/s]evaluate for the 8-th batch, evaluate loss: 0.274809330701828:  33%|███████              | 7/21 [00:02<00:04,  3.41it/s]evaluate for the 8-th batch, evaluate loss: 0.274809330701828:  38%|████████             | 8/21 [00:02<00:03,  3.32it/s]Epoch: 14, train for the 143-th batch, train loss: 0.49158111214637756:  94%|████████▍| 142/151 [00:31<00:02,  4.22it/s]Epoch: 14, train for the 143-th batch, train loss: 0.49158111214637756:  95%|████████▌| 143/151 [00:31<00:01,  4.29it/s]evaluate for the 9-th batch, evaluate loss: 0.21027541160583496:  38%|███████▏           | 8/21 [00:02<00:03,  3.32it/s]evaluate for the 9-th batch, evaluate loss: 0.21027541160583496:  43%|████████▏          | 9/21 [00:02<00:03,  3.41it/s]Epoch: 3, train for the 241-th batch, train loss: 0.4993418753147125:  63%|██████▉    | 240/383 [02:23<01:14,  1.93it/s]Epoch: 3, train for the 241-th batch, train loss: 0.4993418753147125:  63%|██████▉    | 241/383 [02:23<01:16,  1.85it/s]Epoch: 14, train for the 144-th batch, train loss: 0.5011779069900513:  95%|█████████▍| 143/151 [00:32<00:01,  4.29it/s]Epoch: 14, train for the 144-th batch, train loss: 0.5011779069900513:  95%|█████████▌| 144/151 [00:32<00:01,  4.35it/s]Epoch: 5, train for the 55-th batch, train loss: 0.5396624803543091:  23%|██▉          | 54/237 [00:30<01:54,  1.60it/s]Epoch: 5, train for the 55-th batch, train loss: 0.5396624803543091:  23%|███          | 55/237 [00:30<01:51,  1.64it/s]evaluate for the 10-th batch, evaluate loss: 0.2068130373954773:  43%|████████▏          | 9/21 [00:02<00:03,  3.41it/s]evaluate for the 10-th batch, evaluate loss: 0.2068130373954773:  48%|████████▌         | 10/21 [00:02<00:03,  3.35it/s]Epoch: 7, train for the 136-th batch, train loss: 0.5301687121391296:  92%|██████████▏| 135/146 [01:23<00:06,  1.66it/s]Epoch: 7, train for the 136-th batch, train loss: 0.5301687121391296:  93%|██████████▏| 136/146 [01:23<00:05,  1.67it/s]Epoch: 14, train for the 145-th batch, train loss: 0.5583940148353577:  95%|█████████▌| 144/151 [00:32<00:01,  4.35it/s]Epoch: 14, train for the 145-th batch, train loss: 0.5583940148353577:  96%|█████████▌| 145/151 [00:32<00:01,  4.39it/s]evaluate for the 11-th batch, evaluate loss: 0.14862151443958282:  48%|████████         | 10/21 [00:03<00:03,  3.35it/s]evaluate for the 11-th batch, evaluate loss: 0.14862151443958282:  52%|████████▉        | 11/21 [00:03<00:02,  3.45it/s]Epoch: 14, train for the 146-th batch, train loss: 0.5253661870956421:  96%|█████████▌| 145/151 [00:32<00:01,  4.39it/s]Epoch: 14, train for the 146-th batch, train loss: 0.5253661870956421:  97%|█████████▋| 146/151 [00:32<00:01,  4.40it/s]Epoch: 3, train for the 242-th batch, train loss: 0.4403415620326996:  63%|██████▉    | 241/383 [02:24<01:16,  1.85it/s]Epoch: 3, train for the 242-th batch, train loss: 0.4403415620326996:  63%|██████▉    | 242/383 [02:24<01:18,  1.80it/s]Epoch: 14, train for the 147-th batch, train loss: 0.5430576801300049:  97%|█████████▋| 146/151 [00:32<00:01,  4.40it/s]Epoch: 14, train for the 147-th batch, train loss: 0.5430576801300049:  97%|█████████▋| 147/151 [00:32<00:00,  4.45it/s]Epoch: 5, train for the 56-th batch, train loss: 0.6164687871932983:  23%|███          | 55/237 [00:30<01:51,  1.64it/s]Epoch: 5, train for the 56-th batch, train loss: 0.6164687871932983:  24%|███          | 56/237 [00:30<01:48,  1.67it/s]evaluate for the 12-th batch, evaluate loss: 0.26455482840538025:  52%|████████▉        | 11/21 [00:03<00:02,  3.45it/s]evaluate for the 12-th batch, evaluate loss: 0.26455482840538025:  57%|█████████▋       | 12/21 [00:03<00:02,  3.40it/s]Epoch: 7, train for the 137-th batch, train loss: 0.535942018032074:  93%|███████████▏| 136/146 [01:24<00:05,  1.67it/s]Epoch: 7, train for the 137-th batch, train loss: 0.535942018032074:  94%|███████████▎| 137/146 [01:24<00:05,  1.69it/s]Epoch: 14, train for the 148-th batch, train loss: 0.6175369620323181:  97%|█████████▋| 147/151 [00:33<00:00,  4.45it/s]Epoch: 14, train for the 148-th batch, train loss: 0.6175369620323181:  98%|█████████▊| 148/151 [00:33<00:00,  4.48it/s]evaluate for the 13-th batch, evaluate loss: 0.21527773141860962:  57%|█████████▋       | 12/21 [00:03<00:02,  3.40it/s]evaluate for the 13-th batch, evaluate loss: 0.21527773141860962:  62%|██████████▌      | 13/21 [00:03<00:02,  3.52it/s]Epoch: 14, train for the 149-th batch, train loss: 0.5240963101387024:  98%|█████████▊| 148/151 [00:33<00:00,  4.48it/s]Epoch: 14, train for the 149-th batch, train loss: 0.5240963101387024:  99%|█████████▊| 149/151 [00:33<00:00,  4.45it/s]Epoch: 3, train for the 243-th batch, train loss: 0.46836477518081665:  63%|██████▎   | 242/383 [02:24<01:18,  1.80it/s]Epoch: 3, train for the 243-th batch, train loss: 0.46836477518081665:  63%|██████▎   | 243/383 [02:24<01:18,  1.78it/s]evaluate for the 14-th batch, evaluate loss: 0.18783576786518097:  62%|██████████▌      | 13/21 [00:04<00:02,  3.52it/s]evaluate for the 14-th batch, evaluate loss: 0.18783576786518097:  67%|███████████▎     | 14/21 [00:04<00:02,  3.45it/s]Epoch: 5, train for the 57-th batch, train loss: 0.7991667985916138:  24%|███          | 56/237 [00:31<01:48,  1.67it/s]Epoch: 5, train for the 57-th batch, train loss: 0.7991667985916138:  24%|███▏         | 57/237 [00:31<01:48,  1.67it/s]Epoch: 7, train for the 138-th batch, train loss: 0.5577278137207031:  94%|██████████▎| 137/146 [01:24<00:05,  1.69it/s]Epoch: 7, train for the 138-th batch, train loss: 0.5577278137207031:  95%|██████████▍| 138/146 [01:24<00:04,  1.70it/s]Epoch: 14, train for the 150-th batch, train loss: 0.5018572807312012:  99%|█████████▊| 149/151 [00:33<00:00,  4.45it/s]Epoch: 14, train for the 150-th batch, train loss: 0.5018572807312012:  99%|█████████▉| 150/151 [00:33<00:00,  4.46it/s]Epoch: 14, train for the 151-th batch, train loss: 0.6388928890228271:  99%|█████████▉| 150/151 [00:33<00:00,  4.46it/s]Epoch: 14, train for the 151-th batch, train loss: 0.6388928890228271: 100%|██████████| 151/151 [00:33<00:00,  4.84it/s]Epoch: 14, train for the 151-th batch, train loss: 0.6388928890228271: 100%|██████████| 151/151 [00:33<00:00,  4.48it/s]
  0%|                                                                                            | 0/46 [00:00<?, ?it/s]evaluate for the 15-th batch, evaluate loss: 0.17948521673679352:  67%|███████████▎     | 14/21 [00:04<00:02,  3.45it/s]evaluate for the 15-th batch, evaluate loss: 0.17948521673679352:  71%|████████████▏    | 15/21 [00:04<00:01,  3.57it/s]evaluate for the 1-th batch, evaluate loss: 0.4939158856868744:   0%|                            | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.4939158856868744:   2%|▍                   | 1/46 [00:00<00:04,  9.56it/s]evaluate for the 2-th batch, evaluate loss: 0.503863513469696:   2%|▍                    | 1/46 [00:00<00:04,  9.56it/s]evaluate for the 2-th batch, evaluate loss: 0.503863513469696:   4%|▉                    | 2/46 [00:00<00:04,  9.58it/s]Epoch: 3, train for the 244-th batch, train loss: 0.37131187319755554:  63%|██████▎   | 243/383 [02:25<01:18,  1.78it/s]Epoch: 3, train for the 244-th batch, train loss: 0.37131187319755554:  64%|██████▎   | 244/383 [02:25<01:19,  1.76it/s]evaluate for the 3-th batch, evaluate loss: 0.4803961217403412:   4%|▊                   | 2/46 [00:00<00:04,  9.58it/s]evaluate for the 3-th batch, evaluate loss: 0.4803961217403412:   7%|█▎                  | 3/46 [00:00<00:04,  9.56it/s]evaluate for the 16-th batch, evaluate loss: 0.22160270810127258:  71%|████████████▏    | 15/21 [00:04<00:01,  3.57it/s]evaluate for the 16-th batch, evaluate loss: 0.22160270810127258:  76%|████████████▉    | 16/21 [00:04<00:01,  3.49it/s]Epoch: 7, train for the 139-th batch, train loss: 0.5616018772125244:  95%|██████████▍| 138/146 [01:25<00:04,  1.70it/s]Epoch: 7, train for the 139-th batch, train loss: 0.5616018772125244:  95%|██████████▍| 139/146 [01:25<00:04,  1.71it/s]Epoch: 5, train for the 58-th batch, train loss: 0.6490011215209961:  24%|███▏         | 57/237 [00:32<01:48,  1.67it/s]Epoch: 5, train for the 58-th batch, train loss: 0.6490011215209961:  24%|███▏         | 58/237 [00:32<01:47,  1.67it/s]evaluate for the 4-th batch, evaluate loss: 0.5055676698684692:   7%|█▎                  | 3/46 [00:00<00:04,  9.56it/s]evaluate for the 4-th batch, evaluate loss: 0.5055676698684692:   9%|█▋                  | 4/46 [00:00<00:04,  9.57it/s]evaluate for the 5-th batch, evaluate loss: 0.4830499589443207:   9%|█▋                  | 4/46 [00:00<00:04,  9.57it/s]evaluate for the 5-th batch, evaluate loss: 0.4830499589443207:  11%|██▏                 | 5/46 [00:00<00:04,  9.58it/s]evaluate for the 17-th batch, evaluate loss: 0.21343223750591278:  76%|████████████▉    | 16/21 [00:04<00:01,  3.49it/s]evaluate for the 17-th batch, evaluate loss: 0.21343223750591278:  81%|█████████████▊   | 17/21 [00:04<00:01,  3.60it/s]evaluate for the 6-th batch, evaluate loss: 0.5564317107200623:  11%|██▏                 | 5/46 [00:00<00:04,  9.58it/s]evaluate for the 6-th batch, evaluate loss: 0.5564317107200623:  13%|██▌                 | 6/46 [00:00<00:04,  9.60it/s]evaluate for the 7-th batch, evaluate loss: 0.4819747507572174:  13%|██▌                 | 6/46 [00:00<00:04,  9.60it/s]evaluate for the 7-th batch, evaluate loss: 0.4819747507572174:  15%|███                 | 7/46 [00:00<00:04,  9.65it/s]Epoch: 3, train for the 245-th batch, train loss: 0.3919367790222168:  64%|███████    | 244/383 [02:25<01:19,  1.76it/s]Epoch: 3, train for the 245-th batch, train loss: 0.3919367790222168:  64%|███████    | 245/383 [02:25<01:19,  1.73it/s]evaluate for the 8-th batch, evaluate loss: 0.5595210194587708:  15%|███                 | 7/46 [00:00<00:04,  9.65it/s]evaluate for the 8-th batch, evaluate loss: 0.5595210194587708:  17%|███▍                | 8/46 [00:00<00:03,  9.64it/s]evaluate for the 18-th batch, evaluate loss: 0.19577144086360931:  81%|█████████████▊   | 17/21 [00:05<00:01,  3.60it/s]evaluate for the 18-th batch, evaluate loss: 0.19577144086360931:  86%|██████████████▌  | 18/21 [00:05<00:00,  3.53it/s]evaluate for the 9-th batch, evaluate loss: 0.5260399580001831:  17%|███▍                | 8/46 [00:00<00:03,  9.64it/s]evaluate for the 9-th batch, evaluate loss: 0.5260399580001831:  20%|███▉                | 9/46 [00:00<00:03,  9.62it/s]Epoch: 7, train for the 140-th batch, train loss: 0.5199366807937622:  95%|██████████▍| 139/146 [01:25<00:04,  1.71it/s]Epoch: 7, train for the 140-th batch, train loss: 0.5199366807937622:  96%|██████████▌| 140/146 [01:25<00:03,  1.71it/s]Epoch: 5, train for the 59-th batch, train loss: 0.7989736199378967:  24%|███▏         | 58/237 [00:32<01:47,  1.67it/s]Epoch: 5, train for the 59-th batch, train loss: 0.7989736199378967:  25%|███▏         | 59/237 [00:32<01:46,  1.67it/s]evaluate for the 10-th batch, evaluate loss: 0.5339222550392151:  20%|███▋               | 9/46 [00:01<00:03,  9.62it/s]evaluate for the 10-th batch, evaluate loss: 0.5339222550392151:  22%|███▉              | 10/46 [00:01<00:03,  9.63it/s]evaluate for the 11-th batch, evaluate loss: 0.5242704749107361:  22%|███▉              | 10/46 [00:01<00:03,  9.63it/s]evaluate for the 11-th batch, evaluate loss: 0.5242704749107361:  24%|████▎             | 11/46 [00:01<00:03,  9.64it/s]evaluate for the 19-th batch, evaluate loss: 0.26095664501190186:  86%|██████████████▌  | 18/21 [00:05<00:00,  3.53it/s]evaluate for the 19-th batch, evaluate loss: 0.26095664501190186:  90%|███████████████▍ | 19/21 [00:05<00:00,  3.61it/s]evaluate for the 12-th batch, evaluate loss: 0.47639036178588867:  24%|████             | 11/46 [00:01<00:03,  9.64it/s]evaluate for the 12-th batch, evaluate loss: 0.47639036178588867:  26%|████▍            | 12/46 [00:01<00:03,  9.66it/s]evaluate for the 13-th batch, evaluate loss: 0.4959704577922821:  26%|████▋             | 12/46 [00:01<00:03,  9.66it/s]evaluate for the 13-th batch, evaluate loss: 0.4959704577922821:  28%|█████             | 13/46 [00:01<00:03,  9.66it/s]Epoch: 3, train for the 246-th batch, train loss: 0.4512649178504944:  64%|███████    | 245/383 [02:26<01:19,  1.73it/s]Epoch: 3, train for the 246-th batch, train loss: 0.4512649178504944:  64%|███████    | 246/383 [02:26<01:20,  1.71it/s]evaluate for the 20-th batch, evaluate loss: 0.232754185795784:  90%|█████████████████▏ | 19/21 [00:05<00:00,  3.61it/s]evaluate for the 20-th batch, evaluate loss: 0.232754185795784:  95%|██████████████████ | 20/21 [00:05<00:00,  3.55it/s]evaluate for the 14-th batch, evaluate loss: 0.5887439846992493:  28%|█████             | 13/46 [00:01<00:03,  9.66it/s]evaluate for the 14-th batch, evaluate loss: 0.5887439846992493:  30%|█████▍            | 14/46 [00:01<00:03,  9.67it/s]evaluate for the 21-th batch, evaluate loss: 0.09576667845249176:  95%|████████████████▏| 20/21 [00:05<00:00,  3.55it/s]evaluate for the 21-th batch, evaluate loss: 0.09576667845249176: 100%|█████████████████| 21/21 [00:05<00:00,  3.62it/s]
INFO:root:Epoch: 8, learning rate: 0.0001, train loss: 0.2046
INFO:root:train average_precision, 0.9727
INFO:root:train roc_auc, 0.9643
INFO:root:validate loss: 0.1483
INFO:root:validate average_precision, 0.9866
INFO:root:validate roc_auc, 0.9842
INFO:root:new node validate loss: 0.2137
INFO:root:new node validate first_1_average_precision, 0.9175
INFO:root:new node validate first_1_roc_auc, 0.9250
INFO:root:new node validate first_3_average_precision, 0.9543
INFO:root:new node validate first_3_roc_auc, 0.9565
INFO:root:new node validate first_10_average_precision, 0.9704
INFO:root:new node validate first_10_roc_auc, 0.9702
INFO:root:new node validate average_precision, 0.9721
INFO:root:new node validate roc_auc, 0.9700
INFO:root:save model ./saved_models/DyGFormer/ia-reality-call/DyGFormer_seed0_dygformer-ia-reality-call-old/DyGFormer_seed0_dygformer-ia-reality-call-old.pkl
Epoch: 7, train for the 141-th batch, train loss: 0.5264397859573364:  96%|██████████▌| 140/146 [01:26<00:03,  1.71it/s]Epoch: 7, train for the 141-th batch, train loss: 0.5264397859573364:  97%|██████████▌| 141/146 [01:26<00:02,  1.75it/s]evaluate for the 15-th batch, evaluate loss: 0.5385061502456665:  30%|█████▍            | 14/46 [00:01<00:03,  9.67it/s]evaluate for the 15-th batch, evaluate loss: 0.5385061502456665:  33%|█████▊            | 15/46 [00:01<00:03,  9.66it/s]Epoch: 5, train for the 60-th batch, train loss: 0.8397969007492065:  25%|███▏         | 59/237 [00:33<01:46,  1.67it/s]Epoch: 5, train for the 60-th batch, train loss: 0.8397969007492065:  25%|███▎         | 60/237 [00:33<01:46,  1.67it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]evaluate for the 16-th batch, evaluate loss: 0.5704017877578735:  33%|█████▊            | 15/46 [00:01<00:03,  9.66it/s]evaluate for the 16-th batch, evaluate loss: 0.5704017877578735:  35%|██████▎           | 16/46 [00:01<00:03,  9.67it/s]evaluate for the 17-th batch, evaluate loss: 0.452965646982193:  35%|██████▌            | 16/46 [00:01<00:03,  9.67it/s]evaluate for the 17-th batch, evaluate loss: 0.452965646982193:  37%|███████            | 17/46 [00:01<00:03,  9.65it/s]evaluate for the 18-th batch, evaluate loss: 0.5017656683921814:  37%|██████▋           | 17/46 [00:01<00:03,  9.65it/s]evaluate for the 18-th batch, evaluate loss: 0.5017656683921814:  39%|███████           | 18/46 [00:01<00:02,  9.62it/s]Epoch: 7, train for the 142-th batch, train loss: 0.49617788195610046:  97%|█████████▋| 141/146 [01:26<00:02,  1.75it/s]Epoch: 7, train for the 142-th batch, train loss: 0.49617788195610046:  97%|█████████▋| 142/146 [01:26<00:02,  1.90it/s]evaluate for the 19-th batch, evaluate loss: 0.5248019099235535:  39%|███████           | 18/46 [00:01<00:02,  9.62it/s]evaluate for the 19-th batch, evaluate loss: 0.5248019099235535:  41%|███████▍          | 19/46 [00:01<00:02,  9.62it/s]Epoch: 3, train for the 247-th batch, train loss: 0.3290979564189911:  64%|███████    | 246/383 [02:27<01:20,  1.71it/s]Epoch: 3, train for the 247-th batch, train loss: 0.3290979564189911:  64%|███████    | 247/383 [02:27<01:20,  1.69it/s]evaluate for the 20-th batch, evaluate loss: 0.539131760597229:  41%|███████▊           | 19/46 [00:02<00:02,  9.62it/s]evaluate for the 20-th batch, evaluate loss: 0.539131760597229:  43%|████████▎          | 20/46 [00:02<00:02,  9.64it/s]Epoch: 9, train for the 1-th batch, train loss: 0.9930475950241089:   0%|                       | 0/119 [00:00<?, ?it/s]Epoch: 9, train for the 1-th batch, train loss: 0.9930475950241089:   1%|▏              | 1/119 [00:00<01:02,  1.90it/s]evaluate for the 21-th batch, evaluate loss: 0.529753565788269:  43%|████████▎          | 20/46 [00:02<00:02,  9.64it/s]evaluate for the 21-th batch, evaluate loss: 0.529753565788269:  46%|████████▋          | 21/46 [00:02<00:02,  9.66it/s]Epoch: 5, train for the 61-th batch, train loss: 0.8022352457046509:  25%|███▎         | 60/237 [00:33<01:46,  1.67it/s]Epoch: 5, train for the 61-th batch, train loss: 0.8022352457046509:  26%|███▎         | 61/237 [00:33<01:45,  1.66it/s]evaluate for the 22-th batch, evaluate loss: 0.519919216632843:  46%|████████▋          | 21/46 [00:02<00:02,  9.66it/s]evaluate for the 22-th batch, evaluate loss: 0.519919216632843:  48%|█████████          | 22/46 [00:02<00:02,  9.65it/s]evaluate for the 23-th batch, evaluate loss: 0.47668567299842834:  48%|████████▏        | 22/46 [00:02<00:02,  9.65it/s]evaluate for the 23-th batch, evaluate loss: 0.47668567299842834:  50%|████████▌        | 23/46 [00:02<00:02,  9.56it/s]Epoch: 7, train for the 143-th batch, train loss: 0.5123128890991211:  97%|██████████▋| 142/146 [01:27<00:02,  1.90it/s]Epoch: 7, train for the 143-th batch, train loss: 0.5123128890991211:  98%|██████████▊| 143/146 [01:27<00:01,  1.85it/s]evaluate for the 24-th batch, evaluate loss: 0.483061283826828:  50%|█████████▌         | 23/46 [00:02<00:02,  9.56it/s]evaluate for the 24-th batch, evaluate loss: 0.483061283826828:  52%|█████████▉         | 24/46 [00:02<00:02,  9.59it/s]evaluate for the 25-th batch, evaluate loss: 0.5361344218254089:  52%|█████████▍        | 24/46 [00:02<00:02,  9.59it/s]evaluate for the 25-th batch, evaluate loss: 0.5361344218254089:  54%|█████████▊        | 25/46 [00:02<00:02,  9.61it/s]Epoch: 3, train for the 248-th batch, train loss: 0.3528175950050354:  64%|███████    | 247/383 [02:27<01:20,  1.69it/s]Epoch: 3, train for the 248-th batch, train loss: 0.3528175950050354:  65%|███████    | 248/383 [02:27<01:20,  1.69it/s]Epoch: 9, train for the 2-th batch, train loss: 0.5091890692710876:   1%|▏              | 1/119 [00:01<01:02,  1.90it/s]Epoch: 9, train for the 2-th batch, train loss: 0.5091890692710876:   2%|▎              | 2/119 [00:01<01:02,  1.88it/s]evaluate for the 26-th batch, evaluate loss: 0.5527956485748291:  54%|█████████▊        | 25/46 [00:02<00:02,  9.61it/s]evaluate for the 26-th batch, evaluate loss: 0.5527956485748291:  57%|██████████▏       | 26/46 [00:02<00:02,  9.60it/s]Epoch: 5, train for the 62-th batch, train loss: 0.5919349789619446:  26%|███▎         | 61/237 [00:34<01:45,  1.66it/s]Epoch: 5, train for the 62-th batch, train loss: 0.5919349789619446:  26%|███▍         | 62/237 [00:34<01:43,  1.68it/s]evaluate for the 27-th batch, evaluate loss: 0.4943431317806244:  57%|██████████▏       | 26/46 [00:02<00:02,  9.60it/s]evaluate for the 27-th batch, evaluate loss: 0.4943431317806244:  59%|██████████▌       | 27/46 [00:02<00:01,  9.59it/s]evaluate for the 28-th batch, evaluate loss: 0.522165060043335:  59%|███████████▏       | 27/46 [00:02<00:01,  9.59it/s]evaluate for the 28-th batch, evaluate loss: 0.522165060043335:  61%|███████████▌       | 28/46 [00:02<00:01,  9.59it/s]evaluate for the 29-th batch, evaluate loss: 0.4939938187599182:  61%|██████████▉       | 28/46 [00:03<00:01,  9.59it/s]evaluate for the 29-th batch, evaluate loss: 0.4939938187599182:  63%|███████████▎      | 29/46 [00:03<00:01,  9.60it/s]evaluate for the 30-th batch, evaluate loss: 0.4941615164279938:  63%|███████████▎      | 29/46 [00:03<00:01,  9.60it/s]evaluate for the 30-th batch, evaluate loss: 0.4941615164279938:  65%|███████████▋      | 30/46 [00:03<00:01,  9.60it/s]Epoch: 7, train for the 144-th batch, train loss: 0.5028044581413269:  98%|██████████▊| 143/146 [01:28<00:01,  1.85it/s]Epoch: 7, train for the 144-th batch, train loss: 0.5028044581413269:  99%|██████████▊| 144/146 [01:28<00:01,  1.76it/s]Epoch: 3, train for the 249-th batch, train loss: 0.46157750487327576:  65%|██████▍   | 248/383 [02:28<01:20,  1.69it/s]Epoch: 3, train for the 249-th batch, train loss: 0.46157750487327576:  65%|██████▌   | 249/383 [02:28<01:19,  1.69it/s]evaluate for the 31-th batch, evaluate loss: 0.5222378969192505:  65%|███████████▋      | 30/46 [00:03<00:01,  9.60it/s]evaluate for the 31-th batch, evaluate loss: 0.5222378969192505:  67%|████████████▏     | 31/46 [00:03<00:01,  9.57it/s]Epoch: 9, train for the 3-th batch, train loss: 0.3254389762878418:   2%|▎              | 2/119 [00:01<01:02,  1.88it/s]Epoch: 9, train for the 3-th batch, train loss: 0.3254389762878418:   3%|▍              | 3/119 [00:01<01:05,  1.78it/s]evaluate for the 32-th batch, evaluate loss: 0.48237523436546326:  67%|███████████▍     | 31/46 [00:03<00:01,  9.57it/s]evaluate for the 32-th batch, evaluate loss: 0.48237523436546326:  70%|███████████▊     | 32/46 [00:03<00:01,  9.57it/s]Epoch: 5, train for the 63-th batch, train loss: 0.5436736941337585:  26%|███▍         | 62/237 [00:35<01:43,  1.68it/s]Epoch: 5, train for the 63-th batch, train loss: 0.5436736941337585:  27%|███▍         | 63/237 [00:35<01:42,  1.70it/s]evaluate for the 33-th batch, evaluate loss: 0.4947979152202606:  70%|████████████▌     | 32/46 [00:03<00:01,  9.57it/s]evaluate for the 33-th batch, evaluate loss: 0.4947979152202606:  72%|████████████▉     | 33/46 [00:03<00:01,  9.58it/s]evaluate for the 34-th batch, evaluate loss: 0.4827626645565033:  72%|████████████▉     | 33/46 [00:03<00:01,  9.58it/s]evaluate for the 34-th batch, evaluate loss: 0.4827626645565033:  74%|█████████████▎    | 34/46 [00:03<00:01,  9.60it/s]evaluate for the 35-th batch, evaluate loss: 0.4811009168624878:  74%|█████████████▎    | 34/46 [00:03<00:01,  9.60it/s]evaluate for the 35-th batch, evaluate loss: 0.4811009168624878:  76%|█████████████▋    | 35/46 [00:03<00:01,  9.64it/s]Epoch: 7, train for the 145-th batch, train loss: 0.5168645977973938:  99%|██████████▊| 144/146 [01:28<00:01,  1.76it/s]Epoch: 7, train for the 145-th batch, train loss: 0.5168645977973938:  99%|██████████▉| 145/146 [01:28<00:00,  1.73it/s]evaluate for the 36-th batch, evaluate loss: 0.47150716185569763:  76%|████████████▉    | 35/46 [00:03<00:01,  9.64it/s]evaluate for the 36-th batch, evaluate loss: 0.47150716185569763:  78%|█████████████▎   | 36/46 [00:03<00:01,  9.65it/s]Epoch: 3, train for the 250-th batch, train loss: 0.46075910329818726:  65%|██████▌   | 249/383 [02:28<01:19,  1.69it/s]Epoch: 3, train for the 250-th batch, train loss: 0.46075910329818726:  65%|██████▌   | 250/383 [02:28<01:18,  1.70it/s]Epoch: 9, train for the 4-th batch, train loss: 0.32130470871925354:   3%|▎             | 3/119 [00:02<01:05,  1.78it/s]Epoch: 9, train for the 4-th batch, train loss: 0.32130470871925354:   3%|▍             | 4/119 [00:02<01:04,  1.78it/s]evaluate for the 37-th batch, evaluate loss: 0.5062383413314819:  78%|██████████████    | 36/46 [00:03<00:01,  9.65it/s]evaluate for the 37-th batch, evaluate loss: 0.5062383413314819:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.63it/s]Epoch: 5, train for the 64-th batch, train loss: 0.6605742573738098:  27%|███▍         | 63/237 [00:35<01:42,  1.70it/s]Epoch: 5, train for the 64-th batch, train loss: 0.6605742573738098:  27%|███▌         | 64/237 [00:35<01:42,  1.69it/s]evaluate for the 38-th batch, evaluate loss: 0.5338889956474304:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.63it/s]evaluate for the 38-th batch, evaluate loss: 0.5338889956474304:  83%|██████████████▊   | 38/46 [00:03<00:00,  9.63it/s]evaluate for the 39-th batch, evaluate loss: 0.5298370718955994:  83%|██████████████▊   | 38/46 [00:04<00:00,  9.63it/s]evaluate for the 39-th batch, evaluate loss: 0.5298370718955994:  85%|███████████████▎  | 39/46 [00:04<00:00,  9.63it/s]Epoch: 7, train for the 146-th batch, train loss: 0.5008485913276672:  99%|██████████▉| 145/146 [01:29<00:00,  1.73it/s]Epoch: 7, train for the 146-th batch, train loss: 0.5008485913276672: 100%|███████████| 146/146 [01:29<00:00,  1.89it/s]Epoch: 7, train for the 146-th batch, train loss: 0.5008485913276672: 100%|███████████| 146/146 [01:29<00:00,  1.64it/s]
  0%|                                                                                            | 0/38 [00:00<?, ?it/s]evaluate for the 40-th batch, evaluate loss: 0.47172752022743225:  85%|██████████████▍  | 39/46 [00:04<00:00,  9.63it/s]evaluate for the 40-th batch, evaluate loss: 0.47172752022743225:  87%|██████████████▊  | 40/46 [00:04<00:00,  9.62it/s]evaluate for the 41-th batch, evaluate loss: 0.4818772077560425:  87%|███████████████▋  | 40/46 [00:04<00:00,  9.62it/s]evaluate for the 41-th batch, evaluate loss: 0.4818772077560425:  89%|████████████████  | 41/46 [00:04<00:00,  9.61it/s]evaluate for the 42-th batch, evaluate loss: 0.47576406598091125:  89%|███████████████▏ | 41/46 [00:04<00:00,  9.61it/s]evaluate for the 42-th batch, evaluate loss: 0.47576406598091125:  91%|███████████████▌ | 42/46 [00:04<00:00,  9.62it/s]Epoch: 3, train for the 251-th batch, train loss: 0.48443374037742615:  65%|██████▌   | 250/383 [02:29<01:18,  1.70it/s]Epoch: 3, train for the 251-th batch, train loss: 0.48443374037742615:  66%|██████▌   | 251/383 [02:29<01:17,  1.70it/s]Epoch: 9, train for the 5-th batch, train loss: 0.34842756390571594:   3%|▍             | 4/119 [00:02<01:04,  1.78it/s]Epoch: 9, train for the 5-th batch, train loss: 0.34842756390571594:   4%|▌             | 5/119 [00:02<01:04,  1.76it/s]evaluate for the 1-th batch, evaluate loss: 0.4832230508327484:   0%|                            | 0/38 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.4832230508327484:   3%|▌                   | 1/38 [00:00<00:11,  3.20it/s]evaluate for the 43-th batch, evaluate loss: 0.5311568379402161:  91%|████████████████▍ | 42/46 [00:04<00:00,  9.62it/s]evaluate for the 43-th batch, evaluate loss: 0.5311568379402161:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.64it/s]Epoch: 5, train for the 65-th batch, train loss: 0.6472872495651245:  27%|███▌         | 64/237 [00:36<01:42,  1.69it/s]Epoch: 5, train for the 65-th batch, train loss: 0.6472872495651245:  27%|███▌         | 65/237 [00:36<01:41,  1.69it/s]evaluate for the 44-th batch, evaluate loss: 0.516255259513855:  93%|█████████████████▊ | 43/46 [00:04<00:00,  9.64it/s]evaluate for the 44-th batch, evaluate loss: 0.516255259513855:  96%|██████████████████▏| 44/46 [00:04<00:00,  9.65it/s]evaluate for the 45-th batch, evaluate loss: 0.49447521567344666:  96%|████████████████▎| 44/46 [00:04<00:00,  9.65it/s]evaluate for the 45-th batch, evaluate loss: 0.49447521567344666:  98%|████████████████▋| 45/46 [00:04<00:00,  9.67it/s]evaluate for the 2-th batch, evaluate loss: 0.49443021416664124:   3%|▌                  | 1/38 [00:00<00:11,  3.20it/s]evaluate for the 2-th batch, evaluate loss: 0.49443021416664124:   5%|█                  | 2/38 [00:00<00:10,  3.58it/s]evaluate for the 46-th batch, evaluate loss: 0.5087332725524902:  98%|█████████████████▌| 45/46 [00:04<00:00,  9.67it/s]evaluate for the 46-th batch, evaluate loss: 0.5087332725524902: 100%|██████████████████| 46/46 [00:04<00:00,  9.65it/s]
  0%|                                                                                            | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6340447664260864:   0%|                            | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6340447664260864:   4%|▊                   | 1/25 [00:00<00:02,  9.17it/s]Epoch: 3, train for the 252-th batch, train loss: 0.3797166645526886:  66%|███████▏   | 251/383 [02:30<01:17,  1.70it/s]Epoch: 3, train for the 252-th batch, train loss: 0.3797166645526886:  66%|███████▏   | 252/383 [02:30<01:17,  1.70it/s]evaluate for the 2-th batch, evaluate loss: 0.6461254954338074:   4%|▊                   | 1/25 [00:00<00:02,  9.17it/s]evaluate for the 2-th batch, evaluate loss: 0.6461254954338074:   8%|█▌                  | 2/25 [00:00<00:02,  9.15it/s]Epoch: 9, train for the 6-th batch, train loss: 0.28929001092910767:   4%|▌             | 5/119 [00:03<01:04,  1.76it/s]Epoch: 9, train for the 6-th batch, train loss: 0.28929001092910767:   5%|▋             | 6/119 [00:03<01:05,  1.72it/s]evaluate for the 3-th batch, evaluate loss: 0.49085405468940735:   5%|█                  | 2/38 [00:00<00:10,  3.58it/s]evaluate for the 3-th batch, evaluate loss: 0.49085405468940735:   8%|█▌                 | 3/38 [00:00<00:10,  3.35it/s]evaluate for the 3-th batch, evaluate loss: 0.6818038821220398:   8%|█▌                  | 2/25 [00:00<00:02,  9.15it/s]evaluate for the 3-th batch, evaluate loss: 0.6818038821220398:  12%|██▍                 | 3/25 [00:00<00:02,  9.17it/s]Epoch: 5, train for the 66-th batch, train loss: 0.6288872361183167:  27%|███▌         | 65/237 [00:36<01:41,  1.69it/s]Epoch: 5, train for the 66-th batch, train loss: 0.6288872361183167:  28%|███▌         | 66/237 [00:36<01:40,  1.70it/s]evaluate for the 4-th batch, evaluate loss: 0.6673347353935242:  12%|██▍                 | 3/25 [00:00<00:02,  9.17it/s]evaluate for the 4-th batch, evaluate loss: 0.6673347353935242:  16%|███▏                | 4/25 [00:00<00:02,  9.17it/s]evaluate for the 4-th batch, evaluate loss: 0.47733524441719055:   8%|█▌                 | 3/38 [00:01<00:10,  3.35it/s]evaluate for the 4-th batch, evaluate loss: 0.47733524441719055:  11%|██                 | 4/38 [00:01<00:09,  3.47it/s]evaluate for the 5-th batch, evaluate loss: 0.666485607624054:  16%|███▎                 | 4/25 [00:00<00:02,  9.17it/s]evaluate for the 5-th batch, evaluate loss: 0.666485607624054:  20%|████▏                | 5/25 [00:00<00:02,  9.18it/s]evaluate for the 6-th batch, evaluate loss: 0.7039615511894226:  20%|████                | 5/25 [00:00<00:02,  9.18it/s]evaluate for the 6-th batch, evaluate loss: 0.7039615511894226:  24%|████▊               | 6/25 [00:00<00:02,  9.18it/s]evaluate for the 7-th batch, evaluate loss: 0.7277511358261108:  24%|████▊               | 6/25 [00:00<00:02,  9.18it/s]evaluate for the 7-th batch, evaluate loss: 0.7277511358261108:  28%|█████▌              | 7/25 [00:00<00:01,  9.19it/s]Epoch: 3, train for the 253-th batch, train loss: 0.3813703954219818:  66%|███████▏   | 252/383 [02:30<01:17,  1.70it/s]Epoch: 3, train for the 253-th batch, train loss: 0.3813703954219818:  66%|███████▎   | 253/383 [02:30<01:16,  1.71it/s]evaluate for the 5-th batch, evaluate loss: 0.5013893246650696:  11%|██                  | 4/38 [00:01<00:09,  3.47it/s]evaluate for the 5-th batch, evaluate loss: 0.5013893246650696:  13%|██▋                 | 5/38 [00:01<00:09,  3.43it/s]Epoch: 9, train for the 7-th batch, train loss: 0.24836356937885284:   5%|▋             | 6/119 [00:03<01:05,  1.72it/s]Epoch: 9, train for the 7-th batch, train loss: 0.24836356937885284:   6%|▊             | 7/119 [00:03<01:05,  1.71it/s]evaluate for the 8-th batch, evaluate loss: 0.7089343667030334:  28%|█████▌              | 7/25 [00:00<00:01,  9.19it/s]evaluate for the 8-th batch, evaluate loss: 0.7089343667030334:  32%|██████▍             | 8/25 [00:00<00:01,  9.19it/s]Epoch: 5, train for the 67-th batch, train loss: 0.7110726237297058:  28%|███▌         | 66/237 [00:37<01:40,  1.70it/s]Epoch: 5, train for the 67-th batch, train loss: 0.7110726237297058:  28%|███▋         | 67/237 [00:37<01:40,  1.69it/s]evaluate for the 9-th batch, evaluate loss: 0.6892594695091248:  32%|██████▍             | 8/25 [00:00<00:01,  9.19it/s]evaluate for the 9-th batch, evaluate loss: 0.6892594695091248:  36%|███████▏            | 9/25 [00:00<00:01,  9.20it/s]evaluate for the 6-th batch, evaluate loss: 0.4839601516723633:  13%|██▋                 | 5/38 [00:01<00:09,  3.43it/s]evaluate for the 6-th batch, evaluate loss: 0.4839601516723633:  16%|███▏                | 6/38 [00:01<00:08,  3.59it/s]evaluate for the 10-th batch, evaluate loss: 0.7202690243721008:  36%|██████▊            | 9/25 [00:01<00:01,  9.20it/s]evaluate for the 10-th batch, evaluate loss: 0.7202690243721008:  40%|███████▏          | 10/25 [00:01<00:01,  9.23it/s]evaluate for the 11-th batch, evaluate loss: 0.7204655408859253:  40%|███████▏          | 10/25 [00:01<00:01,  9.23it/s]evaluate for the 11-th batch, evaluate loss: 0.7204655408859253:  44%|███████▉          | 11/25 [00:01<00:01,  9.25it/s]evaluate for the 12-th batch, evaluate loss: 0.6882308125495911:  44%|███████▉          | 11/25 [00:01<00:01,  9.25it/s]evaluate for the 12-th batch, evaluate loss: 0.6882308125495911:  48%|████████▋         | 12/25 [00:01<00:01,  9.24it/s]evaluate for the 7-th batch, evaluate loss: 0.4592416286468506:  16%|███▏                | 6/38 [00:02<00:08,  3.59it/s]evaluate for the 7-th batch, evaluate loss: 0.4592416286468506:  18%|███▋                | 7/38 [00:02<00:08,  3.53it/s]Epoch: 3, train for the 254-th batch, train loss: 0.47217124700546265:  66%|██████▌   | 253/383 [02:31<01:16,  1.71it/s]Epoch: 3, train for the 254-th batch, train loss: 0.47217124700546265:  66%|██████▋   | 254/383 [02:31<01:15,  1.70it/s]evaluate for the 13-th batch, evaluate loss: 0.6569851636886597:  48%|████████▋         | 12/25 [00:01<00:01,  9.24it/s]evaluate for the 13-th batch, evaluate loss: 0.6569851636886597:  52%|█████████▎        | 13/25 [00:01<00:01,  9.24it/s]Epoch: 9, train for the 8-th batch, train loss: 0.2327146977186203:   6%|▉              | 7/119 [00:04<01:05,  1.71it/s]Epoch: 9, train for the 8-th batch, train loss: 0.2327146977186203:   7%|█              | 8/119 [00:04<01:05,  1.71it/s]evaluate for the 14-th batch, evaluate loss: 0.7397862076759338:  52%|█████████▎        | 13/25 [00:01<00:01,  9.24it/s]evaluate for the 14-th batch, evaluate loss: 0.7397862076759338:  56%|██████████        | 14/25 [00:01<00:01,  9.24it/s]Epoch: 5, train for the 68-th batch, train loss: 0.6061592698097229:  28%|███▋         | 67/237 [00:38<01:40,  1.69it/s]Epoch: 5, train for the 68-th batch, train loss: 0.6061592698097229:  29%|███▋         | 68/237 [00:38<01:39,  1.69it/s]evaluate for the 8-th batch, evaluate loss: 0.5348185896873474:  18%|███▋                | 7/38 [00:02<00:08,  3.53it/s]evaluate for the 8-th batch, evaluate loss: 0.5348185896873474:  21%|████▏               | 8/38 [00:02<00:08,  3.65it/s]evaluate for the 15-th batch, evaluate loss: 0.715204656124115:  56%|██████████▋        | 14/25 [00:01<00:01,  9.24it/s]evaluate for the 15-th batch, evaluate loss: 0.715204656124115:  60%|███████████▍       | 15/25 [00:01<00:01,  9.14it/s]evaluate for the 16-th batch, evaluate loss: 0.653542697429657:  60%|███████████▍       | 15/25 [00:01<00:01,  9.14it/s]evaluate for the 16-th batch, evaluate loss: 0.653542697429657:  64%|████████████▏      | 16/25 [00:01<00:00,  9.13it/s]evaluate for the 17-th batch, evaluate loss: 0.6549745202064514:  64%|███████████▌      | 16/25 [00:01<00:00,  9.13it/s]evaluate for the 17-th batch, evaluate loss: 0.6549745202064514:  68%|████████████▏     | 17/25 [00:01<00:00,  9.15it/s]evaluate for the 9-th batch, evaluate loss: 0.501354992389679:  21%|████▍                | 8/38 [00:02<00:08,  3.65it/s]evaluate for the 9-th batch, evaluate loss: 0.501354992389679:  24%|████▉                | 9/38 [00:02<00:08,  3.59it/s]evaluate for the 18-th batch, evaluate loss: 0.6252439618110657:  68%|████████████▏     | 17/25 [00:01<00:00,  9.15it/s]evaluate for the 18-th batch, evaluate loss: 0.6252439618110657:  72%|████████████▉     | 18/25 [00:01<00:00,  9.20it/s]Epoch: 3, train for the 255-th batch, train loss: 0.3883450925350189:  66%|███████▎   | 254/383 [02:31<01:15,  1.70it/s]Epoch: 3, train for the 255-th batch, train loss: 0.3883450925350189:  67%|███████▎   | 255/383 [02:31<01:15,  1.69it/s]Epoch: 9, train for the 9-th batch, train loss: 0.2552683651447296:   7%|█              | 8/119 [00:05<01:05,  1.71it/s]Epoch: 9, train for the 9-th batch, train loss: 0.2552683651447296:   8%|█▏             | 9/119 [00:05<01:05,  1.68it/s]evaluate for the 19-th batch, evaluate loss: 0.5836681723594666:  72%|████████████▉     | 18/25 [00:02<00:00,  9.20it/s]evaluate for the 19-th batch, evaluate loss: 0.5836681723594666:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.12it/s]Epoch: 5, train for the 69-th batch, train loss: 0.5300990343093872:  29%|███▋         | 68/237 [00:38<01:39,  1.69it/s]Epoch: 5, train for the 69-th batch, train loss: 0.5300990343093872:  29%|███▊         | 69/237 [00:38<01:38,  1.70it/s]evaluate for the 20-th batch, evaluate loss: 0.6472801566123962:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.12it/s]evaluate for the 20-th batch, evaluate loss: 0.6472801566123962:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.15it/s]evaluate for the 10-th batch, evaluate loss: 0.5195751190185547:  24%|████▌              | 9/38 [00:02<00:08,  3.59it/s]evaluate for the 10-th batch, evaluate loss: 0.5195751190185547:  26%|████▋             | 10/38 [00:02<00:07,  3.60it/s]evaluate for the 21-th batch, evaluate loss: 0.7155959010124207:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.15it/s]evaluate for the 21-th batch, evaluate loss: 0.7155959010124207:  84%|███████████████   | 21/25 [00:02<00:00,  9.16it/s]evaluate for the 22-th batch, evaluate loss: 0.5982273817062378:  84%|███████████████   | 21/25 [00:02<00:00,  9.16it/s]evaluate for the 22-th batch, evaluate loss: 0.5982273817062378:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.14it/s]evaluate for the 11-th batch, evaluate loss: 0.5012620687484741:  26%|████▋             | 10/38 [00:03<00:07,  3.60it/s]evaluate for the 11-th batch, evaluate loss: 0.5012620687484741:  29%|█████▏            | 11/38 [00:03<00:07,  3.63it/s]evaluate for the 23-th batch, evaluate loss: 0.6554762125015259:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.14it/s]evaluate for the 23-th batch, evaluate loss: 0.6554762125015259:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.14it/s]Epoch: 3, train for the 256-th batch, train loss: 0.4210437536239624:  67%|███████▎   | 255/383 [02:32<01:15,  1.69it/s]Epoch: 3, train for the 256-th batch, train loss: 0.4210437536239624:  67%|███████▎   | 256/383 [02:32<01:14,  1.70it/s]evaluate for the 24-th batch, evaluate loss: 0.6514286398887634:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.14it/s]evaluate for the 24-th batch, evaluate loss: 0.6514286398887634:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.13it/s]Epoch: 9, train for the 10-th batch, train loss: 0.2618228793144226:   8%|█             | 9/119 [00:05<01:05,  1.68it/s]Epoch: 9, train for the 10-th batch, train loss: 0.2618228793144226:   8%|█            | 10/119 [00:05<01:05,  1.67it/s]Epoch: 5, train for the 70-th batch, train loss: 0.5567411780357361:  29%|███▊         | 69/237 [00:39<01:38,  1.70it/s]Epoch: 5, train for the 70-th batch, train loss: 0.5567411780357361:  30%|███▊         | 70/237 [00:39<01:37,  1.71it/s]evaluate for the 25-th batch, evaluate loss: 0.6988682150840759:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.13it/s]evaluate for the 25-th batch, evaluate loss: 0.6988682150840759: 100%|██████████████████| 25/25 [00:02<00:00,  9.24it/s]
INFO:root:Epoch: 14, learning rate: 0.0001, train loss: 0.5696
INFO:root:train average_precision, 0.8208
INFO:root:train roc_auc, 0.7853
INFO:root:validate loss: 0.5092
INFO:root:validate average_precision, 0.8427
INFO:root:validate roc_auc, 0.8044
INFO:root:new node validate loss: 0.6740
INFO:root:new node validate first_1_average_precision, 0.5979
INFO:root:new node validate first_1_roc_auc, 0.5445
INFO:root:new node validate first_3_average_precision, 0.6827
INFO:root:new node validate first_3_roc_auc, 0.6407
INFO:root:new node validate first_10_average_precision, 0.7483
INFO:root:new node validate first_10_roc_auc, 0.7130
INFO:root:new node validate average_precision, 0.7107
INFO:root:new node validate roc_auc, 0.6592
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]evaluate for the 12-th batch, evaluate loss: 0.5489369034767151:  29%|█████▏            | 11/38 [00:03<00:07,  3.63it/s]evaluate for the 12-th batch, evaluate loss: 0.5489369034767151:  32%|█████▋            | 12/38 [00:03<00:07,  3.54it/s]Epoch: 15, train for the 1-th batch, train loss: 0.8296452164649963:   0%|                      | 0/151 [00:00<?, ?it/s]Epoch: 15, train for the 1-th batch, train loss: 0.8296452164649963:   1%|              | 1/151 [00:00<00:26,  5.77it/s]evaluate for the 13-th batch, evaluate loss: 0.5432572960853577:  32%|█████▋            | 12/38 [00:03<00:07,  3.54it/s]evaluate for the 13-th batch, evaluate loss: 0.5432572960853577:  34%|██████▏           | 13/38 [00:03<00:06,  3.68it/s]Epoch: 15, train for the 2-th batch, train loss: 0.8738396763801575:   1%|              | 1/151 [00:00<00:26,  5.77it/s]Epoch: 15, train for the 2-th batch, train loss: 0.8738396763801575:   1%|▏             | 2/151 [00:00<00:26,  5.68it/s]Epoch: 3, train for the 257-th batch, train loss: 0.4754781126976013:  67%|███████▎   | 256/383 [02:33<01:14,  1.70it/s]Epoch: 3, train for the 257-th batch, train loss: 0.4754781126976013:  67%|███████▍   | 257/383 [02:33<01:13,  1.71it/s]Epoch: 15, train for the 3-th batch, train loss: 0.6437398195266724:   1%|▏             | 2/151 [00:00<00:26,  5.68it/s]Epoch: 15, train for the 3-th batch, train loss: 0.6437398195266724:   2%|▎             | 3/151 [00:00<00:25,  5.82it/s]Epoch: 9, train for the 11-th batch, train loss: 0.22804443538188934:   8%|█           | 10/119 [00:06<01:05,  1.67it/s]Epoch: 9, train for the 11-th batch, train loss: 0.22804443538188934:   9%|█           | 11/119 [00:06<01:04,  1.68it/s]Epoch: 5, train for the 71-th batch, train loss: 0.6139973998069763:  30%|███▊         | 70/237 [00:39<01:37,  1.71it/s]Epoch: 5, train for the 71-th batch, train loss: 0.6139973998069763:  30%|███▉         | 71/237 [00:39<01:37,  1.70it/s]evaluate for the 14-th batch, evaluate loss: 0.4837420582771301:  34%|██████▏           | 13/38 [00:03<00:06,  3.68it/s]evaluate for the 14-th batch, evaluate loss: 0.4837420582771301:  37%|██████▋           | 14/38 [00:03<00:06,  3.55it/s]Epoch: 15, train for the 4-th batch, train loss: 0.5661183595657349:   2%|▎             | 3/151 [00:00<00:25,  5.82it/s]Epoch: 15, train for the 4-th batch, train loss: 0.5661183595657349:   3%|▎             | 4/151 [00:00<00:25,  5.84it/s]evaluate for the 15-th batch, evaluate loss: 0.505742073059082:  37%|███████            | 14/38 [00:04<00:06,  3.55it/s]evaluate for the 15-th batch, evaluate loss: 0.505742073059082:  39%|███████▌           | 15/38 [00:04<00:05,  4.19it/s]Epoch: 15, train for the 5-th batch, train loss: 0.4943379759788513:   3%|▎             | 4/151 [00:00<00:25,  5.84it/s]Epoch: 15, train for the 5-th batch, train loss: 0.4943379759788513:   3%|▍             | 5/151 [00:00<00:25,  5.64it/s]evaluate for the 16-th batch, evaluate loss: 0.53090500831604:  39%|███████▉            | 15/38 [00:04<00:05,  4.19it/s]evaluate for the 16-th batch, evaluate loss: 0.53090500831604:  42%|████████▍           | 16/38 [00:04<00:05,  4.20it/s]Epoch: 3, train for the 258-th batch, train loss: 0.40409407019615173:  67%|██████▋   | 257/383 [02:33<01:13,  1.71it/s]Epoch: 3, train for the 258-th batch, train loss: 0.40409407019615173:  67%|██████▋   | 258/383 [02:33<01:13,  1.70it/s]Epoch: 15, train for the 6-th batch, train loss: 0.607425332069397:   3%|▍              | 5/151 [00:01<00:25,  5.64it/s]Epoch: 15, train for the 6-th batch, train loss: 0.607425332069397:   4%|▌              | 6/151 [00:01<00:26,  5.49it/s]Epoch: 5, train for the 72-th batch, train loss: 0.6272701025009155:  30%|███▉         | 71/237 [00:40<01:37,  1.70it/s]Epoch: 5, train for the 72-th batch, train loss: 0.6272701025009155:  30%|███▉         | 72/237 [00:40<01:37,  1.69it/s]Epoch: 9, train for the 12-th batch, train loss: 0.2839268743991852:   9%|█▏           | 11/119 [00:07<01:04,  1.68it/s]Epoch: 9, train for the 12-th batch, train loss: 0.2839268743991852:  10%|█▎           | 12/119 [00:07<01:06,  1.60it/s]Epoch: 15, train for the 7-th batch, train loss: 0.4753829538822174:   4%|▌             | 6/151 [00:01<00:26,  5.49it/s]Epoch: 15, train for the 7-th batch, train loss: 0.4753829538822174:   5%|▋             | 7/151 [00:01<00:26,  5.37it/s]evaluate for the 17-th batch, evaluate loss: 0.4926872253417969:  42%|███████▌          | 16/38 [00:04<00:05,  4.20it/s]evaluate for the 17-th batch, evaluate loss: 0.4926872253417969:  45%|████████          | 17/38 [00:04<00:05,  3.88it/s]Epoch: 15, train for the 8-th batch, train loss: 0.6370025277137756:   5%|▋             | 7/151 [00:01<00:26,  5.37it/s]Epoch: 15, train for the 8-th batch, train loss: 0.6370025277137756:   5%|▋             | 8/151 [00:01<00:26,  5.34it/s]evaluate for the 18-th batch, evaluate loss: 0.533708393573761:  45%|████████▌          | 17/38 [00:04<00:05,  3.88it/s]evaluate for the 18-th batch, evaluate loss: 0.533708393573761:  47%|█████████          | 18/38 [00:04<00:05,  3.96it/s]Epoch: 5, train for the 73-th batch, train loss: 0.6343751549720764:  30%|███▉         | 72/237 [00:40<01:37,  1.69it/s]Epoch: 5, train for the 73-th batch, train loss: 0.6343751549720764:  31%|████         | 73/237 [00:40<01:29,  1.83it/s]Epoch: 15, train for the 9-th batch, train loss: 0.7196005582809448:   5%|▋             | 8/151 [00:01<00:26,  5.34it/s]Epoch: 15, train for the 9-th batch, train loss: 0.7196005582809448:   6%|▊             | 9/151 [00:01<00:27,  5.14it/s]Epoch: 3, train for the 259-th batch, train loss: 0.4055940806865692:  67%|███████▍   | 258/383 [02:34<01:13,  1.70it/s]Epoch: 3, train for the 259-th batch, train loss: 0.4055940806865692:  68%|███████▍   | 259/383 [02:34<01:18,  1.59it/s]Epoch: 9, train for the 13-th batch, train loss: 0.22345387935638428:  10%|█▏          | 12/119 [00:07<01:06,  1.60it/s]Epoch: 9, train for the 13-th batch, train loss: 0.22345387935638428:  11%|█▎          | 13/119 [00:07<01:05,  1.62it/s]evaluate for the 19-th batch, evaluate loss: 0.5006471276283264:  47%|████████▌         | 18/38 [00:05<00:05,  3.96it/s]evaluate for the 19-th batch, evaluate loss: 0.5006471276283264:  50%|█████████         | 19/38 [00:05<00:05,  3.69it/s]Epoch: 15, train for the 10-th batch, train loss: 0.6411991715431213:   6%|▊            | 9/151 [00:01<00:27,  5.14it/s]Epoch: 15, train for the 10-th batch, train loss: 0.6411991715431213:   7%|▊           | 10/151 [00:01<00:27,  5.04it/s]Epoch: 15, train for the 11-th batch, train loss: 0.5234218239784241:   7%|▊           | 10/151 [00:02<00:27,  5.04it/s]Epoch: 15, train for the 11-th batch, train loss: 0.5234218239784241:   7%|▊           | 11/151 [00:02<00:27,  5.00it/s]evaluate for the 20-th batch, evaluate loss: 0.48664534091949463:  50%|████████▌        | 19/38 [00:05<00:05,  3.69it/s]evaluate for the 20-th batch, evaluate loss: 0.48664534091949463:  53%|████████▉        | 20/38 [00:05<00:04,  3.72it/s]Epoch: 5, train for the 74-th batch, train loss: 0.6499789357185364:  31%|████         | 73/237 [00:41<01:29,  1.83it/s]Epoch: 5, train for the 74-th batch, train loss: 0.6499789357185364:  31%|████         | 74/237 [00:41<01:30,  1.80it/s]Epoch: 15, train for the 12-th batch, train loss: 0.4741361439228058:   7%|▊           | 11/151 [00:02<00:27,  5.00it/s]Epoch: 15, train for the 12-th batch, train loss: 0.4741361439228058:   8%|▉           | 12/151 [00:02<00:27,  5.00it/s]Epoch: 3, train for the 260-th batch, train loss: 0.3922010064125061:  68%|███████▍   | 259/383 [02:34<01:18,  1.59it/s]Epoch: 3, train for the 260-th batch, train loss: 0.3922010064125061:  68%|███████▍   | 260/383 [02:34<01:16,  1.61it/s]evaluate for the 21-th batch, evaluate loss: 0.4946707785129547:  53%|█████████▍        | 20/38 [00:05<00:04,  3.72it/s]evaluate for the 21-th batch, evaluate loss: 0.4946707785129547:  55%|█████████▉        | 21/38 [00:05<00:04,  3.59it/s]Epoch: 9, train for the 14-th batch, train loss: 0.2270871251821518:  11%|█▍           | 13/119 [00:08<01:05,  1.62it/s]Epoch: 9, train for the 14-th batch, train loss: 0.2270871251821518:  12%|█▌           | 14/119 [00:08<01:04,  1.63it/s]Epoch: 15, train for the 13-th batch, train loss: 0.5917331576347351:   8%|▉           | 12/151 [00:02<00:27,  5.00it/s]Epoch: 15, train for the 13-th batch, train loss: 0.5917331576347351:   9%|█           | 13/151 [00:02<00:27,  4.97it/s]evaluate for the 22-th batch, evaluate loss: 0.5185511112213135:  55%|█████████▉        | 21/38 [00:05<00:04,  3.59it/s]evaluate for the 22-th batch, evaluate loss: 0.5185511112213135:  58%|██████████▍       | 22/38 [00:05<00:04,  3.68it/s]Epoch: 15, train for the 14-th batch, train loss: 0.6721087694168091:   9%|█           | 13/151 [00:02<00:27,  4.97it/s]Epoch: 15, train for the 14-th batch, train loss: 0.6721087694168091:   9%|█           | 14/151 [00:02<00:28,  4.88it/s]Epoch: 5, train for the 75-th batch, train loss: 0.6111325621604919:  31%|████         | 74/237 [00:41<01:30,  1.80it/s]Epoch: 5, train for the 75-th batch, train loss: 0.6111325621604919:  32%|████         | 75/237 [00:41<01:32,  1.75it/s]Epoch: 15, train for the 15-th batch, train loss: 0.5500932931900024:   9%|█           | 14/151 [00:02<00:28,  4.88it/s]Epoch: 15, train for the 15-th batch, train loss: 0.5500932931900024:  10%|█▏          | 15/151 [00:02<00:28,  4.85it/s]evaluate for the 23-th batch, evaluate loss: 0.5202286243438721:  58%|██████████▍       | 22/38 [00:06<00:04,  3.68it/s]evaluate for the 23-th batch, evaluate loss: 0.5202286243438721:  61%|██████████▉       | 23/38 [00:06<00:04,  3.60it/s]Epoch: 3, train for the 261-th batch, train loss: 0.41537904739379883:  68%|██████▊   | 260/383 [02:35<01:16,  1.61it/s]Epoch: 3, train for the 261-th batch, train loss: 0.41537904739379883:  68%|██████▊   | 261/383 [02:35<01:14,  1.63it/s]Epoch: 9, train for the 15-th batch, train loss: 0.20388515293598175:  12%|█▍          | 14/119 [00:08<01:04,  1.63it/s]Epoch: 9, train for the 15-th batch, train loss: 0.20388515293598175:  13%|█▌          | 15/119 [00:08<01:03,  1.65it/s]Epoch: 15, train for the 16-th batch, train loss: 0.5866067409515381:  10%|█▏          | 15/151 [00:03<00:28,  4.85it/s]Epoch: 15, train for the 16-th batch, train loss: 0.5866067409515381:  11%|█▎          | 16/151 [00:03<00:27,  4.86it/s]evaluate for the 24-th batch, evaluate loss: 0.49264654517173767:  61%|██████████▎      | 23/38 [00:06<00:04,  3.60it/s]evaluate for the 24-th batch, evaluate loss: 0.49264654517173767:  63%|██████████▋      | 24/38 [00:06<00:03,  3.67it/s]Epoch: 15, train for the 17-th batch, train loss: 0.5255715847015381:  11%|█▎          | 16/151 [00:03<00:27,  4.86it/s]Epoch: 15, train for the 17-th batch, train loss: 0.5255715847015381:  11%|█▎          | 17/151 [00:03<00:27,  4.93it/s]Epoch: 5, train for the 76-th batch, train loss: 0.6237828135490417:  32%|████         | 75/237 [00:42<01:32,  1.75it/s]Epoch: 5, train for the 76-th batch, train loss: 0.6237828135490417:  32%|████▏        | 76/237 [00:42<01:33,  1.72it/s]evaluate for the 25-th batch, evaluate loss: 0.5046883225440979:  63%|███████████▎      | 24/38 [00:06<00:03,  3.67it/s]evaluate for the 25-th batch, evaluate loss: 0.5046883225440979:  66%|███████████▊      | 25/38 [00:06<00:03,  3.60it/s]Epoch: 15, train for the 18-th batch, train loss: 0.5719699263572693:  11%|█▎          | 17/151 [00:03<00:27,  4.93it/s]Epoch: 15, train for the 18-th batch, train loss: 0.5719699263572693:  12%|█▍          | 18/151 [00:03<00:27,  4.93it/s]Epoch: 3, train for the 262-th batch, train loss: 0.4157305955886841:  68%|███████▍   | 261/383 [02:36<01:14,  1.63it/s]Epoch: 3, train for the 262-th batch, train loss: 0.4157305955886841:  68%|███████▌   | 262/383 [02:36<01:14,  1.63it/s]Epoch: 9, train for the 16-th batch, train loss: 0.22862233221530914:  13%|█▌          | 15/119 [00:09<01:03,  1.65it/s]Epoch: 9, train for the 16-th batch, train loss: 0.22862233221530914:  13%|█▌          | 16/119 [00:09<01:02,  1.65it/s]Epoch: 15, train for the 19-th batch, train loss: 0.46897321939468384:  12%|█▎         | 18/151 [00:03<00:27,  4.93it/s]Epoch: 15, train for the 19-th batch, train loss: 0.46897321939468384:  13%|█▍         | 19/151 [00:03<00:26,  4.95it/s]evaluate for the 26-th batch, evaluate loss: 0.5115747451782227:  66%|███████████▊      | 25/38 [00:07<00:03,  3.60it/s]evaluate for the 26-th batch, evaluate loss: 0.5115747451782227:  68%|████████████▎     | 26/38 [00:07<00:03,  3.61it/s]Epoch: 15, train for the 20-th batch, train loss: 0.5160112380981445:  13%|█▌          | 19/151 [00:03<00:26,  4.95it/s]Epoch: 15, train for the 20-th batch, train loss: 0.5160112380981445:  13%|█▌          | 20/151 [00:03<00:26,  4.90it/s]Epoch: 5, train for the 77-th batch, train loss: 0.5950813889503479:  32%|████▏        | 76/237 [00:43<01:33,  1.72it/s]Epoch: 5, train for the 77-th batch, train loss: 0.5950813889503479:  32%|████▏        | 77/237 [00:43<01:33,  1.72it/s]evaluate for the 27-th batch, evaluate loss: 0.5302996635437012:  68%|████████████▎     | 26/38 [00:07<00:03,  3.61it/s]evaluate for the 27-th batch, evaluate loss: 0.5302996635437012:  71%|████████████▊     | 27/38 [00:07<00:03,  3.62it/s]Epoch: 15, train for the 21-th batch, train loss: 0.5182982683181763:  13%|█▌          | 20/151 [00:04<00:26,  4.90it/s]Epoch: 15, train for the 21-th batch, train loss: 0.5182982683181763:  14%|█▋          | 21/151 [00:04<00:27,  4.79it/s]Epoch: 3, train for the 263-th batch, train loss: 0.44748878479003906:  68%|██████▊   | 262/383 [02:36<01:14,  1.63it/s]Epoch: 3, train for the 263-th batch, train loss: 0.44748878479003906:  69%|██████▊   | 263/383 [02:36<01:12,  1.65it/s]Epoch: 9, train for the 17-th batch, train loss: 0.2111041247844696:  13%|█▋           | 16/119 [00:10<01:02,  1.65it/s]Epoch: 9, train for the 17-th batch, train loss: 0.2111041247844696:  14%|█▊           | 17/119 [00:10<01:01,  1.65it/s]evaluate for the 28-th batch, evaluate loss: 0.5371707677841187:  71%|████████████▊     | 27/38 [00:07<00:03,  3.62it/s]evaluate for the 28-th batch, evaluate loss: 0.5371707677841187:  74%|█████████████▎    | 28/38 [00:07<00:02,  3.53it/s]Epoch: 15, train for the 22-th batch, train loss: 0.4693661332130432:  14%|█▋          | 21/151 [00:04<00:27,  4.79it/s]Epoch: 15, train for the 22-th batch, train loss: 0.4693661332130432:  15%|█▋          | 22/151 [00:04<00:27,  4.77it/s]Epoch: 15, train for the 23-th batch, train loss: 0.5028294324874878:  15%|█▋          | 22/151 [00:04<00:27,  4.77it/s]Epoch: 15, train for the 23-th batch, train loss: 0.5028294324874878:  15%|█▊          | 23/151 [00:04<00:26,  4.75it/s]Epoch: 5, train for the 78-th batch, train loss: 0.5663885474205017:  32%|████▏        | 77/237 [00:43<01:33,  1.72it/s]Epoch: 5, train for the 78-th batch, train loss: 0.5663885474205017:  33%|████▎        | 78/237 [00:43<01:33,  1.70it/s]evaluate for the 29-th batch, evaluate loss: 0.5130104422569275:  74%|█████████████▎    | 28/38 [00:07<00:02,  3.53it/s]evaluate for the 29-th batch, evaluate loss: 0.5130104422569275:  76%|█████████████▋    | 29/38 [00:07<00:02,  3.68it/s]Epoch: 3, train for the 264-th batch, train loss: 0.3612712323665619:  69%|███████▌   | 263/383 [02:37<01:12,  1.65it/s]Epoch: 3, train for the 264-th batch, train loss: 0.3612712323665619:  69%|███████▌   | 264/383 [02:37<01:11,  1.65it/s]Epoch: 15, train for the 24-th batch, train loss: 0.5446758270263672:  15%|█▊          | 23/151 [00:04<00:26,  4.75it/s]Epoch: 15, train for the 24-th batch, train loss: 0.5446758270263672:  16%|█▉          | 24/151 [00:04<00:26,  4.72it/s]Epoch: 9, train for the 18-th batch, train loss: 0.23516498506069183:  14%|█▋          | 17/119 [00:10<01:01,  1.65it/s]Epoch: 9, train for the 18-th batch, train loss: 0.23516498506069183:  15%|█▊          | 18/119 [00:10<01:00,  1.66it/s]evaluate for the 30-th batch, evaluate loss: 0.5393249988555908:  76%|█████████████▋    | 29/38 [00:08<00:02,  3.68it/s]evaluate for the 30-th batch, evaluate loss: 0.5393249988555908:  79%|██████████████▏   | 30/38 [00:08<00:02,  3.55it/s]Epoch: 15, train for the 25-th batch, train loss: 0.7918230891227722:  16%|█▉          | 24/151 [00:04<00:26,  4.72it/s]Epoch: 15, train for the 25-th batch, train loss: 0.7918230891227722:  17%|█▉          | 25/151 [00:04<00:27,  4.64it/s]evaluate for the 31-th batch, evaluate loss: 0.5323217511177063:  79%|██████████████▏   | 30/38 [00:08<00:02,  3.55it/s]evaluate for the 31-th batch, evaluate loss: 0.5323217511177063:  82%|██████████████▋   | 31/38 [00:08<00:01,  3.72it/s]Epoch: 5, train for the 79-th batch, train loss: 0.6215941309928894:  33%|████▎        | 78/237 [00:44<01:33,  1.70it/s]Epoch: 5, train for the 79-th batch, train loss: 0.6215941309928894:  33%|████▎        | 79/237 [00:44<01:34,  1.67it/s]Epoch: 15, train for the 26-th batch, train loss: 0.5899936556816101:  17%|█▉          | 25/151 [00:05<00:27,  4.64it/s]Epoch: 15, train for the 26-th batch, train loss: 0.5899936556816101:  17%|██          | 26/151 [00:05<00:26,  4.65it/s]Epoch: 3, train for the 265-th batch, train loss: 0.4120405912399292:  69%|███████▌   | 264/383 [02:37<01:11,  1.65it/s]Epoch: 3, train for the 265-th batch, train loss: 0.4120405912399292:  69%|███████▌   | 265/383 [02:37<01:11,  1.64it/s]Epoch: 15, train for the 27-th batch, train loss: 0.6240513920783997:  17%|██          | 26/151 [00:05<00:26,  4.65it/s]Epoch: 15, train for the 27-th batch, train loss: 0.6240513920783997:  18%|██▏         | 27/151 [00:05<00:26,  4.64it/s]Epoch: 9, train for the 19-th batch, train loss: 0.19326083362102509:  15%|█▊          | 18/119 [00:11<01:00,  1.66it/s]Epoch: 9, train for the 19-th batch, train loss: 0.19326083362102509:  16%|█▉          | 19/119 [00:11<01:00,  1.65it/s]evaluate for the 32-th batch, evaluate loss: 0.4991811215877533:  82%|██████████████▋   | 31/38 [00:08<00:01,  3.72it/s]evaluate for the 32-th batch, evaluate loss: 0.4991811215877533:  84%|███████████████▏  | 32/38 [00:08<00:01,  3.52it/s]Epoch: 15, train for the 28-th batch, train loss: 0.2969949543476105:  18%|██▏         | 27/151 [00:05<00:26,  4.64it/s]Epoch: 15, train for the 28-th batch, train loss: 0.2969949543476105:  19%|██▏         | 28/151 [00:05<00:26,  4.71it/s]evaluate for the 33-th batch, evaluate loss: 0.4891093969345093:  84%|███████████████▏  | 32/38 [00:09<00:01,  3.52it/s]evaluate for the 33-th batch, evaluate loss: 0.4891093969345093:  87%|███████████████▋  | 33/38 [00:09<00:01,  3.58it/s]Epoch: 5, train for the 80-th batch, train loss: 0.5833102464675903:  33%|████▎        | 79/237 [00:45<01:34,  1.67it/s]Epoch: 5, train for the 80-th batch, train loss: 0.5833102464675903:  34%|████▍        | 80/237 [00:45<01:34,  1.66it/s]Epoch: 15, train for the 29-th batch, train loss: 0.642811119556427:  19%|██▍          | 28/151 [00:05<00:26,  4.71it/s]Epoch: 15, train for the 29-th batch, train loss: 0.642811119556427:  19%|██▍          | 29/151 [00:05<00:26,  4.68it/s]Epoch: 3, train for the 266-th batch, train loss: 0.5205451250076294:  69%|███████▌   | 265/383 [02:38<01:11,  1.64it/s]Epoch: 3, train for the 266-th batch, train loss: 0.5205451250076294:  69%|███████▋   | 266/383 [02:38<01:11,  1.64it/s]evaluate for the 34-th batch, evaluate loss: 0.5132716298103333:  87%|███████████████▋  | 33/38 [00:09<00:01,  3.58it/s]evaluate for the 34-th batch, evaluate loss: 0.5132716298103333:  89%|████████████████  | 34/38 [00:09<00:01,  3.50it/s]Epoch: 9, train for the 20-th batch, train loss: 0.2437552809715271:  16%|██           | 19/119 [00:11<01:00,  1.65it/s]Epoch: 9, train for the 20-th batch, train loss: 0.2437552809715271:  17%|██▏          | 20/119 [00:11<00:59,  1.65it/s]Epoch: 15, train for the 30-th batch, train loss: 0.7237867712974548:  19%|██▎         | 29/151 [00:06<00:26,  4.68it/s]Epoch: 15, train for the 30-th batch, train loss: 0.7237867712974548:  20%|██▍         | 30/151 [00:06<00:26,  4.63it/s]evaluate for the 35-th batch, evaluate loss: 0.5442579388618469:  89%|████████████████  | 34/38 [00:09<00:01,  3.50it/s]evaluate for the 35-th batch, evaluate loss: 0.5442579388618469:  92%|████████████████▌ | 35/38 [00:09<00:00,  3.62it/s]Epoch: 15, train for the 31-th batch, train loss: 0.6818745136260986:  20%|██▍         | 30/151 [00:06<00:26,  4.63it/s]Epoch: 15, train for the 31-th batch, train loss: 0.6818745136260986:  21%|██▍         | 31/151 [00:06<00:26,  4.60it/s]Epoch: 5, train for the 81-th batch, train loss: 0.581117570400238:  34%|████▋         | 80/237 [00:45<01:34,  1.66it/s]Epoch: 5, train for the 81-th batch, train loss: 0.581117570400238:  34%|████▊         | 81/237 [00:45<01:33,  1.67it/s]Epoch: 15, train for the 32-th batch, train loss: 0.6560254693031311:  21%|██▍         | 31/151 [00:06<00:26,  4.60it/s]Epoch: 15, train for the 32-th batch, train loss: 0.6560254693031311:  21%|██▌         | 32/151 [00:06<00:26,  4.57it/s]evaluate for the 36-th batch, evaluate loss: 0.5588496327400208:  92%|████████████████▌ | 35/38 [00:09<00:00,  3.62it/s]evaluate for the 36-th batch, evaluate loss: 0.5588496327400208:  95%|█████████████████ | 36/38 [00:09<00:00,  3.55it/s]Epoch: 3, train for the 267-th batch, train loss: 0.38383591175079346:  69%|██████▉   | 266/383 [02:39<01:11,  1.64it/s]Epoch: 3, train for the 267-th batch, train loss: 0.38383591175079346:  70%|██████▉   | 267/383 [02:39<01:10,  1.65it/s]Epoch: 9, train for the 21-th batch, train loss: 0.27564480900764465:  17%|██          | 20/119 [00:12<00:59,  1.65it/s]Epoch: 9, train for the 21-th batch, train loss: 0.27564480900764465:  18%|██          | 21/119 [00:12<00:59,  1.66it/s]Epoch: 15, train for the 33-th batch, train loss: 0.6360583901405334:  21%|██▌         | 32/151 [00:06<00:26,  4.57it/s]Epoch: 15, train for the 33-th batch, train loss: 0.6360583901405334:  22%|██▌         | 33/151 [00:06<00:25,  4.55it/s]evaluate for the 37-th batch, evaluate loss: 0.5001514554023743:  95%|█████████████████ | 36/38 [00:10<00:00,  3.55it/s]evaluate for the 37-th batch, evaluate loss: 0.5001514554023743:  97%|█████████████████▌| 37/38 [00:10<00:00,  3.65it/s]Epoch: 15, train for the 34-th batch, train loss: 0.43033164739608765:  22%|██▍        | 33/151 [00:06<00:25,  4.55it/s]Epoch: 15, train for the 34-th batch, train loss: 0.43033164739608765:  23%|██▍        | 34/151 [00:06<00:25,  4.64it/s]Epoch: 5, train for the 82-th batch, train loss: 0.5923693180084229:  34%|████▍        | 81/237 [00:46<01:33,  1.67it/s]Epoch: 5, train for the 82-th batch, train loss: 0.5923693180084229:  35%|████▍        | 82/237 [00:46<01:32,  1.67it/s]evaluate for the 38-th batch, evaluate loss: 0.50492924451828:  97%|███████████████████▍| 37/38 [00:10<00:00,  3.65it/s]evaluate for the 38-th batch, evaluate loss: 0.50492924451828: 100%|████████████████████| 38/38 [00:10<00:00,  3.62it/s]evaluate for the 38-th batch, evaluate loss: 0.50492924451828: 100%|████████████████████| 38/38 [00:10<00:00,  3.64it/s]
  0%|                                                                                            | 0/20 [00:00<?, ?it/s]Epoch: 15, train for the 35-th batch, train loss: 0.6612588167190552:  23%|██▋         | 34/151 [00:07<00:25,  4.64it/s]Epoch: 15, train for the 35-th batch, train loss: 0.6612588167190552:  23%|██▊         | 35/151 [00:07<00:25,  4.61it/s]Epoch: 3, train for the 268-th batch, train loss: 0.44363510608673096:  70%|██████▉   | 267/383 [02:39<01:10,  1.65it/s]Epoch: 3, train for the 268-th batch, train loss: 0.44363510608673096:  70%|██████▉   | 268/383 [02:39<01:09,  1.65it/s]Epoch: 9, train for the 22-th batch, train loss: 0.26306551694869995:  18%|██          | 21/119 [00:13<00:59,  1.66it/s]Epoch: 9, train for the 22-th batch, train loss: 0.26306551694869995:  18%|██▏         | 22/119 [00:13<00:58,  1.65it/s]Epoch: 15, train for the 36-th batch, train loss: 0.5804311633110046:  23%|██▊         | 35/151 [00:07<00:25,  4.61it/s]Epoch: 15, train for the 36-th batch, train loss: 0.5804311633110046:  24%|██▊         | 36/151 [00:07<00:24,  4.64it/s]evaluate for the 1-th batch, evaluate loss: 0.5663525462150574:   0%|                            | 0/20 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.5663525462150574:   5%|█                   | 1/20 [00:00<00:05,  3.40it/s]Epoch: 15, train for the 37-th batch, train loss: 0.5594445466995239:  24%|██▊         | 36/151 [00:07<00:24,  4.64it/s]Epoch: 15, train for the 37-th batch, train loss: 0.5594445466995239:  25%|██▉         | 37/151 [00:07<00:24,  4.69it/s]Epoch: 5, train for the 83-th batch, train loss: 0.6178095936775208:  35%|████▍        | 82/237 [00:46<01:32,  1.67it/s]Epoch: 5, train for the 83-th batch, train loss: 0.6178095936775208:  35%|████▌        | 83/237 [00:46<01:33,  1.65it/s]evaluate for the 2-th batch, evaluate loss: 0.6161164045333862:   5%|█                   | 1/20 [00:00<00:05,  3.40it/s]evaluate for the 2-th batch, evaluate loss: 0.6161164045333862:  10%|██                  | 2/20 [00:00<00:05,  3.38it/s]Epoch: 3, train for the 269-th batch, train loss: 0.4924493432044983:  70%|███████▋   | 268/383 [02:40<01:09,  1.65it/s]Epoch: 3, train for the 269-th batch, train loss: 0.4924493432044983:  70%|███████▋   | 269/383 [02:40<01:09,  1.64it/s]Epoch: 15, train for the 38-th batch, train loss: 0.6710003018379211:  25%|██▉         | 37/151 [00:07<00:24,  4.69it/s]Epoch: 15, train for the 38-th batch, train loss: 0.6710003018379211:  25%|███         | 38/151 [00:07<00:24,  4.63it/s]Epoch: 9, train for the 23-th batch, train loss: 0.24652525782585144:  18%|██▏         | 22/119 [00:13<00:58,  1.65it/s]Epoch: 9, train for the 23-th batch, train loss: 0.24652525782585144:  19%|██▎         | 23/119 [00:13<00:58,  1.65it/s]evaluate for the 3-th batch, evaluate loss: 0.5836552977561951:  10%|██                  | 2/20 [00:00<00:05,  3.38it/s]evaluate for the 3-th batch, evaluate loss: 0.5836552977561951:  15%|███                 | 3/20 [00:00<00:05,  3.35it/s]Epoch: 15, train for the 39-th batch, train loss: 0.5745521187782288:  25%|███         | 38/151 [00:08<00:24,  4.63it/s]Epoch: 15, train for the 39-th batch, train loss: 0.5745521187782288:  26%|███         | 39/151 [00:08<00:24,  4.66it/s]Epoch: 5, train for the 84-th batch, train loss: 0.6115995645523071:  35%|████▌        | 83/237 [00:47<01:33,  1.65it/s]Epoch: 5, train for the 84-th batch, train loss: 0.6115995645523071:  35%|████▌        | 84/237 [00:47<01:33,  1.64it/s]evaluate for the 4-th batch, evaluate loss: 0.5678120851516724:  15%|███                 | 3/20 [00:01<00:05,  3.35it/s]evaluate for the 4-th batch, evaluate loss: 0.5678120851516724:  20%|████                | 4/20 [00:01<00:04,  3.44it/s]Epoch: 15, train for the 40-th batch, train loss: 0.5245229005813599:  26%|███         | 39/151 [00:08<00:24,  4.66it/s]Epoch: 15, train for the 40-th batch, train loss: 0.5245229005813599:  26%|███▏        | 40/151 [00:08<00:26,  4.20it/s]Epoch: 3, train for the 270-th batch, train loss: 0.3945861756801605:  70%|███████▋   | 269/383 [02:40<01:09,  1.64it/s]Epoch: 3, train for the 270-th batch, train loss: 0.3945861756801605:  70%|███████▊   | 270/383 [02:40<01:08,  1.64it/s]Epoch: 9, train for the 24-th batch, train loss: 0.2429380863904953:  19%|██▌          | 23/119 [00:14<00:58,  1.65it/s]Epoch: 9, train for the 24-th batch, train loss: 0.2429380863904953:  20%|██▌          | 24/119 [00:14<00:57,  1.66it/s]Epoch: 15, train for the 41-th batch, train loss: 0.6195217967033386:  26%|███▏        | 40/151 [00:08<00:26,  4.20it/s]Epoch: 15, train for the 41-th batch, train loss: 0.6195217967033386:  27%|███▎        | 41/151 [00:08<00:25,  4.28it/s]evaluate for the 5-th batch, evaluate loss: 0.5925917029380798:  20%|████                | 4/20 [00:01<00:04,  3.44it/s]evaluate for the 5-th batch, evaluate loss: 0.5925917029380798:  25%|█████               | 5/20 [00:01<00:04,  3.38it/s]Epoch: 15, train for the 42-th batch, train loss: 0.5901745557785034:  27%|███▎        | 41/151 [00:08<00:25,  4.28it/s]Epoch: 15, train for the 42-th batch, train loss: 0.5901745557785034:  28%|███▎        | 42/151 [00:08<00:25,  4.36it/s]evaluate for the 6-th batch, evaluate loss: 0.6114456653594971:  25%|█████               | 5/20 [00:01<00:04,  3.38it/s]evaluate for the 6-th batch, evaluate loss: 0.6114456653594971:  30%|██████              | 6/20 [00:01<00:04,  3.47it/s]Epoch: 5, train for the 85-th batch, train loss: 0.5961812734603882:  35%|████▌        | 84/237 [00:48<01:33,  1.64it/s]Epoch: 5, train for the 85-th batch, train loss: 0.5961812734603882:  36%|████▋        | 85/237 [00:48<01:32,  1.64it/s]Epoch: 15, train for the 43-th batch, train loss: 0.6180625557899475:  28%|███▎        | 42/151 [00:08<00:25,  4.36it/s]Epoch: 15, train for the 43-th batch, train loss: 0.6180625557899475:  28%|███▍        | 43/151 [00:08<00:24,  4.40it/s]Epoch: 3, train for the 271-th batch, train loss: 0.4171024560928345:  70%|███████▊   | 270/383 [02:41<01:08,  1.64it/s]Epoch: 3, train for the 271-th batch, train loss: 0.4171024560928345:  71%|███████▊   | 271/383 [02:41<01:08,  1.64it/s]Epoch: 9, train for the 25-th batch, train loss: 0.2352682501077652:  20%|██▌          | 24/119 [00:14<00:57,  1.66it/s]Epoch: 9, train for the 25-th batch, train loss: 0.2352682501077652:  21%|██▋          | 25/119 [00:14<00:56,  1.67it/s]evaluate for the 7-th batch, evaluate loss: 0.6659933924674988:  30%|██████              | 6/20 [00:02<00:04,  3.47it/s]evaluate for the 7-th batch, evaluate loss: 0.6659933924674988:  35%|███████             | 7/20 [00:02<00:03,  3.40it/s]Epoch: 15, train for the 44-th batch, train loss: 0.57802814245224:  28%|███▉          | 43/151 [00:09<00:24,  4.40it/s]Epoch: 15, train for the 44-th batch, train loss: 0.57802814245224:  29%|████          | 44/151 [00:09<00:24,  4.42it/s]Epoch: 15, train for the 45-th batch, train loss: 0.5854809284210205:  29%|███▍        | 44/151 [00:09<00:24,  4.42it/s]Epoch: 15, train for the 45-th batch, train loss: 0.5854809284210205:  30%|███▌        | 45/151 [00:09<00:23,  4.44it/s]evaluate for the 8-th batch, evaluate loss: 0.653124988079071:  35%|███████▎             | 7/20 [00:02<00:03,  3.40it/s]evaluate for the 8-th batch, evaluate loss: 0.653124988079071:  40%|████████▍            | 8/20 [00:02<00:03,  3.50it/s]Epoch: 5, train for the 86-th batch, train loss: 0.6293966174125671:  36%|████▋        | 85/237 [00:48<01:32,  1.64it/s]Epoch: 5, train for the 86-th batch, train loss: 0.6293966174125671:  36%|████▋        | 86/237 [00:48<01:31,  1.64it/s]Epoch: 9, train for the 26-th batch, train loss: 0.18322086334228516:  21%|██▌         | 25/119 [00:15<00:56,  1.67it/s]Epoch: 9, train for the 26-th batch, train loss: 0.18322086334228516:  22%|██▌         | 26/119 [00:15<00:55,  1.68it/s]Epoch: 3, train for the 272-th batch, train loss: 0.3272976577281952:  71%|███████▊   | 271/383 [02:42<01:08,  1.64it/s]Epoch: 3, train for the 272-th batch, train loss: 0.3272976577281952:  71%|███████▊   | 272/383 [02:42<01:07,  1.64it/s]Epoch: 15, train for the 46-th batch, train loss: 0.595973014831543:  30%|███▊         | 45/151 [00:09<00:23,  4.44it/s]Epoch: 15, train for the 46-th batch, train loss: 0.595973014831543:  30%|███▉         | 46/151 [00:09<00:23,  4.47it/s]evaluate for the 9-th batch, evaluate loss: 0.5961174964904785:  40%|████████            | 8/20 [00:02<00:03,  3.50it/s]evaluate for the 9-th batch, evaluate loss: 0.5961174964904785:  45%|█████████           | 9/20 [00:02<00:03,  3.43it/s]Epoch: 15, train for the 47-th batch, train loss: 0.6242281198501587:  30%|███▋        | 46/151 [00:09<00:23,  4.47it/s]Epoch: 15, train for the 47-th batch, train loss: 0.6242281198501587:  31%|███▋        | 47/151 [00:09<00:23,  4.48it/s]evaluate for the 10-th batch, evaluate loss: 0.6013635396957397:  45%|████████▌          | 9/20 [00:02<00:03,  3.43it/s]evaluate for the 10-th batch, evaluate loss: 0.6013635396957397:  50%|█████████         | 10/20 [00:02<00:02,  3.57it/s]Epoch: 5, train for the 87-th batch, train loss: 0.605392575263977:  36%|█████         | 86/237 [00:49<01:31,  1.64it/s]Epoch: 5, train for the 87-th batch, train loss: 0.605392575263977:  37%|█████▏        | 87/237 [00:49<01:31,  1.65it/s]Epoch: 15, train for the 48-th batch, train loss: 0.5884096622467041:  31%|███▋        | 47/151 [00:10<00:23,  4.48it/s]Epoch: 15, train for the 48-th batch, train loss: 0.5884096622467041:  32%|███▊        | 48/151 [00:10<00:22,  4.48it/s]Epoch: 9, train for the 27-th batch, train loss: 0.19715820252895355:  22%|██▌         | 26/119 [00:16<00:55,  1.68it/s]Epoch: 9, train for the 27-th batch, train loss: 0.19715820252895355:  23%|██▋         | 27/119 [00:16<00:54,  1.70it/s]Epoch: 3, train for the 273-th batch, train loss: 0.39679834246635437:  71%|███████   | 272/383 [02:42<01:07,  1.64it/s]Epoch: 3, train for the 273-th batch, train loss: 0.39679834246635437:  71%|███████▏  | 273/383 [02:42<01:06,  1.64it/s]evaluate for the 11-th batch, evaluate loss: 0.6042186617851257:  50%|█████████         | 10/20 [00:03<00:02,  3.57it/s]evaluate for the 11-th batch, evaluate loss: 0.6042186617851257:  55%|█████████▉        | 11/20 [00:03<00:02,  3.46it/s]Epoch: 15, train for the 49-th batch, train loss: 0.49945205450057983:  32%|███▍       | 48/151 [00:10<00:22,  4.48it/s]Epoch: 15, train for the 49-th batch, train loss: 0.49945205450057983:  32%|███▌       | 49/151 [00:10<00:22,  4.52it/s]Epoch: 15, train for the 50-th batch, train loss: 0.5905826687812805:  32%|███▉        | 49/151 [00:10<00:22,  4.52it/s]Epoch: 15, train for the 50-th batch, train loss: 0.5905826687812805:  33%|███▉        | 50/151 [00:10<00:22,  4.51it/s]evaluate for the 12-th batch, evaluate loss: 0.6238430142402649:  55%|█████████▉        | 11/20 [00:03<00:02,  3.46it/s]evaluate for the 12-th batch, evaluate loss: 0.6238430142402649:  60%|██████████▊       | 12/20 [00:03<00:02,  3.61it/s]Epoch: 5, train for the 88-th batch, train loss: 0.651134192943573:  37%|█████▏        | 87/237 [00:49<01:31,  1.65it/s]Epoch: 5, train for the 88-th batch, train loss: 0.651134192943573:  37%|█████▏        | 88/237 [00:49<01:30,  1.64it/s]Epoch: 15, train for the 51-th batch, train loss: 0.586299479007721:  33%|████▎        | 50/151 [00:10<00:22,  4.51it/s]Epoch: 15, train for the 51-th batch, train loss: 0.586299479007721:  34%|████▍        | 51/151 [00:10<00:22,  4.52it/s]Epoch: 9, train for the 28-th batch, train loss: 0.22270190715789795:  23%|██▋         | 27/119 [00:16<00:54,  1.70it/s]Epoch: 9, train for the 28-th batch, train loss: 0.22270190715789795:  24%|██▊         | 28/119 [00:16<00:53,  1.71it/s]Epoch: 3, train for the 274-th batch, train loss: 0.39840567111968994:  71%|███████▏  | 273/383 [02:43<01:06,  1.64it/s]Epoch: 3, train for the 274-th batch, train loss: 0.39840567111968994:  72%|███████▏  | 274/383 [02:43<01:06,  1.64it/s]evaluate for the 13-th batch, evaluate loss: 0.6323314309120178:  60%|██████████▊       | 12/20 [00:03<00:02,  3.61it/s]evaluate for the 13-th batch, evaluate loss: 0.6323314309120178:  65%|███████████▋      | 13/20 [00:03<00:02,  3.48it/s]Epoch: 15, train for the 52-th batch, train loss: 0.529640257358551:  34%|████▍        | 51/151 [00:10<00:22,  4.52it/s]Epoch: 15, train for the 52-th batch, train loss: 0.529640257358551:  34%|████▍        | 52/151 [00:10<00:21,  4.55it/s]evaluate for the 14-th batch, evaluate loss: 0.62676602602005:  65%|█████████████       | 13/20 [00:03<00:02,  3.48it/s]evaluate for the 14-th batch, evaluate loss: 0.62676602602005:  70%|██████████████      | 14/20 [00:03<00:01,  3.65it/s]Epoch: 15, train for the 53-th batch, train loss: 0.6090551018714905:  34%|████▏       | 52/151 [00:11<00:21,  4.55it/s]Epoch: 15, train for the 53-th batch, train loss: 0.6090551018714905:  35%|████▏       | 53/151 [00:11<00:21,  4.52it/s]Epoch: 5, train for the 89-th batch, train loss: 0.6590530276298523:  37%|████▊        | 88/237 [00:50<01:30,  1.64it/s]Epoch: 5, train for the 89-th batch, train loss: 0.6590530276298523:  38%|████▉        | 89/237 [00:50<01:30,  1.64it/s]Epoch: 9, train for the 29-th batch, train loss: 0.19096484780311584:  24%|██▊         | 28/119 [00:17<00:53,  1.71it/s]Epoch: 9, train for the 29-th batch, train loss: 0.19096484780311584:  24%|██▉         | 29/119 [00:17<00:52,  1.70it/s]Epoch: 15, train for the 54-th batch, train loss: 0.5818325877189636:  35%|████▏       | 53/151 [00:11<00:21,  4.52it/s]Epoch: 15, train for the 54-th batch, train loss: 0.5818325877189636:  36%|████▎       | 54/151 [00:11<00:21,  4.52it/s]evaluate for the 15-th batch, evaluate loss: 0.6176238059997559:  70%|████████████▌     | 14/20 [00:04<00:01,  3.65it/s]evaluate for the 15-th batch, evaluate loss: 0.6176238059997559:  75%|█████████████▌    | 15/20 [00:04<00:01,  3.49it/s]Epoch: 3, train for the 275-th batch, train loss: 0.4281507134437561:  72%|███████▊   | 274/383 [02:44<01:06,  1.64it/s]Epoch: 3, train for the 275-th batch, train loss: 0.4281507134437561:  72%|███████▉   | 275/383 [02:44<01:05,  1.64it/s]Epoch: 15, train for the 55-th batch, train loss: 0.5795283913612366:  36%|████▎       | 54/151 [00:11<00:21,  4.52it/s]Epoch: 15, train for the 55-th batch, train loss: 0.5795283913612366:  36%|████▎       | 55/151 [00:11<00:21,  4.51it/s]evaluate for the 16-th batch, evaluate loss: 0.5993301868438721:  75%|█████████████▌    | 15/20 [00:04<00:01,  3.49it/s]evaluate for the 16-th batch, evaluate loss: 0.5993301868438721:  80%|██████████████▍   | 16/20 [00:04<00:01,  3.63it/s]Epoch: 15, train for the 56-th batch, train loss: 0.48746782541275024:  36%|████       | 55/151 [00:11<00:21,  4.51it/s]Epoch: 15, train for the 56-th batch, train loss: 0.48746782541275024:  37%|████       | 56/151 [00:11<00:21,  4.51it/s]Epoch: 5, train for the 90-th batch, train loss: 0.6362233757972717:  38%|████▉        | 89/237 [00:51<01:30,  1.64it/s]Epoch: 5, train for the 90-th batch, train loss: 0.6362233757972717:  38%|████▉        | 90/237 [00:51<01:29,  1.64it/s]Epoch: 9, train for the 30-th batch, train loss: 0.1943601816892624:  24%|███▏         | 29/119 [00:17<00:52,  1.70it/s]Epoch: 9, train for the 30-th batch, train loss: 0.1943601816892624:  25%|███▎         | 30/119 [00:17<00:52,  1.69it/s]evaluate for the 17-th batch, evaluate loss: 0.5864207148551941:  80%|██████████████▍   | 16/20 [00:04<00:01,  3.63it/s]evaluate for the 17-th batch, evaluate loss: 0.5864207148551941:  85%|███████████████▎  | 17/20 [00:04<00:00,  3.45it/s]Epoch: 3, train for the 276-th batch, train loss: 0.5357128977775574:  72%|███████▉   | 275/383 [02:44<01:05,  1.64it/s]Epoch: 3, train for the 276-th batch, train loss: 0.5357128977775574:  72%|███████▉   | 276/383 [02:44<01:05,  1.65it/s]Epoch: 15, train for the 57-th batch, train loss: 0.5438615083694458:  37%|████▍       | 56/151 [00:12<00:21,  4.51it/s]Epoch: 15, train for the 57-th batch, train loss: 0.5438615083694458:  38%|████▌       | 57/151 [00:12<00:20,  4.51it/s]evaluate for the 18-th batch, evaluate loss: 0.6557261347770691:  85%|███████████████▎  | 17/20 [00:05<00:00,  3.45it/s]evaluate for the 18-th batch, evaluate loss: 0.6557261347770691:  90%|████████████████▏ | 18/20 [00:05<00:00,  3.53it/s]Epoch: 15, train for the 58-th batch, train loss: 0.4371194839477539:  38%|████▌       | 57/151 [00:12<00:20,  4.51it/s]Epoch: 15, train for the 58-th batch, train loss: 0.4371194839477539:  38%|████▌       | 58/151 [00:12<00:20,  4.57it/s]Epoch: 15, train for the 59-th batch, train loss: 0.5086383819580078:  38%|████▌       | 58/151 [00:12<00:20,  4.57it/s]Epoch: 15, train for the 59-th batch, train loss: 0.5086383819580078:  39%|████▋       | 59/151 [00:12<00:20,  4.55it/s]Epoch: 5, train for the 91-th batch, train loss: 0.6284875273704529:  38%|████▉        | 90/237 [00:51<01:29,  1.64it/s]Epoch: 5, train for the 91-th batch, train loss: 0.6284875273704529:  38%|████▉        | 91/237 [00:51<01:28,  1.65it/s]Epoch: 9, train for the 31-th batch, train loss: 0.21720388531684875:  25%|███         | 30/119 [00:18<00:52,  1.69it/s]Epoch: 9, train for the 31-th batch, train loss: 0.21720388531684875:  26%|███▏        | 31/119 [00:18<00:52,  1.68it/s]evaluate for the 19-th batch, evaluate loss: 0.6569221615791321:  90%|████████████████▏ | 18/20 [00:05<00:00,  3.53it/s]evaluate for the 19-th batch, evaluate loss: 0.6569221615791321:  95%|█████████████████ | 19/20 [00:05<00:00,  3.38it/s]Epoch: 3, train for the 277-th batch, train loss: 0.3699510991573334:  72%|███████▉   | 276/383 [02:45<01:05,  1.65it/s]Epoch: 3, train for the 277-th batch, train loss: 0.3699510991573334:  72%|███████▉   | 277/383 [02:45<01:04,  1.65it/s]evaluate for the 20-th batch, evaluate loss: 0.6345160007476807:  95%|█████████████████ | 19/20 [00:05<00:00,  3.38it/s]evaluate for the 20-th batch, evaluate loss: 0.6345160007476807: 100%|██████████████████| 20/20 [00:05<00:00,  3.60it/s]
INFO:root:Epoch: 7, learning rate: 0.0001, train loss: 0.5024
INFO:root:train average_precision, 0.8510
INFO:root:train roc_auc, 0.8259
INFO:root:validate loss: 0.5099
INFO:root:validate average_precision, 0.8442
INFO:root:validate roc_auc, 0.8109
INFO:root:new node validate loss: 0.6146
INFO:root:new node validate first_1_average_precision, 0.6448
INFO:root:new node validate first_1_roc_auc, 0.5746
INFO:root:new node validate first_3_average_precision, 0.7025
INFO:root:new node validate first_3_roc_auc, 0.6402
INFO:root:new node validate first_10_average_precision, 0.7470
INFO:root:new node validate first_10_roc_auc, 0.6970
INFO:root:new node validate average_precision, 0.7526
INFO:root:new node validate roc_auc, 0.7148
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]Epoch: 15, train for the 60-th batch, train loss: 0.5184450149536133:  39%|████▋       | 59/151 [00:12<00:20,  4.55it/s]Epoch: 15, train for the 60-th batch, train loss: 0.5184450149536133:  40%|████▊       | 60/151 [00:12<00:20,  4.49it/s]Epoch: 15, train for the 61-th batch, train loss: 0.4493981897830963:  40%|████▊       | 60/151 [00:12<00:20,  4.49it/s]Epoch: 15, train for the 61-th batch, train loss: 0.4493981897830963:  40%|████▊       | 61/151 [00:12<00:19,  4.51it/s]Epoch: 5, train for the 92-th batch, train loss: 0.5540062785148621:  38%|████▉        | 91/237 [00:52<01:28,  1.65it/s]Epoch: 5, train for the 92-th batch, train loss: 0.5540062785148621:  39%|█████        | 92/237 [00:52<01:26,  1.67it/s]Epoch: 8, train for the 1-th batch, train loss: 0.8515931367874146:   0%|                       | 0/146 [00:00<?, ?it/s]Epoch: 9, train for the 32-th batch, train loss: 0.1757371872663498:  26%|███▍         | 31/119 [00:19<00:52,  1.68it/s]Epoch: 8, train for the 1-th batch, train loss: 0.8515931367874146:   1%|               | 1/146 [00:00<01:07,  2.15it/s]Epoch: 9, train for the 32-th batch, train loss: 0.1757371872663498:  27%|███▍         | 32/119 [00:19<00:51,  1.70it/s]Epoch: 15, train for the 62-th batch, train loss: 0.5703844428062439:  40%|████▊       | 61/151 [00:13<00:19,  4.51it/s]Epoch: 15, train for the 62-th batch, train loss: 0.5703844428062439:  41%|████▉       | 62/151 [00:13<00:19,  4.49it/s]Epoch: 3, train for the 278-th batch, train loss: 0.3721015155315399:  72%|███████▉   | 277/383 [02:45<01:04,  1.65it/s]Epoch: 3, train for the 278-th batch, train loss: 0.3721015155315399:  73%|███████▉   | 278/383 [02:45<01:03,  1.66it/s]Epoch: 15, train for the 63-th batch, train loss: 0.5647774934768677:  41%|████▉       | 62/151 [00:13<00:19,  4.49it/s]Epoch: 15, train for the 63-th batch, train loss: 0.5647774934768677:  42%|█████       | 63/151 [00:13<00:19,  4.47it/s]Epoch: 15, train for the 64-th batch, train loss: 0.5308774709701538:  42%|█████       | 63/151 [00:13<00:19,  4.47it/s]Epoch: 15, train for the 64-th batch, train loss: 0.5308774709701538:  42%|█████       | 64/151 [00:13<00:19,  4.47it/s]Epoch: 8, train for the 2-th batch, train loss: 0.4949289560317993:   1%|               | 1/146 [00:01<01:07,  2.15it/s]Epoch: 8, train for the 2-th batch, train loss: 0.4949289560317993:   1%|▏              | 2/146 [00:01<01:15,  1.90it/s]Epoch: 5, train for the 93-th batch, train loss: 0.6108895540237427:  39%|█████        | 92/237 [00:52<01:26,  1.67it/s]Epoch: 5, train for the 93-th batch, train loss: 0.6108895540237427:  39%|█████        | 93/237 [00:52<01:27,  1.64it/s]Epoch: 9, train for the 33-th batch, train loss: 0.22610530257225037:  27%|███▏        | 32/119 [00:19<00:51,  1.70it/s]Epoch: 9, train for the 33-th batch, train loss: 0.22610530257225037:  28%|███▎        | 33/119 [00:19<00:51,  1.67it/s]Epoch: 15, train for the 65-th batch, train loss: 0.4628356695175171:  42%|█████       | 64/151 [00:13<00:19,  4.47it/s]Epoch: 15, train for the 65-th batch, train loss: 0.4628356695175171:  43%|█████▏      | 65/151 [00:13<00:19,  4.47it/s]Epoch: 3, train for the 279-th batch, train loss: 0.469120591878891:  73%|████████▋   | 278/383 [02:46<01:03,  1.66it/s]Epoch: 3, train for the 279-th batch, train loss: 0.469120591878891:  73%|████████▋   | 279/383 [02:46<01:03,  1.65it/s]Epoch: 15, train for the 66-th batch, train loss: 0.4337068796157837:  43%|█████▏      | 65/151 [00:14<00:19,  4.47it/s]Epoch: 15, train for the 66-th batch, train loss: 0.4337068796157837:  44%|█████▏      | 66/151 [00:14<00:18,  4.49it/s]Epoch: 8, train for the 3-th batch, train loss: 0.3990241587162018:   1%|▏              | 2/146 [00:01<01:15,  1.90it/s]Epoch: 8, train for the 3-th batch, train loss: 0.3990241587162018:   2%|▎              | 3/146 [00:01<01:19,  1.80it/s]Epoch: 15, train for the 67-th batch, train loss: 0.5742820501327515:  44%|█████▏      | 66/151 [00:14<00:18,  4.49it/s]Epoch: 15, train for the 67-th batch, train loss: 0.5742820501327515:  44%|█████▎      | 67/151 [00:14<00:18,  4.49it/s]Epoch: 5, train for the 94-th batch, train loss: 0.6318380832672119:  39%|█████        | 93/237 [00:53<01:27,  1.64it/s]Epoch: 5, train for the 94-th batch, train loss: 0.6318380832672119:  40%|█████▏       | 94/237 [00:53<01:27,  1.64it/s]Epoch: 9, train for the 34-th batch, train loss: 0.18508096039295197:  28%|███▎        | 33/119 [00:20<00:51,  1.67it/s]Epoch: 9, train for the 34-th batch, train loss: 0.18508096039295197:  29%|███▍        | 34/119 [00:20<00:50,  1.68it/s]Epoch: 3, train for the 280-th batch, train loss: 0.38291221857070923:  73%|███████▎  | 279/383 [02:47<01:03,  1.65it/s]Epoch: 3, train for the 280-th batch, train loss: 0.38291221857070923:  73%|███████▎  | 280/383 [02:47<01:02,  1.65it/s]Epoch: 15, train for the 68-th batch, train loss: 0.5383294820785522:  44%|█████▎      | 67/151 [00:14<00:18,  4.49it/s]Epoch: 15, train for the 68-th batch, train loss: 0.5383294820785522:  45%|█████▍      | 68/151 [00:14<00:18,  4.48it/s]Epoch: 15, train for the 69-th batch, train loss: 0.5513275265693665:  45%|█████▍      | 68/151 [00:14<00:18,  4.48it/s]Epoch: 15, train for the 69-th batch, train loss: 0.5513275265693665:  46%|█████▍      | 69/151 [00:14<00:18,  4.47it/s]Epoch: 8, train for the 4-th batch, train loss: 0.3787858486175537:   2%|▎              | 3/146 [00:02<01:19,  1.80it/s]Epoch: 8, train for the 4-th batch, train loss: 0.3787858486175537:   3%|▍              | 4/146 [00:02<01:19,  1.79it/s]Epoch: 5, train for the 95-th batch, train loss: 0.6257341504096985:  40%|█████▏       | 94/237 [00:54<01:27,  1.64it/s]Epoch: 5, train for the 95-th batch, train loss: 0.6257341504096985:  40%|█████▏       | 95/237 [00:54<01:26,  1.64it/s]Epoch: 9, train for the 35-th batch, train loss: 0.15911081433296204:  29%|███▍        | 34/119 [00:20<00:50,  1.68it/s]Epoch: 9, train for the 35-th batch, train loss: 0.15911081433296204:  29%|███▌        | 35/119 [00:20<00:49,  1.68it/s]Epoch: 15, train for the 70-th batch, train loss: 0.5482759475708008:  46%|█████▍      | 69/151 [00:14<00:18,  4.47it/s]Epoch: 15, train for the 70-th batch, train loss: 0.5482759475708008:  46%|█████▌      | 70/151 [00:14<00:18,  4.47it/s]Epoch: 3, train for the 281-th batch, train loss: 0.47784867882728577:  73%|███████▎  | 280/383 [02:47<01:02,  1.65it/s]Epoch: 3, train for the 281-th batch, train loss: 0.47784867882728577:  73%|███████▎  | 281/383 [02:47<01:01,  1.65it/s]Epoch: 15, train for the 71-th batch, train loss: 0.5724401473999023:  46%|█████▌      | 70/151 [00:15<00:18,  4.47it/s]Epoch: 15, train for the 71-th batch, train loss: 0.5724401473999023:  47%|█████▋      | 71/151 [00:15<00:17,  4.47it/s]Epoch: 8, train for the 5-th batch, train loss: 0.4274251461029053:   3%|▍              | 4/146 [00:02<01:19,  1.79it/s]Epoch: 8, train for the 5-th batch, train loss: 0.4274251461029053:   3%|▌              | 5/146 [00:02<01:16,  1.85it/s]Epoch: 15, train for the 72-th batch, train loss: 0.5517958998680115:  47%|█████▋      | 71/151 [00:15<00:17,  4.47it/s]Epoch: 15, train for the 72-th batch, train loss: 0.5517958998680115:  48%|█████▋      | 72/151 [00:15<00:17,  4.47it/s]Epoch: 9, train for the 36-th batch, train loss: 0.16322943568229675:  29%|███▌        | 35/119 [00:21<00:49,  1.68it/s]Epoch: 9, train for the 36-th batch, train loss: 0.16322943568229675:  30%|███▋        | 36/119 [00:21<00:49,  1.69it/s]Epoch: 5, train for the 96-th batch, train loss: 0.6206227540969849:  40%|█████▏       | 95/237 [00:54<01:26,  1.64it/s]Epoch: 5, train for the 96-th batch, train loss: 0.6206227540969849:  41%|█████▎       | 96/237 [00:54<01:25,  1.65it/s]Epoch: 15, train for the 73-th batch, train loss: 0.5300906300544739:  48%|█████▋      | 72/151 [00:15<00:17,  4.47it/s]Epoch: 15, train for the 73-th batch, train loss: 0.5300906300544739:  48%|█████▊      | 73/151 [00:15<00:17,  4.46it/s]Epoch: 3, train for the 282-th batch, train loss: 0.47325560450553894:  73%|███████▎  | 281/383 [02:48<01:01,  1.65it/s]Epoch: 3, train for the 282-th batch, train loss: 0.47325560450553894:  74%|███████▎  | 282/383 [02:48<01:01,  1.65it/s]Epoch: 15, train for the 74-th batch, train loss: 0.51474529504776:  48%|██████▊       | 73/151 [00:15<00:17,  4.46it/s]Epoch: 15, train for the 74-th batch, train loss: 0.51474529504776:  49%|██████▊       | 74/151 [00:15<00:17,  4.46it/s]Epoch: 8, train for the 6-th batch, train loss: 0.488677442073822:   3%|▌               | 5/146 [00:03<01:16,  1.85it/s]Epoch: 8, train for the 6-th batch, train loss: 0.488677442073822:   4%|▋               | 6/146 [00:03<01:19,  1.76it/s]Epoch: 15, train for the 75-th batch, train loss: 0.49825987219810486:  49%|█████▍     | 74/151 [00:16<00:17,  4.46it/s]Epoch: 15, train for the 75-th batch, train loss: 0.49825987219810486:  50%|█████▍     | 75/151 [00:16<00:16,  4.48it/s]Epoch: 9, train for the 37-th batch, train loss: 0.1624167561531067:  30%|███▉         | 36/119 [00:22<00:49,  1.69it/s]Epoch: 9, train for the 37-th batch, train loss: 0.1624167561531067:  31%|████         | 37/119 [00:22<00:49,  1.67it/s]Epoch: 5, train for the 97-th batch, train loss: 0.5800565481185913:  41%|█████▎       | 96/237 [00:55<01:25,  1.65it/s]Epoch: 5, train for the 97-th batch, train loss: 0.5800565481185913:  41%|█████▎       | 97/237 [00:55<01:25,  1.64it/s]Epoch: 15, train for the 76-th batch, train loss: 0.4791993498802185:  50%|█████▉      | 75/151 [00:16<00:16,  4.48it/s]Epoch: 15, train for the 76-th batch, train loss: 0.4791993498802185:  50%|██████      | 76/151 [00:16<00:16,  4.51it/s]Epoch: 3, train for the 283-th batch, train loss: 0.42551878094673157:  74%|███████▎  | 282/383 [02:48<01:01,  1.65it/s]Epoch: 3, train for the 283-th batch, train loss: 0.42551878094673157:  74%|███████▍  | 283/383 [02:48<01:01,  1.64it/s]Epoch: 15, train for the 77-th batch, train loss: 0.5504045486450195:  50%|██████      | 76/151 [00:16<00:16,  4.51it/s]Epoch: 15, train for the 77-th batch, train loss: 0.5504045486450195:  51%|██████      | 77/151 [00:16<00:16,  4.49it/s]Epoch: 8, train for the 7-th batch, train loss: 0.4611002206802368:   4%|▌              | 6/146 [00:03<01:19,  1.76it/s]Epoch: 8, train for the 7-th batch, train loss: 0.4611002206802368:   5%|▋              | 7/146 [00:03<01:20,  1.73it/s]Epoch: 9, train for the 38-th batch, train loss: 0.1550283581018448:  31%|████         | 37/119 [00:22<00:49,  1.67it/s]Epoch: 9, train for the 38-th batch, train loss: 0.1550283581018448:  32%|████▏        | 38/119 [00:22<00:48,  1.66it/s]Epoch: 15, train for the 78-th batch, train loss: 0.5733362436294556:  51%|██████      | 77/151 [00:16<00:16,  4.49it/s]Epoch: 15, train for the 78-th batch, train loss: 0.5733362436294556:  52%|██████▏     | 78/151 [00:16<00:16,  4.47it/s]Epoch: 5, train for the 98-th batch, train loss: 0.5865179300308228:  41%|█████▎       | 97/237 [00:55<01:25,  1.64it/s]Epoch: 5, train for the 98-th batch, train loss: 0.5865179300308228:  41%|█████▍       | 98/237 [00:55<01:25,  1.64it/s]Epoch: 3, train for the 284-th batch, train loss: 0.4933736324310303:  74%|████████▏  | 283/383 [02:49<01:01,  1.64it/s]Epoch: 3, train for the 284-th batch, train loss: 0.4933736324310303:  74%|████████▏  | 284/383 [02:49<01:00,  1.63it/s]Epoch: 15, train for the 79-th batch, train loss: 0.5966315269470215:  52%|██████▏     | 78/151 [00:16<00:16,  4.47it/s]Epoch: 15, train for the 79-th batch, train loss: 0.5966315269470215:  52%|██████▎     | 79/151 [00:16<00:16,  4.46it/s]Epoch: 8, train for the 8-th batch, train loss: 0.39622145891189575:   5%|▋             | 7/146 [00:04<01:20,  1.73it/s]Epoch: 8, train for the 8-th batch, train loss: 0.39622145891189575:   5%|▊             | 8/146 [00:04<01:18,  1.76it/s]Epoch: 15, train for the 80-th batch, train loss: 0.5325318574905396:  52%|██████▎     | 79/151 [00:17<00:16,  4.46it/s]Epoch: 15, train for the 80-th batch, train loss: 0.5325318574905396:  53%|██████▎     | 80/151 [00:17<00:15,  4.46it/s]Epoch: 9, train for the 39-th batch, train loss: 0.20662671327590942:  32%|███▊        | 38/119 [00:23<00:48,  1.66it/s]Epoch: 9, train for the 39-th batch, train loss: 0.20662671327590942:  33%|███▉        | 39/119 [00:23<00:47,  1.68it/s]Epoch: 5, train for the 99-th batch, train loss: 0.5739585161209106:  41%|█████▍       | 98/237 [00:56<01:25,  1.64it/s]Epoch: 5, train for the 99-th batch, train loss: 0.5739585161209106:  42%|█████▍       | 99/237 [00:56<01:24,  1.63it/s]Epoch: 15, train for the 81-th batch, train loss: 0.5749551057815552:  53%|██████▎     | 80/151 [00:17<00:15,  4.46it/s]Epoch: 15, train for the 81-th batch, train loss: 0.5749551057815552:  54%|██████▍     | 81/151 [00:17<00:15,  4.46it/s]Epoch: 3, train for the 285-th batch, train loss: 0.461383581161499:  74%|████████▉   | 284/383 [02:50<01:00,  1.63it/s]Epoch: 3, train for the 285-th batch, train loss: 0.461383581161499:  74%|████████▉   | 285/383 [02:50<01:00,  1.63it/s]Epoch: 15, train for the 82-th batch, train loss: 0.5608790516853333:  54%|██████▍     | 81/151 [00:17<00:15,  4.46it/s]Epoch: 15, train for the 82-th batch, train loss: 0.5608790516853333:  54%|██████▌     | 82/151 [00:17<00:15,  4.46it/s]Epoch: 8, train for the 9-th batch, train loss: 0.4093836545944214:   5%|▊              | 8/146 [00:05<01:18,  1.76it/s]Epoch: 8, train for the 9-th batch, train loss: 0.4093836545944214:   6%|▉              | 9/146 [00:05<01:18,  1.75it/s]Epoch: 15, train for the 83-th batch, train loss: 0.5548902750015259:  54%|██████▌     | 82/151 [00:17<00:15,  4.46it/s]Epoch: 15, train for the 83-th batch, train loss: 0.5548902750015259:  55%|██████▌     | 83/151 [00:17<00:15,  4.44it/s]Epoch: 9, train for the 40-th batch, train loss: 0.16978222131729126:  33%|███▉        | 39/119 [00:23<00:47,  1.68it/s]Epoch: 9, train for the 40-th batch, train loss: 0.16978222131729126:  34%|████        | 40/119 [00:23<00:46,  1.68it/s]Epoch: 5, train for the 100-th batch, train loss: 0.613561749458313:  42%|█████▍       | 99/237 [00:57<01:24,  1.63it/s]Epoch: 5, train for the 100-th batch, train loss: 0.613561749458313:  42%|█████       | 100/237 [00:57<01:24,  1.63it/s]Epoch: 15, train for the 84-th batch, train loss: 0.5918803215026855:  55%|██████▌     | 83/151 [00:18<00:15,  4.44it/s]Epoch: 15, train for the 84-th batch, train loss: 0.5918803215026855:  56%|██████▋     | 84/151 [00:18<00:15,  4.44it/s]Epoch: 3, train for the 286-th batch, train loss: 0.3965862989425659:  74%|████████▏  | 285/383 [02:50<01:00,  1.63it/s]Epoch: 3, train for the 286-th batch, train loss: 0.3965862989425659:  75%|████████▏  | 286/383 [02:50<00:59,  1.63it/s]Epoch: 8, train for the 10-th batch, train loss: 0.4103049635887146:   6%|▊             | 9/146 [00:05<01:18,  1.75it/s]Epoch: 8, train for the 10-th batch, train loss: 0.4103049635887146:   7%|▉            | 10/146 [00:05<01:15,  1.80it/s]Epoch: 15, train for the 85-th batch, train loss: 0.586613655090332:  56%|███████▏     | 84/151 [00:18<00:15,  4.44it/s]Epoch: 15, train for the 85-th batch, train loss: 0.586613655090332:  56%|███████▎     | 85/151 [00:18<00:14,  4.45it/s]Epoch: 9, train for the 41-th batch, train loss: 0.22186627984046936:  34%|████        | 40/119 [00:24<00:46,  1.68it/s]Epoch: 9, train for the 41-th batch, train loss: 0.22186627984046936:  34%|████▏       | 41/119 [00:24<00:46,  1.67it/s]Epoch: 5, train for the 101-th batch, train loss: 0.6079648733139038:  42%|████▋      | 100/237 [00:57<01:24,  1.63it/s]Epoch: 5, train for the 101-th batch, train loss: 0.6079648733139038:  43%|████▋      | 101/237 [00:57<01:23,  1.63it/s]Epoch: 8, train for the 11-th batch, train loss: 0.4229400157928467:   7%|▉            | 10/146 [00:06<01:15,  1.80it/s]Epoch: 8, train for the 11-th batch, train loss: 0.4229400157928467:   8%|▉            | 11/146 [00:06<01:14,  1.82it/s]Epoch: 3, train for the 287-th batch, train loss: 0.4305473864078522:  75%|████████▏  | 286/383 [02:51<00:59,  1.63it/s]Epoch: 3, train for the 287-th batch, train loss: 0.4305473864078522:  75%|████████▏  | 287/383 [02:51<00:58,  1.63it/s]Epoch: 15, train for the 86-th batch, train loss: 0.5026311874389648:  56%|██████▊     | 85/151 [00:18<00:14,  4.45it/s]Epoch: 15, train for the 86-th batch, train loss: 0.5026311874389648:  57%|██████▊     | 86/151 [00:18<00:19,  3.34it/s]Epoch: 15, train for the 87-th batch, train loss: 0.5375058650970459:  57%|██████▊     | 86/151 [00:19<00:19,  3.34it/s]Epoch: 15, train for the 87-th batch, train loss: 0.5375058650970459:  58%|██████▉     | 87/151 [00:19<00:17,  3.61it/s]Epoch: 9, train for the 42-th batch, train loss: 0.20848849415779114:  34%|████▏       | 41/119 [00:24<00:46,  1.67it/s]Epoch: 9, train for the 42-th batch, train loss: 0.20848849415779114:  35%|████▏       | 42/119 [00:24<00:45,  1.69it/s]Epoch: 5, train for the 102-th batch, train loss: 0.6395552158355713:  43%|████▋      | 101/237 [00:58<01:23,  1.63it/s]Epoch: 5, train for the 102-th batch, train loss: 0.6395552158355713:  43%|████▋      | 102/237 [00:58<01:22,  1.63it/s]Epoch: 15, train for the 88-th batch, train loss: 0.5877030491828918:  58%|██████▉     | 87/151 [00:19<00:17,  3.61it/s]Epoch: 15, train for the 88-th batch, train loss: 0.5877030491828918:  58%|██████▉     | 88/151 [00:19<00:16,  3.82it/s]Epoch: 8, train for the 12-th batch, train loss: 0.40535253286361694:   8%|▉           | 11/146 [00:06<01:14,  1.82it/s]Epoch: 8, train for the 12-th batch, train loss: 0.40535253286361694:   8%|▉           | 12/146 [00:06<01:12,  1.86it/s]Epoch: 3, train for the 288-th batch, train loss: 0.48029088973999023:  75%|███████▍  | 287/383 [02:51<00:58,  1.63it/s]Epoch: 3, train for the 288-th batch, train loss: 0.48029088973999023:  75%|███████▌  | 288/383 [02:51<00:58,  1.63it/s]Epoch: 15, train for the 89-th batch, train loss: 0.5981577038764954:  58%|██████▉     | 88/151 [00:19<00:16,  3.82it/s]Epoch: 15, train for the 89-th batch, train loss: 0.5981577038764954:  59%|███████     | 89/151 [00:19<00:15,  3.99it/s]Epoch: 15, train for the 90-th batch, train loss: 0.5683799386024475:  59%|███████     | 89/151 [00:19<00:15,  3.99it/s]Epoch: 15, train for the 90-th batch, train loss: 0.5683799386024475:  60%|███████▏    | 90/151 [00:19<00:14,  4.11it/s]Epoch: 9, train for the 43-th batch, train loss: 0.22901348769664764:  35%|████▏       | 42/119 [00:25<00:45,  1.69it/s]Epoch: 9, train for the 43-th batch, train loss: 0.22901348769664764:  36%|████▎       | 43/119 [00:25<00:46,  1.65it/s]Epoch: 5, train for the 103-th batch, train loss: 0.6529836654663086:  43%|████▋      | 102/237 [00:59<01:22,  1.63it/s]Epoch: 5, train for the 103-th batch, train loss: 0.6529836654663086:  43%|████▊      | 103/237 [00:59<01:22,  1.63it/s]Epoch: 8, train for the 13-th batch, train loss: 0.42424026131629944:   8%|▉           | 12/146 [00:07<01:12,  1.86it/s]Epoch: 8, train for the 13-th batch, train loss: 0.42424026131629944:   9%|█           | 13/146 [00:07<01:15,  1.77it/s]Epoch: 15, train for the 91-th batch, train loss: 0.4849335253238678:  60%|███████▏    | 90/151 [00:19<00:14,  4.11it/s]Epoch: 15, train for the 91-th batch, train loss: 0.4849335253238678:  60%|███████▏    | 91/151 [00:19<00:14,  4.21it/s]Epoch: 3, train for the 289-th batch, train loss: 0.42194339632987976:  75%|███████▌  | 288/383 [02:52<00:58,  1.63it/s]Epoch: 3, train for the 289-th batch, train loss: 0.42194339632987976:  75%|███████▌  | 289/383 [02:52<00:57,  1.63it/s]Epoch: 15, train for the 92-th batch, train loss: 0.5588456392288208:  60%|███████▏    | 91/151 [00:20<00:14,  4.21it/s]Epoch: 15, train for the 92-th batch, train loss: 0.5588456392288208:  61%|███████▎    | 92/151 [00:20<00:13,  4.26it/s]Epoch: 9, train for the 44-th batch, train loss: 0.1738494634628296:  36%|████▋        | 43/119 [00:26<00:46,  1.65it/s]Epoch: 9, train for the 44-th batch, train loss: 0.1738494634628296:  37%|████▊        | 44/119 [00:26<00:42,  1.75it/s]Epoch: 15, train for the 93-th batch, train loss: 0.5245331525802612:  61%|███████▎    | 92/151 [00:20<00:13,  4.26it/s]Epoch: 15, train for the 93-th batch, train loss: 0.5245331525802612:  62%|███████▍    | 93/151 [00:20<00:13,  4.32it/s]Epoch: 5, train for the 104-th batch, train loss: 0.624509334564209:  43%|█████▏      | 103/237 [00:59<01:22,  1.63it/s]Epoch: 5, train for the 104-th batch, train loss: 0.624509334564209:  44%|█████▎      | 104/237 [00:59<01:21,  1.63it/s]Epoch: 8, train for the 14-th batch, train loss: 0.36650580167770386:   9%|█           | 13/146 [00:07<01:15,  1.77it/s]Epoch: 8, train for the 14-th batch, train loss: 0.36650580167770386:  10%|█▏          | 14/146 [00:07<01:17,  1.70it/s]Epoch: 15, train for the 94-th batch, train loss: 0.514992356300354:  62%|████████     | 93/151 [00:20<00:13,  4.32it/s]Epoch: 15, train for the 94-th batch, train loss: 0.514992356300354:  62%|████████     | 94/151 [00:20<00:13,  4.34it/s]Epoch: 3, train for the 290-th batch, train loss: 0.39057043194770813:  75%|███████▌  | 289/383 [02:53<00:57,  1.63it/s]Epoch: 3, train for the 290-th batch, train loss: 0.39057043194770813:  76%|███████▌  | 290/383 [02:53<00:57,  1.63it/s]Epoch: 15, train for the 95-th batch, train loss: 0.5292097330093384:  62%|███████▍    | 94/151 [00:20<00:13,  4.34it/s]Epoch: 15, train for the 95-th batch, train loss: 0.5292097330093384:  63%|███████▌    | 95/151 [00:20<00:12,  4.37it/s]Epoch: 9, train for the 45-th batch, train loss: 0.15036292374134064:  37%|████▍       | 44/119 [00:26<00:42,  1.75it/s]Epoch: 9, train for the 45-th batch, train loss: 0.15036292374134064:  38%|████▌       | 45/119 [00:26<00:42,  1.72it/s]Epoch: 15, train for the 96-th batch, train loss: 0.5555675625801086:  63%|███████▌    | 95/151 [00:21<00:12,  4.37it/s]Epoch: 15, train for the 96-th batch, train loss: 0.5555675625801086:  64%|███████▋    | 96/151 [00:21<00:12,  4.39it/s]Epoch: 5, train for the 105-th batch, train loss: 0.6163175702095032:  44%|████▊      | 104/237 [01:00<01:21,  1.63it/s]Epoch: 5, train for the 105-th batch, train loss: 0.6163175702095032:  44%|████▊      | 105/237 [01:00<01:21,  1.63it/s]Epoch: 8, train for the 15-th batch, train loss: 0.37536177039146423:  10%|█▏          | 14/146 [00:08<01:17,  1.70it/s]Epoch: 8, train for the 15-th batch, train loss: 0.37536177039146423:  10%|█▏          | 15/146 [00:08<01:17,  1.69it/s]Epoch: 3, train for the 291-th batch, train loss: 0.33979523181915283:  76%|███████▌  | 290/383 [02:53<00:57,  1.63it/s]Epoch: 3, train for the 291-th batch, train loss: 0.33979523181915283:  76%|███████▌  | 291/383 [02:53<00:56,  1.62it/s]Epoch: 15, train for the 97-th batch, train loss: 0.5927788019180298:  64%|███████▋    | 96/151 [00:21<00:12,  4.39it/s]Epoch: 15, train for the 97-th batch, train loss: 0.5927788019180298:  64%|███████▋    | 97/151 [00:21<00:12,  4.40it/s]Epoch: 9, train for the 46-th batch, train loss: 0.21633583307266235:  38%|████▌       | 45/119 [00:27<00:42,  1.72it/s]Epoch: 9, train for the 46-th batch, train loss: 0.21633583307266235:  39%|████▋       | 46/119 [00:27<00:42,  1.71it/s]Epoch: 15, train for the 98-th batch, train loss: 0.5914636254310608:  64%|███████▋    | 97/151 [00:21<00:12,  4.40it/s]Epoch: 15, train for the 98-th batch, train loss: 0.5914636254310608:  65%|███████▊    | 98/151 [00:21<00:12,  4.41it/s]Epoch: 8, train for the 16-th batch, train loss: 0.47661590576171875:  10%|█▏          | 15/146 [00:09<01:17,  1.69it/s]Epoch: 8, train for the 16-th batch, train loss: 0.47661590576171875:  11%|█▎          | 16/146 [00:09<01:16,  1.70it/s]Epoch: 15, train for the 99-th batch, train loss: 0.6100679636001587:  65%|███████▊    | 98/151 [00:21<00:12,  4.41it/s]Epoch: 15, train for the 99-th batch, train loss: 0.6100679636001587:  66%|███████▊    | 99/151 [00:21<00:11,  4.44it/s]Epoch: 3, train for the 292-th batch, train loss: 0.42147672176361084:  76%|███████▌  | 291/383 [02:54<00:56,  1.62it/s]Epoch: 3, train for the 292-th batch, train loss: 0.42147672176361084:  76%|███████▌  | 292/383 [02:54<00:53,  1.70it/s]Epoch: 5, train for the 106-th batch, train loss: 0.6320418119430542:  44%|████▊      | 105/237 [01:00<01:21,  1.63it/s]Epoch: 5, train for the 106-th batch, train loss: 0.6320418119430542:  45%|████▉      | 106/237 [01:00<01:24,  1.55it/s]Epoch: 15, train for the 100-th batch, train loss: 0.6627916097640991:  66%|███████▏   | 99/151 [00:21<00:11,  4.44it/s]Epoch: 15, train for the 100-th batch, train loss: 0.6627916097640991:  66%|██████▌   | 100/151 [00:21<00:11,  4.44it/s]Epoch: 9, train for the 47-th batch, train loss: 0.23006026446819305:  39%|████▋       | 46/119 [00:27<00:42,  1.71it/s]Epoch: 9, train for the 47-th batch, train loss: 0.23006026446819305:  39%|████▋       | 47/119 [00:27<00:42,  1.71it/s]Epoch: 15, train for the 101-th batch, train loss: 0.6691944599151611:  66%|██████▌   | 100/151 [00:22<00:11,  4.44it/s]Epoch: 15, train for the 101-th batch, train loss: 0.6691944599151611:  67%|██████▋   | 101/151 [00:22<00:11,  4.46it/s]Epoch: 8, train for the 17-th batch, train loss: 0.41268637776374817:  11%|█▎          | 16/146 [00:09<01:16,  1.70it/s]Epoch: 8, train for the 17-th batch, train loss: 0.41268637776374817:  12%|█▍          | 17/146 [00:09<01:15,  1.71it/s]Epoch: 5, train for the 107-th batch, train loss: 0.6075563430786133:  45%|████▉      | 106/237 [01:01<01:24,  1.55it/s]Epoch: 5, train for the 107-th batch, train loss: 0.6075563430786133:  45%|████▉      | 107/237 [01:01<01:22,  1.58it/s]Epoch: 3, train for the 293-th batch, train loss: 0.4191465675830841:  76%|████████▍  | 292/383 [02:54<00:53,  1.70it/s]Epoch: 3, train for the 293-th batch, train loss: 0.4191465675830841:  77%|████████▍  | 293/383 [02:54<00:54,  1.65it/s]Epoch: 15, train for the 102-th batch, train loss: 0.6022558212280273:  67%|██████▋   | 101/151 [00:22<00:11,  4.46it/s]Epoch: 15, train for the 102-th batch, train loss: 0.6022558212280273:  68%|██████▊   | 102/151 [00:22<00:10,  4.47it/s]Epoch: 8, train for the 18-th batch, train loss: 0.38592419028282166:  12%|█▍          | 17/146 [00:09<01:15,  1.71it/s]Epoch: 8, train for the 18-th batch, train loss: 0.38592419028282166:  12%|█▍          | 18/146 [00:09<01:01,  2.07it/s]Epoch: 15, train for the 103-th batch, train loss: 0.6121992468833923:  68%|██████▊   | 102/151 [00:22<00:10,  4.47it/s]Epoch: 15, train for the 103-th batch, train loss: 0.6121992468833923:  68%|██████▊   | 103/151 [00:22<00:10,  4.49it/s]Epoch: 15, train for the 104-th batch, train loss: 0.6419227719306946:  68%|██████▊   | 103/151 [00:22<00:10,  4.49it/s]Epoch: 15, train for the 104-th batch, train loss: 0.6419227719306946:  69%|██████▉   | 104/151 [00:22<00:10,  4.50it/s]Epoch: 8, train for the 19-th batch, train loss: 0.420926034450531:  12%|█▋            | 18/146 [00:10<01:01,  2.07it/s]Epoch: 8, train for the 19-th batch, train loss: 0.420926034450531:  13%|█▊            | 19/146 [00:10<00:55,  2.27it/s]Epoch: 5, train for the 108-th batch, train loss: 0.6178793907165527:  45%|████▉      | 107/237 [01:02<01:22,  1.58it/s]Epoch: 5, train for the 108-th batch, train loss: 0.6178793907165527:  46%|█████      | 108/237 [01:02<01:22,  1.57it/s]Epoch: 3, train for the 294-th batch, train loss: 0.31546375155448914:  77%|███████▋  | 293/383 [02:55<00:54,  1.65it/s]Epoch: 3, train for the 294-th batch, train loss: 0.31546375155448914:  77%|███████▋  | 294/383 [02:55<00:54,  1.63it/s]Epoch: 15, train for the 105-th batch, train loss: 0.5716962814331055:  69%|██████▉   | 104/151 [00:23<00:10,  4.50it/s]Epoch: 15, train for the 105-th batch, train loss: 0.5716962814331055:  70%|██████▉   | 105/151 [00:23<00:10,  4.50it/s]Epoch: 9, train for the 48-th batch, train loss: 0.21395671367645264:  39%|████▋       | 47/119 [00:29<00:42,  1.71it/s]Epoch: 9, train for the 48-th batch, train loss: 0.21395671367645264:  40%|████▊       | 48/119 [00:29<00:53,  1.33it/s]Epoch: 15, train for the 106-th batch, train loss: 0.5370528101921082:  70%|██████▉   | 105/151 [00:23<00:10,  4.50it/s]Epoch: 15, train for the 106-th batch, train loss: 0.5370528101921082:  70%|███████   | 106/151 [00:23<00:10,  4.50it/s]Epoch: 8, train for the 20-th batch, train loss: 0.46407201886177063:  13%|█▌          | 19/146 [00:10<00:55,  2.27it/s]Epoch: 8, train for the 20-th batch, train loss: 0.46407201886177063:  14%|█▋          | 20/146 [00:10<00:54,  2.30it/s]Epoch: 15, train for the 107-th batch, train loss: 0.5544871687889099:  70%|███████   | 106/151 [00:23<00:10,  4.50it/s]Epoch: 15, train for the 107-th batch, train loss: 0.5544871687889099:  71%|███████   | 107/151 [00:23<00:09,  4.50it/s]Epoch: 5, train for the 109-th batch, train loss: 0.643247663974762:  46%|█████▍      | 108/237 [01:02<01:22,  1.57it/s]Epoch: 5, train for the 109-th batch, train loss: 0.643247663974762:  46%|█████▌      | 109/237 [01:02<01:21,  1.57it/s]Epoch: 3, train for the 295-th batch, train loss: 0.37436166405677795:  77%|███████▋  | 294/383 [02:56<00:54,  1.63it/s]Epoch: 3, train for the 295-th batch, train loss: 0.37436166405677795:  77%|███████▋  | 295/383 [02:56<00:54,  1.60it/s]Epoch: 15, train for the 108-th batch, train loss: 0.5671480894088745:  71%|███████   | 107/151 [00:23<00:09,  4.50it/s]Epoch: 15, train for the 108-th batch, train loss: 0.5671480894088745:  72%|███████▏  | 108/151 [00:23<00:09,  4.50it/s]Epoch: 9, train for the 49-th batch, train loss: 0.21181875467300415:  40%|████▊       | 48/119 [00:29<00:53,  1.33it/s]Epoch: 9, train for the 49-th batch, train loss: 0.21181875467300415:  41%|████▉       | 49/119 [00:29<00:49,  1.41it/s]Epoch: 8, train for the 21-th batch, train loss: 0.425716757774353:  14%|█▉            | 20/146 [00:11<00:54,  2.30it/s]Epoch: 8, train for the 21-th batch, train loss: 0.425716757774353:  14%|██            | 21/146 [00:11<01:01,  2.05it/s]Epoch: 15, train for the 109-th batch, train loss: 0.5302178263664246:  72%|███████▏  | 108/151 [00:23<00:09,  4.50it/s]Epoch: 15, train for the 109-th batch, train loss: 0.5302178263664246:  72%|███████▏  | 109/151 [00:23<00:09,  4.50it/s]Epoch: 15, train for the 110-th batch, train loss: 0.5820913314819336:  72%|███████▏  | 109/151 [00:24<00:09,  4.50it/s]Epoch: 15, train for the 110-th batch, train loss: 0.5820913314819336:  73%|███████▎  | 110/151 [00:24<00:09,  4.50it/s]Epoch: 5, train for the 110-th batch, train loss: 0.6059801578521729:  46%|█████      | 109/237 [01:03<01:21,  1.57it/s]Epoch: 5, train for the 110-th batch, train loss: 0.6059801578521729:  46%|█████      | 110/237 [01:03<01:20,  1.57it/s]Epoch: 3, train for the 296-th batch, train loss: 0.3033387362957001:  77%|████████▍  | 295/383 [02:56<00:54,  1.60it/s]Epoch: 3, train for the 296-th batch, train loss: 0.3033387362957001:  77%|████████▌  | 296/383 [02:56<00:54,  1.59it/s]Epoch: 9, train for the 50-th batch, train loss: 0.18562261760234833:  41%|████▉       | 49/119 [00:30<00:49,  1.41it/s]Epoch: 9, train for the 50-th batch, train loss: 0.18562261760234833:  42%|█████       | 50/119 [00:30<00:46,  1.47it/s]Epoch: 15, train for the 111-th batch, train loss: 0.5381094813346863:  73%|███████▎  | 110/151 [00:24<00:09,  4.50it/s]Epoch: 15, train for the 111-th batch, train loss: 0.5381094813346863:  74%|███████▎  | 111/151 [00:24<00:08,  4.50it/s]Epoch: 8, train for the 22-th batch, train loss: 0.41025614738464355:  14%|█▋          | 21/146 [00:11<01:01,  2.05it/s]Epoch: 8, train for the 22-th batch, train loss: 0.41025614738464355:  15%|█▊          | 22/146 [00:11<01:05,  1.90it/s]Epoch: 15, train for the 112-th batch, train loss: 0.5200084447860718:  74%|███████▎  | 111/151 [00:24<00:08,  4.50it/s]Epoch: 15, train for the 112-th batch, train loss: 0.5200084447860718:  74%|███████▍  | 112/151 [00:24<00:08,  4.48it/s]Epoch: 15, train for the 113-th batch, train loss: 0.5030509829521179:  74%|███████▍  | 112/151 [00:24<00:08,  4.48it/s]Epoch: 15, train for the 113-th batch, train loss: 0.5030509829521179:  75%|███████▍  | 113/151 [00:24<00:09,  4.06it/s]Epoch: 5, train for the 111-th batch, train loss: 0.6274977922439575:  46%|█████      | 110/237 [01:04<01:20,  1.57it/s]Epoch: 5, train for the 111-th batch, train loss: 0.6274977922439575:  47%|█████▏     | 111/237 [01:04<01:20,  1.56it/s]Epoch: 3, train for the 297-th batch, train loss: 0.555229127407074:  77%|█████████▎  | 296/383 [02:57<00:54,  1.59it/s]Epoch: 3, train for the 297-th batch, train loss: 0.555229127407074:  78%|█████████▎  | 297/383 [02:57<00:54,  1.58it/s]Epoch: 9, train for the 51-th batch, train loss: 0.2230181097984314:  42%|█████▍       | 50/119 [00:30<00:46,  1.47it/s]Epoch: 9, train for the 51-th batch, train loss: 0.2230181097984314:  43%|█████▌       | 51/119 [00:30<00:44,  1.52it/s]Epoch: 8, train for the 23-th batch, train loss: 0.4610613286495209:  15%|█▉           | 22/146 [00:12<01:05,  1.90it/s]Epoch: 8, train for the 23-th batch, train loss: 0.4610613286495209:  16%|██           | 23/146 [00:12<01:06,  1.84it/s]Epoch: 15, train for the 114-th batch, train loss: 0.5122150778770447:  75%|███████▍  | 113/151 [00:25<00:09,  4.06it/s]Epoch: 15, train for the 114-th batch, train loss: 0.5122150778770447:  75%|███████▌  | 114/151 [00:25<00:08,  4.18it/s]Epoch: 15, train for the 115-th batch, train loss: 0.5315122604370117:  75%|███████▌  | 114/151 [00:25<00:08,  4.18it/s]Epoch: 15, train for the 115-th batch, train loss: 0.5315122604370117:  76%|███████▌  | 115/151 [00:25<00:08,  4.27it/s]Epoch: 9, train for the 52-th batch, train loss: 0.2739912271499634:  43%|█████▌       | 51/119 [00:31<00:44,  1.52it/s]Epoch: 9, train for the 52-th batch, train loss: 0.2739912271499634:  44%|█████▋       | 52/119 [00:31<00:42,  1.57it/s]Epoch: 15, train for the 116-th batch, train loss: 0.5097827315330505:  76%|███████▌  | 115/151 [00:25<00:08,  4.27it/s]Epoch: 15, train for the 116-th batch, train loss: 0.5097827315330505:  77%|███████▋  | 116/151 [00:25<00:08,  4.34it/s]Epoch: 5, train for the 112-th batch, train loss: 0.6539009213447571:  47%|█████▏     | 111/237 [01:04<01:20,  1.56it/s]Epoch: 5, train for the 112-th batch, train loss: 0.6539009213447571:  47%|█████▏     | 112/237 [01:04<01:20,  1.56it/s]Epoch: 3, train for the 298-th batch, train loss: 0.4136911630630493:  78%|████████▌  | 297/383 [02:58<00:54,  1.58it/s]Epoch: 3, train for the 298-th batch, train loss: 0.4136911630630493:  78%|████████▌  | 298/383 [02:58<00:54,  1.57it/s]Epoch: 8, train for the 24-th batch, train loss: 0.4221931993961334:  16%|██           | 23/146 [00:13<01:06,  1.84it/s]Epoch: 8, train for the 24-th batch, train loss: 0.4221931993961334:  16%|██▏          | 24/146 [00:13<01:08,  1.78it/s]Epoch: 15, train for the 117-th batch, train loss: 0.5146039724349976:  77%|███████▋  | 116/151 [00:25<00:08,  4.34it/s]Epoch: 15, train for the 117-th batch, train loss: 0.5146039724349976:  77%|███████▋  | 117/151 [00:25<00:07,  4.39it/s]Epoch: 15, train for the 118-th batch, train loss: 0.48745810985565186:  77%|██████▉  | 117/151 [00:26<00:07,  4.39it/s]Epoch: 15, train for the 118-th batch, train loss: 0.48745810985565186:  78%|███████  | 118/151 [00:26<00:07,  4.42it/s]Epoch: 9, train for the 53-th batch, train loss: 0.16787609457969666:  44%|█████▏      | 52/119 [00:32<00:42,  1.57it/s]Epoch: 9, train for the 53-th batch, train loss: 0.16787609457969666:  45%|█████▎      | 53/119 [00:32<00:41,  1.60it/s]Epoch: 5, train for the 113-th batch, train loss: 0.6029731035232544:  47%|█████▏     | 112/237 [01:05<01:20,  1.56it/s]Epoch: 5, train for the 113-th batch, train loss: 0.6029731035232544:  48%|█████▏     | 113/237 [01:05<01:19,  1.57it/s]Epoch: 3, train for the 299-th batch, train loss: 0.4534803330898285:  78%|████████▌  | 298/383 [02:58<00:54,  1.57it/s]Epoch: 3, train for the 299-th batch, train loss: 0.4534803330898285:  78%|████████▌  | 299/383 [02:58<00:53,  1.57it/s]Epoch: 15, train for the 119-th batch, train loss: 0.525273323059082:  78%|████████▌  | 118/151 [00:26<00:07,  4.42it/s]Epoch: 15, train for the 119-th batch, train loss: 0.525273323059082:  79%|████████▋  | 119/151 [00:26<00:07,  4.45it/s]Epoch: 8, train for the 25-th batch, train loss: 0.45403119921684265:  16%|█▉          | 24/146 [00:13<01:08,  1.78it/s]Epoch: 8, train for the 25-th batch, train loss: 0.45403119921684265:  17%|██          | 25/146 [00:13<01:09,  1.75it/s]Epoch: 15, train for the 120-th batch, train loss: 0.5775681734085083:  79%|███████▉  | 119/151 [00:26<00:07,  4.45it/s]Epoch: 15, train for the 120-th batch, train loss: 0.5775681734085083:  79%|███████▉  | 120/151 [00:26<00:06,  4.47it/s]Epoch: 15, train for the 121-th batch, train loss: 0.4760282039642334:  79%|███████▉  | 120/151 [00:26<00:06,  4.47it/s]Epoch: 15, train for the 121-th batch, train loss: 0.4760282039642334:  80%|████████  | 121/151 [00:26<00:06,  4.45it/s]Epoch: 9, train for the 54-th batch, train loss: 0.1720672994852066:  45%|█████▊       | 53/119 [00:32<00:41,  1.60it/s]Epoch: 9, train for the 54-th batch, train loss: 0.1720672994852066:  45%|█████▉       | 54/119 [00:32<00:40,  1.62it/s]Epoch: 5, train for the 114-th batch, train loss: 0.5949681997299194:  48%|█████▏     | 113/237 [01:06<01:19,  1.57it/s]Epoch: 5, train for the 114-th batch, train loss: 0.5949681997299194:  48%|█████▎     | 114/237 [01:06<01:18,  1.57it/s]Epoch: 3, train for the 300-th batch, train loss: 0.4266134202480316:  78%|████████▌  | 299/383 [02:59<00:53,  1.57it/s]Epoch: 3, train for the 300-th batch, train loss: 0.4266134202480316:  78%|████████▌  | 300/383 [02:59<00:52,  1.57it/s]Epoch: 8, train for the 26-th batch, train loss: 0.4185153841972351:  17%|██▏          | 25/146 [00:14<01:09,  1.75it/s]Epoch: 8, train for the 26-th batch, train loss: 0.4185153841972351:  18%|██▎          | 26/146 [00:14<01:09,  1.73it/s]Epoch: 15, train for the 122-th batch, train loss: 0.5629302263259888:  80%|████████  | 121/151 [00:26<00:06,  4.45it/s]Epoch: 15, train for the 122-th batch, train loss: 0.5629302263259888:  81%|████████  | 122/151 [00:26<00:06,  4.46it/s]Epoch: 15, train for the 123-th batch, train loss: 0.5475960373878479:  81%|████████  | 122/151 [00:27<00:06,  4.46it/s]Epoch: 15, train for the 123-th batch, train loss: 0.5475960373878479:  81%|████████▏ | 123/151 [00:27<00:06,  4.47it/s]Epoch: 9, train for the 55-th batch, train loss: 0.16826871037483215:  45%|█████▍      | 54/119 [00:33<00:40,  1.62it/s]Epoch: 9, train for the 55-th batch, train loss: 0.16826871037483215:  46%|█████▌      | 55/119 [00:33<00:39,  1.64it/s]Epoch: 15, train for the 124-th batch, train loss: 0.5429872274398804:  81%|████████▏ | 123/151 [00:27<00:06,  4.47it/s]Epoch: 15, train for the 124-th batch, train loss: 0.5429872274398804:  82%|████████▏ | 124/151 [00:27<00:06,  4.48it/s]Epoch: 5, train for the 115-th batch, train loss: 0.6100189089775085:  48%|█████▎     | 114/237 [01:06<01:18,  1.57it/s]Epoch: 5, train for the 115-th batch, train loss: 0.6100189089775085:  49%|█████▎     | 115/237 [01:06<01:17,  1.57it/s]Epoch: 3, train for the 301-th batch, train loss: 0.33593887090682983:  78%|███████▊  | 300/383 [03:00<00:52,  1.57it/s]Epoch: 3, train for the 301-th batch, train loss: 0.33593887090682983:  79%|███████▊  | 301/383 [03:00<00:52,  1.57it/s]Epoch: 8, train for the 27-th batch, train loss: 0.41722235083580017:  18%|██▏         | 26/146 [00:14<01:09,  1.73it/s]Epoch: 8, train for the 27-th batch, train loss: 0.41722235083580017:  18%|██▏         | 27/146 [00:14<01:09,  1.70it/s]Epoch: 15, train for the 125-th batch, train loss: 0.535997748374939:  82%|█████████  | 124/151 [00:27<00:06,  4.48it/s]Epoch: 15, train for the 125-th batch, train loss: 0.535997748374939:  83%|█████████  | 125/151 [00:27<00:05,  4.50it/s]Epoch: 15, train for the 126-th batch, train loss: 0.5532949566841125:  83%|████████▎ | 125/151 [00:27<00:05,  4.50it/s]Epoch: 15, train for the 126-th batch, train loss: 0.5532949566841125:  83%|████████▎ | 126/151 [00:27<00:05,  4.51it/s]Epoch: 9, train for the 56-th batch, train loss: 0.1918891817331314:  46%|██████       | 55/119 [00:33<00:39,  1.64it/s]Epoch: 9, train for the 56-th batch, train loss: 0.1918891817331314:  47%|██████       | 56/119 [00:33<00:38,  1.64it/s]Epoch: 15, train for the 127-th batch, train loss: 0.5381439328193665:  83%|████████▎ | 126/151 [00:28<00:05,  4.51it/s]Epoch: 15, train for the 127-th batch, train loss: 0.5381439328193665:  84%|████████▍ | 127/151 [00:28<00:05,  4.50it/s]Epoch: 8, train for the 28-th batch, train loss: 0.4181616008281708:  18%|██▍          | 27/146 [00:15<01:09,  1.70it/s]Epoch: 8, train for the 28-th batch, train loss: 0.4181616008281708:  19%|██▍          | 28/146 [00:15<01:10,  1.68it/s]Epoch: 5, train for the 116-th batch, train loss: 0.5992254614830017:  49%|█████▎     | 115/237 [01:07<01:17,  1.57it/s]Epoch: 5, train for the 116-th batch, train loss: 0.5992254614830017:  49%|█████▍     | 116/237 [01:07<01:17,  1.57it/s]Epoch: 3, train for the 302-th batch, train loss: 0.372755229473114:  79%|█████████▍  | 301/383 [03:00<00:52,  1.57it/s]Epoch: 3, train for the 302-th batch, train loss: 0.372755229473114:  79%|█████████▍  | 302/383 [03:00<00:51,  1.57it/s]Epoch: 15, train for the 128-th batch, train loss: 0.6018349528312683:  84%|████████▍ | 127/151 [00:28<00:05,  4.50it/s]Epoch: 15, train for the 128-th batch, train loss: 0.6018349528312683:  85%|████████▍ | 128/151 [00:28<00:05,  4.50it/s]Epoch: 15, train for the 129-th batch, train loss: 0.5381972193717957:  85%|████████▍ | 128/151 [00:28<00:05,  4.50it/s]Epoch: 15, train for the 129-th batch, train loss: 0.5381972193717957:  85%|████████▌ | 129/151 [00:28<00:04,  4.50it/s]Epoch: 9, train for the 57-th batch, train loss: 0.1706765741109848:  47%|██████       | 56/119 [00:34<00:38,  1.64it/s]Epoch: 9, train for the 57-th batch, train loss: 0.1706765741109848:  48%|██████▏      | 57/119 [00:34<00:37,  1.64it/s]Epoch: 15, train for the 130-th batch, train loss: 0.5429624319076538:  85%|████████▌ | 129/151 [00:28<00:04,  4.50it/s]Epoch: 15, train for the 130-th batch, train loss: 0.5429624319076538:  86%|████████▌ | 130/151 [00:28<00:04,  4.51it/s]Epoch: 8, train for the 29-th batch, train loss: 0.4474138021469116:  19%|██▍          | 28/146 [00:16<01:10,  1.68it/s]Epoch: 8, train for the 29-th batch, train loss: 0.4474138021469116:  20%|██▌          | 29/146 [00:16<01:09,  1.69it/s]Epoch: 5, train for the 117-th batch, train loss: 0.6193452477455139:  49%|█████▍     | 116/237 [01:07<01:17,  1.57it/s]Epoch: 5, train for the 117-th batch, train loss: 0.6193452477455139:  49%|█████▍     | 117/237 [01:07<01:16,  1.57it/s]Epoch: 3, train for the 303-th batch, train loss: 0.3686770796775818:  79%|████████▋  | 302/383 [03:01<00:51,  1.57it/s]Epoch: 3, train for the 303-th batch, train loss: 0.3686770796775818:  79%|████████▋  | 303/383 [03:01<00:50,  1.57it/s]Epoch: 15, train for the 131-th batch, train loss: 0.5055447816848755:  86%|████████▌ | 130/151 [00:28<00:04,  4.51it/s]Epoch: 15, train for the 131-th batch, train loss: 0.5055447816848755:  87%|████████▋ | 131/151 [00:28<00:04,  4.50it/s]Epoch: 15, train for the 132-th batch, train loss: 0.529211699962616:  87%|█████████▌ | 131/151 [00:29<00:04,  4.50it/s]Epoch: 15, train for the 132-th batch, train loss: 0.529211699962616:  87%|█████████▌ | 132/151 [00:29<00:04,  4.48it/s]Epoch: 9, train for the 58-th batch, train loss: 0.1566804200410843:  48%|██████▏      | 57/119 [00:35<00:37,  1.64it/s]Epoch: 9, train for the 58-th batch, train loss: 0.1566804200410843:  49%|██████▎      | 58/119 [00:35<00:36,  1.66it/s]Epoch: 8, train for the 30-th batch, train loss: 0.38912343978881836:  20%|██▍         | 29/146 [00:16<01:09,  1.69it/s]Epoch: 8, train for the 30-th batch, train loss: 0.38912343978881836:  21%|██▍         | 30/146 [00:16<01:09,  1.68it/s]Epoch: 15, train for the 133-th batch, train loss: 0.537039577960968:  87%|█████████▌ | 132/151 [00:29<00:04,  4.48it/s]Epoch: 15, train for the 133-th batch, train loss: 0.537039577960968:  88%|█████████▋ | 133/151 [00:29<00:04,  4.47it/s]Epoch: 5, train for the 118-th batch, train loss: 0.601621150970459:  49%|█████▉      | 117/237 [01:08<01:16,  1.57it/s]Epoch: 5, train for the 118-th batch, train loss: 0.601621150970459:  50%|█████▉      | 118/237 [01:08<01:16,  1.56it/s]Epoch: 3, train for the 304-th batch, train loss: 0.43883833289146423:  79%|███████▉  | 303/383 [03:02<00:50,  1.57it/s]Epoch: 3, train for the 304-th batch, train loss: 0.43883833289146423:  79%|███████▉  | 304/383 [03:02<00:50,  1.57it/s]Epoch: 15, train for the 134-th batch, train loss: 0.5359260439872742:  88%|████████▊ | 133/151 [00:29<00:04,  4.47it/s]Epoch: 15, train for the 134-th batch, train loss: 0.5359260439872742:  89%|████████▊ | 134/151 [00:29<00:03,  4.49it/s]Epoch: 9, train for the 59-th batch, train loss: 0.1819363385438919:  49%|██████▎      | 58/119 [00:35<00:36,  1.66it/s]Epoch: 9, train for the 59-th batch, train loss: 0.1819363385438919:  50%|██████▍      | 59/119 [00:35<00:36,  1.66it/s]Epoch: 15, train for the 135-th batch, train loss: 0.5394923090934753:  89%|████████▊ | 134/151 [00:29<00:03,  4.49it/s]Epoch: 15, train for the 135-th batch, train loss: 0.5394923090934753:  89%|████████▉ | 135/151 [00:29<00:03,  4.50it/s]Epoch: 8, train for the 31-th batch, train loss: 0.4778323173522949:  21%|██▋          | 30/146 [00:17<01:09,  1.68it/s]Epoch: 8, train for the 31-th batch, train loss: 0.4778323173522949:  21%|██▊          | 31/146 [00:17<01:08,  1.67it/s]Epoch: 15, train for the 136-th batch, train loss: 0.5625096559524536:  89%|████████▉ | 135/151 [00:30<00:03,  4.50it/s]Epoch: 15, train for the 136-th batch, train loss: 0.5625096559524536:  90%|█████████ | 136/151 [00:30<00:03,  4.51it/s]Epoch: 5, train for the 119-th batch, train loss: 0.629462480545044:  50%|█████▉      | 118/237 [01:09<01:16,  1.56it/s]Epoch: 5, train for the 119-th batch, train loss: 0.629462480545044:  50%|██████      | 119/237 [01:09<01:15,  1.56it/s]Epoch: 3, train for the 305-th batch, train loss: 0.5593364238739014:  79%|████████▋  | 304/383 [03:02<00:50,  1.57it/s]Epoch: 3, train for the 305-th batch, train loss: 0.5593364238739014:  80%|████████▊  | 305/383 [03:02<00:49,  1.56it/s]Epoch: 15, train for the 137-th batch, train loss: 0.6120820641517639:  90%|█████████ | 136/151 [00:30<00:03,  4.51it/s]Epoch: 15, train for the 137-th batch, train loss: 0.6120820641517639:  91%|█████████ | 137/151 [00:30<00:03,  4.50it/s]Epoch: 9, train for the 60-th batch, train loss: 0.13910770416259766:  50%|█████▉      | 59/119 [00:36<00:36,  1.66it/s]Epoch: 9, train for the 60-th batch, train loss: 0.13910770416259766:  50%|██████      | 60/119 [00:36<00:35,  1.66it/s]Epoch: 15, train for the 138-th batch, train loss: 0.5788474678993225:  91%|█████████ | 137/151 [00:30<00:03,  4.50it/s]Epoch: 15, train for the 138-th batch, train loss: 0.5788474678993225:  91%|█████████▏| 138/151 [00:30<00:02,  4.51it/s]Epoch: 8, train for the 32-th batch, train loss: 0.4459548890590668:  21%|██▊          | 31/146 [00:17<01:08,  1.67it/s]Epoch: 8, train for the 32-th batch, train loss: 0.4459548890590668:  22%|██▊          | 32/146 [00:17<01:08,  1.66it/s]Epoch: 15, train for the 139-th batch, train loss: 0.560572624206543:  91%|██████████ | 138/151 [00:30<00:02,  4.51it/s]Epoch: 15, train for the 139-th batch, train loss: 0.560572624206543:  92%|██████████▏| 139/151 [00:30<00:02,  4.52it/s]Epoch: 5, train for the 120-th batch, train loss: 0.6008905172348022:  50%|█████▌     | 119/237 [01:09<01:15,  1.56it/s]Epoch: 5, train for the 120-th batch, train loss: 0.6008905172348022:  51%|█████▌     | 120/237 [01:09<01:15,  1.55it/s]Epoch: 3, train for the 306-th batch, train loss: 0.35762274265289307:  80%|███████▉  | 305/383 [03:03<00:49,  1.56it/s]Epoch: 3, train for the 306-th batch, train loss: 0.35762274265289307:  80%|███████▉  | 306/383 [03:03<00:49,  1.56it/s]Epoch: 15, train for the 140-th batch, train loss: 0.48496490716934204:  92%|████████▎| 139/151 [00:30<00:02,  4.52it/s]Epoch: 15, train for the 140-th batch, train loss: 0.48496490716934204:  93%|████████▎| 140/151 [00:30<00:02,  4.52it/s]Epoch: 9, train for the 61-th batch, train loss: 0.13686774671077728:  50%|██████      | 60/119 [00:36<00:35,  1.66it/s]Epoch: 9, train for the 61-th batch, train loss: 0.13686774671077728:  51%|██████▏     | 61/119 [00:36<00:35,  1.64it/s]Epoch: 15, train for the 141-th batch, train loss: 0.5720446109771729:  93%|█████████▎| 140/151 [00:31<00:02,  4.52it/s]Epoch: 15, train for the 141-th batch, train loss: 0.5720446109771729:  93%|█████████▎| 141/151 [00:31<00:02,  4.52it/s]Epoch: 8, train for the 33-th batch, train loss: 0.44848325848579407:  22%|██▋         | 32/146 [00:18<01:08,  1.66it/s]Epoch: 8, train for the 33-th batch, train loss: 0.44848325848579407:  23%|██▋         | 33/146 [00:18<01:08,  1.65it/s]Epoch: 5, train for the 121-th batch, train loss: 0.572611927986145:  51%|██████      | 120/237 [01:10<01:15,  1.55it/s]Epoch: 5, train for the 121-th batch, train loss: 0.572611927986145:  51%|██████▏     | 121/237 [01:10<01:14,  1.56it/s]Epoch: 15, train for the 142-th batch, train loss: 0.5798233151435852:  93%|█████████▎| 141/151 [00:31<00:02,  4.52it/s]Epoch: 15, train for the 142-th batch, train loss: 0.5798233151435852:  94%|█████████▍| 142/151 [00:31<00:01,  4.50it/s]Epoch: 3, train for the 307-th batch, train loss: 0.3881518542766571:  80%|████████▊  | 306/383 [03:03<00:49,  1.56it/s]Epoch: 3, train for the 307-th batch, train loss: 0.3881518542766571:  80%|████████▊  | 307/383 [03:03<00:48,  1.56it/s]Epoch: 15, train for the 143-th batch, train loss: 0.46560320258140564:  94%|████████▍| 142/151 [00:31<00:01,  4.50it/s]Epoch: 15, train for the 143-th batch, train loss: 0.46560320258140564:  95%|████████▌| 143/151 [00:31<00:01,  4.51it/s]Epoch: 9, train for the 62-th batch, train loss: 0.2286291867494583:  51%|██████▋      | 61/119 [00:37<00:35,  1.64it/s]Epoch: 9, train for the 62-th batch, train loss: 0.2286291867494583:  52%|██████▊      | 62/119 [00:37<00:34,  1.63it/s]Epoch: 8, train for the 34-th batch, train loss: 0.4263625741004944:  23%|██▉          | 33/146 [00:19<01:08,  1.65it/s]Epoch: 8, train for the 34-th batch, train loss: 0.4263625741004944:  23%|███          | 34/146 [00:19<01:08,  1.64it/s]Epoch: 15, train for the 144-th batch, train loss: 0.5195692181587219:  95%|█████████▍| 143/151 [00:31<00:01,  4.51it/s]Epoch: 15, train for the 144-th batch, train loss: 0.5195692181587219:  95%|█████████▌| 144/151 [00:31<00:01,  4.50it/s]Epoch: 5, train for the 122-th batch, train loss: 0.6435498595237732:  51%|█████▌     | 121/237 [01:11<01:14,  1.56it/s]Epoch: 5, train for the 122-th batch, train loss: 0.6435498595237732:  51%|█████▋     | 122/237 [01:11<01:13,  1.56it/s]Epoch: 3, train for the 308-th batch, train loss: 0.4079322814941406:  80%|████████▊  | 307/383 [03:04<00:48,  1.56it/s]Epoch: 3, train for the 308-th batch, train loss: 0.4079322814941406:  80%|████████▊  | 308/383 [03:04<00:47,  1.57it/s]Epoch: 15, train for the 145-th batch, train loss: 0.5214036107063293:  95%|█████████▌| 144/151 [00:32<00:01,  4.50it/s]Epoch: 15, train for the 145-th batch, train loss: 0.5214036107063293:  96%|█████████▌| 145/151 [00:32<00:01,  4.48it/s]Epoch: 9, train for the 63-th batch, train loss: 0.14392951130867004:  52%|██████▎     | 62/119 [00:38<00:34,  1.63it/s]Epoch: 9, train for the 63-th batch, train loss: 0.14392951130867004:  53%|██████▎     | 63/119 [00:38<00:34,  1.63it/s]Epoch: 15, train for the 146-th batch, train loss: 0.5395625829696655:  96%|█████████▌| 145/151 [00:32<00:01,  4.48it/s]Epoch: 15, train for the 146-th batch, train loss: 0.5395625829696655:  97%|█████████▋| 146/151 [00:32<00:01,  4.49it/s]Epoch: 8, train for the 35-th batch, train loss: 0.4498097896575928:  23%|███          | 34/146 [00:19<01:08,  1.64it/s]Epoch: 8, train for the 35-th batch, train loss: 0.4498097896575928:  24%|███          | 35/146 [00:19<01:07,  1.64it/s]Epoch: 15, train for the 147-th batch, train loss: 0.5417311787605286:  97%|█████████▋| 146/151 [00:32<00:01,  4.49it/s]Epoch: 15, train for the 147-th batch, train loss: 0.5417311787605286:  97%|█████████▋| 147/151 [00:32<00:00,  4.49it/s]Epoch: 5, train for the 123-th batch, train loss: 0.589183509349823:  51%|██████▏     | 122/237 [01:11<01:13,  1.56it/s]Epoch: 5, train for the 123-th batch, train loss: 0.589183509349823:  52%|██████▏     | 123/237 [01:11<01:12,  1.57it/s]Epoch: 3, train for the 309-th batch, train loss: 0.4854053258895874:  80%|████████▊  | 308/383 [03:05<00:47,  1.57it/s]Epoch: 3, train for the 309-th batch, train loss: 0.4854053258895874:  81%|████████▊  | 309/383 [03:05<00:47,  1.56it/s]Epoch: 15, train for the 148-th batch, train loss: 0.5894460678100586:  97%|█████████▋| 147/151 [00:32<00:00,  4.49it/s]Epoch: 15, train for the 148-th batch, train loss: 0.5894460678100586:  98%|█████████▊| 148/151 [00:32<00:00,  4.49it/s]Epoch: 9, train for the 64-th batch, train loss: 0.14270220696926117:  53%|██████▎     | 63/119 [00:38<00:34,  1.63it/s]Epoch: 9, train for the 64-th batch, train loss: 0.14270220696926117:  54%|██████▍     | 64/119 [00:38<00:33,  1.63it/s]Epoch: 15, train for the 149-th batch, train loss: 0.5014384388923645:  98%|█████████▊| 148/151 [00:32<00:00,  4.49it/s]Epoch: 15, train for the 149-th batch, train loss: 0.5014384388923645:  99%|█████████▊| 149/151 [00:32<00:00,  4.49it/s]Epoch: 8, train for the 36-th batch, train loss: 0.42564985156059265:  24%|██▉         | 35/146 [00:20<01:07,  1.64it/s]Epoch: 8, train for the 36-th batch, train loss: 0.42564985156059265:  25%|██▉         | 36/146 [00:20<01:07,  1.63it/s]Epoch: 15, train for the 150-th batch, train loss: 0.49882856011390686:  99%|████████▉| 149/151 [00:33<00:00,  4.49it/s]Epoch: 15, train for the 150-th batch, train loss: 0.49882856011390686:  99%|████████▉| 150/151 [00:33<00:00,  4.50it/s]Epoch: 5, train for the 124-th batch, train loss: 0.6136677265167236:  52%|█████▋     | 123/237 [01:12<01:12,  1.57it/s]Epoch: 5, train for the 124-th batch, train loss: 0.6136677265167236:  52%|█████▊     | 124/237 [01:12<01:12,  1.56it/s]Epoch: 3, train for the 310-th batch, train loss: 0.5007559657096863:  81%|████████▊  | 309/383 [03:05<00:47,  1.56it/s]Epoch: 3, train for the 310-th batch, train loss: 0.5007559657096863:  81%|████████▉  | 310/383 [03:05<00:46,  1.56it/s]Epoch: 15, train for the 151-th batch, train loss: 0.613053560256958:  99%|██████████▉| 150/151 [00:33<00:00,  4.50it/s]Epoch: 15, train for the 151-th batch, train loss: 0.613053560256958: 100%|███████████| 151/151 [00:33<00:00,  4.99it/s]Epoch: 15, train for the 151-th batch, train loss: 0.613053560256958: 100%|███████████| 151/151 [00:33<00:00,  4.53it/s]
  0%|                                                                                            | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.492764949798584:   0%|                             | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.492764949798584:   2%|▍                    | 1/46 [00:00<00:04,  9.51it/s]Epoch: 9, train for the 65-th batch, train loss: 0.1564653366804123:  54%|██████▉      | 64/119 [00:39<00:33,  1.63it/s]Epoch: 9, train for the 65-th batch, train loss: 0.1564653366804123:  55%|███████      | 65/119 [00:39<00:33,  1.63it/s]evaluate for the 2-th batch, evaluate loss: 0.4997212886810303:   2%|▍                   | 1/46 [00:00<00:04,  9.51it/s]evaluate for the 2-th batch, evaluate loss: 0.4997212886810303:   4%|▊                   | 2/46 [00:00<00:04,  9.63it/s]evaluate for the 3-th batch, evaluate loss: 0.4804081618785858:   4%|▊                   | 2/46 [00:00<00:04,  9.63it/s]evaluate for the 3-th batch, evaluate loss: 0.4804081618785858:   7%|█▎                  | 3/46 [00:00<00:04,  9.63it/s]Epoch: 8, train for the 37-th batch, train loss: 0.4477696120738983:  25%|███▏         | 36/146 [00:20<01:07,  1.63it/s]Epoch: 8, train for the 37-th batch, train loss: 0.4477696120738983:  25%|███▎         | 37/146 [00:20<01:06,  1.63it/s]evaluate for the 4-th batch, evaluate loss: 0.5104429721832275:   7%|█▎                  | 3/46 [00:00<00:04,  9.63it/s]evaluate for the 4-th batch, evaluate loss: 0.5104429721832275:   9%|█▋                  | 4/46 [00:00<00:04,  9.68it/s]evaluate for the 5-th batch, evaluate loss: 0.47605475783348083:   9%|█▋                 | 4/46 [00:00<00:04,  9.68it/s]evaluate for the 5-th batch, evaluate loss: 0.47605475783348083:  11%|██                 | 5/46 [00:00<00:04,  9.68it/s]evaluate for the 6-th batch, evaluate loss: 0.5490441918373108:  11%|██▏                 | 5/46 [00:00<00:04,  9.68it/s]evaluate for the 6-th batch, evaluate loss: 0.5490441918373108:  13%|██▌                 | 6/46 [00:00<00:04,  9.71it/s]Epoch: 5, train for the 125-th batch, train loss: 0.6026307344436646:  52%|█████▊     | 124/237 [01:13<01:12,  1.56it/s]Epoch: 5, train for the 125-th batch, train loss: 0.6026307344436646:  53%|█████▊     | 125/237 [01:13<01:11,  1.56it/s]Epoch: 3, train for the 311-th batch, train loss: 0.45034369826316833:  81%|████████  | 310/383 [03:06<00:46,  1.56it/s]Epoch: 3, train for the 311-th batch, train loss: 0.45034369826316833:  81%|████████  | 311/383 [03:06<00:46,  1.56it/s]evaluate for the 7-th batch, evaluate loss: 0.47040727734565735:  13%|██▍                | 6/46 [00:00<00:04,  9.71it/s]evaluate for the 7-th batch, evaluate loss: 0.47040727734565735:  15%|██▉                | 7/46 [00:00<00:04,  9.72it/s]Epoch: 9, train for the 66-th batch, train loss: 0.18134555220603943:  55%|██████▌     | 65/119 [00:39<00:33,  1.63it/s]Epoch: 9, train for the 66-th batch, train loss: 0.18134555220603943:  55%|██████▋     | 66/119 [00:39<00:32,  1.63it/s]evaluate for the 8-th batch, evaluate loss: 0.5570504069328308:  15%|███                 | 7/46 [00:00<00:04,  9.72it/s]evaluate for the 8-th batch, evaluate loss: 0.5570504069328308:  17%|███▍                | 8/46 [00:00<00:03,  9.71it/s]evaluate for the 9-th batch, evaluate loss: 0.5247873663902283:  17%|███▍                | 8/46 [00:00<00:03,  9.71it/s]evaluate for the 9-th batch, evaluate loss: 0.5247873663902283:  20%|███▉                | 9/46 [00:00<00:03,  9.71it/s]Epoch: 8, train for the 38-th batch, train loss: 0.47659438848495483:  25%|███         | 37/146 [00:21<01:06,  1.63it/s]Epoch: 8, train for the 38-th batch, train loss: 0.47659438848495483:  26%|███         | 38/146 [00:21<01:06,  1.63it/s]evaluate for the 10-th batch, evaluate loss: 0.5309481620788574:  20%|███▋               | 9/46 [00:01<00:03,  9.71it/s]evaluate for the 10-th batch, evaluate loss: 0.5309481620788574:  22%|███▉              | 10/46 [00:01<00:03,  9.71it/s]evaluate for the 11-th batch, evaluate loss: 0.5229587554931641:  22%|███▉              | 10/46 [00:01<00:03,  9.71it/s]evaluate for the 11-th batch, evaluate loss: 0.5229587554931641:  24%|████▎             | 11/46 [00:01<00:03,  9.67it/s]evaluate for the 12-th batch, evaluate loss: 0.4706379473209381:  24%|████▎             | 11/46 [00:01<00:03,  9.67it/s]evaluate for the 12-th batch, evaluate loss: 0.4706379473209381:  26%|████▋             | 12/46 [00:01<00:03,  9.67it/s]Epoch: 5, train for the 126-th batch, train loss: 0.5684250593185425:  53%|█████▊     | 125/237 [01:13<01:11,  1.56it/s]Epoch: 5, train for the 126-th batch, train loss: 0.5684250593185425:  53%|█████▊     | 126/237 [01:13<01:10,  1.56it/s]Epoch: 3, train for the 312-th batch, train loss: 0.32387110590934753:  81%|████████  | 311/383 [03:07<00:46,  1.56it/s]Epoch: 3, train for the 312-th batch, train loss: 0.32387110590934753:  81%|████████▏ | 312/383 [03:07<00:45,  1.56it/s]evaluate for the 13-th batch, evaluate loss: 0.49457821249961853:  26%|████▍            | 12/46 [00:01<00:03,  9.67it/s]evaluate for the 13-th batch, evaluate loss: 0.49457821249961853:  28%|████▊            | 13/46 [00:01<00:03,  9.67it/s]Epoch: 9, train for the 67-th batch, train loss: 0.23399652540683746:  55%|██████▋     | 66/119 [00:40<00:32,  1.63it/s]Epoch: 9, train for the 67-th batch, train loss: 0.23399652540683746:  56%|██████▊     | 67/119 [00:40<00:31,  1.63it/s]evaluate for the 14-th batch, evaluate loss: 0.5840157270431519:  28%|█████             | 13/46 [00:01<00:03,  9.67it/s]evaluate for the 14-th batch, evaluate loss: 0.5840157270431519:  30%|█████▍            | 14/46 [00:01<00:03,  9.67it/s]Epoch: 8, train for the 39-th batch, train loss: 0.4666171967983246:  26%|███▍         | 38/146 [00:22<01:06,  1.63it/s]Epoch: 8, train for the 39-th batch, train loss: 0.4666171967983246:  27%|███▍         | 39/146 [00:22<01:05,  1.63it/s]evaluate for the 15-th batch, evaluate loss: 0.5384893417358398:  30%|█████▍            | 14/46 [00:01<00:03,  9.67it/s]evaluate for the 15-th batch, evaluate loss: 0.5384893417358398:  33%|█████▊            | 15/46 [00:01<00:03,  9.66it/s]evaluate for the 16-th batch, evaluate loss: 0.569138765335083:  33%|██████▏            | 15/46 [00:01<00:03,  9.66it/s]evaluate for the 16-th batch, evaluate loss: 0.569138765335083:  35%|██████▌            | 16/46 [00:01<00:03,  9.62it/s]evaluate for the 17-th batch, evaluate loss: 0.45028606057167053:  35%|█████▉           | 16/46 [00:01<00:03,  9.62it/s]evaluate for the 17-th batch, evaluate loss: 0.45028606057167053:  37%|██████▎          | 17/46 [00:01<00:03,  9.64it/s]evaluate for the 18-th batch, evaluate loss: 0.5020093321800232:  37%|██████▋           | 17/46 [00:01<00:03,  9.64it/s]evaluate for the 18-th batch, evaluate loss: 0.5020093321800232:  39%|███████           | 18/46 [00:01<00:02,  9.63it/s]Epoch: 5, train for the 127-th batch, train loss: 0.6144704222679138:  53%|█████▊     | 126/237 [01:14<01:10,  1.56it/s]Epoch: 5, train for the 127-th batch, train loss: 0.6144704222679138:  54%|█████▉     | 127/237 [01:14<01:10,  1.56it/s]Epoch: 3, train for the 313-th batch, train loss: 0.4640769958496094:  81%|████████▉  | 312/383 [03:07<00:45,  1.56it/s]Epoch: 3, train for the 313-th batch, train loss: 0.4640769958496094:  82%|████████▉  | 313/383 [03:07<00:44,  1.57it/s]evaluate for the 19-th batch, evaluate loss: 0.5193813443183899:  39%|███████           | 18/46 [00:01<00:02,  9.63it/s]evaluate for the 19-th batch, evaluate loss: 0.5193813443183899:  41%|███████▍          | 19/46 [00:01<00:02,  9.65it/s]Epoch: 9, train for the 68-th batch, train loss: 0.11971767246723175:  56%|██████▊     | 67/119 [00:41<00:31,  1.63it/s]Epoch: 9, train for the 68-th batch, train loss: 0.11971767246723175:  57%|██████▊     | 68/119 [00:41<00:31,  1.63it/s]evaluate for the 20-th batch, evaluate loss: 0.532649040222168:  41%|███████▊           | 19/46 [00:02<00:02,  9.65it/s]evaluate for the 20-th batch, evaluate loss: 0.532649040222168:  43%|████████▎          | 20/46 [00:02<00:02,  9.69it/s]Epoch: 8, train for the 40-th batch, train loss: 0.48468199372291565:  27%|███▏        | 39/146 [00:22<01:05,  1.63it/s]Epoch: 8, train for the 40-th batch, train loss: 0.48468199372291565:  27%|███▎        | 40/146 [00:22<01:05,  1.63it/s]evaluate for the 21-th batch, evaluate loss: 0.5229905843734741:  43%|███████▊          | 20/46 [00:02<00:02,  9.69it/s]evaluate for the 21-th batch, evaluate loss: 0.5229905843734741:  46%|████████▏         | 21/46 [00:02<00:02,  9.66it/s]evaluate for the 22-th batch, evaluate loss: 0.5188609957695007:  46%|████████▏         | 21/46 [00:02<00:02,  9.66it/s]evaluate for the 22-th batch, evaluate loss: 0.5188609957695007:  48%|████████▌         | 22/46 [00:02<00:02,  9.67it/s]evaluate for the 23-th batch, evaluate loss: 0.4738902151584625:  48%|████████▌         | 22/46 [00:02<00:02,  9.67it/s]evaluate for the 23-th batch, evaluate loss: 0.4738902151584625:  50%|█████████         | 23/46 [00:02<00:02,  9.62it/s]evaluate for the 24-th batch, evaluate loss: 0.4803227186203003:  50%|█████████         | 23/46 [00:02<00:02,  9.62it/s]evaluate for the 24-th batch, evaluate loss: 0.4803227186203003:  52%|█████████▍        | 24/46 [00:02<00:02,  9.65it/s]Epoch: 5, train for the 128-th batch, train loss: 0.6198218464851379:  54%|█████▉     | 127/237 [01:15<01:10,  1.56it/s]Epoch: 5, train for the 128-th batch, train loss: 0.6198218464851379:  54%|█████▉     | 128/237 [01:15<01:09,  1.56it/s]Epoch: 3, train for the 314-th batch, train loss: 0.40506353974342346:  82%|████████▏ | 313/383 [03:08<00:44,  1.57it/s]Epoch: 3, train for the 314-th batch, train loss: 0.40506353974342346:  82%|████████▏ | 314/383 [03:08<00:44,  1.56it/s]evaluate for the 25-th batch, evaluate loss: 0.5337952971458435:  52%|█████████▍        | 24/46 [00:02<00:02,  9.65it/s]evaluate for the 25-th batch, evaluate loss: 0.5337952971458435:  54%|█████████▊        | 25/46 [00:02<00:02,  9.66it/s]Epoch: 9, train for the 69-th batch, train loss: 0.13746587932109833:  57%|██████▊     | 68/119 [00:41<00:31,  1.63it/s]Epoch: 9, train for the 69-th batch, train loss: 0.13746587932109833:  58%|██████▉     | 69/119 [00:41<00:30,  1.63it/s]evaluate for the 26-th batch, evaluate loss: 0.5507475733757019:  54%|█████████▊        | 25/46 [00:02<00:02,  9.66it/s]evaluate for the 26-th batch, evaluate loss: 0.5507475733757019:  57%|██████████▏       | 26/46 [00:02<00:02,  9.68it/s]Epoch: 8, train for the 41-th batch, train loss: 0.4229685962200165:  27%|███▌         | 40/146 [00:23<01:05,  1.63it/s]Epoch: 8, train for the 41-th batch, train loss: 0.4229685962200165:  28%|███▋         | 41/146 [00:23<01:04,  1.63it/s]evaluate for the 27-th batch, evaluate loss: 0.4931820333003998:  57%|██████████▏       | 26/46 [00:02<00:02,  9.68it/s]evaluate for the 27-th batch, evaluate loss: 0.4931820333003998:  59%|██████████▌       | 27/46 [00:02<00:01,  9.67it/s]evaluate for the 28-th batch, evaluate loss: 0.516596794128418:  59%|███████████▏       | 27/46 [00:02<00:01,  9.67it/s]evaluate for the 28-th batch, evaluate loss: 0.516596794128418:  61%|███████████▌       | 28/46 [00:02<00:01,  9.69it/s]evaluate for the 29-th batch, evaluate loss: 0.4896901249885559:  61%|██████████▉       | 28/46 [00:02<00:01,  9.69it/s]evaluate for the 29-th batch, evaluate loss: 0.4896901249885559:  63%|███████████▎      | 29/46 [00:02<00:01,  9.71it/s]evaluate for the 30-th batch, evaluate loss: 0.4916963577270508:  63%|███████████▎      | 29/46 [00:03<00:01,  9.71it/s]evaluate for the 30-th batch, evaluate loss: 0.4916963577270508:  65%|███████████▋      | 30/46 [00:03<00:01,  9.71it/s]Epoch: 5, train for the 129-th batch, train loss: 0.6248534917831421:  54%|█████▉     | 128/237 [01:15<01:09,  1.56it/s]Epoch: 5, train for the 129-th batch, train loss: 0.6248534917831421:  54%|█████▉     | 129/237 [01:15<01:09,  1.56it/s]Epoch: 3, train for the 315-th batch, train loss: 0.4736177325248718:  82%|█████████  | 314/383 [03:09<00:44,  1.56it/s]Epoch: 3, train for the 315-th batch, train loss: 0.4736177325248718:  82%|█████████  | 315/383 [03:09<00:43,  1.56it/s]evaluate for the 31-th batch, evaluate loss: 0.5196799635887146:  65%|███████████▋      | 30/46 [00:03<00:01,  9.71it/s]evaluate for the 31-th batch, evaluate loss: 0.5196799635887146:  67%|████████████▏     | 31/46 [00:03<00:01,  9.72it/s]Epoch: 9, train for the 70-th batch, train loss: 0.11273125559091568:  58%|██████▉     | 69/119 [00:42<00:30,  1.63it/s]Epoch: 9, train for the 70-th batch, train loss: 0.11273125559091568:  59%|███████     | 70/119 [00:42<00:30,  1.62it/s]evaluate for the 32-th batch, evaluate loss: 0.4778854250907898:  67%|████████████▏     | 31/46 [00:03<00:01,  9.72it/s]evaluate for the 32-th batch, evaluate loss: 0.4778854250907898:  70%|████████████▌     | 32/46 [00:03<00:01,  9.72it/s]Epoch: 8, train for the 42-th batch, train loss: 0.47698742151260376:  28%|███▎        | 41/146 [00:24<01:04,  1.63it/s]Epoch: 8, train for the 42-th batch, train loss: 0.47698742151260376:  29%|███▍        | 42/146 [00:24<01:04,  1.62it/s]evaluate for the 33-th batch, evaluate loss: 0.4920692443847656:  70%|████████████▌     | 32/46 [00:03<00:01,  9.72it/s]evaluate for the 33-th batch, evaluate loss: 0.4920692443847656:  72%|████████████▉     | 33/46 [00:03<00:01,  9.71it/s]evaluate for the 34-th batch, evaluate loss: 0.4787605106830597:  72%|████████████▉     | 33/46 [00:03<00:01,  9.71it/s]evaluate for the 34-th batch, evaluate loss: 0.4787605106830597:  74%|█████████████▎    | 34/46 [00:03<00:01,  9.70it/s]evaluate for the 35-th batch, evaluate loss: 0.476449191570282:  74%|██████████████     | 34/46 [00:03<00:01,  9.70it/s]evaluate for the 35-th batch, evaluate loss: 0.476449191570282:  76%|██████████████▍    | 35/46 [00:03<00:01,  9.70it/s]evaluate for the 36-th batch, evaluate loss: 0.46474742889404297:  76%|████████████▉    | 35/46 [00:03<00:01,  9.70it/s]evaluate for the 36-th batch, evaluate loss: 0.46474742889404297:  78%|█████████████▎   | 36/46 [00:03<00:01,  9.72it/s]evaluate for the 37-th batch, evaluate loss: 0.5015493035316467:  78%|██████████████    | 36/46 [00:03<00:01,  9.72it/s]evaluate for the 37-th batch, evaluate loss: 0.5015493035316467:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.70it/s]Epoch: 5, train for the 130-th batch, train loss: 0.6434500813484192:  54%|█████▉     | 129/237 [01:16<01:09,  1.56it/s]Epoch: 5, train for the 130-th batch, train loss: 0.6434500813484192:  55%|██████     | 130/237 [01:16<01:08,  1.56it/s]Epoch: 3, train for the 316-th batch, train loss: 0.35561835765838623:  82%|████████▏ | 315/383 [03:09<00:43,  1.56it/s]Epoch: 3, train for the 316-th batch, train loss: 0.35561835765838623:  83%|████████▎ | 316/383 [03:09<00:43,  1.56it/s]Epoch: 9, train for the 71-th batch, train loss: 0.22042544186115265:  59%|███████     | 70/119 [00:43<00:30,  1.62it/s]Epoch: 9, train for the 71-th batch, train loss: 0.22042544186115265:  60%|███████▏    | 71/119 [00:43<00:29,  1.62it/s]evaluate for the 38-th batch, evaluate loss: 0.5315088629722595:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.70it/s]evaluate for the 38-th batch, evaluate loss: 0.5315088629722595:  83%|██████████████▊   | 38/46 [00:03<00:00,  9.71it/s]Epoch: 8, train for the 43-th batch, train loss: 0.4587792158126831:  29%|███▋         | 42/146 [00:24<01:04,  1.62it/s]Epoch: 8, train for the 43-th batch, train loss: 0.4587792158126831:  29%|███▊         | 43/146 [00:24<01:03,  1.62it/s]evaluate for the 39-th batch, evaluate loss: 0.5328249335289001:  83%|██████████████▊   | 38/46 [00:04<00:00,  9.71it/s]evaluate for the 39-th batch, evaluate loss: 0.5328249335289001:  85%|███████████████▎  | 39/46 [00:04<00:00,  9.71it/s]evaluate for the 40-th batch, evaluate loss: 0.4668610990047455:  85%|███████████████▎  | 39/46 [00:04<00:00,  9.71it/s]evaluate for the 40-th batch, evaluate loss: 0.4668610990047455:  87%|███████████████▋  | 40/46 [00:04<00:00,  9.71it/s]evaluate for the 41-th batch, evaluate loss: 0.4787980020046234:  87%|███████████████▋  | 40/46 [00:04<00:00,  9.71it/s]evaluate for the 41-th batch, evaluate loss: 0.4787980020046234:  89%|████████████████  | 41/46 [00:04<00:00,  9.72it/s]evaluate for the 42-th batch, evaluate loss: 0.46856197714805603:  89%|███████████████▏ | 41/46 [00:04<00:00,  9.72it/s]evaluate for the 42-th batch, evaluate loss: 0.46856197714805603:  91%|███████████████▌ | 42/46 [00:04<00:00,  9.74it/s]evaluate for the 43-th batch, evaluate loss: 0.5316588282585144:  91%|████████████████▍ | 42/46 [00:04<00:00,  9.74it/s]evaluate for the 43-th batch, evaluate loss: 0.5316588282585144:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.70it/s]Epoch: 5, train for the 131-th batch, train loss: 0.6026551723480225:  55%|██████     | 130/237 [01:16<01:08,  1.56it/s]Epoch: 9, train for the 72-th batch, train loss: 0.21374090015888214:  60%|███████▏    | 71/119 [00:43<00:29,  1.62it/s]Epoch: 5, train for the 131-th batch, train loss: 0.6026551723480225:  55%|██████     | 131/237 [01:16<01:08,  1.56it/s]Epoch: 9, train for the 72-th batch, train loss: 0.21374090015888214:  61%|███████▎    | 72/119 [00:43<00:28,  1.62it/s]Epoch: 3, train for the 317-th batch, train loss: 0.3413555920124054:  83%|█████████  | 316/383 [03:10<00:43,  1.56it/s]Epoch: 3, train for the 317-th batch, train loss: 0.3413555920124054:  83%|█████████  | 317/383 [03:10<00:42,  1.56it/s]evaluate for the 44-th batch, evaluate loss: 0.5125108361244202:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.70it/s]evaluate for the 44-th batch, evaluate loss: 0.5125108361244202:  96%|█████████████████▏| 44/46 [00:04<00:00,  9.67it/s]Epoch: 8, train for the 44-th batch, train loss: 0.4106556475162506:  29%|███▊         | 43/146 [00:25<01:03,  1.62it/s]Epoch: 8, train for the 44-th batch, train loss: 0.4106556475162506:  30%|███▉         | 44/146 [00:25<01:02,  1.62it/s]evaluate for the 45-th batch, evaluate loss: 0.49350592494010925:  96%|████████████████▎| 44/46 [00:04<00:00,  9.67it/s]evaluate for the 45-th batch, evaluate loss: 0.49350592494010925:  98%|████████████████▋| 45/46 [00:04<00:00,  9.64it/s]evaluate for the 46-th batch, evaluate loss: 0.5039920806884766:  98%|█████████████████▌| 45/46 [00:04<00:00,  9.64it/s]evaluate for the 46-th batch, evaluate loss: 0.5039920806884766: 100%|██████████████████| 46/46 [00:04<00:00,  9.71it/s]
  0%|                                                                                            | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.636961042881012:   0%|                             | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.636961042881012:   4%|▊                    | 1/25 [00:00<00:02,  9.27it/s]evaluate for the 2-th batch, evaluate loss: 0.6496075391769409:   4%|▊                   | 1/25 [00:00<00:02,  9.27it/s]evaluate for the 2-th batch, evaluate loss: 0.6496075391769409:   8%|█▌                  | 2/25 [00:00<00:02,  9.23it/s]evaluate for the 3-th batch, evaluate loss: 0.6886499524116516:   8%|█▌                  | 2/25 [00:00<00:02,  9.23it/s]evaluate for the 3-th batch, evaluate loss: 0.6886499524116516:  12%|██▍                 | 3/25 [00:00<00:02,  9.22it/s]Epoch: 9, train for the 73-th batch, train loss: 0.16726864874362946:  61%|███████▎    | 72/119 [00:44<00:28,  1.62it/s]Epoch: 9, train for the 73-th batch, train loss: 0.16726864874362946:  61%|███████▎    | 73/119 [00:44<00:28,  1.62it/s]Epoch: 5, train for the 132-th batch, train loss: 0.6235108375549316:  55%|██████     | 131/237 [01:17<01:08,  1.56it/s]Epoch: 5, train for the 132-th batch, train loss: 0.6235108375549316:  56%|██████▏    | 132/237 [01:17<01:07,  1.55it/s]Epoch: 3, train for the 318-th batch, train loss: 0.4017578661441803:  83%|█████████  | 317/383 [03:11<00:42,  1.56it/s]Epoch: 3, train for the 318-th batch, train loss: 0.4017578661441803:  83%|█████████▏ | 318/383 [03:11<00:41,  1.55it/s]evaluate for the 4-th batch, evaluate loss: 0.674692690372467:  12%|██▌                  | 3/25 [00:00<00:02,  9.22it/s]evaluate for the 4-th batch, evaluate loss: 0.674692690372467:  16%|███▎                 | 4/25 [00:00<00:02,  9.25it/s]Epoch: 8, train for the 45-th batch, train loss: 0.4361996352672577:  30%|███▉         | 44/146 [00:25<01:02,  1.62it/s]Epoch: 8, train for the 45-th batch, train loss: 0.4361996352672577:  31%|████         | 45/146 [00:25<01:02,  1.63it/s]evaluate for the 5-th batch, evaluate loss: 0.671657383441925:  16%|███▎                 | 4/25 [00:00<00:02,  9.25it/s]evaluate for the 5-th batch, evaluate loss: 0.671657383441925:  20%|████▏                | 5/25 [00:00<00:02,  9.23it/s]evaluate for the 6-th batch, evaluate loss: 0.71559077501297:  20%|████▍                 | 5/25 [00:00<00:02,  9.23it/s]evaluate for the 6-th batch, evaluate loss: 0.71559077501297:  24%|█████▎                | 6/25 [00:00<00:02,  9.23it/s]evaluate for the 7-th batch, evaluate loss: 0.7356181144714355:  24%|████▊               | 6/25 [00:00<00:02,  9.23it/s]evaluate for the 7-th batch, evaluate loss: 0.7356181144714355:  28%|█████▌              | 7/25 [00:00<00:01,  9.24it/s]evaluate for the 8-th batch, evaluate loss: 0.7210157513618469:  28%|█████▌              | 7/25 [00:00<00:01,  9.24it/s]evaluate for the 8-th batch, evaluate loss: 0.7210157513618469:  32%|██████▍             | 8/25 [00:00<00:01,  9.26it/s]Epoch: 9, train for the 74-th batch, train loss: 0.2042582631111145:  61%|███████▉     | 73/119 [00:44<00:28,  1.62it/s]Epoch: 9, train for the 74-th batch, train loss: 0.2042582631111145:  62%|████████     | 74/119 [00:44<00:27,  1.63it/s]evaluate for the 9-th batch, evaluate loss: 0.6991357207298279:  32%|██████▍             | 8/25 [00:00<00:01,  9.26it/s]evaluate for the 9-th batch, evaluate loss: 0.6991357207298279:  36%|███████▏            | 9/25 [00:00<00:01,  9.27it/s]Epoch: 5, train for the 133-th batch, train loss: 0.6072266697883606:  56%|██████▏    | 132/237 [01:18<01:07,  1.55it/s]Epoch: 5, train for the 133-th batch, train loss: 0.6072266697883606:  56%|██████▏    | 133/237 [01:18<01:06,  1.55it/s]Epoch: 3, train for the 319-th batch, train loss: 0.4942210912704468:  83%|█████████▏ | 318/383 [03:11<00:41,  1.55it/s]Epoch: 3, train for the 319-th batch, train loss: 0.4942210912704468:  83%|█████████▏ | 319/383 [03:11<00:41,  1.55it/s]evaluate for the 10-th batch, evaluate loss: 0.7292867302894592:  36%|██████▊            | 9/25 [00:01<00:01,  9.27it/s]evaluate for the 10-th batch, evaluate loss: 0.7292867302894592:  40%|███████▏          | 10/25 [00:01<00:01,  9.27it/s]Epoch: 8, train for the 46-th batch, train loss: 0.4627518355846405:  31%|████         | 45/146 [00:26<01:02,  1.63it/s]Epoch: 8, train for the 46-th batch, train loss: 0.4627518355846405:  32%|████         | 46/146 [00:26<01:01,  1.63it/s]evaluate for the 11-th batch, evaluate loss: 0.7297930717468262:  40%|███████▏          | 10/25 [00:01<00:01,  9.27it/s]evaluate for the 11-th batch, evaluate loss: 0.7297930717468262:  44%|███████▉          | 11/25 [00:01<00:01,  9.17it/s]evaluate for the 12-th batch, evaluate loss: 0.6983183026313782:  44%|███████▉          | 11/25 [00:01<00:01,  9.17it/s]evaluate for the 12-th batch, evaluate loss: 0.6983183026313782:  48%|████████▋         | 12/25 [00:01<00:01,  9.17it/s]evaluate for the 13-th batch, evaluate loss: 0.6625145077705383:  48%|████████▋         | 12/25 [00:01<00:01,  9.17it/s]evaluate for the 13-th batch, evaluate loss: 0.6625145077705383:  52%|█████████▎        | 13/25 [00:01<00:01,  9.20it/s]evaluate for the 14-th batch, evaluate loss: 0.7509537935256958:  52%|█████████▎        | 13/25 [00:01<00:01,  9.20it/s]evaluate for the 14-th batch, evaluate loss: 0.7509537935256958:  56%|██████████        | 14/25 [00:01<00:01,  9.24it/s]Epoch: 9, train for the 75-th batch, train loss: 0.16145317256450653:  62%|███████▍    | 74/119 [00:45<00:27,  1.63it/s]Epoch: 9, train for the 75-th batch, train loss: 0.16145317256450653:  63%|███████▌    | 75/119 [00:45<00:27,  1.63it/s]evaluate for the 15-th batch, evaluate loss: 0.7251270413398743:  56%|██████████        | 14/25 [00:01<00:01,  9.24it/s]evaluate for the 15-th batch, evaluate loss: 0.7251270413398743:  60%|██████████▊       | 15/25 [00:01<00:01,  9.25it/s]Epoch: 5, train for the 134-th batch, train loss: 0.6529775857925415:  56%|██████▏    | 133/237 [01:18<01:06,  1.55it/s]Epoch: 5, train for the 134-th batch, train loss: 0.6529775857925415:  57%|██████▏    | 134/237 [01:18<01:06,  1.55it/s]Epoch: 3, train for the 320-th batch, train loss: 0.5497003793716431:  83%|█████████▏ | 319/383 [03:12<00:41,  1.55it/s]Epoch: 3, train for the 320-th batch, train loss: 0.5497003793716431:  84%|█████████▏ | 320/383 [03:12<00:40,  1.55it/s]Epoch: 8, train for the 47-th batch, train loss: 0.4924062490463257:  32%|████         | 46/146 [00:27<01:01,  1.63it/s]Epoch: 8, train for the 47-th batch, train loss: 0.4924062490463257:  32%|████▏        | 47/146 [00:27<01:00,  1.63it/s]evaluate for the 16-th batch, evaluate loss: 0.6589190363883972:  60%|██████████▊       | 15/25 [00:01<00:01,  9.25it/s]evaluate for the 16-th batch, evaluate loss: 0.6589190363883972:  64%|███████████▌      | 16/25 [00:01<00:00,  9.24it/s]evaluate for the 17-th batch, evaluate loss: 0.6591857075691223:  64%|███████████▌      | 16/25 [00:01<00:00,  9.24it/s]evaluate for the 17-th batch, evaluate loss: 0.6591857075691223:  68%|████████████▏     | 17/25 [00:01<00:00,  9.23it/s]evaluate for the 18-th batch, evaluate loss: 0.62923264503479:  68%|█████████████▌      | 17/25 [00:01<00:00,  9.23it/s]evaluate for the 18-th batch, evaluate loss: 0.62923264503479:  72%|██████████████▍     | 18/25 [00:01<00:00,  9.21it/s]evaluate for the 19-th batch, evaluate loss: 0.5877566933631897:  72%|████████████▉     | 18/25 [00:02<00:00,  9.21it/s]evaluate for the 19-th batch, evaluate loss: 0.5877566933631897:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.13it/s]evaluate for the 20-th batch, evaluate loss: 0.6554991006851196:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.13it/s]evaluate for the 20-th batch, evaluate loss: 0.6554991006851196:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.13it/s]Epoch: 9, train for the 76-th batch, train loss: 0.1914902627468109:  63%|████████▏    | 75/119 [00:46<00:27,  1.63it/s]Epoch: 9, train for the 76-th batch, train loss: 0.1914902627468109:  64%|████████▎    | 76/119 [00:46<00:26,  1.63it/s]evaluate for the 21-th batch, evaluate loss: 0.7250179648399353:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.13it/s]evaluate for the 21-th batch, evaluate loss: 0.7250179648399353:  84%|███████████████   | 21/25 [00:02<00:00,  9.17it/s]Epoch: 5, train for the 135-th batch, train loss: 0.6472135186195374:  57%|██████▏    | 134/237 [01:19<01:06,  1.55it/s]Epoch: 5, train for the 135-th batch, train loss: 0.6472135186195374:  57%|██████▎    | 135/237 [01:19<01:05,  1.55it/s]Epoch: 3, train for the 321-th batch, train loss: 0.5843960642814636:  84%|█████████▏ | 320/383 [03:12<00:40,  1.55it/s]Epoch: 3, train for the 321-th batch, train loss: 0.5843960642814636:  84%|█████████▏ | 321/383 [03:12<00:39,  1.55it/s]Epoch: 8, train for the 48-th batch, train loss: 0.4721125364303589:  32%|████▏        | 47/146 [00:27<01:00,  1.63it/s]Epoch: 8, train for the 48-th batch, train loss: 0.4721125364303589:  33%|████▎        | 48/146 [00:27<01:00,  1.63it/s]evaluate for the 22-th batch, evaluate loss: 0.6008682250976562:  84%|███████████████   | 21/25 [00:02<00:00,  9.17it/s]evaluate for the 22-th batch, evaluate loss: 0.6008682250976562:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.20it/s]evaluate for the 23-th batch, evaluate loss: 0.6597174406051636:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.20it/s]evaluate for the 23-th batch, evaluate loss: 0.6597174406051636:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.22it/s]evaluate for the 24-th batch, evaluate loss: 0.659026026725769:  92%|█████████████████▍ | 23/25 [00:02<00:00,  9.22it/s]evaluate for the 24-th batch, evaluate loss: 0.659026026725769:  96%|██████████████████▏| 24/25 [00:02<00:00,  9.21it/s]evaluate for the 25-th batch, evaluate loss: 0.705783486366272:  96%|██████████████████▏| 24/25 [00:02<00:00,  9.21it/s]evaluate for the 25-th batch, evaluate loss: 0.705783486366272: 100%|███████████████████| 25/25 [00:02<00:00,  9.29it/s]
INFO:root:Epoch: 15, learning rate: 0.0001, train loss: 0.5613
INFO:root:train average_precision, 0.8180
INFO:root:train roc_auc, 0.7817
INFO:root:validate loss: 0.5061
INFO:root:validate average_precision, 0.8443
INFO:root:validate roc_auc, 0.8051
INFO:root:new node validate loss: 0.6812
INFO:root:new node validate first_1_average_precision, 0.5990
INFO:root:new node validate first_1_roc_auc, 0.5433
INFO:root:new node validate first_3_average_precision, 0.6821
INFO:root:new node validate first_3_roc_auc, 0.6402
INFO:root:new node validate first_10_average_precision, 0.7489
INFO:root:new node validate first_10_roc_auc, 0.7123
INFO:root:new node validate average_precision, 0.7120
INFO:root:new node validate roc_auc, 0.6586
INFO:root:save model ./saved_models/DyGFormer/ia-retweet-pol/DyGFormer_seed0_dygformer-ia-retweet-pol-old/DyGFormer_seed0_dygformer-ia-retweet-pol-old.pkl
Epoch: 9, train for the 77-th batch, train loss: 0.14197713136672974:  64%|███████▋    | 76/119 [00:46<00:26,  1.63it/s]Epoch: 9, train for the 77-th batch, train loss: 0.14197713136672974:  65%|███████▊    | 77/119 [00:46<00:25,  1.63it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]Epoch: 8, train for the 49-th batch, train loss: 0.5108156204223633:  33%|████▎        | 48/146 [00:28<01:00,  1.63it/s]Epoch: 8, train for the 49-th batch, train loss: 0.5108156204223633:  34%|████▎        | 49/146 [00:28<00:59,  1.63it/s]Epoch: 5, train for the 136-th batch, train loss: 0.6259081363677979:  57%|██████▎    | 135/237 [01:20<01:05,  1.55it/s]Epoch: 5, train for the 136-th batch, train loss: 0.6259081363677979:  57%|██████▎    | 136/237 [01:20<01:05,  1.55it/s]Epoch: 3, train for the 322-th batch, train loss: 0.528838574886322:  84%|██████████  | 321/383 [03:13<00:39,  1.55it/s]Epoch: 3, train for the 322-th batch, train loss: 0.528838574886322:  84%|██████████  | 322/383 [03:13<00:39,  1.55it/s]Epoch: 16, train for the 1-th batch, train loss: 1.1277542114257812:   0%|                      | 0/151 [00:00<?, ?it/s]Epoch: 16, train for the 1-th batch, train loss: 1.1277542114257812:   1%|              | 1/151 [00:00<00:25,  5.92it/s]Epoch: 16, train for the 2-th batch, train loss: 1.1694719791412354:   1%|              | 1/151 [00:00<00:25,  5.92it/s]Epoch: 16, train for the 2-th batch, train loss: 1.1694719791412354:   1%|▏             | 2/151 [00:00<00:25,  5.74it/s]Epoch: 16, train for the 3-th batch, train loss: 0.5147283673286438:   1%|▏             | 2/151 [00:00<00:25,  5.74it/s]Epoch: 16, train for the 3-th batch, train loss: 0.5147283673286438:   2%|▎             | 3/151 [00:00<00:24,  5.95it/s]Epoch: 9, train for the 78-th batch, train loss: 0.180652916431427:  65%|█████████     | 77/119 [00:47<00:25,  1.63it/s]Epoch: 9, train for the 78-th batch, train loss: 0.180652916431427:  66%|█████████▏    | 78/119 [00:47<00:25,  1.63it/s]Epoch: 16, train for the 4-th batch, train loss: 0.47583526372909546:   2%|▎            | 3/151 [00:00<00:24,  5.95it/s]Epoch: 16, train for the 4-th batch, train loss: 0.47583526372909546:   3%|▎            | 4/151 [00:00<00:24,  5.98it/s]Epoch: 8, train for the 50-th batch, train loss: 0.5156562328338623:  34%|████▎        | 49/146 [00:28<00:59,  1.63it/s]Epoch: 8, train for the 50-th batch, train loss: 0.5156562328338623:  34%|████▍        | 50/146 [00:28<00:58,  1.63it/s]Epoch: 5, train for the 137-th batch, train loss: 0.6534914970397949:  57%|██████▎    | 136/237 [01:20<01:05,  1.55it/s]Epoch: 5, train for the 137-th batch, train loss: 0.6534914970397949:  58%|██████▎    | 137/237 [01:20<01:04,  1.55it/s]Epoch: 3, train for the 323-th batch, train loss: 0.4277326762676239:  84%|█████████▏ | 322/383 [03:14<00:39,  1.55it/s]Epoch: 3, train for the 323-th batch, train loss: 0.4277326762676239:  84%|█████████▎ | 323/383 [03:14<00:38,  1.55it/s]Epoch: 16, train for the 5-th batch, train loss: 0.520576536655426:   3%|▍              | 4/151 [00:00<00:24,  5.98it/s]Epoch: 16, train for the 5-th batch, train loss: 0.520576536655426:   3%|▍              | 5/151 [00:00<00:25,  5.74it/s]Epoch: 16, train for the 6-th batch, train loss: 0.509304940700531:   3%|▍              | 5/151 [00:01<00:25,  5.74it/s]Epoch: 16, train for the 6-th batch, train loss: 0.509304940700531:   4%|▌              | 6/151 [00:01<00:25,  5.62it/s]Epoch: 9, train for the 79-th batch, train loss: 0.17161144316196442:  66%|███████▊    | 78/119 [00:47<00:25,  1.63it/s]Epoch: 9, train for the 79-th batch, train loss: 0.17161144316196442:  66%|███████▉    | 79/119 [00:47<00:24,  1.63it/s]Epoch: 16, train for the 7-th batch, train loss: 0.46795013546943665:   4%|▌            | 6/151 [00:01<00:25,  5.62it/s]Epoch: 16, train for the 7-th batch, train loss: 0.46795013546943665:   5%|▌            | 7/151 [00:01<00:26,  5.45it/s]Epoch: 8, train for the 51-th batch, train loss: 0.45049721002578735:  34%|████        | 50/146 [00:29<00:58,  1.63it/s]Epoch: 8, train for the 51-th batch, train loss: 0.45049721002578735:  35%|████▏       | 51/146 [00:29<00:58,  1.63it/s]Epoch: 5, train for the 138-th batch, train loss: 0.6732673645019531:  58%|██████▎    | 137/237 [01:21<01:04,  1.55it/s]Epoch: 5, train for the 138-th batch, train loss: 0.6732673645019531:  58%|██████▍    | 138/237 [01:21<01:03,  1.55it/s]Epoch: 3, train for the 324-th batch, train loss: 0.33370718359947205:  84%|████████▍ | 323/383 [03:14<00:38,  1.55it/s]Epoch: 3, train for the 324-th batch, train loss: 0.33370718359947205:  85%|████████▍ | 324/383 [03:14<00:37,  1.55it/s]Epoch: 16, train for the 8-th batch, train loss: 0.7037970423698425:   5%|▋             | 7/151 [00:01<00:26,  5.45it/s]Epoch: 16, train for the 8-th batch, train loss: 0.7037970423698425:   5%|▋             | 8/151 [00:01<00:26,  5.37it/s]Epoch: 16, train for the 9-th batch, train loss: 0.666138768196106:   5%|▊              | 8/151 [00:01<00:26,  5.37it/s]Epoch: 16, train for the 9-th batch, train loss: 0.666138768196106:   6%|▉              | 9/151 [00:01<00:27,  5.23it/s]Epoch: 9, train for the 80-th batch, train loss: 0.16677476465702057:  66%|███████▉    | 79/119 [00:48<00:24,  1.63it/s]Epoch: 9, train for the 80-th batch, train loss: 0.16677476465702057:  67%|████████    | 80/119 [00:48<00:23,  1.63it/s]Epoch: 16, train for the 10-th batch, train loss: 0.5925407409667969:   6%|▊            | 9/151 [00:01<00:27,  5.23it/s]Epoch: 16, train for the 10-th batch, train loss: 0.5925407409667969:   7%|▊           | 10/151 [00:01<00:27,  5.12it/s]Epoch: 8, train for the 52-th batch, train loss: 0.4922386109828949:  35%|████▌        | 51/146 [00:30<00:58,  1.63it/s]Epoch: 8, train for the 52-th batch, train loss: 0.4922386109828949:  36%|████▋        | 52/146 [00:30<00:57,  1.63it/s]Epoch: 16, train for the 11-th batch, train loss: 0.5223832726478577:   7%|▊           | 10/151 [00:02<00:27,  5.12it/s]Epoch: 16, train for the 11-th batch, train loss: 0.5223832726478577:   7%|▊           | 11/151 [00:02<00:27,  5.11it/s]Epoch: 5, train for the 139-th batch, train loss: 0.6266562938690186:  58%|██████▍    | 138/237 [01:22<01:03,  1.55it/s]Epoch: 5, train for the 139-th batch, train loss: 0.6266562938690186:  59%|██████▍    | 139/237 [01:22<01:02,  1.56it/s]Epoch: 3, train for the 325-th batch, train loss: 0.37509435415267944:  85%|████████▍ | 324/383 [03:15<00:37,  1.55it/s]Epoch: 3, train for the 325-th batch, train loss: 0.37509435415267944:  85%|████████▍ | 325/383 [03:15<00:37,  1.56it/s]Epoch: 16, train for the 12-th batch, train loss: 0.6729834079742432:   7%|▊           | 11/151 [00:02<00:27,  5.11it/s]Epoch: 16, train for the 12-th batch, train loss: 0.6729834079742432:   8%|▉           | 12/151 [00:02<00:27,  5.03it/s]Epoch: 9, train for the 81-th batch, train loss: 0.19967950880527496:  67%|████████    | 80/119 [00:49<00:23,  1.63it/s]Epoch: 9, train for the 81-th batch, train loss: 0.19967950880527496:  68%|████████▏   | 81/119 [00:49<00:23,  1.63it/s]Epoch: 16, train for the 13-th batch, train loss: 0.5590839385986328:   8%|▉           | 12/151 [00:02<00:27,  5.03it/s]Epoch: 16, train for the 13-th batch, train loss: 0.5590839385986328:   9%|█           | 13/151 [00:02<00:27,  5.04it/s]Epoch: 8, train for the 53-th batch, train loss: 0.4123331606388092:  36%|████▋        | 52/146 [00:30<00:57,  1.63it/s]Epoch: 8, train for the 53-th batch, train loss: 0.4123331606388092:  36%|████▋        | 53/146 [00:30<00:57,  1.63it/s]Epoch: 16, train for the 14-th batch, train loss: 0.5646989345550537:   9%|█           | 13/151 [00:02<00:27,  5.04it/s]Epoch: 16, train for the 14-th batch, train loss: 0.5646989345550537:   9%|█           | 14/151 [00:02<00:27,  5.03it/s]Epoch: 5, train for the 140-th batch, train loss: 0.6104412078857422:  59%|██████▍    | 139/237 [01:22<01:02,  1.56it/s]Epoch: 5, train for the 140-th batch, train loss: 0.6104412078857422:  59%|██████▍    | 140/237 [01:22<01:02,  1.55it/s]Epoch: 3, train for the 326-th batch, train loss: 0.42370253801345825:  85%|████████▍ | 325/383 [03:16<00:37,  1.56it/s]Epoch: 3, train for the 326-th batch, train loss: 0.42370253801345825:  85%|████████▌ | 326/383 [03:16<00:36,  1.55it/s]Epoch: 16, train for the 15-th batch, train loss: 0.540949285030365:   9%|█▏           | 14/151 [00:02<00:27,  5.03it/s]Epoch: 16, train for the 15-th batch, train loss: 0.540949285030365:  10%|█▎           | 15/151 [00:02<00:27,  4.97it/s]Epoch: 9, train for the 82-th batch, train loss: 0.1935071498155594:  68%|████████▊    | 81/119 [00:49<00:23,  1.63it/s]Epoch: 9, train for the 82-th batch, train loss: 0.1935071498155594:  69%|████████▉    | 82/119 [00:49<00:22,  1.63it/s]Epoch: 16, train for the 16-th batch, train loss: 0.5528861284255981:  10%|█▏          | 15/151 [00:03<00:27,  4.97it/s]Epoch: 16, train for the 16-th batch, train loss: 0.5528861284255981:  11%|█▎          | 16/151 [00:03<00:27,  4.96it/s]Epoch: 8, train for the 54-th batch, train loss: 0.5113358497619629:  36%|████▋        | 53/146 [00:31<00:57,  1.63it/s]Epoch: 8, train for the 54-th batch, train loss: 0.5113358497619629:  37%|████▊        | 54/146 [00:31<00:56,  1.63it/s]Epoch: 16, train for the 17-th batch, train loss: 0.5647256374359131:  11%|█▎          | 16/151 [00:03<00:27,  4.96it/s]Epoch: 16, train for the 17-th batch, train loss: 0.5647256374359131:  11%|█▎          | 17/151 [00:03<00:26,  4.99it/s]Epoch: 5, train for the 141-th batch, train loss: 0.6456091403961182:  59%|██████▍    | 140/237 [01:23<01:02,  1.55it/s]Epoch: 5, train for the 141-th batch, train loss: 0.6456091403961182:  59%|██████▌    | 141/237 [01:23<01:01,  1.55it/s]Epoch: 3, train for the 327-th batch, train loss: 0.483690470457077:  85%|██████████▏ | 326/383 [03:16<00:36,  1.55it/s]Epoch: 3, train for the 327-th batch, train loss: 0.483690470457077:  85%|██████████▏ | 327/383 [03:16<00:36,  1.55it/s]Epoch: 16, train for the 18-th batch, train loss: 0.5159367918968201:  11%|█▎          | 17/151 [00:03<00:26,  4.99it/s]Epoch: 16, train for the 18-th batch, train loss: 0.5159367918968201:  12%|█▍          | 18/151 [00:03<00:26,  5.00it/s]Epoch: 16, train for the 19-th batch, train loss: 0.5000130534172058:  12%|█▍          | 18/151 [00:03<00:26,  5.00it/s]Epoch: 16, train for the 19-th batch, train loss: 0.5000130534172058:  13%|█▌          | 19/151 [00:03<00:26,  4.98it/s]Epoch: 9, train for the 83-th batch, train loss: 0.18399205803871155:  69%|████████▎   | 82/119 [00:50<00:22,  1.63it/s]Epoch: 9, train for the 83-th batch, train loss: 0.18399205803871155:  70%|████████▎   | 83/119 [00:50<00:22,  1.63it/s]Epoch: 8, train for the 55-th batch, train loss: 0.5074472427368164:  37%|████▊        | 54/146 [00:32<00:56,  1.63it/s]Epoch: 8, train for the 55-th batch, train loss: 0.5074472427368164:  38%|████▉        | 55/146 [00:32<00:55,  1.63it/s]Epoch: 16, train for the 20-th batch, train loss: 0.45962560176849365:  13%|█▍         | 19/151 [00:03<00:26,  4.98it/s]Epoch: 16, train for the 20-th batch, train loss: 0.45962560176849365:  13%|█▍         | 20/151 [00:03<00:26,  4.91it/s]Epoch: 5, train for the 142-th batch, train loss: 0.6528830528259277:  59%|██████▌    | 141/237 [01:24<01:01,  1.55it/s]Epoch: 5, train for the 142-th batch, train loss: 0.6528830528259277:  60%|██████▌    | 142/237 [01:24<01:01,  1.55it/s]Epoch: 3, train for the 328-th batch, train loss: 0.43329286575317383:  85%|████████▌ | 327/383 [03:17<00:36,  1.55it/s]Epoch: 3, train for the 328-th batch, train loss: 0.43329286575317383:  86%|████████▌ | 328/383 [03:17<00:35,  1.55it/s]Epoch: 16, train for the 21-th batch, train loss: 0.4995855987071991:  13%|█▌          | 20/151 [00:04<00:26,  4.91it/s]Epoch: 16, train for the 21-th batch, train loss: 0.4995855987071991:  14%|█▋          | 21/151 [00:04<00:26,  4.86it/s]Epoch: 9, train for the 84-th batch, train loss: 0.15367373824119568:  70%|████████▎   | 83/119 [00:51<00:22,  1.63it/s]Epoch: 9, train for the 84-th batch, train loss: 0.15367373824119568:  71%|████████▍   | 84/119 [00:51<00:21,  1.63it/s]Epoch: 16, train for the 22-th batch, train loss: 0.5618792772293091:  14%|█▋          | 21/151 [00:04<00:26,  4.86it/s]Epoch: 16, train for the 22-th batch, train loss: 0.5618792772293091:  15%|█▋          | 22/151 [00:04<00:26,  4.82it/s]Epoch: 8, train for the 56-th batch, train loss: 0.5071538686752319:  38%|████▉        | 55/146 [00:32<00:55,  1.63it/s]Epoch: 8, train for the 56-th batch, train loss: 0.5071538686752319:  38%|████▉        | 56/146 [00:32<00:55,  1.63it/s]Epoch: 16, train for the 23-th batch, train loss: 0.5516033172607422:  15%|█▋          | 22/151 [00:04<00:26,  4.82it/s]Epoch: 16, train for the 23-th batch, train loss: 0.5516033172607422:  15%|█▊          | 23/151 [00:04<00:26,  4.80it/s]Epoch: 5, train for the 143-th batch, train loss: 0.6173192262649536:  60%|██████▌    | 142/237 [01:24<01:01,  1.55it/s]Epoch: 3, train for the 329-th batch, train loss: 0.4113062918186188:  86%|█████████▍ | 328/383 [03:18<00:35,  1.55it/s]Epoch: 5, train for the 143-th batch, train loss: 0.6173192262649536:  60%|██████▋    | 143/237 [01:24<01:01,  1.54it/s]Epoch: 3, train for the 329-th batch, train loss: 0.4113062918186188:  86%|█████████▍ | 329/383 [03:18<00:35,  1.54it/s]Epoch: 16, train for the 24-th batch, train loss: 0.5253759622573853:  15%|█▊          | 23/151 [00:04<00:26,  4.80it/s]Epoch: 16, train for the 24-th batch, train loss: 0.5253759622573853:  16%|█▉          | 24/151 [00:04<00:26,  4.79it/s]Epoch: 9, train for the 85-th batch, train loss: 0.16075552999973297:  71%|████████▍   | 84/119 [00:51<00:21,  1.63it/s]Epoch: 9, train for the 85-th batch, train loss: 0.16075552999973297:  71%|████████▌   | 85/119 [00:51<00:20,  1.63it/s]Epoch: 16, train for the 25-th batch, train loss: 0.4353391230106354:  16%|█▉          | 24/151 [00:04<00:26,  4.79it/s]Epoch: 16, train for the 25-th batch, train loss: 0.4353391230106354:  17%|█▉          | 25/151 [00:04<00:26,  4.80it/s]Epoch: 8, train for the 57-th batch, train loss: 0.5139298439025879:  38%|████▉        | 56/146 [00:33<00:55,  1.63it/s]Epoch: 8, train for the 57-th batch, train loss: 0.5139298439025879:  39%|█████        | 57/146 [00:33<00:54,  1.63it/s]Epoch: 16, train for the 26-th batch, train loss: 0.4273882210254669:  17%|█▉          | 25/151 [00:05<00:26,  4.80it/s]Epoch: 16, train for the 26-th batch, train loss: 0.4273882210254669:  17%|██          | 26/151 [00:05<00:25,  4.81it/s]Epoch: 3, train for the 330-th batch, train loss: 0.471029132604599:  86%|██████████▎ | 329/383 [03:18<00:35,  1.54it/s]Epoch: 5, train for the 144-th batch, train loss: 0.650348961353302:  60%|███████▏    | 143/237 [01:25<01:01,  1.54it/s]Epoch: 3, train for the 330-th batch, train loss: 0.471029132604599:  86%|██████████▎ | 330/383 [03:18<00:34,  1.54it/s]Epoch: 5, train for the 144-th batch, train loss: 0.650348961353302:  61%|███████▎    | 144/237 [01:25<01:00,  1.54it/s]Epoch: 16, train for the 27-th batch, train loss: 0.3411800265312195:  17%|██          | 26/151 [00:05<00:25,  4.81it/s]Epoch: 16, train for the 27-th batch, train loss: 0.3411800265312195:  18%|██▏         | 27/151 [00:05<00:25,  4.81it/s]Epoch: 9, train for the 86-th batch, train loss: 0.175714910030365:  71%|██████████    | 85/119 [00:52<00:20,  1.63it/s]Epoch: 9, train for the 86-th batch, train loss: 0.175714910030365:  72%|██████████    | 86/119 [00:52<00:20,  1.63it/s]Epoch: 16, train for the 28-th batch, train loss: 0.7896347641944885:  18%|██▏         | 27/151 [00:05<00:25,  4.81it/s]Epoch: 16, train for the 28-th batch, train loss: 0.7896347641944885:  19%|██▏         | 28/151 [00:05<00:26,  4.72it/s]Epoch: 8, train for the 58-th batch, train loss: 0.5003166198730469:  39%|█████        | 57/146 [00:33<00:54,  1.63it/s]Epoch: 8, train for the 58-th batch, train loss: 0.5003166198730469:  40%|█████▏       | 58/146 [00:33<00:54,  1.63it/s]Epoch: 16, train for the 29-th batch, train loss: 0.481865257024765:  19%|██▍          | 28/151 [00:05<00:26,  4.72it/s]Epoch: 16, train for the 29-th batch, train loss: 0.481865257024765:  19%|██▍          | 29/151 [00:05<00:25,  4.74it/s]Epoch: 16, train for the 30-th batch, train loss: 0.6933066248893738:  19%|██▎         | 29/151 [00:05<00:25,  4.74it/s]Epoch: 16, train for the 30-th batch, train loss: 0.6933066248893738:  20%|██▍         | 30/151 [00:05<00:25,  4.68it/s]Epoch: 3, train for the 331-th batch, train loss: 0.46955370903015137:  86%|████████▌ | 330/383 [03:19<00:34,  1.54it/s]Epoch: 5, train for the 145-th batch, train loss: 0.6623310446739197:  61%|██████▋    | 144/237 [01:26<01:00,  1.54it/s]Epoch: 3, train for the 331-th batch, train loss: 0.46955370903015137:  86%|████████▋ | 331/383 [03:19<00:33,  1.53it/s]Epoch: 5, train for the 145-th batch, train loss: 0.6623310446739197:  61%|██████▋    | 145/237 [01:26<01:00,  1.53it/s]Epoch: 9, train for the 87-th batch, train loss: 0.19452877342700958:  72%|████████▋   | 86/119 [00:52<00:20,  1.63it/s]Epoch: 9, train for the 87-th batch, train loss: 0.19452877342700958:  73%|████████▊   | 87/119 [00:52<00:19,  1.63it/s]Epoch: 16, train for the 31-th batch, train loss: 0.6783839464187622:  20%|██▍         | 30/151 [00:06<00:25,  4.68it/s]Epoch: 16, train for the 31-th batch, train loss: 0.6783839464187622:  21%|██▍         | 31/151 [00:06<00:26,  4.59it/s]Epoch: 8, train for the 59-th batch, train loss: 0.4193568229675293:  40%|█████▏       | 58/146 [00:34<00:54,  1.63it/s]Epoch: 8, train for the 59-th batch, train loss: 0.4193568229675293:  40%|█████▎       | 59/146 [00:34<00:53,  1.63it/s]Epoch: 16, train for the 32-th batch, train loss: 0.5266489386558533:  21%|██▍         | 31/151 [00:06<00:26,  4.59it/s]Epoch: 16, train for the 32-th batch, train loss: 0.5266489386558533:  21%|██▌         | 32/151 [00:06<00:25,  4.62it/s]Epoch: 16, train for the 33-th batch, train loss: 0.5290221571922302:  21%|██▌         | 32/151 [00:06<00:25,  4.62it/s]Epoch: 16, train for the 33-th batch, train loss: 0.5290221571922302:  22%|██▌         | 33/151 [00:06<00:25,  4.63it/s]Epoch: 3, train for the 332-th batch, train loss: 0.49035826325416565:  86%|████████▋ | 331/383 [03:20<00:33,  1.53it/s]Epoch: 5, train for the 146-th batch, train loss: 0.6195033192634583:  61%|██████▋    | 145/237 [01:26<01:00,  1.53it/s]Epoch: 3, train for the 332-th batch, train loss: 0.49035826325416565:  87%|████████▋ | 332/383 [03:20<00:33,  1.53it/s]Epoch: 5, train for the 146-th batch, train loss: 0.6195033192634583:  62%|██████▊    | 146/237 [01:26<00:59,  1.53it/s]Epoch: 9, train for the 88-th batch, train loss: 0.20905518531799316:  73%|████████▊   | 87/119 [00:53<00:19,  1.63it/s]Epoch: 9, train for the 88-th batch, train loss: 0.20905518531799316:  74%|████████▊   | 88/119 [00:53<00:19,  1.63it/s]Epoch: 16, train for the 34-th batch, train loss: 0.6233687996864319:  22%|██▌         | 33/151 [00:06<00:25,  4.63it/s]Epoch: 16, train for the 34-th batch, train loss: 0.6233687996864319:  23%|██▋         | 34/151 [00:06<00:25,  4.60it/s]Epoch: 8, train for the 60-th batch, train loss: 0.5000717639923096:  40%|█████▎       | 59/146 [00:35<00:53,  1.63it/s]Epoch: 8, train for the 60-th batch, train loss: 0.5000717639923096:  41%|█████▎       | 60/146 [00:35<00:52,  1.63it/s]Epoch: 3, train for the 333-th batch, train loss: 0.4669820964336395:  87%|█████████▌ | 332/383 [03:20<00:33,  1.53it/s]Epoch: 5, train for the 147-th batch, train loss: 0.6644182205200195:  62%|██████▊    | 146/237 [01:27<00:59,  1.53it/s]Epoch: 3, train for the 333-th batch, train loss: 0.4669820964336395:  87%|█████████▌ | 333/383 [03:20<00:32,  1.53it/s]Epoch: 5, train for the 147-th batch, train loss: 0.6644182205200195:  62%|██████▊    | 147/237 [01:27<00:58,  1.53it/s]Epoch: 16, train for the 35-th batch, train loss: 0.6465476751327515:  23%|██▋         | 34/151 [00:07<00:25,  4.60it/s]Epoch: 16, train for the 35-th batch, train loss: 0.6465476751327515:  23%|██▊         | 35/151 [00:07<00:33,  3.51it/s]Epoch: 9, train for the 89-th batch, train loss: 0.20018506050109863:  74%|████████▊   | 88/119 [00:54<00:19,  1.63it/s]Epoch: 9, train for the 89-th batch, train loss: 0.20018506050109863:  75%|████████▉   | 89/119 [00:54<00:18,  1.63it/s]Epoch: 8, train for the 61-th batch, train loss: 0.5592085123062134:  41%|█████▎       | 60/146 [00:35<00:52,  1.63it/s]Epoch: 8, train for the 61-th batch, train loss: 0.5592085123062134:  42%|█████▍       | 61/146 [00:35<00:52,  1.63it/s]Epoch: 16, train for the 36-th batch, train loss: 0.649459719657898:  23%|███          | 35/151 [00:07<00:33,  3.51it/s]Epoch: 16, train for the 36-th batch, train loss: 0.649459719657898:  24%|███          | 36/151 [00:07<00:30,  3.78it/s]Epoch: 16, train for the 37-th batch, train loss: 0.6705146431922913:  24%|██▊         | 36/151 [00:07<00:30,  3.78it/s]Epoch: 16, train for the 37-th batch, train loss: 0.6705146431922913:  25%|██▉         | 37/151 [00:07<00:28,  3.98it/s]Epoch: 5, train for the 148-th batch, train loss: 0.6030798554420471:  62%|██████▊    | 147/237 [01:27<00:58,  1.53it/s]Epoch: 5, train for the 148-th batch, train loss: 0.6030798554420471:  62%|██████▊    | 148/237 [01:27<00:53,  1.65it/s]Epoch: 16, train for the 38-th batch, train loss: 0.5799020528793335:  25%|██▉         | 37/151 [00:07<00:28,  3.98it/s]Epoch: 16, train for the 38-th batch, train loss: 0.5799020528793335:  25%|███         | 38/151 [00:07<00:27,  4.18it/s]Epoch: 9, train for the 90-th batch, train loss: 0.18155662715435028:  75%|████████▉   | 89/119 [00:54<00:18,  1.63it/s]Epoch: 9, train for the 90-th batch, train loss: 0.18155662715435028:  76%|█████████   | 90/119 [00:54<00:17,  1.63it/s]Epoch: 5, train for the 149-th batch, train loss: 0.6137519478797913:  62%|██████▊    | 148/237 [01:28<00:53,  1.65it/s]Epoch: 5, train for the 149-th batch, train loss: 0.6137519478797913:  63%|██████▉    | 149/237 [01:28<00:44,  1.97it/s]Epoch: 8, train for the 62-th batch, train loss: 0.4984779953956604:  42%|█████▍       | 61/146 [00:36<00:52,  1.63it/s]Epoch: 8, train for the 62-th batch, train loss: 0.4984779953956604:  42%|█████▌       | 62/146 [00:36<00:51,  1.63it/s]Epoch: 16, train for the 39-th batch, train loss: 0.6577988266944885:  25%|███         | 38/151 [00:08<00:27,  4.18it/s]Epoch: 16, train for the 39-th batch, train loss: 0.6577988266944885:  26%|███         | 39/151 [00:08<00:26,  4.28it/s]Epoch: 3, train for the 334-th batch, train loss: 0.42169180512428284:  87%|████████▋ | 333/383 [03:21<00:32,  1.53it/s]Epoch: 3, train for the 334-th batch, train loss: 0.42169180512428284:  87%|████████▋ | 334/383 [03:21<00:36,  1.33it/s]Epoch: 16, train for the 40-th batch, train loss: 0.6076146364212036:  26%|███         | 39/151 [00:08<00:26,  4.28it/s]Epoch: 16, train for the 40-th batch, train loss: 0.6076146364212036:  26%|███▏        | 40/151 [00:08<00:25,  4.34it/s]Epoch: 5, train for the 150-th batch, train loss: 0.6317040324211121:  63%|██████▉    | 149/237 [01:28<00:44,  1.97it/s]Epoch: 5, train for the 150-th batch, train loss: 0.6317040324211121:  63%|██████▉    | 150/237 [01:28<00:43,  1.98it/s]Epoch: 9, train for the 91-th batch, train loss: 0.17072488367557526:  76%|█████████   | 90/119 [00:55<00:17,  1.63it/s]Epoch: 9, train for the 91-th batch, train loss: 0.17072488367557526:  76%|█████████▏  | 91/119 [00:55<00:17,  1.63it/s]Epoch: 16, train for the 41-th batch, train loss: 0.6019381284713745:  26%|███▏        | 40/151 [00:08<00:25,  4.34it/s]Epoch: 16, train for the 41-th batch, train loss: 0.6019381284713745:  27%|███▎        | 41/151 [00:08<00:25,  4.39it/s]Epoch: 8, train for the 63-th batch, train loss: 0.48248428106307983:  42%|█████       | 62/146 [00:36<00:51,  1.63it/s]Epoch: 8, train for the 63-th batch, train loss: 0.48248428106307983:  43%|█████▏      | 63/146 [00:36<00:50,  1.63it/s]Epoch: 16, train for the 42-th batch, train loss: 0.561854362487793:  27%|███▌         | 41/151 [00:08<00:25,  4.39it/s]Epoch: 16, train for the 42-th batch, train loss: 0.561854362487793:  28%|███▌         | 42/151 [00:08<00:24,  4.43it/s]Epoch: 3, train for the 335-th batch, train loss: 0.5030910968780518:  87%|█████████▌ | 334/383 [03:22<00:36,  1.33it/s]Epoch: 3, train for the 335-th batch, train loss: 0.5030910968780518:  87%|█████████▌ | 335/383 [03:22<00:33,  1.42it/s]Epoch: 16, train for the 43-th batch, train loss: 0.4953632652759552:  28%|███▎        | 42/151 [00:09<00:24,  4.43it/s]Epoch: 16, train for the 43-th batch, train loss: 0.4953632652759552:  28%|███▍        | 43/151 [00:09<00:23,  4.52it/s]Epoch: 5, train for the 151-th batch, train loss: 0.6324765086174011:  63%|██████▉    | 150/237 [01:29<00:43,  1.98it/s]Epoch: 5, train for the 151-th batch, train loss: 0.6324765086174011:  64%|███████    | 151/237 [01:29<00:45,  1.89it/s]Epoch: 9, train for the 92-th batch, train loss: 0.1986270248889923:  76%|█████████▉   | 91/119 [00:55<00:17,  1.63it/s]Epoch: 9, train for the 92-th batch, train loss: 0.1986270248889923:  77%|██████████   | 92/119 [00:55<00:16,  1.63it/s]Epoch: 16, train for the 44-th batch, train loss: 0.4060724973678589:  28%|███▍        | 43/151 [00:09<00:23,  4.52it/s]Epoch: 16, train for the 44-th batch, train loss: 0.4060724973678589:  29%|███▍        | 44/151 [00:09<00:23,  4.58it/s]Epoch: 8, train for the 64-th batch, train loss: 0.5267694592475891:  43%|█████▌       | 63/146 [00:37<00:50,  1.63it/s]Epoch: 8, train for the 64-th batch, train loss: 0.5267694592475891:  44%|█████▋       | 64/146 [00:37<00:50,  1.63it/s]Epoch: 3, train for the 336-th batch, train loss: 0.4808701276779175:  87%|█████████▌ | 335/383 [03:22<00:33,  1.42it/s]Epoch: 3, train for the 336-th batch, train loss: 0.4808701276779175:  88%|█████████▋ | 336/383 [03:22<00:31,  1.50it/s]Epoch: 16, train for the 45-th batch, train loss: 0.5411701202392578:  29%|███▍        | 44/151 [00:09<00:23,  4.58it/s]Epoch: 16, train for the 45-th batch, train loss: 0.5411701202392578:  30%|███▌        | 45/151 [00:09<00:23,  4.59it/s]Epoch: 16, train for the 46-th batch, train loss: 0.6285847425460815:  30%|███▌        | 45/151 [00:09<00:23,  4.59it/s]Epoch: 16, train for the 46-th batch, train loss: 0.6285847425460815:  30%|███▋        | 46/151 [00:09<00:22,  4.58it/s]Epoch: 3, train for the 337-th batch, train loss: 0.5225874185562134:  88%|█████████▋ | 336/383 [03:23<00:31,  1.50it/s]Epoch: 3, train for the 337-th batch, train loss: 0.5225874185562134:  88%|█████████▋ | 337/383 [03:23<00:26,  1.74it/s]Epoch: 9, train for the 93-th batch, train loss: 0.16989579796791077:  77%|█████████▎  | 92/119 [00:56<00:16,  1.63it/s]Epoch: 9, train for the 93-th batch, train loss: 0.16989579796791077:  78%|█████████▍  | 93/119 [00:56<00:15,  1.63it/s]Epoch: 16, train for the 47-th batch, train loss: 0.636013925075531:  30%|███▉         | 46/151 [00:09<00:22,  4.58it/s]Epoch: 16, train for the 47-th batch, train loss: 0.636013925075531:  31%|████         | 47/151 [00:09<00:22,  4.53it/s]Epoch: 8, train for the 65-th batch, train loss: 0.5414154529571533:  44%|█████▋       | 64/146 [00:38<00:50,  1.63it/s]Epoch: 8, train for the 65-th batch, train loss: 0.5414154529571533:  45%|█████▊       | 65/146 [00:38<00:49,  1.63it/s]Epoch: 3, train for the 338-th batch, train loss: 0.44155630469322205:  88%|████████▊ | 337/383 [03:23<00:26,  1.74it/s]Epoch: 3, train for the 338-th batch, train loss: 0.44155630469322205:  88%|████████▊ | 338/383 [03:23<00:22,  2.01it/s]Epoch: 5, train for the 152-th batch, train loss: 0.6471813917160034:  64%|███████    | 151/237 [01:30<00:45,  1.89it/s]Epoch: 5, train for the 152-th batch, train loss: 0.6471813917160034:  64%|███████    | 152/237 [01:30<00:56,  1.51it/s]Epoch: 16, train for the 48-th batch, train loss: 0.362582802772522:  31%|████         | 47/151 [00:10<00:22,  4.53it/s]Epoch: 16, train for the 48-th batch, train loss: 0.362582802772522:  32%|████▏        | 48/151 [00:10<00:22,  4.60it/s]Epoch: 16, train for the 49-th batch, train loss: 0.6310848593711853:  32%|███▊        | 48/151 [00:10<00:22,  4.60it/s]Epoch: 16, train for the 49-th batch, train loss: 0.6310848593711853:  32%|███▉        | 49/151 [00:10<00:22,  4.58it/s]Epoch: 9, train for the 94-th batch, train loss: 0.1971704214811325:  78%|██████████▏  | 93/119 [00:57<00:15,  1.63it/s]Epoch: 9, train for the 94-th batch, train loss: 0.1971704214811325:  79%|██████████▎  | 94/119 [00:57<00:15,  1.63it/s]Epoch: 16, train for the 50-th batch, train loss: 0.4553057849407196:  32%|███▉        | 49/151 [00:10<00:22,  4.58it/s]Epoch: 16, train for the 50-th batch, train loss: 0.4553057849407196:  33%|███▉        | 50/151 [00:10<00:22,  4.59it/s]Epoch: 8, train for the 66-th batch, train loss: 0.5616532564163208:  45%|█████▊       | 65/146 [00:38<00:49,  1.63it/s]Epoch: 8, train for the 66-th batch, train loss: 0.5616532564163208:  45%|█████▉       | 66/146 [00:38<00:49,  1.63it/s]Epoch: 3, train for the 339-th batch, train loss: 0.42760711908340454:  88%|████████▊ | 338/383 [03:24<00:22,  2.01it/s]Epoch: 3, train for the 339-th batch, train loss: 0.42760711908340454:  89%|████████▊ | 339/383 [03:24<00:23,  1.84it/s]Epoch: 5, train for the 153-th batch, train loss: 0.6430533528327942:  64%|███████    | 152/237 [01:30<00:56,  1.51it/s]Epoch: 5, train for the 153-th batch, train loss: 0.6430533528327942:  65%|███████    | 153/237 [01:30<00:55,  1.52it/s]Epoch: 16, train for the 51-th batch, train loss: 0.6721234917640686:  33%|███▉        | 50/151 [00:10<00:22,  4.59it/s]Epoch: 16, train for the 51-th batch, train loss: 0.6721234917640686:  34%|████        | 51/151 [00:10<00:21,  4.56it/s]Epoch: 16, train for the 52-th batch, train loss: 0.6349042057991028:  34%|████        | 51/151 [00:11<00:21,  4.56it/s]Epoch: 16, train for the 52-th batch, train loss: 0.6349042057991028:  34%|████▏       | 52/151 [00:11<00:21,  4.54it/s]Epoch: 9, train for the 95-th batch, train loss: 0.15317614376544952:  79%|█████████▍  | 94/119 [00:57<00:15,  1.63it/s]Epoch: 9, train for the 95-th batch, train loss: 0.15317614376544952:  80%|█████████▌  | 95/119 [00:57<00:14,  1.63it/s]Epoch: 8, train for the 67-th batch, train loss: 0.533841073513031:  45%|██████▎       | 66/146 [00:39<00:49,  1.63it/s]Epoch: 8, train for the 67-th batch, train loss: 0.533841073513031:  46%|██████▍       | 67/146 [00:39<00:48,  1.63it/s]Epoch: 16, train for the 53-th batch, train loss: 0.4853105843067169:  34%|████▏       | 52/151 [00:11<00:21,  4.54it/s]Epoch: 16, train for the 53-th batch, train loss: 0.4853105843067169:  35%|████▏       | 53/151 [00:11<00:21,  4.58it/s]Epoch: 3, train for the 340-th batch, train loss: 0.48080405592918396:  89%|████████▊ | 339/383 [03:24<00:23,  1.84it/s]Epoch: 3, train for the 340-th batch, train loss: 0.48080405592918396:  89%|████████▉ | 340/383 [03:24<00:24,  1.74it/s]Epoch: 5, train for the 154-th batch, train loss: 0.6213754415512085:  65%|███████    | 153/237 [01:31<00:55,  1.52it/s]Epoch: 5, train for the 154-th batch, train loss: 0.6213754415512085:  65%|███████▏   | 154/237 [01:31<00:54,  1.53it/s]Epoch: 16, train for the 54-th batch, train loss: 0.5749664902687073:  35%|████▏       | 53/151 [00:11<00:21,  4.58it/s]Epoch: 16, train for the 54-th batch, train loss: 0.5749664902687073:  36%|████▎       | 54/151 [00:11<00:21,  4.55it/s]Epoch: 9, train for the 96-th batch, train loss: 0.15438760817050934:  80%|█████████▌  | 95/119 [00:58<00:14,  1.63it/s]Epoch: 9, train for the 96-th batch, train loss: 0.15438760817050934:  81%|█████████▋  | 96/119 [00:58<00:14,  1.63it/s]Epoch: 16, train for the 55-th batch, train loss: 0.5735437870025635:  36%|████▎       | 54/151 [00:11<00:21,  4.55it/s]Epoch: 16, train for the 55-th batch, train loss: 0.5735437870025635:  36%|████▎       | 55/151 [00:11<00:21,  4.53it/s]Epoch: 8, train for the 68-th batch, train loss: 0.5170996189117432:  46%|█████▉       | 67/146 [00:39<00:48,  1.63it/s]Epoch: 8, train for the 68-th batch, train loss: 0.5170996189117432:  47%|██████       | 68/146 [00:39<00:47,  1.63it/s]Epoch: 16, train for the 56-th batch, train loss: 0.4870752692222595:  36%|████▎       | 55/151 [00:11<00:21,  4.53it/s]Epoch: 16, train for the 56-th batch, train loss: 0.4870752692222595:  37%|████▍       | 56/151 [00:11<00:21,  4.52it/s]Epoch: 3, train for the 341-th batch, train loss: 0.37170395255088806:  89%|████████▉ | 340/383 [03:25<00:24,  1.74it/s]Epoch: 3, train for the 341-th batch, train loss: 0.37170395255088806:  89%|████████▉ | 341/383 [03:25<00:25,  1.67it/s]Epoch: 5, train for the 155-th batch, train loss: 0.6223037838935852:  65%|███████▏   | 154/237 [01:32<00:54,  1.53it/s]Epoch: 5, train for the 155-th batch, train loss: 0.6223037838935852:  65%|███████▏   | 155/237 [01:32<00:53,  1.53it/s]Epoch: 16, train for the 57-th batch, train loss: 0.5667822957038879:  37%|████▍       | 56/151 [00:12<00:21,  4.52it/s]Epoch: 16, train for the 57-th batch, train loss: 0.5667822957038879:  38%|████▌       | 57/151 [00:12<00:20,  4.51it/s]Epoch: 9, train for the 97-th batch, train loss: 0.19908878207206726:  81%|█████████▋  | 96/119 [00:58<00:14,  1.63it/s]Epoch: 9, train for the 97-th batch, train loss: 0.19908878207206726:  82%|█████████▊  | 97/119 [00:58<00:13,  1.63it/s]Epoch: 16, train for the 58-th batch, train loss: 0.5427302122116089:  38%|████▌       | 57/151 [00:12<00:20,  4.51it/s]Epoch: 16, train for the 58-th batch, train loss: 0.5427302122116089:  38%|████▌       | 58/151 [00:12<00:20,  4.52it/s]Epoch: 8, train for the 69-th batch, train loss: 0.5360311269760132:  47%|██████       | 68/146 [00:40<00:47,  1.63it/s]Epoch: 8, train for the 69-th batch, train loss: 0.5360311269760132:  47%|██████▏      | 69/146 [00:40<00:47,  1.63it/s]Epoch: 16, train for the 59-th batch, train loss: 0.4978084862232208:  38%|████▌       | 58/151 [00:12<00:20,  4.52it/s]Epoch: 16, train for the 59-th batch, train loss: 0.4978084862232208:  39%|████▋       | 59/151 [00:12<00:20,  4.54it/s]Epoch: 3, train for the 342-th batch, train loss: 0.4102114140987396:  89%|█████████▊ | 341/383 [03:26<00:25,  1.67it/s]Epoch: 3, train for the 342-th batch, train loss: 0.4102114140987396:  89%|█████████▊ | 342/383 [03:26<00:25,  1.63it/s]Epoch: 5, train for the 156-th batch, train loss: 0.6252644062042236:  65%|███████▏   | 155/237 [01:32<00:53,  1.53it/s]Epoch: 5, train for the 156-th batch, train loss: 0.6252644062042236:  66%|███████▏   | 156/237 [01:32<00:52,  1.53it/s]Epoch: 16, train for the 60-th batch, train loss: 0.5331066250801086:  39%|████▋       | 59/151 [00:12<00:20,  4.54it/s]Epoch: 16, train for the 60-th batch, train loss: 0.5331066250801086:  40%|████▊       | 60/151 [00:12<00:20,  4.53it/s]Epoch: 9, train for the 98-th batch, train loss: 0.1310490369796753:  82%|██████████▌  | 97/119 [00:59<00:13,  1.63it/s]Epoch: 9, train for the 98-th batch, train loss: 0.1310490369796753:  82%|██████████▋  | 98/119 [00:59<00:12,  1.63it/s]Epoch: 16, train for the 61-th batch, train loss: 0.5498221516609192:  40%|████▊       | 60/151 [00:12<00:20,  4.53it/s]Epoch: 16, train for the 61-th batch, train loss: 0.5498221516609192:  40%|████▊       | 61/151 [00:12<00:19,  4.51it/s]Epoch: 8, train for the 70-th batch, train loss: 0.5620216131210327:  47%|██████▏      | 69/146 [00:41<00:47,  1.63it/s]Epoch: 8, train for the 70-th batch, train loss: 0.5620216131210327:  48%|██████▏      | 70/146 [00:41<00:46,  1.63it/s]Epoch: 16, train for the 62-th batch, train loss: 0.5447336435317993:  40%|████▊       | 61/151 [00:13<00:19,  4.51it/s]Epoch: 16, train for the 62-th batch, train loss: 0.5447336435317993:  41%|████▉       | 62/151 [00:13<00:19,  4.55it/s]Epoch: 3, train for the 343-th batch, train loss: 0.4064196050167084:  89%|█████████▊ | 342/383 [03:26<00:25,  1.63it/s]Epoch: 5, train for the 157-th batch, train loss: 0.6638357639312744:  66%|███████▏   | 156/237 [01:33<00:52,  1.53it/s]Epoch: 3, train for the 343-th batch, train loss: 0.4064196050167084:  90%|█████████▊ | 343/383 [03:26<00:25,  1.60it/s]Epoch: 5, train for the 157-th batch, train loss: 0.6638357639312744:  66%|███████▎   | 157/237 [01:33<00:52,  1.53it/s]Epoch: 16, train for the 63-th batch, train loss: 0.5644676089286804:  41%|████▉       | 62/151 [00:13<00:19,  4.55it/s]Epoch: 16, train for the 63-th batch, train loss: 0.5644676089286804:  42%|█████       | 63/151 [00:13<00:19,  4.52it/s]Epoch: 9, train for the 99-th batch, train loss: 0.16983599960803986:  82%|█████████▉  | 98/119 [01:00<00:12,  1.63it/s]Epoch: 9, train for the 99-th batch, train loss: 0.16983599960803986:  83%|█████████▉  | 99/119 [01:00<00:12,  1.63it/s]Epoch: 8, train for the 71-th batch, train loss: 0.5275232791900635:  48%|██████▏      | 70/146 [00:41<00:46,  1.63it/s]Epoch: 8, train for the 71-th batch, train loss: 0.5275232791900635:  49%|██████▎      | 71/146 [00:41<00:45,  1.63it/s]Epoch: 16, train for the 64-th batch, train loss: 0.5190723538398743:  42%|█████       | 63/151 [00:13<00:19,  4.52it/s]Epoch: 16, train for the 64-th batch, train loss: 0.5190723538398743:  42%|█████       | 64/151 [00:13<00:21,  4.07it/s]Epoch: 16, train for the 65-th batch, train loss: 0.48836931586265564:  42%|████▋      | 64/151 [00:13<00:21,  4.07it/s]Epoch: 16, train for the 65-th batch, train loss: 0.48836931586265564:  43%|████▋      | 65/151 [00:13<00:20,  4.20it/s]Epoch: 3, train for the 344-th batch, train loss: 0.4570111632347107:  90%|█████████▊ | 343/383 [03:27<00:25,  1.60it/s]Epoch: 5, train for the 158-th batch, train loss: 0.6109586358070374:  66%|███████▎   | 157/237 [01:34<00:52,  1.53it/s]Epoch: 3, train for the 344-th batch, train loss: 0.4570111632347107:  90%|█████████▉ | 344/383 [03:27<00:24,  1.58it/s]Epoch: 5, train for the 158-th batch, train loss: 0.6109586358070374:  67%|███████▎   | 158/237 [01:34<00:51,  1.53it/s]Epoch: 9, train for the 100-th batch, train loss: 0.19873982667922974:  83%|█████████▏ | 99/119 [01:00<00:12,  1.63it/s]Epoch: 9, train for the 100-th batch, train loss: 0.19873982667922974:  84%|████████▍ | 100/119 [01:00<00:11,  1.63it/s]Epoch: 16, train for the 66-th batch, train loss: 0.44008544087409973:  43%|████▋      | 65/151 [00:14<00:20,  4.20it/s]Epoch: 16, train for the 66-th batch, train loss: 0.44008544087409973:  44%|████▊      | 66/151 [00:14<00:19,  4.32it/s]Epoch: 8, train for the 72-th batch, train loss: 0.5419146418571472:  49%|██████▎      | 71/146 [00:42<00:45,  1.63it/s]Epoch: 8, train for the 72-th batch, train loss: 0.5419146418571472:  49%|██████▍      | 72/146 [00:42<00:45,  1.63it/s]Epoch: 16, train for the 67-th batch, train loss: 0.4260120689868927:  44%|█████▏      | 66/151 [00:14<00:19,  4.32it/s]Epoch: 16, train for the 67-th batch, train loss: 0.4260120689868927:  44%|█████▎      | 67/151 [00:14<00:19,  4.42it/s]Epoch: 16, train for the 68-th batch, train loss: 0.5792833566665649:  44%|█████▎      | 67/151 [00:14<00:19,  4.42it/s]Epoch: 16, train for the 68-th batch, train loss: 0.5792833566665649:  45%|█████▍      | 68/151 [00:14<00:18,  4.44it/s]Epoch: 3, train for the 345-th batch, train loss: 0.3899877071380615:  90%|█████████▉ | 344/383 [03:28<00:24,  1.58it/s]Epoch: 5, train for the 159-th batch, train loss: 0.6420339941978455:  67%|███████▎   | 158/237 [01:34<00:51,  1.53it/s]Epoch: 3, train for the 345-th batch, train loss: 0.3899877071380615:  90%|█████████▉ | 345/383 [03:28<00:24,  1.56it/s]Epoch: 5, train for the 159-th batch, train loss: 0.6420339941978455:  67%|███████▍   | 159/237 [01:34<00:50,  1.53it/s]Epoch: 9, train for the 101-th batch, train loss: 0.13872455060482025:  84%|████████▍ | 100/119 [01:01<00:11,  1.63it/s]Epoch: 9, train for the 101-th batch, train loss: 0.13872455060482025:  85%|████████▍ | 101/119 [01:01<00:11,  1.63it/s]Epoch: 16, train for the 69-th batch, train loss: 0.5854933261871338:  45%|█████▍      | 68/151 [00:14<00:18,  4.44it/s]Epoch: 16, train for the 69-th batch, train loss: 0.5854933261871338:  46%|█████▍      | 69/151 [00:14<00:18,  4.46it/s]Epoch: 8, train for the 73-th batch, train loss: 0.5098643898963928:  49%|██████▍      | 72/146 [00:43<00:45,  1.63it/s]Epoch: 8, train for the 73-th batch, train loss: 0.5098643898963928:  50%|██████▌      | 73/146 [00:43<00:44,  1.63it/s]Epoch: 16, train for the 70-th batch, train loss: 0.548462450504303:  46%|█████▉       | 69/151 [00:15<00:18,  4.46it/s]Epoch: 16, train for the 70-th batch, train loss: 0.548462450504303:  46%|██████       | 70/151 [00:15<00:18,  4.47it/s]Epoch: 16, train for the 71-th batch, train loss: 0.5743188858032227:  46%|█████▌      | 70/151 [00:15<00:18,  4.47it/s]Epoch: 16, train for the 71-th batch, train loss: 0.5743188858032227:  47%|█████▋      | 71/151 [00:15<00:17,  4.46it/s]Epoch: 9, train for the 102-th batch, train loss: 0.17096808552742004:  85%|████████▍ | 101/119 [01:02<00:11,  1.63it/s]Epoch: 9, train for the 102-th batch, train loss: 0.17096808552742004:  86%|████████▌ | 102/119 [01:02<00:10,  1.63it/s]Epoch: 3, train for the 346-th batch, train loss: 0.39118582010269165:  90%|█████████ | 345/383 [03:28<00:24,  1.56it/s]Epoch: 5, train for the 160-th batch, train loss: 0.6367980241775513:  67%|███████▍   | 159/237 [01:35<00:50,  1.53it/s]Epoch: 3, train for the 346-th batch, train loss: 0.39118582010269165:  90%|█████████ | 346/383 [03:28<00:23,  1.55it/s]Epoch: 5, train for the 160-th batch, train loss: 0.6367980241775513:  68%|███████▍   | 160/237 [01:35<00:50,  1.53it/s]Epoch: 8, train for the 74-th batch, train loss: 0.5306587219238281:  50%|██████▌      | 73/146 [00:43<00:44,  1.63it/s]Epoch: 8, train for the 74-th batch, train loss: 0.5306587219238281:  51%|██████▌      | 74/146 [00:43<00:44,  1.63it/s]Epoch: 16, train for the 72-th batch, train loss: 0.5143693089485168:  47%|█████▋      | 71/151 [00:15<00:17,  4.46it/s]Epoch: 16, train for the 72-th batch, train loss: 0.5143693089485168:  48%|█████▋      | 72/151 [00:15<00:17,  4.47it/s]Epoch: 16, train for the 73-th batch, train loss: 0.5359201431274414:  48%|█████▋      | 72/151 [00:15<00:17,  4.47it/s]Epoch: 16, train for the 73-th batch, train loss: 0.5359201431274414:  48%|█████▊      | 73/151 [00:15<00:17,  4.48it/s]Epoch: 9, train for the 103-th batch, train loss: 0.20071658492088318:  86%|████████▌ | 102/119 [01:02<00:10,  1.63it/s]Epoch: 9, train for the 103-th batch, train loss: 0.20071658492088318:  87%|████████▋ | 103/119 [01:02<00:09,  1.63it/s]Epoch: 16, train for the 74-th batch, train loss: 0.5116665363311768:  48%|█████▊      | 73/151 [00:15<00:17,  4.48it/s]Epoch: 16, train for the 74-th batch, train loss: 0.5116665363311768:  49%|█████▉      | 74/151 [00:15<00:17,  4.50it/s]Epoch: 3, train for the 347-th batch, train loss: 0.35970935225486755:  90%|█████████ | 346/383 [03:29<00:23,  1.55it/s]Epoch: 5, train for the 161-th batch, train loss: 0.6474370360374451:  68%|███████▍   | 160/237 [01:36<00:50,  1.53it/s]Epoch: 3, train for the 347-th batch, train loss: 0.35970935225486755:  91%|█████████ | 347/383 [03:29<00:23,  1.54it/s]Epoch: 5, train for the 161-th batch, train loss: 0.6474370360374451:  68%|███████▍   | 161/237 [01:36<00:49,  1.53it/s]Epoch: 8, train for the 75-th batch, train loss: 0.5751080513000488:  51%|██████▌      | 74/146 [00:44<00:44,  1.63it/s]Epoch: 8, train for the 75-th batch, train loss: 0.5751080513000488:  51%|██████▋      | 75/146 [00:44<00:43,  1.63it/s]Epoch: 16, train for the 75-th batch, train loss: 0.4799611568450928:  49%|█████▉      | 74/151 [00:16<00:17,  4.50it/s]Epoch: 16, train for the 75-th batch, train loss: 0.4799611568450928:  50%|█████▉      | 75/151 [00:16<00:16,  4.54it/s]Epoch: 16, train for the 76-th batch, train loss: 0.5769979953765869:  50%|█████▉      | 75/151 [00:16<00:16,  4.54it/s]Epoch: 16, train for the 76-th batch, train loss: 0.5769979953765869:  50%|██████      | 76/151 [00:16<00:16,  4.52it/s]Epoch: 9, train for the 104-th batch, train loss: 0.20744985342025757:  87%|████████▋ | 103/119 [01:03<00:09,  1.63it/s]Epoch: 9, train for the 104-th batch, train loss: 0.20744985342025757:  87%|████████▋ | 104/119 [01:03<00:09,  1.64it/s]Epoch: 16, train for the 77-th batch, train loss: 0.5303385257720947:  50%|██████      | 76/151 [00:16<00:16,  4.52it/s]Epoch: 16, train for the 77-th batch, train loss: 0.5303385257720947:  51%|██████      | 77/151 [00:16<00:16,  4.51it/s]Epoch: 3, train for the 348-th batch, train loss: 0.5263064503669739:  91%|█████████▉ | 347/383 [03:30<00:23,  1.54it/s]Epoch: 5, train for the 162-th batch, train loss: 0.638057291507721:  68%|████████▏   | 161/237 [01:36<00:49,  1.53it/s]Epoch: 3, train for the 348-th batch, train loss: 0.5263064503669739:  91%|█████████▉ | 348/383 [03:30<00:22,  1.54it/s]Epoch: 5, train for the 162-th batch, train loss: 0.638057291507721:  68%|████████▏   | 162/237 [01:36<00:49,  1.53it/s]Epoch: 8, train for the 76-th batch, train loss: 0.5527005791664124:  51%|██████▋      | 75/146 [00:44<00:43,  1.63it/s]Epoch: 8, train for the 76-th batch, train loss: 0.5527005791664124:  52%|██████▊      | 76/146 [00:44<00:42,  1.64it/s]Epoch: 16, train for the 78-th batch, train loss: 0.5063403248786926:  51%|██████      | 77/151 [00:16<00:16,  4.51it/s]Epoch: 16, train for the 78-th batch, train loss: 0.5063403248786926:  52%|██████▏     | 78/151 [00:16<00:16,  4.53it/s]Epoch: 16, train for the 79-th batch, train loss: 0.5993255376815796:  52%|██████▏     | 78/151 [00:17<00:16,  4.53it/s]Epoch: 16, train for the 79-th batch, train loss: 0.5993255376815796:  52%|██████▎     | 79/151 [00:17<00:15,  4.53it/s]Epoch: 9, train for the 105-th batch, train loss: 0.1505298614501953:  87%|█████████▌ | 104/119 [01:03<00:09,  1.64it/s]Epoch: 9, train for the 105-th batch, train loss: 0.1505298614501953:  88%|█████████▋ | 105/119 [01:03<00:08,  1.63it/s]Epoch: 16, train for the 80-th batch, train loss: 0.5659627914428711:  52%|██████▎     | 79/151 [00:17<00:15,  4.53it/s]Epoch: 16, train for the 80-th batch, train loss: 0.5659627914428711:  53%|██████▎     | 80/151 [00:17<00:15,  4.52it/s]Epoch: 3, train for the 349-th batch, train loss: 0.48866671323776245:  91%|█████████ | 348/383 [03:30<00:22,  1.54it/s]Epoch: 5, train for the 163-th batch, train loss: 0.6391319632530212:  68%|███████▌   | 162/237 [01:37<00:49,  1.53it/s]Epoch: 3, train for the 349-th batch, train loss: 0.48866671323776245:  91%|█████████ | 349/383 [03:30<00:22,  1.53it/s]Epoch: 5, train for the 163-th batch, train loss: 0.6391319632530212:  69%|███████▌   | 163/237 [01:37<00:48,  1.53it/s]Epoch: 8, train for the 77-th batch, train loss: 0.580191433429718:  52%|███████▎      | 76/146 [00:45<00:42,  1.64it/s]Epoch: 8, train for the 77-th batch, train loss: 0.580191433429718:  53%|███████▍      | 77/146 [00:45<00:42,  1.63it/s]Epoch: 16, train for the 81-th batch, train loss: 0.5611502528190613:  53%|██████▎     | 80/151 [00:17<00:15,  4.52it/s]Epoch: 16, train for the 81-th batch, train loss: 0.5611502528190613:  54%|██████▍     | 81/151 [00:17<00:15,  4.50it/s]Epoch: 16, train for the 82-th batch, train loss: 0.5706173777580261:  54%|██████▍     | 81/151 [00:17<00:15,  4.50it/s]Epoch: 16, train for the 82-th batch, train loss: 0.5706173777580261:  54%|██████▌     | 82/151 [00:17<00:15,  4.51it/s]Epoch: 9, train for the 106-th batch, train loss: 0.13773277401924133:  88%|████████▊ | 105/119 [01:04<00:08,  1.63it/s]Epoch: 9, train for the 106-th batch, train loss: 0.13773277401924133:  89%|████████▉ | 106/119 [01:04<00:07,  1.63it/s]Epoch: 16, train for the 83-th batch, train loss: 0.571745753288269:  54%|███████      | 82/151 [00:17<00:15,  4.51it/s]Epoch: 16, train for the 83-th batch, train loss: 0.571745753288269:  55%|███████▏     | 83/151 [00:17<00:15,  4.51it/s]Epoch: 3, train for the 350-th batch, train loss: 0.3776102662086487:  91%|██████████ | 349/383 [03:31<00:22,  1.53it/s]Epoch: 5, train for the 164-th batch, train loss: 0.6399059891700745:  69%|███████▌   | 163/237 [01:38<00:48,  1.53it/s]Epoch: 3, train for the 350-th batch, train loss: 0.3776102662086487:  91%|██████████ | 350/383 [03:31<00:21,  1.53it/s]Epoch: 5, train for the 164-th batch, train loss: 0.6399059891700745:  69%|███████▌   | 164/237 [01:38<00:47,  1.53it/s]Epoch: 9, train for the 107-th batch, train loss: 0.21331748366355896:  89%|████████▉ | 106/119 [01:04<00:07,  1.63it/s]Epoch: 9, train for the 107-th batch, train loss: 0.21331748366355896:  90%|████████▉ | 107/119 [01:04<00:06,  2.00it/s]Epoch: 16, train for the 84-th batch, train loss: 0.5877023935317993:  55%|██████▌     | 83/151 [00:18<00:15,  4.51it/s]Epoch: 16, train for the 84-th batch, train loss: 0.5877023935317993:  56%|██████▋     | 84/151 [00:18<00:14,  4.51it/s]Epoch: 8, train for the 78-th batch, train loss: 0.5316539406776428:  53%|██████▊      | 77/146 [00:46<00:42,  1.63it/s]Epoch: 8, train for the 78-th batch, train loss: 0.5316539406776428:  53%|██████▉      | 78/146 [00:46<00:46,  1.45it/s]Epoch: 16, train for the 85-th batch, train loss: 0.5649992823600769:  56%|██████▋     | 84/151 [00:18<00:14,  4.51it/s]Epoch: 16, train for the 85-th batch, train loss: 0.5649992823600769:  56%|██████▊     | 85/151 [00:18<00:14,  4.53it/s]Epoch: 16, train for the 86-th batch, train loss: 0.5576953887939453:  56%|██████▊     | 85/151 [00:18<00:14,  4.53it/s]Epoch: 16, train for the 86-th batch, train loss: 0.5576953887939453:  57%|██████▊     | 86/151 [00:18<00:14,  4.52it/s]Epoch: 3, train for the 351-th batch, train loss: 0.4234668016433716:  91%|██████████ | 350/383 [03:32<00:21,  1.53it/s]Epoch: 5, train for the 165-th batch, train loss: 0.6212210059165955:  69%|███████▌   | 164/237 [01:38<00:47,  1.53it/s]Epoch: 3, train for the 351-th batch, train loss: 0.4234668016433716:  92%|██████████ | 351/383 [03:32<00:20,  1.53it/s]Epoch: 5, train for the 165-th batch, train loss: 0.6212210059165955:  70%|███████▋   | 165/237 [01:38<00:47,  1.53it/s]Epoch: 9, train for the 108-th batch, train loss: 0.11290393769741058:  90%|████████▉ | 107/119 [01:05<00:06,  2.00it/s]Epoch: 9, train for the 108-th batch, train loss: 0.11290393769741058:  91%|█████████ | 108/119 [01:05<00:05,  1.85it/s]Epoch: 8, train for the 79-th batch, train loss: 0.5333916544914246:  53%|██████▉      | 78/146 [00:46<00:46,  1.45it/s]Epoch: 8, train for the 79-th batch, train loss: 0.5333916544914246:  54%|███████      | 79/146 [00:46<00:42,  1.56it/s]Epoch: 16, train for the 87-th batch, train loss: 0.5644472241401672:  57%|██████▊     | 86/151 [00:18<00:14,  4.52it/s]Epoch: 16, train for the 87-th batch, train loss: 0.5644472241401672:  58%|██████▉     | 87/151 [00:18<00:14,  4.51it/s]Epoch: 16, train for the 88-th batch, train loss: 0.609218418598175:  58%|███████▍     | 87/151 [00:19<00:14,  4.51it/s]Epoch: 16, train for the 88-th batch, train loss: 0.609218418598175:  58%|███████▌     | 88/151 [00:19<00:13,  4.50it/s]Epoch: 9, train for the 109-th batch, train loss: 0.19976072013378143:  91%|█████████ | 108/119 [01:05<00:05,  1.85it/s]Epoch: 9, train for the 109-th batch, train loss: 0.19976072013378143:  92%|█████████▏| 109/119 [01:05<00:05,  1.78it/s]Epoch: 3, train for the 352-th batch, train loss: 0.376737505197525:  92%|██████████▉ | 351/383 [03:32<00:20,  1.53it/s]Epoch: 5, train for the 166-th batch, train loss: 0.603988766670227:  70%|████████▎   | 165/237 [01:39<00:47,  1.53it/s]Epoch: 3, train for the 352-th batch, train loss: 0.376737505197525:  92%|███████████ | 352/383 [03:32<00:20,  1.53it/s]Epoch: 5, train for the 166-th batch, train loss: 0.603988766670227:  70%|████████▍   | 166/237 [01:39<00:46,  1.52it/s]Epoch: 16, train for the 89-th batch, train loss: 0.5643624067306519:  58%|██████▉     | 88/151 [00:19<00:13,  4.50it/s]Epoch: 16, train for the 89-th batch, train loss: 0.5643624067306519:  59%|███████     | 89/151 [00:19<00:13,  4.51it/s]Epoch: 8, train for the 80-th batch, train loss: 0.5448763966560364:  54%|███████      | 79/146 [00:47<00:42,  1.56it/s]Epoch: 8, train for the 80-th batch, train loss: 0.5448763966560364:  55%|███████      | 80/146 [00:47<00:41,  1.60it/s]Epoch: 16, train for the 90-th batch, train loss: 0.577244758605957:  59%|███████▋     | 89/151 [00:19<00:13,  4.51it/s]Epoch: 16, train for the 90-th batch, train loss: 0.577244758605957:  60%|███████▋     | 90/151 [00:19<00:13,  4.51it/s]Epoch: 16, train for the 91-th batch, train loss: 0.4773794114589691:  60%|███████▏    | 90/151 [00:19<00:13,  4.51it/s]Epoch: 16, train for the 91-th batch, train loss: 0.4773794114589691:  60%|███████▏    | 91/151 [00:19<00:13,  4.48it/s]Epoch: 9, train for the 110-th batch, train loss: 0.13463237881660461:  92%|█████████▏| 109/119 [01:06<00:05,  1.78it/s]Epoch: 9, train for the 110-th batch, train loss: 0.13463237881660461:  92%|█████████▏| 110/119 [01:06<00:05,  1.72it/s]Epoch: 8, train for the 81-th batch, train loss: 0.5244731903076172:  55%|███████      | 80/146 [00:48<00:41,  1.60it/s]Epoch: 8, train for the 81-th batch, train loss: 0.5244731903076172:  55%|███████▏     | 81/146 [00:48<00:40,  1.61it/s]Epoch: 5, train for the 167-th batch, train loss: 0.6280359029769897:  70%|███████▋   | 166/237 [01:39<00:46,  1.52it/s]Epoch: 3, train for the 353-th batch, train loss: 0.49232885241508484:  92%|█████████▏| 352/383 [03:33<00:20,  1.53it/s]Epoch: 5, train for the 167-th batch, train loss: 0.6280359029769897:  70%|███████▊   | 167/237 [01:39<00:45,  1.53it/s]Epoch: 3, train for the 353-th batch, train loss: 0.49232885241508484:  92%|█████████▏| 353/383 [03:33<00:19,  1.53it/s]Epoch: 16, train for the 92-th batch, train loss: 0.5608294606208801:  60%|███████▏    | 91/151 [00:19<00:13,  4.48it/s]Epoch: 16, train for the 92-th batch, train loss: 0.5608294606208801:  61%|███████▎    | 92/151 [00:19<00:13,  4.49it/s]Epoch: 16, train for the 93-th batch, train loss: 0.5253565311431885:  61%|███████▎    | 92/151 [00:20<00:13,  4.49it/s]Epoch: 16, train for the 93-th batch, train loss: 0.5253565311431885:  62%|███████▍    | 93/151 [00:20<00:12,  4.50it/s]Epoch: 16, train for the 94-th batch, train loss: 0.5274842977523804:  62%|███████▍    | 93/151 [00:20<00:12,  4.50it/s]Epoch: 16, train for the 94-th batch, train loss: 0.5274842977523804:  62%|███████▍    | 94/151 [00:20<00:12,  4.50it/s]Epoch: 9, train for the 111-th batch, train loss: 0.17561981081962585:  92%|█████████▏| 110/119 [01:07<00:05,  1.72it/s]Epoch: 9, train for the 111-th batch, train loss: 0.17561981081962585:  93%|█████████▎| 111/119 [01:07<00:04,  1.65it/s]Epoch: 8, train for the 82-th batch, train loss: 0.5196501016616821:  55%|███████▏     | 81/146 [00:48<00:40,  1.61it/s]Epoch: 8, train for the 82-th batch, train loss: 0.5196501016616821:  56%|███████▎     | 82/146 [00:48<00:40,  1.60it/s]Epoch: 3, train for the 354-th batch, train loss: 0.38605791330337524:  92%|█████████▏| 353/383 [03:34<00:19,  1.53it/s]Epoch: 5, train for the 168-th batch, train loss: 0.6557766795158386:  70%|███████▊   | 167/237 [01:40<00:45,  1.53it/s]Epoch: 3, train for the 354-th batch, train loss: 0.38605791330337524:  92%|█████████▏| 354/383 [03:34<00:18,  1.53it/s]Epoch: 5, train for the 168-th batch, train loss: 0.6557766795158386:  71%|███████▊   | 168/237 [01:40<00:45,  1.53it/s]Epoch: 16, train for the 95-th batch, train loss: 0.5403474569320679:  62%|███████▍    | 94/151 [00:20<00:12,  4.50it/s]Epoch: 16, train for the 95-th batch, train loss: 0.5403474569320679:  63%|███████▌    | 95/151 [00:20<00:12,  4.50it/s]Epoch: 16, train for the 96-th batch, train loss: 0.5548161268234253:  63%|███████▌    | 95/151 [00:20<00:12,  4.50it/s]Epoch: 16, train for the 96-th batch, train loss: 0.5548161268234253:  64%|███████▋    | 96/151 [00:20<00:12,  4.51it/s]Epoch: 5, train for the 169-th batch, train loss: 0.6515383720397949:  71%|███████▊   | 168/237 [01:41<00:45,  1.53it/s]Epoch: 5, train for the 169-th batch, train loss: 0.6515383720397949:  71%|███████▊   | 169/237 [01:41<00:39,  1.71it/s]Epoch: 16, train for the 97-th batch, train loss: 0.5772327184677124:  64%|███████▋    | 96/151 [00:21<00:12,  4.51it/s]Epoch: 16, train for the 97-th batch, train loss: 0.5772327184677124:  64%|███████▋    | 97/151 [00:21<00:11,  4.51it/s]Epoch: 9, train for the 112-th batch, train loss: 0.11512765288352966:  93%|█████████▎| 111/119 [01:07<00:04,  1.65it/s]Epoch: 8, train for the 83-th batch, train loss: 0.5275737643241882:  56%|███████▎     | 82/146 [00:49<00:40,  1.60it/s]Epoch: 9, train for the 112-th batch, train loss: 0.11512765288352966:  94%|█████████▍| 112/119 [01:07<00:04,  1.61it/s]Epoch: 8, train for the 83-th batch, train loss: 0.5275737643241882:  57%|███████▍     | 83/146 [00:49<00:40,  1.57it/s]Epoch: 16, train for the 98-th batch, train loss: 0.6070497035980225:  64%|███████▋    | 97/151 [00:21<00:11,  4.51it/s]Epoch: 16, train for the 98-th batch, train loss: 0.6070497035980225:  65%|███████▊    | 98/151 [00:21<00:11,  4.52it/s]Epoch: 3, train for the 355-th batch, train loss: 0.48858416080474854:  92%|█████████▏| 354/383 [03:34<00:18,  1.53it/s]Epoch: 3, train for the 355-th batch, train loss: 0.48858416080474854:  93%|█████████▎| 355/383 [03:34<00:18,  1.50it/s]Epoch: 16, train for the 99-th batch, train loss: 0.6433145999908447:  65%|███████▊    | 98/151 [00:21<00:11,  4.52it/s]Epoch: 16, train for the 99-th batch, train loss: 0.6433145999908447:  66%|███████▊    | 99/151 [00:21<00:11,  4.52it/s]Epoch: 5, train for the 170-th batch, train loss: 0.6433464884757996:  71%|███████▊   | 169/237 [01:41<00:39,  1.71it/s]Epoch: 5, train for the 170-th batch, train loss: 0.6433464884757996:  72%|███████▉   | 170/237 [01:41<00:39,  1.72it/s]Epoch: 16, train for the 100-th batch, train loss: 0.6667503118515015:  66%|███████▏   | 99/151 [00:21<00:11,  4.52it/s]Epoch: 16, train for the 100-th batch, train loss: 0.6667503118515015:  66%|██████▌   | 100/151 [00:21<00:11,  4.52it/s]Epoch: 8, train for the 84-th batch, train loss: 0.5068068504333496:  57%|███████▍     | 83/146 [00:50<00:40,  1.57it/s]Epoch: 9, train for the 113-th batch, train loss: 0.16153812408447266:  94%|█████████▍| 112/119 [01:08<00:04,  1.61it/s]Epoch: 8, train for the 84-th batch, train loss: 0.5068068504333496:  58%|███████▍     | 84/146 [00:50<00:40,  1.55it/s]Epoch: 9, train for the 113-th batch, train loss: 0.16153812408447266:  95%|█████████▍| 113/119 [01:08<00:03,  1.57it/s]Epoch: 3, train for the 356-th batch, train loss: 0.3505913317203522:  93%|██████████▏| 355/383 [03:35<00:18,  1.50it/s]Epoch: 3, train for the 356-th batch, train loss: 0.3505913317203522:  93%|██████████▏| 356/383 [03:35<00:17,  1.55it/s]Epoch: 16, train for the 101-th batch, train loss: 0.7041395902633667:  66%|██████▌   | 100/151 [00:21<00:11,  4.52it/s]Epoch: 16, train for the 101-th batch, train loss: 0.7041395902633667:  67%|██████▋   | 101/151 [00:21<00:11,  4.50it/s]Epoch: 16, train for the 102-th batch, train loss: 0.6046633124351501:  67%|██████▋   | 101/151 [00:22<00:11,  4.50it/s]Epoch: 16, train for the 102-th batch, train loss: 0.6046633124351501:  68%|██████▊   | 102/151 [00:22<00:10,  4.49it/s]Epoch: 5, train for the 171-th batch, train loss: 0.6543104648590088:  72%|███████▉   | 170/237 [01:42<00:39,  1.72it/s]Epoch: 5, train for the 171-th batch, train loss: 0.6543104648590088:  72%|███████▉   | 171/237 [01:42<00:38,  1.71it/s]Epoch: 16, train for the 103-th batch, train loss: 0.6500595808029175:  68%|██████▊   | 102/151 [00:22<00:10,  4.49it/s]Epoch: 16, train for the 103-th batch, train loss: 0.6500595808029175:  68%|██████▊   | 103/151 [00:22<00:10,  4.49it/s]Epoch: 3, train for the 357-th batch, train loss: 0.47541606426239014:  93%|█████████▎| 356/383 [03:35<00:17,  1.55it/s]Epoch: 3, train for the 357-th batch, train loss: 0.47541606426239014:  93%|█████████▎| 357/383 [03:35<00:16,  1.59it/s]Epoch: 8, train for the 85-th batch, train loss: 0.5054340958595276:  58%|███████▍     | 84/146 [00:50<00:40,  1.55it/s]Epoch: 9, train for the 114-th batch, train loss: 0.16243058443069458:  95%|█████████▍| 113/119 [01:09<00:03,  1.57it/s]Epoch: 8, train for the 85-th batch, train loss: 0.5054340958595276:  58%|███████▌     | 85/146 [00:50<00:39,  1.54it/s]Epoch: 9, train for the 114-th batch, train loss: 0.16243058443069458:  96%|█████████▌| 114/119 [01:09<00:03,  1.55it/s]Epoch: 16, train for the 104-th batch, train loss: 0.6350390315055847:  68%|██████▊   | 103/151 [00:22<00:10,  4.49it/s]Epoch: 16, train for the 104-th batch, train loss: 0.6350390315055847:  69%|██████▉   | 104/151 [00:22<00:10,  4.50it/s]Epoch: 5, train for the 172-th batch, train loss: 0.600946843624115:  72%|████████▋   | 171/237 [01:42<00:38,  1.71it/s]Epoch: 5, train for the 172-th batch, train loss: 0.600946843624115:  73%|████████▋   | 172/237 [01:42<00:38,  1.71it/s]Epoch: 16, train for the 105-th batch, train loss: 0.5795840620994568:  69%|██████▉   | 104/151 [00:22<00:10,  4.50it/s]Epoch: 16, train for the 105-th batch, train loss: 0.5795840620994568:  70%|██████▉   | 105/151 [00:22<00:10,  4.51it/s]Epoch: 16, train for the 106-th batch, train loss: 0.53236985206604:  70%|████████▎   | 105/151 [00:23<00:10,  4.51it/s]Epoch: 16, train for the 106-th batch, train loss: 0.53236985206604:  70%|████████▍   | 106/151 [00:23<00:09,  4.52it/s]Epoch: 3, train for the 358-th batch, train loss: 0.5301825404167175:  93%|██████████▎| 357/383 [03:36<00:16,  1.59it/s]Epoch: 3, train for the 358-th batch, train loss: 0.5301825404167175:  93%|██████████▎| 358/383 [03:36<00:15,  1.62it/s]Epoch: 8, train for the 86-th batch, train loss: 0.5176343321800232:  58%|███████▌     | 85/146 [00:51<00:39,  1.54it/s]Epoch: 9, train for the 115-th batch, train loss: 0.18546800315380096:  96%|█████████▌| 114/119 [01:09<00:03,  1.55it/s]Epoch: 8, train for the 86-th batch, train loss: 0.5176343321800232:  59%|███████▋     | 86/146 [00:51<00:39,  1.53it/s]Epoch: 9, train for the 115-th batch, train loss: 0.18546800315380096:  97%|█████████▋| 115/119 [01:09<00:02,  1.54it/s]Epoch: 16, train for the 107-th batch, train loss: 0.5269933342933655:  70%|███████   | 106/151 [00:23<00:09,  4.52it/s]Epoch: 16, train for the 107-th batch, train loss: 0.5269933342933655:  71%|███████   | 107/151 [00:23<00:09,  4.52it/s]Epoch: 5, train for the 173-th batch, train loss: 0.6317406296730042:  73%|███████▉   | 172/237 [01:43<00:38,  1.71it/s]Epoch: 5, train for the 173-th batch, train loss: 0.6317406296730042:  73%|████████   | 173/237 [01:43<00:37,  1.71it/s]Epoch: 16, train for the 108-th batch, train loss: 0.5600537061691284:  71%|███████   | 107/151 [00:23<00:09,  4.52it/s]Epoch: 16, train for the 108-th batch, train loss: 0.5600537061691284:  72%|███████▏  | 108/151 [00:23<00:09,  4.51it/s]Epoch: 3, train for the 359-th batch, train loss: 0.4572368264198303:  93%|██████████▎| 358/383 [03:37<00:15,  1.62it/s]Epoch: 3, train for the 359-th batch, train loss: 0.4572368264198303:  94%|██████████▎| 359/383 [03:37<00:14,  1.65it/s]Epoch: 16, train for the 109-th batch, train loss: 0.5432516932487488:  72%|███████▏  | 108/151 [00:23<00:09,  4.51it/s]Epoch: 16, train for the 109-th batch, train loss: 0.5432516932487488:  72%|███████▏  | 109/151 [00:23<00:09,  4.51it/s]Epoch: 8, train for the 87-th batch, train loss: 0.5608035326004028:  59%|███████▋     | 86/146 [00:52<00:39,  1.53it/s]Epoch: 9, train for the 116-th batch, train loss: 0.1173456609249115:  97%|██████████▋| 115/119 [01:10<00:02,  1.54it/s]Epoch: 8, train for the 87-th batch, train loss: 0.5608035326004028:  60%|███████▋     | 87/146 [00:52<00:38,  1.52it/s]Epoch: 9, train for the 116-th batch, train loss: 0.1173456609249115:  97%|██████████▋| 116/119 [01:10<00:01,  1.53it/s]Epoch: 5, train for the 174-th batch, train loss: 0.6135194301605225:  73%|████████   | 173/237 [01:43<00:37,  1.71it/s]Epoch: 5, train for the 174-th batch, train loss: 0.6135194301605225:  73%|████████   | 174/237 [01:43<00:36,  1.71it/s]Epoch: 16, train for the 110-th batch, train loss: 0.5529744029045105:  72%|███████▏  | 109/151 [00:23<00:09,  4.51it/s]Epoch: 16, train for the 110-th batch, train loss: 0.5529744029045105:  73%|███████▎  | 110/151 [00:23<00:09,  4.51it/s]Epoch: 16, train for the 111-th batch, train loss: 0.5250612497329712:  73%|███████▎  | 110/151 [00:24<00:09,  4.51it/s]Epoch: 16, train for the 111-th batch, train loss: 0.5250612497329712:  74%|███████▎  | 111/151 [00:24<00:08,  4.51it/s]Epoch: 3, train for the 360-th batch, train loss: 0.47899329662323:  94%|████████████▏| 359/383 [03:37<00:14,  1.65it/s]Epoch: 3, train for the 360-th batch, train loss: 0.47899329662323:  94%|████████████▏| 360/383 [03:37<00:13,  1.67it/s]Epoch: 16, train for the 112-th batch, train loss: 0.5436128973960876:  74%|███████▎  | 111/151 [00:24<00:08,  4.51it/s]Epoch: 16, train for the 112-th batch, train loss: 0.5436128973960876:  74%|███████▍  | 112/151 [00:24<00:08,  4.49it/s]Epoch: 5, train for the 175-th batch, train loss: 0.6326155662536621:  73%|████████   | 174/237 [01:44<00:36,  1.71it/s]Epoch: 5, train for the 175-th batch, train loss: 0.6326155662536621:  74%|████████   | 175/237 [01:44<00:36,  1.71it/s]Epoch: 9, train for the 117-th batch, train loss: 0.18297293782234192:  97%|█████████▋| 116/119 [01:11<00:01,  1.53it/s]Epoch: 8, train for the 88-th batch, train loss: 0.5704569220542908:  60%|███████▋     | 87/146 [00:52<00:38,  1.52it/s]Epoch: 9, train for the 117-th batch, train loss: 0.18297293782234192:  98%|█████████▊| 117/119 [01:11<00:01,  1.52it/s]Epoch: 8, train for the 88-th batch, train loss: 0.5704569220542908:  60%|███████▊     | 88/146 [00:52<00:38,  1.52it/s]Epoch: 16, train for the 113-th batch, train loss: 0.5502638816833496:  74%|███████▍  | 112/151 [00:24<00:08,  4.49it/s]Epoch: 16, train for the 113-th batch, train loss: 0.5502638816833496:  75%|███████▍  | 113/151 [00:24<00:08,  4.49it/s]Epoch: 3, train for the 361-th batch, train loss: 0.4274807572364807:  94%|██████████▎| 360/383 [03:38<00:13,  1.67it/s]Epoch: 3, train for the 361-th batch, train loss: 0.4274807572364807:  94%|██████████▎| 361/383 [03:38<00:13,  1.68it/s]Epoch: 16, train for the 114-th batch, train loss: 0.4952422082424164:  75%|███████▍  | 113/151 [00:24<00:08,  4.49it/s]Epoch: 16, train for the 114-th batch, train loss: 0.4952422082424164:  75%|███████▌  | 114/151 [00:24<00:08,  4.50it/s]Epoch: 16, train for the 115-th batch, train loss: 0.5063750147819519:  75%|███████▌  | 114/151 [00:25<00:08,  4.50it/s]Epoch: 16, train for the 115-th batch, train loss: 0.5063750147819519:  76%|███████▌  | 115/151 [00:25<00:08,  4.50it/s]Epoch: 5, train for the 176-th batch, train loss: 0.6232195496559143:  74%|████████   | 175/237 [01:45<00:36,  1.71it/s]Epoch: 5, train for the 176-th batch, train loss: 0.6232195496559143:  74%|████████▏  | 176/237 [01:45<00:35,  1.71it/s]Epoch: 9, train for the 118-th batch, train loss: 0.1260678470134735:  98%|██████████▊| 117/119 [01:11<00:01,  1.52it/s]Epoch: 8, train for the 89-th batch, train loss: 0.516536295413971:  60%|████████▍     | 88/146 [00:53<00:38,  1.52it/s]Epoch: 9, train for the 118-th batch, train loss: 0.1260678470134735:  99%|██████████▉| 118/119 [01:11<00:00,  1.52it/s]Epoch: 8, train for the 89-th batch, train loss: 0.516536295413971:  61%|████████▌     | 89/146 [00:53<00:37,  1.51it/s]Epoch: 16, train for the 116-th batch, train loss: 0.5136238932609558:  76%|███████▌  | 115/151 [00:25<00:08,  4.50it/s]Epoch: 16, train for the 116-th batch, train loss: 0.5136238932609558:  77%|███████▋  | 116/151 [00:25<00:07,  4.50it/s]Epoch: 3, train for the 362-th batch, train loss: 0.4064824879169464:  94%|██████████▎| 361/383 [03:38<00:13,  1.68it/s]Epoch: 3, train for the 362-th batch, train loss: 0.4064824879169464:  95%|██████████▍| 362/383 [03:38<00:12,  1.69it/s]Epoch: 9, train for the 119-th batch, train loss: 0.20339335501194:  99%|████████████▉| 118/119 [01:12<00:00,  1.52it/s]Epoch: 9, train for the 119-th batch, train loss: 0.20339335501194: 100%|█████████████| 119/119 [01:12<00:00,  1.84it/s]Epoch: 9, train for the 119-th batch, train loss: 0.20339335501194: 100%|█████████████| 119/119 [01:12<00:00,  1.65it/s]
  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 16, train for the 117-th batch, train loss: 0.5115025043487549:  77%|███████▋  | 116/151 [00:25<00:07,  4.50it/s]Epoch: 16, train for the 117-th batch, train loss: 0.5115025043487549:  77%|███████▋  | 117/151 [00:25<00:07,  4.49it/s]Epoch: 5, train for the 177-th batch, train loss: 0.643780529499054:  74%|████████▉   | 176/237 [01:45<00:35,  1.71it/s]Epoch: 5, train for the 177-th batch, train loss: 0.643780529499054:  75%|████████▉   | 177/237 [01:45<00:35,  1.71it/s]Epoch: 16, train for the 118-th batch, train loss: 0.49813494086265564:  77%|██████▉  | 117/151 [00:25<00:07,  4.49it/s]Epoch: 16, train for the 118-th batch, train loss: 0.49813494086265564:  78%|███████  | 118/151 [00:25<00:07,  4.50it/s]Epoch: 8, train for the 90-th batch, train loss: 0.5482553243637085:  61%|███████▉     | 89/146 [00:53<00:37,  1.51it/s]Epoch: 8, train for the 90-th batch, train loss: 0.5482553243637085:  62%|████████     | 90/146 [00:53<00:35,  1.57it/s]evaluate for the 1-th batch, evaluate loss: 0.12890328466892242:   0%|                           | 0/40 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.12890328466892242:   2%|▍                  | 1/40 [00:00<00:12,  3.15it/s]Epoch: 16, train for the 119-th batch, train loss: 0.5243369340896606:  78%|███████▊  | 118/151 [00:25<00:07,  4.50it/s]Epoch: 16, train for the 119-th batch, train loss: 0.5243369340896606:  79%|███████▉  | 119/151 [00:25<00:07,  4.51it/s]Epoch: 3, train for the 363-th batch, train loss: 0.5111737847328186:  95%|██████████▍| 362/383 [03:39<00:12,  1.69it/s]Epoch: 3, train for the 363-th batch, train loss: 0.5111737847328186:  95%|██████████▍| 363/383 [03:39<00:11,  1.69it/s]evaluate for the 2-th batch, evaluate loss: 0.17543338239192963:   2%|▍                  | 1/40 [00:00<00:12,  3.15it/s]evaluate for the 2-th batch, evaluate loss: 0.17543338239192963:   5%|▉                  | 2/40 [00:00<00:10,  3.47it/s]Epoch: 16, train for the 120-th batch, train loss: 0.5726425051689148:  79%|███████▉  | 119/151 [00:26<00:07,  4.51it/s]Epoch: 16, train for the 120-th batch, train loss: 0.5726425051689148:  79%|███████▉  | 120/151 [00:26<00:06,  4.50it/s]Epoch: 5, train for the 178-th batch, train loss: 0.6223771572113037:  75%|████████▏  | 177/237 [01:46<00:35,  1.71it/s]Epoch: 5, train for the 178-th batch, train loss: 0.6223771572113037:  75%|████████▎  | 178/237 [01:46<00:34,  1.71it/s]evaluate for the 3-th batch, evaluate loss: 0.21519245207309723:   5%|▉                  | 2/40 [00:00<00:10,  3.47it/s]evaluate for the 3-th batch, evaluate loss: 0.21519245207309723:   8%|█▍                 | 3/40 [00:00<00:10,  3.38it/s]Epoch: 8, train for the 91-th batch, train loss: 0.5369145274162292:  62%|████████     | 90/146 [00:54<00:35,  1.57it/s]Epoch: 8, train for the 91-th batch, train loss: 0.5369145274162292:  62%|████████     | 91/146 [00:54<00:34,  1.60it/s]Epoch: 16, train for the 121-th batch, train loss: 0.5035805106163025:  79%|███████▉  | 120/151 [00:26<00:06,  4.50it/s]Epoch: 16, train for the 121-th batch, train loss: 0.5035805106163025:  80%|████████  | 121/151 [00:26<00:06,  4.51it/s]Epoch: 3, train for the 364-th batch, train loss: 0.42544853687286377:  95%|█████████▍| 363/383 [03:40<00:11,  1.69it/s]Epoch: 3, train for the 364-th batch, train loss: 0.42544853687286377:  95%|█████████▌| 364/383 [03:40<00:11,  1.70it/s]Epoch: 16, train for the 122-th batch, train loss: 0.5361663699150085:  80%|████████  | 121/151 [00:26<00:06,  4.51it/s]Epoch: 16, train for the 122-th batch, train loss: 0.5361663699150085:  81%|████████  | 122/151 [00:26<00:06,  4.47it/s]evaluate for the 4-th batch, evaluate loss: 0.1353677660226822:   8%|█▌                  | 3/40 [00:01<00:10,  3.38it/s]evaluate for the 4-th batch, evaluate loss: 0.1353677660226822:  10%|██                  | 4/40 [00:01<00:10,  3.58it/s]Epoch: 16, train for the 123-th batch, train loss: 0.5258854031562805:  81%|████████  | 122/151 [00:26<00:06,  4.47it/s]Epoch: 16, train for the 123-th batch, train loss: 0.5258854031562805:  81%|████████▏ | 123/151 [00:26<00:06,  4.47it/s]Epoch: 5, train for the 179-th batch, train loss: 0.6135774254798889:  75%|████████▎  | 178/237 [01:46<00:34,  1.71it/s]Epoch: 5, train for the 179-th batch, train loss: 0.6135774254798889:  76%|████████▎  | 179/237 [01:46<00:33,  1.71it/s]evaluate for the 5-th batch, evaluate loss: 0.17999869585037231:  10%|█▉                 | 4/40 [00:01<00:10,  3.58it/s]evaluate for the 5-th batch, evaluate loss: 0.17999869585037231:  12%|██▍                | 5/40 [00:01<00:09,  3.51it/s]Epoch: 8, train for the 92-th batch, train loss: 0.5500553846359253:  62%|████████     | 91/146 [00:55<00:34,  1.60it/s]Epoch: 8, train for the 92-th batch, train loss: 0.5500553846359253:  63%|████████▏    | 92/146 [00:55<00:33,  1.62it/s]Epoch: 16, train for the 124-th batch, train loss: 0.5513097047805786:  81%|████████▏ | 123/151 [00:27<00:06,  4.47it/s]Epoch: 16, train for the 124-th batch, train loss: 0.5513097047805786:  82%|████████▏ | 124/151 [00:27<00:06,  4.49it/s]Epoch: 3, train for the 365-th batch, train loss: 0.4493367671966553:  95%|██████████▍| 364/383 [03:40<00:11,  1.70it/s]Epoch: 3, train for the 365-th batch, train loss: 0.4493367671966553:  95%|██████████▍| 365/383 [03:40<00:10,  1.70it/s]evaluate for the 6-th batch, evaluate loss: 0.1655438393354416:  12%|██▌                 | 5/40 [00:01<00:09,  3.51it/s]evaluate for the 6-th batch, evaluate loss: 0.1655438393354416:  15%|███                 | 6/40 [00:01<00:09,  3.62it/s]Epoch: 16, train for the 125-th batch, train loss: 0.5628933310508728:  82%|████████▏ | 124/151 [00:27<00:06,  4.49it/s]Epoch: 16, train for the 125-th batch, train loss: 0.5628933310508728:  83%|████████▎ | 125/151 [00:27<00:05,  4.50it/s]Epoch: 5, train for the 180-th batch, train loss: 0.6393555402755737:  76%|████████▎  | 179/237 [01:47<00:33,  1.71it/s]Epoch: 5, train for the 180-th batch, train loss: 0.6393555402755737:  76%|████████▎  | 180/237 [01:47<00:33,  1.71it/s]evaluate for the 7-th batch, evaluate loss: 0.11494876444339752:  15%|██▊                | 6/40 [00:01<00:09,  3.62it/s]evaluate for the 7-th batch, evaluate loss: 0.11494876444339752:  18%|███▎               | 7/40 [00:01<00:09,  3.57it/s]Epoch: 16, train for the 126-th batch, train loss: 0.5564959049224854:  83%|████████▎ | 125/151 [00:27<00:05,  4.50it/s]Epoch: 16, train for the 126-th batch, train loss: 0.5564959049224854:  83%|████████▎ | 126/151 [00:27<00:05,  4.50it/s]Epoch: 8, train for the 93-th batch, train loss: 0.5363781452178955:  63%|████████▏    | 92/146 [00:55<00:33,  1.62it/s]Epoch: 8, train for the 93-th batch, train loss: 0.5363781452178955:  64%|████████▎    | 93/146 [00:55<00:32,  1.63it/s]Epoch: 16, train for the 127-th batch, train loss: 0.5382880568504333:  83%|████████▎ | 126/151 [00:27<00:05,  4.50it/s]Epoch: 16, train for the 127-th batch, train loss: 0.5382880568504333:  84%|████████▍ | 127/151 [00:27<00:05,  4.51it/s]evaluate for the 8-th batch, evaluate loss: 0.1260029822587967:  18%|███▌                | 7/40 [00:02<00:09,  3.57it/s]evaluate for the 8-th batch, evaluate loss: 0.1260029822587967:  20%|████                | 8/40 [00:02<00:08,  3.60it/s]Epoch: 3, train for the 366-th batch, train loss: 0.4541807770729065:  95%|██████████▍| 365/383 [03:41<00:10,  1.70it/s]Epoch: 3, train for the 366-th batch, train loss: 0.4541807770729065:  96%|██████████▌| 366/383 [03:41<00:09,  1.70it/s]Epoch: 16, train for the 128-th batch, train loss: 0.5682033896446228:  84%|████████▍ | 127/151 [00:27<00:05,  4.51it/s]Epoch: 16, train for the 128-th batch, train loss: 0.5682033896446228:  85%|████████▍ | 128/151 [00:27<00:05,  4.51it/s]evaluate for the 9-th batch, evaluate loss: 0.17511799931526184:  20%|███▊               | 8/40 [00:02<00:08,  3.60it/s]evaluate for the 9-th batch, evaluate loss: 0.17511799931526184:  22%|████▎              | 9/40 [00:02<00:08,  3.61it/s]Epoch: 5, train for the 181-th batch, train loss: 0.6604325771331787:  76%|████████▎  | 180/237 [01:48<00:33,  1.71it/s]Epoch: 5, train for the 181-th batch, train loss: 0.6604325771331787:  76%|████████▍  | 181/237 [01:48<00:32,  1.71it/s]Epoch: 16, train for the 129-th batch, train loss: 0.5489645004272461:  85%|████████▍ | 128/151 [00:28<00:05,  4.51it/s]Epoch: 16, train for the 129-th batch, train loss: 0.5489645004272461:  85%|████████▌ | 129/151 [00:28<00:04,  4.52it/s]Epoch: 8, train for the 94-th batch, train loss: 0.5591199398040771:  64%|████████▎    | 93/146 [00:56<00:32,  1.63it/s]Epoch: 8, train for the 94-th batch, train loss: 0.5591199398040771:  64%|████████▎    | 94/146 [00:56<00:31,  1.63it/s]evaluate for the 10-th batch, evaluate loss: 0.1907442808151245:  22%|████▎              | 9/40 [00:02<00:08,  3.61it/s]evaluate for the 10-th batch, evaluate loss: 0.1907442808151245:  25%|████▌             | 10/40 [00:02<00:08,  3.52it/s]Epoch: 3, train for the 367-th batch, train loss: 0.43975454568862915:  96%|█████████▌| 366/383 [03:41<00:09,  1.70it/s]Epoch: 3, train for the 367-th batch, train loss: 0.43975454568862915:  96%|█████████▌| 367/383 [03:41<00:09,  1.71it/s]Epoch: 16, train for the 130-th batch, train loss: 0.544662594795227:  85%|█████████▍ | 129/151 [00:28<00:04,  4.52it/s]Epoch: 16, train for the 130-th batch, train loss: 0.544662594795227:  86%|█████████▍ | 130/151 [00:28<00:04,  4.52it/s]evaluate for the 11-th batch, evaluate loss: 0.17271701991558075:  25%|████▎            | 10/40 [00:03<00:08,  3.52it/s]evaluate for the 11-th batch, evaluate loss: 0.17271701991558075:  28%|████▋            | 11/40 [00:03<00:07,  3.65it/s]Epoch: 16, train for the 131-th batch, train loss: 0.5426336526870728:  86%|████████▌ | 130/151 [00:28<00:04,  4.52it/s]Epoch: 16, train for the 131-th batch, train loss: 0.5426336526870728:  87%|████████▋ | 131/151 [00:28<00:04,  4.52it/s]Epoch: 5, train for the 182-th batch, train loss: 0.6300142407417297:  76%|████████▍  | 181/237 [01:48<00:32,  1.71it/s]Epoch: 5, train for the 182-th batch, train loss: 0.6300142407417297:  77%|████████▍  | 182/237 [01:48<00:32,  1.71it/s]Epoch: 8, train for the 95-th batch, train loss: 0.5170101523399353:  64%|████████▎    | 94/146 [00:56<00:31,  1.63it/s]Epoch: 8, train for the 95-th batch, train loss: 0.5170101523399353:  65%|████████▍    | 95/146 [00:56<00:30,  1.65it/s]Epoch: 16, train for the 132-th batch, train loss: 0.5492412447929382:  87%|████████▋ | 131/151 [00:28<00:04,  4.52it/s]Epoch: 16, train for the 132-th batch, train loss: 0.5492412447929382:  87%|████████▋ | 132/151 [00:28<00:04,  4.50it/s]evaluate for the 12-th batch, evaluate loss: 0.1432117223739624:  28%|████▉             | 11/40 [00:03<00:07,  3.65it/s]evaluate for the 12-th batch, evaluate loss: 0.1432117223739624:  30%|█████▍            | 12/40 [00:03<00:07,  3.54it/s]Epoch: 3, train for the 368-th batch, train loss: 0.4828556776046753:  96%|██████████▌| 367/383 [03:42<00:09,  1.71it/s]Epoch: 3, train for the 368-th batch, train loss: 0.4828556776046753:  96%|██████████▌| 368/383 [03:42<00:08,  1.71it/s]Epoch: 16, train for the 133-th batch, train loss: 0.5377141833305359:  87%|████████▋ | 132/151 [00:29<00:04,  4.50it/s]Epoch: 16, train for the 133-th batch, train loss: 0.5377141833305359:  88%|████████▊ | 133/151 [00:29<00:04,  4.48it/s]evaluate for the 13-th batch, evaluate loss: 0.12820033729076385:  30%|█████            | 12/40 [00:03<00:07,  3.54it/s]evaluate for the 13-th batch, evaluate loss: 0.12820033729076385:  32%|█████▌           | 13/40 [00:03<00:07,  3.71it/s]Epoch: 5, train for the 183-th batch, train loss: 0.6400007605552673:  77%|████████▍  | 182/237 [01:49<00:32,  1.71it/s]Epoch: 5, train for the 183-th batch, train loss: 0.6400007605552673:  77%|████████▍  | 183/237 [01:49<00:31,  1.71it/s]Epoch: 16, train for the 134-th batch, train loss: 0.5373373031616211:  88%|████████▊ | 133/151 [00:29<00:04,  4.48it/s]Epoch: 16, train for the 134-th batch, train loss: 0.5373373031616211:  89%|████████▊ | 134/151 [00:29<00:03,  4.49it/s]Epoch: 8, train for the 96-th batch, train loss: 0.57846599817276:  65%|█████████▊     | 95/146 [00:57<00:30,  1.65it/s]Epoch: 8, train for the 96-th batch, train loss: 0.57846599817276:  66%|█████████▊     | 96/146 [00:57<00:30,  1.65it/s]evaluate for the 14-th batch, evaluate loss: 0.16367104649543762:  32%|█████▌           | 13/40 [00:03<00:07,  3.71it/s]evaluate for the 14-th batch, evaluate loss: 0.16367104649543762:  35%|█████▉           | 14/40 [00:03<00:07,  3.53it/s]Epoch: 3, train for the 369-th batch, train loss: 0.41989028453826904:  96%|█████████▌| 368/383 [03:42<00:08,  1.71it/s]Epoch: 3, train for the 369-th batch, train loss: 0.41989028453826904:  96%|█████████▋| 369/383 [03:42<00:08,  1.70it/s]Epoch: 16, train for the 135-th batch, train loss: 0.5337006449699402:  89%|████████▊ | 134/151 [00:29<00:03,  4.49it/s]Epoch: 16, train for the 135-th batch, train loss: 0.5337006449699402:  89%|████████▉ | 135/151 [00:29<00:03,  4.07it/s]evaluate for the 15-th batch, evaluate loss: 0.1807086020708084:  35%|██████▎           | 14/40 [00:04<00:07,  3.53it/s]evaluate for the 15-th batch, evaluate loss: 0.1807086020708084:  38%|██████▊           | 15/40 [00:04<00:06,  3.60it/s]Epoch: 5, train for the 184-th batch, train loss: 0.6336891651153564:  77%|████████▍  | 183/237 [01:49<00:31,  1.71it/s]Epoch: 5, train for the 184-th batch, train loss: 0.6336891651153564:  78%|████████▌  | 184/237 [01:49<00:31,  1.70it/s]Epoch: 16, train for the 136-th batch, train loss: 0.5276630520820618:  89%|████████▉ | 135/151 [00:29<00:03,  4.07it/s]Epoch: 16, train for the 136-th batch, train loss: 0.5276630520820618:  90%|█████████ | 136/151 [00:29<00:03,  4.20it/s]Epoch: 8, train for the 97-th batch, train loss: 0.5017920136451721:  66%|████████▌    | 96/146 [00:58<00:30,  1.65it/s]Epoch: 8, train for the 97-th batch, train loss: 0.5017920136451721:  66%|████████▋    | 97/146 [00:58<00:29,  1.65it/s]evaluate for the 16-th batch, evaluate loss: 0.1941250115633011:  38%|██████▊           | 15/40 [00:04<00:06,  3.60it/s]evaluate for the 16-th batch, evaluate loss: 0.1941250115633011:  40%|███████▏          | 16/40 [00:04<00:06,  3.44it/s]Epoch: 16, train for the 137-th batch, train loss: 0.6058623790740967:  90%|█████████ | 136/151 [00:30<00:03,  4.20it/s]Epoch: 16, train for the 137-th batch, train loss: 0.6058623790740967:  91%|█████████ | 137/151 [00:30<00:03,  4.29it/s]Epoch: 3, train for the 370-th batch, train loss: 0.4269564747810364:  96%|██████████▌| 369/383 [03:43<00:08,  1.70it/s]Epoch: 3, train for the 370-th batch, train loss: 0.4269564747810364:  97%|██████████▋| 370/383 [03:43<00:07,  1.70it/s]Epoch: 16, train for the 138-th batch, train loss: 0.605143666267395:  91%|█████████▉ | 137/151 [00:30<00:03,  4.29it/s]Epoch: 16, train for the 138-th batch, train loss: 0.605143666267395:  91%|██████████ | 138/151 [00:30<00:02,  4.36it/s]evaluate for the 17-th batch, evaluate loss: 0.13325782120227814:  40%|██████▊          | 16/40 [00:04<00:06,  3.44it/s]evaluate for the 17-th batch, evaluate loss: 0.13325782120227814:  42%|███████▏         | 17/40 [00:04<00:06,  3.51it/s]Epoch: 5, train for the 185-th batch, train loss: 0.6230705380439758:  78%|████████▌  | 184/237 [01:50<00:31,  1.70it/s]Epoch: 5, train for the 185-th batch, train loss: 0.6230705380439758:  78%|████████▌  | 185/237 [01:50<00:30,  1.70it/s]Epoch: 16, train for the 139-th batch, train loss: 0.5582708120346069:  91%|█████████▏| 138/151 [00:30<00:02,  4.36it/s]Epoch: 16, train for the 139-th batch, train loss: 0.5582708120346069:  92%|█████████▏| 139/151 [00:30<00:02,  4.40it/s]evaluate for the 18-th batch, evaluate loss: 0.13933372497558594:  42%|███████▏         | 17/40 [00:05<00:06,  3.51it/s]evaluate for the 18-th batch, evaluate loss: 0.13933372497558594:  45%|███████▋         | 18/40 [00:05<00:06,  3.44it/s]Epoch: 8, train for the 98-th batch, train loss: 0.550887405872345:  66%|█████████▎    | 97/146 [00:58<00:29,  1.65it/s]Epoch: 8, train for the 98-th batch, train loss: 0.550887405872345:  67%|█████████▍    | 98/146 [00:58<00:28,  1.66it/s]Epoch: 3, train for the 371-th batch, train loss: 0.43181726336479187:  97%|█████████▋| 370/383 [03:44<00:07,  1.70it/s]Epoch: 3, train for the 371-th batch, train loss: 0.43181726336479187:  97%|█████████▋| 371/383 [03:44<00:07,  1.70it/s]Epoch: 16, train for the 140-th batch, train loss: 0.5385493636131287:  92%|█████████▏| 139/151 [00:30<00:02,  4.40it/s]Epoch: 16, train for the 140-th batch, train loss: 0.5385493636131287:  93%|█████████▎| 140/151 [00:30<00:02,  4.44it/s]evaluate for the 19-th batch, evaluate loss: 0.15032026171684265:  45%|███████▋         | 18/40 [00:05<00:06,  3.44it/s]evaluate for the 19-th batch, evaluate loss: 0.15032026171684265:  48%|████████         | 19/40 [00:05<00:05,  3.58it/s]Epoch: 16, train for the 141-th batch, train loss: 0.5554422736167908:  93%|█████████▎| 140/151 [00:30<00:02,  4.44it/s]Epoch: 16, train for the 141-th batch, train loss: 0.5554422736167908:  93%|█████████▎| 141/151 [00:30<00:02,  4.46it/s]Epoch: 5, train for the 186-th batch, train loss: 0.6169556379318237:  78%|████████▌  | 185/237 [01:51<00:30,  1.70it/s]Epoch: 5, train for the 186-th batch, train loss: 0.6169556379318237:  78%|████████▋  | 186/237 [01:51<00:31,  1.61it/s]evaluate for the 20-th batch, evaluate loss: 0.15088053047657013:  48%|████████         | 19/40 [00:05<00:05,  3.58it/s]evaluate for the 20-th batch, evaluate loss: 0.15088053047657013:  50%|████████▌        | 20/40 [00:05<00:05,  3.52it/s]Epoch: 16, train for the 142-th batch, train loss: 0.5680683851242065:  93%|█████████▎| 141/151 [00:31<00:02,  4.46it/s]Epoch: 16, train for the 142-th batch, train loss: 0.5680683851242065:  94%|█████████▍| 142/151 [00:31<00:02,  4.45it/s]Epoch: 3, train for the 372-th batch, train loss: 0.5209214687347412:  97%|██████████▋| 371/383 [03:44<00:07,  1.70it/s]Epoch: 3, train for the 372-th batch, train loss: 0.5209214687347412:  97%|██████████▋| 372/383 [03:44<00:06,  1.78it/s]Epoch: 8, train for the 99-th batch, train loss: 0.5371415019035339:  67%|████████▋    | 98/146 [00:59<00:28,  1.66it/s]Epoch: 8, train for the 99-th batch, train loss: 0.5371415019035339:  68%|████████▊    | 99/146 [00:59<00:28,  1.66it/s]Epoch: 16, train for the 143-th batch, train loss: 0.46990716457366943:  94%|████████▍| 142/151 [00:31<00:02,  4.45it/s]Epoch: 16, train for the 143-th batch, train loss: 0.46990716457366943:  95%|████████▌| 143/151 [00:31<00:01,  4.46it/s]evaluate for the 21-th batch, evaluate loss: 0.09190137684345245:  50%|████████▌        | 20/40 [00:05<00:05,  3.52it/s]evaluate for the 21-th batch, evaluate loss: 0.09190137684345245:  52%|████████▉        | 21/40 [00:05<00:05,  3.62it/s]Epoch: 16, train for the 144-th batch, train loss: 0.5097237825393677:  95%|█████████▍| 143/151 [00:31<00:01,  4.46it/s]Epoch: 16, train for the 144-th batch, train loss: 0.5097237825393677:  95%|█████████▌| 144/151 [00:31<00:01,  4.03it/s]Epoch: 5, train for the 187-th batch, train loss: 0.6494382619857788:  78%|████████▋  | 186/237 [01:51<00:31,  1.61it/s]Epoch: 5, train for the 187-th batch, train loss: 0.6494382619857788:  79%|████████▋  | 187/237 [01:51<00:30,  1.64it/s]evaluate for the 22-th batch, evaluate loss: 0.15047600865364075:  52%|████████▉        | 21/40 [00:06<00:05,  3.62it/s]evaluate for the 22-th batch, evaluate loss: 0.15047600865364075:  55%|█████████▎       | 22/40 [00:06<00:05,  3.56it/s]Epoch: 3, train for the 373-th batch, train loss: 0.47042861580848694:  97%|█████████▋| 372/383 [03:45<00:06,  1.78it/s]Epoch: 3, train for the 373-th batch, train loss: 0.47042861580848694:  97%|█████████▋| 373/383 [03:45<00:05,  1.76it/s]Epoch: 8, train for the 100-th batch, train loss: 0.5580461621284485:  68%|████████▏   | 99/146 [00:59<00:28,  1.66it/s]Epoch: 8, train for the 100-th batch, train loss: 0.5580461621284485:  68%|███████▌   | 100/146 [00:59<00:27,  1.66it/s]Epoch: 16, train for the 145-th batch, train loss: 0.5181540846824646:  95%|█████████▌| 144/151 [00:31<00:01,  4.03it/s]Epoch: 16, train for the 145-th batch, train loss: 0.5181540846824646:  96%|█████████▌| 145/151 [00:31<00:01,  4.15it/s]evaluate for the 23-th batch, evaluate loss: 0.15443216264247894:  55%|█████████▎       | 22/40 [00:06<00:05,  3.56it/s]evaluate for the 23-th batch, evaluate loss: 0.15443216264247894:  57%|█████████▊       | 23/40 [00:06<00:04,  3.58it/s]Epoch: 16, train for the 146-th batch, train loss: 0.5270031094551086:  96%|█████████▌| 145/151 [00:32<00:01,  4.15it/s]Epoch: 16, train for the 146-th batch, train loss: 0.5270031094551086:  97%|█████████▋| 146/151 [00:32<00:01,  4.26it/s]evaluate for the 24-th batch, evaluate loss: 0.14908793568611145:  57%|█████████▊       | 23/40 [00:06<00:04,  3.58it/s]evaluate for the 24-th batch, evaluate loss: 0.14908793568611145:  60%|██████████▏      | 24/40 [00:06<00:04,  3.59it/s]Epoch: 5, train for the 188-th batch, train loss: 0.6532460451126099:  79%|████████▋  | 187/237 [01:52<00:30,  1.64it/s]Epoch: 5, train for the 188-th batch, train loss: 0.6532460451126099:  79%|████████▋  | 188/237 [01:52<00:29,  1.67it/s]Epoch: 16, train for the 147-th batch, train loss: 0.5423917174339294:  97%|█████████▋| 146/151 [00:32<00:01,  4.26it/s]Epoch: 16, train for the 147-th batch, train loss: 0.5423917174339294:  97%|█████████▋| 147/151 [00:32<00:00,  4.33it/s]Epoch: 3, train for the 374-th batch, train loss: 0.4979809522628784:  97%|██████████▋| 373/383 [03:45<00:05,  1.76it/s]Epoch: 3, train for the 374-th batch, train loss: 0.4979809522628784:  98%|██████████▋| 374/383 [03:45<00:05,  1.73it/s]Epoch: 8, train for the 101-th batch, train loss: 0.5302927494049072:  68%|███████▌   | 100/146 [01:00<00:27,  1.66it/s]Epoch: 8, train for the 101-th batch, train loss: 0.5302927494049072:  69%|███████▌   | 101/146 [01:00<00:27,  1.65it/s]evaluate for the 25-th batch, evaluate loss: 0.15227088332176208:  60%|██████████▏      | 24/40 [00:07<00:04,  3.59it/s]evaluate for the 25-th batch, evaluate loss: 0.15227088332176208:  62%|██████████▋      | 25/40 [00:07<00:04,  3.50it/s]Epoch: 16, train for the 148-th batch, train loss: 0.58602374792099:  97%|███████████▋| 147/151 [00:32<00:00,  4.33it/s]Epoch: 16, train for the 148-th batch, train loss: 0.58602374792099:  98%|███████████▊| 148/151 [00:32<00:00,  4.38it/s]Epoch: 16, train for the 149-th batch, train loss: 0.5443754196166992:  98%|█████████▊| 148/151 [00:32<00:00,  4.38it/s]Epoch: 16, train for the 149-th batch, train loss: 0.5443754196166992:  99%|█████████▊| 149/151 [00:32<00:00,  4.42it/s]evaluate for the 26-th batch, evaluate loss: 0.12154432386159897:  62%|██████████▋      | 25/40 [00:07<00:04,  3.50it/s]evaluate for the 26-th batch, evaluate loss: 0.12154432386159897:  65%|███████████      | 26/40 [00:07<00:03,  3.65it/s]Epoch: 5, train for the 189-th batch, train loss: 0.65964674949646:  79%|██████████▎  | 188/237 [01:52<00:29,  1.67it/s]Epoch: 5, train for the 189-th batch, train loss: 0.65964674949646:  80%|██████████▎  | 189/237 [01:52<00:28,  1.69it/s]Epoch: 3, train for the 375-th batch, train loss: 0.5354930758476257:  98%|██████████▋| 374/383 [03:46<00:05,  1.73it/s]Epoch: 3, train for the 375-th batch, train loss: 0.5354930758476257:  98%|██████████▊| 375/383 [03:46<00:04,  1.71it/s]Epoch: 16, train for the 150-th batch, train loss: 0.5057624578475952:  99%|█████████▊| 149/151 [00:32<00:00,  4.42it/s]Epoch: 16, train for the 150-th batch, train loss: 0.5057624578475952:  99%|█████████▉| 150/151 [00:32<00:00,  4.45it/s]Epoch: 8, train for the 102-th batch, train loss: 0.5472618937492371:  69%|███████▌   | 101/146 [01:01<00:27,  1.65it/s]Epoch: 8, train for the 102-th batch, train loss: 0.5472618937492371:  70%|███████▋   | 102/146 [01:01<00:26,  1.66it/s]evaluate for the 27-th batch, evaluate loss: 0.14205899834632874:  65%|███████████      | 26/40 [00:07<00:03,  3.65it/s]evaluate for the 27-th batch, evaluate loss: 0.14205899834632874:  68%|███████████▍     | 27/40 [00:07<00:03,  3.54it/s]Epoch: 16, train for the 151-th batch, train loss: 0.5736538171768188:  99%|█████████▉| 150/151 [00:33<00:00,  4.45it/s]Epoch: 16, train for the 151-th batch, train loss: 0.5736538171768188: 100%|██████████| 151/151 [00:33<00:00,  4.94it/s]Epoch: 16, train for the 151-th batch, train loss: 0.5736538171768188: 100%|██████████| 151/151 [00:33<00:00,  4.56it/s]
  0%|                                                                                            | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.4957250952720642:   0%|                            | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.4957250952720642:   2%|▍                   | 1/46 [00:00<00:04,  9.73it/s]evaluate for the 28-th batch, evaluate loss: 0.11378765106201172:  68%|███████████▍     | 27/40 [00:07<00:03,  3.54it/s]evaluate for the 28-th batch, evaluate loss: 0.11378765106201172:  70%|███████████▉     | 28/40 [00:07<00:03,  3.71it/s]evaluate for the 2-th batch, evaluate loss: 0.5023879408836365:   2%|▍                   | 1/46 [00:00<00:04,  9.73it/s]evaluate for the 2-th batch, evaluate loss: 0.5023879408836365:   4%|▊                   | 2/46 [00:00<00:04,  9.63it/s]Epoch: 5, train for the 190-th batch, train loss: 0.6269373297691345:  80%|████████▊  | 189/237 [01:53<00:28,  1.69it/s]Epoch: 5, train for the 190-th batch, train loss: 0.6269373297691345:  80%|████████▊  | 190/237 [01:53<00:27,  1.68it/s]evaluate for the 3-th batch, evaluate loss: 0.48133811354637146:   4%|▊                  | 2/46 [00:00<00:04,  9.63it/s]evaluate for the 3-th batch, evaluate loss: 0.48133811354637146:   7%|█▏                 | 3/46 [00:00<00:04,  9.63it/s]evaluate for the 4-th batch, evaluate loss: 0.510449230670929:   7%|█▎                   | 3/46 [00:00<00:04,  9.63it/s]evaluate for the 4-th batch, evaluate loss: 0.510449230670929:   9%|█▊                   | 4/46 [00:00<00:04,  9.61it/s]Epoch: 3, train for the 376-th batch, train loss: 0.4143007695674896:  98%|██████████▊| 375/383 [03:46<00:04,  1.71it/s]Epoch: 3, train for the 376-th batch, train loss: 0.4143007695674896:  98%|██████████▊| 376/383 [03:46<00:04,  1.69it/s]Epoch: 8, train for the 103-th batch, train loss: 0.5401124954223633:  70%|███████▋   | 102/146 [01:01<00:26,  1.66it/s]Epoch: 8, train for the 103-th batch, train loss: 0.5401124954223633:  71%|███████▊   | 103/146 [01:01<00:25,  1.65it/s]evaluate for the 29-th batch, evaluate loss: 0.15959686040878296:  70%|███████████▉     | 28/40 [00:08<00:03,  3.71it/s]evaluate for the 29-th batch, evaluate loss: 0.15959686040878296:  72%|████████████▎    | 29/40 [00:08<00:03,  3.54it/s]evaluate for the 5-th batch, evaluate loss: 0.48040053248405457:   9%|█▋                 | 4/46 [00:00<00:04,  9.61it/s]evaluate for the 5-th batch, evaluate loss: 0.48040053248405457:  11%|██                 | 5/46 [00:00<00:04,  9.62it/s]evaluate for the 6-th batch, evaluate loss: 0.5481290221214294:  11%|██▏                 | 5/46 [00:00<00:04,  9.62it/s]evaluate for the 6-th batch, evaluate loss: 0.5481290221214294:  13%|██▌                 | 6/46 [00:00<00:04,  9.64it/s]evaluate for the 7-th batch, evaluate loss: 0.46809759736061096:  13%|██▍                | 6/46 [00:00<00:04,  9.64it/s]evaluate for the 7-th batch, evaluate loss: 0.46809759736061096:  15%|██▉                | 7/46 [00:00<00:04,  9.66it/s]evaluate for the 30-th batch, evaluate loss: 0.16718730330467224:  72%|████████████▎    | 29/40 [00:08<00:03,  3.54it/s]evaluate for the 30-th batch, evaluate loss: 0.16718730330467224:  75%|████████████▊    | 30/40 [00:08<00:02,  3.60it/s]evaluate for the 8-th batch, evaluate loss: 0.5566625595092773:  15%|███                 | 7/46 [00:00<00:04,  9.66it/s]evaluate for the 8-th batch, evaluate loss: 0.5566625595092773:  17%|███▍                | 8/46 [00:00<00:03,  9.68it/s]Epoch: 5, train for the 191-th batch, train loss: 0.640318751335144:  80%|█████████▌  | 190/237 [01:54<00:27,  1.68it/s]Epoch: 5, train for the 191-th batch, train loss: 0.640318751335144:  81%|█████████▋  | 191/237 [01:54<00:27,  1.67it/s]evaluate for the 9-th batch, evaluate loss: 0.523082971572876:  17%|███▋                 | 8/46 [00:00<00:03,  9.68it/s]evaluate for the 9-th batch, evaluate loss: 0.523082971572876:  20%|████                 | 9/46 [00:00<00:03,  9.69it/s]Epoch: 3, train for the 377-th batch, train loss: 0.4830685257911682:  98%|██████████▊| 376/383 [03:47<00:04,  1.69it/s]Epoch: 3, train for the 377-th batch, train loss: 0.4830685257911682:  98%|██████████▊| 377/383 [03:47<00:03,  1.68it/s]evaluate for the 10-th batch, evaluate loss: 0.530179500579834:  20%|███▉                | 9/46 [00:01<00:03,  9.69it/s]evaluate for the 10-th batch, evaluate loss: 0.530179500579834:  22%|████▏              | 10/46 [00:01<00:03,  9.71it/s]evaluate for the 31-th batch, evaluate loss: 0.1743701547384262:  75%|█████████████▌    | 30/40 [00:08<00:02,  3.60it/s]evaluate for the 31-th batch, evaluate loss: 0.1743701547384262:  78%|█████████████▉    | 31/40 [00:08<00:02,  3.51it/s]Epoch: 8, train for the 104-th batch, train loss: 0.5468742251396179:  71%|███████▊   | 103/146 [01:02<00:25,  1.65it/s]Epoch: 8, train for the 104-th batch, train loss: 0.5468742251396179:  71%|███████▊   | 104/146 [01:02<00:25,  1.65it/s]evaluate for the 11-th batch, evaluate loss: 0.5224903225898743:  22%|███▉              | 10/46 [00:01<00:03,  9.71it/s]evaluate for the 11-th batch, evaluate loss: 0.5224903225898743:  24%|████▎             | 11/46 [00:01<00:03,  9.70it/s]evaluate for the 12-th batch, evaluate loss: 0.4744853079319:  24%|█████                | 11/46 [00:01<00:03,  9.70it/s]evaluate for the 12-th batch, evaluate loss: 0.4744853079319:  26%|█████▍               | 12/46 [00:01<00:03,  9.69it/s]evaluate for the 32-th batch, evaluate loss: 0.12907563149929047:  78%|█████████████▏   | 31/40 [00:08<00:02,  3.51it/s]evaluate for the 32-th batch, evaluate loss: 0.12907563149929047:  80%|█████████████▌   | 32/40 [00:08<00:02,  3.62it/s]evaluate for the 13-th batch, evaluate loss: 0.49242380261421204:  26%|████▍            | 12/46 [00:01<00:03,  9.69it/s]evaluate for the 13-th batch, evaluate loss: 0.49242380261421204:  28%|████▊            | 13/46 [00:01<00:03,  9.68it/s]evaluate for the 14-th batch, evaluate loss: 0.5877318382263184:  28%|█████             | 13/46 [00:01<00:03,  9.68it/s]evaluate for the 14-th batch, evaluate loss: 0.5877318382263184:  30%|█████▍            | 14/46 [00:01<00:03,  9.68it/s]Epoch: 5, train for the 192-th batch, train loss: 0.6358405947685242:  81%|████████▊  | 191/237 [01:54<00:27,  1.67it/s]Epoch: 5, train for the 192-th batch, train loss: 0.6358405947685242:  81%|████████▉  | 192/237 [01:54<00:27,  1.66it/s]evaluate for the 15-th batch, evaluate loss: 0.5367050170898438:  30%|█████▍            | 14/46 [00:01<00:03,  9.68it/s]evaluate for the 15-th batch, evaluate loss: 0.5367050170898438:  33%|█████▊            | 15/46 [00:01<00:03,  9.71it/s]evaluate for the 33-th batch, evaluate loss: 0.1375698298215866:  80%|██████████████▍   | 32/40 [00:09<00:02,  3.62it/s]evaluate for the 33-th batch, evaluate loss: 0.1375698298215866:  82%|██████████████▊   | 33/40 [00:09<00:01,  3.55it/s]Epoch: 3, train for the 378-th batch, train loss: 0.4903796315193176:  98%|██████████▊| 377/383 [03:48<00:03,  1.68it/s]Epoch: 3, train for the 378-th batch, train loss: 0.4903796315193176:  99%|██████████▊| 378/383 [03:48<00:03,  1.66it/s]evaluate for the 16-th batch, evaluate loss: 0.5690942406654358:  33%|█████▊            | 15/46 [00:01<00:03,  9.71it/s]evaluate for the 16-th batch, evaluate loss: 0.5690942406654358:  35%|██████▎           | 16/46 [00:01<00:03,  9.69it/s]Epoch: 8, train for the 105-th batch, train loss: 0.5498137474060059:  71%|███████▊   | 104/146 [01:02<00:25,  1.65it/s]Epoch: 8, train for the 105-th batch, train loss: 0.5498137474060059:  72%|███████▉   | 105/146 [01:02<00:24,  1.66it/s]evaluate for the 17-th batch, evaluate loss: 0.4508306682109833:  35%|██████▎           | 16/46 [00:01<00:03,  9.69it/s]evaluate for the 17-th batch, evaluate loss: 0.4508306682109833:  37%|██████▋           | 17/46 [00:01<00:02,  9.69it/s]evaluate for the 18-th batch, evaluate loss: 0.5002483129501343:  37%|██████▋           | 17/46 [00:01<00:02,  9.69it/s]evaluate for the 18-th batch, evaluate loss: 0.5002483129501343:  39%|███████           | 18/46 [00:01<00:02,  9.66it/s]evaluate for the 34-th batch, evaluate loss: 0.11039175093173981:  82%|██████████████   | 33/40 [00:09<00:01,  3.55it/s]evaluate for the 34-th batch, evaluate loss: 0.11039175093173981:  85%|██████████████▍  | 34/40 [00:09<00:01,  3.64it/s]evaluate for the 19-th batch, evaluate loss: 0.5166376233100891:  39%|███████           | 18/46 [00:01<00:02,  9.66it/s]evaluate for the 19-th batch, evaluate loss: 0.5166376233100891:  41%|███████▍          | 19/46 [00:01<00:02,  9.68it/s]evaluate for the 20-th batch, evaluate loss: 0.5330560803413391:  41%|███████▍          | 19/46 [00:02<00:02,  9.68it/s]evaluate for the 20-th batch, evaluate loss: 0.5330560803413391:  43%|███████▊          | 20/46 [00:02<00:02,  9.69it/s]Epoch: 5, train for the 193-th batch, train loss: 0.6220546960830688:  81%|████████▉  | 192/237 [01:55<00:27,  1.66it/s]Epoch: 5, train for the 193-th batch, train loss: 0.6220546960830688:  81%|████████▉  | 193/237 [01:55<00:26,  1.65it/s]evaluate for the 35-th batch, evaluate loss: 0.153008833527565:  85%|████████████████▏  | 34/40 [00:09<00:01,  3.64it/s]evaluate for the 35-th batch, evaluate loss: 0.153008833527565:  88%|████████████████▋  | 35/40 [00:09<00:01,  3.59it/s]evaluate for the 21-th batch, evaluate loss: 0.5242828726768494:  43%|███████▊          | 20/46 [00:02<00:02,  9.69it/s]evaluate for the 21-th batch, evaluate loss: 0.5242828726768494:  46%|████████▏         | 21/46 [00:02<00:02,  9.70it/s]Epoch: 3, train for the 379-th batch, train loss: 0.4192346930503845:  99%|██████████▊| 378/383 [03:48<00:03,  1.66it/s]Epoch: 3, train for the 379-th batch, train loss: 0.4192346930503845:  99%|██████████▉| 379/383 [03:48<00:02,  1.66it/s]evaluate for the 22-th batch, evaluate loss: 0.5191048383712769:  46%|████████▏         | 21/46 [00:02<00:02,  9.70it/s]evaluate for the 22-th batch, evaluate loss: 0.5191048383712769:  48%|████████▌         | 22/46 [00:02<00:02,  9.71it/s]Epoch: 8, train for the 106-th batch, train loss: 0.5270761251449585:  72%|███████▉   | 105/146 [01:03<00:24,  1.66it/s]Epoch: 8, train for the 106-th batch, train loss: 0.5270761251449585:  73%|███████▉   | 106/146 [01:03<00:24,  1.66it/s]evaluate for the 23-th batch, evaluate loss: 0.4712977409362793:  48%|████████▌         | 22/46 [00:02<00:02,  9.71it/s]evaluate for the 23-th batch, evaluate loss: 0.4712977409362793:  50%|█████████         | 23/46 [00:02<00:02,  9.68it/s]evaluate for the 36-th batch, evaluate loss: 0.13883447647094727:  88%|██████████████▉  | 35/40 [00:10<00:01,  3.59it/s]evaluate for the 36-th batch, evaluate loss: 0.13883447647094727:  90%|███████████████▎ | 36/40 [00:10<00:01,  3.61it/s]evaluate for the 24-th batch, evaluate loss: 0.4776422083377838:  50%|█████████         | 23/46 [00:02<00:02,  9.68it/s]evaluate for the 24-th batch, evaluate loss: 0.4776422083377838:  52%|█████████▍        | 24/46 [00:02<00:02,  9.66it/s]evaluate for the 25-th batch, evaluate loss: 0.5328636169433594:  52%|█████████▍        | 24/46 [00:02<00:02,  9.66it/s]evaluate for the 25-th batch, evaluate loss: 0.5328636169433594:  54%|█████████▊        | 25/46 [00:02<00:02,  9.66it/s]evaluate for the 26-th batch, evaluate loss: 0.5522010922431946:  54%|█████████▊        | 25/46 [00:02<00:02,  9.66it/s]evaluate for the 26-th batch, evaluate loss: 0.5522010922431946:  57%|██████████▏       | 26/46 [00:02<00:02,  9.66it/s]evaluate for the 37-th batch, evaluate loss: 0.20748649537563324:  90%|███████████████▎ | 36/40 [00:10<00:01,  3.61it/s]evaluate for the 37-th batch, evaluate loss: 0.20748649537563324:  92%|███████████████▋ | 37/40 [00:10<00:00,  3.61it/s]Epoch: 5, train for the 194-th batch, train loss: 0.6354401111602783:  81%|████████▉  | 193/237 [01:55<00:26,  1.65it/s]Epoch: 5, train for the 194-th batch, train loss: 0.6354401111602783:  82%|█████████  | 194/237 [01:55<00:26,  1.65it/s]evaluate for the 27-th batch, evaluate loss: 0.49342527985572815:  57%|█████████▌       | 26/46 [00:02<00:02,  9.66it/s]evaluate for the 27-th batch, evaluate loss: 0.49342527985572815:  59%|█████████▉       | 27/46 [00:02<00:01,  9.69it/s]Epoch: 3, train for the 380-th batch, train loss: 0.45429128408432007:  99%|█████████▉| 379/383 [03:49<00:02,  1.66it/s]Epoch: 3, train for the 380-th batch, train loss: 0.45429128408432007:  99%|█████████▉| 380/383 [03:49<00:01,  1.65it/s]Epoch: 8, train for the 107-th batch, train loss: 0.5313739776611328:  73%|███████▉   | 106/146 [01:04<00:24,  1.66it/s]Epoch: 8, train for the 107-th batch, train loss: 0.5313739776611328:  73%|████████   | 107/146 [01:04<00:23,  1.65it/s]evaluate for the 28-th batch, evaluate loss: 0.5193866491317749:  59%|██████████▌       | 27/46 [00:02<00:01,  9.69it/s]evaluate for the 28-th batch, evaluate loss: 0.5193866491317749:  61%|██████████▉       | 28/46 [00:02<00:01,  9.70it/s]evaluate for the 29-th batch, evaluate loss: 0.4910057783126831:  61%|██████████▉       | 28/46 [00:02<00:01,  9.70it/s]evaluate for the 29-th batch, evaluate loss: 0.4910057783126831:  63%|███████████▎      | 29/46 [00:02<00:01,  9.70it/s]evaluate for the 38-th batch, evaluate loss: 0.13200794160366058:  92%|███████████████▋ | 37/40 [00:10<00:00,  3.61it/s]evaluate for the 38-th batch, evaluate loss: 0.13200794160366058:  95%|████████████████▏| 38/40 [00:10<00:00,  3.52it/s]evaluate for the 30-th batch, evaluate loss: 0.4927900731563568:  63%|███████████▎      | 29/46 [00:03<00:01,  9.70it/s]evaluate for the 30-th batch, evaluate loss: 0.4927900731563568:  65%|███████████▋      | 30/46 [00:03<00:01,  9.69it/s]evaluate for the 31-th batch, evaluate loss: 0.521210789680481:  65%|████████████▍      | 30/46 [00:03<00:01,  9.69it/s]evaluate for the 31-th batch, evaluate loss: 0.521210789680481:  67%|████████████▊      | 31/46 [00:03<00:01,  9.67it/s]evaluate for the 39-th batch, evaluate loss: 0.15389741957187653:  95%|████████████████▏| 38/40 [00:10<00:00,  3.52it/s]evaluate for the 39-th batch, evaluate loss: 0.15389741957187653:  98%|████████████████▌| 39/40 [00:10<00:00,  3.66it/s]evaluate for the 40-th batch, evaluate loss: 0.05931640416383743:  98%|████████████████▌| 39/40 [00:10<00:00,  3.66it/s]evaluate for the 40-th batch, evaluate loss: 0.05931640416383743: 100%|█████████████████| 40/40 [00:10<00:00,  3.65it/s]
  0%|                                                                                            | 0/21 [00:00<?, ?it/s]evaluate for the 32-th batch, evaluate loss: 0.4767400920391083:  67%|████████████▏     | 31/46 [00:03<00:01,  9.67it/s]evaluate for the 32-th batch, evaluate loss: 0.4767400920391083:  70%|████████████▌     | 32/46 [00:03<00:01,  9.65it/s]Epoch: 5, train for the 195-th batch, train loss: 0.6466020345687866:  82%|█████████  | 194/237 [01:56<00:26,  1.65it/s]Epoch: 5, train for the 195-th batch, train loss: 0.6466020345687866:  82%|█████████  | 195/237 [01:56<00:25,  1.64it/s]evaluate for the 33-th batch, evaluate loss: 0.4952361285686493:  70%|████████████▌     | 32/46 [00:03<00:01,  9.65it/s]evaluate for the 33-th batch, evaluate loss: 0.4952361285686493:  72%|████████████▉     | 33/46 [00:03<00:01,  9.65it/s]Epoch: 8, train for the 108-th batch, train loss: 0.5470513701438904:  73%|████████   | 107/146 [01:04<00:23,  1.65it/s]Epoch: 8, train for the 108-th batch, train loss: 0.5470513701438904:  74%|████████▏  | 108/146 [01:04<00:22,  1.67it/s]Epoch: 3, train for the 381-th batch, train loss: 0.5256329774856567:  99%|██████████▉| 380/383 [03:50<00:01,  1.65it/s]Epoch: 3, train for the 381-th batch, train loss: 0.5256329774856567:  99%|██████████▉| 381/383 [03:50<00:01,  1.64it/s]evaluate for the 34-th batch, evaluate loss: 0.47562703490257263:  72%|████████████▏    | 33/46 [00:03<00:01,  9.65it/s]evaluate for the 34-th batch, evaluate loss: 0.47562703490257263:  74%|████████████▌    | 34/46 [00:03<00:01,  9.65it/s]evaluate for the 1-th batch, evaluate loss: 0.20970098674297333:   0%|                           | 0/21 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.20970098674297333:   5%|▉                  | 1/21 [00:00<00:06,  3.24it/s]evaluate for the 35-th batch, evaluate loss: 0.47970885038375854:  74%|████████████▌    | 34/46 [00:03<00:01,  9.65it/s]evaluate for the 35-th batch, evaluate loss: 0.47970885038375854:  76%|████████████▉    | 35/46 [00:03<00:01,  9.65it/s]evaluate for the 36-th batch, evaluate loss: 0.4662887454032898:  76%|█████████████▋    | 35/46 [00:03<00:01,  9.65it/s]evaluate for the 36-th batch, evaluate loss: 0.4662887454032898:  78%|██████████████    | 36/46 [00:03<00:01,  9.63it/s]evaluate for the 37-th batch, evaluate loss: 0.5025748610496521:  78%|██████████████    | 36/46 [00:03<00:01,  9.63it/s]evaluate for the 37-th batch, evaluate loss: 0.5025748610496521:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.64it/s]evaluate for the 2-th batch, evaluate loss: 0.2524329423904419:   5%|▉                   | 1/21 [00:00<00:06,  3.24it/s]evaluate for the 2-th batch, evaluate loss: 0.2524329423904419:  10%|█▉                  | 2/21 [00:00<00:05,  3.59it/s]evaluate for the 38-th batch, evaluate loss: 0.5298265218734741:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.64it/s]evaluate for the 38-th batch, evaluate loss: 0.5298265218734741:  83%|██████████████▊   | 38/46 [00:03<00:00,  9.65it/s]Epoch: 5, train for the 196-th batch, train loss: 0.641530454158783:  82%|█████████▊  | 195/237 [01:57<00:25,  1.64it/s]Epoch: 5, train for the 196-th batch, train loss: 0.641530454158783:  83%|█████████▉  | 196/237 [01:57<00:24,  1.64it/s]evaluate for the 39-th batch, evaluate loss: 0.532360851764679:  83%|███████████████▋   | 38/46 [00:04<00:00,  9.65it/s]evaluate for the 39-th batch, evaluate loss: 0.532360851764679:  85%|████████████████   | 39/46 [00:04<00:00,  9.68it/s]Epoch: 8, train for the 109-th batch, train loss: 0.5287905931472778:  74%|████████▏  | 108/146 [01:05<00:22,  1.67it/s]Epoch: 8, train for the 109-th batch, train loss: 0.5287905931472778:  75%|████████▏  | 109/146 [01:05<00:21,  1.69it/s]Epoch: 3, train for the 382-th batch, train loss: 0.48214367032051086:  99%|█████████▉| 381/383 [03:50<00:01,  1.64it/s]Epoch: 3, train for the 382-th batch, train loss: 0.48214367032051086: 100%|█████████▉| 382/383 [03:50<00:00,  1.64it/s]evaluate for the 40-th batch, evaluate loss: 0.46653178334236145:  85%|██████████████▍  | 39/46 [00:04<00:00,  9.68it/s]evaluate for the 40-th batch, evaluate loss: 0.46653178334236145:  87%|██████████████▊  | 40/46 [00:04<00:00,  9.68it/s]evaluate for the 3-th batch, evaluate loss: 0.25435900688171387:  10%|█▊                 | 2/21 [00:00<00:05,  3.59it/s]evaluate for the 3-th batch, evaluate loss: 0.25435900688171387:  14%|██▋                | 3/21 [00:00<00:05,  3.42it/s]evaluate for the 41-th batch, evaluate loss: 0.4789365231990814:  87%|███████████████▋  | 40/46 [00:04<00:00,  9.68it/s]evaluate for the 41-th batch, evaluate loss: 0.4789365231990814:  89%|████████████████  | 41/46 [00:04<00:00,  9.65it/s]evaluate for the 4-th batch, evaluate loss: 0.21328602731227875:  14%|██▋                | 3/21 [00:01<00:05,  3.42it/s]evaluate for the 4-th batch, evaluate loss: 0.21328602731227875:  19%|███▌               | 4/21 [00:01<00:04,  4.13it/s]evaluate for the 42-th batch, evaluate loss: 0.4714689552783966:  89%|████████████████  | 41/46 [00:04<00:00,  9.65it/s]evaluate for the 42-th batch, evaluate loss: 0.4714689552783966:  91%|████████████████▍ | 42/46 [00:04<00:00,  9.67it/s]evaluate for the 43-th batch, evaluate loss: 0.5328535437583923:  91%|████████████████▍ | 42/46 [00:04<00:00,  9.67it/s]evaluate for the 43-th batch, evaluate loss: 0.5328535437583923:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.68it/s]Epoch: 5, train for the 197-th batch, train loss: 0.6309252977371216:  83%|█████████  | 196/237 [01:57<00:24,  1.64it/s]Epoch: 5, train for the 197-th batch, train loss: 0.6309252977371216:  83%|█████████▏ | 197/237 [01:57<00:24,  1.64it/s]evaluate for the 44-th batch, evaluate loss: 0.5135406851768494:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.68it/s]evaluate for the 44-th batch, evaluate loss: 0.5135406851768494:  96%|█████████████████▏| 44/46 [00:04<00:00,  9.66it/s]evaluate for the 5-th batch, evaluate loss: 0.22534672915935516:  19%|███▌               | 4/21 [00:01<00:04,  4.13it/s]evaluate for the 5-th batch, evaluate loss: 0.22534672915935516:  24%|████▌              | 5/21 [00:01<00:04,  3.87it/s]evaluate for the 45-th batch, evaluate loss: 0.49668052792549133:  96%|████████████████▎| 44/46 [00:04<00:00,  9.66it/s]evaluate for the 45-th batch, evaluate loss: 0.49668052792549133:  98%|████████████████▋| 45/46 [00:04<00:00,  9.62it/s]Epoch: 3, train for the 383-th batch, train loss: 0.415530800819397: 100%|███████████▉| 382/383 [03:51<00:00,  1.64it/s]Epoch: 3, train for the 383-th batch, train loss: 0.415530800819397: 100%|████████████| 383/383 [03:51<00:00,  1.64it/s]Epoch: 3, train for the 383-th batch, train loss: 0.415530800819397: 100%|████████████| 383/383 [03:51<00:00,  1.66it/s]
  0%|                                                                                           | 0/106 [00:00<?, ?it/s]Epoch: 8, train for the 110-th batch, train loss: 0.5045120120048523:  75%|████████▏  | 109/146 [01:06<00:21,  1.69it/s]Epoch: 8, train for the 110-th batch, train loss: 0.5045120120048523:  75%|████████▎  | 110/146 [01:06<00:22,  1.63it/s]evaluate for the 46-th batch, evaluate loss: 0.5066920518875122:  98%|█████████████████▌| 45/46 [00:04<00:00,  9.62it/s]evaluate for the 46-th batch, evaluate loss: 0.5066920518875122: 100%|██████████████████| 46/46 [00:04<00:00,  9.69it/s]
  0%|                                                                                            | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.630905032157898:   0%|                             | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.630905032157898:   4%|▊                    | 1/25 [00:00<00:02,  9.22it/s]evaluate for the 6-th batch, evaluate loss: 0.24772456288337708:  24%|████▌              | 5/21 [00:01<00:04,  3.87it/s]evaluate for the 6-th batch, evaluate loss: 0.24772456288337708:  29%|█████▍             | 6/21 [00:01<00:03,  3.81it/s]evaluate for the 2-th batch, evaluate loss: 0.6458120346069336:   4%|▊                   | 1/25 [00:00<00:02,  9.22it/s]evaluate for the 2-th batch, evaluate loss: 0.6458120346069336:   8%|█▌                  | 2/25 [00:00<00:02,  9.20it/s]evaluate for the 1-th batch, evaluate loss: 0.4437324106693268:   0%|                           | 0/106 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.4437324106693268:   1%|▏                  | 1/106 [00:00<00:27,  3.82it/s]evaluate for the 3-th batch, evaluate loss: 0.6831660866737366:   8%|█▌                  | 2/25 [00:00<00:02,  9.20it/s]evaluate for the 3-th batch, evaluate loss: 0.6831660866737366:  12%|██▍                 | 3/25 [00:00<00:02,  9.22it/s]Epoch: 5, train for the 198-th batch, train loss: 0.6142818927764893:  83%|█████████▏ | 197/237 [01:58<00:24,  1.64it/s]Epoch: 5, train for the 198-th batch, train loss: 0.6142818927764893:  84%|█████████▏ | 198/237 [01:58<00:23,  1.64it/s]evaluate for the 7-th batch, evaluate loss: 0.24211125075817108:  29%|█████▍             | 6/21 [00:01<00:03,  3.81it/s]evaluate for the 7-th batch, evaluate loss: 0.24211125075817108:  33%|██████▎            | 7/21 [00:01<00:03,  3.68it/s]evaluate for the 4-th batch, evaluate loss: 0.6711369156837463:  12%|██▍                 | 3/25 [00:00<00:02,  9.22it/s]evaluate for the 4-th batch, evaluate loss: 0.6711369156837463:  16%|███▏                | 4/25 [00:00<00:02,  9.21it/s]evaluate for the 2-th batch, evaluate loss: 0.5575159192085266:   1%|▏                  | 1/106 [00:00<00:27,  3.82it/s]evaluate for the 2-th batch, evaluate loss: 0.5575159192085266:   2%|▎                  | 2/106 [00:00<00:29,  3.54it/s]evaluate for the 5-th batch, evaluate loss: 0.6681908965110779:  16%|███▏                | 4/25 [00:00<00:02,  9.21it/s]evaluate for the 5-th batch, evaluate loss: 0.6681908965110779:  20%|████                | 5/25 [00:00<00:02,  9.25it/s]Epoch: 8, train for the 111-th batch, train loss: 0.5216125845909119:  75%|████████▎  | 110/146 [01:06<00:22,  1.63it/s]Epoch: 8, train for the 111-th batch, train loss: 0.5216125845909119:  76%|████████▎  | 111/146 [01:06<00:21,  1.64it/s]evaluate for the 6-th batch, evaluate loss: 0.7113947868347168:  20%|████                | 5/25 [00:00<00:02,  9.25it/s]evaluate for the 6-th batch, evaluate loss: 0.7113947868347168:  24%|████▊               | 6/25 [00:00<00:02,  9.23it/s]evaluate for the 8-th batch, evaluate loss: 0.29386964440345764:  33%|██████▎            | 7/21 [00:02<00:03,  3.68it/s]evaluate for the 8-th batch, evaluate loss: 0.29386964440345764:  38%|███████▏           | 8/21 [00:02<00:03,  3.59it/s]evaluate for the 3-th batch, evaluate loss: 0.49447718262672424:   2%|▎                 | 2/106 [00:00<00:29,  3.54it/s]evaluate for the 3-th batch, evaluate loss: 0.49447718262672424:   3%|▌                 | 3/106 [00:00<00:27,  3.78it/s]evaluate for the 7-th batch, evaluate loss: 0.7334732413291931:  24%|████▊               | 6/25 [00:00<00:02,  9.23it/s]evaluate for the 7-th batch, evaluate loss: 0.7334732413291931:  28%|█████▌              | 7/25 [00:00<00:01,  9.22it/s]evaluate for the 8-th batch, evaluate loss: 0.713299572467804:  28%|█████▉               | 7/25 [00:00<00:01,  9.22it/s]evaluate for the 8-th batch, evaluate loss: 0.713299572467804:  32%|██████▋              | 8/25 [00:00<00:01,  9.24it/s]evaluate for the 9-th batch, evaluate loss: 0.6934909820556641:  32%|██████▍             | 8/25 [00:00<00:01,  9.24it/s]evaluate for the 9-th batch, evaluate loss: 0.6934909820556641:  36%|███████▏            | 9/25 [00:00<00:01,  9.25it/s]Epoch: 5, train for the 199-th batch, train loss: 0.6176608800888062:  84%|█████████▏ | 198/237 [01:58<00:23,  1.64it/s]Epoch: 5, train for the 199-th batch, train loss: 0.6176608800888062:  84%|█████████▏ | 199/237 [01:58<00:23,  1.65it/s]evaluate for the 9-th batch, evaluate loss: 0.2235763818025589:  38%|███████▌            | 8/21 [00:02<00:03,  3.59it/s]evaluate for the 9-th batch, evaluate loss: 0.2235763818025589:  43%|████████▌           | 9/21 [00:02<00:03,  3.51it/s]evaluate for the 4-th batch, evaluate loss: 0.44607874751091003:   3%|▌                 | 3/106 [00:01<00:27,  3.78it/s]evaluate for the 4-th batch, evaluate loss: 0.44607874751091003:   4%|▋                 | 4/106 [00:01<00:28,  3.59it/s]evaluate for the 10-th batch, evaluate loss: 0.7234797477722168:  36%|██████▊            | 9/25 [00:01<00:01,  9.25it/s]evaluate for the 10-th batch, evaluate loss: 0.7234797477722168:  40%|███████▏          | 10/25 [00:01<00:01,  9.25it/s]Epoch: 8, train for the 112-th batch, train loss: 0.5589953660964966:  76%|████████▎  | 111/146 [01:07<00:21,  1.64it/s]Epoch: 8, train for the 112-th batch, train loss: 0.5589953660964966:  77%|████████▍  | 112/146 [01:07<00:20,  1.65it/s]evaluate for the 11-th batch, evaluate loss: 0.7299293279647827:  40%|███████▏          | 10/25 [00:01<00:01,  9.25it/s]evaluate for the 11-th batch, evaluate loss: 0.7299293279647827:  44%|███████▉          | 11/25 [00:01<00:01,  9.24it/s]evaluate for the 12-th batch, evaluate loss: 0.6894381642341614:  44%|███████▉          | 11/25 [00:01<00:01,  9.24it/s]evaluate for the 12-th batch, evaluate loss: 0.6894381642341614:  48%|████████▋         | 12/25 [00:01<00:01,  9.23it/s]evaluate for the 5-th batch, evaluate loss: 0.5336229205131531:   4%|▋                  | 4/106 [00:01<00:28,  3.59it/s]evaluate for the 5-th batch, evaluate loss: 0.5336229205131531:   5%|▉                  | 5/106 [00:01<00:26,  3.75it/s]evaluate for the 10-th batch, evaluate loss: 0.21778610348701477:  43%|███████▋          | 9/21 [00:02<00:03,  3.51it/s]evaluate for the 10-th batch, evaluate loss: 0.21778610348701477:  48%|████████         | 10/21 [00:02<00:03,  3.47it/s]evaluate for the 13-th batch, evaluate loss: 0.6571592688560486:  48%|████████▋         | 12/25 [00:01<00:01,  9.23it/s]evaluate for the 13-th batch, evaluate loss: 0.6571592688560486:  52%|█████████▎        | 13/25 [00:01<00:01,  9.25it/s]evaluate for the 14-th batch, evaluate loss: 0.7399322390556335:  52%|█████████▎        | 13/25 [00:01<00:01,  9.25it/s]evaluate for the 14-th batch, evaluate loss: 0.7399322390556335:  56%|██████████        | 14/25 [00:01<00:01,  9.27it/s]evaluate for the 11-th batch, evaluate loss: 0.15970773994922638:  48%|████████         | 10/21 [00:03<00:03,  3.47it/s]evaluate for the 11-th batch, evaluate loss: 0.15970773994922638:  52%|████████▉        | 11/21 [00:03<00:02,  3.48it/s]Epoch: 5, train for the 200-th batch, train loss: 0.6186187863349915:  84%|█████████▏ | 199/237 [01:59<00:23,  1.65it/s]Epoch: 5, train for the 200-th batch, train loss: 0.6186187863349915:  84%|█████████▎ | 200/237 [01:59<00:22,  1.65it/s]evaluate for the 6-th batch, evaluate loss: 0.5141881704330444:   5%|▉                  | 5/106 [00:01<00:26,  3.75it/s]evaluate for the 6-th batch, evaluate loss: 0.5141881704330444:   6%|█                  | 6/106 [00:01<00:28,  3.53it/s]evaluate for the 15-th batch, evaluate loss: 0.7195916771888733:  56%|██████████        | 14/25 [00:01<00:01,  9.27it/s]evaluate for the 15-th batch, evaluate loss: 0.7195916771888733:  60%|██████████▊       | 15/25 [00:01<00:01,  9.26it/s]evaluate for the 16-th batch, evaluate loss: 0.6612628102302551:  60%|██████████▊       | 15/25 [00:01<00:01,  9.26it/s]evaluate for the 16-th batch, evaluate loss: 0.6612628102302551:  64%|███████████▌      | 16/25 [00:01<00:00,  9.27it/s]Epoch: 8, train for the 113-th batch, train loss: 0.48718032240867615:  77%|███████▋  | 112/146 [01:07<00:20,  1.65it/s]Epoch: 8, train for the 113-th batch, train loss: 0.48718032240867615:  77%|███████▋  | 113/146 [01:07<00:19,  1.66it/s]evaluate for the 17-th batch, evaluate loss: 0.6601284146308899:  64%|███████████▌      | 16/25 [00:01<00:00,  9.27it/s]evaluate for the 17-th batch, evaluate loss: 0.6601284146308899:  68%|████████████▏     | 17/25 [00:01<00:00,  9.27it/s]evaluate for the 7-th batch, evaluate loss: 0.4547509551048279:   6%|█                  | 6/106 [00:01<00:28,  3.53it/s]evaluate for the 7-th batch, evaluate loss: 0.4547509551048279:   7%|█▎                 | 7/106 [00:01<00:27,  3.60it/s]evaluate for the 12-th batch, evaluate loss: 0.2817155420780182:  52%|█████████▍        | 11/21 [00:03<00:02,  3.48it/s]evaluate for the 12-th batch, evaluate loss: 0.2817155420780182:  57%|██████████▎       | 12/21 [00:03<00:02,  3.41it/s]evaluate for the 18-th batch, evaluate loss: 0.628214955329895:  68%|████████████▉      | 17/25 [00:01<00:00,  9.27it/s]evaluate for the 18-th batch, evaluate loss: 0.628214955329895:  72%|█████████████▋     | 18/25 [00:01<00:00,  9.28it/s]evaluate for the 19-th batch, evaluate loss: 0.5888854265213013:  72%|████████████▉     | 18/25 [00:02<00:00,  9.28it/s]evaluate for the 19-th batch, evaluate loss: 0.5888854265213013:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.21it/s]evaluate for the 20-th batch, evaluate loss: 0.6480664014816284:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.21it/s]evaluate for the 20-th batch, evaluate loss: 0.6480664014816284:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.17it/s]evaluate for the 8-th batch, evaluate loss: 0.3655892014503479:   7%|█▎                 | 7/106 [00:02<00:27,  3.60it/s]evaluate for the 8-th batch, evaluate loss: 0.3655892014503479:   8%|█▍                 | 8/106 [00:02<00:27,  3.53it/s]evaluate for the 13-th batch, evaluate loss: 0.22871223092079163:  57%|█████████▋       | 12/21 [00:03<00:02,  3.41it/s]evaluate for the 13-th batch, evaluate loss: 0.22871223092079163:  62%|██████████▌      | 13/21 [00:03<00:02,  3.47it/s]Epoch: 5, train for the 201-th batch, train loss: 0.6170066595077515:  84%|█████████▎ | 200/237 [02:00<00:22,  1.65it/s]Epoch: 5, train for the 201-th batch, train loss: 0.6170066595077515:  85%|█████████▎ | 201/237 [02:00<00:21,  1.67it/s]evaluate for the 21-th batch, evaluate loss: 0.7188755869865417:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.17it/s]evaluate for the 21-th batch, evaluate loss: 0.7188755869865417:  84%|███████████████   | 21/25 [00:02<00:00,  9.16it/s]Epoch: 8, train for the 114-th batch, train loss: 0.5400679707527161:  77%|████████▌  | 113/146 [01:08<00:19,  1.66it/s]Epoch: 8, train for the 114-th batch, train loss: 0.5400679707527161:  78%|████████▌  | 114/146 [01:08<00:19,  1.67it/s]evaluate for the 22-th batch, evaluate loss: 0.6016033291816711:  84%|███████████████   | 21/25 [00:02<00:00,  9.16it/s]evaluate for the 22-th batch, evaluate loss: 0.6016033291816711:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.18it/s]evaluate for the 9-th batch, evaluate loss: 0.5668758153915405:   8%|█▍                 | 8/106 [00:02<00:27,  3.53it/s]evaluate for the 9-th batch, evaluate loss: 0.5668758153915405:   8%|█▌                 | 9/106 [00:02<00:26,  3.66it/s]evaluate for the 14-th batch, evaluate loss: 0.19480933248996735:  62%|██████████▌      | 13/21 [00:03<00:02,  3.47it/s]evaluate for the 14-th batch, evaluate loss: 0.19480933248996735:  67%|███████████▎     | 14/21 [00:03<00:02,  3.41it/s]evaluate for the 23-th batch, evaluate loss: 0.658726155757904:  88%|████████████████▋  | 22/25 [00:02<00:00,  9.18it/s]evaluate for the 23-th batch, evaluate loss: 0.658726155757904:  92%|█████████████████▍ | 23/25 [00:02<00:00,  9.19it/s]evaluate for the 24-th batch, evaluate loss: 0.6551096439361572:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.19it/s]evaluate for the 24-th batch, evaluate loss: 0.6551096439361572:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.19it/s]evaluate for the 25-th batch, evaluate loss: 0.7014479041099548:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.19it/s]evaluate for the 25-th batch, evaluate loss: 0.7014479041099548: 100%|██████████████████| 25/25 [00:02<00:00,  9.29it/s]
INFO:root:Epoch: 16, learning rate: 0.0001, train loss: 0.5604
INFO:root:train average_precision, 0.8234
INFO:root:train roc_auc, 0.7905
INFO:root:validate loss: 0.5065
INFO:root:validate average_precision, 0.8440
INFO:root:validate roc_auc, 0.8056
INFO:root:new node validate loss: 0.6773
INFO:root:new node validate first_1_average_precision, 0.5946
INFO:root:new node validate first_1_roc_auc, 0.5419
INFO:root:new node validate first_3_average_precision, 0.6786
INFO:root:new node validate first_3_roc_auc, 0.6377
INFO:root:new node validate first_10_average_precision, 0.7469
INFO:root:new node validate first_10_roc_auc, 0.7114
INFO:root:new node validate average_precision, 0.7093
INFO:root:new node validate roc_auc, 0.6582
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]evaluate for the 10-th batch, evaluate loss: 0.40268370509147644:   8%|█▍               | 9/106 [00:02<00:26,  3.66it/s]evaluate for the 10-th batch, evaluate loss: 0.40268370509147644:   9%|█▌              | 10/106 [00:02<00:26,  3.59it/s]evaluate for the 15-th batch, evaluate loss: 0.19334135949611664:  67%|███████████▎     | 14/21 [00:04<00:02,  3.41it/s]evaluate for the 15-th batch, evaluate loss: 0.19334135949611664:  71%|████████████▏    | 15/21 [00:04<00:01,  3.50it/s]Epoch: 5, train for the 202-th batch, train loss: 0.5853222012519836:  85%|█████████▎ | 201/237 [02:00<00:21,  1.67it/s]Epoch: 5, train for the 202-th batch, train loss: 0.5853222012519836:  85%|█████████▍ | 202/237 [02:00<00:20,  1.68it/s]Epoch: 17, train for the 1-th batch, train loss: 0.9624844193458557:   0%|                      | 0/151 [00:00<?, ?it/s]Epoch: 17, train for the 1-th batch, train loss: 0.9624844193458557:   1%|              | 1/151 [00:00<00:25,  5.92it/s]Epoch: 8, train for the 115-th batch, train loss: 0.5124824643135071:  78%|████████▌  | 114/146 [01:09<00:19,  1.67it/s]Epoch: 8, train for the 115-th batch, train loss: 0.5124824643135071:  79%|████████▋  | 115/146 [01:09<00:18,  1.68it/s]evaluate for the 11-th batch, evaluate loss: 0.39608851075172424:   9%|█▌              | 10/106 [00:03<00:26,  3.59it/s]evaluate for the 11-th batch, evaluate loss: 0.39608851075172424:  10%|█▋              | 11/106 [00:03<00:25,  3.69it/s]Epoch: 17, train for the 2-th batch, train loss: 0.9951089024543762:   1%|              | 1/151 [00:00<00:25,  5.92it/s]Epoch: 17, train for the 2-th batch, train loss: 0.9951089024543762:   1%|▏             | 2/151 [00:00<00:25,  5.75it/s]evaluate for the 16-th batch, evaluate loss: 0.24153301119804382:  71%|████████████▏    | 15/21 [00:04<00:01,  3.50it/s]evaluate for the 16-th batch, evaluate loss: 0.24153301119804382:  76%|████████████▉    | 16/21 [00:04<00:01,  3.42it/s]Epoch: 17, train for the 3-th batch, train loss: 0.5680930614471436:   1%|▏             | 2/151 [00:00<00:25,  5.75it/s]Epoch: 17, train for the 3-th batch, train loss: 0.5680930614471436:   2%|▎             | 3/151 [00:00<00:24,  5.94it/s]evaluate for the 12-th batch, evaluate loss: 0.3861716091632843:  10%|█▊               | 11/106 [00:03<00:25,  3.69it/s]evaluate for the 12-th batch, evaluate loss: 0.3861716091632843:  11%|█▉               | 12/106 [00:03<00:25,  3.63it/s]evaluate for the 17-th batch, evaluate loss: 0.22221867740154266:  76%|████████████▉    | 16/21 [00:04<00:01,  3.42it/s]evaluate for the 17-th batch, evaluate loss: 0.22221867740154266:  81%|█████████████▊   | 17/21 [00:04<00:01,  3.56it/s]Epoch: 17, train for the 4-th batch, train loss: 0.618931770324707:   2%|▎              | 3/151 [00:00<00:24,  5.94it/s]Epoch: 17, train for the 4-th batch, train loss: 0.618931770324707:   3%|▍              | 4/151 [00:00<00:24,  5.94it/s]Epoch: 5, train for the 203-th batch, train loss: 0.6075025200843811:  85%|█████████▍ | 202/237 [02:01<00:20,  1.68it/s]Epoch: 5, train for the 203-th batch, train loss: 0.6075025200843811:  86%|█████████▍ | 203/237 [02:01<00:20,  1.67it/s]Epoch: 8, train for the 116-th batch, train loss: 0.5172265768051147:  79%|████████▋  | 115/146 [01:09<00:18,  1.68it/s]Epoch: 8, train for the 116-th batch, train loss: 0.5172265768051147:  79%|████████▋  | 116/146 [01:09<00:17,  1.69it/s]evaluate for the 13-th batch, evaluate loss: 0.4411144256591797:  11%|█▉               | 12/106 [00:03<00:25,  3.63it/s]evaluate for the 13-th batch, evaluate loss: 0.4411144256591797:  12%|██               | 13/106 [00:03<00:25,  3.64it/s]Epoch: 17, train for the 5-th batch, train loss: 0.6998093128204346:   3%|▎             | 4/151 [00:00<00:24,  5.94it/s]Epoch: 17, train for the 5-th batch, train loss: 0.6998093128204346:   3%|▍             | 5/151 [00:00<00:25,  5.63it/s]evaluate for the 18-th batch, evaluate loss: 0.21409893035888672:  81%|█████████████▊   | 17/21 [00:05<00:01,  3.56it/s]evaluate for the 18-th batch, evaluate loss: 0.21409893035888672:  86%|██████████████▌  | 18/21 [00:05<00:00,  3.46it/s]Epoch: 17, train for the 6-th batch, train loss: 0.597980260848999:   3%|▍              | 5/151 [00:01<00:25,  5.63it/s]Epoch: 17, train for the 6-th batch, train loss: 0.597980260848999:   4%|▌              | 6/151 [00:01<00:26,  5.51it/s]evaluate for the 14-th batch, evaluate loss: 0.40849798917770386:  12%|█▉              | 13/106 [00:03<00:25,  3.64it/s]evaluate for the 14-th batch, evaluate loss: 0.40849798917770386:  13%|██              | 14/106 [00:03<00:25,  3.66it/s]evaluate for the 19-th batch, evaluate loss: 0.2743065655231476:  86%|███████████████▍  | 18/21 [00:05<00:00,  3.46it/s]evaluate for the 19-th batch, evaluate loss: 0.2743065655231476:  90%|████████████████▎ | 19/21 [00:05<00:00,  3.59it/s]Epoch: 17, train for the 7-th batch, train loss: 0.4663214683532715:   4%|▌             | 6/151 [00:01<00:26,  5.51it/s]Epoch: 17, train for the 7-th batch, train loss: 0.4663214683532715:   5%|▋             | 7/151 [00:01<00:26,  5.43it/s]Epoch: 5, train for the 204-th batch, train loss: 0.6491045951843262:  86%|█████████▍ | 203/237 [02:01<00:20,  1.67it/s]Epoch: 5, train for the 204-th batch, train loss: 0.6491045951843262:  86%|█████████▍ | 204/237 [02:01<00:19,  1.67it/s]Epoch: 8, train for the 117-th batch, train loss: 0.5964359045028687:  79%|████████▋  | 116/146 [01:10<00:17,  1.69it/s]Epoch: 8, train for the 117-th batch, train loss: 0.5964359045028687:  80%|████████▊  | 117/146 [01:10<00:16,  1.71it/s]evaluate for the 15-th batch, evaluate loss: 0.5339142680168152:  13%|██▏              | 14/106 [00:04<00:25,  3.66it/s]evaluate for the 15-th batch, evaluate loss: 0.5339142680168152:  14%|██▍              | 15/106 [00:04<00:25,  3.57it/s]Epoch: 17, train for the 8-th batch, train loss: 0.6478816270828247:   5%|▋             | 7/151 [00:01<00:26,  5.43it/s]Epoch: 17, train for the 8-th batch, train loss: 0.6478816270828247:   5%|▋             | 8/151 [00:01<00:26,  5.40it/s]evaluate for the 20-th batch, evaluate loss: 0.24693608283996582:  90%|███████████████▍ | 19/21 [00:05<00:00,  3.59it/s]evaluate for the 20-th batch, evaluate loss: 0.24693608283996582:  95%|████████████████▏| 20/21 [00:05<00:00,  3.48it/s]evaluate for the 21-th batch, evaluate loss: 0.1505449414253235:  95%|█████████████████▏| 20/21 [00:05<00:00,  3.48it/s]evaluate for the 21-th batch, evaluate loss: 0.1505449414253235: 100%|██████████████████| 21/21 [00:05<00:00,  3.69it/s]
INFO:root:Epoch: 9, learning rate: 0.0001, train loss: 0.2028
INFO:root:train average_precision, 0.9736
INFO:root:train roc_auc, 0.9656
INFO:root:validate loss: 0.1490
INFO:root:validate average_precision, 0.9873
INFO:root:validate roc_auc, 0.9854
INFO:root:new node validate loss: 0.2280
INFO:root:new node validate first_1_average_precision, 0.9169
INFO:root:new node validate first_1_roc_auc, 0.9207
INFO:root:new node validate first_3_average_precision, 0.9545
INFO:root:new node validate first_3_roc_auc, 0.9556
INFO:root:new node validate first_10_average_precision, 0.9705
INFO:root:new node validate first_10_roc_auc, 0.9699
INFO:root:new node validate average_precision, 0.9709
INFO:root:new node validate roc_auc, 0.9685
INFO:root:save model ./saved_models/DyGFormer/ia-reality-call/DyGFormer_seed0_dygformer-ia-reality-call-old/DyGFormer_seed0_dygformer-ia-reality-call-old.pkl
Epoch: 17, train for the 9-th batch, train loss: 0.5734983682632446:   5%|▋             | 8/151 [00:01<00:26,  5.40it/s]Epoch: 17, train for the 9-th batch, train loss: 0.5734983682632446:   6%|▊             | 9/151 [00:01<00:26,  5.35it/s]evaluate for the 16-th batch, evaluate loss: 0.4697912633419037:  14%|██▍              | 15/106 [00:04<00:25,  3.57it/s]evaluate for the 16-th batch, evaluate loss: 0.4697912633419037:  15%|██▌              | 16/106 [00:04<00:24,  3.70it/s]Epoch: 8, train for the 118-th batch, train loss: 0.4779196083545685:  80%|████████▊  | 117/146 [01:10<00:16,  1.71it/s]Epoch: 8, train for the 118-th batch, train loss: 0.4779196083545685:  81%|████████▉  | 118/146 [01:10<00:13,  2.00it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]Epoch: 17, train for the 10-th batch, train loss: 0.5378742814064026:   6%|▊            | 9/151 [00:01<00:26,  5.35it/s]Epoch: 17, train for the 10-th batch, train loss: 0.5378742814064026:   7%|▊           | 10/151 [00:01<00:27,  5.22it/s]Epoch: 5, train for the 205-th batch, train loss: 0.6304816007614136:  86%|█████████▍ | 204/237 [02:02<00:19,  1.67it/s]Epoch: 5, train for the 205-th batch, train loss: 0.6304816007614136:  86%|█████████▌ | 205/237 [02:02<00:19,  1.68it/s]evaluate for the 17-th batch, evaluate loss: 0.44127440452575684:  15%|██▍             | 16/106 [00:04<00:24,  3.70it/s]evaluate for the 17-th batch, evaluate loss: 0.44127440452575684:  16%|██▌             | 17/106 [00:04<00:24,  3.59it/s]Epoch: 17, train for the 11-th batch, train loss: 0.5573661923408508:   7%|▊           | 10/151 [00:02<00:27,  5.22it/s]Epoch: 17, train for the 11-th batch, train loss: 0.5573661923408508:   7%|▊           | 11/151 [00:02<00:27,  5.15it/s]evaluate for the 18-th batch, evaluate loss: 0.3712373375892639:  16%|██▋              | 17/106 [00:04<00:24,  3.59it/s]evaluate for the 18-th batch, evaluate loss: 0.3712373375892639:  17%|██▉              | 18/106 [00:04<00:23,  3.76it/s]Epoch: 8, train for the 119-th batch, train loss: 0.5255381464958191:  81%|████████▉  | 118/146 [01:10<00:13,  2.00it/s]Epoch: 8, train for the 119-th batch, train loss: 0.5255381464958191:  82%|████████▉  | 119/146 [01:10<00:13,  1.97it/s]Epoch: 17, train for the 12-th batch, train loss: 0.6007326245307922:   7%|▊           | 11/151 [00:02<00:27,  5.15it/s]Epoch: 17, train for the 12-th batch, train loss: 0.6007326245307922:   8%|▉           | 12/151 [00:02<00:27,  5.11it/s]Epoch: 10, train for the 1-th batch, train loss: 0.9840087890625:   0%|                         | 0/119 [00:00<?, ?it/s]Epoch: 10, train for the 1-th batch, train loss: 0.9840087890625:   1%|▏                | 1/119 [00:00<01:06,  1.78it/s]Epoch: 17, train for the 13-th batch, train loss: 0.640082836151123:   8%|█            | 12/151 [00:02<00:27,  5.11it/s]Epoch: 17, train for the 13-th batch, train loss: 0.640082836151123:   9%|█            | 13/151 [00:02<00:27,  5.05it/s]Epoch: 5, train for the 206-th batch, train loss: 0.6572611927986145:  86%|█████████▌ | 205/237 [02:03<00:19,  1.68it/s]Epoch: 5, train for the 206-th batch, train loss: 0.6572611927986145:  87%|█████████▌ | 206/237 [02:03<00:18,  1.68it/s]evaluate for the 19-th batch, evaluate loss: 0.3577996790409088:  17%|██▉              | 18/106 [00:05<00:23,  3.76it/s]evaluate for the 19-th batch, evaluate loss: 0.3577996790409088:  18%|███              | 19/106 [00:05<00:24,  3.58it/s]Epoch: 17, train for the 14-th batch, train loss: 0.7021501660346985:   9%|█           | 13/151 [00:02<00:27,  5.05it/s]Epoch: 17, train for the 14-th batch, train loss: 0.7021501660346985:   9%|█           | 14/151 [00:02<00:28,  4.88it/s]evaluate for the 20-th batch, evaluate loss: 0.41611143946647644:  18%|██▊             | 19/106 [00:05<00:24,  3.58it/s]evaluate for the 20-th batch, evaluate loss: 0.41611143946647644:  19%|███             | 20/106 [00:05<00:23,  3.65it/s]Epoch: 8, train for the 120-th batch, train loss: 0.5201310515403748:  82%|████████▉  | 119/146 [01:11<00:13,  1.97it/s]Epoch: 8, train for the 120-th batch, train loss: 0.5201310515403748:  82%|█████████  | 120/146 [01:11<00:13,  1.88it/s]Epoch: 17, train for the 15-th batch, train loss: 0.5430676341056824:   9%|█           | 14/151 [00:02<00:28,  4.88it/s]Epoch: 17, train for the 15-th batch, train loss: 0.5430676341056824:  10%|█▏          | 15/151 [00:02<00:28,  4.85it/s]Epoch: 10, train for the 2-th batch, train loss: 0.48222044110298157:   1%|             | 1/119 [00:01<01:06,  1.78it/s]Epoch: 10, train for the 2-th batch, train loss: 0.48222044110298157:   2%|▏            | 2/119 [00:01<01:06,  1.75it/s]evaluate for the 21-th batch, evaluate loss: 0.3045457899570465:  19%|███▏             | 20/106 [00:05<00:23,  3.65it/s]evaluate for the 21-th batch, evaluate loss: 0.3045457899570465:  20%|███▎             | 21/106 [00:05<00:23,  3.55it/s]Epoch: 17, train for the 16-th batch, train loss: 0.5451288223266602:  10%|█▏          | 15/151 [00:03<00:28,  4.85it/s]Epoch: 17, train for the 16-th batch, train loss: 0.5451288223266602:  11%|█▎          | 16/151 [00:03<00:27,  4.85it/s]Epoch: 5, train for the 207-th batch, train loss: 0.6447776556015015:  87%|█████████▌ | 206/237 [02:03<00:18,  1.68it/s]Epoch: 5, train for the 207-th batch, train loss: 0.6447776556015015:  87%|█████████▌ | 207/237 [02:03<00:17,  1.68it/s]Epoch: 17, train for the 17-th batch, train loss: 0.6203829050064087:  11%|█▎          | 16/151 [00:03<00:27,  4.85it/s]Epoch: 17, train for the 17-th batch, train loss: 0.6203829050064087:  11%|█▎          | 17/151 [00:03<00:27,  4.87it/s]evaluate for the 22-th batch, evaluate loss: 0.3818318843841553:  20%|███▎             | 21/106 [00:06<00:23,  3.55it/s]evaluate for the 22-th batch, evaluate loss: 0.3818318843841553:  21%|███▌             | 22/106 [00:06<00:22,  3.66it/s]Epoch: 8, train for the 121-th batch, train loss: 0.519843339920044:  82%|█████████▊  | 120/146 [01:12<00:13,  1.88it/s]Epoch: 8, train for the 121-th batch, train loss: 0.519843339920044:  83%|█████████▉  | 121/146 [01:12<00:13,  1.82it/s]Epoch: 17, train for the 18-th batch, train loss: 0.690011203289032:  11%|█▍           | 17/151 [00:03<00:27,  4.87it/s]Epoch: 17, train for the 18-th batch, train loss: 0.690011203289032:  12%|█▌           | 18/151 [00:03<00:27,  4.81it/s]Epoch: 10, train for the 3-th batch, train loss: 0.33569514751434326:   2%|▏            | 2/119 [00:01<01:06,  1.75it/s]Epoch: 10, train for the 3-th batch, train loss: 0.33569514751434326:   3%|▎            | 3/119 [00:01<01:07,  1.71it/s]evaluate for the 23-th batch, evaluate loss: 0.400981068611145:  21%|███▋              | 22/106 [00:06<00:22,  3.66it/s]evaluate for the 23-th batch, evaluate loss: 0.400981068611145:  22%|███▉              | 23/106 [00:06<00:23,  3.59it/s]Epoch: 5, train for the 208-th batch, train loss: 0.646068274974823:  87%|██████████▍ | 207/237 [02:04<00:17,  1.68it/s]Epoch: 5, train for the 208-th batch, train loss: 0.646068274974823:  88%|██████████▌ | 208/237 [02:04<00:17,  1.69it/s]Epoch: 17, train for the 19-th batch, train loss: 0.6162320375442505:  12%|█▍          | 18/151 [00:03<00:27,  4.81it/s]Epoch: 17, train for the 19-th batch, train loss: 0.6162320375442505:  13%|█▌          | 19/151 [00:03<00:27,  4.79it/s]evaluate for the 24-th batch, evaluate loss: 0.3452817499637604:  22%|███▋             | 23/106 [00:06<00:23,  3.59it/s]evaluate for the 24-th batch, evaluate loss: 0.3452817499637604:  23%|███▊             | 24/106 [00:06<00:22,  3.69it/s]Epoch: 17, train for the 20-th batch, train loss: 0.3538709282875061:  13%|█▌          | 19/151 [00:03<00:27,  4.79it/s]Epoch: 17, train for the 20-th batch, train loss: 0.3538709282875061:  13%|█▌          | 20/151 [00:03<00:27,  4.81it/s]Epoch: 8, train for the 122-th batch, train loss: 0.5726400017738342:  83%|█████████  | 121/146 [01:12<00:13,  1.82it/s]Epoch: 8, train for the 122-th batch, train loss: 0.5726400017738342:  84%|█████████▏ | 122/146 [01:12<00:13,  1.79it/s]Epoch: 10, train for the 4-th batch, train loss: 0.33116596937179565:   3%|▎            | 3/119 [00:02<01:07,  1.71it/s]Epoch: 10, train for the 4-th batch, train loss: 0.33116596937179565:   3%|▍            | 4/119 [00:02<01:07,  1.69it/s]Epoch: 17, train for the 21-th batch, train loss: 0.38261935114860535:  13%|█▍         | 20/151 [00:04<00:27,  4.81it/s]Epoch: 17, train for the 21-th batch, train loss: 0.38261935114860535:  14%|█▌         | 21/151 [00:04<00:27,  4.81it/s]evaluate for the 25-th batch, evaluate loss: 0.44420555233955383:  23%|███▌            | 24/106 [00:06<00:22,  3.69it/s]evaluate for the 25-th batch, evaluate loss: 0.44420555233955383:  24%|███▊            | 25/106 [00:06<00:22,  3.64it/s]Epoch: 5, train for the 209-th batch, train loss: 0.6070734858512878:  88%|█████████▋ | 208/237 [02:04<00:17,  1.69it/s]Epoch: 5, train for the 209-th batch, train loss: 0.6070734858512878:  88%|█████████▋ | 209/237 [02:04<00:16,  1.68it/s]Epoch: 17, train for the 22-th batch, train loss: 0.6402878165245056:  14%|█▋          | 21/151 [00:04<00:27,  4.81it/s]Epoch: 17, train for the 22-th batch, train loss: 0.6402878165245056:  15%|█▋          | 22/151 [00:04<00:27,  4.72it/s]evaluate for the 26-th batch, evaluate loss: 0.41111499071121216:  24%|███▊            | 25/106 [00:07<00:22,  3.64it/s]evaluate for the 26-th batch, evaluate loss: 0.41111499071121216:  25%|███▉            | 26/106 [00:07<00:21,  3.67it/s]Epoch: 17, train for the 23-th batch, train loss: 0.657617449760437:  15%|█▉           | 22/151 [00:04<00:27,  4.72it/s]Epoch: 17, train for the 23-th batch, train loss: 0.657617449760437:  15%|█▉           | 23/151 [00:04<00:27,  4.64it/s]Epoch: 8, train for the 123-th batch, train loss: 0.5826441645622253:  84%|█████████▏ | 122/146 [01:13<00:13,  1.79it/s]Epoch: 8, train for the 123-th batch, train loss: 0.5826441645622253:  84%|█████████▎ | 123/146 [01:13<00:12,  1.77it/s]evaluate for the 27-th batch, evaluate loss: 0.29445573687553406:  25%|███▉            | 26/106 [00:07<00:21,  3.67it/s]evaluate for the 27-th batch, evaluate loss: 0.29445573687553406:  25%|████            | 27/106 [00:07<00:21,  3.67it/s]Epoch: 10, train for the 5-th batch, train loss: 0.39161744713783264:   3%|▍            | 4/119 [00:02<01:07,  1.69it/s]Epoch: 10, train for the 5-th batch, train loss: 0.39161744713783264:   4%|▌            | 5/119 [00:02<01:08,  1.68it/s]Epoch: 17, train for the 24-th batch, train loss: 0.5778871178627014:  15%|█▊          | 23/151 [00:04<00:27,  4.64it/s]Epoch: 17, train for the 24-th batch, train loss: 0.5778871178627014:  16%|█▉          | 24/151 [00:04<00:27,  4.58it/s]Epoch: 5, train for the 210-th batch, train loss: 0.6301004886627197:  88%|█████████▋ | 209/237 [02:05<00:16,  1.68it/s]Epoch: 5, train for the 210-th batch, train loss: 0.6301004886627197:  89%|█████████▋ | 210/237 [02:05<00:16,  1.68it/s]evaluate for the 28-th batch, evaluate loss: 0.3251835107803345:  25%|████▎            | 27/106 [00:07<00:21,  3.67it/s]evaluate for the 28-th batch, evaluate loss: 0.3251835107803345:  26%|████▍            | 28/106 [00:07<00:21,  3.58it/s]Epoch: 17, train for the 25-th batch, train loss: 0.5099554061889648:  16%|█▉          | 24/151 [00:04<00:27,  4.58it/s]Epoch: 17, train for the 25-th batch, train loss: 0.5099554061889648:  17%|█▉          | 25/151 [00:04<00:27,  4.62it/s]Epoch: 8, train for the 124-th batch, train loss: 0.5670561194419861:  84%|█████████▎ | 123/146 [01:13<00:12,  1.77it/s]Epoch: 8, train for the 124-th batch, train loss: 0.5670561194419861:  85%|█████████▎ | 124/146 [01:13<00:12,  1.75it/s]Epoch: 17, train for the 26-th batch, train loss: 0.6909355521202087:  17%|█▉          | 25/151 [00:05<00:27,  4.62it/s]Epoch: 17, train for the 26-th batch, train loss: 0.6909355521202087:  17%|██          | 26/151 [00:05<00:27,  4.56it/s]evaluate for the 29-th batch, evaluate loss: 0.3737216591835022:  26%|████▍            | 28/106 [00:07<00:21,  3.58it/s]evaluate for the 29-th batch, evaluate loss: 0.3737216591835022:  27%|████▋            | 29/106 [00:07<00:20,  3.71it/s]Epoch: 10, train for the 6-th batch, train loss: 0.3478820025920868:   4%|▌             | 5/119 [00:03<01:08,  1.68it/s]Epoch: 10, train for the 6-th batch, train loss: 0.3478820025920868:   5%|▋             | 6/119 [00:03<01:07,  1.67it/s]Epoch: 17, train for the 27-th batch, train loss: 0.36096736788749695:  17%|█▉         | 26/151 [00:05<00:27,  4.56it/s]Epoch: 17, train for the 27-th batch, train loss: 0.36096736788749695:  18%|█▉         | 27/151 [00:05<00:27,  4.58it/s]Epoch: 5, train for the 211-th batch, train loss: 0.6378784775733948:  89%|█████████▋ | 210/237 [02:06<00:16,  1.68it/s]Epoch: 5, train for the 211-th batch, train loss: 0.6378784775733948:  89%|█████████▊ | 211/237 [02:06<00:15,  1.69it/s]evaluate for the 30-th batch, evaluate loss: 0.41673094034194946:  27%|████▍           | 29/106 [00:08<00:20,  3.71it/s]evaluate for the 30-th batch, evaluate loss: 0.41673094034194946:  28%|████▌           | 30/106 [00:08<00:21,  3.58it/s]Epoch: 17, train for the 28-th batch, train loss: 0.7088240385055542:  18%|██▏         | 27/151 [00:05<00:27,  4.58it/s]Epoch: 17, train for the 28-th batch, train loss: 0.7088240385055542:  19%|██▏         | 28/151 [00:05<00:27,  4.52it/s]evaluate for the 31-th batch, evaluate loss: 0.3461589813232422:  28%|████▊            | 30/106 [00:08<00:21,  3.58it/s]evaluate for the 31-th batch, evaluate loss: 0.3461589813232422:  29%|████▉            | 31/106 [00:08<00:19,  3.77it/s]Epoch: 8, train for the 125-th batch, train loss: 0.5003077387809753:  85%|█████████▎ | 124/146 [01:14<00:12,  1.75it/s]Epoch: 8, train for the 125-th batch, train loss: 0.5003077387809753:  86%|█████████▍ | 125/146 [01:14<00:12,  1.71it/s]Epoch: 17, train for the 29-th batch, train loss: 0.5591520071029663:  19%|██▏         | 28/151 [00:05<00:27,  4.52it/s]Epoch: 17, train for the 29-th batch, train loss: 0.5591520071029663:  19%|██▎         | 29/151 [00:05<00:26,  4.58it/s]Epoch: 10, train for the 7-th batch, train loss: 0.27088695764541626:   5%|▋            | 6/119 [00:04<01:07,  1.67it/s]Epoch: 10, train for the 7-th batch, train loss: 0.27088695764541626:   6%|▊            | 7/119 [00:04<01:07,  1.66it/s]Epoch: 5, train for the 212-th batch, train loss: 0.6437397599220276:  89%|█████████▊ | 211/237 [02:06<00:15,  1.69it/s]Epoch: 5, train for the 212-th batch, train loss: 0.6437397599220276:  89%|█████████▊ | 212/237 [02:06<00:14,  1.68it/s]evaluate for the 32-th batch, evaluate loss: 0.4374859929084778:  29%|████▉            | 31/106 [00:08<00:19,  3.77it/s]evaluate for the 32-th batch, evaluate loss: 0.4374859929084778:  30%|█████▏           | 32/106 [00:08<00:20,  3.59it/s]Epoch: 17, train for the 30-th batch, train loss: 0.5148868560791016:  19%|██▎         | 29/151 [00:06<00:26,  4.58it/s]Epoch: 17, train for the 30-th batch, train loss: 0.5148868560791016:  20%|██▍         | 30/151 [00:06<00:26,  4.61it/s]Epoch: 17, train for the 31-th batch, train loss: 0.5695773363113403:  20%|██▍         | 30/151 [00:06<00:26,  4.61it/s]Epoch: 17, train for the 31-th batch, train loss: 0.5695773363113403:  21%|██▍         | 31/151 [00:06<00:26,  4.58it/s]evaluate for the 33-th batch, evaluate loss: 0.4444279372692108:  30%|█████▏           | 32/106 [00:09<00:20,  3.59it/s]evaluate for the 33-th batch, evaluate loss: 0.4444279372692108:  31%|█████▎           | 33/106 [00:09<00:19,  3.67it/s]Epoch: 8, train for the 126-th batch, train loss: 0.5466733574867249:  86%|█████████▍ | 125/146 [01:15<00:12,  1.71it/s]Epoch: 8, train for the 126-th batch, train loss: 0.5466733574867249:  86%|█████████▍ | 126/146 [01:15<00:11,  1.69it/s]Epoch: 17, train for the 32-th batch, train loss: 0.4866923391819:  21%|███            | 31/151 [00:06<00:26,  4.58it/s]Epoch: 17, train for the 32-th batch, train loss: 0.4866923391819:  21%|███▏           | 32/151 [00:06<00:25,  4.60it/s]Epoch: 10, train for the 8-th batch, train loss: 0.1908520609140396:   6%|▊             | 7/119 [00:04<01:07,  1.66it/s]Epoch: 10, train for the 8-th batch, train loss: 0.1908520609140396:   7%|▉             | 8/119 [00:04<01:07,  1.65it/s]Epoch: 5, train for the 213-th batch, train loss: 0.640905499458313:  89%|██████████▋ | 212/237 [02:07<00:14,  1.68it/s]Epoch: 5, train for the 213-th batch, train loss: 0.640905499458313:  90%|██████████▊ | 213/237 [02:07<00:14,  1.68it/s]evaluate for the 34-th batch, evaluate loss: 0.45870569348335266:  31%|████▉           | 33/106 [00:09<00:19,  3.67it/s]evaluate for the 34-th batch, evaluate loss: 0.45870569348335266:  32%|█████▏          | 34/106 [00:09<00:20,  3.51it/s]Epoch: 17, train for the 33-th batch, train loss: 0.6260623335838318:  21%|██▌         | 32/151 [00:06<00:25,  4.60it/s]Epoch: 17, train for the 33-th batch, train loss: 0.6260623335838318:  22%|██▌         | 33/151 [00:06<00:25,  4.55it/s]evaluate for the 35-th batch, evaluate loss: 0.3856481611728668:  32%|█████▍           | 34/106 [00:09<00:20,  3.51it/s]evaluate for the 35-th batch, evaluate loss: 0.3856481611728668:  33%|█████▌           | 35/106 [00:09<00:19,  3.59it/s]Epoch: 17, train for the 34-th batch, train loss: 0.4055858850479126:  22%|██▌         | 33/151 [00:06<00:25,  4.55it/s]Epoch: 17, train for the 34-th batch, train loss: 0.4055858850479126:  23%|██▋         | 34/151 [00:06<00:25,  4.60it/s]Epoch: 8, train for the 127-th batch, train loss: 0.5517643690109253:  86%|█████████▍ | 126/146 [01:15<00:11,  1.69it/s]Epoch: 8, train for the 127-th batch, train loss: 0.5517643690109253:  87%|█████████▌ | 127/146 [01:15<00:11,  1.67it/s]Epoch: 10, train for the 9-th batch, train loss: 0.257609486579895:   7%|█              | 8/119 [00:05<01:07,  1.65it/s]Epoch: 10, train for the 9-th batch, train loss: 0.257609486579895:   8%|█▏             | 9/119 [00:05<01:06,  1.65it/s]Epoch: 17, train for the 35-th batch, train loss: 0.47428953647613525:  23%|██▍        | 34/151 [00:07<00:25,  4.60it/s]Epoch: 17, train for the 35-th batch, train loss: 0.47428953647613525:  23%|██▌        | 35/151 [00:07<00:25,  4.61it/s]evaluate for the 36-th batch, evaluate loss: 0.4391991198062897:  33%|█████▌           | 35/106 [00:09<00:19,  3.59it/s]evaluate for the 36-th batch, evaluate loss: 0.4391991198062897:  34%|█████▊           | 36/106 [00:09<00:19,  3.54it/s]Epoch: 5, train for the 214-th batch, train loss: 0.6598197817802429:  90%|█████████▉ | 213/237 [02:07<00:14,  1.68it/s]Epoch: 5, train for the 214-th batch, train loss: 0.6598197817802429:  90%|█████████▉ | 214/237 [02:07<00:13,  1.68it/s]Epoch: 17, train for the 36-th batch, train loss: 0.5936881303787231:  23%|██▊         | 35/151 [00:07<00:25,  4.61it/s]Epoch: 17, train for the 36-th batch, train loss: 0.5936881303787231:  24%|██▊         | 36/151 [00:07<00:25,  4.59it/s]evaluate for the 37-th batch, evaluate loss: 0.48932570219039917:  34%|█████▍          | 36/106 [00:10<00:19,  3.54it/s]evaluate for the 37-th batch, evaluate loss: 0.48932570219039917:  35%|█████▌          | 37/106 [00:10<00:18,  3.65it/s]Epoch: 8, train for the 128-th batch, train loss: 0.5418974757194519:  87%|█████████▌ | 127/146 [01:16<00:11,  1.67it/s]Epoch: 8, train for the 128-th batch, train loss: 0.5418974757194519:  88%|█████████▋ | 128/146 [01:16<00:10,  1.68it/s]Epoch: 17, train for the 37-th batch, train loss: 0.7151467800140381:  24%|██▊         | 36/151 [00:07<00:25,  4.59it/s]Epoch: 17, train for the 37-th batch, train loss: 0.7151467800140381:  25%|██▉         | 37/151 [00:07<00:25,  4.54it/s]Epoch: 10, train for the 10-th batch, train loss: 0.2861126661300659:   8%|▉            | 9/119 [00:05<01:06,  1.65it/s]Epoch: 10, train for the 10-th batch, train loss: 0.2861126661300659:   8%|█           | 10/119 [00:05<01:05,  1.66it/s]evaluate for the 38-th batch, evaluate loss: 0.5058055520057678:  35%|█████▉           | 37/106 [00:10<00:18,  3.65it/s]evaluate for the 38-th batch, evaluate loss: 0.5058055520057678:  36%|██████           | 38/106 [00:10<00:18,  3.59it/s]Epoch: 5, train for the 215-th batch, train loss: 0.6357349753379822:  90%|█████████▉ | 214/237 [02:08<00:13,  1.68it/s]Epoch: 5, train for the 215-th batch, train loss: 0.6357349753379822:  91%|█████████▉ | 215/237 [02:08<00:12,  1.69it/s]Epoch: 17, train for the 38-th batch, train loss: 0.7201781272888184:  25%|██▉         | 37/151 [00:07<00:25,  4.54it/s]Epoch: 17, train for the 38-th batch, train loss: 0.7201781272888184:  25%|███         | 38/151 [00:07<00:25,  4.48it/s]evaluate for the 39-th batch, evaluate loss: 0.3921814262866974:  36%|██████           | 38/106 [00:10<00:18,  3.59it/s]evaluate for the 39-th batch, evaluate loss: 0.3921814262866974:  37%|██████▎          | 39/106 [00:10<00:18,  3.67it/s]Epoch: 17, train for the 39-th batch, train loss: 0.5854995846748352:  25%|███         | 38/151 [00:08<00:25,  4.48it/s]Epoch: 17, train for the 39-th batch, train loss: 0.5854995846748352:  26%|███         | 39/151 [00:08<00:24,  4.50it/s]Epoch: 8, train for the 129-th batch, train loss: 0.5538209676742554:  88%|█████████▋ | 128/146 [01:16<00:10,  1.68it/s]Epoch: 8, train for the 129-th batch, train loss: 0.5538209676742554:  88%|█████████▋ | 129/146 [01:16<00:10,  1.65it/s]evaluate for the 40-th batch, evaluate loss: 0.4297215938568115:  37%|██████▎          | 39/106 [00:11<00:18,  3.67it/s]evaluate for the 40-th batch, evaluate loss: 0.4297215938568115:  38%|██████▍          | 40/106 [00:11<00:18,  3.63it/s]Epoch: 17, train for the 40-th batch, train loss: 0.39097312092781067:  26%|██▊        | 39/151 [00:08<00:24,  4.50it/s]Epoch: 17, train for the 40-th batch, train loss: 0.39097312092781067:  26%|██▉        | 40/151 [00:08<00:24,  4.56it/s]Epoch: 10, train for the 11-th batch, train loss: 0.22652538120746613:   8%|▉          | 10/119 [00:06<01:05,  1.66it/s]Epoch: 10, train for the 11-th batch, train loss: 0.22652538120746613:   9%|█          | 11/119 [00:06<01:05,  1.64it/s]Epoch: 5, train for the 216-th batch, train loss: 0.610797107219696:  91%|██████████▉ | 215/237 [02:09<00:12,  1.69it/s]Epoch: 5, train for the 216-th batch, train loss: 0.610797107219696:  91%|██████████▉ | 216/237 [02:09<00:12,  1.68it/s]Epoch: 17, train for the 41-th batch, train loss: 0.535564124584198:  26%|███▍         | 40/151 [00:08<00:24,  4.56it/s]Epoch: 17, train for the 41-th batch, train loss: 0.535564124584198:  27%|███▌         | 41/151 [00:08<00:24,  4.56it/s]evaluate for the 41-th batch, evaluate loss: 0.41911056637763977:  38%|██████          | 40/106 [00:11<00:18,  3.63it/s]evaluate for the 41-th batch, evaluate loss: 0.41911056637763977:  39%|██████▏         | 41/106 [00:11<00:18,  3.61it/s]Epoch: 17, train for the 42-th batch, train loss: 0.6124298572540283:  27%|███▎        | 41/151 [00:08<00:24,  4.56it/s]Epoch: 17, train for the 42-th batch, train loss: 0.6124298572540283:  28%|███▎        | 42/151 [00:08<00:24,  4.51it/s]Epoch: 8, train for the 130-th batch, train loss: 0.5144625306129456:  88%|█████████▋ | 129/146 [01:17<00:10,  1.65it/s]Epoch: 8, train for the 130-th batch, train loss: 0.5144625306129456:  89%|█████████▊ | 130/146 [01:17<00:09,  1.64it/s]evaluate for the 42-th batch, evaluate loss: 0.3732312023639679:  39%|██████▌          | 41/106 [00:11<00:18,  3.61it/s]evaluate for the 42-th batch, evaluate loss: 0.3732312023639679:  40%|██████▋          | 42/106 [00:11<00:17,  3.63it/s]Epoch: 17, train for the 43-th batch, train loss: 0.6429257988929749:  28%|███▎        | 42/151 [00:08<00:24,  4.51it/s]Epoch: 17, train for the 43-th batch, train loss: 0.6429257988929749:  28%|███▍        | 43/151 [00:08<00:24,  4.48it/s]Epoch: 10, train for the 12-th batch, train loss: 0.30254581570625305:   9%|█          | 11/119 [00:07<01:05,  1.64it/s]Epoch: 10, train for the 12-th batch, train loss: 0.30254581570625305:  10%|█          | 12/119 [00:07<01:05,  1.63it/s]Epoch: 5, train for the 217-th batch, train loss: 0.610630214214325:  91%|██████████▉ | 216/237 [02:09<00:12,  1.68it/s]Epoch: 5, train for the 217-th batch, train loss: 0.610630214214325:  92%|██████████▉ | 217/237 [02:09<00:11,  1.68it/s]evaluate for the 43-th batch, evaluate loss: 0.34251636266708374:  40%|██████▎         | 42/106 [00:11<00:17,  3.63it/s]evaluate for the 43-th batch, evaluate loss: 0.34251636266708374:  41%|██████▍         | 43/106 [00:11<00:17,  3.56it/s]Epoch: 17, train for the 44-th batch, train loss: 0.5859084129333496:  28%|███▍        | 43/151 [00:09<00:24,  4.48it/s]Epoch: 17, train for the 44-th batch, train loss: 0.5859084129333496:  29%|███▍        | 44/151 [00:09<00:24,  4.45it/s]evaluate for the 44-th batch, evaluate loss: 0.41399702429771423:  41%|██████▍         | 43/106 [00:12<00:17,  3.56it/s]evaluate for the 44-th batch, evaluate loss: 0.41399702429771423:  42%|██████▋         | 44/106 [00:12<00:16,  3.71it/s]Epoch: 17, train for the 45-th batch, train loss: 0.5972830653190613:  29%|███▍        | 44/151 [00:09<00:24,  4.45it/s]Epoch: 17, train for the 45-th batch, train loss: 0.5972830653190613:  30%|███▌        | 45/151 [00:09<00:23,  4.43it/s]Epoch: 8, train for the 131-th batch, train loss: 0.5292854309082031:  89%|█████████▊ | 130/146 [01:18<00:09,  1.64it/s]Epoch: 8, train for the 131-th batch, train loss: 0.5292854309082031:  90%|█████████▊ | 131/146 [01:18<00:09,  1.64it/s]Epoch: 10, train for the 13-th batch, train loss: 0.23025478422641754:  10%|█          | 12/119 [00:07<01:05,  1.63it/s]Epoch: 10, train for the 13-th batch, train loss: 0.23025478422641754:  11%|█▏         | 13/119 [00:07<01:04,  1.64it/s]Epoch: 5, train for the 218-th batch, train loss: 0.6495212912559509:  92%|██████████ | 217/237 [02:10<00:11,  1.68it/s]Epoch: 5, train for the 218-th batch, train loss: 0.6495212912559509:  92%|██████████ | 218/237 [02:10<00:11,  1.68it/s]Epoch: 17, train for the 46-th batch, train loss: 0.4909091591835022:  30%|███▌        | 45/151 [00:09<00:23,  4.43it/s]Epoch: 17, train for the 46-th batch, train loss: 0.4909091591835022:  30%|███▋        | 46/151 [00:09<00:23,  4.48it/s]evaluate for the 45-th batch, evaluate loss: 0.43340131640434265:  42%|██████▋         | 44/106 [00:12<00:16,  3.71it/s]evaluate for the 45-th batch, evaluate loss: 0.43340131640434265:  42%|██████▊         | 45/106 [00:12<00:16,  3.59it/s]Epoch: 17, train for the 47-th batch, train loss: 0.6072788238525391:  30%|███▋        | 46/151 [00:09<00:23,  4.48it/s]Epoch: 17, train for the 47-th batch, train loss: 0.6072788238525391:  31%|███▋        | 47/151 [00:09<00:23,  4.47it/s]evaluate for the 46-th batch, evaluate loss: 0.43203723430633545:  42%|██████▊         | 45/106 [00:12<00:16,  3.59it/s]evaluate for the 46-th batch, evaluate loss: 0.43203723430633545:  43%|██████▉         | 46/106 [00:12<00:16,  3.75it/s]Epoch: 8, train for the 132-th batch, train loss: 0.5485431551933289:  90%|█████████▊ | 131/146 [01:18<00:09,  1.64it/s]Epoch: 8, train for the 132-th batch, train loss: 0.5485431551933289:  90%|█████████▉ | 132/146 [01:18<00:08,  1.64it/s]Epoch: 17, train for the 48-th batch, train loss: 0.5581164360046387:  31%|███▋        | 47/151 [00:10<00:23,  4.47it/s]Epoch: 17, train for the 48-th batch, train loss: 0.5581164360046387:  32%|███▊        | 48/151 [00:10<00:23,  4.46it/s]Epoch: 10, train for the 14-th batch, train loss: 0.2829003930091858:  11%|█▎          | 13/119 [00:08<01:04,  1.64it/s]Epoch: 10, train for the 14-th batch, train loss: 0.2829003930091858:  12%|█▍          | 14/119 [00:08<01:04,  1.63it/s]Epoch: 5, train for the 219-th batch, train loss: 0.6230601668357849:  92%|██████████ | 218/237 [02:10<00:11,  1.68it/s]Epoch: 5, train for the 219-th batch, train loss: 0.6230601668357849:  92%|██████████▏| 219/237 [02:10<00:10,  1.67it/s]evaluate for the 47-th batch, evaluate loss: 0.325703501701355:  43%|███████▊          | 46/106 [00:12<00:16,  3.75it/s]evaluate for the 47-th batch, evaluate loss: 0.325703501701355:  44%|███████▉          | 47/106 [00:12<00:16,  3.56it/s]Epoch: 17, train for the 49-th batch, train loss: 0.5995237827301025:  32%|███▊        | 48/151 [00:10<00:23,  4.46it/s]Epoch: 17, train for the 49-th batch, train loss: 0.5995237827301025:  32%|███▉        | 49/151 [00:10<00:22,  4.44it/s]evaluate for the 48-th batch, evaluate loss: 0.45505291223526:  44%|████████▍          | 47/106 [00:13<00:16,  3.56it/s]evaluate for the 48-th batch, evaluate loss: 0.45505291223526:  45%|████████▌          | 48/106 [00:13<00:16,  3.62it/s]Epoch: 17, train for the 50-th batch, train loss: 0.5831224918365479:  32%|███▉        | 49/151 [00:10<00:22,  4.44it/s]Epoch: 17, train for the 50-th batch, train loss: 0.5831224918365479:  33%|███▉        | 50/151 [00:10<00:22,  4.43it/s]Epoch: 8, train for the 133-th batch, train loss: 0.5129354596138:  90%|████████████▋ | 132/146 [01:19<00:08,  1.64it/s]Epoch: 8, train for the 133-th batch, train loss: 0.5129354596138:  91%|████████████▊ | 133/146 [01:19<00:07,  1.64it/s]Epoch: 17, train for the 51-th batch, train loss: 0.6318643093109131:  33%|███▉        | 50/151 [00:10<00:22,  4.43it/s]Epoch: 17, train for the 51-th batch, train loss: 0.6318643093109131:  34%|████        | 51/151 [00:10<00:22,  4.42it/s]evaluate for the 49-th batch, evaluate loss: 0.4465232789516449:  45%|███████▋         | 48/106 [00:13<00:16,  3.62it/s]evaluate for the 49-th batch, evaluate loss: 0.4465232789516449:  46%|███████▊         | 49/106 [00:13<00:16,  3.55it/s]Epoch: 5, train for the 220-th batch, train loss: 0.642724871635437:  92%|███████████ | 219/237 [02:11<00:10,  1.67it/s]Epoch: 5, train for the 220-th batch, train loss: 0.642724871635437:  93%|███████████▏| 220/237 [02:11<00:10,  1.68it/s]Epoch: 10, train for the 15-th batch, train loss: 0.2205096036195755:  12%|█▍          | 14/119 [00:09<01:04,  1.63it/s]Epoch: 10, train for the 15-th batch, train loss: 0.2205096036195755:  13%|█▌          | 15/119 [00:09<01:03,  1.64it/s]Epoch: 17, train for the 52-th batch, train loss: 0.6104744076728821:  34%|████        | 51/151 [00:10<00:22,  4.42it/s]Epoch: 17, train for the 52-th batch, train loss: 0.6104744076728821:  34%|████▏       | 52/151 [00:10<00:22,  4.42it/s]evaluate for the 50-th batch, evaluate loss: 0.37933507561683655:  46%|███████▍        | 49/106 [00:13<00:16,  3.55it/s]evaluate for the 50-th batch, evaluate loss: 0.37933507561683655:  47%|███████▌        | 50/106 [00:13<00:15,  3.68it/s]Epoch: 17, train for the 53-th batch, train loss: 0.5356718897819519:  34%|████▏       | 52/151 [00:11<00:22,  4.42it/s]Epoch: 17, train for the 53-th batch, train loss: 0.5356718897819519:  35%|████▏       | 53/151 [00:11<00:21,  4.47it/s]Epoch: 8, train for the 134-th batch, train loss: 0.5202937126159668:  91%|██████████ | 133/146 [01:20<00:07,  1.64it/s]Epoch: 8, train for the 134-th batch, train loss: 0.5202937126159668:  92%|██████████ | 134/146 [01:20<00:07,  1.64it/s]evaluate for the 51-th batch, evaluate loss: 0.4473358392715454:  47%|████████         | 50/106 [00:14<00:15,  3.68it/s]evaluate for the 51-th batch, evaluate loss: 0.4473358392715454:  48%|████████▏        | 51/106 [00:14<00:15,  3.60it/s]Epoch: 5, train for the 221-th batch, train loss: 0.6361153721809387:  93%|██████████▏| 220/237 [02:12<00:10,  1.68it/s]Epoch: 5, train for the 221-th batch, train loss: 0.6361153721809387:  93%|██████████▎| 221/237 [02:12<00:09,  1.69it/s]Epoch: 10, train for the 16-th batch, train loss: 0.24818767607212067:  13%|█▍         | 15/119 [00:09<01:03,  1.64it/s]Epoch: 10, train for the 16-th batch, train loss: 0.24818767607212067:  13%|█▍         | 16/119 [00:09<01:02,  1.65it/s]Epoch: 17, train for the 54-th batch, train loss: 0.5621668100357056:  35%|████▏       | 53/151 [00:11<00:21,  4.47it/s]Epoch: 17, train for the 54-th batch, train loss: 0.5621668100357056:  36%|████▎       | 54/151 [00:11<00:21,  4.46it/s]evaluate for the 52-th batch, evaluate loss: 0.4440249800682068:  48%|████████▏        | 51/106 [00:14<00:15,  3.60it/s]evaluate for the 52-th batch, evaluate loss: 0.4440249800682068:  49%|████████▎        | 52/106 [00:14<00:14,  3.70it/s]Epoch: 17, train for the 55-th batch, train loss: 0.5735177397727966:  36%|████▎       | 54/151 [00:11<00:21,  4.46it/s]Epoch: 17, train for the 55-th batch, train loss: 0.5735177397727966:  36%|████▎       | 55/151 [00:11<00:21,  4.45it/s]Epoch: 8, train for the 135-th batch, train loss: 0.5323893427848816:  92%|██████████ | 134/146 [01:20<00:07,  1.64it/s]Epoch: 8, train for the 135-th batch, train loss: 0.5323893427848816:  92%|██████████▏| 135/146 [01:20<00:06,  1.66it/s]evaluate for the 53-th batch, evaluate loss: 0.5104314088821411:  49%|████████▎        | 52/106 [00:14<00:14,  3.70it/s]evaluate for the 53-th batch, evaluate loss: 0.5104314088821411:  50%|████████▌        | 53/106 [00:14<00:14,  3.67it/s]Epoch: 17, train for the 56-th batch, train loss: 0.3525743782520294:  36%|████▎       | 55/151 [00:11<00:21,  4.45it/s]Epoch: 17, train for the 56-th batch, train loss: 0.3525743782520294:  37%|████▍       | 56/151 [00:11<00:21,  4.52it/s]Epoch: 5, train for the 222-th batch, train loss: 0.6252997517585754:  93%|██████████▎| 221/237 [02:12<00:09,  1.69it/s]Epoch: 5, train for the 222-th batch, train loss: 0.6252997517585754:  94%|██████████▎| 222/237 [02:12<00:08,  1.68it/s]Epoch: 10, train for the 17-th batch, train loss: 0.21470434963703156:  13%|█▍         | 16/119 [00:10<01:02,  1.65it/s]Epoch: 10, train for the 17-th batch, train loss: 0.21470434963703156:  14%|█▌         | 17/119 [00:10<01:01,  1.65it/s]Epoch: 17, train for the 57-th batch, train loss: 0.4271736145019531:  37%|████▍       | 56/151 [00:12<00:21,  4.52it/s]Epoch: 17, train for the 57-th batch, train loss: 0.4271736145019531:  38%|████▌       | 57/151 [00:12<00:20,  4.56it/s]evaluate for the 54-th batch, evaluate loss: 0.38006678223609924:  50%|████████        | 53/106 [00:14<00:14,  3.67it/s]evaluate for the 54-th batch, evaluate loss: 0.38006678223609924:  51%|████████▏       | 54/106 [00:14<00:14,  3.62it/s]Epoch: 17, train for the 58-th batch, train loss: 0.5452996492385864:  38%|████▌       | 57/151 [00:12<00:20,  4.56it/s]Epoch: 17, train for the 58-th batch, train loss: 0.5452996492385864:  38%|████▌       | 58/151 [00:12<00:20,  4.53it/s]evaluate for the 55-th batch, evaluate loss: 0.3417353332042694:  51%|████████▋        | 54/106 [00:15<00:14,  3.62it/s]evaluate for the 55-th batch, evaluate loss: 0.3417353332042694:  52%|████████▊        | 55/106 [00:15<00:13,  3.64it/s]Epoch: 8, train for the 136-th batch, train loss: 0.5086563229560852:  92%|██████████▏| 135/146 [01:21<00:06,  1.66it/s]Epoch: 8, train for the 136-th batch, train loss: 0.5086563229560852:  93%|██████████▏| 136/146 [01:21<00:06,  1.65it/s]Epoch: 17, train for the 59-th batch, train loss: 0.5289981365203857:  38%|████▌       | 58/151 [00:12<00:20,  4.53it/s]Epoch: 17, train for the 59-th batch, train loss: 0.5289981365203857:  39%|████▋       | 59/151 [00:12<00:20,  4.51it/s]Epoch: 5, train for the 223-th batch, train loss: 0.6289777755737305:  94%|██████████▎| 222/237 [02:13<00:08,  1.68it/s]Epoch: 5, train for the 223-th batch, train loss: 0.6289777755737305:  94%|██████████▎| 223/237 [02:13<00:08,  1.68it/s]Epoch: 10, train for the 18-th batch, train loss: 0.2322262078523636:  14%|█▋          | 17/119 [00:10<01:01,  1.65it/s]Epoch: 10, train for the 18-th batch, train loss: 0.2322262078523636:  15%|█▊          | 18/119 [00:10<01:01,  1.65it/s]evaluate for the 56-th batch, evaluate loss: 0.44032400846481323:  52%|████████▎       | 55/106 [00:15<00:13,  3.64it/s]evaluate for the 56-th batch, evaluate loss: 0.44032400846481323:  53%|████████▍       | 56/106 [00:15<00:14,  3.56it/s]Epoch: 17, train for the 60-th batch, train loss: 0.3826311528682709:  39%|████▋       | 59/151 [00:12<00:20,  4.51it/s]Epoch: 17, train for the 60-th batch, train loss: 0.3826311528682709:  40%|████▊       | 60/151 [00:12<00:20,  4.53it/s]evaluate for the 57-th batch, evaluate loss: 0.3953571617603302:  53%|████████▉        | 56/106 [00:15<00:14,  3.56it/s]evaluate for the 57-th batch, evaluate loss: 0.3953571617603302:  54%|█████████▏       | 57/106 [00:15<00:13,  3.71it/s]Epoch: 17, train for the 61-th batch, train loss: 0.32737335562705994:  40%|████▎      | 60/151 [00:12<00:20,  4.53it/s]Epoch: 17, train for the 61-th batch, train loss: 0.32737335562705994:  40%|████▍      | 61/151 [00:12<00:19,  4.57it/s]Epoch: 8, train for the 137-th batch, train loss: 0.5364537835121155:  93%|██████████▏| 136/146 [01:21<00:06,  1.65it/s]Epoch: 8, train for the 137-th batch, train loss: 0.5364537835121155:  94%|██████████▎| 137/146 [01:21<00:05,  1.65it/s]Epoch: 5, train for the 224-th batch, train loss: 0.653570294380188:  94%|███████████▎| 223/237 [02:13<00:08,  1.68it/s]Epoch: 5, train for the 224-th batch, train loss: 0.653570294380188:  95%|███████████▎| 224/237 [02:13<00:07,  1.68it/s]Epoch: 17, train for the 62-th batch, train loss: 0.45745936036109924:  40%|████▍      | 61/151 [00:13<00:19,  4.57it/s]Epoch: 17, train for the 62-th batch, train loss: 0.45745936036109924:  41%|████▌      | 62/151 [00:13<00:19,  4.56it/s]Epoch: 10, train for the 19-th batch, train loss: 0.21330860257148743:  15%|█▋         | 18/119 [00:11<01:01,  1.65it/s]Epoch: 10, train for the 19-th batch, train loss: 0.21330860257148743:  16%|█▊         | 19/119 [00:11<01:00,  1.64it/s]evaluate for the 58-th batch, evaluate loss: 0.3835519254207611:  54%|█████████▏       | 57/106 [00:15<00:13,  3.71it/s]evaluate for the 58-th batch, evaluate loss: 0.3835519254207611:  55%|█████████▎       | 58/106 [00:15<00:13,  3.59it/s]Epoch: 17, train for the 63-th batch, train loss: 0.5943247079849243:  41%|████▉       | 62/151 [00:13<00:19,  4.56it/s]Epoch: 17, train for the 63-th batch, train loss: 0.5943247079849243:  42%|█████       | 63/151 [00:13<00:19,  4.53it/s]evaluate for the 59-th batch, evaluate loss: 0.42200925946235657:  55%|████████▊       | 58/106 [00:16<00:13,  3.59it/s]evaluate for the 59-th batch, evaluate loss: 0.42200925946235657:  56%|████████▉       | 59/106 [00:16<00:12,  3.75it/s]Epoch: 17, train for the 64-th batch, train loss: 0.5895419716835022:  42%|█████       | 63/151 [00:13<00:19,  4.53it/s]Epoch: 17, train for the 64-th batch, train loss: 0.5895419716835022:  42%|█████       | 64/151 [00:13<00:19,  4.51it/s]Epoch: 8, train for the 138-th batch, train loss: 0.5600101947784424:  94%|██████████▎| 137/146 [01:22<00:05,  1.65it/s]Epoch: 8, train for the 138-th batch, train loss: 0.5600101947784424:  95%|██████████▍| 138/146 [01:22<00:04,  1.65it/s]Epoch: 5, train for the 225-th batch, train loss: 0.6585350036621094:  95%|██████████▍| 224/237 [02:14<00:07,  1.68it/s]Epoch: 5, train for the 225-th batch, train loss: 0.6585350036621094:  95%|██████████▍| 225/237 [02:14<00:07,  1.67it/s]evaluate for the 60-th batch, evaluate loss: 0.42320945858955383:  56%|████████▉       | 59/106 [00:16<00:12,  3.75it/s]evaluate for the 60-th batch, evaluate loss: 0.42320945858955383:  57%|█████████       | 60/106 [00:16<00:12,  3.56it/s]Epoch: 10, train for the 20-th batch, train loss: 0.2631154954433441:  16%|█▉          | 19/119 [00:12<01:00,  1.64it/s]Epoch: 10, train for the 20-th batch, train loss: 0.2631154954433441:  17%|██          | 20/119 [00:12<01:00,  1.64it/s]Epoch: 17, train for the 65-th batch, train loss: 0.5277659296989441:  42%|█████       | 64/151 [00:13<00:19,  4.51it/s]Epoch: 17, train for the 65-th batch, train loss: 0.5277659296989441:  43%|█████▏      | 65/151 [00:13<00:19,  4.50it/s]evaluate for the 61-th batch, evaluate loss: 0.4484042525291443:  57%|█████████▌       | 60/106 [00:16<00:12,  3.56it/s]evaluate for the 61-th batch, evaluate loss: 0.4484042525291443:  58%|█████████▊       | 61/106 [00:16<00:12,  3.62it/s]Epoch: 17, train for the 66-th batch, train loss: 0.6069721579551697:  43%|█████▏      | 65/151 [00:14<00:19,  4.50it/s]Epoch: 17, train for the 66-th batch, train loss: 0.6069721579551697:  44%|█████▏      | 66/151 [00:14<00:18,  4.51it/s]Epoch: 8, train for the 139-th batch, train loss: 0.575530469417572:  95%|███████████▎| 138/146 [01:23<00:04,  1.65it/s]Epoch: 8, train for the 139-th batch, train loss: 0.575530469417572:  95%|███████████▍| 139/146 [01:23<00:04,  1.64it/s]Epoch: 17, train for the 67-th batch, train loss: 0.4263552129268646:  44%|█████▏      | 66/151 [00:14<00:18,  4.51it/s]Epoch: 17, train for the 67-th batch, train loss: 0.4263552129268646:  44%|█████▎      | 67/151 [00:14<00:18,  4.57it/s]evaluate for the 62-th batch, evaluate loss: 0.44585463404655457:  58%|█████████▏      | 61/106 [00:17<00:12,  3.62it/s]evaluate for the 62-th batch, evaluate loss: 0.44585463404655457:  58%|█████████▎      | 62/106 [00:17<00:12,  3.55it/s]Epoch: 5, train for the 226-th batch, train loss: 0.6228296160697937:  95%|██████████▍| 225/237 [02:14<00:07,  1.67it/s]Epoch: 5, train for the 226-th batch, train loss: 0.6228296160697937:  95%|██████████▍| 226/237 [02:14<00:06,  1.68it/s]Epoch: 10, train for the 21-th batch, train loss: 0.2507074177265167:  17%|██          | 20/119 [00:12<01:00,  1.64it/s]Epoch: 10, train for the 21-th batch, train loss: 0.2507074177265167:  18%|██          | 21/119 [00:12<00:59,  1.64it/s]Epoch: 17, train for the 68-th batch, train loss: 0.41687873005867004:  44%|████▉      | 67/151 [00:14<00:18,  4.57it/s]Epoch: 17, train for the 68-th batch, train loss: 0.41687873005867004:  45%|████▉      | 68/151 [00:14<00:18,  4.60it/s]evaluate for the 63-th batch, evaluate loss: 0.45675963163375854:  58%|█████████▎      | 62/106 [00:17<00:12,  3.55it/s]evaluate for the 63-th batch, evaluate loss: 0.45675963163375854:  59%|█████████▌      | 63/106 [00:17<00:11,  3.67it/s]Epoch: 17, train for the 69-th batch, train loss: 0.615843653678894:  45%|█████▊       | 68/151 [00:14<00:18,  4.60it/s]Epoch: 17, train for the 69-th batch, train loss: 0.615843653678894:  46%|█████▉       | 69/151 [00:14<00:17,  4.57it/s]Epoch: 8, train for the 140-th batch, train loss: 0.5083932280540466:  95%|██████████▍| 139/146 [01:23<00:04,  1.64it/s]Epoch: 8, train for the 140-th batch, train loss: 0.5083932280540466:  96%|██████████▌| 140/146 [01:23<00:03,  1.64it/s]evaluate for the 64-th batch, evaluate loss: 0.4037480056285858:  59%|██████████       | 63/106 [00:17<00:11,  3.67it/s]evaluate for the 64-th batch, evaluate loss: 0.4037480056285858:  60%|██████████▎      | 64/106 [00:17<00:11,  3.61it/s]Epoch: 17, train for the 70-th batch, train loss: 0.3126351535320282:  46%|█████▍      | 69/151 [00:14<00:17,  4.57it/s]Epoch: 17, train for the 70-th batch, train loss: 0.3126351535320282:  46%|█████▌      | 70/151 [00:14<00:17,  4.62it/s]Epoch: 5, train for the 227-th batch, train loss: 0.6376798748970032:  95%|██████████▍| 226/237 [02:15<00:06,  1.68it/s]Epoch: 5, train for the 227-th batch, train loss: 0.6376798748970032:  96%|██████████▌| 227/237 [02:15<00:05,  1.69it/s]Epoch: 10, train for the 22-th batch, train loss: 0.24723903834819794:  18%|█▉         | 21/119 [00:13<00:59,  1.64it/s]Epoch: 10, train for the 22-th batch, train loss: 0.24723903834819794:  18%|██         | 22/119 [00:13<00:59,  1.64it/s]evaluate for the 65-th batch, evaluate loss: 0.49680206179618835:  60%|█████████▋      | 64/106 [00:17<00:11,  3.61it/s]evaluate for the 65-th batch, evaluate loss: 0.49680206179618835:  61%|█████████▊      | 65/106 [00:17<00:11,  3.70it/s]Epoch: 17, train for the 71-th batch, train loss: 0.679322361946106:  46%|██████       | 70/151 [00:15<00:17,  4.62it/s]Epoch: 17, train for the 71-th batch, train loss: 0.679322361946106:  47%|██████       | 71/151 [00:15<00:17,  4.58it/s]Epoch: 17, train for the 72-th batch, train loss: 0.607795774936676:  47%|██████       | 71/151 [00:15<00:17,  4.58it/s]Epoch: 17, train for the 72-th batch, train loss: 0.607795774936676:  48%|██████▏      | 72/151 [00:15<00:17,  4.56it/s]evaluate for the 66-th batch, evaluate loss: 0.4090920090675354:  61%|██████████▍      | 65/106 [00:18<00:11,  3.70it/s]evaluate for the 66-th batch, evaluate loss: 0.4090920090675354:  62%|██████████▌      | 66/106 [00:18<00:10,  3.65it/s]Epoch: 8, train for the 141-th batch, train loss: 0.5448288321495056:  96%|██████████▌| 140/146 [01:24<00:03,  1.64it/s]Epoch: 8, train for the 141-th batch, train loss: 0.5448288321495056:  97%|██████████▌| 141/146 [01:24<00:03,  1.64it/s]Epoch: 5, train for the 228-th batch, train loss: 0.6295447945594788:  96%|██████████▌| 227/237 [02:16<00:05,  1.69it/s]Epoch: 5, train for the 228-th batch, train loss: 0.6295447945594788:  96%|██████████▌| 228/237 [02:16<00:05,  1.67it/s]Epoch: 17, train for the 73-th batch, train loss: 0.5967162847518921:  48%|█████▋      | 72/151 [00:15<00:17,  4.56it/s]Epoch: 17, train for the 73-th batch, train loss: 0.5967162847518921:  48%|█████▊      | 73/151 [00:15<00:17,  4.54it/s]Epoch: 10, train for the 23-th batch, train loss: 0.22533507645130157:  18%|██         | 22/119 [00:13<00:59,  1.64it/s]Epoch: 10, train for the 23-th batch, train loss: 0.22533507645130157:  19%|██▏        | 23/119 [00:13<00:58,  1.64it/s]evaluate for the 67-th batch, evaluate loss: 0.4048188030719757:  62%|██████████▌      | 66/106 [00:18<00:10,  3.65it/s]evaluate for the 67-th batch, evaluate loss: 0.4048188030719757:  63%|██████████▋      | 67/106 [00:18<00:10,  3.63it/s]Epoch: 17, train for the 74-th batch, train loss: 0.5365287065505981:  48%|█████▊      | 73/151 [00:15<00:17,  4.54it/s]Epoch: 17, train for the 74-th batch, train loss: 0.5365287065505981:  49%|█████▉      | 74/151 [00:15<00:17,  4.52it/s]evaluate for the 68-th batch, evaluate loss: 0.44137489795684814:  63%|██████████      | 67/106 [00:18<00:10,  3.63it/s]evaluate for the 68-th batch, evaluate loss: 0.44137489795684814:  64%|██████████▎     | 68/106 [00:18<00:10,  3.65it/s]Epoch: 17, train for the 75-th batch, train loss: 0.5366729497909546:  49%|█████▉      | 74/151 [00:16<00:17,  4.52it/s]Epoch: 17, train for the 75-th batch, train loss: 0.5366729497909546:  50%|█████▉      | 75/151 [00:16<00:16,  4.52it/s]Epoch: 8, train for the 142-th batch, train loss: 0.48346251249313354:  97%|█████████▋| 141/146 [01:24<00:03,  1.64it/s]Epoch: 8, train for the 142-th batch, train loss: 0.48346251249313354:  97%|█████████▋| 142/146 [01:24<00:02,  1.64it/s]Epoch: 5, train for the 229-th batch, train loss: 0.6550930142402649:  96%|██████████▌| 228/237 [02:16<00:05,  1.67it/s]Epoch: 5, train for the 229-th batch, train loss: 0.6550930142402649:  97%|██████████▋| 229/237 [02:16<00:04,  1.68it/s]Epoch: 10, train for the 24-th batch, train loss: 0.20960895717144012:  19%|██▏        | 23/119 [00:14<00:58,  1.64it/s]Epoch: 10, train for the 24-th batch, train loss: 0.20960895717144012:  20%|██▏        | 24/119 [00:14<00:57,  1.64it/s]evaluate for the 69-th batch, evaluate loss: 0.39908477663993835:  64%|██████████▎     | 68/106 [00:19<00:10,  3.65it/s]evaluate for the 69-th batch, evaluate loss: 0.39908477663993835:  65%|██████████▍     | 69/106 [00:19<00:10,  3.57it/s]Epoch: 17, train for the 76-th batch, train loss: 0.5721828937530518:  50%|█████▉      | 75/151 [00:16<00:16,  4.52it/s]Epoch: 17, train for the 76-th batch, train loss: 0.5721828937530518:  50%|██████      | 76/151 [00:16<00:18,  4.06it/s]evaluate for the 70-th batch, evaluate loss: 0.3953903615474701:  65%|███████████      | 69/106 [00:19<00:10,  3.57it/s]evaluate for the 70-th batch, evaluate loss: 0.3953903615474701:  66%|███████████▏     | 70/106 [00:19<00:09,  3.71it/s]Epoch: 17, train for the 77-th batch, train loss: 0.5538334250450134:  50%|██████      | 76/151 [00:16<00:18,  4.06it/s]Epoch: 17, train for the 77-th batch, train loss: 0.5538334250450134:  51%|██████      | 77/151 [00:16<00:17,  4.17it/s]Epoch: 8, train for the 143-th batch, train loss: 0.5395850539207458:  97%|██████████▋| 142/146 [01:25<00:02,  1.64it/s]Epoch: 8, train for the 143-th batch, train loss: 0.5395850539207458:  98%|██████████▊| 143/146 [01:25<00:01,  1.64it/s]Epoch: 5, train for the 230-th batch, train loss: 0.6397101879119873:  97%|██████████▋| 229/237 [02:17<00:04,  1.68it/s]Epoch: 5, train for the 230-th batch, train loss: 0.6397101879119873:  97%|██████████▋| 230/237 [02:17<00:04,  1.68it/s]Epoch: 17, train for the 78-th batch, train loss: 0.5538317561149597:  51%|██████      | 77/151 [00:16<00:17,  4.17it/s]Epoch: 17, train for the 78-th batch, train loss: 0.5538317561149597:  52%|██████▏     | 78/151 [00:16<00:17,  4.26it/s]evaluate for the 71-th batch, evaluate loss: 0.3456459641456604:  66%|███████████▏     | 70/106 [00:19<00:09,  3.71it/s]evaluate for the 71-th batch, evaluate loss: 0.3456459641456604:  67%|███████████▍     | 71/106 [00:19<00:09,  3.59it/s]Epoch: 10, train for the 25-th batch, train loss: 0.21971677243709564:  20%|██▏        | 24/119 [00:15<00:57,  1.64it/s]Epoch: 10, train for the 25-th batch, train loss: 0.21971677243709564:  21%|██▎        | 25/119 [00:15<00:57,  1.63it/s]Epoch: 17, train for the 79-th batch, train loss: 0.5955374240875244:  52%|██████▏     | 78/151 [00:17<00:17,  4.26it/s]Epoch: 17, train for the 79-th batch, train loss: 0.5955374240875244:  52%|██████▎     | 79/151 [00:17<00:16,  4.33it/s]evaluate for the 72-th batch, evaluate loss: 0.3273175358772278:  67%|███████████▍     | 71/106 [00:19<00:09,  3.59it/s]evaluate for the 72-th batch, evaluate loss: 0.3273175358772278:  68%|███████████▌     | 72/106 [00:19<00:09,  3.76it/s]Epoch: 17, train for the 80-th batch, train loss: 0.5754525065422058:  52%|██████▎     | 79/151 [00:17<00:16,  4.33it/s]Epoch: 17, train for the 80-th batch, train loss: 0.5754525065422058:  53%|██████▎     | 80/151 [00:17<00:16,  4.38it/s]Epoch: 8, train for the 144-th batch, train loss: 0.5100738406181335:  98%|██████████▊| 143/146 [01:26<00:01,  1.64it/s]Epoch: 8, train for the 144-th batch, train loss: 0.5100738406181335:  99%|██████████▊| 144/146 [01:26<00:01,  1.63it/s]Epoch: 5, train for the 231-th batch, train loss: 0.6350310444831848:  97%|██████████▋| 230/237 [02:17<00:04,  1.68it/s]Epoch: 5, train for the 231-th batch, train loss: 0.6350310444831848:  97%|██████████▋| 231/237 [02:17<00:03,  1.67it/s]evaluate for the 73-th batch, evaluate loss: 0.3970089554786682:  68%|███████████▌     | 72/106 [00:20<00:09,  3.76it/s]evaluate for the 73-th batch, evaluate loss: 0.3970089554786682:  69%|███████████▋     | 73/106 [00:20<00:09,  3.57it/s]Epoch: 17, train for the 81-th batch, train loss: 0.5783355832099915:  53%|██████▎     | 80/151 [00:17<00:16,  4.38it/s]Epoch: 17, train for the 81-th batch, train loss: 0.5783355832099915:  54%|██████▍     | 81/151 [00:17<00:15,  4.41it/s]Epoch: 10, train for the 26-th batch, train loss: 0.19149497151374817:  21%|██▎        | 25/119 [00:15<00:57,  1.63it/s]Epoch: 10, train for the 26-th batch, train loss: 0.19149497151374817:  22%|██▍        | 26/119 [00:15<00:56,  1.63it/s]evaluate for the 74-th batch, evaluate loss: 0.4607419967651367:  69%|███████████▋     | 73/106 [00:20<00:09,  3.57it/s]evaluate for the 74-th batch, evaluate loss: 0.4607419967651367:  70%|███████████▊     | 74/106 [00:20<00:08,  3.62it/s]Epoch: 17, train for the 82-th batch, train loss: 0.5666549801826477:  54%|██████▍     | 81/151 [00:17<00:15,  4.41it/s]Epoch: 17, train for the 82-th batch, train loss: 0.5666549801826477:  54%|██████▌     | 82/151 [00:17<00:15,  4.43it/s]Epoch: 17, train for the 83-th batch, train loss: 0.5877870321273804:  54%|██████▌     | 82/151 [00:17<00:15,  4.43it/s]Epoch: 17, train for the 83-th batch, train loss: 0.5877870321273804:  55%|██████▌     | 83/151 [00:17<00:15,  4.45it/s]evaluate for the 75-th batch, evaluate loss: 0.40159761905670166:  70%|███████████▏    | 74/106 [00:20<00:08,  3.62it/s]evaluate for the 75-th batch, evaluate loss: 0.40159761905670166:  71%|███████████▎    | 75/106 [00:20<00:08,  3.55it/s]Epoch: 8, train for the 145-th batch, train loss: 0.5171561241149902:  99%|██████████▊| 144/146 [01:26<00:01,  1.63it/s]Epoch: 8, train for the 145-th batch, train loss: 0.5171561241149902:  99%|██████████▉| 145/146 [01:26<00:00,  1.64it/s]Epoch: 5, train for the 232-th batch, train loss: 0.6424628496170044:  97%|██████████▋| 231/237 [02:18<00:03,  1.67it/s]Epoch: 5, train for the 232-th batch, train loss: 0.6424628496170044:  98%|██████████▊| 232/237 [02:18<00:02,  1.68it/s]Epoch: 10, train for the 27-th batch, train loss: 0.22627608478069305:  22%|██▍        | 26/119 [00:16<00:56,  1.63it/s]Epoch: 10, train for the 27-th batch, train loss: 0.22627608478069305:  23%|██▍        | 27/119 [00:16<00:54,  1.68it/s]evaluate for the 76-th batch, evaluate loss: 0.434964120388031:  71%|████████████▋     | 75/106 [00:20<00:08,  3.55it/s]evaluate for the 76-th batch, evaluate loss: 0.434964120388031:  72%|████████████▉     | 76/106 [00:20<00:08,  3.67it/s]Epoch: 17, train for the 84-th batch, train loss: 0.601712167263031:  55%|███████▏     | 83/151 [00:18<00:15,  4.45it/s]Epoch: 17, train for the 84-th batch, train loss: 0.601712167263031:  56%|███████▏     | 84/151 [00:18<00:17,  3.73it/s]Epoch: 8, train for the 146-th batch, train loss: 0.44310691952705383:  99%|█████████▉| 145/146 [01:27<00:00,  1.64it/s]Epoch: 8, train for the 146-th batch, train loss: 0.44310691952705383: 100%|██████████| 146/146 [01:27<00:00,  1.81it/s]Epoch: 8, train for the 146-th batch, train loss: 0.44310691952705383: 100%|██████████| 146/146 [01:27<00:00,  1.68it/s]
  0%|                                                                                            | 0/38 [00:00<?, ?it/s]evaluate for the 77-th batch, evaluate loss: 0.4027312994003296:  72%|████████████▏    | 76/106 [00:21<00:08,  3.67it/s]evaluate for the 77-th batch, evaluate loss: 0.4027312994003296:  73%|████████████▎    | 77/106 [00:21<00:08,  3.60it/s]Epoch: 17, train for the 85-th batch, train loss: 0.5959811806678772:  56%|██████▋     | 84/151 [00:18<00:17,  3.73it/s]Epoch: 17, train for the 85-th batch, train loss: 0.5959811806678772:  56%|██████▊     | 85/151 [00:18<00:16,  3.93it/s]Epoch: 5, train for the 233-th batch, train loss: 0.6416231393814087:  98%|██████████▊| 232/237 [02:19<00:02,  1.68it/s]Epoch: 5, train for the 233-th batch, train loss: 0.6416231393814087:  98%|██████████▊| 233/237 [02:19<00:02,  1.69it/s]Epoch: 10, train for the 28-th batch, train loss: 0.237858384847641:  23%|██▉          | 27/119 [00:16<00:54,  1.68it/s]Epoch: 10, train for the 28-th batch, train loss: 0.237858384847641:  24%|███          | 28/119 [00:16<00:54,  1.67it/s]evaluate for the 1-th batch, evaluate loss: 0.47656285762786865:   0%|                           | 0/38 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.47656285762786865:   3%|▌                  | 1/38 [00:00<00:11,  3.16it/s]Epoch: 17, train for the 86-th batch, train loss: 0.5748237371444702:  56%|██████▊     | 85/151 [00:18<00:16,  3.93it/s]evaluate for the 78-th batch, evaluate loss: 0.4511045813560486:  73%|████████████▎    | 77/106 [00:21<00:08,  3.60it/s]Epoch: 17, train for the 86-th batch, train loss: 0.5748237371444702:  57%|██████▊     | 86/151 [00:18<00:15,  4.09it/s]evaluate for the 78-th batch, evaluate loss: 0.4511045813560486:  74%|████████████▌    | 78/106 [00:21<00:07,  3.69it/s]evaluate for the 2-th batch, evaluate loss: 0.4868898391723633:   3%|▌                   | 1/38 [00:00<00:11,  3.16it/s]evaluate for the 2-th batch, evaluate loss: 0.4868898391723633:   5%|█                   | 2/38 [00:00<00:10,  3.48it/s]Epoch: 17, train for the 87-th batch, train loss: 0.5749527812004089:  57%|██████▊     | 86/151 [00:18<00:15,  4.09it/s]Epoch: 17, train for the 87-th batch, train loss: 0.5749527812004089:  58%|██████▉     | 87/151 [00:18<00:15,  4.20it/s]evaluate for the 79-th batch, evaluate loss: 0.4844242036342621:  74%|████████████▌    | 78/106 [00:21<00:07,  3.69it/s]evaluate for the 79-th batch, evaluate loss: 0.4844242036342621:  75%|████████████▋    | 79/106 [00:21<00:07,  3.63it/s]evaluate for the 3-th batch, evaluate loss: 0.4836634695529938:   5%|█                   | 2/38 [00:00<00:10,  3.48it/s]evaluate for the 3-th batch, evaluate loss: 0.4836634695529938:   8%|█▌                  | 3/38 [00:00<00:07,  4.76it/s]Epoch: 5, train for the 234-th batch, train loss: 0.6240567564964294:  98%|██████████▊| 233/237 [02:19<00:02,  1.69it/s]Epoch: 5, train for the 234-th batch, train loss: 0.6240567564964294:  99%|██████████▊| 234/237 [02:19<00:01,  1.68it/s]Epoch: 17, train for the 88-th batch, train loss: 0.5972459316253662:  58%|██████▉     | 87/151 [00:19<00:15,  4.20it/s]Epoch: 17, train for the 88-th batch, train loss: 0.5972459316253662:  58%|██████▉     | 88/151 [00:19<00:14,  4.28it/s]evaluate for the 4-th batch, evaluate loss: 0.4687148928642273:   8%|█▌                  | 3/38 [00:00<00:07,  4.76it/s]evaluate for the 4-th batch, evaluate loss: 0.4687148928642273:  11%|██                  | 4/38 [00:00<00:06,  5.26it/s]evaluate for the 80-th batch, evaluate loss: 0.4793175756931305:  75%|████████████▋    | 79/106 [00:22<00:07,  3.63it/s]evaluate for the 80-th batch, evaluate loss: 0.4793175756931305:  75%|████████████▊    | 80/106 [00:22<00:07,  3.65it/s]Epoch: 17, train for the 89-th batch, train loss: 0.5749672055244446:  58%|██████▉     | 88/151 [00:19<00:14,  4.28it/s]Epoch: 17, train for the 89-th batch, train loss: 0.5749672055244446:  59%|███████     | 89/151 [00:19<00:14,  4.34it/s]evaluate for the 5-th batch, evaluate loss: 0.4955596923828125:  11%|██                  | 4/38 [00:01<00:06,  5.26it/s]evaluate for the 5-th batch, evaluate loss: 0.4955596923828125:  13%|██▋                 | 5/38 [00:01<00:06,  5.39it/s]evaluate for the 6-th batch, evaluate loss: 0.485587477684021:  13%|██▊                  | 5/38 [00:01<00:06,  5.39it/s]evaluate for the 6-th batch, evaluate loss: 0.485587477684021:  16%|███▎                 | 6/38 [00:01<00:05,  6.39it/s]evaluate for the 81-th batch, evaluate loss: 0.462534099817276:  75%|█████████████▌    | 80/106 [00:22<00:07,  3.65it/s]evaluate for the 81-th batch, evaluate loss: 0.462534099817276:  76%|█████████████▊    | 81/106 [00:22<00:06,  3.67it/s]evaluate for the 7-th batch, evaluate loss: 0.45069175958633423:  16%|███                | 6/38 [00:01<00:05,  6.39it/s]evaluate for the 7-th batch, evaluate loss: 0.45069175958633423:  18%|███▌               | 7/38 [00:01<00:04,  7.21it/s]Epoch: 17, train for the 90-th batch, train loss: 0.5663763880729675:  59%|███████     | 89/151 [00:19<00:14,  4.34it/s]Epoch: 17, train for the 90-th batch, train loss: 0.5663763880729675:  60%|███████▏    | 90/151 [00:19<00:13,  4.39it/s]Epoch: 10, train for the 29-th batch, train loss: 0.19672898948192596:  24%|██▌        | 28/119 [00:17<00:54,  1.67it/s]Epoch: 10, train for the 29-th batch, train loss: 0.19672898948192596:  24%|██▋        | 29/119 [00:17<01:06,  1.36it/s]Epoch: 5, train for the 235-th batch, train loss: 0.6183146238327026:  99%|██████████▊| 234/237 [02:20<00:01,  1.68it/s]Epoch: 5, train for the 235-th batch, train loss: 0.6183146238327026:  99%|██████████▉| 235/237 [02:20<00:01,  1.68it/s]Epoch: 17, train for the 91-th batch, train loss: 0.4394695460796356:  60%|███████▏    | 90/151 [00:19<00:13,  4.39it/s]Epoch: 17, train for the 91-th batch, train loss: 0.4394695460796356:  60%|███████▏    | 91/151 [00:19<00:13,  4.46it/s]evaluate for the 82-th batch, evaluate loss: 0.4381846487522125:  76%|████████████▉    | 81/106 [00:22<00:06,  3.67it/s]evaluate for the 82-th batch, evaluate loss: 0.4381846487522125:  77%|█████████████▏   | 82/106 [00:22<00:06,  3.57it/s]evaluate for the 8-th batch, evaluate loss: 0.5348610877990723:  18%|███▋                | 7/38 [00:01<00:04,  7.21it/s]evaluate for the 8-th batch, evaluate loss: 0.5348610877990723:  21%|████▏               | 8/38 [00:01<00:05,  5.61it/s]Epoch: 17, train for the 92-th batch, train loss: 0.5542525053024292:  60%|███████▏    | 91/151 [00:20<00:13,  4.46it/s]Epoch: 17, train for the 92-th batch, train loss: 0.5542525053024292:  61%|███████▎    | 92/151 [00:20<00:13,  4.46it/s]evaluate for the 83-th batch, evaluate loss: 0.3914112150669098:  77%|█████████████▏   | 82/106 [00:22<00:06,  3.57it/s]evaluate for the 83-th batch, evaluate loss: 0.3914112150669098:  78%|█████████████▎   | 83/106 [00:22<00:06,  3.71it/s]evaluate for the 9-th batch, evaluate loss: 0.5000890493392944:  21%|████▏               | 8/38 [00:01<00:05,  5.61it/s]evaluate for the 9-th batch, evaluate loss: 0.5000890493392944:  24%|████▋               | 9/38 [00:01<00:06,  4.68it/s]Epoch: 17, train for the 93-th batch, train loss: 0.5329450368881226:  61%|███████▎    | 92/151 [00:20<00:13,  4.46it/s]Epoch: 17, train for the 93-th batch, train loss: 0.5329450368881226:  62%|███████▍    | 93/151 [00:20<00:12,  4.48it/s]Epoch: 10, train for the 30-th batch, train loss: 0.19618409872055054:  24%|██▋        | 29/119 [00:18<01:06,  1.36it/s]Epoch: 10, train for the 30-th batch, train loss: 0.19618409872055054:  25%|██▊        | 30/119 [00:18<01:01,  1.44it/s]Epoch: 5, train for the 236-th batch, train loss: 0.6045569181442261:  99%|██████████▉| 235/237 [02:20<00:01,  1.68it/s]Epoch: 5, train for the 236-th batch, train loss: 0.6045569181442261: 100%|██████████▉| 236/237 [02:20<00:00,  1.69it/s]evaluate for the 84-th batch, evaluate loss: 0.47677725553512573:  78%|████████████▌   | 83/106 [00:23<00:06,  3.71it/s]evaluate for the 84-th batch, evaluate loss: 0.47677725553512573:  79%|████████████▋   | 84/106 [00:23<00:06,  3.59it/s]evaluate for the 10-th batch, evaluate loss: 0.5118404626846313:  24%|████▌              | 9/38 [00:02<00:06,  4.68it/s]evaluate for the 10-th batch, evaluate loss: 0.5118404626846313:  26%|████▋             | 10/38 [00:02<00:06,  4.19it/s]Epoch: 17, train for the 94-th batch, train loss: 0.5481691360473633:  62%|███████▍    | 93/151 [00:20<00:12,  4.48it/s]Epoch: 17, train for the 94-th batch, train loss: 0.5481691360473633:  62%|███████▍    | 94/151 [00:20<00:12,  4.48it/s]evaluate for the 85-th batch, evaluate loss: 0.46353277564048767:  79%|████████████▋   | 84/106 [00:23<00:06,  3.59it/s]evaluate for the 85-th batch, evaluate loss: 0.46353277564048767:  80%|████████████▊   | 85/106 [00:23<00:05,  3.64it/s]Epoch: 17, train for the 95-th batch, train loss: 0.5250841975212097:  62%|███████▍    | 94/151 [00:20<00:12,  4.48it/s]Epoch: 17, train for the 95-th batch, train loss: 0.5250841975212097:  63%|███████▌    | 95/151 [00:20<00:12,  4.48it/s]evaluate for the 11-th batch, evaluate loss: 0.50600665807724:  26%|█████▎              | 10/38 [00:02<00:06,  4.19it/s]evaluate for the 11-th batch, evaluate loss: 0.50600665807724:  29%|█████▊              | 11/38 [00:02<00:06,  4.00it/s]Epoch: 5, train for the 237-th batch, train loss: 0.6021585464477539: 100%|██████████▉| 236/237 [02:21<00:00,  1.69it/s]Epoch: 5, train for the 237-th batch, train loss: 0.6021585464477539: 100%|███████████| 237/237 [02:21<00:00,  1.81it/s]Epoch: 5, train for the 237-th batch, train loss: 0.6021585464477539: 100%|███████████| 237/237 [02:21<00:00,  1.68it/s]
  0%|                                                                                            | 0/66 [00:00<?, ?it/s]Epoch: 10, train for the 31-th batch, train loss: 0.24562138319015503:  25%|██▊        | 30/119 [00:19<01:01,  1.44it/s]Epoch: 10, train for the 31-th batch, train loss: 0.24562138319015503:  26%|██▊        | 31/119 [00:19<00:58,  1.50it/s]evaluate for the 86-th batch, evaluate loss: 0.44207465648651123:  80%|████████████▊   | 85/106 [00:23<00:05,  3.64it/s]evaluate for the 86-th batch, evaluate loss: 0.44207465648651123:  81%|████████████▉   | 86/106 [00:23<00:05,  3.63it/s]Epoch: 17, train for the 96-th batch, train loss: 0.554286777973175:  63%|████████▏    | 95/151 [00:20<00:12,  4.48it/s]Epoch: 17, train for the 96-th batch, train loss: 0.554286777973175:  64%|████████▎    | 96/151 [00:20<00:12,  4.48it/s]evaluate for the 12-th batch, evaluate loss: 0.555745542049408:  29%|█████▌             | 11/38 [00:02<00:06,  4.00it/s]evaluate for the 12-th batch, evaluate loss: 0.555745542049408:  32%|██████             | 12/38 [00:02<00:06,  3.77it/s]evaluate for the 1-th batch, evaluate loss: 0.586699903011322:   0%|                             | 0/66 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.586699903011322:   2%|▎                    | 1/66 [00:00<00:17,  3.67it/s]Epoch: 17, train for the 97-th batch, train loss: 0.5959223508834839:  64%|███████▋    | 96/151 [00:21<00:12,  4.48it/s]Epoch: 17, train for the 97-th batch, train loss: 0.5959223508834839:  64%|███████▋    | 97/151 [00:21<00:12,  4.48it/s]evaluate for the 87-th batch, evaluate loss: 0.4572499692440033:  81%|█████████████▊   | 86/106 [00:23<00:05,  3.63it/s]evaluate for the 87-th batch, evaluate loss: 0.4572499692440033:  82%|█████████████▉   | 87/106 [00:23<00:05,  3.64it/s]evaluate for the 13-th batch, evaluate loss: 0.5396627187728882:  32%|█████▋            | 12/38 [00:02<00:06,  3.77it/s]evaluate for the 13-th batch, evaluate loss: 0.5396627187728882:  34%|██████▏           | 13/38 [00:02<00:06,  3.85it/s]evaluate for the 2-th batch, evaluate loss: 0.5712001919746399:   2%|▎                   | 1/66 [00:00<00:17,  3.67it/s]evaluate for the 2-th batch, evaluate loss: 0.5712001919746399:   3%|▌                   | 2/66 [00:00<00:17,  3.66it/s]Epoch: 17, train for the 98-th batch, train loss: 0.6146253347396851:  64%|███████▋    | 97/151 [00:21<00:12,  4.48it/s]Epoch: 17, train for the 98-th batch, train loss: 0.6146253347396851:  65%|███████▊    | 98/151 [00:21<00:11,  4.49it/s]evaluate for the 88-th batch, evaluate loss: 0.5122601389884949:  82%|█████████████▉   | 87/106 [00:24<00:05,  3.64it/s]evaluate for the 88-th batch, evaluate loss: 0.5122601389884949:  83%|██████████████   | 88/106 [00:24<00:04,  3.65it/s]Epoch: 10, train for the 32-th batch, train loss: 0.21364855766296387:  26%|██▊        | 31/119 [00:19<00:58,  1.50it/s]Epoch: 10, train for the 32-th batch, train loss: 0.21364855766296387:  27%|██▉        | 32/119 [00:19<00:56,  1.55it/s]evaluate for the 14-th batch, evaluate loss: 0.4689734876155853:  34%|██████▏           | 13/38 [00:03<00:06,  3.85it/s]evaluate for the 14-th batch, evaluate loss: 0.4689734876155853:  37%|██████▋           | 14/38 [00:03<00:06,  3.66it/s]evaluate for the 3-th batch, evaluate loss: 0.605405867099762:   3%|▋                    | 2/66 [00:00<00:17,  3.66it/s]evaluate for the 3-th batch, evaluate loss: 0.605405867099762:   5%|▉                    | 3/66 [00:00<00:17,  3.66it/s]Epoch: 17, train for the 99-th batch, train loss: 0.6420868039131165:  65%|███████▊    | 98/151 [00:21<00:11,  4.49it/s]Epoch: 17, train for the 99-th batch, train loss: 0.6420868039131165:  66%|███████▊    | 99/151 [00:21<00:11,  4.49it/s]evaluate for the 89-th batch, evaluate loss: 0.4338447153568268:  83%|██████████████   | 88/106 [00:24<00:04,  3.65it/s]evaluate for the 89-th batch, evaluate loss: 0.4338447153568268:  84%|██████████████▎  | 89/106 [00:24<00:04,  3.65it/s]evaluate for the 15-th batch, evaluate loss: 0.5015528798103333:  37%|██████▋           | 14/38 [00:03<00:06,  3.66it/s]evaluate for the 15-th batch, evaluate loss: 0.5015528798103333:  39%|███████           | 15/38 [00:03<00:06,  3.81it/s]Epoch: 17, train for the 100-th batch, train loss: 0.6617906093597412:  66%|███████▏   | 99/151 [00:21<00:11,  4.49it/s]Epoch: 17, train for the 100-th batch, train loss: 0.6617906093597412:  66%|██████▌   | 100/151 [00:21<00:11,  4.50it/s]evaluate for the 4-th batch, evaluate loss: 0.6041977405548096:   5%|▉                   | 3/66 [00:01<00:17,  3.66it/s]evaluate for the 4-th batch, evaluate loss: 0.6041977405548096:   6%|█▏                  | 4/66 [00:01<00:16,  3.67it/s]evaluate for the 90-th batch, evaluate loss: 0.3679831624031067:  84%|██████████████▎  | 89/106 [00:24<00:04,  3.65it/s]evaluate for the 90-th batch, evaluate loss: 0.3679831624031067:  85%|██████████████▍  | 90/106 [00:24<00:04,  3.66it/s]Epoch: 17, train for the 101-th batch, train loss: 0.6900240778923035:  66%|██████▌   | 100/151 [00:22<00:11,  4.50it/s]Epoch: 17, train for the 101-th batch, train loss: 0.6900240778923035:  67%|██████▋   | 101/151 [00:22<00:11,  4.46it/s]Epoch: 10, train for the 33-th batch, train loss: 0.2006574422121048:  27%|███▏        | 32/119 [00:20<00:56,  1.55it/s]Epoch: 10, train for the 33-th batch, train loss: 0.2006574422121048:  28%|███▎        | 33/119 [00:20<00:54,  1.57it/s]evaluate for the 16-th batch, evaluate loss: 0.5317776203155518:  39%|███████           | 15/38 [00:03<00:06,  3.81it/s]evaluate for the 16-th batch, evaluate loss: 0.5317776203155518:  42%|███████▌          | 16/38 [00:03<00:06,  3.58it/s]evaluate for the 5-th batch, evaluate loss: 0.6251930594444275:   6%|█▏                  | 4/66 [00:01<00:16,  3.67it/s]evaluate for the 5-th batch, evaluate loss: 0.6251930594444275:   8%|█▌                  | 5/66 [00:01<00:16,  3.66it/s]Epoch: 17, train for the 102-th batch, train loss: 0.5891374945640564:  67%|██████▋   | 101/151 [00:22<00:11,  4.46it/s]Epoch: 17, train for the 102-th batch, train loss: 0.5891374945640564:  68%|██████▊   | 102/151 [00:22<00:11,  4.45it/s]evaluate for the 91-th batch, evaluate loss: 0.39494064450263977:  85%|█████████████▌  | 90/106 [00:25<00:04,  3.66it/s]evaluate for the 91-th batch, evaluate loss: 0.39494064450263977:  86%|█████████████▋  | 91/106 [00:25<00:04,  3.66it/s]evaluate for the 17-th batch, evaluate loss: 0.49242979288101196:  42%|███████▏         | 16/38 [00:04<00:06,  3.58it/s]evaluate for the 17-th batch, evaluate loss: 0.49242979288101196:  45%|███████▌         | 17/38 [00:04<00:05,  3.61it/s]evaluate for the 6-th batch, evaluate loss: 0.6475549340248108:   8%|█▌                  | 5/66 [00:01<00:16,  3.66it/s]evaluate for the 6-th batch, evaluate loss: 0.6475549340248108:   9%|█▊                  | 6/66 [00:01<00:16,  3.67it/s]Epoch: 17, train for the 103-th batch, train loss: 0.629266619682312:  68%|███████▍   | 102/151 [00:22<00:11,  4.45it/s]Epoch: 17, train for the 103-th batch, train loss: 0.629266619682312:  68%|███████▌   | 103/151 [00:22<00:10,  4.45it/s]evaluate for the 92-th batch, evaluate loss: 0.46425750851631165:  86%|█████████████▋  | 91/106 [00:25<00:04,  3.66it/s]evaluate for the 92-th batch, evaluate loss: 0.46425750851631165:  87%|█████████████▉  | 92/106 [00:25<00:03,  3.67it/s]evaluate for the 7-th batch, evaluate loss: 0.6121808290481567:   9%|█▊                  | 6/66 [00:01<00:16,  3.67it/s]evaluate for the 7-th batch, evaluate loss: 0.6121808290481567:  11%|██                  | 7/66 [00:01<00:16,  3.66it/s]evaluate for the 18-th batch, evaluate loss: 0.537190318107605:  45%|████████▌          | 17/38 [00:04<00:05,  3.61it/s]evaluate for the 18-th batch, evaluate loss: 0.537190318107605:  47%|█████████          | 18/38 [00:04<00:05,  3.51it/s]Epoch: 10, train for the 34-th batch, train loss: 0.1905050277709961:  28%|███▎        | 33/119 [00:20<00:54,  1.57it/s]Epoch: 10, train for the 34-th batch, train loss: 0.1905050277709961:  29%|███▍        | 34/119 [00:20<00:52,  1.61it/s]Epoch: 17, train for the 104-th batch, train loss: 0.6653332114219666:  68%|██████▊   | 103/151 [00:22<00:10,  4.45it/s]Epoch: 17, train for the 104-th batch, train loss: 0.6653332114219666:  69%|██████▉   | 104/151 [00:22<00:10,  4.46it/s]evaluate for the 93-th batch, evaluate loss: 0.40571510791778564:  87%|█████████████▉  | 92/106 [00:25<00:03,  3.67it/s]evaluate for the 93-th batch, evaluate loss: 0.40571510791778564:  88%|██████████████  | 93/106 [00:25<00:03,  3.67it/s]evaluate for the 8-th batch, evaluate loss: 0.6106944680213928:  11%|██                  | 7/66 [00:02<00:16,  3.66it/s]evaluate for the 8-th batch, evaluate loss: 0.6106944680213928:  12%|██▍                 | 8/66 [00:02<00:15,  3.67it/s]Epoch: 17, train for the 105-th batch, train loss: 0.5454477667808533:  69%|██████▉   | 104/151 [00:22<00:10,  4.46it/s]Epoch: 17, train for the 105-th batch, train loss: 0.5454477667808533:  70%|██████▉   | 105/151 [00:22<00:10,  4.48it/s]evaluate for the 19-th batch, evaluate loss: 0.5007535219192505:  47%|████████▌         | 18/38 [00:04<00:05,  3.51it/s]evaluate for the 19-th batch, evaluate loss: 0.5007535219192505:  50%|█████████         | 19/38 [00:04<00:05,  3.57it/s]evaluate for the 94-th batch, evaluate loss: 0.41516047716140747:  88%|██████████████  | 93/106 [00:25<00:03,  3.67it/s]evaluate for the 94-th batch, evaluate loss: 0.41516047716140747:  89%|██████████████▏ | 94/106 [00:25<00:03,  3.67it/s]Epoch: 17, train for the 106-th batch, train loss: 0.5375144481658936:  70%|██████▉   | 105/151 [00:23<00:10,  4.48it/s]Epoch: 17, train for the 106-th batch, train loss: 0.5375144481658936:  70%|███████   | 106/151 [00:23<00:10,  4.47it/s]evaluate for the 9-th batch, evaluate loss: 0.5579434633255005:  12%|██▍                 | 8/66 [00:02<00:15,  3.67it/s]evaluate for the 9-th batch, evaluate loss: 0.5579434633255005:  14%|██▋                 | 9/66 [00:02<00:15,  3.67it/s]evaluate for the 20-th batch, evaluate loss: 0.48250120878219604:  50%|████████▌        | 19/38 [00:04<00:05,  3.57it/s]evaluate for the 20-th batch, evaluate loss: 0.48250120878219604:  53%|████████▉        | 20/38 [00:04<00:05,  3.51it/s]Epoch: 10, train for the 35-th batch, train loss: 0.1528104990720749:  29%|███▍        | 34/119 [00:21<00:52,  1.61it/s]Epoch: 10, train for the 35-th batch, train loss: 0.1528104990720749:  29%|███▌        | 35/119 [00:21<00:51,  1.63it/s]evaluate for the 95-th batch, evaluate loss: 0.3644045293331146:  89%|███████████████  | 94/106 [00:26<00:03,  3.67it/s]evaluate for the 95-th batch, evaluate loss: 0.3644045293331146:  90%|███████████████▏ | 95/106 [00:26<00:02,  3.67it/s]Epoch: 17, train for the 107-th batch, train loss: 0.5485327243804932:  70%|███████   | 106/151 [00:23<00:10,  4.47it/s]Epoch: 17, train for the 107-th batch, train loss: 0.5485327243804932:  71%|███████   | 107/151 [00:23<00:09,  4.47it/s]evaluate for the 10-th batch, evaluate loss: 0.5799861550331116:  14%|██▌                | 9/66 [00:02<00:15,  3.67it/s]evaluate for the 10-th batch, evaluate loss: 0.5799861550331116:  15%|██▋               | 10/66 [00:02<00:15,  3.66it/s]evaluate for the 21-th batch, evaluate loss: 0.489577978849411:  53%|██████████         | 20/38 [00:05<00:05,  3.51it/s]evaluate for the 21-th batch, evaluate loss: 0.489577978849411:  55%|██████████▌        | 21/38 [00:05<00:04,  3.62it/s]Epoch: 17, train for the 108-th batch, train loss: 0.5630970597267151:  71%|███████   | 107/151 [00:23<00:09,  4.47it/s]Epoch: 17, train for the 108-th batch, train loss: 0.5630970597267151:  72%|███████▏  | 108/151 [00:23<00:09,  4.45it/s]evaluate for the 96-th batch, evaluate loss: 0.4012257754802704:  90%|███████████████▏ | 95/106 [00:26<00:02,  3.67it/s]evaluate for the 96-th batch, evaluate loss: 0.4012257754802704:  91%|███████████████▍ | 96/106 [00:26<00:02,  3.67it/s]evaluate for the 11-th batch, evaluate loss: 0.5749937295913696:  15%|██▋               | 10/66 [00:03<00:15,  3.66it/s]evaluate for the 11-th batch, evaluate loss: 0.5749937295913696:  17%|███               | 11/66 [00:03<00:14,  3.67it/s]evaluate for the 22-th batch, evaluate loss: 0.5132085084915161:  55%|█████████▉        | 21/38 [00:05<00:04,  3.62it/s]evaluate for the 22-th batch, evaluate loss: 0.5132085084915161:  58%|██████████▍       | 22/38 [00:05<00:04,  3.57it/s]Epoch: 17, train for the 109-th batch, train loss: 0.5281423926353455:  72%|███████▏  | 108/151 [00:23<00:09,  4.45it/s]Epoch: 17, train for the 109-th batch, train loss: 0.5281423926353455:  72%|███████▏  | 109/151 [00:23<00:09,  4.44it/s]Epoch: 10, train for the 36-th batch, train loss: 0.1817348450422287:  29%|███▌        | 35/119 [00:22<00:51,  1.63it/s]Epoch: 10, train for the 36-th batch, train loss: 0.1817348450422287:  30%|███▋        | 36/119 [00:22<00:50,  1.64it/s]evaluate for the 97-th batch, evaluate loss: 0.3995792269706726:  91%|███████████████▍ | 96/106 [00:26<00:02,  3.67it/s]evaluate for the 97-th batch, evaluate loss: 0.3995792269706726:  92%|███████████████▌ | 97/106 [00:26<00:02,  3.67it/s]evaluate for the 12-th batch, evaluate loss: 0.6207906603813171:  17%|███               | 11/66 [00:03<00:14,  3.67it/s]evaluate for the 12-th batch, evaluate loss: 0.6207906603813171:  18%|███▎              | 12/66 [00:03<00:14,  3.66it/s]evaluate for the 23-th batch, evaluate loss: 0.514298141002655:  58%|███████████        | 22/38 [00:05<00:04,  3.57it/s]evaluate for the 23-th batch, evaluate loss: 0.514298141002655:  61%|███████████▌       | 23/38 [00:05<00:04,  3.65it/s]Epoch: 17, train for the 110-th batch, train loss: 0.579876184463501:  72%|███████▉   | 109/151 [00:24<00:09,  4.44it/s]Epoch: 17, train for the 110-th batch, train loss: 0.579876184463501:  73%|████████   | 110/151 [00:24<00:09,  4.44it/s]evaluate for the 98-th batch, evaluate loss: 0.44253337383270264:  92%|██████████████▋ | 97/106 [00:26<00:02,  3.67it/s]evaluate for the 98-th batch, evaluate loss: 0.44253337383270264:  92%|██████████████▊ | 98/106 [00:26<00:02,  3.67it/s]Epoch: 17, train for the 111-th batch, train loss: 0.5205013751983643:  73%|███████▎  | 110/151 [00:24<00:09,  4.44it/s]Epoch: 17, train for the 111-th batch, train loss: 0.5205013751983643:  74%|███████▎  | 111/151 [00:24<00:09,  4.44it/s]evaluate for the 13-th batch, evaluate loss: 0.5958165526390076:  18%|███▎              | 12/66 [00:03<00:14,  3.66it/s]evaluate for the 13-th batch, evaluate loss: 0.5958165526390076:  20%|███▌              | 13/66 [00:03<00:14,  3.67it/s]evaluate for the 24-th batch, evaluate loss: 0.4888354241847992:  61%|██████████▉       | 23/38 [00:06<00:04,  3.65it/s]evaluate for the 24-th batch, evaluate loss: 0.4888354241847992:  63%|███████████▎      | 24/38 [00:06<00:03,  3.60it/s]evaluate for the 99-th batch, evaluate loss: 0.4018857777118683:  92%|███████████████▋ | 98/106 [00:27<00:02,  3.67it/s]evaluate for the 99-th batch, evaluate loss: 0.4018857777118683:  93%|███████████████▉ | 99/106 [00:27<00:01,  3.67it/s]Epoch: 10, train for the 37-th batch, train loss: 0.1565725952386856:  30%|███▋        | 36/119 [00:22<00:50,  1.64it/s]Epoch: 10, train for the 37-th batch, train loss: 0.1565725952386856:  31%|███▋        | 37/119 [00:22<00:50,  1.64it/s]Epoch: 17, train for the 112-th batch, train loss: 0.4952777624130249:  74%|███████▎  | 111/151 [00:24<00:09,  4.44it/s]Epoch: 17, train for the 112-th batch, train loss: 0.4952777624130249:  74%|███████▍  | 112/151 [00:24<00:08,  4.45it/s]evaluate for the 14-th batch, evaluate loss: 0.5982396006584167:  20%|███▌              | 13/66 [00:03<00:14,  3.67it/s]evaluate for the 14-th batch, evaluate loss: 0.5982396006584167:  21%|███▊              | 14/66 [00:03<00:14,  3.66it/s]evaluate for the 25-th batch, evaluate loss: 0.4975893199443817:  63%|███████████▎      | 24/38 [00:06<00:03,  3.60it/s]evaluate for the 25-th batch, evaluate loss: 0.4975893199443817:  66%|███████████▊      | 25/38 [00:06<00:03,  3.54it/s]evaluate for the 100-th batch, evaluate loss: 0.36745941638946533:  93%|██████████████ | 99/106 [00:27<00:01,  3.67it/s]evaluate for the 100-th batch, evaluate loss: 0.36745941638946533:  94%|█████████████▏| 100/106 [00:27<00:01,  3.67it/s]Epoch: 17, train for the 113-th batch, train loss: 0.5446824431419373:  74%|███████▍  | 112/151 [00:24<00:08,  4.45it/s]Epoch: 17, train for the 113-th batch, train loss: 0.5446824431419373:  75%|███████▍  | 113/151 [00:24<00:08,  4.45it/s]evaluate for the 15-th batch, evaluate loss: 0.6320992708206177:  21%|███▊              | 14/66 [00:04<00:14,  3.66it/s]evaluate for the 15-th batch, evaluate loss: 0.6320992708206177:  23%|████              | 15/66 [00:04<00:13,  3.67it/s]evaluate for the 26-th batch, evaluate loss: 0.5117555260658264:  66%|███████████▊      | 25/38 [00:06<00:03,  3.54it/s]evaluate for the 26-th batch, evaluate loss: 0.5117555260658264:  68%|████████████▎     | 26/38 [00:06<00:03,  3.57it/s]Epoch: 17, train for the 114-th batch, train loss: 0.5086135268211365:  75%|███████▍  | 113/151 [00:24<00:08,  4.45it/s]Epoch: 17, train for the 114-th batch, train loss: 0.5086135268211365:  75%|███████▌  | 114/151 [00:24<00:08,  4.46it/s]evaluate for the 101-th batch, evaluate loss: 0.4175524115562439:  94%|██████████████▏| 100/106 [00:27<00:01,  3.67it/s]evaluate for the 101-th batch, evaluate loss: 0.4175524115562439:  95%|██████████████▎| 101/106 [00:27<00:01,  3.67it/s]Epoch: 10, train for the 38-th batch, train loss: 0.13369496166706085:  31%|███▍       | 37/119 [00:23<00:50,  1.64it/s]Epoch: 10, train for the 38-th batch, train loss: 0.13369496166706085:  32%|███▌       | 38/119 [00:23<00:49,  1.65it/s]evaluate for the 16-th batch, evaluate loss: 0.5670841336250305:  23%|████              | 15/66 [00:04<00:13,  3.67it/s]evaluate for the 16-th batch, evaluate loss: 0.5670841336250305:  24%|████▎             | 16/66 [00:04<00:13,  3.67it/s]Epoch: 17, train for the 115-th batch, train loss: 0.49016299843788147:  75%|██████▊  | 114/151 [00:25<00:08,  4.46it/s]Epoch: 17, train for the 115-th batch, train loss: 0.49016299843788147:  76%|██████▊  | 115/151 [00:25<00:08,  4.47it/s]evaluate for the 27-th batch, evaluate loss: 0.5371660590171814:  68%|████████████▎     | 26/38 [00:06<00:03,  3.57it/s]evaluate for the 27-th batch, evaluate loss: 0.5371660590171814:  71%|████████████▊     | 27/38 [00:06<00:03,  3.50it/s]evaluate for the 102-th batch, evaluate loss: 0.40640318393707275:  95%|█████████████▎| 101/106 [00:28<00:01,  3.67it/s]evaluate for the 102-th batch, evaluate loss: 0.40640318393707275:  96%|█████████████▍| 102/106 [00:28<00:01,  3.67it/s]evaluate for the 17-th batch, evaluate loss: 0.6040404438972473:  24%|████▎             | 16/66 [00:04<00:13,  3.67it/s]evaluate for the 17-th batch, evaluate loss: 0.6040404438972473:  26%|████▋             | 17/66 [00:04<00:13,  3.67it/s]Epoch: 17, train for the 116-th batch, train loss: 0.47234877943992615:  76%|██████▊  | 115/151 [00:25<00:08,  4.47it/s]Epoch: 17, train for the 116-th batch, train loss: 0.47234877943992615:  77%|██████▉  | 116/151 [00:25<00:07,  4.48it/s]evaluate for the 28-th batch, evaluate loss: 0.5390936136245728:  71%|████████████▊     | 27/38 [00:07<00:03,  3.50it/s]evaluate for the 28-th batch, evaluate loss: 0.5390936136245728:  74%|█████████████▎    | 28/38 [00:07<00:02,  3.65it/s]evaluate for the 103-th batch, evaluate loss: 0.5055298209190369:  96%|██████████████▍| 102/106 [00:28<00:01,  3.67it/s]evaluate for the 103-th batch, evaluate loss: 0.5055298209190369:  97%|██████████████▌| 103/106 [00:28<00:00,  3.66it/s]Epoch: 17, train for the 117-th batch, train loss: 0.4940730929374695:  77%|███████▋  | 116/151 [00:25<00:07,  4.48it/s]Epoch: 17, train for the 117-th batch, train loss: 0.4940730929374695:  77%|███████▋  | 117/151 [00:25<00:07,  4.48it/s]evaluate for the 18-th batch, evaluate loss: 0.6071054935455322:  26%|████▋             | 17/66 [00:04<00:13,  3.67it/s]evaluate for the 18-th batch, evaluate loss: 0.6071054935455322:  27%|████▉             | 18/66 [00:04<00:13,  3.67it/s]Epoch: 10, train for the 39-th batch, train loss: 0.22654248774051666:  32%|███▌       | 38/119 [00:23<00:49,  1.65it/s]Epoch: 10, train for the 39-th batch, train loss: 0.22654248774051666:  33%|███▌       | 39/119 [00:23<00:48,  1.65it/s]evaluate for the 29-th batch, evaluate loss: 0.5140331983566284:  74%|█████████████▎    | 28/38 [00:07<00:02,  3.65it/s]evaluate for the 29-th batch, evaluate loss: 0.5140331983566284:  76%|█████████████▋    | 29/38 [00:07<00:02,  3.54it/s]evaluate for the 104-th batch, evaluate loss: 0.42009127140045166:  97%|█████████████▌| 103/106 [00:28<00:00,  3.66it/s]evaluate for the 104-th batch, evaluate loss: 0.42009127140045166:  98%|█████████████▋| 104/106 [00:28<00:00,  3.66it/s]Epoch: 17, train for the 118-th batch, train loss: 0.48046234250068665:  77%|██████▉  | 117/151 [00:25<00:07,  4.48it/s]Epoch: 17, train for the 118-th batch, train loss: 0.48046234250068665:  78%|███████  | 118/151 [00:25<00:07,  4.48it/s]evaluate for the 19-th batch, evaluate loss: 0.6034906506538391:  27%|████▉             | 18/66 [00:05<00:13,  3.67it/s]evaluate for the 19-th batch, evaluate loss: 0.6034906506538391:  29%|█████▏            | 19/66 [00:05<00:12,  3.66it/s]evaluate for the 30-th batch, evaluate loss: 0.5314911007881165:  76%|█████████████▋    | 29/38 [00:07<00:02,  3.54it/s]evaluate for the 30-th batch, evaluate loss: 0.5314911007881165:  79%|██████████████▏   | 30/38 [00:07<00:02,  3.70it/s]Epoch: 17, train for the 119-th batch, train loss: 0.5160740613937378:  78%|███████▊  | 118/151 [00:26<00:07,  4.48it/s]Epoch: 17, train for the 119-th batch, train loss: 0.5160740613937378:  79%|███████▉  | 119/151 [00:26<00:07,  4.49it/s]evaluate for the 105-th batch, evaluate loss: 0.43784064054489136:  98%|█████████████▋| 104/106 [00:28<00:00,  3.66it/s]evaluate for the 105-th batch, evaluate loss: 0.43784064054489136:  99%|█████████████▊| 105/106 [00:28<00:00,  3.66it/s]evaluate for the 20-th batch, evaluate loss: 0.6274627447128296:  29%|█████▏            | 19/66 [00:05<00:12,  3.66it/s]evaluate for the 20-th batch, evaluate loss: 0.6274627447128296:  30%|█████▍            | 20/66 [00:05<00:12,  3.66it/s]evaluate for the 106-th batch, evaluate loss: 0.5684068202972412:  99%|██████████████▊| 105/106 [00:29<00:00,  3.66it/s]evaluate for the 106-th batch, evaluate loss: 0.5684068202972412: 100%|███████████████| 106/106 [00:29<00:00,  4.17it/s]evaluate for the 106-th batch, evaluate loss: 0.5684068202972412: 100%|███████████████| 106/106 [00:29<00:00,  3.65it/s]
  0%|                                                                                            | 0/78 [00:00<?, ?it/s]Epoch: 17, train for the 120-th batch, train loss: 0.5958157777786255:  79%|███████▉  | 119/151 [00:26<00:07,  4.49it/s]Epoch: 17, train for the 120-th batch, train loss: 0.5958157777786255:  79%|███████▉  | 120/151 [00:26<00:06,  4.49it/s]Epoch: 10, train for the 40-th batch, train loss: 0.18746425211429596:  33%|███▌       | 39/119 [00:24<00:48,  1.65it/s]Epoch: 10, train for the 40-th batch, train loss: 0.18746425211429596:  34%|███▋       | 40/119 [00:24<00:48,  1.65it/s]evaluate for the 31-th batch, evaluate loss: 0.5332469940185547:  79%|██████████████▏   | 30/38 [00:07<00:02,  3.70it/s]evaluate for the 31-th batch, evaluate loss: 0.5332469940185547:  82%|██████████████▋   | 31/38 [00:07<00:01,  3.51it/s]evaluate for the 21-th batch, evaluate loss: 0.6208888292312622:  30%|█████▍            | 20/66 [00:05<00:12,  3.66it/s]evaluate for the 21-th batch, evaluate loss: 0.6208888292312622:  32%|█████▋            | 21/66 [00:05<00:12,  3.67it/s]Epoch: 17, train for the 121-th batch, train loss: 0.4762573838233948:  79%|███████▉  | 120/151 [00:26<00:06,  4.49it/s]Epoch: 17, train for the 121-th batch, train loss: 0.4762573838233948:  80%|████████  | 121/151 [00:26<00:06,  4.48it/s]evaluate for the 1-th batch, evaluate loss: 0.5766502022743225:   0%|                            | 0/78 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.5766502022743225:   1%|▎                   | 1/78 [00:00<00:21,  3.60it/s]evaluate for the 32-th batch, evaluate loss: 0.5013599395751953:  82%|██████████████▋   | 31/38 [00:08<00:01,  3.51it/s]evaluate for the 32-th batch, evaluate loss: 0.5013599395751953:  84%|███████████████▏  | 32/38 [00:08<00:01,  3.57it/s]Epoch: 17, train for the 122-th batch, train loss: 0.5616742372512817:  80%|████████  | 121/151 [00:26<00:06,  4.48it/s]Epoch: 17, train for the 122-th batch, train loss: 0.5616742372512817:  81%|████████  | 122/151 [00:26<00:06,  4.49it/s]evaluate for the 22-th batch, evaluate loss: 0.6138947606086731:  32%|█████▋            | 21/66 [00:05<00:12,  3.67it/s]evaluate for the 22-th batch, evaluate loss: 0.6138947606086731:  33%|██████            | 22/66 [00:05<00:11,  3.70it/s]evaluate for the 2-th batch, evaluate loss: 0.5766398310661316:   1%|▎                   | 1/78 [00:00<00:21,  3.60it/s]evaluate for the 2-th batch, evaluate loss: 0.5766398310661316:   3%|▌                   | 2/78 [00:00<00:21,  3.59it/s]evaluate for the 33-th batch, evaluate loss: 0.4808662235736847:  84%|███████████████▏  | 32/38 [00:08<00:01,  3.57it/s]evaluate for the 33-th batch, evaluate loss: 0.4808662235736847:  87%|███████████████▋  | 33/38 [00:08<00:01,  3.51it/s]Epoch: 10, train for the 41-th batch, train loss: 0.22535106539726257:  34%|███▋       | 40/119 [00:25<00:48,  1.65it/s]Epoch: 10, train for the 41-th batch, train loss: 0.22535106539726257:  34%|███▊       | 41/119 [00:25<00:47,  1.65it/s]Epoch: 17, train for the 123-th batch, train loss: 0.5472802519798279:  81%|████████  | 122/151 [00:26<00:06,  4.49it/s]Epoch: 17, train for the 123-th batch, train loss: 0.5472802519798279:  81%|████████▏ | 123/151 [00:26<00:06,  4.48it/s]evaluate for the 23-th batch, evaluate loss: 0.63226717710495:  33%|██████▋             | 22/66 [00:06<00:11,  3.70it/s]evaluate for the 23-th batch, evaluate loss: 0.63226717710495:  35%|██████▉             | 23/66 [00:06<00:11,  3.72it/s]evaluate for the 3-th batch, evaluate loss: 0.5113856792449951:   3%|▌                   | 2/78 [00:00<00:21,  3.59it/s]evaluate for the 3-th batch, evaluate loss: 0.5113856792449951:   4%|▊                   | 3/78 [00:00<00:20,  3.60it/s]evaluate for the 34-th batch, evaluate loss: 0.5131588578224182:  87%|███████████████▋  | 33/38 [00:08<00:01,  3.51it/s]evaluate for the 34-th batch, evaluate loss: 0.5131588578224182:  89%|████████████████  | 34/38 [00:08<00:01,  3.63it/s]Epoch: 17, train for the 124-th batch, train loss: 0.5412994623184204:  81%|████████▏ | 123/151 [00:27<00:06,  4.48it/s]Epoch: 17, train for the 124-th batch, train loss: 0.5412994623184204:  82%|████████▏ | 124/151 [00:27<00:06,  4.48it/s]evaluate for the 24-th batch, evaluate loss: 0.6220978498458862:  35%|██████▎           | 23/66 [00:06<00:11,  3.72it/s]evaluate for the 24-th batch, evaluate loss: 0.6220978498458862:  36%|██████▌           | 24/66 [00:06<00:11,  3.74it/s]evaluate for the 4-th batch, evaluate loss: 0.6225239634513855:   4%|▊                   | 3/78 [00:01<00:20,  3.60it/s]evaluate for the 4-th batch, evaluate loss: 0.6225239634513855:   5%|█                   | 4/78 [00:01<00:20,  3.59it/s]Epoch: 17, train for the 125-th batch, train loss: 0.578467607498169:  82%|█████████  | 124/151 [00:27<00:06,  4.48it/s]Epoch: 17, train for the 125-th batch, train loss: 0.578467607498169:  83%|█████████  | 125/151 [00:27<00:05,  4.48it/s]evaluate for the 35-th batch, evaluate loss: 0.5392059683799744:  89%|████████████████  | 34/38 [00:09<00:01,  3.63it/s]evaluate for the 35-th batch, evaluate loss: 0.5392059683799744:  92%|████████████████▌ | 35/38 [00:09<00:00,  3.57it/s]Epoch: 10, train for the 42-th batch, train loss: 0.22491544485092163:  34%|███▊       | 41/119 [00:25<00:47,  1.65it/s]Epoch: 10, train for the 42-th batch, train loss: 0.22491544485092163:  35%|███▉       | 42/119 [00:25<00:46,  1.66it/s]evaluate for the 25-th batch, evaluate loss: 0.647996723651886:  36%|██████▉            | 24/66 [00:06<00:11,  3.74it/s]evaluate for the 25-th batch, evaluate loss: 0.647996723651886:  38%|███████▏           | 25/66 [00:06<00:10,  3.75it/s]Epoch: 17, train for the 126-th batch, train loss: 0.5211319923400879:  83%|████████▎ | 125/151 [00:27<00:05,  4.48it/s]Epoch: 17, train for the 126-th batch, train loss: 0.5211319923400879:  83%|████████▎ | 126/151 [00:27<00:05,  4.48it/s]evaluate for the 5-th batch, evaluate loss: 0.5344796776771545:   5%|█                   | 4/78 [00:01<00:20,  3.59it/s]evaluate for the 5-th batch, evaluate loss: 0.5344796776771545:   6%|█▎                  | 5/78 [00:01<00:20,  3.59it/s]evaluate for the 36-th batch, evaluate loss: 0.5630351305007935:  92%|████████████████▌ | 35/38 [00:09<00:00,  3.57it/s]evaluate for the 36-th batch, evaluate loss: 0.5630351305007935:  95%|█████████████████ | 36/38 [00:09<00:00,  3.65it/s]evaluate for the 26-th batch, evaluate loss: 0.6068344712257385:  38%|██████▊           | 25/66 [00:07<00:10,  3.75it/s]evaluate for the 26-th batch, evaluate loss: 0.6068344712257385:  39%|███████           | 26/66 [00:07<00:10,  3.76it/s]Epoch: 17, train for the 127-th batch, train loss: 0.5365230441093445:  83%|████████▎ | 126/151 [00:27<00:05,  4.48it/s]Epoch: 17, train for the 127-th batch, train loss: 0.5365230441093445:  84%|████████▍ | 127/151 [00:27<00:05,  4.48it/s]evaluate for the 6-th batch, evaluate loss: 0.4825330376625061:   6%|█▎                  | 5/78 [00:01<00:20,  3.59it/s]evaluate for the 6-th batch, evaluate loss: 0.4825330376625061:   8%|█▌                  | 6/78 [00:01<00:20,  3.59it/s]evaluate for the 37-th batch, evaluate loss: 0.48936325311660767:  95%|████████████████ | 36/38 [00:09<00:00,  3.65it/s]evaluate for the 37-th batch, evaluate loss: 0.48936325311660767:  97%|████████████████▌| 37/38 [00:09<00:00,  3.62it/s]evaluate for the 27-th batch, evaluate loss: 0.6200475692749023:  39%|███████           | 26/66 [00:07<00:10,  3.76it/s]evaluate for the 27-th batch, evaluate loss: 0.6200475692749023:  41%|███████▎          | 27/66 [00:07<00:10,  3.75it/s]Epoch: 17, train for the 128-th batch, train loss: 0.6206403374671936:  84%|████████▍ | 127/151 [00:28<00:05,  4.48it/s]Epoch: 17, train for the 128-th batch, train loss: 0.6206403374671936:  85%|████████▍ | 128/151 [00:28<00:05,  4.47it/s]Epoch: 10, train for the 43-th batch, train loss: 0.19494903087615967:  35%|███▉       | 42/119 [00:26<00:46,  1.66it/s]Epoch: 10, train for the 43-th batch, train loss: 0.19494903087615967:  36%|███▉       | 43/119 [00:26<00:46,  1.65it/s]evaluate for the 7-th batch, evaluate loss: 0.5996937155723572:   8%|█▌                  | 6/78 [00:01<00:20,  3.59it/s]evaluate for the 7-th batch, evaluate loss: 0.5996937155723572:   9%|█▊                  | 7/78 [00:01<00:19,  3.59it/s]evaluate for the 38-th batch, evaluate loss: 0.4952947497367859:  97%|█████████████████▌| 37/38 [00:09<00:00,  3.62it/s]evaluate for the 38-th batch, evaluate loss: 0.4952947497367859: 100%|██████████████████| 38/38 [00:09<00:00,  3.56it/s]evaluate for the 38-th batch, evaluate loss: 0.4952947497367859: 100%|██████████████████| 38/38 [00:09<00:00,  3.83it/s]
  0%|                                                                                            | 0/20 [00:00<?, ?it/s]Epoch: 17, train for the 129-th batch, train loss: 0.5635004639625549:  85%|████████▍ | 128/151 [00:28<00:05,  4.47it/s]Epoch: 17, train for the 129-th batch, train loss: 0.5635004639625549:  85%|████████▌ | 129/151 [00:28<00:04,  4.48it/s]evaluate for the 28-th batch, evaluate loss: 0.6196764707565308:  41%|███████▎          | 27/66 [00:07<00:10,  3.75it/s]evaluate for the 28-th batch, evaluate loss: 0.6196764707565308:  42%|███████▋          | 28/66 [00:07<00:10,  3.71it/s]evaluate for the 8-th batch, evaluate loss: 0.46882614493370056:   9%|█▋                 | 7/78 [00:02<00:19,  3.59it/s]evaluate for the 8-th batch, evaluate loss: 0.46882614493370056:  10%|█▉                 | 8/78 [00:02<00:19,  3.59it/s]Epoch: 17, train for the 130-th batch, train loss: 0.5548456907272339:  85%|████████▌ | 129/151 [00:28<00:04,  4.48it/s]Epoch: 17, train for the 130-th batch, train loss: 0.5548456907272339:  86%|████████▌ | 130/151 [00:28<00:04,  4.48it/s]evaluate for the 1-th batch, evaluate loss: 0.567993700504303:   0%|                             | 0/20 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.567993700504303:   5%|█                    | 1/20 [00:00<00:05,  3.58it/s]evaluate for the 29-th batch, evaluate loss: 0.6244425177574158:  42%|███████▋          | 28/66 [00:07<00:10,  3.71it/s]evaluate for the 29-th batch, evaluate loss: 0.6244425177574158:  44%|███████▉          | 29/66 [00:07<00:09,  3.71it/s]Epoch: 10, train for the 44-th batch, train loss: 0.17633695900440216:  36%|███▉       | 43/119 [00:26<00:46,  1.65it/s]Epoch: 10, train for the 44-th batch, train loss: 0.17633695900440216:  37%|████       | 44/119 [00:26<00:45,  1.67it/s]Epoch: 17, train for the 131-th batch, train loss: 0.5082431435585022:  86%|████████▌ | 130/151 [00:28<00:04,  4.48it/s]Epoch: 17, train for the 131-th batch, train loss: 0.5082431435585022:  87%|████████▋ | 131/151 [00:28<00:04,  4.48it/s]evaluate for the 9-th batch, evaluate loss: 0.4435030221939087:  10%|██                  | 8/78 [00:02<00:19,  3.59it/s]evaluate for the 9-th batch, evaluate loss: 0.4435030221939087:  12%|██▎                 | 9/78 [00:02<00:19,  3.55it/s]evaluate for the 2-th batch, evaluate loss: 0.6216458082199097:   5%|█                   | 1/20 [00:00<00:05,  3.58it/s]evaluate for the 2-th batch, evaluate loss: 0.6216458082199097:  10%|██                  | 2/20 [00:00<00:05,  3.40it/s]evaluate for the 30-th batch, evaluate loss: 0.6138100028038025:  44%|███████▉          | 29/66 [00:08<00:09,  3.71it/s]evaluate for the 30-th batch, evaluate loss: 0.6138100028038025:  45%|████████▏         | 30/66 [00:08<00:09,  3.73it/s]Epoch: 17, train for the 132-th batch, train loss: 0.5100424289703369:  87%|████████▋ | 131/151 [00:29<00:04,  4.48it/s]Epoch: 17, train for the 132-th batch, train loss: 0.5100424289703369:  87%|████████▋ | 132/151 [00:29<00:04,  4.47it/s]evaluate for the 10-th batch, evaluate loss: 0.534024178981781:  12%|██▎                 | 9/78 [00:02<00:19,  3.55it/s]evaluate for the 10-th batch, evaluate loss: 0.534024178981781:  13%|██▍                | 10/78 [00:02<00:19,  3.56it/s]evaluate for the 3-th batch, evaluate loss: 0.5884917974472046:  10%|██                  | 2/20 [00:00<00:05,  3.40it/s]evaluate for the 3-th batch, evaluate loss: 0.5884917974472046:  15%|███                 | 3/20 [00:00<00:04,  3.53it/s]evaluate for the 31-th batch, evaluate loss: 0.6163478493690491:  45%|████████▏         | 30/66 [00:08<00:09,  3.73it/s]evaluate for the 31-th batch, evaluate loss: 0.6163478493690491:  47%|████████▍         | 31/66 [00:08<00:09,  3.74it/s]Epoch: 17, train for the 133-th batch, train loss: 0.52693772315979:  87%|██████████▍ | 132/151 [00:29<00:04,  4.47it/s]Epoch: 17, train for the 133-th batch, train loss: 0.52693772315979:  88%|██████████▌ | 133/151 [00:29<00:04,  4.48it/s]Epoch: 10, train for the 45-th batch, train loss: 0.17729462683200836:  37%|████       | 44/119 [00:27<00:45,  1.67it/s]Epoch: 10, train for the 45-th batch, train loss: 0.17729462683200836:  38%|████▏      | 45/119 [00:27<00:44,  1.67it/s]evaluate for the 11-th batch, evaluate loss: 0.5435461401939392:  13%|██▎               | 10/78 [00:03<00:19,  3.56it/s]evaluate for the 11-th batch, evaluate loss: 0.5435461401939392:  14%|██▌               | 11/78 [00:03<00:18,  3.56it/s]evaluate for the 32-th batch, evaluate loss: 0.612539529800415:  47%|████████▉          | 31/66 [00:08<00:09,  3.74it/s]evaluate for the 32-th batch, evaluate loss: 0.612539529800415:  48%|█████████▏         | 32/66 [00:08<00:09,  3.76it/s]Epoch: 17, train for the 134-th batch, train loss: 0.5325582027435303:  88%|████████▊ | 133/151 [00:29<00:04,  4.48it/s]Epoch: 17, train for the 134-th batch, train loss: 0.5325582027435303:  89%|████████▊ | 134/151 [00:29<00:03,  4.48it/s]evaluate for the 4-th batch, evaluate loss: 0.5792875289916992:  15%|███                 | 3/20 [00:01<00:04,  3.53it/s]evaluate for the 4-th batch, evaluate loss: 0.5792875289916992:  20%|████                | 4/20 [00:01<00:04,  3.42it/s]evaluate for the 12-th batch, evaluate loss: 0.5654822587966919:  14%|██▌               | 11/78 [00:03<00:18,  3.56it/s]evaluate for the 12-th batch, evaluate loss: 0.5654822587966919:  15%|██▊               | 12/78 [00:03<00:18,  3.57it/s]Epoch: 17, train for the 135-th batch, train loss: 0.5297145247459412:  89%|████████▊ | 134/151 [00:29<00:03,  4.48it/s]Epoch: 17, train for the 135-th batch, train loss: 0.5297145247459412:  89%|████████▉ | 135/151 [00:29<00:03,  4.49it/s]evaluate for the 33-th batch, evaluate loss: 0.6125367879867554:  48%|████████▋         | 32/66 [00:08<00:09,  3.76it/s]evaluate for the 33-th batch, evaluate loss: 0.6125367879867554:  50%|█████████         | 33/66 [00:08<00:08,  3.76it/s]evaluate for the 5-th batch, evaluate loss: 0.59996098279953:  20%|████▍                 | 4/20 [00:01<00:04,  3.42it/s]evaluate for the 5-th batch, evaluate loss: 0.59996098279953:  25%|█████▌                | 5/20 [00:01<00:04,  3.56it/s]Epoch: 17, train for the 136-th batch, train loss: 0.5800300240516663:  89%|████████▉ | 135/151 [00:29<00:03,  4.49it/s]Epoch: 17, train for the 136-th batch, train loss: 0.5800300240516663:  90%|█████████ | 136/151 [00:29<00:03,  4.50it/s]Epoch: 10, train for the 46-th batch, train loss: 0.21250686049461365:  38%|████▏      | 45/119 [00:28<00:44,  1.67it/s]Epoch: 10, train for the 46-th batch, train loss: 0.21250686049461365:  39%|████▎      | 46/119 [00:28<00:43,  1.69it/s]evaluate for the 13-th batch, evaluate loss: 0.5831824541091919:  15%|██▊               | 12/78 [00:03<00:18,  3.57it/s]evaluate for the 13-th batch, evaluate loss: 0.5831824541091919:  17%|███               | 13/78 [00:03<00:18,  3.58it/s]evaluate for the 34-th batch, evaluate loss: 0.6110559105873108:  50%|█████████         | 33/66 [00:09<00:08,  3.76it/s]evaluate for the 34-th batch, evaluate loss: 0.6110559105873108:  52%|█████████▎        | 34/66 [00:09<00:08,  3.76it/s]evaluate for the 6-th batch, evaluate loss: 0.6378430128097534:  25%|█████               | 5/20 [00:01<00:04,  3.56it/s]evaluate for the 6-th batch, evaluate loss: 0.6378430128097534:  30%|██████              | 6/20 [00:01<00:04,  3.45it/s]Epoch: 17, train for the 137-th batch, train loss: 0.624222457408905:  90%|█████████▉ | 136/151 [00:30<00:03,  4.50it/s]Epoch: 17, train for the 137-th batch, train loss: 0.624222457408905:  91%|█████████▉ | 137/151 [00:30<00:03,  4.50it/s]evaluate for the 14-th batch, evaluate loss: 0.41944456100463867:  17%|██▊              | 13/78 [00:03<00:18,  3.58it/s]evaluate for the 14-th batch, evaluate loss: 0.41944456100463867:  18%|███              | 14/78 [00:03<00:17,  3.58it/s]evaluate for the 35-th batch, evaluate loss: 0.6232122182846069:  52%|█████████▎        | 34/66 [00:09<00:08,  3.76it/s]evaluate for the 35-th batch, evaluate loss: 0.6232122182846069:  53%|█████████▌        | 35/66 [00:09<00:08,  3.76it/s]evaluate for the 7-th batch, evaluate loss: 0.6992785334587097:  30%|██████              | 6/20 [00:01<00:04,  3.45it/s]evaluate for the 7-th batch, evaluate loss: 0.6992785334587097:  35%|███████             | 7/20 [00:01<00:03,  3.60it/s]Epoch: 17, train for the 138-th batch, train loss: 0.5780928134918213:  91%|█████████ | 137/151 [00:30<00:03,  4.50it/s]Epoch: 17, train for the 138-th batch, train loss: 0.5780928134918213:  91%|█████████▏| 138/151 [00:30<00:02,  4.49it/s]evaluate for the 15-th batch, evaluate loss: 0.45960769057273865:  18%|███              | 14/78 [00:04<00:17,  3.58it/s]evaluate for the 15-th batch, evaluate loss: 0.45960769057273865:  19%|███▎             | 15/78 [00:04<00:17,  3.59it/s]Epoch: 10, train for the 47-th batch, train loss: 0.252122700214386:  39%|█████        | 46/119 [00:28<00:43,  1.69it/s]Epoch: 10, train for the 47-th batch, train loss: 0.252122700214386:  39%|█████▏       | 47/119 [00:28<00:42,  1.70it/s]evaluate for the 36-th batch, evaluate loss: 0.6462695002555847:  53%|█████████▌        | 35/66 [00:09<00:08,  3.76it/s]evaluate for the 36-th batch, evaluate loss: 0.6462695002555847:  55%|█████████▊        | 36/66 [00:09<00:07,  3.77it/s]Epoch: 17, train for the 139-th batch, train loss: 0.5412119626998901:  91%|█████████▏| 138/151 [00:30<00:02,  4.49it/s]Epoch: 17, train for the 139-th batch, train loss: 0.5412119626998901:  92%|█████████▏| 139/151 [00:30<00:02,  4.49it/s]evaluate for the 8-th batch, evaluate loss: 0.6695614457130432:  35%|███████             | 7/20 [00:02<00:03,  3.60it/s]evaluate for the 8-th batch, evaluate loss: 0.6695614457130432:  40%|████████            | 8/20 [00:02<00:03,  3.48it/s]evaluate for the 16-th batch, evaluate loss: 0.4547393023967743:  19%|███▍              | 15/78 [00:04<00:17,  3.59it/s]evaluate for the 16-th batch, evaluate loss: 0.4547393023967743:  21%|███▋              | 16/78 [00:04<00:17,  3.59it/s]evaluate for the 37-th batch, evaluate loss: 0.6553648114204407:  55%|█████████▊        | 36/66 [00:09<00:07,  3.77it/s]evaluate for the 37-th batch, evaluate loss: 0.6553648114204407:  56%|██████████        | 37/66 [00:09<00:07,  3.77it/s]Epoch: 17, train for the 140-th batch, train loss: 0.5272047519683838:  92%|█████████▏| 139/151 [00:30<00:02,  4.49it/s]Epoch: 17, train for the 140-th batch, train loss: 0.5272047519683838:  93%|█████████▎| 140/151 [00:30<00:02,  4.49it/s]evaluate for the 9-th batch, evaluate loss: 0.6134275794029236:  40%|████████            | 8/20 [00:02<00:03,  3.48it/s]evaluate for the 9-th batch, evaluate loss: 0.6134275794029236:  45%|█████████           | 9/20 [00:02<00:03,  3.63it/s]Epoch: 17, train for the 141-th batch, train loss: 0.5519028902053833:  93%|█████████▎| 140/151 [00:31<00:02,  4.49it/s]Epoch: 17, train for the 141-th batch, train loss: 0.5519028902053833:  93%|█████████▎| 141/151 [00:31<00:02,  4.49it/s]evaluate for the 17-th batch, evaluate loss: 0.4586065709590912:  21%|███▋              | 16/78 [00:04<00:17,  3.59it/s]evaluate for the 17-th batch, evaluate loss: 0.4586065709590912:  22%|███▉              | 17/78 [00:04<00:16,  3.59it/s]evaluate for the 38-th batch, evaluate loss: 0.6109141707420349:  56%|██████████        | 37/66 [00:10<00:07,  3.77it/s]evaluate for the 38-th batch, evaluate loss: 0.6109141707420349:  58%|██████████▎       | 38/66 [00:10<00:07,  3.75it/s]Epoch: 10, train for the 48-th batch, train loss: 0.20765066146850586:  39%|████▎      | 47/119 [00:29<00:42,  1.70it/s]Epoch: 10, train for the 48-th batch, train loss: 0.20765066146850586:  40%|████▍      | 48/119 [00:29<00:41,  1.71it/s]evaluate for the 10-th batch, evaluate loss: 0.6101118922233582:  45%|████████▌          | 9/20 [00:02<00:03,  3.63it/s]evaluate for the 10-th batch, evaluate loss: 0.6101118922233582:  50%|█████████         | 10/20 [00:02<00:02,  3.49it/s]Epoch: 17, train for the 142-th batch, train loss: 0.565945565700531:  93%|██████████▎| 141/151 [00:31<00:02,  4.49it/s]Epoch: 17, train for the 142-th batch, train loss: 0.565945565700531:  94%|██████████▎| 142/151 [00:31<00:02,  4.49it/s]evaluate for the 18-th batch, evaluate loss: 0.462157666683197:  22%|████▏              | 17/78 [00:05<00:16,  3.59it/s]evaluate for the 18-th batch, evaluate loss: 0.462157666683197:  23%|████▍              | 18/78 [00:05<00:16,  3.58it/s]evaluate for the 39-th batch, evaluate loss: 0.6086565852165222:  58%|██████████▎       | 38/66 [00:10<00:07,  3.75it/s]evaluate for the 39-th batch, evaluate loss: 0.6086565852165222:  59%|██████████▋       | 39/66 [00:10<00:07,  3.72it/s]evaluate for the 11-th batch, evaluate loss: 0.6101497411727905:  50%|█████████         | 10/20 [00:03<00:02,  3.49it/s]evaluate for the 11-th batch, evaluate loss: 0.6101497411727905:  55%|█████████▉        | 11/20 [00:03<00:02,  3.67it/s]Epoch: 17, train for the 143-th batch, train loss: 0.4692539572715759:  94%|█████████▍| 142/151 [00:31<00:02,  4.49it/s]Epoch: 17, train for the 143-th batch, train loss: 0.4692539572715759:  95%|█████████▍| 143/151 [00:31<00:01,  4.49it/s]evaluate for the 40-th batch, evaluate loss: 0.6133116483688354:  59%|██████████▋       | 39/66 [00:10<00:07,  3.72it/s]evaluate for the 40-th batch, evaluate loss: 0.6133116483688354:  61%|██████████▉       | 40/66 [00:10<00:07,  3.67it/s]evaluate for the 19-th batch, evaluate loss: 0.5703226327896118:  23%|████▏             | 18/78 [00:05<00:16,  3.58it/s]evaluate for the 19-th batch, evaluate loss: 0.5703226327896118:  24%|████▍             | 19/78 [00:05<00:16,  3.54it/s]Epoch: 10, train for the 49-th batch, train loss: 0.19257409870624542:  40%|████▍      | 48/119 [00:29<00:41,  1.71it/s]Epoch: 10, train for the 49-th batch, train loss: 0.19257409870624542:  41%|████▌      | 49/119 [00:29<00:41,  1.71it/s]Epoch: 17, train for the 144-th batch, train loss: 0.5291187167167664:  95%|█████████▍| 143/151 [00:31<00:01,  4.49it/s]Epoch: 17, train for the 144-th batch, train loss: 0.5291187167167664:  95%|█████████▌| 144/151 [00:31<00:01,  4.49it/s]evaluate for the 12-th batch, evaluate loss: 0.6339386105537415:  55%|█████████▉        | 11/20 [00:03<00:02,  3.67it/s]evaluate for the 12-th batch, evaluate loss: 0.6339386105537415:  60%|██████████▊       | 12/20 [00:03<00:02,  3.50it/s]evaluate for the 41-th batch, evaluate loss: 0.580077588558197:  61%|███████████▌       | 40/66 [00:11<00:07,  3.67it/s]evaluate for the 41-th batch, evaluate loss: 0.580077588558197:  62%|███████████▊       | 41/66 [00:11<00:06,  3.68it/s]evaluate for the 20-th batch, evaluate loss: 0.39904364943504333:  24%|████▏            | 19/78 [00:05<00:16,  3.54it/s]evaluate for the 20-th batch, evaluate loss: 0.39904364943504333:  26%|████▎            | 20/78 [00:05<00:16,  3.55it/s]Epoch: 17, train for the 145-th batch, train loss: 0.5085160732269287:  95%|█████████▌| 144/151 [00:31<00:01,  4.49it/s]Epoch: 17, train for the 145-th batch, train loss: 0.5085160732269287:  96%|█████████▌| 145/151 [00:31<00:01,  4.50it/s]evaluate for the 13-th batch, evaluate loss: 0.6545563340187073:  60%|██████████▊       | 12/20 [00:03<00:02,  3.50it/s]evaluate for the 13-th batch, evaluate loss: 0.6545563340187073:  65%|███████████▋      | 13/20 [00:03<00:01,  3.63it/s]Epoch: 17, train for the 146-th batch, train loss: 0.5352484583854675:  96%|█████████▌| 145/151 [00:32<00:01,  4.50it/s]Epoch: 17, train for the 146-th batch, train loss: 0.5352484583854675:  97%|█████████▋| 146/151 [00:32<00:01,  4.49it/s]evaluate for the 42-th batch, evaluate loss: 0.6154777407646179:  62%|███████████▏      | 41/66 [00:11<00:06,  3.68it/s]evaluate for the 42-th batch, evaluate loss: 0.6154777407646179:  64%|███████████▍      | 42/66 [00:11<00:06,  3.69it/s]evaluate for the 21-th batch, evaluate loss: 0.43450942635536194:  26%|████▎            | 20/78 [00:05<00:16,  3.55it/s]evaluate for the 21-th batch, evaluate loss: 0.43450942635536194:  27%|████▌            | 21/78 [00:05<00:16,  3.56it/s]Epoch: 10, train for the 50-th batch, train loss: 0.17181269824504852:  41%|████▌      | 49/119 [00:30<00:41,  1.71it/s]Epoch: 10, train for the 50-th batch, train loss: 0.17181269824504852:  42%|████▌      | 50/119 [00:30<00:40,  1.69it/s]evaluate for the 14-th batch, evaluate loss: 0.6401046514511108:  65%|███████████▋      | 13/20 [00:03<00:01,  3.63it/s]evaluate for the 14-th batch, evaluate loss: 0.6401046514511108:  70%|████████████▌     | 14/20 [00:03<00:01,  3.45it/s]Epoch: 17, train for the 147-th batch, train loss: 0.5603691339492798:  97%|█████████▋| 146/151 [00:32<00:01,  4.49it/s]Epoch: 17, train for the 147-th batch, train loss: 0.5603691339492798:  97%|█████████▋| 147/151 [00:32<00:00,  4.49it/s]evaluate for the 43-th batch, evaluate loss: 0.5899873971939087:  64%|███████████▍      | 42/66 [00:11<00:06,  3.69it/s]evaluate for the 43-th batch, evaluate loss: 0.5899873971939087:  65%|███████████▋      | 43/66 [00:11<00:06,  3.71it/s]evaluate for the 22-th batch, evaluate loss: 0.5036927461624146:  27%|████▊             | 21/78 [00:06<00:16,  3.56it/s]evaluate for the 22-th batch, evaluate loss: 0.5036927461624146:  28%|█████             | 22/78 [00:06<00:15,  3.57it/s]evaluate for the 15-th batch, evaluate loss: 0.6334011554718018:  70%|████████████▌     | 14/20 [00:04<00:01,  3.45it/s]evaluate for the 15-th batch, evaluate loss: 0.6334011554718018:  75%|█████████████▌    | 15/20 [00:04<00:01,  3.54it/s]Epoch: 17, train for the 148-th batch, train loss: 0.5727352499961853:  97%|█████████▋| 147/151 [00:32<00:00,  4.49it/s]Epoch: 17, train for the 148-th batch, train loss: 0.5727352499961853:  98%|█████████▊| 148/151 [00:32<00:00,  4.48it/s]evaluate for the 44-th batch, evaluate loss: 0.5838369727134705:  65%|███████████▋      | 43/66 [00:11<00:06,  3.71it/s]evaluate for the 44-th batch, evaluate loss: 0.5838369727134705:  67%|████████████      | 44/66 [00:11<00:05,  3.73it/s]evaluate for the 23-th batch, evaluate loss: 0.43421581387519836:  28%|████▊            | 22/78 [00:06<00:15,  3.57it/s]evaluate for the 23-th batch, evaluate loss: 0.43421581387519836:  29%|█████            | 23/78 [00:06<00:15,  3.58it/s]Epoch: 17, train for the 149-th batch, train loss: 0.5124537944793701:  98%|█████████▊| 148/151 [00:32<00:00,  4.48it/s]Epoch: 17, train for the 149-th batch, train loss: 0.5124537944793701:  99%|█████████▊| 149/151 [00:32<00:00,  4.48it/s]Epoch: 10, train for the 51-th batch, train loss: 0.21368083357810974:  42%|████▌      | 50/119 [00:31<00:40,  1.69it/s]Epoch: 10, train for the 51-th batch, train loss: 0.21368083357810974:  43%|████▋      | 51/119 [00:31<00:40,  1.69it/s]evaluate for the 16-th batch, evaluate loss: 0.6044952869415283:  75%|█████████████▌    | 15/20 [00:04<00:01,  3.54it/s]evaluate for the 16-th batch, evaluate loss: 0.6044952869415283:  80%|██████████████▍   | 16/20 [00:04<00:01,  3.38it/s]evaluate for the 45-th batch, evaluate loss: 0.6279565095901489:  67%|████████████      | 44/66 [00:12<00:05,  3.73it/s]evaluate for the 45-th batch, evaluate loss: 0.6279565095901489:  68%|████████████▎     | 45/66 [00:12<00:05,  3.74it/s]evaluate for the 24-th batch, evaluate loss: 0.48147884011268616:  29%|█████            | 23/78 [00:06<00:15,  3.58it/s]evaluate for the 24-th batch, evaluate loss: 0.48147884011268616:  31%|█████▏           | 24/78 [00:06<00:15,  3.58it/s]Epoch: 17, train for the 150-th batch, train loss: 0.5332379937171936:  99%|█████████▊| 149/151 [00:33<00:00,  4.48it/s]Epoch: 17, train for the 150-th batch, train loss: 0.5332379937171936:  99%|█████████▉| 150/151 [00:33<00:00,  4.47it/s]evaluate for the 17-th batch, evaluate loss: 0.5959154963493347:  80%|██████████████▍   | 16/20 [00:04<00:01,  3.38it/s]evaluate for the 17-th batch, evaluate loss: 0.5959154963493347:  85%|███████████████▎  | 17/20 [00:04<00:00,  3.48it/s]Epoch: 17, train for the 151-th batch, train loss: 0.574934184551239:  99%|██████████▉| 150/151 [00:33<00:00,  4.47it/s]Epoch: 17, train for the 151-th batch, train loss: 0.574934184551239: 100%|███████████| 151/151 [00:33<00:00,  4.96it/s]Epoch: 17, train for the 151-th batch, train loss: 0.574934184551239: 100%|███████████| 151/151 [00:33<00:00,  4.55it/s]
  0%|                                                                                            | 0/46 [00:00<?, ?it/s]evaluate for the 46-th batch, evaluate loss: 0.6320174932479858:  68%|████████████▎     | 45/66 [00:12<00:05,  3.74it/s]evaluate for the 46-th batch, evaluate loss: 0.6320174932479858:  70%|████████████▌     | 46/66 [00:12<00:05,  3.75it/s]evaluate for the 1-th batch, evaluate loss: 0.49829962849617004:   0%|                           | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.49829962849617004:   2%|▍                  | 1/46 [00:00<00:04,  9.61it/s]evaluate for the 25-th batch, evaluate loss: 0.5300367474555969:  31%|█████▌            | 24/78 [00:06<00:15,  3.58it/s]evaluate for the 25-th batch, evaluate loss: 0.5300367474555969:  32%|█████▊            | 25/78 [00:06<00:14,  3.58it/s]evaluate for the 2-th batch, evaluate loss: 0.5035097599029541:   2%|▍                   | 1/46 [00:00<00:04,  9.61it/s]evaluate for the 2-th batch, evaluate loss: 0.5035097599029541:   4%|▊                   | 2/46 [00:00<00:04,  9.58it/s]Epoch: 10, train for the 52-th batch, train loss: 0.26883286237716675:  43%|████▋      | 51/119 [00:31<00:40,  1.69it/s]Epoch: 10, train for the 52-th batch, train loss: 0.26883286237716675:  44%|████▊      | 52/119 [00:31<00:39,  1.68it/s]evaluate for the 47-th batch, evaluate loss: 0.642879843711853:  70%|█████████████▏     | 46/66 [00:12<00:05,  3.75it/s]evaluate for the 47-th batch, evaluate loss: 0.642879843711853:  71%|█████████████▌     | 47/66 [00:12<00:05,  3.75it/s]evaluate for the 18-th batch, evaluate loss: 0.6757140755653381:  85%|███████████████▎  | 17/20 [00:05<00:00,  3.48it/s]evaluate for the 18-th batch, evaluate loss: 0.6757140755653381:  90%|████████████████▏ | 18/20 [00:05<00:00,  3.33it/s]evaluate for the 3-th batch, evaluate loss: 0.4818805456161499:   4%|▊                   | 2/46 [00:00<00:04,  9.58it/s]evaluate for the 3-th batch, evaluate loss: 0.4818805456161499:   7%|█▎                  | 3/46 [00:00<00:04,  9.54it/s]evaluate for the 26-th batch, evaluate loss: 0.5046205520629883:  32%|█████▊            | 25/78 [00:07<00:14,  3.58it/s]evaluate for the 26-th batch, evaluate loss: 0.5046205520629883:  33%|██████            | 26/78 [00:07<00:14,  3.58it/s]evaluate for the 4-th batch, evaluate loss: 0.50982266664505:   7%|█▍                    | 3/46 [00:00<00:04,  9.54it/s]evaluate for the 4-th batch, evaluate loss: 0.50982266664505:   9%|█▉                    | 4/46 [00:00<00:04,  9.54it/s]evaluate for the 5-th batch, evaluate loss: 0.47466400265693665:   9%|█▋                 | 4/46 [00:00<00:04,  9.54it/s]evaluate for the 5-th batch, evaluate loss: 0.47466400265693665:  11%|██                 | 5/46 [00:00<00:04,  9.53it/s]evaluate for the 48-th batch, evaluate loss: 0.6325659155845642:  71%|████████████▊     | 47/66 [00:12<00:05,  3.75it/s]evaluate for the 48-th batch, evaluate loss: 0.6325659155845642:  73%|█████████████     | 48/66 [00:12<00:04,  3.76it/s]evaluate for the 19-th batch, evaluate loss: 0.6730555295944214:  90%|████████████████▏ | 18/20 [00:05<00:00,  3.33it/s]evaluate for the 19-th batch, evaluate loss: 0.6730555295944214:  95%|█████████████████ | 19/20 [00:05<00:00,  3.45it/s]evaluate for the 6-th batch, evaluate loss: 0.555687665939331:  11%|██▎                  | 5/46 [00:00<00:04,  9.53it/s]evaluate for the 6-th batch, evaluate loss: 0.555687665939331:  13%|██▋                  | 6/46 [00:00<00:04,  9.55it/s]evaluate for the 20-th batch, evaluate loss: 0.6589359045028687:  95%|█████████████████ | 19/20 [00:05<00:00,  3.45it/s]evaluate for the 20-th batch, evaluate loss: 0.6589359045028687: 100%|██████████████████| 20/20 [00:05<00:00,  3.61it/s]
evaluate for the 27-th batch, evaluate loss: 0.5747232437133789:  33%|██████            | 26/78 [00:07<00:14,  3.58it/s]evaluate for the 27-th batch, evaluate loss: 0.5747232437133789:  35%|██████▏           | 27/78 [00:07<00:14,  3.59it/s]INFO:root:Epoch: 8, learning rate: 0.0001, train loss: 0.5004
INFO:root:train average_precision, 0.8520
INFO:root:train roc_auc, 0.8272
INFO:root:validate loss: 0.5069
INFO:root:validate average_precision, 0.8466
INFO:root:validate roc_auc, 0.8142
INFO:root:new node validate loss: 0.6284
INFO:root:new node validate first_1_average_precision, 0.6419
INFO:root:new node validate first_1_roc_auc, 0.5672
INFO:root:new node validate first_3_average_precision, 0.7018
INFO:root:new node validate first_3_roc_auc, 0.6372
INFO:root:new node validate first_10_average_precision, 0.7477
INFO:root:new node validate first_10_roc_auc, 0.6955
INFO:root:new node validate average_precision, 0.7530
INFO:root:new node validate roc_auc, 0.7147
INFO:root:save model ./saved_models/DyGFormer/ia-escorts-dynamic/DyGFormer_seed0_dygformer-ia-escorts-dynamic-old/DyGFormer_seed0_dygformer-ia-escorts-dynamic-old.pkl
Epoch: 10, train for the 53-th batch, train loss: 0.1807980239391327:  44%|█████▏      | 52/119 [00:32<00:39,  1.68it/s]Epoch: 10, train for the 53-th batch, train loss: 0.1807980239391327:  45%|█████▎      | 53/119 [00:32<00:36,  1.83it/s]evaluate for the 7-th batch, evaluate loss: 0.4835742115974426:  13%|██▌                 | 6/46 [00:00<00:04,  9.55it/s]evaluate for the 7-th batch, evaluate loss: 0.4835742115974426:  15%|███                 | 7/46 [00:00<00:04,  9.58it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]evaluate for the 49-th batch, evaluate loss: 0.6459556221961975:  73%|█████████████     | 48/66 [00:13<00:04,  3.76it/s]evaluate for the 49-th batch, evaluate loss: 0.6459556221961975:  74%|█████████████▎    | 49/66 [00:13<00:04,  3.75it/s]evaluate for the 8-th batch, evaluate loss: 0.559667706489563:  15%|███▏                 | 7/46 [00:00<00:04,  9.58it/s]evaluate for the 8-th batch, evaluate loss: 0.559667706489563:  17%|███▋                 | 8/46 [00:00<00:03,  9.55it/s]evaluate for the 9-th batch, evaluate loss: 0.5256230235099792:  17%|███▍                | 8/46 [00:00<00:03,  9.55it/s]evaluate for the 9-th batch, evaluate loss: 0.5256230235099792:  20%|███▉                | 9/46 [00:00<00:03,  9.57it/s]evaluate for the 28-th batch, evaluate loss: 0.5885187387466431:  35%|██████▏           | 27/78 [00:07<00:14,  3.59it/s]evaluate for the 28-th batch, evaluate loss: 0.5885187387466431:  36%|██████▍           | 28/78 [00:07<00:13,  3.59it/s]evaluate for the 10-th batch, evaluate loss: 0.534648597240448:  20%|███▉                | 9/46 [00:01<00:03,  9.57it/s]evaluate for the 10-th batch, evaluate loss: 0.534648597240448:  22%|████▏              | 10/46 [00:01<00:03,  9.57it/s]evaluate for the 50-th batch, evaluate loss: 0.632280707359314:  74%|██████████████     | 49/66 [00:13<00:04,  3.75it/s]evaluate for the 50-th batch, evaluate loss: 0.632280707359314:  76%|██████████████▍    | 50/66 [00:13<00:04,  3.75it/s]evaluate for the 11-th batch, evaluate loss: 0.5242006778717041:  22%|███▉              | 10/46 [00:01<00:03,  9.57it/s]evaluate for the 11-th batch, evaluate loss: 0.5242006778717041:  24%|████▎             | 11/46 [00:01<00:03,  9.58it/s]evaluate for the 29-th batch, evaluate loss: 0.516470730304718:  36%|██████▊            | 28/78 [00:08<00:13,  3.59it/s]evaluate for the 29-th batch, evaluate loss: 0.516470730304718:  37%|███████            | 29/78 [00:08<00:13,  3.59it/s]Epoch: 10, train for the 54-th batch, train loss: 0.17769570648670197:  45%|████▉      | 53/119 [00:32<00:36,  1.83it/s]Epoch: 10, train for the 54-th batch, train loss: 0.17769570648670197:  45%|████▉      | 54/119 [00:32<00:35,  1.85it/s]evaluate for the 12-th batch, evaluate loss: 0.4733611047267914:  24%|████▎             | 11/46 [00:01<00:03,  9.58it/s]evaluate for the 12-th batch, evaluate loss: 0.4733611047267914:  26%|████▋             | 12/46 [00:01<00:03,  9.55it/s]Epoch: 9, train for the 1-th batch, train loss: 0.7390376329421997:   0%|                       | 0/146 [00:00<?, ?it/s]Epoch: 9, train for the 1-th batch, train loss: 0.7390376329421997:   1%|               | 1/146 [00:00<01:11,  2.03it/s]evaluate for the 13-th batch, evaluate loss: 0.49703449010849:  26%|█████▏              | 12/46 [00:01<00:03,  9.55it/s]evaluate for the 13-th batch, evaluate loss: 0.49703449010849:  28%|█████▋              | 13/46 [00:01<00:03,  9.55it/s]evaluate for the 51-th batch, evaluate loss: 0.6018892526626587:  76%|█████████████▋    | 50/66 [00:13<00:04,  3.75it/s]evaluate for the 51-th batch, evaluate loss: 0.6018892526626587:  77%|█████████████▉    | 51/66 [00:13<00:04,  3.72it/s]evaluate for the 14-th batch, evaluate loss: 0.589111864566803:  28%|█████▎             | 13/46 [00:01<00:03,  9.55it/s]evaluate for the 14-th batch, evaluate loss: 0.589111864566803:  30%|█████▊             | 14/46 [00:01<00:03,  9.58it/s]evaluate for the 30-th batch, evaluate loss: 0.5905933380126953:  37%|██████▋           | 29/78 [00:08<00:13,  3.59it/s]evaluate for the 30-th batch, evaluate loss: 0.5905933380126953:  38%|██████▉           | 30/78 [00:08<00:13,  3.59it/s]evaluate for the 15-th batch, evaluate loss: 0.5413588285446167:  30%|█████▍            | 14/46 [00:01<00:03,  9.58it/s]evaluate for the 15-th batch, evaluate loss: 0.5413588285446167:  33%|█████▊            | 15/46 [00:01<00:03,  9.59it/s]evaluate for the 52-th batch, evaluate loss: 0.6171590089797974:  77%|█████████████▉    | 51/66 [00:14<00:04,  3.72it/s]evaluate for the 52-th batch, evaluate loss: 0.6171590089797974:  79%|██████████████▏   | 52/66 [00:14<00:03,  3.72it/s]evaluate for the 16-th batch, evaluate loss: 0.5743333697319031:  33%|█████▊            | 15/46 [00:01<00:03,  9.59it/s]evaluate for the 16-th batch, evaluate loss: 0.5743333697319031:  35%|██████▎           | 16/46 [00:01<00:03,  9.57it/s]evaluate for the 17-th batch, evaluate loss: 0.4541260004043579:  35%|██████▎           | 16/46 [00:01<00:03,  9.57it/s]evaluate for the 17-th batch, evaluate loss: 0.4541260004043579:  37%|██████▋           | 17/46 [00:01<00:03,  9.58it/s]evaluate for the 31-th batch, evaluate loss: 0.41477516293525696:  38%|██████▌          | 30/78 [00:08<00:13,  3.59it/s]evaluate for the 31-th batch, evaluate loss: 0.41477516293525696:  40%|██████▊          | 31/78 [00:08<00:13,  3.55it/s]Epoch: 9, train for the 2-th batch, train loss: 0.4779694676399231:   1%|               | 1/146 [00:01<01:11,  2.03it/s]Epoch: 9, train for the 2-th batch, train loss: 0.4779694676399231:   1%|▏              | 2/146 [00:01<01:16,  1.89it/s]Epoch: 10, train for the 55-th batch, train loss: 0.2019149661064148:  45%|█████▍      | 54/119 [00:33<00:35,  1.85it/s]Epoch: 10, train for the 55-th batch, train loss: 0.2019149661064148:  46%|█████▌      | 55/119 [00:33<00:36,  1.76it/s]evaluate for the 18-th batch, evaluate loss: 0.5021765232086182:  37%|██████▋           | 17/46 [00:01<00:03,  9.58it/s]evaluate for the 18-th batch, evaluate loss: 0.5021765232086182:  39%|███████           | 18/46 [00:01<00:02,  9.57it/s]evaluate for the 53-th batch, evaluate loss: 0.6197962164878845:  79%|██████████████▏   | 52/66 [00:14<00:03,  3.72it/s]evaluate for the 53-th batch, evaluate loss: 0.6197962164878845:  80%|██████████████▍   | 53/66 [00:14<00:03,  3.74it/s]evaluate for the 19-th batch, evaluate loss: 0.5266490578651428:  39%|███████           | 18/46 [00:01<00:02,  9.57it/s]evaluate for the 19-th batch, evaluate loss: 0.5266490578651428:  41%|███████▍          | 19/46 [00:01<00:02,  9.58it/s]evaluate for the 32-th batch, evaluate loss: 0.49156004190444946:  40%|██████▊          | 31/78 [00:08<00:13,  3.55it/s]evaluate for the 32-th batch, evaluate loss: 0.49156004190444946:  41%|██████▉          | 32/78 [00:08<00:12,  3.56it/s]evaluate for the 20-th batch, evaluate loss: 0.5337369441986084:  41%|███████▍          | 19/46 [00:02<00:02,  9.58it/s]evaluate for the 20-th batch, evaluate loss: 0.5337369441986084:  43%|███████▊          | 20/46 [00:02<00:02,  9.58it/s]evaluate for the 54-th batch, evaluate loss: 0.6510105729103088:  80%|██████████████▍   | 53/66 [00:14<00:03,  3.74it/s]evaluate for the 54-th batch, evaluate loss: 0.6510105729103088:  82%|██████████████▋   | 54/66 [00:14<00:03,  3.75it/s]evaluate for the 21-th batch, evaluate loss: 0.5232098698616028:  43%|███████▊          | 20/46 [00:02<00:02,  9.58it/s]evaluate for the 21-th batch, evaluate loss: 0.5232098698616028:  46%|████████▏         | 21/46 [00:02<00:02,  9.57it/s]evaluate for the 22-th batch, evaluate loss: 0.5209947824478149:  46%|████████▏         | 21/46 [00:02<00:02,  9.57it/s]evaluate for the 22-th batch, evaluate loss: 0.5209947824478149:  48%|████████▌         | 22/46 [00:02<00:02,  9.55it/s]evaluate for the 33-th batch, evaluate loss: 0.5338295102119446:  41%|███████▍          | 32/78 [00:09<00:12,  3.56it/s]evaluate for the 33-th batch, evaluate loss: 0.5338295102119446:  42%|███████▌          | 33/78 [00:09<00:12,  3.57it/s]Epoch: 9, train for the 3-th batch, train loss: 0.3504648506641388:   1%|▏              | 2/146 [00:01<01:16,  1.89it/s]Epoch: 9, train for the 3-th batch, train loss: 0.3504648506641388:   2%|▎              | 3/146 [00:01<01:17,  1.84it/s]evaluate for the 23-th batch, evaluate loss: 0.4709361493587494:  48%|████████▌         | 22/46 [00:02<00:02,  9.55it/s]evaluate for the 23-th batch, evaluate loss: 0.4709361493587494:  50%|█████████         | 23/46 [00:02<00:02,  9.55it/s]evaluate for the 55-th batch, evaluate loss: 0.6008472442626953:  82%|██████████████▋   | 54/66 [00:14<00:03,  3.75it/s]evaluate for the 55-th batch, evaluate loss: 0.6008472442626953:  83%|███████████████   | 55/66 [00:14<00:02,  3.76it/s]Epoch: 10, train for the 56-th batch, train loss: 0.18669389188289642:  46%|█████      | 55/119 [00:33<00:36,  1.76it/s]Epoch: 10, train for the 56-th batch, train loss: 0.18669389188289642:  47%|█████▏     | 56/119 [00:33<00:36,  1.73it/s]evaluate for the 24-th batch, evaluate loss: 0.4779580533504486:  50%|█████████         | 23/46 [00:02<00:02,  9.55it/s]evaluate for the 24-th batch, evaluate loss: 0.4779580533504486:  52%|█████████▍        | 24/46 [00:02<00:02,  9.56it/s]evaluate for the 25-th batch, evaluate loss: 0.5406816601753235:  52%|█████████▍        | 24/46 [00:02<00:02,  9.56it/s]evaluate for the 25-th batch, evaluate loss: 0.5406816601753235:  54%|█████████▊        | 25/46 [00:02<00:02,  9.57it/s]evaluate for the 34-th batch, evaluate loss: 0.43211814761161804:  42%|███████▏         | 33/78 [00:09<00:12,  3.57it/s]evaluate for the 34-th batch, evaluate loss: 0.43211814761161804:  44%|███████▍         | 34/78 [00:09<00:12,  3.58it/s]evaluate for the 56-th batch, evaluate loss: 0.6215235590934753:  83%|███████████████   | 55/66 [00:15<00:02,  3.76it/s]evaluate for the 56-th batch, evaluate loss: 0.6215235590934753:  85%|███████████████▎  | 56/66 [00:15<00:02,  3.76it/s]evaluate for the 26-th batch, evaluate loss: 0.5611308813095093:  54%|█████████▊        | 25/46 [00:02<00:02,  9.57it/s]evaluate for the 26-th batch, evaluate loss: 0.5611308813095093:  57%|██████████▏       | 26/46 [00:02<00:02,  9.56it/s]evaluate for the 27-th batch, evaluate loss: 0.4932482838630676:  57%|██████████▏       | 26/46 [00:02<00:02,  9.56it/s]evaluate for the 27-th batch, evaluate loss: 0.4932482838630676:  59%|██████████▌       | 27/46 [00:02<00:01,  9.54it/s]evaluate for the 35-th batch, evaluate loss: 0.5520170331001282:  44%|███████▊          | 34/78 [00:09<00:12,  3.58it/s]evaluate for the 35-th batch, evaluate loss: 0.5520170331001282:  45%|████████          | 35/78 [00:09<00:12,  3.58it/s]evaluate for the 28-th batch, evaluate loss: 0.5250062942504883:  59%|██████████▌       | 27/46 [00:02<00:01,  9.54it/s]evaluate for the 28-th batch, evaluate loss: 0.5250062942504883:  61%|██████████▉       | 28/46 [00:02<00:01,  9.56it/s]evaluate for the 57-th batch, evaluate loss: 0.6104966402053833:  85%|███████████████▎  | 56/66 [00:15<00:02,  3.76it/s]evaluate for the 57-th batch, evaluate loss: 0.6104966402053833:  86%|███████████████▌  | 57/66 [00:15<00:02,  3.76it/s]Epoch: 9, train for the 4-th batch, train loss: 0.36732399463653564:   2%|▎             | 3/146 [00:02<01:17,  1.84it/s]Epoch: 9, train for the 4-th batch, train loss: 0.36732399463653564:   3%|▍             | 4/146 [00:02<01:18,  1.80it/s]evaluate for the 29-th batch, evaluate loss: 0.49007728695869446:  61%|██████████▎      | 28/46 [00:03<00:01,  9.56it/s]evaluate for the 29-th batch, evaluate loss: 0.49007728695869446:  63%|██████████▋      | 29/46 [00:03<00:01,  9.56it/s]Epoch: 10, train for the 57-th batch, train loss: 0.12018878012895584:  47%|█████▏     | 56/119 [00:34<00:36,  1.73it/s]Epoch: 10, train for the 57-th batch, train loss: 0.12018878012895584:  48%|█████▎     | 57/119 [00:34<00:36,  1.71it/s]evaluate for the 30-th batch, evaluate loss: 0.4991801977157593:  63%|███████████▎      | 29/46 [00:03<00:01,  9.56it/s]evaluate for the 30-th batch, evaluate loss: 0.4991801977157593:  65%|███████████▋      | 30/46 [00:03<00:01,  9.57it/s]evaluate for the 36-th batch, evaluate loss: 0.530105471611023:  45%|████████▌          | 35/78 [00:10<00:12,  3.58it/s]evaluate for the 36-th batch, evaluate loss: 0.530105471611023:  46%|████████▊          | 36/78 [00:10<00:11,  3.58it/s]evaluate for the 58-th batch, evaluate loss: 0.6493465900421143:  86%|███████████████▌  | 57/66 [00:15<00:02,  3.76it/s]evaluate for the 58-th batch, evaluate loss: 0.6493465900421143:  88%|███████████████▊  | 58/66 [00:15<00:02,  3.77it/s]evaluate for the 31-th batch, evaluate loss: 0.518697202205658:  65%|████████████▍      | 30/46 [00:03<00:01,  9.57it/s]evaluate for the 31-th batch, evaluate loss: 0.518697202205658:  67%|████████████▊      | 31/46 [00:03<00:01,  9.56it/s]evaluate for the 32-th batch, evaluate loss: 0.4747917056083679:  67%|████████████▏     | 31/46 [00:03<00:01,  9.56it/s]evaluate for the 32-th batch, evaluate loss: 0.4747917056083679:  70%|████████████▌     | 32/46 [00:03<00:01,  9.54it/s]evaluate for the 33-th batch, evaluate loss: 0.4987122118473053:  70%|████████████▌     | 32/46 [00:03<00:01,  9.54it/s]evaluate for the 33-th batch, evaluate loss: 0.4987122118473053:  72%|████████████▉     | 33/46 [00:03<00:01,  9.57it/s]evaluate for the 37-th batch, evaluate loss: 0.5434267520904541:  46%|████████▎         | 36/78 [00:10<00:11,  3.58it/s]evaluate for the 37-th batch, evaluate loss: 0.5434267520904541:  47%|████████▌         | 37/78 [00:10<00:11,  3.59it/s]Epoch: 9, train for the 5-th batch, train loss: 0.3644002079963684:   3%|▍              | 4/146 [00:02<01:18,  1.80it/s]Epoch: 9, train for the 5-th batch, train loss: 0.3644002079963684:   3%|▌              | 5/146 [00:02<01:16,  1.85it/s]evaluate for the 59-th batch, evaluate loss: 0.6278179883956909:  88%|███████████████▊  | 58/66 [00:15<00:02,  3.77it/s]evaluate for the 59-th batch, evaluate loss: 0.6278179883956909:  89%|████████████████  | 59/66 [00:15<00:01,  3.77it/s]evaluate for the 34-th batch, evaluate loss: 0.47923514246940613:  72%|████████████▏    | 33/46 [00:03<00:01,  9.57it/s]evaluate for the 34-th batch, evaluate loss: 0.47923514246940613:  74%|████████████▌    | 34/46 [00:03<00:01,  9.56it/s]Epoch: 10, train for the 58-th batch, train loss: 0.17035751044750214:  48%|█████▎     | 57/119 [00:35<00:36,  1.71it/s]Epoch: 10, train for the 58-th batch, train loss: 0.17035751044750214:  49%|█████▎     | 58/119 [00:35<00:35,  1.73it/s]evaluate for the 35-th batch, evaluate loss: 0.4807829260826111:  74%|█████████████▎    | 34/46 [00:03<00:01,  9.56it/s]evaluate for the 35-th batch, evaluate loss: 0.4807829260826111:  76%|█████████████▋    | 35/46 [00:03<00:01,  9.57it/s]evaluate for the 38-th batch, evaluate loss: 0.5137680172920227:  47%|████████▌         | 37/78 [00:10<00:11,  3.59it/s]evaluate for the 38-th batch, evaluate loss: 0.5137680172920227:  49%|████████▊         | 38/78 [00:10<00:11,  3.59it/s]evaluate for the 60-th batch, evaluate loss: 0.6459152102470398:  89%|████████████████  | 59/66 [00:16<00:01,  3.77it/s]evaluate for the 60-th batch, evaluate loss: 0.6459152102470398:  91%|████████████████▎ | 60/66 [00:16<00:01,  3.77it/s]evaluate for the 36-th batch, evaluate loss: 0.46539244055747986:  76%|████████████▉    | 35/46 [00:03<00:01,  9.57it/s]evaluate for the 36-th batch, evaluate loss: 0.46539244055747986:  78%|█████████████▎   | 36/46 [00:03<00:01,  9.56it/s]evaluate for the 37-th batch, evaluate loss: 0.5047578811645508:  78%|██████████████    | 36/46 [00:03<00:01,  9.56it/s]evaluate for the 37-th batch, evaluate loss: 0.5047578811645508:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.56it/s]evaluate for the 38-th batch, evaluate loss: 0.5345118045806885:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.56it/s]evaluate for the 38-th batch, evaluate loss: 0.5345118045806885:  83%|██████████████▊   | 38/46 [00:03<00:00,  9.57it/s]evaluate for the 39-th batch, evaluate loss: 0.5473710298538208:  49%|████████▊         | 38/78 [00:10<00:11,  3.59it/s]evaluate for the 39-th batch, evaluate loss: 0.5473710298538208:  50%|█████████         | 39/78 [00:10<00:10,  3.59it/s]evaluate for the 61-th batch, evaluate loss: 0.6404609680175781:  91%|████████████████▎ | 60/66 [00:16<00:01,  3.77it/s]evaluate for the 61-th batch, evaluate loss: 0.6404609680175781:  92%|████████████████▋ | 61/66 [00:16<00:01,  3.75it/s]evaluate for the 39-th batch, evaluate loss: 0.5332444906234741:  83%|██████████████▊   | 38/46 [00:04<00:00,  9.57it/s]evaluate for the 39-th batch, evaluate loss: 0.5332444906234741:  85%|███████████████▎  | 39/46 [00:04<00:00,  9.56it/s]Epoch: 9, train for the 6-th batch, train loss: 0.6416622996330261:   3%|▌              | 5/146 [00:03<01:16,  1.85it/s]Epoch: 9, train for the 6-th batch, train loss: 0.6416622996330261:   4%|▌              | 6/146 [00:03<01:20,  1.74it/s]evaluate for the 40-th batch, evaluate loss: 0.46581825613975525:  85%|██████████████▍  | 39/46 [00:04<00:00,  9.56it/s]evaluate for the 40-th batch, evaluate loss: 0.46581825613975525:  87%|██████████████▊  | 40/46 [00:04<00:00,  9.54it/s]Epoch: 10, train for the 59-th batch, train loss: 0.16703206300735474:  49%|█████▎     | 58/119 [00:35<00:35,  1.73it/s]Epoch: 10, train for the 59-th batch, train loss: 0.16703206300735474:  50%|█████▍     | 59/119 [00:35<00:35,  1.68it/s]evaluate for the 41-th batch, evaluate loss: 0.4850858449935913:  87%|███████████████▋  | 40/46 [00:04<00:00,  9.54it/s]evaluate for the 41-th batch, evaluate loss: 0.4850858449935913:  89%|████████████████  | 41/46 [00:04<00:00,  9.56it/s]evaluate for the 40-th batch, evaluate loss: 0.49738654494285583:  50%|████████▌        | 39/78 [00:11<00:10,  3.59it/s]evaluate for the 40-th batch, evaluate loss: 0.49738654494285583:  51%|████████▋        | 40/78 [00:11<00:10,  3.58it/s]evaluate for the 62-th batch, evaluate loss: 0.6654962301254272:  92%|████████████████▋ | 61/66 [00:16<00:01,  3.75it/s]evaluate for the 62-th batch, evaluate loss: 0.6654962301254272:  94%|████████████████▉ | 62/66 [00:16<00:01,  3.72it/s]evaluate for the 42-th batch, evaluate loss: 0.4720527231693268:  89%|████████████████  | 41/46 [00:04<00:00,  9.56it/s]evaluate for the 42-th batch, evaluate loss: 0.4720527231693268:  91%|████████████████▍ | 42/46 [00:04<00:00,  9.58it/s]evaluate for the 43-th batch, evaluate loss: 0.5348682403564453:  91%|████████████████▍ | 42/46 [00:04<00:00,  9.58it/s]evaluate for the 43-th batch, evaluate loss: 0.5348682403564453:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.57it/s]evaluate for the 63-th batch, evaluate loss: 0.6311773061752319:  94%|████████████████▉ | 62/66 [00:16<00:01,  3.72it/s]evaluate for the 63-th batch, evaluate loss: 0.6311773061752319:  95%|█████████████████▏| 63/66 [00:16<00:00,  3.66it/s]evaluate for the 41-th batch, evaluate loss: 0.5140485167503357:  51%|█████████▏        | 40/78 [00:11<00:10,  3.58it/s]evaluate for the 41-th batch, evaluate loss: 0.5140485167503357:  53%|█████████▍        | 41/78 [00:11<00:10,  3.54it/s]evaluate for the 44-th batch, evaluate loss: 0.5126734972000122:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.57it/s]evaluate for the 44-th batch, evaluate loss: 0.5126734972000122:  96%|█████████████████▏| 44/46 [00:04<00:00,  9.56it/s]Epoch: 9, train for the 7-th batch, train loss: 0.38056960701942444:   4%|▌             | 6/146 [00:03<01:20,  1.74it/s]Epoch: 9, train for the 7-th batch, train loss: 0.38056960701942444:   5%|▋             | 7/146 [00:03<01:20,  1.73it/s]evaluate for the 45-th batch, evaluate loss: 0.48976966738700867:  96%|████████████████▎| 44/46 [00:04<00:00,  9.56it/s]evaluate for the 45-th batch, evaluate loss: 0.48976966738700867:  98%|████████████████▋| 45/46 [00:04<00:00,  9.57it/s]evaluate for the 46-th batch, evaluate loss: 0.5056898593902588:  98%|█████████████████▌| 45/46 [00:04<00:00,  9.57it/s]evaluate for the 46-th batch, evaluate loss: 0.5056898593902588: 100%|██████████████████| 46/46 [00:04<00:00,  9.59it/s]
  0%|                                                                                            | 0/25 [00:00<?, ?it/s]evaluate for the 64-th batch, evaluate loss: 0.6242085695266724:  95%|█████████████████▏| 63/66 [00:17<00:00,  3.66it/s]evaluate for the 64-th batch, evaluate loss: 0.6242085695266724:  97%|█████████████████▍| 64/66 [00:17<00:00,  3.68it/s]evaluate for the 42-th batch, evaluate loss: 0.4871079921722412:  53%|█████████▍        | 41/78 [00:11<00:10,  3.54it/s]evaluate for the 42-th batch, evaluate loss: 0.4871079921722412:  54%|█████████▋        | 42/78 [00:11<00:10,  3.55it/s]Epoch: 10, train for the 60-th batch, train loss: 0.13657331466674805:  50%|█████▍     | 59/119 [00:36<00:35,  1.68it/s]Epoch: 10, train for the 60-th batch, train loss: 0.13657331466674805:  50%|█████▌     | 60/119 [00:36<00:35,  1.68it/s]evaluate for the 1-th batch, evaluate loss: 0.6461313962936401:   0%|                            | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6461313962936401:   4%|▊                   | 1/25 [00:00<00:02,  9.15it/s]evaluate for the 2-th batch, evaluate loss: 0.659293532371521:   4%|▊                    | 1/25 [00:00<00:02,  9.15it/s]evaluate for the 2-th batch, evaluate loss: 0.659293532371521:   8%|█▋                   | 2/25 [00:00<00:02,  9.13it/s]evaluate for the 65-th batch, evaluate loss: 0.5962533950805664:  97%|█████████████████▍| 64/66 [00:17<00:00,  3.68it/s]evaluate for the 65-th batch, evaluate loss: 0.5962533950805664:  98%|█████████████████▋| 65/66 [00:17<00:00,  3.70it/s]evaluate for the 3-th batch, evaluate loss: 0.7007297873497009:   8%|█▌                  | 2/25 [00:00<00:02,  9.13it/s]evaluate for the 3-th batch, evaluate loss: 0.7007297873497009:  12%|██▍                 | 3/25 [00:00<00:02,  9.12it/s]evaluate for the 43-th batch, evaluate loss: 0.5415401458740234:  54%|█████████▋        | 42/78 [00:12<00:10,  3.55it/s]evaluate for the 43-th batch, evaluate loss: 0.5415401458740234:  55%|█████████▉        | 43/78 [00:12<00:09,  3.55it/s]Epoch: 9, train for the 8-th batch, train loss: 0.40262648463249207:   5%|▋             | 7/146 [00:04<01:20,  1.73it/s]Epoch: 9, train for the 8-th batch, train loss: 0.40262648463249207:   5%|▊             | 8/146 [00:04<01:16,  1.81it/s]evaluate for the 4-th batch, evaluate loss: 0.6830950975418091:  12%|██▍                 | 3/25 [00:00<00:02,  9.12it/s]evaluate for the 4-th batch, evaluate loss: 0.6830950975418091:  16%|███▏                | 4/25 [00:00<00:02,  9.14it/s]evaluate for the 66-th batch, evaluate loss: 0.6420629620552063:  98%|█████████████████▋| 65/66 [00:17<00:00,  3.70it/s]evaluate for the 66-th batch, evaluate loss: 0.6420629620552063: 100%|██████████████████| 66/66 [00:17<00:00,  4.05it/s]evaluate for the 66-th batch, evaluate loss: 0.6420629620552063: 100%|██████████████████| 66/66 [00:17<00:00,  3.73it/s]
  0%|                                                                                            | 0/40 [00:00<?, ?it/s]evaluate for the 5-th batch, evaluate loss: 0.6826357841491699:  16%|███▏                | 4/25 [00:00<00:02,  9.14it/s]evaluate for the 5-th batch, evaluate loss: 0.6826357841491699:  20%|████                | 5/25 [00:00<00:02,  9.14it/s]evaluate for the 44-th batch, evaluate loss: 0.45436716079711914:  55%|█████████▎       | 43/78 [00:12<00:09,  3.55it/s]evaluate for the 44-th batch, evaluate loss: 0.45436716079711914:  56%|█████████▌       | 44/78 [00:12<00:09,  3.57it/s]evaluate for the 6-th batch, evaluate loss: 0.7259141206741333:  20%|████                | 5/25 [00:00<00:02,  9.14it/s]evaluate for the 6-th batch, evaluate loss: 0.7259141206741333:  24%|████▊               | 6/25 [00:00<00:02,  9.14it/s]Epoch: 10, train for the 61-th batch, train loss: 0.14663513004779816:  50%|█████▌     | 60/119 [00:36<00:35,  1.68it/s]Epoch: 10, train for the 61-th batch, train loss: 0.14663513004779816:  51%|█████▋     | 61/119 [00:36<00:34,  1.66it/s]evaluate for the 7-th batch, evaluate loss: 0.749622106552124:  24%|█████                | 6/25 [00:00<00:02,  9.14it/s]evaluate for the 7-th batch, evaluate loss: 0.749622106552124:  28%|█████▉               | 7/25 [00:00<00:01,  9.11it/s]evaluate for the 1-th batch, evaluate loss: 0.6483911275863647:   0%|                            | 0/40 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6483911275863647:   2%|▌                   | 1/40 [00:00<00:10,  3.67it/s]Epoch: 9, train for the 9-th batch, train loss: 0.4047207534313202:   5%|▊              | 8/146 [00:04<01:16,  1.81it/s]Epoch: 9, train for the 9-th batch, train loss: 0.4047207534313202:   6%|▉              | 9/146 [00:04<01:12,  1.90it/s]evaluate for the 8-th batch, evaluate loss: 0.7294844388961792:  28%|█████▌              | 7/25 [00:00<00:01,  9.11it/s]evaluate for the 8-th batch, evaluate loss: 0.7294844388961792:  32%|██████▍             | 8/25 [00:00<00:01,  9.10it/s]evaluate for the 45-th batch, evaluate loss: 0.5446975827217102:  56%|██████████▏       | 44/78 [00:12<00:09,  3.57it/s]evaluate for the 45-th batch, evaluate loss: 0.5446975827217102:  58%|██████████▍       | 45/78 [00:12<00:09,  3.61it/s]evaluate for the 9-th batch, evaluate loss: 0.7134288549423218:  32%|██████▍             | 8/25 [00:00<00:01,  9.10it/s]evaluate for the 9-th batch, evaluate loss: 0.7134288549423218:  36%|███████▏            | 9/25 [00:00<00:01,  9.12it/s]evaluate for the 2-th batch, evaluate loss: 0.6877716183662415:   2%|▌                   | 1/40 [00:00<00:10,  3.67it/s]evaluate for the 2-th batch, evaluate loss: 0.6877716183662415:   5%|█                   | 2/40 [00:00<00:10,  3.68it/s]evaluate for the 10-th batch, evaluate loss: 0.7459397912025452:  36%|██████▊            | 9/25 [00:01<00:01,  9.12it/s]evaluate for the 10-th batch, evaluate loss: 0.7459397912025452:  40%|███████▏          | 10/25 [00:01<00:01,  9.11it/s]evaluate for the 46-th batch, evaluate loss: 0.6010255217552185:  58%|██████████▍       | 45/78 [00:12<00:09,  3.61it/s]evaluate for the 46-th batch, evaluate loss: 0.6010255217552185:  59%|██████████▌       | 46/78 [00:12<00:08,  3.63it/s]evaluate for the 11-th batch, evaluate loss: 0.7414330840110779:  40%|███████▏          | 10/25 [00:01<00:01,  9.11it/s]evaluate for the 11-th batch, evaluate loss: 0.7414330840110779:  44%|███████▉          | 11/25 [00:01<00:01,  9.14it/s]evaluate for the 12-th batch, evaluate loss: 0.7047845125198364:  44%|███████▉          | 11/25 [00:01<00:01,  9.14it/s]evaluate for the 12-th batch, evaluate loss: 0.7047845125198364:  48%|████████▋         | 12/25 [00:01<00:01,  9.14it/s]evaluate for the 3-th batch, evaluate loss: 0.6654142737388611:   5%|█                   | 2/40 [00:00<00:10,  3.68it/s]evaluate for the 3-th batch, evaluate loss: 0.6654142737388611:   8%|█▌                  | 3/40 [00:00<00:10,  3.68it/s]Epoch: 9, train for the 10-th batch, train loss: 0.4361321032047272:   6%|▊             | 9/146 [00:05<01:12,  1.90it/s]Epoch: 9, train for the 10-th batch, train loss: 0.4361321032047272:   7%|▉            | 10/146 [00:05<01:09,  1.94it/s]Epoch: 10, train for the 62-th batch, train loss: 0.22618728876113892:  51%|█████▋     | 61/119 [00:37<00:34,  1.66it/s]Epoch: 10, train for the 62-th batch, train loss: 0.22618728876113892:  52%|█████▋     | 62/119 [00:37<00:35,  1.59it/s]evaluate for the 13-th batch, evaluate loss: 0.6700774431228638:  48%|████████▋         | 12/25 [00:01<00:01,  9.14it/s]evaluate for the 13-th batch, evaluate loss: 0.6700774431228638:  52%|█████████▎        | 13/25 [00:01<00:01,  9.16it/s]evaluate for the 47-th batch, evaluate loss: 0.47655341029167175:  59%|██████████       | 46/78 [00:13<00:08,  3.63it/s]evaluate for the 47-th batch, evaluate loss: 0.47655341029167175:  60%|██████████▏      | 47/78 [00:13<00:08,  3.65it/s]evaluate for the 14-th batch, evaluate loss: 0.7654703855514526:  52%|█████████▎        | 13/25 [00:01<00:01,  9.16it/s]evaluate for the 14-th batch, evaluate loss: 0.7654703855514526:  56%|██████████        | 14/25 [00:01<00:01,  9.15it/s]evaluate for the 4-th batch, evaluate loss: 0.6828016638755798:   8%|█▌                  | 3/40 [00:01<00:10,  3.68it/s]evaluate for the 4-th batch, evaluate loss: 0.6828016638755798:  10%|██                  | 4/40 [00:01<00:09,  3.68it/s]evaluate for the 15-th batch, evaluate loss: 0.7377843856811523:  56%|██████████        | 14/25 [00:01<00:01,  9.15it/s]evaluate for the 15-th batch, evaluate loss: 0.7377843856811523:  60%|██████████▊       | 15/25 [00:01<00:01,  9.15it/s]evaluate for the 48-th batch, evaluate loss: 0.5869899988174438:  60%|██████████▊       | 47/78 [00:13<00:08,  3.65it/s]evaluate for the 48-th batch, evaluate loss: 0.5869899988174438:  62%|███████████       | 48/78 [00:13<00:08,  3.66it/s]evaluate for the 16-th batch, evaluate loss: 0.6681742668151855:  60%|██████████▊       | 15/25 [00:01<00:01,  9.15it/s]evaluate for the 16-th batch, evaluate loss: 0.6681742668151855:  64%|███████████▌      | 16/25 [00:01<00:00,  9.15it/s]evaluate for the 17-th batch, evaluate loss: 0.6725786328315735:  64%|███████████▌      | 16/25 [00:01<00:00,  9.15it/s]evaluate for the 17-th batch, evaluate loss: 0.6725786328315735:  68%|████████████▏     | 17/25 [00:01<00:00,  9.16it/s]Epoch: 9, train for the 11-th batch, train loss: 0.5211778879165649:   7%|▉            | 10/146 [00:05<01:09,  1.94it/s]Epoch: 9, train for the 11-th batch, train loss: 0.5211778879165649:   8%|▉            | 11/146 [00:05<01:09,  1.95it/s]evaluate for the 5-th batch, evaluate loss: 0.649914562702179:  10%|██                   | 4/40 [00:01<00:09,  3.68it/s]evaluate for the 5-th batch, evaluate loss: 0.649914562702179:  12%|██▋                  | 5/40 [00:01<00:09,  3.68it/s]evaluate for the 18-th batch, evaluate loss: 0.640052318572998:  68%|████████████▉      | 17/25 [00:01<00:00,  9.16it/s]evaluate for the 18-th batch, evaluate loss: 0.640052318572998:  72%|█████████████▋     | 18/25 [00:01<00:00,  9.15it/s]evaluate for the 49-th batch, evaluate loss: 0.5045527815818787:  62%|███████████       | 48/78 [00:13<00:08,  3.66it/s]evaluate for the 49-th batch, evaluate loss: 0.5045527815818787:  63%|███████████▎      | 49/78 [00:13<00:07,  3.67it/s]Epoch: 10, train for the 63-th batch, train loss: 0.13849979639053345:  52%|█████▋     | 62/119 [00:38<00:35,  1.59it/s]Epoch: 10, train for the 63-th batch, train loss: 0.13849979639053345:  53%|█████▊     | 63/119 [00:38<00:34,  1.61it/s]evaluate for the 19-th batch, evaluate loss: 0.5967038869857788:  72%|████████████▉     | 18/25 [00:02<00:00,  9.15it/s]evaluate for the 19-th batch, evaluate loss: 0.5967038869857788:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.14it/s]evaluate for the 6-th batch, evaluate loss: 0.6705273389816284:  12%|██▌                 | 5/40 [00:01<00:09,  3.68it/s]evaluate for the 6-th batch, evaluate loss: 0.6705273389816284:  15%|███                 | 6/40 [00:01<00:09,  3.68it/s]evaluate for the 20-th batch, evaluate loss: 0.6669816374778748:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.14it/s]evaluate for the 20-th batch, evaluate loss: 0.6669816374778748:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.16it/s]evaluate for the 50-th batch, evaluate loss: 0.5334872603416443:  63%|███████████▎      | 49/78 [00:13<00:07,  3.67it/s]evaluate for the 50-th batch, evaluate loss: 0.5334872603416443:  64%|███████████▌      | 50/78 [00:13<00:07,  3.68it/s]evaluate for the 21-th batch, evaluate loss: 0.7358865737915039:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.16it/s]evaluate for the 21-th batch, evaluate loss: 0.7358865737915039:  84%|███████████████   | 21/25 [00:02<00:00,  9.15it/s]evaluate for the 22-th batch, evaluate loss: 0.6118003129959106:  84%|███████████████   | 21/25 [00:02<00:00,  9.15it/s]evaluate for the 22-th batch, evaluate loss: 0.6118003129959106:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.14it/s]evaluate for the 7-th batch, evaluate loss: 0.6759852170944214:  15%|███                 | 6/40 [00:01<00:09,  3.68it/s]evaluate for the 7-th batch, evaluate loss: 0.6759852170944214:  18%|███▌                | 7/40 [00:01<00:08,  3.69it/s]Epoch: 9, train for the 12-th batch, train loss: 0.4675575792789459:   8%|▉            | 11/146 [00:06<01:09,  1.95it/s]Epoch: 9, train for the 12-th batch, train loss: 0.4675575792789459:   8%|█            | 12/146 [00:06<01:10,  1.89it/s]evaluate for the 23-th batch, evaluate loss: 0.6718443632125854:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.14it/s]evaluate for the 23-th batch, evaluate loss: 0.6718443632125854:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.13it/s]evaluate for the 51-th batch, evaluate loss: 0.5210656523704529:  64%|███████████▌      | 50/78 [00:14<00:07,  3.68it/s]evaluate for the 51-th batch, evaluate loss: 0.5210656523704529:  65%|███████████▊      | 51/78 [00:14<00:07,  3.68it/s]Epoch: 10, train for the 64-th batch, train loss: 0.14796504378318787:  53%|█████▊     | 63/119 [00:38<00:34,  1.61it/s]Epoch: 10, train for the 64-th batch, train loss: 0.14796504378318787:  54%|█████▉     | 64/119 [00:38<00:33,  1.63it/s]evaluate for the 24-th batch, evaluate loss: 0.6647630929946899:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.13it/s]evaluate for the 24-th batch, evaluate loss: 0.6647630929946899:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.12it/s]evaluate for the 8-th batch, evaluate loss: 0.7172138094902039:  18%|███▌                | 7/40 [00:02<00:08,  3.69it/s]evaluate for the 8-th batch, evaluate loss: 0.7172138094902039:  20%|████                | 8/40 [00:02<00:08,  3.68it/s]evaluate for the 25-th batch, evaluate loss: 0.719007670879364:  96%|██████████████████▏| 24/25 [00:02<00:00,  9.12it/s]evaluate for the 25-th batch, evaluate loss: 0.719007670879364: 100%|███████████████████| 25/25 [00:02<00:00,  9.20it/s]
INFO:root:Epoch: 17, learning rate: 0.0001, train loss: 0.5610
INFO:root:train average_precision, 0.8188
INFO:root:train roc_auc, 0.7818
INFO:root:validate loss: 0.5087
INFO:root:validate average_precision, 0.8422
INFO:root:validate roc_auc, 0.8020
INFO:root:new node validate loss: 0.6921
INFO:root:new node validate first_1_average_precision, 0.5925
INFO:root:new node validate first_1_roc_auc, 0.5362
INFO:root:new node validate first_3_average_precision, 0.6748
INFO:root:new node validate first_3_roc_auc, 0.6343
INFO:root:new node validate first_10_average_precision, 0.7444
INFO:root:new node validate first_10_roc_auc, 0.7083
INFO:root:new node validate average_precision, 0.7079
INFO:root:new node validate roc_auc, 0.6549
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]evaluate for the 52-th batch, evaluate loss: 0.49514228105545044:  65%|███████████      | 51/78 [00:14<00:07,  3.68it/s]evaluate for the 52-th batch, evaluate loss: 0.49514228105545044:  67%|███████████▎     | 52/78 [00:14<00:07,  3.68it/s]Epoch: 18, train for the 1-th batch, train loss: 1.26127028465271:   0%|                        | 0/151 [00:00<?, ?it/s]Epoch: 18, train for the 1-th batch, train loss: 1.26127028465271:   1%|                | 1/151 [00:00<00:25,  5.88it/s]evaluate for the 9-th batch, evaluate loss: 0.6880622506141663:  20%|████                | 8/40 [00:02<00:08,  3.68it/s]evaluate for the 9-th batch, evaluate loss: 0.6880622506141663:  22%|████▌               | 9/40 [00:02<00:08,  3.69it/s]Epoch: 9, train for the 13-th batch, train loss: 0.495343416929245:   8%|█▏            | 12/146 [00:07<01:10,  1.89it/s]Epoch: 9, train for the 13-th batch, train loss: 0.495343416929245:   9%|█▏            | 13/146 [00:07<01:13,  1.81it/s]evaluate for the 53-th batch, evaluate loss: 0.46889129281044006:  67%|███████████▎     | 52/78 [00:14<00:07,  3.68it/s]evaluate for the 53-th batch, evaluate loss: 0.46889129281044006:  68%|███████████▌     | 53/78 [00:14<00:06,  3.68it/s]Epoch: 18, train for the 2-th batch, train loss: 1.2519283294677734:   1%|              | 1/151 [00:00<00:25,  5.88it/s]Epoch: 18, train for the 2-th batch, train loss: 1.2519283294677734:   1%|▏             | 2/151 [00:00<00:26,  5.70it/s]Epoch: 10, train for the 65-th batch, train loss: 0.1603342741727829:  54%|██████▍     | 64/119 [00:39<00:33,  1.63it/s]Epoch: 10, train for the 65-th batch, train loss: 0.1603342741727829:  55%|██████▌     | 65/119 [00:39<00:33,  1.63it/s]evaluate for the 10-th batch, evaluate loss: 0.6857075095176697:  22%|████▎              | 9/40 [00:02<00:08,  3.69it/s]evaluate for the 10-th batch, evaluate loss: 0.6857075095176697:  25%|████▌             | 10/40 [00:02<00:08,  3.69it/s]Epoch: 18, train for the 3-th batch, train loss: 0.5498531460762024:   1%|▏             | 2/151 [00:00<00:26,  5.70it/s]Epoch: 18, train for the 3-th batch, train loss: 0.5498531460762024:   2%|▎             | 3/151 [00:00<00:25,  5.87it/s]evaluate for the 54-th batch, evaluate loss: 0.5291620492935181:  68%|████████████▏     | 53/78 [00:15<00:06,  3.68it/s]evaluate for the 54-th batch, evaluate loss: 0.5291620492935181:  69%|████████████▍     | 54/78 [00:15<00:06,  3.69it/s]Epoch: 18, train for the 4-th batch, train loss: 0.6082450151443481:   2%|▎             | 3/151 [00:00<00:25,  5.87it/s]Epoch: 18, train for the 4-th batch, train loss: 0.6082450151443481:   3%|▎             | 4/151 [00:00<00:25,  5.85it/s]evaluate for the 11-th batch, evaluate loss: 0.6829347610473633:  25%|████▌             | 10/40 [00:02<00:08,  3.69it/s]evaluate for the 11-th batch, evaluate loss: 0.6829347610473633:  28%|████▉             | 11/40 [00:02<00:07,  3.68it/s]Epoch: 18, train for the 5-th batch, train loss: 0.5832253694534302:   3%|▎             | 4/151 [00:00<00:25,  5.85it/s]Epoch: 18, train for the 5-th batch, train loss: 0.5832253694534302:   3%|▍             | 5/151 [00:00<00:25,  5.63it/s]evaluate for the 55-th batch, evaluate loss: 0.5503798127174377:  69%|████████████▍     | 54/78 [00:15<00:06,  3.69it/s]evaluate for the 55-th batch, evaluate loss: 0.5503798127174377:  71%|████████████▋     | 55/78 [00:15<00:06,  3.69it/s]Epoch: 9, train for the 14-th batch, train loss: 0.457925409078598:   9%|█▏            | 13/146 [00:07<01:13,  1.81it/s]Epoch: 9, train for the 14-th batch, train loss: 0.457925409078598:  10%|█▎            | 14/146 [00:07<01:15,  1.75it/s]evaluate for the 12-th batch, evaluate loss: 0.7412189245223999:  28%|████▉             | 11/40 [00:03<00:07,  3.68it/s]evaluate for the 12-th batch, evaluate loss: 0.7412189245223999:  30%|█████▍            | 12/40 [00:03<00:07,  3.69it/s]Epoch: 18, train for the 6-th batch, train loss: 0.5754304528236389:   3%|▍             | 5/151 [00:01<00:25,  5.63it/s]Epoch: 18, train for the 6-th batch, train loss: 0.5754304528236389:   4%|▌             | 6/151 [00:01<00:26,  5.51it/s]Epoch: 10, train for the 66-th batch, train loss: 0.1896800994873047:  55%|██████▌     | 65/119 [00:40<00:33,  1.63it/s]Epoch: 10, train for the 66-th batch, train loss: 0.1896800994873047:  55%|██████▋     | 66/119 [00:40<00:32,  1.63it/s]evaluate for the 56-th batch, evaluate loss: 0.5635268092155457:  71%|████████████▋     | 55/78 [00:15<00:06,  3.69it/s]evaluate for the 56-th batch, evaluate loss: 0.5635268092155457:  72%|████████████▉     | 56/78 [00:15<00:05,  3.69it/s]Epoch: 18, train for the 7-th batch, train loss: 0.5515267848968506:   4%|▌             | 6/151 [00:01<00:26,  5.51it/s]Epoch: 18, train for the 7-th batch, train loss: 0.5515267848968506:   5%|▋             | 7/151 [00:01<00:26,  5.37it/s]evaluate for the 13-th batch, evaluate loss: 0.6762243509292603:  30%|█████▍            | 12/40 [00:03<00:07,  3.69it/s]evaluate for the 13-th batch, evaluate loss: 0.6762243509292603:  32%|█████▊            | 13/40 [00:03<00:07,  3.68it/s]evaluate for the 57-th batch, evaluate loss: 0.5470620393753052:  72%|████████████▉     | 56/78 [00:15<00:05,  3.69it/s]evaluate for the 57-th batch, evaluate loss: 0.5470620393753052:  73%|█████████████▏    | 57/78 [00:15<00:05,  3.70it/s]Epoch: 18, train for the 8-th batch, train loss: 0.7866007685661316:   5%|▋             | 7/151 [00:01<00:26,  5.37it/s]Epoch: 18, train for the 8-th batch, train loss: 0.7866007685661316:   5%|▋             | 8/151 [00:01<00:27,  5.29it/s]Epoch: 9, train for the 15-th batch, train loss: 0.4336067736148834:  10%|█▏           | 14/146 [00:08<01:15,  1.75it/s]Epoch: 9, train for the 15-th batch, train loss: 0.4336067736148834:  10%|█▎           | 15/146 [00:08<01:16,  1.71it/s]evaluate for the 14-th batch, evaluate loss: 0.7348638772964478:  32%|█████▊            | 13/40 [00:03<00:07,  3.68it/s]evaluate for the 14-th batch, evaluate loss: 0.7348638772964478:  35%|██████▎           | 14/40 [00:03<00:07,  3.69it/s]Epoch: 18, train for the 9-th batch, train loss: 0.5245743989944458:   5%|▋             | 8/151 [00:01<00:27,  5.29it/s]Epoch: 18, train for the 9-th batch, train loss: 0.5245743989944458:   6%|▊             | 9/151 [00:01<00:27,  5.24it/s]evaluate for the 58-th batch, evaluate loss: 0.6206719279289246:  73%|█████████████▏    | 57/78 [00:16<00:05,  3.70it/s]evaluate for the 58-th batch, evaluate loss: 0.6206719279289246:  74%|█████████████▍    | 58/78 [00:16<00:05,  3.69it/s]Epoch: 10, train for the 67-th batch, train loss: 0.23153647780418396:  55%|██████     | 66/119 [00:40<00:32,  1.63it/s]Epoch: 10, train for the 67-th batch, train loss: 0.23153647780418396:  56%|██████▏    | 67/119 [00:40<00:31,  1.63it/s]Epoch: 18, train for the 10-th batch, train loss: 0.6182605624198914:   6%|▊            | 9/151 [00:01<00:27,  5.24it/s]Epoch: 18, train for the 10-th batch, train loss: 0.6182605624198914:   7%|▊           | 10/151 [00:01<00:27,  5.08it/s]evaluate for the 15-th batch, evaluate loss: 0.7340432405471802:  35%|██████▎           | 14/40 [00:04<00:07,  3.69it/s]evaluate for the 15-th batch, evaluate loss: 0.7340432405471802:  38%|██████▊           | 15/40 [00:04<00:06,  3.69it/s]evaluate for the 59-th batch, evaluate loss: 0.593045711517334:  74%|██████████████▏    | 58/78 [00:16<00:05,  3.69it/s]evaluate for the 59-th batch, evaluate loss: 0.593045711517334:  76%|██████████████▎    | 59/78 [00:16<00:05,  3.70it/s]Epoch: 18, train for the 11-th batch, train loss: 0.5409470796585083:   7%|▊           | 10/151 [00:02<00:27,  5.08it/s]Epoch: 18, train for the 11-th batch, train loss: 0.5409470796585083:   7%|▊           | 11/151 [00:02<00:27,  5.07it/s]evaluate for the 16-th batch, evaluate loss: 0.689654529094696:  38%|███████▏           | 15/40 [00:04<00:06,  3.69it/s]evaluate for the 16-th batch, evaluate loss: 0.689654529094696:  40%|███████▌           | 16/40 [00:04<00:06,  3.68it/s]Epoch: 9, train for the 16-th batch, train loss: 0.45901626348495483:  10%|█▏          | 15/146 [00:08<01:16,  1.71it/s]Epoch: 9, train for the 16-th batch, train loss: 0.45901626348495483:  11%|█▎          | 16/146 [00:08<01:16,  1.70it/s]evaluate for the 60-th batch, evaluate loss: 0.6106666326522827:  76%|█████████████▌    | 59/78 [00:16<00:05,  3.70it/s]evaluate for the 60-th batch, evaluate loss: 0.6106666326522827:  77%|█████████████▊    | 60/78 [00:16<00:04,  3.70it/s]Epoch: 18, train for the 12-th batch, train loss: 0.4894512891769409:   7%|▊           | 11/151 [00:02<00:27,  5.07it/s]Epoch: 18, train for the 12-th batch, train loss: 0.4894512891769409:   8%|▉           | 12/151 [00:02<00:27,  5.06it/s]Epoch: 10, train for the 68-th batch, train loss: 0.1230384036898613:  56%|██████▊     | 67/119 [00:41<00:31,  1.63it/s]Epoch: 10, train for the 68-th batch, train loss: 0.1230384036898613:  57%|██████▊     | 68/119 [00:41<00:31,  1.64it/s]evaluate for the 17-th batch, evaluate loss: 0.6846665143966675:  40%|███████▏          | 16/40 [00:04<00:06,  3.68it/s]evaluate for the 17-th batch, evaluate loss: 0.6846665143966675:  42%|███████▋          | 17/40 [00:04<00:06,  3.68it/s]Epoch: 18, train for the 13-th batch, train loss: 0.5732155442237854:   8%|▉           | 12/151 [00:02<00:27,  5.06it/s]Epoch: 18, train for the 13-th batch, train loss: 0.5732155442237854:   9%|█           | 13/151 [00:02<00:27,  5.02it/s]evaluate for the 61-th batch, evaluate loss: 0.5583695769309998:  77%|█████████████▊    | 60/78 [00:16<00:04,  3.70it/s]evaluate for the 61-th batch, evaluate loss: 0.5583695769309998:  78%|██████████████    | 61/78 [00:16<00:04,  3.70it/s]Epoch: 18, train for the 14-th batch, train loss: 0.4520934224128723:   9%|█           | 13/151 [00:02<00:27,  5.02it/s]Epoch: 18, train for the 14-th batch, train loss: 0.4520934224128723:   9%|█           | 14/151 [00:02<00:27,  5.00it/s]evaluate for the 18-th batch, evaluate loss: 0.6969133019447327:  42%|███████▋          | 17/40 [00:04<00:06,  3.68it/s]evaluate for the 18-th batch, evaluate loss: 0.6969133019447327:  45%|████████          | 18/40 [00:04<00:05,  3.68it/s]Epoch: 9, train for the 17-th batch, train loss: 0.44671323895454407:  11%|█▎          | 16/146 [00:09<01:16,  1.70it/s]Epoch: 9, train for the 17-th batch, train loss: 0.44671323895454407:  12%|█▍          | 17/146 [00:09<01:14,  1.72it/s]evaluate for the 62-th batch, evaluate loss: 0.6259125471115112:  78%|██████████████    | 61/78 [00:17<00:04,  3.70it/s]evaluate for the 62-th batch, evaluate loss: 0.6259125471115112:  79%|██████████████▎   | 62/78 [00:17<00:04,  3.70it/s]Epoch: 18, train for the 15-th batch, train loss: 0.3579879701137543:   9%|█           | 14/151 [00:02<00:27,  5.00it/s]Epoch: 18, train for the 15-th batch, train loss: 0.3579879701137543:  10%|█▏          | 15/151 [00:02<00:27,  4.96it/s]Epoch: 10, train for the 69-th batch, train loss: 0.13306742906570435:  57%|██████▎    | 68/119 [00:41<00:31,  1.64it/s]Epoch: 10, train for the 69-th batch, train loss: 0.13306742906570435:  58%|██████▍    | 69/119 [00:41<00:30,  1.66it/s]evaluate for the 19-th batch, evaluate loss: 0.7098026871681213:  45%|████████          | 18/40 [00:05<00:05,  3.68it/s]evaluate for the 19-th batch, evaluate loss: 0.7098026871681213:  48%|████████▌         | 19/40 [00:05<00:05,  3.69it/s]evaluate for the 63-th batch, evaluate loss: 0.546330451965332:  79%|███████████████    | 62/78 [00:17<00:04,  3.70it/s]evaluate for the 63-th batch, evaluate loss: 0.546330451965332:  81%|███████████████▎   | 63/78 [00:17<00:04,  3.69it/s]Epoch: 18, train for the 16-th batch, train loss: 0.26113730669021606:  10%|█          | 15/151 [00:03<00:27,  4.96it/s]Epoch: 18, train for the 16-th batch, train loss: 0.26113730669021606:  11%|█▏         | 16/151 [00:03<00:27,  4.96it/s]evaluate for the 20-th batch, evaluate loss: 0.745947539806366:  48%|█████████          | 19/40 [00:05<00:05,  3.69it/s]evaluate for the 20-th batch, evaluate loss: 0.745947539806366:  50%|█████████▌         | 20/40 [00:05<00:05,  3.68it/s]Epoch: 18, train for the 17-th batch, train loss: 0.23389019072055817:  11%|█▏         | 16/151 [00:03<00:27,  4.96it/s]Epoch: 18, train for the 17-th batch, train loss: 0.23389019072055817:  11%|█▏         | 17/151 [00:03<00:26,  5.00it/s]evaluate for the 64-th batch, evaluate loss: 0.5763124823570251:  81%|██████████████▌   | 63/78 [00:17<00:04,  3.69it/s]evaluate for the 64-th batch, evaluate loss: 0.5763124823570251:  82%|██████████████▊   | 64/78 [00:17<00:03,  3.70it/s]Epoch: 9, train for the 18-th batch, train loss: 0.4578525125980377:  12%|█▌           | 17/146 [00:10<01:14,  1.72it/s]Epoch: 9, train for the 18-th batch, train loss: 0.4578525125980377:  12%|█▌           | 18/146 [00:10<01:16,  1.68it/s]Epoch: 18, train for the 18-th batch, train loss: 1.1441210508346558:  11%|█▎          | 17/151 [00:03<00:26,  5.00it/s]Epoch: 18, train for the 18-th batch, train loss: 1.1441210508346558:  12%|█▍          | 18/151 [00:03<00:27,  4.91it/s]evaluate for the 21-th batch, evaluate loss: 0.7471634745597839:  50%|█████████         | 20/40 [00:05<00:05,  3.68it/s]evaluate for the 21-th batch, evaluate loss: 0.7471634745597839:  52%|█████████▍        | 21/40 [00:05<00:05,  3.69it/s]Epoch: 10, train for the 70-th batch, train loss: 0.1475021094083786:  58%|██████▉     | 69/119 [00:42<00:30,  1.66it/s]Epoch: 10, train for the 70-th batch, train loss: 0.1475021094083786:  59%|███████     | 70/119 [00:42<00:29,  1.65it/s]evaluate for the 65-th batch, evaluate loss: 0.5183435082435608:  82%|██████████████▊   | 64/78 [00:17<00:03,  3.70it/s]evaluate for the 65-th batch, evaluate loss: 0.5183435082435608:  83%|███████████████   | 65/78 [00:17<00:03,  3.69it/s]Epoch: 18, train for the 19-th batch, train loss: 0.4887169301509857:  12%|█▍          | 18/151 [00:03<00:27,  4.91it/s]Epoch: 18, train for the 19-th batch, train loss: 0.4887169301509857:  13%|█▌          | 19/151 [00:03<00:26,  4.89it/s]evaluate for the 22-th batch, evaluate loss: 0.7584403157234192:  52%|█████████▍        | 21/40 [00:05<00:05,  3.69it/s]evaluate for the 22-th batch, evaluate loss: 0.7584403157234192:  55%|█████████▉        | 22/40 [00:05<00:04,  3.69it/s]evaluate for the 66-th batch, evaluate loss: 0.5524442791938782:  83%|███████████████   | 65/78 [00:18<00:03,  3.69it/s]evaluate for the 66-th batch, evaluate loss: 0.5524442791938782:  85%|███████████████▏  | 66/78 [00:18<00:03,  3.69it/s]Epoch: 18, train for the 20-th batch, train loss: 0.6960585117340088:  13%|█▌          | 19/151 [00:03<00:26,  4.89it/s]Epoch: 18, train for the 20-th batch, train loss: 0.6960585117340088:  13%|█▌          | 20/151 [00:03<00:27,  4.79it/s]Epoch: 9, train for the 19-th batch, train loss: 0.42785269021987915:  12%|█▍          | 18/146 [00:10<01:16,  1.68it/s]Epoch: 9, train for the 19-th batch, train loss: 0.42785269021987915:  13%|█▌          | 19/146 [00:10<01:17,  1.65it/s]evaluate for the 23-th batch, evaluate loss: 0.6766499876976013:  55%|█████████▉        | 22/40 [00:06<00:04,  3.69it/s]evaluate for the 23-th batch, evaluate loss: 0.6766499876976013:  57%|██████████▎       | 23/40 [00:06<00:04,  3.69it/s]Epoch: 18, train for the 21-th batch, train loss: 0.46310049295425415:  13%|█▍         | 20/151 [00:04<00:27,  4.79it/s]Epoch: 18, train for the 21-th batch, train loss: 0.46310049295425415:  14%|█▌         | 21/151 [00:04<00:27,  4.81it/s]Epoch: 10, train for the 71-th batch, train loss: 0.192884162068367:  59%|███████▋     | 70/119 [00:43<00:29,  1.65it/s]Epoch: 10, train for the 71-th batch, train loss: 0.192884162068367:  60%|███████▊     | 71/119 [00:43<00:29,  1.63it/s]evaluate for the 67-th batch, evaluate loss: 0.5633106231689453:  85%|███████████████▏  | 66/78 [00:18<00:03,  3.69it/s]evaluate for the 67-th batch, evaluate loss: 0.5633106231689453:  86%|███████████████▍  | 67/78 [00:18<00:02,  3.69it/s]evaluate for the 24-th batch, evaluate loss: 0.7037695050239563:  57%|██████████▎       | 23/40 [00:06<00:04,  3.69it/s]evaluate for the 24-th batch, evaluate loss: 0.7037695050239563:  60%|██████████▊       | 24/40 [00:06<00:04,  3.69it/s]Epoch: 18, train for the 22-th batch, train loss: 0.633117139339447:  14%|█▊           | 21/151 [00:04<00:27,  4.81it/s]Epoch: 18, train for the 22-th batch, train loss: 0.633117139339447:  15%|█▉           | 22/151 [00:04<00:27,  4.73it/s]evaluate for the 68-th batch, evaluate loss: 0.5410493612289429:  86%|███████████████▍  | 67/78 [00:18<00:02,  3.69it/s]evaluate for the 68-th batch, evaluate loss: 0.5410493612289429:  87%|███████████████▋  | 68/78 [00:18<00:02,  3.69it/s]Epoch: 18, train for the 23-th batch, train loss: 0.5852504968643188:  15%|█▋          | 22/151 [00:04<00:27,  4.73it/s]Epoch: 18, train for the 23-th batch, train loss: 0.5852504968643188:  15%|█▊          | 23/151 [00:04<00:27,  4.73it/s]evaluate for the 25-th batch, evaluate loss: 0.6883796453475952:  60%|██████████▊       | 24/40 [00:06<00:04,  3.69it/s]evaluate for the 25-th batch, evaluate loss: 0.6883796453475952:  62%|███████████▎      | 25/40 [00:06<00:04,  3.68it/s]Epoch: 9, train for the 20-th batch, train loss: 0.4870803654193878:  13%|█▋           | 19/146 [00:11<01:17,  1.65it/s]Epoch: 9, train for the 20-th batch, train loss: 0.4870803654193878:  14%|█▊           | 20/146 [00:11<01:16,  1.64it/s]evaluate for the 69-th batch, evaluate loss: 0.503438413143158:  87%|████████████████▌  | 68/78 [00:19<00:02,  3.69it/s]evaluate for the 69-th batch, evaluate loss: 0.503438413143158:  88%|████████████████▊  | 69/78 [00:19<00:02,  3.69it/s]Epoch: 10, train for the 72-th batch, train loss: 0.1681610494852066:  60%|███████▏    | 71/119 [00:43<00:29,  1.63it/s]Epoch: 10, train for the 72-th batch, train loss: 0.1681610494852066:  61%|███████▎    | 72/119 [00:43<00:28,  1.63it/s]Epoch: 18, train for the 24-th batch, train loss: 0.48925939202308655:  15%|█▋         | 23/151 [00:04<00:27,  4.73it/s]Epoch: 18, train for the 24-th batch, train loss: 0.48925939202308655:  16%|█▋         | 24/151 [00:04<00:26,  4.76it/s]evaluate for the 26-th batch, evaluate loss: 0.6979094743728638:  62%|███████████▎      | 25/40 [00:07<00:04,  3.68it/s]evaluate for the 26-th batch, evaluate loss: 0.6979094743728638:  65%|███████████▋      | 26/40 [00:07<00:03,  3.69it/s]evaluate for the 70-th batch, evaluate loss: 0.5163854956626892:  88%|███████████████▉  | 69/78 [00:19<00:02,  3.69it/s]evaluate for the 70-th batch, evaluate loss: 0.5163854956626892:  90%|████████████████▏ | 70/78 [00:19<00:02,  3.69it/s]Epoch: 18, train for the 25-th batch, train loss: 0.6803390383720398:  16%|█▉          | 24/151 [00:04<00:26,  4.76it/s]Epoch: 18, train for the 25-th batch, train loss: 0.6803390383720398:  17%|█▉          | 25/151 [00:04<00:26,  4.69it/s]evaluate for the 27-th batch, evaluate loss: 0.7105215191841125:  65%|███████████▋      | 26/40 [00:07<00:03,  3.69it/s]evaluate for the 27-th batch, evaluate loss: 0.7105215191841125:  68%|████████████▏     | 27/40 [00:07<00:03,  3.68it/s]Epoch: 18, train for the 26-th batch, train loss: 0.6895060539245605:  17%|█▉          | 25/151 [00:05<00:26,  4.69it/s]Epoch: 18, train for the 26-th batch, train loss: 0.6895060539245605:  17%|██          | 26/151 [00:05<00:26,  4.63it/s]Epoch: 9, train for the 21-th batch, train loss: 0.4339357018470764:  14%|█▊           | 20/146 [00:11<01:16,  1.64it/s]Epoch: 9, train for the 21-th batch, train loss: 0.4339357018470764:  14%|█▊           | 21/146 [00:11<01:16,  1.64it/s]evaluate for the 71-th batch, evaluate loss: 0.4674326479434967:  90%|████████████████▏ | 70/78 [00:19<00:02,  3.69it/s]evaluate for the 71-th batch, evaluate loss: 0.4674326479434967:  91%|████████████████▍ | 71/78 [00:19<00:01,  3.69it/s]Epoch: 10, train for the 73-th batch, train loss: 0.161439448595047:  61%|███████▊     | 72/119 [00:44<00:28,  1.63it/s]Epoch: 10, train for the 73-th batch, train loss: 0.161439448595047:  61%|███████▉     | 73/119 [00:44<00:28,  1.63it/s]evaluate for the 28-th batch, evaluate loss: 0.7100687026977539:  68%|████████████▏     | 27/40 [00:07<00:03,  3.68it/s]evaluate for the 28-th batch, evaluate loss: 0.7100687026977539:  70%|████████████▌     | 28/40 [00:07<00:03,  3.69it/s]Epoch: 18, train for the 27-th batch, train loss: 0.6121463775634766:  17%|██          | 26/151 [00:05<00:26,  4.63it/s]Epoch: 18, train for the 27-th batch, train loss: 0.6121463775634766:  18%|██▏         | 27/151 [00:05<00:29,  4.17it/s]evaluate for the 72-th batch, evaluate loss: 0.5798552632331848:  91%|████████████████▍ | 71/78 [00:19<00:01,  3.69it/s]evaluate for the 72-th batch, evaluate loss: 0.5798552632331848:  92%|████████████████▌ | 72/78 [00:19<00:01,  3.69it/s]evaluate for the 29-th batch, evaluate loss: 0.7384626865386963:  70%|████████████▌     | 28/40 [00:07<00:03,  3.69it/s]evaluate for the 29-th batch, evaluate loss: 0.7384626865386963:  72%|█████████████     | 29/40 [00:07<00:02,  3.68it/s]Epoch: 18, train for the 28-th batch, train loss: 0.6772706508636475:  18%|██▏         | 27/151 [00:05<00:29,  4.17it/s]Epoch: 18, train for the 28-th batch, train loss: 0.6772706508636475:  19%|██▏         | 28/151 [00:05<00:28,  4.25it/s]evaluate for the 73-th batch, evaluate loss: 0.4811919331550598:  92%|████████████████▌ | 72/78 [00:20<00:01,  3.69it/s]evaluate for the 73-th batch, evaluate loss: 0.4811919331550598:  94%|████████████████▊ | 73/78 [00:20<00:01,  3.69it/s]Epoch: 9, train for the 22-th batch, train loss: 0.43360456824302673:  14%|█▋          | 21/146 [00:12<01:16,  1.64it/s]Epoch: 9, train for the 22-th batch, train loss: 0.43360456824302673:  15%|█▊          | 22/146 [00:12<01:15,  1.64it/s]Epoch: 18, train for the 29-th batch, train loss: 0.692595899105072:  19%|██▍          | 28/151 [00:05<00:28,  4.25it/s]Epoch: 18, train for the 29-th batch, train loss: 0.692595899105072:  19%|██▍          | 29/151 [00:05<00:28,  4.32it/s]evaluate for the 30-th batch, evaluate loss: 0.7282165288925171:  72%|█████████████     | 29/40 [00:08<00:02,  3.68it/s]evaluate for the 30-th batch, evaluate loss: 0.7282165288925171:  75%|█████████████▌    | 30/40 [00:08<00:02,  3.68it/s]Epoch: 10, train for the 74-th batch, train loss: 0.21793033182621002:  61%|██████▋    | 73/119 [00:44<00:28,  1.63it/s]Epoch: 10, train for the 74-th batch, train loss: 0.21793033182621002:  62%|██████▊    | 74/119 [00:44<00:27,  1.63it/s]evaluate for the 74-th batch, evaluate loss: 0.5394481420516968:  94%|████████████████▊ | 73/78 [00:20<00:01,  3.69it/s]evaluate for the 74-th batch, evaluate loss: 0.5394481420516968:  95%|█████████████████ | 74/78 [00:20<00:01,  3.69it/s]Epoch: 18, train for the 30-th batch, train loss: 0.6494828462600708:  19%|██▎         | 29/151 [00:06<00:28,  4.32it/s]Epoch: 18, train for the 30-th batch, train loss: 0.6494828462600708:  20%|██▍         | 30/151 [00:06<00:27,  4.40it/s]evaluate for the 31-th batch, evaluate loss: 0.6883636116981506:  75%|█████████████▌    | 30/40 [00:08<00:02,  3.68it/s]evaluate for the 31-th batch, evaluate loss: 0.6883636116981506:  78%|█████████████▉    | 31/40 [00:08<00:02,  3.68it/s]evaluate for the 75-th batch, evaluate loss: 0.5410772562026978:  95%|█████████████████ | 74/78 [00:20<00:01,  3.69it/s]evaluate for the 75-th batch, evaluate loss: 0.5410772562026978:  96%|█████████████████▎| 75/78 [00:20<00:00,  3.69it/s]Epoch: 18, train for the 31-th batch, train loss: 0.574286699295044:  20%|██▌          | 30/151 [00:06<00:27,  4.40it/s]Epoch: 18, train for the 31-th batch, train loss: 0.574286699295044:  21%|██▋          | 31/151 [00:06<00:26,  4.49it/s]Epoch: 9, train for the 23-th batch, train loss: 0.46461018919944763:  15%|█▊          | 22/146 [00:13<01:15,  1.64it/s]Epoch: 9, train for the 23-th batch, train loss: 0.46461018919944763:  16%|█▉          | 23/146 [00:13<01:15,  1.63it/s]evaluate for the 32-th batch, evaluate loss: 0.7323102951049805:  78%|█████████████▉    | 31/40 [00:08<00:02,  3.68it/s]evaluate for the 32-th batch, evaluate loss: 0.7323102951049805:  80%|██████████████▍   | 32/40 [00:08<00:02,  3.68it/s]evaluate for the 76-th batch, evaluate loss: 0.5874996185302734:  96%|█████████████████▎| 75/78 [00:20<00:00,  3.69it/s]evaluate for the 76-th batch, evaluate loss: 0.5874996185302734:  97%|█████████████████▌| 76/78 [00:20<00:00,  3.70it/s]Epoch: 10, train for the 75-th batch, train loss: 0.13815030455589294:  62%|██████▊    | 74/119 [00:45<00:27,  1.63it/s]Epoch: 10, train for the 75-th batch, train loss: 0.13815030455589294:  63%|██████▉    | 75/119 [00:45<00:26,  1.63it/s]Epoch: 18, train for the 32-th batch, train loss: 0.68185955286026:  21%|██▊           | 31/151 [00:06<00:26,  4.49it/s]Epoch: 18, train for the 32-th batch, train loss: 0.68185955286026:  21%|██▉           | 32/151 [00:06<00:26,  4.50it/s]evaluate for the 33-th batch, evaluate loss: 0.7231721878051758:  80%|██████████████▍   | 32/40 [00:08<00:02,  3.68it/s]evaluate for the 33-th batch, evaluate loss: 0.7231721878051758:  82%|██████████████▊   | 33/40 [00:08<00:01,  3.68it/s]Epoch: 18, train for the 33-th batch, train loss: 0.6462129354476929:  21%|██▌         | 32/151 [00:06<00:26,  4.50it/s]Epoch: 18, train for the 33-th batch, train loss: 0.6462129354476929:  22%|██▌         | 33/151 [00:06<00:26,  4.41it/s]evaluate for the 77-th batch, evaluate loss: 0.531585156917572:  97%|██████████████████▌| 76/78 [00:21<00:00,  3.70it/s]evaluate for the 77-th batch, evaluate loss: 0.531585156917572:  99%|██████████████████▊| 77/78 [00:21<00:00,  3.69it/s]evaluate for the 34-th batch, evaluate loss: 0.7387050986289978:  82%|██████████████▊   | 33/40 [00:09<00:01,  3.68it/s]evaluate for the 34-th batch, evaluate loss: 0.7387050986289978:  85%|███████████████▎  | 34/40 [00:09<00:01,  3.68it/s]Epoch: 18, train for the 34-th batch, train loss: 0.5312305092811584:  22%|██▌         | 33/151 [00:07<00:26,  4.41it/s]Epoch: 18, train for the 34-th batch, train loss: 0.5312305092811584:  23%|██▋         | 34/151 [00:07<00:25,  4.50it/s]evaluate for the 78-th batch, evaluate loss: 0.647677481174469:  99%|██████████████████▊| 77/78 [00:21<00:00,  3.69it/s]evaluate for the 78-th batch, evaluate loss: 0.647677481174469: 100%|███████████████████| 78/78 [00:21<00:00,  4.05it/s]evaluate for the 78-th batch, evaluate loss: 0.647677481174469: 100%|███████████████████| 78/78 [00:21<00:00,  3.64it/s]
Epoch: 9, train for the 24-th batch, train loss: 0.4719406068325043:  16%|██           | 23/146 [00:13<01:15,  1.63it/s]Epoch: 9, train for the 24-th batch, train loss: 0.4719406068325043:  16%|██▏          | 24/146 [00:13<01:14,  1.63it/s]INFO:root:Epoch: 3, learning rate: 0.0001, train loss: 0.3951
INFO:root:train average_precision, 0.9098
INFO:root:train roc_auc, 0.8945
INFO:root:validate loss: 0.4244
INFO:root:validate average_precision, 0.8971
INFO:root:validate roc_auc, 0.8730
INFO:root:new node validate loss: 0.5270
INFO:root:new node validate first_1_average_precision, 0.8331
INFO:root:new node validate first_1_roc_auc, 0.8030
INFO:root:new node validate first_3_average_precision, 0.8139
INFO:root:new node validate first_3_roc_auc, 0.7742
INFO:root:new node validate first_10_average_precision, 0.8284
INFO:root:new node validate first_10_roc_auc, 0.7930
INFO:root:new node validate average_precision, 0.8404
INFO:root:new node validate roc_auc, 0.8038
INFO:root:save model ./saved_models/DyGFormer/ia-slashdot-reply-dir/DyGFormer_seed0_dygformer-ia-slashdot-reply-dir-old/DyGFormer_seed0_dygformer-ia-slashdot-reply-dir-old.pkl
evaluate for the 35-th batch, evaluate loss: 0.7267532348632812:  85%|███████████████▎  | 34/40 [00:09<00:01,  3.68it/s]evaluate for the 35-th batch, evaluate loss: 0.7267532348632812:  88%|███████████████▊  | 35/40 [00:09<00:01,  4.43it/s]Epoch: 10, train for the 76-th batch, train loss: 0.18735843896865845:  63%|██████▉    | 75/119 [00:46<00:26,  1.63it/s]Epoch: 10, train for the 76-th batch, train loss: 0.18735843896865845:  64%|███████    | 76/119 [00:46<00:26,  1.63it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/383 [00:00<?, ?it/s]Epoch: 18, train for the 35-th batch, train loss: 0.6808921098709106:  23%|██▋         | 34/151 [00:07<00:25,  4.50it/s]Epoch: 18, train for the 35-th batch, train loss: 0.6808921098709106:  23%|██▊         | 35/151 [00:07<00:25,  4.50it/s]evaluate for the 36-th batch, evaluate loss: 0.7597370147705078:  88%|███████████████▊  | 35/40 [00:09<00:01,  4.43it/s]evaluate for the 36-th batch, evaluate loss: 0.7597370147705078:  90%|████████████████▏ | 36/40 [00:09<00:00,  4.96it/s]Epoch: 18, train for the 36-th batch, train loss: 0.5832010507583618:  23%|██▊         | 35/151 [00:07<00:25,  4.50it/s]Epoch: 18, train for the 36-th batch, train loss: 0.5832010507583618:  24%|██▊         | 36/151 [00:07<00:25,  4.57it/s]evaluate for the 37-th batch, evaluate loss: 0.7662882804870605:  90%|████████████████▏ | 36/40 [00:09<00:00,  4.96it/s]evaluate for the 37-th batch, evaluate loss: 0.7662882804870605:  92%|████████████████▋ | 37/40 [00:09<00:00,  4.52it/s]Epoch: 9, train for the 25-th batch, train loss: 0.47498175501823425:  16%|█▉          | 24/146 [00:14<01:14,  1.63it/s]Epoch: 9, train for the 25-th batch, train loss: 0.47498175501823425:  17%|██          | 25/146 [00:14<01:14,  1.63it/s]Epoch: 4, train for the 1-th batch, train loss: 0.527201771736145:   0%|                        | 0/383 [00:00<?, ?it/s]Epoch: 4, train for the 1-th batch, train loss: 0.527201771736145:   0%|                | 1/383 [00:00<02:49,  2.25it/s]Epoch: 18, train for the 37-th batch, train loss: 0.618509829044342:  24%|███          | 36/151 [00:07<00:25,  4.57it/s]Epoch: 18, train for the 37-th batch, train loss: 0.618509829044342:  25%|███▏         | 37/151 [00:07<00:24,  4.59it/s]Epoch: 10, train for the 77-th batch, train loss: 0.14701713621616364:  64%|███████    | 76/119 [00:46<00:26,  1.63it/s]Epoch: 10, train for the 77-th batch, train loss: 0.14701713621616364:  65%|███████    | 77/119 [00:46<00:25,  1.63it/s]evaluate for the 38-th batch, evaluate loss: 0.7828092575073242:  92%|████████████████▋ | 37/40 [00:10<00:00,  4.52it/s]evaluate for the 38-th batch, evaluate loss: 0.7828092575073242:  95%|█████████████████ | 38/40 [00:10<00:00,  4.18it/s]Epoch: 18, train for the 38-th batch, train loss: 0.6875916719436646:  25%|██▉         | 37/151 [00:07<00:24,  4.59it/s]Epoch: 18, train for the 38-th batch, train loss: 0.6875916719436646:  25%|███         | 38/151 [00:07<00:24,  4.56it/s]evaluate for the 39-th batch, evaluate loss: 0.7481487989425659:  95%|█████████████████ | 38/40 [00:10<00:00,  4.18it/s]evaluate for the 39-th batch, evaluate loss: 0.7481487989425659:  98%|█████████████████▌| 39/40 [00:10<00:00,  3.95it/s]Epoch: 4, train for the 2-th batch, train loss: 0.5042635798454285:   0%|               | 1/383 [00:00<02:49,  2.25it/s]Epoch: 4, train for the 2-th batch, train loss: 0.5042635798454285:   1%|               | 2/383 [00:00<02:52,  2.21it/s]Epoch: 18, train for the 39-th batch, train loss: 0.603196918964386:  25%|███▎         | 38/151 [00:08<00:24,  4.56it/s]Epoch: 18, train for the 39-th batch, train loss: 0.603196918964386:  26%|███▎         | 39/151 [00:08<00:24,  4.57it/s]Epoch: 9, train for the 26-th batch, train loss: 0.43126413226127625:  17%|██          | 25/146 [00:14<01:14,  1.63it/s]Epoch: 9, train for the 26-th batch, train loss: 0.43126413226127625:  18%|██▏         | 26/146 [00:14<01:13,  1.64it/s]Epoch: 18, train for the 40-th batch, train loss: 0.5583152174949646:  26%|███         | 39/151 [00:08<00:24,  4.57it/s]Epoch: 18, train for the 40-th batch, train loss: 0.5583152174949646:  26%|███▏        | 40/151 [00:08<00:24,  4.58it/s]evaluate for the 40-th batch, evaluate loss: 0.7434391379356384:  98%|█████████████████▌| 39/40 [00:10<00:00,  3.95it/s]evaluate for the 40-th batch, evaluate loss: 0.7434391379356384: 100%|██████████████████| 40/40 [00:10<00:00,  3.86it/s]evaluate for the 40-th batch, evaluate loss: 0.7434391379356384: 100%|██████████████████| 40/40 [00:10<00:00,  3.77it/s]
INFO:root:Epoch: 5, learning rate: 0.0001, train loss: 0.6076
INFO:root:train average_precision, 0.7605
INFO:root:train roc_auc, 0.7431
INFO:root:validate loss: 0.6166
INFO:root:validate average_precision, 0.7262
INFO:root:validate roc_auc, 0.7072
INFO:root:new node validate loss: 0.7109
INFO:root:new node validate first_1_average_precision, 0.5877
INFO:root:new node validate first_1_roc_auc, 0.5676
INFO:root:new node validate first_3_average_precision, 0.5907
INFO:root:new node validate first_3_roc_auc, 0.5829
INFO:root:new node validate first_10_average_precision, 0.6174
INFO:root:new node validate first_10_roc_auc, 0.6189
INFO:root:new node validate average_precision, 0.5890
INFO:root:new node validate roc_auc, 0.5784
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/237 [00:00<?, ?it/s]Epoch: 10, train for the 78-th batch, train loss: 0.17109443247318268:  65%|███████    | 77/119 [00:47<00:25,  1.63it/s]Epoch: 10, train for the 78-th batch, train loss: 0.17109443247318268:  66%|███████▏   | 78/119 [00:47<00:25,  1.63it/s]Epoch: 18, train for the 41-th batch, train loss: 0.4028485119342804:  26%|███▏        | 40/151 [00:08<00:24,  4.58it/s]Epoch: 18, train for the 41-th batch, train loss: 0.4028485119342804:  27%|███▎        | 41/151 [00:08<00:23,  4.65it/s]Epoch: 4, train for the 3-th batch, train loss: 0.4470542073249817:   1%|               | 2/383 [00:01<02:52,  2.21it/s]Epoch: 4, train for the 3-th batch, train loss: 0.4470542073249817:   1%|               | 3/383 [00:01<02:59,  2.12it/s]Epoch: 18, train for the 42-th batch, train loss: 0.5369158387184143:  27%|███▎        | 41/151 [00:08<00:23,  4.65it/s]Epoch: 18, train for the 42-th batch, train loss: 0.5369158387184143:  28%|███▎        | 42/151 [00:08<00:23,  4.64it/s]Epoch: 6, train for the 1-th batch, train loss: 1.1422542333602905:   0%|                       | 0/237 [00:00<?, ?it/s]Epoch: 6, train for the 1-th batch, train loss: 1.1422542333602905:   0%|               | 1/237 [00:00<01:37,  2.42it/s]Epoch: 9, train for the 27-th batch, train loss: 0.4514935314655304:  18%|██▎          | 26/146 [00:15<01:13,  1.64it/s]Epoch: 9, train for the 27-th batch, train loss: 0.4514935314655304:  18%|██▍          | 27/146 [00:15<01:12,  1.63it/s]Epoch: 18, train for the 43-th batch, train loss: 0.6997948884963989:  28%|███▎        | 42/151 [00:08<00:23,  4.64it/s]Epoch: 18, train for the 43-th batch, train loss: 0.6997948884963989:  28%|███▍        | 43/151 [00:08<00:23,  4.60it/s]Epoch: 10, train for the 79-th batch, train loss: 0.1701730638742447:  66%|███████▊    | 78/119 [00:47<00:25,  1.63it/s]Epoch: 10, train for the 79-th batch, train loss: 0.1701730638742447:  66%|███████▉    | 79/119 [00:47<00:24,  1.63it/s]Epoch: 4, train for the 4-th batch, train loss: 0.3578529357910156:   1%|               | 3/383 [00:01<02:59,  2.12it/s]Epoch: 4, train for the 4-th batch, train loss: 0.3578529357910156:   1%|▏              | 4/383 [00:01<03:07,  2.02it/s]Epoch: 18, train for the 44-th batch, train loss: 0.6821540594100952:  28%|███▍        | 43/151 [00:09<00:23,  4.60it/s]Epoch: 18, train for the 44-th batch, train loss: 0.6821540594100952:  29%|███▍        | 44/151 [00:09<00:23,  4.57it/s]Epoch: 6, train for the 2-th batch, train loss: 0.8363476395606995:   0%|               | 1/237 [00:00<01:37,  2.42it/s]Epoch: 6, train for the 2-th batch, train loss: 0.8363476395606995:   1%|▏              | 2/237 [00:00<01:42,  2.30it/s]Epoch: 18, train for the 45-th batch, train loss: 0.6802908778190613:  29%|███▍        | 44/151 [00:09<00:23,  4.57it/s]Epoch: 18, train for the 45-th batch, train loss: 0.6802908778190613:  30%|███▌        | 45/151 [00:09<00:23,  4.55it/s]Epoch: 9, train for the 28-th batch, train loss: 0.45811864733695984:  18%|██▏         | 27/146 [00:16<01:12,  1.63it/s]Epoch: 9, train for the 28-th batch, train loss: 0.45811864733695984:  19%|██▎         | 28/146 [00:16<01:12,  1.63it/s]Epoch: 10, train for the 80-th batch, train loss: 0.17295050621032715:  66%|███████▎   | 79/119 [00:48<00:24,  1.63it/s]Epoch: 10, train for the 80-th batch, train loss: 0.17295050621032715:  67%|███████▍   | 80/119 [00:48<00:23,  1.63it/s]Epoch: 18, train for the 46-th batch, train loss: 0.6753560304641724:  30%|███▌        | 45/151 [00:09<00:23,  4.55it/s]Epoch: 18, train for the 46-th batch, train loss: 0.6753560304641724:  30%|███▋        | 46/151 [00:09<00:23,  4.54it/s]Epoch: 4, train for the 5-th batch, train loss: 0.37812063097953796:   1%|▏             | 4/383 [00:02<03:07,  2.02it/s]Epoch: 4, train for the 5-th batch, train loss: 0.37812063097953796:   1%|▏             | 5/383 [00:02<03:11,  1.97it/s]Epoch: 6, train for the 3-th batch, train loss: 0.7566851377487183:   1%|▏              | 2/237 [00:01<01:42,  2.30it/s]Epoch: 6, train for the 3-th batch, train loss: 0.7566851377487183:   1%|▏              | 3/237 [00:01<01:40,  2.32it/s]Epoch: 18, train for the 47-th batch, train loss: 0.6466944813728333:  30%|███▋        | 46/151 [00:09<00:23,  4.54it/s]Epoch: 18, train for the 47-th batch, train loss: 0.6466944813728333:  31%|███▋        | 47/151 [00:09<00:23,  4.52it/s]Epoch: 18, train for the 48-th batch, train loss: 0.6165178418159485:  31%|███▋        | 47/151 [00:10<00:23,  4.52it/s]Epoch: 18, train for the 48-th batch, train loss: 0.6165178418159485:  32%|███▊        | 48/151 [00:10<00:22,  4.51it/s]Epoch: 9, train for the 29-th batch, train loss: 0.4424854815006256:  19%|██▍          | 28/146 [00:16<01:12,  1.63it/s]Epoch: 9, train for the 29-th batch, train loss: 0.4424854815006256:  20%|██▌          | 29/146 [00:16<01:11,  1.63it/s]Epoch: 6, train for the 4-th batch, train loss: 0.6349346041679382:   1%|▏              | 3/237 [00:01<01:40,  2.32it/s]Epoch: 6, train for the 4-th batch, train loss: 0.6349346041679382:   2%|▎              | 4/237 [00:01<01:43,  2.24it/s]Epoch: 4, train for the 6-th batch, train loss: 0.4033864140510559:   1%|▏              | 5/383 [00:03<03:11,  1.97it/s]Epoch: 4, train for the 6-th batch, train loss: 0.4033864140510559:   2%|▏              | 6/383 [00:03<03:19,  1.89it/s]Epoch: 10, train for the 81-th batch, train loss: 0.17685873806476593:  67%|███████▍   | 80/119 [00:49<00:23,  1.63it/s]Epoch: 10, train for the 81-th batch, train loss: 0.17685873806476593:  68%|███████▍   | 81/119 [00:49<00:23,  1.63it/s]Epoch: 18, train for the 49-th batch, train loss: 0.643330991268158:  32%|████▏        | 48/151 [00:10<00:22,  4.51it/s]Epoch: 18, train for the 49-th batch, train loss: 0.643330991268158:  32%|████▏        | 49/151 [00:10<00:22,  4.51it/s]Epoch: 18, train for the 50-th batch, train loss: 0.42331358790397644:  32%|███▌       | 49/151 [00:10<00:22,  4.51it/s]Epoch: 18, train for the 50-th batch, train loss: 0.42331358790397644:  33%|███▋       | 50/151 [00:10<00:22,  4.58it/s]Epoch: 6, train for the 5-th batch, train loss: 0.6836767196655273:   2%|▎              | 4/237 [00:02<01:43,  2.24it/s]Epoch: 6, train for the 5-th batch, train loss: 0.6836767196655273:   2%|▎              | 5/237 [00:02<01:46,  2.17it/s]Epoch: 9, train for the 30-th batch, train loss: 0.44832149147987366:  20%|██▍         | 29/146 [00:17<01:11,  1.63it/s]Epoch: 9, train for the 30-th batch, train loss: 0.44832149147987366:  21%|██▍         | 30/146 [00:17<01:11,  1.63it/s]Epoch: 4, train for the 7-th batch, train loss: 0.4011344015598297:   2%|▏              | 6/383 [00:03<03:19,  1.89it/s]Epoch: 4, train for the 7-th batch, train loss: 0.4011344015598297:   2%|▎              | 7/383 [00:03<03:13,  1.95it/s]Epoch: 18, train for the 51-th batch, train loss: 0.6315115094184875:  33%|███▉        | 50/151 [00:10<00:22,  4.58it/s]Epoch: 18, train for the 51-th batch, train loss: 0.6315115094184875:  34%|████        | 51/151 [00:10<00:21,  4.56it/s]Epoch: 10, train for the 82-th batch, train loss: 0.20490625500679016:  68%|███████▍   | 81/119 [00:49<00:23,  1.63it/s]Epoch: 10, train for the 82-th batch, train loss: 0.20490625500679016:  69%|███████▌   | 82/119 [00:49<00:22,  1.63it/s]Epoch: 18, train for the 52-th batch, train loss: 0.5666053295135498:  34%|████        | 51/151 [00:10<00:21,  4.56it/s]Epoch: 18, train for the 52-th batch, train loss: 0.5666053295135498:  34%|████▏       | 52/151 [00:10<00:21,  4.57it/s]Epoch: 6, train for the 6-th batch, train loss: 0.5780779719352722:   2%|▎              | 5/237 [00:02<01:46,  2.17it/s]Epoch: 6, train for the 6-th batch, train loss: 0.5780779719352722:   3%|▍              | 6/237 [00:02<01:45,  2.19it/s]Epoch: 18, train for the 53-th batch, train loss: 0.5888436436653137:  34%|████▏       | 52/151 [00:11<00:21,  4.57it/s]Epoch: 18, train for the 53-th batch, train loss: 0.5888436436653137:  35%|████▏       | 53/151 [00:11<00:21,  4.57it/s]Epoch: 4, train for the 8-th batch, train loss: 0.2645564675331116:   2%|▎              | 7/383 [00:04<03:13,  1.95it/s]Epoch: 4, train for the 8-th batch, train loss: 0.2645564675331116:   2%|▎              | 8/383 [00:04<03:11,  1.95it/s]Epoch: 9, train for the 31-th batch, train loss: 0.5274296998977661:  21%|██▋          | 30/146 [00:18<01:11,  1.63it/s]Epoch: 9, train for the 31-th batch, train loss: 0.5274296998977661:  21%|██▊          | 31/146 [00:18<01:10,  1.63it/s]Epoch: 18, train for the 54-th batch, train loss: 0.5703905820846558:  35%|████▏       | 53/151 [00:11<00:21,  4.57it/s]Epoch: 18, train for the 54-th batch, train loss: 0.5703905820846558:  36%|████▎       | 54/151 [00:11<00:21,  4.55it/s]Epoch: 10, train for the 83-th batch, train loss: 0.1785181760787964:  69%|████████▎   | 82/119 [00:50<00:22,  1.63it/s]Epoch: 10, train for the 83-th batch, train loss: 0.1785181760787964:  70%|████████▎   | 83/119 [00:50<00:22,  1.63it/s]Epoch: 6, train for the 7-th batch, train loss: 0.5596808791160583:   3%|▍              | 6/237 [00:03<01:45,  2.19it/s]Epoch: 6, train for the 7-th batch, train loss: 0.5596808791160583:   3%|▍              | 7/237 [00:03<01:43,  2.21it/s]Epoch: 18, train for the 55-th batch, train loss: 0.4404570758342743:  36%|████▎       | 54/151 [00:11<00:21,  4.55it/s]Epoch: 18, train for the 55-th batch, train loss: 0.4404570758342743:  36%|████▎       | 55/151 [00:11<00:20,  4.60it/s]Epoch: 10, train for the 84-th batch, train loss: 0.1567348688840866:  70%|████████▎   | 83/119 [00:50<00:22,  1.63it/s]Epoch: 10, train for the 84-th batch, train loss: 0.1567348688840866:  71%|████████▍   | 84/119 [00:50<00:18,  1.92it/s]Epoch: 4, train for the 9-th batch, train loss: 0.3552490472793579:   2%|▎              | 8/383 [00:04<03:11,  1.95it/s]Epoch: 4, train for the 9-th batch, train loss: 0.3552490472793579:   2%|▎              | 9/383 [00:04<03:18,  1.88it/s]Epoch: 18, train for the 56-th batch, train loss: 0.5302748680114746:  36%|████▎       | 55/151 [00:11<00:20,  4.60it/s]Epoch: 18, train for the 56-th batch, train loss: 0.5302748680114746:  37%|████▍       | 56/151 [00:11<00:20,  4.56it/s]Epoch: 6, train for the 8-th batch, train loss: 0.6358321905136108:   3%|▍              | 7/237 [00:03<01:43,  2.21it/s]Epoch: 6, train for the 8-th batch, train loss: 0.6358321905136108:   3%|▌              | 8/237 [00:03<01:45,  2.17it/s]Epoch: 18, train for the 57-th batch, train loss: 0.5784779191017151:  37%|████▍       | 56/151 [00:12<00:20,  4.56it/s]Epoch: 18, train for the 57-th batch, train loss: 0.5784779191017151:  38%|████▌       | 57/151 [00:12<00:20,  4.54it/s]Epoch: 10, train for the 85-th batch, train loss: 0.19592490792274475:  71%|███████▊   | 84/119 [00:51<00:18,  1.92it/s]Epoch: 10, train for the 85-th batch, train loss: 0.19592490792274475:  71%|███████▊   | 85/119 [00:51<00:16,  2.03it/s]Epoch: 18, train for the 58-th batch, train loss: 0.578741192817688:  38%|████▉        | 57/151 [00:12<00:20,  4.54it/s]Epoch: 18, train for the 58-th batch, train loss: 0.578741192817688:  38%|████▉        | 58/151 [00:12<00:20,  4.53it/s]Epoch: 4, train for the 10-th batch, train loss: 0.33412230014801025:   2%|▎            | 9/383 [00:05<03:18,  1.88it/s]Epoch: 4, train for the 10-th batch, train loss: 0.33412230014801025:   3%|▎           | 10/383 [00:05<03:19,  1.87it/s]Epoch: 9, train for the 32-th batch, train loss: 0.4382745623588562:  21%|██▊          | 31/146 [00:19<01:10,  1.63it/s]Epoch: 9, train for the 32-th batch, train loss: 0.4382745623588562:  22%|██▊          | 32/146 [00:19<01:25,  1.34it/s]Epoch: 6, train for the 9-th batch, train loss: 0.6005136966705322:   3%|▌              | 8/237 [00:04<01:45,  2.17it/s]Epoch: 6, train for the 9-th batch, train loss: 0.6005136966705322:   4%|▌              | 9/237 [00:04<01:45,  2.16it/s]Epoch: 18, train for the 59-th batch, train loss: 0.5043253898620605:  38%|████▌       | 58/151 [00:12<00:20,  4.53it/s]Epoch: 18, train for the 59-th batch, train loss: 0.5043253898620605:  39%|████▋       | 59/151 [00:12<00:20,  4.53it/s]Epoch: 10, train for the 86-th batch, train loss: 0.19601859152317047:  71%|███████▊   | 85/119 [00:51<00:16,  2.03it/s]Epoch: 10, train for the 86-th batch, train loss: 0.19601859152317047:  72%|███████▉   | 86/119 [00:51<00:15,  2.19it/s]Epoch: 18, train for the 60-th batch, train loss: 0.5605735778808594:  39%|████▋       | 59/151 [00:12<00:20,  4.53it/s]Epoch: 18, train for the 60-th batch, train loss: 0.5605735778808594:  40%|████▊       | 60/151 [00:12<00:20,  4.52it/s]Epoch: 4, train for the 11-th batch, train loss: 0.26964840292930603:   3%|▎           | 10/383 [00:05<03:19,  1.87it/s]Epoch: 4, train for the 11-th batch, train loss: 0.26964840292930603:   3%|▎           | 11/383 [00:05<03:22,  1.84it/s]Epoch: 6, train for the 10-th batch, train loss: 0.6211675405502319:   4%|▌             | 9/237 [00:04<01:45,  2.16it/s]Epoch: 18, train for the 61-th batch, train loss: 0.5663027167320251:  40%|████▊       | 60/151 [00:12<00:20,  4.52it/s]Epoch: 6, train for the 10-th batch, train loss: 0.6211675405502319:   4%|▌            | 10/237 [00:04<01:45,  2.15it/s]Epoch: 18, train for the 61-th batch, train loss: 0.5663027167320251:  40%|████▊       | 61/151 [00:12<00:19,  4.51it/s]Epoch: 9, train for the 33-th batch, train loss: 0.47000154852867126:  22%|██▋         | 32/146 [00:19<01:25,  1.34it/s]Epoch: 9, train for the 33-th batch, train loss: 0.47000154852867126:  23%|██▋         | 33/146 [00:19<01:19,  1.42it/s]Epoch: 10, train for the 87-th batch, train loss: 0.2042863368988037:  72%|████████▋   | 86/119 [00:52<00:15,  2.19it/s]Epoch: 10, train for the 87-th batch, train loss: 0.2042863368988037:  73%|████████▊   | 87/119 [00:52<00:15,  2.02it/s]Epoch: 18, train for the 62-th batch, train loss: 0.5802542567253113:  40%|████▊       | 61/151 [00:13<00:19,  4.51it/s]Epoch: 18, train for the 62-th batch, train loss: 0.5802542567253113:  41%|████▉       | 62/151 [00:13<00:19,  4.51it/s]Epoch: 18, train for the 63-th batch, train loss: 0.56435227394104:  41%|█████▋        | 62/151 [00:13<00:19,  4.51it/s]Epoch: 18, train for the 63-th batch, train loss: 0.56435227394104:  42%|█████▊        | 63/151 [00:13<00:19,  4.51it/s]Epoch: 6, train for the 11-th batch, train loss: 0.6433007121086121:   4%|▌            | 10/237 [00:05<01:45,  2.15it/s]Epoch: 6, train for the 11-th batch, train loss: 0.6433007121086121:   5%|▌            | 11/237 [00:05<01:46,  2.12it/s]Epoch: 4, train for the 12-th batch, train loss: 0.25029316544532776:   3%|▎           | 11/383 [00:06<03:22,  1.84it/s]Epoch: 4, train for the 12-th batch, train loss: 0.25029316544532776:   3%|▍           | 12/383 [00:06<03:24,  1.81it/s]Epoch: 18, train for the 64-th batch, train loss: 0.5809574723243713:  42%|█████       | 63/151 [00:13<00:19,  4.51it/s]Epoch: 18, train for the 64-th batch, train loss: 0.5809574723243713:  42%|█████       | 64/151 [00:13<00:19,  4.50it/s]Epoch: 9, train for the 34-th batch, train loss: 0.47373369336128235:  23%|██▋         | 33/146 [00:20<01:19,  1.42it/s]Epoch: 9, train for the 34-th batch, train loss: 0.47373369336128235:  23%|██▊         | 34/146 [00:20<01:16,  1.46it/s]Epoch: 10, train for the 88-th batch, train loss: 0.22822292149066925:  73%|████████   | 87/119 [00:52<00:15,  2.02it/s]Epoch: 10, train for the 88-th batch, train loss: 0.22822292149066925:  74%|████████▏  | 88/119 [00:52<00:16,  1.87it/s]Epoch: 18, train for the 65-th batch, train loss: 0.48586007952690125:  42%|████▋      | 64/151 [00:13<00:19,  4.50it/s]Epoch: 18, train for the 65-th batch, train loss: 0.48586007952690125:  43%|████▋      | 65/151 [00:13<00:19,  4.50it/s]Epoch: 6, train for the 12-th batch, train loss: 0.5959806442260742:   5%|▌            | 11/237 [00:05<01:46,  2.12it/s]Epoch: 6, train for the 12-th batch, train loss: 0.5959806442260742:   5%|▋            | 12/237 [00:05<01:47,  2.08it/s]Epoch: 4, train for the 13-th batch, train loss: 0.2836179733276367:   3%|▍            | 12/383 [00:06<03:24,  1.81it/s]Epoch: 4, train for the 13-th batch, train loss: 0.2836179733276367:   3%|▍            | 13/383 [00:06<03:21,  1.83it/s]Epoch: 18, train for the 66-th batch, train loss: 0.5547912120819092:  43%|█████▏      | 65/151 [00:14<00:19,  4.50it/s]Epoch: 18, train for the 66-th batch, train loss: 0.5547912120819092:  44%|█████▏      | 66/151 [00:14<00:18,  4.50it/s]Epoch: 9, train for the 35-th batch, train loss: 0.45247289538383484:  23%|██▊         | 34/146 [00:21<01:16,  1.46it/s]Epoch: 9, train for the 35-th batch, train loss: 0.45247289538383484:  24%|██▉         | 35/146 [00:21<01:14,  1.50it/s]Epoch: 18, train for the 67-th batch, train loss: 0.5882459282875061:  44%|█████▏      | 66/151 [00:14<00:18,  4.50it/s]Epoch: 18, train for the 67-th batch, train loss: 0.5882459282875061:  44%|█████▎      | 67/151 [00:14<00:18,  4.50it/s]Epoch: 6, train for the 13-th batch, train loss: 0.5870252847671509:   5%|▋            | 12/237 [00:05<01:47,  2.08it/s]Epoch: 6, train for the 13-th batch, train loss: 0.5870252847671509:   5%|▋            | 13/237 [00:05<01:46,  2.11it/s]Epoch: 10, train for the 89-th batch, train loss: 0.1874869465827942:  74%|████████▊   | 88/119 [00:53<00:16,  1.87it/s]Epoch: 10, train for the 89-th batch, train loss: 0.1874869465827942:  75%|████████▉   | 89/119 [00:53<00:16,  1.78it/s]Epoch: 18, train for the 68-th batch, train loss: 0.5472709536552429:  44%|█████▎      | 67/151 [00:14<00:18,  4.50it/s]Epoch: 18, train for the 68-th batch, train loss: 0.5472709536552429:  45%|█████▍      | 68/151 [00:14<00:18,  4.51it/s]Epoch: 4, train for the 14-th batch, train loss: 0.27833622694015503:   3%|▍           | 13/383 [00:07<03:21,  1.83it/s]Epoch: 4, train for the 14-th batch, train loss: 0.27833622694015503:   4%|▍           | 14/383 [00:07<03:19,  1.85it/s]Epoch: 18, train for the 69-th batch, train loss: 0.41082343459129333:  45%|████▉      | 68/151 [00:14<00:18,  4.51it/s]Epoch: 18, train for the 69-th batch, train loss: 0.41082343459129333:  46%|█████      | 69/151 [00:14<00:17,  4.57it/s]Epoch: 9, train for the 36-th batch, train loss: 0.41324499249458313:  24%|██▉         | 35/146 [00:21<01:14,  1.50it/s]Epoch: 9, train for the 36-th batch, train loss: 0.41324499249458313:  25%|██▉         | 36/146 [00:21<01:11,  1.53it/s]Epoch: 6, train for the 14-th batch, train loss: 0.7969813942909241:   5%|▋            | 13/237 [00:06<01:46,  2.11it/s]Epoch: 6, train for the 14-th batch, train loss: 0.7969813942909241:   6%|▊            | 14/237 [00:06<01:49,  2.04it/s]Epoch: 18, train for the 70-th batch, train loss: 0.5395292639732361:  46%|█████▍      | 69/151 [00:14<00:17,  4.57it/s]Epoch: 18, train for the 70-th batch, train loss: 0.5395292639732361:  46%|█████▌      | 70/151 [00:14<00:17,  4.55it/s]Epoch: 10, train for the 90-th batch, train loss: 0.1894126683473587:  75%|████████▉   | 89/119 [00:53<00:16,  1.78it/s]Epoch: 10, train for the 90-th batch, train loss: 0.1894126683473587:  76%|█████████   | 90/119 [00:53<00:16,  1.73it/s]Epoch: 4, train for the 15-th batch, train loss: 0.25611865520477295:   4%|▍           | 14/383 [00:07<03:19,  1.85it/s]Epoch: 4, train for the 15-th batch, train loss: 0.25611865520477295:   4%|▍           | 15/383 [00:07<03:20,  1.83it/s]Epoch: 18, train for the 71-th batch, train loss: 0.5513932108879089:  46%|█████▌      | 70/151 [00:15<00:17,  4.55it/s]Epoch: 18, train for the 71-th batch, train loss: 0.5513932108879089:  47%|█████▋      | 71/151 [00:15<00:17,  4.53it/s]Epoch: 18, train for the 72-th batch, train loss: 0.542496919631958:  47%|██████       | 71/151 [00:15<00:17,  4.53it/s]Epoch: 18, train for the 72-th batch, train loss: 0.542496919631958:  48%|██████▏      | 72/151 [00:15<00:17,  4.53it/s]Epoch: 9, train for the 37-th batch, train loss: 0.47008392214775085:  25%|██▉         | 36/146 [00:22<01:11,  1.53it/s]Epoch: 9, train for the 37-th batch, train loss: 0.47008392214775085:  25%|███         | 37/146 [00:22<01:09,  1.57it/s]Epoch: 4, train for the 16-th batch, train loss: 0.32355231046676636:   4%|▍           | 15/383 [00:08<03:20,  1.83it/s]Epoch: 4, train for the 16-th batch, train loss: 0.32355231046676636:   4%|▌           | 16/383 [00:08<03:04,  1.99it/s]Epoch: 18, train for the 73-th batch, train loss: 0.5480186939239502:  48%|█████▋      | 72/151 [00:15<00:17,  4.53it/s]Epoch: 18, train for the 73-th batch, train loss: 0.5480186939239502:  48%|█████▊      | 73/151 [00:15<00:17,  4.51it/s]Epoch: 10, train for the 91-th batch, train loss: 0.18215598165988922:  76%|████████▎  | 90/119 [00:54<00:16,  1.73it/s]Epoch: 10, train for the 91-th batch, train loss: 0.18215598165988922:  76%|████████▍  | 91/119 [00:54<00:16,  1.71it/s]Epoch: 6, train for the 15-th batch, train loss: 0.5182510018348694:   6%|▊            | 14/237 [00:07<01:49,  2.04it/s]Epoch: 6, train for the 15-th batch, train loss: 0.5182510018348694:   6%|▊            | 15/237 [00:07<02:13,  1.66it/s]Epoch: 18, train for the 74-th batch, train loss: 0.5428763628005981:  48%|█████▊      | 73/151 [00:15<00:17,  4.51it/s]Epoch: 18, train for the 74-th batch, train loss: 0.5428763628005981:  49%|█████▉      | 74/151 [00:15<00:17,  4.50it/s]Epoch: 4, train for the 17-th batch, train loss: 0.3963453471660614:   4%|▌            | 16/383 [00:08<03:04,  1.99it/s]Epoch: 4, train for the 17-th batch, train loss: 0.3963453471660614:   4%|▌            | 17/383 [00:08<02:45,  2.21it/s]Epoch: 18, train for the 75-th batch, train loss: 0.5115236639976501:  49%|█████▉      | 74/151 [00:16<00:17,  4.50it/s]Epoch: 18, train for the 75-th batch, train loss: 0.5115236639976501:  50%|█████▉      | 75/151 [00:16<00:16,  4.51it/s]Epoch: 9, train for the 38-th batch, train loss: 0.4874332845211029:  25%|███▎         | 37/146 [00:22<01:09,  1.57it/s]Epoch: 9, train for the 38-th batch, train loss: 0.4874332845211029:  26%|███▍         | 38/146 [00:22<01:08,  1.58it/s]Epoch: 10, train for the 92-th batch, train loss: 0.19989001750946045:  76%|████████▍  | 91/119 [00:55<00:16,  1.71it/s]Epoch: 10, train for the 92-th batch, train loss: 0.19989001750946045:  77%|████████▌  | 92/119 [00:55<00:16,  1.68it/s]Epoch: 18, train for the 76-th batch, train loss: 0.5719704031944275:  50%|█████▉      | 75/151 [00:16<00:16,  4.51it/s]Epoch: 18, train for the 76-th batch, train loss: 0.5719704031944275:  50%|██████      | 76/151 [00:16<00:16,  4.50it/s]Epoch: 6, train for the 16-th batch, train loss: 0.6656874418258667:   6%|▊            | 15/237 [00:07<02:13,  1.66it/s]Epoch: 6, train for the 16-th batch, train loss: 0.6656874418258667:   7%|▉            | 16/237 [00:07<02:08,  1.72it/s]Epoch: 4, train for the 18-th batch, train loss: 0.32453086972236633:   4%|▌           | 17/383 [00:09<02:45,  2.21it/s]Epoch: 4, train for the 18-th batch, train loss: 0.32453086972236633:   5%|▌           | 18/383 [00:09<02:55,  2.08it/s]Epoch: 18, train for the 77-th batch, train loss: 0.40900084376335144:  50%|█████▌     | 76/151 [00:16<00:16,  4.50it/s]Epoch: 18, train for the 77-th batch, train loss: 0.40900084376335144:  51%|█████▌     | 77/151 [00:16<00:16,  4.56it/s]Epoch: 18, train for the 78-th batch, train loss: 0.5520362854003906:  51%|██████      | 77/151 [00:16<00:16,  4.56it/s]Epoch: 18, train for the 78-th batch, train loss: 0.5520362854003906:  52%|██████▏     | 78/151 [00:16<00:16,  4.54it/s]Epoch: 9, train for the 39-th batch, train loss: 0.47572770714759827:  26%|███         | 38/146 [00:23<01:08,  1.58it/s]Epoch: 9, train for the 39-th batch, train loss: 0.47572770714759827:  27%|███▏        | 39/146 [00:23<01:07,  1.59it/s]Epoch: 6, train for the 17-th batch, train loss: 0.5912906527519226:   7%|▉            | 16/237 [00:08<02:08,  1.72it/s]Epoch: 6, train for the 17-th batch, train loss: 0.5912906527519226:   7%|▉            | 17/237 [00:08<02:06,  1.75it/s]Epoch: 10, train for the 93-th batch, train loss: 0.17607317864894867:  77%|████████▌  | 92/119 [00:55<00:16,  1.68it/s]Epoch: 10, train for the 93-th batch, train loss: 0.17607317864894867:  78%|████████▌  | 93/119 [00:55<00:15,  1.66it/s]Epoch: 18, train for the 79-th batch, train loss: 0.5981915593147278:  52%|██████▏     | 78/151 [00:16<00:16,  4.54it/s]Epoch: 18, train for the 79-th batch, train loss: 0.5981915593147278:  52%|██████▎     | 79/151 [00:16<00:15,  4.52it/s]Epoch: 4, train for the 19-th batch, train loss: 0.2548360824584961:   5%|▌            | 18/383 [00:09<02:55,  2.08it/s]Epoch: 4, train for the 19-th batch, train loss: 0.2548360824584961:   5%|▋            | 19/383 [00:09<03:03,  1.99it/s]Epoch: 18, train for the 80-th batch, train loss: 0.5828238725662231:  52%|██████▎     | 79/151 [00:17<00:15,  4.52it/s]Epoch: 18, train for the 80-th batch, train loss: 0.5828238725662231:  53%|██████▎     | 80/151 [00:17<00:15,  4.52it/s]Epoch: 9, train for the 40-th batch, train loss: 0.5121057033538818:  27%|███▍         | 39/146 [00:24<01:07,  1.59it/s]Epoch: 9, train for the 40-th batch, train loss: 0.5121057033538818:  27%|███▌         | 40/146 [00:24<01:06,  1.60it/s]Epoch: 18, train for the 81-th batch, train loss: 0.5631969571113586:  53%|██████▎     | 80/151 [00:17<00:15,  4.52it/s]Epoch: 18, train for the 81-th batch, train loss: 0.5631969571113586:  54%|██████▍     | 81/151 [00:17<00:15,  4.51it/s]Epoch: 6, train for the 18-th batch, train loss: 0.5691085457801819:   7%|▉            | 17/237 [00:09<02:06,  1.75it/s]Epoch: 6, train for the 18-th batch, train loss: 0.5691085457801819:   8%|▉            | 18/237 [00:09<02:03,  1.77it/s]Epoch: 10, train for the 94-th batch, train loss: 0.1808004379272461:  78%|█████████▍  | 93/119 [00:56<00:15,  1.66it/s]Epoch: 10, train for the 94-th batch, train loss: 0.1808004379272461:  79%|█████████▍  | 94/119 [00:56<00:15,  1.65it/s]Epoch: 4, train for the 20-th batch, train loss: 0.28781595826148987:   5%|▌           | 19/383 [00:10<03:03,  1.99it/s]Epoch: 4, train for the 20-th batch, train loss: 0.28781595826148987:   5%|▋           | 20/383 [00:10<03:10,  1.91it/s]Epoch: 18, train for the 82-th batch, train loss: 0.5418545007705688:  54%|██████▍     | 81/151 [00:17<00:15,  4.51it/s]Epoch: 18, train for the 82-th batch, train loss: 0.5418545007705688:  54%|██████▌     | 82/151 [00:17<00:15,  4.50it/s]Epoch: 18, train for the 83-th batch, train loss: 0.5321893692016602:  54%|██████▌     | 82/151 [00:17<00:15,  4.50it/s]Epoch: 18, train for the 83-th batch, train loss: 0.5321893692016602:  55%|██████▌     | 83/151 [00:17<00:15,  4.50it/s]Epoch: 6, train for the 19-th batch, train loss: 0.46292731165885925:   8%|▉           | 18/237 [00:09<02:03,  1.77it/s]Epoch: 6, train for the 19-th batch, train loss: 0.46292731165885925:   8%|▉           | 19/237 [00:09<01:59,  1.82it/s]Epoch: 9, train for the 41-th batch, train loss: 0.4765183925628662:  27%|███▌         | 40/146 [00:24<01:06,  1.60it/s]Epoch: 9, train for the 41-th batch, train loss: 0.4765183925628662:  28%|███▋         | 41/146 [00:24<01:05,  1.61it/s]Epoch: 18, train for the 84-th batch, train loss: 0.5708916187286377:  55%|██████▌     | 83/151 [00:18<00:15,  4.50it/s]Epoch: 18, train for the 84-th batch, train loss: 0.5708916187286377:  56%|██████▋     | 84/151 [00:18<00:14,  4.51it/s]Epoch: 4, train for the 21-th batch, train loss: 0.3170336186885834:   5%|▋            | 20/383 [00:10<03:10,  1.91it/s]Epoch: 4, train for the 21-th batch, train loss: 0.3170336186885834:   5%|▋            | 21/383 [00:10<03:14,  1.86it/s]Epoch: 10, train for the 95-th batch, train loss: 0.13143862783908844:  79%|████████▋  | 94/119 [00:57<00:15,  1.65it/s]Epoch: 10, train for the 95-th batch, train loss: 0.13143862783908844:  80%|████████▊  | 95/119 [00:57<00:14,  1.64it/s]Epoch: 18, train for the 85-th batch, train loss: 0.5900513529777527:  56%|██████▋     | 84/151 [00:18<00:14,  4.51it/s]Epoch: 18, train for the 85-th batch, train loss: 0.5900513529777527:  56%|██████▊     | 85/151 [00:18<00:14,  4.50it/s]Epoch: 6, train for the 20-th batch, train loss: 0.6266793012619019:   8%|█            | 19/237 [00:10<01:59,  1.82it/s]Epoch: 6, train for the 20-th batch, train loss: 0.6266793012619019:   8%|█            | 20/237 [00:10<01:59,  1.82it/s]Epoch: 18, train for the 86-th batch, train loss: 0.5373229384422302:  56%|██████▊     | 85/151 [00:18<00:14,  4.50it/s]Epoch: 18, train for the 86-th batch, train loss: 0.5373229384422302:  57%|██████▊     | 86/151 [00:18<00:14,  4.51it/s]Epoch: 9, train for the 42-th batch, train loss: 0.4842180609703064:  28%|███▋         | 41/146 [00:25<01:05,  1.61it/s]Epoch: 9, train for the 42-th batch, train loss: 0.4842180609703064:  29%|███▋         | 42/146 [00:25<01:04,  1.62it/s]Epoch: 4, train for the 22-th batch, train loss: 0.29466742277145386:   5%|▋           | 21/383 [00:11<03:14,  1.86it/s]Epoch: 4, train for the 22-th batch, train loss: 0.29466742277145386:   6%|▋           | 22/383 [00:11<03:15,  1.85it/s]Epoch: 18, train for the 87-th batch, train loss: 0.5509511232376099:  57%|██████▊     | 86/151 [00:18<00:14,  4.51it/s]Epoch: 18, train for the 87-th batch, train loss: 0.5509511232376099:  58%|██████▉     | 87/151 [00:18<00:14,  4.51it/s]Epoch: 10, train for the 96-th batch, train loss: 0.16629084944725037:  80%|████████▊  | 95/119 [00:57<00:14,  1.64it/s]Epoch: 10, train for the 96-th batch, train loss: 0.16629084944725037:  81%|████████▊  | 96/119 [00:57<00:13,  1.64it/s]Epoch: 18, train for the 88-th batch, train loss: 0.5886638164520264:  58%|██████▉     | 87/151 [00:18<00:14,  4.51it/s]Epoch: 18, train for the 88-th batch, train loss: 0.5886638164520264:  58%|██████▉     | 88/151 [00:18<00:13,  4.50it/s]Epoch: 6, train for the 21-th batch, train loss: 0.577375054359436:   8%|█▏            | 20/237 [00:10<01:59,  1.82it/s]Epoch: 6, train for the 21-th batch, train loss: 0.577375054359436:   9%|█▏            | 21/237 [00:10<01:59,  1.80it/s]Epoch: 18, train for the 89-th batch, train loss: 0.5670340061187744:  58%|██████▉     | 88/151 [00:19<00:13,  4.50it/s]Epoch: 18, train for the 89-th batch, train loss: 0.5670340061187744:  59%|███████     | 89/151 [00:19<00:13,  4.51it/s]Epoch: 9, train for the 43-th batch, train loss: 0.46013468503952026:  29%|███▍        | 42/146 [00:25<01:04,  1.62it/s]Epoch: 9, train for the 43-th batch, train loss: 0.46013468503952026:  29%|███▌        | 43/146 [00:25<01:03,  1.62it/s]Epoch: 4, train for the 23-th batch, train loss: 0.322829008102417:   6%|▊             | 22/383 [00:12<03:15,  1.85it/s]Epoch: 4, train for the 23-th batch, train loss: 0.322829008102417:   6%|▊             | 23/383 [00:12<03:19,  1.80it/s]Epoch: 10, train for the 97-th batch, train loss: 0.15725523233413696:  81%|████████▊  | 96/119 [00:58<00:13,  1.64it/s]Epoch: 10, train for the 97-th batch, train loss: 0.15725523233413696:  82%|████████▉  | 97/119 [00:58<00:13,  1.64it/s]Epoch: 18, train for the 90-th batch, train loss: 0.5489576458930969:  59%|███████     | 89/151 [00:19<00:13,  4.51it/s]Epoch: 18, train for the 90-th batch, train loss: 0.5489576458930969:  60%|███████▏    | 90/151 [00:19<00:13,  4.51it/s]Epoch: 18, train for the 91-th batch, train loss: 0.45628759264945984:  60%|██████▌    | 90/151 [00:19<00:13,  4.51it/s]Epoch: 18, train for the 91-th batch, train loss: 0.45628759264945984:  60%|██████▋    | 91/151 [00:19<00:13,  4.52it/s]Epoch: 6, train for the 22-th batch, train loss: 0.5219385623931885:   9%|█▏           | 21/237 [00:11<01:59,  1.80it/s]Epoch: 6, train for the 22-th batch, train loss: 0.5219385623931885:   9%|█▏           | 22/237 [00:11<01:59,  1.80it/s]Epoch: 9, train for the 44-th batch, train loss: 0.4980882406234741:  29%|███▊         | 43/146 [00:26<01:03,  1.62it/s]Epoch: 9, train for the 44-th batch, train loss: 0.4980882406234741:  30%|███▉         | 44/146 [00:26<01:02,  1.62it/s]Epoch: 4, train for the 24-th batch, train loss: 0.2889211177825928:   6%|▊            | 23/383 [00:12<03:19,  1.80it/s]Epoch: 4, train for the 24-th batch, train loss: 0.2889211177825928:   6%|▊            | 24/383 [00:12<03:21,  1.78it/s]Epoch: 18, train for the 92-th batch, train loss: 0.5596031546592712:  60%|███████▏    | 91/151 [00:19<00:13,  4.52it/s]Epoch: 18, train for the 92-th batch, train loss: 0.5596031546592712:  61%|███████▎    | 92/151 [00:19<00:13,  4.51it/s]Epoch: 10, train for the 98-th batch, train loss: 0.13559743762016296:  82%|████████▉  | 97/119 [00:58<00:13,  1.64it/s]Epoch: 10, train for the 98-th batch, train loss: 0.13559743762016296:  82%|█████████  | 98/119 [00:58<00:12,  1.64it/s]Epoch: 18, train for the 93-th batch, train loss: 0.5294517278671265:  61%|███████▎    | 92/151 [00:20<00:13,  4.51it/s]Epoch: 18, train for the 93-th batch, train loss: 0.5294517278671265:  62%|███████▍    | 93/151 [00:20<00:12,  4.50it/s]Epoch: 6, train for the 23-th batch, train loss: 0.5340638160705566:   9%|█▏           | 22/237 [00:11<01:59,  1.80it/s]Epoch: 6, train for the 23-th batch, train loss: 0.5340638160705566:  10%|█▎           | 23/237 [00:11<01:58,  1.80it/s]Epoch: 18, train for the 94-th batch, train loss: 0.5174786448478699:  62%|███████▍    | 93/151 [00:20<00:12,  4.50it/s]Epoch: 18, train for the 94-th batch, train loss: 0.5174786448478699:  62%|███████▍    | 94/151 [00:20<00:12,  4.49it/s]Epoch: 4, train for the 25-th batch, train loss: 0.35009902715682983:   6%|▊           | 24/383 [00:13<03:21,  1.78it/s]Epoch: 4, train for the 25-th batch, train loss: 0.35009902715682983:   7%|▊           | 25/383 [00:13<03:23,  1.76it/s]Epoch: 9, train for the 45-th batch, train loss: 0.4262522757053375:  30%|███▉         | 44/146 [00:27<01:02,  1.62it/s]Epoch: 9, train for the 45-th batch, train loss: 0.4262522757053375:  31%|████         | 45/146 [00:27<01:01,  1.63it/s]Epoch: 18, train for the 95-th batch, train loss: 0.52117919921875:  62%|████████▋     | 94/151 [00:20<00:12,  4.49it/s]Epoch: 18, train for the 95-th batch, train loss: 0.52117919921875:  63%|████████▊     | 95/151 [00:20<00:12,  4.49it/s]Epoch: 10, train for the 99-th batch, train loss: 0.15620729327201843:  82%|█████████  | 98/119 [00:59<00:12,  1.64it/s]Epoch: 10, train for the 99-th batch, train loss: 0.15620729327201843:  83%|█████████▏ | 99/119 [00:59<00:12,  1.65it/s]Epoch: 6, train for the 24-th batch, train loss: 0.4820016324520111:  10%|█▎           | 23/237 [00:12<01:58,  1.80it/s]Epoch: 6, train for the 24-th batch, train loss: 0.4820016324520111:  10%|█▎           | 24/237 [00:12<01:57,  1.81it/s]Epoch: 18, train for the 96-th batch, train loss: 0.5611417889595032:  63%|███████▌    | 95/151 [00:20<00:12,  4.49it/s]Epoch: 18, train for the 96-th batch, train loss: 0.5611417889595032:  64%|███████▋    | 96/151 [00:20<00:12,  4.49it/s]Epoch: 18, train for the 97-th batch, train loss: 0.5907313227653503:  64%|███████▋    | 96/151 [00:20<00:12,  4.49it/s]Epoch: 18, train for the 97-th batch, train loss: 0.5907313227653503:  64%|███████▋    | 97/151 [00:20<00:11,  4.50it/s]Epoch: 4, train for the 26-th batch, train loss: 0.3099280893802643:   7%|▊            | 25/383 [00:13<03:23,  1.76it/s]Epoch: 4, train for the 26-th batch, train loss: 0.3099280893802643:   7%|▉            | 26/383 [00:13<03:23,  1.76it/s]Epoch: 9, train for the 46-th batch, train loss: 0.47622907161712646:  31%|███▋        | 45/146 [00:27<01:01,  1.63it/s]Epoch: 9, train for the 46-th batch, train loss: 0.47622907161712646:  32%|███▊        | 46/146 [00:27<01:00,  1.65it/s]Epoch: 10, train for the 100-th batch, train loss: 0.17684513330459595:  83%|████████▎ | 99/119 [01:00<00:12,  1.65it/s]Epoch: 10, train for the 100-th batch, train loss: 0.17684513330459595:  84%|███████▌ | 100/119 [01:00<00:11,  1.65it/s]Epoch: 18, train for the 98-th batch, train loss: 0.6129103899002075:  64%|███████▋    | 97/151 [00:21<00:11,  4.50it/s]Epoch: 18, train for the 98-th batch, train loss: 0.6129103899002075:  65%|███████▊    | 98/151 [00:21<00:11,  4.51it/s]Epoch: 6, train for the 25-th batch, train loss: 0.47674983739852905:  10%|█▏          | 24/237 [00:12<01:57,  1.81it/s]Epoch: 6, train for the 25-th batch, train loss: 0.47674983739852905:  11%|█▎          | 25/237 [00:12<01:56,  1.82it/s]Epoch: 18, train for the 99-th batch, train loss: 0.6335886120796204:  65%|███████▊    | 98/151 [00:21<00:11,  4.51it/s]Epoch: 18, train for the 99-th batch, train loss: 0.6335886120796204:  66%|███████▊    | 99/151 [00:21<00:11,  4.50it/s]Epoch: 4, train for the 27-th batch, train loss: 0.25541773438453674:   7%|▊           | 26/383 [00:14<03:23,  1.76it/s]Epoch: 4, train for the 27-th batch, train loss: 0.25541773438453674:   7%|▊           | 27/383 [00:14<03:25,  1.73it/s]Epoch: 9, train for the 47-th batch, train loss: 0.4983081817626953:  32%|████         | 46/146 [00:28<01:00,  1.65it/s]Epoch: 9, train for the 47-th batch, train loss: 0.4983081817626953:  32%|████▏        | 47/146 [00:28<01:00,  1.65it/s]Epoch: 18, train for the 100-th batch, train loss: 0.6670492887496948:  66%|███████▏   | 99/151 [00:21<00:11,  4.50it/s]Epoch: 18, train for the 100-th batch, train loss: 0.6670492887496948:  66%|██████▌   | 100/151 [00:21<00:11,  4.50it/s]Epoch: 10, train for the 101-th batch, train loss: 0.154556542634964:  84%|█████████▏ | 100/119 [01:00<00:11,  1.65it/s]Epoch: 10, train for the 101-th batch, train loss: 0.154556542634964:  85%|█████████▎ | 101/119 [01:00<00:10,  1.65it/s]Epoch: 6, train for the 26-th batch, train loss: 0.3841191828250885:  11%|█▎           | 25/237 [00:13<01:56,  1.82it/s]Epoch: 6, train for the 26-th batch, train loss: 0.3841191828250885:  11%|█▍           | 26/237 [00:13<01:56,  1.81it/s]Epoch: 18, train for the 101-th batch, train loss: 0.6837518215179443:  66%|██████▌   | 100/151 [00:21<00:11,  4.50it/s]Epoch: 18, train for the 101-th batch, train loss: 0.6837518215179443:  67%|██████▋   | 101/151 [00:21<00:11,  4.50it/s]Epoch: 18, train for the 102-th batch, train loss: 0.6204906105995178:  67%|██████▋   | 101/151 [00:22<00:11,  4.50it/s]Epoch: 18, train for the 102-th batch, train loss: 0.6204906105995178:  68%|██████▊   | 102/151 [00:22<00:10,  4.49it/s]Epoch: 4, train for the 28-th batch, train loss: 0.24592775106430054:   7%|▊           | 27/383 [00:14<03:25,  1.73it/s]Epoch: 4, train for the 28-th batch, train loss: 0.24592775106430054:   7%|▉           | 28/383 [00:14<03:22,  1.75it/s]Epoch: 9, train for the 48-th batch, train loss: 0.47203436493873596:  32%|███▊        | 47/146 [00:28<01:00,  1.65it/s]Epoch: 9, train for the 48-th batch, train loss: 0.47203436493873596:  33%|███▉        | 48/146 [00:28<00:59,  1.66it/s]Epoch: 18, train for the 103-th batch, train loss: 0.6100912094116211:  68%|██████▊   | 102/151 [00:22<00:10,  4.49it/s]Epoch: 18, train for the 103-th batch, train loss: 0.6100912094116211:  68%|██████▊   | 103/151 [00:22<00:10,  4.48it/s]Epoch: 10, train for the 102-th batch, train loss: 0.17398715019226074:  85%|███████▋ | 101/119 [01:01<00:10,  1.65it/s]Epoch: 10, train for the 102-th batch, train loss: 0.17398715019226074:  86%|███████▋ | 102/119 [01:01<00:10,  1.66it/s]Epoch: 6, train for the 27-th batch, train loss: 0.4454772174358368:  11%|█▍           | 26/237 [00:13<01:56,  1.81it/s]Epoch: 6, train for the 27-th batch, train loss: 0.4454772174358368:  11%|█▍           | 27/237 [00:13<01:56,  1.80it/s]Epoch: 18, train for the 104-th batch, train loss: 0.5939895510673523:  68%|██████▊   | 103/151 [00:22<00:10,  4.48it/s]Epoch: 18, train for the 104-th batch, train loss: 0.5939895510673523:  69%|██████▉   | 104/151 [00:22<00:10,  4.50it/s]Epoch: 4, train for the 29-th batch, train loss: 0.23686760663986206:   7%|▉           | 28/383 [00:15<03:22,  1.75it/s]Epoch: 4, train for the 29-th batch, train loss: 0.23686760663986206:   8%|▉           | 29/383 [00:15<03:19,  1.77it/s]Epoch: 18, train for the 105-th batch, train loss: 0.5556376576423645:  69%|██████▉   | 104/151 [00:22<00:10,  4.50it/s]Epoch: 18, train for the 105-th batch, train loss: 0.5556376576423645:  70%|██████▉   | 105/151 [00:22<00:10,  4.50it/s]Epoch: 9, train for the 49-th batch, train loss: 0.47581854462623596:  33%|███▉        | 48/146 [00:29<00:59,  1.66it/s]Epoch: 9, train for the 49-th batch, train loss: 0.47581854462623596:  34%|████        | 49/146 [00:29<00:58,  1.64it/s]Epoch: 6, train for the 28-th batch, train loss: 0.5789512395858765:  11%|█▍           | 27/237 [00:14<01:56,  1.80it/s]Epoch: 6, train for the 28-th batch, train loss: 0.5789512395858765:  12%|█▌           | 28/237 [00:14<01:55,  1.82it/s]Epoch: 18, train for the 106-th batch, train loss: 0.5680468082427979:  70%|██████▉   | 105/151 [00:22<00:10,  4.50it/s]Epoch: 18, train for the 106-th batch, train loss: 0.5680468082427979:  70%|███████   | 106/151 [00:22<00:09,  4.51it/s]Epoch: 10, train for the 103-th batch, train loss: 0.19031578302383423:  86%|███████▋ | 102/119 [01:01<00:10,  1.66it/s]Epoch: 10, train for the 103-th batch, train loss: 0.19031578302383423:  87%|███████▊ | 103/119 [01:01<00:09,  1.64it/s]Epoch: 18, train for the 107-th batch, train loss: 0.5734155774116516:  70%|███████   | 106/151 [00:23<00:09,  4.51it/s]Epoch: 18, train for the 107-th batch, train loss: 0.5734155774116516:  71%|███████   | 107/151 [00:23<00:09,  4.52it/s]Epoch: 4, train for the 30-th batch, train loss: 0.2527615427970886:   8%|▉            | 29/383 [00:15<03:19,  1.77it/s]Epoch: 4, train for the 30-th batch, train loss: 0.2527615427970886:   8%|█            | 30/383 [00:15<03:18,  1.78it/s]Epoch: 18, train for the 108-th batch, train loss: 0.5900886058807373:  71%|███████   | 107/151 [00:23<00:09,  4.52it/s]Epoch: 18, train for the 108-th batch, train loss: 0.5900886058807373:  72%|███████▏  | 108/151 [00:23<00:09,  4.51it/s]Epoch: 9, train for the 50-th batch, train loss: 0.5168877840042114:  34%|████▎        | 49/146 [00:30<00:58,  1.64it/s]Epoch: 9, train for the 50-th batch, train loss: 0.5168877840042114:  34%|████▍        | 50/146 [00:30<00:58,  1.63it/s]Epoch: 6, train for the 29-th batch, train loss: 0.4143611788749695:  12%|█▌           | 28/237 [00:15<01:55,  1.82it/s]Epoch: 6, train for the 29-th batch, train loss: 0.4143611788749695:  12%|█▌           | 29/237 [00:15<01:55,  1.80it/s]Epoch: 10, train for the 104-th batch, train loss: 0.20482423901557922:  87%|███████▊ | 103/119 [01:02<00:09,  1.64it/s]Epoch: 10, train for the 104-th batch, train loss: 0.20482423901557922:  87%|███████▊ | 104/119 [01:02<00:09,  1.63it/s]Epoch: 18, train for the 109-th batch, train loss: 0.5626431703567505:  72%|███████▏  | 108/151 [00:23<00:09,  4.51it/s]Epoch: 18, train for the 109-th batch, train loss: 0.5626431703567505:  72%|███████▏  | 109/151 [00:23<00:09,  4.50it/s]Epoch: 4, train for the 31-th batch, train loss: 0.31460118293762207:   8%|▉           | 30/383 [00:16<03:18,  1.78it/s]Epoch: 4, train for the 31-th batch, train loss: 0.31460118293762207:   8%|▉           | 31/383 [00:16<03:17,  1.78it/s]Epoch: 18, train for the 110-th batch, train loss: 0.5740785002708435:  72%|███████▏  | 109/151 [00:23<00:09,  4.50it/s]Epoch: 18, train for the 110-th batch, train loss: 0.5740785002708435:  73%|███████▎  | 110/151 [00:23<00:09,  4.52it/s]Epoch: 6, train for the 30-th batch, train loss: 0.8340005278587341:  12%|█▌           | 29/237 [00:15<01:55,  1.80it/s]Epoch: 6, train for the 30-th batch, train loss: 0.8340005278587341:  13%|█▋           | 30/237 [00:15<01:53,  1.83it/s]Epoch: 18, train for the 111-th batch, train loss: 0.5225894451141357:  73%|███████▎  | 110/151 [00:24<00:09,  4.52it/s]Epoch: 18, train for the 111-th batch, train loss: 0.5225894451141357:  74%|███████▎  | 111/151 [00:24<00:08,  4.52it/s]Epoch: 9, train for the 51-th batch, train loss: 0.4671042561531067:  34%|████▍        | 50/146 [00:30<00:58,  1.63it/s]Epoch: 9, train for the 51-th batch, train loss: 0.4671042561531067:  35%|████▌        | 51/146 [00:30<00:58,  1.63it/s]Epoch: 10, train for the 105-th batch, train loss: 0.14876413345336914:  87%|███████▊ | 104/119 [01:03<00:09,  1.63it/s]Epoch: 10, train for the 105-th batch, train loss: 0.14876413345336914:  88%|███████▉ | 105/119 [01:03<00:08,  1.63it/s]Epoch: 18, train for the 112-th batch, train loss: 0.5279785990715027:  74%|███████▎  | 111/151 [00:24<00:08,  4.52it/s]Epoch: 18, train for the 112-th batch, train loss: 0.5279785990715027:  74%|███████▍  | 112/151 [00:24<00:08,  4.51it/s]Epoch: 4, train for the 32-th batch, train loss: 0.2681618928909302:   8%|█            | 31/383 [00:17<03:17,  1.78it/s]Epoch: 4, train for the 32-th batch, train loss: 0.2681618928909302:   8%|█            | 32/383 [00:17<03:17,  1.78it/s]Epoch: 18, train for the 113-th batch, train loss: 0.5643605589866638:  74%|███████▍  | 112/151 [00:24<00:08,  4.51it/s]Epoch: 18, train for the 113-th batch, train loss: 0.5643605589866638:  75%|███████▍  | 113/151 [00:24<00:08,  4.51it/s]Epoch: 6, train for the 31-th batch, train loss: 0.39086151123046875:  13%|█▌          | 30/237 [00:16<01:53,  1.83it/s]Epoch: 6, train for the 31-th batch, train loss: 0.39086151123046875:  13%|█▌          | 31/237 [00:16<01:51,  1.84it/s]Epoch: 9, train for the 52-th batch, train loss: 0.48265936970710754:  35%|████▏       | 51/146 [00:31<00:58,  1.63it/s]Epoch: 9, train for the 52-th batch, train loss: 0.48265936970710754:  36%|████▎       | 52/146 [00:31<00:57,  1.63it/s]Epoch: 18, train for the 114-th batch, train loss: 0.5225310921669006:  75%|███████▍  | 113/151 [00:24<00:08,  4.51it/s]Epoch: 18, train for the 114-th batch, train loss: 0.5225310921669006:  75%|███████▌  | 114/151 [00:24<00:08,  4.49it/s]Epoch: 10, train for the 106-th batch, train loss: 0.12090260535478592:  88%|███████▉ | 105/119 [01:03<00:08,  1.63it/s]Epoch: 10, train for the 106-th batch, train loss: 0.12090260535478592:  89%|████████ | 106/119 [01:03<00:07,  1.63it/s]Epoch: 4, train for the 33-th batch, train loss: 0.22726592421531677:   8%|█           | 32/383 [00:17<03:17,  1.78it/s]Epoch: 4, train for the 33-th batch, train loss: 0.22726592421531677:   9%|█           | 33/383 [00:17<03:14,  1.80it/s]Epoch: 18, train for the 115-th batch, train loss: 0.514512300491333:  75%|████████▎  | 114/151 [00:24<00:08,  4.49it/s]Epoch: 18, train for the 115-th batch, train loss: 0.514512300491333:  76%|████████▍  | 115/151 [00:24<00:08,  4.50it/s]Epoch: 6, train for the 32-th batch, train loss: 0.6488999724388123:  13%|█▋           | 31/237 [00:16<01:51,  1.84it/s]Epoch: 6, train for the 32-th batch, train loss: 0.6488999724388123:  14%|█▊           | 32/237 [00:16<01:52,  1.83it/s]Epoch: 18, train for the 116-th batch, train loss: 0.4959143400192261:  76%|███████▌  | 115/151 [00:25<00:08,  4.50it/s]Epoch: 18, train for the 116-th batch, train loss: 0.4959143400192261:  77%|███████▋  | 116/151 [00:25<00:07,  4.49it/s]Epoch: 9, train for the 53-th batch, train loss: 0.45075783133506775:  36%|████▎       | 52/146 [00:32<00:57,  1.63it/s]Epoch: 9, train for the 53-th batch, train loss: 0.45075783133506775:  36%|████▎       | 53/146 [00:32<00:57,  1.63it/s]Epoch: 18, train for the 117-th batch, train loss: 0.49328768253326416:  77%|██████▉  | 116/151 [00:25<00:07,  4.49it/s]Epoch: 18, train for the 117-th batch, train loss: 0.49328768253326416:  77%|██████▉  | 117/151 [00:25<00:07,  4.50it/s]Epoch: 10, train for the 107-th batch, train loss: 0.209329754114151:  89%|█████████▊ | 106/119 [01:04<00:07,  1.63it/s]Epoch: 10, train for the 107-th batch, train loss: 0.209329754114151:  90%|█████████▉ | 107/119 [01:04<00:07,  1.63it/s]Epoch: 4, train for the 34-th batch, train loss: 0.27166658639907837:   9%|█           | 33/383 [00:18<03:14,  1.80it/s]Epoch: 4, train for the 34-th batch, train loss: 0.27166658639907837:   9%|█           | 34/383 [00:18<03:15,  1.79it/s]Epoch: 18, train for the 118-th batch, train loss: 0.4811197519302368:  77%|███████▋  | 117/151 [00:25<00:07,  4.50it/s]Epoch: 18, train for the 118-th batch, train loss: 0.4811197519302368:  78%|███████▊  | 118/151 [00:25<00:07,  4.49it/s]Epoch: 6, train for the 33-th batch, train loss: 0.5125551223754883:  14%|█▊           | 32/237 [00:17<01:52,  1.83it/s]Epoch: 6, train for the 33-th batch, train loss: 0.5125551223754883:  14%|█▊           | 33/237 [00:17<01:52,  1.82it/s]Epoch: 18, train for the 119-th batch, train loss: 0.5230556130409241:  78%|███████▊  | 118/151 [00:25<00:07,  4.49it/s]Epoch: 18, train for the 119-th batch, train loss: 0.5230556130409241:  79%|███████▉  | 119/151 [00:25<00:07,  4.50it/s]Epoch: 9, train for the 54-th batch, train loss: 0.4953418970108032:  36%|████▋        | 53/146 [00:32<00:57,  1.63it/s]Epoch: 9, train for the 54-th batch, train loss: 0.4953418970108032:  37%|████▊        | 54/146 [00:32<00:56,  1.63it/s]Epoch: 4, train for the 35-th batch, train loss: 0.2916865348815918:   9%|█▏           | 34/383 [00:18<03:15,  1.79it/s]Epoch: 4, train for the 35-th batch, train loss: 0.2916865348815918:   9%|█▏           | 35/383 [00:18<03:10,  1.83it/s]Epoch: 18, train for the 120-th batch, train loss: 0.5918463468551636:  79%|███████▉  | 119/151 [00:26<00:07,  4.50it/s]Epoch: 18, train for the 120-th batch, train loss: 0.5918463468551636:  79%|███████▉  | 120/151 [00:26<00:06,  4.51it/s]Epoch: 10, train for the 108-th batch, train loss: 0.12771373987197876:  90%|████████ | 107/119 [01:04<00:07,  1.63it/s]Epoch: 10, train for the 108-th batch, train loss: 0.12771373987197876:  91%|████████▏| 108/119 [01:04<00:06,  1.63it/s]Epoch: 18, train for the 121-th batch, train loss: 0.5048260688781738:  79%|███████▉  | 120/151 [00:26<00:06,  4.51it/s]Epoch: 18, train for the 121-th batch, train loss: 0.5048260688781738:  80%|████████  | 121/151 [00:26<00:06,  4.52it/s]Epoch: 6, train for the 34-th batch, train loss: 0.5842381715774536:  14%|█▊           | 33/237 [00:17<01:52,  1.82it/s]Epoch: 6, train for the 34-th batch, train loss: 0.5842381715774536:  14%|█▊           | 34/237 [00:17<01:59,  1.70it/s]Epoch: 4, train for the 36-th batch, train loss: 0.29507818818092346:   9%|█           | 35/383 [00:19<03:10,  1.83it/s]Epoch: 4, train for the 36-th batch, train loss: 0.29507818818092346:   9%|█▏          | 36/383 [00:19<03:06,  1.86it/s]Epoch: 18, train for the 122-th batch, train loss: 0.5280761122703552:  80%|████████  | 121/151 [00:26<00:06,  4.52it/s]Epoch: 18, train for the 122-th batch, train loss: 0.5280761122703552:  81%|████████  | 122/151 [00:26<00:06,  4.51it/s]Epoch: 10, train for the 109-th batch, train loss: 0.1954260915517807:  91%|█████████ | 108/119 [01:05<00:06,  1.63it/s]Epoch: 10, train for the 109-th batch, train loss: 0.1954260915517807:  92%|█████████▏| 109/119 [01:05<00:05,  1.71it/s]Epoch: 9, train for the 55-th batch, train loss: 0.5208145380020142:  37%|████▊        | 54/146 [00:33<00:56,  1.63it/s]Epoch: 9, train for the 55-th batch, train loss: 0.5208145380020142:  38%|████▉        | 55/146 [00:33<00:58,  1.55it/s]Epoch: 18, train for the 123-th batch, train loss: 0.5055312514305115:  81%|████████  | 122/151 [00:26<00:06,  4.51it/s]Epoch: 18, train for the 123-th batch, train loss: 0.5055312514305115:  81%|████████▏ | 123/151 [00:26<00:06,  4.04it/s]Epoch: 6, train for the 35-th batch, train loss: 0.4893791377544403:  14%|█▊           | 34/237 [00:18<01:59,  1.70it/s]Epoch: 6, train for the 35-th batch, train loss: 0.4893791377544403:  15%|█▉           | 35/237 [00:18<01:56,  1.73it/s]Epoch: 18, train for the 124-th batch, train loss: 0.5302941203117371:  81%|████████▏ | 123/151 [00:27<00:06,  4.04it/s]Epoch: 18, train for the 124-th batch, train loss: 0.5302941203117371:  82%|████████▏ | 124/151 [00:27<00:06,  4.15it/s]Epoch: 4, train for the 37-th batch, train loss: 0.30909231305122375:   9%|█▏          | 36/383 [00:19<03:06,  1.86it/s]Epoch: 4, train for the 37-th batch, train loss: 0.30909231305122375:  10%|█▏          | 37/383 [00:19<03:09,  1.83it/s]Epoch: 10, train for the 110-th batch, train loss: 0.14765889942646027:  92%|████████▏| 109/119 [01:06<00:05,  1.71it/s]Epoch: 10, train for the 110-th batch, train loss: 0.14765889942646027:  92%|████████▎| 110/119 [01:06<00:05,  1.69it/s]Epoch: 9, train for the 56-th batch, train loss: 0.5250587463378906:  38%|████▉        | 55/146 [00:33<00:58,  1.55it/s]Epoch: 9, train for the 56-th batch, train loss: 0.5250587463378906:  38%|████▉        | 56/146 [00:33<00:56,  1.58it/s]Epoch: 18, train for the 125-th batch, train loss: 0.5233434438705444:  82%|████████▏ | 124/151 [00:27<00:06,  4.15it/s]Epoch: 18, train for the 125-th batch, train loss: 0.5233434438705444:  83%|████████▎ | 125/151 [00:27<00:06,  4.25it/s]Epoch: 18, train for the 126-th batch, train loss: 0.5389487147331238:  83%|████████▎ | 125/151 [00:27<00:06,  4.25it/s]Epoch: 18, train for the 126-th batch, train loss: 0.5389487147331238:  83%|████████▎ | 126/151 [00:27<00:05,  4.32it/s]Epoch: 6, train for the 36-th batch, train loss: 0.5584697723388672:  15%|█▉           | 35/237 [00:19<01:56,  1.73it/s]Epoch: 6, train for the 36-th batch, train loss: 0.5584697723388672:  15%|█▉           | 36/237 [00:19<01:57,  1.71it/s]Epoch: 4, train for the 38-th batch, train loss: 0.2737314999103546:  10%|█▎           | 37/383 [00:20<03:09,  1.83it/s]Epoch: 4, train for the 38-th batch, train loss: 0.2737314999103546:  10%|█▎           | 38/383 [00:20<03:12,  1.79it/s]Epoch: 18, train for the 127-th batch, train loss: 0.5334596633911133:  83%|████████▎ | 126/151 [00:27<00:05,  4.32it/s]Epoch: 18, train for the 127-th batch, train loss: 0.5334596633911133:  84%|████████▍ | 127/151 [00:27<00:05,  4.37it/s]Epoch: 10, train for the 111-th batch, train loss: 0.1780528724193573:  92%|█████████▏| 110/119 [01:06<00:05,  1.69it/s]Epoch: 10, train for the 111-th batch, train loss: 0.1780528724193573:  93%|█████████▎| 111/119 [01:06<00:04,  1.66it/s]Epoch: 9, train for the 57-th batch, train loss: 0.5548591613769531:  38%|████▉        | 56/146 [00:34<00:56,  1.58it/s]Epoch: 9, train for the 57-th batch, train loss: 0.5548591613769531:  39%|█████        | 57/146 [00:34<00:55,  1.60it/s]Epoch: 18, train for the 128-th batch, train loss: 0.5992650985717773:  84%|████████▍ | 127/151 [00:27<00:05,  4.37it/s]Epoch: 18, train for the 128-th batch, train loss: 0.5992650985717773:  85%|████████▍ | 128/151 [00:27<00:05,  4.40it/s]Epoch: 6, train for the 37-th batch, train loss: 0.5942518711090088:  15%|█▉           | 36/237 [00:19<01:57,  1.71it/s]Epoch: 6, train for the 37-th batch, train loss: 0.5942518711090088:  16%|██           | 37/237 [00:19<01:56,  1.72it/s]Epoch: 18, train for the 129-th batch, train loss: 0.5621945261955261:  85%|████████▍ | 128/151 [00:28<00:05,  4.40it/s]Epoch: 18, train for the 129-th batch, train loss: 0.5621945261955261:  85%|████████▌ | 129/151 [00:28<00:04,  4.42it/s]Epoch: 4, train for the 39-th batch, train loss: 0.2516869306564331:  10%|█▎           | 38/383 [00:20<03:12,  1.79it/s]Epoch: 4, train for the 39-th batch, train loss: 0.2516869306564331:  10%|█▎           | 39/383 [00:20<03:14,  1.77it/s]Epoch: 18, train for the 130-th batch, train loss: 0.517416775226593:  85%|█████████▍ | 129/151 [00:28<00:04,  4.42it/s]Epoch: 18, train for the 130-th batch, train loss: 0.517416775226593:  86%|█████████▍ | 130/151 [00:28<00:04,  4.45it/s]Epoch: 10, train for the 112-th batch, train loss: 0.1451268494129181:  93%|█████████▎| 111/119 [01:07<00:04,  1.66it/s]Epoch: 10, train for the 112-th batch, train loss: 0.1451268494129181:  94%|█████████▍| 112/119 [01:07<00:04,  1.62it/s]Epoch: 9, train for the 58-th batch, train loss: 0.48566439747810364:  39%|████▋       | 57/146 [00:35<00:55,  1.60it/s]Epoch: 9, train for the 58-th batch, train loss: 0.48566439747810364:  40%|████▊       | 58/146 [00:35<00:55,  1.60it/s]Epoch: 18, train for the 131-th batch, train loss: 0.5308293700218201:  86%|████████▌ | 130/151 [00:28<00:04,  4.45it/s]Epoch: 18, train for the 131-th batch, train loss: 0.5308293700218201:  87%|████████▋ | 131/151 [00:28<00:04,  4.47it/s]Epoch: 6, train for the 38-th batch, train loss: 0.4906877875328064:  16%|██           | 37/237 [00:20<01:56,  1.72it/s]Epoch: 6, train for the 38-th batch, train loss: 0.4906877875328064:  16%|██           | 38/237 [00:20<01:55,  1.72it/s]Epoch: 4, train for the 40-th batch, train loss: 0.29005372524261475:  10%|█▏          | 39/383 [00:21<03:14,  1.77it/s]Epoch: 18, train for the 132-th batch, train loss: 0.5161669254302979:  87%|████████▋ | 131/151 [00:28<00:04,  4.47it/s]Epoch: 4, train for the 40-th batch, train loss: 0.29005372524261475:  10%|█▎          | 40/383 [00:21<03:15,  1.75it/s]Epoch: 18, train for the 132-th batch, train loss: 0.5161669254302979:  87%|████████▋ | 132/151 [00:28<00:04,  4.48it/s]Epoch: 18, train for the 133-th batch, train loss: 0.5524373650550842:  87%|████████▋ | 132/151 [00:29<00:04,  4.48it/s]Epoch: 18, train for the 133-th batch, train loss: 0.5524373650550842:  88%|████████▊ | 133/151 [00:29<00:04,  4.38it/s]Epoch: 10, train for the 113-th batch, train loss: 0.18384435772895813:  94%|████████▍| 112/119 [01:08<00:04,  1.62it/s]Epoch: 10, train for the 113-th batch, train loss: 0.18384435772895813:  95%|████████▌| 113/119 [01:08<00:03,  1.59it/s]Epoch: 9, train for the 59-th batch, train loss: 0.4537385404109955:  40%|█████▏       | 58/146 [00:35<00:55,  1.60it/s]Epoch: 9, train for the 59-th batch, train loss: 0.4537385404109955:  40%|█████▎       | 59/146 [00:35<00:55,  1.57it/s]Epoch: 6, train for the 39-th batch, train loss: 0.5770359039306641:  16%|██           | 38/237 [00:20<01:55,  1.72it/s]Epoch: 6, train for the 39-th batch, train loss: 0.5770359039306641:  16%|██▏          | 39/237 [00:20<01:56,  1.70it/s]Epoch: 18, train for the 134-th batch, train loss: 0.5472544431686401:  88%|████████▊ | 133/151 [00:29<00:04,  4.38it/s]Epoch: 18, train for the 134-th batch, train loss: 0.5472544431686401:  89%|████████▊ | 134/151 [00:29<00:03,  4.41it/s]Epoch: 4, train for the 41-th batch, train loss: 0.18296673893928528:  10%|█▎          | 40/383 [00:22<03:15,  1.75it/s]Epoch: 4, train for the 41-th batch, train loss: 0.18296673893928528:  11%|█▎          | 41/383 [00:22<03:18,  1.72it/s]Epoch: 18, train for the 135-th batch, train loss: 0.5291152000427246:  89%|████████▊ | 134/151 [00:29<00:03,  4.41it/s]Epoch: 18, train for the 135-th batch, train loss: 0.5291152000427246:  89%|████████▉ | 135/151 [00:29<00:03,  4.42it/s]Epoch: 18, train for the 136-th batch, train loss: 0.5639081597328186:  89%|████████▉ | 135/151 [00:29<00:03,  4.42it/s]Epoch: 18, train for the 136-th batch, train loss: 0.5639081597328186:  90%|█████████ | 136/151 [00:29<00:03,  4.45it/s]Epoch: 10, train for the 114-th batch, train loss: 0.15042930841445923:  95%|████████▌| 113/119 [01:08<00:03,  1.59it/s]Epoch: 10, train for the 114-th batch, train loss: 0.15042930841445923:  96%|████████▌| 114/119 [01:08<00:03,  1.57it/s]Epoch: 9, train for the 60-th batch, train loss: 0.522951066493988:  40%|█████▋        | 59/146 [00:36<00:55,  1.57it/s]Epoch: 9, train for the 60-th batch, train loss: 0.522951066493988:  41%|█████▊        | 60/146 [00:36<00:55,  1.56it/s]Epoch: 6, train for the 40-th batch, train loss: 0.5177860856056213:  16%|██▏          | 39/237 [00:21<01:56,  1.70it/s]Epoch: 6, train for the 40-th batch, train loss: 0.5177860856056213:  17%|██▏          | 40/237 [00:21<01:55,  1.70it/s]Epoch: 18, train for the 137-th batch, train loss: 0.6173876523971558:  90%|█████████ | 136/151 [00:29<00:03,  4.45it/s]Epoch: 18, train for the 137-th batch, train loss: 0.6173876523971558:  91%|█████████ | 137/151 [00:29<00:03,  4.47it/s]Epoch: 4, train for the 42-th batch, train loss: 0.28788721561431885:  11%|█▎          | 41/383 [00:22<03:18,  1.72it/s]Epoch: 4, train for the 42-th batch, train loss: 0.28788721561431885:  11%|█▎          | 42/383 [00:22<03:19,  1.71it/s]Epoch: 18, train for the 138-th batch, train loss: 0.6288307905197144:  91%|█████████ | 137/151 [00:30<00:03,  4.47it/s]Epoch: 18, train for the 138-th batch, train loss: 0.6288307905197144:  91%|█████████▏| 138/151 [00:30<00:02,  4.46it/s]Epoch: 18, train for the 139-th batch, train loss: 0.5710872411727905:  91%|█████████▏| 138/151 [00:30<00:02,  4.46it/s]Epoch: 18, train for the 139-th batch, train loss: 0.5710872411727905:  92%|█████████▏| 139/151 [00:30<00:02,  4.47it/s]Epoch: 6, train for the 41-th batch, train loss: 0.3670699894428253:  17%|██▏          | 40/237 [00:22<01:55,  1.70it/s]Epoch: 6, train for the 41-th batch, train loss: 0.3670699894428253:  17%|██▏          | 41/237 [00:22<01:55,  1.70it/s]Epoch: 10, train for the 115-th batch, train loss: 0.14552201330661774:  96%|████████▌| 114/119 [01:09<00:03,  1.57it/s]Epoch: 9, train for the 61-th batch, train loss: 0.5365743637084961:  41%|█████▎       | 60/146 [00:37<00:55,  1.56it/s]Epoch: 10, train for the 115-th batch, train loss: 0.14552201330661774:  97%|████████▋| 115/119 [01:09<00:02,  1.55it/s]Epoch: 9, train for the 61-th batch, train loss: 0.5365743637084961:  42%|█████▍       | 61/146 [00:37<00:55,  1.54it/s]Epoch: 4, train for the 43-th batch, train loss: 0.2904377579689026:  11%|█▍           | 42/383 [00:23<03:19,  1.71it/s]Epoch: 4, train for the 43-th batch, train loss: 0.2904377579689026:  11%|█▍           | 43/383 [00:23<03:19,  1.70it/s]Epoch: 18, train for the 140-th batch, train loss: 0.5160306692123413:  92%|█████████▏| 139/151 [00:30<00:02,  4.47it/s]Epoch: 18, train for the 140-th batch, train loss: 0.5160306692123413:  93%|█████████▎| 140/151 [00:30<00:02,  4.48it/s]Epoch: 18, train for the 141-th batch, train loss: 0.5591977834701538:  93%|█████████▎| 140/151 [00:30<00:02,  4.48it/s]Epoch: 18, train for the 141-th batch, train loss: 0.5591977834701538:  93%|█████████▎| 141/151 [00:30<00:02,  4.48it/s]Epoch: 6, train for the 42-th batch, train loss: 0.5382723808288574:  17%|██▏          | 41/237 [00:22<01:55,  1.70it/s]Epoch: 6, train for the 42-th batch, train loss: 0.5382723808288574:  18%|██▎          | 42/237 [00:22<01:55,  1.68it/s]Epoch: 18, train for the 142-th batch, train loss: 0.5484520792961121:  93%|█████████▎| 141/151 [00:31<00:02,  4.48it/s]Epoch: 18, train for the 142-th batch, train loss: 0.5484520792961121:  94%|█████████▍| 142/151 [00:31<00:02,  4.45it/s]Epoch: 10, train for the 116-th batch, train loss: 0.09818153083324432:  97%|████████▋| 115/119 [01:10<00:02,  1.55it/s]Epoch: 9, train for the 62-th batch, train loss: 0.47955435514450073:  42%|█████       | 61/146 [00:37<00:55,  1.54it/s]Epoch: 10, train for the 116-th batch, train loss: 0.09818153083324432:  97%|████████▊| 116/119 [01:10<00:01,  1.54it/s]Epoch: 9, train for the 62-th batch, train loss: 0.47955435514450073:  42%|█████       | 62/146 [00:37<00:54,  1.53it/s]Epoch: 4, train for the 44-th batch, train loss: 0.25130486488342285:  11%|█▎          | 43/383 [00:23<03:19,  1.70it/s]Epoch: 4, train for the 44-th batch, train loss: 0.25130486488342285:  11%|█▍          | 44/383 [00:23<03:20,  1.69it/s]Epoch: 18, train for the 143-th batch, train loss: 0.5003564357757568:  94%|█████████▍| 142/151 [00:31<00:02,  4.45it/s]Epoch: 18, train for the 143-th batch, train loss: 0.5003564357757568:  95%|█████████▍| 143/151 [00:31<00:01,  4.46it/s]Epoch: 18, train for the 144-th batch, train loss: 0.5041329860687256:  95%|█████████▍| 143/151 [00:31<00:01,  4.46it/s]Epoch: 18, train for the 144-th batch, train loss: 0.5041329860687256:  95%|█████████▌| 144/151 [00:31<00:01,  4.46it/s]Epoch: 6, train for the 43-th batch, train loss: 0.3832363784313202:  18%|██▎          | 42/237 [00:23<01:55,  1.68it/s]Epoch: 6, train for the 43-th batch, train loss: 0.3832363784313202:  18%|██▎          | 43/237 [00:23<01:54,  1.69it/s]Epoch: 18, train for the 145-th batch, train loss: 0.5301991105079651:  95%|█████████▌| 144/151 [00:31<00:01,  4.46it/s]Epoch: 18, train for the 145-th batch, train loss: 0.5301991105079651:  96%|█████████▌| 145/151 [00:31<00:01,  4.47it/s]Epoch: 10, train for the 117-th batch, train loss: 0.18559661507606506:  97%|████████▊| 116/119 [01:10<00:01,  1.54it/s]Epoch: 9, train for the 63-th batch, train loss: 0.49851828813552856:  42%|█████       | 62/146 [00:38<00:54,  1.53it/s]Epoch: 10, train for the 117-th batch, train loss: 0.18559661507606506:  98%|████████▊| 117/119 [01:10<00:01,  1.53it/s]Epoch: 9, train for the 63-th batch, train loss: 0.49851828813552856:  43%|█████▏      | 63/146 [00:38<00:54,  1.52it/s]Epoch: 4, train for the 45-th batch, train loss: 0.26727423071861267:  11%|█▍          | 44/383 [00:24<03:20,  1.69it/s]Epoch: 4, train for the 45-th batch, train loss: 0.26727423071861267:  12%|█▍          | 45/383 [00:24<03:20,  1.69it/s]Epoch: 18, train for the 146-th batch, train loss: 0.5508281588554382:  96%|█████████▌| 145/151 [00:31<00:01,  4.47it/s]Epoch: 18, train for the 146-th batch, train loss: 0.5508281588554382:  97%|█████████▋| 146/151 [00:31<00:01,  4.47it/s]Epoch: 18, train for the 147-th batch, train loss: 0.5359060168266296:  97%|█████████▋| 146/151 [00:32<00:01,  4.47it/s]Epoch: 18, train for the 147-th batch, train loss: 0.5359060168266296:  97%|█████████▋| 147/151 [00:32<00:00,  4.49it/s]Epoch: 6, train for the 44-th batch, train loss: 0.3288869857788086:  18%|██▎          | 43/237 [00:23<01:54,  1.69it/s]Epoch: 6, train for the 44-th batch, train loss: 0.3288869857788086:  19%|██▍          | 44/237 [00:23<01:53,  1.70it/s]Epoch: 4, train for the 46-th batch, train loss: 0.3992763161659241:  12%|█▌           | 45/383 [00:25<03:20,  1.69it/s]Epoch: 4, train for the 46-th batch, train loss: 0.3992763161659241:  12%|█▌           | 46/383 [00:25<03:18,  1.69it/s]Epoch: 18, train for the 148-th batch, train loss: 0.5647744536399841:  97%|█████████▋| 147/151 [00:32<00:00,  4.49it/s]Epoch: 18, train for the 148-th batch, train loss: 0.5647744536399841:  98%|█████████▊| 148/151 [00:32<00:00,  4.51it/s]Epoch: 10, train for the 118-th batch, train loss: 0.13567280769348145:  98%|████████▊| 117/119 [01:11<00:01,  1.53it/s]Epoch: 9, train for the 64-th batch, train loss: 0.5080808997154236:  43%|█████▌       | 63/146 [00:39<00:54,  1.52it/s]Epoch: 10, train for the 118-th batch, train loss: 0.13567280769348145:  99%|████████▉| 118/119 [01:11<00:00,  1.52it/s]Epoch: 9, train for the 64-th batch, train loss: 0.5080808997154236:  44%|█████▋       | 64/146 [00:39<00:54,  1.52it/s]Epoch: 18, train for the 149-th batch, train loss: 0.5113980174064636:  98%|█████████▊| 148/151 [00:32<00:00,  4.51it/s]Epoch: 18, train for the 149-th batch, train loss: 0.5113980174064636:  99%|█████████▊| 149/151 [00:32<00:00,  4.50it/s]Epoch: 10, train for the 119-th batch, train loss: 0.13913460075855255:  99%|████████▉| 118/119 [01:11<00:00,  1.52it/s]Epoch: 10, train for the 119-th batch, train loss: 0.13913460075855255: 100%|█████████| 119/119 [01:11<00:00,  1.82it/s]Epoch: 10, train for the 119-th batch, train loss: 0.13913460075855255: 100%|█████████| 119/119 [01:11<00:00,  1.66it/s]
  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 6, train for the 45-th batch, train loss: 0.8218839764595032:  19%|██▍          | 44/237 [00:24<01:53,  1.70it/s]Epoch: 6, train for the 45-th batch, train loss: 0.8218839764595032:  19%|██▍          | 45/237 [00:24<01:55,  1.67it/s]Epoch: 18, train for the 150-th batch, train loss: 0.5139051079750061:  99%|█████████▊| 149/151 [00:32<00:00,  4.50it/s]Epoch: 18, train for the 150-th batch, train loss: 0.5139051079750061:  99%|█████████▉| 150/151 [00:32<00:00,  4.49it/s]Epoch: 4, train for the 47-th batch, train loss: 0.2496405988931656:  12%|█▌           | 46/383 [00:25<03:18,  1.69it/s]Epoch: 4, train for the 47-th batch, train loss: 0.2496405988931656:  12%|█▌           | 47/383 [00:25<03:19,  1.69it/s]Epoch: 18, train for the 151-th batch, train loss: 0.5905606150627136:  99%|█████████▉| 150/151 [00:32<00:00,  4.49it/s]Epoch: 18, train for the 151-th batch, train loss: 0.5905606150627136: 100%|██████████| 151/151 [00:32<00:00,  4.97it/s]Epoch: 18, train for the 151-th batch, train loss: 0.5905606150627136: 100%|██████████| 151/151 [00:32<00:00,  4.58it/s]
  0%|                                                                                            | 0/46 [00:00<?, ?it/s]Epoch: 9, train for the 65-th batch, train loss: 0.5484418272972107:  44%|█████▋       | 64/146 [00:39<00:54,  1.52it/s]Epoch: 9, train for the 65-th batch, train loss: 0.5484418272972107:  45%|█████▊       | 65/146 [00:39<00:51,  1.58it/s]evaluate for the 1-th batch, evaluate loss: 0.12610912322998047:   0%|                           | 0/40 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.12610912322998047:   2%|▍                  | 1/40 [00:00<00:12,  3.21it/s]evaluate for the 1-th batch, evaluate loss: 0.499576598405838:   0%|                             | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.499576598405838:   2%|▍                    | 1/46 [00:00<00:04,  9.51it/s]evaluate for the 2-th batch, evaluate loss: 0.5060963034629822:   2%|▍                   | 1/46 [00:00<00:04,  9.51it/s]evaluate for the 2-th batch, evaluate loss: 0.5060963034629822:   4%|▊                   | 2/46 [00:00<00:04,  9.56it/s]evaluate for the 2-th batch, evaluate loss: 0.16464106738567352:   2%|▍                  | 1/40 [00:00<00:12,  3.21it/s]evaluate for the 2-th batch, evaluate loss: 0.16464106738567352:   5%|▉                  | 2/40 [00:00<00:10,  3.57it/s]evaluate for the 3-th batch, evaluate loss: 0.4860283136367798:   4%|▊                   | 2/46 [00:00<00:04,  9.56it/s]evaluate for the 3-th batch, evaluate loss: 0.4860283136367798:   7%|█▎                  | 3/46 [00:00<00:04,  9.52it/s]Epoch: 6, train for the 46-th batch, train loss: 0.3192598223686218:  19%|██▍          | 45/237 [00:24<01:55,  1.67it/s]Epoch: 6, train for the 46-th batch, train loss: 0.3192598223686218:  19%|██▌          | 46/237 [00:24<01:52,  1.70it/s]evaluate for the 4-th batch, evaluate loss: 0.5094971060752869:   7%|█▎                  | 3/46 [00:00<00:04,  9.52it/s]evaluate for the 4-th batch, evaluate loss: 0.5094971060752869:   9%|█▋                  | 4/46 [00:00<00:04,  9.53it/s]evaluate for the 5-th batch, evaluate loss: 0.4832775890827179:   9%|█▋                  | 4/46 [00:00<00:04,  9.53it/s]evaluate for the 5-th batch, evaluate loss: 0.4832775890827179:  11%|██▏                 | 5/46 [00:00<00:04,  9.53it/s]Epoch: 4, train for the 48-th batch, train loss: 0.2539326548576355:  12%|█▌           | 47/383 [00:26<03:19,  1.69it/s]Epoch: 4, train for the 48-th batch, train loss: 0.2539326548576355:  13%|█▋           | 48/383 [00:26<03:15,  1.71it/s]Epoch: 9, train for the 66-th batch, train loss: 0.5237354636192322:  45%|█████▊       | 65/146 [00:40<00:51,  1.58it/s]Epoch: 9, train for the 66-th batch, train loss: 0.5237354636192322:  45%|█████▉       | 66/146 [00:40<00:50,  1.60it/s]evaluate for the 3-th batch, evaluate loss: 0.20298078656196594:   5%|▉                  | 2/40 [00:00<00:10,  3.57it/s]evaluate for the 3-th batch, evaluate loss: 0.20298078656196594:   8%|█▍                 | 3/40 [00:00<00:11,  3.35it/s]evaluate for the 6-th batch, evaluate loss: 0.5532283186912537:  11%|██▏                 | 5/46 [00:00<00:04,  9.53it/s]evaluate for the 6-th batch, evaluate loss: 0.5532283186912537:  13%|██▌                 | 6/46 [00:00<00:04,  9.56it/s]evaluate for the 7-th batch, evaluate loss: 0.46927782893180847:  13%|██▍                | 6/46 [00:00<00:04,  9.56it/s]evaluate for the 7-th batch, evaluate loss: 0.46927782893180847:  15%|██▉                | 7/46 [00:00<00:04,  9.59it/s]evaluate for the 8-th batch, evaluate loss: 0.5579553842544556:  15%|███                 | 7/46 [00:00<00:04,  9.59it/s]evaluate for the 8-th batch, evaluate loss: 0.5579553842544556:  17%|███▍                | 8/46 [00:00<00:03,  9.60it/s]evaluate for the 4-th batch, evaluate loss: 0.13762402534484863:   8%|█▍                 | 3/40 [00:01<00:11,  3.35it/s]evaluate for the 4-th batch, evaluate loss: 0.13762402534484863:  10%|█▉                 | 4/40 [00:01<00:10,  3.48it/s]evaluate for the 9-th batch, evaluate loss: 0.5241187810897827:  17%|███▍                | 8/46 [00:00<00:03,  9.60it/s]evaluate for the 9-th batch, evaluate loss: 0.5241187810897827:  20%|███▉                | 9/46 [00:00<00:03,  9.60it/s]Epoch: 6, train for the 47-th batch, train loss: 0.3320159912109375:  19%|██▌          | 46/237 [00:25<01:52,  1.70it/s]Epoch: 6, train for the 47-th batch, train loss: 0.3320159912109375:  20%|██▌          | 47/237 [00:25<01:50,  1.72it/s]evaluate for the 10-th batch, evaluate loss: 0.5314407348632812:  20%|███▋               | 9/46 [00:01<00:03,  9.60it/s]evaluate for the 10-th batch, evaluate loss: 0.5314407348632812:  22%|███▉              | 10/46 [00:01<00:03,  9.60it/s]Epoch: 4, train for the 49-th batch, train loss: 0.2785463035106659:  13%|█▋           | 48/383 [00:26<03:15,  1.71it/s]Epoch: 4, train for the 49-th batch, train loss: 0.2785463035106659:  13%|█▋           | 49/383 [00:26<03:14,  1.71it/s]evaluate for the 11-th batch, evaluate loss: 0.5222621560096741:  22%|███▉              | 10/46 [00:01<00:03,  9.60it/s]evaluate for the 11-th batch, evaluate loss: 0.5222621560096741:  24%|████▎             | 11/46 [00:01<00:03,  9.64it/s]evaluate for the 5-th batch, evaluate loss: 0.17182178795337677:  10%|█▉                 | 4/40 [00:01<00:10,  3.48it/s]evaluate for the 5-th batch, evaluate loss: 0.17182178795337677:  12%|██▍                | 5/40 [00:01<00:10,  3.44it/s]Epoch: 9, train for the 67-th batch, train loss: 0.5590216517448425:  45%|█████▉       | 66/146 [00:40<00:50,  1.60it/s]Epoch: 9, train for the 67-th batch, train loss: 0.5590216517448425:  46%|█████▉       | 67/146 [00:40<00:48,  1.62it/s]evaluate for the 12-th batch, evaluate loss: 0.47624024748802185:  24%|████             | 11/46 [00:01<00:03,  9.64it/s]evaluate for the 12-th batch, evaluate loss: 0.47624024748802185:  26%|████▍            | 12/46 [00:01<00:03,  9.55it/s]evaluate for the 13-th batch, evaluate loss: 0.496672660112381:  26%|████▉              | 12/46 [00:01<00:03,  9.55it/s]evaluate for the 13-th batch, evaluate loss: 0.496672660112381:  28%|█████▎             | 13/46 [00:01<00:03,  9.56it/s]evaluate for the 6-th batch, evaluate loss: 0.16254860162734985:  12%|██▍                | 5/40 [00:01<00:10,  3.44it/s]evaluate for the 6-th batch, evaluate loss: 0.16254860162734985:  15%|██▊                | 6/40 [00:01<00:09,  3.60it/s]evaluate for the 14-th batch, evaluate loss: 0.5891621112823486:  28%|█████             | 13/46 [00:01<00:03,  9.56it/s]evaluate for the 14-th batch, evaluate loss: 0.5891621112823486:  30%|█████▍            | 14/46 [00:01<00:03,  9.58it/s]evaluate for the 15-th batch, evaluate loss: 0.5408719778060913:  30%|█████▍            | 14/46 [00:01<00:03,  9.58it/s]evaluate for the 15-th batch, evaluate loss: 0.5408719778060913:  33%|█████▊            | 15/46 [00:01<00:03,  9.58it/s]Epoch: 6, train for the 48-th batch, train loss: 0.6350029706954956:  20%|██▌          | 47/237 [00:26<01:50,  1.72it/s]Epoch: 6, train for the 48-th batch, train loss: 0.6350029706954956:  20%|██▋          | 48/237 [00:26<01:52,  1.68it/s]evaluate for the 16-th batch, evaluate loss: 0.5692290663719177:  33%|█████▊            | 15/46 [00:01<00:03,  9.58it/s]evaluate for the 16-th batch, evaluate loss: 0.5692290663719177:  35%|██████▎           | 16/46 [00:01<00:03,  9.60it/s]evaluate for the 7-th batch, evaluate loss: 0.11114077270030975:  15%|██▊                | 6/40 [00:02<00:09,  3.60it/s]evaluate for the 7-th batch, evaluate loss: 0.11114077270030975:  18%|███▎               | 7/40 [00:02<00:09,  3.55it/s]Epoch: 4, train for the 50-th batch, train loss: 0.22183318436145782:  13%|█▌          | 49/383 [00:27<03:14,  1.71it/s]Epoch: 4, train for the 50-th batch, train loss: 0.22183318436145782:  13%|█▌          | 50/383 [00:27<03:16,  1.69it/s]evaluate for the 17-th batch, evaluate loss: 0.455202579498291:  35%|██████▌            | 16/46 [00:01<00:03,  9.60it/s]evaluate for the 17-th batch, evaluate loss: 0.455202579498291:  37%|███████            | 17/46 [00:01<00:03,  9.60it/s]Epoch: 9, train for the 68-th batch, train loss: 0.5207914710044861:  46%|█████▉       | 67/146 [00:41<00:48,  1.62it/s]Epoch: 9, train for the 68-th batch, train loss: 0.5207914710044861:  47%|██████       | 68/146 [00:41<00:47,  1.63it/s]evaluate for the 18-th batch, evaluate loss: 0.49946174025535583:  37%|██████▎          | 17/46 [00:01<00:03,  9.60it/s]evaluate for the 18-th batch, evaluate loss: 0.49946174025535583:  39%|██████▋          | 18/46 [00:01<00:02,  9.59it/s]evaluate for the 19-th batch, evaluate loss: 0.5227143168449402:  39%|███████           | 18/46 [00:01<00:02,  9.59it/s]evaluate for the 19-th batch, evaluate loss: 0.5227143168449402:  41%|███████▍          | 19/46 [00:01<00:02,  9.58it/s]evaluate for the 8-th batch, evaluate loss: 0.12661314010620117:  18%|███▎               | 7/40 [00:02<00:09,  3.55it/s]evaluate for the 8-th batch, evaluate loss: 0.12661314010620117:  20%|███▊               | 8/40 [00:02<00:08,  3.64it/s]Epoch: 6, train for the 49-th batch, train loss: 0.46090567111968994:  20%|██▍         | 48/237 [00:26<01:52,  1.68it/s]Epoch: 6, train for the 49-th batch, train loss: 0.46090567111968994:  21%|██▍         | 49/237 [00:26<01:45,  1.78it/s]evaluate for the 20-th batch, evaluate loss: 0.5375612378120422:  41%|███████▍          | 19/46 [00:02<00:02,  9.58it/s]evaluate for the 20-th batch, evaluate loss: 0.5375612378120422:  43%|███████▊          | 20/46 [00:02<00:02,  9.59it/s]evaluate for the 21-th batch, evaluate loss: 0.5274130702018738:  43%|███████▊          | 20/46 [00:02<00:02,  9.59it/s]evaluate for the 21-th batch, evaluate loss: 0.5274130702018738:  46%|████████▏         | 21/46 [00:02<00:02,  9.60it/s]evaluate for the 9-th batch, evaluate loss: 0.17395702004432678:  20%|███▊               | 8/40 [00:02<00:08,  3.64it/s]evaluate for the 9-th batch, evaluate loss: 0.17395702004432678:  22%|████▎              | 9/40 [00:02<00:08,  3.59it/s]evaluate for the 22-th batch, evaluate loss: 0.5222168564796448:  46%|████████▏         | 21/46 [00:02<00:02,  9.60it/s]evaluate for the 22-th batch, evaluate loss: 0.5222168564796448:  48%|████████▌         | 22/46 [00:02<00:02,  9.59it/s]evaluate for the 23-th batch, evaluate loss: 0.47556936740875244:  48%|████████▏        | 22/46 [00:02<00:02,  9.59it/s]evaluate for the 23-th batch, evaluate loss: 0.47556936740875244:  50%|████████▌        | 23/46 [00:02<00:02,  9.58it/s]Epoch: 9, train for the 69-th batch, train loss: 0.5421255230903625:  47%|██████       | 68/146 [00:42<00:47,  1.63it/s]Epoch: 9, train for the 69-th batch, train loss: 0.5421255230903625:  47%|██████▏      | 69/146 [00:42<00:47,  1.64it/s]Epoch: 6, train for the 50-th batch, train loss: 0.4701228141784668:  21%|██▋          | 49/237 [00:27<01:45,  1.78it/s]Epoch: 6, train for the 50-th batch, train loss: 0.4701228141784668:  21%|██▋          | 50/237 [00:27<01:36,  1.94it/s]evaluate for the 24-th batch, evaluate loss: 0.48372718691825867:  50%|████████▌        | 23/46 [00:02<00:02,  9.58it/s]evaluate for the 24-th batch, evaluate loss: 0.48372718691825867:  52%|████████▊        | 24/46 [00:02<00:02,  9.61it/s]evaluate for the 10-th batch, evaluate loss: 0.195445254445076:  22%|████▌               | 9/40 [00:02<00:08,  3.59it/s]evaluate for the 10-th batch, evaluate loss: 0.195445254445076:  25%|████▊              | 10/40 [00:02<00:08,  3.53it/s]Epoch: 4, train for the 51-th batch, train loss: 0.2856447994709015:  13%|█▋           | 50/383 [00:28<03:16,  1.69it/s]Epoch: 4, train for the 51-th batch, train loss: 0.2856447994709015:  13%|█▋           | 51/383 [00:28<03:40,  1.51it/s]evaluate for the 25-th batch, evaluate loss: 0.5363033413887024:  52%|█████████▍        | 24/46 [00:02<00:02,  9.61it/s]evaluate for the 25-th batch, evaluate loss: 0.5363033413887024:  54%|█████████▊        | 25/46 [00:02<00:02,  9.62it/s]evaluate for the 26-th batch, evaluate loss: 0.5574970841407776:  54%|█████████▊        | 25/46 [00:02<00:02,  9.62it/s]evaluate for the 26-th batch, evaluate loss: 0.5574970841407776:  57%|██████████▏       | 26/46 [00:02<00:02,  9.59it/s]evaluate for the 27-th batch, evaluate loss: 0.4941042363643646:  57%|██████████▏       | 26/46 [00:02<00:02,  9.59it/s]evaluate for the 27-th batch, evaluate loss: 0.4941042363643646:  59%|██████████▌       | 27/46 [00:02<00:01,  9.60it/s]evaluate for the 11-th batch, evaluate loss: 0.16488122940063477:  25%|████▎            | 10/40 [00:03<00:08,  3.53it/s]evaluate for the 11-th batch, evaluate loss: 0.16488122940063477:  28%|████▋            | 11/40 [00:03<00:08,  3.58it/s]evaluate for the 28-th batch, evaluate loss: 0.523294985294342:  59%|███████████▏       | 27/46 [00:02<00:01,  9.60it/s]evaluate for the 28-th batch, evaluate loss: 0.523294985294342:  61%|███████████▌       | 28/46 [00:02<00:01,  9.61it/s]Epoch: 6, train for the 51-th batch, train loss: 0.43881934881210327:  21%|██▌         | 50/237 [00:27<01:36,  1.94it/s]Epoch: 6, train for the 51-th batch, train loss: 0.43881934881210327:  22%|██▌         | 51/237 [00:27<01:32,  2.01it/s]evaluate for the 29-th batch, evaluate loss: 0.49401533603668213:  61%|██████████▎      | 28/46 [00:03<00:01,  9.61it/s]evaluate for the 29-th batch, evaluate loss: 0.49401533603668213:  63%|██████████▋      | 29/46 [00:03<00:01,  9.64it/s]Epoch: 9, train for the 70-th batch, train loss: 0.5599014759063721:  47%|██████▏      | 69/146 [00:42<00:47,  1.64it/s]Epoch: 9, train for the 70-th batch, train loss: 0.5599014759063721:  48%|██████▏      | 70/146 [00:42<00:46,  1.64it/s]evaluate for the 30-th batch, evaluate loss: 0.49255430698394775:  63%|██████████▋      | 29/46 [00:03<00:01,  9.64it/s]evaluate for the 30-th batch, evaluate loss: 0.49255430698394775:  65%|███████████      | 30/46 [00:03<00:01,  9.66it/s]evaluate for the 12-th batch, evaluate loss: 0.1384352445602417:  28%|████▉             | 11/40 [00:03<00:08,  3.58it/s]evaluate for the 12-th batch, evaluate loss: 0.1384352445602417:  30%|█████▍            | 12/40 [00:03<00:07,  3.50it/s]Epoch: 4, train for the 52-th batch, train loss: 0.3404797315597534:  13%|█▋           | 51/383 [00:28<03:40,  1.51it/s]Epoch: 4, train for the 52-th batch, train loss: 0.3404797315597534:  14%|█▊           | 52/383 [00:28<03:31,  1.57it/s]evaluate for the 31-th batch, evaluate loss: 0.5185744166374207:  65%|███████████▋      | 30/46 [00:03<00:01,  9.66it/s]evaluate for the 31-th batch, evaluate loss: 0.5185744166374207:  67%|████████████▏     | 31/46 [00:03<00:01,  9.64it/s]evaluate for the 32-th batch, evaluate loss: 0.479026198387146:  67%|████████████▊      | 31/46 [00:03<00:01,  9.64it/s]evaluate for the 32-th batch, evaluate loss: 0.479026198387146:  70%|█████████████▏     | 32/46 [00:03<00:01,  9.62it/s]evaluate for the 13-th batch, evaluate loss: 0.12700597941875458:  30%|█████            | 12/40 [00:03<00:07,  3.50it/s]evaluate for the 13-th batch, evaluate loss: 0.12700597941875458:  32%|█████▌           | 13/40 [00:03<00:07,  3.66it/s]evaluate for the 33-th batch, evaluate loss: 0.5026213526725769:  70%|████████████▌     | 32/46 [00:03<00:01,  9.62it/s]evaluate for the 33-th batch, evaluate loss: 0.5026213526725769:  72%|████████████▉     | 33/46 [00:03<00:01,  9.63it/s]Epoch: 6, train for the 52-th batch, train loss: 0.7025021910667419:  22%|██▊          | 51/237 [00:28<01:32,  2.01it/s]Epoch: 6, train for the 52-th batch, train loss: 0.7025021910667419:  22%|██▊          | 52/237 [00:28<01:36,  1.91it/s]evaluate for the 34-th batch, evaluate loss: 0.4858051836490631:  72%|████████████▉     | 33/46 [00:03<00:01,  9.63it/s]evaluate for the 34-th batch, evaluate loss: 0.4858051836490631:  74%|█████████████▎    | 34/46 [00:03<00:01,  9.53it/s]Epoch: 9, train for the 71-th batch, train loss: 0.5434895753860474:  48%|██████▏      | 70/146 [00:43<00:46,  1.64it/s]Epoch: 9, train for the 71-th batch, train loss: 0.5434895753860474:  49%|██████▎      | 71/146 [00:43<00:45,  1.64it/s]evaluate for the 35-th batch, evaluate loss: 0.4864462614059448:  74%|█████████████▎    | 34/46 [00:03<00:01,  9.53it/s]evaluate for the 35-th batch, evaluate loss: 0.4864462614059448:  76%|█████████████▋    | 35/46 [00:03<00:01,  9.54it/s]evaluate for the 14-th batch, evaluate loss: 0.15504060685634613:  32%|█████▌           | 13/40 [00:03<00:07,  3.66it/s]evaluate for the 14-th batch, evaluate loss: 0.15504060685634613:  35%|█████▉           | 14/40 [00:03<00:07,  3.53it/s]Epoch: 4, train for the 53-th batch, train loss: 0.3109092712402344:  14%|█▊           | 52/383 [00:29<03:31,  1.57it/s]Epoch: 4, train for the 53-th batch, train loss: 0.3109092712402344:  14%|█▊           | 53/383 [00:29<03:23,  1.62it/s]evaluate for the 36-th batch, evaluate loss: 0.46994903683662415:  76%|████████████▉    | 35/46 [00:03<00:01,  9.54it/s]evaluate for the 36-th batch, evaluate loss: 0.46994903683662415:  78%|█████████████▎   | 36/46 [00:03<00:01,  9.56it/s]evaluate for the 37-th batch, evaluate loss: 0.5013080835342407:  78%|██████████████    | 36/46 [00:03<00:01,  9.56it/s]evaluate for the 37-th batch, evaluate loss: 0.5013080835342407:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.59it/s]evaluate for the 15-th batch, evaluate loss: 0.17441190779209137:  35%|█████▉           | 14/40 [00:04<00:07,  3.53it/s]evaluate for the 15-th batch, evaluate loss: 0.17441190779209137:  38%|██████▍          | 15/40 [00:04<00:06,  3.68it/s]evaluate for the 38-th batch, evaluate loss: 0.5339244604110718:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.59it/s]evaluate for the 38-th batch, evaluate loss: 0.5339244604110718:  83%|██████████████▊   | 38/46 [00:03<00:00,  9.60it/s]evaluate for the 39-th batch, evaluate loss: 0.5361456871032715:  83%|██████████████▊   | 38/46 [00:04<00:00,  9.60it/s]evaluate for the 39-th batch, evaluate loss: 0.5361456871032715:  85%|███████████████▎  | 39/46 [00:04<00:00,  9.60it/s]Epoch: 6, train for the 53-th batch, train loss: 0.685075581073761:  22%|███           | 52/237 [00:28<01:36,  1.91it/s]Epoch: 6, train for the 53-th batch, train loss: 0.685075581073761:  22%|███▏          | 53/237 [00:28<01:41,  1.81it/s]evaluate for the 40-th batch, evaluate loss: 0.470330148935318:  85%|████████████████   | 39/46 [00:04<00:00,  9.60it/s]evaluate for the 40-th batch, evaluate loss: 0.470330148935318:  87%|████████████████▌  | 40/46 [00:04<00:00,  9.61it/s]Epoch: 9, train for the 72-th batch, train loss: 0.5231189131736755:  49%|██████▎      | 71/146 [00:43<00:45,  1.64it/s]Epoch: 9, train for the 72-th batch, train loss: 0.5231189131736755:  49%|██████▍      | 72/146 [00:43<00:45,  1.64it/s]evaluate for the 16-th batch, evaluate loss: 0.19551420211791992:  38%|██████▍          | 15/40 [00:04<00:06,  3.68it/s]evaluate for the 16-th batch, evaluate loss: 0.19551420211791992:  40%|██████▊          | 16/40 [00:04<00:06,  3.50it/s]evaluate for the 41-th batch, evaluate loss: 0.4831902086734772:  87%|███████████████▋  | 40/46 [00:04<00:00,  9.61it/s]evaluate for the 41-th batch, evaluate loss: 0.4831902086734772:  89%|████████████████  | 41/46 [00:04<00:00,  9.60it/s]Epoch: 4, train for the 54-th batch, train loss: 0.2509852349758148:  14%|█▊           | 53/383 [00:30<03:23,  1.62it/s]Epoch: 4, train for the 54-th batch, train loss: 0.2509852349758148:  14%|█▊           | 54/383 [00:30<03:18,  1.66it/s]evaluate for the 42-th batch, evaluate loss: 0.47175467014312744:  89%|███████████████▏ | 41/46 [00:04<00:00,  9.60it/s]evaluate for the 42-th batch, evaluate loss: 0.47175467014312744:  91%|███████████████▌ | 42/46 [00:04<00:00,  9.61it/s]evaluate for the 43-th batch, evaluate loss: 0.5315892100334167:  91%|████████████████▍ | 42/46 [00:04<00:00,  9.61it/s]evaluate for the 43-th batch, evaluate loss: 0.5315892100334167:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.60it/s]evaluate for the 17-th batch, evaluate loss: 0.12696272134780884:  40%|██████▊          | 16/40 [00:04<00:06,  3.50it/s]evaluate for the 17-th batch, evaluate loss: 0.12696272134780884:  42%|███████▏         | 17/40 [00:04<00:06,  3.55it/s]evaluate for the 44-th batch, evaluate loss: 0.517197847366333:  93%|█████████████████▊ | 43/46 [00:04<00:00,  9.60it/s]evaluate for the 44-th batch, evaluate loss: 0.517197847366333:  96%|██████████████████▏| 44/46 [00:04<00:00,  9.59it/s]evaluate for the 45-th batch, evaluate loss: 0.49854910373687744:  96%|████████████████▎| 44/46 [00:04<00:00,  9.59it/s]evaluate for the 45-th batch, evaluate loss: 0.49854910373687744:  98%|████████████████▋| 45/46 [00:04<00:00,  9.59it/s]Epoch: 6, train for the 54-th batch, train loss: 0.4134213626384735:  22%|██▉          | 53/237 [00:29<01:41,  1.81it/s]Epoch: 6, train for the 54-th batch, train loss: 0.4134213626384735:  23%|██▉          | 54/237 [00:29<01:42,  1.78it/s]evaluate for the 46-th batch, evaluate loss: 0.5044090747833252:  98%|█████████████████▌| 45/46 [00:04<00:00,  9.59it/s]evaluate for the 46-th batch, evaluate loss: 0.5044090747833252: 100%|██████████████████| 46/46 [00:04<00:00,  9.62it/s]
  0%|                                                                                            | 0/25 [00:00<?, ?it/s]evaluate for the 18-th batch, evaluate loss: 0.1363292783498764:  42%|███████▋          | 17/40 [00:05<00:06,  3.55it/s]evaluate for the 18-th batch, evaluate loss: 0.1363292783498764:  45%|████████          | 18/40 [00:05<00:06,  3.50it/s]Epoch: 9, train for the 73-th batch, train loss: 0.5124478936195374:  49%|██████▍      | 72/146 [00:44<00:45,  1.64it/s]Epoch: 9, train for the 73-th batch, train loss: 0.5124478936195374:  50%|██████▌      | 73/146 [00:44<00:44,  1.65it/s]Epoch: 4, train for the 55-th batch, train loss: 0.34958887100219727:  14%|█▋          | 54/383 [00:30<03:18,  1.66it/s]Epoch: 4, train for the 55-th batch, train loss: 0.34958887100219727:  14%|█▋          | 55/383 [00:30<03:17,  1.66it/s]evaluate for the 1-th batch, evaluate loss: 0.6377544403076172:   0%|                            | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6377544403076172:   4%|▊                   | 1/25 [00:00<00:02,  9.25it/s]evaluate for the 2-th batch, evaluate loss: 0.6504631042480469:   4%|▊                   | 1/25 [00:00<00:02,  9.25it/s]evaluate for the 2-th batch, evaluate loss: 0.6504631042480469:   8%|█▌                  | 2/25 [00:00<00:02,  9.16it/s]evaluate for the 19-th batch, evaluate loss: 0.14098244905471802:  45%|███████▋         | 18/40 [00:05<00:06,  3.50it/s]evaluate for the 19-th batch, evaluate loss: 0.14098244905471802:  48%|████████         | 19/40 [00:05<00:05,  3.64it/s]evaluate for the 3-th batch, evaluate loss: 0.6897961497306824:   8%|█▌                  | 2/25 [00:00<00:02,  9.16it/s]evaluate for the 3-th batch, evaluate loss: 0.6897961497306824:  12%|██▍                 | 3/25 [00:00<00:02,  9.17it/s]evaluate for the 4-th batch, evaluate loss: 0.672644317150116:  12%|██▌                  | 3/25 [00:00<00:02,  9.17it/s]evaluate for the 4-th batch, evaluate loss: 0.672644317150116:  16%|███▎                 | 4/25 [00:00<00:02,  9.18it/s]evaluate for the 5-th batch, evaluate loss: 0.6727259755134583:  16%|███▏                | 4/25 [00:00<00:02,  9.18it/s]evaluate for the 5-th batch, evaluate loss: 0.6727259755134583:  20%|████                | 5/25 [00:00<00:02,  9.21it/s]Epoch: 6, train for the 55-th batch, train loss: 0.49588534235954285:  23%|██▋         | 54/237 [00:29<01:42,  1.78it/s]Epoch: 6, train for the 55-th batch, train loss: 0.49588534235954285:  23%|██▊         | 55/237 [00:29<01:44,  1.74it/s]evaluate for the 20-th batch, evaluate loss: 0.14819373190402985:  48%|████████         | 19/40 [00:05<00:05,  3.64it/s]evaluate for the 20-th batch, evaluate loss: 0.14819373190402985:  50%|████████▌        | 20/40 [00:05<00:05,  3.59it/s]evaluate for the 6-th batch, evaluate loss: 0.7124664187431335:  20%|████                | 5/25 [00:00<00:02,  9.21it/s]evaluate for the 6-th batch, evaluate loss: 0.7124664187431335:  24%|████▊               | 6/25 [00:00<00:02,  9.16it/s]Epoch: 9, train for the 74-th batch, train loss: 0.5315420627593994:  50%|██████▌      | 73/146 [00:45<00:44,  1.65it/s]Epoch: 9, train for the 74-th batch, train loss: 0.5315420627593994:  51%|██████▌      | 74/146 [00:45<00:43,  1.65it/s]Epoch: 4, train for the 56-th batch, train loss: 0.30648747086524963:  14%|█▋          | 55/383 [00:31<03:17,  1.66it/s]Epoch: 4, train for the 56-th batch, train loss: 0.30648747086524963:  15%|█▊          | 56/383 [00:31<03:16,  1.66it/s]evaluate for the 7-th batch, evaluate loss: 0.7378202080726624:  24%|████▊               | 6/25 [00:00<00:02,  9.16it/s]evaluate for the 7-th batch, evaluate loss: 0.7378202080726624:  28%|█████▌              | 7/25 [00:00<00:01,  9.16it/s]evaluate for the 21-th batch, evaluate loss: 0.09180328249931335:  50%|████████▌        | 20/40 [00:05<00:05,  3.59it/s]evaluate for the 21-th batch, evaluate loss: 0.09180328249931335:  52%|████████▉        | 21/40 [00:05<00:05,  3.67it/s]evaluate for the 8-th batch, evaluate loss: 0.7159037590026855:  28%|█████▌              | 7/25 [00:00<00:01,  9.16it/s]evaluate for the 8-th batch, evaluate loss: 0.7159037590026855:  32%|██████▍             | 8/25 [00:00<00:01,  9.16it/s]evaluate for the 9-th batch, evaluate loss: 0.701397716999054:  32%|██████▋              | 8/25 [00:00<00:01,  9.16it/s]evaluate for the 9-th batch, evaluate loss: 0.701397716999054:  36%|███████▌             | 9/25 [00:00<00:01,  9.18it/s]evaluate for the 10-th batch, evaluate loss: 0.730993390083313:  36%|███████▏            | 9/25 [00:01<00:01,  9.18it/s]evaluate for the 10-th batch, evaluate loss: 0.730993390083313:  40%|███████▌           | 10/25 [00:01<00:01,  9.19it/s]evaluate for the 22-th batch, evaluate loss: 0.14296157658100128:  52%|████████▉        | 21/40 [00:06<00:05,  3.67it/s]evaluate for the 22-th batch, evaluate loss: 0.14296157658100128:  55%|█████████▎       | 22/40 [00:06<00:04,  3.61it/s]Epoch: 6, train for the 56-th batch, train loss: 0.6244927644729614:  23%|███          | 55/237 [00:30<01:44,  1.74it/s]Epoch: 6, train for the 56-th batch, train loss: 0.6244927644729614:  24%|███          | 56/237 [00:30<01:46,  1.70it/s]evaluate for the 11-th batch, evaluate loss: 0.7323307394981384:  40%|███████▏          | 10/25 [00:01<00:01,  9.19it/s]evaluate for the 11-th batch, evaluate loss: 0.7323307394981384:  44%|███████▉          | 11/25 [00:01<00:01,  9.19it/s]Epoch: 9, train for the 75-th batch, train loss: 0.5434551239013672:  51%|██████▌      | 74/146 [00:45<00:43,  1.65it/s]Epoch: 9, train for the 75-th batch, train loss: 0.5434551239013672:  51%|██████▋      | 75/146 [00:45<00:43,  1.65it/s]evaluate for the 12-th batch, evaluate loss: 0.6968429684638977:  44%|███████▉          | 11/25 [00:01<00:01,  9.19it/s]evaluate for the 12-th batch, evaluate loss: 0.6968429684638977:  48%|████████▋         | 12/25 [00:01<00:01,  9.19it/s]Epoch: 4, train for the 57-th batch, train loss: 0.28756365180015564:  15%|█▊          | 56/383 [00:31<03:16,  1.66it/s]Epoch: 4, train for the 57-th batch, train loss: 0.28756365180015564:  15%|█▊          | 57/383 [00:31<03:17,  1.65it/s]evaluate for the 13-th batch, evaluate loss: 0.6609456539154053:  48%|████████▋         | 12/25 [00:01<00:01,  9.19it/s]evaluate for the 13-th batch, evaluate loss: 0.6609456539154053:  52%|█████████▎        | 13/25 [00:01<00:01,  9.19it/s]evaluate for the 23-th batch, evaluate loss: 0.1484108716249466:  55%|█████████▉        | 22/40 [00:06<00:04,  3.61it/s]evaluate for the 23-th batch, evaluate loss: 0.1484108716249466:  57%|██████████▎       | 23/40 [00:06<00:04,  3.54it/s]evaluate for the 14-th batch, evaluate loss: 0.7474821209907532:  52%|█████████▎        | 13/25 [00:01<00:01,  9.19it/s]evaluate for the 14-th batch, evaluate loss: 0.7474821209907532:  56%|██████████        | 14/25 [00:01<00:01,  9.18it/s]evaluate for the 15-th batch, evaluate loss: 0.7291357517242432:  56%|██████████        | 14/25 [00:01<00:01,  9.18it/s]evaluate for the 15-th batch, evaluate loss: 0.7291357517242432:  60%|██████████▊       | 15/25 [00:01<00:01,  9.18it/s]Epoch: 6, train for the 57-th batch, train loss: 0.5973097085952759:  24%|███          | 56/237 [00:31<01:46,  1.70it/s]Epoch: 6, train for the 57-th batch, train loss: 0.5973097085952759:  24%|███▏         | 57/237 [00:31<01:40,  1.79it/s]evaluate for the 24-th batch, evaluate loss: 0.14732588827610016:  57%|█████████▊       | 23/40 [00:06<00:04,  3.54it/s]evaluate for the 24-th batch, evaluate loss: 0.14732588827610016:  60%|██████████▏      | 24/40 [00:06<00:04,  3.57it/s]evaluate for the 16-th batch, evaluate loss: 0.6586133241653442:  60%|██████████▊       | 15/25 [00:01<00:01,  9.18it/s]evaluate for the 16-th batch, evaluate loss: 0.6586133241653442:  64%|███████████▌      | 16/25 [00:01<00:00,  9.18it/s]evaluate for the 17-th batch, evaluate loss: 0.6627213954925537:  64%|███████████▌      | 16/25 [00:01<00:00,  9.18it/s]evaluate for the 17-th batch, evaluate loss: 0.6627213954925537:  68%|████████████▏     | 17/25 [00:01<00:00,  9.18it/s]Epoch: 9, train for the 76-th batch, train loss: 0.5583189725875854:  51%|██████▋      | 75/146 [00:46<00:43,  1.65it/s]Epoch: 9, train for the 76-th batch, train loss: 0.5583189725875854:  52%|██████▊      | 76/146 [00:46<00:42,  1.65it/s]evaluate for the 18-th batch, evaluate loss: 0.6251447796821594:  68%|████████████▏     | 17/25 [00:01<00:00,  9.18it/s]evaluate for the 18-th batch, evaluate loss: 0.6251447796821594:  72%|████████████▉     | 18/25 [00:01<00:00,  9.19it/s]evaluate for the 25-th batch, evaluate loss: 0.14632530510425568:  60%|██████████▏      | 24/40 [00:07<00:04,  3.57it/s]evaluate for the 25-th batch, evaluate loss: 0.14632530510425568:  62%|██████████▋      | 25/40 [00:07<00:04,  3.50it/s]Epoch: 4, train for the 58-th batch, train loss: 0.2568172514438629:  15%|█▉           | 57/383 [00:32<03:17,  1.65it/s]Epoch: 4, train for the 58-th batch, train loss: 0.2568172514438629:  15%|█▉           | 58/383 [00:32<03:26,  1.57it/s]evaluate for the 19-th batch, evaluate loss: 0.5908768773078918:  72%|████████████▉     | 18/25 [00:02<00:00,  9.19it/s]evaluate for the 19-th batch, evaluate loss: 0.5908768773078918:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.18it/s]Epoch: 6, train for the 58-th batch, train loss: 0.44793713092803955:  24%|██▉         | 57/237 [00:31<01:40,  1.79it/s]Epoch: 6, train for the 58-th batch, train loss: 0.44793713092803955:  24%|██▉         | 58/237 [00:31<01:37,  1.83it/s]evaluate for the 20-th batch, evaluate loss: 0.6614596247673035:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.18it/s]evaluate for the 20-th batch, evaluate loss: 0.6614596247673035:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.19it/s]evaluate for the 26-th batch, evaluate loss: 0.11855699121952057:  62%|██████████▋      | 25/40 [00:07<00:04,  3.50it/s]evaluate for the 26-th batch, evaluate loss: 0.11855699121952057:  65%|███████████      | 26/40 [00:07<00:03,  3.68it/s]evaluate for the 21-th batch, evaluate loss: 0.7219451665878296:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.19it/s]evaluate for the 21-th batch, evaluate loss: 0.7219451665878296:  84%|███████████████   | 21/25 [00:02<00:00,  9.19it/s]evaluate for the 22-th batch, evaluate loss: 0.607038140296936:  84%|███████████████▉   | 21/25 [00:02<00:00,  9.19it/s]evaluate for the 22-th batch, evaluate loss: 0.607038140296936:  88%|████████████████▋  | 22/25 [00:02<00:00,  9.21it/s]Epoch: 9, train for the 77-th batch, train loss: 0.5640769004821777:  52%|██████▊      | 76/146 [00:46<00:42,  1.65it/s]Epoch: 9, train for the 77-th batch, train loss: 0.5640769004821777:  53%|██████▊      | 77/146 [00:46<00:41,  1.65it/s]evaluate for the 23-th batch, evaluate loss: 0.665045440196991:  88%|████████████████▋  | 22/25 [00:02<00:00,  9.21it/s]evaluate for the 23-th batch, evaluate loss: 0.665045440196991:  92%|█████████████████▍ | 23/25 [00:02<00:00,  9.16it/s]evaluate for the 27-th batch, evaluate loss: 0.13936911523342133:  65%|███████████      | 26/40 [00:07<00:03,  3.68it/s]evaluate for the 27-th batch, evaluate loss: 0.13936911523342133:  68%|███████████▍     | 27/40 [00:07<00:03,  3.55it/s]evaluate for the 24-th batch, evaluate loss: 0.6577383875846863:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.16it/s]evaluate for the 24-th batch, evaluate loss: 0.6577383875846863:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.17it/s]Epoch: 4, train for the 59-th batch, train loss: 0.27357855439186096:  15%|█▊          | 58/383 [00:33<03:26,  1.57it/s]Epoch: 4, train for the 59-th batch, train loss: 0.27357855439186096:  15%|█▊          | 59/383 [00:33<03:21,  1.61it/s]evaluate for the 25-th batch, evaluate loss: 0.7014673948287964:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.17it/s]evaluate for the 25-th batch, evaluate loss: 0.7014673948287964: 100%|██████████████████| 25/25 [00:02<00:00,  9.25it/s]
INFO:root:Epoch: 18, learning rate: 0.0001, train loss: 0.5729
INFO:root:train average_precision, 0.8113
INFO:root:train roc_auc, 0.7761
INFO:root:validate loss: 0.5093
INFO:root:validate average_precision, 0.8427
INFO:root:validate roc_auc, 0.8051
INFO:root:new node validate loss: 0.6816
INFO:root:new node validate first_1_average_precision, 0.5906
INFO:root:new node validate first_1_roc_auc, 0.5418
INFO:root:new node validate first_3_average_precision, 0.6737
INFO:root:new node validate first_3_roc_auc, 0.6362
INFO:root:new node validate first_10_average_precision, 0.7431
INFO:root:new node validate first_10_roc_auc, 0.7093
INFO:root:new node validate average_precision, 0.7064
INFO:root:new node validate roc_auc, 0.6571
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]Epoch: 6, train for the 59-th batch, train loss: 0.6336880922317505:  24%|███▏         | 58/237 [00:32<01:37,  1.83it/s]Epoch: 6, train for the 59-th batch, train loss: 0.6336880922317505:  25%|███▏         | 59/237 [00:32<01:39,  1.78it/s]evaluate for the 28-th batch, evaluate loss: 0.11016649007797241:  68%|███████████▍     | 27/40 [00:07<00:03,  3.55it/s]evaluate for the 28-th batch, evaluate loss: 0.11016649007797241:  70%|███████████▉     | 28/40 [00:07<00:03,  3.70it/s]Epoch: 19, train for the 1-th batch, train loss: 1.137842059135437:   0%|                       | 0/151 [00:00<?, ?it/s]Epoch: 19, train for the 1-th batch, train loss: 1.137842059135437:   1%|               | 1/151 [00:00<00:26,  5.75it/s]Epoch: 19, train for the 2-th batch, train loss: 1.1702572107315063:   1%|              | 1/151 [00:00<00:26,  5.75it/s]Epoch: 19, train for the 2-th batch, train loss: 1.1702572107315063:   1%|▏             | 2/151 [00:00<00:26,  5.66it/s]Epoch: 9, train for the 78-th batch, train loss: 0.5398582220077515:  53%|██████▊      | 77/146 [00:47<00:41,  1.65it/s]Epoch: 9, train for the 78-th batch, train loss: 0.5398582220077515:  53%|██████▉      | 78/146 [00:47<00:41,  1.65it/s]evaluate for the 29-th batch, evaluate loss: 0.15997035801410675:  70%|███████████▉     | 28/40 [00:08<00:03,  3.70it/s]evaluate for the 29-th batch, evaluate loss: 0.15997035801410675:  72%|████████████▎    | 29/40 [00:08<00:03,  3.50it/s]Epoch: 4, train for the 60-th batch, train loss: 0.31671202182769775:  15%|█▊          | 59/383 [00:33<03:21,  1.61it/s]Epoch: 4, train for the 60-th batch, train loss: 0.31671202182769775:  16%|█▉          | 60/383 [00:33<03:17,  1.64it/s]Epoch: 19, train for the 3-th batch, train loss: 0.527044415473938:   1%|▏              | 2/151 [00:00<00:26,  5.66it/s]Epoch: 19, train for the 3-th batch, train loss: 0.527044415473938:   2%|▎              | 3/151 [00:00<00:25,  5.87it/s]Epoch: 6, train for the 60-th batch, train loss: 0.39829376339912415:  25%|██▉         | 59/237 [00:32<01:39,  1.78it/s]Epoch: 6, train for the 60-th batch, train loss: 0.39829376339912415:  25%|███         | 60/237 [00:32<01:39,  1.77it/s]evaluate for the 30-th batch, evaluate loss: 0.1644015908241272:  72%|█████████████     | 29/40 [00:08<00:03,  3.50it/s]evaluate for the 30-th batch, evaluate loss: 0.1644015908241272:  75%|█████████████▌    | 30/40 [00:08<00:02,  3.57it/s]Epoch: 19, train for the 4-th batch, train loss: 0.4085038900375366:   2%|▎             | 3/151 [00:00<00:25,  5.87it/s]Epoch: 19, train for the 4-th batch, train loss: 0.4085038900375366:   3%|▎             | 4/151 [00:00<00:24,  5.92it/s]Epoch: 19, train for the 5-th batch, train loss: 0.5169379115104675:   3%|▎             | 4/151 [00:00<00:24,  5.92it/s]Epoch: 19, train for the 5-th batch, train loss: 0.5169379115104675:   3%|▍             | 5/151 [00:00<00:25,  5.68it/s]evaluate for the 31-th batch, evaluate loss: 0.1620594710111618:  75%|█████████████▌    | 30/40 [00:08<00:02,  3.57it/s]evaluate for the 31-th batch, evaluate loss: 0.1620594710111618:  78%|█████████████▉    | 31/40 [00:08<00:02,  3.52it/s]Epoch: 9, train for the 79-th batch, train loss: 0.5452282428741455:  53%|██████▉      | 78/146 [00:48<00:41,  1.65it/s]Epoch: 9, train for the 79-th batch, train loss: 0.5452282428741455:  54%|███████      | 79/146 [00:48<00:40,  1.65it/s]Epoch: 19, train for the 6-th batch, train loss: 0.7600975036621094:   3%|▍             | 5/151 [00:01<00:25,  5.68it/s]Epoch: 19, train for the 6-th batch, train loss: 0.7600975036621094:   4%|▌             | 6/151 [00:01<00:26,  5.38it/s]Epoch: 4, train for the 61-th batch, train loss: 0.2928391098976135:  16%|██           | 60/383 [00:34<03:17,  1.64it/s]Epoch: 4, train for the 61-th batch, train loss: 0.2928391098976135:  16%|██           | 61/383 [00:34<03:14,  1.66it/s]evaluate for the 32-th batch, evaluate loss: 0.13526242971420288:  78%|█████████████▏   | 31/40 [00:08<00:02,  3.52it/s]evaluate for the 32-th batch, evaluate loss: 0.13526242971420288:  80%|█████████████▌   | 32/40 [00:08<00:02,  3.64it/s]Epoch: 6, train for the 61-th batch, train loss: 0.712454080581665:  25%|███▌          | 60/237 [00:33<01:39,  1.77it/s]Epoch: 6, train for the 61-th batch, train loss: 0.712454080581665:  26%|███▌          | 61/237 [00:33<01:40,  1.74it/s]Epoch: 19, train for the 7-th batch, train loss: 0.5563756227493286:   4%|▌             | 6/151 [00:01<00:26,  5.38it/s]Epoch: 19, train for the 7-th batch, train loss: 0.5563756227493286:   5%|▋             | 7/151 [00:01<00:27,  5.28it/s]Epoch: 19, train for the 8-th batch, train loss: 0.901191234588623:   5%|▋              | 7/151 [00:01<00:27,  5.28it/s]Epoch: 19, train for the 8-th batch, train loss: 0.901191234588623:   5%|▊              | 8/151 [00:01<00:27,  5.16it/s]evaluate for the 33-th batch, evaluate loss: 0.13783837854862213:  80%|█████████████▌   | 32/40 [00:09<00:02,  3.64it/s]evaluate for the 33-th batch, evaluate loss: 0.13783837854862213:  82%|██████████████   | 33/40 [00:09<00:01,  3.60it/s]Epoch: 9, train for the 80-th batch, train loss: 0.5723685622215271:  54%|███████      | 79/146 [00:48<00:40,  1.65it/s]Epoch: 9, train for the 80-th batch, train loss: 0.5723685622215271:  55%|███████      | 80/146 [00:48<00:39,  1.65it/s]Epoch: 19, train for the 9-th batch, train loss: 0.5480555891990662:   5%|▋             | 8/151 [00:01<00:27,  5.16it/s]Epoch: 19, train for the 9-th batch, train loss: 0.5480555891990662:   6%|▊             | 9/151 [00:01<00:27,  5.17it/s]Epoch: 4, train for the 62-th batch, train loss: 0.3503180742263794:  16%|██           | 61/383 [00:34<03:14,  1.66it/s]Epoch: 4, train for the 62-th batch, train loss: 0.3503180742263794:  16%|██           | 62/383 [00:34<03:12,  1.67it/s]evaluate for the 34-th batch, evaluate loss: 0.10981131345033646:  82%|██████████████   | 33/40 [00:09<00:01,  3.60it/s]evaluate for the 34-th batch, evaluate loss: 0.10981131345033646:  85%|██████████████▍  | 34/40 [00:09<00:01,  3.66it/s]Epoch: 6, train for the 62-th batch, train loss: 0.6535944938659668:  26%|███▎         | 61/237 [00:33<01:40,  1.74it/s]Epoch: 6, train for the 62-th batch, train loss: 0.6535944938659668:  26%|███▍         | 62/237 [00:33<01:41,  1.72it/s]Epoch: 19, train for the 10-th batch, train loss: 0.6810426115989685:   6%|▊            | 9/151 [00:01<00:27,  5.17it/s]Epoch: 19, train for the 10-th batch, train loss: 0.6810426115989685:   7%|▊           | 10/151 [00:01<00:28,  5.00it/s]evaluate for the 35-th batch, evaluate loss: 0.14703930914402008:  85%|██████████████▍  | 34/40 [00:09<00:01,  3.66it/s]evaluate for the 35-th batch, evaluate loss: 0.14703930914402008:  88%|██████████████▉  | 35/40 [00:09<00:01,  3.60it/s]Epoch: 19, train for the 11-th batch, train loss: 0.5270417928695679:   7%|▊           | 10/151 [00:02<00:28,  5.00it/s]Epoch: 19, train for the 11-th batch, train loss: 0.5270417928695679:   7%|▊           | 11/151 [00:02<00:28,  4.98it/s]Epoch: 9, train for the 81-th batch, train loss: 0.5122094750404358:  55%|███████      | 80/146 [00:49<00:39,  1.65it/s]Epoch: 9, train for the 81-th batch, train loss: 0.5122094750404358:  55%|███████▏     | 81/146 [00:49<00:39,  1.65it/s]Epoch: 4, train for the 63-th batch, train loss: 0.326715886592865:  16%|██▎           | 62/383 [00:35<03:12,  1.67it/s]Epoch: 4, train for the 63-th batch, train loss: 0.326715886592865:  16%|██▎           | 63/383 [00:35<03:11,  1.67it/s]Epoch: 19, train for the 12-th batch, train loss: 0.6171964406967163:   7%|▊           | 11/151 [00:02<00:28,  4.98it/s]Epoch: 19, train for the 12-th batch, train loss: 0.6171964406967163:   8%|▉           | 12/151 [00:02<00:27,  4.97it/s]evaluate for the 36-th batch, evaluate loss: 0.1455189734697342:  88%|███████████████▊  | 35/40 [00:10<00:01,  3.60it/s]evaluate for the 36-th batch, evaluate loss: 0.1455189734697342:  90%|████████████████▏ | 36/40 [00:10<00:01,  3.51it/s]Epoch: 6, train for the 63-th batch, train loss: 0.5709494948387146:  26%|███▍         | 62/237 [00:34<01:41,  1.72it/s]Epoch: 6, train for the 63-th batch, train loss: 0.5709494948387146:  27%|███▍         | 63/237 [00:34<01:41,  1.71it/s]Epoch: 19, train for the 13-th batch, train loss: 0.5882555246353149:   8%|▉           | 12/151 [00:02<00:27,  4.97it/s]Epoch: 19, train for the 13-th batch, train loss: 0.5882555246353149:   9%|█           | 13/151 [00:02<00:27,  4.99it/s]evaluate for the 37-th batch, evaluate loss: 0.19837477803230286:  90%|███████████████▎ | 36/40 [00:10<00:01,  3.51it/s]evaluate for the 37-th batch, evaluate loss: 0.19837477803230286:  92%|███████████████▋ | 37/40 [00:10<00:00,  3.63it/s]Epoch: 19, train for the 14-th batch, train loss: 0.5416548848152161:   9%|█           | 13/151 [00:02<00:27,  4.99it/s]Epoch: 19, train for the 14-th batch, train loss: 0.5416548848152161:   9%|█           | 14/151 [00:02<00:27,  4.99it/s]Epoch: 9, train for the 82-th batch, train loss: 0.5462402701377869:  55%|███████▏     | 81/146 [00:50<00:39,  1.65it/s]Epoch: 9, train for the 82-th batch, train loss: 0.5462402701377869:  56%|███████▎     | 82/146 [00:50<00:38,  1.65it/s]Epoch: 4, train for the 64-th batch, train loss: 0.32480597496032715:  16%|█▉          | 63/383 [00:36<03:11,  1.67it/s]Epoch: 4, train for the 64-th batch, train loss: 0.32480597496032715:  17%|██          | 64/383 [00:36<03:10,  1.67it/s]Epoch: 19, train for the 15-th batch, train loss: 0.4893578290939331:   9%|█           | 14/151 [00:02<00:27,  4.99it/s]Epoch: 19, train for the 15-th batch, train loss: 0.4893578290939331:  10%|█▏          | 15/151 [00:02<00:27,  4.95it/s]evaluate for the 38-th batch, evaluate loss: 0.12621735036373138:  92%|███████████████▋ | 37/40 [00:10<00:00,  3.63it/s]evaluate for the 38-th batch, evaluate loss: 0.12621735036373138:  95%|████████████████▏| 38/40 [00:10<00:00,  3.54it/s]Epoch: 6, train for the 64-th batch, train loss: 0.5068826079368591:  27%|███▍         | 63/237 [00:35<01:41,  1.71it/s]Epoch: 6, train for the 64-th batch, train loss: 0.5068826079368591:  27%|███▌         | 64/237 [00:35<01:40,  1.72it/s]Epoch: 19, train for the 16-th batch, train loss: 0.44015753269195557:  10%|█          | 15/151 [00:03<00:27,  4.95it/s]Epoch: 19, train for the 16-th batch, train loss: 0.44015753269195557:  11%|█▏         | 16/151 [00:03<00:27,  4.95it/s]evaluate for the 39-th batch, evaluate loss: 0.14523833990097046:  95%|████████████████▏| 38/40 [00:10<00:00,  3.54it/s]evaluate for the 39-th batch, evaluate loss: 0.14523833990097046:  98%|████████████████▌| 39/40 [00:10<00:00,  3.73it/s]evaluate for the 40-th batch, evaluate loss: 0.05791564658284187:  98%|████████████████▌| 39/40 [00:10<00:00,  3.73it/s]evaluate for the 40-th batch, evaluate loss: 0.05791564658284187: 100%|█████████████████| 40/40 [00:10<00:00,  3.66it/s]
  0%|                                                                                            | 0/21 [00:00<?, ?it/s]Epoch: 19, train for the 17-th batch, train loss: 0.644156813621521:  11%|█▍           | 16/151 [00:03<00:27,  4.95it/s]Epoch: 19, train for the 17-th batch, train loss: 0.644156813621521:  11%|█▍           | 17/151 [00:03<00:27,  4.92it/s]Epoch: 9, train for the 83-th batch, train loss: 0.5241527557373047:  56%|███████▎     | 82/146 [00:50<00:38,  1.65it/s]Epoch: 9, train for the 83-th batch, train loss: 0.5241527557373047:  57%|███████▍     | 83/146 [00:50<00:37,  1.66it/s]Epoch: 4, train for the 65-th batch, train loss: 0.29285967350006104:  17%|██          | 64/383 [00:36<03:10,  1.67it/s]Epoch: 4, train for the 65-th batch, train loss: 0.29285967350006104:  17%|██          | 65/383 [00:36<03:08,  1.69it/s]evaluate for the 1-th batch, evaluate loss: 0.20140692591667175:   0%|                           | 0/21 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.20140692591667175:   5%|▉                  | 1/21 [00:00<00:06,  3.27it/s]Epoch: 19, train for the 18-th batch, train loss: 0.5044246315956116:  11%|█▎          | 17/151 [00:03<00:27,  4.92it/s]Epoch: 19, train for the 18-th batch, train loss: 0.5044246315956116:  12%|█▍          | 18/151 [00:03<00:26,  4.95it/s]Epoch: 6, train for the 65-th batch, train loss: 0.6249688863754272:  27%|███▌         | 64/237 [00:35<01:40,  1.72it/s]Epoch: 6, train for the 65-th batch, train loss: 0.6249688863754272:  27%|███▌         | 65/237 [00:35<01:40,  1.71it/s]Epoch: 19, train for the 19-th batch, train loss: 0.5484023690223694:  12%|█▍          | 18/151 [00:03<00:26,  4.95it/s]Epoch: 19, train for the 19-th batch, train loss: 0.5484023690223694:  13%|█▌          | 19/151 [00:03<00:26,  4.92it/s]evaluate for the 2-th batch, evaluate loss: 0.2265472561120987:   5%|▉                   | 1/21 [00:00<00:06,  3.27it/s]evaluate for the 2-th batch, evaluate loss: 0.2265472561120987:  10%|█▉                  | 2/21 [00:00<00:05,  3.73it/s]Epoch: 19, train for the 20-th batch, train loss: 0.30755382776260376:  13%|█▍         | 19/151 [00:03<00:26,  4.92it/s]Epoch: 19, train for the 20-th batch, train loss: 0.30755382776260376:  13%|█▍         | 20/151 [00:03<00:26,  4.92it/s]Epoch: 9, train for the 84-th batch, train loss: 0.49628207087516785:  57%|██████▊     | 83/146 [00:51<00:37,  1.66it/s]Epoch: 9, train for the 84-th batch, train loss: 0.49628207087516785:  58%|██████▉     | 84/146 [00:51<00:37,  1.66it/s]evaluate for the 3-th batch, evaluate loss: 0.2339194416999817:  10%|█▉                  | 2/21 [00:00<00:05,  3.73it/s]evaluate for the 3-th batch, evaluate loss: 0.2339194416999817:  14%|██▊                 | 3/21 [00:00<00:05,  3.43it/s]Epoch: 4, train for the 66-th batch, train loss: 0.3390491306781769:  17%|██▏          | 65/383 [00:37<03:08,  1.69it/s]Epoch: 4, train for the 66-th batch, train loss: 0.3390491306781769:  17%|██▏          | 66/383 [00:37<03:07,  1.69it/s]Epoch: 19, train for the 21-th batch, train loss: 0.3903198838233948:  13%|█▌          | 20/151 [00:04<00:26,  4.92it/s]Epoch: 19, train for the 21-th batch, train loss: 0.3903198838233948:  14%|█▋          | 21/151 [00:04<00:26,  4.91it/s]Epoch: 6, train for the 66-th batch, train loss: 0.6119965314865112:  27%|███▌         | 65/237 [00:36<01:40,  1.71it/s]Epoch: 6, train for the 66-th batch, train loss: 0.6119965314865112:  28%|███▌         | 66/237 [00:36<01:40,  1.70it/s]evaluate for the 4-th batch, evaluate loss: 0.20362325012683868:  14%|██▋                | 3/21 [00:01<00:05,  3.43it/s]evaluate for the 4-th batch, evaluate loss: 0.20362325012683868:  19%|███▌               | 4/21 [00:01<00:04,  3.56it/s]Epoch: 19, train for the 22-th batch, train loss: 0.7079331874847412:  14%|█▋          | 21/151 [00:04<00:26,  4.91it/s]Epoch: 19, train for the 22-th batch, train loss: 0.7079331874847412:  15%|█▋          | 22/151 [00:04<00:26,  4.80it/s]Epoch: 19, train for the 23-th batch, train loss: 0.7488758563995361:  15%|█▋          | 22/151 [00:04<00:26,  4.80it/s]Epoch: 19, train for the 23-th batch, train loss: 0.7488758563995361:  15%|█▊          | 23/151 [00:04<00:27,  4.73it/s]Epoch: 9, train for the 85-th batch, train loss: 0.510789692401886:  58%|████████      | 84/146 [00:51<00:37,  1.66it/s]Epoch: 9, train for the 85-th batch, train loss: 0.510789692401886:  58%|████████▏     | 85/146 [00:51<00:36,  1.66it/s]evaluate for the 5-th batch, evaluate loss: 0.20139457285404205:  19%|███▌               | 4/21 [00:01<00:04,  3.56it/s]evaluate for the 5-th batch, evaluate loss: 0.20139457285404205:  24%|████▌              | 5/21 [00:01<00:04,  3.37it/s]Epoch: 4, train for the 67-th batch, train loss: 0.31400400400161743:  17%|██          | 66/383 [00:37<03:07,  1.69it/s]Epoch: 4, train for the 67-th batch, train loss: 0.31400400400161743:  17%|██          | 67/383 [00:37<03:08,  1.68it/s]Epoch: 19, train for the 24-th batch, train loss: 0.3711816370487213:  15%|█▊          | 23/151 [00:04<00:27,  4.73it/s]Epoch: 19, train for the 24-th batch, train loss: 0.3711816370487213:  16%|█▉          | 24/151 [00:04<00:26,  4.75it/s]Epoch: 6, train for the 67-th batch, train loss: 0.5536853671073914:  28%|███▌         | 66/237 [00:36<01:40,  1.70it/s]Epoch: 6, train for the 67-th batch, train loss: 0.5536853671073914:  28%|███▋         | 67/237 [00:36<01:40,  1.68it/s]evaluate for the 6-th batch, evaluate loss: 0.2205103039741516:  24%|████▊               | 5/21 [00:01<00:04,  3.37it/s]evaluate for the 6-th batch, evaluate loss: 0.2205103039741516:  29%|█████▋              | 6/21 [00:01<00:04,  3.47it/s]Epoch: 19, train for the 25-th batch, train loss: 0.48839911818504333:  16%|█▋         | 24/151 [00:04<00:26,  4.75it/s]Epoch: 19, train for the 25-th batch, train loss: 0.48839911818504333:  17%|█▊         | 25/151 [00:04<00:26,  4.76it/s]Epoch: 19, train for the 26-th batch, train loss: 0.7436079382896423:  17%|█▉          | 25/151 [00:05<00:26,  4.76it/s]Epoch: 19, train for the 26-th batch, train loss: 0.7436079382896423:  17%|██          | 26/151 [00:05<00:26,  4.68it/s]Epoch: 9, train for the 86-th batch, train loss: 0.5543578267097473:  58%|███████▌     | 85/146 [00:52<00:36,  1.66it/s]Epoch: 9, train for the 86-th batch, train loss: 0.5543578267097473:  59%|███████▋     | 86/146 [00:52<00:36,  1.66it/s]evaluate for the 7-th batch, evaluate loss: 0.22490645945072174:  29%|█████▍             | 6/21 [00:02<00:04,  3.47it/s]evaluate for the 7-th batch, evaluate loss: 0.22490645945072174:  33%|██████▎            | 7/21 [00:02<00:04,  3.36it/s]Epoch: 4, train for the 68-th batch, train loss: 0.31378981471061707:  17%|██          | 67/383 [00:38<03:08,  1.68it/s]Epoch: 4, train for the 68-th batch, train loss: 0.31378981471061707:  18%|██▏         | 68/383 [00:38<03:08,  1.67it/s]Epoch: 6, train for the 68-th batch, train loss: 0.6277042627334595:  28%|███▋         | 67/237 [00:37<01:40,  1.68it/s]Epoch: 6, train for the 68-th batch, train loss: 0.6277042627334595:  29%|███▋         | 68/237 [00:37<01:41,  1.67it/s]Epoch: 19, train for the 27-th batch, train loss: 0.6461847424507141:  17%|██          | 26/151 [00:05<00:26,  4.68it/s]Epoch: 19, train for the 27-th batch, train loss: 0.6461847424507141:  18%|██▏         | 27/151 [00:05<00:26,  4.65it/s]evaluate for the 8-th batch, evaluate loss: 0.2644953429698944:  33%|██████▋             | 7/21 [00:02<00:04,  3.36it/s]evaluate for the 8-th batch, evaluate loss: 0.2644953429698944:  38%|███████▌            | 8/21 [00:02<00:03,  3.43it/s]Epoch: 19, train for the 28-th batch, train loss: 0.6412983536720276:  18%|██▏         | 27/151 [00:05<00:26,  4.65it/s]Epoch: 19, train for the 28-th batch, train loss: 0.6412983536720276:  19%|██▏         | 28/151 [00:05<00:26,  4.61it/s]evaluate for the 9-th batch, evaluate loss: 0.20236441493034363:  38%|███████▏           | 8/21 [00:02<00:03,  3.43it/s]evaluate for the 9-th batch, evaluate loss: 0.20236441493034363:  43%|████████▏          | 9/21 [00:02<00:03,  3.38it/s]Epoch: 9, train for the 87-th batch, train loss: 0.5800612568855286:  59%|███████▋     | 86/146 [00:53<00:36,  1.66it/s]Epoch: 9, train for the 87-th batch, train loss: 0.5800612568855286:  60%|███████▋     | 87/146 [00:53<00:35,  1.67it/s]Epoch: 19, train for the 29-th batch, train loss: 0.5981537699699402:  19%|██▏         | 28/151 [00:05<00:26,  4.61it/s]Epoch: 19, train for the 29-th batch, train loss: 0.5981537699699402:  19%|██▎         | 29/151 [00:05<00:26,  4.63it/s]Epoch: 4, train for the 69-th batch, train loss: 0.29032886028289795:  18%|██▏         | 68/383 [00:39<03:08,  1.67it/s]Epoch: 4, train for the 69-th batch, train loss: 0.29032886028289795:  18%|██▏         | 69/383 [00:39<03:08,  1.66it/s]Epoch: 6, train for the 69-th batch, train loss: 0.6109052896499634:  29%|███▋         | 68/237 [00:38<01:41,  1.67it/s]Epoch: 6, train for the 69-th batch, train loss: 0.6109052896499634:  29%|███▊         | 69/237 [00:38<01:40,  1.66it/s]Epoch: 19, train for the 30-th batch, train loss: 0.5298187732696533:  19%|██▎         | 29/151 [00:06<00:26,  4.63it/s]Epoch: 19, train for the 30-th batch, train loss: 0.5298187732696533:  20%|██▍         | 30/151 [00:06<00:25,  4.65it/s]evaluate for the 10-th batch, evaluate loss: 0.1981646865606308:  43%|████████▏          | 9/21 [00:02<00:03,  3.38it/s]evaluate for the 10-th batch, evaluate loss: 0.1981646865606308:  48%|████████▌         | 10/21 [00:02<00:03,  3.44it/s]Epoch: 19, train for the 31-th batch, train loss: 0.4451645314693451:  20%|██▍         | 30/151 [00:06<00:25,  4.65it/s]Epoch: 19, train for the 31-th batch, train loss: 0.4451645314693451:  21%|██▍         | 31/151 [00:06<00:25,  4.71it/s]evaluate for the 11-th batch, evaluate loss: 0.141993448138237:  48%|█████████          | 10/21 [00:03<00:03,  3.44it/s]evaluate for the 11-th batch, evaluate loss: 0.141993448138237:  52%|█████████▉         | 11/21 [00:03<00:02,  3.40it/s]Epoch: 9, train for the 88-th batch, train loss: 0.5426835417747498:  60%|███████▋     | 87/146 [00:53<00:35,  1.67it/s]Epoch: 9, train for the 88-th batch, train loss: 0.5426835417747498:  60%|███████▊     | 88/146 [00:53<00:34,  1.68it/s]Epoch: 4, train for the 70-th batch, train loss: 0.3857371509075165:  18%|██▎          | 69/383 [00:39<03:08,  1.66it/s]Epoch: 4, train for the 70-th batch, train loss: 0.3857371509075165:  18%|██▍          | 70/383 [00:39<03:08,  1.66it/s]Epoch: 19, train for the 32-th batch, train loss: 0.6363725662231445:  21%|██▍         | 31/151 [00:06<00:25,  4.71it/s]Epoch: 19, train for the 32-th batch, train loss: 0.6363725662231445:  21%|██▌         | 32/151 [00:06<00:25,  4.66it/s]Epoch: 6, train for the 70-th batch, train loss: 0.597057044506073:  29%|████          | 69/237 [00:38<01:40,  1.66it/s]Epoch: 6, train for the 70-th batch, train loss: 0.597057044506073:  30%|████▏         | 70/237 [00:38<01:40,  1.66it/s]evaluate for the 12-th batch, evaluate loss: 0.2511550188064575:  52%|█████████▍        | 11/21 [00:03<00:02,  3.40it/s]evaluate for the 12-th batch, evaluate loss: 0.2511550188064575:  57%|██████████▎       | 12/21 [00:03<00:02,  3.54it/s]Epoch: 19, train for the 33-th batch, train loss: 0.5367344617843628:  21%|██▌         | 32/151 [00:06<00:25,  4.66it/s]Epoch: 19, train for the 33-th batch, train loss: 0.5367344617843628:  22%|██▌         | 33/151 [00:06<00:25,  4.63it/s]Epoch: 19, train for the 34-th batch, train loss: 0.5543951392173767:  22%|██▌         | 33/151 [00:06<00:25,  4.63it/s]Epoch: 19, train for the 34-th batch, train loss: 0.5543951392173767:  23%|██▋         | 34/151 [00:06<00:25,  4.66it/s]evaluate for the 13-th batch, evaluate loss: 0.2068384885787964:  57%|██████████▎       | 12/21 [00:03<00:02,  3.54it/s]evaluate for the 13-th batch, evaluate loss: 0.2068384885787964:  62%|███████████▏      | 13/21 [00:03<00:02,  3.47it/s]Epoch: 9, train for the 89-th batch, train loss: 0.5155074596405029:  60%|███████▊     | 88/146 [00:54<00:34,  1.68it/s]Epoch: 9, train for the 89-th batch, train loss: 0.5155074596405029:  61%|███████▉     | 89/146 [00:54<00:33,  1.69it/s]Epoch: 4, train for the 71-th batch, train loss: 0.34240788221359253:  18%|██▏         | 70/383 [00:40<03:08,  1.66it/s]Epoch: 4, train for the 71-th batch, train loss: 0.34240788221359253:  19%|██▏         | 71/383 [00:40<03:09,  1.65it/s]Epoch: 19, train for the 35-th batch, train loss: 0.5851287841796875:  23%|██▋         | 34/151 [00:07<00:25,  4.66it/s]Epoch: 19, train for the 35-th batch, train loss: 0.5851287841796875:  23%|██▊         | 35/151 [00:07<00:25,  4.62it/s]evaluate for the 14-th batch, evaluate loss: 0.17988044023513794:  62%|██████████▌      | 13/21 [00:04<00:02,  3.47it/s]evaluate for the 14-th batch, evaluate loss: 0.17988044023513794:  67%|███████████▎     | 14/21 [00:04<00:01,  3.59it/s]Epoch: 6, train for the 71-th batch, train loss: 0.5698977708816528:  30%|███▊         | 70/237 [00:39<01:40,  1.66it/s]Epoch: 6, train for the 71-th batch, train loss: 0.5698977708816528:  30%|███▉         | 71/237 [00:39<01:40,  1.65it/s]Epoch: 19, train for the 36-th batch, train loss: 0.6101610660552979:  23%|██▊         | 35/151 [00:07<00:25,  4.62it/s]Epoch: 19, train for the 36-th batch, train loss: 0.6101610660552979:  24%|██▊         | 36/151 [00:07<00:24,  4.62it/s]evaluate for the 15-th batch, evaluate loss: 0.1725349724292755:  67%|████████████      | 14/21 [00:04<00:01,  3.59it/s]evaluate for the 15-th batch, evaluate loss: 0.1725349724292755:  71%|████████████▊     | 15/21 [00:04<00:01,  3.52it/s]Epoch: 9, train for the 90-th batch, train loss: 0.5321289896965027:  61%|███████▉     | 89/146 [00:54<00:33,  1.69it/s]Epoch: 9, train for the 90-th batch, train loss: 0.5321289896965027:  62%|████████     | 90/146 [00:54<00:32,  1.70it/s]Epoch: 19, train for the 37-th batch, train loss: 0.5921136140823364:  24%|██▊         | 36/151 [00:07<00:24,  4.62it/s]Epoch: 19, train for the 37-th batch, train loss: 0.5921136140823364:  25%|██▉         | 37/151 [00:07<00:24,  4.64it/s]Epoch: 4, train for the 72-th batch, train loss: 0.30642250180244446:  19%|██▏         | 71/383 [00:40<03:09,  1.65it/s]Epoch: 4, train for the 72-th batch, train loss: 0.30642250180244446:  19%|██▎         | 72/383 [00:40<03:07,  1.66it/s]evaluate for the 16-th batch, evaluate loss: 0.21927997469902039:  71%|████████████▏    | 15/21 [00:04<00:01,  3.52it/s]evaluate for the 16-th batch, evaluate loss: 0.21927997469902039:  76%|████████████▉    | 16/21 [00:04<00:01,  3.62it/s]Epoch: 19, train for the 38-th batch, train loss: 0.6509339213371277:  25%|██▉         | 37/151 [00:07<00:24,  4.64it/s]Epoch: 19, train for the 38-th batch, train loss: 0.6509339213371277:  25%|███         | 38/151 [00:07<00:24,  4.59it/s]Epoch: 6, train for the 72-th batch, train loss: 0.47942081093788147:  30%|███▌        | 71/237 [00:39<01:40,  1.65it/s]Epoch: 6, train for the 72-th batch, train loss: 0.47942081093788147:  30%|███▋        | 72/237 [00:39<01:38,  1.67it/s]Epoch: 19, train for the 39-th batch, train loss: 0.6177722215652466:  25%|███         | 38/151 [00:08<00:24,  4.59it/s]Epoch: 19, train for the 39-th batch, train loss: 0.6177722215652466:  26%|███         | 39/151 [00:08<00:24,  4.58it/s]evaluate for the 17-th batch, evaluate loss: 0.21771389245986938:  76%|████████████▉    | 16/21 [00:04<00:01,  3.62it/s]evaluate for the 17-th batch, evaluate loss: 0.21771389245986938:  81%|█████████████▊   | 17/21 [00:04<00:01,  3.56it/s]Epoch: 9, train for the 91-th batch, train loss: 0.5366295576095581:  62%|████████     | 90/146 [00:55<00:32,  1.70it/s]Epoch: 9, train for the 91-th batch, train loss: 0.5366295576095581:  62%|████████     | 91/146 [00:55<00:32,  1.70it/s]Epoch: 19, train for the 40-th batch, train loss: 0.4698716700077057:  26%|███         | 39/151 [00:08<00:24,  4.58it/s]Epoch: 19, train for the 40-th batch, train loss: 0.4698716700077057:  26%|███▏        | 40/151 [00:08<00:24,  4.62it/s]Epoch: 4, train for the 73-th batch, train loss: 0.2860046625137329:  19%|██▍          | 72/383 [00:41<03:07,  1.66it/s]Epoch: 4, train for the 73-th batch, train loss: 0.2860046625137329:  19%|██▍          | 73/383 [00:41<03:05,  1.67it/s]evaluate for the 18-th batch, evaluate loss: 0.19321776926517487:  81%|█████████████▊   | 17/21 [00:05<00:01,  3.56it/s]evaluate for the 18-th batch, evaluate loss: 0.19321776926517487:  86%|██████████████▌  | 18/21 [00:05<00:00,  3.61it/s]Epoch: 6, train for the 73-th batch, train loss: 0.5638295412063599:  30%|███▉         | 72/237 [00:40<01:38,  1.67it/s]Epoch: 6, train for the 73-th batch, train loss: 0.5638295412063599:  31%|████         | 73/237 [00:40<01:37,  1.68it/s]Epoch: 19, train for the 41-th batch, train loss: 0.5627785921096802:  26%|███▏        | 40/151 [00:08<00:24,  4.62it/s]Epoch: 19, train for the 41-th batch, train loss: 0.5627785921096802:  27%|███▎        | 41/151 [00:08<00:23,  4.59it/s]evaluate for the 19-th batch, evaluate loss: 0.25431254506111145:  86%|██████████████▌  | 18/21 [00:05<00:00,  3.61it/s]evaluate for the 19-th batch, evaluate loss: 0.25431254506111145:  90%|███████████████▍ | 19/21 [00:05<00:00,  3.55it/s]Epoch: 19, train for the 42-th batch, train loss: 0.612607479095459:  27%|███▌         | 41/151 [00:08<00:23,  4.59it/s]Epoch: 19, train for the 42-th batch, train loss: 0.612607479095459:  28%|███▌         | 42/151 [00:08<00:23,  4.57it/s]Epoch: 9, train for the 92-th batch, train loss: 0.5636734962463379:  62%|████████     | 91/146 [00:55<00:32,  1.70it/s]Epoch: 9, train for the 92-th batch, train loss: 0.5636734962463379:  63%|████████▏    | 92/146 [00:55<00:31,  1.69it/s]Epoch: 4, train for the 74-th batch, train loss: 0.2581426501274109:  19%|██▍          | 73/383 [00:42<03:05,  1.67it/s]Epoch: 4, train for the 74-th batch, train loss: 0.2581426501274109:  19%|██▌          | 74/383 [00:42<03:04,  1.67it/s]Epoch: 19, train for the 43-th batch, train loss: 0.6474647521972656:  28%|███▎        | 42/151 [00:08<00:23,  4.57it/s]Epoch: 19, train for the 43-th batch, train loss: 0.6474647521972656:  28%|███▍        | 43/151 [00:08<00:24,  4.50it/s]evaluate for the 20-th batch, evaluate loss: 0.21292389929294586:  90%|███████████████▍ | 19/21 [00:05<00:00,  3.55it/s]evaluate for the 20-th batch, evaluate loss: 0.21292389929294586:  95%|████████████████▏| 20/21 [00:05<00:00,  3.52it/s]evaluate for the 21-th batch, evaluate loss: 0.12407232820987701:  95%|████████████████▏| 20/21 [00:05<00:00,  3.52it/s]evaluate for the 21-th batch, evaluate loss: 0.12407232820987701: 100%|█████████████████| 21/21 [00:05<00:00,  3.65it/s]
INFO:root:Epoch: 10, learning rate: 0.0001, train loss: 0.2038
INFO:root:train average_precision, 0.9737
INFO:root:train roc_auc, 0.9658
INFO:root:validate loss: 0.1454
INFO:root:validate average_precision, 0.9875
INFO:root:validate roc_auc, 0.9850
INFO:root:new node validate loss: 0.2072
INFO:root:new node validate first_1_average_precision, 0.9229
INFO:root:new node validate first_1_roc_auc, 0.9278
INFO:root:new node validate first_3_average_precision, 0.9591
INFO:root:new node validate first_3_roc_auc, 0.9600
INFO:root:new node validate first_10_average_precision, 0.9731
INFO:root:new node validate first_10_roc_auc, 0.9722
INFO:root:new node validate average_precision, 0.9742
INFO:root:new node validate roc_auc, 0.9723
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 6, train for the 74-th batch, train loss: 0.44141608476638794:  31%|███▋        | 73/237 [00:41<01:37,  1.68it/s]Epoch: 6, train for the 74-th batch, train loss: 0.44141608476638794:  31%|███▋        | 74/237 [00:41<01:35,  1.70it/s]Epoch: 19, train for the 44-th batch, train loss: 0.5957702398300171:  28%|███▍        | 43/151 [00:09<00:24,  4.50it/s]Epoch: 19, train for the 44-th batch, train loss: 0.5957702398300171:  29%|███▍        | 44/151 [00:09<00:23,  4.49it/s]evaluate for the 1-th batch, evaluate loss: 0.13234372437000275:   0%|                           | 0/40 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.13234372437000275:   2%|▍                  | 1/40 [00:00<00:11,  3.48it/s]Epoch: 9, train for the 93-th batch, train loss: 0.5256924033164978:  63%|████████▏    | 92/146 [00:56<00:31,  1.69it/s]Epoch: 9, train for the 93-th batch, train loss: 0.5256924033164978:  64%|████████▎    | 93/146 [00:56<00:31,  1.69it/s]Epoch: 19, train for the 45-th batch, train loss: 0.5807520151138306:  29%|███▍        | 44/151 [00:09<00:23,  4.49it/s]Epoch: 19, train for the 45-th batch, train loss: 0.5807520151138306:  30%|███▌        | 45/151 [00:09<00:23,  4.50it/s]Epoch: 4, train for the 75-th batch, train loss: 0.26487666368484497:  19%|██▎         | 74/383 [00:42<03:04,  1.67it/s]Epoch: 4, train for the 75-th batch, train loss: 0.26487666368484497:  20%|██▎         | 75/383 [00:42<03:02,  1.69it/s]evaluate for the 2-th batch, evaluate loss: 0.26679614186286926:   2%|▍                  | 1/40 [00:00<00:11,  3.48it/s]evaluate for the 2-th batch, evaluate loss: 0.26679614186286926:   5%|▉                  | 2/40 [00:00<00:10,  3.66it/s]Epoch: 19, train for the 46-th batch, train loss: 0.39603114128112793:  30%|███▎       | 45/151 [00:09<00:23,  4.50it/s]Epoch: 19, train for the 46-th batch, train loss: 0.39603114128112793:  30%|███▎       | 46/151 [00:09<00:22,  4.59it/s]Epoch: 6, train for the 75-th batch, train loss: 0.67912358045578:  31%|████▋          | 74/237 [00:41<01:35,  1.70it/s]Epoch: 6, train for the 75-th batch, train loss: 0.67912358045578:  32%|████▋          | 75/237 [00:41<01:35,  1.69it/s]Epoch: 19, train for the 47-th batch, train loss: 0.5812317728996277:  30%|███▋        | 46/151 [00:09<00:22,  4.59it/s]Epoch: 19, train for the 47-th batch, train loss: 0.5812317728996277:  31%|███▋        | 47/151 [00:09<00:22,  4.58it/s]evaluate for the 3-th batch, evaluate loss: 0.18041759729385376:   5%|▉                  | 2/40 [00:00<00:10,  3.66it/s]evaluate for the 3-th batch, evaluate loss: 0.18041759729385376:   8%|█▍                 | 3/40 [00:00<00:10,  3.56it/s]Epoch: 9, train for the 94-th batch, train loss: 0.5357862114906311:  64%|████████▎    | 93/146 [00:57<00:31,  1.69it/s]Epoch: 9, train for the 94-th batch, train loss: 0.5357862114906311:  64%|████████▎    | 94/146 [00:57<00:30,  1.68it/s]Epoch: 19, train for the 48-th batch, train loss: 0.32292455434799194:  31%|███▍       | 47/151 [00:09<00:22,  4.58it/s]Epoch: 19, train for the 48-th batch, train loss: 0.32292455434799194:  32%|███▍       | 48/151 [00:09<00:22,  4.66it/s]Epoch: 4, train for the 76-th batch, train loss: 0.32672572135925293:  20%|██▎         | 75/383 [00:43<03:02,  1.69it/s]Epoch: 4, train for the 76-th batch, train loss: 0.32672572135925293:  20%|██▍         | 76/383 [00:43<03:01,  1.69it/s]evaluate for the 4-th batch, evaluate loss: 0.10314277559518814:   8%|█▍                 | 3/40 [00:01<00:10,  3.56it/s]evaluate for the 4-th batch, evaluate loss: 0.10314277559518814:  10%|█▉                 | 4/40 [00:01<00:10,  3.49it/s]Epoch: 6, train for the 76-th batch, train loss: 0.6642345190048218:  32%|████         | 75/237 [00:42<01:35,  1.69it/s]Epoch: 6, train for the 76-th batch, train loss: 0.6642345190048218:  32%|████▏        | 76/237 [00:42<01:35,  1.69it/s]Epoch: 19, train for the 49-th batch, train loss: 0.3997945785522461:  32%|███▊        | 48/151 [00:10<00:22,  4.66it/s]Epoch: 19, train for the 49-th batch, train loss: 0.3997945785522461:  32%|███▉        | 49/151 [00:10<00:21,  4.69it/s]evaluate for the 5-th batch, evaluate loss: 0.16585543751716614:  10%|█▉                 | 4/40 [00:01<00:10,  3.49it/s]evaluate for the 5-th batch, evaluate loss: 0.16585543751716614:  12%|██▍                | 5/40 [00:01<00:09,  3.54it/s]Epoch: 19, train for the 50-th batch, train loss: 0.6277483105659485:  32%|███▉        | 49/151 [00:10<00:21,  4.69it/s]Epoch: 19, train for the 50-th batch, train loss: 0.6277483105659485:  33%|███▉        | 50/151 [00:10<00:21,  4.63it/s]Epoch: 9, train for the 95-th batch, train loss: 0.5198565721511841:  64%|████████▎    | 94/146 [00:57<00:30,  1.68it/s]Epoch: 9, train for the 95-th batch, train loss: 0.5198565721511841:  65%|████████▍    | 95/146 [00:57<00:30,  1.68it/s]Epoch: 4, train for the 77-th batch, train loss: 0.3264258801937103:  20%|██▌          | 76/383 [00:43<03:01,  1.69it/s]Epoch: 4, train for the 77-th batch, train loss: 0.3264258801937103:  20%|██▌          | 77/383 [00:43<03:01,  1.69it/s]Epoch: 19, train for the 51-th batch, train loss: 0.6058878898620605:  33%|███▉        | 50/151 [00:10<00:21,  4.63it/s]Epoch: 19, train for the 51-th batch, train loss: 0.6058878898620605:  34%|████        | 51/151 [00:10<00:21,  4.60it/s]evaluate for the 6-th batch, evaluate loss: 0.1363246589899063:  12%|██▌                 | 5/40 [00:01<00:09,  3.54it/s]evaluate for the 6-th batch, evaluate loss: 0.1363246589899063:  15%|███                 | 6/40 [00:01<00:09,  3.47it/s]Epoch: 6, train for the 77-th batch, train loss: 0.6562293767929077:  32%|████▏        | 76/237 [00:42<01:35,  1.69it/s]Epoch: 6, train for the 77-th batch, train loss: 0.6562293767929077:  32%|████▏        | 77/237 [00:42<01:34,  1.69it/s]Epoch: 19, train for the 52-th batch, train loss: 0.6529749631881714:  34%|████        | 51/151 [00:10<00:21,  4.60it/s]Epoch: 19, train for the 52-th batch, train loss: 0.6529749631881714:  34%|████▏       | 52/151 [00:10<00:21,  4.57it/s]evaluate for the 7-th batch, evaluate loss: 0.1414051353931427:  15%|███                 | 6/40 [00:01<00:09,  3.47it/s]evaluate for the 7-th batch, evaluate loss: 0.1414051353931427:  18%|███▌                | 7/40 [00:01<00:09,  3.64it/s]Epoch: 19, train for the 53-th batch, train loss: 0.5818579196929932:  34%|████▏       | 52/151 [00:11<00:21,  4.57it/s]Epoch: 19, train for the 53-th batch, train loss: 0.5818579196929932:  35%|████▏       | 53/151 [00:11<00:21,  4.56it/s]Epoch: 9, train for the 96-th batch, train loss: 0.5440995097160339:  65%|████████▍    | 95/146 [00:58<00:30,  1.68it/s]Epoch: 9, train for the 96-th batch, train loss: 0.5440995097160339:  66%|████████▌    | 96/146 [00:58<00:29,  1.68it/s]evaluate for the 8-th batch, evaluate loss: 0.12128007411956787:  18%|███▎               | 7/40 [00:02<00:09,  3.64it/s]evaluate for the 8-th batch, evaluate loss: 0.12128007411956787:  20%|███▊               | 8/40 [00:02<00:09,  3.52it/s]Epoch: 4, train for the 78-th batch, train loss: 0.29281488060951233:  20%|██▍         | 77/383 [00:44<03:01,  1.69it/s]Epoch: 4, train for the 78-th batch, train loss: 0.29281488060951233:  20%|██▍         | 78/383 [00:44<03:00,  1.69it/s]Epoch: 6, train for the 78-th batch, train loss: 0.5785650014877319:  32%|████▏        | 77/237 [00:43<01:34,  1.69it/s]Epoch: 6, train for the 78-th batch, train loss: 0.5785650014877319:  33%|████▎        | 78/237 [00:43<01:34,  1.68it/s]Epoch: 19, train for the 54-th batch, train loss: 0.5093029737472534:  35%|████▏       | 53/151 [00:11<00:21,  4.56it/s]Epoch: 19, train for the 54-th batch, train loss: 0.5093029737472534:  36%|████▎       | 54/151 [00:11<00:23,  4.08it/s]evaluate for the 9-th batch, evaluate loss: 0.13020730018615723:  20%|███▊               | 8/40 [00:02<00:09,  3.52it/s]evaluate for the 9-th batch, evaluate loss: 0.13020730018615723:  22%|████▎              | 9/40 [00:02<00:08,  3.71it/s]Epoch: 19, train for the 55-th batch, train loss: 0.5805083513259888:  36%|████▎       | 54/151 [00:11<00:23,  4.08it/s]Epoch: 19, train for the 55-th batch, train loss: 0.5805083513259888:  36%|████▎       | 55/151 [00:11<00:22,  4.20it/s]Epoch: 9, train for the 97-th batch, train loss: 0.5602686405181885:  66%|████████▌    | 96/146 [00:58<00:29,  1.68it/s]Epoch: 9, train for the 97-th batch, train loss: 0.5602686405181885:  66%|████████▋    | 97/146 [00:58<00:29,  1.66it/s]evaluate for the 10-th batch, evaluate loss: 0.13988743722438812:  22%|████              | 9/40 [00:02<00:08,  3.71it/s]evaluate for the 10-th batch, evaluate loss: 0.13988743722438812:  25%|████▎            | 10/40 [00:02<00:08,  3.51it/s]Epoch: 4, train for the 79-th batch, train loss: 0.3280933201313019:  20%|██▋          | 78/383 [00:45<03:00,  1.69it/s]Epoch: 4, train for the 79-th batch, train loss: 0.3280933201313019:  21%|██▋          | 79/383 [00:45<03:00,  1.69it/s]Epoch: 19, train for the 56-th batch, train loss: 0.40941137075424194:  36%|████       | 55/151 [00:11<00:22,  4.20it/s]Epoch: 19, train for the 56-th batch, train loss: 0.40941137075424194:  37%|████       | 56/151 [00:11<00:21,  4.33it/s]Epoch: 6, train for the 79-th batch, train loss: 0.6322044134140015:  33%|████▎        | 78/237 [00:44<01:34,  1.68it/s]Epoch: 6, train for the 79-th batch, train loss: 0.6322044134140015:  33%|████▎        | 79/237 [00:44<01:34,  1.68it/s]Epoch: 19, train for the 57-th batch, train loss: 0.4179087281227112:  37%|████▍       | 56/151 [00:12<00:21,  4.33it/s]Epoch: 19, train for the 57-th batch, train loss: 0.4179087281227112:  38%|████▌       | 57/151 [00:12<00:21,  4.43it/s]evaluate for the 11-th batch, evaluate loss: 0.13915523886680603:  25%|████▎            | 10/40 [00:03<00:08,  3.51it/s]evaluate for the 11-th batch, evaluate loss: 0.13915523886680603:  28%|████▋            | 11/40 [00:03<00:08,  3.57it/s]Epoch: 19, train for the 58-th batch, train loss: 0.4712940454483032:  38%|████▌       | 57/151 [00:12<00:21,  4.43it/s]Epoch: 19, train for the 58-th batch, train loss: 0.4712940454483032:  38%|████▌       | 58/151 [00:12<00:20,  4.47it/s]evaluate for the 12-th batch, evaluate loss: 0.1606721431016922:  28%|████▉             | 11/40 [00:03<00:08,  3.57it/s]evaluate for the 12-th batch, evaluate loss: 0.1606721431016922:  30%|█████▍            | 12/40 [00:03<00:07,  3.51it/s]Epoch: 9, train for the 98-th batch, train loss: 0.5523722171783447:  66%|████████▋    | 97/146 [00:59<00:29,  1.66it/s]Epoch: 9, train for the 98-th batch, train loss: 0.5523722171783447:  67%|████████▋    | 98/146 [00:59<00:28,  1.67it/s]Epoch: 4, train for the 80-th batch, train loss: 0.23525743186473846:  21%|██▍         | 79/383 [00:45<03:00,  1.69it/s]Epoch: 4, train for the 80-th batch, train loss: 0.23525743186473846:  21%|██▌         | 80/383 [00:45<03:00,  1.68it/s]Epoch: 19, train for the 59-th batch, train loss: 0.5289608836174011:  38%|████▌       | 58/151 [00:12<00:20,  4.47it/s]Epoch: 19, train for the 59-th batch, train loss: 0.5289608836174011:  39%|████▋       | 59/151 [00:12<00:20,  4.48it/s]Epoch: 6, train for the 80-th batch, train loss: 0.6171391010284424:  33%|████▎        | 79/237 [00:44<01:34,  1.68it/s]Epoch: 6, train for the 80-th batch, train loss: 0.6171391010284424:  34%|████▍        | 80/237 [00:44<01:33,  1.67it/s]evaluate for the 13-th batch, evaluate loss: 0.13627871870994568:  30%|█████            | 12/40 [00:03<00:07,  3.51it/s]evaluate for the 13-th batch, evaluate loss: 0.13627871870994568:  32%|█████▌           | 13/40 [00:03<00:07,  3.62it/s]Epoch: 19, train for the 60-th batch, train loss: 0.5064128041267395:  39%|████▋       | 59/151 [00:12<00:20,  4.48it/s]Epoch: 19, train for the 60-th batch, train loss: 0.5064128041267395:  40%|████▊       | 60/151 [00:12<00:20,  4.49it/s]evaluate for the 14-th batch, evaluate loss: 0.15822908282279968:  32%|█████▌           | 13/40 [00:03<00:07,  3.62it/s]evaluate for the 14-th batch, evaluate loss: 0.15822908282279968:  35%|█████▉           | 14/40 [00:03<00:07,  3.55it/s]Epoch: 19, train for the 61-th batch, train loss: 0.5329816341400146:  40%|████▊       | 60/151 [00:12<00:20,  4.49it/s]Epoch: 19, train for the 61-th batch, train loss: 0.5329816341400146:  40%|████▊       | 61/151 [00:12<00:20,  4.49it/s]Epoch: 9, train for the 99-th batch, train loss: 0.5622296333312988:  67%|████████▋    | 98/146 [01:00<00:28,  1.67it/s]Epoch: 9, train for the 99-th batch, train loss: 0.5622296333312988:  68%|████████▊    | 99/146 [01:00<00:28,  1.67it/s]Epoch: 4, train for the 81-th batch, train loss: 0.28842487931251526:  21%|██▌         | 80/383 [00:46<03:00,  1.68it/s]Epoch: 4, train for the 81-th batch, train loss: 0.28842487931251526:  21%|██▌         | 81/383 [00:46<03:00,  1.68it/s]Epoch: 19, train for the 62-th batch, train loss: 0.46599897742271423:  40%|████▍      | 61/151 [00:13<00:20,  4.49it/s]Epoch: 19, train for the 62-th batch, train loss: 0.46599897742271423:  41%|████▌      | 62/151 [00:13<00:19,  4.54it/s]evaluate for the 15-th batch, evaluate loss: 0.12039632350206375:  35%|█████▉           | 14/40 [00:04<00:07,  3.55it/s]evaluate for the 15-th batch, evaluate loss: 0.12039632350206375:  38%|██████▍          | 15/40 [00:04<00:06,  3.64it/s]Epoch: 6, train for the 81-th batch, train loss: 0.5653190016746521:  34%|████▍        | 80/237 [00:45<01:33,  1.67it/s]Epoch: 6, train for the 81-th batch, train loss: 0.5653190016746521:  34%|████▍        | 81/237 [00:45<01:32,  1.69it/s]Epoch: 19, train for the 63-th batch, train loss: 0.5668381452560425:  41%|████▉       | 62/151 [00:13<00:19,  4.54it/s]Epoch: 19, train for the 63-th batch, train loss: 0.5668381452560425:  42%|█████       | 63/151 [00:13<00:19,  4.52it/s]evaluate for the 16-th batch, evaluate loss: 0.13374115526676178:  38%|██████▍          | 15/40 [00:04<00:06,  3.64it/s]evaluate for the 16-th batch, evaluate loss: 0.13374115526676178:  40%|██████▊          | 16/40 [00:04<00:06,  3.58it/s]Epoch: 9, train for the 100-th batch, train loss: 0.5582674145698547:  68%|████████▏   | 99/146 [01:00<00:28,  1.67it/s]Epoch: 9, train for the 100-th batch, train loss: 0.5582674145698547:  68%|███████▌   | 100/146 [01:00<00:27,  1.66it/s]Epoch: 19, train for the 64-th batch, train loss: 0.5101358890533447:  42%|█████       | 63/151 [00:13<00:19,  4.52it/s]Epoch: 19, train for the 64-th batch, train loss: 0.5101358890533447:  42%|█████       | 64/151 [00:13<00:19,  4.50it/s]Epoch: 4, train for the 82-th batch, train loss: 0.3498283922672272:  21%|██▋          | 81/383 [00:46<03:00,  1.68it/s]Epoch: 4, train for the 82-th batch, train loss: 0.3498283922672272:  21%|██▊          | 82/383 [00:46<02:58,  1.68it/s]evaluate for the 17-th batch, evaluate loss: 0.1705302745103836:  40%|███████▏          | 16/40 [00:04<00:06,  3.58it/s]evaluate for the 17-th batch, evaluate loss: 0.1705302745103836:  42%|███████▋          | 17/40 [00:04<00:06,  3.61it/s]Epoch: 6, train for the 82-th batch, train loss: 0.556907057762146:  34%|████▊         | 81/237 [00:45<01:32,  1.69it/s]Epoch: 6, train for the 82-th batch, train loss: 0.556907057762146:  35%|████▊         | 82/237 [00:45<01:30,  1.70it/s]Epoch: 19, train for the 65-th batch, train loss: 0.49071189761161804:  42%|████▋      | 64/151 [00:13<00:19,  4.50it/s]Epoch: 19, train for the 65-th batch, train loss: 0.49071189761161804:  43%|████▋      | 65/151 [00:13<00:19,  4.50it/s]evaluate for the 18-th batch, evaluate loss: 0.15038661658763885:  42%|███████▏         | 17/40 [00:05<00:06,  3.61it/s]evaluate for the 18-th batch, evaluate loss: 0.15038661658763885:  45%|███████▋         | 18/40 [00:05<00:06,  3.63it/s]Epoch: 19, train for the 66-th batch, train loss: 0.44930464029312134:  43%|████▋      | 65/151 [00:14<00:19,  4.50it/s]Epoch: 19, train for the 66-th batch, train loss: 0.44930464029312134:  44%|████▊      | 66/151 [00:14<00:18,  4.52it/s]Epoch: 9, train for the 101-th batch, train loss: 0.5663524270057678:  68%|███████▌   | 100/146 [01:01<00:27,  1.66it/s]Epoch: 9, train for the 101-th batch, train loss: 0.5663524270057678:  69%|███████▌   | 101/146 [01:01<00:27,  1.66it/s]Epoch: 4, train for the 83-th batch, train loss: 0.30713123083114624:  21%|██▌         | 82/383 [00:47<02:58,  1.68it/s]Epoch: 4, train for the 83-th batch, train loss: 0.30713123083114624:  22%|██▌         | 83/383 [00:47<02:55,  1.71it/s]Epoch: 19, train for the 67-th batch, train loss: 0.5783485174179077:  44%|█████▏      | 66/151 [00:14<00:18,  4.52it/s]Epoch: 19, train for the 67-th batch, train loss: 0.5783485174179077:  44%|█████▎      | 67/151 [00:14<00:18,  4.52it/s]evaluate for the 19-th batch, evaluate loss: 0.14381857216358185:  45%|███████▋         | 18/40 [00:05<00:06,  3.63it/s]evaluate for the 19-th batch, evaluate loss: 0.14381857216358185:  48%|████████         | 19/40 [00:05<00:05,  3.54it/s]Epoch: 6, train for the 83-th batch, train loss: 0.6530675888061523:  35%|████▍        | 82/237 [00:46<01:30,  1.70it/s]Epoch: 6, train for the 83-th batch, train loss: 0.6530675888061523:  35%|████▌        | 83/237 [00:46<01:30,  1.69it/s]Epoch: 19, train for the 68-th batch, train loss: 0.549873948097229:  44%|█████▊       | 67/151 [00:14<00:18,  4.52it/s]Epoch: 19, train for the 68-th batch, train loss: 0.549873948097229:  45%|█████▊       | 68/151 [00:14<00:18,  4.52it/s]evaluate for the 20-th batch, evaluate loss: 0.1504756063222885:  48%|████████▌         | 19/40 [00:05<00:05,  3.54it/s]evaluate for the 20-th batch, evaluate loss: 0.1504756063222885:  50%|█████████         | 20/40 [00:05<00:05,  3.67it/s]Epoch: 19, train for the 69-th batch, train loss: 0.5405399799346924:  45%|█████▍      | 68/151 [00:14<00:18,  4.52it/s]Epoch: 19, train for the 69-th batch, train loss: 0.5405399799346924:  46%|█████▍      | 69/151 [00:14<00:18,  4.52it/s]Epoch: 9, train for the 102-th batch, train loss: 0.5418030619621277:  69%|███████▌   | 101/146 [01:01<00:27,  1.66it/s]Epoch: 9, train for the 102-th batch, train loss: 0.5418030619621277:  70%|███████▋   | 102/146 [01:01<00:26,  1.67it/s]Epoch: 4, train for the 84-th batch, train loss: 0.361452579498291:  22%|███           | 83/383 [00:48<02:55,  1.71it/s]Epoch: 4, train for the 84-th batch, train loss: 0.361452579498291:  22%|███           | 84/383 [00:48<02:56,  1.70it/s]evaluate for the 21-th batch, evaluate loss: 0.1869407594203949:  50%|█████████         | 20/40 [00:05<00:05,  3.67it/s]evaluate for the 21-th batch, evaluate loss: 0.1869407594203949:  52%|█████████▍        | 21/40 [00:05<00:05,  3.55it/s]Epoch: 19, train for the 70-th batch, train loss: 0.5690390467643738:  46%|█████▍      | 69/151 [00:14<00:18,  4.52it/s]Epoch: 19, train for the 70-th batch, train loss: 0.5690390467643738:  46%|█████▌      | 70/151 [00:14<00:17,  4.53it/s]Epoch: 6, train for the 84-th batch, train loss: 0.5332517027854919:  35%|████▌        | 83/237 [00:46<01:30,  1.69it/s]Epoch: 6, train for the 84-th batch, train loss: 0.5332517027854919:  35%|████▌        | 84/237 [00:46<01:30,  1.69it/s]evaluate for the 22-th batch, evaluate loss: 0.1365763396024704:  52%|█████████▍        | 21/40 [00:06<00:05,  3.55it/s]evaluate for the 22-th batch, evaluate loss: 0.1365763396024704:  55%|█████████▉        | 22/40 [00:06<00:04,  3.71it/s]Epoch: 19, train for the 71-th batch, train loss: 0.5511393547058105:  46%|█████▌      | 70/151 [00:15<00:17,  4.53it/s]Epoch: 19, train for the 71-th batch, train loss: 0.5511393547058105:  47%|█████▋      | 71/151 [00:15<00:17,  4.52it/s]Epoch: 19, train for the 72-th batch, train loss: 0.5514753460884094:  47%|█████▋      | 71/151 [00:15<00:17,  4.52it/s]Epoch: 19, train for the 72-th batch, train loss: 0.5514753460884094:  48%|█████▋      | 72/151 [00:15<00:17,  4.51it/s]Epoch: 4, train for the 85-th batch, train loss: 0.380054771900177:  22%|███           | 84/383 [00:48<02:56,  1.70it/s]Epoch: 4, train for the 85-th batch, train loss: 0.380054771900177:  22%|███           | 85/383 [00:48<02:55,  1.69it/s]Epoch: 9, train for the 103-th batch, train loss: 0.5942638516426086:  70%|███████▋   | 102/146 [01:02<00:26,  1.67it/s]Epoch: 9, train for the 103-th batch, train loss: 0.5942638516426086:  71%|███████▊   | 103/146 [01:02<00:25,  1.65it/s]evaluate for the 23-th batch, evaluate loss: 0.12860484421253204:  55%|█████████▎       | 22/40 [00:06<00:04,  3.71it/s]evaluate for the 23-th batch, evaluate loss: 0.12860484421253204:  57%|█████████▊       | 23/40 [00:06<00:04,  3.52it/s]Epoch: 6, train for the 85-th batch, train loss: 0.6256317496299744:  35%|████▌        | 84/237 [00:47<01:30,  1.69it/s]Epoch: 6, train for the 85-th batch, train loss: 0.6256317496299744:  36%|████▋        | 85/237 [00:47<01:30,  1.68it/s]Epoch: 19, train for the 73-th batch, train loss: 0.5177993178367615:  48%|█████▋      | 72/151 [00:15<00:17,  4.51it/s]Epoch: 19, train for the 73-th batch, train loss: 0.5177993178367615:  48%|█████▊      | 73/151 [00:15<00:17,  4.50it/s]evaluate for the 24-th batch, evaluate loss: 0.12549148499965668:  57%|█████████▊       | 23/40 [00:06<00:04,  3.52it/s]evaluate for the 24-th batch, evaluate loss: 0.12549148499965668:  60%|██████████▏      | 24/40 [00:06<00:04,  3.58it/s]Epoch: 19, train for the 74-th batch, train loss: 0.5333108305931091:  48%|█████▊      | 73/151 [00:15<00:17,  4.50it/s]Epoch: 19, train for the 74-th batch, train loss: 0.5333108305931091:  49%|█████▉      | 74/151 [00:15<00:17,  4.50it/s]evaluate for the 25-th batch, evaluate loss: 0.11751864105463028:  60%|██████████▏      | 24/40 [00:07<00:04,  3.58it/s]evaluate for the 25-th batch, evaluate loss: 0.11751864105463028:  62%|██████████▋      | 25/40 [00:07<00:04,  3.51it/s]Epoch: 4, train for the 86-th batch, train loss: 0.2913528084754944:  22%|██▉          | 85/383 [00:49<02:55,  1.69it/s]Epoch: 4, train for the 86-th batch, train loss: 0.2913528084754944:  22%|██▉          | 86/383 [00:49<02:56,  1.68it/s]Epoch: 9, train for the 104-th batch, train loss: 0.5514790415763855:  71%|███████▊   | 103/146 [01:03<00:25,  1.65it/s]Epoch: 9, train for the 104-th batch, train loss: 0.5514790415763855:  71%|███████▊   | 104/146 [01:03<00:25,  1.66it/s]Epoch: 19, train for the 75-th batch, train loss: 0.5203426480293274:  49%|█████▉      | 74/151 [00:16<00:17,  4.50it/s]Epoch: 19, train for the 75-th batch, train loss: 0.5203426480293274:  50%|█████▉      | 75/151 [00:16<00:16,  4.50it/s]Epoch: 6, train for the 86-th batch, train loss: 0.6853737235069275:  36%|████▋        | 85/237 [00:48<01:30,  1.68it/s]Epoch: 6, train for the 86-th batch, train loss: 0.6853737235069275:  36%|████▋        | 86/237 [00:48<01:30,  1.67it/s]evaluate for the 26-th batch, evaluate loss: 0.14623859524726868:  62%|██████████▋      | 25/40 [00:07<00:04,  3.51it/s]evaluate for the 26-th batch, evaluate loss: 0.14623859524726868:  65%|███████████      | 26/40 [00:07<00:03,  3.64it/s]Epoch: 19, train for the 76-th batch, train loss: 0.5649856328964233:  50%|█████▉      | 75/151 [00:16<00:16,  4.50it/s]Epoch: 19, train for the 76-th batch, train loss: 0.5649856328964233:  50%|██████      | 76/151 [00:16<00:16,  4.50it/s]Epoch: 19, train for the 77-th batch, train loss: 0.5337454080581665:  50%|██████      | 76/151 [00:16<00:16,  4.50it/s]Epoch: 19, train for the 77-th batch, train loss: 0.5337454080581665:  51%|██████      | 77/151 [00:16<00:16,  4.49it/s]evaluate for the 27-th batch, evaluate loss: 0.1896952986717224:  65%|███████████▋      | 26/40 [00:07<00:03,  3.64it/s]evaluate for the 27-th batch, evaluate loss: 0.1896952986717224:  68%|████████████▏     | 27/40 [00:07<00:03,  3.56it/s]Epoch: 9, train for the 105-th batch, train loss: 0.5516931414604187:  71%|███████▊   | 104/146 [01:03<00:25,  1.66it/s]Epoch: 9, train for the 105-th batch, train loss: 0.5516931414604187:  72%|███████▉   | 105/146 [01:03<00:24,  1.66it/s]Epoch: 4, train for the 87-th batch, train loss: 0.40383094549179077:  22%|██▋         | 86/383 [00:49<02:56,  1.68it/s]Epoch: 4, train for the 87-th batch, train loss: 0.40383094549179077:  23%|██▋         | 87/383 [00:49<02:57,  1.67it/s]Epoch: 19, train for the 78-th batch, train loss: 0.5392706990242004:  51%|██████      | 77/151 [00:16<00:16,  4.49it/s]Epoch: 19, train for the 78-th batch, train loss: 0.5392706990242004:  52%|██████▏     | 78/151 [00:16<00:16,  4.50it/s]Epoch: 6, train for the 87-th batch, train loss: 0.6164592504501343:  36%|████▋        | 86/237 [00:48<01:30,  1.67it/s]Epoch: 6, train for the 87-th batch, train loss: 0.6164592504501343:  37%|████▊        | 87/237 [00:48<01:29,  1.67it/s]evaluate for the 28-th batch, evaluate loss: 0.13578897714614868:  68%|███████████▍     | 27/40 [00:07<00:03,  3.56it/s]evaluate for the 28-th batch, evaluate loss: 0.13578897714614868:  70%|███████████▉     | 28/40 [00:07<00:03,  3.65it/s]Epoch: 19, train for the 79-th batch, train loss: 0.5407642126083374:  52%|██████▏     | 78/151 [00:16<00:16,  4.50it/s]Epoch: 19, train for the 79-th batch, train loss: 0.5407642126083374:  52%|██████▎     | 79/151 [00:16<00:15,  4.53it/s]evaluate for the 29-th batch, evaluate loss: 0.2084045708179474:  70%|████████████▌     | 28/40 [00:08<00:03,  3.65it/s]evaluate for the 29-th batch, evaluate loss: 0.2084045708179474:  72%|█████████████     | 29/40 [00:08<00:03,  3.63it/s]Epoch: 19, train for the 80-th batch, train loss: 0.5585387349128723:  52%|██████▎     | 79/151 [00:17<00:15,  4.53it/s]Epoch: 19, train for the 80-th batch, train loss: 0.5585387349128723:  53%|██████▎     | 80/151 [00:17<00:15,  4.52it/s]Epoch: 4, train for the 88-th batch, train loss: 0.35272857546806335:  23%|██▋         | 87/383 [00:50<02:57,  1.67it/s]Epoch: 4, train for the 88-th batch, train loss: 0.35272857546806335:  23%|██▊         | 88/383 [00:50<02:56,  1.67it/s]Epoch: 9, train for the 106-th batch, train loss: 0.5534411668777466:  72%|███████▉   | 105/146 [01:04<00:24,  1.66it/s]Epoch: 9, train for the 106-th batch, train loss: 0.5534411668777466:  73%|███████▉   | 106/146 [01:04<00:24,  1.65it/s]Epoch: 6, train for the 88-th batch, train loss: 0.5755615234375:  37%|█████▊          | 87/237 [00:49<01:29,  1.67it/s]Epoch: 6, train for the 88-th batch, train loss: 0.5755615234375:  37%|█████▉          | 88/237 [00:49<01:28,  1.68it/s]evaluate for the 30-th batch, evaluate loss: 0.203399196267128:  72%|█████████████▊     | 29/40 [00:08<00:03,  3.63it/s]evaluate for the 30-th batch, evaluate loss: 0.203399196267128:  75%|██████████████▎    | 30/40 [00:08<00:02,  3.57it/s]Epoch: 19, train for the 81-th batch, train loss: 0.5470467805862427:  53%|██████▎     | 80/151 [00:17<00:15,  4.52it/s]Epoch: 19, train for the 81-th batch, train loss: 0.5470467805862427:  54%|██████▍     | 81/151 [00:17<00:15,  4.49it/s]evaluate for the 31-th batch, evaluate loss: 0.13557563722133636:  75%|████████████▊    | 30/40 [00:08<00:02,  3.57it/s]evaluate for the 31-th batch, evaluate loss: 0.13557563722133636:  78%|█████████████▏   | 31/40 [00:08<00:02,  3.59it/s]Epoch: 4, train for the 89-th batch, train loss: 0.3550701141357422:  23%|██▉          | 88/383 [00:51<02:56,  1.67it/s]Epoch: 4, train for the 89-th batch, train loss: 0.3550701141357422:  23%|███          | 89/383 [00:51<02:55,  1.68it/s]Epoch: 9, train for the 107-th batch, train loss: 0.5587477087974548:  73%|███████▉   | 106/146 [01:04<00:24,  1.65it/s]Epoch: 9, train for the 107-th batch, train loss: 0.5587477087974548:  73%|████████   | 107/146 [01:04<00:23,  1.65it/s]Epoch: 19, train for the 82-th batch, train loss: 0.5530074834823608:  54%|██████▍     | 81/151 [00:17<00:15,  4.49it/s]Epoch: 19, train for the 82-th batch, train loss: 0.5530074834823608:  54%|██████▌     | 82/151 [00:17<00:19,  3.46it/s]Epoch: 6, train for the 89-th batch, train loss: 0.4515625536441803:  37%|████▊        | 88/237 [00:49<01:28,  1.68it/s]Epoch: 6, train for the 89-th batch, train loss: 0.4515625536441803:  38%|████▉        | 89/237 [00:49<01:26,  1.70it/s]evaluate for the 32-th batch, evaluate loss: 0.16079816222190857:  78%|█████████████▏   | 31/40 [00:08<00:02,  3.59it/s]evaluate for the 32-th batch, evaluate loss: 0.16079816222190857:  80%|█████████████▌   | 32/40 [00:08<00:02,  3.51it/s]Epoch: 19, train for the 83-th batch, train loss: 0.39987659454345703:  54%|█████▉     | 82/151 [00:17<00:19,  3.46it/s]Epoch: 19, train for the 83-th batch, train loss: 0.39987659454345703:  55%|██████     | 83/151 [00:17<00:18,  3.75it/s]evaluate for the 33-th batch, evaluate loss: 0.14919312298297882:  80%|█████████████▌   | 32/40 [00:09<00:02,  3.51it/s]evaluate for the 33-th batch, evaluate loss: 0.14919312298297882:  82%|██████████████   | 33/40 [00:09<00:01,  3.67it/s]Epoch: 19, train for the 84-th batch, train loss: 0.5488718152046204:  55%|██████▌     | 83/151 [00:18<00:18,  3.75it/s]Epoch: 19, train for the 84-th batch, train loss: 0.5488718152046204:  56%|██████▋     | 84/151 [00:18<00:16,  3.96it/s]Epoch: 4, train for the 90-th batch, train loss: 0.3408335745334625:  23%|███          | 89/383 [00:51<02:55,  1.68it/s]Epoch: 4, train for the 90-th batch, train loss: 0.3408335745334625:  23%|███          | 90/383 [00:51<02:52,  1.69it/s]Epoch: 9, train for the 108-th batch, train loss: 0.5555816292762756:  73%|████████   | 107/146 [01:05<00:23,  1.65it/s]Epoch: 9, train for the 108-th batch, train loss: 0.5555816292762756:  74%|████████▏  | 108/146 [01:05<00:22,  1.66it/s]evaluate for the 34-th batch, evaluate loss: 0.197842538356781:  82%|███████████████▋   | 33/40 [00:09<00:01,  3.67it/s]evaluate for the 34-th batch, evaluate loss: 0.197842538356781:  85%|████████████████▏  | 34/40 [00:09<00:01,  3.55it/s]Epoch: 19, train for the 85-th batch, train loss: 0.5758740901947021:  56%|██████▋     | 84/151 [00:18<00:16,  3.96it/s]Epoch: 19, train for the 85-th batch, train loss: 0.5758740901947021:  56%|██████▊     | 85/151 [00:18<00:16,  4.10it/s]Epoch: 6, train for the 90-th batch, train loss: 0.6058908104896545:  38%|████▉        | 89/237 [00:50<01:26,  1.70it/s]Epoch: 6, train for the 90-th batch, train loss: 0.6058908104896545:  38%|████▉        | 90/237 [00:50<01:26,  1.69it/s]Epoch: 19, train for the 86-th batch, train loss: 0.5650767683982849:  56%|██████▊     | 85/151 [00:18<00:16,  4.10it/s]Epoch: 19, train for the 86-th batch, train loss: 0.5650767683982849:  57%|██████▊     | 86/151 [00:18<00:15,  4.21it/s]evaluate for the 35-th batch, evaluate loss: 0.14893653988838196:  85%|██████████████▍  | 34/40 [00:09<00:01,  3.55it/s]evaluate for the 35-th batch, evaluate loss: 0.14893653988838196:  88%|██████████████▉  | 35/40 [00:09<00:01,  3.71it/s]Epoch: 19, train for the 87-th batch, train loss: 0.5535223484039307:  57%|██████▊     | 86/151 [00:18<00:15,  4.21it/s]Epoch: 19, train for the 87-th batch, train loss: 0.5535223484039307:  58%|██████▉     | 87/151 [00:18<00:14,  4.30it/s]Epoch: 4, train for the 91-th batch, train loss: 0.2915869653224945:  23%|███          | 90/383 [00:52<02:52,  1.69it/s]Epoch: 4, train for the 91-th batch, train loss: 0.2915869653224945:  24%|███          | 91/383 [00:52<02:51,  1.70it/s]Epoch: 9, train for the 109-th batch, train loss: 0.5226743817329407:  74%|████████▏  | 108/146 [01:06<00:22,  1.66it/s]Epoch: 9, train for the 109-th batch, train loss: 0.5226743817329407:  75%|████████▏  | 109/146 [01:06<00:22,  1.65it/s]evaluate for the 36-th batch, evaluate loss: 0.18621060252189636:  88%|██████████████▉  | 35/40 [00:10<00:01,  3.71it/s]evaluate for the 36-th batch, evaluate loss: 0.18621060252189636:  90%|███████████████▎ | 36/40 [00:10<00:01,  3.51it/s]Epoch: 6, train for the 91-th batch, train loss: 0.6385207176208496:  38%|████▉        | 90/237 [00:51<01:26,  1.69it/s]Epoch: 6, train for the 91-th batch, train loss: 0.6385207176208496:  38%|████▉        | 91/237 [00:51<01:26,  1.69it/s]Epoch: 19, train for the 88-th batch, train loss: 0.5832133889198303:  58%|██████▉     | 87/151 [00:19<00:14,  4.30it/s]Epoch: 19, train for the 88-th batch, train loss: 0.5832133889198303:  58%|██████▉     | 88/151 [00:19<00:14,  4.36it/s]evaluate for the 37-th batch, evaluate loss: 0.10256502032279968:  90%|███████████████▎ | 36/40 [00:10<00:01,  3.51it/s]evaluate for the 37-th batch, evaluate loss: 0.10256502032279968:  92%|███████████████▋ | 37/40 [00:10<00:00,  3.57it/s]Epoch: 19, train for the 89-th batch, train loss: 0.5874199271202087:  58%|██████▉     | 88/151 [00:19<00:14,  4.36it/s]Epoch: 19, train for the 89-th batch, train loss: 0.5874199271202087:  59%|███████     | 89/151 [00:19<00:14,  4.41it/s]Epoch: 4, train for the 92-th batch, train loss: 0.3152780830860138:  24%|███          | 91/383 [00:52<02:51,  1.70it/s]Epoch: 4, train for the 92-th batch, train loss: 0.3152780830860138:  24%|███          | 92/383 [00:52<02:52,  1.69it/s]Epoch: 19, train for the 90-th batch, train loss: 0.5461163520812988:  59%|███████     | 89/151 [00:19<00:14,  4.41it/s]Epoch: 19, train for the 90-th batch, train loss: 0.5461163520812988:  60%|███████▏    | 90/151 [00:19<00:13,  4.43it/s]evaluate for the 38-th batch, evaluate loss: 0.12185513973236084:  92%|███████████████▋ | 37/40 [00:10<00:00,  3.57it/s]evaluate for the 38-th batch, evaluate loss: 0.12185513973236084:  95%|████████████████▏| 38/40 [00:10<00:00,  3.50it/s]Epoch: 9, train for the 110-th batch, train loss: 0.5261884927749634:  75%|████████▏  | 109/146 [01:06<00:22,  1.65it/s]Epoch: 9, train for the 110-th batch, train loss: 0.5261884927749634:  75%|████████▎  | 110/146 [01:06<00:21,  1.66it/s]Epoch: 6, train for the 92-th batch, train loss: 0.6233720779418945:  38%|████▉        | 91/237 [00:51<01:26,  1.69it/s]Epoch: 6, train for the 92-th batch, train loss: 0.6233720779418945:  39%|█████        | 92/237 [00:51<01:26,  1.68it/s]Epoch: 19, train for the 91-th batch, train loss: 0.4665902853012085:  60%|███████▏    | 90/151 [00:19<00:13,  4.43it/s]Epoch: 19, train for the 91-th batch, train loss: 0.4665902853012085:  60%|███████▏    | 91/151 [00:19<00:13,  4.45it/s]evaluate for the 39-th batch, evaluate loss: 0.1268738955259323:  95%|█████████████████ | 38/40 [00:10<00:00,  3.50it/s]evaluate for the 39-th batch, evaluate loss: 0.1268738955259323:  98%|█████████████████▌| 39/40 [00:10<00:00,  3.62it/s]evaluate for the 40-th batch, evaluate loss: 0.309924840927124:  98%|██████████████████▌| 39/40 [00:10<00:00,  3.62it/s]evaluate for the 40-th batch, evaluate loss: 0.309924840927124: 100%|███████████████████| 40/40 [00:10<00:00,  3.67it/s]
  0%|                                                                                            | 0/24 [00:00<?, ?it/s]Epoch: 19, train for the 92-th batch, train loss: 0.5406322479248047:  60%|███████▏    | 91/151 [00:19<00:13,  4.45it/s]Epoch: 19, train for the 92-th batch, train loss: 0.5406322479248047:  61%|███████▎    | 92/151 [00:19<00:13,  4.47it/s]Epoch: 4, train for the 93-th batch, train loss: 0.3697117567062378:  24%|███          | 92/383 [00:53<02:52,  1.69it/s]Epoch: 4, train for the 93-th batch, train loss: 0.3697117567062378:  24%|███▏         | 93/383 [00:53<02:52,  1.68it/s]evaluate for the 1-th batch, evaluate loss: 0.31515565514564514:   0%|                           | 0/24 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.31515565514564514:   4%|▊                  | 1/24 [00:00<00:07,  3.27it/s]Epoch: 9, train for the 111-th batch, train loss: 0.5314717888832092:  75%|████████▎  | 110/146 [01:07<00:21,  1.66it/s]Epoch: 9, train for the 111-th batch, train loss: 0.5314717888832092:  76%|████████▎  | 111/146 [01:07<00:20,  1.68it/s]Epoch: 19, train for the 93-th batch, train loss: 0.5134444832801819:  61%|███████▎    | 92/151 [00:20<00:13,  4.47it/s]Epoch: 19, train for the 93-th batch, train loss: 0.5134444832801819:  62%|███████▍    | 93/151 [00:20<00:13,  4.45it/s]Epoch: 6, train for the 93-th batch, train loss: 0.5830441117286682:  39%|█████        | 92/237 [00:52<01:26,  1.68it/s]Epoch: 6, train for the 93-th batch, train loss: 0.5830441117286682:  39%|█████        | 93/237 [00:52<01:26,  1.67it/s]evaluate for the 2-th batch, evaluate loss: 0.2604716122150421:   4%|▊                   | 1/24 [00:00<00:07,  3.27it/s]evaluate for the 2-th batch, evaluate loss: 0.2604716122150421:   8%|█▋                  | 2/24 [00:00<00:06,  3.61it/s]Epoch: 19, train for the 94-th batch, train loss: 0.5277463793754578:  62%|███████▍    | 93/151 [00:20<00:13,  4.45it/s]Epoch: 19, train for the 94-th batch, train loss: 0.5277463793754578:  62%|███████▍    | 94/151 [00:20<00:12,  4.47it/s]Epoch: 19, train for the 95-th batch, train loss: 0.512592613697052:  62%|████████     | 94/151 [00:20<00:12,  4.47it/s]Epoch: 19, train for the 95-th batch, train loss: 0.512592613697052:  63%|████████▏    | 95/151 [00:20<00:12,  4.47it/s]Epoch: 4, train for the 94-th batch, train loss: 0.31214091181755066:  24%|██▉         | 93/383 [00:53<02:52,  1.68it/s]Epoch: 4, train for the 94-th batch, train loss: 0.31214091181755066:  25%|██▉         | 94/383 [00:53<02:50,  1.69it/s]evaluate for the 3-th batch, evaluate loss: 0.22905008494853973:   8%|█▌                 | 2/24 [00:00<00:06,  3.61it/s]evaluate for the 3-th batch, evaluate loss: 0.22905008494853973:  12%|██▍                | 3/24 [00:00<00:06,  3.48it/s]Epoch: 9, train for the 112-th batch, train loss: 0.576254665851593:  76%|█████████   | 111/146 [01:07<00:20,  1.68it/s]Epoch: 9, train for the 112-th batch, train loss: 0.576254665851593:  77%|█████████▏  | 112/146 [01:07<00:20,  1.69it/s]Epoch: 6, train for the 94-th batch, train loss: 0.5711212158203125:  39%|█████        | 93/237 [00:52<01:26,  1.67it/s]Epoch: 6, train for the 94-th batch, train loss: 0.5711212158203125:  40%|█████▏       | 94/237 [00:52<01:24,  1.68it/s]Epoch: 19, train for the 96-th batch, train loss: 0.5539681911468506:  63%|███████▌    | 95/151 [00:20<00:12,  4.47it/s]Epoch: 19, train for the 96-th batch, train loss: 0.5539681911468506:  64%|███████▋    | 96/151 [00:20<00:12,  4.48it/s]evaluate for the 4-th batch, evaluate loss: 0.21343445777893066:  12%|██▍                | 3/24 [00:01<00:06,  3.48it/s]evaluate for the 4-th batch, evaluate loss: 0.21343445777893066:  17%|███▏               | 4/24 [00:01<00:05,  3.61it/s]Epoch: 19, train for the 97-th batch, train loss: 0.6006858944892883:  64%|███████▋    | 96/151 [00:21<00:12,  4.48it/s]Epoch: 19, train for the 97-th batch, train loss: 0.6006858944892883:  64%|███████▋    | 97/151 [00:21<00:12,  4.50it/s]evaluate for the 5-th batch, evaluate loss: 0.22955359518527985:  17%|███▏               | 4/24 [00:01<00:05,  3.61it/s]evaluate for the 5-th batch, evaluate loss: 0.22955359518527985:  21%|███▉               | 5/24 [00:01<00:05,  3.50it/s]Epoch: 9, train for the 113-th batch, train loss: 0.5211958885192871:  77%|████████▍  | 112/146 [01:08<00:20,  1.69it/s]Epoch: 4, train for the 95-th batch, train loss: 0.3796204924583435:  25%|███▏         | 94/383 [00:54<02:50,  1.69it/s]Epoch: 9, train for the 113-th batch, train loss: 0.5211958885192871:  77%|████████▌  | 113/146 [01:08<00:19,  1.71it/s]Epoch: 4, train for the 95-th batch, train loss: 0.3796204924583435:  25%|███▏         | 95/383 [00:54<02:52,  1.67it/s]Epoch: 19, train for the 98-th batch, train loss: 0.6349815726280212:  64%|███████▋    | 97/151 [00:21<00:12,  4.50it/s]Epoch: 19, train for the 98-th batch, train loss: 0.6349815726280212:  65%|███████▊    | 98/151 [00:21<00:11,  4.50it/s]Epoch: 6, train for the 95-th batch, train loss: 0.597998857498169:  40%|█████▌        | 94/237 [00:53<01:24,  1.68it/s]Epoch: 6, train for the 95-th batch, train loss: 0.597998857498169:  40%|█████▌        | 95/237 [00:53<01:25,  1.66it/s]evaluate for the 6-th batch, evaluate loss: 0.23988811671733856:  21%|███▉               | 5/24 [00:01<00:05,  3.50it/s]evaluate for the 6-th batch, evaluate loss: 0.23988811671733856:  25%|████▊              | 6/24 [00:01<00:05,  3.60it/s]Epoch: 19, train for the 99-th batch, train loss: 0.6124998927116394:  65%|███████▊    | 98/151 [00:21<00:11,  4.50it/s]Epoch: 19, train for the 99-th batch, train loss: 0.6124998927116394:  66%|███████▊    | 99/151 [00:21<00:11,  4.52it/s]Epoch: 19, train for the 100-th batch, train loss: 0.6657276153564453:  66%|███████▏   | 99/151 [00:21<00:11,  4.52it/s]Epoch: 19, train for the 100-th batch, train loss: 0.6657276153564453:  66%|██████▌   | 100/151 [00:21<00:11,  4.51it/s]evaluate for the 7-th batch, evaluate loss: 0.19988632202148438:  25%|████▊              | 6/24 [00:01<00:05,  3.60it/s]evaluate for the 7-th batch, evaluate loss: 0.19988632202148438:  29%|█████▌             | 7/24 [00:01<00:04,  3.51it/s]Epoch: 9, train for the 114-th batch, train loss: 0.5948929786682129:  77%|████████▌  | 113/146 [01:09<00:19,  1.71it/s]Epoch: 9, train for the 114-th batch, train loss: 0.5948929786682129:  78%|████████▌  | 114/146 [01:09<00:18,  1.72it/s]Epoch: 4, train for the 96-th batch, train loss: 0.35486719012260437:  25%|██▉         | 95/383 [00:55<02:52,  1.67it/s]Epoch: 4, train for the 96-th batch, train loss: 0.35486719012260437:  25%|███         | 96/383 [00:55<02:53,  1.65it/s]Epoch: 19, train for the 101-th batch, train loss: 0.6663075685501099:  66%|██████▌   | 100/151 [00:21<00:11,  4.51it/s]Epoch: 19, train for the 101-th batch, train loss: 0.6663075685501099:  67%|██████▋   | 101/151 [00:21<00:11,  4.51it/s]Epoch: 6, train for the 96-th batch, train loss: 0.6401553153991699:  40%|█████▏       | 95/237 [00:54<01:25,  1.66it/s]Epoch: 6, train for the 96-th batch, train loss: 0.6401553153991699:  41%|█████▎       | 96/237 [00:54<01:25,  1.65it/s]evaluate for the 8-th batch, evaluate loss: 0.25020188093185425:  29%|█████▌             | 7/24 [00:02<00:04,  3.51it/s]evaluate for the 8-th batch, evaluate loss: 0.25020188093185425:  33%|██████▎            | 8/24 [00:02<00:04,  3.60it/s]Epoch: 19, train for the 102-th batch, train loss: 0.5629073977470398:  67%|██████▋   | 101/151 [00:22<00:11,  4.51it/s]Epoch: 19, train for the 102-th batch, train loss: 0.5629073977470398:  68%|██████▊   | 102/151 [00:22<00:10,  4.53it/s]evaluate for the 9-th batch, evaluate loss: 0.2681916058063507:  33%|██████▋             | 8/24 [00:02<00:04,  3.60it/s]evaluate for the 9-th batch, evaluate loss: 0.2681916058063507:  38%|███████▌            | 9/24 [00:02<00:04,  3.53it/s]Epoch: 19, train for the 103-th batch, train loss: 0.6287554502487183:  68%|██████▊   | 102/151 [00:22<00:10,  4.53it/s]Epoch: 19, train for the 103-th batch, train loss: 0.6287554502487183:  68%|██████▊   | 103/151 [00:22<00:10,  4.50it/s]Epoch: 9, train for the 115-th batch, train loss: 0.538311243057251:  78%|█████████▎  | 114/146 [01:09<00:18,  1.72it/s]Epoch: 9, train for the 115-th batch, train loss: 0.538311243057251:  79%|█████████▍  | 115/146 [01:09<00:18,  1.72it/s]Epoch: 4, train for the 97-th batch, train loss: 0.26467010378837585:  25%|███         | 96/383 [00:55<02:53,  1.65it/s]Epoch: 4, train for the 97-th batch, train loss: 0.26467010378837585:  25%|███         | 97/383 [00:55<02:53,  1.65it/s]evaluate for the 10-th batch, evaluate loss: 0.25643762946128845:  38%|██████▊           | 9/24 [00:02<00:04,  3.53it/s]evaluate for the 10-th batch, evaluate loss: 0.25643762946128845:  42%|███████          | 10/24 [00:02<00:03,  3.61it/s]Epoch: 19, train for the 104-th batch, train loss: 0.6327680945396423:  68%|██████▊   | 103/151 [00:22<00:10,  4.50it/s]Epoch: 19, train for the 104-th batch, train loss: 0.6327680945396423:  69%|██████▉   | 104/151 [00:22<00:10,  4.49it/s]Epoch: 6, train for the 97-th batch, train loss: 0.5195724368095398:  41%|█████▎       | 96/237 [00:54<01:25,  1.65it/s]Epoch: 6, train for the 97-th batch, train loss: 0.5195724368095398:  41%|█████▎       | 97/237 [00:54<01:24,  1.66it/s]Epoch: 19, train for the 105-th batch, train loss: 0.5492599606513977:  69%|██████▉   | 104/151 [00:22<00:10,  4.49it/s]Epoch: 19, train for the 105-th batch, train loss: 0.5492599606513977:  70%|██████▉   | 105/151 [00:22<00:10,  4.49it/s]evaluate for the 11-th batch, evaluate loss: 0.17116159200668335:  42%|███████          | 10/24 [00:03<00:03,  3.61it/s]evaluate for the 11-th batch, evaluate loss: 0.17116159200668335:  46%|███████▊         | 11/24 [00:03<00:03,  3.53it/s]Epoch: 9, train for the 116-th batch, train loss: 0.5246917009353638:  79%|████████▋  | 115/146 [01:10<00:18,  1.72it/s]Epoch: 9, train for the 116-th batch, train loss: 0.5246917009353638:  79%|████████▋  | 116/146 [01:10<00:17,  1.71it/s]Epoch: 19, train for the 106-th batch, train loss: 0.5652050375938416:  70%|██████▉   | 105/151 [00:23<00:10,  4.49it/s]Epoch: 19, train for the 106-th batch, train loss: 0.5652050375938416:  70%|███████   | 106/151 [00:23<00:10,  4.50it/s]Epoch: 4, train for the 98-th batch, train loss: 0.2664019465446472:  25%|███▎         | 97/383 [00:56<02:53,  1.65it/s]Epoch: 4, train for the 98-th batch, train loss: 0.2664019465446472:  26%|███▎         | 98/383 [00:56<02:49,  1.68it/s]evaluate for the 12-th batch, evaluate loss: 0.26361408829689026:  46%|███████▊         | 11/24 [00:03<00:03,  3.53it/s]evaluate for the 12-th batch, evaluate loss: 0.26361408829689026:  50%|████████▌        | 12/24 [00:03<00:03,  3.58it/s]Epoch: 6, train for the 98-th batch, train loss: 0.5340171456336975:  41%|█████▎       | 97/237 [00:55<01:24,  1.66it/s]Epoch: 6, train for the 98-th batch, train loss: 0.5340171456336975:  41%|█████▍       | 98/237 [00:55<01:22,  1.68it/s]Epoch: 19, train for the 107-th batch, train loss: 0.5108485817909241:  70%|███████   | 106/151 [00:23<00:10,  4.50it/s]Epoch: 19, train for the 107-th batch, train loss: 0.5108485817909241:  71%|███████   | 107/151 [00:23<00:09,  4.51it/s]evaluate for the 13-th batch, evaluate loss: 0.18917842209339142:  50%|████████▌        | 12/24 [00:03<00:03,  3.58it/s]evaluate for the 13-th batch, evaluate loss: 0.18917842209339142:  54%|█████████▏       | 13/24 [00:03<00:03,  3.53it/s]Epoch: 19, train for the 108-th batch, train loss: 0.5404946804046631:  71%|███████   | 107/151 [00:23<00:09,  4.51it/s]Epoch: 19, train for the 108-th batch, train loss: 0.5404946804046631:  72%|███████▏  | 108/151 [00:23<00:09,  4.51it/s]Epoch: 9, train for the 117-th batch, train loss: 0.589917004108429:  79%|█████████▌  | 116/146 [01:10<00:17,  1.71it/s]Epoch: 9, train for the 117-th batch, train loss: 0.589917004108429:  80%|█████████▌  | 117/146 [01:10<00:17,  1.70it/s]Epoch: 4, train for the 99-th batch, train loss: 0.3298153281211853:  26%|███▎         | 98/383 [00:56<02:49,  1.68it/s]Epoch: 4, train for the 99-th batch, train loss: 0.3298153281211853:  26%|███▎         | 99/383 [00:56<02:50,  1.67it/s]Epoch: 19, train for the 109-th batch, train loss: 0.5482546091079712:  72%|███████▏  | 108/151 [00:23<00:09,  4.51it/s]Epoch: 19, train for the 109-th batch, train loss: 0.5482546091079712:  72%|███████▏  | 109/151 [00:23<00:09,  4.51it/s]evaluate for the 14-th batch, evaluate loss: 0.18610607087612152:  54%|█████████▏       | 13/24 [00:03<00:03,  3.53it/s]evaluate for the 14-th batch, evaluate loss: 0.18610607087612152:  58%|█████████▉       | 14/24 [00:03<00:02,  3.50it/s]Epoch: 6, train for the 99-th batch, train loss: 0.5900723934173584:  41%|█████▍       | 98/237 [00:55<01:22,  1.68it/s]Epoch: 6, train for the 99-th batch, train loss: 0.5900723934173584:  42%|█████▍       | 99/237 [00:55<01:22,  1.67it/s]Epoch: 19, train for the 110-th batch, train loss: 0.561575710773468:  72%|███████▉   | 109/151 [00:23<00:09,  4.51it/s]Epoch: 19, train for the 110-th batch, train loss: 0.561575710773468:  73%|████████   | 110/151 [00:23<00:09,  4.52it/s]evaluate for the 15-th batch, evaluate loss: 0.23075267672538757:  58%|█████████▉       | 14/24 [00:04<00:02,  3.50it/s]evaluate for the 15-th batch, evaluate loss: 0.23075267672538757:  62%|██████████▋      | 15/24 [00:04<00:02,  3.45it/s]Epoch: 19, train for the 111-th batch, train loss: 0.5256012678146362:  73%|███████▎  | 110/151 [00:24<00:09,  4.52it/s]Epoch: 19, train for the 111-th batch, train loss: 0.5256012678146362:  74%|███████▎  | 111/151 [00:24<00:08,  4.51it/s]Epoch: 9, train for the 118-th batch, train loss: 0.4944334924221039:  80%|████████▊  | 117/146 [01:11<00:17,  1.70it/s]Epoch: 9, train for the 118-th batch, train loss: 0.4944334924221039:  81%|████████▉  | 118/146 [01:11<00:16,  1.69it/s]Epoch: 4, train for the 100-th batch, train loss: 0.34172651171684265:  26%|██▊        | 99/383 [00:57<02:50,  1.67it/s]Epoch: 4, train for the 100-th batch, train loss: 0.34172651171684265:  26%|██▌       | 100/383 [00:57<02:49,  1.67it/s]evaluate for the 16-th batch, evaluate loss: 0.2338133603334427:  62%|███████████▎      | 15/24 [00:04<00:02,  3.45it/s]evaluate for the 16-th batch, evaluate loss: 0.2338133603334427:  67%|████████████      | 16/24 [00:04<00:02,  3.42it/s]Epoch: 19, train for the 112-th batch, train loss: 0.5148873925209045:  74%|███████▎  | 111/151 [00:24<00:08,  4.51it/s]Epoch: 19, train for the 112-th batch, train loss: 0.5148873925209045:  74%|███████▍  | 112/151 [00:24<00:08,  4.50it/s]Epoch: 6, train for the 100-th batch, train loss: 0.6027876138687134:  42%|█████       | 99/237 [00:56<01:22,  1.67it/s]Epoch: 6, train for the 100-th batch, train loss: 0.6027876138687134:  42%|████▋      | 100/237 [00:56<01:22,  1.66it/s]Epoch: 19, train for the 113-th batch, train loss: 0.5672999024391174:  74%|███████▍  | 112/151 [00:24<00:08,  4.50it/s]Epoch: 19, train for the 113-th batch, train loss: 0.5672999024391174:  75%|███████▍  | 113/151 [00:24<00:08,  4.50it/s]evaluate for the 17-th batch, evaluate loss: 0.2668658494949341:  67%|████████████      | 16/24 [00:04<00:02,  3.42it/s]evaluate for the 17-th batch, evaluate loss: 0.2668658494949341:  71%|████████████▊     | 17/24 [00:04<00:02,  3.46it/s]Epoch: 9, train for the 119-th batch, train loss: 0.5331884622573853:  81%|████████▉  | 118/146 [01:12<00:16,  1.69it/s]Epoch: 9, train for the 119-th batch, train loss: 0.5331884622573853:  82%|████████▉  | 119/146 [01:12<00:16,  1.69it/s]Epoch: 19, train for the 114-th batch, train loss: 0.5102025866508484:  75%|███████▍  | 113/151 [00:24<00:08,  4.50it/s]Epoch: 19, train for the 114-th batch, train loss: 0.5102025866508484:  75%|███████▌  | 114/151 [00:24<00:08,  4.50it/s]Epoch: 4, train for the 101-th batch, train loss: 0.3669692575931549:  26%|██▊        | 100/383 [00:58<02:49,  1.67it/s]Epoch: 4, train for the 101-th batch, train loss: 0.3669692575931549:  26%|██▉        | 101/383 [00:58<02:50,  1.65it/s]evaluate for the 18-th batch, evaluate loss: 0.22720198333263397:  71%|████████████     | 17/24 [00:05<00:02,  3.46it/s]evaluate for the 18-th batch, evaluate loss: 0.22720198333263397:  75%|████████████▊    | 18/24 [00:05<00:01,  3.41it/s]Epoch: 6, train for the 101-th batch, train loss: 0.5964826941490173:  42%|████▋      | 100/237 [00:57<01:22,  1.66it/s]Epoch: 6, train for the 101-th batch, train loss: 0.5964826941490173:  43%|████▋      | 101/237 [00:57<01:22,  1.65it/s]Epoch: 19, train for the 115-th batch, train loss: 0.5302428603172302:  75%|███████▌  | 114/151 [00:25<00:08,  4.50it/s]Epoch: 19, train for the 115-th batch, train loss: 0.5302428603172302:  76%|███████▌  | 115/151 [00:25<00:08,  4.47it/s]evaluate for the 19-th batch, evaluate loss: 0.19782434403896332:  75%|████████████▊    | 18/24 [00:05<00:01,  3.41it/s]evaluate for the 19-th batch, evaluate loss: 0.19782434403896332:  79%|█████████████▍   | 19/24 [00:05<00:01,  3.48it/s]Epoch: 19, train for the 116-th batch, train loss: 0.49264082312583923:  76%|██████▊  | 115/151 [00:25<00:08,  4.47it/s]Epoch: 19, train for the 116-th batch, train loss: 0.49264082312583923:  77%|██████▉  | 116/151 [00:25<00:07,  4.48it/s]Epoch: 9, train for the 120-th batch, train loss: 0.5089939832687378:  82%|████████▉  | 119/146 [01:12<00:16,  1.69it/s]Epoch: 9, train for the 120-th batch, train loss: 0.5089939832687378:  82%|█████████  | 120/146 [01:12<00:15,  1.69it/s]Epoch: 19, train for the 117-th batch, train loss: 0.5159531235694885:  77%|███████▋  | 116/151 [00:25<00:07,  4.48it/s]Epoch: 19, train for the 117-th batch, train loss: 0.5159531235694885:  77%|███████▋  | 117/151 [00:25<00:07,  4.48it/s]Epoch: 4, train for the 102-th batch, train loss: 0.34988752007484436:  26%|██▋       | 101/383 [00:58<02:50,  1.65it/s]Epoch: 4, train for the 102-th batch, train loss: 0.34988752007484436:  27%|██▋       | 102/383 [00:58<02:50,  1.64it/s]evaluate for the 20-th batch, evaluate loss: 0.2244008630514145:  79%|██████████████▎   | 19/24 [00:05<00:01,  3.48it/s]evaluate for the 20-th batch, evaluate loss: 0.2244008630514145:  83%|███████████████   | 20/24 [00:05<00:01,  3.42it/s]Epoch: 6, train for the 102-th batch, train loss: 0.6574230194091797:  43%|████▋      | 101/237 [00:57<01:22,  1.65it/s]Epoch: 6, train for the 102-th batch, train loss: 0.6574230194091797:  43%|████▋      | 102/237 [00:57<01:22,  1.64it/s]Epoch: 19, train for the 118-th batch, train loss: 0.470594197511673:  77%|████████▌  | 117/151 [00:25<00:07,  4.48it/s]Epoch: 19, train for the 118-th batch, train loss: 0.470594197511673:  78%|████████▌  | 118/151 [00:25<00:07,  4.48it/s]evaluate for the 21-th batch, evaluate loss: 0.2284063994884491:  83%|███████████████   | 20/24 [00:05<00:01,  3.42it/s]evaluate for the 21-th batch, evaluate loss: 0.2284063994884491:  88%|███████████████▊  | 21/24 [00:05<00:00,  3.54it/s]Epoch: 19, train for the 119-th batch, train loss: 0.5130541920661926:  78%|███████▊  | 118/151 [00:25<00:07,  4.48it/s]Epoch: 19, train for the 119-th batch, train loss: 0.5130541920661926:  79%|███████▉  | 119/151 [00:25<00:07,  4.49it/s]Epoch: 9, train for the 121-th batch, train loss: 0.5222852826118469:  82%|█████████  | 120/146 [01:13<00:15,  1.69it/s]Epoch: 9, train for the 121-th batch, train loss: 0.5222852826118469:  83%|█████████  | 121/146 [01:13<00:14,  1.70it/s]evaluate for the 22-th batch, evaluate loss: 0.21514980494976044:  88%|██████████████▉  | 21/24 [00:06<00:00,  3.54it/s]evaluate for the 22-th batch, evaluate loss: 0.21514980494976044:  92%|███████████████▌ | 22/24 [00:06<00:00,  3.46it/s]Epoch: 4, train for the 103-th batch, train loss: 0.2731238305568695:  27%|██▉        | 102/383 [00:59<02:50,  1.64it/s]Epoch: 4, train for the 103-th batch, train loss: 0.2731238305568695:  27%|██▉        | 103/383 [00:59<02:50,  1.64it/s]Epoch: 19, train for the 120-th batch, train loss: 0.5855151414871216:  79%|███████▉  | 119/151 [00:26<00:07,  4.49it/s]Epoch: 19, train for the 120-th batch, train loss: 0.5855151414871216:  79%|███████▉  | 120/151 [00:26<00:06,  4.49it/s]Epoch: 6, train for the 103-th batch, train loss: 0.5899683237075806:  43%|████▋      | 102/237 [00:58<01:22,  1.64it/s]Epoch: 6, train for the 103-th batch, train loss: 0.5899683237075806:  43%|████▊      | 103/237 [00:58<01:21,  1.64it/s]evaluate for the 23-th batch, evaluate loss: 0.1813054233789444:  92%|████████████████▌ | 22/24 [00:06<00:00,  3.46it/s]evaluate for the 23-th batch, evaluate loss: 0.1813054233789444:  96%|█████████████████▎| 23/24 [00:06<00:00,  3.58it/s]evaluate for the 24-th batch, evaluate loss: 0.2565661072731018:  96%|█████████████████▎| 23/24 [00:06<00:00,  3.58it/s]evaluate for the 24-th batch, evaluate loss: 0.2565661072731018: 100%|██████████████████| 24/24 [00:06<00:00,  3.65it/s]
INFO:root:test loss: 0.1550
INFO:root:test average_precision, 0.9887
INFO:root:test roc_auc, 0.9874
INFO:root:new node test loss: 0.2306
INFO:root:new node test first_1_average_precision, 0.8799
INFO:root:new node test first_1_roc_auc, 0.8843
INFO:root:new node test first_3_average_precision, 0.9220
INFO:root:new node test first_3_roc_auc, 0.9184
INFO:root:new node test first_10_average_precision, 0.9593
INFO:root:new node test first_10_roc_auc, 0.9536
INFO:root:new node test average_precision, 0.9709
INFO:root:new node test roc_auc, 0.9674
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]Epoch: 19, train for the 121-th batch, train loss: 0.48733171820640564:  79%|███████▏ | 120/151 [00:26<00:06,  4.49it/s]Epoch: 19, train for the 121-th batch, train loss: 0.48733171820640564:  80%|███████▏ | 121/151 [00:26<00:06,  4.46it/s]Epoch: 9, train for the 122-th batch, train loss: 0.5622404217720032:  83%|█████████  | 121/146 [01:13<00:14,  1.70it/s]Epoch: 9, train for the 122-th batch, train loss: 0.5622404217720032:  84%|█████████▏ | 122/146 [01:13<00:13,  1.72it/s]Epoch: 19, train for the 122-th batch, train loss: 0.5598684549331665:  80%|████████  | 121/151 [00:26<00:06,  4.46it/s]Epoch: 19, train for the 122-th batch, train loss: 0.5598684549331665:  81%|████████  | 122/151 [00:26<00:06,  4.47it/s]Epoch: 4, train for the 104-th batch, train loss: 0.30317413806915283:  27%|██▋       | 103/383 [01:00<02:50,  1.64it/s]Epoch: 4, train for the 104-th batch, train loss: 0.30317413806915283:  27%|██▋       | 104/383 [01:00<02:47,  1.66it/s]Epoch: 19, train for the 123-th batch, train loss: 0.531920313835144:  81%|████████▉  | 122/151 [00:26<00:06,  4.47it/s]Epoch: 19, train for the 123-th batch, train loss: 0.531920313835144:  81%|████████▉  | 123/151 [00:26<00:06,  4.48it/s]Epoch: 6, train for the 104-th batch, train loss: 0.608146071434021:  43%|█████▏      | 103/237 [00:58<01:21,  1.64it/s]Epoch: 6, train for the 104-th batch, train loss: 0.608146071434021:  44%|█████▎      | 104/237 [00:58<01:20,  1.65it/s]Epoch: 11, train for the 1-th batch, train loss: 0.9744141101837158:   0%|                      | 0/119 [00:00<?, ?it/s]Epoch: 11, train for the 1-th batch, train loss: 0.9744141101837158:   1%|              | 1/119 [00:00<01:00,  1.95it/s]Epoch: 19, train for the 124-th batch, train loss: 0.5369450449943542:  81%|████████▏ | 123/151 [00:27<00:06,  4.48it/s]Epoch: 19, train for the 124-th batch, train loss: 0.5369450449943542:  82%|████████▏ | 124/151 [00:27<00:06,  4.49it/s]Epoch: 9, train for the 123-th batch, train loss: 0.5840309262275696:  84%|█████████▏ | 122/146 [01:14<00:13,  1.72it/s]Epoch: 9, train for the 123-th batch, train loss: 0.5840309262275696:  84%|█████████▎ | 123/146 [01:14<00:13,  1.71it/s]Epoch: 19, train for the 125-th batch, train loss: 0.5553802847862244:  82%|████████▏ | 124/151 [00:27<00:06,  4.49it/s]Epoch: 19, train for the 125-th batch, train loss: 0.5553802847862244:  83%|████████▎ | 125/151 [00:27<00:05,  4.42it/s]Epoch: 4, train for the 105-th batch, train loss: 0.290263295173645:  27%|███▎        | 104/383 [01:00<02:47,  1.66it/s]Epoch: 4, train for the 105-th batch, train loss: 0.290263295173645:  27%|███▎        | 105/383 [01:00<02:49,  1.64it/s]Epoch: 11, train for the 2-th batch, train loss: 0.48470792174339294:   1%|             | 1/119 [00:01<01:00,  1.95it/s]Epoch: 11, train for the 2-th batch, train loss: 0.48470792174339294:   2%|▏            | 2/119 [00:01<01:01,  1.89it/s]Epoch: 6, train for the 105-th batch, train loss: 0.5966021418571472:  44%|████▊      | 104/237 [00:59<01:20,  1.65it/s]Epoch: 6, train for the 105-th batch, train loss: 0.5966021418571472:  44%|████▊      | 105/237 [00:59<01:20,  1.64it/s]Epoch: 19, train for the 126-th batch, train loss: 0.5349757671356201:  83%|████████▎ | 125/151 [00:27<00:05,  4.42it/s]Epoch: 19, train for the 126-th batch, train loss: 0.5349757671356201:  83%|████████▎ | 126/151 [00:27<00:05,  4.44it/s]Epoch: 9, train for the 124-th batch, train loss: 0.5697564482688904:  84%|█████████▎ | 123/146 [01:14<00:13,  1.71it/s]Epoch: 9, train for the 124-th batch, train loss: 0.5697564482688904:  85%|█████████▎ | 124/146 [01:14<00:12,  1.71it/s]Epoch: 19, train for the 127-th batch, train loss: 0.5455703735351562:  83%|████████▎ | 126/151 [00:27<00:05,  4.44it/s]Epoch: 19, train for the 127-th batch, train loss: 0.5455703735351562:  84%|████████▍ | 127/151 [00:27<00:05,  4.46it/s]Epoch: 4, train for the 106-th batch, train loss: 0.3172658383846283:  27%|███        | 105/383 [01:01<02:49,  1.64it/s]Epoch: 4, train for the 106-th batch, train loss: 0.3172658383846283:  28%|███        | 106/383 [01:01<02:49,  1.64it/s]Epoch: 19, train for the 128-th batch, train loss: 0.5659469962120056:  84%|████████▍ | 127/151 [00:28<00:05,  4.46it/s]Epoch: 19, train for the 128-th batch, train loss: 0.5659469962120056:  85%|████████▍ | 128/151 [00:28<00:05,  4.46it/s]Epoch: 11, train for the 3-th batch, train loss: 0.33595335483551025:   2%|▏            | 2/119 [00:01<01:01,  1.89it/s]Epoch: 11, train for the 3-th batch, train loss: 0.33595335483551025:   3%|▎            | 3/119 [00:01<01:03,  1.82it/s]Epoch: 6, train for the 106-th batch, train loss: 0.6053462624549866:  44%|████▊      | 105/237 [01:00<01:20,  1.64it/s]Epoch: 6, train for the 106-th batch, train loss: 0.6053462624549866:  45%|████▉      | 106/237 [01:00<01:19,  1.64it/s]Epoch: 19, train for the 129-th batch, train loss: 0.5386437773704529:  85%|████████▍ | 128/151 [00:28<00:05,  4.46it/s]Epoch: 19, train for the 129-th batch, train loss: 0.5386437773704529:  85%|████████▌ | 129/151 [00:28<00:04,  4.47it/s]Epoch: 9, train for the 125-th batch, train loss: 0.510753333568573:  85%|██████████▏ | 124/146 [01:15<00:12,  1.71it/s]Epoch: 9, train for the 125-th batch, train loss: 0.510753333568573:  86%|██████████▎ | 125/146 [01:15<00:12,  1.70it/s]Epoch: 19, train for the 130-th batch, train loss: 0.5360960364341736:  85%|████████▌ | 129/151 [00:28<00:04,  4.47it/s]Epoch: 19, train for the 130-th batch, train loss: 0.5360960364341736:  86%|████████▌ | 130/151 [00:28<00:04,  4.47it/s]Epoch: 4, train for the 107-th batch, train loss: 0.40246832370758057:  28%|██▊       | 106/383 [01:01<02:49,  1.64it/s]Epoch: 4, train for the 107-th batch, train loss: 0.40246832370758057:  28%|██▊       | 107/383 [01:01<02:47,  1.64it/s]Epoch: 11, train for the 4-th batch, train loss: 0.3155568242073059:   3%|▎             | 3/119 [00:02<01:03,  1.82it/s]Epoch: 11, train for the 4-th batch, train loss: 0.3155568242073059:   3%|▍             | 4/119 [00:02<01:03,  1.80it/s]Epoch: 19, train for the 131-th batch, train loss: 0.5219634175300598:  86%|████████▌ | 130/151 [00:28<00:04,  4.47it/s]Epoch: 19, train for the 131-th batch, train loss: 0.5219634175300598:  87%|████████▋ | 131/151 [00:28<00:04,  4.44it/s]Epoch: 6, train for the 107-th batch, train loss: 0.5348758697509766:  45%|████▉      | 106/237 [01:00<01:19,  1.64it/s]Epoch: 6, train for the 107-th batch, train loss: 0.5348758697509766:  45%|████▉      | 107/237 [01:00<01:18,  1.65it/s]Epoch: 19, train for the 132-th batch, train loss: 0.5316635370254517:  87%|████████▋ | 131/151 [00:28<00:04,  4.44it/s]Epoch: 19, train for the 132-th batch, train loss: 0.5316635370254517:  87%|████████▋ | 132/151 [00:28<00:04,  4.46it/s]Epoch: 9, train for the 126-th batch, train loss: 0.571831226348877:  86%|██████████▎ | 125/146 [01:16<00:12,  1.70it/s]Epoch: 9, train for the 126-th batch, train loss: 0.571831226348877:  86%|██████████▎ | 126/146 [01:16<00:11,  1.72it/s]Epoch: 19, train for the 133-th batch, train loss: 0.5391552448272705:  87%|████████▋ | 132/151 [00:29<00:04,  4.46it/s]Epoch: 19, train for the 133-th batch, train loss: 0.5391552448272705:  88%|████████▊ | 133/151 [00:29<00:04,  4.47it/s]Epoch: 4, train for the 108-th batch, train loss: 0.27505984902381897:  28%|██▊       | 107/383 [01:02<02:47,  1.64it/s]Epoch: 4, train for the 108-th batch, train loss: 0.27505984902381897:  28%|██▊       | 108/383 [01:02<02:45,  1.66it/s]Epoch: 11, train for the 5-th batch, train loss: 0.35408660769462585:   3%|▍            | 4/119 [00:02<01:03,  1.80it/s]Epoch: 11, train for the 5-th batch, train loss: 0.35408660769462585:   4%|▌            | 5/119 [00:02<01:05,  1.74it/s]Epoch: 6, train for the 108-th batch, train loss: 0.630179226398468:  45%|█████▍      | 107/237 [01:01<01:18,  1.65it/s]Epoch: 6, train for the 108-th batch, train loss: 0.630179226398468:  46%|█████▍      | 108/237 [01:01<01:17,  1.66it/s]Epoch: 19, train for the 134-th batch, train loss: 0.5474519729614258:  88%|████████▊ | 133/151 [00:29<00:04,  4.47it/s]Epoch: 19, train for the 134-th batch, train loss: 0.5474519729614258:  89%|████████▊ | 134/151 [00:29<00:03,  4.48it/s]Epoch: 9, train for the 127-th batch, train loss: 0.5472586750984192:  86%|█████████▍ | 126/146 [01:16<00:11,  1.72it/s]Epoch: 9, train for the 127-th batch, train loss: 0.5472586750984192:  87%|█████████▌ | 127/146 [01:16<00:11,  1.71it/s]Epoch: 19, train for the 135-th batch, train loss: 0.5439271330833435:  89%|████████▊ | 134/151 [00:29<00:03,  4.48it/s]Epoch: 19, train for the 135-th batch, train loss: 0.5439271330833435:  89%|████████▉ | 135/151 [00:29<00:03,  4.45it/s]Epoch: 4, train for the 109-th batch, train loss: 0.3587280809879303:  28%|███        | 108/383 [01:03<02:45,  1.66it/s]Epoch: 4, train for the 109-th batch, train loss: 0.3587280809879303:  28%|███▏       | 109/383 [01:03<02:44,  1.66it/s]Epoch: 19, train for the 136-th batch, train loss: 0.550667405128479:  89%|█████████▊ | 135/151 [00:29<00:03,  4.45it/s]Epoch: 19, train for the 136-th batch, train loss: 0.550667405128479:  90%|█████████▉ | 136/151 [00:29<00:03,  4.45it/s]Epoch: 11, train for the 6-th batch, train loss: 0.28566688299179077:   4%|▌            | 5/119 [00:03<01:05,  1.74it/s]Epoch: 11, train for the 6-th batch, train loss: 0.28566688299179077:   5%|▋            | 6/119 [00:03<01:05,  1.72it/s]Epoch: 6, train for the 109-th batch, train loss: 0.5692037343978882:  46%|█████      | 108/237 [01:01<01:17,  1.66it/s]Epoch: 6, train for the 109-th batch, train loss: 0.5692037343978882:  46%|█████      | 109/237 [01:01<01:16,  1.67it/s]Epoch: 19, train for the 137-th batch, train loss: 0.5810601711273193:  90%|█████████ | 136/151 [00:30<00:03,  4.45it/s]Epoch: 19, train for the 137-th batch, train loss: 0.5810601711273193:  91%|█████████ | 137/151 [00:30<00:03,  4.47it/s]Epoch: 9, train for the 128-th batch, train loss: 0.5460865497589111:  87%|█████████▌ | 127/146 [01:17<00:11,  1.71it/s]Epoch: 9, train for the 128-th batch, train loss: 0.5460865497589111:  88%|█████████▋ | 128/146 [01:17<00:10,  1.70it/s]Epoch: 19, train for the 138-th batch, train loss: 0.6048085689544678:  91%|█████████ | 137/151 [00:30<00:03,  4.47it/s]Epoch: 19, train for the 138-th batch, train loss: 0.6048085689544678:  91%|█████████▏| 138/151 [00:30<00:02,  4.48it/s]Epoch: 4, train for the 110-th batch, train loss: 0.3540714979171753:  28%|███▏       | 109/383 [01:03<02:44,  1.66it/s]Epoch: 4, train for the 110-th batch, train loss: 0.3540714979171753:  29%|███▏       | 110/383 [01:03<02:43,  1.67it/s]Epoch: 11, train for the 7-th batch, train loss: 0.23765765130519867:   5%|▋            | 6/119 [00:03<01:05,  1.72it/s]Epoch: 11, train for the 7-th batch, train loss: 0.23765765130519867:   6%|▊            | 7/119 [00:03<01:05,  1.71it/s]Epoch: 19, train for the 139-th batch, train loss: 0.5605098009109497:  91%|█████████▏| 138/151 [00:30<00:02,  4.48it/s]Epoch: 19, train for the 139-th batch, train loss: 0.5605098009109497:  92%|█████████▏| 139/151 [00:30<00:02,  4.47it/s]Epoch: 6, train for the 110-th batch, train loss: 0.6334940791130066:  46%|█████      | 109/237 [01:02<01:16,  1.67it/s]Epoch: 6, train for the 110-th batch, train loss: 0.6334940791130066:  46%|█████      | 110/237 [01:02<01:15,  1.68it/s]Epoch: 19, train for the 140-th batch, train loss: 0.5076482892036438:  92%|█████████▏| 139/151 [00:30<00:02,  4.47it/s]Epoch: 19, train for the 140-th batch, train loss: 0.5076482892036438:  93%|█████████▎| 140/151 [00:30<00:02,  4.48it/s]Epoch: 9, train for the 129-th batch, train loss: 0.5928226113319397:  88%|█████████▋ | 128/146 [01:17<00:10,  1.70it/s]Epoch: 9, train for the 129-th batch, train loss: 0.5928226113319397:  88%|█████████▋ | 129/146 [01:17<00:10,  1.69it/s]Epoch: 19, train for the 141-th batch, train loss: 0.5811498761177063:  93%|█████████▎| 140/151 [00:30<00:02,  4.48it/s]Epoch: 19, train for the 141-th batch, train loss: 0.5811498761177063:  93%|█████████▎| 141/151 [00:30<00:02,  4.49it/s]Epoch: 4, train for the 111-th batch, train loss: 0.38032087683677673:  29%|██▊       | 110/383 [01:04<02:43,  1.67it/s]Epoch: 4, train for the 111-th batch, train loss: 0.38032087683677673:  29%|██▉       | 111/383 [01:04<02:42,  1.68it/s]Epoch: 11, train for the 8-th batch, train loss: 0.19398799538612366:   6%|▊            | 7/119 [00:04<01:05,  1.71it/s]Epoch: 11, train for the 8-th batch, train loss: 0.19398799538612366:   7%|▊            | 8/119 [00:04<01:05,  1.70it/s]Epoch: 6, train for the 111-th batch, train loss: 0.635970950126648:  46%|█████▌      | 110/237 [01:03<01:15,  1.68it/s]Epoch: 6, train for the 111-th batch, train loss: 0.635970950126648:  47%|█████▌      | 111/237 [01:03<01:14,  1.68it/s]Epoch: 19, train for the 142-th batch, train loss: 0.5628040432929993:  93%|█████████▎| 141/151 [00:31<00:02,  4.49it/s]Epoch: 19, train for the 142-th batch, train loss: 0.5628040432929993:  94%|█████████▍| 142/151 [00:31<00:02,  4.49it/s]Epoch: 9, train for the 130-th batch, train loss: 0.5259683728218079:  88%|█████████▋ | 129/146 [01:18<00:10,  1.69it/s]Epoch: 9, train for the 130-th batch, train loss: 0.5259683728218079:  89%|█████████▊ | 130/146 [01:18<00:09,  1.69it/s]Epoch: 19, train for the 143-th batch, train loss: 0.47445282340049744:  94%|████████▍| 142/151 [00:31<00:02,  4.49it/s]Epoch: 19, train for the 143-th batch, train loss: 0.47445282340049744:  95%|████████▌| 143/151 [00:31<00:01,  4.49it/s]Epoch: 4, train for the 112-th batch, train loss: 0.41608038544654846:  29%|██▉       | 111/383 [01:04<02:42,  1.68it/s]Epoch: 4, train for the 112-th batch, train loss: 0.41608038544654846:  29%|██▉       | 112/383 [01:04<02:41,  1.68it/s]Epoch: 19, train for the 144-th batch, train loss: 0.4997100234031677:  95%|█████████▍| 143/151 [00:31<00:01,  4.49it/s]Epoch: 19, train for the 144-th batch, train loss: 0.4997100234031677:  95%|█████████▌| 144/151 [00:31<00:01,  4.49it/s]Epoch: 11, train for the 9-th batch, train loss: 0.2390139400959015:   7%|▉             | 8/119 [00:05<01:05,  1.70it/s]Epoch: 11, train for the 9-th batch, train loss: 0.2390139400959015:   8%|█             | 9/119 [00:05<01:05,  1.69it/s]Epoch: 6, train for the 112-th batch, train loss: 0.6444132924079895:  47%|█████▏     | 111/237 [01:03<01:14,  1.68it/s]Epoch: 6, train for the 112-th batch, train loss: 0.6444132924079895:  47%|█████▏     | 112/237 [01:03<01:14,  1.68it/s]Epoch: 19, train for the 145-th batch, train loss: 0.5437242984771729:  95%|█████████▌| 144/151 [00:31<00:01,  4.49it/s]Epoch: 19, train for the 145-th batch, train loss: 0.5437242984771729:  96%|█████████▌| 145/151 [00:31<00:01,  4.48it/s]Epoch: 9, train for the 131-th batch, train loss: 0.5609186291694641:  89%|█████████▊ | 130/146 [01:19<00:09,  1.69it/s]Epoch: 9, train for the 131-th batch, train loss: 0.5609186291694641:  90%|█████████▊ | 131/146 [01:19<00:08,  1.68it/s]Epoch: 19, train for the 146-th batch, train loss: 0.5232813954353333:  96%|█████████▌| 145/151 [00:32<00:01,  4.48it/s]Epoch: 19, train for the 146-th batch, train loss: 0.5232813954353333:  97%|█████████▋| 146/151 [00:32<00:01,  4.48it/s]Epoch: 4, train for the 113-th batch, train loss: 0.3237389326095581:  29%|███▏       | 112/383 [01:05<02:41,  1.68it/s]Epoch: 4, train for the 113-th batch, train loss: 0.3237389326095581:  30%|███▏       | 113/383 [01:05<02:38,  1.70it/s]Epoch: 19, train for the 147-th batch, train loss: 0.5442661643028259:  97%|█████████▋| 146/151 [00:32<00:01,  4.48it/s]Epoch: 19, train for the 147-th batch, train loss: 0.5442661643028259:  97%|█████████▋| 147/151 [00:32<00:00,  4.48it/s]Epoch: 6, train for the 113-th batch, train loss: 0.4714810848236084:  47%|█████▏     | 112/237 [01:04<01:14,  1.68it/s]Epoch: 6, train for the 113-th batch, train loss: 0.4714810848236084:  48%|█████▏     | 113/237 [01:04<01:12,  1.71it/s]Epoch: 19, train for the 148-th batch, train loss: 0.5920337438583374:  97%|█████████▋| 147/151 [00:32<00:00,  4.48it/s]Epoch: 19, train for the 148-th batch, train loss: 0.5920337438583374:  98%|█████████▊| 148/151 [00:32<00:00,  4.48it/s]Epoch: 11, train for the 10-th batch, train loss: 0.24197633564472198:   8%|▉           | 9/119 [00:06<01:05,  1.69it/s]Epoch: 11, train for the 10-th batch, train loss: 0.24197633564472198:   8%|▉          | 10/119 [00:06<01:14,  1.47it/s]Epoch: 19, train for the 149-th batch, train loss: 0.5093732476234436:  98%|█████████▊| 148/151 [00:32<00:00,  4.48it/s]Epoch: 19, train for the 149-th batch, train loss: 0.5093732476234436:  99%|█████████▊| 149/151 [00:32<00:00,  4.48it/s]Epoch: 4, train for the 114-th batch, train loss: 0.33547714352607727:  30%|██▉       | 113/383 [01:05<02:38,  1.70it/s]Epoch: 4, train for the 114-th batch, train loss: 0.33547714352607727:  30%|██▉       | 114/383 [01:05<02:37,  1.71it/s]Epoch: 11, train for the 11-th batch, train loss: 0.21951930224895477:   8%|▉          | 10/119 [00:06<01:14,  1.47it/s]Epoch: 11, train for the 11-th batch, train loss: 0.21951930224895477:   9%|█          | 11/119 [00:06<00:58,  1.85it/s]Epoch: 6, train for the 114-th batch, train loss: 0.6480785012245178:  48%|█████▏     | 113/237 [01:04<01:12,  1.71it/s]Epoch: 6, train for the 114-th batch, train loss: 0.6480785012245178:  48%|█████▎     | 114/237 [01:04<01:12,  1.70it/s]Epoch: 9, train for the 132-th batch, train loss: 0.5608823299407959:  90%|█████████▊ | 131/146 [01:20<00:08,  1.68it/s]Epoch: 9, train for the 132-th batch, train loss: 0.5608823299407959:  90%|█████████▉ | 132/146 [01:20<00:09,  1.40it/s]Epoch: 19, train for the 150-th batch, train loss: 0.5489743947982788:  99%|█████████▊| 149/151 [00:33<00:00,  4.48it/s]Epoch: 19, train for the 150-th batch, train loss: 0.5489743947982788:  99%|█████████▉| 150/151 [00:33<00:00,  4.02it/s]Epoch: 19, train for the 151-th batch, train loss: 0.5542593002319336:  99%|█████████▉| 150/151 [00:33<00:00,  4.02it/s]Epoch: 19, train for the 151-th batch, train loss: 0.5542593002319336: 100%|██████████| 151/151 [00:33<00:00,  4.55it/s]Epoch: 19, train for the 151-th batch, train loss: 0.5542593002319336: 100%|██████████| 151/151 [00:33<00:00,  4.55it/s]
  0%|                                                                                            | 0/46 [00:00<?, ?it/s]Epoch: 11, train for the 12-th batch, train loss: 0.26013267040252686:   9%|█          | 11/119 [00:06<00:58,  1.85it/s]Epoch: 11, train for the 12-th batch, train loss: 0.26013267040252686:  10%|█          | 12/119 [00:06<00:55,  1.93it/s]Epoch: 4, train for the 115-th batch, train loss: 0.32288649678230286:  30%|██▉       | 114/383 [01:06<02:37,  1.71it/s]Epoch: 4, train for the 115-th batch, train loss: 0.32288649678230286:  30%|███       | 115/383 [01:06<02:29,  1.79it/s]evaluate for the 1-th batch, evaluate loss: 0.4943743348121643:   0%|                            | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.4943743348121643:   2%|▍                   | 1/46 [00:00<00:04,  9.57it/s]evaluate for the 2-th batch, evaluate loss: 0.5046901106834412:   2%|▍                   | 1/46 [00:00<00:04,  9.57it/s]evaluate for the 2-th batch, evaluate loss: 0.5046901106834412:   4%|▊                   | 2/46 [00:00<00:04,  9.55it/s]Epoch: 9, train for the 133-th batch, train loss: 0.5265626907348633:  90%|█████████▉ | 132/146 [01:20<00:09,  1.40it/s]Epoch: 9, train for the 133-th batch, train loss: 0.5265626907348633:  91%|██████████ | 133/146 [01:20<00:08,  1.49it/s]evaluate for the 3-th batch, evaluate loss: 0.4832670986652374:   4%|▊                   | 2/46 [00:00<00:04,  9.55it/s]evaluate for the 3-th batch, evaluate loss: 0.4832670986652374:   7%|█▎                  | 3/46 [00:00<00:04,  9.59it/s]Epoch: 4, train for the 116-th batch, train loss: 0.3484267592430115:  30%|███▎       | 115/383 [01:06<02:29,  1.79it/s]Epoch: 4, train for the 116-th batch, train loss: 0.3484267592430115:  30%|███▎       | 116/383 [01:06<02:12,  2.01it/s]evaluate for the 4-th batch, evaluate loss: 0.5051365494728088:   7%|█▎                  | 3/46 [00:00<00:04,  9.59it/s]evaluate for the 4-th batch, evaluate loss: 0.5051365494728088:   9%|█▋                  | 4/46 [00:00<00:04,  9.61it/s]evaluate for the 5-th batch, evaluate loss: 0.48042044043540955:   9%|█▋                 | 4/46 [00:00<00:04,  9.61it/s]evaluate for the 5-th batch, evaluate loss: 0.48042044043540955:  11%|██                 | 5/46 [00:00<00:04,  9.67it/s]Epoch: 6, train for the 115-th batch, train loss: 0.6359323263168335:  48%|█████▎     | 114/237 [01:05<01:12,  1.70it/s]Epoch: 6, train for the 115-th batch, train loss: 0.6359323263168335:  49%|█████▎     | 115/237 [01:05<01:22,  1.48it/s]Epoch: 11, train for the 13-th batch, train loss: 0.23259226977825165:  10%|█          | 12/119 [00:07<00:55,  1.93it/s]Epoch: 11, train for the 13-th batch, train loss: 0.23259226977825165:  11%|█▏         | 13/119 [00:07<00:56,  1.87it/s]evaluate for the 6-th batch, evaluate loss: 0.5511375069618225:  11%|██▏                 | 5/46 [00:00<00:04,  9.67it/s]evaluate for the 6-th batch, evaluate loss: 0.5511375069618225:  13%|██▌                 | 6/46 [00:00<00:04,  9.62it/s]evaluate for the 7-th batch, evaluate loss: 0.46881797909736633:  13%|██▍                | 6/46 [00:00<00:04,  9.62it/s]evaluate for the 7-th batch, evaluate loss: 0.46881797909736633:  15%|██▉                | 7/46 [00:00<00:04,  9.64it/s]Epoch: 4, train for the 117-th batch, train loss: 0.2927578091621399:  30%|███▎       | 116/383 [01:07<02:12,  2.01it/s]Epoch: 4, train for the 117-th batch, train loss: 0.2927578091621399:  31%|███▎       | 117/383 [01:07<02:02,  2.18it/s]evaluate for the 8-th batch, evaluate loss: 0.5546422004699707:  15%|███                 | 7/46 [00:00<00:04,  9.64it/s]evaluate for the 8-th batch, evaluate loss: 0.5546422004699707:  17%|███▍                | 8/46 [00:00<00:03,  9.62it/s]Epoch: 9, train for the 134-th batch, train loss: 0.5184938311576843:  91%|██████████ | 133/146 [01:21<00:08,  1.49it/s]Epoch: 9, train for the 134-th batch, train loss: 0.5184938311576843:  92%|██████████ | 134/146 [01:21<00:07,  1.56it/s]evaluate for the 9-th batch, evaluate loss: 0.5239543914794922:  17%|███▍                | 8/46 [00:00<00:03,  9.62it/s]evaluate for the 9-th batch, evaluate loss: 0.5239543914794922:  20%|███▉                | 9/46 [00:00<00:03,  9.61it/s]evaluate for the 10-th batch, evaluate loss: 0.5265942811965942:  20%|███▋               | 9/46 [00:01<00:03,  9.61it/s]evaluate for the 10-th batch, evaluate loss: 0.5265942811965942:  22%|███▉              | 10/46 [00:01<00:03,  9.60it/s]evaluate for the 11-th batch, evaluate loss: 0.5217269062995911:  22%|███▉              | 10/46 [00:01<00:03,  9.60it/s]evaluate for the 11-th batch, evaluate loss: 0.5217269062995911:  24%|████▎             | 11/46 [00:01<00:03,  9.60it/s]Epoch: 6, train for the 116-th batch, train loss: 0.6105270385742188:  49%|█████▎     | 115/237 [01:06<01:22,  1.48it/s]Epoch: 6, train for the 116-th batch, train loss: 0.6105270385742188:  49%|█████▍     | 116/237 [01:06<01:19,  1.53it/s]Epoch: 11, train for the 14-th batch, train loss: 0.24150842428207397:  11%|█▏         | 13/119 [00:07<00:56,  1.87it/s]Epoch: 11, train for the 14-th batch, train loss: 0.24150842428207397:  12%|█▎         | 14/119 [00:07<00:58,  1.80it/s]evaluate for the 12-th batch, evaluate loss: 0.46966856718063354:  24%|████             | 11/46 [00:01<00:03,  9.60it/s]evaluate for the 12-th batch, evaluate loss: 0.46966856718063354:  26%|████▍            | 12/46 [00:01<00:03,  9.61it/s]Epoch: 4, train for the 118-th batch, train loss: 0.37391477823257446:  31%|███       | 117/383 [01:07<02:02,  2.18it/s]Epoch: 4, train for the 118-th batch, train loss: 0.37391477823257446:  31%|███       | 118/383 [01:07<02:11,  2.01it/s]evaluate for the 13-th batch, evaluate loss: 0.4965507686138153:  26%|████▋             | 12/46 [00:01<00:03,  9.61it/s]evaluate for the 13-th batch, evaluate loss: 0.4965507686138153:  28%|█████             | 13/46 [00:01<00:03,  9.64it/s]evaluate for the 14-th batch, evaluate loss: 0.5830006003379822:  28%|█████             | 13/46 [00:01<00:03,  9.64it/s]evaluate for the 14-th batch, evaluate loss: 0.5830006003379822:  30%|█████▍            | 14/46 [00:01<00:03,  9.66it/s]evaluate for the 15-th batch, evaluate loss: 0.5361273288726807:  30%|█████▍            | 14/46 [00:01<00:03,  9.66it/s]evaluate for the 15-th batch, evaluate loss: 0.5361273288726807:  33%|█████▊            | 15/46 [00:01<00:03,  9.64it/s]Epoch: 9, train for the 135-th batch, train loss: 0.5230058431625366:  92%|██████████ | 134/146 [01:21<00:07,  1.56it/s]Epoch: 9, train for the 135-th batch, train loss: 0.5230058431625366:  92%|██████████▏| 135/146 [01:21<00:07,  1.51it/s]evaluate for the 16-th batch, evaluate loss: 0.5679097771644592:  33%|█████▊            | 15/46 [00:01<00:03,  9.64it/s]evaluate for the 16-th batch, evaluate loss: 0.5679097771644592:  35%|██████▎           | 16/46 [00:01<00:03,  9.64it/s]Epoch: 11, train for the 15-th batch, train loss: 0.20507611334323883:  12%|█▎         | 14/119 [00:08<00:58,  1.80it/s]Epoch: 11, train for the 15-th batch, train loss: 0.20507611334323883:  13%|█▍         | 15/119 [00:08<00:56,  1.85it/s]evaluate for the 17-th batch, evaluate loss: 0.4481903612613678:  35%|██████▎           | 16/46 [00:01<00:03,  9.64it/s]evaluate for the 17-th batch, evaluate loss: 0.4481903612613678:  37%|██████▋           | 17/46 [00:01<00:03,  9.62it/s]Epoch: 6, train for the 117-th batch, train loss: 0.5342088341712952:  49%|█████▍     | 116/237 [01:07<01:19,  1.53it/s]Epoch: 6, train for the 117-th batch, train loss: 0.5342088341712952:  49%|█████▍     | 117/237 [01:07<01:16,  1.56it/s]evaluate for the 18-th batch, evaluate loss: 0.4977136254310608:  37%|██████▋           | 17/46 [00:01<00:03,  9.62it/s]evaluate for the 18-th batch, evaluate loss: 0.4977136254310608:  39%|███████           | 18/46 [00:01<00:02,  9.60it/s]Epoch: 4, train for the 119-th batch, train loss: 0.39614439010620117:  31%|███       | 118/383 [01:08<02:11,  2.01it/s]Epoch: 4, train for the 119-th batch, train loss: 0.39614439010620117:  31%|███       | 119/383 [01:08<02:19,  1.89it/s]evaluate for the 19-th batch, evaluate loss: 0.5221961140632629:  39%|███████           | 18/46 [00:01<00:02,  9.60it/s]evaluate for the 19-th batch, evaluate loss: 0.5221961140632629:  41%|███████▍          | 19/46 [00:01<00:02,  9.60it/s]evaluate for the 20-th batch, evaluate loss: 0.5329461693763733:  41%|███████▍          | 19/46 [00:02<00:02,  9.60it/s]evaluate for the 20-th batch, evaluate loss: 0.5329461693763733:  43%|███████▊          | 20/46 [00:02<00:02,  9.61it/s]evaluate for the 21-th batch, evaluate loss: 0.524262011051178:  43%|████████▎          | 20/46 [00:02<00:02,  9.61it/s]evaluate for the 21-th batch, evaluate loss: 0.524262011051178:  46%|████████▋          | 21/46 [00:02<00:02,  9.61it/s]Epoch: 9, train for the 136-th batch, train loss: 0.5446231961250305:  92%|██████████▏| 135/146 [01:22<00:07,  1.51it/s]Epoch: 9, train for the 136-th batch, train loss: 0.5446231961250305:  93%|██████████▏| 136/146 [01:22<00:06,  1.56it/s]Epoch: 11, train for the 16-th batch, train loss: 0.25018569827079773:  13%|█▍         | 15/119 [00:09<00:56,  1.85it/s]Epoch: 11, train for the 16-th batch, train loss: 0.25018569827079773:  13%|█▍         | 16/119 [00:09<00:57,  1.80it/s]evaluate for the 22-th batch, evaluate loss: 0.5174686908721924:  46%|████████▏         | 21/46 [00:02<00:02,  9.61it/s]evaluate for the 22-th batch, evaluate loss: 0.5174686908721924:  48%|████████▌         | 22/46 [00:02<00:02,  9.63it/s]evaluate for the 23-th batch, evaluate loss: 0.4705052971839905:  48%|████████▌         | 22/46 [00:02<00:02,  9.63it/s]evaluate for the 23-th batch, evaluate loss: 0.4705052971839905:  50%|█████████         | 23/46 [00:02<00:02,  9.61it/s]Epoch: 6, train for the 118-th batch, train loss: 0.6024326086044312:  49%|█████▍     | 117/237 [01:07<01:16,  1.56it/s]Epoch: 6, train for the 118-th batch, train loss: 0.6024326086044312:  50%|█████▍     | 118/237 [01:07<01:15,  1.57it/s]evaluate for the 24-th batch, evaluate loss: 0.4815073311328888:  50%|█████████         | 23/46 [00:02<00:02,  9.61it/s]evaluate for the 24-th batch, evaluate loss: 0.4815073311328888:  52%|█████████▍        | 24/46 [00:02<00:02,  9.61it/s]Epoch: 4, train for the 120-th batch, train loss: 0.37933555245399475:  31%|███       | 119/383 [01:08<02:19,  1.89it/s]Epoch: 4, train for the 120-th batch, train loss: 0.37933555245399475:  31%|███▏      | 120/383 [01:08<02:26,  1.80it/s]evaluate for the 25-th batch, evaluate loss: 0.5361419320106506:  52%|█████████▍        | 24/46 [00:02<00:02,  9.61it/s]evaluate for the 25-th batch, evaluate loss: 0.5361419320106506:  54%|█████████▊        | 25/46 [00:02<00:02,  9.62it/s]evaluate for the 26-th batch, evaluate loss: 0.5530909895896912:  54%|█████████▊        | 25/46 [00:02<00:02,  9.62it/s]evaluate for the 26-th batch, evaluate loss: 0.5530909895896912:  57%|██████████▏       | 26/46 [00:02<00:02,  9.61it/s]Epoch: 9, train for the 137-th batch, train loss: 0.5237196087837219:  93%|██████████▏| 136/146 [01:23<00:06,  1.56it/s]Epoch: 9, train for the 137-th batch, train loss: 0.5237196087837219:  94%|██████████▎| 137/146 [01:23<00:05,  1.60it/s]evaluate for the 27-th batch, evaluate loss: 0.49079930782318115:  57%|█████████▌       | 26/46 [00:02<00:02,  9.61it/s]evaluate for the 27-th batch, evaluate loss: 0.49079930782318115:  59%|█████████▉       | 27/46 [00:02<00:01,  9.59it/s]Epoch: 11, train for the 17-th batch, train loss: 0.2049848437309265:  13%|█▌          | 16/119 [00:09<00:57,  1.80it/s]Epoch: 11, train for the 17-th batch, train loss: 0.2049848437309265:  14%|█▋          | 17/119 [00:09<00:58,  1.75it/s]evaluate for the 28-th batch, evaluate loss: 0.518007218837738:  59%|███████████▏       | 27/46 [00:02<00:01,  9.59it/s]evaluate for the 28-th batch, evaluate loss: 0.518007218837738:  61%|███████████▌       | 28/46 [00:02<00:01,  9.61it/s]Epoch: 6, train for the 119-th batch, train loss: 0.5193176865577698:  50%|█████▍     | 118/237 [01:08<01:15,  1.57it/s]Epoch: 6, train for the 119-th batch, train loss: 0.5193176865577698:  50%|█████▌     | 119/237 [01:08<01:13,  1.60it/s]evaluate for the 29-th batch, evaluate loss: 0.49169471859931946:  61%|██████████▎      | 28/46 [00:03<00:01,  9.61it/s]evaluate for the 29-th batch, evaluate loss: 0.49169471859931946:  63%|██████████▋      | 29/46 [00:03<00:01,  9.59it/s]evaluate for the 30-th batch, evaluate loss: 0.49117523431777954:  63%|██████████▋      | 29/46 [00:03<00:01,  9.59it/s]evaluate for the 30-th batch, evaluate loss: 0.49117523431777954:  65%|███████████      | 30/46 [00:03<00:01,  9.57it/s]Epoch: 4, train for the 121-th batch, train loss: 0.377236008644104:  31%|███▊        | 120/383 [01:09<02:26,  1.80it/s]Epoch: 4, train for the 121-th batch, train loss: 0.377236008644104:  32%|███▊        | 121/383 [01:09<02:29,  1.75it/s]evaluate for the 31-th batch, evaluate loss: 0.5149550437927246:  65%|███████████▋      | 30/46 [00:03<00:01,  9.57it/s]evaluate for the 31-th batch, evaluate loss: 0.5149550437927246:  67%|████████████▏     | 31/46 [00:03<00:01,  9.57it/s]evaluate for the 32-th batch, evaluate loss: 0.4769820272922516:  67%|████████████▏     | 31/46 [00:03<00:01,  9.57it/s]evaluate for the 32-th batch, evaluate loss: 0.4769820272922516:  70%|████████████▌     | 32/46 [00:03<00:01,  9.59it/s]Epoch: 9, train for the 138-th batch, train loss: 0.5570622682571411:  94%|██████████▎| 137/146 [01:23<00:05,  1.60it/s]Epoch: 9, train for the 138-th batch, train loss: 0.5570622682571411:  95%|██████████▍| 138/146 [01:23<00:04,  1.63it/s]evaluate for the 33-th batch, evaluate loss: 0.49377182126045227:  70%|███████████▊     | 32/46 [00:03<00:01,  9.59it/s]evaluate for the 33-th batch, evaluate loss: 0.49377182126045227:  72%|████████████▏    | 33/46 [00:03<00:01,  9.60it/s]Epoch: 11, train for the 18-th batch, train loss: 0.21712474524974823:  14%|█▌         | 17/119 [00:10<00:58,  1.75it/s]Epoch: 11, train for the 18-th batch, train loss: 0.21712474524974823:  15%|█▋         | 18/119 [00:10<00:58,  1.72it/s]evaluate for the 34-th batch, evaluate loss: 0.48170730471611023:  72%|████████████▏    | 33/46 [00:03<00:01,  9.60it/s]evaluate for the 34-th batch, evaluate loss: 0.48170730471611023:  74%|████████████▌    | 34/46 [00:03<00:01,  9.62it/s]Epoch: 6, train for the 120-th batch, train loss: 0.5610924959182739:  50%|█████▌     | 119/237 [01:08<01:13,  1.60it/s]Epoch: 6, train for the 120-th batch, train loss: 0.5610924959182739:  51%|█████▌     | 120/237 [01:08<01:12,  1.61it/s]evaluate for the 35-th batch, evaluate loss: 0.4835532307624817:  74%|█████████████▎    | 34/46 [00:03<00:01,  9.62it/s]evaluate for the 35-th batch, evaluate loss: 0.4835532307624817:  76%|█████████████▋    | 35/46 [00:03<00:01,  9.62it/s]evaluate for the 36-th batch, evaluate loss: 0.46733957529067993:  76%|████████████▉    | 35/46 [00:03<00:01,  9.62it/s]evaluate for the 36-th batch, evaluate loss: 0.46733957529067993:  78%|█████████████▎   | 36/46 [00:03<00:01,  9.62it/s]Epoch: 4, train for the 122-th batch, train loss: 0.42365142703056335:  32%|███▏      | 121/383 [01:10<02:29,  1.75it/s]Epoch: 4, train for the 122-th batch, train loss: 0.42365142703056335:  32%|███▏      | 122/383 [01:10<02:31,  1.72it/s]evaluate for the 37-th batch, evaluate loss: 0.49860167503356934:  78%|█████████████▎   | 36/46 [00:03<00:01,  9.62it/s]evaluate for the 37-th batch, evaluate loss: 0.49860167503356934:  80%|█████████████▋   | 37/46 [00:03<00:00,  9.64it/s]Epoch: 9, train for the 139-th batch, train loss: 0.5763303637504578:  95%|██████████▍| 138/146 [01:24<00:04,  1.63it/s]Epoch: 9, train for the 139-th batch, train loss: 0.5763303637504578:  95%|██████████▍| 139/146 [01:24<00:04,  1.65it/s]evaluate for the 38-th batch, evaluate loss: 0.5310836434364319:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.64it/s]evaluate for the 38-th batch, evaluate loss: 0.5310836434364319:  83%|██████████████▊   | 38/46 [00:03<00:00,  9.62it/s]evaluate for the 39-th batch, evaluate loss: 0.530860185623169:  83%|███████████████▋   | 38/46 [00:04<00:00,  9.62it/s]evaluate for the 39-th batch, evaluate loss: 0.530860185623169:  85%|████████████████   | 39/46 [00:04<00:00,  9.62it/s]Epoch: 11, train for the 19-th batch, train loss: 0.18087612092494965:  15%|█▋         | 18/119 [00:10<00:58,  1.72it/s]Epoch: 11, train for the 19-th batch, train loss: 0.18087612092494965:  16%|█▊         | 19/119 [00:10<00:58,  1.70it/s]evaluate for the 40-th batch, evaluate loss: 0.46858546137809753:  85%|██████████████▍  | 39/46 [00:04<00:00,  9.62it/s]evaluate for the 40-th batch, evaluate loss: 0.46858546137809753:  87%|██████████████▊  | 40/46 [00:04<00:00,  9.59it/s]Epoch: 6, train for the 121-th batch, train loss: 0.65184086561203:  51%|██████▌      | 120/237 [01:09<01:12,  1.61it/s]Epoch: 6, train for the 121-th batch, train loss: 0.65184086561203:  51%|██████▋      | 121/237 [01:09<01:11,  1.61it/s]evaluate for the 41-th batch, evaluate loss: 0.47686338424682617:  87%|██████████████▊  | 40/46 [00:04<00:00,  9.59it/s]evaluate for the 41-th batch, evaluate loss: 0.47686338424682617:  89%|███████████████▏ | 41/46 [00:04<00:00,  9.61it/s]evaluate for the 42-th batch, evaluate loss: 0.47027960419654846:  89%|███████████████▏ | 41/46 [00:04<00:00,  9.61it/s]evaluate for the 42-th batch, evaluate loss: 0.47027960419654846:  91%|███████████████▌ | 42/46 [00:04<00:00,  9.60it/s]Epoch: 4, train for the 123-th batch, train loss: 0.3519900441169739:  32%|███▌       | 122/383 [01:10<02:31,  1.72it/s]Epoch: 4, train for the 123-th batch, train loss: 0.3519900441169739:  32%|███▌       | 123/383 [01:10<02:33,  1.69it/s]evaluate for the 43-th batch, evaluate loss: 0.5304126739501953:  91%|████████████████▍ | 42/46 [00:04<00:00,  9.60it/s]evaluate for the 43-th batch, evaluate loss: 0.5304126739501953:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.61it/s]Epoch: 9, train for the 140-th batch, train loss: 0.49591270089149475:  95%|█████████▌| 139/146 [01:24<00:04,  1.65it/s]Epoch: 9, train for the 140-th batch, train loss: 0.49591270089149475:  96%|█████████▌| 140/146 [01:24<00:03,  1.66it/s]evaluate for the 44-th batch, evaluate loss: 0.5115954279899597:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.61it/s]evaluate for the 44-th batch, evaluate loss: 0.5115954279899597:  96%|█████████████████▏| 44/46 [00:04<00:00,  9.62it/s]evaluate for the 45-th batch, evaluate loss: 0.4906657338142395:  96%|█████████████████▏| 44/46 [00:04<00:00,  9.62it/s]evaluate for the 45-th batch, evaluate loss: 0.4906657338142395:  98%|█████████████████▌| 45/46 [00:04<00:00,  9.62it/s]Epoch: 11, train for the 20-th batch, train loss: 0.2374630719423294:  16%|█▉          | 19/119 [00:11<00:58,  1.70it/s]Epoch: 11, train for the 20-th batch, train loss: 0.2374630719423294:  17%|██          | 20/119 [00:11<00:58,  1.69it/s]evaluate for the 46-th batch, evaluate loss: 0.5046378374099731:  98%|█████████████████▌| 45/46 [00:04<00:00,  9.62it/s]evaluate for the 46-th batch, evaluate loss: 0.5046378374099731: 100%|██████████████████| 46/46 [00:04<00:00,  9.64it/s]
  0%|                                                                                            | 0/25 [00:00<?, ?it/s]Epoch: 6, train for the 122-th batch, train loss: 0.597478449344635:  51%|██████▏     | 121/237 [01:10<01:11,  1.61it/s]Epoch: 6, train for the 122-th batch, train loss: 0.597478449344635:  51%|██████▏     | 122/237 [01:10<01:10,  1.62it/s]evaluate for the 1-th batch, evaluate loss: 0.6331725716590881:   0%|                            | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6331725716590881:   4%|▊                   | 1/25 [00:00<00:02,  9.19it/s]evaluate for the 2-th batch, evaluate loss: 0.6498072147369385:   4%|▊                   | 1/25 [00:00<00:02,  9.19it/s]evaluate for the 2-th batch, evaluate loss: 0.6498072147369385:   8%|█▌                  | 2/25 [00:00<00:02,  9.18it/s]Epoch: 4, train for the 124-th batch, train loss: 0.35403937101364136:  32%|███▏      | 123/383 [01:11<02:33,  1.69it/s]Epoch: 4, train for the 124-th batch, train loss: 0.35403937101364136:  32%|███▏      | 124/383 [01:11<02:34,  1.68it/s]evaluate for the 3-th batch, evaluate loss: 0.6887932419776917:   8%|█▌                  | 2/25 [00:00<00:02,  9.18it/s]evaluate for the 3-th batch, evaluate loss: 0.6887932419776917:  12%|██▍                 | 3/25 [00:00<00:02,  9.17it/s]Epoch: 9, train for the 141-th batch, train loss: 0.5328434705734253:  96%|██████████▌| 140/146 [01:25<00:03,  1.66it/s]Epoch: 9, train for the 141-th batch, train loss: 0.5328434705734253:  97%|██████████▌| 141/146 [01:25<00:03,  1.65it/s]evaluate for the 4-th batch, evaluate loss: 0.6667985916137695:  12%|██▍                 | 3/25 [00:00<00:02,  9.17it/s]evaluate for the 4-th batch, evaluate loss: 0.6667985916137695:  16%|███▏                | 4/25 [00:00<00:02,  9.10it/s]Epoch: 11, train for the 21-th batch, train loss: 0.26656627655029297:  17%|█▊         | 20/119 [00:12<00:58,  1.69it/s]Epoch: 11, train for the 21-th batch, train loss: 0.26656627655029297:  18%|█▉         | 21/119 [00:12<00:58,  1.67it/s]evaluate for the 5-th batch, evaluate loss: 0.6669328808784485:  16%|███▏                | 4/25 [00:00<00:02,  9.10it/s]evaluate for the 5-th batch, evaluate loss: 0.6669328808784485:  20%|████                | 5/25 [00:00<00:02,  9.10it/s]evaluate for the 6-th batch, evaluate loss: 0.7095670700073242:  20%|████                | 5/25 [00:00<00:02,  9.10it/s]evaluate for the 6-th batch, evaluate loss: 0.7095670700073242:  24%|████▊               | 6/25 [00:00<00:02,  9.11it/s]Epoch: 6, train for the 123-th batch, train loss: 0.6275468468666077:  51%|█████▋     | 122/237 [01:10<01:10,  1.62it/s]Epoch: 6, train for the 123-th batch, train loss: 0.6275468468666077:  52%|█████▋     | 123/237 [01:10<01:09,  1.63it/s]evaluate for the 7-th batch, evaluate loss: 0.7314268350601196:  24%|████▊               | 6/25 [00:00<00:02,  9.11it/s]evaluate for the 7-th batch, evaluate loss: 0.7314268350601196:  28%|█████▌              | 7/25 [00:00<00:01,  9.11it/s]Epoch: 4, train for the 125-th batch, train loss: 0.35827821493148804:  32%|███▏      | 124/383 [01:12<02:34,  1.68it/s]Epoch: 4, train for the 125-th batch, train loss: 0.35827821493148804:  33%|███▎      | 125/383 [01:12<02:34,  1.67it/s]evaluate for the 8-th batch, evaluate loss: 0.7127245664596558:  28%|█████▌              | 7/25 [00:00<00:01,  9.11it/s]evaluate for the 8-th batch, evaluate loss: 0.7127245664596558:  32%|██████▍             | 8/25 [00:00<00:01,  9.12it/s]evaluate for the 9-th batch, evaluate loss: 0.6912127733230591:  32%|██████▍             | 8/25 [00:00<00:01,  9.12it/s]evaluate for the 9-th batch, evaluate loss: 0.6912127733230591:  36%|███████▏            | 9/25 [00:00<00:01,  9.15it/s]Epoch: 9, train for the 142-th batch, train loss: 0.5039934515953064:  97%|██████████▌| 141/146 [01:26<00:03,  1.65it/s]Epoch: 9, train for the 142-th batch, train loss: 0.5039934515953064:  97%|██████████▋| 142/146 [01:26<00:02,  1.64it/s]evaluate for the 10-th batch, evaluate loss: 0.7251795530319214:  36%|██████▊            | 9/25 [00:01<00:01,  9.15it/s]evaluate for the 10-th batch, evaluate loss: 0.7251795530319214:  40%|███████▏          | 10/25 [00:01<00:01,  9.17it/s]Epoch: 11, train for the 22-th batch, train loss: 0.2365608513355255:  18%|██          | 21/119 [00:12<00:58,  1.67it/s]Epoch: 11, train for the 22-th batch, train loss: 0.2365608513355255:  18%|██▏         | 22/119 [00:12<00:58,  1.66it/s]evaluate for the 11-th batch, evaluate loss: 0.7237585186958313:  40%|███████▏          | 10/25 [00:01<00:01,  9.17it/s]evaluate for the 11-th batch, evaluate loss: 0.7237585186958313:  44%|███████▉          | 11/25 [00:01<00:01,  9.17it/s]Epoch: 6, train for the 124-th batch, train loss: 0.6145453453063965:  52%|█████▋     | 123/237 [01:11<01:09,  1.63it/s]Epoch: 6, train for the 124-th batch, train loss: 0.6145453453063965:  52%|█████▊     | 124/237 [01:11<01:09,  1.63it/s]evaluate for the 12-th batch, evaluate loss: 0.6973631381988525:  44%|███████▉          | 11/25 [00:01<00:01,  9.17it/s]evaluate for the 12-th batch, evaluate loss: 0.6973631381988525:  48%|████████▋         | 12/25 [00:01<00:01,  9.16it/s]evaluate for the 13-th batch, evaluate loss: 0.6553425192832947:  48%|████████▋         | 12/25 [00:01<00:01,  9.16it/s]evaluate for the 13-th batch, evaluate loss: 0.6553425192832947:  52%|█████████▎        | 13/25 [00:01<00:01,  9.17it/s]Epoch: 4, train for the 126-th batch, train loss: 0.33528006076812744:  33%|███▎      | 125/383 [01:12<02:34,  1.67it/s]Epoch: 4, train for the 126-th batch, train loss: 0.33528006076812744:  33%|███▎      | 126/383 [01:12<02:34,  1.66it/s]evaluate for the 14-th batch, evaluate loss: 0.7454077005386353:  52%|█████████▎        | 13/25 [00:01<00:01,  9.17it/s]evaluate for the 14-th batch, evaluate loss: 0.7454077005386353:  56%|██████████        | 14/25 [00:01<00:01,  9.19it/s]Epoch: 9, train for the 143-th batch, train loss: 0.5483271479606628:  97%|██████████▋| 142/146 [01:26<00:02,  1.64it/s]Epoch: 9, train for the 143-th batch, train loss: 0.5483271479606628:  98%|██████████▊| 143/146 [01:26<00:01,  1.64it/s]evaluate for the 15-th batch, evaluate loss: 0.7195481061935425:  56%|██████████        | 14/25 [00:01<00:01,  9.19it/s]evaluate for the 15-th batch, evaluate loss: 0.7195481061935425:  60%|██████████▊       | 15/25 [00:01<00:01,  9.19it/s]evaluate for the 16-th batch, evaluate loss: 0.664272665977478:  60%|███████████▍       | 15/25 [00:01<00:01,  9.19it/s]evaluate for the 16-th batch, evaluate loss: 0.664272665977478:  64%|████████████▏      | 16/25 [00:01<00:00,  9.21it/s]Epoch: 11, train for the 23-th batch, train loss: 0.22917339205741882:  18%|██         | 22/119 [00:13<00:58,  1.66it/s]Epoch: 11, train for the 23-th batch, train loss: 0.22917339205741882:  19%|██▏        | 23/119 [00:13<00:58,  1.65it/s]evaluate for the 17-th batch, evaluate loss: 0.6577905416488647:  64%|███████████▌      | 16/25 [00:01<00:00,  9.21it/s]evaluate for the 17-th batch, evaluate loss: 0.6577905416488647:  68%|████████████▏     | 17/25 [00:01<00:00,  9.22it/s]Epoch: 6, train for the 125-th batch, train loss: 0.5479409098625183:  52%|█████▊     | 124/237 [01:11<01:09,  1.63it/s]Epoch: 6, train for the 125-th batch, train loss: 0.5479409098625183:  53%|█████▊     | 125/237 [01:11<01:08,  1.64it/s]evaluate for the 18-th batch, evaluate loss: 0.6280622482299805:  68%|████████████▏     | 17/25 [00:01<00:00,  9.22it/s]evaluate for the 18-th batch, evaluate loss: 0.6280622482299805:  72%|████████████▉     | 18/25 [00:01<00:00,  9.21it/s]Epoch: 4, train for the 127-th batch, train loss: 0.3643608093261719:  33%|███▌       | 126/383 [01:13<02:34,  1.66it/s]Epoch: 4, train for the 127-th batch, train loss: 0.3643608093261719:  33%|███▋       | 127/383 [01:13<02:34,  1.65it/s]evaluate for the 19-th batch, evaluate loss: 0.5909125804901123:  72%|████████████▉     | 18/25 [00:02<00:00,  9.21it/s]evaluate for the 19-th batch, evaluate loss: 0.5909125804901123:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.19it/s]evaluate for the 20-th batch, evaluate loss: 0.6541233658790588:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.19it/s]evaluate for the 20-th batch, evaluate loss: 0.6541233658790588:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.21it/s]Epoch: 9, train for the 144-th batch, train loss: 0.5160007476806641:  98%|██████████▊| 143/146 [01:27<00:01,  1.64it/s]Epoch: 9, train for the 144-th batch, train loss: 0.5160007476806641:  99%|██████████▊| 144/146 [01:27<00:01,  1.64it/s]evaluate for the 21-th batch, evaluate loss: 0.7197116017341614:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.21it/s]evaluate for the 21-th batch, evaluate loss: 0.7197116017341614:  84%|███████████████   | 21/25 [00:02<00:00,  9.22it/s]Epoch: 11, train for the 24-th batch, train loss: 0.18879760801792145:  19%|██▏        | 23/119 [00:13<00:58,  1.65it/s]Epoch: 11, train for the 24-th batch, train loss: 0.18879760801792145:  20%|██▏        | 24/119 [00:13<00:57,  1.64it/s]evaluate for the 22-th batch, evaluate loss: 0.5974230170249939:  84%|███████████████   | 21/25 [00:02<00:00,  9.22it/s]evaluate for the 22-th batch, evaluate loss: 0.5974230170249939:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.21it/s]Epoch: 6, train for the 126-th batch, train loss: 0.6350640058517456:  53%|█████▊     | 125/237 [01:12<01:08,  1.64it/s]Epoch: 6, train for the 126-th batch, train loss: 0.6350640058517456:  53%|█████▊     | 126/237 [01:12<01:07,  1.64it/s]evaluate for the 23-th batch, evaluate loss: 0.6560131907463074:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.21it/s]evaluate for the 23-th batch, evaluate loss: 0.6560131907463074:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.20it/s]evaluate for the 24-th batch, evaluate loss: 0.6536491513252258:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.20it/s]evaluate for the 24-th batch, evaluate loss: 0.6536491513252258:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.19it/s]Epoch: 4, train for the 128-th batch, train loss: 0.4410921335220337:  33%|███▋       | 127/383 [01:13<02:34,  1.65it/s]Epoch: 4, train for the 128-th batch, train loss: 0.4410921335220337:  33%|███▋       | 128/383 [01:13<02:34,  1.65it/s]evaluate for the 25-th batch, evaluate loss: 0.6993721723556519:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.19it/s]evaluate for the 25-th batch, evaluate loss: 0.6993721723556519: 100%|██████████████████| 25/25 [00:02<00:00,  9.24it/s]
INFO:root:Epoch: 19, learning rate: 0.0001, train loss: 0.5572
INFO:root:train average_precision, 0.8230
INFO:root:train roc_auc, 0.7897
INFO:root:validate loss: 0.5060
INFO:root:validate average_precision, 0.8452
INFO:root:validate roc_auc, 0.8077
INFO:root:new node validate loss: 0.6775
INFO:root:new node validate first_1_average_precision, 0.5945
INFO:root:new node validate first_1_roc_auc, 0.5447
INFO:root:new node validate first_3_average_precision, 0.6761
INFO:root:new node validate first_3_roc_auc, 0.6378
INFO:root:new node validate first_10_average_precision, 0.7464
INFO:root:new node validate first_10_roc_auc, 0.7118
INFO:root:new node validate average_precision, 0.7116
INFO:root:new node validate roc_auc, 0.6624
INFO:root:save model ./saved_models/DyGFormer/ia-retweet-pol/DyGFormer_seed0_dygformer-ia-retweet-pol-old/DyGFormer_seed0_dygformer-ia-retweet-pol-old.pkl
Epoch: 9, train for the 145-th batch, train loss: 0.5623104572296143:  99%|██████████▊| 144/146 [01:27<00:01,  1.64it/s]Epoch: 9, train for the 145-th batch, train loss: 0.5623104572296143:  99%|██████████▉| 145/146 [01:27<00:00,  1.63it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]Epoch: 11, train for the 25-th batch, train loss: 0.2313476949930191:  20%|██▍         | 24/119 [00:14<00:57,  1.64it/s]Epoch: 11, train for the 25-th batch, train loss: 0.2313476949930191:  21%|██▌         | 25/119 [00:14<00:56,  1.68it/s]Epoch: 20, train for the 1-th batch, train loss: 1.041341781616211:   0%|                       | 0/151 [00:00<?, ?it/s]Epoch: 20, train for the 1-th batch, train loss: 1.041341781616211:   1%|               | 1/151 [00:00<00:25,  5.88it/s]Epoch: 6, train for the 127-th batch, train loss: 0.6086406707763672:  53%|█████▊     | 126/237 [01:13<01:07,  1.64it/s]Epoch: 6, train for the 127-th batch, train loss: 0.6086406707763672:  54%|█████▉     | 127/237 [01:13<01:07,  1.63it/s]Epoch: 20, train for the 2-th batch, train loss: 1.1049216985702515:   1%|              | 1/151 [00:00<00:25,  5.88it/s]Epoch: 20, train for the 2-th batch, train loss: 1.1049216985702515:   1%|▏             | 2/151 [00:00<00:25,  5.78it/s]Epoch: 9, train for the 146-th batch, train loss: 0.49636587500572205:  99%|█████████▉| 145/146 [01:28<00:00,  1.63it/s]Epoch: 9, train for the 146-th batch, train loss: 0.49636587500572205: 100%|██████████| 146/146 [01:28<00:00,  1.81it/s]Epoch: 9, train for the 146-th batch, train loss: 0.49636587500572205: 100%|██████████| 146/146 [01:28<00:00,  1.65it/s]
  0%|                                                                                            | 0/38 [00:00<?, ?it/s]Epoch: 4, train for the 129-th batch, train loss: 0.3082929253578186:  33%|███▋       | 128/383 [01:14<02:34,  1.65it/s]Epoch: 4, train for the 129-th batch, train loss: 0.3082929253578186:  34%|███▋       | 129/383 [01:14<02:34,  1.64it/s]Epoch: 20, train for the 3-th batch, train loss: 0.4103296101093292:   1%|▏             | 2/151 [00:00<00:25,  5.78it/s]Epoch: 20, train for the 3-th batch, train loss: 0.4103296101093292:   2%|▎             | 3/151 [00:00<00:24,  5.99it/s]Epoch: 20, train for the 4-th batch, train loss: 0.6004327535629272:   2%|▎             | 3/151 [00:00<00:24,  5.99it/s]Epoch: 20, train for the 4-th batch, train loss: 0.6004327535629272:   3%|▎             | 4/151 [00:00<00:24,  5.96it/s]Epoch: 11, train for the 26-th batch, train loss: 0.20382189750671387:  21%|██▎        | 25/119 [00:15<00:56,  1.68it/s]Epoch: 11, train for the 26-th batch, train loss: 0.20382189750671387:  22%|██▍        | 26/119 [00:15<00:55,  1.68it/s]evaluate for the 1-th batch, evaluate loss: 0.487160325050354:   0%|                             | 0/38 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.487160325050354:   3%|▌                    | 1/38 [00:00<00:11,  3.22it/s]Epoch: 20, train for the 5-th batch, train loss: 0.6598544120788574:   3%|▎             | 4/151 [00:00<00:24,  5.96it/s]Epoch: 20, train for the 5-th batch, train loss: 0.6598544120788574:   3%|▍             | 5/151 [00:00<00:25,  5.68it/s]Epoch: 6, train for the 128-th batch, train loss: 0.6219682693481445:  54%|█████▉     | 127/237 [01:13<01:07,  1.63it/s]Epoch: 6, train for the 128-th batch, train loss: 0.6219682693481445:  54%|█████▉     | 128/237 [01:13<01:06,  1.63it/s]evaluate for the 2-th batch, evaluate loss: 0.48930853605270386:   3%|▌                  | 1/38 [00:00<00:11,  3.22it/s]evaluate for the 2-th batch, evaluate loss: 0.48930853605270386:   5%|█                  | 2/38 [00:00<00:10,  3.58it/s]Epoch: 4, train for the 130-th batch, train loss: 0.3395984470844269:  34%|███▋       | 129/383 [01:15<02:34,  1.64it/s]Epoch: 4, train for the 130-th batch, train loss: 0.3395984470844269:  34%|███▋       | 130/383 [01:15<02:34,  1.64it/s]Epoch: 20, train for the 6-th batch, train loss: 0.5773817896842957:   3%|▍             | 5/151 [00:01<00:25,  5.68it/s]Epoch: 20, train for the 6-th batch, train loss: 0.5773817896842957:   4%|▌             | 6/151 [00:01<00:26,  5.54it/s]Epoch: 20, train for the 7-th batch, train loss: 0.6622393727302551:   4%|▌             | 6/151 [00:01<00:26,  5.54it/s]Epoch: 20, train for the 7-th batch, train loss: 0.6622393727302551:   5%|▋             | 7/151 [00:01<00:26,  5.34it/s]Epoch: 11, train for the 27-th batch, train loss: 0.20167861878871918:  22%|██▍        | 26/119 [00:15<00:55,  1.68it/s]Epoch: 11, train for the 27-th batch, train loss: 0.20167861878871918:  23%|██▍        | 27/119 [00:15<00:55,  1.67it/s]evaluate for the 3-th batch, evaluate loss: 0.4900737702846527:   5%|█                   | 2/38 [00:00<00:10,  3.58it/s]evaluate for the 3-th batch, evaluate loss: 0.4900737702846527:   8%|█▌                  | 3/38 [00:00<00:10,  3.35it/s]Epoch: 6, train for the 129-th batch, train loss: 0.625286877155304:  54%|██████▍     | 128/237 [01:14<01:06,  1.63it/s]Epoch: 6, train for the 129-th batch, train loss: 0.625286877155304:  54%|██████▌     | 129/237 [01:14<01:01,  1.76it/s]Epoch: 20, train for the 8-th batch, train loss: 0.6357610821723938:   5%|▋             | 7/151 [00:01<00:26,  5.34it/s]Epoch: 20, train for the 8-th batch, train loss: 0.6357610821723938:   5%|▋             | 8/151 [00:01<00:26,  5.36it/s]evaluate for the 4-th batch, evaluate loss: 0.4779752194881439:   8%|█▌                  | 3/38 [00:01<00:10,  3.35it/s]evaluate for the 4-th batch, evaluate loss: 0.4779752194881439:  11%|██                  | 4/38 [00:01<00:09,  3.47it/s]Epoch: 20, train for the 9-th batch, train loss: 0.6216246485710144:   5%|▋             | 8/151 [00:01<00:26,  5.36it/s]Epoch: 20, train for the 9-th batch, train loss: 0.6216246485710144:   6%|▊             | 9/151 [00:01<00:26,  5.30it/s]Epoch: 4, train for the 131-th batch, train loss: 0.35335269570350647:  34%|███▍      | 130/383 [01:15<02:34,  1.64it/s]Epoch: 4, train for the 131-th batch, train loss: 0.35335269570350647:  34%|███▍      | 131/383 [01:15<02:33,  1.64it/s]Epoch: 20, train for the 10-th batch, train loss: 0.5763391256332397:   6%|▊            | 9/151 [00:01<00:26,  5.30it/s]Epoch: 20, train for the 10-th batch, train loss: 0.5763391256332397:   7%|▊           | 10/151 [00:01<00:27,  5.19it/s]evaluate for the 5-th batch, evaluate loss: 0.5133525729179382:  11%|██                  | 4/38 [00:01<00:09,  3.47it/s]evaluate for the 5-th batch, evaluate loss: 0.5133525729179382:  13%|██▋                 | 5/38 [00:01<00:09,  3.43it/s]Epoch: 11, train for the 28-th batch, train loss: 0.21485355496406555:  23%|██▍        | 27/119 [00:16<00:55,  1.67it/s]Epoch: 11, train for the 28-th batch, train loss: 0.21485355496406555:  24%|██▌        | 28/119 [00:16<00:54,  1.67it/s]Epoch: 6, train for the 130-th batch, train loss: 0.6169666647911072:  54%|█████▉     | 129/237 [01:14<01:01,  1.76it/s]Epoch: 6, train for the 130-th batch, train loss: 0.6169666647911072:  55%|██████     | 130/237 [01:14<01:01,  1.73it/s]Epoch: 20, train for the 11-th batch, train loss: 0.40366241335868835:   7%|▋          | 10/151 [00:02<00:27,  5.19it/s]Epoch: 20, train for the 11-th batch, train loss: 0.40366241335868835:   7%|▊          | 11/151 [00:02<00:27,  5.13it/s]evaluate for the 6-th batch, evaluate loss: 0.49998119473457336:  13%|██▌                | 5/38 [00:01<00:09,  3.43it/s]evaluate for the 6-th batch, evaluate loss: 0.49998119473457336:  16%|███                | 6/38 [00:01<00:08,  3.59it/s]Epoch: 4, train for the 132-th batch, train loss: 0.39227089285850525:  34%|███▍      | 131/383 [01:16<02:33,  1.64it/s]Epoch: 4, train for the 132-th batch, train loss: 0.39227089285850525:  34%|███▍      | 132/383 [01:16<02:28,  1.69it/s]Epoch: 20, train for the 12-th batch, train loss: 0.633520245552063:   7%|▉            | 11/151 [00:02<00:27,  5.13it/s]Epoch: 20, train for the 12-th batch, train loss: 0.633520245552063:   8%|█            | 12/151 [00:02<00:27,  5.05it/s]evaluate for the 7-th batch, evaluate loss: 0.4631699025630951:  16%|███▏                | 6/38 [00:02<00:08,  3.59it/s]evaluate for the 7-th batch, evaluate loss: 0.4631699025630951:  18%|███▋                | 7/38 [00:02<00:08,  3.53it/s]Epoch: 20, train for the 13-th batch, train loss: 0.6200411319732666:   8%|▉           | 12/151 [00:02<00:27,  5.05it/s]Epoch: 20, train for the 13-th batch, train loss: 0.6200411319732666:   9%|█           | 13/151 [00:02<00:27,  5.00it/s]Epoch: 11, train for the 29-th batch, train loss: 0.19243478775024414:  24%|██▌        | 28/119 [00:16<00:54,  1.67it/s]Epoch: 11, train for the 29-th batch, train loss: 0.19243478775024414:  24%|██▋        | 29/119 [00:16<00:53,  1.68it/s]Epoch: 20, train for the 14-th batch, train loss: 0.5814803838729858:   9%|█           | 13/151 [00:02<00:27,  5.00it/s]Epoch: 20, train for the 14-th batch, train loss: 0.5814803838729858:   9%|█           | 14/151 [00:02<00:27,  4.98it/s]evaluate for the 8-th batch, evaluate loss: 0.5275208353996277:  18%|███▋                | 7/38 [00:02<00:08,  3.53it/s]evaluate for the 8-th batch, evaluate loss: 0.5275208353996277:  21%|████▏               | 8/38 [00:02<00:08,  3.64it/s]Epoch: 6, train for the 131-th batch, train loss: 0.6031981706619263:  55%|██████     | 130/237 [01:15<01:01,  1.73it/s]Epoch: 6, train for the 131-th batch, train loss: 0.6031981706619263:  55%|██████     | 131/237 [01:15<01:06,  1.61it/s]Epoch: 4, train for the 133-th batch, train loss: 0.39985892176628113:  34%|███▍      | 132/383 [01:16<02:28,  1.69it/s]Epoch: 4, train for the 133-th batch, train loss: 0.39985892176628113:  35%|███▍      | 133/383 [01:16<02:22,  1.76it/s]Epoch: 20, train for the 15-th batch, train loss: 0.47530072927474976:   9%|█          | 14/151 [00:02<00:27,  4.98it/s]Epoch: 20, train for the 15-th batch, train loss: 0.47530072927474976:  10%|█          | 15/151 [00:02<00:27,  4.94it/s]evaluate for the 9-th batch, evaluate loss: 0.5221490859985352:  21%|████▏               | 8/38 [00:02<00:08,  3.64it/s]evaluate for the 9-th batch, evaluate loss: 0.5221490859985352:  24%|████▋               | 9/38 [00:02<00:08,  3.60it/s]Epoch: 20, train for the 16-th batch, train loss: 0.49839121103286743:  10%|█          | 15/151 [00:03<00:27,  4.94it/s]Epoch: 20, train for the 16-th batch, train loss: 0.49839121103286743:  11%|█▏         | 16/151 [00:03<00:27,  4.92it/s]Epoch: 11, train for the 30-th batch, train loss: 0.1978142112493515:  24%|██▉         | 29/119 [00:17<00:53,  1.68it/s]Epoch: 11, train for the 30-th batch, train loss: 0.1978142112493515:  25%|███         | 30/119 [00:17<00:53,  1.66it/s]evaluate for the 10-th batch, evaluate loss: 0.5285941958427429:  24%|████▌              | 9/38 [00:02<00:08,  3.60it/s]evaluate for the 10-th batch, evaluate loss: 0.5285941958427429:  26%|████▋             | 10/38 [00:02<00:07,  3.58it/s]Epoch: 20, train for the 17-th batch, train loss: 0.6056860089302063:  11%|█▎          | 16/151 [00:03<00:27,  4.92it/s]Epoch: 20, train for the 17-th batch, train loss: 0.6056860089302063:  11%|█▎          | 17/151 [00:03<00:27,  4.92it/s]Epoch: 6, train for the 132-th batch, train loss: 0.5112197995185852:  55%|██████     | 131/237 [01:16<01:06,  1.61it/s]Epoch: 6, train for the 132-th batch, train loss: 0.5112197995185852:  56%|██████▏    | 132/237 [01:16<01:04,  1.63it/s]Epoch: 4, train for the 134-th batch, train loss: 0.4388572573661804:  35%|███▊       | 133/383 [01:17<02:22,  1.76it/s]Epoch: 4, train for the 134-th batch, train loss: 0.4388572573661804:  35%|███▊       | 134/383 [01:17<02:23,  1.73it/s]evaluate for the 11-th batch, evaluate loss: 0.5147793292999268:  26%|████▋             | 10/38 [00:03<00:07,  3.58it/s]evaluate for the 11-th batch, evaluate loss: 0.5147793292999268:  29%|█████▏            | 11/38 [00:03<00:06,  3.95it/s]Epoch: 20, train for the 18-th batch, train loss: 0.6463539004325867:  11%|█▎          | 17/151 [00:03<00:27,  4.92it/s]Epoch: 20, train for the 18-th batch, train loss: 0.6463539004325867:  12%|█▍          | 18/151 [00:03<00:27,  4.88it/s]evaluate for the 12-th batch, evaluate loss: 0.5573049187660217:  29%|█████▏            | 11/38 [00:03<00:06,  3.95it/s]evaluate for the 12-th batch, evaluate loss: 0.5573049187660217:  32%|█████▋            | 12/38 [00:03<00:06,  4.16it/s]Epoch: 20, train for the 19-th batch, train loss: 0.6420599222183228:  12%|█▍          | 18/151 [00:03<00:27,  4.88it/s]Epoch: 20, train for the 19-th batch, train loss: 0.6420599222183228:  13%|█▌          | 19/151 [00:03<00:27,  4.80it/s]Epoch: 11, train for the 31-th batch, train loss: 0.2069534957408905:  25%|███         | 30/119 [00:18<00:53,  1.66it/s]Epoch: 11, train for the 31-th batch, train loss: 0.2069534957408905:  26%|███▏        | 31/119 [00:18<00:55,  1.59it/s]Epoch: 6, train for the 133-th batch, train loss: 0.5627169609069824:  56%|██████▏    | 132/237 [01:16<01:04,  1.63it/s]Epoch: 6, train for the 133-th batch, train loss: 0.5627169609069824:  56%|██████▏    | 133/237 [01:16<01:04,  1.62it/s]Epoch: 20, train for the 20-th batch, train loss: 0.46634620428085327:  13%|█▍         | 19/151 [00:03<00:27,  4.80it/s]Epoch: 20, train for the 20-th batch, train loss: 0.46634620428085327:  13%|█▍         | 20/151 [00:03<00:27,  4.77it/s]Epoch: 4, train for the 135-th batch, train loss: 0.3526727259159088:  35%|███▊       | 134/383 [01:17<02:23,  1.73it/s]Epoch: 4, train for the 135-th batch, train loss: 0.3526727259159088:  35%|███▉       | 135/383 [01:17<02:25,  1.71it/s]evaluate for the 13-th batch, evaluate loss: 0.5403417944908142:  32%|█████▋            | 12/38 [00:03<00:06,  4.16it/s]evaluate for the 13-th batch, evaluate loss: 0.5403417944908142:  34%|██████▏           | 13/38 [00:03<00:06,  3.89it/s]Epoch: 20, train for the 21-th batch, train loss: 0.4487757086753845:  13%|█▌          | 20/151 [00:04<00:27,  4.77it/s]Epoch: 20, train for the 21-th batch, train loss: 0.4487757086753845:  14%|█▋          | 21/151 [00:04<00:27,  4.80it/s]evaluate for the 14-th batch, evaluate loss: 0.489682674407959:  34%|██████▌            | 13/38 [00:03<00:06,  3.89it/s]evaluate for the 14-th batch, evaluate loss: 0.489682674407959:  37%|███████            | 14/38 [00:03<00:06,  3.82it/s]Epoch: 20, train for the 22-th batch, train loss: 0.37184715270996094:  14%|█▌         | 21/151 [00:04<00:27,  4.80it/s]Epoch: 20, train for the 22-th batch, train loss: 0.37184715270996094:  15%|█▌         | 22/151 [00:04<00:26,  4.84it/s]Epoch: 11, train for the 32-th batch, train loss: 0.16368035972118378:  26%|██▊        | 31/119 [00:18<00:55,  1.59it/s]Epoch: 11, train for the 32-th batch, train loss: 0.16368035972118378:  27%|██▉        | 32/119 [00:18<00:54,  1.61it/s]evaluate for the 15-th batch, evaluate loss: 0.5209107995033264:  37%|██████▋           | 14/38 [00:04<00:06,  3.82it/s]evaluate for the 15-th batch, evaluate loss: 0.5209107995033264:  39%|███████           | 15/38 [00:04<00:06,  3.66it/s]Epoch: 20, train for the 23-th batch, train loss: 0.48555755615234375:  15%|█▌         | 22/151 [00:04<00:26,  4.84it/s]Epoch: 20, train for the 23-th batch, train loss: 0.48555755615234375:  15%|█▋         | 23/151 [00:04<00:26,  4.80it/s]Epoch: 6, train for the 134-th batch, train loss: 0.6409340500831604:  56%|██████▏    | 133/237 [01:17<01:04,  1.62it/s]Epoch: 6, train for the 134-th batch, train loss: 0.6409340500831604:  57%|██████▏    | 134/237 [01:17<01:04,  1.60it/s]Epoch: 4, train for the 136-th batch, train loss: 0.3633735477924347:  35%|███▉       | 135/383 [01:18<02:25,  1.71it/s]Epoch: 4, train for the 136-th batch, train loss: 0.3633735477924347:  36%|███▉       | 136/383 [01:18<02:28,  1.67it/s]Epoch: 20, train for the 24-th batch, train loss: 0.6007405519485474:  15%|█▊          | 23/151 [00:04<00:26,  4.80it/s]Epoch: 20, train for the 24-th batch, train loss: 0.6007405519485474:  16%|█▉          | 24/151 [00:04<00:26,  4.72it/s]evaluate for the 16-th batch, evaluate loss: 0.5244585275650024:  39%|███████           | 15/38 [00:04<00:06,  3.66it/s]evaluate for the 16-th batch, evaluate loss: 0.5244585275650024:  42%|███████▌          | 16/38 [00:04<00:05,  3.78it/s]Epoch: 20, train for the 25-th batch, train loss: 0.7636657357215881:  16%|█▉          | 24/151 [00:04<00:26,  4.72it/s]Epoch: 20, train for the 25-th batch, train loss: 0.7636657357215881:  17%|█▉          | 25/151 [00:04<00:27,  4.64it/s]Epoch: 11, train for the 33-th batch, train loss: 0.20248320698738098:  27%|██▉        | 32/119 [00:19<00:54,  1.61it/s]Epoch: 11, train for the 33-th batch, train loss: 0.20248320698738098:  28%|███        | 33/119 [00:19<00:52,  1.63it/s]evaluate for the 17-th batch, evaluate loss: 0.4935516119003296:  42%|███████▌          | 16/38 [00:04<00:05,  3.78it/s]evaluate for the 17-th batch, evaluate loss: 0.4935516119003296:  45%|████████          | 17/38 [00:04<00:05,  3.62it/s]Epoch: 6, train for the 135-th batch, train loss: 0.6447233557701111:  57%|██████▏    | 134/237 [01:18<01:04,  1.60it/s]Epoch: 6, train for the 135-th batch, train loss: 0.6447233557701111:  57%|██████▎    | 135/237 [01:18<01:04,  1.58it/s]Epoch: 4, train for the 137-th batch, train loss: 0.34262070059776306:  36%|███▌      | 136/383 [01:19<02:28,  1.67it/s]Epoch: 4, train for the 137-th batch, train loss: 0.34262070059776306:  36%|███▌      | 137/383 [01:19<02:31,  1.63it/s]Epoch: 20, train for the 26-th batch, train loss: 0.7530784606933594:  17%|█▉          | 25/151 [00:05<00:27,  4.64it/s]Epoch: 20, train for the 26-th batch, train loss: 0.7530784606933594:  17%|██          | 26/151 [00:05<00:27,  4.59it/s]evaluate for the 18-th batch, evaluate loss: 0.5444574356079102:  45%|████████          | 17/38 [00:04<00:05,  3.62it/s]evaluate for the 18-th batch, evaluate loss: 0.5444574356079102:  47%|████████▌         | 18/38 [00:04<00:05,  3.77it/s]Epoch: 20, train for the 27-th batch, train loss: 0.5631561279296875:  17%|██          | 26/151 [00:05<00:27,  4.59it/s]Epoch: 20, train for the 27-th batch, train loss: 0.5631561279296875:  18%|██▏         | 27/151 [00:05<00:26,  4.61it/s]Epoch: 11, train for the 34-th batch, train loss: 0.17628875374794006:  28%|███        | 33/119 [00:19<00:52,  1.63it/s]Epoch: 11, train for the 34-th batch, train loss: 0.17628875374794006:  29%|███▏       | 34/119 [00:19<00:52,  1.63it/s]evaluate for the 19-th batch, evaluate loss: 0.5050620436668396:  47%|████████▌         | 18/38 [00:05<00:05,  3.77it/s]evaluate for the 19-th batch, evaluate loss: 0.5050620436668396:  50%|█████████         | 19/38 [00:05<00:05,  3.55it/s]Epoch: 20, train for the 28-th batch, train loss: 0.6913834810256958:  18%|██▏         | 27/151 [00:05<00:26,  4.61it/s]Epoch: 20, train for the 28-th batch, train loss: 0.6913834810256958:  19%|██▏         | 28/151 [00:05<00:27,  4.55it/s]Epoch: 6, train for the 136-th batch, train loss: 0.6224133372306824:  57%|██████▎    | 135/237 [01:18<01:04,  1.58it/s]Epoch: 6, train for the 136-th batch, train loss: 0.6224133372306824:  57%|██████▎    | 136/237 [01:18<01:04,  1.57it/s]Epoch: 4, train for the 138-th batch, train loss: 0.2793589234352112:  36%|███▉       | 137/383 [01:19<02:31,  1.63it/s]Epoch: 4, train for the 138-th batch, train loss: 0.2793589234352112:  36%|███▉       | 138/383 [01:19<02:32,  1.60it/s]Epoch: 20, train for the 29-th batch, train loss: 0.6510887742042542:  19%|██▏         | 28/151 [00:05<00:27,  4.55it/s]Epoch: 20, train for the 29-th batch, train loss: 0.6510887742042542:  19%|██▎         | 29/151 [00:05<00:26,  4.53it/s]evaluate for the 20-th batch, evaluate loss: 0.4902208745479584:  50%|█████████         | 19/38 [00:05<00:05,  3.55it/s]evaluate for the 20-th batch, evaluate loss: 0.4902208745479584:  53%|█████████▍        | 20/38 [00:05<00:05,  3.60it/s]Epoch: 20, train for the 30-th batch, train loss: 0.701200008392334:  19%|██▍          | 29/151 [00:06<00:26,  4.53it/s]Epoch: 20, train for the 30-th batch, train loss: 0.701200008392334:  20%|██▌          | 30/151 [00:06<00:27,  4.42it/s]evaluate for the 21-th batch, evaluate loss: 0.5049508810043335:  53%|█████████▍        | 20/38 [00:05<00:05,  3.60it/s]evaluate for the 21-th batch, evaluate loss: 0.5049508810043335:  55%|█████████▉        | 21/38 [00:05<00:04,  3.53it/s]Epoch: 11, train for the 35-th batch, train loss: 0.1867024153470993:  29%|███▍        | 34/119 [00:20<00:52,  1.63it/s]Epoch: 11, train for the 35-th batch, train loss: 0.1867024153470993:  29%|███▌        | 35/119 [00:20<00:51,  1.64it/s]evaluate for the 22-th batch, evaluate loss: 0.5231841802597046:  55%|█████████▉        | 21/38 [00:06<00:04,  3.53it/s]evaluate for the 22-th batch, evaluate loss: 0.5231841802597046:  58%|██████████▍       | 22/38 [00:06<00:04,  3.64it/s]Epoch: 6, train for the 137-th batch, train loss: 0.6737203598022461:  57%|██████▎    | 136/237 [01:19<01:04,  1.57it/s]Epoch: 6, train for the 137-th batch, train loss: 0.6737203598022461:  58%|██████▎    | 137/237 [01:19<01:03,  1.56it/s]Epoch: 4, train for the 139-th batch, train loss: 0.3935287296772003:  36%|███▉       | 138/383 [01:20<02:32,  1.60it/s]Epoch: 4, train for the 139-th batch, train loss: 0.3935287296772003:  36%|███▉       | 139/383 [01:20<02:33,  1.59it/s]Epoch: 20, train for the 31-th batch, train loss: 0.5601742267608643:  20%|██▍         | 30/151 [00:06<00:27,  4.42it/s]Epoch: 20, train for the 31-th batch, train loss: 0.5601742267608643:  21%|██▍         | 31/151 [00:06<00:37,  3.19it/s]evaluate for the 23-th batch, evaluate loss: 0.520500123500824:  58%|███████████        | 22/38 [00:06<00:04,  3.64it/s]evaluate for the 23-th batch, evaluate loss: 0.520500123500824:  61%|███████████▌       | 23/38 [00:06<00:04,  3.57it/s]Epoch: 11, train for the 36-th batch, train loss: 0.18719328939914703:  29%|███▏       | 35/119 [00:21<00:51,  1.64it/s]Epoch: 11, train for the 36-th batch, train loss: 0.18719328939914703:  30%|███▎       | 36/119 [00:21<00:50,  1.66it/s]Epoch: 20, train for the 32-th batch, train loss: 0.440487265586853:  21%|██▋          | 31/151 [00:06<00:37,  3.19it/s]Epoch: 20, train for the 32-th batch, train loss: 0.440487265586853:  21%|██▊          | 32/151 [00:06<00:33,  3.52it/s]evaluate for the 24-th batch, evaluate loss: 0.49852362275123596:  61%|██████████▎      | 23/38 [00:06<00:04,  3.57it/s]evaluate for the 24-th batch, evaluate loss: 0.49852362275123596:  63%|██████████▋      | 24/38 [00:06<00:03,  3.65it/s]Epoch: 20, train for the 33-th batch, train loss: 0.5586689114570618:  21%|██▌         | 32/151 [00:07<00:33,  3.52it/s]Epoch: 20, train for the 33-th batch, train loss: 0.5586689114570618:  22%|██▌         | 33/151 [00:07<00:31,  3.79it/s]Epoch: 6, train for the 138-th batch, train loss: 0.6727970838546753:  58%|██████▎    | 137/237 [01:20<01:03,  1.56it/s]Epoch: 6, train for the 138-th batch, train loss: 0.6727970838546753:  58%|██████▍    | 138/237 [01:20<01:03,  1.56it/s]Epoch: 4, train for the 140-th batch, train loss: 0.33350521326065063:  36%|███▋      | 139/383 [01:21<02:33,  1.59it/s]Epoch: 4, train for the 140-th batch, train loss: 0.33350521326065063:  37%|███▋      | 140/383 [01:21<02:34,  1.57it/s]Epoch: 20, train for the 34-th batch, train loss: 0.5895595550537109:  22%|██▌         | 33/151 [00:07<00:31,  3.79it/s]Epoch: 20, train for the 34-th batch, train loss: 0.5895595550537109:  23%|██▋         | 34/151 [00:07<00:29,  3.96it/s]evaluate for the 25-th batch, evaluate loss: 0.5103310942649841:  63%|███████████▎      | 24/38 [00:06<00:03,  3.65it/s]evaluate for the 25-th batch, evaluate loss: 0.5103310942649841:  66%|███████████▊      | 25/38 [00:06<00:03,  3.59it/s]Epoch: 11, train for the 37-th batch, train loss: 0.1485198438167572:  30%|███▋        | 36/119 [00:21<00:50,  1.66it/s]Epoch: 11, train for the 37-th batch, train loss: 0.1485198438167572:  31%|███▋        | 37/119 [00:21<00:49,  1.65it/s]Epoch: 20, train for the 35-th batch, train loss: 0.4733128249645233:  23%|██▋         | 34/151 [00:07<00:29,  3.96it/s]Epoch: 20, train for the 35-th batch, train loss: 0.4733128249645233:  23%|██▊         | 35/151 [00:07<00:27,  4.16it/s]evaluate for the 26-th batch, evaluate loss: 0.5077908039093018:  66%|███████████▊      | 25/38 [00:07<00:03,  3.59it/s]evaluate for the 26-th batch, evaluate loss: 0.5077908039093018:  68%|████████████▎     | 26/38 [00:07<00:03,  3.61it/s]Epoch: 20, train for the 36-th batch, train loss: 0.623134970664978:  23%|███          | 35/151 [00:07<00:27,  4.16it/s]Epoch: 20, train for the 36-th batch, train loss: 0.623134970664978:  24%|███          | 36/151 [00:07<00:26,  4.27it/s]Epoch: 6, train for the 139-th batch, train loss: 0.6214379668235779:  58%|██████▍    | 138/237 [01:20<01:03,  1.56it/s]Epoch: 6, train for the 139-th batch, train loss: 0.6214379668235779:  59%|██████▍    | 139/237 [01:20<01:03,  1.55it/s]Epoch: 4, train for the 141-th batch, train loss: 0.40225252509117126:  37%|███▋      | 140/383 [01:21<02:34,  1.57it/s]Epoch: 4, train for the 141-th batch, train loss: 0.40225252509117126:  37%|███▋      | 141/383 [01:21<02:34,  1.56it/s]evaluate for the 27-th batch, evaluate loss: 0.5353819727897644:  68%|████████████▎     | 26/38 [00:07<00:03,  3.61it/s]evaluate for the 27-th batch, evaluate loss: 0.5353819727897644:  71%|████████████▊     | 27/38 [00:07<00:03,  3.62it/s]Epoch: 20, train for the 37-th batch, train loss: 0.48457014560699463:  24%|██▌        | 36/151 [00:07<00:26,  4.27it/s]Epoch: 20, train for the 37-th batch, train loss: 0.48457014560699463:  25%|██▋        | 37/151 [00:07<00:25,  4.40it/s]Epoch: 11, train for the 38-th batch, train loss: 0.1682216376066208:  31%|███▋        | 37/119 [00:22<00:49,  1.65it/s]Epoch: 11, train for the 38-th batch, train loss: 0.1682216376066208:  32%|███▊        | 38/119 [00:22<00:49,  1.65it/s]evaluate for the 28-th batch, evaluate loss: 0.548486053943634:  71%|█████████████▌     | 27/38 [00:07<00:03,  3.62it/s]evaluate for the 28-th batch, evaluate loss: 0.548486053943634:  74%|██████████████     | 28/38 [00:07<00:02,  3.53it/s]Epoch: 20, train for the 38-th batch, train loss: 0.6261758208274841:  25%|██▉         | 37/151 [00:08<00:25,  4.40it/s]Epoch: 20, train for the 38-th batch, train loss: 0.6261758208274841:  25%|███         | 38/151 [00:08<00:25,  4.44it/s]Epoch: 20, train for the 39-th batch, train loss: 0.46061381697654724:  25%|██▊        | 38/151 [00:08<00:25,  4.44it/s]Epoch: 20, train for the 39-th batch, train loss: 0.46061381697654724:  26%|██▊        | 39/151 [00:08<00:24,  4.52it/s]evaluate for the 29-th batch, evaluate loss: 0.5111759305000305:  74%|█████████████▎    | 28/38 [00:07<00:02,  3.53it/s]evaluate for the 29-th batch, evaluate loss: 0.5111759305000305:  76%|█████████████▋    | 29/38 [00:07<00:02,  3.67it/s]Epoch: 6, train for the 140-th batch, train loss: 0.6368160247802734:  59%|██████▍    | 139/237 [01:21<01:03,  1.55it/s]Epoch: 6, train for the 140-th batch, train loss: 0.6368160247802734:  59%|██████▍    | 140/237 [01:21<01:02,  1.56it/s]Epoch: 4, train for the 142-th batch, train loss: 0.35684067010879517:  37%|███▋      | 141/383 [01:22<02:34,  1.56it/s]Epoch: 4, train for the 142-th batch, train loss: 0.35684067010879517:  37%|███▋      | 142/383 [01:22<02:34,  1.56it/s]Epoch: 20, train for the 40-th batch, train loss: 0.4305747151374817:  26%|███         | 39/151 [00:08<00:24,  4.52it/s]Epoch: 20, train for the 40-th batch, train loss: 0.4305747151374817:  26%|███▏        | 40/151 [00:08<00:24,  4.59it/s]Epoch: 11, train for the 39-th batch, train loss: 0.2066887468099594:  32%|███▊        | 38/119 [00:22<00:49,  1.65it/s]Epoch: 11, train for the 39-th batch, train loss: 0.2066887468099594:  33%|███▉        | 39/119 [00:22<00:48,  1.66it/s]evaluate for the 30-th batch, evaluate loss: 0.5401619672775269:  76%|█████████████▋    | 29/38 [00:08<00:02,  3.67it/s]evaluate for the 30-th batch, evaluate loss: 0.5401619672775269:  79%|██████████████▏   | 30/38 [00:08<00:02,  3.55it/s]Epoch: 20, train for the 41-th batch, train loss: 0.6134634017944336:  26%|███▏        | 40/151 [00:08<00:24,  4.59it/s]Epoch: 20, train for the 41-th batch, train loss: 0.6134634017944336:  27%|███▎        | 41/151 [00:08<00:24,  4.55it/s]evaluate for the 31-th batch, evaluate loss: 0.5313848257064819:  79%|██████████████▏   | 30/38 [00:08<00:02,  3.55it/s]evaluate for the 31-th batch, evaluate loss: 0.5313848257064819:  82%|██████████████▋   | 31/38 [00:08<00:01,  3.72it/s]Epoch: 20, train for the 42-th batch, train loss: 0.6016011834144592:  27%|███▎        | 41/151 [00:08<00:24,  4.55it/s]Epoch: 20, train for the 42-th batch, train loss: 0.6016011834144592:  28%|███▎        | 42/151 [00:09<00:24,  4.51it/s]Epoch: 6, train for the 141-th batch, train loss: 0.6378740668296814:  59%|██████▍    | 140/237 [01:21<01:02,  1.56it/s]Epoch: 6, train for the 141-th batch, train loss: 0.6378740668296814:  59%|██████▌    | 141/237 [01:21<01:01,  1.56it/s]Epoch: 4, train for the 143-th batch, train loss: 0.34556689858436584:  37%|███▋      | 142/383 [01:23<02:34,  1.56it/s]Epoch: 4, train for the 143-th batch, train loss: 0.34556689858436584:  37%|███▋      | 143/383 [01:23<02:33,  1.57it/s]Epoch: 11, train for the 40-th batch, train loss: 0.1616940200328827:  33%|███▉        | 39/119 [00:23<00:48,  1.66it/s]Epoch: 11, train for the 40-th batch, train loss: 0.1616940200328827:  34%|████        | 40/119 [00:23<00:47,  1.66it/s]evaluate for the 32-th batch, evaluate loss: 0.5055282711982727:  82%|██████████████▋   | 31/38 [00:08<00:01,  3.72it/s]evaluate for the 32-th batch, evaluate loss: 0.5055282711982727:  84%|███████████████▏  | 32/38 [00:08<00:01,  3.54it/s]Epoch: 20, train for the 43-th batch, train loss: 0.6335641145706177:  28%|███▎        | 42/151 [00:09<00:24,  4.51it/s]Epoch: 20, train for the 43-th batch, train loss: 0.6335641145706177:  28%|███▍        | 43/151 [00:09<00:24,  4.47it/s]Epoch: 20, train for the 44-th batch, train loss: 0.5981325507164001:  28%|███▍        | 43/151 [00:09<00:24,  4.47it/s]Epoch: 20, train for the 44-th batch, train loss: 0.5981325507164001:  29%|███▍        | 44/151 [00:09<00:24,  4.46it/s]evaluate for the 33-th batch, evaluate loss: 0.49604880809783936:  84%|██████████████▎  | 32/38 [00:09<00:01,  3.54it/s]evaluate for the 33-th batch, evaluate loss: 0.49604880809783936:  87%|██████████████▊  | 33/38 [00:09<00:01,  3.59it/s]Epoch: 20, train for the 45-th batch, train loss: 0.39625442028045654:  29%|███▏       | 44/151 [00:09<00:24,  4.46it/s]Epoch: 20, train for the 45-th batch, train loss: 0.39625442028045654:  30%|███▎       | 45/151 [00:09<00:23,  4.51it/s]Epoch: 6, train for the 142-th batch, train loss: 0.6246931552886963:  59%|██████▌    | 141/237 [01:22<01:01,  1.56it/s]Epoch: 6, train for the 142-th batch, train loss: 0.6246931552886963:  60%|██████▌    | 142/237 [01:22<01:00,  1.56it/s]Epoch: 4, train for the 144-th batch, train loss: 0.33103784918785095:  37%|███▋      | 143/383 [01:23<02:33,  1.57it/s]Epoch: 4, train for the 144-th batch, train loss: 0.33103784918785095:  38%|███▊      | 144/383 [01:23<02:32,  1.56it/s]evaluate for the 34-th batch, evaluate loss: 0.5152574181556702:  87%|███████████████▋  | 33/38 [00:09<00:01,  3.59it/s]evaluate for the 34-th batch, evaluate loss: 0.5152574181556702:  89%|████████████████  | 34/38 [00:09<00:01,  3.45it/s]Epoch: 11, train for the 41-th batch, train loss: 0.24766360223293304:  34%|███▋       | 40/119 [00:24<00:47,  1.66it/s]Epoch: 11, train for the 41-th batch, train loss: 0.24766360223293304:  34%|███▊       | 41/119 [00:24<00:47,  1.66it/s]Epoch: 20, train for the 46-th batch, train loss: 0.4095960855484009:  30%|███▌        | 45/151 [00:09<00:23,  4.51it/s]Epoch: 20, train for the 46-th batch, train loss: 0.4095960855484009:  30%|███▋        | 46/151 [00:09<00:23,  4.56it/s]evaluate for the 35-th batch, evaluate loss: 0.5384554266929626:  89%|████████████████  | 34/38 [00:09<00:01,  3.45it/s]evaluate for the 35-th batch, evaluate loss: 0.5384554266929626:  92%|████████████████▌ | 35/38 [00:09<00:00,  3.55it/s]Epoch: 20, train for the 47-th batch, train loss: 0.577664315700531:  30%|███▉         | 46/151 [00:10<00:23,  4.56it/s]Epoch: 20, train for the 47-th batch, train loss: 0.577664315700531:  31%|████         | 47/151 [00:10<00:22,  4.53it/s]Epoch: 20, train for the 48-th batch, train loss: 0.5913611650466919:  31%|███▋        | 47/151 [00:10<00:22,  4.53it/s]Epoch: 20, train for the 48-th batch, train loss: 0.5913611650466919:  32%|███▊        | 48/151 [00:10<00:22,  4.52it/s]Epoch: 6, train for the 143-th batch, train loss: 0.632818341255188:  60%|███████▏    | 142/237 [01:23<01:00,  1.56it/s]Epoch: 6, train for the 143-th batch, train loss: 0.632818341255188:  60%|███████▏    | 143/237 [01:23<01:00,  1.56it/s]Epoch: 4, train for the 145-th batch, train loss: 0.4080341160297394:  38%|████▏      | 144/383 [01:24<02:32,  1.56it/s]Epoch: 4, train for the 145-th batch, train loss: 0.4080341160297394:  38%|████▏      | 145/383 [01:24<02:32,  1.56it/s]evaluate for the 36-th batch, evaluate loss: 0.5739569067955017:  92%|████████████████▌ | 35/38 [00:09<00:00,  3.55it/s]evaluate for the 36-th batch, evaluate loss: 0.5739569067955017:  95%|█████████████████ | 36/38 [00:09<00:00,  3.50it/s]Epoch: 11, train for the 42-th batch, train loss: 0.22213956713676453:  34%|███▊       | 41/119 [00:24<00:47,  1.66it/s]Epoch: 11, train for the 42-th batch, train loss: 0.22213956713676453:  35%|███▉       | 42/119 [00:24<00:46,  1.66it/s]Epoch: 20, train for the 49-th batch, train loss: 0.446954607963562:  32%|████▏        | 48/151 [00:10<00:22,  4.52it/s]Epoch: 20, train for the 49-th batch, train loss: 0.446954607963562:  32%|████▏        | 49/151 [00:10<00:22,  4.53it/s]evaluate for the 37-th batch, evaluate loss: 0.5098155736923218:  95%|█████████████████ | 36/38 [00:10<00:00,  3.50it/s]evaluate for the 37-th batch, evaluate loss: 0.5098155736923218:  97%|█████████████████▌| 37/38 [00:10<00:00,  3.62it/s]Epoch: 20, train for the 50-th batch, train loss: 0.42356646060943604:  32%|███▌       | 49/151 [00:10<00:22,  4.53it/s]Epoch: 20, train for the 50-th batch, train loss: 0.42356646060943604:  33%|███▋       | 50/151 [00:10<00:22,  4.58it/s]evaluate for the 38-th batch, evaluate loss: 0.5136985778808594:  97%|█████████████████▌| 37/38 [00:10<00:00,  3.62it/s]evaluate for the 38-th batch, evaluate loss: 0.5136985778808594: 100%|██████████████████| 38/38 [00:10<00:00,  3.59it/s]evaluate for the 38-th batch, evaluate loss: 0.5136985778808594: 100%|██████████████████| 38/38 [00:10<00:00,  3.62it/s]
  0%|                                                                                            | 0/20 [00:00<?, ?it/s]Epoch: 6, train for the 144-th batch, train loss: 0.6409556269645691:  60%|██████▋    | 143/237 [01:23<01:00,  1.56it/s]Epoch: 6, train for the 144-th batch, train loss: 0.6409556269645691:  61%|██████▋    | 144/237 [01:23<00:59,  1.56it/s]Epoch: 4, train for the 146-th batch, train loss: 0.36033445596694946:  38%|███▊      | 145/383 [01:25<02:32,  1.56it/s]Epoch: 4, train for the 146-th batch, train loss: 0.36033445596694946:  38%|███▊      | 146/383 [01:25<02:32,  1.56it/s]Epoch: 20, train for the 51-th batch, train loss: 0.6188063621520996:  33%|███▉        | 50/151 [00:10<00:22,  4.58it/s]Epoch: 20, train for the 51-th batch, train loss: 0.6188063621520996:  34%|████        | 51/151 [00:10<00:21,  4.56it/s]Epoch: 11, train for the 43-th batch, train loss: 0.22878539562225342:  35%|███▉       | 42/119 [00:25<00:46,  1.66it/s]Epoch: 11, train for the 43-th batch, train loss: 0.22878539562225342:  36%|███▉       | 43/119 [00:25<00:45,  1.66it/s]evaluate for the 1-th batch, evaluate loss: 0.5706149339675903:   0%|                            | 0/20 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.5706149339675903:   5%|█                   | 1/20 [00:00<00:05,  3.65it/s]Epoch: 20, train for the 52-th batch, train loss: 0.5337857007980347:  34%|████        | 51/151 [00:11<00:21,  4.56it/s]Epoch: 20, train for the 52-th batch, train loss: 0.5337857007980347:  34%|████▏       | 52/151 [00:11<00:21,  4.56it/s]Epoch: 20, train for the 53-th batch, train loss: 0.6216642260551453:  34%|████▏       | 52/151 [00:11<00:21,  4.56it/s]Epoch: 20, train for the 53-th batch, train loss: 0.6216642260551453:  35%|████▏       | 53/151 [00:11<00:21,  4.54it/s]evaluate for the 2-th batch, evaluate loss: 0.61865234375:   5%|█▎                       | 1/20 [00:00<00:05,  3.65it/s]evaluate for the 2-th batch, evaluate loss: 0.61865234375:  10%|██▌                      | 2/20 [00:00<00:05,  3.50it/s]Epoch: 11, train for the 44-th batch, train loss: 0.17565955221652985:  36%|███▉       | 43/119 [00:25<00:45,  1.66it/s]Epoch: 11, train for the 44-th batch, train loss: 0.17565955221652985:  37%|████       | 44/119 [00:25<00:45,  1.67it/s]Epoch: 6, train for the 145-th batch, train loss: 0.6394627094268799:  61%|██████▋    | 144/237 [01:24<00:59,  1.56it/s]Epoch: 6, train for the 145-th batch, train loss: 0.6394627094268799:  61%|██████▋    | 145/237 [01:24<00:59,  1.55it/s]Epoch: 4, train for the 147-th batch, train loss: 0.32527321577072144:  38%|███▊      | 146/383 [01:25<02:32,  1.56it/s]Epoch: 4, train for the 147-th batch, train loss: 0.32527321577072144:  38%|███▊      | 147/383 [01:25<02:32,  1.55it/s]Epoch: 20, train for the 54-th batch, train loss: 0.5602217316627502:  35%|████▏       | 53/151 [00:11<00:21,  4.54it/s]Epoch: 20, train for the 54-th batch, train loss: 0.5602217316627502:  36%|████▎       | 54/151 [00:11<00:21,  4.54it/s]evaluate for the 3-th batch, evaluate loss: 0.5966717600822449:  10%|██                  | 2/20 [00:00<00:05,  3.50it/s]evaluate for the 3-th batch, evaluate loss: 0.5966717600822449:  15%|███                 | 3/20 [00:00<00:04,  3.48it/s]Epoch: 20, train for the 55-th batch, train loss: 0.5657052397727966:  36%|████▎       | 54/151 [00:11<00:21,  4.54it/s]Epoch: 20, train for the 55-th batch, train loss: 0.5657052397727966:  36%|████▎       | 55/151 [00:11<00:21,  4.50it/s]evaluate for the 4-th batch, evaluate loss: 0.5770718455314636:  15%|███                 | 3/20 [00:01<00:04,  3.48it/s]evaluate for the 4-th batch, evaluate loss: 0.5770718455314636:  20%|████                | 4/20 [00:01<00:04,  3.43it/s]Epoch: 20, train for the 56-th batch, train loss: 0.48152610659599304:  36%|████       | 55/151 [00:12<00:21,  4.50it/s]Epoch: 20, train for the 56-th batch, train loss: 0.48152610659599304:  37%|████       | 56/151 [00:12<00:21,  4.48it/s]Epoch: 11, train for the 45-th batch, train loss: 0.16857463121414185:  37%|████       | 44/119 [00:26<00:45,  1.67it/s]Epoch: 11, train for the 45-th batch, train loss: 0.16857463121414185:  38%|████▏      | 45/119 [00:26<00:44,  1.67it/s]Epoch: 6, train for the 146-th batch, train loss: 0.6064289212226868:  61%|██████▋    | 145/237 [01:25<00:59,  1.55it/s]Epoch: 6, train for the 146-th batch, train loss: 0.6064289212226868:  62%|██████▊    | 146/237 [01:25<00:58,  1.55it/s]Epoch: 4, train for the 148-th batch, train loss: 0.35206565260887146:  38%|███▊      | 147/383 [01:26<02:32,  1.55it/s]Epoch: 4, train for the 148-th batch, train loss: 0.35206565260887146:  39%|███▊      | 148/383 [01:26<02:31,  1.55it/s]Epoch: 20, train for the 57-th batch, train loss: 0.5373802185058594:  37%|████▍       | 56/151 [00:12<00:21,  4.48it/s]Epoch: 20, train for the 57-th batch, train loss: 0.5373802185058594:  38%|████▌       | 57/151 [00:12<00:21,  4.45it/s]evaluate for the 5-th batch, evaluate loss: 0.6033890247344971:  20%|████                | 4/20 [00:01<00:04,  3.43it/s]evaluate for the 5-th batch, evaluate loss: 0.6033890247344971:  25%|█████               | 5/20 [00:01<00:04,  3.39it/s]Epoch: 20, train for the 58-th batch, train loss: 0.5576486587524414:  38%|████▌       | 57/151 [00:12<00:21,  4.45it/s]Epoch: 20, train for the 58-th batch, train loss: 0.5576486587524414:  38%|████▌       | 58/151 [00:12<00:20,  4.44it/s]evaluate for the 6-th batch, evaluate loss: 0.6156164407730103:  25%|█████               | 5/20 [00:01<00:04,  3.39it/s]evaluate for the 6-th batch, evaluate loss: 0.6156164407730103:  30%|██████              | 6/20 [00:01<00:04,  3.43it/s]Epoch: 20, train for the 59-th batch, train loss: 0.5333795547485352:  38%|████▌       | 58/151 [00:12<00:20,  4.44it/s]Epoch: 20, train for the 59-th batch, train loss: 0.5333795547485352:  39%|████▋       | 59/151 [00:12<00:20,  4.44it/s]Epoch: 11, train for the 46-th batch, train loss: 0.21456694602966309:  38%|████▏      | 45/119 [00:27<00:44,  1.67it/s]Epoch: 11, train for the 46-th batch, train loss: 0.21456694602966309:  39%|████▎      | 46/119 [00:27<00:43,  1.67it/s]Epoch: 4, train for the 149-th batch, train loss: 0.39881452918052673:  39%|███▊      | 148/383 [01:26<02:31,  1.55it/s]Epoch: 4, train for the 149-th batch, train loss: 0.39881452918052673:  39%|███▉      | 149/383 [01:26<02:30,  1.56it/s]Epoch: 6, train for the 147-th batch, train loss: 0.6493825912475586:  62%|██████▊    | 146/237 [01:25<00:58,  1.55it/s]Epoch: 6, train for the 147-th batch, train loss: 0.6493825912475586:  62%|██████▊    | 147/237 [01:25<00:57,  1.55it/s]evaluate for the 7-th batch, evaluate loss: 0.6739999055862427:  30%|██████              | 6/20 [00:02<00:04,  3.43it/s]evaluate for the 7-th batch, evaluate loss: 0.6739999055862427:  35%|███████             | 7/20 [00:02<00:03,  3.38it/s]Epoch: 20, train for the 60-th batch, train loss: 0.5144967436790466:  39%|████▋       | 59/151 [00:13<00:20,  4.44it/s]Epoch: 20, train for the 60-th batch, train loss: 0.5144967436790466:  40%|████▊       | 60/151 [00:13<00:20,  4.46it/s]Epoch: 20, train for the 61-th batch, train loss: 0.36521515250205994:  40%|████▎      | 60/151 [00:13<00:20,  4.46it/s]Epoch: 20, train for the 61-th batch, train loss: 0.36521515250205994:  40%|████▍      | 61/151 [00:13<00:19,  4.55it/s]evaluate for the 8-th batch, evaluate loss: 0.6444271802902222:  35%|███████             | 7/20 [00:02<00:03,  3.38it/s]evaluate for the 8-th batch, evaluate loss: 0.6444271802902222:  40%|████████            | 8/20 [00:02<00:03,  3.45it/s]Epoch: 11, train for the 47-th batch, train loss: 0.2664176821708679:  39%|████▋       | 46/119 [00:27<00:43,  1.67it/s]Epoch: 11, train for the 47-th batch, train loss: 0.2664176821708679:  39%|████▋       | 47/119 [00:27<00:42,  1.68it/s]Epoch: 20, train for the 62-th batch, train loss: 0.5764036178588867:  40%|████▊       | 61/151 [00:13<00:19,  4.55it/s]Epoch: 20, train for the 62-th batch, train loss: 0.5764036178588867:  41%|████▉       | 62/151 [00:13<00:19,  4.54it/s]evaluate for the 9-th batch, evaluate loss: 0.5903449058532715:  40%|████████            | 8/20 [00:02<00:03,  3.45it/s]evaluate for the 9-th batch, evaluate loss: 0.5903449058532715:  45%|█████████           | 9/20 [00:02<00:03,  3.40it/s]Epoch: 6, train for the 148-th batch, train loss: 0.5366005301475525:  62%|██████▊    | 147/237 [01:26<00:57,  1.55it/s]Epoch: 6, train for the 148-th batch, train loss: 0.5366005301475525:  62%|██████▊    | 148/237 [01:26<00:57,  1.56it/s]Epoch: 4, train for the 150-th batch, train loss: 0.46090933680534363:  39%|███▉      | 149/383 [01:27<02:30,  1.56it/s]Epoch: 4, train for the 150-th batch, train loss: 0.46090933680534363:  39%|███▉      | 150/383 [01:27<02:29,  1.56it/s]Epoch: 20, train for the 63-th batch, train loss: 0.5814328193664551:  41%|████▉       | 62/151 [00:13<00:19,  4.54it/s]Epoch: 20, train for the 63-th batch, train loss: 0.5814328193664551:  42%|█████       | 63/151 [00:13<00:19,  4.49it/s]evaluate for the 10-th batch, evaluate loss: 0.596366286277771:  45%|█████████           | 9/20 [00:02<00:03,  3.40it/s]evaluate for the 10-th batch, evaluate loss: 0.596366286277771:  50%|█████████▌         | 10/20 [00:02<00:02,  3.49it/s]Epoch: 20, train for the 64-th batch, train loss: 0.4189954996109009:  42%|█████       | 63/151 [00:13<00:19,  4.49it/s]Epoch: 20, train for the 64-th batch, train loss: 0.4189954996109009:  42%|█████       | 64/151 [00:13<00:19,  4.52it/s]Epoch: 6, train for the 149-th batch, train loss: 0.6352640390396118:  62%|██████▊    | 148/237 [01:26<00:57,  1.56it/s]Epoch: 6, train for the 149-th batch, train loss: 0.6352640390396118:  63%|██████▉    | 149/237 [01:26<00:50,  1.76it/s]Epoch: 11, train for the 48-th batch, train loss: 0.25161296129226685:  39%|████▎      | 47/119 [00:28<00:42,  1.68it/s]Epoch: 11, train for the 48-th batch, train loss: 0.25161296129226685:  40%|████▍      | 48/119 [00:28<00:42,  1.68it/s]Epoch: 20, train for the 65-th batch, train loss: 0.5055474638938904:  42%|█████       | 64/151 [00:14<00:19,  4.52it/s]Epoch: 20, train for the 65-th batch, train loss: 0.5055474638938904:  43%|█████▏      | 65/151 [00:14<00:19,  4.51it/s]evaluate for the 11-th batch, evaluate loss: 0.5950453877449036:  50%|█████████         | 10/20 [00:03<00:02,  3.49it/s]evaluate for the 11-th batch, evaluate loss: 0.5950453877449036:  55%|█████████▉        | 11/20 [00:03<00:02,  3.42it/s]Epoch: 6, train for the 150-th batch, train loss: 0.5929529666900635:  63%|██████▉    | 149/237 [01:27<00:50,  1.76it/s]Epoch: 6, train for the 150-th batch, train loss: 0.5929529666900635:  63%|██████▉    | 150/237 [01:27<00:43,  1.99it/s]Epoch: 20, train for the 66-th batch, train loss: 0.5338837504386902:  43%|█████▏      | 65/151 [00:14<00:19,  4.51it/s]Epoch: 20, train for the 66-th batch, train loss: 0.5338837504386902:  44%|█████▏      | 66/151 [00:14<00:18,  4.51it/s]evaluate for the 12-th batch, evaluate loss: 0.6131213903427124:  55%|█████████▉        | 11/20 [00:03<00:02,  3.42it/s]evaluate for the 12-th batch, evaluate loss: 0.6131213903427124:  60%|██████████▊       | 12/20 [00:03<00:02,  3.56it/s]Epoch: 20, train for the 67-th batch, train loss: 0.4235784709453583:  44%|█████▏      | 66/151 [00:14<00:18,  4.51it/s]Epoch: 20, train for the 67-th batch, train loss: 0.4235784709453583:  44%|█████▎      | 67/151 [00:14<00:18,  4.56it/s]Epoch: 6, train for the 151-th batch, train loss: 0.611103892326355:  63%|███████▌    | 150/237 [01:27<00:43,  1.99it/s]Epoch: 6, train for the 151-th batch, train loss: 0.611103892326355:  64%|███████▋    | 151/237 [01:27<00:36,  2.35it/s]Epoch: 11, train for the 49-th batch, train loss: 0.22805167734622955:  40%|████▍      | 48/119 [00:28<00:42,  1.68it/s]Epoch: 11, train for the 49-th batch, train loss: 0.22805167734622955:  41%|████▌      | 49/119 [00:28<00:41,  1.69it/s]evaluate for the 13-th batch, evaluate loss: 0.6393448710441589:  60%|██████████▊       | 12/20 [00:03<00:02,  3.56it/s]evaluate for the 13-th batch, evaluate loss: 0.6393448710441589:  65%|███████████▋      | 13/20 [00:03<00:02,  3.46it/s]Epoch: 4, train for the 151-th batch, train loss: 0.46697044372558594:  39%|███▉      | 150/383 [01:28<02:29,  1.56it/s]Epoch: 4, train for the 151-th batch, train loss: 0.46697044372558594:  39%|███▉      | 151/383 [01:28<03:07,  1.24it/s]Epoch: 20, train for the 68-th batch, train loss: 0.5350552797317505:  44%|█████▎      | 67/151 [00:14<00:18,  4.56it/s]Epoch: 20, train for the 68-th batch, train loss: 0.5350552797317505:  45%|█████▍      | 68/151 [00:14<00:18,  4.55it/s]evaluate for the 14-th batch, evaluate loss: 0.6214010119438171:  65%|███████████▋      | 13/20 [00:04<00:02,  3.46it/s]evaluate for the 14-th batch, evaluate loss: 0.6214010119438171:  70%|████████████▌     | 14/20 [00:04<00:01,  3.59it/s]Epoch: 20, train for the 69-th batch, train loss: 0.5242413878440857:  45%|█████▍      | 68/151 [00:14<00:18,  4.55it/s]Epoch: 20, train for the 69-th batch, train loss: 0.5242413878440857:  46%|█████▍      | 69/151 [00:14<00:18,  4.53it/s]Epoch: 6, train for the 152-th batch, train loss: 0.6105106472969055:  64%|███████    | 151/237 [01:27<00:36,  2.35it/s]Epoch: 6, train for the 152-th batch, train loss: 0.6105106472969055:  64%|███████    | 152/237 [01:27<00:38,  2.23it/s]Epoch: 11, train for the 50-th batch, train loss: 0.18744319677352905:  41%|████▌      | 49/119 [00:29<00:41,  1.69it/s]Epoch: 11, train for the 50-th batch, train loss: 0.18744319677352905:  42%|████▌      | 50/119 [00:29<00:40,  1.71it/s]Epoch: 20, train for the 70-th batch, train loss: 0.554493248462677:  46%|█████▉       | 69/151 [00:15<00:18,  4.53it/s]Epoch: 20, train for the 70-th batch, train loss: 0.554493248462677:  46%|██████       | 70/151 [00:15<00:17,  4.52it/s]evaluate for the 15-th batch, evaluate loss: 0.6101452708244324:  70%|████████████▌     | 14/20 [00:04<00:01,  3.59it/s]evaluate for the 15-th batch, evaluate loss: 0.6101452708244324:  75%|█████████████▌    | 15/20 [00:04<00:01,  3.48it/s]Epoch: 4, train for the 152-th batch, train loss: 0.3970661461353302:  39%|████▎      | 151/383 [01:29<03:07,  1.24it/s]Epoch: 4, train for the 152-th batch, train loss: 0.3970661461353302:  40%|████▎      | 152/383 [01:29<02:51,  1.35it/s]Epoch: 20, train for the 71-th batch, train loss: 0.5508823990821838:  46%|█████▌      | 70/151 [00:15<00:17,  4.52it/s]Epoch: 20, train for the 71-th batch, train loss: 0.5508823990821838:  47%|█████▋      | 71/151 [00:15<00:17,  4.49it/s]evaluate for the 16-th batch, evaluate loss: 0.5907995700836182:  75%|█████████████▌    | 15/20 [00:04<00:01,  3.48it/s]evaluate for the 16-th batch, evaluate loss: 0.5907995700836182:  80%|██████████████▍   | 16/20 [00:04<00:01,  3.63it/s]Epoch: 6, train for the 153-th batch, train loss: 0.6300506591796875:  64%|███████    | 152/237 [01:28<00:38,  2.23it/s]Epoch: 6, train for the 153-th batch, train loss: 0.6300506591796875:  65%|███████    | 153/237 [01:28<00:41,  2.03it/s]Epoch: 20, train for the 72-th batch, train loss: 0.509705662727356:  47%|██████       | 71/151 [00:15<00:17,  4.49it/s]Epoch: 20, train for the 72-th batch, train loss: 0.509705662727356:  48%|██████▏      | 72/151 [00:15<00:17,  4.39it/s]Epoch: 11, train for the 51-th batch, train loss: 0.21238292753696442:  42%|████▌      | 50/119 [00:30<00:40,  1.71it/s]Epoch: 11, train for the 51-th batch, train loss: 0.21238292753696442:  43%|████▋      | 51/119 [00:30<00:39,  1.70it/s]evaluate for the 17-th batch, evaluate loss: 0.5949517488479614:  80%|██████████████▍   | 16/20 [00:04<00:01,  3.63it/s]evaluate for the 17-th batch, evaluate loss: 0.5949517488479614:  85%|███████████████▎  | 17/20 [00:04<00:00,  3.49it/s]Epoch: 20, train for the 73-th batch, train loss: 0.5841530561447144:  48%|█████▋      | 72/151 [00:15<00:17,  4.39it/s]Epoch: 20, train for the 73-th batch, train loss: 0.5841530561447144:  48%|█████▊      | 73/151 [00:15<00:17,  4.41it/s]Epoch: 4, train for the 153-th batch, train loss: 0.44701454043388367:  40%|███▉      | 152/383 [01:29<02:51,  1.35it/s]Epoch: 4, train for the 153-th batch, train loss: 0.44701454043388367:  40%|███▉      | 153/383 [01:29<02:40,  1.43it/s]evaluate for the 18-th batch, evaluate loss: 0.6421993970870972:  85%|███████████████▎  | 17/20 [00:05<00:00,  3.49it/s]evaluate for the 18-th batch, evaluate loss: 0.6421993970870972:  90%|████████████████▏ | 18/20 [00:05<00:00,  3.66it/s]Epoch: 20, train for the 74-th batch, train loss: 0.5233167409896851:  48%|█████▊      | 73/151 [00:16<00:17,  4.41it/s]Epoch: 20, train for the 74-th batch, train loss: 0.5233167409896851:  49%|█████▉      | 74/151 [00:16<00:17,  4.42it/s]Epoch: 6, train for the 154-th batch, train loss: 0.6195919513702393:  65%|███████    | 153/237 [01:29<00:41,  2.03it/s]Epoch: 6, train for the 154-th batch, train loss: 0.6195919513702393:  65%|███████▏   | 154/237 [01:29<00:43,  1.92it/s]Epoch: 11, train for the 52-th batch, train loss: 0.24166564643383026:  43%|████▋      | 51/119 [00:30<00:39,  1.70it/s]Epoch: 11, train for the 52-th batch, train loss: 0.24166564643383026:  44%|████▊      | 52/119 [00:30<00:39,  1.69it/s]Epoch: 20, train for the 75-th batch, train loss: 0.5034680962562561:  49%|█████▉      | 74/151 [00:16<00:17,  4.42it/s]Epoch: 20, train for the 75-th batch, train loss: 0.5034680962562561:  50%|█████▉      | 75/151 [00:16<00:17,  4.44it/s]evaluate for the 19-th batch, evaluate loss: 0.6541295647621155:  90%|████████████████▏ | 18/20 [00:05<00:00,  3.66it/s]evaluate for the 19-th batch, evaluate loss: 0.6541295647621155:  95%|█████████████████ | 19/20 [00:05<00:00,  3.48it/s]evaluate for the 20-th batch, evaluate loss: 0.6375676393508911:  95%|█████████████████ | 19/20 [00:05<00:00,  3.48it/s]evaluate for the 20-th batch, evaluate loss: 0.6375676393508911: 100%|██████████████████| 20/20 [00:05<00:00,  3.61it/s]
INFO:root:Epoch: 9, learning rate: 0.0001, train loss: 0.5116
INFO:root:train average_precision, 0.8449
INFO:root:train roc_auc, 0.8198
INFO:root:validate loss: 0.5149
INFO:root:validate average_precision, 0.8431
INFO:root:validate roc_auc, 0.8095
INFO:root:new node validate loss: 0.6143
INFO:root:new node validate first_1_average_precision, 0.6428
INFO:root:new node validate first_1_roc_auc, 0.5597
INFO:root:new node validate first_3_average_precision, 0.6974
INFO:root:new node validate first_3_roc_auc, 0.6273
INFO:root:new node validate first_10_average_precision, 0.7414
INFO:root:new node validate first_10_roc_auc, 0.6875
INFO:root:new node validate average_precision, 0.7515
INFO:root:new node validate roc_auc, 0.7108
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]Epoch: 4, train for the 154-th batch, train loss: 0.37650421261787415:  40%|███▉      | 153/383 [01:30<02:40,  1.43it/s]Epoch: 4, train for the 154-th batch, train loss: 0.37650421261787415:  40%|████      | 154/383 [01:30<02:32,  1.50it/s]Epoch: 20, train for the 76-th batch, train loss: 0.5950941443443298:  50%|█████▉      | 75/151 [00:16<00:17,  4.44it/s]Epoch: 20, train for the 76-th batch, train loss: 0.5950941443443298:  50%|██████      | 76/151 [00:16<00:16,  4.45it/s]Epoch: 20, train for the 77-th batch, train loss: 0.3915567696094513:  50%|██████      | 76/151 [00:16<00:16,  4.45it/s]Epoch: 20, train for the 77-th batch, train loss: 0.3915567696094513:  51%|██████      | 77/151 [00:16<00:16,  4.48it/s]Epoch: 6, train for the 155-th batch, train loss: 0.6216368675231934:  65%|███████▏   | 154/237 [01:29<00:43,  1.92it/s]Epoch: 6, train for the 155-th batch, train loss: 0.6216368675231934:  65%|███████▏   | 155/237 [01:29<00:44,  1.85it/s]Epoch: 11, train for the 53-th batch, train loss: 0.18583376705646515:  44%|████▊      | 52/119 [00:31<00:39,  1.69it/s]Epoch: 11, train for the 53-th batch, train loss: 0.18583376705646515:  45%|████▉      | 53/119 [00:31<00:38,  1.70it/s]Epoch: 10, train for the 1-th batch, train loss: 0.7649812698364258:   0%|                      | 0/146 [00:00<?, ?it/s]Epoch: 10, train for the 1-th batch, train loss: 0.7649812698364258:   1%|              | 1/146 [00:00<01:07,  2.16it/s]Epoch: 20, train for the 78-th batch, train loss: 0.4387313723564148:  51%|██████      | 77/151 [00:16<00:16,  4.48it/s]Epoch: 20, train for the 78-th batch, train loss: 0.4387313723564148:  52%|██████▏     | 78/151 [00:16<00:16,  4.53it/s]Epoch: 4, train for the 155-th batch, train loss: 0.35517555475234985:  40%|████      | 154/383 [01:31<02:32,  1.50it/s]Epoch: 4, train for the 155-th batch, train loss: 0.35517555475234985:  40%|████      | 155/383 [01:31<02:26,  1.56it/s]Epoch: 20, train for the 79-th batch, train loss: 0.4910520017147064:  52%|██████▏     | 78/151 [00:17<00:16,  4.53it/s]Epoch: 20, train for the 79-th batch, train loss: 0.4910520017147064:  52%|██████▎     | 79/151 [00:17<00:15,  4.52it/s]Epoch: 6, train for the 156-th batch, train loss: 0.6442413926124573:  65%|███████▏   | 155/237 [01:30<00:44,  1.85it/s]Epoch: 6, train for the 156-th batch, train loss: 0.6442413926124573:  66%|███████▏   | 156/237 [01:30<00:45,  1.80it/s]Epoch: 20, train for the 80-th batch, train loss: 0.5937541127204895:  52%|██████▎     | 79/151 [00:17<00:15,  4.52it/s]Epoch: 20, train for the 80-th batch, train loss: 0.5937541127204895:  53%|██████▎     | 80/151 [00:17<00:15,  4.49it/s]Epoch: 10, train for the 2-th batch, train loss: 0.5827445983886719:   1%|              | 1/146 [00:01<01:07,  2.16it/s]Epoch: 10, train for the 2-th batch, train loss: 0.5827445983886719:   1%|▏             | 2/146 [00:01<01:15,  1.92it/s]Epoch: 11, train for the 54-th batch, train loss: 0.18128535151481628:  45%|████▉      | 53/119 [00:31<00:38,  1.70it/s]Epoch: 11, train for the 54-th batch, train loss: 0.18128535151481628:  45%|████▉      | 54/119 [00:31<00:39,  1.66it/s]Epoch: 4, train for the 156-th batch, train loss: 0.4019628167152405:  40%|████▍      | 155/383 [01:31<02:26,  1.56it/s]Epoch: 4, train for the 156-th batch, train loss: 0.4019628167152405:  41%|████▍      | 156/383 [01:31<02:22,  1.60it/s]Epoch: 20, train for the 81-th batch, train loss: 0.598542332649231:  53%|██████▉      | 80/151 [00:17<00:15,  4.49it/s]Epoch: 20, train for the 81-th batch, train loss: 0.598542332649231:  54%|██████▉      | 81/151 [00:17<00:17,  4.03it/s]Epoch: 20, train for the 82-th batch, train loss: 0.5578791499137878:  54%|██████▍     | 81/151 [00:17<00:17,  4.03it/s]Epoch: 20, train for the 82-th batch, train loss: 0.5578791499137878:  54%|██████▌     | 82/151 [00:17<00:16,  4.15it/s]Epoch: 6, train for the 157-th batch, train loss: 0.651025652885437:  66%|███████▉    | 156/237 [01:30<00:45,  1.80it/s]Epoch: 6, train for the 157-th batch, train loss: 0.651025652885437:  66%|███████▉    | 157/237 [01:30<00:45,  1.77it/s]Epoch: 10, train for the 3-th batch, train loss: 0.5005712509155273:   1%|▏             | 2/146 [00:01<01:15,  1.92it/s]Epoch: 10, train for the 3-th batch, train loss: 0.5005712509155273:   2%|▎             | 3/146 [00:01<01:17,  1.85it/s]Epoch: 11, train for the 55-th batch, train loss: 0.18235236406326294:  45%|████▉      | 54/119 [00:32<00:39,  1.66it/s]Epoch: 11, train for the 55-th batch, train loss: 0.18235236406326294:  46%|█████      | 55/119 [00:32<00:38,  1.66it/s]Epoch: 20, train for the 83-th batch, train loss: 0.5477661490440369:  54%|██████▌     | 82/151 [00:18<00:16,  4.15it/s]Epoch: 20, train for the 83-th batch, train loss: 0.5477661490440369:  55%|██████▌     | 83/151 [00:18<00:16,  4.21it/s]Epoch: 4, train for the 157-th batch, train loss: 0.39987829327583313:  41%|████      | 156/383 [01:32<02:22,  1.60it/s]Epoch: 4, train for the 157-th batch, train loss: 0.39987829327583313:  41%|████      | 157/383 [01:32<02:19,  1.63it/s]Epoch: 20, train for the 84-th batch, train loss: 0.5538155436515808:  55%|██████▌     | 83/151 [00:18<00:16,  4.21it/s]Epoch: 20, train for the 84-th batch, train loss: 0.5538155436515808:  56%|██████▋     | 84/151 [00:18<00:15,  4.28it/s]Epoch: 10, train for the 4-th batch, train loss: 0.47444048523902893:   2%|▎            | 3/146 [00:02<01:17,  1.85it/s]Epoch: 10, train for the 4-th batch, train loss: 0.47444048523902893:   3%|▎            | 4/146 [00:02<01:16,  1.86it/s]Epoch: 6, train for the 158-th batch, train loss: 0.5967803597450256:  66%|███████▎   | 157/237 [01:31<00:45,  1.77it/s]Epoch: 6, train for the 158-th batch, train loss: 0.5967803597450256:  67%|███████▎   | 158/237 [01:31<00:45,  1.75it/s]Epoch: 20, train for the 85-th batch, train loss: 0.5600814819335938:  56%|██████▋     | 84/151 [00:18<00:15,  4.28it/s]Epoch: 20, train for the 85-th batch, train loss: 0.5600814819335938:  56%|██████▊     | 85/151 [00:18<00:15,  4.33it/s]Epoch: 11, train for the 56-th batch, train loss: 0.19936853647232056:  46%|█████      | 55/119 [00:33<00:38,  1.66it/s]Epoch: 11, train for the 56-th batch, train loss: 0.19936853647232056:  47%|█████▏     | 56/119 [00:33<00:37,  1.67it/s]Epoch: 4, train for the 158-th batch, train loss: 0.473732590675354:  41%|████▉       | 157/383 [01:32<02:19,  1.63it/s]Epoch: 4, train for the 158-th batch, train loss: 0.473732590675354:  41%|████▉       | 158/383 [01:32<02:16,  1.65it/s]Epoch: 20, train for the 86-th batch, train loss: 0.5462670922279358:  56%|██████▊     | 85/151 [00:18<00:15,  4.33it/s]Epoch: 20, train for the 86-th batch, train loss: 0.5462670922279358:  57%|██████▊     | 86/151 [00:18<00:14,  4.37it/s]Epoch: 20, train for the 87-th batch, train loss: 0.5445821285247803:  57%|██████▊     | 86/151 [00:19<00:14,  4.37it/s]Epoch: 20, train for the 87-th batch, train loss: 0.5445821285247803:  58%|██████▉     | 87/151 [00:19<00:14,  4.39it/s]Epoch: 10, train for the 5-th batch, train loss: 0.5216999650001526:   3%|▍             | 4/146 [00:02<01:16,  1.86it/s]Epoch: 10, train for the 5-th batch, train loss: 0.5216999650001526:   3%|▍             | 5/146 [00:02<01:18,  1.79it/s]Epoch: 6, train for the 159-th batch, train loss: 0.6214655637741089:  67%|███████▎   | 158/237 [01:32<00:45,  1.75it/s]Epoch: 6, train for the 159-th batch, train loss: 0.6214655637741089:  67%|███████▍   | 159/237 [01:32<00:45,  1.73it/s]Epoch: 11, train for the 57-th batch, train loss: 0.1565728783607483:  47%|█████▋      | 56/119 [00:33<00:37,  1.67it/s]Epoch: 11, train for the 57-th batch, train loss: 0.1565728783607483:  48%|█████▋      | 57/119 [00:33<00:37,  1.67it/s]Epoch: 20, train for the 88-th batch, train loss: 0.6123895645141602:  58%|██████▉     | 87/151 [00:19<00:14,  4.39it/s]Epoch: 20, train for the 88-th batch, train loss: 0.6123895645141602:  58%|██████▉     | 88/151 [00:19<00:14,  4.41it/s]Epoch: 4, train for the 159-th batch, train loss: 0.3736412823200226:  41%|████▌      | 158/383 [01:33<02:16,  1.65it/s]Epoch: 4, train for the 159-th batch, train loss: 0.3736412823200226:  42%|████▌      | 159/383 [01:33<02:14,  1.66it/s]Epoch: 20, train for the 89-th batch, train loss: 0.5507843494415283:  58%|██████▉     | 88/151 [00:19<00:14,  4.41it/s]Epoch: 20, train for the 89-th batch, train loss: 0.5507843494415283:  59%|███████     | 89/151 [00:19<00:14,  4.42it/s]Epoch: 6, train for the 160-th batch, train loss: 0.6104048490524292:  67%|███████▍   | 159/237 [01:32<00:45,  1.73it/s]Epoch: 6, train for the 160-th batch, train loss: 0.6104048490524292:  68%|███████▍   | 160/237 [01:32<00:44,  1.72it/s]Epoch: 20, train for the 90-th batch, train loss: 0.5851065516471863:  59%|███████     | 89/151 [00:19<00:14,  4.42it/s]Epoch: 20, train for the 90-th batch, train loss: 0.5851065516471863:  60%|███████▏    | 90/151 [00:19<00:13,  4.42it/s]Epoch: 10, train for the 6-th batch, train loss: 0.4826076030731201:   3%|▍             | 5/146 [00:03<01:18,  1.79it/s]Epoch: 10, train for the 6-th batch, train loss: 0.4826076030731201:   4%|▌             | 6/146 [00:03<01:22,  1.71it/s]Epoch: 11, train for the 58-th batch, train loss: 0.15554742515087128:  48%|█████▎     | 57/119 [00:34<00:37,  1.67it/s]Epoch: 11, train for the 58-th batch, train loss: 0.15554742515087128:  49%|█████▎     | 58/119 [00:34<00:37,  1.64it/s]Epoch: 20, train for the 91-th batch, train loss: 0.4912891387939453:  60%|███████▏    | 90/151 [00:20<00:13,  4.42it/s]Epoch: 20, train for the 91-th batch, train loss: 0.4912891387939453:  60%|███████▏    | 91/151 [00:20<00:13,  4.42it/s]Epoch: 4, train for the 160-th batch, train loss: 0.43452104926109314:  42%|████▏     | 159/383 [01:34<02:14,  1.66it/s]Epoch: 4, train for the 160-th batch, train loss: 0.43452104926109314:  42%|████▏     | 160/383 [01:34<02:13,  1.67it/s]Epoch: 20, train for the 92-th batch, train loss: 0.5604446530342102:  60%|███████▏    | 91/151 [00:20<00:13,  4.42it/s]Epoch: 20, train for the 92-th batch, train loss: 0.5604446530342102:  61%|███████▎    | 92/151 [00:20<00:13,  4.44it/s]Epoch: 6, train for the 161-th batch, train loss: 0.6361097097396851:  68%|███████▍   | 160/237 [01:33<00:44,  1.72it/s]Epoch: 6, train for the 161-th batch, train loss: 0.6361097097396851:  68%|███████▍   | 161/237 [01:33<00:44,  1.71it/s]Epoch: 10, train for the 7-th batch, train loss: 0.44643083214759827:   4%|▌            | 6/146 [00:03<01:22,  1.71it/s]Epoch: 10, train for the 7-th batch, train loss: 0.44643083214759827:   5%|▌            | 7/146 [00:03<01:21,  1.71it/s]Epoch: 20, train for the 93-th batch, train loss: 0.529019296169281:  61%|███████▉     | 92/151 [00:20<00:13,  4.44it/s]Epoch: 20, train for the 93-th batch, train loss: 0.529019296169281:  62%|████████     | 93/151 [00:20<00:13,  4.43it/s]Epoch: 11, train for the 59-th batch, train loss: 0.17056630551815033:  49%|█████▎     | 58/119 [00:34<00:37,  1.64it/s]Epoch: 11, train for the 59-th batch, train loss: 0.17056630551815033:  50%|█████▍     | 59/119 [00:34<00:36,  1.65it/s]Epoch: 4, train for the 161-th batch, train loss: 0.39697250723838806:  42%|████▏     | 160/383 [01:34<02:13,  1.67it/s]Epoch: 4, train for the 161-th batch, train loss: 0.39697250723838806:  42%|████▏     | 161/383 [01:34<02:11,  1.68it/s]Epoch: 20, train for the 94-th batch, train loss: 0.5391978621482849:  62%|███████▍    | 93/151 [00:20<00:13,  4.43it/s]Epoch: 20, train for the 94-th batch, train loss: 0.5391978621482849:  62%|███████▍    | 94/151 [00:20<00:12,  4.45it/s]Epoch: 20, train for the 95-th batch, train loss: 0.538910984992981:  62%|████████     | 94/151 [00:20<00:12,  4.45it/s]Epoch: 20, train for the 95-th batch, train loss: 0.538910984992981:  63%|████████▏    | 95/151 [00:20<00:12,  4.46it/s]Epoch: 10, train for the 8-th batch, train loss: 0.45241209864616394:   5%|▌            | 7/146 [00:04<01:21,  1.71it/s]Epoch: 10, train for the 8-th batch, train loss: 0.45241209864616394:   5%|▋            | 8/146 [00:04<01:19,  1.74it/s]Epoch: 6, train for the 162-th batch, train loss: 0.6224859952926636:  68%|███████▍   | 161/237 [01:33<00:44,  1.71it/s]Epoch: 6, train for the 162-th batch, train loss: 0.6224859952926636:  68%|███████▌   | 162/237 [01:33<00:43,  1.71it/s]Epoch: 20, train for the 96-th batch, train loss: 0.5646554231643677:  63%|███████▌    | 95/151 [00:21<00:12,  4.46it/s]Epoch: 20, train for the 96-th batch, train loss: 0.5646554231643677:  64%|███████▋    | 96/151 [00:21<00:12,  4.49it/s]Epoch: 11, train for the 60-th batch, train loss: 0.1407894343137741:  50%|█████▉      | 59/119 [00:35<00:36,  1.65it/s]Epoch: 11, train for the 60-th batch, train loss: 0.1407894343137741:  50%|██████      | 60/119 [00:35<00:35,  1.66it/s]Epoch: 4, train for the 162-th batch, train loss: 0.41463416814804077:  42%|████▏     | 161/383 [01:35<02:11,  1.68it/s]Epoch: 4, train for the 162-th batch, train loss: 0.41463416814804077:  42%|████▏     | 162/383 [01:35<02:11,  1.69it/s]Epoch: 20, train for the 97-th batch, train loss: 0.597820520401001:  64%|████████▎    | 96/151 [00:21<00:12,  4.49it/s]Epoch: 20, train for the 97-th batch, train loss: 0.597820520401001:  64%|████████▎    | 97/151 [00:21<00:11,  4.50it/s]Epoch: 10, train for the 9-th batch, train loss: 0.479883074760437:   5%|▊              | 8/146 [00:05<01:19,  1.74it/s]Epoch: 10, train for the 9-th batch, train loss: 0.479883074760437:   6%|▉              | 9/146 [00:05<01:16,  1.78it/s]Epoch: 6, train for the 163-th batch, train loss: 0.6549467444419861:  68%|███████▌   | 162/237 [01:34<00:43,  1.71it/s]Epoch: 6, train for the 163-th batch, train loss: 0.6549467444419861:  69%|███████▌   | 163/237 [01:34<00:43,  1.71it/s]Epoch: 20, train for the 98-th batch, train loss: 0.6165180206298828:  64%|███████▋    | 97/151 [00:21<00:11,  4.50it/s]Epoch: 20, train for the 98-th batch, train loss: 0.6165180206298828:  65%|███████▊    | 98/151 [00:21<00:11,  4.51it/s]Epoch: 11, train for the 61-th batch, train loss: 0.14807772636413574:  50%|█████▌     | 60/119 [00:36<00:35,  1.66it/s]Epoch: 11, train for the 61-th batch, train loss: 0.14807772636413574:  51%|█████▋     | 61/119 [00:36<00:34,  1.66it/s]Epoch: 20, train for the 99-th batch, train loss: 0.6551246643066406:  65%|███████▊    | 98/151 [00:21<00:11,  4.51it/s]Epoch: 20, train for the 99-th batch, train loss: 0.6551246643066406:  66%|███████▊    | 99/151 [00:21<00:11,  4.49it/s]Epoch: 4, train for the 163-th batch, train loss: 0.335806280374527:  42%|█████       | 162/383 [01:35<02:11,  1.69it/s]Epoch: 4, train for the 163-th batch, train loss: 0.335806280374527:  43%|█████       | 163/383 [01:35<02:10,  1.69it/s]Epoch: 20, train for the 100-th batch, train loss: 0.6656193137168884:  66%|███████▏   | 99/151 [00:22<00:11,  4.49it/s]Epoch: 20, train for the 100-th batch, train loss: 0.6656193137168884:  66%|██████▌   | 100/151 [00:22<00:11,  4.50it/s]Epoch: 10, train for the 10-th batch, train loss: 0.5068662762641907:   6%|▊            | 9/146 [00:05<01:16,  1.78it/s]Epoch: 10, train for the 10-th batch, train loss: 0.5068662762641907:   7%|▊           | 10/146 [00:05<01:16,  1.77it/s]Epoch: 6, train for the 164-th batch, train loss: 0.6335991621017456:  69%|███████▌   | 163/237 [01:34<00:43,  1.71it/s]Epoch: 6, train for the 164-th batch, train loss: 0.6335991621017456:  69%|███████▌   | 164/237 [01:34<00:42,  1.71it/s]Epoch: 20, train for the 101-th batch, train loss: 0.6692036390304565:  66%|██████▌   | 100/151 [00:22<00:11,  4.50it/s]Epoch: 20, train for the 101-th batch, train loss: 0.6692036390304565:  67%|██████▋   | 101/151 [00:22<00:11,  4.52it/s]Epoch: 11, train for the 62-th batch, train loss: 0.2241426259279251:  51%|██████▏     | 61/119 [00:36<00:34,  1.66it/s]Epoch: 11, train for the 62-th batch, train loss: 0.2241426259279251:  52%|██████▎     | 62/119 [00:36<00:34,  1.67it/s]Epoch: 4, train for the 164-th batch, train loss: 0.37526360154151917:  43%|████▎     | 163/383 [01:36<02:10,  1.69it/s]Epoch: 4, train for the 164-th batch, train loss: 0.37526360154151917:  43%|████▎     | 164/383 [01:36<02:09,  1.69it/s]Epoch: 20, train for the 102-th batch, train loss: 0.5842669606208801:  67%|██████▋   | 101/151 [00:22<00:11,  4.52it/s]Epoch: 20, train for the 102-th batch, train loss: 0.5842669606208801:  68%|██████▊   | 102/151 [00:22<00:10,  4.51it/s]Epoch: 10, train for the 11-th batch, train loss: 0.4557638466358185:   7%|▊           | 10/146 [00:06<01:16,  1.77it/s]Epoch: 10, train for the 11-th batch, train loss: 0.4557638466358185:   8%|▉           | 11/146 [00:06<01:14,  1.81it/s]Epoch: 20, train for the 103-th batch, train loss: 0.6111869215965271:  68%|██████▊   | 102/151 [00:22<00:10,  4.51it/s]Epoch: 20, train for the 103-th batch, train loss: 0.6111869215965271:  68%|██████▊   | 103/151 [00:22<00:10,  4.49it/s]Epoch: 6, train for the 165-th batch, train loss: 0.629764974117279:  69%|████████▎   | 164/237 [01:35<00:42,  1.71it/s]Epoch: 6, train for the 165-th batch, train loss: 0.629764974117279:  70%|████████▎   | 165/237 [01:35<00:42,  1.70it/s]Epoch: 20, train for the 104-th batch, train loss: 0.6400078535079956:  68%|██████▊   | 103/151 [00:22<00:10,  4.49it/s]Epoch: 20, train for the 104-th batch, train loss: 0.6400078535079956:  69%|██████▉   | 104/151 [00:22<00:10,  4.50it/s]Epoch: 11, train for the 63-th batch, train loss: 0.16106124222278595:  52%|█████▋     | 62/119 [00:37<00:34,  1.67it/s]Epoch: 11, train for the 63-th batch, train loss: 0.16106124222278595:  53%|█████▊     | 63/119 [00:37<00:33,  1.68it/s]Epoch: 4, train for the 165-th batch, train loss: 0.34038466215133667:  43%|████▎     | 164/383 [01:37<02:09,  1.69it/s]Epoch: 4, train for the 165-th batch, train loss: 0.34038466215133667:  43%|████▎     | 165/383 [01:37<02:08,  1.70it/s]Epoch: 10, train for the 12-th batch, train loss: 0.3921530842781067:   8%|▉           | 11/146 [00:06<01:14,  1.81it/s]Epoch: 10, train for the 12-th batch, train loss: 0.3921530842781067:   8%|▉           | 12/146 [00:06<01:12,  1.84it/s]Epoch: 20, train for the 105-th batch, train loss: 0.5627894997596741:  69%|██████▉   | 104/151 [00:23<00:10,  4.50it/s]Epoch: 20, train for the 105-th batch, train loss: 0.5627894997596741:  70%|██████▉   | 105/151 [00:23<00:10,  4.48it/s]Epoch: 6, train for the 166-th batch, train loss: 0.6144950985908508:  70%|███████▋   | 165/237 [01:36<00:42,  1.70it/s]Epoch: 6, train for the 166-th batch, train loss: 0.6144950985908508:  70%|███████▋   | 166/237 [01:36<00:41,  1.70it/s]Epoch: 20, train for the 106-th batch, train loss: 0.5464141368865967:  70%|██████▉   | 105/151 [00:23<00:10,  4.48it/s]Epoch: 20, train for the 106-th batch, train loss: 0.5464141368865967:  70%|███████   | 106/151 [00:23<00:10,  4.48it/s]Epoch: 11, train for the 64-th batch, train loss: 0.12643641233444214:  53%|█████▊     | 63/119 [00:37<00:33,  1.68it/s]Epoch: 11, train for the 64-th batch, train loss: 0.12643641233444214:  54%|█████▉     | 64/119 [00:37<00:33,  1.65it/s]Epoch: 20, train for the 107-th batch, train loss: 0.5474709868431091:  70%|███████   | 106/151 [00:23<00:10,  4.48it/s]Epoch: 20, train for the 107-th batch, train loss: 0.5474709868431091:  71%|███████   | 107/151 [00:23<00:09,  4.48it/s]Epoch: 4, train for the 166-th batch, train loss: 0.43246909976005554:  43%|████▎     | 165/383 [01:37<02:08,  1.70it/s]Epoch: 4, train for the 166-th batch, train loss: 0.43246909976005554:  43%|████▎     | 166/383 [01:37<02:07,  1.70it/s]Epoch: 10, train for the 13-th batch, train loss: 0.4562569558620453:   8%|▉           | 12/146 [00:07<01:12,  1.84it/s]Epoch: 10, train for the 13-th batch, train loss: 0.4562569558620453:   9%|█           | 13/146 [00:07<01:14,  1.78it/s]Epoch: 20, train for the 108-th batch, train loss: 0.48643049597740173:  71%|██████▍  | 107/151 [00:23<00:09,  4.48it/s]Epoch: 20, train for the 108-th batch, train loss: 0.48643049597740173:  72%|██████▍  | 108/151 [00:23<00:09,  4.51it/s]Epoch: 6, train for the 167-th batch, train loss: 0.6260345578193665:  70%|███████▋   | 166/237 [01:36<00:41,  1.70it/s]Epoch: 6, train for the 167-th batch, train loss: 0.6260345578193665:  70%|███████▊   | 167/237 [01:36<00:41,  1.70it/s]Epoch: 20, train for the 109-th batch, train loss: 0.5354841351509094:  72%|███████▏  | 108/151 [00:24<00:09,  4.51it/s]Epoch: 20, train for the 109-th batch, train loss: 0.5354841351509094:  72%|███████▏  | 109/151 [00:24<00:09,  4.50it/s]Epoch: 11, train for the 65-th batch, train loss: 0.16865089535713196:  54%|█████▉     | 64/119 [00:38<00:33,  1.65it/s]Epoch: 11, train for the 65-th batch, train loss: 0.16865089535713196:  55%|██████     | 65/119 [00:38<00:32,  1.67it/s]Epoch: 4, train for the 167-th batch, train loss: 0.35723862051963806:  43%|████▎     | 166/383 [01:38<02:07,  1.70it/s]Epoch: 4, train for the 167-th batch, train loss: 0.35723862051963806:  44%|████▎     | 167/383 [01:38<02:07,  1.70it/s]Epoch: 20, train for the 110-th batch, train loss: 0.5641422867774963:  72%|███████▏  | 109/151 [00:24<00:09,  4.50it/s]Epoch: 20, train for the 110-th batch, train loss: 0.5641422867774963:  73%|███████▎  | 110/151 [00:24<00:09,  4.49it/s]Epoch: 10, train for the 14-th batch, train loss: 0.41982758045196533:   9%|▉          | 13/146 [00:07<01:14,  1.78it/s]Epoch: 10, train for the 14-th batch, train loss: 0.41982758045196533:  10%|█          | 14/146 [00:07<01:15,  1.74it/s]Epoch: 20, train for the 111-th batch, train loss: 0.5251824855804443:  73%|███████▎  | 110/151 [00:24<00:09,  4.49it/s]Epoch: 20, train for the 111-th batch, train loss: 0.5251824855804443:  74%|███████▎  | 111/151 [00:24<00:08,  4.51it/s]Epoch: 6, train for the 168-th batch, train loss: 0.6448960900306702:  70%|███████▊   | 167/237 [01:37<00:41,  1.70it/s]Epoch: 6, train for the 168-th batch, train loss: 0.6448960900306702:  71%|███████▊   | 168/237 [01:37<00:40,  1.70it/s]Epoch: 20, train for the 112-th batch, train loss: 0.5211624503135681:  74%|███████▎  | 111/151 [00:24<00:08,  4.51it/s]Epoch: 20, train for the 112-th batch, train loss: 0.5211624503135681:  74%|███████▍  | 112/151 [00:24<00:08,  4.48it/s]Epoch: 11, train for the 66-th batch, train loss: 0.1599849909543991:  55%|██████▌     | 65/119 [00:39<00:32,  1.67it/s]Epoch: 11, train for the 66-th batch, train loss: 0.1599849909543991:  55%|██████▋     | 66/119 [00:39<00:31,  1.66it/s]Epoch: 4, train for the 168-th batch, train loss: 0.3001740872859955:  44%|████▊      | 167/383 [01:38<02:07,  1.70it/s]Epoch: 4, train for the 168-th batch, train loss: 0.3001740872859955:  44%|████▊      | 168/383 [01:38<02:06,  1.70it/s]Epoch: 10, train for the 15-th batch, train loss: 0.4202667772769928:  10%|█▏          | 14/146 [00:08<01:15,  1.74it/s]Epoch: 10, train for the 15-th batch, train loss: 0.4202667772769928:  10%|█▏          | 15/146 [00:08<01:16,  1.71it/s]Epoch: 20, train for the 113-th batch, train loss: 0.5720154643058777:  74%|███████▍  | 112/151 [00:24<00:08,  4.48it/s]Epoch: 20, train for the 113-th batch, train loss: 0.5720154643058777:  75%|███████▍  | 113/151 [00:24<00:08,  4.47it/s]Epoch: 6, train for the 169-th batch, train loss: 0.6462257504463196:  71%|███████▊   | 168/237 [01:37<00:40,  1.70it/s]Epoch: 6, train for the 169-th batch, train loss: 0.6462257504463196:  71%|███████▊   | 169/237 [01:37<00:40,  1.70it/s]Epoch: 20, train for the 114-th batch, train loss: 0.5227566361427307:  75%|███████▍  | 113/151 [00:25<00:08,  4.47it/s]Epoch: 20, train for the 114-th batch, train loss: 0.5227566361427307:  75%|███████▌  | 114/151 [00:25<00:08,  4.46it/s]Epoch: 20, train for the 115-th batch, train loss: 0.5313550233840942:  75%|███████▌  | 114/151 [00:25<00:08,  4.46it/s]Epoch: 20, train for the 115-th batch, train loss: 0.5313550233840942:  76%|███████▌  | 115/151 [00:25<00:08,  4.47it/s]Epoch: 4, train for the 169-th batch, train loss: 0.35753098130226135:  44%|████▍     | 168/383 [01:39<02:06,  1.70it/s]Epoch: 4, train for the 169-th batch, train loss: 0.35753098130226135:  44%|████▍     | 169/383 [01:39<02:06,  1.70it/s]Epoch: 11, train for the 67-th batch, train loss: 0.212514266371727:  55%|███████▏     | 66/119 [00:39<00:31,  1.66it/s]Epoch: 11, train for the 67-th batch, train loss: 0.212514266371727:  56%|███████▎     | 67/119 [00:39<00:31,  1.66it/s]Epoch: 10, train for the 16-th batch, train loss: 0.44818922877311707:  10%|█▏         | 15/146 [00:09<01:16,  1.71it/s]Epoch: 10, train for the 16-th batch, train loss: 0.44818922877311707:  11%|█▏         | 16/146 [00:09<01:16,  1.69it/s]Epoch: 20, train for the 116-th batch, train loss: 0.4795953929424286:  76%|███████▌  | 115/151 [00:25<00:08,  4.47it/s]Epoch: 20, train for the 116-th batch, train loss: 0.4795953929424286:  77%|███████▋  | 116/151 [00:25<00:07,  4.48it/s]Epoch: 6, train for the 170-th batch, train loss: 0.6404544711112976:  71%|███████▊   | 169/237 [01:38<00:40,  1.70it/s]Epoch: 6, train for the 170-th batch, train loss: 0.6404544711112976:  72%|███████▉   | 170/237 [01:38<00:39,  1.70it/s]Epoch: 20, train for the 117-th batch, train loss: 0.5321545004844666:  77%|███████▋  | 116/151 [00:25<00:07,  4.48it/s]Epoch: 20, train for the 117-th batch, train loss: 0.5321545004844666:  77%|███████▋  | 117/151 [00:25<00:07,  4.49it/s]Epoch: 4, train for the 170-th batch, train loss: 0.36534467339515686:  44%|████▍     | 169/383 [01:39<02:06,  1.70it/s]Epoch: 4, train for the 170-th batch, train loss: 0.36534467339515686:  44%|████▍     | 170/383 [01:39<02:05,  1.70it/s]Epoch: 11, train for the 68-th batch, train loss: 0.1233014389872551:  56%|██████▊     | 67/119 [00:40<00:31,  1.66it/s]Epoch: 11, train for the 68-th batch, train loss: 0.1233014389872551:  57%|██████▊     | 68/119 [00:40<00:30,  1.65it/s]Epoch: 20, train for the 118-th batch, train loss: 0.47656095027923584:  77%|██████▉  | 117/151 [00:26<00:07,  4.49it/s]Epoch: 20, train for the 118-th batch, train loss: 0.47656095027923584:  78%|███████  | 118/151 [00:26<00:07,  4.50it/s]Epoch: 10, train for the 17-th batch, train loss: 0.4347435534000397:  11%|█▎          | 16/146 [00:09<01:16,  1.69it/s]Epoch: 10, train for the 17-th batch, train loss: 0.4347435534000397:  12%|█▍          | 17/146 [00:09<01:16,  1.70it/s]Epoch: 6, train for the 171-th batch, train loss: 0.6437274813652039:  72%|███████▉   | 170/237 [01:39<00:39,  1.70it/s]Epoch: 6, train for the 171-th batch, train loss: 0.6437274813652039:  72%|███████▉   | 171/237 [01:39<00:38,  1.70it/s]Epoch: 20, train for the 119-th batch, train loss: 0.5238957405090332:  78%|███████▊  | 118/151 [00:26<00:07,  4.50it/s]Epoch: 20, train for the 119-th batch, train loss: 0.5238957405090332:  79%|███████▉  | 119/151 [00:26<00:07,  4.50it/s]Epoch: 20, train for the 120-th batch, train loss: 0.5828651189804077:  79%|███████▉  | 119/151 [00:26<00:07,  4.50it/s]Epoch: 20, train for the 120-th batch, train loss: 0.5828651189804077:  79%|███████▉  | 120/151 [00:26<00:06,  4.51it/s]Epoch: 4, train for the 171-th batch, train loss: 0.4881179630756378:  44%|████▉      | 170/383 [01:40<02:05,  1.70it/s]Epoch: 4, train for the 171-th batch, train loss: 0.4881179630756378:  45%|████▉      | 171/383 [01:40<02:04,  1.70it/s]Epoch: 11, train for the 69-th batch, train loss: 0.1446298062801361:  57%|██████▊     | 68/119 [00:40<00:30,  1.65it/s]Epoch: 11, train for the 69-th batch, train loss: 0.1446298062801361:  58%|██████▉     | 69/119 [00:40<00:29,  1.67it/s]Epoch: 20, train for the 121-th batch, train loss: 0.4880683124065399:  79%|███████▉  | 120/151 [00:26<00:06,  4.51it/s]Epoch: 20, train for the 121-th batch, train loss: 0.4880683124065399:  80%|████████  | 121/151 [00:26<00:06,  4.51it/s]Epoch: 10, train for the 18-th batch, train loss: 0.38862594962120056:  12%|█▎         | 17/146 [00:10<01:16,  1.70it/s]Epoch: 10, train for the 18-th batch, train loss: 0.38862594962120056:  12%|█▎         | 18/146 [00:10<01:15,  1.69it/s]Epoch: 6, train for the 172-th batch, train loss: 0.5877660512924194:  72%|███████▉   | 171/237 [01:39<00:38,  1.70it/s]Epoch: 6, train for the 172-th batch, train loss: 0.5877660512924194:  73%|███████▉   | 172/237 [01:39<00:38,  1.70it/s]Epoch: 20, train for the 122-th batch, train loss: 0.5495480895042419:  80%|████████  | 121/151 [00:26<00:06,  4.51it/s]Epoch: 20, train for the 122-th batch, train loss: 0.5495480895042419:  81%|████████  | 122/151 [00:26<00:06,  4.50it/s]Epoch: 4, train for the 172-th batch, train loss: 0.31890869140625:  45%|█████▊       | 171/383 [01:41<02:04,  1.70it/s]Epoch: 4, train for the 172-th batch, train loss: 0.31890869140625:  45%|█████▊       | 172/383 [01:41<02:04,  1.70it/s]Epoch: 20, train for the 123-th batch, train loss: 0.5256181955337524:  81%|████████  | 122/151 [00:27<00:06,  4.50it/s]Epoch: 20, train for the 123-th batch, train loss: 0.5256181955337524:  81%|████████▏ | 123/151 [00:27<00:06,  4.49it/s]Epoch: 11, train for the 70-th batch, train loss: 0.12188970297574997:  58%|██████▍    | 69/119 [00:41<00:29,  1.67it/s]Epoch: 11, train for the 70-th batch, train loss: 0.12188970297574997:  59%|██████▍    | 70/119 [00:41<00:29,  1.68it/s]Epoch: 10, train for the 19-th batch, train loss: 0.44948676228523254:  12%|█▎         | 18/146 [00:10<01:15,  1.69it/s]Epoch: 10, train for the 19-th batch, train loss: 0.44948676228523254:  13%|█▍         | 19/146 [00:10<01:15,  1.68it/s]Epoch: 20, train for the 124-th batch, train loss: 0.525830090045929:  81%|████████▉  | 123/151 [00:27<00:06,  4.49it/s]Epoch: 20, train for the 124-th batch, train loss: 0.525830090045929:  82%|█████████  | 124/151 [00:27<00:06,  4.49it/s]Epoch: 6, train for the 173-th batch, train loss: 0.5988693237304688:  73%|███████▉   | 172/237 [01:40<00:38,  1.70it/s]Epoch: 6, train for the 173-th batch, train loss: 0.5988693237304688:  73%|████████   | 173/237 [01:40<00:37,  1.70it/s]Epoch: 20, train for the 125-th batch, train loss: 0.5555866360664368:  82%|████████▏ | 124/151 [00:27<00:06,  4.49it/s]Epoch: 20, train for the 125-th batch, train loss: 0.5555866360664368:  83%|████████▎ | 125/151 [00:27<00:05,  4.50it/s]Epoch: 4, train for the 173-th batch, train loss: 0.3767072856426239:  45%|████▉      | 172/383 [01:41<02:04,  1.70it/s]Epoch: 4, train for the 173-th batch, train loss: 0.3767072856426239:  45%|████▉      | 173/383 [01:41<02:03,  1.70it/s]Epoch: 11, train for the 71-th batch, train loss: 0.17002694308757782:  59%|██████▍    | 70/119 [00:42<00:29,  1.68it/s]Epoch: 11, train for the 71-th batch, train loss: 0.17002694308757782:  60%|██████▌    | 71/119 [00:42<00:28,  1.66it/s]Epoch: 20, train for the 126-th batch, train loss: 0.5384982824325562:  83%|████████▎ | 125/151 [00:27<00:05,  4.50it/s]Epoch: 20, train for the 126-th batch, train loss: 0.5384982824325562:  83%|████████▎ | 126/151 [00:27<00:05,  4.49it/s]Epoch: 10, train for the 20-th batch, train loss: 0.44377708435058594:  13%|█▍         | 19/146 [00:11<01:15,  1.68it/s]Epoch: 10, train for the 20-th batch, train loss: 0.44377708435058594:  14%|█▌         | 20/146 [00:11<01:16,  1.66it/s]Epoch: 6, train for the 174-th batch, train loss: 0.6126633882522583:  73%|████████   | 173/237 [01:40<00:37,  1.70it/s]Epoch: 6, train for the 174-th batch, train loss: 0.6126633882522583:  73%|████████   | 174/237 [01:40<00:37,  1.70it/s]Epoch: 20, train for the 127-th batch, train loss: 0.5600066781044006:  83%|████████▎ | 126/151 [00:28<00:05,  4.49it/s]Epoch: 20, train for the 127-th batch, train loss: 0.5600066781044006:  84%|████████▍ | 127/151 [00:28<00:05,  4.49it/s]Epoch: 20, train for the 128-th batch, train loss: 0.6003934144973755:  84%|████████▍ | 127/151 [00:28<00:05,  4.49it/s]Epoch: 20, train for the 128-th batch, train loss: 0.6003934144973755:  85%|████████▍ | 128/151 [00:28<00:05,  4.50it/s]Epoch: 4, train for the 174-th batch, train loss: 0.48863211274147034:  45%|████▌     | 173/383 [01:42<02:03,  1.70it/s]Epoch: 4, train for the 174-th batch, train loss: 0.48863211274147034:  45%|████▌     | 174/383 [01:42<02:02,  1.70it/s]Epoch: 11, train for the 72-th batch, train loss: 0.17578624188899994:  60%|██████▌    | 71/119 [00:42<00:28,  1.66it/s]Epoch: 11, train for the 72-th batch, train loss: 0.17578624188899994:  61%|██████▋    | 72/119 [00:42<00:28,  1.65it/s]Epoch: 20, train for the 129-th batch, train loss: 0.5909900069236755:  85%|████████▍ | 128/151 [00:28<00:05,  4.50it/s]Epoch: 20, train for the 129-th batch, train loss: 0.5909900069236755:  85%|████████▌ | 129/151 [00:28<00:04,  4.50it/s]Epoch: 10, train for the 21-th batch, train loss: 0.4093547761440277:  14%|█▋          | 20/146 [00:12<01:16,  1.66it/s]Epoch: 10, train for the 21-th batch, train loss: 0.4093547761440277:  14%|█▋          | 21/146 [00:12<01:15,  1.65it/s]Epoch: 6, train for the 175-th batch, train loss: 0.6256557703018188:  73%|████████   | 174/237 [01:41<00:37,  1.70it/s]Epoch: 6, train for the 175-th batch, train loss: 0.6256557703018188:  74%|████████   | 175/237 [01:41<00:36,  1.70it/s]Epoch: 20, train for the 130-th batch, train loss: 0.5452057123184204:  85%|████████▌ | 129/151 [00:28<00:04,  4.50it/s]Epoch: 20, train for the 130-th batch, train loss: 0.5452057123184204:  86%|████████▌ | 130/151 [00:28<00:04,  4.51it/s]Epoch: 4, train for the 175-th batch, train loss: 0.3930390179157257:  45%|████▉      | 174/383 [01:42<02:02,  1.70it/s]Epoch: 4, train for the 175-th batch, train loss: 0.3930390179157257:  46%|█████      | 175/383 [01:42<02:02,  1.70it/s]Epoch: 20, train for the 131-th batch, train loss: 0.5176914930343628:  86%|████████▌ | 130/151 [00:28<00:04,  4.51it/s]Epoch: 20, train for the 131-th batch, train loss: 0.5176914930343628:  87%|████████▋ | 131/151 [00:28<00:04,  4.39it/s]Epoch: 11, train for the 73-th batch, train loss: 0.1358852982521057:  61%|███████▎    | 72/119 [00:43<00:28,  1.65it/s]Epoch: 11, train for the 73-th batch, train loss: 0.1358852982521057:  61%|███████▎    | 73/119 [00:43<00:27,  1.64it/s]Epoch: 10, train for the 22-th batch, train loss: 0.4875900149345398:  14%|█▋          | 21/146 [00:12<01:15,  1.65it/s]Epoch: 10, train for the 22-th batch, train loss: 0.4875900149345398:  15%|█▊          | 22/146 [00:12<01:14,  1.66it/s]Epoch: 20, train for the 132-th batch, train loss: 0.530488908290863:  87%|█████████▌ | 131/151 [00:29<00:04,  4.39it/s]Epoch: 20, train for the 132-th batch, train loss: 0.530488908290863:  87%|█████████▌ | 132/151 [00:29<00:04,  4.41it/s]Epoch: 6, train for the 176-th batch, train loss: 0.5599280595779419:  74%|████████   | 175/237 [01:42<00:36,  1.70it/s]Epoch: 6, train for the 176-th batch, train loss: 0.5599280595779419:  74%|████████▏  | 176/237 [01:42<00:35,  1.71it/s]Epoch: 20, train for the 133-th batch, train loss: 0.5366946458816528:  87%|████████▋ | 132/151 [00:29<00:04,  4.41it/s]Epoch: 20, train for the 133-th batch, train loss: 0.5366946458816528:  88%|████████▊ | 133/151 [00:29<00:04,  4.42it/s]Epoch: 4, train for the 176-th batch, train loss: 0.3802972435951233:  46%|█████      | 175/383 [01:43<02:02,  1.70it/s]Epoch: 4, train for the 176-th batch, train loss: 0.3802972435951233:  46%|█████      | 176/383 [01:43<02:01,  1.71it/s]Epoch: 11, train for the 74-th batch, train loss: 0.19154958426952362:  61%|██████▋    | 73/119 [00:43<00:27,  1.64it/s]Epoch: 11, train for the 74-th batch, train loss: 0.19154958426952362:  62%|██████▊    | 74/119 [00:43<00:27,  1.66it/s]Epoch: 20, train for the 134-th batch, train loss: 0.5437940359115601:  88%|████████▊ | 133/151 [00:29<00:04,  4.42it/s]Epoch: 20, train for the 134-th batch, train loss: 0.5437940359115601:  89%|████████▊ | 134/151 [00:29<00:03,  4.42it/s]Epoch: 10, train for the 23-th batch, train loss: 0.42890867590904236:  15%|█▋         | 22/146 [00:13<01:14,  1.66it/s]Epoch: 10, train for the 23-th batch, train loss: 0.42890867590904236:  16%|█▋         | 23/146 [00:13<01:14,  1.65it/s]Epoch: 6, train for the 177-th batch, train loss: 0.6375933885574341:  74%|████████▏  | 176/237 [01:42<00:35,  1.71it/s]Epoch: 6, train for the 177-th batch, train loss: 0.6375933885574341:  75%|████████▏  | 177/237 [01:42<00:35,  1.71it/s]Epoch: 20, train for the 135-th batch, train loss: 0.5368923544883728:  89%|████████▊ | 134/151 [00:29<00:03,  4.42it/s]Epoch: 20, train for the 135-th batch, train loss: 0.5368923544883728:  89%|████████▉ | 135/151 [00:29<00:03,  4.45it/s]Epoch: 20, train for the 136-th batch, train loss: 0.5461083650588989:  89%|████████▉ | 135/151 [00:30<00:03,  4.45it/s]Epoch: 20, train for the 136-th batch, train loss: 0.5461083650588989:  90%|█████████ | 136/151 [00:30<00:03,  4.46it/s]Epoch: 4, train for the 177-th batch, train loss: 0.35727283358573914:  46%|████▌     | 176/383 [01:44<02:01,  1.71it/s]Epoch: 4, train for the 177-th batch, train loss: 0.35727283358573914:  46%|████▌     | 177/383 [01:44<02:00,  1.70it/s]Epoch: 11, train for the 75-th batch, train loss: 0.15625005960464478:  62%|██████▊    | 74/119 [00:44<00:27,  1.66it/s]Epoch: 11, train for the 75-th batch, train loss: 0.15625005960464478:  63%|██████▉    | 75/119 [00:44<00:26,  1.65it/s]Epoch: 20, train for the 137-th batch, train loss: 0.6058956384658813:  90%|█████████ | 136/151 [00:30<00:03,  4.46it/s]Epoch: 20, train for the 137-th batch, train loss: 0.6058956384658813:  91%|█████████ | 137/151 [00:30<00:03,  4.44it/s]Epoch: 6, train for the 178-th batch, train loss: 0.6387603878974915:  75%|████████▏  | 177/237 [01:43<00:35,  1.71it/s]Epoch: 6, train for the 178-th batch, train loss: 0.6387603878974915:  75%|████████▎  | 178/237 [01:43<00:34,  1.70it/s]Epoch: 10, train for the 24-th batch, train loss: 0.48409682512283325:  16%|█▋         | 23/146 [00:13<01:14,  1.65it/s]Epoch: 10, train for the 24-th batch, train loss: 0.48409682512283325:  16%|█▊         | 24/146 [00:13<01:14,  1.65it/s]Epoch: 20, train for the 138-th batch, train loss: 0.6090631484985352:  91%|█████████ | 137/151 [00:30<00:03,  4.44it/s]Epoch: 20, train for the 138-th batch, train loss: 0.6090631484985352:  91%|█████████▏| 138/151 [00:30<00:02,  4.46it/s]Epoch: 4, train for the 178-th batch, train loss: 0.44823023676872253:  46%|████▌     | 177/383 [01:44<02:00,  1.70it/s]Epoch: 4, train for the 178-th batch, train loss: 0.44823023676872253:  46%|████▋     | 178/383 [01:44<02:00,  1.71it/s]Epoch: 20, train for the 139-th batch, train loss: 0.5601793527603149:  91%|█████████▏| 138/151 [00:30<00:02,  4.46it/s]Epoch: 20, train for the 139-th batch, train loss: 0.5601793527603149:  92%|█████████▏| 139/151 [00:30<00:02,  4.47it/s]Epoch: 11, train for the 76-th batch, train loss: 0.21432150900363922:  63%|██████▉    | 75/119 [00:45<00:26,  1.65it/s]Epoch: 11, train for the 76-th batch, train loss: 0.21432150900363922:  64%|███████    | 76/119 [00:45<00:26,  1.64it/s]Epoch: 6, train for the 179-th batch, train loss: 0.607822835445404:  75%|█████████   | 178/237 [01:43<00:34,  1.70it/s]Epoch: 6, train for the 179-th batch, train loss: 0.607822835445404:  76%|█████████   | 179/237 [01:43<00:34,  1.71it/s]Epoch: 20, train for the 140-th batch, train loss: 0.5163097381591797:  92%|█████████▏| 139/151 [00:30<00:02,  4.47it/s]Epoch: 20, train for the 140-th batch, train loss: 0.5163097381591797:  93%|█████████▎| 140/151 [00:30<00:02,  4.48it/s]Epoch: 10, train for the 25-th batch, train loss: 0.4974680244922638:  16%|█▉          | 24/146 [00:14<01:14,  1.65it/s]Epoch: 10, train for the 25-th batch, train loss: 0.4974680244922638:  17%|██          | 25/146 [00:14<01:13,  1.65it/s]Epoch: 20, train for the 141-th batch, train loss: 0.5353714823722839:  93%|█████████▎| 140/151 [00:31<00:02,  4.48it/s]Epoch: 20, train for the 141-th batch, train loss: 0.5353714823722839:  93%|█████████▎| 141/151 [00:31<00:02,  4.48it/s]Epoch: 4, train for the 179-th batch, train loss: 0.3422969877719879:  46%|█████      | 178/383 [01:45<02:00,  1.71it/s]Epoch: 4, train for the 179-th batch, train loss: 0.3422969877719879:  47%|█████▏     | 179/383 [01:45<01:59,  1.71it/s]Epoch: 20, train for the 142-th batch, train loss: 0.5539492964744568:  93%|█████████▎| 141/151 [00:31<00:02,  4.48it/s]Epoch: 20, train for the 142-th batch, train loss: 0.5539492964744568:  94%|█████████▍| 142/151 [00:31<00:02,  4.49it/s]Epoch: 11, train for the 77-th batch, train loss: 0.14443790912628174:  64%|███████    | 76/119 [00:45<00:26,  1.64it/s]Epoch: 11, train for the 77-th batch, train loss: 0.14443790912628174:  65%|███████    | 77/119 [00:45<00:25,  1.66it/s]Epoch: 6, train for the 180-th batch, train loss: 0.6511996388435364:  76%|████████▎  | 179/237 [01:44<00:34,  1.71it/s]Epoch: 6, train for the 180-th batch, train loss: 0.6511996388435364:  76%|████████▎  | 180/237 [01:44<00:33,  1.70it/s]Epoch: 10, train for the 26-th batch, train loss: 0.36493220925331116:  17%|█▉         | 25/146 [00:15<01:13,  1.65it/s]Epoch: 10, train for the 26-th batch, train loss: 0.36493220925331116:  18%|█▉         | 26/146 [00:15<01:12,  1.66it/s]Epoch: 20, train for the 143-th batch, train loss: 0.47063666582107544:  94%|████████▍| 142/151 [00:31<00:02,  4.49it/s]Epoch: 20, train for the 143-th batch, train loss: 0.47063666582107544:  95%|████████▌| 143/151 [00:31<00:01,  4.49it/s]Epoch: 4, train for the 180-th batch, train loss: 0.3910825252532959:  47%|█████▏     | 179/383 [01:45<01:59,  1.71it/s]Epoch: 4, train for the 180-th batch, train loss: 0.3910825252532959:  47%|█████▏     | 180/383 [01:45<01:59,  1.70it/s]Epoch: 20, train for the 144-th batch, train loss: 0.5009396076202393:  95%|█████████▍| 143/151 [00:31<00:01,  4.49it/s]Epoch: 20, train for the 144-th batch, train loss: 0.5009396076202393:  95%|█████████▌| 144/151 [00:31<00:01,  4.48it/s]Epoch: 11, train for the 78-th batch, train loss: 0.18244034051895142:  65%|███████    | 77/119 [00:46<00:25,  1.66it/s]Epoch: 11, train for the 78-th batch, train loss: 0.18244034051895142:  66%|███████▏   | 78/119 [00:46<00:24,  1.65it/s]Epoch: 20, train for the 145-th batch, train loss: 0.5293654203414917:  95%|█████████▌| 144/151 [00:32<00:01,  4.48it/s]Epoch: 20, train for the 145-th batch, train loss: 0.5293654203414917:  96%|█████████▌| 145/151 [00:32<00:01,  4.45it/s]Epoch: 6, train for the 181-th batch, train loss: 0.6477621793746948:  76%|████████▎  | 180/237 [01:44<00:33,  1.70it/s]Epoch: 6, train for the 181-th batch, train loss: 0.6477621793746948:  76%|████████▍  | 181/237 [01:44<00:32,  1.71it/s]Epoch: 10, train for the 27-th batch, train loss: 0.5321087837219238:  18%|██▏         | 26/146 [00:15<01:12,  1.66it/s]Epoch: 10, train for the 27-th batch, train loss: 0.5321087837219238:  18%|██▏         | 27/146 [00:15<01:12,  1.65it/s]Epoch: 20, train for the 146-th batch, train loss: 0.525752067565918:  96%|██████████▌| 145/151 [00:32<00:01,  4.45it/s]Epoch: 20, train for the 146-th batch, train loss: 0.525752067565918:  97%|██████████▋| 146/151 [00:32<00:01,  4.46it/s]Epoch: 4, train for the 181-th batch, train loss: 0.36874762177467346:  47%|████▋     | 180/383 [01:46<01:59,  1.70it/s]Epoch: 4, train for the 181-th batch, train loss: 0.36874762177467346:  47%|████▋     | 181/383 [01:46<01:58,  1.71it/s]Epoch: 20, train for the 147-th batch, train loss: 0.567754864692688:  97%|██████████▋| 146/151 [00:32<00:01,  4.46it/s]Epoch: 20, train for the 147-th batch, train loss: 0.567754864692688:  97%|██████████▋| 147/151 [00:32<00:00,  4.46it/s]Epoch: 11, train for the 79-th batch, train loss: 0.16183750331401825:  66%|███████▏   | 78/119 [00:46<00:24,  1.65it/s]Epoch: 11, train for the 79-th batch, train loss: 0.16183750331401825:  66%|███████▎   | 79/119 [00:46<00:24,  1.65it/s]Epoch: 6, train for the 182-th batch, train loss: 0.644612729549408:  76%|█████████▏  | 181/237 [01:45<00:32,  1.71it/s]Epoch: 6, train for the 182-th batch, train loss: 0.644612729549408:  77%|█████████▏  | 182/237 [01:45<00:32,  1.71it/s]Epoch: 20, train for the 148-th batch, train loss: 0.5707795023918152:  97%|█████████▋| 147/151 [00:32<00:00,  4.46it/s]Epoch: 20, train for the 148-th batch, train loss: 0.5707795023918152:  98%|█████████▊| 148/151 [00:32<00:00,  4.41it/s]Epoch: 10, train for the 28-th batch, train loss: 0.4418570101261139:  18%|██▏         | 27/146 [00:16<01:12,  1.65it/s]Epoch: 10, train for the 28-th batch, train loss: 0.4418570101261139:  19%|██▎         | 28/146 [00:16<01:11,  1.64it/s]Epoch: 20, train for the 149-th batch, train loss: 0.5314632058143616:  98%|█████████▊| 148/151 [00:32<00:00,  4.41it/s]Epoch: 20, train for the 149-th batch, train loss: 0.5314632058143616:  99%|█████████▊| 149/151 [00:32<00:00,  4.42it/s]Epoch: 4, train for the 182-th batch, train loss: 0.5007727742195129:  47%|█████▏     | 181/383 [01:47<01:58,  1.71it/s]Epoch: 4, train for the 182-th batch, train loss: 0.5007727742195129:  48%|█████▏     | 182/383 [01:47<01:57,  1.71it/s]Epoch: 20, train for the 150-th batch, train loss: 0.5293658375740051:  99%|█████████▊| 149/151 [00:33<00:00,  4.42it/s]Epoch: 20, train for the 150-th batch, train loss: 0.5293658375740051:  99%|█████████▉| 150/151 [00:33<00:00,  4.43it/s]Epoch: 11, train for the 80-th batch, train loss: 0.18514911830425262:  66%|███████▎   | 79/119 [00:47<00:24,  1.65it/s]Epoch: 11, train for the 80-th batch, train loss: 0.18514911830425262:  67%|███████▍   | 80/119 [00:47<00:23,  1.64it/s]Epoch: 6, train for the 183-th batch, train loss: 0.6396774053573608:  77%|████████▍  | 182/237 [01:46<00:32,  1.71it/s]Epoch: 6, train for the 183-th batch, train loss: 0.6396774053573608:  77%|████████▍  | 183/237 [01:46<00:31,  1.71it/s]Epoch: 20, train for the 151-th batch, train loss: 0.5932310819625854:  99%|█████████▉| 150/151 [00:33<00:00,  4.43it/s]Epoch: 20, train for the 151-th batch, train loss: 0.5932310819625854: 100%|██████████| 151/151 [00:33<00:00,  4.92it/s]Epoch: 20, train for the 151-th batch, train loss: 0.5932310819625854: 100%|██████████| 151/151 [00:33<00:00,  4.53it/s]
  0%|                                                                                            | 0/46 [00:00<?, ?it/s]Epoch: 10, train for the 29-th batch, train loss: 0.46687784790992737:  19%|██         | 28/146 [00:16<01:11,  1.64it/s]Epoch: 10, train for the 29-th batch, train loss: 0.46687784790992737:  20%|██▏        | 29/146 [00:16<01:11,  1.64it/s]evaluate for the 1-th batch, evaluate loss: 0.49296876788139343:   0%|                           | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.49296876788139343:   2%|▍                  | 1/46 [00:00<00:04,  9.53it/s]evaluate for the 2-th batch, evaluate loss: 0.5018553733825684:   2%|▍                   | 1/46 [00:00<00:04,  9.53it/s]evaluate for the 2-th batch, evaluate loss: 0.5018553733825684:   4%|▊                   | 2/46 [00:00<00:04,  9.58it/s]Epoch: 4, train for the 183-th batch, train loss: 0.38970285654067993:  48%|████▊     | 182/383 [01:47<01:57,  1.71it/s]Epoch: 4, train for the 183-th batch, train loss: 0.38970285654067993:  48%|████▊     | 183/383 [01:47<01:57,  1.71it/s]evaluate for the 3-th batch, evaluate loss: 0.4795304834842682:   4%|▊                   | 2/46 [00:00<00:04,  9.58it/s]evaluate for the 3-th batch, evaluate loss: 0.4795304834842682:   7%|█▎                  | 3/46 [00:00<00:04,  9.56it/s]evaluate for the 4-th batch, evaluate loss: 0.5065811276435852:   7%|█▎                  | 3/46 [00:00<00:04,  9.56it/s]evaluate for the 4-th batch, evaluate loss: 0.5065811276435852:   9%|█▋                  | 4/46 [00:00<00:04,  9.55it/s]Epoch: 11, train for the 81-th batch, train loss: 0.21354827284812927:  67%|███████▍   | 80/119 [00:48<00:23,  1.64it/s]Epoch: 11, train for the 81-th batch, train loss: 0.21354827284812927:  68%|███████▍   | 81/119 [00:48<00:23,  1.64it/s]Epoch: 6, train for the 184-th batch, train loss: 0.6367582082748413:  77%|████████▍  | 183/237 [01:46<00:31,  1.71it/s]Epoch: 6, train for the 184-th batch, train loss: 0.6367582082748413:  78%|████████▌  | 184/237 [01:46<00:31,  1.71it/s]evaluate for the 5-th batch, evaluate loss: 0.48031675815582275:   9%|█▋                 | 4/46 [00:00<00:04,  9.55it/s]evaluate for the 5-th batch, evaluate loss: 0.48031675815582275:  11%|██                 | 5/46 [00:00<00:04,  9.59it/s]evaluate for the 6-th batch, evaluate loss: 0.5519998669624329:  11%|██▏                 | 5/46 [00:00<00:04,  9.59it/s]evaluate for the 6-th batch, evaluate loss: 0.5519998669624329:  13%|██▌                 | 6/46 [00:00<00:04,  9.57it/s]Epoch: 10, train for the 30-th batch, train loss: 0.47597330808639526:  20%|██▏        | 29/146 [00:17<01:11,  1.64it/s]Epoch: 10, train for the 30-th batch, train loss: 0.47597330808639526:  21%|██▎        | 30/146 [00:17<01:10,  1.65it/s]evaluate for the 7-th batch, evaluate loss: 0.46963924169540405:  13%|██▍                | 6/46 [00:00<00:04,  9.57it/s]evaluate for the 7-th batch, evaluate loss: 0.46963924169540405:  15%|██▉                | 7/46 [00:00<00:04,  9.58it/s]Epoch: 4, train for the 184-th batch, train loss: 0.40857627987861633:  48%|████▊     | 183/383 [01:48<01:57,  1.71it/s]Epoch: 4, train for the 184-th batch, train loss: 0.40857627987861633:  48%|████▊     | 184/383 [01:48<01:56,  1.71it/s]evaluate for the 8-th batch, evaluate loss: 0.5546031594276428:  15%|███                 | 7/46 [00:00<00:04,  9.58it/s]evaluate for the 8-th batch, evaluate loss: 0.5546031594276428:  17%|███▍                | 8/46 [00:00<00:03,  9.58it/s]evaluate for the 9-th batch, evaluate loss: 0.5251392126083374:  17%|███▍                | 8/46 [00:00<00:03,  9.58it/s]evaluate for the 9-th batch, evaluate loss: 0.5251392126083374:  20%|███▉                | 9/46 [00:00<00:03,  9.57it/s]evaluate for the 10-th batch, evaluate loss: 0.5304722189903259:  20%|███▋               | 9/46 [00:01<00:03,  9.57it/s]evaluate for the 10-th batch, evaluate loss: 0.5304722189903259:  22%|███▉              | 10/46 [00:01<00:03,  9.59it/s]Epoch: 6, train for the 185-th batch, train loss: 0.5992757081985474:  78%|████████▌  | 184/237 [01:47<00:31,  1.71it/s]Epoch: 6, train for the 185-th batch, train loss: 0.5992757081985474:  78%|████████▌  | 185/237 [01:47<00:30,  1.71it/s]Epoch: 11, train for the 82-th batch, train loss: 0.1883970946073532:  68%|████████▏   | 81/119 [00:48<00:23,  1.64it/s]Epoch: 11, train for the 82-th batch, train loss: 0.1883970946073532:  69%|████████▎   | 82/119 [00:48<00:22,  1.65it/s]evaluate for the 11-th batch, evaluate loss: 0.5195960998535156:  22%|███▉              | 10/46 [00:01<00:03,  9.59it/s]evaluate for the 11-th batch, evaluate loss: 0.5195960998535156:  24%|████▎             | 11/46 [00:01<00:03,  9.61it/s]evaluate for the 12-th batch, evaluate loss: 0.4658605754375458:  24%|████▎             | 11/46 [00:01<00:03,  9.61it/s]evaluate for the 12-th batch, evaluate loss: 0.4658605754375458:  26%|████▋             | 12/46 [00:01<00:03,  9.59it/s]Epoch: 10, train for the 31-th batch, train loss: 0.5113078951835632:  21%|██▍         | 30/146 [00:18<01:10,  1.65it/s]Epoch: 10, train for the 31-th batch, train loss: 0.5113078951835632:  21%|██▌         | 31/146 [00:18<01:09,  1.65it/s]evaluate for the 13-th batch, evaluate loss: 0.4921834468841553:  26%|████▋             | 12/46 [00:01<00:03,  9.59it/s]evaluate for the 13-th batch, evaluate loss: 0.4921834468841553:  28%|█████             | 13/46 [00:01<00:03,  9.59it/s]Epoch: 4, train for the 185-th batch, train loss: 0.35146602988243103:  48%|████▊     | 184/383 [01:48<01:56,  1.71it/s]Epoch: 4, train for the 185-th batch, train loss: 0.35146602988243103:  48%|████▊     | 185/383 [01:48<01:55,  1.71it/s]evaluate for the 14-th batch, evaluate loss: 0.5859712362289429:  28%|█████             | 13/46 [00:01<00:03,  9.59it/s]evaluate for the 14-th batch, evaluate loss: 0.5859712362289429:  30%|█████▍            | 14/46 [00:01<00:03,  9.60it/s]evaluate for the 15-th batch, evaluate loss: 0.5340318083763123:  30%|█████▍            | 14/46 [00:01<00:03,  9.60it/s]evaluate for the 15-th batch, evaluate loss: 0.5340318083763123:  33%|█████▊            | 15/46 [00:01<00:03,  9.63it/s]evaluate for the 16-th batch, evaluate loss: 0.566436231136322:  33%|██████▏            | 15/46 [00:01<00:03,  9.63it/s]evaluate for the 16-th batch, evaluate loss: 0.566436231136322:  35%|██████▌            | 16/46 [00:01<00:03,  9.59it/s]Epoch: 6, train for the 186-th batch, train loss: 0.5964943766593933:  78%|████████▌  | 185/237 [01:47<00:30,  1.71it/s]Epoch: 6, train for the 186-th batch, train loss: 0.5964943766593933:  78%|████████▋  | 186/237 [01:47<00:29,  1.71it/s]Epoch: 11, train for the 83-th batch, train loss: 0.17157074809074402:  69%|███████▌   | 82/119 [00:49<00:22,  1.65it/s]Epoch: 11, train for the 83-th batch, train loss: 0.17157074809074402:  70%|███████▋   | 83/119 [00:49<00:21,  1.65it/s]evaluate for the 17-th batch, evaluate loss: 0.4461017847061157:  35%|██████▎           | 16/46 [00:01<00:03,  9.59it/s]evaluate for the 17-th batch, evaluate loss: 0.4461017847061157:  37%|██████▋           | 17/46 [00:01<00:03,  9.57it/s]Epoch: 10, train for the 32-th batch, train loss: 0.4639699459075928:  21%|██▌         | 31/146 [00:18<01:09,  1.65it/s]Epoch: 10, train for the 32-th batch, train loss: 0.4639699459075928:  22%|██▋         | 32/146 [00:18<01:09,  1.64it/s]evaluate for the 18-th batch, evaluate loss: 0.4981926381587982:  37%|██████▋           | 17/46 [00:01<00:03,  9.57it/s]evaluate for the 18-th batch, evaluate loss: 0.4981926381587982:  39%|███████           | 18/46 [00:01<00:02,  9.56it/s]Epoch: 4, train for the 186-th batch, train loss: 0.37728258967399597:  48%|████▊     | 185/383 [01:49<01:55,  1.71it/s]Epoch: 4, train for the 186-th batch, train loss: 0.37728258967399597:  49%|████▊     | 186/383 [01:49<01:55,  1.71it/s]evaluate for the 19-th batch, evaluate loss: 0.519431471824646:  39%|███████▍           | 18/46 [00:01<00:02,  9.56it/s]evaluate for the 19-th batch, evaluate loss: 0.519431471824646:  41%|███████▊           | 19/46 [00:01<00:02,  9.58it/s]evaluate for the 20-th batch, evaluate loss: 0.5322984457015991:  41%|███████▍          | 19/46 [00:02<00:02,  9.58it/s]evaluate for the 20-th batch, evaluate loss: 0.5322984457015991:  43%|███████▊          | 20/46 [00:02<00:02,  9.61it/s]evaluate for the 21-th batch, evaluate loss: 0.5233106017112732:  43%|███████▊          | 20/46 [00:02<00:02,  9.61it/s]evaluate for the 21-th batch, evaluate loss: 0.5233106017112732:  46%|████████▏         | 21/46 [00:02<00:02,  9.63it/s]Epoch: 6, train for the 187-th batch, train loss: 0.633941650390625:  78%|█████████▍  | 186/237 [01:48<00:29,  1.71it/s]Epoch: 6, train for the 187-th batch, train loss: 0.633941650390625:  79%|█████████▍  | 187/237 [01:48<00:29,  1.71it/s]evaluate for the 22-th batch, evaluate loss: 0.5178046822547913:  46%|████████▏         | 21/46 [00:02<00:02,  9.63it/s]evaluate for the 22-th batch, evaluate loss: 0.5178046822547913:  48%|████████▌         | 22/46 [00:02<00:02,  9.63it/s]Epoch: 11, train for the 84-th batch, train loss: 0.13721482455730438:  70%|███████▋   | 83/119 [00:50<00:21,  1.65it/s]Epoch: 11, train for the 84-th batch, train loss: 0.13721482455730438:  71%|███████▊   | 84/119 [00:50<00:21,  1.64it/s]evaluate for the 23-th batch, evaluate loss: 0.47300902009010315:  48%|████████▏        | 22/46 [00:02<00:02,  9.63it/s]evaluate for the 23-th batch, evaluate loss: 0.47300902009010315:  50%|████████▌        | 23/46 [00:02<00:02,  9.60it/s]Epoch: 10, train for the 33-th batch, train loss: 0.47733503580093384:  22%|██▍        | 32/146 [00:19<01:09,  1.64it/s]Epoch: 10, train for the 33-th batch, train loss: 0.47733503580093384:  23%|██▍        | 33/146 [00:19<01:08,  1.64it/s]evaluate for the 24-th batch, evaluate loss: 0.4766068160533905:  50%|█████████         | 23/46 [00:02<00:02,  9.60it/s]evaluate for the 24-th batch, evaluate loss: 0.4766068160533905:  52%|█████████▍        | 24/46 [00:02<00:02,  9.59it/s]Epoch: 4, train for the 187-th batch, train loss: 0.3715388774871826:  49%|█████▎     | 186/383 [01:49<01:55,  1.71it/s]Epoch: 4, train for the 187-th batch, train loss: 0.3715388774871826:  49%|█████▎     | 187/383 [01:49<01:54,  1.71it/s]evaluate for the 25-th batch, evaluate loss: 0.5378323197364807:  52%|█████████▍        | 24/46 [00:02<00:02,  9.59it/s]evaluate for the 25-th batch, evaluate loss: 0.5378323197364807:  54%|█████████▊        | 25/46 [00:02<00:02,  9.60it/s]evaluate for the 26-th batch, evaluate loss: 0.552744448184967:  54%|██████████▎        | 25/46 [00:02<00:02,  9.60it/s]evaluate for the 26-th batch, evaluate loss: 0.552744448184967:  57%|██████████▋        | 26/46 [00:02<00:02,  9.58it/s]evaluate for the 27-th batch, evaluate loss: 0.4873844087123871:  57%|██████████▏       | 26/46 [00:02<00:02,  9.58it/s]evaluate for the 27-th batch, evaluate loss: 0.4873844087123871:  59%|██████████▌       | 27/46 [00:02<00:01,  9.57it/s]Epoch: 6, train for the 188-th batch, train loss: 0.6289018988609314:  79%|████████▋  | 187/237 [01:49<00:29,  1.71it/s]Epoch: 6, train for the 188-th batch, train loss: 0.6289018988609314:  79%|████████▋  | 188/237 [01:49<00:28,  1.70it/s]evaluate for the 28-th batch, evaluate loss: 0.5195813775062561:  59%|██████████▌       | 27/46 [00:02<00:01,  9.57it/s]evaluate for the 28-th batch, evaluate loss: 0.5195813775062561:  61%|██████████▉       | 28/46 [00:02<00:01,  9.57it/s]Epoch: 11, train for the 85-th batch, train loss: 0.15781889855861664:  71%|███████▊   | 84/119 [00:50<00:21,  1.64it/s]Epoch: 11, train for the 85-th batch, train loss: 0.15781889855861664:  71%|███████▊   | 85/119 [00:50<00:20,  1.64it/s]evaluate for the 29-th batch, evaluate loss: 0.49000343680381775:  61%|██████████▎      | 28/46 [00:03<00:01,  9.57it/s]evaluate for the 29-th batch, evaluate loss: 0.49000343680381775:  63%|██████████▋      | 29/46 [00:03<00:01,  9.58it/s]Epoch: 10, train for the 34-th batch, train loss: 0.4698651432991028:  23%|██▋         | 33/146 [00:19<01:08,  1.64it/s]Epoch: 10, train for the 34-th batch, train loss: 0.4698651432991028:  23%|██▊         | 34/146 [00:19<01:08,  1.64it/s]evaluate for the 30-th batch, evaluate loss: 0.49148014187812805:  63%|██████████▋      | 29/46 [00:03<00:01,  9.58it/s]evaluate for the 30-th batch, evaluate loss: 0.49148014187812805:  65%|███████████      | 30/46 [00:03<00:01,  9.57it/s]Epoch: 4, train for the 188-th batch, train loss: 0.440182626247406:  49%|█████▊      | 187/383 [01:50<01:54,  1.71it/s]Epoch: 4, train for the 188-th batch, train loss: 0.440182626247406:  49%|█████▉      | 188/383 [01:50<01:54,  1.70it/s]evaluate for the 31-th batch, evaluate loss: 0.5184497833251953:  65%|███████████▋      | 30/46 [00:03<00:01,  9.57it/s]evaluate for the 31-th batch, evaluate loss: 0.5184497833251953:  67%|████████████▏     | 31/46 [00:03<00:01,  9.55it/s]evaluate for the 32-th batch, evaluate loss: 0.4768366813659668:  67%|████████████▏     | 31/46 [00:03<00:01,  9.55it/s]evaluate for the 32-th batch, evaluate loss: 0.4768366813659668:  70%|████████████▌     | 32/46 [00:03<00:01,  9.52it/s]evaluate for the 33-th batch, evaluate loss: 0.4944269359111786:  70%|████████████▌     | 32/46 [00:03<00:01,  9.52it/s]evaluate for the 33-th batch, evaluate loss: 0.4944269359111786:  72%|████████████▉     | 33/46 [00:03<00:01,  9.52it/s]Epoch: 6, train for the 189-th batch, train loss: 0.6488888263702393:  79%|████████▋  | 188/237 [01:49<00:28,  1.70it/s]Epoch: 6, train for the 189-th batch, train loss: 0.6488888263702393:  80%|████████▊  | 189/237 [01:49<00:28,  1.70it/s]evaluate for the 34-th batch, evaluate loss: 0.48189041018486023:  72%|████████████▏    | 33/46 [00:03<00:01,  9.52it/s]evaluate for the 34-th batch, evaluate loss: 0.48189041018486023:  74%|████████████▌    | 34/46 [00:03<00:01,  9.55it/s]Epoch: 11, train for the 86-th batch, train loss: 0.18365779519081116:  71%|███████▊   | 85/119 [00:51<00:20,  1.64it/s]Epoch: 11, train for the 86-th batch, train loss: 0.18365779519081116:  72%|███████▉   | 86/119 [00:51<00:20,  1.64it/s]evaluate for the 35-th batch, evaluate loss: 0.4840362071990967:  74%|█████████████▎    | 34/46 [00:03<00:01,  9.55it/s]evaluate for the 35-th batch, evaluate loss: 0.4840362071990967:  76%|█████████████▋    | 35/46 [00:03<00:01,  9.55it/s]Epoch: 10, train for the 35-th batch, train loss: 0.46288469433784485:  23%|██▌        | 34/146 [00:20<01:08,  1.64it/s]Epoch: 10, train for the 35-th batch, train loss: 0.46288469433784485:  24%|██▋        | 35/146 [00:20<01:07,  1.63it/s]Epoch: 4, train for the 189-th batch, train loss: 0.38352108001708984:  49%|████▉     | 188/383 [01:51<01:54,  1.70it/s]Epoch: 4, train for the 189-th batch, train loss: 0.38352108001708984:  49%|████▉     | 189/383 [01:51<01:54,  1.70it/s]evaluate for the 36-th batch, evaluate loss: 0.4648993909358978:  76%|█████████████▋    | 35/46 [00:03<00:01,  9.55it/s]evaluate for the 36-th batch, evaluate loss: 0.4648993909358978:  78%|██████████████    | 36/46 [00:03<00:01,  9.57it/s]evaluate for the 37-th batch, evaluate loss: 0.5049059391021729:  78%|██████████████    | 36/46 [00:03<00:01,  9.57it/s]evaluate for the 37-th batch, evaluate loss: 0.5049059391021729:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.54it/s]evaluate for the 38-th batch, evaluate loss: 0.5303979516029358:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.54it/s]evaluate for the 38-th batch, evaluate loss: 0.5303979516029358:  83%|██████████████▊   | 38/46 [00:03<00:00,  9.53it/s]Epoch: 6, train for the 190-th batch, train loss: 0.6235107779502869:  80%|████████▊  | 189/237 [01:50<00:28,  1.70it/s]Epoch: 6, train for the 190-th batch, train loss: 0.6235107779502869:  80%|████████▊  | 190/237 [01:50<00:27,  1.70it/s]evaluate for the 39-th batch, evaluate loss: 0.5296517014503479:  83%|██████████████▊   | 38/46 [00:04<00:00,  9.53it/s]evaluate for the 39-th batch, evaluate loss: 0.5296517014503479:  85%|███████████████▎  | 39/46 [00:04<00:00,  9.55it/s]Epoch: 11, train for the 87-th batch, train loss: 0.17986300587654114:  72%|███████▉   | 86/119 [00:51<00:20,  1.64it/s]Epoch: 11, train for the 87-th batch, train loss: 0.17986300587654114:  73%|████████   | 87/119 [00:51<00:19,  1.63it/s]evaluate for the 40-th batch, evaluate loss: 0.465559184551239:  85%|████████████████   | 39/46 [00:04<00:00,  9.55it/s]evaluate for the 40-th batch, evaluate loss: 0.465559184551239:  87%|████████████████▌  | 40/46 [00:04<00:00,  9.55it/s]evaluate for the 41-th batch, evaluate loss: 0.4799535274505615:  87%|███████████████▋  | 40/46 [00:04<00:00,  9.55it/s]evaluate for the 41-th batch, evaluate loss: 0.4799535274505615:  89%|████████████████  | 41/46 [00:04<00:00,  9.54it/s]Epoch: 10, train for the 36-th batch, train loss: 0.4224480092525482:  24%|██▉         | 35/146 [00:21<01:07,  1.63it/s]Epoch: 10, train for the 36-th batch, train loss: 0.4224480092525482:  25%|██▉         | 36/146 [00:21<01:07,  1.63it/s]Epoch: 4, train for the 190-th batch, train loss: 0.41684067249298096:  49%|████▉     | 189/383 [01:51<01:54,  1.70it/s]Epoch: 4, train for the 190-th batch, train loss: 0.41684067249298096:  50%|████▉     | 190/383 [01:51<01:53,  1.70it/s]evaluate for the 42-th batch, evaluate loss: 0.4687651991844177:  89%|████████████████  | 41/46 [00:04<00:00,  9.54it/s]evaluate for the 42-th batch, evaluate loss: 0.4687651991844177:  91%|████████████████▍ | 42/46 [00:04<00:00,  9.54it/s]evaluate for the 43-th batch, evaluate loss: 0.5340064764022827:  91%|████████████████▍ | 42/46 [00:04<00:00,  9.54it/s]evaluate for the 43-th batch, evaluate loss: 0.5340064764022827:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.54it/s]evaluate for the 44-th batch, evaluate loss: 0.512288510799408:  93%|█████████████████▊ | 43/46 [00:04<00:00,  9.54it/s]evaluate for the 44-th batch, evaluate loss: 0.512288510799408:  96%|██████████████████▏| 44/46 [00:04<00:00,  9.56it/s]Epoch: 6, train for the 191-th batch, train loss: 0.646571934223175:  80%|█████████▌  | 190/237 [01:50<00:27,  1.70it/s]Epoch: 6, train for the 191-th batch, train loss: 0.646571934223175:  81%|█████████▋  | 191/237 [01:50<00:27,  1.70it/s]evaluate for the 45-th batch, evaluate loss: 0.49379396438598633:  96%|████████████████▎| 44/46 [00:04<00:00,  9.56it/s]evaluate for the 45-th batch, evaluate loss: 0.49379396438598633:  98%|████████████████▋| 45/46 [00:04<00:00,  9.57it/s]Epoch: 11, train for the 88-th batch, train loss: 0.2252628207206726:  73%|████████▊   | 87/119 [00:52<00:19,  1.63it/s]Epoch: 11, train for the 88-th batch, train loss: 0.2252628207206726:  74%|████████▊   | 88/119 [00:52<00:19,  1.63it/s]evaluate for the 46-th batch, evaluate loss: 0.4992747902870178:  98%|█████████████████▌| 45/46 [00:04<00:00,  9.57it/s]evaluate for the 46-th batch, evaluate loss: 0.4992747902870178: 100%|██████████████████| 46/46 [00:04<00:00,  9.60it/s]
  0%|                                                                                            | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6364325881004333:   0%|                            | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6364325881004333:   4%|▊                   | 1/25 [00:00<00:02,  9.07it/s]Epoch: 4, train for the 191-th batch, train loss: 0.3379148542881012:  50%|█████▍     | 190/383 [01:52<01:53,  1.70it/s]Epoch: 4, train for the 191-th batch, train loss: 0.3379148542881012:  50%|█████▍     | 191/383 [01:52<01:52,  1.70it/s]Epoch: 10, train for the 37-th batch, train loss: 0.49974319338798523:  25%|██▋        | 36/146 [00:21<01:07,  1.63it/s]Epoch: 10, train for the 37-th batch, train loss: 0.49974319338798523:  25%|██▊        | 37/146 [00:21<01:06,  1.63it/s]evaluate for the 2-th batch, evaluate loss: 0.6490280032157898:   4%|▊                   | 1/25 [00:00<00:02,  9.07it/s]evaluate for the 2-th batch, evaluate loss: 0.6490280032157898:   8%|█▌                  | 2/25 [00:00<00:02,  9.12it/s]evaluate for the 3-th batch, evaluate loss: 0.6949694752693176:   8%|█▌                  | 2/25 [00:00<00:02,  9.12it/s]evaluate for the 3-th batch, evaluate loss: 0.6949694752693176:  12%|██▍                 | 3/25 [00:00<00:02,  9.12it/s]Epoch: 6, train for the 192-th batch, train loss: 0.6473290920257568:  81%|████████▊  | 191/237 [01:51<00:27,  1.70it/s]Epoch: 6, train for the 192-th batch, train loss: 0.6473290920257568:  81%|████████▉  | 192/237 [01:51<00:26,  1.70it/s]evaluate for the 4-th batch, evaluate loss: 0.6727777719497681:  12%|██▍                 | 3/25 [00:00<00:02,  9.12it/s]evaluate for the 4-th batch, evaluate loss: 0.6727777719497681:  16%|███▏                | 4/25 [00:00<00:02,  9.13it/s]evaluate for the 5-th batch, evaluate loss: 0.6732834577560425:  16%|███▏                | 4/25 [00:00<00:02,  9.13it/s]evaluate for the 5-th batch, evaluate loss: 0.6732834577560425:  20%|████                | 5/25 [00:00<00:02,  9.15it/s]Epoch: 11, train for the 89-th batch, train loss: 0.19537955522537231:  74%|████████▏  | 88/119 [00:53<00:19,  1.63it/s]Epoch: 11, train for the 89-th batch, train loss: 0.19537955522537231:  75%|████████▏  | 89/119 [00:53<00:18,  1.63it/s]evaluate for the 6-th batch, evaluate loss: 0.716420590877533:  20%|████▏                | 5/25 [00:00<00:02,  9.15it/s]evaluate for the 6-th batch, evaluate loss: 0.716420590877533:  24%|█████                | 6/25 [00:00<00:02,  9.17it/s]Epoch: 4, train for the 192-th batch, train loss: 0.40814733505249023:  50%|████▉     | 191/383 [01:52<01:52,  1.70it/s]Epoch: 4, train for the 192-th batch, train loss: 0.40814733505249023:  50%|█████     | 192/383 [01:52<01:52,  1.70it/s]Epoch: 10, train for the 38-th batch, train loss: 0.44594085216522217:  25%|██▊        | 37/146 [00:22<01:06,  1.63it/s]Epoch: 10, train for the 38-th batch, train loss: 0.44594085216522217:  26%|██▊        | 38/146 [00:22<01:06,  1.63it/s]evaluate for the 7-th batch, evaluate loss: 0.7376177906990051:  24%|████▊               | 6/25 [00:00<00:02,  9.17it/s]evaluate for the 7-th batch, evaluate loss: 0.7376177906990051:  28%|█████▌              | 7/25 [00:00<00:01,  9.15it/s]evaluate for the 8-th batch, evaluate loss: 0.7172879576683044:  28%|█████▌              | 7/25 [00:00<00:01,  9.15it/s]evaluate for the 8-th batch, evaluate loss: 0.7172879576683044:  32%|██████▍             | 8/25 [00:00<00:01,  9.08it/s]evaluate for the 9-th batch, evaluate loss: 0.697083592414856:  32%|██████▋              | 8/25 [00:00<00:01,  9.08it/s]evaluate for the 9-th batch, evaluate loss: 0.697083592414856:  36%|███████▌             | 9/25 [00:00<00:01,  9.12it/s]Epoch: 6, train for the 193-th batch, train loss: 0.6663707494735718:  81%|████████▉  | 192/237 [01:52<00:26,  1.70it/s]Epoch: 6, train for the 193-th batch, train loss: 0.6663707494735718:  81%|████████▉  | 193/237 [01:52<00:25,  1.70it/s]evaluate for the 10-th batch, evaluate loss: 0.7295812964439392:  36%|██████▊            | 9/25 [00:01<00:01,  9.12it/s]evaluate for the 10-th batch, evaluate loss: 0.7295812964439392:  40%|███████▏          | 10/25 [00:01<00:01,  9.15it/s]evaluate for the 11-th batch, evaluate loss: 0.7327248454093933:  40%|███████▏          | 10/25 [00:01<00:01,  9.15it/s]evaluate for the 11-th batch, evaluate loss: 0.7327248454093933:  44%|███████▉          | 11/25 [00:01<00:01,  9.16it/s]Epoch: 11, train for the 90-th batch, train loss: 0.1886119395494461:  75%|████████▉   | 89/119 [00:53<00:18,  1.63it/s]Epoch: 11, train for the 90-th batch, train loss: 0.1886119395494461:  76%|█████████   | 90/119 [00:53<00:17,  1.63it/s]Epoch: 4, train for the 193-th batch, train loss: 0.41259029507637024:  50%|█████     | 192/383 [01:53<01:52,  1.70it/s]Epoch: 4, train for the 193-th batch, train loss: 0.41259029507637024:  50%|█████     | 193/383 [01:53<01:51,  1.70it/s]evaluate for the 12-th batch, evaluate loss: 0.6975837349891663:  44%|███████▉          | 11/25 [00:01<00:01,  9.16it/s]evaluate for the 12-th batch, evaluate loss: 0.6975837349891663:  48%|████████▋         | 12/25 [00:01<00:01,  9.17it/s]Epoch: 10, train for the 39-th batch, train loss: 0.45566290616989136:  26%|██▊        | 38/146 [00:23<01:06,  1.63it/s]Epoch: 10, train for the 39-th batch, train loss: 0.45566290616989136:  27%|██▉        | 39/146 [00:23<01:05,  1.62it/s]evaluate for the 13-th batch, evaluate loss: 0.6618296504020691:  48%|████████▋         | 12/25 [00:01<00:01,  9.17it/s]evaluate for the 13-th batch, evaluate loss: 0.6618296504020691:  52%|█████████▎        | 13/25 [00:01<00:01,  9.17it/s]evaluate for the 14-th batch, evaluate loss: 0.7506829500198364:  52%|█████████▎        | 13/25 [00:01<00:01,  9.17it/s]evaluate for the 14-th batch, evaluate loss: 0.7506829500198364:  56%|██████████        | 14/25 [00:01<00:01,  9.20it/s]Epoch: 6, train for the 194-th batch, train loss: 0.6235365271568298:  81%|████████▉  | 193/237 [01:52<00:25,  1.70it/s]Epoch: 6, train for the 194-th batch, train loss: 0.6235365271568298:  82%|█████████  | 194/237 [01:52<00:25,  1.70it/s]evaluate for the 15-th batch, evaluate loss: 0.7209382653236389:  56%|██████████        | 14/25 [00:01<00:01,  9.20it/s]evaluate for the 15-th batch, evaluate loss: 0.7209382653236389:  60%|██████████▊       | 15/25 [00:01<00:01,  9.19it/s]evaluate for the 16-th batch, evaluate loss: 0.6716548204421997:  60%|██████████▊       | 15/25 [00:01<00:01,  9.19it/s]evaluate for the 16-th batch, evaluate loss: 0.6716548204421997:  64%|███████████▌      | 16/25 [00:01<00:00,  9.20it/s]Epoch: 11, train for the 91-th batch, train loss: 0.19275504350662231:  76%|████████▎  | 90/119 [00:54<00:17,  1.63it/s]Epoch: 11, train for the 91-th batch, train loss: 0.19275504350662231:  76%|████████▍  | 91/119 [00:54<00:17,  1.62it/s]evaluate for the 17-th batch, evaluate loss: 0.6651493906974792:  64%|███████████▌      | 16/25 [00:01<00:00,  9.20it/s]evaluate for the 17-th batch, evaluate loss: 0.6651493906974792:  68%|████████████▏     | 17/25 [00:01<00:00,  9.17it/s]Epoch: 4, train for the 194-th batch, train loss: 0.3647666871547699:  50%|█████▌     | 193/383 [01:54<01:51,  1.70it/s]Epoch: 4, train for the 194-th batch, train loss: 0.3647666871547699:  51%|█████▌     | 194/383 [01:54<01:50,  1.70it/s]evaluate for the 18-th batch, evaluate loss: 0.632983922958374:  68%|████████████▉      | 17/25 [00:01<00:00,  9.17it/s]evaluate for the 18-th batch, evaluate loss: 0.632983922958374:  72%|█████████████▋     | 18/25 [00:01<00:00,  9.16it/s]Epoch: 10, train for the 40-th batch, train loss: 0.4796944260597229:  27%|███▏        | 39/146 [00:23<01:05,  1.62it/s]Epoch: 10, train for the 40-th batch, train loss: 0.4796944260597229:  27%|███▎        | 40/146 [00:23<01:05,  1.62it/s]evaluate for the 19-th batch, evaluate loss: 0.5942418575286865:  72%|████████████▉     | 18/25 [00:02<00:00,  9.16it/s]evaluate for the 19-th batch, evaluate loss: 0.5942418575286865:  76%|█████████████▋    | 19/25 [00:02<00:00,  9.17it/s]Epoch: 6, train for the 195-th batch, train loss: 0.6212108731269836:  82%|█████████  | 194/237 [01:53<00:25,  1.70it/s]Epoch: 6, train for the 195-th batch, train loss: 0.6212108731269836:  82%|█████████  | 195/237 [01:53<00:24,  1.71it/s]evaluate for the 20-th batch, evaluate loss: 0.658698558807373:  76%|██████████████▍    | 19/25 [00:02<00:00,  9.17it/s]evaluate for the 20-th batch, evaluate loss: 0.658698558807373:  80%|███████████████▏   | 20/25 [00:02<00:00,  9.17it/s]evaluate for the 21-th batch, evaluate loss: 0.73047935962677:  80%|████████████████    | 20/25 [00:02<00:00,  9.17it/s]evaluate for the 21-th batch, evaluate loss: 0.73047935962677:  84%|████████████████▊   | 21/25 [00:02<00:00,  9.17it/s]evaluate for the 22-th batch, evaluate loss: 0.5987091064453125:  84%|███████████████   | 21/25 [00:02<00:00,  9.17it/s]evaluate for the 22-th batch, evaluate loss: 0.5987091064453125:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.18it/s]Epoch: 11, train for the 92-th batch, train loss: 0.20124775171279907:  76%|████████▍  | 91/119 [00:54<00:17,  1.62it/s]Epoch: 11, train for the 92-th batch, train loss: 0.20124775171279907:  77%|████████▌  | 92/119 [00:54<00:16,  1.62it/s]Epoch: 4, train for the 195-th batch, train loss: 0.42312246561050415:  51%|█████     | 194/383 [01:54<01:50,  1.70it/s]Epoch: 4, train for the 195-th batch, train loss: 0.42312246561050415:  51%|█████     | 195/383 [01:54<01:50,  1.70it/s]evaluate for the 23-th batch, evaluate loss: 0.6624064445495605:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.18it/s]evaluate for the 23-th batch, evaluate loss: 0.6624064445495605:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.16it/s]Epoch: 10, train for the 41-th batch, train loss: 0.4369814991950989:  27%|███▎        | 40/146 [00:24<01:05,  1.62it/s]Epoch: 10, train for the 41-th batch, train loss: 0.4369814991950989:  28%|███▎        | 41/146 [00:24<01:04,  1.62it/s]evaluate for the 24-th batch, evaluate loss: 0.6621733903884888:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.16it/s]evaluate for the 24-th batch, evaluate loss: 0.6621733903884888:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.16it/s]evaluate for the 25-th batch, evaluate loss: 0.7051428556442261:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.16it/s]evaluate for the 25-th batch, evaluate loss: 0.7051428556442261: 100%|██████████████████| 25/25 [00:02<00:00,  9.23it/s]
INFO:root:Epoch: 20, learning rate: 0.0001, train loss: 0.5568
INFO:root:train average_precision, 0.8258
INFO:root:train roc_auc, 0.7946
INFO:root:validate loss: 0.5057
INFO:root:validate average_precision, 0.8446
INFO:root:validate roc_auc, 0.8070
INFO:root:new node validate loss: 0.6828
INFO:root:new node validate first_1_average_precision, 0.5995
INFO:root:new node validate first_1_roc_auc, 0.5456
INFO:root:new node validate first_3_average_precision, 0.6801
INFO:root:new node validate first_3_roc_auc, 0.6378
INFO:root:new node validate first_10_average_precision, 0.7473
INFO:root:new node validate first_10_roc_auc, 0.7118
INFO:root:new node validate average_precision, 0.7107
INFO:root:new node validate roc_auc, 0.6617
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                            | 0/46 [00:00<?, ?it/s]Epoch: 6, train for the 196-th batch, train loss: 0.6471179127693176:  82%|█████████  | 195/237 [01:53<00:24,  1.71it/s]Epoch: 6, train for the 196-th batch, train loss: 0.6471179127693176:  83%|█████████  | 196/237 [01:53<00:24,  1.70it/s]evaluate for the 1-th batch, evaluate loss: 0.5135647058486938:   0%|                            | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.5135647058486938:   2%|▍                   | 1/46 [00:00<00:04,  9.68it/s]evaluate for the 2-th batch, evaluate loss: 0.5313181281089783:   2%|▍                   | 1/46 [00:00<00:04,  9.68it/s]evaluate for the 2-th batch, evaluate loss: 0.5313181281089783:   4%|▊                   | 2/46 [00:00<00:04,  9.70it/s]evaluate for the 3-th batch, evaluate loss: 0.5500279068946838:   4%|▊                   | 2/46 [00:00<00:04,  9.70it/s]evaluate for the 3-th batch, evaluate loss: 0.5500279068946838:   7%|█▎                  | 3/46 [00:00<00:04,  9.73it/s]Epoch: 4, train for the 196-th batch, train loss: 0.44327354431152344:  51%|█████     | 195/383 [01:55<01:50,  1.70it/s]Epoch: 4, train for the 196-th batch, train loss: 0.44327354431152344:  51%|█████     | 196/383 [01:55<01:49,  1.71it/s]Epoch: 11, train for the 93-th batch, train loss: 0.15056483447551727:  77%|████████▌  | 92/119 [00:55<00:16,  1.62it/s]Epoch: 11, train for the 93-th batch, train loss: 0.15056483447551727:  78%|████████▌  | 93/119 [00:55<00:16,  1.62it/s]evaluate for the 4-th batch, evaluate loss: 0.5486745834350586:   7%|█▎                  | 3/46 [00:00<00:04,  9.73it/s]evaluate for the 4-th batch, evaluate loss: 0.5486745834350586:   9%|█▋                  | 4/46 [00:00<00:04,  9.71it/s]Epoch: 10, train for the 42-th batch, train loss: 0.4712170958518982:  28%|███▎        | 41/146 [00:24<01:04,  1.62it/s]Epoch: 10, train for the 42-th batch, train loss: 0.4712170958518982:  29%|███▍        | 42/146 [00:24<01:03,  1.63it/s]evaluate for the 5-th batch, evaluate loss: 0.5167738795280457:   9%|█▋                  | 4/46 [00:00<00:04,  9.71it/s]evaluate for the 5-th batch, evaluate loss: 0.5167738795280457:  11%|██▏                 | 5/46 [00:00<00:04,  9.69it/s]evaluate for the 6-th batch, evaluate loss: 0.47377830743789673:  11%|██                 | 5/46 [00:00<00:04,  9.69it/s]evaluate for the 6-th batch, evaluate loss: 0.47377830743789673:  13%|██▍                | 6/46 [00:00<00:04,  9.69it/s]Epoch: 6, train for the 197-th batch, train loss: 0.6298895478248596:  83%|█████████  | 196/237 [01:54<00:24,  1.70it/s]Epoch: 6, train for the 197-th batch, train loss: 0.6298895478248596:  83%|█████████▏ | 197/237 [01:54<00:23,  1.71it/s]evaluate for the 7-th batch, evaluate loss: 0.5413101315498352:  13%|██▌                 | 6/46 [00:00<00:04,  9.69it/s]evaluate for the 7-th batch, evaluate loss: 0.5413101315498352:  15%|███                 | 7/46 [00:00<00:04,  9.67it/s]evaluate for the 8-th batch, evaluate loss: 0.5133025050163269:  15%|███                 | 7/46 [00:00<00:04,  9.67it/s]evaluate for the 8-th batch, evaluate loss: 0.5133025050163269:  17%|███▍                | 8/46 [00:00<00:03,  9.66it/s]evaluate for the 9-th batch, evaluate loss: 0.5166119933128357:  17%|███▍                | 8/46 [00:00<00:03,  9.66it/s]evaluate for the 9-th batch, evaluate loss: 0.5166119933128357:  20%|███▉                | 9/46 [00:00<00:03,  9.65it/s]Epoch: 4, train for the 197-th batch, train loss: 0.4576079547405243:  51%|█████▋     | 196/383 [01:55<01:49,  1.71it/s]Epoch: 4, train for the 197-th batch, train loss: 0.4576079547405243:  51%|█████▋     | 197/383 [01:55<01:49,  1.70it/s]Epoch: 11, train for the 94-th batch, train loss: 0.16820695996284485:  78%|████████▌  | 93/119 [00:56<00:16,  1.62it/s]Epoch: 11, train for the 94-th batch, train loss: 0.16820695996284485:  79%|████████▋  | 94/119 [00:56<00:15,  1.63it/s]evaluate for the 10-th batch, evaluate loss: 0.5215856432914734:  20%|███▋               | 9/46 [00:01<00:03,  9.65it/s]evaluate for the 10-th batch, evaluate loss: 0.5215856432914734:  22%|███▉              | 10/46 [00:01<00:03,  9.65it/s]Epoch: 10, train for the 43-th batch, train loss: 0.4665672183036804:  29%|███▍        | 42/146 [00:25<01:03,  1.63it/s]Epoch: 10, train for the 43-th batch, train loss: 0.4665672183036804:  29%|███▌        | 43/146 [00:25<01:03,  1.63it/s]evaluate for the 11-th batch, evaluate loss: 0.5219903588294983:  22%|███▉              | 10/46 [00:01<00:03,  9.65it/s]evaluate for the 11-th batch, evaluate loss: 0.5219903588294983:  24%|████▎             | 11/46 [00:01<00:03,  9.69it/s]Epoch: 6, train for the 198-th batch, train loss: 0.6158943176269531:  83%|█████████▏ | 197/237 [01:54<00:23,  1.71it/s]Epoch: 6, train for the 198-th batch, train loss: 0.6158943176269531:  84%|█████████▏ | 198/237 [01:54<00:22,  1.70it/s]evaluate for the 12-th batch, evaluate loss: 0.4902002215385437:  24%|████▎             | 11/46 [00:01<00:03,  9.69it/s]evaluate for the 12-th batch, evaluate loss: 0.4902002215385437:  26%|████▋             | 12/46 [00:01<00:03,  9.13it/s]evaluate for the 13-th batch, evaluate loss: 0.4848502278327942:  26%|████▋             | 12/46 [00:01<00:03,  9.13it/s]evaluate for the 13-th batch, evaluate loss: 0.4848502278327942:  28%|█████             | 13/46 [00:01<00:03,  9.29it/s]evaluate for the 14-th batch, evaluate loss: 0.4832141101360321:  28%|█████             | 13/46 [00:01<00:03,  9.29it/s]evaluate for the 14-th batch, evaluate loss: 0.4832141101360321:  30%|█████▍            | 14/46 [00:01<00:03,  9.39it/s]Epoch: 4, train for the 198-th batch, train loss: 0.3528810739517212:  51%|█████▋     | 197/383 [01:56<01:49,  1.70it/s]Epoch: 4, train for the 198-th batch, train loss: 0.3528810739517212:  52%|█████▋     | 198/383 [01:56<01:49,  1.69it/s]evaluate for the 15-th batch, evaluate loss: 0.5150839686393738:  30%|█████▍            | 14/46 [00:01<00:03,  9.39it/s]evaluate for the 15-th batch, evaluate loss: 0.5150839686393738:  33%|█████▊            | 15/46 [00:01<00:03,  9.48it/s]Epoch: 11, train for the 95-th batch, train loss: 0.16415397822856903:  79%|████████▋  | 94/119 [00:56<00:15,  1.63it/s]Epoch: 11, train for the 95-th batch, train loss: 0.16415397822856903:  80%|████████▊  | 95/119 [00:56<00:14,  1.62it/s]evaluate for the 16-th batch, evaluate loss: 0.49907025694847107:  33%|█████▌           | 15/46 [00:01<00:03,  9.48it/s]evaluate for the 16-th batch, evaluate loss: 0.49907025694847107:  35%|█████▉           | 16/46 [00:01<00:03,  9.53it/s]Epoch: 10, train for the 44-th batch, train loss: 0.4547082483768463:  29%|███▌        | 43/146 [00:26<01:03,  1.63it/s]Epoch: 10, train for the 44-th batch, train loss: 0.4547082483768463:  30%|███▌        | 44/146 [00:26<01:02,  1.62it/s]evaluate for the 17-th batch, evaluate loss: 0.44855040311813354:  35%|█████▉           | 16/46 [00:01<00:03,  9.53it/s]evaluate for the 17-th batch, evaluate loss: 0.44855040311813354:  37%|██████▎          | 17/46 [00:01<00:03,  9.59it/s]Epoch: 6, train for the 199-th batch, train loss: 0.5936998724937439:  84%|█████████▏ | 198/237 [01:55<00:22,  1.70it/s]Epoch: 6, train for the 199-th batch, train loss: 0.5936998724937439:  84%|█████████▏ | 199/237 [01:55<00:22,  1.69it/s]evaluate for the 18-th batch, evaluate loss: 0.471648633480072:  37%|███████            | 17/46 [00:01<00:03,  9.59it/s]evaluate for the 18-th batch, evaluate loss: 0.471648633480072:  39%|███████▍           | 18/46 [00:01<00:02,  9.62it/s]evaluate for the 19-th batch, evaluate loss: 0.4913069009780884:  39%|███████           | 18/46 [00:01<00:02,  9.62it/s]evaluate for the 19-th batch, evaluate loss: 0.4913069009780884:  41%|███████▍          | 19/46 [00:01<00:02,  9.63it/s]evaluate for the 20-th batch, evaluate loss: 0.5281083583831787:  41%|███████▍          | 19/46 [00:02<00:02,  9.63it/s]evaluate for the 20-th batch, evaluate loss: 0.5281083583831787:  43%|███████▊          | 20/46 [00:02<00:02,  9.63it/s]Epoch: 4, train for the 199-th batch, train loss: 0.3472287654876709:  52%|█████▋     | 198/383 [01:57<01:49,  1.69it/s]Epoch: 4, train for the 199-th batch, train loss: 0.3472287654876709:  52%|█████▋     | 199/383 [01:57<01:48,  1.70it/s]evaluate for the 21-th batch, evaluate loss: 0.4997331500053406:  43%|███████▊          | 20/46 [00:02<00:02,  9.63it/s]evaluate for the 21-th batch, evaluate loss: 0.4997331500053406:  46%|████████▏         | 21/46 [00:02<00:02,  9.62it/s]Epoch: 11, train for the 96-th batch, train loss: 0.14411382377147675:  80%|████████▊  | 95/119 [00:57<00:14,  1.62it/s]Epoch: 11, train for the 96-th batch, train loss: 0.14411382377147675:  81%|████████▊  | 96/119 [00:57<00:14,  1.62it/s]evaluate for the 22-th batch, evaluate loss: 0.4927825927734375:  46%|████████▏         | 21/46 [00:02<00:02,  9.62it/s]evaluate for the 22-th batch, evaluate loss: 0.4927825927734375:  48%|████████▌         | 22/46 [00:02<00:02,  9.63it/s]Epoch: 10, train for the 45-th batch, train loss: 0.4458656311035156:  30%|███▌        | 44/146 [00:26<01:02,  1.62it/s]Epoch: 10, train for the 45-th batch, train loss: 0.4458656311035156:  31%|███▋        | 45/146 [00:26<01:02,  1.62it/s]evaluate for the 23-th batch, evaluate loss: 0.5096842050552368:  48%|████████▌         | 22/46 [00:02<00:02,  9.63it/s]evaluate for the 23-th batch, evaluate loss: 0.5096842050552368:  50%|█████████         | 23/46 [00:02<00:02,  9.63it/s]Epoch: 6, train for the 200-th batch, train loss: 0.6146885752677917:  84%|█████████▏ | 199/237 [01:56<00:22,  1.69it/s]Epoch: 6, train for the 200-th batch, train loss: 0.6146885752677917:  84%|█████████▎ | 200/237 [01:56<00:21,  1.70it/s]evaluate for the 24-th batch, evaluate loss: 0.5117866396903992:  50%|█████████         | 23/46 [00:02<00:02,  9.63it/s]evaluate for the 24-th batch, evaluate loss: 0.5117866396903992:  52%|█████████▍        | 24/46 [00:02<00:02,  9.62it/s]evaluate for the 25-th batch, evaluate loss: 0.5187239050865173:  52%|█████████▍        | 24/46 [00:02<00:02,  9.62it/s]evaluate for the 25-th batch, evaluate loss: 0.5187239050865173:  54%|█████████▊        | 25/46 [00:02<00:02,  9.58it/s]Epoch: 4, train for the 200-th batch, train loss: 0.41411131620407104:  52%|█████▏    | 199/383 [01:57<01:48,  1.70it/s]Epoch: 4, train for the 200-th batch, train loss: 0.41411131620407104:  52%|█████▏    | 200/383 [01:57<01:47,  1.70it/s]evaluate for the 26-th batch, evaluate loss: 0.5451736450195312:  54%|█████████▊        | 25/46 [00:02<00:02,  9.58it/s]evaluate for the 26-th batch, evaluate loss: 0.5451736450195312:  57%|██████████▏       | 26/46 [00:02<00:02,  9.59it/s]evaluate for the 27-th batch, evaluate loss: 0.4873003363609314:  57%|██████████▏       | 26/46 [00:02<00:02,  9.59it/s]evaluate for the 27-th batch, evaluate loss: 0.4873003363609314:  59%|██████████▌       | 27/46 [00:02<00:01,  9.59it/s]Epoch: 11, train for the 97-th batch, train loss: 0.1686454713344574:  81%|█████████▋  | 96/119 [00:58<00:14,  1.62it/s]Epoch: 11, train for the 97-th batch, train loss: 0.1686454713344574:  82%|█████████▊  | 97/119 [00:58<00:13,  1.63it/s]evaluate for the 28-th batch, evaluate loss: 0.4660674035549164:  59%|██████████▌       | 27/46 [00:02<00:01,  9.59it/s]evaluate for the 28-th batch, evaluate loss: 0.4660674035549164:  61%|██████████▉       | 28/46 [00:02<00:01,  9.58it/s]Epoch: 10, train for the 46-th batch, train loss: 0.4843747615814209:  31%|███▋        | 45/146 [00:27<01:02,  1.62it/s]Epoch: 10, train for the 46-th batch, train loss: 0.4843747615814209:  32%|███▊        | 46/146 [00:27<01:01,  1.62it/s]Epoch: 6, train for the 201-th batch, train loss: 0.6482691764831543:  84%|█████████▎ | 200/237 [01:56<00:21,  1.70it/s]Epoch: 6, train for the 201-th batch, train loss: 0.6482691764831543:  85%|█████████▎ | 201/237 [01:56<00:21,  1.70it/s]evaluate for the 29-th batch, evaluate loss: 0.5317612290382385:  61%|██████████▉       | 28/46 [00:03<00:01,  9.58it/s]evaluate for the 29-th batch, evaluate loss: 0.5317612290382385:  63%|███████████▎      | 29/46 [00:03<00:01,  9.59it/s]evaluate for the 30-th batch, evaluate loss: 0.4861794412136078:  63%|███████████▎      | 29/46 [00:03<00:01,  9.59it/s]evaluate for the 30-th batch, evaluate loss: 0.4861794412136078:  65%|███████████▋      | 30/46 [00:03<00:01,  9.60it/s]evaluate for the 31-th batch, evaluate loss: 0.536575436592102:  65%|████████████▍      | 30/46 [00:03<00:01,  9.60it/s]evaluate for the 31-th batch, evaluate loss: 0.536575436592102:  67%|████████████▊      | 31/46 [00:03<00:01,  9.65it/s]Epoch: 4, train for the 201-th batch, train loss: 0.4635622203350067:  52%|█████▋     | 200/383 [01:58<01:47,  1.70it/s]Epoch: 4, train for the 201-th batch, train loss: 0.4635622203350067:  52%|█████▊     | 201/383 [01:58<01:47,  1.70it/s]evaluate for the 32-th batch, evaluate loss: 0.5615207552909851:  67%|████████████▏     | 31/46 [00:03<00:01,  9.65it/s]evaluate for the 32-th batch, evaluate loss: 0.5615207552909851:  70%|████████████▌     | 32/46 [00:03<00:01,  9.65it/s]Epoch: 11, train for the 98-th batch, train loss: 0.1206946149468422:  82%|█████████▊  | 97/119 [00:58<00:13,  1.63it/s]Epoch: 11, train for the 98-th batch, train loss: 0.1206946149468422:  82%|█████████▉  | 98/119 [00:58<00:12,  1.62it/s]evaluate for the 33-th batch, evaluate loss: 0.5169632434844971:  70%|████████████▌     | 32/46 [00:03<00:01,  9.65it/s]evaluate for the 33-th batch, evaluate loss: 0.5169632434844971:  72%|████████████▉     | 33/46 [00:03<00:01,  9.67it/s]evaluate for the 34-th batch, evaluate loss: 0.46936866641044617:  72%|████████████▏    | 33/46 [00:03<00:01,  9.67it/s]evaluate for the 34-th batch, evaluate loss: 0.46936866641044617:  74%|████████████▌    | 34/46 [00:03<00:01,  9.69it/s]Epoch: 6, train for the 202-th batch, train loss: 0.5997461676597595:  85%|█████████▎ | 201/237 [01:57<00:21,  1.70it/s]Epoch: 6, train for the 202-th batch, train loss: 0.5997461676597595:  85%|█████████▍ | 202/237 [01:57<00:20,  1.70it/s]Epoch: 10, train for the 47-th batch, train loss: 0.5491507053375244:  32%|███▊        | 46/146 [00:27<01:01,  1.62it/s]Epoch: 10, train for the 47-th batch, train loss: 0.5491507053375244:  32%|███▊        | 47/146 [00:27<01:00,  1.62it/s]evaluate for the 35-th batch, evaluate loss: 0.5337461829185486:  74%|█████████████▎    | 34/46 [00:03<00:01,  9.69it/s]evaluate for the 35-th batch, evaluate loss: 0.5337461829185486:  76%|█████████████▋    | 35/46 [00:03<00:01,  9.68it/s]evaluate for the 36-th batch, evaluate loss: 0.48327574133872986:  76%|████████████▉    | 35/46 [00:03<00:01,  9.68it/s]evaluate for the 36-th batch, evaluate loss: 0.48327574133872986:  78%|█████████████▎   | 36/46 [00:03<00:01,  9.67it/s]evaluate for the 37-th batch, evaluate loss: 0.5187509655952454:  78%|██████████████    | 36/46 [00:03<00:01,  9.67it/s]evaluate for the 37-th batch, evaluate loss: 0.5187509655952454:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.68it/s]Epoch: 4, train for the 202-th batch, train loss: 0.47408774495124817:  52%|█████▏    | 201/383 [01:58<01:47,  1.70it/s]Epoch: 4, train for the 202-th batch, train loss: 0.47408774495124817:  53%|█████▎    | 202/383 [01:58<01:46,  1.70it/s]evaluate for the 38-th batch, evaluate loss: 0.4789539873600006:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.68it/s]evaluate for the 38-th batch, evaluate loss: 0.4789539873600006:  83%|██████████████▊   | 38/46 [00:03<00:00,  9.70it/s]Epoch: 11, train for the 99-th batch, train loss: 0.16459307074546814:  82%|█████████  | 98/119 [00:59<00:12,  1.62it/s]Epoch: 11, train for the 99-th batch, train loss: 0.16459307074546814:  83%|█████████▏ | 99/119 [00:59<00:12,  1.63it/s]evaluate for the 39-th batch, evaluate loss: 0.4906315505504608:  83%|██████████████▊   | 38/46 [00:04<00:00,  9.70it/s]evaluate for the 39-th batch, evaluate loss: 0.4906315505504608:  85%|███████████████▎  | 39/46 [00:04<00:00,  9.58it/s]evaluate for the 40-th batch, evaluate loss: 0.4750292897224426:  85%|███████████████▎  | 39/46 [00:04<00:00,  9.58it/s]evaluate for the 40-th batch, evaluate loss: 0.4750292897224426:  87%|███████████████▋  | 40/46 [00:04<00:00,  9.63it/s]Epoch: 6, train for the 203-th batch, train loss: 0.6301931738853455:  85%|█████████▍ | 202/237 [01:57<00:20,  1.70it/s]Epoch: 6, train for the 203-th batch, train loss: 0.6301931738853455:  86%|█████████▍ | 203/237 [01:57<00:20,  1.70it/s]Epoch: 10, train for the 48-th batch, train loss: 0.4841521978378296:  32%|███▊        | 47/146 [00:28<01:00,  1.62it/s]Epoch: 10, train for the 48-th batch, train loss: 0.4841521978378296:  33%|███▉        | 48/146 [00:28<01:00,  1.63it/s]evaluate for the 41-th batch, evaluate loss: 0.5364723801612854:  87%|███████████████▋  | 40/46 [00:04<00:00,  9.63it/s]evaluate for the 41-th batch, evaluate loss: 0.5364723801612854:  89%|████████████████  | 41/46 [00:04<00:00,  9.63it/s]evaluate for the 42-th batch, evaluate loss: 0.49418172240257263:  89%|███████████████▏ | 41/46 [00:04<00:00,  9.63it/s]evaluate for the 42-th batch, evaluate loss: 0.49418172240257263:  91%|███████████████▌ | 42/46 [00:04<00:00,  9.64it/s]Epoch: 4, train for the 203-th batch, train loss: 0.4300268888473511:  53%|█████▊     | 202/383 [01:59<01:46,  1.70it/s]Epoch: 4, train for the 203-th batch, train loss: 0.4300268888473511:  53%|█████▊     | 203/383 [01:59<01:45,  1.70it/s]evaluate for the 43-th batch, evaluate loss: 0.4391709566116333:  91%|████████████████▍ | 42/46 [00:04<00:00,  9.64it/s]evaluate for the 43-th batch, evaluate loss: 0.4391709566116333:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.60it/s]evaluate for the 44-th batch, evaluate loss: 0.507564902305603:  93%|█████████████████▊ | 43/46 [00:04<00:00,  9.60it/s]evaluate for the 44-th batch, evaluate loss: 0.507564902305603:  96%|██████████████████▏| 44/46 [00:04<00:00,  9.60it/s]Epoch: 11, train for the 100-th batch, train loss: 0.1619192212820053:  83%|█████████▏ | 99/119 [00:59<00:12,  1.63it/s]Epoch: 11, train for the 100-th batch, train loss: 0.1619192212820053:  84%|████████▍ | 100/119 [00:59<00:11,  1.63it/s]evaluate for the 45-th batch, evaluate loss: 0.5323533415794373:  96%|█████████████████▏| 44/46 [00:04<00:00,  9.60it/s]evaluate for the 45-th batch, evaluate loss: 0.5323533415794373:  98%|█████████████████▌| 45/46 [00:04<00:00,  9.61it/s]Epoch: 6, train for the 204-th batch, train loss: 0.6343788504600525:  86%|█████████▍ | 203/237 [01:58<00:20,  1.70it/s]Epoch: 6, train for the 204-th batch, train loss: 0.6343788504600525:  86%|█████████▍ | 204/237 [01:58<00:19,  1.70it/s]evaluate for the 46-th batch, evaluate loss: 0.5795972943305969:  98%|█████████████████▌| 45/46 [00:04<00:00,  9.61it/s]evaluate for the 46-th batch, evaluate loss: 0.5795972943305969: 100%|██████████████████| 46/46 [00:04<00:00,  9.63it/s]
  0%|                                                                                            | 0/26 [00:00<?, ?it/s]Epoch: 10, train for the 49-th batch, train loss: 0.46760016679763794:  33%|███▌       | 48/146 [00:29<01:00,  1.63it/s]Epoch: 10, train for the 49-th batch, train loss: 0.46760016679763794:  34%|███▋       | 49/146 [00:29<00:59,  1.63it/s]evaluate for the 1-th batch, evaluate loss: 0.6770488023757935:   0%|                            | 0/26 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6770488023757935:   4%|▊                   | 1/26 [00:00<00:02,  9.24it/s]evaluate for the 2-th batch, evaluate loss: 0.7738653421401978:   4%|▊                   | 1/26 [00:00<00:02,  9.24it/s]evaluate for the 2-th batch, evaluate loss: 0.7738653421401978:   8%|█▌                  | 2/26 [00:00<00:02,  9.21it/s]Epoch: 4, train for the 204-th batch, train loss: 0.34678593277931213:  53%|█████▎    | 203/383 [01:59<01:45,  1.70it/s]Epoch: 4, train for the 204-th batch, train loss: 0.34678593277931213:  53%|█████▎    | 204/383 [01:59<01:45,  1.70it/s]evaluate for the 3-th batch, evaluate loss: 0.6916601657867432:   8%|█▌                  | 2/26 [00:00<00:02,  9.21it/s]evaluate for the 3-th batch, evaluate loss: 0.6916601657867432:  12%|██▎                 | 3/26 [00:00<00:02,  9.22it/s]evaluate for the 4-th batch, evaluate loss: 0.6863665580749512:  12%|██▎                 | 3/26 [00:00<00:02,  9.22it/s]evaluate for the 4-th batch, evaluate loss: 0.6863665580749512:  15%|███                 | 4/26 [00:00<00:02,  9.24it/s]Epoch: 11, train for the 101-th batch, train loss: 0.1449102759361267:  84%|████████▍ | 100/119 [01:00<00:11,  1.63it/s]Epoch: 11, train for the 101-th batch, train loss: 0.1449102759361267:  85%|████████▍ | 101/119 [01:00<00:11,  1.63it/s]evaluate for the 5-th batch, evaluate loss: 0.720159113407135:  15%|███▏                 | 4/26 [00:00<00:02,  9.24it/s]evaluate for the 5-th batch, evaluate loss: 0.720159113407135:  19%|████                 | 5/26 [00:00<00:02,  9.24it/s]Epoch: 6, train for the 205-th batch, train loss: 0.6278454065322876:  86%|█████████▍ | 204/237 [01:59<00:19,  1.70it/s]Epoch: 6, train for the 205-th batch, train loss: 0.6278454065322876:  86%|█████████▌ | 205/237 [01:59<00:18,  1.70it/s]Epoch: 10, train for the 50-th batch, train loss: 0.504788339138031:  34%|████▎        | 49/146 [00:29<00:59,  1.63it/s]Epoch: 10, train for the 50-th batch, train loss: 0.504788339138031:  34%|████▍        | 50/146 [00:29<00:58,  1.63it/s]evaluate for the 6-th batch, evaluate loss: 0.6568533182144165:  19%|███▊                | 5/26 [00:00<00:02,  9.24it/s]evaluate for the 6-th batch, evaluate loss: 0.6568533182144165:  23%|████▌               | 6/26 [00:00<00:02,  9.20it/s]evaluate for the 7-th batch, evaluate loss: 0.6819397807121277:  23%|████▌               | 6/26 [00:00<00:02,  9.20it/s]evaluate for the 7-th batch, evaluate loss: 0.6819397807121277:  27%|█████▍              | 7/26 [00:00<00:02,  9.21it/s]Epoch: 4, train for the 205-th batch, train loss: 0.37298697233200073:  53%|█████▎    | 204/383 [02:00<01:45,  1.70it/s]Epoch: 4, train for the 205-th batch, train loss: 0.37298697233200073:  54%|█████▎    | 205/383 [02:00<01:44,  1.70it/s]evaluate for the 8-th batch, evaluate loss: 0.6957266330718994:  27%|█████▍              | 7/26 [00:00<00:02,  9.21it/s]evaluate for the 8-th batch, evaluate loss: 0.6957266330718994:  31%|██████▏             | 8/26 [00:00<00:01,  9.22it/s]evaluate for the 9-th batch, evaluate loss: 0.6681230664253235:  31%|██████▏             | 8/26 [00:00<00:01,  9.22it/s]evaluate for the 9-th batch, evaluate loss: 0.6681230664253235:  35%|██████▉             | 9/26 [00:00<00:01,  9.23it/s]evaluate for the 10-th batch, evaluate loss: 0.6115854978561401:  35%|██████▌            | 9/26 [00:01<00:01,  9.23it/s]evaluate for the 10-th batch, evaluate loss: 0.6115854978561401:  38%|██████▉           | 10/26 [00:01<00:01,  9.23it/s]Epoch: 11, train for the 102-th batch, train loss: 0.16035278141498566:  85%|███████▋ | 101/119 [01:01<00:11,  1.63it/s]Epoch: 11, train for the 102-th batch, train loss: 0.16035278141498566:  86%|███████▋ | 102/119 [01:01<00:10,  1.63it/s]Epoch: 6, train for the 206-th batch, train loss: 0.6578338742256165:  86%|█████████▌ | 205/237 [01:59<00:18,  1.70it/s]Epoch: 6, train for the 206-th batch, train loss: 0.6578338742256165:  87%|█████████▌ | 206/237 [01:59<00:18,  1.70it/s]evaluate for the 11-th batch, evaluate loss: 0.6826456785202026:  38%|██████▉           | 10/26 [00:01<00:01,  9.23it/s]evaluate for the 11-th batch, evaluate loss: 0.6826456785202026:  42%|███████▌          | 11/26 [00:01<00:01,  9.22it/s]Epoch: 10, train for the 51-th batch, train loss: 0.469647616147995:  34%|████▍        | 50/146 [00:30<00:58,  1.63it/s]Epoch: 10, train for the 51-th batch, train loss: 0.469647616147995:  35%|████▌        | 51/146 [00:30<00:58,  1.63it/s]evaluate for the 12-th batch, evaluate loss: 0.7202388048171997:  42%|███████▌          | 11/26 [00:01<00:01,  9.22it/s]evaluate for the 12-th batch, evaluate loss: 0.7202388048171997:  46%|████████▎         | 12/26 [00:01<00:01,  9.21it/s]evaluate for the 13-th batch, evaluate loss: 0.7236160039901733:  46%|████████▎         | 12/26 [00:01<00:01,  9.21it/s]evaluate for the 13-th batch, evaluate loss: 0.7236160039901733:  50%|█████████         | 13/26 [00:01<00:01,  9.21it/s]Epoch: 4, train for the 206-th batch, train loss: 0.41790297627449036:  54%|█████▎    | 205/383 [02:01<01:44,  1.70it/s]Epoch: 4, train for the 206-th batch, train loss: 0.41790297627449036:  54%|█████▍    | 206/383 [02:01<01:43,  1.70it/s]evaluate for the 14-th batch, evaluate loss: 0.6162516474723816:  50%|█████████         | 13/26 [00:01<00:01,  9.21it/s]evaluate for the 14-th batch, evaluate loss: 0.6162516474723816:  54%|█████████▋        | 14/26 [00:01<00:01,  9.24it/s]evaluate for the 15-th batch, evaluate loss: 0.7078383564949036:  54%|█████████▋        | 14/26 [00:01<00:01,  9.24it/s]evaluate for the 15-th batch, evaluate loss: 0.7078383564949036:  58%|██████████▍       | 15/26 [00:01<00:01,  9.25it/s]Epoch: 11, train for the 103-th batch, train loss: 0.18473954498767853:  86%|███████▋ | 102/119 [01:01<00:10,  1.63it/s]Epoch: 11, train for the 103-th batch, train loss: 0.18473954498767853:  87%|███████▊ | 103/119 [01:01<00:09,  1.63it/s]evaluate for the 16-th batch, evaluate loss: 0.7250397205352783:  58%|██████████▍       | 15/26 [00:01<00:01,  9.25it/s]evaluate for the 16-th batch, evaluate loss: 0.7250397205352783:  62%|███████████       | 16/26 [00:01<00:01,  9.24it/s]Epoch: 6, train for the 207-th batch, train loss: 0.6320737600326538:  87%|█████████▌ | 206/237 [02:00<00:18,  1.70it/s]Epoch: 6, train for the 207-th batch, train loss: 0.6320737600326538:  87%|█████████▌ | 207/237 [02:00<00:17,  1.70it/s]evaluate for the 17-th batch, evaluate loss: 0.6767528653144836:  62%|███████████       | 16/26 [00:01<00:01,  9.24it/s]evaluate for the 17-th batch, evaluate loss: 0.6767528653144836:  65%|███████████▊      | 17/26 [00:01<00:00,  9.24it/s]Epoch: 10, train for the 52-th batch, train loss: 0.47368478775024414:  35%|███▊       | 51/146 [00:31<00:58,  1.63it/s]Epoch: 10, train for the 52-th batch, train loss: 0.47368478775024414:  36%|███▉       | 52/146 [00:31<00:57,  1.63it/s]evaluate for the 18-th batch, evaluate loss: 0.7240598797798157:  65%|███████████▊      | 17/26 [00:01<00:00,  9.24it/s]evaluate for the 18-th batch, evaluate loss: 0.7240598797798157:  69%|████████████▍     | 18/26 [00:01<00:00,  9.25it/s]Epoch: 4, train for the 207-th batch, train loss: 0.39867064356803894:  54%|█████▍    | 206/383 [02:01<01:43,  1.70it/s]Epoch: 4, train for the 207-th batch, train loss: 0.39867064356803894:  54%|█████▍    | 207/383 [02:01<01:43,  1.70it/s]evaluate for the 19-th batch, evaluate loss: 0.6273371577262878:  69%|████████████▍     | 18/26 [00:02<00:00,  9.25it/s]evaluate for the 19-th batch, evaluate loss: 0.6273371577262878:  73%|█████████████▏    | 19/26 [00:02<00:00,  9.24it/s]evaluate for the 20-th batch, evaluate loss: 0.6442312002182007:  73%|█████████████▏    | 19/26 [00:02<00:00,  9.24it/s]evaluate for the 20-th batch, evaluate loss: 0.6442312002182007:  77%|█████████████▊    | 20/26 [00:02<00:00,  9.21it/s]evaluate for the 21-th batch, evaluate loss: 0.67864990234375:  77%|███████████████▍    | 20/26 [00:02<00:00,  9.21it/s]evaluate for the 21-th batch, evaluate loss: 0.67864990234375:  81%|████████████████▏   | 21/26 [00:02<00:00,  9.22it/s]Epoch: 6, train for the 208-th batch, train loss: 0.664417564868927:  87%|██████████▍ | 207/237 [02:00<00:17,  1.70it/s]Epoch: 6, train for the 208-th batch, train loss: 0.664417564868927:  88%|██████████▌ | 208/237 [02:00<00:17,  1.70it/s]Epoch: 10, train for the 53-th batch, train loss: 0.4592333137989044:  36%|████▎       | 52/146 [00:31<00:57,  1.63it/s]Epoch: 10, train for the 53-th batch, train loss: 0.4592333137989044:  36%|████▎       | 53/146 [00:31<00:53,  1.75it/s]evaluate for the 22-th batch, evaluate loss: 0.7139511704444885:  81%|██████████████▌   | 21/26 [00:02<00:00,  9.22it/s]evaluate for the 22-th batch, evaluate loss: 0.7139511704444885:  85%|███████████████▏  | 22/26 [00:02<00:00,  9.21it/s]Epoch: 11, train for the 104-th batch, train loss: 0.1974438577890396:  87%|████████▋ | 103/119 [01:02<00:09,  1.63it/s]Epoch: 11, train for the 104-th batch, train loss: 0.1974438577890396:  87%|████████▋ | 104/119 [01:02<00:09,  1.55it/s]evaluate for the 23-th batch, evaluate loss: 0.6874735951423645:  85%|███████████████▏  | 22/26 [00:02<00:00,  9.21it/s]evaluate for the 23-th batch, evaluate loss: 0.6874735951423645:  88%|███████████████▉  | 23/26 [00:02<00:00,  9.14it/s]evaluate for the 24-th batch, evaluate loss: 0.6979594826698303:  88%|███████████████▉  | 23/26 [00:02<00:00,  9.14it/s]evaluate for the 24-th batch, evaluate loss: 0.6979594826698303:  92%|████████████████▌ | 24/26 [00:02<00:00,  9.14it/s]Epoch: 4, train for the 208-th batch, train loss: 0.4289681017398834:  54%|█████▉     | 207/383 [02:02<01:43,  1.70it/s]Epoch: 4, train for the 208-th batch, train loss: 0.4289681017398834:  54%|█████▉     | 208/383 [02:02<01:42,  1.70it/s]evaluate for the 25-th batch, evaluate loss: 0.73686683177948:  92%|██████████████████▍ | 24/26 [00:02<00:00,  9.14it/s]evaluate for the 25-th batch, evaluate loss: 0.73686683177948:  96%|███████████████████▏| 25/26 [00:02<00:00,  9.18it/s]evaluate for the 26-th batch, evaluate loss: 0.7184939384460449:  96%|█████████████████▎| 25/26 [00:02<00:00,  9.18it/s]evaluate for the 26-th batch, evaluate loss: 0.7184939384460449: 100%|██████████████████| 26/26 [00:02<00:00,  9.41it/s]
INFO:root:test loss: 0.5079
INFO:root:test average_precision, 0.8434
INFO:root:test roc_auc, 0.8068
INFO:root:new node test loss: 0.6902
INFO:root:new node test first_1_average_precision, 0.4929
INFO:root:new node test first_1_roc_auc, 0.4242
INFO:root:new node test first_3_average_precision, 0.5650
INFO:root:new node test first_3_roc_auc, 0.5173
INFO:root:new node test first_10_average_precision, 0.6812
INFO:root:new node test first_10_roc_auc, 0.6437
INFO:root:new node test average_precision, 0.7101
INFO:root:new node test roc_auc, 0.6610
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]Epoch: 6, train for the 209-th batch, train loss: 0.634222149848938:  88%|██████████▌ | 208/237 [02:01<00:17,  1.70it/s]Epoch: 6, train for the 209-th batch, train loss: 0.634222149848938:  88%|██████████▌ | 209/237 [02:01<00:16,  1.70it/s]Epoch: 10, train for the 54-th batch, train loss: 0.48808059096336365:  36%|███▉       | 53/146 [00:32<00:53,  1.75it/s]Epoch: 10, train for the 54-th batch, train loss: 0.48808059096336365:  37%|████       | 54/146 [00:32<00:53,  1.73it/s]Epoch: 21, train for the 1-th batch, train loss: 1.2164864540100098:   0%|                      | 0/151 [00:00<?, ?it/s]Epoch: 21, train for the 1-th batch, train loss: 1.2164864540100098:   1%|              | 1/151 [00:00<00:25,  5.77it/s]Epoch: 11, train for the 105-th batch, train loss: 0.16969867050647736:  87%|███████▊ | 104/119 [01:03<00:09,  1.55it/s]Epoch: 11, train for the 105-th batch, train loss: 0.16969867050647736:  88%|███████▉ | 105/119 [01:03<00:08,  1.58it/s]Epoch: 21, train for the 2-th batch, train loss: 1.273148775100708:   1%|               | 1/151 [00:00<00:25,  5.77it/s]Epoch: 21, train for the 2-th batch, train loss: 1.273148775100708:   1%|▏              | 2/151 [00:00<00:26,  5.66it/s]Epoch: 4, train for the 209-th batch, train loss: 0.38590681552886963:  54%|█████▍    | 208/383 [02:02<01:42,  1.70it/s]Epoch: 4, train for the 209-th batch, train loss: 0.38590681552886963:  55%|█████▍    | 209/383 [02:02<01:42,  1.70it/s]Epoch: 21, train for the 3-th batch, train loss: 0.5885546207427979:   1%|▏             | 2/151 [00:00<00:26,  5.66it/s]Epoch: 21, train for the 3-th batch, train loss: 0.5885546207427979:   2%|▎             | 3/151 [00:00<00:32,  4.51it/s]Epoch: 21, train for the 4-th batch, train loss: 0.5705322623252869:   2%|▎             | 3/151 [00:00<00:32,  4.51it/s]Epoch: 21, train for the 4-th batch, train loss: 0.5705322623252869:   3%|▎             | 4/151 [00:00<00:29,  4.93it/s]Epoch: 6, train for the 210-th batch, train loss: 0.6341421008110046:  88%|█████████▋ | 209/237 [02:02<00:16,  1.70it/s]Epoch: 6, train for the 210-th batch, train loss: 0.6341421008110046:  89%|█████████▋ | 210/237 [02:02<00:16,  1.64it/s]Epoch: 10, train for the 55-th batch, train loss: 0.5337274074554443:  37%|████▍       | 54/146 [00:32<00:53,  1.73it/s]Epoch: 10, train for the 55-th batch, train loss: 0.5337274074554443:  38%|████▌       | 55/146 [00:32<00:55,  1.65it/s]Epoch: 11, train for the 106-th batch, train loss: 0.11575747281312943:  88%|███████▉ | 105/119 [01:03<00:08,  1.58it/s]Epoch: 11, train for the 106-th batch, train loss: 0.11575747281312943:  89%|████████ | 106/119 [01:03<00:08,  1.54it/s]Epoch: 21, train for the 5-th batch, train loss: 0.5559387803077698:   3%|▎             | 4/151 [00:00<00:29,  4.93it/s]Epoch: 21, train for the 5-th batch, train loss: 0.5559387803077698:   3%|▍             | 5/151 [00:00<00:28,  5.04it/s]Epoch: 4, train for the 210-th batch, train loss: 0.454996794462204:  55%|██████▌     | 209/383 [02:03<01:42,  1.70it/s]Epoch: 4, train for the 210-th batch, train loss: 0.454996794462204:  55%|██████▌     | 210/383 [02:03<01:45,  1.63it/s]Epoch: 21, train for the 6-th batch, train loss: 0.6626661419868469:   3%|▍             | 5/151 [00:01<00:28,  5.04it/s]Epoch: 21, train for the 6-th batch, train loss: 0.6626661419868469:   4%|▌             | 6/151 [00:01<00:28,  5.10it/s]Epoch: 21, train for the 7-th batch, train loss: 0.44710493087768555:   4%|▌            | 6/151 [00:01<00:28,  5.10it/s]Epoch: 21, train for the 7-th batch, train loss: 0.44710493087768555:   5%|▌            | 7/151 [00:01<00:28,  5.12it/s]Epoch: 10, train for the 56-th batch, train loss: 0.4710005223751068:  38%|████▌       | 55/146 [00:33<00:55,  1.65it/s]Epoch: 10, train for the 56-th batch, train loss: 0.4710005223751068:  38%|████▌       | 56/146 [00:33<00:53,  1.68it/s]Epoch: 6, train for the 211-th batch, train loss: 0.6279389262199402:  89%|█████████▋ | 210/237 [02:02<00:16,  1.64it/s]Epoch: 6, train for the 211-th batch, train loss: 0.6279389262199402:  89%|█████████▊ | 211/237 [02:02<00:16,  1.55it/s]Epoch: 11, train for the 107-th batch, train loss: 0.17598216235637665:  89%|████████ | 106/119 [01:04<00:08,  1.54it/s]Epoch: 11, train for the 107-th batch, train loss: 0.17598216235637665:  90%|████████ | 107/119 [01:04<00:07,  1.57it/s]Epoch: 21, train for the 8-th batch, train loss: 0.8199054002761841:   5%|▋             | 7/151 [00:01<00:28,  5.12it/s]Epoch: 21, train for the 8-th batch, train loss: 0.8199054002761841:   5%|▋             | 8/151 [00:01<00:27,  5.14it/s]Epoch: 4, train for the 211-th batch, train loss: 0.40720030665397644:  55%|█████▍    | 210/383 [02:04<01:45,  1.63it/s]Epoch: 4, train for the 211-th batch, train loss: 0.40720030665397644:  55%|█████▌    | 211/383 [02:04<01:38,  1.74it/s]Epoch: 21, train for the 9-th batch, train loss: 0.5904648303985596:   5%|▋             | 8/151 [00:01<00:27,  5.14it/s]Epoch: 21, train for the 9-th batch, train loss: 0.5904648303985596:   6%|▊             | 9/151 [00:01<00:27,  5.09it/s]Epoch: 21, train for the 10-th batch, train loss: 0.5537869930267334:   6%|▊            | 9/151 [00:01<00:27,  5.09it/s]Epoch: 21, train for the 10-th batch, train loss: 0.5537869930267334:   7%|▊           | 10/151 [00:01<00:28,  5.03it/s]Epoch: 10, train for the 57-th batch, train loss: 0.49265265464782715:  38%|████▏      | 56/146 [00:33<00:53,  1.68it/s]Epoch: 10, train for the 57-th batch, train loss: 0.49265265464782715:  39%|████▎      | 57/146 [00:33<00:53,  1.67it/s]Epoch: 6, train for the 212-th batch, train loss: 0.616531491279602:  89%|██████████▋ | 211/237 [02:03<00:16,  1.55it/s]Epoch: 6, train for the 212-th batch, train loss: 0.616531491279602:  89%|██████████▋ | 212/237 [02:03<00:15,  1.58it/s]Epoch: 21, train for the 11-th batch, train loss: 0.5637302398681641:   7%|▊           | 10/151 [00:02<00:28,  5.03it/s]Epoch: 21, train for the 11-th batch, train loss: 0.5637302398681641:   7%|▊           | 11/151 [00:02<00:27,  5.01it/s]Epoch: 11, train for the 108-th batch, train loss: 0.12142382562160492:  90%|████████ | 107/119 [01:04<00:07,  1.57it/s]Epoch: 11, train for the 108-th batch, train loss: 0.12142382562160492:  91%|████████▏| 108/119 [01:04<00:06,  1.59it/s]Epoch: 4, train for the 212-th batch, train loss: 0.4387643039226532:  55%|██████     | 211/383 [02:04<01:38,  1.74it/s]Epoch: 4, train for the 212-th batch, train loss: 0.4387643039226532:  55%|██████     | 212/383 [02:04<01:39,  1.72it/s]Epoch: 21, train for the 12-th batch, train loss: 0.505804181098938:   7%|▉            | 11/151 [00:02<00:27,  5.01it/s]Epoch: 21, train for the 12-th batch, train loss: 0.505804181098938:   8%|█            | 12/151 [00:02<00:32,  4.25it/s]Epoch: 11, train for the 109-th batch, train loss: 0.20416556298732758:  91%|████████▏| 108/119 [01:05<00:06,  1.59it/s]Epoch: 11, train for the 109-th batch, train loss: 0.20416556298732758:  92%|████████▏| 109/119 [01:05<00:05,  1.73it/s]Epoch: 21, train for the 13-th batch, train loss: 0.6055001616477966:   8%|▉           | 12/151 [00:02<00:32,  4.25it/s]Epoch: 21, train for the 13-th batch, train loss: 0.6055001616477966:   9%|█           | 13/151 [00:02<00:31,  4.41it/s]Epoch: 10, train for the 58-th batch, train loss: 0.5094380974769592:  39%|████▋       | 57/146 [00:34<00:53,  1.67it/s]Epoch: 10, train for the 58-th batch, train loss: 0.5094380974769592:  40%|████▊       | 58/146 [00:34<00:57,  1.53it/s]Epoch: 6, train for the 213-th batch, train loss: 0.6373132467269897:  89%|█████████▊ | 212/237 [02:04<00:15,  1.58it/s]Epoch: 6, train for the 213-th batch, train loss: 0.6373132467269897:  90%|█████████▉ | 213/237 [02:04<00:15,  1.56it/s]Epoch: 21, train for the 14-th batch, train loss: 0.5808313488960266:   9%|█           | 13/151 [00:02<00:31,  4.41it/s]Epoch: 21, train for the 14-th batch, train loss: 0.5808313488960266:   9%|█           | 14/151 [00:02<00:30,  4.55it/s]Epoch: 4, train for the 213-th batch, train loss: 0.5564919114112854:  55%|██████     | 212/383 [02:05<01:39,  1.72it/s]Epoch: 4, train for the 213-th batch, train loss: 0.5564919114112854:  56%|██████     | 213/383 [02:05<01:48,  1.57it/s]usage: Interface for the link prediction task [-h]
                                              [--dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}]
                                              [--batch_size BATCH_SIZE]
                                              [--model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}]
                                              [--gpu GPU]
                                              [--num_neighbors NUM_NEIGHBORS]
                                              [--sample_neighbor_strategy {uniform,recent,time_interval_aware}]
                                              [--time_scaling_factor TIME_SCALING_FACTOR]
                                              [--num_walk_heads NUM_WALK_HEADS]
                                              [--num_heads NUM_HEADS]
                                              [--num_layers NUM_LAYERS]
                                              [--walk_length WALK_LENGTH]
                                              [--time_gap TIME_GAP]
                                              [--time_feat_dim TIME_FEAT_DIM]
                                              [--position_feat_dim POSITION_FEAT_DIM]
                                              [--edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}]
                                              [--time_window_mode {fixed_proportion,repeat_interval}]
                                              [--patch_size PATCH_SIZE]
                                              [--channel_embedding_dim CHANNEL_EMBEDDING_DIM]
                                              [--max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH]
                                              [--learning_rate LEARNING_RATE]
                                              [--dropout DROPOUT]
                                              [--num_epochs NUM_EPOCHS]
                                              [--optimizer {SGD,Adam,RMSprop,AdamW}]
                                              [--weight_decay WEIGHT_DECAY]
                                              [--patience PATIENCE]
                                              [--val_ratio VAL_RATIO]
                                              [--test_ratio TEST_RATIO]
                                              [--num_runs NUM_RUNS]
                                              [--test_interval_epochs TEST_INTERVAL_EPOCHS]
                                              [--negative_sample_strategy {random,historical,inductive}]
                                              [--load_best_configs]
                                              [--use_wandb USE_WANDB]
                                              [--use_ROPe]
                                              [--t1_factor_of_t2 T1_FACTOR_OF_T2]
                                              [--use_init_method] [--emb_proj]
                                              [--init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}]
                                              [--clip CLIP]
Interface for the link prediction task: error: argument --dataset_name: expected one argument
True
usage: Interface for the link prediction task [-h]
                                              [--dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}]
                                              [--batch_size BATCH_SIZE]
                                              [--model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}]
                                              [--gpu GPU]
                                              [--num_neighbors NUM_NEIGHBORS]
                                              [--sample_neighbor_strategy {uniform,recent,time_interval_aware}]
                                              [--time_scaling_factor TIME_SCALING_FACTOR]
                                              [--num_walk_heads NUM_WALK_HEADS]
                                              [--num_heads NUM_HEADS]
                                              [--num_layers NUM_LAYERS]
                                              [--walk_length WALK_LENGTH]
                                              [--time_gap TIME_GAP]
                                              [--time_feat_dim TIME_FEAT_DIM]
                                              [--position_feat_dim POSITION_FEAT_DIM]
                                              [--edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}]
                                              [--time_window_mode {fixed_proportion,repeat_interval}]
                                              [--patch_size PATCH_SIZE]
                                              [--channel_embedding_dim CHANNEL_EMBEDDING_DIM]
                                              [--max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH]
                                              [--learning_rate LEARNING_RATE]
                                              [--dropout DROPOUT]
                                              [--num_epochs NUM_EPOCHS]
                                              [--optimizer {SGD,Adam,RMSprop,AdamW}]
                                              [--weight_decay WEIGHT_DECAY]
                                              [--patience PATIENCE]
                                              [--val_ratio VAL_RATIO]
                                              [--test_ratio TEST_RATIO]
                                              [--num_runs NUM_RUNS]
                                              [--test_interval_epochs TEST_INTERVAL_EPOCHS]
                                              [--negative_sample_strategy {random,historical,inductive}]
                                              [--load_best_configs]
                                              [--use_wandb USE_WANDB]
                                              [--use_ROPe]
                                              [--t1_factor_of_t2 T1_FACTOR_OF_T2]
                                              [--use_init_method] [--emb_proj]
                                              [--init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}]
                                              [--clip CLIP]

options:
  -h, --help            show this help message and exit
  --dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}
                        dataset to be used
  --batch_size BATCH_SIZE
                        batch size
  --model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}
                        name of the model, note that EdgeBank is only
                        applicable for evaluation
  --gpu GPU             number of gpu to use
  --num_neighbors NUM_NEIGHBORS
                        number of neighbors to sample for each node
  --sample_neighbor_strategy {uniform,recent,time_interval_aware}
                        how to sample historical neighbors
  --time_scaling_factor TIME_SCALING_FACTOR
                        the hyperparameter that controls the sampling
                        preference with time interval, a large
                        time_scaling_factor tends to sample more on recent
                        links, 0.0 corresponds to uniform sampling, it works
                        when sample_neighbor_strategy == time_interval_aware
  --num_walk_heads NUM_WALK_HEADS
                        number of heads used for the attention in walk encoder
  --num_heads NUM_HEADS
                        number of heads used in attention layer
  --num_layers NUM_LAYERS
                        number of model layers
  --walk_length WALK_LENGTH
                        length of each random walk
  --time_gap TIME_GAP   time gap for neighbors to compute node features
  --time_feat_dim TIME_FEAT_DIM
                        dimension of the time embedding
  --position_feat_dim POSITION_FEAT_DIM
                        dimension of the position embedding
  --edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}
                        how memory of EdgeBank works
  --time_window_mode {fixed_proportion,repeat_interval}
                        how to select the time window size for time window
                        memory
  --patch_size PATCH_SIZE
                        patch size
  --channel_embedding_dim CHANNEL_EMBEDDING_DIM
                        dimension of each channel embedding
  --max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH
                        maximal length of the input sequence of each node
  --learning_rate LEARNING_RATE
                        learning rate
  --dropout DROPOUT     dropout rate
  --num_epochs NUM_EPOCHS
                        number of epochs
  --optimizer {SGD,Adam,RMSprop,AdamW}
                        name of optimizer
  --weight_decay WEIGHT_DECAY
                        weight decay
  --patience PATIENCE   patience for early stopping
  --val_ratio VAL_RATIO
                        ratio of validation set
  --test_ratio TEST_RATIO
                        ratio of test set
  --num_runs NUM_RUNS   number of runs
  --test_interval_epochs TEST_INTERVAL_EPOCHS
                        how many epochs to perform testing once
  --negative_sample_strategy {random,historical,inductive}
                        strategy for the negative edge sampling
  --load_best_configs   whether to load the best configurations
  --use_wandb USE_WANDB
                        do you want to track this run using wandb? If arg is
                        `no`, then don`t track. Else, track using wandb and
                        name the run `run_name_dataset`
  --use_ROPe            Use ROPe embeddings for DecoLP
  --t1_factor_of_t2 T1_FACTOR_OF_T2
                        t1 factor of t2 when t2 is 0.04 of total time
  --use_init_method     Use new init method
  --emb_proj            Use embedding projection before weighted average
  --init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}
  --clip CLIP           clip val for grad of time-transformation operation
usage: Interface for the link prediction task [-h]
                                              [--dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}]
                                              [--batch_size BATCH_SIZE]
                                              [--model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}]
                                              [--gpu GPU]
                                              [--num_neighbors NUM_NEIGHBORS]
                                              [--sample_neighbor_strategy {uniform,recent,time_interval_aware}]
                                              [--time_scaling_factor TIME_SCALING_FACTOR]
                                              [--num_walk_heads NUM_WALK_HEADS]
                                              [--num_heads NUM_HEADS]
                                              [--num_layers NUM_LAYERS]
                                              [--walk_length WALK_LENGTH]
                                              [--time_gap TIME_GAP]
                                              [--time_feat_dim TIME_FEAT_DIM]
                                              [--position_feat_dim POSITION_FEAT_DIM]
                                              [--edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}]
                                              [--time_window_mode {fixed_proportion,repeat_interval}]
                                              [--patch_size PATCH_SIZE]
                                              [--channel_embedding_dim CHANNEL_EMBEDDING_DIM]
                                              [--max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH]
                                              [--learning_rate LEARNING_RATE]
                                              [--dropout DROPOUT]
                                              [--num_epochs NUM_EPOCHS]
                                              [--optimizer {SGD,Adam,RMSprop,AdamW}]
                                              [--weight_decay WEIGHT_DECAY]
                                              [--patience PATIENCE]
                                              [--val_ratio VAL_RATIO]
                                              [--test_ratio TEST_RATIO]
                                              [--num_runs NUM_RUNS]
                                              [--test_interval_epochs TEST_INTERVAL_EPOCHS]
                                              [--negative_sample_strategy {random,historical,inductive}]
                                              [--load_best_configs]
                                              [--use_wandb USE_WANDB]
                                              [--use_ROPe]
                                              [--t1_factor_of_t2 T1_FACTOR_OF_T2]
                                              [--use_init_method] [--emb_proj]
                                              [--init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}]
                                              [--clip CLIP]
Interface for the link prediction task: error: argument --dataset_name: expected one argument
True
usage: Interface for the link prediction task [-h]
                                              [--dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}]
                                              [--batch_size BATCH_SIZE]
                                              [--model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}]
                                              [--gpu GPU]
                                              [--num_neighbors NUM_NEIGHBORS]
                                              [--sample_neighbor_strategy {uniform,recent,time_interval_aware}]
                                              [--time_scaling_factor TIME_SCALING_FACTOR]
                                              [--num_walk_heads NUM_WALK_HEADS]
                                              [--num_heads NUM_HEADS]
                                              [--num_layers NUM_LAYERS]
                                              [--walk_length WALK_LENGTH]
                                              [--time_gap TIME_GAP]
                                              [--time_feat_dim TIME_FEAT_DIM]
                                              [--position_feat_dim POSITION_FEAT_DIM]
                                              [--edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}]
                                              [--time_window_mode {fixed_proportion,repeat_interval}]
                                              [--patch_size PATCH_SIZE]
                                              [--channel_embedding_dim CHANNEL_EMBEDDING_DIM]
                                              [--max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH]
                                              [--learning_rate LEARNING_RATE]
                                              [--dropout DROPOUT]
                                              [--num_epochs NUM_EPOCHS]
                                              [--optimizer {SGD,Adam,RMSprop,AdamW}]
                                              [--weight_decay WEIGHT_DECAY]
                                              [--patience PATIENCE]
                                              [--val_ratio VAL_RATIO]
                                              [--test_ratio TEST_RATIO]
                                              [--num_runs NUM_RUNS]
                                              [--test_interval_epochs TEST_INTERVAL_EPOCHS]
                                              [--negative_sample_strategy {random,historical,inductive}]
                                              [--load_best_configs]
                                              [--use_wandb USE_WANDB]
                                              [--use_ROPe]
                                              [--t1_factor_of_t2 T1_FACTOR_OF_T2]
                                              [--use_init_method] [--emb_proj]
                                              [--init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}]
                                              [--clip CLIP]

options:
  -h, --help            show this help message and exit
  --dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}
                        dataset to be used
  --batch_size BATCH_SIZE
                        batch size
  --model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}
                        name of the model, note that EdgeBank is only
                        applicable for evaluation
  --gpu GPU             number of gpu to use
  --num_neighbors NUM_NEIGHBORS
                        number of neighbors to sample for each node
  --sample_neighbor_strategy {uniform,recent,time_interval_aware}
                        how to sample historical neighbors
  --time_scaling_factor TIME_SCALING_FACTOR
                        the hyperparameter that controls the sampling
                        preference with time interval, a large
                        time_scaling_factor tends to sample more on recent
                        links, 0.0 corresponds to uniform sampling, it works
                        when sample_neighbor_strategy == time_interval_aware
  --num_walk_heads NUM_WALK_HEADS
                        number of heads used for the attention in walk encoder
  --num_heads NUM_HEADS
                        number of heads used in attention layer
  --num_layers NUM_LAYERS
                        number of model layers
  --walk_length WALK_LENGTH
                        length of each random walk
  --time_gap TIME_GAP   time gap for neighbors to compute node features
  --time_feat_dim TIME_FEAT_DIM
                        dimension of the time embedding
  --position_feat_dim POSITION_FEAT_DIM
                        dimension of the position embedding
  --edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}
                        how memory of EdgeBank works
  --time_window_mode {fixed_proportion,repeat_interval}
                        how to select the time window size for time window
                        memory
  --patch_size PATCH_SIZE
                        patch size
  --channel_embedding_dim CHANNEL_EMBEDDING_DIM
                        dimension of each channel embedding
  --max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH
                        maximal length of the input sequence of each node
  --learning_rate LEARNING_RATE
                        learning rate
  --dropout DROPOUT     dropout rate
  --num_epochs NUM_EPOCHS
                        number of epochs
  --optimizer {SGD,Adam,RMSprop,AdamW}
                        name of optimizer
  --weight_decay WEIGHT_DECAY
                        weight decay
  --patience PATIENCE   patience for early stopping
  --val_ratio VAL_RATIO
                        ratio of validation set
  --test_ratio TEST_RATIO
                        ratio of test set
  --num_runs NUM_RUNS   number of runs
  --test_interval_epochs TEST_INTERVAL_EPOCHS
                        how many epochs to perform testing once
  --negative_sample_strategy {random,historical,inductive}
                        strategy for the negative edge sampling
  --load_best_configs   whether to load the best configurations
  --use_wandb USE_WANDB
                        do you want to track this run using wandb? If arg is
                        `no`, then don`t track. Else, track using wandb and
                        name the run `run_name_dataset`
  --use_ROPe            Use ROPe embeddings for DecoLP
  --t1_factor_of_t2 T1_FACTOR_OF_T2
                        t1 factor of t2 when t2 is 0.04 of total time
  --use_init_method     Use new init method
  --emb_proj            Use embedding projection before weighted average
  --init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}
  --clip CLIP           clip val for grad of time-transformation operation
usage: Interface for the link prediction task [-h]
                                              [--dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}]
                                              [--batch_size BATCH_SIZE]
                                              [--model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}]
                                              [--gpu GPU]
                                              [--num_neighbors NUM_NEIGHBORS]
                                              [--sample_neighbor_strategy {uniform,recent,time_interval_aware}]
                                              [--time_scaling_factor TIME_SCALING_FACTOR]
                                              [--num_walk_heads NUM_WALK_HEADS]
                                              [--num_heads NUM_HEADS]
                                              [--num_layers NUM_LAYERS]
                                              [--walk_length WALK_LENGTH]
                                              [--time_gap TIME_GAP]
                                              [--time_feat_dim TIME_FEAT_DIM]
                                              [--position_feat_dim POSITION_FEAT_DIM]
                                              [--edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}]
                                              [--time_window_mode {fixed_proportion,repeat_interval}]
                                              [--patch_size PATCH_SIZE]
                                              [--channel_embedding_dim CHANNEL_EMBEDDING_DIM]
                                              [--max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH]
                                              [--learning_rate LEARNING_RATE]
                                              [--dropout DROPOUT]
                                              [--num_epochs NUM_EPOCHS]
                                              [--optimizer {SGD,Adam,RMSprop,AdamW}]
                                              [--weight_decay WEIGHT_DECAY]
                                              [--patience PATIENCE]
                                              [--val_ratio VAL_RATIO]
                                              [--test_ratio TEST_RATIO]
                                              [--num_runs NUM_RUNS]
                                              [--test_interval_epochs TEST_INTERVAL_EPOCHS]
                                              [--negative_sample_strategy {random,historical,inductive}]
                                              [--load_best_configs]
                                              [--use_wandb USE_WANDB]
                                              [--use_ROPe]
                                              [--t1_factor_of_t2 T1_FACTOR_OF_T2]
                                              [--use_init_method] [--emb_proj]
                                              [--init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}]
                                              [--clip CLIP]
Interface for the link prediction task: error: argument --dataset_name: expected one argument
True
usage: Interface for the link prediction task [-h]
                                              [--dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}]
                                              [--batch_size BATCH_SIZE]
                                              [--model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}]
                                              [--gpu GPU]
                                              [--num_neighbors NUM_NEIGHBORS]
                                              [--sample_neighbor_strategy {uniform,recent,time_interval_aware}]
                                              [--time_scaling_factor TIME_SCALING_FACTOR]
                                              [--num_walk_heads NUM_WALK_HEADS]
                                              [--num_heads NUM_HEADS]
                                              [--num_layers NUM_LAYERS]
                                              [--walk_length WALK_LENGTH]
                                              [--time_gap TIME_GAP]
                                              [--time_feat_dim TIME_FEAT_DIM]
                                              [--position_feat_dim POSITION_FEAT_DIM]
                                              [--edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}]
                                              [--time_window_mode {fixed_proportion,repeat_interval}]
                                              [--patch_size PATCH_SIZE]
                                              [--channel_embedding_dim CHANNEL_EMBEDDING_DIM]
                                              [--max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH]
                                              [--learning_rate LEARNING_RATE]
                                              [--dropout DROPOUT]
                                              [--num_epochs NUM_EPOCHS]
                                              [--optimizer {SGD,Adam,RMSprop,AdamW}]
                                              [--weight_decay WEIGHT_DECAY]
                                              [--patience PATIENCE]
                                              [--val_ratio VAL_RATIO]
                                              [--test_ratio TEST_RATIO]
                                              [--num_runs NUM_RUNS]
                                              [--test_interval_epochs TEST_INTERVAL_EPOCHS]
                                              [--negative_sample_strategy {random,historical,inductive}]
                                              [--load_best_configs]
                                              [--use_wandb USE_WANDB]
                                              [--use_ROPe]
                                              [--t1_factor_of_t2 T1_FACTOR_OF_T2]
                                              [--use_init_method] [--emb_proj]
                                              [--init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}]
                                              [--clip CLIP]

options:
  -h, --help            show this help message and exit
  --dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}
                        dataset to be used
  --batch_size BATCH_SIZE
                        batch size
  --model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}
                        name of the model, note that EdgeBank is only
                        applicable for evaluation
  --gpu GPU             number of gpu to use
  --num_neighbors NUM_NEIGHBORS
                        number of neighbors to sample for each node
  --sample_neighbor_strategy {uniform,recent,time_interval_aware}
                        how to sample historical neighbors
  --time_scaling_factor TIME_SCALING_FACTOR
                        the hyperparameter that controls the sampling
                        preference with time interval, a large
                        time_scaling_factor tends to sample more on recent
                        links, 0.0 corresponds to uniform sampling, it works
                        when sample_neighbor_strategy == time_interval_aware
  --num_walk_heads NUM_WALK_HEADS
                        number of heads used for the attention in walk encoder
  --num_heads NUM_HEADS
                        number of heads used in attention layer
  --num_layers NUM_LAYERS
                        number of model layers
  --walk_length WALK_LENGTH
                        length of each random walk
  --time_gap TIME_GAP   time gap for neighbors to compute node features
  --time_feat_dim TIME_FEAT_DIM
                        dimension of the time embedding
  --position_feat_dim POSITION_FEAT_DIM
                        dimension of the position embedding
  --edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}
                        how memory of EdgeBank works
  --time_window_mode {fixed_proportion,repeat_interval}
                        how to select the time window size for time window
                        memory
  --patch_size PATCH_SIZE
                        patch size
  --channel_embedding_dim CHANNEL_EMBEDDING_DIM
                        dimension of each channel embedding
  --max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH
                        maximal length of the input sequence of each node
  --learning_rate LEARNING_RATE
                        learning rate
  --dropout DROPOUT     dropout rate
  --num_epochs NUM_EPOCHS
                        number of epochs
  --optimizer {SGD,Adam,RMSprop,AdamW}
                        name of optimizer
  --weight_decay WEIGHT_DECAY
                        weight decay
  --patience PATIENCE   patience for early stopping
  --val_ratio VAL_RATIO
                        ratio of validation set
  --test_ratio TEST_RATIO
                        ratio of test set
  --num_runs NUM_RUNS   number of runs
  --test_interval_epochs TEST_INTERVAL_EPOCHS
                        how many epochs to perform testing once
  --negative_sample_strategy {random,historical,inductive}
                        strategy for the negative edge sampling
  --load_best_configs   whether to load the best configurations
  --use_wandb USE_WANDB
                        do you want to track this run using wandb? If arg is
                        `no`, then don`t track. Else, track using wandb and
                        name the run `run_name_dataset`
  --use_ROPe            Use ROPe embeddings for DecoLP
  --t1_factor_of_t2 T1_FACTOR_OF_T2
                        t1 factor of t2 when t2 is 0.04 of total time
  --use_init_method     Use new init method
  --emb_proj            Use embedding projection before weighted average
  --init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}
  --clip CLIP           clip val for grad of time-transformation operation
True
  0%|          | 0/95577 [00:00<?, ?it/s]usage: Interface for the link prediction task [-h]
                                              [--dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}]
                                              [--batch_size BATCH_SIZE]
                                              [--model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}]
                                              [--gpu GPU]
                                              [--num_neighbors NUM_NEIGHBORS]
                                              [--sample_neighbor_strategy {uniform,recent,time_interval_aware}]
                                              [--time_scaling_factor TIME_SCALING_FACTOR]
                                              [--num_walk_heads NUM_WALK_HEADS]
                                              [--num_heads NUM_HEADS]
                                              [--num_layers NUM_LAYERS]
                                              [--walk_length WALK_LENGTH]
                                              [--time_gap TIME_GAP]
                                              [--time_feat_dim TIME_FEAT_DIM]
                                              [--position_feat_dim POSITION_FEAT_DIM]
                                              [--edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}]
                                              [--time_window_mode {fixed_proportion,repeat_interval}]
                                              [--patch_size PATCH_SIZE]
                                              [--channel_embedding_dim CHANNEL_EMBEDDING_DIM]
                                              [--max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH]
                                              [--learning_rate LEARNING_RATE]
                                              [--dropout DROPOUT]
                                              [--num_epochs NUM_EPOCHS]
                                              [--optimizer {SGD,Adam,RMSprop,AdamW}]
                                              [--weight_decay WEIGHT_DECAY]
                                              [--patience PATIENCE]
                                              [--val_ratio VAL_RATIO]
                                              [--test_ratio TEST_RATIO]
                                              [--num_runs NUM_RUNS]
                                              [--test_interval_epochs TEST_INTERVAL_EPOCHS]
                                              [--negative_sample_strategy {random,historical,inductive}]
                                              [--load_best_configs]
                                              [--use_wandb USE_WANDB]
                                              [--use_ROPe]
                                              [--t1_factor_of_t2 T1_FACTOR_OF_T2]
                                              [--use_init_method] [--emb_proj]
                                              [--init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}]
                                              [--clip CLIP]
Interface for the link prediction task: error: argument --dataset_name: expected one argument
True
usage: Interface for the link prediction task [-h]
                                              [--dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}]
                                              [--batch_size BATCH_SIZE]
                                              [--model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}]
                                              [--gpu GPU]
                                              [--num_neighbors NUM_NEIGHBORS]
                                              [--sample_neighbor_strategy {uniform,recent,time_interval_aware}]
                                              [--time_scaling_factor TIME_SCALING_FACTOR]
                                              [--num_walk_heads NUM_WALK_HEADS]
                                              [--num_heads NUM_HEADS]
                                              [--num_layers NUM_LAYERS]
                                              [--walk_length WALK_LENGTH]
                                              [--time_gap TIME_GAP]
                                              [--time_feat_dim TIME_FEAT_DIM]
                                              [--position_feat_dim POSITION_FEAT_DIM]
                                              [--edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}]
                                              [--time_window_mode {fixed_proportion,repeat_interval}]
                                              [--patch_size PATCH_SIZE]
                                              [--channel_embedding_dim CHANNEL_EMBEDDING_DIM]
                                              [--max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH]
                                              [--learning_rate LEARNING_RATE]
                                              [--dropout DROPOUT]
                                              [--num_epochs NUM_EPOCHS]
                                              [--optimizer {SGD,Adam,RMSprop,AdamW}]
                                              [--weight_decay WEIGHT_DECAY]
                                              [--patience PATIENCE]
                                              [--val_ratio VAL_RATIO]
                                              [--test_ratio TEST_RATIO]
                                              [--num_runs NUM_RUNS]
                                              [--test_interval_epochs TEST_INTERVAL_EPOCHS]
                                              [--negative_sample_strategy {random,historical,inductive}]
                                              [--load_best_configs]
                                              [--use_wandb USE_WANDB]
                                              [--use_ROPe]
                                              [--t1_factor_of_t2 T1_FACTOR_OF_T2]
                                              [--use_init_method] [--emb_proj]
                                              [--init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}]
                                              [--clip CLIP]

options:
  -h, --help            show this help message and exit
  --dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}
                        dataset to be used
  --batch_size BATCH_SIZE
                        batch size
  --model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}
                        name of the model, note that EdgeBank is only
                        applicable for evaluation
  --gpu GPU             number of gpu to use
  --num_neighbors NUM_NEIGHBORS
                        number of neighbors to sample for each node
  --sample_neighbor_strategy {uniform,recent,time_interval_aware}
                        how to sample historical neighbors
  --time_scaling_factor TIME_SCALING_FACTOR
                        the hyperparameter that controls the sampling
                        preference with time interval, a large
                        time_scaling_factor tends to sample more on recent
                        links, 0.0 corresponds to uniform sampling, it works
                        when sample_neighbor_strategy == time_interval_aware
  --num_walk_heads NUM_WALK_HEADS
                        number of heads used for the attention in walk encoder
  --num_heads NUM_HEADS
                        number of heads used in attention layer
  --num_layers NUM_LAYERS
                        number of model layers
  --walk_length WALK_LENGTH
                        length of each random walk
  --time_gap TIME_GAP   time gap for neighbors to compute node features
  --time_feat_dim TIME_FEAT_DIM
                        dimension of the time embedding
  --position_feat_dim POSITION_FEAT_DIM
                        dimension of the position embedding
  --edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}
                        how memory of EdgeBank works
  --time_window_mode {fixed_proportion,repeat_interval}
                        how to select the time window size for time window
                        memory
  --patch_size PATCH_SIZE
                        patch size
  --channel_embedding_dim CHANNEL_EMBEDDING_DIM
                        dimension of each channel embedding
  --max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH
                        maximal length of the input sequence of each node
  --learning_rate LEARNING_RATE
                        learning rate
  --dropout DROPOUT     dropout rate
  --num_epochs NUM_EPOCHS
                        number of epochs
  --optimizer {SGD,Adam,RMSprop,AdamW}
                        name of optimizer
  --weight_decay WEIGHT_DECAY
                        weight decay
  --patience PATIENCE   patience for early stopping
  --val_ratio VAL_RATIO
                        ratio of validation set
  --test_ratio TEST_RATIO
                        ratio of test set
  --num_runs NUM_RUNS   number of runs
  --test_interval_epochs TEST_INTERVAL_EPOCHS
                        how many epochs to perform testing once
  --negative_sample_strategy {random,historical,inductive}
                        strategy for the negative edge sampling
  --load_best_configs   whether to load the best configurations
  --use_wandb USE_WANDB
                        do you want to track this run using wandb? If arg is
                        `no`, then don`t track. Else, track using wandb and
                        name the run `run_name_dataset`
  --use_ROPe            Use ROPe embeddings for DecoLP
  --t1_factor_of_t2 T1_FACTOR_OF_T2
                        t1 factor of t2 when t2 is 0.04 of total time
  --use_init_method     Use new init method
  --emb_proj            Use embedding projection before weighted average
  --init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}
  --clip CLIP           clip val for grad of time-transformation operation
Epoch: 21, train for the 15-th batch, train loss: 0.48910006880760193:   9%|█          | 14/151 [00:03<00:30,  4.55it/s]Epoch: 21, train for the 15-th batch, train loss: 0.48910006880760193:  10%|█          | 15/151 [00:03<00:29,  4.61it/s]  9%|▊         | 8235/95577 [00:00<00:01, 82348.12it/s]Epoch: 10, train for the 59-th batch, train loss: 0.4329740107059479:  40%|████▊       | 58/146 [00:35<00:57,  1.53it/s]Epoch: 10, train for the 59-th batch, train loss: 0.4329740107059479:  40%|████▊       | 59/146 [00:35<00:50,  1.73it/s] 17%|█▋        | 16571/95577 [00:00<00:00, 82943.15it/s]Epoch: 21, train for the 16-th batch, train loss: 0.5369924306869507:  10%|█▏          | 15/151 [00:03<00:29,  4.61it/s]Epoch: 21, train for the 16-th batch, train loss: 0.5369924306869507:  11%|█▎          | 16/151 [00:03<00:28,  4.67it/s]usage: Interface for the link prediction task [-h]
                                              [--dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}]
                                              [--batch_size BATCH_SIZE]
                                              [--model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}]
                                              [--gpu GPU]
                                              [--num_neighbors NUM_NEIGHBORS]
                                              [--sample_neighbor_strategy {uniform,recent,time_interval_aware}]
                                              [--time_scaling_factor TIME_SCALING_FACTOR]
                                              [--num_walk_heads NUM_WALK_HEADS]
                                              [--num_heads NUM_HEADS]
                                              [--num_layers NUM_LAYERS]
                                              [--walk_length WALK_LENGTH]
                                              [--time_gap TIME_GAP]
                                              [--time_feat_dim TIME_FEAT_DIM]
                                              [--position_feat_dim POSITION_FEAT_DIM]
                                              [--edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}]
                                              [--time_window_mode {fixed_proportion,repeat_interval}]
                                              [--patch_size PATCH_SIZE]
                                              [--channel_embedding_dim CHANNEL_EMBEDDING_DIM]
                                              [--max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH]
                                              [--learning_rate LEARNING_RATE]
                                              [--dropout DROPOUT]
                                              [--num_epochs NUM_EPOCHS]
                                              [--optimizer {SGD,Adam,RMSprop,AdamW}]
                                              [--weight_decay WEIGHT_DECAY]
                                              [--patience PATIENCE]
                                              [--val_ratio VAL_RATIO]
                                              [--test_ratio TEST_RATIO]
                                              [--num_runs NUM_RUNS]
                                              [--test_interval_epochs TEST_INTERVAL_EPOCHS]
                                              [--negative_sample_strategy {random,historical,inductive}]
                                              [--load_best_configs]
                                              [--use_wandb USE_WANDB]
                                              [--use_ROPe]
                                              [--t1_factor_of_t2 T1_FACTOR_OF_T2]
                                              [--use_init_method] [--emb_proj]
                                              [--init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}]
                                              [--clip CLIP]
Interface for the link prediction task: error: argument --dataset_name: expected one argument
True
usage: Interface for the link prediction task [-h]
                                              [--dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}]
                                              [--batch_size BATCH_SIZE]
                                              [--model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}]
                                              [--gpu GPU]
                                              [--num_neighbors NUM_NEIGHBORS]
                                              [--sample_neighbor_strategy {uniform,recent,time_interval_aware}]
                                              [--time_scaling_factor TIME_SCALING_FACTOR]
                                              [--num_walk_heads NUM_WALK_HEADS]
                                              [--num_heads NUM_HEADS]
                                              [--num_layers NUM_LAYERS]
                                              [--walk_length WALK_LENGTH]
                                              [--time_gap TIME_GAP]
                                              [--time_feat_dim TIME_FEAT_DIM]
                                              [--position_feat_dim POSITION_FEAT_DIM]
                                              [--edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}]
                                              [--time_window_mode {fixed_proportion,repeat_interval}]
                                              [--patch_size PATCH_SIZE]
                                              [--channel_embedding_dim CHANNEL_EMBEDDING_DIM]
                                              [--max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH]
                                              [--learning_rate LEARNING_RATE]
                                              [--dropout DROPOUT]
                                              [--num_epochs NUM_EPOCHS]
                                              [--optimizer {SGD,Adam,RMSprop,AdamW}]
                                              [--weight_decay WEIGHT_DECAY]
                                              [--patience PATIENCE]
                                              [--val_ratio VAL_RATIO]
                                              [--test_ratio TEST_RATIO]
                                              [--num_runs NUM_RUNS]
                                              [--test_interval_epochs TEST_INTERVAL_EPOCHS]
                                              [--negative_sample_strategy {random,historical,inductive}]
                                              [--load_best_configs]
                                              [--use_wandb USE_WANDB]
                                              [--use_ROPe]
                                              [--t1_factor_of_t2 T1_FACTOR_OF_T2]
                                              [--use_init_method] [--emb_proj]
                                              [--init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}]
                                              [--clip CLIP]

options:
  -h, --help            show this help message and exit
  --dataset_name {wikipedia,reddit,mooc,lastfm,myket,enron,SocialEvo,uci,Flights,CanParl,USLegis,UNtrade,UNvote,Contacts,ia-slashdot-reply-dir,ia-digg-reply,ia-retweet-pol,ia-escorts-dynamic,ia-movielens-user2tags-10m,ia-reality-call}
                        dataset to be used
  --batch_size BATCH_SIZE
                        batch size
  --model_name {JODIE,DyRep,TGAT,TGN,CAWN,EdgeBank,TCL,GraphMixer,DyGFormer,DecoLP}
                        name of the model, note that EdgeBank is only
                        applicable for evaluation
  --gpu GPU             number of gpu to use
  --num_neighbors NUM_NEIGHBORS
                        number of neighbors to sample for each node
  --sample_neighbor_strategy {uniform,recent,time_interval_aware}
                        how to sample historical neighbors
  --time_scaling_factor TIME_SCALING_FACTOR
                        the hyperparameter that controls the sampling
                        preference with time interval, a large
                        time_scaling_factor tends to sample more on recent
                        links, 0.0 corresponds to uniform sampling, it works
                        when sample_neighbor_strategy == time_interval_aware
  --num_walk_heads NUM_WALK_HEADS
                        number of heads used for the attention in walk encoder
  --num_heads NUM_HEADS
                        number of heads used in attention layer
  --num_layers NUM_LAYERS
                        number of model layers
  --walk_length WALK_LENGTH
                        length of each random walk
  --time_gap TIME_GAP   time gap for neighbors to compute node features
  --time_feat_dim TIME_FEAT_DIM
                        dimension of the time embedding
  --position_feat_dim POSITION_FEAT_DIM
                        dimension of the position embedding
  --edge_bank_memory_mode {unlimited_memory,time_window_memory,repeat_threshold_memory}
                        how memory of EdgeBank works
  --time_window_mode {fixed_proportion,repeat_interval}
                        how to select the time window size for time window
                        memory
  --patch_size PATCH_SIZE
                        patch size
  --channel_embedding_dim CHANNEL_EMBEDDING_DIM
                        dimension of each channel embedding
  --max_input_sequence_length MAX_INPUT_SEQUENCE_LENGTH
                        maximal length of the input sequence of each node
  --learning_rate LEARNING_RATE
                        learning rate
  --dropout DROPOUT     dropout rate
  --num_epochs NUM_EPOCHS
                        number of epochs
  --optimizer {SGD,Adam,RMSprop,AdamW}
                        name of optimizer
  --weight_decay WEIGHT_DECAY
                        weight decay
  --patience PATIENCE   patience for early stopping
  --val_ratio VAL_RATIO
                        ratio of validation set
  --test_ratio TEST_RATIO
                        ratio of test set
  --num_runs NUM_RUNS   number of runs
  --test_interval_epochs TEST_INTERVAL_EPOCHS
                        how many epochs to perform testing once
  --negative_sample_strategy {random,historical,inductive}
                        strategy for the negative edge sampling
  --load_best_configs   whether to load the best configurations
  --use_wandb USE_WANDB
                        do you want to track this run using wandb? If arg is
                        `no`, then don`t track. Else, track using wandb and
                        name the run `run_name_dataset`
  --use_ROPe            Use ROPe embeddings for DecoLP
  --t1_factor_of_t2 T1_FACTOR_OF_T2
                        t1 factor of t2 when t2 is 0.04 of total time
  --use_init_method     Use new init method
  --emb_proj            Use embedding projection before weighted average
  --init_weights {degree,log-degree,time-exp,time-linear,time-fourier,time-mlp,time-mlp2,time-3unite}
  --clip CLIP           clip val for grad of time-transformation operation
Epoch: 6, train for the 214-th batch, train loss: 0.6461380124092102:  90%|█████████▉ | 213/237 [02:04<00:15,  1.56it/s]Epoch: 6, train for the 214-th batch, train loss: 0.6461380124092102:  90%|█████████▉ | 214/237 [02:04<00:14,  1.59it/s]Epoch: 21, train for the 17-th batch, train loss: 0.6659473776817322:  11%|█▎          | 16/151 [00:03<00:28,  4.67it/s]Epoch: 21, train for the 17-th batch, train loss: 0.6659473776817322:  11%|█▎          | 17/151 [00:03<00:28,  4.71it/s]Epoch: 10, train for the 60-th batch, train loss: 0.49858319759368896:  40%|████▍      | 59/146 [00:35<00:50,  1.73it/s]Epoch: 10, train for the 60-th batch, train loss: 0.49858319759368896:  41%|████▌      | 60/146 [00:35<00:43,  1.96it/s]Epoch: 4, train for the 214-th batch, train loss: 0.3922153413295746:  56%|██████     | 213/383 [02:06<01:48,  1.57it/s]Epoch: 4, train for the 214-th batch, train loss: 0.3922153413295746:  56%|██████▏    | 214/383 [02:06<01:45,  1.60it/s]Epoch: 11, train for the 110-th batch, train loss: 0.14883071184158325:  92%|████████▏| 109/119 [01:06<00:05,  1.73it/s]Epoch: 11, train for the 110-th batch, train loss: 0.14883071184158325:  92%|████████▎| 110/119 [01:06<00:06,  1.46it/s] 26%|██▌       | 24866/95577 [00:00<00:01, 40346.23it/s]Epoch: 21, train for the 18-th batch, train loss: 0.6577600240707397:  11%|█▎          | 17/151 [00:03<00:28,  4.71it/s]Epoch: 21, train for the 18-th batch, train loss: 0.6577600240707397:  12%|█▍          | 18/151 [00:03<00:28,  4.73it/s] 32%|███▏      | 30517/95577 [00:00<00:01, 40751.07it/s] 37%|███▋      | 35583/95577 [00:00<00:01, 40826.43it/s]Epoch: 21, train for the 19-th batch, train loss: 0.5562927722930908:  12%|█▍          | 18/151 [00:03<00:28,  4.73it/s]Epoch: 21, train for the 19-th batch, train loss: 0.5562927722930908:  13%|█▌          | 19/151 [00:03<00:28,  4.71it/s]Epoch: 4, train for the 215-th batch, train loss: 0.38393446803092957:  56%|█████▌    | 214/383 [02:06<01:45,  1.60it/s]Epoch: 4, train for the 215-th batch, train loss: 0.38393446803092957:  56%|█████▌    | 215/383 [02:06<01:34,  1.79it/s] 43%|████▎     | 41225/95577 [00:00<00:01, 44654.14it/s] 49%|████▉     | 47214/95577 [00:00<00:00, 48609.20it/s]Epoch: 21, train for the 20-th batch, train loss: 0.4851393401622772:  13%|█▌          | 19/151 [00:04<00:28,  4.71it/s]Epoch: 21, train for the 20-th batch, train loss: 0.4851393401622772:  13%|█▌          | 20/151 [00:04<00:27,  4.72it/s]Epoch: 10, train for the 61-th batch, train loss: 0.5267852544784546:  41%|████▉       | 60/146 [00:36<00:43,  1.96it/s]Epoch: 11, train for the 111-th batch, train loss: 0.17789894342422485:  92%|████████▎| 110/119 [01:06<00:06,  1.46it/s]Epoch: 10, train for the 61-th batch, train loss: 0.5267852544784546:  42%|█████       | 61/146 [00:36<00:47,  1.80it/s]Epoch: 11, train for the 111-th batch, train loss: 0.17789894342422485:  93%|████████▍| 111/119 [01:06<00:05,  1.49it/s] 55%|█████▍    | 52559/95577 [00:01<00:01, 42422.97it/s]Epoch: 4, train for the 216-th batch, train loss: 0.40497496724128723:  56%|█████▌    | 215/383 [02:06<01:34,  1.79it/s]Epoch: 4, train for the 216-th batch, train loss: 0.40497496724128723:  56%|█████▋    | 216/383 [02:06<01:19,  2.10it/s] 60%|██████    | 57416/95577 [00:01<00:00, 43942.64it/s]Epoch: 21, train for the 21-th batch, train loss: 0.640283465385437:  13%|█▋           | 20/151 [00:04<00:27,  4.72it/s]Epoch: 21, train for the 21-th batch, train loss: 0.640283465385437:  14%|█▊           | 21/151 [00:04<00:27,  4.66it/s]Epoch: 6, train for the 215-th batch, train loss: 0.6277021169662476:  90%|█████████▉ | 214/237 [02:05<00:14,  1.59it/s]Epoch: 6, train for the 215-th batch, train loss: 0.6277021169662476:  91%|█████████▉ | 215/237 [02:05<00:16,  1.33it/s] 65%|██████▌   | 62147/95577 [00:01<00:00, 43769.39it/s] 70%|███████   | 66907/95577 [00:01<00:00, 44791.23it/s]Epoch: 21, train for the 22-th batch, train loss: 0.47456783056259155:  14%|█▌         | 21/151 [00:04<00:27,  4.66it/s]Epoch: 21, train for the 22-th batch, train loss: 0.47456783056259155:  15%|█▌         | 22/151 [00:04<00:27,  4.71it/s] 75%|███████▍  | 71561/95577 [00:01<00:00, 39099.03it/s]Epoch: 4, train for the 217-th batch, train loss: 0.45380640029907227:  56%|█████▋    | 216/383 [02:07<01:19,  2.10it/s]Epoch: 4, train for the 217-th batch, train loss: 0.45380640029907227:  57%|█████▋    | 217/383 [02:07<01:19,  2.08it/s]Epoch: 21, train for the 23-th batch, train loss: 0.6080840826034546:  15%|█▋          | 22/151 [00:04<00:27,  4.71it/s]Epoch: 21, train for the 23-th batch, train loss: 0.6080840826034546:  15%|█▊          | 23/151 [00:04<00:27,  4.67it/s] 81%|████████  | 76944/95577 [00:01<00:00, 39996.65it/s]Epoch: 10, train for the 62-th batch, train loss: 0.4894575774669647:  42%|█████       | 61/146 [00:36<00:47,  1.80it/s]Epoch: 11, train for the 112-th batch, train loss: 0.15142697095870972:  93%|████████▍| 111/119 [01:07<00:05,  1.49it/s]Epoch: 10, train for the 62-th batch, train loss: 0.4894575774669647:  42%|█████       | 62/146 [00:36<00:49,  1.70it/s]Epoch: 11, train for the 112-th batch, train loss: 0.15142697095870972:  94%|████████▍| 112/119 [01:07<00:04,  1.49it/s] 85%|████████▌ | 81457/95577 [00:01<00:00, 39463.50it/s]Epoch: 21, train for the 24-th batch, train loss: 0.46336326003074646:  15%|█▋         | 23/151 [00:05<00:27,  4.67it/s]Epoch: 21, train for the 24-th batch, train loss: 0.46336326003074646:  16%|█▋         | 24/151 [00:05<00:27,  4.70it/s]Epoch: 6, train for the 216-th batch, train loss: 0.635517418384552:  91%|██████████▉ | 215/237 [02:06<00:16,  1.33it/s]Epoch: 6, train for the 216-th batch, train loss: 0.635517418384552:  91%|██████████▉ | 216/237 [02:06<00:14,  1.42it/s] 91%|█████████▏| 87295/95577 [00:01<00:00, 42666.86it/s] 97%|█████████▋| 92261/95577 [00:02<00:00, 41829.62it/s]Epoch: 21, train for the 25-th batch, train loss: 0.533741295337677:  16%|██           | 24/151 [00:05<00:27,  4.70it/s]Epoch: 21, train for the 25-th batch, train loss: 0.533741295337677:  17%|██▏          | 25/151 [00:05<00:26,  4.71it/s]100%|█████████▉| 95576/95577 [00:02<00:00, 43901.15it/s]
Epoch: 4, train for the 218-th batch, train loss: 0.5033464431762695:  57%|██████▏    | 217/383 [02:07<01:19,  2.08it/s]Epoch: 4, train for the 218-th batch, train loss: 0.5033464431762695:  57%|██████▎    | 218/383 [02:07<01:24,  1.95it/s]Epoch: 21, train for the 26-th batch, train loss: 0.551475465297699:  17%|██▏          | 25/151 [00:05<00:26,  4.71it/s]Epoch: 21, train for the 26-th batch, train loss: 0.551475465297699:  17%|██▏          | 26/151 [00:05<00:26,  4.71it/s]  0%|          | 0/95577 [00:00<?, ?it/s]100%|██████████| 95577/95577 [00:00<00:00, 1858924.81it/s]
Epoch: 11, train for the 113-th batch, train loss: 0.15140151977539062:  94%|████████▍| 112/119 [01:08<00:04,  1.49it/s]Epoch: 10, train for the 63-th batch, train loss: 0.4885687828063965:  42%|█████       | 62/146 [00:37<00:49,  1.70it/s]Epoch: 11, train for the 113-th batch, train loss: 0.15140151977539062:  95%|████████▌| 113/119 [01:08<00:04,  1.50it/s]Epoch: 10, train for the 63-th batch, train loss: 0.4885687828063965:  43%|█████▏      | 63/146 [00:37<00:50,  1.63it/s]Epoch: 6, train for the 217-th batch, train loss: 0.6045129299163818:  91%|██████████ | 216/237 [02:06<00:14,  1.42it/s]Epoch: 6, train for the 217-th batch, train loss: 0.6045129299163818:  92%|██████████ | 217/237 [02:06<00:13,  1.50it/s]Epoch: 21, train for the 27-th batch, train loss: 0.6020739078521729:  17%|██          | 26/151 [00:05<00:26,  4.71it/s]Epoch: 21, train for the 27-th batch, train loss: 0.6020739078521729:  18%|██▏         | 27/151 [00:05<00:26,  4.67it/s]Epoch: 4, train for the 219-th batch, train loss: 0.3530447781085968:  57%|██████▎    | 218/383 [02:08<01:24,  1.95it/s]Epoch: 4, train for the 219-th batch, train loss: 0.3530447781085968:  57%|██████▎    | 219/383 [02:08<01:27,  1.87it/s]Epoch: 21, train for the 28-th batch, train loss: 0.40297114849090576:  18%|█▉         | 27/151 [00:05<00:26,  4.67it/s]Epoch: 21, train for the 28-th batch, train loss: 0.40297114849090576:  19%|██         | 28/151 [00:05<00:29,  4.18it/s]Epoch: 6, train for the 218-th batch, train loss: 0.629699170589447:  92%|██████████▉ | 217/237 [02:07<00:13,  1.50it/s]Epoch: 6, train for the 218-th batch, train loss: 0.629699170589447:  92%|███████████ | 218/237 [02:07<00:12,  1.55it/s]Epoch: 21, train for the 29-th batch, train loss: 0.7077204585075378:  19%|██▏         | 28/151 [00:06<00:29,  4.18it/s]Epoch: 21, train for the 29-th batch, train loss: 0.7077204585075378:  19%|██▎         | 29/151 [00:06<00:29,  4.10it/s]Epoch: 11, train for the 114-th batch, train loss: 0.16101935505867004:  95%|████████▌| 113/119 [01:08<00:04,  1.50it/s]Epoch: 10, train for the 64-th batch, train loss: 0.540757417678833:  43%|█████▌       | 63/146 [00:38<00:50,  1.63it/s]Epoch: 11, train for the 114-th batch, train loss: 0.16101935505867004:  96%|████████▌| 114/119 [01:08<00:03,  1.50it/s]Epoch: 10, train for the 64-th batch, train loss: 0.540757417678833:  44%|█████▋       | 64/146 [00:38<00:51,  1.59it/s]Epoch: 21, train for the 30-th batch, train loss: 0.7062812447547913:  19%|██▎         | 29/151 [00:06<00:29,  4.10it/s]Epoch: 21, train for the 30-th batch, train loss: 0.7062812447547913:  20%|██▍         | 30/151 [00:06<00:28,  4.21it/s]Epoch: 4, train for the 220-th batch, train loss: 0.3703737258911133:  57%|██████▎    | 219/383 [02:08<01:27,  1.87it/s]Epoch: 4, train for the 220-th batch, train loss: 0.3703737258911133:  57%|██████▎    | 220/383 [02:08<01:29,  1.81it/s]Epoch: 21, train for the 31-th batch, train loss: 0.7119399905204773:  20%|██▍         | 30/151 [00:06<00:28,  4.21it/s]Epoch: 21, train for the 31-th batch, train loss: 0.7119399905204773:  21%|██▍         | 31/151 [00:06<00:27,  4.29it/s]Epoch: 6, train for the 219-th batch, train loss: 0.6093390583992004:  92%|██████████ | 218/237 [02:08<00:12,  1.55it/s]Epoch: 6, train for the 219-th batch, train loss: 0.6093390583992004:  92%|██████████▏| 219/237 [02:08<00:11,  1.59it/s]Epoch: 21, train for the 32-th batch, train loss: 0.6861733198165894:  21%|██▍         | 31/151 [00:06<00:27,  4.29it/s]Epoch: 21, train for the 32-th batch, train loss: 0.6861733198165894:  21%|██▌         | 32/151 [00:06<00:27,  4.36it/s]Epoch: 11, train for the 115-th batch, train loss: 0.15333077311515808:  96%|████████▌| 114/119 [01:09<00:03,  1.50it/s]Epoch: 10, train for the 65-th batch, train loss: 0.6010843515396118:  44%|█████▎      | 64/146 [00:38<00:51,  1.59it/s]Epoch: 11, train for the 115-th batch, train loss: 0.15333077311515808:  97%|████████▋| 115/119 [01:09<00:02,  1.50it/s]Epoch: 10, train for the 65-th batch, train loss: 0.6010843515396118:  45%|█████▎      | 65/146 [00:38<00:51,  1.56it/s]Epoch: 4, train for the 221-th batch, train loss: 0.4641576409339905:  57%|██████▎    | 220/383 [02:09<01:29,  1.81it/s]Epoch: 4, train for the 221-th batch, train loss: 0.4641576409339905:  58%|██████▎    | 221/383 [02:09<01:30,  1.78it/s]Epoch: 21, train for the 33-th batch, train loss: 0.6663790941238403:  21%|██▌         | 32/151 [00:07<00:27,  4.36it/s]Epoch: 21, train for the 33-th batch, train loss: 0.6663790941238403:  22%|██▌         | 33/151 [00:07<00:26,  4.38it/s]wandb: Currently logged in as: singhayush4499 (fb-graph-proj). Use `wandb login --relogin` to force relogin
Epoch: 21, train for the 34-th batch, train loss: 0.6580589413642883:  22%|██▌         | 33/151 [00:07<00:26,  4.38it/s]Epoch: 21, train for the 34-th batch, train loss: 0.6580589413642883:  23%|██▋         | 34/151 [00:07<00:26,  4.43it/s]Epoch: 6, train for the 220-th batch, train loss: 0.6391408443450928:  92%|██████████▏| 219/237 [02:08<00:11,  1.59it/s]Epoch: 6, train for the 220-th batch, train loss: 0.6391408443450928:  93%|██████████▏| 220/237 [02:08<00:10,  1.63it/s]Epoch: 10, train for the 66-th batch, train loss: 0.5483182072639465:  45%|█████▎      | 65/146 [00:39<00:51,  1.56it/s]Epoch: 11, train for the 116-th batch, train loss: 0.12340404093265533:  97%|████████▋| 115/119 [01:10<00:02,  1.50it/s]Epoch: 10, train for the 66-th batch, train loss: 0.5483182072639465:  45%|█████▍      | 66/146 [00:39<00:51,  1.55it/s]Epoch: 11, train for the 116-th batch, train loss: 0.12340404093265533:  97%|████████▊| 116/119 [01:10<00:01,  1.50it/s]Epoch: 21, train for the 35-th batch, train loss: 0.6640051603317261:  23%|██▋         | 34/151 [00:07<00:26,  4.43it/s]Epoch: 21, train for the 35-th batch, train loss: 0.6640051603317261:  23%|██▊         | 35/151 [00:07<00:26,  4.46it/s]Epoch: 4, train for the 222-th batch, train loss: 0.45656371116638184:  58%|█████▊    | 221/383 [02:10<01:30,  1.78it/s]Epoch: 4, train for the 222-th batch, train loss: 0.45656371116638184:  58%|█████▊    | 222/383 [02:10<01:31,  1.76it/s]Epoch: 21, train for the 36-th batch, train loss: 0.4672733247280121:  23%|██▊         | 35/151 [00:07<00:26,  4.46it/s]Epoch: 21, train for the 36-th batch, train loss: 0.4672733247280121:  24%|██▊         | 36/151 [00:07<00:25,  4.56it/s]Epoch: 6, train for the 221-th batch, train loss: 0.6179283261299133:  93%|██████████▏| 220/237 [02:09<00:10,  1.63it/s]Epoch: 6, train for the 221-th batch, train loss: 0.6179283261299133:  93%|██████████▎| 221/237 [02:09<00:09,  1.65it/s]Epoch: 21, train for the 37-th batch, train loss: 0.6185463070869446:  24%|██▊         | 36/151 [00:07<00:25,  4.56it/s]Epoch: 21, train for the 37-th batch, train loss: 0.6185463070869446:  25%|██▉         | 37/151 [00:07<00:24,  4.56it/s]Epoch: 21, train for the 38-th batch, train loss: 0.5352011919021606:  25%|██▉         | 37/151 [00:08<00:24,  4.56it/s]Epoch: 21, train for the 38-th batch, train loss: 0.5352011919021606:  25%|███         | 38/151 [00:08<00:24,  4.60it/s]wandb: - Waiting for wandb.init()...Epoch: 10, train for the 67-th batch, train loss: 0.5410985350608826:  45%|█████▍      | 66/146 [00:40<00:51,  1.55it/s]Epoch: 10, train for the 67-th batch, train loss: 0.5410985350608826:  46%|█████▌      | 67/146 [00:40<00:51,  1.54it/s]Epoch: 11, train for the 117-th batch, train loss: 0.19886407256126404:  97%|████████▊| 116/119 [01:10<00:01,  1.50it/s]Epoch: 11, train for the 117-th batch, train loss: 0.19886407256126404:  98%|████████▊| 117/119 [01:10<00:01,  1.50it/s]Epoch: 4, train for the 223-th batch, train loss: 0.3843275010585785:  58%|██████▍    | 222/383 [02:10<01:31,  1.76it/s]Epoch: 4, train for the 223-th batch, train loss: 0.3843275010585785:  58%|██████▍    | 223/383 [02:10<01:31,  1.74it/s]Epoch: 21, train for the 39-th batch, train loss: 0.6396799087524414:  25%|███         | 38/151 [00:08<00:24,  4.60it/s]Epoch: 21, train for the 39-th batch, train loss: 0.6396799087524414:  26%|███         | 39/151 [00:08<00:24,  4.56it/s]Epoch: 6, train for the 222-th batch, train loss: 0.6148912906646729:  93%|██████████▎| 221/237 [02:09<00:09,  1.65it/s]Epoch: 6, train for the 222-th batch, train loss: 0.6148912906646729:  94%|██████████▎| 222/237 [02:09<00:09,  1.66it/s]Epoch: 21, train for the 40-th batch, train loss: 0.49248945713043213:  26%|██▊        | 39/151 [00:08<00:24,  4.56it/s]Epoch: 21, train for the 40-th batch, train loss: 0.49248945713043213:  26%|██▉        | 40/151 [00:08<00:24,  4.60it/s]wandb: \ Waiting for wandb.init()...Epoch: 21, train for the 41-th batch, train loss: 0.6118144989013672:  26%|███▏        | 40/151 [00:08<00:24,  4.60it/s]Epoch: 21, train for the 41-th batch, train loss: 0.6118144989013672:  27%|███▎        | 41/151 [00:08<00:24,  4.56it/s]Epoch: 4, train for the 224-th batch, train loss: 0.49407103657722473:  58%|█████▊    | 223/383 [02:11<01:31,  1.74it/s]Epoch: 4, train for the 224-th batch, train loss: 0.49407103657722473:  58%|█████▊    | 224/383 [02:11<01:32,  1.73it/s]Epoch: 11, train for the 118-th batch, train loss: 0.12010714411735535:  98%|████████▊| 117/119 [01:11<00:01,  1.50it/s]Epoch: 10, train for the 68-th batch, train loss: 0.5243031978607178:  46%|█████▌      | 67/146 [00:40<00:51,  1.54it/s]Epoch: 11, train for the 118-th batch, train loss: 0.12010714411735535:  99%|████████▉| 118/119 [01:11<00:00,  1.50it/s]Epoch: 10, train for the 68-th batch, train loss: 0.5243031978607178:  47%|█████▌      | 68/146 [00:40<00:51,  1.52it/s]Epoch: 21, train for the 42-th batch, train loss: 0.412665992975235:  27%|███▌         | 41/151 [00:09<00:24,  4.56it/s]Epoch: 21, train for the 42-th batch, train loss: 0.412665992975235:  28%|███▌         | 42/151 [00:09<00:23,  4.61it/s]wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/ayush/DyGLib/wandb/run-20240407_160006-qmulxqoa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dygformer-ia-movielens-user2tags-10m-old
wandb: ⭐️ View project at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib
wandb: 🚀 View run at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib/runs/qmulxqoa
INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-movielens-user2tags-10m', batch_size=200, model_name='DyGFormer', gpu=2, num_neighbors=20, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=2, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='old', use_ROPe=False, t1_factor_of_t2=1, use_init_method=False, emb_proj=False, init_weights=None, clip=1.0, device='cuda:2', seed=0, save_model_name='DyGFormer_seed0_dygformer-ia-movielens-user2tags-10m-old')
Epoch: 6, train for the 223-th batch, train loss: 0.616410493850708:  94%|███████████▏| 222/237 [02:10<00:09,  1.66it/s]Epoch: 6, train for the 223-th batch, train loss: 0.616410493850708:  94%|███████████▎| 223/237 [02:10<00:08,  1.67it/s]Epoch: 11, train for the 119-th batch, train loss: 0.21028763055801392:  99%|████████▉| 118/119 [01:11<00:00,  1.50it/s]Epoch: 11, train for the 119-th batch, train loss: 0.21028763055801392: 100%|█████████| 119/119 [01:11<00:00,  1.81it/s]Epoch: 11, train for the 119-th batch, train loss: 0.21028763055801392: 100%|█████████| 119/119 [01:11<00:00,  1.65it/s]
  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 21, train for the 43-th batch, train loss: 0.6283702850341797:  28%|███▎        | 42/151 [00:09<00:23,  4.61it/s]Epoch: 21, train for the 43-th batch, train loss: 0.6283702850341797:  28%|███▍        | 43/151 [00:09<00:23,  4.59it/s]Epoch: 4, train for the 225-th batch, train loss: 0.4537579119205475:  58%|██████▍    | 224/383 [02:11<01:32,  1.73it/s]Epoch: 4, train for the 225-th batch, train loss: 0.4537579119205475:  59%|██████▍    | 225/383 [02:11<01:31,  1.72it/s]Epoch: 10, train for the 69-th batch, train loss: 0.5248879194259644:  47%|█████▌      | 68/146 [00:41<00:51,  1.52it/s]Epoch: 10, train for the 69-th batch, train loss: 0.5248879194259644:  47%|█████▋      | 69/146 [00:41<00:48,  1.58it/s]evaluate for the 1-th batch, evaluate loss: 0.13144844770431519:   0%|                           | 0/40 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.13144844770431519:   2%|▍                  | 1/40 [00:00<00:12,  3.19it/s]Epoch: 21, train for the 44-th batch, train loss: 0.5688826441764832:  28%|███▍        | 43/151 [00:09<00:23,  4.59it/s]Epoch: 21, train for the 44-th batch, train loss: 0.5688826441764832:  29%|███▍        | 44/151 [00:09<00:23,  4.54it/s]Epoch: 21, train for the 45-th batch, train loss: 0.5857531428337097:  29%|███▍        | 44/151 [00:09<00:23,  4.54it/s]Epoch: 21, train for the 45-th batch, train loss: 0.5857531428337097:  30%|███▌        | 45/151 [00:09<00:23,  4.50it/s]Epoch: 6, train for the 224-th batch, train loss: 0.6650004982948303:  94%|██████████▎| 223/237 [02:11<00:08,  1.67it/s]Epoch: 6, train for the 224-th batch, train loss: 0.6650004982948303:  95%|██████████▍| 224/237 [02:11<00:07,  1.68it/s]evaluate for the 2-th batch, evaluate loss: 0.16486625373363495:   2%|▍                  | 1/40 [00:00<00:12,  3.19it/s]evaluate for the 2-th batch, evaluate loss: 0.16486625373363495:   5%|▉                  | 2/40 [00:00<00:10,  3.56it/s]Epoch: 21, train for the 46-th batch, train loss: 0.6174076795578003:  30%|███▌        | 45/151 [00:09<00:23,  4.50it/s]Epoch: 21, train for the 46-th batch, train loss: 0.6174076795578003:  30%|███▋        | 46/151 [00:09<00:23,  4.49it/s]Epoch: 4, train for the 226-th batch, train loss: 0.47660520672798157:  59%|█████▊    | 225/383 [02:12<01:31,  1.72it/s]Epoch: 4, train for the 226-th batch, train loss: 0.47660520672798157:  59%|█████▉    | 226/383 [02:12<01:31,  1.71it/s]Epoch: 10, train for the 70-th batch, train loss: 0.5505624413490295:  47%|█████▋      | 69/146 [00:42<00:48,  1.58it/s]Epoch: 10, train for the 70-th batch, train loss: 0.5505624413490295:  48%|█████▊      | 70/146 [00:42<00:47,  1.60it/s]evaluate for the 3-th batch, evaluate loss: 0.20310837030410767:   5%|▉                  | 2/40 [00:00<00:10,  3.56it/s]evaluate for the 3-th batch, evaluate loss: 0.20310837030410767:   8%|█▍                 | 3/40 [00:00<00:11,  3.34it/s]Epoch: 21, train for the 47-th batch, train loss: 0.4344829320907593:  30%|███▋        | 46/151 [00:10<00:23,  4.49it/s]Epoch: 21, train for the 47-th batch, train loss: 0.4344829320907593:  31%|███▋        | 47/151 [00:10<00:23,  4.48it/s]Epoch: 6, train for the 225-th batch, train loss: 0.64009690284729:  95%|████████████▎| 224/237 [02:11<00:07,  1.68it/s]Epoch: 6, train for the 225-th batch, train loss: 0.64009690284729:  95%|████████████▎| 225/237 [02:11<00:06,  1.73it/s]evaluate for the 4-th batch, evaluate loss: 0.13269934058189392:   8%|█▍                 | 3/40 [00:01<00:11,  3.34it/s]evaluate for the 4-th batch, evaluate loss: 0.13269934058189392:  10%|█▉                 | 4/40 [00:01<00:10,  3.47it/s]Epoch: 21, train for the 48-th batch, train loss: 0.5524656772613525:  31%|███▋        | 47/151 [00:10<00:23,  4.48it/s]Epoch: 21, train for the 48-th batch, train loss: 0.5524656772613525:  32%|███▊        | 48/151 [00:10<00:22,  4.48it/s]evaluate for the 5-th batch, evaluate loss: 0.17036543786525726:  10%|█▉                 | 4/40 [00:01<00:10,  3.47it/s]evaluate for the 5-th batch, evaluate loss: 0.17036543786525726:  12%|██▍                | 5/40 [00:01<00:10,  3.42it/s]Epoch: 21, train for the 49-th batch, train loss: 0.5831316709518433:  32%|███▊        | 48/151 [00:10<00:22,  4.48it/s]Epoch: 21, train for the 49-th batch, train loss: 0.5831316709518433:  32%|███▉        | 49/151 [00:10<00:23,  4.39it/s]Epoch: 10, train for the 71-th batch, train loss: 0.5494509935379028:  48%|█████▊      | 70/146 [00:42<00:47,  1.60it/s]Epoch: 10, train for the 71-th batch, train loss: 0.5494509935379028:  49%|█████▊      | 71/146 [00:42<00:46,  1.62it/s]Epoch: 4, train for the 227-th batch, train loss: 0.4897662103176117:  59%|██████▍    | 226/383 [02:13<01:31,  1.71it/s]Epoch: 4, train for the 227-th batch, train loss: 0.4897662103176117:  59%|██████▌    | 227/383 [02:13<01:37,  1.60it/s]Epoch: 6, train for the 226-th batch, train loss: 0.6650524735450745:  95%|██████████▍| 225/237 [02:12<00:06,  1.73it/s]Epoch: 6, train for the 226-th batch, train loss: 0.6650524735450745:  95%|██████████▍| 226/237 [02:12<00:06,  1.78it/s]Epoch: 21, train for the 50-th batch, train loss: 0.5790778398513794:  32%|███▉        | 49/151 [00:10<00:23,  4.39it/s]Epoch: 21, train for the 50-th batch, train loss: 0.5790778398513794:  33%|███▉        | 50/151 [00:10<00:23,  4.39it/s]evaluate for the 6-th batch, evaluate loss: 0.15999756753444672:  12%|██▍                | 5/40 [00:01<00:10,  3.42it/s]evaluate for the 6-th batch, evaluate loss: 0.15999756753444672:  15%|██▊                | 6/40 [00:01<00:09,  3.60it/s]Epoch: 21, train for the 51-th batch, train loss: 0.6356495022773743:  33%|███▉        | 50/151 [00:11<00:23,  4.39it/s]Epoch: 21, train for the 51-th batch, train loss: 0.6356495022773743:  34%|████        | 51/151 [00:11<00:23,  4.34it/s]evaluate for the 7-th batch, evaluate loss: 0.10741990059614182:  15%|██▊                | 6/40 [00:02<00:09,  3.60it/s]evaluate for the 7-th batch, evaluate loss: 0.10741990059614182:  18%|███▎               | 7/40 [00:02<00:09,  3.54it/s]Epoch: 10, train for the 72-th batch, train loss: 0.5101757049560547:  49%|█████▊      | 71/146 [00:43<00:46,  1.62it/s]Epoch: 10, train for the 72-th batch, train loss: 0.5101757049560547:  49%|█████▉      | 72/146 [00:43<00:45,  1.64it/s]Epoch: 21, train for the 52-th batch, train loss: 0.6147023439407349:  34%|████        | 51/151 [00:11<00:23,  4.34it/s]Epoch: 21, train for the 52-th batch, train loss: 0.6147023439407349:  34%|████▏       | 52/151 [00:11<00:22,  4.37it/s]Epoch: 4, train for the 228-th batch, train loss: 0.4212868809700012:  59%|██████▌    | 227/383 [02:13<01:37,  1.60it/s]Epoch: 4, train for the 228-th batch, train loss: 0.4212868809700012:  60%|██████▌    | 228/383 [02:13<01:36,  1.61it/s]Epoch: 6, train for the 227-th batch, train loss: 0.6344473361968994:  95%|██████████▍| 226/237 [02:12<00:06,  1.78it/s]Epoch: 6, train for the 227-th batch, train loss: 0.6344473361968994:  96%|██████████▌| 227/237 [02:12<00:05,  1.75it/s]evaluate for the 8-th batch, evaluate loss: 0.12299024313688278:  18%|███▎               | 7/40 [00:02<00:09,  3.54it/s]evaluate for the 8-th batch, evaluate loss: 0.12299024313688278:  20%|███▊               | 8/40 [00:02<00:08,  3.63it/s]Epoch: 21, train for the 53-th batch, train loss: 0.6049972176551819:  34%|████▏       | 52/151 [00:11<00:22,  4.37it/s]Epoch: 21, train for the 53-th batch, train loss: 0.6049972176551819:  35%|████▏       | 53/151 [00:11<00:22,  4.33it/s]evaluate for the 9-th batch, evaluate loss: 0.17053937911987305:  20%|███▊               | 8/40 [00:02<00:08,  3.63it/s]evaluate for the 9-th batch, evaluate loss: 0.17053937911987305:  22%|████▎              | 9/40 [00:02<00:08,  3.60it/s]Epoch: 21, train for the 54-th batch, train loss: 0.491781085729599:  35%|████▌        | 53/151 [00:11<00:22,  4.33it/s]Epoch: 21, train for the 54-th batch, train loss: 0.491781085729599:  36%|████▋        | 54/151 [00:11<00:22,  4.40it/s]Epoch: 10, train for the 73-th batch, train loss: 0.5143756866455078:  49%|█████▉      | 72/146 [00:43<00:45,  1.64it/s]Epoch: 10, train for the 73-th batch, train loss: 0.5143756866455078:  50%|██████      | 73/146 [00:43<00:44,  1.64it/s]INFO:root:model -> Sequential(
  (0): DyGFormer(
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (memory_bank): MemoryBank(num_nodes=16530, memory_dim=172)
    (neighbor_co_occurrence_encoder): NeighborCooccurrenceEncoder(
      (neighbor_co_occurrence_encode_layer): Sequential(
        (0): Linear(in_features=1, out_features=50, bias=True)
        (1): ReLU()
        (2): Linear(in_features=50, out_features=50, bias=True)
      )
    )
    (projection_layer): ModuleDict(
      (node): Linear(in_features=172, out_features=50, bias=True)
      (edge): Linear(in_features=172, out_features=50, bias=True)
      (time): Linear(in_features=100, out_features=50, bias=True)
      (neighbor_co_occurrence): Linear(in_features=50, out_features=50, bias=True)
    )
    (transformers): ModuleList(
      (0-1): 2 x TransformerEncoder(
        (multi_head_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)
        )
        (dropout): Dropout(p=0.1, inplace=False)
        (linear_layers): ModuleList(
          (0): Linear(in_features=200, out_features=800, bias=True)
          (1): Linear(in_features=800, out_features=200, bias=True)
        )
        (norm_layers): ModuleList(
          (0-1): 2 x LayerNorm((200,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_layer): Linear(in_features=200, out_features=172, bias=True)
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: DyGFormer, #parameters: 4348140 B, 4246.23046875 KB, 4.146709442138672 MB.
The dataset has 95577 interactions, involving 16527 different nodes
The training dataset has 48189 interactions, involving 10566 different nodes
The validation dataset has 14336 interactions, involving 5209 different nodes
The test dataset has 14337 interactions, involving 5377 different nodes
The new node validation dataset has 6745 interactions, involving 3518 different nodes
The new node test dataset has 7521 interactions, involving 3885 different nodes
1652 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/241 [00:00<?, ?it/s]Epoch: 4, train for the 229-th batch, train loss: 0.3919428288936615:  60%|██████▌    | 228/383 [02:14<01:36,  1.61it/s]Epoch: 4, train for the 229-th batch, train loss: 0.3919428288936615:  60%|██████▌    | 229/383 [02:14<01:36,  1.59it/s]evaluate for the 10-th batch, evaluate loss: 0.18688789010047913:  22%|████              | 9/40 [00:02<00:08,  3.60it/s]evaluate for the 10-th batch, evaluate loss: 0.18688789010047913:  25%|████▎            | 10/40 [00:02<00:08,  3.54it/s]Epoch: 6, train for the 228-th batch, train loss: 0.6372684240341187:  96%|██████████▌| 227/237 [02:13<00:05,  1.75it/s]Epoch: 6, train for the 228-th batch, train loss: 0.6372684240341187:  96%|██████████▌| 228/237 [02:13<00:05,  1.71it/s]Epoch: 21, train for the 55-th batch, train loss: 0.5499469041824341:  36%|████▎       | 54/151 [00:12<00:22,  4.40it/s]Epoch: 21, train for the 55-th batch, train loss: 0.5499469041824341:  36%|████▎       | 55/151 [00:12<00:24,  3.85it/s]evaluate for the 11-th batch, evaluate loss: 0.15950444340705872:  25%|████▎            | 10/40 [00:03<00:08,  3.54it/s]evaluate for the 11-th batch, evaluate loss: 0.15950444340705872:  28%|████▋            | 11/40 [00:03<00:08,  3.57it/s]Epoch: 21, train for the 56-th batch, train loss: 0.5065740942955017:  36%|████▎       | 55/151 [00:12<00:24,  3.85it/s]Epoch: 21, train for the 56-th batch, train loss: 0.5065740942955017:  37%|████▍       | 56/151 [00:12<00:24,  3.93it/s]Epoch: 10, train for the 74-th batch, train loss: 0.5211074948310852:  50%|██████      | 73/146 [00:44<00:44,  1.64it/s]Epoch: 10, train for the 74-th batch, train loss: 0.5211074948310852:  51%|██████      | 74/146 [00:44<00:43,  1.64it/s]evaluate for the 12-th batch, evaluate loss: 0.13859428465366364:  28%|████▋            | 11/40 [00:03<00:08,  3.57it/s]evaluate for the 12-th batch, evaluate loss: 0.13859428465366364:  30%|█████            | 12/40 [00:03<00:08,  3.50it/s]Epoch: 21, train for the 57-th batch, train loss: 0.4615717828273773:  37%|████▍       | 56/151 [00:12<00:24,  3.93it/s]Epoch: 21, train for the 57-th batch, train loss: 0.4615717828273773:  38%|████▌       | 57/151 [00:12<00:22,  4.09it/s]Epoch: 4, train for the 230-th batch, train loss: 0.44266095757484436:  60%|█████▉    | 229/383 [02:15<01:36,  1.59it/s]Epoch: 4, train for the 230-th batch, train loss: 0.44266095757484436:  60%|██████    | 230/383 [02:15<01:36,  1.58it/s]Epoch: 6, train for the 229-th batch, train loss: 0.6742308735847473:  96%|██████████▌| 228/237 [02:13<00:05,  1.71it/s]Epoch: 6, train for the 229-th batch, train loss: 0.6742308735847473:  97%|██████████▋| 229/237 [02:13<00:04,  1.66it/s]Epoch: 21, train for the 58-th batch, train loss: 0.5570036768913269:  38%|████▌       | 57/151 [00:12<00:22,  4.09it/s]Epoch: 21, train for the 58-th batch, train loss: 0.5570036768913269:  38%|████▌       | 58/151 [00:12<00:22,  4.12it/s]evaluate for the 13-th batch, evaluate loss: 0.12415756285190582:  30%|█████            | 12/40 [00:03<00:08,  3.50it/s]evaluate for the 13-th batch, evaluate loss: 0.12415756285190582:  32%|█████▌           | 13/40 [00:03<00:07,  3.65it/s]Epoch: 21, train for the 59-th batch, train loss: 0.5069831013679504:  38%|████▌       | 58/151 [00:13<00:22,  4.12it/s]Epoch: 21, train for the 59-th batch, train loss: 0.5069831013679504:  39%|████▋       | 59/151 [00:13<00:21,  4.20it/s]Epoch: 10, train for the 75-th batch, train loss: 0.556178092956543:  51%|██████▌      | 74/146 [00:45<00:43,  1.64it/s]Epoch: 10, train for the 75-th batch, train loss: 0.556178092956543:  51%|██████▋      | 75/146 [00:45<00:42,  1.65it/s]evaluate for the 14-th batch, evaluate loss: 0.15394622087478638:  32%|█████▌           | 13/40 [00:03<00:07,  3.65it/s]evaluate for the 14-th batch, evaluate loss: 0.15394622087478638:  35%|█████▉           | 14/40 [00:03<00:07,  3.53it/s]Epoch: 21, train for the 60-th batch, train loss: 0.4613007605075836:  39%|████▋       | 59/151 [00:13<00:21,  4.20it/s]Epoch: 21, train for the 60-th batch, train loss: 0.4613007605075836:  40%|████▊       | 60/151 [00:13<00:21,  4.26it/s]Epoch: 4, train for the 231-th batch, train loss: 0.47521212697029114:  60%|██████    | 230/383 [02:15<01:36,  1.58it/s]Epoch: 4, train for the 231-th batch, train loss: 0.47521212697029114:  60%|██████    | 231/383 [02:15<01:37,  1.56it/s]Epoch: 6, train for the 230-th batch, train loss: 0.6308366656303406:  97%|██████████▋| 229/237 [02:14<00:04,  1.66it/s]Epoch: 6, train for the 230-th batch, train loss: 0.6308366656303406:  97%|██████████▋| 230/237 [02:14<00:04,  1.62it/s]evaluate for the 15-th batch, evaluate loss: 0.171290323138237:  35%|██████▋            | 14/40 [00:04<00:07,  3.53it/s]evaluate for the 15-th batch, evaluate loss: 0.171290323138237:  38%|███████▏           | 15/40 [00:04<00:06,  3.70it/s]Epoch: 21, train for the 61-th batch, train loss: 0.4970560371875763:  40%|████▊       | 60/151 [00:13<00:21,  4.26it/s]Epoch: 21, train for the 61-th batch, train loss: 0.4970560371875763:  40%|████▊       | 61/151 [00:13<00:20,  4.32it/s]Epoch: 10, train for the 76-th batch, train loss: 0.5264582633972168:  51%|██████▏     | 75/146 [00:45<00:42,  1.65it/s]Epoch: 10, train for the 76-th batch, train loss: 0.5264582633972168:  52%|██████▏     | 76/146 [00:45<00:42,  1.64it/s]evaluate for the 16-th batch, evaluate loss: 0.1923675686120987:  38%|██████▊           | 15/40 [00:04<00:06,  3.70it/s]evaluate for the 16-th batch, evaluate loss: 0.1923675686120987:  40%|███████▏          | 16/40 [00:04<00:06,  3.51it/s]Epoch: 21, train for the 62-th batch, train loss: 0.5869662165641785:  40%|████▊       | 61/151 [00:13<00:20,  4.32it/s]Epoch: 21, train for the 62-th batch, train loss: 0.5869662165641785:  41%|████▉       | 62/151 [00:13<00:20,  4.29it/s]evaluate for the 17-th batch, evaluate loss: 0.12235526740550995:  40%|██████▊          | 16/40 [00:04<00:06,  3.51it/s]evaluate for the 17-th batch, evaluate loss: 0.12235526740550995:  42%|███████▏         | 17/40 [00:04<00:06,  3.57it/s]Epoch: 4, train for the 232-th batch, train loss: 0.369179904460907:  60%|███████▏    | 231/383 [02:16<01:37,  1.56it/s]Epoch: 6, train for the 231-th batch, train loss: 0.6473572254180908:  97%|██████████▋| 230/237 [02:15<00:04,  1.62it/s]Epoch: 4, train for the 232-th batch, train loss: 0.369179904460907:  61%|███████▎    | 232/383 [02:16<01:37,  1.55it/s]Epoch: 6, train for the 231-th batch, train loss: 0.6473572254180908:  97%|██████████▋| 231/237 [02:15<00:03,  1.59it/s]Epoch: 21, train for the 63-th batch, train loss: 0.546626091003418:  41%|█████▎       | 62/151 [00:13<00:20,  4.29it/s]Epoch: 21, train for the 63-th batch, train loss: 0.546626091003418:  42%|█████▍       | 63/151 [00:13<00:20,  4.31it/s]evaluate for the 18-th batch, evaluate loss: 0.1322418600320816:  42%|███████▋          | 17/40 [00:05<00:06,  3.57it/s]evaluate for the 18-th batch, evaluate loss: 0.1322418600320816:  45%|████████          | 18/40 [00:05<00:06,  3.49it/s]Epoch: 10, train for the 77-th batch, train loss: 0.5364779233932495:  52%|██████▏     | 76/146 [00:46<00:42,  1.64it/s]Epoch: 10, train for the 77-th batch, train loss: 0.5364779233932495:  53%|██████▎     | 77/146 [00:46<00:41,  1.65it/s]Epoch: 21, train for the 64-th batch, train loss: 0.46868178248405457:  42%|████▌      | 63/151 [00:14<00:20,  4.31it/s]Epoch: 21, train for the 64-th batch, train loss: 0.46868178248405457:  42%|████▋      | 64/151 [00:14<00:23,  3.75it/s]evaluate for the 19-th batch, evaluate loss: 0.14059185981750488:  45%|███████▋         | 18/40 [00:05<00:06,  3.49it/s]evaluate for the 19-th batch, evaluate loss: 0.14059185981750488:  48%|████████         | 19/40 [00:05<00:05,  3.61it/s]Epoch: 21, train for the 65-th batch, train loss: 0.4714875817298889:  42%|█████       | 64/151 [00:14<00:23,  3.75it/s]Epoch: 21, train for the 65-th batch, train loss: 0.4714875817298889:  43%|█████▏      | 65/151 [00:14<00:21,  3.96it/s]Epoch: 4, train for the 233-th batch, train loss: 0.45790931582450867:  61%|██████    | 232/383 [02:17<01:37,  1.55it/s]Epoch: 6, train for the 232-th batch, train loss: 0.6324533820152283:  97%|██████████▋| 231/237 [02:15<00:03,  1.59it/s]Epoch: 4, train for the 233-th batch, train loss: 0.45790931582450867:  61%|██████    | 233/383 [02:17<01:37,  1.55it/s]Epoch: 6, train for the 232-th batch, train loss: 0.6324533820152283:  98%|██████████▊| 232/237 [02:15<00:03,  1.57it/s]evaluate for the 20-th batch, evaluate loss: 0.1433439552783966:  48%|████████▌         | 19/40 [00:05<00:05,  3.61it/s]evaluate for the 20-th batch, evaluate loss: 0.1433439552783966:  50%|█████████         | 20/40 [00:05<00:05,  3.55it/s]Epoch: 10, train for the 78-th batch, train loss: 0.5153205990791321:  53%|██████▎     | 77/146 [00:46<00:41,  1.65it/s]Epoch: 10, train for the 78-th batch, train loss: 0.5153205990791321:  53%|██████▍     | 78/146 [00:46<00:40,  1.66it/s]Epoch: 21, train for the 66-th batch, train loss: 0.5253729820251465:  43%|█████▏      | 65/151 [00:14<00:21,  3.96it/s]Epoch: 21, train for the 66-th batch, train loss: 0.5253729820251465:  44%|█████▏      | 66/151 [00:14<00:24,  3.52it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6942182779312134:   0%|                       | 0/241 [00:03<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6942182779312134:   0%|               | 1/241 [00:03<12:20,  3.09s/it]evaluate for the 21-th batch, evaluate loss: 0.08496260643005371:  50%|████████▌        | 20/40 [00:05<00:05,  3.55it/s]evaluate for the 21-th batch, evaluate loss: 0.08496260643005371:  52%|████████▉        | 21/40 [00:05<00:05,  3.64it/s]Epoch: 4, train for the 234-th batch, train loss: 0.42536482214927673:  61%|██████    | 233/383 [02:17<01:37,  1.55it/s]Epoch: 6, train for the 233-th batch, train loss: 0.6596830487251282:  98%|██████████▊| 232/237 [02:16<00:03,  1.57it/s]Epoch: 4, train for the 234-th batch, train loss: 0.42536482214927673:  61%|██████    | 234/383 [02:17<01:36,  1.54it/s]Epoch: 6, train for the 233-th batch, train loss: 0.6596830487251282:  98%|██████████▊| 233/237 [02:16<00:02,  1.56it/s]Epoch: 21, train for the 67-th batch, train loss: 0.5829876661300659:  44%|█████▏      | 66/151 [00:15<00:24,  3.52it/s]Epoch: 21, train for the 67-th batch, train loss: 0.5829876661300659:  44%|█████▎      | 67/151 [00:15<00:27,  3.06it/s]evaluate for the 22-th batch, evaluate loss: 0.13811805844306946:  52%|████████▉        | 21/40 [00:06<00:05,  3.64it/s]evaluate for the 22-th batch, evaluate loss: 0.13811805844306946:  55%|█████████▎       | 22/40 [00:06<00:04,  3.60it/s]Epoch: 10, train for the 79-th batch, train loss: 0.5728524923324585:  53%|██████▍     | 78/146 [00:47<00:40,  1.66it/s]Epoch: 10, train for the 79-th batch, train loss: 0.5728524923324585:  54%|██████▍     | 79/146 [00:47<00:40,  1.65it/s]evaluate for the 23-th batch, evaluate loss: 0.14907622337341309:  55%|█████████▎       | 22/40 [00:06<00:04,  3.60it/s]evaluate for the 23-th batch, evaluate loss: 0.14907622337341309:  57%|█████████▊       | 23/40 [00:06<00:04,  3.57it/s]Epoch: 21, train for the 68-th batch, train loss: 0.5317288637161255:  44%|█████▎      | 67/151 [00:15<00:27,  3.06it/s]Epoch: 21, train for the 68-th batch, train loss: 0.5317288637161255:  45%|█████▍      | 68/151 [00:15<00:29,  2.83it/s]Epoch: 1, train for the 2-th batch, train loss: 0.6797316670417786:   0%|               | 1/241 [00:03<12:20,  3.09s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6797316670417786:   1%|               | 2/241 [00:03<07:04,  1.78s/it]evaluate for the 24-th batch, evaluate loss: 0.14725220203399658:  57%|█████████▊       | 23/40 [00:06<00:04,  3.57it/s]evaluate for the 24-th batch, evaluate loss: 0.14725220203399658:  60%|██████████▏      | 24/40 [00:06<00:04,  3.59it/s]Epoch: 4, train for the 235-th batch, train loss: 0.3810887932777405:  61%|██████▋    | 234/383 [02:18<01:36,  1.54it/s]Epoch: 6, train for the 234-th batch, train loss: 0.6495309472084045:  98%|██████████▊| 233/237 [02:17<00:02,  1.56it/s]Epoch: 4, train for the 235-th batch, train loss: 0.3810887932777405:  61%|██████▋    | 235/383 [02:18<01:36,  1.54it/s]Epoch: 6, train for the 234-th batch, train loss: 0.6495309472084045:  99%|██████████▊| 234/237 [02:17<00:01,  1.55it/s]Epoch: 10, train for the 80-th batch, train loss: 0.5418709516525269:  54%|██████▍     | 79/146 [00:48<00:40,  1.65it/s]Epoch: 10, train for the 80-th batch, train loss: 0.5418709516525269:  55%|██████▌     | 80/146 [00:48<00:40,  1.65it/s]Epoch: 21, train for the 69-th batch, train loss: 0.5025551319122314:  45%|█████▍      | 68/151 [00:16<00:29,  2.83it/s]Epoch: 21, train for the 69-th batch, train loss: 0.5025551319122314:  46%|█████▍      | 69/151 [00:16<00:30,  2.66it/s]evaluate for the 25-th batch, evaluate loss: 0.14168481528759003:  60%|██████████▏      | 24/40 [00:07<00:04,  3.59it/s]evaluate for the 25-th batch, evaluate loss: 0.14168481528759003:  62%|██████████▋      | 25/40 [00:07<00:04,  3.51it/s]Epoch: 1, train for the 3-th batch, train loss: 0.6723028421401978:   1%|               | 2/241 [00:04<07:04,  1.78s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6723028421401978:   1%|▏              | 3/241 [00:04<04:48,  1.21s/it]evaluate for the 26-th batch, evaluate loss: 0.1142263412475586:  62%|███████████▎      | 25/40 [00:07<00:04,  3.51it/s]evaluate for the 26-th batch, evaluate loss: 0.1142263412475586:  65%|███████████▋      | 26/40 [00:07<00:03,  3.65it/s]Epoch: 6, train for the 235-th batch, train loss: 0.6110744476318359:  99%|██████████▊| 234/237 [02:17<00:01,  1.55it/s]Epoch: 4, train for the 236-th batch, train loss: 0.4012454152107239:  61%|██████▋    | 235/383 [02:19<01:36,  1.54it/s]Epoch: 6, train for the 235-th batch, train loss: 0.6110744476318359:  99%|██████████▉| 235/237 [02:17<00:01,  1.54it/s]Epoch: 4, train for the 236-th batch, train loss: 0.4012454152107239:  62%|██████▊    | 236/383 [02:19<01:35,  1.53it/s]Epoch: 10, train for the 81-th batch, train loss: 0.5291072130203247:  55%|██████▌     | 80/146 [00:48<00:40,  1.65it/s]Epoch: 10, train for the 81-th batch, train loss: 0.5291072130203247:  55%|██████▋     | 81/146 [00:48<00:39,  1.66it/s]Epoch: 21, train for the 70-th batch, train loss: 0.5599139332771301:  46%|█████▍      | 69/151 [00:16<00:30,  2.66it/s]Epoch: 21, train for the 70-th batch, train loss: 0.5599139332771301:  46%|█████▌      | 70/151 [00:16<00:34,  2.33it/s]evaluate for the 27-th batch, evaluate loss: 0.13916686177253723:  65%|███████████      | 26/40 [00:07<00:03,  3.65it/s]evaluate for the 27-th batch, evaluate loss: 0.13916686177253723:  68%|███████████▍     | 27/40 [00:07<00:03,  3.53it/s]evaluate for the 28-th batch, evaluate loss: 0.10151758044958115:  68%|███████████▍     | 27/40 [00:07<00:03,  3.53it/s]evaluate for the 28-th batch, evaluate loss: 0.10151758044958115:  70%|███████████▉     | 28/40 [00:07<00:03,  3.70it/s]Epoch: 21, train for the 71-th batch, train loss: 0.549712061882019:  46%|██████       | 70/151 [00:17<00:34,  2.33it/s]Epoch: 21, train for the 71-th batch, train loss: 0.549712061882019:  47%|██████       | 71/151 [00:17<00:34,  2.34it/s]Epoch: 1, train for the 4-th batch, train loss: 0.684744119644165:   1%|▏               | 3/241 [00:05<04:48,  1.21s/it]Epoch: 1, train for the 4-th batch, train loss: 0.684744119644165:   2%|▎               | 4/241 [00:05<04:09,  1.05s/it]Epoch: 6, train for the 236-th batch, train loss: 0.6129922866821289:  99%|██████████▉| 235/237 [02:18<00:01,  1.54it/s]Epoch: 4, train for the 237-th batch, train loss: 0.431801974773407:  62%|███████▍    | 236/383 [02:19<01:35,  1.53it/s]Epoch: 6, train for the 236-th batch, train loss: 0.6129922866821289: 100%|██████████▉| 236/237 [02:18<00:00,  1.54it/s]Epoch: 4, train for the 237-th batch, train loss: 0.431801974773407:  62%|███████▍    | 237/383 [02:19<01:35,  1.53it/s]Epoch: 10, train for the 82-th batch, train loss: 0.5357134342193604:  55%|██████▋     | 81/146 [00:49<00:39,  1.66it/s]Epoch: 10, train for the 82-th batch, train loss: 0.5357134342193604:  56%|██████▋     | 82/146 [00:49<00:38,  1.65it/s]evaluate for the 29-th batch, evaluate loss: 0.15939749777317047:  70%|███████████▉     | 28/40 [00:08<00:03,  3.70it/s]evaluate for the 29-th batch, evaluate loss: 0.15939749777317047:  72%|████████████▎    | 29/40 [00:08<00:03,  3.53it/s]evaluate for the 30-th batch, evaluate loss: 0.16591776907444:  72%|██████████████▌     | 29/40 [00:08<00:03,  3.53it/s]evaluate for the 30-th batch, evaluate loss: 0.16591776907444:  75%|███████████████     | 30/40 [00:08<00:02,  3.58it/s]Epoch: 6, train for the 237-th batch, train loss: 0.635355532169342: 100%|███████████▉| 236/237 [02:18<00:00,  1.54it/s]Epoch: 6, train for the 237-th batch, train loss: 0.635355532169342: 100%|████████████| 237/237 [02:18<00:00,  1.72it/s]Epoch: 6, train for the 237-th batch, train loss: 0.635355532169342: 100%|████████████| 237/237 [02:18<00:00,  1.71it/s]
  0%|                                                                                            | 0/66 [00:00<?, ?it/s]Epoch: 21, train for the 72-th batch, train loss: 0.5508076548576355:  47%|█████▋      | 71/151 [00:17<00:34,  2.34it/s]Epoch: 21, train for the 72-th batch, train loss: 0.5508076548576355:  48%|█████▋      | 72/151 [00:17<00:38,  2.06it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6694874167442322:   2%|▏              | 4/241 [00:05<04:09,  1.05s/it]Epoch: 1, train for the 5-th batch, train loss: 0.6694874167442322:   2%|▎              | 5/241 [00:05<03:25,  1.15it/s]Epoch: 4, train for the 238-th batch, train loss: 0.38176795840263367:  62%|██████▏   | 237/383 [02:20<01:35,  1.53it/s]Epoch: 4, train for the 238-th batch, train loss: 0.38176795840263367:  62%|██████▏   | 238/383 [02:20<01:31,  1.59it/s]evaluate for the 31-th batch, evaluate loss: 0.1624162644147873:  75%|█████████████▌    | 30/40 [00:08<00:02,  3.58it/s]Epoch: 10, train for the 83-th batch, train loss: 0.5335422158241272:  56%|██████▋     | 82/146 [00:49<00:38,  1.65it/s]evaluate for the 31-th batch, evaluate loss: 0.1624162644147873:  78%|█████████████▉    | 31/40 [00:08<00:02,  3.44it/s]Epoch: 10, train for the 83-th batch, train loss: 0.5335422158241272:  57%|██████▊     | 83/146 [00:49<00:38,  1.65it/s]evaluate for the 1-th batch, evaluate loss: 0.5949709415435791:   0%|                            | 0/66 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.5949709415435791:   2%|▎                   | 1/66 [00:00<00:18,  3.43it/s]evaluate for the 32-th batch, evaluate loss: 0.13220666348934174:  78%|█████████████▏   | 31/40 [00:08<00:02,  3.44it/s]evaluate for the 32-th batch, evaluate loss: 0.13220666348934174:  80%|█████████████▌   | 32/40 [00:08<00:02,  3.63it/s]evaluate for the 2-th batch, evaluate loss: 0.5840044021606445:   2%|▎                   | 1/66 [00:00<00:18,  3.43it/s]evaluate for the 2-th batch, evaluate loss: 0.5840044021606445:   3%|▌                   | 2/66 [00:00<00:17,  3.58it/s]evaluate for the 33-th batch, evaluate loss: 0.1354287713766098:  80%|██████████████▍   | 32/40 [00:09<00:02,  3.63it/s]evaluate for the 33-th batch, evaluate loss: 0.1354287713766098:  82%|██████████████▊   | 33/40 [00:09<00:01,  4.15it/s]Epoch: 4, train for the 239-th batch, train loss: 0.3945801556110382:  62%|██████▊    | 238/383 [02:20<01:31,  1.59it/s]Epoch: 4, train for the 239-th batch, train loss: 0.3945801556110382:  62%|██████▊    | 239/383 [02:20<01:29,  1.61it/s]Epoch: 21, train for the 73-th batch, train loss: 0.5409387946128845:  48%|█████▋      | 72/151 [00:18<00:38,  2.06it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6847092509269714:   2%|▎              | 5/241 [00:06<03:25,  1.15it/s]Epoch: 21, train for the 73-th batch, train loss: 0.5409387946128845:  48%|█████▊      | 73/151 [00:18<00:41,  1.87it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6847092509269714:   2%|▎              | 6/241 [00:06<03:06,  1.26it/s]evaluate for the 3-th batch, evaluate loss: 0.5964309573173523:   3%|▌                   | 2/66 [00:00<00:17,  3.58it/s]evaluate for the 3-th batch, evaluate loss: 0.5964309573173523:   5%|▉                   | 3/66 [00:00<00:18,  3.48it/s]Epoch: 10, train for the 84-th batch, train loss: 0.492536336183548:  57%|███████▍     | 83/146 [00:50<00:38,  1.65it/s]Epoch: 10, train for the 84-th batch, train loss: 0.492536336183548:  58%|███████▍     | 84/146 [00:50<00:39,  1.58it/s]evaluate for the 34-th batch, evaluate loss: 0.10521429032087326:  82%|██████████████   | 33/40 [00:09<00:01,  4.15it/s]evaluate for the 34-th batch, evaluate loss: 0.10521429032087326:  85%|██████████████▍  | 34/40 [00:09<00:01,  3.81it/s]evaluate for the 4-th batch, evaluate loss: 0.6058953404426575:   5%|▉                   | 3/66 [00:01<00:18,  3.48it/s]evaluate for the 4-th batch, evaluate loss: 0.6058953404426575:   6%|█▏                  | 4/66 [00:01<00:16,  3.69it/s]evaluate for the 35-th batch, evaluate loss: 0.14559237658977509:  85%|██████████████▍  | 34/40 [00:09<00:01,  3.81it/s]evaluate for the 35-th batch, evaluate loss: 0.14559237658977509:  88%|██████████████▉  | 35/40 [00:09<00:01,  3.79it/s]Epoch: 4, train for the 240-th batch, train loss: 0.47311264276504517:  62%|██████▏   | 239/383 [02:21<01:29,  1.61it/s]Epoch: 4, train for the 240-th batch, train loss: 0.47311264276504517:  63%|██████▎   | 240/383 [02:21<01:27,  1.64it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6129098534584045:   2%|▎              | 6/241 [00:07<03:06,  1.26it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6129098534584045:   3%|▍              | 7/241 [00:07<02:53,  1.35it/s]Epoch: 21, train for the 74-th batch, train loss: 0.5311707258224487:  48%|█████▊      | 73/151 [00:19<00:41,  1.87it/s]Epoch: 21, train for the 74-th batch, train loss: 0.5311707258224487:  49%|█████▉      | 74/151 [00:19<00:43,  1.76it/s]evaluate for the 5-th batch, evaluate loss: 0.6133232712745667:   6%|█▏                  | 4/66 [00:01<00:16,  3.69it/s]evaluate for the 5-th batch, evaluate loss: 0.6133232712745667:   8%|█▌                  | 5/66 [00:01<00:17,  3.56it/s]Epoch: 10, train for the 85-th batch, train loss: 0.4939092993736267:  58%|██████▉     | 84/146 [00:51<00:39,  1.58it/s]evaluate for the 36-th batch, evaluate loss: 0.14079338312149048:  88%|██████████████▉  | 35/40 [00:10<00:01,  3.79it/s]Epoch: 10, train for the 85-th batch, train loss: 0.4939092993736267:  58%|██████▉     | 85/146 [00:51<00:38,  1.60it/s]evaluate for the 36-th batch, evaluate loss: 0.14079338312149048:  90%|███████████████▎ | 36/40 [00:10<00:01,  3.57it/s]evaluate for the 6-th batch, evaluate loss: 0.6435076594352722:   8%|█▌                  | 5/66 [00:01<00:17,  3.56it/s]evaluate for the 6-th batch, evaluate loss: 0.6435076594352722:   9%|█▊                  | 6/66 [00:01<00:15,  3.76it/s]evaluate for the 37-th batch, evaluate loss: 0.19817668199539185:  90%|███████████████▎ | 36/40 [00:10<00:01,  3.57it/s]evaluate for the 37-th batch, evaluate loss: 0.19817668199539185:  92%|███████████████▋ | 37/40 [00:10<00:00,  3.61it/s]Epoch: 4, train for the 241-th batch, train loss: 0.44198310375213623:  63%|██████▎   | 240/383 [02:22<01:27,  1.64it/s]Epoch: 4, train for the 241-th batch, train loss: 0.44198310375213623:  63%|██████▎   | 241/383 [02:22<01:26,  1.65it/s]evaluate for the 38-th batch, evaluate loss: 0.12209057807922363:  92%|███████████████▋ | 37/40 [00:10<00:00,  3.61it/s]evaluate for the 38-th batch, evaluate loss: 0.12209057807922363:  95%|████████████████▏| 38/40 [00:10<00:00,  4.32it/s]evaluate for the 7-th batch, evaluate loss: 0.6133849024772644:   9%|█▊                  | 6/66 [00:01<00:15,  3.76it/s]evaluate for the 7-th batch, evaluate loss: 0.6133849024772644:  11%|██                  | 7/66 [00:01<00:16,  3.57it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6639301776885986:   3%|▍              | 7/241 [00:07<02:53,  1.35it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6639301776885986:   3%|▍              | 8/241 [00:07<02:42,  1.44it/s]Epoch: 21, train for the 75-th batch, train loss: 0.527274489402771:  49%|██████▎      | 74/151 [00:19<00:43,  1.76it/s]Epoch: 21, train for the 75-th batch, train loss: 0.527274489402771:  50%|██████▍      | 75/151 [00:19<00:44,  1.72it/s]evaluate for the 39-th batch, evaluate loss: 0.13953709602355957:  95%|████████████████▏| 38/40 [00:10<00:00,  4.32it/s]evaluate for the 39-th batch, evaluate loss: 0.13953709602355957:  98%|████████████████▌| 39/40 [00:10<00:00,  4.57it/s]evaluate for the 40-th batch, evaluate loss: 0.04650593921542168:  98%|████████████████▌| 39/40 [00:10<00:00,  4.57it/s]evaluate for the 40-th batch, evaluate loss: 0.04650593921542168: 100%|█████████████████| 40/40 [00:10<00:00,  3.75it/s]
  0%|                                                                                            | 0/21 [00:00<?, ?it/s]evaluate for the 8-th batch, evaluate loss: 0.612791121006012:  11%|██▏                  | 7/66 [00:02<00:16,  3.57it/s]evaluate for the 8-th batch, evaluate loss: 0.612791121006012:  12%|██▌                  | 8/66 [00:02<00:15,  3.64it/s]evaluate for the 1-th batch, evaluate loss: 0.2089713215827942:   0%|                            | 0/21 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.2089713215827942:   5%|▉                   | 1/21 [00:00<00:02,  7.72it/s]evaluate for the 2-th batch, evaluate loss: 0.22396185994148254:   5%|▉                  | 1/21 [00:00<00:02,  7.72it/s]evaluate for the 2-th batch, evaluate loss: 0.22396185994148254:  10%|█▊                 | 2/21 [00:00<00:02,  8.48it/s]evaluate for the 9-th batch, evaluate loss: 0.5534434914588928:  12%|██▍                 | 8/66 [00:02<00:15,  3.64it/s]evaluate for the 9-th batch, evaluate loss: 0.5534434914588928:  14%|██▋                 | 9/66 [00:02<00:16,  3.55it/s]Epoch: 4, train for the 242-th batch, train loss: 0.47209447622299194:  63%|██████▎   | 241/383 [02:22<01:26,  1.65it/s]Epoch: 4, train for the 242-th batch, train loss: 0.47209447622299194:  63%|██████▎   | 242/383 [02:22<01:25,  1.66it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6522231698036194:   3%|▍              | 8/241 [00:08<02:42,  1.44it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6522231698036194:   4%|▌              | 9/241 [00:08<02:32,  1.52it/s]evaluate for the 3-th batch, evaluate loss: 0.2297900915145874:  10%|█▉                  | 2/21 [00:00<00:02,  8.48it/s]evaluate for the 3-th batch, evaluate loss: 0.2297900915145874:  14%|██▊                 | 3/21 [00:00<00:02,  7.20it/s]Epoch: 10, train for the 86-th batch, train loss: 0.5357365012168884:  58%|██████▉     | 85/146 [00:52<00:38,  1.60it/s]Epoch: 10, train for the 86-th batch, train loss: 0.5357365012168884:  59%|███████     | 86/146 [00:52<00:44,  1.33it/s]Epoch: 21, train for the 76-th batch, train loss: 0.45262610912323:  50%|██████▉       | 75/151 [00:20<00:44,  1.72it/s]Epoch: 21, train for the 76-th batch, train loss: 0.45262610912323:  50%|███████       | 76/151 [00:20<00:43,  1.72it/s]evaluate for the 10-th batch, evaluate loss: 0.5782179832458496:  14%|██▌                | 9/66 [00:02<00:16,  3.55it/s]evaluate for the 10-th batch, evaluate loss: 0.5782179832458496:  15%|██▋               | 10/66 [00:02<00:15,  3.68it/s]evaluate for the 4-th batch, evaluate loss: 0.20234033465385437:  14%|██▋                | 3/21 [00:00<00:02,  7.20it/s]evaluate for the 4-th batch, evaluate loss: 0.20234033465385437:  19%|███▌               | 4/21 [00:00<00:03,  5.40it/s]evaluate for the 11-th batch, evaluate loss: 0.568850040435791:  15%|██▉                | 10/66 [00:03<00:15,  3.68it/s]evaluate for the 11-th batch, evaluate loss: 0.568850040435791:  17%|███▏               | 11/66 [00:03<00:15,  3.61it/s]Epoch: 4, train for the 243-th batch, train loss: 0.45474356412887573:  63%|██████▎   | 242/383 [02:23<01:25,  1.66it/s]Epoch: 4, train for the 243-th batch, train loss: 0.45474356412887573:  63%|██████▎   | 243/383 [02:23<01:23,  1.67it/s]evaluate for the 5-th batch, evaluate loss: 0.19618342816829681:  19%|███▌               | 4/21 [00:00<00:03,  5.40it/s]evaluate for the 5-th batch, evaluate loss: 0.19618342816829681:  24%|████▌              | 5/21 [00:00<00:03,  4.41it/s]Epoch: 10, train for the 87-th batch, train loss: 0.5423346757888794:  59%|███████     | 86/146 [00:52<00:44,  1.33it/s]Epoch: 10, train for the 87-th batch, train loss: 0.5423346757888794:  60%|███████▏    | 87/146 [00:52<00:41,  1.44it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6710469126701355:   4%|▌             | 9/241 [00:08<02:32,  1.52it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6710469126701355:   4%|▌            | 10/241 [00:08<02:29,  1.54it/s]Epoch: 21, train for the 77-th batch, train loss: 0.490945965051651:  50%|██████▌      | 76/151 [00:20<00:43,  1.72it/s]Epoch: 21, train for the 77-th batch, train loss: 0.490945965051651:  51%|██████▋      | 77/151 [00:20<00:43,  1.70it/s]evaluate for the 12-th batch, evaluate loss: 0.6137415766716003:  17%|███               | 11/66 [00:03<00:15,  3.61it/s]evaluate for the 12-th batch, evaluate loss: 0.6137415766716003:  18%|███▎              | 12/66 [00:03<00:14,  3.70it/s]evaluate for the 6-th batch, evaluate loss: 0.22658370435237885:  24%|████▌              | 5/21 [00:01<00:03,  4.41it/s]evaluate for the 6-th batch, evaluate loss: 0.22658370435237885:  29%|█████▍             | 6/21 [00:01<00:03,  4.18it/s]evaluate for the 13-th batch, evaluate loss: 0.5929802060127258:  18%|███▎              | 12/66 [00:03<00:14,  3.70it/s]evaluate for the 13-th batch, evaluate loss: 0.5929802060127258:  20%|███▌              | 13/66 [00:03<00:14,  3.63it/s]evaluate for the 7-th batch, evaluate loss: 0.2201906144618988:  29%|█████▋              | 6/21 [00:01<00:03,  4.18it/s]evaluate for the 7-th batch, evaluate loss: 0.2201906144618988:  33%|██████▋             | 7/21 [00:01<00:03,  3.86it/s]Epoch: 4, train for the 244-th batch, train loss: 0.3476051390171051:  63%|██████▉    | 243/383 [02:23<01:23,  1.67it/s]Epoch: 4, train for the 244-th batch, train loss: 0.3476051390171051:  64%|███████    | 244/383 [02:23<01:23,  1.67it/s]Epoch: 10, train for the 88-th batch, train loss: 0.5647946000099182:  60%|███████▏    | 87/146 [00:53<00:41,  1.44it/s]Epoch: 10, train for the 88-th batch, train loss: 0.5647946000099182:  60%|███████▏    | 88/146 [00:53<00:38,  1.52it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6141066551208496:   4%|▌            | 10/241 [00:09<02:29,  1.54it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6141066551208496:   5%|▌            | 11/241 [00:09<02:29,  1.54it/s]Epoch: 21, train for the 78-th batch, train loss: 0.5295239686965942:  51%|██████      | 77/151 [00:21<00:43,  1.70it/s]Epoch: 21, train for the 78-th batch, train loss: 0.5295239686965942:  52%|██████▏     | 78/151 [00:21<00:44,  1.66it/s]evaluate for the 14-th batch, evaluate loss: 0.6016289591789246:  20%|███▌              | 13/66 [00:03<00:14,  3.63it/s]evaluate for the 14-th batch, evaluate loss: 0.6016289591789246:  21%|███▊              | 14/66 [00:03<00:14,  3.65it/s]evaluate for the 8-th batch, evaluate loss: 0.26502177119255066:  33%|██████▎            | 7/21 [00:01<00:03,  3.86it/s]evaluate for the 8-th batch, evaluate loss: 0.26502177119255066:  38%|███████▏           | 8/21 [00:01<00:03,  3.83it/s]evaluate for the 15-th batch, evaluate loss: 0.6223348379135132:  21%|███▊              | 14/66 [00:04<00:14,  3.65it/s]evaluate for the 15-th batch, evaluate loss: 0.6223348379135132:  23%|████              | 15/66 [00:04<00:13,  3.66it/s]evaluate for the 9-th batch, evaluate loss: 0.18879863619804382:  38%|███████▏           | 8/21 [00:02<00:03,  3.83it/s]evaluate for the 9-th batch, evaluate loss: 0.18879863619804382:  43%|████████▏          | 9/21 [00:02<00:03,  3.67it/s]Epoch: 10, train for the 89-th batch, train loss: 0.5237918496131897:  60%|███████▏    | 88/146 [00:53<00:38,  1.52it/s]Epoch: 10, train for the 89-th batch, train loss: 0.5237918496131897:  61%|███████▎    | 89/146 [00:53<00:36,  1.58it/s]Epoch: 4, train for the 245-th batch, train loss: 0.3820044696331024:  64%|███████    | 244/383 [02:24<01:23,  1.67it/s]Epoch: 4, train for the 245-th batch, train loss: 0.3820044696331024:  64%|███████    | 245/383 [02:24<01:22,  1.67it/s]evaluate for the 16-th batch, evaluate loss: 0.5610954165458679:  23%|████              | 15/66 [00:04<00:13,  3.66it/s]evaluate for the 16-th batch, evaluate loss: 0.5610954165458679:  24%|████▎             | 16/66 [00:04<00:13,  3.57it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6774566769599915:   5%|▌            | 11/241 [00:10<02:29,  1.54it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6774566769599915:   5%|▋            | 12/241 [00:10<02:29,  1.54it/s]Epoch: 21, train for the 79-th batch, train loss: 0.6244789958000183:  52%|██████▏     | 78/151 [00:22<00:44,  1.66it/s]Epoch: 21, train for the 79-th batch, train loss: 0.6244789958000183:  52%|██████▎     | 79/151 [00:22<00:44,  1.62it/s]evaluate for the 10-th batch, evaluate loss: 0.19664418697357178:  43%|███████▋          | 9/21 [00:02<00:03,  3.67it/s]evaluate for the 10-th batch, evaluate loss: 0.19664418697357178:  48%|████████         | 10/21 [00:02<00:02,  3.71it/s]evaluate for the 17-th batch, evaluate loss: 0.6071857213973999:  24%|████▎             | 16/66 [00:04<00:13,  3.57it/s]evaluate for the 17-th batch, evaluate loss: 0.6071857213973999:  26%|████▋             | 17/66 [00:04<00:13,  3.72it/s]evaluate for the 11-th batch, evaluate loss: 0.13803301751613617:  48%|████████         | 10/21 [00:02<00:02,  3.71it/s]evaluate for the 11-th batch, evaluate loss: 0.13803301751613617:  52%|████████▉        | 11/21 [00:02<00:02,  3.61it/s]Epoch: 10, train for the 90-th batch, train loss: 0.542162299156189:  61%|███████▉     | 89/146 [00:54<00:36,  1.58it/s]Epoch: 10, train for the 90-th batch, train loss: 0.542162299156189:  62%|████████     | 90/146 [00:54<00:34,  1.62it/s]Epoch: 4, train for the 246-th batch, train loss: 0.4440990388393402:  64%|███████    | 245/383 [02:25<01:22,  1.67it/s]Epoch: 4, train for the 246-th batch, train loss: 0.4440990388393402:  64%|███████    | 246/383 [02:25<01:21,  1.68it/s]evaluate for the 18-th batch, evaluate loss: 0.6115268468856812:  26%|████▋             | 17/66 [00:04<00:13,  3.72it/s]evaluate for the 18-th batch, evaluate loss: 0.6115268468856812:  27%|████▉             | 18/66 [00:04<00:13,  3.60it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6849333047866821:   5%|▋            | 12/241 [00:10<02:29,  1.54it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6849333047866821:   5%|▋            | 13/241 [00:10<02:19,  1.63it/s]evaluate for the 12-th batch, evaluate loss: 0.2462225705385208:  52%|█████████▍        | 11/21 [00:02<00:02,  3.61it/s]evaluate for the 12-th batch, evaluate loss: 0.2462225705385208:  57%|██████████▎       | 12/21 [00:02<00:02,  3.66it/s]evaluate for the 19-th batch, evaluate loss: 0.604243278503418:  27%|█████▏             | 18/66 [00:05<00:13,  3.60it/s]evaluate for the 19-th batch, evaluate loss: 0.604243278503418:  29%|█████▍             | 19/66 [00:05<00:12,  3.78it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6427801251411438:   5%|▋            | 13/241 [00:10<02:19,  1.63it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6427801251411438:   6%|▊            | 14/241 [00:10<01:51,  2.04it/s]evaluate for the 13-th batch, evaluate loss: 0.19514362514019012:  57%|█████████▋       | 12/21 [00:03<00:02,  3.66it/s]evaluate for the 13-th batch, evaluate loss: 0.19514362514019012:  62%|██████████▌      | 13/21 [00:03<00:02,  3.59it/s]Epoch: 21, train for the 80-th batch, train loss: 0.4994409382343292:  52%|██████▎     | 79/151 [00:23<00:44,  1.62it/s]Epoch: 21, train for the 80-th batch, train loss: 0.4994409382343292:  53%|██████▎     | 80/151 [00:23<00:50,  1.42it/s]Epoch: 10, train for the 91-th batch, train loss: 0.5545052886009216:  62%|███████▍    | 90/146 [00:55<00:34,  1.62it/s]Epoch: 10, train for the 91-th batch, train loss: 0.5545052886009216:  62%|███████▍    | 91/146 [00:55<00:33,  1.63it/s]Epoch: 4, train for the 247-th batch, train loss: 0.3435244560241699:  64%|███████    | 246/383 [02:25<01:21,  1.68it/s]Epoch: 4, train for the 247-th batch, train loss: 0.3435244560241699:  64%|███████    | 247/383 [02:25<01:21,  1.67it/s]evaluate for the 20-th batch, evaluate loss: 0.6289690732955933:  29%|█████▏            | 19/66 [00:05<00:12,  3.78it/s]evaluate for the 20-th batch, evaluate loss: 0.6289690732955933:  30%|█████▍            | 20/66 [00:05<00:12,  3.57it/s]evaluate for the 14-th batch, evaluate loss: 0.17475910484790802:  62%|██████████▌      | 13/21 [00:03<00:02,  3.59it/s]evaluate for the 14-th batch, evaluate loss: 0.17475910484790802:  67%|███████████▎     | 14/21 [00:03<00:01,  3.55it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6444258093833923:   6%|▊            | 14/241 [00:11<01:51,  2.04it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6444258093833923:   6%|▊            | 15/241 [00:11<01:49,  2.06it/s]evaluate for the 21-th batch, evaluate loss: 0.6140955686569214:  30%|█████▍            | 20/66 [00:05<00:12,  3.57it/s]evaluate for the 21-th batch, evaluate loss: 0.6140955686569214:  32%|█████▋            | 21/66 [00:05<00:12,  3.63it/s]evaluate for the 15-th batch, evaluate loss: 0.1708354949951172:  67%|████████████      | 14/21 [00:03<00:01,  3.55it/s]evaluate for the 15-th batch, evaluate loss: 0.1708354949951172:  71%|████████████▊     | 15/21 [00:03<00:01,  3.49it/s]Epoch: 21, train for the 81-th batch, train loss: 0.5586801767349243:  53%|██████▎     | 80/151 [00:23<00:50,  1.42it/s]Epoch: 21, train for the 81-th batch, train loss: 0.5586801767349243:  54%|██████▍     | 81/151 [00:23<00:47,  1.48it/s]evaluate for the 22-th batch, evaluate loss: 0.601792573928833:  32%|██████             | 21/66 [00:06<00:12,  3.63it/s]evaluate for the 22-th batch, evaluate loss: 0.601792573928833:  33%|██████▎            | 22/66 [00:06<00:12,  3.56it/s]Epoch: 10, train for the 92-th batch, train loss: 0.5520564913749695:  62%|███████▍    | 91/146 [00:55<00:33,  1.63it/s]Epoch: 10, train for the 92-th batch, train loss: 0.5520564913749695:  63%|███████▌    | 92/146 [00:55<00:32,  1.64it/s]Epoch: 4, train for the 248-th batch, train loss: 0.39403823018074036:  64%|██████▍   | 247/383 [02:26<01:21,  1.67it/s]Epoch: 4, train for the 248-th batch, train loss: 0.39403823018074036:  65%|██████▍   | 248/383 [02:26<01:20,  1.68it/s]evaluate for the 16-th batch, evaluate loss: 0.21273711323738098:  71%|████████████▏    | 15/21 [00:04<00:01,  3.49it/s]evaluate for the 16-th batch, evaluate loss: 0.21273711323738098:  76%|████████████▉    | 16/21 [00:04<00:01,  3.44it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6400214433670044:   6%|▊            | 15/241 [00:11<01:49,  2.06it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6400214433670044:   7%|▊            | 16/241 [00:11<01:54,  1.97it/s]evaluate for the 23-th batch, evaluate loss: 0.6234140396118164:  33%|██████            | 22/66 [00:06<00:12,  3.56it/s]evaluate for the 23-th batch, evaluate loss: 0.6234140396118164:  35%|██████▎           | 23/66 [00:06<00:11,  3.68it/s]evaluate for the 17-th batch, evaluate loss: 0.21320880949497223:  76%|████████████▉    | 16/21 [00:04<00:01,  3.44it/s]evaluate for the 17-th batch, evaluate loss: 0.21320880949497223:  81%|█████████████▊   | 17/21 [00:04<00:01,  3.46it/s]Epoch: 21, train for the 82-th batch, train loss: 0.5107499361038208:  54%|██████▍     | 81/151 [00:24<00:47,  1.48it/s]Epoch: 21, train for the 82-th batch, train loss: 0.5107499361038208:  54%|██████▌     | 82/151 [00:24<00:44,  1.56it/s]evaluate for the 24-th batch, evaluate loss: 0.6234564781188965:  35%|██████▎           | 23/66 [00:06<00:11,  3.68it/s]evaluate for the 24-th batch, evaluate loss: 0.6234564781188965:  36%|██████▌           | 24/66 [00:06<00:11,  3.61it/s]Epoch: 4, train for the 249-th batch, train loss: 0.44453078508377075:  65%|██████▍   | 248/383 [02:26<01:20,  1.68it/s]Epoch: 4, train for the 249-th batch, train loss: 0.44453078508377075:  65%|██████▌   | 249/383 [02:26<01:19,  1.69it/s]Epoch: 10, train for the 93-th batch, train loss: 0.5537790060043335:  63%|███████▌    | 92/146 [00:56<00:32,  1.64it/s]Epoch: 10, train for the 93-th batch, train loss: 0.5537790060043335:  64%|███████▋    | 93/146 [00:56<00:32,  1.65it/s]Epoch: 1, train for the 17-th batch, train loss: 0.5771465301513672:   7%|▊            | 16/241 [00:12<01:54,  1.97it/s]Epoch: 1, train for the 17-th batch, train loss: 0.5771465301513672:   7%|▉            | 17/241 [00:12<01:58,  1.89it/s]evaluate for the 18-th batch, evaluate loss: 0.19113364815711975:  81%|█████████████▊   | 17/21 [00:04<00:01,  3.46it/s]evaluate for the 18-th batch, evaluate loss: 0.19113364815711975:  86%|██████████████▌  | 18/21 [00:04<00:00,  3.40it/s]evaluate for the 25-th batch, evaluate loss: 0.6499168276786804:  36%|██████▌           | 24/66 [00:06<00:11,  3.61it/s]evaluate for the 25-th batch, evaluate loss: 0.6499168276786804:  38%|██████▊           | 25/66 [00:06<00:11,  3.70it/s]evaluate for the 19-th batch, evaluate loss: 0.24855545163154602:  86%|██████████████▌  | 18/21 [00:04<00:00,  3.40it/s]evaluate for the 19-th batch, evaluate loss: 0.24855545163154602:  90%|███████████████▍ | 19/21 [00:04<00:00,  3.45it/s]Epoch: 21, train for the 83-th batch, train loss: 0.5630250573158264:  54%|██████▌     | 82/151 [00:24<00:44,  1.56it/s]Epoch: 21, train for the 83-th batch, train loss: 0.5630250573158264:  55%|██████▌     | 83/151 [00:24<00:42,  1.61it/s]evaluate for the 26-th batch, evaluate loss: 0.6087505221366882:  38%|██████▊           | 25/66 [00:07<00:11,  3.70it/s]evaluate for the 26-th batch, evaluate loss: 0.6087505221366882:  39%|███████           | 26/66 [00:07<00:11,  3.63it/s]Epoch: 10, train for the 94-th batch, train loss: 0.5112427473068237:  64%|███████▋    | 93/146 [00:56<00:32,  1.65it/s]Epoch: 10, train for the 94-th batch, train loss: 0.5112427473068237:  64%|███████▋    | 94/146 [00:56<00:31,  1.66it/s]Epoch: 4, train for the 250-th batch, train loss: 0.4045330286026001:  65%|███████▏   | 249/383 [02:27<01:19,  1.69it/s]Epoch: 4, train for the 250-th batch, train loss: 0.4045330286026001:  65%|███████▏   | 250/383 [02:27<01:19,  1.68it/s]evaluate for the 20-th batch, evaluate loss: 0.212502583861351:  90%|█████████████████▏ | 19/21 [00:05<00:00,  3.45it/s]evaluate for the 20-th batch, evaluate loss: 0.212502583861351:  95%|██████████████████ | 20/21 [00:05<00:00,  3.40it/s]Epoch: 1, train for the 18-th batch, train loss: 0.61618971824646:   7%|█              | 17/241 [00:13<01:58,  1.89it/s]Epoch: 1, train for the 18-th batch, train loss: 0.61618971824646:   7%|█              | 18/241 [00:13<02:02,  1.83it/s]evaluate for the 27-th batch, evaluate loss: 0.620640754699707:  39%|███████▍           | 26/66 [00:07<00:11,  3.63it/s]evaluate for the 27-th batch, evaluate loss: 0.620640754699707:  41%|███████▊           | 27/66 [00:07<00:10,  3.65it/s]evaluate for the 21-th batch, evaluate loss: 0.13910944759845734:  95%|████████████████▏| 20/21 [00:05<00:00,  3.40it/s]evaluate for the 21-th batch, evaluate loss: 0.13910944759845734: 100%|█████████████████| 21/21 [00:05<00:00,  3.97it/s]
INFO:root:Epoch: 11, learning rate: 0.0001, train loss: 0.1997
INFO:root:train average_precision, 0.9743
INFO:root:train roc_auc, 0.9665
INFO:root:validate loss: 0.1424
INFO:root:validate average_precision, 0.9878
INFO:root:validate roc_auc, 0.9855
INFO:root:new node validate loss: 0.2048
INFO:root:new node validate first_1_average_precision, 0.9223
INFO:root:new node validate first_1_roc_auc, 0.9288
INFO:root:new node validate first_3_average_precision, 0.9591
INFO:root:new node validate first_3_roc_auc, 0.9606
INFO:root:new node validate first_10_average_precision, 0.9735
INFO:root:new node validate first_10_roc_auc, 0.9732
INFO:root:new node validate average_precision, 0.9748
INFO:root:new node validate roc_auc, 0.9735
INFO:root:save model ./saved_models/DyGFormer/ia-reality-call/DyGFormer_seed0_dygformer-ia-reality-call-old/DyGFormer_seed0_dygformer-ia-reality-call-old.pkl
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]Epoch: 10, train for the 95-th batch, train loss: 0.5384882092475891:  64%|███████▋    | 94/146 [00:57<00:31,  1.66it/s]Epoch: 10, train for the 95-th batch, train loss: 0.5384882092475891:  65%|███████▊    | 95/146 [00:57<00:26,  1.93it/s]evaluate for the 28-th batch, evaluate loss: 0.6200077533721924:  41%|███████▎          | 27/66 [00:07<00:10,  3.65it/s]evaluate for the 28-th batch, evaluate loss: 0.6200077533721924:  42%|███████▋          | 28/66 [00:07<00:10,  3.66it/s]Epoch: 21, train for the 84-th batch, train loss: 0.5696250200271606:  55%|██████▌     | 83/151 [00:25<00:42,  1.61it/s]Epoch: 21, train for the 84-th batch, train loss: 0.5696250200271606:  56%|██████▋     | 84/151 [00:25<00:40,  1.64it/s]evaluate for the 29-th batch, evaluate loss: 0.6093525886535645:  42%|███████▋          | 28/66 [00:07<00:10,  3.66it/s]evaluate for the 29-th batch, evaluate loss: 0.6093525886535645:  44%|███████▉          | 29/66 [00:07<00:08,  4.27it/s]evaluate for the 30-th batch, evaluate loss: 0.6093920469284058:  44%|███████▉          | 29/66 [00:07<00:08,  4.27it/s]evaluate for the 30-th batch, evaluate loss: 0.6093920469284058:  45%|████████▏         | 30/66 [00:07<00:07,  5.12it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6418958306312561:   7%|▉            | 18/241 [00:13<02:02,  1.83it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6418958306312561:   8%|█            | 19/241 [00:13<02:04,  1.78it/s]evaluate for the 31-th batch, evaluate loss: 0.6093654632568359:  45%|████████▏         | 30/66 [00:08<00:07,  5.12it/s]evaluate for the 31-th batch, evaluate loss: 0.6093654632568359:  47%|████████▍         | 31/66 [00:08<00:06,  5.55it/s]Epoch: 12, train for the 1-th batch, train loss: 0.9761483073234558:   0%|                      | 0/119 [00:00<?, ?it/s]Epoch: 12, train for the 1-th batch, train loss: 0.9761483073234558:   1%|              | 1/119 [00:00<01:07,  1.74it/s]Epoch: 4, train for the 251-th batch, train loss: 0.47338390350341797:  65%|██████▌   | 250/383 [02:28<01:19,  1.68it/s]Epoch: 4, train for the 251-th batch, train loss: 0.47338390350341797:  66%|██████▌   | 251/383 [02:28<01:30,  1.46it/s]Epoch: 10, train for the 96-th batch, train loss: 0.5395448207855225:  65%|███████▊    | 95/146 [00:57<00:26,  1.93it/s]Epoch: 10, train for the 96-th batch, train loss: 0.5395448207855225:  66%|███████▉    | 96/146 [00:57<00:27,  1.82it/s]Epoch: 21, train for the 85-th batch, train loss: 0.559356153011322:  56%|███████▏     | 84/151 [00:25<00:40,  1.64it/s]Epoch: 21, train for the 85-th batch, train loss: 0.559356153011322:  56%|███████▎     | 85/151 [00:25<00:39,  1.65it/s]evaluate for the 32-th batch, evaluate loss: 0.6091576814651489:  47%|████████▍         | 31/66 [00:08<00:06,  5.55it/s]evaluate for the 32-th batch, evaluate loss: 0.6091576814651489:  48%|████████▋         | 32/66 [00:08<00:06,  4.94it/s]Epoch: 1, train for the 20-th batch, train loss: 0.5898400545120239:   8%|█            | 19/241 [00:14<02:04,  1.78it/s]Epoch: 1, train for the 20-th batch, train loss: 0.5898400545120239:   8%|█            | 20/241 [00:14<02:04,  1.77it/s]evaluate for the 33-th batch, evaluate loss: 0.6076743006706238:  48%|████████▋         | 32/66 [00:08<00:06,  4.94it/s]evaluate for the 33-th batch, evaluate loss: 0.6076743006706238:  50%|█████████         | 33/66 [00:08<00:07,  4.42it/s]Epoch: 12, train for the 2-th batch, train loss: 0.48257574439048767:   1%|             | 1/119 [00:01<01:07,  1.74it/s]Epoch: 12, train for the 2-th batch, train loss: 0.48257574439048767:   2%|▏            | 2/119 [00:01<01:09,  1.68it/s]Epoch: 4, train for the 252-th batch, train loss: 0.4168446660041809:  66%|███████▏   | 251/383 [02:28<01:30,  1.46it/s]Epoch: 4, train for the 252-th batch, train loss: 0.4168446660041809:  66%|███████▏   | 252/383 [02:28<01:26,  1.51it/s]Epoch: 10, train for the 97-th batch, train loss: 0.5437097549438477:  66%|███████▉    | 96/146 [00:58<00:27,  1.82it/s]Epoch: 10, train for the 97-th batch, train loss: 0.5437097549438477:  66%|███████▉    | 97/146 [00:58<00:27,  1.78it/s]Epoch: 21, train for the 86-th batch, train loss: 0.5532218217849731:  56%|██████▊     | 85/151 [00:26<00:39,  1.65it/s]Epoch: 21, train for the 86-th batch, train loss: 0.5532218217849731:  57%|██████▊     | 86/151 [00:26<00:38,  1.68it/s]evaluate for the 34-th batch, evaluate loss: 0.5984963178634644:  50%|█████████         | 33/66 [00:08<00:07,  4.42it/s]evaluate for the 34-th batch, evaluate loss: 0.5984963178634644:  52%|█████████▎        | 34/66 [00:08<00:07,  4.08it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6435418725013733:   8%|█            | 20/241 [00:14<02:04,  1.77it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6435418725013733:   9%|█▏           | 21/241 [00:14<02:03,  1.78it/s]evaluate for the 35-th batch, evaluate loss: 0.613584041595459:  52%|█████████▊         | 34/66 [00:09<00:07,  4.08it/s]evaluate for the 35-th batch, evaluate loss: 0.613584041595459:  53%|██████████         | 35/66 [00:09<00:07,  3.96it/s]Epoch: 4, train for the 253-th batch, train loss: 0.3648883104324341:  66%|███████▏   | 252/383 [02:29<01:26,  1.51it/s]Epoch: 4, train for the 253-th batch, train loss: 0.3648883104324341:  66%|███████▎   | 253/383 [02:29<01:23,  1.56it/s]Epoch: 12, train for the 3-th batch, train loss: 0.313031405210495:   2%|▎              | 2/119 [00:01<01:09,  1.68it/s]Epoch: 12, train for the 3-th batch, train loss: 0.313031405210495:   3%|▍              | 3/119 [00:01<01:10,  1.64it/s]Epoch: 10, train for the 98-th batch, train loss: 0.5627853870391846:  66%|███████▉    | 97/146 [00:59<00:27,  1.78it/s]Epoch: 10, train for the 98-th batch, train loss: 0.5627853870391846:  67%|████████    | 98/146 [00:59<00:27,  1.74it/s]Epoch: 21, train for the 87-th batch, train loss: 0.5372238159179688:  57%|██████▊     | 86/151 [00:27<00:38,  1.68it/s]Epoch: 21, train for the 87-th batch, train loss: 0.5372238159179688:  58%|██████▉     | 87/151 [00:27<00:37,  1.71it/s]evaluate for the 36-th batch, evaluate loss: 0.6301536560058594:  53%|█████████▌        | 35/66 [00:09<00:07,  3.96it/s]evaluate for the 36-th batch, evaluate loss: 0.6301536560058594:  55%|█████████▊        | 36/66 [00:09<00:07,  3.77it/s]evaluate for the 37-th batch, evaluate loss: 0.6480025053024292:  55%|█████████▊        | 36/66 [00:09<00:07,  3.77it/s]evaluate for the 37-th batch, evaluate loss: 0.6480025053024292:  56%|██████████        | 37/66 [00:09<00:07,  3.85it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6669446229934692:   9%|█▏           | 21/241 [00:15<02:03,  1.78it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6669446229934692:   9%|█▏           | 22/241 [00:15<02:06,  1.74it/s]Epoch: 4, train for the 254-th batch, train loss: 0.47410085797309875:  66%|██████▌   | 253/383 [02:30<01:23,  1.56it/s]Epoch: 4, train for the 254-th batch, train loss: 0.47410085797309875:  66%|██████▋   | 254/383 [02:30<01:20,  1.60it/s]evaluate for the 38-th batch, evaluate loss: 0.6024245023727417:  56%|██████████        | 37/66 [00:10<00:07,  3.85it/s]evaluate for the 38-th batch, evaluate loss: 0.6024245023727417:  58%|██████████▎       | 38/66 [00:10<00:07,  3.68it/s]Epoch: 21, train for the 88-th batch, train loss: 0.6097337007522583:  58%|██████▉     | 87/151 [00:27<00:37,  1.71it/s]Epoch: 21, train for the 88-th batch, train loss: 0.6097337007522583:  58%|██████▉     | 88/151 [00:27<00:37,  1.69it/s]Epoch: 12, train for the 4-th batch, train loss: 0.3075881600379944:   3%|▎             | 3/119 [00:02<01:10,  1.64it/s]Epoch: 12, train for the 4-th batch, train loss: 0.3075881600379944:   3%|▍             | 4/119 [00:02<01:12,  1.59it/s]Epoch: 10, train for the 99-th batch, train loss: 0.5403100252151489:  67%|████████    | 98/146 [00:59<00:27,  1.74it/s]Epoch: 10, train for the 99-th batch, train loss: 0.5403100252151489:  68%|████████▏   | 99/146 [00:59<00:27,  1.69it/s]evaluate for the 39-th batch, evaluate loss: 0.5992152094841003:  58%|██████████▎       | 38/66 [00:10<00:07,  3.68it/s]evaluate for the 39-th batch, evaluate loss: 0.5992152094841003:  59%|██████████▋       | 39/66 [00:10<00:07,  3.83it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6583228707313538:   9%|█▏           | 22/241 [00:16<02:06,  1.74it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6583228707313538:  10%|█▏           | 23/241 [00:16<02:06,  1.72it/s]Epoch: 4, train for the 255-th batch, train loss: 0.3838469684123993:  66%|███████▎   | 254/383 [02:30<01:20,  1.60it/s]Epoch: 4, train for the 255-th batch, train loss: 0.3838469684123993:  67%|███████▎   | 255/383 [02:30<01:18,  1.62it/s]evaluate for the 40-th batch, evaluate loss: 0.6066603064537048:  59%|██████████▋       | 39/66 [00:10<00:07,  3.83it/s]evaluate for the 40-th batch, evaluate loss: 0.6066603064537048:  61%|██████████▉       | 40/66 [00:10<00:07,  3.62it/s]Epoch: 21, train for the 89-th batch, train loss: 0.5648649334907532:  58%|██████▉     | 88/151 [00:28<00:37,  1.69it/s]Epoch: 21, train for the 89-th batch, train loss: 0.5648649334907532:  59%|███████     | 89/151 [00:28<00:36,  1.69it/s]Epoch: 12, train for the 5-th batch, train loss: 0.33066755533218384:   3%|▍            | 4/119 [00:03<01:12,  1.59it/s]Epoch: 10, train for the 100-th batch, train loss: 0.5757302641868591:  68%|███████▍   | 99/146 [01:00<00:27,  1.69it/s]Epoch: 12, train for the 5-th batch, train loss: 0.33066755533218384:   4%|▌            | 5/119 [00:03<01:12,  1.57it/s]Epoch: 10, train for the 100-th batch, train loss: 0.5757302641868591:  68%|██████▊   | 100/146 [01:00<00:28,  1.63it/s]evaluate for the 41-th batch, evaluate loss: 0.5773255228996277:  61%|██████████▉       | 40/66 [00:10<00:07,  3.62it/s]evaluate for the 41-th batch, evaluate loss: 0.5773255228996277:  62%|███████████▏      | 41/66 [00:10<00:06,  3.68it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6379090547561646:  10%|█▏           | 23/241 [00:16<02:06,  1.72it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6379090547561646:  10%|█▎           | 24/241 [00:16<02:07,  1.71it/s]evaluate for the 42-th batch, evaluate loss: 0.6141911745071411:  62%|███████████▏      | 41/66 [00:11<00:06,  3.68it/s]evaluate for the 42-th batch, evaluate loss: 0.6141911745071411:  64%|███████████▍      | 42/66 [00:11<00:06,  3.58it/s]Epoch: 4, train for the 256-th batch, train loss: 0.4325466752052307:  67%|███████▎   | 255/383 [02:31<01:18,  1.62it/s]Epoch: 4, train for the 256-th batch, train loss: 0.4325466752052307:  67%|███████▎   | 256/383 [02:31<01:17,  1.64it/s]Epoch: 21, train for the 90-th batch, train loss: 0.5571133494377136:  59%|███████     | 89/151 [00:28<00:36,  1.69it/s]Epoch: 21, train for the 90-th batch, train loss: 0.5571133494377136:  60%|███████▏    | 90/151 [00:28<00:36,  1.69it/s]Epoch: 12, train for the 6-th batch, train loss: 0.27303314208984375:   4%|▌            | 5/119 [00:03<01:12,  1.57it/s]Epoch: 12, train for the 6-th batch, train loss: 0.27303314208984375:   5%|▋            | 6/119 [00:03<01:11,  1.58it/s]Epoch: 10, train for the 101-th batch, train loss: 0.5136617422103882:  68%|██████▊   | 100/146 [01:00<00:28,  1.63it/s]Epoch: 10, train for the 101-th batch, train loss: 0.5136617422103882:  69%|██████▉   | 101/146 [01:00<00:28,  1.60it/s]evaluate for the 43-th batch, evaluate loss: 0.5778249502182007:  64%|███████████▍      | 42/66 [00:11<00:06,  3.58it/s]evaluate for the 43-th batch, evaluate loss: 0.5778249502182007:  65%|███████████▋      | 43/66 [00:11<00:06,  3.69it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6637563705444336:  10%|█▎           | 24/241 [00:17<02:07,  1.71it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6637563705444336:  10%|█▎           | 25/241 [00:17<02:06,  1.70it/s]evaluate for the 44-th batch, evaluate loss: 0.5808757543563843:  65%|███████████▋      | 43/66 [00:11<00:06,  3.69it/s]evaluate for the 44-th batch, evaluate loss: 0.5808757543563843:  67%|████████████      | 44/66 [00:11<00:06,  3.61it/s]Epoch: 4, train for the 257-th batch, train loss: 0.4918721914291382:  67%|███████▎   | 256/383 [02:31<01:17,  1.64it/s]Epoch: 4, train for the 257-th batch, train loss: 0.4918721914291382:  67%|███████▍   | 257/383 [02:31<01:15,  1.66it/s]Epoch: 21, train for the 91-th batch, train loss: 0.4792107939720154:  60%|███████▏    | 90/151 [00:29<00:36,  1.69it/s]Epoch: 21, train for the 91-th batch, train loss: 0.4792107939720154:  60%|███████▏    | 91/151 [00:29<00:35,  1.69it/s]evaluate for the 45-th batch, evaluate loss: 0.6192924976348877:  67%|████████████      | 44/66 [00:11<00:06,  3.61it/s]evaluate for the 45-th batch, evaluate loss: 0.6192924976348877:  68%|████████████▎     | 45/66 [00:11<00:05,  3.69it/s]Epoch: 12, train for the 7-th batch, train loss: 0.23859919607639313:   5%|▋            | 6/119 [00:04<01:11,  1.58it/s]Epoch: 12, train for the 7-th batch, train loss: 0.23859919607639313:   6%|▊            | 7/119 [00:04<01:11,  1.56it/s]Epoch: 10, train for the 102-th batch, train loss: 0.5196698904037476:  69%|██████▉   | 101/146 [01:01<00:28,  1.60it/s]Epoch: 10, train for the 102-th batch, train loss: 0.5196698904037476:  70%|██████▉   | 102/146 [01:01<00:27,  1.60it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6375066041946411:  10%|█▎           | 25/241 [00:17<02:06,  1.70it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6375066041946411:  11%|█▍           | 26/241 [00:17<02:05,  1.72it/s]evaluate for the 46-th batch, evaluate loss: 0.6238117218017578:  68%|████████████▎     | 45/66 [00:12<00:05,  3.69it/s]evaluate for the 46-th batch, evaluate loss: 0.6238117218017578:  70%|████████████▌     | 46/66 [00:12<00:05,  3.65it/s]Epoch: 4, train for the 258-th batch, train loss: 0.4032577872276306:  67%|███████▍   | 257/383 [02:32<01:15,  1.66it/s]Epoch: 4, train for the 258-th batch, train loss: 0.4032577872276306:  67%|███████▍   | 258/383 [02:32<01:15,  1.66it/s]Epoch: 21, train for the 92-th batch, train loss: 0.56419837474823:  60%|████████▍     | 91/151 [00:30<00:35,  1.69it/s]Epoch: 21, train for the 92-th batch, train loss: 0.56419837474823:  61%|████████▌     | 92/151 [00:30<00:34,  1.71it/s]evaluate for the 47-th batch, evaluate loss: 0.6273702383041382:  70%|████████████▌     | 46/66 [00:12<00:05,  3.65it/s]evaluate for the 47-th batch, evaluate loss: 0.6273702383041382:  71%|████████████▊     | 47/66 [00:12<00:05,  3.66it/s]Epoch: 12, train for the 8-th batch, train loss: 0.19480910897254944:   6%|▊            | 7/119 [00:05<01:11,  1.56it/s]Epoch: 12, train for the 8-th batch, train loss: 0.19480910897254944:   7%|▊            | 8/119 [00:05<01:11,  1.55it/s]Epoch: 10, train for the 103-th batch, train loss: 0.5231956243515015:  70%|██████▉   | 102/146 [01:02<00:27,  1.60it/s]Epoch: 10, train for the 103-th batch, train loss: 0.5231956243515015:  71%|███████   | 103/146 [01:02<00:27,  1.58it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6548764109611511:  11%|█▍           | 26/241 [00:18<02:05,  1.72it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6548764109611511:  11%|█▍           | 27/241 [00:18<02:05,  1.71it/s]evaluate for the 48-th batch, evaluate loss: 0.631712794303894:  71%|█████████████▌     | 47/66 [00:12<00:05,  3.66it/s]evaluate for the 48-th batch, evaluate loss: 0.631712794303894:  73%|█████████████▊     | 48/66 [00:12<00:04,  3.67it/s]Epoch: 4, train for the 259-th batch, train loss: 0.39548009634017944:  67%|██████▋   | 258/383 [02:33<01:15,  1.66it/s]Epoch: 4, train for the 259-th batch, train loss: 0.39548009634017944:  68%|██████▊   | 259/383 [02:33<01:14,  1.66it/s]Epoch: 21, train for the 93-th batch, train loss: 0.5119694471359253:  61%|███████▎    | 92/151 [00:30<00:34,  1.71it/s]Epoch: 21, train for the 93-th batch, train loss: 0.5119694471359253:  62%|███████▍    | 93/151 [00:30<00:34,  1.70it/s]evaluate for the 49-th batch, evaluate loss: 0.6413789391517639:  73%|█████████████     | 48/66 [00:13<00:04,  3.67it/s]evaluate for the 49-th batch, evaluate loss: 0.6413789391517639:  74%|█████████████▎    | 49/66 [00:13<00:04,  3.58it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6195367574691772:  11%|█▍           | 27/241 [00:18<02:05,  1.71it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6195367574691772:  12%|█▌           | 28/241 [00:18<02:03,  1.73it/s]Epoch: 12, train for the 9-th batch, train loss: 0.2313421368598938:   7%|▉             | 8/119 [00:05<01:11,  1.55it/s]Epoch: 12, train for the 9-th batch, train loss: 0.2313421368598938:   8%|█             | 9/119 [00:05<01:11,  1.55it/s]Epoch: 10, train for the 104-th batch, train loss: 0.5590783953666687:  71%|███████   | 103/146 [01:02<00:27,  1.58it/s]Epoch: 10, train for the 104-th batch, train loss: 0.5590783953666687:  71%|███████   | 104/146 [01:02<00:26,  1.56it/s]evaluate for the 50-th batch, evaluate loss: 0.6285037398338318:  74%|█████████████▎    | 49/66 [00:13<00:04,  3.58it/s]evaluate for the 50-th batch, evaluate loss: 0.6285037398338318:  76%|█████████████▋    | 50/66 [00:13<00:04,  3.71it/s]Epoch: 4, train for the 260-th batch, train loss: 0.4330427348613739:  68%|███████▍   | 259/383 [02:33<01:14,  1.66it/s]Epoch: 4, train for the 260-th batch, train loss: 0.4330427348613739:  68%|███████▍   | 260/383 [02:33<01:13,  1.68it/s]Epoch: 21, train for the 94-th batch, train loss: 0.48539358377456665:  62%|██████▊    | 93/151 [00:31<00:34,  1.70it/s]Epoch: 21, train for the 94-th batch, train loss: 0.48539358377456665:  62%|██████▊    | 94/151 [00:31<00:33,  1.72it/s]evaluate for the 51-th batch, evaluate loss: 0.6136513352394104:  76%|█████████████▋    | 50/66 [00:13<00:04,  3.71it/s]evaluate for the 51-th batch, evaluate loss: 0.6136513352394104:  77%|█████████████▉    | 51/66 [00:13<00:04,  3.59it/s]evaluate for the 52-th batch, evaluate loss: 0.6193355917930603:  77%|█████████████▉    | 51/66 [00:13<00:04,  3.59it/s]evaluate for the 52-th batch, evaluate loss: 0.6193355917930603:  79%|██████████████▏   | 52/66 [00:13<00:03,  3.76it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6396675109863281:  12%|█▌           | 28/241 [00:19<02:03,  1.73it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6396675109863281:  12%|█▌           | 29/241 [00:19<02:03,  1.72it/s]Epoch: 12, train for the 10-th batch, train loss: 0.24969986081123352:   8%|▉           | 9/119 [00:06<01:11,  1.55it/s]Epoch: 12, train for the 10-th batch, train loss: 0.24969986081123352:   8%|▉          | 10/119 [00:06<01:10,  1.54it/s]Epoch: 10, train for the 105-th batch, train loss: 0.5546798706054688:  71%|███████   | 104/146 [01:03<00:26,  1.56it/s]Epoch: 10, train for the 105-th batch, train loss: 0.5546798706054688:  72%|███████▏  | 105/146 [01:03<00:26,  1.55it/s]Epoch: 4, train for the 261-th batch, train loss: 0.4525288939476013:  68%|███████▍   | 260/383 [02:34<01:13,  1.68it/s]Epoch: 4, train for the 261-th batch, train loss: 0.4525288939476013:  68%|███████▍   | 261/383 [02:34<01:13,  1.67it/s]Epoch: 21, train for the 95-th batch, train loss: 0.5232067704200745:  62%|███████▍    | 94/151 [00:31<00:33,  1.72it/s]Epoch: 21, train for the 95-th batch, train loss: 0.5232067704200745:  63%|███████▌    | 95/151 [00:31<00:32,  1.72it/s]evaluate for the 53-th batch, evaluate loss: 0.606200635433197:  79%|██████████████▉    | 52/66 [00:14<00:03,  3.76it/s]evaluate for the 53-th batch, evaluate loss: 0.606200635433197:  80%|███████████████▎   | 53/66 [00:14<00:03,  3.58it/s]evaluate for the 54-th batch, evaluate loss: 0.6401855945587158:  80%|██████████████▍   | 53/66 [00:14<00:03,  3.58it/s]evaluate for the 54-th batch, evaluate loss: 0.6401855945587158:  82%|██████████████▋   | 54/66 [00:14<00:03,  3.64it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6685011386871338:  12%|█▌           | 29/241 [00:20<02:03,  1.72it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6685011386871338:  12%|█▌           | 30/241 [00:20<02:03,  1.70it/s]Epoch: 12, train for the 11-th batch, train loss: 0.22710108757019043:   8%|▉          | 10/119 [00:07<01:10,  1.54it/s]Epoch: 12, train for the 11-th batch, train loss: 0.22710108757019043:   9%|█          | 11/119 [00:07<01:10,  1.53it/s]Epoch: 10, train for the 106-th batch, train loss: 0.527891218662262:  72%|███████▉   | 105/146 [01:04<00:26,  1.55it/s]Epoch: 10, train for the 106-th batch, train loss: 0.527891218662262:  73%|███████▉   | 106/146 [01:04<00:25,  1.54it/s]evaluate for the 55-th batch, evaluate loss: 0.5990597009658813:  82%|██████████████▋   | 54/66 [00:14<00:03,  3.64it/s]evaluate for the 55-th batch, evaluate loss: 0.5990597009658813:  83%|███████████████   | 55/66 [00:14<00:03,  3.53it/s]Epoch: 4, train for the 262-th batch, train loss: 0.4252442717552185:  68%|███████▍   | 261/383 [02:34<01:13,  1.67it/s]Epoch: 4, train for the 262-th batch, train loss: 0.4252442717552185:  68%|███████▌   | 262/383 [02:34<01:12,  1.67it/s]Epoch: 21, train for the 96-th batch, train loss: 0.5400099158287048:  63%|███████▌    | 95/151 [00:32<00:32,  1.72it/s]Epoch: 21, train for the 96-th batch, train loss: 0.5400099158287048:  64%|███████▋    | 96/151 [00:32<00:32,  1.70it/s]evaluate for the 56-th batch, evaluate loss: 0.6287567615509033:  83%|███████████████   | 55/66 [00:14<00:03,  3.53it/s]evaluate for the 56-th batch, evaluate loss: 0.6287567615509033:  85%|███████████████▎  | 56/66 [00:14<00:02,  3.66it/s]Epoch: 1, train for the 31-th batch, train loss: 0.5671889185905457:  12%|█▌           | 30/241 [00:20<02:03,  1.70it/s]Epoch: 1, train for the 31-th batch, train loss: 0.5671889185905457:  13%|█▋           | 31/241 [00:20<02:01,  1.72it/s]evaluate for the 57-th batch, evaluate loss: 0.6140556931495667:  85%|███████████████▎  | 56/66 [00:15<00:02,  3.66it/s]evaluate for the 57-th batch, evaluate loss: 0.6140556931495667:  86%|███████████████▌  | 57/66 [00:15<00:02,  3.59it/s]Epoch: 12, train for the 12-th batch, train loss: 0.28058338165283203:   9%|█          | 11/119 [00:07<01:10,  1.53it/s]Epoch: 10, train for the 107-th batch, train loss: 0.5353057980537415:  73%|███████▎  | 106/146 [01:04<00:25,  1.54it/s]Epoch: 12, train for the 12-th batch, train loss: 0.28058338165283203:  10%|█          | 12/119 [00:07<01:10,  1.52it/s]Epoch: 10, train for the 107-th batch, train loss: 0.5353057980537415:  73%|███████▎  | 107/146 [01:04<00:25,  1.53it/s]Epoch: 21, train for the 97-th batch, train loss: 0.593046247959137:  64%|████████▎    | 96/151 [00:32<00:32,  1.70it/s]Epoch: 4, train for the 263-th batch, train loss: 0.48163658380508423:  68%|██████▊   | 262/383 [02:35<01:12,  1.67it/s]Epoch: 21, train for the 97-th batch, train loss: 0.593046247959137:  64%|████████▎    | 97/151 [00:32<00:31,  1.72it/s]Epoch: 4, train for the 263-th batch, train loss: 0.48163658380508423:  69%|██████▊   | 263/383 [02:35<01:11,  1.68it/s]evaluate for the 58-th batch, evaluate loss: 0.6508309841156006:  86%|███████████████▌  | 57/66 [00:15<00:02,  3.59it/s]evaluate for the 58-th batch, evaluate loss: 0.6508309841156006:  88%|███████████████▊  | 58/66 [00:15<00:02,  3.70it/s]Epoch: 1, train for the 32-th batch, train loss: 0.5170425176620483:  13%|█▋           | 31/241 [00:21<02:01,  1.72it/s]Epoch: 1, train for the 32-th batch, train loss: 0.5170425176620483:  13%|█▋           | 32/241 [00:21<02:02,  1.71it/s]evaluate for the 59-th batch, evaluate loss: 0.6204211115837097:  88%|███████████████▊  | 58/66 [00:15<00:02,  3.70it/s]evaluate for the 59-th batch, evaluate loss: 0.6204211115837097:  89%|████████████████  | 59/66 [00:15<00:01,  3.64it/s]Epoch: 21, train for the 98-th batch, train loss: 0.5893262028694153:  64%|███████▋    | 97/151 [00:33<00:31,  1.72it/s]Epoch: 21, train for the 98-th batch, train loss: 0.5893262028694153:  65%|███████▊    | 98/151 [00:33<00:31,  1.71it/s]Epoch: 4, train for the 264-th batch, train loss: 0.38263359665870667:  69%|██████▊   | 263/383 [02:36<01:11,  1.68it/s]Epoch: 4, train for the 264-th batch, train loss: 0.38263359665870667:  69%|██████▉   | 264/383 [02:36<01:11,  1.67it/s]Epoch: 10, train for the 108-th batch, train loss: 0.5760470032691956:  73%|███████▎  | 107/146 [01:05<00:25,  1.53it/s]Epoch: 12, train for the 13-th batch, train loss: 0.21467280387878418:  10%|█          | 12/119 [00:08<01:10,  1.52it/s]Epoch: 10, train for the 108-th batch, train loss: 0.5760470032691956:  74%|███████▍  | 108/146 [01:05<00:24,  1.52it/s]Epoch: 12, train for the 13-th batch, train loss: 0.21467280387878418:  11%|█▏         | 13/119 [00:08<01:09,  1.52it/s]evaluate for the 60-th batch, evaluate loss: 0.6382293701171875:  89%|████████████████  | 59/66 [00:16<00:01,  3.64it/s]evaluate for the 60-th batch, evaluate loss: 0.6382293701171875:  91%|████████████████▎ | 60/66 [00:16<00:01,  3.65it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6389693021774292:  13%|█▋           | 32/241 [00:21<02:02,  1.71it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6389693021774292:  14%|█▊           | 33/241 [00:21<02:00,  1.72it/s]evaluate for the 61-th batch, evaluate loss: 0.6320921778678894:  91%|████████████████▎ | 60/66 [00:16<00:01,  3.65it/s]evaluate for the 61-th batch, evaluate loss: 0.6320921778678894:  92%|████████████████▋ | 61/66 [00:16<00:01,  3.66it/s]Epoch: 21, train for the 99-th batch, train loss: 0.6052282452583313:  65%|███████▊    | 98/151 [00:34<00:31,  1.71it/s]Epoch: 21, train for the 99-th batch, train loss: 0.6052282452583313:  66%|███████▊    | 99/151 [00:34<00:30,  1.72it/s]Epoch: 4, train for the 265-th batch, train loss: 0.40789172053337097:  69%|██████▉   | 264/383 [02:36<01:11,  1.67it/s]Epoch: 4, train for the 265-th batch, train loss: 0.40789172053337097:  69%|██████▉   | 265/383 [02:36<01:10,  1.67it/s]Epoch: 12, train for the 14-th batch, train loss: 0.24414023756980896:  11%|█▏         | 13/119 [00:09<01:09,  1.52it/s]Epoch: 10, train for the 109-th batch, train loss: 0.5044798254966736:  74%|███████▍  | 108/146 [01:06<00:24,  1.52it/s]Epoch: 12, train for the 14-th batch, train loss: 0.24414023756980896:  12%|█▎         | 14/119 [00:09<01:09,  1.51it/s]Epoch: 10, train for the 109-th batch, train loss: 0.5044798254966736:  75%|███████▍  | 109/146 [01:06<00:24,  1.51it/s]evaluate for the 62-th batch, evaluate loss: 0.6604610085487366:  92%|████████████████▋ | 61/66 [00:16<00:01,  3.66it/s]evaluate for the 62-th batch, evaluate loss: 0.6604610085487366:  94%|████████████████▉ | 62/66 [00:16<00:01,  3.57it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6027169823646545:  14%|█▊           | 33/241 [00:22<02:00,  1.72it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6027169823646545:  14%|█▊           | 34/241 [00:22<02:01,  1.70it/s]evaluate for the 63-th batch, evaluate loss: 0.632559061050415:  94%|█████████████████▊ | 62/66 [00:16<00:01,  3.57it/s]evaluate for the 63-th batch, evaluate loss: 0.632559061050415:  95%|██████████████████▏| 63/66 [00:16<00:00,  3.70it/s]Epoch: 21, train for the 100-th batch, train loss: 0.651844322681427:  66%|███████▊    | 99/151 [00:34<00:30,  1.72it/s]Epoch: 21, train for the 100-th batch, train loss: 0.651844322681427:  66%|███████▎   | 100/151 [00:34<00:29,  1.71it/s]Epoch: 4, train for the 266-th batch, train loss: 0.456062912940979:  69%|████████▎   | 265/383 [02:37<01:10,  1.67it/s]Epoch: 4, train for the 266-th batch, train loss: 0.456062912940979:  69%|████████▎   | 266/383 [02:37<01:09,  1.69it/s]evaluate for the 64-th batch, evaluate loss: 0.6230286955833435:  95%|█████████████████▏| 63/66 [00:17<00:00,  3.70it/s]evaluate for the 64-th batch, evaluate loss: 0.6230286955833435:  97%|█████████████████▍| 64/66 [00:17<00:00,  3.57it/s]Epoch: 10, train for the 110-th batch, train loss: 0.5108518004417419:  75%|███████▍  | 109/146 [01:06<00:24,  1.51it/s]Epoch: 12, train for the 15-th batch, train loss: 0.19546352326869965:  12%|█▎         | 14/119 [00:09<01:09,  1.51it/s]Epoch: 10, train for the 110-th batch, train loss: 0.5108518004417419:  75%|███████▌  | 110/146 [01:06<00:23,  1.51it/s]Epoch: 12, train for the 15-th batch, train loss: 0.19546352326869965:  13%|█▍         | 15/119 [00:09<01:08,  1.51it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6364625692367554:  14%|█▊           | 34/241 [00:23<02:01,  1.70it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6364625692367554:  15%|█▉           | 35/241 [00:23<02:01,  1.69it/s]evaluate for the 65-th batch, evaluate loss: 0.5919878482818604:  97%|█████████████████▍| 64/66 [00:17<00:00,  3.57it/s]evaluate for the 65-th batch, evaluate loss: 0.5919878482818604:  98%|█████████████████▋| 65/66 [00:17<00:00,  3.77it/s]evaluate for the 66-th batch, evaluate loss: 0.6376510262489319:  98%|█████████████████▋| 65/66 [00:17<00:00,  3.77it/s]evaluate for the 66-th batch, evaluate loss: 0.6376510262489319: 100%|██████████████████| 66/66 [00:17<00:00,  3.88it/s]evaluate for the 66-th batch, evaluate loss: 0.6376510262489319: 100%|██████████████████| 66/66 [00:17<00:00,  3.74it/s]
  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 21, train for the 101-th batch, train loss: 0.6655464172363281:  66%|██████▌   | 100/151 [00:35<00:29,  1.71it/s]Epoch: 21, train for the 101-th batch, train loss: 0.6655464172363281:  67%|██████▋   | 101/151 [00:35<00:29,  1.69it/s]Epoch: 4, train for the 267-th batch, train loss: 0.4292246103286743:  69%|███████▋   | 266/383 [02:37<01:09,  1.69it/s]Epoch: 4, train for the 267-th batch, train loss: 0.4292246103286743:  70%|███████▋   | 267/383 [02:37<01:08,  1.69it/s]evaluate for the 1-th batch, evaluate loss: 0.644421398639679:   0%|                             | 0/40 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.644421398639679:   2%|▌                    | 1/40 [00:00<00:10,  3.90it/s]Epoch: 10, train for the 111-th batch, train loss: 0.5537108182907104:  75%|███████▌  | 110/146 [01:07<00:23,  1.51it/s]Epoch: 12, train for the 16-th batch, train loss: 0.24046073853969574:  13%|█▍         | 15/119 [00:10<01:08,  1.51it/s]Epoch: 10, train for the 111-th batch, train loss: 0.5537108182907104:  76%|███████▌  | 111/146 [01:07<00:23,  1.51it/s]Epoch: 12, train for the 16-th batch, train loss: 0.24046073853969574:  13%|█▍         | 16/119 [00:10<01:08,  1.51it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6318725347518921:  15%|█▉           | 35/241 [00:23<02:01,  1.69it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6318725347518921:  15%|█▉           | 36/241 [00:23<02:01,  1.69it/s]evaluate for the 2-th batch, evaluate loss: 0.6677830219268799:   2%|▌                   | 1/40 [00:00<00:10,  3.90it/s]evaluate for the 2-th batch, evaluate loss: 0.6677830219268799:   5%|█                   | 2/40 [00:00<00:10,  3.55it/s]Epoch: 4, train for the 268-th batch, train loss: 0.4386109411716461:  70%|███████▋   | 267/383 [02:38<01:08,  1.69it/s]Epoch: 4, train for the 268-th batch, train loss: 0.4386109411716461:  70%|███████▋   | 268/383 [02:38<01:06,  1.72it/s]Epoch: 21, train for the 102-th batch, train loss: 0.5897578597068787:  67%|██████▋   | 101/151 [00:35<00:29,  1.69it/s]Epoch: 21, train for the 102-th batch, train loss: 0.5897578597068787:  68%|██████▊   | 102/151 [00:35<00:28,  1.69it/s]evaluate for the 3-th batch, evaluate loss: 0.6508735418319702:   5%|█                   | 2/40 [00:00<00:10,  3.55it/s]evaluate for the 3-th batch, evaluate loss: 0.6508735418319702:   8%|█▌                  | 3/40 [00:00<00:10,  3.69it/s]Epoch: 1, train for the 37-th batch, train loss: 0.5983443260192871:  15%|█▉           | 36/241 [00:24<02:01,  1.69it/s]Epoch: 1, train for the 37-th batch, train loss: 0.5983443260192871:  15%|█▉           | 37/241 [00:24<02:00,  1.69it/s]Epoch: 12, train for the 17-th batch, train loss: 0.22013439238071442:  13%|█▍         | 16/119 [00:11<01:08,  1.51it/s]Epoch: 10, train for the 112-th batch, train loss: 0.5390933156013489:  76%|███████▌  | 111/146 [01:08<00:23,  1.51it/s]Epoch: 12, train for the 17-th batch, train loss: 0.22013439238071442:  14%|█▌         | 17/119 [00:11<01:07,  1.51it/s]Epoch: 10, train for the 112-th batch, train loss: 0.5390933156013489:  77%|███████▋  | 112/146 [01:08<00:22,  1.51it/s]evaluate for the 4-th batch, evaluate loss: 0.6728767156600952:   8%|█▌                  | 3/40 [00:01<00:10,  3.69it/s]evaluate for the 4-th batch, evaluate loss: 0.6728767156600952:  10%|██                  | 4/40 [00:01<00:10,  3.56it/s]Epoch: 4, train for the 269-th batch, train loss: 0.4982214868068695:  70%|███████▋   | 268/383 [02:38<01:06,  1.72it/s]Epoch: 4, train for the 269-th batch, train loss: 0.4982214868068695:  70%|███████▋   | 269/383 [02:38<01:05,  1.73it/s]Epoch: 21, train for the 103-th batch, train loss: 0.6225789785385132:  68%|██████▊   | 102/151 [00:36<00:28,  1.69it/s]Epoch: 21, train for the 103-th batch, train loss: 0.6225789785385132:  68%|██████▊   | 103/151 [00:36<00:28,  1.69it/s]evaluate for the 5-th batch, evaluate loss: 0.6377636790275574:  10%|██                  | 4/40 [00:01<00:10,  3.56it/s]evaluate for the 5-th batch, evaluate loss: 0.6377636790275574:  12%|██▌                 | 5/40 [00:01<00:09,  3.66it/s]Epoch: 1, train for the 38-th batch, train loss: 0.5414361953735352:  15%|█▉           | 37/241 [00:24<02:00,  1.69it/s]Epoch: 1, train for the 38-th batch, train loss: 0.5414361953735352:  16%|██           | 38/241 [00:24<01:59,  1.69it/s]Epoch: 12, train for the 18-th batch, train loss: 0.21020613610744476:  14%|█▌         | 17/119 [00:11<01:07,  1.51it/s]Epoch: 10, train for the 113-th batch, train loss: 0.4937194585800171:  77%|███████▋  | 112/146 [01:08<00:22,  1.51it/s]Epoch: 12, train for the 18-th batch, train loss: 0.21020613610744476:  15%|█▋         | 18/119 [00:11<01:07,  1.50it/s]Epoch: 10, train for the 113-th batch, train loss: 0.4937194585800171:  77%|███████▋  | 113/146 [01:08<00:21,  1.51it/s]evaluate for the 6-th batch, evaluate loss: 0.6560068130493164:  12%|██▌                 | 5/40 [00:01<00:09,  3.66it/s]evaluate for the 6-th batch, evaluate loss: 0.6560068130493164:  15%|███                 | 6/40 [00:01<00:09,  3.58it/s]Epoch: 4, train for the 270-th batch, train loss: 0.4050980806350708:  70%|███████▋   | 269/383 [02:39<01:05,  1.73it/s]Epoch: 4, train for the 270-th batch, train loss: 0.4050980806350708:  70%|███████▊   | 270/383 [02:39<01:05,  1.73it/s]Epoch: 21, train for the 104-th batch, train loss: 0.625853419303894:  68%|███████▌   | 103/151 [00:37<00:28,  1.69it/s]Epoch: 21, train for the 104-th batch, train loss: 0.625853419303894:  69%|███████▌   | 104/151 [00:37<00:27,  1.69it/s]evaluate for the 7-th batch, evaluate loss: 0.654498279094696:  15%|███▏                 | 6/40 [00:01<00:09,  3.58it/s]evaluate for the 7-th batch, evaluate loss: 0.654498279094696:  18%|███▋                 | 7/40 [00:01<00:09,  3.66it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6685835719108582:  16%|██           | 38/241 [00:25<01:59,  1.69it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6685835719108582:  16%|██           | 39/241 [00:25<01:59,  1.69it/s]evaluate for the 8-th batch, evaluate loss: 0.6938058137893677:  18%|███▌                | 7/40 [00:02<00:09,  3.66it/s]evaluate for the 8-th batch, evaluate loss: 0.6938058137893677:  20%|████                | 8/40 [00:02<00:08,  3.58it/s]Epoch: 12, train for the 19-th batch, train loss: 0.2004377245903015:  15%|█▊          | 18/119 [00:12<01:07,  1.50it/s]Epoch: 10, train for the 114-th batch, train loss: 0.5360666513442993:  77%|███████▋  | 113/146 [01:09<00:21,  1.51it/s]Epoch: 12, train for the 19-th batch, train loss: 0.2004377245903015:  16%|█▉          | 19/119 [00:12<01:06,  1.50it/s]Epoch: 10, train for the 114-th batch, train loss: 0.5360666513442993:  78%|███████▊  | 114/146 [01:09<00:21,  1.50it/s]Epoch: 4, train for the 271-th batch, train loss: 0.3798881471157074:  70%|███████▊   | 270/383 [02:40<01:05,  1.73it/s]Epoch: 4, train for the 271-th batch, train loss: 0.3798881471157074:  71%|███████▊   | 271/383 [02:40<01:04,  1.73it/s]Epoch: 21, train for the 105-th batch, train loss: 0.5656741857528687:  69%|██████▉   | 104/151 [00:37<00:27,  1.69it/s]Epoch: 21, train for the 105-th batch, train loss: 0.5656741857528687:  70%|██████▉   | 105/151 [00:37<00:27,  1.69it/s]evaluate for the 9-th batch, evaluate loss: 0.667206883430481:  20%|████▏                | 8/40 [00:02<00:08,  3.58it/s]evaluate for the 9-th batch, evaluate loss: 0.667206883430481:  22%|████▋                | 9/40 [00:02<00:08,  3.64it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6563732624053955:  16%|██           | 39/241 [00:26<01:59,  1.69it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6563732624053955:  17%|██▏          | 40/241 [00:26<01:59,  1.69it/s]evaluate for the 10-th batch, evaluate loss: 0.6629161834716797:  22%|████▎              | 9/40 [00:02<00:08,  3.64it/s]evaluate for the 10-th batch, evaluate loss: 0.6629161834716797:  25%|████▌             | 10/40 [00:02<00:08,  3.59it/s]Epoch: 4, train for the 272-th batch, train loss: 0.3060329258441925:  71%|███████▊   | 271/383 [02:40<01:04,  1.73it/s]Epoch: 4, train for the 272-th batch, train loss: 0.3060329258441925:  71%|███████▊   | 272/383 [02:40<01:04,  1.72it/s]Epoch: 12, train for the 20-th batch, train loss: 0.23757018148899078:  16%|█▊         | 19/119 [00:13<01:06,  1.50it/s]Epoch: 10, train for the 115-th batch, train loss: 0.5123957991600037:  78%|███████▊  | 114/146 [01:10<00:21,  1.50it/s]Epoch: 12, train for the 20-th batch, train loss: 0.23757018148899078:  17%|█▊         | 20/119 [00:13<01:05,  1.50it/s]Epoch: 10, train for the 115-th batch, train loss: 0.5123957991600037:  79%|███████▉  | 115/146 [01:10<00:20,  1.50it/s]Epoch: 21, train for the 106-th batch, train loss: 0.5491830706596375:  70%|██████▉   | 105/151 [00:38<00:27,  1.69it/s]Epoch: 21, train for the 106-th batch, train loss: 0.5491830706596375:  70%|███████   | 106/151 [00:38<00:26,  1.69it/s]evaluate for the 11-th batch, evaluate loss: 0.6704301834106445:  25%|████▌             | 10/40 [00:03<00:08,  3.59it/s]evaluate for the 11-th batch, evaluate loss: 0.6704301834106445:  28%|████▉             | 11/40 [00:03<00:08,  3.56it/s]Epoch: 1, train for the 41-th batch, train loss: 0.630254328250885:  17%|██▎           | 40/241 [00:26<01:59,  1.69it/s]Epoch: 1, train for the 41-th batch, train loss: 0.630254328250885:  17%|██▍           | 41/241 [00:26<01:58,  1.69it/s]evaluate for the 12-th batch, evaluate loss: 0.7282034754753113:  28%|████▉             | 11/40 [00:03<00:08,  3.56it/s]evaluate for the 12-th batch, evaluate loss: 0.7282034754753113:  30%|█████▍            | 12/40 [00:03<00:07,  3.52it/s]Epoch: 4, train for the 273-th batch, train loss: 0.40791070461273193:  71%|███████   | 272/383 [02:41<01:04,  1.72it/s]Epoch: 4, train for the 273-th batch, train loss: 0.40791070461273193:  71%|███████▏  | 273/383 [02:41<01:04,  1.71it/s]Epoch: 21, train for the 107-th batch, train loss: 0.5516246557235718:  70%|███████   | 106/151 [00:38<00:26,  1.69it/s]Epoch: 21, train for the 107-th batch, train loss: 0.5516246557235718:  71%|███████   | 107/151 [00:38<00:26,  1.69it/s]Epoch: 12, train for the 21-th batch, train loss: 0.28662702441215515:  17%|█▊         | 20/119 [00:13<01:05,  1.50it/s]Epoch: 10, train for the 116-th batch, train loss: 0.5297631621360779:  79%|███████▉  | 115/146 [01:10<00:20,  1.50it/s]Epoch: 12, train for the 21-th batch, train loss: 0.28662702441215515:  18%|█▉         | 21/119 [00:13<01:05,  1.50it/s]Epoch: 10, train for the 116-th batch, train loss: 0.5297631621360779:  79%|███████▉  | 116/146 [01:10<00:19,  1.50it/s]evaluate for the 13-th batch, evaluate loss: 0.6598041653633118:  30%|█████▍            | 12/40 [00:03<00:07,  3.52it/s]evaluate for the 13-th batch, evaluate loss: 0.6598041653633118:  32%|█████▊            | 13/40 [00:03<00:07,  3.47it/s]