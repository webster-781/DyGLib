True
  0%|          | 0/52049 [00:00<?, ?it/s]True
  0%|          | 0/61156 [00:00<?, ?it/s]True
  0%|          | 0/95577 [00:00<?, ?it/s]True
  0%|          | 0/87626 [00:00<?, ?it/s]True
  0%|          | 0/50631 [00:00<?, ?it/s]  9%|▊         | 5294/61156 [00:00<00:01, 52930.59it/s]True
  0%|          | 0/140777 [00:00<?, ?it/s] 11%|█         | 5698/52049 [00:00<00:01, 45763.46it/s]  9%|▉         | 8385/95577 [00:00<00:01, 83847.28it/s]  7%|▋         | 6323/87626 [00:00<00:01, 63222.22it/s]  9%|▉         | 4640/50631 [00:00<00:01, 44331.80it/s]  4%|▍         | 5922/140777 [00:00<00:02, 59218.08it/s] 17%|█▋        | 10588/61156 [00:00<00:01, 49682.37it/s] 20%|█▉        | 10275/52049 [00:00<00:00, 44304.41it/s] 20%|█▉        | 10057/50631 [00:00<00:00, 50009.10it/s] 14%|█▍        | 12646/87626 [00:00<00:01, 46129.20it/s] 10%|▉         | 13865/140777 [00:00<00:01, 71102.34it/s] 18%|█▊        | 16770/95577 [00:00<00:01, 52587.07it/s] 28%|██▊       | 14816/52049 [00:00<00:00, 44772.23it/s] 25%|██▌       | 15570/61156 [00:00<00:01, 44307.81it/s] 30%|██▉       | 15069/50631 [00:00<00:00, 48818.92it/s] 16%|█▌        | 22080/140777 [00:00<00:01, 76143.18it/s] 20%|██        | 17531/87626 [00:00<00:01, 42531.61it/s] 33%|███▎      | 20210/61156 [00:00<00:00, 45058.96it/s] 38%|███▊      | 19837/52049 [00:00<00:00, 44036.90it/s] 24%|██▎       | 22676/95577 [00:00<00:01, 46517.83it/s] 39%|███▉      | 19960/50631 [00:00<00:00, 45867.53it/s] 26%|██▋       | 23094/87626 [00:00<00:01, 46836.83it/s] 40%|████      | 24758/61156 [00:00<00:00, 44157.09it/s] 48%|████▊     | 24902/52049 [00:00<00:00, 42276.20it/s] 21%|██        | 29695/140777 [00:00<00:01, 59605.27it/s] 29%|██▉       | 27663/95577 [00:00<00:01, 42873.20it/s] 32%|███▏      | 27946/87626 [00:00<00:01, 47149.41it/s] 49%|████▉     | 25010/50631 [00:00<00:00, 43483.16it/s] 49%|████▉     | 30270/61156 [00:00<00:00, 44814.34it/s] 57%|█████▋    | 29543/52049 [00:00<00:00, 42074.45it/s] 34%|███▍      | 32271/95577 [00:00<00:01, 42731.45it/s] 60%|█████▉    | 30331/50631 [00:00<00:00, 46486.98it/s] 37%|███▋      | 32773/87626 [00:00<00:01, 46363.26it/s] 57%|█████▋    | 34764/61156 [00:00<00:00, 42495.78it/s] 66%|██████▋   | 34528/52049 [00:00<00:00, 42763.15it/s] 39%|███▉      | 37157/95577 [00:00<00:01, 44432.00it/s] 70%|███████   | 35587/50631 [00:00<00:00, 48344.80it/s] 26%|██▋       | 37282/140777 [00:00<00:02, 39856.96it/s] 65%|██████▍   | 39601/61156 [00:00<00:00, 43609.69it/s] 43%|████▎     | 37481/87626 [00:00<00:01, 39222.58it/s] 75%|███████▍  | 38944/52049 [00:00<00:00, 42188.39it/s] 45%|████▍     | 42831/95577 [00:00<00:01, 47926.05it/s] 80%|███████▉  | 40472/50631 [00:00<00:00, 42987.98it/s] 32%|███▏      | 45375/140777 [00:00<00:01, 48587.44it/s] 49%|████▉     | 43081/87626 [00:00<00:01, 43707.55it/s] 72%|███████▏  | 43980/61156 [00:00<00:00, 43055.06it/s] 84%|████████▎ | 43504/52049 [00:01<00:00, 43170.60it/s] 51%|█████     | 48939/95577 [00:00<00:00, 51722.04it/s] 91%|█████████ | 45855/50631 [00:00<00:00, 46002.37it/s] 79%|███████▉  | 48509/61156 [00:01<00:00, 43512.54it/s] 54%|█████▍    | 47672/87626 [00:01<00:00, 43296.92it/s] 92%|█████████▏| 47832/52049 [00:01<00:00, 41343.79it/s] 57%|█████▋    | 54237/95577 [00:01<00:00, 43961.25it/s] 60%|█████▉    | 52341/87626 [00:01<00:00, 44240.30it/s] 87%|████████▋ | 53002/61156 [00:01<00:00, 43008.51it/s]100%|█████████▉| 50628/50631 [00:01<00:00, 44701.90it/s]
 37%|███▋      | 52126/140777 [00:01<00:02, 40607.32it/s]100%|█████████▉| 52048/52049 [00:01<00:00, 42345.07it/s]
 62%|██████▏   | 59197/95577 [00:01<00:00, 45428.02it/s] 43%|████▎     | 60174/140777 [00:01<00:01, 48707.69it/s] 95%|█████████▌| 58380/61156 [00:01<00:00, 43260.94it/s]  0%|          | 0/50631 [00:00<?, ?it/s]100%|██████████| 50631/50631 [00:00<00:00, 1960087.92it/s]
 65%|██████▍   | 56878/87626 [00:01<00:00, 38073.00it/s]  0%|          | 0/52049 [00:00<?, ?it/s]100%|█████████▉| 61155/61156 [00:01<00:00, 44046.43it/s]
 67%|██████▋   | 63947/95577 [00:01<00:00, 44856.84it/s] 49%|████▊     | 68344/140777 [00:01<00:01, 56129.57it/s]100%|██████████| 52049/52049 [00:00<00:00, 2038178.78it/s]
 71%|███████   | 61920/87626 [00:01<00:00, 38519.93it/s]  0%|          | 0/61156 [00:00<?, ?it/s] 72%|███████▏  | 68573/95577 [00:01<00:00, 37704.33it/s]100%|██████████| 61156/61156 [00:00<00:00, 2004868.26it/s]
 77%|███████▋  | 67762/87626 [00:01<00:00, 43610.30it/s] 77%|███████▋  | 73486/95577 [00:01<00:00, 40515.25it/s] 53%|█████▎    | 75011/140777 [00:01<00:01, 40365.05it/s] 83%|████████▎ | 72318/87626 [00:01<00:00, 39584.20it/s] 58%|█████▊    | 82166/140777 [00:01<00:01, 46378.96it/s] 81%|████████▏ | 77785/95577 [00:01<00:00, 37640.75it/s] 87%|████████▋ | 76461/87626 [00:01<00:00, 39809.02it/s] 86%|████████▌ | 81741/95577 [00:01<00:00, 34922.41it/s] 92%|█████████▏| 80575/87626 [00:01<00:00, 38244.60it/s] 91%|█████████▏| 87295/95577 [00:02<00:00, 37601.83it/s] 96%|█████████▋| 84493/87626 [00:02<00:00, 37149.38it/s]100%|█████████▉| 87625/87626 [00:02<00:00, 40658.45it/s]
 63%|██████▎   | 89180/140777 [00:02<00:01, 31588.27it/s] 95%|█████████▌| 91163/95577 [00:02<00:00, 35725.50it/s] 69%|██████▉   | 97223/140777 [00:02<00:01, 39359.20it/s]100%|█████████▉| 95576/95577 [00:02<00:00, 42025.29it/s]
  0%|          | 0/87626 [00:00<?, ?it/s]100%|██████████| 87626/87626 [00:00<00:00, 1911421.73it/s]
  0%|          | 0/95577 [00:00<?, ?it/s]100%|██████████| 95577/95577 [00:00<00:00, 1988585.71it/s]
 73%|███████▎  | 102894/140777 [00:02<00:01, 32052.66it/s] 79%|███████▉  | 110985/140777 [00:02<00:00, 40167.77it/s] 85%|████████▍ | 119057/140777 [00:02<00:00, 47944.28it/s]wandb: Currently logged in as: singhayush4499 (fb-graph-proj). Use `wandb login --relogin` to force relogin
 89%|████████▉ | 125426/140777 [00:02<00:00, 39987.85it/s]wandb: Currently logged in as: singhayush4499 (fb-graph-proj). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: singhayush4499 (fb-graph-proj). Use `wandb login --relogin` to force relogin
 93%|█████████▎| 130706/140777 [00:03<00:00, 29084.52it/s] 99%|█████████▊| 138820/140777 [00:03<00:00, 37357.26it/s]100%|█████████▉| 140776/140777 [00:03<00:00, 40642.29it/s]
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/ayush/DyGLib/wandb/run-20240313_190157-51k241ur
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tgn-ia-escorts-dynamic-reparamcorr-time-mlp
wandb: ⭐️ View project at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib
wandb: 🚀 View run at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib/runs/51k241ur
INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-escorts-dynamic', batch_size=200, model_name='TGN', gpu=1, num_neighbors=10, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=1, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='reparamcorr-time-mlp', use_ROPe=False, t1_factor_of_t2=1, use_init_method=True, emb_proj=False, init_weights='time-mlp', clip=1.0, device='cuda:1', seed=0, save_model_name='TGN_seed0_tgn-ia-escorts-dynamic-reparamcorr-time-mlp')
  0%|          | 0/140777 [00:00<?, ?it/s]100%|██████████| 140777/140777 [00:00<00:00, 1918166.54it/s]
wandb: Currently logged in as: singhayush4499 (fb-graph-proj). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/ayush/DyGLib/wandb/run-20240313_190157-3aowzgh6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tgn-ia-reality-call-reparamcorr-time-mlp
wandb: ⭐️ View project at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib
wandb: 🚀 View run at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib/runs/3aowzgh6
INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-reality-call', batch_size=200, model_name='TGN', gpu=1, num_neighbors=10, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=1, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='reparamcorr-time-mlp', use_ROPe=False, t1_factor_of_t2=1, use_init_method=True, emb_proj=False, init_weights='time-mlp', clip=1.0, device='cuda:1', seed=0, save_model_name='TGN_seed0_tgn-ia-reality-call-reparamcorr-time-mlp')
wandb: Currently logged in as: singhayush4499 (fb-graph-proj). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/ayush/DyGLib/wandb/run-20240313_190158-kyq7knvw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tgn-ia-retweet-pol-reparamcorr-time-mlp
wandb: ⭐️ View project at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib
wandb: 🚀 View run at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib/runs/kyq7knvw
INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-retweet-pol', batch_size=200, model_name='TGN', gpu=0, num_neighbors=10, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=1, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='reparamcorr-time-mlp', use_ROPe=False, t1_factor_of_t2=1, use_init_method=True, emb_proj=False, init_weights='time-mlp', clip=1.0, device='cuda:0', seed=0, save_model_name='TGN_seed0_tgn-ia-retweet-pol-reparamcorr-time-mlp')
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/ayush/DyGLib/wandb/run-20240313_190159-p9uxbysy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tgn-ia-digg-reply-reparamcorr-time-mlp
wandb: ⭐️ View project at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib
wandb: 🚀 View run at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib/runs/p9uxbysy
INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-digg-reply', batch_size=200, model_name='TGN', gpu=2, num_neighbors=10, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=1, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='reparamcorr-time-mlp', use_ROPe=False, t1_factor_of_t2=1, use_init_method=True, emb_proj=False, init_weights='time-mlp', clip=1.0, device='cuda:2', seed=0, save_model_name='TGN_seed0_tgn-ia-digg-reply-reparamcorr-time-mlp')
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/ayush/DyGLib/wandb/run-20240313_190159-nn0j7uqn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tgn-ia-movielens-user2tags-10m-reparamcorr-time-mlp
wandb: ⭐️ View project at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib
wandb: 🚀 View run at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib/runs/nn0j7uqn
INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-movielens-user2tags-10m', batch_size=200, model_name='TGN', gpu=0, num_neighbors=10, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=1, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='reparamcorr-time-mlp', use_ROPe=False, t1_factor_of_t2=1, use_init_method=True, emb_proj=False, init_weights='time-mlp', clip=1.0, device='cuda:0', seed=0, save_model_name='TGN_seed0_tgn-ia-movielens-user2tags-10m-reparamcorr-time-mlp')
INFO:root:model -> Sequential(
  (0): MemoryModel(
    (mlp_for_mean): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
    )
    (mlp_for_sigma): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
    )
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (emb_proj): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
      (1): ReLU()
    )
    (time_transformation_for_init): TimeInitTransformMLP(
      (mlp_for_time): Sequential(
        (0): Linear(in_features=1, out_features=1, bias=True)
        (1): ReLU()
        (2): Linear(in_features=1, out_features=1, bias=True)
        (3): ReLU()
        (4): Linear(in_features=1, out_features=1, bias=True)
      )
    )
    (message_aggregator): MessageAggregator()
    (memory_bank): MemoryBank(num_nodes=10108, memory_dim=172)
    (memory_updater): GRUMemoryUpdater(
      (memory_bank): MemoryBank(num_nodes=10108, memory_dim=172)
      (memory_updater): GRUCell(616, 172)
    )
    (embedding_module): GraphAttentionEmbedding(
      (time_encoder): TimeEncoder(
        (w): Linear(in_features=1, out_features=100, bias=True)
      )
      (temporal_conv_layers): ModuleList(
        (0): MultiHeadAttention(
          (query_projection): Linear(in_features=272, out_features=272, bias=False)
          (key_projection): Linear(in_features=444, out_features=272, bias=False)
          (value_projection): Linear(in_features=444, out_features=272, bias=False)
          (layer_norm): LayerNorm((272,), eps=1e-05, elementwise_affine=True)
          (residual_fc): Linear(in_features=272, out_features=272, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (merge_layers): ModuleList(
        (0): MergeLayer(
          (fc1): Linear(in_features=444, out_features=172, bias=True)
          (fc2): Linear(in_features=172, out_features=172, bias=True)
          (act): ReLU()
        )
      )
    )
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: TGN, #parameters: 4212972 B, 4114.23046875 KB, 4.017803192138672 MB.
The dataset has 50631 interactions, involving 10106 different nodes
The training dataset has 29100 interactions, involving 7154 different nodes
The validation dataset has 7596 interactions, involving 4118 different nodes
The test dataset has 7577 interactions, involving 4144 different nodes
The new node validation dataset has 3845 interactions, involving 2930 different nodes
The new node test dataset has 4829 interactions, involving 3346 different nodes
1010 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]INFO:root:model -> Sequential(
  (0): MemoryModel(
    (mlp_for_mean): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
    )
    (mlp_for_sigma): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
    )
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (emb_proj): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
      (1): ReLU()
    )
    (time_transformation_for_init): TimeInitTransformMLP(
      (mlp_for_time): Sequential(
        (0): Linear(in_features=1, out_features=1, bias=True)
        (1): ReLU()
        (2): Linear(in_features=1, out_features=1, bias=True)
        (3): ReLU()
        (4): Linear(in_features=1, out_features=1, bias=True)
      )
    )
    (message_aggregator): MessageAggregator()
    (memory_bank): MemoryBank(num_nodes=27046, memory_dim=172)
    (memory_updater): GRUMemoryUpdater(
      (memory_bank): MemoryBank(num_nodes=27046, memory_dim=172)
      (memory_updater): GRUCell(616, 172)
    )
    (embedding_module): GraphAttentionEmbedding(
      (time_encoder): TimeEncoder(
        (w): Linear(in_features=1, out_features=100, bias=True)
      )
      (temporal_conv_layers): ModuleList(
        (0): MultiHeadAttention(
          (query_projection): Linear(in_features=272, out_features=272, bias=False)
          (key_projection): Linear(in_features=444, out_features=272, bias=False)
          (value_projection): Linear(in_features=444, out_features=272, bias=False)
          (layer_norm): LayerNorm((272,), eps=1e-05, elementwise_affine=True)
          (residual_fc): Linear(in_features=272, out_features=272, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (merge_layers): ModuleList(
        (0): MergeLayer(
          (fc1): Linear(in_features=444, out_features=172, bias=True)
          (fc2): Linear(in_features=172, out_features=172, bias=True)
          (act): ReLU()
        )
      )
    )
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: TGN, #parameters: 4212972 B, 4114.23046875 KB, 4.017803192138672 MB.
The dataset has 52049 interactions, involving 6809 different nodes
The training dataset has 23625 interactions, involving 3838 different nodes
The validation dataset has 7807 interactions, involving 1715 different nodes
The test dataset has 7808 interactions, involving 1937 different nodes
The new node validation dataset has 4011 interactions, involving 1185 different nodes
The new node test dataset has 4611 interactions, involving 1531 different nodes
680 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]INFO:root:model -> Sequential(
  (0): MemoryModel(
    (mlp_for_mean): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
    )
    (mlp_for_sigma): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
    )
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (emb_proj): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
      (1): ReLU()
    )
    (time_transformation_for_init): TimeInitTransformMLP(
      (mlp_for_time): Sequential(
        (0): Linear(in_features=1, out_features=1, bias=True)
        (1): ReLU()
        (2): Linear(in_features=1, out_features=1, bias=True)
        (3): ReLU()
        (4): Linear(in_features=1, out_features=1, bias=True)
      )
    )
    (message_aggregator): MessageAggregator()
    (memory_bank): MemoryBank(num_nodes=18471, memory_dim=172)
    (memory_updater): GRUMemoryUpdater(
      (memory_bank): MemoryBank(num_nodes=18471, memory_dim=172)
      (memory_updater): GRUCell(616, 172)
    )
    (embedding_module): GraphAttentionEmbedding(
      (time_encoder): TimeEncoder(
        (w): Linear(in_features=1, out_features=100, bias=True)
      )
      (temporal_conv_layers): ModuleList(
        (0): MultiHeadAttention(
          (query_projection): Linear(in_features=272, out_features=272, bias=False)
          (key_projection): Linear(in_features=444, out_features=272, bias=False)
          (value_projection): Linear(in_features=444, out_features=272, bias=False)
          (layer_norm): LayerNorm((272,), eps=1e-05, elementwise_affine=True)
          (residual_fc): Linear(in_features=272, out_features=272, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (merge_layers): ModuleList(
        (0): MergeLayer(
          (fc1): Linear(in_features=444, out_features=172, bias=True)
          (fc2): Linear(in_features=172, out_features=172, bias=True)
          (act): ReLU()
        )
      )
    )
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: TGN, #parameters: 4212972 B, 4114.23046875 KB, 4.017803192138672 MB.
The dataset has 61156 interactions, involving 18470 different nodes
The training dataset has 30070 interactions, involving 12678 different nodes
The validation dataset has 9173 interactions, involving 5479 different nodes
The test dataset has 9174 interactions, involving 5328 different nodes
The new node validation dataset has 4957 interactions, involving 4196 different nodes
The new node test dataset has 5073 interactions, involving 4153 different nodes
1847 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]wandb: Currently logged in as: singhayush4499 (fb-graph-proj). Use `wandb login --relogin` to force relogin
Epoch: 1, train for the 1-th batch, train loss: 0.6922900080680847:   0%|                       | 0/146 [00:01<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6922900080680847:   1%|               | 1/146 [00:01<02:46,  1.15s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6919350028038025:   1%|               | 1/146 [00:01<02:46,  1.15s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6919350028038025:   1%|▏              | 2/146 [00:01<01:16,  1.88it/s]Epoch: 1, train for the 3-th batch, train loss: 0.6922286748886108:   1%|▏              | 2/146 [00:01<01:16,  1.88it/s]INFO:root:model -> Sequential(
  (0): MemoryModel(
    (mlp_for_mean): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
    )
    (mlp_for_sigma): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
    )
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (emb_proj): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
      (1): ReLU()
    )
    (time_transformation_for_init): TimeInitTransformMLP(
      (mlp_for_time): Sequential(
        (0): Linear(in_features=1, out_features=1, bias=True)
        (1): ReLU()
        (2): Linear(in_features=1, out_features=1, bias=True)
        (3): ReLU()
        (4): Linear(in_features=1, out_features=1, bias=True)
      )
    )
    (message_aggregator): MessageAggregator()
    (memory_bank): MemoryBank(num_nodes=30400, memory_dim=172)
    (memory_updater): GRUMemoryUpdater(
      (memory_bank): MemoryBank(num_nodes=30400, memory_dim=172)
      (memory_updater): GRUCell(616, 172)
    )
    (embedding_module): GraphAttentionEmbedding(
      (time_encoder): TimeEncoder(
        (w): Linear(in_features=1, out_features=100, bias=True)
      )
      (temporal_conv_layers): ModuleList(
        (0): MultiHeadAttention(
          (query_projection): Linear(in_features=272, out_features=272, bias=False)
          (key_projection): Linear(in_features=444, out_features=272, bias=False)
          (value_projection): Linear(in_features=444, out_features=272, bias=False)
          (layer_norm): LayerNorm((272,), eps=1e-05, elementwise_affine=True)
          (residual_fc): Linear(in_features=272, out_features=272, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (merge_layers): ModuleList(
        (0): MergeLayer(
          (fc1): Linear(in_features=444, out_features=172, bias=True)
          (fc2): Linear(in_features=172, out_features=172, bias=True)
          (act): ReLU()
        )
      )
    )
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: TGN, #parameters: 4212972 B, 4114.23046875 KB, 4.017803192138672 MB.
The dataset has 87626 interactions, involving 30398 different nodes
The training dataset has 47297 interactions, involving 21540 different nodes
The validation dataset has 13144 interactions, involving 9241 different nodes
The test dataset has 13144 interactions, involving 9511 different nodes
The new node validation dataset has 7995 interactions, involving 7321 different nodes
The new node test dataset has 8239 interactions, involving 7732 different nodes
3039 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/237 [00:00<?, ?it/s]INFO:root:model -> Sequential(
  (0): MemoryModel(
    (mlp_for_mean): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
    )
    (mlp_for_sigma): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
    )
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (emb_proj): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
      (1): ReLU()
    )
    (time_transformation_for_init): TimeInitTransformMLP(
      (mlp_for_time): Sequential(
        (0): Linear(in_features=1, out_features=1, bias=True)
        (1): ReLU()
        (2): Linear(in_features=1, out_features=1, bias=True)
        (3): ReLU()
        (4): Linear(in_features=1, out_features=1, bias=True)
      )
    )
    (message_aggregator): MessageAggregator()
    (memory_bank): MemoryBank(num_nodes=16530, memory_dim=172)
    (memory_updater): GRUMemoryUpdater(
      (memory_bank): MemoryBank(num_nodes=16530, memory_dim=172)
      (memory_updater): GRUCell(616, 172)
    )
    (embedding_module): GraphAttentionEmbedding(
      (time_encoder): TimeEncoder(
        (w): Linear(in_features=1, out_features=100, bias=True)
      )
      (temporal_conv_layers): ModuleList(
        (0): MultiHeadAttention(
          (query_projection): Linear(in_features=272, out_features=272, bias=False)
          (key_projection): Linear(in_features=444, out_features=272, bias=False)
          (value_projection): Linear(in_features=444, out_features=272, bias=False)
          (layer_norm): LayerNorm((272,), eps=1e-05, elementwise_affine=True)
          (residual_fc): Linear(in_features=272, out_features=272, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (merge_layers): ModuleList(
        (0): MergeLayer(
          (fc1): Linear(in_features=444, out_features=172, bias=True)
          (fc2): Linear(in_features=172, out_features=172, bias=True)
          (act): ReLU()
        )
      )
    )
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: TGN, #parameters: 4212972 B, 4114.23046875 KB, 4.017803192138672 MB.
The dataset has 95577 interactions, involving 16527 different nodes
The training dataset has 48189 interactions, involving 10566 different nodes
The validation dataset has 14336 interactions, involving 5209 different nodes
The test dataset has 14337 interactions, involving 5377 different nodes
The new node validation dataset has 6745 interactions, involving 3518 different nodes
The new node test dataset has 7521 interactions, involving 3885 different nodes
1652 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/241 [00:00<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6972087025642395:   0%|                       | 0/119 [00:01<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6972087025642395:   1%|▏              | 1/119 [00:01<02:18,  1.17s/it]Epoch: 1, train for the 1-th batch, train loss: 0.6933267116546631:   0%|                       | 0/151 [00:01<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6933267116546631:   1%|               | 1/151 [00:01<03:00,  1.20s/it]Epoch: 1, train for the 2-th batch, train loss: 0.689045250415802:   1%|▏               | 1/119 [00:01<02:18,  1.17s/it]Epoch: 1, train for the 2-th batch, train loss: 0.689045250415802:   2%|▎               | 2/119 [00:01<01:07,  1.74it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6921128034591675:   1%|▏              | 2/146 [00:01<01:16,  1.88it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6921128034591675:   3%|▍              | 4/146 [00:01<00:44,  3.21it/s]Epoch: 1, train for the 2-th batch, train loss: 0.6934100985527039:   1%|               | 1/151 [00:01<03:00,  1.20s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6934100985527039:   1%|▏              | 2/151 [00:01<01:22,  1.80it/s]Epoch: 1, train for the 3-th batch, train loss: 0.6819557547569275:   2%|▎              | 2/119 [00:01<01:07,  1.74it/s]Epoch: 1, train for the 3-th batch, train loss: 0.6819557547569275:   3%|▍              | 3/119 [00:01<00:44,  2.60it/s]Epoch: 1, train for the 3-th batch, train loss: 0.6932093501091003:   1%|▏              | 2/151 [00:01<01:22,  1.80it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6911434531211853:   3%|▍              | 4/146 [00:01<00:44,  3.21it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6911434531211853:   3%|▌              | 5/146 [00:01<00:36,  3.81it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6923797726631165:   1%|▏              | 2/151 [00:01<01:22,  1.80it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6923797726631165:   3%|▍              | 4/151 [00:01<00:40,  3.66it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6735696196556091:   3%|▍              | 3/119 [00:01<00:44,  2.60it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6916861534118652:   3%|▌              | 5/146 [00:01<00:36,  3.81it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6735696196556091:   3%|▌              | 4/119 [00:01<00:34,  3.30it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6916861534118652:   4%|▌              | 6/146 [00:01<00:32,  4.27it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6925634741783142:   3%|▍              | 4/151 [00:01<00:40,  3.66it/s]Epoch: 1, train for the 6-th batch, train loss: 0.691596508026123:   3%|▍               | 4/151 [00:01<00:40,  3.66it/s]wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/ayush/DyGLib/wandb/run-20240313_190200-44nu88d6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tgn-ia-slashdot-reply-dir-reparamcorr-time-mlp
wandb: ⭐️ View project at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib
wandb: 🚀 View run at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib/runs/44nu88d6
Epoch: 1, train for the 6-th batch, train loss: 0.691596508026123:   4%|▋               | 6/151 [00:01<00:27,  5.19it/s]INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-slashdot-reply-dir', batch_size=200, model_name='TGN', gpu=2, num_neighbors=10, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=1, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='reparamcorr-time-mlp', use_ROPe=False, t1_factor_of_t2=1, use_init_method=True, emb_proj=False, init_weights='time-mlp', clip=1.0, device='cuda:2', seed=0, save_model_name='TGN_seed0_tgn-ia-slashdot-reply-dir-reparamcorr-time-mlp')
Epoch: 1, train for the 7-th batch, train loss: 0.6900573372840881:   4%|▌              | 6/146 [00:02<00:32,  4.27it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6900573372840881:   5%|▋              | 7/146 [00:02<00:29,  4.78it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6692484021186829:   3%|▌              | 4/119 [00:01<00:34,  3.30it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6692484021186829:   4%|▋              | 5/119 [00:01<00:28,  3.96it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6915373802185059:   4%|▌              | 6/151 [00:01<00:27,  5.19it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6915373802185059:   5%|▋              | 7/151 [00:01<00:24,  5.89it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6890224814414978:   5%|▋              | 7/146 [00:02<00:29,  4.78it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6592231392860413:   4%|▋              | 5/119 [00:01<00:28,  3.96it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6890224814414978:   5%|▊              | 8/146 [00:02<00:27,  5.08it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6592231392860413:   5%|▊              | 6/119 [00:01<00:25,  4.52it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6928209066390991:   5%|▋              | 7/151 [00:01<00:24,  5.89it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6928209066390991:   5%|▊              | 8/151 [00:01<00:22,  6.46it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6874010562896729:   5%|▊              | 8/146 [00:02<00:27,  5.08it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6874010562896729:   6%|▉              | 9/146 [00:02<00:23,  5.94it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6915391683578491:   5%|▊              | 8/151 [00:02<00:22,  6.46it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6915391683578491:   6%|▉              | 9/151 [00:02<00:20,  7.09it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6934471726417542:   0%|                       | 0/237 [00:01<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6934471726417542:   0%|               | 1/237 [00:01<04:06,  1.04s/it]Epoch: 1, train for the 10-th batch, train loss: 0.6861294507980347:   6%|▊             | 9/146 [00:02<00:23,  5.94it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6916670799255371:   6%|▊             | 9/151 [00:02<00:20,  7.09it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6916670799255371:   7%|▊            | 10/151 [00:02<00:18,  7.46it/s]Epoch: 1, train for the 7-th batch, train loss: 0.648536205291748:   5%|▊               | 6/119 [00:02<00:25,  4.52it/s]Epoch: 1, train for the 7-th batch, train loss: 0.648536205291748:   6%|▉               | 7/119 [00:02<00:26,  4.25it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6852777004241943:   6%|▊             | 9/146 [00:02<00:23,  5.94it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6852777004241943:   8%|▉            | 11/146 [00:02<00:18,  7.11it/s]Epoch: 1, train for the 1-th batch, train loss: 0.693142831325531:   0%|                        | 0/241 [00:01<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.693142831325531:   0%|                | 1/241 [00:01<04:38,  1.16s/it]Epoch: 1, train for the 11-th batch, train loss: 0.6910911202430725:   7%|▊            | 10/151 [00:02<00:18,  7.46it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6910911202430725:   7%|▉            | 11/151 [00:02<00:18,  7.46it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6359993815422058:   6%|▉              | 7/119 [00:02<00:26,  4.25it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6359993815422058:   7%|█              | 8/119 [00:02<00:22,  4.89it/s]Epoch: 1, train for the 2-th batch, train loss: 0.6914501786231995:   0%|               | 1/237 [00:01<04:06,  1.04s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6914501786231995:   1%|▏              | 2/237 [00:01<02:13,  1.77it/s]Epoch: 1, train for the 2-th batch, train loss: 0.6978585124015808:   0%|               | 1/241 [00:01<04:38,  1.16s/it]Epoch: 1, train for the 12-th batch, train loss: 0.6844889521598816:   8%|▉            | 11/146 [00:02<00:18,  7.11it/s]Epoch: 1, train for the 2-th batch, train loss: 0.6978585124015808:   1%|               | 2/241 [00:01<02:12,  1.80it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6844889521598816:   8%|█            | 12/146 [00:02<00:18,  7.13it/s]Epoch: 1, train for the 3-th batch, train loss: 0.6899179816246033:   1%|▏              | 2/237 [00:01<02:13,  1.77it/s]Epoch: 1, train for the 3-th batch, train loss: 0.6913541555404663:   1%|               | 2/241 [00:01<02:12,  1.80it/s]Epoch: 1, train for the 3-th batch, train loss: 0.6899179816246033:   1%|▏              | 3/237 [00:01<01:25,  2.73it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6363545656204224:   7%|█              | 8/119 [00:02<00:22,  4.89it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6363545656204224:   8%|█▏             | 9/119 [00:02<00:20,  5.28it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6836904287338257:   8%|█            | 12/146 [00:02<00:18,  7.13it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6836904287338257:   9%|█▏           | 13/146 [00:02<00:18,  7.17it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6901347041130066:   1%|▏              | 3/237 [00:01<01:25,  2.73it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6901347041130066:   2%|▎              | 4/237 [00:01<01:02,  3.75it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6901310682296753:   7%|▉            | 11/151 [00:02<00:18,  7.46it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6901310682296753:   8%|█            | 12/151 [00:02<00:24,  5.71it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6861663460731506:   1%|               | 2/241 [00:01<02:12,  1.80it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6861663460731506:   2%|▏              | 4/241 [00:01<01:06,  3.56it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6259267926216125:   8%|█             | 9/119 [00:02<00:20,  5.28it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6259267926216125:   8%|█            | 10/119 [00:02<00:20,  5.42it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6818404793739319:   9%|█▏           | 13/146 [00:02<00:18,  7.17it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6818404793739319:  10%|█▏           | 14/146 [00:02<00:18,  7.04it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6879322528839111:   2%|▎              | 4/237 [00:01<01:02,  3.75it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6879322528839111:   2%|▎              | 5/237 [00:01<00:48,  4.74it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6913345456123352:   8%|█            | 12/151 [00:02<00:24,  5.71it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6913345456123352:   9%|█            | 13/151 [00:02<00:23,  5.77it/s]Epoch: 1, train for the 5-th batch, train loss: 0.687782883644104:   2%|▎               | 4/241 [00:01<01:06,  3.56it/s]Epoch: 1, train for the 5-th batch, train loss: 0.687782883644104:   2%|▎               | 5/241 [00:01<00:56,  4.15it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6895214915275574:   2%|▎              | 5/237 [00:01<00:48,  4.74it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6895214915275574:   3%|▍              | 6/237 [00:01<00:41,  5.62it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6141393780708313:   8%|█            | 10/119 [00:02<00:20,  5.42it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6141393780708313:   9%|█▏           | 11/119 [00:02<00:19,  5.66it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6818013787269592:  10%|█▏           | 14/146 [00:03<00:18,  7.04it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6818013787269592:  10%|█▎           | 15/146 [00:03<00:19,  6.84it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6915930509567261:   9%|█            | 13/151 [00:02<00:23,  5.77it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6876020431518555:   3%|▍              | 6/237 [00:01<00:41,  5.62it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6915930509567261:   9%|█▏           | 14/151 [00:02<00:22,  5.98it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6876020431518555:   3%|▍              | 7/237 [00:01<00:36,  6.35it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6913431882858276:   2%|▎              | 5/241 [00:01<00:56,  4.15it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6913431882858276:   2%|▎              | 6/241 [00:01<00:51,  4.57it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6033916473388672:   9%|█▏           | 11/119 [00:03<00:19,  5.66it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6033916473388672:  10%|█▎           | 12/119 [00:03<00:17,  6.09it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6794115900993347:  10%|█▎           | 15/146 [00:03<00:19,  6.84it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6794115900993347:  11%|█▍           | 16/146 [00:03<00:19,  6.63it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6860418319702148:   3%|▍              | 7/237 [00:01<00:36,  6.35it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6860418319702148:   3%|▌              | 8/237 [00:01<00:34,  6.73it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6859821677207947:   2%|▎              | 6/241 [00:02<00:51,  4.57it/s]Epoch: 1, train for the 13-th batch, train loss: 0.5907928347587585:  10%|█▎           | 12/119 [00:03<00:17,  6.09it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6859821677207947:   3%|▍              | 7/241 [00:02<00:47,  4.97it/s]Epoch: 1, train for the 13-th batch, train loss: 0.5907928347587585:  11%|█▍           | 13/119 [00:03<00:16,  6.28it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6917192935943604:   9%|█▏           | 14/151 [00:03<00:22,  5.98it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6917192935943604:  10%|█▎           | 15/151 [00:03<00:24,  5.62it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6773934960365295:  11%|█▍           | 16/146 [00:03<00:19,  6.63it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6773934960365295:  12%|█▌           | 17/146 [00:03<00:19,  6.72it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6862450838088989:   3%|▌              | 8/237 [00:02<00:34,  6.73it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6862450838088989:   4%|▌              | 9/237 [00:02<00:32,  7.00it/s]INFO:root:model -> Sequential(
  (0): MemoryModel(
    (mlp_for_mean): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
    )
    (mlp_for_sigma): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
    )
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (emb_proj): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
      (1): ReLU()
    )
    (time_transformation_for_init): TimeInitTransformMLP(
      (mlp_for_time): Sequential(
        (0): Linear(in_features=1, out_features=1, bias=True)
        (1): ReLU()
        (2): Linear(in_features=1, out_features=1, bias=True)
        (3): ReLU()
        (4): Linear(in_features=1, out_features=1, bias=True)
      )
    )
    (message_aggregator): MessageAggregator()
    (memory_bank): MemoryBank(num_nodes=51085, memory_dim=172)
    (memory_updater): GRUMemoryUpdater(
      (memory_bank): MemoryBank(num_nodes=51085, memory_dim=172)
      (memory_updater): GRUCell(616, 172)
    )
    (embedding_module): GraphAttentionEmbedding(
      (time_encoder): TimeEncoder(
        (w): Linear(in_features=1, out_features=100, bias=True)
      )
      (temporal_conv_layers): ModuleList(
        (0): MultiHeadAttention(
          (query_projection): Linear(in_features=272, out_features=272, bias=False)
          (key_projection): Linear(in_features=444, out_features=272, bias=False)
          (value_projection): Linear(in_features=444, out_features=272, bias=False)
          (layer_norm): LayerNorm((272,), eps=1e-05, elementwise_affine=True)
          (residual_fc): Linear(in_features=272, out_features=272, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (merge_layers): ModuleList(
        (0): MergeLayer(
          (fc1): Linear(in_features=444, out_features=172, bias=True)
          (fc2): Linear(in_features=172, out_features=172, bias=True)
          (act): ReLU()
        )
      )
    )
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: TGN, #parameters: 4212972 B, 4114.23046875 KB, 4.017803192138672 MB.
Epoch: 1, train for the 14-th batch, train loss: 0.5885704159736633:  11%|█▍           | 13/119 [00:03<00:16,  6.28it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6908077001571655:   3%|▍              | 7/241 [00:02<00:47,  4.97it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6908077001571655:   3%|▍              | 8/241 [00:02<00:42,  5.53it/s]The dataset has 140777 interactions, involving 51083 different nodes
The training dataset has 76599 interactions, involving 34496 different nodes
The validation dataset has 21116 interactions, involving 10542 different nodes
The test dataset has 21117 interactions, involving 10424 different nodes
The new node validation dataset has 15534 interactions, involving 9790 different nodes
The new node test dataset has 16568 interactions, involving 9911 different nodes
5108 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/383 [00:00<?, ?it/s]Epoch: 1, train for the 14-th batch, train loss: 0.5885704159736633:  12%|█▌           | 14/119 [00:03<00:16,  6.56it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6759532690048218:  12%|█▌           | 17/146 [00:03<00:19,  6.72it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6759532690048218:  12%|█▌           | 18/146 [00:03<00:18,  6.89it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6840085387229919:   4%|▌             | 9/237 [00:02<00:32,  7.00it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6901016235351562:  10%|█▎           | 15/151 [00:03<00:24,  5.62it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6840085387229919:   4%|▌            | 10/237 [00:02<00:30,  7.55it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6901016235351562:  11%|█▍           | 16/151 [00:03<00:23,  5.65it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6841994524002075:   3%|▍              | 8/241 [00:02<00:42,  5.53it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6841994524002075:   4%|▌              | 9/241 [00:02<00:38,  6.09it/s]Epoch: 1, train for the 11-th batch, train loss: 0.67884361743927:   4%|▋              | 10/237 [00:02<00:30,  7.55it/s]Epoch: 1, train for the 11-th batch, train loss: 0.67884361743927:   5%|▋              | 11/237 [00:02<00:28,  8.02it/s]Epoch: 1, train for the 15-th batch, train loss: 0.5759131908416748:  12%|█▌           | 14/119 [00:03<00:16,  6.56it/s]Epoch: 1, train for the 15-th batch, train loss: 0.5759131908416748:  13%|█▋           | 15/119 [00:03<00:16,  6.29it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6773922443389893:  12%|█▌           | 18/146 [00:03<00:18,  6.89it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6773922443389893:  13%|█▋           | 19/146 [00:03<00:18,  6.76it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6903638243675232:  11%|█▍           | 16/151 [00:03<00:23,  5.65it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6903638243675232:  11%|█▍           | 17/151 [00:03<00:22,  5.83it/s]Epoch: 1, train for the 10-th batch, train loss: 0.683622419834137:   4%|▌              | 9/241 [00:02<00:38,  6.09it/s]Epoch: 1, train for the 10-th batch, train loss: 0.683622419834137:   4%|▌             | 10/241 [00:02<00:34,  6.66it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6814175844192505:   5%|▌            | 11/237 [00:02<00:28,  8.02it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6814175844192505:   5%|▋            | 12/237 [00:02<00:28,  7.98it/s]Epoch: 1, train for the 16-th batch, train loss: 0.5726773142814636:  13%|█▋           | 15/119 [00:03<00:16,  6.29it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6735014915466309:  13%|█▋           | 19/146 [00:03<00:18,  6.76it/s]Epoch: 1, train for the 16-th batch, train loss: 0.5726773142814636:  13%|█▋           | 16/119 [00:03<00:16,  6.16it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6735014915466309:  14%|█▊           | 20/146 [00:03<00:19,  6.56it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6740692257881165:   4%|▌            | 10/241 [00:02<00:34,  6.66it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6740692257881165:   5%|▌            | 11/241 [00:02<00:35,  6.49it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6888660788536072:  11%|█▍           | 17/151 [00:03<00:22,  5.83it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6888660788536072:  12%|█▌           | 18/151 [00:03<00:23,  5.58it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6852307915687561:   5%|▋            | 12/237 [00:02<00:28,  7.98it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6852307915687561:   5%|▋            | 13/237 [00:02<00:29,  7.72it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6730670928955078:  14%|█▊           | 20/146 [00:04<00:19,  6.56it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6730670928955078:  14%|█▊           | 21/146 [00:04<00:19,  6.55it/s]Epoch: 1, train for the 17-th batch, train loss: 0.5533961653709412:  13%|█▋           | 16/119 [00:03<00:16,  6.16it/s]Epoch: 1, train for the 17-th batch, train loss: 0.5533961653709412:  14%|█▊           | 17/119 [00:03<00:16,  6.01it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6814322471618652:   5%|▌            | 11/241 [00:02<00:35,  6.49it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6814322471618652:   5%|▋            | 12/241 [00:02<00:33,  6.83it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6832968592643738:   5%|▋            | 13/237 [00:02<00:29,  7.72it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6832968592643738:   6%|▊            | 14/237 [00:02<00:28,  7.95it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6892563700675964:  12%|█▌           | 18/151 [00:03<00:23,  5.58it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6892563700675964:  13%|█▋           | 19/151 [00:03<00:23,  5.60it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6702778339385986:  14%|█▊           | 21/146 [00:04<00:19,  6.55it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6702778339385986:  15%|█▉           | 22/146 [00:04<00:17,  6.94it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6818147301673889:   6%|▊            | 14/237 [00:02<00:28,  7.95it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6818147301673889:   6%|▊            | 15/237 [00:02<00:27,  8.17it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6759742498397827:   5%|▋            | 12/241 [00:02<00:33,  6.83it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6759742498397827:   5%|▋            | 13/241 [00:02<00:32,  7.04it/s]Epoch: 1, train for the 18-th batch, train loss: 0.5535358190536499:  14%|█▊           | 17/119 [00:03<00:16,  6.01it/s]Epoch: 1, train for the 18-th batch, train loss: 0.5535358190536499:  15%|█▉           | 18/119 [00:03<00:16,  6.07it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6871777772903442:  13%|█▋           | 19/151 [00:03<00:23,  5.60it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6871777772903442:  13%|█▋           | 20/151 [00:03<00:22,  5.94it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6669604182243347:  15%|█▉           | 22/146 [00:04<00:17,  6.94it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6669604182243347:  16%|██           | 23/146 [00:04<00:17,  7.05it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6801719665527344:   6%|▊            | 15/237 [00:02<00:27,  8.17it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6801719665527344:   7%|▉            | 16/237 [00:02<00:27,  7.92it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6792921423912048:   5%|▋            | 13/241 [00:02<00:32,  7.04it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6792921423912048:   6%|▊            | 14/241 [00:02<00:31,  7.16it/s]Epoch: 1, train for the 19-th batch, train loss: 0.542674720287323:  15%|██            | 18/119 [00:04<00:16,  6.07it/s]Epoch: 1, train for the 19-th batch, train loss: 0.542674720287323:  16%|██▏           | 19/119 [00:04<00:15,  6.31it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6866348385810852:  13%|█▋           | 20/151 [00:04<00:22,  5.94it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6866348385810852:  14%|█▊           | 21/151 [00:04<00:20,  6.28it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6768336892127991:   7%|▉            | 16/237 [00:03<00:27,  7.92it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6768336892127991:   7%|▉            | 17/237 [00:03<00:28,  7.76it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6765924096107483:   6%|▊            | 14/241 [00:03<00:31,  7.16it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6618452072143555:  16%|██           | 23/146 [00:04<00:17,  7.05it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6765924096107483:   6%|▊            | 15/241 [00:03<00:31,  7.13it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6618452072143555:  16%|██▏          | 24/146 [00:04<00:18,  6.57it/s]Epoch: 1, train for the 20-th batch, train loss: 0.5474479794502258:  16%|██           | 19/119 [00:04<00:15,  6.31it/s]Epoch: 1, train for the 20-th batch, train loss: 0.5474479794502258:  17%|██▏          | 20/119 [00:04<00:15,  6.30it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6822733879089355:  14%|█▊           | 21/151 [00:04<00:20,  6.28it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6822733879089355:  15%|█▉           | 22/151 [00:04<00:19,  6.51it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6691386103630066:   7%|▉            | 17/237 [00:03<00:28,  7.76it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6711718440055847:   6%|▊            | 15/241 [00:03<00:31,  7.13it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6711718440055847:   7%|▊            | 16/241 [00:03<00:30,  7.41it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6691386103630066:   8%|▉            | 18/237 [00:03<00:28,  7.59it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6636407375335693:  16%|██▏          | 24/146 [00:04<00:18,  6.57it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6636407375335693:  17%|██▏          | 25/146 [00:04<00:18,  6.64it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6837823987007141:  15%|█▉           | 22/151 [00:04<00:19,  6.51it/s]Epoch: 1, train for the 21-th batch, train loss: 0.5405134558677673:  17%|██▏          | 20/119 [00:04<00:15,  6.30it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6837823987007141:  15%|█▉           | 23/151 [00:04<00:18,  6.85it/s]Epoch: 1, train for the 21-th batch, train loss: 0.5405134558677673:  18%|██▎          | 21/119 [00:04<00:15,  6.32it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6683981418609619:   8%|▉            | 18/237 [00:03<00:28,  7.59it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6683981418609619:   8%|█            | 19/237 [00:03<00:28,  7.56it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6587875485420227:  17%|██▏          | 25/146 [00:04<00:18,  6.64it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6587875485420227:  18%|██▎          | 26/146 [00:04<00:17,  6.89it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6820003986358643:  15%|█▉           | 23/151 [00:04<00:18,  6.85it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6820003986358643:  16%|██           | 24/151 [00:04<00:17,  7.06it/s]Epoch: 1, train for the 22-th batch, train loss: 0.5294896960258484:  18%|██▎          | 21/119 [00:04<00:15,  6.32it/s]Epoch: 1, train for the 22-th batch, train loss: 0.5294896960258484:  18%|██▍          | 22/119 [00:04<00:14,  6.47it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6674947738647461:   7%|▊            | 16/241 [00:03<00:30,  7.41it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6674947738647461:   7%|▉            | 17/241 [00:03<00:37,  5.92it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6951098442077637:   0%|                       | 0/383 [00:01<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6951098442077637:   0%|               | 1/383 [00:01<08:24,  1.32s/it]Epoch: 1, train for the 27-th batch, train loss: 0.6611393094062805:  18%|██▎          | 26/146 [00:04<00:17,  6.89it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6611393094062805:  18%|██▍          | 27/146 [00:04<00:16,  7.28it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6703802347183228:   8%|█            | 19/237 [00:03<00:28,  7.56it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6703802347183228:   8%|█            | 20/237 [00:03<00:32,  6.71it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6872387528419495:  16%|██           | 24/151 [00:04<00:17,  7.06it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6872387528419495:  17%|██▏          | 25/151 [00:04<00:18,  6.91it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6624734401702881:   7%|▉            | 17/241 [00:03<00:37,  5.92it/s]Epoch: 1, train for the 23-th batch, train loss: 0.5265941619873047:  18%|██▍          | 22/119 [00:04<00:14,  6.47it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6624734401702881:   7%|▉            | 18/241 [00:03<00:34,  6.41it/s]Epoch: 1, train for the 23-th batch, train loss: 0.5265941619873047:  19%|██▌          | 23/119 [00:04<00:14,  6.55it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6473925113677979:  18%|██▍          | 27/146 [00:05<00:16,  7.28it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6473925113677979:  19%|██▍          | 28/146 [00:05<00:16,  7.10it/s]Epoch: 1, train for the 2-th batch, train loss: 0.6882769465446472:   0%|               | 1/383 [00:01<08:24,  1.32s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6882769465446472:   1%|               | 2/383 [00:01<04:14,  1.50it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6727234721183777:   8%|█            | 20/237 [00:03<00:32,  6.71it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6837283372879028:  17%|██▏          | 25/151 [00:04<00:18,  6.91it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6727234721183777:   9%|█▏           | 21/237 [00:03<00:35,  6.13it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6644630432128906:   7%|▉            | 18/241 [00:03<00:34,  6.41it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6837283372879028:  17%|██▏          | 26/151 [00:04<00:18,  6.64it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6644630432128906:   8%|█            | 19/241 [00:03<00:35,  6.33it/s]Epoch: 1, train for the 24-th batch, train loss: 0.5304581522941589:  19%|██▌          | 23/119 [00:04<00:14,  6.55it/s]Epoch: 1, train for the 24-th batch, train loss: 0.5304581522941589:  20%|██▌          | 24/119 [00:04<00:15,  6.29it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6537991166114807:  19%|██▍          | 28/146 [00:05<00:16,  7.10it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6537991166114807:  20%|██▌          | 29/146 [00:05<00:17,  6.81it/s]Epoch: 1, train for the 3-th batch, train loss: 0.6904691457748413:   1%|               | 2/383 [00:01<04:14,  1.50it/s]Epoch: 1, train for the 3-th batch, train loss: 0.6904691457748413:   1%|               | 3/383 [00:01<02:44,  2.31it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6628088355064392:   9%|█▏           | 21/237 [00:03<00:35,  6.13it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6850714087486267:  17%|██▏          | 26/151 [00:04<00:18,  6.64it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6505218148231506:   8%|█            | 19/241 [00:03<00:35,  6.33it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6628088355064392:   9%|█▏           | 22/237 [00:03<00:36,  5.93it/s]Epoch: 1, train for the 25-th batch, train loss: 0.5021932125091553:  20%|██▌          | 24/119 [00:05<00:15,  6.29it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6850714087486267:  18%|██▎          | 27/151 [00:04<00:19,  6.30it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6505218148231506:   8%|█            | 20/241 [00:03<00:35,  6.18it/s]Epoch: 1, train for the 25-th batch, train loss: 0.5021932125091553:  21%|██▋          | 25/119 [00:05<00:14,  6.29it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6481446027755737:  20%|██▌          | 29/146 [00:05<00:17,  6.81it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6481446027755737:  21%|██▋          | 30/146 [00:05<00:17,  6.59it/s]Epoch: 1, train for the 4-th batch, train loss: 0.687883734703064:   1%|▏               | 3/383 [00:01<02:44,  2.31it/s]Epoch: 1, train for the 4-th batch, train loss: 0.687883734703064:   1%|▏               | 4/383 [00:01<02:03,  3.06it/s]Epoch: 1, train for the 26-th batch, train loss: 0.5112825036048889:  21%|██▋          | 25/119 [00:05<00:14,  6.29it/s]Epoch: 1, train for the 26-th batch, train loss: 0.5112825036048889:  22%|██▊          | 26/119 [00:05<00:14,  6.50it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6766802668571472:   8%|█            | 20/241 [00:04<00:35,  6.18it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6766802668571472:   9%|█▏           | 21/241 [00:04<00:35,  6.16it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6573860049247742:   9%|█▏           | 22/237 [00:04<00:36,  5.93it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6573860049247742:  10%|█▎           | 23/237 [00:04<00:38,  5.62it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6431770324707031:  21%|██▋          | 30/146 [00:05<00:17,  6.59it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6431770324707031:  21%|██▊          | 31/146 [00:05<00:17,  6.46it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6827006340026855:  18%|██▎          | 27/151 [00:05<00:19,  6.30it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6827006340026855:  19%|██▍          | 28/151 [00:05<00:21,  5.61it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6866216659545898:   1%|▏              | 4/383 [00:01<02:03,  3.06it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6866216659545898:   1%|▏              | 5/383 [00:02<01:40,  3.77it/s]Epoch: 1, train for the 27-th batch, train loss: 0.5281291604042053:  22%|██▊          | 26/119 [00:05<00:14,  6.50it/s]Epoch: 1, train for the 27-th batch, train loss: 0.5281291604042053:  23%|██▉          | 27/119 [00:05<00:13,  6.78it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6647674441337585:   9%|█▏           | 21/241 [00:04<00:35,  6.16it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6647674441337585:   9%|█▏           | 22/241 [00:04<00:34,  6.37it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6387411952018738:  21%|██▊          | 31/146 [00:05<00:17,  6.46it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6387411952018738:  22%|██▊          | 32/146 [00:05<00:17,  6.44it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6907879710197449:  19%|██▍          | 28/151 [00:05<00:21,  5.61it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6907879710197449:  19%|██▍          | 29/151 [00:05<00:21,  5.59it/s]Epoch: 1, train for the 28-th batch, train loss: 0.5423087477684021:  23%|██▉          | 27/119 [00:05<00:13,  6.78it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6559814214706421:   9%|█▏           | 22/241 [00:04<00:34,  6.37it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6523625254631042:  10%|█▎           | 23/237 [00:04<00:38,  5.62it/s]Epoch: 1, train for the 28-th batch, train loss: 0.5423087477684021:  24%|███          | 28/119 [00:05<00:13,  6.58it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6870135068893433:   1%|▏              | 5/383 [00:02<01:40,  3.77it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6559814214706421:  10%|█▏           | 23/241 [00:04<00:32,  6.65it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6523625254631042:  10%|█▎           | 24/237 [00:04<00:43,  4.95it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6870135068893433:   2%|▏              | 6/383 [00:02<01:34,  4.01it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6312958598136902:  22%|██▊          | 32/146 [00:05<00:17,  6.44it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6312958598136902:  23%|██▉          | 33/146 [00:05<00:16,  6.87it/s]Epoch: 1, train for the 29-th batch, train loss: 0.5016618967056274:  24%|███          | 28/119 [00:05<00:13,  6.58it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6868559122085571:  19%|██▍          | 29/151 [00:05<00:21,  5.59it/s]Epoch: 1, train for the 29-th batch, train loss: 0.5016618967056274:  24%|███▏         | 29/119 [00:05<00:13,  6.71it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6868559122085571:  20%|██▌          | 30/151 [00:05<00:21,  5.52it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6618198752403259:  10%|█▏           | 23/241 [00:04<00:32,  6.65it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6618198752403259:  10%|█▎           | 24/241 [00:04<00:33,  6.51it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6328970193862915:  23%|██▉          | 33/146 [00:05<00:16,  6.87it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6328970193862915:  23%|███          | 34/146 [00:05<00:17,  6.57it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6577770709991455:  10%|█▎           | 24/237 [00:04<00:43,  4.95it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6901381015777588:   2%|▏              | 6/383 [00:02<01:34,  4.01it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6901381015777588:   2%|▎              | 7/383 [00:02<01:30,  4.16it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6577770709991455:  11%|█▎           | 25/237 [00:04<00:44,  4.74it/s]Epoch: 1, train for the 30-th batch, train loss: 0.5060012936592102:  24%|███▏         | 29/119 [00:05<00:13,  6.71it/s]Epoch: 1, train for the 30-th batch, train loss: 0.5060012936592102:  25%|███▎         | 30/119 [00:05<00:12,  6.98it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6853776574134827:  20%|██▌          | 30/151 [00:05<00:21,  5.52it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6853776574134827:  21%|██▋          | 31/151 [00:05<00:20,  5.85it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6669557094573975:  10%|█▎           | 24/241 [00:04<00:33,  6.51it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6669557094573975:  10%|█▎           | 25/241 [00:04<00:33,  6.45it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6258853673934937:  23%|███          | 34/146 [00:06<00:17,  6.57it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6258853673934937:  24%|███          | 35/146 [00:06<00:17,  6.45it/s]Epoch: 1, train for the 31-th batch, train loss: 0.5059361457824707:  25%|███▎         | 30/119 [00:05<00:12,  6.98it/s]Epoch: 1, train for the 31-th batch, train loss: 0.5059361457824707:  26%|███▍         | 31/119 [00:05<00:12,  6.90it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6809025406837463:   2%|▎              | 7/383 [00:02<01:30,  4.16it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6453596949577332:  11%|█▎           | 25/237 [00:04<00:44,  4.74it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6801568269729614:  21%|██▋          | 31/151 [00:05<00:20,  5.85it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6809025406837463:   2%|▎              | 8/383 [00:02<01:29,  4.18it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6453596949577332:  11%|█▍           | 26/237 [00:04<00:45,  4.60it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6801568269729614:  21%|██▊          | 32/151 [00:05<00:20,  5.73it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6734492182731628:  10%|█▎           | 25/241 [00:04<00:33,  6.45it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6152385473251343:  24%|███          | 35/146 [00:06<00:17,  6.45it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6734492182731628:  11%|█▍           | 26/241 [00:04<00:34,  6.28it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6152385473251343:  25%|███▏         | 36/146 [00:06<00:16,  6.75it/s]Epoch: 1, train for the 32-th batch, train loss: 0.48571425676345825:  26%|███▏        | 31/119 [00:06<00:12,  6.90it/s]Epoch: 1, train for the 32-th batch, train loss: 0.48571425676345825:  27%|███▏        | 32/119 [00:06<00:11,  7.34it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6789000034332275:  21%|██▊          | 32/151 [00:06<00:20,  5.73it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6789000034332275:  22%|██▊          | 33/151 [00:06<00:21,  5.52it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6148447394371033:  25%|███▏         | 36/146 [00:06<00:16,  6.75it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6562952399253845:  11%|█▍           | 26/241 [00:05<00:34,  6.28it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6148447394371033:  25%|███▎         | 37/146 [00:06<00:17,  6.36it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6562952399253845:  11%|█▍           | 27/241 [00:05<00:36,  5.86it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6491923332214355:  11%|█▍           | 26/237 [00:05<00:45,  4.60it/s]Epoch: 1, train for the 33-th batch, train loss: 0.5360750555992126:  27%|███▍         | 32/119 [00:06<00:11,  7.34it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6886007785797119:   2%|▎              | 8/383 [00:02<01:29,  4.18it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6491923332214355:  11%|█▍           | 27/237 [00:05<00:46,  4.50it/s]Epoch: 1, train for the 33-th batch, train loss: 0.5360750555992126:  28%|███▌         | 33/119 [00:06<00:12,  6.73it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6886007785797119:   2%|▎              | 9/383 [00:02<01:29,  4.16it/s]Epoch: 1, train for the 38-th batch, train loss: 0.6057800054550171:  25%|███▎         | 37/146 [00:06<00:17,  6.36it/s]Epoch: 1, train for the 38-th batch, train loss: 0.6057800054550171:  26%|███▍         | 38/146 [00:06<00:16,  6.58it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6249796152114868:  11%|█▍           | 27/241 [00:05<00:36,  5.86it/s]Epoch: 1, train for the 34-th batch, train loss: 0.681901216506958:  22%|███           | 33/151 [00:06<00:21,  5.52it/s]Epoch: 1, train for the 34-th batch, train loss: 0.4966733455657959:  28%|███▌         | 33/119 [00:06<00:12,  6.73it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6249796152114868:  12%|█▌           | 28/241 [00:05<00:37,  5.64it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6456267237663269:  11%|█▍           | 27/237 [00:05<00:46,  4.50it/s]Epoch: 1, train for the 34-th batch, train loss: 0.4966733455657959:  29%|███▋         | 34/119 [00:06<00:13,  6.43it/s]Epoch: 1, train for the 34-th batch, train loss: 0.681901216506958:  23%|███▏          | 34/151 [00:06<00:22,  5.25it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6456267237663269:  12%|█▌           | 28/237 [00:05<00:43,  4.76it/s]Epoch: 1, train for the 10-th batch, train loss: 0.683815598487854:   2%|▎              | 9/383 [00:03<01:29,  4.16it/s]Epoch: 1, train for the 10-th batch, train loss: 0.683815598487854:   3%|▎             | 10/383 [00:03<01:26,  4.31it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6090292930603027:  26%|███▍         | 38/146 [00:06<00:16,  6.58it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6090292930603027:  27%|███▍         | 39/146 [00:06<00:15,  6.82it/s]Epoch: 1, train for the 35-th batch, train loss: 0.49534544348716736:  29%|███▍        | 34/119 [00:06<00:13,  6.43it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6483301520347595:  12%|█▌           | 28/241 [00:05<00:37,  5.64it/s]Epoch: 1, train for the 35-th batch, train loss: 0.49534544348716736:  29%|███▌        | 35/119 [00:06<00:12,  6.53it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6483301520347595:  12%|█▌           | 29/241 [00:05<00:36,  5.87it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6387763619422913:  12%|█▌           | 28/237 [00:05<00:43,  4.76it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6821856498718262:  23%|██▉          | 34/151 [00:06<00:22,  5.25it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6387763619422913:  12%|█▌           | 29/237 [00:05<00:42,  4.95it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6007162928581238:  27%|███▍         | 39/146 [00:06<00:15,  6.82it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6821856498718262:  23%|███          | 35/151 [00:06<00:22,  5.21it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6007162928581238:  27%|███▌         | 40/146 [00:06<00:15,  7.05it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6760001182556152:   3%|▎            | 10/383 [00:03<01:26,  4.31it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6760001182556152:   3%|▎            | 11/383 [00:03<01:21,  4.57it/s]Epoch: 1, train for the 36-th batch, train loss: 0.48623237013816833:  29%|███▌        | 35/119 [00:06<00:12,  6.53it/s]Epoch: 1, train for the 36-th batch, train loss: 0.48623237013816833:  30%|███▋        | 36/119 [00:06<00:12,  6.78it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6665868759155273:  12%|█▌           | 29/241 [00:05<00:36,  5.87it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6665868759155273:  12%|█▌           | 30/241 [00:05<00:35,  5.97it/s]Epoch: 1, train for the 41-th batch, train loss: 0.5690287947654724:  27%|███▌         | 40/146 [00:07<00:15,  7.05it/s]Epoch: 1, train for the 41-th batch, train loss: 0.5690287947654724:  28%|███▋         | 41/146 [00:07<00:15,  6.95it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6933531165122986:  23%|███          | 35/151 [00:06<00:22,  5.21it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6399372220039368:  12%|█▌           | 29/237 [00:05<00:42,  4.95it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6933531165122986:  24%|███          | 36/151 [00:06<00:21,  5.35it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6399372220039368:  13%|█▋           | 30/237 [00:05<00:40,  5.09it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6694417595863342:   3%|▎            | 11/383 [00:03<01:21,  4.57it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6694417595863342:   3%|▍            | 12/383 [00:03<01:16,  4.83it/s]Epoch: 1, train for the 37-th batch, train loss: 0.4975404739379883:  30%|███▉         | 36/119 [00:06<00:12,  6.78it/s]Epoch: 1, train for the 37-th batch, train loss: 0.4975404739379883:  31%|████         | 37/119 [00:06<00:11,  6.88it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6221490502357483:  12%|█▌           | 30/241 [00:05<00:35,  5.97it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6221490502357483:  13%|█▋           | 31/241 [00:05<00:32,  6.45it/s]Epoch: 1, train for the 42-th batch, train loss: 0.5686655044555664:  28%|███▋         | 41/146 [00:07<00:15,  6.95it/s]Epoch: 1, train for the 42-th batch, train loss: 0.5686655044555664:  29%|███▋         | 42/146 [00:07<00:14,  7.40it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6317543387413025:  13%|█▋           | 30/237 [00:05<00:40,  5.09it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6855753064155579:  24%|███          | 36/151 [00:06<00:21,  5.35it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6317543387413025:  13%|█▋           | 31/237 [00:05<00:39,  5.27it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6855753064155579:  25%|███▏         | 37/151 [00:06<00:21,  5.43it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6752030849456787:   3%|▍            | 12/383 [00:03<01:16,  4.83it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6058704257011414:  13%|█▋           | 31/241 [00:05<00:32,  6.45it/s]Epoch: 1, train for the 38-th batch, train loss: 0.47593775391578674:  31%|███▋        | 37/119 [00:06<00:11,  6.88it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6752030849456787:   3%|▍            | 13/383 [00:03<01:14,  4.99it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6058704257011414:  13%|█▋           | 32/241 [00:05<00:33,  6.31it/s]Epoch: 1, train for the 38-th batch, train loss: 0.47593775391578674:  32%|███▊        | 38/119 [00:06<00:12,  6.36it/s]Epoch: 1, train for the 43-th batch, train loss: 0.5698993802070618:  29%|███▋         | 42/146 [00:07<00:14,  7.40it/s]Epoch: 1, train for the 43-th batch, train loss: 0.5698993802070618:  29%|███▊         | 43/146 [00:07<00:15,  6.75it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6395508646965027:  13%|█▋           | 31/237 [00:05<00:39,  5.27it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6395508646965027:  14%|█▊           | 32/237 [00:05<00:36,  5.64it/s]Epoch: 1, train for the 38-th batch, train loss: 0.678305983543396:  25%|███▍          | 37/151 [00:07<00:21,  5.43it/s]Epoch: 1, train for the 38-th batch, train loss: 0.678305983543396:  25%|███▌          | 38/151 [00:07<00:20,  5.50it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6574548482894897:  13%|█▋           | 32/241 [00:05<00:33,  6.31it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6574548482894897:  14%|█▊           | 33/241 [00:06<00:33,  6.26it/s]Epoch: 1, train for the 39-th batch, train loss: 0.5016283392906189:  32%|████▏        | 38/119 [00:07<00:12,  6.36it/s]Epoch: 1, train for the 39-th batch, train loss: 0.5016283392906189:  33%|████▎        | 39/119 [00:07<00:12,  6.19it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6828272342681885:   3%|▍            | 13/383 [00:03<01:14,  4.99it/s]Epoch: 1, train for the 44-th batch, train loss: 0.5480684638023376:  29%|███▊         | 43/146 [00:07<00:15,  6.75it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6828272342681885:   4%|▍            | 14/383 [00:03<01:14,  4.93it/s]Epoch: 1, train for the 44-th batch, train loss: 0.5480684638023376:  30%|███▉         | 44/146 [00:07<00:15,  6.50it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6168952584266663:  14%|█▊           | 32/237 [00:06<00:36,  5.64it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6168952584266663:  14%|█▊           | 33/237 [00:06<00:35,  5.70it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6784870624542236:  25%|███▎         | 38/151 [00:07<00:20,  5.50it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6784870624542236:  26%|███▎         | 39/151 [00:07<00:20,  5.45it/s]Epoch: 1, train for the 40-th batch, train loss: 0.5069851279258728:  33%|████▎        | 39/119 [00:07<00:12,  6.19it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6229292154312134:  14%|█▊           | 33/241 [00:06<00:33,  6.26it/s]Epoch: 1, train for the 40-th batch, train loss: 0.5069851279258728:  34%|████▎        | 40/119 [00:07<00:12,  6.21it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6229292154312134:  14%|█▊           | 34/241 [00:06<00:34,  6.03it/s]Epoch: 1, train for the 45-th batch, train loss: 0.5699600577354431:  30%|███▉         | 44/146 [00:07<00:15,  6.50it/s]Epoch: 1, train for the 45-th batch, train loss: 0.5699600577354431:  31%|████         | 45/146 [00:07<00:16,  6.30it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6695032119750977:   4%|▍            | 14/383 [00:04<01:14,  4.93it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6695032119750977:   4%|▌            | 15/383 [00:04<01:14,  4.95it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6149902939796448:  14%|█▊           | 33/237 [00:06<00:35,  5.70it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6149902939796448:  14%|█▊           | 34/237 [00:06<00:35,  5.71it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6685354709625244:  26%|███▎         | 39/151 [00:07<00:20,  5.45it/s]Epoch: 1, train for the 41-th batch, train loss: 0.53033447265625:  34%|█████          | 40/119 [00:07<00:12,  6.21it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6685354709625244:  26%|███▍         | 40/151 [00:07<00:20,  5.51it/s]Epoch: 1, train for the 41-th batch, train loss: 0.53033447265625:  34%|█████▏         | 41/119 [00:07<00:12,  6.22it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6373609900474548:  14%|█▊           | 34/241 [00:06<00:34,  6.03it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6373609900474548:  15%|█▉           | 35/241 [00:06<00:35,  5.88it/s]Epoch: 1, train for the 46-th batch, train loss: 0.5563206672668457:  31%|████         | 45/146 [00:07<00:16,  6.30it/s]Epoch: 1, train for the 46-th batch, train loss: 0.5563206672668457:  32%|████         | 46/146 [00:07<00:15,  6.36it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6835131645202637:   4%|▌            | 15/383 [00:04<01:14,  4.95it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6835131645202637:   4%|▌            | 16/383 [00:04<01:11,  5.12it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6110622882843018:  14%|█▊           | 34/237 [00:06<00:35,  5.71it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6110622882843018:  15%|█▉           | 35/237 [00:06<00:36,  5.58it/s]Epoch: 1, train for the 42-th batch, train loss: 0.5582599639892578:  34%|████▍        | 41/119 [00:07<00:12,  6.22it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6641851663589478:  26%|███▍         | 40/151 [00:07<00:20,  5.51it/s]Epoch: 1, train for the 42-th batch, train loss: 0.5582599639892578:  35%|████▌        | 42/119 [00:07<00:12,  6.19it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6641851663589478:  27%|███▌         | 41/151 [00:07<00:19,  5.55it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6406891345977783:  15%|█▉           | 35/241 [00:06<00:35,  5.88it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6406891345977783:  15%|█▉           | 36/241 [00:06<00:34,  6.01it/s]Epoch: 1, train for the 47-th batch, train loss: 0.5567367672920227:  32%|████         | 46/146 [00:07<00:15,  6.36it/s]Epoch: 1, train for the 47-th batch, train loss: 0.5567367672920227:  32%|████▏        | 47/146 [00:07<00:15,  6.22it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6821673512458801:   4%|▌            | 16/383 [00:04<01:11,  5.12it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6821673512458801:   4%|▌            | 17/383 [00:04<01:07,  5.40it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6060440540313721:  15%|█▉           | 35/237 [00:06<00:36,  5.58it/s]Epoch: 1, train for the 43-th batch, train loss: 0.49145716428756714:  35%|████▏       | 42/119 [00:07<00:12,  6.19it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6060440540313721:  15%|█▉           | 36/237 [00:06<00:35,  5.61it/s]Epoch: 1, train for the 43-th batch, train loss: 0.49145716428756714:  36%|████▎       | 43/119 [00:07<00:12,  6.30it/s]Epoch: 1, train for the 48-th batch, train loss: 0.5548810958862305:  32%|████▏        | 47/146 [00:08<00:15,  6.22it/s]Epoch: 1, train for the 48-th batch, train loss: 0.5548810958862305:  33%|████▎        | 48/146 [00:08<00:15,  6.37it/s]Epoch: 1, train for the 42-th batch, train loss: 0.662628710269928:  27%|███▊          | 41/151 [00:07<00:19,  5.55it/s]Epoch: 1, train for the 37-th batch, train loss: 0.5899965167045593:  15%|█▉           | 36/241 [00:06<00:34,  6.01it/s]Epoch: 1, train for the 42-th batch, train loss: 0.662628710269928:  28%|███▉          | 42/151 [00:07<00:20,  5.38it/s]Epoch: 1, train for the 37-th batch, train loss: 0.5899965167045593:  15%|█▉           | 37/241 [00:06<00:35,  5.78it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6787129640579224:   4%|▌            | 17/383 [00:04<01:07,  5.40it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6787129640579224:   5%|▌            | 18/383 [00:04<01:07,  5.42it/s]Epoch: 1, train for the 37-th batch, train loss: 0.5976731181144714:  15%|█▉           | 36/237 [00:06<00:35,  5.61it/s]Epoch: 1, train for the 37-th batch, train loss: 0.5976731181144714:  16%|██           | 37/237 [00:06<00:34,  5.76it/s]Epoch: 1, train for the 44-th batch, train loss: 0.4709891676902771:  36%|████▋        | 43/119 [00:07<00:12,  6.30it/s]Epoch: 1, train for the 44-th batch, train loss: 0.4709891676902771:  37%|████▊        | 44/119 [00:07<00:12,  6.12it/s]Epoch: 1, train for the 49-th batch, train loss: 0.5465639233589172:  33%|████▎        | 48/146 [00:08<00:15,  6.37it/s]Epoch: 1, train for the 49-th batch, train loss: 0.5465639233589172:  34%|████▎        | 49/146 [00:08<00:15,  6.33it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6771156191825867:  28%|███▌         | 42/151 [00:07<00:20,  5.38it/s]Epoch: 1, train for the 38-th batch, train loss: 0.604864239692688:  15%|██▏           | 37/241 [00:06<00:35,  5.78it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6771156191825867:  28%|███▋         | 43/151 [00:07<00:20,  5.34it/s]Epoch: 1, train for the 38-th batch, train loss: 0.604864239692688:  16%|██▏           | 38/241 [00:06<00:36,  5.57it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6574190258979797:   5%|▌            | 18/383 [00:04<01:07,  5.42it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6574190258979797:   5%|▋            | 19/383 [00:04<01:07,  5.37it/s]Epoch: 1, train for the 38-th batch, train loss: 0.5942612886428833:  16%|██           | 37/237 [00:07<00:34,  5.76it/s]Epoch: 1, train for the 38-th batch, train loss: 0.5942612886428833:  16%|██           | 38/237 [00:07<00:35,  5.62it/s]Epoch: 1, train for the 45-th batch, train loss: 0.44775688648223877:  37%|████▍       | 44/119 [00:08<00:12,  6.12it/s]Epoch: 1, train for the 45-th batch, train loss: 0.44775688648223877:  38%|████▌       | 45/119 [00:08<00:12,  5.98it/s]Epoch: 1, train for the 50-th batch, train loss: 0.5304349660873413:  34%|████▎        | 49/146 [00:08<00:15,  6.33it/s]Epoch: 1, train for the 50-th batch, train loss: 0.5304349660873413:  34%|████▍        | 50/146 [00:08<00:15,  6.10it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6341241598129272:  16%|██           | 38/241 [00:07<00:36,  5.57it/s]Epoch: 1, train for the 44-th batch, train loss: 0.6500047445297241:  28%|███▋         | 43/151 [00:08<00:20,  5.34it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6341241598129272:  16%|██           | 39/241 [00:07<00:35,  5.65it/s]Epoch: 1, train for the 44-th batch, train loss: 0.6500047445297241:  29%|███▊         | 44/151 [00:08<00:19,  5.36it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6542203426361084:   5%|▋            | 19/383 [00:04<01:07,  5.37it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6542203426361084:   5%|▋            | 20/383 [00:04<01:06,  5.43it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6162093877792358:  16%|██           | 38/237 [00:07<00:35,  5.62it/s]Epoch: 1, train for the 46-th batch, train loss: 0.5144970417022705:  38%|████▉        | 45/119 [00:08<00:12,  5.98it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6162093877792358:  16%|██▏          | 39/237 [00:07<00:35,  5.61it/s]Epoch: 1, train for the 46-th batch, train loss: 0.5144970417022705:  39%|█████        | 46/119 [00:08<00:12,  5.89it/s]Epoch: 1, train for the 51-th batch, train loss: 0.5377774834632874:  34%|████▍        | 50/146 [00:08<00:15,  6.10it/s]Epoch: 1, train for the 51-th batch, train loss: 0.5377774834632874:  35%|████▌        | 51/146 [00:08<00:15,  6.12it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6453118920326233:  16%|██           | 39/241 [00:07<00:35,  5.65it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6453118920326233:  17%|██▏          | 40/241 [00:07<00:33,  5.97it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6476823091506958:  29%|███▊         | 44/151 [00:08<00:19,  5.36it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6476823091506958:  30%|███▊         | 45/151 [00:08<00:19,  5.33it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6728576421737671:   5%|▋            | 20/383 [00:05<01:06,  5.43it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6728576421737671:   5%|▋            | 21/383 [00:05<01:08,  5.31it/s]Epoch: 1, train for the 52-th batch, train loss: 0.5273852348327637:  35%|████▌        | 51/146 [00:08<00:15,  6.12it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6182366609573364:  17%|██▏          | 40/241 [00:07<00:33,  5.97it/s]Epoch: 1, train for the 52-th batch, train loss: 0.5273852348327637:  36%|████▋        | 52/146 [00:08<00:15,  6.24it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6182366609573364:  17%|██▏          | 41/241 [00:07<00:31,  6.38it/s]Epoch: 1, train for the 40-th batch, train loss: 0.605017364025116:  16%|██▎           | 39/237 [00:07<00:35,  5.61it/s]Epoch: 1, train for the 47-th batch, train loss: 0.5627341270446777:  39%|█████        | 46/119 [00:08<00:12,  5.89it/s]Epoch: 1, train for the 47-th batch, train loss: 0.5627341270446777:  39%|█████▏       | 47/119 [00:08<00:12,  5.58it/s]Epoch: 1, train for the 40-th batch, train loss: 0.605017364025116:  17%|██▎           | 40/237 [00:07<00:36,  5.36it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6608050465583801:  30%|███▊         | 45/151 [00:08<00:19,  5.33it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6608050465583801:  30%|███▉         | 46/151 [00:08<00:18,  5.71it/s]Epoch: 1, train for the 53-th batch, train loss: 0.5147238373756409:  36%|████▋        | 52/146 [00:08<00:15,  6.24it/s]Epoch: 1, train for the 53-th batch, train loss: 0.5147238373756409:  36%|████▋        | 53/146 [00:08<00:13,  6.65it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6171445250511169:  17%|██▏          | 41/241 [00:07<00:31,  6.38it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6171445250511169:  17%|██▎          | 42/241 [00:07<00:33,  6.03it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6698805093765259:   5%|▋            | 21/383 [00:05<01:08,  5.31it/s]Epoch: 1, train for the 48-th batch, train loss: 0.5020372867584229:  39%|█████▏       | 47/119 [00:08<00:12,  5.58it/s]Epoch: 1, train for the 48-th batch, train loss: 0.5020372867584229:  40%|█████▏       | 48/119 [00:08<00:12,  5.55it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6698805093765259:   6%|▋            | 22/383 [00:05<01:11,  5.02it/s]Epoch: 1, train for the 41-th batch, train loss: 0.5996727347373962:  17%|██▏          | 40/237 [00:07<00:36,  5.36it/s]Epoch: 1, train for the 41-th batch, train loss: 0.5996727347373962:  17%|██▏          | 41/237 [00:07<00:37,  5.22it/s]Epoch: 1, train for the 47-th batch, train loss: 0.64443439245224:  30%|████▌          | 46/151 [00:08<00:18,  5.71it/s]Epoch: 1, train for the 54-th batch, train loss: 0.5143117904663086:  36%|████▋        | 53/146 [00:09<00:13,  6.65it/s]Epoch: 1, train for the 54-th batch, train loss: 0.5143117904663086:  37%|████▊        | 54/146 [00:09<00:13,  6.97it/s]Epoch: 1, train for the 47-th batch, train loss: 0.64443439245224:  31%|████▋          | 47/151 [00:08<00:18,  5.56it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6282623410224915:  17%|██▎          | 42/241 [00:07<00:33,  6.03it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6282623410224915:  18%|██▎          | 43/241 [00:07<00:30,  6.40it/s]Epoch: 1, train for the 49-th batch, train loss: 0.543255090713501:  40%|█████▋        | 48/119 [00:08<00:12,  5.55it/s]Epoch: 1, train for the 49-th batch, train loss: 0.543255090713501:  41%|█████▊        | 49/119 [00:08<00:12,  5.49it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6660821437835693:   6%|▋            | 22/383 [00:05<01:11,  5.02it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6660821437835693:   6%|▊            | 23/383 [00:05<01:12,  4.97it/s]Epoch: 1, train for the 55-th batch, train loss: 0.5594935417175293:  37%|████▊        | 54/146 [00:09<00:13,  6.97it/s]Epoch: 1, train for the 55-th batch, train loss: 0.5594935417175293:  38%|████▉        | 55/146 [00:09<00:14,  6.43it/s]Epoch: 1, train for the 48-th batch, train loss: 0.6420935392379761:  31%|████         | 47/151 [00:08<00:18,  5.56it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6064234375953674:  17%|██▏          | 41/237 [00:07<00:37,  5.22it/s]Epoch: 1, train for the 48-th batch, train loss: 0.6420935392379761:  32%|████▏        | 48/151 [00:08<00:19,  5.39it/s]Epoch: 1, train for the 44-th batch, train loss: 0.6587508320808411:  18%|██▎          | 43/241 [00:07<00:30,  6.40it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6064234375953674:  18%|██▎          | 42/237 [00:07<00:39,  4.92it/s]Epoch: 1, train for the 44-th batch, train loss: 0.6587508320808411:  18%|██▎          | 44/241 [00:07<00:30,  6.44it/s]Epoch: 1, train for the 50-th batch, train loss: 0.5092722773551941:  41%|█████▎       | 49/119 [00:09<00:12,  5.49it/s]Epoch: 1, train for the 50-th batch, train loss: 0.5092722773551941:  42%|█████▍       | 50/119 [00:09<00:12,  5.46it/s]Epoch: 1, train for the 56-th batch, train loss: 0.5275880098342896:  38%|████▉        | 55/146 [00:09<00:14,  6.43it/s]Epoch: 1, train for the 56-th batch, train loss: 0.5275880098342896:  38%|████▉        | 56/146 [00:09<00:14,  6.10it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6465291976928711:   6%|▊            | 23/383 [00:05<01:12,  4.97it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6465291976928711:   6%|▊            | 24/383 [00:05<01:14,  4.84it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6411199569702148:  32%|████▏        | 48/151 [00:09<00:19,  5.39it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6144822835922241:  18%|██▎          | 42/237 [00:08<00:39,  4.92it/s]Epoch: 1, train for the 45-th batch, train loss: 0.5569142699241638:  18%|██▎          | 44/241 [00:08<00:30,  6.44it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6411199569702148:  32%|████▏        | 49/151 [00:09<00:19,  5.21it/s]Epoch: 1, train for the 45-th batch, train loss: 0.5569142699241638:  19%|██▍          | 45/241 [00:08<00:32,  6.05it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6144822835922241:  18%|██▎          | 43/237 [00:08<00:39,  4.92it/s]Epoch: 1, train for the 51-th batch, train loss: 0.5434321761131287:  42%|█████▍       | 50/119 [00:09<00:12,  5.46it/s]Epoch: 1, train for the 51-th batch, train loss: 0.5434321761131287:  43%|█████▌       | 51/119 [00:09<00:11,  5.87it/s]Epoch: 1, train for the 57-th batch, train loss: 0.5605137348175049:  38%|████▉        | 56/146 [00:09<00:14,  6.10it/s]Epoch: 1, train for the 57-th batch, train loss: 0.5605137348175049:  39%|█████        | 57/146 [00:09<00:13,  6.40it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6613731384277344:   6%|▊            | 24/383 [00:06<01:14,  4.84it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6613731384277344:   7%|▊            | 25/383 [00:06<01:13,  4.89it/s]Epoch: 1, train for the 44-th batch, train loss: 0.5961210131645203:  18%|██▎          | 43/237 [00:08<00:39,  4.92it/s]Epoch: 1, train for the 46-th batch, train loss: 0.5721395611763:  19%|██▉             | 45/241 [00:08<00:32,  6.05it/s]Epoch: 1, train for the 50-th batch, train loss: 0.6207790970802307:  32%|████▏        | 49/151 [00:09<00:19,  5.21it/s]Epoch: 1, train for the 44-th batch, train loss: 0.5961210131645203:  19%|██▍          | 44/237 [00:08<00:39,  4.90it/s]Epoch: 1, train for the 46-th batch, train loss: 0.5721395611763:  19%|███             | 46/241 [00:08<00:34,  5.59it/s]Epoch: 1, train for the 50-th batch, train loss: 0.6207790970802307:  33%|████▎        | 50/151 [00:09<00:20,  5.02it/s]Epoch: 1, train for the 52-th batch, train loss: 0.5298194885253906:  43%|█████▌       | 51/119 [00:09<00:11,  5.87it/s]Epoch: 1, train for the 52-th batch, train loss: 0.5298194885253906:  44%|█████▋       | 52/119 [00:09<00:11,  5.79it/s]Epoch: 1, train for the 58-th batch, train loss: 0.5532915592193604:  39%|█████        | 57/146 [00:09<00:13,  6.40it/s]Epoch: 1, train for the 58-th batch, train loss: 0.5532915592193604:  40%|█████▏       | 58/146 [00:09<00:13,  6.35it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6579920649528503:   7%|▊            | 25/383 [00:06<01:13,  4.89it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6579920649528503:   7%|▉            | 26/383 [00:06<01:11,  4.99it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6002127528190613:  19%|██▍          | 44/237 [00:08<00:39,  4.90it/s]Epoch: 1, train for the 53-th batch, train loss: 0.4711993634700775:  44%|█████▋       | 52/119 [00:09<00:11,  5.79it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6321445107460022:  19%|██▍          | 46/241 [00:08<00:34,  5.59it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6002127528190613:  19%|██▍          | 45/237 [00:08<00:38,  5.00it/s]Epoch: 1, train for the 53-th batch, train loss: 0.4711993634700775:  45%|█████▊       | 53/119 [00:09<00:11,  5.86it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6321445107460022:  20%|██▌          | 47/241 [00:08<00:35,  5.42it/s]Epoch: 1, train for the 51-th batch, train loss: 0.6488606929779053:  33%|████▎        | 50/151 [00:09<00:20,  5.02it/s]Epoch: 1, train for the 51-th batch, train loss: 0.6488606929779053:  34%|████▍        | 51/151 [00:09<00:20,  4.86it/s]Epoch: 1, train for the 59-th batch, train loss: 0.47657278180122375:  40%|████▊       | 58/146 [00:09<00:13,  6.35it/s]Epoch: 1, train for the 59-th batch, train loss: 0.47657278180122375:  40%|████▊       | 59/146 [00:09<00:14,  5.83it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6280709505081177:   7%|▉            | 26/383 [00:06<01:11,  4.99it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6280709505081177:   7%|▉            | 27/383 [00:06<01:08,  5.17it/s]Epoch: 1, train for the 48-th batch, train loss: 0.577117383480072:  20%|██▋           | 47/241 [00:08<00:35,  5.42it/s]Epoch: 1, train for the 46-th batch, train loss: 0.5688359141349792:  19%|██▍          | 45/237 [00:08<00:38,  5.00it/s]Epoch: 1, train for the 48-th batch, train loss: 0.577117383480072:  20%|██▊           | 48/241 [00:08<00:33,  5.71it/s]Epoch: 1, train for the 46-th batch, train loss: 0.5688359141349792:  19%|██▌          | 46/237 [00:08<00:36,  5.16it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6486479043960571:  34%|████▍        | 51/151 [00:09<00:20,  4.86it/s]Epoch: 1, train for the 54-th batch, train loss: 0.5128617286682129:  45%|█████▊       | 53/119 [00:09<00:11,  5.86it/s]Epoch: 1, train for the 54-th batch, train loss: 0.5128617286682129:  45%|█████▉       | 54/119 [00:09<00:11,  5.46it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6486479043960571:  34%|████▍        | 52/151 [00:09<00:19,  5.06it/s]Epoch: 1, train for the 60-th batch, train loss: 0.5207791924476624:  40%|█████▎       | 59/146 [00:10<00:14,  5.83it/s]Epoch: 1, train for the 60-th batch, train loss: 0.5207791924476624:  41%|█████▎       | 60/146 [00:10<00:15,  5.72it/s]Epoch: 1, train for the 49-th batch, train loss: 0.5885576605796814:  20%|██▌          | 48/241 [00:08<00:33,  5.71it/s]Epoch: 1, train for the 49-th batch, train loss: 0.5885576605796814:  20%|██▋          | 49/241 [00:08<00:33,  5.79it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6312472224235535:   7%|▉            | 27/383 [00:06<01:08,  5.17it/s]Epoch: 1, train for the 53-th batch, train loss: 0.6481478810310364:  34%|████▍        | 52/151 [00:09<00:19,  5.06it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6312472224235535:   7%|▉            | 28/383 [00:06<01:13,  4.86it/s]Epoch: 1, train for the 47-th batch, train loss: 0.5908949375152588:  19%|██▌          | 46/237 [00:08<00:36,  5.16it/s]Epoch: 1, train for the 53-th batch, train loss: 0.6481478810310364:  35%|████▌        | 53/151 [00:09<00:18,  5.28it/s]Epoch: 1, train for the 55-th batch, train loss: 0.4585549831390381:  45%|█████▉       | 54/119 [00:09<00:11,  5.46it/s]Epoch: 1, train for the 47-th batch, train loss: 0.5908949375152588:  20%|██▌          | 47/237 [00:08<00:38,  4.95it/s]Epoch: 1, train for the 55-th batch, train loss: 0.4585549831390381:  46%|██████       | 55/119 [00:09<00:11,  5.42it/s]Epoch: 1, train for the 61-th batch, train loss: 0.5576413869857788:  41%|█████▎       | 60/146 [00:10<00:15,  5.72it/s]Epoch: 1, train for the 61-th batch, train loss: 0.5576413869857788:  42%|█████▍       | 61/146 [00:10<00:15,  5.60it/s]Epoch: 1, train for the 50-th batch, train loss: 0.5938084125518799:  20%|██▋          | 49/241 [00:08<00:33,  5.79it/s]Epoch: 1, train for the 50-th batch, train loss: 0.5938084125518799:  21%|██▋          | 50/241 [00:08<00:31,  5.98it/s]Epoch: 1, train for the 54-th batch, train loss: 0.6087167859077454:  35%|████▌        | 53/151 [00:10<00:18,  5.28it/s]Epoch: 1, train for the 54-th batch, train loss: 0.6087167859077454:  36%|████▋        | 54/151 [00:10<00:18,  5.34it/s]Epoch: 1, train for the 56-th batch, train loss: 0.4808427393436432:  46%|██████       | 55/119 [00:10<00:11,  5.42it/s]Epoch: 1, train for the 56-th batch, train loss: 0.4808427393436432:  47%|██████       | 56/119 [00:10<00:11,  5.45it/s]Epoch: 1, train for the 51-th batch, train loss: 0.5682579874992371:  21%|██▋          | 50/241 [00:09<00:31,  5.98it/s]Epoch: 1, train for the 51-th batch, train loss: 0.5682579874992371:  21%|██▊          | 51/241 [00:09<00:29,  6.49it/s]Epoch: 1, train for the 62-th batch, train loss: 0.49168798327445984:  42%|█████       | 61/146 [00:10<00:15,  5.60it/s]Epoch: 1, train for the 62-th batch, train loss: 0.49168798327445984:  42%|█████       | 62/146 [00:10<00:15,  5.48it/s]Epoch: 1, train for the 29-th batch, train loss: 0.5948908925056458:   7%|▉            | 28/383 [00:06<01:13,  4.86it/s]Epoch: 1, train for the 48-th batch, train loss: 0.5893187522888184:  20%|██▌          | 47/237 [00:09<00:38,  4.95it/s]Epoch: 1, train for the 29-th batch, train loss: 0.5948908925056458:   8%|▉            | 29/383 [00:06<01:19,  4.47it/s]Epoch: 1, train for the 48-th batch, train loss: 0.5893187522888184:  20%|██▋          | 48/237 [00:09<00:40,  4.65it/s]Epoch: 1, train for the 55-th batch, train loss: 0.6145523190498352:  36%|████▋        | 54/151 [00:10<00:18,  5.34it/s]Epoch: 1, train for the 55-th batch, train loss: 0.6145523190498352:  36%|████▋        | 55/151 [00:10<00:16,  5.65it/s]Epoch: 1, train for the 52-th batch, train loss: 0.589367687702179:  21%|██▉           | 51/241 [00:09<00:29,  6.49it/s]Epoch: 1, train for the 52-th batch, train loss: 0.589367687702179:  22%|███           | 52/241 [00:09<00:28,  6.70it/s]Epoch: 1, train for the 57-th batch, train loss: 0.4795377552509308:  47%|██████       | 56/119 [00:10<00:11,  5.45it/s]Epoch: 1, train for the 57-th batch, train loss: 0.4795377552509308:  48%|██████▏      | 57/119 [00:10<00:11,  5.48it/s]Epoch: 1, train for the 63-th batch, train loss: 0.5299150943756104:  42%|█████▌       | 62/146 [00:10<00:15,  5.48it/s]Epoch: 1, train for the 63-th batch, train loss: 0.5299150943756104:  43%|█████▌       | 63/146 [00:10<00:14,  5.64it/s]Epoch: 1, train for the 49-th batch, train loss: 0.5761139392852783:  20%|██▋          | 48/237 [00:09<00:40,  4.65it/s]Epoch: 1, train for the 30-th batch, train loss: 0.605720043182373:   8%|█             | 29/383 [00:07<01:19,  4.47it/s]Epoch: 1, train for the 30-th batch, train loss: 0.605720043182373:   8%|█             | 30/383 [00:07<01:19,  4.45it/s]Epoch: 1, train for the 49-th batch, train loss: 0.5761139392852783:  21%|██▋          | 49/237 [00:09<00:41,  4.57it/s]Epoch: 1, train for the 53-th batch, train loss: 0.4426732659339905:  22%|██▊          | 52/241 [00:09<00:28,  6.70it/s]Epoch: 1, train for the 56-th batch, train loss: 0.5697759985923767:  36%|████▋        | 55/151 [00:10<00:16,  5.65it/s]Epoch: 1, train for the 53-th batch, train loss: 0.4426732659339905:  22%|██▊          | 53/241 [00:09<00:28,  6.53it/s]Epoch: 1, train for the 56-th batch, train loss: 0.5697759985923767:  37%|████▊        | 56/151 [00:10<00:17,  5.45it/s]Epoch: 1, train for the 64-th batch, train loss: 0.5745149254798889:  43%|█████▌       | 63/146 [00:10<00:14,  5.64it/s]Epoch: 1, train for the 58-th batch, train loss: 0.47634658217430115:  48%|█████▋      | 57/119 [00:10<00:11,  5.48it/s]Epoch: 1, train for the 64-th batch, train loss: 0.5745149254798889:  44%|█████▋       | 64/146 [00:10<00:14,  5.70it/s]Epoch: 1, train for the 58-th batch, train loss: 0.47634658217430115:  49%|█████▊      | 58/119 [00:10<00:11,  5.37it/s]Epoch: 1, train for the 54-th batch, train loss: 0.4423106908798218:  22%|██▊          | 53/241 [00:09<00:28,  6.53it/s]Epoch: 1, train for the 54-th batch, train loss: 0.4423106908798218:  22%|██▉          | 54/241 [00:09<00:28,  6.55it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6016514301300049:   8%|█            | 30/383 [00:07<01:19,  4.45it/s]Epoch: 1, train for the 57-th batch, train loss: 0.5747578740119934:  37%|████▊        | 56/151 [00:10<00:17,  5.45it/s]Epoch: 1, train for the 50-th batch, train loss: 0.5867716670036316:  21%|██▋          | 49/237 [00:09<00:41,  4.57it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6016514301300049:   8%|█            | 31/383 [00:07<01:20,  4.35it/s]Epoch: 1, train for the 50-th batch, train loss: 0.5867716670036316:  21%|██▋          | 50/237 [00:09<00:42,  4.45it/s]Epoch: 1, train for the 57-th batch, train loss: 0.5747578740119934:  38%|████▉        | 57/151 [00:10<00:17,  5.35it/s]Epoch: 1, train for the 65-th batch, train loss: 0.6488219499588013:  44%|█████▋       | 64/146 [00:10<00:14,  5.70it/s]Epoch: 1, train for the 65-th batch, train loss: 0.6488219499588013:  45%|█████▊       | 65/146 [00:10<00:14,  5.72it/s]Epoch: 1, train for the 59-th batch, train loss: 0.4863942265510559:  49%|██████▎      | 58/119 [00:10<00:11,  5.37it/s]Epoch: 1, train for the 59-th batch, train loss: 0.4863942265510559:  50%|██████▍      | 59/119 [00:10<00:11,  5.37it/s]Epoch: 1, train for the 55-th batch, train loss: 0.4173702001571655:  22%|██▉          | 54/241 [00:09<00:28,  6.55it/s]Epoch: 1, train for the 55-th batch, train loss: 0.4173702001571655:  23%|██▉          | 55/241 [00:09<00:27,  6.77it/s]Epoch: 1, train for the 58-th batch, train loss: 0.5662432909011841:  38%|████▉        | 57/151 [00:10<00:17,  5.35it/s]Epoch: 1, train for the 51-th batch, train loss: 0.5635809302330017:  21%|██▋          | 50/237 [00:09<00:42,  4.45it/s]Epoch: 1, train for the 58-th batch, train loss: 0.5662432909011841:  38%|████▉        | 58/151 [00:10<00:17,  5.36it/s]Epoch: 1, train for the 51-th batch, train loss: 0.5635809302330017:  22%|██▊          | 51/237 [00:09<00:40,  4.60it/s]Epoch: 1, train for the 56-th batch, train loss: 0.4438132643699646:  23%|██▉          | 55/241 [00:09<00:27,  6.77it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6172426342964172:   8%|█            | 31/383 [00:07<01:20,  4.35it/s]Epoch: 1, train for the 56-th batch, train loss: 0.4438132643699646:  23%|███          | 56/241 [00:09<00:26,  6.95it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6172426342964172:   8%|█            | 32/383 [00:07<01:20,  4.36it/s]Epoch: 1, train for the 66-th batch, train loss: 0.5447219610214233:  45%|█████▊       | 65/146 [00:11<00:14,  5.72it/s]Epoch: 1, train for the 60-th batch, train loss: 0.46686163544654846:  50%|█████▉      | 59/119 [00:10<00:11,  5.37it/s]Epoch: 1, train for the 66-th batch, train loss: 0.5447219610214233:  45%|█████▉       | 66/146 [00:11<00:15,  5.25it/s]Epoch: 1, train for the 60-th batch, train loss: 0.46686163544654846:  50%|██████      | 60/119 [00:10<00:11,  5.16it/s]Epoch: 1, train for the 59-th batch, train loss: 0.5428081750869751:  38%|████▉        | 58/151 [00:10<00:17,  5.36it/s]Epoch: 1, train for the 59-th batch, train loss: 0.5428081750869751:  39%|█████        | 59/151 [00:10<00:16,  5.63it/s]Epoch: 1, train for the 57-th batch, train loss: 0.4640728533267975:  23%|███          | 56/241 [00:09<00:26,  6.95it/s]Epoch: 1, train for the 57-th batch, train loss: 0.4640728533267975:  24%|███          | 57/241 [00:09<00:27,  6.65it/s]Epoch: 1, train for the 52-th batch, train loss: 0.5584319829940796:  22%|██▊          | 51/237 [00:09<00:40,  4.60it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6352077126502991:   8%|█            | 32/383 [00:07<01:20,  4.36it/s]Epoch: 1, train for the 52-th batch, train loss: 0.5584319829940796:  22%|██▊          | 52/237 [00:10<00:42,  4.38it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6352077126502991:   9%|█            | 33/383 [00:07<01:20,  4.33it/s]Epoch: 1, train for the 61-th batch, train loss: 0.4906099736690521:  50%|██████▌      | 60/119 [00:11<00:11,  5.16it/s]Epoch: 1, train for the 67-th batch, train loss: 0.578529953956604:  45%|██████▎       | 66/146 [00:11<00:15,  5.25it/s]Epoch: 1, train for the 61-th batch, train loss: 0.4906099736690521:  51%|██████▋      | 61/119 [00:11<00:11,  4.91it/s]Epoch: 1, train for the 67-th batch, train loss: 0.578529953956604:  46%|██████▍       | 67/146 [00:11<00:16,  4.93it/s]Epoch: 1, train for the 60-th batch, train loss: 0.5554623007774353:  39%|█████        | 59/151 [00:11<00:16,  5.63it/s]Epoch: 1, train for the 58-th batch, train loss: 0.36783021688461304:  24%|██▊         | 57/241 [00:10<00:27,  6.65it/s]Epoch: 1, train for the 60-th batch, train loss: 0.5554623007774353:  40%|█████▏       | 60/151 [00:11<00:16,  5.64it/s]Epoch: 1, train for the 58-th batch, train loss: 0.36783021688461304:  24%|██▉         | 58/241 [00:10<00:26,  6.81it/s]Epoch: 1, train for the 53-th batch, train loss: 0.5839064121246338:  22%|██▊          | 52/237 [00:10<00:42,  4.38it/s]Epoch: 1, train for the 53-th batch, train loss: 0.5839064121246338:  22%|██▉          | 53/237 [00:10<00:38,  4.77it/s]Epoch: 1, train for the 62-th batch, train loss: 0.5203103423118591:  51%|██████▋      | 61/119 [00:11<00:11,  4.91it/s]Epoch: 1, train for the 62-th batch, train loss: 0.5203103423118591:  52%|██████▊      | 62/119 [00:11<00:10,  5.24it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6014524698257446:   9%|█            | 33/383 [00:08<01:20,  4.33it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5609081983566284:  46%|█████▉       | 67/146 [00:11<00:16,  4.93it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6014524698257446:   9%|█▏           | 34/383 [00:08<01:17,  4.49it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5609081983566284:  47%|██████       | 68/146 [00:11<00:15,  4.99it/s]Epoch: 1, train for the 59-th batch, train loss: 0.6215097904205322:  24%|███▏         | 58/241 [00:10<00:26,  6.81it/s]Epoch: 1, train for the 59-th batch, train loss: 0.6215097904205322:  24%|███▏         | 59/241 [00:10<00:28,  6.50it/s]Epoch: 1, train for the 61-th batch, train loss: 0.5949127674102783:  40%|█████▏       | 60/151 [00:11<00:16,  5.64it/s]Epoch: 1, train for the 61-th batch, train loss: 0.5949127674102783:  40%|█████▎       | 61/151 [00:11<00:17,  5.29it/s]Epoch: 1, train for the 54-th batch, train loss: 0.5334696769714355:  22%|██▉          | 53/237 [00:10<00:38,  4.77it/s]Epoch: 1, train for the 54-th batch, train loss: 0.5334696769714355:  23%|██▉          | 54/237 [00:10<00:35,  5.16it/s]Epoch: 1, train for the 63-th batch, train loss: 0.46919554471969604:  52%|██████▎     | 62/119 [00:11<00:10,  5.24it/s]Epoch: 1, train for the 63-th batch, train loss: 0.46919554471969604:  53%|██████▎     | 63/119 [00:11<00:10,  5.36it/s]Epoch: 1, train for the 69-th batch, train loss: 0.5508858561515808:  47%|██████       | 68/146 [00:11<00:15,  4.99it/s]Epoch: 1, train for the 69-th batch, train loss: 0.5508858561515808:  47%|██████▏      | 69/146 [00:11<00:14,  5.39it/s]Epoch: 1, train for the 35-th batch, train loss: 0.5957554578781128:   9%|█▏           | 34/383 [00:08<01:17,  4.49it/s]Epoch: 1, train for the 60-th batch, train loss: 0.6058658957481384:  24%|███▏         | 59/241 [00:10<00:28,  6.50it/s]Epoch: 1, train for the 35-th batch, train loss: 0.5957554578781128:   9%|█▏           | 35/383 [00:08<01:12,  4.83it/s]Epoch: 1, train for the 60-th batch, train loss: 0.6058658957481384:  25%|███▏         | 60/241 [00:10<00:27,  6.61it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6100003123283386:  40%|█████▎       | 61/151 [00:11<00:17,  5.29it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6100003123283386:  41%|█████▎       | 62/151 [00:11<00:15,  5.60it/s]Epoch: 1, train for the 55-th batch, train loss: 0.5716825127601624:  23%|██▉          | 54/237 [00:10<00:35,  5.16it/s]Epoch: 1, train for the 55-th batch, train loss: 0.5716825127601624:  23%|███          | 55/237 [00:10<00:33,  5.47it/s]Epoch: 1, train for the 61-th batch, train loss: 0.553770124912262:  25%|███▍          | 60/241 [00:10<00:27,  6.61it/s]Epoch: 1, train for the 64-th batch, train loss: 0.4671264886856079:  53%|██████▉      | 63/119 [00:11<00:10,  5.36it/s]Epoch: 1, train for the 61-th batch, train loss: 0.553770124912262:  25%|███▌          | 61/241 [00:10<00:26,  6.69it/s]Epoch: 1, train for the 64-th batch, train loss: 0.4671264886856079:  54%|██████▉      | 64/119 [00:11<00:10,  5.44it/s]Epoch: 1, train for the 70-th batch, train loss: 0.5917893052101135:  47%|██████▏      | 69/146 [00:11<00:14,  5.39it/s]Epoch: 1, train for the 70-th batch, train loss: 0.5917893052101135:  48%|██████▏      | 70/146 [00:11<00:14,  5.38it/s]Epoch: 1, train for the 36-th batch, train loss: 0.5960798859596252:   9%|█▏           | 35/383 [00:08<01:12,  4.83it/s]Epoch: 1, train for the 36-th batch, train loss: 0.5960798859596252:   9%|█▏           | 36/383 [00:08<01:10,  4.90it/s]Epoch: 1, train for the 63-th batch, train loss: 0.5402699708938599:  41%|█████▎       | 62/151 [00:11<00:15,  5.60it/s]Epoch: 1, train for the 63-th batch, train loss: 0.5402699708938599:  42%|█████▍       | 63/151 [00:11<00:15,  5.79it/s]Epoch: 1, train for the 56-th batch, train loss: 0.5754714608192444:  23%|███          | 55/237 [00:10<00:33,  5.47it/s]Epoch: 1, train for the 56-th batch, train loss: 0.5754714608192444:  24%|███          | 56/237 [00:10<00:31,  5.80it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6462350487709045:  25%|███▎         | 61/241 [00:10<00:26,  6.69it/s]Epoch: 1, train for the 65-th batch, train loss: 0.4748525023460388:  54%|██████▉      | 64/119 [00:11<00:10,  5.44it/s]Epoch: 1, train for the 65-th batch, train loss: 0.4748525023460388:  55%|███████      | 65/119 [00:11<00:09,  5.87it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6462350487709045:  26%|███▎         | 62/241 [00:10<00:26,  6.77it/s]Epoch: 1, train for the 37-th batch, train loss: 0.5945939421653748:   9%|█▏           | 36/383 [00:08<01:10,  4.90it/s]Epoch: 1, train for the 37-th batch, train loss: 0.5945939421653748:  10%|█▎           | 37/383 [00:08<01:08,  5.02it/s]Epoch: 1, train for the 64-th batch, train loss: 0.5228713750839233:  42%|█████▍       | 63/151 [00:11<00:15,  5.79it/s]Epoch: 1, train for the 57-th batch, train loss: 0.5722422003746033:  24%|███          | 56/237 [00:10<00:31,  5.80it/s]Epoch: 1, train for the 71-th batch, train loss: 0.5274628400802612:  48%|██████▏      | 70/146 [00:12<00:14,  5.38it/s]Epoch: 1, train for the 57-th batch, train loss: 0.5722422003746033:  24%|███▏         | 57/237 [00:10<00:31,  5.68it/s]Epoch: 1, train for the 64-th batch, train loss: 0.5228713750839233:  42%|█████▌       | 64/151 [00:11<00:16,  5.39it/s]Epoch: 1, train for the 71-th batch, train loss: 0.5274628400802612:  49%|██████▎      | 71/146 [00:12<00:15,  4.77it/s]Epoch: 1, train for the 63-th batch, train loss: 0.8850657939910889:  26%|███▎         | 62/241 [00:10<00:26,  6.77it/s]Epoch: 1, train for the 66-th batch, train loss: 0.47718191146850586:  55%|██████▌     | 65/119 [00:11<00:09,  5.87it/s]Epoch: 1, train for the 66-th batch, train loss: 0.47718191146850586:  55%|██████▋     | 66/119 [00:11<00:08,  6.00it/s]Epoch: 1, train for the 63-th batch, train loss: 0.8850657939910889:  26%|███▍         | 63/241 [00:10<00:26,  6.63it/s]Epoch: 1, train for the 38-th batch, train loss: 0.5875484943389893:  10%|█▎           | 37/383 [00:08<01:08,  5.02it/s]Epoch: 1, train for the 38-th batch, train loss: 0.5875484943389893:  10%|█▎           | 38/383 [00:08<01:08,  5.06it/s]Epoch: 1, train for the 65-th batch, train loss: 0.48749983310699463:  42%|█████       | 64/151 [00:12<00:16,  5.39it/s]Epoch: 1, train for the 58-th batch, train loss: 0.6165417432785034:  24%|███▏         | 57/237 [00:11<00:31,  5.68it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5699970722198486:  49%|██████▎      | 71/146 [00:12<00:15,  4.77it/s]Epoch: 1, train for the 64-th batch, train loss: 0.3317715525627136:  26%|███▍         | 63/241 [00:10<00:26,  6.63it/s]Epoch: 1, train for the 58-th batch, train loss: 0.6165417432785034:  24%|███▏         | 58/237 [00:11<00:32,  5.55it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5699970722198486:  49%|██████▍      | 72/146 [00:12<00:15,  4.93it/s]Epoch: 1, train for the 65-th batch, train loss: 0.48749983310699463:  43%|█████▏      | 65/151 [00:12<00:16,  5.34it/s]Epoch: 1, train for the 67-th batch, train loss: 0.5686529278755188:  55%|███████▏     | 66/119 [00:12<00:08,  6.00it/s]Epoch: 1, train for the 64-th batch, train loss: 0.3317715525627136:  27%|███▍         | 64/241 [00:11<00:28,  6.19it/s]Epoch: 1, train for the 67-th batch, train loss: 0.5686529278755188:  56%|███████▎     | 67/119 [00:12<00:09,  5.72it/s]Epoch: 1, train for the 39-th batch, train loss: 0.5627095699310303:  10%|█▎           | 38/383 [00:08<01:08,  5.06it/s]Epoch: 1, train for the 39-th batch, train loss: 0.5627095699310303:  10%|█▎           | 39/383 [00:08<01:08,  5.02it/s]Epoch: 1, train for the 73-th batch, train loss: 0.537820041179657:  49%|██████▉       | 72/146 [00:12<00:15,  4.93it/s]Epoch: 1, train for the 73-th batch, train loss: 0.537820041179657:  50%|███████       | 73/146 [00:12<00:14,  5.04it/s]Epoch: 1, train for the 59-th batch, train loss: 0.6186933517456055:  24%|███▏         | 58/237 [00:11<00:32,  5.55it/s]Epoch: 1, train for the 66-th batch, train loss: 0.5576972365379333:  43%|█████▌       | 65/151 [00:12<00:16,  5.34it/s]Epoch: 1, train for the 65-th batch, train loss: 0.38401225209236145:  27%|███▏        | 64/241 [00:11<00:28,  6.19it/s]Epoch: 1, train for the 66-th batch, train loss: 0.5576972365379333:  44%|█████▋       | 66/151 [00:12<00:16,  5.22it/s]Epoch: 1, train for the 68-th batch, train loss: 0.463080495595932:  56%|███████▉      | 67/119 [00:12<00:09,  5.72it/s]Epoch: 1, train for the 65-th batch, train loss: 0.38401225209236145:  27%|███▏        | 65/241 [00:11<00:29,  5.88it/s]Epoch: 1, train for the 59-th batch, train loss: 0.6186933517456055:  25%|███▏         | 59/237 [00:11<00:33,  5.33it/s]Epoch: 1, train for the 68-th batch, train loss: 0.463080495595932:  57%|████████      | 68/119 [00:12<00:09,  5.53it/s]Epoch: 1, train for the 40-th batch, train loss: 0.5599589347839355:  10%|█▎           | 39/383 [00:09<01:08,  5.02it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5723360180854797:  50%|██████▌      | 73/146 [00:12<00:14,  5.04it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5723360180854797:  51%|██████▌      | 74/146 [00:12<00:13,  5.32it/s]Epoch: 1, train for the 40-th batch, train loss: 0.5599589347839355:  10%|█▎           | 40/383 [00:09<01:07,  5.05it/s]Epoch: 1, train for the 60-th batch, train loss: 0.5330016016960144:  25%|███▏         | 59/237 [00:11<00:33,  5.33it/s]Epoch: 1, train for the 66-th batch, train loss: 0.321479856967926:  27%|███▊          | 65/241 [00:11<00:29,  5.88it/s]Epoch: 1, train for the 66-th batch, train loss: 0.321479856967926:  27%|███▊          | 66/241 [00:11<00:30,  5.71it/s]Epoch: 1, train for the 69-th batch, train loss: 0.6355524063110352:  57%|███████▍     | 68/119 [00:12<00:09,  5.53it/s]Epoch: 1, train for the 60-th batch, train loss: 0.5330016016960144:  25%|███▎         | 60/237 [00:11<00:33,  5.33it/s]Epoch: 1, train for the 67-th batch, train loss: 0.6148892641067505:  44%|█████▋       | 66/151 [00:12<00:16,  5.22it/s]Epoch: 1, train for the 69-th batch, train loss: 0.6355524063110352:  58%|███████▌     | 69/119 [00:12<00:09,  5.50it/s]Epoch: 1, train for the 67-th batch, train loss: 0.6148892641067505:  44%|█████▊       | 67/151 [00:12<00:16,  5.07it/s]Epoch: 1, train for the 75-th batch, train loss: 0.6055454015731812:  51%|██████▌      | 74/146 [00:12<00:13,  5.32it/s]Epoch: 1, train for the 75-th batch, train loss: 0.6055454015731812:  51%|██████▋      | 75/146 [00:12<00:12,  5.77it/s]Epoch: 1, train for the 67-th batch, train loss: 0.41328051686286926:  27%|███▎        | 66/241 [00:11<00:30,  5.71it/s]Epoch: 1, train for the 61-th batch, train loss: 0.5879080295562744:  25%|███▎         | 60/237 [00:11<00:33,  5.33it/s]Epoch: 1, train for the 67-th batch, train loss: 0.41328051686286926:  28%|███▎        | 67/241 [00:11<00:29,  5.89it/s]Epoch: 1, train for the 61-th batch, train loss: 0.5879080295562744:  26%|███▎         | 61/237 [00:11<00:31,  5.53it/s]Epoch: 1, train for the 70-th batch, train loss: 0.4983082711696625:  58%|███████▌     | 69/119 [00:12<00:09,  5.50it/s]Epoch: 1, train for the 70-th batch, train loss: 0.4983082711696625:  59%|███████▋     | 70/119 [00:12<00:08,  5.63it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5539962649345398:  44%|█████▊       | 67/151 [00:12<00:16,  5.07it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5539962649345398:  45%|█████▊       | 68/151 [00:12<00:16,  5.16it/s]Epoch: 1, train for the 76-th batch, train loss: 0.558613657951355:  51%|███████▏      | 75/146 [00:13<00:12,  5.77it/s]Epoch: 1, train for the 41-th batch, train loss: 0.5237679481506348:  10%|█▎           | 40/383 [00:09<01:07,  5.05it/s]Epoch: 1, train for the 76-th batch, train loss: 0.558613657951355:  52%|███████▎      | 76/146 [00:13<00:11,  6.04it/s]Epoch: 1, train for the 41-th batch, train loss: 0.5237679481506348:  11%|█▍           | 41/383 [00:09<01:17,  4.39it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5417091250419617:  28%|███▌         | 67/241 [00:11<00:29,  5.89it/s]Epoch: 1, train for the 71-th batch, train loss: 0.4690444767475128:  59%|███████▋     | 70/119 [00:12<00:08,  5.63it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5417091250419617:  28%|███▋         | 68/241 [00:11<00:28,  6.00it/s]Epoch: 1, train for the 71-th batch, train loss: 0.4690444767475128:  60%|███████▊     | 71/119 [00:12<00:08,  5.97it/s]Epoch: 1, train for the 69-th batch, train loss: 0.5118169784545898:  45%|█████▊       | 68/151 [00:12<00:16,  5.16it/s]Epoch: 1, train for the 69-th batch, train loss: 0.5118169784545898:  46%|█████▉       | 69/151 [00:12<00:15,  5.36it/s]Epoch: 1, train for the 77-th batch, train loss: 0.59145587682724:  52%|███████▊       | 76/146 [00:13<00:11,  6.04it/s]Epoch: 1, train for the 77-th batch, train loss: 0.59145587682724:  53%|███████▉       | 77/146 [00:13<00:11,  6.01it/s]Epoch: 1, train for the 42-th batch, train loss: 0.5516077876091003:  11%|█▍           | 41/383 [00:09<01:17,  4.39it/s]Epoch: 1, train for the 42-th batch, train loss: 0.5516077876091003:  11%|█▍           | 42/383 [00:09<01:14,  4.60it/s]Epoch: 1, train for the 69-th batch, train loss: 0.6519659757614136:  28%|███▋         | 68/241 [00:11<00:28,  6.00it/s]Epoch: 1, train for the 62-th batch, train loss: 0.5531963109970093:  26%|███▎         | 61/237 [00:11<00:31,  5.53it/s]Epoch: 1, train for the 69-th batch, train loss: 0.6519659757614136:  29%|███▋         | 69/241 [00:11<00:27,  6.21it/s]Epoch: 1, train for the 72-th batch, train loss: 0.49140578508377075:  60%|███████▏    | 71/119 [00:13<00:08,  5.97it/s]Epoch: 1, train for the 62-th batch, train loss: 0.5531963109970093:  26%|███▍         | 62/237 [00:11<00:38,  4.51it/s]Epoch: 1, train for the 72-th batch, train loss: 0.49140578508377075:  61%|███████▎    | 72/119 [00:13<00:07,  6.03it/s]Epoch: 1, train for the 70-th batch, train loss: 0.561206042766571:  46%|██████▍       | 69/151 [00:12<00:15,  5.36it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5942962765693665:  53%|██████▊      | 77/146 [00:13<00:11,  6.01it/s]Epoch: 1, train for the 70-th batch, train loss: 0.561206042766571:  46%|██████▍       | 70/151 [00:12<00:14,  5.45it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5942962765693665:  53%|██████▉      | 78/146 [00:13<00:10,  6.31it/s]Epoch: 1, train for the 71-th batch, train loss: 0.5416862368583679:  46%|██████       | 70/151 [00:13<00:14,  5.45it/s]Epoch: 1, train for the 71-th batch, train loss: 0.5416862368583679:  47%|██████       | 71/151 [00:13<00:12,  6.17it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5700706243515015:  53%|██████▉      | 78/146 [00:13<00:10,  6.31it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5700706243515015:  54%|███████      | 79/146 [00:13<00:12,  5.42it/s]Epoch: 1, train for the 70-th batch, train loss: 0.5480228066444397:  29%|███▋         | 69/241 [00:12<00:27,  6.21it/s]Epoch: 1, train for the 70-th batch, train loss: 0.5480228066444397:  29%|███▊         | 70/241 [00:12<00:37,  4.57it/s]Epoch: 1, train for the 63-th batch, train loss: 0.6026040315628052:  26%|███▍         | 62/237 [00:12<00:38,  4.51it/s]Epoch: 1, train for the 73-th batch, train loss: 0.4522530734539032:  61%|███████▊     | 72/119 [00:13<00:07,  6.03it/s]Epoch: 1, train for the 73-th batch, train loss: 0.4522530734539032:  61%|███████▉     | 73/119 [00:13<00:10,  4.34it/s]Epoch: 1, train for the 63-th batch, train loss: 0.6026040315628052:  27%|███▍         | 63/237 [00:12<00:47,  3.70it/s]Epoch: 1, train for the 80-th batch, train loss: 0.607772946357727:  54%|███████▌      | 79/146 [00:13<00:12,  5.42it/s]Epoch: 1, train for the 80-th batch, train loss: 0.607772946357727:  55%|███████▋      | 80/146 [00:13<00:11,  5.84it/s]Epoch: 1, train for the 43-th batch, train loss: 0.5840514898300171:  11%|█▍           | 42/383 [00:10<01:14,  4.60it/s]Epoch: 1, train for the 43-th batch, train loss: 0.5840514898300171:  11%|█▍           | 43/383 [00:10<01:43,  3.28it/s]Epoch: 1, train for the 71-th batch, train loss: 0.6496877670288086:  29%|███▊         | 70/241 [00:12<00:37,  4.57it/s]Epoch: 1, train for the 72-th batch, train loss: 0.548991858959198:  47%|██████▌       | 71/151 [00:13<00:12,  6.17it/s]Epoch: 1, train for the 71-th batch, train loss: 0.6496877670288086:  29%|███▊         | 71/241 [00:12<00:34,  4.98it/s]Epoch: 1, train for the 72-th batch, train loss: 0.548991858959198:  48%|██████▋       | 72/151 [00:13<00:16,  4.67it/s]Epoch: 1, train for the 64-th batch, train loss: 0.5971828103065491:  27%|███▍         | 63/237 [00:12<00:47,  3.70it/s]Epoch: 1, train for the 64-th batch, train loss: 0.5971828103065491:  27%|███▌         | 64/237 [00:12<00:40,  4.23it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5406790971755981:  55%|███████      | 80/146 [00:13<00:11,  5.84it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5406790971755981:  55%|███████▏     | 81/146 [00:13<00:09,  6.55it/s]Epoch: 1, train for the 74-th batch, train loss: 0.4677097201347351:  61%|███████▉     | 73/119 [00:13<00:10,  4.34it/s]Epoch: 1, train for the 74-th batch, train loss: 0.4677097201347351:  62%|████████     | 74/119 [00:13<00:10,  4.31it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5926825404167175:  29%|███▊         | 71/241 [00:12<00:34,  4.98it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5926825404167175:  30%|███▉         | 72/241 [00:12<00:31,  5.43it/s]Epoch: 1, train for the 44-th batch, train loss: 0.5289265513420105:  11%|█▍           | 43/383 [00:10<01:43,  3.28it/s]Epoch: 1, train for the 44-th batch, train loss: 0.5289265513420105:  11%|█▍           | 44/383 [00:10<01:30,  3.74it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5883485674858093:  55%|███████▏     | 81/146 [00:13<00:09,  6.55it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5883485674858093:  56%|███████▎     | 82/146 [00:13<00:09,  6.84it/s]Epoch: 1, train for the 75-th batch, train loss: 0.49309325218200684:  62%|███████▍    | 74/119 [00:13<00:10,  4.31it/s]Epoch: 1, train for the 73-th batch, train loss: 0.526846706867218:  30%|████▏         | 72/241 [00:12<00:31,  5.43it/s]Epoch: 1, train for the 75-th batch, train loss: 0.49309325218200684:  63%|███████▌    | 75/119 [00:13<00:08,  4.93it/s]Epoch: 1, train for the 73-th batch, train loss: 0.526846706867218:  30%|████▏         | 73/241 [00:12<00:27,  6.06it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5623569488525391:  56%|███████▎     | 82/146 [00:14<00:09,  6.84it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5623569488525391:  57%|███████▍     | 83/146 [00:14<00:08,  7.03it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5482836961746216:  48%|██████▏      | 72/151 [00:13<00:16,  4.67it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5482836961746216:  48%|██████▎      | 73/151 [00:13<00:20,  3.79it/s]Epoch: 1, train for the 45-th batch, train loss: 0.5249822735786438:  11%|█▍           | 44/383 [00:10<01:30,  3.74it/s]Epoch: 1, train for the 65-th batch, train loss: 0.5949737429618835:  27%|███▌         | 64/237 [00:12<00:40,  4.23it/s]Epoch: 1, train for the 45-th batch, train loss: 0.5249822735786438:  12%|█▌           | 45/383 [00:10<01:34,  3.58it/s]Epoch: 1, train for the 65-th batch, train loss: 0.5949737429618835:  27%|███▌         | 65/237 [00:12<00:50,  3.40it/s]Epoch: 1, train for the 76-th batch, train loss: 0.508845865726471:  63%|████████▊     | 75/119 [00:14<00:08,  4.93it/s]Epoch: 1, train for the 74-th batch, train loss: 0.4958748519420624:  48%|██████▎      | 73/151 [00:13<00:20,  3.79it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5497437715530396:  57%|███████▍     | 83/146 [00:14<00:08,  7.03it/s]Epoch: 1, train for the 76-th batch, train loss: 0.508845865726471:  64%|████████▉     | 76/119 [00:14<00:09,  4.38it/s]Epoch: 1, train for the 74-th batch, train loss: 0.4668017029762268:  30%|███▉         | 73/241 [00:12<00:27,  6.06it/s]Epoch: 1, train for the 74-th batch, train loss: 0.4958748519420624:  49%|██████▎      | 74/151 [00:13<00:17,  4.34it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5497437715530396:  58%|███████▍     | 84/146 [00:14<00:10,  6.05it/s]Epoch: 1, train for the 74-th batch, train loss: 0.4668017029762268:  31%|███▉         | 74/241 [00:12<00:34,  4.86it/s]Epoch: 1, train for the 46-th batch, train loss: 0.5479264855384827:  12%|█▌           | 45/383 [00:10<01:34,  3.58it/s]Epoch: 1, train for the 66-th batch, train loss: 0.6177183985710144:  27%|███▌         | 65/237 [00:13<00:50,  3.40it/s]Epoch: 1, train for the 77-th batch, train loss: 0.4588305950164795:  64%|████████▎    | 76/119 [00:14<00:09,  4.38it/s]Epoch: 1, train for the 46-th batch, train loss: 0.5479264855384827:  12%|█▌           | 46/383 [00:10<01:31,  3.70it/s]Epoch: 1, train for the 77-th batch, train loss: 0.4588305950164795:  65%|████████▍    | 77/119 [00:14<00:09,  4.67it/s]Epoch: 1, train for the 75-th batch, train loss: 0.5598894953727722:  49%|██████▎      | 74/151 [00:14<00:17,  4.34it/s]Epoch: 1, train for the 75-th batch, train loss: 0.3828269839286804:  31%|███▉         | 74/241 [00:13<00:34,  4.86it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5594907402992249:  58%|███████▍     | 84/146 [00:14<00:10,  6.05it/s]Epoch: 1, train for the 66-th batch, train loss: 0.6177183985710144:  28%|███▌         | 66/237 [00:13<00:48,  3.52it/s]Epoch: 1, train for the 75-th batch, train loss: 0.5598894953727722:  50%|██████▍      | 75/151 [00:14<00:16,  4.60it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5594907402992249:  58%|███████▌     | 85/146 [00:14<00:10,  5.81it/s]Epoch: 1, train for the 75-th batch, train loss: 0.3828269839286804:  31%|████         | 75/241 [00:13<00:32,  5.03it/s]Epoch: 1, train for the 67-th batch, train loss: 0.5741527676582336:  28%|███▌         | 66/237 [00:13<00:48,  3.52it/s]Epoch: 1, train for the 67-th batch, train loss: 0.5741527676582336:  28%|███▋         | 67/237 [00:13<00:41,  4.10it/s]Epoch: 1, train for the 78-th batch, train loss: 0.501761794090271:  65%|█████████     | 77/119 [00:14<00:09,  4.67it/s]Epoch: 1, train for the 78-th batch, train loss: 0.501761794090271:  66%|█████████▏    | 78/119 [00:14<00:08,  4.80it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5685510039329529:  58%|███████▌     | 85/146 [00:14<00:10,  5.81it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5685510039329529:  59%|███████▋     | 86/146 [00:14<00:11,  5.44it/s]Epoch: 1, train for the 76-th batch, train loss: 0.28216394782066345:  31%|███▋        | 75/241 [00:13<00:32,  5.03it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5547953844070435:  28%|███▋         | 67/237 [00:13<00:41,  4.10it/s]Epoch: 1, train for the 76-th batch, train loss: 0.28216394782066345:  32%|███▊        | 76/241 [00:13<00:36,  4.53it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5547953844070435:  29%|███▋         | 68/237 [00:13<00:36,  4.67it/s]Epoch: 1, train for the 87-th batch, train loss: 0.5991656184196472:  59%|███████▋     | 86/146 [00:14<00:11,  5.44it/s]Epoch: 1, train for the 87-th batch, train loss: 0.5991656184196472:  60%|███████▋     | 87/146 [00:14<00:11,  5.31it/s]Epoch: 1, train for the 79-th batch, train loss: 0.4712795913219452:  66%|████████▌    | 78/119 [00:14<00:08,  4.80it/s]Epoch: 1, train for the 79-th batch, train loss: 0.4712795913219452:  66%|████████▋    | 79/119 [00:14<00:08,  4.57it/s]Epoch: 1, train for the 77-th batch, train loss: 0.3924593925476074:  32%|████         | 76/241 [00:13<00:36,  4.53it/s]Epoch: 1, train for the 76-th batch, train loss: 0.5931258797645569:  50%|██████▍      | 75/151 [00:14<00:16,  4.60it/s]Epoch: 1, train for the 77-th batch, train loss: 0.3924593925476074:  32%|████▏        | 77/241 [00:13<00:32,  4.98it/s]Epoch: 1, train for the 76-th batch, train loss: 0.5931258797645569:  50%|██████▌      | 76/151 [00:14<00:21,  3.52it/s]Epoch: 1, train for the 69-th batch, train loss: 0.5657033324241638:  29%|███▋         | 68/237 [00:13<00:36,  4.67it/s]Epoch: 1, train for the 47-th batch, train loss: 0.4718126654624939:  12%|█▌           | 46/383 [00:11<01:31,  3.70it/s]Epoch: 1, train for the 69-th batch, train loss: 0.5657033324241638:  29%|███▊         | 69/237 [00:13<00:35,  4.74it/s]Epoch: 1, train for the 47-th batch, train loss: 0.4718126654624939:  12%|█▌           | 47/383 [00:11<01:56,  2.88it/s]Epoch: 1, train for the 88-th batch, train loss: 0.6187474727630615:  60%|███████▋     | 87/146 [00:15<00:11,  5.31it/s]Epoch: 1, train for the 88-th batch, train loss: 0.6187474727630615:  60%|███████▊     | 88/146 [00:15<00:10,  5.67it/s]Epoch: 1, train for the 80-th batch, train loss: 0.5140916109085083:  66%|████████▋    | 79/119 [00:14<00:08,  4.57it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5743382573127747:  32%|████▏        | 77/241 [00:13<00:32,  4.98it/s]Epoch: 1, train for the 80-th batch, train loss: 0.5140916109085083:  67%|████████▋    | 80/119 [00:14<00:08,  4.87it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5743382573127747:  32%|████▏        | 78/241 [00:13<00:31,  5.20it/s]Epoch: 1, train for the 77-th batch, train loss: 0.5316162705421448:  50%|██████▌      | 76/151 [00:14<00:21,  3.52it/s]Epoch: 1, train for the 77-th batch, train loss: 0.5316162705421448:  51%|██████▋      | 77/151 [00:14<00:19,  3.85it/s]Epoch: 1, train for the 70-th batch, train loss: 0.5648396015167236:  29%|███▊         | 69/237 [00:13<00:35,  4.74it/s]Epoch: 1, train for the 70-th batch, train loss: 0.5648396015167236:  30%|███▊         | 70/237 [00:13<00:34,  4.81it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5551486611366272:  60%|███████▊     | 88/146 [00:15<00:10,  5.67it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5551486611366272:  61%|███████▉     | 89/146 [00:15<00:09,  5.81it/s]Epoch: 1, train for the 79-th batch, train loss: 0.49229082465171814:  32%|███▉        | 78/241 [00:13<00:31,  5.20it/s]Epoch: 1, train for the 79-th batch, train loss: 0.49229082465171814:  33%|███▉        | 79/241 [00:13<00:28,  5.71it/s]Epoch: 1, train for the 81-th batch, train loss: 0.4668395519256592:  67%|████████▋    | 80/119 [00:14<00:08,  4.87it/s]Epoch: 1, train for the 81-th batch, train loss: 0.4668395519256592:  68%|████████▊    | 81/119 [00:15<00:07,  5.17it/s]Epoch: 1, train for the 48-th batch, train loss: 0.45338743925094604:  12%|█▍          | 47/383 [00:11<01:56,  2.88it/s]Epoch: 1, train for the 48-th batch, train loss: 0.45338743925094604:  13%|█▌          | 48/383 [00:11<01:56,  2.89it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5332173109054565:  51%|██████▋      | 77/151 [00:15<00:19,  3.85it/s]Epoch: 1, train for the 80-th batch, train loss: 0.36923447251319885:  33%|███▉        | 79/241 [00:14<00:28,  5.71it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5332173109054565:  52%|██████▋      | 78/151 [00:15<00:21,  3.45it/s]Epoch: 1, train for the 80-th batch, train loss: 0.36923447251319885:  33%|███▉        | 80/241 [00:14<00:32,  4.94it/s]Epoch: 1, train for the 71-th batch, train loss: 0.5757157802581787:  30%|███▊         | 70/237 [00:14<00:34,  4.81it/s]Epoch: 1, train for the 49-th batch, train loss: 0.5049368739128113:  13%|█▋           | 48/383 [00:11<01:56,  2.89it/s]Epoch: 1, train for the 71-th batch, train loss: 0.5757157802581787:  30%|███▉         | 71/237 [00:14<00:41,  4.01it/s]Epoch: 1, train for the 49-th batch, train loss: 0.5049368739128113:  13%|█▋           | 49/383 [00:11<01:40,  3.34it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5165998935699463:  68%|████████▊    | 81/119 [00:15<00:07,  5.17it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5606998205184937:  61%|███████▉     | 89/146 [00:15<00:09,  5.81it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5606998205184937:  62%|████████     | 90/146 [00:15<00:12,  4.40it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5165998935699463:  69%|████████▉    | 82/119 [00:15<00:08,  4.32it/s]Epoch: 1, train for the 81-th batch, train loss: 0.442990779876709:  33%|████▋         | 80/241 [00:14<00:32,  4.94it/s]Epoch: 1, train for the 81-th batch, train loss: 0.442990779876709:  34%|████▋         | 81/241 [00:14<00:28,  5.60it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5809423923492432:  62%|████████     | 90/146 [00:15<00:12,  4.40it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5809423923492432:  62%|████████     | 91/146 [00:15<00:11,  4.81it/s]Epoch: 1, train for the 83-th batch, train loss: 0.4963650405406952:  69%|████████▉    | 82/119 [00:15<00:08,  4.32it/s]Epoch: 1, train for the 83-th batch, train loss: 0.4963650405406952:  70%|█████████    | 83/119 [00:15<00:07,  4.56it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5914771556854248:  52%|██████▋      | 78/151 [00:15<00:21,  3.45it/s]Epoch: 1, train for the 82-th batch, train loss: 0.4235279858112335:  34%|████▎        | 81/241 [00:14<00:28,  5.60it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5914771556854248:  52%|██████▊      | 79/151 [00:15<00:21,  3.40it/s]Epoch: 1, train for the 82-th batch, train loss: 0.4235279858112335:  34%|████▍        | 82/241 [00:14<00:28,  5.55it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5460183024406433:  30%|███▉         | 71/237 [00:14<00:41,  4.01it/s]Epoch: 1, train for the 50-th batch, train loss: 0.39343610405921936:  13%|█▌          | 49/383 [00:12<01:40,  3.34it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5460183024406433:  30%|███▉         | 72/237 [00:14<00:43,  3.76it/s]Epoch: 1, train for the 50-th batch, train loss: 0.39343610405921936:  13%|█▌          | 50/383 [00:12<01:41,  3.28it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5334952473640442:  34%|████▍        | 82/241 [00:14<00:28,  5.55it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5334952473640442:  34%|████▍        | 83/241 [00:14<00:28,  5.57it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5524116158485413:  30%|███▉         | 72/237 [00:14<00:43,  3.76it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5524116158485413:  31%|████         | 73/237 [00:14<00:39,  4.20it/s]Epoch: 1, train for the 84-th batch, train loss: 0.43575018644332886:  70%|████████▎   | 83/119 [00:15<00:07,  4.56it/s]Epoch: 1, train for the 80-th batch, train loss: 0.5418234467506409:  52%|██████▊      | 79/151 [00:15<00:21,  3.40it/s]Epoch: 1, train for the 51-th batch, train loss: 0.38294219970703125:  13%|█▌          | 50/383 [00:12<01:41,  3.28it/s]Epoch: 1, train for the 84-th batch, train loss: 0.43575018644332886:  71%|████████▍   | 84/119 [00:15<00:08,  4.18it/s]Epoch: 1, train for the 51-th batch, train loss: 0.38294219970703125:  13%|█▌          | 51/383 [00:12<01:31,  3.62it/s]Epoch: 1, train for the 80-th batch, train loss: 0.5418234467506409:  53%|██████▉      | 80/151 [00:15<00:20,  3.51it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5785596966743469:  62%|████████     | 91/146 [00:16<00:11,  4.81it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5785596966743469:  63%|████████▏    | 92/146 [00:16<00:13,  3.97it/s]Epoch: 1, train for the 84-th batch, train loss: 0.6008960008621216:  34%|████▍        | 83/241 [00:14<00:28,  5.57it/s]Epoch: 1, train for the 84-th batch, train loss: 0.6008960008621216:  35%|████▌        | 84/241 [00:14<00:27,  5.71it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5509918928146362:  31%|████         | 73/237 [00:14<00:39,  4.20it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5509918928146362:  31%|████         | 74/237 [00:14<00:36,  4.52it/s]Epoch: 1, train for the 85-th batch, train loss: 0.44898414611816406:  71%|████████▍   | 84/119 [00:15<00:08,  4.18it/s]Epoch: 1, train for the 85-th batch, train loss: 0.44898414611816406:  71%|████████▌   | 85/119 [00:15<00:07,  4.47it/s]Epoch: 1, train for the 93-th batch, train loss: 0.589268147945404:  63%|████████▊     | 92/146 [00:16<00:13,  3.97it/s]Epoch: 1, train for the 93-th batch, train loss: 0.589268147945404:  64%|████████▉     | 93/146 [00:16<00:11,  4.43it/s]Epoch: 1, train for the 75-th batch, train loss: 0.5450885891914368:  31%|████         | 74/237 [00:14<00:36,  4.52it/s]Epoch: 1, train for the 85-th batch, train loss: 0.46753019094467163:  35%|████▏       | 84/241 [00:14<00:27,  5.71it/s]Epoch: 1, train for the 85-th batch, train loss: 0.46753019094467163:  35%|████▏       | 85/241 [00:14<00:28,  5.49it/s]Epoch: 1, train for the 75-th batch, train loss: 0.5450885891914368:  32%|████         | 75/237 [00:14<00:33,  4.84it/s]Epoch: 1, train for the 86-th batch, train loss: 0.45831000804901123:  71%|████████▌   | 85/119 [00:16<00:07,  4.47it/s]Epoch: 1, train for the 86-th batch, train loss: 0.45831000804901123:  72%|████████▋   | 86/119 [00:16<00:06,  4.85it/s]Epoch: 1, train for the 81-th batch, train loss: 0.543804943561554:  53%|███████▍      | 80/151 [00:16<00:20,  3.51it/s]Epoch: 1, train for the 81-th batch, train loss: 0.543804943561554:  54%|███████▌      | 81/151 [00:16<00:22,  3.12it/s]Epoch: 1, train for the 52-th batch, train loss: 0.4494364857673645:  13%|█▋           | 51/383 [00:12<01:31,  3.62it/s]Epoch: 1, train for the 52-th batch, train loss: 0.4494364857673645:  14%|█▊           | 52/383 [00:12<01:49,  3.02it/s]Epoch: 1, train for the 82-th batch, train loss: 0.508859395980835:  54%|███████▌      | 81/151 [00:16<00:22,  3.12it/s]Epoch: 1, train for the 82-th batch, train loss: 0.508859395980835:  54%|███████▌      | 82/151 [00:16<00:18,  3.81it/s]Epoch: 1, train for the 86-th batch, train loss: 0.4811370074748993:  35%|████▌        | 85/241 [00:15<00:28,  5.49it/s]Epoch: 1, train for the 94-th batch, train loss: 0.5886793732643127:  64%|████████▎    | 93/146 [00:16<00:11,  4.43it/s]Epoch: 1, train for the 86-th batch, train loss: 0.4811370074748993:  36%|████▋        | 86/241 [00:15<00:31,  4.86it/s]Epoch: 1, train for the 76-th batch, train loss: 0.5501105189323425:  32%|████         | 75/237 [00:15<00:33,  4.84it/s]Epoch: 1, train for the 87-th batch, train loss: 0.4757779538631439:  72%|█████████▍   | 86/119 [00:16<00:06,  4.85it/s]Epoch: 1, train for the 94-th batch, train loss: 0.5886793732643127:  64%|████████▎    | 94/146 [00:16<00:13,  3.73it/s]Epoch: 1, train for the 87-th batch, train loss: 0.4757779538631439:  73%|█████████▌   | 87/119 [00:16<00:06,  4.72it/s]Epoch: 1, train for the 76-th batch, train loss: 0.5501105189323425:  32%|████▏        | 76/237 [00:15<00:36,  4.42it/s]Epoch: 1, train for the 53-th batch, train loss: 0.45620450377464294:  14%|█▋          | 52/383 [00:13<01:49,  3.02it/s]Epoch: 1, train for the 53-th batch, train loss: 0.45620450377464294:  14%|█▋          | 53/383 [00:13<01:32,  3.55it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5680813193321228:  64%|████████▎    | 94/146 [00:16<00:13,  3.73it/s]Epoch: 1, train for the 87-th batch, train loss: 0.4882994592189789:  36%|████▋        | 86/241 [00:15<00:31,  4.86it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5680813193321228:  65%|████████▍    | 95/146 [00:16<00:12,  4.18it/s]Epoch: 1, train for the 87-th batch, train loss: 0.4882994592189789:  36%|████▋        | 87/241 [00:15<00:31,  4.93it/s]Epoch: 1, train for the 83-th batch, train loss: 0.530322253704071:  54%|███████▌      | 82/151 [00:16<00:18,  3.81it/s]Epoch: 1, train for the 88-th batch, train loss: 0.501965343952179:  73%|██████████▏   | 87/119 [00:16<00:06,  4.72it/s]Epoch: 1, train for the 88-th batch, train loss: 0.501965343952179:  74%|██████████▎   | 88/119 [00:16<00:06,  4.88it/s]Epoch: 1, train for the 54-th batch, train loss: 0.3766075670719147:  14%|█▊           | 53/383 [00:13<01:32,  3.55it/s]Epoch: 1, train for the 83-th batch, train loss: 0.530322253704071:  55%|███████▋      | 83/151 [00:16<00:17,  3.96it/s]Epoch: 1, train for the 54-th batch, train loss: 0.3766075670719147:  14%|█▊           | 54/383 [00:13<01:20,  4.10it/s]Epoch: 1, train for the 77-th batch, train loss: 0.6068450808525085:  32%|████▏        | 76/237 [00:15<00:36,  4.42it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5347145795822144:  74%|█████████▌   | 88/119 [00:16<00:06,  4.88it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5347145795822144:  75%|█████████▋   | 89/119 [00:16<00:06,  4.60it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5508315563201904:  55%|███████▏     | 83/151 [00:16<00:17,  3.96it/s]Epoch: 1, train for the 77-th batch, train loss: 0.6068450808525085:  32%|████▏        | 77/237 [00:15<00:46,  3.47it/s]Epoch: 1, train for the 88-th batch, train loss: 0.4925878047943115:  36%|████▋        | 87/241 [00:15<00:31,  4.93it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5508315563201904:  56%|███████▏     | 84/151 [00:16<00:17,  3.92it/s]Epoch: 1, train for the 88-th batch, train loss: 0.4925878047943115:  37%|████▋        | 88/241 [00:15<00:34,  4.40it/s]Epoch: 1, train for the 55-th batch, train loss: 0.5368257761001587:  14%|█▊           | 54/383 [00:13<01:20,  4.10it/s]Epoch: 1, train for the 96-th batch, train loss: 0.5778264999389648:  65%|████████▍    | 95/146 [00:17<00:12,  4.18it/s]Epoch: 1, train for the 96-th batch, train loss: 0.5778264999389648:  66%|████████▌    | 96/146 [00:17<00:13,  3.81it/s]Epoch: 1, train for the 55-th batch, train loss: 0.5368257761001587:  14%|█▊           | 55/383 [00:13<01:24,  3.90it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5130617618560791:  75%|█████████▋   | 89/119 [00:16<00:06,  4.60it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5130617618560791:  76%|█████████▊   | 90/119 [00:16<00:05,  5.00it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5845947265625:  32%|█████▏          | 77/237 [00:15<00:46,  3.47it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5845947265625:  33%|█████▎          | 78/237 [00:15<00:40,  3.93it/s]Epoch: 1, train for the 97-th batch, train loss: 0.603276252746582:  66%|█████████▏    | 96/146 [00:17<00:13,  3.81it/s]Epoch: 1, train for the 89-th batch, train loss: 0.514370858669281:  37%|█████         | 88/241 [00:15<00:34,  4.40it/s]Epoch: 1, train for the 97-th batch, train loss: 0.603276252746582:  66%|█████████▎    | 97/146 [00:17<00:11,  4.33it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5435827970504761:  56%|███████▏     | 84/151 [00:16<00:17,  3.92it/s]Epoch: 1, train for the 89-th batch, train loss: 0.514370858669281:  37%|█████▏        | 89/241 [00:15<00:32,  4.63it/s]Epoch: 1, train for the 56-th batch, train loss: 0.49357596039772034:  14%|█▋          | 55/383 [00:13<01:24,  3.90it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5435827970504761:  56%|███████▎     | 85/151 [00:16<00:15,  4.14it/s]Epoch: 1, train for the 56-th batch, train loss: 0.49357596039772034:  15%|█▊          | 56/383 [00:13<01:16,  4.29it/s]Epoch: 1, train for the 91-th batch, train loss: 0.4722071588039398:  76%|█████████▊   | 90/119 [00:17<00:05,  5.00it/s]Epoch: 1, train for the 91-th batch, train loss: 0.4722071588039398:  76%|█████████▉   | 91/119 [00:17<00:05,  5.55it/s]Epoch: 1, train for the 98-th batch, train loss: 0.5889991521835327:  66%|████████▋    | 97/146 [00:17<00:11,  4.33it/s]Epoch: 1, train for the 98-th batch, train loss: 0.5889991521835327:  67%|████████▋    | 98/146 [00:17<00:09,  4.90it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5651398301124573:  33%|████▎        | 78/237 [00:16<00:40,  3.93it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5651398301124573:  33%|████▎        | 79/237 [00:16<00:37,  4.23it/s]Epoch: 1, train for the 92-th batch, train loss: 0.4681883156299591:  76%|█████████▉   | 91/119 [00:17<00:05,  5.55it/s]Epoch: 1, train for the 92-th batch, train loss: 0.4681883156299591:  77%|██████████   | 92/119 [00:17<00:04,  6.06it/s]Epoch: 1, train for the 99-th batch, train loss: 0.6033099293708801:  67%|████████▋    | 98/146 [00:17<00:09,  4.90it/s]Epoch: 1, train for the 99-th batch, train loss: 0.6033099293708801:  68%|████████▊    | 99/146 [00:17<00:10,  4.64it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5270724296569824:  56%|███████▎     | 85/151 [00:17<00:15,  4.14it/s]Epoch: 1, train for the 90-th batch, train loss: 0.47210127115249634:  37%|████▍       | 89/241 [00:16<00:32,  4.63it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5270724296569824:  57%|███████▍     | 86/151 [00:17<00:18,  3.44it/s]Epoch: 1, train for the 90-th batch, train loss: 0.47210127115249634:  37%|████▍       | 90/241 [00:16<00:42,  3.58it/s]Epoch: 1, train for the 57-th batch, train loss: 0.38134074211120605:  15%|█▊          | 56/383 [00:14<01:16,  4.29it/s]Epoch: 1, train for the 93-th batch, train loss: 0.41975951194763184:  77%|█████████▎  | 92/119 [00:17<00:04,  6.06it/s]Epoch: 1, train for the 100-th batch, train loss: 0.6226054430007935:  68%|████████▏   | 99/146 [00:17<00:10,  4.64it/s]Epoch: 1, train for the 80-th batch, train loss: 0.5827493667602539:  33%|████▎        | 79/237 [00:16<00:37,  4.23it/s]Epoch: 1, train for the 93-th batch, train loss: 0.41975951194763184:  78%|█████████▍  | 93/119 [00:17<00:05,  4.71it/s]Epoch: 1, train for the 57-th batch, train loss: 0.38134074211120605:  15%|█▊          | 57/383 [00:14<01:42,  3.18it/s]Epoch: 1, train for the 100-th batch, train loss: 0.6226054430007935:  68%|███████▌   | 100/146 [00:17<00:09,  4.97it/s]Epoch: 1, train for the 80-th batch, train loss: 0.5827493667602539:  34%|████▍        | 80/237 [00:16<00:44,  3.53it/s]Epoch: 1, train for the 87-th batch, train loss: 0.5237582921981812:  57%|███████▍     | 86/151 [00:17<00:18,  3.44it/s]Epoch: 1, train for the 91-th batch, train loss: 0.4822930097579956:  37%|████▊        | 90/241 [00:16<00:42,  3.58it/s]Epoch: 1, train for the 87-th batch, train loss: 0.5237582921981812:  58%|███████▍     | 87/151 [00:17<00:16,  3.81it/s]Epoch: 1, train for the 91-th batch, train loss: 0.4822930097579956:  38%|████▉        | 91/241 [00:16<00:38,  3.94it/s]Epoch: 1, train for the 94-th batch, train loss: 0.4900251626968384:  78%|██████████▏  | 93/119 [00:17<00:05,  4.71it/s]Epoch: 1, train for the 94-th batch, train loss: 0.4900251626968384:  79%|██████████▎  | 94/119 [00:17<00:04,  5.04it/s]Epoch: 1, train for the 58-th batch, train loss: 0.34027746319770813:  15%|█▊          | 57/383 [00:14<01:42,  3.18it/s]Epoch: 1, train for the 58-th batch, train loss: 0.34027746319770813:  15%|█▊          | 58/383 [00:14<01:30,  3.58it/s]Epoch: 1, train for the 88-th batch, train loss: 0.5701794624328613:  58%|███████▍     | 87/151 [00:17<00:16,  3.81it/s]Epoch: 1, train for the 88-th batch, train loss: 0.5701794624328613:  58%|███████▌     | 88/151 [00:17<00:14,  4.45it/s]Epoch: 1, train for the 92-th batch, train loss: 0.3921884596347809:  38%|████▉        | 91/241 [00:16<00:38,  3.94it/s]Epoch: 1, train for the 92-th batch, train loss: 0.3921884596347809:  38%|████▉        | 92/241 [00:16<00:40,  3.67it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5747994184494019:  68%|███████▌   | 100/146 [00:18<00:09,  4.97it/s]Epoch: 1, train for the 81-th batch, train loss: 0.6107048392295837:  34%|████▍        | 80/237 [00:16<00:44,  3.53it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5747994184494019:  69%|███████▌   | 101/146 [00:18<00:12,  3.53it/s]Epoch: 1, train for the 81-th batch, train loss: 0.6107048392295837:  34%|████▍        | 81/237 [00:16<00:53,  2.92it/s]Epoch: 1, train for the 95-th batch, train loss: 0.4445513188838959:  79%|██████████▎  | 94/119 [00:18<00:04,  5.04it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5424902439117432:  58%|███████▌     | 88/151 [00:17<00:14,  4.45it/s]Epoch: 1, train for the 95-th batch, train loss: 0.4445513188838959:  80%|██████████▍  | 95/119 [00:18<00:05,  4.11it/s]Epoch: 1, train for the 93-th batch, train loss: 0.43825408816337585:  38%|████▌       | 92/241 [00:16<00:40,  3.67it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5424902439117432:  59%|███████▋     | 89/151 [00:18<00:15,  4.03it/s]Epoch: 1, train for the 59-th batch, train loss: 0.38970673084259033:  15%|█▊          | 58/383 [00:14<01:30,  3.58it/s]Epoch: 1, train for the 93-th batch, train loss: 0.43825408816337585:  39%|████▋       | 93/241 [00:16<00:34,  4.33it/s]Epoch: 1, train for the 59-th batch, train loss: 0.38970673084259033:  15%|█▊          | 59/383 [00:14<01:37,  3.31it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5616050362586975:  69%|███████▌   | 101/146 [00:18<00:12,  3.53it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5616050362586975:  70%|███████▋   | 102/146 [00:18<00:10,  4.19it/s]Epoch: 1, train for the 82-th batch, train loss: 0.6125146150588989:  34%|████▍        | 81/237 [00:17<00:53,  2.92it/s]Epoch: 1, train for the 82-th batch, train loss: 0.6125146150588989:  35%|████▍        | 82/237 [00:17<00:44,  3.48it/s]Epoch: 1, train for the 96-th batch, train loss: 0.4291516840457916:  80%|██████████▍  | 95/119 [00:18<00:05,  4.11it/s]Epoch: 1, train for the 96-th batch, train loss: 0.4291516840457916:  81%|██████████▍  | 96/119 [00:18<00:05,  4.51it/s]Epoch: 1, train for the 94-th batch, train loss: 0.3724001348018646:  39%|█████        | 93/241 [00:17<00:34,  4.33it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5517842769622803:  59%|███████▋     | 89/151 [00:18<00:15,  4.03it/s]Epoch: 1, train for the 94-th batch, train loss: 0.3724001348018646:  39%|█████        | 94/241 [00:17<00:31,  4.61it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5517842769622803:  60%|███████▋     | 90/151 [00:18<00:14,  4.22it/s]Epoch: 1, train for the 60-th batch, train loss: 0.3314831852912903:  15%|██           | 59/383 [00:15<01:37,  3.31it/s]Epoch: 1, train for the 60-th batch, train loss: 0.3314831852912903:  16%|██           | 60/383 [00:15<01:29,  3.62it/s]Epoch: 1, train for the 97-th batch, train loss: 0.4626160264015198:  81%|██████████▍  | 96/119 [00:18<00:05,  4.51it/s]Epoch: 1, train for the 97-th batch, train loss: 0.4626160264015198:  82%|██████████▌  | 97/119 [00:18<00:04,  4.95it/s]Epoch: 1, train for the 95-th batch, train loss: 0.4433702826499939:  39%|█████        | 94/241 [00:17<00:31,  4.61it/s]Epoch: 1, train for the 95-th batch, train loss: 0.4433702826499939:  39%|█████        | 95/241 [00:17<00:28,  5.10it/s]Epoch: 1, train for the 61-th batch, train loss: 0.35563230514526367:  16%|█▉          | 60/383 [00:15<01:29,  3.62it/s]Epoch: 1, train for the 61-th batch, train loss: 0.35563230514526367:  16%|█▉          | 61/383 [00:15<01:17,  4.17it/s]Epoch: 1, train for the 98-th batch, train loss: 0.4068056344985962:  82%|██████████▌  | 97/119 [00:18<00:04,  4.95it/s]Epoch: 1, train for the 98-th batch, train loss: 0.4068056344985962:  82%|██████████▋  | 98/119 [00:18<00:03,  5.59it/s]Epoch: 1, train for the 103-th batch, train loss: 0.5794193148612976:  70%|███████▋   | 102/146 [00:18<00:10,  4.19it/s]Epoch: 1, train for the 103-th batch, train loss: 0.5794193148612976:  71%|███████▊   | 103/146 [00:18<00:13,  3.22it/s]Epoch: 1, train for the 99-th batch, train loss: 0.4507491886615753:  82%|██████████▋  | 98/119 [00:18<00:03,  5.59it/s]Epoch: 1, train for the 99-th batch, train loss: 0.4507491886615753:  83%|██████████▊  | 99/119 [00:18<00:03,  5.95it/s]Epoch: 1, train for the 96-th batch, train loss: 0.5554461479187012:  39%|█████        | 95/241 [00:17<00:28,  5.10it/s]Epoch: 1, train for the 91-th batch, train loss: 0.49287474155426025:  60%|███████▏    | 90/151 [00:18<00:14,  4.22it/s]Epoch: 1, train for the 91-th batch, train loss: 0.49287474155426025:  60%|███████▏    | 91/151 [00:18<00:17,  3.47it/s]Epoch: 1, train for the 96-th batch, train loss: 0.5554461479187012:  40%|█████▏       | 96/241 [00:17<00:31,  4.53it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5858525037765503:  35%|████▍        | 82/237 [00:17<00:44,  3.48it/s]Epoch: 1, train for the 62-th batch, train loss: 0.495883971452713:  16%|██▏           | 61/383 [00:15<01:17,  4.17it/s]Epoch: 1, train for the 62-th batch, train loss: 0.495883971452713:  16%|██▎           | 62/383 [00:15<01:20,  3.99it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5858525037765503:  35%|████▌        | 83/237 [00:17<00:55,  2.75it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5876702666282654:  71%|███████▊   | 103/146 [00:19<00:13,  3.22it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5876702666282654:  71%|███████▊   | 104/146 [00:19<00:10,  3.84it/s]Epoch: 1, train for the 105-th batch, train loss: 0.6115843057632446:  71%|███████▊   | 104/146 [00:19<00:10,  3.84it/s]Epoch: 1, train for the 105-th batch, train loss: 0.6115843057632446:  72%|███████▉   | 105/146 [00:19<00:08,  4.56it/s]Epoch: 1, train for the 84-th batch, train loss: 0.6135427951812744:  35%|████▌        | 83/237 [00:17<00:55,  2.75it/s]Epoch: 1, train for the 84-th batch, train loss: 0.6135427951812744:  35%|████▌        | 84/237 [00:17<00:47,  3.21it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5894993543624878:  72%|███████▉   | 105/146 [00:19<00:08,  4.56it/s]Epoch: 1, train for the 97-th batch, train loss: 0.3587661683559418:  40%|█████▏       | 96/241 [00:17<00:31,  4.53it/s]Epoch: 1, train for the 92-th batch, train loss: 0.544192373752594:  60%|████████▍     | 91/151 [00:19<00:17,  3.47it/s]Epoch: 1, train for the 63-th batch, train loss: 0.4177400469779968:  16%|██           | 62/383 [00:15<01:20,  3.99it/s]Epoch: 1, train for the 97-th batch, train loss: 0.3587661683559418:  40%|█████▏       | 97/241 [00:17<00:39,  3.67it/s]Epoch: 1, train for the 92-th batch, train loss: 0.544192373752594:  61%|████████▌     | 92/151 [00:19<00:19,  3.09it/s]Epoch: 1, train for the 63-th batch, train loss: 0.4177400469779968:  16%|██▏          | 63/383 [00:15<01:30,  3.52it/s]Epoch: 1, train for the 100-th batch, train loss: 0.4911845922470093:  83%|█████████▉  | 99/119 [00:19<00:03,  5.95it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5376613736152649:  35%|████▌        | 84/237 [00:18<00:47,  3.21it/s]Epoch: 1, train for the 100-th batch, train loss: 0.4911845922470093:  84%|█████████▏ | 100/119 [00:19<00:05,  3.56it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5376613736152649:  36%|████▋        | 85/237 [00:18<00:45,  3.35it/s]Epoch: 1, train for the 107-th batch, train loss: 0.6032831072807312:  72%|███████▉   | 105/146 [00:19<00:08,  4.56it/s]Epoch: 1, train for the 107-th batch, train loss: 0.6032831072807312:  73%|████████   | 107/146 [00:19<00:07,  5.08it/s]Epoch: 1, train for the 98-th batch, train loss: 0.4870249927043915:  40%|█████▏       | 97/241 [00:18<00:39,  3.67it/s]Epoch: 1, train for the 98-th batch, train loss: 0.4870249927043915:  41%|█████▎       | 98/241 [00:18<00:34,  4.09it/s]Epoch: 1, train for the 64-th batch, train loss: 0.4227665662765503:  16%|██▏          | 63/383 [00:15<01:30,  3.52it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5544096231460571:  61%|███████▉     | 92/151 [00:19<00:19,  3.09it/s]Epoch: 1, train for the 64-th batch, train loss: 0.4227665662765503:  17%|██▏          | 64/383 [00:16<01:21,  3.93it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5544096231460571:  62%|████████     | 93/151 [00:19<00:16,  3.50it/s]Epoch: 1, train for the 101-th batch, train loss: 0.48590967059135437:  84%|████████▍ | 100/119 [00:19<00:05,  3.56it/s]Epoch: 1, train for the 101-th batch, train loss: 0.48590967059135437:  85%|████████▍ | 101/119 [00:19<00:04,  4.17it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5875311493873596:  36%|████▋        | 85/237 [00:18<00:45,  3.35it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5875311493873596:  36%|████▋        | 86/237 [00:18<00:38,  3.90it/s]Epoch: 1, train for the 108-th batch, train loss: 0.6112731099128723:  73%|████████   | 107/146 [00:19<00:07,  5.08it/s]Epoch: 1, train for the 108-th batch, train loss: 0.6112731099128723:  74%|████████▏  | 108/146 [00:19<00:06,  5.52it/s]Epoch: 1, train for the 99-th batch, train loss: 0.2971651256084442:  41%|█████▎       | 98/241 [00:18<00:34,  4.09it/s]Epoch: 1, train for the 99-th batch, train loss: 0.2971651256084442:  41%|█████▎       | 99/241 [00:18<00:30,  4.73it/s]Epoch: 1, train for the 94-th batch, train loss: 0.5413703918457031:  62%|████████     | 93/151 [00:19<00:16,  3.50it/s]Epoch: 1, train for the 94-th batch, train loss: 0.5413703918457031:  62%|████████     | 94/151 [00:19<00:14,  4.04it/s]Epoch: 1, train for the 102-th batch, train loss: 0.49013784527778625:  85%|████████▍ | 101/119 [00:19<00:04,  4.17it/s]Epoch: 1, train for the 102-th batch, train loss: 0.49013784527778625:  86%|████████▌ | 102/119 [00:19<00:03,  4.60it/s]Epoch: 1, train for the 109-th batch, train loss: 0.5635364651679993:  74%|████████▏  | 108/146 [00:19<00:06,  5.52it/s]Epoch: 1, train for the 109-th batch, train loss: 0.5635364651679993:  75%|████████▏  | 109/146 [00:19<00:06,  5.66it/s]Epoch: 1, train for the 65-th batch, train loss: 0.34475943446159363:  17%|██          | 64/383 [00:16<01:21,  3.93it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5364530086517334:  62%|████████     | 94/151 [00:19<00:14,  4.04it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5364530086517334:  63%|████████▏    | 95/151 [00:19<00:13,  4.22it/s]Epoch: 1, train for the 103-th batch, train loss: 0.45559918880462646:  86%|████████▌ | 102/119 [00:19<00:03,  4.60it/s]Epoch: 1, train for the 65-th batch, train loss: 0.34475943446159363:  17%|██          | 65/383 [00:16<01:32,  3.43it/s]Epoch: 1, train for the 103-th batch, train loss: 0.45559918880462646:  87%|████████▋ | 103/119 [00:19<00:03,  4.94it/s]Epoch: 1, train for the 110-th batch, train loss: 0.5613798499107361:  75%|████████▏  | 109/146 [00:19<00:06,  5.66it/s]Epoch: 1, train for the 110-th batch, train loss: 0.5613798499107361:  75%|████████▎  | 110/146 [00:20<00:06,  5.68it/s]Epoch: 1, train for the 87-th batch, train loss: 0.5789452791213989:  36%|████▋        | 86/237 [00:18<00:38,  3.90it/s]Epoch: 1, train for the 87-th batch, train loss: 0.5789452791213989:  37%|████▊        | 87/237 [00:18<00:47,  3.14it/s]Epoch: 1, train for the 96-th batch, train loss: 0.5381888747215271:  63%|████████▏    | 95/151 [00:19<00:13,  4.22it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5119286179542542:  41%|████▉       | 99/241 [00:18<00:30,  4.73it/s]Epoch: 1, train for the 66-th batch, train loss: 0.42386144399642944:  17%|██          | 65/383 [00:16<01:32,  3.43it/s]Epoch: 1, train for the 96-th batch, train loss: 0.5381888747215271:  64%|████████▎    | 96/151 [00:19<00:12,  4.55it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5119286179542542:  41%|████▌      | 100/241 [00:18<00:39,  3.53it/s]Epoch: 1, train for the 66-th batch, train loss: 0.42386144399642944:  17%|██          | 66/383 [00:16<01:22,  3.83it/s]Epoch: 1, train for the 104-th batch, train loss: 0.46218016743659973:  87%|████████▋ | 103/119 [00:19<00:03,  4.94it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5625157952308655:  75%|████████▎  | 110/146 [00:20<00:06,  5.68it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5625157952308655:  76%|████████▎  | 111/146 [00:20<00:06,  5.58it/s]Epoch: 1, train for the 104-th batch, train loss: 0.46218016743659973:  87%|████████▋ | 104/119 [00:19<00:03,  4.71it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5340099334716797:  64%|████████▎    | 96/151 [00:19<00:12,  4.55it/s]Epoch: 1, train for the 88-th batch, train loss: 0.6163355112075806:  37%|████▊        | 87/237 [00:18<00:47,  3.14it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5340099334716797:  64%|████████▎    | 97/151 [00:19<00:11,  4.58it/s]Epoch: 1, train for the 67-th batch, train loss: 0.5246101021766663:  17%|██▏          | 66/383 [00:16<01:22,  3.83it/s]Epoch: 1, train for the 88-th batch, train loss: 0.6163355112075806:  37%|████▊        | 88/237 [00:18<00:44,  3.37it/s]Epoch: 1, train for the 67-th batch, train loss: 0.5246101021766663:  17%|██▎          | 67/383 [00:16<01:19,  3.99it/s]Epoch: 1, train for the 68-th batch, train loss: 0.39049941301345825:  17%|██          | 67/383 [00:16<01:19,  3.99it/s]Epoch: 1, train for the 68-th batch, train loss: 0.39049941301345825:  18%|██▏         | 68/383 [00:16<01:08,  4.61it/s]Epoch: 1, train for the 101-th batch, train loss: 0.48302802443504333:  41%|████▏     | 100/241 [00:19<00:39,  3.53it/s]Epoch: 1, train for the 98-th batch, train loss: 0.5236606001853943:  64%|████████▎    | 97/151 [00:20<00:11,  4.58it/s]Epoch: 1, train for the 101-th batch, train loss: 0.48302802443504333:  42%|████▏     | 101/241 [00:19<00:48,  2.91it/s]Epoch: 1, train for the 98-th batch, train loss: 0.5236606001853943:  65%|████████▍    | 98/151 [00:20<00:12,  4.16it/s]Epoch: 1, train for the 112-th batch, train loss: 0.633184552192688:  76%|█████████   | 111/146 [00:20<00:06,  5.58it/s]Epoch: 1, train for the 112-th batch, train loss: 0.633184552192688:  77%|█████████▏  | 112/146 [00:20<00:08,  3.88it/s]Epoch: 1, train for the 105-th batch, train loss: 0.4909636676311493:  87%|█████████▌ | 104/119 [00:20<00:03,  4.71it/s]Epoch: 1, train for the 105-th batch, train loss: 0.4909636676311493:  88%|█████████▋ | 105/119 [00:20<00:04,  3.45it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5673263669013977:  37%|████▊        | 88/237 [00:19<00:44,  3.37it/s]Epoch: 1, train for the 69-th batch, train loss: 0.33816230297088623:  18%|██▏         | 68/383 [00:17<01:08,  4.61it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5673263669013977:  38%|████▉        | 89/237 [00:19<00:47,  3.09it/s]Epoch: 1, train for the 69-th batch, train loss: 0.33816230297088623:  18%|██▏         | 69/383 [00:17<01:10,  4.47it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5711246132850647:  42%|████▌      | 101/241 [00:19<00:48,  2.91it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5711246132850647:  42%|████▋      | 102/241 [00:19<00:40,  3.47it/s]Epoch: 1, train for the 99-th batch, train loss: 0.529651939868927:  65%|█████████     | 98/151 [00:20<00:12,  4.16it/s]Epoch: 1, train for the 113-th batch, train loss: 0.5499279499053955:  77%|████████▍  | 112/146 [00:20<00:08,  3.88it/s]Epoch: 1, train for the 99-th batch, train loss: 0.529651939868927:  66%|█████████▏    | 99/151 [00:20<00:11,  4.51it/s]Epoch: 1, train for the 113-th batch, train loss: 0.5499279499053955:  77%|████████▌  | 113/146 [00:20<00:07,  4.32it/s]Epoch: 1, train for the 106-th batch, train loss: 0.43311917781829834:  88%|████████▊ | 105/119 [00:20<00:04,  3.45it/s]Epoch: 1, train for the 106-th batch, train loss: 0.43311917781829834:  89%|████████▉ | 106/119 [00:20<00:03,  3.94it/s]Epoch: 1, train for the 103-th batch, train loss: 0.5146062970161438:  42%|████▋      | 102/241 [00:19<00:40,  3.47it/s]Epoch: 1, train for the 103-th batch, train loss: 0.5146062970161438:  43%|████▋      | 103/241 [00:19<00:34,  4.01it/s]Epoch: 1, train for the 107-th batch, train loss: 0.4979715645313263:  89%|█████████▊ | 106/119 [00:20<00:03,  3.94it/s]Epoch: 1, train for the 107-th batch, train loss: 0.4979715645313263:  90%|█████████▉ | 107/119 [00:20<00:02,  4.52it/s]Epoch: 1, train for the 70-th batch, train loss: 0.5007084012031555:  18%|██▎          | 69/383 [00:17<01:10,  4.47it/s]Epoch: 1, train for the 90-th batch, train loss: 0.6021789312362671:  38%|████▉        | 89/237 [00:19<00:47,  3.09it/s]Epoch: 1, train for the 70-th batch, train loss: 0.5007084012031555:  18%|██▍          | 70/383 [00:17<01:16,  4.11it/s]Epoch: 1, train for the 90-th batch, train loss: 0.6021789312362671:  38%|████▉        | 90/237 [00:19<00:46,  3.17it/s]Epoch: 1, train for the 71-th batch, train loss: 0.4645344913005829:  18%|██▍          | 70/383 [00:17<01:16,  4.11it/s]Epoch: 1, train for the 114-th batch, train loss: 0.5889068245887756:  77%|████████▌  | 113/146 [00:21<00:07,  4.32it/s]Epoch: 1, train for the 114-th batch, train loss: 0.5889068245887756:  78%|████████▌  | 114/146 [00:21<00:09,  3.49it/s]Epoch: 1, train for the 108-th batch, train loss: 0.3886765241622925:  90%|█████████▉ | 107/119 [00:20<00:02,  4.52it/s]Epoch: 1, train for the 71-th batch, train loss: 0.4645344913005829:  19%|██▍          | 71/383 [00:17<01:12,  4.32it/s]Epoch: 1, train for the 108-th batch, train loss: 0.3886765241622925:  91%|█████████▉ | 108/119 [00:20<00:02,  4.27it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5570735931396484:  38%|████▉        | 90/237 [00:19<00:46,  3.17it/s]Epoch: 1, train for the 104-th batch, train loss: 0.511044979095459:  43%|█████▏      | 103/241 [00:19<00:34,  4.01it/s]Epoch: 1, train for the 100-th batch, train loss: 0.6127949953079224:  66%|███████▊    | 99/151 [00:20<00:11,  4.51it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5570735931396484:  38%|████▉        | 91/237 [00:19<00:42,  3.44it/s]Epoch: 1, train for the 104-th batch, train loss: 0.511044979095459:  43%|█████▏      | 104/241 [00:19<00:37,  3.63it/s]Epoch: 1, train for the 100-th batch, train loss: 0.6127949953079224:  66%|███████▎   | 100/151 [00:20<00:15,  3.32it/s]Epoch: 1, train for the 115-th batch, train loss: 0.6282766461372375:  78%|████████▌  | 114/146 [00:21<00:09,  3.49it/s]Epoch: 1, train for the 115-th batch, train loss: 0.6282766461372375:  79%|████████▋  | 115/146 [00:21<00:07,  3.94it/s]Epoch: 1, train for the 72-th batch, train loss: 0.4494311511516571:  19%|██▍          | 71/383 [00:17<01:12,  4.32it/s]Epoch: 1, train for the 109-th batch, train loss: 0.45146986842155457:  91%|█████████ | 108/119 [00:21<00:02,  4.27it/s]Epoch: 1, train for the 109-th batch, train loss: 0.45146986842155457:  92%|█████████▏| 109/119 [00:21<00:02,  4.52it/s]Epoch: 1, train for the 72-th batch, train loss: 0.4494311511516571:  19%|██▍          | 72/383 [00:17<01:10,  4.43it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5436567664146423:  38%|████▉        | 91/237 [00:20<00:42,  3.44it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5436567664146423:  39%|█████        | 92/237 [00:20<00:38,  3.76it/s]Epoch: 1, train for the 73-th batch, train loss: 0.3533838987350464:  19%|██▍          | 72/383 [00:18<01:10,  4.43it/s]Epoch: 1, train for the 105-th batch, train loss: 0.4686054587364197:  43%|████▋      | 104/241 [00:20<00:37,  3.63it/s]Epoch: 1, train for the 73-th batch, train loss: 0.3533838987350464:  19%|██▍          | 73/383 [00:18<01:08,  4.53it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5334507822990417:  39%|█████        | 92/237 [00:20<00:38,  3.76it/s]Epoch: 1, train for the 105-th batch, train loss: 0.4686054587364197:  44%|████▊      | 105/241 [00:20<00:42,  3.19it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5334507822990417:  39%|█████        | 93/237 [00:20<00:35,  4.02it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5791015028953552:  66%|███████▎   | 100/151 [00:21<00:15,  3.32it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5318909883499146:  44%|████▊      | 105/241 [00:20<00:42,  3.19it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5791015028953552:  67%|███████▎   | 101/151 [00:21<00:18,  2.77it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5318909883499146:  44%|████▊      | 106/241 [00:20<00:34,  3.89it/s]Epoch: 1, train for the 116-th batch, train loss: 0.5833365321159363:  79%|████████▋  | 115/146 [00:21<00:07,  3.94it/s]Epoch: 1, train for the 116-th batch, train loss: 0.5833365321159363:  79%|████████▋  | 116/146 [00:21<00:08,  3.34it/s]Epoch: 1, train for the 110-th batch, train loss: 0.427356481552124:  92%|██████████▉ | 109/119 [00:21<00:02,  4.52it/s]Epoch: 1, train for the 110-th batch, train loss: 0.427356481552124:  92%|███████████ | 110/119 [00:21<00:02,  3.64it/s]Epoch: 1, train for the 94-th batch, train loss: 0.5760133266448975:  39%|█████        | 93/237 [00:20<00:35,  4.02it/s]Epoch: 1, train for the 74-th batch, train loss: 0.25356701016426086:  19%|██▎         | 73/383 [00:18<01:08,  4.53it/s]Epoch: 1, train for the 94-th batch, train loss: 0.5760133266448975:  40%|█████▏       | 94/237 [00:20<00:33,  4.25it/s]Epoch: 1, train for the 74-th batch, train loss: 0.25356701016426086:  19%|██▎         | 74/383 [00:18<01:11,  4.35it/s]Epoch: 1, train for the 107-th batch, train loss: 0.5756335854530334:  44%|████▊      | 106/241 [00:20<00:34,  3.89it/s]Epoch: 1, train for the 117-th batch, train loss: 0.6148477792739868:  79%|████████▋  | 116/146 [00:21<00:08,  3.34it/s]Epoch: 1, train for the 117-th batch, train loss: 0.6148477792739868:  80%|████████▊  | 117/146 [00:21<00:07,  3.84it/s]Epoch: 1, train for the 107-th batch, train loss: 0.5756335854530334:  44%|████▉      | 107/241 [00:20<00:31,  4.25it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5527211427688599:  67%|███████▎   | 101/151 [00:21<00:18,  2.77it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5130258798599243:  92%|██████████▏| 110/119 [00:21<00:02,  3.64it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5527211427688599:  68%|███████▍   | 102/151 [00:21<00:15,  3.17it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5130258798599243:  93%|██████████▎| 111/119 [00:21<00:01,  4.04it/s]Epoch: 1, train for the 108-th batch, train loss: 0.48669156432151794:  44%|████▍     | 107/241 [00:20<00:31,  4.25it/s]Epoch: 1, train for the 108-th batch, train loss: 0.48669156432151794:  45%|████▍     | 108/241 [00:20<00:29,  4.55it/s]Epoch: 1, train for the 103-th batch, train loss: 0.5885857343673706:  68%|███████▍   | 102/151 [00:21<00:15,  3.17it/s]Epoch: 1, train for the 103-th batch, train loss: 0.5885857343673706:  68%|███████▌   | 103/151 [00:21<00:13,  3.58it/s]Epoch: 1, train for the 75-th batch, train loss: 0.3031395971775055:  19%|██▌          | 74/383 [00:18<01:11,  4.35it/s]Epoch: 1, train for the 75-th batch, train loss: 0.3031395971775055:  20%|██▌          | 75/383 [00:18<01:21,  3.77it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5159429311752319:  40%|█████▏       | 94/237 [00:20<00:33,  4.25it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5159429311752319:  40%|█████▏       | 95/237 [00:20<00:40,  3.50it/s]Epoch: 1, train for the 109-th batch, train loss: 0.6880326867103577:  45%|████▉      | 108/241 [00:20<00:29,  4.55it/s]Epoch: 1, train for the 109-th batch, train loss: 0.6880326867103577:  45%|████▉      | 109/241 [00:20<00:25,  5.09it/s]Epoch: 1, train for the 118-th batch, train loss: 0.5780699253082275:  80%|████████▊  | 117/146 [00:22<00:07,  3.84it/s]Epoch: 1, train for the 112-th batch, train loss: 0.4852631688117981:  93%|██████████▎| 111/119 [00:22<00:01,  4.04it/s]Epoch: 1, train for the 118-th batch, train loss: 0.5780699253082275:  81%|████████▉  | 118/146 [00:22<00:08,  3.46it/s]Epoch: 1, train for the 112-th batch, train loss: 0.4852631688117981:  94%|██████████▎| 112/119 [00:22<00:01,  3.64it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5643149614334106:  68%|███████▌   | 103/151 [00:22<00:13,  3.58it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5643149614334106:  69%|███████▌   | 104/151 [00:22<00:12,  3.73it/s]Epoch: 1, train for the 76-th batch, train loss: 0.3950619697570801:  20%|██▌          | 75/383 [00:18<01:21,  3.77it/s]Epoch: 1, train for the 110-th batch, train loss: 0.6133682727813721:  45%|████▉      | 109/241 [00:21<00:25,  5.09it/s]Epoch: 1, train for the 110-th batch, train loss: 0.6133682727813721:  46%|█████      | 110/241 [00:21<00:24,  5.40it/s]Epoch: 1, train for the 76-th batch, train loss: 0.3950619697570801:  20%|██▌          | 76/383 [00:18<01:17,  3.96it/s]Epoch: 1, train for the 96-th batch, train loss: 0.5347106456756592:  40%|█████▏       | 95/237 [00:21<00:40,  3.50it/s]Epoch: 1, train for the 96-th batch, train loss: 0.5347106456756592:  41%|█████▎       | 96/237 [00:21<00:37,  3.77it/s]Epoch: 1, train for the 119-th batch, train loss: 0.5880880355834961:  81%|████████▉  | 118/146 [00:22<00:08,  3.46it/s]Epoch: 1, train for the 113-th batch, train loss: 0.41959503293037415:  94%|█████████▍| 112/119 [00:22<00:01,  3.64it/s]Epoch: 1, train for the 119-th batch, train loss: 0.5880880355834961:  82%|████████▉  | 119/146 [00:22<00:07,  3.85it/s]Epoch: 1, train for the 113-th batch, train loss: 0.41959503293037415:  95%|█████████▍| 113/119 [00:22<00:01,  4.05it/s]Epoch: 1, train for the 114-th batch, train loss: 0.45696982741355896:  95%|█████████▍| 113/119 [00:22<00:01,  4.05it/s]Epoch: 1, train for the 114-th batch, train loss: 0.45696982741355896:  96%|█████████▌| 114/119 [00:22<00:01,  4.66it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5678115487098694:  82%|████████▉  | 119/146 [00:22<00:07,  3.85it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5678115487098694:  82%|█████████  | 120/146 [00:22<00:07,  3.67it/s]Epoch: 1, train for the 77-th batch, train loss: 0.4124302268028259:  20%|██▌          | 76/383 [00:19<01:17,  3.96it/s]Epoch: 1, train for the 77-th batch, train loss: 0.4124302268028259:  20%|██▌          | 77/383 [00:19<01:29,  3.42it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5423153638839722:  41%|█████▎       | 96/237 [00:21<00:37,  3.77it/s]Epoch: 1, train for the 111-th batch, train loss: 0.23588813841342926:  46%|████▌     | 110/241 [00:21<00:24,  5.40it/s]Epoch: 1, train for the 111-th batch, train loss: 0.23588813841342926:  46%|████▌     | 111/241 [00:21<00:35,  3.66it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5423153638839722:  41%|█████▎       | 97/237 [00:21<00:44,  3.16it/s]Epoch: 1, train for the 115-th batch, train loss: 0.4257872700691223:  96%|██████████▌| 114/119 [00:22<00:01,  4.66it/s]Epoch: 1, train for the 121-th batch, train loss: 0.588660717010498:  82%|█████████▊  | 120/146 [00:22<00:07,  3.67it/s]Epoch: 1, train for the 105-th batch, train loss: 0.5347071886062622:  69%|███████▌   | 104/151 [00:22<00:12,  3.73it/s]Epoch: 1, train for the 115-th batch, train loss: 0.4257872700691223:  97%|██████████▋| 115/119 [00:22<00:00,  4.16it/s]Epoch: 1, train for the 121-th batch, train loss: 0.588660717010498:  83%|█████████▉  | 121/146 [00:22<00:05,  4.27it/s]Epoch: 1, train for the 105-th batch, train loss: 0.5347071886062622:  70%|███████▋   | 105/151 [00:22<00:16,  2.82it/s]Epoch: 1, train for the 78-th batch, train loss: 0.4376637041568756:  20%|██▌          | 77/383 [00:19<01:29,  3.42it/s]Epoch: 1, train for the 78-th batch, train loss: 0.4376637041568756:  20%|██▋          | 78/383 [00:19<01:18,  3.90it/s]Epoch: 1, train for the 112-th batch, train loss: 0.3096925616264343:  46%|█████      | 111/241 [00:21<00:35,  3.66it/s]Epoch: 1, train for the 112-th batch, train loss: 0.3096925616264343:  46%|█████      | 112/241 [00:21<00:31,  4.14it/s]Epoch: 1, train for the 116-th batch, train loss: 0.4141027629375458:  97%|██████████▋| 115/119 [00:22<00:00,  4.16it/s]Epoch: 1, train for the 98-th batch, train loss: 0.5344188809394836:  41%|█████▎       | 97/237 [00:21<00:44,  3.16it/s]Epoch: 1, train for the 116-th batch, train loss: 0.4141027629375458:  97%|██████████▋| 116/119 [00:22<00:00,  4.64it/s]Epoch: 1, train for the 98-th batch, train loss: 0.5344188809394836:  41%|█████▍       | 98/237 [00:21<00:39,  3.48it/s]Epoch: 1, train for the 79-th batch, train loss: 0.4804888665676117:  20%|██▋          | 78/383 [00:19<01:18,  3.90it/s]Epoch: 1, train for the 122-th batch, train loss: 0.5989713072776794:  83%|█████████  | 121/146 [00:23<00:05,  4.27it/s]Epoch: 1, train for the 122-th batch, train loss: 0.5989713072776794:  84%|█████████▏ | 122/146 [00:23<00:05,  4.39it/s]Epoch: 1, train for the 79-th batch, train loss: 0.4804888665676117:  21%|██▋          | 79/383 [00:19<01:08,  4.44it/s]Epoch: 1, train for the 113-th batch, train loss: 0.40191763639450073:  46%|████▋     | 112/241 [00:21<00:31,  4.14it/s]Epoch: 1, train for the 113-th batch, train loss: 0.40191763639450073:  47%|████▋     | 113/241 [00:21<00:25,  4.96it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5416467785835266:  70%|███████▋   | 105/151 [00:22<00:16,  2.82it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5416467785835266:  70%|███████▋   | 106/151 [00:22<00:15,  2.98it/s]Epoch: 1, train for the 99-th batch, train loss: 0.5577337145805359:  41%|█████▍       | 98/237 [00:21<00:39,  3.48it/s]Epoch: 1, train for the 99-th batch, train loss: 0.5577337145805359:  42%|█████▍       | 99/237 [00:21<00:34,  4.00it/s]Epoch: 1, train for the 107-th batch, train loss: 0.5310021638870239:  70%|███████▋   | 106/151 [00:23<00:15,  2.98it/s]Epoch: 1, train for the 107-th batch, train loss: 0.5310021638870239:  71%|███████▊   | 107/151 [00:23<00:11,  3.71it/s]Epoch: 1, train for the 114-th batch, train loss: 0.4201696813106537:  47%|█████▏     | 113/241 [00:22<00:25,  4.96it/s]Epoch: 1, train for the 114-th batch, train loss: 0.4201696813106537:  47%|█████▏     | 114/241 [00:22<00:29,  4.28it/s]Epoch: 1, train for the 117-th batch, train loss: 0.3960522413253784:  97%|██████████▋| 116/119 [00:23<00:00,  4.64it/s]Epoch: 1, train for the 117-th batch, train loss: 0.3960522413253784:  98%|██████████▊| 117/119 [00:23<00:00,  3.60it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5622202754020691:  42%|█████       | 99/237 [00:22<00:34,  4.00it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5066375136375427:  71%|███████▊   | 107/151 [00:23<00:11,  3.71it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5066375136375427:  72%|███████▊   | 108/151 [00:23<00:10,  4.18it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5622202754020691:  42%|████▋      | 100/237 [00:22<00:34,  4.01it/s]Epoch: 1, train for the 80-th batch, train loss: 0.3214908540248871:  21%|██▋          | 79/383 [00:20<01:08,  4.44it/s]Epoch: 1, train for the 123-th batch, train loss: 0.6189252734184265:  84%|█████████▏ | 122/146 [00:23<00:05,  4.39it/s]Epoch: 1, train for the 123-th batch, train loss: 0.6189252734184265:  84%|█████████▎ | 123/146 [00:23<00:06,  3.56it/s]Epoch: 1, train for the 80-th batch, train loss: 0.3214908540248871:  21%|██▋          | 80/383 [00:20<01:25,  3.55it/s]Epoch: 1, train for the 115-th batch, train loss: 0.4338412284851074:  47%|█████▏     | 114/241 [00:22<00:29,  4.28it/s]Epoch: 1, train for the 115-th batch, train loss: 0.4338412284851074:  48%|█████▏     | 115/241 [00:22<00:26,  4.83it/s]Epoch: 1, train for the 118-th batch, train loss: 0.41555625200271606:  98%|█████████▊| 117/119 [00:23<00:00,  3.60it/s]Epoch: 1, train for the 118-th batch, train loss: 0.41555625200271606:  99%|█████████▉| 118/119 [00:23<00:00,  4.10it/s]Epoch: 1, train for the 109-th batch, train loss: 0.5006865859031677:  72%|███████▊   | 108/151 [00:23<00:10,  4.18it/s]Epoch: 1, train for the 109-th batch, train loss: 0.5006865859031677:  72%|███████▉   | 109/151 [00:23<00:09,  4.53it/s]Epoch: 1, train for the 81-th batch, train loss: 0.29323050379753113:  21%|██▌         | 80/383 [00:20<01:25,  3.55it/s]Epoch: 1, train for the 81-th batch, train loss: 0.29323050379753113:  21%|██▌         | 81/383 [00:20<01:16,  3.96it/s]Epoch: 1, train for the 124-th batch, train loss: 0.5913350582122803:  84%|█████████▎ | 123/146 [00:23<00:06,  3.56it/s]Epoch: 1, train for the 124-th batch, train loss: 0.5913350582122803:  85%|█████████▎ | 124/146 [00:23<00:05,  3.71it/s]Epoch: 1, train for the 125-th batch, train loss: 0.5762670040130615:  85%|█████████▎ | 124/146 [00:23<00:05,  3.71it/s]Epoch: 1, train for the 116-th batch, train loss: 0.5200721025466919:  48%|█████▏     | 115/241 [00:22<00:26,  4.83it/s]Epoch: 1, train for the 125-th batch, train loss: 0.5762670040130615:  86%|█████████▍ | 125/146 [00:23<00:04,  4.53it/s]Epoch: 1, train for the 116-th batch, train loss: 0.5200721025466919:  48%|█████▎     | 116/241 [00:22<00:28,  4.37it/s]Epoch: 1, train for the 117-th batch, train loss: 0.5904907584190369:  48%|█████▎     | 116/241 [00:22<00:28,  4.37it/s]Epoch: 1, train for the 117-th batch, train loss: 0.5904907584190369:  49%|█████▎     | 117/241 [00:22<00:24,  5.08it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5859541893005371:  42%|████▋      | 100/237 [00:22<00:34,  4.01it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5859541893005371:  43%|████▋      | 101/237 [00:22<00:45,  3.00it/s]Epoch: 1, train for the 110-th batch, train loss: 0.5625261068344116:  72%|███████▉   | 109/151 [00:23<00:09,  4.53it/s]Epoch: 1, train for the 119-th batch, train loss: 0.28002578020095825:  99%|█████████▉| 118/119 [00:23<00:00,  4.10it/s]Epoch: 1, train for the 119-th batch, train loss: 0.28002578020095825: 100%|██████████| 119/119 [00:23<00:00,  3.41it/s]Epoch: 1, train for the 119-th batch, train loss: 0.28002578020095825: 100%|██████████| 119/119 [00:23<00:00,  4.99it/s]
Epoch: 1, train for the 110-th batch, train loss: 0.5625261068344116:  73%|████████   | 110/151 [00:23<00:11,  3.71it/s]Epoch: 1, train for the 82-th batch, train loss: 0.4281595051288605:  21%|██▋          | 81/383 [00:20<01:16,  3.96it/s]Epoch: 1, train for the 82-th batch, train loss: 0.4281595051288605:  21%|██▊          | 82/383 [00:20<01:27,  3.44it/s]Epoch: 1, train for the 126-th batch, train loss: 0.607460618019104:  86%|██████████▎ | 125/146 [00:24<00:04,  4.53it/s]Epoch: 1, train for the 126-th batch, train loss: 0.607460618019104:  86%|██████████▎ | 126/146 [00:24<00:04,  4.40it/s]Epoch: 1, train for the 118-th batch, train loss: 0.48035934567451477:  49%|████▊     | 117/241 [00:22<00:24,  5.08it/s]Epoch: 1, train for the 118-th batch, train loss: 0.48035934567451477:  49%|████▉     | 118/241 [00:22<00:22,  5.48it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5082547068595886:  73%|████████   | 110/151 [00:23<00:11,  3.71it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5800945162773132:  43%|████▋      | 101/237 [00:22<00:45,  3.00it/s]  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5082547068595886:  74%|████████   | 111/151 [00:23<00:09,  4.29it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5800945162773132:  43%|████▋      | 102/237 [00:22<00:39,  3.45it/s]Epoch: 1, train for the 83-th batch, train loss: 0.45806944370269775:  21%|██▌         | 82/383 [00:20<01:27,  3.44it/s]evaluate for the 1-th batch, evaluate loss: 0.5404439568519592:   0%|                            | 0/40 [00:00<?, ?it/s]Epoch: 1, train for the 83-th batch, train loss: 0.45806944370269775:  22%|██▌         | 83/383 [00:20<01:17,  3.88it/s]Epoch: 1, train for the 119-th batch, train loss: 0.36026060581207275:  49%|████▉     | 118/241 [00:22<00:22,  5.48it/s]Epoch: 1, train for the 119-th batch, train loss: 0.36026060581207275:  49%|████▉     | 119/241 [00:22<00:21,  5.76it/s]evaluate for the 2-th batch, evaluate loss: 0.5455147624015808:   0%|                            | 0/40 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5455147624015808:   5%|█                   | 2/40 [00:00<00:01, 19.16it/s]evaluate for the 3-th batch, evaluate loss: 0.588617742061615:   5%|█                    | 2/40 [00:00<00:01, 19.16it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5977966785430908:  49%|█████▍     | 119/241 [00:23<00:21,  5.76it/s]evaluate for the 4-th batch, evaluate loss: 0.6416035294532776:   5%|█                   | 2/40 [00:00<00:01, 19.16it/s]evaluate for the 4-th batch, evaluate loss: 0.6416035294532776:  10%|██                  | 4/40 [00:00<00:01, 19.14it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5977966785430908:  50%|█████▍     | 120/241 [00:23<00:19,  6.36it/s]Epoch: 1, train for the 84-th batch, train loss: 0.48564955592155457:  22%|██▌         | 83/383 [00:20<01:17,  3.88it/s]evaluate for the 5-th batch, evaluate loss: 0.6634636521339417:  10%|██                  | 4/40 [00:00<00:01, 19.14it/s]Epoch: 1, train for the 84-th batch, train loss: 0.48564955592155457:  22%|██▋         | 84/383 [00:20<01:11,  4.20it/s]Epoch: 1, train for the 127-th batch, train loss: 0.5687617063522339:  86%|█████████▍ | 126/146 [00:24<00:04,  4.40it/s]evaluate for the 6-th batch, evaluate loss: 0.5974816679954529:  10%|██                  | 4/40 [00:00<00:01, 19.14it/s]evaluate for the 6-th batch, evaluate loss: 0.5974816679954529:  15%|███                 | 6/40 [00:00<00:02, 15.13it/s]Epoch: 1, train for the 127-th batch, train loss: 0.5687617063522339:  87%|█████████▌ | 127/146 [00:24<00:05,  3.32it/s]Epoch: 1, train for the 112-th batch, train loss: 0.4659252166748047:  74%|████████   | 111/151 [00:24<00:09,  4.29it/s]Epoch: 1, train for the 121-th batch, train loss: 0.5668166279792786:  50%|█████▍     | 120/241 [00:23<00:19,  6.36it/s]Epoch: 1, train for the 103-th batch, train loss: 0.6204109191894531:  43%|████▋      | 102/237 [00:23<00:39,  3.45it/s]Epoch: 1, train for the 112-th batch, train loss: 0.4659252166748047:  74%|████████▏  | 112/151 [00:24<00:11,  3.50it/s]Epoch: 1, train for the 121-th batch, train loss: 0.5668166279792786:  50%|█████▌     | 121/241 [00:23<00:20,  5.90it/s]evaluate for the 7-th batch, evaluate loss: 0.6301460862159729:  15%|███                 | 6/40 [00:00<00:02, 15.13it/s]Epoch: 1, train for the 85-th batch, train loss: 0.4567689895629883:  22%|██▊          | 84/383 [00:21<01:11,  4.20it/s]Epoch: 1, train for the 103-th batch, train loss: 0.6204109191894531:  43%|████▊      | 103/237 [00:23<00:44,  3.02it/s]Epoch: 1, train for the 85-th batch, train loss: 0.4567689895629883:  22%|██▉          | 85/383 [00:21<01:07,  4.42it/s]evaluate for the 8-th batch, evaluate loss: 0.5574378967285156:  15%|███                 | 6/40 [00:00<00:02, 15.13it/s]evaluate for the 8-th batch, evaluate loss: 0.5574378967285156:  20%|████                | 8/40 [00:00<00:02, 14.19it/s]Epoch: 1, train for the 128-th batch, train loss: 0.6044582724571228:  87%|█████████▌ | 127/146 [00:24<00:05,  3.32it/s]Epoch: 1, train for the 128-th batch, train loss: 0.6044582724571228:  88%|█████████▋ | 128/146 [00:24<00:04,  3.79it/s]Epoch: 1, train for the 113-th batch, train loss: 0.5441044569015503:  74%|████████▏  | 112/151 [00:24<00:11,  3.50it/s]Epoch: 1, train for the 113-th batch, train loss: 0.5441044569015503:  75%|████████▏  | 113/151 [00:24<00:09,  3.99it/s]evaluate for the 9-th batch, evaluate loss: 0.6031440496444702:  20%|████                | 8/40 [00:00<00:02, 14.19it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5520328283309937:  43%|████▊      | 103/237 [00:23<00:44,  3.02it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5520328283309937:  44%|████▊      | 104/237 [00:23<00:38,  3.43it/s]Epoch: 1, train for the 86-th batch, train loss: 0.3029468059539795:  22%|██▉          | 85/383 [00:21<01:07,  4.42it/s]Epoch: 1, train for the 129-th batch, train loss: 0.6113390326499939:  88%|█████████▋ | 128/146 [00:24<00:04,  3.79it/s]Epoch: 1, train for the 129-th batch, train loss: 0.6113390326499939:  88%|█████████▋ | 129/146 [00:24<00:03,  4.58it/s]Epoch: 1, train for the 86-th batch, train loss: 0.3029468059539795:  22%|██▉          | 86/383 [00:21<01:06,  4.43it/s]Epoch: 1, train for the 114-th batch, train loss: 0.4939967095851898:  75%|████████▏  | 113/151 [00:24<00:09,  3.99it/s]Epoch: 1, train for the 114-th batch, train loss: 0.4939967095851898:  75%|████████▎  | 114/151 [00:24<00:07,  4.63it/s]evaluate for the 10-th batch, evaluate loss: 0.6758542656898499:  20%|███▊               | 8/40 [00:00<00:02, 14.19it/s]evaluate for the 10-th batch, evaluate loss: 0.6758542656898499:  25%|████▌             | 10/40 [00:00<00:02, 10.18it/s]evaluate for the 11-th batch, evaluate loss: 0.5714898705482483:  25%|████▌             | 10/40 [00:00<00:02, 10.18it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5465033054351807:  88%|█████████▋ | 129/146 [00:25<00:03,  4.58it/s]evaluate for the 12-th batch, evaluate loss: 0.57196444272995:  25%|█████               | 10/40 [00:00<00:02, 10.18it/s]evaluate for the 12-th batch, evaluate loss: 0.57196444272995:  30%|██████              | 12/40 [00:00<00:02, 11.88it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5465033054351807:  89%|█████████▊ | 130/146 [00:25<00:03,  4.27it/s]Epoch: 1, train for the 105-th batch, train loss: 0.5652419924736023:  44%|████▊      | 104/237 [00:23<00:38,  3.43it/s]Epoch: 1, train for the 122-th batch, train loss: 0.4350453019142151:  50%|█████▌     | 121/241 [00:23<00:20,  5.90it/s]Epoch: 1, train for the 115-th batch, train loss: 0.46349066495895386:  75%|███████▌  | 114/151 [00:24<00:07,  4.63it/s]Epoch: 1, train for the 115-th batch, train loss: 0.46349066495895386:  76%|███████▌  | 115/151 [00:24<00:08,  4.37it/s]Epoch: 1, train for the 105-th batch, train loss: 0.5652419924736023:  44%|████▊      | 105/237 [00:23<00:40,  3.28it/s]Epoch: 1, train for the 122-th batch, train loss: 0.4350453019142151:  51%|█████▌     | 122/241 [00:23<00:34,  3.49it/s]Epoch: 1, train for the 87-th batch, train loss: 0.45053350925445557:  22%|██▋         | 86/383 [00:21<01:06,  4.43it/s]evaluate for the 13-th batch, evaluate loss: 0.6268068552017212:  30%|█████▍            | 12/40 [00:01<00:02, 11.88it/s]Epoch: 1, train for the 87-th batch, train loss: 0.45053350925445557:  23%|██▋         | 87/383 [00:21<01:17,  3.82it/s]Epoch: 1, train for the 131-th batch, train loss: 0.5766428112983704:  89%|█████████▊ | 130/146 [00:25<00:03,  4.27it/s]Epoch: 1, train for the 131-th batch, train loss: 0.5766428112983704:  90%|█████████▊ | 131/146 [00:25<00:03,  4.88it/s]evaluate for the 14-th batch, evaluate loss: 0.5663082003593445:  30%|█████▍            | 12/40 [00:01<00:02, 11.88it/s]evaluate for the 14-th batch, evaluate loss: 0.5663082003593445:  35%|██████▎           | 14/40 [00:01<00:02, 11.64it/s]evaluate for the 15-th batch, evaluate loss: 0.588884174823761:  35%|██████▋            | 14/40 [00:01<00:02, 11.64it/s]Epoch: 1, train for the 123-th batch, train loss: 0.6322376132011414:  51%|█████▌     | 122/241 [00:24<00:34,  3.49it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5819441676139832:  44%|████▊      | 105/237 [00:24<00:40,  3.28it/s]Epoch: 1, train for the 123-th batch, train loss: 0.6322376132011414:  51%|█████▌     | 123/241 [00:24<00:31,  3.74it/s]Epoch: 1, train for the 116-th batch, train loss: 0.4543541967868805:  76%|████████▍  | 115/151 [00:25<00:08,  4.37it/s]Epoch: 1, train for the 116-th batch, train loss: 0.4543541967868805:  77%|████████▍  | 116/151 [00:25<00:08,  4.28it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5819441676139832:  45%|████▉      | 106/237 [00:24<00:37,  3.49it/s]Epoch: 1, train for the 124-th batch, train loss: 0.38365980982780457:  51%|█████     | 123/241 [00:24<00:31,  3.74it/s]Epoch: 1, train for the 124-th batch, train loss: 0.38365980982780457:  51%|█████▏    | 124/241 [00:24<00:27,  4.30it/s]Epoch: 1, train for the 88-th batch, train loss: 0.4903595745563507:  23%|██▉          | 87/383 [00:22<01:17,  3.82it/s]Epoch: 1, train for the 88-th batch, train loss: 0.4903595745563507:  23%|██▉          | 88/383 [00:22<01:26,  3.42it/s]evaluate for the 16-th batch, evaluate loss: 0.6148864030838013:  35%|██████▎           | 14/40 [00:01<00:02, 11.64it/s]evaluate for the 16-th batch, evaluate loss: 0.6148864030838013:  40%|███████▏          | 16/40 [00:01<00:02,  9.53it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5612576007843018:  90%|█████████▊ | 131/146 [00:25<00:03,  4.88it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5612576007843018:  90%|█████████▉ | 132/146 [00:25<00:03,  3.87it/s]evaluate for the 17-th batch, evaluate loss: 0.6502568125724792:  40%|███████▏          | 16/40 [00:01<00:02,  9.53it/s]evaluate for the 18-th batch, evaluate loss: 0.5757812261581421:  40%|███████▏          | 16/40 [00:01<00:02,  9.53it/s]evaluate for the 18-th batch, evaluate loss: 0.5757812261581421:  45%|████████          | 18/40 [00:01<00:02, 10.67it/s]Epoch: 1, train for the 107-th batch, train loss: 0.5824944972991943:  45%|████▉      | 106/237 [00:24<00:37,  3.49it/s]Epoch: 1, train for the 117-th batch, train loss: 0.528334379196167:  77%|█████████▏  | 116/151 [00:25<00:08,  4.28it/s]Epoch: 1, train for the 125-th batch, train loss: 0.4896560311317444:  51%|█████▋     | 124/241 [00:24<00:27,  4.30it/s]Epoch: 1, train for the 117-th batch, train loss: 0.528334379196167:  77%|█████████▎  | 117/151 [00:25<00:09,  3.70it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5010591149330139:  23%|██▉          | 88/383 [00:22<01:26,  3.42it/s]Epoch: 1, train for the 107-th batch, train loss: 0.5824944972991943:  45%|████▉      | 107/237 [00:24<00:40,  3.23it/s]Epoch: 1, train for the 133-th batch, train loss: 0.5416175127029419:  90%|█████████▉ | 132/146 [00:25<00:03,  3.87it/s]Epoch: 1, train for the 125-th batch, train loss: 0.4896560311317444:  52%|█████▋     | 125/241 [00:24<00:27,  4.26it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5010591149330139:  23%|███          | 89/383 [00:22<01:18,  3.76it/s]Epoch: 1, train for the 133-th batch, train loss: 0.5416175127029419:  91%|██████████ | 133/146 [00:25<00:02,  4.48it/s]evaluate for the 19-th batch, evaluate loss: 0.6323820948600769:  45%|████████          | 18/40 [00:01<00:02, 10.67it/s]evaluate for the 20-th batch, evaluate loss: 0.6068495512008667:  45%|████████          | 18/40 [00:01<00:02, 10.67it/s]evaluate for the 20-th batch, evaluate loss: 0.6068495512008667:  50%|█████████         | 20/40 [00:01<00:01, 10.56it/s]Epoch: 1, train for the 118-th batch, train loss: 0.4463827311992645:  77%|████████▌  | 117/151 [00:25<00:09,  3.70it/s]Epoch: 1, train for the 126-th batch, train loss: 0.43870413303375244:  52%|█████▏    | 125/241 [00:24<00:27,  4.26it/s]evaluate for the 21-th batch, evaluate loss: 0.5709450244903564:  50%|█████████         | 20/40 [00:01<00:01, 10.56it/s]Epoch: 1, train for the 118-th batch, train loss: 0.4463827311992645:  78%|████████▌  | 118/151 [00:25<00:08,  3.89it/s]Epoch: 1, train for the 126-th batch, train loss: 0.43870413303375244:  52%|█████▏    | 126/241 [00:24<00:26,  4.36it/s]Epoch: 1, train for the 134-th batch, train loss: 0.6003821492195129:  91%|██████████ | 133/146 [00:26<00:02,  4.48it/s]evaluate for the 22-th batch, evaluate loss: 0.5524976253509521:  50%|█████████         | 20/40 [00:01<00:01, 10.56it/s]evaluate for the 22-th batch, evaluate loss: 0.5524976253509521:  55%|█████████▉        | 22/40 [00:01<00:01,  9.90it/s]Epoch: 1, train for the 134-th batch, train loss: 0.6003821492195129:  92%|██████████ | 134/146 [00:26<00:03,  3.73it/s]Epoch: 1, train for the 90-th batch, train loss: 0.488014817237854:  23%|███▎          | 89/383 [00:22<01:18,  3.76it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5870836973190308:  45%|████▉      | 107/237 [00:24<00:40,  3.23it/s]Epoch: 1, train for the 90-th batch, train loss: 0.488014817237854:  23%|███▎          | 90/383 [00:22<01:30,  3.22it/s]Epoch: 1, train for the 119-th batch, train loss: 0.5309144258499146:  78%|████████▌  | 118/151 [00:25<00:08,  3.89it/s]Epoch: 1, train for the 127-th batch, train loss: 0.5515850782394409:  52%|█████▊     | 126/241 [00:24<00:26,  4.36it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5870836973190308:  46%|█████      | 108/237 [00:24<00:45,  2.86it/s]Epoch: 1, train for the 127-th batch, train loss: 0.5515850782394409:  53%|█████▊     | 127/241 [00:24<00:26,  4.34it/s]evaluate for the 23-th batch, evaluate loss: 0.4975660741329193:  55%|█████████▉        | 22/40 [00:02<00:01,  9.90it/s]Epoch: 1, train for the 119-th batch, train loss: 0.5309144258499146:  79%|████████▋  | 119/151 [00:25<00:08,  3.95it/s]Epoch: 1, train for the 135-th batch, train loss: 0.5945005416870117:  92%|██████████ | 134/146 [00:26<00:03,  3.73it/s]evaluate for the 24-th batch, evaluate loss: 0.5825657248497009:  55%|█████████▉        | 22/40 [00:02<00:01,  9.90it/s]evaluate for the 24-th batch, evaluate loss: 0.5825657248497009:  60%|██████████▊       | 24/40 [00:02<00:01, 10.54it/s]Epoch: 1, train for the 135-th batch, train loss: 0.5945005416870117:  92%|██████████▏| 135/146 [00:26<00:02,  4.21it/s]evaluate for the 25-th batch, evaluate loss: 0.590836226940155:  60%|███████████▍       | 24/40 [00:02<00:01, 10.54it/s]Epoch: 1, train for the 109-th batch, train loss: 0.6355003118515015:  46%|█████      | 108/237 [00:25<00:45,  2.86it/s]Epoch: 1, train for the 109-th batch, train loss: 0.6355003118515015:  46%|█████      | 109/237 [00:25<00:39,  3.27it/s]Epoch: 1, train for the 91-th batch, train loss: 0.3270668685436249:  23%|███          | 90/383 [00:22<01:30,  3.22it/s]Epoch: 1, train for the 91-th batch, train loss: 0.3270668685436249:  24%|███          | 91/383 [00:22<01:25,  3.43it/s]Epoch: 1, train for the 136-th batch, train loss: 0.5637476444244385:  92%|██████████▏| 135/146 [00:26<00:02,  4.21it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5240817666053772:  79%|████████▋  | 119/151 [00:26<00:08,  3.95it/s]evaluate for the 26-th batch, evaluate loss: 0.5666344165802002:  60%|██████████▊       | 24/40 [00:02<00:01, 10.54it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5436286330223083:  53%|█████▊     | 127/241 [00:25<00:26,  4.34it/s]evaluate for the 26-th batch, evaluate loss: 0.5666344165802002:  65%|███████████▋      | 26/40 [00:02<00:01, 11.56it/s]Epoch: 1, train for the 136-th batch, train loss: 0.5637476444244385:  93%|██████████▏| 136/146 [00:26<00:02,  4.86it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5436286330223083:  53%|█████▊     | 128/241 [00:25<00:26,  4.21it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5240817666053772:  79%|████████▋  | 120/151 [00:26<00:07,  3.95it/s]evaluate for the 27-th batch, evaluate loss: 0.6045240163803101:  65%|███████████▋      | 26/40 [00:02<00:01, 11.56it/s]Epoch: 1, train for the 129-th batch, train loss: 0.4829810857772827:  53%|█████▊     | 128/241 [00:25<00:26,  4.21it/s]Epoch: 1, train for the 129-th batch, train loss: 0.4829810857772827:  54%|█████▉     | 129/241 [00:25<00:25,  4.45it/s]Epoch: 1, train for the 137-th batch, train loss: 0.5446975827217102:  93%|██████████▏| 136/146 [00:26<00:02,  4.86it/s]Epoch: 1, train for the 137-th batch, train loss: 0.5446975827217102:  94%|██████████▎| 137/146 [00:26<00:01,  4.67it/s]Epoch: 1, train for the 138-th batch, train loss: 0.5885005593299866:  94%|██████████▎| 137/146 [00:26<00:01,  4.67it/s]Epoch: 1, train for the 138-th batch, train loss: 0.5885005593299866:  95%|██████████▍| 138/146 [00:26<00:01,  5.10it/s]evaluate for the 28-th batch, evaluate loss: 0.5258958339691162:  65%|███████████▋      | 26/40 [00:02<00:01, 11.56it/s]evaluate for the 28-th batch, evaluate loss: 0.5258958339691162:  70%|████████████▌     | 28/40 [00:02<00:01,  8.18it/s]Epoch: 1, train for the 130-th batch, train loss: 0.4894961416721344:  54%|█████▉     | 129/241 [00:25<00:25,  4.45it/s]Epoch: 1, train for the 92-th batch, train loss: 0.36708471179008484:  24%|██▊         | 91/383 [00:23<01:25,  3.43it/s]Epoch: 1, train for the 121-th batch, train loss: 0.497200608253479:  79%|█████████▌  | 120/151 [00:26<00:07,  3.95it/s]Epoch: 1, train for the 130-th batch, train loss: 0.4894961416721344:  54%|█████▉     | 130/241 [00:25<00:24,  4.49it/s]Epoch: 1, train for the 110-th batch, train loss: 0.6011426448822021:  46%|█████      | 109/237 [00:25<00:39,  3.27it/s]Epoch: 1, train for the 121-th batch, train loss: 0.497200608253479:  80%|█████████▌  | 121/151 [00:26<00:09,  3.28it/s]Epoch: 1, train for the 92-th batch, train loss: 0.36708471179008484:  24%|██▉         | 92/383 [00:23<01:40,  2.91it/s]evaluate for the 29-th batch, evaluate loss: 0.5943824052810669:  70%|████████████▌     | 28/40 [00:02<00:01,  8.18it/s]Epoch: 1, train for the 110-th batch, train loss: 0.6011426448822021:  46%|█████      | 110/237 [00:25<00:46,  2.72it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5987059473991394:  95%|██████████▍| 138/146 [00:27<00:01,  5.10it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5987059473991394:  95%|██████████▍| 139/146 [00:27<00:01,  5.66it/s]evaluate for the 30-th batch, evaluate loss: 0.5453195571899414:  70%|████████████▌     | 28/40 [00:02<00:01,  8.18it/s]evaluate for the 30-th batch, evaluate loss: 0.5453195571899414:  75%|█████████████▌    | 30/40 [00:02<00:01,  9.00it/s]Epoch: 1, train for the 131-th batch, train loss: 0.5551279187202454:  54%|█████▉     | 130/241 [00:25<00:24,  4.49it/s]Epoch: 1, train for the 93-th batch, train loss: 0.45616674423217773:  24%|██▉         | 92/383 [00:23<01:40,  2.91it/s]Epoch: 1, train for the 122-th batch, train loss: 0.4420008361339569:  80%|████████▊  | 121/151 [00:26<00:09,  3.28it/s]Epoch: 1, train for the 131-th batch, train loss: 0.5551279187202454:  54%|█████▉     | 131/241 [00:25<00:23,  4.60it/s]Epoch: 1, train for the 93-th batch, train loss: 0.45616674423217773:  24%|██▉         | 93/383 [00:23<01:26,  3.34it/s]evaluate for the 31-th batch, evaluate loss: 0.5466183423995972:  75%|█████████████▌    | 30/40 [00:02<00:01,  9.00it/s]Epoch: 1, train for the 122-th batch, train loss: 0.4420008361339569:  81%|████████▉  | 122/151 [00:26<00:07,  3.66it/s]Epoch: 1, train for the 140-th batch, train loss: 0.5727304220199585:  95%|██████████▍| 139/146 [00:27<00:01,  5.66it/s]Epoch: 1, train for the 140-th batch, train loss: 0.5727304220199585:  96%|██████████▌| 140/146 [00:27<00:01,  5.63it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5088436007499695:  54%|█████▉     | 131/241 [00:25<00:23,  4.60it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5088436007499695:  55%|██████     | 132/241 [00:25<00:21,  5.15it/s]Epoch: 1, train for the 94-th batch, train loss: 0.3176727294921875:  24%|███▏         | 93/383 [00:23<01:26,  3.34it/s]Epoch: 1, train for the 94-th batch, train loss: 0.3176727294921875:  25%|███▏         | 94/383 [00:23<01:13,  3.91it/s]evaluate for the 32-th batch, evaluate loss: 0.5777570605278015:  75%|█████████████▌    | 30/40 [00:03<00:01,  9.00it/s]evaluate for the 32-th batch, evaluate loss: 0.5777570605278015:  80%|██████████████▍   | 32/40 [00:03<00:00,  8.24it/s]Epoch: 1, train for the 141-th batch, train loss: 0.5767524838447571:  96%|██████████▌| 140/146 [00:27<00:01,  5.63it/s]Epoch: 1, train for the 141-th batch, train loss: 0.5767524838447571:  97%|██████████▌| 141/146 [00:27<00:00,  5.40it/s]evaluate for the 33-th batch, evaluate loss: 0.5623324513435364:  80%|██████████████▍   | 32/40 [00:03<00:00,  8.24it/s]Epoch: 1, train for the 133-th batch, train loss: 0.43890470266342163:  55%|█████▍    | 132/241 [00:26<00:21,  5.15it/s]Epoch: 1, train for the 111-th batch, train loss: 0.6008304357528687:  46%|█████      | 110/237 [00:26<00:46,  2.72it/s]Epoch: 1, train for the 133-th batch, train loss: 0.43890470266342163:  55%|█████▌    | 133/241 [00:26<00:21,  5.04it/s]evaluate for the 34-th batch, evaluate loss: 0.6093263626098633:  80%|██████████████▍   | 32/40 [00:03<00:00,  8.24it/s]evaluate for the 34-th batch, evaluate loss: 0.6093263626098633:  85%|███████████████▎  | 34/40 [00:03<00:00,  9.57it/s]Epoch: 1, train for the 111-th batch, train loss: 0.6008304357528687:  47%|█████▏     | 111/237 [00:26<00:52,  2.39it/s]Epoch: 1, train for the 123-th batch, train loss: 0.5399571061134338:  81%|████████▉  | 122/151 [00:27<00:07,  3.66it/s]Epoch: 1, train for the 95-th batch, train loss: 0.4259973466396332:  25%|███▏         | 94/383 [00:24<01:13,  3.91it/s]Epoch: 1, train for the 123-th batch, train loss: 0.5399571061134338:  81%|████████▉  | 123/151 [00:27<00:08,  3.25it/s]Epoch: 1, train for the 95-th batch, train loss: 0.4259973466396332:  25%|███▏         | 95/383 [00:24<01:12,  3.96it/s]Epoch: 1, train for the 142-th batch, train loss: 0.5461276769638062:  97%|██████████▌| 141/146 [00:27<00:00,  5.40it/s]evaluate for the 35-th batch, evaluate loss: 0.6215984225273132:  85%|███████████████▎  | 34/40 [00:03<00:00,  9.57it/s]Epoch: 1, train for the 142-th batch, train loss: 0.5461276769638062:  97%|██████████▋| 142/146 [00:27<00:00,  5.57it/s]evaluate for the 36-th batch, evaluate loss: 0.5633825063705444:  85%|███████████████▎  | 34/40 [00:03<00:00,  9.57it/s]evaluate for the 36-th batch, evaluate loss: 0.5633825063705444:  90%|████████████████▏ | 36/40 [00:03<00:00, 10.85it/s]evaluate for the 37-th batch, evaluate loss: 0.5614185333251953:  90%|████████████████▏ | 36/40 [00:03<00:00, 10.85it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5015875697135925:  55%|██████     | 133/241 [00:26<00:21,  5.04it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5015875697135925:  56%|██████     | 134/241 [00:26<00:21,  4.95it/s]Epoch: 1, train for the 124-th batch, train loss: 0.4673258364200592:  81%|████████▉  | 123/151 [00:27<00:08,  3.25it/s]Epoch: 1, train for the 124-th batch, train loss: 0.4673258364200592:  82%|█████████  | 124/151 [00:27<00:07,  3.62it/s]evaluate for the 38-th batch, evaluate loss: 0.5751086473464966:  90%|████████████████▏ | 36/40 [00:03<00:00, 10.85it/s]evaluate for the 38-th batch, evaluate loss: 0.5751086473464966:  95%|█████████████████ | 38/40 [00:03<00:00, 11.27it/s]Epoch: 1, train for the 143-th batch, train loss: 0.5589534044265747:  97%|██████████▋| 142/146 [00:27<00:00,  5.57it/s]Epoch: 1, train for the 143-th batch, train loss: 0.5589534044265747:  98%|██████████▊| 143/146 [00:27<00:00,  4.83it/s]evaluate for the 39-th batch, evaluate loss: 0.6120988130569458:  95%|█████████████████ | 38/40 [00:03<00:00, 11.27it/s]Epoch: 1, train for the 96-th batch, train loss: 0.428352952003479:  25%|███▍          | 95/383 [00:24<01:12,  3.96it/s]Epoch: 1, train for the 112-th batch, train loss: 0.618791401386261:  47%|█████▌      | 111/237 [00:26<00:52,  2.39it/s]Epoch: 1, train for the 112-th batch, train loss: 0.618791401386261:  47%|█████▋      | 112/237 [00:26<00:50,  2.48it/s]Epoch: 1, train for the 96-th batch, train loss: 0.428352952003479:  25%|███▌          | 96/383 [00:24<01:18,  3.64it/s]evaluate for the 40-th batch, evaluate loss: 0.643646240234375:  95%|██████████████████ | 38/40 [00:03<00:00, 11.27it/s]evaluate for the 40-th batch, evaluate loss: 0.643646240234375: 100%|███████████████████| 40/40 [00:03<00:00, 12.95it/s]evaluate for the 40-th batch, evaluate loss: 0.643646240234375: 100%|███████████████████| 40/40 [00:03<00:00, 10.94it/s]
Epoch: 1, train for the 135-th batch, train loss: 0.3982926607131958:  56%|██████     | 134/241 [00:26<00:21,  4.95it/s]Epoch: 1, train for the 135-th batch, train loss: 0.3982926607131958:  56%|██████▏    | 135/241 [00:26<00:26,  3.98it/s]Epoch: 1, train for the 113-th batch, train loss: 0.6209047436714172:  47%|█████▏     | 112/237 [00:26<00:50,  2.48it/s]Epoch: 1, train for the 97-th batch, train loss: 0.2908302843570709:  25%|███▎         | 96/383 [00:24<01:18,  3.64it/s]  0%|                                                                                            | 0/21 [00:00<?, ?it/s]Epoch: 1, train for the 113-th batch, train loss: 0.6209047436714172:  48%|█████▏     | 113/237 [00:26<00:43,  2.88it/s]Epoch: 1, train for the 97-th batch, train loss: 0.2908302843570709:  25%|███▎         | 97/383 [00:24<01:14,  3.84it/s]Epoch: 1, train for the 125-th batch, train loss: 0.5048182010650635:  82%|█████████  | 124/151 [00:27<00:07,  3.62it/s]Epoch: 1, train for the 144-th batch, train loss: 0.5462111830711365:  98%|██████████▊| 143/146 [00:28<00:00,  4.83it/s]Epoch: 1, train for the 144-th batch, train loss: 0.5462111830711365:  99%|██████████▊| 144/146 [00:28<00:00,  4.31it/s]Epoch: 1, train for the 125-th batch, train loss: 0.5048182010650635:  83%|█████████  | 125/151 [00:27<00:08,  3.21it/s]Epoch: 1, train for the 136-th batch, train loss: 0.4356338679790497:  56%|██████▏    | 135/241 [00:26<00:26,  3.98it/s]Epoch: 1, train for the 136-th batch, train loss: 0.4356338679790497:  56%|██████▏    | 136/241 [00:26<00:24,  4.34it/s]Epoch: 1, train for the 145-th batch, train loss: 0.5575652718544006:  99%|██████████▊| 144/146 [00:28<00:00,  4.31it/s]Epoch: 1, train for the 145-th batch, train loss: 0.5575652718544006:  99%|██████████▉| 145/146 [00:28<00:00,  4.88it/s]Epoch: 1, train for the 114-th batch, train loss: 0.57651287317276:  48%|██████▏      | 113/237 [00:26<00:43,  2.88it/s]evaluate for the 1-th batch, evaluate loss: 0.7673547863960266:   0%|                            | 0/21 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.7673547863960266:   5%|▉                   | 1/21 [00:00<00:04,  4.76it/s]Epoch: 1, train for the 98-th batch, train loss: 0.29655492305755615:  25%|███         | 97/383 [00:24<01:14,  3.84it/s]Epoch: 1, train for the 114-th batch, train loss: 0.57651287317276:  48%|██████▎      | 114/237 [00:26<00:38,  3.24it/s]Epoch: 1, train for the 98-th batch, train loss: 0.29655492305755615:  26%|███         | 98/383 [00:24<01:11,  3.99it/s]evaluate for the 2-th batch, evaluate loss: 0.8544756770133972:   5%|▉                   | 1/21 [00:00<00:04,  4.76it/s]evaluate for the 3-th batch, evaluate loss: 0.8555743098258972:   5%|▉                   | 1/21 [00:00<00:04,  4.76it/s]evaluate for the 3-th batch, evaluate loss: 0.8555743098258972:  14%|██▊                 | 3/21 [00:00<00:02,  8.75it/s]evaluate for the 4-th batch, evaluate loss: 0.7689856290817261:  14%|██▊                 | 3/21 [00:00<00:02,  8.75it/s]Epoch: 1, train for the 137-th batch, train loss: 0.46064162254333496:  56%|█████▋    | 136/241 [00:27<00:24,  4.34it/s]Epoch: 1, train for the 126-th batch, train loss: 0.47571760416030884:  83%|████████▎ | 125/151 [00:28<00:08,  3.21it/s]Epoch: 1, train for the 137-th batch, train loss: 0.46064162254333496:  57%|█████▋    | 137/241 [00:27<00:26,  3.88it/s]evaluate for the 5-th batch, evaluate loss: 0.8289210796356201:  14%|██▊                 | 3/21 [00:00<00:02,  8.75it/s]evaluate for the 5-th batch, evaluate loss: 0.8289210796356201:  24%|████▊               | 5/21 [00:00<00:01, 11.82it/s]Epoch: 1, train for the 126-th batch, train loss: 0.47571760416030884:  83%|████████▎ | 126/151 [00:28<00:08,  2.86it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5073758363723755:  99%|██████████▉| 145/146 [00:28<00:00,  4.88it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5073758363723755: 100%|███████████| 146/146 [00:28<00:00,  4.02it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5073758363723755: 100%|███████████| 146/146 [00:28<00:00,  5.09it/s]
evaluate for the 6-th batch, evaluate loss: 0.821184515953064:  24%|█████                | 5/21 [00:00<00:01, 11.82it/s]Epoch: 1, train for the 99-th batch, train loss: 0.3141055405139923:  26%|███▎         | 98/383 [00:25<01:11,  3.99it/s]Epoch: 1, train for the 115-th batch, train loss: 0.5507445931434631:  48%|█████▎     | 114/237 [00:27<00:38,  3.24it/s]  0%|                                                                                            | 0/38 [00:00<?, ?it/s]Epoch: 1, train for the 99-th batch, train loss: 0.3141055405139923:  26%|███▎         | 99/383 [00:25<01:21,  3.48it/s]Epoch: 1, train for the 138-th batch, train loss: 0.3854096233844757:  57%|██████▎    | 137/241 [00:27<00:26,  3.88it/s]Epoch: 1, train for the 138-th batch, train loss: 0.3854096233844757:  57%|██████▎    | 138/241 [00:27<00:23,  4.45it/s]evaluate for the 7-th batch, evaluate loss: 0.7319312691688538:  24%|████▊               | 5/21 [00:00<00:01, 11.82it/s]evaluate for the 7-th batch, evaluate loss: 0.7319312691688538:  33%|██████▋             | 7/21 [00:00<00:01, 12.63it/s]Epoch: 1, train for the 115-th batch, train loss: 0.5507445931434631:  49%|█████▎     | 115/237 [00:27<00:41,  2.95it/s]evaluate for the 1-th batch, evaluate loss: 0.5485225915908813:   0%|                            | 0/38 [00:00<?, ?it/s]Epoch: 1, train for the 127-th batch, train loss: 0.47870445251464844:  83%|████████▎ | 126/151 [00:28<00:08,  2.86it/s]Epoch: 1, train for the 127-th batch, train loss: 0.47870445251464844:  84%|████████▍ | 127/151 [00:28<00:07,  3.27it/s]evaluate for the 8-th batch, evaluate loss: 0.7514793872833252:  33%|██████▋             | 7/21 [00:00<00:01, 12.63it/s]evaluate for the 2-th batch, evaluate loss: 0.5683262348175049:   0%|                            | 0/38 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5683262348175049:   5%|█                   | 2/38 [00:00<00:02, 13.71it/s]evaluate for the 9-th batch, evaluate loss: 0.7422265410423279:  33%|██████▋             | 7/21 [00:00<00:01, 12.63it/s]evaluate for the 9-th batch, evaluate loss: 0.7422265410423279:  43%|████████▌           | 9/21 [00:00<00:00, 13.06it/s]evaluate for the 3-th batch, evaluate loss: 0.5318644046783447:   5%|█                   | 2/38 [00:00<00:02, 13.71it/s]Epoch: 1, train for the 116-th batch, train loss: 0.60936039686203:  49%|██████▎      | 115/237 [00:27<00:41,  2.95it/s]evaluate for the 10-th batch, evaluate loss: 0.717072606086731:  43%|████████▌           | 9/21 [00:00<00:00, 13.06it/s]Epoch: 1, train for the 116-th batch, train loss: 0.60936039686203:  49%|██████▎      | 116/237 [00:27<00:36,  3.33it/s]evaluate for the 4-th batch, evaluate loss: 0.5501609444618225:   5%|█                   | 2/38 [00:00<00:02, 13.71it/s]evaluate for the 4-th batch, evaluate loss: 0.5501609444618225:  11%|██                  | 4/38 [00:00<00:02, 15.39it/s]evaluate for the 11-th batch, evaluate loss: 0.7007293701171875:  43%|████████▏          | 9/21 [00:00<00:00, 13.06it/s]evaluate for the 11-th batch, evaluate loss: 0.7007293701171875:  52%|█████████▍        | 11/21 [00:00<00:00, 13.63it/s]evaluate for the 5-th batch, evaluate loss: 0.6058274507522583:  11%|██                  | 4/38 [00:00<00:02, 15.39it/s]evaluate for the 6-th batch, evaluate loss: 0.5530862808227539:  11%|██                  | 4/38 [00:00<00:02, 15.39it/s]evaluate for the 6-th batch, evaluate loss: 0.5530862808227539:  16%|███▏                | 6/38 [00:00<00:02, 15.84it/s]evaluate for the 12-th batch, evaluate loss: 0.6998899579048157:  52%|█████████▍        | 11/21 [00:00<00:00, 13.63it/s]evaluate for the 7-th batch, evaluate loss: 0.5284314751625061:  16%|███▏                | 6/38 [00:00<00:02, 15.84it/s]Epoch: 1, train for the 128-th batch, train loss: 0.49555841088294983:  84%|████████▍ | 127/151 [00:28<00:07,  3.27it/s]Epoch: 1, train for the 139-th batch, train loss: 0.3729429543018341:  57%|██████▎    | 138/241 [00:27<00:23,  4.45it/s]evaluate for the 13-th batch, evaluate loss: 0.6742379069328308:  52%|█████████▍        | 11/21 [00:01<00:00, 13.63it/s]evaluate for the 13-th batch, evaluate loss: 0.6742379069328308:  62%|███████████▏      | 13/21 [00:01<00:00, 13.16it/s]Epoch: 1, train for the 128-th batch, train loss: 0.49555841088294983:  85%|████████▍ | 128/151 [00:28<00:07,  3.03it/s]Epoch: 1, train for the 139-th batch, train loss: 0.3729429543018341:  58%|██████▎    | 139/241 [00:27<00:29,  3.41it/s]evaluate for the 8-th batch, evaluate loss: 0.5351394414901733:  16%|███▏                | 6/38 [00:00<00:02, 15.84it/s]evaluate for the 8-th batch, evaluate loss: 0.5351394414901733:  21%|████▏               | 8/38 [00:00<00:01, 16.74it/s]evaluate for the 14-th batch, evaluate loss: 0.6191861629486084:  62%|███████████▏      | 13/21 [00:01<00:00, 13.16it/s]evaluate for the 9-th batch, evaluate loss: 0.555816113948822:  21%|████▍                | 8/38 [00:00<00:01, 16.74it/s]Epoch: 1, train for the 117-th batch, train loss: 0.5806381106376648:  49%|█████▍     | 116/237 [00:27<00:36,  3.33it/s]Epoch: 1, train for the 100-th batch, train loss: 0.4751662313938141:  26%|███         | 99/383 [00:25<01:21,  3.48it/s]Epoch: 1, train for the 117-th batch, train loss: 0.5806381106376648:  49%|█████▍     | 117/237 [00:27<00:38,  3.09it/s]Epoch: 1, train for the 100-th batch, train loss: 0.4751662313938141:  26%|██▊        | 100/383 [00:25<01:49,  2.59it/s]evaluate for the 10-th batch, evaluate loss: 0.5784605145454407:  21%|████               | 8/38 [00:00<00:01, 16.74it/s]evaluate for the 10-th batch, evaluate loss: 0.5784605145454407:  26%|████▋             | 10/38 [00:00<00:01, 14.96it/s]evaluate for the 15-th batch, evaluate loss: 0.6454917788505554:  62%|███████████▏      | 13/21 [00:01<00:00, 13.16it/s]evaluate for the 15-th batch, evaluate loss: 0.6454917788505554:  71%|████████████▊     | 15/21 [00:01<00:00, 12.29it/s]Epoch: 1, train for the 140-th batch, train loss: 0.36803102493286133:  58%|█████▊    | 139/241 [00:27<00:29,  3.41it/s]Epoch: 1, train for the 129-th batch, train loss: 0.49960944056510925:  85%|████████▍ | 128/151 [00:29<00:07,  3.03it/s]Epoch: 1, train for the 140-th batch, train loss: 0.36803102493286133:  58%|█████▊    | 140/241 [00:28<00:26,  3.74it/s]Epoch: 1, train for the 129-th batch, train loss: 0.49960944056510925:  85%|████████▌ | 129/151 [00:29<00:06,  3.31it/s]evaluate for the 11-th batch, evaluate loss: 0.5110001564025879:  26%|████▋             | 10/38 [00:00<00:01, 14.96it/s]evaluate for the 16-th batch, evaluate loss: 0.6082659959793091:  71%|████████████▊     | 15/21 [00:01<00:00, 12.29it/s]evaluate for the 12-th batch, evaluate loss: 0.5973603129386902:  26%|████▋             | 10/38 [00:00<00:01, 14.96it/s]evaluate for the 12-th batch, evaluate loss: 0.5973603129386902:  32%|█████▋            | 12/38 [00:00<00:01, 15.36it/s]Epoch: 1, train for the 118-th batch, train loss: 0.558142900466919:  49%|█████▉      | 117/237 [00:28<00:38,  3.09it/s]Epoch: 1, train for the 118-th batch, train loss: 0.558142900466919:  50%|█████▉      | 118/237 [00:28<00:33,  3.51it/s]evaluate for the 17-th batch, evaluate loss: 0.568446695804596:  71%|█████████████▌     | 15/21 [00:01<00:00, 12.29it/s]evaluate for the 17-th batch, evaluate loss: 0.568446695804596:  81%|███████████████▍   | 17/21 [00:01<00:00, 11.91it/s]Epoch: 1, train for the 130-th batch, train loss: 0.49454230070114136:  85%|████████▌ | 129/151 [00:29<00:06,  3.31it/s]Epoch: 1, train for the 130-th batch, train loss: 0.49454230070114136:  86%|████████▌ | 130/151 [00:29<00:05,  3.79it/s]evaluate for the 18-th batch, evaluate loss: 0.6092591285705566:  81%|██████████████▌   | 17/21 [00:01<00:00, 11.91it/s]evaluate for the 19-th batch, evaluate loss: 0.5733766555786133:  81%|██████████████▌   | 17/21 [00:01<00:00, 11.91it/s]evaluate for the 19-th batch, evaluate loss: 0.5733766555786133:  90%|████████████████▎ | 19/21 [00:01<00:00, 13.50it/s]evaluate for the 13-th batch, evaluate loss: 0.5419072508811951:  32%|█████▋            | 12/38 [00:00<00:01, 15.36it/s]evaluate for the 20-th batch, evaluate loss: 0.5370727181434631:  90%|████████████████▎ | 19/21 [00:01<00:00, 13.50it/s]evaluate for the 14-th batch, evaluate loss: 0.48126405477523804:  32%|█████▎           | 12/38 [00:01<00:01, 15.36it/s]evaluate for the 14-th batch, evaluate loss: 0.48126405477523804:  37%|██████▎          | 14/38 [00:01<00:02, 11.85it/s]evaluate for the 21-th batch, evaluate loss: 0.6719325184822083:  90%|████████████████▎ | 19/21 [00:01<00:00, 13.50it/s]evaluate for the 21-th batch, evaluate loss: 0.6719325184822083: 100%|██████████████████| 21/21 [00:01<00:00, 14.78it/s]evaluate for the 21-th batch, evaluate loss: 0.6719325184822083: 100%|██████████████████| 21/21 [00:01<00:00, 12.76it/s]
Epoch: 1, train for the 101-th batch, train loss: 0.525234580039978:  26%|███▏        | 100/383 [00:26<01:49,  2.59it/s]evaluate for the 15-th batch, evaluate loss: 0.5097483992576599:  37%|██████▋           | 14/38 [00:01<00:02, 11.85it/s]Epoch: 1, train for the 101-th batch, train loss: 0.525234580039978:  26%|███▏        | 101/383 [00:26<02:00,  2.35it/s]Epoch: 1, train for the 119-th batch, train loss: 0.5637397766113281:  50%|█████▍     | 118/237 [00:28<00:33,  3.51it/s]Epoch: 1, train for the 141-th batch, train loss: 0.47179460525512695:  58%|█████▊    | 140/241 [00:28<00:26,  3.74it/s]Epoch: 1, train for the 131-th batch, train loss: 0.4200541079044342:  86%|█████████▍ | 130/151 [00:29<00:05,  3.79it/s]Epoch: 1, train for the 141-th batch, train loss: 0.47179460525512695:  59%|█████▊    | 141/241 [00:28<00:33,  3.00it/s]Epoch: 1, train for the 119-th batch, train loss: 0.5637397766113281:  50%|█████▌     | 119/237 [00:28<00:36,  3.24it/s]Epoch: 1, train for the 131-th batch, train loss: 0.4200541079044342:  87%|█████████▌ | 131/151 [00:29<00:05,  3.66it/s]evaluate for the 16-th batch, evaluate loss: 0.5721773505210876:  37%|██████▋           | 14/38 [00:01<00:02, 11.85it/s]evaluate for the 16-th batch, evaluate loss: 0.5721773505210876:  42%|███████▌          | 16/38 [00:01<00:01, 11.32it/s]INFO:root:Epoch: 1, learning rate: 0.0001, train loss: 0.5065
INFO:root:train average_precision, 0.8458
INFO:root:train roc_auc, 0.8492
INFO:root:validate loss: 0.5888
INFO:root:validate average_precision, 0.7716
INFO:root:validate roc_auc, 0.7701
INFO:root:new node validate loss: 0.7022
INFO:root:new node validate first_1_average_precision, 0.7675
INFO:root:new node validate first_1_roc_auc, 0.7994
INFO:root:new node validate first_3_average_precision, 0.7333
INFO:root:new node validate first_3_roc_auc, 0.7407
INFO:root:new node validate first_10_average_precision, 0.6739
INFO:root:new node validate first_10_roc_auc, 0.6777
INFO:root:new node validate average_precision, 0.6266
INFO:root:new node validate roc_auc, 0.6480
INFO:root:save model ./saved_models/TGN/ia-reality-call/TGN_seed0_tgn-ia-reality-call-reparamcorr-time-mlp/TGN_seed0_tgn-ia-reality-call-reparamcorr-time-mlp.pkl
evaluate for the 17-th batch, evaluate loss: 0.5223255753517151:  42%|███████▌          | 16/38 [00:01<00:01, 11.32it/s]Epoch: 1, train for the 142-th batch, train loss: 0.47168952226638794:  59%|█████▊    | 141/241 [00:28<00:33,  3.00it/s]evaluate for the 18-th batch, evaluate loss: 0.5610979199409485:  42%|███████▌          | 16/38 [00:01<00:01, 11.32it/s]evaluate for the 18-th batch, evaluate loss: 0.5610979199409485:  47%|████████▌         | 18/38 [00:01<00:01, 12.64it/s]Epoch: 1, train for the 142-th batch, train loss: 0.47168952226638794:  59%|█████▉    | 142/241 [00:28<00:28,  3.53it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5770485401153564:  50%|█████▌     | 119/237 [00:28<00:36,  3.24it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5770485401153564:  51%|█████▌     | 120/237 [00:28<00:33,  3.54it/s]evaluate for the 19-th batch, evaluate loss: 0.5336396098136902:  47%|████████▌         | 18/38 [00:01<00:01, 12.64it/s]Epoch: 1, train for the 132-th batch, train loss: 0.4984026253223419:  87%|█████████▌ | 131/151 [00:29<00:05,  3.66it/s]Epoch: 1, train for the 132-th batch, train loss: 0.4984026253223419:  87%|█████████▌ | 132/151 [00:29<00:05,  3.72it/s]Epoch: 1, train for the 102-th batch, train loss: 0.45702409744262695:  26%|██▋       | 101/383 [00:26<02:00,  2.35it/s]evaluate for the 20-th batch, evaluate loss: 0.48577558994293213:  47%|████████         | 18/38 [00:01<00:01, 12.64it/s]evaluate for the 20-th batch, evaluate loss: 0.48577558994293213:  53%|████████▉        | 20/38 [00:01<00:01, 13.47it/s]Epoch: 1, train for the 102-th batch, train loss: 0.45702409744262695:  27%|██▋       | 102/383 [00:26<01:51,  2.53it/s]Epoch: 1, train for the 103-th batch, train loss: 0.2917593717575073:  27%|██▉        | 102/383 [00:26<01:51,  2.53it/s]Epoch: 1, train for the 103-th batch, train loss: 0.2917593717575073:  27%|██▉        | 103/383 [00:26<01:31,  3.06it/s]evaluate for the 21-th batch, evaluate loss: 0.4909835755825043:  53%|█████████▍        | 20/38 [00:01<00:01, 13.47it/s]evaluate for the 22-th batch, evaluate loss: 0.5340918898582458:  53%|█████████▍        | 20/38 [00:01<00:01, 13.47it/s]evaluate for the 22-th batch, evaluate loss: 0.5340918898582458:  58%|██████████▍       | 22/38 [00:01<00:01, 10.93it/s]Epoch: 1, train for the 121-th batch, train loss: 0.6074153780937195:  51%|█████▌     | 120/237 [00:29<00:33,  3.54it/s]Epoch: 1, train for the 143-th batch, train loss: 0.4451880156993866:  59%|██████▍    | 142/241 [00:29<00:28,  3.53it/s]Epoch: 1, train for the 133-th batch, train loss: 0.4835052788257599:  87%|█████████▌ | 132/151 [00:30<00:05,  3.72it/s]Epoch: 1, train for the 143-th batch, train loss: 0.4451880156993866:  59%|██████▌    | 143/241 [00:29<00:31,  3.08it/s]Epoch: 1, train for the 121-th batch, train loss: 0.6074153780937195:  51%|█████▌     | 121/237 [00:29<00:35,  3.26it/s]evaluate for the 23-th batch, evaluate loss: 0.5210933089256287:  58%|██████████▍       | 22/38 [00:01<00:01, 10.93it/s]Epoch: 1, train for the 133-th batch, train loss: 0.4835052788257599:  88%|█████████▋ | 133/151 [00:30<00:05,  3.49it/s]Epoch: 1, train for the 104-th batch, train loss: 0.3321417272090912:  27%|██▉        | 103/383 [00:26<01:31,  3.06it/s]evaluate for the 24-th batch, evaluate loss: 0.5183468461036682:  58%|██████████▍       | 22/38 [00:01<00:01, 10.93it/s]Epoch: 1, train for the 104-th batch, train loss: 0.3321417272090912:  27%|██▉        | 104/383 [00:27<01:20,  3.46it/s]evaluate for the 25-th batch, evaluate loss: 0.5374428629875183:  58%|██████████▍       | 22/38 [00:01<00:01, 10.93it/s]evaluate for the 25-th batch, evaluate loss: 0.5374428629875183:  66%|███████████▊      | 25/38 [00:01<00:00, 13.31it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]Epoch: 1, train for the 144-th batch, train loss: 0.5647733211517334:  59%|██████▌    | 143/241 [00:29<00:31,  3.08it/s]Epoch: 1, train for the 144-th batch, train loss: 0.5647733211517334:  60%|██████▌    | 144/241 [00:29<00:26,  3.60it/s]Epoch: 1, train for the 134-th batch, train loss: 0.4485412836074829:  88%|█████████▋ | 133/151 [00:30<00:05,  3.49it/s]Epoch: 1, train for the 134-th batch, train loss: 0.4485412836074829:  89%|█████████▊ | 134/151 [00:30<00:04,  3.88it/s]Epoch: 2, train for the 1-th batch, train loss: 0.8745275735855103:   0%|                       | 0/119 [00:00<?, ?it/s]Epoch: 2, train for the 1-th batch, train loss: 0.8745275735855103:   1%|▏              | 1/119 [00:00<00:24,  4.91it/s]evaluate for the 26-th batch, evaluate loss: 0.5225049257278442:  66%|███████████▊      | 25/38 [00:02<00:00, 13.31it/s]Epoch: 1, train for the 122-th batch, train loss: 0.582395076751709:  51%|██████▏     | 121/237 [00:29<00:35,  3.26it/s]Epoch: 1, train for the 145-th batch, train loss: 0.6327762007713318:  60%|██████▌    | 144/241 [00:29<00:26,  3.60it/s]Epoch: 1, train for the 135-th batch, train loss: 0.4368552267551422:  89%|█████████▊ | 134/151 [00:30<00:04,  3.88it/s]Epoch: 1, train for the 122-th batch, train loss: 0.582395076751709:  51%|██████▏     | 122/237 [00:29<00:40,  2.83it/s]Epoch: 1, train for the 145-th batch, train loss: 0.6327762007713318:  60%|██████▌    | 145/241 [00:29<00:27,  3.51it/s]Epoch: 2, train for the 2-th batch, train loss: 0.9168765544891357:   1%|▏              | 1/119 [00:00<00:24,  4.91it/s]Epoch: 1, train for the 135-th batch, train loss: 0.4368552267551422:  89%|█████████▊ | 135/151 [00:30<00:04,  3.78it/s]Epoch: 2, train for the 2-th batch, train loss: 0.9168765544891357:   2%|▎              | 2/119 [00:00<00:19,  6.11it/s]evaluate for the 27-th batch, evaluate loss: 0.5454912185668945:  66%|███████████▊      | 25/38 [00:02<00:00, 13.31it/s]evaluate for the 27-th batch, evaluate loss: 0.5454912185668945:  71%|████████████▊     | 27/38 [00:02<00:01,  9.52it/s]Epoch: 1, train for the 105-th batch, train loss: 0.2642427384853363:  27%|██▉        | 104/383 [00:27<01:20,  3.46it/s]Epoch: 1, train for the 105-th batch, train loss: 0.2642427384853363:  27%|███        | 105/383 [00:27<01:36,  2.89it/s]evaluate for the 28-th batch, evaluate loss: 0.5608546137809753:  71%|████████████▊     | 27/38 [00:02<00:01,  9.52it/s]Epoch: 1, train for the 123-th batch, train loss: 0.5342811942100525:  51%|█████▋     | 122/237 [00:29<00:40,  2.83it/s]Epoch: 2, train for the 3-th batch, train loss: 0.7188558578491211:   2%|▎              | 2/119 [00:00<00:19,  6.11it/s]evaluate for the 29-th batch, evaluate loss: 0.5306537747383118:  71%|████████████▊     | 27/38 [00:02<00:01,  9.52it/s]evaluate for the 29-th batch, evaluate loss: 0.5306537747383118:  76%|█████████████▋    | 29/38 [00:02<00:00, 10.08it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5758134722709656:  60%|██████▌    | 145/241 [00:29<00:27,  3.51it/s]Epoch: 2, train for the 3-th batch, train loss: 0.7188558578491211:   3%|▍              | 3/119 [00:00<00:19,  5.97it/s]Epoch: 1, train for the 123-th batch, train loss: 0.5342811942100525:  52%|█████▋     | 123/237 [00:29<00:35,  3.22it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5758134722709656:  61%|██████▋    | 146/241 [00:29<00:24,  3.84it/s]Epoch: 1, train for the 136-th batch, train loss: 0.5078983306884766:  89%|█████████▊ | 135/151 [00:30<00:04,  3.78it/s]Epoch: 1, train for the 136-th batch, train loss: 0.5078983306884766:  90%|█████████▉ | 136/151 [00:30<00:03,  3.93it/s]evaluate for the 30-th batch, evaluate loss: 0.5472373366355896:  76%|█████████████▋    | 29/38 [00:02<00:00, 10.08it/s]Epoch: 1, train for the 106-th batch, train loss: 0.313896119594574:  27%|███▎        | 105/383 [00:27<01:36,  2.89it/s]Epoch: 1, train for the 106-th batch, train loss: 0.313896119594574:  28%|███▎        | 106/383 [00:27<01:25,  3.24it/s]Epoch: 2, train for the 4-th batch, train loss: 0.6476885676383972:   3%|▍              | 3/119 [00:00<00:19,  5.97it/s]Epoch: 2, train for the 4-th batch, train loss: 0.6476885676383972:   3%|▌              | 4/119 [00:00<00:18,  6.37it/s]evaluate for the 31-th batch, evaluate loss: 0.5342861413955688:  76%|█████████████▋    | 29/38 [00:02<00:00, 10.08it/s]evaluate for the 31-th batch, evaluate loss: 0.5342861413955688:  82%|██████████████▋   | 31/38 [00:02<00:00,  9.97it/s]Epoch: 1, train for the 137-th batch, train loss: 0.4645654261112213:  90%|█████████▉ | 136/151 [00:31<00:03,  3.93it/s]Epoch: 1, train for the 137-th batch, train loss: 0.4645654261112213:  91%|█████████▉ | 137/151 [00:31<00:03,  4.22it/s]Epoch: 2, train for the 5-th batch, train loss: 0.5732473134994507:   3%|▌              | 4/119 [00:00<00:18,  6.37it/s]Epoch: 2, train for the 5-th batch, train loss: 0.5732473134994507:   4%|▋              | 5/119 [00:00<00:16,  6.91it/s]Epoch: 1, train for the 107-th batch, train loss: 0.4598414897918701:  28%|███        | 106/383 [00:27<01:25,  3.24it/s]Epoch: 1, train for the 107-th batch, train loss: 0.4598414897918701:  28%|███        | 107/383 [00:27<01:13,  3.77it/s]Epoch: 2, train for the 6-th batch, train loss: 0.5997532606124878:   4%|▋              | 5/119 [00:00<00:16,  6.91it/s]Epoch: 1, train for the 124-th batch, train loss: 0.5297577381134033:  52%|█████▋     | 123/237 [00:30<00:35,  3.22it/s]Epoch: 1, train for the 138-th batch, train loss: 0.43864020705223083:  91%|█████████ | 137/151 [00:31<00:03,  4.22it/s]Epoch: 1, train for the 138-th batch, train loss: 0.43864020705223083:  91%|█████████▏| 138/151 [00:31<00:02,  4.85it/s]Epoch: 1, train for the 124-th batch, train loss: 0.5297577381134033:  52%|█████▊     | 124/237 [00:30<00:37,  3.05it/s]Epoch: 1, train for the 147-th batch, train loss: 0.5409543514251709:  61%|██████▋    | 146/241 [00:30<00:24,  3.84it/s]Epoch: 1, train for the 108-th batch, train loss: 0.47382497787475586:  28%|██▊       | 107/383 [00:28<01:13,  3.77it/s]Epoch: 1, train for the 147-th batch, train loss: 0.5409543514251709:  61%|██████▋    | 147/241 [00:30<00:30,  3.08it/s]evaluate for the 32-th batch, evaluate loss: 0.5007243752479553:  82%|██████████████▋   | 31/38 [00:02<00:00,  9.97it/s]Epoch: 2, train for the 7-th batch, train loss: 0.592600405216217:   4%|▋               | 5/119 [00:01<00:16,  6.91it/s]Epoch: 1, train for the 108-th batch, train loss: 0.47382497787475586:  28%|██▊       | 108/383 [00:28<01:08,  3.99it/s]Epoch: 2, train for the 7-th batch, train loss: 0.592600405216217:   6%|▉               | 7/119 [00:01<00:14,  7.51it/s]Epoch: 1, train for the 139-th batch, train loss: 0.46944135427474976:  91%|█████████▏| 138/151 [00:31<00:02,  4.85it/s]Epoch: 1, train for the 139-th batch, train loss: 0.46944135427474976:  92%|█████████▏| 139/151 [00:31<00:02,  5.27it/s]Epoch: 1, train for the 125-th batch, train loss: 0.5197300910949707:  52%|█████▊     | 124/237 [00:30<00:37,  3.05it/s]evaluate for the 33-th batch, evaluate loss: 0.5192370414733887:  82%|██████████████▋   | 31/38 [00:02<00:00,  9.97it/s]evaluate for the 33-th batch, evaluate loss: 0.5192370414733887:  87%|███████████████▋  | 33/38 [00:02<00:00,  8.03it/s]Epoch: 1, train for the 125-th batch, train loss: 0.5197300910949707:  53%|█████▊     | 125/237 [00:30<00:31,  3.51it/s]Epoch: 2, train for the 8-th batch, train loss: 0.5421689748764038:   6%|▉              | 7/119 [00:01<00:14,  7.51it/s]evaluate for the 34-th batch, evaluate loss: 0.5341055989265442:  87%|███████████████▋  | 33/38 [00:03<00:00,  8.03it/s]evaluate for the 34-th batch, evaluate loss: 0.5341055989265442:  89%|████████████████  | 34/38 [00:03<00:00,  7.88it/s]Epoch: 1, train for the 148-th batch, train loss: 0.5398421883583069:  61%|██████▋    | 147/241 [00:30<00:30,  3.08it/s]Epoch: 2, train for the 8-th batch, train loss: 0.5421689748764038:   7%|█              | 8/119 [00:01<00:16,  6.53it/s]Epoch: 1, train for the 109-th batch, train loss: 0.4005748927593231:  28%|███        | 108/383 [00:28<01:08,  3.99it/s]Epoch: 1, train for the 148-th batch, train loss: 0.5398421883583069:  61%|██████▊    | 148/241 [00:30<00:27,  3.38it/s]Epoch: 1, train for the 109-th batch, train loss: 0.4005748927593231:  28%|███▏       | 109/383 [00:28<01:07,  4.03it/s]Epoch: 2, train for the 9-th batch, train loss: 0.539023756980896:   7%|█               | 8/119 [00:01<00:16,  6.53it/s]Epoch: 2, train for the 9-th batch, train loss: 0.539023756980896:   8%|█▏              | 9/119 [00:01<00:15,  7.15it/s]Epoch: 1, train for the 110-th batch, train loss: 0.3862537443637848:  28%|███▏       | 109/383 [00:28<01:07,  4.03it/s]Epoch: 1, train for the 110-th batch, train loss: 0.3862537443637848:  29%|███▏       | 110/383 [00:28<01:00,  4.53it/s]Epoch: 1, train for the 140-th batch, train loss: 0.43383559584617615:  92%|█████████▏| 139/151 [00:31<00:02,  5.27it/s]Epoch: 2, train for the 10-th batch, train loss: 0.5717608332633972:   8%|█             | 9/119 [00:01<00:15,  7.15it/s]Epoch: 2, train for the 10-th batch, train loss: 0.5717608332633972:   8%|█            | 10/119 [00:01<00:14,  7.75it/s]Epoch: 1, train for the 140-th batch, train loss: 0.43383559584617615:  93%|█████████▎| 140/151 [00:31<00:02,  3.99it/s]Epoch: 2, train for the 11-th batch, train loss: 0.5032495856285095:   8%|█            | 10/119 [00:01<00:14,  7.75it/s]Epoch: 2, train for the 11-th batch, train loss: 0.5032495856285095:   9%|█▏           | 11/119 [00:01<00:13,  7.76it/s]evaluate for the 35-th batch, evaluate loss: 0.554679274559021:  89%|█████████████████  | 34/38 [00:03<00:00,  7.88it/s]evaluate for the 35-th batch, evaluate loss: 0.554679274559021:  92%|█████████████████▌ | 35/38 [00:03<00:00,  5.90it/s]Epoch: 1, train for the 149-th batch, train loss: 0.43423232436180115:  61%|██████▏   | 148/241 [00:30<00:27,  3.38it/s]Epoch: 1, train for the 149-th batch, train loss: 0.43423232436180115:  62%|██████▏   | 149/241 [00:30<00:29,  3.13it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5468147397041321:  53%|█████▊     | 125/237 [00:30<00:31,  3.51it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5248180627822876:  29%|███▏       | 110/383 [00:28<01:00,  4.53it/s]Epoch: 1, train for the 141-th batch, train loss: 0.4409564137458801:  93%|██████████▏| 140/151 [00:31<00:02,  3.99it/s]evaluate for the 36-th batch, evaluate loss: 0.5861547589302063:  92%|████████████████▌ | 35/38 [00:03<00:00,  5.90it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5468147397041321:  53%|█████▊     | 126/237 [00:30<00:40,  2.74it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5248180627822876:  29%|███▏       | 111/383 [00:28<01:00,  4.49it/s]Epoch: 1, train for the 141-th batch, train loss: 0.4409564137458801:  93%|██████████▎| 141/151 [00:31<00:02,  4.21it/s]Epoch: 2, train for the 12-th batch, train loss: 0.48897498846054077:   9%|█           | 11/119 [00:01<00:13,  7.76it/s]Epoch: 2, train for the 12-th batch, train loss: 0.48897498846054077:  10%|█▏          | 12/119 [00:01<00:14,  7.21it/s]evaluate for the 37-th batch, evaluate loss: 0.5005275011062622:  92%|████████████████▌ | 35/38 [00:03<00:00,  5.90it/s]evaluate for the 37-th batch, evaluate loss: 0.5005275011062622:  97%|█████████████████▌| 37/38 [00:03<00:00,  7.27it/s]Epoch: 1, train for the 150-th batch, train loss: 0.49931657314300537:  62%|██████▏   | 149/241 [00:30<00:29,  3.13it/s]Epoch: 1, train for the 150-th batch, train loss: 0.49931657314300537:  62%|██████▏   | 150/241 [00:31<00:24,  3.65it/s]Epoch: 1, train for the 127-th batch, train loss: 0.5749208927154541:  53%|█████▊     | 126/237 [00:31<00:40,  2.74it/s]Epoch: 1, train for the 127-th batch, train loss: 0.5749208927154541:  54%|█████▉     | 127/237 [00:31<00:34,  3.22it/s]Epoch: 2, train for the 13-th batch, train loss: 0.44524210691452026:  10%|█▏          | 12/119 [00:01<00:14,  7.21it/s]Epoch: 2, train for the 13-th batch, train loss: 0.44524210691452026:  11%|█▎          | 13/119 [00:01<00:14,  7.39it/s]evaluate for the 38-th batch, evaluate loss: 0.5267598628997803:  97%|█████████████████▌| 37/38 [00:03<00:00,  7.27it/s]evaluate for the 38-th batch, evaluate loss: 0.5267598628997803: 100%|██████████████████| 38/38 [00:03<00:00,  7.32it/s]evaluate for the 38-th batch, evaluate loss: 0.5267598628997803: 100%|██████████████████| 38/38 [00:03<00:00, 10.10it/s]
Epoch: 1, train for the 142-th batch, train loss: 0.44013193249702454:  93%|█████████▎| 141/151 [00:32<00:02,  4.21it/s]Epoch: 1, train for the 112-th batch, train loss: 0.46812689304351807:  29%|██▉       | 111/383 [00:28<01:00,  4.49it/s]Epoch: 1, train for the 142-th batch, train loss: 0.44013193249702454:  94%|█████████▍| 142/151 [00:32<00:02,  4.10it/s]Epoch: 1, train for the 112-th batch, train loss: 0.46812689304351807:  29%|██▉       | 112/383 [00:28<01:03,  4.24it/s]Epoch: 2, train for the 14-th batch, train loss: 0.42233991622924805:  11%|█▎          | 13/119 [00:01<00:14,  7.39it/s]Epoch: 2, train for the 14-th batch, train loss: 0.42233991622924805:  12%|█▍          | 14/119 [00:01<00:14,  7.47it/s]Epoch: 1, train for the 151-th batch, train loss: 0.4339141845703125:  62%|██████▊    | 150/241 [00:31<00:24,  3.65it/s]Epoch: 1, train for the 128-th batch, train loss: 0.537352442741394:  54%|██████▍     | 127/237 [00:31<00:34,  3.22it/s]Epoch: 1, train for the 151-th batch, train loss: 0.4339141845703125:  63%|██████▉    | 151/241 [00:31<00:24,  3.71it/s]Epoch: 1, train for the 128-th batch, train loss: 0.537352442741394:  54%|██████▍     | 128/237 [00:31<00:30,  3.53it/s]Epoch: 1, train for the 113-th batch, train loss: 0.48577430844306946:  29%|██▉       | 112/383 [00:29<01:03,  4.24it/s]Epoch: 1, train for the 113-th batch, train loss: 0.48577430844306946:  30%|██▉       | 113/383 [00:29<01:00,  4.45it/s]Epoch: 2, train for the 15-th batch, train loss: 0.4351179003715515:  12%|█▌           | 14/119 [00:02<00:14,  7.47it/s]Epoch: 2, train for the 15-th batch, train loss: 0.4351179003715515:  13%|█▋           | 15/119 [00:02<00:14,  7.42it/s]Epoch: 1, train for the 143-th batch, train loss: 0.4237678349018097:  94%|██████████▎| 142/151 [00:32<00:02,  4.10it/s]  0%|                                                                                            | 0/20 [00:00<?, ?it/s]Epoch: 1, train for the 143-th batch, train loss: 0.4237678349018097:  95%|██████████▍| 143/151 [00:32<00:01,  4.07it/s]evaluate for the 1-th batch, evaluate loss: 0.7935231328010559:   0%|                            | 0/20 [00:00<?, ?it/s]Epoch: 2, train for the 16-th batch, train loss: 0.45137548446655273:  13%|█▌          | 15/119 [00:02<00:14,  7.42it/s]Epoch: 1, train for the 114-th batch, train loss: 0.37137869000434875:  30%|██▉       | 113/383 [00:29<01:00,  4.45it/s]Epoch: 2, train for the 16-th batch, train loss: 0.45137548446655273:  13%|█▌          | 16/119 [00:02<00:15,  6.80it/s]evaluate for the 2-th batch, evaluate loss: 0.8133440017700195:   0%|                            | 0/20 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.8133440017700195:  10%|██                  | 2/20 [00:00<00:01, 13.12it/s]Epoch: 1, train for the 114-th batch, train loss: 0.37137869000434875:  30%|██▉       | 114/383 [00:29<00:59,  4.52it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5860702991485596:  54%|█████▉     | 128/237 [00:31<00:30,  3.53it/s]Epoch: 1, train for the 152-th batch, train loss: 0.33297961950302124:  63%|██████▎   | 151/241 [00:31<00:24,  3.71it/s]Epoch: 1, train for the 152-th batch, train loss: 0.33297961950302124:  63%|██████▎   | 152/241 [00:31<00:25,  3.51it/s]Epoch: 1, train for the 144-th batch, train loss: 0.4027262032032013:  95%|██████████▍| 143/151 [00:32<00:01,  4.07it/s]evaluate for the 3-th batch, evaluate loss: 0.716266930103302:  10%|██                   | 2/20 [00:00<00:01, 13.12it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5860702991485596:  54%|█████▉     | 129/237 [00:31<00:31,  3.38it/s]Epoch: 1, train for the 144-th batch, train loss: 0.4027262032032013:  95%|██████████▍| 144/151 [00:32<00:01,  4.17it/s]Epoch: 2, train for the 17-th batch, train loss: 0.3894606828689575:  13%|█▋           | 16/119 [00:02<00:15,  6.80it/s]Epoch: 2, train for the 17-th batch, train loss: 0.3894606828689575:  14%|█▊           | 17/119 [00:02<00:15,  6.75it/s]evaluate for the 4-th batch, evaluate loss: 0.7147343754768372:  10%|██                  | 2/20 [00:00<00:01, 13.12it/s]evaluate for the 4-th batch, evaluate loss: 0.7147343754768372:  20%|████                | 4/20 [00:00<00:01, 13.10it/s]evaluate for the 5-th batch, evaluate loss: 0.7188299894332886:  20%|████                | 4/20 [00:00<00:01, 13.10it/s]Epoch: 1, train for the 115-th batch, train loss: 0.40243399143218994:  30%|██▉       | 114/383 [00:29<00:59,  4.52it/s]Epoch: 1, train for the 115-th batch, train loss: 0.40243399143218994:  30%|███       | 115/383 [00:29<00:59,  4.47it/s]Epoch: 2, train for the 18-th batch, train loss: 0.42934226989746094:  14%|█▋          | 17/119 [00:02<00:15,  6.75it/s]Epoch: 2, train for the 18-th batch, train loss: 0.42934226989746094:  15%|█▊          | 18/119 [00:02<00:15,  6.33it/s]evaluate for the 6-th batch, evaluate loss: 0.7193459868431091:  20%|████                | 4/20 [00:00<00:01, 13.10it/s]evaluate for the 6-th batch, evaluate loss: 0.7193459868431091:  30%|██████              | 6/20 [00:00<00:01, 12.29it/s]evaluate for the 7-th batch, evaluate loss: 0.7500567436218262:  30%|██████              | 6/20 [00:00<00:01, 12.29it/s]Epoch: 1, train for the 153-th batch, train loss: 0.41122329235076904:  63%|██████▎   | 152/241 [00:31<00:25,  3.51it/s]Epoch: 1, train for the 145-th batch, train loss: 0.4240501821041107:  95%|██████████▍| 144/151 [00:33<00:01,  4.17it/s]Epoch: 1, train for the 116-th batch, train loss: 0.4177771806716919:  30%|███▎       | 115/383 [00:29<00:59,  4.47it/s]Epoch: 1, train for the 153-th batch, train loss: 0.41122329235076904:  63%|██████▎   | 153/241 [00:31<00:27,  3.17it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5709143280982971:  54%|█████▉     | 129/237 [00:32<00:31,  3.38it/s]Epoch: 1, train for the 145-th batch, train loss: 0.4240501821041107:  96%|██████████▌| 145/151 [00:33<00:01,  3.55it/s]Epoch: 1, train for the 116-th batch, train loss: 0.4177771806716919:  30%|███▎       | 116/383 [00:29<00:59,  4.48it/s]Epoch: 2, train for the 19-th batch, train loss: 0.45115184783935547:  15%|█▊          | 18/119 [00:02<00:15,  6.33it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5709143280982971:  55%|██████     | 130/237 [00:32<00:35,  3.01it/s]Epoch: 2, train for the 19-th batch, train loss: 0.45115184783935547:  16%|█▉          | 19/119 [00:02<00:15,  6.32it/s]evaluate for the 8-th batch, evaluate loss: 0.7073135375976562:  30%|██████              | 6/20 [00:00<00:01, 12.29it/s]evaluate for the 8-th batch, evaluate loss: 0.7073135375976562:  40%|████████            | 8/20 [00:00<00:00, 12.33it/s]evaluate for the 9-th batch, evaluate loss: 0.648526132106781:  40%|████████▍            | 8/20 [00:00<00:00, 12.33it/s]Epoch: 1, train for the 154-th batch, train loss: 0.32617220282554626:  63%|██████▎   | 153/241 [00:32<00:27,  3.17it/s]Epoch: 1, train for the 154-th batch, train loss: 0.32617220282554626:  64%|██████▍   | 154/241 [00:32<00:23,  3.76it/s]Epoch: 1, train for the 146-th batch, train loss: 0.4017532169818878:  96%|██████████▌| 145/151 [00:33<00:01,  3.55it/s]Epoch: 2, train for the 20-th batch, train loss: 0.3913055658340454:  16%|██           | 19/119 [00:02<00:15,  6.32it/s]Epoch: 2, train for the 20-th batch, train loss: 0.3913055658340454:  17%|██▏          | 20/119 [00:02<00:15,  6.54it/s]Epoch: 1, train for the 146-th batch, train loss: 0.4017532169818878:  97%|██████████▋| 146/151 [00:33<00:01,  3.99it/s]evaluate for the 10-th batch, evaluate loss: 0.6382597088813782:  40%|███████▌           | 8/20 [00:00<00:00, 12.33it/s]evaluate for the 10-th batch, evaluate loss: 0.6382597088813782:  50%|█████████         | 10/20 [00:00<00:00, 12.22it/s]Epoch: 1, train for the 131-th batch, train loss: 0.5402261018753052:  55%|██████     | 130/237 [00:32<00:35,  3.01it/s]Epoch: 1, train for the 131-th batch, train loss: 0.5402261018753052:  55%|██████     | 131/237 [00:32<00:32,  3.25it/s]evaluate for the 11-th batch, evaluate loss: 0.6435986161231995:  50%|█████████         | 10/20 [00:00<00:00, 12.22it/s]Epoch: 2, train for the 21-th batch, train loss: 0.4549035131931305:  17%|██▏          | 20/119 [00:03<00:15,  6.54it/s]Epoch: 2, train for the 21-th batch, train loss: 0.4549035131931305:  18%|██▎          | 21/119 [00:03<00:14,  6.58it/s]evaluate for the 12-th batch, evaluate loss: 0.6836167573928833:  50%|█████████         | 10/20 [00:00<00:00, 12.22it/s]evaluate for the 12-th batch, evaluate loss: 0.6836167573928833:  60%|██████████▊       | 12/20 [00:00<00:00, 11.86it/s]Epoch: 2, train for the 22-th batch, train loss: 0.44843268394470215:  18%|██          | 21/119 [00:03<00:14,  6.58it/s]evaluate for the 13-th batch, evaluate loss: 0.7007296085357666:  60%|██████████▊       | 12/20 [00:01<00:00, 11.86it/s]Epoch: 2, train for the 22-th batch, train loss: 0.44843268394470215:  18%|██▏         | 22/119 [00:03<00:15,  6.41it/s]Epoch: 1, train for the 117-th batch, train loss: 0.32716381549835205:  30%|███       | 116/383 [00:30<00:59,  4.48it/s]evaluate for the 14-th batch, evaluate loss: 0.7108425498008728:  60%|██████████▊       | 12/20 [00:01<00:00, 11.86it/s]evaluate for the 14-th batch, evaluate loss: 0.7108425498008728:  70%|████████████▌     | 14/20 [00:01<00:00, 11.79it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5827006697654724:  55%|██████     | 131/237 [00:32<00:32,  3.25it/s]Epoch: 1, train for the 147-th batch, train loss: 0.43209680914878845:  97%|█████████▋| 146/151 [00:33<00:01,  3.99it/s]Epoch: 1, train for the 155-th batch, train loss: 0.36962658166885376:  64%|██████▍   | 154/241 [00:32<00:23,  3.76it/s]Epoch: 1, train for the 117-th batch, train loss: 0.32716381549835205:  31%|███       | 117/383 [00:30<01:25,  3.11it/s]Epoch: 1, train for the 155-th batch, train loss: 0.36962658166885376:  64%|██████▍   | 155/241 [00:32<00:27,  3.15it/s]Epoch: 1, train for the 147-th batch, train loss: 0.43209680914878845:  97%|█████████▋| 147/151 [00:33<00:01,  3.38it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5827006697654724:  56%|██████▏    | 132/237 [00:32<00:33,  3.17it/s]Epoch: 2, train for the 23-th batch, train loss: 0.46988576650619507:  18%|██▏         | 22/119 [00:03<00:15,  6.41it/s]Epoch: 2, train for the 23-th batch, train loss: 0.46988576650619507:  19%|██▎         | 23/119 [00:03<00:14,  6.54it/s]evaluate for the 15-th batch, evaluate loss: 0.6940327882766724:  70%|████████████▌     | 14/20 [00:01<00:00, 11.79it/s]evaluate for the 16-th batch, evaluate loss: 0.6489071249961853:  70%|████████████▌     | 14/20 [00:01<00:00, 11.79it/s]evaluate for the 16-th batch, evaluate loss: 0.6489071249961853:  80%|██████████████▍   | 16/20 [00:01<00:00, 11.51it/s]Epoch: 1, train for the 156-th batch, train loss: 0.28705352544784546:  64%|██████▍   | 155/241 [00:32<00:27,  3.15it/s]Epoch: 1, train for the 156-th batch, train loss: 0.28705352544784546:  65%|██████▍   | 156/241 [00:32<00:23,  3.56it/s]Epoch: 1, train for the 148-th batch, train loss: 0.3904609680175781:  97%|██████████▋| 147/151 [00:33<00:01,  3.38it/s]Epoch: 1, train for the 148-th batch, train loss: 0.3904609680175781:  98%|██████████▊| 148/151 [00:33<00:00,  3.70it/s]Epoch: 2, train for the 24-th batch, train loss: 0.43397536873817444:  19%|██▎         | 23/119 [00:03<00:14,  6.54it/s]Epoch: 2, train for the 24-th batch, train loss: 0.43397536873817444:  20%|██▍         | 24/119 [00:03<00:15,  6.20it/s]evaluate for the 17-th batch, evaluate loss: 0.6963125467300415:  80%|██████████████▍   | 16/20 [00:01<00:00, 11.51it/s]evaluate for the 18-th batch, evaluate loss: 0.6974314451217651:  80%|██████████████▍   | 16/20 [00:01<00:00, 11.51it/s]evaluate for the 18-th batch, evaluate loss: 0.6974314451217651:  90%|████████████████▏ | 18/20 [00:01<00:00, 11.71it/s]Epoch: 2, train for the 25-th batch, train loss: 0.4101068079471588:  20%|██▌          | 24/119 [00:03<00:15,  6.20it/s]Epoch: 2, train for the 25-th batch, train loss: 0.4101068079471588:  21%|██▋          | 25/119 [00:03<00:15,  6.22it/s]evaluate for the 19-th batch, evaluate loss: 0.7255312204360962:  90%|████████████████▏ | 18/20 [00:01<00:00, 11.71it/s]evaluate for the 20-th batch, evaluate loss: 0.7078246474266052:  90%|████████████████▏ | 18/20 [00:01<00:00, 11.71it/s]evaluate for the 20-th batch, evaluate loss: 0.7078246474266052: 100%|██████████████████| 20/20 [00:01<00:00, 12.72it/s]evaluate for the 20-th batch, evaluate loss: 0.7078246474266052: 100%|██████████████████| 20/20 [00:01<00:00, 12.26it/s]
Epoch: 1, train for the 133-th batch, train loss: 0.5310054421424866:  56%|██████▏    | 132/237 [00:33<00:33,  3.17it/s]Epoch: 1, train for the 118-th batch, train loss: 0.3379296064376831:  31%|███▎       | 117/383 [00:30<01:25,  3.11it/s]Epoch: 1, train for the 133-th batch, train loss: 0.5310054421424866:  56%|██████▏    | 133/237 [00:33<00:36,  2.82it/s]Epoch: 1, train for the 118-th batch, train loss: 0.3379296064376831:  31%|███▍       | 118/383 [00:30<01:39,  2.68it/s]Epoch: 2, train for the 26-th batch, train loss: 0.3950481414794922:  21%|██▋          | 25/119 [00:03<00:15,  6.22it/s]Epoch: 1, train for the 157-th batch, train loss: 0.26754871010780334:  65%|██████▍   | 156/241 [00:33<00:23,  3.56it/s]Epoch: 2, train for the 26-th batch, train loss: 0.3950481414794922:  22%|██▊          | 26/119 [00:03<00:15,  6.18it/s]Epoch: 1, train for the 157-th batch, train loss: 0.26754871010780334:  65%|██████▌   | 157/241 [00:33<00:26,  3.21it/s]Epoch: 1, train for the 149-th batch, train loss: 0.38648754358291626:  98%|█████████▊| 148/151 [00:34<00:00,  3.70it/s]Epoch: 1, train for the 149-th batch, train loss: 0.38648754358291626:  99%|█████████▊| 149/151 [00:34<00:00,  3.24it/s]INFO:root:Epoch: 1, learning rate: 0.0001, train loss: 0.5962
INFO:root:train average_precision, 0.7909
INFO:root:train roc_auc, 0.7838
INFO:root:validate loss: 0.5378
INFO:root:validate average_precision, 0.8210
Epoch: 1, train for the 134-th batch, train loss: 0.5938473343849182:  56%|██████▏    | 133/237 [00:33<00:36,  2.82it/s]INFO:root:validate roc_auc, 0.7925
INFO:root:new node validate loss: 0.7065
INFO:root:new node validate first_1_average_precision, 0.5488
INFO:root:new node validate first_1_roc_auc, 0.4438
INFO:root:new node validate first_3_average_precision, 0.5720
INFO:root:new node validate first_3_roc_auc, 0.4946
INFO:root:new node validate first_10_average_precision, 0.5764
INFO:root:new node validate first_10_roc_auc, 0.5382
INFO:root:new node validate average_precision, 0.6321
INFO:root:new node validate roc_auc, 0.6266
INFO:root:save model ./saved_models/TGN/ia-escorts-dynamic/TGN_seed0_tgn-ia-escorts-dynamic-reparamcorr-time-mlp/TGN_seed0_tgn-ia-escorts-dynamic-reparamcorr-time-mlp.pkl
Epoch: 2, train for the 27-th batch, train loss: 0.49884232878685:  22%|███▎           | 26/119 [00:04<00:15,  6.18it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5938473343849182:  57%|██████▏    | 134/237 [00:33<00:32,  3.21it/s]Epoch: 2, train for the 27-th batch, train loss: 0.49884232878685:  23%|███▍           | 27/119 [00:04<00:14,  6.53it/s]Epoch: 1, train for the 119-th batch, train loss: 0.44017744064331055:  31%|███       | 118/383 [00:31<01:39,  2.68it/s]Epoch: 1, train for the 158-th batch, train loss: 0.2915719747543335:  65%|███████▏   | 157/241 [00:33<00:26,  3.21it/s]Epoch: 1, train for the 158-th batch, train loss: 0.2915719747543335:  66%|███████▏   | 158/241 [00:33<00:21,  3.86it/s]Epoch: 1, train for the 119-th batch, train loss: 0.44017744064331055:  31%|███       | 119/383 [00:31<01:27,  3.00it/s]Epoch: 1, train for the 150-th batch, train loss: 0.4698869585990906:  99%|██████████▊| 149/151 [00:34<00:00,  3.24it/s]Epoch: 1, train for the 150-th batch, train loss: 0.4698869585990906:  99%|██████████▉| 150/151 [00:34<00:00,  3.81it/s]Epoch: 2, train for the 28-th batch, train loss: 0.47670990228652954:  23%|██▋         | 27/119 [00:04<00:14,  6.53it/s]Epoch: 2, train for the 28-th batch, train loss: 0.47670990228652954:  24%|██▊         | 28/119 [00:04<00:12,  7.28it/s]Epoch: 1, train for the 159-th batch, train loss: 0.1859716773033142:  66%|███████▏   | 158/241 [00:33<00:21,  3.86it/s]Epoch: 1, train for the 159-th batch, train loss: 0.1859716773033142:  66%|███████▎   | 159/241 [00:33<00:18,  4.32it/s]Epoch: 2, train for the 29-th batch, train loss: 0.4510710835456848:  24%|███          | 28/119 [00:04<00:12,  7.28it/s]Epoch: 2, train for the 29-th batch, train loss: 0.4510710835456848:  24%|███▏         | 29/119 [00:04<00:11,  7.91it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5030443072319031:  99%|██████████▉| 150/151 [00:34<00:00,  3.81it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5030443072319031: 100%|███████████| 151/151 [00:34<00:00,  4.23it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5030443072319031: 100%|███████████| 151/151 [00:34<00:00,  4.37it/s]
Epoch: 2, train for the 30-th batch, train loss: 0.4471186697483063:  24%|███▏         | 29/119 [00:04<00:11,  7.91it/s]Epoch: 1, train for the 135-th batch, train loss: 0.6221951246261597:  57%|██████▏    | 134/237 [00:33<00:32,  3.21it/s]Epoch: 1, train for the 135-th batch, train loss: 0.6221951246261597:  57%|██████▎    | 135/237 [00:33<00:32,  3.15it/s]  0%|                                                                                            | 0/46 [00:00<?, ?it/s]Epoch: 2, train for the 31-th batch, train loss: 0.4522075951099396:  24%|███▏         | 29/119 [00:04<00:11,  7.91it/s]Epoch: 2, train for the 31-th batch, train loss: 0.4522075951099396:  26%|███▍         | 31/119 [00:04<00:09,  9.00it/s]Epoch: 1, train for the 120-th batch, train loss: 0.45579463243484497:  31%|███       | 119/383 [00:31<01:27,  3.00it/s]Epoch: 1, train for the 160-th batch, train loss: 0.5601891875267029:  66%|███████▎   | 159/241 [00:33<00:18,  4.32it/s]evaluate for the 1-th batch, evaluate loss: 0.6760240197181702:   0%|                            | 0/46 [00:00<?, ?it/s]Epoch: 1, train for the 160-th batch, train loss: 0.5601891875267029:  66%|███████▎   | 160/241 [00:33<00:19,  4.17it/s]Epoch: 1, train for the 120-th batch, train loss: 0.45579463243484497:  31%|███▏      | 120/383 [00:31<01:34,  2.79it/s]Epoch: 1, train for the 136-th batch, train loss: 0.6015679240226746:  57%|██████▎    | 135/237 [00:33<00:32,  3.15it/s]Epoch: 1, train for the 136-th batch, train loss: 0.6015679240226746:  57%|██████▎    | 136/237 [00:33<00:27,  3.69it/s]Epoch: 2, train for the 32-th batch, train loss: 0.3712140619754791:  26%|███▍         | 31/119 [00:04<00:09,  9.00it/s]Epoch: 2, train for the 32-th batch, train loss: 0.3712140619754791:  27%|███▍         | 32/119 [00:04<00:10,  8.69it/s]evaluate for the 2-th batch, evaluate loss: 0.6621036529541016:   0%|                            | 0/46 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.6621036529541016:   4%|▊                   | 2/46 [00:00<00:05,  8.59it/s]Epoch: 1, train for the 161-th batch, train loss: 0.3426893651485443:  66%|███████▎   | 160/241 [00:33<00:19,  4.17it/s]Epoch: 1, train for the 161-th batch, train loss: 0.3426893651485443:  67%|███████▎   | 161/241 [00:33<00:17,  4.55it/s]Epoch: 2, train for the 33-th batch, train loss: 0.4410529136657715:  27%|███▍         | 32/119 [00:04<00:10,  8.69it/s]Epoch: 2, train for the 33-th batch, train loss: 0.4410529136657715:  28%|███▌         | 33/119 [00:04<00:09,  8.61it/s]Epoch: 1, train for the 121-th batch, train loss: 0.4143465459346771:  31%|███▍       | 120/383 [00:31<01:34,  2.79it/s]Epoch: 1, train for the 137-th batch, train loss: 0.5843917727470398:  57%|██████▎    | 136/237 [00:33<00:27,  3.69it/s]Epoch: 1, train for the 121-th batch, train loss: 0.4143465459346771:  32%|███▍       | 121/383 [00:31<01:23,  3.12it/s]evaluate for the 3-th batch, evaluate loss: 0.7110008001327515:   4%|▊                   | 2/46 [00:00<00:05,  8.59it/s]Epoch: 1, train for the 137-th batch, train loss: 0.5843917727470398:  58%|██████▎    | 137/237 [00:33<00:25,  3.93it/s]Epoch: 1, train for the 162-th batch, train loss: 0.19410867989063263:  67%|██████▋   | 161/241 [00:33<00:17,  4.55it/s]Epoch: 2, train for the 34-th batch, train loss: 0.4281021058559418:  28%|███▌         | 33/119 [00:04<00:09,  8.61it/s]Epoch: 1, train for the 162-th batch, train loss: 0.19410867989063263:  67%|██████▋   | 162/241 [00:34<00:15,  5.18it/s]Epoch: 2, train for the 34-th batch, train loss: 0.4281021058559418:  29%|███▋         | 34/119 [00:04<00:09,  8.52it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]Epoch: 1, train for the 138-th batch, train loss: 0.5898381471633911:  58%|██████▎    | 137/237 [00:34<00:25,  3.93it/s]evaluate for the 4-th batch, evaluate loss: 0.6380831003189087:   4%|▊                   | 2/46 [00:00<00:05,  8.59it/s]evaluate for the 4-th batch, evaluate loss: 0.6380831003189087:   9%|█▋                  | 4/46 [00:00<00:05,  7.80it/s]Epoch: 1, train for the 138-th batch, train loss: 0.5898381471633911:  58%|██████▍    | 138/237 [00:34<00:22,  4.33it/s]Epoch: 2, train for the 35-th batch, train loss: 0.41995635628700256:  29%|███▍        | 34/119 [00:04<00:09,  8.52it/s]Epoch: 2, train for the 35-th batch, train loss: 0.41995635628700256:  29%|███▌        | 35/119 [00:04<00:10,  8.13it/s]Epoch: 2, train for the 1-th batch, train loss: 0.8976021409034729:   0%|                       | 0/146 [00:00<?, ?it/s]Epoch: 2, train for the 1-th batch, train loss: 0.8976021409034729:   1%|               | 1/146 [00:00<00:14,  9.85it/s]evaluate for the 5-th batch, evaluate loss: 0.6345115303993225:   9%|█▋                  | 4/46 [00:00<00:05,  7.80it/s]Epoch: 1, train for the 122-th batch, train loss: 0.5244175791740417:  32%|███▍       | 121/383 [00:32<01:23,  3.12it/s]Epoch: 1, train for the 163-th batch, train loss: 0.16711822152137756:  67%|██████▋   | 162/241 [00:34<00:15,  5.18it/s]evaluate for the 6-th batch, evaluate loss: 0.6027410626411438:   9%|█▋                  | 4/46 [00:00<00:05,  7.80it/s]evaluate for the 6-th batch, evaluate loss: 0.6027410626411438:  13%|██▌                 | 6/46 [00:00<00:04,  9.89it/s]Epoch: 1, train for the 122-th batch, train loss: 0.5244175791740417:  32%|███▌       | 122/383 [00:32<01:24,  3.08it/s]Epoch: 1, train for the 163-th batch, train loss: 0.16711822152137756:  68%|██████▊   | 163/241 [00:34<00:16,  4.68it/s]Epoch: 1, train for the 139-th batch, train loss: 0.581032395362854:  58%|██████▉     | 138/237 [00:34<00:22,  4.33it/s]Epoch: 2, train for the 36-th batch, train loss: 0.4267682433128357:  29%|███▊         | 35/119 [00:05<00:10,  8.13it/s]Epoch: 2, train for the 36-th batch, train loss: 0.4267682433128357:  30%|███▉         | 36/119 [00:05<00:11,  7.26it/s]Epoch: 1, train for the 139-th batch, train loss: 0.581032395362854:  59%|███████     | 139/237 [00:34<00:21,  4.55it/s]Epoch: 2, train for the 2-th batch, train loss: 0.7717157602310181:   1%|               | 1/146 [00:00<00:14,  9.85it/s]Epoch: 2, train for the 2-th batch, train loss: 0.7717157602310181:   1%|▏              | 2/146 [00:00<00:19,  7.48it/s]evaluate for the 7-th batch, evaluate loss: 0.7040002346038818:  13%|██▌                 | 6/46 [00:00<00:04,  9.89it/s]Epoch: 1, train for the 164-th batch, train loss: 0.14581845700740814:  68%|██████▊   | 163/241 [00:34<00:16,  4.68it/s]evaluate for the 8-th batch, evaluate loss: 0.6712431311607361:  13%|██▌                 | 6/46 [00:00<00:04,  9.89it/s]evaluate for the 8-th batch, evaluate loss: 0.6712431311607361:  17%|███▍                | 8/46 [00:00<00:03, 10.36it/s]Epoch: 1, train for the 164-th batch, train loss: 0.14581845700740814:  68%|██████▊   | 164/241 [00:34<00:15,  4.82it/s]Epoch: 2, train for the 37-th batch, train loss: 0.43961650133132935:  30%|███▋        | 36/119 [00:05<00:11,  7.26it/s]Epoch: 2, train for the 37-th batch, train loss: 0.43961650133132935:  31%|███▋        | 37/119 [00:05<00:11,  7.01it/s]Epoch: 1, train for the 123-th batch, train loss: 0.4373249411582947:  32%|███▌       | 122/383 [00:32<01:24,  3.08it/s]Epoch: 2, train for the 3-th batch, train loss: 0.6904150247573853:   1%|▏              | 2/146 [00:00<00:19,  7.48it/s]Epoch: 1, train for the 123-th batch, train loss: 0.4373249411582947:  32%|███▌       | 123/383 [00:32<01:17,  3.35it/s]Epoch: 2, train for the 3-th batch, train loss: 0.6904150247573853:   2%|▎              | 3/146 [00:00<00:21,  6.70it/s]Epoch: 2, train for the 38-th batch, train loss: 0.4051334261894226:  31%|████         | 37/119 [00:05<00:11,  7.01it/s]Epoch: 2, train for the 38-th batch, train loss: 0.4051334261894226:  32%|████▏        | 38/119 [00:05<00:11,  6.78it/s]Epoch: 2, train for the 4-th batch, train loss: 0.6500680446624756:   2%|▎              | 3/146 [00:00<00:21,  6.70it/s]Epoch: 2, train for the 4-th batch, train loss: 0.6500680446624756:   3%|▍              | 4/146 [00:00<00:21,  6.62it/s]Epoch: 2, train for the 39-th batch, train loss: 0.47143927216529846:  32%|███▊        | 38/119 [00:05<00:11,  6.78it/s]Epoch: 2, train for the 39-th batch, train loss: 0.47143927216529846:  33%|███▉        | 39/119 [00:05<00:12,  6.49it/s]Epoch: 2, train for the 5-th batch, train loss: 0.6375704407691956:   3%|▍              | 4/146 [00:00<00:21,  6.62it/s]Epoch: 2, train for the 5-th batch, train loss: 0.6375704407691956:   3%|▌              | 5/146 [00:00<00:21,  6.62it/s]Epoch: 1, train for the 165-th batch, train loss: 0.22400718927383423:  68%|██████▊   | 164/241 [00:34<00:15,  4.82it/s]evaluate for the 9-th batch, evaluate loss: 0.6641325354576111:  17%|███▍                | 8/46 [00:01<00:03, 10.36it/s]Epoch: 1, train for the 165-th batch, train loss: 0.22400718927383423:  68%|██████▊   | 165/241 [00:34<00:19,  3.83it/s]Epoch: 1, train for the 124-th batch, train loss: 0.48763853311538696:  32%|███▏      | 123/383 [00:32<01:17,  3.35it/s]Epoch: 1, train for the 140-th batch, train loss: 0.5679916143417358:  59%|██████▍    | 139/237 [00:34<00:21,  4.55it/s]Epoch: 1, train for the 124-th batch, train loss: 0.48763853311538696:  32%|███▏      | 124/383 [00:32<01:22,  3.12it/s]Epoch: 1, train for the 140-th batch, train loss: 0.5679916143417358:  59%|██████▍    | 140/237 [00:34<00:31,  3.09it/s]evaluate for the 10-th batch, evaluate loss: 0.5985843539237976:  17%|███▎               | 8/46 [00:01<00:03, 10.36it/s]evaluate for the 10-th batch, evaluate loss: 0.5985843539237976:  22%|███▉              | 10/46 [00:01<00:05,  6.70it/s]Epoch: 2, train for the 6-th batch, train loss: 0.6198310852050781:   3%|▌              | 5/146 [00:00<00:21,  6.62it/s]Epoch: 2, train for the 6-th batch, train loss: 0.6198310852050781:   4%|▌              | 6/146 [00:00<00:21,  6.41it/s]Epoch: 1, train for the 166-th batch, train loss: 0.24137237668037415:  68%|██████▊   | 165/241 [00:34<00:19,  3.83it/s]Epoch: 2, train for the 40-th batch, train loss: 0.3911205530166626:  33%|████▎        | 39/119 [00:05<00:12,  6.49it/s]Epoch: 2, train for the 40-th batch, train loss: 0.3911205530166626:  34%|████▎        | 40/119 [00:05<00:12,  6.09it/s]Epoch: 1, train for the 166-th batch, train loss: 0.24137237668037415:  69%|██████▉   | 166/241 [00:35<00:17,  4.37it/s]evaluate for the 11-th batch, evaluate loss: 0.6945643424987793:  22%|███▉              | 10/46 [00:01<00:05,  6.70it/s]evaluate for the 12-th batch, evaluate loss: 0.601787269115448:  22%|████▏              | 10/46 [00:01<00:05,  6.70it/s]evaluate for the 12-th batch, evaluate loss: 0.601787269115448:  26%|████▉              | 12/46 [00:01<00:04,  8.03it/s]Epoch: 1, train for the 125-th batch, train loss: 0.45751428604125977:  32%|███▏      | 124/383 [00:32<01:22,  3.12it/s]Epoch: 1, train for the 125-th batch, train loss: 0.45751428604125977:  33%|███▎      | 125/383 [00:32<01:16,  3.37it/s]Epoch: 2, train for the 7-th batch, train loss: 0.5760924220085144:   4%|▌              | 6/146 [00:01<00:21,  6.41it/s]Epoch: 2, train for the 7-th batch, train loss: 0.5760924220085144:   5%|▋              | 7/146 [00:01<00:21,  6.49it/s]Epoch: 2, train for the 41-th batch, train loss: 0.4284992814064026:  34%|████▎        | 40/119 [00:05<00:12,  6.09it/s]Epoch: 2, train for the 41-th batch, train loss: 0.4284992814064026:  34%|████▍        | 41/119 [00:05<00:13,  5.95it/s]Epoch: 2, train for the 8-th batch, train loss: 0.5416135191917419:   5%|▋              | 7/146 [00:01<00:21,  6.49it/s]Epoch: 2, train for the 8-th batch, train loss: 0.5416135191917419:   5%|▊              | 8/146 [00:01<00:20,  6.86it/s]evaluate for the 13-th batch, evaluate loss: 0.6241006255149841:  26%|████▋             | 12/46 [00:01<00:04,  8.03it/s]Epoch: 2, train for the 42-th batch, train loss: 0.44714561104774475:  34%|████▏       | 41/119 [00:06<00:13,  5.95it/s]Epoch: 2, train for the 42-th batch, train loss: 0.44714561104774475:  35%|████▏       | 42/119 [00:06<00:12,  6.18it/s]evaluate for the 14-th batch, evaluate loss: 0.67629075050354:  26%|█████▏              | 12/46 [00:01<00:04,  8.03it/s]evaluate for the 14-th batch, evaluate loss: 0.67629075050354:  30%|██████              | 14/46 [00:01<00:04,  7.87it/s]Epoch: 2, train for the 9-th batch, train loss: 0.5503153204917908:   5%|▊              | 8/146 [00:01<00:20,  6.86it/s]Epoch: 2, train for the 9-th batch, train loss: 0.5503153204917908:   6%|▉              | 9/146 [00:01<00:19,  7.12it/s]Epoch: 1, train for the 167-th batch, train loss: 0.13331417739391327:  69%|██████▉   | 166/241 [00:35<00:17,  4.37it/s]Epoch: 1, train for the 141-th batch, train loss: 0.619048535823822:  59%|███████     | 140/237 [00:35<00:31,  3.09it/s]Epoch: 1, train for the 126-th batch, train loss: 0.3686787188053131:  33%|███▌       | 125/383 [00:33<01:16,  3.37it/s]evaluate for the 15-th batch, evaluate loss: 0.6358249187469482:  30%|█████▍            | 14/46 [00:01<00:04,  7.87it/s]Epoch: 1, train for the 167-th batch, train loss: 0.13331417739391327:  69%|██████▉   | 167/241 [00:35<00:21,  3.44it/s]Epoch: 1, train for the 141-th batch, train loss: 0.619048535823822:  59%|███████▏    | 141/237 [00:35<00:37,  2.54it/s]Epoch: 2, train for the 43-th batch, train loss: 0.4508366286754608:  35%|████▌        | 42/119 [00:06<00:12,  6.18it/s]Epoch: 1, train for the 126-th batch, train loss: 0.3686787188053131:  33%|███▌       | 126/383 [00:33<01:19,  3.22it/s]Epoch: 2, train for the 43-th batch, train loss: 0.4508366286754608:  36%|████▋        | 43/119 [00:06<00:12,  6.28it/s]Epoch: 2, train for the 10-th batch, train loss: 0.5454330444335938:   6%|▊             | 9/146 [00:01<00:19,  7.12it/s]Epoch: 2, train for the 10-th batch, train loss: 0.5454330444335938:   7%|▉            | 10/146 [00:01<00:18,  7.32it/s]evaluate for the 16-th batch, evaluate loss: 0.6076332330703735:  30%|█████▍            | 14/46 [00:01<00:04,  7.87it/s]evaluate for the 16-th batch, evaluate loss: 0.6076332330703735:  35%|██████▎           | 16/46 [00:01<00:03,  8.77it/s]Epoch: 1, train for the 168-th batch, train loss: 0.13906797766685486:  69%|██████▉   | 167/241 [00:35<00:21,  3.44it/s]Epoch: 1, train for the 168-th batch, train loss: 0.13906797766685486:  70%|██████▉   | 168/241 [00:35<00:18,  4.00it/s]Epoch: 1, train for the 142-th batch, train loss: 0.6068446040153503:  59%|██████▌    | 141/237 [00:35<00:37,  2.54it/s]Epoch: 2, train for the 44-th batch, train loss: 0.4091944098472595:  36%|████▋        | 43/119 [00:06<00:12,  6.28it/s]Epoch: 2, train for the 44-th batch, train loss: 0.4091944098472595:  37%|████▊        | 44/119 [00:06<00:12,  5.94it/s]Epoch: 1, train for the 142-th batch, train loss: 0.6068446040153503:  60%|██████▌    | 142/237 [00:35<00:32,  2.96it/s]Epoch: 1, train for the 127-th batch, train loss: 0.4086245596408844:  33%|███▌       | 126/383 [00:33<01:19,  3.22it/s]Epoch: 2, train for the 11-th batch, train loss: 0.5584524273872375:   7%|▉            | 10/146 [00:01<00:18,  7.32it/s]Epoch: 2, train for the 11-th batch, train loss: 0.5584524273872375:   8%|▉            | 11/146 [00:01<00:20,  6.68it/s]Epoch: 1, train for the 127-th batch, train loss: 0.4086245596408844:  33%|███▋       | 127/383 [00:33<01:14,  3.46it/s]Epoch: 2, train for the 45-th batch, train loss: 0.42231443524360657:  37%|████▍       | 44/119 [00:06<00:12,  5.94it/s]Epoch: 2, train for the 45-th batch, train loss: 0.42231443524360657:  38%|████▌       | 45/119 [00:06<00:11,  6.24it/s]Epoch: 2, train for the 12-th batch, train loss: 0.5401023030281067:   8%|▉            | 11/146 [00:01<00:20,  6.68it/s]Epoch: 2, train for the 12-th batch, train loss: 0.5401023030281067:   8%|█            | 12/146 [00:01<00:20,  6.63it/s]Epoch: 1, train for the 169-th batch, train loss: 0.5599570870399475:  70%|███████▋   | 168/241 [00:35<00:18,  4.00it/s]evaluate for the 17-th batch, evaluate loss: 0.5037865042686462:  35%|██████▎           | 16/46 [00:02<00:03,  8.77it/s]Epoch: 1, train for the 143-th batch, train loss: 0.5524992346763611:  60%|██████▌    | 142/237 [00:35<00:32,  2.96it/s]Epoch: 1, train for the 169-th batch, train loss: 0.5599570870399475:  70%|███████▋   | 169/241 [00:35<00:19,  3.78it/s]Epoch: 2, train for the 46-th batch, train loss: 0.4191626310348511:  38%|████▉        | 45/119 [00:06<00:11,  6.24it/s]Epoch: 1, train for the 143-th batch, train loss: 0.5524992346763611:  60%|██████▋    | 143/237 [00:35<00:29,  3.18it/s]Epoch: 2, train for the 46-th batch, train loss: 0.4191626310348511:  39%|█████        | 46/119 [00:06<00:10,  6.70it/s]evaluate for the 18-th batch, evaluate loss: 0.6090906262397766:  35%|██████▎           | 16/46 [00:02<00:03,  8.77it/s]evaluate for the 18-th batch, evaluate loss: 0.6090906262397766:  39%|███████           | 18/46 [00:02<00:04,  6.96it/s]Epoch: 2, train for the 13-th batch, train loss: 0.5449632406234741:   8%|█            | 12/146 [00:01<00:20,  6.63it/s]Epoch: 1, train for the 128-th batch, train loss: 0.49612411856651306:  33%|███▎      | 127/383 [00:33<01:14,  3.46it/s]Epoch: 2, train for the 13-th batch, train loss: 0.5449632406234741:   9%|█▏           | 13/146 [00:01<00:19,  6.91it/s]Epoch: 1, train for the 128-th batch, train loss: 0.49612411856651306:  33%|███▎      | 128/383 [00:33<01:15,  3.37it/s]evaluate for the 19-th batch, evaluate loss: 0.6710898280143738:  39%|███████           | 18/46 [00:02<00:04,  6.96it/s]evaluate for the 19-th batch, evaluate loss: 0.6710898280143738:  41%|███████▍          | 19/46 [00:02<00:03,  7.27it/s]Epoch: 1, train for the 170-th batch, train loss: 0.6860793232917786:  70%|███████▋   | 169/241 [00:36<00:19,  3.78it/s]Epoch: 1, train for the 170-th batch, train loss: 0.6860793232917786:  71%|███████▊   | 170/241 [00:36<00:17,  4.15it/s]Epoch: 2, train for the 47-th batch, train loss: 0.4653526544570923:  39%|█████        | 46/119 [00:06<00:10,  6.70it/s]Epoch: 2, train for the 47-th batch, train loss: 0.4653526544570923:  39%|█████▏       | 47/119 [00:06<00:11,  6.45it/s]Epoch: 1, train for the 144-th batch, train loss: 0.5831199884414673:  60%|██████▋    | 143/237 [00:36<00:29,  3.18it/s]Epoch: 2, train for the 14-th batch, train loss: 0.5046678781509399:   9%|█▏           | 13/146 [00:02<00:19,  6.91it/s]Epoch: 2, train for the 14-th batch, train loss: 0.5046678781509399:  10%|█▏           | 14/146 [00:02<00:19,  6.63it/s]Epoch: 1, train for the 144-th batch, train loss: 0.5831199884414673:  61%|██████▋    | 144/237 [00:36<00:26,  3.48it/s]evaluate for the 20-th batch, evaluate loss: 0.6181687116622925:  41%|███████▍          | 19/46 [00:02<00:03,  7.27it/s]Epoch: 1, train for the 129-th batch, train loss: 0.46433883905410767:  33%|███▎      | 128/383 [00:34<01:15,  3.37it/s]Epoch: 1, train for the 129-th batch, train loss: 0.46433883905410767:  34%|███▎      | 129/383 [00:34<01:09,  3.67it/s]Epoch: 2, train for the 48-th batch, train loss: 0.4660114049911499:  39%|█████▏       | 47/119 [00:07<00:11,  6.45it/s]Epoch: 2, train for the 48-th batch, train loss: 0.4660114049911499:  40%|█████▏       | 48/119 [00:07<00:11,  6.45it/s]Epoch: 2, train for the 15-th batch, train loss: 0.4924123287200928:  10%|█▏           | 14/146 [00:02<00:19,  6.63it/s]Epoch: 1, train for the 171-th batch, train loss: 0.655227541923523:  71%|████████▍   | 170/241 [00:36<00:17,  4.15it/s]Epoch: 2, train for the 15-th batch, train loss: 0.4924123287200928:  10%|█▎           | 15/146 [00:02<00:19,  6.75it/s]Epoch: 1, train for the 171-th batch, train loss: 0.655227541923523:  71%|████████▌   | 171/241 [00:36<00:16,  4.25it/s]evaluate for the 21-th batch, evaluate loss: 0.6292396187782288:  41%|███████▍          | 19/46 [00:02<00:03,  7.27it/s]evaluate for the 21-th batch, evaluate loss: 0.6292396187782288:  46%|████████▏         | 21/46 [00:02<00:03,  7.42it/s]evaluate for the 22-th batch, evaluate loss: 0.5915712714195251:  46%|████████▏         | 21/46 [00:02<00:03,  7.42it/s]Epoch: 2, train for the 49-th batch, train loss: 0.45034071803092957:  40%|████▊       | 48/119 [00:07<00:11,  6.45it/s]Epoch: 2, train for the 49-th batch, train loss: 0.45034071803092957:  41%|████▉       | 49/119 [00:07<00:11,  6.20it/s]Epoch: 1, train for the 172-th batch, train loss: 0.6572611927986145:  71%|███████▊   | 171/241 [00:36<00:16,  4.25it/s]Epoch: 2, train for the 16-th batch, train loss: 0.4818490445613861:  10%|█▎           | 15/146 [00:02<00:19,  6.75it/s]Epoch: 1, train for the 172-th batch, train loss: 0.6572611927986145:  71%|███████▊   | 172/241 [00:36<00:14,  4.74it/s]Epoch: 2, train for the 16-th batch, train loss: 0.4818490445613861:  11%|█▍           | 16/146 [00:02<00:20,  6.46it/s]evaluate for the 23-th batch, evaluate loss: 0.6261757016181946:  46%|████████▏         | 21/46 [00:02<00:03,  7.42it/s]evaluate for the 23-th batch, evaluate loss: 0.6261757016181946:  50%|█████████         | 23/46 [00:02<00:02,  7.79it/s]Epoch: 1, train for the 130-th batch, train loss: 0.33647212386131287:  34%|███▎      | 129/383 [00:34<01:09,  3.67it/s]Epoch: 1, train for the 145-th batch, train loss: 0.5794249773025513:  61%|██████▋    | 144/237 [00:36<00:26,  3.48it/s]Epoch: 2, train for the 50-th batch, train loss: 0.4135655164718628:  41%|█████▎       | 49/119 [00:07<00:11,  6.20it/s]Epoch: 2, train for the 50-th batch, train loss: 0.4135655164718628:  42%|█████▍       | 50/119 [00:07<00:10,  6.29it/s]Epoch: 1, train for the 145-th batch, train loss: 0.5794249773025513:  61%|██████▋    | 145/237 [00:36<00:30,  2.98it/s]Epoch: 1, train for the 130-th batch, train loss: 0.33647212386131287:  34%|███▍      | 130/383 [00:34<01:15,  3.34it/s]Epoch: 1, train for the 173-th batch, train loss: 0.46738454699516296:  71%|███████▏  | 172/241 [00:36<00:14,  4.74it/s]Epoch: 2, train for the 17-th batch, train loss: 0.4464671313762665:  11%|█▍           | 16/146 [00:02<00:20,  6.46it/s]evaluate for the 24-th batch, evaluate loss: 0.577846348285675:  50%|█████████▌         | 23/46 [00:02<00:02,  7.79it/s]Epoch: 2, train for the 17-th batch, train loss: 0.4464671313762665:  12%|█▌           | 17/146 [00:02<00:20,  6.45it/s]Epoch: 1, train for the 173-th batch, train loss: 0.46738454699516296:  72%|███████▏  | 173/241 [00:36<00:13,  5.03it/s]Epoch: 2, train for the 51-th batch, train loss: 0.4817061424255371:  42%|█████▍       | 50/119 [00:07<00:10,  6.29it/s]evaluate for the 25-th batch, evaluate loss: 0.6156736016273499:  50%|█████████         | 23/46 [00:03<00:02,  7.79it/s]evaluate for the 25-th batch, evaluate loss: 0.6156736016273499:  54%|█████████▊        | 25/46 [00:03<00:02,  8.67it/s]Epoch: 2, train for the 51-th batch, train loss: 0.4817061424255371:  43%|█████▌       | 51/119 [00:07<00:10,  6.48it/s]Epoch: 2, train for the 18-th batch, train loss: 0.45630353689193726:  12%|█▍          | 17/146 [00:02<00:20,  6.45it/s]Epoch: 2, train for the 18-th batch, train loss: 0.45630353689193726:  12%|█▍          | 18/146 [00:02<00:20,  6.25it/s]Epoch: 1, train for the 174-th batch, train loss: 0.5313438177108765:  72%|███████▉   | 173/241 [00:36<00:13,  5.03it/s]evaluate for the 26-th batch, evaluate loss: 0.6594336032867432:  54%|█████████▊        | 25/46 [00:03<00:02,  8.67it/s]Epoch: 1, train for the 174-th batch, train loss: 0.5313438177108765:  72%|███████▉   | 174/241 [00:36<00:13,  5.06it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5806204676628113:  61%|██████▋    | 145/237 [00:36<00:30,  2.98it/s]Epoch: 1, train for the 131-th batch, train loss: 0.49764135479927063:  34%|███▍      | 130/383 [00:34<01:15,  3.34it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5806204676628113:  62%|██████▊    | 146/237 [00:36<00:28,  3.20it/s]Epoch: 2, train for the 52-th batch, train loss: 0.4600331783294678:  43%|█████▌       | 51/119 [00:07<00:10,  6.48it/s]Epoch: 2, train for the 52-th batch, train loss: 0.4600331783294678:  44%|█████▋       | 52/119 [00:07<00:10,  6.50it/s]Epoch: 1, train for the 131-th batch, train loss: 0.49764135479927063:  34%|███▍      | 131/383 [00:34<01:14,  3.40it/s]Epoch: 2, train for the 19-th batch, train loss: 0.4675738513469696:  12%|█▌           | 18/146 [00:02<00:20,  6.25it/s]Epoch: 2, train for the 19-th batch, train loss: 0.4675738513469696:  13%|█▋           | 19/146 [00:02<00:19,  6.52it/s]Epoch: 2, train for the 53-th batch, train loss: 0.3999357521533966:  44%|█████▋       | 52/119 [00:07<00:10,  6.50it/s]Epoch: 2, train for the 53-th batch, train loss: 0.3999357521533966:  45%|█████▊       | 53/119 [00:07<00:10,  6.41it/s]Epoch: 2, train for the 20-th batch, train loss: 0.4231373071670532:  13%|█▋           | 19/146 [00:02<00:19,  6.52it/s]Epoch: 2, train for the 20-th batch, train loss: 0.4231373071670532:  14%|█▊           | 20/146 [00:02<00:19,  6.45it/s]Epoch: 1, train for the 147-th batch, train loss: 0.6011806726455688:  62%|██████▊    | 146/237 [00:37<00:28,  3.20it/s]Epoch: 1, train for the 175-th batch, train loss: 0.35526132583618164:  72%|███████▏  | 174/241 [00:37<00:13,  5.06it/s]evaluate for the 27-th batch, evaluate loss: 0.6426494717597961:  54%|█████████▊        | 25/46 [00:03<00:02,  8.67it/s]evaluate for the 27-th batch, evaluate loss: 0.6426494717597961:  59%|██████████▌       | 27/46 [00:03<00:02,  6.93it/s]Epoch: 1, train for the 175-th batch, train loss: 0.35526132583618164:  73%|███████▎  | 175/241 [00:37<00:15,  4.25it/s]Epoch: 1, train for the 147-th batch, train loss: 0.6011806726455688:  62%|██████▊    | 147/237 [00:37<00:27,  3.27it/s]Epoch: 2, train for the 54-th batch, train loss: 0.4002297818660736:  45%|█████▊       | 53/119 [00:07<00:10,  6.41it/s]Epoch: 2, train for the 54-th batch, train loss: 0.4002297818660736:  45%|█████▉       | 54/119 [00:07<00:09,  6.65it/s]Epoch: 2, train for the 21-th batch, train loss: 0.46379920840263367:  14%|█▋          | 20/146 [00:03<00:19,  6.45it/s]Epoch: 2, train for the 21-th batch, train loss: 0.46379920840263367:  14%|█▋          | 21/146 [00:03<00:18,  6.66it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5045018196105957:  34%|███▊       | 131/383 [00:35<01:14,  3.40it/s]evaluate for the 28-th batch, evaluate loss: 0.6050097346305847:  59%|██████████▌       | 27/46 [00:03<00:02,  6.93it/s]evaluate for the 28-th batch, evaluate loss: 0.6050097346305847:  61%|██████████▉       | 28/46 [00:03<00:02,  7.33it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5045018196105957:  34%|███▊       | 132/383 [00:35<01:20,  3.12it/s]Epoch: 1, train for the 176-th batch, train loss: 0.4141756296157837:  73%|███████▉   | 175/241 [00:37<00:15,  4.25it/s]evaluate for the 29-th batch, evaluate loss: 0.5842043161392212:  61%|██████████▉       | 28/46 [00:03<00:02,  7.33it/s]Epoch: 1, train for the 176-th batch, train loss: 0.4141756296157837:  73%|████████   | 176/241 [00:37<00:13,  4.67it/s]Epoch: 1, train for the 148-th batch, train loss: 0.5643696784973145:  62%|██████▊    | 147/237 [00:37<00:27,  3.27it/s]Epoch: 2, train for the 55-th batch, train loss: 0.41176557540893555:  45%|█████▍      | 54/119 [00:08<00:09,  6.65it/s]Epoch: 2, train for the 55-th batch, train loss: 0.41176557540893555:  46%|█████▌      | 55/119 [00:08<00:10,  6.39it/s]Epoch: 1, train for the 148-th batch, train loss: 0.5643696784973145:  62%|██████▊    | 148/237 [00:37<00:24,  3.65it/s]Epoch: 2, train for the 22-th batch, train loss: 0.42590606212615967:  14%|█▋          | 21/146 [00:03<00:18,  6.66it/s]Epoch: 2, train for the 22-th batch, train loss: 0.42590606212615967:  15%|█▊          | 22/146 [00:03<00:19,  6.49it/s]evaluate for the 30-th batch, evaluate loss: 0.6202625632286072:  61%|██████████▉       | 28/46 [00:03<00:02,  7.33it/s]evaluate for the 30-th batch, evaluate loss: 0.6202625632286072:  65%|███████████▋      | 30/46 [00:03<00:01,  8.47it/s]Epoch: 1, train for the 133-th batch, train loss: 0.5242353677749634:  34%|███▊       | 132/383 [00:35<01:20,  3.12it/s]Epoch: 1, train for the 133-th batch, train loss: 0.5242353677749634:  35%|███▊       | 133/383 [00:35<01:11,  3.49it/s]Epoch: 2, train for the 56-th batch, train loss: 0.42060035467147827:  46%|█████▌      | 55/119 [00:08<00:10,  6.39it/s]Epoch: 2, train for the 56-th batch, train loss: 0.42060035467147827:  47%|█████▋      | 56/119 [00:08<00:09,  6.32it/s]Epoch: 2, train for the 23-th batch, train loss: 0.4555390775203705:  15%|█▉           | 22/146 [00:03<00:19,  6.49it/s]Epoch: 2, train for the 23-th batch, train loss: 0.4555390775203705:  16%|██           | 23/146 [00:03<00:19,  6.38it/s]Epoch: 2, train for the 57-th batch, train loss: 0.3811527490615845:  47%|██████       | 56/119 [00:08<00:09,  6.32it/s]Epoch: 2, train for the 57-th batch, train loss: 0.3811527490615845:  48%|██████▏      | 57/119 [00:08<00:09,  6.26it/s]Epoch: 1, train for the 134-th batch, train loss: 0.6470054984092712:  35%|███▊       | 133/383 [00:35<01:11,  3.49it/s]Epoch: 1, train for the 177-th batch, train loss: 0.3252195715904236:  73%|████████   | 176/241 [00:37<00:13,  4.67it/s]evaluate for the 31-th batch, evaluate loss: 0.4837423264980316:  65%|███████████▋      | 30/46 [00:04<00:01,  8.47it/s]evaluate for the 31-th batch, evaluate loss: 0.4837423264980316:  67%|████████████▏     | 31/46 [00:04<00:02,  6.57it/s]Epoch: 1, train for the 177-th batch, train loss: 0.3252195715904236:  73%|████████   | 177/241 [00:37<00:17,  3.75it/s]Epoch: 2, train for the 24-th batch, train loss: 0.45362040400505066:  16%|█▉          | 23/146 [00:03<00:19,  6.38it/s]Epoch: 1, train for the 134-th batch, train loss: 0.6470054984092712:  35%|███▊       | 134/383 [00:35<01:07,  3.69it/s]Epoch: 2, train for the 24-th batch, train loss: 0.45362040400505066:  16%|█▉          | 24/146 [00:03<00:19,  6.28it/s]Epoch: 1, train for the 149-th batch, train loss: 0.5783860087394714:  62%|██████▊    | 148/237 [00:37<00:24,  3.65it/s]Epoch: 1, train for the 149-th batch, train loss: 0.5783860087394714:  63%|██████▉    | 149/237 [00:37<00:27,  3.15it/s]evaluate for the 32-th batch, evaluate loss: 0.5459140539169312:  67%|████████████▏     | 31/46 [00:04<00:02,  6.57it/s]evaluate for the 32-th batch, evaluate loss: 0.5459140539169312:  70%|████████████▌     | 32/46 [00:04<00:02,  6.97it/s]Epoch: 1, train for the 178-th batch, train loss: 0.5250018835067749:  73%|████████   | 177/241 [00:37<00:17,  3.75it/s]Epoch: 1, train for the 178-th batch, train loss: 0.5250018835067749:  74%|████████   | 178/241 [00:37<00:14,  4.28it/s]Epoch: 2, train for the 58-th batch, train loss: 0.4009402394294739:  48%|██████▏      | 57/119 [00:08<00:09,  6.26it/s]Epoch: 2, train for the 58-th batch, train loss: 0.4009402394294739:  49%|██████▎      | 58/119 [00:08<00:10,  5.85it/s]Epoch: 2, train for the 25-th batch, train loss: 0.43746620416641235:  16%|█▉          | 24/146 [00:03<00:19,  6.28it/s]Epoch: 2, train for the 25-th batch, train loss: 0.43746620416641235:  17%|██          | 25/146 [00:03<00:19,  6.16it/s]Epoch: 1, train for the 135-th batch, train loss: 0.4265837073326111:  35%|███▊       | 134/383 [00:35<01:07,  3.69it/s]Epoch: 1, train for the 135-th batch, train loss: 0.4265837073326111:  35%|███▉       | 135/383 [00:35<01:04,  3.85it/s]Epoch: 1, train for the 150-th batch, train loss: 0.5523207187652588:  63%|██████▉    | 149/237 [00:38<00:27,  3.15it/s]evaluate for the 33-th batch, evaluate loss: 0.5966094136238098:  70%|████████████▌     | 32/46 [00:04<00:02,  6.97it/s]evaluate for the 33-th batch, evaluate loss: 0.5966094136238098:  72%|████████████▉     | 33/46 [00:04<00:02,  6.38it/s]Epoch: 2, train for the 59-th batch, train loss: 0.41932249069213867:  49%|█████▊      | 58/119 [00:08<00:10,  5.85it/s]Epoch: 2, train for the 59-th batch, train loss: 0.41932249069213867:  50%|█████▉      | 59/119 [00:08<00:09,  6.12it/s]Epoch: 1, train for the 150-th batch, train loss: 0.5523207187652588:  63%|██████▉    | 150/237 [00:38<00:26,  3.34it/s]Epoch: 2, train for the 26-th batch, train loss: 0.4113922417163849:  17%|██▏          | 25/146 [00:03<00:19,  6.16it/s]Epoch: 2, train for the 26-th batch, train loss: 0.4113922417163849:  18%|██▎          | 26/146 [00:03<00:19,  6.07it/s]Epoch: 1, train for the 136-th batch, train loss: 0.3835897445678711:  35%|███▉       | 135/383 [00:35<01:04,  3.85it/s]evaluate for the 34-th batch, evaluate loss: 0.5271690487861633:  72%|████████████▉     | 33/46 [00:04<00:02,  6.38it/s]evaluate for the 34-th batch, evaluate loss: 0.5271690487861633:  74%|█████████████▎    | 34/46 [00:04<00:01,  6.57it/s]Epoch: 1, train for the 179-th batch, train loss: 0.4782905876636505:  74%|████████   | 178/241 [00:38<00:14,  4.28it/s]Epoch: 1, train for the 136-th batch, train loss: 0.3835897445678711:  36%|███▉       | 136/383 [00:36<01:01,  4.04it/s]Epoch: 1, train for the 179-th batch, train loss: 0.4782905876636505:  74%|████████▏  | 179/241 [00:38<00:15,  3.92it/s]Epoch: 2, train for the 60-th batch, train loss: 0.40523120760917664:  50%|█████▉      | 59/119 [00:08<00:09,  6.12it/s]Epoch: 2, train for the 60-th batch, train loss: 0.40523120760917664:  50%|██████      | 60/119 [00:08<00:09,  5.95it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5719520449638367:  63%|██████▉    | 150/237 [00:38<00:26,  3.34it/s]Epoch: 2, train for the 27-th batch, train loss: 0.44423484802246094:  18%|██▏         | 26/146 [00:04<00:19,  6.07it/s]Epoch: 2, train for the 27-th batch, train loss: 0.44423484802246094:  18%|██▏         | 27/146 [00:04<00:20,  5.94it/s]evaluate for the 35-th batch, evaluate loss: 0.6300908923149109:  74%|█████████████▎    | 34/46 [00:04<00:01,  6.57it/s]evaluate for the 35-th batch, evaluate loss: 0.6300908923149109:  76%|█████████████▋    | 35/46 [00:04<00:01,  7.15it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5719520449638367:  64%|███████    | 151/237 [00:38<00:24,  3.57it/s]Epoch: 1, train for the 180-th batch, train loss: 0.6111818552017212:  74%|████████▏  | 179/241 [00:38<00:15,  3.92it/s]Epoch: 1, train for the 180-th batch, train loss: 0.6111818552017212:  75%|████████▏  | 180/241 [00:38<00:13,  4.45it/s]evaluate for the 36-th batch, evaluate loss: 0.5453603863716125:  76%|█████████████▋    | 35/46 [00:04<00:01,  7.15it/s]Epoch: 1, train for the 137-th batch, train loss: 0.49139535427093506:  36%|███▌      | 136/383 [00:36<01:01,  4.04it/s]Epoch: 2, train for the 61-th batch, train loss: 0.4536813199520111:  50%|██████▌      | 60/119 [00:09<00:09,  5.95it/s]Epoch: 2, train for the 61-th batch, train loss: 0.4536813199520111:  51%|██████▋      | 61/119 [00:09<00:09,  5.92it/s]Epoch: 1, train for the 137-th batch, train loss: 0.49139535427093506:  36%|███▌      | 137/383 [00:36<00:59,  4.14it/s]Epoch: 2, train for the 28-th batch, train loss: 0.4385795593261719:  18%|██▍          | 27/146 [00:04<00:20,  5.94it/s]evaluate for the 37-th batch, evaluate loss: 0.6418853402137756:  76%|█████████████▋    | 35/46 [00:04<00:01,  7.15it/s]evaluate for the 37-th batch, evaluate loss: 0.6418853402137756:  80%|██████████████▍   | 37/46 [00:04<00:01,  8.92it/s]Epoch: 2, train for the 28-th batch, train loss: 0.4385795593261719:  19%|██▍          | 28/146 [00:04<00:20,  5.88it/s]Epoch: 1, train for the 152-th batch, train loss: 0.5397592782974243:  64%|███████    | 151/237 [00:38<00:24,  3.57it/s]Epoch: 1, train for the 181-th batch, train loss: 0.8539679050445557:  75%|████████▏  | 180/241 [00:38<00:13,  4.45it/s]Epoch: 1, train for the 152-th batch, train loss: 0.5397592782974243:  64%|███████    | 152/237 [00:38<00:21,  3.94it/s]Epoch: 1, train for the 181-th batch, train loss: 0.8539679050445557:  75%|████████▎  | 181/241 [00:38<00:11,  5.02it/s]evaluate for the 38-th batch, evaluate loss: 0.5438663363456726:  80%|██████████████▍   | 37/46 [00:04<00:01,  8.92it/s]evaluate for the 38-th batch, evaluate loss: 0.5438663363456726:  83%|██████████████▊   | 38/46 [00:04<00:00,  9.03it/s]Epoch: 2, train for the 62-th batch, train loss: 0.4750464856624603:  51%|██████▋      | 61/119 [00:09<00:09,  5.92it/s]Epoch: 2, train for the 62-th batch, train loss: 0.4750464856624603:  52%|██████▊      | 62/119 [00:09<00:09,  6.02it/s]Epoch: 2, train for the 29-th batch, train loss: 0.393603652715683:  19%|██▋           | 28/146 [00:04<00:20,  5.88it/s]Epoch: 1, train for the 138-th batch, train loss: 0.29319822788238525:  36%|███▌      | 137/383 [00:36<00:59,  4.14it/s]Epoch: 2, train for the 29-th batch, train loss: 0.393603652715683:  20%|██▊           | 29/146 [00:04<00:19,  5.96it/s]Epoch: 1, train for the 138-th batch, train loss: 0.29319822788238525:  36%|███▌      | 138/383 [00:36<00:56,  4.36it/s]Epoch: 2, train for the 30-th batch, train loss: 0.43469032645225525:  20%|██▍         | 29/146 [00:04<00:19,  5.96it/s]Epoch: 2, train for the 30-th batch, train loss: 0.43469032645225525:  21%|██▍         | 30/146 [00:04<00:20,  5.72it/s]evaluate for the 39-th batch, evaluate loss: 0.5867596864700317:  83%|██████████████▊   | 38/46 [00:05<00:00,  9.03it/s]evaluate for the 39-th batch, evaluate loss: 0.5867596864700317:  85%|███████████████▎  | 39/46 [00:05<00:01,  6.32it/s]Epoch: 1, train for the 153-th batch, train loss: 0.5345074534416199:  64%|███████    | 152/237 [00:38<00:21,  3.94it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5019325613975525:  36%|███▉       | 138/383 [00:36<00:56,  4.36it/s]Epoch: 1, train for the 182-th batch, train loss: 0.6607301235198975:  75%|████████▎  | 181/241 [00:38<00:11,  5.02it/s]Epoch: 2, train for the 31-th batch, train loss: 0.49329614639282227:  21%|██▍         | 30/146 [00:04<00:20,  5.72it/s]Epoch: 2, train for the 31-th batch, train loss: 0.49329614639282227:  21%|██▌         | 31/146 [00:04<00:17,  6.49it/s]evaluate for the 40-th batch, evaluate loss: 0.5720016360282898:  85%|███████████████▎  | 39/46 [00:05<00:01,  6.32it/s]Epoch: 1, train for the 153-th batch, train loss: 0.5345074534416199:  65%|███████    | 153/237 [00:38<00:25,  3.28it/s]Epoch: 1, train for the 182-th batch, train loss: 0.6607301235198975:  76%|████████▎  | 182/241 [00:38<00:15,  3.77it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5019325613975525:  36%|███▉       | 139/383 [00:36<01:00,  4.02it/s]evaluate for the 41-th batch, evaluate loss: 0.6021445989608765:  85%|███████████████▎  | 39/46 [00:05<00:01,  6.32it/s]evaluate for the 41-th batch, evaluate loss: 0.6021445989608765:  89%|████████████████  | 41/46 [00:05<00:00,  8.17it/s]Epoch: 2, train for the 63-th batch, train loss: 0.4488844871520996:  52%|██████▊      | 62/119 [00:09<00:09,  6.02it/s]Epoch: 2, train for the 63-th batch, train loss: 0.4488844871520996:  53%|██████▉      | 63/119 [00:09<00:14,  3.96it/s]Epoch: 2, train for the 32-th batch, train loss: 0.44319769740104675:  21%|██▌         | 31/146 [00:04<00:17,  6.49it/s]Epoch: 2, train for the 32-th batch, train loss: 0.44319769740104675:  22%|██▋         | 32/146 [00:04<00:17,  6.66it/s]Epoch: 1, train for the 183-th batch, train loss: 0.3929245173931122:  76%|████████▎  | 182/241 [00:39<00:15,  3.77it/s]evaluate for the 42-th batch, evaluate loss: 0.5368740558624268:  89%|████████████████  | 41/46 [00:05<00:00,  8.17it/s]evaluate for the 42-th batch, evaluate loss: 0.5368740558624268:  91%|████████████████▍ | 42/46 [00:05<00:00,  8.26it/s]Epoch: 1, train for the 183-th batch, train loss: 0.3929245173931122:  76%|████████▎  | 183/241 [00:39<00:14,  4.04it/s]Epoch: 1, train for the 154-th batch, train loss: 0.5507495403289795:  65%|███████    | 153/237 [00:39<00:25,  3.28it/s]Epoch: 1, train for the 140-th batch, train loss: 0.41183075308799744:  36%|███▋      | 139/383 [00:36<01:00,  4.02it/s]Epoch: 1, train for the 154-th batch, train loss: 0.5507495403289795:  65%|███████▏   | 154/237 [00:39<00:24,  3.45it/s]Epoch: 2, train for the 64-th batch, train loss: 0.4172061085700989:  53%|██████▉      | 63/119 [00:09<00:14,  3.96it/s]Epoch: 2, train for the 64-th batch, train loss: 0.4172061085700989:  54%|██████▉      | 64/119 [00:09<00:12,  4.39it/s]Epoch: 1, train for the 140-th batch, train loss: 0.41183075308799744:  37%|███▋      | 140/383 [00:37<01:02,  3.91it/s]Epoch: 2, train for the 33-th batch, train loss: 0.44665518403053284:  22%|██▋         | 32/146 [00:05<00:17,  6.66it/s]Epoch: 2, train for the 33-th batch, train loss: 0.44665518403053284:  23%|██▋         | 33/146 [00:05<00:17,  6.29it/s]evaluate for the 43-th batch, evaluate loss: 0.6429013013839722:  91%|████████████████▍ | 42/46 [00:05<00:00,  8.26it/s]evaluate for the 43-th batch, evaluate loss: 0.6429013013839722:  93%|████████████████▊ | 43/46 [00:05<00:00,  7.10it/s]Epoch: 2, train for the 65-th batch, train loss: 0.462223082780838:  54%|███████▌      | 64/119 [00:10<00:12,  4.39it/s]Epoch: 2, train for the 65-th batch, train loss: 0.462223082780838:  55%|███████▋      | 65/119 [00:10<00:10,  5.05it/s]Epoch: 2, train for the 34-th batch, train loss: 0.47587263584136963:  23%|██▋         | 33/146 [00:05<00:17,  6.29it/s]Epoch: 2, train for the 34-th batch, train loss: 0.47587263584136963:  23%|██▊         | 34/146 [00:05<00:17,  6.37it/s]evaluate for the 44-th batch, evaluate loss: 0.5461872220039368:  93%|████████████████▊ | 43/46 [00:05<00:00,  7.10it/s]Epoch: 1, train for the 184-th batch, train loss: 0.5970333814620972:  76%|████████▎  | 183/241 [00:39<00:14,  4.04it/s]Epoch: 1, train for the 184-th batch, train loss: 0.5970333814620972:  76%|████████▍  | 184/241 [00:39<00:15,  3.71it/s]evaluate for the 45-th batch, evaluate loss: 0.5283970236778259:  93%|████████████████▊ | 43/46 [00:05<00:00,  7.10it/s]evaluate for the 45-th batch, evaluate loss: 0.5283970236778259:  98%|█████████████████▌| 45/46 [00:05<00:00,  9.04it/s]Epoch: 2, train for the 66-th batch, train loss: 0.45646873116493225:  55%|██████▌     | 65/119 [00:10<00:10,  5.05it/s]Epoch: 2, train for the 66-th batch, train loss: 0.45646873116493225:  55%|██████▋     | 66/119 [00:10<00:09,  5.53it/s]Epoch: 1, train for the 155-th batch, train loss: 0.5895246267318726:  65%|███████▏   | 154/237 [00:39<00:24,  3.45it/s]Epoch: 1, train for the 141-th batch, train loss: 0.5207628011703491:  37%|████       | 140/383 [00:37<01:02,  3.91it/s]Epoch: 2, train for the 35-th batch, train loss: 0.4433992803096771:  23%|███          | 34/146 [00:05<00:17,  6.37it/s]Epoch: 2, train for the 35-th batch, train loss: 0.4433992803096771:  24%|███          | 35/146 [00:05<00:16,  6.68it/s]evaluate for the 46-th batch, evaluate loss: 0.5040640234947205:  98%|█████████████████▌| 45/46 [00:05<00:00,  9.04it/s]evaluate for the 46-th batch, evaluate loss: 0.5040640234947205: 100%|██████████████████| 46/46 [00:05<00:00,  7.86it/s]
Epoch: 1, train for the 155-th batch, train loss: 0.5895246267318726:  65%|███████▏   | 155/237 [00:39<00:25,  3.23it/s]Epoch: 1, train for the 141-th batch, train loss: 0.5207628011703491:  37%|████       | 141/383 [00:37<01:07,  3.57it/s]Epoch: 1, train for the 185-th batch, train loss: 0.658737301826477:  76%|█████████▏  | 184/241 [00:39<00:15,  3.71it/s]Epoch: 1, train for the 185-th batch, train loss: 0.658737301826477:  77%|█████████▏  | 185/241 [00:39<00:13,  4.28it/s]Epoch: 2, train for the 67-th batch, train loss: 0.4917158782482147:  55%|███████▏     | 66/119 [00:10<00:09,  5.53it/s]Epoch: 2, train for the 67-th batch, train loss: 0.4917158782482147:  56%|███████▎     | 67/119 [00:10<00:08,  5.80it/s]Epoch: 2, train for the 36-th batch, train loss: 0.45047804713249207:  24%|██▉         | 35/146 [00:05<00:16,  6.68it/s]Epoch: 2, train for the 36-th batch, train loss: 0.45047804713249207:  25%|██▉         | 36/146 [00:05<00:16,  6.75it/s]Epoch: 1, train for the 156-th batch, train loss: 0.5653144121170044:  65%|███████▏   | 155/237 [00:39<00:25,  3.23it/s]Epoch: 1, train for the 156-th batch, train loss: 0.5653144121170044:  66%|███████▏   | 156/237 [00:39<00:21,  3.73it/s]Epoch: 1, train for the 186-th batch, train loss: 0.5399505496025085:  77%|████████▍  | 185/241 [00:39<00:13,  4.28it/s]Epoch: 1, train for the 186-th batch, train loss: 0.5399505496025085:  77%|████████▍  | 186/241 [00:39<00:11,  5.00it/s]Epoch: 1, train for the 142-th batch, train loss: 0.5101442337036133:  37%|████       | 141/383 [00:37<01:07,  3.57it/s]Epoch: 2, train for the 68-th batch, train loss: 0.4074920415878296:  56%|███████▎     | 67/119 [00:10<00:08,  5.80it/s]Epoch: 1, train for the 142-th batch, train loss: 0.5101442337036133:  37%|████       | 142/383 [00:37<01:04,  3.74it/s]Epoch: 2, train for the 68-th batch, train loss: 0.4074920415878296:  57%|███████▍     | 68/119 [00:10<00:08,  5.91it/s]Epoch: 2, train for the 37-th batch, train loss: 0.4877476096153259:  25%|███▏         | 36/146 [00:05<00:16,  6.75it/s]Epoch: 2, train for the 37-th batch, train loss: 0.4877476096153259:  25%|███▎         | 37/146 [00:05<00:16,  6.66it/s]Epoch: 1, train for the 187-th batch, train loss: 0.6367507576942444:  77%|████████▍  | 186/241 [00:39<00:11,  5.00it/s]Epoch: 1, train for the 187-th batch, train loss: 0.6367507576942444:  78%|████████▌  | 187/241 [00:39<00:09,  5.71it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5357967019081116:  57%|███████▍     | 68/119 [00:10<00:08,  5.91it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5357967019081116:  58%|███████▌     | 69/119 [00:10<00:07,  6.29it/s]Epoch: 2, train for the 38-th batch, train loss: 0.4362425208091736:  25%|███▎         | 37/146 [00:05<00:16,  6.66it/s]Epoch: 2, train for the 38-th batch, train loss: 0.4362425208091736:  26%|███▍         | 38/146 [00:05<00:16,  6.71it/s]Epoch: 2, train for the 70-th batch, train loss: 0.4240434467792511:  58%|███████▌     | 69/119 [00:10<00:07,  6.29it/s]Epoch: 1, train for the 157-th batch, train loss: 0.579067587852478:  66%|███████▉    | 156/237 [00:40<00:21,  3.73it/s]Epoch: 1, train for the 143-th batch, train loss: 0.38812437653541565:  37%|███▋      | 142/383 [00:37<01:04,  3.74it/s]Epoch: 1, train for the 188-th batch, train loss: 0.42060011625289917:  78%|███████▊  | 187/241 [00:40<00:09,  5.71it/s]Epoch: 2, train for the 70-th batch, train loss: 0.4240434467792511:  59%|███████▋     | 70/119 [00:10<00:07,  6.54it/s]Epoch: 1, train for the 188-th batch, train loss: 0.42060011625289917:  78%|███████▊  | 188/241 [00:40<00:10,  5.21it/s]Epoch: 1, train for the 157-th batch, train loss: 0.579067587852478:  66%|███████▉    | 157/237 [00:40<00:23,  3.35it/s]Epoch: 1, train for the 143-th batch, train loss: 0.38812437653541565:  37%|███▋      | 143/383 [00:37<01:07,  3.54it/s]Epoch: 2, train for the 39-th batch, train loss: 0.4835405647754669:  26%|███▍         | 38/146 [00:05<00:16,  6.71it/s]Epoch: 2, train for the 39-th batch, train loss: 0.4835405647754669:  27%|███▍         | 39/146 [00:05<00:15,  6.85it/s]Epoch: 2, train for the 71-th batch, train loss: 0.4310462772846222:  59%|███████▋     | 70/119 [00:10<00:07,  6.54it/s]Epoch: 2, train for the 71-th batch, train loss: 0.4310462772846222:  60%|███████▊     | 71/119 [00:10<00:07,  6.84it/s]  0%|                                                                                            | 0/25 [00:00<?, ?it/s]Epoch: 1, train for the 189-th batch, train loss: 0.3802516460418701:  78%|████████▌  | 188/241 [00:40<00:10,  5.21it/s]Epoch: 1, train for the 158-th batch, train loss: 0.5296564698219299:  66%|███████▎   | 157/237 [00:40<00:23,  3.35it/s]Epoch: 2, train for the 40-th batch, train loss: 0.47232192754745483:  27%|███▏        | 39/146 [00:06<00:15,  6.85it/s]Epoch: 2, train for the 40-th batch, train loss: 0.47232192754745483:  27%|███▎        | 40/146 [00:06<00:15,  6.76it/s]Epoch: 1, train for the 189-th batch, train loss: 0.3802516460418701:  78%|████████▋  | 189/241 [00:40<00:09,  5.27it/s]Epoch: 1, train for the 158-th batch, train loss: 0.5296564698219299:  67%|███████▎   | 158/237 [00:40<00:21,  3.72it/s]Epoch: 1, train for the 144-th batch, train loss: 0.362902969121933:  37%|████▍       | 143/383 [00:38<01:07,  3.54it/s]evaluate for the 1-th batch, evaluate loss: 1.0758782625198364:   0%|                            | 0/25 [00:00<?, ?it/s]Epoch: 1, train for the 144-th batch, train loss: 0.362902969121933:  38%|████▌       | 144/383 [00:38<01:03,  3.78it/s]Epoch: 2, train for the 72-th batch, train loss: 0.46707233786582947:  60%|███████▏    | 71/119 [00:11<00:07,  6.84it/s]Epoch: 2, train for the 72-th batch, train loss: 0.46707233786582947:  61%|███████▎    | 72/119 [00:11<00:06,  6.77it/s]evaluate for the 2-th batch, evaluate loss: 1.0457087755203247:   0%|                            | 0/25 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 1.0457087755203247:   8%|█▌                  | 2/25 [00:00<00:01, 13.11it/s]Epoch: 2, train for the 41-th batch, train loss: 0.4524047076702118:  27%|███▌         | 40/146 [00:06<00:15,  6.76it/s]Epoch: 2, train for the 41-th batch, train loss: 0.4524047076702118:  28%|███▋         | 41/146 [00:06<00:14,  7.14it/s]evaluate for the 3-th batch, evaluate loss: 0.9957848787307739:   8%|█▌                  | 2/25 [00:00<00:01, 13.11it/s]evaluate for the 4-th batch, evaluate loss: 0.9676536917686462:   8%|█▌                  | 2/25 [00:00<00:01, 13.11it/s]evaluate for the 4-th batch, evaluate loss: 0.9676536917686462:  16%|███▏                | 4/25 [00:00<00:01, 13.54it/s]Epoch: 2, train for the 73-th batch, train loss: 0.3926991820335388:  61%|███████▊     | 72/119 [00:11<00:06,  6.77it/s]Epoch: 2, train for the 73-th batch, train loss: 0.3926991820335388:  61%|███████▉     | 73/119 [00:11<00:07,  6.16it/s]Epoch: 2, train for the 42-th batch, train loss: 0.4893715977668762:  28%|███▋         | 41/146 [00:06<00:14,  7.14it/s]Epoch: 2, train for the 42-th batch, train loss: 0.4893715977668762:  29%|███▋         | 42/146 [00:06<00:15,  6.59it/s]Epoch: 1, train for the 190-th batch, train loss: 0.362602174282074:  78%|█████████▍  | 189/241 [00:40<00:09,  5.27it/s]evaluate for the 5-th batch, evaluate loss: 0.9304525256156921:  16%|███▏                | 4/25 [00:00<00:01, 13.54it/s]Epoch: 1, train for the 190-th batch, train loss: 0.362602174282074:  79%|█████████▍  | 190/241 [00:40<00:12,  4.19it/s]Epoch: 1, train for the 145-th batch, train loss: 0.4797353148460388:  38%|████▏      | 144/383 [00:38<01:03,  3.78it/s]Epoch: 1, train for the 159-th batch, train loss: 0.584532618522644:  67%|████████    | 158/237 [00:40<00:21,  3.72it/s]Epoch: 1, train for the 145-th batch, train loss: 0.4797353148460388:  38%|████▏      | 145/383 [00:38<01:07,  3.53it/s]Epoch: 1, train for the 159-th batch, train loss: 0.584532618522644:  67%|████████    | 159/237 [00:40<00:23,  3.31it/s]Epoch: 2, train for the 74-th batch, train loss: 0.4688587188720703:  61%|███████▉     | 73/119 [00:11<00:07,  6.16it/s]evaluate for the 6-th batch, evaluate loss: 0.9725936651229858:  16%|███▏                | 4/25 [00:00<00:01, 13.54it/s]evaluate for the 6-th batch, evaluate loss: 0.9725936651229858:  24%|████▊               | 6/25 [00:00<00:01, 12.52it/s]Epoch: 2, train for the 74-th batch, train loss: 0.4688587188720703:  62%|████████     | 74/119 [00:11<00:07,  6.21it/s]Epoch: 2, train for the 43-th batch, train loss: 0.45947960019111633:  29%|███▍        | 42/146 [00:06<00:15,  6.59it/s]Epoch: 2, train for the 43-th batch, train loss: 0.45947960019111633:  29%|███▌        | 43/146 [00:06<00:15,  6.51it/s]Epoch: 1, train for the 191-th batch, train loss: 0.36145320534706116:  79%|███████▉  | 190/241 [00:40<00:12,  4.19it/s]Epoch: 1, train for the 191-th batch, train loss: 0.36145320534706116:  79%|███████▉  | 191/241 [00:40<00:10,  4.65it/s]evaluate for the 7-th batch, evaluate loss: 0.9002787470817566:  24%|████▊               | 6/25 [00:00<00:01, 12.52it/s]evaluate for the 8-th batch, evaluate loss: 0.9338322281837463:  24%|████▊               | 6/25 [00:00<00:01, 12.52it/s]evaluate for the 8-th batch, evaluate loss: 0.9338322281837463:  32%|██████▍             | 8/25 [00:00<00:01, 11.96it/s]Epoch: 1, train for the 160-th batch, train loss: 0.5846385955810547:  67%|███████▍   | 159/237 [00:40<00:23,  3.31it/s]Epoch: 2, train for the 75-th batch, train loss: 0.4501684606075287:  62%|████████     | 74/119 [00:11<00:07,  6.21it/s]Epoch: 2, train for the 75-th batch, train loss: 0.4501684606075287:  63%|████████▏    | 75/119 [00:11<00:07,  5.74it/s]Epoch: 2, train for the 44-th batch, train loss: 0.44565603137016296:  29%|███▌        | 43/146 [00:06<00:15,  6.51it/s]Epoch: 1, train for the 160-th batch, train loss: 0.5846385955810547:  68%|███████▍   | 160/237 [00:40<00:21,  3.51it/s]Epoch: 2, train for the 44-th batch, train loss: 0.44565603137016296:  30%|███▌        | 44/146 [00:06<00:16,  6.05it/s]evaluate for the 9-th batch, evaluate loss: 0.8448193073272705:  32%|██████▍             | 8/25 [00:00<00:01, 11.96it/s]evaluate for the 10-th batch, evaluate loss: 0.8725172281265259:  32%|██████             | 8/25 [00:00<00:01, 11.96it/s]evaluate for the 10-th batch, evaluate loss: 0.8725172281265259:  40%|███████▏          | 10/25 [00:00<00:01, 12.69it/s]evaluate for the 11-th batch, evaluate loss: 0.8320286273956299:  40%|███████▏          | 10/25 [00:00<00:01, 12.69it/s]Epoch: 2, train for the 45-th batch, train loss: 0.47580549120903015:  30%|███▌        | 44/146 [00:06<00:16,  6.05it/s]Epoch: 2, train for the 45-th batch, train loss: 0.47580549120903015:  31%|███▋        | 45/146 [00:06<00:16,  6.08it/s]Epoch: 2, train for the 76-th batch, train loss: 0.45218148827552795:  63%|███████▌    | 75/119 [00:11<00:07,  5.74it/s]Epoch: 2, train for the 76-th batch, train loss: 0.45218148827552795:  64%|███████▋    | 76/119 [00:11<00:07,  5.52it/s]Epoch: 1, train for the 146-th batch, train loss: 0.3877403140068054:  38%|████▏      | 145/383 [00:38<01:07,  3.53it/s]Epoch: 1, train for the 192-th batch, train loss: 0.33751845359802246:  79%|███████▉  | 191/241 [00:41<00:10,  4.65it/s]Epoch: 1, train for the 161-th batch, train loss: 0.6055739521980286:  68%|███████▍   | 160/237 [00:41<00:21,  3.51it/s]evaluate for the 12-th batch, evaluate loss: 0.7107369899749756:  40%|███████▏          | 10/25 [00:00<00:01, 12.69it/s]evaluate for the 12-th batch, evaluate loss: 0.7107369899749756:  48%|████████▋         | 12/25 [00:00<00:01, 12.89it/s]Epoch: 1, train for the 192-th batch, train loss: 0.33751845359802246:  80%|███████▉  | 192/241 [00:41<00:13,  3.74it/s]Epoch: 1, train for the 161-th batch, train loss: 0.6055739521980286:  68%|███████▍   | 161/237 [00:41<00:21,  3.55it/s]Epoch: 2, train for the 46-th batch, train loss: 0.47853735089302063:  31%|███▋        | 45/146 [00:07<00:16,  6.08it/s]Epoch: 2, train for the 46-th batch, train loss: 0.47853735089302063:  32%|███▊        | 46/146 [00:07<00:15,  6.58it/s]Epoch: 1, train for the 146-th batch, train loss: 0.3877403140068054:  38%|████▏      | 146/383 [00:38<01:25,  2.76it/s]evaluate for the 13-th batch, evaluate loss: 0.7611269950866699:  48%|████████▋         | 12/25 [00:01<00:01, 12.89it/s]Epoch: 2, train for the 77-th batch, train loss: 0.42692065238952637:  64%|███████▋    | 76/119 [00:11<00:07,  5.52it/s]Epoch: 2, train for the 77-th batch, train loss: 0.42692065238952637:  65%|███████▊    | 77/119 [00:11<00:07,  5.53it/s]Epoch: 1, train for the 193-th batch, train loss: 0.3773078918457031:  80%|████████▊  | 192/241 [00:41<00:13,  3.74it/s]Epoch: 1, train for the 193-th batch, train loss: 0.3773078918457031:  80%|████████▊  | 193/241 [00:41<00:11,  4.26it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5346316695213318:  32%|████         | 46/146 [00:07<00:15,  6.58it/s]evaluate for the 14-th batch, evaluate loss: 0.7595703601837158:  48%|████████▋         | 12/25 [00:01<00:01, 12.89it/s]evaluate for the 14-th batch, evaluate loss: 0.7595703601837158:  56%|██████████        | 14/25 [00:01<00:00, 12.18it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5346316695213318:  32%|████▏        | 47/146 [00:07<00:14,  6.65it/s]Epoch: 1, train for the 162-th batch, train loss: 0.6348663568496704:  68%|███████▍   | 161/237 [00:41<00:21,  3.55it/s]Epoch: 1, train for the 162-th batch, train loss: 0.6348663568496704:  68%|███████▌   | 162/237 [00:41<00:19,  3.84it/s]evaluate for the 15-th batch, evaluate loss: 0.8248167037963867:  56%|██████████        | 14/25 [00:01<00:00, 12.18it/s]Epoch: 1, train for the 147-th batch, train loss: 0.35771191120147705:  38%|███▊      | 146/383 [00:39<01:25,  2.76it/s]Epoch: 2, train for the 78-th batch, train loss: 0.416015088558197:  65%|█████████     | 77/119 [00:12<00:07,  5.53it/s]Epoch: 2, train for the 78-th batch, train loss: 0.416015088558197:  66%|█████████▏    | 78/119 [00:12<00:07,  5.62it/s]Epoch: 1, train for the 147-th batch, train loss: 0.35771191120147705:  38%|███▊      | 147/383 [00:39<01:19,  2.98it/s]evaluate for the 16-th batch, evaluate loss: 0.7881717681884766:  56%|██████████        | 14/25 [00:01<00:00, 12.18it/s]evaluate for the 16-th batch, evaluate loss: 0.7881717681884766:  64%|███████████▌      | 16/25 [00:01<00:00, 12.77it/s]Epoch: 2, train for the 48-th batch, train loss: 0.49020352959632874:  32%|███▊        | 47/146 [00:07<00:14,  6.65it/s]Epoch: 2, train for the 48-th batch, train loss: 0.49020352959632874:  33%|███▉        | 48/146 [00:07<00:15,  6.49it/s]evaluate for the 17-th batch, evaluate loss: 0.7470537424087524:  64%|███████████▌      | 16/25 [00:01<00:00, 12.77it/s]Epoch: 2, train for the 79-th batch, train loss: 0.4275205433368683:  66%|████████▌    | 78/119 [00:12<00:07,  5.62it/s]Epoch: 2, train for the 79-th batch, train loss: 0.4275205433368683:  66%|████████▋    | 79/119 [00:12<00:06,  5.93it/s]evaluate for the 18-th batch, evaluate loss: 0.7275352478027344:  64%|███████████▌      | 16/25 [00:01<00:00, 12.77it/s]evaluate for the 18-th batch, evaluate loss: 0.7275352478027344:  72%|████████████▉     | 18/25 [00:01<00:00, 13.34it/s]Epoch: 2, train for the 49-th batch, train loss: 0.48464998602867126:  33%|███▉        | 48/146 [00:07<00:15,  6.49it/s]Epoch: 2, train for the 49-th batch, train loss: 0.48464998602867126:  34%|████        | 49/146 [00:07<00:15,  6.42it/s]Epoch: 1, train for the 148-th batch, train loss: 0.3011411726474762:  38%|████▏      | 147/383 [00:39<01:19,  2.98it/s]evaluate for the 19-th batch, evaluate loss: 0.6610303521156311:  72%|████████████▉     | 18/25 [00:01<00:00, 13.34it/s]Epoch: 1, train for the 194-th batch, train loss: 0.3717866539955139:  80%|████████▊  | 193/241 [00:41<00:11,  4.26it/s]Epoch: 1, train for the 148-th batch, train loss: 0.3011411726474762:  39%|████▎      | 148/383 [00:39<01:12,  3.22it/s]Epoch: 1, train for the 163-th batch, train loss: 0.6303892731666565:  68%|███████▌   | 162/237 [00:41<00:19,  3.84it/s]Epoch: 2, train for the 80-th batch, train loss: 0.4353717565536499:  66%|████████▋    | 79/119 [00:12<00:06,  5.93it/s]Epoch: 1, train for the 194-th batch, train loss: 0.3717866539955139:  80%|████████▊  | 194/241 [00:41<00:13,  3.46it/s]Epoch: 2, train for the 80-th batch, train loss: 0.4353717565536499:  67%|████████▋    | 80/119 [00:12<00:06,  6.21it/s]Epoch: 1, train for the 163-th batch, train loss: 0.6303892731666565:  69%|███████▌   | 163/237 [00:41<00:21,  3.43it/s]evaluate for the 20-th batch, evaluate loss: 0.6805912852287292:  72%|████████████▉     | 18/25 [00:01<00:00, 13.34it/s]evaluate for the 20-th batch, evaluate loss: 0.6805912852287292:  80%|██████████████▍   | 20/25 [00:01<00:00, 12.54it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5341163277626038:  34%|████▎        | 49/146 [00:07<00:15,  6.42it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5341163277626038:  34%|████▍        | 50/146 [00:07<00:14,  6.56it/s]Epoch: 1, train for the 195-th batch, train loss: 0.5675321221351624:  80%|████████▊  | 194/241 [00:41<00:13,  3.46it/s]Epoch: 1, train for the 195-th batch, train loss: 0.5675321221351624:  81%|████████▉  | 195/241 [00:41<00:11,  3.92it/s]Epoch: 1, train for the 149-th batch, train loss: 0.3989364206790924:  39%|████▎      | 148/383 [00:39<01:12,  3.22it/s]Epoch: 2, train for the 51-th batch, train loss: 0.4558793902397156:  34%|████▍        | 50/146 [00:07<00:14,  6.56it/s]Epoch: 2, train for the 51-th batch, train loss: 0.4558793902397156:  35%|████▌        | 51/146 [00:07<00:13,  6.82it/s]Epoch: 1, train for the 149-th batch, train loss: 0.3989364206790924:  39%|████▎      | 149/383 [00:39<01:07,  3.46it/s]evaluate for the 21-th batch, evaluate loss: 0.7003728151321411:  80%|██████████████▍   | 20/25 [00:01<00:00, 12.54it/s]Epoch: 2, train for the 52-th batch, train loss: 0.5005536079406738:  35%|████▌        | 51/146 [00:07<00:13,  6.82it/s]Epoch: 2, train for the 52-th batch, train loss: 0.5005536079406738:  36%|████▋        | 52/146 [00:07<00:12,  7.37it/s]evaluate for the 22-th batch, evaluate loss: 0.631748378276825:  80%|███████████████▏   | 20/25 [00:01<00:00, 12.54it/s]evaluate for the 22-th batch, evaluate loss: 0.631748378276825:  88%|████████████████▋  | 22/25 [00:01<00:00, 10.01it/s]evaluate for the 23-th batch, evaluate loss: 0.6402144432067871:  88%|███████████████▊  | 22/25 [00:01<00:00, 10.01it/s]Epoch: 2, train for the 53-th batch, train loss: 0.4800524413585663:  36%|████▋        | 52/146 [00:08<00:12,  7.37it/s]Epoch: 2, train for the 53-th batch, train loss: 0.4800524413585663:  36%|████▋        | 53/146 [00:08<00:11,  7.89it/s]evaluate for the 24-th batch, evaluate loss: 0.6487432718276978:  88%|███████████████▊  | 22/25 [00:01<00:00, 10.01it/s]evaluate for the 24-th batch, evaluate loss: 0.6487432718276978:  96%|█████████████████▎| 24/25 [00:01<00:00, 11.24it/s]Epoch: 1, train for the 196-th batch, train loss: 0.5175345540046692:  81%|████████▉  | 195/241 [00:42<00:11,  3.92it/s]Epoch: 1, train for the 196-th batch, train loss: 0.5175345540046692:  81%|████████▉  | 196/241 [00:42<00:12,  3.48it/s]evaluate for the 25-th batch, evaluate loss: 0.6042250394821167:  96%|█████████████████▎| 24/25 [00:02<00:00, 11.24it/s]evaluate for the 25-th batch, evaluate loss: 0.6042250394821167: 100%|██████████████████| 25/25 [00:02<00:00, 12.13it/s]
Epoch: 1, train for the 150-th batch, train loss: 0.5496660470962524:  39%|████▎      | 149/383 [00:40<01:07,  3.46it/s]Epoch: 1, train for the 164-th batch, train loss: 0.6128371953964233:  69%|███████▌   | 163/237 [00:42<00:21,  3.43it/s]Epoch: 2, train for the 81-th batch, train loss: 0.46530988812446594:  67%|████████    | 80/119 [00:13<00:06,  6.21it/s]Epoch: 2, train for the 54-th batch, train loss: 0.4982694983482361:  36%|████▋        | 53/146 [00:08<00:11,  7.89it/s]Epoch: 1, train for the 164-th batch, train loss: 0.6128371953964233:  69%|███████▌   | 164/237 [00:42<00:27,  2.64it/s]Epoch: 2, train for the 81-th batch, train loss: 0.46530988812446594:  68%|████████▏   | 81/119 [00:13<00:11,  3.43it/s]Epoch: 2, train for the 54-th batch, train loss: 0.4982694983482361:  37%|████▊        | 54/146 [00:08<00:13,  7.02it/s]Epoch: 1, train for the 150-th batch, train loss: 0.5496660470962524:  39%|████▎      | 150/383 [00:40<01:13,  3.16it/s]Epoch: 1, train for the 197-th batch, train loss: 0.5286954045295715:  81%|████████▉  | 196/241 [00:42<00:12,  3.48it/s]INFO:root:Epoch: 1, learning rate: 0.0001, train loss: 0.5725
INFO:root:train average_precision, 0.7736
INFO:root:train roc_auc, 0.7481
INFO:root:validate loss: 0.6085
INFO:root:validate average_precision, 0.7561
INFO:root:validate roc_auc, 0.7865
INFO:root:new node validate loss: 0.8103
INFO:root:new node validate first_1_average_precision, 0.6621
INFO:root:new node validate first_1_roc_auc, 0.6720
INFO:root:new node validate first_3_average_precision, 0.6306
INFO:root:new node validate first_3_roc_auc, 0.6171
INFO:root:new node validate first_10_average_precision, 0.6334
INFO:root:new node validate first_10_roc_auc, 0.6244
INFO:root:new node validate average_precision, 0.6749
INFO:root:new node validate roc_auc, 0.6669
INFO:root:save model ./saved_models/TGN/ia-retweet-pol/TGN_seed0_tgn-ia-retweet-pol-reparamcorr-time-mlp/TGN_seed0_tgn-ia-retweet-pol-reparamcorr-time-mlp.pkl
Epoch: 1, train for the 197-th batch, train loss: 0.5286954045295715:  82%|████████▉  | 197/241 [00:42<00:11,  3.74it/s]Epoch: 1, train for the 165-th batch, train loss: 0.5989560484886169:  69%|███████▌   | 164/237 [00:42<00:27,  2.64it/s]Epoch: 1, train for the 165-th batch, train loss: 0.5989560484886169:  70%|███████▋   | 165/237 [00:42<00:23,  3.07it/s]Epoch: 2, train for the 82-th batch, train loss: 0.4416942596435547:  68%|████████▊    | 81/119 [00:13<00:11,  3.43it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5080657005310059:  37%|████▊        | 54/146 [00:08<00:13,  7.02it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5789358615875244:  39%|████▎      | 150/383 [00:40<01:13,  3.16it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5080657005310059:  38%|████▉        | 55/146 [00:08<00:14,  6.13it/s]Epoch: 2, train for the 82-th batch, train loss: 0.4416942596435547:  69%|████████▉    | 82/119 [00:13<00:09,  3.73it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5789358615875244:  39%|████▎      | 151/383 [00:40<01:08,  3.39it/s]Epoch: 1, train for the 198-th batch, train loss: 0.5219375491142273:  82%|████████▉  | 197/241 [00:42<00:11,  3.74it/s]Epoch: 1, train for the 198-th batch, train loss: 0.5219375491142273:  82%|█████████  | 198/241 [00:42<00:09,  4.43it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5066859722137451:  38%|████▉        | 55/146 [00:08<00:14,  6.13it/s]Epoch: 2, train for the 83-th batch, train loss: 0.46345648169517517:  69%|████████▎   | 82/119 [00:13<00:09,  3.73it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5066859722137451:  38%|████▉        | 56/146 [00:08<00:17,  5.18it/s]Epoch: 2, train for the 83-th batch, train loss: 0.46345648169517517:  70%|████████▎   | 83/119 [00:13<00:09,  3.73it/s]Epoch: 1, train for the 199-th batch, train loss: 0.35434490442276:  82%|██████████▋  | 198/241 [00:42<00:09,  4.43it/s]Epoch: 1, train for the 152-th batch, train loss: 0.4854896366596222:  39%|████▎      | 151/383 [00:40<01:08,  3.39it/s]Epoch: 1, train for the 199-th batch, train loss: 0.35434490442276:  83%|██████████▋  | 199/241 [00:42<00:09,  4.51it/s]Epoch: 1, train for the 152-th batch, train loss: 0.4854896366596222:  40%|████▎      | 152/383 [00:40<01:06,  3.47it/s]Epoch: 1, train for the 166-th batch, train loss: 0.5475494861602783:  70%|███████▋   | 165/237 [00:42<00:23,  3.07it/s]Epoch: 2, train for the 57-th batch, train loss: 0.5167206525802612:  38%|████▉        | 56/146 [00:08<00:17,  5.18it/s]Epoch: 1, train for the 200-th batch, train loss: 0.4444366693496704:  83%|█████████  | 199/241 [00:42<00:09,  4.51it/s]Epoch: 2, train for the 57-th batch, train loss: 0.5167206525802612:  39%|█████        | 57/146 [00:08<00:16,  5.47it/s]Epoch: 1, train for the 200-th batch, train loss: 0.4444366693496704:  83%|█████████▏ | 200/241 [00:42<00:08,  5.01it/s]Epoch: 2, train for the 84-th batch, train loss: 0.42719221115112305:  70%|████████▎   | 83/119 [00:13<00:09,  3.73it/s]Epoch: 1, train for the 166-th batch, train loss: 0.5475494861602783:  70%|███████▋   | 166/237 [00:42<00:25,  2.74it/s]Epoch: 2, train for the 84-th batch, train loss: 0.42719221115112305:  71%|████████▍   | 84/119 [00:13<00:08,  4.13it/s]Epoch: 1, train for the 153-th batch, train loss: 0.5358883738517761:  40%|████▎      | 152/383 [00:40<01:06,  3.47it/s]Epoch: 1, train for the 153-th batch, train loss: 0.5358883738517761:  40%|████▍      | 153/383 [00:40<01:00,  3.78it/s]Epoch: 2, train for the 58-th batch, train loss: 0.49395468831062317:  39%|████▋       | 57/146 [00:08<00:16,  5.47it/s]Epoch: 2, train for the 58-th batch, train loss: 0.49395468831062317:  40%|████▊       | 58/146 [00:09<00:15,  5.54it/s]Epoch: 2, train for the 85-th batch, train loss: 0.45642751455307007:  71%|████████▍   | 84/119 [00:13<00:08,  4.13it/s]Epoch: 1, train for the 167-th batch, train loss: 0.5909255146980286:  70%|███████▋   | 166/237 [00:43<00:25,  2.74it/s]Epoch: 2, train for the 85-th batch, train loss: 0.45642751455307007:  71%|████████▌   | 85/119 [00:13<00:07,  4.57it/s]Epoch: 1, train for the 167-th batch, train loss: 0.5909255146980286:  70%|███████▊   | 167/237 [00:43<00:21,  3.18it/s]Epoch: 1, train for the 201-th batch, train loss: 0.4821336269378662:  83%|█████████▏ | 200/241 [00:43<00:08,  5.01it/s]Epoch: 1, train for the 201-th batch, train loss: 0.4821336269378662:  83%|█████████▏ | 201/241 [00:43<00:08,  4.55it/s]Epoch: 1, train for the 154-th batch, train loss: 0.4152407646179199:  40%|████▍      | 153/383 [00:41<01:00,  3.78it/s]Epoch: 2, train for the 59-th batch, train loss: 0.4839771091938019:  40%|█████▏       | 58/146 [00:09<00:15,  5.54it/s]Epoch: 2, train for the 59-th batch, train loss: 0.4839771091938019:  40%|█████▎       | 59/146 [00:09<00:14,  5.86it/s]Epoch: 1, train for the 154-th batch, train loss: 0.4152407646179199:  40%|████▍      | 154/383 [00:41<00:58,  3.93it/s]Epoch: 2, train for the 86-th batch, train loss: 0.46940430998802185:  71%|████████▌   | 85/119 [00:14<00:07,  4.57it/s]Epoch: 2, train for the 86-th batch, train loss: 0.46940430998802185:  72%|████████▋   | 86/119 [00:14<00:06,  4.91it/s]Epoch: 1, train for the 202-th batch, train loss: 0.2830529510974884:  83%|█████████▏ | 201/241 [00:43<00:08,  4.55it/s]Epoch: 1, train for the 202-th batch, train loss: 0.2830529510974884:  84%|█████████▏ | 202/241 [00:43<00:07,  4.94it/s]Epoch: 2, train for the 60-th batch, train loss: 0.508408784866333:  40%|█████▋        | 59/146 [00:09<00:14,  5.86it/s]Epoch: 2, train for the 60-th batch, train loss: 0.508408784866333:  41%|█████▊        | 60/146 [00:09<00:13,  6.23it/s]Epoch: 1, train for the 168-th batch, train loss: 0.6009279489517212:  70%|███████▊   | 167/237 [00:43<00:21,  3.18it/s]Epoch: 2, train for the 87-th batch, train loss: 0.48570165038108826:  72%|████████▋   | 86/119 [00:14<00:06,  4.91it/s]Epoch: 2, train for the 87-th batch, train loss: 0.48570165038108826:  73%|████████▊   | 87/119 [00:14<00:06,  5.29it/s]Epoch: 1, train for the 168-th batch, train loss: 0.6009279489517212:  71%|███████▊   | 168/237 [00:43<00:21,  3.23it/s]Epoch: 1, train for the 155-th batch, train loss: 0.3195527195930481:  40%|████▍      | 154/383 [00:41<00:58,  3.93it/s]Epoch: 1, train for the 203-th batch, train loss: 0.5177798867225647:  84%|█████████▏ | 202/241 [00:43<00:07,  4.94it/s]Epoch: 1, train for the 155-th batch, train loss: 0.3195527195930481:  40%|████▍      | 155/383 [00:41<00:55,  4.07it/s]Epoch: 1, train for the 203-th batch, train loss: 0.5177798867225647:  84%|█████████▎ | 203/241 [00:43<00:06,  5.57it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5506505966186523:  41%|█████▎       | 60/146 [00:09<00:13,  6.23it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5506505966186523:  42%|█████▍       | 61/146 [00:09<00:13,  6.43it/s]Epoch: 2, train for the 88-th batch, train loss: 0.4774622321128845:  73%|█████████▌   | 87/119 [00:14<00:06,  5.29it/s]Epoch: 2, train for the 88-th batch, train loss: 0.4774622321128845:  74%|█████████▌   | 88/119 [00:14<00:05,  5.68it/s]Epoch: 1, train for the 204-th batch, train loss: 0.4821357727050781:  84%|█████████▎ | 203/241 [00:43<00:06,  5.57it/s]Epoch: 1, train for the 204-th batch, train loss: 0.4821357727050781:  85%|█████████▎ | 204/241 [00:43<00:06,  5.84it/s]Epoch: 2, train for the 62-th batch, train loss: 0.4788851737976074:  42%|█████▍       | 61/146 [00:09<00:13,  6.43it/s]Epoch: 2, train for the 62-th batch, train loss: 0.4788851737976074:  42%|█████▌       | 62/146 [00:09<00:13,  6.36it/s]Epoch: 2, train for the 89-th batch, train loss: 0.46050527691841125:  74%|████████▊   | 88/119 [00:14<00:05,  5.68it/s]Epoch: 2, train for the 89-th batch, train loss: 0.46050527691841125:  75%|████████▉   | 89/119 [00:14<00:05,  5.98it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]Epoch: 2, train for the 63-th batch, train loss: 0.5146252512931824:  42%|█████▌       | 62/146 [00:09<00:13,  6.36it/s]Epoch: 2, train for the 63-th batch, train loss: 0.5146252512931824:  43%|█████▌       | 63/146 [00:09<00:12,  6.81it/s]Epoch: 1, train for the 156-th batch, train loss: 0.3326079547405243:  40%|████▍      | 155/383 [00:41<00:55,  4.07it/s]Epoch: 1, train for the 169-th batch, train loss: 0.6012986898422241:  71%|███████▊   | 168/237 [00:43<00:21,  3.23it/s]Epoch: 2, train for the 1-th batch, train loss: 0.8878486156463623:   0%|                       | 0/151 [00:00<?, ?it/s]Epoch: 1, train for the 156-th batch, train loss: 0.3326079547405243:  41%|████▍      | 156/383 [00:41<01:04,  3.51it/s]Epoch: 2, train for the 90-th batch, train loss: 0.4435898959636688:  75%|█████████▋   | 89/119 [00:14<00:05,  5.98it/s]Epoch: 2, train for the 90-th batch, train loss: 0.4435898959636688:  76%|█████████▊   | 90/119 [00:14<00:04,  6.30it/s]Epoch: 1, train for the 169-th batch, train loss: 0.6012986898422241:  71%|███████▊   | 169/237 [00:43<00:23,  2.89it/s]Epoch: 1, train for the 205-th batch, train loss: 0.4877301752567291:  85%|█████████▎ | 204/241 [00:43<00:06,  5.84it/s]Epoch: 1, train for the 205-th batch, train loss: 0.4877301752567291:  85%|█████████▎ | 205/241 [00:43<00:07,  4.78it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5310379266738892:  43%|█████▌       | 63/146 [00:09<00:12,  6.81it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5310379266738892:  44%|█████▋       | 64/146 [00:09<00:11,  6.85it/s]Epoch: 2, train for the 2-th batch, train loss: 0.8238859176635742:   0%|                       | 0/151 [00:00<?, ?it/s]Epoch: 2, train for the 2-th batch, train loss: 0.8238859176635742:   1%|▏              | 2/151 [00:00<00:17,  8.60it/s]Epoch: 2, train for the 91-th batch, train loss: 0.4638586938381195:  76%|█████████▊   | 90/119 [00:14<00:04,  6.30it/s]Epoch: 2, train for the 91-th batch, train loss: 0.4638586938381195:  76%|█████████▉   | 91/119 [00:14<00:04,  6.28it/s]Epoch: 1, train for the 157-th batch, train loss: 0.44869136810302734:  41%|████      | 156/383 [00:41<01:04,  3.51it/s]Epoch: 1, train for the 170-th batch, train loss: 0.5687517523765564:  71%|███████▊   | 169/237 [00:44<00:23,  2.89it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5546287298202515:  44%|█████▋       | 64/146 [00:09<00:11,  6.85it/s]Epoch: 1, train for the 157-th batch, train loss: 0.44869136810302734:  41%|████      | 157/383 [00:41<01:01,  3.69it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5546287298202515:  45%|█████▊       | 65/146 [00:09<00:11,  6.97it/s]Epoch: 1, train for the 206-th batch, train loss: 0.5550205707550049:  85%|█████████▎ | 205/241 [00:44<00:07,  4.78it/s]Epoch: 1, train for the 170-th batch, train loss: 0.5687517523765564:  72%|███████▉   | 170/237 [00:44<00:21,  3.19it/s]Epoch: 1, train for the 206-th batch, train loss: 0.5550205707550049:  85%|█████████▍ | 206/241 [00:44<00:07,  4.91it/s]Epoch: 2, train for the 3-th batch, train loss: 0.7685519456863403:   1%|▏              | 2/151 [00:00<00:17,  8.60it/s]Epoch: 2, train for the 3-th batch, train loss: 0.7685519456863403:   2%|▎              | 3/151 [00:00<00:19,  7.44it/s]Epoch: 2, train for the 92-th batch, train loss: 0.42759454250335693:  76%|█████████▏  | 91/119 [00:14<00:04,  6.28it/s]Epoch: 2, train for the 92-th batch, train loss: 0.42759454250335693:  77%|█████████▎  | 92/119 [00:14<00:04,  6.54it/s]Epoch: 2, train for the 66-th batch, train loss: 0.5282911062240601:  45%|█████▊       | 65/146 [00:10<00:11,  6.97it/s]Epoch: 2, train for the 66-th batch, train loss: 0.5282911062240601:  45%|█████▉       | 66/146 [00:10<00:11,  7.26it/s]Epoch: 1, train for the 171-th batch, train loss: 0.583641529083252:  72%|████████▌   | 170/237 [00:44<00:21,  3.19it/s]Epoch: 2, train for the 4-th batch, train loss: 0.7124196290969849:   2%|▎              | 3/151 [00:00<00:19,  7.44it/s]Epoch: 2, train for the 4-th batch, train loss: 0.7124196290969849:   3%|▍              | 4/151 [00:00<00:20,  7.21it/s]Epoch: 1, train for the 171-th batch, train loss: 0.583641529083252:  72%|████████▋   | 171/237 [00:44<00:18,  3.58it/s]Epoch: 2, train for the 93-th batch, train loss: 0.42587020993232727:  77%|█████████▎  | 92/119 [00:15<00:04,  6.54it/s]Epoch: 2, train for the 93-th batch, train loss: 0.42587020993232727:  78%|█████████▍  | 93/119 [00:15<00:04,  6.16it/s]Epoch: 2, train for the 5-th batch, train loss: 0.7103595733642578:   3%|▍              | 4/151 [00:00<00:20,  7.21it/s]Epoch: 2, train for the 67-th batch, train loss: 0.5591652393341064:  45%|█████▉       | 66/146 [00:10<00:11,  7.26it/s]Epoch: 2, train for the 67-th batch, train loss: 0.5591652393341064:  46%|█████▉       | 67/146 [00:10<00:11,  6.74it/s]Epoch: 1, train for the 207-th batch, train loss: 0.5552515387535095:  85%|█████████▍ | 206/241 [00:44<00:07,  4.91it/s]Epoch: 1, train for the 158-th batch, train loss: 0.4771749675273895:  41%|████▌      | 157/383 [00:42<01:01,  3.69it/s]Epoch: 2, train for the 94-th batch, train loss: 0.4292989671230316:  78%|██████████▏  | 93/119 [00:15<00:04,  6.16it/s]Epoch: 1, train for the 207-th batch, train loss: 0.5552515387535095:  86%|█████████▍ | 207/241 [00:44<00:08,  3.87it/s]Epoch: 2, train for the 94-th batch, train loss: 0.4292989671230316:  79%|██████████▎  | 94/119 [00:15<00:04,  6.09it/s]Epoch: 2, train for the 6-th batch, train loss: 0.6951942443847656:   3%|▍              | 4/151 [00:00<00:20,  7.21it/s]Epoch: 2, train for the 6-th batch, train loss: 0.6951942443847656:   4%|▌              | 6/151 [00:00<00:18,  7.70it/s]Epoch: 1, train for the 172-th batch, train loss: 0.5298787355422974:  72%|███████▉   | 171/237 [00:44<00:18,  3.58it/s]Epoch: 2, train for the 68-th batch, train loss: 0.513985276222229:  46%|██████▍       | 67/146 [00:10<00:11,  6.74it/s]Epoch: 2, train for the 68-th batch, train loss: 0.513985276222229:  47%|██████▌       | 68/146 [00:10<00:11,  6.58it/s]Epoch: 1, train for the 158-th batch, train loss: 0.4771749675273895:  41%|████▌      | 158/383 [00:42<01:14,  3.02it/s]Epoch: 1, train for the 172-th batch, train loss: 0.5298787355422974:  73%|███████▉   | 172/237 [00:44<00:18,  3.59it/s]Epoch: 2, train for the 95-th batch, train loss: 0.35552722215652466:  79%|█████████▍  | 94/119 [00:15<00:04,  6.09it/s]Epoch: 2, train for the 95-th batch, train loss: 0.35552722215652466:  80%|█████████▌  | 95/119 [00:15<00:04,  5.72it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5311615467071533:  47%|██████       | 68/146 [00:10<00:11,  6.58it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5311615467071533:  47%|██████▏      | 69/146 [00:10<00:12,  6.17it/s]Epoch: 2, train for the 7-th batch, train loss: 0.6771693229675293:   4%|▌              | 6/151 [00:00<00:18,  7.70it/s]Epoch: 1, train for the 208-th batch, train loss: 0.5372602939605713:  86%|█████████▍ | 207/241 [00:44<00:08,  3.87it/s]Epoch: 2, train for the 7-th batch, train loss: 0.6771693229675293:   5%|▋              | 7/151 [00:00<00:21,  6.56it/s]Epoch: 1, train for the 208-th batch, train loss: 0.5372602939605713:  86%|█████████▍ | 208/241 [00:44<00:08,  3.93it/s]Epoch: 1, train for the 159-th batch, train loss: 0.4931483864784241:  41%|████▌      | 158/383 [00:42<01:14,  3.02it/s]Epoch: 1, train for the 159-th batch, train loss: 0.4931483864784241:  42%|████▌      | 159/383 [00:42<01:10,  3.19it/s]Epoch: 2, train for the 96-th batch, train loss: 0.37338805198669434:  80%|█████████▌  | 95/119 [00:15<00:04,  5.72it/s]Epoch: 2, train for the 96-th batch, train loss: 0.37338805198669434:  81%|█████████▋  | 96/119 [00:15<00:03,  5.91it/s]Epoch: 2, train for the 8-th batch, train loss: 0.717228353023529:   5%|▋               | 7/151 [00:01<00:21,  6.56it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5353518128395081:  47%|██████▏      | 69/146 [00:10<00:12,  6.17it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5353518128395081:  48%|██████▏      | 70/146 [00:10<00:12,  5.98it/s]Epoch: 2, train for the 8-th batch, train loss: 0.717228353023529:   5%|▊               | 8/151 [00:01<00:22,  6.36it/s]Epoch: 1, train for the 209-th batch, train loss: 0.5174506902694702:  86%|█████████▍ | 208/241 [00:44<00:08,  3.93it/s]Epoch: 1, train for the 209-th batch, train loss: 0.5174506902694702:  87%|█████████▌ | 209/241 [00:44<00:07,  4.25it/s]Epoch: 2, train for the 97-th batch, train loss: 0.431395024061203:  81%|███████████▎  | 96/119 [00:15<00:03,  5.91it/s]Epoch: 2, train for the 97-th batch, train loss: 0.431395024061203:  82%|███████████▍  | 97/119 [00:15<00:03,  6.29it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5473338961601257:  48%|██████▏      | 70/146 [00:10<00:12,  5.98it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5473338961601257:  49%|██████▎      | 71/146 [00:10<00:12,  6.16it/s]Epoch: 1, train for the 160-th batch, train loss: 0.48109525442123413:  42%|████▏     | 159/383 [00:42<01:10,  3.19it/s]Epoch: 1, train for the 173-th batch, train loss: 0.5123531222343445:  73%|███████▉   | 172/237 [00:45<00:18,  3.59it/s]Epoch: 2, train for the 9-th batch, train loss: 0.7162579298019409:   5%|▊              | 8/151 [00:01<00:22,  6.36it/s]Epoch: 2, train for the 9-th batch, train loss: 0.7162579298019409:   6%|▉              | 9/151 [00:01<00:23,  6.13it/s]Epoch: 1, train for the 160-th batch, train loss: 0.48109525442123413:  42%|████▏     | 160/383 [00:42<01:07,  3.29it/s]Epoch: 1, train for the 210-th batch, train loss: 0.4186188280582428:  87%|█████████▌ | 209/241 [00:45<00:07,  4.25it/s]Epoch: 1, train for the 173-th batch, train loss: 0.5123531222343445:  73%|████████   | 173/237 [00:45<00:22,  2.81it/s]Epoch: 1, train for the 210-th batch, train loss: 0.4186188280582428:  87%|█████████▌ | 210/241 [00:45<00:06,  4.59it/s]Epoch: 2, train for the 98-th batch, train loss: 0.4196071922779083:  82%|██████████▌  | 97/119 [00:15<00:03,  6.29it/s]Epoch: 2, train for the 98-th batch, train loss: 0.4196071922779083:  82%|██████████▋  | 98/119 [00:15<00:03,  6.65it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5561749935150146:  49%|██████▎      | 71/146 [00:11<00:12,  6.16it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5561749935150146:  49%|██████▍      | 72/146 [00:11<00:12,  6.03it/s]Epoch: 2, train for the 10-th batch, train loss: 0.728872537612915:   6%|▉              | 9/151 [00:01<00:23,  6.13it/s]Epoch: 2, train for the 10-th batch, train loss: 0.728872537612915:   7%|▉             | 10/151 [00:01<00:22,  6.15it/s]Epoch: 2, train for the 99-th batch, train loss: 0.41123825311660767:  82%|█████████▉  | 98/119 [00:16<00:03,  6.65it/s]Epoch: 2, train for the 99-th batch, train loss: 0.41123825311660767:  83%|█████████▉  | 99/119 [00:16<00:03,  6.46it/s]Epoch: 1, train for the 174-th batch, train loss: 0.5528447031974792:  73%|████████   | 173/237 [00:45<00:22,  2.81it/s]Epoch: 2, train for the 11-th batch, train loss: 0.7455581426620483:   7%|▊            | 10/151 [00:01<00:22,  6.15it/s]Epoch: 2, train for the 11-th batch, train loss: 0.7455581426620483:   7%|▉            | 11/151 [00:01<00:20,  6.88it/s]Epoch: 1, train for the 161-th batch, train loss: 0.4911743104457855:  42%|████▌      | 160/383 [00:43<01:07,  3.29it/s]Epoch: 1, train for the 174-th batch, train loss: 0.5528447031974792:  73%|████████   | 174/237 [00:45<00:20,  3.11it/s]Epoch: 2, train for the 73-th batch, train loss: 0.5305783748626709:  49%|██████▍      | 72/146 [00:11<00:12,  6.03it/s]Epoch: 2, train for the 73-th batch, train loss: 0.5305783748626709:  50%|██████▌      | 73/146 [00:11<00:11,  6.27it/s]Epoch: 1, train for the 161-th batch, train loss: 0.4911743104457855:  42%|████▌      | 161/383 [00:43<01:06,  3.36it/s]Epoch: 2, train for the 100-th batch, train loss: 0.4490930140018463:  83%|█████████▉  | 99/119 [00:16<00:03,  6.46it/s]Epoch: 2, train for the 100-th batch, train loss: 0.4490930140018463:  84%|█████████▏ | 100/119 [00:16<00:02,  6.60it/s]Epoch: 2, train for the 12-th batch, train loss: 0.7368432283401489:   7%|▉            | 11/151 [00:01<00:20,  6.88it/s]Epoch: 2, train for the 12-th batch, train loss: 0.7368432283401489:   8%|█            | 12/151 [00:01<00:18,  7.46it/s]Epoch: 1, train for the 211-th batch, train loss: 0.38613730669021606:  87%|████████▋ | 210/241 [00:45<00:06,  4.59it/s]Epoch: 2, train for the 74-th batch, train loss: 0.5456988215446472:  50%|██████▌      | 73/146 [00:11<00:11,  6.27it/s]Epoch: 1, train for the 211-th batch, train loss: 0.38613730669021606:  88%|████████▊ | 211/241 [00:45<00:07,  3.76it/s]Epoch: 2, train for the 74-th batch, train loss: 0.5456988215446472:  51%|██████▌      | 74/146 [00:11<00:10,  6.57it/s]Epoch: 2, train for the 101-th batch, train loss: 0.4385266900062561:  84%|█████████▏ | 100/119 [00:16<00:02,  6.60it/s]Epoch: 2, train for the 13-th batch, train loss: 0.707090437412262:   8%|█             | 12/151 [00:01<00:18,  7.46it/s]Epoch: 2, train for the 101-th batch, train loss: 0.4385266900062561:  85%|█████████▎ | 101/119 [00:16<00:02,  6.57it/s]Epoch: 2, train for the 13-th batch, train loss: 0.707090437412262:   9%|█▏            | 13/151 [00:01<00:18,  7.32it/s]Epoch: 1, train for the 162-th batch, train loss: 0.5310205817222595:  42%|████▌      | 161/383 [00:43<01:06,  3.36it/s]Epoch: 1, train for the 162-th batch, train loss: 0.5310205817222595:  42%|████▋      | 162/383 [00:43<01:03,  3.51it/s]Epoch: 2, train for the 75-th batch, train loss: 0.5189529061317444:  51%|██████▌      | 74/146 [00:11<00:10,  6.57it/s]Epoch: 1, train for the 212-th batch, train loss: 0.5080373287200928:  88%|█████████▋ | 211/241 [00:45<00:07,  3.76it/s]Epoch: 2, train for the 75-th batch, train loss: 0.5189529061317444:  51%|██████▋      | 75/146 [00:11<00:10,  6.50it/s]Epoch: 1, train for the 212-th batch, train loss: 0.5080373287200928:  88%|█████████▋ | 212/241 [00:45<00:06,  4.16it/s]Epoch: 2, train for the 102-th batch, train loss: 0.44207504391670227:  85%|████████▍ | 101/119 [00:16<00:02,  6.57it/s]Epoch: 2, train for the 14-th batch, train loss: 0.7055169343948364:   9%|█            | 13/151 [00:01<00:18,  7.32it/s]Epoch: 2, train for the 102-th batch, train loss: 0.44207504391670227:  86%|████████▌ | 102/119 [00:16<00:02,  6.71it/s]Epoch: 1, train for the 175-th batch, train loss: 0.5659826397895813:  73%|████████   | 174/237 [00:45<00:20,  3.11it/s]Epoch: 2, train for the 14-th batch, train loss: 0.7055169343948364:   9%|█▏           | 14/151 [00:02<00:19,  7.08it/s]Epoch: 1, train for the 175-th batch, train loss: 0.5659826397895813:  74%|████████   | 175/237 [00:45<00:21,  2.84it/s]Epoch: 1, train for the 163-th batch, train loss: 0.39909476041793823:  42%|████▏     | 162/383 [00:43<01:03,  3.51it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5620933175086975:  51%|██████▋      | 75/146 [00:11<00:10,  6.50it/s]Epoch: 1, train for the 213-th batch, train loss: 0.5162597298622131:  88%|█████████▋ | 212/241 [00:45<00:06,  4.16it/s]Epoch: 1, train for the 163-th batch, train loss: 0.39909476041793823:  43%|████▎     | 163/383 [00:43<00:56,  3.91it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5620933175086975:  52%|██████▊      | 76/146 [00:11<00:10,  6.39it/s]Epoch: 1, train for the 213-th batch, train loss: 0.5162597298622131:  88%|█████████▋ | 213/241 [00:45<00:06,  4.65it/s]Epoch: 2, train for the 103-th batch, train loss: 0.4492452144622803:  86%|█████████▍ | 102/119 [00:16<00:02,  6.71it/s]Epoch: 2, train for the 15-th batch, train loss: 0.6929162740707397:   9%|█▏           | 14/151 [00:02<00:19,  7.08it/s]Epoch: 2, train for the 103-th batch, train loss: 0.4492452144622803:  87%|█████████▌ | 103/119 [00:16<00:02,  6.50it/s]Epoch: 2, train for the 15-th batch, train loss: 0.6929162740707397:  10%|█▎           | 15/151 [00:02<00:19,  6.90it/s]Epoch: 1, train for the 176-th batch, train loss: 0.5591989159584045:  74%|████████   | 175/237 [00:46<00:21,  2.84it/s]Epoch: 2, train for the 77-th batch, train loss: 0.5837154388427734:  52%|██████▊      | 76/146 [00:11<00:10,  6.39it/s]Epoch: 2, train for the 77-th batch, train loss: 0.5837154388427734:  53%|██████▊      | 77/146 [00:11<00:10,  6.35it/s]Epoch: 1, train for the 176-th batch, train loss: 0.5591989159584045:  74%|████████▏  | 176/237 [00:46<00:19,  3.18it/s]Epoch: 2, train for the 16-th batch, train loss: 0.6889596581459045:  10%|█▎           | 15/151 [00:02<00:19,  6.90it/s]Epoch: 2, train for the 16-th batch, train loss: 0.6889596581459045:  11%|█▍           | 16/151 [00:02<00:18,  7.36it/s]Epoch: 2, train for the 104-th batch, train loss: 0.42846259474754333:  87%|████████▋ | 103/119 [00:16<00:02,  6.50it/s]Epoch: 2, train for the 104-th batch, train loss: 0.42846259474754333:  87%|████████▋ | 104/119 [00:16<00:02,  6.39it/s]Epoch: 2, train for the 78-th batch, train loss: 0.5434872508049011:  53%|██████▊      | 77/146 [00:12<00:10,  6.35it/s]Epoch: 2, train for the 78-th batch, train loss: 0.5434872508049011:  53%|██████▉      | 78/146 [00:12<00:09,  6.84it/s]Epoch: 2, train for the 17-th batch, train loss: 0.69559246301651:  11%|█▌             | 16/151 [00:02<00:18,  7.36it/s]Epoch: 2, train for the 17-th batch, train loss: 0.69559246301651:  11%|█▋             | 17/151 [00:02<00:17,  7.58it/s]Epoch: 1, train for the 214-th batch, train loss: 0.48170456290245056:  88%|████████▊ | 213/241 [00:46<00:06,  4.65it/s]Epoch: 1, train for the 164-th batch, train loss: 0.39169761538505554:  43%|████▎     | 163/383 [00:44<00:56,  3.91it/s]Epoch: 1, train for the 214-th batch, train loss: 0.48170456290245056:  89%|████████▉ | 214/241 [00:46<00:07,  3.79it/s]Epoch: 1, train for the 177-th batch, train loss: 0.5743383765220642:  74%|████████▏  | 176/237 [00:46<00:19,  3.18it/s]Epoch: 2, train for the 105-th batch, train loss: 0.38790273666381836:  87%|████████▋ | 104/119 [00:16<00:02,  6.39it/s]Epoch: 2, train for the 105-th batch, train loss: 0.38790273666381836:  88%|████████▊ | 105/119 [00:17<00:02,  6.30it/s]Epoch: 1, train for the 164-th batch, train loss: 0.39169761538505554:  43%|████▎     | 164/383 [00:44<01:06,  3.29it/s]Epoch: 1, train for the 177-th batch, train loss: 0.5743383765220642:  75%|████████▏  | 177/237 [00:46<00:17,  3.40it/s]Epoch: 2, train for the 79-th batch, train loss: 0.5637499094009399:  53%|██████▉      | 78/146 [00:12<00:09,  6.84it/s]Epoch: 2, train for the 79-th batch, train loss: 0.5637499094009399:  54%|███████      | 79/146 [00:12<00:10,  6.63it/s]Epoch: 2, train for the 18-th batch, train loss: 0.6772419810295105:  11%|█▍           | 17/151 [00:02<00:17,  7.58it/s]Epoch: 2, train for the 18-th batch, train loss: 0.6772419810295105:  12%|█▌           | 18/151 [00:02<00:18,  7.29it/s]Epoch: 1, train for the 215-th batch, train loss: 0.5164788961410522:  89%|█████████▊ | 214/241 [00:46<00:07,  3.79it/s]Epoch: 1, train for the 215-th batch, train loss: 0.5164788961410522:  89%|█████████▊ | 215/241 [00:46<00:05,  4.34it/s]Epoch: 2, train for the 106-th batch, train loss: 0.4229358434677124:  88%|█████████▋ | 105/119 [00:17<00:02,  6.30it/s]Epoch: 2, train for the 106-th batch, train loss: 0.4229358434677124:  89%|█████████▊ | 106/119 [00:17<00:02,  6.12it/s]Epoch: 2, train for the 19-th batch, train loss: 0.6885242462158203:  12%|█▌           | 18/151 [00:02<00:18,  7.29it/s]Epoch: 2, train for the 19-th batch, train loss: 0.6885242462158203:  13%|█▋           | 19/151 [00:02<00:18,  7.20it/s]Epoch: 2, train for the 80-th batch, train loss: 0.544369101524353:  54%|███████▌      | 79/146 [00:12<00:10,  6.63it/s]Epoch: 2, train for the 80-th batch, train loss: 0.544369101524353:  55%|███████▋      | 80/146 [00:12<00:10,  6.18it/s]Epoch: 1, train for the 178-th batch, train loss: 0.5734893679618835:  75%|████████▏  | 177/237 [00:46<00:17,  3.40it/s]Epoch: 1, train for the 165-th batch, train loss: 0.3512057662010193:  43%|████▋      | 164/383 [00:44<01:06,  3.29it/s]Epoch: 1, train for the 178-th batch, train loss: 0.5734893679618835:  75%|████████▎  | 178/237 [00:46<00:16,  3.57it/s]Epoch: 1, train for the 216-th batch, train loss: 0.5924937129020691:  89%|█████████▊ | 215/241 [00:46<00:05,  4.34it/s]Epoch: 1, train for the 216-th batch, train loss: 0.5924937129020691:  90%|█████████▊ | 216/241 [00:46<00:05,  4.72it/s]Epoch: 2, train for the 107-th batch, train loss: 0.38971632719039917:  89%|████████▉ | 106/119 [00:17<00:02,  6.12it/s]Epoch: 1, train for the 165-th batch, train loss: 0.3512057662010193:  43%|████▋      | 165/383 [00:44<01:06,  3.30it/s]Epoch: 2, train for the 20-th batch, train loss: 0.671286940574646:  13%|█▊            | 19/151 [00:02<00:18,  7.20it/s]Epoch: 2, train for the 107-th batch, train loss: 0.38971632719039917:  90%|████████▉ | 107/119 [00:17<00:01,  6.23it/s]Epoch: 2, train for the 20-th batch, train loss: 0.671286940574646:  13%|█▊            | 20/151 [00:02<00:17,  7.36it/s]Epoch: 2, train for the 81-th batch, train loss: 0.5176535844802856:  55%|███████      | 80/146 [00:12<00:10,  6.18it/s]Epoch: 2, train for the 81-th batch, train loss: 0.5176535844802856:  55%|███████▏     | 81/146 [00:12<00:10,  6.25it/s]Epoch: 2, train for the 108-th batch, train loss: 0.32576608657836914:  90%|████████▉ | 107/119 [00:17<00:01,  6.23it/s]Epoch: 2, train for the 108-th batch, train loss: 0.32576608657836914:  91%|█████████ | 108/119 [00:17<00:01,  6.40it/s]Epoch: 2, train for the 82-th batch, train loss: 0.540226399898529:  55%|███████▊      | 81/146 [00:12<00:10,  6.25it/s]Epoch: 1, train for the 217-th batch, train loss: 0.510875940322876:  90%|██████████▊ | 216/241 [00:46<00:05,  4.72it/s]Epoch: 2, train for the 82-th batch, train loss: 0.540226399898529:  56%|███████▊      | 82/146 [00:12<00:09,  6.60it/s]Epoch: 1, train for the 217-th batch, train loss: 0.510875940322876:  90%|██████████▊ | 217/241 [00:46<00:05,  4.60it/s]Epoch: 1, train for the 179-th batch, train loss: 0.5500504374504089:  75%|████████▎  | 178/237 [00:46<00:16,  3.57it/s]Epoch: 1, train for the 166-th batch, train loss: 0.5459235906600952:  43%|████▋      | 165/383 [00:44<01:06,  3.30it/s]Epoch: 1, train for the 179-th batch, train loss: 0.5500504374504089:  76%|████████▎  | 179/237 [00:46<00:16,  3.44it/s]Epoch: 1, train for the 166-th batch, train loss: 0.5459235906600952:  43%|████▊      | 166/383 [00:44<01:03,  3.40it/s]Epoch: 2, train for the 21-th batch, train loss: 0.6681862473487854:  13%|█▋           | 20/151 [00:03<00:17,  7.36it/s]Epoch: 2, train for the 21-th batch, train loss: 0.6681862473487854:  14%|█▊           | 21/151 [00:03<00:22,  5.70it/s]Epoch: 2, train for the 109-th batch, train loss: 0.41381189227104187:  91%|█████████ | 108/119 [00:17<00:01,  6.40it/s]Epoch: 2, train for the 109-th batch, train loss: 0.41381189227104187:  92%|█████████▏| 109/119 [00:17<00:01,  6.19it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5297133922576904:  56%|███████▎     | 82/146 [00:12<00:09,  6.60it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5297133922576904:  57%|███████▍     | 83/146 [00:12<00:09,  6.63it/s]Epoch: 1, train for the 218-th batch, train loss: 0.4265208840370178:  90%|█████████▉ | 217/241 [00:46<00:05,  4.60it/s]Epoch: 2, train for the 22-th batch, train loss: 0.6317108273506165:  14%|█▊           | 21/151 [00:03<00:22,  5.70it/s]Epoch: 1, train for the 218-th batch, train loss: 0.4265208840370178:  90%|█████████▉ | 218/241 [00:46<00:04,  4.88it/s]Epoch: 2, train for the 22-th batch, train loss: 0.6317108273506165:  15%|█▉           | 22/151 [00:03<00:20,  6.35it/s]Epoch: 1, train for the 167-th batch, train loss: 0.45357954502105713:  43%|████▎     | 166/383 [00:44<01:03,  3.40it/s]Epoch: 1, train for the 167-th batch, train loss: 0.45357954502105713:  44%|████▎     | 167/383 [00:44<00:58,  3.71it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5163416266441345:  57%|███████▍     | 83/146 [00:12<00:09,  6.63it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5163416266441345:  58%|███████▍     | 84/146 [00:12<00:09,  6.43it/s]Epoch: 2, train for the 110-th batch, train loss: 0.4599681794643402:  92%|██████████ | 109/119 [00:17<00:01,  6.19it/s]Epoch: 2, train for the 23-th batch, train loss: 0.663592517375946:  15%|██            | 22/151 [00:03<00:20,  6.35it/s]Epoch: 2, train for the 110-th batch, train loss: 0.4599681794643402:  92%|██████████▏| 110/119 [00:17<00:01,  5.80it/s]Epoch: 2, train for the 23-th batch, train loss: 0.663592517375946:  15%|██▏           | 23/151 [00:03<00:19,  6.63it/s]Epoch: 2, train for the 24-th batch, train loss: 0.643274188041687:  15%|██▏           | 23/151 [00:03<00:19,  6.63it/s]Epoch: 2, train for the 85-th batch, train loss: 0.5022381544113159:  58%|███████▍     | 84/146 [00:13<00:09,  6.43it/s]Epoch: 2, train for the 24-th batch, train loss: 0.643274188041687:  16%|██▏           | 24/151 [00:03<00:18,  7.00it/s]Epoch: 2, train for the 85-th batch, train loss: 0.5022381544113159:  58%|███████▌     | 85/146 [00:13<00:09,  6.55it/s]Epoch: 1, train for the 180-th batch, train loss: 0.5946398973464966:  76%|████████▎  | 179/237 [00:47<00:16,  3.44it/s]Epoch: 2, train for the 111-th batch, train loss: 0.4517541527748108:  92%|██████████▏| 110/119 [00:18<00:01,  5.80it/s]Epoch: 2, train for the 111-th batch, train loss: 0.4517541527748108:  93%|██████████▎| 111/119 [00:18<00:01,  5.86it/s]Epoch: 1, train for the 180-th batch, train loss: 0.5946398973464966:  76%|████████▎  | 180/237 [00:47<00:19,  2.97it/s]Epoch: 1, train for the 168-th batch, train loss: 0.273630291223526:  44%|█████▏      | 167/383 [00:45<00:58,  3.71it/s]Epoch: 2, train for the 25-th batch, train loss: 0.704526960849762:  16%|██▏           | 24/151 [00:03<00:18,  7.00it/s]Epoch: 1, train for the 168-th batch, train loss: 0.273630291223526:  44%|█████▎      | 168/383 [00:45<01:01,  3.51it/s]Epoch: 1, train for the 219-th batch, train loss: 0.47155246138572693:  90%|█████████ | 218/241 [00:47<00:04,  4.88it/s]Epoch: 2, train for the 25-th batch, train loss: 0.704526960849762:  17%|██▎           | 25/151 [00:03<00:18,  6.93it/s]Epoch: 1, train for the 219-th batch, train loss: 0.47155246138572693:  91%|█████████ | 219/241 [00:47<00:06,  3.66it/s]Epoch: 2, train for the 112-th batch, train loss: 0.4116417467594147:  93%|██████████▎| 111/119 [00:18<00:01,  5.86it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5251909494400024:  58%|███████▌     | 85/146 [00:13<00:09,  6.55it/s]Epoch: 2, train for the 112-th batch, train loss: 0.4116417467594147:  94%|██████████▎| 112/119 [00:18<00:01,  5.66it/s]Epoch: 1, train for the 181-th batch, train loss: 0.5377323627471924:  76%|████████▎  | 180/237 [00:47<00:19,  2.97it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5251909494400024:  59%|███████▋     | 86/146 [00:13<00:10,  5.68it/s]Epoch: 2, train for the 26-th batch, train loss: 0.6452686786651611:  17%|██▏          | 25/151 [00:03<00:18,  6.93it/s]Epoch: 2, train for the 26-th batch, train loss: 0.6452686786651611:  17%|██▏          | 26/151 [00:03<00:17,  7.23it/s]Epoch: 1, train for the 181-th batch, train loss: 0.5377323627471924:  76%|████████▍  | 181/237 [00:47<00:17,  3.28it/s]Epoch: 1, train for the 169-th batch, train loss: 0.3550618588924408:  44%|████▊      | 168/383 [00:45<01:01,  3.51it/s]Epoch: 1, train for the 169-th batch, train loss: 0.3550618588924408:  44%|████▊      | 169/383 [00:45<00:57,  3.69it/s]Epoch: 2, train for the 113-th batch, train loss: 0.41307902336120605:  94%|█████████▍| 112/119 [00:18<00:01,  5.66it/s]Epoch: 2, train for the 113-th batch, train loss: 0.41307902336120605:  95%|█████████▍| 113/119 [00:18<00:01,  5.68it/s]Epoch: 2, train for the 27-th batch, train loss: 0.6601266264915466:  17%|██▏          | 26/151 [00:03<00:17,  7.23it/s]Epoch: 2, train for the 87-th batch, train loss: 0.5428722500801086:  59%|███████▋     | 86/146 [00:13<00:10,  5.68it/s]Epoch: 2, train for the 27-th batch, train loss: 0.6601266264915466:  18%|██▎          | 27/151 [00:03<00:17,  7.19it/s]Epoch: 2, train for the 87-th batch, train loss: 0.5428722500801086:  60%|███████▋     | 87/146 [00:13<00:10,  5.53it/s]Epoch: 1, train for the 220-th batch, train loss: 0.5754322409629822:  91%|█████████▉ | 219/241 [00:47<00:06,  3.66it/s]Epoch: 1, train for the 182-th batch, train loss: 0.5848249793052673:  76%|████████▍  | 181/237 [00:47<00:17,  3.28it/s]Epoch: 1, train for the 220-th batch, train loss: 0.5754322409629822:  91%|██████████ | 220/241 [00:47<00:05,  3.52it/s]Epoch: 1, train for the 182-th batch, train loss: 0.5848249793052673:  77%|████████▍  | 182/237 [00:47<00:15,  3.55it/s]Epoch: 2, train for the 28-th batch, train loss: 0.6629316806793213:  18%|██▎          | 27/151 [00:04<00:17,  7.19it/s]Epoch: 2, train for the 114-th batch, train loss: 0.3745768666267395:  95%|██████████▍| 113/119 [00:18<00:01,  5.68it/s]Epoch: 2, train for the 28-th batch, train loss: 0.6629316806793213:  19%|██▍          | 28/151 [00:04<00:17,  6.90it/s]Epoch: 2, train for the 114-th batch, train loss: 0.3745768666267395:  96%|██████████▌| 114/119 [00:18<00:00,  5.67it/s]Epoch: 2, train for the 88-th batch, train loss: 0.5688694715499878:  60%|███████▋     | 87/146 [00:13<00:10,  5.53it/s]Epoch: 2, train for the 88-th batch, train loss: 0.5688694715499878:  60%|███████▊     | 88/146 [00:13<00:10,  5.48it/s]Epoch: 1, train for the 221-th batch, train loss: 0.5273136496543884:  91%|██████████ | 220/241 [00:47<00:05,  3.52it/s]Epoch: 1, train for the 221-th batch, train loss: 0.5273136496543884:  92%|██████████ | 221/241 [00:47<00:04,  4.02it/s]Epoch: 2, train for the 29-th batch, train loss: 0.6772423982620239:  19%|██▍          | 28/151 [00:04<00:17,  6.90it/s]Epoch: 2, train for the 29-th batch, train loss: 0.6772423982620239:  19%|██▍          | 29/151 [00:04<00:17,  7.13it/s]Epoch: 1, train for the 170-th batch, train loss: 0.3825087249279022:  44%|████▊      | 169/383 [00:45<00:57,  3.69it/s]Epoch: 1, train for the 183-th batch, train loss: 0.5875442624092102:  77%|████████▍  | 182/237 [00:47<00:15,  3.55it/s]Epoch: 2, train for the 115-th batch, train loss: 0.35603803396224976:  96%|█████████▌| 114/119 [00:18<00:00,  5.67it/s]Epoch: 2, train for the 115-th batch, train loss: 0.35603803396224976:  97%|█████████▋| 115/119 [00:18<00:00,  5.70it/s]Epoch: 1, train for the 170-th batch, train loss: 0.3825087249279022:  44%|████▉      | 170/383 [00:45<01:04,  3.30it/s]Epoch: 1, train for the 183-th batch, train loss: 0.5875442624092102:  77%|████████▍  | 183/237 [00:48<00:14,  3.64it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5216313004493713:  60%|███████▊     | 88/146 [00:13<00:10,  5.48it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5216313004493713:  61%|███████▉     | 89/146 [00:13<00:10,  5.49it/s]Epoch: 1, train for the 222-th batch, train loss: 0.4995175302028656:  92%|██████████ | 221/241 [00:48<00:04,  4.02it/s]Epoch: 2, train for the 30-th batch, train loss: 0.6611819863319397:  19%|██▍          | 29/151 [00:04<00:17,  7.13it/s]Epoch: 2, train for the 30-th batch, train loss: 0.6611819863319397:  20%|██▌          | 30/151 [00:04<00:17,  7.03it/s]Epoch: 1, train for the 222-th batch, train loss: 0.4995175302028656:  92%|██████████▏| 222/241 [00:48<00:04,  4.22it/s]Epoch: 2, train for the 116-th batch, train loss: 0.3328093886375427:  97%|██████████▋| 115/119 [00:18<00:00,  5.70it/s]Epoch: 2, train for the 116-th batch, train loss: 0.3328093886375427:  97%|██████████▋| 116/119 [00:18<00:00,  6.02it/s]Epoch: 2, train for the 90-th batch, train loss: 0.5303256511688232:  61%|███████▉     | 89/146 [00:14<00:10,  5.49it/s]Epoch: 2, train for the 90-th batch, train loss: 0.5303256511688232:  62%|████████     | 90/146 [00:14<00:09,  5.67it/s]Epoch: 1, train for the 171-th batch, train loss: 0.5251238346099854:  44%|████▉      | 170/383 [00:46<01:04,  3.30it/s]Epoch: 1, train for the 171-th batch, train loss: 0.5251238346099854:  45%|████▉      | 171/383 [00:46<00:59,  3.56it/s]Epoch: 2, train for the 31-th batch, train loss: 0.6357387900352478:  20%|██▌          | 30/151 [00:04<00:17,  7.03it/s]Epoch: 2, train for the 31-th batch, train loss: 0.6357387900352478:  21%|██▋          | 31/151 [00:04<00:17,  6.79it/s]Epoch: 1, train for the 223-th batch, train loss: 0.4817478060722351:  92%|██████████▏| 222/241 [00:48<00:04,  4.22it/s]Epoch: 2, train for the 117-th batch, train loss: 0.36694762110710144:  97%|█████████▋| 116/119 [00:19<00:00,  6.02it/s]Epoch: 1, train for the 223-th batch, train loss: 0.4817478060722351:  93%|██████████▏| 223/241 [00:48<00:04,  4.47it/s]Epoch: 2, train for the 117-th batch, train loss: 0.36694762110710144:  98%|█████████▊| 117/119 [00:19<00:00,  6.09it/s]Epoch: 2, train for the 91-th batch, train loss: 0.5414672493934631:  62%|████████     | 90/146 [00:14<00:09,  5.67it/s]Epoch: 2, train for the 91-th batch, train loss: 0.5414672493934631:  62%|████████     | 91/146 [00:14<00:08,  6.15it/s]Epoch: 2, train for the 32-th batch, train loss: 0.6641415357589722:  21%|██▋          | 31/151 [00:04<00:17,  6.79it/s]Epoch: 2, train for the 32-th batch, train loss: 0.6641415357589722:  21%|██▊          | 32/151 [00:04<00:17,  6.69it/s]Epoch: 2, train for the 118-th batch, train loss: 0.3577699065208435:  98%|██████████▊| 117/119 [00:19<00:00,  6.09it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5411933660507202:  62%|████████     | 91/146 [00:14<00:08,  6.15it/s]Epoch: 2, train for the 118-th batch, train loss: 0.3577699065208435:  99%|██████████▉| 118/119 [00:19<00:00,  5.75it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5411933660507202:  63%|████████▏    | 92/146 [00:14<00:08,  6.04it/s]Epoch: 1, train for the 224-th batch, train loss: 0.49671027064323425:  93%|█████████▎| 223/241 [00:48<00:04,  4.47it/s]Epoch: 1, train for the 224-th batch, train loss: 0.49671027064323425:  93%|█████████▎| 224/241 [00:48<00:03,  4.43it/s]Epoch: 1, train for the 172-th batch, train loss: 0.3397970497608185:  45%|████▉      | 171/383 [00:46<00:59,  3.56it/s]Epoch: 1, train for the 184-th batch, train loss: 0.5609093904495239:  77%|████████▍  | 183/237 [00:48<00:14,  3.64it/s]Epoch: 2, train for the 33-th batch, train loss: 0.6535218954086304:  21%|██▊          | 32/151 [00:04<00:17,  6.69it/s]Epoch: 1, train for the 172-th batch, train loss: 0.3397970497608185:  45%|████▉      | 172/383 [00:46<01:02,  3.40it/s]Epoch: 2, train for the 33-th batch, train loss: 0.6535218954086304:  22%|██▊          | 33/151 [00:04<00:18,  6.52it/s]Epoch: 1, train for the 184-th batch, train loss: 0.5609093904495239:  78%|████████▌  | 184/237 [00:48<00:19,  2.76it/s]Epoch: 2, train for the 119-th batch, train loss: 0.32583045959472656:  99%|█████████▉| 118/119 [00:19<00:00,  5.75it/s]Epoch: 2, train for the 119-th batch, train loss: 0.32583045959472656: 100%|██████████| 119/119 [00:19<00:00,  6.38it/s]Epoch: 2, train for the 119-th batch, train loss: 0.32583045959472656: 100%|██████████| 119/119 [00:19<00:00,  6.15it/s]
Epoch: 2, train for the 93-th batch, train loss: 0.53839111328125:  63%|█████████▍     | 92/146 [00:14<00:08,  6.04it/s]Epoch: 1, train for the 225-th batch, train loss: 0.5315685272216797:  93%|██████████▏| 224/241 [00:48<00:03,  4.43it/s]Epoch: 2, train for the 93-th batch, train loss: 0.53839111328125:  64%|█████████▌     | 93/146 [00:14<00:08,  6.03it/s]Epoch: 1, train for the 225-th batch, train loss: 0.5315685272216797:  93%|██████████▎| 225/241 [00:48<00:03,  4.79it/s]Epoch: 2, train for the 34-th batch, train loss: 0.6451879739761353:  22%|██▊          | 33/151 [00:04<00:18,  6.52it/s]Epoch: 2, train for the 34-th batch, train loss: 0.6451879739761353:  23%|██▉          | 34/151 [00:04<00:17,  6.74it/s]  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 2, train for the 94-th batch, train loss: 0.5426343679428101:  64%|████████▎    | 93/146 [00:14<00:08,  6.03it/s]Epoch: 1, train for the 173-th batch, train loss: 0.4440193772315979:  45%|████▉      | 172/383 [00:46<01:02,  3.40it/s]Epoch: 2, train for the 94-th batch, train loss: 0.5426343679428101:  64%|████████▎    | 94/146 [00:14<00:08,  6.38it/s]Epoch: 1, train for the 185-th batch, train loss: 0.5481314659118652:  78%|████████▌  | 184/237 [00:48<00:19,  2.76it/s]Epoch: 2, train for the 35-th batch, train loss: 0.6545007824897766:  23%|██▉          | 34/151 [00:05<00:17,  6.74it/s]Epoch: 2, train for the 35-th batch, train loss: 0.6545007824897766:  23%|███          | 35/151 [00:05<00:16,  6.95it/s]evaluate for the 1-th batch, evaluate loss: 0.6070274710655212:   0%|                            | 0/40 [00:00<?, ?it/s]Epoch: 1, train for the 173-th batch, train loss: 0.4440193772315979:  45%|████▉      | 173/383 [00:46<01:01,  3.44it/s]Epoch: 1, train for the 185-th batch, train loss: 0.5481314659118652:  78%|████████▌  | 185/237 [00:48<00:17,  2.89it/s]evaluate for the 2-th batch, evaluate loss: 0.586901068687439:   0%|                             | 0/40 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.586901068687439:   5%|█                    | 2/40 [00:00<00:02, 13.89it/s]Epoch: 2, train for the 36-th batch, train loss: 0.6617374420166016:  23%|███          | 35/151 [00:05<00:16,  6.95it/s]Epoch: 2, train for the 36-th batch, train loss: 0.6617374420166016:  24%|███          | 36/151 [00:05<00:16,  7.16it/s]Epoch: 2, train for the 95-th batch, train loss: 0.5248357057571411:  64%|████████▎    | 94/146 [00:14<00:08,  6.38it/s]Epoch: 2, train for the 95-th batch, train loss: 0.5248357057571411:  65%|████████▍    | 95/146 [00:14<00:08,  6.08it/s]evaluate for the 3-th batch, evaluate loss: 0.6435075402259827:   5%|█                   | 2/40 [00:00<00:02, 13.89it/s]evaluate for the 4-th batch, evaluate loss: 0.7144618034362793:   5%|█                   | 2/40 [00:00<00:02, 13.89it/s]evaluate for the 4-th batch, evaluate loss: 0.7144618034362793:  10%|██                  | 4/40 [00:00<00:02, 14.97it/s]Epoch: 1, train for the 174-th batch, train loss: 0.5656508803367615:  45%|████▉      | 173/383 [00:46<01:01,  3.44it/s]Epoch: 1, train for the 226-th batch, train loss: 0.5555323362350464:  93%|██████████▎| 225/241 [00:49<00:03,  4.79it/s]Epoch: 1, train for the 226-th batch, train loss: 0.5555323362350464:  94%|██████████▎| 226/241 [00:49<00:03,  3.76it/s]evaluate for the 5-th batch, evaluate loss: 0.709732174873352:  10%|██                   | 4/40 [00:00<00:02, 14.97it/s]Epoch: 1, train for the 174-th batch, train loss: 0.5656508803367615:  45%|████▉      | 174/383 [00:46<00:58,  3.55it/s]Epoch: 1, train for the 186-th batch, train loss: 0.5505822896957397:  78%|████████▌  | 185/237 [00:49<00:17,  2.89it/s]Epoch: 2, train for the 37-th batch, train loss: 0.6689750552177429:  24%|███          | 36/151 [00:05<00:16,  7.16it/s]Epoch: 2, train for the 37-th batch, train loss: 0.6689750552177429:  25%|███▏         | 37/151 [00:05<00:16,  6.87it/s]Epoch: 1, train for the 186-th batch, train loss: 0.5505822896957397:  78%|████████▋  | 186/237 [00:49<00:16,  3.11it/s]evaluate for the 6-th batch, evaluate loss: 0.6382173299789429:  10%|██                  | 4/40 [00:00<00:02, 14.97it/s]Epoch: 1, train for the 227-th batch, train loss: 0.5443744659423828:  94%|██████████▎| 226/241 [00:49<00:03,  3.76it/s]Epoch: 2, train for the 96-th batch, train loss: 0.5687201023101807:  65%|████████▍    | 95/146 [00:15<00:08,  6.08it/s]evaluate for the 7-th batch, evaluate loss: 0.6936037540435791:  10%|██                  | 4/40 [00:00<00:02, 14.97it/s]Epoch: 1, train for the 227-th batch, train loss: 0.5443744659423828:  94%|██████████▎| 227/241 [00:49<00:03,  4.31it/s]evaluate for the 7-th batch, evaluate loss: 0.6936037540435791:  18%|███▌                | 7/40 [00:00<00:02, 15.69it/s]Epoch: 2, train for the 96-th batch, train loss: 0.5687201023101807:  66%|████████▌    | 96/146 [00:15<00:09,  5.17it/s]Epoch: 1, train for the 175-th batch, train loss: 0.43153226375579834:  45%|████▌     | 174/383 [00:47<00:58,  3.55it/s]Epoch: 2, train for the 38-th batch, train loss: 0.6439131498336792:  25%|███▏         | 37/151 [00:05<00:16,  6.87it/s]evaluate for the 8-th batch, evaluate loss: 0.64632648229599:  18%|███▊                  | 7/40 [00:00<00:02, 15.69it/s]Epoch: 2, train for the 38-th batch, train loss: 0.6439131498336792:  25%|███▎         | 38/151 [00:05<00:18,  6.27it/s]Epoch: 1, train for the 175-th batch, train loss: 0.43153226375579834:  46%|████▌     | 175/383 [00:47<00:54,  3.81it/s]Epoch: 2, train for the 97-th batch, train loss: 0.5091068148612976:  66%|████████▌    | 96/146 [00:15<00:09,  5.17it/s]evaluate for the 9-th batch, evaluate loss: 0.6736595630645752:  18%|███▌                | 7/40 [00:00<00:02, 15.69it/s]evaluate for the 9-th batch, evaluate loss: 0.6736595630645752:  22%|████▌               | 9/40 [00:00<00:02, 13.84it/s]Epoch: 2, train for the 97-th batch, train loss: 0.5091068148612976:  66%|████████▋    | 97/146 [00:15<00:09,  5.28it/s]Epoch: 2, train for the 39-th batch, train loss: 0.6159047484397888:  25%|███▎         | 38/151 [00:05<00:18,  6.27it/s]Epoch: 2, train for the 39-th batch, train loss: 0.6159047484397888:  26%|███▎         | 39/151 [00:05<00:16,  6.64it/s]evaluate for the 10-th batch, evaluate loss: 0.7412587404251099:  22%|████▎              | 9/40 [00:00<00:02, 13.84it/s]Epoch: 1, train for the 187-th batch, train loss: 0.5826559662818909:  78%|████████▋  | 186/237 [00:49<00:16,  3.11it/s]Epoch: 1, train for the 187-th batch, train loss: 0.5826559662818909:  79%|████████▋  | 187/237 [00:49<00:17,  2.93it/s]Epoch: 2, train for the 40-th batch, train loss: 0.6222261190414429:  26%|███▎         | 39/151 [00:05<00:16,  6.64it/s]Epoch: 2, train for the 40-th batch, train loss: 0.6222261190414429:  26%|███▍         | 40/151 [00:05<00:15,  7.20it/s]evaluate for the 11-th batch, evaluate loss: 0.6058847904205322:  22%|████▎              | 9/40 [00:00<00:02, 13.84it/s]evaluate for the 11-th batch, evaluate loss: 0.6058847904205322:  28%|████▉             | 11/40 [00:00<00:02, 13.22it/s]Epoch: 2, train for the 98-th batch, train loss: 0.4971291720867157:  66%|████████▋    | 97/146 [00:15<00:09,  5.28it/s]Epoch: 2, train for the 98-th batch, train loss: 0.4971291720867157:  67%|████████▋    | 98/146 [00:15<00:08,  5.47it/s]Epoch: 1, train for the 176-th batch, train loss: 0.5032753348350525:  46%|█████      | 175/383 [00:47<00:54,  3.81it/s]Epoch: 1, train for the 228-th batch, train loss: 0.5200479030609131:  94%|██████████▎| 227/241 [00:49<00:03,  4.31it/s]evaluate for the 12-th batch, evaluate loss: 0.611530065536499:  28%|█████▏             | 11/40 [00:00<00:02, 13.22it/s]Epoch: 1, train for the 228-th batch, train loss: 0.5200479030609131:  95%|██████████▍| 228/241 [00:49<00:03,  3.46it/s]Epoch: 1, train for the 176-th batch, train loss: 0.5032753348350525:  46%|█████      | 176/383 [00:47<00:59,  3.48it/s]Epoch: 1, train for the 188-th batch, train loss: 0.6203756332397461:  79%|████████▋  | 187/237 [00:49<00:17,  2.93it/s]Epoch: 2, train for the 41-th batch, train loss: 0.6368676424026489:  26%|███▍         | 40/151 [00:05<00:15,  7.20it/s]Epoch: 2, train for the 99-th batch, train loss: 0.5639795064926147:  67%|████████▋    | 98/146 [00:15<00:08,  5.47it/s]Epoch: 2, train for the 41-th batch, train loss: 0.6368676424026489:  27%|███▌         | 41/151 [00:05<00:16,  6.75it/s]evaluate for the 13-th batch, evaluate loss: 0.6439628601074219:  28%|████▉             | 11/40 [00:00<00:02, 13.22it/s]evaluate for the 13-th batch, evaluate loss: 0.6439628601074219:  32%|█████▊            | 13/40 [00:00<00:02, 13.25it/s]Epoch: 1, train for the 188-th batch, train loss: 0.6203756332397461:  79%|████████▋  | 188/237 [00:49<00:14,  3.35it/s]Epoch: 2, train for the 99-th batch, train loss: 0.5639795064926147:  68%|████████▊    | 99/146 [00:15<00:08,  5.84it/s]Epoch: 1, train for the 229-th batch, train loss: 0.48976802825927734:  95%|█████████▍| 228/241 [00:49<00:03,  3.46it/s]evaluate for the 14-th batch, evaluate loss: 0.6233477592468262:  32%|█████▊            | 13/40 [00:01<00:02, 13.25it/s]Epoch: 1, train for the 229-th batch, train loss: 0.48976802825927734:  95%|█████████▌| 229/241 [00:49<00:03,  3.92it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5971659421920776:  27%|███▌         | 41/151 [00:06<00:16,  6.75it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5971659421920776:  28%|███▌         | 42/151 [00:06<00:17,  6.26it/s]Epoch: 1, train for the 189-th batch, train loss: 0.5906498432159424:  79%|████████▋  | 188/237 [00:49<00:14,  3.35it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5688361525535583:  68%|████████▏   | 99/146 [00:15<00:08,  5.84it/s]evaluate for the 15-th batch, evaluate loss: 0.6396937370300293:  32%|█████▊            | 13/40 [00:01<00:02, 13.25it/s]evaluate for the 15-th batch, evaluate loss: 0.6396937370300293:  38%|██████▊           | 15/40 [00:01<00:02, 12.24it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5688361525535583:  68%|███████▌   | 100/146 [00:15<00:08,  5.60it/s]Epoch: 1, train for the 189-th batch, train loss: 0.5906498432159424:  80%|████████▊  | 189/237 [00:49<00:13,  3.64it/s]evaluate for the 16-th batch, evaluate loss: 0.7222208976745605:  38%|██████▊           | 15/40 [00:01<00:02, 12.24it/s]Epoch: 2, train for the 43-th batch, train loss: 0.642956554889679:  28%|███▉          | 42/151 [00:06<00:17,  6.26it/s]Epoch: 2, train for the 43-th batch, train loss: 0.642956554889679:  28%|███▉          | 43/151 [00:06<00:16,  6.62it/s]evaluate for the 17-th batch, evaluate loss: 0.6799725890159607:  38%|██████▊           | 15/40 [00:01<00:02, 12.24it/s]evaluate for the 17-th batch, evaluate loss: 0.6799725890159607:  42%|███████▋          | 17/40 [00:01<00:01, 12.61it/s]Epoch: 2, train for the 101-th batch, train loss: 0.5130454301834106:  68%|███████▌   | 100/146 [00:15<00:08,  5.60it/s]Epoch: 2, train for the 101-th batch, train loss: 0.5130454301834106:  69%|███████▌   | 101/146 [00:15<00:07,  5.90it/s]evaluate for the 18-th batch, evaluate loss: 0.6315654516220093:  42%|███████▋          | 17/40 [00:01<00:01, 12.61it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5700180530548096:  28%|███▋         | 43/151 [00:06<00:16,  6.62it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5700180530548096:  29%|███▊         | 44/151 [00:06<00:14,  7.16it/s]Epoch: 1, train for the 177-th batch, train loss: 0.44313910603523254:  46%|████▌     | 176/383 [00:48<00:59,  3.48it/s]Epoch: 1, train for the 230-th batch, train loss: 0.46319708228111267:  95%|█████████▌| 229/241 [00:50<00:03,  3.92it/s]Epoch: 2, train for the 102-th batch, train loss: 0.5418900847434998:  69%|███████▌   | 101/146 [00:16<00:07,  5.90it/s]Epoch: 1, train for the 190-th batch, train loss: 0.5472182631492615:  80%|████████▊  | 189/237 [00:50<00:13,  3.64it/s]evaluate for the 19-th batch, evaluate loss: 0.7331153750419617:  42%|███████▋          | 17/40 [00:01<00:01, 12.61it/s]evaluate for the 19-th batch, evaluate loss: 0.7331153750419617:  48%|████████▌         | 19/40 [00:01<00:01, 12.76it/s]Epoch: 2, train for the 102-th batch, train loss: 0.5418900847434998:  70%|███████▋   | 102/146 [00:16<00:07,  6.12it/s]Epoch: 1, train for the 230-th batch, train loss: 0.46319708228111267:  95%|█████████▌| 230/241 [00:50<00:03,  3.37it/s]Epoch: 1, train for the 177-th batch, train loss: 0.44313910603523254:  46%|████▌     | 177/383 [00:48<01:17,  2.66it/s]Epoch: 1, train for the 190-th batch, train loss: 0.5472182631492615:  80%|████████▊  | 190/237 [00:50<00:13,  3.49it/s]evaluate for the 20-th batch, evaluate loss: 0.7023344039916992:  48%|████████▌         | 19/40 [00:01<00:01, 12.76it/s]Epoch: 2, train for the 45-th batch, train loss: 0.5764642357826233:  29%|███▊         | 44/151 [00:06<00:14,  7.16it/s]Epoch: 2, train for the 45-th batch, train loss: 0.5764642357826233:  30%|███▊         | 45/151 [00:06<00:14,  7.12it/s]Epoch: 2, train for the 103-th batch, train loss: 0.5545905232429504:  70%|███████▋   | 102/146 [00:16<00:07,  6.12it/s]evaluate for the 21-th batch, evaluate loss: 0.5754117369651794:  48%|████████▌         | 19/40 [00:01<00:01, 12.76it/s]evaluate for the 21-th batch, evaluate loss: 0.5754117369651794:  52%|█████████▍        | 21/40 [00:01<00:01, 13.10it/s]Epoch: 2, train for the 103-th batch, train loss: 0.5545905232429504:  71%|███████▊   | 103/146 [00:16<00:06,  6.42it/s]Epoch: 1, train for the 231-th batch, train loss: 0.5075476169586182:  95%|██████████▍| 230/241 [00:50<00:03,  3.37it/s]Epoch: 1, train for the 231-th batch, train loss: 0.5075476169586182:  96%|██████████▌| 231/241 [00:50<00:02,  3.83it/s]evaluate for the 22-th batch, evaluate loss: 0.619170069694519:  52%|█████████▉         | 21/40 [00:01<00:01, 13.10it/s]Epoch: 2, train for the 46-th batch, train loss: 0.5943295955657959:  30%|███▊         | 45/151 [00:06<00:14,  7.12it/s]Epoch: 2, train for the 46-th batch, train loss: 0.5943295955657959:  30%|███▉         | 46/151 [00:06<00:15,  6.94it/s]evaluate for the 23-th batch, evaluate loss: 0.5777955651283264:  52%|█████████▍        | 21/40 [00:01<00:01, 13.10it/s]evaluate for the 23-th batch, evaluate loss: 0.5777955651283264:  57%|██████████▎       | 23/40 [00:01<00:01, 12.97it/s]Epoch: 2, train for the 104-th batch, train loss: 0.5303277373313904:  71%|███████▊   | 103/146 [00:16<00:06,  6.42it/s]Epoch: 2, train for the 104-th batch, train loss: 0.5303277373313904:  71%|███████▊   | 104/146 [00:16<00:06,  6.27it/s]Epoch: 1, train for the 178-th batch, train loss: 0.5873441696166992:  46%|█████      | 177/383 [00:48<01:17,  2.66it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5972240567207336:  30%|███▉         | 46/151 [00:06<00:15,  6.94it/s]Epoch: 1, train for the 178-th batch, train loss: 0.5873441696166992:  46%|█████      | 178/383 [00:48<01:13,  2.77it/s]Epoch: 1, train for the 191-th batch, train loss: 0.5506816506385803:  80%|████████▊  | 190/237 [00:50<00:13,  3.49it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5972240567207336:  31%|████         | 47/151 [00:06<00:14,  7.20it/s]evaluate for the 24-th batch, evaluate loss: 0.6775224208831787:  57%|██████████▎       | 23/40 [00:01<00:01, 12.97it/s]Epoch: 1, train for the 191-th batch, train loss: 0.5506816506385803:  81%|████████▊  | 191/237 [00:50<00:14,  3.26it/s]Epoch: 2, train for the 105-th batch, train loss: 0.5504500269889832:  71%|███████▊   | 104/146 [00:16<00:06,  6.27it/s]Epoch: 2, train for the 105-th batch, train loss: 0.5504500269889832:  72%|███████▉   | 105/146 [00:16<00:05,  6.83it/s]Epoch: 1, train for the 232-th batch, train loss: 0.4724389612674713:  96%|██████████▌| 231/241 [00:50<00:02,  3.83it/s]Epoch: 1, train for the 232-th batch, train loss: 0.4724389612674713:  96%|██████████▌| 232/241 [00:50<00:02,  3.67it/s]Epoch: 2, train for the 48-th batch, train loss: 0.5516662001609802:  31%|████         | 47/151 [00:06<00:14,  7.20it/s]Epoch: 2, train for the 48-th batch, train loss: 0.5516662001609802:  32%|████▏        | 48/151 [00:06<00:14,  6.93it/s]Epoch: 2, train for the 106-th batch, train loss: 0.5187342166900635:  72%|███████▉   | 105/146 [00:16<00:05,  6.83it/s]Epoch: 1, train for the 179-th batch, train loss: 0.3910175859928131:  46%|█████      | 178/383 [00:48<01:13,  2.77it/s]Epoch: 1, train for the 192-th batch, train loss: 0.5717108845710754:  81%|████████▊  | 191/237 [00:50<00:14,  3.26it/s]Epoch: 1, train for the 179-th batch, train loss: 0.3910175859928131:  47%|█████▏     | 179/383 [00:48<01:06,  3.08it/s]evaluate for the 25-th batch, evaluate loss: 0.6598699688911438:  57%|██████████▎       | 23/40 [00:02<00:01, 12.97it/s]evaluate for the 25-th batch, evaluate loss: 0.6598699688911438:  62%|███████████▎      | 25/40 [00:02<00:01, 10.18it/s]Epoch: 1, train for the 192-th batch, train loss: 0.5717108845710754:  81%|████████▉  | 192/237 [00:50<00:12,  3.56it/s]Epoch: 1, train for the 233-th batch, train loss: 0.4590315818786621:  96%|██████████▌| 232/241 [00:50<00:02,  3.67it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5804595351219177:  72%|███████▉   | 105/146 [00:16<00:05,  6.83it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5804595351219177:  73%|████████   | 107/146 [00:16<00:04,  7.84it/s]Epoch: 1, train for the 233-th batch, train loss: 0.4590315818786621:  97%|██████████▋| 233/241 [00:50<00:01,  4.18it/s]evaluate for the 26-th batch, evaluate loss: 0.6497820615768433:  62%|███████████▎      | 25/40 [00:02<00:01, 10.18it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5709977746009827:  32%|████▏        | 48/151 [00:07<00:14,  6.93it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5709977746009827:  32%|████▏        | 49/151 [00:07<00:16,  6.23it/s]evaluate for the 27-th batch, evaluate loss: 0.6713575720787048:  62%|███████████▎      | 25/40 [00:02<00:01, 10.18it/s]evaluate for the 27-th batch, evaluate loss: 0.6713575720787048:  68%|████████████▏     | 27/40 [00:02<00:01, 10.38it/s]Epoch: 1, train for the 180-th batch, train loss: 0.5199456810951233:  47%|█████▏     | 179/383 [00:48<01:06,  3.08it/s]Epoch: 2, train for the 108-th batch, train loss: 0.539616584777832:  73%|████████▊   | 107/146 [00:16<00:04,  7.84it/s]Epoch: 2, train for the 108-th batch, train loss: 0.539616584777832:  74%|████████▉   | 108/146 [00:16<00:05,  7.15it/s]Epoch: 1, train for the 234-th batch, train loss: 0.5390600562095642:  97%|██████████▋| 233/241 [00:51<00:01,  4.18it/s]Epoch: 1, train for the 193-th batch, train loss: 0.5661105513572693:  81%|████████▉  | 192/237 [00:51<00:12,  3.56it/s]Epoch: 1, train for the 180-th batch, train loss: 0.5199456810951233:  47%|█████▏     | 180/383 [00:48<01:01,  3.32it/s]Epoch: 1, train for the 234-th batch, train loss: 0.5390600562095642:  97%|██████████▋| 234/241 [00:51<00:01,  4.43it/s]evaluate for the 28-th batch, evaluate loss: 0.6118066310882568:  68%|████████████▏     | 27/40 [00:02<00:01, 10.38it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5568482875823975:  32%|████▏        | 49/151 [00:07<00:16,  6.23it/s]Epoch: 1, train for the 193-th batch, train loss: 0.5661105513572693:  81%|████████▉  | 193/237 [00:51<00:11,  3.69it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5568482875823975:  33%|████▎        | 50/151 [00:07<00:16,  6.19it/s]Epoch: 2, train for the 109-th batch, train loss: 0.5584346055984497:  74%|████████▏  | 108/146 [00:17<00:05,  7.15it/s]Epoch: 2, train for the 109-th batch, train loss: 0.5584346055984497:  75%|████████▏  | 109/146 [00:17<00:05,  7.26it/s]Epoch: 1, train for the 235-th batch, train loss: 0.5622661709785461:  97%|██████████▋| 234/241 [00:51<00:01,  4.43it/s]evaluate for the 29-th batch, evaluate loss: 0.6816030144691467:  68%|████████████▏     | 27/40 [00:02<00:01, 10.38it/s]evaluate for the 29-th batch, evaluate loss: 0.6816030144691467:  72%|█████████████     | 29/40 [00:02<00:01,  9.13it/s]Epoch: 1, train for the 235-th batch, train loss: 0.5622661709785461:  98%|██████████▋| 235/241 [00:51<00:01,  4.44it/s]Epoch: 2, train for the 110-th batch, train loss: 0.4994305372238159:  75%|████████▏  | 109/146 [00:17<00:05,  7.26it/s]Epoch: 2, train for the 110-th batch, train loss: 0.4994305372238159:  75%|████████▎  | 110/146 [00:17<00:05,  6.99it/s]evaluate for the 30-th batch, evaluate loss: 0.6255747675895691:  72%|█████████████     | 29/40 [00:02<00:01,  9.13it/s]Epoch: 2, train for the 51-th batch, train loss: 0.6098744869232178:  33%|████▎        | 50/151 [00:07<00:16,  6.19it/s]Epoch: 2, train for the 51-th batch, train loss: 0.6098744869232178:  34%|████▍        | 51/151 [00:07<00:19,  5.15it/s]Epoch: 1, train for the 181-th batch, train loss: 0.40156105160713196:  47%|████▋     | 180/383 [00:49<01:01,  3.32it/s]evaluate for the 31-th batch, evaluate loss: 0.6564919352531433:  72%|█████████████     | 29/40 [00:02<00:01,  9.13it/s]evaluate for the 31-th batch, evaluate loss: 0.6564919352531433:  78%|█████████████▉    | 31/40 [00:02<00:00, 10.62it/s]Epoch: 1, train for the 236-th batch, train loss: 0.4917198419570923:  98%|██████████▋| 235/241 [00:51<00:01,  4.44it/s]Epoch: 1, train for the 181-th batch, train loss: 0.40156105160713196:  47%|████▋     | 181/383 [00:49<01:05,  3.08it/s]Epoch: 1, train for the 236-th batch, train loss: 0.4917198419570923:  98%|██████████▊| 236/241 [00:51<00:01,  4.77it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5180657505989075:  75%|████████▎  | 110/146 [00:17<00:05,  6.99it/s]evaluate for the 32-th batch, evaluate loss: 0.6347735524177551:  78%|█████████████▉    | 31/40 [00:02<00:00, 10.62it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5180657505989075:  76%|████████▎  | 111/146 [00:17<00:05,  7.00it/s]Epoch: 1, train for the 194-th batch, train loss: 0.5431857705116272:  81%|████████▉  | 193/237 [00:51<00:11,  3.69it/s]Epoch: 2, train for the 52-th batch, train loss: 0.6259793639183044:  34%|████▍        | 51/151 [00:07<00:19,  5.15it/s]Epoch: 2, train for the 52-th batch, train loss: 0.6259793639183044:  34%|████▍        | 52/151 [00:07<00:19,  5.14it/s]evaluate for the 33-th batch, evaluate loss: 0.6143717169761658:  78%|█████████████▉    | 31/40 [00:02<00:00, 10.62it/s]evaluate for the 33-th batch, evaluate loss: 0.6143717169761658:  82%|██████████████▊   | 33/40 [00:02<00:00, 10.72it/s]Epoch: 1, train for the 194-th batch, train loss: 0.5431857705116272:  82%|█████████  | 194/237 [00:51<00:14,  2.95it/s]Epoch: 2, train for the 112-th batch, train loss: 0.5615692734718323:  76%|████████▎  | 111/146 [00:17<00:05,  7.00it/s]Epoch: 2, train for the 112-th batch, train loss: 0.5615692734718323:  77%|████████▍  | 112/146 [00:17<00:05,  6.76it/s]evaluate for the 34-th batch, evaluate loss: 0.644765317440033:  82%|███████████████▋   | 33/40 [00:02<00:00, 10.72it/s]Epoch: 1, train for the 182-th batch, train loss: 0.5140037536621094:  47%|█████▏     | 181/383 [00:49<01:05,  3.08it/s]Epoch: 2, train for the 53-th batch, train loss: 0.6128097176551819:  34%|████▍        | 52/151 [00:07<00:19,  5.14it/s]Epoch: 1, train for the 182-th batch, train loss: 0.5140037536621094:  48%|█████▏     | 182/383 [00:49<01:02,  3.21it/s]Epoch: 1, train for the 237-th batch, train loss: 0.477655291557312:  98%|███████████▊| 236/241 [00:51<00:01,  4.77it/s]Epoch: 2, train for the 53-th batch, train loss: 0.6128097176551819:  35%|████▌        | 53/151 [00:07<00:17,  5.44it/s]Epoch: 2, train for the 113-th batch, train loss: 0.4807801842689514:  77%|████████▍  | 112/146 [00:17<00:05,  6.76it/s]Epoch: 2, train for the 113-th batch, train loss: 0.4807801842689514:  77%|████████▌  | 113/146 [00:17<00:04,  7.42it/s]Epoch: 1, train for the 237-th batch, train loss: 0.477655291557312:  98%|███████████▊| 237/241 [00:51<00:00,  4.32it/s]Epoch: 2, train for the 114-th batch, train loss: 0.5299190282821655:  77%|████████▌  | 113/146 [00:17<00:04,  7.42it/s]Epoch: 2, train for the 114-th batch, train loss: 0.5299190282821655:  78%|████████▌  | 114/146 [00:17<00:04,  7.66it/s]Epoch: 2, train for the 54-th batch, train loss: 0.5713468790054321:  35%|████▌        | 53/151 [00:08<00:17,  5.44it/s]Epoch: 2, train for the 54-th batch, train loss: 0.5713468790054321:  36%|████▋        | 54/151 [00:08<00:17,  5.66it/s]Epoch: 1, train for the 183-th batch, train loss: 0.448208212852478:  48%|█████▋      | 182/383 [00:49<01:02,  3.21it/s]Epoch: 1, train for the 238-th batch, train loss: 0.4676847755908966:  98%|██████████▊| 237/241 [00:51<00:00,  4.32it/s]Epoch: 1, train for the 183-th batch, train loss: 0.448208212852478:  48%|█████▋      | 183/383 [00:49<00:55,  3.61it/s]Epoch: 1, train for the 238-th batch, train loss: 0.4676847755908966:  99%|██████████▊| 238/241 [00:51<00:00,  4.48it/s]evaluate for the 35-th batch, evaluate loss: 0.7040753960609436:  82%|██████████████▊   | 33/40 [00:03<00:00, 10.72it/s]evaluate for the 35-th batch, evaluate loss: 0.7040753960609436:  88%|███████████████▊  | 35/40 [00:03<00:00,  8.26it/s]Epoch: 2, train for the 115-th batch, train loss: 0.5528478026390076:  78%|████████▌  | 114/146 [00:17<00:04,  7.66it/s]Epoch: 2, train for the 115-th batch, train loss: 0.5528478026390076:  79%|████████▋  | 115/146 [00:17<00:04,  7.59it/s]evaluate for the 36-th batch, evaluate loss: 0.631207287311554:  88%|████████████████▋  | 35/40 [00:03<00:00,  8.26it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5413920879364014:  36%|████▋        | 54/151 [00:08<00:17,  5.66it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5413920879364014:  36%|████▋        | 55/151 [00:08<00:16,  5.71it/s]evaluate for the 37-th batch, evaluate loss: 0.6277212500572205:  88%|███████████████▊  | 35/40 [00:03<00:00,  8.26it/s]evaluate for the 37-th batch, evaluate loss: 0.6277212500572205:  92%|████████████████▋ | 37/40 [00:03<00:00,  9.42it/s]Epoch: 1, train for the 239-th batch, train loss: 0.5452038645744324:  99%|██████████▊| 238/241 [00:52<00:00,  4.48it/s]Epoch: 2, train for the 116-th batch, train loss: 0.5241661667823792:  79%|████████▋  | 115/146 [00:18<00:04,  7.59it/s]Epoch: 1, train for the 239-th batch, train loss: 0.5452038645744324:  99%|██████████▉| 239/241 [00:52<00:00,  4.69it/s]Epoch: 2, train for the 116-th batch, train loss: 0.5241661667823792:  79%|████████▋  | 116/146 [00:18<00:04,  7.17it/s]evaluate for the 38-th batch, evaluate loss: 0.6402682065963745:  92%|████████████████▋ | 37/40 [00:03<00:00,  9.42it/s]Epoch: 2, train for the 56-th batch, train loss: 0.495168000459671:  36%|█████         | 55/151 [00:08<00:16,  5.71it/s]Epoch: 1, train for the 195-th batch, train loss: 0.5425001382827759:  82%|█████████  | 194/237 [00:52<00:14,  2.95it/s]Epoch: 1, train for the 184-th batch, train loss: 0.4562283456325531:  48%|█████▎     | 183/383 [00:50<00:55,  3.61it/s]Epoch: 2, train for the 56-th batch, train loss: 0.495168000459671:  37%|█████▏        | 56/151 [00:08<00:16,  5.92it/s]Epoch: 1, train for the 195-th batch, train loss: 0.5425001382827759:  82%|█████████  | 195/237 [00:52<00:18,  2.31it/s]Epoch: 1, train for the 184-th batch, train loss: 0.4562283456325531:  48%|█████▎     | 184/383 [00:50<00:57,  3.43it/s]Epoch: 2, train for the 117-th batch, train loss: 0.5655893683433533:  79%|████████▋  | 116/146 [00:18<00:04,  7.17it/s]Epoch: 2, train for the 117-th batch, train loss: 0.5655893683433533:  80%|████████▊  | 117/146 [00:18<00:03,  7.54it/s]Epoch: 1, train for the 240-th batch, train loss: 0.4347590208053589:  99%|██████████▉| 239/241 [00:52<00:00,  4.69it/s]Epoch: 1, train for the 240-th batch, train loss: 0.4347590208053589: 100%|██████████▉| 240/241 [00:52<00:00,  5.00it/s]Epoch: 2, train for the 57-th batch, train loss: 0.512978196144104:  37%|█████▏        | 56/151 [00:08<00:16,  5.92it/s]Epoch: 2, train for the 57-th batch, train loss: 0.512978196144104:  38%|█████▎        | 57/151 [00:08<00:15,  6.14it/s]Epoch: 2, train for the 118-th batch, train loss: 0.5570356845855713:  80%|████████▊  | 117/146 [00:18<00:03,  7.54it/s]evaluate for the 39-th batch, evaluate loss: 0.6520966291427612:  92%|████████████████▋ | 37/40 [00:03<00:00,  9.42it/s]evaluate for the 39-th batch, evaluate loss: 0.6520966291427612:  98%|█████████████████▌| 39/40 [00:03<00:00,  8.28it/s]Epoch: 2, train for the 118-th batch, train loss: 0.5570356845855713:  81%|████████▉  | 118/146 [00:18<00:03,  7.34it/s]Epoch: 1, train for the 185-th batch, train loss: 0.394325315952301:  48%|█████▊      | 184/383 [00:50<00:57,  3.43it/s]evaluate for the 40-th batch, evaluate loss: 0.529293417930603:  98%|██████████████████▌| 39/40 [00:03<00:00,  8.28it/s]evaluate for the 40-th batch, evaluate loss: 0.529293417930603: 100%|███████████████████| 40/40 [00:03<00:00, 10.88it/s]
Epoch: 1, train for the 185-th batch, train loss: 0.394325315952301:  48%|█████▊      | 185/383 [00:50<00:54,  3.65it/s]Epoch: 1, train for the 241-th batch, train loss: 0.451946884393692: 100%|███████████▉| 240/241 [00:52<00:00,  5.00it/s]Epoch: 2, train for the 58-th batch, train loss: 0.538672924041748:  38%|█████▎        | 57/151 [00:08<00:15,  6.14it/s]Epoch: 1, train for the 241-th batch, train loss: 0.451946884393692: 100%|████████████| 241/241 [00:52<00:00,  4.98it/s]Epoch: 1, train for the 241-th batch, train loss: 0.451946884393692: 100%|████████████| 241/241 [00:52<00:00,  4.59it/s]
Epoch: 2, train for the 58-th batch, train loss: 0.538672924041748:  38%|█████▍        | 58/151 [00:08<00:14,  6.21it/s]Epoch: 1, train for the 196-th batch, train loss: 0.5478695631027222:  82%|█████████  | 195/237 [00:52<00:18,  2.31it/s]Epoch: 1, train for the 196-th batch, train loss: 0.5478695631027222:  83%|█████████  | 196/237 [00:52<00:16,  2.47it/s]Epoch: 2, train for the 119-th batch, train loss: 0.5261548757553101:  81%|████████▉  | 118/146 [00:18<00:03,  7.34it/s]  0%|                                                                                            | 0/21 [00:00<?, ?it/s]Epoch: 2, train for the 119-th batch, train loss: 0.5261548757553101:  82%|████████▉  | 119/146 [00:18<00:04,  6.61it/s]evaluate for the 1-th batch, evaluate loss: 0.9421315789222717:   0%|                            | 0/21 [00:00<?, ?it/s]Epoch: 2, train for the 59-th batch, train loss: 0.5254541039466858:  38%|████▉        | 58/151 [00:08<00:14,  6.21it/s]Epoch: 2, train for the 59-th batch, train loss: 0.5254541039466858:  39%|█████        | 59/151 [00:08<00:15,  5.99it/s]Epoch: 1, train for the 186-th batch, train loss: 0.40400686860084534:  48%|████▊     | 185/383 [00:50<00:54,  3.65it/s]Epoch: 1, train for the 197-th batch, train loss: 0.5336034297943115:  83%|█████████  | 196/237 [00:52<00:16,  2.47it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5398938059806824:  82%|████████▉  | 119/146 [00:18<00:04,  6.61it/s]Epoch: 1, train for the 186-th batch, train loss: 0.40400686860084534:  49%|████▊     | 186/383 [00:50<00:54,  3.64it/s]evaluate for the 2-th batch, evaluate loss: 1.038635015487671:   0%|                             | 0/21 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 1.038635015487671:  10%|██                   | 2/21 [00:00<00:01, 11.76it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5398938059806824:  82%|█████████  | 120/146 [00:18<00:04,  6.44it/s]Epoch: 1, train for the 197-th batch, train loss: 0.5336034297943115:  83%|█████████▏ | 197/237 [00:52<00:13,  2.93it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5180865526199341:  39%|█████        | 59/151 [00:09<00:15,  5.99it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5180865526199341:  40%|█████▏       | 60/151 [00:09<00:13,  6.50it/s]evaluate for the 3-th batch, evaluate loss: 1.0326945781707764:  10%|█▉                  | 2/21 [00:00<00:01, 11.76it/s]Epoch: 2, train for the 121-th batch, train loss: 0.5745492577552795:  82%|█████████  | 120/146 [00:18<00:04,  6.44it/s]Epoch: 2, train for the 121-th batch, train loss: 0.5745492577552795:  83%|█████████  | 121/146 [00:18<00:03,  6.65it/s]evaluate for the 4-th batch, evaluate loss: 0.9250649809837341:  10%|█▉                  | 2/21 [00:00<00:01, 11.76it/s]evaluate for the 4-th batch, evaluate loss: 0.9250649809837341:  19%|███▊                | 4/21 [00:00<00:01, 12.42it/s]Epoch: 1, train for the 187-th batch, train loss: 0.31474414467811584:  49%|████▊     | 186/383 [00:50<00:54,  3.64it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5838096737861633:  40%|█████▏       | 60/151 [00:09<00:13,  6.50it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5838096737861633:  40%|█████▎       | 61/151 [00:09<00:13,  6.60it/s]evaluate for the 5-th batch, evaluate loss: 0.9604278206825256:  19%|███▊                | 4/21 [00:00<00:01, 12.42it/s]Epoch: 1, train for the 187-th batch, train loss: 0.31474414467811584:  49%|████▉     | 187/383 [00:50<00:51,  3.83it/s]Epoch: 2, train for the 122-th batch, train loss: 0.5429764986038208:  83%|█████████  | 121/146 [00:18<00:03,  6.65it/s]Epoch: 2, train for the 122-th batch, train loss: 0.5429764986038208:  84%|█████████▏ | 122/146 [00:18<00:03,  6.83it/s]evaluate for the 6-th batch, evaluate loss: 0.9394112825393677:  19%|███▊                | 4/21 [00:00<00:01, 12.42it/s]evaluate for the 6-th batch, evaluate loss: 0.9394112825393677:  29%|█████▋              | 6/21 [00:00<00:01, 13.81it/s]  0%|                                                                                            | 0/72 [00:00<?, ?it/s]Epoch: 1, train for the 198-th batch, train loss: 0.5251867771148682:  83%|█████████▏ | 197/237 [00:53<00:13,  2.93it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5641046166419983:  40%|█████▎       | 61/151 [00:09<00:13,  6.60it/s]evaluate for the 7-th batch, evaluate loss: 0.8399091362953186:  29%|█████▋              | 6/21 [00:00<00:01, 13.81it/s]evaluate for the 1-th batch, evaluate loss: 0.5386770367622375:   0%|                            | 0/72 [00:00<?, ?it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5641046166419983:  41%|█████▎       | 62/151 [00:09<00:13,  6.44it/s]Epoch: 1, train for the 198-th batch, train loss: 0.5251867771148682:  84%|█████████▏ | 198/237 [00:53<00:13,  2.87it/s]Epoch: 2, train for the 123-th batch, train loss: 0.5700206756591797:  84%|█████████▏ | 122/146 [00:19<00:03,  6.83it/s]evaluate for the 2-th batch, evaluate loss: 0.6074497699737549:   0%|                            | 0/72 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.6074497699737549:   3%|▌                   | 2/72 [00:00<00:05, 11.71it/s]evaluate for the 8-th batch, evaluate loss: 0.8754004240036011:  29%|█████▋              | 6/21 [00:00<00:01, 13.81it/s]evaluate for the 8-th batch, evaluate loss: 0.8754004240036011:  38%|███████▌            | 8/21 [00:00<00:01, 12.59it/s]Epoch: 2, train for the 123-th batch, train loss: 0.5700206756591797:  84%|█████████▎ | 123/146 [00:19<00:03,  6.31it/s]Epoch: 1, train for the 188-th batch, train loss: 0.4456271231174469:  49%|█████▎     | 187/383 [00:51<00:51,  3.83it/s]Epoch: 1, train for the 188-th batch, train loss: 0.4456271231174469:  49%|█████▍     | 188/383 [00:51<00:53,  3.67it/s]Epoch: 2, train for the 63-th batch, train loss: 0.5851583480834961:  41%|█████▎       | 62/151 [00:09<00:13,  6.44it/s]evaluate for the 9-th batch, evaluate loss: 0.8064389824867249:  38%|███████▌            | 8/21 [00:00<00:01, 12.59it/s]evaluate for the 3-th batch, evaluate loss: 0.5931703448295593:   3%|▌                   | 2/72 [00:00<00:05, 11.71it/s]Epoch: 2, train for the 63-th batch, train loss: 0.5851583480834961:  42%|█████▍       | 63/151 [00:09<00:14,  6.18it/s]Epoch: 1, train for the 199-th batch, train loss: 0.5402082800865173:  84%|█████████▏ | 198/237 [00:53<00:13,  2.87it/s]Epoch: 1, train for the 199-th batch, train loss: 0.5402082800865173:  84%|█████████▏ | 199/237 [00:53<00:12,  3.14it/s]evaluate for the 10-th batch, evaluate loss: 0.8190314173698425:  38%|███████▏           | 8/21 [00:00<00:01, 12.59it/s]evaluate for the 10-th batch, evaluate loss: 0.8190314173698425:  48%|████████▌         | 10/21 [00:00<00:00, 11.99it/s]Epoch: 2, train for the 124-th batch, train loss: 0.5741113424301147:  84%|█████████▎ | 123/146 [00:19<00:03,  6.31it/s]Epoch: 2, train for the 124-th batch, train loss: 0.5741113424301147:  85%|█████████▎ | 124/146 [00:19<00:03,  5.84it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5400487780570984:  42%|█████▍       | 63/151 [00:09<00:14,  6.18it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5400487780570984:  42%|█████▌       | 64/151 [00:09<00:13,  6.38it/s]evaluate for the 11-th batch, evaluate loss: 0.7419711947441101:  48%|████████▌         | 10/21 [00:00<00:00, 11.99it/s]Epoch: 1, train for the 189-th batch, train loss: 0.440794438123703:  49%|█████▉      | 188/383 [00:51<00:53,  3.67it/s]Epoch: 1, train for the 189-th batch, train loss: 0.440794438123703:  49%|█████▉      | 189/383 [00:51<00:51,  3.75it/s]evaluate for the 12-th batch, evaluate loss: 0.790323793888092:  48%|█████████          | 10/21 [00:00<00:00, 11.99it/s]evaluate for the 12-th batch, evaluate loss: 0.790323793888092:  57%|██████████▊        | 12/21 [00:00<00:00, 12.50it/s]evaluate for the 4-th batch, evaluate loss: 0.6201490163803101:   3%|▌                   | 2/72 [00:00<00:05, 11.71it/s]evaluate for the 4-th batch, evaluate loss: 0.6201490163803101:   6%|█                   | 4/72 [00:00<00:09,  7.15it/s]Epoch: 2, train for the 125-th batch, train loss: 0.5397664308547974:  85%|█████████▎ | 124/146 [00:19<00:03,  5.84it/s]Epoch: 2, train for the 125-th batch, train loss: 0.5397664308547974:  86%|█████████▍ | 125/146 [00:19<00:03,  5.82it/s]Epoch: 2, train for the 65-th batch, train loss: 0.4915676414966583:  42%|█████▌       | 64/151 [00:09<00:13,  6.38it/s]evaluate for the 13-th batch, evaluate loss: 0.7397897243499756:  57%|██████████▎       | 12/21 [00:01<00:00, 12.50it/s]Epoch: 2, train for the 65-th batch, train loss: 0.4915676414966583:  43%|█████▌       | 65/151 [00:09<00:13,  6.21it/s]evaluate for the 5-th batch, evaluate loss: 0.5867181420326233:   6%|█                   | 4/72 [00:00<00:09,  7.15it/s]evaluate for the 6-th batch, evaluate loss: 0.6355029344558716:   6%|█                   | 4/72 [00:00<00:09,  7.15it/s]evaluate for the 6-th batch, evaluate loss: 0.6355029344558716:   8%|█▋                  | 6/72 [00:00<00:07,  9.09it/s]evaluate for the 14-th batch, evaluate loss: 0.6961767673492432:  57%|██████████▎       | 12/21 [00:01<00:00, 12.50it/s]evaluate for the 14-th batch, evaluate loss: 0.6961767673492432:  67%|████████████      | 14/21 [00:01<00:00, 11.98it/s]Epoch: 1, train for the 190-th batch, train loss: 0.5373391509056091:  49%|█████▍     | 189/383 [00:51<00:51,  3.75it/s]Epoch: 2, train for the 126-th batch, train loss: 0.5658183693885803:  86%|█████████▍ | 125/146 [00:19<00:03,  5.82it/s]Epoch: 2, train for the 126-th batch, train loss: 0.5658183693885803:  86%|█████████▍ | 126/146 [00:19<00:03,  5.83it/s]Epoch: 2, train for the 66-th batch, train loss: 0.5280080437660217:  43%|█████▌       | 65/151 [00:10<00:13,  6.21it/s]evaluate for the 7-th batch, evaluate loss: 0.664068877696991:   8%|█▊                   | 6/72 [00:00<00:07,  9.09it/s]Epoch: 1, train for the 190-th batch, train loss: 0.5373391509056091:  50%|█████▍     | 190/383 [00:51<00:51,  3.78it/s]Epoch: 2, train for the 66-th batch, train loss: 0.5280080437660217:  44%|█████▋       | 66/151 [00:10<00:13,  6.10it/s]evaluate for the 15-th batch, evaluate loss: 0.7252088785171509:  67%|████████████      | 14/21 [00:01<00:00, 11.98it/s]Epoch: 1, train for the 200-th batch, train loss: 0.5584352016448975:  84%|█████████▏ | 199/237 [00:53<00:12,  3.14it/s]Epoch: 1, train for the 200-th batch, train loss: 0.5584352016448975:  84%|█████████▎ | 200/237 [00:53<00:13,  2.80it/s]evaluate for the 8-th batch, evaluate loss: 0.6122426986694336:   8%|█▋                  | 6/72 [00:00<00:07,  9.09it/s]evaluate for the 8-th batch, evaluate loss: 0.6122426986694336:  11%|██▏                 | 8/72 [00:00<00:06, 10.18it/s]Epoch: 2, train for the 127-th batch, train loss: 0.5268191695213318:  86%|█████████▍ | 126/146 [00:19<00:03,  5.83it/s]Epoch: 2, train for the 127-th batch, train loss: 0.5268191695213318:  87%|█████████▌ | 127/146 [00:19<00:03,  6.29it/s]Epoch: 2, train for the 67-th batch, train loss: 0.5551937818527222:  44%|█████▋       | 66/151 [00:10<00:13,  6.10it/s]evaluate for the 9-th batch, evaluate loss: 0.6639094352722168:  11%|██▏                 | 8/72 [00:00<00:06, 10.18it/s]Epoch: 2, train for the 67-th batch, train loss: 0.5551937818527222:  44%|█████▊       | 67/151 [00:10<00:14,  5.99it/s]Epoch: 1, train for the 191-th batch, train loss: 0.42504915595054626:  50%|████▉     | 190/383 [00:51<00:51,  3.78it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5564281344413757:  87%|█████████▌ | 127/146 [00:19<00:03,  6.29it/s]Epoch: 1, train for the 191-th batch, train loss: 0.42504915595054626:  50%|████▉     | 191/383 [00:51<00:49,  3.86it/s]evaluate for the 16-th batch, evaluate loss: 0.6761409044265747:  67%|████████████      | 14/21 [00:01<00:00, 11.98it/s]evaluate for the 16-th batch, evaluate loss: 0.6761409044265747:  76%|█████████████▋    | 16/21 [00:01<00:00,  9.36it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5564281344413757:  88%|█████████▋ | 128/146 [00:19<00:02,  6.41it/s]Epoch: 2, train for the 68-th batch, train loss: 0.5310405492782593:  44%|█████▊       | 67/151 [00:10<00:14,  5.99it/s]Epoch: 2, train for the 68-th batch, train loss: 0.5310405492782593:  45%|█████▊       | 68/151 [00:10<00:13,  6.34it/s]evaluate for the 17-th batch, evaluate loss: 0.6104564666748047:  76%|█████████████▋    | 16/21 [00:01<00:00,  9.36it/s]Epoch: 2, train for the 129-th batch, train loss: 0.5503039956092834:  88%|█████████▋ | 128/146 [00:20<00:02,  6.41it/s]Epoch: 2, train for the 129-th batch, train loss: 0.5503039956092834:  88%|█████████▋ | 129/146 [00:20<00:02,  6.42it/s]evaluate for the 18-th batch, evaluate loss: 0.6574963927268982:  76%|█████████████▋    | 16/21 [00:01<00:00,  9.36it/s]Epoch: 1, train for the 201-th batch, train loss: 0.5522128343582153:  84%|█████████▎ | 200/237 [00:54<00:13,  2.80it/s]evaluate for the 18-th batch, evaluate loss: 0.6574963927268982:  86%|███████████████▍  | 18/21 [00:01<00:00, 10.16it/s]Epoch: 1, train for the 192-th batch, train loss: 0.48325395584106445:  50%|████▉     | 191/383 [00:52<00:49,  3.86it/s]Epoch: 1, train for the 201-th batch, train loss: 0.5522128343582153:  85%|█████████▎ | 201/237 [00:54<00:13,  2.70it/s]evaluate for the 10-th batch, evaluate loss: 0.5800685286521912:  11%|██                 | 8/72 [00:01<00:06, 10.18it/s]evaluate for the 10-th batch, evaluate loss: 0.5800685286521912:  14%|██▌               | 10/72 [00:01<00:08,  7.61it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5098832845687866:  45%|█████▊       | 68/151 [00:10<00:13,  6.34it/s]Epoch: 1, train for the 192-th batch, train loss: 0.48325395584106445:  50%|█████     | 192/383 [00:52<00:47,  4.02it/s]evaluate for the 19-th batch, evaluate loss: 0.5796236395835876:  86%|███████████████▍  | 18/21 [00:01<00:00, 10.16it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5098832845687866:  46%|█████▉       | 69/151 [00:10<00:13,  6.12it/s]Epoch: 2, train for the 130-th batch, train loss: 0.49236562848091125:  88%|████████▊ | 129/146 [00:20<00:02,  6.42it/s]evaluate for the 11-th batch, evaluate loss: 0.5689784288406372:  14%|██▌               | 10/72 [00:01<00:08,  7.61it/s]Epoch: 2, train for the 130-th batch, train loss: 0.49236562848091125:  89%|████████▉ | 130/146 [00:20<00:02,  6.44it/s]evaluate for the 20-th batch, evaluate loss: 0.5422671437263489:  86%|███████████████▍  | 18/21 [00:01<00:00, 10.16it/s]evaluate for the 20-th batch, evaluate loss: 0.5422671437263489:  95%|█████████████████▏| 20/21 [00:01<00:00, 10.78it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5439472198486328:  46%|█████▉       | 69/151 [00:10<00:13,  6.12it/s]Epoch: 1, train for the 202-th batch, train loss: 0.5620051026344299:  85%|█████████▎ | 201/237 [00:54<00:13,  2.70it/s]evaluate for the 21-th batch, evaluate loss: 0.6867806315422058:  95%|█████████████████▏| 20/21 [00:01<00:00, 10.78it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5439472198486328:  46%|██████       | 70/151 [00:10<00:13,  6.22it/s]evaluate for the 21-th batch, evaluate loss: 0.6867806315422058: 100%|██████████████████| 21/21 [00:01<00:00, 11.37it/s]
Epoch: 1, train for the 202-th batch, train loss: 0.5620051026344299:  85%|█████████▍ | 202/237 [00:54<00:11,  3.04it/s]Epoch: 1, train for the 193-th batch, train loss: 0.5560764670372009:  50%|█████▌     | 192/383 [00:52<00:47,  4.02it/s]Epoch: 1, train for the 193-th batch, train loss: 0.5560764670372009:  50%|█████▌     | 193/383 [00:52<00:48,  3.92it/s]evaluate for the 12-th batch, evaluate loss: 0.6483234763145447:  14%|██▌               | 10/72 [00:01<00:08,  7.61it/s]evaluate for the 12-th batch, evaluate loss: 0.6483234763145447:  17%|███               | 12/72 [00:01<00:08,  7.14it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5318784713745117:  89%|█████████▊ | 130/146 [00:20<00:02,  6.44it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5318784713745117:  90%|█████████▊ | 131/146 [00:20<00:02,  5.47it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5775352716445923:  46%|██████       | 70/151 [00:10<00:13,  6.22it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5775352716445923:  47%|██████       | 71/151 [00:10<00:13,  5.79it/s]evaluate for the 13-th batch, evaluate loss: 0.6116816401481628:  17%|███               | 12/72 [00:01<00:08,  7.14it/s]INFO:root:Epoch: 2, learning rate: 0.0001, train loss: 0.4499
INFO:root:train average_precision, 0.8780
INFO:root:train roc_auc, 0.8723
INFO:root:validate loss: 0.6483
INFO:root:validate average_precision, 0.7644
INFO:root:validate roc_auc, 0.7738
INFO:root:new node validate loss: 0.7917
INFO:root:new node validate first_1_average_precision, 0.8082
INFO:root:new node validate first_1_roc_auc, 0.8303
INFO:root:new node validate first_3_average_precision, 0.7679
INFO:root:new node validate first_3_roc_auc, 0.7637
INFO:root:new node validate first_10_average_precision, 0.7041
INFO:root:new node validate first_10_roc_auc, 0.6990
INFO:root:new node validate average_precision, 0.6535
INFO:root:new node validate roc_auc, 0.6647
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]evaluate for the 14-th batch, evaluate loss: 0.6155431866645813:  17%|███               | 12/72 [00:01<00:08,  7.14it/s]evaluate for the 14-th batch, evaluate loss: 0.6155431866645813:  19%|███▌              | 14/72 [00:01<00:06,  8.32it/s]Epoch: 2, train for the 132-th batch, train loss: 0.5109536051750183:  90%|█████████▊ | 131/146 [00:20<00:02,  5.47it/s]Epoch: 2, train for the 132-th batch, train loss: 0.5109536051750183:  90%|█████████▉ | 132/146 [00:20<00:02,  5.61it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5248138904571533:  47%|██████       | 71/151 [00:11<00:13,  5.79it/s]evaluate for the 15-th batch, evaluate loss: 0.5936754941940308:  19%|███▌              | 14/72 [00:01<00:06,  8.32it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5248138904571533:  48%|██████▏      | 72/151 [00:11<00:13,  5.84it/s]Epoch: 3, train for the 1-th batch, train loss: 1.093563199043274:   0%|                        | 0/119 [00:00<?, ?it/s]Epoch: 3, train for the 1-th batch, train loss: 1.093563199043274:   1%|▏               | 1/119 [00:00<00:16,  7.28it/s]Epoch: 1, train for the 203-th batch, train loss: 0.5384398102760315:  85%|█████████▍ | 202/237 [00:54<00:11,  3.04it/s]Epoch: 1, train for the 194-th batch, train loss: 0.4362470209598541:  50%|█████▌     | 193/383 [00:52<00:48,  3.92it/s]evaluate for the 16-th batch, evaluate loss: 0.6968855857849121:  19%|███▌              | 14/72 [00:01<00:06,  8.32it/s]evaluate for the 16-th batch, evaluate loss: 0.6968855857849121:  22%|████              | 16/72 [00:01<00:05,  9.83it/s]Epoch: 1, train for the 203-th batch, train loss: 0.5384398102760315:  86%|█████████▍ | 203/237 [00:54<00:12,  2.79it/s]Epoch: 1, train for the 194-th batch, train loss: 0.4362470209598541:  51%|█████▌     | 194/383 [00:52<00:54,  3.47it/s]Epoch: 2, train for the 133-th batch, train loss: 0.5400273203849792:  90%|█████████▉ | 132/146 [00:20<00:02,  5.61it/s]Epoch: 2, train for the 133-th batch, train loss: 0.5400273203849792:  91%|██████████ | 133/146 [00:20<00:02,  5.94it/s]evaluate for the 17-th batch, evaluate loss: 0.604686439037323:  22%|████▏              | 16/72 [00:01<00:05,  9.83it/s]Epoch: 2, train for the 73-th batch, train loss: 0.5254582762718201:  48%|██████▏      | 72/151 [00:11<00:13,  5.84it/s]Epoch: 2, train for the 73-th batch, train loss: 0.5254582762718201:  48%|██████▎      | 73/151 [00:11<00:13,  5.87it/s]evaluate for the 18-th batch, evaluate loss: 0.5590481162071228:  22%|████              | 16/72 [00:01<00:05,  9.83it/s]Epoch: 3, train for the 2-th batch, train loss: 1.219093918800354:   1%|▏               | 1/119 [00:00<00:16,  7.28it/s]evaluate for the 18-th batch, evaluate loss: 0.5590481162071228:  25%|████▌             | 18/72 [00:01<00:04, 11.16it/s]Epoch: 3, train for the 2-th batch, train loss: 1.219093918800354:   2%|▎               | 2/119 [00:00<00:17,  6.82it/s]Epoch: 2, train for the 134-th batch, train loss: 0.5100610852241516:  91%|██████████ | 133/146 [00:20<00:02,  5.94it/s]Epoch: 2, train for the 134-th batch, train loss: 0.5100610852241516:  92%|██████████ | 134/146 [00:20<00:01,  6.31it/s]evaluate for the 19-th batch, evaluate loss: 0.6115023493766785:  25%|████▌             | 18/72 [00:02<00:04, 11.16it/s]Epoch: 1, train for the 204-th batch, train loss: 0.5379798412322998:  86%|█████████▍ | 203/237 [00:55<00:12,  2.79it/s]Epoch: 1, train for the 204-th batch, train loss: 0.5379798412322998:  86%|█████████▍ | 204/237 [00:55<00:10,  3.18it/s]Epoch: 1, train for the 195-th batch, train loss: 0.4815557301044464:  51%|█████▌     | 194/383 [00:52<00:54,  3.47it/s]Epoch: 3, train for the 3-th batch, train loss: 0.9242196679115295:   2%|▎              | 2/119 [00:00<00:17,  6.82it/s]Epoch: 2, train for the 74-th batch, train loss: 0.4807392656803131:  48%|██████▎      | 73/151 [00:11<00:13,  5.87it/s]Epoch: 3, train for the 3-th batch, train loss: 0.9242196679115295:   3%|▍              | 3/119 [00:00<00:17,  6.72it/s]Epoch: 2, train for the 74-th batch, train loss: 0.4807392656803131:  49%|██████▎      | 74/151 [00:11<00:13,  5.77it/s]Epoch: 1, train for the 195-th batch, train loss: 0.4815557301044464:  51%|█████▌     | 195/383 [00:52<00:53,  3.54it/s]Epoch: 2, train for the 135-th batch, train loss: 0.5279242992401123:  92%|██████████ | 134/146 [00:21<00:01,  6.31it/s]Epoch: 2, train for the 135-th batch, train loss: 0.5279242992401123:  92%|██████████▏| 135/146 [00:21<00:01,  6.22it/s]Epoch: 3, train for the 4-th batch, train loss: 0.7395873069763184:   3%|▍              | 3/119 [00:00<00:17,  6.72it/s]Epoch: 3, train for the 4-th batch, train loss: 0.7395873069763184:   3%|▌              | 4/119 [00:00<00:16,  6.80it/s]Epoch: 2, train for the 75-th batch, train loss: 0.5233728885650635:  49%|██████▎      | 74/151 [00:11<00:13,  5.77it/s]Epoch: 2, train for the 75-th batch, train loss: 0.5233728885650635:  50%|██████▍      | 75/151 [00:11<00:12,  5.96it/s]Epoch: 2, train for the 136-th batch, train loss: 0.5244016051292419:  92%|██████████▏| 135/146 [00:21<00:01,  6.22it/s]evaluate for the 20-th batch, evaluate loss: 0.5772819519042969:  25%|████▌             | 18/72 [00:02<00:04, 11.16it/s]evaluate for the 20-th batch, evaluate loss: 0.5772819519042969:  28%|█████             | 20/72 [00:02<00:06,  8.13it/s]Epoch: 2, train for the 136-th batch, train loss: 0.5244016051292419:  93%|██████████▏| 136/146 [00:21<00:01,  6.14it/s]Epoch: 3, train for the 5-th batch, train loss: 0.5277277827262878:   3%|▌              | 4/119 [00:00<00:16,  6.80it/s]Epoch: 3, train for the 5-th batch, train loss: 0.5277277827262878:   4%|▋              | 5/119 [00:00<00:16,  7.02it/s]Epoch: 1, train for the 196-th batch, train loss: 0.5253227353096008:  51%|█████▌     | 195/383 [00:53<00:53,  3.54it/s]Epoch: 1, train for the 205-th batch, train loss: 0.5295016765594482:  86%|█████████▍ | 204/237 [00:55<00:10,  3.18it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5492400527000427:  50%|██████▍      | 75/151 [00:11<00:12,  5.96it/s]evaluate for the 21-th batch, evaluate loss: 0.6766902208328247:  28%|█████             | 20/72 [00:02<00:06,  8.13it/s]Epoch: 1, train for the 205-th batch, train loss: 0.5295016765594482:  86%|█████████▌ | 205/237 [00:55<00:10,  3.03it/s]Epoch: 1, train for the 196-th batch, train loss: 0.5253227353096008:  51%|█████▋     | 196/383 [00:53<00:53,  3.46it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5492400527000427:  50%|██████▌      | 76/151 [00:11<00:12,  5.96it/s]Epoch: 2, train for the 137-th batch, train loss: 0.4959900677204132:  93%|██████████▏| 136/146 [00:21<00:01,  6.14it/s]evaluate for the 22-th batch, evaluate loss: 0.5436679124832153:  28%|█████             | 20/72 [00:02<00:06,  8.13it/s]evaluate for the 22-th batch, evaluate loss: 0.5436679124832153:  31%|█████▌            | 22/72 [00:02<00:05,  9.13it/s]Epoch: 2, train for the 137-th batch, train loss: 0.4959900677204132:  94%|██████████▎| 137/146 [00:21<00:01,  6.24it/s]Epoch: 3, train for the 6-th batch, train loss: 0.4321064352989197:   4%|▋              | 5/119 [00:00<00:16,  7.02it/s]Epoch: 3, train for the 6-th batch, train loss: 0.4321064352989197:   5%|▊              | 6/119 [00:00<00:17,  6.63it/s]evaluate for the 23-th batch, evaluate loss: 0.5785256624221802:  31%|█████▌            | 22/72 [00:02<00:05,  9.13it/s]Epoch: 2, train for the 77-th batch, train loss: 0.5132879614830017:  50%|██████▌      | 76/151 [00:11<00:12,  5.96it/s]Epoch: 2, train for the 77-th batch, train loss: 0.5132879614830017:  51%|██████▋      | 77/151 [00:11<00:12,  5.70it/s]Epoch: 1, train for the 206-th batch, train loss: 0.5591462850570679:  86%|█████████▌ | 205/237 [00:55<00:10,  3.03it/s]Epoch: 2, train for the 138-th batch, train loss: 0.5243077874183655:  94%|██████████▎| 137/146 [00:21<00:01,  6.24it/s]Epoch: 2, train for the 138-th batch, train loss: 0.5243077874183655:  95%|██████████▍| 138/146 [00:21<00:01,  6.45it/s]Epoch: 1, train for the 197-th batch, train loss: 0.5713831782341003:  51%|█████▋     | 196/383 [00:53<00:53,  3.46it/s]Epoch: 1, train for the 206-th batch, train loss: 0.5591462850570679:  87%|█████████▌ | 206/237 [00:55<00:09,  3.30it/s]Epoch: 3, train for the 7-th batch, train loss: 0.46298947930336:   5%|▊                | 6/119 [00:01<00:17,  6.63it/s]Epoch: 3, train for the 7-th batch, train loss: 0.46298947930336:   6%|█                | 7/119 [00:01<00:16,  6.66it/s]Epoch: 1, train for the 197-th batch, train loss: 0.5713831782341003:  51%|█████▋     | 197/383 [00:53<00:52,  3.51it/s]Epoch: 2, train for the 78-th batch, train loss: 0.498269259929657:  51%|███████▏      | 77/151 [00:11<00:12,  5.70it/s]Epoch: 2, train for the 78-th batch, train loss: 0.498269259929657:  52%|███████▏      | 78/151 [00:12<00:11,  6.26it/s]Epoch: 2, train for the 139-th batch, train loss: 0.5542188286781311:  95%|██████████▍| 138/146 [00:21<00:01,  6.45it/s]Epoch: 2, train for the 139-th batch, train loss: 0.5542188286781311:  95%|██████████▍| 139/146 [00:21<00:01,  6.67it/s]Epoch: 3, train for the 8-th batch, train loss: 0.4862063229084015:   6%|▉              | 7/119 [00:01<00:16,  6.66it/s]Epoch: 3, train for the 8-th batch, train loss: 0.4862063229084015:   7%|█              | 8/119 [00:01<00:16,  6.62it/s]Epoch: 2, train for the 79-th batch, train loss: 0.5795876383781433:  52%|██████▋      | 78/151 [00:12<00:11,  6.26it/s]Epoch: 2, train for the 79-th batch, train loss: 0.5795876383781433:  52%|██████▊      | 79/151 [00:12<00:10,  6.75it/s]Epoch: 2, train for the 140-th batch, train loss: 0.5069106817245483:  95%|██████████▍| 139/146 [00:21<00:01,  6.67it/s]Epoch: 1, train for the 207-th batch, train loss: 0.5458102226257324:  87%|█████████▌ | 206/237 [00:55<00:09,  3.30it/s]evaluate for the 24-th batch, evaluate loss: 0.6086426377296448:  31%|█████▌            | 22/72 [00:02<00:05,  9.13it/s]evaluate for the 24-th batch, evaluate loss: 0.6086426377296448:  33%|██████            | 24/72 [00:02<00:06,  7.08it/s]Epoch: 2, train for the 140-th batch, train loss: 0.5069106817245483:  96%|██████████▌| 140/146 [00:21<00:00,  6.47it/s]Epoch: 1, train for the 207-th batch, train loss: 0.5458102226257324:  87%|█████████▌ | 207/237 [00:55<00:08,  3.36it/s]Epoch: 1, train for the 198-th batch, train loss: 0.4496031105518341:  51%|█████▋     | 197/383 [00:53<00:52,  3.51it/s]Epoch: 3, train for the 9-th batch, train loss: 0.5685616731643677:   7%|█              | 8/119 [00:01<00:16,  6.62it/s]Epoch: 3, train for the 9-th batch, train loss: 0.5685616731643677:   8%|█▏             | 9/119 [00:01<00:16,  6.85it/s]Epoch: 1, train for the 198-th batch, train loss: 0.4496031105518341:  52%|█████▋     | 198/383 [00:53<00:53,  3.47it/s]evaluate for the 25-th batch, evaluate loss: 0.71341872215271:  33%|██████▋             | 24/72 [00:03<00:06,  7.08it/s]Epoch: 2, train for the 80-th batch, train loss: 0.532669723033905:  52%|███████▎      | 79/151 [00:12<00:10,  6.75it/s]Epoch: 2, train for the 80-th batch, train loss: 0.532669723033905:  53%|███████▍      | 80/151 [00:12<00:11,  6.38it/s]evaluate for the 26-th batch, evaluate loss: 0.4936690330505371:  33%|██████            | 24/72 [00:03<00:06,  7.08it/s]evaluate for the 26-th batch, evaluate loss: 0.4936690330505371:  36%|██████▌           | 26/72 [00:03<00:05,  8.04it/s]Epoch: 2, train for the 141-th batch, train loss: 0.5347887873649597:  96%|██████████▌| 140/146 [00:22<00:00,  6.47it/s]Epoch: 2, train for the 141-th batch, train loss: 0.5347887873649597:  97%|██████████▌| 141/146 [00:22<00:00,  6.16it/s]Epoch: 1, train for the 208-th batch, train loss: 0.5323087573051453:  87%|█████████▌ | 207/237 [00:56<00:08,  3.36it/s]Epoch: 3, train for the 10-th batch, train loss: 0.6004891395568848:   8%|█             | 9/119 [00:01<00:16,  6.85it/s]Epoch: 3, train for the 10-th batch, train loss: 0.6004891395568848:   8%|█            | 10/119 [00:01<00:16,  6.50it/s]Epoch: 1, train for the 208-th batch, train loss: 0.5323087573051453:  88%|█████████▋ | 208/237 [00:56<00:07,  3.64it/s]Epoch: 2, train for the 81-th batch, train loss: 0.5194184184074402:  53%|██████▉      | 80/151 [00:12<00:11,  6.38it/s]Epoch: 1, train for the 199-th batch, train loss: 0.4915190041065216:  52%|█████▋     | 198/383 [00:54<00:53,  3.47it/s]Epoch: 2, train for the 81-th batch, train loss: 0.5194184184074402:  54%|██████▉      | 81/151 [00:12<00:10,  6.49it/s]Epoch: 1, train for the 199-th batch, train loss: 0.4915190041065216:  52%|█████▋     | 199/383 [00:54<00:48,  3.76it/s]Epoch: 2, train for the 142-th batch, train loss: 0.4878411293029785:  97%|██████████▌| 141/146 [00:22<00:00,  6.16it/s]Epoch: 2, train for the 142-th batch, train loss: 0.4878411293029785:  97%|██████████▋| 142/146 [00:22<00:00,  6.30it/s]Epoch: 3, train for the 11-th batch, train loss: 0.5764092803001404:   8%|█            | 10/119 [00:01<00:16,  6.50it/s]Epoch: 3, train for the 11-th batch, train loss: 0.5764092803001404:   9%|█▏           | 11/119 [00:01<00:16,  6.54it/s]Epoch: 2, train for the 82-th batch, train loss: 0.5138700008392334:  54%|██████▉      | 81/151 [00:12<00:10,  6.49it/s]Epoch: 2, train for the 82-th batch, train loss: 0.5138700008392334:  54%|███████      | 82/151 [00:12<00:10,  6.79it/s]evaluate for the 27-th batch, evaluate loss: 0.5008261799812317:  36%|██████▌           | 26/72 [00:03<00:05,  8.04it/s]evaluate for the 27-th batch, evaluate loss: 0.5008261799812317:  38%|██████▊           | 27/72 [00:03<00:06,  6.67it/s]evaluate for the 28-th batch, evaluate loss: 0.540381133556366:  38%|███████▏           | 27/72 [00:03<00:06,  6.67it/s]Epoch: 2, train for the 143-th batch, train loss: 0.5372011661529541:  97%|██████████▋| 142/146 [00:22<00:00,  6.30it/s]Epoch: 2, train for the 143-th batch, train loss: 0.5372011661529541:  98%|██████████▊| 143/146 [00:22<00:00,  6.14it/s]Epoch: 3, train for the 12-th batch, train loss: 0.5852976441383362:   9%|█▏           | 11/119 [00:01<00:16,  6.54it/s]Epoch: 3, train for the 12-th batch, train loss: 0.5852976441383362:  10%|█▎           | 12/119 [00:01<00:16,  6.39it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5393306016921997:  54%|███████      | 82/151 [00:12<00:10,  6.79it/s]evaluate for the 29-th batch, evaluate loss: 0.5613043308258057:  38%|██████▊           | 27/72 [00:03<00:06,  6.67it/s]evaluate for the 29-th batch, evaluate loss: 0.5613043308258057:  40%|███████▎          | 29/72 [00:03<00:05,  8.07it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5393306016921997:  55%|███████▏     | 83/151 [00:12<00:11,  6.12it/s]Epoch: 1, train for the 209-th batch, train loss: 0.5368209481239319:  88%|█████████▋ | 208/237 [00:56<00:07,  3.64it/s]Epoch: 1, train for the 200-th batch, train loss: 0.534089982509613:  52%|██████▏     | 199/383 [00:54<00:48,  3.76it/s]Epoch: 1, train for the 209-th batch, train loss: 0.5368209481239319:  88%|█████████▋ | 209/237 [00:56<00:08,  3.24it/s]evaluate for the 30-th batch, evaluate loss: 0.5835064649581909:  40%|███████▎          | 29/72 [00:03<00:05,  8.07it/s]Epoch: 1, train for the 200-th batch, train loss: 0.534089982509613:  52%|██████▎     | 200/383 [00:54<00:54,  3.36it/s]Epoch: 2, train for the 144-th batch, train loss: 0.4825555384159088:  98%|██████████▊| 143/146 [00:22<00:00,  6.14it/s]Epoch: 2, train for the 144-th batch, train loss: 0.4825555384159088:  99%|██████████▊| 144/146 [00:22<00:00,  6.08it/s]Epoch: 3, train for the 13-th batch, train loss: 0.46317625045776367:  10%|█▏          | 12/119 [00:01<00:16,  6.39it/s]Epoch: 3, train for the 13-th batch, train loss: 0.46317625045776367:  11%|█▎          | 13/119 [00:01<00:16,  6.46it/s]evaluate for the 31-th batch, evaluate loss: 0.6308740973472595:  40%|███████▎          | 29/72 [00:03<00:05,  8.07it/s]evaluate for the 31-th batch, evaluate loss: 0.6308740973472595:  43%|███████▊          | 31/72 [00:03<00:04,  8.85it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5454719662666321:  55%|███████▏     | 83/151 [00:12<00:11,  6.12it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5454719662666321:  56%|███████▏     | 84/151 [00:12<00:11,  5.91it/s]Epoch: 1, train for the 210-th batch, train loss: 0.5655100345611572:  88%|█████████▋ | 209/237 [00:56<00:08,  3.24it/s]evaluate for the 32-th batch, evaluate loss: 0.5441522002220154:  43%|███████▊          | 31/72 [00:03<00:04,  8.85it/s]Epoch: 1, train for the 210-th batch, train loss: 0.5655100345611572:  89%|█████████▋ | 210/237 [00:56<00:07,  3.53it/s]Epoch: 2, train for the 145-th batch, train loss: 0.513640820980072:  99%|███████████▊| 144/146 [00:22<00:00,  6.08it/s]Epoch: 3, train for the 14-th batch, train loss: 0.48873716592788696:  11%|█▎          | 13/119 [00:02<00:16,  6.46it/s]Epoch: 2, train for the 85-th batch, train loss: 0.5050500631332397:  56%|███████▏     | 84/151 [00:13<00:11,  5.91it/s]Epoch: 3, train for the 14-th batch, train loss: 0.48873716592788696:  12%|█▍          | 14/119 [00:02<00:17,  5.91it/s]Epoch: 2, train for the 145-th batch, train loss: 0.513640820980072:  99%|███████████▉| 145/146 [00:22<00:00,  5.59it/s]Epoch: 2, train for the 85-th batch, train loss: 0.5050500631332397:  56%|███████▎     | 85/151 [00:13<00:10,  6.40it/s]Epoch: 1, train for the 211-th batch, train loss: 0.535815954208374:  89%|██████████▋ | 210/237 [00:56<00:07,  3.53it/s]Epoch: 2, train for the 86-th batch, train loss: 0.49976643919944763:  56%|██████▊     | 85/151 [00:13<00:10,  6.40it/s]Epoch: 1, train for the 211-th batch, train loss: 0.535815954208374:  89%|██████████▋ | 211/237 [00:57<00:06,  3.96it/s]Epoch: 2, train for the 86-th batch, train loss: 0.49976643919944763:  57%|██████▊     | 86/151 [00:13<00:09,  6.62it/s]Epoch: 3, train for the 15-th batch, train loss: 0.5361388325691223:  12%|█▌           | 14/119 [00:02<00:17,  5.91it/s]Epoch: 2, train for the 146-th batch, train loss: 0.5074642300605774:  99%|██████████▉| 145/146 [00:22<00:00,  5.59it/s]Epoch: 3, train for the 15-th batch, train loss: 0.5361388325691223:  13%|█▋           | 15/119 [00:02<00:17,  5.81it/s]Epoch: 2, train for the 146-th batch, train loss: 0.5074642300605774: 100%|███████████| 146/146 [00:22<00:00,  5.58it/s]Epoch: 2, train for the 146-th batch, train loss: 0.5074642300605774: 100%|███████████| 146/146 [00:22<00:00,  6.37it/s]
Epoch: 1, train for the 201-th batch, train loss: 0.5935276746749878:  52%|█████▋     | 200/383 [00:54<00:54,  3.36it/s]evaluate for the 33-th batch, evaluate loss: 0.6027941107749939:  43%|███████▊          | 31/72 [00:04<00:04,  8.85it/s]evaluate for the 33-th batch, evaluate loss: 0.6027941107749939:  46%|████████▎         | 33/72 [00:04<00:05,  7.35it/s]Epoch: 1, train for the 201-th batch, train loss: 0.5935276746749878:  52%|█████▊     | 201/383 [00:54<01:03,  2.85it/s]Epoch: 3, train for the 16-th batch, train loss: 0.4888235628604889:  13%|█▋           | 15/119 [00:02<00:17,  5.81it/s]Epoch: 3, train for the 16-th batch, train loss: 0.4888235628604889:  13%|█▋           | 16/119 [00:02<00:16,  6.31it/s]evaluate for the 34-th batch, evaluate loss: 0.6692332029342651:  46%|████████▎         | 33/72 [00:04<00:05,  7.35it/s]Epoch: 2, train for the 87-th batch, train loss: 0.4924025535583496:  57%|███████▍     | 86/151 [00:13<00:09,  6.62it/s]Epoch: 1, train for the 212-th batch, train loss: 0.5429701805114746:  89%|█████████▊ | 211/237 [00:57<00:06,  3.96it/s]Epoch: 2, train for the 87-th batch, train loss: 0.4924025535583496:  58%|███████▍     | 87/151 [00:13<00:10,  6.05it/s]Epoch: 1, train for the 212-th batch, train loss: 0.5429701805114746:  89%|█████████▊ | 212/237 [00:57<00:06,  4.12it/s]  0%|                                                                                            | 0/38 [00:00<?, ?it/s]evaluate for the 35-th batch, evaluate loss: 0.6041969060897827:  46%|████████▎         | 33/72 [00:04<00:05,  7.35it/s]evaluate for the 35-th batch, evaluate loss: 0.6041969060897827:  49%|████████▊         | 35/72 [00:04<00:04,  8.45it/s]Epoch: 1, train for the 202-th batch, train loss: 0.5158329606056213:  52%|█████▊     | 201/383 [00:55<01:03,  2.85it/s]Epoch: 3, train for the 17-th batch, train loss: 0.4426676034927368:  13%|█▋           | 16/119 [00:02<00:16,  6.31it/s]Epoch: 3, train for the 17-th batch, train loss: 0.4426676034927368:  14%|█▊           | 17/119 [00:02<00:15,  6.73it/s]evaluate for the 1-th batch, evaluate loss: 0.5719567537307739:   0%|                            | 0/38 [00:00<?, ?it/s]Epoch: 1, train for the 202-th batch, train loss: 0.5158329606056213:  53%|█████▊     | 202/383 [00:55<00:56,  3.22it/s]evaluate for the 2-th batch, evaluate loss: 0.585300624370575:   0%|                             | 0/38 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.585300624370575:   5%|█                    | 2/38 [00:00<00:02, 15.97it/s]Epoch: 1, train for the 213-th batch, train loss: 0.5403560996055603:  89%|█████████▊ | 212/237 [00:57<00:06,  4.12it/s]Epoch: 3, train for the 18-th batch, train loss: 0.4259263575077057:  14%|█▊           | 17/119 [00:02<00:15,  6.73it/s]Epoch: 3, train for the 18-th batch, train loss: 0.4259263575077057:  15%|█▉           | 18/119 [00:02<00:14,  6.74it/s]evaluate for the 3-th batch, evaluate loss: 0.5290625095367432:   5%|█                   | 2/38 [00:00<00:02, 15.97it/s]Epoch: 1, train for the 213-th batch, train loss: 0.5403560996055603:  90%|█████████▉ | 213/237 [00:57<00:05,  4.17it/s]Epoch: 2, train for the 88-th batch, train loss: 0.5310113430023193:  58%|███████▍     | 87/151 [00:13<00:10,  6.05it/s]Epoch: 2, train for the 88-th batch, train loss: 0.5310113430023193:  58%|███████▌     | 88/151 [00:13<00:12,  5.01it/s]Epoch: 1, train for the 203-th batch, train loss: 0.47088322043418884:  53%|█████▎    | 202/383 [00:55<00:56,  3.22it/s]evaluate for the 4-th batch, evaluate loss: 0.5354864597320557:   5%|█                   | 2/38 [00:00<00:02, 15.97it/s]evaluate for the 4-th batch, evaluate loss: 0.5354864597320557:  11%|██                  | 4/38 [00:00<00:02, 14.47it/s]Epoch: 1, train for the 203-th batch, train loss: 0.47088322043418884:  53%|█████▎    | 203/383 [00:55<00:51,  3.49it/s]evaluate for the 36-th batch, evaluate loss: 0.6390808820724487:  49%|████████▊         | 35/72 [00:04<00:04,  8.45it/s]evaluate for the 36-th batch, evaluate loss: 0.6390808820724487:  50%|█████████         | 36/72 [00:04<00:05,  6.59it/s]evaluate for the 5-th batch, evaluate loss: 0.6150338649749756:  11%|██                  | 4/38 [00:00<00:02, 14.47it/s]evaluate for the 6-th batch, evaluate loss: 0.5537095665931702:  11%|██                  | 4/38 [00:00<00:02, 14.47it/s]evaluate for the 6-th batch, evaluate loss: 0.5537095665931702:  16%|███▏                | 6/38 [00:00<00:02, 15.46it/s]evaluate for the 37-th batch, evaluate loss: 0.5963169932365417:  50%|█████████         | 36/72 [00:04<00:05,  6.59it/s]Epoch: 2, train for the 89-th batch, train loss: 0.513626754283905:  58%|████████▏     | 88/151 [00:13<00:12,  5.01it/s]Epoch: 2, train for the 89-th batch, train loss: 0.513626754283905:  59%|████████▎     | 89/151 [00:13<00:12,  5.09it/s]evaluate for the 38-th batch, evaluate loss: 0.5100169777870178:  50%|█████████         | 36/72 [00:04<00:05,  6.59it/s]evaluate for the 38-th batch, evaluate loss: 0.5100169777870178:  53%|█████████▌        | 38/72 [00:04<00:04,  7.92it/s]Epoch: 3, train for the 19-th batch, train loss: 0.41765296459198:  15%|██▎            | 18/119 [00:03<00:14,  6.74it/s]evaluate for the 7-th batch, evaluate loss: 0.5104712843894958:  16%|███▏                | 6/38 [00:00<00:02, 15.46it/s]Epoch: 3, train for the 19-th batch, train loss: 0.41765296459198:  16%|██▍            | 19/119 [00:03<00:18,  5.28it/s]evaluate for the 39-th batch, evaluate loss: 0.5183379054069519:  53%|█████████▌        | 38/72 [00:04<00:04,  7.92it/s]evaluate for the 8-th batch, evaluate loss: 0.5121440887451172:  16%|███▏                | 6/38 [00:00<00:02, 15.46it/s]evaluate for the 8-th batch, evaluate loss: 0.5121440887451172:  21%|████▏               | 8/38 [00:00<00:02, 14.10it/s]Epoch: 1, train for the 204-th batch, train loss: 0.31471359729766846:  53%|█████▎    | 203/383 [00:55<00:51,  3.49it/s]Epoch: 1, train for the 214-th batch, train loss: 0.5598515868186951:  90%|█████████▉ | 213/237 [00:57<00:05,  4.17it/s]Epoch: 1, train for the 204-th batch, train loss: 0.31471359729766846:  53%|█████▎    | 204/383 [00:55<00:51,  3.50it/s]Epoch: 2, train for the 90-th batch, train loss: 0.4975954294204712:  59%|███████▋     | 89/151 [00:14<00:12,  5.09it/s]Epoch: 1, train for the 214-th batch, train loss: 0.5598515868186951:  90%|█████████▉ | 214/237 [00:57<00:06,  3.56it/s]evaluate for the 40-th batch, evaluate loss: 0.590190589427948:  53%|██████████         | 38/72 [00:04<00:04,  7.92it/s]evaluate for the 40-th batch, evaluate loss: 0.590190589427948:  56%|██████████▌        | 40/72 [00:04<00:03,  9.36it/s]Epoch: 2, train for the 90-th batch, train loss: 0.4975954294204712:  60%|███████▋     | 90/151 [00:14<00:11,  5.33it/s]evaluate for the 9-th batch, evaluate loss: 0.5453578233718872:  21%|████▏               | 8/38 [00:00<00:02, 14.10it/s]Epoch: 3, train for the 20-th batch, train loss: 0.41115817427635193:  16%|█▉          | 19/119 [00:03<00:18,  5.28it/s]Epoch: 3, train for the 20-th batch, train loss: 0.41115817427635193:  17%|██          | 20/119 [00:03<00:18,  5.37it/s]evaluate for the 10-th batch, evaluate loss: 0.5452932715415955:  21%|████               | 8/38 [00:00<00:02, 14.10it/s]evaluate for the 10-th batch, evaluate loss: 0.5452932715415955:  26%|████▋             | 10/38 [00:00<00:02, 13.83it/s]Epoch: 2, train for the 91-th batch, train loss: 0.4911140203475952:  60%|███████▋     | 90/151 [00:14<00:11,  5.33it/s]evaluate for the 11-th batch, evaluate loss: 0.48338058590888977:  26%|████▍            | 10/38 [00:00<00:02, 13.83it/s]Epoch: 2, train for the 91-th batch, train loss: 0.4911140203475952:  60%|███████▊     | 91/151 [00:14<00:10,  5.56it/s]Epoch: 1, train for the 205-th batch, train loss: 0.39311057329177856:  53%|█████▎    | 204/383 [00:55<00:51,  3.50it/s]Epoch: 3, train for the 21-th batch, train loss: 0.4543725848197937:  17%|██▏          | 20/119 [00:03<00:18,  5.37it/s]Epoch: 3, train for the 21-th batch, train loss: 0.4543725848197937:  18%|██▎          | 21/119 [00:03<00:17,  5.68it/s]evaluate for the 12-th batch, evaluate loss: 0.5766419768333435:  26%|████▋             | 10/38 [00:00<00:02, 13.83it/s]evaluate for the 12-th batch, evaluate loss: 0.5766419768333435:  32%|█████▋            | 12/38 [00:00<00:01, 14.39it/s]Epoch: 1, train for the 205-th batch, train loss: 0.39311057329177856:  54%|█████▎    | 205/383 [00:55<00:49,  3.58it/s]evaluate for the 13-th batch, evaluate loss: 0.5322295427322388:  32%|█████▋            | 12/38 [00:00<00:01, 14.39it/s]evaluate for the 41-th batch, evaluate loss: 0.6467334032058716:  56%|██████████        | 40/72 [00:05<00:03,  9.36it/s]Epoch: 1, train for the 215-th batch, train loss: 0.5503301024436951:  90%|█████████▉ | 214/237 [00:58<00:06,  3.56it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5344025492668152:  60%|███████▊     | 91/151 [00:14<00:10,  5.56it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5344025492668152:  61%|███████▉     | 92/151 [00:14<00:10,  5.50it/s]Epoch: 1, train for the 215-th batch, train loss: 0.5503301024436951:  91%|█████████▉ | 215/237 [00:58<00:06,  3.26it/s]Epoch: 3, train for the 22-th batch, train loss: 0.40954115986824036:  18%|██          | 21/119 [00:03<00:17,  5.68it/s]evaluate for the 14-th batch, evaluate loss: 0.4702727794647217:  32%|█████▋            | 12/38 [00:00<00:01, 14.39it/s]evaluate for the 14-th batch, evaluate loss: 0.4702727794647217:  37%|██████▋           | 14/38 [00:00<00:01, 14.33it/s]Epoch: 3, train for the 22-th batch, train loss: 0.40954115986824036:  18%|██▏         | 22/119 [00:03<00:16,  5.89it/s]evaluate for the 42-th batch, evaluate loss: 0.5960763692855835:  56%|██████████        | 40/72 [00:05<00:03,  9.36it/s]evaluate for the 42-th batch, evaluate loss: 0.5960763692855835:  58%|██████████▌       | 42/72 [00:05<00:03,  7.51it/s]evaluate for the 15-th batch, evaluate loss: 0.47814247012138367:  37%|██████▎          | 14/38 [00:01<00:01, 14.33it/s]Epoch: 1, train for the 206-th batch, train loss: 0.49104610085487366:  54%|█████▎    | 205/383 [00:56<00:49,  3.58it/s]evaluate for the 43-th batch, evaluate loss: 0.6452966928482056:  58%|██████████▌       | 42/72 [00:05<00:03,  7.51it/s]Epoch: 1, train for the 206-th batch, train loss: 0.49104610085487366:  54%|█████▍    | 206/383 [00:56<00:47,  3.70it/s]evaluate for the 16-th batch, evaluate loss: 0.5588417053222656:  37%|██████▋           | 14/38 [00:01<00:01, 14.33it/s]evaluate for the 16-th batch, evaluate loss: 0.5588417053222656:  42%|███████▌          | 16/38 [00:01<00:01, 13.04it/s]Epoch: 3, train for the 23-th batch, train loss: 0.45034706592559814:  18%|██▏         | 22/119 [00:03<00:16,  5.89it/s]Epoch: 3, train for the 23-th batch, train loss: 0.45034706592559814:  19%|██▎         | 23/119 [00:03<00:16,  5.69it/s]Epoch: 1, train for the 216-th batch, train loss: 0.4704413414001465:  91%|█████████▉ | 215/237 [00:58<00:06,  3.26it/s]evaluate for the 44-th batch, evaluate loss: 0.5599949359893799:  58%|██████████▌       | 42/72 [00:05<00:03,  7.51it/s]evaluate for the 44-th batch, evaluate loss: 0.5599949359893799:  61%|███████████       | 44/72 [00:05<00:03,  8.26it/s]Epoch: 2, train for the 93-th batch, train loss: 0.5835332870483398:  61%|███████▉     | 92/151 [00:14<00:10,  5.50it/s]Epoch: 2, train for the 93-th batch, train loss: 0.5835332870483398:  62%|████████     | 93/151 [00:14<00:11,  5.00it/s]Epoch: 1, train for the 216-th batch, train loss: 0.4704413414001465:  91%|██████████ | 216/237 [00:58<00:06,  3.49it/s]evaluate for the 17-th batch, evaluate loss: 0.5232552886009216:  42%|███████▌          | 16/38 [00:01<00:01, 13.04it/s]evaluate for the 18-th batch, evaluate loss: 0.5601419806480408:  42%|███████▌          | 16/38 [00:01<00:01, 13.04it/s]evaluate for the 18-th batch, evaluate loss: 0.5601419806480408:  47%|████████▌         | 18/38 [00:01<00:01, 14.10it/s]Epoch: 1, train for the 207-th batch, train loss: 0.43126577138900757:  54%|█████▍    | 206/383 [00:56<00:47,  3.70it/s]Epoch: 3, train for the 24-th batch, train loss: 0.3751971423625946:  19%|██▌          | 23/119 [00:03<00:16,  5.69it/s]Epoch: 3, train for the 24-th batch, train loss: 0.3751971423625946:  20%|██▌          | 24/119 [00:03<00:16,  5.84it/s]evaluate for the 19-th batch, evaluate loss: 0.535714864730835:  47%|█████████          | 18/38 [00:01<00:01, 14.10it/s]Epoch: 2, train for the 94-th batch, train loss: 0.5318723320960999:  62%|████████     | 93/151 [00:14<00:11,  5.00it/s]Epoch: 1, train for the 207-th batch, train loss: 0.43126577138900757:  54%|█████▍    | 207/383 [00:56<00:46,  3.81it/s]Epoch: 2, train for the 94-th batch, train loss: 0.5318723320960999:  62%|████████     | 94/151 [00:14<00:10,  5.24it/s]evaluate for the 20-th batch, evaluate loss: 0.4686506986618042:  47%|████████▌         | 18/38 [00:01<00:01, 14.10it/s]evaluate for the 20-th batch, evaluate loss: 0.4686506986618042:  53%|█████████▍        | 20/38 [00:01<00:01, 14.98it/s]evaluate for the 21-th batch, evaluate loss: 0.4811750054359436:  53%|█████████▍        | 20/38 [00:01<00:01, 14.98it/s]evaluate for the 45-th batch, evaluate loss: 0.6187102198600769:  61%|███████████       | 44/72 [00:05<00:03,  8.26it/s]evaluate for the 45-th batch, evaluate loss: 0.6187102198600769:  62%|███████████▎      | 45/72 [00:05<00:04,  6.55it/s]Epoch: 3, train for the 25-th batch, train loss: 0.4072233736515045:  20%|██▌          | 24/119 [00:04<00:16,  5.84it/s]Epoch: 3, train for the 25-th batch, train loss: 0.4072233736515045:  21%|██▋          | 25/119 [00:04<00:15,  5.88it/s]evaluate for the 22-th batch, evaluate loss: 0.5138207674026489:  53%|█████████▍        | 20/38 [00:01<00:01, 14.98it/s]evaluate for the 22-th batch, evaluate loss: 0.5138207674026489:  58%|██████████▍       | 22/38 [00:01<00:01, 15.34it/s]Epoch: 2, train for the 95-th batch, train loss: 0.48467758297920227:  62%|███████▍    | 94/151 [00:15<00:10,  5.24it/s]evaluate for the 46-th batch, evaluate loss: 0.6851950883865356:  62%|███████████▎      | 45/72 [00:05<00:04,  6.55it/s]Epoch: 2, train for the 95-th batch, train loss: 0.48467758297920227:  63%|███████▌    | 95/151 [00:15<00:10,  5.26it/s]evaluate for the 23-th batch, evaluate loss: 0.5133396983146667:  58%|██████████▍       | 22/38 [00:01<00:01, 15.34it/s]Epoch: 3, train for the 26-th batch, train loss: 0.3774975836277008:  21%|██▋          | 25/119 [00:04<00:15,  5.88it/s]evaluate for the 47-th batch, evaluate loss: 0.36460310220718384:  62%|██████████▋      | 45/72 [00:05<00:04,  6.55it/s]evaluate for the 47-th batch, evaluate loss: 0.36460310220718384:  65%|███████████      | 47/72 [00:05<00:03,  7.90it/s]Epoch: 3, train for the 26-th batch, train loss: 0.3774975836277008:  22%|██▊          | 26/119 [00:04<00:14,  6.22it/s]Epoch: 1, train for the 208-th batch, train loss: 0.47390857338905334:  54%|█████▍    | 207/383 [00:56<00:46,  3.81it/s]Epoch: 1, train for the 217-th batch, train loss: 0.4900984764099121:  91%|██████████ | 216/237 [00:58<00:06,  3.49it/s]evaluate for the 24-th batch, evaluate loss: 0.50248122215271:  58%|███████████▌        | 22/38 [00:01<00:01, 15.34it/s]evaluate for the 24-th batch, evaluate loss: 0.50248122215271:  63%|████████████▋       | 24/38 [00:01<00:00, 14.94it/s]Epoch: 1, train for the 208-th batch, train loss: 0.47390857338905334:  54%|█████▍    | 208/383 [00:56<00:49,  3.52it/s]evaluate for the 48-th batch, evaluate loss: 0.38949960470199585:  65%|███████████      | 47/72 [00:05<00:03,  7.90it/s]Epoch: 1, train for the 217-th batch, train loss: 0.4900984764099121:  92%|██████████ | 217/237 [00:58<00:06,  2.88it/s]Epoch: 2, train for the 96-th batch, train loss: 0.5288100838661194:  63%|████████▏    | 95/151 [00:15<00:10,  5.26it/s]evaluate for the 25-th batch, evaluate loss: 0.5240187048912048:  63%|███████████▎      | 24/38 [00:01<00:00, 14.94it/s]Epoch: 2, train for the 96-th batch, train loss: 0.5288100838661194:  64%|████████▎    | 96/151 [00:15<00:10,  5.48it/s]evaluate for the 49-th batch, evaluate loss: 0.5202388763427734:  65%|███████████▊      | 47/72 [00:05<00:03,  7.90it/s]evaluate for the 49-th batch, evaluate loss: 0.5202388763427734:  68%|████████████▎     | 49/72 [00:05<00:02,  9.36it/s]Epoch: 3, train for the 27-th batch, train loss: 0.455467164516449:  22%|███           | 26/119 [00:04<00:14,  6.22it/s]evaluate for the 26-th batch, evaluate loss: 0.5112893581390381:  63%|███████████▎      | 24/38 [00:01<00:00, 14.94it/s]evaluate for the 26-th batch, evaluate loss: 0.5112893581390381:  68%|████████████▎     | 26/38 [00:01<00:00, 13.95it/s]Epoch: 3, train for the 27-th batch, train loss: 0.455467164516449:  23%|███▏          | 27/119 [00:04<00:15,  5.96it/s]evaluate for the 50-th batch, evaluate loss: 0.45763927698135376:  68%|███████████▌     | 49/72 [00:06<00:02,  9.36it/s]evaluate for the 27-th batch, evaluate loss: 0.5355788469314575:  68%|████████████▎     | 26/38 [00:01<00:00, 13.95it/s]Epoch: 2, train for the 97-th batch, train loss: 0.524144172668457:  64%|████████▉     | 96/151 [00:15<00:10,  5.48it/s]Epoch: 1, train for the 218-th batch, train loss: 0.5263506174087524:  92%|██████████ | 217/237 [00:59<00:06,  2.88it/s]evaluate for the 51-th batch, evaluate loss: 0.5507150888442993:  68%|████████████▎     | 49/72 [00:06<00:02,  9.36it/s]evaluate for the 51-th batch, evaluate loss: 0.5507150888442993:  71%|████████████▊     | 51/72 [00:06<00:02, 10.11it/s]Epoch: 2, train for the 97-th batch, train loss: 0.524144172668457:  64%|████████▉     | 97/151 [00:15<00:10,  5.18it/s]Epoch: 1, train for the 218-th batch, train loss: 0.5263506174087524:  92%|██████████ | 218/237 [00:59<00:06,  3.13it/s]Epoch: 1, train for the 209-th batch, train loss: 0.42879176139831543:  54%|█████▍    | 208/383 [00:57<00:49,  3.52it/s]Epoch: 1, train for the 209-th batch, train loss: 0.42879176139831543:  55%|█████▍    | 209/383 [00:57<00:50,  3.45it/s]Epoch: 3, train for the 28-th batch, train loss: 0.45289283990859985:  23%|██▋         | 27/119 [00:04<00:15,  5.96it/s]evaluate for the 28-th batch, evaluate loss: 0.5357273817062378:  68%|████████████▎     | 26/38 [00:01<00:00, 13.95it/s]evaluate for the 28-th batch, evaluate loss: 0.5357273817062378:  74%|█████████████▎    | 28/38 [00:01<00:00, 12.91it/s]Epoch: 3, train for the 28-th batch, train loss: 0.45289283990859985:  24%|██▊         | 28/119 [00:04<00:15,  5.82it/s]Epoch: 2, train for the 98-th batch, train loss: 0.5064332485198975:  64%|████████▎    | 97/151 [00:15<00:10,  5.18it/s]evaluate for the 29-th batch, evaluate loss: 0.5436987280845642:  74%|█████████████▎    | 28/38 [00:02<00:00, 12.91it/s]Epoch: 2, train for the 98-th batch, train loss: 0.5064332485198975:  65%|████████▍    | 98/151 [00:15<00:09,  5.69it/s]Epoch: 3, train for the 29-th batch, train loss: 0.4324917495250702:  24%|███          | 28/119 [00:04<00:15,  5.82it/s]evaluate for the 52-th batch, evaluate loss: 0.5201447010040283:  71%|████████████▊     | 51/72 [00:06<00:02, 10.11it/s]Epoch: 3, train for the 29-th batch, train loss: 0.4324917495250702:  24%|███▏         | 29/119 [00:04<00:14,  6.04it/s]evaluate for the 30-th batch, evaluate loss: 0.5447443127632141:  74%|█████████████▎    | 28/38 [00:02<00:00, 12.91it/s]evaluate for the 30-th batch, evaluate loss: 0.5447443127632141:  79%|██████████████▏   | 30/38 [00:02<00:00, 12.86it/s]Epoch: 1, train for the 210-th batch, train loss: 0.5143485069274902:  55%|██████     | 209/383 [00:57<00:50,  3.45it/s]evaluate for the 31-th batch, evaluate loss: 0.5185250639915466:  79%|██████████████▏   | 30/38 [00:02<00:00, 12.86it/s]Epoch: 1, train for the 219-th batch, train loss: 0.5367664098739624:  92%|██████████ | 218/237 [00:59<00:06,  3.13it/s]Epoch: 1, train for the 210-th batch, train loss: 0.5143485069274902:  55%|██████     | 210/383 [00:57<00:47,  3.63it/s]evaluate for the 53-th batch, evaluate loss: 0.5641939640045166:  71%|████████████▊     | 51/72 [00:06<00:02, 10.11it/s]evaluate for the 53-th batch, evaluate loss: 0.5641939640045166:  74%|█████████████▎    | 53/72 [00:06<00:02,  8.55it/s]Epoch: 2, train for the 99-th batch, train loss: 0.5241519212722778:  65%|████████▍    | 98/151 [00:15<00:09,  5.69it/s]Epoch: 1, train for the 219-th batch, train loss: 0.5367664098739624:  92%|██████████▏| 219/237 [00:59<00:05,  3.15it/s]Epoch: 2, train for the 99-th batch, train loss: 0.5241519212722778:  66%|████████▌    | 99/151 [00:15<00:09,  5.57it/s]Epoch: 3, train for the 30-th batch, train loss: 0.40026891231536865:  24%|██▉         | 29/119 [00:04<00:14,  6.04it/s]evaluate for the 32-th batch, evaluate loss: 0.47454822063446045:  79%|█████████████▍   | 30/38 [00:02<00:00, 12.86it/s]evaluate for the 32-th batch, evaluate loss: 0.47454822063446045:  84%|██████████████▎  | 32/38 [00:02<00:00, 12.88it/s]Epoch: 3, train for the 30-th batch, train loss: 0.40026891231536865:  25%|███         | 30/119 [00:04<00:14,  6.08it/s]evaluate for the 54-th batch, evaluate loss: 0.6610973477363586:  74%|█████████████▎    | 53/72 [00:06<00:02,  8.55it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5679275989532471:  66%|███████▊    | 99/151 [00:15<00:09,  5.57it/s]Epoch: 3, train for the 31-th batch, train loss: 0.4387073516845703:  25%|███▎         | 30/119 [00:04<00:14,  6.08it/s]Epoch: 3, train for the 31-th batch, train loss: 0.4387073516845703:  26%|███▍         | 31/119 [00:04<00:13,  6.50it/s]Epoch: 1, train for the 211-th batch, train loss: 0.43737342953681946:  55%|█████▍    | 210/383 [00:57<00:47,  3.63it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5679275989532471:  66%|███████▎   | 100/151 [00:15<00:09,  5.57it/s]Epoch: 1, train for the 211-th batch, train loss: 0.43737342953681946:  55%|█████▌    | 211/383 [00:57<00:45,  3.76it/s]Epoch: 3, train for the 32-th batch, train loss: 0.3843922019004822:  26%|███▍         | 31/119 [00:05<00:13,  6.50it/s]Epoch: 2, train for the 101-th batch, train loss: 0.5945081114768982:  66%|███████▎   | 100/151 [00:16<00:09,  5.57it/s]Epoch: 2, train for the 101-th batch, train loss: 0.5945081114768982:  67%|███████▎   | 101/151 [00:16<00:08,  5.91it/s]Epoch: 3, train for the 33-th batch, train loss: 0.4386744499206543:  26%|███▍         | 31/119 [00:05<00:13,  6.50it/s]Epoch: 3, train for the 33-th batch, train loss: 0.4386744499206543:  28%|███▌         | 33/119 [00:05<00:10,  7.82it/s]evaluate for the 33-th batch, evaluate loss: 0.4806858003139496:  84%|███████████████▏  | 32/38 [00:02<00:00, 12.88it/s]Epoch: 2, train for the 102-th batch, train loss: 0.49812057614326477:  67%|██████▋   | 101/151 [00:16<00:08,  5.91it/s]evaluate for the 55-th batch, evaluate loss: 0.5960385799407959:  74%|█████████████▎    | 53/72 [00:06<00:02,  8.55it/s]evaluate for the 55-th batch, evaluate loss: 0.5960385799407959:  76%|█████████████▊    | 55/72 [00:06<00:02,  6.43it/s]Epoch: 2, train for the 102-th batch, train loss: 0.49812057614326477:  68%|██████▊   | 102/151 [00:16<00:07,  6.38it/s]Epoch: 1, train for the 212-th batch, train loss: 0.6123707294464111:  55%|██████     | 211/383 [00:57<00:45,  3.76it/s]Epoch: 3, train for the 34-th batch, train loss: 0.42671477794647217:  28%|███▎        | 33/119 [00:05<00:10,  7.82it/s]Epoch: 1, train for the 220-th batch, train loss: 0.53866046667099:  92%|████████████ | 219/237 [01:00<00:05,  3.15it/s]Epoch: 3, train for the 34-th batch, train loss: 0.42671477794647217:  29%|███▍        | 34/119 [00:05<00:10,  7.80it/s]evaluate for the 34-th batch, evaluate loss: 0.5277805328369141:  84%|███████████████▏  | 32/38 [00:02<00:00, 12.88it/s]evaluate for the 34-th batch, evaluate loss: 0.5277805328369141:  89%|████████████████  | 34/38 [00:02<00:00,  8.07it/s]Epoch: 1, train for the 212-th batch, train loss: 0.6123707294464111:  55%|██████     | 212/383 [00:57<00:48,  3.55it/s]evaluate for the 56-th batch, evaluate loss: 0.6143290996551514:  76%|█████████████▊    | 55/72 [00:06<00:02,  6.43it/s]Epoch: 1, train for the 220-th batch, train loss: 0.53866046667099:  93%|████████████ | 220/237 [01:00<00:06,  2.59it/s]evaluate for the 35-th batch, evaluate loss: 0.5382713675498962:  89%|████████████████  | 34/38 [00:02<00:00,  8.07it/s]evaluate for the 57-th batch, evaluate loss: 0.5967807769775391:  76%|█████████████▊    | 55/72 [00:07<00:02,  6.43it/s]evaluate for the 57-th batch, evaluate loss: 0.5967807769775391:  79%|██████████████▎   | 57/72 [00:07<00:02,  7.45it/s]Epoch: 2, train for the 103-th batch, train loss: 0.5770204663276672:  68%|███████▍   | 102/151 [00:16<00:07,  6.38it/s]Epoch: 3, train for the 35-th batch, train loss: 0.4145970940589905:  29%|███▋         | 34/119 [00:05<00:10,  7.80it/s]evaluate for the 36-th batch, evaluate loss: 0.5521470308303833:  89%|████████████████  | 34/38 [00:02<00:00,  8.07it/s]evaluate for the 36-th batch, evaluate loss: 0.5521470308303833:  95%|█████████████████ | 36/38 [00:02<00:00,  9.17it/s]Epoch: 3, train for the 35-th batch, train loss: 0.4145970940589905:  29%|███▊         | 35/119 [00:05<00:11,  7.34it/s]Epoch: 2, train for the 103-th batch, train loss: 0.5770204663276672:  68%|███████▌   | 103/151 [00:16<00:08,  5.82it/s]Epoch: 1, train for the 221-th batch, train loss: 0.5564965605735779:  93%|██████████▏| 220/237 [01:00<00:06,  2.59it/s]Epoch: 3, train for the 36-th batch, train loss: 0.396261066198349:  29%|████          | 35/119 [00:05<00:11,  7.34it/s]Epoch: 3, train for the 36-th batch, train loss: 0.396261066198349:  30%|████▏         | 36/119 [00:05<00:10,  7.62it/s]Epoch: 1, train for the 213-th batch, train loss: 0.576204776763916:  55%|██████▋     | 212/383 [00:58<00:48,  3.55it/s]Epoch: 2, train for the 104-th batch, train loss: 0.5518590211868286:  68%|███████▌   | 103/151 [00:16<00:08,  5.82it/s]Epoch: 1, train for the 221-th batch, train loss: 0.5564965605735779:  93%|██████████▎| 221/237 [01:00<00:05,  2.93it/s]Epoch: 2, train for the 104-th batch, train loss: 0.5518590211868286:  69%|███████▌   | 104/151 [00:16<00:07,  6.28it/s]Epoch: 1, train for the 213-th batch, train loss: 0.576204776763916:  56%|██████▋     | 213/383 [00:58<00:48,  3.54it/s]Epoch: 3, train for the 37-th batch, train loss: 0.465646892786026:  30%|████▏         | 36/119 [00:05<00:10,  7.62it/s]Epoch: 3, train for the 37-th batch, train loss: 0.465646892786026:  31%|████▎         | 37/119 [00:05<00:10,  7.70it/s]evaluate for the 58-th batch, evaluate loss: 0.6205112934112549:  79%|██████████████▎   | 57/72 [00:07<00:02,  7.45it/s]evaluate for the 58-th batch, evaluate loss: 0.6205112934112549:  81%|██████████████▌   | 58/72 [00:07<00:02,  6.13it/s]Epoch: 2, train for the 105-th batch, train loss: 0.5276526808738708:  69%|███████▌   | 104/151 [00:16<00:07,  6.28it/s]evaluate for the 59-th batch, evaluate loss: 0.5391636490821838:  81%|██████████████▌   | 58/72 [00:07<00:02,  6.13it/s]Epoch: 2, train for the 105-th batch, train loss: 0.5276526808738708:  70%|███████▋   | 105/151 [00:16<00:07,  6.02it/s]Epoch: 3, train for the 38-th batch, train loss: 0.39367592334747314:  31%|███▋        | 37/119 [00:05<00:10,  7.70it/s]evaluate for the 37-th batch, evaluate loss: 0.5034254789352417:  95%|█████████████████ | 36/38 [00:03<00:00,  9.17it/s]Epoch: 1, train for the 214-th batch, train loss: 0.4875277578830719:  56%|██████     | 213/383 [00:58<00:48,  3.54it/s]Epoch: 3, train for the 38-th batch, train loss: 0.39367592334747314:  32%|███▊        | 38/119 [00:05<00:10,  7.82it/s]evaluate for the 60-th batch, evaluate loss: 0.5540900826454163:  81%|██████████████▌   | 58/72 [00:07<00:02,  6.13it/s]evaluate for the 60-th batch, evaluate loss: 0.5540900826454163:  83%|███████████████   | 60/72 [00:07<00:01,  7.81it/s]Epoch: 1, train for the 222-th batch, train loss: 0.5443410277366638:  93%|██████████▎| 221/237 [01:00<00:05,  2.93it/s]Epoch: 1, train for the 214-th batch, train loss: 0.4875277578830719:  56%|██████▏    | 214/383 [00:58<00:46,  3.62it/s]evaluate for the 38-th batch, evaluate loss: 0.5326409339904785:  95%|█████████████████ | 36/38 [00:03<00:00,  9.17it/s]evaluate for the 38-th batch, evaluate loss: 0.5326409339904785: 100%|██████████████████| 38/38 [00:03<00:00,  7.09it/s]evaluate for the 38-th batch, evaluate loss: 0.5326409339904785: 100%|██████████████████| 38/38 [00:03<00:00, 11.34it/s]
evaluate for the 61-th batch, evaluate loss: 0.48674288392066956:  83%|██████████████▏  | 60/72 [00:07<00:01,  7.81it/s]Epoch: 1, train for the 222-th batch, train loss: 0.5443410277366638:  94%|██████████▎| 222/237 [01:00<00:05,  2.92it/s]Epoch: 2, train for the 106-th batch, train loss: 0.500027060508728:  70%|████████▎   | 105/151 [00:16<00:07,  6.02it/s]evaluate for the 62-th batch, evaluate loss: 0.5031022429466248:  83%|███████████████   | 60/72 [00:07<00:01,  7.81it/s]evaluate for the 62-th batch, evaluate loss: 0.5031022429466248:  86%|███████████████▌  | 62/72 [00:07<00:01,  9.08it/s]Epoch: 2, train for the 106-th batch, train loss: 0.500027060508728:  70%|████████▍   | 106/151 [00:16<00:08,  5.49it/s]Epoch: 3, train for the 39-th batch, train loss: 0.44528868794441223:  32%|███▊        | 38/119 [00:06<00:10,  7.82it/s]Epoch: 3, train for the 39-th batch, train loss: 0.44528868794441223:  33%|███▉        | 39/119 [00:06<00:11,  6.76it/s]evaluate for the 63-th batch, evaluate loss: 0.5460489988327026:  86%|███████████████▌  | 62/72 [00:07<00:01,  9.08it/s]  0%|                                                                                            | 0/20 [00:00<?, ?it/s]Epoch: 1, train for the 215-th batch, train loss: 0.38756194710731506:  56%|█████▌    | 214/383 [00:58<00:46,  3.62it/s]evaluate for the 1-th batch, evaluate loss: 0.9078478813171387:   0%|                            | 0/20 [00:00<?, ?it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5038201212882996:  70%|███████▋   | 106/151 [00:17<00:08,  5.49it/s]Epoch: 1, train for the 215-th batch, train loss: 0.38756194710731506:  56%|█████▌    | 215/383 [00:58<00:45,  3.66it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5038201212882996:  71%|███████▊   | 107/151 [00:17<00:07,  5.76it/s]Epoch: 3, train for the 40-th batch, train loss: 0.3814537227153778:  33%|████▎        | 39/119 [00:06<00:11,  6.76it/s]Epoch: 3, train for the 40-th batch, train loss: 0.3814537227153778:  34%|████▎        | 40/119 [00:06<00:11,  6.64it/s]evaluate for the 2-th batch, evaluate loss: 0.8349537253379822:   0%|                            | 0/20 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.8349537253379822:  10%|██                  | 2/20 [00:00<00:01, 16.85it/s]evaluate for the 3-th batch, evaluate loss: 0.7097868323326111:  10%|██                  | 2/20 [00:00<00:01, 16.85it/s]Epoch: 2, train for the 108-th batch, train loss: 0.5255445241928101:  71%|███████▊   | 107/151 [00:17<00:07,  5.76it/s]evaluate for the 64-th batch, evaluate loss: 0.5702247023582458:  86%|███████████████▌  | 62/72 [00:07<00:01,  9.08it/s]evaluate for the 64-th batch, evaluate loss: 0.5702247023582458:  89%|████████████████  | 64/72 [00:07<00:00,  8.09it/s]Epoch: 2, train for the 108-th batch, train loss: 0.5255445241928101:  72%|███████▊   | 108/151 [00:17<00:07,  6.06it/s]Epoch: 3, train for the 41-th batch, train loss: 0.4173533618450165:  34%|████▎        | 40/119 [00:06<00:11,  6.64it/s]Epoch: 3, train for the 41-th batch, train loss: 0.4173533618450165:  34%|████▍        | 41/119 [00:06<00:12,  6.22it/s]evaluate for the 4-th batch, evaluate loss: 0.6947596669197083:  10%|██                  | 2/20 [00:00<00:01, 16.85it/s]evaluate for the 4-th batch, evaluate loss: 0.6947596669197083:  20%|████                | 4/20 [00:00<00:01, 12.77it/s]evaluate for the 65-th batch, evaluate loss: 0.5627862811088562:  89%|████████████████  | 64/72 [00:08<00:00,  8.09it/s]evaluate for the 5-th batch, evaluate loss: 0.7415711283683777:  20%|████                | 4/20 [00:00<00:01, 12.77it/s]Epoch: 1, train for the 216-th batch, train loss: 0.49236175417900085:  56%|█████▌    | 215/383 [00:58<00:45,  3.66it/s]Epoch: 1, train for the 223-th batch, train loss: 0.5009123682975769:  94%|██████████▎| 222/237 [01:01<00:05,  2.92it/s]Epoch: 2, train for the 109-th batch, train loss: 0.48326385021209717:  72%|███████▏  | 108/151 [00:17<00:07,  6.06it/s]Epoch: 2, train for the 109-th batch, train loss: 0.48326385021209717:  72%|███████▏  | 109/151 [00:17<00:07,  5.93it/s]Epoch: 1, train for the 223-th batch, train loss: 0.5009123682975769:  94%|██████████▎| 223/237 [01:01<00:05,  2.45it/s]Epoch: 1, train for the 216-th batch, train loss: 0.49236175417900085:  56%|█████▋    | 216/383 [00:59<00:49,  3.38it/s]evaluate for the 6-th batch, evaluate loss: 0.7645565867424011:  20%|████                | 4/20 [00:00<00:01, 12.77it/s]evaluate for the 6-th batch, evaluate loss: 0.7645565867424011:  30%|██████              | 6/20 [00:00<00:01, 12.89it/s]Epoch: 3, train for the 42-th batch, train loss: 0.4715782403945923:  34%|████▍        | 41/119 [00:06<00:12,  6.22it/s]Epoch: 3, train for the 42-th batch, train loss: 0.4715782403945923:  35%|████▌        | 42/119 [00:06<00:12,  5.95it/s]Epoch: 2, train for the 110-th batch, train loss: 0.5377293825149536:  72%|███████▉   | 109/151 [00:17<00:07,  5.93it/s]evaluate for the 66-th batch, evaluate loss: 0.4636150896549225:  89%|████████████████  | 64/72 [00:08<00:00,  8.09it/s]evaluate for the 66-th batch, evaluate loss: 0.4636150896549225:  92%|████████████████▌ | 66/72 [00:08<00:00,  7.54it/s]evaluate for the 7-th batch, evaluate loss: 0.7632616758346558:  30%|██████              | 6/20 [00:00<00:01, 12.89it/s]Epoch: 2, train for the 110-th batch, train loss: 0.5377293825149536:  73%|████████   | 110/151 [00:17<00:06,  6.21it/s]evaluate for the 67-th batch, evaluate loss: 0.5886638760566711:  92%|████████████████▌ | 66/72 [00:08<00:00,  7.54it/s]Epoch: 1, train for the 217-th batch, train loss: 0.46764764189720154:  56%|█████▋    | 216/383 [00:59<00:49,  3.38it/s]Epoch: 3, train for the 43-th batch, train loss: 0.42870283126831055:  35%|████▏       | 42/119 [00:06<00:12,  5.95it/s]Epoch: 3, train for the 43-th batch, train loss: 0.42870283126831055:  36%|████▎       | 43/119 [00:06<00:12,  6.01it/s]evaluate for the 8-th batch, evaluate loss: 0.7232329249382019:  30%|██████              | 6/20 [00:00<00:01, 12.89it/s]evaluate for the 8-th batch, evaluate loss: 0.7232329249382019:  40%|████████            | 8/20 [00:00<00:01, 11.78it/s]Epoch: 1, train for the 217-th batch, train loss: 0.46764764189720154:  57%|█████▋    | 217/383 [00:59<00:47,  3.52it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5169023275375366:  73%|████████   | 110/151 [00:17<00:06,  6.21it/s]evaluate for the 9-th batch, evaluate loss: 0.691614031791687:  40%|████████▍            | 8/20 [00:00<00:01, 11.78it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5169023275375366:  74%|████████   | 111/151 [00:17<00:06,  6.12it/s]Epoch: 3, train for the 44-th batch, train loss: 0.38193947076797485:  36%|████▎       | 43/119 [00:06<00:12,  6.01it/s]Epoch: 3, train for the 44-th batch, train loss: 0.38193947076797485:  37%|████▍       | 44/119 [00:06<00:12,  6.03it/s]evaluate for the 10-th batch, evaluate loss: 0.6714364290237427:  40%|███████▌           | 8/20 [00:00<00:01, 11.78it/s]evaluate for the 10-th batch, evaluate loss: 0.6714364290237427:  50%|█████████         | 10/20 [00:00<00:00, 11.85it/s]Epoch: 2, train for the 112-th batch, train loss: 0.46130651235580444:  74%|███████▎  | 111/151 [00:17<00:06,  6.12it/s]Epoch: 2, train for the 112-th batch, train loss: 0.46130651235580444:  74%|███████▍  | 112/151 [00:17<00:06,  6.43it/s]evaluate for the 11-th batch, evaluate loss: 0.6867905259132385:  50%|█████████         | 10/20 [00:00<00:00, 11.85it/s]evaluate for the 68-th batch, evaluate loss: 0.5422289967536926:  92%|████████████████▌ | 66/72 [00:08<00:00,  7.54it/s]evaluate for the 68-th batch, evaluate loss: 0.5422289967536926:  94%|█████████████████ | 68/72 [00:08<00:00,  6.72it/s]Epoch: 1, train for the 218-th batch, train loss: 0.5085429549217224:  57%|██████▏    | 217/383 [00:59<00:47,  3.52it/s]Epoch: 3, train for the 45-th batch, train loss: 0.41269737482070923:  37%|████▍       | 44/119 [00:07<00:12,  6.03it/s]Epoch: 3, train for the 45-th batch, train loss: 0.41269737482070923:  38%|████▌       | 45/119 [00:07<00:11,  6.30it/s]Epoch: 1, train for the 224-th batch, train loss: 0.54541015625:  94%|███████████████ | 223/237 [01:01<00:05,  2.45it/s]evaluate for the 69-th batch, evaluate loss: 0.578353762626648:  94%|█████████████████▉ | 68/72 [00:08<00:00,  6.72it/s]evaluate for the 12-th batch, evaluate loss: 0.723419189453125:  50%|█████████▌         | 10/20 [00:00<00:00, 11.85it/s]evaluate for the 12-th batch, evaluate loss: 0.723419189453125:  60%|███████████▍       | 12/20 [00:00<00:00, 12.07it/s]Epoch: 1, train for the 218-th batch, train loss: 0.5085429549217224:  57%|██████▎    | 218/383 [00:59<00:47,  3.48it/s]Epoch: 1, train for the 224-th batch, train loss: 0.54541015625:  95%|███████████████ | 224/237 [01:01<00:06,  2.17it/s]Epoch: 2, train for the 113-th batch, train loss: 0.5151013731956482:  74%|████████▏  | 112/151 [00:18<00:06,  6.43it/s]evaluate for the 70-th batch, evaluate loss: 0.5958045721054077:  94%|█████████████████ | 68/72 [00:08<00:00,  6.72it/s]evaluate for the 70-th batch, evaluate loss: 0.5958045721054077:  97%|█████████████████▌| 70/72 [00:08<00:00,  7.98it/s]Epoch: 2, train for the 113-th batch, train loss: 0.5151013731956482:  75%|████████▏  | 113/151 [00:18<00:06,  5.91it/s]evaluate for the 13-th batch, evaluate loss: 0.7359635829925537:  60%|██████████▊       | 12/20 [00:01<00:00, 12.07it/s]Epoch: 3, train for the 46-th batch, train loss: 0.4214979410171509:  38%|████▉        | 45/119 [00:07<00:11,  6.30it/s]evaluate for the 71-th batch, evaluate loss: 0.6110037565231323:  97%|█████████████████▌| 70/72 [00:08<00:00,  7.98it/s]Epoch: 3, train for the 46-th batch, train loss: 0.4214979410171509:  39%|█████        | 46/119 [00:07<00:11,  6.18it/s]evaluate for the 14-th batch, evaluate loss: 0.7210598587989807:  60%|██████████▊       | 12/20 [00:01<00:00, 12.07it/s]evaluate for the 14-th batch, evaluate loss: 0.7210598587989807:  70%|████████████▌     | 14/20 [00:01<00:00, 11.24it/s]evaluate for the 72-th batch, evaluate loss: 0.6381280422210693:  97%|█████████████████▌| 70/72 [00:08<00:00,  7.98it/s]evaluate for the 72-th batch, evaluate loss: 0.6381280422210693: 100%|██████████████████| 72/72 [00:08<00:00,  9.29it/s]evaluate for the 72-th batch, evaluate loss: 0.6381280422210693: 100%|██████████████████| 72/72 [00:08<00:00,  8.08it/s]
Epoch: 2, train for the 114-th batch, train loss: 0.513083279132843:  75%|████████▉   | 113/151 [00:18<00:06,  5.91it/s]evaluate for the 15-th batch, evaluate loss: 0.7398456931114197:  70%|████████████▌     | 14/20 [00:01<00:00, 11.24it/s]Epoch: 1, train for the 219-th batch, train loss: 0.33688658475875854:  57%|█████▋    | 218/383 [00:59<00:47,  3.48it/s]Epoch: 2, train for the 114-th batch, train loss: 0.513083279132843:  75%|█████████   | 114/151 [00:18<00:06,  5.59it/s]Epoch: 1, train for the 225-th batch, train loss: 0.5570690631866455:  95%|██████████▍| 224/237 [01:02<00:06,  2.17it/s]Epoch: 3, train for the 47-th batch, train loss: 0.5076256394386292:  39%|█████        | 46/119 [00:07<00:11,  6.18it/s]Epoch: 1, train for the 219-th batch, train loss: 0.33688658475875854:  57%|█████▋    | 219/383 [00:59<00:48,  3.39it/s]Epoch: 3, train for the 47-th batch, train loss: 0.5076256394386292:  39%|█████▏       | 47/119 [00:07<00:11,  6.11it/s]evaluate for the 16-th batch, evaluate loss: 0.6757680177688599:  70%|████████████▌     | 14/20 [00:01<00:00, 11.24it/s]evaluate for the 16-th batch, evaluate loss: 0.6757680177688599:  80%|██████████████▍   | 16/20 [00:01<00:00, 12.58it/s]Epoch: 1, train for the 225-th batch, train loss: 0.5570690631866455:  95%|██████████▍| 225/237 [01:02<00:05,  2.39it/s]evaluate for the 17-th batch, evaluate loss: 0.7265551686286926:  80%|██████████████▍   | 16/20 [00:01<00:00, 12.58it/s]Epoch: 2, train for the 115-th batch, train loss: 0.4588851034641266:  75%|████████▎  | 114/151 [00:18<00:06,  5.59it/s]Epoch: 2, train for the 115-th batch, train loss: 0.4588851034641266:  76%|████████▍  | 115/151 [00:18<00:06,  5.98it/s]Epoch: 3, train for the 48-th batch, train loss: 0.47221407294273376:  39%|████▋       | 47/119 [00:07<00:11,  6.11it/s]Epoch: 3, train for the 48-th batch, train loss: 0.47221407294273376:  40%|████▊       | 48/119 [00:07<00:11,  6.16it/s]evaluate for the 18-th batch, evaluate loss: 0.7158182859420776:  80%|██████████████▍   | 16/20 [00:01<00:00, 12.58it/s]evaluate for the 18-th batch, evaluate loss: 0.7158182859420776:  90%|████████████████▏ | 18/20 [00:01<00:00, 12.56it/s]Epoch: 1, train for the 226-th batch, train loss: 0.5610702633857727:  95%|██████████▍| 225/237 [01:02<00:05,  2.39it/s]evaluate for the 19-th batch, evaluate loss: 0.7696574926376343:  90%|████████████████▏ | 18/20 [00:01<00:00, 12.56it/s]Epoch: 1, train for the 220-th batch, train loss: 0.419828861951828:  57%|██████▊     | 219/383 [01:00<00:48,  3.39it/s]Epoch: 1, train for the 226-th batch, train loss: 0.5610702633857727:  95%|██████████▍| 226/237 [01:02<00:03,  2.76it/s]Epoch: 1, train for the 220-th batch, train loss: 0.419828861951828:  57%|██████▉     | 220/383 [01:00<00:48,  3.38it/s]evaluate for the 20-th batch, evaluate loss: 0.6966159343719482:  90%|████████████████▏ | 18/20 [00:01<00:00, 12.56it/s]evaluate for the 20-th batch, evaluate loss: 0.6966159343719482: 100%|██████████████████| 20/20 [00:01<00:00, 13.26it/s]evaluate for the 20-th batch, evaluate loss: 0.6966159343719482: 100%|██████████████████| 20/20 [00:01<00:00, 12.60it/s]
Epoch: 2, train for the 116-th batch, train loss: 0.4441952407360077:  76%|████████▍  | 115/151 [00:18<00:06,  5.98it/s]Epoch: 3, train for the 49-th batch, train loss: 0.43320322036743164:  40%|████▊       | 48/119 [00:07<00:11,  6.16it/s]Epoch: 2, train for the 116-th batch, train loss: 0.4441952407360077:  77%|████████▍  | 116/151 [00:18<00:06,  5.45it/s]Epoch: 3, train for the 49-th batch, train loss: 0.43320322036743164:  41%|████▉       | 49/119 [00:07<00:11,  6.00it/s]Epoch: 1, train for the 227-th batch, train loss: 0.5119531750679016:  95%|██████████▍| 226/237 [01:02<00:03,  2.76it/s]Epoch: 3, train for the 50-th batch, train loss: 0.4509502351284027:  41%|█████▎       | 49/119 [00:07<00:11,  6.00it/s]Epoch: 3, train for the 50-th batch, train loss: 0.4509502351284027:  42%|█████▍       | 50/119 [00:07<00:10,  6.56it/s]Epoch: 2, train for the 117-th batch, train loss: 0.4748777151107788:  77%|████████▍  | 116/151 [00:18<00:06,  5.45it/s]Epoch: 1, train for the 227-th batch, train loss: 0.5119531750679016:  96%|██████████▌| 227/237 [01:02<00:03,  3.15it/s]Epoch: 1, train for the 221-th batch, train loss: 0.4406348466873169:  57%|██████▎    | 220/383 [01:00<00:48,  3.38it/s]Epoch: 2, train for the 117-th batch, train loss: 0.4748777151107788:  77%|████████▌  | 117/151 [00:18<00:06,  5.64it/s]Epoch: 1, train for the 221-th batch, train loss: 0.4406348466873169:  58%|██████▎    | 221/383 [01:00<00:45,  3.57it/s]INFO:root:Epoch: 2, learning rate: 0.0001, train loss: 0.5219
INFO:root:train average_precision, 0.8372
INFO:root:train roc_auc, 0.8306
INFO:root:validate loss: 0.5270
INFO:root:validate average_precision, 0.8285
INFO:root:validate roc_auc, 0.8162
INFO:root:new node validate loss: 0.7347
INFO:root:new node validate first_1_average_precision, 0.5300
INFO:root:new node validate first_1_roc_auc, 0.4413
INFO:root:new node validate first_3_average_precision, 0.5571
INFO:root:new node validate first_3_roc_auc, 0.5071
INFO:root:new node validate first_10_average_precision, 0.5720
INFO:root:new node validate first_10_roc_auc, 0.5585
INFO:root:new node validate average_precision, 0.6320
INFO:root:new node validate roc_auc, 0.6362
INFO:root:save model ./saved_models/TGN/ia-escorts-dynamic/TGN_seed0_tgn-ia-escorts-dynamic-reparamcorr-time-mlp/TGN_seed0_tgn-ia-escorts-dynamic-reparamcorr-time-mlp.pkl
Epoch: 3, train for the 51-th batch, train loss: 0.46905797719955444:  42%|█████       | 50/119 [00:07<00:10,  6.56it/s]Epoch: 3, train for the 51-th batch, train loss: 0.46905797719955444:  43%|█████▏      | 51/119 [00:07<00:09,  6.88it/s]  0%|                                                                                            | 0/34 [00:00<?, ?it/s]Epoch: 2, train for the 118-th batch, train loss: 0.44178977608680725:  77%|███████▋  | 117/151 [00:18<00:06,  5.64it/s]Epoch: 2, train for the 118-th batch, train loss: 0.44178977608680725:  78%|███████▊  | 118/151 [00:18<00:05,  5.59it/s]evaluate for the 1-th batch, evaluate loss: 0.7082000374794006:   0%|                            | 0/34 [00:00<?, ?it/s]Epoch: 3, train for the 52-th batch, train loss: 0.47904741764068604:  43%|█████▏      | 51/119 [00:08<00:09,  6.88it/s]Epoch: 3, train for the 52-th batch, train loss: 0.47904741764068604:  44%|█████▏      | 52/119 [00:08<00:08,  7.59it/s]evaluate for the 2-th batch, evaluate loss: 0.8303043246269226:   0%|                            | 0/34 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.8303043246269226:   6%|█▏                  | 2/34 [00:00<00:02, 11.49it/s]Epoch: 1, train for the 228-th batch, train loss: 0.5096156001091003:  96%|██████████▌| 227/237 [01:02<00:03,  3.15it/s]Epoch: 3, train for the 53-th batch, train loss: 0.4168577492237091:  44%|█████▋       | 52/119 [00:08<00:08,  7.59it/s]Epoch: 1, train for the 222-th batch, train loss: 0.4786255955696106:  58%|██████▎    | 221/383 [01:00<00:45,  3.57it/s]Epoch: 3, train for the 53-th batch, train loss: 0.4168577492237091:  45%|█████▊       | 53/119 [00:08<00:08,  8.04it/s]Epoch: 1, train for the 228-th batch, train loss: 0.5096156001091003:  96%|██████████▌| 228/237 [01:02<00:02,  3.10it/s]Epoch: 1, train for the 222-th batch, train loss: 0.4786255955696106:  58%|██████▍    | 222/383 [01:00<00:46,  3.49it/s]evaluate for the 3-th batch, evaluate loss: 0.8335882425308228:   6%|█▏                  | 2/34 [00:00<00:02, 11.49it/s]Epoch: 2, train for the 119-th batch, train loss: 0.5001128911972046:  78%|████████▌  | 118/151 [00:19<00:05,  5.59it/s]Epoch: 2, train for the 119-th batch, train loss: 0.5001128911972046:  79%|████████▋  | 119/151 [00:19<00:05,  5.39it/s]Epoch: 3, train for the 54-th batch, train loss: 0.4030139744281769:  45%|█████▊       | 53/119 [00:08<00:08,  8.04it/s]Epoch: 3, train for the 54-th batch, train loss: 0.4030139744281769:  45%|█████▉       | 54/119 [00:08<00:07,  8.47it/s]evaluate for the 4-th batch, evaluate loss: 0.7983328104019165:   6%|█▏                  | 2/34 [00:00<00:02, 11.49it/s]evaluate for the 4-th batch, evaluate loss: 0.7983328104019165:  12%|██▎                 | 4/34 [00:00<00:02, 11.26it/s]Epoch: 1, train for the 229-th batch, train loss: 0.5904751420021057:  96%|██████████▌| 228/237 [01:03<00:02,  3.10it/s]Epoch: 1, train for the 229-th batch, train loss: 0.5904751420021057:  97%|██████████▋| 229/237 [01:03<00:02,  3.43it/s]evaluate for the 5-th batch, evaluate loss: 0.7814415097236633:  12%|██▎                 | 4/34 [00:00<00:02, 11.26it/s]Epoch: 3, train for the 55-th batch, train loss: 0.42061078548431396:  45%|█████▍      | 54/119 [00:08<00:07,  8.47it/s]Epoch: 1, train for the 223-th batch, train loss: 0.4363614618778229:  58%|██████▍    | 222/383 [01:00<00:46,  3.49it/s]Epoch: 3, train for the 55-th batch, train loss: 0.42061078548431396:  46%|█████▌      | 55/119 [00:08<00:08,  7.80it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5248830914497375:  79%|████████▋  | 119/151 [00:19<00:05,  5.39it/s]Epoch: 1, train for the 223-th batch, train loss: 0.4363614618778229:  58%|██████▍    | 223/383 [01:00<00:43,  3.67it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5248830914497375:  79%|████████▋  | 120/151 [00:19<00:06,  5.11it/s]evaluate for the 6-th batch, evaluate loss: 0.8466933965682983:  12%|██▎                 | 4/34 [00:00<00:02, 11.26it/s]evaluate for the 6-th batch, evaluate loss: 0.8466933965682983:  18%|███▌                | 6/34 [00:00<00:02, 12.20it/s]Epoch: 3, train for the 56-th batch, train loss: 0.42918819189071655:  46%|█████▌      | 55/119 [00:08<00:08,  7.80it/s]Epoch: 3, train for the 56-th batch, train loss: 0.42918819189071655:  47%|█████▋      | 56/119 [00:08<00:07,  8.05it/s]evaluate for the 7-th batch, evaluate loss: 0.9142081141471863:  18%|███▌                | 6/34 [00:00<00:02, 12.20it/s]Epoch: 3, train for the 57-th batch, train loss: 0.39120832085609436:  47%|█████▋      | 56/119 [00:08<00:07,  8.05it/s]Epoch: 3, train for the 57-th batch, train loss: 0.39120832085609436:  48%|█████▋      | 57/119 [00:08<00:07,  8.00it/s]Epoch: 2, train for the 121-th batch, train loss: 0.5010273456573486:  79%|████████▋  | 120/151 [00:19<00:06,  5.11it/s]evaluate for the 8-th batch, evaluate loss: 0.9612569808959961:  18%|███▌                | 6/34 [00:00<00:02, 12.20it/s]evaluate for the 8-th batch, evaluate loss: 0.9612569808959961:  24%|████▋               | 8/34 [00:00<00:02, 11.17it/s]Epoch: 2, train for the 121-th batch, train loss: 0.5010273456573486:  80%|████████▊  | 121/151 [00:19<00:06,  4.93it/s]Epoch: 1, train for the 230-th batch, train loss: 0.5563667416572571:  97%|██████████▋| 229/237 [01:03<00:02,  3.43it/s]Epoch: 1, train for the 224-th batch, train loss: 0.528644323348999:  58%|██████▉     | 223/383 [01:01<00:43,  3.67it/s]evaluate for the 9-th batch, evaluate loss: 0.8262414336204529:  24%|████▋               | 8/34 [00:00<00:02, 11.17it/s]Epoch: 3, train for the 58-th batch, train loss: 0.4148181080818176:  48%|██████▏      | 57/119 [00:08<00:07,  8.00it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]Epoch: 1, train for the 230-th batch, train loss: 0.5563667416572571:  97%|██████████▋| 230/237 [01:03<00:02,  3.22it/s]Epoch: 1, train for the 224-th batch, train loss: 0.528644323348999:  58%|███████     | 224/383 [01:01<00:45,  3.48it/s]evaluate for the 10-th batch, evaluate loss: 0.751672625541687:  24%|████▋               | 8/34 [00:00<00:02, 11.17it/s]evaluate for the 10-th batch, evaluate loss: 0.751672625541687:  29%|█████▌             | 10/34 [00:00<00:02, 11.73it/s]Epoch: 2, train for the 122-th batch, train loss: 0.4595712125301361:  80%|████████▊  | 121/151 [00:19<00:06,  4.93it/s]Epoch: 2, train for the 122-th batch, train loss: 0.4595712125301361:  81%|████████▉  | 122/151 [00:19<00:05,  5.13it/s]Epoch: 3, train for the 1-th batch, train loss: 1.0193700790405273:   0%|                       | 0/146 [00:00<?, ?it/s]Epoch: 3, train for the 1-th batch, train loss: 1.0193700790405273:   1%|               | 1/146 [00:00<00:22,  6.53it/s]evaluate for the 11-th batch, evaluate loss: 0.6811009049415588:  29%|█████▎            | 10/34 [00:00<00:02, 11.73it/s]Epoch: 3, train for the 59-th batch, train loss: 0.41807806491851807:  48%|█████▋      | 57/119 [00:08<00:07,  8.00it/s]Epoch: 3, train for the 59-th batch, train loss: 0.41807806491851807:  50%|█████▉      | 59/119 [00:08<00:07,  7.67it/s]evaluate for the 12-th batch, evaluate loss: 0.7691858410835266:  29%|█████▎            | 10/34 [00:01<00:02, 11.73it/s]evaluate for the 12-th batch, evaluate loss: 0.7691858410835266:  35%|██████▎           | 12/34 [00:01<00:01, 11.83it/s]Epoch: 1, train for the 231-th batch, train loss: 0.5262534618377686:  97%|██████████▋| 230/237 [01:03<00:02,  3.22it/s]Epoch: 2, train for the 123-th batch, train loss: 0.49204298853874207:  81%|████████  | 122/151 [00:19<00:05,  5.13it/s]Epoch: 3, train for the 2-th batch, train loss: 0.8230319023132324:   1%|               | 1/146 [00:00<00:22,  6.53it/s]Epoch: 3, train for the 2-th batch, train loss: 0.8230319023132324:   1%|▏              | 2/146 [00:00<00:20,  7.17it/s]evaluate for the 13-th batch, evaluate loss: 0.6087675094604492:  35%|██████▎           | 12/34 [00:01<00:01, 11.83it/s]Epoch: 2, train for the 123-th batch, train loss: 0.49204298853874207:  81%|████████▏ | 123/151 [00:19<00:05,  5.13it/s]Epoch: 1, train for the 231-th batch, train loss: 0.5262534618377686:  97%|██████████▋| 231/237 [01:03<00:01,  3.24it/s]Epoch: 3, train for the 60-th batch, train loss: 0.39522895216941833:  50%|█████▉      | 59/119 [00:09<00:07,  7.67it/s]Epoch: 3, train for the 60-th batch, train loss: 0.39522895216941833:  50%|██████      | 60/119 [00:09<00:08,  7.09it/s]Epoch: 1, train for the 225-th batch, train loss: 0.4293719530105591:  58%|██████▍    | 224/383 [01:01<00:45,  3.48it/s]evaluate for the 14-th batch, evaluate loss: 0.66522216796875:  35%|███████             | 12/34 [00:01<00:01, 11.83it/s]evaluate for the 14-th batch, evaluate loss: 0.66522216796875:  41%|████████▏           | 14/34 [00:01<00:01, 12.29it/s]Epoch: 3, train for the 3-th batch, train loss: 0.7512463331222534:   1%|▏              | 2/146 [00:00<00:20,  7.17it/s]Epoch: 3, train for the 3-th batch, train loss: 0.7512463331222534:   2%|▎              | 3/146 [00:00<00:17,  8.06it/s]Epoch: 1, train for the 225-th batch, train loss: 0.4293719530105591:  59%|██████▍    | 225/383 [01:01<00:50,  3.10it/s]Epoch: 2, train for the 124-th batch, train loss: 0.49877113103866577:  81%|████████▏ | 123/151 [00:20<00:05,  5.13it/s]evaluate for the 15-th batch, evaluate loss: 0.7278590202331543:  41%|███████▍          | 14/34 [00:01<00:01, 12.29it/s]Epoch: 1, train for the 232-th batch, train loss: 0.5423061847686768:  97%|██████████▋| 231/237 [01:03<00:01,  3.24it/s]Epoch: 2, train for the 124-th batch, train loss: 0.49877113103866577:  82%|████████▏ | 124/151 [00:20<00:05,  5.06it/s]Epoch: 3, train for the 61-th batch, train loss: 0.43826255202293396:  50%|██████      | 60/119 [00:09<00:08,  7.09it/s]Epoch: 3, train for the 61-th batch, train loss: 0.43826255202293396:  51%|██████▏     | 61/119 [00:09<00:08,  6.60it/s]Epoch: 1, train for the 232-th batch, train loss: 0.5423061847686768:  98%|██████████▊| 232/237 [01:03<00:01,  3.48it/s]Epoch: 3, train for the 4-th batch, train loss: 0.6963081955909729:   2%|▎              | 3/146 [00:00<00:17,  8.06it/s]Epoch: 3, train for the 4-th batch, train loss: 0.6963081955909729:   3%|▍              | 4/146 [00:00<00:20,  6.97it/s]evaluate for the 16-th batch, evaluate loss: 0.6667464375495911:  41%|███████▍          | 14/34 [00:01<00:01, 12.29it/s]evaluate for the 16-th batch, evaluate loss: 0.6667464375495911:  47%|████████▍         | 16/34 [00:01<00:01, 11.65it/s]Epoch: 1, train for the 226-th batch, train loss: 0.5066120028495789:  59%|██████▍    | 225/383 [01:01<00:50,  3.10it/s]Epoch: 1, train for the 226-th batch, train loss: 0.5066120028495789:  59%|██████▍    | 226/383 [01:01<00:47,  3.30it/s]evaluate for the 17-th batch, evaluate loss: 0.6698316931724548:  47%|████████▍         | 16/34 [00:01<00:01, 11.65it/s]Epoch: 2, train for the 125-th batch, train loss: 0.48507189750671387:  82%|████████▏ | 124/151 [00:20<00:05,  5.06it/s]Epoch: 3, train for the 5-th batch, train loss: 0.668426513671875:   3%|▍               | 4/146 [00:00<00:20,  6.97it/s]Epoch: 3, train for the 5-th batch, train loss: 0.668426513671875:   3%|▌               | 5/146 [00:00<00:21,  6.66it/s]Epoch: 2, train for the 125-th batch, train loss: 0.48507189750671387:  83%|████████▎ | 125/151 [00:20<00:05,  4.85it/s]evaluate for the 18-th batch, evaluate loss: 0.6292204260826111:  47%|████████▍         | 16/34 [00:01<00:01, 11.65it/s]evaluate for the 18-th batch, evaluate loss: 0.6292204260826111:  53%|█████████▌        | 18/34 [00:01<00:01, 11.30it/s]Epoch: 1, train for the 233-th batch, train loss: 0.5647660493850708:  98%|██████████▊| 232/237 [01:04<00:01,  3.48it/s]Epoch: 1, train for the 233-th batch, train loss: 0.5647660493850708:  98%|██████████▊| 233/237 [01:04<00:01,  3.53it/s]evaluate for the 19-th batch, evaluate loss: 0.646155834197998:  53%|██████████         | 18/34 [00:01<00:01, 11.30it/s]Epoch: 3, train for the 6-th batch, train loss: 0.6316618323326111:   3%|▌              | 5/146 [00:00<00:21,  6.66it/s]Epoch: 3, train for the 6-th batch, train loss: 0.6316618323326111:   4%|▌              | 6/146 [00:00<00:18,  7.39it/s]Epoch: 2, train for the 126-th batch, train loss: 0.464080810546875:  83%|█████████▉  | 125/151 [00:20<00:05,  4.85it/s]Epoch: 1, train for the 227-th batch, train loss: 0.5621896386146545:  59%|██████▍    | 226/383 [01:02<00:47,  3.30it/s]evaluate for the 20-th batch, evaluate loss: 0.7052193880081177:  53%|█████████▌        | 18/34 [00:01<00:01, 11.30it/s]evaluate for the 20-th batch, evaluate loss: 0.7052193880081177:  59%|██████████▌       | 20/34 [00:01<00:01, 11.98it/s]Epoch: 2, train for the 126-th batch, train loss: 0.464080810546875:  83%|██████████  | 126/151 [00:20<00:04,  5.04it/s]Epoch: 1, train for the 227-th batch, train loss: 0.5621896386146545:  59%|██████▌    | 227/383 [01:02<00:45,  3.44it/s]Epoch: 1, train for the 234-th batch, train loss: 0.5129302740097046:  98%|██████████▊| 233/237 [01:04<00:01,  3.53it/s]evaluate for the 21-th batch, evaluate loss: 0.6910144090652466:  59%|██████████▌       | 20/34 [00:01<00:01, 11.98it/s]Epoch: 1, train for the 234-th batch, train loss: 0.5129302740097046:  99%|██████████▊| 234/237 [01:04<00:00,  3.89it/s]Epoch: 3, train for the 7-th batch, train loss: 0.573734700679779:   4%|▋               | 6/146 [00:01<00:18,  7.39it/s]Epoch: 3, train for the 7-th batch, train loss: 0.573734700679779:   5%|▊               | 7/146 [00:01<00:23,  5.89it/s]evaluate for the 22-th batch, evaluate loss: 0.3845794200897217:  59%|██████████▌       | 20/34 [00:01<00:01, 11.98it/s]evaluate for the 22-th batch, evaluate loss: 0.3845794200897217:  65%|███████████▋      | 22/34 [00:01<00:01, 11.90it/s]Epoch: 2, train for the 127-th batch, train loss: 0.46835586428642273:  83%|████████▎ | 126/151 [00:20<00:04,  5.04it/s]Epoch: 3, train for the 62-th batch, train loss: 0.4996230900287628:  51%|██████▋      | 61/119 [00:09<00:08,  6.60it/s]Epoch: 2, train for the 127-th batch, train loss: 0.46835586428642273:  84%|████████▍ | 127/151 [00:20<00:04,  5.04it/s]Epoch: 3, train for the 62-th batch, train loss: 0.4996230900287628:  52%|██████▊      | 62/119 [00:09<00:15,  3.71it/s]evaluate for the 23-th batch, evaluate loss: 0.42876532673835754:  65%|███████████      | 22/34 [00:01<00:01, 11.90it/s]Epoch: 1, train for the 228-th batch, train loss: 0.44020628929138184:  59%|█████▉    | 227/383 [01:02<00:45,  3.44it/s]Epoch: 1, train for the 235-th batch, train loss: 0.4841558635234833:  99%|██████████▊| 234/237 [01:04<00:00,  3.89it/s]Epoch: 3, train for the 8-th batch, train loss: 0.5213152766227722:   5%|▋              | 7/146 [00:01<00:23,  5.89it/s]Epoch: 3, train for the 8-th batch, train loss: 0.5213152766227722:   5%|▊              | 8/146 [00:01<00:22,  6.03it/s]Epoch: 1, train for the 228-th batch, train loss: 0.44020628929138184:  60%|█████▉    | 228/383 [01:02<00:45,  3.41it/s]Epoch: 1, train for the 235-th batch, train loss: 0.4841558635234833:  99%|██████████▉| 235/237 [01:04<00:00,  3.96it/s]evaluate for the 24-th batch, evaluate loss: 0.5302456617355347:  65%|███████████▋      | 22/34 [00:02<00:01, 11.90it/s]evaluate for the 24-th batch, evaluate loss: 0.5302456617355347:  71%|████████████▋     | 24/34 [00:02<00:00, 11.66it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5190539956092834:  84%|█████████▎ | 127/151 [00:20<00:04,  5.04it/s]Epoch: 3, train for the 63-th batch, train loss: 0.45141714811325073:  52%|██████▎     | 62/119 [00:10<00:15,  3.71it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5190539956092834:  85%|█████████▎ | 128/151 [00:20<00:04,  5.12it/s]Epoch: 3, train for the 63-th batch, train loss: 0.45141714811325073:  53%|██████▎     | 63/119 [00:10<00:13,  4.09it/s]evaluate for the 25-th batch, evaluate loss: 0.5482616424560547:  71%|████████████▋     | 24/34 [00:02<00:00, 11.66it/s]Epoch: 3, train for the 9-th batch, train loss: 0.4913640022277832:   5%|▊              | 8/146 [00:01<00:22,  6.03it/s]Epoch: 3, train for the 9-th batch, train loss: 0.4913640022277832:   6%|▉              | 9/146 [00:01<00:21,  6.26it/s]evaluate for the 26-th batch, evaluate loss: 0.6511359810829163:  71%|████████████▋     | 24/34 [00:02<00:00, 11.66it/s]evaluate for the 26-th batch, evaluate loss: 0.6511359810829163:  76%|█████████████▊    | 26/34 [00:02<00:00, 11.47it/s]Epoch: 3, train for the 64-th batch, train loss: 0.46867072582244873:  53%|██████▎     | 63/119 [00:10<00:13,  4.09it/s]Epoch: 3, train for the 64-th batch, train loss: 0.46867072582244873:  54%|██████▍     | 64/119 [00:10<00:12,  4.52it/s]Epoch: 1, train for the 229-th batch, train loss: 0.3216458857059479:  60%|██████▌    | 228/383 [01:02<00:45,  3.41it/s]Epoch: 3, train for the 10-th batch, train loss: 0.491915225982666:   6%|▉              | 9/146 [00:01<00:21,  6.26it/s]Epoch: 2, train for the 129-th batch, train loss: 0.537652850151062:  85%|██████████▏ | 128/151 [00:21<00:04,  5.12it/s]Epoch: 3, train for the 10-th batch, train loss: 0.491915225982666:   7%|▉             | 10/146 [00:01<00:20,  6.58it/s]Epoch: 1, train for the 236-th batch, train loss: 0.5111653208732605:  99%|██████████▉| 235/237 [01:04<00:00,  3.96it/s]evaluate for the 27-th batch, evaluate loss: 0.6814616322517395:  76%|█████████████▊    | 26/34 [00:02<00:00, 11.47it/s]Epoch: 1, train for the 229-th batch, train loss: 0.3216458857059479:  60%|██████▌    | 229/383 [01:02<00:44,  3.43it/s]Epoch: 2, train for the 129-th batch, train loss: 0.537652850151062:  85%|██████████▎ | 129/151 [00:21<00:04,  4.73it/s]Epoch: 1, train for the 236-th batch, train loss: 0.5111653208732605: 100%|██████████▉| 236/237 [01:04<00:00,  3.76it/s]evaluate for the 28-th batch, evaluate loss: 0.5306548476219177:  76%|█████████████▊    | 26/34 [00:02<00:00, 11.47it/s]evaluate for the 28-th batch, evaluate loss: 0.5306548476219177:  82%|██████████████▊   | 28/34 [00:02<00:00, 11.62it/s]Epoch: 3, train for the 65-th batch, train loss: 0.455483078956604:  54%|███████▌      | 64/119 [00:10<00:12,  4.52it/s]Epoch: 3, train for the 11-th batch, train loss: 0.4858109951019287:   7%|▉            | 10/146 [00:01<00:20,  6.58it/s]Epoch: 3, train for the 11-th batch, train loss: 0.4858109951019287:   8%|▉            | 11/146 [00:01<00:20,  6.55it/s]Epoch: 3, train for the 65-th batch, train loss: 0.455483078956604:  55%|███████▋      | 65/119 [00:10<00:11,  4.58it/s]evaluate for the 29-th batch, evaluate loss: 0.5022377967834473:  82%|██████████████▊   | 28/34 [00:02<00:00, 11.62it/s]Epoch: 2, train for the 130-th batch, train loss: 0.4726189374923706:  85%|█████████▍ | 129/151 [00:21<00:04,  4.73it/s]Epoch: 2, train for the 130-th batch, train loss: 0.4726189374923706:  86%|█████████▍ | 130/151 [00:21<00:04,  4.83it/s]Epoch: 1, train for the 237-th batch, train loss: 0.47120359539985657: 100%|█████████▉| 236/237 [01:05<00:00,  3.76it/s]evaluate for the 30-th batch, evaluate loss: 0.5098666548728943:  82%|██████████████▊   | 28/34 [00:02<00:00, 11.62it/s]evaluate for the 30-th batch, evaluate loss: 0.5098666548728943:  88%|███████████████▉  | 30/34 [00:02<00:00, 11.35it/s]Epoch: 1, train for the 230-th batch, train loss: 0.4868715703487396:  60%|██████▌    | 229/383 [01:03<00:44,  3.43it/s]Epoch: 1, train for the 237-th batch, train loss: 0.47120359539985657: 100%|██████████| 237/237 [01:05<00:00,  3.84it/s]Epoch: 1, train for the 237-th batch, train loss: 0.47120359539985657: 100%|██████████| 237/237 [01:05<00:00,  3.63it/s]
Epoch: 3, train for the 12-th batch, train loss: 0.48521673679351807:   8%|▉           | 11/146 [00:01<00:20,  6.55it/s]Epoch: 3, train for the 12-th batch, train loss: 0.48521673679351807:   8%|▉           | 12/146 [00:01<00:20,  6.66it/s]Epoch: 3, train for the 66-th batch, train loss: 0.4118300676345825:  55%|███████      | 65/119 [00:10<00:11,  4.58it/s]Epoch: 3, train for the 66-th batch, train loss: 0.4118300676345825:  55%|███████▏     | 66/119 [00:10<00:10,  4.95it/s]Epoch: 1, train for the 230-th batch, train loss: 0.4868715703487396:  60%|██████▌    | 230/383 [01:03<00:46,  3.29it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5011025667190552:  86%|█████████▍ | 130/151 [00:21<00:04,  4.83it/s]evaluate for the 31-th batch, evaluate loss: 0.6061341762542725:  88%|███████████████▉  | 30/34 [00:02<00:00, 11.35it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5011025667190552:  87%|█████████▌ | 131/151 [00:21<00:03,  5.08it/s]Epoch: 3, train for the 13-th batch, train loss: 0.5087435245513916:   8%|█            | 12/146 [00:01<00:20,  6.66it/s]Epoch: 3, train for the 13-th batch, train loss: 0.5087435245513916:   9%|█▏           | 13/146 [00:01<00:19,  6.65it/s]evaluate for the 32-th batch, evaluate loss: 0.5264770984649658:  88%|███████████████▉  | 30/34 [00:02<00:00, 11.35it/s]evaluate for the 32-th batch, evaluate loss: 0.5264770984649658:  94%|████████████████▉ | 32/34 [00:02<00:00, 11.33it/s]Epoch: 3, train for the 67-th batch, train loss: 0.4819852411746979:  55%|███████▏     | 66/119 [00:10<00:10,  4.95it/s]Epoch: 3, train for the 67-th batch, train loss: 0.4819852411746979:  56%|███████▎     | 67/119 [00:10<00:09,  5.24it/s]evaluate for the 33-th batch, evaluate loss: 0.5634707808494568:  94%|████████████████▉ | 32/34 [00:02<00:00, 11.33it/s]Epoch: 2, train for the 132-th batch, train loss: 0.48365363478660583:  87%|████████▋ | 131/151 [00:21<00:03,  5.08it/s]Epoch: 3, train for the 14-th batch, train loss: 0.4645707607269287:   9%|█▏           | 13/146 [00:02<00:19,  6.65it/s]Epoch: 3, train for the 14-th batch, train loss: 0.4645707607269287:  10%|█▏           | 14/146 [00:02<00:19,  6.88it/s]Epoch: 2, train for the 132-th batch, train loss: 0.48365363478660583:  87%|████████▋ | 132/151 [00:21<00:03,  5.20it/s]evaluate for the 34-th batch, evaluate loss: 0.5492532849311829:  94%|████████████████▉ | 32/34 [00:02<00:00, 11.33it/s]evaluate for the 34-th batch, evaluate loss: 0.5492532849311829: 100%|██████████████████| 34/34 [00:02<00:00, 11.21it/s]evaluate for the 34-th batch, evaluate loss: 0.5492532849311829: 100%|██████████████████| 34/34 [00:02<00:00, 11.55it/s]
Epoch: 3, train for the 68-th batch, train loss: 0.3998255431652069:  56%|███████▎     | 67/119 [00:10<00:09,  5.24it/s]Epoch: 3, train for the 68-th batch, train loss: 0.3998255431652069:  57%|███████▍     | 68/119 [00:10<00:09,  5.45it/s]Epoch: 3, train for the 15-th batch, train loss: 0.5262762308120728:  10%|█▏           | 14/146 [00:02<00:19,  6.88it/s]Epoch: 3, train for the 15-th batch, train loss: 0.5262762308120728:  10%|█▎           | 15/146 [00:02<00:17,  7.43it/s]  0%|                                                                                            | 0/66 [00:00<?, ?it/s]Epoch: 1, train for the 231-th batch, train loss: 0.5425682067871094:  60%|██████▌    | 230/383 [01:03<00:46,  3.29it/s]Epoch: 2, train for the 133-th batch, train loss: 0.4353715479373932:  87%|█████████▌ | 132/151 [00:21<00:03,  5.20it/s]Epoch: 1, train for the 231-th batch, train loss: 0.5425682067871094:  60%|██████▋    | 231/383 [01:03<00:53,  2.84it/s]evaluate for the 1-th batch, evaluate loss: 0.6298624873161316:   0%|                            | 0/66 [00:00<?, ?it/s]Epoch: 2, train for the 133-th batch, train loss: 0.4353715479373932:  88%|█████████▋ | 133/151 [00:22<00:03,  4.80it/s]Epoch: 3, train for the 69-th batch, train loss: 0.4690113067626953:  57%|███████▍     | 68/119 [00:11<00:09,  5.45it/s]Epoch: 3, train for the 69-th batch, train loss: 0.4690113067626953:  58%|███████▌     | 69/119 [00:11<00:09,  5.32it/s]INFO:root:Epoch: 1, learning rate: 0.0001, train loss: 0.5090
INFO:root:train average_precision, 0.8530
INFO:root:train roc_auc, 0.8184
INFO:root:validate loss: 0.5817
INFO:root:validate average_precision, 0.7950
INFO:root:validate roc_auc, 0.7863
INFO:root:new node validate loss: 0.6684
INFO:root:new node validate first_1_average_precision, 0.6114
INFO:root:new node validate first_1_roc_auc, 0.6030
INFO:root:new node validate first_3_average_precision, 0.5985
INFO:root:new node validate first_3_roc_auc, 0.5449
INFO:root:new node validate first_10_average_precision, 0.6444
INFO:root:new node validate first_10_roc_auc, 0.5905
INFO:root:new node validate average_precision, 0.7645
INFO:root:new node validate roc_auc, 0.7241
INFO:root:save model ./saved_models/TGN/ia-movielens-user2tags-10m/TGN_seed0_tgn-ia-movielens-user2tags-10m-reparamcorr-time-mlp/TGN_seed0_tgn-ia-movielens-user2tags-10m-reparamcorr-time-mlp.pkl
Epoch: 3, train for the 16-th batch, train loss: 0.5035186409950256:  10%|█▎           | 15/146 [00:02<00:17,  7.43it/s]Epoch: 3, train for the 16-th batch, train loss: 0.5035186409950256:  11%|█▍           | 16/146 [00:02<00:19,  6.71it/s]evaluate for the 2-th batch, evaluate loss: 0.5978273749351501:   0%|                            | 0/66 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5978273749351501:   3%|▌                   | 2/66 [00:00<00:06,  9.68it/s]Epoch: 2, train for the 134-th batch, train loss: 0.4580894410610199:  88%|█████████▋ | 133/151 [00:22<00:03,  4.80it/s]Epoch: 2, train for the 134-th batch, train loss: 0.4580894410610199:  89%|█████████▊ | 134/151 [00:22<00:03,  5.34it/s]Epoch: 1, train for the 232-th batch, train loss: 0.36044928431510925:  60%|██████    | 231/383 [01:03<00:53,  2.84it/s]Epoch: 3, train for the 70-th batch, train loss: 0.3895573616027832:  58%|███████▌     | 69/119 [00:11<00:09,  5.32it/s]Epoch: 3, train for the 70-th batch, train loss: 0.3895573616027832:  59%|███████▋     | 70/119 [00:11<00:09,  5.22it/s]Epoch: 3, train for the 17-th batch, train loss: 0.437588095664978:  11%|█▌            | 16/146 [00:02<00:19,  6.71it/s]Epoch: 3, train for the 17-th batch, train loss: 0.437588095664978:  12%|█▋            | 17/146 [00:02<00:20,  6.32it/s]Epoch: 1, train for the 232-th batch, train loss: 0.36044928431510925:  61%|██████    | 232/383 [01:03<00:49,  3.07it/s]Epoch: 2, train for the 135-th batch, train loss: 0.43774664402008057:  89%|████████▊ | 134/151 [00:22<00:03,  5.34it/s]Epoch: 2, train for the 135-th batch, train loss: 0.43774664402008057:  89%|████████▉ | 135/151 [00:22<00:02,  5.82it/s]Epoch: 3, train for the 71-th batch, train loss: 0.4043756127357483:  59%|███████▋     | 70/119 [00:11<00:09,  5.22it/s]Epoch: 3, train for the 71-th batch, train loss: 0.4043756127357483:  60%|███████▊     | 71/119 [00:11<00:08,  5.53it/s]Epoch: 3, train for the 18-th batch, train loss: 0.4918493628501892:  12%|█▌           | 17/146 [00:02<00:20,  6.32it/s]Epoch: 3, train for the 18-th batch, train loss: 0.4918493628501892:  12%|█▌           | 18/146 [00:02<00:21,  6.09it/s]Epoch: 2, train for the 136-th batch, train loss: 0.47928953170776367:  89%|████████▉ | 135/151 [00:22<00:02,  5.82it/s]Epoch: 2, train for the 136-th batch, train loss: 0.47928953170776367:  90%|█████████ | 136/151 [00:22<00:02,  6.06it/s]Epoch: 1, train for the 233-th batch, train loss: 0.4438057541847229:  61%|██████▋    | 232/383 [01:04<00:49,  3.07it/s]evaluate for the 3-th batch, evaluate loss: 0.6614277362823486:   3%|▌                   | 2/66 [00:00<00:06,  9.68it/s]evaluate for the 3-th batch, evaluate loss: 0.6614277362823486:   5%|▉                   | 3/66 [00:00<00:13,  4.71it/s]Epoch: 1, train for the 233-th batch, train loss: 0.4438057541847229:  61%|██████▋    | 233/383 [01:04<00:45,  3.32it/s]Epoch: 3, train for the 72-th batch, train loss: 0.44604361057281494:  60%|███████▏    | 71/119 [00:11<00:08,  5.53it/s]Epoch: 3, train for the 72-th batch, train loss: 0.44604361057281494:  61%|███████▎    | 72/119 [00:11<00:08,  5.70it/s]Epoch: 2, train for the 137-th batch, train loss: 0.5202879309654236:  90%|█████████▉ | 136/151 [00:22<00:02,  6.06it/s]evaluate for the 4-th batch, evaluate loss: 0.6380751132965088:   5%|▉                   | 3/66 [00:00<00:13,  4.71it/s]Epoch: 3, train for the 19-th batch, train loss: 0.47544991970062256:  12%|█▍          | 18/146 [00:02<00:21,  6.09it/s]Epoch: 3, train for the 19-th batch, train loss: 0.47544991970062256:  13%|█▌          | 19/146 [00:02<00:21,  6.02it/s]Epoch: 2, train for the 137-th batch, train loss: 0.5202879309654236:  91%|█████████▉ | 137/151 [00:22<00:02,  6.06it/s]evaluate for the 5-th batch, evaluate loss: 0.6230797171592712:   5%|▉                   | 3/66 [00:00<00:13,  4.71it/s]evaluate for the 5-th batch, evaluate loss: 0.6230797171592712:   8%|█▌                  | 5/66 [00:00<00:09,  6.41it/s]Epoch: 3, train for the 73-th batch, train loss: 0.4313635230064392:  61%|███████▊     | 72/119 [00:11<00:08,  5.70it/s]Epoch: 3, train for the 73-th batch, train loss: 0.4313635230064392:  61%|███████▉     | 73/119 [00:11<00:08,  5.70it/s]Epoch: 3, train for the 20-th batch, train loss: 0.461201936006546:  13%|█▊            | 19/146 [00:03<00:21,  6.02it/s]Epoch: 3, train for the 20-th batch, train loss: 0.461201936006546:  14%|█▉            | 20/146 [00:03<00:20,  6.06it/s]Epoch: 2, train for the 138-th batch, train loss: 0.4754767119884491:  91%|█████████▉ | 137/151 [00:22<00:02,  6.06it/s]Epoch: 2, train for the 138-th batch, train loss: 0.4754767119884491:  91%|██████████ | 138/151 [00:22<00:02,  6.02it/s]evaluate for the 6-th batch, evaluate loss: 0.609553337097168:   8%|█▌                   | 5/66 [00:00<00:09,  6.41it/s]evaluate for the 6-th batch, evaluate loss: 0.609553337097168:   9%|█▉                   | 6/66 [00:00<00:08,  6.91it/s]Epoch: 2, train for the 139-th batch, train loss: 0.493923544883728:  91%|██████████▉ | 138/151 [00:22<00:02,  6.02it/s]Epoch: 3, train for the 74-th batch, train loss: 0.4564952850341797:  61%|███████▉     | 73/119 [00:11<00:08,  5.70it/s]Epoch: 3, train for the 74-th batch, train loss: 0.4564952850341797:  62%|████████     | 74/119 [00:11<00:07,  5.81it/s]Epoch: 2, train for the 139-th batch, train loss: 0.493923544883728:  92%|███████████ | 139/151 [00:22<00:01,  6.47it/s]evaluate for the 7-th batch, evaluate loss: 0.651283860206604:   9%|█▉                   | 6/66 [00:00<00:08,  6.91it/s]Epoch: 3, train for the 21-th batch, train loss: 0.4359205961227417:  14%|█▊           | 20/146 [00:03<00:20,  6.06it/s]Epoch: 3, train for the 21-th batch, train loss: 0.4359205961227417:  14%|█▊           | 21/146 [00:03<00:20,  6.02it/s]Epoch: 2, train for the 140-th batch, train loss: 0.4594676196575165:  92%|██████████▏| 139/151 [00:23<00:01,  6.47it/s]Epoch: 2, train for the 140-th batch, train loss: 0.4594676196575165:  93%|██████████▏| 140/151 [00:23<00:01,  6.72it/s]Epoch: 3, train for the 75-th batch, train loss: 0.4397847354412079:  62%|████████     | 74/119 [00:12<00:07,  5.81it/s]Epoch: 3, train for the 75-th batch, train loss: 0.4397847354412079:  63%|████████▏    | 75/119 [00:12<00:07,  5.82it/s]Epoch: 3, train for the 22-th batch, train loss: 0.42715904116630554:  14%|█▋          | 21/146 [00:03<00:20,  6.02it/s]Epoch: 1, train for the 234-th batch, train loss: 0.4621651768684387:  61%|██████▋    | 233/383 [01:04<00:45,  3.32it/s]Epoch: 3, train for the 22-th batch, train loss: 0.42715904116630554:  15%|█▊          | 22/146 [00:03<00:20,  6.06it/s]evaluate for the 8-th batch, evaluate loss: 0.5876075029373169:   9%|█▊                  | 6/66 [00:01<00:08,  6.91it/s]evaluate for the 8-th batch, evaluate loss: 0.5876075029373169:  12%|██▍                 | 8/66 [00:01<00:08,  7.09it/s]Epoch: 1, train for the 234-th batch, train loss: 0.4621651768684387:  61%|██████▋    | 234/383 [01:04<00:59,  2.50it/s]Epoch: 2, train for the 141-th batch, train loss: 0.46742621064186096:  93%|█████████▎| 140/151 [00:23<00:01,  6.72it/s]Epoch: 2, train for the 141-th batch, train loss: 0.46742621064186096:  93%|█████████▎| 141/151 [00:23<00:01,  6.62it/s]evaluate for the 9-th batch, evaluate loss: 0.6221831440925598:  12%|██▍                 | 8/66 [00:01<00:08,  7.09it/s]evaluate for the 9-th batch, evaluate loss: 0.6221831440925598:  14%|██▋                 | 9/66 [00:01<00:07,  7.59it/s]Epoch: 3, train for the 76-th batch, train loss: 0.43534716963768005:  63%|███████▌    | 75/119 [00:12<00:07,  5.82it/s]Epoch: 3, train for the 76-th batch, train loss: 0.43534716963768005:  64%|███████▋    | 76/119 [00:12<00:07,  5.82it/s]Epoch: 3, train for the 23-th batch, train loss: 0.47724002599716187:  15%|█▊          | 22/146 [00:03<00:20,  6.06it/s]Epoch: 3, train for the 23-th batch, train loss: 0.47724002599716187:  16%|█▉          | 23/146 [00:03<00:20,  5.88it/s]Epoch: 2, train for the 142-th batch, train loss: 0.43910089135169983:  93%|█████████▎| 141/151 [00:23<00:01,  6.62it/s]Epoch: 2, train for the 142-th batch, train loss: 0.43910089135169983:  94%|█████████▍| 142/151 [00:23<00:01,  6.72it/s]Epoch: 1, train for the 235-th batch, train loss: 0.3442862629890442:  61%|██████▋    | 234/383 [01:04<00:59,  2.50it/s]Epoch: 1, train for the 235-th batch, train loss: 0.3442862629890442:  61%|██████▋    | 235/383 [01:04<00:52,  2.80it/s]Epoch: 3, train for the 77-th batch, train loss: 0.4215468168258667:  64%|████████▎    | 76/119 [00:12<00:07,  5.82it/s]Epoch: 3, train for the 24-th batch, train loss: 0.4411916136741638:  16%|██           | 23/146 [00:03<00:20,  5.88it/s]Epoch: 2, train for the 143-th batch, train loss: 0.4547412097454071:  94%|██████████▎| 142/151 [00:23<00:01,  6.72it/s]Epoch: 3, train for the 24-th batch, train loss: 0.4411916136741638:  16%|██▏          | 24/146 [00:03<00:21,  5.75it/s]Epoch: 3, train for the 77-th batch, train loss: 0.4215468168258667:  65%|████████▍    | 77/119 [00:12<00:07,  5.38it/s]Epoch: 2, train for the 143-th batch, train loss: 0.4547412097454071:  95%|██████████▍| 143/151 [00:23<00:01,  7.05it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/241 [00:00<?, ?it/s]evaluate for the 10-th batch, evaluate loss: 0.606342613697052:  14%|██▋                 | 9/66 [00:01<00:07,  7.59it/s]evaluate for the 10-th batch, evaluate loss: 0.606342613697052:  15%|██▉                | 10/66 [00:01<00:10,  5.49it/s]Epoch: 3, train for the 25-th batch, train loss: 0.4693402647972107:  16%|██▏          | 24/146 [00:03<00:21,  5.75it/s]Epoch: 3, train for the 25-th batch, train loss: 0.4693402647972107:  17%|██▏          | 25/146 [00:03<00:19,  6.24it/s]Epoch: 3, train for the 78-th batch, train loss: 0.49738809466362:  65%|█████████▋     | 77/119 [00:12<00:07,  5.38it/s]Epoch: 3, train for the 78-th batch, train loss: 0.49738809466362:  66%|█████████▊     | 78/119 [00:12<00:07,  5.45it/s]Epoch: 2, train for the 144-th batch, train loss: 0.4174375534057617:  95%|██████████▍| 143/151 [00:23<00:01,  7.05it/s]Epoch: 2, train for the 1-th batch, train loss: 0.5588622689247131:   0%|                       | 0/241 [00:00<?, ?it/s]Epoch: 2, train for the 1-th batch, train loss: 0.5588622689247131:   0%|               | 1/241 [00:00<00:36,  6.52it/s]Epoch: 1, train for the 236-th batch, train loss: 0.40192174911499023:  61%|██████▏   | 235/383 [01:05<00:52,  2.80it/s]evaluate for the 11-th batch, evaluate loss: 0.6246418952941895:  15%|██▋               | 10/66 [00:01<00:10,  5.49it/s]evaluate for the 11-th batch, evaluate loss: 0.6246418952941895:  17%|███               | 11/66 [00:01<00:09,  5.73it/s]Epoch: 2, train for the 144-th batch, train loss: 0.4174375534057617:  95%|██████████▍| 144/151 [00:23<00:01,  5.88it/s]Epoch: 3, train for the 26-th batch, train loss: 0.37517881393432617:  17%|██          | 25/146 [00:04<00:19,  6.24it/s]Epoch: 3, train for the 26-th batch, train loss: 0.37517881393432617:  18%|██▏         | 26/146 [00:04<00:18,  6.47it/s]Epoch: 1, train for the 236-th batch, train loss: 0.40192174911499023:  62%|██████▏   | 236/383 [01:05<00:52,  2.81it/s]Epoch: 3, train for the 79-th batch, train loss: 0.45633459091186523:  66%|███████▊    | 78/119 [00:12<00:07,  5.45it/s]Epoch: 3, train for the 79-th batch, train loss: 0.45633459091186523:  66%|███████▉    | 79/119 [00:12<00:06,  5.72it/s]Epoch: 2, train for the 2-th batch, train loss: 0.38332101702690125:   0%|              | 1/241 [00:00<00:36,  6.52it/s]Epoch: 2, train for the 2-th batch, train loss: 0.38332101702690125:   1%|              | 2/241 [00:00<00:31,  7.53it/s]evaluate for the 12-th batch, evaluate loss: 0.5894519686698914:  17%|███               | 11/66 [00:01<00:09,  5.73it/s]evaluate for the 12-th batch, evaluate loss: 0.5894519686698914:  18%|███▎              | 12/66 [00:01<00:08,  6.08it/s]Epoch: 3, train for the 27-th batch, train loss: 0.4441332221031189:  18%|██▎          | 26/146 [00:04<00:18,  6.47it/s]Epoch: 3, train for the 27-th batch, train loss: 0.4441332221031189:  18%|██▍          | 27/146 [00:04<00:17,  6.63it/s]Epoch: 2, train for the 145-th batch, train loss: 0.413231760263443:  95%|███████████▍| 144/151 [00:23<00:01,  5.88it/s]Epoch: 2, train for the 3-th batch, train loss: 0.44501861929893494:   1%|              | 2/241 [00:00<00:31,  7.53it/s]Epoch: 2, train for the 3-th batch, train loss: 0.44501861929893494:   1%|▏             | 3/241 [00:00<00:28,  8.28it/s]Epoch: 2, train for the 145-th batch, train loss: 0.413231760263443:  96%|███████████▌| 145/151 [00:23<00:01,  5.42it/s]evaluate for the 13-th batch, evaluate loss: 0.6183542013168335:  18%|███▎              | 12/66 [00:02<00:08,  6.08it/s]evaluate for the 13-th batch, evaluate loss: 0.6183542013168335:  20%|███▌              | 13/66 [00:02<00:07,  6.78it/s]Epoch: 3, train for the 80-th batch, train loss: 0.4718065559864044:  66%|████████▋    | 79/119 [00:13<00:06,  5.72it/s]Epoch: 3, train for the 80-th batch, train loss: 0.4718065559864044:  67%|████████▋    | 80/119 [00:13<00:06,  5.78it/s]Epoch: 3, train for the 28-th batch, train loss: 0.44723039865493774:  18%|██▏         | 27/146 [00:04<00:17,  6.63it/s]Epoch: 3, train for the 28-th batch, train loss: 0.44723039865493774:  19%|██▎         | 28/146 [00:04<00:17,  6.63it/s]Epoch: 2, train for the 4-th batch, train loss: 0.4299735128879547:   1%|▏              | 3/241 [00:00<00:28,  8.28it/s]Epoch: 2, train for the 4-th batch, train loss: 0.4299735128879547:   2%|▏              | 4/241 [00:00<00:28,  8.46it/s]Epoch: 3, train for the 81-th batch, train loss: 0.39600929617881775:  67%|████████    | 80/119 [00:13<00:06,  5.78it/s]Epoch: 1, train for the 237-th batch, train loss: 0.4061082601547241:  62%|██████▊    | 236/383 [01:05<00:52,  2.81it/s]Epoch: 3, train for the 81-th batch, train loss: 0.39600929617881775:  68%|████████▏   | 81/119 [00:13<00:06,  6.06it/s]Epoch: 2, train for the 146-th batch, train loss: 0.40168339014053345:  96%|█████████▌| 145/151 [00:24<00:01,  5.42it/s]Epoch: 2, train for the 5-th batch, train loss: 0.46911320090293884:   2%|▏             | 4/241 [00:00<00:28,  8.46it/s]Epoch: 2, train for the 146-th batch, train loss: 0.40168339014053345:  97%|█████████▋| 146/151 [00:24<00:00,  5.17it/s]Epoch: 2, train for the 5-th batch, train loss: 0.46911320090293884:   2%|▎             | 5/241 [00:00<00:26,  8.76it/s]Epoch: 3, train for the 29-th batch, train loss: 0.3935846984386444:  19%|██▍          | 28/146 [00:04<00:17,  6.63it/s]Epoch: 3, train for the 29-th batch, train loss: 0.3935846984386444:  20%|██▌          | 29/146 [00:04<00:17,  6.82it/s]Epoch: 1, train for the 237-th batch, train loss: 0.4061082601547241:  62%|██████▊    | 237/383 [01:05<00:53,  2.70it/s]evaluate for the 14-th batch, evaluate loss: 0.6078792214393616:  20%|███▌              | 13/66 [00:02<00:07,  6.78it/s]evaluate for the 14-th batch, evaluate loss: 0.6078792214393616:  21%|███▊              | 14/66 [00:02<00:10,  5.14it/s]Epoch: 3, train for the 82-th batch, train loss: 0.4310958683490753:  68%|████████▊    | 81/119 [00:13<00:06,  6.06it/s]Epoch: 3, train for the 82-th batch, train loss: 0.4310958683490753:  69%|████████▉    | 82/119 [00:13<00:06,  5.97it/s]Epoch: 2, train for the 6-th batch, train loss: 0.6320916414260864:   2%|▎              | 5/241 [00:00<00:26,  8.76it/s]Epoch: 2, train for the 6-th batch, train loss: 0.6320916414260864:   2%|▎              | 6/241 [00:00<00:30,  7.69it/s]Epoch: 3, train for the 30-th batch, train loss: 0.404081255197525:  20%|██▊           | 29/146 [00:04<00:17,  6.82it/s]Epoch: 3, train for the 30-th batch, train loss: 0.404081255197525:  21%|██▉           | 30/146 [00:04<00:17,  6.61it/s]Epoch: 2, train for the 147-th batch, train loss: 0.43840399384498596:  97%|█████████▋| 146/151 [00:24<00:00,  5.17it/s]Epoch: 2, train for the 147-th batch, train loss: 0.43840399384498596:  97%|█████████▋| 147/151 [00:24<00:00,  4.96it/s]Epoch: 1, train for the 238-th batch, train loss: 0.3518988788127899:  62%|██████▊    | 237/383 [01:06<00:53,  2.70it/s]evaluate for the 15-th batch, evaluate loss: 0.6216319799423218:  21%|███▊              | 14/66 [00:02<00:10,  5.14it/s]evaluate for the 15-th batch, evaluate loss: 0.6216319799423218:  23%|████              | 15/66 [00:02<00:09,  5.38it/s]Epoch: 2, train for the 7-th batch, train loss: 0.383216917514801:   2%|▍               | 6/241 [00:00<00:30,  7.69it/s]Epoch: 2, train for the 7-th batch, train loss: 0.383216917514801:   3%|▍               | 7/241 [00:00<00:31,  7.33it/s]Epoch: 1, train for the 238-th batch, train loss: 0.3518988788127899:  62%|██████▊    | 238/383 [01:06<00:51,  2.81it/s]Epoch: 3, train for the 83-th batch, train loss: 0.43452131748199463:  69%|████████▎   | 82/119 [00:13<00:06,  5.97it/s]Epoch: 3, train for the 31-th batch, train loss: 0.4986077845096588:  21%|██▋          | 30/146 [00:04<00:17,  6.61it/s]Epoch: 3, train for the 83-th batch, train loss: 0.43452131748199463:  70%|████████▎   | 83/119 [00:13<00:06,  5.50it/s]Epoch: 3, train for the 31-th batch, train loss: 0.4986077845096588:  21%|██▊          | 31/146 [00:04<00:18,  6.17it/s]Epoch: 2, train for the 148-th batch, train loss: 0.39734625816345215:  97%|█████████▋| 147/151 [00:24<00:00,  4.96it/s]Epoch: 2, train for the 8-th batch, train loss: 0.7228478789329529:   3%|▍              | 7/241 [00:01<00:31,  7.33it/s]Epoch: 2, train for the 8-th batch, train loss: 0.7228478789329529:   3%|▍              | 8/241 [00:01<00:29,  7.89it/s]Epoch: 2, train for the 148-th batch, train loss: 0.39734625816345215:  98%|█████████▊| 148/151 [00:24<00:00,  4.96it/s]Epoch: 3, train for the 32-th batch, train loss: 0.46375641226768494:  21%|██▌         | 31/146 [00:04<00:18,  6.17it/s]Epoch: 3, train for the 32-th batch, train loss: 0.46375641226768494:  22%|██▋         | 32/146 [00:04<00:18,  6.16it/s]Epoch: 3, train for the 84-th batch, train loss: 0.44572874903678894:  70%|████████▎   | 83/119 [00:13<00:06,  5.50it/s]evaluate for the 16-th batch, evaluate loss: 0.5664040446281433:  23%|████              | 15/66 [00:02<00:09,  5.38it/s]evaluate for the 16-th batch, evaluate loss: 0.5664040446281433:  24%|████▎             | 16/66 [00:02<00:10,  4.81it/s]Epoch: 3, train for the 84-th batch, train loss: 0.44572874903678894:  71%|████████▍   | 84/119 [00:13<00:06,  5.48it/s]Epoch: 3, train for the 33-th batch, train loss: 0.4324001967906952:  22%|██▊          | 32/146 [00:05<00:18,  6.16it/s]Epoch: 3, train for the 33-th batch, train loss: 0.4324001967906952:  23%|██▉          | 33/146 [00:05<00:17,  6.38it/s]Epoch: 1, train for the 239-th batch, train loss: 0.43709442019462585:  62%|██████▏   | 238/383 [01:06<00:51,  2.81it/s]evaluate for the 17-th batch, evaluate loss: 0.5966410040855408:  24%|████▎             | 16/66 [00:02<00:10,  4.81it/s]evaluate for the 17-th batch, evaluate loss: 0.5966410040855408:  26%|████▋             | 17/66 [00:02<00:09,  5.36it/s]Epoch: 3, train for the 85-th batch, train loss: 0.40968284010887146:  71%|████████▍   | 84/119 [00:13<00:06,  5.48it/s]Epoch: 2, train for the 149-th batch, train loss: 0.41449877619743347:  98%|█████████▊| 148/151 [00:24<00:00,  4.96it/s]Epoch: 3, train for the 85-th batch, train loss: 0.40968284010887146:  71%|████████▌   | 85/119 [00:13<00:06,  5.64it/s]Epoch: 2, train for the 9-th batch, train loss: 0.631107747554779:   3%|▌               | 8/241 [00:01<00:29,  7.89it/s]Epoch: 1, train for the 239-th batch, train loss: 0.43709442019462585:  62%|██████▏   | 239/383 [01:06<00:52,  2.75it/s]Epoch: 2, train for the 9-th batch, train loss: 0.631107747554779:   4%|▌               | 9/241 [00:01<00:42,  5.46it/s]Epoch: 2, train for the 149-th batch, train loss: 0.41449877619743347:  99%|█████████▊| 149/151 [00:24<00:00,  4.27it/s]Epoch: 3, train for the 34-th batch, train loss: 0.496348112821579:  23%|███▏          | 33/146 [00:05<00:17,  6.38it/s]Epoch: 3, train for the 34-th batch, train loss: 0.496348112821579:  23%|███▎          | 34/146 [00:05<00:16,  6.97it/s]evaluate for the 18-th batch, evaluate loss: 0.6257340312004089:  26%|████▋             | 17/66 [00:03<00:09,  5.36it/s]evaluate for the 18-th batch, evaluate loss: 0.6257340312004089:  27%|████▉             | 18/66 [00:03<00:08,  5.87it/s]Epoch: 2, train for the 10-th batch, train loss: 0.5442790985107422:   4%|▌             | 9/241 [00:01<00:42,  5.46it/s]Epoch: 2, train for the 10-th batch, train loss: 0.5442790985107422:   4%|▌            | 10/241 [00:01<00:41,  5.55it/s]Epoch: 3, train for the 35-th batch, train loss: 0.43793216347694397:  23%|██▊         | 34/146 [00:05<00:16,  6.97it/s]Epoch: 3, train for the 35-th batch, train loss: 0.43793216347694397:  24%|██▉         | 35/146 [00:05<00:15,  7.17it/s]Epoch: 2, train for the 150-th batch, train loss: 0.443570613861084:  99%|███████████▊| 149/151 [00:25<00:00,  4.27it/s]evaluate for the 19-th batch, evaluate loss: 0.5846109390258789:  27%|████▉             | 18/66 [00:03<00:08,  5.87it/s]evaluate for the 19-th batch, evaluate loss: 0.5846109390258789:  29%|█████▏            | 19/66 [00:03<00:07,  6.25it/s]Epoch: 2, train for the 150-th batch, train loss: 0.443570613861084:  99%|███████████▉| 150/151 [00:25<00:00,  4.30it/s]Epoch: 1, train for the 240-th batch, train loss: 0.552602231502533:  62%|███████▍    | 239/383 [01:06<00:52,  2.75it/s]Epoch: 3, train for the 86-th batch, train loss: 0.4743240773677826:  71%|█████████▎   | 85/119 [00:14<00:06,  5.64it/s]Epoch: 3, train for the 86-th batch, train loss: 0.4743240773677826:  72%|█████████▍   | 86/119 [00:14<00:06,  4.79it/s]Epoch: 1, train for the 240-th batch, train loss: 0.552602231502533:  63%|███████▌    | 240/383 [01:06<00:49,  2.91it/s]Epoch: 2, train for the 11-th batch, train loss: 0.3650018572807312:   4%|▌            | 10/241 [00:01<00:41,  5.55it/s]Epoch: 3, train for the 36-th batch, train loss: 0.4159926176071167:  24%|███          | 35/146 [00:05<00:15,  7.17it/s]Epoch: 2, train for the 11-th batch, train loss: 0.3650018572807312:   5%|▌            | 11/241 [00:01<00:38,  5.97it/s]Epoch: 3, train for the 36-th batch, train loss: 0.4159926176071167:  25%|███▏         | 36/146 [00:05<00:14,  7.37it/s]Epoch: 3, train for the 87-th batch, train loss: 0.42535850405693054:  72%|████████▋   | 86/119 [00:14<00:06,  4.79it/s]Epoch: 3, train for the 87-th batch, train loss: 0.42535850405693054:  73%|████████▊   | 87/119 [00:14<00:06,  5.22it/s]Epoch: 2, train for the 12-th batch, train loss: 0.5515841841697693:   5%|▌            | 11/241 [00:01<00:38,  5.97it/s]Epoch: 2, train for the 12-th batch, train loss: 0.5515841841697693:   5%|▋            | 12/241 [00:01<00:34,  6.57it/s]evaluate for the 20-th batch, evaluate loss: 0.636120080947876:  29%|█████▍             | 19/66 [00:03<00:07,  6.25it/s]evaluate for the 20-th batch, evaluate loss: 0.636120080947876:  30%|█████▊             | 20/66 [00:03<00:08,  5.68it/s]Epoch: 3, train for the 37-th batch, train loss: 0.4531557261943817:  25%|███▏         | 36/146 [00:05<00:14,  7.37it/s]Epoch: 3, train for the 37-th batch, train loss: 0.4531557261943817:  25%|███▎         | 37/146 [00:05<00:15,  7.17it/s]Epoch: 2, train for the 151-th batch, train loss: 0.49718335270881653:  99%|█████████▉| 150/151 [00:25<00:00,  4.30it/s]Epoch: 2, train for the 151-th batch, train loss: 0.49718335270881653: 100%|██████████| 151/151 [00:25<00:00,  4.14it/s]Epoch: 2, train for the 151-th batch, train loss: 0.49718335270881653: 100%|██████████| 151/151 [00:25<00:00,  5.96it/s]
Epoch: 2, train for the 13-th batch, train loss: 0.5053831934928894:   5%|▋            | 12/241 [00:01<00:34,  6.57it/s]Epoch: 2, train for the 13-th batch, train loss: 0.5053831934928894:   5%|▋            | 13/241 [00:01<00:31,  7.18it/s]Epoch: 1, train for the 241-th batch, train loss: 0.60048907995224:  63%|████████▏    | 240/383 [01:07<00:49,  2.91it/s]evaluate for the 21-th batch, evaluate loss: 0.6477857828140259:  30%|█████▍            | 20/66 [00:03<00:08,  5.68it/s]evaluate for the 21-th batch, evaluate loss: 0.6477857828140259:  32%|█████▋            | 21/66 [00:03<00:07,  6.18it/s]Epoch: 3, train for the 88-th batch, train loss: 0.4715292155742645:  73%|█████████▌   | 87/119 [00:14<00:06,  5.22it/s]Epoch: 3, train for the 88-th batch, train loss: 0.4715292155742645:  74%|█████████▌   | 88/119 [00:14<00:05,  5.58it/s]Epoch: 3, train for the 38-th batch, train loss: 0.4479164481163025:  25%|███▎         | 37/146 [00:05<00:15,  7.17it/s]Epoch: 3, train for the 38-th batch, train loss: 0.4479164481163025:  26%|███▍         | 38/146 [00:05<00:15,  7.05it/s]Epoch: 1, train for the 241-th batch, train loss: 0.60048907995224:  63%|████████▏    | 241/383 [01:07<00:47,  2.96it/s]Epoch: 2, train for the 14-th batch, train loss: 0.6161102056503296:   5%|▋            | 13/241 [00:01<00:31,  7.18it/s]evaluate for the 22-th batch, evaluate loss: 0.6080148220062256:  32%|█████▋            | 21/66 [00:03<00:07,  6.18it/s]evaluate for the 22-th batch, evaluate loss: 0.6080148220062256:  33%|██████            | 22/66 [00:03<00:06,  6.80it/s]Epoch: 3, train for the 89-th batch, train loss: 0.47501251101493835:  74%|████████▊   | 88/119 [00:14<00:05,  5.58it/s]Epoch: 3, train for the 89-th batch, train loss: 0.47501251101493835:  75%|████████▉   | 89/119 [00:14<00:04,  6.04it/s]Epoch: 3, train for the 39-th batch, train loss: 0.4608097970485687:  26%|███▍         | 38/146 [00:05<00:15,  7.05it/s]Epoch: 3, train for the 39-th batch, train loss: 0.4608097970485687:  27%|███▍         | 39/146 [00:05<00:14,  7.14it/s]  0%|                                                                                            | 0/46 [00:00<?, ?it/s]Epoch: 2, train for the 15-th batch, train loss: 0.5325803160667419:   5%|▋            | 13/241 [00:02<00:31,  7.18it/s]Epoch: 2, train for the 15-th batch, train loss: 0.5325803160667419:   6%|▊            | 15/241 [00:02<00:29,  7.71it/s]evaluate for the 23-th batch, evaluate loss: 0.6273261308670044:  33%|██████            | 22/66 [00:03<00:06,  6.80it/s]evaluate for the 23-th batch, evaluate loss: 0.6273261308670044:  35%|██████▎           | 23/66 [00:03<00:05,  7.29it/s]Epoch: 1, train for the 242-th batch, train loss: 0.48280224204063416:  63%|██████▎   | 241/383 [01:07<00:47,  2.96it/s]evaluate for the 1-th batch, evaluate loss: 0.6843324899673462:   0%|                            | 0/46 [00:00<?, ?it/s]Epoch: 3, train for the 90-th batch, train loss: 0.4295668601989746:  75%|█████████▋   | 89/119 [00:14<00:04,  6.04it/s]Epoch: 3, train for the 90-th batch, train loss: 0.4295668601989746:  76%|█████████▊   | 90/119 [00:14<00:04,  5.82it/s]Epoch: 1, train for the 242-th batch, train loss: 0.48280224204063416:  63%|██████▎   | 242/383 [01:07<00:45,  3.11it/s]Epoch: 3, train for the 40-th batch, train loss: 0.42578062415122986:  27%|███▏        | 39/146 [00:06<00:14,  7.14it/s]Epoch: 3, train for the 40-th batch, train loss: 0.42578062415122986:  27%|███▎        | 40/146 [00:06<00:15,  6.68it/s]Epoch: 2, train for the 16-th batch, train loss: 0.4535173177719116:   6%|▊            | 15/241 [00:02<00:29,  7.71it/s]evaluate for the 2-th batch, evaluate loss: 0.6614677906036377:   0%|                            | 0/46 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.6614677906036377:   4%|▊                   | 2/46 [00:00<00:03, 11.49it/s]Epoch: 2, train for the 16-th batch, train loss: 0.4535173177719116:   7%|▊            | 16/241 [00:02<00:30,  7.29it/s]evaluate for the 3-th batch, evaluate loss: 0.7020525932312012:   4%|▊                   | 2/46 [00:00<00:03, 11.49it/s]Epoch: 3, train for the 91-th batch, train loss: 0.4317157566547394:  76%|█████████▊   | 90/119 [00:15<00:04,  5.82it/s]Epoch: 3, train for the 91-th batch, train loss: 0.4317157566547394:  76%|█████████▉   | 91/119 [00:15<00:05,  5.53it/s]Epoch: 3, train for the 41-th batch, train loss: 0.44145575165748596:  27%|███▎        | 40/146 [00:06<00:15,  6.68it/s]Epoch: 3, train for the 41-th batch, train loss: 0.44145575165748596:  28%|███▎        | 41/146 [00:06<00:16,  6.31it/s]Epoch: 2, train for the 17-th batch, train loss: 0.4159811735153198:   7%|▊            | 16/241 [00:02<00:30,  7.29it/s]evaluate for the 4-th batch, evaluate loss: 0.6284710168838501:   4%|▊                   | 2/46 [00:00<00:03, 11.49it/s]evaluate for the 4-th batch, evaluate loss: 0.6284710168838501:   9%|█▋                  | 4/46 [00:00<00:03, 11.29it/s]Epoch: 2, train for the 17-th batch, train loss: 0.4159811735153198:   7%|▉            | 17/241 [00:02<00:32,  6.80it/s]evaluate for the 24-th batch, evaluate loss: 0.6160942912101746:  35%|██████▎           | 23/66 [00:04<00:05,  7.29it/s]evaluate for the 24-th batch, evaluate loss: 0.6160942912101746:  36%|██████▌           | 24/66 [00:04<00:08,  5.04it/s]Epoch: 1, train for the 243-th batch, train loss: 0.5405004024505615:  63%|██████▉    | 242/383 [01:07<00:45,  3.11it/s]evaluate for the 5-th batch, evaluate loss: 0.6356599926948547:   9%|█▋                  | 4/46 [00:00<00:03, 11.29it/s]Epoch: 1, train for the 243-th batch, train loss: 0.5405004024505615:  63%|██████▉    | 243/383 [01:07<00:44,  3.15it/s]Epoch: 3, train for the 92-th batch, train loss: 0.463153600692749:  76%|██████████▋   | 91/119 [00:15<00:05,  5.53it/s]evaluate for the 25-th batch, evaluate loss: 0.5919643044471741:  36%|██████▌           | 24/66 [00:04<00:08,  5.04it/s]evaluate for the 25-th batch, evaluate loss: 0.5919643044471741:  38%|██████▊           | 25/66 [00:04<00:07,  5.81it/s]Epoch: 2, train for the 18-th batch, train loss: 0.4367237091064453:   7%|▉            | 17/241 [00:02<00:32,  6.80it/s]Epoch: 3, train for the 92-th batch, train loss: 0.463153600692749:  77%|██████████▊   | 92/119 [00:15<00:04,  5.74it/s]Epoch: 2, train for the 18-th batch, train loss: 0.4367237091064453:   7%|▉            | 18/241 [00:02<00:32,  6.81it/s]Epoch: 3, train for the 42-th batch, train loss: 0.45359092950820923:  28%|███▎        | 41/146 [00:06<00:16,  6.31it/s]evaluate for the 6-th batch, evaluate loss: 0.6027804613113403:   9%|█▋                  | 4/46 [00:00<00:03, 11.29it/s]evaluate for the 6-th batch, evaluate loss: 0.6027804613113403:  13%|██▌                 | 6/46 [00:00<00:03, 11.73it/s]Epoch: 3, train for the 42-th batch, train loss: 0.45359092950820923:  29%|███▍        | 42/146 [00:06<00:17,  6.09it/s]evaluate for the 7-th batch, evaluate loss: 0.6819512248039246:  13%|██▌                 | 6/46 [00:00<00:03, 11.73it/s]Epoch: 2, train for the 19-th batch, train loss: 0.4473695158958435:   7%|▉            | 18/241 [00:02<00:32,  6.81it/s]Epoch: 2, train for the 19-th batch, train loss: 0.4473695158958435:   8%|█            | 19/241 [00:02<00:33,  6.66it/s]evaluate for the 8-th batch, evaluate loss: 0.6591756343841553:  13%|██▌                 | 6/46 [00:00<00:03, 11.73it/s]evaluate for the 8-th batch, evaluate loss: 0.6591756343841553:  17%|███▍                | 8/46 [00:00<00:03, 12.11it/s]Epoch: 3, train for the 93-th batch, train loss: 0.4156453609466553:  77%|██████████   | 92/119 [00:15<00:04,  5.74it/s]Epoch: 3, train for the 43-th batch, train loss: 0.45625367760658264:  29%|███▍        | 42/146 [00:06<00:17,  6.09it/s]Epoch: 3, train for the 43-th batch, train loss: 0.45625367760658264:  29%|███▌        | 43/146 [00:06<00:17,  5.93it/s]Epoch: 3, train for the 93-th batch, train loss: 0.4156453609466553:  78%|██████████▏  | 93/119 [00:15<00:04,  5.44it/s]evaluate for the 9-th batch, evaluate loss: 0.6479108929634094:  17%|███▍                | 8/46 [00:00<00:03, 12.11it/s]Epoch: 2, train for the 20-th batch, train loss: 0.44897913932800293:   8%|▉           | 19/241 [00:02<00:33,  6.66it/s]evaluate for the 26-th batch, evaluate loss: 0.6187554597854614:  38%|██████▊           | 25/66 [00:04<00:07,  5.81it/s]evaluate for the 26-th batch, evaluate loss: 0.6187554597854614:  39%|███████           | 26/66 [00:04<00:08,  4.71it/s]Epoch: 2, train for the 20-th batch, train loss: 0.44897913932800293:   8%|▉           | 20/241 [00:02<00:32,  6.85it/s]evaluate for the 10-th batch, evaluate loss: 0.6062577962875366:  17%|███▎               | 8/46 [00:00<00:03, 12.11it/s]evaluate for the 10-th batch, evaluate loss: 0.6062577962875366:  22%|███▉              | 10/46 [00:00<00:02, 12.17it/s]Epoch: 3, train for the 44-th batch, train loss: 0.4230769872665405:  29%|███▊         | 43/146 [00:06<00:17,  5.93it/s]Epoch: 3, train for the 44-th batch, train loss: 0.4230769872665405:  30%|███▉         | 44/146 [00:06<00:16,  6.13it/s]Epoch: 3, train for the 94-th batch, train loss: 0.3902342617511749:  78%|██████████▏  | 93/119 [00:15<00:04,  5.44it/s]Epoch: 3, train for the 94-th batch, train loss: 0.3902342617511749:  79%|██████████▎  | 94/119 [00:15<00:04,  5.34it/s]evaluate for the 11-th batch, evaluate loss: 0.6891814470291138:  22%|███▉              | 10/46 [00:00<00:02, 12.17it/s]Epoch: 1, train for the 244-th batch, train loss: 0.363743394613266:  63%|███████▌    | 243/383 [01:08<00:44,  3.15it/s]evaluate for the 27-th batch, evaluate loss: 0.5914043188095093:  39%|███████           | 26/66 [00:04<00:08,  4.71it/s]evaluate for the 27-th batch, evaluate loss: 0.5914043188095093:  41%|███████▎          | 27/66 [00:04<00:07,  5.21it/s]Epoch: 2, train for the 21-th batch, train loss: 0.6522775888442993:   8%|█            | 20/241 [00:03<00:32,  6.85it/s]Epoch: 2, train for the 21-th batch, train loss: 0.6522775888442993:   9%|█▏           | 21/241 [00:03<00:33,  6.51it/s]Epoch: 3, train for the 45-th batch, train loss: 0.4653365910053253:  30%|███▉         | 44/146 [00:06<00:16,  6.13it/s]Epoch: 1, train for the 244-th batch, train loss: 0.363743394613266:  64%|███████▋    | 244/383 [01:08<00:52,  2.62it/s]Epoch: 3, train for the 45-th batch, train loss: 0.4653365910053253:  31%|████         | 45/146 [00:06<00:15,  6.42it/s]evaluate for the 12-th batch, evaluate loss: 0.5897619724273682:  22%|███▉              | 10/46 [00:01<00:02, 12.17it/s]evaluate for the 12-th batch, evaluate loss: 0.5897619724273682:  26%|████▋             | 12/46 [00:01<00:02, 12.07it/s]Epoch: 3, train for the 95-th batch, train loss: 0.3565545678138733:  79%|██████████▎  | 94/119 [00:15<00:04,  5.34it/s]Epoch: 3, train for the 95-th batch, train loss: 0.3565545678138733:  80%|██████████▍  | 95/119 [00:15<00:04,  5.73it/s]evaluate for the 28-th batch, evaluate loss: 0.6800889372825623:  41%|███████▎          | 27/66 [00:04<00:07,  5.21it/s]evaluate for the 28-th batch, evaluate loss: 0.6800889372825623:  42%|███████▋          | 28/66 [00:04<00:06,  5.99it/s]Epoch: 2, train for the 22-th batch, train loss: 0.5692453384399414:   9%|█▏           | 21/241 [00:03<00:33,  6.51it/s]Epoch: 2, train for the 22-th batch, train loss: 0.5692453384399414:   9%|█▏           | 22/241 [00:03<00:32,  6.73it/s]evaluate for the 13-th batch, evaluate loss: 0.6377079486846924:  26%|████▋             | 12/46 [00:01<00:02, 12.07it/s]Epoch: 3, train for the 46-th batch, train loss: 0.45627471804618835:  31%|███▋        | 45/146 [00:07<00:15,  6.42it/s]Epoch: 3, train for the 46-th batch, train loss: 0.45627471804618835:  32%|███▊        | 46/146 [00:07<00:15,  6.62it/s]evaluate for the 29-th batch, evaluate loss: 0.5683766603469849:  42%|███████▋          | 28/66 [00:04<00:06,  5.99it/s]evaluate for the 29-th batch, evaluate loss: 0.5683766603469849:  44%|███████▉          | 29/66 [00:04<00:05,  6.45it/s]Epoch: 3, train for the 96-th batch, train loss: 0.4096807837486267:  80%|██████████▍  | 95/119 [00:15<00:04,  5.73it/s]Epoch: 1, train for the 245-th batch, train loss: 0.46199363470077515:  64%|██████▎   | 244/383 [01:08<00:52,  2.62it/s]Epoch: 3, train for the 96-th batch, train loss: 0.4096807837486267:  81%|██████████▍  | 96/119 [00:15<00:03,  5.79it/s]evaluate for the 14-th batch, evaluate loss: 0.6594042181968689:  26%|████▋             | 12/46 [00:01<00:02, 12.07it/s]evaluate for the 14-th batch, evaluate loss: 0.6594042181968689:  30%|█████▍            | 14/46 [00:01<00:02, 11.07it/s]Epoch: 2, train for the 23-th batch, train loss: 0.495732843875885:   9%|█▎            | 22/241 [00:03<00:32,  6.73it/s]Epoch: 1, train for the 245-th batch, train loss: 0.46199363470077515:  64%|██████▍   | 245/383 [01:08<00:47,  2.90it/s]Epoch: 2, train for the 23-th batch, train loss: 0.495732843875885:  10%|█▎            | 23/241 [00:03<00:32,  6.69it/s]Epoch: 3, train for the 47-th batch, train loss: 0.5017801523208618:  32%|████         | 46/146 [00:07<00:15,  6.62it/s]Epoch: 3, train for the 47-th batch, train loss: 0.5017801523208618:  32%|████▏        | 47/146 [00:07<00:14,  6.71it/s]Epoch: 3, train for the 97-th batch, train loss: 0.3847883641719818:  81%|██████████▍  | 96/119 [00:16<00:03,  5.79it/s]Epoch: 2, train for the 24-th batch, train loss: 0.5667285919189453:  10%|█▏           | 23/241 [00:03<00:32,  6.69it/s]Epoch: 3, train for the 97-th batch, train loss: 0.3847883641719818:  82%|██████████▌  | 97/119 [00:16<00:03,  5.99it/s]Epoch: 2, train for the 24-th batch, train loss: 0.5667285919189453:  10%|█▎           | 24/241 [00:03<00:30,  7.17it/s]Epoch: 3, train for the 48-th batch, train loss: 0.4466763138771057:  32%|████▏        | 47/146 [00:07<00:14,  6.71it/s]Epoch: 3, train for the 48-th batch, train loss: 0.4466763138771057:  33%|████▎        | 48/146 [00:07<00:14,  6.92it/s]Epoch: 1, train for the 246-th batch, train loss: 0.5100590586662292:  64%|███████    | 245/383 [01:08<00:47,  2.90it/s]evaluate for the 15-th batch, evaluate loss: 0.621842622756958:  30%|█████▊             | 14/46 [00:01<00:02, 11.07it/s]evaluate for the 30-th batch, evaluate loss: 0.6325958371162415:  44%|███████▉          | 29/66 [00:05<00:05,  6.45it/s]evaluate for the 30-th batch, evaluate loss: 0.6325958371162415:  45%|████████▏         | 30/66 [00:05<00:07,  4.94it/s]Epoch: 2, train for the 25-th batch, train loss: 0.5916649699211121:  10%|█▎           | 24/241 [00:03<00:30,  7.17it/s]Epoch: 2, train for the 25-th batch, train loss: 0.5916649699211121:  10%|█▎           | 25/241 [00:03<00:29,  7.33it/s]Epoch: 1, train for the 246-th batch, train loss: 0.5100590586662292:  64%|███████    | 246/383 [01:08<00:44,  3.11it/s]Epoch: 3, train for the 98-th batch, train loss: 0.3883810341358185:  82%|██████████▌  | 97/119 [00:16<00:03,  5.99it/s]Epoch: 3, train for the 98-th batch, train loss: 0.3883810341358185:  82%|██████████▋  | 98/119 [00:16<00:03,  5.78it/s]evaluate for the 16-th batch, evaluate loss: 0.6050851941108704:  30%|█████▍            | 14/46 [00:01<00:02, 11.07it/s]evaluate for the 16-th batch, evaluate loss: 0.6050851941108704:  35%|██████▎           | 16/46 [00:01<00:03,  8.62it/s]Epoch: 3, train for the 49-th batch, train loss: 0.474565714597702:  33%|████▌         | 48/146 [00:07<00:14,  6.92it/s]Epoch: 3, train for the 49-th batch, train loss: 0.474565714597702:  34%|████▋         | 49/146 [00:07<00:14,  6.63it/s]evaluate for the 31-th batch, evaluate loss: 0.5834546089172363:  45%|████████▏         | 30/66 [00:05<00:07,  4.94it/s]evaluate for the 31-th batch, evaluate loss: 0.5834546089172363:  47%|████████▍         | 31/66 [00:05<00:06,  5.75it/s]Epoch: 2, train for the 26-th batch, train loss: 0.6627526879310608:  10%|█▎           | 25/241 [00:03<00:29,  7.33it/s]Epoch: 2, train for the 26-th batch, train loss: 0.6627526879310608:  11%|█▍           | 26/241 [00:03<00:29,  7.39it/s]evaluate for the 17-th batch, evaluate loss: 0.49790045619010925:  35%|█████▉           | 16/46 [00:01<00:03,  8.62it/s]evaluate for the 17-th batch, evaluate loss: 0.49790045619010925:  37%|██████▎          | 17/46 [00:01<00:03,  8.84it/s]Epoch: 1, train for the 247-th batch, train loss: 0.3395809829235077:  64%|███████    | 246/383 [01:08<00:44,  3.11it/s]Epoch: 2, train for the 27-th batch, train loss: 0.6799131035804749:  11%|█▍           | 26/241 [00:03<00:29,  7.39it/s]Epoch: 1, train for the 247-th batch, train loss: 0.3395809829235077:  64%|███████    | 247/383 [01:08<00:40,  3.39it/s]Epoch: 2, train for the 27-th batch, train loss: 0.6799131035804749:  11%|█▍           | 27/241 [00:03<00:27,  7.75it/s]Epoch: 3, train for the 99-th batch, train loss: 0.39245468378067017:  82%|█████████▉  | 98/119 [00:16<00:03,  5.78it/s]Epoch: 3, train for the 50-th batch, train loss: 0.47752645611763:  34%|█████          | 49/146 [00:07<00:14,  6.63it/s]Epoch: 3, train for the 50-th batch, train loss: 0.47752645611763:  34%|█████▏         | 50/146 [00:07<00:16,  5.93it/s]Epoch: 3, train for the 99-th batch, train loss: 0.39245468378067017:  83%|█████████▉  | 99/119 [00:16<00:03,  5.22it/s]Epoch: 2, train for the 28-th batch, train loss: 0.4412257671356201:  11%|█▍           | 27/241 [00:03<00:27,  7.75it/s]Epoch: 2, train for the 28-th batch, train loss: 0.4412257671356201:  12%|█▌           | 28/241 [00:03<00:26,  8.12it/s]evaluate for the 18-th batch, evaluate loss: 0.6187399625778198:  37%|██████▋           | 17/46 [00:01<00:03,  8.84it/s]evaluate for the 18-th batch, evaluate loss: 0.6187399625778198:  39%|███████           | 18/46 [00:01<00:04,  6.88it/s]Epoch: 3, train for the 51-th batch, train loss: 0.4617065191268921:  34%|████▍        | 50/146 [00:07<00:16,  5.93it/s]evaluate for the 32-th batch, evaluate loss: 0.6859807968139648:  47%|████████▍         | 31/66 [00:05<00:06,  5.75it/s]evaluate for the 32-th batch, evaluate loss: 0.6859807968139648:  48%|████████▋         | 32/66 [00:05<00:07,  4.46it/s]Epoch: 3, train for the 51-th batch, train loss: 0.4617065191268921:  35%|████▌        | 51/146 [00:07<00:15,  6.11it/s]Epoch: 3, train for the 100-th batch, train loss: 0.4118264317512512:  83%|█████████▉  | 99/119 [00:16<00:03,  5.22it/s]Epoch: 3, train for the 100-th batch, train loss: 0.4118264317512512:  84%|█████████▏ | 100/119 [00:16<00:03,  5.23it/s]Epoch: 2, train for the 29-th batch, train loss: 0.6114172339439392:  12%|█▌           | 28/241 [00:04<00:26,  8.12it/s]Epoch: 2, train for the 29-th batch, train loss: 0.6114172339439392:  12%|█▌           | 29/241 [00:04<00:27,  7.65it/s]evaluate for the 19-th batch, evaluate loss: 0.6436901688575745:  39%|███████           | 18/46 [00:02<00:04,  6.88it/s]Epoch: 3, train for the 52-th batch, train loss: 0.4374077618122101:  35%|████▌        | 51/146 [00:07<00:15,  6.11it/s]Epoch: 3, train for the 52-th batch, train loss: 0.4374077618122101:  36%|████▋        | 52/146 [00:07<00:14,  6.55it/s]Epoch: 1, train for the 248-th batch, train loss: 0.37211287021636963:  64%|██████▍   | 247/383 [01:09<00:40,  3.39it/s]evaluate for the 33-th batch, evaluate loss: 0.6349973678588867:  48%|████████▋         | 32/66 [00:05<00:07,  4.46it/s]evaluate for the 33-th batch, evaluate loss: 0.6349973678588867:  50%|█████████         | 33/66 [00:05<00:06,  4.94it/s]evaluate for the 20-th batch, evaluate loss: 0.6321800351142883:  39%|███████           | 18/46 [00:02<00:04,  6.88it/s]evaluate for the 20-th batch, evaluate loss: 0.6321800351142883:  43%|███████▊          | 20/46 [00:02<00:03,  8.17it/s]Epoch: 1, train for the 248-th batch, train loss: 0.37211287021636963:  65%|██████▍   | 248/383 [01:09<00:43,  3.12it/s]Epoch: 3, train for the 101-th batch, train loss: 0.41764959692955017:  84%|████████▍ | 100/119 [00:16<00:03,  5.23it/s]Epoch: 3, train for the 101-th batch, train loss: 0.41764959692955017:  85%|████████▍ | 101/119 [00:16<00:03,  5.47it/s]Epoch: 2, train for the 30-th batch, train loss: 0.6717243194580078:  12%|█▌           | 29/241 [00:04<00:27,  7.65it/s]evaluate for the 21-th batch, evaluate loss: 0.6371229887008667:  43%|███████▊          | 20/46 [00:02<00:03,  8.17it/s]Epoch: 2, train for the 30-th batch, train loss: 0.6717243194580078:  12%|█▌           | 30/241 [00:04<00:29,  7.17it/s]Epoch: 3, train for the 53-th batch, train loss: 0.44100072979927063:  36%|████▎       | 52/146 [00:08<00:14,  6.55it/s]evaluate for the 34-th batch, evaluate loss: 0.6225208044052124:  50%|█████████         | 33/66 [00:05<00:06,  4.94it/s]evaluate for the 34-th batch, evaluate loss: 0.6225208044052124:  52%|█████████▎        | 34/66 [00:05<00:05,  5.76it/s]Epoch: 3, train for the 53-th batch, train loss: 0.44100072979927063:  36%|████▎       | 53/146 [00:08<00:13,  6.87it/s]evaluate for the 22-th batch, evaluate loss: 0.6082399487495422:  43%|███████▊          | 20/46 [00:02<00:03,  8.17it/s]evaluate for the 22-th batch, evaluate loss: 0.6082399487495422:  48%|████████▌         | 22/46 [00:02<00:02,  9.25it/s]Epoch: 3, train for the 102-th batch, train loss: 0.46878179907798767:  85%|████████▍ | 101/119 [00:16<00:03,  5.47it/s]Epoch: 3, train for the 102-th batch, train loss: 0.46878179907798767:  86%|████████▌ | 102/119 [00:16<00:02,  5.72it/s]evaluate for the 35-th batch, evaluate loss: 0.6239883303642273:  52%|█████████▎        | 34/66 [00:05<00:05,  5.76it/s]evaluate for the 35-th batch, evaluate loss: 0.6239883303642273:  53%|█████████▌        | 35/66 [00:05<00:04,  6.37it/s]Epoch: 2, train for the 31-th batch, train loss: 0.48900794982910156:  12%|█▍          | 30/241 [00:04<00:29,  7.17it/s]Epoch: 2, train for the 31-th batch, train loss: 0.48900794982910156:  13%|█▌          | 31/241 [00:04<00:30,  6.88it/s]Epoch: 3, train for the 54-th batch, train loss: 0.4802034795284271:  36%|████▋        | 53/146 [00:08<00:13,  6.87it/s]Epoch: 3, train for the 54-th batch, train loss: 0.4802034795284271:  37%|████▊        | 54/146 [00:08<00:13,  6.65it/s]Epoch: 2, train for the 32-th batch, train loss: 0.409403920173645:  13%|█▊            | 31/241 [00:04<00:30,  6.88it/s]Epoch: 2, train for the 32-th batch, train loss: 0.409403920173645:  13%|█▊            | 32/241 [00:04<00:27,  7.52it/s]Epoch: 3, train for the 103-th batch, train loss: 0.39764347672462463:  86%|████████▌ | 102/119 [00:17<00:02,  5.72it/s]Epoch: 3, train for the 103-th batch, train loss: 0.39764347672462463:  87%|████████▋ | 103/119 [00:17<00:02,  5.86it/s]evaluate for the 23-th batch, evaluate loss: 0.6258545517921448:  48%|████████▌         | 22/46 [00:02<00:02,  9.25it/s]Epoch: 3, train for the 55-th batch, train loss: 0.5261355638504028:  37%|████▊        | 54/146 [00:08<00:13,  6.65it/s]Epoch: 3, train for the 55-th batch, train loss: 0.5261355638504028:  38%|████▉        | 55/146 [00:08<00:13,  6.74it/s]Epoch: 2, train for the 33-th batch, train loss: 0.6062723994255066:  13%|█▋           | 32/241 [00:04<00:27,  7.52it/s]evaluate for the 24-th batch, evaluate loss: 0.5806962251663208:  48%|████████▌         | 22/46 [00:02<00:02,  9.25it/s]Epoch: 2, train for the 33-th batch, train loss: 0.6062723994255066:  14%|█▊           | 33/241 [00:04<00:27,  7.51it/s]evaluate for the 24-th batch, evaluate loss: 0.5806962251663208:  52%|█████████▍        | 24/46 [00:02<00:02,  8.13it/s]Epoch: 1, train for the 249-th batch, train loss: 0.46485158801078796:  65%|██████▍   | 248/383 [01:09<00:43,  3.12it/s]Epoch: 3, train for the 104-th batch, train loss: 0.4199233949184418:  87%|█████████▌ | 103/119 [00:17<00:02,  5.86it/s]evaluate for the 36-th batch, evaluate loss: 0.5800761580467224:  53%|█████████▌        | 35/66 [00:06<00:04,  6.37it/s]evaluate for the 36-th batch, evaluate loss: 0.5800761580467224:  55%|█████████▊        | 36/66 [00:06<00:06,  4.87it/s]Epoch: 3, train for the 104-th batch, train loss: 0.4199233949184418:  87%|█████████▌ | 104/119 [00:17<00:02,  5.84it/s]Epoch: 1, train for the 249-th batch, train loss: 0.46485158801078796:  65%|██████▌   | 249/383 [01:09<00:51,  2.62it/s]Epoch: 3, train for the 56-th batch, train loss: 0.4746529757976532:  38%|████▉        | 55/146 [00:08<00:13,  6.74it/s]evaluate for the 25-th batch, evaluate loss: 0.6168219447135925:  52%|█████████▍        | 24/46 [00:02<00:02,  8.13it/s]Epoch: 3, train for the 56-th batch, train loss: 0.4746529757976532:  38%|████▉        | 56/146 [00:08<00:13,  6.53it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5158416628837585:  14%|█▊           | 33/241 [00:04<00:27,  7.51it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5158416628837585:  14%|█▊           | 34/241 [00:04<00:28,  7.24it/s]evaluate for the 37-th batch, evaluate loss: 0.6056184768676758:  55%|█████████▊        | 36/66 [00:06<00:06,  4.87it/s]evaluate for the 37-th batch, evaluate loss: 0.6056184768676758:  56%|██████████        | 37/66 [00:06<00:05,  5.70it/s]evaluate for the 26-th batch, evaluate loss: 0.6406298875808716:  52%|█████████▍        | 24/46 [00:02<00:02,  8.13it/s]evaluate for the 26-th batch, evaluate loss: 0.6406298875808716:  57%|██████████▏       | 26/46 [00:02<00:02,  8.78it/s]Epoch: 3, train for the 105-th batch, train loss: 0.4108065664768219:  87%|█████████▌ | 104/119 [00:17<00:02,  5.84it/s]Epoch: 3, train for the 105-th batch, train loss: 0.4108065664768219:  88%|█████████▋ | 105/119 [00:17<00:02,  5.94it/s]Epoch: 3, train for the 57-th batch, train loss: 0.49889668822288513:  38%|████▌       | 56/146 [00:08<00:13,  6.53it/s]Epoch: 3, train for the 57-th batch, train loss: 0.49889668822288513:  39%|████▋       | 57/146 [00:08<00:14,  6.36it/s]evaluate for the 27-th batch, evaluate loss: 0.6232945322990417:  57%|██████████▏       | 26/46 [00:02<00:02,  8.78it/s]evaluate for the 27-th batch, evaluate loss: 0.6232945322990417:  59%|██████████▌       | 27/46 [00:02<00:02,  8.95it/s]Epoch: 1, train for the 250-th batch, train loss: 0.45595699548721313:  65%|██████▌   | 249/383 [01:10<00:51,  2.62it/s]Epoch: 2, train for the 35-th batch, train loss: 0.5668654441833496:  14%|█▊           | 34/241 [00:04<00:28,  7.24it/s]Epoch: 2, train for the 35-th batch, train loss: 0.5668654441833496:  15%|█▉           | 35/241 [00:04<00:30,  6.77it/s]Epoch: 1, train for the 250-th batch, train loss: 0.45595699548721313:  65%|██████▌   | 250/383 [01:10<00:45,  2.92it/s]Epoch: 3, train for the 106-th batch, train loss: 0.41909652948379517:  88%|████████▊ | 105/119 [00:17<00:02,  5.94it/s]Epoch: 3, train for the 106-th batch, train loss: 0.41909652948379517:  89%|████████▉ | 106/119 [00:17<00:02,  5.99it/s]Epoch: 2, train for the 36-th batch, train loss: 0.5608273148536682:  15%|█▉           | 35/241 [00:05<00:30,  6.77it/s]evaluate for the 38-th batch, evaluate loss: 0.5409496426582336:  56%|██████████        | 37/66 [00:06<00:05,  5.70it/s]evaluate for the 38-th batch, evaluate loss: 0.5409496426582336:  58%|██████████▎       | 38/66 [00:06<00:05,  5.13it/s]Epoch: 3, train for the 58-th batch, train loss: 0.4908592998981476:  39%|█████        | 57/146 [00:08<00:14,  6.36it/s]Epoch: 3, train for the 58-th batch, train loss: 0.4908592998981476:  40%|█████▏       | 58/146 [00:08<00:13,  6.35it/s]evaluate for the 28-th batch, evaluate loss: 0.6114248633384705:  59%|██████████▌       | 27/46 [00:03<00:02,  8.95it/s]evaluate for the 28-th batch, evaluate loss: 0.6114248633384705:  61%|██████████▉       | 28/46 [00:03<00:02,  7.01it/s]evaluate for the 39-th batch, evaluate loss: 0.670185923576355:  58%|██████████▉        | 38/66 [00:06<00:05,  5.13it/s]evaluate for the 39-th batch, evaluate loss: 0.670185923576355:  59%|███████████▏       | 39/66 [00:06<00:04,  5.50it/s]Epoch: 3, train for the 107-th batch, train loss: 0.41538313031196594:  89%|████████▉ | 106/119 [00:17<00:02,  5.99it/s]Epoch: 2, train for the 37-th batch, train loss: 0.4748865067958832:  15%|█▉           | 35/241 [00:05<00:30,  6.77it/s]Epoch: 3, train for the 107-th batch, train loss: 0.41538313031196594:  90%|████████▉ | 107/119 [00:17<00:02,  5.85it/s]Epoch: 2, train for the 37-th batch, train loss: 0.4748865067958832:  15%|█▉           | 37/241 [00:05<00:28,  7.16it/s]Epoch: 3, train for the 59-th batch, train loss: 0.43010038137435913:  40%|████▊       | 58/146 [00:09<00:13,  6.35it/s]Epoch: 3, train for the 59-th batch, train loss: 0.43010038137435913:  40%|████▊       | 59/146 [00:09<00:14,  5.97it/s]Epoch: 1, train for the 251-th batch, train loss: 0.44151756167411804:  65%|██████▌   | 250/383 [01:10<00:45,  2.92it/s]evaluate for the 29-th batch, evaluate loss: 0.5872759222984314:  61%|██████████▉       | 28/46 [00:03<00:02,  7.01it/s]evaluate for the 40-th batch, evaluate loss: 0.6082252860069275:  59%|██████████▋       | 39/66 [00:06<00:04,  5.50it/s]evaluate for the 40-th batch, evaluate loss: 0.6082252860069275:  61%|██████████▉       | 40/66 [00:06<00:04,  6.26it/s]Epoch: 1, train for the 251-th batch, train loss: 0.44151756167411804:  66%|██████▌   | 251/383 [01:10<00:45,  2.92it/s]Epoch: 2, train for the 38-th batch, train loss: 0.5478553771972656:  15%|█▉           | 37/241 [00:05<00:28,  7.16it/s]evaluate for the 30-th batch, evaluate loss: 0.6168619990348816:  61%|██████████▉       | 28/46 [00:03<00:02,  7.01it/s]evaluate for the 30-th batch, evaluate loss: 0.6168619990348816:  65%|███████████▋      | 30/46 [00:03<00:01,  8.07it/s]Epoch: 3, train for the 108-th batch, train loss: 0.3160472512245178:  90%|█████████▉ | 107/119 [00:17<00:02,  5.85it/s]Epoch: 2, train for the 38-th batch, train loss: 0.5478553771972656:  16%|██           | 38/241 [00:05<00:29,  6.84it/s]Epoch: 3, train for the 108-th batch, train loss: 0.3160472512245178:  91%|█████████▉ | 108/119 [00:17<00:01,  5.77it/s]Epoch: 3, train for the 60-th batch, train loss: 0.48866361379623413:  40%|████▊       | 59/146 [00:09<00:14,  5.97it/s]Epoch: 3, train for the 60-th batch, train loss: 0.48866361379623413:  41%|████▉       | 60/146 [00:09<00:14,  6.08it/s]evaluate for the 41-th batch, evaluate loss: 0.5908733010292053:  61%|██████████▉       | 40/66 [00:07<00:04,  6.26it/s]evaluate for the 41-th batch, evaluate loss: 0.5908733010292053:  62%|███████████▏      | 41/66 [00:07<00:03,  6.34it/s]evaluate for the 31-th batch, evaluate loss: 0.4893370568752289:  65%|███████████▋      | 30/46 [00:03<00:01,  8.07it/s]evaluate for the 31-th batch, evaluate loss: 0.4893370568752289:  67%|████████████▏     | 31/46 [00:03<00:01,  8.31it/s]Epoch: 2, train for the 39-th batch, train loss: 0.5971401929855347:  16%|██           | 38/241 [00:05<00:29,  6.84it/s]Epoch: 2, train for the 39-th batch, train loss: 0.5971401929855347:  16%|██           | 39/241 [00:05<00:30,  6.65it/s]Epoch: 1, train for the 252-th batch, train loss: 0.37241896986961365:  66%|██████▌   | 251/383 [01:10<00:45,  2.92it/s]Epoch: 3, train for the 109-th batch, train loss: 0.39362001419067383:  91%|█████████ | 108/119 [00:18<00:01,  5.77it/s]Epoch: 1, train for the 252-th batch, train loss: 0.37241896986961365:  66%|██████▌   | 252/383 [01:10<00:42,  3.08it/s]Epoch: 3, train for the 61-th batch, train loss: 0.5165551900863647:  41%|█████▎       | 60/146 [00:09<00:14,  6.08it/s]Epoch: 3, train for the 109-th batch, train loss: 0.39362001419067383:  92%|█████████▏| 109/119 [00:18<00:01,  5.30it/s]Epoch: 3, train for the 61-th batch, train loss: 0.5165551900863647:  42%|█████▍       | 61/146 [00:09<00:15,  5.62it/s]Epoch: 2, train for the 40-th batch, train loss: 0.6314056515693665:  16%|██           | 39/241 [00:05<00:30,  6.65it/s]Epoch: 2, train for the 41-th batch, train loss: 0.5320712327957153:  16%|██           | 39/241 [00:05<00:30,  6.65it/s]evaluate for the 42-th batch, evaluate loss: 0.6476219296455383:  62%|███████████▏      | 41/66 [00:07<00:03,  6.34it/s]evaluate for the 42-th batch, evaluate loss: 0.6476219296455383:  64%|███████████▍      | 42/66 [00:07<00:04,  5.14it/s]Epoch: 2, train for the 41-th batch, train loss: 0.5320712327957153:  17%|██▏          | 41/241 [00:05<00:25,  7.80it/s]Epoch: 3, train for the 110-th batch, train loss: 0.4269176423549652:  92%|██████████ | 109/119 [00:18<00:01,  5.30it/s]Epoch: 3, train for the 62-th batch, train loss: 0.4659779965877533:  42%|█████▍       | 61/146 [00:09<00:15,  5.62it/s]Epoch: 3, train for the 110-th batch, train loss: 0.4269176423549652:  92%|██████████▏| 110/119 [00:18<00:01,  5.31it/s]Epoch: 3, train for the 62-th batch, train loss: 0.4659779965877533:  42%|█████▌       | 62/146 [00:09<00:15,  5.53it/s]Epoch: 1, train for the 253-th batch, train loss: 0.3574996888637543:  66%|███████▏   | 252/383 [01:10<00:42,  3.08it/s]evaluate for the 43-th batch, evaluate loss: 0.547818124294281:  64%|████████████       | 42/66 [00:07<00:04,  5.14it/s]evaluate for the 43-th batch, evaluate loss: 0.547818124294281:  65%|████████████▍      | 43/66 [00:07<00:03,  5.86it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5444819331169128:  17%|██▏          | 41/241 [00:05<00:25,  7.80it/s]evaluate for the 32-th batch, evaluate loss: 0.5535723567008972:  67%|████████████▏     | 31/46 [00:03<00:01,  8.31it/s]evaluate for the 32-th batch, evaluate loss: 0.5535723567008972:  70%|████████████▌     | 32/46 [00:03<00:02,  5.46it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5444819331169128:  17%|██▎          | 42/241 [00:05<00:25,  7.75it/s]Epoch: 1, train for the 253-th batch, train loss: 0.3574996888637543:  66%|███████▎   | 253/383 [01:11<00:41,  3.16it/s]evaluate for the 33-th batch, evaluate loss: 0.5873703360557556:  70%|████████████▌     | 32/46 [00:03<00:02,  5.46it/s]evaluate for the 44-th batch, evaluate loss: 0.6131318807601929:  65%|███████████▋      | 43/66 [00:07<00:03,  5.86it/s]evaluate for the 44-th batch, evaluate loss: 0.6131318807601929:  67%|████████████      | 44/66 [00:07<00:03,  6.57it/s]Epoch: 2, train for the 43-th batch, train loss: 0.6109426021575928:  17%|██▎          | 42/241 [00:06<00:25,  7.75it/s]Epoch: 2, train for the 43-th batch, train loss: 0.6109426021575928:  18%|██▎          | 43/241 [00:06<00:25,  7.71it/s]Epoch: 3, train for the 111-th batch, train loss: 0.47009581327438354:  92%|█████████▏| 110/119 [00:18<00:01,  5.31it/s]Epoch: 3, train for the 63-th batch, train loss: 0.45325177907943726:  42%|█████       | 62/146 [00:09<00:15,  5.53it/s]Epoch: 3, train for the 63-th batch, train loss: 0.45325177907943726:  43%|█████▏      | 63/146 [00:09<00:15,  5.23it/s]Epoch: 3, train for the 111-th batch, train loss: 0.47009581327438354:  93%|█████████▎| 111/119 [00:18<00:01,  5.03it/s]evaluate for the 34-th batch, evaluate loss: 0.5243903398513794:  70%|████████████▌     | 32/46 [00:04<00:02,  5.46it/s]evaluate for the 34-th batch, evaluate loss: 0.5243903398513794:  74%|█████████████▎    | 34/46 [00:04<00:01,  6.61it/s]Epoch: 2, train for the 44-th batch, train loss: 0.6274953484535217:  18%|██▎          | 43/241 [00:06<00:25,  7.71it/s]Epoch: 2, train for the 44-th batch, train loss: 0.6274953484535217:  18%|██▎          | 44/241 [00:06<00:25,  7.59it/s]evaluate for the 45-th batch, evaluate loss: 0.5961223244667053:  67%|████████████      | 44/66 [00:07<00:03,  6.57it/s]evaluate for the 45-th batch, evaluate loss: 0.5961223244667053:  68%|████████████▎     | 45/66 [00:07<00:03,  6.23it/s]Epoch: 1, train for the 254-th batch, train loss: 0.46136313676834106:  66%|██████▌   | 253/383 [01:11<00:41,  3.16it/s]Epoch: 3, train for the 64-th batch, train loss: 0.502214789390564:  43%|██████        | 63/146 [00:10<00:15,  5.23it/s]Epoch: 3, train for the 64-th batch, train loss: 0.502214789390564:  44%|██████▏       | 64/146 [00:10<00:14,  5.58it/s]Epoch: 3, train for the 112-th batch, train loss: 0.3865485191345215:  93%|██████████▎| 111/119 [00:18<00:01,  5.03it/s]Epoch: 1, train for the 254-th batch, train loss: 0.46136313676834106:  66%|██████▋   | 254/383 [01:11<00:40,  3.19it/s]Epoch: 3, train for the 112-th batch, train loss: 0.3865485191345215:  94%|██████████▎| 112/119 [00:18<00:01,  5.13it/s]Epoch: 2, train for the 45-th batch, train loss: 0.4848565459251404:  18%|██▎          | 44/241 [00:06<00:25,  7.59it/s]Epoch: 2, train for the 45-th batch, train loss: 0.4848565459251404:  19%|██▍          | 45/241 [00:06<00:25,  7.82it/s]Epoch: 3, train for the 65-th batch, train loss: 0.5294287204742432:  44%|█████▋       | 64/146 [00:10<00:14,  5.58it/s]Epoch: 3, train for the 65-th batch, train loss: 0.5294287204742432:  45%|█████▊       | 65/146 [00:10<00:14,  5.73it/s]Epoch: 3, train for the 113-th batch, train loss: 0.3953907787799835:  94%|██████████▎| 112/119 [00:18<00:01,  5.13it/s]Epoch: 3, train for the 113-th batch, train loss: 0.3953907787799835:  95%|██████████▍| 113/119 [00:18<00:01,  5.42it/s]Epoch: 2, train for the 46-th batch, train loss: 0.5329663157463074:  19%|██▍          | 45/241 [00:06<00:25,  7.82it/s]evaluate for the 35-th batch, evaluate loss: 0.5978566408157349:  74%|█████████████▎    | 34/46 [00:04<00:01,  6.61it/s]evaluate for the 35-th batch, evaluate loss: 0.5978566408157349:  76%|█████████████▋    | 35/46 [00:04<00:02,  5.17it/s]Epoch: 2, train for the 46-th batch, train loss: 0.5329663157463074:  19%|██▍          | 46/241 [00:06<00:27,  7.19it/s]Epoch: 1, train for the 255-th batch, train loss: 0.4402296245098114:  66%|███████▎   | 254/383 [01:11<00:40,  3.19it/s]evaluate for the 46-th batch, evaluate loss: 0.6413571238517761:  68%|████████████▎     | 45/66 [00:08<00:03,  6.23it/s]evaluate for the 46-th batch, evaluate loss: 0.6413571238517761:  70%|████████████▌     | 46/66 [00:08<00:04,  4.92it/s]Epoch: 3, train for the 66-th batch, train loss: 0.5127451419830322:  45%|█████▊       | 65/146 [00:10<00:14,  5.73it/s]Epoch: 3, train for the 66-th batch, train loss: 0.5127451419830322:  45%|█████▉       | 66/146 [00:10<00:13,  6.15it/s]Epoch: 1, train for the 255-th batch, train loss: 0.4402296245098114:  67%|███████▎   | 255/383 [01:11<00:39,  3.23it/s]evaluate for the 36-th batch, evaluate loss: 0.54514080286026:  76%|███████████████▏    | 35/46 [00:04<00:02,  5.17it/s]Epoch: 3, train for the 114-th batch, train loss: 0.371714323759079:  95%|███████████▍| 113/119 [00:19<00:01,  5.42it/s]evaluate for the 47-th batch, evaluate loss: 0.6072007417678833:  70%|████████████▌     | 46/66 [00:08<00:04,  4.92it/s]Epoch: 3, train for the 114-th batch, train loss: 0.371714323759079:  96%|███████████▍| 114/119 [00:19<00:00,  5.44it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5925211906433105:  19%|██▍          | 46/241 [00:06<00:27,  7.19it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5925211906433105:  20%|██▌          | 47/241 [00:06<00:27,  7.00it/s]evaluate for the 37-th batch, evaluate loss: 0.6245839595794678:  76%|█████████████▋    | 35/46 [00:04<00:02,  5.17it/s]evaluate for the 37-th batch, evaluate loss: 0.6245839595794678:  80%|██████████████▍   | 37/46 [00:04<00:01,  6.54it/s]Epoch: 3, train for the 67-th batch, train loss: 0.5263245701789856:  45%|█████▉       | 66/146 [00:10<00:13,  6.15it/s]Epoch: 3, train for the 67-th batch, train loss: 0.5263245701789856:  46%|█████▉       | 67/146 [00:10<00:12,  6.31it/s]Epoch: 1, train for the 256-th batch, train loss: 0.3788916766643524:  67%|███████▎   | 255/383 [01:11<00:39,  3.23it/s]Epoch: 2, train for the 48-th batch, train loss: 0.5559591054916382:  20%|██▌          | 47/241 [00:06<00:27,  7.00it/s]Epoch: 3, train for the 115-th batch, train loss: 0.37389618158340454:  96%|█████████▌| 114/119 [00:19<00:00,  5.44it/s]Epoch: 2, train for the 48-th batch, train loss: 0.5559591054916382:  20%|██▌          | 48/241 [00:06<00:26,  7.23it/s]Epoch: 3, train for the 115-th batch, train loss: 0.37389618158340454:  97%|█████████▋| 115/119 [00:19<00:00,  5.57it/s]Epoch: 1, train for the 256-th batch, train loss: 0.3788916766643524:  67%|███████▎   | 256/383 [01:11<00:36,  3.51it/s]Epoch: 3, train for the 68-th batch, train loss: 0.4604160487651825:  46%|█████▉       | 67/146 [00:10<00:12,  6.31it/s]Epoch: 3, train for the 68-th batch, train loss: 0.4604160487651825:  47%|██████       | 68/146 [00:10<00:12,  6.45it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5441213846206665:  20%|██▌          | 48/241 [00:06<00:26,  7.23it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5441213846206665:  20%|██▋          | 49/241 [00:06<00:25,  7.63it/s]Epoch: 3, train for the 116-th batch, train loss: 0.3756413161754608:  97%|██████████▋| 115/119 [00:19<00:00,  5.57it/s]evaluate for the 38-th batch, evaluate loss: 0.5400979518890381:  80%|██████████████▍   | 37/46 [00:04<00:01,  6.54it/s]evaluate for the 38-th batch, evaluate loss: 0.5400979518890381:  83%|██████████████▊   | 38/46 [00:04<00:01,  5.56it/s]Epoch: 3, train for the 116-th batch, train loss: 0.3756413161754608:  97%|██████████▋| 116/119 [00:19<00:00,  5.74it/s]evaluate for the 48-th batch, evaluate loss: 0.6299327611923218:  70%|████████████▌     | 46/66 [00:08<00:04,  4.92it/s]evaluate for the 48-th batch, evaluate loss: 0.6299327611923218:  73%|█████████████     | 48/66 [00:08<00:03,  4.74it/s]Epoch: 3, train for the 69-th batch, train loss: 0.4945654273033142:  47%|██████       | 68/146 [00:10<00:12,  6.45it/s]Epoch: 3, train for the 69-th batch, train loss: 0.4945654273033142:  47%|██████▏      | 69/146 [00:10<00:12,  6.11it/s]Epoch: 1, train for the 257-th batch, train loss: 0.5271421670913696:  67%|███████▎   | 256/383 [01:12<00:36,  3.51it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5497387051582336:  20%|██▋          | 49/241 [00:06<00:25,  7.63it/s]evaluate for the 39-th batch, evaluate loss: 0.5855106711387634:  83%|██████████████▊   | 38/46 [00:04<00:01,  5.56it/s]evaluate for the 39-th batch, evaluate loss: 0.5855106711387634:  85%|███████████████▎  | 39/46 [00:04<00:01,  6.11it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5497387051582336:  21%|██▋          | 50/241 [00:06<00:27,  6.98it/s]evaluate for the 49-th batch, evaluate loss: 0.5934609770774841:  73%|█████████████     | 48/66 [00:08<00:03,  4.74it/s]evaluate for the 49-th batch, evaluate loss: 0.5934609770774841:  74%|█████████████▎    | 49/66 [00:08<00:03,  5.33it/s]Epoch: 3, train for the 117-th batch, train loss: 0.335650235414505:  97%|███████████▋| 116/119 [00:19<00:00,  5.74it/s]Epoch: 1, train for the 257-th batch, train loss: 0.5271421670913696:  67%|███████▍   | 257/383 [01:12<00:36,  3.49it/s]Epoch: 3, train for the 117-th batch, train loss: 0.335650235414505:  98%|███████████▊| 117/119 [00:19<00:00,  6.02it/s]evaluate for the 40-th batch, evaluate loss: 0.5537577271461487:  85%|███████████████▎  | 39/46 [00:05<00:01,  6.11it/s]Epoch: 3, train for the 70-th batch, train loss: 0.503961980342865:  47%|██████▌       | 69/146 [00:10<00:12,  6.11it/s]Epoch: 3, train for the 70-th batch, train loss: 0.503961980342865:  48%|██████▋       | 70/146 [00:10<00:11,  6.39it/s]evaluate for the 50-th batch, evaluate loss: 0.644307017326355:  74%|██████████████     | 49/66 [00:08<00:03,  5.33it/s]evaluate for the 50-th batch, evaluate loss: 0.644307017326355:  76%|██████████████▍    | 50/66 [00:08<00:02,  6.01it/s]Epoch: 2, train for the 51-th batch, train loss: 0.5354145765304565:  21%|██▋          | 50/241 [00:07<00:27,  6.98it/s]Epoch: 2, train for the 51-th batch, train loss: 0.5354145765304565:  21%|██▊          | 51/241 [00:07<00:28,  6.74it/s]evaluate for the 41-th batch, evaluate loss: 0.5777798295021057:  85%|███████████████▎  | 39/46 [00:05<00:01,  6.11it/s]evaluate for the 41-th batch, evaluate loss: 0.5777798295021057:  89%|████████████████  | 41/46 [00:05<00:00,  7.36it/s]Epoch: 3, train for the 118-th batch, train loss: 0.3306829631328583:  98%|██████████▊| 117/119 [00:19<00:00,  6.02it/s]Epoch: 3, train for the 118-th batch, train loss: 0.3306829631328583:  99%|██████████▉| 118/119 [00:19<00:00,  5.63it/s]evaluate for the 51-th batch, evaluate loss: 0.6110612154006958:  76%|█████████████▋    | 50/66 [00:08<00:02,  6.01it/s]evaluate for the 51-th batch, evaluate loss: 0.6110612154006958:  77%|█████████████▉    | 51/66 [00:08<00:02,  6.43it/s]Epoch: 3, train for the 71-th batch, train loss: 0.5376206040382385:  48%|██████▏      | 70/146 [00:11<00:11,  6.39it/s]Epoch: 3, train for the 71-th batch, train loss: 0.5376206040382385:  49%|██████▎      | 71/146 [00:11<00:12,  6.03it/s]Epoch: 2, train for the 52-th batch, train loss: 0.5648178458213806:  21%|██▊          | 51/241 [00:07<00:28,  6.74it/s]Epoch: 2, train for the 52-th batch, train loss: 0.5648178458213806:  22%|██▊          | 52/241 [00:07<00:27,  6.76it/s]Epoch: 3, train for the 119-th batch, train loss: 0.33588695526123047:  99%|█████████▉| 118/119 [00:19<00:00,  5.63it/s]Epoch: 3, train for the 119-th batch, train loss: 0.33588695526123047: 100%|██████████| 119/119 [00:19<00:00,  6.22it/s]Epoch: 3, train for the 119-th batch, train loss: 0.33588695526123047: 100%|██████████| 119/119 [00:19<00:00,  5.96it/s]
Epoch: 2, train for the 53-th batch, train loss: 0.4098775386810303:  22%|██▊          | 52/241 [00:07<00:27,  6.76it/s]Epoch: 2, train for the 53-th batch, train loss: 0.4098775386810303:  22%|██▊          | 53/241 [00:07<00:25,  7.36it/s]  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 1, train for the 258-th batch, train loss: 0.38674861192703247:  67%|██████▋   | 257/383 [01:12<00:36,  3.49it/s]Epoch: 3, train for the 72-th batch, train loss: 0.5191794633865356:  49%|██████▎      | 71/146 [00:11<00:12,  6.03it/s]Epoch: 3, train for the 72-th batch, train loss: 0.5191794633865356:  49%|██████▍      | 72/146 [00:11<00:11,  6.26it/s]Epoch: 1, train for the 258-th batch, train loss: 0.38674861192703247:  67%|██████▋   | 258/383 [01:12<00:41,  2.98it/s]evaluate for the 1-th batch, evaluate loss: 0.6273154020309448:   0%|                            | 0/40 [00:00<?, ?it/s]Epoch: 2, train for the 54-th batch, train loss: 0.3671908378601074:  22%|██▊          | 53/241 [00:07<00:25,  7.36it/s]evaluate for the 2-th batch, evaluate loss: 0.6235141754150391:   0%|                            | 0/40 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.6235141754150391:   5%|█                   | 2/40 [00:00<00:02, 17.35it/s]Epoch: 2, train for the 54-th batch, train loss: 0.3671908378601074:  22%|██▉          | 54/241 [00:07<00:24,  7.63it/s]evaluate for the 42-th batch, evaluate loss: 0.5390602350234985:  89%|████████████████  | 41/46 [00:05<00:00,  7.36it/s]evaluate for the 42-th batch, evaluate loss: 0.5390602350234985:  91%|████████████████▍ | 42/46 [00:05<00:00,  5.20it/s]evaluate for the 52-th batch, evaluate loss: 0.607763409614563:  77%|██████████████▋    | 51/66 [00:09<00:02,  6.43it/s]evaluate for the 52-th batch, evaluate loss: 0.607763409614563:  79%|██████████████▉    | 52/66 [00:09<00:03,  4.64it/s]evaluate for the 3-th batch, evaluate loss: 0.6811743974685669:   5%|█                   | 2/40 [00:00<00:02, 17.35it/s]evaluate for the 43-th batch, evaluate loss: 0.6219194531440735:  91%|████████████████▍ | 42/46 [00:05<00:00,  5.20it/s]evaluate for the 43-th batch, evaluate loss: 0.6219194531440735:  93%|████████████████▊ | 43/46 [00:05<00:00,  5.72it/s]Epoch: 3, train for the 73-th batch, train loss: 0.5195245146751404:  49%|██████▍      | 72/146 [00:11<00:11,  6.26it/s]Epoch: 2, train for the 55-th batch, train loss: 0.35364866256713867:  22%|██▋         | 54/241 [00:07<00:24,  7.63it/s]Epoch: 3, train for the 73-th batch, train loss: 0.5195245146751404:  50%|██████▌      | 73/146 [00:11<00:14,  5.16it/s]evaluate for the 4-th batch, evaluate loss: 0.7533918619155884:   5%|█                   | 2/40 [00:00<00:02, 17.35it/s]evaluate for the 4-th batch, evaluate loss: 0.7533918619155884:  10%|██                  | 4/40 [00:00<00:02, 13.39it/s]Epoch: 2, train for the 55-th batch, train loss: 0.35364866256713867:  23%|██▋         | 55/241 [00:07<00:26,  6.95it/s]Epoch: 1, train for the 259-th batch, train loss: 0.4397454261779785:  67%|███████▍   | 258/383 [01:12<00:41,  2.98it/s]evaluate for the 53-th batch, evaluate loss: 0.5654237866401672:  79%|██████████████▏   | 52/66 [00:09<00:03,  4.64it/s]evaluate for the 53-th batch, evaluate loss: 0.5654237866401672:  80%|██████████████▍   | 53/66 [00:09<00:02,  5.27it/s]evaluate for the 44-th batch, evaluate loss: 0.5341999530792236:  93%|████████████████▊ | 43/46 [00:05<00:00,  5.72it/s]evaluate for the 44-th batch, evaluate loss: 0.5341999530792236:  96%|█████████████████▏| 44/46 [00:05<00:00,  6.41it/s]evaluate for the 5-th batch, evaluate loss: 0.7556340098381042:  10%|██                  | 4/40 [00:00<00:02, 13.39it/s]Epoch: 1, train for the 259-th batch, train loss: 0.4397454261779785:  68%|███████▍   | 259/383 [01:12<00:41,  3.02it/s]Epoch: 2, train for the 56-th batch, train loss: 0.38090044260025024:  23%|██▋         | 55/241 [00:07<00:26,  6.95it/s]Epoch: 3, train for the 74-th batch, train loss: 0.5033729672431946:  50%|██████▌      | 73/146 [00:11<00:14,  5.16it/s]Epoch: 3, train for the 74-th batch, train loss: 0.5033729672431946:  51%|██████▌      | 74/146 [00:11<00:12,  5.57it/s]Epoch: 2, train for the 56-th batch, train loss: 0.38090044260025024:  23%|██▊         | 56/241 [00:07<00:26,  6.94it/s]evaluate for the 6-th batch, evaluate loss: 0.6542579531669617:  10%|██                  | 4/40 [00:00<00:02, 13.39it/s]evaluate for the 6-th batch, evaluate loss: 0.6542579531669617:  15%|███                 | 6/40 [00:00<00:02, 12.93it/s]evaluate for the 7-th batch, evaluate loss: 0.7136290073394775:  15%|███                 | 6/40 [00:00<00:02, 12.93it/s]Epoch: 2, train for the 57-th batch, train loss: 0.4226805865764618:  23%|███          | 56/241 [00:07<00:26,  6.94it/s]Epoch: 2, train for the 57-th batch, train loss: 0.4226805865764618:  24%|███          | 57/241 [00:07<00:25,  7.10it/s]evaluate for the 54-th batch, evaluate loss: 0.6287439465522766:  80%|██████████████▍   | 53/66 [00:09<00:02,  5.27it/s]evaluate for the 54-th batch, evaluate loss: 0.6287439465522766:  82%|██████████████▋   | 54/66 [00:09<00:02,  4.69it/s]Epoch: 3, train for the 75-th batch, train loss: 0.5392663478851318:  51%|██████▌      | 74/146 [00:11<00:12,  5.57it/s]evaluate for the 8-th batch, evaluate loss: 0.6934549808502197:  15%|███                 | 6/40 [00:00<00:02, 12.93it/s]evaluate for the 8-th batch, evaluate loss: 0.6934549808502197:  20%|████                | 8/40 [00:00<00:02, 12.59it/s]Epoch: 3, train for the 75-th batch, train loss: 0.5392663478851318:  51%|██████▋      | 75/146 [00:11<00:13,  5.43it/s]Epoch: 2, train for the 58-th batch, train loss: 0.3270770311355591:  24%|███          | 57/241 [00:08<00:25,  7.10it/s]evaluate for the 9-th batch, evaluate loss: 0.6958143711090088:  20%|████                | 8/40 [00:00<00:02, 12.59it/s]evaluate for the 45-th batch, evaluate loss: 0.5247158408164978:  96%|█████████████████▏| 44/46 [00:06<00:00,  6.41it/s]evaluate for the 45-th batch, evaluate loss: 0.5247158408164978:  98%|█████████████████▌| 45/46 [00:06<00:00,  4.95it/s]Epoch: 2, train for the 58-th batch, train loss: 0.3270770311355591:  24%|███▏         | 58/241 [00:08<00:26,  7.04it/s]evaluate for the 55-th batch, evaluate loss: 0.5500496625900269:  82%|██████████████▋   | 54/66 [00:09<00:02,  4.69it/s]Epoch: 1, train for the 260-th batch, train loss: 0.40716859698295593:  68%|██████▊   | 259/383 [01:13<00:41,  3.02it/s]evaluate for the 55-th batch, evaluate loss: 0.5500496625900269:  83%|███████████████   | 55/66 [00:09<00:02,  5.18it/s]evaluate for the 10-th batch, evaluate loss: 0.7781312465667725:  20%|███▊               | 8/40 [00:00<00:02, 12.59it/s]evaluate for the 10-th batch, evaluate loss: 0.7781312465667725:  25%|████▌             | 10/40 [00:00<00:02, 12.77it/s]Epoch: 3, train for the 76-th batch, train loss: 0.5071893334388733:  51%|██████▋      | 75/146 [00:12<00:13,  5.43it/s]Epoch: 3, train for the 76-th batch, train loss: 0.5071893334388733:  52%|██████▊      | 76/146 [00:12<00:12,  5.58it/s]Epoch: 1, train for the 260-th batch, train loss: 0.40716859698295593:  68%|██████▊   | 260/383 [01:13<00:43,  2.81it/s]evaluate for the 46-th batch, evaluate loss: 0.49382010102272034:  98%|████████████████▋| 45/46 [00:06<00:00,  4.95it/s]evaluate for the 46-th batch, evaluate loss: 0.49382010102272034: 100%|█████████████████| 46/46 [00:06<00:00,  7.50it/s]
evaluate for the 11-th batch, evaluate loss: 0.6432615518569946:  25%|████▌             | 10/40 [00:00<00:02, 12.77it/s]evaluate for the 56-th batch, evaluate loss: 0.6057084798812866:  83%|███████████████   | 55/66 [00:09<00:02,  5.18it/s]evaluate for the 56-th batch, evaluate loss: 0.6057084798812866:  85%|███████████████▎  | 56/66 [00:09<00:01,  5.91it/s]Epoch: 3, train for the 77-th batch, train loss: 0.5666292905807495:  52%|██████▊      | 76/146 [00:12<00:12,  5.58it/s]evaluate for the 57-th batch, evaluate loss: 0.6482730507850647:  85%|███████████████▎  | 56/66 [00:09<00:01,  5.91it/s]evaluate for the 12-th batch, evaluate loss: 0.6377156972885132:  25%|████▌             | 10/40 [00:00<00:02, 12.77it/s]evaluate for the 12-th batch, evaluate loss: 0.6377156972885132:  30%|█████▍            | 12/40 [00:00<00:02, 12.34it/s]Epoch: 3, train for the 77-th batch, train loss: 0.5666292905807495:  53%|██████▊      | 77/146 [00:12<00:11,  5.85it/s]Epoch: 1, train for the 261-th batch, train loss: 0.36742842197418213:  68%|██████▊   | 260/383 [01:13<00:43,  2.81it/s]evaluate for the 13-th batch, evaluate loss: 0.644813060760498:  30%|█████▋             | 12/40 [00:01<00:02, 12.34it/s]evaluate for the 58-th batch, evaluate loss: 0.6016325950622559:  85%|███████████████▎  | 56/66 [00:10<00:01,  5.91it/s]evaluate for the 58-th batch, evaluate loss: 0.6016325950622559:  88%|███████████████▊  | 58/66 [00:10<00:01,  7.38it/s]Epoch: 1, train for the 261-th batch, train loss: 0.36742842197418213:  68%|██████▊   | 261/383 [01:13<00:40,  3.02it/s]evaluate for the 14-th batch, evaluate loss: 0.6641296744346619:  30%|█████▍            | 12/40 [00:01<00:02, 12.34it/s]evaluate for the 14-th batch, evaluate loss: 0.6641296744346619:  35%|██████▎           | 14/40 [00:01<00:02, 12.48it/s]Epoch: 3, train for the 78-th batch, train loss: 0.4924261271953583:  53%|██████▊      | 77/146 [00:12<00:11,  5.85it/s]Epoch: 3, train for the 78-th batch, train loss: 0.4924261271953583:  53%|██████▉      | 78/146 [00:12<00:11,  5.94it/s]evaluate for the 59-th batch, evaluate loss: 0.5834898948669434:  88%|███████████████▊  | 58/66 [00:10<00:01,  7.38it/s]evaluate for the 59-th batch, evaluate loss: 0.5834898948669434:  89%|████████████████  | 59/66 [00:10<00:00,  7.79it/s]evaluate for the 15-th batch, evaluate loss: 0.6691192984580994:  35%|██████▎           | 14/40 [00:01<00:02, 12.48it/s]evaluate for the 16-th batch, evaluate loss: 0.7457098364830017:  35%|██████▎           | 14/40 [00:01<00:02, 12.48it/s]evaluate for the 16-th batch, evaluate loss: 0.7457098364830017:  40%|███████▏          | 16/40 [00:01<00:01, 12.40it/s]Epoch: 3, train for the 79-th batch, train loss: 0.5328263640403748:  53%|██████▉      | 78/146 [00:12<00:11,  5.94it/s]Epoch: 3, train for the 79-th batch, train loss: 0.5328263640403748:  54%|███████      | 79/146 [00:12<00:11,  6.00it/s]Epoch: 2, train for the 59-th batch, train loss: 0.5885756611824036:  24%|███▏         | 58/241 [00:08<00:26,  7.04it/s]Epoch: 2, train for the 59-th batch, train loss: 0.5885756611824036:  24%|███▏         | 59/241 [00:08<00:49,  3.66it/s]Epoch: 1, train for the 262-th batch, train loss: 0.4195174276828766:  68%|███████▍   | 261/383 [01:13<00:40,  3.02it/s]evaluate for the 60-th batch, evaluate loss: 0.5679419636726379:  89%|████████████████  | 59/66 [00:10<00:00,  7.79it/s]evaluate for the 60-th batch, evaluate loss: 0.5679419636726379:  91%|████████████████▎ | 60/66 [00:10<00:00,  7.11it/s]evaluate for the 17-th batch, evaluate loss: 0.6981672644615173:  40%|███████▏          | 16/40 [00:01<00:01, 12.40it/s]Epoch: 1, train for the 262-th batch, train loss: 0.4195174276828766:  68%|███████▌   | 262/383 [01:13<00:38,  3.14it/s]evaluate for the 18-th batch, evaluate loss: 0.6562844514846802:  40%|███████▏          | 16/40 [00:01<00:01, 12.40it/s]evaluate for the 18-th batch, evaluate loss: 0.6562844514846802:  45%|████████          | 18/40 [00:01<00:01, 12.33it/s]Epoch: 3, train for the 80-th batch, train loss: 0.5121508836746216:  54%|███████      | 79/146 [00:12<00:11,  6.00it/s]  0%|                                                                                            | 0/25 [00:00<?, ?it/s]Epoch: 3, train for the 80-th batch, train loss: 0.5121508836746216:  55%|███████      | 80/146 [00:12<00:11,  5.96it/s]evaluate for the 61-th batch, evaluate loss: 0.6318992972373962:  91%|████████████████▎ | 60/66 [00:10<00:00,  7.11it/s]evaluate for the 61-th batch, evaluate loss: 0.6318992972373962:  92%|████████████████▋ | 61/66 [00:10<00:00,  7.31it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5595542192459106:  24%|███▏         | 59/241 [00:08<00:49,  3.66it/s]evaluate for the 19-th batch, evaluate loss: 0.7631853818893433:  45%|████████          | 18/40 [00:01<00:01, 12.33it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5595542192459106:  25%|███▏         | 60/241 [00:08<00:45,  3.94it/s]evaluate for the 1-th batch, evaluate loss: 1.0287175178527832:   0%|                            | 0/25 [00:00<?, ?it/s]Epoch: 3, train for the 81-th batch, train loss: 0.4821029603481293:  55%|███████      | 80/146 [00:12<00:11,  5.96it/s]evaluate for the 2-th batch, evaluate loss: 0.9966548681259155:   0%|                            | 0/25 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.9966548681259155:   8%|█▌                  | 2/25 [00:00<00:01, 12.86it/s]evaluate for the 20-th batch, evaluate loss: 0.7518344521522522:  45%|████████          | 18/40 [00:01<00:01, 12.33it/s]evaluate for the 20-th batch, evaluate loss: 0.7518344521522522:  50%|█████████         | 20/40 [00:01<00:01, 12.17it/s]Epoch: 3, train for the 81-th batch, train loss: 0.4821029603481293:  55%|███████▏     | 81/146 [00:12<00:10,  5.99it/s]evaluate for the 21-th batch, evaluate loss: 0.615470826625824:  50%|█████████▌         | 20/40 [00:01<00:01, 12.17it/s]evaluate for the 3-th batch, evaluate loss: 0.9592608213424683:   8%|█▌                  | 2/25 [00:00<00:01, 12.86it/s]evaluate for the 22-th batch, evaluate loss: 0.6446295976638794:  50%|█████████         | 20/40 [00:01<00:01, 12.17it/s]Epoch: 1, train for the 263-th batch, train loss: 0.4972754418849945:  68%|███████▌   | 262/383 [01:14<00:38,  3.14it/s]evaluate for the 4-th batch, evaluate loss: 0.9368763566017151:   8%|█▌                  | 2/25 [00:00<00:01, 12.86it/s]evaluate for the 4-th batch, evaluate loss: 0.9368763566017151:  16%|███▏                | 4/25 [00:00<00:01, 13.36it/s]Epoch: 3, train for the 82-th batch, train loss: 0.531223714351654:  55%|███████▊      | 81/146 [00:13<00:10,  5.99it/s]evaluate for the 23-th batch, evaluate loss: 0.5966921448707581:  50%|█████████         | 20/40 [00:01<00:01, 12.17it/s]evaluate for the 23-th batch, evaluate loss: 0.5966921448707581:  57%|██████████▎       | 23/40 [00:01<00:01, 14.04it/s]evaluate for the 62-th batch, evaluate loss: 0.6339357495307922:  92%|████████████████▋ | 61/66 [00:10<00:00,  7.31it/s]evaluate for the 62-th batch, evaluate loss: 0.6339357495307922:  94%|████████████████▉ | 62/66 [00:10<00:00,  5.39it/s]Epoch: 3, train for the 82-th batch, train loss: 0.531223714351654:  56%|███████▊      | 82/146 [00:13<00:10,  6.06it/s]Epoch: 1, train for the 263-th batch, train loss: 0.4972754418849945:  69%|███████▌   | 263/383 [01:14<00:42,  2.85it/s]evaluate for the 5-th batch, evaluate loss: 0.9020642042160034:  16%|███▏                | 4/25 [00:00<00:01, 13.36it/s]evaluate for the 24-th batch, evaluate loss: 0.714100182056427:  57%|██████████▉        | 23/40 [00:01<00:01, 14.04it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5405358076095581:  25%|███▏         | 60/241 [00:09<00:45,  3.94it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5405358076095581:  25%|███▎         | 61/241 [00:09<00:51,  3.51it/s]evaluate for the 63-th batch, evaluate loss: 0.5458723306655884:  94%|████████████████▉ | 62/66 [00:10<00:00,  5.39it/s]evaluate for the 63-th batch, evaluate loss: 0.5458723306655884:  95%|█████████████████▏| 63/66 [00:10<00:00,  6.09it/s]evaluate for the 6-th batch, evaluate loss: 0.9331139326095581:  16%|███▏                | 4/25 [00:00<00:01, 13.36it/s]evaluate for the 6-th batch, evaluate loss: 0.9331139326095581:  24%|████▊               | 6/25 [00:00<00:01, 12.67it/s]Epoch: 3, train for the 83-th batch, train loss: 0.4963829815387726:  56%|███████▎     | 82/146 [00:13<00:10,  6.06it/s]evaluate for the 25-th batch, evaluate loss: 0.6654968857765198:  57%|██████████▎       | 23/40 [00:01<00:01, 14.04it/s]evaluate for the 25-th batch, evaluate loss: 0.6654968857765198:  62%|███████████▎      | 25/40 [00:01<00:01, 13.14it/s]Epoch: 3, train for the 83-th batch, train loss: 0.4963829815387726:  57%|███████▍     | 83/146 [00:13<00:10,  5.85it/s]evaluate for the 7-th batch, evaluate loss: 0.8770084381103516:  24%|████▊               | 6/25 [00:00<00:01, 12.67it/s]evaluate for the 26-th batch, evaluate loss: 0.6807339191436768:  62%|███████████▎      | 25/40 [00:02<00:01, 13.14it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5897544026374817:  25%|███▎         | 61/241 [00:09<00:51,  3.51it/s]evaluate for the 64-th batch, evaluate loss: 0.5674316883087158:  95%|█████████████████▏| 63/66 [00:11<00:00,  6.09it/s]evaluate for the 64-th batch, evaluate loss: 0.5674316883087158:  97%|█████████████████▍| 64/66 [00:11<00:00,  6.10it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5897544026374817:  26%|███▎         | 62/241 [00:09<00:45,  3.93it/s]evaluate for the 8-th batch, evaluate loss: 0.8924825191497803:  24%|████▊               | 6/25 [00:00<00:01, 12.67it/s]evaluate for the 8-th batch, evaluate loss: 0.8924825191497803:  32%|██████▍             | 8/25 [00:00<00:01, 11.95it/s]evaluate for the 27-th batch, evaluate loss: 0.7146482467651367:  62%|███████████▎      | 25/40 [00:02<00:01, 13.14it/s]evaluate for the 27-th batch, evaluate loss: 0.7146482467651367:  68%|████████████▏     | 27/40 [00:02<00:01, 12.37it/s]Epoch: 3, train for the 84-th batch, train loss: 0.4718727469444275:  57%|███████▍     | 83/146 [00:13<00:10,  5.85it/s]Epoch: 3, train for the 84-th batch, train loss: 0.4718727469444275:  58%|███████▍     | 84/146 [00:13<00:10,  5.73it/s]evaluate for the 65-th batch, evaluate loss: 0.6573113799095154:  97%|█████████████████▍| 64/66 [00:11<00:00,  6.10it/s]evaluate for the 65-th batch, evaluate loss: 0.6573113799095154:  98%|█████████████████▋| 65/66 [00:11<00:00,  6.41it/s]evaluate for the 28-th batch, evaluate loss: 0.6521145701408386:  68%|████████████▏     | 27/40 [00:02<00:01, 12.37it/s]Epoch: 2, train for the 63-th batch, train loss: 0.7858088612556458:  26%|███▎         | 62/241 [00:09<00:45,  3.93it/s]evaluate for the 9-th batch, evaluate loss: 0.8231263756752014:  32%|██████▍             | 8/25 [00:00<00:01, 11.95it/s]Epoch: 2, train for the 63-th batch, train loss: 0.7858088612556458:  26%|███▍         | 63/241 [00:09<00:42,  4.18it/s]Epoch: 3, train for the 85-th batch, train loss: 0.5233991742134094:  58%|███████▍     | 84/146 [00:13<00:10,  5.73it/s]evaluate for the 29-th batch, evaluate loss: 0.7041295170783997:  68%|████████████▏     | 27/40 [00:02<00:01, 12.37it/s]evaluate for the 29-th batch, evaluate loss: 0.7041295170783997:  72%|█████████████     | 29/40 [00:02<00:00, 12.40it/s]Epoch: 3, train for the 85-th batch, train loss: 0.5233991742134094:  58%|███████▌     | 85/146 [00:13<00:10,  5.96it/s]evaluate for the 10-th batch, evaluate loss: 0.8426982164382935:  32%|██████             | 8/25 [00:00<00:01, 11.95it/s]evaluate for the 10-th batch, evaluate loss: 0.8426982164382935:  40%|███████▏          | 10/25 [00:00<00:01, 10.63it/s]evaluate for the 30-th batch, evaluate loss: 0.668241560459137:  72%|█████████████▊     | 29/40 [00:02<00:00, 12.40it/s]Epoch: 2, train for the 64-th batch, train loss: 0.31406110525131226:  26%|███▏        | 63/241 [00:09<00:42,  4.18it/s]Epoch: 2, train for the 64-th batch, train loss: 0.31406110525131226:  27%|███▏        | 64/241 [00:09<00:37,  4.66it/s]evaluate for the 11-th batch, evaluate loss: 0.7986418008804321:  40%|███████▏          | 10/25 [00:00<00:01, 10.63it/s]Epoch: 1, train for the 264-th batch, train loss: 0.3823075294494629:  69%|███████▌   | 263/383 [01:14<00:42,  2.85it/s]Epoch: 3, train for the 86-th batch, train loss: 0.4814210534095764:  58%|███████▌     | 85/146 [00:13<00:10,  5.96it/s]evaluate for the 31-th batch, evaluate loss: 0.7144385576248169:  72%|█████████████     | 29/40 [00:02<00:00, 12.40it/s]evaluate for the 31-th batch, evaluate loss: 0.7144385576248169:  78%|█████████████▉    | 31/40 [00:02<00:00, 12.42it/s]evaluate for the 66-th batch, evaluate loss: 0.5977444052696228:  98%|█████████████████▋| 65/66 [00:11<00:00,  6.41it/s]evaluate for the 66-th batch, evaluate loss: 0.5977444052696228: 100%|██████████████████| 66/66 [00:11<00:00,  5.23it/s]evaluate for the 66-th batch, evaluate loss: 0.5977444052696228: 100%|██████████████████| 66/66 [00:11<00:00,  5.76it/s]
Epoch: 3, train for the 86-th batch, train loss: 0.4814210534095764:  59%|███████▋     | 86/146 [00:13<00:10,  6.00it/s]Epoch: 1, train for the 264-th batch, train loss: 0.3823075294494629:  69%|███████▌   | 264/383 [01:15<00:53,  2.22it/s]evaluate for the 12-th batch, evaluate loss: 0.6968457102775574:  40%|███████▏          | 10/25 [00:01<00:01, 10.63it/s]evaluate for the 12-th batch, evaluate loss: 0.6968457102775574:  48%|████████▋         | 12/25 [00:01<00:01, 11.14it/s]evaluate for the 32-th batch, evaluate loss: 0.655056893825531:  78%|██████████████▋    | 31/40 [00:02<00:00, 12.42it/s]Epoch: 2, train for the 65-th batch, train loss: 0.3669743537902832:  27%|███▍         | 64/241 [00:09<00:37,  4.66it/s]Epoch: 2, train for the 65-th batch, train loss: 0.3669743537902832:  27%|███▌         | 65/241 [00:09<00:35,  4.95it/s]evaluate for the 13-th batch, evaluate loss: 0.7176530957221985:  48%|████████▋         | 12/25 [00:01<00:01, 11.14it/s]Epoch: 3, train for the 87-th batch, train loss: 0.5192610621452332:  59%|███████▋     | 86/146 [00:13<00:10,  6.00it/s]Epoch: 3, train for the 87-th batch, train loss: 0.5192610621452332:  60%|███████▋     | 87/146 [00:13<00:09,  6.13it/s]evaluate for the 33-th batch, evaluate loss: 0.6165580749511719:  78%|█████████████▉    | 31/40 [00:02<00:00, 12.42it/s]evaluate for the 33-th batch, evaluate loss: 0.6165580749511719:  82%|██████████████▊   | 33/40 [00:02<00:00, 12.34it/s]Epoch: 1, train for the 265-th batch, train loss: 0.3685702383518219:  69%|███████▌   | 264/383 [01:15<00:53,  2.22it/s]evaluate for the 34-th batch, evaluate loss: 0.6312189698219299:  82%|██████████████▊   | 33/40 [00:02<00:00, 12.34it/s]evaluate for the 14-th batch, evaluate loss: 0.7282243371009827:  48%|████████▋         | 12/25 [00:01<00:01, 11.14it/s]evaluate for the 14-th batch, evaluate loss: 0.7282243371009827:  56%|██████████        | 14/25 [00:01<00:01, 10.54it/s]Epoch: 1, train for the 265-th batch, train loss: 0.3685702383518219:  69%|███████▌   | 265/383 [01:15<00:45,  2.58it/s]Epoch: 2, train for the 66-th batch, train loss: 0.2961386740207672:  27%|███▌         | 65/241 [00:10<00:35,  4.95it/s]Epoch: 2, train for the 66-th batch, train loss: 0.2961386740207672:  27%|███▌         | 66/241 [00:10<00:34,  5.11it/s]Epoch: 3, train for the 88-th batch, train loss: 0.5208870768547058:  60%|███████▋     | 87/146 [00:14<00:09,  6.13it/s]evaluate for the 35-th batch, evaluate loss: 0.7216988801956177:  82%|██████████████▊   | 33/40 [00:02<00:00, 12.34it/s]evaluate for the 35-th batch, evaluate loss: 0.7216988801956177:  88%|███████████████▊  | 35/40 [00:02<00:00, 12.31it/s]Epoch: 3, train for the 88-th batch, train loss: 0.5208870768547058:  60%|███████▊     | 88/146 [00:14<00:09,  6.01it/s]evaluate for the 15-th batch, evaluate loss: 0.7616470456123352:  56%|██████████        | 14/25 [00:01<00:01, 10.54it/s]evaluate for the 36-th batch, evaluate loss: 0.6686064004898071:  88%|███████████████▊  | 35/40 [00:02<00:00, 12.31it/s]evaluate for the 16-th batch, evaluate loss: 0.7634092569351196:  56%|██████████        | 14/25 [00:01<00:01, 10.54it/s]evaluate for the 16-th batch, evaluate loss: 0.7634092569351196:  64%|███████████▌      | 16/25 [00:01<00:00,  9.88it/s]Epoch: 2, train for the 67-th batch, train loss: 0.402828574180603:  27%|███▊          | 66/241 [00:10<00:34,  5.11it/s]Epoch: 2, train for the 67-th batch, train loss: 0.402828574180603:  28%|███▉          | 67/241 [00:10<00:32,  5.27it/s]evaluate for the 37-th batch, evaluate loss: 0.6645253896713257:  88%|███████████████▊  | 35/40 [00:02<00:00, 12.31it/s]evaluate for the 37-th batch, evaluate loss: 0.6645253896713257:  92%|████████████████▋ | 37/40 [00:02<00:00, 11.95it/s]Epoch: 3, train for the 89-th batch, train loss: 0.5077849626541138:  60%|███████▊     | 88/146 [00:14<00:09,  6.01it/s]Epoch: 3, train for the 89-th batch, train loss: 0.5077849626541138:  61%|███████▉     | 89/146 [00:14<00:09,  5.82it/s]evaluate for the 17-th batch, evaluate loss: 0.6865864992141724:  64%|███████████▌      | 16/25 [00:01<00:00,  9.88it/s]evaluate for the 38-th batch, evaluate loss: 0.6670379638671875:  92%|████████████████▋ | 37/40 [00:03<00:00, 11.95it/s]Epoch: 1, train for the 266-th batch, train loss: 0.46047279238700867:  69%|██████▉   | 265/383 [01:15<00:45,  2.58it/s]Epoch: 2, train for the 68-th batch, train loss: 0.4910845160484314:  28%|███▌         | 67/241 [00:10<00:32,  5.27it/s]Epoch: 2, train for the 68-th batch, train loss: 0.4910845160484314:  28%|███▋         | 68/241 [00:10<00:30,  5.67it/s]Epoch: 1, train for the 266-th batch, train loss: 0.46047279238700867:  69%|██████▉   | 266/383 [01:15<00:45,  2.56it/s]evaluate for the 39-th batch, evaluate loss: 0.6925637722015381:  92%|████████████████▋ | 37/40 [00:03<00:00, 11.95it/s]evaluate for the 39-th batch, evaluate loss: 0.6925637722015381:  98%|█████████████████▌| 39/40 [00:03<00:00, 12.16it/s]evaluate for the 18-th batch, evaluate loss: 0.6951143741607666:  64%|███████████▌      | 16/25 [00:01<00:00,  9.88it/s]evaluate for the 18-th batch, evaluate loss: 0.6951143741607666:  72%|████████████▉     | 18/25 [00:01<00:00,  9.92it/s]Epoch: 3, train for the 90-th batch, train loss: 0.519810140132904:  61%|████████▌     | 89/146 [00:14<00:09,  5.82it/s]Epoch: 3, train for the 90-th batch, train loss: 0.519810140132904:  62%|████████▋     | 90/146 [00:14<00:09,  5.81it/s]evaluate for the 40-th batch, evaluate loss: 0.5185670852661133:  98%|█████████████████▌| 39/40 [00:03<00:00, 12.16it/s]evaluate for the 40-th batch, evaluate loss: 0.5185670852661133: 100%|██████████████████| 40/40 [00:03<00:00, 12.67it/s]
Epoch: 2, train for the 69-th batch, train loss: 0.62836754322052:  28%|████▏          | 68/241 [00:10<00:30,  5.67it/s]Epoch: 2, train for the 69-th batch, train loss: 0.62836754322052:  29%|████▎          | 69/241 [00:10<00:29,  5.92it/s]evaluate for the 19-th batch, evaluate loss: 0.6080952286720276:  72%|████████████▉     | 18/25 [00:01<00:00,  9.92it/s]Epoch: 3, train for the 91-th batch, train loss: 0.5334609150886536:  62%|████████     | 90/146 [00:14<00:09,  5.81it/s]Epoch: 3, train for the 91-th batch, train loss: 0.5334609150886536:  62%|████████     | 91/146 [00:14<00:09,  6.04it/s]evaluate for the 20-th batch, evaluate loss: 0.623873770236969:  72%|█████████████▋     | 18/25 [00:01<00:00,  9.92it/s]evaluate for the 20-th batch, evaluate loss: 0.623873770236969:  80%|███████████████▏   | 20/25 [00:01<00:00, 10.13it/s]Epoch: 1, train for the 267-th batch, train loss: 0.3837848901748657:  69%|███████▋   | 266/383 [01:15<00:45,  2.56it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5148037672042847:  29%|███▋         | 69/241 [00:10<00:29,  5.92it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5148037672042847:  29%|███▊         | 70/241 [00:10<00:27,  6.32it/s]Epoch: 1, train for the 267-th batch, train loss: 0.3837848901748657:  70%|███████▋   | 267/383 [01:15<00:40,  2.83it/s]evaluate for the 21-th batch, evaluate loss: 0.6468310356140137:  80%|██████████████▍   | 20/25 [00:01<00:00, 10.13it/s]  0%|                                                                                            | 0/21 [00:00<?, ?it/s]Epoch: 3, train for the 92-th batch, train loss: 0.5690504908561707:  62%|████████     | 91/146 [00:14<00:09,  6.04it/s]Epoch: 3, train for the 92-th batch, train loss: 0.5690504908561707:  63%|████████▏    | 92/146 [00:14<00:08,  6.16it/s]evaluate for the 1-th batch, evaluate loss: 0.9825443625450134:   0%|                            | 0/21 [00:00<?, ?it/s]evaluate for the 22-th batch, evaluate loss: 0.573380172252655:  80%|███████████████▏   | 20/25 [00:02<00:00, 10.13it/s]evaluate for the 22-th batch, evaluate loss: 0.573380172252655:  88%|████████████████▋  | 22/25 [00:02<00:00, 10.38it/s]Epoch: 2, train for the 71-th batch, train loss: 0.6113629937171936:  29%|███▊         | 70/241 [00:10<00:27,  6.32it/s]Epoch: 2, train for the 71-th batch, train loss: 0.6113629937171936:  29%|███▊         | 71/241 [00:10<00:26,  6.37it/s]evaluate for the 2-th batch, evaluate loss: 1.1022042036056519:   0%|                            | 0/21 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 1.1022042036056519:  10%|█▉                  | 2/21 [00:00<00:01, 13.92it/s]Epoch: 3, train for the 93-th batch, train loss: 0.4979112148284912:  63%|████████▏    | 92/146 [00:14<00:08,  6.16it/s]Epoch: 3, train for the 93-th batch, train loss: 0.4979112148284912:  64%|████████▎    | 93/146 [00:14<00:08,  6.34it/s]evaluate for the 23-th batch, evaluate loss: 0.5686349272727966:  88%|███████████████▊  | 22/25 [00:02<00:00, 10.38it/s]evaluate for the 3-th batch, evaluate loss: 1.103345274925232:  10%|██                   | 2/21 [00:00<00:01, 13.92it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5535662174224854:  29%|███▊         | 71/241 [00:11<00:26,  6.37it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5535662174224854:  30%|███▉         | 72/241 [00:11<00:25,  6.55it/s]evaluate for the 24-th batch, evaluate loss: 0.6016027927398682:  88%|███████████████▊  | 22/25 [00:02<00:00, 10.38it/s]evaluate for the 24-th batch, evaluate loss: 0.6016027927398682:  96%|█████████████████▎| 24/25 [00:02<00:00, 10.18it/s]evaluate for the 4-th batch, evaluate loss: 0.9720370173454285:  10%|█▉                  | 2/21 [00:00<00:01, 13.92it/s]evaluate for the 4-th batch, evaluate loss: 0.9720370173454285:  19%|███▊                | 4/21 [00:00<00:01, 12.11it/s]Epoch: 1, train for the 268-th batch, train loss: 0.39729663729667664:  70%|██████▉   | 267/383 [01:16<00:40,  2.83it/s]Epoch: 3, train for the 94-th batch, train loss: 0.48880279064178467:  64%|███████▋    | 93/146 [00:15<00:08,  6.34it/s]Epoch: 3, train for the 94-th batch, train loss: 0.48880279064178467:  64%|███████▋    | 94/146 [00:15<00:08,  6.26it/s]Epoch: 1, train for the 268-th batch, train loss: 0.39729663729667664:  70%|██████▉   | 268/383 [01:16<00:42,  2.71it/s]evaluate for the 25-th batch, evaluate loss: 0.5278549194335938:  96%|█████████████████▎| 24/25 [00:02<00:00, 10.18it/s]evaluate for the 25-th batch, evaluate loss: 0.5278549194335938: 100%|██████████████████| 25/25 [00:02<00:00, 10.66it/s]
evaluate for the 5-th batch, evaluate loss: 1.015318751335144:  19%|████                 | 4/21 [00:00<00:01, 12.11it/s]Epoch: 2, train for the 73-th batch, train loss: 0.49706166982650757:  30%|███▌        | 72/241 [00:11<00:25,  6.55it/s]Epoch: 2, train for the 73-th batch, train loss: 0.49706166982650757:  30%|███▋        | 73/241 [00:11<00:27,  6.07it/s]evaluate for the 6-th batch, evaluate loss: 0.9878997802734375:  19%|███▊                | 4/21 [00:00<00:01, 12.11it/s]evaluate for the 6-th batch, evaluate loss: 0.9878997802734375:  29%|█████▋              | 6/21 [00:00<00:01, 12.24it/s]Epoch: 3, train for the 95-th batch, train loss: 0.5313724875450134:  64%|████████▎    | 94/146 [00:15<00:08,  6.26it/s]Epoch: 3, train for the 95-th batch, train loss: 0.5313724875450134:  65%|████████▍    | 95/146 [00:15<00:08,  6.29it/s]evaluate for the 7-th batch, evaluate loss: 0.8707058429718018:  29%|█████▋              | 6/21 [00:00<00:01, 12.24it/s]  0%|                                                                                            | 0/40 [00:00<?, ?it/s]INFO:root:Epoch: 2, learning rate: 0.0001, train loss: 0.5611
INFO:root:train average_precision, 0.7760
Epoch: 2, train for the 74-th batch, train loss: 0.42361655831336975:  30%|███▋        | 73/241 [00:11<00:27,  6.07it/s]INFO:root:train roc_auc, 0.7557
INFO:root:validate loss: 0.6032
INFO:root:validate average_precision, 0.7239
INFO:root:validate roc_auc, 0.7677
INFO:root:new node validate loss: 0.7676
INFO:root:new node validate first_1_average_precision, 0.6855
INFO:root:new node validate first_1_roc_auc, 0.7034
INFO:root:new node validate first_3_average_precision, 0.6410
INFO:root:new node validate first_3_roc_auc, 0.6364
INFO:root:new node validate first_10_average_precision, 0.6338
INFO:root:new node validate first_10_roc_auc, 0.6300
INFO:root:new node validate average_precision, 0.6696
INFO:root:new node validate roc_auc, 0.6675
evaluate for the 8-th batch, evaluate loss: 0.889106035232544:  29%|██████               | 6/21 [00:00<00:01, 12.24it/s]evaluate for the 8-th batch, evaluate loss: 0.889106035232544:  38%|████████             | 8/21 [00:00<00:01, 11.88it/s]evaluate for the 1-th batch, evaluate loss: 0.8725507259368896:   0%|                            | 0/40 [00:00<?, ?it/s]Epoch: 2, train for the 74-th batch, train loss: 0.42361655831336975:  31%|███▋        | 74/241 [00:11<00:29,  5.64it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]Epoch: 3, train for the 96-th batch, train loss: 0.5135902166366577:  65%|████████▍    | 95/146 [00:15<00:08,  6.29it/s]Epoch: 3, train for the 96-th batch, train loss: 0.5135902166366577:  66%|████████▌    | 96/146 [00:15<00:08,  6.03it/s]Epoch: 1, train for the 269-th batch, train loss: 0.4655630886554718:  70%|███████▋   | 268/383 [01:16<00:42,  2.71it/s]evaluate for the 9-th batch, evaluate loss: 0.8198897838592529:  38%|███████▌            | 8/21 [00:00<00:01, 11.88it/s]Epoch: 2, train for the 75-th batch, train loss: 0.36660873889923096:  31%|███▋        | 74/241 [00:11<00:29,  5.64it/s]Epoch: 1, train for the 269-th batch, train loss: 0.4655630886554718:  70%|███████▋   | 269/383 [01:16<00:43,  2.65it/s]Epoch: 2, train for the 75-th batch, train loss: 0.36660873889923096:  31%|███▋        | 75/241 [00:11<00:26,  6.24it/s]evaluate for the 2-th batch, evaluate loss: 0.898608386516571:   0%|                             | 0/40 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.898608386516571:   5%|█                    | 2/40 [00:00<00:04,  9.17it/s]evaluate for the 10-th batch, evaluate loss: 0.8484757542610168:  38%|███████▏           | 8/21 [00:00<00:01, 11.88it/s]evaluate for the 10-th batch, evaluate loss: 0.8484757542610168:  48%|████████▌         | 10/21 [00:00<00:00, 11.83it/s]Epoch: 3, train for the 97-th batch, train loss: 0.5207177400588989:  66%|████████▌    | 96/146 [00:15<00:08,  6.03it/s]Epoch: 3, train for the 97-th batch, train loss: 0.5207177400588989:  66%|████████▋    | 97/146 [00:15<00:08,  5.97it/s]Epoch: 3, train for the 1-th batch, train loss: 0.8470735549926758:   0%|                       | 0/151 [00:00<?, ?it/s]Epoch: 3, train for the 1-th batch, train loss: 0.8470735549926758:   1%|               | 1/151 [00:00<00:35,  4.19it/s]evaluate for the 11-th batch, evaluate loss: 0.7465347647666931:  48%|████████▌         | 10/21 [00:00<00:00, 11.83it/s]evaluate for the 3-th batch, evaluate loss: 0.8477107286453247:   5%|█                   | 2/40 [00:00<00:04,  9.17it/s]evaluate for the 3-th batch, evaluate loss: 0.8477107286453247:   8%|█▌                  | 3/40 [00:00<00:04,  7.91it/s]evaluate for the 12-th batch, evaluate loss: 0.7918127775192261:  48%|████████▌         | 10/21 [00:01<00:00, 11.83it/s]evaluate for the 12-th batch, evaluate loss: 0.7918127775192261:  57%|██████████▎       | 12/21 [00:01<00:00, 11.51it/s]Epoch: 3, train for the 98-th batch, train loss: 0.4794238209724426:  66%|████████▋    | 97/146 [00:15<00:08,  5.97it/s]Epoch: 1, train for the 270-th batch, train loss: 0.4739302694797516:  70%|███████▋   | 269/383 [01:16<00:43,  2.65it/s]Epoch: 3, train for the 98-th batch, train loss: 0.4794238209724426:  67%|████████▋    | 98/146 [00:15<00:08,  5.82it/s]evaluate for the 4-th batch, evaluate loss: 0.9046270847320557:   8%|█▌                  | 3/40 [00:00<00:04,  7.91it/s]evaluate for the 4-th batch, evaluate loss: 0.9046270847320557:  10%|██                  | 4/40 [00:00<00:04,  8.18it/s]evaluate for the 13-th batch, evaluate loss: 0.7284390330314636:  57%|██████████▎       | 12/21 [00:01<00:00, 11.51it/s]Epoch: 2, train for the 76-th batch, train loss: 0.25893333554267883:  31%|███▋        | 75/241 [00:11<00:26,  6.24it/s]Epoch: 3, train for the 2-th batch, train loss: 0.8425140976905823:   1%|               | 1/151 [00:00<00:35,  4.19it/s]Epoch: 3, train for the 2-th batch, train loss: 0.8425140976905823:   1%|▏              | 2/151 [00:00<00:30,  4.94it/s]Epoch: 1, train for the 270-th batch, train loss: 0.4739302694797516:  70%|███████▊   | 270/383 [01:17<00:40,  2.78it/s]Epoch: 2, train for the 76-th batch, train loss: 0.25893333554267883:  32%|███▊        | 76/241 [00:11<00:34,  4.82it/s]evaluate for the 14-th batch, evaluate loss: 0.7101507186889648:  57%|██████████▎       | 12/21 [00:01<00:00, 11.51it/s]evaluate for the 14-th batch, evaluate loss: 0.7101507186889648:  67%|████████████      | 14/21 [00:01<00:00, 12.60it/s]evaluate for the 5-th batch, evaluate loss: 0.8921139240264893:  10%|██                  | 4/40 [00:00<00:04,  8.18it/s]evaluate for the 5-th batch, evaluate loss: 0.8921139240264893:  12%|██▌                 | 5/40 [00:00<00:04,  8.28it/s]Epoch: 3, train for the 99-th batch, train loss: 0.5341053009033203:  67%|████████▋    | 98/146 [00:15<00:08,  5.82it/s]evaluate for the 15-th batch, evaluate loss: 0.7511913776397705:  67%|████████████      | 14/21 [00:01<00:00, 12.60it/s]Epoch: 3, train for the 99-th batch, train loss: 0.5341053009033203:  68%|████████▊    | 99/146 [00:15<00:07,  5.89it/s]Epoch: 3, train for the 3-th batch, train loss: 0.8148984313011169:   1%|▏              | 2/151 [00:00<00:30,  4.94it/s]Epoch: 3, train for the 3-th batch, train loss: 0.8148984313011169:   2%|▎              | 3/151 [00:00<00:28,  5.23it/s]Epoch: 2, train for the 77-th batch, train loss: 0.36840498447418213:  32%|███▊        | 76/241 [00:12<00:34,  4.82it/s]Epoch: 2, train for the 77-th batch, train loss: 0.36840498447418213:  32%|███▊        | 77/241 [00:12<00:32,  4.97it/s]evaluate for the 16-th batch, evaluate loss: 0.6881545782089233:  67%|████████████      | 14/21 [00:01<00:00, 12.60it/s]evaluate for the 16-th batch, evaluate loss: 0.6881545782089233:  76%|█████████████▋    | 16/21 [00:01<00:00, 12.70it/s]evaluate for the 6-th batch, evaluate loss: 0.8008720874786377:  12%|██▌                 | 5/40 [00:00<00:04,  8.28it/s]evaluate for the 6-th batch, evaluate loss: 0.8008720874786377:  15%|███                 | 6/40 [00:00<00:04,  7.60it/s]Epoch: 1, train for the 271-th batch, train loss: 0.3818189203739166:  70%|███████▊   | 270/383 [01:17<00:40,  2.78it/s]Epoch: 3, train for the 100-th batch, train loss: 0.5875965356826782:  68%|████████▏   | 99/146 [00:16<00:07,  5.89it/s]Epoch: 3, train for the 100-th batch, train loss: 0.5875965356826782:  68%|███████▌   | 100/146 [00:16<00:07,  6.00it/s]evaluate for the 17-th batch, evaluate loss: 0.6389917731285095:  76%|█████████████▋    | 16/21 [00:01<00:00, 12.70it/s]Epoch: 1, train for the 271-th batch, train loss: 0.3818189203739166:  71%|███████▊   | 271/383 [01:17<00:38,  2.89it/s]evaluate for the 7-th batch, evaluate loss: 0.7425042390823364:  15%|███                 | 6/40 [00:00<00:04,  7.60it/s]evaluate for the 7-th batch, evaluate loss: 0.7425042390823364:  18%|███▌                | 7/40 [00:00<00:04,  8.17it/s]Epoch: 3, train for the 4-th batch, train loss: 0.7749846577644348:   2%|▎              | 3/151 [00:00<00:28,  5.23it/s]Epoch: 3, train for the 4-th batch, train loss: 0.7749846577644348:   3%|▍              | 4/151 [00:00<00:27,  5.43it/s]evaluate for the 18-th batch, evaluate loss: 0.6598892211914062:  76%|█████████████▋    | 16/21 [00:01<00:00, 12.70it/s]evaluate for the 18-th batch, evaluate loss: 0.6598892211914062:  86%|███████████████▍  | 18/21 [00:01<00:00, 12.90it/s]Epoch: 2, train for the 78-th batch, train loss: 0.5384460687637329:  32%|████▏        | 77/241 [00:12<00:32,  4.97it/s]Epoch: 2, train for the 78-th batch, train loss: 0.5384460687637329:  32%|████▏        | 78/241 [00:12<00:31,  5.14it/s]Epoch: 3, train for the 101-th batch, train loss: 0.5334468483924866:  68%|███████▌   | 100/146 [00:16<00:07,  6.00it/s]Epoch: 3, train for the 101-th batch, train loss: 0.5334468483924866:  69%|███████▌   | 101/146 [00:16<00:06,  6.46it/s]evaluate for the 19-th batch, evaluate loss: 0.5858394503593445:  86%|███████████████▍  | 18/21 [00:01<00:00, 12.90it/s]evaluate for the 8-th batch, evaluate loss: 0.8224278092384338:  18%|███▌                | 7/40 [00:01<00:04,  8.17it/s]evaluate for the 8-th batch, evaluate loss: 0.8224278092384338:  20%|████                | 8/40 [00:01<00:04,  7.54it/s]evaluate for the 20-th batch, evaluate loss: 0.5583028793334961:  86%|███████████████▍  | 18/21 [00:01<00:00, 12.90it/s]evaluate for the 20-th batch, evaluate loss: 0.5583028793334961:  95%|█████████████████▏| 20/21 [00:01<00:00, 13.12it/s]Epoch: 3, train for the 5-th batch, train loss: 0.774662971496582:   3%|▍               | 4/151 [00:00<00:27,  5.43it/s]Epoch: 3, train for the 5-th batch, train loss: 0.774662971496582:   3%|▌               | 5/151 [00:00<00:25,  5.72it/s]Epoch: 2, train for the 79-th batch, train loss: 0.4536949694156647:  32%|████▏        | 78/241 [00:12<00:31,  5.14it/s]evaluate for the 21-th batch, evaluate loss: 0.6382290720939636:  95%|█████████████████▏| 20/21 [00:01<00:00, 13.12it/s]evaluate for the 21-th batch, evaluate loss: 0.6382290720939636: 100%|██████████████████| 21/21 [00:01<00:00, 12.75it/s]
Epoch: 2, train for the 79-th batch, train loss: 0.4536949694156647:  33%|████▎        | 79/241 [00:12<00:30,  5.23it/s]evaluate for the 9-th batch, evaluate loss: 0.8611668348312378:  20%|████                | 8/40 [00:01<00:04,  7.54it/s]evaluate for the 9-th batch, evaluate loss: 0.8611668348312378:  22%|████▌               | 9/40 [00:01<00:03,  8.13it/s]Epoch: 3, train for the 102-th batch, train loss: 0.5211959481239319:  69%|███████▌   | 101/146 [00:16<00:06,  6.46it/s]Epoch: 3, train for the 102-th batch, train loss: 0.5211959481239319:  70%|███████▋   | 102/146 [00:16<00:07,  5.61it/s]Epoch: 3, train for the 6-th batch, train loss: 0.7645504474639893:   3%|▍              | 5/151 [00:01<00:25,  5.72it/s]Epoch: 3, train for the 6-th batch, train loss: 0.7645504474639893:   4%|▌              | 6/151 [00:01<00:23,  6.09it/s]Epoch: 2, train for the 80-th batch, train loss: 0.33080223202705383:  33%|███▉        | 79/241 [00:12<00:30,  5.23it/s]evaluate for the 10-th batch, evaluate loss: 0.8332430720329285:  22%|████▎              | 9/40 [00:01<00:03,  8.13it/s]Epoch: 2, train for the 80-th batch, train loss: 0.33080223202705383:  33%|███▉        | 80/241 [00:12<00:28,  5.72it/s]Epoch: 3, train for the 103-th batch, train loss: 0.5058065056800842:  70%|███████▋   | 102/146 [00:16<00:07,  5.61it/s]Epoch: 3, train for the 103-th batch, train loss: 0.5058065056800842:  71%|███████▊   | 103/146 [00:16<00:06,  6.17it/s]Epoch: 3, train for the 7-th batch, train loss: 0.7129473686218262:   4%|▌              | 6/151 [00:01<00:23,  6.09it/s]Epoch: 3, train for the 7-th batch, train loss: 0.7129473686218262:   5%|▋              | 7/151 [00:01<00:21,  6.56it/s]evaluate for the 11-th batch, evaluate loss: 0.8425884246826172:  22%|████▎              | 9/40 [00:01<00:03,  8.13it/s]evaluate for the 11-th batch, evaluate loss: 0.8425884246826172:  28%|████▉             | 11/40 [00:01<00:03,  9.19it/s]Epoch: 1, train for the 272-th batch, train loss: 0.3603909909725189:  71%|███████▊   | 271/383 [01:17<00:38,  2.89it/s]Epoch: 2, train for the 81-th batch, train loss: 0.3632720410823822:  33%|████▎        | 80/241 [00:12<00:28,  5.72it/s]Epoch: 2, train for the 81-th batch, train loss: 0.3632720410823822:  34%|████▎        | 81/241 [00:12<00:26,  6.14it/s]INFO:root:Epoch: 3, learning rate: 0.0001, train loss: 0.4517
INFO:root:train average_precision, 0.8869
INFO:root:train roc_auc, 0.8807
INFO:root:validate loss: 0.6764
INFO:root:validate average_precision, 0.7095
INFO:root:validate roc_auc, 0.7563
INFO:root:new node validate loss: 0.8138
INFO:root:new node validate first_1_average_precision, 0.8212
INFO:root:new node validate first_1_roc_auc, 0.8380
INFO:root:new node validate first_3_average_precision, 0.7763
INFO:root:new node validate first_3_roc_auc, 0.7681
INFO:root:new node validate first_10_average_precision, 0.7097
INFO:root:new node validate first_10_roc_auc, 0.7046
INFO:root:new node validate average_precision, 0.6659
INFO:root:new node validate roc_auc, 0.6811
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]Epoch: 1, train for the 272-th batch, train loss: 0.3603909909725189:  71%|███████▊   | 272/383 [01:17<00:45,  2.45it/s]evaluate for the 12-th batch, evaluate loss: 0.7789364457130432:  28%|████▉             | 11/40 [00:01<00:03,  9.19it/s]Epoch: 3, train for the 104-th batch, train loss: 0.5332053303718567:  71%|███████▊   | 103/146 [00:16<00:06,  6.17it/s]Epoch: 3, train for the 104-th batch, train loss: 0.5332053303718567:  71%|███████▊   | 104/146 [00:16<00:06,  6.61it/s]Epoch: 3, train for the 8-th batch, train loss: 0.7083955407142639:   5%|▋              | 7/151 [00:01<00:21,  6.56it/s]Epoch: 3, train for the 8-th batch, train loss: 0.7083955407142639:   5%|▊              | 8/151 [00:01<00:21,  6.77it/s]Epoch: 2, train for the 82-th batch, train loss: 0.43338072299957275:  34%|████        | 81/241 [00:12<00:26,  6.14it/s]Epoch: 4, train for the 1-th batch, train loss: 1.0587148666381836:   0%|                       | 0/119 [00:00<?, ?it/s]Epoch: 4, train for the 1-th batch, train loss: 1.0587148666381836:   1%|▏              | 1/119 [00:00<00:17,  6.57it/s]Epoch: 2, train for the 82-th batch, train loss: 0.43338072299957275:  34%|████        | 82/241 [00:12<00:26,  5.93it/s]evaluate for the 13-th batch, evaluate loss: 0.8407313227653503:  28%|████▉             | 11/40 [00:01<00:03,  9.19it/s]evaluate for the 13-th batch, evaluate loss: 0.8407313227653503:  32%|█████▊            | 13/40 [00:01<00:03,  8.82it/s]Epoch: 3, train for the 105-th batch, train loss: 0.5361169576644897:  71%|███████▊   | 104/146 [00:16<00:06,  6.61it/s]Epoch: 3, train for the 105-th batch, train loss: 0.5361169576644897:  72%|███████▉   | 105/146 [00:16<00:06,  6.42it/s]Epoch: 3, train for the 9-th batch, train loss: 0.683837354183197:   5%|▊               | 8/151 [00:01<00:21,  6.77it/s]Epoch: 3, train for the 9-th batch, train loss: 0.683837354183197:   6%|▉               | 9/151 [00:01<00:21,  6.58it/s]Epoch: 1, train for the 273-th batch, train loss: 0.44512590765953064:  71%|███████   | 272/383 [01:18<00:45,  2.45it/s]Epoch: 4, train for the 2-th batch, train loss: 1.1094292402267456:   1%|▏              | 1/119 [00:00<00:17,  6.57it/s]Epoch: 4, train for the 2-th batch, train loss: 1.1094292402267456:   2%|▎              | 2/119 [00:00<00:16,  7.22it/s]evaluate for the 14-th batch, evaluate loss: 0.8611677885055542:  32%|█████▊            | 13/40 [00:01<00:03,  8.82it/s]evaluate for the 14-th batch, evaluate loss: 0.8611677885055542:  35%|██████▎           | 14/40 [00:01<00:02,  8.72it/s]Epoch: 2, train for the 83-th batch, train loss: 0.520891010761261:  34%|████▊         | 82/241 [00:13<00:26,  5.93it/s]Epoch: 2, train for the 83-th batch, train loss: 0.520891010761261:  34%|████▊         | 83/241 [00:13<00:26,  6.03it/s]Epoch: 1, train for the 273-th batch, train loss: 0.44512590765953064:  71%|███████▏  | 273/383 [01:18<00:41,  2.65it/s]Epoch: 3, train for the 10-th batch, train loss: 0.670610249042511:   6%|▉              | 9/151 [00:01<00:21,  6.58it/s]Epoch: 3, train for the 106-th batch, train loss: 0.5284526944160461:  72%|███████▉   | 105/146 [00:16<00:06,  6.42it/s]Epoch: 3, train for the 10-th batch, train loss: 0.670610249042511:   7%|▉             | 10/151 [00:01<00:20,  6.94it/s]Epoch: 3, train for the 106-th batch, train loss: 0.5284526944160461:  73%|███████▉   | 106/146 [00:16<00:06,  6.49it/s]evaluate for the 15-th batch, evaluate loss: 0.8160108923912048:  35%|██████▎           | 14/40 [00:01<00:02,  8.72it/s]evaluate for the 15-th batch, evaluate loss: 0.8160108923912048:  38%|██████▊           | 15/40 [00:01<00:02,  8.99it/s]Epoch: 4, train for the 3-th batch, train loss: 0.9027048349380493:   2%|▎              | 2/119 [00:00<00:16,  7.22it/s]Epoch: 4, train for the 3-th batch, train loss: 0.9027048349380493:   3%|▍              | 3/119 [00:00<00:15,  7.41it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5654277205467224:  34%|████▍        | 83/241 [00:13<00:26,  6.03it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5654277205467224:  35%|████▌        | 84/241 [00:13<00:26,  6.00it/s]Epoch: 3, train for the 107-th batch, train loss: 0.5505223870277405:  73%|███████▉   | 106/146 [00:17<00:06,  6.49it/s]Epoch: 3, train for the 107-th batch, train loss: 0.5505223870277405:  73%|████████   | 107/146 [00:17<00:05,  6.64it/s]evaluate for the 16-th batch, evaluate loss: 0.7622334957122803:  38%|██████▊           | 15/40 [00:01<00:02,  8.99it/s]evaluate for the 16-th batch, evaluate loss: 0.7622334957122803:  40%|███████▏          | 16/40 [00:01<00:02,  8.58it/s]Epoch: 3, train for the 11-th batch, train loss: 0.6631510853767395:   7%|▊            | 10/151 [00:01<00:20,  6.94it/s]Epoch: 3, train for the 11-th batch, train loss: 0.6631510853767395:   7%|▉            | 11/151 [00:01<00:21,  6.45it/s]Epoch: 4, train for the 4-th batch, train loss: 0.8015914559364319:   3%|▍              | 3/119 [00:00<00:15,  7.41it/s]Epoch: 4, train for the 4-th batch, train loss: 0.8015914559364319:   3%|▌              | 4/119 [00:00<00:16,  6.90it/s]Epoch: 1, train for the 274-th batch, train loss: 0.32788634300231934:  71%|███████▏  | 273/383 [01:18<00:41,  2.65it/s]Epoch: 2, train for the 85-th batch, train loss: 0.4292225241661072:  35%|████▌        | 84/241 [00:13<00:26,  6.00it/s]evaluate for the 17-th batch, evaluate loss: 0.7471230626106262:  40%|███████▏          | 16/40 [00:02<00:02,  8.58it/s]evaluate for the 17-th batch, evaluate loss: 0.7471230626106262:  42%|███████▋          | 17/40 [00:02<00:02,  8.59it/s]Epoch: 2, train for the 85-th batch, train loss: 0.4292225241661072:  35%|████▌        | 85/241 [00:13<00:25,  6.05it/s]Epoch: 3, train for the 108-th batch, train loss: 0.5548068284988403:  73%|████████   | 107/146 [00:17<00:05,  6.64it/s]Epoch: 3, train for the 12-th batch, train loss: 0.6798245906829834:   7%|▉            | 11/151 [00:01<00:21,  6.45it/s]Epoch: 1, train for the 274-th batch, train loss: 0.32788634300231934:  72%|███████▏  | 274/383 [01:18<00:40,  2.70it/s]Epoch: 3, train for the 108-th batch, train loss: 0.5548068284988403:  74%|████████▏  | 108/146 [00:17<00:06,  6.19it/s]Epoch: 3, train for the 12-th batch, train loss: 0.6798245906829834:   8%|█            | 12/151 [00:01<00:21,  6.47it/s]Epoch: 4, train for the 5-th batch, train loss: 0.6082355976104736:   3%|▌              | 4/119 [00:00<00:16,  6.90it/s]Epoch: 4, train for the 5-th batch, train loss: 0.6082355976104736:   4%|▋              | 5/119 [00:00<00:16,  6.90it/s]evaluate for the 18-th batch, evaluate loss: 0.7878937125205994:  42%|███████▋          | 17/40 [00:02<00:02,  8.59it/s]evaluate for the 18-th batch, evaluate loss: 0.7878937125205994:  45%|████████          | 18/40 [00:02<00:02,  8.30it/s]Epoch: 2, train for the 86-th batch, train loss: 0.47200682759284973:  35%|████▏       | 85/241 [00:13<00:25,  6.05it/s]Epoch: 2, train for the 86-th batch, train loss: 0.47200682759284973:  36%|████▎       | 86/241 [00:13<00:24,  6.40it/s]Epoch: 3, train for the 109-th batch, train loss: 0.4939410388469696:  74%|████████▏  | 108/146 [00:17<00:06,  6.19it/s]Epoch: 3, train for the 109-th batch, train loss: 0.4939410388469696:  75%|████████▏  | 109/146 [00:17<00:05,  6.22it/s]Epoch: 3, train for the 13-th batch, train loss: 0.6795918345451355:   8%|█            | 12/151 [00:02<00:21,  6.47it/s]Epoch: 3, train for the 13-th batch, train loss: 0.6795918345451355:   9%|█            | 13/151 [00:02<00:21,  6.31it/s]Epoch: 4, train for the 6-th batch, train loss: 0.4668746888637543:   4%|▋              | 5/119 [00:00<00:16,  6.90it/s]Epoch: 4, train for the 6-th batch, train loss: 0.4668746888637543:   5%|▊              | 6/119 [00:00<00:17,  6.32it/s]Epoch: 2, train for the 87-th batch, train loss: 0.5141246914863586:  36%|████▋        | 86/241 [00:13<00:24,  6.40it/s]evaluate for the 19-th batch, evaluate loss: 0.841990053653717:  45%|████████▌          | 18/40 [00:02<00:02,  8.30it/s]evaluate for the 19-th batch, evaluate loss: 0.841990053653717:  48%|█████████          | 19/40 [00:02<00:02,  7.55it/s]Epoch: 1, train for the 275-th batch, train loss: 0.4552532732486725:  72%|███████▊   | 274/383 [01:18<00:40,  2.70it/s]Epoch: 2, train for the 87-th batch, train loss: 0.5141246914863586:  36%|████▋        | 87/241 [00:13<00:24,  6.41it/s]Epoch: 3, train for the 14-th batch, train loss: 0.6787066459655762:   9%|█            | 13/151 [00:02<00:21,  6.31it/s]Epoch: 3, train for the 14-th batch, train loss: 0.6787066459655762:   9%|█▏           | 14/151 [00:02<00:20,  6.55it/s]Epoch: 1, train for the 275-th batch, train loss: 0.4552532732486725:  72%|███████▉   | 275/383 [01:18<00:38,  2.83it/s]Epoch: 3, train for the 110-th batch, train loss: 0.46472957730293274:  75%|███████▍  | 109/146 [00:17<00:05,  6.22it/s]Epoch: 3, train for the 110-th batch, train loss: 0.46472957730293274:  75%|███████▌  | 110/146 [00:17<00:05,  6.23it/s]Epoch: 4, train for the 7-th batch, train loss: 0.39126503467559814:   5%|▋             | 6/119 [00:01<00:17,  6.32it/s]Epoch: 4, train for the 7-th batch, train loss: 0.39126503467559814:   6%|▊             | 7/119 [00:01<00:16,  6.62it/s]evaluate for the 20-th batch, evaluate loss: 0.7574416995048523:  48%|████████▌         | 19/40 [00:02<00:02,  7.55it/s]evaluate for the 20-th batch, evaluate loss: 0.7574416995048523:  50%|█████████         | 20/40 [00:02<00:02,  7.81it/s]Epoch: 2, train for the 88-th batch, train loss: 0.47175201773643494:  36%|████▎       | 87/241 [00:13<00:24,  6.41it/s]Epoch: 2, train for the 88-th batch, train loss: 0.47175201773643494:  37%|████▍       | 88/241 [00:13<00:23,  6.54it/s]Epoch: 3, train for the 15-th batch, train loss: 0.6666476130485535:   9%|█▏           | 14/151 [00:02<00:20,  6.55it/s]Epoch: 3, train for the 15-th batch, train loss: 0.6666476130485535:  10%|█▎           | 15/151 [00:02<00:20,  6.51it/s]Epoch: 3, train for the 111-th batch, train loss: 0.5163695216178894:  75%|████████▎  | 110/146 [00:17<00:05,  6.23it/s]Epoch: 3, train for the 111-th batch, train loss: 0.5163695216178894:  76%|████████▎  | 111/146 [00:17<00:05,  6.12it/s]evaluate for the 21-th batch, evaluate loss: 0.7374658584594727:  50%|█████████         | 20/40 [00:02<00:02,  7.81it/s]evaluate for the 21-th batch, evaluate loss: 0.7374658584594727:  52%|█████████▍        | 21/40 [00:02<00:02,  7.83it/s]Epoch: 4, train for the 8-th batch, train loss: 0.36635977029800415:   6%|▊             | 7/119 [00:01<00:16,  6.62it/s]Epoch: 4, train for the 8-th batch, train loss: 0.36635977029800415:   7%|▉             | 8/119 [00:01<00:17,  6.52it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5171110033988953:  37%|████▋        | 88/241 [00:13<00:23,  6.54it/s]Epoch: 1, train for the 276-th batch, train loss: 0.6054269075393677:  72%|███████▉   | 275/383 [01:19<00:38,  2.83it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5171110033988953:  37%|████▊        | 89/241 [00:14<00:23,  6.47it/s]evaluate for the 22-th batch, evaluate loss: 0.7652981281280518:  52%|█████████▍        | 21/40 [00:02<00:02,  7.83it/s]Epoch: 3, train for the 16-th batch, train loss: 0.6607112884521484:  10%|█▎           | 15/151 [00:02<00:20,  6.51it/s]evaluate for the 22-th batch, evaluate loss: 0.7652981281280518:  55%|█████████▉        | 22/40 [00:02<00:02,  7.96it/s]Epoch: 1, train for the 276-th batch, train loss: 0.6054269075393677:  72%|███████▉   | 276/383 [01:19<00:36,  2.95it/s]Epoch: 3, train for the 16-th batch, train loss: 0.6607112884521484:  11%|█▍           | 16/151 [00:02<00:20,  6.45it/s]Epoch: 3, train for the 112-th batch, train loss: 0.5561137199401855:  76%|████████▎  | 111/146 [00:17<00:05,  6.12it/s]Epoch: 3, train for the 112-th batch, train loss: 0.5561137199401855:  77%|████████▍  | 112/146 [00:17<00:05,  6.25it/s]Epoch: 4, train for the 9-th batch, train loss: 0.42653411626815796:   7%|▉             | 8/119 [00:01<00:17,  6.52it/s]Epoch: 4, train for the 9-th batch, train loss: 0.42653411626815796:   8%|█             | 9/119 [00:01<00:17,  6.29it/s]Epoch: 2, train for the 90-th batch, train loss: 0.45398351550102234:  37%|████▍       | 89/241 [00:14<00:23,  6.47it/s]Epoch: 2, train for the 90-th batch, train loss: 0.45398351550102234:  37%|████▍       | 90/241 [00:14<00:22,  6.61it/s]evaluate for the 23-th batch, evaluate loss: 0.771958589553833:  55%|██████████▍        | 22/40 [00:02<00:02,  7.96it/s]evaluate for the 23-th batch, evaluate loss: 0.771958589553833:  57%|██████████▉        | 23/40 [00:02<00:02,  8.01it/s]Epoch: 3, train for the 17-th batch, train loss: 0.6628279089927673:  11%|█▍           | 16/151 [00:02<00:20,  6.45it/s]Epoch: 3, train for the 17-th batch, train loss: 0.6628279089927673:  11%|█▍           | 17/151 [00:02<00:21,  6.31it/s]Epoch: 3, train for the 113-th batch, train loss: 0.48103079199790955:  77%|███████▋  | 112/146 [00:18<00:05,  6.25it/s]Epoch: 3, train for the 113-th batch, train loss: 0.48103079199790955:  77%|███████▋  | 113/146 [00:18<00:05,  6.07it/s]Epoch: 2, train for the 91-th batch, train loss: 0.5003864765167236:  37%|████▊        | 90/241 [00:14<00:22,  6.61it/s]Epoch: 4, train for the 10-th batch, train loss: 0.4354766309261322:   8%|█             | 9/119 [00:01<00:17,  6.29it/s]Epoch: 4, train for the 10-th batch, train loss: 0.4354766309261322:   8%|█            | 10/119 [00:01<00:17,  6.21it/s]evaluate for the 24-th batch, evaluate loss: 0.8236463665962219:  57%|██████████▎       | 23/40 [00:02<00:02,  8.01it/s]evaluate for the 24-th batch, evaluate loss: 0.8236463665962219:  60%|██████████▊       | 24/40 [00:02<00:01,  8.18it/s]Epoch: 2, train for the 91-th batch, train loss: 0.5003864765167236:  38%|████▉        | 91/241 [00:14<00:22,  6.58it/s]Epoch: 3, train for the 18-th batch, train loss: 0.6563949584960938:  11%|█▍           | 17/151 [00:02<00:21,  6.31it/s]Epoch: 3, train for the 18-th batch, train loss: 0.6563949584960938:  12%|█▌           | 18/151 [00:02<00:20,  6.55it/s]evaluate for the 25-th batch, evaluate loss: 0.802952229976654:  60%|███████████▍       | 24/40 [00:03<00:01,  8.18it/s]evaluate for the 25-th batch, evaluate loss: 0.802952229976654:  62%|███████████▉       | 25/40 [00:03<00:01,  8.30it/s]Epoch: 3, train for the 114-th batch, train loss: 0.5222102403640747:  77%|████████▌  | 113/146 [00:18<00:05,  6.07it/s]Epoch: 2, train for the 92-th batch, train loss: 0.3973981738090515:  38%|████▉        | 91/241 [00:14<00:22,  6.58it/s]Epoch: 3, train for the 114-th batch, train loss: 0.5222102403640747:  78%|████████▌  | 114/146 [00:18<00:05,  5.92it/s]Epoch: 4, train for the 11-th batch, train loss: 0.4835682213306427:   8%|█            | 10/119 [00:01<00:17,  6.21it/s]Epoch: 4, train for the 11-th batch, train loss: 0.4835682213306427:   9%|█▏           | 11/119 [00:01<00:17,  6.15it/s]Epoch: 2, train for the 92-th batch, train loss: 0.3973981738090515:  38%|████▉        | 92/241 [00:14<00:23,  6.43it/s]Epoch: 3, train for the 19-th batch, train loss: 0.6709765195846558:  12%|█▌           | 18/151 [00:03<00:20,  6.55it/s]Epoch: 3, train for the 19-th batch, train loss: 0.6709765195846558:  13%|█▋           | 19/151 [00:03<00:21,  6.28it/s]evaluate for the 26-th batch, evaluate loss: 0.7635300755500793:  62%|███████████▎      | 25/40 [00:03<00:01,  8.30it/s]evaluate for the 26-th batch, evaluate loss: 0.7635300755500793:  65%|███████████▋      | 26/40 [00:03<00:01,  7.32it/s]Epoch: 1, train for the 277-th batch, train loss: 0.42386436462402344:  72%|███████▏  | 276/383 [01:19<00:36,  2.95it/s]Epoch: 2, train for the 93-th batch, train loss: 0.42487913370132446:  38%|████▌       | 92/241 [00:14<00:23,  6.43it/s]Epoch: 4, train for the 12-th batch, train loss: 0.44759878516197205:   9%|█           | 11/119 [00:01<00:17,  6.15it/s]Epoch: 4, train for the 12-th batch, train loss: 0.44759878516197205:  10%|█▏          | 12/119 [00:01<00:17,  5.99it/s]Epoch: 2, train for the 93-th batch, train loss: 0.42487913370132446:  39%|████▋       | 93/241 [00:14<00:24,  6.13it/s]Epoch: 3, train for the 115-th batch, train loss: 0.5413562059402466:  78%|████████▌  | 114/146 [00:18<00:05,  5.92it/s]Epoch: 1, train for the 277-th batch, train loss: 0.42386436462402344:  72%|███████▏  | 277/383 [01:19<00:44,  2.40it/s]Epoch: 3, train for the 115-th batch, train loss: 0.5413562059402466:  79%|████████▋  | 115/146 [00:18<00:05,  5.43it/s]Epoch: 3, train for the 20-th batch, train loss: 0.6466376185417175:  13%|█▋           | 19/151 [00:03<00:21,  6.28it/s]Epoch: 3, train for the 20-th batch, train loss: 0.6466376185417175:  13%|█▋           | 20/151 [00:03<00:20,  6.39it/s]evaluate for the 27-th batch, evaluate loss: 0.7760570645332336:  65%|███████████▋      | 26/40 [00:03<00:01,  7.32it/s]evaluate for the 27-th batch, evaluate loss: 0.7760570645332336:  68%|████████████▏     | 27/40 [00:03<00:01,  7.63it/s]Epoch: 4, train for the 13-th batch, train loss: 0.5072404146194458:  10%|█▎           | 12/119 [00:01<00:17,  5.99it/s]Epoch: 4, train for the 13-th batch, train loss: 0.5072404146194458:  11%|█▍           | 13/119 [00:02<00:16,  6.45it/s]Epoch: 2, train for the 94-th batch, train loss: 0.36229148507118225:  39%|████▋       | 93/241 [00:14<00:24,  6.13it/s]Epoch: 3, train for the 116-th batch, train loss: 0.49792250990867615:  79%|███████▉  | 115/146 [00:18<00:05,  5.43it/s]Epoch: 2, train for the 94-th batch, train loss: 0.36229148507118225:  39%|████▋       | 94/241 [00:14<00:24,  6.08it/s]evaluate for the 28-th batch, evaluate loss: 0.7913251519203186:  68%|████████████▏     | 27/40 [00:03<00:01,  7.63it/s]evaluate for the 28-th batch, evaluate loss: 0.7913251519203186:  70%|████████████▌     | 28/40 [00:03<00:01,  7.99it/s]Epoch: 3, train for the 116-th batch, train loss: 0.49792250990867615:  79%|███████▉  | 116/146 [00:18<00:05,  5.69it/s]Epoch: 3, train for the 21-th batch, train loss: 0.6620170474052429:  13%|█▋           | 20/151 [00:03<00:20,  6.39it/s]Epoch: 3, train for the 21-th batch, train loss: 0.6620170474052429:  14%|█▊           | 21/151 [00:03<00:20,  6.22it/s]evaluate for the 29-th batch, evaluate loss: 0.7556586861610413:  70%|████████████▌     | 28/40 [00:03<00:01,  7.99it/s]Epoch: 4, train for the 14-th batch, train loss: 0.4978513717651367:  11%|█▍           | 13/119 [00:02<00:16,  6.45it/s]evaluate for the 29-th batch, evaluate loss: 0.7556586861610413:  72%|█████████████     | 29/40 [00:03<00:01,  8.43it/s]Epoch: 4, train for the 14-th batch, train loss: 0.4978513717651367:  12%|█▌           | 14/119 [00:02<00:16,  6.38it/s]Epoch: 2, train for the 95-th batch, train loss: 0.4367727041244507:  39%|█████        | 94/241 [00:14<00:24,  6.08it/s]Epoch: 2, train for the 95-th batch, train loss: 0.4367727041244507:  39%|█████        | 95/241 [00:14<00:23,  6.28it/s]Epoch: 3, train for the 117-th batch, train loss: 0.5331601500511169:  79%|████████▋  | 116/146 [00:18<00:05,  5.69it/s]Epoch: 3, train for the 117-th batch, train loss: 0.5331601500511169:  80%|████████▊  | 117/146 [00:18<00:04,  5.81it/s]Epoch: 3, train for the 22-th batch, train loss: 0.6449266076087952:  14%|█▊           | 21/151 [00:03<00:20,  6.22it/s]Epoch: 3, train for the 22-th batch, train loss: 0.6449266076087952:  15%|█▉           | 22/151 [00:03<00:19,  6.46it/s]evaluate for the 30-th batch, evaluate loss: 0.764947772026062:  72%|█████████████▊     | 29/40 [00:03<00:01,  8.43it/s]evaluate for the 30-th batch, evaluate loss: 0.764947772026062:  75%|██████████████▎    | 30/40 [00:03<00:01,  8.81it/s]Epoch: 4, train for the 15-th batch, train loss: 0.5248962044715881:  12%|█▌           | 14/119 [00:02<00:16,  6.38it/s]Epoch: 1, train for the 278-th batch, train loss: 0.35052192211151123:  72%|███████▏  | 277/383 [01:20<00:44,  2.40it/s]Epoch: 4, train for the 15-th batch, train loss: 0.5248962044715881:  13%|█▋           | 15/119 [00:02<00:15,  6.72it/s]Epoch: 2, train for the 96-th batch, train loss: 0.5218952894210815:  39%|█████        | 95/241 [00:15<00:23,  6.28it/s]Epoch: 1, train for the 278-th batch, train loss: 0.35052192211151123:  73%|███████▎  | 278/383 [01:20<00:44,  2.35it/s]Epoch: 2, train for the 96-th batch, train loss: 0.5218952894210815:  40%|█████▏       | 96/241 [00:15<00:22,  6.41it/s]evaluate for the 31-th batch, evaluate loss: 0.7077227830886841:  75%|█████████████▌    | 30/40 [00:03<00:01,  8.81it/s]Epoch: 3, train for the 118-th batch, train loss: 0.5028144121170044:  80%|████████▊  | 117/146 [00:18<00:04,  5.81it/s]Epoch: 3, train for the 118-th batch, train loss: 0.5028144121170044:  81%|████████▉  | 118/146 [00:18<00:04,  6.14it/s]Epoch: 3, train for the 23-th batch, train loss: 0.6340504288673401:  15%|█▉           | 22/151 [00:03<00:19,  6.46it/s]Epoch: 3, train for the 23-th batch, train loss: 0.6340504288673401:  15%|█▉           | 23/151 [00:03<00:19,  6.58it/s]Epoch: 4, train for the 16-th batch, train loss: 0.4969558119773865:  13%|█▋           | 15/119 [00:02<00:15,  6.72it/s]Epoch: 4, train for the 16-th batch, train loss: 0.4969558119773865:  13%|█▋           | 16/119 [00:02<00:15,  6.78it/s]Epoch: 2, train for the 97-th batch, train loss: 0.36654338240623474:  40%|████▊       | 96/241 [00:15<00:22,  6.41it/s]evaluate for the 32-th batch, evaluate loss: 0.7226887345314026:  75%|█████████████▌    | 30/40 [00:03<00:01,  8.81it/s]evaluate for the 32-th batch, evaluate loss: 0.7226887345314026:  80%|██████████████▍   | 32/40 [00:03<00:00,  8.71it/s]Epoch: 2, train for the 97-th batch, train loss: 0.36654338240623474:  40%|████▊       | 97/241 [00:15<00:22,  6.54it/s]Epoch: 3, train for the 119-th batch, train loss: 0.5228546261787415:  81%|████████▉  | 118/146 [00:19<00:04,  6.14it/s]Epoch: 3, train for the 119-th batch, train loss: 0.5228546261787415:  82%|████████▉  | 119/146 [00:19<00:04,  6.21it/s]Epoch: 3, train for the 24-th batch, train loss: 0.6282363533973694:  15%|█▉           | 23/151 [00:03<00:19,  6.58it/s]Epoch: 3, train for the 24-th batch, train loss: 0.6282363533973694:  16%|██           | 24/151 [00:03<00:19,  6.64it/s]Epoch: 1, train for the 279-th batch, train loss: 0.5218489170074463:  73%|███████▉   | 278/383 [01:20<00:44,  2.35it/s]Epoch: 4, train for the 17-th batch, train loss: 0.4997032880783081:  13%|█▋           | 16/119 [00:02<00:15,  6.78it/s]Epoch: 4, train for the 17-th batch, train loss: 0.4997032880783081:  14%|█▊           | 17/119 [00:02<00:15,  6.61it/s]evaluate for the 33-th batch, evaluate loss: 0.7537270784378052:  80%|██████████████▍   | 32/40 [00:03<00:00,  8.71it/s]evaluate for the 33-th batch, evaluate loss: 0.7537270784378052:  82%|██████████████▊   | 33/40 [00:03<00:00,  8.65it/s]Epoch: 1, train for the 279-th batch, train loss: 0.5218489170074463:  73%|████████   | 279/383 [01:20<00:39,  2.63it/s]Epoch: 2, train for the 98-th batch, train loss: 0.48237404227256775:  40%|████▊       | 97/241 [00:15<00:22,  6.54it/s]Epoch: 2, train for the 98-th batch, train loss: 0.48237404227256775:  41%|████▉       | 98/241 [00:15<00:21,  6.53it/s]Epoch: 3, train for the 120-th batch, train loss: 0.486676961183548:  82%|█████████▊  | 119/146 [00:19<00:04,  6.21it/s]Epoch: 3, train for the 120-th batch, train loss: 0.486676961183548:  82%|█████████▊  | 120/146 [00:19<00:04,  6.12it/s]Epoch: 3, train for the 25-th batch, train loss: 0.676327109336853:  16%|██▏           | 24/151 [00:03<00:19,  6.64it/s]Epoch: 3, train for the 25-th batch, train loss: 0.676327109336853:  17%|██▎           | 25/151 [00:03<00:19,  6.42it/s]evaluate for the 34-th batch, evaluate loss: 0.7043648958206177:  82%|██████████████▊   | 33/40 [00:04<00:00,  8.65it/s]evaluate for the 34-th batch, evaluate loss: 0.7043648958206177:  85%|███████████████▎  | 34/40 [00:04<00:00,  8.27it/s]Epoch: 4, train for the 18-th batch, train loss: 0.4647218585014343:  14%|█▊           | 17/119 [00:02<00:15,  6.61it/s]Epoch: 4, train for the 18-th batch, train loss: 0.4647218585014343:  15%|█▉           | 18/119 [00:02<00:15,  6.49it/s]Epoch: 2, train for the 99-th batch, train loss: 0.31050410866737366:  41%|████▉       | 98/241 [00:15<00:21,  6.53it/s]Epoch: 2, train for the 99-th batch, train loss: 0.31050410866737366:  41%|████▉       | 99/241 [00:15<00:22,  6.36it/s]Epoch: 3, train for the 121-th batch, train loss: 0.49563345313072205:  82%|████████▏ | 120/146 [00:19<00:04,  6.12it/s]Epoch: 3, train for the 26-th batch, train loss: 0.6258262991905212:  17%|██▏          | 25/151 [00:04<00:19,  6.42it/s]Epoch: 3, train for the 121-th batch, train loss: 0.49563345313072205:  83%|████████▎ | 121/146 [00:19<00:04,  6.01it/s]Epoch: 3, train for the 26-th batch, train loss: 0.6258262991905212:  17%|██▏          | 26/151 [00:04<00:19,  6.34it/s]Epoch: 4, train for the 19-th batch, train loss: 0.4344576597213745:  15%|█▉           | 18/119 [00:02<00:15,  6.49it/s]Epoch: 4, train for the 19-th batch, train loss: 0.4344576597213745:  16%|██           | 19/119 [00:02<00:15,  6.46it/s]Epoch: 1, train for the 280-th batch, train loss: 0.30958619713783264:  73%|███████▎  | 279/383 [01:20<00:39,  2.63it/s]evaluate for the 35-th batch, evaluate loss: 0.7098087668418884:  85%|███████████████▎  | 34/40 [00:04<00:00,  8.27it/s]evaluate for the 35-th batch, evaluate loss: 0.7098087668418884:  88%|███████████████▊  | 35/40 [00:04<00:00,  7.25it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5186241269111633:  41%|████▉       | 99/241 [00:15<00:22,  6.36it/s]Epoch: 1, train for the 280-th batch, train loss: 0.30958619713783264:  73%|███████▎  | 280/383 [01:20<00:39,  2.64it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5186241269111633:  41%|████▌      | 100/241 [00:15<00:23,  6.01it/s]Epoch: 3, train for the 122-th batch, train loss: 0.5240880250930786:  83%|█████████  | 121/146 [00:19<00:04,  6.01it/s]Epoch: 3, train for the 27-th batch, train loss: 0.6351540088653564:  17%|██▏          | 26/151 [00:04<00:19,  6.34it/s]Epoch: 3, train for the 122-th batch, train loss: 0.5240880250930786:  84%|█████████▏ | 122/146 [00:19<00:04,  5.97it/s]Epoch: 3, train for the 27-th batch, train loss: 0.6351540088653564:  18%|██▎          | 27/151 [00:04<00:19,  6.27it/s]Epoch: 4, train for the 20-th batch, train loss: 0.41372793912887573:  16%|█▉          | 19/119 [00:03<00:15,  6.46it/s]evaluate for the 36-th batch, evaluate loss: 0.7623698115348816:  88%|███████████████▊  | 35/40 [00:04<00:00,  7.25it/s]evaluate for the 36-th batch, evaluate loss: 0.7623698115348816:  90%|████████████████▏ | 36/40 [00:04<00:00,  7.23it/s]Epoch: 4, train for the 20-th batch, train loss: 0.41372793912887573:  17%|██          | 20/119 [00:03<00:15,  6.45it/s]Epoch: 2, train for the 101-th batch, train loss: 0.4868563115596771:  41%|████▌      | 100/241 [00:15<00:23,  6.01it/s]Epoch: 3, train for the 123-th batch, train loss: 0.5215186476707458:  84%|█████████▏ | 122/146 [00:19<00:04,  5.97it/s]Epoch: 3, train for the 123-th batch, train loss: 0.5215186476707458:  84%|█████████▎ | 123/146 [00:19<00:03,  6.14it/s]Epoch: 2, train for the 101-th batch, train loss: 0.4868563115596771:  42%|████▌      | 101/241 [00:15<00:24,  5.83it/s]evaluate for the 37-th batch, evaluate loss: 0.7126867771148682:  90%|████████████████▏ | 36/40 [00:04<00:00,  7.23it/s]evaluate for the 37-th batch, evaluate loss: 0.7126867771148682:  92%|████████████████▋ | 37/40 [00:04<00:00,  7.49it/s]Epoch: 3, train for the 28-th batch, train loss: 0.6385138034820557:  18%|██▎          | 27/151 [00:04<00:19,  6.27it/s]Epoch: 3, train for the 28-th batch, train loss: 0.6385138034820557:  19%|██▍          | 28/151 [00:04<00:20,  6.08it/s]Epoch: 4, train for the 21-th batch, train loss: 0.4536476731300354:  17%|██▏          | 20/119 [00:03<00:15,  6.45it/s]Epoch: 4, train for the 21-th batch, train loss: 0.4536476731300354:  18%|██▎          | 21/119 [00:03<00:15,  6.33it/s]Epoch: 1, train for the 281-th batch, train loss: 0.45499876141548157:  73%|███████▎  | 280/383 [01:21<00:39,  2.64it/s]evaluate for the 38-th batch, evaluate loss: 0.6938303112983704:  92%|████████████████▋ | 37/40 [00:04<00:00,  7.49it/s]evaluate for the 38-th batch, evaluate loss: 0.6938303112983704:  95%|█████████████████ | 38/40 [00:04<00:00,  7.92it/s]Epoch: 1, train for the 281-th batch, train loss: 0.45499876141548157:  73%|███████▎  | 281/383 [01:21<00:36,  2.79it/s]Epoch: 3, train for the 124-th batch, train loss: 0.516685426235199:  84%|██████████  | 123/146 [00:19<00:03,  6.14it/s]Epoch: 3, train for the 124-th batch, train loss: 0.516685426235199:  85%|██████████▏ | 124/146 [00:19<00:03,  6.27it/s]Epoch: 4, train for the 22-th batch, train loss: 0.48143401741981506:  18%|██          | 21/119 [00:03<00:15,  6.33it/s]Epoch: 3, train for the 29-th batch, train loss: 0.6748237609863281:  19%|██▍          | 28/151 [00:04<00:20,  6.08it/s]Epoch: 3, train for the 29-th batch, train loss: 0.6748237609863281:  19%|██▍          | 29/151 [00:04<00:20,  5.97it/s]Epoch: 2, train for the 102-th batch, train loss: 0.5207219123840332:  42%|████▌      | 101/241 [00:16<00:24,  5.83it/s]Epoch: 4, train for the 22-th batch, train loss: 0.48143401741981506:  18%|██▏         | 22/119 [00:03<00:15,  6.41it/s]evaluate for the 39-th batch, evaluate loss: 0.7198420763015747:  95%|█████████████████ | 38/40 [00:04<00:00,  7.92it/s]evaluate for the 39-th batch, evaluate loss: 0.7198420763015747:  98%|█████████████████▌| 39/40 [00:04<00:00,  7.94it/s]Epoch: 2, train for the 102-th batch, train loss: 0.5207219123840332:  42%|████▋      | 102/241 [00:16<00:26,  5.18it/s]Epoch: 3, train for the 125-th batch, train loss: 0.5238105058670044:  85%|█████████▎ | 124/146 [00:20<00:03,  6.27it/s]Epoch: 3, train for the 125-th batch, train loss: 0.5238105058670044:  86%|█████████▍ | 125/146 [00:20<00:03,  6.33it/s]Epoch: 3, train for the 30-th batch, train loss: 0.6521923542022705:  19%|██▍          | 29/151 [00:04<00:20,  5.97it/s]Epoch: 3, train for the 30-th batch, train loss: 0.6521923542022705:  20%|██▌          | 30/151 [00:04<00:20,  6.04it/s]Epoch: 4, train for the 23-th batch, train loss: 0.45287826657295227:  18%|██▏         | 22/119 [00:03<00:15,  6.41it/s]Epoch: 4, train for the 23-th batch, train loss: 0.45287826657295227:  19%|██▎         | 23/119 [00:03<00:15,  6.17it/s]Epoch: 2, train for the 103-th batch, train loss: 0.49081695079803467:  42%|████▏     | 102/241 [00:16<00:26,  5.18it/s]Epoch: 1, train for the 282-th batch, train loss: 0.5286970138549805:  73%|████████   | 281/383 [01:21<00:36,  2.79it/s]evaluate for the 40-th batch, evaluate loss: 0.7638676762580872:  98%|█████████████████▌| 39/40 [00:04<00:00,  7.94it/s]evaluate for the 40-th batch, evaluate loss: 0.7638676762580872: 100%|██████████████████| 40/40 [00:04<00:00,  7.27it/s]evaluate for the 40-th batch, evaluate loss: 0.7638676762580872: 100%|██████████████████| 40/40 [00:04<00:00,  8.06it/s]
Epoch: 2, train for the 103-th batch, train loss: 0.49081695079803467:  43%|████▎     | 103/241 [00:16<00:25,  5.41it/s]Epoch: 3, train for the 126-th batch, train loss: 0.5569100379943848:  86%|█████████▍ | 125/146 [00:20<00:03,  6.33it/s]Epoch: 1, train for the 282-th batch, train loss: 0.5286970138549805:  74%|████████   | 282/383 [01:21<00:35,  2.82it/s]Epoch: 3, train for the 126-th batch, train loss: 0.5569100379943848:  86%|█████████▍ | 126/146 [00:20<00:03,  6.27it/s]Epoch: 4, train for the 24-th batch, train loss: 0.40830734372138977:  19%|██▎         | 23/119 [00:03<00:15,  6.17it/s]Epoch: 4, train for the 24-th batch, train loss: 0.40830734372138977:  20%|██▍         | 24/119 [00:03<00:14,  6.49it/s]Epoch: 3, train for the 31-th batch, train loss: 0.6702824234962463:  20%|██▌          | 30/151 [00:04<00:20,  6.04it/s]Epoch: 3, train for the 31-th batch, train loss: 0.6702824234962463:  21%|██▋          | 31/151 [00:04<00:19,  6.02it/s]Epoch: 2, train for the 104-th batch, train loss: 0.49499258399009705:  43%|████▎     | 103/241 [00:16<00:25,  5.41it/s]Epoch: 2, train for the 104-th batch, train loss: 0.49499258399009705:  43%|████▎     | 104/241 [00:16<00:23,  5.73it/s]Epoch: 3, train for the 127-th batch, train loss: 0.5266521573066711:  86%|█████████▍ | 126/146 [00:20<00:03,  6.27it/s]Epoch: 3, train for the 127-th batch, train loss: 0.5266521573066711:  87%|█████████▌ | 127/146 [00:20<00:03,  6.29it/s]Epoch: 4, train for the 25-th batch, train loss: 0.4153522849082947:  20%|██▌          | 24/119 [00:03<00:14,  6.49it/s]Epoch: 4, train for the 25-th batch, train loss: 0.4153522849082947:  21%|██▋          | 25/119 [00:03<00:14,  6.52it/s]Epoch: 3, train for the 32-th batch, train loss: 0.6584748029708862:  21%|██▋          | 31/151 [00:05<00:19,  6.02it/s]Epoch: 1, train for the 283-th batch, train loss: 0.49638381600379944:  74%|███████▎  | 282/383 [01:21<00:35,  2.82it/s]Epoch: 3, train for the 32-th batch, train loss: 0.6584748029708862:  21%|██▊          | 32/151 [00:05<00:19,  6.01it/s]Epoch: 2, train for the 105-th batch, train loss: 0.43401801586151123:  43%|████▎     | 104/241 [00:16<00:23,  5.73it/s]Epoch: 2, train for the 105-th batch, train loss: 0.43401801586151123:  44%|████▎     | 105/241 [00:16<00:23,  5.76it/s]Epoch: 1, train for the 283-th batch, train loss: 0.49638381600379944:  74%|███████▍  | 283/383 [01:21<00:32,  3.05it/s]Epoch: 3, train for the 128-th batch, train loss: 0.5292860269546509:  87%|█████████▌ | 127/146 [00:20<00:03,  6.29it/s]Epoch: 3, train for the 128-th batch, train loss: 0.5292860269546509:  88%|█████████▋ | 128/146 [00:20<00:02,  6.34it/s]Epoch: 4, train for the 26-th batch, train loss: 0.40467581152915955:  21%|██▌         | 25/119 [00:03<00:14,  6.52it/s]Epoch: 4, train for the 26-th batch, train loss: 0.40467581152915955:  22%|██▌         | 26/119 [00:03<00:13,  6.99it/s]Epoch: 3, train for the 33-th batch, train loss: 0.6118753552436829:  21%|██▊          | 32/151 [00:05<00:19,  6.01it/s]Epoch: 3, train for the 33-th batch, train loss: 0.6118753552436829:  22%|██▊          | 33/151 [00:05<00:18,  6.25it/s]INFO:root:Epoch: 1, learning rate: 0.0001, train loss: 0.5822
INFO:root:train average_precision, 0.7933
INFO:root:train roc_auc, 0.7650
INFO:root:validate loss: 0.6114
INFO:root:validate average_precision, 0.7662
INFO:root:validate roc_auc, 0.7726
INFO:root:new node validate loss: 0.7879
INFO:root:new node validate first_1_average_precision, 0.6509
INFO:root:new node validate first_1_roc_auc, 0.6161
INFO:root:new node validate first_3_average_precision, 0.6397
INFO:root:new node validate first_3_roc_auc, 0.5977
INFO:root:new node validate first_10_average_precision, 0.6522
INFO:root:new node validate first_10_roc_auc, 0.6140
INFO:root:new node validate average_precision, 0.6739
INFO:root:new node validate roc_auc, 0.6364
INFO:root:save model ./saved_models/TGN/ia-digg-reply/TGN_seed0_tgn-ia-digg-reply-reparamcorr-time-mlp/TGN_seed0_tgn-ia-digg-reply-reparamcorr-time-mlp.pkl
Epoch: 2, train for the 106-th batch, train loss: 0.49191901087760925:  44%|████▎     | 105/241 [00:16<00:23,  5.76it/s]Epoch: 2, train for the 106-th batch, train loss: 0.49191901087760925:  44%|████▍     | 106/241 [00:16<00:22,  5.89it/s]Epoch: 1, train for the 284-th batch, train loss: 0.465970903635025:  74%|████████▊   | 283/383 [01:21<00:32,  3.05it/s]Epoch: 3, train for the 129-th batch, train loss: 0.5111098885536194:  88%|█████████▋ | 128/146 [00:20<00:02,  6.34it/s]Epoch: 3, train for the 129-th batch, train loss: 0.5111098885536194:  88%|█████████▋ | 129/146 [00:20<00:02,  6.36it/s]Epoch: 4, train for the 27-th batch, train loss: 0.45974281430244446:  22%|██▌         | 26/119 [00:04<00:13,  6.99it/s]Epoch: 4, train for the 27-th batch, train loss: 0.45974281430244446:  23%|██▋         | 27/119 [00:04<00:14,  6.48it/s]Epoch: 1, train for the 284-th batch, train loss: 0.465970903635025:  74%|████████▉   | 284/383 [01:22<00:29,  3.31it/s]Epoch: 3, train for the 34-th batch, train loss: 0.6245743632316589:  22%|██▊          | 33/151 [00:05<00:18,  6.25it/s]Epoch: 3, train for the 34-th batch, train loss: 0.6245743632316589:  23%|██▉          | 34/151 [00:05<00:18,  6.18it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5416144728660583:  44%|████▊      | 106/241 [00:16<00:22,  5.89it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5416144728660583:  44%|████▉      | 107/241 [00:16<00:22,  6.09it/s]Epoch: 3, train for the 130-th batch, train loss: 0.4453789293766022:  88%|█████████▋ | 129/146 [00:20<00:02,  6.36it/s]Epoch: 3, train for the 130-th batch, train loss: 0.4453789293766022:  89%|█████████▊ | 130/146 [00:20<00:02,  6.60it/s]Epoch: 4, train for the 28-th batch, train loss: 0.45526304841041565:  23%|██▋         | 27/119 [00:04<00:14,  6.48it/s]Epoch: 4, train for the 28-th batch, train loss: 0.45526304841041565:  24%|██▊         | 28/119 [00:04<00:13,  6.54it/s]Epoch: 3, train for the 35-th batch, train loss: 0.6311126351356506:  23%|██▉          | 34/151 [00:05<00:18,  6.18it/s]Epoch: 3, train for the 35-th batch, train loss: 0.6311126351356506:  23%|███          | 35/151 [00:05<00:19,  6.10it/s]Epoch: 2, train for the 108-th batch, train loss: 0.5090238451957703:  44%|████▉      | 107/241 [00:17<00:22,  6.09it/s]Epoch: 1, train for the 285-th batch, train loss: 0.39265885949134827:  74%|███████▍  | 284/383 [01:22<00:29,  3.31it/s]Epoch: 2, train for the 108-th batch, train loss: 0.5090238451957703:  45%|████▉      | 108/241 [00:17<00:21,  6.19it/s]Epoch: 3, train for the 131-th batch, train loss: 0.5204792022705078:  89%|█████████▊ | 130/146 [00:21<00:02,  6.60it/s]Epoch: 3, train for the 131-th batch, train loss: 0.5204792022705078:  90%|█████████▊ | 131/146 [00:21<00:02,  6.27it/s]Epoch: 4, train for the 29-th batch, train loss: 0.41305357217788696:  24%|██▊         | 28/119 [00:04<00:13,  6.54it/s]Epoch: 1, train for the 285-th batch, train loss: 0.39265885949134827:  74%|███████▍  | 285/383 [01:22<00:29,  3.38it/s]Epoch: 4, train for the 29-th batch, train loss: 0.41305357217788696:  24%|██▉         | 29/119 [00:04<00:13,  6.63it/s]Epoch: 3, train for the 36-th batch, train loss: 0.6802846193313599:  23%|███          | 35/151 [00:05<00:19,  6.10it/s]Epoch: 3, train for the 36-th batch, train loss: 0.6802846193313599:  24%|███          | 36/151 [00:05<00:19,  5.98it/s]Epoch: 2, train for the 109-th batch, train loss: 0.6856673359870911:  45%|████▉      | 108/241 [00:17<00:21,  6.19it/s]Epoch: 2, train for the 109-th batch, train loss: 0.6856673359870911:  45%|████▉      | 109/241 [00:17<00:21,  6.10it/s]Epoch: 4, train for the 30-th batch, train loss: 0.3738763630390167:  24%|███▏         | 29/119 [00:04<00:13,  6.63it/s]Epoch: 4, train for the 30-th batch, train loss: 0.3738763630390167:  25%|███▎         | 30/119 [00:04<00:14,  6.25it/s]Epoch: 3, train for the 132-th batch, train loss: 0.5126853585243225:  90%|█████████▊ | 131/146 [00:21<00:02,  6.27it/s]Epoch: 3, train for the 132-th batch, train loss: 0.5126853585243225:  90%|█████████▉ | 132/146 [00:21<00:02,  5.78it/s]Epoch: 1, train for the 286-th batch, train loss: 0.3810611367225647:  74%|████████▏  | 285/383 [01:22<00:29,  3.38it/s]Epoch: 3, train for the 37-th batch, train loss: 0.6656463742256165:  24%|███          | 36/151 [00:05<00:19,  5.98it/s]Epoch: 3, train for the 37-th batch, train loss: 0.6656463742256165:  25%|███▏         | 37/151 [00:05<00:18,  6.08it/s]Epoch: 1, train for the 286-th batch, train loss: 0.3810611367225647:  75%|████████▏  | 286/383 [01:22<00:27,  3.52it/s]Epoch: 2, train for the 110-th batch, train loss: 0.5918310284614563:  45%|████▉      | 109/241 [00:17<00:21,  6.10it/s]Epoch: 2, train for the 110-th batch, train loss: 0.5918310284614563:  46%|█████      | 110/241 [00:17<00:21,  6.07it/s]Epoch: 4, train for the 31-th batch, train loss: 0.45462021231651306:  25%|███         | 30/119 [00:04<00:14,  6.25it/s]Epoch: 4, train for the 31-th batch, train loss: 0.45462021231651306:  26%|███▏        | 31/119 [00:04<00:13,  6.55it/s]Epoch: 3, train for the 133-th batch, train loss: 0.47368937730789185:  90%|█████████ | 132/146 [00:21<00:02,  5.78it/s]Epoch: 3, train for the 38-th batch, train loss: 0.6326435208320618:  25%|███▏         | 37/151 [00:06<00:18,  6.08it/s]Epoch: 3, train for the 133-th batch, train loss: 0.47368937730789185:  91%|█████████ | 133/146 [00:21<00:02,  5.68it/s]Epoch: 3, train for the 38-th batch, train loss: 0.6326435208320618:  25%|███▎         | 38/151 [00:06<00:17,  6.41it/s]Epoch: 1, train for the 287-th batch, train loss: 0.40030142664909363:  75%|███████▍  | 286/383 [01:22<00:27,  3.52it/s]Epoch: 2, train for the 111-th batch, train loss: 0.2048584669828415:  46%|█████      | 110/241 [00:17<00:21,  6.07it/s]Epoch: 4, train for the 32-th batch, train loss: 0.40369734168052673:  26%|███▏        | 31/119 [00:04<00:13,  6.55it/s]Epoch: 2, train for the 111-th batch, train loss: 0.2048584669828415:  46%|█████      | 111/241 [00:17<00:21,  5.93it/s]Epoch: 4, train for the 32-th batch, train loss: 0.40369734168052673:  27%|███▏        | 32/119 [00:04<00:12,  6.78it/s]Epoch: 1, train for the 287-th batch, train loss: 0.40030142664909363:  75%|███████▍  | 287/383 [01:22<00:25,  3.76it/s]Epoch: 3, train for the 134-th batch, train loss: 0.5328125357627869:  91%|██████████ | 133/146 [00:21<00:02,  5.68it/s]Epoch: 3, train for the 39-th batch, train loss: 0.6255469918251038:  25%|███▎         | 38/151 [00:06<00:17,  6.41it/s]Epoch: 3, train for the 134-th batch, train loss: 0.5328125357627869:  92%|██████████ | 134/146 [00:21<00:02,  5.95it/s]Epoch: 3, train for the 39-th batch, train loss: 0.6255469918251038:  26%|███▎         | 39/151 [00:06<00:17,  6.41it/s]Epoch: 2, train for the 112-th batch, train loss: 0.3427527844905853:  46%|█████      | 111/241 [00:17<00:21,  5.93it/s]Epoch: 4, train for the 33-th batch, train loss: 0.4367481768131256:  27%|███▍         | 32/119 [00:05<00:12,  6.78it/s]Epoch: 4, train for the 33-th batch, train loss: 0.4367481768131256:  28%|███▌         | 33/119 [00:05<00:12,  6.90it/s]Epoch: 2, train for the 112-th batch, train loss: 0.3427527844905853:  46%|█████      | 112/241 [00:17<00:20,  6.17it/s]Epoch: 3, train for the 135-th batch, train loss: 0.5031858086585999:  92%|██████████ | 134/146 [00:21<00:02,  5.95it/s]Epoch: 3, train for the 135-th batch, train loss: 0.5031858086585999:  92%|██████████▏| 135/146 [00:21<00:01,  6.17it/s]Epoch: 3, train for the 40-th batch, train loss: 0.6042681336402893:  26%|███▎         | 39/151 [00:06<00:17,  6.41it/s]Epoch: 3, train for the 40-th batch, train loss: 0.6042681336402893:  26%|███▍         | 40/151 [00:06<00:17,  6.36it/s]Epoch: 1, train for the 288-th batch, train loss: 0.5165955424308777:  75%|████████▏  | 287/383 [01:23<00:25,  3.76it/s]Epoch: 4, train for the 34-th batch, train loss: 0.3993905186653137:  28%|███▌         | 33/119 [00:05<00:12,  6.90it/s]Epoch: 2, train for the 113-th batch, train loss: 0.4074980914592743:  46%|█████      | 112/241 [00:17<00:20,  6.17it/s]Epoch: 4, train for the 34-th batch, train loss: 0.3993905186653137:  29%|███▋         | 34/119 [00:05<00:12,  6.93it/s]Epoch: 2, train for the 113-th batch, train loss: 0.4074980914592743:  47%|█████▏     | 113/241 [00:17<00:20,  6.26it/s]Epoch: 1, train for the 288-th batch, train loss: 0.5165955424308777:  75%|████████▎  | 288/383 [01:23<00:26,  3.65it/s]Epoch: 3, train for the 136-th batch, train loss: 0.5090327262878418:  92%|██████████▏| 135/146 [00:21<00:01,  6.17it/s]Epoch: 3, train for the 136-th batch, train loss: 0.5090327262878418:  93%|██████████▏| 136/146 [00:21<00:01,  6.37it/s]Epoch: 3, train for the 41-th batch, train loss: 0.5921022891998291:  26%|███▍         | 40/151 [00:06<00:17,  6.36it/s]Epoch: 3, train for the 41-th batch, train loss: 0.5921022891998291:  27%|███▌         | 41/151 [00:06<00:17,  6.38it/s]Epoch: 4, train for the 35-th batch, train loss: 0.39483916759490967:  29%|███▍        | 34/119 [00:05<00:12,  6.93it/s]Epoch: 4, train for the 35-th batch, train loss: 0.39483916759490967:  29%|███▌        | 35/119 [00:05<00:12,  6.90it/s]Epoch: 2, train for the 114-th batch, train loss: 0.43011438846588135:  47%|████▋     | 113/241 [00:18<00:20,  6.26it/s]Epoch: 2, train for the 114-th batch, train loss: 0.43011438846588135:  47%|████▋     | 114/241 [00:18<00:20,  6.33it/s]Epoch: 3, train for the 137-th batch, train loss: 0.4606139659881592:  93%|██████████▏| 136/146 [00:22<00:01,  6.37it/s]Epoch: 1, train for the 289-th batch, train loss: 0.48665952682495117:  75%|███████▌  | 288/383 [01:23<00:26,  3.65it/s]Epoch: 3, train for the 137-th batch, train loss: 0.4606139659881592:  94%|██████████▎| 137/146 [00:22<00:01,  6.34it/s]Epoch: 4, train for the 36-th batch, train loss: 0.388598769903183:  29%|████          | 35/119 [00:05<00:12,  6.90it/s]Epoch: 4, train for the 36-th batch, train loss: 0.388598769903183:  30%|████▏         | 36/119 [00:05<00:11,  7.04it/s]Epoch: 1, train for the 289-th batch, train loss: 0.48665952682495117:  75%|███████▌  | 289/383 [01:23<00:25,  3.70it/s]Epoch: 3, train for the 42-th batch, train loss: 0.598432183265686:  27%|███▊          | 41/151 [00:06<00:17,  6.38it/s]Epoch: 3, train for the 42-th batch, train loss: 0.598432183265686:  28%|███▉          | 42/151 [00:06<00:18,  5.95it/s]Epoch: 2, train for the 115-th batch, train loss: 0.4182455241680145:  47%|█████▏     | 114/241 [00:18<00:20,  6.33it/s]Epoch: 2, train for the 115-th batch, train loss: 0.4182455241680145:  48%|█████▏     | 115/241 [00:18<00:20,  6.24it/s]Epoch: 3, train for the 138-th batch, train loss: 0.5177735090255737:  94%|██████████▎| 137/146 [00:22<00:01,  6.34it/s]Epoch: 3, train for the 138-th batch, train loss: 0.5177735090255737:  95%|██████████▍| 138/146 [00:22<00:01,  6.12it/s]Epoch: 4, train for the 37-th batch, train loss: 0.4312410354614258:  30%|███▉         | 36/119 [00:05<00:11,  7.04it/s]Epoch: 4, train for the 37-th batch, train loss: 0.4312410354614258:  31%|████         | 37/119 [00:05<00:11,  6.86it/s]Epoch: 3, train for the 43-th batch, train loss: 0.6316748857498169:  28%|███▌         | 42/151 [00:06<00:18,  5.95it/s]Epoch: 2, train for the 116-th batch, train loss: 0.5168648958206177:  48%|█████▏     | 115/241 [00:18<00:20,  6.24it/s]Epoch: 3, train for the 43-th batch, train loss: 0.6316748857498169:  28%|███▋         | 43/151 [00:06<00:19,  5.68it/s]Epoch: 2, train for the 116-th batch, train loss: 0.5168648958206177:  48%|█████▎     | 116/241 [00:18<00:20,  6.06it/s]Epoch: 1, train for the 290-th batch, train loss: 0.4319247007369995:  75%|████████▎  | 289/383 [01:23<00:25,  3.70it/s]Epoch: 1, train for the 290-th batch, train loss: 0.4319247007369995:  76%|████████▎  | 290/383 [01:23<00:26,  3.53it/s]Epoch: 3, train for the 139-th batch, train loss: 0.5091695785522461:  95%|██████████▍| 138/146 [00:22<00:01,  6.12it/s]Epoch: 4, train for the 38-th batch, train loss: 0.4070744514465332:  31%|████         | 37/119 [00:05<00:11,  6.86it/s]Epoch: 4, train for the 38-th batch, train loss: 0.4070744514465332:  32%|████▏        | 38/119 [00:05<00:12,  6.38it/s]Epoch: 3, train for the 139-th batch, train loss: 0.5091695785522461:  95%|██████████▍| 139/146 [00:22<00:01,  5.75it/s]Epoch: 3, train for the 44-th batch, train loss: 0.5607930421829224:  28%|███▋         | 43/151 [00:07<00:19,  5.68it/s]Epoch: 2, train for the 117-th batch, train loss: 0.5836269855499268:  48%|█████▎     | 116/241 [00:18<00:20,  6.06it/s]Epoch: 3, train for the 44-th batch, train loss: 0.5607930421829224:  29%|███▊         | 44/151 [00:07<00:18,  5.64it/s]Epoch: 2, train for the 117-th batch, train loss: 0.5836269855499268:  49%|█████▎     | 117/241 [00:18<00:21,  5.76it/s]Epoch: 4, train for the 39-th batch, train loss: 0.4357285499572754:  32%|████▏        | 38/119 [00:05<00:12,  6.38it/s]Epoch: 4, train for the 39-th batch, train loss: 0.4357285499572754:  33%|████▎        | 39/119 [00:05<00:11,  6.68it/s]Epoch: 1, train for the 291-th batch, train loss: 0.3581416606903076:  76%|████████▎  | 290/383 [01:23<00:26,  3.53it/s]Epoch: 1, train for the 291-th batch, train loss: 0.3581416606903076:  76%|████████▎  | 291/383 [01:23<00:25,  3.60it/s]Epoch: 3, train for the 45-th batch, train loss: 0.5759080648422241:  29%|███▊         | 44/151 [00:07<00:18,  5.64it/s]Epoch: 2, train for the 118-th batch, train loss: 0.4843919277191162:  49%|█████▎     | 117/241 [00:18<00:21,  5.76it/s]Epoch: 3, train for the 45-th batch, train loss: 0.5759080648422241:  30%|███▊         | 45/151 [00:07<00:19,  5.43it/s]Epoch: 2, train for the 118-th batch, train loss: 0.4843919277191162:  49%|█████▍     | 118/241 [00:18<00:21,  5.65it/s]Epoch: 4, train for the 40-th batch, train loss: 0.3751161992549896:  33%|████▎        | 39/119 [00:06<00:11,  6.68it/s]Epoch: 3, train for the 140-th batch, train loss: 0.4990125894546509:  95%|██████████▍| 139/146 [00:22<00:01,  5.75it/s]Epoch: 4, train for the 40-th batch, train loss: 0.3751161992549896:  34%|████▎        | 40/119 [00:06<00:12,  6.37it/s]Epoch: 3, train for the 140-th batch, train loss: 0.4990125894546509:  96%|██████████▌| 140/146 [00:22<00:01,  4.61it/s]Epoch: 3, train for the 46-th batch, train loss: 0.614779531955719:  30%|████▏         | 45/151 [00:07<00:19,  5.43it/s]Epoch: 3, train for the 46-th batch, train loss: 0.614779531955719:  30%|████▎         | 46/151 [00:07<00:18,  5.75it/s]Epoch: 2, train for the 119-th batch, train loss: 0.361030638217926:  49%|█████▉      | 118/241 [00:18<00:21,  5.65it/s]Epoch: 4, train for the 41-th batch, train loss: 0.47845420241355896:  34%|████        | 40/119 [00:06<00:12,  6.37it/s]Epoch: 2, train for the 119-th batch, train loss: 0.361030638217926:  49%|█████▉      | 119/241 [00:19<00:21,  5.56it/s]Epoch: 4, train for the 41-th batch, train loss: 0.47845420241355896:  34%|████▏       | 41/119 [00:06<00:12,  6.33it/s]Epoch: 3, train for the 141-th batch, train loss: 0.5121387839317322:  96%|██████████▌| 140/146 [00:22<00:01,  4.61it/s]Epoch: 3, train for the 141-th batch, train loss: 0.5121387839317322:  97%|██████████▌| 141/146 [00:22<00:01,  4.78it/s]Epoch: 1, train for the 292-th batch, train loss: 0.4656851887702942:  76%|████████▎  | 291/383 [01:24<00:25,  3.60it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/237 [00:00<?, ?it/s]Epoch: 3, train for the 47-th batch, train loss: 0.5851758718490601:  30%|███▉         | 46/151 [00:07<00:18,  5.75it/s]Epoch: 1, train for the 292-th batch, train loss: 0.4656851887702942:  76%|████████▍  | 292/383 [01:24<00:27,  3.34it/s]Epoch: 3, train for the 47-th batch, train loss: 0.5851758718490601:  31%|████         | 47/151 [00:07<00:18,  5.59it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5758571028709412:  49%|█████▍     | 119/241 [00:19<00:21,  5.56it/s]Epoch: 3, train for the 142-th batch, train loss: 0.5085370540618896:  97%|██████████▌| 141/146 [00:23<00:01,  4.78it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5758571028709412:  50%|█████▍     | 120/241 [00:19<00:21,  5.66it/s]Epoch: 3, train for the 142-th batch, train loss: 0.5085370540618896:  97%|██████████▋| 142/146 [00:23<00:00,  5.47it/s]Epoch: 2, train for the 1-th batch, train loss: 0.6135299801826477:   0%|                       | 0/237 [00:00<?, ?it/s]Epoch: 2, train for the 1-th batch, train loss: 0.6135299801826477:   0%|               | 1/237 [00:00<00:28,  8.40it/s]Epoch: 4, train for the 42-th batch, train loss: 0.4225906729698181:  34%|████▍        | 41/119 [00:06<00:12,  6.33it/s]Epoch: 4, train for the 42-th batch, train loss: 0.4225906729698181:  35%|████▌        | 42/119 [00:06<00:14,  5.31it/s]Epoch: 3, train for the 143-th batch, train loss: 0.5165820717811584:  97%|██████████▋| 142/146 [00:23<00:00,  5.47it/s]Epoch: 3, train for the 143-th batch, train loss: 0.5165820717811584:  98%|██████████▊| 143/146 [00:23<00:00,  5.84it/s]Epoch: 3, train for the 48-th batch, train loss: 0.5521039366722107:  31%|████         | 47/151 [00:07<00:18,  5.59it/s]Epoch: 2, train for the 121-th batch, train loss: 0.5452501177787781:  50%|█████▍     | 120/241 [00:19<00:21,  5.66it/s]Epoch: 3, train for the 48-th batch, train loss: 0.5521039366722107:  32%|████▏        | 48/151 [00:07<00:19,  5.42it/s]Epoch: 2, train for the 121-th batch, train loss: 0.5452501177787781:  50%|█████▌     | 121/241 [00:19<00:21,  5.59it/s]Epoch: 2, train for the 2-th batch, train loss: 0.600618302822113:   0%|                | 1/237 [00:00<00:28,  8.40it/s]Epoch: 2, train for the 2-th batch, train loss: 0.600618302822113:   1%|▏               | 2/237 [00:00<00:34,  6.87it/s]Epoch: 3, train for the 144-th batch, train loss: 0.4600871503353119:  98%|██████████▊| 143/146 [00:23<00:00,  5.84it/s]Epoch: 3, train for the 144-th batch, train loss: 0.4600871503353119:  99%|██████████▊| 144/146 [00:23<00:00,  6.29it/s]Epoch: 1, train for the 293-th batch, train loss: 0.4493894875049591:  76%|████████▍  | 292/383 [01:24<00:27,  3.34it/s]Epoch: 1, train for the 293-th batch, train loss: 0.4493894875049591:  77%|████████▍  | 293/383 [01:24<00:28,  3.14it/s]Epoch: 2, train for the 3-th batch, train loss: 0.6144781112670898:   1%|▏              | 2/237 [00:00<00:34,  6.87it/s]Epoch: 3, train for the 49-th batch, train loss: 0.5791757106781006:  32%|████▏        | 48/151 [00:08<00:19,  5.42it/s]Epoch: 2, train for the 3-th batch, train loss: 0.6144781112670898:   1%|▏              | 3/237 [00:00<00:30,  7.68it/s]Epoch: 2, train for the 122-th batch, train loss: 0.4261506497859955:  50%|█████▌     | 121/241 [00:19<00:21,  5.59it/s]Epoch: 3, train for the 49-th batch, train loss: 0.5791757106781006:  32%|████▏        | 49/151 [00:08<00:18,  5.49it/s]Epoch: 2, train for the 122-th batch, train loss: 0.4261506497859955:  51%|█████▌     | 122/241 [00:19<00:21,  5.56it/s]Epoch: 3, train for the 145-th batch, train loss: 0.5080365538597107:  99%|██████████▊| 144/146 [00:23<00:00,  6.29it/s]Epoch: 3, train for the 145-th batch, train loss: 0.5080365538597107:  99%|██████████▉| 145/146 [00:23<00:00,  6.70it/s]Epoch: 4, train for the 43-th batch, train loss: 0.46001872420310974:  35%|████▏       | 42/119 [00:06<00:14,  5.31it/s]Epoch: 2, train for the 4-th batch, train loss: 0.6015118360519409:   1%|▏              | 3/237 [00:00<00:30,  7.68it/s]Epoch: 4, train for the 43-th batch, train loss: 0.46001872420310974:  36%|████▎       | 43/119 [00:06<00:19,  3.99it/s]Epoch: 2, train for the 4-th batch, train loss: 0.6015118360519409:   2%|▎              | 4/237 [00:00<00:31,  7.29it/s]Epoch: 3, train for the 146-th batch, train loss: 0.4451356828212738:  99%|██████████▉| 145/146 [00:23<00:00,  6.70it/s]Epoch: 3, train for the 50-th batch, train loss: 0.5811839699745178:  32%|████▏        | 49/151 [00:08<00:18,  5.49it/s]Epoch: 3, train for the 146-th batch, train loss: 0.4451356828212738: 100%|███████████| 146/146 [00:23<00:00,  6.72it/s]Epoch: 3, train for the 146-th batch, train loss: 0.4451356828212738: 100%|███████████| 146/146 [00:23<00:00,  6.19it/s]
Epoch: 2, train for the 123-th batch, train loss: 0.6674952507019043:  51%|█████▌     | 122/241 [00:19<00:21,  5.56it/s]Epoch: 3, train for the 50-th batch, train loss: 0.5811839699745178:  33%|████▎        | 50/151 [00:08<00:18,  5.34it/s]Epoch: 2, train for the 123-th batch, train loss: 0.6674952507019043:  51%|█████▌     | 123/241 [00:19<00:22,  5.34it/s]Epoch: 1, train for the 294-th batch, train loss: 0.2849140167236328:  77%|████████▍  | 293/383 [01:24<00:28,  3.14it/s]  0%|                                                                                            | 0/38 [00:00<?, ?it/s]Epoch: 2, train for the 5-th batch, train loss: 0.6411134004592896:   2%|▎              | 4/237 [00:00<00:31,  7.29it/s]Epoch: 4, train for the 44-th batch, train loss: 0.4034874737262726:  36%|████▋        | 43/119 [00:07<00:19,  3.99it/s]Epoch: 2, train for the 5-th batch, train loss: 0.6411134004592896:   2%|▎              | 5/237 [00:00<00:32,  7.12it/s]Epoch: 4, train for the 44-th batch, train loss: 0.4034874737262726:  37%|████▊        | 44/119 [00:07<00:16,  4.52it/s]Epoch: 1, train for the 294-th batch, train loss: 0.2849140167236328:  77%|████████▍  | 294/383 [01:24<00:28,  3.10it/s]evaluate for the 1-th batch, evaluate loss: 0.6155637502670288:   0%|                            | 0/38 [00:00<?, ?it/s]Epoch: 3, train for the 51-th batch, train loss: 0.6116763353347778:  33%|████▎        | 50/151 [00:08<00:18,  5.34it/s]Epoch: 2, train for the 124-th batch, train loss: 0.39240092039108276:  51%|█████     | 123/241 [00:19<00:22,  5.34it/s]Epoch: 3, train for the 51-th batch, train loss: 0.6116763353347778:  34%|████▍        | 51/151 [00:08<00:19,  5.19it/s]Epoch: 2, train for the 124-th batch, train loss: 0.39240092039108276:  51%|█████▏    | 124/241 [00:19<00:22,  5.31it/s]Epoch: 2, train for the 6-th batch, train loss: 0.6864396333694458:   2%|▎              | 5/237 [00:00<00:32,  7.12it/s]Epoch: 2, train for the 6-th batch, train loss: 0.6864396333694458:   3%|▍              | 6/237 [00:00<00:32,  7.11it/s]evaluate for the 2-th batch, evaluate loss: 0.6237928867340088:   0%|                            | 0/38 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.6237928867340088:   5%|█                   | 2/38 [00:00<00:02, 12.78it/s]Epoch: 4, train for the 45-th batch, train loss: 0.35748475790023804:  37%|████▍       | 44/119 [00:07<00:16,  4.52it/s]Epoch: 4, train for the 45-th batch, train loss: 0.35748475790023804:  38%|████▌       | 45/119 [00:07<00:14,  5.00it/s]evaluate for the 3-th batch, evaluate loss: 0.5573493242263794:   5%|█                   | 2/38 [00:00<00:02, 12.78it/s]Epoch: 2, train for the 7-th batch, train loss: 0.6772551536560059:   3%|▍              | 6/237 [00:00<00:32,  7.11it/s]Epoch: 2, train for the 7-th batch, train loss: 0.6772551536560059:   3%|▍              | 7/237 [00:00<00:33,  6.86it/s]Epoch: 4, train for the 46-th batch, train loss: 0.3995799422264099:  38%|████▉        | 45/119 [00:07<00:14,  5.00it/s]evaluate for the 4-th batch, evaluate loss: 0.5607390999794006:   5%|█                   | 2/38 [00:00<00:02, 12.78it/s]evaluate for the 4-th batch, evaluate loss: 0.5607390999794006:  11%|██                  | 4/38 [00:00<00:03, 11.06it/s]Epoch: 4, train for the 46-th batch, train loss: 0.3995799422264099:  39%|█████        | 46/119 [00:07<00:14,  5.01it/s]Epoch: 3, train for the 52-th batch, train loss: 0.6126105189323425:  34%|████▍        | 51/151 [00:08<00:19,  5.19it/s]Epoch: 2, train for the 125-th batch, train loss: 0.4750988781452179:  51%|█████▋     | 124/241 [00:20<00:22,  5.31it/s]Epoch: 1, train for the 295-th batch, train loss: 0.32240405678749084:  77%|███████▋  | 294/383 [01:25<00:28,  3.10it/s]Epoch: 3, train for the 52-th batch, train loss: 0.6126105189323425:  34%|████▍        | 52/151 [00:08<00:20,  4.76it/s]Epoch: 2, train for the 125-th batch, train loss: 0.4750988781452179:  52%|█████▋     | 125/241 [00:20<00:24,  4.80it/s]evaluate for the 5-th batch, evaluate loss: 0.649971067905426:  11%|██▏                  | 4/38 [00:00<00:03, 11.06it/s]Epoch: 2, train for the 8-th batch, train loss: 0.7220895290374756:   3%|▍              | 7/237 [00:01<00:33,  6.86it/s]Epoch: 2, train for the 8-th batch, train loss: 0.7220895290374756:   3%|▌              | 8/237 [00:01<00:31,  7.17it/s]Epoch: 1, train for the 295-th batch, train loss: 0.32240405678749084:  77%|███████▋  | 295/383 [01:25<00:30,  2.86it/s]Epoch: 4, train for the 47-th batch, train loss: 0.45359209179878235:  39%|████▋       | 46/119 [00:07<00:14,  5.01it/s]evaluate for the 6-th batch, evaluate loss: 0.5734652876853943:  11%|██                  | 4/38 [00:00<00:03, 11.06it/s]evaluate for the 6-th batch, evaluate loss: 0.5734652876853943:  16%|███▏                | 6/38 [00:00<00:02, 11.46it/s]Epoch: 4, train for the 47-th batch, train loss: 0.45359209179878235:  39%|████▋       | 47/119 [00:07<00:13,  5.24it/s]Epoch: 2, train for the 9-th batch, train loss: 0.617381751537323:   3%|▌               | 8/237 [00:01<00:31,  7.17it/s]Epoch: 2, train for the 9-th batch, train loss: 0.617381751537323:   4%|▌               | 9/237 [00:01<00:32,  7.04it/s]evaluate for the 7-th batch, evaluate loss: 0.5458050966262817:  16%|███▏                | 6/38 [00:00<00:02, 11.46it/s]Epoch: 2, train for the 126-th batch, train loss: 0.4434753954410553:  52%|█████▋     | 125/241 [00:20<00:24,  4.80it/s]Epoch: 3, train for the 53-th batch, train loss: 0.5817588567733765:  34%|████▍        | 52/151 [00:08<00:20,  4.76it/s]Epoch: 2, train for the 126-th batch, train loss: 0.4434753954410553:  52%|█████▊     | 126/241 [00:20<00:24,  4.62it/s]Epoch: 3, train for the 53-th batch, train loss: 0.5817588567733765:  35%|████▌        | 53/151 [00:08<00:21,  4.52it/s]evaluate for the 8-th batch, evaluate loss: 0.5325716733932495:  16%|███▏                | 6/38 [00:00<00:02, 11.46it/s]evaluate for the 8-th batch, evaluate loss: 0.5325716733932495:  21%|████▏               | 8/38 [00:00<00:02, 11.66it/s]Epoch: 4, train for the 48-th batch, train loss: 0.451046884059906:  39%|█████▌        | 47/119 [00:07<00:13,  5.24it/s]Epoch: 1, train for the 296-th batch, train loss: 0.2834266722202301:  77%|████████▍  | 295/383 [01:25<00:30,  2.86it/s]Epoch: 4, train for the 48-th batch, train loss: 0.451046884059906:  40%|█████▋        | 48/119 [00:07<00:13,  5.41it/s]Epoch: 1, train for the 296-th batch, train loss: 0.2834266722202301:  77%|████████▌  | 296/383 [01:25<00:29,  2.97it/s]evaluate for the 9-th batch, evaluate loss: 0.5650980472564697:  21%|████▏               | 8/38 [00:00<00:02, 11.66it/s]Epoch: 2, train for the 127-th batch, train loss: 0.4991924464702606:  52%|█████▊     | 126/241 [00:20<00:24,  4.62it/s]Epoch: 2, train for the 127-th batch, train loss: 0.4991924464702606:  53%|█████▊     | 127/241 [00:20<00:22,  5.02it/s]Epoch: 3, train for the 54-th batch, train loss: 0.5394064784049988:  35%|████▌        | 53/151 [00:09<00:21,  4.52it/s]Epoch: 3, train for the 54-th batch, train loss: 0.5394064784049988:  36%|████▋        | 54/151 [00:09<00:20,  4.74it/s]Epoch: 2, train for the 10-th batch, train loss: 0.7241436839103699:   4%|▌             | 9/237 [00:01<00:32,  7.04it/s]Epoch: 2, train for the 10-th batch, train loss: 0.7241436839103699:   4%|▌            | 10/237 [00:01<00:42,  5.34it/s]Epoch: 4, train for the 49-th batch, train loss: 0.4263995289802551:  40%|█████▏       | 48/119 [00:07<00:13,  5.41it/s]evaluate for the 10-th batch, evaluate loss: 0.5631005167961121:  21%|████               | 8/38 [00:00<00:02, 11.66it/s]evaluate for the 10-th batch, evaluate loss: 0.5631005167961121:  26%|████▋             | 10/38 [00:00<00:02, 11.23it/s]Epoch: 4, train for the 49-th batch, train loss: 0.4263995289802551:  41%|█████▎       | 49/119 [00:07<00:12,  5.41it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5253947973251343:  53%|█████▊     | 127/241 [00:20<00:22,  5.02it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5253947973251343:  53%|█████▊     | 128/241 [00:20<00:21,  5.38it/s]evaluate for the 11-th batch, evaluate loss: 0.48662853240966797:  26%|████▍            | 10/38 [00:00<00:02, 11.23it/s]Epoch: 3, train for the 55-th batch, train loss: 0.5640508532524109:  36%|████▋        | 54/151 [00:09<00:20,  4.74it/s]Epoch: 1, train for the 297-th batch, train loss: 0.5113112926483154:  77%|████████▌  | 296/383 [01:25<00:29,  2.97it/s]Epoch: 3, train for the 55-th batch, train loss: 0.5640508532524109:  36%|████▋        | 55/151 [00:09<00:19,  4.94it/s]Epoch: 2, train for the 11-th batch, train loss: 0.6297860741615295:   4%|▌            | 10/237 [00:01<00:42,  5.34it/s]Epoch: 2, train for the 11-th batch, train loss: 0.6297860741615295:   5%|▌            | 11/237 [00:01<00:41,  5.51it/s]Epoch: 1, train for the 297-th batch, train loss: 0.5113112926483154:  78%|████████▌  | 297/383 [01:25<00:27,  3.09it/s]Epoch: 4, train for the 50-th batch, train loss: 0.3652593493461609:  41%|█████▎       | 49/119 [00:08<00:12,  5.41it/s]evaluate for the 12-th batch, evaluate loss: 0.5900306701660156:  26%|████▋             | 10/38 [00:01<00:02, 11.23it/s]evaluate for the 12-th batch, evaluate loss: 0.5900306701660156:  32%|█████▋            | 12/38 [00:01<00:02, 11.05it/s]Epoch: 2, train for the 129-th batch, train loss: 0.4203135669231415:  53%|█████▊     | 128/241 [00:20<00:21,  5.38it/s]Epoch: 4, train for the 50-th batch, train loss: 0.3652593493461609:  42%|█████▍       | 50/119 [00:08<00:12,  5.40it/s]Epoch: 2, train for the 129-th batch, train loss: 0.4203135669231415:  54%|█████▉     | 129/241 [00:20<00:19,  5.76it/s]Epoch: 3, train for the 56-th batch, train loss: 0.4815652668476105:  36%|████▋        | 55/151 [00:09<00:19,  4.94it/s]Epoch: 3, train for the 56-th batch, train loss: 0.4815652668476105:  37%|████▊        | 56/151 [00:09<00:17,  5.35it/s]evaluate for the 13-th batch, evaluate loss: 0.5541127920150757:  32%|█████▋            | 12/38 [00:01<00:02, 11.05it/s]Epoch: 2, train for the 12-th batch, train loss: 0.6913085579872131:   5%|▌            | 11/237 [00:01<00:41,  5.51it/s]Epoch: 2, train for the 12-th batch, train loss: 0.6913085579872131:   5%|▋            | 12/237 [00:01<00:40,  5.50it/s]Epoch: 4, train for the 51-th batch, train loss: 0.468763142824173:  42%|█████▉        | 50/119 [00:08<00:12,  5.40it/s]evaluate for the 14-th batch, evaluate loss: 0.47636085748672485:  32%|█████▎           | 12/38 [00:01<00:02, 11.05it/s]evaluate for the 14-th batch, evaluate loss: 0.47636085748672485:  37%|██████▎          | 14/38 [00:01<00:02, 11.10it/s]Epoch: 4, train for the 51-th batch, train loss: 0.468763142824173:  43%|██████        | 51/119 [00:08<00:12,  5.53it/s]Epoch: 2, train for the 130-th batch, train loss: 0.4808712601661682:  54%|█████▉     | 129/241 [00:21<00:19,  5.76it/s]Epoch: 2, train for the 130-th batch, train loss: 0.4808712601661682:  54%|█████▉     | 130/241 [00:21<00:20,  5.40it/s]Epoch: 3, train for the 57-th batch, train loss: 0.5383214354515076:  37%|████▊        | 56/151 [00:09<00:17,  5.35it/s]Epoch: 1, train for the 298-th batch, train loss: 0.4727858901023865:  78%|████████▌  | 297/383 [01:26<00:27,  3.09it/s]evaluate for the 15-th batch, evaluate loss: 0.48605743050575256:  37%|██████▎          | 14/38 [00:01<00:02, 11.10it/s]Epoch: 3, train for the 57-th batch, train loss: 0.5383214354515076:  38%|████▉        | 57/151 [00:09<00:17,  5.50it/s]Epoch: 2, train for the 13-th batch, train loss: 0.6477219462394714:   5%|▋            | 12/237 [00:02<00:40,  5.50it/s]Epoch: 2, train for the 13-th batch, train loss: 0.6477219462394714:   5%|▋            | 13/237 [00:02<00:37,  6.00it/s]Epoch: 1, train for the 298-th batch, train loss: 0.4727858901023865:  78%|████████▌  | 298/383 [01:26<00:26,  3.15it/s]evaluate for the 16-th batch, evaluate loss: 0.5574216246604919:  37%|██████▋           | 14/38 [00:01<00:02, 11.10it/s]evaluate for the 16-th batch, evaluate loss: 0.5574216246604919:  42%|███████▌          | 16/38 [00:01<00:01, 11.61it/s]Epoch: 4, train for the 52-th batch, train loss: 0.4464532434940338:  43%|█████▌       | 51/119 [00:08<00:12,  5.53it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5229310393333435:  54%|█████▉     | 130/241 [00:21<00:20,  5.40it/s]Epoch: 4, train for the 52-th batch, train loss: 0.4464532434940338:  44%|█████▋       | 52/119 [00:08<00:12,  5.52it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5229310393333435:  54%|█████▉     | 131/241 [00:21<00:19,  5.73it/s]evaluate for the 17-th batch, evaluate loss: 0.5103058815002441:  42%|███████▌          | 16/38 [00:01<00:01, 11.61it/s]Epoch: 2, train for the 14-th batch, train loss: 0.7057974338531494:   5%|▋            | 13/237 [00:02<00:37,  6.00it/s]Epoch: 2, train for the 14-th batch, train loss: 0.7057974338531494:   6%|▊            | 14/237 [00:02<00:37,  6.01it/s]evaluate for the 18-th batch, evaluate loss: 0.5640996098518372:  42%|███████▌          | 16/38 [00:01<00:01, 11.61it/s]evaluate for the 18-th batch, evaluate loss: 0.5640996098518372:  47%|████████▌         | 18/38 [00:01<00:01, 12.63it/s]Epoch: 1, train for the 299-th batch, train loss: 0.5090634822845459:  78%|████████▌  | 298/383 [01:26<00:26,  3.15it/s]Epoch: 2, train for the 132-th batch, train loss: 0.4874056875705719:  54%|█████▉     | 131/241 [00:21<00:19,  5.73it/s]evaluate for the 19-th batch, evaluate loss: 0.5449990630149841:  47%|████████▌         | 18/38 [00:01<00:01, 12.63it/s]Epoch: 2, train for the 132-th batch, train loss: 0.4874056875705719:  55%|██████     | 132/241 [00:21<00:19,  5.72it/s]Epoch: 1, train for the 299-th batch, train loss: 0.5090634822845459:  78%|████████▌  | 299/383 [01:26<00:25,  3.31it/s]Epoch: 3, train for the 58-th batch, train loss: 0.5115601420402527:  38%|████▉        | 57/151 [00:09<00:17,  5.50it/s]Epoch: 4, train for the 53-th batch, train loss: 0.4045746624469757:  44%|█████▋       | 52/119 [00:08<00:12,  5.52it/s]Epoch: 3, train for the 58-th batch, train loss: 0.5115601420402527:  38%|████▉        | 58/151 [00:09<00:20,  4.46it/s]Epoch: 4, train for the 53-th batch, train loss: 0.4045746624469757:  45%|█████▊       | 53/119 [00:08<00:12,  5.24it/s]Epoch: 2, train for the 15-th batch, train loss: 0.7439913749694824:   6%|▊            | 14/237 [00:02<00:37,  6.01it/s]evaluate for the 20-th batch, evaluate loss: 0.47820326685905457:  47%|████████         | 18/38 [00:01<00:01, 12.63it/s]evaluate for the 20-th batch, evaluate loss: 0.47820326685905457:  53%|████████▉        | 20/38 [00:01<00:01, 12.43it/s]Epoch: 2, train for the 15-th batch, train loss: 0.7439913749694824:   6%|▊            | 15/237 [00:02<00:37,  5.90it/s]evaluate for the 21-th batch, evaluate loss: 0.49630865454673767:  53%|████████▉        | 20/38 [00:01<00:01, 12.43it/s]Epoch: 4, train for the 54-th batch, train loss: 0.37517309188842773:  45%|█████▎      | 53/119 [00:08<00:12,  5.24it/s]Epoch: 4, train for the 54-th batch, train loss: 0.37517309188842773:  45%|█████▍      | 54/119 [00:08<00:12,  5.30it/s]Epoch: 2, train for the 133-th batch, train loss: 0.3868267834186554:  55%|██████     | 132/241 [00:21<00:19,  5.72it/s]Epoch: 2, train for the 133-th batch, train loss: 0.3868267834186554:  55%|██████     | 133/241 [00:21<00:20,  5.16it/s]Epoch: 3, train for the 59-th batch, train loss: 0.49668726325035095:  38%|████▌       | 58/151 [00:10<00:20,  4.46it/s]evaluate for the 22-th batch, evaluate loss: 0.512930154800415:  53%|██████████         | 20/38 [00:01<00:01, 12.43it/s]evaluate for the 22-th batch, evaluate loss: 0.512930154800415:  58%|███████████        | 22/38 [00:01<00:01, 12.17it/s]Epoch: 3, train for the 59-th batch, train loss: 0.49668726325035095:  39%|████▋       | 59/151 [00:10<00:20,  4.39it/s]Epoch: 2, train for the 16-th batch, train loss: 0.6653355360031128:   6%|▊            | 15/237 [00:02<00:37,  5.90it/s]Epoch: 2, train for the 16-th batch, train loss: 0.6653355360031128:   7%|▉            | 16/237 [00:02<00:40,  5.50it/s]evaluate for the 23-th batch, evaluate loss: 0.5067391991615295:  58%|██████████▍       | 22/38 [00:01<00:01, 12.17it/s]Epoch: 4, train for the 55-th batch, train loss: 0.41653746366500854:  45%|█████▍      | 54/119 [00:09<00:12,  5.30it/s]Epoch: 4, train for the 55-th batch, train loss: 0.41653746366500854:  46%|█████▌      | 55/119 [00:09<00:11,  5.62it/s]evaluate for the 24-th batch, evaluate loss: 0.5106115341186523:  58%|██████████▍       | 22/38 [00:01<00:01, 12.17it/s]evaluate for the 24-th batch, evaluate loss: 0.5106115341186523:  63%|███████████▎      | 24/38 [00:01<00:01, 13.14it/s]Epoch: 2, train for the 134-th batch, train loss: 0.49980056285858154:  55%|█████▌    | 133/241 [00:21<00:20,  5.16it/s]Epoch: 2, train for the 17-th batch, train loss: 0.6939229965209961:   7%|▉            | 16/237 [00:02<00:40,  5.50it/s]Epoch: 2, train for the 134-th batch, train loss: 0.49980056285858154:  56%|█████▌    | 134/241 [00:21<00:20,  5.30it/s]Epoch: 2, train for the 17-th batch, train loss: 0.6939229965209961:   7%|▉            | 17/237 [00:02<00:36,  5.96it/s]evaluate for the 25-th batch, evaluate loss: 0.525871217250824:  63%|████████████       | 24/38 [00:02<00:01, 13.14it/s]Epoch: 3, train for the 60-th batch, train loss: 0.5027124881744385:  39%|█████        | 59/151 [00:10<00:20,  4.39it/s]Epoch: 3, train for the 60-th batch, train loss: 0.5027124881744385:  40%|█████▏       | 60/151 [00:10<00:19,  4.62it/s]Epoch: 4, train for the 56-th batch, train loss: 0.4020196795463562:  46%|██████       | 55/119 [00:09<00:11,  5.62it/s]evaluate for the 26-th batch, evaluate loss: 0.5182533264160156:  63%|███████████▎      | 24/38 [00:02<00:01, 13.14it/s]evaluate for the 26-th batch, evaluate loss: 0.5182533264160156:  68%|████████████▎     | 26/38 [00:02<00:00, 13.67it/s]Epoch: 1, train for the 300-th batch, train loss: 0.510910153388977:  78%|█████████▎  | 299/383 [01:27<00:25,  3.31it/s]Epoch: 4, train for the 56-th batch, train loss: 0.4020196795463562:  47%|██████       | 56/119 [00:09<00:10,  5.85it/s]Epoch: 2, train for the 18-th batch, train loss: 0.6259046792984009:   7%|▉            | 17/237 [00:02<00:36,  5.96it/s]evaluate for the 27-th batch, evaluate loss: 0.5270715355873108:  68%|████████████▎     | 26/38 [00:02<00:00, 13.67it/s]Epoch: 2, train for the 18-th batch, train loss: 0.6259046792984009:   8%|▉            | 18/237 [00:02<00:35,  6.21it/s]Epoch: 1, train for the 300-th batch, train loss: 0.510910153388977:  78%|█████████▍  | 300/383 [01:27<00:32,  2.58it/s]Epoch: 2, train for the 135-th batch, train loss: 0.35593488812446594:  56%|█████▌    | 134/241 [00:22<00:20,  5.30it/s]Epoch: 3, train for the 61-th batch, train loss: 0.5336647033691406:  40%|█████▏       | 60/151 [00:10<00:19,  4.62it/s]Epoch: 2, train for the 135-th batch, train loss: 0.35593488812446594:  56%|█████▌    | 135/241 [00:22<00:20,  5.22it/s]Epoch: 3, train for the 61-th batch, train loss: 0.5336647033691406:  40%|█████▎       | 61/151 [00:10<00:18,  4.95it/s]evaluate for the 28-th batch, evaluate loss: 0.5278249979019165:  68%|████████████▎     | 26/38 [00:02<00:00, 13.67it/s]evaluate for the 28-th batch, evaluate loss: 0.5278249979019165:  74%|█████████████▎    | 28/38 [00:02<00:00, 13.61it/s]Epoch: 4, train for the 57-th batch, train loss: 0.4097486734390259:  47%|██████       | 56/119 [00:09<00:10,  5.85it/s]Epoch: 4, train for the 57-th batch, train loss: 0.4097486734390259:  48%|██████▏      | 57/119 [00:09<00:10,  5.91it/s]evaluate for the 29-th batch, evaluate loss: 0.5555369257926941:  74%|█████████████▎    | 28/38 [00:02<00:00, 13.61it/s]Epoch: 2, train for the 19-th batch, train loss: 0.5929161310195923:   8%|▉            | 18/237 [00:03<00:35,  6.21it/s]Epoch: 2, train for the 19-th batch, train loss: 0.5929161310195923:   8%|█            | 19/237 [00:03<00:36,  6.02it/s]Epoch: 3, train for the 62-th batch, train loss: 0.5624696016311646:  40%|█████▎       | 61/151 [00:10<00:18,  4.95it/s]Epoch: 2, train for the 136-th batch, train loss: 0.43545272946357727:  56%|█████▌    | 135/241 [00:22<00:20,  5.22it/s]Epoch: 3, train for the 62-th batch, train loss: 0.5624696016311646:  41%|█████▎       | 62/151 [00:10<00:17,  4.95it/s]Epoch: 2, train for the 136-th batch, train loss: 0.43545272946357727:  56%|█████▋    | 136/241 [00:22<00:20,  5.04it/s]Epoch: 4, train for the 58-th batch, train loss: 0.3873138427734375:  48%|██████▏      | 57/119 [00:09<00:10,  5.91it/s]evaluate for the 30-th batch, evaluate loss: 0.5427290797233582:  74%|█████████████▎    | 28/38 [00:02<00:00, 13.61it/s]evaluate for the 30-th batch, evaluate loss: 0.5427290797233582:  79%|██████████████▏   | 30/38 [00:02<00:00, 12.43it/s]Epoch: 4, train for the 58-th batch, train loss: 0.3873138427734375:  49%|██████▎      | 58/119 [00:09<00:10,  5.86it/s]Epoch: 2, train for the 20-th batch, train loss: 0.6123489141464233:   8%|█            | 19/237 [00:03<00:36,  6.02it/s]Epoch: 2, train for the 20-th batch, train loss: 0.6123489141464233:   8%|█            | 20/237 [00:03<00:34,  6.36it/s]evaluate for the 31-th batch, evaluate loss: 0.513761579990387:  79%|███████████████    | 30/38 [00:02<00:00, 12.43it/s]Epoch: 3, train for the 63-th batch, train loss: 0.5426759123802185:  41%|█████▎       | 62/151 [00:10<00:17,  4.95it/s]Epoch: 3, train for the 63-th batch, train loss: 0.5426759123802185:  42%|█████▍       | 63/151 [00:10<00:16,  5.38it/s]Epoch: 4, train for the 59-th batch, train loss: 0.4302273094654083:  49%|██████▎      | 58/119 [00:09<00:10,  5.86it/s]Epoch: 2, train for the 21-th batch, train loss: 0.61874920129776:   8%|█▎             | 20/237 [00:03<00:34,  6.36it/s]Epoch: 4, train for the 59-th batch, train loss: 0.4302273094654083:  50%|██████▍      | 59/119 [00:09<00:09,  6.13it/s]Epoch: 2, train for the 137-th batch, train loss: 0.4219772219657898:  56%|██████▏    | 136/241 [00:22<00:20,  5.04it/s]Epoch: 2, train for the 21-th batch, train loss: 0.61874920129776:   9%|█▎             | 21/237 [00:03<00:31,  6.81it/s]Epoch: 2, train for the 137-th batch, train loss: 0.4219772219657898:  57%|██████▎    | 137/241 [00:22<00:20,  5.02it/s]Epoch: 1, train for the 301-th batch, train loss: 0.2965542674064636:  78%|████████▌  | 300/383 [01:27<00:32,  2.58it/s]evaluate for the 32-th batch, evaluate loss: 0.47452113032341003:  79%|█████████████▍   | 30/38 [00:02<00:00, 12.43it/s]evaluate for the 32-th batch, evaluate loss: 0.47452113032341003:  84%|██████████████▎  | 32/38 [00:02<00:00, 11.28it/s]Epoch: 1, train for the 301-th batch, train loss: 0.2965542674064636:  79%|████████▋  | 301/383 [01:27<00:34,  2.36it/s]Epoch: 3, train for the 64-th batch, train loss: 0.5180666446685791:  42%|█████▍       | 63/151 [00:11<00:16,  5.38it/s]Epoch: 2, train for the 22-th batch, train loss: 0.5867320895195007:   9%|█▏           | 21/237 [00:03<00:31,  6.81it/s]Epoch: 3, train for the 64-th batch, train loss: 0.5180666446685791:  42%|█████▌       | 64/151 [00:11<00:15,  5.62it/s]Epoch: 2, train for the 22-th batch, train loss: 0.5867320895195007:   9%|█▏           | 22/237 [00:03<00:30,  7.14it/s]Epoch: 4, train for the 60-th batch, train loss: 0.4087507426738739:  50%|██████▍      | 59/119 [00:09<00:09,  6.13it/s]evaluate for the 33-th batch, evaluate loss: 0.4944182336330414:  84%|███████████████▏  | 32/38 [00:02<00:00, 11.28it/s]Epoch: 2, train for the 138-th batch, train loss: 0.42062532901763916:  57%|█████▋    | 137/241 [00:22<00:20,  5.02it/s]Epoch: 4, train for the 60-th batch, train loss: 0.4087507426738739:  50%|██████▌      | 60/119 [00:09<00:09,  5.93it/s]Epoch: 2, train for the 138-th batch, train loss: 0.42062532901763916:  57%|█████▋    | 138/241 [00:22<00:19,  5.31it/s]evaluate for the 34-th batch, evaluate loss: 0.5385987162590027:  84%|███████████████▏  | 32/38 [00:02<00:00, 11.28it/s]evaluate for the 34-th batch, evaluate loss: 0.5385987162590027:  89%|████████████████  | 34/38 [00:02<00:00, 10.93it/s]Epoch: 2, train for the 23-th batch, train loss: 0.5735496282577515:   9%|█▏           | 22/237 [00:03<00:30,  7.14it/s]Epoch: 3, train for the 65-th batch, train loss: 0.46758127212524414:  42%|█████       | 64/151 [00:11<00:15,  5.62it/s]Epoch: 2, train for the 23-th batch, train loss: 0.5735496282577515:  10%|█▎           | 23/237 [00:03<00:31,  6.83it/s]Epoch: 3, train for the 65-th batch, train loss: 0.46758127212524414:  43%|█████▏      | 65/151 [00:11<00:15,  5.66it/s]Epoch: 2, train for the 139-th batch, train loss: 0.32823044061660767:  57%|█████▋    | 138/241 [00:22<00:19,  5.31it/s]Epoch: 4, train for the 61-th batch, train loss: 0.43215927481651306:  50%|██████      | 60/119 [00:10<00:09,  5.93it/s]Epoch: 1, train for the 302-th batch, train loss: 0.3127809762954712:  79%|████████▋  | 301/383 [01:27<00:34,  2.36it/s]evaluate for the 35-th batch, evaluate loss: 0.5228990912437439:  89%|████████████████  | 34/38 [00:02<00:00, 10.93it/s]Epoch: 2, train for the 139-th batch, train loss: 0.32823044061660767:  58%|█████▊    | 139/241 [00:22<00:19,  5.35it/s]Epoch: 4, train for the 61-th batch, train loss: 0.43215927481651306:  51%|██████▏     | 61/119 [00:10<00:10,  5.62it/s]Epoch: 2, train for the 24-th batch, train loss: 0.5597895979881287:  10%|█▎           | 23/237 [00:03<00:31,  6.83it/s]Epoch: 2, train for the 24-th batch, train loss: 0.5597895979881287:  10%|█▎           | 24/237 [00:03<00:30,  6.91it/s]Epoch: 1, train for the 302-th batch, train loss: 0.3127809762954712:  79%|████████▋  | 302/383 [01:28<00:32,  2.50it/s]Epoch: 3, train for the 66-th batch, train loss: 0.5210160613059998:  43%|█████▌       | 65/151 [00:11<00:15,  5.66it/s]Epoch: 3, train for the 66-th batch, train loss: 0.5210160613059998:  44%|█████▋       | 66/151 [00:11<00:14,  5.72it/s]Epoch: 4, train for the 62-th batch, train loss: 0.422860711812973:  51%|███████▏      | 61/119 [00:10<00:10,  5.62it/s]Epoch: 2, train for the 140-th batch, train loss: 0.36649027466773987:  58%|█████▊    | 139/241 [00:22<00:19,  5.35it/s]Epoch: 4, train for the 62-th batch, train loss: 0.422860711812973:  52%|███████▎      | 62/119 [00:10<00:09,  6.14it/s]Epoch: 2, train for the 140-th batch, train loss: 0.36649027466773987:  58%|█████▊    | 140/241 [00:22<00:17,  5.68it/s]Epoch: 2, train for the 25-th batch, train loss: 0.5846230387687683:  10%|█▎           | 24/237 [00:03<00:30,  6.91it/s]Epoch: 2, train for the 25-th batch, train loss: 0.5846230387687683:  11%|█▎           | 25/237 [00:03<00:31,  6.63it/s]Epoch: 3, train for the 67-th batch, train loss: 0.5529415011405945:  44%|█████▋       | 66/151 [00:11<00:14,  5.72it/s]Epoch: 3, train for the 67-th batch, train loss: 0.5529415011405945:  44%|█████▊       | 67/151 [00:11<00:14,  5.70it/s]Epoch: 4, train for the 63-th batch, train loss: 0.4252149164676666:  52%|██████▊      | 62/119 [00:10<00:09,  6.14it/s]evaluate for the 36-th batch, evaluate loss: 0.5455202460289001:  89%|████████████████  | 34/38 [00:03<00:00, 10.93it/s]evaluate for the 36-th batch, evaluate loss: 0.5455202460289001:  95%|█████████████████ | 36/38 [00:03<00:00,  8.07it/s]Epoch: 4, train for the 63-th batch, train loss: 0.4252149164676666:  53%|██████▉      | 63/119 [00:10<00:08,  6.28it/s]Epoch: 2, train for the 141-th batch, train loss: 0.4610957205295563:  58%|██████▍    | 140/241 [00:23<00:17,  5.68it/s]Epoch: 2, train for the 141-th batch, train loss: 0.4610957205295563:  59%|██████▍    | 141/241 [00:23<00:17,  5.68it/s]Epoch: 2, train for the 26-th batch, train loss: 0.5330809950828552:  11%|█▎           | 25/237 [00:04<00:31,  6.63it/s]evaluate for the 37-th batch, evaluate loss: 0.5165081024169922:  95%|█████████████████ | 36/38 [00:03<00:00,  8.07it/s]Epoch: 2, train for the 26-th batch, train loss: 0.5330809950828552:  11%|█▍           | 26/237 [00:04<00:30,  6.90it/s]Epoch: 3, train for the 68-th batch, train loss: 0.5293453335762024:  44%|█████▊       | 67/151 [00:11<00:14,  5.70it/s]Epoch: 3, train for the 68-th batch, train loss: 0.5293453335762024:  45%|█████▊       | 68/151 [00:11<00:13,  5.94it/s]evaluate for the 38-th batch, evaluate loss: 0.5449358224868774:  95%|█████████████████ | 36/38 [00:03<00:00,  8.07it/s]evaluate for the 38-th batch, evaluate loss: 0.5449358224868774: 100%|██████████████████| 38/38 [00:03<00:00,  8.99it/s]evaluate for the 38-th batch, evaluate loss: 0.5449358224868774: 100%|██████████████████| 38/38 [00:03<00:00, 11.05it/s]
Epoch: 4, train for the 64-th batch, train loss: 0.41467002034187317:  53%|██████▎     | 63/119 [00:10<00:08,  6.28it/s]Epoch: 2, train for the 142-th batch, train loss: 0.44897031784057617:  59%|█████▊    | 141/241 [00:23<00:17,  5.68it/s]Epoch: 4, train for the 64-th batch, train loss: 0.41467002034187317:  54%|██████▍     | 64/119 [00:10<00:09,  5.73it/s]Epoch: 2, train for the 142-th batch, train loss: 0.44897031784057617:  59%|█████▉    | 142/241 [00:23<00:17,  5.63it/s]Epoch: 1, train for the 303-th batch, train loss: 0.3519434630870819:  79%|████████▋  | 302/383 [01:28<00:32,  2.50it/s]Epoch: 2, train for the 27-th batch, train loss: 0.6042171120643616:  11%|█▍           | 26/237 [00:04<00:30,  6.90it/s]Epoch: 2, train for the 27-th batch, train loss: 0.6042171120643616:  11%|█▍           | 27/237 [00:04<00:34,  6.15it/s]Epoch: 3, train for the 69-th batch, train loss: 0.5388416051864624:  45%|█████▊       | 68/151 [00:11<00:13,  5.94it/s]Epoch: 1, train for the 303-th batch, train loss: 0.3519434630870819:  79%|████████▋  | 303/383 [01:28<00:34,  2.32it/s]Epoch: 3, train for the 69-th batch, train loss: 0.5388416051864624:  46%|█████▉       | 69/151 [00:11<00:13,  6.10it/s]Epoch: 4, train for the 65-th batch, train loss: 0.43612003326416016:  54%|██████▍     | 64/119 [00:10<00:09,  5.73it/s]Epoch: 4, train for the 65-th batch, train loss: 0.43612003326416016:  55%|██████▌     | 65/119 [00:10<00:08,  6.22it/s]Epoch: 3, train for the 70-th batch, train loss: 0.5283001661300659:  46%|█████▉       | 69/151 [00:12<00:13,  6.10it/s]  0%|                                                                                            | 0/20 [00:00<?, ?it/s]Epoch: 3, train for the 70-th batch, train loss: 0.5283001661300659:  46%|██████       | 70/151 [00:12<00:12,  6.35it/s]Epoch: 2, train for the 28-th batch, train loss: 0.6187692284584045:  11%|█▍           | 27/237 [00:04<00:34,  6.15it/s]Epoch: 2, train for the 28-th batch, train loss: 0.6187692284584045:  12%|█▌           | 28/237 [00:04<00:35,  5.86it/s]Epoch: 4, train for the 66-th batch, train loss: 0.3960703909397125:  55%|███████      | 65/119 [00:10<00:08,  6.22it/s]Epoch: 4, train for the 66-th batch, train loss: 0.3960703909397125:  55%|███████▏     | 66/119 [00:10<00:08,  6.26it/s]evaluate for the 1-th batch, evaluate loss: 0.9872432351112366:   0%|                            | 0/20 [00:00<?, ?it/s]Epoch: 2, train for the 143-th batch, train loss: 0.4673573970794678:  59%|██████▍    | 142/241 [00:23<00:17,  5.63it/s]Epoch: 2, train for the 143-th batch, train loss: 0.4673573970794678:  59%|██████▌    | 143/241 [00:23<00:21,  4.62it/s]Epoch: 1, train for the 304-th batch, train loss: 0.3968474864959717:  79%|████████▋  | 303/383 [01:28<00:34,  2.32it/s]evaluate for the 2-th batch, evaluate loss: 0.8779361248016357:   0%|                            | 0/20 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.8779361248016357:  10%|██                  | 2/20 [00:00<00:01, 13.61it/s]Epoch: 2, train for the 29-th batch, train loss: 0.5539199709892273:  12%|█▌           | 28/237 [00:04<00:35,  5.86it/s]Epoch: 3, train for the 71-th batch, train loss: 0.5632548332214355:  46%|██████       | 70/151 [00:12<00:12,  6.35it/s]Epoch: 1, train for the 304-th batch, train loss: 0.3968474864959717:  79%|████████▋  | 304/383 [01:28<00:31,  2.51it/s]Epoch: 2, train for the 29-th batch, train loss: 0.5539199709892273:  12%|█▌           | 29/237 [00:04<00:34,  6.07it/s]Epoch: 3, train for the 71-th batch, train loss: 0.5632548332214355:  47%|██████       | 71/151 [00:12<00:13,  6.12it/s]Epoch: 4, train for the 67-th batch, train loss: 0.5097719430923462:  55%|███████▏     | 66/119 [00:11<00:08,  6.26it/s]evaluate for the 3-th batch, evaluate loss: 0.738591730594635:  10%|██                   | 2/20 [00:00<00:01, 13.61it/s]Epoch: 4, train for the 67-th batch, train loss: 0.5097719430923462:  56%|███████▎     | 67/119 [00:11<00:08,  5.98it/s]Epoch: 2, train for the 144-th batch, train loss: 0.5598362684249878:  59%|██████▌    | 143/241 [00:23<00:21,  4.62it/s]Epoch: 2, train for the 144-th batch, train loss: 0.5598362684249878:  60%|██████▌    | 144/241 [00:23<00:20,  4.80it/s]Epoch: 3, train for the 72-th batch, train loss: 0.5307306051254272:  47%|██████       | 71/151 [00:12<00:13,  6.12it/s]evaluate for the 4-th batch, evaluate loss: 0.72794109582901:  10%|██▏                   | 2/20 [00:00<00:01, 13.61it/s]evaluate for the 4-th batch, evaluate loss: 0.72794109582901:  20%|████▍                 | 4/20 [00:00<00:01, 11.36it/s]Epoch: 2, train for the 30-th batch, train loss: 0.5532158017158508:  12%|█▌           | 29/237 [00:04<00:34,  6.07it/s]Epoch: 3, train for the 72-th batch, train loss: 0.5307306051254272:  48%|██████▏      | 72/151 [00:12<00:13,  6.06it/s]Epoch: 2, train for the 30-th batch, train loss: 0.5532158017158508:  13%|█▋           | 30/237 [00:04<00:35,  5.89it/s]Epoch: 4, train for the 68-th batch, train loss: 0.3932541310787201:  56%|███████▎     | 67/119 [00:11<00:08,  5.98it/s]Epoch: 1, train for the 305-th batch, train loss: 0.5130946636199951:  79%|████████▋  | 304/383 [01:29<00:31,  2.51it/s]Epoch: 4, train for the 68-th batch, train loss: 0.3932541310787201:  57%|███████▍     | 68/119 [00:11<00:08,  5.80it/s]evaluate for the 5-th batch, evaluate loss: 0.762645423412323:  20%|████▏                | 4/20 [00:00<00:01, 11.36it/s]Epoch: 2, train for the 145-th batch, train loss: 0.5752615332603455:  60%|██████▌    | 144/241 [00:23<00:20,  4.80it/s]Epoch: 2, train for the 145-th batch, train loss: 0.5752615332603455:  60%|██████▌    | 145/241 [00:24<00:19,  4.89it/s]Epoch: 2, train for the 31-th batch, train loss: 0.5535454154014587:  13%|█▋           | 30/237 [00:04<00:35,  5.89it/s]Epoch: 2, train for the 31-th batch, train loss: 0.5535454154014587:  13%|█▋           | 31/237 [00:04<00:33,  6.13it/s]evaluate for the 6-th batch, evaluate loss: 0.7828590273857117:  20%|████                | 4/20 [00:00<00:01, 11.36it/s]evaluate for the 6-th batch, evaluate loss: 0.7828590273857117:  30%|██████              | 6/20 [00:00<00:01, 11.71it/s]Epoch: 1, train for the 305-th batch, train loss: 0.5130946636199951:  80%|████████▊  | 305/383 [01:29<00:29,  2.63it/s]Epoch: 3, train for the 73-th batch, train loss: 0.5295706987380981:  48%|██████▏      | 72/151 [00:12<00:13,  6.06it/s]Epoch: 3, train for the 73-th batch, train loss: 0.5295706987380981:  48%|██████▎      | 73/151 [00:12<00:13,  5.75it/s]Epoch: 4, train for the 69-th batch, train loss: 0.42343786358833313:  57%|██████▊     | 68/119 [00:11<00:08,  5.80it/s]Epoch: 4, train for the 69-th batch, train loss: 0.42343786358833313:  58%|██████▉     | 69/119 [00:11<00:08,  5.84it/s]evaluate for the 7-th batch, evaluate loss: 0.7777403593063354:  30%|██████              | 6/20 [00:00<00:01, 11.71it/s]Epoch: 2, train for the 146-th batch, train loss: 0.5646054744720459:  60%|██████▌    | 145/241 [00:24<00:19,  4.89it/s]Epoch: 2, train for the 146-th batch, train loss: 0.5646054744720459:  61%|██████▋    | 146/241 [00:24<00:17,  5.32it/s]Epoch: 2, train for the 32-th batch, train loss: 0.5794671773910522:  13%|█▋           | 31/237 [00:05<00:33,  6.13it/s]Epoch: 2, train for the 32-th batch, train loss: 0.5794671773910522:  14%|█▊           | 32/237 [00:05<00:33,  6.04it/s]evaluate for the 8-th batch, evaluate loss: 0.7316277027130127:  30%|██████              | 6/20 [00:00<00:01, 11.71it/s]evaluate for the 8-th batch, evaluate loss: 0.7316277027130127:  40%|████████            | 8/20 [00:00<00:01, 11.60it/s]Epoch: 3, train for the 74-th batch, train loss: 0.4805237054824829:  48%|██████▎      | 73/151 [00:12<00:13,  5.75it/s]Epoch: 3, train for the 74-th batch, train loss: 0.4805237054824829:  49%|██████▎      | 74/151 [00:12<00:13,  5.65it/s]Epoch: 4, train for the 70-th batch, train loss: 0.3839859664440155:  58%|███████▌     | 69/119 [00:11<00:08,  5.84it/s]evaluate for the 9-th batch, evaluate loss: 0.7062981724739075:  40%|████████            | 8/20 [00:00<00:01, 11.60it/s]Epoch: 4, train for the 70-th batch, train loss: 0.3839859664440155:  59%|███████▋     | 70/119 [00:11<00:08,  6.00it/s]Epoch: 2, train for the 147-th batch, train loss: 0.6135411858558655:  61%|██████▋    | 146/241 [00:24<00:17,  5.32it/s]Epoch: 1, train for the 306-th batch, train loss: 0.34900274872779846:  80%|███████▉  | 305/383 [01:29<00:29,  2.63it/s]Epoch: 2, train for the 147-th batch, train loss: 0.6135411858558655:  61%|██████▋    | 147/241 [00:24<00:17,  5.47it/s]evaluate for the 10-th batch, evaluate loss: 0.6668053865432739:  40%|███████▌           | 8/20 [00:00<00:01, 11.60it/s]evaluate for the 10-th batch, evaluate loss: 0.6668053865432739:  50%|█████████         | 10/20 [00:00<00:00, 12.63it/s]Epoch: 2, train for the 33-th batch, train loss: 0.5219182372093201:  14%|█▊           | 32/237 [00:05<00:33,  6.04it/s]Epoch: 2, train for the 33-th batch, train loss: 0.5219182372093201:  14%|█▊           | 33/237 [00:05<00:33,  6.15it/s]Epoch: 1, train for the 306-th batch, train loss: 0.34900274872779846:  80%|███████▉  | 306/383 [01:29<00:28,  2.74it/s]Epoch: 3, train for the 75-th batch, train loss: 0.4935435652732849:  49%|██████▎      | 74/151 [00:12<00:13,  5.65it/s]Epoch: 3, train for the 75-th batch, train loss: 0.4935435652732849:  50%|██████▍      | 75/151 [00:12<00:13,  5.66it/s]Epoch: 2, train for the 148-th batch, train loss: 0.5094977617263794:  61%|██████▋    | 147/241 [00:24<00:17,  5.47it/s]Epoch: 4, train for the 71-th batch, train loss: 0.4090757668018341:  59%|███████▋     | 70/119 [00:11<00:08,  6.00it/s]Epoch: 2, train for the 148-th batch, train loss: 0.5094977617263794:  61%|██████▊    | 148/241 [00:24<00:15,  5.95it/s]evaluate for the 11-th batch, evaluate loss: 0.6897711753845215:  50%|█████████         | 10/20 [00:00<00:00, 12.63it/s]Epoch: 4, train for the 71-th batch, train loss: 0.4090757668018341:  60%|███████▊     | 71/119 [00:11<00:08,  5.77it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5216757655143738:  14%|█▊           | 33/237 [00:05<00:33,  6.15it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5216757655143738:  14%|█▊           | 34/237 [00:05<00:33,  6.06it/s]evaluate for the 12-th batch, evaluate loss: 0.7367823123931885:  50%|█████████         | 10/20 [00:01<00:00, 12.63it/s]evaluate for the 12-th batch, evaluate loss: 0.7367823123931885:  60%|██████████▊       | 12/20 [00:01<00:00, 11.59it/s]Epoch: 3, train for the 76-th batch, train loss: 0.5471273064613342:  50%|██████▍      | 75/151 [00:13<00:13,  5.66it/s]Epoch: 3, train for the 76-th batch, train loss: 0.5471273064613342:  50%|██████▌      | 76/151 [00:13<00:12,  5.86it/s]Epoch: 2, train for the 149-th batch, train loss: 0.42227408289909363:  61%|██████▏   | 148/241 [00:24<00:15,  5.95it/s]Epoch: 4, train for the 72-th batch, train loss: 0.4106455147266388:  60%|███████▊     | 71/119 [00:11<00:08,  5.77it/s]Epoch: 4, train for the 72-th batch, train loss: 0.4106455147266388:  61%|███████▊     | 72/119 [00:11<00:07,  5.88it/s]Epoch: 2, train for the 149-th batch, train loss: 0.42227408289909363:  62%|██████▏   | 149/241 [00:24<00:15,  5.90it/s]evaluate for the 13-th batch, evaluate loss: 0.7295877933502197:  60%|██████████▊       | 12/20 [00:01<00:00, 11.59it/s]Epoch: 2, train for the 35-th batch, train loss: 0.5240963697433472:  14%|█▊           | 34/237 [00:05<00:33,  6.06it/s]Epoch: 2, train for the 35-th batch, train loss: 0.5240963697433472:  15%|█▉           | 35/237 [00:05<00:31,  6.38it/s]evaluate for the 14-th batch, evaluate loss: 0.7303634285926819:  60%|██████████▊       | 12/20 [00:01<00:00, 11.59it/s]evaluate for the 14-th batch, evaluate loss: 0.7303634285926819:  70%|████████████▌     | 14/20 [00:01<00:00, 11.43it/s]Epoch: 3, train for the 77-th batch, train loss: 0.5148600935935974:  50%|██████▌      | 76/151 [00:13<00:12,  5.86it/s]Epoch: 3, train for the 77-th batch, train loss: 0.5148600935935974:  51%|██████▋      | 77/151 [00:13<00:12,  6.01it/s]Epoch: 4, train for the 73-th batch, train loss: 0.4125623106956482:  61%|███████▊     | 72/119 [00:12<00:07,  5.88it/s]Epoch: 4, train for the 73-th batch, train loss: 0.4125623106956482:  61%|███████▉     | 73/119 [00:12<00:07,  6.03it/s]Epoch: 1, train for the 307-th batch, train loss: 0.31663620471954346:  80%|███████▉  | 306/383 [01:29<00:28,  2.74it/s]Epoch: 2, train for the 150-th batch, train loss: 0.45088034868240356:  62%|██████▏   | 149/241 [00:24<00:15,  5.90it/s]evaluate for the 15-th batch, evaluate loss: 0.733718991279602:  70%|█████████████▎     | 14/20 [00:01<00:00, 11.43it/s]Epoch: 2, train for the 150-th batch, train loss: 0.45088034868240356:  62%|██████▏   | 150/241 [00:24<00:15,  5.83it/s]Epoch: 2, train for the 36-th batch, train loss: 0.5663986206054688:  15%|█▉           | 35/237 [00:05<00:31,  6.38it/s]Epoch: 2, train for the 36-th batch, train loss: 0.5663986206054688:  15%|█▉           | 36/237 [00:05<00:33,  6.05it/s]Epoch: 1, train for the 307-th batch, train loss: 0.31663620471954346:  80%|████████  | 307/383 [01:30<00:30,  2.45it/s]evaluate for the 16-th batch, evaluate loss: 0.6808708310127258:  70%|████████████▌     | 14/20 [00:01<00:00, 11.43it/s]evaluate for the 16-th batch, evaluate loss: 0.6808708310127258:  80%|██████████████▍   | 16/20 [00:01<00:00, 11.93it/s]Epoch: 3, train for the 78-th batch, train loss: 0.4962402880191803:  51%|██████▋      | 77/151 [00:13<00:12,  6.01it/s]Epoch: 3, train for the 78-th batch, train loss: 0.4962402880191803:  52%|██████▋      | 78/151 [00:13<00:12,  6.00it/s]Epoch: 4, train for the 74-th batch, train loss: 0.41444921493530273:  61%|███████▎    | 73/119 [00:12<00:07,  6.03it/s]Epoch: 4, train for the 74-th batch, train loss: 0.41444921493530273:  62%|███████▍    | 74/119 [00:12<00:07,  6.31it/s]evaluate for the 17-th batch, evaluate loss: 0.7310947775840759:  80%|██████████████▍   | 16/20 [00:01<00:00, 11.93it/s]Epoch: 2, train for the 151-th batch, train loss: 0.3737335801124573:  62%|██████▊    | 150/241 [00:24<00:15,  5.83it/s]Epoch: 2, train for the 151-th batch, train loss: 0.3737335801124573:  63%|██████▉    | 151/241 [00:24<00:15,  5.99it/s]Epoch: 2, train for the 37-th batch, train loss: 0.5447254180908203:  15%|█▉           | 36/237 [00:05<00:33,  6.05it/s]Epoch: 2, train for the 37-th batch, train loss: 0.5447254180908203:  16%|██           | 37/237 [00:05<00:32,  6.14it/s]evaluate for the 18-th batch, evaluate loss: 0.7207072973251343:  80%|██████████████▍   | 16/20 [00:01<00:00, 11.93it/s]evaluate for the 18-th batch, evaluate loss: 0.7207072973251343:  90%|████████████████▏ | 18/20 [00:01<00:00, 12.49it/s]Epoch: 3, train for the 79-th batch, train loss: 0.5685084462165833:  52%|██████▋      | 78/151 [00:13<00:12,  6.00it/s]Epoch: 4, train for the 75-th batch, train loss: 0.40508222579956055:  62%|███████▍    | 74/119 [00:12<00:07,  6.31it/s]Epoch: 3, train for the 79-th batch, train loss: 0.5685084462165833:  52%|██████▊      | 79/151 [00:13<00:12,  5.82it/s]Epoch: 4, train for the 75-th batch, train loss: 0.40508222579956055:  63%|███████▌    | 75/119 [00:12<00:07,  6.24it/s]evaluate for the 19-th batch, evaluate loss: 0.7690874934196472:  90%|████████████████▏ | 18/20 [00:01<00:00, 12.49it/s]Epoch: 2, train for the 152-th batch, train loss: 0.2880517244338989:  63%|██████▉    | 151/241 [00:25<00:15,  5.99it/s]Epoch: 2, train for the 38-th batch, train loss: 0.5194847583770752:  16%|██           | 37/237 [00:05<00:32,  6.14it/s]Epoch: 2, train for the 152-th batch, train loss: 0.2880517244338989:  63%|██████▉    | 152/241 [00:25<00:14,  6.09it/s]Epoch: 2, train for the 38-th batch, train loss: 0.5194847583770752:  16%|██           | 38/237 [00:06<00:30,  6.48it/s]evaluate for the 20-th batch, evaluate loss: 0.7141905426979065:  90%|████████████████▏ | 18/20 [00:01<00:00, 12.49it/s]evaluate for the 20-th batch, evaluate loss: 0.7141905426979065: 100%|██████████████████| 20/20 [00:01<00:00, 12.74it/s]evaluate for the 20-th batch, evaluate loss: 0.7141905426979065: 100%|██████████████████| 20/20 [00:01<00:00, 12.18it/s]
Epoch: 4, train for the 76-th batch, train loss: 0.4134371876716614:  63%|████████▏    | 75/119 [00:12<00:07,  6.24it/s]Epoch: 2, train for the 39-th batch, train loss: 0.596365213394165:  16%|██▏           | 38/237 [00:06<00:30,  6.48it/s]Epoch: 3, train for the 80-th batch, train loss: 0.5114089846611023:  52%|██████▊      | 79/151 [00:13<00:12,  5.82it/s]Epoch: 4, train for the 76-th batch, train loss: 0.4134371876716614:  64%|████████▎    | 76/119 [00:12<00:07,  5.77it/s]Epoch: 2, train for the 39-th batch, train loss: 0.596365213394165:  16%|██▎           | 39/237 [00:06<00:30,  6.48it/s]Epoch: 2, train for the 153-th batch, train loss: 0.3935854136943817:  63%|██████▉    | 152/241 [00:25<00:14,  6.09it/s]Epoch: 3, train for the 80-th batch, train loss: 0.5114089846611023:  53%|██████▉      | 80/151 [00:13<00:13,  5.35it/s]Epoch: 2, train for the 153-th batch, train loss: 0.3935854136943817:  63%|██████▉    | 153/241 [00:25<00:15,  5.77it/s]Epoch: 1, train for the 308-th batch, train loss: 0.3817460536956787:  80%|████████▊  | 307/383 [01:30<00:30,  2.45it/s]INFO:root:Epoch: 3, learning rate: 0.0001, train loss: 0.5049
INFO:root:train average_precision, 0.8542
INFO:root:train roc_auc, 0.8484
INFO:root:validate loss: 0.5371
INFO:root:validate average_precision, 0.8238
INFO:root:validate roc_auc, 0.8150
INFO:root:new node validate loss: 0.7498
INFO:root:new node validate first_1_average_precision, 0.5297
INFO:root:new node validate first_1_roc_auc, 0.4431
INFO:root:new node validate first_3_average_precision, 0.5532
INFO:root:new node validate first_3_roc_auc, 0.5095
INFO:root:new node validate first_10_average_precision, 0.5603
INFO:root:new node validate first_10_roc_auc, 0.5497
INFO:root:new node validate average_precision, 0.6200
INFO:root:new node validate roc_auc, 0.6311
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]Epoch: 1, train for the 308-th batch, train loss: 0.3817460536956787:  80%|████████▊  | 308/383 [01:30<00:33,  2.23it/s]Epoch: 2, train for the 40-th batch, train loss: 0.5839795470237732:  16%|██▏          | 39/237 [00:06<00:30,  6.48it/s]Epoch: 2, train for the 40-th batch, train loss: 0.5839795470237732:  17%|██▏          | 40/237 [00:06<00:30,  6.55it/s]Epoch: 4, train for the 77-th batch, train loss: 0.3879489004611969:  64%|████████▎    | 76/119 [00:12<00:07,  5.77it/s]Epoch: 4, train for the 77-th batch, train loss: 0.3879489004611969:  65%|████████▍    | 77/119 [00:12<00:07,  5.87it/s]Epoch: 4, train for the 1-th batch, train loss: 1.0462589263916016:   0%|                       | 0/146 [00:00<?, ?it/s]Epoch: 2, train for the 154-th batch, train loss: 0.31734997034072876:  63%|██████▎   | 153/241 [00:25<00:15,  5.77it/s]Epoch: 3, train for the 81-th batch, train loss: 0.5143274664878845:  53%|██████▉      | 80/151 [00:14<00:13,  5.35it/s]Epoch: 3, train for the 81-th batch, train loss: 0.5143274664878845:  54%|██████▉      | 81/151 [00:14<00:13,  5.18it/s]Epoch: 2, train for the 154-th batch, train loss: 0.31734997034072876:  64%|██████▍   | 154/241 [00:25<00:15,  5.47it/s]Epoch: 2, train for the 41-th batch, train loss: 0.5365645289421082:  17%|██▏          | 40/237 [00:06<00:30,  6.55it/s]Epoch: 2, train for the 41-th batch, train loss: 0.5365645289421082:  17%|██▏          | 41/237 [00:06<00:31,  6.22it/s]Epoch: 4, train for the 78-th batch, train loss: 0.46615996956825256:  65%|███████▊    | 77/119 [00:12<00:07,  5.87it/s]Epoch: 4, train for the 78-th batch, train loss: 0.46615996956825256:  66%|███████▊    | 78/119 [00:12<00:07,  5.79it/s]Epoch: 1, train for the 309-th batch, train loss: 0.3823321461677551:  80%|████████▊  | 308/383 [01:30<00:33,  2.23it/s]Epoch: 4, train for the 2-th batch, train loss: 0.8085675835609436:   0%|                       | 0/146 [00:00<?, ?it/s]Epoch: 4, train for the 2-th batch, train loss: 0.8085675835609436:   1%|▏              | 2/146 [00:00<00:19,  7.42it/s]Epoch: 1, train for the 309-th batch, train loss: 0.3823321461677551:  81%|████████▊  | 309/383 [01:30<00:29,  2.48it/s]Epoch: 3, train for the 82-th batch, train loss: 0.5293695330619812:  54%|██████▉      | 81/151 [00:14<00:13,  5.18it/s]Epoch: 2, train for the 155-th batch, train loss: 0.3718104958534241:  64%|███████    | 154/241 [00:25<00:15,  5.47it/s]Epoch: 3, train for the 82-th batch, train loss: 0.5293695330619812:  54%|███████      | 82/151 [00:14<00:13,  5.05it/s]Epoch: 2, train for the 155-th batch, train loss: 0.3718104958534241:  64%|███████    | 155/241 [00:25<00:16,  5.14it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5650576949119568:  17%|██▏          | 41/237 [00:06<00:31,  6.22it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5650576949119568:  18%|██▎          | 42/237 [00:06<00:30,  6.31it/s]Epoch: 4, train for the 79-th batch, train loss: 0.41417834162712097:  66%|███████▊    | 78/119 [00:13<00:07,  5.79it/s]Epoch: 4, train for the 79-th batch, train loss: 0.41417834162712097:  66%|███████▉    | 79/119 [00:13<00:06,  6.00it/s]Epoch: 4, train for the 3-th batch, train loss: 0.7421208024024963:   1%|▏              | 2/146 [00:00<00:19,  7.42it/s]Epoch: 4, train for the 3-th batch, train loss: 0.7421208024024963:   2%|▎              | 3/146 [00:00<00:20,  6.91it/s]Epoch: 3, train for the 83-th batch, train loss: 0.5718867778778076:  54%|███████      | 82/151 [00:14<00:13,  5.05it/s]Epoch: 3, train for the 83-th batch, train loss: 0.5718867778778076:  55%|███████▏     | 83/151 [00:14<00:12,  5.55it/s]Epoch: 2, train for the 156-th batch, train loss: 0.2630845904350281:  64%|███████    | 155/241 [00:25<00:16,  5.14it/s]Epoch: 2, train for the 156-th batch, train loss: 0.2630845904350281:  65%|███████    | 156/241 [00:25<00:15,  5.34it/s]Epoch: 4, train for the 80-th batch, train loss: 0.45334097743034363:  66%|███████▉    | 79/119 [00:13<00:06,  6.00it/s]Epoch: 4, train for the 80-th batch, train loss: 0.45334097743034363:  67%|████████    | 80/119 [00:13<00:06,  6.05it/s]Epoch: 4, train for the 4-th batch, train loss: 0.6936221122741699:   2%|▎              | 3/146 [00:00<00:20,  6.91it/s]Epoch: 4, train for the 4-th batch, train loss: 0.6936221122741699:   3%|▍              | 4/146 [00:00<00:21,  6.70it/s]Epoch: 3, train for the 84-th batch, train loss: 0.5105852484703064:  55%|███████▏     | 83/151 [00:14<00:12,  5.55it/s]Epoch: 2, train for the 43-th batch, train loss: 0.6128020286560059:  18%|██▎          | 42/237 [00:06<00:30,  6.31it/s]Epoch: 1, train for the 310-th batch, train loss: 0.4173814058303833:  81%|████████▊  | 309/383 [01:31<00:29,  2.48it/s]Epoch: 3, train for the 84-th batch, train loss: 0.5105852484703064:  56%|███████▏     | 84/151 [00:14<00:11,  5.76it/s]Epoch: 2, train for the 43-th batch, train loss: 0.6128020286560059:  18%|██▎          | 43/237 [00:06<00:36,  5.28it/s]Epoch: 2, train for the 157-th batch, train loss: 0.2122602015733719:  65%|███████    | 156/241 [00:26<00:15,  5.34it/s]Epoch: 2, train for the 157-th batch, train loss: 0.2122602015733719:  65%|███████▏   | 157/241 [00:26<00:14,  5.71it/s]Epoch: 1, train for the 310-th batch, train loss: 0.4173814058303833:  81%|████████▉  | 310/383 [01:31<00:28,  2.57it/s]Epoch: 4, train for the 81-th batch, train loss: 0.40216124057769775:  67%|████████    | 80/119 [00:13<00:06,  6.05it/s]Epoch: 4, train for the 81-th batch, train loss: 0.40216124057769775:  68%|████████▏   | 81/119 [00:13<00:06,  6.29it/s]Epoch: 4, train for the 5-th batch, train loss: 0.6838667988777161:   3%|▍              | 4/146 [00:00<00:21,  6.70it/s]Epoch: 4, train for the 5-th batch, train loss: 0.6838667988777161:   3%|▌              | 5/146 [00:00<00:20,  6.77it/s]Epoch: 3, train for the 85-th batch, train loss: 0.4961192309856415:  56%|███████▏     | 84/151 [00:14<00:11,  5.76it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5614259839057922:  18%|██▎          | 43/237 [00:07<00:36,  5.28it/s]Epoch: 3, train for the 85-th batch, train loss: 0.4961192309856415:  56%|███████▎     | 85/151 [00:14<00:10,  6.12it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5614259839057922:  19%|██▍          | 44/237 [00:07<00:33,  5.75it/s]Epoch: 2, train for the 158-th batch, train loss: 0.25809401273727417:  65%|██████▌   | 157/241 [00:26<00:14,  5.71it/s]Epoch: 2, train for the 158-th batch, train loss: 0.25809401273727417:  66%|██████▌   | 158/241 [00:26<00:13,  6.00it/s]Epoch: 4, train for the 82-th batch, train loss: 0.41384923458099365:  68%|████████▏   | 81/119 [00:13<00:06,  6.29it/s]Epoch: 4, train for the 82-th batch, train loss: 0.41384923458099365:  69%|████████▎   | 82/119 [00:13<00:05,  6.39it/s]Epoch: 4, train for the 6-th batch, train loss: 0.6461614966392517:   3%|▌              | 5/146 [00:00<00:20,  6.77it/s]Epoch: 4, train for the 6-th batch, train loss: 0.6461614966392517:   4%|▌              | 6/146 [00:00<00:20,  6.69it/s]Epoch: 3, train for the 86-th batch, train loss: 0.5282418727874756:  56%|███████▎     | 85/151 [00:14<00:10,  6.12it/s]Epoch: 3, train for the 86-th batch, train loss: 0.5282418727874756:  57%|███████▍     | 86/151 [00:14<00:10,  6.17it/s]Epoch: 1, train for the 311-th batch, train loss: 0.4897254705429077:  81%|████████▉  | 310/383 [01:31<00:28,  2.57it/s]Epoch: 2, train for the 45-th batch, train loss: 0.5859937071800232:  19%|██▍          | 44/237 [00:07<00:33,  5.75it/s]Epoch: 2, train for the 45-th batch, train loss: 0.5859937071800232:  19%|██▍          | 45/237 [00:07<00:35,  5.48it/s]Epoch: 1, train for the 311-th batch, train loss: 0.4897254705429077:  81%|████████▉  | 311/383 [01:31<00:26,  2.72it/s]Epoch: 4, train for the 83-th batch, train loss: 0.45734095573425293:  69%|████████▎   | 82/119 [00:13<00:05,  6.39it/s]Epoch: 3, train for the 87-th batch, train loss: 0.5154271721839905:  57%|███████▍     | 86/151 [00:14<00:10,  6.17it/s]Epoch: 4, train for the 7-th batch, train loss: 0.602364718914032:   4%|▋               | 6/146 [00:01<00:20,  6.69it/s]Epoch: 4, train for the 7-th batch, train loss: 0.602364718914032:   5%|▊               | 7/146 [00:01<00:20,  6.64it/s]Epoch: 4, train for the 83-th batch, train loss: 0.45734095573425293:  70%|████████▎   | 83/119 [00:13<00:05,  6.02it/s]Epoch: 3, train for the 87-th batch, train loss: 0.5154271721839905:  58%|███████▍     | 87/151 [00:14<00:09,  6.73it/s]Epoch: 2, train for the 46-th batch, train loss: 0.5260645151138306:  19%|██▍          | 45/237 [00:07<00:35,  5.48it/s]Epoch: 2, train for the 159-th batch, train loss: 0.2140384018421173:  66%|███████▏   | 158/241 [00:26<00:13,  6.00it/s]Epoch: 2, train for the 159-th batch, train loss: 0.2140384018421173:  66%|███████▎   | 159/241 [00:26<00:18,  4.38it/s]Epoch: 2, train for the 46-th batch, train loss: 0.5260645151138306:  19%|██▌          | 46/237 [00:07<00:35,  5.32it/s]Epoch: 4, train for the 8-th batch, train loss: 0.5391323566436768:   5%|▋              | 7/146 [00:01<00:20,  6.64it/s]Epoch: 4, train for the 8-th batch, train loss: 0.5391323566436768:   5%|▊              | 8/146 [00:01<00:21,  6.43it/s]Epoch: 3, train for the 88-th batch, train loss: 0.5321363806724548:  58%|███████▍     | 87/151 [00:15<00:09,  6.73it/s]Epoch: 4, train for the 84-th batch, train loss: 0.40621763467788696:  70%|████████▎   | 83/119 [00:13<00:05,  6.02it/s]Epoch: 3, train for the 88-th batch, train loss: 0.5321363806724548:  58%|███████▌     | 88/151 [00:15<00:09,  6.45it/s]Epoch: 4, train for the 84-th batch, train loss: 0.40621763467788696:  71%|████████▍   | 84/119 [00:13<00:05,  5.88it/s]Epoch: 1, train for the 312-th batch, train loss: 0.2638514041900635:  81%|████████▉  | 311/383 [01:31<00:26,  2.72it/s]Epoch: 1, train for the 312-th batch, train loss: 0.2638514041900635:  81%|████████▉  | 312/383 [01:31<00:24,  2.84it/s]Epoch: 2, train for the 160-th batch, train loss: 0.5797117352485657:  66%|███████▎   | 159/241 [00:26<00:18,  4.38it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5906436443328857:  19%|██▌          | 46/237 [00:07<00:35,  5.32it/s]Epoch: 2, train for the 160-th batch, train loss: 0.5797117352485657:  66%|███████▎   | 160/241 [00:26<00:16,  4.92it/s]Epoch: 4, train for the 9-th batch, train loss: 0.5013300180435181:   5%|▊              | 8/146 [00:01<00:21,  6.43it/s]Epoch: 4, train for the 9-th batch, train loss: 0.5013300180435181:   6%|▉              | 9/146 [00:01<00:20,  6.71it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5906436443328857:  20%|██▌          | 47/237 [00:07<00:33,  5.60it/s]Epoch: 3, train for the 89-th batch, train loss: 0.5254106521606445:  58%|███████▌     | 88/151 [00:15<00:09,  6.45it/s]Epoch: 4, train for the 85-th batch, train loss: 0.38698291778564453:  71%|████████▍   | 84/119 [00:14<00:05,  5.88it/s]Epoch: 3, train for the 89-th batch, train loss: 0.5254106521606445:  59%|███████▋     | 89/151 [00:15<00:10,  6.16it/s]Epoch: 4, train for the 85-th batch, train loss: 0.38698291778564453:  71%|████████▌   | 85/119 [00:14<00:05,  5.78it/s]Epoch: 4, train for the 10-th batch, train loss: 0.5002673864364624:   6%|▊             | 9/146 [00:01<00:20,  6.71it/s]Epoch: 4, train for the 10-th batch, train loss: 0.5002673864364624:   7%|▉            | 10/146 [00:01<00:18,  7.18it/s]Epoch: 2, train for the 48-th batch, train loss: 0.5659934878349304:  20%|██▌          | 47/237 [00:07<00:33,  5.60it/s]Epoch: 3, train for the 90-th batch, train loss: 0.4990091621875763:  59%|███████▋     | 89/151 [00:15<00:10,  6.16it/s]Epoch: 2, train for the 48-th batch, train loss: 0.5659934878349304:  20%|██▋          | 48/237 [00:07<00:34,  5.45it/s]Epoch: 3, train for the 90-th batch, train loss: 0.4990091621875763:  60%|███████▋     | 90/151 [00:15<00:09,  6.40it/s]Epoch: 4, train for the 86-th batch, train loss: 0.43569186329841614:  71%|████████▌   | 85/119 [00:14<00:05,  5.78it/s]Epoch: 4, train for the 86-th batch, train loss: 0.43569186329841614:  72%|████████▋   | 86/119 [00:14<00:05,  5.64it/s]Epoch: 4, train for the 11-th batch, train loss: 0.48779669404029846:   7%|▊           | 10/146 [00:01<00:18,  7.18it/s]Epoch: 4, train for the 11-th batch, train loss: 0.48779669404029846:   8%|▉           | 11/146 [00:01<00:19,  7.03it/s]Epoch: 3, train for the 91-th batch, train loss: 0.478462278842926:  60%|████████▎     | 90/151 [00:15<00:09,  6.40it/s]Epoch: 2, train for the 49-th batch, train loss: 0.540780246257782:  20%|██▊           | 48/237 [00:07<00:34,  5.45it/s]Epoch: 3, train for the 91-th batch, train loss: 0.478462278842926:  60%|████████▍     | 91/151 [00:15<00:08,  6.86it/s]Epoch: 2, train for the 49-th batch, train loss: 0.540780246257782:  21%|██▉           | 49/237 [00:07<00:31,  6.05it/s]Epoch: 2, train for the 161-th batch, train loss: 0.32079002261161804:  66%|██████▋   | 160/241 [00:27<00:16,  4.92it/s]Epoch: 2, train for the 161-th batch, train loss: 0.32079002261161804:  67%|██████▋   | 161/241 [00:27<00:20,  3.89it/s]Epoch: 1, train for the 313-th batch, train loss: 0.5085561871528625:  81%|████████▉  | 312/383 [01:32<00:24,  2.84it/s]Epoch: 4, train for the 87-th batch, train loss: 0.4521369934082031:  72%|█████████▍   | 86/119 [00:14<00:05,  5.64it/s]Epoch: 4, train for the 12-th batch, train loss: 0.4678095281124115:   8%|▉            | 11/146 [00:01<00:19,  7.03it/s]Epoch: 4, train for the 12-th batch, train loss: 0.4678095281124115:   8%|█            | 12/146 [00:01<00:20,  6.68it/s]Epoch: 4, train for the 87-th batch, train loss: 0.4521369934082031:  73%|█████████▌   | 87/119 [00:14<00:05,  5.57it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5954440832138062:  21%|██▋          | 49/237 [00:08<00:31,  6.05it/s]Epoch: 3, train for the 92-th batch, train loss: 0.5703455805778503:  60%|███████▊     | 91/151 [00:15<00:08,  6.86it/s]Epoch: 3, train for the 92-th batch, train loss: 0.5703455805778503:  61%|███████▉     | 92/151 [00:15<00:08,  6.88it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5954440832138062:  21%|██▋          | 50/237 [00:08<00:30,  6.16it/s]Epoch: 1, train for the 313-th batch, train loss: 0.5085561871528625:  82%|████████▉  | 313/383 [01:32<00:28,  2.48it/s]Epoch: 2, train for the 162-th batch, train loss: 0.1781294196844101:  67%|███████▎   | 161/241 [00:27<00:20,  3.89it/s]Epoch: 2, train for the 162-th batch, train loss: 0.1781294196844101:  67%|███████▍   | 162/241 [00:27<00:17,  4.50it/s]Epoch: 4, train for the 13-th batch, train loss: 0.44462263584136963:   8%|▉           | 12/146 [00:01<00:20,  6.68it/s]Epoch: 4, train for the 13-th batch, train loss: 0.44462263584136963:   9%|█           | 13/146 [00:01<00:19,  6.85it/s]Epoch: 4, train for the 88-th batch, train loss: 0.444955974817276:  73%|██████████▏   | 87/119 [00:14<00:05,  5.57it/s]Epoch: 4, train for the 88-th batch, train loss: 0.444955974817276:  74%|██████████▎   | 88/119 [00:14<00:05,  5.56it/s]Epoch: 3, train for the 93-th batch, train loss: 0.5092220902442932:  61%|███████▉     | 92/151 [00:15<00:08,  6.88it/s]Epoch: 3, train for the 93-th batch, train loss: 0.5092220902442932:  62%|████████     | 93/151 [00:15<00:08,  6.63it/s]Epoch: 2, train for the 51-th batch, train loss: 0.5902964472770691:  21%|██▋          | 50/237 [00:08<00:30,  6.16it/s]Epoch: 2, train for the 163-th batch, train loss: 0.13649898767471313:  67%|██████▋   | 162/241 [00:27<00:17,  4.50it/s]Epoch: 2, train for the 51-th batch, train loss: 0.5902964472770691:  22%|██▊          | 51/237 [00:08<00:31,  5.91it/s]Epoch: 2, train for the 163-th batch, train loss: 0.13649898767471313:  68%|██████▊   | 163/241 [00:27<00:15,  4.94it/s]Epoch: 4, train for the 14-th batch, train loss: 0.41500526666641235:   9%|█           | 13/146 [00:02<00:19,  6.85it/s]Epoch: 4, train for the 14-th batch, train loss: 0.41500526666641235:  10%|█▏          | 14/146 [00:02<00:18,  7.17it/s]Epoch: 1, train for the 314-th batch, train loss: 0.4027654826641083:  82%|████████▉  | 313/383 [01:32<00:28,  2.48it/s]Epoch: 4, train for the 89-th batch, train loss: 0.4529479742050171:  74%|█████████▌   | 88/119 [00:14<00:05,  5.56it/s]Epoch: 4, train for the 89-th batch, train loss: 0.4529479742050171:  75%|█████████▋   | 89/119 [00:14<00:05,  5.74it/s]Epoch: 2, train for the 52-th batch, train loss: 0.5658881664276123:  22%|██▊          | 51/237 [00:08<00:31,  5.91it/s]Epoch: 3, train for the 94-th batch, train loss: 0.4943101406097412:  62%|████████     | 93/151 [00:16<00:08,  6.63it/s]Epoch: 1, train for the 314-th batch, train loss: 0.4027654826641083:  82%|█████████  | 314/383 [01:32<00:25,  2.66it/s]Epoch: 2, train for the 52-th batch, train loss: 0.5658881664276123:  22%|██▊          | 52/237 [00:08<00:29,  6.17it/s]Epoch: 3, train for the 94-th batch, train loss: 0.4943101406097412:  62%|████████     | 94/151 [00:16<00:09,  6.27it/s]Epoch: 2, train for the 164-th batch, train loss: 0.1593496948480606:  68%|███████▍   | 163/241 [00:27<00:15,  4.94it/s]Epoch: 4, train for the 15-th batch, train loss: 0.4514705538749695:  10%|█▏           | 14/146 [00:02<00:18,  7.17it/s]Epoch: 4, train for the 15-th batch, train loss: 0.4514705538749695:  10%|█▎           | 15/146 [00:02<00:17,  7.34it/s]Epoch: 2, train for the 164-th batch, train loss: 0.1593496948480606:  68%|███████▍   | 164/241 [00:27<00:14,  5.27it/s]Epoch: 4, train for the 90-th batch, train loss: 0.4484058916568756:  75%|█████████▋   | 89/119 [00:14<00:05,  5.74it/s]Epoch: 4, train for the 90-th batch, train loss: 0.4484058916568756:  76%|█████████▊   | 90/119 [00:14<00:04,  6.09it/s]Epoch: 2, train for the 53-th batch, train loss: 0.5943029522895813:  22%|██▊          | 52/237 [00:08<00:29,  6.17it/s]Epoch: 2, train for the 53-th batch, train loss: 0.5943029522895813:  22%|██▉          | 53/237 [00:08<00:31,  5.81it/s]Epoch: 2, train for the 165-th batch, train loss: 0.23441700637340546:  68%|██████▊   | 164/241 [00:27<00:14,  5.27it/s]Epoch: 3, train for the 95-th batch, train loss: 0.5149816274642944:  62%|████████     | 94/151 [00:16<00:09,  6.27it/s]Epoch: 2, train for the 165-th batch, train loss: 0.23441700637340546:  68%|██████▊   | 165/241 [00:27<00:14,  5.18it/s]Epoch: 3, train for the 95-th batch, train loss: 0.5149816274642944:  63%|████████▏    | 95/151 [00:16<00:10,  5.55it/s]Epoch: 4, train for the 91-th batch, train loss: 0.413094699382782:  76%|██████████▌   | 90/119 [00:15<00:04,  6.09it/s]Epoch: 4, train for the 91-th batch, train loss: 0.413094699382782:  76%|██████████▋   | 91/119 [00:15<00:04,  6.23it/s]Epoch: 4, train for the 16-th batch, train loss: 0.43633201718330383:  10%|█▏          | 15/146 [00:02<00:17,  7.34it/s]Epoch: 1, train for the 315-th batch, train loss: 0.5122590661048889:  82%|█████████  | 314/383 [01:32<00:25,  2.66it/s]Epoch: 4, train for the 16-th batch, train loss: 0.43633201718330383:  11%|█▎          | 16/146 [00:02<00:23,  5.65it/s]Epoch: 2, train for the 54-th batch, train loss: 0.5157305598258972:  22%|██▉          | 53/237 [00:08<00:31,  5.81it/s]Epoch: 1, train for the 315-th batch, train loss: 0.5122590661048889:  82%|█████████  | 315/383 [01:33<00:25,  2.69it/s]Epoch: 2, train for the 166-th batch, train loss: 0.2415873408317566:  68%|███████▌   | 165/241 [00:27<00:14,  5.18it/s]Epoch: 2, train for the 54-th batch, train loss: 0.5157305598258972:  23%|██▉          | 54/237 [00:08<00:31,  5.83it/s]Epoch: 2, train for the 166-th batch, train loss: 0.2415873408317566:  69%|███████▌   | 166/241 [00:27<00:13,  5.52it/s]Epoch: 3, train for the 96-th batch, train loss: 0.5063495635986328:  63%|████████▏    | 95/151 [00:16<00:10,  5.55it/s]Epoch: 3, train for the 96-th batch, train loss: 0.5063495635986328:  64%|████████▎    | 96/151 [00:16<00:10,  5.40it/s]Epoch: 4, train for the 92-th batch, train loss: 0.4020700752735138:  76%|█████████▉   | 91/119 [00:15<00:04,  6.23it/s]Epoch: 4, train for the 17-th batch, train loss: 0.4324914216995239:  11%|█▍           | 16/146 [00:02<00:23,  5.65it/s]Epoch: 4, train for the 92-th batch, train loss: 0.4020700752735138:  77%|██████████   | 92/119 [00:15<00:04,  5.94it/s]Epoch: 4, train for the 17-th batch, train loss: 0.4324914216995239:  12%|█▌           | 17/146 [00:02<00:22,  5.78it/s]Epoch: 2, train for the 167-th batch, train loss: 0.14623036980628967:  69%|██████▉   | 166/241 [00:28<00:13,  5.52it/s]Epoch: 2, train for the 167-th batch, train loss: 0.14623036980628967:  69%|██████▉   | 167/241 [00:28<00:12,  5.83it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5548034310340881:  23%|██▉          | 54/237 [00:08<00:31,  5.83it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5548034310340881:  23%|███          | 55/237 [00:08<00:32,  5.68it/s]Epoch: 3, train for the 97-th batch, train loss: 0.5360232591629028:  64%|████████▎    | 96/151 [00:16<00:10,  5.40it/s]Epoch: 3, train for the 97-th batch, train loss: 0.5360232591629028:  64%|████████▎    | 97/151 [00:16<00:09,  5.59it/s]Epoch: 1, train for the 316-th batch, train loss: 0.45820119976997375:  82%|████████▏ | 315/383 [01:33<00:25,  2.69it/s]Epoch: 4, train for the 93-th batch, train loss: 0.37571170926094055:  77%|█████████▎  | 92/119 [00:15<00:04,  5.94it/s]Epoch: 4, train for the 93-th batch, train loss: 0.37571170926094055:  78%|█████████▍  | 93/119 [00:15<00:04,  5.78it/s]Epoch: 4, train for the 18-th batch, train loss: 0.4183671474456787:  12%|█▌           | 17/146 [00:02<00:22,  5.78it/s]Epoch: 4, train for the 18-th batch, train loss: 0.4183671474456787:  12%|█▌           | 18/146 [00:02<00:22,  5.64it/s]Epoch: 1, train for the 316-th batch, train loss: 0.45820119976997375:  83%|████████▎ | 316/383 [01:33<00:23,  2.80it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5390992760658264:  23%|███          | 55/237 [00:09<00:32,  5.68it/s]Epoch: 2, train for the 168-th batch, train loss: 0.1147795245051384:  69%|███████▌   | 167/241 [00:28<00:12,  5.83it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5390992760658264:  24%|███          | 56/237 [00:09<00:30,  5.86it/s]Epoch: 2, train for the 168-th batch, train loss: 0.1147795245051384:  70%|███████▋   | 168/241 [00:28<00:12,  5.68it/s]Epoch: 3, train for the 98-th batch, train loss: 0.4985242486000061:  64%|████████▎    | 97/151 [00:16<00:09,  5.59it/s]Epoch: 3, train for the 98-th batch, train loss: 0.4985242486000061:  65%|████████▍    | 98/151 [00:16<00:09,  5.58it/s]Epoch: 4, train for the 94-th batch, train loss: 0.3912854790687561:  78%|██████████▏  | 93/119 [00:15<00:04,  5.78it/s]Epoch: 4, train for the 94-th batch, train loss: 0.3912854790687561:  79%|██████████▎  | 94/119 [00:15<00:04,  5.92it/s]Epoch: 4, train for the 19-th batch, train loss: 0.46183112263679504:  12%|█▍          | 18/146 [00:02<00:22,  5.64it/s]Epoch: 4, train for the 19-th batch, train loss: 0.46183112263679504:  13%|█▌          | 19/146 [00:02<00:22,  5.77it/s]Epoch: 2, train for the 57-th batch, train loss: 0.5579479932785034:  24%|███          | 56/237 [00:09<00:30,  5.86it/s]Epoch: 2, train for the 169-th batch, train loss: 0.6074838638305664:  70%|███████▋   | 168/241 [00:28<00:12,  5.68it/s]Epoch: 2, train for the 169-th batch, train loss: 0.6074838638305664:  70%|███████▋   | 169/241 [00:28<00:12,  5.79it/s]Epoch: 2, train for the 57-th batch, train loss: 0.5579479932785034:  24%|███▏         | 57/237 [00:09<00:30,  5.84it/s]Epoch: 3, train for the 99-th batch, train loss: 0.5575262308120728:  65%|████████▍    | 98/151 [00:16<00:09,  5.58it/s]Epoch: 3, train for the 99-th batch, train loss: 0.5575262308120728:  66%|████████▌    | 99/151 [00:17<00:09,  5.52it/s]Epoch: 4, train for the 20-th batch, train loss: 0.39786067605018616:  13%|█▌          | 19/146 [00:03<00:22,  5.77it/s]Epoch: 4, train for the 20-th batch, train loss: 0.39786067605018616:  14%|█▋          | 20/146 [00:03<00:21,  5.79it/s]Epoch: 4, train for the 95-th batch, train loss: 0.3346695601940155:  79%|██████████▎  | 94/119 [00:15<00:04,  5.92it/s]Epoch: 4, train for the 95-th batch, train loss: 0.3346695601940155:  80%|██████████▍  | 95/119 [00:15<00:04,  5.66it/s]Epoch: 2, train for the 58-th batch, train loss: 0.6168614029884338:  24%|███▏         | 57/237 [00:09<00:30,  5.84it/s]Epoch: 2, train for the 170-th batch, train loss: 0.7302460074424744:  70%|███████▋   | 169/241 [00:28<00:12,  5.79it/s]Epoch: 2, train for the 170-th batch, train loss: 0.7302460074424744:  71%|███████▊   | 170/241 [00:28<00:11,  6.18it/s]Epoch: 2, train for the 58-th batch, train loss: 0.6168614029884338:  24%|███▏         | 58/237 [00:09<00:28,  6.25it/s]Epoch: 3, train for the 100-th batch, train loss: 0.5718604922294617:  66%|███████▊    | 99/151 [00:17<00:09,  5.52it/s]Epoch: 4, train for the 96-th batch, train loss: 0.39414653182029724:  80%|█████████▌  | 95/119 [00:15<00:04,  5.66it/s]Epoch: 3, train for the 100-th batch, train loss: 0.5718604922294617:  66%|███████▎   | 100/151 [00:17<00:09,  5.48it/s]Epoch: 4, train for the 96-th batch, train loss: 0.39414653182029724:  81%|█████████▋  | 96/119 [00:15<00:03,  5.81it/s]Epoch: 2, train for the 171-th batch, train loss: 0.671381950378418:  71%|████████▍   | 170/241 [00:28<00:11,  6.18it/s]Epoch: 2, train for the 59-th batch, train loss: 0.6149271726608276:  24%|███▏         | 58/237 [00:09<00:28,  6.25it/s]Epoch: 2, train for the 171-th batch, train loss: 0.671381950378418:  71%|████████▌   | 171/241 [00:28<00:11,  6.23it/s]Epoch: 4, train for the 21-th batch, train loss: 0.4121016561985016:  14%|█▊           | 20/146 [00:03<00:21,  5.79it/s]Epoch: 4, train for the 21-th batch, train loss: 0.4121016561985016:  14%|█▊           | 21/146 [00:03<00:22,  5.45it/s]Epoch: 2, train for the 59-th batch, train loss: 0.6149271726608276:  25%|███▏         | 59/237 [00:09<00:28,  6.15it/s]Epoch: 1, train for the 317-th batch, train loss: 0.298357218503952:  83%|█████████▉  | 316/383 [01:33<00:23,  2.80it/s]Epoch: 3, train for the 101-th batch, train loss: 0.5690863132476807:  66%|███████▎   | 100/151 [00:17<00:09,  5.48it/s]Epoch: 4, train for the 97-th batch, train loss: 0.3899560868740082:  81%|██████████▍  | 96/119 [00:16<00:03,  5.81it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5465102791786194:  25%|███▏         | 59/237 [00:09<00:28,  6.15it/s]Epoch: 4, train for the 97-th batch, train loss: 0.3899560868740082:  82%|██████████▌  | 97/119 [00:16<00:03,  5.72it/s]Epoch: 2, train for the 172-th batch, train loss: 0.5811335444450378:  71%|███████▊   | 171/241 [00:28<00:11,  6.23it/s]Epoch: 3, train for the 101-th batch, train loss: 0.5690863132476807:  67%|███████▎   | 101/151 [00:17<00:09,  5.39it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5465102791786194:  25%|███▎         | 60/237 [00:09<00:28,  6.18it/s]Epoch: 1, train for the 317-th batch, train loss: 0.298357218503952:  83%|█████████▉  | 317/383 [01:34<00:29,  2.22it/s]Epoch: 2, train for the 172-th batch, train loss: 0.5811335444450378:  71%|███████▊   | 172/241 [00:28<00:11,  6.02it/s]Epoch: 4, train for the 22-th batch, train loss: 0.4371070861816406:  14%|█▊           | 21/146 [00:03<00:22,  5.45it/s]Epoch: 4, train for the 22-th batch, train loss: 0.4371070861816406:  15%|█▉           | 22/146 [00:03<00:22,  5.47it/s]Epoch: 3, train for the 102-th batch, train loss: 0.5020902156829834:  67%|███████▎   | 101/151 [00:17<00:09,  5.39it/s]Epoch: 4, train for the 98-th batch, train loss: 0.3901110887527466:  82%|██████████▌  | 97/119 [00:16<00:03,  5.72it/s]Epoch: 4, train for the 98-th batch, train loss: 0.3901110887527466:  82%|██████████▋  | 98/119 [00:16<00:03,  5.86it/s]Epoch: 3, train for the 102-th batch, train loss: 0.5020902156829834:  68%|███████▍   | 102/151 [00:17<00:08,  5.62it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5746798515319824:  25%|███▎         | 60/237 [00:09<00:28,  6.18it/s]Epoch: 4, train for the 23-th batch, train loss: 0.5003513097763062:  15%|█▉           | 22/146 [00:03<00:22,  5.47it/s]Epoch: 2, train for the 173-th batch, train loss: 0.43672648072242737:  71%|███████▏  | 172/241 [00:29<00:11,  6.02it/s]Epoch: 4, train for the 23-th batch, train loss: 0.5003513097763062:  16%|██           | 23/146 [00:03<00:21,  5.63it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5746798515319824:  26%|███▎         | 61/237 [00:09<00:30,  5.86it/s]Epoch: 2, train for the 173-th batch, train loss: 0.43672648072242737:  72%|███████▏  | 173/241 [00:29<00:11,  5.68it/s]Epoch: 4, train for the 99-th batch, train loss: 0.3648415803909302:  82%|██████████▋  | 98/119 [00:16<00:03,  5.86it/s]Epoch: 4, train for the 99-th batch, train loss: 0.3648415803909302:  83%|██████████▊  | 99/119 [00:16<00:03,  6.26it/s]Epoch: 4, train for the 24-th batch, train loss: 0.5076379776000977:  16%|██           | 23/146 [00:03<00:21,  5.63it/s]Epoch: 3, train for the 103-th batch, train loss: 0.5290860533714294:  68%|███████▍   | 102/151 [00:17<00:08,  5.62it/s]Epoch: 4, train for the 24-th batch, train loss: 0.5076379776000977:  16%|██▏          | 24/146 [00:03<00:20,  5.86it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5610185265541077:  26%|███▎         | 61/237 [00:10<00:30,  5.86it/s]Epoch: 2, train for the 174-th batch, train loss: 0.44480636715888977:  72%|███████▏  | 173/241 [00:29<00:11,  5.68it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5610185265541077:  26%|███▍         | 62/237 [00:10<00:29,  5.88it/s]Epoch: 3, train for the 103-th batch, train loss: 0.5290860533714294:  68%|███████▌   | 103/151 [00:17<00:08,  5.34it/s]Epoch: 2, train for the 174-th batch, train loss: 0.44480636715888977:  72%|███████▏  | 174/241 [00:29<00:11,  5.74it/s]Epoch: 4, train for the 100-th batch, train loss: 0.4410088360309601:  83%|█████████▉  | 99/119 [00:16<00:03,  6.26it/s]Epoch: 4, train for the 100-th batch, train loss: 0.4410088360309601:  84%|█████████▏ | 100/119 [00:16<00:02,  6.64it/s]Epoch: 4, train for the 25-th batch, train loss: 0.47709760069847107:  16%|█▉          | 24/146 [00:03<00:20,  5.86it/s]Epoch: 4, train for the 25-th batch, train loss: 0.47709760069847107:  17%|██          | 25/146 [00:03<00:19,  6.34it/s]Epoch: 1, train for the 318-th batch, train loss: 0.3464753031730652:  83%|█████████  | 317/383 [01:34<00:29,  2.22it/s]Epoch: 3, train for the 104-th batch, train loss: 0.5452136397361755:  68%|███████▌   | 103/151 [00:17<00:08,  5.34it/s]Epoch: 2, train for the 175-th batch, train loss: 0.27472496032714844:  72%|███████▏  | 174/241 [00:29<00:11,  5.74it/s]Epoch: 3, train for the 104-th batch, train loss: 0.5452136397361755:  69%|███████▌   | 104/151 [00:17<00:08,  5.28it/s]Epoch: 2, train for the 63-th batch, train loss: 0.5997207760810852:  26%|███▍         | 62/237 [00:10<00:29,  5.88it/s]Epoch: 4, train for the 101-th batch, train loss: 0.4051038920879364:  84%|█████████▏ | 100/119 [00:16<00:02,  6.64it/s]Epoch: 2, train for the 175-th batch, train loss: 0.27472496032714844:  73%|███████▎  | 175/241 [00:29<00:11,  5.55it/s]Epoch: 4, train for the 101-th batch, train loss: 0.4051038920879364:  85%|█████████▎ | 101/119 [00:16<00:02,  6.45it/s]Epoch: 2, train for the 63-th batch, train loss: 0.5997207760810852:  27%|███▍         | 63/237 [00:10<00:31,  5.45it/s]Epoch: 1, train for the 318-th batch, train loss: 0.3464753031730652:  83%|█████████▏ | 318/383 [01:34<00:31,  2.03it/s]Epoch: 4, train for the 26-th batch, train loss: 0.4856172204017639:  17%|██▏          | 25/146 [00:04<00:19,  6.34it/s]Epoch: 4, train for the 26-th batch, train loss: 0.4856172204017639:  18%|██▎          | 26/146 [00:04<00:18,  6.54it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5907024145126343:  27%|███▍         | 63/237 [00:10<00:31,  5.45it/s]Epoch: 4, train for the 102-th batch, train loss: 0.4220920503139496:  85%|█████████▎ | 101/119 [00:16<00:02,  6.45it/s]Epoch: 4, train for the 102-th batch, train loss: 0.4220920503139496:  86%|█████████▍ | 102/119 [00:16<00:02,  6.11it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5907024145126343:  27%|███▌         | 64/237 [00:10<00:31,  5.42it/s]Epoch: 2, train for the 176-th batch, train loss: 0.41968876123428345:  73%|███████▎  | 175/241 [00:29<00:11,  5.55it/s]Epoch: 3, train for the 105-th batch, train loss: 0.4953802525997162:  69%|███████▌   | 104/151 [00:18<00:08,  5.28it/s]Epoch: 4, train for the 27-th batch, train loss: 0.42046743631362915:  18%|██▏         | 26/146 [00:04<00:18,  6.54it/s]Epoch: 4, train for the 27-th batch, train loss: 0.42046743631362915:  18%|██▏         | 27/146 [00:04<00:19,  6.14it/s]Epoch: 2, train for the 176-th batch, train loss: 0.41968876123428345:  73%|███████▎  | 176/241 [00:29<00:12,  5.11it/s]Epoch: 3, train for the 105-th batch, train loss: 0.4953802525997162:  70%|███████▋   | 105/151 [00:18<00:09,  4.82it/s]Epoch: 1, train for the 319-th batch, train loss: 0.5197542905807495:  83%|█████████▏ | 318/383 [01:34<00:31,  2.03it/s]Epoch: 4, train for the 103-th batch, train loss: 0.4128507077693939:  86%|█████████▍ | 102/119 [00:17<00:02,  6.11it/s]Epoch: 1, train for the 319-th batch, train loss: 0.5197542905807495:  83%|█████████▏ | 319/383 [01:34<00:27,  2.29it/s]Epoch: 4, train for the 103-th batch, train loss: 0.4128507077693939:  87%|█████████▌ | 103/119 [00:17<00:02,  6.34it/s]Epoch: 4, train for the 28-th batch, train loss: 0.4469829201698303:  18%|██▍          | 27/146 [00:04<00:19,  6.14it/s]Epoch: 4, train for the 28-th batch, train loss: 0.4469829201698303:  19%|██▍          | 28/146 [00:04<00:19,  6.19it/s]Epoch: 2, train for the 177-th batch, train loss: 0.38772040605545044:  73%|███████▎  | 176/241 [00:29<00:12,  5.11it/s]Epoch: 3, train for the 106-th batch, train loss: 0.5240434408187866:  70%|███████▋   | 105/151 [00:18<00:09,  4.82it/s]Epoch: 3, train for the 106-th batch, train loss: 0.5240434408187866:  70%|███████▋   | 106/151 [00:18<00:09,  4.68it/s]Epoch: 2, train for the 177-th batch, train loss: 0.38772040605545044:  73%|███████▎  | 177/241 [00:29<00:13,  4.81it/s]Epoch: 4, train for the 104-th batch, train loss: 0.3777427673339844:  87%|█████████▌ | 103/119 [00:17<00:02,  6.34it/s]Epoch: 4, train for the 104-th batch, train loss: 0.3777427673339844:  87%|█████████▌ | 104/119 [00:17<00:02,  6.50it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5861174464225769:  27%|███▌         | 64/237 [00:10<00:31,  5.42it/s]Epoch: 4, train for the 29-th batch, train loss: 0.4106438457965851:  19%|██▍          | 28/146 [00:04<00:19,  6.19it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5861174464225769:  27%|███▌         | 65/237 [00:10<00:38,  4.43it/s]Epoch: 4, train for the 29-th batch, train loss: 0.4106438457965851:  20%|██▌          | 29/146 [00:04<00:18,  6.46it/s]Epoch: 1, train for the 320-th batch, train loss: 0.5727564096450806:  83%|█████████▏ | 319/383 [01:35<00:27,  2.29it/s]Epoch: 1, train for the 320-th batch, train loss: 0.5727564096450806:  84%|█████████▏ | 320/383 [01:35<00:24,  2.59it/s]Epoch: 4, train for the 105-th batch, train loss: 0.36687877774238586:  87%|████████▋ | 104/119 [00:17<00:02,  6.50it/s]Epoch: 2, train for the 66-th batch, train loss: 0.6022077202796936:  27%|███▌         | 65/237 [00:10<00:38,  4.43it/s]Epoch: 4, train for the 105-th batch, train loss: 0.36687877774238586:  88%|████████▊ | 105/119 [00:17<00:02,  6.12it/s]Epoch: 3, train for the 107-th batch, train loss: 0.47233977913856506:  70%|███████   | 106/151 [00:18<00:09,  4.68it/s]Epoch: 2, train for the 178-th batch, train loss: 0.4864231050014496:  73%|████████   | 177/241 [00:30<00:13,  4.81it/s]Epoch: 2, train for the 66-th batch, train loss: 0.6022077202796936:  28%|███▌         | 66/237 [00:10<00:35,  4.85it/s]Epoch: 4, train for the 30-th batch, train loss: 0.38879433274269104:  20%|██▍         | 29/146 [00:04<00:18,  6.46it/s]Epoch: 3, train for the 107-th batch, train loss: 0.47233977913856506:  71%|███████   | 107/151 [00:18<00:09,  4.59it/s]Epoch: 4, train for the 30-th batch, train loss: 0.38879433274269104:  21%|██▍         | 30/146 [00:04<00:18,  6.27it/s]Epoch: 2, train for the 178-th batch, train loss: 0.4864231050014496:  74%|████████   | 178/241 [00:30<00:13,  4.68it/s]Epoch: 2, train for the 67-th batch, train loss: 0.5484938621520996:  28%|███▌         | 66/237 [00:11<00:35,  4.85it/s]Epoch: 4, train for the 106-th batch, train loss: 0.37666556239128113:  88%|████████▊ | 105/119 [00:17<00:02,  6.12it/s]Epoch: 2, train for the 67-th batch, train loss: 0.5484938621520996:  28%|███▋         | 67/237 [00:11<00:34,  4.95it/s]Epoch: 4, train for the 31-th batch, train loss: 0.4559551775455475:  21%|██▋          | 30/146 [00:04<00:18,  6.27it/s]Epoch: 4, train for the 106-th batch, train loss: 0.37666556239128113:  89%|████████▉ | 106/119 [00:17<00:02,  5.64it/s]Epoch: 4, train for the 31-th batch, train loss: 0.4559551775455475:  21%|██▊          | 31/146 [00:04<00:19,  6.02it/s]Epoch: 2, train for the 179-th batch, train loss: 0.471476286649704:  74%|████████▊   | 178/241 [00:30<00:13,  4.68it/s]Epoch: 3, train for the 108-th batch, train loss: 0.5140612125396729:  71%|███████▊   | 107/151 [00:18<00:09,  4.59it/s]Epoch: 2, train for the 179-th batch, train loss: 0.471476286649704:  74%|████████▉   | 179/241 [00:30<00:13,  4.47it/s]Epoch: 3, train for the 108-th batch, train loss: 0.5140612125396729:  72%|███████▊   | 108/151 [00:18<00:09,  4.37it/s]Epoch: 4, train for the 32-th batch, train loss: 0.4605080783367157:  21%|██▊          | 31/146 [00:05<00:19,  6.02it/s]Epoch: 2, train for the 68-th batch, train loss: 0.5400327444076538:  28%|███▋         | 67/237 [00:11<00:34,  4.95it/s]Epoch: 4, train for the 32-th batch, train loss: 0.4605080783367157:  22%|██▊          | 32/146 [00:05<00:17,  6.47it/s]Epoch: 2, train for the 68-th batch, train loss: 0.5400327444076538:  29%|███▋         | 68/237 [00:11<00:31,  5.31it/s]Epoch: 4, train for the 107-th batch, train loss: 0.39932987093925476:  89%|████████▉ | 106/119 [00:17<00:02,  5.64it/s]Epoch: 4, train for the 107-th batch, train loss: 0.39932987093925476:  90%|████████▉ | 107/119 [00:17<00:02,  5.63it/s]Epoch: 2, train for the 180-th batch, train loss: 0.5496135354042053:  74%|████████▏  | 179/241 [00:30<00:13,  4.47it/s]Epoch: 3, train for the 109-th batch, train loss: 0.4729958176612854:  72%|███████▊   | 108/151 [00:19<00:09,  4.37it/s]Epoch: 2, train for the 180-th batch, train loss: 0.5496135354042053:  75%|████████▏  | 180/241 [00:30<00:12,  4.76it/s]Epoch: 4, train for the 33-th batch, train loss: 0.45788535475730896:  22%|██▋         | 32/146 [00:05<00:17,  6.47it/s]Epoch: 1, train for the 321-th batch, train loss: 0.5401673913002014:  84%|█████████▏ | 320/383 [01:35<00:24,  2.59it/s]Epoch: 4, train for the 33-th batch, train loss: 0.45788535475730896:  23%|██▋         | 33/146 [00:05<00:16,  6.87it/s]Epoch: 3, train for the 109-th batch, train loss: 0.4729958176612854:  72%|███████▉   | 109/151 [00:19<00:09,  4.56it/s]Epoch: 2, train for the 69-th batch, train loss: 0.564091145992279:  29%|████          | 68/237 [00:11<00:31,  5.31it/s]Epoch: 2, train for the 69-th batch, train loss: 0.564091145992279:  29%|████          | 69/237 [00:11<00:30,  5.56it/s]Epoch: 1, train for the 321-th batch, train loss: 0.5401673913002014:  84%|█████████▏ | 321/383 [01:35<00:27,  2.24it/s]Epoch: 4, train for the 34-th batch, train loss: 0.4637490212917328:  23%|██▉          | 33/146 [00:05<00:16,  6.87it/s]Epoch: 2, train for the 181-th batch, train loss: 0.8076441287994385:  75%|████████▏  | 180/241 [00:30<00:12,  4.76it/s]Epoch: 4, train for the 108-th batch, train loss: 0.30275198817253113:  90%|████████▉ | 107/119 [00:17<00:02,  5.63it/s]Epoch: 4, train for the 108-th batch, train loss: 0.30275198817253113:  91%|█████████ | 108/119 [00:18<00:02,  5.01it/s]Epoch: 2, train for the 181-th batch, train loss: 0.8076441287994385:  75%|████████▎  | 181/241 [00:30<00:12,  4.97it/s]Epoch: 3, train for the 110-th batch, train loss: 0.49914103746414185:  72%|███████▏  | 109/151 [00:19<00:09,  4.56it/s]Epoch: 4, train for the 35-th batch, train loss: 0.40641990303993225:  23%|██▋         | 33/146 [00:05<00:16,  6.87it/s]Epoch: 4, train for the 35-th batch, train loss: 0.40641990303993225:  24%|██▉         | 35/146 [00:05<00:14,  7.87it/s]Epoch: 3, train for the 110-th batch, train loss: 0.49914103746414185:  73%|███████▎  | 110/151 [00:19<00:08,  4.66it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5398660898208618:  29%|███▊         | 69/237 [00:11<00:30,  5.56it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5398660898208618:  30%|███▊         | 70/237 [00:11<00:30,  5.51it/s]Epoch: 2, train for the 182-th batch, train loss: 0.5688112378120422:  75%|████████▎  | 181/241 [00:30<00:12,  4.97it/s]Epoch: 2, train for the 182-th batch, train loss: 0.5688112378120422:  76%|████████▎  | 182/241 [00:30<00:11,  5.35it/s]Epoch: 4, train for the 109-th batch, train loss: 0.3973202407360077:  91%|█████████▉ | 108/119 [00:18<00:02,  5.01it/s]Epoch: 1, train for the 322-th batch, train loss: 0.5459545850753784:  84%|█████████▏ | 321/383 [01:36<00:27,  2.24it/s]Epoch: 4, train for the 109-th batch, train loss: 0.3973202407360077:  92%|██████████ | 109/119 [00:18<00:01,  5.29it/s]Epoch: 4, train for the 36-th batch, train loss: 0.41863563656806946:  24%|██▉         | 35/146 [00:05<00:14,  7.87it/s]Epoch: 4, train for the 36-th batch, train loss: 0.41863563656806946:  25%|██▉         | 36/146 [00:05<00:14,  7.40it/s]Epoch: 3, train for the 111-th batch, train loss: 0.4801938533782959:  73%|████████   | 110/151 [00:19<00:08,  4.66it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5360656380653381:  30%|███▊         | 70/237 [00:11<00:30,  5.51it/s]Epoch: 3, train for the 111-th batch, train loss: 0.4801938533782959:  74%|████████   | 111/151 [00:19<00:08,  4.93it/s]Epoch: 1, train for the 322-th batch, train loss: 0.5459545850753784:  84%|█████████▏ | 322/383 [01:36<00:25,  2.40it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5360656380653381:  30%|███▉         | 71/237 [00:11<00:29,  5.58it/s]Epoch: 2, train for the 183-th batch, train loss: 0.3753361701965332:  76%|████████▎  | 182/241 [00:31<00:11,  5.35it/s]Epoch: 2, train for the 183-th batch, train loss: 0.3753361701965332:  76%|████████▎  | 183/241 [00:31<00:10,  5.66it/s]Epoch: 4, train for the 110-th batch, train loss: 0.42130401730537415:  92%|█████████▏| 109/119 [00:18<00:01,  5.29it/s]Epoch: 4, train for the 37-th batch, train loss: 0.4582923650741577:  25%|███▏         | 36/146 [00:05<00:14,  7.40it/s]Epoch: 4, train for the 110-th batch, train loss: 0.42130401730537415:  92%|█████████▏| 110/119 [00:18<00:01,  5.27it/s]Epoch: 4, train for the 37-th batch, train loss: 0.4582923650741577:  25%|███▎         | 37/146 [00:05<00:15,  7.01it/s]Epoch: 3, train for the 112-th batch, train loss: 0.45025020837783813:  74%|███████▎  | 111/151 [00:19<00:08,  4.93it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5520843267440796:  30%|███▉         | 71/237 [00:12<00:29,  5.58it/s]Epoch: 3, train for the 112-th batch, train loss: 0.45025020837783813:  74%|███████▍  | 112/151 [00:19<00:07,  5.09it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5520843267440796:  30%|███▉         | 72/237 [00:12<00:28,  5.75it/s]Epoch: 2, train for the 184-th batch, train loss: 0.5712701082229614:  76%|████████▎  | 183/241 [00:31<00:10,  5.66it/s]Epoch: 1, train for the 323-th batch, train loss: 0.416715532541275:  84%|██████████  | 322/383 [01:36<00:25,  2.40it/s]Epoch: 2, train for the 184-th batch, train loss: 0.5712701082229614:  76%|████████▍  | 184/241 [00:31<00:09,  5.81it/s]Epoch: 4, train for the 38-th batch, train loss: 0.4465133547782898:  25%|███▎         | 37/146 [00:05<00:15,  7.01it/s]Epoch: 4, train for the 38-th batch, train loss: 0.4465133547782898:  26%|███▍         | 38/146 [00:05<00:15,  6.95it/s]Epoch: 1, train for the 323-th batch, train loss: 0.416715532541275:  84%|██████████  | 323/383 [01:36<00:22,  2.67it/s]Epoch: 4, train for the 111-th batch, train loss: 0.447454571723938:  92%|███████████ | 110/119 [00:18<00:01,  5.27it/s]Epoch: 4, train for the 111-th batch, train loss: 0.447454571723938:  93%|███████████▏| 111/119 [00:18<00:01,  5.31it/s]Epoch: 3, train for the 113-th batch, train loss: 0.5218824148178101:  74%|████████▏  | 112/151 [00:19<00:07,  5.09it/s]Epoch: 3, train for the 113-th batch, train loss: 0.5218824148178101:  75%|████████▏  | 113/151 [00:19<00:07,  5.39it/s]Epoch: 2, train for the 73-th batch, train loss: 0.5309928059577942:  30%|███▉         | 72/237 [00:12<00:28,  5.75it/s]Epoch: 2, train for the 185-th batch, train loss: 0.5918552875518799:  76%|████████▍  | 184/241 [00:31<00:09,  5.81it/s]Epoch: 2, train for the 73-th batch, train loss: 0.5309928059577942:  31%|████         | 73/237 [00:12<00:29,  5.57it/s]Epoch: 2, train for the 185-th batch, train loss: 0.5918552875518799:  77%|████████▍  | 185/241 [00:31<00:08,  6.28it/s]Epoch: 4, train for the 39-th batch, train loss: 0.4671742916107178:  26%|███▍         | 38/146 [00:06<00:15,  6.95it/s]Epoch: 4, train for the 39-th batch, train loss: 0.4671742916107178:  27%|███▍         | 39/146 [00:06<00:15,  6.86it/s]Epoch: 4, train for the 112-th batch, train loss: 0.38039976358413696:  93%|█████████▎| 111/119 [00:18<00:01,  5.31it/s]Epoch: 4, train for the 112-th batch, train loss: 0.38039976358413696:  94%|█████████▍| 112/119 [00:18<00:01,  5.49it/s]Epoch: 1, train for the 324-th batch, train loss: 0.2962352931499481:  84%|█████████▎ | 323/383 [01:36<00:22,  2.67it/s]Epoch: 3, train for the 114-th batch, train loss: 0.49148330092430115:  75%|███████▍  | 113/151 [00:20<00:07,  5.39it/s]Epoch: 2, train for the 186-th batch, train loss: 0.45470598340034485:  77%|███████▋  | 185/241 [00:31<00:08,  6.28it/s]Epoch: 4, train for the 40-th batch, train loss: 0.44874095916748047:  27%|███▏        | 39/146 [00:06<00:15,  6.86it/s]Epoch: 4, train for the 40-th batch, train loss: 0.44874095916748047:  27%|███▎        | 40/146 [00:06<00:14,  7.10it/s]Epoch: 3, train for the 114-th batch, train loss: 0.49148330092430115:  75%|███████▌  | 114/151 [00:20<00:07,  5.07it/s]Epoch: 2, train for the 186-th batch, train loss: 0.45470598340034485:  77%|███████▋  | 186/241 [00:31<00:09,  5.86it/s]Epoch: 1, train for the 324-th batch, train loss: 0.2962352931499481:  85%|█████████▎ | 324/383 [01:36<00:20,  2.81it/s]Epoch: 4, train for the 113-th batch, train loss: 0.3803318738937378:  94%|██████████▎| 112/119 [00:18<00:01,  5.49it/s]Epoch: 4, train for the 113-th batch, train loss: 0.3803318738937378:  95%|██████████▍| 113/119 [00:18<00:01,  5.81it/s]Epoch: 2, train for the 74-th batch, train loss: 0.533458411693573:  31%|████▎         | 73/237 [00:12<00:29,  5.57it/s]Epoch: 2, train for the 74-th batch, train loss: 0.533458411693573:  31%|████▎         | 74/237 [00:12<00:35,  4.53it/s]Epoch: 4, train for the 41-th batch, train loss: 0.42233002185821533:  27%|███▎        | 40/146 [00:06<00:14,  7.10it/s]Epoch: 4, train for the 41-th batch, train loss: 0.42233002185821533:  28%|███▎        | 41/146 [00:06<00:14,  7.21it/s]Epoch: 2, train for the 187-th batch, train loss: 0.5658974051475525:  77%|████████▍  | 186/241 [00:31<00:09,  5.86it/s]Epoch: 3, train for the 115-th batch, train loss: 0.4720331132411957:  75%|████████▎  | 114/151 [00:20<00:07,  5.07it/s]Epoch: 2, train for the 187-th batch, train loss: 0.5658974051475525:  78%|████████▌  | 187/241 [00:31<00:09,  5.67it/s]Epoch: 4, train for the 114-th batch, train loss: 0.3414023220539093:  95%|██████████▍| 113/119 [00:18<00:01,  5.81it/s]Epoch: 3, train for the 115-th batch, train loss: 0.4720331132411957:  76%|████████▍  | 115/151 [00:20<00:07,  4.97it/s]Epoch: 4, train for the 114-th batch, train loss: 0.3414023220539093:  96%|██████████▌| 114/119 [00:19<00:00,  6.03it/s]Epoch: 4, train for the 42-th batch, train loss: 0.46717727184295654:  28%|███▎        | 41/146 [00:06<00:14,  7.21it/s]Epoch: 4, train for the 42-th batch, train loss: 0.46717727184295654:  29%|███▍        | 42/146 [00:06<00:14,  7.32it/s]Epoch: 1, train for the 325-th batch, train loss: 0.3690146803855896:  85%|█████████▎ | 324/383 [01:36<00:20,  2.81it/s]Epoch: 2, train for the 75-th batch, train loss: 0.5184046030044556:  31%|████         | 74/237 [00:12<00:35,  4.53it/s]Epoch: 2, train for the 75-th batch, train loss: 0.5184046030044556:  32%|████         | 75/237 [00:12<00:35,  4.51it/s]Epoch: 2, train for the 188-th batch, train loss: 0.4294146001338959:  78%|████████▌  | 187/241 [00:31<00:09,  5.67it/s]Epoch: 1, train for the 325-th batch, train loss: 0.3690146803855896:  85%|█████████▎ | 325/383 [01:37<00:20,  2.86it/s]Epoch: 2, train for the 188-th batch, train loss: 0.4294146001338959:  78%|████████▌  | 188/241 [00:31<00:09,  5.58it/s]Epoch: 4, train for the 115-th batch, train loss: 0.3668532371520996:  96%|██████████▌| 114/119 [00:19<00:00,  6.03it/s]Epoch: 3, train for the 116-th batch, train loss: 0.42504942417144775:  76%|███████▌  | 115/151 [00:20<00:07,  4.97it/s]Epoch: 4, train for the 115-th batch, train loss: 0.3668532371520996:  97%|██████████▋| 115/119 [00:19<00:00,  5.89it/s]Epoch: 3, train for the 116-th batch, train loss: 0.42504942417144775:  77%|███████▋  | 116/151 [00:20<00:07,  4.97it/s]Epoch: 4, train for the 43-th batch, train loss: 0.418954998254776:  29%|████          | 42/146 [00:06<00:14,  7.32it/s]Epoch: 4, train for the 43-th batch, train loss: 0.418954998254776:  29%|████          | 43/146 [00:06<00:14,  6.97it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5552855730056763:  32%|████         | 75/237 [00:12<00:35,  4.51it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5552855730056763:  32%|████▏        | 76/237 [00:12<00:33,  4.81it/s]Epoch: 2, train for the 189-th batch, train loss: 0.4133344292640686:  78%|████████▌  | 188/241 [00:32<00:09,  5.58it/s]Epoch: 2, train for the 189-th batch, train loss: 0.4133344292640686:  78%|████████▋  | 189/241 [00:32<00:09,  5.68it/s]Epoch: 4, train for the 116-th batch, train loss: 0.3546665906906128:  97%|██████████▋| 115/119 [00:19<00:00,  5.89it/s]Epoch: 4, train for the 44-th batch, train loss: 0.41441434621810913:  29%|███▌        | 43/146 [00:06<00:14,  6.97it/s]Epoch: 4, train for the 116-th batch, train loss: 0.3546665906906128:  97%|██████████▋| 116/119 [00:19<00:00,  5.55it/s]Epoch: 4, train for the 44-th batch, train loss: 0.41441434621810913:  30%|███▌        | 44/146 [00:06<00:15,  6.39it/s]Epoch: 3, train for the 117-th batch, train loss: 0.49616751074790955:  77%|███████▋  | 116/151 [00:20<00:07,  4.97it/s]Epoch: 3, train for the 117-th batch, train loss: 0.49616751074790955:  77%|███████▋  | 117/151 [00:20<00:07,  4.71it/s]Epoch: 2, train for the 190-th batch, train loss: 0.3635006248950958:  78%|████████▋  | 189/241 [00:32<00:09,  5.68it/s]Epoch: 2, train for the 77-th batch, train loss: 0.5614913702011108:  32%|████▏        | 76/237 [00:13<00:33,  4.81it/s]Epoch: 1, train for the 326-th batch, train loss: 0.4261234700679779:  85%|█████████▎ | 325/383 [01:37<00:20,  2.86it/s]Epoch: 2, train for the 190-th batch, train loss: 0.3635006248950958:  79%|████████▋  | 190/241 [00:32<00:08,  5.72it/s]Epoch: 2, train for the 77-th batch, train loss: 0.5614913702011108:  32%|████▏        | 77/237 [00:13<00:32,  4.85it/s]Epoch: 4, train for the 45-th batch, train loss: 0.4564302861690521:  30%|███▉         | 44/146 [00:06<00:15,  6.39it/s]Epoch: 4, train for the 45-th batch, train loss: 0.4564302861690521:  31%|████         | 45/146 [00:06<00:15,  6.62it/s]Epoch: 4, train for the 117-th batch, train loss: 0.37946948409080505:  97%|█████████▋| 116/119 [00:19<00:00,  5.55it/s]Epoch: 4, train for the 117-th batch, train loss: 0.37946948409080505:  98%|█████████▊| 117/119 [00:19<00:00,  5.50it/s]Epoch: 3, train for the 118-th batch, train loss: 0.4434445798397064:  77%|████████▌  | 117/151 [00:20<00:07,  4.71it/s]Epoch: 1, train for the 326-th batch, train loss: 0.4261234700679779:  85%|█████████▎ | 326/383 [01:37<00:21,  2.67it/s]Epoch: 3, train for the 118-th batch, train loss: 0.4434445798397064:  78%|████████▌  | 118/151 [00:20<00:06,  5.01it/s]Epoch: 2, train for the 78-th batch, train loss: 0.5591278672218323:  32%|████▏        | 77/237 [00:13<00:32,  4.85it/s]Epoch: 2, train for the 191-th batch, train loss: 0.3211255967617035:  79%|████████▋  | 190/241 [00:32<00:08,  5.72it/s]Epoch: 2, train for the 78-th batch, train loss: 0.5591278672218323:  33%|████▎        | 78/237 [00:13<00:29,  5.34it/s]Epoch: 2, train for the 191-th batch, train loss: 0.3211255967617035:  79%|████████▋  | 191/241 [00:32<00:08,  5.75it/s]Epoch: 4, train for the 46-th batch, train loss: 0.45457738637924194:  31%|███▋        | 45/146 [00:07<00:15,  6.62it/s]Epoch: 4, train for the 46-th batch, train loss: 0.45457738637924194:  32%|███▊        | 46/146 [00:07<00:16,  6.06it/s]Epoch: 4, train for the 118-th batch, train loss: 0.3079213500022888:  98%|██████████▊| 117/119 [00:19<00:00,  5.50it/s]Epoch: 4, train for the 118-th batch, train loss: 0.3079213500022888:  99%|██████████▉| 118/119 [00:19<00:00,  5.60it/s]Epoch: 3, train for the 119-th batch, train loss: 0.5128042101860046:  78%|████████▌  | 118/151 [00:21<00:06,  5.01it/s]Epoch: 3, train for the 119-th batch, train loss: 0.5128042101860046:  79%|████████▋  | 119/151 [00:21<00:06,  5.16it/s]Epoch: 2, train for the 79-th batch, train loss: 0.5795970559120178:  33%|████▎        | 78/237 [00:13<00:29,  5.34it/s]Epoch: 2, train for the 192-th batch, train loss: 0.31522148847579956:  79%|███████▉  | 191/241 [00:32<00:08,  5.75it/s]Epoch: 2, train for the 79-th batch, train loss: 0.5795970559120178:  33%|████▎        | 79/237 [00:13<00:29,  5.44it/s]Epoch: 2, train for the 192-th batch, train loss: 0.31522148847579956:  80%|███████▉  | 192/241 [00:32<00:08,  5.86it/s]Epoch: 4, train for the 119-th batch, train loss: 0.2429332286119461:  99%|██████████▉| 118/119 [00:19<00:00,  5.60it/s]Epoch: 4, train for the 119-th batch, train loss: 0.2429332286119461: 100%|███████████| 119/119 [00:19<00:00,  6.11it/s]Epoch: 4, train for the 119-th batch, train loss: 0.2429332286119461: 100%|███████████| 119/119 [00:19<00:00,  5.99it/s]
Epoch: 1, train for the 327-th batch, train loss: 0.4628148674964905:  85%|█████████▎ | 326/383 [01:37<00:21,  2.67it/s]Epoch: 4, train for the 47-th batch, train loss: 0.47488105297088623:  32%|███▊        | 46/146 [00:07<00:16,  6.06it/s]  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 4, train for the 47-th batch, train loss: 0.47488105297088623:  32%|███▊        | 47/146 [00:07<00:17,  5.71it/s]Epoch: 3, train for the 120-th batch, train loss: 0.49406397342681885:  79%|███████▉  | 119/151 [00:21<00:06,  5.16it/s]Epoch: 1, train for the 327-th batch, train loss: 0.4628148674964905:  85%|█████████▍ | 327/383 [01:37<00:20,  2.68it/s]Epoch: 3, train for the 120-th batch, train loss: 0.49406397342681885:  79%|███████▉  | 120/151 [00:21<00:05,  5.29it/s]Epoch: 2, train for the 80-th batch, train loss: 0.565760612487793:  33%|████▋         | 79/237 [00:13<00:29,  5.44it/s]Epoch: 2, train for the 193-th batch, train loss: 0.3808342516422272:  80%|████████▊  | 192/241 [00:32<00:08,  5.86it/s]Epoch: 2, train for the 80-th batch, train loss: 0.565760612487793:  34%|████▋         | 80/237 [00:13<00:27,  5.63it/s]Epoch: 2, train for the 193-th batch, train loss: 0.3808342516422272:  80%|████████▊  | 193/241 [00:32<00:07,  6.06it/s]evaluate for the 1-th batch, evaluate loss: 0.6159867644309998:   0%|                            | 0/40 [00:00<?, ?it/s]Epoch: 4, train for the 48-th batch, train loss: 0.4438621401786804:  32%|████▏        | 47/146 [00:07<00:17,  5.71it/s]Epoch: 4, train for the 48-th batch, train loss: 0.4438621401786804:  33%|████▎        | 48/146 [00:07<00:16,  5.92it/s]evaluate for the 2-th batch, evaluate loss: 0.6124268770217896:   0%|                            | 0/40 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.6124268770217896:   5%|█                   | 2/40 [00:00<00:03, 12.09it/s]Epoch: 3, train for the 121-th batch, train loss: 0.4621323049068451:  79%|████████▋  | 120/151 [00:21<00:05,  5.29it/s]Epoch: 3, train for the 121-th batch, train loss: 0.4621323049068451:  80%|████████▊  | 121/151 [00:21<00:05,  5.26it/s]Epoch: 2, train for the 194-th batch, train loss: 0.3745444416999817:  80%|████████▊  | 193/241 [00:32<00:07,  6.06it/s]evaluate for the 3-th batch, evaluate loss: 0.6891297101974487:   5%|█                   | 2/40 [00:00<00:03, 12.09it/s]Epoch: 2, train for the 81-th batch, train loss: 0.581965446472168:  34%|████▋         | 80/237 [00:13<00:27,  5.63it/s]Epoch: 2, train for the 194-th batch, train loss: 0.3745444416999817:  80%|████████▊  | 194/241 [00:32<00:07,  5.88it/s]Epoch: 2, train for the 81-th batch, train loss: 0.581965446472168:  34%|████▊         | 81/237 [00:13<00:28,  5.46it/s]Epoch: 1, train for the 328-th batch, train loss: 0.3771457076072693:  85%|█████████▍ | 327/383 [01:38<00:20,  2.68it/s]Epoch: 4, train for the 49-th batch, train loss: 0.4748851954936981:  33%|████▎        | 48/146 [00:07<00:16,  5.92it/s]evaluate for the 4-th batch, evaluate loss: 0.7414119839668274:   5%|█                   | 2/40 [00:00<00:03, 12.09it/s]evaluate for the 4-th batch, evaluate loss: 0.7414119839668274:  10%|██                  | 4/40 [00:00<00:02, 12.59it/s]Epoch: 4, train for the 49-th batch, train loss: 0.4748851954936981:  34%|████▎        | 49/146 [00:07<00:16,  5.93it/s]Epoch: 1, train for the 328-th batch, train loss: 0.3771457076072693:  86%|█████████▍ | 328/383 [01:38<00:19,  2.77it/s]evaluate for the 5-th batch, evaluate loss: 0.744907796382904:  10%|██                   | 4/40 [00:00<00:02, 12.59it/s]Epoch: 2, train for the 82-th batch, train loss: 0.6291193962097168:  34%|████▍        | 81/237 [00:13<00:28,  5.46it/s]Epoch: 2, train for the 82-th batch, train loss: 0.6291193962097168:  35%|████▍        | 82/237 [00:13<00:28,  5.48it/s]Epoch: 2, train for the 195-th batch, train loss: 0.5120777487754822:  80%|████████▊  | 194/241 [00:33<00:07,  5.88it/s]Epoch: 3, train for the 122-th batch, train loss: 0.48894232511520386:  80%|████████  | 121/151 [00:21<00:05,  5.26it/s]Epoch: 2, train for the 195-th batch, train loss: 0.5120777487754822:  81%|████████▉  | 195/241 [00:33<00:08,  5.40it/s]evaluate for the 6-th batch, evaluate loss: 0.6610333919525146:  10%|██                  | 4/40 [00:00<00:02, 12.59it/s]Epoch: 4, train for the 50-th batch, train loss: 0.5027405023574829:  34%|████▎        | 49/146 [00:07<00:16,  5.93it/s]evaluate for the 6-th batch, evaluate loss: 0.6610333919525146:  15%|███                 | 6/40 [00:00<00:02, 12.81it/s]Epoch: 4, train for the 50-th batch, train loss: 0.5027405023574829:  34%|████▍        | 50/146 [00:07<00:15,  6.09it/s]Epoch: 3, train for the 122-th batch, train loss: 0.48894232511520386:  81%|████████  | 122/151 [00:21<00:06,  4.77it/s]evaluate for the 7-th batch, evaluate loss: 0.7048364281654358:  15%|███                 | 6/40 [00:00<00:02, 12.81it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5906700491905212:  35%|████▍        | 82/237 [00:14<00:28,  5.48it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5906700491905212:  35%|████▌        | 83/237 [00:14<00:27,  5.58it/s]Epoch: 2, train for the 196-th batch, train loss: 0.4768633544445038:  81%|████████▉  | 195/241 [00:33<00:08,  5.40it/s]Epoch: 4, train for the 51-th batch, train loss: 0.4724610149860382:  34%|████▍        | 50/146 [00:07<00:15,  6.09it/s]evaluate for the 8-th batch, evaluate loss: 0.6714745163917542:  15%|███                 | 6/40 [00:00<00:02, 12.81it/s]evaluate for the 8-th batch, evaluate loss: 0.6714745163917542:  20%|████                | 8/40 [00:00<00:02, 12.62it/s]Epoch: 2, train for the 196-th batch, train loss: 0.4768633544445038:  81%|████████▉  | 196/241 [00:33<00:08,  5.56it/s]Epoch: 4, train for the 51-th batch, train loss: 0.4724610149860382:  35%|████▌        | 51/146 [00:07<00:15,  6.15it/s]Epoch: 3, train for the 123-th batch, train loss: 0.49613937735557556:  81%|████████  | 122/151 [00:21<00:06,  4.77it/s]Epoch: 3, train for the 123-th batch, train loss: 0.49613937735557556:  81%|████████▏ | 123/151 [00:21<00:05,  4.74it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5761207938194275:  35%|████▌        | 83/237 [00:14<00:27,  5.58it/s]evaluate for the 9-th batch, evaluate loss: 0.699243426322937:  20%|████▏                | 8/40 [00:00<00:02, 12.62it/s]Epoch: 2, train for the 197-th batch, train loss: 0.4875437915325165:  81%|████████▉  | 196/241 [00:33<00:08,  5.56it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5761207938194275:  35%|████▌        | 84/237 [00:14<00:26,  5.77it/s]Epoch: 4, train for the 52-th batch, train loss: 0.45709800720214844:  35%|████▏       | 51/146 [00:08<00:15,  6.15it/s]Epoch: 2, train for the 197-th batch, train loss: 0.4875437915325165:  82%|████████▉  | 197/241 [00:33<00:07,  5.80it/s]Epoch: 4, train for the 52-th batch, train loss: 0.45709800720214844:  36%|████▎       | 52/146 [00:08<00:15,  6.23it/s]evaluate for the 10-th batch, evaluate loss: 0.7954807877540588:  20%|███▊               | 8/40 [00:00<00:02, 12.62it/s]evaluate for the 10-th batch, evaluate loss: 0.7954807877540588:  25%|████▌             | 10/40 [00:00<00:02, 12.05it/s]Epoch: 1, train for the 329-th batch, train loss: 0.33423545956611633:  86%|████████▌ | 328/383 [01:38<00:19,  2.77it/s]Epoch: 3, train for the 124-th batch, train loss: 0.44377821683883667:  81%|████████▏ | 123/151 [00:22<00:05,  4.74it/s]evaluate for the 11-th batch, evaluate loss: 0.6430001854896545:  25%|████▌             | 10/40 [00:00<00:02, 12.05it/s]Epoch: 3, train for the 124-th batch, train loss: 0.44377821683883667:  82%|████████▏ | 124/151 [00:22<00:05,  4.80it/s]Epoch: 1, train for the 329-th batch, train loss: 0.33423545956611633:  86%|████████▌ | 329/383 [01:38<00:22,  2.41it/s]Epoch: 2, train for the 85-th batch, train loss: 0.5736528038978577:  35%|████▌        | 84/237 [00:14<00:26,  5.77it/s]Epoch: 4, train for the 53-th batch, train loss: 0.47274360060691833:  36%|████▎       | 52/146 [00:08<00:15,  6.23it/s]Epoch: 4, train for the 53-th batch, train loss: 0.47274360060691833:  36%|████▎       | 53/146 [00:08<00:14,  6.38it/s]evaluate for the 12-th batch, evaluate loss: 0.6271793246269226:  25%|████▌             | 10/40 [00:00<00:02, 12.05it/s]evaluate for the 12-th batch, evaluate loss: 0.6271793246269226:  30%|█████▍            | 12/40 [00:00<00:02, 12.97it/s]Epoch: 2, train for the 85-th batch, train loss: 0.5736528038978577:  36%|████▋        | 85/237 [00:14<00:26,  5.72it/s]Epoch: 2, train for the 198-th batch, train loss: 0.47059744596481323:  82%|████████▏ | 197/241 [00:33<00:07,  5.80it/s]Epoch: 2, train for the 198-th batch, train loss: 0.47059744596481323:  82%|████████▏ | 198/241 [00:33<00:07,  5.51it/s]evaluate for the 13-th batch, evaluate loss: 0.6369571685791016:  30%|█████▍            | 12/40 [00:01<00:02, 12.97it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5682380795478821:  36%|████▋        | 85/237 [00:14<00:26,  5.72it/s]evaluate for the 14-th batch, evaluate loss: 0.6545544862747192:  30%|█████▍            | 12/40 [00:01<00:02, 12.97it/s]Epoch: 4, train for the 54-th batch, train loss: 0.4728063941001892:  36%|████▋        | 53/146 [00:08<00:14,  6.38it/s]evaluate for the 14-th batch, evaluate loss: 0.6545544862747192:  35%|██████▎           | 14/40 [00:01<00:02, 12.58it/s]Epoch: 4, train for the 54-th batch, train loss: 0.4728063941001892:  37%|████▊        | 54/146 [00:08<00:15,  6.10it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5682380795478821:  36%|████▋        | 86/237 [00:14<00:26,  5.66it/s]Epoch: 2, train for the 199-th batch, train loss: 0.3450019061565399:  82%|█████████  | 198/241 [00:33<00:07,  5.51it/s]evaluate for the 15-th batch, evaluate loss: 0.6712925434112549:  35%|██████▎           | 14/40 [00:01<00:02, 12.58it/s]Epoch: 2, train for the 199-th batch, train loss: 0.3450019061565399:  83%|█████████  | 199/241 [00:33<00:07,  5.43it/s]Epoch: 3, train for the 125-th batch, train loss: 0.5016806125640869:  82%|█████████  | 124/151 [00:22<00:05,  4.80it/s]Epoch: 3, train for the 125-th batch, train loss: 0.5016806125640869:  83%|█████████  | 125/151 [00:22<00:06,  4.09it/s]evaluate for the 16-th batch, evaluate loss: 0.77756267786026:  35%|███████             | 14/40 [00:01<00:02, 12.58it/s]evaluate for the 16-th batch, evaluate loss: 0.77756267786026:  40%|████████            | 16/40 [00:01<00:01, 13.00it/s]Epoch: 2, train for the 87-th batch, train loss: 0.5766566395759583:  36%|████▋        | 86/237 [00:14<00:26,  5.66it/s]Epoch: 4, train for the 55-th batch, train loss: 0.4997095465660095:  37%|████▊        | 54/146 [00:08<00:15,  6.10it/s]Epoch: 4, train for the 55-th batch, train loss: 0.4997095465660095:  38%|████▉        | 55/146 [00:08<00:14,  6.19it/s]Epoch: 2, train for the 87-th batch, train loss: 0.5766566395759583:  37%|████▊        | 87/237 [00:14<00:25,  5.94it/s]Epoch: 2, train for the 200-th batch, train loss: 0.4225437045097351:  83%|█████████  | 199/241 [00:33<00:07,  5.43it/s]Epoch: 2, train for the 200-th batch, train loss: 0.4225437045097351:  83%|█████████▏ | 200/241 [00:34<00:07,  5.83it/s]evaluate for the 17-th batch, evaluate loss: 0.6953392624855042:  40%|███████▏          | 16/40 [00:01<00:01, 13.00it/s]Epoch: 3, train for the 126-th batch, train loss: 0.469720721244812:  83%|█████████▉  | 125/151 [00:22<00:06,  4.09it/s]Epoch: 1, train for the 330-th batch, train loss: 0.451709508895874:  86%|██████████▎ | 329/383 [01:39<00:22,  2.41it/s]Epoch: 3, train for the 126-th batch, train loss: 0.469720721244812:  83%|██████████  | 126/151 [00:22<00:05,  4.33it/s]Epoch: 4, train for the 56-th batch, train loss: 0.4869031310081482:  38%|████▉        | 55/146 [00:08<00:14,  6.19it/s]evaluate for the 18-th batch, evaluate loss: 0.6499279141426086:  40%|███████▏          | 16/40 [00:01<00:01, 13.00it/s]evaluate for the 18-th batch, evaluate loss: 0.6499279141426086:  45%|████████          | 18/40 [00:01<00:01, 12.45it/s]Epoch: 4, train for the 56-th batch, train loss: 0.4869031310081482:  38%|████▉        | 56/146 [00:08<00:14,  6.17it/s]Epoch: 2, train for the 88-th batch, train loss: 0.5888667702674866:  37%|████▊        | 87/237 [00:15<00:25,  5.94it/s]Epoch: 2, train for the 201-th batch, train loss: 0.41444697976112366:  83%|████████▎ | 200/241 [00:34<00:07,  5.83it/s]Epoch: 2, train for the 88-th batch, train loss: 0.5888667702674866:  37%|████▊        | 88/237 [00:15<00:26,  5.63it/s]Epoch: 1, train for the 330-th batch, train loss: 0.451709508895874:  86%|██████████▎ | 330/383 [01:39<00:24,  2.14it/s]Epoch: 2, train for the 201-th batch, train loss: 0.41444697976112366:  83%|████████▎ | 201/241 [00:34<00:06,  5.83it/s]evaluate for the 19-th batch, evaluate loss: 0.7708473205566406:  45%|████████          | 18/40 [00:01<00:01, 12.45it/s]Epoch: 3, train for the 127-th batch, train loss: 0.47126418352127075:  83%|████████▎ | 126/151 [00:22<00:05,  4.33it/s]Epoch: 3, train for the 127-th batch, train loss: 0.47126418352127075:  84%|████████▍ | 127/151 [00:22<00:05,  4.72it/s]Epoch: 4, train for the 57-th batch, train loss: 0.49625954031944275:  38%|████▌       | 56/146 [00:08<00:14,  6.17it/s]evaluate for the 20-th batch, evaluate loss: 0.7572007775306702:  45%|████████          | 18/40 [00:01<00:01, 12.45it/s]evaluate for the 20-th batch, evaluate loss: 0.7572007775306702:  50%|█████████         | 20/40 [00:01<00:01, 12.27it/s]Epoch: 4, train for the 57-th batch, train loss: 0.49625954031944275:  39%|████▋       | 57/146 [00:08<00:14,  6.16it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5620241165161133:  37%|████▊        | 88/237 [00:15<00:26,  5.63it/s]Epoch: 2, train for the 202-th batch, train loss: 0.2568798065185547:  83%|█████████▏ | 201/241 [00:34<00:06,  5.83it/s]evaluate for the 21-th batch, evaluate loss: 0.6069698333740234:  50%|█████████         | 20/40 [00:01<00:01, 12.27it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5620241165161133:  38%|████▉        | 89/237 [00:15<00:27,  5.40it/s]Epoch: 2, train for the 202-th batch, train loss: 0.2568798065185547:  84%|█████████▏ | 202/241 [00:34<00:06,  5.62it/s]Epoch: 4, train for the 58-th batch, train loss: 0.48175179958343506:  39%|████▋       | 57/146 [00:09<00:14,  6.16it/s]Epoch: 4, train for the 58-th batch, train loss: 0.48175179958343506:  40%|████▊       | 58/146 [00:09<00:13,  6.68it/s]evaluate for the 22-th batch, evaluate loss: 0.6536130905151367:  50%|█████████         | 20/40 [00:01<00:01, 12.27it/s]evaluate for the 22-th batch, evaluate loss: 0.6536130905151367:  55%|█████████▉        | 22/40 [00:01<00:01, 12.95it/s]Epoch: 3, train for the 128-th batch, train loss: 0.46422141790390015:  84%|████████▍ | 127/151 [00:22<00:05,  4.72it/s]Epoch: 3, train for the 128-th batch, train loss: 0.46422141790390015:  85%|████████▍ | 128/151 [00:22<00:04,  4.91it/s]evaluate for the 23-th batch, evaluate loss: 0.6042210459709167:  55%|█████████▉        | 22/40 [00:01<00:01, 12.95it/s]Epoch: 2, train for the 90-th batch, train loss: 0.5580518841743469:  38%|████▉        | 89/237 [00:15<00:27,  5.40it/s]Epoch: 2, train for the 203-th batch, train loss: 0.4951060712337494:  84%|█████████▏ | 202/241 [00:34<00:06,  5.62it/s]Epoch: 2, train for the 90-th batch, train loss: 0.5580518841743469:  38%|████▉        | 90/237 [00:15<00:26,  5.52it/s]Epoch: 4, train for the 59-th batch, train loss: 0.46698328852653503:  40%|████▊       | 58/146 [00:09<00:13,  6.68it/s]Epoch: 2, train for the 203-th batch, train loss: 0.4951060712337494:  84%|█████████▎ | 203/241 [00:34<00:06,  5.52it/s]evaluate for the 24-th batch, evaluate loss: 0.7039702534675598:  55%|█████████▉        | 22/40 [00:01<00:01, 12.95it/s]evaluate for the 24-th batch, evaluate loss: 0.7039702534675598:  60%|██████████▊       | 24/40 [00:01<00:01, 13.10it/s]Epoch: 4, train for the 59-th batch, train loss: 0.46698328852653503:  40%|████▊       | 59/146 [00:09<00:13,  6.52it/s]Epoch: 3, train for the 129-th batch, train loss: 0.5315002799034119:  85%|█████████▎ | 128/151 [00:23<00:04,  4.91it/s]evaluate for the 25-th batch, evaluate loss: 0.6670807600021362:  60%|██████████▊       | 24/40 [00:01<00:01, 13.10it/s]Epoch: 3, train for the 129-th batch, train loss: 0.5315002799034119:  85%|█████████▍ | 129/151 [00:23<00:04,  4.93it/s]Epoch: 1, train for the 331-th batch, train loss: 0.4114130735397339:  86%|█████████▍ | 330/383 [01:39<00:24,  2.14it/s]Epoch: 4, train for the 60-th batch, train loss: 0.46169885993003845:  40%|████▊       | 59/146 [00:09<00:13,  6.52it/s]Epoch: 2, train for the 91-th batch, train loss: 0.5484208464622498:  38%|████▉        | 90/237 [00:15<00:26,  5.52it/s]evaluate for the 26-th batch, evaluate loss: 0.6727537512779236:  60%|██████████▊       | 24/40 [00:02<00:01, 13.10it/s]evaluate for the 26-th batch, evaluate loss: 0.6727537512779236:  65%|███████████▋      | 26/40 [00:02<00:01, 12.98it/s]Epoch: 2, train for the 204-th batch, train loss: 0.43108779191970825:  84%|████████▍ | 203/241 [00:34<00:06,  5.52it/s]Epoch: 4, train for the 60-th batch, train loss: 0.46169885993003845:  41%|████▉       | 60/146 [00:09<00:13,  6.45it/s]Epoch: 1, train for the 331-th batch, train loss: 0.4114130735397339:  86%|█████████▌ | 331/383 [01:39<00:25,  2.01it/s]Epoch: 2, train for the 204-th batch, train loss: 0.43108779191970825:  85%|████████▍ | 204/241 [00:34<00:06,  5.48it/s]Epoch: 2, train for the 91-th batch, train loss: 0.5484208464622498:  38%|████▉        | 91/237 [00:15<00:27,  5.27it/s]evaluate for the 27-th batch, evaluate loss: 0.6976841688156128:  65%|███████████▋      | 26/40 [00:02<00:01, 12.98it/s]Epoch: 3, train for the 130-th batch, train loss: 0.4876744747161865:  85%|█████████▍ | 129/151 [00:23<00:04,  4.93it/s]Epoch: 3, train for the 130-th batch, train loss: 0.4876744747161865:  86%|█████████▍ | 130/151 [00:23<00:04,  5.24it/s]Epoch: 4, train for the 61-th batch, train loss: 0.5189934968948364:  41%|█████▎       | 60/146 [00:09<00:13,  6.45it/s]Epoch: 4, train for the 61-th batch, train loss: 0.5189934968948364:  42%|█████▍       | 61/146 [00:09<00:12,  6.86it/s]evaluate for the 28-th batch, evaluate loss: 0.661794900894165:  65%|████████████▎      | 26/40 [00:02<00:01, 12.98it/s]evaluate for the 28-th batch, evaluate loss: 0.661794900894165:  70%|█████████████▎     | 28/40 [00:02<00:00, 12.92it/s]Epoch: 2, train for the 205-th batch, train loss: 0.48263826966285706:  85%|████████▍ | 204/241 [00:34<00:06,  5.48it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5149332284927368:  38%|████▉        | 91/237 [00:15<00:27,  5.27it/s]Epoch: 2, train for the 205-th batch, train loss: 0.48263826966285706:  85%|████████▌ | 205/241 [00:34<00:06,  5.38it/s]evaluate for the 29-th batch, evaluate loss: 0.7061402797698975:  70%|████████████▌     | 28/40 [00:02<00:00, 12.92it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5149332284927368:  39%|█████        | 92/237 [00:15<00:28,  5.07it/s]Epoch: 1, train for the 332-th batch, train loss: 0.4157639145851135:  86%|█████████▌ | 331/383 [01:40<00:25,  2.01it/s]evaluate for the 30-th batch, evaluate loss: 0.6616703867912292:  70%|████████████▌     | 28/40 [00:02<00:00, 12.92it/s]evaluate for the 30-th batch, evaluate loss: 0.6616703867912292:  75%|█████████████▌    | 30/40 [00:02<00:00, 13.59it/s]Epoch: 3, train for the 131-th batch, train loss: 0.46358755230903625:  86%|████████▌ | 130/151 [00:23<00:04,  5.24it/s]Epoch: 1, train for the 332-th batch, train loss: 0.4157639145851135:  87%|█████████▌ | 332/383 [01:40<00:22,  2.29it/s]Epoch: 3, train for the 131-th batch, train loss: 0.46358755230903625:  87%|████████▋ | 131/151 [00:23<00:03,  5.04it/s]evaluate for the 31-th batch, evaluate loss: 0.7126699686050415:  75%|█████████████▌    | 30/40 [00:02<00:00, 13.59it/s]Epoch: 2, train for the 206-th batch, train loss: 0.5460534691810608:  85%|█████████▎ | 205/241 [00:35<00:06,  5.38it/s]Epoch: 2, train for the 206-th batch, train loss: 0.5460534691810608:  85%|█████████▍ | 206/241 [00:35<00:06,  5.49it/s]Epoch: 2, train for the 93-th batch, train loss: 0.5374470949172974:  39%|█████        | 92/237 [00:16<00:28,  5.07it/s]evaluate for the 32-th batch, evaluate loss: 0.6650230288505554:  75%|█████████████▌    | 30/40 [00:02<00:00, 13.59it/s]evaluate for the 32-th batch, evaluate loss: 0.6650230288505554:  80%|██████████████▍   | 32/40 [00:02<00:00, 13.58it/s]Epoch: 2, train for the 93-th batch, train loss: 0.5374470949172974:  39%|█████        | 93/237 [00:16<00:28,  5.08it/s]Epoch: 4, train for the 62-th batch, train loss: 0.42920956015586853:  42%|█████       | 61/146 [00:09<00:12,  6.86it/s]Epoch: 4, train for the 62-th batch, train loss: 0.42920956015586853:  42%|█████       | 62/146 [00:09<00:17,  4.90it/s]evaluate for the 33-th batch, evaluate loss: 0.6250604391098022:  80%|██████████████▍   | 32/40 [00:02<00:00, 13.58it/s]Epoch: 3, train for the 132-th batch, train loss: 0.4987170994281769:  87%|█████████▌ | 131/151 [00:23<00:03,  5.04it/s]Epoch: 3, train for the 132-th batch, train loss: 0.4987170994281769:  87%|█████████▌ | 132/151 [00:23<00:03,  4.93it/s]Epoch: 2, train for the 207-th batch, train loss: 0.5123225450515747:  85%|█████████▍ | 206/241 [00:35<00:06,  5.49it/s]evaluate for the 34-th batch, evaluate loss: 0.6205820441246033:  80%|██████████████▍   | 32/40 [00:02<00:00, 13.58it/s]evaluate for the 34-th batch, evaluate loss: 0.6205820441246033:  85%|███████████████▎  | 34/40 [00:02<00:00, 14.49it/s]Epoch: 2, train for the 207-th batch, train loss: 0.5123225450515747:  86%|█████████▍ | 207/241 [00:35<00:06,  5.55it/s]Epoch: 1, train for the 333-th batch, train loss: 0.4269215166568756:  87%|█████████▌ | 332/383 [01:40<00:22,  2.29it/s]Epoch: 4, train for the 63-th batch, train loss: 0.4581836462020874:  42%|█████▌       | 62/146 [00:09<00:17,  4.90it/s]Epoch: 4, train for the 63-th batch, train loss: 0.4581836462020874:  43%|█████▌       | 63/146 [00:09<00:15,  5.39it/s]evaluate for the 35-th batch, evaluate loss: 0.738949179649353:  85%|████████████████▏  | 34/40 [00:02<00:00, 14.49it/s]Epoch: 2, train for the 94-th batch, train loss: 0.6116161346435547:  39%|█████        | 93/237 [00:16<00:28,  5.08it/s]Epoch: 1, train for the 333-th batch, train loss: 0.4269215166568756:  87%|█████████▌ | 333/383 [01:40<00:20,  2.47it/s]Epoch: 2, train for the 94-th batch, train loss: 0.6116161346435547:  40%|█████▏       | 94/237 [00:16<00:28,  5.00it/s]evaluate for the 36-th batch, evaluate loss: 0.6766212582588196:  85%|███████████████▎  | 34/40 [00:02<00:00, 14.49it/s]evaluate for the 36-th batch, evaluate loss: 0.6766212582588196:  90%|████████████████▏ | 36/40 [00:02<00:00, 15.09it/s]Epoch: 3, train for the 133-th batch, train loss: 0.46462035179138184:  87%|████████▋ | 132/151 [00:23<00:03,  4.93it/s]Epoch: 3, train for the 133-th batch, train loss: 0.46462035179138184:  88%|████████▊ | 133/151 [00:23<00:03,  5.14it/s]Epoch: 2, train for the 208-th batch, train loss: 0.509080708026886:  86%|██████████▎ | 207/241 [00:35<00:06,  5.55it/s]Epoch: 4, train for the 64-th batch, train loss: 0.5131275653839111:  43%|█████▌       | 63/146 [00:10<00:15,  5.39it/s]Epoch: 2, train for the 208-th batch, train loss: 0.509080708026886:  86%|██████████▎ | 208/241 [00:35<00:05,  5.59it/s]Epoch: 4, train for the 64-th batch, train loss: 0.5131275653839111:  44%|█████▋       | 64/146 [00:10<00:13,  5.88it/s]evaluate for the 37-th batch, evaluate loss: 0.6708984971046448:  90%|████████████████▏ | 36/40 [00:02<00:00, 15.09it/s]evaluate for the 38-th batch, evaluate loss: 0.6635538339614868:  90%|████████████████▏ | 36/40 [00:02<00:00, 15.09it/s]evaluate for the 38-th batch, evaluate loss: 0.6635538339614868:  95%|█████████████████ | 38/40 [00:02<00:00, 14.36it/s]Epoch: 2, train for the 95-th batch, train loss: 0.51414954662323:  40%|█████▉         | 94/237 [00:16<00:28,  5.00it/s]Epoch: 4, train for the 65-th batch, train loss: 0.5116503238677979:  44%|█████▋       | 64/146 [00:10<00:13,  5.88it/s]Epoch: 4, train for the 65-th batch, train loss: 0.5116503238677979:  45%|█████▊       | 65/146 [00:10<00:13,  6.18it/s]Epoch: 3, train for the 134-th batch, train loss: 0.4590771794319153:  88%|█████████▋ | 133/151 [00:24<00:03,  5.14it/s]Epoch: 2, train for the 95-th batch, train loss: 0.51414954662323:  40%|██████         | 95/237 [00:16<00:30,  4.60it/s]Epoch: 1, train for the 334-th batch, train loss: 0.43138861656188965:  87%|████████▋ | 333/383 [01:40<00:20,  2.47it/s]Epoch: 3, train for the 134-th batch, train loss: 0.4590771794319153:  89%|█████████▊ | 134/151 [00:24<00:03,  5.05it/s]Epoch: 2, train for the 209-th batch, train loss: 0.5420060753822327:  86%|█████████▍ | 208/241 [00:35<00:05,  5.59it/s]Epoch: 2, train for the 209-th batch, train loss: 0.5420060753822327:  87%|█████████▌ | 209/241 [00:35<00:05,  5.40it/s]Epoch: 4, train for the 66-th batch, train loss: 0.5233461856842041:  45%|█████▊       | 65/146 [00:10<00:13,  6.18it/s]Epoch: 1, train for the 334-th batch, train loss: 0.43138861656188965:  87%|████████▋ | 334/383 [01:40<00:18,  2.58it/s]Epoch: 4, train for the 66-th batch, train loss: 0.5233461856842041:  45%|█████▉       | 66/146 [00:10<00:11,  6.89it/s]evaluate for the 39-th batch, evaluate loss: 0.7097944617271423:  95%|█████████████████ | 38/40 [00:03<00:00, 14.36it/s]Epoch: 3, train for the 135-th batch, train loss: 0.4396582245826721:  89%|█████████▊ | 134/151 [00:24<00:03,  5.05it/s]Epoch: 3, train for the 135-th batch, train loss: 0.4396582245826721:  89%|█████████▊ | 135/151 [00:24<00:02,  5.35it/s]Epoch: 2, train for the 96-th batch, train loss: 0.5850386023521423:  40%|█████▏       | 95/237 [00:16<00:30,  4.60it/s]Epoch: 2, train for the 210-th batch, train loss: 0.3577917516231537:  87%|█████████▌ | 209/241 [00:35<00:05,  5.40it/s]evaluate for the 40-th batch, evaluate loss: 0.4536462426185608:  95%|█████████████████ | 38/40 [00:03<00:00, 14.36it/s]evaluate for the 40-th batch, evaluate loss: 0.4536462426185608: 100%|██████████████████| 40/40 [00:03<00:00, 11.39it/s]evaluate for the 40-th batch, evaluate loss: 0.4536462426185608: 100%|██████████████████| 40/40 [00:03<00:00, 12.79it/s]
Epoch: 2, train for the 96-th batch, train loss: 0.5850386023521423:  41%|█████▎       | 96/237 [00:16<00:29,  4.78it/s]Epoch: 2, train for the 210-th batch, train loss: 0.3577917516231537:  87%|█████████▌ | 210/241 [00:35<00:05,  5.66it/s]Epoch: 4, train for the 67-th batch, train loss: 0.5354976654052734:  45%|█████▉       | 66/146 [00:10<00:11,  6.89it/s]Epoch: 4, train for the 67-th batch, train loss: 0.5354976654052734:  46%|█████▉       | 67/146 [00:10<00:12,  6.49it/s]Epoch: 3, train for the 136-th batch, train loss: 0.5145967602729797:  89%|█████████▊ | 135/151 [00:24<00:02,  5.35it/s]Epoch: 3, train for the 136-th batch, train loss: 0.5145967602729797:  90%|█████████▉ | 136/151 [00:24<00:02,  5.55it/s]Epoch: 2, train for the 97-th batch, train loss: 0.5421878695487976:  41%|█████▎       | 96/237 [00:16<00:29,  4.78it/s]Epoch: 4, train for the 68-th batch, train loss: 0.4592588245868683:  46%|█████▉       | 67/146 [00:10<00:12,  6.49it/s]Epoch: 4, train for the 68-th batch, train loss: 0.4592588245868683:  47%|██████       | 68/146 [00:10<00:11,  6.95it/s]Epoch: 2, train for the 97-th batch, train loss: 0.5421878695487976:  41%|█████▎       | 97/237 [00:16<00:29,  4.79it/s]  0%|                                                                                            | 0/21 [00:00<?, ?it/s]Epoch: 2, train for the 211-th batch, train loss: 0.4064609408378601:  87%|█████████▌ | 210/241 [00:36<00:05,  5.66it/s]evaluate for the 1-th batch, evaluate loss: 1.035943865776062:   0%|                             | 0/21 [00:00<?, ?it/s]Epoch: 2, train for the 211-th batch, train loss: 0.4064609408378601:  88%|█████████▋ | 211/241 [00:36<00:06,  4.53it/s]Epoch: 3, train for the 137-th batch, train loss: 0.49429094791412354:  90%|█████████ | 136/151 [00:24<00:02,  5.55it/s]Epoch: 4, train for the 69-th batch, train loss: 0.5143573880195618:  47%|██████       | 68/146 [00:10<00:11,  6.95it/s]Epoch: 4, train for the 69-th batch, train loss: 0.5143573880195618:  47%|██████▏      | 69/146 [00:10<00:11,  6.83it/s]Epoch: 3, train for the 137-th batch, train loss: 0.49429094791412354:  91%|█████████ | 137/151 [00:24<00:02,  5.22it/s]evaluate for the 2-th batch, evaluate loss: 1.120373010635376:   0%|                             | 0/21 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 1.120373010635376:  10%|██                   | 2/21 [00:00<00:01, 14.47it/s]Epoch: 2, train for the 98-th batch, train loss: 0.5314136147499084:  41%|█████▎       | 97/237 [00:17<00:29,  4.79it/s]Epoch: 1, train for the 335-th batch, train loss: 0.47095707058906555:  87%|████████▋ | 334/383 [01:41<00:18,  2.58it/s]Epoch: 2, train for the 98-th batch, train loss: 0.5314136147499084:  41%|█████▍       | 98/237 [00:17<00:29,  4.79it/s]Epoch: 1, train for the 335-th batch, train loss: 0.47095707058906555:  87%|████████▋ | 335/383 [01:41<00:21,  2.26it/s]evaluate for the 3-th batch, evaluate loss: 1.0973248481750488:  10%|█▉                  | 2/21 [00:00<00:01, 14.47it/s]Epoch: 2, train for the 212-th batch, train loss: 0.4722845256328583:  88%|█████████▋ | 211/241 [00:36<00:06,  4.53it/s]Epoch: 2, train for the 212-th batch, train loss: 0.4722845256328583:  88%|█████████▋ | 212/241 [00:36<00:06,  4.83it/s]Epoch: 4, train for the 70-th batch, train loss: 0.505408525466919:  47%|██████▌       | 69/146 [00:10<00:11,  6.83it/s]Epoch: 4, train for the 70-th batch, train loss: 0.505408525466919:  48%|██████▋       | 70/146 [00:10<00:11,  6.48it/s]Epoch: 3, train for the 138-th batch, train loss: 0.5009506344795227:  91%|█████████▉ | 137/151 [00:24<00:02,  5.22it/s]Epoch: 3, train for the 138-th batch, train loss: 0.5009506344795227:  91%|██████████ | 138/151 [00:24<00:02,  5.29it/s]evaluate for the 4-th batch, evaluate loss: 0.9768427014350891:  10%|█▉                  | 2/21 [00:00<00:01, 14.47it/s]evaluate for the 4-th batch, evaluate loss: 0.9768427014350891:  19%|███▊                | 4/21 [00:00<00:01, 12.87it/s]Epoch: 2, train for the 99-th batch, train loss: 0.5762431025505066:  41%|█████▍       | 98/237 [00:17<00:29,  4.79it/s]Epoch: 2, train for the 99-th batch, train loss: 0.5762431025505066:  42%|█████▍       | 99/237 [00:17<00:28,  4.86it/s]Epoch: 2, train for the 213-th batch, train loss: 0.483296662569046:  88%|██████████▌ | 212/241 [00:36<00:06,  4.83it/s]evaluate for the 5-th batch, evaluate loss: 1.0196385383605957:  19%|███▊                | 4/21 [00:00<00:01, 12.87it/s]Epoch: 4, train for the 71-th batch, train loss: 0.579723596572876:  48%|██████▋       | 70/146 [00:11<00:11,  6.48it/s]Epoch: 2, train for the 213-th batch, train loss: 0.483296662569046:  88%|██████████▌ | 213/241 [00:36<00:05,  4.97it/s]Epoch: 4, train for the 71-th batch, train loss: 0.579723596572876:  49%|██████▊       | 71/146 [00:11<00:11,  6.27it/s]Epoch: 1, train for the 336-th batch, train loss: 0.4573430120944977:  87%|█████████▌ | 335/383 [01:41<00:21,  2.26it/s]Epoch: 3, train for the 139-th batch, train loss: 0.5030032992362976:  91%|██████████ | 138/151 [00:25<00:02,  5.29it/s]Epoch: 3, train for the 139-th batch, train loss: 0.5030032992362976:  92%|██████████▏| 139/151 [00:25<00:02,  5.15it/s]evaluate for the 6-th batch, evaluate loss: 0.985106885433197:  19%|████                 | 4/21 [00:00<00:01, 12.87it/s]evaluate for the 6-th batch, evaluate loss: 0.985106885433197:  29%|██████               | 6/21 [00:00<00:01, 11.28it/s]Epoch: 2, train for the 100-th batch, train loss: 0.539772629737854:  42%|█████▍       | 99/237 [00:17<00:28,  4.86it/s]Epoch: 1, train for the 336-th batch, train loss: 0.4573430120944977:  88%|█████████▋ | 336/383 [01:41<00:18,  2.49it/s]Epoch: 2, train for the 100-th batch, train loss: 0.539772629737854:  42%|█████       | 100/237 [00:17<00:26,  5.07it/s]Epoch: 2, train for the 214-th batch, train loss: 0.4413295388221741:  88%|█████████▋ | 213/241 [00:36<00:05,  4.97it/s]Epoch: 4, train for the 72-th batch, train loss: 0.5528785586357117:  49%|██████▎      | 71/146 [00:11<00:11,  6.27it/s]Epoch: 2, train for the 214-th batch, train loss: 0.4413295388221741:  89%|█████████▊ | 214/241 [00:36<00:05,  5.29it/s]Epoch: 4, train for the 72-th batch, train loss: 0.5528785586357117:  49%|██████▍      | 72/146 [00:11<00:11,  6.25it/s]evaluate for the 7-th batch, evaluate loss: 0.8532705307006836:  29%|█████▋              | 6/21 [00:00<00:01, 11.28it/s]evaluate for the 8-th batch, evaluate loss: 0.9017332196235657:  29%|█████▋              | 6/21 [00:00<00:01, 11.28it/s]evaluate for the 8-th batch, evaluate loss: 0.9017332196235657:  38%|███████▌            | 8/21 [00:00<00:01, 11.03it/s]Epoch: 3, train for the 140-th batch, train loss: 0.47401905059814453:  92%|█████████▏| 139/151 [00:25<00:02,  5.15it/s]Epoch: 2, train for the 101-th batch, train loss: 0.5629071593284607:  42%|████▋      | 100/237 [00:17<00:26,  5.07it/s]Epoch: 3, train for the 140-th batch, train loss: 0.47401905059814453:  93%|█████████▎| 140/151 [00:25<00:02,  4.91it/s]Epoch: 2, train for the 101-th batch, train loss: 0.5629071593284607:  43%|████▋      | 101/237 [00:17<00:26,  5.07it/s]Epoch: 4, train for the 73-th batch, train loss: 0.5046284794807434:  49%|██████▍      | 72/146 [00:11<00:11,  6.25it/s]Epoch: 4, train for the 73-th batch, train loss: 0.5046284794807434:  50%|██████▌      | 73/146 [00:11<00:11,  6.34it/s]evaluate for the 9-th batch, evaluate loss: 0.8226649761199951:  38%|███████▌            | 8/21 [00:00<00:01, 11.03it/s]Epoch: 2, train for the 215-th batch, train loss: 0.4369155764579773:  89%|█████████▊ | 214/241 [00:36<00:05,  5.29it/s]Epoch: 2, train for the 215-th batch, train loss: 0.4369155764579773:  89%|█████████▊ | 215/241 [00:36<00:04,  5.31it/s]evaluate for the 10-th batch, evaluate loss: 0.8491886854171753:  38%|███████▏           | 8/21 [00:00<00:01, 11.03it/s]evaluate for the 10-th batch, evaluate loss: 0.8491886854171753:  48%|████████▌         | 10/21 [00:00<00:00, 11.98it/s]Epoch: 1, train for the 337-th batch, train loss: 0.5291351675987244:  88%|█████████▋ | 336/383 [01:42<00:18,  2.49it/s]Epoch: 3, train for the 141-th batch, train loss: 0.4715149998664856:  93%|██████████▏| 140/151 [00:25<00:02,  4.91it/s]Epoch: 3, train for the 141-th batch, train loss: 0.4715149998664856:  93%|██████████▎| 141/151 [00:25<00:01,  5.05it/s]Epoch: 2, train for the 102-th batch, train loss: 0.593393087387085:  43%|█████       | 101/237 [00:17<00:26,  5.07it/s]Epoch: 1, train for the 337-th batch, train loss: 0.5291351675987244:  88%|█████████▋ | 337/383 [01:42<00:18,  2.52it/s]Epoch: 4, train for the 74-th batch, train loss: 0.5212574601173401:  50%|██████▌      | 73/146 [00:11<00:11,  6.34it/s]evaluate for the 11-th batch, evaluate loss: 0.7425954937934875:  48%|████████▌         | 10/21 [00:00<00:00, 11.98it/s]Epoch: 4, train for the 74-th batch, train loss: 0.5212574601173401:  51%|██████▌      | 74/146 [00:11<00:11,  6.07it/s]Epoch: 2, train for the 102-th batch, train loss: 0.593393087387085:  43%|█████▏      | 102/237 [00:17<00:26,  5.03it/s]Epoch: 2, train for the 216-th batch, train loss: 0.5129060745239258:  89%|█████████▊ | 215/241 [00:37<00:04,  5.31it/s]Epoch: 2, train for the 216-th batch, train loss: 0.5129060745239258:  90%|█████████▊ | 216/241 [00:37<00:04,  5.34it/s]evaluate for the 12-th batch, evaluate loss: 0.8071480393409729:  48%|████████▌         | 10/21 [00:01<00:00, 11.98it/s]evaluate for the 12-th batch, evaluate loss: 0.8071480393409729:  57%|██████████▎       | 12/21 [00:01<00:00, 11.88it/s]Epoch: 3, train for the 142-th batch, train loss: 0.4523596167564392:  93%|██████████▎| 141/151 [00:25<00:01,  5.05it/s]Epoch: 4, train for the 75-th batch, train loss: 0.520140528678894:  51%|███████       | 74/146 [00:11<00:11,  6.07it/s]Epoch: 3, train for the 142-th batch, train loss: 0.4523596167564392:  94%|██████████▎| 142/151 [00:25<00:01,  5.11it/s]Epoch: 4, train for the 75-th batch, train loss: 0.520140528678894:  51%|███████▏      | 75/146 [00:11<00:11,  6.04it/s]evaluate for the 13-th batch, evaluate loss: 0.7183260321617126:  57%|██████████▎       | 12/21 [00:01<00:00, 11.88it/s]Epoch: 2, train for the 103-th batch, train loss: 0.5861144065856934:  43%|████▋      | 102/237 [00:18<00:26,  5.03it/s]Epoch: 2, train for the 217-th batch, train loss: 0.4908396303653717:  90%|█████████▊ | 216/241 [00:37<00:04,  5.34it/s]Epoch: 2, train for the 103-th batch, train loss: 0.5861144065856934:  43%|████▊      | 103/237 [00:18<00:27,  4.79it/s]Epoch: 2, train for the 217-th batch, train loss: 0.4908396303653717:  90%|█████████▉ | 217/241 [00:37<00:04,  5.17it/s]evaluate for the 14-th batch, evaluate loss: 0.7012044191360474:  57%|██████████▎       | 12/21 [00:01<00:00, 11.88it/s]evaluate for the 14-th batch, evaluate loss: 0.7012044191360474:  67%|████████████      | 14/21 [00:01<00:00, 11.64it/s]Epoch: 1, train for the 338-th batch, train loss: 0.32548630237579346:  88%|████████▊ | 337/383 [01:42<00:18,  2.52it/s]Epoch: 4, train for the 76-th batch, train loss: 0.5013682246208191:  51%|██████▋      | 75/146 [00:11<00:11,  6.04it/s]Epoch: 4, train for the 76-th batch, train loss: 0.5013682246208191:  52%|██████▊      | 76/146 [00:11<00:10,  6.52it/s]evaluate for the 15-th batch, evaluate loss: 0.7454159259796143:  67%|████████████      | 14/21 [00:01<00:00, 11.64it/s]Epoch: 3, train for the 143-th batch, train loss: 0.42527148127555847:  94%|█████████▍| 142/151 [00:25<00:01,  5.11it/s]Epoch: 1, train for the 338-th batch, train loss: 0.32548630237579346:  88%|████████▊ | 338/383 [01:42<00:17,  2.58it/s]Epoch: 3, train for the 143-th batch, train loss: 0.42527148127555847:  95%|█████████▍| 143/151 [00:25<00:01,  5.16it/s]Epoch: 2, train for the 218-th batch, train loss: 0.4242233633995056:  90%|█████████▉ | 217/241 [00:37<00:04,  5.17it/s]evaluate for the 16-th batch, evaluate loss: 0.6849014759063721:  67%|████████████      | 14/21 [00:01<00:00, 11.64it/s]evaluate for the 16-th batch, evaluate loss: 0.6849014759063721:  76%|█████████████▋    | 16/21 [00:01<00:00, 12.10it/s]Epoch: 2, train for the 218-th batch, train loss: 0.4242233633995056:  90%|█████████▉ | 218/241 [00:37<00:04,  5.42it/s]Epoch: 2, train for the 104-th batch, train loss: 0.5485824942588806:  43%|████▊      | 103/237 [00:18<00:27,  4.79it/s]Epoch: 4, train for the 77-th batch, train loss: 0.5581086874008179:  52%|██████▊      | 76/146 [00:12<00:10,  6.52it/s]Epoch: 2, train for the 104-th batch, train loss: 0.5485824942588806:  44%|████▊      | 104/237 [00:18<00:27,  4.79it/s]Epoch: 4, train for the 77-th batch, train loss: 0.5581086874008179:  53%|██████▊      | 77/146 [00:12<00:10,  6.54it/s]evaluate for the 17-th batch, evaluate loss: 0.6293144822120667:  76%|█████████████▋    | 16/21 [00:01<00:00, 12.10it/s]Epoch: 3, train for the 144-th batch, train loss: 0.3991001844406128:  95%|██████████▍| 143/151 [00:26<00:01,  5.16it/s]Epoch: 3, train for the 144-th batch, train loss: 0.3991001844406128:  95%|██████████▍| 144/151 [00:26<00:01,  5.38it/s]Epoch: 2, train for the 219-th batch, train loss: 0.4460766911506653:  90%|█████████▉ | 218/241 [00:37<00:04,  5.42it/s]evaluate for the 18-th batch, evaluate loss: 0.6587884426116943:  76%|█████████████▋    | 16/21 [00:01<00:00, 12.10it/s]evaluate for the 18-th batch, evaluate loss: 0.6587884426116943:  86%|███████████████▍  | 18/21 [00:01<00:00, 12.54it/s]Epoch: 2, train for the 219-th batch, train loss: 0.4460766911506653:  91%|█████████▉ | 219/241 [00:37<00:03,  5.63it/s]Epoch: 4, train for the 78-th batch, train loss: 0.5152673721313477:  53%|██████▊      | 77/146 [00:12<00:10,  6.54it/s]Epoch: 4, train for the 78-th batch, train loss: 0.5152673721313477:  53%|██████▉      | 78/146 [00:12<00:10,  6.50it/s]evaluate for the 19-th batch, evaluate loss: 0.584291934967041:  86%|████████████████▎  | 18/21 [00:01<00:00, 12.54it/s]Epoch: 1, train for the 339-th batch, train loss: 0.4311157464981079:  88%|█████████▋ | 338/383 [01:42<00:17,  2.58it/s]Epoch: 2, train for the 105-th batch, train loss: 0.5349122285842896:  44%|████▊      | 104/237 [00:18<00:27,  4.79it/s]Epoch: 2, train for the 105-th batch, train loss: 0.5349122285842896:  44%|████▊      | 105/237 [00:18<00:28,  4.69it/s]Epoch: 3, train for the 145-th batch, train loss: 0.4216134548187256:  95%|██████████▍| 144/151 [00:26<00:01,  5.38it/s]evaluate for the 20-th batch, evaluate loss: 0.5453726649284363:  86%|███████████████▍  | 18/21 [00:01<00:00, 12.54it/s]evaluate for the 20-th batch, evaluate loss: 0.5453726649284363:  95%|█████████████████▏| 20/21 [00:01<00:00, 13.26it/s]Epoch: 1, train for the 339-th batch, train loss: 0.4311157464981079:  89%|█████████▋ | 339/383 [01:42<00:16,  2.67it/s]Epoch: 3, train for the 145-th batch, train loss: 0.4216134548187256:  96%|██████████▌| 145/151 [00:26<00:01,  5.35it/s]Epoch: 2, train for the 220-th batch, train loss: 0.5408312678337097:  91%|█████████▉ | 219/241 [00:37<00:03,  5.63it/s]Epoch: 2, train for the 220-th batch, train loss: 0.5408312678337097:  91%|██████████ | 220/241 [00:37<00:03,  5.64it/s]Epoch: 4, train for the 79-th batch, train loss: 0.5275997519493103:  53%|██████▉      | 78/146 [00:12<00:10,  6.50it/s]evaluate for the 21-th batch, evaluate loss: 0.6275357604026794:  95%|█████████████████▏| 20/21 [00:01<00:00, 13.26it/s]evaluate for the 21-th batch, evaluate loss: 0.6275357604026794: 100%|██████████████████| 21/21 [00:01<00:00, 12.39it/s]
Epoch: 4, train for the 79-th batch, train loss: 0.5275997519493103:  54%|███████      | 79/146 [00:12<00:10,  6.26it/s]Epoch: 2, train for the 106-th batch, train loss: 0.5722766518592834:  44%|████▊      | 105/237 [00:18<00:28,  4.69it/s]Epoch: 2, train for the 106-th batch, train loss: 0.5722766518592834:  45%|████▉      | 106/237 [00:18<00:27,  4.78it/s]Epoch: 1, train for the 340-th batch, train loss: 0.4120232164859772:  89%|█████████▋ | 339/383 [01:43<00:16,  2.67it/s]Epoch: 3, train for the 146-th batch, train loss: 0.4138847887516022:  96%|██████████▌| 145/151 [00:26<00:01,  5.35it/s]INFO:root:Epoch: 4, learning rate: 0.0001, train loss: 0.4358
INFO:root:train average_precision, 0.8957
INFO:root:train roc_auc, 0.8888
INFO:root:validate loss: 0.6748
INFO:root:validate average_precision, 0.7066
INFO:root:validate roc_auc, 0.7601
INFO:root:new node validate loss: 0.8146
INFO:root:new node validate first_1_average_precision, 0.8219
INFO:root:new node validate first_1_roc_auc, 0.8330
INFO:root:new node validate first_3_average_precision, 0.7709
INFO:root:new node validate first_3_roc_auc, 0.7612
INFO:root:new node validate first_10_average_precision, 0.7105
INFO:root:new node validate first_10_roc_auc, 0.7066
INFO:root:new node validate average_precision, 0.6744
INFO:root:new node validate roc_auc, 0.6941
Epoch: 2, train for the 221-th batch, train loss: 0.5206632018089294:  91%|██████████ | 220/241 [00:37<00:03,  5.64it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]Epoch: 3, train for the 146-th batch, train loss: 0.4138847887516022:  97%|██████████▋| 146/151 [00:26<00:00,  5.02it/s]Epoch: 4, train for the 80-th batch, train loss: 0.555273175239563:  54%|███████▌      | 79/146 [00:12<00:10,  6.26it/s]Epoch: 4, train for the 80-th batch, train loss: 0.555273175239563:  55%|███████▋      | 80/146 [00:12<00:10,  6.09it/s]Epoch: 2, train for the 221-th batch, train loss: 0.5206632018089294:  92%|██████████ | 221/241 [00:37<00:03,  5.40it/s]Epoch: 1, train for the 340-th batch, train loss: 0.4120232164859772:  89%|█████████▊ | 340/383 [01:43<00:14,  2.90it/s]Epoch: 2, train for the 107-th batch, train loss: 0.6046767830848694:  45%|████▉      | 106/237 [00:18<00:27,  4.78it/s]Epoch: 2, train for the 107-th batch, train loss: 0.6046767830848694:  45%|████▉      | 107/237 [00:18<00:26,  4.96it/s]Epoch: 5, train for the 1-th batch, train loss: 1.031800389289856:   0%|                        | 0/119 [00:00<?, ?it/s]Epoch: 5, train for the 1-th batch, train loss: 1.031800389289856:   1%|▏               | 1/119 [00:00<00:19,  6.13it/s]Epoch: 4, train for the 81-th batch, train loss: 0.4676434099674225:  55%|███████      | 80/146 [00:12<00:10,  6.09it/s]Epoch: 3, train for the 147-th batch, train loss: 0.4005136787891388:  97%|██████████▋| 146/151 [00:26<00:00,  5.02it/s]Epoch: 4, train for the 81-th batch, train loss: 0.4676434099674225:  55%|███████▏     | 81/146 [00:12<00:11,  5.81it/s]Epoch: 2, train for the 222-th batch, train loss: 0.45917049050331116:  92%|█████████▏| 221/241 [00:38<00:03,  5.40it/s]Epoch: 3, train for the 147-th batch, train loss: 0.4005136787891388:  97%|██████████▋| 147/151 [00:26<00:00,  4.91it/s]Epoch: 2, train for the 222-th batch, train loss: 0.45917049050331116:  92%|█████████▏| 222/241 [00:38<00:03,  5.13it/s]Epoch: 5, train for the 2-th batch, train loss: 1.027725338935852:   1%|▏               | 1/119 [00:00<00:19,  6.13it/s]Epoch: 5, train for the 2-th batch, train loss: 1.027725338935852:   2%|▎               | 2/119 [00:00<00:17,  6.77it/s]Epoch: 1, train for the 341-th batch, train loss: 0.3908941447734833:  89%|█████████▊ | 340/383 [01:43<00:14,  2.90it/s]Epoch: 2, train for the 108-th batch, train loss: 0.6151177883148193:  45%|████▉      | 107/237 [00:19<00:26,  4.96it/s]Epoch: 2, train for the 108-th batch, train loss: 0.6151177883148193:  46%|█████      | 108/237 [00:19<00:27,  4.65it/s]Epoch: 1, train for the 341-th batch, train loss: 0.3908941447734833:  89%|█████████▊ | 341/383 [01:43<00:14,  2.86it/s]Epoch: 3, train for the 148-th batch, train loss: 0.4233068823814392:  97%|██████████▋| 147/151 [00:26<00:00,  4.91it/s]Epoch: 2, train for the 223-th batch, train loss: 0.44613927602767944:  92%|█████████▏| 222/241 [00:38<00:03,  5.13it/s]Epoch: 5, train for the 3-th batch, train loss: 0.8320149183273315:   2%|▎              | 2/119 [00:00<00:17,  6.77it/s]Epoch: 3, train for the 148-th batch, train loss: 0.4233068823814392:  98%|██████████▊| 148/151 [00:26<00:00,  4.98it/s]Epoch: 5, train for the 3-th batch, train loss: 0.8320149183273315:   3%|▍              | 3/119 [00:00<00:15,  7.46it/s]Epoch: 2, train for the 223-th batch, train loss: 0.44613927602767944:  93%|█████████▎| 223/241 [00:38<00:03,  5.19it/s]Epoch: 4, train for the 82-th batch, train loss: 0.515715479850769:  55%|███████▊      | 81/146 [00:12<00:11,  5.81it/s]Epoch: 4, train for the 82-th batch, train loss: 0.515715479850769:  56%|███████▊      | 82/146 [00:13<00:12,  5.07it/s]Epoch: 2, train for the 109-th batch, train loss: 0.6067732572555542:  46%|█████      | 108/237 [00:19<00:27,  4.65it/s]Epoch: 2, train for the 109-th batch, train loss: 0.6067732572555542:  46%|█████      | 109/237 [00:19<00:27,  4.68it/s]Epoch: 4, train for the 83-th batch, train loss: 0.4766630530357361:  56%|███████▎     | 82/146 [00:13<00:12,  5.07it/s]Epoch: 4, train for the 83-th batch, train loss: 0.4766630530357361:  57%|███████▍     | 83/146 [00:13<00:11,  5.52it/s]Epoch: 2, train for the 224-th batch, train loss: 0.454874187707901:  93%|███████████ | 223/241 [00:38<00:03,  5.19it/s]Epoch: 2, train for the 224-th batch, train loss: 0.454874187707901:  93%|███████████▏| 224/241 [00:38<00:03,  5.00it/s]Epoch: 1, train for the 342-th batch, train loss: 0.3837129771709442:  89%|█████████▊ | 341/383 [01:43<00:14,  2.86it/s]Epoch: 3, train for the 149-th batch, train loss: 0.3555397093296051:  98%|██████████▊| 148/151 [00:27<00:00,  4.98it/s]Epoch: 5, train for the 4-th batch, train loss: 0.7432835698127747:   3%|▍              | 3/119 [00:00<00:15,  7.46it/s]Epoch: 3, train for the 149-th batch, train loss: 0.3555397093296051:  99%|██████████▊| 149/151 [00:27<00:00,  4.49it/s]Epoch: 5, train for the 4-th batch, train loss: 0.7432835698127747:   3%|▌              | 4/119 [00:00<00:21,  5.31it/s]Epoch: 1, train for the 342-th batch, train loss: 0.3837129771709442:  89%|█████████▊ | 342/383 [01:43<00:13,  2.93it/s]Epoch: 2, train for the 110-th batch, train loss: 0.5581299662590027:  46%|█████      | 109/237 [00:19<00:27,  4.68it/s]Epoch: 4, train for the 84-th batch, train loss: 0.46432235836982727:  57%|██████▊     | 83/146 [00:13<00:11,  5.52it/s]Epoch: 2, train for the 110-th batch, train loss: 0.5581299662590027:  46%|█████      | 110/237 [00:19<00:25,  4.90it/s]Epoch: 4, train for the 84-th batch, train loss: 0.46432235836982727:  58%|██████▉     | 84/146 [00:13<00:10,  5.74it/s]Epoch: 2, train for the 225-th batch, train loss: 0.4787246584892273:  93%|██████████▏| 224/241 [00:38<00:03,  5.00it/s]Epoch: 2, train for the 225-th batch, train loss: 0.4787246584892273:  93%|██████████▎| 225/241 [00:38<00:02,  5.43it/s]Epoch: 5, train for the 5-th batch, train loss: 0.5947805643081665:   3%|▌              | 4/119 [00:00<00:21,  5.31it/s]Epoch: 5, train for the 5-th batch, train loss: 0.5947805643081665:   4%|▋              | 5/119 [00:00<00:19,  5.78it/s]Epoch: 3, train for the 150-th batch, train loss: 0.45448026061058044:  99%|█████████▊| 149/151 [00:27<00:00,  4.49it/s]Epoch: 3, train for the 150-th batch, train loss: 0.45448026061058044:  99%|█████████▉| 150/151 [00:27<00:00,  4.61it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5955429673194885:  46%|█████      | 110/237 [00:19<00:25,  4.90it/s]Epoch: 2, train for the 226-th batch, train loss: 0.5270318984985352:  93%|██████████▎| 225/241 [00:38<00:02,  5.43it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5955429673194885:  47%|█████▏     | 111/237 [00:19<00:24,  5.13it/s]Epoch: 4, train for the 85-th batch, train loss: 0.5085209012031555:  58%|███████▍     | 84/146 [00:13<00:10,  5.74it/s]Epoch: 2, train for the 226-th batch, train loss: 0.5270318984985352:  94%|██████████▎| 226/241 [00:38<00:02,  5.66it/s]Epoch: 4, train for the 85-th batch, train loss: 0.5085209012031555:  58%|███████▌     | 85/146 [00:13<00:10,  5.57it/s]Epoch: 5, train for the 6-th batch, train loss: 0.48889386653900146:   4%|▌             | 5/119 [00:00<00:19,  5.78it/s]Epoch: 5, train for the 6-th batch, train loss: 0.48889386653900146:   5%|▋             | 6/119 [00:00<00:18,  6.12it/s]Epoch: 3, train for the 151-th batch, train loss: 0.46229952573776245:  99%|█████████▉| 150/151 [00:27<00:00,  4.61it/s]Epoch: 3, train for the 151-th batch, train loss: 0.46229952573776245: 100%|██████████| 151/151 [00:27<00:00,  5.02it/s]Epoch: 3, train for the 151-th batch, train loss: 0.46229952573776245: 100%|██████████| 151/151 [00:27<00:00,  5.49it/s]
Epoch: 2, train for the 112-th batch, train loss: 0.5778661966323853:  47%|█████▏     | 111/237 [00:19<00:24,  5.13it/s]Epoch: 2, train for the 112-th batch, train loss: 0.5778661966323853:  47%|█████▏     | 112/237 [00:19<00:23,  5.28it/s]Epoch: 4, train for the 86-th batch, train loss: 0.49137741327285767:  58%|██████▉     | 85/146 [00:13<00:10,  5.57it/s]Epoch: 4, train for the 86-th batch, train loss: 0.49137741327285767:  59%|███████     | 86/146 [00:13<00:10,  5.60it/s]Epoch: 5, train for the 7-th batch, train loss: 0.40557417273521423:   5%|▋             | 6/119 [00:01<00:18,  6.12it/s]Epoch: 5, train for the 7-th batch, train loss: 0.40557417273521423:   6%|▊             | 7/119 [00:01<00:18,  6.17it/s]Epoch: 2, train for the 227-th batch, train loss: 0.46013739705085754:  94%|█████████▍| 226/241 [00:39<00:02,  5.66it/s]Epoch: 2, train for the 227-th batch, train loss: 0.46013739705085754:  94%|█████████▍| 227/241 [00:39<00:02,  5.30it/s]  0%|                                                                                            | 0/46 [00:00<?, ?it/s]Epoch: 1, train for the 343-th batch, train loss: 0.37888869643211365:  89%|████████▉ | 342/383 [01:44<00:13,  2.93it/s]evaluate for the 1-th batch, evaluate loss: 0.6914616823196411:   0%|                            | 0/46 [00:00<?, ?it/s]Epoch: 2, train for the 113-th batch, train loss: 0.602543830871582:  47%|█████▋      | 112/237 [00:20<00:23,  5.28it/s]Epoch: 1, train for the 343-th batch, train loss: 0.37888869643211365:  90%|████████▉ | 343/383 [01:44<00:16,  2.47it/s]Epoch: 2, train for the 113-th batch, train loss: 0.602543830871582:  48%|█████▋      | 113/237 [00:20<00:22,  5.40it/s]Epoch: 2, train for the 228-th batch, train loss: 0.4234670102596283:  94%|██████████▎| 227/241 [00:39<00:02,  5.30it/s]evaluate for the 2-th batch, evaluate loss: 0.6660563349723816:   0%|                            | 0/46 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.6660563349723816:   4%|▊                   | 2/46 [00:00<00:03, 13.66it/s]Epoch: 2, train for the 228-th batch, train loss: 0.4234670102596283:  95%|██████████▍| 228/241 [00:39<00:02,  5.48it/s]Epoch: 5, train for the 8-th batch, train loss: 0.33101776242256165:   6%|▊             | 7/119 [00:01<00:18,  6.17it/s]Epoch: 4, train for the 87-th batch, train loss: 0.5026828050613403:  59%|███████▋     | 86/146 [00:13<00:10,  5.60it/s]Epoch: 5, train for the 8-th batch, train loss: 0.33101776242256165:   7%|▉             | 8/119 [00:01<00:19,  5.74it/s]Epoch: 4, train for the 87-th batch, train loss: 0.5026828050613403:  60%|███████▋     | 87/146 [00:13<00:11,  5.32it/s]evaluate for the 3-th batch, evaluate loss: 0.703387975692749:   4%|▉                    | 2/46 [00:00<00:03, 13.66it/s]Epoch: 2, train for the 114-th batch, train loss: 0.5703612565994263:  48%|█████▏     | 113/237 [00:20<00:22,  5.40it/s]Epoch: 2, train for the 114-th batch, train loss: 0.5703612565994263:  48%|█████▎     | 114/237 [00:20<00:24,  5.02it/s]Epoch: 2, train for the 229-th batch, train loss: 0.403056263923645:  95%|███████████▎| 228/241 [00:39<00:02,  5.48it/s]evaluate for the 4-th batch, evaluate loss: 0.6358916163444519:   4%|▊                   | 2/46 [00:00<00:03, 13.66it/s]evaluate for the 4-th batch, evaluate loss: 0.6358916163444519:   9%|█▋                  | 4/46 [00:00<00:04, 10.33it/s]Epoch: 5, train for the 9-th batch, train loss: 0.37484830617904663:   7%|▉             | 8/119 [00:01<00:19,  5.74it/s]Epoch: 4, train for the 88-th batch, train loss: 0.5419209003448486:  60%|███████▋     | 87/146 [00:14<00:11,  5.32it/s]Epoch: 5, train for the 9-th batch, train loss: 0.37484830617904663:   8%|█             | 9/119 [00:01<00:21,  5.17it/s]Epoch: 2, train for the 229-th batch, train loss: 0.403056263923645:  95%|███████████▍| 229/241 [00:39<00:02,  4.98it/s]Epoch: 4, train for the 88-th batch, train loss: 0.5419209003448486:  60%|███████▊     | 88/146 [00:14<00:11,  4.95it/s]evaluate for the 5-th batch, evaluate loss: 0.6585434079170227:   9%|█▋                  | 4/46 [00:00<00:04, 10.33it/s]Epoch: 2, train for the 115-th batch, train loss: 0.559111475944519:  48%|█████▊      | 114/237 [00:20<00:24,  5.02it/s]Epoch: 1, train for the 344-th batch, train loss: 0.4302385151386261:  90%|█████████▊ | 343/383 [01:44<00:16,  2.47it/s]Epoch: 2, train for the 115-th batch, train loss: 0.559111475944519:  49%|█████▊      | 115/237 [00:20<00:23,  5.09it/s]Epoch: 5, train for the 10-th batch, train loss: 0.3789028823375702:   8%|█             | 9/119 [00:01<00:21,  5.17it/s]Epoch: 5, train for the 10-th batch, train loss: 0.3789028823375702:   8%|█            | 10/119 [00:01<00:20,  5.39it/s]Epoch: 2, train for the 230-th batch, train loss: 0.44346460700035095:  95%|█████████▌| 229/241 [00:39<00:02,  4.98it/s]evaluate for the 6-th batch, evaluate loss: 0.6187390089035034:   9%|█▋                  | 4/46 [00:00<00:04, 10.33it/s]evaluate for the 6-th batch, evaluate loss: 0.6187390089035034:  13%|██▌                 | 6/46 [00:00<00:03, 10.04it/s]Epoch: 1, train for the 344-th batch, train loss: 0.4302385151386261:  90%|█████████▉ | 344/383 [01:44<00:16,  2.32it/s]Epoch: 4, train for the 89-th batch, train loss: 0.547796905040741:  60%|████████▍     | 88/146 [00:14<00:11,  4.95it/s]Epoch: 2, train for the 230-th batch, train loss: 0.44346460700035095:  95%|█████████▌| 230/241 [00:39<00:02,  5.03it/s]Epoch: 4, train for the 89-th batch, train loss: 0.547796905040741:  61%|████████▌     | 89/146 [00:14<00:11,  4.98it/s]Epoch: 2, train for the 116-th batch, train loss: 0.5925281643867493:  49%|█████▎     | 115/237 [00:20<00:23,  5.09it/s]evaluate for the 7-th batch, evaluate loss: 0.6865174174308777:  13%|██▌                 | 6/46 [00:00<00:03, 10.04it/s]Epoch: 2, train for the 116-th batch, train loss: 0.5925281643867493:  49%|█████▍     | 116/237 [00:20<00:22,  5.31it/s]Epoch: 5, train for the 11-th batch, train loss: 0.41113439202308655:   8%|█           | 10/119 [00:01<00:20,  5.39it/s]Epoch: 2, train for the 231-th batch, train loss: 0.4666110873222351:  95%|██████████▍| 230/241 [00:39<00:02,  5.03it/s]Epoch: 5, train for the 11-th batch, train loss: 0.41113439202308655:   9%|█           | 11/119 [00:01<00:19,  5.51it/s]evaluate for the 8-th batch, evaluate loss: 0.6875926852226257:  13%|██▌                 | 6/46 [00:00<00:03, 10.04it/s]evaluate for the 8-th batch, evaluate loss: 0.6875926852226257:  17%|███▍                | 8/46 [00:00<00:03, 10.66it/s]Epoch: 2, train for the 231-th batch, train loss: 0.4666110873222351:  96%|██████████▌| 231/241 [00:39<00:01,  5.25it/s]Epoch: 4, train for the 90-th batch, train loss: 0.5148550868034363:  61%|███████▉     | 89/146 [00:14<00:11,  4.98it/s]Epoch: 4, train for the 90-th batch, train loss: 0.5148550868034363:  62%|████████     | 90/146 [00:14<00:11,  5.07it/s]evaluate for the 9-th batch, evaluate loss: 0.6520317196846008:  17%|███▍                | 8/46 [00:00<00:03, 10.66it/s]Epoch: 1, train for the 345-th batch, train loss: 0.40841513872146606:  90%|████████▉ | 344/383 [01:45<00:16,  2.32it/s]Epoch: 2, train for the 117-th batch, train loss: 0.5534671545028687:  49%|█████▍     | 116/237 [00:20<00:22,  5.31it/s]Epoch: 5, train for the 12-th batch, train loss: 0.420871764421463:   9%|█▎            | 11/119 [00:02<00:19,  5.51it/s]Epoch: 5, train for the 12-th batch, train loss: 0.420871764421463:  10%|█▍            | 12/119 [00:02<00:18,  5.77it/s]Epoch: 2, train for the 117-th batch, train loss: 0.5534671545028687:  49%|█████▍     | 117/237 [00:20<00:23,  5.09it/s]Epoch: 1, train for the 345-th batch, train loss: 0.40841513872146606:  90%|█████████ | 345/383 [01:45<00:15,  2.48it/s]Epoch: 4, train for the 91-th batch, train loss: 0.4945782423019409:  62%|████████     | 90/146 [00:14<00:11,  5.07it/s]Epoch: 4, train for the 91-th batch, train loss: 0.4945782423019409:  62%|████████     | 91/146 [00:14<00:10,  5.48it/s]Epoch: 2, train for the 232-th batch, train loss: 0.42341348528862:  96%|████████████▍| 231/241 [00:40<00:01,  5.25it/s]evaluate for the 10-th batch, evaluate loss: 0.6077423095703125:  17%|███▎               | 8/46 [00:00<00:03, 10.66it/s]evaluate for the 10-th batch, evaluate loss: 0.6077423095703125:  22%|███▉              | 10/46 [00:00<00:03, 10.48it/s]Epoch: 2, train for the 232-th batch, train loss: 0.42341348528862:  96%|████████████▌| 232/241 [00:40<00:01,  5.17it/s]Epoch: 5, train for the 13-th batch, train loss: 0.42440265417099:  10%|█▌             | 12/119 [00:02<00:18,  5.77it/s]Epoch: 5, train for the 13-th batch, train loss: 0.42440265417099:  11%|█▋             | 13/119 [00:02<00:17,  6.18it/s]evaluate for the 11-th batch, evaluate loss: 0.7034466862678528:  22%|███▉              | 10/46 [00:01<00:03, 10.48it/s]Epoch: 4, train for the 92-th batch, train loss: 0.5503688454627991:  62%|████████     | 91/146 [00:14<00:10,  5.48it/s]Epoch: 4, train for the 92-th batch, train loss: 0.5503688454627991:  63%|████████▏    | 92/146 [00:14<00:09,  5.57it/s]Epoch: 2, train for the 118-th batch, train loss: 0.5157480239868164:  49%|█████▍     | 117/237 [00:21<00:23,  5.09it/s]Epoch: 2, train for the 118-th batch, train loss: 0.5157480239868164:  50%|█████▍     | 118/237 [00:21<00:24,  4.90it/s]Epoch: 2, train for the 233-th batch, train loss: 0.43477943539619446:  96%|█████████▋| 232/241 [00:40<00:01,  5.17it/s]evaluate for the 12-th batch, evaluate loss: 0.6182928681373596:  22%|███▉              | 10/46 [00:01<00:03, 10.48it/s]evaluate for the 12-th batch, evaluate loss: 0.6182928681373596:  26%|████▋             | 12/46 [00:01<00:03, 10.20it/s]Epoch: 5, train for the 14-th batch, train loss: 0.4465186297893524:  11%|█▍           | 13/119 [00:02<00:17,  6.18it/s]Epoch: 2, train for the 233-th batch, train loss: 0.43477943539619446:  97%|█████████▋| 233/241 [00:40<00:01,  5.07it/s]Epoch: 5, train for the 14-th batch, train loss: 0.4465186297893524:  12%|█▌           | 14/119 [00:02<00:16,  6.35it/s]Epoch: 4, train for the 93-th batch, train loss: 0.4760255217552185:  63%|████████▏    | 92/146 [00:14<00:09,  5.57it/s]Epoch: 1, train for the 346-th batch, train loss: 0.41944342851638794:  90%|█████████ | 345/383 [01:45<00:15,  2.48it/s]evaluate for the 13-th batch, evaluate loss: 0.662819504737854:  26%|████▉              | 12/46 [00:01<00:03, 10.20it/s]Epoch: 4, train for the 93-th batch, train loss: 0.4760255217552185:  64%|████████▎    | 93/146 [00:14<00:08,  6.11it/s]Epoch: 2, train for the 119-th batch, train loss: 0.5678415298461914:  50%|█████▍     | 118/237 [00:21<00:24,  4.90it/s]Epoch: 1, train for the 346-th batch, train loss: 0.41944342851638794:  90%|█████████ | 346/383 [01:45<00:14,  2.53it/s]Epoch: 2, train for the 119-th batch, train loss: 0.5678415298461914:  50%|█████▌     | 119/237 [00:21<00:22,  5.15it/s]Epoch: 5, train for the 15-th batch, train loss: 0.4457719624042511:  12%|█▌           | 14/119 [00:02<00:16,  6.35it/s]Epoch: 5, train for the 15-th batch, train loss: 0.4457719624042511:  13%|█▋           | 15/119 [00:02<00:16,  6.40it/s]Epoch: 2, train for the 234-th batch, train loss: 0.5729106664657593:  97%|██████████▋| 233/241 [00:40<00:01,  5.07it/s]evaluate for the 14-th batch, evaluate loss: 0.6759746670722961:  26%|████▋             | 12/46 [00:01<00:03, 10.20it/s]evaluate for the 14-th batch, evaluate loss: 0.6759746670722961:  30%|█████▍            | 14/46 [00:01<00:03, 10.30it/s]Epoch: 2, train for the 234-th batch, train loss: 0.5729106664657593:  97%|██████████▋| 234/241 [00:40<00:01,  5.12it/s]Epoch: 4, train for the 94-th batch, train loss: 0.5108919143676758:  64%|████████▎    | 93/146 [00:15<00:08,  6.11it/s]Epoch: 4, train for the 94-th batch, train loss: 0.5108919143676758:  64%|████████▎    | 94/146 [00:15<00:08,  6.42it/s]evaluate for the 15-th batch, evaluate loss: 0.6323860287666321:  30%|█████▍            | 14/46 [00:01<00:03, 10.30it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5928490161895752:  50%|█████▌     | 119/237 [00:21<00:22,  5.15it/s]Epoch: 5, train for the 16-th batch, train loss: 0.4466615915298462:  13%|█▋           | 15/119 [00:02<00:16,  6.40it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5928490161895752:  51%|█████▌     | 120/237 [00:21<00:22,  5.24it/s]Epoch: 5, train for the 16-th batch, train loss: 0.4466615915298462:  13%|█▋           | 16/119 [00:02<00:16,  6.34it/s]Epoch: 4, train for the 95-th batch, train loss: 0.4909403622150421:  64%|████████▎    | 94/146 [00:15<00:08,  6.42it/s]evaluate for the 16-th batch, evaluate loss: 0.6337711811065674:  30%|█████▍            | 14/46 [00:01<00:03, 10.30it/s]evaluate for the 16-th batch, evaluate loss: 0.6337711811065674:  35%|██████▎           | 16/46 [00:01<00:02, 10.23it/s]Epoch: 4, train for the 95-th batch, train loss: 0.4909403622150421:  65%|████████▍    | 95/146 [00:15<00:08,  6.26it/s]Epoch: 2, train for the 235-th batch, train loss: 0.6120153069496155:  97%|██████████▋| 234/241 [00:40<00:01,  5.12it/s]Epoch: 2, train for the 235-th batch, train loss: 0.6120153069496155:  98%|██████████▋| 235/241 [00:40<00:01,  5.01it/s]Epoch: 5, train for the 17-th batch, train loss: 0.4514269530773163:  13%|█▋           | 16/119 [00:02<00:16,  6.34it/s]Epoch: 5, train for the 17-th batch, train loss: 0.4514269530773163:  14%|█▊           | 17/119 [00:02<00:15,  6.77it/s]evaluate for the 17-th batch, evaluate loss: 0.5048749446868896:  35%|██████▎           | 16/46 [00:01<00:02, 10.23it/s]Epoch: 2, train for the 121-th batch, train loss: 0.5957415103912354:  51%|█████▌     | 120/237 [00:21<00:22,  5.24it/s]Epoch: 2, train for the 121-th batch, train loss: 0.5957415103912354:  51%|█████▌     | 121/237 [00:21<00:20,  5.55it/s]Epoch: 4, train for the 96-th batch, train loss: 0.49918845295906067:  65%|███████▊    | 95/146 [00:15<00:08,  6.26it/s]Epoch: 4, train for the 96-th batch, train loss: 0.49918845295906067:  66%|███████▉    | 96/146 [00:15<00:07,  6.44it/s]Epoch: 2, train for the 236-th batch, train loss: 0.502417802810669:  98%|███████████▋| 235/241 [00:40<00:01,  5.01it/s]Epoch: 2, train for the 236-th batch, train loss: 0.502417802810669:  98%|███████████▊| 236/241 [00:40<00:00,  5.47it/s]Epoch: 5, train for the 18-th batch, train loss: 0.4574378728866577:  14%|█▊           | 17/119 [00:02<00:15,  6.77it/s]Epoch: 5, train for the 18-th batch, train loss: 0.4574378728866577:  15%|█▉           | 18/119 [00:02<00:14,  7.04it/s]Epoch: 1, train for the 347-th batch, train loss: 0.3120719790458679:  90%|█████████▉ | 346/383 [01:46<00:14,  2.53it/s]Epoch: 4, train for the 97-th batch, train loss: 0.5197396278381348:  66%|████████▌    | 96/146 [00:15<00:07,  6.44it/s]Epoch: 2, train for the 122-th batch, train loss: 0.5690969824790955:  51%|█████▌     | 121/237 [00:21<00:20,  5.55it/s]evaluate for the 18-th batch, evaluate loss: 0.6399216651916504:  35%|██████▎           | 16/46 [00:01<00:02, 10.23it/s]evaluate for the 18-th batch, evaluate loss: 0.6399216651916504:  39%|███████           | 18/46 [00:01<00:03,  8.95it/s]Epoch: 4, train for the 97-th batch, train loss: 0.5197396278381348:  66%|████████▋    | 97/146 [00:15<00:07,  6.64it/s]Epoch: 2, train for the 122-th batch, train loss: 0.5690969824790955:  51%|█████▋     | 122/237 [00:21<00:21,  5.38it/s]Epoch: 2, train for the 237-th batch, train loss: 0.5036875605583191:  98%|██████████▊| 236/241 [00:40<00:00,  5.47it/s]Epoch: 1, train for the 347-th batch, train loss: 0.3120719790458679:  91%|█████████▉ | 347/383 [01:46<00:16,  2.22it/s]Epoch: 5, train for the 19-th batch, train loss: 0.38256824016571045:  15%|█▊          | 18/119 [00:03<00:14,  7.04it/s]Epoch: 5, train for the 19-th batch, train loss: 0.38256824016571045:  16%|█▉          | 19/119 [00:03<00:14,  7.07it/s]Epoch: 2, train for the 237-th batch, train loss: 0.5036875605583191:  98%|██████████▊| 237/241 [00:41<00:00,  5.59it/s]evaluate for the 19-th batch, evaluate loss: 0.6785209774971008:  39%|███████           | 18/46 [00:01<00:03,  8.95it/s]Epoch: 4, train for the 98-th batch, train loss: 0.49270063638687134:  66%|███████▉    | 97/146 [00:15<00:07,  6.64it/s]Epoch: 4, train for the 98-th batch, train loss: 0.49270063638687134:  67%|████████    | 98/146 [00:15<00:07,  6.85it/s]evaluate for the 20-th batch, evaluate loss: 0.6732420325279236:  39%|███████           | 18/46 [00:01<00:03,  8.95it/s]evaluate for the 20-th batch, evaluate loss: 0.6732420325279236:  43%|███████▊          | 20/46 [00:01<00:02,  9.72it/s]Epoch: 2, train for the 123-th batch, train loss: 0.5257043838500977:  51%|█████▋     | 122/237 [00:22<00:21,  5.38it/s]Epoch: 5, train for the 20-th batch, train loss: 0.41879263520240784:  16%|█▉          | 19/119 [00:03<00:14,  7.07it/s]Epoch: 5, train for the 20-th batch, train loss: 0.41879263520240784:  17%|██          | 20/119 [00:03<00:14,  6.83it/s]Epoch: 2, train for the 123-th batch, train loss: 0.5257043838500977:  52%|█████▋     | 123/237 [00:22<00:21,  5.28it/s]Epoch: 2, train for the 238-th batch, train loss: 0.4716467559337616:  98%|██████████▊| 237/241 [00:41<00:00,  5.59it/s]evaluate for the 21-th batch, evaluate loss: 0.6552305221557617:  43%|███████▊          | 20/46 [00:02<00:02,  9.72it/s]Epoch: 4, train for the 99-th batch, train loss: 0.5345242023468018:  67%|████████▋    | 98/146 [00:15<00:07,  6.85it/s]Epoch: 4, train for the 99-th batch, train loss: 0.5345242023468018:  68%|████████▊    | 99/146 [00:15<00:06,  7.00it/s]Epoch: 2, train for the 238-th batch, train loss: 0.4716467559337616:  99%|██████████▊| 238/241 [00:41<00:00,  5.14it/s]Epoch: 1, train for the 348-th batch, train loss: 0.4645334482192993:  91%|█████████▉ | 347/383 [01:46<00:16,  2.22it/s]Epoch: 5, train for the 21-th batch, train loss: 0.47491925954818726:  17%|██          | 20/119 [00:03<00:14,  6.83it/s]evaluate for the 22-th batch, evaluate loss: 0.611213743686676:  43%|████████▎          | 20/46 [00:02<00:02,  9.72it/s]evaluate for the 22-th batch, evaluate loss: 0.611213743686676:  48%|█████████          | 22/46 [00:02<00:02, 10.07it/s]Epoch: 5, train for the 21-th batch, train loss: 0.47491925954818726:  18%|██          | 21/119 [00:03<00:14,  6.96it/s]Epoch: 2, train for the 124-th batch, train loss: 0.5051640868186951:  52%|█████▋     | 123/237 [00:22<00:21,  5.28it/s]Epoch: 1, train for the 348-th batch, train loss: 0.4645334482192993:  91%|█████████▉ | 348/383 [01:46<00:14,  2.37it/s]Epoch: 2, train for the 124-th batch, train loss: 0.5051640868186951:  52%|█████▊     | 124/237 [00:22<00:21,  5.30it/s]Epoch: 4, train for the 100-th batch, train loss: 0.5479680895805359:  68%|████████▏   | 99/146 [00:15<00:06,  7.00it/s]Epoch: 4, train for the 100-th batch, train loss: 0.5479680895805359:  68%|███████▌   | 100/146 [00:15<00:06,  6.83it/s]Epoch: 2, train for the 239-th batch, train loss: 0.5396830439567566:  99%|██████████▊| 238/241 [00:41<00:00,  5.14it/s]evaluate for the 23-th batch, evaluate loss: 0.6701638698577881:  48%|████████▌         | 22/46 [00:02<00:02, 10.07it/s]Epoch: 2, train for the 239-th batch, train loss: 0.5396830439567566:  99%|██████████▉| 239/241 [00:41<00:00,  5.23it/s]Epoch: 5, train for the 22-th batch, train loss: 0.4036340117454529:  18%|██▎          | 21/119 [00:03<00:14,  6.96it/s]Epoch: 5, train for the 22-th batch, train loss: 0.4036340117454529:  18%|██▍          | 22/119 [00:03<00:14,  6.86it/s]evaluate for the 24-th batch, evaluate loss: 0.6157155632972717:  48%|████████▌         | 22/46 [00:02<00:02, 10.07it/s]evaluate for the 24-th batch, evaluate loss: 0.6157155632972717:  52%|█████████▍        | 24/46 [00:02<00:02,  9.92it/s]Epoch: 4, train for the 101-th batch, train loss: 0.50419682264328:  68%|████████▉    | 100/146 [00:16<00:06,  6.83it/s]Epoch: 4, train for the 101-th batch, train loss: 0.50419682264328:  69%|████████▉    | 101/146 [00:16<00:06,  6.82it/s]Epoch: 2, train for the 125-th batch, train loss: 0.503150999546051:  52%|██████▎     | 124/237 [00:22<00:21,  5.30it/s]Epoch: 2, train for the 125-th batch, train loss: 0.503150999546051:  53%|██████▎     | 125/237 [00:22<00:22,  5.05it/s]Epoch: 5, train for the 23-th batch, train loss: 0.43057647347450256:  18%|██▏         | 22/119 [00:03<00:14,  6.86it/s]Epoch: 2, train for the 240-th batch, train loss: 0.4368598163127899:  99%|██████████▉| 239/241 [00:41<00:00,  5.23it/s]Epoch: 5, train for the 23-th batch, train loss: 0.43057647347450256:  19%|██▎         | 23/119 [00:03<00:14,  6.78it/s]evaluate for the 25-th batch, evaluate loss: 0.6370647549629211:  52%|█████████▍        | 24/46 [00:02<00:02,  9.92it/s]Epoch: 1, train for the 349-th batch, train loss: 0.3779366612434387:  91%|█████████▉ | 348/383 [01:46<00:14,  2.37it/s]Epoch: 2, train for the 240-th batch, train loss: 0.4368598163127899: 100%|██████████▉| 240/241 [00:41<00:00,  5.01it/s]Epoch: 4, train for the 102-th batch, train loss: 0.4670790731906891:  69%|███████▌   | 101/146 [00:16<00:06,  6.82it/s]evaluate for the 26-th batch, evaluate loss: 0.6799755692481995:  52%|█████████▍        | 24/46 [00:02<00:02,  9.92it/s]evaluate for the 26-th batch, evaluate loss: 0.6799755692481995:  57%|██████████▏       | 26/46 [00:02<00:01, 10.37it/s]Epoch: 4, train for the 102-th batch, train loss: 0.4670790731906891:  70%|███████▋   | 102/146 [00:16<00:06,  6.67it/s]Epoch: 1, train for the 349-th batch, train loss: 0.3779366612434387:  91%|██████████ | 349/383 [01:46<00:13,  2.50it/s]evaluate for the 27-th batch, evaluate loss: 0.6545295715332031:  57%|██████████▏       | 26/46 [00:02<00:01, 10.37it/s]Epoch: 2, train for the 126-th batch, train loss: 0.545013964176178:  53%|██████▎     | 125/237 [00:22<00:22,  5.05it/s]Epoch: 2, train for the 126-th batch, train loss: 0.545013964176178:  53%|██████▍     | 126/237 [00:22<00:22,  4.93it/s]Epoch: 4, train for the 103-th batch, train loss: 0.5011872053146362:  70%|███████▋   | 102/146 [00:16<00:06,  6.67it/s]Epoch: 4, train for the 103-th batch, train loss: 0.5011872053146362:  71%|███████▊   | 103/146 [00:16<00:06,  7.06it/s]Epoch: 2, train for the 241-th batch, train loss: 0.44836312532424927: 100%|█████████▉| 240/241 [00:41<00:00,  5.01it/s]Epoch: 2, train for the 241-th batch, train loss: 0.44836312532424927: 100%|██████████| 241/241 [00:41<00:00,  5.11it/s]Epoch: 2, train for the 241-th batch, train loss: 0.44836312532424927: 100%|██████████| 241/241 [00:41<00:00,  5.76it/s]
evaluate for the 28-th batch, evaluate loss: 0.6151174902915955:  57%|██████████▏       | 26/46 [00:02<00:01, 10.37it/s]evaluate for the 28-th batch, evaluate loss: 0.6151174902915955:  61%|██████████▉       | 28/46 [00:02<00:01, 11.01it/s]Epoch: 4, train for the 104-th batch, train loss: 0.49037742614746094:  71%|███████   | 103/146 [00:16<00:06,  7.06it/s]Epoch: 1, train for the 350-th batch, train loss: 0.37909889221191406:  91%|█████████ | 349/383 [01:47<00:13,  2.50it/s]Epoch: 4, train for the 104-th batch, train loss: 0.49037742614746094:  71%|███████   | 104/146 [00:16<00:06,  6.97it/s]evaluate for the 29-th batch, evaluate loss: 0.5903982520103455:  61%|██████████▉       | 28/46 [00:02<00:01, 11.01it/s]Epoch: 2, train for the 127-th batch, train loss: 0.5468474626541138:  53%|█████▊     | 126/237 [00:22<00:22,  4.93it/s]  0%|                                                                                            | 0/72 [00:00<?, ?it/s]Epoch: 1, train for the 350-th batch, train loss: 0.37909889221191406:  91%|█████████▏| 350/383 [01:47<00:12,  2.70it/s]Epoch: 2, train for the 127-th batch, train loss: 0.5468474626541138:  54%|█████▉     | 127/237 [00:22<00:23,  4.77it/s]evaluate for the 30-th batch, evaluate loss: 0.6500088572502136:  61%|██████████▉       | 28/46 [00:02<00:01, 11.01it/s]evaluate for the 30-th batch, evaluate loss: 0.6500088572502136:  65%|███████████▋      | 30/46 [00:02<00:01, 10.36it/s]Epoch: 4, train for the 105-th batch, train loss: 0.5220931172370911:  71%|███████▊   | 104/146 [00:16<00:06,  6.97it/s]Epoch: 5, train for the 24-th batch, train loss: 0.41253164410591125:  19%|██▎         | 23/119 [00:04<00:14,  6.78it/s]Epoch: 4, train for the 105-th batch, train loss: 0.5220931172370911:  72%|███████▉   | 105/146 [00:16<00:05,  7.10it/s]Epoch: 5, train for the 24-th batch, train loss: 0.41253164410591125:  20%|██▍         | 24/119 [00:04<00:23,  4.04it/s]evaluate for the 1-th batch, evaluate loss: 0.5900846123695374:   0%|                            | 0/72 [00:00<?, ?it/s]evaluate for the 31-th batch, evaluate loss: 0.5146647691726685:  65%|███████████▋      | 30/46 [00:03<00:01, 10.36it/s]evaluate for the 2-th batch, evaluate loss: 0.6551955938339233:   0%|                            | 0/72 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.6551955938339233:   3%|▌                   | 2/72 [00:00<00:06, 10.58it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5168596506118774:  54%|█████▉     | 127/237 [00:23<00:23,  4.77it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5168596506118774:  54%|█████▉     | 128/237 [00:23<00:23,  4.70it/s]evaluate for the 32-th batch, evaluate loss: 0.5690027475357056:  65%|███████████▋      | 30/46 [00:03<00:01, 10.36it/s]evaluate for the 32-th batch, evaluate loss: 0.5690027475357056:  70%|████████████▌     | 32/46 [00:03<00:01, 10.05it/s]evaluate for the 3-th batch, evaluate loss: 0.6305810213088989:   3%|▌                   | 2/72 [00:00<00:06, 10.58it/s]Epoch: 5, train for the 25-th batch, train loss: 0.4008060395717621:  20%|██▌          | 24/119 [00:04<00:23,  4.04it/s]Epoch: 1, train for the 351-th batch, train loss: 0.40886908769607544:  91%|█████████▏| 350/383 [01:47<00:12,  2.70it/s]Epoch: 4, train for the 106-th batch, train loss: 0.4835142493247986:  72%|███████▉   | 105/146 [00:16<00:05,  7.10it/s]Epoch: 5, train for the 25-th batch, train loss: 0.4008060395717621:  21%|██▋          | 25/119 [00:04<00:22,  4.14it/s]Epoch: 4, train for the 106-th batch, train loss: 0.4835142493247986:  73%|███████▉   | 106/146 [00:16<00:06,  5.90it/s]evaluate for the 4-th batch, evaluate loss: 0.6420184373855591:   3%|▌                   | 2/72 [00:00<00:06, 10.58it/s]evaluate for the 4-th batch, evaluate loss: 0.6420184373855591:   6%|█                   | 4/72 [00:00<00:05, 12.40it/s]evaluate for the 33-th batch, evaluate loss: 0.6058430075645447:  70%|████████████▌     | 32/46 [00:03<00:01, 10.05it/s]Epoch: 2, train for the 129-th batch, train loss: 0.5625354647636414:  54%|█████▉     | 128/237 [00:23<00:23,  4.70it/s]Epoch: 1, train for the 351-th batch, train loss: 0.40886908769607544:  92%|█████████▏| 351/383 [01:47<00:11,  2.67it/s]Epoch: 2, train for the 129-th batch, train loss: 0.5625354647636414:  54%|█████▉     | 129/237 [00:23<00:21,  4.97it/s]evaluate for the 34-th batch, evaluate loss: 0.5366783142089844:  70%|████████████▌     | 32/46 [00:03<00:01, 10.05it/s]evaluate for the 34-th batch, evaluate loss: 0.5366783142089844:  74%|█████████████▎    | 34/46 [00:03<00:01, 10.41it/s]Epoch: 5, train for the 26-th batch, train loss: 0.3225243389606476:  21%|██▋          | 25/119 [00:04<00:22,  4.14it/s]Epoch: 5, train for the 26-th batch, train loss: 0.3225243389606476:  22%|██▊          | 26/119 [00:04<00:20,  4.61it/s]Epoch: 4, train for the 107-th batch, train loss: 0.5499606728553772:  73%|███████▉   | 106/146 [00:17<00:06,  5.90it/s]Epoch: 4, train for the 107-th batch, train loss: 0.5499606728553772:  73%|████████   | 107/146 [00:17<00:06,  5.67it/s]evaluate for the 35-th batch, evaluate loss: 0.6220369338989258:  74%|█████████████▎    | 34/46 [00:03<00:01, 10.41it/s]evaluate for the 5-th batch, evaluate loss: 0.661821722984314:   6%|█▏                   | 4/72 [00:00<00:05, 12.40it/s]Epoch: 2, train for the 130-th batch, train loss: 0.5187572240829468:  54%|█████▉     | 129/237 [00:23<00:21,  4.97it/s]Epoch: 2, train for the 130-th batch, train loss: 0.5187572240829468:  55%|██████     | 130/237 [00:23<00:21,  5.01it/s]evaluate for the 36-th batch, evaluate loss: 0.5848953127861023:  74%|█████████████▎    | 34/46 [00:03<00:01, 10.41it/s]evaluate for the 36-th batch, evaluate loss: 0.5848953127861023:  78%|██████████████    | 36/46 [00:03<00:00, 10.81it/s]evaluate for the 6-th batch, evaluate loss: 0.6997557878494263:   6%|█                   | 4/72 [00:00<00:05, 12.40it/s]evaluate for the 6-th batch, evaluate loss: 0.6997557878494263:   8%|█▋                  | 6/72 [00:00<00:07,  9.20it/s]Epoch: 1, train for the 352-th batch, train loss: 0.32418325543403625:  92%|█████████▏| 351/383 [01:47<00:11,  2.67it/s]Epoch: 5, train for the 27-th batch, train loss: 0.4264128804206848:  22%|██▊          | 26/119 [00:04<00:20,  4.61it/s]Epoch: 4, train for the 108-th batch, train loss: 0.504726767539978:  73%|████████▊   | 107/146 [00:17<00:06,  5.67it/s]Epoch: 5, train for the 27-th batch, train loss: 0.4264128804206848:  23%|██▉          | 27/119 [00:04<00:20,  4.49it/s]Epoch: 4, train for the 108-th batch, train loss: 0.504726767539978:  74%|████████▉   | 108/146 [00:17<00:06,  5.49it/s]evaluate for the 7-th batch, evaluate loss: 0.6776292324066162:   8%|█▋                  | 6/72 [00:00<00:07,  9.20it/s]evaluate for the 37-th batch, evaluate loss: 0.6528743505477905:  78%|██████████████    | 36/46 [00:03<00:00, 10.81it/s]Epoch: 1, train for the 352-th batch, train loss: 0.32418325543403625:  92%|█████████▏| 352/383 [01:47<00:11,  2.77it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5318340063095093:  55%|██████     | 130/237 [00:23<00:21,  5.01it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5318340063095093:  55%|██████     | 131/237 [00:23<00:21,  4.92it/s]evaluate for the 8-th batch, evaluate loss: 0.6461233496665955:   8%|█▋                  | 6/72 [00:00<00:07,  9.20it/s]evaluate for the 8-th batch, evaluate loss: 0.6461233496665955:  11%|██▏                 | 8/72 [00:00<00:06,  9.25it/s]evaluate for the 38-th batch, evaluate loss: 0.5663190484046936:  78%|██████████████    | 36/46 [00:03<00:00, 10.81it/s]evaluate for the 38-th batch, evaluate loss: 0.5663190484046936:  83%|██████████████▊   | 38/46 [00:03<00:00, 10.20it/s]Epoch: 4, train for the 109-th batch, train loss: 0.5037322044372559:  74%|████████▏  | 108/146 [00:17<00:06,  5.49it/s]Epoch: 4, train for the 109-th batch, train loss: 0.5037322044372559:  75%|████████▏  | 109/146 [00:17<00:06,  5.57it/s]Epoch: 5, train for the 28-th batch, train loss: 0.4055970013141632:  23%|██▉          | 27/119 [00:04<00:20,  4.49it/s]evaluate for the 9-th batch, evaluate loss: 0.6592393517494202:  11%|██▏                 | 8/72 [00:00<00:06,  9.25it/s]Epoch: 5, train for the 28-th batch, train loss: 0.4055970013141632:  24%|███          | 28/119 [00:04<00:19,  4.73it/s]evaluate for the 39-th batch, evaluate loss: 0.6220221519470215:  83%|██████████████▊   | 38/46 [00:03<00:00, 10.20it/s]evaluate for the 10-th batch, evaluate loss: 0.6032161712646484:  11%|██                 | 8/72 [00:00<00:06,  9.25it/s]evaluate for the 10-th batch, evaluate loss: 0.6032161712646484:  14%|██▌               | 10/72 [00:00<00:05, 10.50it/s]Epoch: 2, train for the 132-th batch, train loss: 0.5518880486488342:  55%|██████     | 131/237 [00:23<00:21,  4.92it/s]Epoch: 2, train for the 132-th batch, train loss: 0.5518880486488342:  56%|██████▏    | 132/237 [00:23<00:21,  4.85it/s]evaluate for the 11-th batch, evaluate loss: 0.6112056374549866:  14%|██▌               | 10/72 [00:01<00:05, 10.50it/s]evaluate for the 40-th batch, evaluate loss: 0.5967711210250854:  83%|██████████████▊   | 38/46 [00:03<00:00, 10.20it/s]evaluate for the 40-th batch, evaluate loss: 0.5967711210250854:  87%|███████████████▋  | 40/46 [00:03<00:00,  9.72it/s]Epoch: 4, train for the 110-th batch, train loss: 0.47904953360557556:  75%|███████▍  | 109/146 [00:17<00:06,  5.57it/s]Epoch: 4, train for the 110-th batch, train loss: 0.47904953360557556:  75%|███████▌  | 110/146 [00:17<00:06,  5.39it/s]evaluate for the 12-th batch, evaluate loss: 0.6546388864517212:  14%|██▌               | 10/72 [00:01<00:05, 10.50it/s]evaluate for the 12-th batch, evaluate loss: 0.6546388864517212:  17%|███               | 12/72 [00:01<00:05, 11.33it/s]evaluate for the 41-th batch, evaluate loss: 0.6150895357131958:  87%|███████████████▋  | 40/46 [00:04<00:00,  9.72it/s]evaluate for the 13-th batch, evaluate loss: 0.6242337226867676:  17%|███               | 12/72 [00:01<00:05, 11.33it/s]Epoch: 5, train for the 29-th batch, train loss: 0.4100378453731537:  24%|███          | 28/119 [00:05<00:19,  4.73it/s]Epoch: 1, train for the 353-th batch, train loss: 0.45730215311050415:  92%|█████████▏| 352/383 [01:48<00:11,  2.77it/s]Epoch: 4, train for the 111-th batch, train loss: 0.48016706109046936:  75%|███████▌  | 110/146 [00:17<00:06,  5.39it/s]Epoch: 5, train for the 29-th batch, train loss: 0.4100378453731537:  24%|███▏         | 29/119 [00:05<00:21,  4.11it/s]Epoch: 4, train for the 111-th batch, train loss: 0.48016706109046936:  76%|███████▌  | 111/146 [00:17<00:06,  5.83it/s]evaluate for the 42-th batch, evaluate loss: 0.5580523014068604:  87%|███████████████▋  | 40/46 [00:04<00:00,  9.72it/s]evaluate for the 42-th batch, evaluate loss: 0.5580523014068604:  91%|████████████████▍ | 42/46 [00:04<00:00, 10.22it/s]Epoch: 2, train for the 133-th batch, train loss: 0.5311813950538635:  56%|██████▏    | 132/237 [00:24<00:21,  4.85it/s]evaluate for the 14-th batch, evaluate loss: 0.6329351663589478:  17%|███               | 12/72 [00:01<00:05, 11.33it/s]evaluate for the 14-th batch, evaluate loss: 0.6329351663589478:  19%|███▌              | 14/72 [00:01<00:04, 12.59it/s]Epoch: 2, train for the 133-th batch, train loss: 0.5311813950538635:  56%|██████▏    | 133/237 [00:24<00:22,  4.63it/s]Epoch: 1, train for the 353-th batch, train loss: 0.45730215311050415:  92%|█████████▏| 353/383 [01:48<00:12,  2.36it/s]evaluate for the 15-th batch, evaluate loss: 0.6486167907714844:  19%|███▌              | 14/72 [00:01<00:04, 12.59it/s]evaluate for the 43-th batch, evaluate loss: 0.661742091178894:  91%|█████████████████▎ | 42/46 [00:04<00:00, 10.22it/s]evaluate for the 16-th batch, evaluate loss: 0.719040036201477:  19%|███▋               | 14/72 [00:01<00:04, 12.59it/s]evaluate for the 16-th batch, evaluate loss: 0.719040036201477:  22%|████▏              | 16/72 [00:01<00:04, 12.72it/s]Epoch: 5, train for the 30-th batch, train loss: 0.3455185890197754:  24%|███▏         | 29/119 [00:05<00:21,  4.11it/s]Epoch: 5, train for the 30-th batch, train loss: 0.3455185890197754:  25%|███▎         | 30/119 [00:05<00:20,  4.28it/s]Epoch: 4, train for the 112-th batch, train loss: 0.5327232480049133:  76%|████████▎  | 111/146 [00:18<00:06,  5.83it/s]evaluate for the 44-th batch, evaluate loss: 0.5717747211456299:  91%|████████████████▍ | 42/46 [00:04<00:00, 10.22it/s]evaluate for the 44-th batch, evaluate loss: 0.5717747211456299:  96%|█████████████████▏| 44/46 [00:04<00:00,  9.98it/s]Epoch: 4, train for the 112-th batch, train loss: 0.5327232480049133:  77%|████████▍  | 112/146 [00:18<00:06,  5.33it/s]Epoch: 2, train for the 134-th batch, train loss: 0.5829342603683472:  56%|██████▏    | 133/237 [00:24<00:22,  4.63it/s]evaluate for the 17-th batch, evaluate loss: 0.6235965490341187:  22%|████              | 16/72 [00:01<00:04, 12.72it/s]Epoch: 2, train for the 134-th batch, train loss: 0.5829342603683472:  57%|██████▏    | 134/237 [00:24<00:22,  4.63it/s]Epoch: 1, train for the 354-th batch, train loss: 0.3619403839111328:  92%|██████████▏| 353/383 [01:48<00:12,  2.36it/s]evaluate for the 45-th batch, evaluate loss: 0.5521432757377625:  96%|█████████████████▏| 44/46 [00:04<00:00,  9.98it/s]evaluate for the 18-th batch, evaluate loss: 0.6374011635780334:  22%|████              | 16/72 [00:01<00:04, 12.72it/s]evaluate for the 18-th batch, evaluate loss: 0.6374011635780334:  25%|████▌             | 18/72 [00:01<00:04, 12.88it/s]Epoch: 1, train for the 354-th batch, train loss: 0.3619403839111328:  92%|██████████▏| 354/383 [01:48<00:11,  2.59it/s]Epoch: 5, train for the 31-th batch, train loss: 0.41581130027770996:  25%|███         | 30/119 [00:05<00:20,  4.28it/s]Epoch: 4, train for the 113-th batch, train loss: 0.4411827623844147:  77%|████████▍  | 112/146 [00:18<00:06,  5.33it/s]Epoch: 5, train for the 31-th batch, train loss: 0.41581130027770996:  26%|███▏        | 31/119 [00:05<00:19,  4.50it/s]Epoch: 4, train for the 113-th batch, train loss: 0.4411827623844147:  77%|████████▌  | 113/146 [00:18<00:06,  5.42it/s]evaluate for the 46-th batch, evaluate loss: 0.5220835208892822:  96%|█████████████████▏| 44/46 [00:04<00:00,  9.98it/s]evaluate for the 46-th batch, evaluate loss: 0.5220835208892822: 100%|██████████████████| 46/46 [00:04<00:00, 10.16it/s]evaluate for the 46-th batch, evaluate loss: 0.5220835208892822: 100%|██████████████████| 46/46 [00:04<00:00, 10.20it/s]
evaluate for the 19-th batch, evaluate loss: 0.6733061075210571:  25%|████▌             | 18/72 [00:01<00:04, 12.88it/s]Epoch: 2, train for the 135-th batch, train loss: 0.5840114951133728:  57%|██████▏    | 134/237 [00:24<00:22,  4.63it/s]Epoch: 2, train for the 135-th batch, train loss: 0.5840114951133728:  57%|██████▎    | 135/237 [00:24<00:21,  4.64it/s]evaluate for the 20-th batch, evaluate loss: 0.595257580280304:  25%|████▊              | 18/72 [00:01<00:04, 12.88it/s]evaluate for the 20-th batch, evaluate loss: 0.595257580280304:  28%|█████▎             | 20/72 [00:01<00:04, 11.43it/s]Epoch: 5, train for the 32-th batch, train loss: 0.3692915141582489:  26%|███▍         | 31/119 [00:05<00:19,  4.50it/s]Epoch: 4, train for the 114-th batch, train loss: 0.5115551352500916:  77%|████████▌  | 113/146 [00:18<00:06,  5.42it/s]Epoch: 5, train for the 32-th batch, train loss: 0.3692915141582489:  27%|███▍         | 32/119 [00:05<00:18,  4.69it/s]Epoch: 4, train for the 114-th batch, train loss: 0.5115551352500916:  78%|████████▌  | 114/146 [00:18<00:06,  5.32it/s]evaluate for the 21-th batch, evaluate loss: 0.6601448059082031:  28%|█████             | 20/72 [00:01<00:04, 11.43it/s]Epoch: 1, train for the 355-th batch, train loss: 0.45444679260253906:  92%|█████████▏| 354/383 [01:49<00:11,  2.59it/s]Epoch: 2, train for the 136-th batch, train loss: 0.6055189967155457:  57%|██████▎    | 135/237 [00:24<00:21,  4.64it/s]evaluate for the 22-th batch, evaluate loss: 0.5728484392166138:  28%|█████             | 20/72 [00:01<00:04, 11.43it/s]evaluate for the 22-th batch, evaluate loss: 0.5728484392166138:  31%|█████▌            | 22/72 [00:01<00:04, 12.02it/s]Epoch: 2, train for the 136-th batch, train loss: 0.6055189967155457:  57%|██████▎    | 136/237 [00:24<00:22,  4.48it/s]Epoch: 1, train for the 355-th batch, train loss: 0.45444679260253906:  93%|█████████▎| 355/383 [01:49<00:10,  2.63it/s]evaluate for the 23-th batch, evaluate loss: 0.6086871027946472:  31%|█████▌            | 22/72 [00:01<00:04, 12.02it/s]Epoch: 5, train for the 33-th batch, train loss: 0.39561325311660767:  27%|███▏        | 32/119 [00:06<00:18,  4.69it/s]Epoch: 4, train for the 115-th batch, train loss: 0.5280989408493042:  78%|████████▌  | 114/146 [00:18<00:06,  5.32it/s]Epoch: 5, train for the 33-th batch, train loss: 0.39561325311660767:  28%|███▎        | 33/119 [00:06<00:17,  4.82it/s]Epoch: 4, train for the 115-th batch, train loss: 0.5280989408493042:  79%|████████▋  | 115/146 [00:18<00:05,  5.24it/s]  0%|                                                                                            | 0/25 [00:00<?, ?it/s]evaluate for the 24-th batch, evaluate loss: 0.6456383466720581:  31%|█████▌            | 22/72 [00:02<00:04, 12.02it/s]evaluate for the 24-th batch, evaluate loss: 0.6456383466720581:  33%|██████            | 24/72 [00:02<00:03, 13.27it/s]Epoch: 2, train for the 137-th batch, train loss: 0.6088286638259888:  57%|██████▎    | 136/237 [00:24<00:22,  4.48it/s]evaluate for the 25-th batch, evaluate loss: 0.7496944069862366:  33%|██████            | 24/72 [00:02<00:03, 13.27it/s]Epoch: 2, train for the 137-th batch, train loss: 0.6088286638259888:  58%|██████▎    | 137/237 [00:25<00:22,  4.48it/s]evaluate for the 1-th batch, evaluate loss: 1.0508956909179688:   0%|                            | 0/25 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 1.0508956909179688:   4%|▊                   | 1/25 [00:00<00:03,  7.40it/s]Epoch: 5, train for the 34-th batch, train loss: 0.4077817499637604:  28%|███▌         | 33/119 [00:06<00:17,  4.82it/s]Epoch: 5, train for the 34-th batch, train loss: 0.4077817499637604:  29%|███▋         | 34/119 [00:06<00:16,  5.09it/s]Epoch: 4, train for the 116-th batch, train loss: 0.523433268070221:  79%|█████████▍  | 115/146 [00:18<00:05,  5.24it/s]Epoch: 1, train for the 356-th batch, train loss: 0.274886816740036:  93%|███████████ | 355/383 [01:49<00:10,  2.63it/s]Epoch: 4, train for the 116-th batch, train loss: 0.523433268070221:  79%|█████████▌  | 116/146 [00:18<00:05,  5.17it/s]evaluate for the 26-th batch, evaluate loss: 0.5594754815101624:  33%|██████            | 24/72 [00:02<00:03, 13.27it/s]evaluate for the 26-th batch, evaluate loss: 0.5594754815101624:  36%|██████▌           | 26/72 [00:02<00:03, 12.41it/s]evaluate for the 2-th batch, evaluate loss: 1.0435283184051514:   4%|▊                   | 1/25 [00:00<00:03,  7.40it/s]Epoch: 1, train for the 356-th batch, train loss: 0.274886816740036:  93%|███████████▏| 356/383 [01:49<00:09,  2.72it/s]evaluate for the 27-th batch, evaluate loss: 0.5441639423370361:  36%|██████▌           | 26/72 [00:02<00:03, 12.41it/s]evaluate for the 3-th batch, evaluate loss: 1.0393978357315063:   4%|▊                   | 1/25 [00:00<00:03,  7.40it/s]evaluate for the 3-th batch, evaluate loss: 1.0393978357315063:  12%|██▍                 | 3/25 [00:00<00:02,  9.59it/s]Epoch: 5, train for the 35-th batch, train loss: 0.41475680470466614:  29%|███▍        | 34/119 [00:06<00:16,  5.09it/s]evaluate for the 28-th batch, evaluate loss: 0.6367843151092529:  36%|██████▌           | 26/72 [00:02<00:03, 12.41it/s]evaluate for the 28-th batch, evaluate loss: 0.6367843151092529:  39%|███████           | 28/72 [00:02<00:03, 12.84it/s]Epoch: 5, train for the 35-th batch, train loss: 0.41475680470466614:  29%|███▌        | 35/119 [00:06<00:16,  5.16it/s]Epoch: 2, train for the 138-th batch, train loss: 0.6119483709335327:  58%|██████▎    | 137/237 [00:25<00:22,  4.48it/s]Epoch: 2, train for the 138-th batch, train loss: 0.6119483709335327:  58%|██████▍    | 138/237 [00:25<00:22,  4.41it/s]Epoch: 4, train for the 117-th batch, train loss: 0.5239787697792053:  79%|████████▋  | 116/146 [00:19<00:05,  5.17it/s]Epoch: 4, train for the 117-th batch, train loss: 0.5239787697792053:  80%|████████▊  | 117/146 [00:19<00:05,  5.08it/s]evaluate for the 4-th batch, evaluate loss: 1.0079420804977417:  12%|██▍                 | 3/25 [00:00<00:02,  9.59it/s]evaluate for the 29-th batch, evaluate loss: 0.6228533387184143:  39%|███████           | 28/72 [00:02<00:03, 12.84it/s]Epoch: 5, train for the 36-th batch, train loss: 0.3699360489845276:  29%|███▊         | 35/119 [00:06<00:16,  5.16it/s]evaluate for the 5-th batch, evaluate loss: 0.9358886480331421:  12%|██▍                 | 3/25 [00:00<00:02,  9.59it/s]evaluate for the 5-th batch, evaluate loss: 0.9358886480331421:  20%|████                | 5/25 [00:00<00:01, 10.83it/s]Epoch: 5, train for the 36-th batch, train loss: 0.3699360489845276:  30%|███▉         | 36/119 [00:06<00:15,  5.51it/s]evaluate for the 30-th batch, evaluate loss: 0.6530207395553589:  39%|███████           | 28/72 [00:02<00:03, 12.84it/s]evaluate for the 30-th batch, evaluate loss: 0.6530207395553589:  42%|███████▌          | 30/72 [00:02<00:03, 12.58it/s]Epoch: 4, train for the 118-th batch, train loss: 0.5069273114204407:  80%|████████▊  | 117/146 [00:19<00:05,  5.08it/s]Epoch: 2, train for the 139-th batch, train loss: 0.5850810408592224:  58%|██████▍    | 138/237 [00:25<00:22,  4.41it/s]evaluate for the 6-th batch, evaluate loss: 0.9732264280319214:  20%|████                | 5/25 [00:00<00:01, 10.83it/s]Epoch: 4, train for the 118-th batch, train loss: 0.5069273114204407:  81%|████████▉  | 118/146 [00:19<00:05,  5.25it/s]evaluate for the 31-th batch, evaluate loss: 0.686247706413269:  42%|███████▉           | 30/72 [00:02<00:03, 12.58it/s]Epoch: 2, train for the 139-th batch, train loss: 0.5850810408592224:  59%|██████▍    | 139/237 [00:25<00:22,  4.38it/s]Epoch: 5, train for the 37-th batch, train loss: 0.4277104139328003:  30%|███▉         | 36/119 [00:06<00:15,  5.51it/s]Epoch: 5, train for the 37-th batch, train loss: 0.4277104139328003:  31%|████         | 37/119 [00:06<00:13,  5.99it/s]evaluate for the 7-th batch, evaluate loss: 0.8967304229736328:  20%|████                | 5/25 [00:00<00:01, 10.83it/s]evaluate for the 7-th batch, evaluate loss: 0.8967304229736328:  28%|█████▌              | 7/25 [00:00<00:01, 11.12it/s]evaluate for the 32-th batch, evaluate loss: 0.5408962368965149:  42%|███████▌          | 30/72 [00:02<00:03, 12.58it/s]evaluate for the 32-th batch, evaluate loss: 0.5408962368965149:  44%|████████          | 32/72 [00:02<00:03, 12.51it/s]Epoch: 4, train for the 119-th batch, train loss: 0.4880954623222351:  81%|████████▉  | 118/146 [00:19<00:05,  5.25it/s]Epoch: 4, train for the 119-th batch, train loss: 0.4880954623222351:  82%|████████▉  | 119/146 [00:19<00:04,  5.46it/s]evaluate for the 33-th batch, evaluate loss: 0.6599655151367188:  44%|████████          | 32/72 [00:02<00:03, 12.51it/s]Epoch: 5, train for the 38-th batch, train loss: 0.37583816051483154:  31%|███▋        | 37/119 [00:06<00:13,  5.99it/s]evaluate for the 8-th batch, evaluate loss: 0.918965220451355:  28%|█████▉               | 7/25 [00:00<00:01, 11.12it/s]Epoch: 5, train for the 38-th batch, train loss: 0.37583816051483154:  32%|███▊        | 38/119 [00:06<00:13,  6.14it/s]evaluate for the 34-th batch, evaluate loss: 0.7002922892570496:  44%|████████          | 32/72 [00:02<00:03, 12.51it/s]evaluate for the 34-th batch, evaluate loss: 0.7002922892570496:  47%|████████▌         | 34/72 [00:02<00:03, 12.12it/s]evaluate for the 9-th batch, evaluate loss: 0.8399158120155334:  28%|█████▌              | 7/25 [00:00<00:01, 11.12it/s]evaluate for the 9-th batch, evaluate loss: 0.8399158120155334:  36%|███████▏            | 9/25 [00:00<00:01, 10.37it/s]evaluate for the 35-th batch, evaluate loss: 0.6400781273841858:  47%|████████▌         | 34/72 [00:02<00:03, 12.12it/s]Epoch: 4, train for the 120-th batch, train loss: 0.475166916847229:  82%|█████████▊  | 119/146 [00:19<00:04,  5.46it/s]Epoch: 1, train for the 357-th batch, train loss: 0.37484705448150635:  93%|█████████▎| 356/383 [01:50<00:09,  2.72it/s]Epoch: 4, train for the 120-th batch, train loss: 0.475166916847229:  82%|█████████▊  | 120/146 [00:19<00:04,  5.31it/s]Epoch: 5, train for the 39-th batch, train loss: 0.4437450170516968:  32%|████▏        | 38/119 [00:07<00:13,  6.14it/s]Epoch: 5, train for the 39-th batch, train loss: 0.4437450170516968:  33%|████▎        | 39/119 [00:07<00:13,  6.02it/s]Epoch: 2, train for the 140-th batch, train loss: 0.5442992448806763:  59%|██████▍    | 139/237 [00:25<00:22,  4.38it/s]evaluate for the 10-th batch, evaluate loss: 0.8856731057167053:  36%|██████▊            | 9/25 [00:00<00:01, 10.37it/s]evaluate for the 36-th batch, evaluate loss: 0.6606966853141785:  47%|████████▌         | 34/72 [00:02<00:03, 12.12it/s]evaluate for the 36-th batch, evaluate loss: 0.6606966853141785:  50%|█████████         | 36/72 [00:02<00:02, 12.88it/s]Epoch: 2, train for the 140-th batch, train loss: 0.5442992448806763:  59%|██████▍    | 140/237 [00:25<00:26,  3.62it/s]Epoch: 1, train for the 357-th batch, train loss: 0.37484705448150635:  93%|█████████▎| 357/383 [01:50<00:12,  2.09it/s]evaluate for the 11-th batch, evaluate loss: 0.8232534527778625:  36%|██████▊            | 9/25 [00:01<00:01, 10.37it/s]evaluate for the 11-th batch, evaluate loss: 0.8232534527778625:  44%|███████▉          | 11/25 [00:01<00:01, 10.64it/s]Epoch: 4, train for the 121-th batch, train loss: 0.46145376563072205:  82%|████████▏ | 120/146 [00:19<00:04,  5.31it/s]evaluate for the 37-th batch, evaluate loss: 0.6243311762809753:  50%|█████████         | 36/72 [00:03<00:02, 12.88it/s]Epoch: 4, train for the 121-th batch, train loss: 0.46145376563072205:  83%|████████▎ | 121/146 [00:19<00:04,  5.82it/s]Epoch: 5, train for the 40-th batch, train loss: 0.3623512089252472:  33%|████▎        | 39/119 [00:07<00:13,  6.02it/s]Epoch: 5, train for the 40-th batch, train loss: 0.3623512089252472:  34%|████▎        | 40/119 [00:07<00:13,  5.82it/s]evaluate for the 12-th batch, evaluate loss: 0.7309194207191467:  44%|███████▉          | 11/25 [00:01<00:01, 10.64it/s]evaluate for the 38-th batch, evaluate loss: 0.5721718668937683:  50%|█████████         | 36/72 [00:03<00:02, 12.88it/s]evaluate for the 38-th batch, evaluate loss: 0.5721718668937683:  53%|█████████▌        | 38/72 [00:03<00:02, 12.34it/s]Epoch: 2, train for the 141-th batch, train loss: 0.5751252770423889:  59%|██████▍    | 140/237 [00:26<00:26,  3.62it/s]Epoch: 2, train for the 141-th batch, train loss: 0.5751252770423889:  59%|██████▌    | 141/237 [00:26<00:24,  3.90it/s]Epoch: 4, train for the 122-th batch, train loss: 0.5704238414764404:  83%|█████████  | 121/146 [00:19<00:04,  5.82it/s]Epoch: 4, train for the 122-th batch, train loss: 0.5704238414764404:  84%|█████████▏ | 122/146 [00:19<00:04,  5.97it/s]evaluate for the 39-th batch, evaluate loss: 0.5692890882492065:  53%|█████████▌        | 38/72 [00:03<00:02, 12.34it/s]evaluate for the 13-th batch, evaluate loss: 0.7556939721107483:  44%|███████▉          | 11/25 [00:01<00:01, 10.64it/s]evaluate for the 13-th batch, evaluate loss: 0.7556939721107483:  52%|█████████▎        | 13/25 [00:01<00:01, 10.65it/s]Epoch: 5, train for the 41-th batch, train loss: 0.4038432836532593:  34%|████▎        | 40/119 [00:07<00:13,  5.82it/s]Epoch: 5, train for the 41-th batch, train loss: 0.4038432836532593:  34%|████▍        | 41/119 [00:07<00:13,  5.87it/s]evaluate for the 14-th batch, evaluate loss: 0.7525267004966736:  52%|█████████▎        | 13/25 [00:01<00:01, 10.65it/s]Epoch: 2, train for the 142-th batch, train loss: 0.5774506330490112:  59%|██████▌    | 141/237 [00:26<00:24,  3.90it/s]Epoch: 4, train for the 123-th batch, train loss: 0.5438399910926819:  84%|█████████▏ | 122/146 [00:19<00:04,  5.97it/s]Epoch: 2, train for the 142-th batch, train loss: 0.5774506330490112:  60%|██████▌    | 142/237 [00:26<00:21,  4.39it/s]Epoch: 4, train for the 123-th batch, train loss: 0.5438399910926819:  84%|█████████▎ | 123/146 [00:19<00:03,  6.32it/s]evaluate for the 15-th batch, evaluate loss: 0.8114323019981384:  52%|█████████▎        | 13/25 [00:01<00:01, 10.65it/s]evaluate for the 15-th batch, evaluate loss: 0.8114323019981384:  60%|██████████▊       | 15/25 [00:01<00:00, 11.83it/s]evaluate for the 16-th batch, evaluate loss: 0.7938105463981628:  60%|██████████▊       | 15/25 [00:01<00:00, 11.83it/s]Epoch: 5, train for the 42-th batch, train loss: 0.4391844570636749:  34%|████▍        | 41/119 [00:07<00:13,  5.87it/s]Epoch: 5, train for the 42-th batch, train loss: 0.4391844570636749:  35%|████▌        | 42/119 [00:07<00:12,  6.24it/s]Epoch: 1, train for the 358-th batch, train loss: 0.4751347005367279:  93%|██████████▎| 357/383 [01:50<00:12,  2.09it/s]Epoch: 4, train for the 124-th batch, train loss: 0.49718359112739563:  84%|████████▍ | 123/146 [00:20<00:03,  6.32it/s]Epoch: 2, train for the 143-th batch, train loss: 0.5418679118156433:  60%|██████▌    | 142/237 [00:26<00:21,  4.39it/s]evaluate for the 40-th batch, evaluate loss: 0.6586016416549683:  53%|█████████▌        | 38/72 [00:03<00:02, 12.34it/s]evaluate for the 40-th batch, evaluate loss: 0.6586016416549683:  56%|██████████        | 40/72 [00:03<00:03,  9.01it/s]Epoch: 4, train for the 124-th batch, train loss: 0.49718359112739563:  85%|████████▍ | 124/146 [00:20<00:03,  6.44it/s]evaluate for the 17-th batch, evaluate loss: 0.7376960515975952:  60%|██████████▊       | 15/25 [00:01<00:00, 11.83it/s]evaluate for the 17-th batch, evaluate loss: 0.7376960515975952:  68%|████████████▏     | 17/25 [00:01<00:00, 12.24it/s]Epoch: 2, train for the 143-th batch, train loss: 0.5418679118156433:  60%|██████▋    | 143/237 [00:26<00:19,  4.74it/s]Epoch: 1, train for the 358-th batch, train loss: 0.4751347005367279:  93%|██████████▎| 358/383 [01:50<00:12,  2.02it/s]Epoch: 5, train for the 43-th batch, train loss: 0.4273378551006317:  35%|████▌        | 42/119 [00:07<00:12,  6.24it/s]evaluate for the 41-th batch, evaluate loss: 0.650564968585968:  56%|██████████▌        | 40/72 [00:03<00:03,  9.01it/s]Epoch: 5, train for the 43-th batch, train loss: 0.4273378551006317:  36%|████▋        | 43/119 [00:07<00:12,  6.28it/s]evaluate for the 18-th batch, evaluate loss: 0.7104323506355286:  68%|████████████▏     | 17/25 [00:01<00:00, 12.24it/s]Epoch: 4, train for the 125-th batch, train loss: 0.48622938990592957:  85%|████████▍ | 124/146 [00:20<00:03,  6.44it/s]Epoch: 4, train for the 125-th batch, train loss: 0.48622938990592957:  86%|████████▌ | 125/146 [00:20<00:03,  6.51it/s]evaluate for the 42-th batch, evaluate loss: 0.6478053331375122:  56%|██████████        | 40/72 [00:03<00:03,  9.01it/s]evaluate for the 42-th batch, evaluate loss: 0.6478053331375122:  58%|██████████▌       | 42/72 [00:03<00:03,  9.83it/s]evaluate for the 19-th batch, evaluate loss: 0.6472233533859253:  68%|████████████▏     | 17/25 [00:01<00:00, 12.24it/s]evaluate for the 19-th batch, evaluate loss: 0.6472233533859253:  76%|█████████████▋    | 19/25 [00:01<00:00, 10.99it/s]Epoch: 2, train for the 144-th batch, train loss: 0.5514775514602661:  60%|██████▋    | 143/237 [00:26<00:19,  4.74it/s]evaluate for the 43-th batch, evaluate loss: 0.6452571153640747:  58%|██████████▌       | 42/72 [00:03<00:03,  9.83it/s]Epoch: 5, train for the 44-th batch, train loss: 0.3694114089012146:  36%|████▋        | 43/119 [00:07<00:12,  6.28it/s]Epoch: 5, train for the 44-th batch, train loss: 0.3694114089012146:  37%|████▊        | 44/119 [00:07<00:11,  6.26it/s]Epoch: 2, train for the 144-th batch, train loss: 0.5514775514602661:  61%|██████▋    | 144/237 [00:26<00:20,  4.43it/s]Epoch: 1, train for the 359-th batch, train loss: 0.44661766290664673:  93%|█████████▎| 358/383 [01:50<00:12,  2.02it/s]Epoch: 4, train for the 126-th batch, train loss: 0.511471152305603:  86%|██████████▎ | 125/146 [00:20<00:03,  6.51it/s]Epoch: 4, train for the 126-th batch, train loss: 0.511471152305603:  86%|██████████▎ | 126/146 [00:20<00:03,  6.48it/s]evaluate for the 44-th batch, evaluate loss: 0.5856826305389404:  58%|██████████▌       | 42/72 [00:03<00:03,  9.83it/s]evaluate for the 44-th batch, evaluate loss: 0.5856826305389404:  61%|███████████       | 44/72 [00:03<00:02, 10.31it/s]evaluate for the 20-th batch, evaluate loss: 0.6502249836921692:  76%|█████████████▋    | 19/25 [00:01<00:00, 10.99it/s]Epoch: 1, train for the 359-th batch, train loss: 0.44661766290664673:  94%|█████████▎| 359/383 [01:51<00:10,  2.25it/s]Epoch: 5, train for the 45-th batch, train loss: 0.36789894104003906:  37%|████▍       | 44/119 [00:07<00:11,  6.26it/s]Epoch: 5, train for the 45-th batch, train loss: 0.36789894104003906:  38%|████▌       | 45/119 [00:08<00:12,  6.12it/s]evaluate for the 21-th batch, evaluate loss: 0.676361083984375:  76%|██████████████▍    | 19/25 [00:01<00:00, 10.99it/s]evaluate for the 21-th batch, evaluate loss: 0.676361083984375:  84%|███████████████▉   | 21/25 [00:01<00:00, 10.72it/s]Epoch: 4, train for the 127-th batch, train loss: 0.5065376162528992:  86%|█████████▍ | 126/146 [00:20<00:03,  6.48it/s]Epoch: 2, train for the 145-th batch, train loss: 0.5732796788215637:  61%|██████▋    | 144/237 [00:26<00:20,  4.43it/s]Epoch: 4, train for the 127-th batch, train loss: 0.5065376162528992:  87%|█████████▌ | 127/146 [00:20<00:02,  6.40it/s]Epoch: 2, train for the 145-th batch, train loss: 0.5732796788215637:  61%|██████▋    | 145/237 [00:26<00:20,  4.58it/s]evaluate for the 22-th batch, evaluate loss: 0.596367597579956:  84%|███████████████▉   | 21/25 [00:02<00:00, 10.72it/s]evaluate for the 45-th batch, evaluate loss: 0.6590467095375061:  61%|███████████       | 44/72 [00:04<00:02, 10.31it/s]Epoch: 5, train for the 46-th batch, train loss: 0.394153892993927:  38%|█████▎        | 45/119 [00:08<00:12,  6.12it/s]Epoch: 5, train for the 46-th batch, train loss: 0.394153892993927:  39%|█████▍        | 46/119 [00:08<00:11,  6.27it/s]Epoch: 1, train for the 360-th batch, train loss: 0.3375636637210846:  94%|██████████▎| 359/383 [01:51<00:10,  2.25it/s]evaluate for the 23-th batch, evaluate loss: 0.6003354787826538:  84%|███████████████   | 21/25 [00:02<00:00, 10.72it/s]evaluate for the 23-th batch, evaluate loss: 0.6003354787826538:  92%|████████████████▌ | 23/25 [00:02<00:00, 11.00it/s]Epoch: 4, train for the 128-th batch, train loss: 0.48446357250213623:  87%|████████▋ | 127/146 [00:20<00:02,  6.40it/s]evaluate for the 46-th batch, evaluate loss: 0.7005308866500854:  61%|███████████       | 44/72 [00:04<00:02, 10.31it/s]evaluate for the 46-th batch, evaluate loss: 0.7005308866500854:  64%|███████████▌      | 46/72 [00:04<00:02,  9.13it/s]Epoch: 4, train for the 128-th batch, train loss: 0.48446357250213623:  88%|████████▊ | 128/146 [00:20<00:02,  6.47it/s]Epoch: 2, train for the 146-th batch, train loss: 0.5329474806785583:  61%|██████▋    | 145/237 [00:27<00:20,  4.58it/s]Epoch: 1, train for the 360-th batch, train loss: 0.3375636637210846:  94%|██████████▎| 360/383 [01:51<00:09,  2.47it/s]evaluate for the 24-th batch, evaluate loss: 0.6223201751708984:  92%|████████████████▌ | 23/25 [00:02<00:00, 11.00it/s]Epoch: 2, train for the 146-th batch, train loss: 0.5329474806785583:  62%|██████▊    | 146/237 [00:27<00:19,  4.71it/s]evaluate for the 47-th batch, evaluate loss: 0.45336300134658813:  64%|██████████▊      | 46/72 [00:04<00:02,  9.13it/s]Epoch: 5, train for the 47-th batch, train loss: 0.4538724720478058:  39%|█████        | 46/119 [00:08<00:11,  6.27it/s]Epoch: 5, train for the 47-th batch, train loss: 0.4538724720478058:  39%|█████▏       | 47/119 [00:08<00:11,  6.22it/s]Epoch: 4, train for the 129-th batch, train loss: 0.5052009224891663:  88%|█████████▋ | 128/146 [00:20<00:02,  6.47it/s]evaluate for the 48-th batch, evaluate loss: 0.48304739594459534:  64%|██████████▊      | 46/72 [00:04<00:02,  9.13it/s]evaluate for the 48-th batch, evaluate loss: 0.48304739594459534:  67%|███████████▎     | 48/72 [00:04<00:02, 10.05it/s]Epoch: 4, train for the 129-th batch, train loss: 0.5052009224891663:  88%|█████████▋ | 129/146 [00:20<00:02,  6.44it/s]evaluate for the 25-th batch, evaluate loss: 0.5502264499664307:  92%|████████████████▌ | 23/25 [00:02<00:00, 11.00it/s]evaluate for the 25-th batch, evaluate loss: 0.5502264499664307: 100%|██████████████████| 25/25 [00:02<00:00, 11.07it/s]evaluate for the 25-th batch, evaluate loss: 0.5502264499664307: 100%|██████████████████| 25/25 [00:02<00:00, 10.93it/s]
Epoch: 2, train for the 147-th batch, train loss: 0.5847470164299011:  62%|██████▊    | 146/237 [00:27<00:19,  4.71it/s]Epoch: 2, train for the 147-th batch, train loss: 0.5847470164299011:  62%|██████▊    | 147/237 [00:27<00:18,  4.77it/s]Epoch: 5, train for the 48-th batch, train loss: 0.39510130882263184:  39%|████▋       | 47/119 [00:08<00:11,  6.22it/s]Epoch: 5, train for the 48-th batch, train loss: 0.39510130882263184:  40%|████▊       | 48/119 [00:08<00:11,  6.18it/s]Epoch: 4, train for the 130-th batch, train loss: 0.4174630343914032:  88%|█████████▋ | 129/146 [00:21<00:02,  6.44it/s]Epoch: 4, train for the 130-th batch, train loss: 0.4174630343914032:  89%|█████████▊ | 130/146 [00:21<00:02,  6.01it/s]INFO:root:Epoch: 3, learning rate: 0.0001, train loss: 0.5540
INFO:root:train average_precision, 0.7898
INFO:root:train roc_auc, 0.7675
INFO:root:validate loss: 0.6253
INFO:root:validate average_precision, 0.7280
INFO:root:validate roc_auc, 0.7701
INFO:root:new node validate loss: 0.8020
INFO:root:new node validate first_1_average_precision, 0.6908
INFO:root:new node validate first_1_roc_auc, 0.7063
INFO:root:new node validate first_3_average_precision, 0.6457
INFO:root:new node validate first_3_roc_auc, 0.6384
INFO:root:new node validate first_10_average_precision, 0.6397
INFO:root:new node validate first_10_roc_auc, 0.6330
INFO:root:new node validate average_precision, 0.6742
INFO:root:new node validate roc_auc, 0.6687
evaluate for the 49-th batch, evaluate loss: 0.6073307394981384:  67%|████████████      | 48/72 [00:04<00:02, 10.05it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]Epoch: 5, train for the 49-th batch, train loss: 0.45865973830223083:  40%|████▊       | 48/119 [00:08<00:11,  6.18it/s]Epoch: 5, train for the 49-th batch, train loss: 0.45865973830223083:  41%|████▉       | 49/119 [00:08<00:10,  6.66it/s]Epoch: 2, train for the 148-th batch, train loss: 0.54056316614151:  62%|████████     | 147/237 [00:27<00:18,  4.77it/s]evaluate for the 50-th batch, evaluate loss: 0.49590355157852173:  67%|███████████▎     | 48/72 [00:04<00:02, 10.05it/s]evaluate for the 50-th batch, evaluate loss: 0.49590355157852173:  69%|███████████▊     | 50/72 [00:04<00:02,  8.80it/s]Epoch: 1, train for the 361-th batch, train loss: 0.40384748578071594:  94%|█████████▍| 360/383 [01:51<00:09,  2.47it/s]Epoch: 2, train for the 148-th batch, train loss: 0.54056316614151:  62%|████████     | 148/237 [00:27<00:18,  4.90it/s]evaluate for the 51-th batch, evaluate loss: 0.5839725732803345:  69%|████████████▌     | 50/72 [00:04<00:02,  8.80it/s]Epoch: 1, train for the 361-th batch, train loss: 0.40384748578071594:  94%|█████████▍| 361/383 [01:51<00:09,  2.33it/s]Epoch: 4, train for the 1-th batch, train loss: 0.7620744109153748:   0%|                       | 0/151 [00:00<?, ?it/s]Epoch: 4, train for the 131-th batch, train loss: 0.49202781915664673:  89%|████████▉ | 130/146 [00:21<00:02,  6.01it/s]Epoch: 4, train for the 1-th batch, train loss: 0.7620744109153748:   1%|               | 1/151 [00:00<00:22,  6.81it/s]Epoch: 4, train for the 131-th batch, train loss: 0.49202781915664673:  90%|████████▉ | 131/146 [00:21<00:02,  5.73it/s]Epoch: 5, train for the 50-th batch, train loss: 0.365344762802124:  41%|█████▊        | 49/119 [00:08<00:10,  6.66it/s]Epoch: 5, train for the 50-th batch, train loss: 0.365344762802124:  42%|█████▉        | 50/119 [00:08<00:10,  6.42it/s]evaluate for the 52-th batch, evaluate loss: 0.5789158940315247:  69%|████████████▌     | 50/72 [00:04<00:02,  8.80it/s]evaluate for the 52-th batch, evaluate loss: 0.5789158940315247:  72%|█████████████     | 52/72 [00:04<00:02,  9.71it/s]Epoch: 2, train for the 149-th batch, train loss: 0.5622053146362305:  62%|██████▊    | 148/237 [00:27<00:18,  4.90it/s]Epoch: 2, train for the 149-th batch, train loss: 0.5622053146362305:  63%|██████▉    | 149/237 [00:27<00:17,  4.92it/s]Epoch: 4, train for the 132-th batch, train loss: 0.49326109886169434:  90%|████████▉ | 131/146 [00:21<00:02,  5.73it/s]Epoch: 4, train for the 132-th batch, train loss: 0.49326109886169434:  90%|█████████ | 132/146 [00:21<00:02,  5.90it/s]evaluate for the 53-th batch, evaluate loss: 0.611973226070404:  72%|█████████████▋     | 52/72 [00:04<00:02,  9.71it/s]Epoch: 4, train for the 2-th batch, train loss: 0.7638222575187683:   1%|               | 1/151 [00:00<00:22,  6.81it/s]Epoch: 4, train for the 2-th batch, train loss: 0.7638222575187683:   1%|▏              | 2/151 [00:00<00:24,  6.01it/s]Epoch: 5, train for the 51-th batch, train loss: 0.4677981436252594:  42%|█████▍       | 50/119 [00:08<00:10,  6.42it/s]Epoch: 5, train for the 51-th batch, train loss: 0.4677981436252594:  43%|█████▌       | 51/119 [00:08<00:11,  5.98it/s]Epoch: 4, train for the 3-th batch, train loss: 0.7465571165084839:   1%|▏              | 2/151 [00:00<00:24,  6.01it/s]Epoch: 1, train for the 362-th batch, train loss: 0.3718547821044922:  94%|██████████▎| 361/383 [01:52<00:09,  2.33it/s]Epoch: 4, train for the 3-th batch, train loss: 0.7465571165084839:   2%|▎              | 3/151 [00:00<00:20,  7.14it/s]Epoch: 4, train for the 133-th batch, train loss: 0.45902812480926514:  90%|█████████ | 132/146 [00:21<00:02,  5.90it/s]Epoch: 4, train for the 133-th batch, train loss: 0.45902812480926514:  91%|█████████ | 133/146 [00:21<00:02,  6.21it/s]Epoch: 2, train for the 150-th batch, train loss: 0.5182363986968994:  63%|██████▉    | 149/237 [00:27<00:17,  4.92it/s]Epoch: 1, train for the 362-th batch, train loss: 0.3718547821044922:  95%|██████████▍| 362/383 [01:52<00:08,  2.47it/s]Epoch: 2, train for the 150-th batch, train loss: 0.5182363986968994:  63%|██████▉    | 150/237 [00:27<00:18,  4.81it/s]Epoch: 5, train for the 52-th batch, train loss: 0.4346831738948822:  43%|█████▌       | 51/119 [00:09<00:11,  5.98it/s]Epoch: 5, train for the 52-th batch, train loss: 0.4346831738948822:  44%|█████▋       | 52/119 [00:09<00:10,  6.39it/s]Epoch: 4, train for the 4-th batch, train loss: 0.7107580900192261:   2%|▎              | 3/151 [00:00<00:20,  7.14it/s]Epoch: 4, train for the 134-th batch, train loss: 0.49118033051490784:  91%|█████████ | 133/146 [00:21<00:02,  6.21it/s]Epoch: 4, train for the 134-th batch, train loss: 0.49118033051490784:  92%|█████████▏| 134/146 [00:21<00:01,  6.51it/s]Epoch: 4, train for the 5-th batch, train loss: 0.7182645201683044:   2%|▎              | 3/151 [00:00<00:20,  7.14it/s]Epoch: 4, train for the 5-th batch, train loss: 0.7182645201683044:   3%|▍              | 5/151 [00:00<00:16,  8.61it/s]Epoch: 5, train for the 53-th batch, train loss: 0.35026949644088745:  44%|█████▏      | 52/119 [00:09<00:10,  6.39it/s]Epoch: 5, train for the 53-th batch, train loss: 0.35026949644088745:  45%|█████▎      | 53/119 [00:09<00:10,  6.56it/s]Epoch: 2, train for the 151-th batch, train loss: 0.5852131247520447:  63%|██████▉    | 150/237 [00:28<00:18,  4.81it/s]evaluate for the 54-th batch, evaluate loss: 0.7094908952713013:  72%|█████████████     | 52/72 [00:05<00:02,  9.71it/s]evaluate for the 54-th batch, evaluate loss: 0.7094908952713013:  75%|█████████████▌    | 54/72 [00:05<00:02,  6.81it/s]Epoch: 2, train for the 151-th batch, train loss: 0.5852131247520447:  64%|███████    | 151/237 [00:28<00:18,  4.63it/s]Epoch: 4, train for the 135-th batch, train loss: 0.4825115203857422:  92%|██████████ | 134/146 [00:21<00:01,  6.51it/s]Epoch: 4, train for the 135-th batch, train loss: 0.4825115203857422:  92%|██████████▏| 135/146 [00:21<00:01,  6.60it/s]Epoch: 4, train for the 6-th batch, train loss: 0.7198481559753418:   3%|▍              | 5/151 [00:00<00:16,  8.61it/s]Epoch: 4, train for the 6-th batch, train loss: 0.7198481559753418:   4%|▌              | 6/151 [00:00<00:17,  8.47it/s]evaluate for the 55-th batch, evaluate loss: 0.6352173089981079:  75%|█████████████▌    | 54/72 [00:05<00:02,  6.81it/s]Epoch: 1, train for the 363-th batch, train loss: 0.44250503182411194:  95%|█████████▍| 362/383 [01:52<00:08,  2.47it/s]Epoch: 5, train for the 54-th batch, train loss: 0.38194745779037476:  45%|█████▎      | 53/119 [00:09<00:10,  6.56it/s]Epoch: 5, train for the 54-th batch, train loss: 0.38194745779037476:  45%|█████▍      | 54/119 [00:09<00:10,  6.32it/s]Epoch: 1, train for the 363-th batch, train loss: 0.44250503182411194:  95%|█████████▍| 363/383 [01:52<00:07,  2.51it/s]Epoch: 2, train for the 152-th batch, train loss: 0.5467674732208252:  64%|███████    | 151/237 [00:28<00:18,  4.63it/s]evaluate for the 56-th batch, evaluate loss: 0.6798944473266602:  75%|█████████████▌    | 54/72 [00:05<00:02,  6.81it/s]evaluate for the 56-th batch, evaluate loss: 0.6798944473266602:  78%|██████████████    | 56/72 [00:05<00:02,  7.66it/s]Epoch: 4, train for the 136-th batch, train loss: 0.4941900074481964:  92%|██████████▏| 135/146 [00:22<00:01,  6.60it/s]Epoch: 4, train for the 7-th batch, train loss: 0.6809929609298706:   4%|▌              | 6/151 [00:00<00:17,  8.47it/s]Epoch: 4, train for the 136-th batch, train loss: 0.4941900074481964:  93%|██████████▏| 136/146 [00:22<00:01,  6.40it/s]Epoch: 4, train for the 7-th batch, train loss: 0.6809929609298706:   5%|▋              | 7/151 [00:00<00:18,  7.73it/s]Epoch: 2, train for the 152-th batch, train loss: 0.5467674732208252:  64%|███████    | 152/237 [00:28<00:17,  4.76it/s]Epoch: 5, train for the 55-th batch, train loss: 0.3767605125904083:  45%|█████▉       | 54/119 [00:09<00:10,  6.32it/s]evaluate for the 57-th batch, evaluate loss: 0.6548664569854736:  78%|██████████████    | 56/72 [00:05<00:02,  7.66it/s]evaluate for the 57-th batch, evaluate loss: 0.6548664569854736:  79%|██████████████▎   | 57/72 [00:05<00:01,  7.96it/s]Epoch: 5, train for the 55-th batch, train loss: 0.3767605125904083:  46%|██████       | 55/119 [00:09<00:10,  6.12it/s]Epoch: 4, train for the 137-th batch, train loss: 0.4447343349456787:  93%|██████████▏| 136/146 [00:22<00:01,  6.40it/s]Epoch: 4, train for the 8-th batch, train loss: 0.6945009231567383:   5%|▋              | 7/151 [00:01<00:18,  7.73it/s]Epoch: 4, train for the 8-th batch, train loss: 0.6945009231567383:   5%|▊              | 8/151 [00:01<00:19,  7.33it/s]Epoch: 4, train for the 137-th batch, train loss: 0.4447343349456787:  94%|██████████▎| 137/146 [00:22<00:01,  6.38it/s]Epoch: 2, train for the 153-th batch, train loss: 0.5214425325393677:  64%|███████    | 152/237 [00:28<00:17,  4.76it/s]Epoch: 5, train for the 56-th batch, train loss: 0.39017242193222046:  46%|█████▌      | 55/119 [00:09<00:10,  6.12it/s]Epoch: 5, train for the 56-th batch, train loss: 0.39017242193222046:  47%|█████▋      | 56/119 [00:09<00:10,  6.24it/s]Epoch: 1, train for the 364-th batch, train loss: 0.34767863154411316:  95%|█████████▍| 363/383 [01:52<00:07,  2.51it/s]Epoch: 2, train for the 153-th batch, train loss: 0.5214425325393677:  65%|███████    | 153/237 [00:28<00:18,  4.53it/s]Epoch: 4, train for the 138-th batch, train loss: 0.5387723445892334:  94%|██████████▎| 137/146 [00:22<00:01,  6.38it/s]Epoch: 4, train for the 138-th batch, train loss: 0.5387723445892334:  95%|██████████▍| 138/146 [00:22<00:01,  6.70it/s]Epoch: 1, train for the 364-th batch, train loss: 0.34767863154411316:  95%|█████████▌| 364/383 [01:52<00:07,  2.61it/s]Epoch: 4, train for the 9-th batch, train loss: 0.6602736711502075:   5%|▊              | 8/151 [00:01<00:19,  7.33it/s]Epoch: 4, train for the 9-th batch, train loss: 0.6602736711502075:   6%|▉              | 9/151 [00:01<00:22,  6.24it/s]evaluate for the 58-th batch, evaluate loss: 0.6545376181602478:  79%|██████████████▎   | 57/72 [00:05<00:01,  7.96it/s]evaluate for the 58-th batch, evaluate loss: 0.6545376181602478:  81%|██████████████▌   | 58/72 [00:05<00:02,  6.26it/s]Epoch: 5, train for the 57-th batch, train loss: 0.39477309584617615:  47%|█████▋      | 56/119 [00:09<00:10,  6.24it/s]Epoch: 5, train for the 57-th batch, train loss: 0.39477309584617615:  48%|█████▋      | 57/119 [00:09<00:10,  5.86it/s]Epoch: 4, train for the 139-th batch, train loss: 0.5005300045013428:  95%|██████████▍| 138/146 [00:22<00:01,  6.70it/s]Epoch: 4, train for the 139-th batch, train loss: 0.5005300045013428:  95%|██████████▍| 139/146 [00:22<00:01,  6.38it/s]Epoch: 2, train for the 154-th batch, train loss: 0.5766528844833374:  65%|███████    | 153/237 [00:28<00:18,  4.53it/s]evaluate for the 59-th batch, evaluate loss: 0.6043499708175659:  81%|██████████████▌   | 58/72 [00:05<00:02,  6.26it/s]Epoch: 2, train for the 154-th batch, train loss: 0.5766528844833374:  65%|███████▏   | 154/237 [00:28<00:18,  4.45it/s]Epoch: 4, train for the 10-th batch, train loss: 0.6585773825645447:   6%|▊             | 9/151 [00:01<00:22,  6.24it/s]Epoch: 4, train for the 10-th batch, train loss: 0.6585773825645447:   7%|▊            | 10/151 [00:01<00:21,  6.53it/s]evaluate for the 60-th batch, evaluate loss: 0.58069908618927:  81%|████████████████    | 58/72 [00:06<00:02,  6.26it/s]evaluate for the 60-th batch, evaluate loss: 0.58069908618927:  83%|████████████████▋   | 60/72 [00:06<00:01,  7.52it/s]Epoch: 1, train for the 365-th batch, train loss: 0.3544445037841797:  95%|██████████▍| 364/383 [01:53<00:07,  2.61it/s]Epoch: 5, train for the 58-th batch, train loss: 0.4020257890224457:  48%|██████▏      | 57/119 [00:10<00:10,  5.86it/s]Epoch: 4, train for the 11-th batch, train loss: 0.6402689814567566:   7%|▊            | 10/151 [00:01<00:21,  6.53it/s]Epoch: 5, train for the 58-th batch, train loss: 0.4020257890224457:  49%|██████▎      | 58/119 [00:10<00:10,  5.78it/s]Epoch: 4, train for the 11-th batch, train loss: 0.6402689814567566:   7%|▉            | 11/151 [00:01<00:19,  7.01it/s]Epoch: 1, train for the 365-th batch, train loss: 0.3544445037841797:  95%|██████████▍| 365/383 [01:53<00:06,  2.72it/s]Epoch: 4, train for the 140-th batch, train loss: 0.4749584197998047:  95%|██████████▍| 139/146 [00:22<00:01,  6.38it/s]Epoch: 2, train for the 155-th batch, train loss: 0.5536243319511414:  65%|███████▏   | 154/237 [00:28<00:18,  4.45it/s]Epoch: 4, train for the 140-th batch, train loss: 0.4749584197998047:  96%|██████████▌| 140/146 [00:22<00:01,  5.90it/s]Epoch: 2, train for the 155-th batch, train loss: 0.5536243319511414:  65%|███████▏   | 155/237 [00:28<00:17,  4.67it/s]evaluate for the 61-th batch, evaluate loss: 0.5242382884025574:  83%|███████████████   | 60/72 [00:06<00:01,  7.52it/s]evaluate for the 61-th batch, evaluate loss: 0.5242382884025574:  85%|███████████████▎  | 61/72 [00:06<00:01,  7.07it/s]Epoch: 5, train for the 59-th batch, train loss: 0.4054871201515198:  49%|██████▎      | 58/119 [00:10<00:10,  5.78it/s]Epoch: 5, train for the 59-th batch, train loss: 0.4054871201515198:  50%|██████▍      | 59/119 [00:10<00:09,  6.26it/s]Epoch: 4, train for the 12-th batch, train loss: 0.6472370028495789:   7%|▉            | 11/151 [00:01<00:19,  7.01it/s]Epoch: 4, train for the 141-th batch, train loss: 0.4952562749385834:  96%|██████████▌| 140/146 [00:22<00:01,  5.90it/s]Epoch: 4, train for the 12-th batch, train loss: 0.6472370028495789:   8%|█            | 12/151 [00:01<00:21,  6.47it/s]evaluate for the 62-th batch, evaluate loss: 0.5640607476234436:  85%|███████████████▎  | 61/72 [00:06<00:01,  7.07it/s]Epoch: 4, train for the 141-th batch, train loss: 0.4952562749385834:  97%|██████████▌| 141/146 [00:22<00:00,  5.96it/s]evaluate for the 63-th batch, evaluate loss: 0.5650407075881958:  85%|███████████████▎  | 61/72 [00:06<00:01,  7.07it/s]evaluate for the 63-th batch, evaluate loss: 0.5650407075881958:  88%|███████████████▊  | 63/72 [00:06<00:01,  8.52it/s]Epoch: 2, train for the 156-th batch, train loss: 0.5522529482841492:  65%|███████▏   | 155/237 [00:29<00:17,  4.67it/s]Epoch: 5, train for the 60-th batch, train loss: 0.38231635093688965:  50%|█████▉      | 59/119 [00:10<00:09,  6.26it/s]Epoch: 5, train for the 60-th batch, train loss: 0.38231635093688965:  50%|██████      | 60/119 [00:10<00:09,  6.10it/s]Epoch: 2, train for the 156-th batch, train loss: 0.5522529482841492:  66%|███████▏   | 156/237 [00:29<00:17,  4.53it/s]Epoch: 1, train for the 366-th batch, train loss: 0.47423669695854187:  95%|█████████▌| 365/383 [01:53<00:06,  2.72it/s]Epoch: 4, train for the 142-th batch, train loss: 0.4619775414466858:  97%|██████████▌| 141/146 [00:23<00:00,  5.96it/s]Epoch: 4, train for the 13-th batch, train loss: 0.6567996740341187:   8%|█            | 12/151 [00:01<00:21,  6.47it/s]Epoch: 4, train for the 142-th batch, train loss: 0.4619775414466858:  97%|██████████▋| 142/146 [00:23<00:00,  6.13it/s]evaluate for the 64-th batch, evaluate loss: 0.6010330319404602:  88%|███████████████▊  | 63/72 [00:06<00:01,  8.52it/s]Epoch: 4, train for the 13-th batch, train loss: 0.6567996740341187:   9%|█            | 13/151 [00:01<00:22,  6.21it/s]Epoch: 1, train for the 366-th batch, train loss: 0.47423669695854187:  96%|█████████▌| 366/383 [01:53<00:06,  2.73it/s]Epoch: 5, train for the 61-th batch, train loss: 0.3762776851654053:  50%|██████▌      | 60/119 [00:10<00:09,  6.10it/s]evaluate for the 65-th batch, evaluate loss: 0.6352497935295105:  88%|███████████████▊  | 63/72 [00:06<00:01,  8.52it/s]evaluate for the 65-th batch, evaluate loss: 0.6352497935295105:  90%|████████████████▎ | 65/72 [00:06<00:00,  9.31it/s]Epoch: 5, train for the 61-th batch, train loss: 0.3762776851654053:  51%|██████▋      | 61/119 [00:10<00:09,  6.34it/s]Epoch: 2, train for the 157-th batch, train loss: 0.5415873527526855:  66%|███████▏   | 156/237 [00:29<00:17,  4.53it/s]Epoch: 4, train for the 14-th batch, train loss: 0.6546415686607361:   9%|█            | 13/151 [00:02<00:22,  6.21it/s]Epoch: 4, train for the 143-th batch, train loss: 0.5060006380081177:  97%|██████████▋| 142/146 [00:23<00:00,  6.13it/s]Epoch: 4, train for the 14-th batch, train loss: 0.6546415686607361:   9%|█▏           | 14/151 [00:02<00:21,  6.35it/s]evaluate for the 66-th batch, evaluate loss: 0.5184099674224854:  90%|████████████████▎ | 65/72 [00:06<00:00,  9.31it/s]Epoch: 4, train for the 143-th batch, train loss: 0.5060006380081177:  98%|██████████▊| 143/146 [00:23<00:00,  6.14it/s]Epoch: 2, train for the 157-th batch, train loss: 0.5415873527526855:  66%|███████▎   | 157/237 [00:29<00:17,  4.50it/s]evaluate for the 67-th batch, evaluate loss: 0.6399712562561035:  90%|████████████████▎ | 65/72 [00:06<00:00,  9.31it/s]evaluate for the 67-th batch, evaluate loss: 0.6399712562561035:  93%|████████████████▊ | 67/72 [00:06<00:00, 10.35it/s]Epoch: 5, train for the 62-th batch, train loss: 0.4298563301563263:  51%|██████▋      | 61/119 [00:10<00:09,  6.34it/s]Epoch: 5, train for the 62-th batch, train loss: 0.4298563301563263:  52%|██████▊      | 62/119 [00:10<00:09,  6.30it/s]Epoch: 1, train for the 367-th batch, train loss: 0.3005247414112091:  96%|██████████▌| 366/383 [01:53<00:06,  2.73it/s]Epoch: 4, train for the 15-th batch, train loss: 0.6350024938583374:   9%|█▏           | 14/151 [00:02<00:21,  6.35it/s]Epoch: 4, train for the 15-th batch, train loss: 0.6350024938583374:  10%|█▎           | 15/151 [00:02<00:20,  6.65it/s]Epoch: 4, train for the 144-th batch, train loss: 0.46382957696914673:  98%|█████████▊| 143/146 [00:23<00:00,  6.14it/s]Epoch: 1, train for the 367-th batch, train loss: 0.3005247414112091:  96%|██████████▌| 367/383 [01:53<00:05,  2.88it/s]Epoch: 2, train for the 158-th batch, train loss: 0.5520798563957214:  66%|███████▎   | 157/237 [00:29<00:17,  4.50it/s]Epoch: 4, train for the 144-th batch, train loss: 0.46382957696914673:  99%|█████████▊| 144/146 [00:23<00:00,  6.03it/s]Epoch: 2, train for the 158-th batch, train loss: 0.5520798563957214:  67%|███████▎   | 158/237 [00:29<00:16,  4.70it/s]Epoch: 5, train for the 63-th batch, train loss: 0.40391379594802856:  52%|██████▎     | 62/119 [00:10<00:09,  6.30it/s]Epoch: 4, train for the 16-th batch, train loss: 0.6345998048782349:  10%|█▎           | 15/151 [00:02<00:20,  6.65it/s]Epoch: 5, train for the 63-th batch, train loss: 0.40391379594802856:  53%|██████▎     | 63/119 [00:10<00:08,  6.36it/s]Epoch: 4, train for the 17-th batch, train loss: 0.6457838416099548:  10%|█▎           | 15/151 [00:02<00:20,  6.65it/s]Epoch: 4, train for the 145-th batch, train loss: 0.4696725308895111:  99%|██████████▊| 144/146 [00:23<00:00,  6.03it/s]Epoch: 4, train for the 17-th batch, train loss: 0.6457838416099548:  11%|█▍           | 17/151 [00:02<00:17,  7.55it/s]Epoch: 2, train for the 159-th batch, train loss: 0.5720818042755127:  67%|███████▎   | 158/237 [00:29<00:16,  4.70it/s]Epoch: 4, train for the 145-th batch, train loss: 0.4696725308895111:  99%|██████████▉| 145/146 [00:23<00:00,  5.70it/s]evaluate for the 68-th batch, evaluate loss: 0.5804797410964966:  93%|████████████████▊ | 67/72 [00:06<00:00, 10.35it/s]Epoch: 5, train for the 64-th batch, train loss: 0.4317636787891388:  53%|██████▉      | 63/119 [00:11<00:08,  6.36it/s]Epoch: 2, train for the 159-th batch, train loss: 0.5720818042755127:  67%|███████▍   | 159/237 [00:29<00:16,  4.70it/s]Epoch: 1, train for the 368-th batch, train loss: 0.5208442807197571:  96%|██████████▌| 367/383 [01:54<00:05,  2.88it/s]Epoch: 5, train for the 64-th batch, train loss: 0.4317636787891388:  54%|██████▉      | 64/119 [00:11<00:09,  6.08it/s]evaluate for the 69-th batch, evaluate loss: 0.6624377369880676:  93%|████████████████▊ | 67/72 [00:07<00:00, 10.35it/s]evaluate for the 69-th batch, evaluate loss: 0.6624377369880676:  96%|█████████████████▎| 69/72 [00:07<00:00,  7.72it/s]Epoch: 1, train for the 368-th batch, train loss: 0.5208442807197571:  96%|██████████▌| 368/383 [01:54<00:05,  2.94it/s]Epoch: 4, train for the 146-th batch, train loss: 0.46865707635879517:  99%|█████████▉| 145/146 [00:23<00:00,  5.70it/s]Epoch: 4, train for the 146-th batch, train loss: 0.46865707635879517: 100%|██████████| 146/146 [00:23<00:00,  5.97it/s]Epoch: 4, train for the 146-th batch, train loss: 0.46865707635879517: 100%|██████████| 146/146 [00:23<00:00,  6.16it/s]
Epoch: 4, train for the 18-th batch, train loss: 0.6619819402694702:  11%|█▍           | 17/151 [00:02<00:17,  7.55it/s]evaluate for the 70-th batch, evaluate loss: 0.6290419101715088:  96%|█████████████████▎| 69/72 [00:07<00:00,  7.72it/s]Epoch: 4, train for the 18-th batch, train loss: 0.6619819402694702:  12%|█▌           | 18/151 [00:02<00:19,  6.90it/s]Epoch: 5, train for the 65-th batch, train loss: 0.40810707211494446:  54%|██████▍     | 64/119 [00:11<00:09,  6.08it/s]Epoch: 5, train for the 65-th batch, train loss: 0.40810707211494446:  55%|██████▌     | 65/119 [00:11<00:08,  6.39it/s]Epoch: 2, train for the 160-th batch, train loss: 0.592269778251648:  67%|████████    | 159/237 [00:30<00:16,  4.70it/s]evaluate for the 71-th batch, evaluate loss: 0.6331188082695007:  96%|█████████████████▎| 69/72 [00:07<00:00,  7.72it/s]evaluate for the 71-th batch, evaluate loss: 0.6331188082695007:  99%|█████████████████▊| 71/72 [00:07<00:00,  9.07it/s]Epoch: 2, train for the 160-th batch, train loss: 0.592269778251648:  68%|████████    | 160/237 [00:30<00:16,  4.66it/s]evaluate for the 72-th batch, evaluate loss: 0.6636196970939636:  99%|█████████████████▊| 71/72 [00:07<00:00,  9.07it/s]evaluate for the 72-th batch, evaluate loss: 0.6636196970939636: 100%|██████████████████| 72/72 [00:07<00:00,  9.89it/s]
  0%|                                                                                            | 0/38 [00:00<?, ?it/s]Epoch: 4, train for the 19-th batch, train loss: 0.6131906509399414:  12%|█▌           | 18/151 [00:02<00:19,  6.90it/s]Epoch: 5, train for the 66-th batch, train loss: 0.4005831182003021:  55%|███████      | 65/119 [00:11<00:08,  6.39it/s]Epoch: 4, train for the 19-th batch, train loss: 0.6131906509399414:  13%|█▋           | 19/151 [00:02<00:21,  6.27it/s]Epoch: 5, train for the 66-th batch, train loss: 0.4005831182003021:  55%|███████▏     | 66/119 [00:11<00:08,  6.08it/s]evaluate for the 1-th batch, evaluate loss: 0.7244129180908203:   0%|                            | 0/38 [00:00<?, ?it/s]Epoch: 1, train for the 369-th batch, train loss: 0.34005048871040344:  96%|█████████▌| 368/383 [01:54<00:05,  2.94it/s]Epoch: 2, train for the 161-th batch, train loss: 0.5780715346336365:  68%|███████▍   | 160/237 [00:30<00:16,  4.66it/s]evaluate for the 2-th batch, evaluate loss: 0.6897292733192444:   0%|                            | 0/38 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.6897292733192444:   5%|█                   | 2/38 [00:00<00:02, 15.61it/s]Epoch: 2, train for the 161-th batch, train loss: 0.5780715346336365:  68%|███████▍   | 161/237 [00:30<00:16,  4.63it/s]Epoch: 4, train for the 20-th batch, train loss: 0.6310372948646545:  13%|█▋           | 19/151 [00:02<00:21,  6.27it/s]Epoch: 1, train for the 369-th batch, train loss: 0.34005048871040344:  96%|█████████▋| 369/383 [01:54<00:04,  2.88it/s]Epoch: 4, train for the 20-th batch, train loss: 0.6310372948646545:  13%|█▋           | 20/151 [00:02<00:19,  6.61it/s]Epoch: 5, train for the 67-th batch, train loss: 0.5025202035903931:  55%|███████▏     | 66/119 [00:11<00:08,  6.08it/s]evaluate for the 3-th batch, evaluate loss: 0.6149739027023315:   5%|█                   | 2/38 [00:00<00:02, 15.61it/s]Epoch: 5, train for the 67-th batch, train loss: 0.5025202035903931:  56%|███████▎     | 67/119 [00:11<00:08,  6.08it/s]evaluate for the 4-th batch, evaluate loss: 0.6182687282562256:   5%|█                   | 2/38 [00:00<00:02, 15.61it/s]evaluate for the 4-th batch, evaluate loss: 0.6182687282562256:  11%|██                  | 4/38 [00:00<00:02, 14.76it/s]Epoch: 4, train for the 21-th batch, train loss: 0.6215197443962097:  13%|█▋           | 20/151 [00:03<00:19,  6.61it/s]Epoch: 4, train for the 21-th batch, train loss: 0.6215197443962097:  14%|█▊           | 21/151 [00:03<00:20,  6.41it/s]Epoch: 2, train for the 162-th batch, train loss: 0.5700785517692566:  68%|███████▍   | 161/237 [00:30<00:16,  4.63it/s]Epoch: 2, train for the 162-th batch, train loss: 0.5700785517692566:  68%|███████▌   | 162/237 [00:30<00:16,  4.64it/s]evaluate for the 5-th batch, evaluate loss: 0.7019789218902588:  11%|██                  | 4/38 [00:00<00:02, 14.76it/s]Epoch: 5, train for the 68-th batch, train loss: 0.3448966145515442:  56%|███████▎     | 67/119 [00:11<00:08,  6.08it/s]Epoch: 5, train for the 68-th batch, train loss: 0.3448966145515442:  57%|███████▍     | 68/119 [00:11<00:08,  5.75it/s]evaluate for the 6-th batch, evaluate loss: 0.6420024633407593:  11%|██                  | 4/38 [00:00<00:02, 14.76it/s]evaluate for the 6-th batch, evaluate loss: 0.6420024633407593:  16%|███▏                | 6/38 [00:00<00:02, 13.83it/s]Epoch: 4, train for the 22-th batch, train loss: 0.5948145389556885:  14%|█▊           | 21/151 [00:03<00:20,  6.41it/s]Epoch: 4, train for the 22-th batch, train loss: 0.5948145389556885:  15%|█▉           | 22/151 [00:03<00:19,  6.71it/s]evaluate for the 7-th batch, evaluate loss: 0.6096616983413696:  16%|███▏                | 6/38 [00:00<00:02, 13.83it/s]Epoch: 1, train for the 370-th batch, train loss: 0.35079240798950195:  96%|█████████▋| 369/383 [01:54<00:04,  2.88it/s]Epoch: 5, train for the 69-th batch, train loss: 0.3842275142669678:  57%|███████▍     | 68/119 [00:11<00:08,  5.75it/s]evaluate for the 8-th batch, evaluate loss: 0.5890348553657532:  16%|███▏                | 6/38 [00:00<00:02, 13.83it/s]evaluate for the 8-th batch, evaluate loss: 0.5890348553657532:  21%|████▏               | 8/38 [00:00<00:02, 14.14it/s]Epoch: 5, train for the 69-th batch, train loss: 0.3842275142669678:  58%|███████▌     | 69/119 [00:11<00:08,  5.72it/s]  0%|                                                                                            | 0/34 [00:00<?, ?it/s]Epoch: 4, train for the 23-th batch, train loss: 0.6126537322998047:  15%|█▉           | 22/151 [00:03<00:19,  6.71it/s]Epoch: 4, train for the 23-th batch, train loss: 0.6126537322998047:  15%|█▉           | 23/151 [00:03<00:18,  6.78it/s]evaluate for the 9-th batch, evaluate loss: 0.6324933767318726:  21%|████▏               | 8/38 [00:00<00:02, 14.14it/s]Epoch: 1, train for the 370-th batch, train loss: 0.35079240798950195:  97%|█████████▋| 370/383 [01:55<00:05,  2.59it/s]evaluate for the 1-th batch, evaluate loss: 0.8613404631614685:   0%|                            | 0/34 [00:00<?, ?it/s]evaluate for the 10-th batch, evaluate loss: 0.6106047034263611:  21%|████               | 8/38 [00:00<00:02, 14.14it/s]evaluate for the 10-th batch, evaluate loss: 0.6106047034263611:  26%|████▋             | 10/38 [00:00<00:01, 15.38it/s]evaluate for the 11-th batch, evaluate loss: 0.5221072435379028:  26%|████▋             | 10/38 [00:00<00:01, 15.38it/s]evaluate for the 12-th batch, evaluate loss: 0.6217578053474426:  26%|████▋             | 10/38 [00:00<00:01, 15.38it/s]Epoch: 4, train for the 24-th batch, train loss: 0.6021796464920044:  15%|█▉           | 23/151 [00:03<00:18,  6.78it/s]evaluate for the 2-th batch, evaluate loss: 0.9398176074028015:   0%|                            | 0/34 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.9398176074028015:   6%|█▏                  | 2/34 [00:00<00:03, 10.42it/s]Epoch: 4, train for the 24-th batch, train loss: 0.6021796464920044:  16%|██           | 24/151 [00:03<00:19,  6.38it/s]Epoch: 2, train for the 163-th batch, train loss: 0.6154356002807617:  68%|███████▌   | 162/237 [00:31<00:16,  4.64it/s]evaluate for the 13-th batch, evaluate loss: 0.5961170792579651:  26%|████▋             | 10/38 [00:00<00:01, 15.38it/s]evaluate for the 13-th batch, evaluate loss: 0.5961170792579651:  34%|██████▏           | 13/38 [00:00<00:01, 16.17it/s]evaluate for the 3-th batch, evaluate loss: 0.9425318837165833:   6%|█▏                  | 2/34 [00:00<00:03, 10.42it/s]Epoch: 2, train for the 163-th batch, train loss: 0.6154356002807617:  69%|███████▌   | 163/237 [00:31<00:22,  3.25it/s]Epoch: 5, train for the 70-th batch, train loss: 0.3436351716518402:  58%|███████▌     | 69/119 [00:12<00:08,  5.72it/s]evaluate for the 14-th batch, evaluate loss: 0.5277613401412964:  34%|██████▏           | 13/38 [00:00<00:01, 16.17it/s]Epoch: 5, train for the 70-th batch, train loss: 0.3436351716518402:  59%|███████▋     | 70/119 [00:12<00:11,  4.41it/s]Epoch: 4, train for the 25-th batch, train loss: 0.6769829988479614:  16%|██           | 24/151 [00:03<00:19,  6.38it/s]Epoch: 1, train for the 371-th batch, train loss: 0.3208225965499878:  97%|██████████▋| 370/383 [01:55<00:05,  2.59it/s]Epoch: 4, train for the 25-th batch, train loss: 0.6769829988479614:  17%|██▏          | 25/151 [00:03<00:19,  6.37it/s]evaluate for the 4-th batch, evaluate loss: 0.9220726490020752:   6%|█▏                  | 2/34 [00:00<00:03, 10.42it/s]evaluate for the 4-th batch, evaluate loss: 0.9220726490020752:  12%|██▎                 | 4/34 [00:00<00:02, 11.38it/s]evaluate for the 15-th batch, evaluate loss: 0.5229906439781189:  34%|██████▏           | 13/38 [00:00<00:01, 16.17it/s]evaluate for the 15-th batch, evaluate loss: 0.5229906439781189:  39%|███████           | 15/38 [00:00<00:01, 16.22it/s]evaluate for the 5-th batch, evaluate loss: 0.9522363543510437:  12%|██▎                 | 4/34 [00:00<00:02, 11.38it/s]Epoch: 1, train for the 371-th batch, train loss: 0.3208225965499878:  97%|██████████▋| 371/383 [01:55<00:04,  2.55it/s]Epoch: 2, train for the 164-th batch, train loss: 0.5896753668785095:  69%|███████▌   | 163/237 [00:31<00:22,  3.25it/s]evaluate for the 16-th batch, evaluate loss: 0.5869824290275574:  39%|███████           | 15/38 [00:01<00:01, 16.22it/s]Epoch: 2, train for the 164-th batch, train loss: 0.5896753668785095:  69%|███████▌   | 164/237 [00:31<00:20,  3.56it/s]Epoch: 5, train for the 71-th batch, train loss: 0.3851724863052368:  59%|███████▋     | 70/119 [00:12<00:11,  4.41it/s]Epoch: 5, train for the 71-th batch, train loss: 0.3851724863052368:  60%|███████▊     | 71/119 [00:12<00:10,  4.62it/s]evaluate for the 6-th batch, evaluate loss: 0.9744074940681458:  12%|██▎                 | 4/34 [00:00<00:02, 11.38it/s]evaluate for the 6-th batch, evaluate loss: 0.9744074940681458:  18%|███▌                | 6/34 [00:00<00:02, 11.45it/s]Epoch: 4, train for the 26-th batch, train loss: 0.6064868569374084:  17%|██▏          | 25/151 [00:03<00:19,  6.37it/s]Epoch: 4, train for the 26-th batch, train loss: 0.6064868569374084:  17%|██▏          | 26/151 [00:03<00:21,  5.75it/s]evaluate for the 17-th batch, evaluate loss: 0.5363239049911499:  39%|███████           | 15/38 [00:01<00:01, 16.22it/s]evaluate for the 17-th batch, evaluate loss: 0.5363239049911499:  45%|████████          | 17/38 [00:01<00:01, 13.87it/s]evaluate for the 7-th batch, evaluate loss: 0.9161405563354492:  18%|███▌                | 6/34 [00:00<00:02, 11.45it/s]evaluate for the 8-th batch, evaluate loss: 0.9927976727485657:  18%|███▌                | 6/34 [00:00<00:02, 11.45it/s]evaluate for the 8-th batch, evaluate loss: 0.9927976727485657:  24%|████▋               | 8/34 [00:00<00:02, 11.55it/s]Epoch: 5, train for the 72-th batch, train loss: 0.41144078969955444:  60%|███████▏    | 71/119 [00:12<00:10,  4.62it/s]evaluate for the 18-th batch, evaluate loss: 0.5884464383125305:  45%|████████          | 17/38 [00:01<00:01, 13.87it/s]Epoch: 5, train for the 72-th batch, train loss: 0.41144078969955444:  61%|███████▎    | 72/119 [00:12<00:09,  4.75it/s]Epoch: 4, train for the 27-th batch, train loss: 0.6243115067481995:  17%|██▏          | 26/151 [00:04<00:21,  5.75it/s]Epoch: 4, train for the 27-th batch, train loss: 0.6243115067481995:  18%|██▎          | 27/151 [00:04<00:21,  5.79it/s]evaluate for the 19-th batch, evaluate loss: 0.5712212324142456:  45%|████████          | 17/38 [00:01<00:01, 13.87it/s]evaluate for the 19-th batch, evaluate loss: 0.5712212324142456:  50%|█████████         | 19/38 [00:01<00:01, 12.63it/s]Epoch: 2, train for the 165-th batch, train loss: 0.5820547342300415:  69%|███████▌   | 164/237 [00:31<00:20,  3.56it/s]evaluate for the 9-th batch, evaluate loss: 0.9812409281730652:  24%|████▋               | 8/34 [00:00<00:02, 11.55it/s]Epoch: 1, train for the 372-th batch, train loss: 0.4448634684085846:  97%|██████████▋| 371/383 [01:55<00:04,  2.55it/s]Epoch: 2, train for the 165-th batch, train loss: 0.5820547342300415:  70%|███████▋   | 165/237 [00:31<00:21,  3.41it/s]evaluate for the 10-th batch, evaluate loss: 0.8133987188339233:  24%|████▍              | 8/34 [00:00<00:02, 11.55it/s]evaluate for the 10-th batch, evaluate loss: 0.8133987188339233:  29%|█████▎            | 10/34 [00:00<00:01, 12.18it/s]evaluate for the 20-th batch, evaluate loss: 0.5021324157714844:  50%|█████████         | 19/38 [00:01<00:01, 12.63it/s]Epoch: 1, train for the 372-th batch, train loss: 0.4448634684085846:  97%|██████████▋| 372/383 [01:55<00:04,  2.50it/s]Epoch: 5, train for the 73-th batch, train loss: 0.3579288423061371:  61%|███████▊     | 72/119 [00:12<00:09,  4.75it/s]Epoch: 4, train for the 28-th batch, train loss: 0.6217580437660217:  18%|██▎          | 27/151 [00:04<00:21,  5.79it/s]Epoch: 5, train for the 73-th batch, train loss: 0.3579288423061371:  61%|███████▉     | 73/119 [00:12<00:09,  5.01it/s]Epoch: 4, train for the 28-th batch, train loss: 0.6217580437660217:  19%|██▍          | 28/151 [00:04<00:21,  5.78it/s]evaluate for the 21-th batch, evaluate loss: 0.5229929089546204:  50%|█████████         | 19/38 [00:01<00:01, 12.63it/s]evaluate for the 21-th batch, evaluate loss: 0.5229929089546204:  55%|█████████▉        | 21/38 [00:01<00:01, 12.48it/s]evaluate for the 11-th batch, evaluate loss: 0.7455663084983826:  29%|█████▎            | 10/34 [00:00<00:01, 12.18it/s]evaluate for the 22-th batch, evaluate loss: 0.5447533130645752:  55%|█████████▉        | 21/38 [00:01<00:01, 12.48it/s]evaluate for the 12-th batch, evaluate loss: 0.8401488065719604:  29%|█████▎            | 10/34 [00:01<00:01, 12.18it/s]evaluate for the 12-th batch, evaluate loss: 0.8401488065719604:  35%|██████▎           | 12/34 [00:01<00:01, 11.84it/s]Epoch: 2, train for the 166-th batch, train loss: 0.5167551040649414:  70%|███████▋   | 165/237 [00:31<00:21,  3.41it/s]Epoch: 5, train for the 74-th batch, train loss: 0.39419546723365784:  61%|███████▎    | 73/119 [00:13<00:09,  5.01it/s]Epoch: 4, train for the 29-th batch, train loss: 0.6813181638717651:  19%|██▍          | 28/151 [00:04<00:21,  5.78it/s]evaluate for the 23-th batch, evaluate loss: 0.5262749195098877:  55%|█████████▉        | 21/38 [00:01<00:01, 12.48it/s]evaluate for the 23-th batch, evaluate loss: 0.5262749195098877:  61%|██████████▉       | 23/38 [00:01<00:01, 12.71it/s]Epoch: 5, train for the 74-th batch, train loss: 0.39419546723365784:  62%|███████▍    | 74/119 [00:13<00:08,  5.06it/s]Epoch: 2, train for the 166-th batch, train loss: 0.5167551040649414:  70%|███████▋   | 166/237 [00:31<00:20,  3.54it/s]Epoch: 4, train for the 29-th batch, train loss: 0.6813181638717651:  19%|██▍          | 29/151 [00:04<00:21,  5.66it/s]evaluate for the 13-th batch, evaluate loss: 0.6950333118438721:  35%|██████▎           | 12/34 [00:01<00:01, 11.84it/s]evaluate for the 24-th batch, evaluate loss: 0.5457237362861633:  61%|██████████▉       | 23/38 [00:01<00:01, 12.71it/s]evaluate for the 14-th batch, evaluate loss: 0.7535420656204224:  35%|██████▎           | 12/34 [00:01<00:01, 11.84it/s]evaluate for the 14-th batch, evaluate loss: 0.7535420656204224:  41%|███████▍          | 14/34 [00:01<00:01, 11.81it/s]Epoch: 5, train for the 75-th batch, train loss: 0.3642463684082031:  62%|████████     | 74/119 [00:13<00:08,  5.06it/s]Epoch: 5, train for the 75-th batch, train loss: 0.3642463684082031:  63%|████████▏    | 75/119 [00:13<00:08,  5.43it/s]evaluate for the 25-th batch, evaluate loss: 0.54152512550354:  61%|████████████        | 23/38 [00:01<00:01, 12.71it/s]evaluate for the 25-th batch, evaluate loss: 0.54152512550354:  66%|█████████████▏      | 25/38 [00:01<00:01, 12.72it/s]Epoch: 4, train for the 30-th batch, train loss: 0.6372864842414856:  19%|██▍          | 29/151 [00:04<00:21,  5.66it/s]Epoch: 2, train for the 167-th batch, train loss: 0.5602562427520752:  70%|███████▋   | 166/237 [00:32<00:20,  3.54it/s]Epoch: 4, train for the 30-th batch, train loss: 0.6372864842414856:  20%|██▌          | 30/151 [00:04<00:21,  5.59it/s]evaluate for the 15-th batch, evaluate loss: 0.755874752998352:  41%|███████▊           | 14/34 [00:01<00:01, 11.81it/s]Epoch: 2, train for the 167-th batch, train loss: 0.5602562427520752:  70%|███████▊   | 167/237 [00:32<00:18,  3.77it/s]evaluate for the 26-th batch, evaluate loss: 0.5308870077133179:  66%|███████████▊      | 25/38 [00:01<00:01, 12.72it/s]evaluate for the 16-th batch, evaluate loss: 0.7396575808525085:  41%|███████▍          | 14/34 [00:01<00:01, 11.81it/s]evaluate for the 16-th batch, evaluate loss: 0.7396575808525085:  47%|████████▍         | 16/34 [00:01<00:01, 12.01it/s]Epoch: 1, train for the 373-th batch, train loss: 0.42843717336654663:  97%|█████████▋| 372/383 [01:56<00:04,  2.50it/s]Epoch: 5, train for the 76-th batch, train loss: 0.3862292468547821:  63%|████████▏    | 75/119 [00:13<00:08,  5.43it/s]Epoch: 5, train for the 76-th batch, train loss: 0.3862292468547821:  64%|████████▎    | 76/119 [00:13<00:07,  5.85it/s]evaluate for the 27-th batch, evaluate loss: 0.5375598073005676:  66%|███████████▊      | 25/38 [00:01<00:01, 12.72it/s]evaluate for the 27-th batch, evaluate loss: 0.5375598073005676:  71%|████████████▊     | 27/38 [00:01<00:00, 12.47it/s]Epoch: 4, train for the 31-th batch, train loss: 0.6245156526565552:  20%|██▌          | 30/151 [00:04<00:21,  5.59it/s]Epoch: 4, train for the 31-th batch, train loss: 0.6245156526565552:  21%|██▋          | 31/151 [00:04<00:20,  5.84it/s]evaluate for the 17-th batch, evaluate loss: 0.6739820837974548:  47%|████████▍         | 16/34 [00:01<00:01, 12.01it/s]Epoch: 1, train for the 373-th batch, train loss: 0.42843717336654663:  97%|█████████▋| 373/383 [01:56<00:04,  2.19it/s]Epoch: 2, train for the 168-th batch, train loss: 0.5738351941108704:  70%|███████▊   | 167/237 [00:32<00:18,  3.77it/s]evaluate for the 28-th batch, evaluate loss: 0.5438131093978882:  71%|████████████▊     | 27/38 [00:02<00:00, 12.47it/s]Epoch: 2, train for the 168-th batch, train loss: 0.5738351941108704:  71%|███████▊   | 168/237 [00:32<00:17,  4.05it/s]Epoch: 5, train for the 77-th batch, train loss: 0.3847101628780365:  64%|████████▎    | 76/119 [00:13<00:07,  5.85it/s]Epoch: 5, train for the 77-th batch, train loss: 0.3847101628780365:  65%|████████▍    | 77/119 [00:13<00:06,  6.00it/s]evaluate for the 18-th batch, evaluate loss: 0.6863674521446228:  47%|████████▍         | 16/34 [00:01<00:01, 12.01it/s]evaluate for the 18-th batch, evaluate loss: 0.6863674521446228:  53%|█████████▌        | 18/34 [00:01<00:01, 11.26it/s]evaluate for the 29-th batch, evaluate loss: 0.5650055408477783:  71%|████████████▊     | 27/38 [00:02<00:00, 12.47it/s]evaluate for the 29-th batch, evaluate loss: 0.5650055408477783:  76%|█████████████▋    | 29/38 [00:02<00:00, 12.42it/s]Epoch: 4, train for the 32-th batch, train loss: 0.6306040287017822:  21%|██▋          | 31/151 [00:04<00:20,  5.84it/s]Epoch: 4, train for the 32-th batch, train loss: 0.6306040287017822:  21%|██▊          | 32/151 [00:04<00:20,  5.81it/s]evaluate for the 30-th batch, evaluate loss: 0.5714029669761658:  76%|█████████████▋    | 29/38 [00:02<00:00, 12.42it/s]evaluate for the 19-th batch, evaluate loss: 0.6777308583259583:  53%|█████████▌        | 18/34 [00:01<00:01, 11.26it/s]Epoch: 5, train for the 78-th batch, train loss: 0.41316497325897217:  65%|███████▊    | 77/119 [00:13<00:06,  6.00it/s]evaluate for the 31-th batch, evaluate loss: 0.5533971190452576:  76%|█████████████▋    | 29/38 [00:02<00:00, 12.42it/s]evaluate for the 31-th batch, evaluate loss: 0.5533971190452576:  82%|██████████████▋   | 31/38 [00:02<00:00, 12.54it/s]Epoch: 5, train for the 78-th batch, train loss: 0.41316497325897217:  66%|███████▊    | 78/119 [00:13<00:07,  5.72it/s]evaluate for the 20-th batch, evaluate loss: 0.7455242872238159:  53%|█████████▌        | 18/34 [00:01<00:01, 11.26it/s]evaluate for the 20-th batch, evaluate loss: 0.7455242872238159:  59%|██████████▌       | 20/34 [00:01<00:01, 10.80it/s]Epoch: 4, train for the 33-th batch, train loss: 0.6069581508636475:  21%|██▊          | 32/151 [00:05<00:20,  5.81it/s]evaluate for the 32-th batch, evaluate loss: 0.5089841485023499:  82%|██████████████▋   | 31/38 [00:02<00:00, 12.54it/s]Epoch: 4, train for the 33-th batch, train loss: 0.6069581508636475:  22%|██▊          | 33/151 [00:05<00:20,  5.70it/s]Epoch: 1, train for the 374-th batch, train loss: 0.3255811929702759:  97%|██████████▋| 373/383 [01:56<00:04,  2.19it/s]Epoch: 2, train for the 169-th batch, train loss: 0.5625584125518799:  71%|███████▊   | 168/237 [00:32<00:17,  4.05it/s]Epoch: 2, train for the 169-th batch, train loss: 0.5625584125518799:  71%|███████▊   | 169/237 [00:32<00:18,  3.72it/s]evaluate for the 21-th batch, evaluate loss: 0.7235996723175049:  59%|██████████▌       | 20/34 [00:01<00:01, 10.80it/s]Epoch: 1, train for the 374-th batch, train loss: 0.3255811929702759:  98%|██████████▋| 374/383 [01:56<00:04,  2.25it/s]Epoch: 5, train for the 79-th batch, train loss: 0.4145762622356415:  66%|████████▌    | 78/119 [00:13<00:07,  5.72it/s]evaluate for the 33-th batch, evaluate loss: 0.4995688199996948:  82%|██████████████▋   | 31/38 [00:02<00:00, 12.54it/s]evaluate for the 33-th batch, evaluate loss: 0.4995688199996948:  87%|███████████████▋  | 33/38 [00:02<00:00, 12.48it/s]Epoch: 5, train for the 79-th batch, train loss: 0.4145762622356415:  66%|████████▋    | 79/119 [00:13<00:07,  5.70it/s]evaluate for the 22-th batch, evaluate loss: 0.5000831484794617:  59%|██████████▌       | 20/34 [00:01<00:01, 10.80it/s]evaluate for the 22-th batch, evaluate loss: 0.5000831484794617:  65%|███████████▋      | 22/34 [00:01<00:01, 11.19it/s]Epoch: 4, train for the 34-th batch, train loss: 0.6198080778121948:  22%|██▊          | 33/151 [00:05<00:20,  5.70it/s]Epoch: 4, train for the 34-th batch, train loss: 0.6198080778121948:  23%|██▉          | 34/151 [00:05<00:19,  5.93it/s]evaluate for the 34-th batch, evaluate loss: 0.551835298538208:  87%|████████████████▌  | 33/38 [00:02<00:00, 12.48it/s]evaluate for the 23-th batch, evaluate loss: 0.5468146800994873:  65%|███████████▋      | 22/34 [00:01<00:01, 11.19it/s]Epoch: 2, train for the 170-th batch, train loss: 0.5633344650268555:  71%|███████▊   | 169/237 [00:32<00:18,  3.72it/s]evaluate for the 35-th batch, evaluate loss: 0.5440500974655151:  87%|███████████████▋  | 33/38 [00:02<00:00, 12.48it/s]evaluate for the 35-th batch, evaluate loss: 0.5440500974655151:  92%|████████████████▌ | 35/38 [00:02<00:00, 12.06it/s]Epoch: 2, train for the 170-th batch, train loss: 0.5633344650268555:  72%|███████▉   | 170/237 [00:32<00:17,  3.81it/s]evaluate for the 24-th batch, evaluate loss: 0.6130353212356567:  65%|███████████▋      | 22/34 [00:02<00:01, 11.19it/s]evaluate for the 24-th batch, evaluate loss: 0.6130353212356567:  71%|████████████▋     | 24/34 [00:02<00:00, 11.34it/s]Epoch: 5, train for the 80-th batch, train loss: 0.448713093996048:  66%|█████████▎    | 79/119 [00:14<00:07,  5.70it/s]Epoch: 5, train for the 80-th batch, train loss: 0.448713093996048:  67%|█████████▍    | 80/119 [00:14<00:07,  5.40it/s]evaluate for the 36-th batch, evaluate loss: 0.5483432412147522:  92%|████████████████▌ | 35/38 [00:02<00:00, 12.06it/s]Epoch: 4, train for the 35-th batch, train loss: 0.6215199828147888:  23%|██▉          | 34/151 [00:05<00:19,  5.93it/s]Epoch: 1, train for the 375-th batch, train loss: 0.4513539671897888:  98%|██████████▋| 374/383 [01:57<00:04,  2.25it/s]Epoch: 4, train for the 35-th batch, train loss: 0.6215199828147888:  23%|███          | 35/151 [00:05<00:20,  5.54it/s]evaluate for the 25-th batch, evaluate loss: 0.581612229347229:  71%|█████████████▍     | 24/34 [00:02<00:00, 11.34it/s]evaluate for the 37-th batch, evaluate loss: 0.5353421568870544:  92%|████████████████▌ | 35/38 [00:02<00:00, 12.06it/s]evaluate for the 37-th batch, evaluate loss: 0.5353421568870544:  97%|█████████████████▌| 37/38 [00:02<00:00, 12.94it/s]Epoch: 1, train for the 375-th batch, train loss: 0.4513539671897888:  98%|██████████▊| 375/383 [01:57<00:03,  2.36it/s]evaluate for the 26-th batch, evaluate loss: 0.6698307394981384:  71%|████████████▋     | 24/34 [00:02<00:00, 11.34it/s]evaluate for the 26-th batch, evaluate loss: 0.6698307394981384:  76%|█████████████▊    | 26/34 [00:02<00:00, 11.84it/s]Epoch: 2, train for the 171-th batch, train loss: 0.5750572681427002:  72%|███████▉   | 170/237 [00:33<00:17,  3.81it/s]Epoch: 2, train for the 171-th batch, train loss: 0.5750572681427002:  72%|███████▉   | 171/237 [00:33<00:16,  4.04it/s]Epoch: 5, train for the 81-th batch, train loss: 0.36129119992256165:  67%|████████    | 80/119 [00:14<00:07,  5.40it/s]evaluate for the 38-th batch, evaluate loss: 0.5572748780250549:  97%|█████████████████▌| 37/38 [00:02<00:00, 12.94it/s]evaluate for the 38-th batch, evaluate loss: 0.5572748780250549: 100%|██████████████████| 38/38 [00:02<00:00, 13.13it/s]
Epoch: 4, train for the 36-th batch, train loss: 0.6886857748031616:  23%|███          | 35/151 [00:05<00:20,  5.54it/s]Epoch: 5, train for the 81-th batch, train loss: 0.36129119992256165:  68%|████████▏   | 81/119 [00:14<00:07,  5.25it/s]evaluate for the 27-th batch, evaluate loss: 0.7006509304046631:  76%|█████████████▊    | 26/34 [00:02<00:00, 11.84it/s]Epoch: 4, train for the 36-th batch, train loss: 0.6886857748031616:  24%|███          | 36/151 [00:05<00:20,  5.51it/s]evaluate for the 28-th batch, evaluate loss: 0.5607193112373352:  76%|█████████████▊    | 26/34 [00:02<00:00, 11.84it/s]evaluate for the 28-th batch, evaluate loss: 0.5607193112373352:  82%|██████████████▊   | 28/34 [00:02<00:00, 12.03it/s]Epoch: 2, train for the 172-th batch, train loss: 0.5382019877433777:  72%|███████▉   | 171/237 [00:33<00:16,  4.04it/s]Epoch: 4, train for the 37-th batch, train loss: 0.6579574346542358:  24%|███          | 36/151 [00:05<00:20,  5.51it/s]evaluate for the 29-th batch, evaluate loss: 0.5644000768661499:  82%|██████████████▊   | 28/34 [00:02<00:00, 12.03it/s]Epoch: 4, train for the 37-th batch, train loss: 0.6579574346542358:  25%|███▏         | 37/151 [00:05<00:20,  5.65it/s]Epoch: 5, train for the 82-th batch, train loss: 0.4237973392009735:  68%|████████▊    | 81/119 [00:14<00:07,  5.25it/s]Epoch: 2, train for the 172-th batch, train loss: 0.5382019877433777:  73%|███████▉   | 172/237 [00:33<00:15,  4.25it/s]Epoch: 5, train for the 82-th batch, train loss: 0.4237973392009735:  69%|████████▉    | 82/119 [00:14<00:07,  5.22it/s]evaluate for the 30-th batch, evaluate loss: 0.5551707148551941:  82%|██████████████▊   | 28/34 [00:02<00:00, 12.03it/s]evaluate for the 30-th batch, evaluate loss: 0.5551707148551941:  88%|███████████████▉  | 30/34 [00:02<00:00, 11.92it/s]  0%|                                                                                            | 0/20 [00:00<?, ?it/s]Epoch: 1, train for the 376-th batch, train loss: 0.3178669512271881:  98%|██████████▊| 375/383 [01:57<00:03,  2.36it/s]Epoch: 5, train for the 83-th batch, train loss: 0.39235517382621765:  69%|████████▎   | 82/119 [00:14<00:07,  5.22it/s]Epoch: 5, train for the 83-th batch, train loss: 0.39235517382621765:  70%|████████▎   | 83/119 [00:14<00:06,  5.54it/s]evaluate for the 1-th batch, evaluate loss: 1.0343234539031982:   0%|                            | 0/20 [00:00<?, ?it/s]Epoch: 4, train for the 38-th batch, train loss: 0.6195259690284729:  25%|███▏         | 37/151 [00:06<00:20,  5.65it/s]evaluate for the 31-th batch, evaluate loss: 0.5988993048667908:  88%|███████████████▉  | 30/34 [00:02<00:00, 11.92it/s]Epoch: 2, train for the 173-th batch, train loss: 0.5238734483718872:  73%|███████▉   | 172/237 [00:33<00:15,  4.25it/s]Epoch: 4, train for the 38-th batch, train loss: 0.6195259690284729:  25%|███▎         | 38/151 [00:06<00:20,  5.45it/s]Epoch: 2, train for the 173-th batch, train loss: 0.5238734483718872:  73%|████████   | 173/237 [00:33<00:14,  4.36it/s]Epoch: 1, train for the 376-th batch, train loss: 0.3178669512271881:  98%|██████████▊| 376/383 [01:57<00:03,  2.25it/s]evaluate for the 2-th batch, evaluate loss: 0.8736789226531982:   0%|                            | 0/20 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.8736789226531982:  10%|██                  | 2/20 [00:00<00:01, 13.54it/s]evaluate for the 32-th batch, evaluate loss: 0.5545326471328735:  88%|███████████████▉  | 30/34 [00:02<00:00, 11.92it/s]evaluate for the 32-th batch, evaluate loss: 0.5545326471328735:  94%|████████████████▉ | 32/34 [00:02<00:00, 11.27it/s]Epoch: 5, train for the 84-th batch, train loss: 0.3989836275577545:  70%|█████████    | 83/119 [00:14<00:06,  5.54it/s]Epoch: 5, train for the 84-th batch, train loss: 0.3989836275577545:  71%|█████████▏   | 84/119 [00:14<00:06,  5.71it/s]evaluate for the 3-th batch, evaluate loss: 0.7693354487419128:  10%|██                  | 2/20 [00:00<00:01, 13.54it/s]Epoch: 4, train for the 39-th batch, train loss: 0.6035306453704834:  25%|███▎         | 38/151 [00:06<00:20,  5.45it/s]Epoch: 4, train for the 39-th batch, train loss: 0.6035306453704834:  26%|███▎         | 39/151 [00:06<00:19,  5.63it/s]evaluate for the 33-th batch, evaluate loss: 0.5602220296859741:  94%|████████████████▉ | 32/34 [00:02<00:00, 11.27it/s]Epoch: 2, train for the 174-th batch, train loss: 0.5364609956741333:  73%|████████   | 173/237 [00:33<00:14,  4.36it/s]evaluate for the 4-th batch, evaluate loss: 0.7406282424926758:  10%|██                  | 2/20 [00:00<00:01, 13.54it/s]evaluate for the 4-th batch, evaluate loss: 0.7406282424926758:  20%|████                | 4/20 [00:00<00:01, 11.50it/s]Epoch: 2, train for the 174-th batch, train loss: 0.5364609956741333:  73%|████████   | 174/237 [00:33<00:14,  4.35it/s]evaluate for the 34-th batch, evaluate loss: 0.5543715953826904:  94%|████████████████▉ | 32/34 [00:02<00:00, 11.27it/s]evaluate for the 34-th batch, evaluate loss: 0.5543715953826904: 100%|██████████████████| 34/34 [00:02<00:00, 10.84it/s]evaluate for the 34-th batch, evaluate loss: 0.5543715953826904: 100%|██████████████████| 34/34 [00:02<00:00, 11.40it/s]
Epoch: 5, train for the 85-th batch, train loss: 0.3729563057422638:  71%|█████████▏   | 84/119 [00:14<00:06,  5.71it/s]Epoch: 5, train for the 85-th batch, train loss: 0.3729563057422638:  71%|█████████▎   | 85/119 [00:14<00:06,  5.45it/s]evaluate for the 5-th batch, evaluate loss: 0.7906364798545837:  20%|████                | 4/20 [00:00<00:01, 11.50it/s]Epoch: 1, train for the 377-th batch, train loss: 0.3914797902107239:  98%|██████████▊| 376/383 [01:58<00:03,  2.25it/s]Epoch: 4, train for the 40-th batch, train loss: 0.5854836106300354:  26%|███▎         | 39/151 [00:06<00:19,  5.63it/s]Epoch: 4, train for the 40-th batch, train loss: 0.5854836106300354:  26%|███▍         | 40/151 [00:06<00:21,  5.17it/s]Epoch: 1, train for the 377-th batch, train loss: 0.3914797902107239:  98%|██████████▊| 377/383 [01:58<00:02,  2.37it/s]evaluate for the 6-th batch, evaluate loss: 0.8239209651947021:  20%|████                | 4/20 [00:00<00:01, 11.50it/s]evaluate for the 6-th batch, evaluate loss: 0.8239209651947021:  30%|██████              | 6/20 [00:00<00:01, 11.22it/s]Epoch: 5, train for the 86-th batch, train loss: 0.397865891456604:  71%|██████████    | 85/119 [00:15<00:06,  5.45it/s]Epoch: 2, train for the 175-th batch, train loss: 0.5501859784126282:  73%|████████   | 174/237 [00:33<00:14,  4.35it/s]Epoch: 5, train for the 86-th batch, train loss: 0.397865891456604:  72%|██████████    | 86/119 [00:15<00:05,  5.81it/s]evaluate for the 7-th batch, evaluate loss: 0.8066565990447998:  30%|██████              | 6/20 [00:00<00:01, 11.22it/s]Epoch: 2, train for the 175-th batch, train loss: 0.5501859784126282:  74%|████████   | 175/237 [00:33<00:14,  4.21it/s]evaluate for the 8-th batch, evaluate loss: 0.7571120858192444:  30%|██████              | 6/20 [00:00<00:01, 11.22it/s]evaluate for the 8-th batch, evaluate loss: 0.7571120858192444:  40%|████████            | 8/20 [00:00<00:01, 11.94it/s]INFO:root:Epoch: 2, learning rate: 0.0001, train loss: 0.4684
INFO:root:train average_precision, 0.8621
INFO:root:train roc_auc, 0.8327
INFO:root:validate loss: 0.6235
INFO:root:validate average_precision, 0.7770
INFO:root:validate roc_auc, 0.7805
INFO:root:new node validate loss: 0.7322
INFO:root:new node validate first_1_average_precision, 0.6682
INFO:root:new node validate first_1_roc_auc, 0.6852
INFO:root:new node validate first_3_average_precision, 0.6302
INFO:root:new node validate first_3_roc_auc, 0.6015
INFO:root:new node validate first_10_average_precision, 0.6590
INFO:root:new node validate first_10_roc_auc, 0.6225
INFO:root:new node validate average_precision, 0.7666
INFO:root:new node validate roc_auc, 0.7299
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/241 [00:00<?, ?it/s]Epoch: 4, train for the 41-th batch, train loss: 0.6033751368522644:  26%|███▍         | 40/151 [00:06<00:21,  5.17it/s]Epoch: 5, train for the 87-th batch, train loss: 0.40937209129333496:  72%|████████▋   | 86/119 [00:15<00:05,  5.81it/s]Epoch: 4, train for the 41-th batch, train loss: 0.6033751368522644:  27%|███▌         | 41/151 [00:06<00:22,  4.96it/s]Epoch: 5, train for the 87-th batch, train loss: 0.40937209129333496:  73%|████████▊   | 87/119 [00:15<00:05,  6.17it/s]evaluate for the 9-th batch, evaluate loss: 0.7398739457130432:  40%|████████            | 8/20 [00:00<00:01, 11.94it/s]Epoch: 2, train for the 176-th batch, train loss: 0.5292617082595825:  74%|████████   | 175/237 [00:34<00:14,  4.21it/s]evaluate for the 10-th batch, evaluate loss: 0.7103322148323059:  40%|███████▌           | 8/20 [00:00<00:01, 11.94it/s]evaluate for the 10-th batch, evaluate loss: 0.7103322148323059:  50%|█████████         | 10/20 [00:00<00:00, 12.88it/s]Epoch: 3, train for the 1-th batch, train loss: 0.668861985206604:   0%|                        | 0/241 [00:00<?, ?it/s]Epoch: 3, train for the 1-th batch, train loss: 0.668861985206604:   0%|                | 1/241 [00:00<00:28,  8.32it/s]Epoch: 2, train for the 176-th batch, train loss: 0.5292617082595825:  74%|████████▏  | 176/237 [00:34<00:14,  4.22it/s]Epoch: 5, train for the 88-th batch, train loss: 0.4116470217704773:  73%|█████████▌   | 87/119 [00:15<00:05,  6.17it/s]Epoch: 4, train for the 42-th batch, train loss: 0.6027823686599731:  27%|███▌         | 41/151 [00:06<00:22,  4.96it/s]Epoch: 5, train for the 88-th batch, train loss: 0.4116470217704773:  74%|█████████▌   | 88/119 [00:15<00:05,  5.98it/s]evaluate for the 11-th batch, evaluate loss: 0.7468401193618774:  50%|█████████         | 10/20 [00:00<00:00, 12.88it/s]Epoch: 4, train for the 42-th batch, train loss: 0.6027823686599731:  28%|███▌         | 42/151 [00:06<00:22,  4.95it/s]Epoch: 3, train for the 2-th batch, train loss: 0.4389536678791046:   0%|               | 1/241 [00:00<00:28,  8.32it/s]Epoch: 3, train for the 2-th batch, train loss: 0.4389536678791046:   1%|               | 2/241 [00:00<00:27,  8.71it/s]evaluate for the 12-th batch, evaluate loss: 0.7753939628601074:  50%|█████████         | 10/20 [00:00<00:00, 12.88it/s]evaluate for the 12-th batch, evaluate loss: 0.7753939628601074:  60%|██████████▊       | 12/20 [00:00<00:00, 12.25it/s]Epoch: 2, train for the 177-th batch, train loss: 0.5833602547645569:  74%|████████▏  | 176/237 [00:34<00:14,  4.22it/s]Epoch: 2, train for the 177-th batch, train loss: 0.5833602547645569:  75%|████████▏  | 177/237 [00:34<00:13,  4.46it/s]Epoch: 5, train for the 89-th batch, train loss: 0.42609694600105286:  74%|████████▊   | 88/119 [00:15<00:05,  5.98it/s]Epoch: 5, train for the 89-th batch, train loss: 0.42609694600105286:  75%|████████▉   | 89/119 [00:15<00:04,  6.06it/s]evaluate for the 13-th batch, evaluate loss: 0.7379085421562195:  60%|██████████▊       | 12/20 [00:01<00:00, 12.25it/s]Epoch: 1, train for the 378-th batch, train loss: 0.38346806168556213:  98%|█████████▊| 377/383 [01:58<00:02,  2.37it/s]Epoch: 3, train for the 3-th batch, train loss: 0.48760488629341125:   1%|              | 2/241 [00:00<00:27,  8.71it/s]Epoch: 3, train for the 3-th batch, train loss: 0.48760488629341125:   1%|▏             | 3/241 [00:00<00:34,  6.84it/s]Epoch: 4, train for the 43-th batch, train loss: 0.6155849099159241:  28%|███▌         | 42/151 [00:07<00:22,  4.95it/s]evaluate for the 14-th batch, evaluate loss: 0.7570829391479492:  60%|██████████▊       | 12/20 [00:01<00:00, 12.25it/s]evaluate for the 14-th batch, evaluate loss: 0.7570829391479492:  70%|████████████▌     | 14/20 [00:01<00:00, 12.67it/s]Epoch: 4, train for the 43-th batch, train loss: 0.6155849099159241:  28%|███▋         | 43/151 [00:07<00:22,  4.85it/s]Epoch: 1, train for the 378-th batch, train loss: 0.38346806168556213:  99%|█████████▊| 378/383 [01:58<00:02,  2.03it/s]Epoch: 5, train for the 90-th batch, train loss: 0.4046773910522461:  75%|█████████▋   | 89/119 [00:15<00:04,  6.06it/s]Epoch: 5, train for the 90-th batch, train loss: 0.4046773910522461:  76%|█████████▊   | 90/119 [00:15<00:04,  6.25it/s]evaluate for the 15-th batch, evaluate loss: 0.7931960821151733:  70%|████████████▌     | 14/20 [00:01<00:00, 12.67it/s]Epoch: 2, train for the 178-th batch, train loss: 0.5488928556442261:  75%|████████▏  | 177/237 [00:34<00:13,  4.46it/s]Epoch: 3, train for the 4-th batch, train loss: 0.45212072134017944:   1%|▏             | 3/241 [00:00<00:34,  6.84it/s]Epoch: 3, train for the 4-th batch, train loss: 0.45212072134017944:   2%|▏             | 4/241 [00:00<00:34,  6.82it/s]Epoch: 2, train for the 178-th batch, train loss: 0.5488928556442261:  75%|████████▎  | 178/237 [00:34<00:13,  4.50it/s]evaluate for the 16-th batch, evaluate loss: 0.7212689518928528:  70%|████████████▌     | 14/20 [00:01<00:00, 12.67it/s]evaluate for the 16-th batch, evaluate loss: 0.7212689518928528:  80%|██████████████▍   | 16/20 [00:01<00:00, 12.77it/s]Epoch: 4, train for the 44-th batch, train loss: 0.5593811273574829:  28%|███▋         | 43/151 [00:07<00:22,  4.85it/s]Epoch: 4, train for the 44-th batch, train loss: 0.5593811273574829:  29%|███▊         | 44/151 [00:07<00:21,  4.99it/s]Epoch: 5, train for the 91-th batch, train loss: 0.38071608543395996:  76%|█████████   | 90/119 [00:15<00:04,  6.25it/s]Epoch: 3, train for the 5-th batch, train loss: 0.48816099762916565:   2%|▏             | 4/241 [00:00<00:34,  6.82it/s]evaluate for the 17-th batch, evaluate loss: 0.7489216327667236:  80%|██████████████▍   | 16/20 [00:01<00:00, 12.77it/s]Epoch: 5, train for the 91-th batch, train loss: 0.38071608543395996:  76%|█████████▏  | 91/119 [00:15<00:04,  5.91it/s]Epoch: 3, train for the 5-th batch, train loss: 0.48816099762916565:   2%|▎             | 5/241 [00:00<00:34,  6.90it/s]Epoch: 1, train for the 379-th batch, train loss: 0.2579669952392578:  99%|██████████▊| 378/383 [01:59<00:02,  2.03it/s]Epoch: 4, train for the 45-th batch, train loss: 0.5529859662055969:  29%|███▊         | 44/151 [00:07<00:21,  4.99it/s]evaluate for the 18-th batch, evaluate loss: 0.758084774017334:  80%|███████████████▏   | 16/20 [00:01<00:00, 12.77it/s]evaluate for the 18-th batch, evaluate loss: 0.758084774017334:  90%|█████████████████  | 18/20 [00:01<00:00, 11.95it/s]Epoch: 4, train for the 45-th batch, train loss: 0.5529859662055969:  30%|███▊         | 45/151 [00:07<00:19,  5.33it/s]Epoch: 2, train for the 179-th batch, train loss: 0.5443865656852722:  75%|████████▎  | 178/237 [00:34<00:13,  4.50it/s]Epoch: 3, train for the 6-th batch, train loss: 0.6763395667076111:   2%|▎              | 5/241 [00:00<00:34,  6.90it/s]Epoch: 2, train for the 179-th batch, train loss: 0.5443865656852722:  76%|████████▎  | 179/237 [00:34<00:13,  4.32it/s]Epoch: 3, train for the 6-th batch, train loss: 0.6763395667076111:   2%|▎              | 6/241 [00:00<00:31,  7.38it/s]Epoch: 5, train for the 92-th batch, train loss: 0.42271319031715393:  76%|█████████▏  | 91/119 [00:16<00:04,  5.91it/s]Epoch: 5, train for the 92-th batch, train loss: 0.42271319031715393:  77%|█████████▎  | 92/119 [00:16<00:04,  5.98it/s]evaluate for the 19-th batch, evaluate loss: 0.7963290214538574:  90%|████████████████▏ | 18/20 [00:01<00:00, 11.95it/s]Epoch: 1, train for the 379-th batch, train loss: 0.2579669952392578:  99%|██████████▉| 379/383 [01:59<00:01,  2.15it/s]evaluate for the 20-th batch, evaluate loss: 0.7561362385749817:  90%|████████████████▏ | 18/20 [00:01<00:00, 11.95it/s]evaluate for the 20-th batch, evaluate loss: 0.7561362385749817: 100%|██████████████████| 20/20 [00:01<00:00, 12.45it/s]evaluate for the 20-th batch, evaluate loss: 0.7561362385749817: 100%|██████████████████| 20/20 [00:01<00:00, 12.31it/s]
Epoch: 4, train for the 46-th batch, train loss: 0.6091182827949524:  30%|███▊         | 45/151 [00:07<00:19,  5.33it/s]Epoch: 3, train for the 7-th batch, train loss: 0.4009913504123688:   2%|▎              | 6/241 [00:00<00:31,  7.38it/s]Epoch: 4, train for the 46-th batch, train loss: 0.6091182827949524:  30%|███▉         | 46/151 [00:07<00:19,  5.42it/s]Epoch: 3, train for the 7-th batch, train loss: 0.4009913504123688:   3%|▍              | 7/241 [00:00<00:32,  7.20it/s]Epoch: 2, train for the 180-th batch, train loss: 0.5936952233314514:  76%|████████▎  | 179/237 [00:35<00:13,  4.32it/s]Epoch: 2, train for the 180-th batch, train loss: 0.5936952233314514:  76%|████████▎  | 180/237 [00:35<00:12,  4.45it/s]Epoch: 5, train for the 93-th batch, train loss: 0.386432409286499:  77%|██████████▊   | 92/119 [00:16<00:04,  5.98it/s]Epoch: 5, train for the 93-th batch, train loss: 0.386432409286499:  78%|██████████▉   | 93/119 [00:16<00:04,  5.76it/s]Epoch: 4, train for the 47-th batch, train loss: 0.5617191791534424:  30%|███▉         | 46/151 [00:07<00:19,  5.42it/s]Epoch: 4, train for the 47-th batch, train loss: 0.5617191791534424:  31%|████         | 47/151 [00:07<00:17,  5.86it/s]INFO:root:Epoch: 4, learning rate: 0.0001, train loss: 0.4975
INFO:root:train average_precision, 0.8586
INFO:root:train roc_auc, 0.8537
INFO:root:validate loss: 0.5720
INFO:root:validate average_precision, 0.7716
INFO:root:validate roc_auc, 0.8000
INFO:root:new node validate loss: 0.7819
INFO:root:new node validate first_1_average_precision, 0.5285
INFO:root:new node validate first_1_roc_auc, 0.4407
INFO:root:new node validate first_3_average_precision, 0.5458
INFO:root:new node validate first_3_roc_auc, 0.5083
INFO:root:new node validate first_10_average_precision, 0.5535
INFO:root:new node validate first_10_roc_auc, 0.5471
INFO:root:new node validate average_precision, 0.6064
INFO:root:new node validate roc_auc, 0.6186
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]Epoch: 1, train for the 380-th batch, train loss: 0.34698760509490967:  99%|█████████▉| 379/383 [01:59<00:01,  2.15it/s]Epoch: 5, train for the 94-th batch, train loss: 0.3613733947277069:  78%|██████████▏  | 93/119 [00:16<00:04,  5.76it/s]Epoch: 5, train for the 94-th batch, train loss: 0.3613733947277069:  79%|██████████▎  | 94/119 [00:16<00:04,  6.01it/s]Epoch: 2, train for the 181-th batch, train loss: 0.5427190065383911:  76%|████████▎  | 180/237 [00:35<00:12,  4.45it/s]Epoch: 2, train for the 181-th batch, train loss: 0.5427190065383911:  76%|████████▍  | 181/237 [00:35<00:12,  4.50it/s]Epoch: 5, train for the 1-th batch, train loss: 0.9813172817230225:   0%|                       | 0/146 [00:00<?, ?it/s]Epoch: 5, train for the 1-th batch, train loss: 0.9813172817230225:   1%|               | 1/146 [00:00<00:18,  7.88it/s]Epoch: 3, train for the 8-th batch, train loss: 0.7246571183204651:   3%|▍              | 7/241 [00:01<00:32,  7.20it/s]Epoch: 3, train for the 8-th batch, train loss: 0.7246571183204651:   3%|▍              | 8/241 [00:01<00:43,  5.36it/s]Epoch: 1, train for the 380-th batch, train loss: 0.34698760509490967:  99%|█████████▉| 380/383 [01:59<00:01,  2.25it/s]Epoch: 4, train for the 48-th batch, train loss: 0.5516232252120972:  31%|████         | 47/151 [00:07<00:17,  5.86it/s]Epoch: 4, train for the 48-th batch, train loss: 0.5516232252120972:  32%|████▏        | 48/151 [00:07<00:18,  5.71it/s]Epoch: 5, train for the 95-th batch, train loss: 0.30790919065475464:  79%|█████████▍  | 94/119 [00:16<00:04,  6.01it/s]Epoch: 5, train for the 95-th batch, train loss: 0.30790919065475464:  80%|█████████▌  | 95/119 [00:16<00:03,  6.27it/s]Epoch: 5, train for the 2-th batch, train loss: 0.659817636013031:   1%|                | 1/146 [00:00<00:18,  7.88it/s]Epoch: 3, train for the 9-th batch, train loss: 0.591882586479187:   3%|▌               | 8/241 [00:01<00:43,  5.36it/s]Epoch: 5, train for the 2-th batch, train loss: 0.659817636013031:   1%|▏               | 2/146 [00:00<00:19,  7.50it/s]Epoch: 3, train for the 9-th batch, train loss: 0.591882586479187:   4%|▌               | 9/241 [00:01<00:39,  5.91it/s]Epoch: 2, train for the 182-th batch, train loss: 0.5525261759757996:  76%|████████▍  | 181/237 [00:35<00:12,  4.50it/s]Epoch: 4, train for the 49-th batch, train loss: 0.5525206327438354:  32%|████▏        | 48/151 [00:08<00:18,  5.71it/s]Epoch: 2, train for the 182-th batch, train loss: 0.5525261759757996:  77%|████████▍  | 182/237 [00:35<00:11,  4.59it/s]Epoch: 4, train for the 49-th batch, train loss: 0.5525206327438354:  32%|████▏        | 49/151 [00:08<00:17,  5.73it/s]Epoch: 5, train for the 96-th batch, train loss: 0.3406488001346588:  80%|██████████▍  | 95/119 [00:16<00:03,  6.27it/s]Epoch: 5, train for the 96-th batch, train loss: 0.3406488001346588:  81%|██████████▍  | 96/119 [00:16<00:03,  6.53it/s]Epoch: 3, train for the 10-th batch, train loss: 0.5519881844520569:   4%|▌             | 9/241 [00:01<00:39,  5.91it/s]Epoch: 3, train for the 10-th batch, train loss: 0.5519881844520569:   4%|▌            | 10/241 [00:01<00:37,  6.13it/s]Epoch: 5, train for the 3-th batch, train loss: 0.6368626356124878:   1%|▏              | 2/146 [00:00<00:19,  7.50it/s]Epoch: 5, train for the 3-th batch, train loss: 0.6368626356124878:   2%|▎              | 3/146 [00:00<00:20,  6.87it/s]Epoch: 1, train for the 381-th batch, train loss: 0.40242987871170044:  99%|█████████▉| 380/383 [01:59<00:01,  2.25it/s]Epoch: 4, train for the 50-th batch, train loss: 0.5259824991226196:  32%|████▏        | 49/151 [00:08<00:17,  5.73it/s]Epoch: 4, train for the 50-th batch, train loss: 0.5259824991226196:  33%|████▎        | 50/151 [00:08<00:16,  6.03it/s]Epoch: 5, train for the 97-th batch, train loss: 0.38203734159469604:  81%|█████████▋  | 96/119 [00:16<00:03,  6.53it/s]Epoch: 2, train for the 183-th batch, train loss: 0.5552228093147278:  77%|████████▍  | 182/237 [00:35<00:11,  4.59it/s]Epoch: 5, train for the 97-th batch, train loss: 0.38203734159469604:  82%|█████████▊  | 97/119 [00:16<00:03,  6.75it/s]Epoch: 1, train for the 381-th batch, train loss: 0.40242987871170044:  99%|█████████▉| 381/383 [01:59<00:00,  2.35it/s]Epoch: 2, train for the 183-th batch, train loss: 0.5552228093147278:  77%|████████▍  | 183/237 [00:35<00:11,  4.68it/s]Epoch: 5, train for the 4-th batch, train loss: 0.622577428817749:   2%|▎               | 3/146 [00:00<00:20,  6.87it/s]Epoch: 5, train for the 4-th batch, train loss: 0.622577428817749:   3%|▍               | 4/146 [00:00<00:19,  7.26it/s]Epoch: 3, train for the 11-th batch, train loss: 0.36654266715049744:   4%|▍           | 10/241 [00:01<00:37,  6.13it/s]Epoch: 3, train for the 11-th batch, train loss: 0.36654266715049744:   5%|▌           | 11/241 [00:01<00:36,  6.35it/s]Epoch: 5, train for the 98-th batch, train loss: 0.35174688696861267:  82%|█████████▊  | 97/119 [00:16<00:03,  6.75it/s]Epoch: 5, train for the 98-th batch, train loss: 0.35174688696861267:  82%|█████████▉  | 98/119 [00:16<00:03,  6.81it/s]Epoch: 3, train for the 12-th batch, train loss: 0.5353955626487732:   5%|▌            | 11/241 [00:01<00:36,  6.35it/s]Epoch: 3, train for the 12-th batch, train loss: 0.5353955626487732:   5%|▋            | 12/241 [00:01<00:32,  6.98it/s]Epoch: 5, train for the 5-th batch, train loss: 0.6148208975791931:   3%|▍              | 4/146 [00:00<00:19,  7.26it/s]Epoch: 5, train for the 5-th batch, train loss: 0.6148208975791931:   3%|▌              | 5/146 [00:00<00:19,  7.30it/s]Epoch: 2, train for the 184-th batch, train loss: 0.5438810586929321:  77%|████████▍  | 183/237 [00:35<00:11,  4.68it/s]Epoch: 2, train for the 184-th batch, train loss: 0.5438810586929321:  78%|████████▌  | 184/237 [00:35<00:11,  4.79it/s]Epoch: 3, train for the 13-th batch, train loss: 0.5039710402488708:   5%|▋            | 12/241 [00:01<00:32,  6.98it/s]Epoch: 5, train for the 99-th batch, train loss: 0.3402126729488373:  82%|██████████▋  | 98/119 [00:17<00:03,  6.81it/s]Epoch: 5, train for the 99-th batch, train loss: 0.3402126729488373:  83%|██████████▊  | 99/119 [00:17<00:03,  6.50it/s]Epoch: 5, train for the 6-th batch, train loss: 0.5864892601966858:   3%|▌              | 5/146 [00:00<00:19,  7.30it/s]Epoch: 5, train for the 6-th batch, train loss: 0.5864892601966858:   4%|▌              | 6/146 [00:00<00:20,  6.94it/s]Epoch: 3, train for the 14-th batch, train loss: 0.5787583589553833:   5%|▋            | 12/241 [00:01<00:32,  6.98it/s]Epoch: 3, train for the 14-th batch, train loss: 0.5787583589553833:   6%|▊            | 14/241 [00:01<00:27,  8.39it/s]Epoch: 2, train for the 185-th batch, train loss: 0.5213792324066162:  78%|████████▌  | 184/237 [00:36<00:11,  4.79it/s]Epoch: 2, train for the 185-th batch, train loss: 0.5213792324066162:  78%|████████▌  | 185/237 [00:36<00:10,  4.98it/s]Epoch: 3, train for the 15-th batch, train loss: 0.5208625197410583:   6%|▊            | 14/241 [00:02<00:27,  8.39it/s]Epoch: 1, train for the 382-th batch, train loss: 0.4014990031719208:  99%|██████████▉| 381/383 [02:00<00:00,  2.35it/s]Epoch: 4, train for the 51-th batch, train loss: 0.6116663813591003:  33%|████▎        | 50/151 [00:08<00:16,  6.03it/s]Epoch: 4, train for the 51-th batch, train loss: 0.6116663813591003:  34%|████▍        | 51/151 [00:08<00:26,  3.78it/s]Epoch: 5, train for the 100-th batch, train loss: 0.36904340982437134:  83%|█████████▏ | 99/119 [00:17<00:03,  6.50it/s]Epoch: 5, train for the 100-th batch, train loss: 0.36904340982437134:  84%|████████▍ | 100/119 [00:17<00:03,  6.23it/s]Epoch: 1, train for the 382-th batch, train loss: 0.4014990031719208: 100%|██████████▉| 382/383 [02:00<00:00,  2.28it/s]Epoch: 5, train for the 7-th batch, train loss: 0.5760974287986755:   4%|▌              | 6/146 [00:01<00:20,  6.94it/s]Epoch: 5, train for the 7-th batch, train loss: 0.5760974287986755:   5%|▋              | 7/146 [00:01<00:21,  6.49it/s]Epoch: 3, train for the 16-th batch, train loss: 0.4853375554084778:   6%|▊            | 14/241 [00:02<00:27,  8.39it/s]Epoch: 3, train for the 16-th batch, train loss: 0.4853375554084778:   7%|▊            | 16/241 [00:02<00:25,  8.84it/s]Epoch: 2, train for the 186-th batch, train loss: 0.5066249966621399:  78%|████████▌  | 185/237 [00:36<00:10,  4.98it/s]Epoch: 2, train for the 186-th batch, train loss: 0.5066249966621399:  78%|████████▋  | 186/237 [00:36<00:10,  4.92it/s]Epoch: 4, train for the 52-th batch, train loss: 0.5972006320953369:  34%|████▍        | 51/151 [00:08<00:26,  3.78it/s]Epoch: 4, train for the 52-th batch, train loss: 0.5972006320953369:  34%|████▍        | 52/151 [00:08<00:23,  4.29it/s]Epoch: 5, train for the 101-th batch, train loss: 0.4160045385360718:  84%|█████████▏ | 100/119 [00:17<00:03,  6.23it/s]Epoch: 5, train for the 101-th batch, train loss: 0.4160045385360718:  85%|█████████▎ | 101/119 [00:17<00:02,  6.21it/s]Epoch: 3, train for the 17-th batch, train loss: 0.42549216747283936:   7%|▊           | 16/241 [00:02<00:25,  8.84it/s]Epoch: 3, train for the 17-th batch, train loss: 0.42549216747283936:   7%|▊           | 17/241 [00:02<00:25,  8.89it/s]Epoch: 5, train for the 8-th batch, train loss: 0.5305116772651672:   5%|▋              | 7/146 [00:01<00:21,  6.49it/s]Epoch: 5, train for the 8-th batch, train loss: 0.5305116772651672:   5%|▊              | 8/146 [00:01<00:22,  6.23it/s]Epoch: 1, train for the 383-th batch, train loss: 0.3594222962856293: 100%|██████████▉| 382/383 [02:00<00:00,  2.28it/s]Epoch: 2, train for the 187-th batch, train loss: 0.5690140724182129:  78%|████████▋  | 186/237 [00:36<00:10,  4.92it/s]Epoch: 4, train for the 53-th batch, train loss: 0.622036337852478:  34%|████▊         | 52/151 [00:09<00:23,  4.29it/s]Epoch: 4, train for the 53-th batch, train loss: 0.622036337852478:  35%|████▉         | 53/151 [00:09<00:20,  4.74it/s]Epoch: 1, train for the 383-th batch, train loss: 0.3594222962856293: 100%|███████████| 383/383 [02:00<00:00,  2.52it/s]Epoch: 1, train for the 383-th batch, train loss: 0.3594222962856293: 100%|███████████| 383/383 [02:00<00:00,  3.17it/s]
Epoch: 2, train for the 187-th batch, train loss: 0.5690140724182129:  79%|████████▋  | 187/237 [00:36<00:10,  4.98it/s]Epoch: 5, train for the 102-th batch, train loss: 0.41572263836860657:  85%|████████▍ | 101/119 [00:17<00:02,  6.21it/s]Epoch: 5, train for the 9-th batch, train loss: 0.5055100321769714:   5%|▊              | 8/146 [00:01<00:22,  6.23it/s]Epoch: 5, train for the 102-th batch, train loss: 0.41572263836860657:  86%|████████▌ | 102/119 [00:17<00:02,  5.81it/s]Epoch: 5, train for the 9-th batch, train loss: 0.5055100321769714:   6%|▉              | 9/146 [00:01<00:22,  6.16it/s]Epoch: 4, train for the 54-th batch, train loss: 0.5755016207695007:  35%|████▌        | 53/151 [00:09<00:20,  4.74it/s]Epoch: 4, train for the 54-th batch, train loss: 0.5755016207695007:  36%|████▋        | 54/151 [00:09<00:17,  5.54it/s]Epoch: 5, train for the 10-th batch, train loss: 0.49325916171073914:   6%|▊            | 9/146 [00:01<00:22,  6.16it/s]Epoch: 5, train for the 10-th batch, train loss: 0.49325916171073914:   7%|▊           | 10/146 [00:01<00:19,  6.92it/s]Epoch: 4, train for the 55-th batch, train loss: 0.5618597269058228:  36%|████▋        | 54/151 [00:09<00:17,  5.54it/s]Epoch: 5, train for the 103-th batch, train loss: 0.3974529206752777:  86%|█████████▍ | 102/119 [00:17<00:02,  5.81it/s]Epoch: 4, train for the 55-th batch, train loss: 0.5618597269058228:  36%|████▋        | 55/151 [00:09<00:15,  6.27it/s]Epoch: 5, train for the 103-th batch, train loss: 0.3974529206752777:  87%|█████████▌ | 103/119 [00:17<00:02,  5.77it/s]Epoch: 2, train for the 188-th batch, train loss: 0.5874093770980835:  79%|████████▋  | 187/237 [00:36<00:10,  4.98it/s]Epoch: 5, train for the 11-th batch, train loss: 0.47429007291793823:   7%|▊           | 10/146 [00:01<00:19,  6.92it/s]Epoch: 5, train for the 11-th batch, train loss: 0.47429007291793823:   8%|▉           | 11/146 [00:01<00:18,  7.31it/s]Epoch: 2, train for the 188-th batch, train loss: 0.5874093770980835:  79%|████████▋  | 188/237 [00:36<00:10,  4.53it/s]Epoch: 3, train for the 18-th batch, train loss: 0.43137118220329285:   7%|▊           | 17/241 [00:02<00:25,  8.89it/s]Epoch: 3, train for the 18-th batch, train loss: 0.43137118220329285:   7%|▉           | 18/241 [00:02<00:41,  5.31it/s]Epoch: 4, train for the 56-th batch, train loss: 0.4659065902233124:  36%|████▋        | 55/151 [00:09<00:15,  6.27it/s]Epoch: 4, train for the 56-th batch, train loss: 0.4659065902233124:  37%|████▊        | 56/151 [00:09<00:14,  6.34it/s]Epoch: 5, train for the 104-th batch, train loss: 0.3435874879360199:  87%|█████████▌ | 103/119 [00:18<00:02,  5.77it/s]Epoch: 5, train for the 104-th batch, train loss: 0.3435874879360199:  87%|█████████▌ | 104/119 [00:18<00:02,  5.74it/s]Epoch: 5, train for the 12-th batch, train loss: 0.4593357741832733:   8%|▉            | 11/146 [00:01<00:18,  7.31it/s]Epoch: 5, train for the 12-th batch, train loss: 0.4593357741832733:   8%|█            | 12/146 [00:01<00:18,  7.28it/s]Epoch: 3, train for the 19-th batch, train loss: 0.47467726469039917:   7%|▉           | 18/241 [00:02<00:41,  5.31it/s]Epoch: 2, train for the 189-th batch, train loss: 0.5801485180854797:  79%|████████▋  | 188/237 [00:36<00:10,  4.53it/s]Epoch: 3, train for the 19-th batch, train loss: 0.47467726469039917:   8%|▉           | 19/241 [00:02<00:38,  5.74it/s]Epoch: 2, train for the 189-th batch, train loss: 0.5801485180854797:  80%|████████▊  | 189/237 [00:36<00:09,  4.82it/s]Epoch: 4, train for the 57-th batch, train loss: 0.5109800100326538:  37%|████▊        | 56/151 [00:09<00:14,  6.34it/s]Epoch: 4, train for the 57-th batch, train loss: 0.5109800100326538:  38%|████▉        | 57/151 [00:09<00:14,  6.42it/s]Epoch: 3, train for the 20-th batch, train loss: 0.37307465076446533:   8%|▉           | 19/241 [00:02<00:38,  5.74it/s]Epoch: 3, train for the 20-th batch, train loss: 0.37307465076446533:   8%|▉           | 20/241 [00:02<00:35,  6.31it/s]Epoch: 5, train for the 105-th batch, train loss: 0.36296576261520386:  87%|████████▋ | 104/119 [00:18<00:02,  5.74it/s]Epoch: 2, train for the 190-th batch, train loss: 0.5783947110176086:  80%|████████▊  | 189/237 [00:37<00:09,  4.82it/s]Epoch: 5, train for the 13-th batch, train loss: 0.43322503566741943:   8%|▉           | 12/146 [00:01<00:18,  7.28it/s]Epoch: 5, train for the 13-th batch, train loss: 0.43322503566741943:   9%|█           | 13/146 [00:01<00:19,  6.65it/s]Epoch: 5, train for the 105-th batch, train loss: 0.36296576261520386:  88%|████████▊ | 105/119 [00:18<00:02,  5.52it/s]Epoch: 2, train for the 190-th batch, train loss: 0.5783947110176086:  80%|████████▊  | 190/237 [00:37<00:09,  5.16it/s]Epoch: 4, train for the 58-th batch, train loss: 0.5156384110450745:  38%|████▉        | 57/151 [00:09<00:14,  6.42it/s]Epoch: 4, train for the 58-th batch, train loss: 0.5156384110450745:  38%|████▉        | 58/151 [00:09<00:15,  6.09it/s]Epoch: 5, train for the 14-th batch, train loss: 0.44717851281166077:   9%|█           | 13/146 [00:02<00:19,  6.65it/s]Epoch: 5, train for the 14-th batch, train loss: 0.44717851281166077:  10%|█▏          | 14/146 [00:02<00:20,  6.56it/s]Epoch: 5, train for the 106-th batch, train loss: 0.3810814917087555:  88%|█████████▋ | 105/119 [00:18<00:02,  5.52it/s]Epoch: 2, train for the 191-th batch, train loss: 0.5427753925323486:  80%|████████▊  | 190/237 [00:37<00:09,  5.16it/s]Epoch: 5, train for the 106-th batch, train loss: 0.3810814917087555:  89%|█████████▊ | 106/119 [00:18<00:02,  5.57it/s]Epoch: 4, train for the 59-th batch, train loss: 0.4952947199344635:  38%|████▉        | 58/151 [00:09<00:15,  6.09it/s]Epoch: 2, train for the 191-th batch, train loss: 0.5427753925323486:  81%|████████▊  | 191/237 [00:37<00:08,  5.24it/s]Epoch: 3, train for the 21-th batch, train loss: 0.6257208585739136:   8%|█            | 20/241 [00:03<00:35,  6.31it/s]Epoch: 4, train for the 59-th batch, train loss: 0.4952947199344635:  39%|█████        | 59/151 [00:09<00:13,  6.78it/s]Epoch: 3, train for the 21-th batch, train loss: 0.6257208585739136:   9%|█▏           | 21/241 [00:03<00:41,  5.26it/s]Epoch: 5, train for the 15-th batch, train loss: 0.4203436076641083:  10%|█▏           | 14/146 [00:02<00:20,  6.56it/s]Epoch: 5, train for the 15-th batch, train loss: 0.4203436076641083:  10%|█▎           | 15/146 [00:02<00:19,  6.82it/s]Epoch: 5, train for the 107-th batch, train loss: 0.3397093117237091:  89%|█████████▊ | 106/119 [00:18<00:02,  5.57it/s]Epoch: 5, train for the 107-th batch, train loss: 0.3397093117237091:  90%|█████████▉ | 107/119 [00:18<00:02,  5.65it/s]  0%|                                                                                           | 0/106 [00:00<?, ?it/s]Epoch: 4, train for the 60-th batch, train loss: 0.48224353790283203:  39%|████▋       | 59/151 [00:10<00:13,  6.78it/s]Epoch: 3, train for the 22-th batch, train loss: 0.5362076163291931:   9%|█▏           | 21/241 [00:03<00:41,  5.26it/s]Epoch: 3, train for the 22-th batch, train loss: 0.5362076163291931:   9%|█▏           | 22/241 [00:03<00:41,  5.30it/s]Epoch: 4, train for the 60-th batch, train loss: 0.48224353790283203:  40%|████▊       | 60/151 [00:10<00:14,  6.18it/s]Epoch: 2, train for the 192-th batch, train loss: 0.5537936687469482:  81%|████████▊  | 191/237 [00:37<00:08,  5.24it/s]Epoch: 2, train for the 192-th batch, train loss: 0.5537936687469482:  81%|████████▉  | 192/237 [00:37<00:09,  4.88it/s]Epoch: 5, train for the 108-th batch, train loss: 0.277410626411438:  90%|██████████▊ | 107/119 [00:18<00:02,  5.65it/s]Epoch: 5, train for the 108-th batch, train loss: 0.277410626411438:  91%|██████████▉ | 108/119 [00:18<00:01,  6.39it/s]Epoch: 3, train for the 23-th batch, train loss: 0.4828256070613861:   9%|█▏           | 22/241 [00:03<00:41,  5.30it/s]Epoch: 3, train for the 23-th batch, train loss: 0.4828256070613861:  10%|█▏           | 23/241 [00:03<00:38,  5.63it/s]Epoch: 5, train for the 109-th batch, train loss: 0.38374456763267517:  91%|█████████ | 108/119 [00:18<00:01,  6.39it/s]Epoch: 5, train for the 109-th batch, train loss: 0.38374456763267517:  92%|█████████▏| 109/119 [00:18<00:01,  6.97it/s]Epoch: 2, train for the 193-th batch, train loss: 0.5435358881950378:  81%|████████▉  | 192/237 [00:37<00:09,  4.88it/s]Epoch: 4, train for the 61-th batch, train loss: 0.5544069409370422:  40%|█████▏       | 60/151 [00:10<00:14,  6.18it/s]Epoch: 4, train for the 61-th batch, train loss: 0.5544069409370422:  40%|█████▎       | 61/151 [00:10<00:15,  5.86it/s]Epoch: 2, train for the 193-th batch, train loss: 0.5435358881950378:  81%|████████▉  | 193/237 [00:37<00:08,  5.13it/s]Epoch: 5, train for the 110-th batch, train loss: 0.3440639078617096:  92%|██████████ | 109/119 [00:18<00:01,  6.97it/s]Epoch: 3, train for the 24-th batch, train loss: 0.5366645455360413:  10%|█▏           | 23/241 [00:03<00:38,  5.63it/s]Epoch: 3, train for the 24-th batch, train loss: 0.5366645455360413:  10%|█▎           | 24/241 [00:03<00:35,  6.17it/s]Epoch: 5, train for the 110-th batch, train loss: 0.3440639078617096:  92%|██████████▏| 110/119 [00:18<00:01,  7.46it/s]Epoch: 4, train for the 62-th batch, train loss: 0.5615811944007874:  40%|█████▎       | 61/151 [00:10<00:15,  5.86it/s]Epoch: 2, train for the 194-th batch, train loss: 0.5358748435974121:  81%|████████▉  | 193/237 [00:37<00:08,  5.13it/s]Epoch: 4, train for the 62-th batch, train loss: 0.5615811944007874:  41%|█████▎       | 62/151 [00:10<00:15,  5.90it/s]Epoch: 5, train for the 111-th batch, train loss: 0.407916396856308:  92%|███████████ | 110/119 [00:19<00:01,  7.46it/s]Epoch: 2, train for the 194-th batch, train loss: 0.5358748435974121:  82%|█████████  | 194/237 [00:37<00:08,  5.28it/s]Epoch: 3, train for the 25-th batch, train loss: 0.5821900367736816:  10%|█▎           | 24/241 [00:03<00:35,  6.17it/s]Epoch: 5, train for the 111-th batch, train loss: 0.407916396856308:  93%|███████████▏| 111/119 [00:19<00:01,  7.80it/s]Epoch: 5, train for the 16-th batch, train loss: 0.4582937955856323:  10%|█▎           | 15/146 [00:02<00:19,  6.82it/s]Epoch: 3, train for the 25-th batch, train loss: 0.5821900367736816:  10%|█▎           | 25/241 [00:03<00:32,  6.67it/s]Epoch: 5, train for the 16-th batch, train loss: 0.4582937955856323:  11%|█▍           | 16/146 [00:02<00:33,  3.89it/s]evaluate for the 1-th batch, evaluate loss: 0.5344432592391968:   0%|                           | 0/106 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.5344432592391968:   1%|▏                  | 1/106 [00:00<00:53,  1.97it/s]Epoch: 4, train for the 63-th batch, train loss: 0.5254086852073669:  41%|█████▎       | 62/151 [00:10<00:15,  5.90it/s]Epoch: 5, train for the 17-th batch, train loss: 0.42314383387565613:  11%|█▎          | 16/146 [00:02<00:33,  3.89it/s]Epoch: 5, train for the 17-th batch, train loss: 0.42314383387565613:  12%|█▍          | 17/146 [00:02<00:29,  4.42it/s]Epoch: 4, train for the 63-th batch, train loss: 0.5254086852073669:  42%|█████▍       | 63/151 [00:10<00:15,  5.66it/s]Epoch: 3, train for the 26-th batch, train loss: 0.5930448174476624:  10%|█▎           | 25/241 [00:03<00:32,  6.67it/s]Epoch: 3, train for the 26-th batch, train loss: 0.5930448174476624:  11%|█▍           | 26/241 [00:03<00:33,  6.34it/s]Epoch: 2, train for the 195-th batch, train loss: 0.5246742367744446:  82%|█████████  | 194/237 [00:38<00:08,  5.28it/s]Epoch: 5, train for the 112-th batch, train loss: 0.3146514296531677:  93%|██████████▎| 111/119 [00:19<00:01,  7.80it/s]Epoch: 5, train for the 112-th batch, train loss: 0.3146514296531677:  94%|██████████▎| 112/119 [00:19<00:01,  6.47it/s]Epoch: 2, train for the 195-th batch, train loss: 0.5246742367744446:  82%|█████████  | 195/237 [00:38<00:08,  4.85it/s]evaluate for the 2-th batch, evaluate loss: 0.6304582357406616:   1%|▏                  | 1/106 [00:00<00:53,  1.97it/s]evaluate for the 2-th batch, evaluate loss: 0.6304582357406616:   2%|▎                  | 2/106 [00:00<00:31,  3.28it/s]Epoch: 5, train for the 18-th batch, train loss: 0.3493262827396393:  12%|█▌           | 17/146 [00:02<00:29,  4.42it/s]Epoch: 5, train for the 18-th batch, train loss: 0.3493262827396393:  12%|█▌           | 18/146 [00:02<00:25,  5.08it/s]Epoch: 4, train for the 64-th batch, train loss: 0.5021809935569763:  42%|█████▍       | 63/151 [00:10<00:15,  5.66it/s]Epoch: 3, train for the 27-th batch, train loss: 0.6072601675987244:  11%|█▍           | 26/241 [00:04<00:33,  6.34it/s]Epoch: 4, train for the 64-th batch, train loss: 0.5021809935569763:  42%|█████▌       | 64/151 [00:10<00:15,  5.51it/s]Epoch: 5, train for the 113-th batch, train loss: 0.30618494749069214:  94%|█████████▍| 112/119 [00:19<00:01,  6.47it/s]Epoch: 3, train for the 27-th batch, train loss: 0.6072601675987244:  11%|█▍           | 27/241 [00:04<00:35,  6.06it/s]Epoch: 5, train for the 113-th batch, train loss: 0.30618494749069214:  95%|█████████▍| 113/119 [00:19<00:00,  6.47it/s]Epoch: 5, train for the 19-th batch, train loss: 0.38130897283554077:  12%|█▍          | 18/146 [00:03<00:25,  5.08it/s]Epoch: 5, train for the 19-th batch, train loss: 0.38130897283554077:  13%|█▌          | 19/146 [00:03<00:22,  5.53it/s]Epoch: 2, train for the 196-th batch, train loss: 0.5490718483924866:  82%|█████████  | 195/237 [00:38<00:08,  4.85it/s]evaluate for the 3-th batch, evaluate loss: 0.6643691062927246:   2%|▎                  | 2/106 [00:00<00:31,  3.28it/s]evaluate for the 3-th batch, evaluate loss: 0.6643691062927246:   3%|▌                  | 3/106 [00:00<00:27,  3.79it/s]Epoch: 2, train for the 196-th batch, train loss: 0.5490718483924866:  83%|█████████  | 196/237 [00:38<00:09,  4.45it/s]Epoch: 3, train for the 28-th batch, train loss: 0.4391772449016571:  11%|█▍           | 27/241 [00:04<00:35,  6.06it/s]Epoch: 3, train for the 28-th batch, train loss: 0.4391772449016571:  12%|█▌           | 28/241 [00:04<00:35,  5.97it/s]Epoch: 5, train for the 114-th batch, train loss: 0.3783201277256012:  95%|██████████▍| 113/119 [00:19<00:00,  6.47it/s]Epoch: 4, train for the 65-th batch, train loss: 0.46287962794303894:  42%|█████       | 64/151 [00:11<00:15,  5.51it/s]Epoch: 5, train for the 114-th batch, train loss: 0.3783201277256012:  96%|██████████▌| 114/119 [00:19<00:00,  6.01it/s]Epoch: 4, train for the 65-th batch, train loss: 0.46287962794303894:  43%|█████▏      | 65/151 [00:11<00:16,  5.21it/s]Epoch: 5, train for the 20-th batch, train loss: 0.4150381088256836:  13%|█▋           | 19/146 [00:03<00:22,  5.53it/s]Epoch: 5, train for the 20-th batch, train loss: 0.4150381088256836:  14%|█▊           | 20/146 [00:03<00:21,  5.81it/s]evaluate for the 4-th batch, evaluate loss: 0.590815007686615:   3%|▌                   | 3/106 [00:01<00:27,  3.79it/s]evaluate for the 4-th batch, evaluate loss: 0.590815007686615:   4%|▊                   | 4/106 [00:01<00:22,  4.44it/s]Epoch: 3, train for the 29-th batch, train loss: 0.598317563533783:  12%|█▋            | 28/241 [00:04<00:35,  5.97it/s]Epoch: 3, train for the 29-th batch, train loss: 0.598317563533783:  12%|█▋            | 29/241 [00:04<00:33,  6.27it/s]Epoch: 2, train for the 197-th batch, train loss: 0.5226069688796997:  83%|█████████  | 196/237 [00:38<00:09,  4.45it/s]Epoch: 2, train for the 197-th batch, train loss: 0.5226069688796997:  83%|█████████▏ | 197/237 [00:38<00:09,  4.44it/s]Epoch: 4, train for the 66-th batch, train loss: 0.513508141040802:  43%|██████        | 65/151 [00:11<00:16,  5.21it/s]Epoch: 4, train for the 66-th batch, train loss: 0.513508141040802:  44%|██████        | 66/151 [00:11<00:16,  5.25it/s]Epoch: 5, train for the 115-th batch, train loss: 0.3557300567626953:  96%|██████████▌| 114/119 [00:19<00:00,  6.01it/s]evaluate for the 5-th batch, evaluate loss: 0.605320394039154:   4%|▊                   | 4/106 [00:01<00:22,  4.44it/s]evaluate for the 5-th batch, evaluate loss: 0.605320394039154:   5%|▉                   | 5/106 [00:01<00:19,  5.15it/s]Epoch: 5, train for the 115-th batch, train loss: 0.3557300567626953:  97%|██████████▋| 115/119 [00:19<00:00,  5.48it/s]Epoch: 3, train for the 30-th batch, train loss: 0.6486244201660156:  12%|█▌           | 29/241 [00:04<00:33,  6.27it/s]Epoch: 3, train for the 30-th batch, train loss: 0.6486244201660156:  12%|█▌           | 30/241 [00:04<00:33,  6.39it/s]Epoch: 4, train for the 67-th batch, train loss: 0.5407633781433105:  44%|█████▋       | 66/151 [00:11<00:16,  5.25it/s]Epoch: 5, train for the 116-th batch, train loss: 0.30645042657852173:  97%|█████████▋| 115/119 [00:19<00:00,  5.48it/s]Epoch: 5, train for the 116-th batch, train loss: 0.30645042657852173:  97%|█████████▋| 116/119 [00:19<00:00,  5.92it/s]Epoch: 4, train for the 67-th batch, train loss: 0.5407633781433105:  44%|█████▊       | 67/151 [00:11<00:15,  5.50it/s]Epoch: 2, train for the 198-th batch, train loss: 0.5091699957847595:  83%|█████████▏ | 197/237 [00:38<00:09,  4.44it/s]Epoch: 3, train for the 31-th batch, train loss: 0.4661117494106293:  12%|█▌           | 30/241 [00:04<00:33,  6.39it/s]Epoch: 2, train for the 198-th batch, train loss: 0.5091699957847595:  84%|█████████▏ | 198/237 [00:38<00:08,  4.42it/s]Epoch: 3, train for the 31-th batch, train loss: 0.4661117494106293:  13%|█▋           | 31/241 [00:04<00:31,  6.63it/s]Epoch: 5, train for the 117-th batch, train loss: 0.3422929048538208:  97%|██████████▋| 116/119 [00:20<00:00,  5.92it/s]Epoch: 5, train for the 117-th batch, train loss: 0.3422929048538208:  98%|██████████▊| 117/119 [00:20<00:00,  6.54it/s]evaluate for the 6-th batch, evaluate loss: 0.6928651928901672:   5%|▉                  | 5/106 [00:01<00:19,  5.15it/s]evaluate for the 6-th batch, evaluate loss: 0.6928651928901672:   6%|█                  | 6/106 [00:01<00:23,  4.35it/s]Epoch: 4, train for the 68-th batch, train loss: 0.5033239722251892:  44%|█████▊       | 67/151 [00:11<00:15,  5.50it/s]Epoch: 3, train for the 32-th batch, train loss: 0.4169105291366577:  13%|█▋           | 31/241 [00:04<00:31,  6.63it/s]Epoch: 4, train for the 68-th batch, train loss: 0.5033239722251892:  45%|█████▊       | 68/151 [00:11<00:14,  5.56it/s]Epoch: 3, train for the 32-th batch, train loss: 0.4169105291366577:  13%|█▋           | 32/241 [00:04<00:30,  6.75it/s]Epoch: 5, train for the 21-th batch, train loss: 0.4523788094520569:  14%|█▊           | 20/146 [00:03<00:21,  5.81it/s]Epoch: 5, train for the 21-th batch, train loss: 0.4523788094520569:  14%|█▊           | 21/146 [00:03<00:35,  3.48it/s]Epoch: 2, train for the 199-th batch, train loss: 0.5377086400985718:  84%|█████████▏ | 198/237 [00:38<00:08,  4.42it/s]Epoch: 5, train for the 118-th batch, train loss: 0.2978723347187042:  98%|██████████▊| 117/119 [00:20<00:00,  6.54it/s]Epoch: 2, train for the 199-th batch, train loss: 0.5377086400985718:  84%|█████████▏ | 199/237 [00:39<00:08,  4.52it/s]Epoch: 5, train for the 118-th batch, train loss: 0.2978723347187042:  99%|██████████▉| 118/119 [00:20<00:00,  6.75it/s]evaluate for the 7-th batch, evaluate loss: 0.654671311378479:   6%|█▏                  | 6/106 [00:01<00:23,  4.35it/s]evaluate for the 7-th batch, evaluate loss: 0.654671311378479:   7%|█▎                  | 7/106 [00:01<00:19,  4.97it/s]Epoch: 4, train for the 69-th batch, train loss: 0.48989057540893555:  45%|█████▍      | 68/151 [00:11<00:14,  5.56it/s]Epoch: 3, train for the 33-th batch, train loss: 0.6127455830574036:  13%|█▋           | 32/241 [00:05<00:30,  6.75it/s]Epoch: 4, train for the 69-th batch, train loss: 0.48989057540893555:  46%|█████▍      | 69/151 [00:11<00:14,  5.56it/s]Epoch: 5, train for the 22-th batch, train loss: 0.4434354305267334:  14%|█▊           | 21/146 [00:03<00:35,  3.48it/s]Epoch: 3, train for the 33-th batch, train loss: 0.6127455830574036:  14%|█▊           | 33/241 [00:05<00:33,  6.27it/s]Epoch: 5, train for the 22-th batch, train loss: 0.4434354305267334:  15%|█▉           | 22/146 [00:03<00:30,  4.04it/s]Epoch: 5, train for the 119-th batch, train loss: 0.29317575693130493:  99%|█████████▉| 118/119 [00:20<00:00,  6.75it/s]Epoch: 5, train for the 119-th batch, train loss: 0.29317575693130493: 100%|██████████| 119/119 [00:20<00:00,  6.81it/s]Epoch: 5, train for the 119-th batch, train loss: 0.29317575693130493: 100%|██████████| 119/119 [00:20<00:00,  5.84it/s]
Epoch: 2, train for the 200-th batch, train loss: 0.5725106000900269:  84%|█████████▏ | 199/237 [00:39<00:08,  4.52it/s]evaluate for the 8-th batch, evaluate loss: 0.5259451270103455:   7%|█▎                 | 7/106 [00:01<00:19,  4.97it/s]evaluate for the 8-th batch, evaluate loss: 0.5259451270103455:   8%|█▍                 | 8/106 [00:01<00:19,  5.04it/s]Epoch: 5, train for the 23-th batch, train loss: 0.42137113213539124:  15%|█▊          | 22/146 [00:04<00:30,  4.04it/s]Epoch: 5, train for the 23-th batch, train loss: 0.42137113213539124:  16%|█▉          | 23/146 [00:04<00:26,  4.70it/s]Epoch: 2, train for the 200-th batch, train loss: 0.5725106000900269:  84%|█████████▎ | 200/237 [00:39<00:08,  4.21it/s]Epoch: 4, train for the 70-th batch, train loss: 0.5203208923339844:  46%|█████▉       | 69/151 [00:11<00:14,  5.56it/s]Epoch: 3, train for the 34-th batch, train loss: 0.5170198082923889:  14%|█▊           | 33/241 [00:05<00:33,  6.27it/s]Epoch: 3, train for the 34-th batch, train loss: 0.5170198082923889:  14%|█▊           | 34/241 [00:05<00:35,  5.88it/s]Epoch: 4, train for the 70-th batch, train loss: 0.5203208923339844:  46%|██████       | 70/151 [00:11<00:15,  5.24it/s]  0%|                                                                                            | 0/40 [00:00<?, ?it/s]evaluate for the 9-th batch, evaluate loss: 0.5382745862007141:   8%|█▍                 | 8/106 [00:01<00:19,  5.04it/s]evaluate for the 9-th batch, evaluate loss: 0.5382745862007141:   8%|█▌                 | 9/106 [00:01<00:17,  5.53it/s]Epoch: 5, train for the 24-th batch, train loss: 0.44052016735076904:  16%|█▉          | 23/146 [00:04<00:26,  4.70it/s]Epoch: 5, train for the 24-th batch, train loss: 0.44052016735076904:  16%|█▉          | 24/146 [00:04<00:23,  5.22it/s]evaluate for the 1-th batch, evaluate loss: 0.6185411810874939:   0%|                            | 0/40 [00:00<?, ?it/s]Epoch: 3, train for the 35-th batch, train loss: 0.572243332862854:  14%|█▉            | 34/241 [00:05<00:35,  5.88it/s]Epoch: 3, train for the 35-th batch, train loss: 0.572243332862854:  15%|██            | 35/241 [00:05<00:32,  6.28it/s]Epoch: 2, train for the 201-th batch, train loss: 0.521872341632843:  84%|██████████▏ | 200/237 [00:39<00:08,  4.21it/s]Epoch: 2, train for the 201-th batch, train loss: 0.521872341632843:  85%|██████████▏ | 201/237 [00:39<00:08,  4.34it/s]Epoch: 4, train for the 71-th batch, train loss: 0.5767796039581299:  46%|██████       | 70/151 [00:12<00:15,  5.24it/s]Epoch: 4, train for the 71-th batch, train loss: 0.5767796039581299:  47%|██████       | 71/151 [00:12<00:15,  5.28it/s]evaluate for the 2-th batch, evaluate loss: 0.6263624429702759:   0%|                            | 0/40 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.6263624429702759:   5%|█                   | 2/40 [00:00<00:02, 13.25it/s]Epoch: 5, train for the 25-th batch, train loss: 0.49633094668388367:  16%|█▉          | 24/146 [00:04<00:23,  5.22it/s]Epoch: 5, train for the 25-th batch, train loss: 0.49633094668388367:  17%|██          | 25/146 [00:04<00:22,  5.40it/s]Epoch: 3, train for the 36-th batch, train loss: 0.5643117427825928:  15%|█▉           | 35/241 [00:05<00:32,  6.28it/s]evaluate for the 3-th batch, evaluate loss: 0.69205641746521:   5%|█                     | 2/40 [00:00<00:02, 13.25it/s]Epoch: 3, train for the 36-th batch, train loss: 0.5643117427825928:  15%|█▉           | 36/241 [00:05<00:32,  6.36it/s]evaluate for the 10-th batch, evaluate loss: 0.535830557346344:   8%|█▌                 | 9/106 [00:02<00:17,  5.53it/s]evaluate for the 10-th batch, evaluate loss: 0.535830557346344:   9%|█▋                | 10/106 [00:02<00:18,  5.13it/s]Epoch: 4, train for the 72-th batch, train loss: 0.5236890316009521:  47%|██████       | 71/151 [00:12<00:15,  5.28it/s]evaluate for the 4-th batch, evaluate loss: 0.7362699508666992:   5%|█                   | 2/40 [00:00<00:02, 13.25it/s]evaluate for the 4-th batch, evaluate loss: 0.7362699508666992:  10%|██                  | 4/40 [00:00<00:02, 13.96it/s]Epoch: 4, train for the 72-th batch, train loss: 0.5236890316009521:  48%|██████▏      | 72/151 [00:12<00:13,  5.65it/s]Epoch: 2, train for the 202-th batch, train loss: 0.5402299165725708:  85%|█████████▎ | 201/237 [00:39<00:08,  4.34it/s]Epoch: 5, train for the 26-th batch, train loss: 0.3849126994609833:  17%|██▏          | 25/146 [00:04<00:22,  5.40it/s]Epoch: 5, train for the 26-th batch, train loss: 0.3849126994609833:  18%|██▎          | 26/146 [00:04<00:20,  5.84it/s]evaluate for the 5-th batch, evaluate loss: 0.7328290939331055:  10%|██                  | 4/40 [00:00<00:02, 13.96it/s]Epoch: 3, train for the 37-th batch, train loss: 0.4872923791408539:  15%|█▉           | 36/241 [00:05<00:32,  6.36it/s]Epoch: 3, train for the 37-th batch, train loss: 0.4872923791408539:  15%|█▉           | 37/241 [00:05<00:31,  6.50it/s]Epoch: 2, train for the 202-th batch, train loss: 0.5402299165725708:  85%|█████████▍ | 202/237 [00:39<00:08,  4.17it/s]evaluate for the 11-th batch, evaluate loss: 0.677700936794281:   9%|█▋                | 10/106 [00:02<00:18,  5.13it/s]evaluate for the 11-th batch, evaluate loss: 0.677700936794281:  10%|█▊                | 11/106 [00:02<00:17,  5.30it/s]Epoch: 4, train for the 73-th batch, train loss: 0.5637571811676025:  48%|██████▏      | 72/151 [00:12<00:13,  5.65it/s]Epoch: 5, train for the 27-th batch, train loss: 0.4592859447002411:  18%|██▎          | 26/146 [00:04<00:20,  5.84it/s]Epoch: 5, train for the 27-th batch, train loss: 0.4592859447002411:  18%|██▍          | 27/146 [00:04<00:18,  6.34it/s]evaluate for the 6-th batch, evaluate loss: 0.6944903135299683:  10%|██                  | 4/40 [00:00<00:02, 13.96it/s]evaluate for the 6-th batch, evaluate loss: 0.6944903135299683:  15%|███                 | 6/40 [00:00<00:02, 12.54it/s]Epoch: 4, train for the 73-th batch, train loss: 0.5637571811676025:  48%|██████▎      | 73/151 [00:12<00:13,  5.57it/s]Epoch: 3, train for the 38-th batch, train loss: 0.534063458442688:  15%|██▏           | 37/241 [00:05<00:31,  6.50it/s]Epoch: 3, train for the 38-th batch, train loss: 0.534063458442688:  16%|██▏           | 38/241 [00:05<00:30,  6.59it/s]evaluate for the 7-th batch, evaluate loss: 0.7076594233512878:  15%|███                 | 6/40 [00:00<00:02, 12.54it/s]Epoch: 2, train for the 203-th batch, train loss: 0.5240700840950012:  85%|█████████▍ | 202/237 [00:39<00:08,  4.17it/s]Epoch: 5, train for the 28-th batch, train loss: 0.42382559180259705:  18%|██▏         | 27/146 [00:04<00:18,  6.34it/s]Epoch: 5, train for the 28-th batch, train loss: 0.42382559180259705:  19%|██▎         | 28/146 [00:04<00:18,  6.55it/s]Epoch: 2, train for the 203-th batch, train loss: 0.5240700840950012:  86%|█████████▍ | 203/237 [00:40<00:07,  4.25it/s]evaluate for the 8-th batch, evaluate loss: 0.6679975986480713:  15%|███                 | 6/40 [00:00<00:02, 12.54it/s]evaluate for the 8-th batch, evaluate loss: 0.6679975986480713:  20%|████                | 8/40 [00:00<00:02, 12.59it/s]Epoch: 4, train for the 74-th batch, train loss: 0.5091861486434937:  48%|██████▎      | 73/151 [00:12<00:13,  5.57it/s]Epoch: 3, train for the 39-th batch, train loss: 0.603764533996582:  16%|██▏           | 38/241 [00:06<00:30,  6.59it/s]Epoch: 4, train for the 74-th batch, train loss: 0.5091861486434937:  49%|██████▎      | 74/151 [00:12<00:14,  5.35it/s]evaluate for the 9-th batch, evaluate loss: 0.671485424041748:  20%|████▏                | 8/40 [00:00<00:02, 12.59it/s]Epoch: 3, train for the 39-th batch, train loss: 0.603764533996582:  16%|██▎           | 39/241 [00:06<00:31,  6.47it/s]Epoch: 5, train for the 29-th batch, train loss: 0.43969714641571045:  19%|██▎         | 28/146 [00:04<00:18,  6.55it/s]Epoch: 5, train for the 29-th batch, train loss: 0.43969714641571045:  20%|██▍         | 29/146 [00:04<00:17,  6.56it/s]evaluate for the 10-th batch, evaluate loss: 0.7674238681793213:  20%|███▊               | 8/40 [00:00<00:02, 12.59it/s]evaluate for the 10-th batch, evaluate loss: 0.7674238681793213:  25%|████▌             | 10/40 [00:00<00:02, 13.36it/s]Epoch: 2, train for the 204-th batch, train loss: 0.5159682035446167:  86%|█████████▍ | 203/237 [00:40<00:07,  4.25it/s]Epoch: 3, train for the 40-th batch, train loss: 0.6002255082130432:  16%|██           | 39/241 [00:06<00:31,  6.47it/s]Epoch: 2, train for the 204-th batch, train loss: 0.5159682035446167:  86%|█████████▍ | 204/237 [00:40<00:07,  4.43it/s]evaluate for the 12-th batch, evaluate loss: 0.47298166155815125:  10%|█▋              | 11/106 [00:02<00:17,  5.30it/s]evaluate for the 12-th batch, evaluate loss: 0.47298166155815125:  11%|█▊              | 12/106 [00:02<00:24,  3.91it/s]Epoch: 3, train for the 40-th batch, train loss: 0.6002255082130432:  17%|██▏          | 40/241 [00:06<00:30,  6.65it/s]evaluate for the 11-th batch, evaluate loss: 0.6298559308052063:  25%|████▌             | 10/40 [00:00<00:02, 13.36it/s]evaluate for the 12-th batch, evaluate loss: 0.6184311509132385:  25%|████▌             | 10/40 [00:00<00:02, 13.36it/s]evaluate for the 12-th batch, evaluate loss: 0.6184311509132385:  30%|█████▍            | 12/40 [00:00<00:02, 13.49it/s]evaluate for the 13-th batch, evaluate loss: 0.640385091304779:  30%|█████▋             | 12/40 [00:00<00:02, 13.49it/s]Epoch: 4, train for the 75-th batch, train loss: 0.5386583209037781:  49%|██████▎      | 74/151 [00:12<00:14,  5.35it/s]Epoch: 3, train for the 41-th batch, train loss: 0.5416205525398254:  17%|██▏          | 40/241 [00:06<00:30,  6.65it/s]Epoch: 3, train for the 41-th batch, train loss: 0.5416205525398254:  17%|██▏          | 41/241 [00:06<00:31,  6.30it/s]Epoch: 4, train for the 75-th batch, train loss: 0.5386583209037781:  50%|██████▍      | 75/151 [00:13<00:17,  4.35it/s]Epoch: 2, train for the 205-th batch, train loss: 0.5044240951538086:  86%|█████████▍ | 204/237 [00:40<00:07,  4.43it/s]evaluate for the 13-th batch, evaluate loss: 0.5570771098136902:  11%|█▉               | 12/106 [00:02<00:24,  3.91it/s]evaluate for the 13-th batch, evaluate loss: 0.5570771098136902:  12%|██               | 13/106 [00:02<00:21,  4.24it/s]Epoch: 5, train for the 30-th batch, train loss: 0.42917999625205994:  20%|██▍         | 29/146 [00:05<00:17,  6.56it/s]Epoch: 5, train for the 30-th batch, train loss: 0.42917999625205994:  21%|██▍         | 30/146 [00:05<00:21,  5.31it/s]evaluate for the 14-th batch, evaluate loss: 0.670062243938446:  30%|█████▋             | 12/40 [00:01<00:02, 13.49it/s]evaluate for the 14-th batch, evaluate loss: 0.670062243938446:  35%|██████▋            | 14/40 [00:01<00:01, 13.92it/s]Epoch: 2, train for the 205-th batch, train loss: 0.5044240951538086:  86%|█████████▌ | 205/237 [00:40<00:07,  4.39it/s]evaluate for the 15-th batch, evaluate loss: 0.6673731207847595:  35%|██████▎           | 14/40 [00:01<00:01, 13.92it/s]Epoch: 3, train for the 42-th batch, train loss: 0.5578761696815491:  17%|██▏          | 41/241 [00:06<00:31,  6.30it/s]Epoch: 5, train for the 31-th batch, train loss: 0.4336184561252594:  21%|██▋          | 30/146 [00:05<00:21,  5.31it/s]Epoch: 3, train for the 42-th batch, train loss: 0.5578761696815491:  17%|██▎          | 42/241 [00:06<00:34,  5.84it/s]Epoch: 5, train for the 31-th batch, train loss: 0.4336184561252594:  21%|██▊          | 31/146 [00:05<00:21,  5.39it/s]evaluate for the 16-th batch, evaluate loss: 0.7549883723258972:  35%|██████▎           | 14/40 [00:01<00:01, 13.92it/s]evaluate for the 16-th batch, evaluate loss: 0.7549883723258972:  40%|███████▏          | 16/40 [00:01<00:01, 13.11it/s]Epoch: 4, train for the 76-th batch, train loss: 0.5300832390785217:  50%|██████▍      | 75/151 [00:13<00:17,  4.35it/s]Epoch: 4, train for the 76-th batch, train loss: 0.5300832390785217:  50%|██████▌      | 76/151 [00:13<00:17,  4.35it/s]evaluate for the 17-th batch, evaluate loss: 0.6914572715759277:  40%|███████▏          | 16/40 [00:01<00:01, 13.11it/s]Epoch: 3, train for the 43-th batch, train loss: 0.6383358836174011:  17%|██▎          | 42/241 [00:06<00:34,  5.84it/s]Epoch: 3, train for the 43-th batch, train loss: 0.6383358836174011:  18%|██▎          | 43/241 [00:06<00:31,  6.32it/s]Epoch: 5, train for the 32-th batch, train loss: 0.4054223597049713:  21%|██▊          | 31/146 [00:05<00:21,  5.39it/s]Epoch: 5, train for the 32-th batch, train loss: 0.4054223597049713:  22%|██▊          | 32/146 [00:05<00:19,  5.73it/s]evaluate for the 18-th batch, evaluate loss: 0.6501783728599548:  40%|███████▏          | 16/40 [00:01<00:01, 13.11it/s]evaluate for the 18-th batch, evaluate loss: 0.6501783728599548:  45%|████████          | 18/40 [00:01<00:01, 12.52it/s]Epoch: 4, train for the 77-th batch, train loss: 0.506935179233551:  50%|███████       | 76/151 [00:13<00:17,  4.35it/s]Epoch: 2, train for the 206-th batch, train loss: 0.5130309462547302:  86%|█████████▌ | 205/237 [00:40<00:07,  4.39it/s]evaluate for the 14-th batch, evaluate loss: 0.6399087309837341:  12%|██               | 13/106 [00:03<00:21,  4.24it/s]evaluate for the 14-th batch, evaluate loss: 0.6399087309837341:  13%|██▏              | 14/106 [00:03<00:26,  3.47it/s]Epoch: 4, train for the 77-th batch, train loss: 0.506935179233551:  51%|███████▏      | 77/151 [00:13<00:16,  4.61it/s]Epoch: 2, train for the 206-th batch, train loss: 0.5130309462547302:  87%|█████████▌ | 206/237 [00:40<00:08,  3.53it/s]Epoch: 3, train for the 44-th batch, train loss: 0.6540069580078125:  18%|██▎          | 43/241 [00:06<00:31,  6.32it/s]evaluate for the 19-th batch, evaluate loss: 0.7637879252433777:  45%|████████          | 18/40 [00:01<00:01, 12.52it/s]Epoch: 3, train for the 44-th batch, train loss: 0.6540069580078125:  18%|██▎          | 44/241 [00:06<00:30,  6.39it/s]Epoch: 5, train for the 33-th batch, train loss: 0.40723204612731934:  22%|██▋         | 32/146 [00:05<00:19,  5.73it/s]Epoch: 5, train for the 33-th batch, train loss: 0.40723204612731934:  23%|██▋         | 33/146 [00:05<00:19,  5.95it/s]evaluate for the 20-th batch, evaluate loss: 0.7184181213378906:  45%|████████          | 18/40 [00:01<00:01, 12.52it/s]evaluate for the 20-th batch, evaluate loss: 0.7184181213378906:  50%|█████████         | 20/40 [00:01<00:01, 12.29it/s]Epoch: 4, train for the 78-th batch, train loss: 0.4879935383796692:  51%|██████▋      | 77/151 [00:13<00:16,  4.61it/s]Epoch: 4, train for the 78-th batch, train loss: 0.4879935383796692:  52%|██████▋      | 78/151 [00:13<00:15,  4.85it/s]evaluate for the 15-th batch, evaluate loss: 0.5989475250244141:  13%|██▏              | 14/106 [00:03<00:26,  3.47it/s]evaluate for the 15-th batch, evaluate loss: 0.5989475250244141:  14%|██▍              | 15/106 [00:03<00:23,  3.86it/s]Epoch: 3, train for the 45-th batch, train loss: 0.4855566918849945:  18%|██▎          | 44/241 [00:06<00:30,  6.39it/s]Epoch: 3, train for the 45-th batch, train loss: 0.4855566918849945:  19%|██▍          | 45/241 [00:06<00:30,  6.41it/s]Epoch: 5, train for the 34-th batch, train loss: 0.4513324499130249:  23%|██▉          | 33/146 [00:05<00:19,  5.95it/s]Epoch: 5, train for the 34-th batch, train loss: 0.4513324499130249:  23%|███          | 34/146 [00:05<00:18,  6.12it/s]evaluate for the 21-th batch, evaluate loss: 0.6330114006996155:  50%|█████████         | 20/40 [00:01<00:01, 12.29it/s]Epoch: 2, train for the 207-th batch, train loss: 0.5847535729408264:  87%|█████████▌ | 206/237 [00:41<00:08,  3.53it/s]Epoch: 2, train for the 207-th batch, train loss: 0.5847535729408264:  87%|█████████▌ | 207/237 [00:41<00:08,  3.61it/s]evaluate for the 22-th batch, evaluate loss: 0.6457144618034363:  50%|█████████         | 20/40 [00:01<00:01, 12.29it/s]evaluate for the 22-th batch, evaluate loss: 0.6457144618034363:  55%|█████████▉        | 22/40 [00:01<00:01, 11.58it/s]Epoch: 4, train for the 79-th batch, train loss: 0.5825047492980957:  52%|██████▋      | 78/151 [00:13<00:15,  4.85it/s]Epoch: 4, train for the 79-th batch, train loss: 0.5825047492980957:  52%|██████▊      | 79/151 [00:13<00:14,  4.93it/s]Epoch: 3, train for the 46-th batch, train loss: 0.5240739583969116:  19%|██▍          | 45/241 [00:07<00:30,  6.41it/s]Epoch: 3, train for the 46-th batch, train loss: 0.5240739583969116:  19%|██▍          | 46/241 [00:07<00:32,  6.06it/s]Epoch: 5, train for the 35-th batch, train loss: 0.45059165358543396:  23%|██▊         | 34/146 [00:06<00:18,  6.12it/s]evaluate for the 23-th batch, evaluate loss: 0.5813588500022888:  55%|█████████▉        | 22/40 [00:01<00:01, 11.58it/s]Epoch: 5, train for the 35-th batch, train loss: 0.45059165358543396:  24%|██▉         | 35/146 [00:06<00:18,  6.00it/s]Epoch: 2, train for the 208-th batch, train loss: 0.5462139248847961:  87%|█████████▌ | 207/237 [00:41<00:08,  3.61it/s]Epoch: 2, train for the 208-th batch, train loss: 0.5462139248847961:  88%|█████████▋ | 208/237 [00:41<00:07,  4.01it/s]evaluate for the 24-th batch, evaluate loss: 0.6821503043174744:  55%|█████████▉        | 22/40 [00:01<00:01, 11.58it/s]evaluate for the 24-th batch, evaluate loss: 0.6821503043174744:  60%|██████████▊       | 24/40 [00:01<00:01, 11.92it/s]Epoch: 4, train for the 80-th batch, train loss: 0.5075302720069885:  52%|██████▊      | 79/151 [00:13<00:14,  4.93it/s]Epoch: 5, train for the 36-th batch, train loss: 0.3984176516532898:  24%|███          | 35/146 [00:06<00:18,  6.00it/s]Epoch: 4, train for the 80-th batch, train loss: 0.5075302720069885:  53%|██████▉      | 80/151 [00:13<00:13,  5.30it/s]Epoch: 5, train for the 36-th batch, train loss: 0.3984176516532898:  25%|███▏         | 36/146 [00:06<00:16,  6.52it/s]evaluate for the 16-th batch, evaluate loss: 0.7278585433959961:  14%|██▍              | 15/106 [00:03<00:23,  3.86it/s]evaluate for the 16-th batch, evaluate loss: 0.7278585433959961:  15%|██▌              | 16/106 [00:03<00:25,  3.47it/s]Epoch: 3, train for the 47-th batch, train loss: 0.6170333623886108:  19%|██▍          | 46/241 [00:07<00:32,  6.06it/s]evaluate for the 25-th batch, evaluate loss: 0.6854110360145569:  60%|██████████▊       | 24/40 [00:01<00:01, 11.92it/s]Epoch: 3, train for the 47-th batch, train loss: 0.6170333623886108:  20%|██▌          | 47/241 [00:07<00:32,  6.03it/s]evaluate for the 26-th batch, evaluate loss: 0.6340553760528564:  60%|██████████▊       | 24/40 [00:02<00:01, 11.92it/s]evaluate for the 26-th batch, evaluate loss: 0.6340553760528564:  65%|███████████▋      | 26/40 [00:02<00:01, 11.74it/s]Epoch: 5, train for the 37-th batch, train loss: 0.4281903803348541:  25%|███▏         | 36/146 [00:06<00:16,  6.52it/s]Epoch: 5, train for the 37-th batch, train loss: 0.4281903803348541:  25%|███▎         | 37/146 [00:06<00:17,  6.39it/s]Epoch: 2, train for the 209-th batch, train loss: 0.534069836139679:  88%|██████████▌ | 208/237 [00:41<00:07,  4.01it/s]evaluate for the 27-th batch, evaluate loss: 0.7135896682739258:  65%|███████████▋      | 26/40 [00:02<00:01, 11.74it/s]evaluate for the 17-th batch, evaluate loss: 0.672678530216217:  15%|██▋               | 16/106 [00:04<00:25,  3.47it/s]evaluate for the 17-th batch, evaluate loss: 0.672678530216217:  16%|██▉               | 17/106 [00:04<00:22,  3.91it/s]Epoch: 4, train for the 81-th batch, train loss: 0.4791151285171509:  53%|██████▉      | 80/151 [00:14<00:13,  5.30it/s]Epoch: 3, train for the 48-th batch, train loss: 0.5520684719085693:  20%|██▌          | 47/241 [00:07<00:32,  6.03it/s]Epoch: 4, train for the 81-th batch, train loss: 0.4791151285171509:  54%|██████▉      | 81/151 [00:14<00:13,  5.11it/s]Epoch: 3, train for the 48-th batch, train loss: 0.5520684719085693:  20%|██▌          | 48/241 [00:07<00:32,  5.89it/s]Epoch: 2, train for the 209-th batch, train loss: 0.534069836139679:  88%|██████████▌ | 209/237 [00:41<00:07,  3.88it/s]evaluate for the 28-th batch, evaluate loss: 0.648070752620697:  65%|████████████▎      | 26/40 [00:02<00:01, 11.74it/s]evaluate for the 28-th batch, evaluate loss: 0.648070752620697:  70%|█████████████▎     | 28/40 [00:02<00:00, 12.31it/s]Epoch: 5, train for the 38-th batch, train loss: 0.42023521661758423:  25%|███         | 37/146 [00:06<00:17,  6.39it/s]Epoch: 5, train for the 38-th batch, train loss: 0.42023521661758423:  26%|███         | 38/146 [00:06<00:17,  6.31it/s]evaluate for the 29-th batch, evaluate loss: 0.7056057453155518:  70%|████████████▌     | 28/40 [00:02<00:00, 12.31it/s]Epoch: 3, train for the 49-th batch, train loss: 0.5714446306228638:  20%|██▌          | 48/241 [00:07<00:32,  5.89it/s]Epoch: 3, train for the 49-th batch, train loss: 0.5714446306228638:  20%|██▋          | 49/241 [00:07<00:32,  5.89it/s]evaluate for the 30-th batch, evaluate loss: 0.629116415977478:  70%|█████████████▎     | 28/40 [00:02<00:00, 12.31it/s]evaluate for the 30-th batch, evaluate loss: 0.629116415977478:  75%|██████████████▎    | 30/40 [00:02<00:00, 13.22it/s]Epoch: 2, train for the 210-th batch, train loss: 0.5400897264480591:  88%|█████████▋ | 209/237 [00:41<00:07,  3.88it/s]Epoch: 4, train for the 82-th batch, train loss: 0.5215824842453003:  54%|██████▉      | 81/151 [00:14<00:13,  5.11it/s]Epoch: 4, train for the 82-th batch, train loss: 0.5215824842453003:  54%|███████      | 82/151 [00:14<00:14,  4.92it/s]Epoch: 2, train for the 210-th batch, train loss: 0.5400897264480591:  89%|█████████▋ | 210/237 [00:41<00:06,  4.02it/s]Epoch: 5, train for the 39-th batch, train loss: 0.4550096392631531:  26%|███▍         | 38/146 [00:06<00:17,  6.31it/s]evaluate for the 31-th batch, evaluate loss: 0.7107091546058655:  75%|█████████████▌    | 30/40 [00:02<00:00, 13.22it/s]Epoch: 5, train for the 39-th batch, train loss: 0.4550096392631531:  27%|███▍         | 39/146 [00:06<00:16,  6.40it/s]Epoch: 3, train for the 50-th batch, train loss: 0.5240127444267273:  20%|██▋          | 49/241 [00:07<00:32,  5.89it/s]Epoch: 3, train for the 50-th batch, train loss: 0.5240127444267273:  21%|██▋          | 50/241 [00:07<00:30,  6.18it/s]evaluate for the 32-th batch, evaluate loss: 0.6702277064323425:  75%|█████████████▌    | 30/40 [00:02<00:00, 13.22it/s]evaluate for the 32-th batch, evaluate loss: 0.6702277064323425:  80%|██████████████▍   | 32/40 [00:02<00:00, 13.23it/s]Epoch: 4, train for the 83-th batch, train loss: 0.5215139985084534:  54%|███████      | 82/151 [00:14<00:14,  4.92it/s]evaluate for the 18-th batch, evaluate loss: 0.6018235087394714:  16%|██▋              | 17/106 [00:04<00:22,  3.91it/s]evaluate for the 18-th batch, evaluate loss: 0.6018235087394714:  17%|██▉              | 18/106 [00:04<00:25,  3.39it/s]Epoch: 4, train for the 83-th batch, train loss: 0.5215139985084534:  55%|███████▏     | 83/151 [00:14<00:12,  5.25it/s]Epoch: 5, train for the 40-th batch, train loss: 0.41417577862739563:  27%|███▏        | 39/146 [00:06<00:16,  6.40it/s]evaluate for the 33-th batch, evaluate loss: 0.6095777153968811:  80%|██████████████▍   | 32/40 [00:02<00:00, 13.23it/s]Epoch: 5, train for the 40-th batch, train loss: 0.41417577862739563:  27%|███▎        | 40/146 [00:06<00:15,  6.64it/s]Epoch: 2, train for the 211-th batch, train loss: 0.5059439539909363:  89%|█████████▋ | 210/237 [00:41<00:06,  4.02it/s]Epoch: 3, train for the 51-th batch, train loss: 0.5805291533470154:  21%|██▋          | 50/241 [00:07<00:30,  6.18it/s]Epoch: 3, train for the 51-th batch, train loss: 0.5805291533470154:  21%|██▊          | 51/241 [00:07<00:29,  6.44it/s]evaluate for the 34-th batch, evaluate loss: 0.604313850402832:  80%|███████████████▏   | 32/40 [00:02<00:00, 13.23it/s]evaluate for the 34-th batch, evaluate loss: 0.604313850402832:  85%|████████████████▏  | 34/40 [00:02<00:00, 13.67it/s]Epoch: 2, train for the 211-th batch, train loss: 0.5059439539909363:  89%|█████████▊ | 211/237 [00:42<00:06,  4.04it/s]Epoch: 5, train for the 41-th batch, train loss: 0.4399867057800293:  27%|███▌         | 40/146 [00:06<00:15,  6.64it/s]Epoch: 5, train for the 41-th batch, train loss: 0.4399867057800293:  28%|███▋         | 41/146 [00:06<00:15,  6.95it/s]Epoch: 4, train for the 84-th batch, train loss: 0.5206857323646545:  55%|███████▏     | 83/151 [00:14<00:12,  5.25it/s]evaluate for the 35-th batch, evaluate loss: 0.7099906802177429:  85%|███████████████▎  | 34/40 [00:02<00:00, 13.67it/s]evaluate for the 19-th batch, evaluate loss: 0.40930038690567017:  17%|██▋             | 18/106 [00:04<00:25,  3.39it/s]evaluate for the 19-th batch, evaluate loss: 0.40930038690567017:  18%|██▊             | 19/106 [00:04<00:22,  3.84it/s]Epoch: 4, train for the 84-th batch, train loss: 0.5206857323646545:  56%|███████▏     | 84/151 [00:14<00:12,  5.33it/s]Epoch: 3, train for the 52-th batch, train loss: 0.5620151162147522:  21%|██▊          | 51/241 [00:08<00:29,  6.44it/s]evaluate for the 36-th batch, evaluate loss: 0.6808958053588867:  85%|███████████████▎  | 34/40 [00:02<00:00, 13.67it/s]evaluate for the 36-th batch, evaluate loss: 0.6808958053588867:  90%|████████████████▏ | 36/40 [00:02<00:00, 14.14it/s]Epoch: 3, train for the 52-th batch, train loss: 0.5620151162147522:  22%|██▊          | 52/241 [00:08<00:29,  6.44it/s]Epoch: 5, train for the 42-th batch, train loss: 0.4274762272834778:  28%|███▋         | 41/146 [00:07<00:15,  6.95it/s]Epoch: 5, train for the 42-th batch, train loss: 0.4274762272834778:  29%|███▋         | 42/146 [00:07<00:14,  7.15it/s]evaluate for the 37-th batch, evaluate loss: 0.6492578983306885:  90%|████████████████▏ | 36/40 [00:02<00:00, 14.14it/s]Epoch: 2, train for the 212-th batch, train loss: 0.5344144701957703:  89%|█████████▊ | 211/237 [00:42<00:06,  4.04it/s]Epoch: 4, train for the 85-th batch, train loss: 0.4668510854244232:  56%|███████▏     | 84/151 [00:14<00:12,  5.33it/s]evaluate for the 38-th batch, evaluate loss: 0.6431223154067993:  90%|████████████████▏ | 36/40 [00:02<00:00, 14.14it/s]evaluate for the 38-th batch, evaluate loss: 0.6431223154067993:  95%|█████████████████ | 38/40 [00:02<00:00, 14.53it/s]Epoch: 4, train for the 85-th batch, train loss: 0.4668510854244232:  56%|███████▎     | 85/151 [00:14<00:12,  5.45it/s]Epoch: 3, train for the 53-th batch, train loss: 0.4202928841114044:  22%|██▊          | 52/241 [00:08<00:29,  6.44it/s]Epoch: 2, train for the 212-th batch, train loss: 0.5344144701957703:  89%|█████████▊ | 212/237 [00:42<00:06,  3.96it/s]Epoch: 3, train for the 53-th batch, train loss: 0.4202928841114044:  22%|██▊          | 53/241 [00:08<00:28,  6.62it/s]evaluate for the 39-th batch, evaluate loss: 0.6951168179512024:  95%|█████████████████ | 38/40 [00:02<00:00, 14.53it/s]Epoch: 5, train for the 43-th batch, train loss: 0.4304293394088745:  29%|███▋         | 42/146 [00:07<00:14,  7.15it/s]Epoch: 5, train for the 43-th batch, train loss: 0.4304293394088745:  29%|███▊         | 43/146 [00:07<00:15,  6.85it/s]evaluate for the 40-th batch, evaluate loss: 0.5020373463630676:  95%|█████████████████ | 38/40 [00:03<00:00, 14.53it/s]evaluate for the 40-th batch, evaluate loss: 0.5020373463630676: 100%|██████████████████| 40/40 [00:03<00:00, 14.64it/s]evaluate for the 40-th batch, evaluate loss: 0.5020373463630676: 100%|██████████████████| 40/40 [00:03<00:00, 13.19it/s]
Epoch: 4, train for the 86-th batch, train loss: 0.49755048751831055:  56%|██████▊     | 85/151 [00:15<00:12,  5.45it/s]Epoch: 3, train for the 54-th batch, train loss: 0.37581151723861694:  22%|██▋         | 53/241 [00:08<00:28,  6.62it/s]Epoch: 3, train for the 54-th batch, train loss: 0.37581151723861694:  22%|██▋         | 54/241 [00:08<00:30,  6.22it/s]Epoch: 4, train for the 86-th batch, train loss: 0.49755048751831055:  57%|██████▊     | 86/151 [00:15<00:12,  5.23it/s]Epoch: 2, train for the 213-th batch, train loss: 0.5580683946609497:  89%|█████████▊ | 212/237 [00:42<00:06,  3.96it/s]evaluate for the 20-th batch, evaluate loss: 0.5326998829841614:  18%|███              | 19/106 [00:05<00:22,  3.84it/s]evaluate for the 20-th batch, evaluate loss: 0.5326998829841614:  19%|███▏             | 20/106 [00:05<00:26,  3.30it/s]  0%|                                                                                            | 0/21 [00:00<?, ?it/s]Epoch: 2, train for the 213-th batch, train loss: 0.5580683946609497:  90%|█████████▉ | 213/237 [00:42<00:05,  4.06it/s]Epoch: 5, train for the 44-th batch, train loss: 0.4297199845314026:  29%|███▊         | 43/146 [00:07<00:15,  6.85it/s]Epoch: 5, train for the 44-th batch, train loss: 0.4297199845314026:  30%|███▉         | 44/146 [00:07<00:15,  6.57it/s]Epoch: 3, train for the 55-th batch, train loss: 0.3612781763076782:  22%|██▉          | 54/241 [00:08<00:30,  6.22it/s]evaluate for the 1-th batch, evaluate loss: 1.0105851888656616:   0%|                            | 0/21 [00:00<?, ?it/s]Epoch: 3, train for the 55-th batch, train loss: 0.3612781763076782:  23%|██▉          | 55/241 [00:08<00:28,  6.55it/s]Epoch: 4, train for the 87-th batch, train loss: 0.49402666091918945:  57%|██████▊     | 86/151 [00:15<00:12,  5.23it/s]Epoch: 4, train for the 87-th batch, train loss: 0.49402666091918945:  58%|██████▉     | 87/151 [00:15<00:12,  5.28it/s]Epoch: 5, train for the 45-th batch, train loss: 0.4262947738170624:  30%|███▉         | 44/146 [00:07<00:15,  6.57it/s]evaluate for the 21-th batch, evaluate loss: 0.41698646545410156:  19%|███             | 20/106 [00:05<00:26,  3.30it/s]evaluate for the 21-th batch, evaluate loss: 0.41698646545410156:  20%|███▏            | 21/106 [00:05<00:23,  3.60it/s]Epoch: 5, train for the 45-th batch, train loss: 0.4262947738170624:  31%|████         | 45/146 [00:07<00:16,  6.21it/s]evaluate for the 2-th batch, evaluate loss: 1.1584949493408203:   0%|                            | 0/21 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 1.1584949493408203:  10%|█▉                  | 2/21 [00:00<00:01, 10.34it/s]Epoch: 2, train for the 214-th batch, train loss: 0.5411579608917236:  90%|█████████▉ | 213/237 [00:42<00:05,  4.06it/s]Epoch: 3, train for the 56-th batch, train loss: 0.3764013648033142:  23%|██▉          | 55/241 [00:08<00:28,  6.55it/s]Epoch: 3, train for the 56-th batch, train loss: 0.3764013648033142:  23%|███          | 56/241 [00:08<00:27,  6.63it/s]Epoch: 2, train for the 214-th batch, train loss: 0.5411579608917236:  90%|█████████▉ | 214/237 [00:42<00:05,  4.03it/s]evaluate for the 3-th batch, evaluate loss: 1.113627552986145:  10%|██                   | 2/21 [00:00<00:01, 10.34it/s]Epoch: 4, train for the 88-th batch, train loss: 0.5151680707931519:  58%|███████▍     | 87/151 [00:15<00:12,  5.28it/s]Epoch: 4, train for the 88-th batch, train loss: 0.5151680707931519:  58%|███████▌     | 88/151 [00:15<00:11,  5.44it/s]evaluate for the 4-th batch, evaluate loss: 0.9841687083244324:  10%|█▉                  | 2/21 [00:00<00:01, 10.34it/s]evaluate for the 4-th batch, evaluate loss: 0.9841687083244324:  19%|███▊                | 4/21 [00:00<00:01, 11.75it/s]Epoch: 5, train for the 46-th batch, train loss: 0.4277794063091278:  31%|████         | 45/146 [00:07<00:16,  6.21it/s]Epoch: 3, train for the 57-th batch, train loss: 0.4245363473892212:  23%|███          | 56/241 [00:08<00:27,  6.63it/s]Epoch: 5, train for the 46-th batch, train loss: 0.4277794063091278:  32%|████         | 46/146 [00:07<00:16,  6.16it/s]Epoch: 3, train for the 57-th batch, train loss: 0.4245363473892212:  24%|███          | 57/241 [00:08<00:26,  6.92it/s]Epoch: 2, train for the 215-th batch, train loss: 0.5161037445068359:  90%|█████████▉ | 214/237 [00:42<00:05,  4.03it/s]evaluate for the 5-th batch, evaluate loss: 1.022177815437317:  19%|████                 | 4/21 [00:00<00:01, 11.75it/s]Epoch: 2, train for the 215-th batch, train loss: 0.5161037445068359:  91%|█████████▉ | 215/237 [00:43<00:05,  4.17it/s]Epoch: 5, train for the 47-th batch, train loss: 0.4750599265098572:  32%|████         | 46/146 [00:07<00:16,  6.16it/s]evaluate for the 6-th batch, evaluate loss: 0.9856071472167969:  19%|███▊                | 4/21 [00:00<00:01, 11.75it/s]evaluate for the 6-th batch, evaluate loss: 0.9856071472167969:  29%|█████▋              | 6/21 [00:00<00:01, 12.05it/s]Epoch: 5, train for the 47-th batch, train loss: 0.4750599265098572:  32%|████▏        | 47/146 [00:07<00:15,  6.26it/s]evaluate for the 22-th batch, evaluate loss: 0.5919784903526306:  20%|███▎             | 21/106 [00:05<00:23,  3.60it/s]evaluate for the 22-th batch, evaluate loss: 0.5919784903526306:  21%|███▌             | 22/106 [00:05<00:24,  3.43it/s]Epoch: 4, train for the 89-th batch, train loss: 0.48003843426704407:  58%|██████▉     | 88/151 [00:15<00:11,  5.44it/s]Epoch: 3, train for the 58-th batch, train loss: 0.352439820766449:  24%|███▎          | 57/241 [00:09<00:26,  6.92it/s]Epoch: 3, train for the 58-th batch, train loss: 0.352439820766449:  24%|███▎          | 58/241 [00:09<00:27,  6.61it/s]Epoch: 4, train for the 89-th batch, train loss: 0.48003843426704407:  59%|███████     | 89/151 [00:15<00:11,  5.18it/s]evaluate for the 7-th batch, evaluate loss: 0.8659878373146057:  29%|█████▋              | 6/21 [00:00<00:01, 12.05it/s]evaluate for the 8-th batch, evaluate loss: 0.875298261642456:  29%|██████               | 6/21 [00:00<00:01, 12.05it/s]evaluate for the 8-th batch, evaluate loss: 0.875298261642456:  38%|████████             | 8/21 [00:00<00:01, 11.86it/s]Epoch: 2, train for the 216-th batch, train loss: 0.5231870412826538:  91%|█████████▉ | 215/237 [00:43<00:05,  4.17it/s]Epoch: 5, train for the 48-th batch, train loss: 0.4464990198612213:  32%|████▏        | 47/146 [00:08<00:15,  6.26it/s]Epoch: 5, train for the 48-th batch, train loss: 0.4464990198612213:  33%|████▎        | 48/146 [00:08<00:16,  5.89it/s]evaluate for the 23-th batch, evaluate loss: 0.5769997835159302:  21%|███▌             | 22/106 [00:05<00:24,  3.43it/s]evaluate for the 23-th batch, evaluate loss: 0.5769997835159302:  22%|███▋             | 23/106 [00:05<00:21,  3.82it/s]Epoch: 3, train for the 59-th batch, train loss: 0.5723887085914612:  24%|███▏         | 58/241 [00:09<00:27,  6.61it/s]Epoch: 3, train for the 59-th batch, train loss: 0.5723887085914612:  24%|███▏         | 59/241 [00:09<00:29,  6.18it/s]Epoch: 2, train for the 216-th batch, train loss: 0.5231870412826538:  91%|██████████ | 216/237 [00:43<00:05,  4.11it/s]Epoch: 4, train for the 90-th batch, train loss: 0.525429368019104:  59%|████████▎     | 89/151 [00:15<00:11,  5.18it/s]evaluate for the 9-th batch, evaluate loss: 0.8060731291770935:  38%|███████▌            | 8/21 [00:00<00:01, 11.86it/s]Epoch: 4, train for the 90-th batch, train loss: 0.525429368019104:  60%|████████▎     | 90/151 [00:15<00:12,  4.79it/s]Epoch: 3, train for the 60-th batch, train loss: 0.554166853427887:  24%|███▍          | 59/241 [00:09<00:29,  6.18it/s]Epoch: 5, train for the 49-th batch, train loss: 0.45040878653526306:  33%|███▉        | 48/146 [00:08<00:16,  5.89it/s]Epoch: 5, train for the 49-th batch, train loss: 0.45040878653526306:  34%|████        | 49/146 [00:08<00:16,  6.00it/s]Epoch: 3, train for the 60-th batch, train loss: 0.554166853427887:  25%|███▍          | 60/241 [00:09<00:28,  6.38it/s]evaluate for the 10-th batch, evaluate loss: 0.8736823797225952:  38%|███████▏           | 8/21 [00:00<00:01, 11.86it/s]evaluate for the 10-th batch, evaluate loss: 0.8736823797225952:  48%|████████▌         | 10/21 [00:00<00:00, 11.45it/s]Epoch: 2, train for the 217-th batch, train loss: 0.5080428719520569:  91%|██████████ | 216/237 [00:43<00:05,  4.11it/s]Epoch: 2, train for the 217-th batch, train loss: 0.5080428719520569:  92%|██████████ | 217/237 [00:43<00:04,  4.37it/s]Epoch: 4, train for the 91-th batch, train loss: 0.49359554052352905:  60%|███████▏    | 90/151 [00:16<00:12,  4.79it/s]Epoch: 4, train for the 91-th batch, train loss: 0.49359554052352905:  60%|███████▏    | 91/151 [00:16<00:11,  5.09it/s]evaluate for the 11-th batch, evaluate loss: 0.7401037216186523:  48%|████████▌         | 10/21 [00:00<00:00, 11.45it/s]Epoch: 5, train for the 50-th batch, train loss: 0.4838207960128784:  34%|████▎        | 49/146 [00:08<00:16,  6.00it/s]Epoch: 5, train for the 50-th batch, train loss: 0.4838207960128784:  34%|████▍        | 50/146 [00:08<00:15,  6.18it/s]Epoch: 3, train for the 61-th batch, train loss: 0.5369340181350708:  25%|███▏         | 60/241 [00:09<00:28,  6.38it/s]evaluate for the 12-th batch, evaluate loss: 0.7996526956558228:  48%|████████▌         | 10/21 [00:01<00:00, 11.45it/s]evaluate for the 12-th batch, evaluate loss: 0.7996526956558228:  57%|██████████▎       | 12/21 [00:01<00:00, 11.76it/s]Epoch: 3, train for the 61-th batch, train loss: 0.5369340181350708:  25%|███▎         | 61/241 [00:09<00:28,  6.30it/s]evaluate for the 13-th batch, evaluate loss: 0.7304555177688599:  57%|██████████▎       | 12/21 [00:01<00:00, 11.76it/s]Epoch: 4, train for the 92-th batch, train loss: 0.501541793346405:  60%|████████▍     | 91/151 [00:16<00:11,  5.09it/s]Epoch: 2, train for the 218-th batch, train loss: 0.5012621283531189:  92%|██████████ | 217/237 [00:43<00:04,  4.37it/s]evaluate for the 24-th batch, evaluate loss: 0.5017344355583191:  22%|███▋             | 23/106 [00:06<00:21,  3.82it/s]evaluate for the 24-th batch, evaluate loss: 0.5017344355583191:  23%|███▊             | 24/106 [00:06<00:24,  3.28it/s]Epoch: 4, train for the 92-th batch, train loss: 0.501541793346405:  61%|████████▌     | 92/151 [00:16<00:11,  5.32it/s]Epoch: 2, train for the 218-th batch, train loss: 0.5012621283531189:  92%|██████████ | 218/237 [00:43<00:04,  4.49it/s]Epoch: 5, train for the 51-th batch, train loss: 0.43397924304008484:  34%|████        | 50/146 [00:08<00:15,  6.18it/s]Epoch: 5, train for the 51-th batch, train loss: 0.43397924304008484:  35%|████▏       | 51/146 [00:08<00:14,  6.62it/s]evaluate for the 14-th batch, evaluate loss: 0.6788851618766785:  57%|██████████▎       | 12/21 [00:01<00:00, 11.76it/s]evaluate for the 14-th batch, evaluate loss: 0.6788851618766785:  67%|████████████      | 14/21 [00:01<00:00, 12.53it/s]Epoch: 3, train for the 62-th batch, train loss: 0.5678346157073975:  25%|███▎         | 61/241 [00:09<00:28,  6.30it/s]Epoch: 3, train for the 62-th batch, train loss: 0.5678346157073975:  26%|███▎         | 62/241 [00:09<00:28,  6.21it/s]evaluate for the 15-th batch, evaluate loss: 0.7199124097824097:  67%|████████████      | 14/21 [00:01<00:00, 12.53it/s]Epoch: 4, train for the 93-th batch, train loss: 0.5290834307670593:  61%|███████▉     | 92/151 [00:16<00:11,  5.32it/s]evaluate for the 25-th batch, evaluate loss: 0.5885470509529114:  23%|███▊             | 24/106 [00:06<00:24,  3.28it/s]evaluate for the 25-th batch, evaluate loss: 0.5885470509529114:  24%|████             | 25/106 [00:06<00:21,  3.75it/s]Epoch: 5, train for the 52-th batch, train loss: 0.4699448347091675:  35%|████▌        | 51/146 [00:08<00:14,  6.62it/s]Epoch: 4, train for the 93-th batch, train loss: 0.5290834307670593:  62%|████████     | 93/151 [00:16<00:10,  5.33it/s]Epoch: 5, train for the 52-th batch, train loss: 0.4699448347091675:  36%|████▋        | 52/146 [00:08<00:14,  6.39it/s]evaluate for the 16-th batch, evaluate loss: 0.6788929104804993:  67%|████████████      | 14/21 [00:01<00:00, 12.53it/s]evaluate for the 16-th batch, evaluate loss: 0.6788929104804993:  76%|█████████████▋    | 16/21 [00:01<00:00, 12.31it/s]Epoch: 3, train for the 63-th batch, train loss: 0.7100433111190796:  26%|███▎         | 62/241 [00:09<00:28,  6.21it/s]Epoch: 3, train for the 63-th batch, train loss: 0.7100433111190796:  26%|███▍         | 63/241 [00:09<00:29,  6.13it/s]Epoch: 2, train for the 219-th batch, train loss: 0.5368519425392151:  92%|██████████ | 218/237 [00:43<00:04,  4.49it/s]Epoch: 2, train for the 219-th batch, train loss: 0.5368519425392151:  92%|██████████▏| 219/237 [00:43<00:04,  4.18it/s]evaluate for the 17-th batch, evaluate loss: 0.6294879913330078:  76%|█████████████▋    | 16/21 [00:01<00:00, 12.31it/s]Epoch: 5, train for the 53-th batch, train loss: 0.44531822204589844:  36%|████▎       | 52/146 [00:08<00:14,  6.39it/s]Epoch: 4, train for the 94-th batch, train loss: 0.532905638217926:  62%|████████▌     | 93/151 [00:16<00:10,  5.33it/s]Epoch: 5, train for the 53-th batch, train loss: 0.44531822204589844:  36%|████▎       | 53/146 [00:08<00:14,  6.40it/s]Epoch: 4, train for the 94-th batch, train loss: 0.532905638217926:  62%|████████▋     | 94/151 [00:16<00:10,  5.37it/s]Epoch: 3, train for the 64-th batch, train loss: 0.3019428253173828:  26%|███▍         | 63/241 [00:10<00:29,  6.13it/s]evaluate for the 18-th batch, evaluate loss: 0.6296147108078003:  76%|█████████████▋    | 16/21 [00:01<00:00, 12.31it/s]evaluate for the 18-th batch, evaluate loss: 0.6296147108078003:  86%|███████████████▍  | 18/21 [00:01<00:00, 11.86it/s]Epoch: 3, train for the 64-th batch, train loss: 0.3019428253173828:  27%|███▍         | 64/241 [00:10<00:29,  6.10it/s]evaluate for the 19-th batch, evaluate loss: 0.5393139719963074:  86%|███████████████▍  | 18/21 [00:01<00:00, 11.86it/s]Epoch: 2, train for the 220-th batch, train loss: 0.5347979068756104:  92%|██████████▏| 219/237 [00:44<00:04,  4.18it/s]Epoch: 5, train for the 54-th batch, train loss: 0.43251869082450867:  36%|████▎       | 53/146 [00:09<00:14,  6.40it/s]Epoch: 4, train for the 95-th batch, train loss: 0.5063230395317078:  62%|████████     | 94/151 [00:16<00:10,  5.37it/s]Epoch: 5, train for the 54-th batch, train loss: 0.43251869082450867:  37%|████▍       | 54/146 [00:09<00:14,  6.28it/s]Epoch: 2, train for the 220-th batch, train loss: 0.5347979068756104:  93%|██████████▏| 220/237 [00:44<00:03,  4.26it/s]Epoch: 4, train for the 95-th batch, train loss: 0.5063230395317078:  63%|████████▏    | 95/151 [00:16<00:09,  5.61it/s]evaluate for the 20-th batch, evaluate loss: 0.49287497997283936:  86%|██████████████▌  | 18/21 [00:01<00:00, 11.86it/s]evaluate for the 20-th batch, evaluate loss: 0.49287497997283936:  95%|████████████████▏| 20/21 [00:01<00:00, 12.63it/s]Epoch: 3, train for the 65-th batch, train loss: 0.343932569026947:  27%|███▋          | 64/241 [00:10<00:29,  6.10it/s]evaluate for the 26-th batch, evaluate loss: 0.5661241412162781:  24%|████             | 25/106 [00:06<00:21,  3.75it/s]evaluate for the 26-th batch, evaluate loss: 0.5661241412162781:  25%|████▏            | 26/106 [00:06<00:24,  3.28it/s]Epoch: 3, train for the 65-th batch, train loss: 0.343932569026947:  27%|███▊          | 65/241 [00:10<00:28,  6.13it/s]evaluate for the 21-th batch, evaluate loss: 0.7215213179588318:  95%|█████████████████▏| 20/21 [00:01<00:00, 12.63it/s]evaluate for the 21-th batch, evaluate loss: 0.7215213179588318: 100%|██████████████████| 21/21 [00:01<00:00, 12.12it/s]
Epoch: 4, train for the 96-th batch, train loss: 0.5211986303329468:  63%|████████▏    | 95/151 [00:16<00:09,  5.61it/s]evaluate for the 27-th batch, evaluate loss: 0.41342154145240784:  25%|███▉            | 26/106 [00:06<00:24,  3.28it/s]evaluate for the 27-th batch, evaluate loss: 0.41342154145240784:  25%|████            | 27/106 [00:06<00:20,  3.85it/s]Epoch: 4, train for the 96-th batch, train loss: 0.5211986303329468:  64%|████████▎    | 96/151 [00:16<00:10,  5.43it/s]Epoch: 3, train for the 66-th batch, train loss: 0.2696138024330139:  27%|███▌         | 65/241 [00:10<00:28,  6.13it/s]Epoch: 3, train for the 66-th batch, train loss: 0.2696138024330139:  27%|███▌         | 66/241 [00:10<00:28,  6.11it/s]Epoch: 2, train for the 221-th batch, train loss: 0.527766227722168:  93%|███████████▏| 220/237 [00:44<00:03,  4.26it/s]Epoch: 5, train for the 55-th batch, train loss: 0.49222829937934875:  37%|████▍       | 54/146 [00:09<00:14,  6.28it/s]Epoch: 5, train for the 55-th batch, train loss: 0.49222829937934875:  38%|████▌       | 55/146 [00:09<00:16,  5.40it/s]Epoch: 2, train for the 221-th batch, train loss: 0.527766227722168:  93%|███████████▏| 221/237 [00:44<00:03,  4.06it/s]Epoch: 5, train for the 56-th batch, train loss: 0.4936886429786682:  38%|████▉        | 55/146 [00:09<00:16,  5.40it/s]Epoch: 5, train for the 56-th batch, train loss: 0.4936886429786682:  38%|████▉        | 56/146 [00:09<00:14,  6.11it/s]Epoch: 3, train for the 67-th batch, train loss: 0.39768290519714355:  27%|███▎        | 66/241 [00:10<00:28,  6.11it/s]Epoch: 3, train for the 67-th batch, train loss: 0.39768290519714355:  28%|███▎        | 67/241 [00:10<00:28,  6.01it/s]Epoch: 4, train for the 97-th batch, train loss: 0.5249128341674805:  64%|████████▎    | 96/151 [00:17<00:10,  5.43it/s]Epoch: 4, train for the 97-th batch, train loss: 0.5249128341674805:  64%|████████▎    | 97/151 [00:17<00:10,  5.04it/s]INFO:root:Epoch: 5, learning rate: 0.0001, train loss: 0.4108
INFO:root:train average_precision, 0.9069
INFO:root:train roc_auc, 0.9000
INFO:root:validate loss: 0.6688
INFO:root:validate average_precision, 0.7571
INFO:root:validate roc_auc, 0.7835
INFO:root:new node validate loss: 0.8122
INFO:root:new node validate first_1_average_precision, 0.8343
INFO:root:new node validate first_1_roc_auc, 0.8452
INFO:root:new node validate first_3_average_precision, 0.7866
INFO:root:new node validate first_3_roc_auc, 0.7687
INFO:root:new node validate first_10_average_precision, 0.7373
INFO:root:new node validate first_10_roc_auc, 0.7211
INFO:root:new node validate average_precision, 0.7085
INFO:root:new node validate roc_auc, 0.7109
Epoch: 2, train for the 222-th batch, train loss: 0.5689126253128052:  93%|██████████▎| 221/237 [00:44<00:03,  4.06it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]evaluate for the 28-th batch, evaluate loss: 0.5653870105743408:  25%|████▎            | 27/106 [00:07<00:20,  3.85it/s]evaluate for the 28-th batch, evaluate loss: 0.5653870105743408:  26%|████▍            | 28/106 [00:07<00:20,  3.78it/s]Epoch: 5, train for the 57-th batch, train loss: 0.49872255325317383:  38%|████▌       | 56/146 [00:09<00:14,  6.11it/s]Epoch: 5, train for the 57-th batch, train loss: 0.49872255325317383:  39%|████▋       | 57/146 [00:09<00:13,  6.42it/s]Epoch: 2, train for the 222-th batch, train loss: 0.5689126253128052:  94%|██████████▎| 222/237 [00:44<00:03,  4.16it/s]Epoch: 3, train for the 68-th batch, train loss: 0.5110102891921997:  28%|███▌         | 67/241 [00:10<00:28,  6.01it/s]Epoch: 3, train for the 68-th batch, train loss: 0.5110102891921997:  28%|███▋         | 68/241 [00:10<00:27,  6.38it/s]Epoch: 6, train for the 1-th batch, train loss: 0.9405063390731812:   0%|                       | 0/119 [00:00<?, ?it/s]Epoch: 6, train for the 1-th batch, train loss: 0.9405063390731812:   1%|▏              | 1/119 [00:00<00:15,  7.39it/s]Epoch: 4, train for the 98-th batch, train loss: 0.5052668452262878:  64%|████████▎    | 97/151 [00:17<00:10,  5.04it/s]Epoch: 4, train for the 98-th batch, train loss: 0.5052668452262878:  65%|████████▍    | 98/151 [00:17<00:10,  5.24it/s]Epoch: 5, train for the 58-th batch, train loss: 0.4642273187637329:  39%|█████        | 57/146 [00:09<00:13,  6.42it/s]Epoch: 5, train for the 58-th batch, train loss: 0.4642273187637329:  40%|█████▏       | 58/146 [00:09<00:13,  6.31it/s]Epoch: 3, train for the 69-th batch, train loss: 0.6313177943229675:  28%|███▋         | 68/241 [00:10<00:27,  6.38it/s]Epoch: 3, train for the 69-th batch, train loss: 0.6313177943229675:  29%|███▋         | 69/241 [00:10<00:27,  6.33it/s]evaluate for the 29-th batch, evaluate loss: 0.5593751668930054:  26%|████▍            | 28/106 [00:07<00:20,  3.78it/s]evaluate for the 29-th batch, evaluate loss: 0.5593751668930054:  27%|████▋            | 29/106 [00:07<00:19,  3.86it/s]Epoch: 2, train for the 223-th batch, train loss: 0.5129868984222412:  94%|██████████▎| 222/237 [00:44<00:03,  4.16it/s]Epoch: 6, train for the 2-th batch, train loss: 0.9195007085800171:   1%|▏              | 1/119 [00:00<00:15,  7.39it/s]Epoch: 6, train for the 2-th batch, train loss: 0.9195007085800171:   2%|▎              | 2/119 [00:00<00:16,  7.16it/s]Epoch: 4, train for the 99-th batch, train loss: 0.5444319248199463:  65%|████████▍    | 98/151 [00:17<00:10,  5.24it/s]Epoch: 2, train for the 223-th batch, train loss: 0.5129868984222412:  94%|██████████▎| 223/237 [00:44<00:03,  3.96it/s]Epoch: 4, train for the 99-th batch, train loss: 0.5444319248199463:  66%|████████▌    | 99/151 [00:17<00:09,  5.37it/s]Epoch: 3, train for the 70-th batch, train loss: 0.5123958587646484:  29%|███▋         | 69/241 [00:10<00:27,  6.33it/s]Epoch: 5, train for the 59-th batch, train loss: 0.4200722277164459:  40%|█████▏       | 58/146 [00:09<00:13,  6.31it/s]Epoch: 3, train for the 70-th batch, train loss: 0.5123958587646484:  29%|███▊         | 70/241 [00:10<00:24,  6.87it/s]Epoch: 5, train for the 59-th batch, train loss: 0.4200722277164459:  40%|█████▎       | 59/146 [00:09<00:13,  6.37it/s]Epoch: 6, train for the 3-th batch, train loss: 0.7149795293807983:   2%|▎              | 2/119 [00:00<00:16,  7.16it/s]Epoch: 6, train for the 3-th batch, train loss: 0.7149795293807983:   3%|▍              | 3/119 [00:00<00:15,  7.62it/s]Epoch: 4, train for the 100-th batch, train loss: 0.5777596831321716:  66%|███████▊    | 99/151 [00:17<00:09,  5.37it/s]Epoch: 4, train for the 100-th batch, train loss: 0.5777596831321716:  66%|███████▎   | 100/151 [00:17<00:09,  5.49it/s]Epoch: 3, train for the 71-th batch, train loss: 0.648642897605896:  29%|████          | 70/241 [00:11<00:24,  6.87it/s]evaluate for the 30-th batch, evaluate loss: 0.5677874684333801:  27%|████▋            | 29/106 [00:07<00:19,  3.86it/s]evaluate for the 30-th batch, evaluate loss: 0.5677874684333801:  28%|████▊            | 30/106 [00:07<00:19,  3.88it/s]Epoch: 3, train for the 71-th batch, train loss: 0.648642897605896:  29%|████          | 71/241 [00:11<00:26,  6.47it/s]Epoch: 2, train for the 224-th batch, train loss: 0.497803270816803:  94%|███████████▎| 223/237 [00:45<00:03,  3.96it/s]Epoch: 5, train for the 60-th batch, train loss: 0.4839720129966736:  40%|█████▎       | 59/146 [00:10<00:13,  6.37it/s]Epoch: 6, train for the 4-th batch, train loss: 0.683431088924408:   3%|▍               | 3/119 [00:00<00:15,  7.62it/s]Epoch: 5, train for the 60-th batch, train loss: 0.4839720129966736:  41%|█████▎       | 60/146 [00:10<00:14,  5.81it/s]Epoch: 6, train for the 4-th batch, train loss: 0.683431088924408:   3%|▌               | 4/119 [00:00<00:16,  6.95it/s]Epoch: 2, train for the 224-th batch, train loss: 0.497803270816803:  95%|███████████▎| 224/237 [00:45<00:03,  3.83it/s]Epoch: 4, train for the 101-th batch, train loss: 0.5909159779548645:  66%|███████▎   | 100/151 [00:17<00:09,  5.49it/s]Epoch: 4, train for the 101-th batch, train loss: 0.5909159779548645:  67%|███████▎   | 101/151 [00:17<00:08,  5.68it/s]Epoch: 3, train for the 72-th batch, train loss: 0.5798413157463074:  29%|███▊         | 71/241 [00:11<00:26,  6.47it/s]Epoch: 3, train for the 72-th batch, train loss: 0.5798413157463074:  30%|███▉         | 72/241 [00:11<00:26,  6.26it/s]evaluate for the 31-th batch, evaluate loss: 0.5531839728355408:  28%|████▊            | 30/106 [00:07<00:19,  3.88it/s]evaluate for the 31-th batch, evaluate loss: 0.5531839728355408:  29%|████▉            | 31/106 [00:07<00:17,  4.27it/s]Epoch: 5, train for the 61-th batch, train loss: 0.514674186706543:  41%|█████▊        | 60/146 [00:10<00:14,  5.81it/s]Epoch: 5, train for the 61-th batch, train loss: 0.514674186706543:  42%|█████▊        | 61/146 [00:10<00:15,  5.65it/s]Epoch: 6, train for the 5-th batch, train loss: 0.5846700668334961:   3%|▌              | 4/119 [00:00<00:16,  6.95it/s]Epoch: 6, train for the 5-th batch, train loss: 0.5846700668334961:   4%|▋              | 5/119 [00:00<00:18,  6.12it/s]Epoch: 2, train for the 225-th batch, train loss: 0.5128322243690491:  95%|██████████▍| 224/237 [00:45<00:03,  3.83it/s]evaluate for the 32-th batch, evaluate loss: 0.49160152673721313:  29%|████▋           | 31/106 [00:08<00:17,  4.27it/s]evaluate for the 32-th batch, evaluate loss: 0.49160152673721313:  30%|████▊           | 32/106 [00:08<00:15,  4.86it/s]Epoch: 3, train for the 73-th batch, train loss: 0.4822462499141693:  30%|███▉         | 72/241 [00:11<00:26,  6.26it/s]Epoch: 2, train for the 225-th batch, train loss: 0.5128322243690491:  95%|██████████▍| 225/237 [00:45<00:03,  3.85it/s]Epoch: 4, train for the 102-th batch, train loss: 0.5014178156852722:  67%|███████▎   | 101/151 [00:18<00:08,  5.68it/s]Epoch: 3, train for the 73-th batch, train loss: 0.4822462499141693:  30%|███▉         | 73/241 [00:11<00:27,  6.04it/s]Epoch: 4, train for the 102-th batch, train loss: 0.5014178156852722:  68%|███████▍   | 102/151 [00:18<00:09,  5.21it/s]Epoch: 5, train for the 62-th batch, train loss: 0.4226694107055664:  42%|█████▍       | 61/146 [00:10<00:15,  5.65it/s]Epoch: 5, train for the 62-th batch, train loss: 0.4226694107055664:  42%|█████▌       | 62/146 [00:10<00:14,  5.78it/s]Epoch: 6, train for the 6-th batch, train loss: 0.4465726315975189:   4%|▋              | 5/119 [00:00<00:18,  6.12it/s]Epoch: 6, train for the 6-th batch, train loss: 0.4465726315975189:   5%|▊              | 6/119 [00:00<00:18,  6.06it/s]Epoch: 3, train for the 74-th batch, train loss: 0.4290391504764557:  30%|███▉         | 73/241 [00:11<00:27,  6.04it/s]Epoch: 3, train for the 74-th batch, train loss: 0.4290391504764557:  31%|███▉         | 74/241 [00:11<00:27,  6.17it/s]evaluate for the 33-th batch, evaluate loss: 0.5616814494132996:  30%|█████▏           | 32/106 [00:08<00:15,  4.86it/s]evaluate for the 33-th batch, evaluate loss: 0.5616814494132996:  31%|█████▎           | 33/106 [00:08<00:14,  4.97it/s]Epoch: 4, train for the 103-th batch, train loss: 0.5380855798721313:  68%|███████▍   | 102/151 [00:18<00:09,  5.21it/s]Epoch: 5, train for the 63-th batch, train loss: 0.47236815094947815:  42%|█████       | 62/146 [00:10<00:14,  5.78it/s]Epoch: 4, train for the 103-th batch, train loss: 0.5380855798721313:  68%|███████▌   | 103/151 [00:18<00:09,  5.19it/s]Epoch: 5, train for the 63-th batch, train loss: 0.47236815094947815:  43%|█████▏      | 63/146 [00:10<00:14,  5.69it/s]Epoch: 6, train for the 7-th batch, train loss: 0.42571014165878296:   5%|▋             | 6/119 [00:01<00:18,  6.06it/s]Epoch: 2, train for the 226-th batch, train loss: 0.5328685641288757:  95%|██████████▍| 225/237 [00:45<00:03,  3.85it/s]Epoch: 6, train for the 7-th batch, train loss: 0.42571014165878296:   6%|▊             | 7/119 [00:01<00:19,  5.87it/s]Epoch: 2, train for the 226-th batch, train loss: 0.5328685641288757:  95%|██████████▍| 226/237 [00:45<00:02,  3.68it/s]Epoch: 4, train for the 104-th batch, train loss: 0.5622303485870361:  68%|███████▌   | 103/151 [00:18<00:09,  5.19it/s]Epoch: 4, train for the 104-th batch, train loss: 0.5622303485870361:  69%|███████▌   | 104/151 [00:18<00:08,  5.84it/s]Epoch: 5, train for the 64-th batch, train loss: 0.484302818775177:  43%|██████        | 63/146 [00:10<00:14,  5.69it/s]Epoch: 5, train for the 64-th batch, train loss: 0.484302818775177:  44%|██████▏       | 64/146 [00:10<00:14,  5.69it/s]Epoch: 6, train for the 8-th batch, train loss: 0.3218826949596405:   6%|▉              | 7/119 [00:01<00:19,  5.87it/s]Epoch: 3, train for the 75-th batch, train loss: 0.3621416389942169:  31%|███▉         | 74/241 [00:11<00:27,  6.17it/s]Epoch: 6, train for the 8-th batch, train loss: 0.3218826949596405:   7%|█              | 8/119 [00:01<00:18,  5.92it/s]Epoch: 3, train for the 75-th batch, train loss: 0.3621416389942169:  31%|████         | 75/241 [00:11<00:32,  5.17it/s]Epoch: 4, train for the 105-th batch, train loss: 0.4863244891166687:  69%|███████▌   | 104/151 [00:18<00:08,  5.84it/s]evaluate for the 34-th batch, evaluate loss: 0.5861038565635681:  31%|█████▎           | 33/106 [00:08<00:14,  4.97it/s]evaluate for the 34-th batch, evaluate loss: 0.5861038565635681:  32%|█████▍           | 34/106 [00:08<00:17,  4.18it/s]Epoch: 2, train for the 227-th batch, train loss: 0.523942232131958:  95%|███████████▍| 226/237 [00:45<00:02,  3.68it/s]Epoch: 4, train for the 105-th batch, train loss: 0.4863244891166687:  70%|███████▋   | 105/151 [00:18<00:07,  6.01it/s]Epoch: 2, train for the 227-th batch, train loss: 0.523942232131958:  96%|███████████▍| 227/237 [00:46<00:02,  3.80it/s]Epoch: 3, train for the 76-th batch, train loss: 0.23360943794250488:  31%|███▋        | 75/241 [00:12<00:32,  5.17it/s]Epoch: 3, train for the 76-th batch, train loss: 0.23360943794250488:  32%|███▊        | 76/241 [00:12<00:28,  5.74it/s]Epoch: 5, train for the 65-th batch, train loss: 0.5175992250442505:  44%|█████▋       | 64/146 [00:10<00:14,  5.69it/s]Epoch: 5, train for the 65-th batch, train loss: 0.5175992250442505:  45%|█████▊       | 65/146 [00:10<00:14,  5.71it/s]Epoch: 6, train for the 9-th batch, train loss: 0.3818821310997009:   7%|█              | 8/119 [00:01<00:18,  5.92it/s]Epoch: 6, train for the 9-th batch, train loss: 0.3818821310997009:   8%|█▏             | 9/119 [00:01<00:19,  5.72it/s]Epoch: 4, train for the 106-th batch, train loss: 0.5174759030342102:  70%|███████▋   | 105/151 [00:18<00:07,  6.01it/s]evaluate for the 35-th batch, evaluate loss: 0.5403507351875305:  32%|█████▍           | 34/106 [00:08<00:17,  4.18it/s]evaluate for the 35-th batch, evaluate loss: 0.5403507351875305:  33%|█████▌           | 35/106 [00:08<00:15,  4.61it/s]Epoch: 4, train for the 106-th batch, train loss: 0.5174759030342102:  70%|███████▋   | 106/151 [00:18<00:07,  6.04it/s]Epoch: 3, train for the 77-th batch, train loss: 0.343387633562088:  32%|████▍         | 76/241 [00:12<00:28,  5.74it/s]Epoch: 3, train for the 77-th batch, train loss: 0.343387633562088:  32%|████▍         | 77/241 [00:12<00:28,  5.74it/s]Epoch: 2, train for the 228-th batch, train loss: 0.5299713611602783:  96%|██████████▌| 227/237 [00:46<00:02,  3.80it/s]Epoch: 5, train for the 66-th batch, train loss: 0.5279067754745483:  45%|█████▊       | 65/146 [00:11<00:14,  5.71it/s]Epoch: 5, train for the 66-th batch, train loss: 0.5279067754745483:  45%|█████▉       | 66/146 [00:11<00:14,  5.70it/s]Epoch: 6, train for the 10-th batch, train loss: 0.3777272701263428:   8%|█             | 9/119 [00:01<00:19,  5.72it/s]Epoch: 6, train for the 10-th batch, train loss: 0.3777272701263428:   8%|█            | 10/119 [00:01<00:18,  5.75it/s]Epoch: 2, train for the 228-th batch, train loss: 0.5299713611602783:  96%|██████████▌| 228/237 [00:46<00:02,  3.89it/s]Epoch: 4, train for the 107-th batch, train loss: 0.464850515127182:  70%|████████▍   | 106/151 [00:18<00:07,  6.04it/s]Epoch: 4, train for the 107-th batch, train loss: 0.464850515127182:  71%|████████▌   | 107/151 [00:18<00:07,  5.97it/s]Epoch: 3, train for the 78-th batch, train loss: 0.5550476908683777:  32%|████▏        | 77/241 [00:12<00:28,  5.74it/s]Epoch: 3, train for the 78-th batch, train loss: 0.5550476908683777:  32%|████▏        | 78/241 [00:12<00:25,  6.28it/s]Epoch: 2, train for the 229-th batch, train loss: 0.5746621489524841:  96%|██████████▌| 228/237 [00:46<00:02,  3.89it/s]Epoch: 5, train for the 67-th batch, train loss: 0.5026101469993591:  45%|█████▉       | 66/146 [00:11<00:14,  5.70it/s]Epoch: 5, train for the 67-th batch, train loss: 0.5026101469993591:  46%|█████▉       | 67/146 [00:11<00:14,  5.62it/s]Epoch: 6, train for the 11-th batch, train loss: 0.36656123399734497:   8%|█           | 10/119 [00:01<00:18,  5.75it/s]Epoch: 6, train for the 11-th batch, train loss: 0.36656123399734497:   9%|█           | 11/119 [00:01<00:19,  5.64it/s]Epoch: 2, train for the 229-th batch, train loss: 0.5746621489524841:  97%|██████████▋| 229/237 [00:46<00:01,  4.25it/s]Epoch: 4, train for the 108-th batch, train loss: 0.49023517966270447:  71%|███████   | 107/151 [00:19<00:07,  5.97it/s]Epoch: 3, train for the 79-th batch, train loss: 0.45472267270088196:  32%|███▉        | 78/241 [00:12<00:25,  6.28it/s]Epoch: 4, train for the 108-th batch, train loss: 0.49023517966270447:  72%|███████▏  | 108/151 [00:19<00:07,  5.74it/s]Epoch: 3, train for the 79-th batch, train loss: 0.45472267270088196:  33%|███▉        | 79/241 [00:12<00:26,  6.16it/s]evaluate for the 36-th batch, evaluate loss: 0.6057383418083191:  33%|█████▌           | 35/106 [00:09<00:15,  4.61it/s]evaluate for the 36-th batch, evaluate loss: 0.6057383418083191:  34%|█████▊           | 36/106 [00:09<00:18,  3.77it/s]Epoch: 5, train for the 68-th batch, train loss: 0.49821457266807556:  46%|█████▌      | 67/146 [00:11<00:14,  5.62it/s]Epoch: 5, train for the 68-th batch, train loss: 0.49821457266807556:  47%|█████▌      | 68/146 [00:11<00:12,  6.06it/s]Epoch: 6, train for the 12-th batch, train loss: 0.40762853622436523:   9%|█           | 11/119 [00:01<00:19,  5.64it/s]Epoch: 6, train for the 12-th batch, train loss: 0.40762853622436523:  10%|█▏          | 12/119 [00:01<00:18,  5.78it/s]Epoch: 2, train for the 230-th batch, train loss: 0.5446728467941284:  97%|██████████▋| 229/237 [00:46<00:01,  4.25it/s]Epoch: 3, train for the 80-th batch, train loss: 0.34119659662246704:  33%|███▉        | 79/241 [00:12<00:26,  6.16it/s]Epoch: 2, train for the 230-th batch, train loss: 0.5446728467941284:  97%|██████████▋| 230/237 [00:46<00:01,  4.36it/s]Epoch: 5, train for the 69-th batch, train loss: 0.50620037317276:  47%|██████▉        | 68/146 [00:11<00:12,  6.06it/s]evaluate for the 37-th batch, evaluate loss: 0.5742408037185669:  34%|█████▊           | 36/106 [00:09<00:18,  3.77it/s]evaluate for the 37-th batch, evaluate loss: 0.5742408037185669:  35%|█████▉           | 37/106 [00:09<00:16,  4.28it/s]Epoch: 4, train for the 109-th batch, train loss: 0.4726269841194153:  72%|███████▊   | 108/151 [00:19<00:07,  5.74it/s]Epoch: 3, train for the 80-th batch, train loss: 0.34119659662246704:  33%|███▉        | 80/241 [00:12<00:26,  6.06it/s]Epoch: 5, train for the 69-th batch, train loss: 0.50620037317276:  47%|███████        | 69/146 [00:11<00:11,  6.53it/s]Epoch: 4, train for the 109-th batch, train loss: 0.4726269841194153:  72%|███████▉   | 109/151 [00:19<00:07,  5.61it/s]Epoch: 6, train for the 13-th batch, train loss: 0.3708365261554718:  10%|█▎           | 12/119 [00:02<00:18,  5.78it/s]Epoch: 6, train for the 13-th batch, train loss: 0.3708365261554718:  11%|█▍           | 13/119 [00:02<00:17,  5.95it/s]Epoch: 5, train for the 70-th batch, train loss: 0.48866817355155945:  47%|█████▋      | 69/146 [00:11<00:11,  6.53it/s]Epoch: 2, train for the 231-th batch, train loss: 0.5614013075828552:  97%|██████████▋| 230/237 [00:46<00:01,  4.36it/s]Epoch: 5, train for the 70-th batch, train loss: 0.48866817355155945:  48%|█████▊      | 70/146 [00:11<00:11,  6.67it/s]Epoch: 3, train for the 81-th batch, train loss: 0.3592773675918579:  33%|████▎        | 80/241 [00:12<00:26,  6.06it/s]Epoch: 2, train for the 231-th batch, train loss: 0.5614013075828552:  97%|██████████▋| 231/237 [00:46<00:01,  4.60it/s]Epoch: 3, train for the 81-th batch, train loss: 0.3592773675918579:  34%|████▎        | 81/241 [00:12<00:27,  5.81it/s]Epoch: 4, train for the 110-th batch, train loss: 0.4845619797706604:  72%|███████▉   | 109/151 [00:19<00:07,  5.61it/s]Epoch: 4, train for the 110-th batch, train loss: 0.4845619797706604:  73%|████████   | 110/151 [00:19<00:07,  5.41it/s]Epoch: 6, train for the 14-th batch, train loss: 0.5050368905067444:  11%|█▍           | 13/119 [00:02<00:17,  5.95it/s]Epoch: 6, train for the 14-th batch, train loss: 0.5050368905067444:  12%|█▌           | 14/119 [00:02<00:18,  5.61it/s]Epoch: 5, train for the 71-th batch, train loss: 0.5374659895896912:  48%|██████▏      | 70/146 [00:11<00:11,  6.67it/s]Epoch: 5, train for the 71-th batch, train loss: 0.5374659895896912:  49%|██████▎      | 71/146 [00:11<00:11,  6.68it/s]Epoch: 3, train for the 82-th batch, train loss: 0.4299181401729584:  34%|████▎        | 81/241 [00:12<00:27,  5.81it/s]Epoch: 2, train for the 232-th batch, train loss: 0.5303505659103394:  97%|██████████▋| 231/237 [00:47<00:01,  4.60it/s]Epoch: 3, train for the 82-th batch, train loss: 0.4299181401729584:  34%|████▍        | 82/241 [00:12<00:25,  6.30it/s]Epoch: 2, train for the 232-th batch, train loss: 0.5303505659103394:  98%|██████████▊| 232/237 [00:47<00:01,  4.90it/s]Epoch: 4, train for the 111-th batch, train loss: 0.5047157406806946:  73%|████████   | 110/151 [00:19<00:07,  5.41it/s]Epoch: 4, train for the 111-th batch, train loss: 0.5047157406806946:  74%|████████   | 111/151 [00:19<00:07,  5.41it/s]evaluate for the 38-th batch, evaluate loss: 0.7488311529159546:  35%|█████▉           | 37/106 [00:09<00:16,  4.28it/s]evaluate for the 38-th batch, evaluate loss: 0.7488311529159546:  36%|██████           | 38/106 [00:09<00:19,  3.48it/s]Epoch: 3, train for the 83-th batch, train loss: 0.49678680300712585:  34%|████        | 82/241 [00:13<00:25,  6.30it/s]Epoch: 3, train for the 83-th batch, train loss: 0.49678680300712585:  34%|████▏       | 83/241 [00:13<00:24,  6.44it/s]Epoch: 6, train for the 15-th batch, train loss: 0.4370455741882324:  12%|█▌           | 14/119 [00:02<00:18,  5.61it/s]Epoch: 5, train for the 72-th batch, train loss: 0.533630907535553:  49%|██████▊       | 71/146 [00:12<00:11,  6.68it/s]Epoch: 5, train for the 72-th batch, train loss: 0.533630907535553:  49%|██████▉       | 72/146 [00:12<00:12,  6.15it/s]Epoch: 6, train for the 15-th batch, train loss: 0.4370455741882324:  13%|█▋           | 15/119 [00:02<00:19,  5.35it/s]Epoch: 2, train for the 233-th batch, train loss: 0.5583533644676208:  98%|██████████▊| 232/237 [00:47<00:01,  4.90it/s]Epoch: 4, train for the 112-th batch, train loss: 0.4421866536140442:  74%|████████   | 111/151 [00:19<00:07,  5.41it/s]evaluate for the 39-th batch, evaluate loss: 0.633560836315155:  36%|██████▍           | 38/106 [00:09<00:19,  3.48it/s]evaluate for the 39-th batch, evaluate loss: 0.633560836315155:  37%|██████▌           | 39/106 [00:09<00:16,  4.04it/s]Epoch: 4, train for the 112-th batch, train loss: 0.4421866536140442:  74%|████████▏  | 112/151 [00:19<00:07,  5.54it/s]Epoch: 2, train for the 233-th batch, train loss: 0.5583533644676208:  98%|██████████▊| 233/237 [00:47<00:00,  4.63it/s]Epoch: 3, train for the 84-th batch, train loss: 0.5684133768081665:  34%|████▍        | 83/241 [00:13<00:24,  6.44it/s]Epoch: 5, train for the 73-th batch, train loss: 0.5389131307601929:  49%|██████▍      | 72/146 [00:12<00:12,  6.15it/s]Epoch: 3, train for the 84-th batch, train loss: 0.5684133768081665:  35%|████▌        | 84/241 [00:13<00:25,  6.23it/s]Epoch: 5, train for the 73-th batch, train loss: 0.5389131307601929:  50%|██████▌      | 73/146 [00:12<00:11,  6.13it/s]Epoch: 6, train for the 16-th batch, train loss: 0.46006330847740173:  13%|█▌          | 15/119 [00:02<00:19,  5.35it/s]Epoch: 6, train for the 16-th batch, train loss: 0.46006330847740173:  13%|█▌          | 16/119 [00:02<00:18,  5.48it/s]Epoch: 4, train for the 113-th batch, train loss: 0.5181666612625122:  74%|████████▏  | 112/151 [00:20<00:07,  5.54it/s]Epoch: 4, train for the 113-th batch, train loss: 0.5181666612625122:  75%|████████▏  | 113/151 [00:20<00:06,  5.70it/s]Epoch: 2, train for the 234-th batch, train loss: 0.5284286141395569:  98%|██████████▊| 233/237 [00:47<00:00,  4.63it/s]Epoch: 3, train for the 85-th batch, train loss: 0.44971194863319397:  35%|████▏       | 84/241 [00:13<00:25,  6.23it/s]Epoch: 3, train for the 85-th batch, train loss: 0.44971194863319397:  35%|████▏       | 85/241 [00:13<00:24,  6.35it/s]Epoch: 2, train for the 234-th batch, train loss: 0.5284286141395569:  99%|██████████▊| 234/237 [00:47<00:00,  4.67it/s]Epoch: 5, train for the 74-th batch, train loss: 0.5182710289955139:  50%|██████▌      | 73/146 [00:12<00:11,  6.13it/s]Epoch: 6, train for the 17-th batch, train loss: 0.39439040422439575:  13%|█▌          | 16/119 [00:02<00:18,  5.48it/s]Epoch: 6, train for the 17-th batch, train loss: 0.39439040422439575:  14%|█▋          | 17/119 [00:02<00:18,  5.45it/s]Epoch: 5, train for the 74-th batch, train loss: 0.5182710289955139:  51%|██████▌      | 74/146 [00:12<00:12,  5.76it/s]evaluate for the 40-th batch, evaluate loss: 0.629554271697998:  37%|██████▌           | 39/106 [00:10<00:16,  4.04it/s]evaluate for the 40-th batch, evaluate loss: 0.629554271697998:  38%|██████▊           | 40/106 [00:10<00:17,  3.76it/s]Epoch: 4, train for the 114-th batch, train loss: 0.4832983911037445:  75%|████████▏  | 113/151 [00:20<00:06,  5.70it/s]Epoch: 4, train for the 114-th batch, train loss: 0.4832983911037445:  75%|████████▎  | 114/151 [00:20<00:06,  5.82it/s]Epoch: 3, train for the 86-th batch, train loss: 0.4651035666465759:  35%|████▌        | 85/241 [00:13<00:24,  6.35it/s]Epoch: 3, train for the 86-th batch, train loss: 0.4651035666465759:  36%|████▋        | 86/241 [00:13<00:24,  6.25it/s]Epoch: 2, train for the 235-th batch, train loss: 0.49049463868141174:  99%|█████████▊| 234/237 [00:47<00:00,  4.67it/s]Epoch: 5, train for the 75-th batch, train loss: 0.521840512752533:  51%|███████       | 74/146 [00:12<00:12,  5.76it/s]evaluate for the 41-th batch, evaluate loss: 0.6795440316200256:  38%|██████▍          | 40/106 [00:10<00:17,  3.76it/s]evaluate for the 41-th batch, evaluate loss: 0.6795440316200256:  39%|██████▌          | 41/106 [00:10<00:14,  4.35it/s]Epoch: 5, train for the 75-th batch, train loss: 0.521840512752533:  51%|███████▏      | 75/146 [00:12<00:12,  5.76it/s]Epoch: 6, train for the 18-th batch, train loss: 0.40088438987731934:  14%|█▋          | 17/119 [00:03<00:18,  5.45it/s]Epoch: 2, train for the 235-th batch, train loss: 0.49049463868141174:  99%|█████████▉| 235/237 [00:47<00:00,  4.58it/s]Epoch: 6, train for the 18-th batch, train loss: 0.40088438987731934:  15%|█▊          | 18/119 [00:03<00:18,  5.45it/s]Epoch: 4, train for the 115-th batch, train loss: 0.43021368980407715:  75%|███████▌  | 114/151 [00:20<00:06,  5.82it/s]Epoch: 3, train for the 87-th batch, train loss: 0.5031891465187073:  36%|████▋        | 86/241 [00:13<00:24,  6.25it/s]Epoch: 4, train for the 115-th batch, train loss: 0.43021368980407715:  76%|███████▌  | 115/151 [00:20<00:06,  5.59it/s]Epoch: 3, train for the 87-th batch, train loss: 0.5031891465187073:  36%|████▋        | 87/241 [00:13<00:24,  6.35it/s]Epoch: 5, train for the 76-th batch, train loss: 0.5096816420555115:  51%|██████▋      | 75/146 [00:12<00:12,  5.76it/s]evaluate for the 42-th batch, evaluate loss: 0.6521949768066406:  39%|██████▌          | 41/106 [00:10<00:14,  4.35it/s]evaluate for the 42-th batch, evaluate loss: 0.6521949768066406:  40%|██████▋          | 42/106 [00:10<00:13,  4.90it/s]Epoch: 5, train for the 76-th batch, train loss: 0.5096816420555115:  52%|██████▊      | 76/146 [00:12<00:11,  5.95it/s]Epoch: 6, train for the 19-th batch, train loss: 0.3893600404262543:  15%|█▉           | 18/119 [00:03<00:18,  5.45it/s]Epoch: 6, train for the 19-th batch, train loss: 0.3893600404262543:  16%|██           | 19/119 [00:03<00:17,  5.68it/s]Epoch: 2, train for the 236-th batch, train loss: 0.4979376196861267:  99%|██████████▉| 235/237 [00:47<00:00,  4.58it/s]Epoch: 2, train for the 236-th batch, train loss: 0.4979376196861267: 100%|██████████▉| 236/237 [00:47<00:00,  4.31it/s]Epoch: 4, train for the 116-th batch, train loss: 0.3965185582637787:  76%|████████▍  | 115/151 [00:20<00:06,  5.59it/s]evaluate for the 43-th batch, evaluate loss: 0.5134710073471069:  40%|██████▋          | 42/106 [00:10<00:13,  4.90it/s]evaluate for the 43-th batch, evaluate loss: 0.5134710073471069:  41%|██████▉          | 43/106 [00:10<00:11,  5.40it/s]Epoch: 3, train for the 88-th batch, train loss: 0.4559480845928192:  36%|████▋        | 87/241 [00:13<00:24,  6.35it/s]Epoch: 4, train for the 116-th batch, train loss: 0.3965185582637787:  77%|████████▍  | 116/151 [00:20<00:06,  5.15it/s]Epoch: 3, train for the 88-th batch, train loss: 0.4559480845928192:  37%|████▋        | 88/241 [00:13<00:27,  5.63it/s]Epoch: 5, train for the 77-th batch, train loss: 0.544401228427887:  52%|███████▎      | 76/146 [00:12<00:11,  5.95it/s]Epoch: 6, train for the 20-th batch, train loss: 0.4063662588596344:  16%|██           | 19/119 [00:03<00:17,  5.68it/s]Epoch: 6, train for the 20-th batch, train loss: 0.4063662588596344:  17%|██▏          | 20/119 [00:03<00:18,  5.48it/s]Epoch: 5, train for the 77-th batch, train loss: 0.544401228427887:  53%|███████▍      | 77/146 [00:12<00:12,  5.52it/s]Epoch: 2, train for the 237-th batch, train loss: 0.5275472402572632: 100%|██████████▉| 236/237 [00:48<00:00,  4.31it/s]Epoch: 3, train for the 89-th batch, train loss: 0.486465722322464:  37%|█████         | 88/241 [00:14<00:27,  5.63it/s]Epoch: 3, train for the 89-th batch, train loss: 0.486465722322464:  37%|█████▏        | 89/241 [00:14<00:27,  5.52it/s]Epoch: 2, train for the 237-th batch, train loss: 0.5275472402572632: 100%|███████████| 237/237 [00:48<00:00,  4.31it/s]Epoch: 4, train for the 117-th batch, train loss: 0.489734947681427:  77%|█████████▏  | 116/151 [00:20<00:06,  5.15it/s]Epoch: 2, train for the 237-th batch, train loss: 0.5275472402572632: 100%|███████████| 237/237 [00:48<00:00,  4.92it/s]
Epoch: 6, train for the 21-th batch, train loss: 0.4530849754810333:  17%|██▏          | 20/119 [00:03<00:18,  5.48it/s]Epoch: 6, train for the 21-th batch, train loss: 0.4530849754810333:  18%|██▎          | 21/119 [00:03<00:16,  5.79it/s]Epoch: 4, train for the 117-th batch, train loss: 0.489734947681427:  77%|█████████▎  | 117/151 [00:20<00:06,  4.97it/s]Epoch: 5, train for the 78-th batch, train loss: 0.5307585000991821:  53%|██████▊      | 77/146 [00:13<00:12,  5.52it/s]Epoch: 5, train for the 78-th batch, train loss: 0.5307585000991821:  53%|██████▉      | 78/146 [00:13<00:12,  5.45it/s]evaluate for the 44-th batch, evaluate loss: 0.4990140497684479:  41%|██████▉          | 43/106 [00:10<00:11,  5.40it/s]evaluate for the 44-th batch, evaluate loss: 0.4990140497684479:  42%|███████          | 44/106 [00:10<00:13,  4.68it/s]Epoch: 6, train for the 22-th batch, train loss: 0.38066092133522034:  18%|██          | 21/119 [00:03<00:16,  5.79it/s]Epoch: 6, train for the 22-th batch, train loss: 0.38066092133522034:  18%|██▏         | 22/119 [00:03<00:15,  6.21it/s]Epoch: 3, train for the 90-th batch, train loss: 0.4639977812767029:  37%|████▊        | 89/241 [00:14<00:27,  5.52it/s]Epoch: 3, train for the 90-th batch, train loss: 0.4639977812767029:  37%|████▊        | 90/241 [00:14<00:27,  5.58it/s]Epoch: 5, train for the 79-th batch, train loss: 0.5330701470375061:  53%|██████▉      | 78/146 [00:13<00:12,  5.45it/s]Epoch: 4, train for the 118-th batch, train loss: 0.4627118706703186:  77%|████████▌  | 117/151 [00:21<00:06,  4.97it/s]Epoch: 5, train for the 79-th batch, train loss: 0.5330701470375061:  54%|███████      | 79/146 [00:13<00:11,  5.79it/s]evaluate for the 45-th batch, evaluate loss: 0.5691032409667969:  42%|███████          | 44/106 [00:10<00:13,  4.68it/s]evaluate for the 45-th batch, evaluate loss: 0.5691032409667969:  42%|███████▏         | 45/106 [00:10<00:11,  5.09it/s]Epoch: 4, train for the 118-th batch, train loss: 0.4627118706703186:  78%|████████▌  | 118/151 [00:21<00:06,  4.93it/s]Epoch: 6, train for the 23-th batch, train loss: 0.4342382848262787:  18%|██▍          | 22/119 [00:03<00:15,  6.21it/s]Epoch: 6, train for the 23-th batch, train loss: 0.4342382848262787:  19%|██▌          | 23/119 [00:03<00:15,  6.23it/s]Epoch: 3, train for the 91-th batch, train loss: 0.48790431022644043:  37%|████▍       | 90/241 [00:14<00:27,  5.58it/s]Epoch: 3, train for the 91-th batch, train loss: 0.48790431022644043:  38%|████▌       | 91/241 [00:14<00:26,  5.68it/s]Epoch: 5, train for the 80-th batch, train loss: 0.49624091386795044:  54%|██████▍     | 79/146 [00:13<00:11,  5.79it/s]Epoch: 4, train for the 119-th batch, train loss: 0.4736391305923462:  78%|████████▌  | 118/151 [00:21<00:06,  4.93it/s]Epoch: 5, train for the 80-th batch, train loss: 0.49624091386795044:  55%|██████▌     | 80/146 [00:13<00:11,  5.76it/s]Epoch: 4, train for the 119-th batch, train loss: 0.4736391305923462:  79%|████████▋  | 119/151 [00:21<00:06,  5.25it/s]Epoch: 6, train for the 24-th batch, train loss: 0.3553112745285034:  19%|██▌          | 23/119 [00:04<00:15,  6.23it/s]Epoch: 6, train for the 24-th batch, train loss: 0.3553112745285034:  20%|██▌          | 24/119 [00:04<00:14,  6.51it/s]  0%|                                                                                            | 0/66 [00:00<?, ?it/s]Epoch: 3, train for the 92-th batch, train loss: 0.3776484727859497:  38%|████▉        | 91/241 [00:14<00:26,  5.68it/s]Epoch: 3, train for the 92-th batch, train loss: 0.3776484727859497:  38%|████▉        | 92/241 [00:14<00:25,  5.89it/s]Epoch: 5, train for the 81-th batch, train loss: 0.46533018350601196:  55%|██████▌     | 80/146 [00:13<00:11,  5.76it/s]Epoch: 5, train for the 81-th batch, train loss: 0.46533018350601196:  55%|██████▋     | 81/146 [00:13<00:10,  6.27it/s]evaluate for the 46-th batch, evaluate loss: 0.6423308253288269:  42%|███████▏         | 45/106 [00:11<00:11,  5.09it/s]evaluate for the 46-th batch, evaluate loss: 0.6423308253288269:  43%|███████▍         | 46/106 [00:11<00:13,  4.46it/s]Epoch: 4, train for the 120-th batch, train loss: 0.516990602016449:  79%|█████████▍  | 119/151 [00:21<00:06,  5.25it/s]Epoch: 6, train for the 25-th batch, train loss: 0.35654276609420776:  20%|██▍         | 24/119 [00:04<00:14,  6.51it/s]evaluate for the 1-th batch, evaluate loss: 0.6673102974891663:   0%|                            | 0/66 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6673102974891663:   2%|▎                   | 1/66 [00:00<00:09,  7.20it/s]Epoch: 4, train for the 120-th batch, train loss: 0.516990602016449:  79%|█████████▌  | 120/151 [00:21<00:06,  5.12it/s]Epoch: 6, train for the 25-th batch, train loss: 0.35654276609420776:  21%|██▌         | 25/119 [00:04<00:14,  6.52it/s]Epoch: 3, train for the 93-th batch, train loss: 0.4109029173851013:  38%|████▉        | 92/241 [00:14<00:25,  5.89it/s]Epoch: 5, train for the 82-th batch, train loss: 0.49341997504234314:  55%|██████▋     | 81/146 [00:13<00:10,  6.27it/s]evaluate for the 47-th batch, evaluate loss: 0.50738126039505:  43%|████████▏          | 46/106 [00:11<00:13,  4.46it/s]evaluate for the 47-th batch, evaluate loss: 0.50738126039505:  44%|████████▍          | 47/106 [00:11<00:12,  4.86it/s]Epoch: 5, train for the 82-th batch, train loss: 0.49341997504234314:  56%|██████▋     | 82/146 [00:13<00:10,  6.12it/s]Epoch: 3, train for the 93-th batch, train loss: 0.4109029173851013:  39%|█████        | 93/241 [00:14<00:25,  5.74it/s]evaluate for the 2-th batch, evaluate loss: 0.6090301871299744:   2%|▎                   | 1/66 [00:00<00:09,  7.20it/s]evaluate for the 2-th batch, evaluate loss: 0.6090301871299744:   3%|▌                   | 2/66 [00:00<00:07,  8.04it/s]Epoch: 4, train for the 121-th batch, train loss: 0.4974936544895172:  79%|████████▋  | 120/151 [00:21<00:06,  5.12it/s]Epoch: 4, train for the 121-th batch, train loss: 0.4974936544895172:  80%|████████▊  | 121/151 [00:21<00:05,  5.19it/s]Epoch: 5, train for the 83-th batch, train loss: 0.4722265899181366:  56%|███████▎     | 82/146 [00:13<00:10,  6.12it/s]Epoch: 5, train for the 83-th batch, train loss: 0.4722265899181366:  57%|███████▍     | 83/146 [00:13<00:09,  6.61it/s]evaluate for the 3-th batch, evaluate loss: 0.6947552561759949:   3%|▌                   | 2/66 [00:00<00:07,  8.04it/s]evaluate for the 3-th batch, evaluate loss: 0.6947552561759949:   5%|▉                   | 3/66 [00:00<00:07,  8.78it/s]Epoch: 3, train for the 94-th batch, train loss: 0.34312957525253296:  39%|████▋       | 93/241 [00:15<00:25,  5.74it/s]Epoch: 3, train for the 94-th batch, train loss: 0.34312957525253296:  39%|████▋       | 94/241 [00:15<00:24,  5.92it/s]Epoch: 6, train for the 26-th batch, train loss: 0.36720603704452515:  21%|██▌         | 25/119 [00:04<00:14,  6.52it/s]evaluate for the 4-th batch, evaluate loss: 0.6754706501960754:   5%|▉                   | 3/66 [00:00<00:07,  8.78it/s]evaluate for the 4-th batch, evaluate loss: 0.6754706501960754:   6%|█▏                  | 4/66 [00:00<00:06,  9.09it/s]Epoch: 6, train for the 26-th batch, train loss: 0.36720603704452515:  22%|██▌         | 26/119 [00:04<00:18,  4.92it/s]Epoch: 4, train for the 122-th batch, train loss: 0.4696199595928192:  80%|████████▊  | 121/151 [00:21<00:05,  5.19it/s]Epoch: 5, train for the 84-th batch, train loss: 0.47988325357437134:  57%|██████▊     | 83/146 [00:14<00:09,  6.61it/s]Epoch: 5, train for the 84-th batch, train loss: 0.47988325357437134:  58%|██████▉     | 84/146 [00:14<00:09,  6.58it/s]Epoch: 4, train for the 122-th batch, train loss: 0.4696199595928192:  81%|████████▉  | 122/151 [00:21<00:05,  5.28it/s]Epoch: 3, train for the 95-th batch, train loss: 0.414875864982605:  39%|█████▍        | 94/241 [00:15<00:24,  5.92it/s]Epoch: 3, train for the 95-th batch, train loss: 0.414875864982605:  39%|█████▌        | 95/241 [00:15<00:23,  6.19it/s]evaluate for the 5-th batch, evaluate loss: 0.6519457697868347:   6%|█▏                  | 4/66 [00:00<00:06,  9.09it/s]evaluate for the 5-th batch, evaluate loss: 0.6519457697868347:   8%|█▌                  | 5/66 [00:00<00:06,  9.35it/s]Epoch: 6, train for the 27-th batch, train loss: 0.39121904969215393:  22%|██▌         | 26/119 [00:04<00:18,  4.92it/s]Epoch: 6, train for the 27-th batch, train loss: 0.39121904969215393:  23%|██▋         | 27/119 [00:04<00:16,  5.42it/s]evaluate for the 48-th batch, evaluate loss: 0.6038486361503601:  44%|███████▌         | 47/106 [00:11<00:12,  4.86it/s]evaluate for the 48-th batch, evaluate loss: 0.6038486361503601:  45%|███████▋         | 48/106 [00:11<00:15,  3.71it/s]Epoch: 5, train for the 85-th batch, train loss: 0.49389463663101196:  58%|██████▉     | 84/146 [00:14<00:09,  6.58it/s]Epoch: 5, train for the 85-th batch, train loss: 0.49389463663101196:  58%|██████▉     | 85/146 [00:14<00:09,  6.60it/s]evaluate for the 6-th batch, evaluate loss: 0.662361204624176:   8%|█▌                   | 5/66 [00:00<00:06,  9.35it/s]evaluate for the 6-th batch, evaluate loss: 0.662361204624176:   9%|█▉                   | 6/66 [00:00<00:06,  9.28it/s]Epoch: 4, train for the 123-th batch, train loss: 0.4859883785247803:  81%|████████▉  | 122/151 [00:21<00:05,  5.28it/s]Epoch: 3, train for the 96-th batch, train loss: 0.5048444867134094:  39%|█████        | 95/241 [00:15<00:23,  6.19it/s]Epoch: 3, train for the 96-th batch, train loss: 0.5048444867134094:  40%|█████▏       | 96/241 [00:15<00:25,  5.80it/s]Epoch: 4, train for the 123-th batch, train loss: 0.4859883785247803:  81%|████████▉  | 123/151 [00:22<00:05,  5.03it/s]evaluate for the 7-th batch, evaluate loss: 0.6660681962966919:   9%|█▊                  | 6/66 [00:00<00:06,  9.28it/s]Epoch: 6, train for the 28-th batch, train loss: 0.41293516755104065:  23%|██▋         | 27/119 [00:04<00:16,  5.42it/s]Epoch: 6, train for the 28-th batch, train loss: 0.41293516755104065:  24%|██▊         | 28/119 [00:04<00:16,  5.45it/s]Epoch: 5, train for the 86-th batch, train loss: 0.4957253932952881:  58%|███████▌     | 85/146 [00:14<00:09,  6.60it/s]evaluate for the 49-th batch, evaluate loss: 0.6183251142501831:  45%|███████▋         | 48/106 [00:12<00:15,  3.71it/s]evaluate for the 49-th batch, evaluate loss: 0.6183251142501831:  46%|███████▊         | 49/106 [00:12<00:13,  4.09it/s]Epoch: 5, train for the 86-th batch, train loss: 0.4957253932952881:  59%|███████▋     | 86/146 [00:14<00:09,  6.30it/s]evaluate for the 8-th batch, evaluate loss: 0.6115665435791016:   9%|█▊                  | 6/66 [00:00<00:06,  9.28it/s]evaluate for the 8-th batch, evaluate loss: 0.6115665435791016:  12%|██▍                 | 8/66 [00:00<00:05,  9.85it/s]Epoch: 3, train for the 97-th batch, train loss: 0.35410937666893005:  40%|████▊       | 96/241 [00:15<00:25,  5.80it/s]evaluate for the 9-th batch, evaluate loss: 0.62465500831604:  12%|██▋                   | 8/66 [00:00<00:05,  9.85it/s]Epoch: 4, train for the 124-th batch, train loss: 0.5042023062705994:  81%|████████▉  | 123/151 [00:22<00:05,  5.03it/s]Epoch: 3, train for the 97-th batch, train loss: 0.35410937666893005:  40%|████▊       | 97/241 [00:15<00:26,  5.38it/s]Epoch: 6, train for the 29-th batch, train loss: 0.38172686100006104:  24%|██▊         | 28/119 [00:04<00:16,  5.45it/s]Epoch: 4, train for the 124-th batch, train loss: 0.5042023062705994:  82%|█████████  | 124/151 [00:22<00:05,  4.75it/s]Epoch: 6, train for the 29-th batch, train loss: 0.38172686100006104:  24%|██▉         | 29/119 [00:05<00:16,  5.46it/s]Epoch: 5, train for the 87-th batch, train loss: 0.4939381182193756:  59%|███████▋     | 86/146 [00:14<00:09,  6.30it/s]Epoch: 5, train for the 87-th batch, train loss: 0.4939381182193756:  60%|███████▋     | 87/146 [00:14<00:09,  6.12it/s]evaluate for the 10-th batch, evaluate loss: 0.6340669393539429:  12%|██▎                | 8/66 [00:01<00:05,  9.85it/s]evaluate for the 10-th batch, evaluate loss: 0.6340669393539429:  15%|██▋               | 10/66 [00:01<00:05, 10.66it/s]Epoch: 3, train for the 98-th batch, train loss: 0.458650678396225:  40%|█████▋        | 97/241 [00:15<00:26,  5.38it/s]Epoch: 3, train for the 98-th batch, train loss: 0.458650678396225:  41%|█████▋        | 98/241 [00:15<00:24,  5.79it/s]evaluate for the 11-th batch, evaluate loss: 0.6706793904304504:  15%|██▋               | 10/66 [00:01<00:05, 10.66it/s]Epoch: 6, train for the 30-th batch, train loss: 0.3478359878063202:  24%|███▏         | 29/119 [00:05<00:16,  5.46it/s]Epoch: 6, train for the 30-th batch, train loss: 0.3478359878063202:  25%|███▎         | 30/119 [00:05<00:16,  5.52it/s]Epoch: 4, train for the 125-th batch, train loss: 0.5316129922866821:  82%|█████████  | 124/151 [00:22<00:05,  4.75it/s]Epoch: 4, train for the 125-th batch, train loss: 0.5316129922866821:  83%|█████████  | 125/151 [00:22<00:05,  4.83it/s]Epoch: 5, train for the 88-th batch, train loss: 0.5053247213363647:  60%|███████▋     | 87/146 [00:14<00:09,  6.12it/s]Epoch: 5, train for the 88-th batch, train loss: 0.5053247213363647:  60%|███████▊     | 88/146 [00:14<00:09,  5.94it/s]Epoch: 3, train for the 99-th batch, train loss: 0.3153343200683594:  41%|█████▎       | 98/241 [00:15<00:24,  5.79it/s]Epoch: 3, train for the 99-th batch, train loss: 0.3153343200683594:  41%|█████▎       | 99/241 [00:15<00:22,  6.18it/s]evaluate for the 12-th batch, evaluate loss: 0.6400536298751831:  15%|██▋               | 10/66 [00:01<00:05, 10.66it/s]evaluate for the 12-th batch, evaluate loss: 0.6400536298751831:  18%|███▎              | 12/66 [00:01<00:05, 10.36it/s]evaluate for the 50-th batch, evaluate loss: 0.5349366664886475:  46%|███████▊         | 49/106 [00:12<00:13,  4.09it/s]evaluate for the 50-th batch, evaluate loss: 0.5349366664886475:  47%|████████         | 50/106 [00:12<00:16,  3.30it/s]Epoch: 6, train for the 31-th batch, train loss: 0.40183672308921814:  25%|███         | 30/119 [00:05<00:16,  5.52it/s]Epoch: 6, train for the 31-th batch, train loss: 0.40183672308921814:  26%|███▏        | 31/119 [00:05<00:16,  5.34it/s]Epoch: 5, train for the 89-th batch, train loss: 0.4761975109577179:  60%|███████▊     | 88/146 [00:14<00:09,  5.94it/s]evaluate for the 13-th batch, evaluate loss: 0.6468163728713989:  18%|███▎              | 12/66 [00:01<00:05, 10.36it/s]Epoch: 5, train for the 89-th batch, train loss: 0.4761975109577179:  61%|███████▉     | 89/146 [00:14<00:09,  5.85it/s]Epoch: 4, train for the 126-th batch, train loss: 0.44187453389167786:  83%|████████▎ | 125/151 [00:22<00:05,  4.83it/s]Epoch: 3, train for the 100-th batch, train loss: 0.5023896098136902:  41%|████▉       | 99/241 [00:16<00:22,  6.18it/s]Epoch: 4, train for the 126-th batch, train loss: 0.44187453389167786:  83%|████████▎ | 126/151 [00:22<00:05,  4.72it/s]Epoch: 3, train for the 100-th batch, train loss: 0.5023896098136902:  41%|████▌      | 100/241 [00:16<00:24,  5.82it/s]evaluate for the 51-th batch, evaluate loss: 0.6943495869636536:  47%|████████         | 50/106 [00:12<00:16,  3.30it/s]evaluate for the 51-th batch, evaluate loss: 0.6943495869636536:  48%|████████▏        | 51/106 [00:12<00:14,  3.70it/s]evaluate for the 14-th batch, evaluate loss: 0.6547219753265381:  18%|███▎              | 12/66 [00:01<00:05, 10.36it/s]evaluate for the 14-th batch, evaluate loss: 0.6547219753265381:  21%|███▊              | 14/66 [00:01<00:05,  9.08it/s]Epoch: 6, train for the 32-th batch, train loss: 0.3889094591140747:  26%|███▍         | 31/119 [00:05<00:16,  5.34it/s]Epoch: 6, train for the 32-th batch, train loss: 0.3889094591140747:  27%|███▍         | 32/119 [00:05<00:15,  5.49it/s]Epoch: 5, train for the 90-th batch, train loss: 0.5244723558425903:  61%|███████▉     | 89/146 [00:15<00:09,  5.85it/s]Epoch: 5, train for the 90-th batch, train loss: 0.5244723558425903:  62%|████████     | 90/146 [00:15<00:09,  5.65it/s]Epoch: 4, train for the 127-th batch, train loss: 0.46247053146362305:  83%|████████▎ | 126/151 [00:22<00:05,  4.72it/s]Epoch: 4, train for the 127-th batch, train loss: 0.46247053146362305:  84%|████████▍ | 127/151 [00:22<00:04,  4.83it/s]Epoch: 3, train for the 101-th batch, train loss: 0.47867727279663086:  41%|████▏     | 100/241 [00:16<00:24,  5.82it/s]evaluate for the 15-th batch, evaluate loss: 0.6268702149391174:  21%|███▊              | 14/66 [00:01<00:05,  9.08it/s]Epoch: 3, train for the 101-th batch, train loss: 0.47867727279663086:  42%|████▏     | 101/241 [00:16<00:24,  5.68it/s]Epoch: 6, train for the 33-th batch, train loss: 0.3662010729312897:  27%|███▍         | 32/119 [00:05<00:15,  5.49it/s]Epoch: 6, train for the 33-th batch, train loss: 0.3662010729312897:  28%|███▌         | 33/119 [00:05<00:14,  5.90it/s]evaluate for the 16-th batch, evaluate loss: 0.6124163866043091:  21%|███▊              | 14/66 [00:01<00:05,  9.08it/s]evaluate for the 16-th batch, evaluate loss: 0.6124163866043091:  24%|████▎             | 16/66 [00:01<00:05,  9.70it/s]Epoch: 5, train for the 91-th batch, train loss: 0.510692834854126:  62%|████████▋     | 90/146 [00:15<00:09,  5.65it/s]Epoch: 5, train for the 91-th batch, train loss: 0.510692834854126:  62%|████████▋     | 91/146 [00:15<00:09,  5.76it/s]Epoch: 4, train for the 128-th batch, train loss: 0.4731013774871826:  84%|█████████▎ | 127/151 [00:23<00:04,  4.83it/s]evaluate for the 17-th batch, evaluate loss: 0.6333844065666199:  24%|████▎             | 16/66 [00:01<00:05,  9.70it/s]Epoch: 3, train for the 102-th batch, train loss: 0.5254562497138977:  42%|████▌      | 101/241 [00:16<00:24,  5.68it/s]Epoch: 4, train for the 128-th batch, train loss: 0.4731013774871826:  85%|█████████▎ | 128/151 [00:23<00:04,  4.96it/s]Epoch: 6, train for the 34-th batch, train loss: 0.36686936020851135:  28%|███▎        | 33/119 [00:05<00:14,  5.90it/s]Epoch: 6, train for the 34-th batch, train loss: 0.36686936020851135:  29%|███▍        | 34/119 [00:05<00:13,  6.32it/s]Epoch: 3, train for the 102-th batch, train loss: 0.5254562497138977:  42%|████▋      | 102/241 [00:16<00:25,  5.43it/s]evaluate for the 52-th batch, evaluate loss: 0.6688297986984253:  48%|████████▏        | 51/106 [00:13<00:14,  3.70it/s]evaluate for the 52-th batch, evaluate loss: 0.6688297986984253:  49%|████████▎        | 52/106 [00:13<00:16,  3.35it/s]Epoch: 5, train for the 92-th batch, train loss: 0.5459387302398682:  62%|████████     | 91/146 [00:15<00:09,  5.76it/s]Epoch: 5, train for the 92-th batch, train loss: 0.5459387302398682:  63%|████████▏    | 92/146 [00:15<00:09,  5.94it/s]Epoch: 4, train for the 129-th batch, train loss: 0.48850828409194946:  85%|████████▍ | 128/151 [00:23<00:04,  4.96it/s]evaluate for the 18-th batch, evaluate loss: 0.6524418592453003:  24%|████▎             | 16/66 [00:01<00:05,  9.70it/s]evaluate for the 18-th batch, evaluate loss: 0.6524418592453003:  27%|████▉             | 18/66 [00:01<00:05,  9.08it/s]Epoch: 6, train for the 35-th batch, train loss: 0.3600758910179138:  29%|███▋         | 34/119 [00:05<00:13,  6.32it/s]Epoch: 6, train for the 35-th batch, train loss: 0.3600758910179138:  29%|███▊         | 35/119 [00:05<00:13,  6.32it/s]Epoch: 3, train for the 103-th batch, train loss: 0.4917526841163635:  42%|████▋      | 102/241 [00:16<00:25,  5.43it/s]Epoch: 4, train for the 129-th batch, train loss: 0.48850828409194946:  85%|████████▌ | 129/151 [00:23<00:04,  5.03it/s]Epoch: 3, train for the 103-th batch, train loss: 0.4917526841163635:  43%|████▋      | 103/241 [00:16<00:24,  5.65it/s]evaluate for the 53-th batch, evaluate loss: 0.7461003661155701:  49%|████████▎        | 52/106 [00:13<00:16,  3.35it/s]evaluate for the 53-th batch, evaluate loss: 0.7461003661155701:  50%|████████▌        | 53/106 [00:13<00:13,  3.84it/s]Epoch: 5, train for the 93-th batch, train loss: 0.470028281211853:  63%|████████▊     | 92/146 [00:15<00:09,  5.94it/s]Epoch: 5, train for the 93-th batch, train loss: 0.470028281211853:  64%|████████▉     | 93/146 [00:15<00:09,  5.88it/s]evaluate for the 19-th batch, evaluate loss: 0.6116463541984558:  27%|████▉             | 18/66 [00:02<00:05,  9.08it/s]evaluate for the 19-th batch, evaluate loss: 0.6116463541984558:  29%|█████▏            | 19/66 [00:02<00:05,  8.79it/s]Epoch: 3, train for the 104-th batch, train loss: 0.4889720678329468:  43%|████▋      | 103/241 [00:16<00:24,  5.65it/s]Epoch: 3, train for the 104-th batch, train loss: 0.4889720678329468:  43%|████▋      | 104/241 [00:16<00:22,  6.01it/s]Epoch: 6, train for the 36-th batch, train loss: 0.33722031116485596:  29%|███▌        | 35/119 [00:06<00:13,  6.32it/s]Epoch: 6, train for the 36-th batch, train loss: 0.33722031116485596:  30%|███▋        | 36/119 [00:06<00:13,  6.07it/s]Epoch: 4, train for the 130-th batch, train loss: 0.48498615622520447:  85%|████████▌ | 129/151 [00:23<00:04,  5.03it/s]Epoch: 5, train for the 94-th batch, train loss: 0.4907067120075226:  64%|████████▎    | 93/146 [00:15<00:09,  5.88it/s]Epoch: 4, train for the 130-th batch, train loss: 0.48498615622520447:  86%|████████▌ | 130/151 [00:23<00:04,  4.90it/s]Epoch: 5, train for the 94-th batch, train loss: 0.4907067120075226:  64%|████████▎    | 94/146 [00:15<00:08,  6.22it/s]Epoch: 3, train for the 105-th batch, train loss: 0.452605664730072:  43%|█████▏      | 104/241 [00:16<00:22,  6.01it/s]Epoch: 3, train for the 105-th batch, train loss: 0.452605664730072:  44%|█████▏      | 105/241 [00:16<00:21,  6.31it/s]evaluate for the 20-th batch, evaluate loss: 0.6976553797721863:  29%|█████▏            | 19/66 [00:02<00:05,  8.79it/s]evaluate for the 20-th batch, evaluate loss: 0.6976553797721863:  30%|█████▍            | 20/66 [00:02<00:06,  7.45it/s]Epoch: 6, train for the 37-th batch, train loss: 0.3889729082584381:  30%|███▉         | 36/119 [00:06<00:13,  6.07it/s]evaluate for the 54-th batch, evaluate loss: 0.49591460824012756:  50%|████████        | 53/106 [00:13<00:13,  3.84it/s]evaluate for the 54-th batch, evaluate loss: 0.49591460824012756:  51%|████████▏       | 54/106 [00:13<00:13,  3.75it/s]Epoch: 6, train for the 37-th batch, train loss: 0.3889729082584381:  31%|████         | 37/119 [00:06<00:13,  6.26it/s]Epoch: 5, train for the 95-th batch, train loss: 0.46533942222595215:  64%|███████▋    | 94/146 [00:15<00:08,  6.22it/s]Epoch: 5, train for the 95-th batch, train loss: 0.46533942222595215:  65%|███████▊    | 95/146 [00:15<00:07,  6.43it/s]evaluate for the 21-th batch, evaluate loss: 0.6758058071136475:  30%|█████▍            | 20/66 [00:02<00:06,  7.45it/s]evaluate for the 21-th batch, evaluate loss: 0.6758058071136475:  32%|█████▋            | 21/66 [00:02<00:05,  7.90it/s]Epoch: 4, train for the 131-th batch, train loss: 0.4659864008426666:  86%|█████████▍ | 130/151 [00:23<00:04,  4.90it/s]Epoch: 3, train for the 106-th batch, train loss: 0.5215656161308289:  44%|████▊      | 105/241 [00:17<00:21,  6.31it/s]Epoch: 4, train for the 131-th batch, train loss: 0.4659864008426666:  87%|█████████▌ | 131/151 [00:23<00:04,  4.80it/s]Epoch: 6, train for the 38-th batch, train loss: 0.35316795110702515:  31%|███▋        | 37/119 [00:06<00:13,  6.26it/s]Epoch: 3, train for the 106-th batch, train loss: 0.5215656161308289:  44%|████▊      | 106/241 [00:17<00:21,  6.23it/s]Epoch: 6, train for the 38-th batch, train loss: 0.35316795110702515:  32%|███▊        | 38/119 [00:06<00:12,  6.52it/s]evaluate for the 55-th batch, evaluate loss: 0.43875768780708313:  51%|████████▏       | 54/106 [00:13<00:13,  3.75it/s]evaluate for the 55-th batch, evaluate loss: 0.43875768780708313:  52%|████████▎       | 55/106 [00:13<00:11,  4.34it/s]Epoch: 5, train for the 96-th batch, train loss: 0.500590443611145:  65%|█████████     | 95/146 [00:15<00:07,  6.43it/s]Epoch: 5, train for the 96-th batch, train loss: 0.500590443611145:  66%|█████████▏    | 96/146 [00:16<00:07,  6.39it/s]evaluate for the 22-th batch, evaluate loss: 0.6372076869010925:  32%|█████▋            | 21/66 [00:02<00:05,  7.90it/s]evaluate for the 22-th batch, evaluate loss: 0.6372076869010925:  33%|██████            | 22/66 [00:02<00:05,  7.57it/s]Epoch: 3, train for the 107-th batch, train loss: 0.5357937216758728:  44%|████▊      | 106/241 [00:17<00:21,  6.23it/s]Epoch: 6, train for the 39-th batch, train loss: 0.4073309600353241:  32%|████▏        | 38/119 [00:06<00:12,  6.52it/s]Epoch: 6, train for the 39-th batch, train loss: 0.4073309600353241:  33%|████▎        | 39/119 [00:06<00:12,  6.41it/s]Epoch: 3, train for the 107-th batch, train loss: 0.5357937216758728:  44%|████▉      | 107/241 [00:17<00:21,  6.14it/s]Epoch: 4, train for the 132-th batch, train loss: 0.5064848065376282:  87%|█████████▌ | 131/151 [00:23<00:04,  4.80it/s]Epoch: 4, train for the 132-th batch, train loss: 0.5064848065376282:  87%|█████████▌ | 132/151 [00:23<00:04,  4.69it/s]evaluate for the 56-th batch, evaluate loss: 0.5796031951904297:  52%|████████▊        | 55/106 [00:13<00:11,  4.34it/s]evaluate for the 56-th batch, evaluate loss: 0.5796031951904297:  53%|████████▉        | 56/106 [00:13<00:11,  4.37it/s]evaluate for the 23-th batch, evaluate loss: 0.652073323726654:  33%|██████▎            | 22/66 [00:02<00:05,  7.57it/s]evaluate for the 23-th batch, evaluate loss: 0.652073323726654:  35%|██████▌            | 23/66 [00:02<00:05,  7.55it/s]Epoch: 3, train for the 108-th batch, train loss: 0.4913884103298187:  44%|████▉      | 107/241 [00:17<00:21,  6.14it/s]Epoch: 3, train for the 108-th batch, train loss: 0.4913884103298187:  45%|████▉      | 108/241 [00:17<00:21,  6.32it/s]Epoch: 5, train for the 97-th batch, train loss: 0.49973371624946594:  66%|███████▉    | 96/146 [00:16<00:07,  6.39it/s]Epoch: 6, train for the 40-th batch, train loss: 0.3398006856441498:  33%|████▎        | 39/119 [00:06<00:12,  6.41it/s]Epoch: 4, train for the 133-th batch, train loss: 0.4353629946708679:  87%|█████████▌ | 132/151 [00:24<00:04,  4.69it/s]Epoch: 5, train for the 97-th batch, train loss: 0.49973371624946594:  66%|███████▉    | 97/146 [00:16<00:09,  5.11it/s]Epoch: 6, train for the 40-th batch, train loss: 0.3398006856441498:  34%|████▎        | 40/119 [00:06<00:13,  5.89it/s]evaluate for the 57-th batch, evaluate loss: 0.5640700459480286:  53%|████████▉        | 56/106 [00:14<00:11,  4.37it/s]evaluate for the 57-th batch, evaluate loss: 0.5640700459480286:  54%|█████████▏       | 57/106 [00:14<00:09,  4.96it/s]Epoch: 4, train for the 133-th batch, train loss: 0.4353629946708679:  88%|█████████▋ | 133/151 [00:24<00:03,  5.03it/s]Epoch: 3, train for the 109-th batch, train loss: 0.682668924331665:  45%|█████▍      | 108/241 [00:17<00:21,  6.32it/s]Epoch: 3, train for the 109-th batch, train loss: 0.682668924331665:  45%|█████▍      | 109/241 [00:17<00:19,  6.66it/s]Epoch: 5, train for the 98-th batch, train loss: 0.4985106885433197:  66%|████████▋    | 97/146 [00:16<00:09,  5.11it/s]evaluate for the 24-th batch, evaluate loss: 0.6301174759864807:  35%|██████▎           | 23/66 [00:02<00:05,  7.55it/s]evaluate for the 24-th batch, evaluate loss: 0.6301174759864807:  36%|██████▌           | 24/66 [00:02<00:07,  5.77it/s]Epoch: 5, train for the 98-th batch, train loss: 0.4985106885433197:  67%|████████▋    | 98/146 [00:16<00:08,  5.35it/s]Epoch: 6, train for the 41-th batch, train loss: 0.36996984481811523:  34%|████        | 40/119 [00:06<00:13,  5.89it/s]Epoch: 6, train for the 41-th batch, train loss: 0.36996984481811523:  34%|████▏       | 41/119 [00:06<00:13,  5.88it/s]Epoch: 4, train for the 134-th batch, train loss: 0.47248968482017517:  88%|████████▊ | 133/151 [00:24<00:03,  5.03it/s]Epoch: 4, train for the 134-th batch, train loss: 0.47248968482017517:  89%|████████▊ | 134/151 [00:24<00:03,  5.15it/s]Epoch: 3, train for the 110-th batch, train loss: 0.5930201411247253:  45%|████▉      | 109/241 [00:17<00:19,  6.66it/s]Epoch: 3, train for the 110-th batch, train loss: 0.5930201411247253:  46%|█████      | 110/241 [00:17<00:19,  6.79it/s]evaluate for the 25-th batch, evaluate loss: 0.5967203974723816:  36%|██████▌           | 24/66 [00:03<00:07,  5.77it/s]evaluate for the 25-th batch, evaluate loss: 0.5967203974723816:  38%|██████▊           | 25/66 [00:03<00:06,  6.45it/s]evaluate for the 58-th batch, evaluate loss: 0.5817327499389648:  54%|█████████▏       | 57/106 [00:14<00:09,  4.96it/s]evaluate for the 58-th batch, evaluate loss: 0.5817327499389648:  55%|█████████▎       | 58/106 [00:14<00:11,  4.27it/s]Epoch: 5, train for the 99-th batch, train loss: 0.5376081466674805:  67%|████████▋    | 98/146 [00:16<00:08,  5.35it/s]Epoch: 6, train for the 42-th batch, train loss: 0.426167756319046:  34%|████▊         | 41/119 [00:07<00:13,  5.88it/s]Epoch: 5, train for the 99-th batch, train loss: 0.5376081466674805:  68%|████████▊    | 99/146 [00:16<00:08,  5.48it/s]Epoch: 6, train for the 42-th batch, train loss: 0.426167756319046:  35%|████▉         | 42/119 [00:07<00:13,  5.92it/s]Epoch: 4, train for the 135-th batch, train loss: 0.4660758972167969:  89%|█████████▊ | 134/151 [00:24<00:03,  5.15it/s]evaluate for the 26-th batch, evaluate loss: 0.6661973595619202:  38%|██████▊           | 25/66 [00:03<00:06,  6.45it/s]evaluate for the 26-th batch, evaluate loss: 0.6661973595619202:  39%|███████           | 26/66 [00:03<00:05,  6.93it/s]Epoch: 3, train for the 111-th batch, train loss: 0.20311063528060913:  46%|████▌     | 110/241 [00:17<00:19,  6.79it/s]Epoch: 4, train for the 135-th batch, train loss: 0.4660758972167969:  89%|█████████▊ | 135/151 [00:24<00:03,  5.14it/s]Epoch: 3, train for the 111-th batch, train loss: 0.20311063528060913:  46%|████▌     | 111/241 [00:17<00:20,  6.43it/s]evaluate for the 59-th batch, evaluate loss: 0.6375755667686462:  55%|█████████▎       | 58/106 [00:14<00:11,  4.27it/s]evaluate for the 59-th batch, evaluate loss: 0.6375755667686462:  56%|█████████▍       | 59/106 [00:14<00:09,  4.79it/s]evaluate for the 27-th batch, evaluate loss: 0.5982292890548706:  39%|███████           | 26/66 [00:03<00:05,  6.93it/s]evaluate for the 27-th batch, evaluate loss: 0.5982292890548706:  41%|███████▎          | 27/66 [00:03<00:05,  7.41it/s]Epoch: 5, train for the 100-th batch, train loss: 0.5459051132202148:  68%|████████▏   | 99/146 [00:16<00:08,  5.48it/s]Epoch: 5, train for the 100-th batch, train loss: 0.5459051132202148:  68%|███████▌   | 100/146 [00:16<00:08,  5.56it/s]Epoch: 6, train for the 43-th batch, train loss: 0.376085489988327:  35%|████▉         | 42/119 [00:07<00:13,  5.92it/s]Epoch: 6, train for the 43-th batch, train loss: 0.376085489988327:  36%|█████         | 43/119 [00:07<00:13,  5.58it/s]Epoch: 4, train for the 136-th batch, train loss: 0.4555549919605255:  89%|█████████▊ | 135/151 [00:24<00:03,  5.14it/s]Epoch: 3, train for the 112-th batch, train loss: 0.31036654114723206:  46%|████▌     | 111/241 [00:17<00:20,  6.43it/s]Epoch: 3, train for the 112-th batch, train loss: 0.31036654114723206:  46%|████▋     | 112/241 [00:18<00:21,  6.01it/s]Epoch: 4, train for the 136-th batch, train loss: 0.4555549919605255:  90%|█████████▉ | 136/151 [00:24<00:02,  5.09it/s]Epoch: 5, train for the 101-th batch, train loss: 0.4791913628578186:  68%|███████▌   | 100/146 [00:16<00:08,  5.56it/s]Epoch: 5, train for the 101-th batch, train loss: 0.4791913628578186:  69%|███████▌   | 101/146 [00:16<00:07,  5.87it/s]Epoch: 6, train for the 44-th batch, train loss: 0.3427228331565857:  36%|████▋        | 43/119 [00:07<00:13,  5.58it/s]Epoch: 6, train for the 44-th batch, train loss: 0.3427228331565857:  37%|████▊        | 44/119 [00:07<00:13,  5.71it/s]Epoch: 3, train for the 113-th batch, train loss: 0.4239654541015625:  46%|█████      | 112/241 [00:18<00:21,  6.01it/s]Epoch: 3, train for the 113-th batch, train loss: 0.4239654541015625:  47%|█████▏     | 113/241 [00:18<00:21,  6.04it/s]Epoch: 4, train for the 137-th batch, train loss: 0.45766761898994446:  90%|█████████ | 136/151 [00:24<00:02,  5.09it/s]Epoch: 5, train for the 102-th batch, train loss: 0.45448213815689087:  69%|██████▉   | 101/146 [00:17<00:07,  5.87it/s]evaluate for the 28-th batch, evaluate loss: 0.7375360727310181:  41%|███████▎          | 27/66 [00:03<00:05,  7.41it/s]evaluate for the 28-th batch, evaluate loss: 0.7375360727310181:  42%|███████▋          | 28/66 [00:03<00:06,  5.49it/s]evaluate for the 60-th batch, evaluate loss: 0.5916325449943542:  56%|█████████▍       | 59/106 [00:14<00:09,  4.79it/s]evaluate for the 60-th batch, evaluate loss: 0.5916325449943542:  57%|█████████▌       | 60/106 [00:14<00:11,  4.10it/s]Epoch: 4, train for the 137-th batch, train loss: 0.45766761898994446:  91%|█████████ | 137/151 [00:24<00:02,  5.05it/s]Epoch: 5, train for the 102-th batch, train loss: 0.45448213815689087:  70%|██████▉   | 102/146 [00:17<00:07,  6.13it/s]Epoch: 6, train for the 45-th batch, train loss: 0.34205424785614014:  37%|████▍       | 44/119 [00:07<00:13,  5.71it/s]Epoch: 6, train for the 45-th batch, train loss: 0.34205424785614014:  38%|████▌       | 45/119 [00:07<00:12,  6.12it/s]Epoch: 3, train for the 114-th batch, train loss: 0.39530616998672485:  47%|████▋     | 113/241 [00:18<00:21,  6.04it/s]Epoch: 3, train for the 114-th batch, train loss: 0.39530616998672485:  47%|████▋     | 114/241 [00:18<00:21,  5.97it/s]evaluate for the 29-th batch, evaluate loss: 0.6096701622009277:  42%|███████▋          | 28/66 [00:03<00:06,  5.49it/s]evaluate for the 29-th batch, evaluate loss: 0.6096701622009277:  44%|███████▉          | 29/66 [00:03<00:06,  5.79it/s]evaluate for the 61-th batch, evaluate loss: 0.6038293838500977:  57%|█████████▌       | 60/106 [00:14<00:11,  4.10it/s]evaluate for the 61-th batch, evaluate loss: 0.6038293838500977:  58%|█████████▊       | 61/106 [00:14<00:09,  4.58it/s]Epoch: 4, train for the 138-th batch, train loss: 0.4848168194293976:  91%|█████████▉ | 137/151 [00:25<00:02,  5.05it/s]Epoch: 5, train for the 103-th batch, train loss: 0.5027547478675842:  70%|███████▋   | 102/146 [00:17<00:07,  6.13it/s]Epoch: 4, train for the 138-th batch, train loss: 0.4848168194293976:  91%|██████████ | 138/151 [00:25<00:02,  5.02it/s]Epoch: 5, train for the 103-th batch, train loss: 0.5027547478675842:  71%|███████▊   | 103/146 [00:17<00:07,  5.80it/s]Epoch: 6, train for the 46-th batch, train loss: 0.3582206964492798:  38%|████▉        | 45/119 [00:07<00:12,  6.12it/s]Epoch: 6, train for the 46-th batch, train loss: 0.3582206964492798:  39%|█████        | 46/119 [00:07<00:12,  5.89it/s]evaluate for the 30-th batch, evaluate loss: 0.663047194480896:  44%|████████▎          | 29/66 [00:03<00:06,  5.79it/s]evaluate for the 30-th batch, evaluate loss: 0.663047194480896:  45%|████████▋          | 30/66 [00:03<00:05,  6.53it/s]Epoch: 3, train for the 115-th batch, train loss: 0.38635265827178955:  47%|████▋     | 114/241 [00:18<00:21,  5.97it/s]Epoch: 3, train for the 115-th batch, train loss: 0.38635265827178955:  48%|████▊     | 115/241 [00:18<00:21,  5.83it/s]evaluate for the 31-th batch, evaluate loss: 0.6425617337226868:  45%|████████▏         | 30/66 [00:03<00:05,  6.53it/s]Epoch: 4, train for the 139-th batch, train loss: 0.4679257869720459:  91%|██████████ | 138/151 [00:25<00:02,  5.02it/s]Epoch: 5, train for the 104-th batch, train loss: 0.4960031807422638:  71%|███████▊   | 103/146 [00:17<00:07,  5.80it/s]Epoch: 5, train for the 104-th batch, train loss: 0.4960031807422638:  71%|███████▊   | 104/146 [00:17<00:07,  6.00it/s]Epoch: 4, train for the 139-th batch, train loss: 0.4679257869720459:  92%|██████████▏| 139/151 [00:25<00:02,  5.30it/s]Epoch: 6, train for the 47-th batch, train loss: 0.4197521209716797:  39%|█████        | 46/119 [00:08<00:12,  5.89it/s]Epoch: 6, train for the 47-th batch, train loss: 0.4197521209716797:  39%|█████▏       | 47/119 [00:08<00:12,  5.86it/s]Epoch: 3, train for the 116-th batch, train loss: 0.4988043010234833:  48%|█████▏     | 115/241 [00:18<00:21,  5.83it/s]Epoch: 3, train for the 116-th batch, train loss: 0.4988043010234833:  48%|█████▎     | 116/241 [00:18<00:20,  6.03it/s]Epoch: 5, train for the 105-th batch, train loss: 0.5060128569602966:  71%|███████▊   | 104/146 [00:17<00:07,  6.00it/s]Epoch: 5, train for the 105-th batch, train loss: 0.5060128569602966:  72%|███████▉   | 105/146 [00:17<00:06,  6.41it/s]Epoch: 4, train for the 140-th batch, train loss: 0.4088687300682068:  92%|██████████▏| 139/151 [00:25<00:02,  5.30it/s]Epoch: 4, train for the 140-th batch, train loss: 0.4088687300682068:  93%|██████████▏| 140/151 [00:25<00:02,  5.47it/s]Epoch: 6, train for the 48-th batch, train loss: 0.39809027314186096:  39%|████▋       | 47/119 [00:08<00:12,  5.86it/s]Epoch: 6, train for the 48-th batch, train loss: 0.39809027314186096:  40%|████▊       | 48/119 [00:08<00:11,  6.08it/s]Epoch: 3, train for the 117-th batch, train loss: 0.5861296057701111:  48%|█████▎     | 116/241 [00:18<00:20,  6.03it/s]Epoch: 3, train for the 117-th batch, train loss: 0.5861296057701111:  49%|█████▎     | 117/241 [00:18<00:19,  6.30it/s]Epoch: 5, train for the 106-th batch, train loss: 0.5354822278022766:  72%|███████▉   | 105/146 [00:17<00:06,  6.41it/s]Epoch: 5, train for the 106-th batch, train loss: 0.5354822278022766:  73%|███████▉   | 106/146 [00:17<00:06,  6.59it/s]evaluate for the 32-th batch, evaluate loss: 0.6846218705177307:  45%|████████▏         | 30/66 [00:04<00:05,  6.53it/s]evaluate for the 32-th batch, evaluate loss: 0.6846218705177307:  48%|████████▋         | 32/66 [00:04<00:06,  5.50it/s]Epoch: 6, train for the 49-th batch, train loss: 0.39150890707969666:  40%|████▊       | 48/119 [00:08<00:11,  6.08it/s]evaluate for the 62-th batch, evaluate loss: 0.5737292170524597:  58%|█████████▊       | 61/106 [00:15<00:09,  4.58it/s]evaluate for the 62-th batch, evaluate loss: 0.5737292170524597:  58%|█████████▉       | 62/106 [00:15<00:13,  3.18it/s]Epoch: 6, train for the 49-th batch, train loss: 0.39150890707969666:  41%|████▉       | 49/119 [00:08<00:11,  6.35it/s]Epoch: 3, train for the 118-th batch, train loss: 0.4631587564945221:  49%|█████▎     | 117/241 [00:18<00:19,  6.30it/s]Epoch: 3, train for the 118-th batch, train loss: 0.4631587564945221:  49%|█████▍     | 118/241 [00:18<00:19,  6.44it/s]Epoch: 5, train for the 107-th batch, train loss: 0.4773843586444855:  73%|███████▉   | 106/146 [00:17<00:06,  6.59it/s]Epoch: 5, train for the 107-th batch, train loss: 0.4773843586444855:  73%|████████   | 107/146 [00:17<00:05,  6.56it/s]evaluate for the 33-th batch, evaluate loss: 0.66270512342453:  48%|█████████▋          | 32/66 [00:04<00:06,  5.50it/s]evaluate for the 33-th batch, evaluate loss: 0.66270512342453:  50%|██████████          | 33/66 [00:04<00:05,  6.07it/s]Epoch: 4, train for the 141-th batch, train loss: 0.47557222843170166:  93%|█████████▎| 140/151 [00:25<00:02,  5.47it/s]Epoch: 6, train for the 50-th batch, train loss: 0.33806124329566956:  41%|████▉       | 49/119 [00:08<00:11,  6.35it/s]Epoch: 4, train for the 141-th batch, train loss: 0.47557222843170166:  93%|█████████▎| 141/151 [00:25<00:02,  4.52it/s]Epoch: 6, train for the 50-th batch, train loss: 0.33806124329566956:  42%|█████       | 50/119 [00:08<00:10,  6.37it/s]evaluate for the 63-th batch, evaluate loss: 0.589478075504303:  58%|██████████▌       | 62/106 [00:15<00:13,  3.18it/s]evaluate for the 63-th batch, evaluate loss: 0.589478075504303:  59%|██████████▋       | 63/106 [00:15<00:11,  3.67it/s]Epoch: 3, train for the 119-th batch, train loss: 0.34920814633369446:  49%|████▉     | 118/241 [00:19<00:19,  6.44it/s]Epoch: 5, train for the 108-th batch, train loss: 0.4705871343612671:  73%|████████   | 107/146 [00:18<00:05,  6.56it/s]Epoch: 5, train for the 108-th batch, train loss: 0.4705871343612671:  74%|████████▏  | 108/146 [00:18<00:05,  6.53it/s]Epoch: 3, train for the 119-th batch, train loss: 0.34920814633369446:  49%|████▉     | 119/241 [00:19<00:20,  6.00it/s]evaluate for the 34-th batch, evaluate loss: 0.6582815647125244:  50%|█████████         | 33/66 [00:04<00:05,  6.07it/s]evaluate for the 34-th batch, evaluate loss: 0.6582815647125244:  52%|█████████▎        | 34/66 [00:04<00:05,  5.79it/s]Epoch: 4, train for the 142-th batch, train loss: 0.44626957178115845:  93%|█████████▎| 141/151 [00:25<00:02,  4.52it/s]Epoch: 6, train for the 51-th batch, train loss: 0.442258358001709:  42%|█████▉        | 50/119 [00:08<00:10,  6.37it/s]Epoch: 6, train for the 51-th batch, train loss: 0.442258358001709:  43%|██████        | 51/119 [00:08<00:11,  6.15it/s]Epoch: 4, train for the 142-th batch, train loss: 0.44626957178115845:  94%|█████████▍| 142/151 [00:25<00:01,  4.77it/s]Epoch: 5, train for the 109-th batch, train loss: 0.48625272512435913:  74%|███████▍  | 108/146 [00:18<00:05,  6.53it/s]Epoch: 5, train for the 109-th batch, train loss: 0.48625272512435913:  75%|███████▍  | 109/146 [00:18<00:05,  6.79it/s]evaluate for the 35-th batch, evaluate loss: 0.6862118244171143:  52%|█████████▎        | 34/66 [00:04<00:05,  5.79it/s]Epoch: 3, train for the 120-th batch, train loss: 0.5514282584190369:  49%|█████▍     | 119/241 [00:19<00:20,  6.00it/s]Epoch: 3, train for the 120-th batch, train loss: 0.5514282584190369:  50%|█████▍     | 120/241 [00:19<00:19,  6.14it/s]Epoch: 4, train for the 143-th batch, train loss: 0.40667062997817993:  94%|█████████▍| 142/151 [00:26<00:01,  4.77it/s]Epoch: 6, train for the 52-th batch, train loss: 0.3799469769001007:  43%|█████▌       | 51/119 [00:08<00:11,  6.15it/s]Epoch: 6, train for the 52-th batch, train loss: 0.3799469769001007:  44%|█████▋       | 52/119 [00:08<00:11,  6.00it/s]Epoch: 4, train for the 143-th batch, train loss: 0.40667062997817993:  95%|█████████▍| 143/151 [00:26<00:01,  5.01it/s]Epoch: 5, train for the 110-th batch, train loss: 0.4509429335594177:  75%|████████▏  | 109/146 [00:18<00:05,  6.79it/s]Epoch: 5, train for the 110-th batch, train loss: 0.4509429335594177:  75%|████████▎  | 110/146 [00:18<00:05,  6.64it/s]Epoch: 3, train for the 121-th batch, train loss: 0.49711698293685913:  50%|████▉     | 120/241 [00:19<00:19,  6.14it/s]Epoch: 3, train for the 121-th batch, train loss: 0.49711698293685913:  50%|█████     | 121/241 [00:19<00:19,  6.29it/s]evaluate for the 36-th batch, evaluate loss: 0.6376267671585083:  52%|█████████▎        | 34/66 [00:04<00:05,  5.79it/s]evaluate for the 36-th batch, evaluate loss: 0.6376267671585083:  55%|█████████▊        | 36/66 [00:04<00:04,  6.34it/s]evaluate for the 64-th batch, evaluate loss: 0.4595630168914795:  59%|██████████       | 63/106 [00:16<00:11,  3.67it/s]evaluate for the 64-th batch, evaluate loss: 0.4595630168914795:  60%|██████████▎      | 64/106 [00:16<00:13,  3.20it/s]Epoch: 6, train for the 53-th batch, train loss: 0.376295804977417:  44%|██████        | 52/119 [00:08<00:11,  6.00it/s]Epoch: 6, train for the 53-th batch, train loss: 0.376295804977417:  45%|██████▏       | 53/119 [00:08<00:11,  5.89it/s]evaluate for the 37-th batch, evaluate loss: 0.6759317517280579:  55%|█████████▊        | 36/66 [00:04<00:04,  6.34it/s]evaluate for the 37-th batch, evaluate loss: 0.6759317517280579:  56%|██████████        | 37/66 [00:04<00:04,  6.62it/s]Epoch: 4, train for the 144-th batch, train loss: 0.3907647728919983:  95%|██████████▍| 143/151 [00:26<00:01,  5.01it/s]evaluate for the 65-th batch, evaluate loss: 0.6660344004631042:  60%|██████████▎      | 64/106 [00:16<00:13,  3.20it/s]evaluate for the 65-th batch, evaluate loss: 0.6660344004631042:  61%|██████████▍      | 65/106 [00:16<00:10,  3.80it/s]Epoch: 4, train for the 144-th batch, train loss: 0.3907647728919983:  95%|██████████▍| 144/151 [00:26<00:01,  4.87it/s]Epoch: 5, train for the 111-th batch, train loss: 0.5000177025794983:  75%|████████▎  | 110/146 [00:18<00:05,  6.64it/s]Epoch: 3, train for the 122-th batch, train loss: 0.45833253860473633:  50%|█████     | 121/241 [00:19<00:19,  6.29it/s]Epoch: 5, train for the 111-th batch, train loss: 0.5000177025794983:  76%|████████▎  | 111/146 [00:18<00:05,  5.99it/s]Epoch: 3, train for the 122-th batch, train loss: 0.45833253860473633:  51%|█████     | 122/241 [00:19<00:20,  5.85it/s]Epoch: 6, train for the 54-th batch, train loss: 0.3455132842063904:  45%|█████▊       | 53/119 [00:09<00:11,  5.89it/s]Epoch: 6, train for the 54-th batch, train loss: 0.3455132842063904:  45%|█████▉       | 54/119 [00:09<00:10,  6.24it/s]Epoch: 5, train for the 112-th batch, train loss: 0.48717308044433594:  76%|███████▌  | 111/146 [00:18<00:05,  5.99it/s]Epoch: 5, train for the 112-th batch, train loss: 0.48717308044433594:  77%|███████▋  | 112/146 [00:18<00:05,  6.05it/s]Epoch: 4, train for the 145-th batch, train loss: 0.39621487259864807:  95%|█████████▌| 144/151 [00:26<00:01,  4.87it/s]Epoch: 3, train for the 123-th batch, train loss: 0.7052745819091797:  51%|█████▌     | 122/241 [00:19<00:20,  5.85it/s]Epoch: 4, train for the 145-th batch, train loss: 0.39621487259864807:  96%|█████████▌| 145/151 [00:26<00:01,  4.89it/s]Epoch: 3, train for the 123-th batch, train loss: 0.7052745819091797:  51%|█████▌     | 123/241 [00:19<00:20,  5.71it/s]Epoch: 6, train for the 55-th batch, train loss: 0.35888782143592834:  45%|█████▍      | 54/119 [00:09<00:10,  6.24it/s]Epoch: 6, train for the 55-th batch, train loss: 0.35888782143592834:  46%|█████▌      | 55/119 [00:09<00:09,  6.57it/s]Epoch: 5, train for the 113-th batch, train loss: 0.42740964889526367:  77%|███████▋  | 112/146 [00:18<00:05,  6.05it/s]evaluate for the 66-th batch, evaluate loss: 0.6129754781723022:  61%|██████████▍      | 65/106 [00:16<00:10,  3.80it/s]evaluate for the 66-th batch, evaluate loss: 0.6129754781723022:  62%|██████████▌      | 66/106 [00:16<00:11,  3.62it/s]Epoch: 5, train for the 113-th batch, train loss: 0.42740964889526367:  77%|███████▋  | 113/146 [00:18<00:05,  6.39it/s]Epoch: 6, train for the 56-th batch, train loss: 0.38763388991355896:  46%|█████▌      | 55/119 [00:09<00:09,  6.57it/s]Epoch: 6, train for the 56-th batch, train loss: 0.38763388991355896:  47%|█████▋      | 56/119 [00:09<00:09,  6.58it/s]evaluate for the 38-th batch, evaluate loss: 0.5839094519615173:  56%|██████████        | 37/66 [00:05<00:04,  6.62it/s]evaluate for the 38-th batch, evaluate loss: 0.5839094519615173:  58%|██████████▎       | 38/66 [00:05<00:06,  4.56it/s]Epoch: 3, train for the 124-th batch, train loss: 0.39409339427948:  51%|██████▋      | 123/241 [00:20<00:20,  5.71it/s]Epoch: 4, train for the 146-th batch, train loss: 0.45892298221588135:  96%|█████████▌| 145/151 [00:26<00:01,  4.89it/s]Epoch: 3, train for the 124-th batch, train loss: 0.39409339427948:  51%|██████▋      | 124/241 [00:20<00:22,  5.30it/s]Epoch: 5, train for the 114-th batch, train loss: 0.47721707820892334:  77%|███████▋  | 113/146 [00:18<00:05,  6.39it/s]Epoch: 5, train for the 114-th batch, train loss: 0.47721707820892334:  78%|███████▊  | 114/146 [00:18<00:04,  6.70it/s]evaluate for the 67-th batch, evaluate loss: 0.5714178681373596:  62%|██████████▌      | 66/106 [00:16<00:11,  3.62it/s]evaluate for the 67-th batch, evaluate loss: 0.5714178681373596:  63%|██████████▋      | 67/106 [00:16<00:09,  4.25it/s]Epoch: 4, train for the 146-th batch, train loss: 0.45892298221588135:  97%|█████████▋| 146/151 [00:26<00:01,  4.52it/s]Epoch: 6, train for the 57-th batch, train loss: 0.3408275544643402:  47%|██████       | 56/119 [00:09<00:09,  6.58it/s]evaluate for the 39-th batch, evaluate loss: 0.6834526062011719:  58%|██████████▎       | 38/66 [00:05<00:06,  4.56it/s]evaluate for the 39-th batch, evaluate loss: 0.6834526062011719:  59%|██████████▋       | 39/66 [00:05<00:05,  5.07it/s]Epoch: 6, train for the 57-th batch, train loss: 0.3408275544643402:  48%|██████▏      | 57/119 [00:09<00:09,  6.48it/s]Epoch: 3, train for the 125-th batch, train loss: 0.47503164410591125:  51%|█████▏    | 124/241 [00:20<00:22,  5.30it/s]Epoch: 3, train for the 125-th batch, train loss: 0.47503164410591125:  52%|█████▏    | 125/241 [00:20<00:19,  5.89it/s]Epoch: 5, train for the 115-th batch, train loss: 0.5326621532440186:  78%|████████▌  | 114/146 [00:19<00:04,  6.70it/s]Epoch: 5, train for the 115-th batch, train loss: 0.5326621532440186:  79%|████████▋  | 115/146 [00:19<00:04,  6.57it/s]evaluate for the 68-th batch, evaluate loss: 0.5773071050643921:  63%|██████████▋      | 67/106 [00:16<00:09,  4.25it/s]evaluate for the 68-th batch, evaluate loss: 0.5773071050643921:  64%|██████████▉      | 68/106 [00:16<00:08,  4.52it/s]Epoch: 3, train for the 126-th batch, train loss: 0.4391027092933655:  52%|█████▋     | 125/241 [00:20<00:19,  5.89it/s]Epoch: 6, train for the 58-th batch, train loss: 0.3228226900100708:  48%|██████▏      | 57/119 [00:09<00:09,  6.48it/s]Epoch: 6, train for the 58-th batch, train loss: 0.3228226900100708:  49%|██████▎      | 58/119 [00:09<00:09,  6.42it/s]Epoch: 3, train for the 126-th batch, train loss: 0.4391027092933655:  52%|█████▊     | 126/241 [00:20<00:18,  6.16it/s]evaluate for the 69-th batch, evaluate loss: 0.7051929831504822:  64%|██████████▉      | 68/106 [00:17<00:08,  4.52it/s]evaluate for the 69-th batch, evaluate loss: 0.7051929831504822:  65%|███████████      | 69/106 [00:17<00:07,  5.03it/s]Epoch: 6, train for the 59-th batch, train loss: 0.35931167006492615:  49%|█████▊      | 58/119 [00:09<00:09,  6.42it/s]Epoch: 3, train for the 127-th batch, train loss: 0.4768697917461395:  52%|█████▊     | 126/241 [00:20<00:18,  6.16it/s]Epoch: 6, train for the 59-th batch, train loss: 0.35931167006492615:  50%|█████▉      | 59/119 [00:09<00:08,  7.04it/s]Epoch: 3, train for the 127-th batch, train loss: 0.4768697917461395:  53%|█████▊     | 127/241 [00:20<00:17,  6.69it/s]evaluate for the 40-th batch, evaluate loss: 0.6591419577598572:  59%|██████████▋       | 39/66 [00:05<00:05,  5.07it/s]evaluate for the 40-th batch, evaluate loss: 0.6591419577598572:  61%|██████████▉       | 40/66 [00:05<00:06,  4.23it/s]Epoch: 4, train for the 147-th batch, train loss: 0.4151546359062195:  97%|██████████▋| 146/151 [00:27<00:01,  4.52it/s]Epoch: 6, train for the 60-th batch, train loss: 0.3663465082645416:  50%|██████▍      | 59/119 [00:10<00:08,  7.04it/s]Epoch: 5, train for the 116-th batch, train loss: 0.504878580570221:  79%|█████████▍  | 115/146 [00:19<00:04,  6.57it/s]Epoch: 6, train for the 60-th batch, train loss: 0.3663465082645416:  50%|██████▌      | 60/119 [00:10<00:09,  6.32it/s]Epoch: 3, train for the 128-th batch, train loss: 0.545486330986023:  53%|██████▎     | 127/241 [00:20<00:17,  6.69it/s]Epoch: 5, train for the 116-th batch, train loss: 0.504878580570221:  79%|█████████▌  | 116/146 [00:19<00:06,  4.34it/s]Epoch: 4, train for the 147-th batch, train loss: 0.4151546359062195:  97%|██████████▋| 147/151 [00:27<00:01,  3.12it/s]evaluate for the 70-th batch, evaluate loss: 0.5129116773605347:  65%|███████████      | 69/106 [00:17<00:07,  5.03it/s]evaluate for the 70-th batch, evaluate loss: 0.5129116773605347:  66%|███████████▏     | 70/106 [00:17<00:07,  4.64it/s]Epoch: 3, train for the 128-th batch, train loss: 0.545486330986023:  53%|██████▎     | 128/241 [00:20<00:19,  5.93it/s]evaluate for the 41-th batch, evaluate loss: 0.602149486541748:  61%|███████████▌       | 40/66 [00:06<00:06,  4.23it/s]evaluate for the 41-th batch, evaluate loss: 0.602149486541748:  62%|███████████▊       | 41/66 [00:06<00:05,  4.63it/s]Epoch: 4, train for the 148-th batch, train loss: 0.38203585147857666:  97%|█████████▋| 147/151 [00:27<00:01,  3.12it/s]Epoch: 4, train for the 148-th batch, train loss: 0.38203585147857666:  98%|█████████▊| 148/151 [00:27<00:00,  3.59it/s]Epoch: 3, train for the 129-th batch, train loss: 0.4598618149757385:  53%|█████▊     | 128/241 [00:20<00:19,  5.93it/s]evaluate for the 42-th batch, evaluate loss: 0.6804819107055664:  62%|███████████▏      | 41/66 [00:06<00:05,  4.63it/s]evaluate for the 42-th batch, evaluate loss: 0.6804819107055664:  64%|███████████▍      | 42/66 [00:06<00:04,  4.95it/s]evaluate for the 71-th batch, evaluate loss: 0.583404541015625:  66%|███████████▉      | 70/106 [00:17<00:07,  4.64it/s]evaluate for the 71-th batch, evaluate loss: 0.583404541015625:  67%|████████████      | 71/106 [00:17<00:07,  4.94it/s]Epoch: 6, train for the 61-th batch, train loss: 0.3533056676387787:  50%|██████▌      | 60/119 [00:10<00:09,  6.32it/s]Epoch: 3, train for the 129-th batch, train loss: 0.4598618149757385:  54%|█████▉     | 129/241 [00:20<00:19,  5.82it/s]Epoch: 5, train for the 117-th batch, train loss: 0.5447275638580322:  79%|████████▋  | 116/146 [00:19<00:06,  4.34it/s]Epoch: 6, train for the 61-th batch, train loss: 0.3533056676387787:  51%|██████▋      | 61/119 [00:10<00:10,  5.67it/s]Epoch: 5, train for the 117-th batch, train loss: 0.5447275638580322:  80%|████████▊  | 117/146 [00:19<00:06,  4.42it/s]evaluate for the 43-th batch, evaluate loss: 0.6250081062316895:  64%|███████████▍      | 42/66 [00:06<00:04,  4.95it/s]evaluate for the 43-th batch, evaluate loss: 0.6250081062316895:  65%|███████████▋      | 43/66 [00:06<00:04,  5.73it/s]Epoch: 4, train for the 149-th batch, train loss: 0.3878953754901886:  98%|██████████▊| 148/151 [00:27<00:00,  3.59it/s]Epoch: 3, train for the 130-th batch, train loss: 0.4907946288585663:  54%|█████▉     | 129/241 [00:21<00:19,  5.82it/s]Epoch: 6, train for the 62-th batch, train loss: 0.37762123346328735:  51%|██████▏     | 61/119 [00:10<00:10,  5.67it/s]Epoch: 6, train for the 62-th batch, train loss: 0.37762123346328735:  52%|██████▎     | 62/119 [00:10<00:10,  5.47it/s]Epoch: 5, train for the 118-th batch, train loss: 0.46422284841537476:  80%|████████  | 117/146 [00:19<00:06,  4.42it/s]Epoch: 3, train for the 130-th batch, train loss: 0.4907946288585663:  54%|█████▉     | 130/241 [00:21<00:20,  5.46it/s]Epoch: 4, train for the 149-th batch, train loss: 0.3878953754901886:  99%|██████████▊| 149/151 [00:27<00:00,  3.79it/s]Epoch: 5, train for the 118-th batch, train loss: 0.46422284841537476:  81%|████████  | 118/146 [00:19<00:06,  4.57it/s]Epoch: 3, train for the 131-th batch, train loss: 0.5470005869865417:  54%|█████▉     | 130/241 [00:21<00:20,  5.46it/s]Epoch: 3, train for the 131-th batch, train loss: 0.5470005869865417:  54%|█████▉     | 131/241 [00:21<00:19,  5.77it/s]evaluate for the 44-th batch, evaluate loss: 0.6655750870704651:  65%|███████████▋      | 43/66 [00:06<00:04,  5.73it/s]evaluate for the 44-th batch, evaluate loss: 0.6655750870704651:  67%|████████████      | 44/66 [00:06<00:04,  4.96it/s]evaluate for the 72-th batch, evaluate loss: 0.5632137656211853:  67%|███████████▍     | 71/106 [00:17<00:07,  4.94it/s]evaluate for the 72-th batch, evaluate loss: 0.5632137656211853:  68%|███████████▌     | 72/106 [00:17<00:08,  3.93it/s]Epoch: 6, train for the 63-th batch, train loss: 0.3458881676197052:  52%|██████▊      | 62/119 [00:10<00:10,  5.47it/s]Epoch: 6, train for the 63-th batch, train loss: 0.3458881676197052:  53%|██████▉      | 63/119 [00:10<00:10,  5.53it/s]Epoch: 4, train for the 150-th batch, train loss: 0.41101503372192383:  99%|█████████▊| 149/151 [00:27<00:00,  3.79it/s]Epoch: 5, train for the 119-th batch, train loss: 0.4716545045375824:  81%|████████▉  | 118/146 [00:20<00:06,  4.57it/s]Epoch: 5, train for the 119-th batch, train loss: 0.4716545045375824:  82%|████████▉  | 119/146 [00:20<00:05,  4.77it/s]Epoch: 4, train for the 150-th batch, train loss: 0.41101503372192383:  99%|█████████▉| 150/151 [00:27<00:00,  4.07it/s]evaluate for the 45-th batch, evaluate loss: 0.6321551203727722:  67%|████████████      | 44/66 [00:06<00:04,  4.96it/s]evaluate for the 45-th batch, evaluate loss: 0.6321551203727722:  68%|████████████▎     | 45/66 [00:06<00:03,  5.50it/s]Epoch: 3, train for the 132-th batch, train loss: 0.47528424859046936:  54%|█████▍    | 131/241 [00:21<00:19,  5.77it/s]Epoch: 6, train for the 64-th batch, train loss: 0.37304556369781494:  53%|██████▎     | 63/119 [00:10<00:10,  5.53it/s]Epoch: 6, train for the 64-th batch, train loss: 0.37304556369781494:  54%|██████▍     | 64/119 [00:10<00:09,  5.56it/s]Epoch: 3, train for the 132-th batch, train loss: 0.47528424859046936:  55%|█████▍    | 132/241 [00:21<00:20,  5.40it/s]evaluate for the 73-th batch, evaluate loss: 0.566377580165863:  68%|████████████▏     | 72/106 [00:18<00:08,  3.93it/s]evaluate for the 73-th batch, evaluate loss: 0.566377580165863:  69%|████████████▍     | 73/106 [00:18<00:07,  4.17it/s]Epoch: 5, train for the 120-th batch, train loss: 0.4862847328186035:  82%|████████▉  | 119/146 [00:20<00:05,  4.77it/s]Epoch: 5, train for the 120-th batch, train loss: 0.4862847328186035:  82%|█████████  | 120/146 [00:20<00:05,  4.82it/s]evaluate for the 46-th batch, evaluate loss: 0.6555032134056091:  68%|████████████▎     | 45/66 [00:06<00:03,  5.50it/s]evaluate for the 46-th batch, evaluate loss: 0.6555032134056091:  70%|████████████▌     | 46/66 [00:06<00:03,  5.46it/s]Epoch: 3, train for the 133-th batch, train loss: 0.38371166586875916:  55%|█████▍    | 132/241 [00:21<00:20,  5.40it/s]Epoch: 3, train for the 133-th batch, train loss: 0.38371166586875916:  55%|█████▌    | 133/241 [00:21<00:18,  5.90it/s]Epoch: 6, train for the 65-th batch, train loss: 0.4126755893230438:  54%|██████▉      | 64/119 [00:10<00:09,  5.56it/s]Epoch: 6, train for the 65-th batch, train loss: 0.4126755893230438:  55%|███████      | 65/119 [00:10<00:09,  5.71it/s]evaluate for the 47-th batch, evaluate loss: 0.6440516710281372:  70%|████████████▌     | 46/66 [00:06<00:03,  5.46it/s]Epoch: 3, train for the 134-th batch, train loss: 0.47251060605049133:  55%|█████▌    | 133/241 [00:21<00:18,  5.90it/s]Epoch: 6, train for the 66-th batch, train loss: 0.39218321442604065:  55%|██████▌     | 65/119 [00:11<00:09,  5.71it/s]Epoch: 6, train for the 66-th batch, train loss: 0.39218321442604065:  55%|██████▋     | 66/119 [00:11<00:08,  6.44it/s]Epoch: 3, train for the 134-th batch, train loss: 0.47251060605049133:  56%|█████▌    | 134/241 [00:21<00:17,  6.26it/s]Epoch: 5, train for the 121-th batch, train loss: 0.49685150384902954:  82%|████████▏ | 120/146 [00:20<00:05,  4.82it/s]Epoch: 4, train for the 151-th batch, train loss: 0.4691298305988312:  99%|██████████▉| 150/151 [00:28<00:00,  4.07it/s]Epoch: 5, train for the 121-th batch, train loss: 0.49685150384902954:  83%|████████▎ | 121/146 [00:20<00:05,  4.37it/s]Epoch: 4, train for the 151-th batch, train loss: 0.4691298305988312: 100%|███████████| 151/151 [00:28<00:00,  3.12it/s]Epoch: 4, train for the 151-th batch, train loss: 0.4691298305988312: 100%|███████████| 151/151 [00:28<00:00,  5.32it/s]
evaluate for the 48-th batch, evaluate loss: 0.6761757731437683:  70%|████████████▌     | 46/66 [00:07<00:03,  5.46it/s]evaluate for the 48-th batch, evaluate loss: 0.6761757731437683:  73%|█████████████     | 48/66 [00:07<00:02,  6.28it/s]evaluate for the 74-th batch, evaluate loss: 0.6420354843139648:  69%|███████████▋     | 73/106 [00:18<00:07,  4.17it/s]evaluate for the 74-th batch, evaluate loss: 0.6420354843139648:  70%|███████████▊     | 74/106 [00:18<00:09,  3.55it/s]Epoch: 6, train for the 67-th batch, train loss: 0.4226123094558716:  55%|███████▏     | 66/119 [00:11<00:08,  6.44it/s]Epoch: 6, train for the 67-th batch, train loss: 0.4226123094558716:  56%|███████▎     | 67/119 [00:11<00:07,  6.65it/s]Epoch: 3, train for the 135-th batch, train loss: 0.37596654891967773:  56%|█████▌    | 134/241 [00:21<00:17,  6.26it/s]Epoch: 5, train for the 122-th batch, train loss: 0.5135329365730286:  83%|█████████  | 121/146 [00:20<00:05,  4.37it/s]  0%|                                                                                            | 0/46 [00:00<?, ?it/s]Epoch: 3, train for the 135-th batch, train loss: 0.37596654891967773:  56%|█████▌    | 135/241 [00:21<00:18,  5.74it/s]Epoch: 5, train for the 122-th batch, train loss: 0.5135329365730286:  84%|█████████▏ | 122/146 [00:20<00:05,  4.74it/s]evaluate for the 49-th batch, evaluate loss: 0.653752863407135:  73%|█████████████▊     | 48/66 [00:07<00:02,  6.28it/s]evaluate for the 49-th batch, evaluate loss: 0.653752863407135:  74%|██████████████     | 49/66 [00:07<00:02,  6.56it/s]evaluate for the 75-th batch, evaluate loss: 0.5916734337806702:  70%|███████████▊     | 74/106 [00:18<00:09,  3.55it/s]evaluate for the 75-th batch, evaluate loss: 0.5916734337806702:  71%|████████████     | 75/106 [00:18<00:07,  4.10it/s]Epoch: 6, train for the 68-th batch, train loss: 0.3224509358406067:  56%|███████▎     | 67/119 [00:11<00:07,  6.65it/s]Epoch: 6, train for the 68-th batch, train loss: 0.3224509358406067:  57%|███████▍     | 68/119 [00:11<00:08,  6.37it/s]evaluate for the 1-th batch, evaluate loss: 0.6902915835380554:   0%|                            | 0/46 [00:00<?, ?it/s]Epoch: 5, train for the 123-th batch, train loss: 0.5039954781532288:  84%|█████████▏ | 122/146 [00:20<00:05,  4.74it/s]Epoch: 5, train for the 123-th batch, train loss: 0.5039954781532288:  84%|█████████▎ | 123/146 [00:20<00:04,  4.96it/s]Epoch: 3, train for the 136-th batch, train loss: 0.44912445545196533:  56%|█████▌    | 135/241 [00:22<00:18,  5.74it/s]evaluate for the 2-th batch, evaluate loss: 0.7034245729446411:   0%|                            | 0/46 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.7034245729446411:   4%|▊                   | 2/46 [00:00<00:04,  9.48it/s]Epoch: 3, train for the 136-th batch, train loss: 0.44912445545196533:  56%|█████▋    | 136/241 [00:22<00:19,  5.28it/s]Epoch: 6, train for the 69-th batch, train loss: 0.3768128752708435:  57%|███████▍     | 68/119 [00:11<00:08,  6.37it/s]Epoch: 6, train for the 69-th batch, train loss: 0.3768128752708435:  58%|███████▌     | 69/119 [00:11<00:07,  6.31it/s]evaluate for the 3-th batch, evaluate loss: 0.7281318306922913:   4%|▊                   | 2/46 [00:00<00:04,  9.48it/s]Epoch: 5, train for the 124-th batch, train loss: 0.5331212282180786:  84%|█████████▎ | 123/146 [00:21<00:04,  4.96it/s]Epoch: 5, train for the 124-th batch, train loss: 0.5331212282180786:  85%|█████████▎ | 124/146 [00:21<00:03,  5.56it/s]evaluate for the 50-th batch, evaluate loss: 0.6716164946556091:  74%|█████████████▎    | 49/66 [00:07<00:02,  6.56it/s]evaluate for the 50-th batch, evaluate loss: 0.6716164946556091:  76%|█████████████▋    | 50/66 [00:07<00:03,  4.81it/s]evaluate for the 76-th batch, evaluate loss: 0.6380808353424072:  71%|████████████     | 75/106 [00:18<00:07,  4.10it/s]evaluate for the 76-th batch, evaluate loss: 0.6380808353424072:  72%|████████████▏    | 76/106 [00:18<00:08,  3.67it/s]Epoch: 6, train for the 70-th batch, train loss: 0.3368106782436371:  58%|███████▌     | 69/119 [00:11<00:07,  6.31it/s]evaluate for the 4-th batch, evaluate loss: 0.6673879623413086:   4%|▊                   | 2/46 [00:00<00:04,  9.48it/s]evaluate for the 4-th batch, evaluate loss: 0.6673879623413086:   9%|█▋                  | 4/46 [00:00<00:04,  9.74it/s]Epoch: 3, train for the 137-th batch, train loss: 0.44686469435691833:  56%|█████▋    | 136/241 [00:22<00:19,  5.28it/s]Epoch: 6, train for the 70-th batch, train loss: 0.3368106782436371:  59%|███████▋     | 70/119 [00:11<00:07,  6.26it/s]Epoch: 3, train for the 137-th batch, train loss: 0.44686469435691833:  57%|█████▋    | 137/241 [00:22<00:20,  5.14it/s]Epoch: 5, train for the 125-th batch, train loss: 0.47274720668792725:  85%|████████▍ | 124/146 [00:21<00:03,  5.56it/s]evaluate for the 51-th batch, evaluate loss: 0.6414674520492554:  76%|█████████████▋    | 50/66 [00:07<00:03,  4.81it/s]evaluate for the 51-th batch, evaluate loss: 0.6414674520492554:  77%|█████████████▉    | 51/66 [00:07<00:02,  5.52it/s]Epoch: 5, train for the 125-th batch, train loss: 0.47274720668792725:  86%|████████▌ | 125/146 [00:21<00:03,  5.49it/s]evaluate for the 5-th batch, evaluate loss: 0.6855316758155823:   9%|█▋                  | 4/46 [00:00<00:04,  9.74it/s]evaluate for the 5-th batch, evaluate loss: 0.6855316758155823:  11%|██▏                 | 5/46 [00:00<00:04,  9.41it/s]evaluate for the 77-th batch, evaluate loss: 0.5651755332946777:  72%|████████████▏    | 76/106 [00:19<00:08,  3.67it/s]evaluate for the 77-th batch, evaluate loss: 0.5651755332946777:  73%|████████████▎    | 77/106 [00:19<00:06,  4.19it/s]Epoch: 6, train for the 71-th batch, train loss: 0.36592045426368713:  59%|███████     | 70/119 [00:11<00:07,  6.26it/s]Epoch: 6, train for the 71-th batch, train loss: 0.36592045426368713:  60%|███████▏    | 71/119 [00:11<00:07,  6.31it/s]Epoch: 3, train for the 138-th batch, train loss: 0.4037582278251648:  57%|██████▎    | 137/241 [00:22<00:20,  5.14it/s]evaluate for the 6-th batch, evaluate loss: 0.6379638314247131:  11%|██▏                 | 5/46 [00:00<00:04,  9.41it/s]Epoch: 3, train for the 138-th batch, train loss: 0.4037582278251648:  57%|██████▎    | 138/241 [00:22<00:20,  5.12it/s]evaluate for the 52-th batch, evaluate loss: 0.6340934634208679:  77%|█████████████▉    | 51/66 [00:07<00:02,  5.52it/s]evaluate for the 52-th batch, evaluate loss: 0.6340934634208679:  79%|██████████████▏   | 52/66 [00:07<00:02,  5.98it/s]Epoch: 5, train for the 126-th batch, train loss: 0.4835548996925354:  86%|█████████▍ | 125/146 [00:21<00:03,  5.49it/s]Epoch: 5, train for the 126-th batch, train loss: 0.4835548996925354:  86%|█████████▍ | 126/146 [00:21<00:03,  5.59it/s]evaluate for the 7-th batch, evaluate loss: 0.7387458682060242:  11%|██▏                 | 5/46 [00:00<00:04,  9.41it/s]evaluate for the 7-th batch, evaluate loss: 0.7387458682060242:  15%|███                 | 7/46 [00:00<00:03, 10.23it/s]Epoch: 6, train for the 72-th batch, train loss: 0.4210844039916992:  60%|███████▊     | 71/119 [00:12<00:07,  6.31it/s]evaluate for the 53-th batch, evaluate loss: 0.6090922951698303:  79%|██████████████▏   | 52/66 [00:07<00:02,  5.98it/s]evaluate for the 53-th batch, evaluate loss: 0.6090922951698303:  80%|██████████████▍   | 53/66 [00:08<00:01,  6.68it/s]Epoch: 6, train for the 72-th batch, train loss: 0.4210844039916992:  61%|███████▊     | 72/119 [00:12<00:07,  6.18it/s]Epoch: 3, train for the 139-th batch, train loss: 0.34175893664360046:  57%|█████▋    | 138/241 [00:22<00:20,  5.12it/s]evaluate for the 8-th batch, evaluate loss: 0.6757698059082031:  15%|███                 | 7/46 [00:00<00:03, 10.23it/s]Epoch: 5, train for the 127-th batch, train loss: 0.4962467551231384:  86%|█████████▍ | 126/146 [00:21<00:03,  5.59it/s]Epoch: 5, train for the 127-th batch, train loss: 0.4962467551231384:  87%|█████████▌ | 127/146 [00:21<00:03,  5.78it/s]Epoch: 3, train for the 139-th batch, train loss: 0.34175893664360046:  58%|█████▊    | 139/241 [00:22<00:20,  5.02it/s]Epoch: 6, train for the 73-th batch, train loss: 0.3532736897468567:  61%|███████▊     | 72/119 [00:12<00:07,  6.18it/s]evaluate for the 9-th batch, evaluate loss: 0.710367739200592:  15%|███▏                 | 7/46 [00:00<00:03, 10.23it/s]evaluate for the 9-th batch, evaluate loss: 0.710367739200592:  20%|████                 | 9/46 [00:00<00:03, 10.44it/s]Epoch: 6, train for the 73-th batch, train loss: 0.3532736897468567:  61%|███████▉     | 73/119 [00:12<00:07,  6.35it/s]Epoch: 5, train for the 128-th batch, train loss: 0.44850409030914307:  87%|████████▋ | 127/146 [00:21<00:03,  5.78it/s]Epoch: 3, train for the 140-th batch, train loss: 0.3577709197998047:  58%|██████▎    | 139/241 [00:22<00:20,  5.02it/s]Epoch: 5, train for the 128-th batch, train loss: 0.44850409030914307:  88%|████████▊ | 128/146 [00:21<00:02,  6.10it/s]evaluate for the 10-th batch, evaluate loss: 0.6275119185447693:  20%|███▋               | 9/46 [00:00<00:03, 10.44it/s]Epoch: 3, train for the 140-th batch, train loss: 0.3577709197998047:  58%|██████▍    | 140/241 [00:22<00:18,  5.39it/s]Epoch: 6, train for the 74-th batch, train loss: 0.39730384945869446:  61%|███████▎    | 73/119 [00:12<00:07,  6.35it/s]Epoch: 6, train for the 74-th batch, train loss: 0.39730384945869446:  62%|███████▍    | 74/119 [00:12<00:06,  6.52it/s]evaluate for the 11-th batch, evaluate loss: 0.7321511507034302:  20%|███▋               | 9/46 [00:01<00:03, 10.44it/s]evaluate for the 11-th batch, evaluate loss: 0.7321511507034302:  24%|████▎             | 11/46 [00:01<00:03, 10.42it/s]evaluate for the 78-th batch, evaluate loss: 0.4429401159286499:  73%|████████████▎    | 77/106 [00:19<00:06,  4.19it/s]evaluate for the 78-th batch, evaluate loss: 0.4429401159286499:  74%|████████████▌    | 78/106 [00:19<00:09,  2.96it/s]evaluate for the 54-th batch, evaluate loss: 0.6838523745536804:  80%|██████████████▍   | 53/66 [00:08<00:01,  6.68it/s]evaluate for the 54-th batch, evaluate loss: 0.6838523745536804:  82%|██████████████▋   | 54/66 [00:08<00:02,  4.58it/s]Epoch: 5, train for the 129-th batch, train loss: 0.5300250053405762:  88%|█████████▋ | 128/146 [00:21<00:02,  6.10it/s]Epoch: 5, train for the 129-th batch, train loss: 0.5300250053405762:  88%|█████████▋ | 129/146 [00:21<00:02,  5.88it/s]Epoch: 3, train for the 141-th batch, train loss: 0.4980311393737793:  58%|██████▍    | 140/241 [00:23<00:18,  5.39it/s]evaluate for the 12-th batch, evaluate loss: 0.6371719837188721:  24%|████▎             | 11/46 [00:01<00:03, 10.42it/s]Epoch: 6, train for the 75-th batch, train loss: 0.348408579826355:  62%|████████▋     | 74/119 [00:12<00:06,  6.52it/s]Epoch: 3, train for the 141-th batch, train loss: 0.4980311393737793:  59%|██████▍    | 141/241 [00:23<00:19,  5.09it/s]Epoch: 6, train for the 75-th batch, train loss: 0.348408579826355:  63%|████████▊     | 75/119 [00:12<00:07,  6.15it/s]evaluate for the 55-th batch, evaluate loss: 0.5942530035972595:  82%|██████████████▋   | 54/66 [00:08<00:02,  4.58it/s]evaluate for the 55-th batch, evaluate loss: 0.5942530035972595:  83%|███████████████   | 55/66 [00:08<00:02,  5.38it/s]evaluate for the 13-th batch, evaluate loss: 0.6868116855621338:  24%|████▎             | 11/46 [00:01<00:03, 10.42it/s]evaluate for the 13-th batch, evaluate loss: 0.6868116855621338:  28%|█████             | 13/46 [00:01<00:03, 10.26it/s]Epoch: 5, train for the 130-th batch, train loss: 0.42770543694496155:  88%|████████▊ | 129/146 [00:22<00:02,  5.88it/s]Epoch: 5, train for the 130-th batch, train loss: 0.42770543694496155:  89%|████████▉ | 130/146 [00:22<00:02,  5.86it/s]evaluate for the 79-th batch, evaluate loss: 0.533896803855896:  74%|█████████████▏    | 78/106 [00:19<00:09,  2.96it/s]evaluate for the 79-th batch, evaluate loss: 0.533896803855896:  75%|█████████████▍    | 79/106 [00:19<00:08,  3.33it/s]evaluate for the 56-th batch, evaluate loss: 0.6362811326980591:  83%|███████████████   | 55/66 [00:08<00:02,  5.38it/s]evaluate for the 56-th batch, evaluate loss: 0.6362811326980591:  85%|███████████████▎  | 56/66 [00:08<00:01,  5.75it/s]Epoch: 6, train for the 76-th batch, train loss: 0.37267303466796875:  63%|███████▌    | 75/119 [00:12<00:07,  6.15it/s]Epoch: 3, train for the 142-th batch, train loss: 0.4261381924152374:  59%|██████▍    | 141/241 [00:23<00:19,  5.09it/s]evaluate for the 14-th batch, evaluate loss: 0.7059146165847778:  28%|█████             | 13/46 [00:01<00:03, 10.26it/s]Epoch: 6, train for the 76-th batch, train loss: 0.37267303466796875:  64%|███████▋    | 76/119 [00:12<00:07,  5.81it/s]Epoch: 3, train for the 142-th batch, train loss: 0.4261381924152374:  59%|██████▍    | 142/241 [00:23<00:20,  4.94it/s]Epoch: 5, train for the 131-th batch, train loss: 0.5011266469955444:  89%|█████████▊ | 130/146 [00:22<00:02,  5.86it/s]evaluate for the 15-th batch, evaluate loss: 0.673256516456604:  28%|█████▎             | 13/46 [00:01<00:03, 10.26it/s]evaluate for the 15-th batch, evaluate loss: 0.673256516456604:  33%|██████▏            | 15/46 [00:01<00:03, 10.06it/s]evaluate for the 57-th batch, evaluate loss: 0.688894510269165:  85%|████████████████   | 56/66 [00:08<00:01,  5.75it/s]evaluate for the 57-th batch, evaluate loss: 0.688894510269165:  86%|████████████████▍  | 57/66 [00:08<00:01,  6.38it/s]Epoch: 5, train for the 131-th batch, train loss: 0.5011266469955444:  90%|█████████▊ | 131/146 [00:22<00:02,  5.95it/s]evaluate for the 16-th batch, evaluate loss: 0.6718938946723938:  33%|█████▊            | 15/46 [00:01<00:03, 10.06it/s]Epoch: 6, train for the 77-th batch, train loss: 0.3688056766986847:  64%|████████▎    | 76/119 [00:12<00:07,  5.81it/s]Epoch: 6, train for the 77-th batch, train loss: 0.3688056766986847:  65%|████████▍    | 77/119 [00:12<00:07,  5.91it/s]Epoch: 5, train for the 132-th batch, train loss: 0.4496433138847351:  90%|█████████▊ | 131/146 [00:22<00:02,  5.95it/s]evaluate for the 17-th batch, evaluate loss: 0.5427045822143555:  33%|█████▊            | 15/46 [00:01<00:03, 10.06it/s]evaluate for the 17-th batch, evaluate loss: 0.5427045822143555:  37%|██████▋           | 17/46 [00:01<00:02, 10.78it/s]Epoch: 5, train for the 132-th batch, train loss: 0.4496433138847351:  90%|█████████▉ | 132/146 [00:22<00:02,  6.02it/s]evaluate for the 58-th batch, evaluate loss: 0.619874894618988:  86%|████████████████▍  | 57/66 [00:08<00:01,  6.38it/s]evaluate for the 58-th batch, evaluate loss: 0.619874894618988:  88%|████████████████▋  | 58/66 [00:08<00:01,  6.17it/s]evaluate for the 80-th batch, evaluate loss: 0.6340111494064331:  75%|████████████▋    | 79/106 [00:20<00:08,  3.33it/s]evaluate for the 80-th batch, evaluate loss: 0.6340111494064331:  75%|████████████▊    | 80/106 [00:20<00:08,  3.23it/s]Epoch: 6, train for the 78-th batch, train loss: 0.41152340173721313:  65%|███████▊    | 77/119 [00:13<00:07,  5.91it/s]Epoch: 6, train for the 78-th batch, train loss: 0.41152340173721313:  66%|███████▊    | 78/119 [00:13<00:06,  5.99it/s]Epoch: 3, train for the 143-th batch, train loss: 0.42138171195983887:  59%|█████▉    | 142/241 [00:23<00:20,  4.94it/s]evaluate for the 18-th batch, evaluate loss: 0.6470812559127808:  37%|██████▋           | 17/46 [00:01<00:02, 10.78it/s]Epoch: 3, train for the 143-th batch, train loss: 0.42138171195983887:  59%|█████▉    | 143/241 [00:23<00:23,  4.14it/s]Epoch: 5, train for the 133-th batch, train loss: 0.474497526884079:  90%|██████████▊ | 132/146 [00:22<00:02,  6.02it/s]evaluate for the 59-th batch, evaluate loss: 0.630838930606842:  88%|████████████████▋  | 58/66 [00:09<00:01,  6.17it/s]evaluate for the 59-th batch, evaluate loss: 0.630838930606842:  89%|████████████████▉  | 59/66 [00:09<00:01,  6.53it/s]Epoch: 5, train for the 133-th batch, train loss: 0.474497526884079:  91%|██████████▉ | 133/146 [00:22<00:02,  6.13it/s]evaluate for the 19-th batch, evaluate loss: 0.7105438113212585:  37%|██████▋           | 17/46 [00:01<00:02, 10.78it/s]evaluate for the 19-th batch, evaluate loss: 0.7105438113212585:  41%|███████▍          | 19/46 [00:01<00:02, 10.37it/s]evaluate for the 81-th batch, evaluate loss: 0.575461208820343:  75%|█████████████▌    | 80/106 [00:20<00:08,  3.23it/s]evaluate for the 81-th batch, evaluate loss: 0.575461208820343:  76%|█████████████▊    | 81/106 [00:20<00:06,  3.64it/s]Epoch: 6, train for the 79-th batch, train loss: 0.39527618885040283:  66%|███████▊    | 78/119 [00:13<00:06,  5.99it/s]Epoch: 6, train for the 79-th batch, train loss: 0.39527618885040283:  66%|███████▉    | 79/119 [00:13<00:06,  5.93it/s]evaluate for the 20-th batch, evaluate loss: 0.680486261844635:  41%|███████▊           | 19/46 [00:01<00:02, 10.37it/s]Epoch: 3, train for the 144-th batch, train loss: 0.5399243235588074:  59%|██████▌    | 143/241 [00:23<00:23,  4.14it/s]Epoch: 5, train for the 134-th batch, train loss: 0.5143977999687195:  91%|██████████ | 133/146 [00:22<00:02,  6.13it/s]Epoch: 3, train for the 144-th batch, train loss: 0.5399243235588074:  60%|██████▌    | 144/241 [00:23<00:22,  4.30it/s]Epoch: 5, train for the 134-th batch, train loss: 0.5143977999687195:  92%|██████████ | 134/146 [00:22<00:01,  6.05it/s]evaluate for the 21-th batch, evaluate loss: 0.7141801118850708:  41%|███████▍          | 19/46 [00:02<00:02, 10.37it/s]evaluate for the 21-th batch, evaluate loss: 0.7141801118850708:  46%|████████▏         | 21/46 [00:02<00:02, 10.74it/s]Epoch: 6, train for the 80-th batch, train loss: 0.3965926170349121:  66%|████████▋    | 79/119 [00:13<00:06,  5.93it/s]Epoch: 6, train for the 80-th batch, train loss: 0.3965926170349121:  67%|████████▋    | 80/119 [00:13<00:06,  6.03it/s]evaluate for the 60-th batch, evaluate loss: 0.6224871873855591:  89%|████████████████  | 59/66 [00:09<00:01,  6.53it/s]evaluate for the 60-th batch, evaluate loss: 0.6224871873855591:  91%|████████████████▎ | 60/66 [00:09<00:01,  5.30it/s]Epoch: 5, train for the 135-th batch, train loss: 0.43472734093666077:  92%|█████████▏| 134/146 [00:22<00:01,  6.05it/s]Epoch: 5, train for the 135-th batch, train loss: 0.43472734093666077:  92%|█████████▏| 135/146 [00:22<00:01,  5.97it/s]Epoch: 3, train for the 145-th batch, train loss: 0.5776659250259399:  60%|██████▌    | 144/241 [00:24<00:22,  4.30it/s]evaluate for the 22-th batch, evaluate loss: 0.6576241850852966:  46%|████████▏         | 21/46 [00:02<00:02, 10.74it/s]evaluate for the 61-th batch, evaluate loss: 0.6546149253845215:  91%|████████████████▎ | 60/66 [00:09<00:01,  5.30it/s]Epoch: 3, train for the 145-th batch, train loss: 0.5776659250259399:  60%|██████▌    | 145/241 [00:24<00:21,  4.48it/s]Epoch: 6, train for the 81-th batch, train loss: 0.33762019872665405:  67%|████████    | 80/119 [00:13<00:06,  6.03it/s]Epoch: 6, train for the 81-th batch, train loss: 0.33762019872665405:  68%|████████▏   | 81/119 [00:13<00:06,  6.27it/s]evaluate for the 82-th batch, evaluate loss: 0.5651710629463196:  76%|████████████▉    | 81/106 [00:20<00:06,  3.64it/s]evaluate for the 82-th batch, evaluate loss: 0.5651710629463196:  77%|█████████████▏   | 82/106 [00:20<00:07,  3.33it/s]evaluate for the 23-th batch, evaluate loss: 0.6908968687057495:  46%|████████▏         | 21/46 [00:02<00:02, 10.74it/s]evaluate for the 23-th batch, evaluate loss: 0.6908968687057495:  50%|█████████         | 23/46 [00:02<00:02, 10.06it/s]Epoch: 5, train for the 136-th batch, train loss: 0.4862728714942932:  92%|██████████▏| 135/146 [00:23<00:01,  5.97it/s]Epoch: 3, train for the 146-th batch, train loss: 0.550411581993103:  60%|███████▏    | 145/241 [00:24<00:21,  4.48it/s]evaluate for the 62-th batch, evaluate loss: 0.6603033542633057:  91%|████████████████▎ | 60/66 [00:09<00:01,  5.30it/s]evaluate for the 62-th batch, evaluate loss: 0.6603033542633057:  94%|████████████████▉ | 62/66 [00:09<00:00,  6.26it/s]evaluate for the 24-th batch, evaluate loss: 0.6054514050483704:  50%|█████████         | 23/46 [00:02<00:02, 10.06it/s]Epoch: 5, train for the 136-th batch, train loss: 0.4862728714942932:  93%|██████████▏| 136/146 [00:23<00:01,  5.86it/s]Epoch: 3, train for the 146-th batch, train loss: 0.550411581993103:  61%|███████▎    | 146/241 [00:24<00:19,  4.85it/s]Epoch: 6, train for the 82-th batch, train loss: 0.3860284686088562:  68%|████████▊    | 81/119 [00:13<00:06,  6.27it/s]Epoch: 6, train for the 82-th batch, train loss: 0.3860284686088562:  69%|████████▉    | 82/119 [00:13<00:05,  6.19it/s]evaluate for the 83-th batch, evaluate loss: 0.6134037971496582:  77%|█████████████▏   | 82/106 [00:20<00:07,  3.33it/s]evaluate for the 83-th batch, evaluate loss: 0.6134037971496582:  78%|█████████████▎   | 83/106 [00:20<00:06,  3.63it/s]evaluate for the 25-th batch, evaluate loss: 0.6746176481246948:  50%|█████████         | 23/46 [00:02<00:02, 10.06it/s]evaluate for the 25-th batch, evaluate loss: 0.6746176481246948:  54%|█████████▊        | 25/46 [00:02<00:02, 10.15it/s]evaluate for the 63-th batch, evaluate loss: 0.5717377066612244:  94%|████████████████▉ | 62/66 [00:09<00:00,  6.26it/s]evaluate for the 63-th batch, evaluate loss: 0.5717377066612244:  95%|█████████████████▏| 63/66 [00:09<00:00,  6.27it/s]Epoch: 5, train for the 137-th batch, train loss: 0.4347372353076935:  93%|██████████▏| 136/146 [00:23<00:01,  5.86it/s]Epoch: 5, train for the 137-th batch, train loss: 0.4347372353076935:  94%|██████████▎| 137/146 [00:23<00:01,  5.81it/s]Epoch: 3, train for the 147-th batch, train loss: 0.5855157971382141:  61%|██████▋    | 146/241 [00:24<00:19,  4.85it/s]evaluate for the 26-th batch, evaluate loss: 0.7263991236686707:  54%|█████████▊        | 25/46 [00:02<00:02, 10.15it/s]Epoch: 3, train for the 147-th batch, train loss: 0.5855157971382141:  61%|██████▋    | 147/241 [00:24<00:19,  4.80it/s]Epoch: 6, train for the 83-th batch, train loss: 0.3985275328159332:  69%|████████▉    | 82/119 [00:13<00:05,  6.19it/s]Epoch: 6, train for the 83-th batch, train loss: 0.3985275328159332:  70%|█████████    | 83/119 [00:13<00:06,  5.72it/s]evaluate for the 27-th batch, evaluate loss: 0.6802464127540588:  54%|█████████▊        | 25/46 [00:02<00:02, 10.15it/s]evaluate for the 27-th batch, evaluate loss: 0.6802464127540588:  59%|██████████▌       | 27/46 [00:02<00:01, 10.49it/s]Epoch: 5, train for the 138-th batch, train loss: 0.47244855761528015:  94%|█████████▍| 137/146 [00:23<00:01,  5.81it/s]Epoch: 5, train for the 138-th batch, train loss: 0.47244855761528015:  95%|█████████▍| 138/146 [00:23<00:01,  6.31it/s]Epoch: 3, train for the 148-th batch, train loss: 0.5307872891426086:  61%|██████▋    | 147/241 [00:24<00:19,  4.80it/s]evaluate for the 28-th batch, evaluate loss: 0.6485258340835571:  59%|██████████▌       | 27/46 [00:02<00:01, 10.49it/s]Epoch: 3, train for the 148-th batch, train loss: 0.5307872891426086:  61%|██████▊    | 148/241 [00:24<00:18,  5.01it/s]Epoch: 6, train for the 84-th batch, train loss: 0.38638946413993835:  70%|████████▎   | 83/119 [00:14<00:06,  5.72it/s]Epoch: 6, train for the 84-th batch, train loss: 0.38638946413993835:  71%|████████▍   | 84/119 [00:14<00:05,  5.84it/s]Epoch: 5, train for the 139-th batch, train loss: 0.4855630397796631:  95%|██████████▍| 138/146 [00:23<00:01,  6.31it/s]evaluate for the 64-th batch, evaluate loss: 0.5889139175415039:  95%|█████████████████▏| 63/66 [00:10<00:00,  6.27it/s]evaluate for the 64-th batch, evaluate loss: 0.5889139175415039:  97%|█████████████████▍| 64/66 [00:10<00:00,  4.97it/s]evaluate for the 29-th batch, evaluate loss: 0.6452956795692444:  59%|██████████▌       | 27/46 [00:02<00:01, 10.49it/s]evaluate for the 29-th batch, evaluate loss: 0.6452956795692444:  63%|███████████▎      | 29/46 [00:02<00:01, 10.61it/s]Epoch: 5, train for the 139-th batch, train loss: 0.4855630397796631:  95%|██████████▍| 139/146 [00:23<00:01,  6.13it/s]evaluate for the 84-th batch, evaluate loss: 0.6253139972686768:  78%|█████████████▎   | 83/106 [00:21<00:06,  3.63it/s]evaluate for the 84-th batch, evaluate loss: 0.6253139972686768:  79%|█████████████▍   | 84/106 [00:21<00:06,  3.26it/s]Epoch: 6, train for the 85-th batch, train loss: 0.34741324186325073:  71%|████████▍   | 84/119 [00:14<00:05,  5.84it/s]Epoch: 3, train for the 149-th batch, train loss: 0.3936924338340759:  61%|██████▊    | 148/241 [00:24<00:18,  5.01it/s]Epoch: 6, train for the 85-th batch, train loss: 0.34741324186325073:  71%|████████▌   | 85/119 [00:14<00:05,  5.86it/s]evaluate for the 30-th batch, evaluate loss: 0.7042233943939209:  63%|███████████▎      | 29/46 [00:02<00:01, 10.61it/s]Epoch: 3, train for the 149-th batch, train loss: 0.3936924338340759:  62%|██████▊    | 149/241 [00:24<00:18,  5.00it/s]Epoch: 5, train for the 140-th batch, train loss: 0.49145540595054626:  95%|█████████▌| 139/146 [00:23<00:01,  6.13it/s]Epoch: 5, train for the 140-th batch, train loss: 0.49145540595054626:  96%|█████████▌| 140/146 [00:23<00:00,  6.13it/s]evaluate for the 85-th batch, evaluate loss: 0.6113827228546143:  79%|█████████████▍   | 84/106 [00:21<00:06,  3.26it/s]evaluate for the 85-th batch, evaluate loss: 0.6113827228546143:  80%|█████████████▋   | 85/106 [00:21<00:05,  3.65it/s]evaluate for the 31-th batch, evaluate loss: 0.5242773294448853:  63%|███████████▎      | 29/46 [00:03<00:01, 10.61it/s]evaluate for the 31-th batch, evaluate loss: 0.5242773294448853:  67%|████████████▏     | 31/46 [00:03<00:01,  9.86it/s]Epoch: 6, train for the 86-th batch, train loss: 0.36836814880371094:  71%|████████▌   | 85/119 [00:14<00:05,  5.86it/s]Epoch: 6, train for the 86-th batch, train loss: 0.36836814880371094:  72%|████████▋   | 86/119 [00:14<00:05,  5.99it/s]Epoch: 3, train for the 150-th batch, train loss: 0.38464605808258057:  62%|██████▏   | 149/241 [00:25<00:18,  5.00it/s]Epoch: 5, train for the 141-th batch, train loss: 0.4485190212726593:  96%|██████████▌| 140/146 [00:23<00:00,  6.13it/s]evaluate for the 32-th batch, evaluate loss: 0.6149057149887085:  67%|████████████▏     | 31/46 [00:03<00:01,  9.86it/s]Epoch: 5, train for the 141-th batch, train loss: 0.4485190212726593:  97%|██████████▌| 141/146 [00:23<00:00,  6.29it/s]Epoch: 3, train for the 150-th batch, train loss: 0.38464605808258057:  62%|██████▏   | 150/241 [00:25<00:18,  5.02it/s]evaluate for the 65-th batch, evaluate loss: 0.696829617023468:  97%|██████████████████▍| 64/66 [00:10<00:00,  4.97it/s]evaluate for the 65-th batch, evaluate loss: 0.696829617023468:  98%|██████████████████▋| 65/66 [00:10<00:00,  4.03it/s]evaluate for the 33-th batch, evaluate loss: 0.6429039835929871:  67%|████████████▏     | 31/46 [00:03<00:01,  9.86it/s]evaluate for the 33-th batch, evaluate loss: 0.6429039835929871:  72%|████████████▉     | 33/46 [00:03<00:01, 10.28it/s]Epoch: 6, train for the 87-th batch, train loss: 0.3992193937301636:  72%|█████████▍   | 86/119 [00:14<00:05,  5.99it/s]Epoch: 6, train for the 87-th batch, train loss: 0.3992193937301636:  73%|█████████▌   | 87/119 [00:14<00:05,  6.15it/s]Epoch: 5, train for the 142-th batch, train loss: 0.45469599962234497:  97%|█████████▋| 141/146 [00:24<00:00,  6.29it/s]evaluate for the 66-th batch, evaluate loss: 0.6373367309570312:  98%|█████████████████▋| 65/66 [00:10<00:00,  4.03it/s]evaluate for the 66-th batch, evaluate loss: 0.6373367309570312: 100%|██████████████████| 66/66 [00:10<00:00,  4.83it/s]evaluate for the 66-th batch, evaluate loss: 0.6373367309570312: 100%|██████████████████| 66/66 [00:10<00:00,  6.26it/s]
Epoch: 5, train for the 142-th batch, train loss: 0.45469599962234497:  97%|█████████▋| 142/146 [00:24<00:00,  6.27it/s]Epoch: 3, train for the 151-th batch, train loss: 0.3290129005908966:  62%|██████▊    | 150/241 [00:25<00:18,  5.02it/s]evaluate for the 34-th batch, evaluate loss: 0.5444541573524475:  72%|████████████▉     | 33/46 [00:03<00:01, 10.28it/s]Epoch: 3, train for the 151-th batch, train loss: 0.3290129005908966:  63%|██████▉    | 151/241 [00:25<00:17,  5.15it/s]Epoch: 6, train for the 88-th batch, train loss: 0.41060641407966614:  73%|████████▊   | 87/119 [00:14<00:05,  6.15it/s]Epoch: 6, train for the 88-th batch, train loss: 0.41060641407966614:  74%|████████▊   | 88/119 [00:14<00:04,  6.33it/s]evaluate for the 35-th batch, evaluate loss: 0.6561344861984253:  72%|████████████▉     | 33/46 [00:03<00:01, 10.28it/s]evaluate for the 35-th batch, evaluate loss: 0.6561344861984253:  76%|█████████████▋    | 35/46 [00:03<00:01, 10.32it/s]Epoch: 5, train for the 143-th batch, train loss: 0.4696083664894104:  97%|██████████▋| 142/146 [00:24<00:00,  6.27it/s]evaluate for the 86-th batch, evaluate loss: 0.6102245450019836:  80%|█████████████▋   | 85/106 [00:21<00:05,  3.65it/s]evaluate for the 86-th batch, evaluate loss: 0.6102245450019836:  81%|█████████████▊   | 86/106 [00:21<00:06,  3.15it/s]Epoch: 3, train for the 152-th batch, train loss: 0.30252063274383545:  63%|██████▎   | 151/241 [00:25<00:17,  5.15it/s]Epoch: 5, train for the 143-th batch, train loss: 0.4696083664894104:  98%|██████████▊| 143/146 [00:24<00:00,  6.13it/s]Epoch: 3, train for the 152-th batch, train loss: 0.30252063274383545:  63%|██████▎   | 152/241 [00:25<00:16,  5.34it/s]evaluate for the 36-th batch, evaluate loss: 0.6027213335037231:  76%|█████████████▋    | 35/46 [00:03<00:01, 10.32it/s]Epoch: 6, train for the 89-th batch, train loss: 0.42396166920661926:  74%|████████▊   | 88/119 [00:14<00:04,  6.33it/s]Epoch: 6, train for the 89-th batch, train loss: 0.42396166920661926:  75%|████████▉   | 89/119 [00:14<00:04,  6.50it/s]evaluate for the 37-th batch, evaluate loss: 0.700420081615448:  76%|██████████████▍    | 35/46 [00:03<00:01, 10.32it/s]evaluate for the 37-th batch, evaluate loss: 0.700420081615448:  80%|███████████████▎   | 37/46 [00:03<00:00, 10.69it/s]Epoch: 5, train for the 144-th batch, train loss: 0.4060823619365692:  98%|██████████▊| 143/146 [00:24<00:00,  6.13it/s]evaluate for the 87-th batch, evaluate loss: 0.5714490413665771:  81%|█████████████▊   | 86/106 [00:22<00:06,  3.15it/s]evaluate for the 87-th batch, evaluate loss: 0.5714490413665771:  82%|█████████████▉   | 87/106 [00:22<00:05,  3.67it/s]Epoch: 5, train for the 144-th batch, train loss: 0.4060823619365692:  99%|██████████▊| 144/146 [00:24<00:00,  6.22it/s]Epoch: 6, train for the 90-th batch, train loss: 0.4166179597377777:  75%|█████████▋   | 89/119 [00:14<00:04,  6.50it/s]Epoch: 3, train for the 153-th batch, train loss: 0.3640540540218353:  63%|██████▉    | 152/241 [00:25<00:16,  5.34it/s]evaluate for the 38-th batch, evaluate loss: 0.595524251461029:  80%|███████████████▎   | 37/46 [00:03<00:00, 10.69it/s]Epoch: 6, train for the 90-th batch, train loss: 0.4166179597377777:  76%|█████████▊   | 90/119 [00:14<00:04,  6.53it/s]Epoch: 3, train for the 153-th batch, train loss: 0.3640540540218353:  63%|██████▉    | 153/241 [00:25<00:16,  5.23it/s]evaluate for the 39-th batch, evaluate loss: 0.6436244249343872:  80%|██████████████▍   | 37/46 [00:03<00:00, 10.69it/s]evaluate for the 39-th batch, evaluate loss: 0.6436244249343872:  85%|███████████████▎  | 39/46 [00:03<00:00, 11.16it/s]Epoch: 5, train for the 145-th batch, train loss: 0.47162193059921265:  99%|█████████▊| 144/146 [00:24<00:00,  6.22it/s]Epoch: 5, train for the 145-th batch, train loss: 0.47162193059921265:  99%|█████████▉| 145/146 [00:24<00:00,  6.24it/s]Epoch: 6, train for the 91-th batch, train loss: 0.37691089510917664:  76%|█████████   | 90/119 [00:15<00:04,  6.53it/s]Epoch: 6, train for the 91-th batch, train loss: 0.37691089510917664:  76%|█████████▏  | 91/119 [00:15<00:04,  6.62it/s]Epoch: 3, train for the 154-th batch, train loss: 0.26787036657333374:  63%|██████▎   | 153/241 [00:25<00:16,  5.23it/s]evaluate for the 40-th batch, evaluate loss: 0.6252523064613342:  85%|███████████████▎  | 39/46 [00:03<00:00, 11.16it/s]Epoch: 3, train for the 154-th batch, train loss: 0.26787036657333374:  64%|██████▍   | 154/241 [00:25<00:16,  5.31it/s]Epoch: 5, train for the 146-th batch, train loss: 0.42581403255462646:  99%|█████████▉| 145/146 [00:24<00:00,  6.24it/s]Epoch: 5, train for the 146-th batch, train loss: 0.42581403255462646: 100%|██████████| 146/146 [00:24<00:00,  6.58it/s]Epoch: 5, train for the 146-th batch, train loss: 0.42581403255462646: 100%|██████████| 146/146 [00:24<00:00,  5.92it/s]
evaluate for the 41-th batch, evaluate loss: 0.6497362852096558:  85%|███████████████▎  | 39/46 [00:03<00:00, 11.16it/s]evaluate for the 41-th batch, evaluate loss: 0.6497362852096558:  89%|████████████████  | 41/46 [00:03<00:00, 11.25it/s]evaluate for the 88-th batch, evaluate loss: 0.7064850926399231:  82%|█████████████▉   | 87/106 [00:22<00:05,  3.67it/s]evaluate for the 88-th batch, evaluate loss: 0.7064850926399231:  83%|██████████████   | 88/106 [00:22<00:05,  3.39it/s]evaluate for the 42-th batch, evaluate loss: 0.5839082598686218:  89%|████████████████  | 41/46 [00:04<00:00, 11.25it/s]Epoch: 3, train for the 155-th batch, train loss: 0.3064471483230591:  64%|███████    | 154/241 [00:25<00:16,  5.31it/s]Epoch: 6, train for the 92-th batch, train loss: 0.3891710638999939:  76%|█████████▉   | 91/119 [00:15<00:04,  6.62it/s]Epoch: 3, train for the 155-th batch, train loss: 0.3064471483230591:  64%|███████    | 155/241 [00:25<00:16,  5.24it/s]Epoch: 6, train for the 92-th batch, train loss: 0.3891710638999939:  77%|██████████   | 92/119 [00:15<00:04,  5.48it/s]evaluate for the 89-th batch, evaluate loss: 0.6063863039016724:  83%|██████████████   | 88/106 [00:22<00:05,  3.39it/s]evaluate for the 89-th batch, evaluate loss: 0.6063863039016724:  84%|██████████████▎  | 89/106 [00:22<00:04,  4.03it/s]evaluate for the 43-th batch, evaluate loss: 0.6818956732749939:  89%|████████████████  | 41/46 [00:04<00:00, 11.25it/s]evaluate for the 43-th batch, evaluate loss: 0.6818956732749939:  93%|████████████████▊ | 43/46 [00:04<00:00, 10.92it/s]  0%|                                                                                            | 0/38 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.7519330978393555:   0%|                            | 0/38 [00:00<?, ?it/s]evaluate for the 44-th batch, evaluate loss: 0.5842119455337524:  93%|████████████████▊ | 43/46 [00:04<00:00, 10.92it/s]Epoch: 3, train for the 156-th batch, train loss: 0.264079749584198:  64%|███████▋    | 155/241 [00:26<00:16,  5.24it/s]Epoch: 3, train for the 156-th batch, train loss: 0.264079749584198:  65%|███████▊    | 156/241 [00:26<00:16,  5.10it/s]evaluate for the 2-th batch, evaluate loss: 0.6882907152175903:   0%|                            | 0/38 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.6882907152175903:   5%|█                   | 2/38 [00:00<00:02, 12.34it/s]evaluate for the 45-th batch, evaluate loss: 0.581523597240448:  93%|█████████████████▊ | 43/46 [00:04<00:00, 10.92it/s]evaluate for the 45-th batch, evaluate loss: 0.581523597240448:  98%|██████████████████▌| 45/46 [00:04<00:00, 11.07it/s]Epoch: 6, train for the 93-th batch, train loss: 0.3275197446346283:  77%|██████████   | 92/119 [00:15<00:04,  5.48it/s]Epoch: 6, train for the 93-th batch, train loss: 0.3275197446346283:  78%|██████████▏  | 93/119 [00:15<00:05,  5.08it/s]evaluate for the 3-th batch, evaluate loss: 0.6356624960899353:   5%|█                   | 2/38 [00:00<00:02, 12.34it/s]evaluate for the 46-th batch, evaluate loss: 0.5257779359817505:  98%|█████████████████▌| 45/46 [00:04<00:00, 11.07it/s]evaluate for the 46-th batch, evaluate loss: 0.5257779359817505: 100%|██████████████████| 46/46 [00:04<00:00, 10.54it/s]
evaluate for the 4-th batch, evaluate loss: 0.61918044090271:   5%|█▏                    | 2/38 [00:00<00:02, 12.34it/s]evaluate for the 4-th batch, evaluate loss: 0.61918044090271:  11%|██▎                   | 4/38 [00:00<00:02, 14.07it/s]evaluate for the 90-th batch, evaluate loss: 0.48556798696517944:  84%|█████████████▍  | 89/106 [00:22<00:04,  4.03it/s]evaluate for the 90-th batch, evaluate loss: 0.48556798696517944:  85%|█████████████▌  | 90/106 [00:22<00:04,  3.67it/s]Epoch: 3, train for the 157-th batch, train loss: 0.24801795184612274:  65%|██████▍   | 156/241 [00:26<00:16,  5.10it/s]Epoch: 3, train for the 157-th batch, train loss: 0.24801795184612274:  65%|██████▌   | 157/241 [00:26<00:15,  5.32it/s]Epoch: 6, train for the 94-th batch, train loss: 0.32437247037887573:  78%|█████████▍  | 93/119 [00:15<00:05,  5.08it/s]evaluate for the 5-th batch, evaluate loss: 0.6832196712493896:  11%|██                  | 4/38 [00:00<00:02, 14.07it/s]Epoch: 6, train for the 94-th batch, train loss: 0.32437247037887573:  79%|█████████▍  | 94/119 [00:15<00:04,  5.22it/s]evaluate for the 6-th batch, evaluate loss: 0.6574651002883911:  11%|██                  | 4/38 [00:00<00:02, 14.07it/s]evaluate for the 6-th batch, evaluate loss: 0.6574651002883911:  16%|███▏                | 6/38 [00:00<00:02, 13.46it/s]evaluate for the 91-th batch, evaluate loss: 0.5092459321022034:  85%|██████████████▍  | 90/106 [00:23<00:04,  3.67it/s]evaluate for the 91-th batch, evaluate loss: 0.5092459321022034:  86%|██████████████▌  | 91/106 [00:23<00:03,  3.97it/s]Epoch: 3, train for the 158-th batch, train loss: 0.22648879885673523:  65%|██████▌   | 157/241 [00:26<00:15,  5.32it/s]Epoch: 3, train for the 158-th batch, train loss: 0.22648879885673523:  66%|██████▌   | 158/241 [00:26<00:16,  5.18it/s]Epoch: 6, train for the 95-th batch, train loss: 0.30360159277915955:  79%|█████████▍  | 94/119 [00:15<00:04,  5.22it/s]evaluate for the 7-th batch, evaluate loss: 0.6315308213233948:  16%|███▏                | 6/38 [00:00<00:02, 13.46it/s]Epoch: 6, train for the 95-th batch, train loss: 0.30360159277915955:  80%|█████████▌  | 95/119 [00:15<00:04,  5.29it/s]evaluate for the 8-th batch, evaluate loss: 0.6189268827438354:  16%|███▏                | 6/38 [00:00<00:02, 13.46it/s]evaluate for the 8-th batch, evaluate loss: 0.6189268827438354:  21%|████▏               | 8/38 [00:00<00:02, 12.83it/s]  0%|                                                                                            | 0/40 [00:00<?, ?it/s]  0%|                                                                                            | 0/25 [00:00<?, ?it/s]Epoch: 3, train for the 159-th batch, train loss: 0.20633617043495178:  66%|██████▌   | 158/241 [00:26<00:16,  5.18it/s]evaluate for the 9-th batch, evaluate loss: 0.6612731218338013:  21%|████▏               | 8/38 [00:00<00:02, 12.83it/s]evaluate for the 1-th batch, evaluate loss: 0.9243717789649963:   0%|                            | 0/40 [00:00<?, ?it/s]Epoch: 3, train for the 159-th batch, train loss: 0.20633617043495178:  66%|██████▌   | 159/241 [00:26<00:15,  5.29it/s]Epoch: 6, train for the 96-th batch, train loss: 0.390144020318985:  80%|███████████▏  | 95/119 [00:16<00:04,  5.29it/s]evaluate for the 1-th batch, evaluate loss: 1.0993622541427612:   0%|                            | 0/25 [00:00<?, ?it/s]Epoch: 6, train for the 96-th batch, train loss: 0.390144020318985:  81%|███████████▎  | 96/119 [00:16<00:04,  5.41it/s]evaluate for the 10-th batch, evaluate loss: 0.6208544969558716:  21%|████               | 8/38 [00:00<00:02, 12.83it/s]evaluate for the 10-th batch, evaluate loss: 0.6208544969558716:  26%|████▋             | 10/38 [00:00<00:02, 13.11it/s]evaluate for the 2-th batch, evaluate loss: 1.073720097541809:   0%|                             | 0/25 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.9585533738136292:   0%|                            | 0/40 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 1.073720097541809:   8%|█▋                   | 2/25 [00:00<00:01, 12.42it/s]evaluate for the 2-th batch, evaluate loss: 0.9585533738136292:   5%|█                   | 2/40 [00:00<00:03, 10.76it/s]evaluate for the 11-th batch, evaluate loss: 0.5221993327140808:  26%|████▋             | 10/38 [00:00<00:02, 13.11it/s]Epoch: 3, train for the 160-th batch, train loss: 0.5578824877738953:  66%|███████▎   | 159/241 [00:26<00:15,  5.29it/s]Epoch: 3, train for the 160-th batch, train loss: 0.5578824877738953:  66%|███████▎   | 160/241 [00:26<00:15,  5.23it/s]evaluate for the 3-th batch, evaluate loss: 1.0747946500778198:   8%|█▌                  | 2/25 [00:00<00:01, 12.42it/s]Epoch: 6, train for the 97-th batch, train loss: 0.3481409251689911:  81%|██████████▍  | 96/119 [00:16<00:04,  5.41it/s]evaluate for the 92-th batch, evaluate loss: 0.5141271352767944:  86%|██████████████▌  | 91/106 [00:23<00:03,  3.97it/s]evaluate for the 92-th batch, evaluate loss: 0.5141271352767944:  87%|██████████████▊  | 92/106 [00:23<00:04,  3.36it/s]evaluate for the 12-th batch, evaluate loss: 0.6274080872535706:  26%|████▋             | 10/38 [00:00<00:02, 13.11it/s]evaluate for the 12-th batch, evaluate loss: 0.6274080872535706:  32%|█████▋            | 12/38 [00:00<00:01, 13.00it/s]Epoch: 6, train for the 97-th batch, train loss: 0.3481409251689911:  82%|██████████▌  | 97/119 [00:16<00:04,  5.36it/s]evaluate for the 3-th batch, evaluate loss: 0.915211021900177:   5%|█                    | 2/40 [00:00<00:03, 10.76it/s]evaluate for the 13-th batch, evaluate loss: 0.6223575472831726:  32%|█████▋            | 12/38 [00:00<00:01, 13.00it/s]evaluate for the 4-th batch, evaluate loss: 1.0186201333999634:   8%|█▌                  | 2/25 [00:00<00:01, 12.42it/s]evaluate for the 4-th batch, evaluate loss: 1.0186201333999634:  16%|███▏                | 4/25 [00:00<00:01, 10.77it/s]Epoch: 3, train for the 161-th batch, train loss: 0.3234221637248993:  66%|███████▎   | 160/241 [00:27<00:15,  5.23it/s]evaluate for the 14-th batch, evaluate loss: 0.558087944984436:  32%|██████             | 12/38 [00:01<00:01, 13.00it/s]evaluate for the 14-th batch, evaluate loss: 0.558087944984436:  37%|███████            | 14/38 [00:01<00:01, 12.72it/s]evaluate for the 4-th batch, evaluate loss: 0.9391164779663086:   5%|█                   | 2/40 [00:00<00:03, 10.76it/s]evaluate for the 4-th batch, evaluate loss: 0.9391164779663086:  10%|██                  | 4/40 [00:00<00:04,  8.59it/s]Epoch: 3, train for the 161-th batch, train loss: 0.3234221637248993:  67%|███████▎   | 161/241 [00:27<00:15,  5.23it/s]evaluate for the 5-th batch, evaluate loss: 0.968901515007019:  16%|███▎                 | 4/25 [00:00<00:01, 10.77it/s]Epoch: 6, train for the 98-th batch, train loss: 0.3358854651451111:  82%|██████████▌  | 97/119 [00:16<00:04,  5.36it/s]evaluate for the 93-th batch, evaluate loss: 0.6114255785942078:  87%|██████████████▊  | 92/106 [00:23<00:04,  3.36it/s]evaluate for the 93-th batch, evaluate loss: 0.6114255785942078:  88%|██████████████▉  | 93/106 [00:23<00:03,  3.73it/s]Epoch: 6, train for the 98-th batch, train loss: 0.3358854651451111:  82%|██████████▋  | 98/119 [00:16<00:04,  5.10it/s]evaluate for the 15-th batch, evaluate loss: 0.5452988147735596:  37%|██████▋           | 14/38 [00:01<00:01, 12.72it/s]evaluate for the 6-th batch, evaluate loss: 1.021932601928711:  16%|███▎                 | 4/25 [00:00<00:01, 10.77it/s]evaluate for the 6-th batch, evaluate loss: 1.021932601928711:  24%|█████                | 6/25 [00:00<00:01, 10.70it/s]evaluate for the 5-th batch, evaluate loss: 0.9001458287239075:  10%|██                  | 4/40 [00:00<00:04,  8.59it/s]evaluate for the 5-th batch, evaluate loss: 0.9001458287239075:  12%|██▌                 | 5/40 [00:00<00:04,  8.30it/s]evaluate for the 16-th batch, evaluate loss: 0.6010968685150146:  37%|██████▋           | 14/38 [00:01<00:01, 12.72it/s]evaluate for the 16-th batch, evaluate loss: 0.6010968685150146:  42%|███████▌          | 16/38 [00:01<00:01, 12.70it/s]Epoch: 3, train for the 162-th batch, train loss: 0.17643505334854126:  67%|██████▋   | 161/241 [00:27<00:15,  5.23it/s]Epoch: 3, train for the 162-th batch, train loss: 0.17643505334854126:  67%|██████▋   | 162/241 [00:27<00:14,  5.41it/s]Epoch: 6, train for the 99-th batch, train loss: 0.3165135085582733:  82%|██████████▋  | 98/119 [00:16<00:04,  5.10it/s]Epoch: 6, train for the 99-th batch, train loss: 0.3165135085582733:  83%|██████████▊  | 99/119 [00:16<00:03,  5.36it/s]evaluate for the 17-th batch, evaluate loss: 0.5551334023475647:  42%|███████▌          | 16/38 [00:01<00:01, 12.70it/s]evaluate for the 7-th batch, evaluate loss: 0.9291857481002808:  24%|████▊               | 6/25 [00:00<00:01, 10.70it/s]evaluate for the 6-th batch, evaluate loss: 0.8391207456588745:  12%|██▌                 | 5/40 [00:00<00:04,  8.30it/s]evaluate for the 6-th batch, evaluate loss: 0.8391207456588745:  15%|███                 | 6/40 [00:00<00:03,  8.69it/s]evaluate for the 18-th batch, evaluate loss: 0.6082475781440735:  42%|███████▌          | 16/38 [00:01<00:01, 12.70it/s]evaluate for the 18-th batch, evaluate loss: 0.6082475781440735:  47%|████████▌         | 18/38 [00:01<00:01, 12.93it/s]evaluate for the 8-th batch, evaluate loss: 0.9770634770393372:  24%|████▊               | 6/25 [00:00<00:01, 10.70it/s]evaluate for the 8-th batch, evaluate loss: 0.9770634770393372:  32%|██████▍             | 8/25 [00:00<00:01, 10.56it/s]evaluate for the 7-th batch, evaluate loss: 0.8213911056518555:  15%|███                 | 6/40 [00:00<00:03,  8.69it/s]evaluate for the 7-th batch, evaluate loss: 0.8213911056518555:  18%|███▌                | 7/40 [00:00<00:03,  8.96it/s]Epoch: 3, train for the 163-th batch, train loss: 0.15337762236595154:  67%|██████▋   | 162/241 [00:27<00:14,  5.41it/s]Epoch: 3, train for the 163-th batch, train loss: 0.15337762236595154:  68%|██████▊   | 163/241 [00:27<00:14,  5.39it/s]evaluate for the 19-th batch, evaluate loss: 0.597427487373352:  47%|█████████          | 18/38 [00:01<00:01, 12.93it/s]Epoch: 6, train for the 100-th batch, train loss: 0.34242695569992065:  83%|█████████▏ | 99/119 [00:16<00:03,  5.36it/s]evaluate for the 94-th batch, evaluate loss: 0.6947053074836731:  88%|██████████████▉  | 93/106 [00:24<00:03,  3.73it/s]evaluate for the 94-th batch, evaluate loss: 0.6947053074836731:  89%|███████████████  | 94/106 [00:24<00:03,  3.34it/s]Epoch: 6, train for the 100-th batch, train loss: 0.34242695569992065:  84%|████████▍ | 100/119 [00:16<00:03,  5.35it/s]evaluate for the 9-th batch, evaluate loss: 0.8695284724235535:  32%|██████▍             | 8/25 [00:00<00:01, 10.56it/s]evaluate for the 20-th batch, evaluate loss: 0.518791139125824:  47%|█████████          | 18/38 [00:01<00:01, 12.93it/s]evaluate for the 20-th batch, evaluate loss: 0.518791139125824:  53%|██████████         | 20/38 [00:01<00:01, 12.75it/s]evaluate for the 8-th batch, evaluate loss: 0.913599967956543:  18%|███▋                 | 7/40 [00:00<00:03,  8.96it/s]evaluate for the 8-th batch, evaluate loss: 0.913599967956543:  20%|████▏                | 8/40 [00:00<00:03,  8.32it/s]evaluate for the 10-th batch, evaluate loss: 0.8961237668991089:  32%|██████             | 8/25 [00:00<00:01, 10.56it/s]evaluate for the 10-th batch, evaluate loss: 0.8961237668991089:  40%|███████▏          | 10/25 [00:00<00:01, 10.89it/s]Epoch: 3, train for the 164-th batch, train loss: 0.1474933922290802:  68%|███████▍   | 163/241 [00:27<00:14,  5.39it/s]Epoch: 3, train for the 164-th batch, train loss: 0.1474933922290802:  68%|███████▍   | 164/241 [00:27<00:14,  5.41it/s]evaluate for the 21-th batch, evaluate loss: 0.537165105342865:  53%|██████████         | 20/38 [00:01<00:01, 12.75it/s]Epoch: 6, train for the 101-th batch, train loss: 0.3715037405490875:  84%|█████████▏ | 100/119 [00:17<00:03,  5.35it/s]evaluate for the 11-th batch, evaluate loss: 0.8546687364578247:  40%|███████▏          | 10/25 [00:01<00:01, 10.89it/s]Epoch: 6, train for the 101-th batch, train loss: 0.3715037405490875:  85%|█████████▎ | 101/119 [00:17<00:03,  5.19it/s]evaluate for the 95-th batch, evaluate loss: 0.5632984638214111:  89%|███████████████  | 94/106 [00:24<00:03,  3.34it/s]evaluate for the 95-th batch, evaluate loss: 0.5632984638214111:  90%|███████████████▏ | 95/106 [00:24<00:03,  3.55it/s]evaluate for the 9-th batch, evaluate loss: 0.9231391549110413:  20%|████                | 8/40 [00:01<00:03,  8.32it/s]evaluate for the 9-th batch, evaluate loss: 0.9231391549110413:  22%|████▌               | 9/40 [00:01<00:04,  7.34it/s]evaluate for the 22-th batch, evaluate loss: 0.5613094568252563:  53%|█████████▍        | 20/38 [00:01<00:01, 12.75it/s]evaluate for the 22-th batch, evaluate loss: 0.5613094568252563:  58%|██████████▍       | 22/38 [00:01<00:01, 11.36it/s]evaluate for the 12-th batch, evaluate loss: 0.7433804273605347:  40%|███████▏          | 10/25 [00:01<00:01, 10.89it/s]evaluate for the 12-th batch, evaluate loss: 0.7433804273605347:  48%|████████▋         | 12/25 [00:01<00:01, 10.73it/s]Epoch: 3, train for the 165-th batch, train loss: 0.22401826083660126:  68%|██████▊   | 164/241 [00:27<00:14,  5.41it/s]Epoch: 3, train for the 165-th batch, train loss: 0.22401826083660126:  68%|██████▊   | 165/241 [00:27<00:14,  5.28it/s]evaluate for the 10-th batch, evaluate loss: 0.8852347135543823:  22%|████▎              | 9/40 [00:01<00:04,  7.34it/s]evaluate for the 10-th batch, evaluate loss: 0.8852347135543823:  25%|████▌             | 10/40 [00:01<00:03,  7.57it/s]Epoch: 6, train for the 102-th batch, train loss: 0.38662341237068176:  85%|████████▍ | 101/119 [00:17<00:03,  5.19it/s]evaluate for the 23-th batch, evaluate loss: 0.535859227180481:  58%|███████████        | 22/38 [00:01<00:01, 11.36it/s]Epoch: 6, train for the 102-th batch, train loss: 0.38662341237068176:  86%|████████▌ | 102/119 [00:17<00:03,  5.35it/s]evaluate for the 13-th batch, evaluate loss: 0.7552676200866699:  48%|████████▋         | 12/25 [00:01<00:01, 10.73it/s]evaluate for the 24-th batch, evaluate loss: 0.5556734204292297:  58%|██████████▍       | 22/38 [00:01<00:01, 11.36it/s]evaluate for the 24-th batch, evaluate loss: 0.5556734204292297:  63%|███████████▎      | 24/38 [00:01<00:01, 11.59it/s]evaluate for the 11-th batch, evaluate loss: 0.8826185464859009:  25%|████▌             | 10/40 [00:01<00:03,  7.57it/s]Epoch: 3, train for the 166-th batch, train loss: 0.22069476544857025:  68%|██████▊   | 165/241 [00:28<00:14,  5.28it/s]evaluate for the 25-th batch, evaluate loss: 0.536793053150177:  63%|████████████       | 24/38 [00:02<00:01, 11.59it/s]evaluate for the 14-th batch, evaluate loss: 0.7666317820549011:  48%|████████▋         | 12/25 [00:01<00:01, 10.73it/s]evaluate for the 14-th batch, evaluate loss: 0.7666317820549011:  56%|██████████        | 14/25 [00:01<00:01,  9.70it/s]Epoch: 3, train for the 166-th batch, train loss: 0.22069476544857025:  69%|██████▉   | 166/241 [00:28<00:14,  5.24it/s]Epoch: 6, train for the 103-th batch, train loss: 0.3594977557659149:  86%|█████████▍ | 102/119 [00:17<00:03,  5.35it/s]Epoch: 6, train for the 103-th batch, train loss: 0.3594977557659149:  87%|█████████▌ | 103/119 [00:17<00:02,  5.34it/s]evaluate for the 96-th batch, evaluate loss: 0.5937235355377197:  90%|███████████████▏ | 95/106 [00:24<00:03,  3.55it/s]evaluate for the 96-th batch, evaluate loss: 0.5937235355377197:  91%|███████████████▍ | 96/106 [00:24<00:03,  3.26it/s]evaluate for the 12-th batch, evaluate loss: 0.8589298725128174:  25%|████▌             | 10/40 [00:01<00:03,  7.57it/s]evaluate for the 12-th batch, evaluate loss: 0.8589298725128174:  30%|█████▍            | 12/40 [00:01<00:03,  7.97it/s]evaluate for the 26-th batch, evaluate loss: 0.5313705801963806:  63%|███████████▎      | 24/38 [00:02<00:01, 11.59it/s]evaluate for the 26-th batch, evaluate loss: 0.5313705801963806:  68%|████████████▎     | 26/38 [00:02<00:01, 11.82it/s]evaluate for the 15-th batch, evaluate loss: 0.8200820088386536:  56%|██████████        | 14/25 [00:01<00:01,  9.70it/s]evaluate for the 15-th batch, evaluate loss: 0.8200820088386536:  60%|██████████▊       | 15/25 [00:01<00:01,  9.68it/s]evaluate for the 27-th batch, evaluate loss: 0.5545864701271057:  68%|████████████▎     | 26/38 [00:02<00:01, 11.82it/s]Epoch: 3, train for the 167-th batch, train loss: 0.14291338622570038:  69%|██████▉   | 166/241 [00:28<00:14,  5.24it/s]Epoch: 6, train for the 104-th batch, train loss: 0.4135550558567047:  87%|█████████▌ | 103/119 [00:17<00:02,  5.34it/s]evaluate for the 13-th batch, evaluate loss: 0.8808512091636658:  30%|█████▍            | 12/40 [00:01<00:03,  7.97it/s]evaluate for the 13-th batch, evaluate loss: 0.8808512091636658:  32%|█████▊            | 13/40 [00:01<00:03,  7.91it/s]evaluate for the 16-th batch, evaluate loss: 0.8204649090766907:  60%|██████████▊       | 15/25 [00:01<00:01,  9.68it/s]evaluate for the 16-th batch, evaluate loss: 0.8204649090766907:  64%|███████████▌      | 16/25 [00:01<00:00,  9.65it/s]evaluate for the 28-th batch, evaluate loss: 0.5393105745315552:  68%|████████████▎     | 26/38 [00:02<00:01, 11.82it/s]evaluate for the 28-th batch, evaluate loss: 0.5393105745315552:  74%|█████████████▎    | 28/38 [00:02<00:00, 12.61it/s]Epoch: 6, train for the 104-th batch, train loss: 0.4135550558567047:  87%|█████████▌ | 104/119 [00:17<00:02,  5.48it/s]evaluate for the 97-th batch, evaluate loss: 0.5374805927276611:  91%|███████████████▍ | 96/106 [00:24<00:03,  3.26it/s]evaluate for the 97-th batch, evaluate loss: 0.5374805927276611:  92%|███████████████▌ | 97/106 [00:24<00:02,  3.80it/s]Epoch: 3, train for the 167-th batch, train loss: 0.14291338622570038:  69%|██████▉   | 167/241 [00:28<00:14,  4.98it/s]evaluate for the 29-th batch, evaluate loss: 0.5707980394363403:  74%|█████████████▎    | 28/38 [00:02<00:00, 12.61it/s]evaluate for the 17-th batch, evaluate loss: 0.7637403607368469:  64%|███████████▌      | 16/25 [00:01<00:00,  9.65it/s]evaluate for the 17-th batch, evaluate loss: 0.7637403607368469:  68%|████████████▏     | 17/25 [00:01<00:00,  9.42it/s]evaluate for the 30-th batch, evaluate loss: 0.5902338027954102:  74%|█████████████▎    | 28/38 [00:02<00:00, 12.61it/s]evaluate for the 30-th batch, evaluate loss: 0.5902338027954102:  79%|██████████████▏   | 30/38 [00:02<00:00, 13.15it/s]evaluate for the 14-th batch, evaluate loss: 0.9233865737915039:  32%|█████▊            | 13/40 [00:01<00:03,  7.91it/s]evaluate for the 14-th batch, evaluate loss: 0.9233865737915039:  35%|██████▎           | 14/40 [00:01<00:03,  7.61it/s]evaluate for the 31-th batch, evaluate loss: 0.5507392287254333:  79%|██████████████▏   | 30/38 [00:02<00:00, 13.15it/s]evaluate for the 15-th batch, evaluate loss: 0.8222367167472839:  35%|██████▎           | 14/40 [00:01<00:03,  7.61it/s]Epoch: 3, train for the 168-th batch, train loss: 0.10717912763357162:  69%|██████▉   | 167/241 [00:28<00:14,  4.98it/s]evaluate for the 32-th batch, evaluate loss: 0.5294029712677002:  79%|██████████████▏   | 30/38 [00:02<00:00, 13.15it/s]evaluate for the 32-th batch, evaluate loss: 0.5294029712677002:  84%|███████████████▏  | 32/38 [00:02<00:00, 14.54it/s]evaluate for the 18-th batch, evaluate loss: 0.7379088997840881:  68%|████████████▏     | 17/25 [00:01<00:00,  9.42it/s]evaluate for the 18-th batch, evaluate loss: 0.7379088997840881:  72%|████████████▉     | 18/25 [00:01<00:00,  8.87it/s]Epoch: 3, train for the 168-th batch, train loss: 0.10717912763357162:  70%|██████▉   | 168/241 [00:28<00:15,  4.81it/s]evaluate for the 19-th batch, evaluate loss: 0.6346914172172546:  72%|████████████▉     | 18/25 [00:01<00:00,  8.87it/s]evaluate for the 16-th batch, evaluate loss: 0.8432925343513489:  35%|██████▎           | 14/40 [00:01<00:03,  7.61it/s]evaluate for the 16-th batch, evaluate loss: 0.8432925343513489:  40%|███████▏          | 16/40 [00:01<00:02,  8.45it/s]Epoch: 3, train for the 169-th batch, train loss: 0.5536025762557983:  70%|███████▋   | 168/241 [00:28<00:15,  4.81it/s]evaluate for the 98-th batch, evaluate loss: 0.4830235242843628:  92%|███████████████▌ | 97/106 [00:25<00:02,  3.80it/s]evaluate for the 98-th batch, evaluate loss: 0.4830235242843628:  92%|███████████████▋ | 98/106 [00:25<00:02,  3.29it/s]evaluate for the 20-th batch, evaluate loss: 0.6561658978462219:  72%|████████████▉     | 18/25 [00:02<00:00,  8.87it/s]evaluate for the 20-th batch, evaluate loss: 0.6561658978462219:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.54it/s]Epoch: 3, train for the 169-th batch, train loss: 0.5536025762557983:  70%|███████▋   | 169/241 [00:28<00:14,  4.99it/s]evaluate for the 17-th batch, evaluate loss: 0.8446433544158936:  40%|███████▏          | 16/40 [00:02<00:02,  8.45it/s]evaluate for the 17-th batch, evaluate loss: 0.8446433544158936:  42%|███████▋          | 17/40 [00:02<00:02,  8.01it/s]evaluate for the 33-th batch, evaluate loss: 0.4896395802497864:  84%|███████████████▏  | 32/38 [00:02<00:00, 14.54it/s]Epoch: 6, train for the 105-th batch, train loss: 0.3193212151527405:  87%|█████████▌ | 104/119 [00:18<00:02,  5.48it/s]Epoch: 6, train for the 105-th batch, train loss: 0.3193212151527405:  88%|█████████▋ | 105/119 [00:18<00:03,  3.51it/s]evaluate for the 21-th batch, evaluate loss: 0.6663870811462402:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.54it/s]evaluate for the 21-th batch, evaluate loss: 0.6663870811462402:  84%|███████████████   | 21/25 [00:02<00:00,  9.61it/s]evaluate for the 34-th batch, evaluate loss: 0.5662984251976013:  84%|███████████████▏  | 32/38 [00:02<00:00, 14.54it/s]evaluate for the 34-th batch, evaluate loss: 0.5662984251976013:  89%|████████████████  | 34/38 [00:02<00:00, 10.38it/s]Epoch: 3, train for the 170-th batch, train loss: 0.641925036907196:  70%|████████▍   | 169/241 [00:28<00:14,  4.99it/s]evaluate for the 18-th batch, evaluate loss: 0.8451923727989197:  42%|███████▋          | 17/40 [00:02<00:02,  8.01it/s]evaluate for the 18-th batch, evaluate loss: 0.8451923727989197:  45%|████████          | 18/40 [00:02<00:02,  8.12it/s]Epoch: 3, train for the 170-th batch, train loss: 0.641925036907196:  71%|████████▍   | 170/241 [00:28<00:13,  5.21it/s]evaluate for the 99-th batch, evaluate loss: 0.5958734154701233:  92%|███████████████▋ | 98/106 [00:25<00:02,  3.29it/s]evaluate for the 99-th batch, evaluate loss: 0.5958734154701233:  93%|███████████████▉ | 99/106 [00:25<00:01,  3.70it/s]evaluate for the 22-th batch, evaluate loss: 0.598932147026062:  84%|███████████████▉   | 21/25 [00:02<00:00,  9.61it/s]evaluate for the 22-th batch, evaluate loss: 0.598932147026062:  88%|████████████████▋  | 22/25 [00:02<00:00,  9.42it/s]evaluate for the 35-th batch, evaluate loss: 0.5525393486022949:  89%|████████████████  | 34/38 [00:02<00:00, 10.38it/s]Epoch: 6, train for the 106-th batch, train loss: 0.3446359932422638:  88%|█████████▋ | 105/119 [00:18<00:03,  3.51it/s]evaluate for the 19-th batch, evaluate loss: 0.88710618019104:  45%|█████████           | 18/40 [00:02<00:02,  8.12it/s]evaluate for the 19-th batch, evaluate loss: 0.88710618019104:  48%|█████████▌          | 19/40 [00:02<00:02,  8.25it/s]Epoch: 6, train for the 106-th batch, train loss: 0.3446359932422638:  89%|█████████▊ | 106/119 [00:18<00:03,  3.84it/s]evaluate for the 23-th batch, evaluate loss: 0.596843421459198:  88%|████████████████▋  | 22/25 [00:02<00:00,  9.42it/s]evaluate for the 23-th batch, evaluate loss: 0.596843421459198:  92%|█████████████████▍ | 23/25 [00:02<00:00,  9.27it/s]Epoch: 3, train for the 171-th batch, train loss: 0.5807651281356812:  71%|███████▊   | 170/241 [00:29<00:13,  5.21it/s]Epoch: 3, train for the 171-th batch, train loss: 0.5807651281356812:  71%|███████▊   | 171/241 [00:29<00:13,  5.19it/s]evaluate for the 20-th batch, evaluate loss: 0.7887642979621887:  48%|████████▌         | 19/40 [00:02<00:02,  8.25it/s]Epoch: 6, train for the 107-th batch, train loss: 0.3549731373786926:  89%|█████████▊ | 106/119 [00:18<00:03,  3.84it/s]Epoch: 6, train for the 107-th batch, train loss: 0.3549731373786926:  90%|█████████▉ | 107/119 [00:18<00:02,  4.66it/s]evaluate for the 24-th batch, evaluate loss: 0.6164039969444275:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.27it/s]evaluate for the 36-th batch, evaluate loss: 0.5479209423065186:  89%|████████████████  | 34/38 [00:03<00:00, 10.38it/s]evaluate for the 36-th batch, evaluate loss: 0.5479209423065186:  95%|█████████████████ | 36/38 [00:03<00:00,  8.20it/s]evaluate for the 21-th batch, evaluate loss: 0.862699568271637:  48%|█████████          | 19/40 [00:02<00:02,  8.25it/s]evaluate for the 21-th batch, evaluate loss: 0.862699568271637:  52%|█████████▉         | 21/40 [00:02<00:02,  8.85it/s]Epoch: 3, train for the 172-th batch, train loss: 0.47550657391548157:  71%|███████   | 171/241 [00:29<00:13,  5.19it/s]evaluate for the 25-th batch, evaluate loss: 0.5466834306716919:  92%|████████████████▌ | 23/25 [00:02<00:00,  9.27it/s]evaluate for the 25-th batch, evaluate loss: 0.5466834306716919: 100%|██████████████████| 25/25 [00:02<00:00,  9.47it/s]evaluate for the 25-th batch, evaluate loss: 0.5466834306716919: 100%|██████████████████| 25/25 [00:02<00:00,  9.87it/s]
Epoch: 3, train for the 172-th batch, train loss: 0.47550657391548157:  71%|███████▏  | 172/241 [00:29<00:13,  5.30it/s]evaluate for the 100-th batch, evaluate loss: 0.5196515917778015:  93%|██████████████▉ | 99/106 [00:25<00:01,  3.70it/s]evaluate for the 100-th batch, evaluate loss: 0.5196515917778015:  94%|██████████████▏| 100/106 [00:25<00:01,  3.27it/s]evaluate for the 37-th batch, evaluate loss: 0.5237460136413574:  95%|█████████████████ | 36/38 [00:03<00:00,  8.20it/s]Epoch: 6, train for the 108-th batch, train loss: 0.2672853469848633:  90%|█████████▉ | 107/119 [00:18<00:02,  4.66it/s]Epoch: 6, train for the 108-th batch, train loss: 0.2672853469848633:  91%|█████████▉ | 108/119 [00:18<00:02,  4.73it/s]evaluate for the 22-th batch, evaluate loss: 0.8553279042243958:  52%|█████████▍        | 21/40 [00:02<00:02,  8.85it/s]evaluate for the 22-th batch, evaluate loss: 0.8553279042243958:  55%|█████████▉        | 22/40 [00:02<00:02,  8.68it/s]evaluate for the 38-th batch, evaluate loss: 0.5608518123626709:  95%|█████████████████ | 36/38 [00:03<00:00,  8.20it/s]evaluate for the 38-th batch, evaluate loss: 0.5608518123626709: 100%|██████████████████| 38/38 [00:03<00:00,  9.19it/s]evaluate for the 38-th batch, evaluate loss: 0.5608518123626709: 100%|██████████████████| 38/38 [00:03<00:00, 11.48it/s]
Epoch: 3, train for the 173-th batch, train loss: 0.309753954410553:  71%|████████▌   | 172/241 [00:29<00:13,  5.30it/s]Epoch: 3, train for the 173-th batch, train loss: 0.309753954410553:  72%|████████▌   | 173/241 [00:29<00:11,  5.71it/s]evaluate for the 101-th batch, evaluate loss: 0.6240893602371216:  94%|██████████████▏| 100/106 [00:26<00:01,  3.27it/s]evaluate for the 101-th batch, evaluate loss: 0.6240893602371216:  95%|██████████████▎| 101/106 [00:26<00:01,  3.59it/s]evaluate for the 23-th batch, evaluate loss: 0.8196810483932495:  55%|█████████▉        | 22/40 [00:02<00:02,  8.68it/s]evaluate for the 23-th batch, evaluate loss: 0.8196810483932495:  57%|██████████▎       | 23/40 [00:02<00:02,  7.68it/s]Epoch: 6, train for the 109-th batch, train loss: 0.35431969165802:  91%|███████████▊ | 108/119 [00:18<00:02,  4.73it/s]  0%|                                                                                            | 0/20 [00:00<?, ?it/s]Epoch: 6, train for the 109-th batch, train loss: 0.35431969165802:  92%|███████████▉ | 109/119 [00:18<00:02,  4.75it/s]INFO:root:Epoch: 4, learning rate: 0.0001, train loss: 0.5426
INFO:root:train average_precision, 0.7955
INFO:root:train roc_auc, 0.7769
INFO:root:validate loss: 0.6534
INFO:root:validate average_precision, 0.7097
INFO:root:validate roc_auc, 0.7607
INFO:root:new node validate loss: 0.8203
INFO:root:new node validate first_1_average_precision, 0.6838
INFO:root:new node validate first_1_roc_auc, 0.7086
INFO:root:new node validate first_3_average_precision, 0.6381
INFO:root:new node validate first_3_roc_auc, 0.6346
INFO:root:new node validate first_10_average_precision, 0.6313
INFO:root:new node validate first_10_roc_auc, 0.6268
INFO:root:new node validate average_precision, 0.6679
INFO:root:new node validate roc_auc, 0.6651
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]Epoch: 3, train for the 174-th batch, train loss: 0.3419005870819092:  72%|███████▉   | 173/241 [00:29<00:11,  5.71it/s]Epoch: 3, train for the 174-th batch, train loss: 0.3419005870819092:  72%|███████▉   | 174/241 [00:29<00:11,  5.70it/s]evaluate for the 1-th batch, evaluate loss: 0.9912844896316528:   0%|                            | 0/20 [00:00<?, ?it/s]evaluate for the 24-th batch, evaluate loss: 0.9094328284263611:  57%|██████████▎       | 23/40 [00:02<00:02,  7.68it/s]evaluate for the 24-th batch, evaluate loss: 0.9094328284263611:  60%|██████████▊       | 24/40 [00:02<00:02,  7.72it/s]Epoch: 5, train for the 1-th batch, train loss: 0.711887776851654:   0%|                        | 0/151 [00:00<?, ?it/s]Epoch: 6, train for the 110-th batch, train loss: 0.3489455282688141:  92%|██████████ | 109/119 [00:19<00:02,  4.75it/s]Epoch: 5, train for the 1-th batch, train loss: 0.711887776851654:   1%|                | 1/151 [00:00<00:22,  6.81it/s]evaluate for the 2-th batch, evaluate loss: 0.8513152599334717:   0%|                            | 0/20 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.8513152599334717:  10%|██                  | 2/20 [00:00<00:01, 10.63it/s]Epoch: 6, train for the 110-th batch, train loss: 0.3489455282688141:  92%|██████████▏| 110/119 [00:19<00:01,  4.91it/s]evaluate for the 25-th batch, evaluate loss: 0.8570268750190735:  60%|██████████▊       | 24/40 [00:03<00:02,  7.72it/s]Epoch: 3, train for the 175-th batch, train loss: 0.2506265640258789:  72%|███████▉   | 174/241 [00:29<00:11,  5.70it/s]Epoch: 3, train for the 175-th batch, train loss: 0.2506265640258789:  73%|███████▉   | 175/241 [00:29<00:11,  5.65it/s]evaluate for the 3-th batch, evaluate loss: 0.806901216506958:  10%|██                   | 2/20 [00:00<00:01, 10.63it/s]evaluate for the 26-th batch, evaluate loss: 0.8218052387237549:  60%|██████████▊       | 24/40 [00:03<00:02,  7.72it/s]evaluate for the 26-th batch, evaluate loss: 0.8218052387237549:  65%|███████████▋      | 26/40 [00:03<00:01,  8.81it/s]Epoch: 5, train for the 2-th batch, train loss: 0.7177724242210388:   1%|               | 1/151 [00:00<00:22,  6.81it/s]Epoch: 5, train for the 2-th batch, train loss: 0.7177724242210388:   1%|▏              | 2/151 [00:00<00:21,  6.95it/s]evaluate for the 4-th batch, evaluate loss: 0.7492863535881042:  10%|██                  | 2/20 [00:00<00:01, 10.63it/s]evaluate for the 4-th batch, evaluate loss: 0.7492863535881042:  20%|████                | 4/20 [00:00<00:01, 11.77it/s]Epoch: 6, train for the 111-th batch, train loss: 0.37246760725975037:  92%|█████████▏| 110/119 [00:19<00:01,  4.91it/s]Epoch: 6, train for the 111-th batch, train loss: 0.37246760725975037:  93%|█████████▎| 111/119 [00:19<00:01,  4.99it/s]Epoch: 3, train for the 176-th batch, train loss: 0.3414596617221832:  73%|███████▉   | 175/241 [00:29<00:11,  5.65it/s]evaluate for the 27-th batch, evaluate loss: 0.8025642037391663:  65%|███████████▋      | 26/40 [00:03<00:01,  8.81it/s]evaluate for the 27-th batch, evaluate loss: 0.8025642037391663:  68%|████████████▏     | 27/40 [00:03<00:01,  8.93it/s]evaluate for the 5-th batch, evaluate loss: 0.8132973909378052:  20%|████                | 4/20 [00:00<00:01, 11.77it/s]Epoch: 3, train for the 176-th batch, train loss: 0.3414596617221832:  73%|████████   | 176/241 [00:29<00:11,  5.68it/s]Epoch: 5, train for the 3-th batch, train loss: 0.7165502905845642:   1%|▏              | 2/151 [00:00<00:21,  6.95it/s]Epoch: 5, train for the 3-th batch, train loss: 0.7165502905845642:   2%|▎              | 3/151 [00:00<00:21,  6.99it/s]evaluate for the 102-th batch, evaluate loss: 0.4820510447025299:  95%|██████████████▎| 101/106 [00:26<00:01,  3.59it/s]evaluate for the 102-th batch, evaluate loss: 0.4820510447025299:  96%|██████████████▍| 102/106 [00:26<00:01,  2.90it/s]evaluate for the 6-th batch, evaluate loss: 0.8395544290542603:  20%|████                | 4/20 [00:00<00:01, 11.77it/s]evaluate for the 6-th batch, evaluate loss: 0.8395544290542603:  30%|██████              | 6/20 [00:00<00:01, 12.45it/s]evaluate for the 28-th batch, evaluate loss: 0.8732684254646301:  68%|████████████▏     | 27/40 [00:03<00:01,  8.93it/s]evaluate for the 28-th batch, evaluate loss: 0.8732684254646301:  70%|████████████▌     | 28/40 [00:03<00:01,  8.79it/s]Epoch: 6, train for the 112-th batch, train loss: 0.33119887113571167:  93%|█████████▎| 111/119 [00:19<00:01,  4.99it/s]evaluate for the 7-th batch, evaluate loss: 0.8120653629302979:  30%|██████              | 6/20 [00:00<00:01, 12.45it/s]Epoch: 6, train for the 112-th batch, train loss: 0.33119887113571167:  94%|█████████▍| 112/119 [00:19<00:01,  5.04it/s]Epoch: 3, train for the 177-th batch, train loss: 0.35486578941345215:  73%|███████▎  | 176/241 [00:30<00:11,  5.68it/s]Epoch: 3, train for the 177-th batch, train loss: 0.35486578941345215:  73%|███████▎  | 177/241 [00:30<00:11,  5.40it/s]Epoch: 5, train for the 4-th batch, train loss: 0.7060639262199402:   2%|▎              | 3/151 [00:00<00:21,  6.99it/s]Epoch: 5, train for the 4-th batch, train loss: 0.7060639262199402:   3%|▍              | 4/151 [00:00<00:22,  6.62it/s]evaluate for the 8-th batch, evaluate loss: 0.7780816555023193:  30%|██████              | 6/20 [00:00<00:01, 12.45it/s]evaluate for the 8-th batch, evaluate loss: 0.7780816555023193:  40%|████████            | 8/20 [00:00<00:01, 11.87it/s]evaluate for the 103-th batch, evaluate loss: 0.5516840815544128:  96%|██████████████▍| 102/106 [00:26<00:01,  2.90it/s]evaluate for the 103-th batch, evaluate loss: 0.5516840815544128:  97%|██████████████▌| 103/106 [00:26<00:00,  3.29it/s]evaluate for the 29-th batch, evaluate loss: 0.8704491853713989:  70%|████████████▌     | 28/40 [00:03<00:01,  8.79it/s]evaluate for the 29-th batch, evaluate loss: 0.8704491853713989:  72%|█████████████     | 29/40 [00:03<00:01,  7.83it/s]evaluate for the 30-th batch, evaluate loss: 0.8267231583595276:  72%|█████████████     | 29/40 [00:03<00:01,  7.83it/s]Epoch: 6, train for the 113-th batch, train loss: 0.3291587829589844:  94%|██████████▎| 112/119 [00:19<00:01,  5.04it/s]evaluate for the 9-th batch, evaluate loss: 0.7874187231063843:  40%|████████            | 8/20 [00:00<00:01, 11.87it/s]Epoch: 6, train for the 113-th batch, train loss: 0.3291587829589844:  95%|██████████▍| 113/119 [00:19<00:01,  4.81it/s]Epoch: 3, train for the 178-th batch, train loss: 0.45873090624809265:  73%|███████▎  | 177/241 [00:30<00:11,  5.40it/s]Epoch: 5, train for the 5-th batch, train loss: 0.7163065075874329:   3%|▍              | 4/151 [00:00<00:22,  6.62it/s]Epoch: 5, train for the 5-th batch, train loss: 0.7163065075874329:   3%|▍              | 5/151 [00:00<00:25,  5.68it/s]evaluate for the 10-th batch, evaluate loss: 0.7489919662475586:  40%|███████▌           | 8/20 [00:00<00:01, 11.87it/s]evaluate for the 10-th batch, evaluate loss: 0.7489919662475586:  50%|█████████         | 10/20 [00:00<00:00, 11.51it/s]Epoch: 3, train for the 178-th batch, train loss: 0.45873090624809265:  74%|███████▍  | 178/241 [00:30<00:12,  4.93it/s]evaluate for the 31-th batch, evaluate loss: 0.7792685031890869:  72%|█████████████     | 29/40 [00:03<00:01,  7.83it/s]evaluate for the 31-th batch, evaluate loss: 0.7792685031890869:  78%|█████████████▉    | 31/40 [00:03<00:01,  8.23it/s]evaluate for the 104-th batch, evaluate loss: 0.5825360417366028:  97%|██████████████▌| 103/106 [00:26<00:00,  3.29it/s]evaluate for the 104-th batch, evaluate loss: 0.5825360417366028:  98%|██████████████▋| 104/106 [00:26<00:00,  3.53it/s]evaluate for the 11-th batch, evaluate loss: 0.7759767174720764:  50%|█████████         | 10/20 [00:01<00:00, 11.51it/s]Epoch: 6, train for the 114-th batch, train loss: 0.3252773880958557:  95%|██████████▍| 113/119 [00:19<00:01,  4.81it/s]Epoch: 6, train for the 114-th batch, train loss: 0.3252773880958557:  96%|██████████▌| 114/119 [00:19<00:01,  4.83it/s]Epoch: 5, train for the 6-th batch, train loss: 0.7135266065597534:   3%|▍              | 5/151 [00:00<00:25,  5.68it/s]Epoch: 5, train for the 6-th batch, train loss: 0.7135266065597534:   4%|▌              | 6/151 [00:00<00:25,  5.74it/s]evaluate for the 32-th batch, evaluate loss: 0.8132450580596924:  78%|█████████████▉    | 31/40 [00:03<00:01,  8.23it/s]evaluate for the 32-th batch, evaluate loss: 0.8132450580596924:  80%|██████████████▍   | 32/40 [00:03<00:00,  8.36it/s]Epoch: 3, train for the 179-th batch, train loss: 0.4042746126651764:  74%|████████   | 178/241 [00:30<00:12,  4.93it/s]evaluate for the 12-th batch, evaluate loss: 0.7769051790237427:  50%|█████████         | 10/20 [00:01<00:00, 11.51it/s]evaluate for the 12-th batch, evaluate loss: 0.7769051790237427:  60%|██████████▊       | 12/20 [00:01<00:00, 10.67it/s]Epoch: 3, train for the 179-th batch, train loss: 0.4042746126651764:  74%|████████▏  | 179/241 [00:30<00:13,  4.74it/s]evaluate for the 105-th batch, evaluate loss: 0.5539498925209045:  98%|██████████████▋| 104/106 [00:27<00:00,  3.53it/s]evaluate for the 105-th batch, evaluate loss: 0.5539498925209045:  99%|██████████████▊| 105/106 [00:27<00:00,  3.82it/s]evaluate for the 33-th batch, evaluate loss: 0.8149864077568054:  80%|██████████████▍   | 32/40 [00:03<00:00,  8.36it/s]evaluate for the 33-th batch, evaluate loss: 0.8149864077568054:  82%|██████████████▊   | 33/40 [00:03<00:00,  8.17it/s]Epoch: 6, train for the 115-th batch, train loss: 0.3075294494628906:  96%|██████████▌| 114/119 [00:20<00:01,  4.83it/s]evaluate for the 13-th batch, evaluate loss: 0.7485159039497375:  60%|██████████▊       | 12/20 [00:01<00:00, 10.67it/s]Epoch: 5, train for the 7-th batch, train loss: 0.6930583715438843:   4%|▌              | 6/151 [00:01<00:25,  5.74it/s]Epoch: 6, train for the 115-th batch, train loss: 0.3075294494628906:  97%|██████████▋| 115/119 [00:20<00:00,  4.88it/s]Epoch: 5, train for the 7-th batch, train loss: 0.6930583715438843:   5%|▋              | 7/151 [00:01<00:26,  5.53it/s]Epoch: 3, train for the 180-th batch, train loss: 0.5839858651161194:  74%|████████▏  | 179/241 [00:30<00:13,  4.74it/s]evaluate for the 14-th batch, evaluate loss: 0.7578698396682739:  60%|██████████▊       | 12/20 [00:01<00:00, 10.67it/s]evaluate for the 14-th batch, evaluate loss: 0.7578698396682739:  70%|████████████▌     | 14/20 [00:01<00:00, 10.52it/s]Epoch: 3, train for the 180-th batch, train loss: 0.5839858651161194:  75%|████████▏  | 180/241 [00:30<00:12,  4.86it/s]evaluate for the 106-th batch, evaluate loss: 0.5764046311378479:  99%|██████████████▊| 105/106 [00:27<00:00,  3.82it/s]evaluate for the 106-th batch, evaluate loss: 0.5764046311378479: 100%|███████████████| 106/106 [00:27<00:00,  4.05it/s]evaluate for the 106-th batch, evaluate loss: 0.5764046311378479: 100%|███████████████| 106/106 [00:27<00:00,  3.87it/s]
evaluate for the 15-th batch, evaluate loss: 0.8107030987739563:  70%|████████████▌     | 14/20 [00:01<00:00, 10.52it/s]evaluate for the 34-th batch, evaluate loss: 0.7677969336509705:  82%|██████████████▊   | 33/40 [00:04<00:00,  8.17it/s]evaluate for the 34-th batch, evaluate loss: 0.7677969336509705:  85%|███████████████▎  | 34/40 [00:04<00:00,  7.03it/s]Epoch: 5, train for the 8-th batch, train loss: 0.7018182873725891:   5%|▋              | 7/151 [00:01<00:26,  5.53it/s]Epoch: 5, train for the 8-th batch, train loss: 0.7018182873725891:   5%|▊              | 8/151 [00:01<00:25,  5.70it/s]Epoch: 6, train for the 116-th batch, train loss: 0.3062165081501007:  97%|██████████▋| 115/119 [00:20<00:00,  4.88it/s]evaluate for the 16-th batch, evaluate loss: 0.7259385585784912:  70%|████████████▌     | 14/20 [00:01<00:00, 10.52it/s]evaluate for the 16-th batch, evaluate loss: 0.7259385585784912:  80%|██████████████▍   | 16/20 [00:01<00:00, 11.13it/s]Epoch: 6, train for the 116-th batch, train loss: 0.3062165081501007:  97%|██████████▋| 116/119 [00:20<00:00,  4.78it/s]Epoch: 3, train for the 181-th batch, train loss: 0.7674677968025208:  75%|████████▏  | 180/241 [00:30<00:12,  4.86it/s]Epoch: 3, train for the 181-th batch, train loss: 0.7674677968025208:  75%|████████▎  | 181/241 [00:30<00:11,  5.06it/s]evaluate for the 35-th batch, evaluate loss: 0.779923677444458:  85%|████████████████▏  | 34/40 [00:04<00:00,  7.03it/s]evaluate for the 35-th batch, evaluate loss: 0.779923677444458:  88%|████████████████▋  | 35/40 [00:04<00:00,  7.34it/s]evaluate for the 17-th batch, evaluate loss: 0.7624512910842896:  80%|██████████████▍   | 16/20 [00:01<00:00, 11.13it/s]Epoch: 5, train for the 9-th batch, train loss: 0.6725425720214844:   5%|▊              | 8/151 [00:01<00:25,  5.70it/s]Epoch: 5, train for the 9-th batch, train loss: 0.6725425720214844:   6%|▉              | 9/151 [00:01<00:23,  6.07it/s]evaluate for the 18-th batch, evaluate loss: 0.7594388127326965:  80%|██████████████▍   | 16/20 [00:01<00:00, 11.13it/s]evaluate for the 18-th batch, evaluate loss: 0.7594388127326965:  90%|████████████████▏ | 18/20 [00:01<00:00, 11.47it/s]Epoch: 6, train for the 117-th batch, train loss: 0.2831074595451355:  97%|██████████▋| 116/119 [00:20<00:00,  4.78it/s]Epoch: 3, train for the 182-th batch, train loss: 0.5703672766685486:  75%|████████▎  | 181/241 [00:31<00:11,  5.06it/s]Epoch: 6, train for the 117-th batch, train loss: 0.2831074595451355:  98%|██████████▊| 117/119 [00:20<00:00,  4.88it/s]Epoch: 3, train for the 182-th batch, train loss: 0.5703672766685486:  76%|████████▎  | 182/241 [00:31<00:11,  5.27it/s]evaluate for the 19-th batch, evaluate loss: 0.7944140434265137:  90%|████████████████▏ | 18/20 [00:01<00:00, 11.47it/s]Epoch: 5, train for the 10-th batch, train loss: 0.6616636514663696:   6%|▊             | 9/151 [00:01<00:23,  6.07it/s]Epoch: 5, train for the 10-th batch, train loss: 0.6616636514663696:   7%|▊            | 10/151 [00:01<00:22,  6.24it/s]evaluate for the 36-th batch, evaluate loss: 0.8326808214187622:  88%|███████████████▊  | 35/40 [00:04<00:00,  7.34it/s]evaluate for the 36-th batch, evaluate loss: 0.8326808214187622:  90%|████████████████▏ | 36/40 [00:04<00:00,  6.45it/s]evaluate for the 20-th batch, evaluate loss: 0.7926721572875977:  90%|████████████████▏ | 18/20 [00:01<00:00, 11.47it/s]evaluate for the 20-th batch, evaluate loss: 0.7926721572875977: 100%|██████████████████| 20/20 [00:01<00:00, 12.48it/s]evaluate for the 20-th batch, evaluate loss: 0.7926721572875977: 100%|██████████████████| 20/20 [00:01<00:00, 11.65it/s]
evaluate for the 37-th batch, evaluate loss: 0.7675228118896484:  90%|████████████████▏ | 36/40 [00:04<00:00,  6.45it/s]evaluate for the 37-th batch, evaluate loss: 0.7675228118896484:  92%|████████████████▋ | 37/40 [00:04<00:00,  7.00it/s]Epoch: 6, train for the 118-th batch, train loss: 0.2627897560596466:  98%|██████████▊| 117/119 [00:20<00:00,  4.88it/s]Epoch: 6, train for the 118-th batch, train loss: 0.2627897560596466:  99%|██████████▉| 118/119 [00:20<00:00,  5.02it/s]Epoch: 3, train for the 183-th batch, train loss: 0.35238879919052124:  76%|███████▌  | 182/241 [00:31<00:11,  5.27it/s]Epoch: 3, train for the 183-th batch, train loss: 0.35238879919052124:  76%|███████▌  | 183/241 [00:31<00:11,  5.23it/s]Epoch: 5, train for the 11-th batch, train loss: 0.6652734875679016:   7%|▊            | 10/151 [00:01<00:22,  6.24it/s]Epoch: 5, train for the 11-th batch, train loss: 0.6652734875679016:   7%|▉            | 11/151 [00:01<00:23,  5.97it/s]evaluate for the 38-th batch, evaluate loss: 0.743404746055603:  92%|█████████████████▌ | 37/40 [00:04<00:00,  7.00it/s]evaluate for the 38-th batch, evaluate loss: 0.743404746055603:  95%|██████████████████ | 38/40 [00:04<00:00,  7.64it/s]Epoch: 6, train for the 119-th batch, train loss: 0.3061316907405853:  99%|██████████▉| 118/119 [00:20<00:00,  5.02it/s]Epoch: 6, train for the 119-th batch, train loss: 0.3061316907405853: 100%|███████████| 119/119 [00:20<00:00,  5.84it/s]Epoch: 6, train for the 119-th batch, train loss: 0.3061316907405853: 100%|███████████| 119/119 [00:20<00:00,  5.72it/s]
  0%|                                                                                            | 0/40 [00:00<?, ?it/s]evaluate for the 39-th batch, evaluate loss: 0.7910747528076172:  95%|█████████████████ | 38/40 [00:04<00:00,  7.64it/s]Epoch: 3, train for the 184-th batch, train loss: 0.541961669921875:  76%|█████████   | 183/241 [00:31<00:11,  5.23it/s]Epoch: 3, train for the 184-th batch, train loss: 0.541961669921875:  76%|█████████▏  | 184/241 [00:31<00:10,  5.38it/s]INFO:root:Epoch: 5, learning rate: 0.0001, train loss: 0.4825
INFO:root:train average_precision, 0.8630
INFO:root:train roc_auc, 0.8596
INFO:root:validate loss: 0.5831
INFO:root:validate average_precision, 0.7385
INFO:root:validate roc_auc, 0.7919
INFO:root:new node validate loss: 0.7942
INFO:root:new node validate first_1_average_precision, 0.5351
INFO:root:new node validate first_1_roc_auc, 0.4612
INFO:root:new node validate first_3_average_precision, 0.5479
INFO:root:new node validate first_3_roc_auc, 0.5317
INFO:root:new node validate first_10_average_precision, 0.5522
INFO:root:new node validate first_10_roc_auc, 0.5602
INFO:root:new node validate average_precision, 0.5953
INFO:root:new node validate roc_auc, 0.6216
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.577975332736969:   0%|                             | 0/40 [00:00<?, ?it/s]Epoch: 5, train for the 12-th batch, train loss: 0.6750492453575134:   7%|▉            | 11/151 [00:01<00:23,  5.97it/s]Epoch: 5, train for the 12-th batch, train loss: 0.6750492453575134:   8%|█            | 12/151 [00:02<00:24,  5.73it/s]evaluate for the 40-th batch, evaluate loss: 0.7883601784706116:  95%|█████████████████ | 38/40 [00:04<00:00,  7.64it/s]evaluate for the 40-th batch, evaluate loss: 0.7883601784706116: 100%|██████████████████| 40/40 [00:04<00:00,  8.77it/s]evaluate for the 40-th batch, evaluate loss: 0.7883601784706116: 100%|██████████████████| 40/40 [00:04<00:00,  8.14it/s]
evaluate for the 2-th batch, evaluate loss: 0.5917121767997742:   0%|                            | 0/40 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5917121767997742:   5%|█                   | 2/40 [00:00<00:02, 16.66it/s]Epoch: 6, train for the 1-th batch, train loss: 1.0297882556915283:   0%|                       | 0/146 [00:00<?, ?it/s]Epoch: 6, train for the 1-th batch, train loss: 1.0297882556915283:   1%|               | 1/146 [00:00<00:19,  7.46it/s]evaluate for the 3-th batch, evaluate loss: 0.6583126783370972:   5%|█                   | 2/40 [00:00<00:02, 16.66it/s]evaluate for the 4-th batch, evaluate loss: 0.7239952683448792:   5%|█                   | 2/40 [00:00<00:02, 16.66it/s]evaluate for the 4-th batch, evaluate loss: 0.7239952683448792:  10%|██                  | 4/40 [00:00<00:02, 16.69it/s]Epoch: 3, train for the 185-th batch, train loss: 0.6244129538536072:  76%|████████▍  | 184/241 [00:31<00:10,  5.38it/s]Epoch: 5, train for the 13-th batch, train loss: 0.6816946268081665:   8%|█            | 12/151 [00:02<00:24,  5.73it/s]Epoch: 5, train for the 13-th batch, train loss: 0.6816946268081665:   9%|█            | 13/151 [00:02<00:24,  5.52it/s]Epoch: 3, train for the 185-th batch, train loss: 0.6244129538536072:  77%|████████▍  | 185/241 [00:31<00:11,  4.90it/s]Epoch: 6, train for the 2-th batch, train loss: 0.48907607793807983:   1%|              | 1/146 [00:00<00:19,  7.46it/s]Epoch: 6, train for the 2-th batch, train loss: 0.48907607793807983:   1%|▏             | 2/146 [00:00<00:18,  7.98it/s]evaluate for the 5-th batch, evaluate loss: 0.7302855253219604:  10%|██                  | 4/40 [00:00<00:02, 16.69it/s]Epoch: 5, train for the 14-th batch, train loss: 0.6740707755088806:   9%|█            | 13/151 [00:02<00:24,  5.52it/s]evaluate for the 6-th batch, evaluate loss: 0.651680588722229:  10%|██                   | 4/40 [00:00<00:02, 16.69it/s]evaluate for the 6-th batch, evaluate loss: 0.651680588722229:  15%|███▏                 | 6/40 [00:00<00:02, 14.66it/s]Epoch: 5, train for the 14-th batch, train loss: 0.6740707755088806:   9%|█▏           | 14/151 [00:02<00:22,  6.01it/s]Epoch: 6, train for the 3-th batch, train loss: 0.49166229367256165:   1%|▏             | 2/146 [00:00<00:18,  7.98it/s]Epoch: 6, train for the 3-th batch, train loss: 0.49166229367256165:   2%|▎             | 3/146 [00:00<00:18,  7.92it/s]Epoch: 3, train for the 186-th batch, train loss: 0.47514814138412476:  77%|███████▋  | 185/241 [00:31<00:11,  4.90it/s]Epoch: 3, train for the 186-th batch, train loss: 0.47514814138412476:  77%|███████▋  | 186/241 [00:31<00:10,  5.02it/s]evaluate for the 7-th batch, evaluate loss: 0.6831264495849609:  15%|███                 | 6/40 [00:00<00:02, 14.66it/s]Epoch: 5, train for the 15-th batch, train loss: 0.6558418273925781:   9%|█▏           | 14/151 [00:02<00:22,  6.01it/s]Epoch: 5, train for the 15-th batch, train loss: 0.6558418273925781:  10%|█▎           | 15/151 [00:02<00:21,  6.31it/s]evaluate for the 8-th batch, evaluate loss: 0.6459323167800903:  15%|███                 | 6/40 [00:00<00:02, 14.66it/s]evaluate for the 8-th batch, evaluate loss: 0.6459323167800903:  20%|████                | 8/40 [00:00<00:02, 13.35it/s]INFO:root:Epoch: 2, learning rate: 0.0001, train loss: 0.5648
INFO:root:train average_precision, 0.7881
INFO:root:train roc_auc, 0.7612
INFO:root:validate loss: 0.6464
INFO:root:validate average_precision, 0.7440
Epoch: 6, train for the 4-th batch, train loss: 0.5047106742858887:   2%|▎              | 3/146 [00:00<00:18,  7.92it/s]INFO:root:validate roc_auc, 0.7542
INFO:root:new node validate loss: 0.8494
INFO:root:new node validate first_1_average_precision, 0.6795
INFO:root:new node validate first_1_roc_auc, 0.6467
INFO:root:new node validate first_3_average_precision, 0.6666
INFO:root:new node validate first_3_roc_auc, 0.6204
INFO:root:new node validate first_10_average_precision, 0.6700
INFO:root:new node validate first_10_roc_auc, 0.6273
INFO:root:new node validate average_precision, 0.6785
INFO:root:new node validate roc_auc, 0.6362
Epoch: 6, train for the 4-th batch, train loss: 0.5047106742858887:   3%|▍              | 4/146 [00:00<00:18,  7.56it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/237 [00:00<?, ?it/s]Epoch: 3, train for the 187-th batch, train loss: 0.5660631656646729:  77%|████████▍  | 186/241 [00:32<00:10,  5.02it/s]evaluate for the 9-th batch, evaluate loss: 0.6621094346046448:  20%|████                | 8/40 [00:00<00:02, 13.35it/s]Epoch: 3, train for the 187-th batch, train loss: 0.5660631656646729:  78%|████████▌  | 187/241 [00:32<00:10,  5.21it/s]Epoch: 5, train for the 16-th batch, train loss: 0.6469404697418213:  10%|█▎           | 15/151 [00:02<00:21,  6.31it/s]Epoch: 5, train for the 16-th batch, train loss: 0.6469404697418213:  11%|█▍           | 16/151 [00:02<00:20,  6.73it/s]Epoch: 6, train for the 5-th batch, train loss: 0.5034334659576416:   3%|▍              | 4/146 [00:00<00:18,  7.56it/s]Epoch: 6, train for the 5-th batch, train loss: 0.5034334659576416:   3%|▌              | 5/146 [00:00<00:18,  7.50it/s]evaluate for the 10-th batch, evaluate loss: 0.7321851253509521:  20%|███▊               | 8/40 [00:00<00:02, 13.35it/s]evaluate for the 10-th batch, evaluate loss: 0.7321851253509521:  25%|████▌             | 10/40 [00:00<00:02, 13.62it/s]Epoch: 3, train for the 1-th batch, train loss: 0.8477383852005005:   0%|                       | 0/237 [00:00<?, ?it/s]evaluate for the 11-th batch, evaluate loss: 0.6347280740737915:  25%|████▌             | 10/40 [00:00<00:02, 13.62it/s]Epoch: 3, train for the 1-th batch, train loss: 0.8477383852005005:   0%|               | 1/237 [00:00<00:42,  5.62it/s]Epoch: 3, train for the 188-th batch, train loss: 0.4096502661705017:  78%|████████▌  | 187/241 [00:32<00:10,  5.21it/s]Epoch: 6, train for the 6-th batch, train loss: 0.4896869957447052:   3%|▌              | 5/146 [00:00<00:18,  7.50it/s]Epoch: 6, train for the 6-th batch, train loss: 0.4896869957447052:   4%|▌              | 6/146 [00:00<00:17,  7.90it/s]Epoch: 3, train for the 188-th batch, train loss: 0.4096502661705017:  78%|████████▌  | 188/241 [00:32<00:10,  5.20it/s]Epoch: 5, train for the 17-th batch, train loss: 0.6570230722427368:  11%|█▍           | 16/151 [00:02<00:20,  6.73it/s]Epoch: 5, train for the 17-th batch, train loss: 0.6570230722427368:  11%|█▍           | 17/151 [00:02<00:21,  6.30it/s]evaluate for the 12-th batch, evaluate loss: 0.6106024384498596:  25%|████▌             | 10/40 [00:00<00:02, 13.62it/s]evaluate for the 12-th batch, evaluate loss: 0.6106024384498596:  30%|█████▍            | 12/40 [00:00<00:02, 13.81it/s]Epoch: 3, train for the 2-th batch, train loss: 0.8001900911331177:   0%|               | 1/237 [00:00<00:42,  5.62it/s]Epoch: 3, train for the 2-th batch, train loss: 0.8001900911331177:   1%|▏              | 2/237 [00:00<00:34,  6.74it/s]evaluate for the 13-th batch, evaluate loss: 0.6262872815132141:  30%|█████▍            | 12/40 [00:00<00:02, 13.81it/s]Epoch: 6, train for the 7-th batch, train loss: 0.4937649369239807:   4%|▌              | 6/146 [00:00<00:17,  7.90it/s]Epoch: 6, train for the 7-th batch, train loss: 0.4937649369239807:   5%|▋              | 7/146 [00:00<00:18,  7.34it/s]evaluate for the 14-th batch, evaluate loss: 0.6417820453643799:  30%|█████▍            | 12/40 [00:00<00:02, 13.81it/s]evaluate for the 14-th batch, evaluate loss: 0.6417820453643799:  35%|██████▎           | 14/40 [00:00<00:01, 14.20it/s]Epoch: 3, train for the 189-th batch, train loss: 0.39401334524154663:  78%|███████▊  | 188/241 [00:32<00:10,  5.20it/s]Epoch: 3, train for the 189-th batch, train loss: 0.39401334524154663:  78%|███████▊  | 189/241 [00:32<00:09,  5.35it/s]Epoch: 5, train for the 18-th batch, train loss: 0.6325440406799316:  11%|█▍           | 17/151 [00:02<00:21,  6.30it/s]Epoch: 5, train for the 18-th batch, train loss: 0.6325440406799316:  12%|█▌           | 18/151 [00:02<00:22,  5.96it/s]evaluate for the 15-th batch, evaluate loss: 0.6476544737815857:  35%|██████▎           | 14/40 [00:01<00:01, 14.20it/s]Epoch: 6, train for the 8-th batch, train loss: 0.45164617896080017:   5%|▋             | 7/146 [00:01<00:18,  7.34it/s]Epoch: 6, train for the 8-th batch, train loss: 0.45164617896080017:   5%|▊             | 8/146 [00:01<00:17,  7.70it/s]Epoch: 3, train for the 3-th batch, train loss: 0.7459551692008972:   1%|▏              | 2/237 [00:00<00:34,  6.74it/s]Epoch: 3, train for the 3-th batch, train loss: 0.7459551692008972:   1%|▏              | 3/237 [00:00<00:40,  5.76it/s]evaluate for the 16-th batch, evaluate loss: 0.730993390083313:  35%|██████▋            | 14/40 [00:01<00:01, 14.20it/s]evaluate for the 16-th batch, evaluate loss: 0.730993390083313:  40%|███████▌           | 16/40 [00:01<00:01, 14.09it/s]Epoch: 3, train for the 190-th batch, train loss: 0.32896265387535095:  78%|███████▊  | 189/241 [00:32<00:09,  5.35it/s]Epoch: 3, train for the 190-th batch, train loss: 0.32896265387535095:  79%|███████▉  | 190/241 [00:32<00:09,  5.33it/s]Epoch: 5, train for the 19-th batch, train loss: 0.6281839609146118:  12%|█▌           | 18/151 [00:03<00:22,  5.96it/s]Epoch: 5, train for the 19-th batch, train loss: 0.6281839609146118:  13%|█▋           | 19/151 [00:03<00:22,  5.90it/s]evaluate for the 17-th batch, evaluate loss: 0.6612197756767273:  40%|███████▏          | 16/40 [00:01<00:01, 14.09it/s]Epoch: 6, train for the 9-th batch, train loss: 0.4288274943828583:   5%|▊              | 8/146 [00:01<00:17,  7.70it/s]Epoch: 6, train for the 9-th batch, train loss: 0.4288274943828583:   6%|▉              | 9/146 [00:01<00:17,  7.61it/s]evaluate for the 18-th batch, evaluate loss: 0.6113585233688354:  40%|███████▏          | 16/40 [00:01<00:01, 14.09it/s]evaluate for the 18-th batch, evaluate loss: 0.6113585233688354:  45%|████████          | 18/40 [00:01<00:01, 13.10it/s]Epoch: 3, train for the 4-th batch, train loss: 0.6366451382637024:   1%|▏              | 3/237 [00:00<00:40,  5.76it/s]Epoch: 3, train for the 4-th batch, train loss: 0.6366451382637024:   2%|▎              | 4/237 [00:00<00:43,  5.30it/s]Epoch: 3, train for the 191-th batch, train loss: 0.3257271945476532:  79%|████████▋  | 190/241 [00:32<00:09,  5.33it/s]Epoch: 3, train for the 191-th batch, train loss: 0.3257271945476532:  79%|████████▋  | 191/241 [00:32<00:09,  5.43it/s]Epoch: 5, train for the 20-th batch, train loss: 0.6290833353996277:  13%|█▋           | 19/151 [00:03<00:22,  5.90it/s]Epoch: 5, train for the 20-th batch, train loss: 0.6290833353996277:  13%|█▋           | 20/151 [00:03<00:22,  5.80it/s]evaluate for the 19-th batch, evaluate loss: 0.7386813163757324:  45%|████████          | 18/40 [00:01<00:01, 13.10it/s]Epoch: 6, train for the 10-th batch, train loss: 0.4269711971282959:   6%|▊             | 9/146 [00:01<00:17,  7.61it/s]Epoch: 6, train for the 10-th batch, train loss: 0.4269711971282959:   7%|▉            | 10/146 [00:01<00:19,  7.03it/s]  0%|                                                                                            | 0/78 [00:00<?, ?it/s]Epoch: 3, train for the 5-th batch, train loss: 0.6992874145507812:   2%|▎              | 4/237 [00:00<00:43,  5.30it/s]Epoch: 3, train for the 5-th batch, train loss: 0.6992874145507812:   2%|▎              | 5/237 [00:00<00:39,  5.87it/s]evaluate for the 20-th batch, evaluate loss: 0.7159963846206665:  45%|████████          | 18/40 [00:01<00:01, 13.10it/s]evaluate for the 20-th batch, evaluate loss: 0.7159963846206665:  50%|█████████         | 20/40 [00:01<00:01, 12.62it/s]Epoch: 6, train for the 11-th batch, train loss: 0.41642332077026367:   7%|▊           | 10/146 [00:01<00:19,  7.03it/s]Epoch: 6, train for the 11-th batch, train loss: 0.41642332077026367:   8%|▉           | 11/146 [00:01<00:18,  7.13it/s]evaluate for the 21-th batch, evaluate loss: 0.596062421798706:  50%|█████████▌         | 20/40 [00:01<00:01, 12.62it/s]Epoch: 3, train for the 192-th batch, train loss: 0.29324769973754883:  79%|███████▉  | 191/241 [00:33<00:09,  5.43it/s]evaluate for the 1-th batch, evaluate loss: 1.0226432085037231:   0%|                            | 0/78 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 1.0226432085037231:   1%|▎                   | 1/78 [00:00<00:13,  5.64it/s]Epoch: 3, train for the 192-th batch, train loss: 0.29324769973754883:  80%|███████▉  | 192/241 [00:33<00:09,  5.20it/s]Epoch: 5, train for the 21-th batch, train loss: 0.6115940809249878:  13%|█▋           | 20/151 [00:03<00:22,  5.80it/s]Epoch: 5, train for the 21-th batch, train loss: 0.6115940809249878:  14%|█▊           | 21/151 [00:03<00:24,  5.38it/s]Epoch: 3, train for the 6-th batch, train loss: 0.6591123938560486:   2%|▎              | 5/237 [00:01<00:39,  5.87it/s]Epoch: 3, train for the 6-th batch, train loss: 0.6591123938560486:   3%|▍              | 6/237 [00:01<00:38,  5.99it/s]evaluate for the 22-th batch, evaluate loss: 0.6257817149162292:  50%|█████████         | 20/40 [00:01<00:01, 12.62it/s]evaluate for the 22-th batch, evaluate loss: 0.6257817149162292:  55%|█████████▉        | 22/40 [00:01<00:01, 12.58it/s]Epoch: 6, train for the 12-th batch, train loss: 0.4003106653690338:   8%|▉            | 11/146 [00:01<00:18,  7.13it/s]Epoch: 6, train for the 12-th batch, train loss: 0.4003106653690338:   8%|█            | 12/146 [00:01<00:19,  7.03it/s]evaluate for the 23-th batch, evaluate loss: 0.5510517954826355:  55%|█████████▉        | 22/40 [00:01<00:01, 12.58it/s]evaluate for the 2-th batch, evaluate loss: 1.0528112649917603:   1%|▎                   | 1/78 [00:00<00:13,  5.64it/s]evaluate for the 2-th batch, evaluate loss: 1.0528112649917603:   3%|▌                   | 2/78 [00:00<00:13,  5.82it/s]Epoch: 5, train for the 22-th batch, train loss: 0.5914106369018555:  14%|█▊           | 21/151 [00:03<00:24,  5.38it/s]Epoch: 5, train for the 22-th batch, train loss: 0.5914106369018555:  15%|█▉           | 22/151 [00:03<00:22,  5.64it/s]Epoch: 3, train for the 193-th batch, train loss: 0.3489970266819:  80%|███████████▏  | 192/241 [00:33<00:09,  5.20it/s]Epoch: 3, train for the 7-th batch, train loss: 0.6445842385292053:   3%|▍              | 6/237 [00:01<00:38,  5.99it/s]Epoch: 3, train for the 7-th batch, train loss: 0.6445842385292053:   3%|▍              | 7/237 [00:01<00:38,  6.04it/s]Epoch: 3, train for the 193-th batch, train loss: 0.3489970266819:  80%|███████████▏  | 193/241 [00:33<00:09,  5.02it/s]evaluate for the 24-th batch, evaluate loss: 0.6877273917198181:  55%|█████████▉        | 22/40 [00:01<00:01, 12.58it/s]evaluate for the 24-th batch, evaluate loss: 0.6877273917198181:  60%|██████████▊       | 24/40 [00:01<00:01, 12.08it/s]Epoch: 6, train for the 13-th batch, train loss: 0.39485740661621094:   8%|▉           | 12/146 [00:01<00:19,  7.03it/s]Epoch: 6, train for the 13-th batch, train loss: 0.39485740661621094:   9%|█           | 13/146 [00:01<00:19,  6.82it/s]Epoch: 5, train for the 23-th batch, train loss: 0.6005187034606934:  15%|█▉           | 22/151 [00:03<00:22,  5.64it/s]evaluate for the 25-th batch, evaluate loss: 0.6597377061843872:  60%|██████████▊       | 24/40 [00:01<00:01, 12.08it/s]Epoch: 5, train for the 23-th batch, train loss: 0.6005187034606934:  15%|█▉           | 23/151 [00:03<00:21,  5.85it/s]evaluate for the 3-th batch, evaluate loss: 0.9448949694633484:   3%|▌                   | 2/78 [00:00<00:13,  5.82it/s]evaluate for the 3-th batch, evaluate loss: 0.9448949694633484:   4%|▊                   | 3/78 [00:00<00:13,  5.52it/s]Epoch: 3, train for the 194-th batch, train loss: 0.3242049813270569:  80%|████████▊  | 193/241 [00:33<00:09,  5.02it/s]Epoch: 3, train for the 8-th batch, train loss: 0.6363041996955872:   3%|▍              | 7/237 [00:01<00:38,  6.04it/s]Epoch: 3, train for the 8-th batch, train loss: 0.6363041996955872:   3%|▌              | 8/237 [00:01<00:39,  5.78it/s]Epoch: 6, train for the 14-th batch, train loss: 0.4023810625076294:   9%|█▏           | 13/146 [00:01<00:19,  6.82it/s]Epoch: 6, train for the 14-th batch, train loss: 0.4023810625076294:  10%|█▏           | 14/146 [00:01<00:19,  6.74it/s]evaluate for the 26-th batch, evaluate loss: 0.6108774542808533:  60%|██████████▊       | 24/40 [00:01<00:01, 12.08it/s]evaluate for the 26-th batch, evaluate loss: 0.6108774542808533:  65%|███████████▋      | 26/40 [00:01<00:01, 11.84it/s]Epoch: 3, train for the 194-th batch, train loss: 0.3242049813270569:  80%|████████▊  | 194/241 [00:33<00:09,  5.06it/s]Epoch: 5, train for the 24-th batch, train loss: 0.580593466758728:  15%|██▏           | 23/151 [00:03<00:21,  5.85it/s]Epoch: 5, train for the 24-th batch, train loss: 0.580593466758728:  16%|██▏           | 24/151 [00:04<00:20,  6.14it/s]evaluate for the 27-th batch, evaluate loss: 0.6941984295845032:  65%|███████████▋      | 26/40 [00:02<00:01, 11.84it/s]evaluate for the 4-th batch, evaluate loss: 0.9028642177581787:   4%|▊                   | 3/78 [00:00<00:13,  5.52it/s]evaluate for the 4-th batch, evaluate loss: 0.9028642177581787:   5%|█                   | 4/78 [00:00<00:13,  5.52it/s]Epoch: 6, train for the 15-th batch, train loss: 0.37736600637435913:  10%|█▏          | 14/146 [00:02<00:19,  6.74it/s]Epoch: 6, train for the 15-th batch, train loss: 0.37736600637435913:  10%|█▏          | 15/146 [00:02<00:19,  6.76it/s]Epoch: 3, train for the 9-th batch, train loss: 0.6131959557533264:   3%|▌              | 8/237 [00:01<00:39,  5.78it/s]Epoch: 3, train for the 195-th batch, train loss: 0.5292929410934448:  80%|████████▊  | 194/241 [00:33<00:09,  5.06it/s]Epoch: 3, train for the 9-th batch, train loss: 0.6131959557533264:   4%|▌              | 9/237 [00:01<00:40,  5.63it/s]evaluate for the 28-th batch, evaluate loss: 0.6351173520088196:  65%|███████████▋      | 26/40 [00:02<00:01, 11.84it/s]evaluate for the 28-th batch, evaluate loss: 0.6351173520088196:  70%|████████████▌     | 28/40 [00:02<00:01, 11.65it/s]Epoch: 3, train for the 195-th batch, train loss: 0.5292929410934448:  81%|████████▉  | 195/241 [00:33<00:09,  5.08it/s]evaluate for the 29-th batch, evaluate loss: 0.6895492672920227:  70%|████████████▌     | 28/40 [00:02<00:01, 11.65it/s]evaluate for the 5-th batch, evaluate loss: 1.10417902469635:   5%|█▏                    | 4/78 [00:00<00:13,  5.52it/s]evaluate for the 5-th batch, evaluate loss: 1.10417902469635:   6%|█▍                    | 5/78 [00:00<00:12,  5.75it/s]Epoch: 6, train for the 16-th batch, train loss: 0.38385289907455444:  10%|█▏          | 15/146 [00:02<00:19,  6.76it/s]Epoch: 6, train for the 16-th batch, train loss: 0.38385289907455444:  11%|█▎          | 16/146 [00:02<00:19,  6.60it/s]evaluate for the 30-th batch, evaluate loss: 0.6005533337593079:  70%|████████████▌     | 28/40 [00:02<00:01, 11.65it/s]evaluate for the 30-th batch, evaluate loss: 0.6005533337593079:  75%|█████████████▌    | 30/40 [00:02<00:00, 11.69it/s]Epoch: 3, train for the 196-th batch, train loss: 0.44611242413520813:  81%|████████  | 195/241 [00:33<00:09,  5.08it/s]Epoch: 5, train for the 25-th batch, train loss: 0.6731301546096802:  16%|██           | 24/151 [00:04<00:20,  6.14it/s]Epoch: 3, train for the 196-th batch, train loss: 0.44611242413520813:  81%|████████▏ | 196/241 [00:33<00:08,  5.07it/s]Epoch: 5, train for the 25-th batch, train loss: 0.6731301546096802:  17%|██▏          | 25/151 [00:04<00:26,  4.67it/s]Epoch: 6, train for the 17-th batch, train loss: 0.3913743495941162:  11%|█▍           | 16/146 [00:02<00:19,  6.60it/s]Epoch: 6, train for the 17-th batch, train loss: 0.3913743495941162:  12%|█▌           | 17/146 [00:02<00:18,  6.85it/s]evaluate for the 31-th batch, evaluate loss: 0.7166808247566223:  75%|█████████████▌    | 30/40 [00:02<00:00, 11.69it/s]evaluate for the 6-th batch, evaluate loss: 0.9648669362068176:   6%|█▎                  | 5/78 [00:01<00:12,  5.75it/s]evaluate for the 6-th batch, evaluate loss: 0.9648669362068176:   8%|█▌                  | 6/78 [00:01<00:12,  5.66it/s]Epoch: 3, train for the 10-th batch, train loss: 0.6504948139190674:   4%|▌             | 9/237 [00:01<00:40,  5.63it/s]evaluate for the 32-th batch, evaluate loss: 0.647196352481842:  75%|██████████████▎    | 30/40 [00:02<00:00, 11.69it/s]evaluate for the 32-th batch, evaluate loss: 0.647196352481842:  80%|███████████████▏   | 32/40 [00:02<00:00, 12.04it/s]Epoch: 3, train for the 10-th batch, train loss: 0.6504948139190674:   4%|▌            | 10/237 [00:01<00:51,  4.45it/s]Epoch: 3, train for the 197-th batch, train loss: 0.5044243931770325:  81%|████████▉  | 196/241 [00:33<00:08,  5.07it/s]Epoch: 6, train for the 18-th batch, train loss: 0.38053587079048157:  12%|█▍          | 17/146 [00:02<00:18,  6.85it/s]Epoch: 6, train for the 18-th batch, train loss: 0.38053587079048157:  12%|█▍          | 18/146 [00:02<00:18,  7.10it/s]Epoch: 3, train for the 197-th batch, train loss: 0.5044243931770325:  82%|████████▉  | 197/241 [00:34<00:08,  5.29it/s]Epoch: 5, train for the 26-th batch, train loss: 0.6245703101158142:  17%|██▏          | 25/151 [00:04<00:26,  4.67it/s]evaluate for the 33-th batch, evaluate loss: 0.5807160139083862:  80%|██████████████▍   | 32/40 [00:02<00:00, 12.04it/s]Epoch: 5, train for the 26-th batch, train loss: 0.6245703101158142:  17%|██▏          | 26/151 [00:04<00:25,  4.86it/s]Epoch: 3, train for the 11-th batch, train loss: 0.6473441123962402:   4%|▌            | 10/237 [00:02<00:51,  4.45it/s]Epoch: 3, train for the 11-th batch, train loss: 0.6473441123962402:   5%|▌            | 11/237 [00:02<00:47,  4.77it/s]evaluate for the 7-th batch, evaluate loss: 0.9535645246505737:   8%|█▌                  | 6/78 [00:01<00:12,  5.66it/s]evaluate for the 7-th batch, evaluate loss: 0.9535645246505737:   9%|█▊                  | 7/78 [00:01<00:13,  5.37it/s]evaluate for the 34-th batch, evaluate loss: 0.5919569134712219:  80%|██████████████▍   | 32/40 [00:02<00:00, 12.04it/s]evaluate for the 34-th batch, evaluate loss: 0.5919569134712219:  85%|███████████████▎  | 34/40 [00:02<00:00, 11.42it/s]Epoch: 6, train for the 19-th batch, train loss: 0.3514134883880615:  12%|█▌           | 18/146 [00:02<00:18,  7.10it/s]Epoch: 6, train for the 19-th batch, train loss: 0.3514134883880615:  13%|█▋           | 19/146 [00:02<00:17,  7.08it/s]Epoch: 3, train for the 198-th batch, train loss: 0.46914032101631165:  82%|████████▏ | 197/241 [00:34<00:08,  5.29it/s]evaluate for the 35-th batch, evaluate loss: 0.702862560749054:  85%|████████████████▏  | 34/40 [00:02<00:00, 11.42it/s]Epoch: 5, train for the 27-th batch, train loss: 0.6338860988616943:  17%|██▏          | 26/151 [00:04<00:25,  4.86it/s]Epoch: 3, train for the 198-th batch, train loss: 0.46914032101631165:  82%|████████▏ | 198/241 [00:34<00:08,  5.10it/s]Epoch: 5, train for the 27-th batch, train loss: 0.6338860988616943:  18%|██▎          | 27/151 [00:04<00:25,  4.95it/s]