True
  0%|          | 0/52049 [00:00<?, ?it/s]True
  0%|          | 0/61156 [00:00<?, ?it/s]True
  0%|          | 0/87626 [00:00<?, ?it/s]True
  0%|          | 0/95577 [00:00<?, ?it/s]True
  0%|          | 0/50631 [00:00<?, ?it/s]  7%|▋         | 3796/52049 [00:00<00:01, 37959.76it/s]  2%|▏         | 2015/87626 [00:00<00:04, 20149.20it/s]  9%|▊         | 8239/95577 [00:00<00:01, 82381.63it/s]  9%|▊         | 5203/61156 [00:00<00:01, 41972.40it/s]True
  0%|          | 0/140777 [00:00<?, ?it/s]  9%|▊         | 4385/50631 [00:00<00:01, 43845.76it/s] 15%|█▍        | 7793/52049 [00:00<00:01, 35787.95it/s]  9%|▉         | 8100/87626 [00:00<00:01, 41075.13it/s]  5%|▍         | 6475/140777 [00:00<00:02, 64747.75it/s] 16%|█▌        | 9865/61156 [00:00<00:01, 41383.24it/s] 17%|█▋        | 8770/50631 [00:00<00:00, 43701.80it/s] 24%|██▎       | 12259/52049 [00:00<00:01, 39642.51it/s] 17%|█▋        | 16614/95577 [00:00<00:01, 51994.00it/s] 14%|█▍        | 12533/87626 [00:00<00:01, 42504.23it/s] 11%|█         | 15021/140777 [00:00<00:01, 76926.87it/s] 26%|██▌       | 13156/50631 [00:00<00:00, 43773.26it/s] 25%|██▌       | 15332/61156 [00:00<00:01, 41403.78it/s] 32%|███▏      | 16812/52049 [00:00<00:00, 39915.39it/s] 17%|█▋        | 23745/140777 [00:00<00:01, 81632.63it/s] 19%|█▉        | 16778/87626 [00:00<00:01, 39853.76it/s] 23%|██▎       | 22446/95577 [00:00<00:01, 45894.00it/s] 37%|███▋      | 18570/50631 [00:00<00:00, 43414.18it/s] 33%|███▎      | 19950/61156 [00:00<00:00, 43045.54it/s] 26%|██▌       | 22514/87626 [00:00<00:01, 45816.85it/s] 43%|████▎     | 22515/52049 [00:00<00:00, 40887.59it/s] 48%|████▊     | 24243/50631 [00:00<00:00, 47917.38it/s] 40%|████      | 24733/61156 [00:00<00:00, 43036.17it/s] 29%|██▊       | 27363/95577 [00:00<00:01, 42342.43it/s] 23%|██▎       | 31909/140777 [00:00<00:01, 64289.56it/s] 31%|███▏      | 27479/87626 [00:00<00:01, 47070.11it/s] 53%|█████▎    | 27392/52049 [00:00<00:00, 40901.61it/s] 49%|████▉     | 29898/61156 [00:00<00:00, 45725.40it/s] 57%|█████▋    | 29068/50631 [00:00<00:00, 43581.46it/s] 34%|███▍      | 32271/95577 [00:00<00:01, 42624.37it/s] 37%|███▋      | 32210/87626 [00:00<00:01, 46680.32it/s] 62%|██████▏   | 32036/52049 [00:00<00:00, 41124.62it/s] 67%|██████▋   | 34132/50631 [00:00<00:00, 45675.63it/s] 39%|███▉      | 37056/95577 [00:00<00:01, 44060.09it/s] 56%|█████▋    | 34505/61156 [00:00<00:00, 40436.81it/s] 28%|██▊       | 38765/140777 [00:00<00:02, 42231.83it/s] 42%|████▏     | 36894/87626 [00:00<00:01, 39869.63it/s] 70%|███████   | 36617/52049 [00:00<00:00, 40629.67it/s] 77%|███████▋  | 39019/50631 [00:00<00:00, 46626.14it/s] 45%|████▍     | 42589/95577 [00:00<00:01, 47237.49it/s] 65%|██████▍   | 39601/61156 [00:00<00:00, 42355.82it/s] 34%|███▎      | 47485/140777 [00:00<00:01, 52145.64it/s] 49%|████▉     | 42808/87626 [00:00<00:00, 45160.95it/s] 78%|███████▊  | 40680/52049 [00:01<00:00, 40380.72it/s] 51%|█████     | 48607/95577 [00:01<00:00, 50953.91it/s] 86%|████████▋ | 43738/50631 [00:00<00:00, 42223.54it/s] 72%|███████▏  | 43925/61156 [00:01<00:00, 42153.98it/s] 54%|█████▍    | 47511/87626 [00:01<00:00, 45057.93it/s] 88%|████████▊ | 45775/52049 [00:01<00:00, 43399.20it/s] 96%|█████████▋| 48846/50631 [00:01<00:00, 44682.87it/s] 79%|███████▉  | 48509/61156 [00:01<00:00, 43065.93it/s] 56%|█████▋    | 53827/95577 [00:01<00:00, 43426.20it/s] 38%|███▊      | 53926/140777 [00:01<00:02, 43189.27it/s] 60%|█████▉    | 52458/87626 [00:01<00:00, 46310.71it/s]100%|█████████▉| 50628/50631 [00:01<00:00, 43752.04it/s]
 96%|█████████▋| 50138/52049 [00:01<00:00, 41416.09it/s] 87%|████████▋ | 53002/61156 [00:01<00:00, 42684.10it/s] 61%|██████▏   | 58762/95577 [00:01<00:00, 44966.69it/s]100%|█████████▉| 52048/52049 [00:01<00:00, 40398.22it/s]
 45%|████▍     | 62657/140777 [00:01<00:01, 52522.02it/s] 65%|██████▌   | 57188/87626 [00:01<00:00, 40385.23it/s] 95%|█████████▌| 58380/61156 [00:01<00:00, 43256.28it/s] 66%|██████▋   | 63461/95577 [00:01<00:00, 44481.19it/s] 51%|█████     | 71442/140777 [00:01<00:01, 60719.48it/s]  0%|          | 0/50631 [00:00<?, ?it/s]100%|█████████▉| 61155/61156 [00:01<00:00, 42812.44it/s]
100%|██████████| 50631/50631 [00:00<00:00, 1894125.78it/s]
 71%|███████   | 61920/87626 [00:01<00:00, 40166.17it/s] 71%|███████▏  | 68210/95577 [00:01<00:00, 45306.73it/s] 77%|███████▋  | 67884/87626 [00:01<00:00, 45299.33it/s]  0%|          | 0/52049 [00:00<?, ?it/s] 76%|███████▌  | 72845/95577 [00:01<00:00, 39358.91it/s] 56%|█████▌    | 78589/140777 [00:01<00:01, 46353.62it/s]100%|██████████| 52049/52049 [00:00<00:00, 2037379.88it/s]
 83%|████████▎ | 72583/87626 [00:01<00:00, 45761.71it/s] 62%|██████▏   | 87254/140777 [00:01<00:00, 54670.23it/s] 81%|████████  | 76974/95577 [00:01<00:00, 38546.70it/s] 88%|████████▊ | 77282/87626 [00:01<00:00, 45392.88it/s]  0%|          | 0/61156 [00:00<?, ?it/s]100%|██████████| 61156/61156 [00:00<00:00, 1987547.02it/s]
 85%|████████▌ | 81457/95577 [00:01<00:00, 37995.28it/s] 94%|█████████▎| 82136/87626 [00:01<00:00, 46283.47it/s] 89%|████████▉ | 85347/95577 [00:01<00:00, 38189.53it/s] 99%|█████████▉| 86829/87626 [00:02<00:00, 41364.25it/s]100%|█████████▉| 87625/87626 [00:02<00:00, 42416.93it/s]
 67%|██████▋   | 93953/140777 [00:01<00:01, 37505.01it/s] 94%|█████████▎| 89565/95577 [00:02<00:00, 37669.78it/s] 71%|███████▏  | 100510/140777 [00:02<00:00, 42418.05it/s] 98%|█████████▊| 93379/95577 [00:02<00:00, 37736.67it/s]100%|█████████▉| 95576/95577 [00:02<00:00, 42506.66it/s]
 75%|███████▌  | 106171/140777 [00:02<00:01, 31763.57it/s] 82%|████████▏ | 114850/140777 [00:02<00:00, 40957.24it/s]  0%|          | 0/87626 [00:00<?, ?it/s]100%|██████████| 87626/87626 [00:00<00:00, 1900991.45it/s]
INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-reality-call', batch_size=200, model_name='TGN', gpu=0, num_neighbors=10, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=1, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='lincorrect-time-linear', use_ROPe=False, t1_factor_of_t2=1, use_init_method=True, emb_proj=False, init_weights='time-linear', clip=1.0, device='cuda:0', seed=0, save_model_name='TGN_seed0_dummy-yehxa035')
INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-escorts-dynamic', batch_size=200, model_name='TGN', gpu=0, num_neighbors=10, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=1, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='lincorrect-time-linear', use_ROPe=False, t1_factor_of_t2=1, use_init_method=True, emb_proj=False, init_weights='time-linear', clip=1.0, device='cuda:0', seed=0, save_model_name='TGN_seed0_dummy-4ge5z0wz')
 86%|████████▌ | 120952/140777 [00:02<00:00, 36471.91it/s]  0%|          | 0/95577 [00:00<?, ?it/s]INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-retweet-pol', batch_size=200, model_name='TGN', gpu=2, num_neighbors=10, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=1, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='lincorrect-time-linear', use_ROPe=False, t1_factor_of_t2=1, use_init_method=True, emb_proj=False, init_weights='time-linear', clip=1.0, device='cuda:2', seed=0, save_model_name='TGN_seed0_dummy-o1xc9z6e')
100%|██████████| 95577/95577 [00:00<00:00, 1999595.94it/s]
 92%|█████████▏| 129515/140777 [00:02<00:00, 45467.11it/s] 96%|█████████▌| 135492/140777 [00:03<00:00, 35062.47it/s]100%|█████████▉| 140776/140777 [00:03<00:00, 42680.05it/s]
INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-digg-reply', batch_size=200, model_name='TGN', gpu=1, num_neighbors=10, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=1, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='lincorrect-time-linear', use_ROPe=False, t1_factor_of_t2=1, use_init_method=True, emb_proj=False, init_weights='time-linear', clip=1.0, device='cuda:1', seed=0, save_model_name='TGN_seed0_dummy-s7jpz2dn')
INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-movielens-user2tags-10m', batch_size=200, model_name='TGN', gpu=2, num_neighbors=10, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=1, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='lincorrect-time-linear', use_ROPe=False, t1_factor_of_t2=1, use_init_method=True, emb_proj=False, init_weights='time-linear', clip=1.0, device='cuda:2', seed=0, save_model_name='TGN_seed0_dummy-n0qq8qen')
  0%|          | 0/140777 [00:00<?, ?it/s]100%|██████████| 140777/140777 [00:00<00:00, 1798926.16it/s]
INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-slashdot-reply-dir', batch_size=200, model_name='TGN', gpu=1, num_neighbors=10, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=1, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='lincorrect-time-linear', use_ROPe=False, t1_factor_of_t2=1, use_init_method=True, emb_proj=False, init_weights='time-linear', clip=1.0, device='cuda:1', seed=0, save_model_name='TGN_seed0_dummy-4c7j0fz9')
INFO:root:model -> Sequential(
  (0): MemoryModel(
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (emb_proj): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
      (1): ReLU()
    )
    (time_transformation_for_init): TimeInitTransformLinear(
      (lin): Linear(in_features=1, out_features=1, bias=True)
    )
    (message_aggregator): MessageAggregator()
    (memory_bank): MemoryBank(num_nodes=10108, memory_dim=172)
    (memory_updater): GRUMemoryUpdater(
      (memory_bank): MemoryBank(num_nodes=10108, memory_dim=172)
      (memory_updater): GRUCell(616, 172)
    )
    (embedding_module): GraphAttentionEmbedding(
      (time_encoder): TimeEncoder(
        (w): Linear(in_features=1, out_features=100, bias=True)
      )
      (temporal_conv_layers): ModuleList(
        (0): MultiHeadAttention(
          (query_projection): Linear(in_features=272, out_features=272, bias=False)
          (key_projection): Linear(in_features=444, out_features=272, bias=False)
          (value_projection): Linear(in_features=444, out_features=272, bias=False)
          (layer_norm): LayerNorm((272,), eps=1e-05, elementwise_affine=True)
          (residual_fc): Linear(in_features=272, out_features=272, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (merge_layers): ModuleList(
        (0): MergeLayer(
          (fc1): Linear(in_features=444, out_features=172, bias=True)
          (fc2): Linear(in_features=172, out_features=172, bias=True)
          (act): ReLU()
        )
      )
    )
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: TGN, #parameters: 3974904 B, 3881.7421875 KB, 3.7907638549804688 MB.
INFO:root:model -> Sequential(
  (0): MemoryModel(
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (emb_proj): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
      (1): ReLU()
    )
    (time_transformation_for_init): TimeInitTransformLinear(
      (lin): Linear(in_features=1, out_features=1, bias=True)
    )
    (message_aggregator): MessageAggregator()
    (memory_bank): MemoryBank(num_nodes=27046, memory_dim=172)
    (memory_updater): GRUMemoryUpdater(
      (memory_bank): MemoryBank(num_nodes=27046, memory_dim=172)
      (memory_updater): GRUCell(616, 172)
    )
    (embedding_module): GraphAttentionEmbedding(
      (time_encoder): TimeEncoder(
        (w): Linear(in_features=1, out_features=100, bias=True)
      )
      (temporal_conv_layers): ModuleList(
        (0): MultiHeadAttention(
          (query_projection): Linear(in_features=272, out_features=272, bias=False)
          (key_projection): Linear(in_features=444, out_features=272, bias=False)
          (value_projection): Linear(in_features=444, out_features=272, bias=False)
          (layer_norm): LayerNorm((272,), eps=1e-05, elementwise_affine=True)
          (residual_fc): Linear(in_features=272, out_features=272, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (merge_layers): ModuleList(
        (0): MergeLayer(
          (fc1): Linear(in_features=444, out_features=172, bias=True)
          (fc2): Linear(in_features=172, out_features=172, bias=True)
          (act): ReLU()
        )
      )
    )
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: TGN, #parameters: 3974904 B, 3881.7421875 KB, 3.7907638549804688 MB.
The dataset has 50631 interactions, involving 10106 different nodes
The training dataset has 29100 interactions, involving 7154 different nodes
The validation dataset has 7596 interactions, involving 4118 different nodes
The test dataset has 7577 interactions, involving 4144 different nodes
The new node validation dataset has 3845 interactions, involving 2930 different nodes
The new node test dataset has 4829 interactions, involving 3346 different nodes
1010 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]The dataset has 52049 interactions, involving 6809 different nodes
The training dataset has 23625 interactions, involving 3838 different nodes
The validation dataset has 7807 interactions, involving 1715 different nodes
The test dataset has 7808 interactions, involving 1937 different nodes
The new node validation dataset has 4011 interactions, involving 1185 different nodes
The new node test dataset has 4611 interactions, involving 1531 different nodes
680 nodes were used for the inductive testing, i.e. are never seen during training
INFO:root:model -> Sequential(
  (0): MemoryModel(
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (emb_proj): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
      (1): ReLU()
    )
    (time_transformation_for_init): TimeInitTransformLinear(
      (lin): Linear(in_features=1, out_features=1, bias=True)
    )
    (message_aggregator): MessageAggregator()
    (memory_bank): MemoryBank(num_nodes=18471, memory_dim=172)
    (memory_updater): GRUMemoryUpdater(
      (memory_bank): MemoryBank(num_nodes=18471, memory_dim=172)
      (memory_updater): GRUCell(616, 172)
    )
    (embedding_module): GraphAttentionEmbedding(
      (time_encoder): TimeEncoder(
        (w): Linear(in_features=1, out_features=100, bias=True)
      )
      (temporal_conv_layers): ModuleList(
        (0): MultiHeadAttention(
          (query_projection): Linear(in_features=272, out_features=272, bias=False)
          (key_projection): Linear(in_features=444, out_features=272, bias=False)
          (value_projection): Linear(in_features=444, out_features=272, bias=False)
          (layer_norm): LayerNorm((272,), eps=1e-05, elementwise_affine=True)
          (residual_fc): Linear(in_features=272, out_features=272, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (merge_layers): ModuleList(
        (0): MergeLayer(
          (fc1): Linear(in_features=444, out_features=172, bias=True)
          (fc2): Linear(in_features=172, out_features=172, bias=True)
          (act): ReLU()
        )
      )
    )
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]INFO:root:model name: TGN, #parameters: 3974904 B, 3881.7421875 KB, 3.7907638549804688 MB.
The dataset has 61156 interactions, involving 18470 different nodes
The training dataset has 30070 interactions, involving 12678 different nodes
The validation dataset has 9173 interactions, involving 5479 different nodes
The test dataset has 9174 interactions, involving 5328 different nodes
The new node validation dataset has 4957 interactions, involving 4196 different nodes
The new node test dataset has 5073 interactions, involving 4153 different nodes
1847 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]INFO:root:model -> Sequential(
  (0): MemoryModel(
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (emb_proj): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
      (1): ReLU()
    )
    (time_transformation_for_init): TimeInitTransformLinear(
      (lin): Linear(in_features=1, out_features=1, bias=True)
    )
    (message_aggregator): MessageAggregator()
    (memory_bank): MemoryBank(num_nodes=30400, memory_dim=172)
    (memory_updater): GRUMemoryUpdater(
      (memory_bank): MemoryBank(num_nodes=30400, memory_dim=172)
      (memory_updater): GRUCell(616, 172)
    )
    (embedding_module): GraphAttentionEmbedding(
      (time_encoder): TimeEncoder(
        (w): Linear(in_features=1, out_features=100, bias=True)
      )
      (temporal_conv_layers): ModuleList(
        (0): MultiHeadAttention(
          (query_projection): Linear(in_features=272, out_features=272, bias=False)
          (key_projection): Linear(in_features=444, out_features=272, bias=False)
          (value_projection): Linear(in_features=444, out_features=272, bias=False)
          (layer_norm): LayerNorm((272,), eps=1e-05, elementwise_affine=True)
          (residual_fc): Linear(in_features=272, out_features=272, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (merge_layers): ModuleList(
        (0): MergeLayer(
          (fc1): Linear(in_features=444, out_features=172, bias=True)
          (fc2): Linear(in_features=172, out_features=172, bias=True)
          (act): ReLU()
        )
      )
    )
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: TGN, #parameters: 3974904 B, 3881.7421875 KB, 3.7907638549804688 MB.
The dataset has 87626 interactions, involving 30398 different nodes
The training dataset has 47297 interactions, involving 21540 different nodes
The validation dataset has 13144 interactions, involving 9241 different nodes
The test dataset has 13144 interactions, involving 9511 different nodes
The new node validation dataset has 7995 interactions, involving 7321 different nodes
The new node test dataset has 8239 interactions, involving 7732 different nodes
3039 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/237 [00:00<?, ?it/s]INFO:root:model -> Sequential(
  (0): MemoryModel(
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (emb_proj): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
      (1): ReLU()
    )
    (time_transformation_for_init): TimeInitTransformLinear(
      (lin): Linear(in_features=1, out_features=1, bias=True)
    )
    (message_aggregator): MessageAggregator()
    (memory_bank): MemoryBank(num_nodes=16530, memory_dim=172)
    (memory_updater): GRUMemoryUpdater(
      (memory_bank): MemoryBank(num_nodes=16530, memory_dim=172)
      (memory_updater): GRUCell(616, 172)
    )
    (embedding_module): GraphAttentionEmbedding(
      (time_encoder): TimeEncoder(
        (w): Linear(in_features=1, out_features=100, bias=True)
      )
      (temporal_conv_layers): ModuleList(
        (0): MultiHeadAttention(
          (query_projection): Linear(in_features=272, out_features=272, bias=False)
          (key_projection): Linear(in_features=444, out_features=272, bias=False)
          (value_projection): Linear(in_features=444, out_features=272, bias=False)
          (layer_norm): LayerNorm((272,), eps=1e-05, elementwise_affine=True)
          (residual_fc): Linear(in_features=272, out_features=272, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (merge_layers): ModuleList(
        (0): MergeLayer(
          (fc1): Linear(in_features=444, out_features=172, bias=True)
          (fc2): Linear(in_features=172, out_features=172, bias=True)
          (act): ReLU()
        )
      )
    )
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: TGN, #parameters: 3974904 B, 3881.7421875 KB, 3.7907638549804688 MB.
The dataset has 95577 interactions, involving 16527 different nodes
The training dataset has 48189 interactions, involving 10566 different nodes
The validation dataset has 14336 interactions, involving 5209 different nodes
The test dataset has 14337 interactions, involving 5377 different nodes
The new node validation dataset has 6745 interactions, involving 3518 different nodes
The new node test dataset has 7521 interactions, involving 3885 different nodes
1652 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/241 [00:00<?, ?it/s]INFO:root:model -> Sequential(
  (0): MemoryModel(
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (emb_proj): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
      (1): ReLU()
    )
    (time_transformation_for_init): TimeInitTransformLinear(
      (lin): Linear(in_features=1, out_features=1, bias=True)
    )
    (message_aggregator): MessageAggregator()
    (memory_bank): MemoryBank(num_nodes=51085, memory_dim=172)
    (memory_updater): GRUMemoryUpdater(
      (memory_bank): MemoryBank(num_nodes=51085, memory_dim=172)
      (memory_updater): GRUCell(616, 172)
    )
    (embedding_module): GraphAttentionEmbedding(
      (time_encoder): TimeEncoder(
        (w): Linear(in_features=1, out_features=100, bias=True)
      )
      (temporal_conv_layers): ModuleList(
        (0): MultiHeadAttention(
          (query_projection): Linear(in_features=272, out_features=272, bias=False)
          (key_projection): Linear(in_features=444, out_features=272, bias=False)
          (value_projection): Linear(in_features=444, out_features=272, bias=False)
          (layer_norm): LayerNorm((272,), eps=1e-05, elementwise_affine=True)
          (residual_fc): Linear(in_features=272, out_features=272, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (merge_layers): ModuleList(
        (0): MergeLayer(
          (fc1): Linear(in_features=444, out_features=172, bias=True)
          (fc2): Linear(in_features=172, out_features=172, bias=True)
          (act): ReLU()
        )
      )
    )
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: TGN, #parameters: 3974904 B, 3881.7421875 KB, 3.7907638549804688 MB.
The dataset has 140777 interactions, involving 51083 different nodes
The training dataset has 76599 interactions, involving 34496 different nodes
The validation dataset has 21116 interactions, involving 10542 different nodes
The test dataset has 21117 interactions, involving 10424 different nodes
The new node validation dataset has 15534 interactions, involving 9790 different nodes
The new node test dataset has 16568 interactions, involving 9911 different nodes
5108 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/383 [00:00<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6933676600456238:   0%|                       | 0/146 [00:06<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6933676600456238:   1%|               | 1/146 [00:06<16:47,  6.95s/it]Epoch: 1, train for the 1-th batch, train loss: 0.6951776742935181:   0%|                       | 0/119 [00:07<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6954643130302429:   0%|                       | 0/383 [00:04<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6932621598243713:   0%|                       | 0/237 [00:06<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6929596066474915:   0%|                       | 0/241 [00:06<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6934496760368347:   0%|                       | 0/151 [00:07<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6951776742935181:   1%|▏              | 1/119 [00:07<13:48,  7.03s/it]Epoch: 1, train for the 1-th batch, train loss: 0.6929596066474915:   0%|               | 1/241 [00:06<24:01,  6.01s/it]Epoch: 1, train for the 1-th batch, train loss: 0.6934496760368347:   1%|               | 1/151 [00:07<17:33,  7.02s/it]Epoch: 1, train for the 1-th batch, train loss: 0.6932621598243713:   0%|               | 1/237 [00:06<23:46,  6.05s/it]Epoch: 1, train for the 1-th batch, train loss: 0.6954643130302429:   0%|               | 1/383 [00:04<28:56,  4.55s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6877005696296692:   1%|▏              | 1/119 [00:07<13:48,  7.03s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6931648254394531:   1%|               | 1/146 [00:07<16:47,  6.95s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6955755352973938:   0%|               | 1/241 [00:06<24:01,  6.01s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6931430101394653:   1%|               | 1/151 [00:07<17:33,  7.02s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6931648254394531:   1%|▏              | 2/146 [00:07<07:16,  3.03s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6908218264579773:   0%|               | 1/383 [00:04<28:56,  4.55s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6913686990737915:   0%|               | 1/237 [00:06<23:46,  6.05s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6955755352973938:   1%|               | 2/241 [00:06<10:19,  2.59s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6931430101394653:   1%|▏              | 2/151 [00:07<07:28,  3.01s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6877005696296692:   2%|▎              | 2/119 [00:07<05:52,  3.02s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6908218264579773:   1%|               | 2/383 [00:04<12:39,  1.99s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6913686990737915:   1%|▏              | 2/237 [00:06<10:13,  2.61s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6864984631538391:   2%|▎              | 2/119 [00:07<05:52,  3.02s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6932362914085388:   1%|▏              | 2/146 [00:07<07:16,  3.03s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6932362914085388:   2%|▎              | 3/146 [00:07<04:19,  1.82s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6864984631538391:   3%|▍              | 3/119 [00:07<03:29,  1.81s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6889529228210449:   1%|               | 2/241 [00:06<10:19,  2.59s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6925732493400574:   1%|▏              | 2/151 [00:07<07:28,  3.01s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6889529228210449:   1%|▏              | 3/241 [00:06<06:17,  1.59s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6925732493400574:   2%|▎              | 3/151 [00:07<04:28,  1.82s/it]Epoch: 1, train for the 3-th batch, train loss: 0.693851888179779:   1%|                | 2/383 [00:05<12:39,  1.99s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6914303302764893:   1%|▏              | 2/237 [00:06<10:13,  2.61s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6914303302764893:   1%|▏              | 3/237 [00:06<06:17,  1.61s/it]Epoch: 1, train for the 3-th batch, train loss: 0.693851888179779:   1%|▏               | 3/383 [00:05<08:06,  1.28s/it]Epoch: 1, train for the 4-th batch, train loss: 0.6785052418708801:   3%|▍              | 3/119 [00:07<03:29,  1.81s/it]Epoch: 1, train for the 4-th batch, train loss: 0.6785052418708801:   3%|▌              | 4/119 [00:07<02:10,  1.14s/it]Epoch: 1, train for the 4-th batch, train loss: 0.6931124925613403:   2%|▎              | 3/151 [00:07<04:28,  1.82s/it]Epoch: 1, train for the 4-th batch, train loss: 0.6931124925613403:   3%|▍              | 4/151 [00:07<02:47,  1.14s/it]Epoch: 1, train for the 4-th batch, train loss: 0.6905811429023743:   1%|               | 3/383 [00:05<08:06,  1.28s/it]Epoch: 1, train for the 4-th batch, train loss: 0.6905811429023743:   1%|▏              | 4/383 [00:05<05:11,  1.22it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6923882961273193:   3%|▍              | 4/151 [00:07<02:47,  1.14s/it]Epoch: 1, train for the 5-th batch, train loss: 0.6748557686805725:   3%|▌              | 4/119 [00:07<02:10,  1.14s/it]Epoch: 1, train for the 4-th batch, train loss: 0.6933436393737793:   2%|▎              | 3/146 [00:07<04:19,  1.82s/it]Epoch: 1, train for the 5-th batch, train loss: 0.6748557686805725:   4%|▋              | 5/119 [00:07<01:29,  1.27it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6933436393737793:   3%|▍              | 4/146 [00:07<02:52,  1.21s/it]Epoch: 1, train for the 4-th batch, train loss: 0.6883886456489563:   1%|▏              | 3/241 [00:06<06:17,  1.59s/it]Epoch: 1, train for the 4-th batch, train loss: 0.6883886456489563:   2%|▏              | 4/241 [00:06<04:12,  1.07s/it]Epoch: 1, train for the 5-th batch, train loss: 0.6907283067703247:   1%|▏              | 4/383 [00:05<05:11,  1.22it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6907283067703247:   1%|▏              | 5/383 [00:05<03:37,  1.74it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6906660199165344:   1%|▏              | 3/237 [00:06<06:17,  1.61s/it]Epoch: 1, train for the 4-th batch, train loss: 0.6906660199165344:   2%|▎              | 4/237 [00:06<04:14,  1.09s/it]Epoch: 1, train for the 5-th batch, train loss: 0.6879867911338806:   2%|▏              | 4/241 [00:06<04:12,  1.07s/it]Epoch: 1, train for the 5-th batch, train loss: 0.6927593946456909:   3%|▍              | 4/146 [00:08<02:52,  1.21s/it]Epoch: 1, train for the 5-th batch, train loss: 0.6927593946456909:   3%|▌              | 5/146 [00:08<01:55,  1.22it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6927867531776428:   3%|▍              | 4/151 [00:08<02:47,  1.14s/it]Epoch: 1, train for the 6-th batch, train loss: 0.6927867531776428:   4%|▌              | 6/151 [00:08<01:29,  1.63it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6888416409492493:   1%|▏              | 5/383 [00:05<03:37,  1.74it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6888416409492493:   2%|▏              | 6/383 [00:05<02:40,  2.34it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6923458576202393:   3%|▌              | 5/146 [00:08<01:55,  1.22it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6884830594062805:   2%|▎              | 4/237 [00:07<04:14,  1.09s/it]Epoch: 1, train for the 5-th batch, train loss: 0.6884830594062805:   2%|▎              | 5/237 [00:07<02:53,  1.33it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6898705959320068:   2%|▏              | 4/241 [00:07<04:12,  1.07s/it]Epoch: 1, train for the 6-th batch, train loss: 0.6898705959320068:   2%|▎              | 6/241 [00:07<02:12,  1.77it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6625056266784668:   4%|▋              | 5/119 [00:08<01:29,  1.27it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6625056266784668:   5%|▊              | 6/119 [00:08<01:08,  1.64it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6919369101524353:   4%|▌              | 6/151 [00:08<01:29,  1.63it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6919369101524353:   5%|▋              | 7/151 [00:08<01:09,  2.08it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6926030516624451:   3%|▌              | 5/146 [00:08<01:55,  1.22it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6926030516624451:   5%|▋              | 7/146 [00:08<01:02,  2.22it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6901940107345581:   2%|▏              | 6/383 [00:05<02:40,  2.34it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6901940107345581:   2%|▎              | 7/383 [00:05<02:07,  2.95it/s]Epoch: 1, train for the 6-th batch, train loss: 0.688471794128418:   2%|▎               | 5/237 [00:07<02:53,  1.33it/s]Epoch: 1, train for the 6-th batch, train loss: 0.688471794128418:   3%|▍               | 6/237 [00:07<02:05,  1.84it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6543843150138855:   5%|▊              | 6/119 [00:08<01:08,  1.64it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6543843150138855:   6%|▉              | 7/119 [00:08<00:50,  2.23it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6920763850212097:   5%|▋              | 7/146 [00:08<01:02,  2.22it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6920763850212097:   5%|▊              | 8/146 [00:08<00:49,  2.77it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6922738552093506:   5%|▋              | 7/151 [00:08<01:09,  2.08it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6842291355133057:   2%|▎              | 7/383 [00:05<02:07,  2.95it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6922738552093506:   5%|▊              | 8/151 [00:08<00:56,  2.52it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6842291355133057:   2%|▎              | 8/383 [00:05<01:41,  3.71it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6857671737670898:   2%|▎              | 6/241 [00:07<02:12,  1.77it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6857671737670898:   3%|▍              | 7/241 [00:07<01:52,  2.09it/s]Epoch: 1, train for the 9-th batch, train loss: 0.692197859287262:   5%|▊               | 8/151 [00:08<00:56,  2.52it/s]Epoch: 1, train for the 9-th batch, train loss: 0.692197859287262:   6%|▉               | 9/151 [00:08<00:44,  3.16it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6902485489845276:   3%|▍              | 6/237 [00:07<02:05,  1.84it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6899009346961975:   3%|▍              | 7/241 [00:07<01:52,  2.09it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6902485489845276:   3%|▍              | 7/237 [00:07<01:43,  2.22it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6899009346961975:   3%|▍              | 8/241 [00:07<01:28,  2.64it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6919276118278503:   5%|▊              | 8/146 [00:08<00:49,  2.77it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6919276118278503:   6%|▉              | 9/146 [00:08<00:43,  3.13it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6434661746025085:   6%|▉              | 7/119 [00:08<00:50,  2.23it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6901273727416992:   2%|▎              | 8/383 [00:06<01:41,  3.71it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6434661746025085:   7%|█              | 8/119 [00:08<00:43,  2.55it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6901273727416992:   2%|▎              | 9/383 [00:06<01:32,  4.05it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6921953558921814:   6%|▊             | 9/151 [00:08<00:44,  3.16it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6921953558921814:   7%|▊            | 10/151 [00:08<00:36,  3.87it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6873307824134827:   3%|▍              | 8/241 [00:07<01:28,  2.64it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6873307824134827:   4%|▌              | 9/241 [00:07<01:10,  3.28it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6877738833427429:   3%|▍              | 7/237 [00:07<01:43,  2.22it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6877738833427429:   3%|▌              | 8/237 [00:07<01:19,  2.86it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6921049356460571:   6%|▊             | 9/146 [00:08<00:43,  3.13it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6921049356460571:   7%|▉            | 10/146 [00:08<00:36,  3.78it/s]Epoch: 1, train for the 9-th batch, train loss: 0.641822099685669:   7%|█               | 8/119 [00:08<00:43,  2.55it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6922411918640137:   7%|▊            | 10/151 [00:08<00:36,  3.87it/s]Epoch: 1, train for the 9-th batch, train loss: 0.641822099685669:   8%|█▏              | 9/119 [00:08<00:35,  3.14it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6922411918640137:   7%|▉            | 11/151 [00:08<00:30,  4.59it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6866384744644165:   3%|▌              | 8/237 [00:07<01:19,  2.86it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6852676868438721:   4%|▌             | 9/241 [00:07<01:10,  3.28it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6866384744644165:   4%|▌              | 9/237 [00:07<01:02,  3.68it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6852676868438721:   4%|▌            | 10/241 [00:07<00:57,  4.00it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6317307353019714:   8%|█             | 9/119 [00:08<00:35,  3.14it/s]Epoch: 1, train for the 12-th batch, train loss: 0.69290691614151:   7%|█              | 11/151 [00:08<00:30,  4.59it/s]Epoch: 1, train for the 12-th batch, train loss: 0.69290691614151:   8%|█▏             | 12/151 [00:08<00:26,  5.33it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6795193552970886:   4%|▌            | 10/241 [00:07<00:57,  4.00it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6795193552970886:   5%|▌            | 11/241 [00:07<00:47,  4.81it/s]Epoch: 1, train for the 10-th batch, train loss: 0.685843288898468:   2%|▎              | 9/383 [00:06<01:32,  4.05it/s]Epoch: 1, train for the 10-th batch, train loss: 0.685843288898468:   3%|▎             | 10/383 [00:06<01:41,  3.69it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6925178170204163:   7%|▉            | 10/146 [00:08<00:36,  3.78it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6925178170204163:   8%|▉            | 11/146 [00:08<00:35,  3.85it/s]Epoch: 1, train for the 10-th batch, train loss: 0.685300886631012:   4%|▌              | 9/237 [00:07<01:02,  3.68it/s]Epoch: 1, train for the 10-th batch, train loss: 0.685300886631012:   4%|▌             | 10/237 [00:07<00:54,  4.18it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6268618106842041:   8%|█             | 9/119 [00:08<00:35,  3.14it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6268618106842041:   9%|█▏           | 11/119 [00:08<00:24,  4.41it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6820532083511353:   5%|▌            | 11/241 [00:07<00:47,  4.81it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6820532083511353:   5%|▋            | 12/241 [00:07<00:41,  5.57it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6933109760284424:   8%|█            | 12/151 [00:08<00:26,  5.33it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6933109760284424:   9%|█            | 13/151 [00:08<00:25,  5.37it/s]Epoch: 1, train for the 12-th batch, train loss: 0.692290723323822:   8%|█             | 11/146 [00:09<00:35,  3.85it/s]Epoch: 1, train for the 12-th batch, train loss: 0.692290723323822:   8%|█▏            | 12/146 [00:09<00:28,  4.62it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6815216541290283:   3%|▎            | 10/383 [00:06<01:41,  3.69it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6815216541290283:   3%|▎            | 11/383 [00:06<01:28,  4.21it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6851930618286133:   4%|▌            | 10/237 [00:08<00:54,  4.18it/s]Epoch: 1, train for the 12-th batch, train loss: 0.620071530342102:   9%|█▎            | 11/119 [00:09<00:24,  4.41it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6851930618286133:   5%|▌            | 11/237 [00:08<00:48,  4.69it/s]Epoch: 1, train for the 12-th batch, train loss: 0.620071530342102:  10%|█▍            | 12/119 [00:09<00:21,  4.98it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6808117032051086:   5%|▋            | 12/241 [00:08<00:41,  5.57it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6808117032051086:   5%|▋            | 13/241 [00:08<00:38,  5.88it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6919694542884827:   8%|█            | 12/146 [00:09<00:28,  4.62it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6919694542884827:   9%|█▏           | 13/146 [00:09<00:25,  5.31it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6931553483009338:   9%|█            | 13/151 [00:09<00:25,  5.37it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6931553483009338:   9%|█▏           | 14/151 [00:09<00:24,  5.64it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6750326156616211:   3%|▎            | 11/383 [00:06<01:28,  4.21it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6750326156616211:   3%|▍            | 12/383 [00:06<01:17,  4.77it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6850526332855225:   5%|▋            | 13/241 [00:08<00:38,  5.88it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6043916344642639:  10%|█▎           | 12/119 [00:09<00:21,  4.98it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6043916344642639:  11%|█▍           | 13/119 [00:09<00:19,  5.39it/s]Epoch: 1, train for the 14-th batch, train loss: 0.69222092628479:   9%|█▎             | 13/146 [00:09<00:25,  5.31it/s]Epoch: 1, train for the 14-th batch, train loss: 0.69222092628479:  10%|█▍             | 14/146 [00:09<00:22,  5.98it/s]Epoch: 1, train for the 12-th batch, train loss: 0.686743438243866:   5%|▋             | 11/237 [00:08<00:48,  4.69it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6915716528892517:  10%|█▏           | 14/146 [00:09<00:22,  5.98it/s]Epoch: 1, train for the 12-th batch, train loss: 0.686743438243866:   5%|▋             | 12/237 [00:08<00:51,  4.36it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6798183917999268:   3%|▍            | 12/383 [00:06<01:17,  4.77it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6798183917999268:   3%|▍            | 13/383 [00:06<01:16,  4.84it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6798098683357239:   5%|▋            | 13/241 [00:08<00:38,  5.88it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6798098683357239:   6%|▊            | 15/241 [00:08<00:36,  6.12it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6906705498695374:  10%|█▏           | 14/146 [00:09<00:22,  5.98it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6875860691070557:   5%|▋            | 12/237 [00:08<00:51,  4.36it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6906705498695374:  11%|█▍           | 16/146 [00:09<00:18,  7.19it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6875860691070557:   5%|▋            | 13/237 [00:08<00:44,  5.00it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6022683382034302:  11%|█▍           | 13/119 [00:09<00:19,  5.39it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6022683382034302:  12%|█▌           | 14/119 [00:09<00:22,  4.76it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6861404180526733:   3%|▍            | 13/383 [00:07<01:16,  4.84it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6861404180526733:   4%|▍            | 14/383 [00:07<01:08,  5.38it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6715750694274902:   6%|▊            | 15/241 [00:08<00:36,  6.12it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6715750694274902:   7%|▊            | 16/241 [00:08<00:34,  6.55it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6907978653907776:  11%|█▍           | 16/146 [00:09<00:18,  7.19it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6907978653907776:  12%|█▌           | 17/146 [00:09<00:16,  7.66it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6889598965644836:   5%|▋            | 13/237 [00:08<00:44,  5.00it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6918540000915527:   9%|█▏           | 14/151 [00:09<00:24,  5.64it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6889598965644836:   6%|▊            | 14/237 [00:08<00:39,  5.63it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6918540000915527:  10%|█▎           | 15/151 [00:09<00:34,  3.93it/s]Epoch: 1, train for the 15-th batch, train loss: 0.5914003849029541:  12%|█▌           | 14/119 [00:09<00:22,  4.76it/s]Epoch: 1, train for the 15-th batch, train loss: 0.5914003849029541:  13%|█▋           | 15/119 [00:09<00:19,  5.37it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6904875040054321:  12%|█▌           | 17/146 [00:09<00:16,  7.66it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6904875040054321:  12%|█▌           | 18/146 [00:09<00:16,  7.86it/s]Epoch: 1, train for the 16-th batch, train loss: 0.692074716091156:  10%|█▍            | 15/151 [00:09<00:34,  3.93it/s]Epoch: 1, train for the 16-th batch, train loss: 0.692074716091156:  11%|█▍            | 16/151 [00:09<00:28,  4.78it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6909773349761963:  12%|█▌           | 18/146 [00:09<00:16,  7.86it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6748449802398682:   4%|▍            | 14/383 [00:07<01:08,  5.38it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6748449802398682:   4%|▌            | 15/383 [00:07<01:18,  4.71it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6672145128250122:   7%|▊            | 16/241 [00:08<00:34,  6.55it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6672145128250122:   7%|▉            | 17/241 [00:08<00:42,  5.24it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6901561617851257:  12%|█▌           | 18/146 [00:09<00:16,  7.86it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6901561617851257:  14%|█▊           | 20/146 [00:09<00:13,  9.12it/s]Epoch: 1, train for the 16-th batch, train loss: 0.5962814092636108:  13%|█▋           | 15/119 [00:09<00:19,  5.37it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6903837323188782:   6%|▊            | 14/237 [00:08<00:39,  5.63it/s]Epoch: 1, train for the 16-th batch, train loss: 0.5962814092636108:  13%|█▋           | 16/119 [00:09<00:22,  4.62it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6918354630470276:  11%|█▍           | 16/151 [00:09<00:28,  4.78it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6903837323188782:   6%|▊            | 15/237 [00:08<00:48,  4.53it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6918354630470276:  11%|█▍           | 17/151 [00:09<00:28,  4.67it/s]Epoch: 1, train for the 16-th batch, train loss: 0.686430811882019:   4%|▌             | 15/383 [00:07<01:18,  4.71it/s]Epoch: 1, train for the 16-th batch, train loss: 0.686430811882019:   4%|▌             | 16/383 [00:07<01:13,  5.00it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6910127401351929:  14%|█▊           | 20/146 [00:09<00:13,  9.12it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6650307178497314:   7%|▉            | 17/241 [00:08<00:42,  5.24it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6910127401351929:  14%|█▊           | 21/146 [00:09<00:13,  8.96it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6650307178497314:   7%|▉            | 18/241 [00:08<00:38,  5.76it/s]Epoch: 1, train for the 17-th batch, train loss: 0.5664502382278442:  13%|█▋           | 16/119 [00:10<00:22,  4.62it/s]Epoch: 1, train for the 17-th batch, train loss: 0.5664502382278442:  14%|█▊           | 17/119 [00:10<00:19,  5.28it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6836215257644653:   6%|▊            | 15/237 [00:09<00:48,  4.53it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6836215257644653:   7%|▉            | 16/237 [00:09<00:43,  5.03it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6705319285392761:   7%|▉            | 18/241 [00:09<00:38,  5.76it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6705319285392761:   8%|█            | 19/241 [00:09<00:34,  6.49it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6925706267356873:  11%|█▍           | 17/151 [00:10<00:28,  4.67it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6925706267356873:  12%|█▌           | 18/151 [00:10<00:28,  4.66it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6906999945640564:  14%|█▊           | 21/146 [00:10<00:13,  8.96it/s]Epoch: 1, train for the 17-th batch, train loss: 0.683716356754303:   7%|▉             | 16/237 [00:09<00:43,  5.03it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6906999945640564:  15%|█▉           | 22/146 [00:10<00:16,  7.71it/s]Epoch: 1, train for the 17-th batch, train loss: 0.683716356754303:   7%|█             | 17/237 [00:09<00:37,  5.83it/s]Epoch: 1, train for the 18-th batch, train loss: 0.5653783082962036:  14%|█▊           | 17/119 [00:10<00:19,  5.28it/s]Epoch: 1, train for the 18-th batch, train loss: 0.5653783082962036:  15%|█▉           | 18/119 [00:10<00:18,  5.34it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6908954381942749:  12%|█▌           | 18/151 [00:10<00:28,  4.66it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6908954381942749:  13%|█▋           | 19/151 [00:10<00:23,  5.52it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6826464533805847:   4%|▌            | 16/383 [00:07<01:13,  5.00it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6826464533805847:   4%|▌            | 17/383 [00:07<01:25,  4.29it/s]Epoch: 1, train for the 19-th batch, train loss: 0.555121660232544:  15%|██            | 18/119 [00:10<00:18,  5.34it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6756879091262817:   7%|▉            | 17/237 [00:09<00:37,  5.83it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6756879091262817:   8%|▉            | 18/237 [00:09<00:37,  5.81it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6553226113319397:   8%|█            | 19/241 [00:09<00:34,  6.49it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6553226113319397:   8%|█            | 20/241 [00:09<00:42,  5.23it/s]Epoch: 1, train for the 23-th batch, train loss: 0.689591646194458:  15%|██            | 22/146 [00:10<00:16,  7.71it/s]Epoch: 1, train for the 23-th batch, train loss: 0.689591646194458:  16%|██▏           | 23/146 [00:10<00:18,  6.51it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6908237934112549:  13%|█▋           | 19/151 [00:10<00:23,  5.52it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6908237934112549:  13%|█▋           | 20/151 [00:10<00:22,  5.81it/s]Epoch: 1, train for the 20-th batch, train loss: 0.5542899370193481:  15%|█▉           | 18/119 [00:10<00:18,  5.34it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6843906044960022:   4%|▌            | 17/383 [00:07<01:25,  4.29it/s]Epoch: 1, train for the 20-th batch, train loss: 0.5542899370193481:  17%|██▏          | 20/119 [00:10<00:14,  6.81it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6843906044960022:   5%|▌            | 18/383 [00:07<01:16,  4.79it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6790826320648193:   8%|▉            | 18/237 [00:09<00:37,  5.81it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6856140494346619:   8%|█            | 20/241 [00:09<00:42,  5.23it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6790826320648193:   8%|█            | 19/237 [00:09<00:35,  6.12it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6856140494346619:   9%|█▏           | 21/241 [00:09<00:37,  5.85it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6896719336509705:  16%|██           | 23/146 [00:10<00:18,  6.51it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6896719336509705:  16%|██▏          | 24/146 [00:10<00:18,  6.62it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6905553936958313:  13%|█▋           | 20/151 [00:10<00:22,  5.81it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6905553936958313:  14%|█▊           | 21/151 [00:10<00:22,  5.84it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6585701107978821:   5%|▌            | 18/383 [00:08<01:16,  4.79it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6585701107978821:   5%|▋            | 19/383 [00:08<01:11,  5.06it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6689152121543884:   9%|█▏           | 21/241 [00:09<00:37,  5.85it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6689152121543884:   9%|█▏           | 22/241 [00:09<00:34,  6.37it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6833633780479431:   8%|█            | 19/237 [00:09<00:35,  6.12it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6833633780479431:   8%|█            | 20/237 [00:09<00:34,  6.29it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6896017789840698:  14%|█▊           | 21/151 [00:10<00:22,  5.84it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6896017789840698:  15%|█▉           | 22/151 [00:10<00:19,  6.48it/s]Epoch: 1, train for the 21-th batch, train loss: 0.5579237937927246:  17%|██▏          | 20/119 [00:10<00:14,  6.81it/s]Epoch: 1, train for the 21-th batch, train loss: 0.5579237937927246:  18%|██▎          | 21/119 [00:10<00:17,  5.48it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6654079556465149:   9%|█▏           | 22/241 [00:09<00:34,  6.37it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6654079556465149:  10%|█▏           | 23/241 [00:09<00:31,  6.87it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6900964379310608:  16%|██▏          | 24/146 [00:10<00:18,  6.62it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6900964379310608:  17%|██▏          | 25/146 [00:10<00:20,  5.87it/s]Epoch: 1, train for the 21-th batch, train loss: 0.683721661567688:   8%|█▏            | 20/237 [00:09<00:34,  6.29it/s]Epoch: 1, train for the 21-th batch, train loss: 0.683721661567688:   9%|█▏            | 21/237 [00:09<00:31,  6.79it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6904237866401672:  15%|█▉           | 22/151 [00:10<00:19,  6.48it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6904237866401672:  15%|█▉           | 23/151 [00:10<00:19,  6.56it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6641821265220642:   5%|▋            | 19/383 [00:08<01:11,  5.06it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6641821265220642:   5%|▋            | 20/383 [00:08<01:16,  4.76it/s]Epoch: 1, train for the 22-th batch, train loss: 0.5364669561386108:  18%|██▎          | 21/119 [00:10<00:17,  5.48it/s]Epoch: 1, train for the 22-th batch, train loss: 0.5364669561386108:  18%|██▍          | 22/119 [00:10<00:17,  5.68it/s]Epoch: 1, train for the 24-th batch, train loss: 0.67650306224823:  10%|█▍             | 23/241 [00:09<00:31,  6.87it/s]Epoch: 1, train for the 24-th batch, train loss: 0.67650306224823:  10%|█▍             | 24/241 [00:09<00:31,  6.91it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6785888671875:   9%|█▍              | 21/237 [00:09<00:31,  6.79it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6893233060836792:  17%|██▏          | 25/146 [00:10<00:20,  5.87it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6893233060836792:  18%|██▎          | 26/146 [00:10<00:19,  6.17it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6785888671875:   9%|█▍              | 22/237 [00:09<00:30,  7.03it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6908940076828003:  15%|█▉           | 23/151 [00:10<00:19,  6.56it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6908940076828003:  16%|██           | 24/151 [00:10<00:18,  6.88it/s]Epoch: 1, train for the 23-th batch, train loss: 0.5390170812606812:  18%|██▍          | 22/119 [00:10<00:17,  5.68it/s]Epoch: 1, train for the 23-th batch, train loss: 0.5390170812606812:  19%|██▌          | 23/119 [00:10<00:15,  6.23it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6725098490715027:   5%|▋            | 20/383 [00:08<01:16,  4.76it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6725098490715027:   5%|▋            | 21/383 [00:08<01:16,  4.72it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6917375326156616:  16%|██           | 24/151 [00:11<00:18,  6.88it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6748461723327637:  10%|█▎           | 24/241 [00:10<00:31,  6.91it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6917375326156616:  17%|██▏          | 25/151 [00:11<00:18,  6.99it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6748461723327637:  10%|█▎           | 25/241 [00:10<00:36,  5.90it/s]Epoch: 1, train for the 24-th batch, train loss: 0.5423991084098816:  19%|██▌          | 23/119 [00:11<00:15,  6.23it/s]Epoch: 1, train for the 24-th batch, train loss: 0.5423991084098816:  20%|██▌          | 24/119 [00:11<00:14,  6.62it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6883431673049927:  18%|██▎          | 26/146 [00:11<00:19,  6.17it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6772826910018921:   9%|█▏           | 22/237 [00:10<00:30,  7.03it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6883431673049927:  18%|██▍          | 27/146 [00:11<00:22,  5.30it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6772826910018921:  10%|█▎           | 23/237 [00:10<00:37,  5.64it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6788504719734192:   5%|▋            | 21/383 [00:08<01:16,  4.72it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6788504719734192:   6%|▋            | 22/383 [00:08<01:07,  5.34it/s]Epoch: 1, train for the 25-th batch, train loss: 0.5115311145782471:  20%|██▌          | 24/119 [00:11<00:14,  6.62it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6902743577957153:  17%|██▏          | 25/151 [00:11<00:18,  6.99it/s]Epoch: 1, train for the 25-th batch, train loss: 0.5115311145782471:  21%|██▋          | 25/119 [00:11<00:13,  7.07it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6902743577957153:  17%|██▏          | 26/151 [00:11<00:17,  7.11it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6800395250320435:  10%|█▎           | 25/241 [00:10<00:36,  5.90it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6800395250320435:  11%|█▍           | 26/241 [00:10<00:35,  5.99it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6785188317298889:  10%|█▎           | 23/237 [00:10<00:37,  5.64it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6874198913574219:  18%|██▍          | 27/146 [00:11<00:22,  5.30it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6785188317298889:  10%|█▎           | 24/237 [00:10<00:35,  6.02it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6874198913574219:  19%|██▍          | 28/146 [00:11<00:20,  5.62it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6789559721946716:   6%|▋            | 22/383 [00:08<01:07,  5.34it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6789559721946716:   6%|▊            | 23/383 [00:08<01:02,  5.74it/s]Epoch: 1, train for the 26-th batch, train loss: 0.5211037397384644:  21%|██▋          | 25/119 [00:11<00:13,  7.07it/s]Epoch: 1, train for the 26-th batch, train loss: 0.5211037397384644:  22%|██▊          | 26/119 [00:11<00:12,  7.30it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6903045773506165:  17%|██▏          | 26/151 [00:11<00:17,  7.11it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6903045773506165:  18%|██▎          | 27/151 [00:11<00:18,  6.72it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6681458950042725:  11%|█▍           | 26/241 [00:10<00:35,  5.99it/s]Epoch: 1, train for the 29-th batch, train loss: 0.688101589679718:  19%|██▋           | 28/146 [00:11<00:20,  5.62it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6681458950042725:  11%|█▍           | 27/241 [00:10<00:34,  6.20it/s]Epoch: 1, train for the 29-th batch, train loss: 0.688101589679718:  20%|██▊           | 29/146 [00:11<00:18,  6.23it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6739967465400696:  10%|█▎           | 24/237 [00:10<00:35,  6.02it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6739967465400696:  11%|█▎           | 25/237 [00:10<00:33,  6.26it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6519227623939514:   6%|▊            | 23/383 [00:08<01:02,  5.74it/s]Epoch: 1, train for the 27-th batch, train loss: 0.5441939234733582:  22%|██▊          | 26/119 [00:11<00:12,  7.30it/s]Epoch: 1, train for the 27-th batch, train loss: 0.5441939234733582:  23%|██▉          | 27/119 [00:11<00:12,  7.56it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6519227623939514:   6%|▊            | 24/383 [00:08<00:59,  6.03it/s]Epoch: 1, train for the 25-th batch, train loss: 0.672733724117279:   6%|▉             | 24/383 [00:09<00:59,  6.03it/s]Epoch: 1, train for the 25-th batch, train loss: 0.672733724117279:   7%|▉             | 25/383 [00:09<00:53,  6.64it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6408511996269226:  11%|█▍           | 27/241 [00:10<00:34,  6.20it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6408511996269226:  12%|█▌           | 28/241 [00:10<00:36,  5.77it/s]Epoch: 1, train for the 28-th batch, train loss: 0.5517826080322266:  23%|██▉          | 27/119 [00:11<00:12,  7.56it/s]Epoch: 1, train for the 28-th batch, train loss: 0.5517826080322266:  24%|███          | 28/119 [00:11<00:14,  6.33it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6714299917221069:   7%|▊            | 25/383 [00:09<00:53,  6.64it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6714299917221069:   7%|▉            | 26/383 [00:09<00:51,  6.94it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6896946430206299:  20%|██▌          | 29/146 [00:11<00:18,  6.23it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6718084812164307:  12%|█▌           | 28/241 [00:10<00:36,  5.77it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6896946430206299:  21%|██▋          | 30/146 [00:11<00:24,  4.72it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6911153793334961:  18%|██▎          | 27/151 [00:11<00:18,  6.72it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6619676947593689:  11%|█▎           | 25/237 [00:10<00:33,  6.26it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6718084812164307:  12%|█▌           | 29/241 [00:10<00:34,  6.13it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6911153793334961:  19%|██▍          | 28/151 [00:11<00:26,  4.69it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6619676947593689:  11%|█▍           | 26/237 [00:10<00:44,  4.74it/s]Epoch: 1, train for the 29-th batch, train loss: 0.5080628991127014:  24%|███          | 28/119 [00:11<00:14,  6.33it/s]Epoch: 1, train for the 29-th batch, train loss: 0.5080628991127014:  24%|███▏         | 29/119 [00:11<00:12,  6.94it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6866953372955322:  21%|██▋          | 30/146 [00:11<00:24,  4.72it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6866953372955322:  21%|██▊          | 31/146 [00:11<00:21,  5.36it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6878945827484131:  12%|█▌           | 29/241 [00:10<00:34,  6.13it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6878945827484131:  12%|█▌           | 30/241 [00:10<00:33,  6.24it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6440620422363281:   7%|▉            | 26/383 [00:09<00:51,  6.94it/s]Epoch: 1, train for the 30-th batch, train loss: 0.5010730028152466:  24%|███▏         | 29/119 [00:11<00:12,  6.94it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6440620422363281:   7%|▉            | 27/383 [00:09<00:57,  6.18it/s]Epoch: 1, train for the 30-th batch, train loss: 0.5010730028152466:  25%|███▎         | 30/119 [00:11<00:12,  7.25it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6915433406829834:  19%|██▍          | 28/151 [00:11<00:26,  4.69it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6915433406829834:  19%|██▍          | 29/151 [00:11<00:25,  4.86it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6873688101768494:  21%|██▊          | 31/146 [00:11<00:21,  5.36it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6873688101768494:  22%|██▊          | 32/146 [00:11<00:18,  6.15it/s]Epoch: 1, train for the 31-th batch, train loss: 0.4996492266654968:  25%|███▎         | 30/119 [00:12<00:12,  7.25it/s]Epoch: 1, train for the 31-th batch, train loss: 0.4996492266654968:  26%|███▍         | 31/119 [00:12<00:11,  7.61it/s]Epoch: 1, train for the 27-th batch, train loss: 0.671576976776123:  11%|█▌            | 26/237 [00:11<00:44,  4.74it/s]Epoch: 1, train for the 27-th batch, train loss: 0.671576976776123:  11%|█▌            | 27/237 [00:11<00:50,  4.17it/s]Epoch: 1, train for the 31-th batch, train loss: 0.641399621963501:  12%|█▋            | 30/241 [00:11<00:33,  6.24it/s]Epoch: 1, train for the 31-th batch, train loss: 0.641399621963501:  13%|█▊            | 31/241 [00:11<00:35,  5.91it/s]Epoch: 1, train for the 32-th batch, train loss: 0.503924548625946:  26%|███▋          | 31/119 [00:12<00:11,  7.61it/s]Epoch: 1, train for the 32-th batch, train loss: 0.503924548625946:  27%|███▊          | 32/119 [00:12<00:10,  7.99it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6906538009643555:  19%|██▍          | 29/151 [00:12<00:25,  4.86it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6906538009643555:  20%|██▌          | 30/151 [00:12<00:26,  4.58it/s]Epoch: 1, train for the 33-th batch, train loss: 0.687063992023468:  22%|███           | 32/146 [00:12<00:18,  6.15it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6294784545898438:  13%|█▋           | 31/241 [00:11<00:35,  5.91it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6775943040847778:  11%|█▍           | 27/237 [00:11<00:50,  4.17it/s]Epoch: 1, train for the 33-th batch, train loss: 0.687063992023468:  23%|███▏          | 33/146 [00:12<00:20,  5.48it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6294784545898438:  13%|█▋           | 32/241 [00:11<00:32,  6.51it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6775943040847778:  12%|█▌           | 28/237 [00:11<00:44,  4.72it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6289169192314148:   7%|▉            | 27/383 [00:09<00:57,  6.18it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6289169192314148:   7%|▉            | 28/383 [00:09<01:15,  4.71it/s]Epoch: 1, train for the 33-th batch, train loss: 0.5442340970039368:  27%|███▍         | 32/119 [00:12<00:10,  7.99it/s]Epoch: 1, train for the 33-th batch, train loss: 0.5442340970039368:  28%|███▌         | 33/119 [00:12<00:10,  8.23it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6908412575721741:  20%|██▌          | 30/151 [00:12<00:26,  4.58it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6908412575721741:  21%|██▋          | 31/151 [00:12<00:23,  5.13it/s]Epoch: 1, train for the 34-th batch, train loss: 0.687424898147583:  23%|███▏          | 33/146 [00:12<00:20,  5.48it/s]Epoch: 1, train for the 34-th batch, train loss: 0.687424898147583:  23%|███▎          | 34/146 [00:12<00:19,  5.87it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6865142583847046:  23%|███          | 34/146 [00:12<00:19,  5.87it/s]Epoch: 1, train for the 29-th batch, train loss: 0.661996066570282:  12%|█▋            | 28/237 [00:11<00:44,  4.72it/s]Epoch: 1, train for the 29-th batch, train loss: 0.661996066570282:  12%|█▋            | 29/237 [00:11<00:50,  4.09it/s]Epoch: 1, train for the 34-th batch, train loss: 0.5116332173347473:  28%|███▌         | 33/119 [00:12<00:10,  8.23it/s]Epoch: 1, train for the 34-th batch, train loss: 0.5116332173347473:  29%|███▋         | 34/119 [00:12<00:14,  5.87it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6174017786979675:   7%|▉            | 28/383 [00:10<01:15,  4.71it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6887381672859192:  21%|██▋          | 31/151 [00:12<00:23,  5.13it/s]Epoch: 1, train for the 35-th batch, train loss: 0.5099931955337524:  29%|███▋         | 34/119 [00:12<00:14,  5.87it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6743389964103699:  13%|█▋           | 32/241 [00:11<00:32,  6.51it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6174017786979675:   8%|▉            | 29/383 [00:10<01:37,  3.64it/s]Epoch: 1, train for the 35-th batch, train loss: 0.5099931955337524:  29%|███▊         | 35/119 [00:12<00:13,  6.35it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6887381672859192:  21%|██▊          | 32/151 [00:12<00:28,  4.17it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6743389964103699:  14%|█▊           | 33/241 [00:11<00:51,  4.02it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6578777432441711:  12%|█▌           | 29/237 [00:11<00:50,  4.09it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6844889521598816:  23%|███          | 34/146 [00:12<00:19,  5.87it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6844889521598816:  25%|███▏         | 36/146 [00:12<00:19,  5.75it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6578777432441711:  13%|█▋           | 30/237 [00:11<00:46,  4.48it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6413671970367432:  14%|█▊           | 33/241 [00:11<00:51,  4.02it/s]Epoch: 1, train for the 36-th batch, train loss: 0.5025727152824402:  29%|███▊         | 35/119 [00:12<00:13,  6.35it/s]Epoch: 1, train for the 30-th batch, train loss: 0.615598201751709:   8%|█             | 29/383 [00:10<01:37,  3.64it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6413671970367432:  14%|█▊           | 34/241 [00:11<00:47,  4.36it/s]Epoch: 1, train for the 36-th batch, train loss: 0.5025727152824402:  30%|███▉         | 36/119 [00:12<00:13,  5.99it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6850486397743225:  25%|███▏         | 36/146 [00:12<00:19,  5.75it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6850486397743225:  25%|███▎         | 37/146 [00:12<00:18,  5.78it/s]Epoch: 1, train for the 30-th batch, train loss: 0.615598201751709:   8%|█             | 30/383 [00:10<01:29,  3.94it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6886614561080933:  21%|██▊          | 32/151 [00:12<00:28,  4.17it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6886614561080933:  22%|██▊          | 33/151 [00:12<00:27,  4.22it/s]Epoch: 1, train for the 38-th batch, train loss: 0.6845793128013611:  25%|███▎         | 37/146 [00:13<00:18,  5.78it/s]Epoch: 1, train for the 38-th batch, train loss: 0.6845793128013611:  26%|███▍         | 38/146 [00:13<00:17,  6.14it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6886085271835327:  22%|██▊          | 33/151 [00:13<00:27,  4.22it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6612372994422913:  13%|█▋           | 30/237 [00:12<00:46,  4.48it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6886085271835327:  23%|██▉          | 34/151 [00:13<00:23,  4.91it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6612372994422913:  13%|█▋           | 31/237 [00:12<00:52,  3.90it/s]Epoch: 1, train for the 37-th batch, train loss: 0.5140180587768555:  30%|███▉         | 36/119 [00:13<00:13,  5.99it/s]Epoch: 1, train for the 37-th batch, train loss: 0.5140180587768555:  31%|████         | 37/119 [00:13<00:14,  5.51it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6190183758735657:   8%|█            | 30/383 [00:10<01:29,  3.94it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6611121892929077:  14%|█▊           | 34/241 [00:12<00:47,  4.36it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6190183758735657:   8%|█            | 31/383 [00:10<01:25,  4.10it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6611121892929077:  15%|█▉           | 35/241 [00:12<00:48,  4.26it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6844448447227478:  26%|███▍         | 38/146 [00:13<00:17,  6.14it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6844448447227478:  27%|███▍         | 39/146 [00:13<00:16,  6.50it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6878610849380493:  23%|██▉          | 34/151 [00:13<00:23,  4.91it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6878610849380493:  23%|███          | 35/151 [00:13<00:21,  5.50it/s]Epoch: 1, train for the 32-th batch, train loss: 0.677121102809906:  13%|█▊            | 31/237 [00:12<00:52,  3.90it/s]Epoch: 1, train for the 32-th batch, train loss: 0.677121102809906:  14%|█▉            | 32/237 [00:12<00:46,  4.41it/s]Epoch: 1, train for the 38-th batch, train loss: 0.505577564239502:  31%|████▎         | 37/119 [00:13<00:14,  5.51it/s]Epoch: 1, train for the 38-th batch, train loss: 0.505577564239502:  32%|████▍         | 38/119 [00:13<00:13,  5.83it/s]Epoch: 1, train for the 36-th batch, train loss: 0.660404622554779:  15%|██            | 35/241 [00:12<00:48,  4.26it/s]Epoch: 1, train for the 36-th batch, train loss: 0.660404622554779:  15%|██            | 36/241 [00:12<00:42,  4.86it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6589623093605042:   8%|█            | 31/383 [00:10<01:25,  4.10it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6844692826271057:  27%|███▍         | 39/146 [00:13<00:16,  6.50it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6589623093605042:   8%|█            | 32/383 [00:10<01:17,  4.51it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6844692826271057:  27%|███▌         | 40/146 [00:13<00:15,  6.69it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6888717412948608:  23%|███          | 35/151 [00:13<00:21,  5.50it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6888717412948608:  24%|███          | 36/151 [00:13<00:19,  5.89it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6565531492233276:  14%|█▊           | 32/237 [00:12<00:46,  4.41it/s]Epoch: 1, train for the 39-th batch, train loss: 0.5215393900871277:  32%|████▏        | 38/119 [00:13<00:13,  5.83it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6565531492233276:  14%|█▊           | 33/237 [00:12<00:41,  4.95it/s]Epoch: 1, train for the 39-th batch, train loss: 0.5215393900871277:  33%|████▎        | 39/119 [00:13<00:12,  6.44it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6908580660820007:  24%|███          | 36/151 [00:13<00:19,  5.89it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6908580660820007:  25%|███▏         | 37/151 [00:13<00:17,  6.51it/s]Epoch: 1, train for the 40-th batch, train loss: 0.5490800738334656:  33%|████▎        | 39/119 [00:13<00:12,  6.44it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6815719604492188:  27%|███▌         | 40/146 [00:13<00:15,  6.69it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6815719604492188:  28%|███▋         | 41/146 [00:13<00:18,  5.82it/s]Epoch: 1, train for the 41-th batch, train loss: 0.5577796101570129:  33%|████▎        | 39/119 [00:13<00:12,  6.44it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6167357563972473:  15%|█▉           | 36/241 [00:12<00:42,  4.86it/s]Epoch: 1, train for the 41-th batch, train loss: 0.5577796101570129:  34%|████▍        | 41/119 [00:13<00:10,  7.61it/s]Epoch: 1, train for the 38-th batch, train loss: 0.6888128519058228:  25%|███▏         | 37/151 [00:13<00:17,  6.51it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6167357563972473:  15%|█▉           | 37/241 [00:12<00:48,  4.21it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6441068053245544:  14%|█▊           | 33/237 [00:12<00:41,  4.95it/s]Epoch: 1, train for the 38-th batch, train loss: 0.6888128519058228:  25%|███▎         | 38/151 [00:13<00:17,  6.60it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6441068053245544:  14%|█▊           | 34/237 [00:12<00:43,  4.71it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6634975075721741:   8%|█            | 32/383 [00:11<01:17,  4.51it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6827685236930847:  28%|███▋         | 41/146 [00:13<00:18,  5.82it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6827685236930847:  29%|███▋         | 42/146 [00:13<00:16,  6.43it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6634975075721741:   9%|█            | 33/383 [00:11<01:31,  3.81it/s]Epoch: 1, train for the 42-th batch, train loss: 0.5644547343254089:  34%|████▍        | 41/119 [00:13<00:10,  7.61it/s]Epoch: 1, train for the 38-th batch, train loss: 0.6381710767745972:  15%|█▉           | 37/241 [00:12<00:48,  4.21it/s]Epoch: 1, train for the 42-th batch, train loss: 0.5644547343254089:  35%|████▌        | 42/119 [00:13<00:10,  7.29it/s]Epoch: 1, train for the 38-th batch, train loss: 0.6381710767745972:  16%|██           | 38/241 [00:12<00:43,  4.71it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6864159107208252:  25%|███▎         | 38/151 [00:13<00:17,  6.60it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6613900661468506:  14%|█▊           | 34/237 [00:12<00:43,  4.71it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6799586415290833:  29%|███▋         | 42/146 [00:13<00:16,  6.43it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6799586415290833:  29%|███▊         | 43/146 [00:13<00:15,  6.82it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6864159107208252:  26%|███▎         | 39/151 [00:13<00:17,  6.33it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6613900661468506:  15%|█▉           | 35/237 [00:12<00:40,  5.01it/s]Epoch: 1, train for the 43-th batch, train loss: 0.5235655903816223:  35%|████▌        | 42/119 [00:13<00:10,  7.29it/s]Epoch: 1, train for the 43-th batch, train loss: 0.5235655903816223:  36%|████▋        | 43/119 [00:13<00:09,  7.60it/s]Epoch: 1, train for the 39-th batch, train loss: 0.658586859703064:  16%|██▏           | 38/241 [00:12<00:43,  4.71it/s]Epoch: 1, train for the 39-th batch, train loss: 0.658586859703064:  16%|██▎           | 39/241 [00:12<00:37,  5.35it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6844112873077393:  26%|███▎         | 39/151 [00:13<00:17,  6.33it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6844112873077393:  26%|███▍         | 40/151 [00:13<00:18,  5.97it/s]Epoch: 1, train for the 44-th batch, train loss: 0.4983387887477875:  36%|████▋        | 43/119 [00:13<00:09,  7.60it/s]Epoch: 1, train for the 44-th batch, train loss: 0.4983387887477875:  37%|████▊        | 44/119 [00:13<00:09,  7.84it/s]Epoch: 1, train for the 34-th batch, train loss: 0.632737398147583:   9%|█▏            | 33/383 [00:11<01:31,  3.81it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6570900082588196:  15%|█▉           | 35/237 [00:12<00:40,  5.01it/s]Epoch: 1, train for the 34-th batch, train loss: 0.632737398147583:   9%|█▏            | 34/383 [00:11<01:39,  3.51it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6570900082588196:  15%|█▉           | 36/237 [00:12<00:41,  4.85it/s]Epoch: 1, train for the 44-th batch, train loss: 0.6779223680496216:  29%|███▊         | 43/146 [00:13<00:15,  6.82it/s]Epoch: 1, train for the 44-th batch, train loss: 0.6779223680496216:  30%|███▉         | 44/146 [00:13<00:17,  5.68it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6698030233383179:  16%|██           | 39/241 [00:12<00:37,  5.35it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6698030233383179:  17%|██▏          | 40/241 [00:13<00:38,  5.27it/s]Epoch: 1, train for the 45-th batch, train loss: 0.47387441992759705:  37%|████▍       | 44/119 [00:14<00:09,  7.84it/s]Epoch: 1, train for the 45-th batch, train loss: 0.47387441992759705:  38%|████▌       | 45/119 [00:14<00:09,  7.93it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6861284971237183:  26%|███▍         | 40/151 [00:14<00:18,  5.97it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6861284971237183:  27%|███▌         | 41/151 [00:14<00:17,  6.15it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6819929480552673:  30%|███▉         | 44/146 [00:14<00:17,  5.68it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6819929480552673:  31%|████         | 45/146 [00:14<00:16,  6.09it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6354662179946899:   9%|█▏           | 34/383 [00:11<01:39,  3.51it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6298208236694336:  17%|██▏          | 40/241 [00:13<00:38,  5.27it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6354662179946899:   9%|█▏           | 35/383 [00:11<01:28,  3.93it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6298208236694336:  17%|██▏          | 41/241 [00:13<00:33,  6.04it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6658052206039429:  15%|█▉           | 36/237 [00:13<00:41,  4.85it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6658052206039429:  16%|██           | 37/237 [00:13<00:40,  4.94it/s]Epoch: 1, train for the 46-th batch, train loss: 0.5446685552597046:  38%|████▉        | 45/119 [00:14<00:09,  7.93it/s]Epoch: 1, train for the 46-th batch, train loss: 0.5446685552597046:  39%|█████        | 46/119 [00:14<00:09,  7.98it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6746638417243958:  31%|████         | 45/146 [00:14<00:16,  6.09it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6746638417243958:  32%|████         | 46/146 [00:14<00:15,  6.55it/s]Epoch: 1, train for the 47-th batch, train loss: 0.5927447080612183:  39%|█████        | 46/119 [00:14<00:09,  7.98it/s]Epoch: 1, train for the 47-th batch, train loss: 0.5927447080612183:  39%|█████▏       | 47/119 [00:14<00:09,  7.90it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6270061135292053:   9%|█▏           | 35/383 [00:11<01:28,  3.93it/s]Epoch: 1, train for the 38-th batch, train loss: 0.6554185152053833:  16%|██           | 37/237 [00:13<00:40,  4.94it/s]Epoch: 1, train for the 38-th batch, train loss: 0.6554185152053833:  16%|██           | 38/237 [00:13<00:40,  4.96it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6270061135292053:   9%|█▏           | 36/383 [00:11<01:24,  4.10it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6850364804267883:  27%|███▌         | 41/151 [00:14<00:17,  6.15it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6346965432167053:  17%|██▏          | 41/241 [00:13<00:33,  6.04it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6850364804267883:  28%|███▌         | 42/151 [00:14<00:23,  4.66it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6346965432167053:  17%|██▎          | 42/241 [00:13<00:39,  5.04it/s]Epoch: 1, train for the 48-th batch, train loss: 0.532701849937439:  39%|█████▌        | 47/119 [00:14<00:09,  7.90it/s]Epoch: 1, train for the 48-th batch, train loss: 0.532701849937439:  40%|█████▋        | 48/119 [00:14<00:10,  7.03it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6783212423324585:  32%|████         | 46/146 [00:14<00:15,  6.55it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6783212423324585:  32%|████▏        | 47/146 [00:14<00:18,  5.30it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6751935482025146:  16%|██           | 38/237 [00:13<00:40,  4.96it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6082735061645508:   9%|█▏           | 36/383 [00:12<01:24,  4.10it/s]Epoch: 1, train for the 43-th batch, train loss: 0.656545877456665:  17%|██▍           | 42/241 [00:13<00:39,  5.04it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6751935482025146:  16%|██▏          | 39/237 [00:13<00:40,  4.95it/s]Epoch: 1, train for the 43-th batch, train loss: 0.656545877456665:  18%|██▍           | 43/241 [00:13<00:36,  5.45it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6082735061645508:  10%|█▎           | 37/383 [00:12<01:20,  4.31it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6848234534263611:  28%|███▌         | 42/151 [00:14<00:23,  4.66it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6848234534263611:  28%|███▋         | 43/151 [00:14<00:21,  4.95it/s]Epoch: 1, train for the 49-th batch, train loss: 0.582549512386322:  40%|█████▋        | 48/119 [00:14<00:10,  7.03it/s]Epoch: 1, train for the 49-th batch, train loss: 0.582549512386322:  41%|█████▊        | 49/119 [00:14<00:09,  7.23it/s]Epoch: 1, train for the 48-th batch, train loss: 0.6777830123901367:  32%|████▏        | 47/146 [00:14<00:18,  5.30it/s]Epoch: 1, train for the 48-th batch, train loss: 0.6777830123901367:  33%|████▎        | 48/146 [00:14<00:17,  5.70it/s]Epoch: 1, train for the 44-th batch, train loss: 0.6726495027542114:  18%|██▎          | 43/241 [00:13<00:36,  5.45it/s]Epoch: 1, train for the 44-th batch, train loss: 0.6726495027542114:  18%|██▎          | 44/241 [00:13<00:34,  5.75it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6460064649581909:  16%|██▏          | 39/237 [00:13<00:40,  4.95it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6460064649581909:  17%|██▏          | 40/237 [00:13<00:38,  5.12it/s]Epoch: 1, train for the 38-th batch, train loss: 0.630377471446991:  10%|█▎            | 37/383 [00:12<01:20,  4.31it/s]Epoch: 1, train for the 38-th batch, train loss: 0.630377471446991:  10%|█▍            | 38/383 [00:12<01:18,  4.41it/s]Epoch: 1, train for the 44-th batch, train loss: 0.680027425289154:  28%|███▉          | 43/151 [00:14<00:21,  4.95it/s]Epoch: 1, train for the 44-th batch, train loss: 0.680027425289154:  29%|████          | 44/151 [00:14<00:23,  4.53it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6033957004547119:  18%|██▎          | 44/241 [00:13<00:34,  5.75it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6033957004547119:  19%|██▍          | 45/241 [00:13<00:32,  6.05it/s]Epoch: 1, train for the 50-th batch, train loss: 0.551712691783905:  41%|█████▊        | 49/119 [00:14<00:09,  7.23it/s]Epoch: 1, train for the 50-th batch, train loss: 0.551712691783905:  42%|█████▉        | 50/119 [00:14<00:13,  5.14it/s]Epoch: 1, train for the 39-th batch, train loss: 0.5764803886413574:  10%|█▎           | 38/383 [00:12<01:18,  4.41it/s]Epoch: 1, train for the 39-th batch, train loss: 0.5764803886413574:  10%|█▎           | 39/383 [00:12<01:14,  4.64it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6747239828109741:  33%|████▎        | 48/146 [00:14<00:17,  5.70it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6747239828109741:  34%|████▎        | 49/146 [00:14<00:21,  4.55it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6513422727584839:  17%|██▏          | 40/237 [00:14<00:38,  5.12it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6789929270744324:  29%|███▊         | 44/151 [00:14<00:23,  4.53it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6100395321846008:  19%|██▍          | 45/241 [00:13<00:32,  6.05it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6513422727584839:  17%|██▏          | 41/237 [00:14<00:42,  4.63it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6789929270744324:  30%|███▊         | 45/151 [00:14<00:21,  4.97it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6100395321846008:  19%|██▍          | 46/241 [00:13<00:32,  6.07it/s]Epoch: 1, train for the 51-th batch, train loss: 0.5718179941177368:  42%|█████▍       | 50/119 [00:15<00:13,  5.14it/s]Epoch: 1, train for the 51-th batch, train loss: 0.5718179941177368:  43%|█████▌       | 51/119 [00:15<00:12,  5.46it/s]Epoch: 1, train for the 50-th batch, train loss: 0.6723211407661438:  34%|████▎        | 49/146 [00:15<00:21,  4.55it/s]Epoch: 1, train for the 50-th batch, train loss: 0.6723211407661438:  34%|████▍        | 50/146 [00:15<00:18,  5.08it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6519995927810669:  19%|██▍          | 46/241 [00:14<00:32,  6.07it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6519995927810669:  20%|██▌          | 47/241 [00:14<00:31,  6.13it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6818314790725708:  30%|███▊         | 45/151 [00:15<00:21,  4.97it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6818314790725708:  30%|███▉         | 46/151 [00:15<00:20,  5.02it/s]Epoch: 1, train for the 48-th batch, train loss: 0.6310949921607971:  20%|██▌          | 47/241 [00:14<00:31,  6.13it/s]Epoch: 1, train for the 48-th batch, train loss: 0.6310949921607971:  20%|██▌          | 48/241 [00:14<00:28,  6.88it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6654972434043884:  17%|██▏          | 41/237 [00:14<00:42,  4.63it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6654972434043884:  18%|██▎          | 42/237 [00:14<00:47,  4.10it/s]Epoch: 1, train for the 40-th batch, train loss: 0.5951055884361267:  10%|█▎           | 39/383 [00:12<01:14,  4.64it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6835538148880005:  30%|███▉         | 46/151 [00:15<00:20,  5.02it/s]Epoch: 1, train for the 40-th batch, train loss: 0.5951055884361267:  10%|█▎           | 40/383 [00:12<01:35,  3.58it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6835538148880005:  31%|████         | 47/151 [00:15<00:20,  4.95it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6312721967697144:  20%|██▌          | 48/241 [00:14<00:28,  6.88it/s]Epoch: 1, train for the 51-th batch, train loss: 0.6750605702400208:  34%|████▍        | 50/146 [00:15<00:18,  5.08it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6312721967697144:  20%|██▋          | 49/241 [00:14<00:27,  6.87it/s]Epoch: 1, train for the 51-th batch, train loss: 0.6750605702400208:  35%|████▌        | 51/146 [00:15<00:21,  4.34it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6571954488754272:  18%|██▎          | 42/237 [00:14<00:47,  4.10it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6571954488754272:  18%|██▎          | 43/237 [00:14<00:42,  4.61it/s]Epoch: 1, train for the 52-th batch, train loss: 0.5599741339683533:  43%|█████▌       | 51/119 [00:15<00:12,  5.46it/s]Epoch: 1, train for the 52-th batch, train loss: 0.5599741339683533:  44%|█████▋       | 52/119 [00:15<00:16,  4.15it/s]Epoch: 1, train for the 41-th batch, train loss: 0.5473180413246155:  10%|█▎           | 40/383 [00:13<01:35,  3.58it/s]Epoch: 1, train for the 48-th batch, train loss: 0.6748902797698975:  31%|████         | 47/151 [00:15<00:20,  4.95it/s]Epoch: 1, train for the 41-th batch, train loss: 0.5473180413246155:  11%|█▍           | 41/383 [00:13<01:22,  4.14it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6718463897705078:  35%|████▌        | 51/146 [00:15<00:21,  4.34it/s]Epoch: 1, train for the 48-th batch, train loss: 0.6748902797698975:  32%|████▏        | 48/151 [00:15<00:19,  5.37it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6718463897705078:  36%|████▋        | 52/146 [00:15<00:18,  5.03it/s]Epoch: 1, train for the 53-th batch, train loss: 0.5153856873512268:  44%|█████▋       | 52/119 [00:15<00:16,  4.15it/s]Epoch: 1, train for the 53-th batch, train loss: 0.5153856873512268:  45%|█████▊       | 53/119 [00:15<00:13,  4.81it/s]Epoch: 1, train for the 44-th batch, train loss: 0.6596606969833374:  18%|██▎          | 43/237 [00:14<00:42,  4.61it/s]Epoch: 1, train for the 44-th batch, train loss: 0.6596606969833374:  19%|██▍          | 44/237 [00:14<00:39,  4.84it/s]Epoch: 1, train for the 50-th batch, train loss: 0.6156671047210693:  20%|██▋          | 49/241 [00:14<00:27,  6.87it/s]Epoch: 1, train for the 50-th batch, train loss: 0.6156671047210693:  21%|██▋          | 50/241 [00:14<00:37,  5.04it/s]Epoch: 1, train for the 51-th batch, train loss: 0.6178447008132935:  21%|██▋          | 50/241 [00:14<00:37,  5.04it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6813427805900574:  32%|████▏        | 48/151 [00:15<00:19,  5.37it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6813427805900574:  32%|████▏        | 49/151 [00:15<00:23,  4.42it/s]Epoch: 1, train for the 42-th batch, train loss: 0.5926491618156433:  11%|█▍           | 41/383 [00:13<01:22,  4.14it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6590309143066406:  19%|██▍          | 44/237 [00:14<00:39,  4.84it/s]Epoch: 1, train for the 42-th batch, train loss: 0.5926491618156433:  11%|█▍           | 42/383 [00:13<01:33,  3.63it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6590309143066406:  19%|██▍          | 45/237 [00:14<00:42,  4.47it/s]Epoch: 1, train for the 53-th batch, train loss: 0.6722132563591003:  36%|████▋        | 52/146 [00:15<00:18,  5.03it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6207266449928284:  21%|██▋          | 50/241 [00:14<00:37,  5.04it/s]Epoch: 1, train for the 53-th batch, train loss: 0.6722132563591003:  36%|████▋        | 53/146 [00:15<00:23,  3.96it/s]Epoch: 1, train for the 54-th batch, train loss: 0.5644269585609436:  45%|█████▊       | 53/119 [00:15<00:13,  4.81it/s]Epoch: 1, train for the 54-th batch, train loss: 0.5644269585609436:  45%|█████▉       | 54/119 [00:15<00:16,  4.05it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6207266449928284:  22%|██▊          | 52/241 [00:14<00:28,  6.61it/s]Epoch: 1, train for the 50-th batch, train loss: 0.6717955470085144:  32%|████▏        | 49/151 [00:16<00:23,  4.42it/s]Epoch: 1, train for the 50-th batch, train loss: 0.6717955470085144:  33%|████▎        | 50/151 [00:16<00:20,  4.94it/s]Epoch: 1, train for the 54-th batch, train loss: 0.6683135628700256:  36%|████▋        | 53/146 [00:16<00:23,  3.96it/s]Epoch: 1, train for the 53-th batch, train loss: 0.5097892880439758:  22%|██▊          | 52/241 [00:15<00:28,  6.61it/s]Epoch: 1, train for the 54-th batch, train loss: 0.6683135628700256:  37%|████▊        | 54/146 [00:16<00:20,  4.49it/s]Epoch: 1, train for the 53-th batch, train loss: 0.5097892880439758:  22%|██▊          | 53/241 [00:15<00:28,  6.66it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6193863153457642:  19%|██▍          | 45/237 [00:15<00:42,  4.47it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6448236703872681:  11%|█▍           | 42/383 [00:13<01:33,  3.63it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6193863153457642:  19%|██▌          | 46/237 [00:15<00:42,  4.53it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6448236703872681:  11%|█▍           | 43/383 [00:13<01:29,  3.80it/s]Epoch: 1, train for the 54-th batch, train loss: 0.4708826243877411:  22%|██▊          | 53/241 [00:15<00:28,  6.66it/s]Epoch: 1, train for the 55-th batch, train loss: 0.5098733305931091:  45%|█████▉       | 54/119 [00:16<00:16,  4.05it/s]Epoch: 1, train for the 54-th batch, train loss: 0.4708826243877411:  22%|██▉          | 54/241 [00:15<00:28,  6.51it/s]Epoch: 1, train for the 55-th batch, train loss: 0.5098733305931091:  46%|██████       | 55/119 [00:16<00:17,  3.74it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6417057514190674:  19%|██▌          | 46/237 [00:15<00:42,  4.53it/s]Epoch: 1, train for the 55-th batch, train loss: 0.6734001636505127:  37%|████▊        | 54/146 [00:16<00:20,  4.49it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6417057514190674:  20%|██▌          | 47/237 [00:15<00:39,  4.84it/s]Epoch: 1, train for the 55-th batch, train loss: 0.6734001636505127:  38%|████▉        | 55/146 [00:16<00:20,  4.54it/s]Epoch: 1, train for the 51-th batch, train loss: 0.6782882809638977:  33%|████▎        | 50/151 [00:16<00:20,  4.94it/s]Epoch: 1, train for the 44-th batch, train loss: 0.5698503851890564:  11%|█▍           | 43/383 [00:13<01:29,  3.80it/s]Epoch: 1, train for the 55-th batch, train loss: 0.4667423963546753:  22%|██▉          | 54/241 [00:15<00:28,  6.51it/s]Epoch: 1, train for the 51-th batch, train loss: 0.6782882809638977:  34%|████▍        | 51/151 [00:16<00:24,  4.10it/s]Epoch: 1, train for the 56-th batch, train loss: 0.5329126715660095:  46%|██████       | 55/119 [00:16<00:17,  3.74it/s]Epoch: 1, train for the 55-th batch, train loss: 0.4667423963546753:  23%|██▉          | 55/241 [00:15<00:26,  7.00it/s]Epoch: 1, train for the 56-th batch, train loss: 0.5329126715660095:  47%|██████       | 56/119 [00:16<00:13,  4.51it/s]Epoch: 1, train for the 44-th batch, train loss: 0.5698503851890564:  11%|█▍           | 44/383 [00:13<01:28,  3.83it/s]Epoch: 1, train for the 56-th batch, train loss: 0.6633338928222656:  38%|████▉        | 55/146 [00:16<00:20,  4.54it/s]Epoch: 1, train for the 56-th batch, train loss: 0.6633338928222656:  38%|████▉        | 56/146 [00:16<00:16,  5.30it/s]Epoch: 1, train for the 48-th batch, train loss: 0.6366458535194397:  20%|██▌          | 47/237 [00:15<00:39,  4.84it/s]Epoch: 1, train for the 48-th batch, train loss: 0.6366458535194397:  20%|██▋          | 48/237 [00:15<00:36,  5.19it/s]Epoch: 1, train for the 56-th batch, train loss: 0.49917060136795044:  23%|██▋         | 55/241 [00:15<00:26,  7.00it/s]Epoch: 1, train for the 56-th batch, train loss: 0.49917060136795044:  23%|██▊         | 56/241 [00:15<00:26,  6.86it/s]Epoch: 1, train for the 57-th batch, train loss: 0.5236488580703735:  47%|██████       | 56/119 [00:16<00:13,  4.51it/s]Epoch: 1, train for the 57-th batch, train loss: 0.5236488580703735:  48%|██████▏      | 57/119 [00:16<00:12,  4.85it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6828905940055847:  34%|████▍        | 51/151 [00:16<00:24,  4.10it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6828905940055847:  34%|████▍        | 52/151 [00:16<00:23,  4.25it/s]Epoch: 1, train for the 45-th batch, train loss: 0.5698237419128418:  11%|█▍           | 44/383 [00:14<01:28,  3.83it/s]Epoch: 1, train for the 45-th batch, train loss: 0.5698237419128418:  12%|█▌           | 45/383 [00:14<01:22,  4.10it/s]Epoch: 1, train for the 57-th batch, train loss: 0.5152565836906433:  23%|███          | 56/241 [00:15<00:26,  6.86it/s]Epoch: 1, train for the 57-th batch, train loss: 0.5152565836906433:  24%|███          | 57/241 [00:15<00:26,  7.04it/s]Epoch: 1, train for the 58-th batch, train loss: 0.537956178188324:  48%|██████▋       | 57/119 [00:16<00:12,  4.85it/s]Epoch: 1, train for the 58-th batch, train loss: 0.537956178188324:  49%|██████▊       | 58/119 [00:16<00:11,  5.51it/s]Epoch: 1, train for the 57-th batch, train loss: 0.6611773371696472:  38%|████▉        | 56/146 [00:16<00:16,  5.30it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6369451284408569:  20%|██▋          | 48/237 [00:15<00:36,  5.19it/s]Epoch: 1, train for the 57-th batch, train loss: 0.6611773371696472:  39%|█████        | 57/146 [00:16<00:19,  4.56it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6369451284408569:  21%|██▋          | 49/237 [00:15<00:40,  4.62it/s]Epoch: 1, train for the 46-th batch, train loss: 0.5992761850357056:  12%|█▌           | 45/383 [00:14<01:22,  4.10it/s]Epoch: 1, train for the 53-th batch, train loss: 0.6794619560241699:  34%|████▍        | 52/151 [00:16<00:23,  4.25it/s]Epoch: 1, train for the 58-th batch, train loss: 0.6616426110267639:  39%|█████        | 57/146 [00:16<00:19,  4.56it/s]Epoch: 1, train for the 58-th batch, train loss: 0.4373061954975128:  24%|███          | 57/241 [00:15<00:26,  7.04it/s]Epoch: 1, train for the 58-th batch, train loss: 0.6616426110267639:  40%|█████▏       | 58/146 [00:16<00:16,  5.27it/s]Epoch: 1, train for the 46-th batch, train loss: 0.5992761850357056:  12%|█▌           | 46/383 [00:14<01:21,  4.14it/s]Epoch: 1, train for the 59-th batch, train loss: 0.5665449500083923:  49%|██████▎      | 58/119 [00:16<00:11,  5.51it/s]Epoch: 1, train for the 53-th batch, train loss: 0.6794619560241699:  35%|████▌        | 53/151 [00:16<00:23,  4.13it/s]Epoch: 1, train for the 59-th batch, train loss: 0.5665449500083923:  50%|██████▍      | 59/119 [00:16<00:10,  5.58it/s]Epoch: 1, train for the 58-th batch, train loss: 0.4373061954975128:  24%|███▏         | 58/241 [00:15<00:28,  6.44it/s]Epoch: 1, train for the 50-th batch, train loss: 0.638617753982544:  21%|██▉           | 49/237 [00:15<00:40,  4.62it/s]Epoch: 1, train for the 50-th batch, train loss: 0.638617753982544:  21%|██▉           | 50/237 [00:15<00:36,  5.14it/s]Epoch: 1, train for the 54-th batch, train loss: 0.6655824780464172:  35%|████▌        | 53/151 [00:16<00:23,  4.13it/s]Epoch: 1, train for the 54-th batch, train loss: 0.6655824780464172:  36%|████▋        | 54/151 [00:16<00:20,  4.67it/s]Epoch: 1, train for the 59-th batch, train loss: 0.6436948180198669:  40%|█████▏       | 58/146 [00:16<00:16,  5.27it/s]Epoch: 1, train for the 59-th batch, train loss: 0.6436948180198669:  40%|█████▎       | 59/146 [00:17<00:15,  5.52it/s]Epoch: 1, train for the 59-th batch, train loss: 0.641417920589447:  24%|███▎          | 58/241 [00:15<00:28,  6.44it/s]Epoch: 1, train for the 60-th batch, train loss: 0.5349931120872498:  50%|██████▍      | 59/119 [00:17<00:10,  5.58it/s]Epoch: 1, train for the 59-th batch, train loss: 0.641417920589447:  24%|███▍          | 59/241 [00:16<00:29,  6.11it/s]Epoch: 1, train for the 60-th batch, train loss: 0.5349931120872498:  50%|██████▌      | 60/119 [00:17<00:10,  5.45it/s]Epoch: 1, train for the 47-th batch, train loss: 0.503240704536438:  12%|█▋            | 46/383 [00:14<01:21,  4.14it/s]Epoch: 1, train for the 47-th batch, train loss: 0.503240704536438:  12%|█▋            | 47/383 [00:14<01:19,  4.25it/s]Epoch: 1, train for the 51-th batch, train loss: 0.629958987236023:  21%|██▉           | 50/237 [00:16<00:36,  5.14it/s]Epoch: 1, train for the 51-th batch, train loss: 0.629958987236023:  22%|███           | 51/237 [00:16<00:36,  5.11it/s]Epoch: 1, train for the 55-th batch, train loss: 0.6679431796073914:  36%|████▋        | 54/151 [00:17<00:20,  4.67it/s]Epoch: 1, train for the 55-th batch, train loss: 0.6679431796073914:  36%|████▋        | 55/151 [00:17<00:17,  5.49it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6082712411880493:  22%|██▊          | 51/237 [00:16<00:36,  5.11it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6082712411880493:  22%|██▊          | 52/237 [00:16<00:31,  5.82it/s]Epoch: 1, train for the 60-th batch, train loss: 0.6453628540039062:  40%|█████▎       | 59/146 [00:17<00:15,  5.52it/s]Epoch: 1, train for the 56-th batch, train loss: 0.6332443952560425:  36%|████▋        | 55/151 [00:17<00:17,  5.49it/s]Epoch: 1, train for the 60-th batch, train loss: 0.6453628540039062:  41%|█████▎       | 60/146 [00:17<00:16,  5.23it/s]Epoch: 1, train for the 56-th batch, train loss: 0.6332443952560425:  37%|████▊        | 56/151 [00:17<00:15,  6.07it/s]Epoch: 1, train for the 61-th batch, train loss: 0.5429494976997375:  50%|██████▌      | 60/119 [00:17<00:10,  5.45it/s]Epoch: 1, train for the 61-th batch, train loss: 0.5429494976997375:  51%|██████▋      | 61/119 [00:17<00:11,  4.87it/s]Epoch: 1, train for the 60-th batch, train loss: 0.6828291416168213:  24%|███▏         | 59/241 [00:16<00:29,  6.11it/s]Epoch: 1, train for the 53-th batch, train loss: 0.6178497672080994:  22%|██▊          | 52/237 [00:16<00:31,  5.82it/s]Epoch: 1, train for the 60-th batch, train loss: 0.6828291416168213:  25%|███▏         | 60/241 [00:16<00:36,  4.91it/s]Epoch: 1, train for the 61-th batch, train loss: 0.6507822275161743:  41%|█████▎       | 60/146 [00:17<00:16,  5.23it/s]Epoch: 1, train for the 61-th batch, train loss: 0.6507822275161743:  42%|█████▍       | 61/146 [00:17<00:14,  5.82it/s]Epoch: 1, train for the 53-th batch, train loss: 0.6178497672080994:  22%|██▉          | 53/237 [00:16<00:30,  6.08it/s]Epoch: 1, train for the 48-th batch, train loss: 0.5422139763832092:  12%|█▌           | 47/383 [00:14<01:19,  4.25it/s]Epoch: 1, train for the 57-th batch, train loss: 0.6485725045204163:  37%|████▊        | 56/151 [00:17<00:15,  6.07it/s]Epoch: 1, train for the 48-th batch, train loss: 0.5422139763832092:  13%|█▋           | 48/383 [00:14<01:27,  3.83it/s]Epoch: 1, train for the 57-th batch, train loss: 0.6485725045204163:  38%|████▉        | 57/151 [00:17<00:15,  6.12it/s]Epoch: 1, train for the 62-th batch, train loss: 0.5732836723327637:  51%|██████▋      | 61/119 [00:17<00:11,  4.87it/s]Epoch: 1, train for the 62-th batch, train loss: 0.5732836723327637:  52%|██████▊      | 62/119 [00:17<00:10,  5.59it/s]Epoch: 1, train for the 61-th batch, train loss: 0.6229292154312134:  25%|███▏         | 60/241 [00:16<00:36,  4.91it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6254380941390991:  42%|█████▍       | 61/146 [00:17<00:14,  5.82it/s]Epoch: 1, train for the 61-th batch, train loss: 0.6229292154312134:  25%|███▎         | 61/241 [00:16<00:32,  5.53it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6254380941390991:  42%|█████▌       | 62/146 [00:17<00:13,  6.38it/s]Epoch: 1, train for the 54-th batch, train loss: 0.5721738338470459:  22%|██▉          | 53/237 [00:16<00:30,  6.08it/s]Epoch: 1, train for the 54-th batch, train loss: 0.5721738338470459:  23%|██▉          | 54/237 [00:16<00:30,  6.06it/s]Epoch: 1, train for the 63-th batch, train loss: 0.6322049498558044:  42%|█████▌       | 62/146 [00:17<00:13,  6.38it/s]Epoch: 1, train for the 63-th batch, train loss: 0.6322049498558044:  43%|█████▌       | 63/146 [00:17<00:11,  7.13it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6506713628768921:  25%|███▎         | 61/241 [00:16<00:32,  5.53it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6506713628768921:  26%|███▎         | 62/241 [00:16<00:30,  5.94it/s]Epoch: 1, train for the 63-th batch, train loss: 0.5407807230949402:  52%|██████▊      | 62/119 [00:17<00:10,  5.59it/s]Epoch: 1, train for the 63-th batch, train loss: 0.5407807230949402:  53%|██████▉      | 63/119 [00:17<00:10,  5.11it/s]Epoch: 1, train for the 58-th batch, train loss: 0.6487414240837097:  38%|████▉        | 57/151 [00:17<00:15,  6.12it/s]Epoch: 1, train for the 58-th batch, train loss: 0.6487414240837097:  38%|████▉        | 58/151 [00:17<00:19,  4.73it/s]Epoch: 1, train for the 63-th batch, train loss: 0.7171078324317932:  26%|███▎         | 62/241 [00:16<00:30,  5.94it/s]Epoch: 1, train for the 64-th batch, train loss: 0.5465591549873352:  53%|██████▉      | 63/119 [00:17<00:10,  5.11it/s]Epoch: 1, train for the 63-th batch, train loss: 0.7171078324317932:  26%|███▍         | 63/241 [00:16<00:28,  6.34it/s]Epoch: 1, train for the 49-th batch, train loss: 0.5887061953544617:  13%|█▋           | 48/383 [00:15<01:27,  3.83it/s]Epoch: 1, train for the 49-th batch, train loss: 0.5887061953544617:  13%|█▋           | 49/383 [00:15<01:38,  3.38it/s]Epoch: 1, train for the 64-th batch, train loss: 0.6354463696479797:  43%|█████▌       | 63/146 [00:17<00:11,  7.13it/s]Epoch: 1, train for the 55-th batch, train loss: 0.6270222663879395:  23%|██▉          | 54/237 [00:16<00:30,  6.06it/s]Epoch: 1, train for the 64-th batch, train loss: 0.6354463696479797:  44%|█████▋       | 64/146 [00:17<00:13,  6.04it/s]Epoch: 1, train for the 55-th batch, train loss: 0.6270222663879395:  23%|███          | 55/237 [00:16<00:37,  4.81it/s]Epoch: 1, train for the 59-th batch, train loss: 0.6528686285018921:  38%|████▉        | 58/151 [00:17<00:19,  4.73it/s]Epoch: 1, train for the 59-th batch, train loss: 0.6528686285018921:  39%|█████        | 59/151 [00:17<00:17,  5.26it/s]Epoch: 1, train for the 64-th batch, train loss: 0.3755732774734497:  26%|███▍         | 63/241 [00:16<00:28,  6.34it/s]Epoch: 1, train for the 65-th batch, train loss: 0.5258755087852478:  53%|██████▉      | 63/119 [00:17<00:10,  5.11it/s]Epoch: 1, train for the 65-th batch, train loss: 0.5258755087852478:  55%|███████      | 65/119 [00:17<00:08,  6.48it/s]Epoch: 1, train for the 64-th batch, train loss: 0.3755732774734497:  27%|███▍         | 64/241 [00:16<00:26,  6.66it/s]Epoch: 1, train for the 50-th batch, train loss: 0.41983896493911743:  13%|█▌          | 49/383 [00:15<01:38,  3.38it/s]Epoch: 1, train for the 50-th batch, train loss: 0.41983896493911743:  13%|█▌          | 50/383 [00:15<01:24,  3.95it/s]Epoch: 1, train for the 65-th batch, train loss: 0.6453993320465088:  44%|█████▋       | 64/146 [00:17<00:13,  6.04it/s]Epoch: 1, train for the 65-th batch, train loss: 0.6453993320465088:  45%|█████▊       | 65/146 [00:17<00:12,  6.37it/s]Epoch: 1, train for the 60-th batch, train loss: 0.6574066281318665:  39%|█████        | 59/151 [00:17<00:17,  5.26it/s]Epoch: 1, train for the 66-th batch, train loss: 0.5316542983055115:  55%|███████      | 65/119 [00:17<00:08,  6.48it/s]Epoch: 1, train for the 66-th batch, train loss: 0.5316542983055115:  55%|███████▏     | 66/119 [00:17<00:07,  6.76it/s]Epoch: 1, train for the 60-th batch, train loss: 0.6574066281318665:  40%|█████▏       | 60/151 [00:17<00:16,  5.65it/s]Epoch: 1, train for the 65-th batch, train loss: 0.41893213987350464:  27%|███▏        | 64/241 [00:16<00:26,  6.66it/s]Epoch: 1, train for the 65-th batch, train loss: 0.41893213987350464:  27%|███▏        | 65/241 [00:16<00:26,  6.60it/s]Epoch: 1, train for the 51-th batch, train loss: 0.44454941153526306:  13%|█▌          | 50/383 [00:15<01:24,  3.95it/s]Epoch: 1, train for the 66-th batch, train loss: 0.6219077706336975:  45%|█████▊       | 65/146 [00:18<00:12,  6.37it/s]Epoch: 1, train for the 66-th batch, train loss: 0.6219077706336975:  45%|█████▉       | 66/146 [00:18<00:11,  6.82it/s]Epoch: 1, train for the 51-th batch, train loss: 0.44454941153526306:  13%|█▌          | 51/383 [00:15<01:12,  4.58it/s]Epoch: 1, train for the 56-th batch, train loss: 0.6267480850219727:  23%|███          | 55/237 [00:17<00:37,  4.81it/s]Epoch: 1, train for the 56-th batch, train loss: 0.6267480850219727:  24%|███          | 56/237 [00:17<00:44,  4.10it/s]Epoch: 1, train for the 61-th batch, train loss: 0.6462089419364929:  40%|█████▏       | 60/151 [00:18<00:16,  5.65it/s]Epoch: 1, train for the 67-th batch, train loss: 0.6120514273643494:  55%|███████▏     | 66/119 [00:18<00:07,  6.76it/s]Epoch: 1, train for the 66-th batch, train loss: 0.38540950417518616:  27%|███▏        | 65/241 [00:17<00:26,  6.60it/s]Epoch: 1, train for the 67-th batch, train loss: 0.6120514273643494:  56%|███████▎     | 67/119 [00:18<00:09,  5.76it/s]Epoch: 1, train for the 61-th batch, train loss: 0.6462089419364929:  40%|█████▎       | 61/151 [00:18<00:17,  5.00it/s]Epoch: 1, train for the 66-th batch, train loss: 0.38540950417518616:  27%|███▎        | 66/241 [00:17<00:30,  5.70it/s]Epoch: 1, train for the 67-th batch, train loss: 0.6289806962013245:  45%|█████▉       | 66/146 [00:18<00:11,  6.82it/s]Epoch: 1, train for the 67-th batch, train loss: 0.6289806962013245:  46%|█████▉       | 67/146 [00:18<00:13,  5.78it/s]Epoch: 1, train for the 52-th batch, train loss: 0.46969637274742126:  13%|█▌          | 51/383 [00:15<01:12,  4.58it/s]Epoch: 1, train for the 52-th batch, train loss: 0.46969637274742126:  14%|█▋          | 52/383 [00:15<01:16,  4.33it/s]Epoch: 1, train for the 57-th batch, train loss: 0.600898027420044:  24%|███▎          | 56/237 [00:17<00:44,  4.10it/s]Epoch: 1, train for the 57-th batch, train loss: 0.600898027420044:  24%|███▎          | 57/237 [00:17<00:40,  4.39it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5424759387969971:  56%|███████▎     | 67/119 [00:18<00:09,  5.76it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5424759387969971:  57%|███████▍     | 68/119 [00:18<00:08,  6.25it/s]Epoch: 1, train for the 68-th batch, train loss: 0.6231057643890381:  46%|█████▉       | 67/146 [00:18<00:13,  5.78it/s]Epoch: 1, train for the 68-th batch, train loss: 0.6231057643890381:  47%|██████       | 68/146 [00:18<00:12,  6.22it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6538587212562561:  40%|█████▎       | 61/151 [00:18<00:17,  5.00it/s]Epoch: 1, train for the 67-th batch, train loss: 0.47519469261169434:  27%|███▎        | 66/241 [00:17<00:30,  5.70it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6538587212562561:  41%|█████▎       | 62/151 [00:18<00:17,  5.01it/s]Epoch: 1, train for the 67-th batch, train loss: 0.47519469261169434:  28%|███▎        | 67/241 [00:17<00:31,  5.48it/s]Epoch: 1, train for the 58-th batch, train loss: 0.6182196140289307:  24%|███▏         | 57/237 [00:17<00:40,  4.39it/s]Epoch: 1, train for the 58-th batch, train loss: 0.6182196140289307:  24%|███▏         | 58/237 [00:17<00:35,  5.00it/s]Epoch: 1, train for the 63-th batch, train loss: 0.6438454389572144:  41%|█████▎       | 62/151 [00:18<00:17,  5.01it/s]Epoch: 1, train for the 63-th batch, train loss: 0.6438454389572144:  42%|█████▍       | 63/151 [00:18<00:15,  5.60it/s]Epoch: 1, train for the 53-th batch, train loss: 0.5474222302436829:  14%|█▊           | 52/383 [00:16<01:16,  4.33it/s]Epoch: 1, train for the 53-th batch, train loss: 0.5474222302436829:  14%|█▊           | 53/383 [00:16<01:21,  4.03it/s]Epoch: 1, train for the 69-th batch, train loss: 0.6258057951927185:  57%|███████▍     | 68/119 [00:18<00:08,  6.25it/s]Epoch: 1, train for the 69-th batch, train loss: 0.6258057951927185:  58%|███████▌     | 69/119 [00:18<00:10,  4.73it/s]Epoch: 1, train for the 68-th batch, train loss: 0.565457284450531:  28%|███▉          | 67/241 [00:17<00:31,  5.48it/s]Epoch: 1, train for the 64-th batch, train loss: 0.6326923370361328:  42%|█████▍       | 63/151 [00:18<00:15,  5.60it/s]Epoch: 1, train for the 68-th batch, train loss: 0.565457284450531:  28%|███▉          | 68/241 [00:17<00:36,  4.74it/s]Epoch: 1, train for the 69-th batch, train loss: 0.611284613609314:  47%|██████▌       | 68/146 [00:18<00:12,  6.22it/s]Epoch: 1, train for the 64-th batch, train loss: 0.6326923370361328:  42%|█████▌       | 64/151 [00:18<00:15,  5.70it/s]Epoch: 1, train for the 69-th batch, train loss: 0.611284613609314:  47%|██████▌       | 69/146 [00:18<00:16,  4.68it/s]Epoch: 1, train for the 59-th batch, train loss: 0.6299830675125122:  24%|███▏         | 58/237 [00:17<00:35,  5.00it/s]Epoch: 1, train for the 54-th batch, train loss: 0.46734747290611267:  14%|█▋          | 53/383 [00:16<01:21,  4.03it/s]Epoch: 1, train for the 54-th batch, train loss: 0.46734747290611267:  14%|█▋          | 54/383 [00:16<01:17,  4.26it/s]Epoch: 1, train for the 59-th batch, train loss: 0.6299830675125122:  25%|███▏         | 59/237 [00:17<00:42,  4.15it/s]Epoch: 1, train for the 70-th batch, train loss: 0.554372251033783:  58%|████████      | 69/119 [00:18<00:10,  4.73it/s]Epoch: 1, train for the 69-th batch, train loss: 0.681678056716919:  28%|███▉          | 68/241 [00:17<00:36,  4.74it/s]Epoch: 1, train for the 70-th batch, train loss: 0.554372251033783:  59%|████████▏     | 70/119 [00:18<00:09,  5.25it/s]Epoch: 1, train for the 69-th batch, train loss: 0.681678056716919:  29%|████          | 69/241 [00:17<00:31,  5.40it/s]Epoch: 1, train for the 70-th batch, train loss: 0.6228346824645996:  47%|██████▏      | 69/146 [00:18<00:16,  4.68it/s]Epoch: 1, train for the 70-th batch, train loss: 0.6228346824645996:  48%|██████▏      | 70/146 [00:18<00:14,  5.34it/s]Epoch: 1, train for the 60-th batch, train loss: 0.5917412638664246:  25%|███▏         | 59/237 [00:17<00:42,  4.15it/s]Epoch: 1, train for the 60-th batch, train loss: 0.5917412638664246:  25%|███▎         | 60/237 [00:17<00:37,  4.78it/s]Epoch: 1, train for the 71-th batch, train loss: 0.5448659658432007:  59%|███████▋     | 70/119 [00:18<00:09,  5.25it/s]Epoch: 1, train for the 71-th batch, train loss: 0.5448659658432007:  60%|███████▊     | 71/119 [00:18<00:08,  5.93it/s]Epoch: 1, train for the 61-th batch, train loss: 0.6098294854164124:  25%|███▎         | 60/237 [00:18<00:37,  4.78it/s]Epoch: 1, train for the 61-th batch, train loss: 0.6098294854164124:  26%|███▎         | 61/237 [00:18<00:32,  5.47it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5625973343849182:  60%|███████▊     | 71/119 [00:19<00:08,  5.93it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5625973343849182:  61%|███████▊     | 72/119 [00:19<00:07,  5.95it/s]Epoch: 1, train for the 65-th batch, train loss: 0.6068964004516602:  42%|█████▌       | 64/151 [00:19<00:15,  5.70it/s]Epoch: 1, train for the 70-th batch, train loss: 0.540681004524231:  29%|████          | 69/241 [00:18<00:31,  5.40it/s]Epoch: 1, train for the 65-th batch, train loss: 0.6068964004516602:  43%|█████▌       | 65/151 [00:19<00:22,  3.88it/s]Epoch: 1, train for the 70-th batch, train loss: 0.540681004524231:  29%|████          | 70/241 [00:18<00:39,  4.28it/s]Epoch: 1, train for the 55-th batch, train loss: 0.699060320854187:  14%|█▉            | 54/383 [00:16<01:17,  4.26it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6068528890609741:  26%|███▎         | 61/237 [00:18<00:32,  5.47it/s]Epoch: 1, train for the 55-th batch, train loss: 0.699060320854187:  14%|██            | 55/383 [00:16<01:37,  3.37it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6068528890609741:  26%|███▍         | 62/237 [00:18<00:32,  5.36it/s]Epoch: 1, train for the 71-th batch, train loss: 0.588312029838562:  48%|██████▋       | 70/146 [00:19<00:14,  5.34it/s]Epoch: 1, train for the 71-th batch, train loss: 0.588312029838562:  49%|██████▊       | 71/146 [00:19<00:18,  4.02it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5208116769790649:  61%|███████▊     | 72/119 [00:19<00:07,  5.95it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5208116769790649:  61%|███████▉     | 73/119 [00:19<00:08,  5.46it/s]Epoch: 1, train for the 66-th batch, train loss: 0.6338199973106384:  43%|█████▌       | 65/151 [00:19<00:22,  3.88it/s]Epoch: 1, train for the 71-th batch, train loss: 0.6439575552940369:  29%|███▊         | 70/241 [00:18<00:39,  4.28it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5932059288024902:  49%|██████▎      | 71/146 [00:19<00:18,  4.02it/s]Epoch: 1, train for the 56-th batch, train loss: 0.5674936771392822:  14%|█▊           | 55/383 [00:17<01:37,  3.37it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5932059288024902:  49%|██████▍      | 72/146 [00:19<00:18,  4.01it/s]Epoch: 1, train for the 66-th batch, train loss: 0.6338199973106384:  44%|█████▋       | 66/151 [00:19<00:23,  3.58it/s]Epoch: 1, train for the 71-th batch, train loss: 0.6439575552940369:  29%|███▊         | 71/241 [00:18<00:44,  3.82it/s]Epoch: 1, train for the 63-th batch, train loss: 0.6352366209030151:  26%|███▍         | 62/237 [00:18<00:32,  5.36it/s]Epoch: 1, train for the 56-th batch, train loss: 0.5674936771392822:  15%|█▉           | 56/383 [00:17<01:36,  3.39it/s]Epoch: 1, train for the 63-th batch, train loss: 0.6352366209030151:  27%|███▍         | 63/237 [00:18<00:38,  4.56it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5432369709014893:  61%|███████▉     | 73/119 [00:19<00:08,  5.46it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5432369709014893:  62%|████████     | 74/119 [00:19<00:09,  4.93it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5911335349082947:  49%|██████▍      | 72/146 [00:19<00:18,  4.01it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5911335349082947:  50%|██████▌      | 73/146 [00:19<00:15,  4.76it/s]Epoch: 1, train for the 72-th batch, train loss: 0.6155356168746948:  29%|███▊         | 71/241 [00:18<00:44,  3.82it/s]Epoch: 1, train for the 72-th batch, train loss: 0.6155356168746948:  30%|███▉         | 72/241 [00:18<00:40,  4.19it/s]Epoch: 1, train for the 57-th batch, train loss: 0.49230265617370605:  15%|█▊          | 56/383 [00:17<01:36,  3.39it/s]Epoch: 1, train for the 67-th batch, train loss: 0.6432865858078003:  44%|█████▋       | 66/151 [00:19<00:23,  3.58it/s]Epoch: 1, train for the 57-th batch, train loss: 0.49230265617370605:  15%|█▊          | 57/383 [00:17<01:26,  3.76it/s]Epoch: 1, train for the 75-th batch, train loss: 0.5366492867469788:  62%|████████     | 74/119 [00:19<00:09,  4.93it/s]Epoch: 1, train for the 64-th batch, train loss: 0.6067391633987427:  27%|███▍         | 63/237 [00:18<00:38,  4.56it/s]Epoch: 1, train for the 67-th batch, train loss: 0.6432865858078003:  44%|█████▊       | 67/151 [00:19<00:21,  3.86it/s]Epoch: 1, train for the 75-th batch, train loss: 0.5366492867469788:  63%|████████▏    | 75/119 [00:19<00:08,  5.35it/s]Epoch: 1, train for the 64-th batch, train loss: 0.6067391633987427:  27%|███▌         | 64/237 [00:18<00:36,  4.68it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5794523358345032:  50%|██████▌      | 73/146 [00:19<00:15,  4.76it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5794523358345032:  51%|██████▌      | 74/146 [00:19<00:13,  5.30it/s]Epoch: 1, train for the 65-th batch, train loss: 0.6031576991081238:  27%|███▌         | 64/237 [00:18<00:36,  4.68it/s]Epoch: 1, train for the 65-th batch, train loss: 0.6031576991081238:  27%|███▌         | 65/237 [00:18<00:35,  4.89it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5779644846916199:  30%|███▉         | 72/241 [00:18<00:40,  4.19it/s]Epoch: 1, train for the 76-th batch, train loss: 0.569945216178894:  63%|████████▊     | 75/119 [00:19<00:08,  5.35it/s]Epoch: 1, train for the 76-th batch, train loss: 0.569945216178894:  64%|████████▉     | 76/119 [00:19<00:08,  5.20it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5779644846916199:  30%|███▉         | 73/241 [00:18<00:40,  4.19it/s]Epoch: 1, train for the 75-th batch, train loss: 0.5891141891479492:  51%|██████▌      | 74/146 [00:19<00:13,  5.30it/s]Epoch: 1, train for the 75-th batch, train loss: 0.5891141891479492:  51%|██████▋      | 75/146 [00:20<00:14,  5.04it/s]Epoch: 1, train for the 58-th batch, train loss: 0.42987388372421265:  15%|█▊          | 57/383 [00:17<01:26,  3.76it/s]Epoch: 1, train for the 58-th batch, train loss: 0.42987388372421265:  15%|█▊          | 58/383 [00:17<01:32,  3.51it/s]Epoch: 1, train for the 77-th batch, train loss: 0.5098796486854553:  64%|████████▎    | 76/119 [00:20<00:08,  5.20it/s]Epoch: 1, train for the 77-th batch, train loss: 0.5098796486854553:  65%|████████▍    | 77/119 [00:20<00:07,  5.82it/s]Epoch: 1, train for the 66-th batch, train loss: 0.6038360595703125:  27%|███▌         | 65/237 [00:19<00:35,  4.89it/s]Epoch: 1, train for the 66-th batch, train loss: 0.6038360595703125:  28%|███▌         | 66/237 [00:19<00:33,  5.15it/s]Epoch: 1, train for the 76-th batch, train loss: 0.5697534084320068:  51%|██████▋      | 75/146 [00:20<00:14,  5.04it/s]Epoch: 1, train for the 76-th batch, train loss: 0.5697534084320068:  52%|██████▊      | 76/146 [00:20<00:12,  5.65it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5394949913024902:  30%|███▉         | 73/241 [00:19<00:40,  4.19it/s]Epoch: 1, train for the 68-th batch, train loss: 0.6267120838165283:  44%|█████▊       | 67/151 [00:20<00:21,  3.86it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5394949913024902:  31%|███▉         | 74/241 [00:19<00:38,  4.38it/s]Epoch: 1, train for the 68-th batch, train loss: 0.6267120838165283:  45%|█████▊       | 68/151 [00:20<00:25,  3.25it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5454642176628113:  65%|████████▍    | 77/119 [00:20<00:07,  5.82it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5454642176628113:  66%|████████▌    | 78/119 [00:20<00:06,  6.23it/s]Epoch: 1, train for the 59-th batch, train loss: 0.4890451729297638:  15%|█▉           | 58/383 [00:17<01:32,  3.51it/s]Epoch: 1, train for the 59-th batch, train loss: 0.4890451729297638:  15%|██           | 59/383 [00:17<01:22,  3.95it/s]Epoch: 1, train for the 77-th batch, train loss: 0.5909823179244995:  52%|██████▊      | 76/146 [00:20<00:12,  5.65it/s]Epoch: 1, train for the 77-th batch, train loss: 0.5909823179244995:  53%|██████▊      | 77/146 [00:20<00:11,  6.03it/s]Epoch: 1, train for the 75-th batch, train loss: 0.4348814785480499:  31%|███▉         | 74/241 [00:19<00:38,  4.38it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5368129014968872:  66%|████████▌    | 78/119 [00:20<00:06,  6.23it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5368129014968872:  66%|████████▋    | 79/119 [00:20<00:05,  6.78it/s]Epoch: 1, train for the 75-th batch, train loss: 0.4348814785480499:  31%|████         | 75/241 [00:19<00:35,  4.71it/s]Epoch: 1, train for the 69-th batch, train loss: 0.6242634654045105:  45%|█████▊       | 68/151 [00:20<00:25,  3.25it/s]Epoch: 1, train for the 67-th batch, train loss: 0.5496861338615417:  28%|███▌         | 66/237 [00:19<00:33,  5.15it/s]Epoch: 1, train for the 69-th batch, train loss: 0.6242634654045105:  46%|█████▉       | 69/151 [00:20<00:22,  3.63it/s]Epoch: 1, train for the 67-th batch, train loss: 0.5496861338615417:  28%|███▋         | 67/237 [00:19<00:36,  4.66it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5892764329910278:  53%|██████▊      | 77/146 [00:20<00:11,  6.03it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5892764329910278:  53%|██████▉      | 78/146 [00:20<00:10,  6.45it/s]Epoch: 1, train for the 76-th batch, train loss: 0.2982289493083954:  31%|████         | 75/241 [00:19<00:35,  4.71it/s]Epoch: 1, train for the 76-th batch, train loss: 0.2982289493083954:  32%|████         | 76/241 [00:19<00:30,  5.41it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5541237592697144:  28%|███▋         | 67/237 [00:19<00:36,  4.66it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5541237592697144:  29%|███▋         | 68/237 [00:19<00:31,  5.29it/s]Epoch: 1, train for the 70-th batch, train loss: 0.6118218898773193:  46%|█████▉       | 69/151 [00:20<00:22,  3.63it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5711650848388672:  53%|██████▉      | 78/146 [00:20<00:10,  6.45it/s]Epoch: 1, train for the 80-th batch, train loss: 0.549684464931488:  66%|█████████▎    | 79/119 [00:20<00:05,  6.78it/s]Epoch: 1, train for the 70-th batch, train loss: 0.6118218898773193:  46%|██████       | 70/151 [00:20<00:19,  4.14it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5711650848388672:  54%|███████      | 79/146 [00:20<00:09,  6.85it/s]Epoch: 1, train for the 80-th batch, train loss: 0.549684464931488:  67%|█████████▍    | 80/119 [00:20<00:06,  6.09it/s]Epoch: 1, train for the 60-th batch, train loss: 0.396964967250824:  15%|██▏           | 59/383 [00:18<01:22,  3.95it/s]Epoch: 1, train for the 60-th batch, train loss: 0.396964967250824:  16%|██▏           | 60/383 [00:18<01:27,  3.69it/s]Epoch: 1, train for the 77-th batch, train loss: 0.4069525897502899:  32%|████         | 76/241 [00:19<00:30,  5.41it/s]Epoch: 1, train for the 77-th batch, train loss: 0.4069525897502899:  32%|████▏        | 77/241 [00:19<00:27,  5.94it/s]Epoch: 1, train for the 69-th batch, train loss: 0.590044379234314:  29%|████          | 68/237 [00:19<00:31,  5.29it/s]Epoch: 1, train for the 80-th batch, train loss: 0.5789366960525513:  54%|███████      | 79/146 [00:20<00:09,  6.85it/s]Epoch: 1, train for the 71-th batch, train loss: 0.6289486289024353:  46%|██████       | 70/151 [00:20<00:19,  4.14it/s]Epoch: 1, train for the 80-th batch, train loss: 0.5789366960525513:  55%|███████      | 80/146 [00:20<00:09,  6.90it/s]Epoch: 1, train for the 69-th batch, train loss: 0.590044379234314:  29%|████          | 69/237 [00:19<00:31,  5.39it/s]Epoch: 1, train for the 71-th batch, train loss: 0.6289486289024353:  47%|██████       | 71/151 [00:20<00:17,  4.62it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5212770104408264:  67%|████████▋    | 80/119 [00:20<00:06,  6.09it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5212770104408264:  68%|████████▊    | 81/119 [00:20<00:06,  5.98it/s]Epoch: 1, train for the 78-th batch, train loss: 0.639079749584198:  32%|████▍         | 77/241 [00:19<00:27,  5.94it/s]Epoch: 1, train for the 78-th batch, train loss: 0.639079749584198:  32%|████▌         | 78/241 [00:19<00:26,  6.18it/s]Epoch: 1, train for the 70-th batch, train loss: 0.5722857713699341:  29%|███▊         | 69/237 [00:19<00:31,  5.39it/s]Epoch: 1, train for the 70-th batch, train loss: 0.5722857713699341:  30%|███▊         | 70/237 [00:19<00:29,  5.62it/s]Epoch: 1, train for the 61-th batch, train loss: 0.4362547993659973:  16%|██           | 60/383 [00:18<01:27,  3.69it/s]Epoch: 1, train for the 61-th batch, train loss: 0.4362547993659973:  16%|██           | 61/383 [00:18<01:35,  3.38it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5360358953475952:  55%|███████      | 80/146 [00:20<00:09,  6.90it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5360358953475952:  55%|███████▏     | 81/146 [00:20<00:11,  5.61it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5760660171508789:  68%|████████▊    | 81/119 [00:20<00:06,  5.98it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5514017939567566:  32%|████▏        | 78/241 [00:19<00:26,  6.18it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5760660171508789:  69%|████████▉    | 82/119 [00:20<00:07,  4.96it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5514017939567566:  33%|████▎        | 79/241 [00:19<00:31,  5.17it/s]Epoch: 1, train for the 72-th batch, train loss: 0.6048929691314697:  47%|██████       | 71/151 [00:21<00:17,  4.62it/s]Epoch: 1, train for the 72-th batch, train loss: 0.6048929691314697:  48%|██████▏      | 72/151 [00:21<00:20,  3.85it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5757192373275757:  55%|███████▏     | 81/146 [00:21<00:11,  5.61it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5757192373275757:  56%|███████▎     | 82/146 [00:21<00:10,  5.91it/s]Epoch: 1, train for the 71-th batch, train loss: 0.5464173555374146:  30%|███▊         | 70/237 [00:20<00:29,  5.62it/s]Epoch: 1, train for the 62-th batch, train loss: 0.535203218460083:  16%|██▏           | 61/383 [00:18<01:35,  3.38it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5303536653518677:  69%|████████▉    | 82/119 [00:21<00:07,  4.96it/s]Epoch: 1, train for the 71-th batch, train loss: 0.5464173555374146:  30%|███▉         | 71/237 [00:20<00:33,  4.92it/s]Epoch: 1, train for the 80-th batch, train loss: 0.4439174234867096:  33%|████▎        | 79/241 [00:20<00:31,  5.17it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5303536653518677:  70%|█████████    | 83/119 [00:21<00:06,  5.64it/s]Epoch: 1, train for the 62-th batch, train loss: 0.535203218460083:  16%|██▎           | 62/383 [00:18<01:25,  3.75it/s]Epoch: 1, train for the 80-th batch, train loss: 0.4439174234867096:  33%|████▎        | 80/241 [00:20<00:27,  5.85it/s]Epoch: 1, train for the 73-th batch, train loss: 0.60843425989151:  48%|███████▏       | 72/151 [00:21<00:20,  3.85it/s]Epoch: 1, train for the 73-th batch, train loss: 0.60843425989151:  48%|███████▎       | 73/151 [00:21<00:17,  4.37it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5534009337425232:  56%|███████▎     | 82/146 [00:21<00:10,  5.91it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5534009337425232:  57%|███████▍     | 83/146 [00:21<00:10,  6.16it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5347462892532349:  30%|███▉         | 71/237 [00:20<00:33,  4.92it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5347462892532349:  30%|███▉         | 72/237 [00:20<00:32,  5.12it/s]Epoch: 1, train for the 84-th batch, train loss: 0.503982424736023:  57%|███████▉      | 83/146 [00:21<00:10,  6.16it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5062366724014282:  70%|█████████    | 83/119 [00:21<00:06,  5.64it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5062366724014282:  71%|█████████▏   | 84/119 [00:21<00:07,  4.94it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5842668414115906:  48%|██████▎      | 73/151 [00:21<00:17,  4.37it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5673754811286926:  33%|████▎        | 80/241 [00:20<00:27,  5.85it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5842668414115906:  49%|██████▎      | 74/151 [00:21<00:16,  4.57it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5673754811286926:  34%|████▎        | 81/241 [00:20<00:32,  4.90it/s]Epoch: 1, train for the 63-th batch, train loss: 0.5364278554916382:  16%|██           | 62/383 [00:18<01:25,  3.75it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5327691435813904:  71%|█████████▏   | 84/119 [00:21<00:07,  4.94it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5460023283958435:  30%|███▉         | 72/237 [00:20<00:32,  5.12it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5327691435813904:  71%|█████████▎   | 85/119 [00:21<00:06,  5.57it/s]Epoch: 1, train for the 63-th batch, train loss: 0.5364278554916382:  16%|██▏          | 63/383 [00:18<01:36,  3.31it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5460023283958435:  31%|████         | 73/237 [00:20<00:33,  4.88it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5200584530830383:  57%|███████▍     | 83/146 [00:21<00:10,  6.16it/s]Epoch: 1, train for the 82-th batch, train loss: 0.4890891909599304:  34%|████▎        | 81/241 [00:20<00:32,  4.90it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5200584530830383:  58%|███████▌     | 85/146 [00:21<00:09,  6.13it/s]Epoch: 1, train for the 82-th batch, train loss: 0.4890891909599304:  34%|████▍        | 82/241 [00:20<00:30,  5.21it/s]Epoch: 1, train for the 75-th batch, train loss: 0.5812357068061829:  49%|██████▎      | 74/151 [00:21<00:16,  4.57it/s]Epoch: 1, train for the 75-th batch, train loss: 0.5812357068061829:  50%|██████▍      | 75/151 [00:21<00:15,  4.76it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5471494793891907:  71%|█████████▎   | 85/119 [00:21<00:06,  5.57it/s]Epoch: 1, train for the 64-th batch, train loss: 0.49880996346473694:  16%|█▉          | 63/383 [00:19<01:36,  3.31it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5471494793891907:  72%|█████████▍   | 86/119 [00:21<00:05,  5.83it/s]Epoch: 1, train for the 64-th batch, train loss: 0.49880996346473694:  17%|██          | 64/383 [00:19<01:23,  3.82it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5413872599601746:  34%|████▍        | 82/241 [00:20<00:30,  5.21it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5413872599601746:  34%|████▍        | 83/241 [00:20<00:28,  5.59it/s]Epoch: 1, train for the 87-th batch, train loss: 0.530057430267334:  72%|██████████    | 86/119 [00:21<00:05,  5.83it/s]Epoch: 1, train for the 87-th batch, train loss: 0.530057430267334:  73%|██████████▏   | 87/119 [00:21<00:04,  6.48it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5271807909011841:  31%|████         | 73/237 [00:20<00:33,  4.88it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5271807909011841:  31%|████         | 74/237 [00:20<00:38,  4.25it/s]Epoch: 1, train for the 86-th batch, train loss: 0.557216465473175:  58%|████████▏     | 85/146 [00:21<00:09,  6.13it/s]Epoch: 1, train for the 86-th batch, train loss: 0.557216465473175:  59%|████████▏     | 86/146 [00:21<00:11,  5.13it/s]Epoch: 1, train for the 76-th batch, train loss: 0.6122475862503052:  50%|██████▍      | 75/151 [00:21<00:15,  4.76it/s]Epoch: 1, train for the 76-th batch, train loss: 0.6122475862503052:  50%|██████▌      | 76/151 [00:21<00:17,  4.28it/s]Epoch: 1, train for the 65-th batch, train loss: 0.4262860119342804:  17%|██▏          | 64/383 [00:19<01:23,  3.82it/s]Epoch: 1, train for the 88-th batch, train loss: 0.542693018913269:  73%|██████████▏   | 87/119 [00:21<00:04,  6.48it/s]Epoch: 1, train for the 88-th batch, train loss: 0.542693018913269:  74%|██████████▎   | 88/119 [00:21<00:05,  6.02it/s]Epoch: 1, train for the 87-th batch, train loss: 0.6272739171981812:  59%|███████▋     | 86/146 [00:21<00:11,  5.13it/s]Epoch: 1, train for the 75-th batch, train loss: 0.5789150595664978:  31%|████         | 74/237 [00:20<00:38,  4.25it/s]Epoch: 1, train for the 65-th batch, train loss: 0.4262860119342804:  17%|██▏          | 65/383 [00:19<01:27,  3.64it/s]Epoch: 1, train for the 87-th batch, train loss: 0.6272739171981812:  60%|███████▋     | 87/146 [00:21<00:10,  5.61it/s]Epoch: 1, train for the 84-th batch, train loss: 0.6636819243431091:  34%|████▍        | 83/241 [00:20<00:28,  5.59it/s]Epoch: 1, train for the 75-th batch, train loss: 0.5789150595664978:  32%|████         | 75/237 [00:20<00:34,  4.66it/s]Epoch: 1, train for the 84-th batch, train loss: 0.6636819243431091:  35%|████▌        | 84/241 [00:20<00:33,  4.75it/s]Epoch: 1, train for the 77-th batch, train loss: 0.5884057283401489:  50%|██████▌      | 76/151 [00:22<00:17,  4.28it/s]Epoch: 1, train for the 77-th batch, train loss: 0.5884057283401489:  51%|██████▋      | 77/151 [00:22<00:15,  4.70it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5557475686073303:  74%|█████████▌   | 88/119 [00:22<00:05,  6.02it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5557475686073303:  75%|█████████▋   | 89/119 [00:22<00:04,  6.08it/s]Epoch: 1, train for the 88-th batch, train loss: 0.6152317523956299:  60%|███████▋     | 87/146 [00:22<00:10,  5.61it/s]Epoch: 1, train for the 88-th batch, train loss: 0.6152317523956299:  60%|███████▊     | 88/146 [00:22<00:10,  5.65it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5753517150878906:  51%|██████▋      | 77/151 [00:22<00:15,  4.70it/s]Epoch: 1, train for the 66-th batch, train loss: 0.5011810064315796:  17%|██▏          | 65/383 [00:19<01:27,  3.64it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5753517150878906:  52%|██████▋      | 78/151 [00:22<00:13,  5.40it/s]Epoch: 1, train for the 66-th batch, train loss: 0.5011810064315796:  17%|██▏          | 66/383 [00:19<01:20,  3.96it/s]Epoch: 1, train for the 76-th batch, train loss: 0.5670226216316223:  32%|████         | 75/237 [00:21<00:34,  4.66it/s]Epoch: 1, train for the 76-th batch, train loss: 0.5670226216316223:  32%|████▏        | 76/237 [00:21<00:40,  3.99it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5506706833839417:  75%|█████████▋   | 89/119 [00:22<00:04,  6.08it/s]Epoch: 1, train for the 67-th batch, train loss: 0.5267484784126282:  17%|██▏          | 66/383 [00:19<01:20,  3.96it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5506706833839417:  76%|█████████▊   | 90/119 [00:22<00:05,  5.03it/s]Epoch: 1, train for the 67-th batch, train loss: 0.5267484784126282:  17%|██▎          | 67/383 [00:19<01:17,  4.06it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5140362977981567:  35%|████▌        | 84/241 [00:21<00:33,  4.75it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5974181890487671:  52%|██████▋      | 78/151 [00:22<00:13,  5.40it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5162262320518494:  60%|███████▊     | 88/146 [00:22<00:10,  5.65it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5140362977981567:  35%|████▌        | 85/241 [00:21<00:43,  3.61it/s]Epoch: 1, train for the 77-th batch, train loss: 0.6099724173545837:  32%|████▏        | 76/237 [00:21<00:40,  3.99it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5974181890487671:  52%|██████▊      | 79/151 [00:22<00:15,  4.72it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5162262320518494:  61%|███████▉     | 89/146 [00:22<00:11,  4.79it/s]Epoch: 1, train for the 77-th batch, train loss: 0.6099724173545837:  32%|████▏        | 77/237 [00:21<00:34,  4.59it/s]Epoch: 1, train for the 91-th batch, train loss: 0.513165295124054:  76%|██████████▌   | 90/119 [00:22<00:05,  5.03it/s]Epoch: 1, train for the 91-th batch, train loss: 0.513165295124054:  76%|██████████▋   | 91/119 [00:22<00:05,  5.58it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5500583052635193:  61%|███████▉     | 89/146 [00:22<00:11,  4.79it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5303194522857666:  17%|██▎          | 67/383 [00:20<01:17,  4.06it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5500583052635193:  62%|████████     | 90/146 [00:22<00:10,  5.24it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5204839706420898:  35%|████▌        | 85/241 [00:21<00:43,  3.61it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5303194522857666:  18%|██▎          | 68/383 [00:20<01:13,  4.29it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5204839706420898:  36%|████▋        | 86/241 [00:21<00:38,  4.04it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5795004367828369:  32%|████▏        | 77/237 [00:21<00:34,  4.59it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5795004367828369:  33%|████▎        | 78/237 [00:21<00:32,  4.83it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5362824201583862:  62%|████████     | 90/146 [00:22<00:10,  5.24it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5362824201583862:  62%|████████     | 91/146 [00:22<00:09,  6.05it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5323934555053711:  76%|█████████▉   | 91/119 [00:22<00:05,  5.58it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5802026391029358:  33%|████▎        | 78/237 [00:21<00:32,  4.83it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5323934555053711:  77%|██████████   | 92/119 [00:22<00:05,  5.07it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5802026391029358:  33%|████▎        | 79/237 [00:21<00:29,  5.28it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5088465213775635:  62%|████████     | 91/146 [00:22<00:09,  6.05it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5088465213775635:  63%|████████▏    | 92/146 [00:22<00:08,  6.30it/s]Epoch: 1, train for the 69-th batch, train loss: 0.38000959157943726:  18%|██▏         | 68/383 [00:20<01:13,  4.29it/s]Epoch: 1, train for the 87-th batch, train loss: 0.5489810109138489:  36%|████▋        | 86/241 [00:21<00:38,  4.04it/s]Epoch: 1, train for the 93-th batch, train loss: 0.48266515135765076:  77%|█████████▎  | 92/119 [00:22<00:05,  5.07it/s]Epoch: 1, train for the 69-th batch, train loss: 0.38000959157943726:  18%|██▏         | 69/383 [00:20<01:17,  4.05it/s]Epoch: 1, train for the 93-th batch, train loss: 0.48266515135765076:  78%|█████████▍  | 93/119 [00:22<00:04,  5.72it/s]Epoch: 1, train for the 80-th batch, train loss: 0.562232494354248:  52%|███████▎      | 79/151 [00:22<00:15,  4.72it/s]Epoch: 1, train for the 87-th batch, train loss: 0.5489810109138489:  36%|████▋        | 87/241 [00:21<00:40,  3.84it/s]Epoch: 1, train for the 80-th batch, train loss: 0.587378203868866:  33%|████▋         | 79/237 [00:21<00:29,  5.28it/s]Epoch: 1, train for the 80-th batch, train loss: 0.562232494354248:  53%|███████▍      | 80/151 [00:22<00:20,  3.42it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5954767465591431:  63%|████████▏    | 92/146 [00:22<00:08,  6.30it/s]Epoch: 1, train for the 80-th batch, train loss: 0.587378203868866:  34%|████▋         | 80/237 [00:21<00:28,  5.52it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5954767465591431:  64%|████████▎    | 93/146 [00:22<00:07,  6.69it/s]Epoch: 1, train for the 88-th batch, train loss: 0.5699504613876343:  36%|████▋        | 87/241 [00:21<00:40,  3.84it/s]Epoch: 1, train for the 94-th batch, train loss: 0.5216314196586609:  78%|██████████▏  | 93/119 [00:23<00:04,  5.72it/s]Epoch: 1, train for the 94-th batch, train loss: 0.5216314196586609:  79%|██████████▎  | 94/119 [00:23<00:04,  6.03it/s]Epoch: 1, train for the 88-th batch, train loss: 0.5699504613876343:  37%|████▋        | 88/241 [00:22<00:34,  4.45it/s]Epoch: 1, train for the 70-th batch, train loss: 0.4984152019023895:  18%|██▎          | 69/383 [00:20<01:17,  4.05it/s]Epoch: 1, train for the 70-th batch, train loss: 0.4984152019023895:  18%|██▍          | 70/383 [00:20<01:12,  4.33it/s]Epoch: 1, train for the 94-th batch, train loss: 0.5571396350860596:  64%|████████▎    | 93/146 [00:23<00:07,  6.69it/s]Epoch: 1, train for the 94-th batch, train loss: 0.5571396350860596:  64%|████████▎    | 94/146 [00:23<00:07,  6.78it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5462905764579773:  37%|████▋        | 88/241 [00:22<00:34,  4.45it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5044481754302979:  79%|██████████▎  | 94/119 [00:23<00:04,  6.03it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5044481754302979:  80%|██████████▍  | 95/119 [00:23<00:03,  6.48it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5462905764579773:  37%|████▊        | 89/241 [00:22<00:29,  5.11it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5345334410667419:  64%|████████▎    | 94/146 [00:23<00:07,  6.78it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5345334410667419:  65%|████████▍    | 95/146 [00:23<00:07,  7.13it/s]Epoch: 1, train for the 81-th batch, train loss: 0.6046919822692871:  34%|████▍        | 80/237 [00:22<00:28,  5.52it/s]Epoch: 1, train for the 81-th batch, train loss: 0.6046919822692871:  34%|████▍        | 81/237 [00:22<00:33,  4.61it/s]Epoch: 1, train for the 96-th batch, train loss: 0.4917323887348175:  80%|██████████▍  | 95/119 [00:23<00:03,  6.48it/s]Epoch: 1, train for the 96-th batch, train loss: 0.4917323887348175:  81%|██████████▍  | 96/119 [00:23<00:03,  6.99it/s]Epoch: 1, train for the 71-th batch, train loss: 0.594245970249176:  18%|██▌           | 70/383 [00:20<01:12,  4.33it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5870996713638306:  53%|██████▉      | 80/151 [00:23<00:20,  3.42it/s]Epoch: 1, train for the 71-th batch, train loss: 0.594245970249176:  19%|██▌           | 71/383 [00:20<01:16,  4.10it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5447748303413391:  37%|████▊        | 89/241 [00:22<00:29,  5.11it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5870996713638306:  54%|██████▉      | 81/151 [00:23<00:23,  2.96it/s]Epoch: 1, train for the 82-th batch, train loss: 0.6144390106201172:  34%|████▍        | 81/237 [00:22<00:33,  4.61it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5447748303413391:  37%|████▊        | 90/241 [00:22<00:30,  4.94it/s]Epoch: 1, train for the 96-th batch, train loss: 0.558555543422699:  65%|█████████     | 95/146 [00:23<00:07,  7.13it/s]Epoch: 1, train for the 96-th batch, train loss: 0.558555543422699:  66%|█████████▏    | 96/146 [00:23<00:07,  6.47it/s]Epoch: 1, train for the 82-th batch, train loss: 0.6144390106201172:  35%|████▍        | 82/237 [00:22<00:31,  4.91it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5036324858665466:  81%|██████████▍  | 96/119 [00:23<00:03,  6.99it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5036324858665466:  82%|██████████▌  | 97/119 [00:23<00:03,  6.56it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5669779181480408:  54%|██████▉      | 81/151 [00:23<00:23,  2.96it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5669605135917664:  19%|██▍          | 71/383 [00:21<01:16,  4.10it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5669779181480408:  54%|███████      | 82/151 [00:23<00:19,  3.53it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5728368759155273:  66%|████████▌    | 96/146 [00:23<00:07,  6.47it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5728368759155273:  66%|████████▋    | 97/146 [00:23<00:07,  6.86it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5669605135917664:  19%|██▍          | 72/383 [00:21<01:10,  4.43it/s]Epoch: 1, train for the 98-th batch, train loss: 0.4583163261413574:  82%|██████████▌  | 97/119 [00:23<00:03,  6.56it/s]Epoch: 1, train for the 98-th batch, train loss: 0.4583163261413574:  82%|██████████▋  | 98/119 [00:23<00:03,  6.80it/s]Epoch: 1, train for the 73-th batch, train loss: 0.40655890107154846:  19%|██▎         | 72/383 [00:21<01:10,  4.43it/s]Epoch: 1, train for the 73-th batch, train loss: 0.40655890107154846:  19%|██▎         | 73/383 [00:21<01:01,  5.05it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5452502369880676:  37%|████▊        | 90/241 [00:22<00:30,  4.94it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5452502369880676:  38%|████▉        | 91/241 [00:22<00:36,  4.12it/s]Epoch: 1, train for the 98-th batch, train loss: 0.557198166847229:  66%|█████████▎    | 97/146 [00:23<00:07,  6.86it/s]Epoch: 1, train for the 98-th batch, train loss: 0.557198166847229:  67%|█████████▍    | 98/146 [00:23<00:07,  6.14it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5375866293907166:  54%|███████      | 82/151 [00:23<00:19,  3.53it/s]Epoch: 1, train for the 83-th batch, train loss: 0.6300649046897888:  35%|████▍        | 82/237 [00:22<00:31,  4.91it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5375866293907166:  55%|███████▏     | 83/151 [00:23<00:18,  3.69it/s]Epoch: 1, train for the 83-th batch, train loss: 0.6300649046897888:  35%|████▌        | 83/237 [00:22<00:38,  4.05it/s]Epoch: 1, train for the 74-th batch, train loss: 0.2756749987602234:  19%|██▍          | 73/383 [00:21<01:01,  5.05it/s]Epoch: 1, train for the 99-th batch, train loss: 0.5024319887161255:  82%|██████████▋  | 98/119 [00:23<00:03,  6.80it/s]Epoch: 1, train for the 99-th batch, train loss: 0.5024319887161255:  83%|██████████▊  | 99/119 [00:23<00:03,  5.90it/s]Epoch: 1, train for the 74-th batch, train loss: 0.2756749987602234:  19%|██▌          | 74/383 [00:21<00:56,  5.50it/s]Epoch: 1, train for the 99-th batch, train loss: 0.570904016494751:  67%|█████████▍    | 98/146 [00:23<00:07,  6.14it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5007890462875366:  38%|████▉        | 91/241 [00:22<00:36,  4.12it/s]Epoch: 1, train for the 99-th batch, train loss: 0.570904016494751:  68%|█████████▍    | 99/146 [00:23<00:06,  6.74it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5007890462875366:  38%|████▉        | 92/241 [00:22<00:31,  4.71it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5849555134773254:  55%|███████▏     | 83/151 [00:23<00:18,  3.69it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5849555134773254:  56%|███████▏     | 84/151 [00:23<00:15,  4.29it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5837658047676086:  35%|████▌        | 83/237 [00:22<00:38,  4.05it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5837658047676086:  35%|████▌        | 84/237 [00:22<00:34,  4.46it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5102514028549194:  83%|█████████▉  | 99/119 [00:23<00:03,  5.90it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5102514028549194:  84%|█████████▏ | 100/119 [00:23<00:03,  6.08it/s]Epoch: 1, train for the 75-th batch, train loss: 0.4202210009098053:  19%|██▌          | 74/383 [00:21<00:56,  5.50it/s]Epoch: 1, train for the 75-th batch, train loss: 0.4202210009098053:  20%|██▌          | 75/383 [00:21<00:55,  5.52it/s]Epoch: 1, train for the 85-th batch, train loss: 0.4950545132160187:  56%|███████▏     | 84/151 [00:23<00:15,  4.29it/s]Epoch: 1, train for the 85-th batch, train loss: 0.4950545132160187:  56%|███████▎     | 85/151 [00:24<00:13,  4.99it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5977514982223511:  68%|████████▏   | 99/146 [00:24<00:06,  6.74it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5977514982223511:  68%|███████▌   | 100/146 [00:24<00:09,  5.10it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5298826694488525:  35%|████▌        | 84/237 [00:23<00:34,  4.46it/s]Epoch: 1, train for the 93-th batch, train loss: 0.48729291558265686:  38%|████▌       | 92/241 [00:23<00:31,  4.71it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5750247836112976:  56%|███████▎     | 85/151 [00:24<00:13,  4.99it/s]Epoch: 1, train for the 76-th batch, train loss: 0.5323862433433533:  20%|██▌          | 75/383 [00:21<00:55,  5.52it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5298826694488525:  36%|████▋        | 85/237 [00:23<00:35,  4.31it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5750247836112976:  57%|███████▍     | 86/151 [00:24<00:12,  5.20it/s]Epoch: 1, train for the 93-th batch, train loss: 0.48729291558265686:  39%|████▋       | 93/241 [00:23<00:37,  3.99it/s]Epoch: 1, train for the 76-th batch, train loss: 0.5323862433433533:  20%|██▌          | 76/383 [00:21<00:59,  5.20it/s]Epoch: 1, train for the 101-th batch, train loss: 0.49469611048698425:  84%|████████▍ | 100/119 [00:24<00:03,  6.08it/s]Epoch: 1, train for the 101-th batch, train loss: 0.49469611048698425:  85%|████████▍ | 101/119 [00:24<00:03,  4.77it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5274222493171692:  68%|███████▌   | 100/146 [00:24<00:09,  5.10it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5274222493171692:  69%|███████▌   | 101/146 [00:24<00:08,  5.42it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5992617607116699:  36%|████▋        | 85/237 [00:23<00:35,  4.31it/s]Epoch: 1, train for the 94-th batch, train loss: 0.479531854391098:  39%|█████▍        | 93/241 [00:23<00:37,  3.99it/s]Epoch: 1, train for the 94-th batch, train loss: 0.479531854391098:  39%|█████▍        | 94/241 [00:23<00:32,  4.57it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5992617607116699:  36%|████▋        | 86/237 [00:23<00:31,  4.76it/s]Epoch: 1, train for the 87-th batch, train loss: 0.544333279132843:  57%|███████▉      | 86/151 [00:24<00:12,  5.20it/s]Epoch: 1, train for the 77-th batch, train loss: 0.4945094883441925:  20%|██▌          | 76/383 [00:21<00:59,  5.20it/s]Epoch: 1, train for the 87-th batch, train loss: 0.544333279132843:  58%|████████      | 87/151 [00:24<00:12,  5.21it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5164303183555603:  85%|█████████▎ | 101/119 [00:24<00:03,  4.77it/s]Epoch: 1, train for the 77-th batch, train loss: 0.4945094883441925:  20%|██▌          | 77/383 [00:21<00:59,  5.18it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5164303183555603:  86%|█████████▍ | 102/119 [00:24<00:03,  5.42it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5451557040214539:  69%|███████▌   | 101/146 [00:24<00:08,  5.42it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5451557040214539:  70%|███████▋   | 102/146 [00:24<00:07,  5.91it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5093748569488525:  39%|█████        | 94/241 [00:23<00:32,  4.57it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5093748569488525:  39%|█████        | 95/241 [00:23<00:27,  5.23it/s]Epoch: 1, train for the 88-th batch, train loss: 0.6445066928863525:  58%|███████▍     | 87/151 [00:24<00:12,  5.21it/s]Epoch: 1, train for the 78-th batch, train loss: 0.45981451869010925:  20%|██▍         | 77/383 [00:22<00:59,  5.18it/s]Epoch: 1, train for the 88-th batch, train loss: 0.6445066928863525:  58%|███████▌     | 88/151 [00:24<00:11,  5.59it/s]Epoch: 1, train for the 78-th batch, train loss: 0.45981451869010925:  20%|██▍         | 78/383 [00:22<00:54,  5.55it/s]Epoch: 1, train for the 103-th batch, train loss: 0.4678305685520172:  86%|█████████▍ | 102/119 [00:24<00:03,  5.42it/s]Epoch: 1, train for the 103-th batch, train loss: 0.4678305685520172:  87%|█████████▌ | 103/119 [00:24<00:03,  5.01it/s]Epoch: 1, train for the 87-th batch, train loss: 0.6156049966812134:  36%|████▋        | 86/237 [00:23<00:31,  4.76it/s]Epoch: 1, train for the 96-th batch, train loss: 0.5988832116127014:  39%|█████        | 95/241 [00:23<00:27,  5.23it/s]Epoch: 1, train for the 87-th batch, train loss: 0.6156049966812134:  37%|████▊        | 87/237 [00:23<00:36,  4.16it/s]Epoch: 1, train for the 96-th batch, train loss: 0.5988832116127014:  40%|█████▏       | 96/241 [00:23<00:28,  5.15it/s]Epoch: 1, train for the 103-th batch, train loss: 0.5386489033699036:  70%|███████▋   | 102/146 [00:24<00:07,  5.91it/s]Epoch: 1, train for the 103-th batch, train loss: 0.5386489033699036:  71%|███████▊   | 103/146 [00:24<00:08,  5.26it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5638460516929626:  20%|██▋          | 78/383 [00:22<00:54,  5.55it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5756533145904541:  58%|███████▌     | 88/151 [00:24<00:11,  5.59it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5638460516929626:  21%|██▋          | 79/383 [00:22<00:55,  5.49it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5756533145904541:  59%|███████▋     | 89/151 [00:24<00:11,  5.32it/s]Epoch: 1, train for the 104-th batch, train loss: 0.48183250427246094:  87%|████████▋ | 103/119 [00:24<00:03,  5.01it/s]Epoch: 1, train for the 104-th batch, train loss: 0.48183250427246094:  87%|████████▋ | 104/119 [00:24<00:02,  5.57it/s]Epoch: 1, train for the 97-th batch, train loss: 0.42022979259490967:  40%|████▊       | 96/241 [00:23<00:28,  5.15it/s]Epoch: 1, train for the 88-th batch, train loss: 0.6097177267074585:  37%|████▊        | 87/237 [00:23<00:36,  4.16it/s]Epoch: 1, train for the 97-th batch, train loss: 0.42022979259490967:  40%|████▊       | 97/241 [00:23<00:24,  5.78it/s]Epoch: 1, train for the 104-th batch, train loss: 0.546558678150177:  71%|████████▍   | 103/146 [00:24<00:08,  5.26it/s]Epoch: 1, train for the 88-th batch, train loss: 0.6097177267074585:  37%|████▊        | 88/237 [00:23<00:32,  4.64it/s]Epoch: 1, train for the 104-th batch, train loss: 0.546558678150177:  71%|████████▌   | 104/146 [00:24<00:07,  5.76it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5561639070510864:  59%|███████▋     | 89/151 [00:24<00:11,  5.32it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5561639070510864:  60%|███████▋     | 90/151 [00:24<00:10,  5.71it/s]Epoch: 1, train for the 80-th batch, train loss: 0.3438730835914612:  21%|██▋          | 79/383 [00:22<00:55,  5.49it/s]Epoch: 1, train for the 80-th batch, train loss: 0.3438730835914612:  21%|██▋          | 80/383 [00:22<00:55,  5.48it/s]Epoch: 1, train for the 105-th batch, train loss: 0.4925609827041626:  87%|█████████▌ | 104/119 [00:24<00:02,  5.57it/s]Epoch: 1, train for the 105-th batch, train loss: 0.4925609827041626:  88%|█████████▋ | 105/119 [00:24<00:02,  5.54it/s]Epoch: 1, train for the 105-th batch, train loss: 0.5715831518173218:  71%|███████▊   | 104/146 [00:24<00:07,  5.76it/s]Epoch: 1, train for the 105-th batch, train loss: 0.5715831518173218:  72%|███████▉   | 105/146 [00:24<00:06,  5.89it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5330666899681091:  60%|███████▋     | 90/151 [00:24<00:10,  5.71it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5330666899681091:  60%|███████▊     | 91/151 [00:24<00:09,  6.38it/s]Epoch: 1, train for the 106-th batch, train loss: 0.45696333050727844:  88%|████████▊ | 105/119 [00:25<00:02,  5.54it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5761400461196899:  37%|████▊        | 88/237 [00:24<00:32,  4.64it/s]Epoch: 1, train for the 106-th batch, train loss: 0.45696333050727844:  89%|████████▉ | 106/119 [00:25<00:02,  5.77it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5624107122421265:  72%|███████▉   | 105/146 [00:25<00:06,  5.89it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5761400461196899:  38%|████▉        | 89/237 [00:24<00:36,  4.06it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5624107122421265:  73%|███████▉   | 106/146 [00:25<00:06,  5.95it/s]Epoch: 1, train for the 81-th batch, train loss: 0.3177242577075958:  21%|██▋          | 80/383 [00:22<00:55,  5.48it/s]Epoch: 1, train for the 107-th batch, train loss: 0.5252687335014343:  89%|█████████▊ | 106/119 [00:25<00:02,  5.77it/s]Epoch: 1, train for the 81-th batch, train loss: 0.3177242577075958:  21%|██▋          | 81/383 [00:22<01:06,  4.57it/s]Epoch: 1, train for the 98-th batch, train loss: 0.544162392616272:  40%|█████▋        | 97/241 [00:24<00:24,  5.78it/s]Epoch: 1, train for the 107-th batch, train loss: 0.5252687335014343:  90%|█████████▉ | 107/119 [00:25<00:01,  6.36it/s]Epoch: 1, train for the 98-th batch, train loss: 0.544162392616272:  41%|█████▋        | 98/241 [00:24<00:36,  3.89it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5602899789810181:  60%|███████▊     | 91/151 [00:25<00:09,  6.38it/s]Epoch: 1, train for the 107-th batch, train loss: 0.542926549911499:  73%|████████▋   | 106/146 [00:25<00:06,  5.95it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5602899789810181:  61%|███████▉     | 92/151 [00:25<00:11,  5.27it/s]Epoch: 1, train for the 107-th batch, train loss: 0.542926549911499:  73%|████████▊   | 107/146 [00:25<00:06,  6.35it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5763640999794006:  38%|████▉        | 89/237 [00:24<00:36,  4.06it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5763640999794006:  38%|████▉        | 90/237 [00:24<00:34,  4.23it/s]Epoch: 1, train for the 108-th batch, train loss: 0.41277560591697693:  90%|████████▉ | 107/119 [00:25<00:01,  6.36it/s]Epoch: 1, train for the 99-th batch, train loss: 0.3610519766807556:  41%|█████▎       | 98/241 [00:24<00:36,  3.89it/s]Epoch: 1, train for the 99-th batch, train loss: 0.3610519766807556:  41%|█████▎       | 99/241 [00:24<00:31,  4.49it/s]Epoch: 1, train for the 108-th batch, train loss: 0.41277560591697693:  91%|█████████ | 108/119 [00:25<00:01,  6.25it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5666933655738831:  21%|██▋          | 81/383 [00:22<01:06,  4.57it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5677455067634583:  73%|████████   | 107/146 [00:25<00:06,  6.35it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5677455067634583:  74%|████████▏  | 108/146 [00:25<00:05,  6.55it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5666933655738831:  21%|██▊          | 82/383 [00:22<01:05,  4.62it/s]Epoch: 1, train for the 109-th batch, train loss: 0.5359202027320862:  74%|████████▏  | 108/146 [00:25<00:05,  6.55it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5647287964820862:  41%|████▉       | 99/241 [00:24<00:31,  4.49it/s]Epoch: 1, train for the 109-th batch, train loss: 0.5359202027320862:  75%|████████▏  | 109/146 [00:25<00:06,  5.92it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5788098573684692:  61%|███████▉     | 92/151 [00:25<00:11,  5.27it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5647287964820862:  41%|████▌      | 100/241 [00:24<00:32,  4.34it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5788098573684692:  62%|████████     | 93/151 [00:25<00:14,  4.08it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5430164337158203:  38%|████▉        | 90/237 [00:24<00:34,  4.23it/s]Epoch: 1, train for the 109-th batch, train loss: 0.48937729001045227:  91%|█████████ | 108/119 [00:25<00:01,  6.25it/s]Epoch: 1, train for the 109-th batch, train loss: 0.48937729001045227:  92%|█████████▏| 109/119 [00:25<00:02,  4.99it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5181053280830383:  21%|██▊          | 82/383 [00:23<01:05,  4.62it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5430164337158203:  38%|████▉        | 91/237 [00:24<00:39,  3.68it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5181053280830383:  22%|██▊          | 83/383 [00:23<01:11,  4.21it/s]Epoch: 1, train for the 110-th batch, train loss: 0.5223888158798218:  75%|████████▏  | 109/146 [00:25<00:06,  5.92it/s]Epoch: 1, train for the 110-th batch, train loss: 0.5223888158798218:  75%|████████▎  | 110/146 [00:25<00:05,  6.33it/s]Epoch: 1, train for the 101-th batch, train loss: 0.592289924621582:  41%|████▉       | 100/241 [00:24<00:32,  4.34it/s]Epoch: 1, train for the 101-th batch, train loss: 0.592289924621582:  42%|█████       | 101/241 [00:24<00:29,  4.79it/s]Epoch: 1, train for the 94-th batch, train loss: 0.5490396022796631:  62%|████████     | 93/151 [00:25<00:14,  4.08it/s]Epoch: 1, train for the 110-th batch, train loss: 0.48082074522972107:  92%|█████████▏| 109/119 [00:25<00:02,  4.99it/s]Epoch: 1, train for the 94-th batch, train loss: 0.5490396022796631:  62%|████████     | 94/151 [00:25<00:12,  4.47it/s]Epoch: 1, train for the 110-th batch, train loss: 0.48082074522972107:  92%|█████████▏| 110/119 [00:25<00:01,  5.53it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5589314699172974:  38%|████▉        | 91/237 [00:24<00:39,  3.68it/s]Epoch: 1, train for the 111-th batch, train loss: 0.53082275390625:  75%|█████████▊   | 110/146 [00:25<00:05,  6.33it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5589314699172974:  39%|█████        | 92/237 [00:24<00:35,  4.14it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5132094621658325:  22%|██▊          | 83/383 [00:23<01:11,  4.21it/s]Epoch: 1, train for the 111-th batch, train loss: 0.53082275390625:  76%|█████████▉   | 111/146 [00:25<00:05,  6.77it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5132094621658325:  22%|██▊          | 84/383 [00:23<01:06,  4.52it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5397023558616638:  92%|██████████▏| 110/119 [00:26<00:01,  5.53it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5633687376976013:  62%|████████     | 94/151 [00:26<00:12,  4.47it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5414935350418091:  39%|█████        | 92/237 [00:25<00:35,  4.14it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5397023558616638:  93%|██████████▎| 111/119 [00:26<00:01,  5.24it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5633687376976013:  63%|████████▏    | 95/151 [00:26<00:12,  4.44it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5414935350418091:  39%|█████        | 93/237 [00:25<00:32,  4.49it/s]Epoch: 1, train for the 112-th batch, train loss: 0.4978034496307373:  93%|██████████▎| 111/119 [00:26<00:01,  5.24it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5080456137657166:  22%|██▊          | 84/383 [00:23<01:06,  4.52it/s]Epoch: 1, train for the 112-th batch, train loss: 0.4978034496307373:  94%|██████████▎| 112/119 [00:26<00:01,  6.02it/s]Epoch: 1, train for the 96-th batch, train loss: 0.5732941031455994:  63%|████████▏    | 95/151 [00:26<00:12,  4.44it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5080456137657166:  22%|██▉          | 85/383 [00:23<01:09,  4.31it/s]Epoch: 1, train for the 96-th batch, train loss: 0.5732941031455994:  64%|████████▎    | 96/151 [00:26<00:10,  5.09it/s]Epoch: 1, train for the 112-th batch, train loss: 0.571032702922821:  76%|█████████   | 111/146 [00:26<00:05,  6.77it/s]Epoch: 1, train for the 112-th batch, train loss: 0.571032702922821:  77%|█████████▏  | 112/146 [00:26<00:06,  5.08it/s]Epoch: 1, train for the 94-th batch, train loss: 0.6057150363922119:  39%|█████        | 93/237 [00:25<00:32,  4.49it/s]Epoch: 1, train for the 102-th batch, train loss: 0.6069274544715881:  42%|████▌      | 101/241 [00:25<00:29,  4.79it/s]Epoch: 1, train for the 102-th batch, train loss: 0.6069274544715881:  42%|████▋      | 102/241 [00:25<00:38,  3.62it/s]Epoch: 1, train for the 94-th batch, train loss: 0.6057150363922119:  40%|█████▏       | 94/237 [00:25<00:30,  4.76it/s]Epoch: 1, train for the 113-th batch, train loss: 0.463116317987442:  94%|███████████▎| 112/119 [00:26<00:01,  6.02it/s]Epoch: 1, train for the 113-th batch, train loss: 0.463116317987442:  95%|███████████▍| 113/119 [00:26<00:00,  6.44it/s]Epoch: 1, train for the 86-th batch, train loss: 0.42958712577819824:  22%|██▋         | 85/383 [00:23<01:09,  4.31it/s]Epoch: 1, train for the 113-th batch, train loss: 0.5310996174812317:  77%|████████▍  | 112/146 [00:26<00:06,  5.08it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5622450113296509:  64%|████████▎    | 96/151 [00:26<00:10,  5.09it/s]Epoch: 1, train for the 113-th batch, train loss: 0.5310996174812317:  77%|████████▌  | 113/146 [00:26<00:06,  5.46it/s]Epoch: 1, train for the 86-th batch, train loss: 0.42958712577819824:  22%|██▋         | 86/383 [00:23<01:05,  4.57it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5622450113296509:  64%|████████▎    | 97/151 [00:26<00:10,  5.19it/s]Epoch: 1, train for the 103-th batch, train loss: 0.5515527129173279:  42%|████▋      | 102/241 [00:25<00:38,  3.62it/s]Epoch: 1, train for the 103-th batch, train loss: 0.5515527129173279:  43%|████▋      | 103/241 [00:25<00:33,  4.16it/s]Epoch: 1, train for the 114-th batch, train loss: 0.4877932667732239:  95%|██████████▍| 113/119 [00:26<00:00,  6.44it/s]Epoch: 1, train for the 114-th batch, train loss: 0.4877932667732239:  96%|██████████▌| 114/119 [00:26<00:00,  6.82it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5494676828384399:  40%|█████▏       | 94/237 [00:25<00:30,  4.76it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5494676828384399:  40%|█████▏       | 95/237 [00:25<00:33,  4.22it/s]Epoch: 1, train for the 115-th batch, train loss: 0.4696401059627533:  96%|██████████▌| 114/119 [00:26<00:00,  6.82it/s]Epoch: 1, train for the 115-th batch, train loss: 0.4696401059627533:  97%|██████████▋| 115/119 [00:26<00:00,  5.84it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5712301135063171:  43%|████▋      | 103/241 [00:25<00:33,  4.16it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5712301135063171:  43%|████▋      | 104/241 [00:25<00:34,  4.02it/s]Epoch: 1, train for the 114-th batch, train loss: 0.5490903854370117:  77%|████████▌  | 113/146 [00:26<00:06,  5.46it/s]Epoch: 1, train for the 87-th batch, train loss: 0.49093931913375854:  22%|██▋         | 86/383 [00:24<01:05,  4.57it/s]Epoch: 1, train for the 98-th batch, train loss: 0.5277490019798279:  64%|████████▎    | 97/151 [00:26<00:10,  5.19it/s]Epoch: 1, train for the 114-th batch, train loss: 0.5490903854370117:  78%|████████▌  | 114/146 [00:26<00:07,  4.36it/s]Epoch: 1, train for the 98-th batch, train loss: 0.5277490019798279:  65%|████████▍    | 98/151 [00:26<00:12,  4.20it/s]Epoch: 1, train for the 87-th batch, train loss: 0.49093931913375854:  23%|██▋         | 87/383 [00:24<01:16,  3.86it/s]Epoch: 1, train for the 96-th batch, train loss: 0.538425862789154:  40%|█████▌        | 95/237 [00:25<00:33,  4.22it/s]Epoch: 1, train for the 96-th batch, train loss: 0.538425862789154:  41%|█████▋        | 96/237 [00:25<00:32,  4.38it/s]Epoch: 1, train for the 116-th batch, train loss: 0.42344930768013:  97%|████████████▌| 115/119 [00:26<00:00,  5.84it/s]Epoch: 1, train for the 116-th batch, train loss: 0.42344930768013:  97%|████████████▋| 116/119 [00:26<00:00,  6.39it/s]Epoch: 1, train for the 105-th batch, train loss: 0.4609363079071045:  43%|████▋      | 104/241 [00:25<00:34,  4.02it/s]Epoch: 1, train for the 105-th batch, train loss: 0.4609363079071045:  44%|████▊      | 105/241 [00:25<00:29,  4.67it/s]Epoch: 1, train for the 99-th batch, train loss: 0.5457153916358948:  65%|████████▍    | 98/151 [00:26<00:12,  4.20it/s]Epoch: 1, train for the 99-th batch, train loss: 0.5457153916358948:  66%|████████▌    | 99/151 [00:26<00:11,  4.55it/s]Epoch: 1, train for the 115-th batch, train loss: 0.5597318410873413:  78%|████████▌  | 114/146 [00:26<00:07,  4.36it/s]Epoch: 1, train for the 115-th batch, train loss: 0.5597318410873413:  79%|████████▋  | 115/146 [00:26<00:07,  4.04it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5441854596138:  44%|██████        | 105/241 [00:26<00:29,  4.67it/s]Epoch: 1, train for the 116-th batch, train loss: 0.5622720718383789:  79%|████████▋  | 115/146 [00:27<00:07,  4.04it/s]Epoch: 1, train for the 116-th batch, train loss: 0.5622720718383789:  79%|████████▋  | 116/146 [00:27<00:06,  4.79it/s]Epoch: 1, train for the 117-th batch, train loss: 0.4187329113483429:  97%|██████████▋| 116/119 [00:27<00:00,  6.39it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5441854596138:  44%|██████▏       | 106/241 [00:26<00:32,  4.13it/s]Epoch: 1, train for the 117-th batch, train loss: 0.4187329113483429:  98%|██████████▊| 117/119 [00:27<00:00,  4.68it/s]Epoch: 1, train for the 88-th batch, train loss: 0.4985826909542084:  23%|██▉          | 87/383 [00:24<01:16,  3.86it/s]Epoch: 1, train for the 100-th batch, train loss: 0.6110520362854004:  66%|███████▊    | 99/151 [00:27<00:11,  4.55it/s]Epoch: 1, train for the 100-th batch, train loss: 0.6110520362854004:  66%|███████▎   | 100/151 [00:27<00:11,  4.31it/s]Epoch: 1, train for the 88-th batch, train loss: 0.4985826909542084:  23%|██▉          | 88/383 [00:24<01:32,  3.20it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5461913347244263:  41%|█████▎       | 96/237 [00:26<00:32,  4.38it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5461913347244263:  41%|█████▎       | 97/237 [00:26<00:40,  3.46it/s]Epoch: 1, train for the 107-th batch, train loss: 0.6218324303627014:  44%|████▊      | 106/241 [00:26<00:32,  4.13it/s]Epoch: 1, train for the 117-th batch, train loss: 0.5829050540924072:  79%|████████▋  | 116/146 [00:27<00:06,  4.79it/s]Epoch: 1, train for the 117-th batch, train loss: 0.5829050540924072:  80%|████████▊  | 117/146 [00:27<00:05,  5.28it/s]Epoch: 1, train for the 107-th batch, train loss: 0.6218324303627014:  44%|████▉      | 107/241 [00:26<00:28,  4.66it/s]Epoch: 1, train for the 118-th batch, train loss: 0.44954004883766174:  98%|█████████▊| 117/119 [00:27<00:00,  4.68it/s]Epoch: 1, train for the 118-th batch, train loss: 0.44954004883766174:  99%|█████████▉| 118/119 [00:27<00:00,  5.07it/s]Epoch: 1, train for the 89-th batch, train loss: 0.48080968856811523:  23%|██▊         | 88/383 [00:24<01:32,  3.20it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5618818402290344:  44%|████▉      | 107/241 [00:26<00:28,  4.66it/s]Epoch: 1, train for the 89-th batch, train loss: 0.48080968856811523:  23%|██▊         | 89/383 [00:24<01:23,  3.50it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5618818402290344:  45%|████▉      | 108/241 [00:26<00:24,  5.38it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5982942581176758:  66%|███████▎   | 100/151 [00:27<00:11,  4.31it/s]Epoch: 1, train for the 98-th batch, train loss: 0.5492209792137146:  41%|█████▎       | 97/237 [00:26<00:40,  3.46it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5982942581176758:  67%|███████▎   | 101/151 [00:27<00:13,  3.84it/s]Epoch: 1, train for the 118-th batch, train loss: 0.5294780731201172:  80%|████████▊  | 117/146 [00:27<00:05,  5.28it/s]Epoch: 1, train for the 98-th batch, train loss: 0.5492209792137146:  41%|█████▍       | 98/237 [00:26<00:41,  3.39it/s]Epoch: 1, train for the 118-th batch, train loss: 0.5294780731201172:  81%|████████▉  | 118/146 [00:27<00:05,  4.84it/s]Epoch: 1, train for the 109-th batch, train loss: 0.6443257927894592:  45%|████▉      | 108/241 [00:26<00:24,  5.38it/s]Epoch: 1, train for the 109-th batch, train loss: 0.6443257927894592:  45%|████▉      | 109/241 [00:26<00:22,  5.77it/s]Epoch: 1, train for the 90-th batch, train loss: 0.48088598251342773:  23%|██▊         | 89/383 [00:25<01:23,  3.50it/s]Epoch: 1, train for the 119-th batch, train loss: 0.36144521832466125:  99%|█████████▉| 118/119 [00:27<00:00,  5.07it/s]Epoch: 1, train for the 90-th batch, train loss: 0.48088598251342773:  23%|██▊         | 90/383 [00:25<01:16,  3.84it/s]Epoch: 1, train for the 119-th batch, train loss: 0.36144521832466125: 100%|██████████| 119/119 [00:27<00:00,  4.33it/s]Epoch: 1, train for the 119-th batch, train loss: 0.36144521832466125: 100%|██████████| 119/119 [00:27<00:00,  4.32it/s]
Epoch: 1, train for the 119-th batch, train loss: 0.5305728912353516:  81%|████████▉  | 118/146 [00:27<00:05,  4.84it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5413416028022766:  67%|███████▎   | 101/151 [00:27<00:13,  3.84it/s]Epoch: 1, train for the 119-th batch, train loss: 0.5305728912353516:  82%|████████▉  | 119/146 [00:27<00:05,  5.31it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5413416028022766:  68%|███████▍   | 102/151 [00:27<00:11,  4.27it/s]Epoch: 1, train for the 110-th batch, train loss: 0.6162704229354858:  45%|████▉      | 109/241 [00:26<00:22,  5.77it/s]Epoch: 1, train for the 99-th batch, train loss: 0.5541556477546692:  41%|█████▍       | 98/237 [00:26<00:41,  3.39it/s]Epoch: 1, train for the 110-th batch, train loss: 0.6162704229354858:  46%|█████      | 110/241 [00:26<00:21,  5.97it/s]Epoch: 1, train for the 99-th batch, train loss: 0.5541556477546692:  42%|█████▍       | 99/237 [00:26<00:36,  3.74it/s]Epoch: 1, train for the 91-th batch, train loss: 0.3934343755245209:  23%|███          | 90/383 [00:25<01:16,  3.84it/s]  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 1, train for the 91-th batch, train loss: 0.3934343755245209:  24%|███          | 91/383 [00:25<01:08,  4.27it/s]Epoch: 1, train for the 111-th batch, train loss: 0.32591983675956726:  46%|████▌     | 110/241 [00:26<00:21,  5.97it/s]evaluate for the 1-th batch, evaluate loss: 0.5207842588424683:   0%|                            | 0/40 [00:00<?, ?it/s]Epoch: 1, train for the 111-th batch, train loss: 0.32591983675956726:  46%|████▌     | 111/241 [00:26<00:20,  6.32it/s]evaluate for the 2-th batch, evaluate loss: 0.5458422899246216:   0%|                            | 0/40 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5458422899246216:   5%|█                   | 2/40 [00:00<00:01, 19.87it/s]evaluate for the 3-th batch, evaluate loss: 0.5496700406074524:   5%|█                   | 2/40 [00:00<00:01, 19.87it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5311922430992126:  82%|████████▉  | 119/146 [00:27<00:05,  5.31it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5401297807693481:  42%|█████       | 99/237 [00:26<00:36,  3.74it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5311922430992126:  82%|█████████  | 120/146 [00:27<00:05,  4.56it/s]Epoch: 1, train for the 103-th batch, train loss: 0.6350986957550049:  68%|███████▍   | 102/151 [00:27<00:11,  4.27it/s]evaluate for the 4-th batch, evaluate loss: 0.6230064034461975:   5%|█                   | 2/40 [00:00<00:01, 19.87it/s]Epoch: 1, train for the 103-th batch, train loss: 0.6350986957550049:  68%|███████▌   | 103/151 [00:27<00:12,  3.96it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5401297807693481:  42%|████▋      | 100/237 [00:26<00:36,  3.78it/s]Epoch: 1, train for the 112-th batch, train loss: 0.34507179260253906:  46%|████▌     | 111/241 [00:26<00:20,  6.32it/s]Epoch: 1, train for the 112-th batch, train loss: 0.34507179260253906:  46%|████▋     | 112/241 [00:26<00:22,  5.61it/s]Epoch: 1, train for the 121-th batch, train loss: 0.5337089896202087:  82%|█████████  | 120/146 [00:28<00:05,  4.56it/s]evaluate for the 5-th batch, evaluate loss: 0.5970845818519592:   5%|█                   | 2/40 [00:00<00:01, 19.87it/s]evaluate for the 5-th batch, evaluate loss: 0.5970845818519592:  12%|██▌                 | 5/40 [00:00<00:02, 15.62it/s]Epoch: 1, train for the 121-th batch, train loss: 0.5337089896202087:  83%|█████████  | 121/146 [00:28<00:04,  5.19it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5707578659057617:  68%|███████▌   | 103/151 [00:28<00:12,  3.96it/s]evaluate for the 6-th batch, evaluate loss: 0.5665887594223022:  12%|██▌                 | 5/40 [00:00<00:02, 15.62it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5707578659057617:  69%|███████▌   | 104/151 [00:28<00:10,  4.40it/s]Epoch: 1, train for the 101-th batch, train loss: 0.595850944519043:  42%|█████       | 100/237 [00:27<00:36,  3.78it/s]Epoch: 1, train for the 92-th batch, train loss: 0.42096349596977234:  24%|██▊         | 91/383 [00:25<01:08,  4.27it/s]Epoch: 1, train for the 113-th batch, train loss: 0.3645919859409332:  46%|█████      | 112/241 [00:27<00:22,  5.61it/s]Epoch: 1, train for the 101-th batch, train loss: 0.595850944519043:  43%|█████       | 101/237 [00:27<00:33,  4.07it/s]Epoch: 1, train for the 113-th batch, train loss: 0.3645919859409332:  47%|█████▏     | 113/241 [00:27<00:21,  6.06it/s]Epoch: 1, train for the 92-th batch, train loss: 0.42096349596977234:  24%|██▉         | 92/383 [00:25<01:25,  3.39it/s]evaluate for the 7-th batch, evaluate loss: 0.5890670418739319:  12%|██▌                 | 5/40 [00:00<00:02, 15.62it/s]evaluate for the 7-th batch, evaluate loss: 0.5890670418739319:  18%|███▌                | 7/40 [00:00<00:02, 14.99it/s]Epoch: 1, train for the 122-th batch, train loss: 0.5695372819900513:  83%|█████████  | 121/146 [00:28<00:04,  5.19it/s]Epoch: 1, train for the 122-th batch, train loss: 0.5695372819900513:  84%|█████████▏ | 122/146 [00:28<00:04,  5.38it/s]evaluate for the 8-th batch, evaluate loss: 0.5578437447547913:  18%|███▌                | 7/40 [00:00<00:02, 14.99it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5273492932319641:  24%|███          | 92/383 [00:25<01:25,  3.39it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5273492932319641:  24%|███▏         | 93/383 [00:25<01:13,  3.93it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5799055695533752:  43%|████▋      | 101/237 [00:27<00:33,  4.07it/s]Epoch: 1, train for the 105-th batch, train loss: 0.5391702055931091:  69%|███████▌   | 104/151 [00:28<00:10,  4.40it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5799055695533752:  43%|████▋      | 102/237 [00:27<00:33,  4.07it/s]Epoch: 1, train for the 105-th batch, train loss: 0.5391702055931091:  70%|███████▋   | 105/151 [00:28<00:11,  4.05it/s]evaluate for the 9-th batch, evaluate loss: 0.5802997350692749:  18%|███▌                | 7/40 [00:00<00:02, 14.99it/s]evaluate for the 9-th batch, evaluate loss: 0.5802997350692749:  22%|████▌               | 9/40 [00:00<00:02, 12.35it/s]Epoch: 1, train for the 123-th batch, train loss: 0.5952675342559814:  84%|█████████▏ | 122/146 [00:28<00:04,  5.38it/s]Epoch: 1, train for the 123-th batch, train loss: 0.5952675342559814:  84%|█████████▎ | 123/146 [00:28<00:04,  4.98it/s]Epoch: 1, train for the 114-th batch, train loss: 0.36950454115867615:  47%|████▋     | 113/241 [00:27<00:21,  6.06it/s]evaluate for the 10-th batch, evaluate loss: 0.6123245358467102:  22%|████▎              | 9/40 [00:00<00:02, 12.35it/s]Epoch: 1, train for the 114-th batch, train loss: 0.36950454115867615:  47%|████▋     | 114/241 [00:27<00:27,  4.63it/s]Epoch: 1, train for the 94-th batch, train loss: 0.4026309847831726:  24%|███▏         | 93/383 [00:26<01:13,  3.93it/s]Epoch: 1, train for the 94-th batch, train loss: 0.4026309847831726:  25%|███▏         | 94/383 [00:26<01:08,  4.19it/s]Epoch: 1, train for the 103-th batch, train loss: 0.5986170768737793:  43%|████▋      | 102/237 [00:27<00:33,  4.07it/s]evaluate for the 11-th batch, evaluate loss: 0.567829966545105:  22%|████▌               | 9/40 [00:00<00:02, 12.35it/s]evaluate for the 11-th batch, evaluate loss: 0.567829966545105:  28%|█████▏             | 11/40 [00:00<00:02, 12.39it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5618669986724854:  70%|███████▋   | 105/151 [00:28<00:11,  4.05it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5618669986724854:  70%|███████▋   | 106/151 [00:28<00:10,  4.36it/s]Epoch: 1, train for the 103-th batch, train loss: 0.5986170768737793:  43%|████▊      | 103/237 [00:27<00:31,  4.27it/s]Epoch: 1, train for the 124-th batch, train loss: 0.5353869795799255:  84%|█████████▎ | 123/146 [00:28<00:04,  4.98it/s]Epoch: 1, train for the 115-th batch, train loss: 0.474239706993103:  47%|█████▋      | 114/241 [00:27<00:27,  4.63it/s]Epoch: 1, train for the 124-th batch, train loss: 0.5353869795799255:  85%|█████████▎ | 124/146 [00:28<00:04,  5.25it/s]evaluate for the 12-th batch, evaluate loss: 0.5987159609794617:  28%|████▉             | 11/40 [00:00<00:02, 12.39it/s]Epoch: 1, train for the 115-th batch, train loss: 0.474239706993103:  48%|█████▋      | 115/241 [00:27<00:24,  5.22it/s]Epoch: 1, train for the 125-th batch, train loss: 0.5382401347160339:  85%|█████████▎ | 124/146 [00:28<00:04,  5.25it/s]Epoch: 1, train for the 125-th batch, train loss: 0.5382401347160339:  86%|█████████▍ | 125/146 [00:28<00:03,  5.75it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5332282781600952:  25%|███▏         | 94/383 [00:26<01:08,  4.19it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5499180555343628:  43%|████▊      | 103/237 [00:27<00:31,  4.27it/s]evaluate for the 13-th batch, evaluate loss: 0.6104764342308044:  28%|████▉             | 11/40 [00:01<00:02, 12.39it/s]evaluate for the 13-th batch, evaluate loss: 0.6104764342308044:  32%|█████▊            | 13/40 [00:01<00:02, 10.26it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5499180555343628:  44%|████▊      | 104/237 [00:27<00:31,  4.23it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5332282781600952:  25%|███▏         | 95/383 [00:26<01:15,  3.82it/s]evaluate for the 14-th batch, evaluate loss: 0.5869560241699219:  32%|█████▊            | 13/40 [00:01<00:02, 10.26it/s]Epoch: 1, train for the 116-th batch, train loss: 0.6107048392295837:  48%|█████▏     | 115/241 [00:27<00:24,  5.22it/s]Epoch: 1, train for the 107-th batch, train loss: 0.5606757402420044:  70%|███████▋   | 106/151 [00:28<00:10,  4.36it/s]Epoch: 1, train for the 116-th batch, train loss: 0.6107048392295837:  48%|█████▎     | 116/241 [00:27<00:27,  4.48it/s]Epoch: 1, train for the 107-th batch, train loss: 0.5606757402420044:  71%|███████▊   | 107/151 [00:28<00:11,  3.80it/s]evaluate for the 15-th batch, evaluate loss: 0.6064330339431763:  32%|█████▊            | 13/40 [00:01<00:02, 10.26it/s]evaluate for the 15-th batch, evaluate loss: 0.6064330339431763:  38%|██████▊           | 15/40 [00:01<00:02, 11.22it/s]Epoch: 1, train for the 105-th batch, train loss: 0.5459133982658386:  44%|████▊      | 104/237 [00:27<00:31,  4.23it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5560036897659302:  86%|█████████▍ | 125/146 [00:28<00:03,  5.75it/s]Epoch: 1, train for the 105-th batch, train loss: 0.5459133982658386:  44%|████▊      | 105/237 [00:28<00:28,  4.61it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5560036897659302:  86%|█████████▍ | 126/146 [00:29<00:03,  5.05it/s]evaluate for the 16-th batch, evaluate loss: 0.5922871232032776:  38%|██████▊           | 15/40 [00:01<00:02, 11.22it/s]Epoch: 1, train for the 96-th batch, train loss: 0.4719903767108917:  25%|███▏         | 95/383 [00:26<01:15,  3.82it/s]Epoch: 1, train for the 96-th batch, train loss: 0.4719903767108917:  25%|███▎         | 96/383 [00:26<01:10,  4.07it/s]Epoch: 1, train for the 117-th batch, train loss: 0.6977013945579529:  48%|█████▎     | 116/241 [00:28<00:27,  4.48it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5237101912498474:  71%|███████▊   | 107/151 [00:29<00:11,  3.80it/s]evaluate for the 17-th batch, evaluate loss: 0.6529441475868225:  38%|██████▊           | 15/40 [00:01<00:02, 11.22it/s]evaluate for the 17-th batch, evaluate loss: 0.6529441475868225:  42%|███████▋          | 17/40 [00:01<00:01, 12.21it/s]Epoch: 1, train for the 117-th batch, train loss: 0.6977013945579529:  49%|█████▎     | 117/241 [00:28<00:26,  4.76it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5237101912498474:  72%|███████▊   | 108/151 [00:29<00:10,  4.21it/s]Epoch: 1, train for the 127-th batch, train loss: 0.5009449124336243:  86%|█████████▍ | 126/146 [00:29<00:03,  5.05it/s]evaluate for the 18-th batch, evaluate loss: 0.6152696013450623:  42%|███████▋          | 17/40 [00:01<00:01, 12.21it/s]Epoch: 1, train for the 127-th batch, train loss: 0.5009449124336243:  87%|█████████▌ | 127/146 [00:29<00:03,  5.44it/s]Epoch: 1, train for the 106-th batch, train loss: 0.58347487449646:  44%|█████▊       | 105/237 [00:28<00:28,  4.61it/s]Epoch: 1, train for the 106-th batch, train loss: 0.58347487449646:  45%|█████▊       | 106/237 [00:28<00:27,  4.78it/s]Epoch: 1, train for the 97-th batch, train loss: 0.3784199357032776:  25%|███▎         | 96/383 [00:26<01:10,  4.07it/s]Epoch: 1, train for the 97-th batch, train loss: 0.3784199357032776:  25%|███▎         | 97/383 [00:26<01:03,  4.49it/s]Epoch: 1, train for the 109-th batch, train loss: 0.5306887030601501:  72%|███████▊   | 108/151 [00:29<00:10,  4.21it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5697698593139648:  87%|█████████▌ | 127/146 [00:29<00:03,  5.44it/s]Epoch: 1, train for the 109-th batch, train loss: 0.5306887030601501:  72%|███████▉   | 109/151 [00:29<00:08,  4.70it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5697698593139648:  88%|█████████▋ | 128/146 [00:29<00:02,  6.23it/s]Epoch: 1, train for the 118-th batch, train loss: 0.5792137980461121:  49%|█████▎     | 117/241 [00:28<00:26,  4.76it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5641709566116333:  88%|█████████▋ | 128/146 [00:29<00:02,  6.23it/s]evaluate for the 19-th batch, evaluate loss: 0.6236129403114319:  42%|███████▋          | 17/40 [00:01<00:01, 12.21it/s]evaluate for the 19-th batch, evaluate loss: 0.6236129403114319:  48%|████████▌         | 19/40 [00:01<00:02,  9.59it/s]Epoch: 1, train for the 98-th batch, train loss: 0.339263916015625:  25%|███▌          | 97/383 [00:26<01:03,  4.49it/s]Epoch: 1, train for the 118-th batch, train loss: 0.5792137980461121:  49%|█████▍     | 118/241 [00:28<00:29,  4.16it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5641709566116333:  88%|█████████▋ | 129/146 [00:29<00:02,  6.34it/s]Epoch: 1, train for the 98-th batch, train loss: 0.339263916015625:  26%|███▌          | 98/383 [00:26<01:01,  4.64it/s]Epoch: 1, train for the 107-th batch, train loss: 0.5627509355545044:  45%|████▉      | 106/237 [00:28<00:27,  4.78it/s]Epoch: 1, train for the 110-th batch, train loss: 0.5721877813339233:  72%|███████▉   | 109/151 [00:29<00:08,  4.70it/s]evaluate for the 20-th batch, evaluate loss: 0.6019224524497986:  48%|████████▌         | 19/40 [00:01<00:02,  9.59it/s]Epoch: 1, train for the 107-th batch, train loss: 0.5627509355545044:  45%|████▉      | 107/237 [00:28<00:29,  4.40it/s]Epoch: 1, train for the 110-th batch, train loss: 0.5721877813339233:  73%|████████   | 110/151 [00:29<00:08,  4.71it/s]evaluate for the 21-th batch, evaluate loss: 0.6316078305244446:  48%|████████▌         | 19/40 [00:01<00:02,  9.59it/s]evaluate for the 21-th batch, evaluate loss: 0.6316078305244446:  52%|█████████▍        | 21/40 [00:01<00:01, 10.45it/s]Epoch: 1, train for the 119-th batch, train loss: 0.43812644481658936:  49%|████▉     | 118/241 [00:28<00:29,  4.16it/s]Epoch: 1, train for the 119-th batch, train loss: 0.43812644481658936:  49%|████▉     | 119/241 [00:28<00:26,  4.59it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5254855751991272:  88%|█████████▋ | 129/146 [00:29<00:02,  6.34it/s]Epoch: 1, train for the 99-th batch, train loss: 0.4966159760951996:  26%|███▎         | 98/383 [00:27<01:01,  4.64it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5254855751991272:  89%|█████████▊ | 130/146 [00:29<00:02,  5.92it/s]evaluate for the 22-th batch, evaluate loss: 0.5787143707275391:  52%|█████████▍        | 21/40 [00:01<00:01, 10.45it/s]Epoch: 1, train for the 99-th batch, train loss: 0.4966159760951996:  26%|███▎         | 99/383 [00:27<01:00,  4.68it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5471494793891907:  73%|████████   | 110/151 [00:29<00:08,  4.71it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5471494793891907:  74%|████████   | 111/151 [00:29<00:08,  4.83it/s]Epoch: 1, train for the 120-th batch, train loss: 0.6383547186851501:  49%|█████▍     | 119/241 [00:28<00:26,  4.59it/s]Epoch: 1, train for the 120-th batch, train loss: 0.6383547186851501:  50%|█████▍     | 120/241 [00:28<00:23,  5.23it/s]evaluate for the 23-th batch, evaluate loss: 0.5463576316833496:  52%|█████████▍        | 21/40 [00:02<00:01, 10.45it/s]evaluate for the 23-th batch, evaluate loss: 0.5463576316833496:  57%|██████████▎       | 23/40 [00:02<00:01, 10.07it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5856931805610657:  45%|████▉      | 107/237 [00:28<00:29,  4.40it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5856931805610657:  46%|█████      | 108/237 [00:28<00:33,  3.88it/s]evaluate for the 24-th batch, evaluate loss: 0.600719153881073:  57%|██████████▉        | 23/40 [00:02<00:01, 10.07it/s]evaluate for the 25-th batch, evaluate loss: 0.628785252571106:  57%|██████████▉        | 23/40 [00:02<00:01, 10.07it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5667879581451416:  26%|███         | 99/383 [00:27<01:00,  4.68it/s]Epoch: 1, train for the 112-th batch, train loss: 0.4888744354248047:  74%|████████   | 111/151 [00:29<00:08,  4.83it/s]Epoch: 1, train for the 121-th batch, train loss: 0.6254822611808777:  50%|█████▍     | 120/241 [00:28<00:23,  5.23it/s]Epoch: 1, train for the 112-th batch, train loss: 0.4888744354248047:  74%|████████▏  | 112/151 [00:29<00:08,  4.66it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5667879581451416:  26%|██▊        | 100/383 [00:27<01:04,  4.38it/s]Epoch: 1, train for the 121-th batch, train loss: 0.6254822611808777:  50%|█████▌     | 121/241 [00:28<00:23,  5.16it/s]Epoch: 1, train for the 109-th batch, train loss: 0.6136753559112549:  46%|█████      | 108/237 [00:28<00:33,  3.88it/s]evaluate for the 26-th batch, evaluate loss: 0.5681367516517639:  57%|██████████▎       | 23/40 [00:02<00:01, 10.07it/s]evaluate for the 26-th batch, evaluate loss: 0.5681367516517639:  65%|███████████▋      | 26/40 [00:02<00:01, 12.47it/s]Epoch: 1, train for the 131-th batch, train loss: 0.5423017144203186:  89%|█████████▊ | 130/146 [00:29<00:02,  5.92it/s]Epoch: 1, train for the 109-th batch, train loss: 0.6136753559112549:  46%|█████      | 109/237 [00:28<00:28,  4.43it/s]Epoch: 1, train for the 131-th batch, train loss: 0.5423017144203186:  90%|█████████▊ | 131/146 [00:29<00:03,  4.52it/s]evaluate for the 27-th batch, evaluate loss: 0.624008297920227:  65%|████████████▎      | 26/40 [00:02<00:01, 12.47it/s]Epoch: 1, train for the 113-th batch, train loss: 0.55353844165802:  74%|█████████▋   | 112/151 [00:30<00:08,  4.66it/s]Epoch: 1, train for the 113-th batch, train loss: 0.55353844165802:  75%|█████████▋   | 113/151 [00:30<00:07,  5.07it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5345271825790405:  26%|██▊        | 100/383 [00:27<01:04,  4.38it/s]evaluate for the 28-th batch, evaluate loss: 0.5683823823928833:  65%|███████████▋      | 26/40 [00:02<00:01, 12.47it/s]evaluate for the 28-th batch, evaluate loss: 0.5683823823928833:  70%|████████████▌     | 28/40 [00:02<00:00, 12.38it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5345271825790405:  26%|██▉        | 101/383 [00:27<01:03,  4.45it/s]Epoch: 1, train for the 110-th batch, train loss: 0.577354371547699:  46%|█████▌      | 109/237 [00:29<00:28,  4.43it/s]Epoch: 1, train for the 110-th batch, train loss: 0.577354371547699:  46%|█████▌      | 110/237 [00:29<00:27,  4.58it/s]evaluate for the 29-th batch, evaluate loss: 0.6123726963996887:  70%|████████████▌     | 28/40 [00:02<00:00, 12.38it/s]evaluate for the 30-th batch, evaluate loss: 0.6227919459342957:  70%|████████████▌     | 28/40 [00:02<00:00, 12.38it/s]evaluate for the 30-th batch, evaluate loss: 0.6227919459342957:  75%|█████████████▌    | 30/40 [00:02<00:00, 11.87it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5173352956771851:  90%|█████████▊ | 131/146 [00:30<00:03,  4.52it/s]evaluate for the 31-th batch, evaluate loss: 0.5842358469963074:  75%|█████████████▌    | 30/40 [00:02<00:00, 11.87it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5173352956771851:  90%|█████████▉ | 132/146 [00:30<00:03,  3.70it/s]Epoch: 1, train for the 102-th batch, train loss: 0.484028160572052:  26%|███▏        | 101/383 [00:27<01:03,  4.45it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5693441033363342:  46%|█████      | 110/237 [00:29<00:27,  4.58it/s]Epoch: 1, train for the 102-th batch, train loss: 0.484028160572052:  27%|███▏        | 102/383 [00:27<01:06,  4.23it/s]Epoch: 1, train for the 114-th batch, train loss: 0.4968711733818054:  75%|████████▏  | 113/151 [00:30<00:07,  5.07it/s]Epoch: 1, train for the 122-th batch, train loss: 0.5507530570030212:  50%|█████▌     | 121/241 [00:29<00:23,  5.16it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5693441033363342:  47%|█████▏     | 111/237 [00:29<00:28,  4.41it/s]evaluate for the 32-th batch, evaluate loss: 0.627192497253418:  75%|██████████████▎    | 30/40 [00:02<00:00, 11.87it/s]evaluate for the 32-th batch, evaluate loss: 0.627192497253418:  80%|███████████████▏   | 32/40 [00:02<00:00, 12.96it/s]Epoch: 1, train for the 114-th batch, train loss: 0.4968711733818054:  75%|████████▎  | 114/151 [00:30<00:08,  4.15it/s]Epoch: 1, train for the 122-th batch, train loss: 0.5507530570030212:  51%|█████▌     | 122/241 [00:29<00:33,  3.52it/s]Epoch: 1, train for the 133-th batch, train loss: 0.4888966381549835:  90%|█████████▉ | 132/146 [00:30<00:03,  3.70it/s]evaluate for the 33-th batch, evaluate loss: 0.6012834310531616:  80%|██████████████▍   | 32/40 [00:02<00:00, 12.96it/s]Epoch: 1, train for the 133-th batch, train loss: 0.4888966381549835:  91%|██████████ | 133/146 [00:30<00:03,  4.26it/s]Epoch: 1, train for the 103-th batch, train loss: 0.3282330334186554:  27%|██▉        | 102/383 [00:28<01:06,  4.23it/s]evaluate for the 34-th batch, evaluate loss: 0.6718365550041199:  80%|██████████████▍   | 32/40 [00:02<00:00, 12.96it/s]evaluate for the 34-th batch, evaluate loss: 0.6718365550041199:  85%|███████████████▎  | 34/40 [00:02<00:00, 12.72it/s]Epoch: 1, train for the 103-th batch, train loss: 0.3282330334186554:  27%|██▉        | 103/383 [00:28<01:02,  4.50it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5451300144195557:  91%|██████████ | 133/146 [00:30<00:03,  4.26it/s]Epoch: 1, train for the 123-th batch, train loss: 0.6536162495613098:  51%|█████▌     | 122/241 [00:29<00:33,  3.52it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5451300144195557:  92%|██████████ | 134/146 [00:30<00:02,  5.04it/s]Epoch: 1, train for the 115-th batch, train loss: 0.4868803322315216:  75%|████████▎  | 114/151 [00:30<00:08,  4.15it/s]Epoch: 1, train for the 123-th batch, train loss: 0.6536162495613098:  51%|█████▌     | 123/241 [00:29<00:31,  3.79it/s]Epoch: 1, train for the 115-th batch, train loss: 0.4868803322315216:  76%|████████▍  | 115/151 [00:30<00:08,  4.12it/s]Epoch: 1, train for the 135-th batch, train loss: 0.546449601650238:  92%|███████████ | 134/146 [00:30<00:02,  5.04it/s]Epoch: 1, train for the 135-th batch, train loss: 0.546449601650238:  92%|███████████ | 135/146 [00:30<00:01,  5.82it/s]Epoch: 1, train for the 124-th batch, train loss: 0.42669403553009033:  51%|█████     | 123/241 [00:29<00:31,  3.79it/s]Epoch: 1, train for the 124-th batch, train loss: 0.42669403553009033:  51%|█████▏    | 124/241 [00:29<00:26,  4.41it/s]Epoch: 1, train for the 104-th batch, train loss: 0.4231724739074707:  27%|██▉        | 103/383 [00:28<01:02,  4.50it/s]evaluate for the 35-th batch, evaluate loss: 0.6368436217308044:  85%|███████████████▎  | 34/40 [00:03<00:00, 12.72it/s]Epoch: 1, train for the 116-th batch, train loss: 0.47056517004966736:  76%|███████▌  | 115/151 [00:30<00:08,  4.12it/s]Epoch: 1, train for the 112-th batch, train loss: 0.5816563367843628:  47%|█████▏     | 111/237 [00:29<00:28,  4.41it/s]Epoch: 1, train for the 136-th batch, train loss: 0.5186399817466736:  92%|██████████▏| 135/146 [00:30<00:01,  5.82it/s]Epoch: 1, train for the 104-th batch, train loss: 0.4231724739074707:  27%|██▉        | 104/383 [00:28<01:06,  4.20it/s]Epoch: 1, train for the 136-th batch, train loss: 0.5186399817466736:  93%|██████████▏| 136/146 [00:30<00:01,  6.24it/s]Epoch: 1, train for the 116-th batch, train loss: 0.47056517004966736:  77%|███████▋  | 116/151 [00:30<00:08,  4.32it/s]evaluate for the 36-th batch, evaluate loss: 0.6590726971626282:  85%|███████████████▎  | 34/40 [00:03<00:00, 12.72it/s]evaluate for the 36-th batch, evaluate loss: 0.6590726971626282:  90%|████████████████▏ | 36/40 [00:03<00:00, 10.06it/s]Epoch: 1, train for the 112-th batch, train loss: 0.5816563367843628:  47%|█████▏     | 112/237 [00:29<00:37,  3.35it/s]Epoch: 1, train for the 125-th batch, train loss: 0.5534836649894714:  51%|█████▋     | 124/241 [00:29<00:26,  4.41it/s]Epoch: 1, train for the 125-th batch, train loss: 0.5534836649894714:  52%|█████▋     | 125/241 [00:29<00:23,  5.02it/s]evaluate for the 37-th batch, evaluate loss: 0.6612765192985535:  90%|████████████████▏ | 36/40 [00:03<00:00, 10.06it/s]evaluate for the 38-th batch, evaluate loss: 0.6779365539550781:  90%|████████████████▏ | 36/40 [00:03<00:00, 10.06it/s]evaluate for the 38-th batch, evaluate loss: 0.6779365539550781:  95%|█████████████████ | 38/40 [00:03<00:00, 11.13it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5020073652267456:  52%|█████▋     | 125/241 [00:30<00:23,  5.02it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5020073652267456:  52%|█████▊     | 126/241 [00:30<00:21,  5.32it/s]Epoch: 1, train for the 113-th batch, train loss: 0.5853753685951233:  47%|█████▏     | 112/237 [00:30<00:37,  3.35it/s]Epoch: 1, train for the 113-th batch, train loss: 0.5853753685951233:  48%|█████▏     | 113/237 [00:30<00:34,  3.56it/s]Epoch: 1, train for the 137-th batch, train loss: 0.5028939843177795:  93%|██████████▏| 136/146 [00:31<00:01,  6.24it/s]Epoch: 1, train for the 105-th batch, train loss: 0.3255462944507599:  27%|██▉        | 104/383 [00:28<01:06,  4.20it/s]Epoch: 1, train for the 137-th batch, train loss: 0.5028939843177795:  94%|██████████▎| 137/146 [00:31<00:01,  4.90it/s]Epoch: 1, train for the 105-th batch, train loss: 0.3255462944507599:  27%|███        | 105/383 [00:28<01:15,  3.70it/s]Epoch: 1, train for the 114-th batch, train loss: 0.5501583218574524:  48%|█████▏     | 113/237 [00:30<00:34,  3.56it/s]Epoch: 1, train for the 117-th batch, train loss: 0.5315738320350647:  77%|████████▍  | 116/151 [00:31<00:08,  4.32it/s]Epoch: 1, train for the 127-th batch, train loss: 0.6135058403015137:  52%|█████▊     | 126/241 [00:30<00:21,  5.32it/s]Epoch: 1, train for the 114-th batch, train loss: 0.5501583218574524:  48%|█████▎     | 114/237 [00:30<00:30,  3.99it/s]Epoch: 1, train for the 138-th batch, train loss: 0.5482919812202454:  94%|██████████▎| 137/146 [00:31<00:01,  4.90it/s]evaluate for the 39-th batch, evaluate loss: 0.6763301491737366:  95%|█████████████████ | 38/40 [00:03<00:00, 11.13it/s]Epoch: 1, train for the 117-th batch, train loss: 0.5315738320350647:  77%|████████▌  | 117/151 [00:31<00:09,  3.42it/s]Epoch: 1, train for the 138-th batch, train loss: 0.5482919812202454:  95%|██████████▍| 138/146 [00:31<00:01,  5.44it/s]Epoch: 1, train for the 127-th batch, train loss: 0.6135058403015137:  53%|█████▊     | 127/241 [00:30<00:23,  4.81it/s]evaluate for the 40-th batch, evaluate loss: 0.856203019618988:  95%|██████████████████ | 38/40 [00:03<00:00, 11.13it/s]evaluate for the 40-th batch, evaluate loss: 0.856203019618988: 100%|███████████████████| 40/40 [00:03<00:00,  8.85it/s]evaluate for the 40-th batch, evaluate loss: 0.856203019618988: 100%|███████████████████| 40/40 [00:03<00:00, 11.12it/s]
Epoch: 1, train for the 106-th batch, train loss: 0.3200826048851013:  27%|███        | 105/383 [00:28<01:15,  3.70it/s]Epoch: 1, train for the 106-th batch, train loss: 0.3200826048851013:  28%|███        | 106/383 [00:28<01:08,  4.03it/s]Epoch: 1, train for the 118-th batch, train loss: 0.47892242670059204:  77%|███████▋  | 117/151 [00:31<00:09,  3.42it/s]Epoch: 1, train for the 118-th batch, train loss: 0.47892242670059204:  78%|███████▊  | 118/151 [00:31<00:08,  3.87it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5531509518623352:  95%|██████████▍| 138/146 [00:31<00:01,  5.44it/s]Epoch: 1, train for the 115-th batch, train loss: 0.5339283347129822:  48%|█████▎     | 114/237 [00:30<00:30,  3.99it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5531509518623352:  95%|██████████▍| 139/146 [00:31<00:01,  5.45it/s]Epoch: 1, train for the 115-th batch, train loss: 0.5339283347129822:  49%|█████▎     | 115/237 [00:30<00:29,  4.18it/s]Epoch: 1, train for the 107-th batch, train loss: 0.47771093249320984:  28%|██▊       | 106/383 [00:29<01:08,  4.03it/s]  0%|                                                                                            | 0/21 [00:00<?, ?it/s]Epoch: 1, train for the 107-th batch, train loss: 0.47771093249320984:  28%|██▊       | 107/383 [00:29<01:02,  4.40it/s]evaluate for the 1-th batch, evaluate loss: 0.7362448573112488:   0%|                            | 0/21 [00:00<?, ?it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5839263200759888:  53%|█████▊     | 127/241 [00:30<00:23,  4.81it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5839263200759888:  53%|█████▊     | 128/241 [00:30<00:27,  4.10it/s]evaluate for the 2-th batch, evaluate loss: 0.7538712620735168:   0%|                            | 0/21 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.7538712620735168:  10%|█▉                  | 2/21 [00:00<00:00, 19.54it/s]evaluate for the 3-th batch, evaluate loss: 0.7421914339065552:  10%|█▉                  | 2/21 [00:00<00:00, 19.54it/s]Epoch: 1, train for the 119-th batch, train loss: 0.5224573016166687:  78%|████████▌  | 118/151 [00:31<00:08,  3.87it/s]Epoch: 1, train for the 119-th batch, train loss: 0.5224573016166687:  79%|████████▋  | 119/151 [00:31<00:08,  3.76it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5111778378486633:  53%|█████▊     | 128/241 [00:30<00:27,  4.10it/s]Epoch: 1, train for the 116-th batch, train loss: 0.62882000207901:  49%|██████▎      | 115/237 [00:30<00:29,  4.18it/s]Epoch: 1, train for the 108-th batch, train loss: 0.43677765130996704:  28%|██▊       | 107/383 [00:29<01:02,  4.40it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5111778378486633:  54%|█████▉     | 129/241 [00:30<00:23,  4.76it/s]Epoch: 1, train for the 116-th batch, train loss: 0.62882000207901:  49%|██████▎      | 116/237 [00:30<00:30,  3.95it/s]Epoch: 1, train for the 140-th batch, train loss: 0.523707926273346:  95%|███████████▍| 139/146 [00:31<00:01,  5.45it/s]evaluate for the 4-th batch, evaluate loss: 0.7068189978599548:  10%|█▉                  | 2/21 [00:00<00:00, 19.54it/s]evaluate for the 4-th batch, evaluate loss: 0.7068189978599548:  19%|███▊                | 4/21 [00:00<00:01, 16.70it/s]Epoch: 1, train for the 140-th batch, train loss: 0.523707926273346:  96%|███████████▌| 140/146 [00:31<00:01,  4.46it/s]Epoch: 1, train for the 108-th batch, train loss: 0.43677765130996704:  28%|██▊       | 108/383 [00:29<01:03,  4.35it/s]evaluate for the 5-th batch, evaluate loss: 0.7720149159431458:  19%|███▊                | 4/21 [00:00<00:01, 16.70it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5059067606925964:  54%|█████▉     | 129/241 [00:30<00:23,  4.76it/s]evaluate for the 6-th batch, evaluate loss: 0.7704247832298279:  19%|███▊                | 4/21 [00:00<00:01, 16.70it/s]evaluate for the 6-th batch, evaluate loss: 0.7704247832298279:  29%|█████▋              | 6/21 [00:00<00:00, 15.66it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5059067606925964:  54%|█████▉     | 130/241 [00:30<00:21,  5.08it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5380920767784119:  79%|████████▋  | 119/151 [00:31<00:08,  3.76it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5380920767784119:  79%|████████▋  | 120/151 [00:31<00:07,  3.94it/s]evaluate for the 7-th batch, evaluate loss: 0.7221032381057739:  29%|█████▋              | 6/21 [00:00<00:00, 15.66it/s]Epoch: 1, train for the 117-th batch, train loss: 0.5673620700836182:  49%|█████▍     | 116/237 [00:30<00:30,  3.95it/s]Epoch: 1, train for the 117-th batch, train loss: 0.5673620700836182:  49%|█████▍     | 117/237 [00:31<00:29,  4.07it/s]Epoch: 1, train for the 109-th batch, train loss: 0.43023955821990967:  28%|██▊       | 108/383 [00:29<01:03,  4.35it/s]evaluate for the 8-th batch, evaluate loss: 0.7453360557556152:  29%|█████▋              | 6/21 [00:00<00:00, 15.66it/s]Epoch: 1, train for the 131-th batch, train loss: 0.5548768043518066:  54%|█████▉     | 130/241 [00:31<00:21,  5.08it/s]Epoch: 1, train for the 109-th batch, train loss: 0.43023955821990967:  28%|██▊       | 109/383 [00:29<01:04,  4.26it/s]Epoch: 1, train for the 131-th batch, train loss: 0.5548768043518066:  54%|█████▉     | 131/241 [00:31<00:19,  5.69it/s]evaluate for the 9-th batch, evaluate loss: 0.7625836133956909:  29%|█████▋              | 6/21 [00:00<00:00, 15.66it/s]evaluate for the 9-th batch, evaluate loss: 0.7625836133956909:  43%|████████▌           | 9/21 [00:00<00:00, 17.70it/s]evaluate for the 10-th batch, evaluate loss: 0.7649845480918884:  43%|████████▏          | 9/21 [00:00<00:00, 17.70it/s]Epoch: 1, train for the 141-th batch, train loss: 0.5348172783851624:  96%|██████████▌| 140/146 [00:32<00:01,  4.46it/s]Epoch: 1, train for the 121-th batch, train loss: 0.5072742104530334:  79%|████████▋  | 120/151 [00:32<00:07,  3.94it/s]Epoch: 1, train for the 141-th batch, train loss: 0.5348172783851624:  97%|██████████▌| 141/146 [00:32<00:01,  3.56it/s]Epoch: 1, train for the 118-th batch, train loss: 0.562131404876709:  49%|█████▉      | 117/237 [00:31<00:29,  4.07it/s]evaluate for the 11-th batch, evaluate loss: 0.7719883322715759:  43%|████████▏          | 9/21 [00:00<00:00, 17.70it/s]evaluate for the 11-th batch, evaluate loss: 0.7719883322715759:  52%|█████████▍        | 11/21 [00:00<00:00, 16.94it/s]Epoch: 1, train for the 121-th batch, train loss: 0.5072742104530334:  80%|████████▊  | 121/151 [00:32<00:07,  3.99it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5002352595329285:  54%|█████▉     | 131/241 [00:31<00:19,  5.69it/s]Epoch: 1, train for the 110-th batch, train loss: 0.3788513243198395:  28%|███▏       | 109/383 [00:29<01:04,  4.26it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5002352595329285:  55%|██████     | 132/241 [00:31<00:19,  5.59it/s]Epoch: 1, train for the 118-th batch, train loss: 0.562131404876709:  50%|█████▉      | 118/237 [00:31<00:29,  4.10it/s]Epoch: 1, train for the 110-th batch, train loss: 0.3788513243198395:  29%|███▏       | 110/383 [00:29<01:02,  4.36it/s]evaluate for the 12-th batch, evaluate loss: 0.7234304547309875:  52%|█████████▍        | 11/21 [00:00<00:00, 16.94it/s]Epoch: 1, train for the 142-th batch, train loss: 0.5070449113845825:  97%|██████████▌| 141/146 [00:32<00:01,  3.56it/s]Epoch: 1, train for the 142-th batch, train loss: 0.5070449113845825:  97%|██████████▋| 142/146 [00:32<00:00,  4.21it/s]evaluate for the 13-th batch, evaluate loss: 0.7180419564247131:  52%|█████████▍        | 11/21 [00:00<00:00, 16.94it/s]evaluate for the 13-th batch, evaluate loss: 0.7180419564247131:  62%|███████████▏      | 13/21 [00:00<00:00, 15.46it/s]Epoch: 1, train for the 122-th batch, train loss: 0.5007823705673218:  80%|████████▊  | 121/151 [00:32<00:07,  3.99it/s]Epoch: 1, train for the 122-th batch, train loss: 0.5007823705673218:  81%|████████▉  | 122/151 [00:32<00:06,  4.42it/s]evaluate for the 14-th batch, evaluate loss: 0.6981451511383057:  62%|███████████▏      | 13/21 [00:00<00:00, 15.46it/s]Epoch: 1, train for the 143-th batch, train loss: 0.5283958911895752:  97%|██████████▋| 142/146 [00:32<00:00,  4.21it/s]Epoch: 1, train for the 143-th batch, train loss: 0.5283958911895752:  98%|██████████▊| 143/146 [00:32<00:00,  4.80it/s]evaluate for the 15-th batch, evaluate loss: 0.7243577241897583:  62%|███████████▏      | 13/21 [00:00<00:00, 15.46it/s]evaluate for the 15-th batch, evaluate loss: 0.7243577241897583:  71%|████████████▊     | 15/21 [00:00<00:00, 15.68it/s]evaluate for the 16-th batch, evaluate loss: 0.69526606798172:  71%|██████████████▎     | 15/21 [00:00<00:00, 15.68it/s]Epoch: 1, train for the 111-th batch, train loss: 0.4722537398338318:  29%|███▏       | 110/383 [00:30<01:02,  4.36it/s]Epoch: 1, train for the 111-th batch, train loss: 0.4722537398338318:  29%|███▏       | 111/383 [00:30<01:09,  3.89it/s]evaluate for the 17-th batch, evaluate loss: 0.6697567701339722:  71%|████████████▊     | 15/21 [00:01<00:00, 15.68it/s]evaluate for the 17-th batch, evaluate loss: 0.6697567701339722:  81%|██████████████▌   | 17/21 [00:01<00:00, 16.20it/s]evaluate for the 18-th batch, evaluate loss: 0.6955434083938599:  81%|██████████████▌   | 17/21 [00:01<00:00, 16.20it/s]Epoch: 1, train for the 144-th batch, train loss: 0.4893139600753784:  98%|██████████▊| 143/146 [00:32<00:00,  4.80it/s]Epoch: 1, train for the 144-th batch, train loss: 0.4893139600753784:  99%|██████████▊| 144/146 [00:32<00:00,  4.90it/s]Epoch: 1, train for the 119-th batch, train loss: 0.5364187359809875:  50%|█████▍     | 118/237 [00:31<00:29,  4.10it/s]Epoch: 1, train for the 133-th batch, train loss: 0.44692283868789673:  55%|█████▍    | 132/241 [00:31<00:19,  5.59it/s]Epoch: 1, train for the 123-th batch, train loss: 0.5660075545310974:  81%|████████▉  | 122/151 [00:32<00:06,  4.42it/s]Epoch: 1, train for the 133-th batch, train loss: 0.44692283868789673:  55%|█████▌    | 133/241 [00:31<00:28,  3.78it/s]Epoch: 1, train for the 119-th batch, train loss: 0.5364187359809875:  50%|█████▌     | 119/237 [00:31<00:36,  3.22it/s]Epoch: 1, train for the 123-th batch, train loss: 0.5660075545310974:  81%|████████▉  | 123/151 [00:32<00:07,  3.88it/s]evaluate for the 19-th batch, evaluate loss: 0.7325589656829834:  81%|██████████████▌   | 17/21 [00:01<00:00, 16.20it/s]evaluate for the 19-th batch, evaluate loss: 0.7325589656829834:  90%|████████████████▎ | 19/21 [00:01<00:00, 16.15it/s]Epoch: 1, train for the 112-th batch, train loss: 0.4246317446231842:  29%|███▏       | 111/383 [00:30<01:09,  3.89it/s]Epoch: 1, train for the 112-th batch, train loss: 0.4246317446231842:  29%|███▏       | 112/383 [00:30<01:03,  4.30it/s]Epoch: 1, train for the 145-th batch, train loss: 0.5050992369651794:  99%|██████████▊| 144/146 [00:32<00:00,  4.90it/s]evaluate for the 20-th batch, evaluate loss: 0.7092244029045105:  90%|████████████████▎ | 19/21 [00:01<00:00, 16.15it/s]Epoch: 1, train for the 145-th batch, train loss: 0.5050992369651794:  99%|██████████▉| 145/146 [00:32<00:00,  5.30it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5360750555992126:  55%|██████     | 133/241 [00:31<00:28,  3.78it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5360750555992126:  56%|██████     | 134/241 [00:31<00:24,  4.36it/s]evaluate for the 21-th batch, evaluate loss: 0.9094957113265991:  90%|████████████████▎ | 19/21 [00:01<00:00, 16.15it/s]evaluate for the 21-th batch, evaluate loss: 0.9094957113265991: 100%|██████████████████| 21/21 [00:01<00:00, 15.50it/s]evaluate for the 21-th batch, evaluate loss: 0.9094957113265991: 100%|██████████████████| 21/21 [00:01<00:00, 16.09it/s]
Epoch: 1, train for the 124-th batch, train loss: 0.503486692905426:  81%|█████████▊  | 123/151 [00:32<00:07,  3.88it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5548347234725952:  50%|█████▌     | 119/237 [00:31<00:36,  3.22it/s]Epoch: 1, train for the 124-th batch, train loss: 0.503486692905426:  82%|█████████▊  | 124/151 [00:32<00:06,  4.21it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5548347234725952:  51%|█████▌     | 120/237 [00:31<00:33,  3.53it/s]INFO:root:Epoch: 1, learning rate: 0.0001, train loss: 0.5396
INFO:root:train average_precision, 0.8224
INFO:root:train roc_auc, 0.8278
INFO:root:validate loss: 0.6108
INFO:root:validate average_precision, 0.7557
INFO:root:validate roc_auc, 0.7419
INFO:root:new node validate loss: 0.7393
INFO:root:new node validate first_1_average_precision, 0.6850
INFO:root:new node validate first_1_roc_auc, 0.6604
INFO:root:new node validate first_3_average_precision, 0.6721
INFO:root:new node validate first_3_roc_auc, 0.6458
INFO:root:new node validate first_10_average_precision, 0.6424
INFO:root:new node validate first_10_roc_auc, 0.6223
INFO:root:new node validate average_precision, 0.6049
INFO:root:new node validate roc_auc, 0.5865
Epoch: 1, train for the 146-th batch, train loss: 0.47363531589508057:  99%|█████████▉| 145/146 [00:33<00:00,  5.30it/s]Epoch: 1, train for the 146-th batch, train loss: 0.47363531589508057: 100%|██████████| 146/146 [00:33<00:00,  5.15it/s]Epoch: 1, train for the 146-th batch, train loss: 0.47363531589508057: 100%|██████████| 146/146 [00:33<00:00,  4.42it/s]
INFO:root:save model ./saved_models/TGN/ia-reality-call/TGN_seed0_dummy-yehxa035/TGN_seed0_dummy-yehxa035.pkl
Epoch: 1, train for the 121-th batch, train loss: 0.5877308249473572:  51%|█████▌     | 120/237 [00:32<00:33,  3.53it/s]Epoch: 1, train for the 121-th batch, train loss: 0.5877308249473572:  51%|█████▌     | 121/237 [00:32<00:27,  4.15it/s]  0%|                                                                                            | 0/38 [00:00<?, ?it/s]Epoch: 1, train for the 113-th batch, train loss: 0.4281674921512604:  29%|███▏       | 112/383 [00:30<01:03,  4.30it/s]Epoch: 1, train for the 135-th batch, train loss: 0.40312379598617554:  56%|█████▌    | 134/241 [00:32<00:24,  4.36it/s]evaluate for the 1-th batch, evaluate loss: 0.5034996867179871:   0%|                            | 0/38 [00:00<?, ?it/s]Epoch: 1, train for the 113-th batch, train loss: 0.4281674921512604:  30%|███▏       | 113/383 [00:30<01:15,  3.56it/s]Epoch: 1, train for the 125-th batch, train loss: 0.550220251083374:  82%|█████████▊  | 124/151 [00:33<00:06,  4.21it/s]Epoch: 1, train for the 135-th batch, train loss: 0.40312379598617554:  56%|█████▌    | 135/241 [00:32<00:26,  3.99it/s]Epoch: 1, train for the 125-th batch, train loss: 0.550220251083374:  83%|█████████▉  | 125/151 [00:33<00:06,  4.07it/s]evaluate for the 2-th batch, evaluate loss: 0.5165563225746155:   0%|                            | 0/38 [00:00<?, ?it/s]Epoch: 1, train for the 122-th batch, train loss: 0.550767183303833:  51%|██████▏     | 121/237 [00:32<00:27,  4.15it/s]evaluate for the 3-th batch, evaluate loss: 0.49356168508529663:   0%|                           | 0/38 [00:00<?, ?it/s]evaluate for the 3-th batch, evaluate loss: 0.49356168508529663:   8%|█▌                 | 3/38 [00:00<00:01, 19.98it/s]Epoch: 1, train for the 122-th batch, train loss: 0.550767183303833:  51%|██████▏     | 122/237 [00:32<00:25,  4.47it/s]evaluate for the 4-th batch, evaluate loss: 0.5055515170097351:   8%|█▌                  | 3/38 [00:00<00:01, 19.98it/s]Epoch: 1, train for the 114-th batch, train loss: 0.3919547200202942:  30%|███▏       | 113/383 [00:30<01:15,  3.56it/s]evaluate for the 5-th batch, evaluate loss: 0.5422025918960571:   8%|█▌                  | 3/38 [00:00<00:01, 19.98it/s]Epoch: 1, train for the 114-th batch, train loss: 0.3919547200202942:  30%|███▎       | 114/383 [00:30<01:10,  3.84it/s]Epoch: 1, train for the 136-th batch, train loss: 0.4489826560020447:  56%|██████▏    | 135/241 [00:32<00:26,  3.99it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5255498886108398:  83%|█████████  | 125/151 [00:33<00:06,  4.07it/s]evaluate for the 6-th batch, evaluate loss: 0.5264851450920105:   8%|█▌                  | 3/38 [00:00<00:01, 19.98it/s]evaluate for the 6-th batch, evaluate loss: 0.5264851450920105:  16%|███▏                | 6/38 [00:00<00:01, 22.57it/s]Epoch: 1, train for the 136-th batch, train loss: 0.4489826560020447:  56%|██████▏    | 136/241 [00:32<00:25,  4.13it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5255498886108398:  83%|█████████▏ | 126/151 [00:33<00:05,  4.25it/s]Epoch: 1, train for the 123-th batch, train loss: 0.5189517736434937:  51%|█████▋     | 122/237 [00:32<00:25,  4.47it/s]evaluate for the 7-th batch, evaluate loss: 0.4648680090904236:  16%|███▏                | 6/38 [00:00<00:01, 22.57it/s]Epoch: 1, train for the 123-th batch, train loss: 0.5189517736434937:  52%|█████▋     | 123/237 [00:32<00:24,  4.63it/s]evaluate for the 8-th batch, evaluate loss: 0.49175140261650085:  16%|███                | 6/38 [00:00<00:01, 22.57it/s]evaluate for the 9-th batch, evaluate loss: 0.5112788081169128:  16%|███▏                | 6/38 [00:00<00:01, 22.57it/s]evaluate for the 9-th batch, evaluate loss: 0.5112788081169128:  24%|████▋               | 9/38 [00:00<00:01, 22.16it/s]evaluate for the 10-th batch, evaluate loss: 0.5387439131736755:  24%|████▌              | 9/38 [00:00<00:01, 22.16it/s]Epoch: 1, train for the 124-th batch, train loss: 0.5103901028633118:  52%|█████▋     | 123/237 [00:32<00:24,  4.63it/s]Epoch: 1, train for the 127-th batch, train loss: 0.49602389335632324:  83%|████████▎ | 126/151 [00:33<00:05,  4.25it/s]Epoch: 1, train for the 137-th batch, train loss: 0.4791136682033539:  56%|██████▏    | 136/241 [00:32<00:25,  4.13it/s]Epoch: 1, train for the 124-th batch, train loss: 0.5103901028633118:  52%|█████▊     | 124/237 [00:32<00:22,  4.97it/s]evaluate for the 11-th batch, evaluate loss: 0.491984099149704:  24%|████▋               | 9/38 [00:00<00:01, 22.16it/s]Epoch: 1, train for the 127-th batch, train loss: 0.49602389335632324:  84%|████████▍ | 127/151 [00:33<00:05,  4.22it/s]Epoch: 1, train for the 137-th batch, train loss: 0.4791136682033539:  57%|██████▎    | 137/241 [00:32<00:25,  4.06it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]evaluate for the 12-th batch, evaluate loss: 0.5552001595497131:  24%|████▌              | 9/38 [00:00<00:01, 22.16it/s]evaluate for the 12-th batch, evaluate loss: 0.5552001595497131:  32%|█████▋            | 12/38 [00:00<00:01, 20.57it/s]Epoch: 1, train for the 115-th batch, train loss: 0.3554070293903351:  30%|███▎       | 114/383 [00:31<01:10,  3.84it/s]Epoch: 1, train for the 115-th batch, train loss: 0.3554070293903351:  30%|███▎       | 115/383 [00:31<01:17,  3.45it/s]evaluate for the 13-th batch, evaluate loss: 0.5162437558174133:  32%|█████▋            | 12/38 [00:00<00:01, 20.57it/s]evaluate for the 14-th batch, evaluate loss: 0.4634144604206085:  32%|█████▋            | 12/38 [00:00<00:01, 20.57it/s]Epoch: 1, train for the 125-th batch, train loss: 0.5004327893257141:  52%|█████▊     | 124/237 [00:32<00:22,  4.97it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5332163572311401:  84%|█████████▎ | 127/151 [00:33<00:05,  4.22it/s]Epoch: 1, train for the 138-th batch, train loss: 0.39337316155433655:  57%|█████▋    | 137/241 [00:32<00:25,  4.06it/s]Epoch: 1, train for the 125-th batch, train loss: 0.5004327893257141:  53%|█████▊     | 125/237 [00:32<00:22,  4.99it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5332163572311401:  85%|█████████▎ | 128/151 [00:33<00:05,  4.36it/s]evaluate for the 15-th batch, evaluate loss: 0.49955496191978455:  32%|█████▎           | 12/38 [00:00<00:01, 20.57it/s]evaluate for the 15-th batch, evaluate loss: 0.49955496191978455:  39%|██████▋          | 15/38 [00:00<00:01, 19.55it/s]Epoch: 1, train for the 138-th batch, train loss: 0.39337316155433655:  57%|█████▋    | 138/241 [00:32<00:24,  4.21it/s]Epoch: 2, train for the 1-th batch, train loss: 0.9548770785331726:   0%|                       | 0/119 [00:00<?, ?it/s]Epoch: 2, train for the 1-th batch, train loss: 0.9548770785331726:   1%|▏              | 1/119 [00:00<00:23,  5.09it/s]Epoch: 2, train for the 2-th batch, train loss: 1.0112791061401367:   1%|▏              | 1/119 [00:00<00:23,  5.09it/s]Epoch: 1, train for the 139-th batch, train loss: 0.35510265827178955:  57%|█████▋    | 138/241 [00:33<00:24,  4.21it/s]Epoch: 1, train for the 139-th batch, train loss: 0.35510265827178955:  58%|█████▊    | 139/241 [00:33<00:23,  4.40it/s]Epoch: 2, train for the 3-th batch, train loss: 0.9263918995857239:   1%|▏              | 1/119 [00:00<00:23,  5.09it/s]Epoch: 2, train for the 3-th batch, train loss: 0.9263918995857239:   3%|▍              | 3/119 [00:00<00:15,  7.71it/s]evaluate for the 16-th batch, evaluate loss: 0.5220211148262024:  39%|███████           | 15/38 [00:00<00:01, 19.55it/s]Epoch: 1, train for the 116-th batch, train loss: 0.35152286291122437:  30%|███       | 115/383 [00:31<01:17,  3.45it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5248304605484009:  85%|█████████▎ | 128/151 [00:34<00:05,  4.36it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5248304605484009:  85%|█████████▍ | 129/151 [00:34<00:05,  4.12it/s]Epoch: 1, train for the 116-th batch, train loss: 0.35152286291122437:  30%|███       | 116/383 [00:31<01:25,  3.12it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5468834042549133:  53%|█████▊     | 125/237 [00:33<00:22,  4.99it/s]evaluate for the 17-th batch, evaluate loss: 0.5053645968437195:  39%|███████           | 15/38 [00:01<00:01, 19.55it/s]evaluate for the 17-th batch, evaluate loss: 0.5053645968437195:  45%|████████          | 17/38 [00:01<00:01, 13.01it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5468834042549133:  53%|█████▊     | 126/237 [00:33<00:26,  4.19it/s]Epoch: 1, train for the 140-th batch, train loss: 0.37465253472328186:  58%|█████▊    | 139/241 [00:33<00:23,  4.40it/s]Epoch: 1, train for the 140-th batch, train loss: 0.37465253472328186:  58%|█████▊    | 140/241 [00:33<00:19,  5.06it/s]Epoch: 2, train for the 4-th batch, train loss: 0.7424671649932861:   3%|▍              | 3/119 [00:00<00:15,  7.71it/s]Epoch: 2, train for the 4-th batch, train loss: 0.7424671649932861:   3%|▌              | 4/119 [00:00<00:14,  7.74it/s]evaluate for the 18-th batch, evaluate loss: 0.5184860825538635:  45%|████████          | 17/38 [00:01<00:01, 13.01it/s]evaluate for the 19-th batch, evaluate loss: 0.48643678426742554:  45%|███████▌         | 17/38 [00:01<00:01, 13.01it/s]evaluate for the 19-th batch, evaluate loss: 0.48643678426742554:  50%|████████▌        | 19/38 [00:01<00:01, 13.29it/s]Epoch: 2, train for the 5-th batch, train loss: 0.5919407606124878:   3%|▌              | 4/119 [00:00<00:14,  7.74it/s]Epoch: 2, train for the 5-th batch, train loss: 0.5919407606124878:   4%|▋              | 5/119 [00:00<00:14,  7.91it/s]Epoch: 1, train for the 141-th batch, train loss: 0.4962709844112396:  58%|██████▍    | 140/241 [00:33<00:19,  5.06it/s]Epoch: 2, train for the 6-th batch, train loss: 0.510254442691803:   4%|▋               | 5/119 [00:00<00:14,  7.91it/s]Epoch: 1, train for the 141-th batch, train loss: 0.4962709844112396:  59%|██████▍    | 141/241 [00:33<00:20,  4.85it/s]Epoch: 2, train for the 7-th batch, train loss: 0.546607255935669:   4%|▋               | 5/119 [00:00<00:14,  7.91it/s]Epoch: 2, train for the 7-th batch, train loss: 0.546607255935669:   6%|▉               | 7/119 [00:00<00:11,  9.36it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5366311073303223:  85%|█████████▍ | 129/151 [00:34<00:05,  4.12it/s]evaluate for the 20-th batch, evaluate loss: 0.4408365786075592:  50%|█████████         | 19/38 [00:01<00:01, 13.29it/s]Epoch: 1, train for the 117-th batch, train loss: 0.3302624523639679:  30%|███▎       | 116/383 [00:32<01:25,  3.12it/s]Epoch: 1, train for the 127-th batch, train loss: 0.533156156539917:  53%|██████▍     | 126/237 [00:33<00:26,  4.19it/s]Epoch: 1, train for the 142-th batch, train loss: 0.45848333835601807:  59%|█████▊    | 141/241 [00:33<00:20,  4.85it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5366311073303223:  86%|█████████▍ | 130/151 [00:34<00:06,  3.31it/s]Epoch: 1, train for the 142-th batch, train loss: 0.45848333835601807:  59%|█████▉    | 142/241 [00:33<00:18,  5.32it/s]Epoch: 1, train for the 117-th batch, train loss: 0.3302624523639679:  31%|███▎       | 117/383 [00:32<01:36,  2.77it/s]Epoch: 1, train for the 127-th batch, train loss: 0.533156156539917:  54%|██████▍     | 127/237 [00:33<00:32,  3.39it/s]evaluate for the 21-th batch, evaluate loss: 0.44806671142578125:  50%|████████▌        | 19/38 [00:01<00:01, 13.29it/s]evaluate for the 21-th batch, evaluate loss: 0.44806671142578125:  55%|█████████▍       | 21/38 [00:01<00:01, 10.12it/s]Epoch: 2, train for the 8-th batch, train loss: 0.4826032221317291:   6%|▉              | 7/119 [00:00<00:11,  9.36it/s]Epoch: 2, train for the 8-th batch, train loss: 0.4826032221317291:   7%|█              | 8/119 [00:00<00:12,  8.55it/s]evaluate for the 22-th batch, evaluate loss: 0.508347749710083:  55%|██████████▌        | 21/38 [00:01<00:01, 10.12it/s]Epoch: 1, train for the 131-th batch, train loss: 0.4580836296081543:  86%|█████████▍ | 130/151 [00:34<00:06,  3.31it/s]Epoch: 1, train for the 131-th batch, train loss: 0.4580836296081543:  87%|█████████▌ | 131/151 [00:34<00:05,  3.78it/s]Epoch: 2, train for the 9-th batch, train loss: 0.4983171820640564:   7%|█              | 8/119 [00:01<00:12,  8.55it/s]Epoch: 2, train for the 9-th batch, train loss: 0.4983171820640564:   8%|█▏             | 9/119 [00:01<00:12,  8.82it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5368092060089111:  54%|█████▉     | 127/237 [00:33<00:32,  3.39it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5368092060089111:  54%|█████▉     | 128/237 [00:33<00:29,  3.73it/s]Epoch: 2, train for the 10-th batch, train loss: 0.5434032082557678:   8%|█             | 9/119 [00:01<00:12,  8.82it/s]Epoch: 1, train for the 143-th batch, train loss: 0.4588488042354584:  59%|██████▍    | 142/241 [00:33<00:18,  5.32it/s]evaluate for the 23-th batch, evaluate loss: 0.5052527785301208:  55%|█████████▉        | 21/38 [00:01<00:01, 10.12it/s]evaluate for the 23-th batch, evaluate loss: 0.5052527785301208:  61%|██████████▉       | 23/38 [00:01<00:01,  9.22it/s]Epoch: 1, train for the 143-th batch, train loss: 0.4588488042354584:  59%|██████▌    | 143/241 [00:33<00:22,  4.37it/s]Epoch: 1, train for the 118-th batch, train loss: 0.32719793915748596:  31%|███       | 117/383 [00:32<01:36,  2.77it/s]Epoch: 1, train for the 118-th batch, train loss: 0.32719793915748596:  31%|███       | 118/383 [00:32<01:36,  2.76it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5572707653045654:  54%|█████▉     | 128/237 [00:33<00:29,  3.73it/s]Epoch: 2, train for the 11-th batch, train loss: 0.49111098051071167:   8%|▉            | 9/119 [00:01<00:12,  8.82it/s]evaluate for the 24-th batch, evaluate loss: 0.46923017501831055:  61%|██████████▎      | 23/38 [00:01<00:01,  9.22it/s]Epoch: 2, train for the 11-th batch, train loss: 0.49111098051071167:   9%|█           | 11/119 [00:01<00:12,  8.80it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5263385772705078:  87%|█████████▌ | 131/151 [00:34<00:05,  3.78it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5572707653045654:  54%|█████▉     | 129/237 [00:33<00:27,  3.99it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5263385772705078:  87%|█████████▌ | 132/151 [00:34<00:05,  3.71it/s]Epoch: 1, train for the 144-th batch, train loss: 0.5888596773147583:  59%|██████▌    | 143/241 [00:33<00:22,  4.37it/s]Epoch: 1, train for the 144-th batch, train loss: 0.5888596773147583:  60%|██████▌    | 144/241 [00:34<00:19,  4.86it/s]evaluate for the 25-th batch, evaluate loss: 0.5199058055877686:  61%|██████████▉       | 23/38 [00:01<00:01,  9.22it/s]evaluate for the 25-th batch, evaluate loss: 0.5199058055877686:  66%|███████████▊      | 25/38 [00:01<00:01,  9.78it/s]Epoch: 2, train for the 12-th batch, train loss: 0.5245625972747803:   9%|█▏           | 11/119 [00:01<00:12,  8.80it/s]Epoch: 2, train for the 12-th batch, train loss: 0.5245625972747803:  10%|█▎           | 12/119 [00:01<00:12,  8.34it/s]evaluate for the 26-th batch, evaluate loss: 0.47888895869255066:  66%|███████████▏     | 25/38 [00:02<00:01,  9.78it/s]Epoch: 1, train for the 119-th batch, train loss: 0.381992369890213:  31%|███▋        | 118/383 [00:32<01:36,  2.76it/s]Epoch: 1, train for the 133-th batch, train loss: 0.5172570943832397:  87%|█████████▌ | 132/151 [00:35<00:05,  3.71it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5404796600341797:  54%|█████▉     | 129/237 [00:34<00:27,  3.99it/s]Epoch: 1, train for the 119-th batch, train loss: 0.381992369890213:  31%|███▋        | 119/383 [00:32<01:27,  3.02it/s]Epoch: 1, train for the 133-th batch, train loss: 0.5172570943832397:  88%|█████████▋ | 133/151 [00:35<00:04,  4.08it/s]Epoch: 2, train for the 13-th batch, train loss: 0.4521298110485077:  10%|█▎           | 12/119 [00:01<00:12,  8.34it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5404796600341797:  55%|██████     | 130/237 [00:34<00:25,  4.13it/s]Epoch: 2, train for the 14-th batch, train loss: 0.4672505557537079:  10%|█▎           | 12/119 [00:01<00:12,  8.34it/s]Epoch: 2, train for the 14-th batch, train loss: 0.4672505557537079:  12%|█▌           | 14/119 [00:01<00:11,  9.02it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5020995140075684:  88%|█████████▋ | 133/151 [00:35<00:04,  4.08it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5020995140075684:  89%|█████████▊ | 134/151 [00:35<00:03,  4.72it/s]Epoch: 1, train for the 120-th batch, train loss: 0.41779136657714844:  31%|███       | 119/383 [00:32<01:27,  3.02it/s]Epoch: 2, train for the 15-th batch, train loss: 0.4745097756385803:  12%|█▌           | 14/119 [00:01<00:11,  9.02it/s]evaluate for the 27-th batch, evaluate loss: 0.5028800368309021:  66%|███████████▊      | 25/38 [00:02<00:01,  9.78it/s]evaluate for the 27-th batch, evaluate loss: 0.5028800368309021:  71%|████████████▊     | 27/38 [00:02<00:01,  7.94it/s]Epoch: 1, train for the 120-th batch, train loss: 0.41779136657714844:  31%|███▏      | 120/383 [00:32<01:18,  3.35it/s]Epoch: 1, train for the 131-th batch, train loss: 0.51349937915802:  55%|███████▏     | 130/237 [00:34<00:25,  4.13it/s]Epoch: 1, train for the 131-th batch, train loss: 0.51349937915802:  55%|███████▏     | 131/237 [00:34<00:25,  4.15it/s]Epoch: 1, train for the 145-th batch, train loss: 0.6293120980262756:  60%|██████▌    | 144/241 [00:34<00:19,  4.86it/s]evaluate for the 28-th batch, evaluate loss: 0.5024950504302979:  71%|████████████▊     | 27/38 [00:02<00:01,  7.94it/s]Epoch: 1, train for the 145-th batch, train loss: 0.6293120980262756:  60%|██████▌    | 145/241 [00:34<00:26,  3.64it/s]Epoch: 1, train for the 135-th batch, train loss: 0.49167028069496155:  89%|████████▊ | 134/151 [00:35<00:03,  4.72it/s]Epoch: 1, train for the 135-th batch, train loss: 0.49167028069496155:  89%|████████▉ | 135/151 [00:35<00:03,  4.89it/s]evaluate for the 29-th batch, evaluate loss: 0.48382309079170227:  71%|████████████     | 27/38 [00:02<00:01,  7.94it/s]evaluate for the 29-th batch, evaluate loss: 0.48382309079170227:  76%|████████████▉    | 29/38 [00:02<00:00,  9.26it/s]Epoch: 2, train for the 16-th batch, train loss: 0.5198491811752319:  12%|█▌           | 14/119 [00:01<00:11,  9.02it/s]Epoch: 2, train for the 16-th batch, train loss: 0.5198491811752319:  13%|█▋           | 16/119 [00:01<00:11,  8.76it/s]evaluate for the 30-th batch, evaluate loss: 0.5059493184089661:  76%|█████████████▋    | 29/38 [00:02<00:00,  9.26it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5610001087188721:  60%|██████▌    | 145/241 [00:34<00:26,  3.64it/s]Epoch: 1, train for the 132-th batch, train loss: 0.563211977481842:  55%|██████▋     | 131/237 [00:34<00:25,  4.15it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5610001087188721:  61%|██████▋    | 146/241 [00:34<00:22,  4.16it/s]Epoch: 1, train for the 132-th batch, train loss: 0.563211977481842:  56%|██████▋     | 132/237 [00:34<00:23,  4.40it/s]Epoch: 1, train for the 121-th batch, train loss: 0.43217992782592773:  31%|███▏      | 120/383 [00:33<01:18,  3.35it/s]Epoch: 2, train for the 17-th batch, train loss: 0.43761682510375977:  13%|█▌          | 16/119 [00:01<00:11,  8.76it/s]Epoch: 2, train for the 17-th batch, train loss: 0.43761682510375977:  14%|█▋          | 17/119 [00:01<00:11,  8.85it/s]Epoch: 1, train for the 121-th batch, train loss: 0.43217992782592773:  32%|███▏      | 121/383 [00:33<01:15,  3.46it/s]Epoch: 2, train for the 18-th batch, train loss: 0.5145494937896729:  14%|█▊           | 17/119 [00:02<00:11,  8.85it/s]Epoch: 2, train for the 18-th batch, train loss: 0.5145494937896729:  15%|█▉           | 18/119 [00:02<00:11,  8.98it/s]Epoch: 2, train for the 19-th batch, train loss: 0.5500885248184204:  15%|█▉           | 18/119 [00:02<00:11,  8.98it/s]evaluate for the 31-th batch, evaluate loss: 0.5028449892997742:  76%|█████████████▋    | 29/38 [00:02<00:00,  9.26it/s]evaluate for the 31-th batch, evaluate loss: 0.5028449892997742:  82%|██████████████▋   | 31/38 [00:02<00:00,  7.92it/s]Epoch: 1, train for the 133-th batch, train loss: 0.5132474899291992:  56%|██████▏    | 132/237 [00:34<00:23,  4.40it/s]Epoch: 1, train for the 147-th batch, train loss: 0.557532787322998:  61%|███████▎    | 146/241 [00:34<00:22,  4.16it/s]Epoch: 1, train for the 122-th batch, train loss: 0.4998129904270172:  32%|███▍       | 121/383 [00:33<01:15,  3.46it/s]Epoch: 1, train for the 133-th batch, train loss: 0.5132474899291992:  56%|██████▏    | 133/237 [00:34<00:25,  4.14it/s]Epoch: 1, train for the 147-th batch, train loss: 0.557532787322998:  61%|███████▎    | 147/241 [00:34<00:24,  3.87it/s]evaluate for the 32-th batch, evaluate loss: 0.4867396950721741:  82%|██████████████▋   | 31/38 [00:02<00:00,  7.92it/s]Epoch: 1, train for the 122-th batch, train loss: 0.4998129904270172:  32%|███▌       | 122/383 [00:33<01:12,  3.58it/s]Epoch: 1, train for the 136-th batch, train loss: 0.5487919449806213:  89%|█████████▊ | 135/151 [00:35<00:03,  4.89it/s]Epoch: 2, train for the 20-th batch, train loss: 0.44783759117126465:  15%|█▊          | 18/119 [00:02<00:11,  8.98it/s]Epoch: 1, train for the 136-th batch, train loss: 0.5487919449806213:  90%|█████████▉ | 136/151 [00:35<00:04,  3.52it/s]Epoch: 2, train for the 20-th batch, train loss: 0.44783759117126465:  17%|██          | 20/119 [00:02<00:11,  8.93it/s]evaluate for the 33-th batch, evaluate loss: 0.4842696189880371:  82%|██████████████▋   | 31/38 [00:02<00:00,  7.92it/s]evaluate for the 33-th batch, evaluate loss: 0.4842696189880371:  87%|███████████████▋  | 33/38 [00:02<00:00,  9.38it/s]Epoch: 1, train for the 148-th batch, train loss: 0.5100642442703247:  61%|██████▋    | 147/241 [00:35<00:24,  3.87it/s]evaluate for the 34-th batch, evaluate loss: 0.49846792221069336:  87%|██████████████▊  | 33/38 [00:02<00:00,  9.38it/s]Epoch: 1, train for the 148-th batch, train loss: 0.5100642442703247:  61%|██████▊    | 148/241 [00:35<00:21,  4.41it/s]Epoch: 2, train for the 21-th batch, train loss: 0.5033110976219177:  17%|██▏          | 20/119 [00:02<00:11,  8.93it/s]Epoch: 2, train for the 21-th batch, train loss: 0.5033110976219177:  18%|██▎          | 21/119 [00:02<00:11,  8.57it/s]Epoch: 1, train for the 123-th batch, train loss: 0.41002753376960754:  32%|███▏      | 122/383 [00:33<01:12,  3.58it/s]Epoch: 1, train for the 137-th batch, train loss: 0.5392698049545288:  90%|█████████▉ | 136/151 [00:36<00:04,  3.52it/s]Epoch: 1, train for the 123-th batch, train loss: 0.41002753376960754:  32%|███▏      | 123/383 [00:33<01:08,  3.79it/s]Epoch: 1, train for the 137-th batch, train loss: 0.5392698049545288:  91%|█████████▉ | 137/151 [00:36<00:03,  3.92it/s]Epoch: 2, train for the 22-th batch, train loss: 0.5245932936668396:  18%|██▎          | 21/119 [00:02<00:11,  8.57it/s]Epoch: 1, train for the 149-th batch, train loss: 0.3807913362979889:  61%|██████▊    | 148/241 [00:35<00:21,  4.41it/s]Epoch: 1, train for the 149-th batch, train loss: 0.3807913362979889:  62%|██████▊    | 149/241 [00:35<00:18,  4.93it/s]evaluate for the 35-th batch, evaluate loss: 0.5315691232681274:  87%|███████████████▋  | 33/38 [00:03<00:00,  9.38it/s]evaluate for the 35-th batch, evaluate loss: 0.5315691232681274:  92%|████████████████▌ | 35/38 [00:03<00:00,  8.80it/s]Epoch: 2, train for the 23-th batch, train loss: 0.5494613647460938:  18%|██▎          | 21/119 [00:02<00:11,  8.57it/s]evaluate for the 36-th batch, evaluate loss: 0.5328254699707031:  92%|████████████████▌ | 35/38 [00:03<00:00,  8.80it/s]Epoch: 2, train for the 23-th batch, train loss: 0.5494613647460938:  19%|██▌          | 23/119 [00:02<00:11,  8.34it/s]Epoch: 1, train for the 138-th batch, train loss: 0.5292426943778992:  91%|█████████▉ | 137/151 [00:36<00:03,  3.92it/s]Epoch: 1, train for the 150-th batch, train loss: 0.38003119826316833:  62%|██████▏   | 149/241 [00:35<00:18,  4.93it/s]evaluate for the 37-th batch, evaluate loss: 0.4603431522846222:  92%|████████████████▌ | 35/38 [00:03<00:00,  8.80it/s]evaluate for the 37-th batch, evaluate loss: 0.4603431522846222:  97%|█████████████████▌| 37/38 [00:03<00:00,  9.63it/s]Epoch: 1, train for the 124-th batch, train loss: 0.46126577258110046:  32%|███▏      | 123/383 [00:33<01:08,  3.79it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5686830282211304:  56%|██████▏    | 133/237 [00:35<00:25,  4.14it/s]Epoch: 1, train for the 150-th batch, train loss: 0.38003119826316833:  62%|██████▏   | 150/241 [00:35<00:18,  4.88it/s]Epoch: 1, train for the 138-th batch, train loss: 0.5292426943778992:  91%|██████████ | 138/151 [00:36<00:03,  3.87it/s]Epoch: 1, train for the 124-th batch, train loss: 0.46126577258110046:  32%|███▏      | 124/383 [00:33<01:10,  3.67it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5686830282211304:  57%|██████▏    | 134/237 [00:35<00:34,  3.01it/s]evaluate for the 38-th batch, evaluate loss: 0.4990321099758148:  97%|█████████████████▌| 37/38 [00:03<00:00,  9.63it/s]evaluate for the 38-th batch, evaluate loss: 0.4990321099758148: 100%|██████████████████| 38/38 [00:03<00:00, 11.16it/s]
Epoch: 2, train for the 24-th batch, train loss: 0.4792023301124573:  19%|██▌          | 23/119 [00:02<00:11,  8.34it/s]Epoch: 2, train for the 24-th batch, train loss: 0.4792023301124573:  20%|██▌          | 24/119 [00:02<00:11,  7.96it/s]Epoch: 1, train for the 151-th batch, train loss: 0.38788923621177673:  62%|██████▏   | 150/241 [00:35<00:18,  4.88it/s]Epoch: 1, train for the 151-th batch, train loss: 0.38788923621177673:  63%|██████▎   | 151/241 [00:35<00:17,  5.27it/s]Epoch: 2, train for the 25-th batch, train loss: 0.4623056650161743:  20%|██▌          | 24/119 [00:02<00:11,  7.96it/s]Epoch: 1, train for the 125-th batch, train loss: 0.4230802059173584:  32%|███▌       | 124/383 [00:34<01:10,  3.67it/s]Epoch: 2, train for the 25-th batch, train loss: 0.4623056650161743:  21%|██▋          | 25/119 [00:02<00:11,  8.24it/s]Epoch: 1, train for the 125-th batch, train loss: 0.4230802059173584:  33%|███▌       | 125/383 [00:34<01:04,  3.98it/s]Epoch: 2, train for the 26-th batch, train loss: 0.4669523239135742:  21%|██▋          | 25/119 [00:03<00:11,  8.24it/s]Epoch: 2, train for the 26-th batch, train loss: 0.4669523239135742:  22%|██▊          | 26/119 [00:03<00:11,  8.34it/s]Epoch: 1, train for the 135-th batch, train loss: 0.5992160439491272:  57%|██████▏    | 134/237 [00:35<00:34,  3.01it/s]  0%|                                                                                            | 0/20 [00:00<?, ?it/s]Epoch: 1, train for the 135-th batch, train loss: 0.5992160439491272:  57%|██████▎    | 135/237 [00:35<00:33,  3.01it/s]Epoch: 1, train for the 126-th batch, train loss: 0.3664429783821106:  33%|███▌       | 125/383 [00:34<01:04,  3.98it/s]evaluate for the 1-th batch, evaluate loss: 0.7204946875572205:   0%|                            | 0/20 [00:00<?, ?it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5138490796089172:  91%|██████████ | 138/151 [00:36<00:03,  3.87it/s]Epoch: 1, train for the 152-th batch, train loss: 0.31361234188079834:  63%|██████▎   | 151/241 [00:35<00:17,  5.27it/s]Epoch: 2, train for the 27-th batch, train loss: 0.5676084756851196:  22%|██▊          | 26/119 [00:03<00:11,  8.34it/s]Epoch: 1, train for the 126-th batch, train loss: 0.3664429783821106:  33%|███▌       | 126/383 [00:34<01:01,  4.18it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5138490796089172:  92%|██████████▏| 139/151 [00:36<00:03,  3.21it/s]Epoch: 1, train for the 152-th batch, train loss: 0.31361234188079834:  63%|██████▎   | 152/241 [00:35<00:19,  4.58it/s]Epoch: 2, train for the 27-th batch, train loss: 0.5676084756851196:  23%|██▉          | 27/119 [00:03<00:11,  8.05it/s]evaluate for the 2-th batch, evaluate loss: 0.7608678340911865:   0%|                            | 0/20 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.7608678340911865:  10%|██                  | 2/20 [00:00<00:01, 15.32it/s]Epoch: 1, train for the 136-th batch, train loss: 0.599542498588562:  57%|██████▊     | 135/237 [00:35<00:33,  3.01it/s]evaluate for the 3-th batch, evaluate loss: 0.6634619235992432:  10%|██                  | 2/20 [00:00<00:01, 15.32it/s]Epoch: 1, train for the 136-th batch, train loss: 0.599542498588562:  57%|██████▉     | 136/237 [00:35<00:29,  3.48it/s]Epoch: 2, train for the 28-th batch, train loss: 0.5518131852149963:  23%|██▉          | 27/119 [00:03<00:11,  8.05it/s]Epoch: 2, train for the 28-th batch, train loss: 0.5518131852149963:  24%|███          | 28/119 [00:03<00:11,  7.96it/s]Epoch: 1, train for the 153-th batch, train loss: 0.36292633414268494:  63%|██████▎   | 152/241 [00:35<00:19,  4.58it/s]evaluate for the 4-th batch, evaluate loss: 0.6826355457305908:  10%|██                  | 2/20 [00:00<00:01, 15.32it/s]evaluate for the 4-th batch, evaluate loss: 0.6826355457305908:  20%|████                | 4/20 [00:00<00:01, 15.24it/s]Epoch: 1, train for the 153-th batch, train loss: 0.36292633414268494:  63%|██████▎   | 153/241 [00:36<00:17,  5.01it/s]Epoch: 1, train for the 127-th batch, train loss: 0.3635709285736084:  33%|███▌       | 126/383 [00:34<01:01,  4.18it/s]Epoch: 1, train for the 127-th batch, train loss: 0.3635709285736084:  33%|███▋       | 127/383 [00:34<01:00,  4.20it/s]Epoch: 1, train for the 137-th batch, train loss: 0.5652818083763123:  57%|██████▎    | 136/237 [00:36<00:29,  3.48it/s]Epoch: 1, train for the 137-th batch, train loss: 0.5652818083763123:  58%|██████▎    | 137/237 [00:36<00:24,  4.00it/s]Epoch: 2, train for the 29-th batch, train loss: 0.5034507513046265:  24%|███          | 28/119 [00:03<00:11,  7.96it/s]Epoch: 2, train for the 29-th batch, train loss: 0.5034507513046265:  24%|███▏         | 29/119 [00:03<00:11,  7.68it/s]evaluate for the 5-th batch, evaluate loss: 0.6897907853126526:  20%|████                | 4/20 [00:00<00:01, 15.24it/s]Epoch: 1, train for the 140-th batch, train loss: 0.47064945101737976:  92%|█████████▏| 139/151 [00:37<00:03,  3.21it/s]Epoch: 1, train for the 140-th batch, train loss: 0.47064945101737976:  93%|█████████▎| 140/151 [00:37<00:03,  3.10it/s]evaluate for the 6-th batch, evaluate loss: 0.7120781540870667:  20%|████                | 4/20 [00:00<00:01, 15.24it/s]evaluate for the 6-th batch, evaluate loss: 0.7120781540870667:  30%|██████              | 6/20 [00:00<00:01, 12.27it/s]Epoch: 1, train for the 154-th batch, train loss: 0.2757873237133026:  63%|██████▉    | 153/241 [00:36<00:17,  5.01it/s]Epoch: 2, train for the 30-th batch, train loss: 0.528130054473877:  24%|███▍          | 29/119 [00:03<00:11,  7.68it/s]Epoch: 2, train for the 30-th batch, train loss: 0.528130054473877:  25%|███▌          | 30/119 [00:03<00:12,  7.23it/s]evaluate for the 7-th batch, evaluate loss: 0.7532599568367004:  30%|██████              | 6/20 [00:00<00:01, 12.27it/s]Epoch: 1, train for the 154-th batch, train loss: 0.2757873237133026:  64%|███████    | 154/241 [00:36<00:19,  4.46it/s]Epoch: 1, train for the 138-th batch, train loss: 0.6011638641357422:  58%|██████▎    | 137/237 [00:36<00:24,  4.00it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5227372646331787:  33%|███▋       | 127/383 [00:34<01:00,  4.20it/s]Epoch: 1, train for the 141-th batch, train loss: 0.49475064873695374:  93%|█████████▎| 140/151 [00:37<00:03,  3.10it/s]Epoch: 1, train for the 141-th batch, train loss: 0.49475064873695374:  93%|█████████▎| 141/151 [00:37<00:02,  3.68it/s]evaluate for the 8-th batch, evaluate loss: 0.692454993724823:  30%|██████▎              | 6/20 [00:00<00:01, 12.27it/s]evaluate for the 8-th batch, evaluate loss: 0.692454993724823:  40%|████████▍            | 8/20 [00:00<00:00, 12.91it/s]Epoch: 1, train for the 138-th batch, train loss: 0.6011638641357422:  58%|██████▍    | 138/237 [00:36<00:24,  4.03it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5227372646331787:  33%|███▋       | 128/383 [00:34<01:04,  3.97it/s]Epoch: 2, train for the 31-th batch, train loss: 0.5093749761581421:  25%|███▎         | 30/119 [00:03<00:12,  7.23it/s]Epoch: 2, train for the 31-th batch, train loss: 0.5093749761581421:  26%|███▍         | 31/119 [00:03<00:11,  7.38it/s]evaluate for the 9-th batch, evaluate loss: 0.6365994215011597:  40%|████████            | 8/20 [00:00<00:00, 12.91it/s]Epoch: 1, train for the 155-th batch, train loss: 0.29781538248062134:  64%|██████▍   | 154/241 [00:36<00:19,  4.46it/s]Epoch: 1, train for the 155-th batch, train loss: 0.29781538248062134:  64%|██████▍   | 155/241 [00:36<00:17,  4.92it/s]evaluate for the 10-th batch, evaluate loss: 0.656836986541748:  40%|████████            | 8/20 [00:00<00:00, 12.91it/s]evaluate for the 10-th batch, evaluate loss: 0.656836986541748:  50%|█████████▌         | 10/20 [00:00<00:00, 13.24it/s]Epoch: 1, train for the 142-th batch, train loss: 0.4949652850627899:  93%|██████████▎| 141/151 [00:37<00:02,  3.68it/s]Epoch: 1, train for the 129-th batch, train loss: 0.383878231048584:  33%|████        | 128/383 [00:35<01:04,  3.97it/s]Epoch: 1, train for the 142-th batch, train loss: 0.4949652850627899:  94%|██████████▎| 142/151 [00:37<00:02,  4.04it/s]Epoch: 1, train for the 129-th batch, train loss: 0.383878231048584:  34%|████        | 129/383 [00:35<01:00,  4.20it/s]Epoch: 2, train for the 32-th batch, train loss: 0.43710288405418396:  26%|███▏        | 31/119 [00:03<00:11,  7.38it/s]Epoch: 2, train for the 32-th batch, train loss: 0.43710288405418396:  27%|███▏        | 32/119 [00:03<00:12,  6.77it/s]evaluate for the 11-th batch, evaluate loss: 0.6142393350601196:  50%|█████████         | 10/20 [00:00<00:00, 13.24it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5813487768173218:  58%|██████▍    | 138/237 [00:36<00:24,  4.03it/s]evaluate for the 12-th batch, evaluate loss: 0.6732004880905151:  50%|█████████         | 10/20 [00:00<00:00, 13.24it/s]evaluate for the 12-th batch, evaluate loss: 0.6732004880905151:  60%|██████████▊       | 12/20 [00:00<00:00, 13.03it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5813487768173218:  59%|██████▍    | 139/237 [00:36<00:26,  3.68it/s]evaluate for the 13-th batch, evaluate loss: 0.6961327791213989:  60%|██████████▊       | 12/20 [00:00<00:00, 13.03it/s]Epoch: 2, train for the 33-th batch, train loss: 0.5292007327079773:  27%|███▍         | 32/119 [00:04<00:12,  6.77it/s]Epoch: 2, train for the 33-th batch, train loss: 0.5292007327079773:  28%|███▌         | 33/119 [00:04<00:13,  6.61it/s]Epoch: 1, train for the 130-th batch, train loss: 0.3864068388938904:  34%|███▋       | 129/383 [00:35<01:00,  4.20it/s]Epoch: 1, train for the 156-th batch, train loss: 0.21976043283939362:  64%|██████▍   | 155/241 [00:36<00:17,  4.92it/s]Epoch: 1, train for the 143-th batch, train loss: 0.4429550766944885:  94%|██████████▎| 142/151 [00:37<00:02,  4.04it/s]Epoch: 1, train for the 143-th batch, train loss: 0.4429550766944885:  95%|██████████▍| 143/151 [00:37<00:01,  4.01it/s]evaluate for the 14-th batch, evaluate loss: 0.707684338092804:  60%|███████████▍       | 12/20 [00:01<00:00, 13.03it/s]evaluate for the 14-th batch, evaluate loss: 0.707684338092804:  70%|█████████████▎     | 14/20 [00:01<00:00, 13.29it/s]Epoch: 1, train for the 156-th batch, train loss: 0.21976043283939362:  65%|██████▍   | 156/241 [00:36<00:21,  3.98it/s]Epoch: 1, train for the 130-th batch, train loss: 0.3864068388938904:  34%|███▋       | 130/383 [00:35<01:00,  4.21it/s]evaluate for the 15-th batch, evaluate loss: 0.688606858253479:  70%|█████████████▎     | 14/20 [00:01<00:00, 13.29it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5014763474464417:  28%|███▌         | 33/119 [00:04<00:13,  6.61it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5014763474464417:  29%|███▋         | 34/119 [00:04<00:12,  6.76it/s]evaluate for the 16-th batch, evaluate loss: 0.6563220620155334:  70%|████████████▌     | 14/20 [00:01<00:00, 13.29it/s]evaluate for the 16-th batch, evaluate loss: 0.6563220620155334:  80%|██████████████▍   | 16/20 [00:01<00:00, 13.83it/s]Epoch: 1, train for the 157-th batch, train loss: 0.19115354120731354:  65%|██████▍   | 156/241 [00:36<00:21,  3.98it/s]Epoch: 1, train for the 144-th batch, train loss: 0.4637793302536011:  95%|██████████▍| 143/151 [00:38<00:01,  4.01it/s]evaluate for the 17-th batch, evaluate loss: 0.6917728781700134:  80%|██████████████▍   | 16/20 [00:01<00:00, 13.83it/s]Epoch: 1, train for the 157-th batch, train loss: 0.19115354120731354:  65%|██████▌   | 157/241 [00:37<00:20,  4.20it/s]Epoch: 1, train for the 140-th batch, train loss: 0.5538479685783386:  59%|██████▍    | 139/237 [00:37<00:26,  3.68it/s]Epoch: 2, train for the 35-th batch, train loss: 0.4734361171722412:  29%|███▋         | 34/119 [00:04<00:12,  6.76it/s]Epoch: 2, train for the 35-th batch, train loss: 0.4734361171722412:  29%|███▊         | 35/119 [00:04<00:12,  6.80it/s]Epoch: 1, train for the 144-th batch, train loss: 0.4637793302536011:  95%|██████████▍| 144/151 [00:38<00:01,  4.05it/s]Epoch: 1, train for the 131-th batch, train loss: 0.4103919267654419:  34%|███▋       | 130/383 [00:35<01:00,  4.21it/s]Epoch: 1, train for the 140-th batch, train loss: 0.5538479685783386:  59%|██████▍    | 140/237 [00:37<00:29,  3.29it/s]evaluate for the 18-th batch, evaluate loss: 0.6811991930007935:  80%|██████████████▍   | 16/20 [00:01<00:00, 13.83it/s]evaluate for the 18-th batch, evaluate loss: 0.6811991930007935:  90%|████████████████▏ | 18/20 [00:01<00:00, 13.98it/s]Epoch: 1, train for the 131-th batch, train loss: 0.4103919267654419:  34%|███▊       | 131/383 [00:35<01:01,  4.07it/s]evaluate for the 19-th batch, evaluate loss: 0.7252517342567444:  90%|████████████████▏ | 18/20 [00:01<00:00, 13.98it/s]Epoch: 2, train for the 36-th batch, train loss: 0.5214536786079407:  29%|███▊         | 35/119 [00:04<00:12,  6.80it/s]Epoch: 2, train for the 36-th batch, train loss: 0.5214536786079407:  30%|███▉         | 36/119 [00:04<00:12,  6.79it/s]Epoch: 1, train for the 158-th batch, train loss: 0.20901130139827728:  65%|██████▌   | 157/241 [00:37<00:20,  4.20it/s]evaluate for the 20-th batch, evaluate loss: 0.7017879486083984:  90%|████████████████▏ | 18/20 [00:01<00:00, 13.98it/s]evaluate for the 20-th batch, evaluate loss: 0.7017879486083984: 100%|██████████████████| 20/20 [00:01<00:00, 13.85it/s]evaluate for the 20-th batch, evaluate loss: 0.7017879486083984: 100%|██████████████████| 20/20 [00:01<00:00, 13.59it/s]
Epoch: 1, train for the 145-th batch, train loss: 0.4968518614768982:  95%|██████████▍| 144/151 [00:38<00:01,  4.05it/s]Epoch: 1, train for the 141-th batch, train loss: 0.5946485996246338:  59%|██████▍    | 140/237 [00:37<00:29,  3.29it/s]Epoch: 1, train for the 158-th batch, train loss: 0.20901130139827728:  66%|██████▌   | 158/241 [00:37<00:19,  4.29it/s]Epoch: 1, train for the 145-th batch, train loss: 0.4968518614768982:  96%|██████████▌| 145/151 [00:38<00:01,  4.20it/s]Epoch: 1, train for the 141-th batch, train loss: 0.5946485996246338:  59%|██████▌    | 141/237 [00:37<00:26,  3.67it/s]Epoch: 2, train for the 37-th batch, train loss: 0.5048972964286804:  30%|███▉         | 36/119 [00:04<00:12,  6.79it/s]Epoch: 2, train for the 37-th batch, train loss: 0.5048972964286804:  31%|████         | 37/119 [00:04<00:11,  7.41it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5075656771659851:  96%|██████████▌| 145/151 [00:38<00:01,  4.20it/s]Epoch: 1, train for the 132-th batch, train loss: 0.4523219168186188:  34%|███▊       | 131/383 [00:35<01:01,  4.07it/s]INFO:root:Epoch: 1, learning rate: 0.0001, train loss: 0.6099
INFO:root:train average_precision, 0.7527
INFO:root:train roc_auc, 0.7485
INFO:root:validate loss: 0.5004
Epoch: 1, train for the 146-th batch, train loss: 0.5075656771659851:  97%|██████████▋| 146/151 [00:38<00:01,  4.78it/s]INFO:root:validate average_precision, 0.8423
INFO:root:validate roc_auc, 0.8405
INFO:root:new node validate loss: 0.6902
INFO:root:new node validate first_1_average_precision, 0.5810
INFO:root:new node validate first_1_roc_auc, 0.5085
INFO:root:new node validate first_3_average_precision, 0.5955
INFO:root:new node validate first_3_roc_auc, 0.5565
INFO:root:new node validate first_10_average_precision, 0.6061
INFO:root:new node validate first_10_roc_auc, 0.6000
INFO:root:new node validate average_precision, 0.6525
INFO:root:new node validate roc_auc, 0.6558
INFO:root:save model ./saved_models/TGN/ia-escorts-dynamic/TGN_seed0_dummy-4ge5z0wz/TGN_seed0_dummy-4ge5z0wz.pkl
Epoch: 1, train for the 132-th batch, train loss: 0.4523219168186188:  34%|███▊       | 132/383 [00:35<01:08,  3.64it/s]Epoch: 2, train for the 38-th batch, train loss: 0.4999670386314392:  31%|████         | 37/119 [00:04<00:11,  7.41it/s]Epoch: 2, train for the 38-th batch, train loss: 0.4999670386314392:  32%|████▏        | 38/119 [00:04<00:11,  7.35it/s]Epoch: 1, train for the 159-th batch, train loss: 0.12286126613616943:  66%|██████▌   | 158/241 [00:37<00:19,  4.29it/s]Epoch: 1, train for the 159-th batch, train loss: 0.12286126613616943:  66%|██████▌   | 159/241 [00:37<00:19,  4.15it/s]Epoch: 2, train for the 39-th batch, train loss: 0.5366382002830505:  32%|████▏        | 38/119 [00:04<00:11,  7.35it/s]Epoch: 2, train for the 39-th batch, train loss: 0.5366382002830505:  33%|████▎        | 39/119 [00:04<00:10,  7.93it/s]Epoch: 1, train for the 147-th batch, train loss: 0.49810275435447693:  97%|█████████▋| 146/151 [00:38<00:01,  4.78it/s]Epoch: 1, train for the 133-th batch, train loss: 0.48992034792900085:  34%|███▍      | 132/383 [00:36<01:08,  3.64it/s]Epoch: 1, train for the 147-th batch, train loss: 0.49810275435447693:  97%|█████████▋| 147/151 [00:38<00:00,  5.03it/s]Epoch: 1, train for the 133-th batch, train loss: 0.48992034792900085:  35%|███▍      | 133/383 [00:36<01:01,  4.09it/s]Epoch: 2, train for the 40-th batch, train loss: 0.5047042369842529:  33%|████▎        | 39/119 [00:04<00:10,  7.93it/s]Epoch: 1, train for the 160-th batch, train loss: 0.6486974954605103:  66%|███████▎   | 159/241 [00:37<00:19,  4.15it/s]Epoch: 1, train for the 160-th batch, train loss: 0.6486974954605103:  66%|███████▎   | 160/241 [00:37<00:18,  4.46it/s]Epoch: 2, train for the 41-th batch, train loss: 0.5051063299179077:  33%|████▎        | 39/119 [00:05<00:10,  7.93it/s]Epoch: 2, train for the 41-th batch, train loss: 0.5051063299179077:  34%|████▍        | 41/119 [00:05<00:08,  8.98it/s]Epoch: 1, train for the 142-th batch, train loss: 0.597907304763794:  59%|███████▏    | 141/237 [00:37<00:26,  3.67it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5213264226913452:  35%|███▊       | 133/383 [00:36<01:01,  4.09it/s]Epoch: 1, train for the 142-th batch, train loss: 0.597907304763794:  60%|███████▏    | 142/237 [00:37<00:32,  2.88it/s]Epoch: 1, train for the 161-th batch, train loss: 0.3511649966239929:  66%|███████▎   | 160/241 [00:37<00:18,  4.46it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5213264226913452:  35%|███▊       | 134/383 [00:36<00:58,  4.28it/s]Epoch: 1, train for the 161-th batch, train loss: 0.3511649966239929:  67%|███████▎   | 161/241 [00:37<00:15,  5.18it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5310019850730896:  34%|████▍        | 41/119 [00:05<00:08,  8.98it/s]Epoch: 1, train for the 148-th batch, train loss: 0.47104963660240173:  97%|█████████▋| 147/151 [00:38<00:00,  5.03it/s]Epoch: 1, train for the 148-th batch, train loss: 0.47104963660240173:  98%|█████████▊| 148/151 [00:38<00:00,  4.45it/s]Epoch: 2, train for the 43-th batch, train loss: 0.5341609716415405:  34%|████▍        | 41/119 [00:05<00:08,  8.98it/s]Epoch: 2, train for the 43-th batch, train loss: 0.5341609716415405:  36%|████▋        | 43/119 [00:05<00:07,  9.66it/s]Epoch: 1, train for the 162-th batch, train loss: 0.1364031732082367:  67%|███████▎   | 161/241 [00:37<00:15,  5.18it/s]Epoch: 1, train for the 135-th batch, train loss: 0.4611488878726959:  35%|███▊       | 134/383 [00:36<00:58,  4.28it/s]Epoch: 1, train for the 162-th batch, train loss: 0.1364031732082367:  67%|███████▍   | 162/241 [00:37<00:14,  5.48it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5075398683547974:  36%|████▋        | 43/119 [00:05<00:07,  9.66it/s]Epoch: 1, train for the 135-th batch, train loss: 0.4611488878726959:  35%|███▉       | 135/383 [00:36<00:55,  4.49it/s]Epoch: 1, train for the 143-th batch, train loss: 0.5411774516105652:  60%|██████▌    | 142/237 [00:38<00:32,  2.88it/s]Epoch: 1, train for the 149-th batch, train loss: 0.4409342110157013:  98%|██████████▊| 148/151 [00:39<00:00,  4.45it/s]Epoch: 2, train for the 45-th batch, train loss: 0.5293031930923462:  36%|████▋        | 43/119 [00:05<00:07,  9.66it/s]Epoch: 1, train for the 149-th batch, train loss: 0.4409342110157013:  99%|██████████▊| 149/151 [00:39<00:00,  4.48it/s]Epoch: 2, train for the 45-th batch, train loss: 0.5293031930923462:  38%|████▉        | 45/119 [00:05<00:07, 10.24it/s]Epoch: 1, train for the 143-th batch, train loss: 0.5411774516105652:  60%|██████▋    | 143/237 [00:38<00:31,  2.95it/s]Epoch: 1, train for the 163-th batch, train loss: 0.14016205072402954:  67%|██████▋   | 162/241 [00:38<00:14,  5.48it/s]Epoch: 1, train for the 163-th batch, train loss: 0.14016205072402954:  68%|██████▊   | 163/241 [00:38<00:14,  5.49it/s]Epoch: 1, train for the 136-th batch, train loss: 0.39225876331329346:  35%|███▌      | 135/383 [00:36<00:55,  4.49it/s]Epoch: 2, train for the 46-th batch, train loss: 0.5140731930732727:  38%|████▉        | 45/119 [00:05<00:07, 10.24it/s]Epoch: 1, train for the 136-th batch, train loss: 0.39225876331329346:  36%|███▌      | 136/383 [00:36<00:53,  4.63it/s]Epoch: 1, train for the 150-th batch, train loss: 0.5312080383300781:  99%|██████████▊| 149/151 [00:39<00:00,  4.48it/s]Epoch: 1, train for the 150-th batch, train loss: 0.5312080383300781:  99%|██████████▉| 150/151 [00:39<00:00,  4.90it/s]Epoch: 1, train for the 164-th batch, train loss: 0.11291497945785522:  68%|██████▊   | 163/241 [00:38<00:14,  5.49it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5603384971618652:  38%|████▉        | 45/119 [00:05<00:07, 10.24it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]Epoch: 1, train for the 164-th batch, train loss: 0.11291497945785522:  68%|██████▊   | 164/241 [00:38<00:12,  6.15it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5603384971618652:  39%|█████▏       | 47/119 [00:05<00:06, 10.46it/s]Epoch: 1, train for the 144-th batch, train loss: 0.5597597360610962:  60%|██████▋    | 143/237 [00:38<00:31,  2.95it/s]Epoch: 1, train for the 144-th batch, train loss: 0.5597597360610962:  61%|██████▋    | 144/237 [00:38<00:28,  3.25it/s]Epoch: 2, train for the 1-th batch, train loss: 0.9383929967880249:   0%|                       | 0/146 [00:00<?, ?it/s]Epoch: 2, train for the 1-th batch, train loss: 0.9383929967880249:   1%|               | 1/146 [00:00<00:17,  8.09it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5263885259628296:  99%|██████████▉| 150/151 [00:39<00:00,  4.90it/s]Epoch: 2, train for the 48-th batch, train loss: 0.549763560295105:  39%|█████▌        | 47/119 [00:05<00:06, 10.46it/s]Epoch: 1, train for the 137-th batch, train loss: 0.47383788228034973:  36%|███▌      | 136/383 [00:36<00:53,  4.63it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5263885259628296: 100%|███████████| 151/151 [00:39<00:00,  5.01it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5263885259628296: 100%|███████████| 151/151 [00:39<00:00,  3.83it/s]
Epoch: 1, train for the 137-th batch, train loss: 0.47383788228034973:  36%|███▌      | 137/383 [00:36<00:56,  4.34it/s]Epoch: 2, train for the 2-th batch, train loss: 0.8632838726043701:   1%|               | 1/146 [00:00<00:17,  8.09it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5143571496009827:  39%|█████▏       | 47/119 [00:05<00:06, 10.46it/s]  0%|                                                                                            | 0/46 [00:00<?, ?it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5143571496009827:  41%|█████▎       | 49/119 [00:05<00:07,  8.93it/s]Epoch: 2, train for the 3-th batch, train loss: 0.7930164337158203:   1%|               | 1/146 [00:00<00:17,  8.09it/s]Epoch: 2, train for the 3-th batch, train loss: 0.7930164337158203:   2%|▎              | 3/146 [00:00<00:15,  9.35it/s]Epoch: 1, train for the 165-th batch, train loss: 0.2000318467617035:  68%|███████▍   | 164/241 [00:38<00:12,  6.15it/s]evaluate for the 1-th batch, evaluate loss: 0.5525270104408264:   0%|                            | 0/46 [00:00<?, ?it/s]Epoch: 1, train for the 165-th batch, train loss: 0.2000318467617035:  68%|███████▌   | 165/241 [00:38<00:16,  4.54it/s]Epoch: 1, train for the 145-th batch, train loss: 0.5678267478942871:  61%|██████▋    | 144/237 [00:38<00:28,  3.25it/s]Epoch: 1, train for the 138-th batch, train loss: 0.2824225127696991:  36%|███▉       | 137/383 [00:37<00:56,  4.34it/s]Epoch: 1, train for the 145-th batch, train loss: 0.5678267478942871:  61%|██████▋    | 145/237 [00:38<00:29,  3.14it/s]evaluate for the 2-th batch, evaluate loss: 0.5419986844062805:   0%|                            | 0/46 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5419986844062805:   4%|▊                   | 2/46 [00:00<00:03, 14.65it/s]Epoch: 2, train for the 50-th batch, train loss: 0.4920198321342468:  41%|█████▎       | 49/119 [00:06<00:07,  8.93it/s]Epoch: 2, train for the 50-th batch, train loss: 0.4920198321342468:  42%|█████▍       | 50/119 [00:06<00:08,  8.31it/s]Epoch: 2, train for the 4-th batch, train loss: 0.7280970811843872:   2%|▎              | 3/146 [00:00<00:15,  9.35it/s]Epoch: 1, train for the 138-th batch, train loss: 0.2824225127696991:  36%|███▉       | 138/383 [00:37<00:58,  4.22it/s]Epoch: 2, train for the 4-th batch, train loss: 0.7280970811843872:   3%|▍              | 4/146 [00:00<00:16,  8.59it/s]Epoch: 1, train for the 166-th batch, train loss: 0.2419765740633011:  68%|███████▌   | 165/241 [00:38<00:16,  4.54it/s]evaluate for the 3-th batch, evaluate loss: 0.5298249125480652:   4%|▊                   | 2/46 [00:00<00:03, 14.65it/s]Epoch: 1, train for the 166-th batch, train loss: 0.2419765740633011:  69%|███████▌   | 166/241 [00:38<00:15,  4.84it/s]Epoch: 2, train for the 5-th batch, train loss: 0.6911277770996094:   3%|▍              | 4/146 [00:00<00:16,  8.59it/s]Epoch: 2, train for the 5-th batch, train loss: 0.6911277770996094:   3%|▌              | 5/146 [00:00<00:16,  8.30it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5731651782989502:  61%|██████▋    | 145/237 [00:38<00:29,  3.14it/s]evaluate for the 4-th batch, evaluate loss: 0.529204249382019:   4%|▉                    | 2/46 [00:00<00:03, 14.65it/s]evaluate for the 4-th batch, evaluate loss: 0.529204249382019:   9%|█▊                   | 4/46 [00:00<00:03, 12.57it/s]Epoch: 2, train for the 51-th batch, train loss: 0.5690444707870483:  42%|█████▍       | 50/119 [00:06<00:08,  8.31it/s]Epoch: 2, train for the 51-th batch, train loss: 0.5690444707870483:  43%|█████▌       | 51/119 [00:06<00:08,  7.63it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5215911269187927:  36%|███▉       | 138/383 [00:37<00:58,  4.22it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5731651782989502:  62%|██████▊    | 146/237 [00:38<00:26,  3.47it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5215911269187927:  36%|███▉       | 139/383 [00:37<00:56,  4.32it/s]Epoch: 2, train for the 6-th batch, train loss: 0.673663318157196:   3%|▌               | 5/146 [00:00<00:16,  8.30it/s]Epoch: 2, train for the 6-th batch, train loss: 0.673663318157196:   4%|▋               | 6/146 [00:00<00:16,  8.51it/s]Epoch: 2, train for the 52-th batch, train loss: 0.55396568775177:  43%|██████▍        | 51/119 [00:06<00:08,  7.63it/s]Epoch: 2, train for the 52-th batch, train loss: 0.55396568775177:  44%|██████▌        | 52/119 [00:06<00:08,  7.69it/s]Epoch: 2, train for the 7-th batch, train loss: 0.6536770462989807:   4%|▌              | 6/146 [00:00<00:16,  8.51it/s]Epoch: 2, train for the 7-th batch, train loss: 0.6536770462989807:   5%|▋              | 7/146 [00:00<00:16,  8.65it/s]Epoch: 1, train for the 167-th batch, train loss: 0.11371707916259766:  69%|██████▉   | 166/241 [00:39<00:15,  4.84it/s]evaluate for the 5-th batch, evaluate loss: 0.5466919541358948:   9%|█▋                  | 4/46 [00:00<00:03, 12.57it/s]Epoch: 1, train for the 167-th batch, train loss: 0.11371707916259766:  69%|██████▉   | 167/241 [00:39<00:17,  4.20it/s]Epoch: 1, train for the 140-th batch, train loss: 0.43425196409225464:  36%|███▋      | 139/383 [00:37<00:56,  4.32it/s]Epoch: 2, train for the 53-th batch, train loss: 0.4906877875328064:  44%|█████▋       | 52/119 [00:06<00:08,  7.69it/s]Epoch: 2, train for the 53-th batch, train loss: 0.4906877875328064:  45%|█████▊       | 53/119 [00:06<00:08,  7.74it/s]Epoch: 1, train for the 147-th batch, train loss: 0.5833940505981445:  62%|██████▊    | 146/237 [00:39<00:26,  3.47it/s]evaluate for the 6-th batch, evaluate loss: 0.5347265005111694:   9%|█▋                  | 4/46 [00:00<00:03, 12.57it/s]evaluate for the 6-th batch, evaluate loss: 0.5347265005111694:  13%|██▌                 | 6/46 [00:00<00:04,  9.33it/s]Epoch: 1, train for the 140-th batch, train loss: 0.43425196409225464:  37%|███▋      | 140/383 [00:37<00:54,  4.42it/s]Epoch: 2, train for the 8-th batch, train loss: 0.6671852469444275:   5%|▋              | 7/146 [00:00<00:16,  8.65it/s]Epoch: 1, train for the 147-th batch, train loss: 0.5833940505981445:  62%|██████▊    | 147/237 [00:39<00:25,  3.56it/s]Epoch: 1, train for the 168-th batch, train loss: 0.1149299368262291:  69%|███████▌   | 167/241 [00:39<00:17,  4.20it/s]evaluate for the 7-th batch, evaluate loss: 0.5631925463676453:  13%|██▌                 | 6/46 [00:00<00:04,  9.33it/s]Epoch: 1, train for the 168-th batch, train loss: 0.1149299368262291:  70%|███████▋   | 168/241 [00:39<00:16,  4.47it/s]Epoch: 2, train for the 54-th batch, train loss: 0.5342762470245361:  45%|█████▊       | 53/119 [00:06<00:08,  7.74it/s]Epoch: 2, train for the 54-th batch, train loss: 0.5342762470245361:  45%|█████▉       | 54/119 [00:06<00:09,  7.02it/s]Epoch: 2, train for the 9-th batch, train loss: 0.685126006603241:   5%|▊               | 7/146 [00:01<00:16,  8.65it/s]Epoch: 2, train for the 9-th batch, train loss: 0.685126006603241:   6%|▉               | 9/146 [00:01<00:16,  8.35it/s]Epoch: 1, train for the 141-th batch, train loss: 0.43568092584609985:  37%|███▋      | 140/383 [00:37<00:54,  4.42it/s]Epoch: 1, train for the 141-th batch, train loss: 0.43568092584609985:  37%|███▋      | 141/383 [00:37<00:55,  4.36it/s]evaluate for the 8-th batch, evaluate loss: 0.5503429770469666:  13%|██▌                 | 6/46 [00:00<00:04,  9.33it/s]evaluate for the 8-th batch, evaluate loss: 0.5503429770469666:  17%|███▍                | 8/46 [00:00<00:04,  8.14it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5007867813110352:  45%|█████▉       | 54/119 [00:06<00:09,  7.02it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5007867813110352:  46%|██████       | 55/119 [00:06<00:08,  7.22it/s]Epoch: 2, train for the 10-th batch, train loss: 0.6910932660102844:   6%|▊             | 9/146 [00:01<00:16,  8.35it/s]Epoch: 2, train for the 10-th batch, train loss: 0.6910932660102844:   7%|▉            | 10/146 [00:01<00:17,  7.73it/s]evaluate for the 9-th batch, evaluate loss: 0.5704501867294312:  17%|███▍                | 8/46 [00:00<00:04,  8.14it/s]Epoch: 1, train for the 169-th batch, train loss: 0.6220358610153198:  70%|███████▋   | 168/241 [00:39<00:16,  4.47it/s]Epoch: 1, train for the 169-th batch, train loss: 0.6220358610153198:  70%|███████▋   | 169/241 [00:39<00:16,  4.33it/s]evaluate for the 10-th batch, evaluate loss: 0.5143699049949646:  17%|███▎               | 8/46 [00:00<00:04,  8.14it/s]evaluate for the 10-th batch, evaluate loss: 0.5143699049949646:  22%|███▉              | 10/46 [00:00<00:03, 10.20it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5339435338973999:  46%|██████       | 55/119 [00:06<00:08,  7.22it/s]Epoch: 1, train for the 148-th batch, train loss: 0.5371211171150208:  62%|██████▊    | 147/237 [00:39<00:25,  3.56it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5339435338973999:  47%|██████       | 56/119 [00:06<00:08,  7.17it/s]Epoch: 1, train for the 142-th batch, train loss: 0.46589764952659607:  37%|███▋      | 141/383 [00:38<00:55,  4.36it/s]evaluate for the 11-th batch, evaluate loss: 0.5638437271118164:  22%|███▉              | 10/46 [00:01<00:03, 10.20it/s]Epoch: 2, train for the 11-th batch, train loss: 0.7068465352058411:   7%|▉            | 10/146 [00:01<00:17,  7.73it/s]Epoch: 2, train for the 11-th batch, train loss: 0.7068465352058411:   8%|▉            | 11/146 [00:01<00:17,  7.63it/s]Epoch: 1, train for the 148-th batch, train loss: 0.5371211171150208:  62%|██████▊    | 148/237 [00:39<00:29,  2.99it/s]Epoch: 1, train for the 142-th batch, train loss: 0.46589764952659607:  37%|███▋      | 142/383 [00:38<00:56,  4.25it/s]Epoch: 1, train for the 170-th batch, train loss: 0.7401524186134338:  70%|███████▋   | 169/241 [00:39<00:16,  4.33it/s]evaluate for the 12-th batch, evaluate loss: 0.5157150030136108:  22%|███▉              | 10/46 [00:01<00:03, 10.20it/s]evaluate for the 12-th batch, evaluate loss: 0.5157150030136108:  26%|████▋             | 12/46 [00:01<00:03, 11.19it/s]Epoch: 1, train for the 170-th batch, train loss: 0.7401524186134338:  71%|███████▊   | 170/241 [00:39<00:14,  4.84it/s]Epoch: 2, train for the 57-th batch, train loss: 0.47230789065361023:  47%|█████▋      | 56/119 [00:07<00:08,  7.17it/s]Epoch: 2, train for the 57-th batch, train loss: 0.47230789065361023:  48%|█████▋      | 57/119 [00:07<00:08,  7.42it/s]Epoch: 2, train for the 12-th batch, train loss: 0.6906097531318665:   8%|▉            | 11/146 [00:01<00:17,  7.63it/s]Epoch: 2, train for the 12-th batch, train loss: 0.6906097531318665:   8%|█            | 12/146 [00:01<00:17,  7.80it/s]evaluate for the 13-th batch, evaluate loss: 0.5263801217079163:  26%|████▋             | 12/46 [00:01<00:03, 11.19it/s]Epoch: 1, train for the 171-th batch, train loss: 0.607357382774353:  71%|████████▍   | 170/241 [00:39<00:14,  4.84it/s]Epoch: 1, train for the 171-th batch, train loss: 0.607357382774353:  71%|████████▌   | 171/241 [00:39<00:13,  5.34it/s]Epoch: 2, train for the 58-th batch, train loss: 0.5148397088050842:  48%|██████▏      | 57/119 [00:07<00:08,  7.42it/s]Epoch: 1, train for the 143-th batch, train loss: 0.3802817463874817:  37%|████       | 142/383 [00:38<00:56,  4.25it/s]Epoch: 2, train for the 58-th batch, train loss: 0.5148397088050842:  49%|██████▎      | 58/119 [00:07<00:08,  7.29it/s]Epoch: 2, train for the 13-th batch, train loss: 0.6855831146240234:   8%|█            | 12/146 [00:01<00:17,  7.80it/s]Epoch: 1, train for the 143-th batch, train loss: 0.3802817463874817:  37%|████       | 143/383 [00:38<00:57,  4.19it/s]Epoch: 2, train for the 13-th batch, train loss: 0.6855831146240234:   9%|█▏           | 13/146 [00:01<00:17,  7.43it/s]Epoch: 2, train for the 59-th batch, train loss: 0.5453755855560303:  49%|██████▎      | 58/119 [00:07<00:08,  7.29it/s]Epoch: 2, train for the 59-th batch, train loss: 0.5453755855560303:  50%|██████▍      | 59/119 [00:07<00:08,  7.39it/s]Epoch: 2, train for the 14-th batch, train loss: 0.6755132675170898:   9%|█▏           | 13/146 [00:01<00:17,  7.43it/s]evaluate for the 14-th batch, evaluate loss: 0.5680684447288513:  26%|████▋             | 12/46 [00:01<00:03, 11.19it/s]evaluate for the 14-th batch, evaluate loss: 0.5680684447288513:  30%|█████▍            | 14/46 [00:01<00:03,  8.80it/s]Epoch: 2, train for the 14-th batch, train loss: 0.6755132675170898:  10%|█▏           | 14/146 [00:01<00:17,  7.41it/s]Epoch: 1, train for the 172-th batch, train loss: 0.5359270572662354:  71%|███████▊   | 171/241 [00:40<00:13,  5.34it/s]Epoch: 1, train for the 172-th batch, train loss: 0.5359270572662354:  71%|███████▊   | 172/241 [00:40<00:13,  4.98it/s]Epoch: 1, train for the 144-th batch, train loss: 0.33795738220214844:  37%|███▋      | 143/383 [00:38<00:57,  4.19it/s]Epoch: 1, train for the 149-th batch, train loss: 0.5424304008483887:  62%|██████▊    | 148/237 [00:40<00:29,  2.99it/s]evaluate for the 15-th batch, evaluate loss: 0.5515518188476562:  30%|█████▍            | 14/46 [00:01<00:03,  8.80it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5203247666358948:  50%|██████▍      | 59/119 [00:07<00:08,  7.39it/s]Epoch: 1, train for the 144-th batch, train loss: 0.33795738220214844:  38%|███▊      | 144/383 [00:38<00:56,  4.21it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5203247666358948:  50%|██████▌      | 60/119 [00:07<00:08,  7.29it/s]Epoch: 1, train for the 149-th batch, train loss: 0.5424304008483887:  63%|██████▉    | 149/237 [00:40<00:33,  2.60it/s]Epoch: 2, train for the 15-th batch, train loss: 0.6568822860717773:  10%|█▏           | 14/146 [00:01<00:17,  7.41it/s]Epoch: 2, train for the 15-th batch, train loss: 0.6568822860717773:  10%|█▎           | 15/146 [00:01<00:17,  7.37it/s]evaluate for the 16-th batch, evaluate loss: 0.5150614976882935:  30%|█████▍            | 14/46 [00:01<00:03,  8.80it/s]evaluate for the 16-th batch, evaluate loss: 0.5150614976882935:  35%|██████▎           | 16/46 [00:01<00:03,  9.57it/s]evaluate for the 17-th batch, evaluate loss: 0.4572535455226898:  35%|██████▎           | 16/46 [00:01<00:03,  9.57it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5247575044631958:  50%|██████▌      | 60/119 [00:07<00:08,  7.29it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5247575044631958:  51%|██████▋      | 61/119 [00:07<00:08,  6.49it/s]Epoch: 2, train for the 16-th batch, train loss: 0.6276257634162903:  10%|█▎           | 15/146 [00:02<00:17,  7.37it/s]Epoch: 2, train for the 16-th batch, train loss: 0.6276257634162903:  11%|█▍           | 16/146 [00:02<00:18,  6.91it/s]Epoch: 1, train for the 150-th batch, train loss: 0.5289872884750366:  63%|██████▉    | 149/237 [00:40<00:33,  2.60it/s]Epoch: 1, train for the 173-th batch, train loss: 0.33227404952049255:  71%|███████▏  | 172/241 [00:40<00:13,  4.98it/s]Epoch: 1, train for the 145-th batch, train loss: 0.4610745906829834:  38%|████▏      | 144/383 [00:38<00:56,  4.21it/s]Epoch: 1, train for the 150-th batch, train loss: 0.5289872884750366:  63%|██████▉    | 150/237 [00:40<00:29,  2.95it/s]Epoch: 1, train for the 173-th batch, train loss: 0.33227404952049255:  72%|███████▏  | 173/241 [00:40<00:16,  4.25it/s]Epoch: 1, train for the 145-th batch, train loss: 0.4610745906829834:  38%|████▏      | 145/383 [00:38<01:00,  3.90it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5735296010971069:  51%|██████▋      | 61/119 [00:07<00:08,  6.49it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5735296010971069:  52%|██████▊      | 62/119 [00:07<00:08,  6.51it/s]Epoch: 2, train for the 17-th batch, train loss: 0.6244209408760071:  11%|█▍           | 16/146 [00:02<00:18,  6.91it/s]Epoch: 2, train for the 17-th batch, train loss: 0.6244209408760071:  12%|█▌           | 17/146 [00:02<00:19,  6.75it/s]evaluate for the 18-th batch, evaluate loss: 0.5077501535415649:  35%|██████▎           | 16/46 [00:01<00:03,  9.57it/s]evaluate for the 18-th batch, evaluate loss: 0.5077501535415649:  39%|███████           | 18/46 [00:01<00:03,  8.18it/s]Epoch: 1, train for the 174-th batch, train loss: 0.42134714126586914:  72%|███████▏  | 173/241 [00:40<00:16,  4.25it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5489057302474976:  63%|██████▉    | 150/237 [00:40<00:29,  2.95it/s]Epoch: 1, train for the 174-th batch, train loss: 0.42134714126586914:  72%|███████▏  | 174/241 [00:40<00:14,  4.73it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5489057302474976:  64%|███████    | 151/237 [00:40<00:25,  3.38it/s]Epoch: 2, train for the 18-th batch, train loss: 0.6057220101356506:  12%|█▌           | 17/146 [00:02<00:19,  6.75it/s]evaluate for the 19-th batch, evaluate loss: 0.5778334736824036:  39%|███████           | 18/46 [00:02<00:03,  8.18it/s]Epoch: 1, train for the 146-th batch, train loss: 0.3628544807434082:  38%|████▏      | 145/383 [00:39<01:00,  3.90it/s]Epoch: 1, train for the 146-th batch, train loss: 0.3628544807434082:  38%|████▏      | 146/383 [00:39<00:58,  4.02it/s]Epoch: 2, train for the 19-th batch, train loss: 0.6052159070968628:  12%|█▌           | 17/146 [00:02<00:19,  6.75it/s]Epoch: 2, train for the 19-th batch, train loss: 0.6052159070968628:  13%|█▋           | 19/146 [00:02<00:16,  7.78it/s]Epoch: 2, train for the 63-th batch, train loss: 0.5167733430862427:  52%|██████▊      | 62/119 [00:08<00:08,  6.51it/s]Epoch: 1, train for the 152-th batch, train loss: 0.521547257900238:  64%|███████▋    | 151/237 [00:40<00:25,  3.38it/s]Epoch: 2, train for the 63-th batch, train loss: 0.5167733430862427:  53%|██████▉      | 63/119 [00:08<00:10,  5.36it/s]Epoch: 1, train for the 175-th batch, train loss: 0.2736906409263611:  72%|███████▉   | 174/241 [00:40<00:14,  4.73it/s]evaluate for the 20-th batch, evaluate loss: 0.5220330953598022:  39%|███████           | 18/46 [00:02<00:03,  8.18it/s]evaluate for the 20-th batch, evaluate loss: 0.5220330953598022:  43%|███████▊          | 20/46 [00:02<00:03,  8.08it/s]Epoch: 1, train for the 152-th batch, train loss: 0.521547257900238:  64%|███████▋    | 152/237 [00:40<00:22,  3.71it/s]Epoch: 1, train for the 175-th batch, train loss: 0.2736906409263611:  73%|███████▉   | 175/241 [00:40<00:14,  4.59it/s]Epoch: 2, train for the 20-th batch, train loss: 0.5941686034202576:  13%|█▋           | 19/146 [00:02<00:16,  7.78it/s]Epoch: 2, train for the 20-th batch, train loss: 0.5941686034202576:  14%|█▊           | 20/146 [00:02<00:15,  7.97it/s]Epoch: 1, train for the 147-th batch, train loss: 0.3207530379295349:  38%|████▏      | 146/383 [00:39<00:58,  4.02it/s]evaluate for the 21-th batch, evaluate loss: 0.5569994449615479:  43%|███████▊          | 20/46 [00:02<00:03,  8.08it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5192066431045532:  53%|██████▉      | 63/119 [00:08<00:10,  5.36it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5192066431045532:  54%|██████▉      | 64/119 [00:08<00:09,  5.81it/s]Epoch: 1, train for the 147-th batch, train loss: 0.3207530379295349:  38%|████▏      | 147/383 [00:39<00:57,  4.11it/s]Epoch: 1, train for the 176-th batch, train loss: 0.3708098530769348:  73%|███████▉   | 175/241 [00:40<00:14,  4.59it/s]Epoch: 1, train for the 176-th batch, train loss: 0.3708098530769348:  73%|████████   | 176/241 [00:40<00:12,  5.13it/s]Epoch: 2, train for the 21-th batch, train loss: 0.59737628698349:  14%|██             | 20/146 [00:02<00:15,  7.97it/s]Epoch: 2, train for the 21-th batch, train loss: 0.59737628698349:  14%|██▏            | 21/146 [00:02<00:15,  7.87it/s]Epoch: 1, train for the 153-th batch, train loss: 0.5341013073921204:  64%|███████    | 152/237 [00:40<00:22,  3.71it/s]evaluate for the 22-th batch, evaluate loss: 0.5276358127593994:  43%|███████▊          | 20/46 [00:02<00:03,  8.08it/s]evaluate for the 22-th batch, evaluate loss: 0.5276358127593994:  48%|████████▌         | 22/46 [00:02<00:02,  8.81it/s]Epoch: 1, train for the 153-th batch, train loss: 0.5341013073921204:  65%|███████    | 153/237 [00:40<00:20,  4.01it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5261978507041931:  54%|██████▉      | 64/119 [00:08<00:09,  5.81it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5261978507041931:  55%|███████      | 65/119 [00:08<00:08,  6.21it/s]evaluate for the 23-th batch, evaluate loss: 0.4751131236553192:  48%|████████▌         | 22/46 [00:02<00:02,  8.81it/s]evaluate for the 23-th batch, evaluate loss: 0.4751131236553192:  50%|█████████         | 23/46 [00:02<00:02,  8.85it/s]Epoch: 2, train for the 22-th batch, train loss: 0.6010497212409973:  14%|█▊           | 21/146 [00:02<00:15,  7.87it/s]Epoch: 2, train for the 22-th batch, train loss: 0.6010497212409973:  15%|█▉           | 22/146 [00:02<00:15,  7.90it/s]Epoch: 1, train for the 177-th batch, train loss: 0.28567054867744446:  73%|███████▎  | 176/241 [00:41<00:12,  5.13it/s]Epoch: 1, train for the 148-th batch, train loss: 0.3172281086444855:  38%|████▏      | 147/383 [00:39<00:57,  4.11it/s]Epoch: 1, train for the 177-th batch, train loss: 0.28567054867744446:  73%|███████▎  | 177/241 [00:41<00:12,  5.18it/s]Epoch: 2, train for the 66-th batch, train loss: 0.5492238402366638:  55%|███████      | 65/119 [00:08<00:08,  6.21it/s]Epoch: 1, train for the 148-th batch, train loss: 0.3172281086444855:  39%|████▎      | 148/383 [00:39<00:57,  4.11it/s]Epoch: 2, train for the 66-th batch, train loss: 0.5492238402366638:  55%|███████▏     | 66/119 [00:08<00:07,  6.79it/s]Epoch: 2, train for the 23-th batch, train loss: 0.6038644313812256:  15%|█▉           | 22/146 [00:02<00:15,  7.90it/s]Epoch: 2, train for the 23-th batch, train loss: 0.6038644313812256:  16%|██           | 23/146 [00:02<00:14,  8.38it/s]Epoch: 1, train for the 178-th batch, train loss: 0.4558003842830658:  73%|████████   | 177/241 [00:41<00:12,  5.18it/s]Epoch: 1, train for the 178-th batch, train loss: 0.4558003842830658:  74%|████████   | 178/241 [00:41<00:10,  5.91it/s]Epoch: 2, train for the 67-th batch, train loss: 0.5768430233001709:  55%|███████▏     | 66/119 [00:08<00:07,  6.79it/s]Epoch: 2, train for the 67-th batch, train loss: 0.5768430233001709:  56%|███████▎     | 67/119 [00:08<00:07,  6.85it/s]Epoch: 2, train for the 24-th batch, train loss: 0.6006824970245361:  16%|██           | 23/146 [00:03<00:14,  8.38it/s]Epoch: 2, train for the 24-th batch, train loss: 0.6006824970245361:  16%|██▏          | 24/146 [00:03<00:15,  7.98it/s]Epoch: 1, train for the 149-th batch, train loss: 0.398099809885025:  39%|████▋       | 148/383 [00:39<00:57,  4.11it/s]Epoch: 1, train for the 154-th batch, train loss: 0.5561736822128296:  65%|███████    | 153/237 [00:41<00:20,  4.01it/s]Epoch: 1, train for the 149-th batch, train loss: 0.398099809885025:  39%|████▋       | 149/383 [00:39<00:55,  4.20it/s]Epoch: 1, train for the 154-th batch, train loss: 0.5561736822128296:  65%|███████▏   | 154/237 [00:41<00:24,  3.42it/s]Epoch: 2, train for the 68-th batch, train loss: 0.49620845913887024:  56%|██████▊     | 67/119 [00:08<00:07,  6.85it/s]Epoch: 2, train for the 68-th batch, train loss: 0.49620845913887024:  57%|██████▊     | 68/119 [00:08<00:07,  6.90it/s]Epoch: 1, train for the 179-th batch, train loss: 0.4105081856250763:  74%|████████   | 178/241 [00:41<00:10,  5.91it/s]evaluate for the 24-th batch, evaluate loss: 0.5040609836578369:  50%|█████████         | 23/46 [00:02<00:02,  8.85it/s]evaluate for the 24-th batch, evaluate loss: 0.5040609836578369:  52%|█████████▍        | 24/46 [00:02<00:03,  6.02it/s]Epoch: 2, train for the 25-th batch, train loss: 0.5853460431098938:  16%|██▏          | 24/146 [00:03<00:15,  7.98it/s]Epoch: 1, train for the 179-th batch, train loss: 0.4105081856250763:  74%|████████▏  | 179/241 [00:41<00:11,  5.35it/s]Epoch: 2, train for the 25-th batch, train loss: 0.5853460431098938:  17%|██▏          | 25/146 [00:03<00:15,  7.75it/s]evaluate for the 25-th batch, evaluate loss: 0.5053480267524719:  52%|█████████▍        | 24/46 [00:02<00:03,  6.02it/s]Epoch: 1, train for the 150-th batch, train loss: 0.5757564902305603:  39%|████▎      | 149/383 [00:40<00:55,  4.20it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5822079181671143:  57%|███████▍     | 68/119 [00:08<00:07,  6.90it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5822079181671143:  58%|███████▌     | 69/119 [00:08<00:07,  6.93it/s]Epoch: 1, train for the 150-th batch, train loss: 0.5757564902305603:  39%|████▎      | 150/383 [00:40<00:54,  4.29it/s]Epoch: 2, train for the 26-th batch, train loss: 0.5494393110275269:  17%|██▏          | 25/146 [00:03<00:15,  7.75it/s]Epoch: 2, train for the 26-th batch, train loss: 0.5494393110275269:  18%|██▎          | 26/146 [00:03<00:16,  7.47it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5114386081695557:  58%|███████▌     | 69/119 [00:09<00:07,  6.93it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5114386081695557:  59%|███████▋     | 70/119 [00:09<00:07,  6.85it/s]Epoch: 2, train for the 27-th batch, train loss: 0.5791457891464233:  18%|██▎          | 26/146 [00:03<00:16,  7.47it/s]Epoch: 1, train for the 180-th batch, train loss: 0.5552153587341309:  74%|████████▏  | 179/241 [00:41<00:11,  5.35it/s]Epoch: 2, train for the 27-th batch, train loss: 0.5791457891464233:  18%|██▍          | 27/146 [00:03<00:16,  7.27it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5412315726280212:  39%|████▎      | 150/383 [00:40<00:54,  4.29it/s]Epoch: 1, train for the 155-th batch, train loss: 0.5671120882034302:  65%|███████▏   | 154/237 [00:41<00:24,  3.42it/s]Epoch: 1, train for the 180-th batch, train loss: 0.5552153587341309:  75%|████████▏  | 180/241 [00:41<00:13,  4.45it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5412315726280212:  39%|████▎      | 151/383 [00:40<00:52,  4.44it/s]Epoch: 1, train for the 155-th batch, train loss: 0.5671120882034302:  65%|███████▏   | 155/237 [00:41<00:27,  3.01it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5172886848449707:  59%|███████▋     | 70/119 [00:09<00:07,  6.85it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5172886848449707:  60%|███████▊     | 71/119 [00:09<00:06,  7.21it/s]Epoch: 2, train for the 28-th batch, train loss: 0.5614284873008728:  18%|██▍          | 27/146 [00:03<00:16,  7.27it/s]Epoch: 2, train for the 28-th batch, train loss: 0.5614284873008728:  19%|██▍          | 28/146 [00:03<00:15,  7.60it/s]Epoch: 1, train for the 181-th batch, train loss: 0.7217243909835815:  75%|████████▏  | 180/241 [00:41<00:13,  4.45it/s]evaluate for the 26-th batch, evaluate loss: 0.5494024753570557:  52%|█████████▍        | 24/46 [00:03<00:03,  6.02it/s]evaluate for the 26-th batch, evaluate loss: 0.5494024753570557:  57%|██████████▏       | 26/46 [00:03<00:03,  5.18it/s]Epoch: 1, train for the 181-th batch, train loss: 0.7217243909835815:  75%|████████▎  | 181/241 [00:41<00:12,  4.84it/s]evaluate for the 27-th batch, evaluate loss: 0.522483229637146:  57%|██████████▋        | 26/46 [00:03<00:03,  5.18it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5447099208831787:  60%|███████▊     | 71/119 [00:09<00:06,  7.21it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5447099208831787:  61%|███████▊     | 72/119 [00:09<00:06,  6.83it/s]Epoch: 2, train for the 29-th batch, train loss: 0.5326244831085205:  19%|██▍          | 28/146 [00:03<00:15,  7.60it/s]Epoch: 2, train for the 29-th batch, train loss: 0.5326244831085205:  20%|██▌          | 29/146 [00:03<00:16,  7.13it/s]Epoch: 1, train for the 156-th batch, train loss: 0.5536715984344482:  65%|███████▏   | 155/237 [00:42<00:27,  3.01it/s]Epoch: 1, train for the 152-th batch, train loss: 0.4309472143650055:  39%|████▎      | 151/383 [00:40<00:52,  4.44it/s]Epoch: 1, train for the 182-th batch, train loss: 0.6561182737350464:  75%|████████▎  | 181/241 [00:42<00:12,  4.84it/s]Epoch: 1, train for the 156-th batch, train loss: 0.5536715984344482:  66%|███████▏   | 156/237 [00:42<00:25,  3.18it/s]evaluate for the 28-th batch, evaluate loss: 0.5335626006126404:  57%|██████████▏       | 26/46 [00:03<00:03,  5.18it/s]evaluate for the 28-th batch, evaluate loss: 0.5335626006126404:  61%|██████████▉       | 28/46 [00:03<00:02,  6.51it/s]Epoch: 1, train for the 152-th batch, train loss: 0.4309472143650055:  40%|████▎      | 152/383 [00:40<00:56,  4.08it/s]Epoch: 1, train for the 182-th batch, train loss: 0.6561182737350464:  76%|████████▎  | 182/241 [00:42<00:11,  5.12it/s]Epoch: 2, train for the 73-th batch, train loss: 0.4775363504886627:  61%|███████▊     | 72/119 [00:09<00:06,  6.83it/s]Epoch: 2, train for the 73-th batch, train loss: 0.4775363504886627:  61%|███████▉     | 73/119 [00:09<00:06,  7.04it/s]evaluate for the 29-th batch, evaluate loss: 0.49129608273506165:  61%|██████████▎      | 28/46 [00:03<00:02,  6.51it/s]Epoch: 1, train for the 183-th batch, train loss: 0.5074894428253174:  76%|████████▎  | 182/241 [00:42<00:11,  5.12it/s]Epoch: 1, train for the 183-th batch, train loss: 0.5074894428253174:  76%|████████▎  | 183/241 [00:42<00:10,  5.57it/s]Epoch: 2, train for the 30-th batch, train loss: 0.5390334725379944:  20%|██▌          | 29/146 [00:03<00:16,  7.13it/s]Epoch: 2, train for the 30-th batch, train loss: 0.5390334725379944:  21%|██▋          | 30/146 [00:03<00:19,  5.93it/s]Epoch: 1, train for the 153-th batch, train loss: 0.4919721186161041:  40%|████▎      | 152/383 [00:40<00:56,  4.08it/s]Epoch: 2, train for the 74-th batch, train loss: 0.5683024525642395:  61%|███████▉     | 73/119 [00:09<00:06,  7.04it/s]Epoch: 2, train for the 74-th batch, train loss: 0.5683024525642395:  62%|████████     | 74/119 [00:09<00:06,  6.68it/s]Epoch: 1, train for the 157-th batch, train loss: 0.5797247290611267:  66%|███████▏   | 156/237 [00:42<00:25,  3.18it/s]Epoch: 1, train for the 153-th batch, train loss: 0.4919721186161041:  40%|████▍      | 153/383 [00:40<00:56,  4.08it/s]Epoch: 1, train for the 157-th batch, train loss: 0.5797247290611267:  66%|███████▎   | 157/237 [00:42<00:24,  3.30it/s]evaluate for the 30-th batch, evaluate loss: 0.5101996064186096:  61%|██████████▉       | 28/46 [00:03<00:02,  6.51it/s]evaluate for the 30-th batch, evaluate loss: 0.5101996064186096:  65%|███████████▋      | 30/46 [00:03<00:02,  6.71it/s]Epoch: 2, train for the 31-th batch, train loss: 0.5528125762939453:  21%|██▋          | 30/146 [00:04<00:19,  5.93it/s]Epoch: 2, train for the 31-th batch, train loss: 0.5528125762939453:  21%|██▊          | 31/146 [00:04<00:17,  6.60it/s]Epoch: 1, train for the 184-th batch, train loss: 0.5852745175361633:  76%|████████▎  | 183/241 [00:42<00:10,  5.57it/s]Epoch: 1, train for the 184-th batch, train loss: 0.5852745175361633:  76%|████████▍  | 184/241 [00:42<00:10,  5.59it/s]evaluate for the 31-th batch, evaluate loss: 0.4504021406173706:  65%|███████████▋      | 30/46 [00:03<00:02,  6.71it/s]Epoch: 2, train for the 75-th batch, train loss: 0.5221253037452698:  62%|████████     | 74/119 [00:09<00:06,  6.68it/s]Epoch: 2, train for the 75-th batch, train loss: 0.5221253037452698:  63%|████████▏    | 75/119 [00:09<00:06,  6.67it/s]Epoch: 2, train for the 32-th batch, train loss: 0.5468427538871765:  21%|██▊          | 31/146 [00:04<00:17,  6.60it/s]evaluate for the 32-th batch, evaluate loss: 0.48835447430610657:  65%|███████████      | 30/46 [00:03<00:02,  6.71it/s]evaluate for the 32-th batch, evaluate loss: 0.48835447430610657:  70%|███████████▊     | 32/46 [00:03<00:01,  8.05it/s]Epoch: 2, train for the 32-th batch, train loss: 0.5468427538871765:  22%|██▊          | 32/146 [00:04<00:16,  6.92it/s]Epoch: 1, train for the 154-th batch, train loss: 0.44828471541404724:  40%|███▉      | 153/383 [00:41<00:56,  4.08it/s]Epoch: 1, train for the 185-th batch, train loss: 0.6705052852630615:  76%|████████▍  | 184/241 [00:42<00:10,  5.59it/s]Epoch: 1, train for the 158-th batch, train loss: 0.5180110931396484:  66%|███████▎   | 157/237 [00:42<00:24,  3.30it/s]Epoch: 1, train for the 154-th batch, train loss: 0.44828471541404724:  40%|████      | 154/383 [00:41<00:56,  4.05it/s]evaluate for the 33-th batch, evaluate loss: 0.4948784410953522:  70%|████████████▌     | 32/46 [00:04<00:01,  8.05it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5374413728713989:  63%|████████▏    | 75/119 [00:09<00:06,  6.67it/s]Epoch: 1, train for the 185-th batch, train loss: 0.6705052852630615:  77%|████████▍  | 185/241 [00:42<00:09,  5.70it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5374413728713989:  64%|████████▎    | 76/119 [00:09<00:06,  6.73it/s]Epoch: 1, train for the 158-th batch, train loss: 0.5180110931396484:  67%|███████▎   | 158/237 [00:42<00:22,  3.47it/s]Epoch: 2, train for the 33-th batch, train loss: 0.5476083159446716:  22%|██▊          | 32/146 [00:04<00:16,  6.92it/s]Epoch: 2, train for the 33-th batch, train loss: 0.5476083159446716:  23%|██▉          | 33/146 [00:04<00:15,  7.20it/s]Epoch: 1, train for the 186-th batch, train loss: 0.6141391396522522:  77%|████████▍  | 185/241 [00:42<00:09,  5.70it/s]Epoch: 1, train for the 186-th batch, train loss: 0.6141391396522522:  77%|████████▍  | 186/241 [00:42<00:08,  6.14it/s]Epoch: 2, train for the 77-th batch, train loss: 0.4846796989440918:  64%|████████▎    | 76/119 [00:10<00:06,  6.73it/s]evaluate for the 34-th batch, evaluate loss: 0.4588474929332733:  70%|████████████▌     | 32/46 [00:04<00:01,  8.05it/s]evaluate for the 34-th batch, evaluate loss: 0.4588474929332733:  74%|█████████████▎    | 34/46 [00:04<00:01,  7.61it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5572420954704285:  23%|██▉          | 33/146 [00:04<00:15,  7.20it/s]Epoch: 2, train for the 77-th batch, train loss: 0.4846796989440918:  65%|████████▍    | 77/119 [00:10<00:06,  6.04it/s]Epoch: 1, train for the 155-th batch, train loss: 0.32818397879600525:  40%|████      | 154/383 [00:41<00:56,  4.05it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5572420954704285:  23%|███          | 34/146 [00:04<00:16,  6.60it/s]Epoch: 1, train for the 159-th batch, train loss: 0.574356198310852:  67%|████████    | 158/237 [00:42<00:22,  3.47it/s]Epoch: 1, train for the 187-th batch, train loss: 0.659774899482727:  77%|█████████▎  | 186/241 [00:42<00:08,  6.14it/s]Epoch: 1, train for the 159-th batch, train loss: 0.574356198310852:  67%|████████    | 159/237 [00:42<00:21,  3.56it/s]Epoch: 1, train for the 155-th batch, train loss: 0.32818397879600525:  40%|████      | 155/383 [00:41<00:59,  3.84it/s]Epoch: 1, train for the 187-th batch, train loss: 0.659774899482727:  78%|█████████▎  | 187/241 [00:42<00:08,  6.29it/s]evaluate for the 35-th batch, evaluate loss: 0.5130842328071594:  74%|█████████████▎    | 34/46 [00:04<00:01,  7.61it/s]Epoch: 2, train for the 78-th batch, train loss: 0.4980475902557373:  65%|████████▍    | 77/119 [00:10<00:06,  6.04it/s]Epoch: 2, train for the 78-th batch, train loss: 0.4980475902557373:  66%|████████▌    | 78/119 [00:10<00:06,  6.27it/s]Epoch: 2, train for the 35-th batch, train loss: 0.506269633769989:  23%|███▎          | 34/146 [00:04<00:16,  6.60it/s]Epoch: 2, train for the 35-th batch, train loss: 0.506269633769989:  24%|███▎          | 35/146 [00:04<00:16,  6.57it/s]evaluate for the 36-th batch, evaluate loss: 0.4651683568954468:  74%|█████████████▎    | 34/46 [00:04<00:01,  7.61it/s]evaluate for the 36-th batch, evaluate loss: 0.4651683568954468:  78%|██████████████    | 36/46 [00:04<00:01,  8.42it/s]Epoch: 1, train for the 188-th batch, train loss: 0.4906390905380249:  78%|████████▌  | 187/241 [00:42<00:08,  6.29it/s]Epoch: 1, train for the 188-th batch, train loss: 0.4906390905380249:  78%|████████▌  | 188/241 [00:43<00:08,  6.38it/s]Epoch: 1, train for the 160-th batch, train loss: 0.5735011696815491:  67%|███████▍   | 159/237 [00:43<00:21,  3.56it/s]Epoch: 2, train for the 36-th batch, train loss: 0.5118353366851807:  24%|███          | 35/146 [00:04<00:16,  6.57it/s]evaluate for the 37-th batch, evaluate loss: 0.5177112221717834:  78%|██████████████    | 36/46 [00:04<00:01,  8.42it/s]Epoch: 1, train for the 160-th batch, train loss: 0.5735011696815491:  68%|███████▍   | 160/237 [00:43<00:20,  3.70it/s]Epoch: 1, train for the 156-th batch, train loss: 0.37007230520248413:  40%|████      | 155/383 [00:41<00:59,  3.84it/s]Epoch: 2, train for the 37-th batch, train loss: 0.5483188629150391:  24%|███          | 35/146 [00:04<00:16,  6.57it/s]Epoch: 2, train for the 37-th batch, train loss: 0.5483188629150391:  25%|███▎         | 37/146 [00:04<00:14,  7.59it/s]Epoch: 1, train for the 156-th batch, train loss: 0.37007230520248413:  41%|████      | 156/383 [00:41<01:03,  3.56it/s]Epoch: 2, train for the 79-th batch, train loss: 0.4887402355670929:  66%|████████▌    | 78/119 [00:10<00:06,  6.27it/s]Epoch: 2, train for the 79-th batch, train loss: 0.4887402355670929:  66%|████████▋    | 79/119 [00:10<00:07,  5.20it/s]Epoch: 1, train for the 161-th batch, train loss: 0.5893889665603638:  68%|███████▍   | 160/237 [00:43<00:20,  3.70it/s]Epoch: 1, train for the 189-th batch, train loss: 0.41970932483673096:  78%|███████▊  | 188/241 [00:43<00:08,  6.38it/s]evaluate for the 38-th batch, evaluate loss: 0.48637330532073975:  78%|█████████████▎   | 36/46 [00:04<00:01,  8.42it/s]evaluate for the 38-th batch, evaluate loss: 0.48637330532073975:  83%|██████████████   | 38/46 [00:04<00:01,  7.83it/s]Epoch: 1, train for the 189-th batch, train loss: 0.41970932483673096:  78%|███████▊  | 189/241 [00:43<00:09,  5.36it/s]Epoch: 2, train for the 38-th batch, train loss: 0.5106731653213501:  25%|███▎         | 37/146 [00:05<00:14,  7.59it/s]Epoch: 1, train for the 161-th batch, train loss: 0.5893889665603638:  68%|███████▍   | 161/237 [00:43<00:18,  4.11it/s]Epoch: 2, train for the 38-th batch, train loss: 0.5106731653213501:  26%|███▍         | 38/146 [00:05<00:13,  7.78it/s]Epoch: 2, train for the 80-th batch, train loss: 0.522345781326294:  66%|█████████▎    | 79/119 [00:10<00:07,  5.20it/s]Epoch: 2, train for the 80-th batch, train loss: 0.522345781326294:  67%|█████████▍    | 80/119 [00:10<00:06,  5.63it/s]evaluate for the 39-th batch, evaluate loss: 0.4836418330669403:  83%|██████████████▊   | 38/46 [00:04<00:01,  7.83it/s]Epoch: 1, train for the 157-th batch, train loss: 0.4457762539386749:  41%|████▍      | 156/383 [00:41<01:03,  3.56it/s]Epoch: 2, train for the 39-th batch, train loss: 0.5533263087272644:  26%|███▍         | 38/146 [00:05<00:13,  7.78it/s]Epoch: 2, train for the 39-th batch, train loss: 0.5533263087272644:  27%|███▍         | 39/146 [00:05<00:13,  7.70it/s]Epoch: 1, train for the 157-th batch, train loss: 0.4457762539386749:  41%|████▌      | 157/383 [00:41<01:00,  3.74it/s]Epoch: 1, train for the 190-th batch, train loss: 0.4108918011188507:  78%|████████▋  | 189/241 [00:43<00:09,  5.36it/s]evaluate for the 40-th batch, evaluate loss: 0.47941192984580994:  83%|██████████████   | 38/46 [00:04<00:01,  7.83it/s]evaluate for the 40-th batch, evaluate loss: 0.47941192984580994:  87%|██████████████▊  | 40/46 [00:04<00:00,  8.63it/s]Epoch: 1, train for the 190-th batch, train loss: 0.4108918011188507:  79%|████████▋  | 190/241 [00:43<00:09,  5.36it/s]Epoch: 2, train for the 81-th batch, train loss: 0.523911714553833:  67%|█████████▍    | 80/119 [00:10<00:06,  5.63it/s]Epoch: 2, train for the 81-th batch, train loss: 0.523911714553833:  68%|█████████▌    | 81/119 [00:10<00:06,  6.09it/s]evaluate for the 41-th batch, evaluate loss: 0.4885358214378357:  87%|███████████████▋  | 40/46 [00:04<00:00,  8.63it/s]Epoch: 2, train for the 40-th batch, train loss: 0.517948567867279:  27%|███▋          | 39/146 [00:05<00:13,  7.70it/s]Epoch: 2, train for the 40-th batch, train loss: 0.517948567867279:  27%|███▊          | 40/146 [00:05<00:13,  8.04it/s]Epoch: 1, train for the 162-th batch, train loss: 0.5909925699234009:  68%|███████▍   | 161/237 [00:43<00:18,  4.11it/s]Epoch: 1, train for the 162-th batch, train loss: 0.5909925699234009:  68%|███████▌   | 162/237 [00:43<00:20,  3.59it/s]Epoch: 2, train for the 82-th batch, train loss: 0.5312684774398804:  68%|████████▊    | 81/119 [00:10<00:06,  6.09it/s]Epoch: 2, train for the 82-th batch, train loss: 0.5312684774398804:  69%|████████▉    | 82/119 [00:11<00:06,  6.01it/s]Epoch: 2, train for the 41-th batch, train loss: 0.4998771548271179:  27%|███▌         | 40/146 [00:05<00:13,  8.04it/s]Epoch: 2, train for the 41-th batch, train loss: 0.4998771548271179:  28%|███▋         | 41/146 [00:05<00:13,  7.62it/s]Epoch: 1, train for the 191-th batch, train loss: 0.3736124336719513:  79%|████████▋  | 190/241 [00:43<00:09,  5.36it/s]Epoch: 1, train for the 158-th batch, train loss: 0.4112628102302551:  41%|████▌      | 157/383 [00:42<01:00,  3.74it/s]evaluate for the 42-th batch, evaluate loss: 0.46123817563056946:  87%|██████████████▊  | 40/46 [00:05<00:00,  8.63it/s]evaluate for the 42-th batch, evaluate loss: 0.46123817563056946:  91%|███████████████▌ | 42/46 [00:05<00:00,  8.03it/s]Epoch: 1, train for the 191-th batch, train loss: 0.3736124336719513:  79%|████████▋  | 191/241 [00:43<00:10,  4.65it/s]Epoch: 1, train for the 158-th batch, train loss: 0.4112628102302551:  41%|████▌      | 158/383 [00:42<01:05,  3.45it/s]Epoch: 1, train for the 163-th batch, train loss: 0.5894217491149902:  68%|███████▌   | 162/237 [00:43<00:20,  3.59it/s]evaluate for the 43-th batch, evaluate loss: 0.529386043548584:  91%|█████████████████▎ | 42/46 [00:05<00:00,  8.03it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5230951309204102:  69%|████████▉    | 82/119 [00:11<00:06,  6.01it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5230951309204102:  70%|█████████    | 83/119 [00:11<00:05,  6.02it/s]Epoch: 1, train for the 163-th batch, train loss: 0.5894217491149902:  69%|███████▌   | 163/237 [00:43<00:18,  4.02it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5054788589477539:  28%|███▋         | 41/146 [00:05<00:13,  7.62it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5054788589477539:  29%|███▋         | 42/146 [00:05<00:14,  7.05it/s]Epoch: 1, train for the 192-th batch, train loss: 0.335750013589859:  79%|█████████▌  | 191/241 [00:43<00:10,  4.65it/s]evaluate for the 44-th batch, evaluate loss: 0.47845718264579773:  91%|███████████████▌ | 42/46 [00:05<00:00,  8.03it/s]evaluate for the 44-th batch, evaluate loss: 0.47845718264579773:  96%|████████████████▎| 44/46 [00:05<00:00,  8.82it/s]Epoch: 1, train for the 192-th batch, train loss: 0.335750013589859:  80%|█████████▌  | 192/241 [00:43<00:09,  4.96it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5036148428916931:  70%|█████████    | 83/119 [00:11<00:05,  6.02it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5036148428916931:  71%|█████████▏   | 84/119 [00:11<00:05,  5.98it/s]Epoch: 2, train for the 43-th batch, train loss: 0.5126060247421265:  29%|███▋         | 42/146 [00:05<00:14,  7.05it/s]Epoch: 2, train for the 43-th batch, train loss: 0.5126060247421265:  29%|███▊         | 43/146 [00:05<00:15,  6.82it/s]Epoch: 1, train for the 159-th batch, train loss: 0.4666665196418762:  41%|████▌      | 158/383 [00:42<01:05,  3.45it/s]Epoch: 1, train for the 159-th batch, train loss: 0.4666665196418762:  42%|████▌      | 159/383 [00:42<01:08,  3.29it/s]Epoch: 2, train for the 85-th batch, train loss: 0.544805109500885:  71%|█████████▉    | 84/119 [00:11<00:05,  5.98it/s]Epoch: 2, train for the 85-th batch, train loss: 0.544805109500885:  71%|██████████    | 85/119 [00:11<00:05,  6.18it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5091040134429932:  29%|███▊         | 43/146 [00:05<00:15,  6.82it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5091040134429932:  30%|███▉         | 44/146 [00:05<00:14,  6.90it/s]Epoch: 1, train for the 193-th batch, train loss: 0.3388043940067291:  80%|████████▊  | 192/241 [00:44<00:09,  4.96it/s]Epoch: 1, train for the 164-th batch, train loss: 0.5979090929031372:  69%|███████▌   | 163/237 [00:44<00:18,  4.02it/s]evaluate for the 45-th batch, evaluate loss: 0.48292258381843567:  96%|████████████████▎| 44/46 [00:05<00:00,  8.82it/s]evaluate for the 45-th batch, evaluate loss: 0.48292258381843567:  98%|████████████████▋| 45/46 [00:05<00:00,  6.92it/s]Epoch: 1, train for the 193-th batch, train loss: 0.3388043940067291:  80%|████████▊  | 193/241 [00:44<00:10,  4.40it/s]Epoch: 1, train for the 164-th batch, train loss: 0.5979090929031372:  69%|███████▌   | 164/237 [00:44<00:21,  3.41it/s]evaluate for the 46-th batch, evaluate loss: 0.4751458168029785:  98%|█████████████████▌| 45/46 [00:05<00:00,  6.92it/s]evaluate for the 46-th batch, evaluate loss: 0.4751458168029785: 100%|██████████████████| 46/46 [00:05<00:00,  8.07it/s]
Epoch: 1, train for the 160-th batch, train loss: 0.49953779578208923:  42%|████▏     | 159/383 [00:42<01:08,  3.29it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5542107820510864:  71%|█████████▎   | 85/119 [00:11<00:05,  6.18it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5542107820510864:  72%|█████████▍   | 86/119 [00:11<00:05,  6.22it/s]Epoch: 1, train for the 160-th batch, train loss: 0.49953779578208923:  42%|████▏     | 160/383 [00:42<01:01,  3.65it/s]Epoch: 2, train for the 45-th batch, train loss: 0.512839674949646:  30%|████▏         | 44/146 [00:06<00:14,  6.90it/s]Epoch: 2, train for the 45-th batch, train loss: 0.512839674949646:  31%|████▎         | 45/146 [00:06<00:15,  6.56it/s]Epoch: 1, train for the 194-th batch, train loss: 0.3430800437927246:  80%|████████▊  | 193/241 [00:44<00:10,  4.40it/s]Epoch: 1, train for the 194-th batch, train loss: 0.3430800437927246:  80%|████████▊  | 194/241 [00:44<00:09,  4.82it/s]Epoch: 1, train for the 165-th batch, train loss: 0.55038982629776:  69%|████████▉    | 164/237 [00:44<00:21,  3.41it/s]Epoch: 1, train for the 165-th batch, train loss: 0.55038982629776:  70%|█████████    | 165/237 [00:44<00:19,  3.64it/s]Epoch: 2, train for the 46-th batch, train loss: 0.5144016742706299:  31%|████         | 45/146 [00:06<00:15,  6.56it/s]Epoch: 2, train for the 46-th batch, train loss: 0.5144016742706299:  32%|████         | 46/146 [00:06<00:14,  6.72it/s]Epoch: 2, train for the 87-th batch, train loss: 0.5232794880867004:  72%|█████████▍   | 86/119 [00:11<00:05,  6.22it/s]Epoch: 1, train for the 195-th batch, train loss: 0.595585823059082:  80%|█████████▋  | 194/241 [00:44<00:09,  4.82it/s]Epoch: 1, train for the 161-th batch, train loss: 0.45372098684310913:  42%|████▏     | 160/383 [00:43<01:01,  3.65it/s]Epoch: 2, train for the 87-th batch, train loss: 0.5232794880867004:  73%|█████████▌   | 87/119 [00:11<00:05,  5.96it/s]Epoch: 1, train for the 195-th batch, train loss: 0.595585823059082:  81%|█████████▋  | 195/241 [00:44<00:08,  5.34it/s]Epoch: 1, train for the 161-th batch, train loss: 0.45372098684310913:  42%|████▏     | 161/383 [00:43<00:56,  3.96it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5716802477836609:  32%|████         | 46/146 [00:06<00:14,  6.72it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5716802477836609:  32%|████▏        | 47/146 [00:06<00:13,  7.40it/s]Epoch: 2, train for the 88-th batch, train loss: 0.5560383200645447:  73%|█████████▌   | 87/119 [00:11<00:05,  5.96it/s]Epoch: 2, train for the 88-th batch, train loss: 0.5560383200645447:  74%|█████████▌   | 88/119 [00:11<00:04,  6.26it/s]Epoch: 1, train for the 196-th batch, train loss: 0.5231695175170898:  81%|████████▉  | 195/241 [00:44<00:08,  5.34it/s]Epoch: 2, train for the 48-th batch, train loss: 0.5454549193382263:  32%|████▏        | 47/146 [00:06<00:13,  7.40it/s]Epoch: 2, train for the 48-th batch, train loss: 0.5454549193382263:  33%|████▎        | 48/146 [00:06<00:12,  7.88it/s]Epoch: 1, train for the 196-th batch, train loss: 0.5231695175170898:  81%|████████▉  | 196/241 [00:44<00:08,  5.39it/s]Epoch: 1, train for the 162-th batch, train loss: 0.4583529531955719:  42%|████▌      | 161/383 [00:43<00:56,  3.96it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5017502307891846:  74%|█████████▌   | 88/119 [00:12<00:04,  6.26it/s]Epoch: 1, train for the 162-th batch, train loss: 0.4583529531955719:  42%|████▋      | 162/383 [00:43<00:55,  4.00it/s]Epoch: 1, train for the 166-th batch, train loss: 0.5252636671066284:  70%|███████▋   | 165/237 [00:44<00:19,  3.64it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5017502307891846:  75%|█████████▋   | 89/119 [00:12<00:04,  6.54it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5124608278274536:  33%|████▎        | 48/146 [00:06<00:12,  7.88it/s]Epoch: 1, train for the 166-th batch, train loss: 0.5252636671066284:  70%|███████▋   | 166/237 [00:44<00:20,  3.42it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5124608278274536:  34%|████▎        | 49/146 [00:06<00:12,  7.95it/s]Epoch: 1, train for the 197-th batch, train loss: 0.5607028603553772:  81%|████████▉  | 196/241 [00:44<00:08,  5.39it/s]  0%|                                                                                            | 0/25 [00:00<?, ?it/s]Epoch: 1, train for the 197-th batch, train loss: 0.5607028603553772:  82%|████████▉  | 197/241 [00:44<00:08,  5.25it/s]Epoch: 2, train for the 90-th batch, train loss: 0.5249991416931152:  75%|█████████▋   | 89/119 [00:12<00:04,  6.54it/s]Epoch: 2, train for the 90-th batch, train loss: 0.5249991416931152:  76%|█████████▊   | 90/119 [00:12<00:04,  6.68it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5701977610588074:  34%|████▎        | 49/146 [00:06<00:12,  7.95it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5701977610588074:  34%|████▍        | 50/146 [00:06<00:12,  7.65it/s]evaluate for the 1-th batch, evaluate loss: 0.7607200741767883:   0%|                            | 0/25 [00:00<?, ?it/s]Epoch: 1, train for the 167-th batch, train loss: 0.5593066811561584:  70%|███████▋   | 166/237 [00:45<00:20,  3.42it/s]Epoch: 1, train for the 163-th batch, train loss: 0.3853325843811035:  42%|████▋      | 162/383 [00:43<00:55,  4.00it/s]Epoch: 1, train for the 198-th batch, train loss: 0.5251973867416382:  82%|████████▉  | 197/241 [00:45<00:08,  5.25it/s]evaluate for the 2-th batch, evaluate loss: 0.752081573009491:   0%|                             | 0/25 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.752081573009491:   8%|█▋                   | 2/25 [00:00<00:01, 12.55it/s]Epoch: 1, train for the 167-th batch, train loss: 0.5593066811561584:  70%|███████▊   | 167/237 [00:45<00:19,  3.60it/s]Epoch: 1, train for the 198-th batch, train loss: 0.5251973867416382:  82%|█████████  | 198/241 [00:45<00:07,  5.57it/s]Epoch: 1, train for the 163-th batch, train loss: 0.3853325843811035:  43%|████▋      | 163/383 [00:43<00:57,  3.83it/s]Epoch: 2, train for the 91-th batch, train loss: 0.5007261037826538:  76%|█████████▊   | 90/119 [00:12<00:04,  6.68it/s]Epoch: 2, train for the 91-th batch, train loss: 0.5007261037826538:  76%|█████████▉   | 91/119 [00:12<00:04,  6.42it/s]Epoch: 2, train for the 51-th batch, train loss: 0.5307953357696533:  34%|████▍        | 50/146 [00:06<00:12,  7.65it/s]Epoch: 2, train for the 51-th batch, train loss: 0.5307953357696533:  35%|████▌        | 51/146 [00:06<00:13,  7.23it/s]evaluate for the 3-th batch, evaluate loss: 0.7955380082130432:   8%|█▌                  | 2/25 [00:00<00:01, 12.55it/s]Epoch: 1, train for the 199-th batch, train loss: 0.35681700706481934:  82%|████████▏ | 198/241 [00:45<00:07,  5.57it/s]Epoch: 1, train for the 199-th batch, train loss: 0.35681700706481934:  83%|████████▎ | 199/241 [00:45<00:07,  5.63it/s]evaluate for the 4-th batch, evaluate loss: 0.7239419221878052:   8%|█▌                  | 2/25 [00:00<00:01, 12.55it/s]evaluate for the 4-th batch, evaluate loss: 0.7239419221878052:  16%|███▏                | 4/25 [00:00<00:01, 11.51it/s]Epoch: 1, train for the 168-th batch, train loss: 0.5810009241104126:  70%|███████▊   | 167/237 [00:45<00:19,  3.60it/s]Epoch: 2, train for the 52-th batch, train loss: 0.537417471408844:  35%|████▉         | 51/146 [00:06<00:13,  7.23it/s]Epoch: 1, train for the 168-th batch, train loss: 0.5810009241104126:  71%|███████▊   | 168/237 [00:45<00:18,  3.82it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5117054581642151:  76%|█████████▉   | 91/119 [00:12<00:04,  6.42it/s]Epoch: 2, train for the 52-th batch, train loss: 0.537417471408844:  36%|████▉         | 52/146 [00:06<00:13,  6.81it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5117054581642151:  77%|██████████   | 92/119 [00:12<00:04,  5.93it/s]Epoch: 1, train for the 164-th batch, train loss: 0.36999619007110596:  43%|████▎     | 163/383 [00:43<00:57,  3.83it/s]evaluate for the 5-th batch, evaluate loss: 0.746324896812439:  16%|███▎                 | 4/25 [00:00<00:01, 11.51it/s]Epoch: 1, train for the 164-th batch, train loss: 0.36999619007110596:  43%|████▎     | 164/383 [00:43<00:58,  3.73it/s]Epoch: 1, train for the 200-th batch, train loss: 0.44890546798706055:  83%|████████▎ | 199/241 [00:45<00:07,  5.63it/s]evaluate for the 6-th batch, evaluate loss: 0.745023787021637:  16%|███▎                 | 4/25 [00:00<00:01, 11.51it/s]evaluate for the 6-th batch, evaluate loss: 0.745023787021637:  24%|█████                | 6/25 [00:00<00:01, 12.30it/s]Epoch: 2, train for the 53-th batch, train loss: 0.5464988350868225:  36%|████▋        | 52/146 [00:07<00:13,  6.81it/s]Epoch: 2, train for the 53-th batch, train loss: 0.5464988350868225:  36%|████▋        | 53/146 [00:07<00:12,  7.33it/s]Epoch: 1, train for the 200-th batch, train loss: 0.44890546798706055:  83%|████████▎ | 200/241 [00:45<00:07,  5.76it/s]Epoch: 2, train for the 93-th batch, train loss: 0.4753125309944153:  77%|██████████   | 92/119 [00:12<00:04,  5.93it/s]Epoch: 2, train for the 93-th batch, train loss: 0.4753125309944153:  78%|██████████▏  | 93/119 [00:12<00:04,  5.86it/s]evaluate for the 7-th batch, evaluate loss: 0.7708937525749207:  24%|████▊               | 6/25 [00:00<00:01, 12.30it/s]Epoch: 1, train for the 165-th batch, train loss: 0.333903431892395:  43%|█████▏      | 164/383 [00:44<00:58,  3.73it/s]Epoch: 2, train for the 54-th batch, train loss: 0.5584245324134827:  36%|████▋        | 53/146 [00:07<00:12,  7.33it/s]Epoch: 2, train for the 54-th batch, train loss: 0.5584245324134827:  37%|████▊        | 54/146 [00:07<00:12,  7.33it/s]Epoch: 1, train for the 201-th batch, train loss: 0.4889516830444336:  83%|█████████▏ | 200/241 [00:45<00:07,  5.76it/s]evaluate for the 8-th batch, evaluate loss: 0.747109591960907:  24%|█████                | 6/25 [00:00<00:01, 12.30it/s]evaluate for the 8-th batch, evaluate loss: 0.747109591960907:  32%|██████▋              | 8/25 [00:00<00:01, 12.10it/s]Epoch: 1, train for the 201-th batch, train loss: 0.4889516830444336:  83%|█████████▏ | 201/241 [00:45<00:06,  5.83it/s]Epoch: 1, train for the 169-th batch, train loss: 0.5674533843994141:  71%|███████▊   | 168/237 [00:45<00:18,  3.82it/s]Epoch: 1, train for the 165-th batch, train loss: 0.333903431892395:  43%|█████▏      | 165/383 [00:44<00:55,  3.92it/s]Epoch: 2, train for the 94-th batch, train loss: 0.495636522769928:  78%|██████████▉   | 93/119 [00:12<00:04,  5.86it/s]Epoch: 2, train for the 94-th batch, train loss: 0.495636522769928:  79%|███████████   | 94/119 [00:12<00:03,  6.36it/s]Epoch: 1, train for the 169-th batch, train loss: 0.5674533843994141:  71%|███████▊   | 169/237 [00:45<00:19,  3.52it/s]evaluate for the 9-th batch, evaluate loss: 0.7171643376350403:  32%|██████▍             | 8/25 [00:00<00:01, 12.10it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5630533695220947:  37%|████▊        | 54/146 [00:07<00:12,  7.33it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5630533695220947:  38%|████▉        | 55/146 [00:07<00:11,  7.86it/s]Epoch: 1, train for the 202-th batch, train loss: 0.25810402631759644:  83%|████████▎ | 201/241 [00:45<00:06,  5.83it/s]evaluate for the 10-th batch, evaluate loss: 0.769996166229248:  32%|██████▍             | 8/25 [00:00<00:01, 12.10it/s]evaluate for the 10-th batch, evaluate loss: 0.769996166229248:  40%|███████▌           | 10/25 [00:00<00:01, 11.91it/s]Epoch: 1, train for the 202-th batch, train loss: 0.25810402631759644:  84%|████████▍ | 202/241 [00:45<00:06,  5.83it/s]Epoch: 2, train for the 95-th batch, train loss: 0.46162134408950806:  79%|█████████▍  | 94/119 [00:13<00:03,  6.36it/s]Epoch: 1, train for the 170-th batch, train loss: 0.5628708004951477:  71%|███████▊   | 169/237 [00:45<00:19,  3.52it/s]Epoch: 2, train for the 95-th batch, train loss: 0.46162134408950806:  80%|█████████▌  | 95/119 [00:13<00:04,  5.72it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5286783576011658:  38%|████▉        | 55/146 [00:07<00:11,  7.86it/s]evaluate for the 11-th batch, evaluate loss: 0.7546212673187256:  40%|███████▏          | 10/25 [00:00<00:01, 11.91it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5286783576011658:  38%|████▉        | 56/146 [00:07<00:13,  6.83it/s]Epoch: 1, train for the 170-th batch, train loss: 0.5628708004951477:  72%|███████▉   | 170/237 [00:45<00:18,  3.70it/s]Epoch: 1, train for the 203-th batch, train loss: 0.585810661315918:  84%|██████████  | 202/241 [00:45<00:06,  5.83it/s]evaluate for the 12-th batch, evaluate loss: 0.6704286336898804:  40%|███████▏          | 10/25 [00:01<00:01, 11.91it/s]evaluate for the 12-th batch, evaluate loss: 0.6704286336898804:  48%|████████▋         | 12/25 [00:01<00:01, 11.80it/s]Epoch: 1, train for the 203-th batch, train loss: 0.585810661315918:  84%|██████████  | 203/241 [00:45<00:06,  5.86it/s]Epoch: 2, train for the 96-th batch, train loss: 0.4408777058124542:  80%|██████████▍  | 95/119 [00:13<00:04,  5.72it/s]Epoch: 1, train for the 166-th batch, train loss: 0.4653647541999817:  43%|████▋      | 165/383 [00:44<00:55,  3.92it/s]Epoch: 2, train for the 57-th batch, train loss: 0.5491928458213806:  38%|████▉        | 56/146 [00:07<00:13,  6.83it/s]Epoch: 2, train for the 96-th batch, train loss: 0.4408777058124542:  81%|██████████▍  | 96/119 [00:13<00:03,  5.85it/s]Epoch: 2, train for the 57-th batch, train loss: 0.5491928458213806:  39%|█████        | 57/146 [00:07<00:13,  6.74it/s]evaluate for the 13-th batch, evaluate loss: 0.6797254681587219:  48%|████████▋         | 12/25 [00:01<00:01, 11.80it/s]Epoch: 1, train for the 166-th batch, train loss: 0.4653647541999817:  43%|████▊      | 166/383 [00:44<01:06,  3.25it/s]Epoch: 1, train for the 171-th batch, train loss: 0.5650452375411987:  72%|███████▉   | 170/237 [00:46<00:18,  3.70it/s]Epoch: 1, train for the 204-th batch, train loss: 0.4791724383831024:  84%|█████████▎ | 203/241 [00:46<00:06,  5.86it/s]Epoch: 1, train for the 204-th batch, train loss: 0.4791724383831024:  85%|█████████▎ | 204/241 [00:46<00:06,  6.01it/s]evaluate for the 14-th batch, evaluate loss: 0.7292921543121338:  48%|████████▋         | 12/25 [00:01<00:01, 11.80it/s]evaluate for the 14-th batch, evaluate loss: 0.7292921543121338:  56%|██████████        | 14/25 [00:01<00:00, 11.91it/s]Epoch: 1, train for the 171-th batch, train loss: 0.5650452375411987:  72%|███████▉   | 171/237 [00:46<00:17,  3.85it/s]Epoch: 2, train for the 97-th batch, train loss: 0.5054072141647339:  81%|██████████▍  | 96/119 [00:13<00:03,  5.85it/s]Epoch: 2, train for the 58-th batch, train loss: 0.5309150218963623:  39%|█████        | 57/146 [00:07<00:13,  6.74it/s]Epoch: 2, train for the 97-th batch, train loss: 0.5054072141647339:  82%|██████████▌  | 97/119 [00:13<00:03,  5.94it/s]Epoch: 2, train for the 58-th batch, train loss: 0.5309150218963623:  40%|█████▏       | 58/146 [00:07<00:13,  6.58it/s]evaluate for the 15-th batch, evaluate loss: 0.7698735594749451:  56%|██████████        | 14/25 [00:01<00:00, 11.91it/s]Epoch: 1, train for the 167-th batch, train loss: 0.43072909116744995:  43%|████▎     | 166/383 [00:44<01:06,  3.25it/s]Epoch: 1, train for the 205-th batch, train loss: 0.4824499487876892:  85%|█████████▎ | 204/241 [00:46<00:06,  6.01it/s]Epoch: 1, train for the 167-th batch, train loss: 0.43072909116744995:  44%|████▎     | 167/383 [00:44<01:00,  3.59it/s]Epoch: 1, train for the 205-th batch, train loss: 0.4824499487876892:  85%|█████████▎ | 205/241 [00:46<00:05,  6.09it/s]evaluate for the 16-th batch, evaluate loss: 0.688006579875946:  56%|██████████▋        | 14/25 [00:01<00:00, 11.91it/s]evaluate for the 16-th batch, evaluate loss: 0.688006579875946:  64%|████████████▏      | 16/25 [00:01<00:00, 11.64it/s]Epoch: 2, train for the 98-th batch, train loss: 0.46071067452430725:  82%|█████████▊  | 97/119 [00:13<00:03,  5.94it/s]Epoch: 2, train for the 98-th batch, train loss: 0.46071067452430725:  82%|█████████▉  | 98/119 [00:13<00:03,  6.24it/s]Epoch: 1, train for the 172-th batch, train loss: 0.5297314524650574:  72%|███████▉   | 171/237 [00:46<00:17,  3.85it/s]Epoch: 2, train for the 59-th batch, train loss: 0.5100013017654419:  40%|█████▏       | 58/146 [00:08<00:13,  6.58it/s]evaluate for the 17-th batch, evaluate loss: 0.6807570457458496:  64%|███████████▌      | 16/25 [00:01<00:00, 11.64it/s]Epoch: 2, train for the 59-th batch, train loss: 0.5100013017654419:  40%|█████▎       | 59/146 [00:08<00:14,  6.12it/s]Epoch: 1, train for the 172-th batch, train loss: 0.5297314524650574:  73%|███████▉   | 172/237 [00:46<00:16,  3.89it/s]evaluate for the 18-th batch, evaluate loss: 0.6496114730834961:  64%|███████████▌      | 16/25 [00:01<00:00, 11.64it/s]evaluate for the 18-th batch, evaluate loss: 0.6496114730834961:  72%|████████████▉     | 18/25 [00:01<00:00, 12.31it/s]Epoch: 2, train for the 99-th batch, train loss: 0.48017188906669617:  82%|█████████▉  | 98/119 [00:13<00:03,  6.24it/s]Epoch: 2, train for the 99-th batch, train loss: 0.48017188906669617:  83%|█████████▉  | 99/119 [00:13<00:03,  6.60it/s]evaluate for the 19-th batch, evaluate loss: 0.6298666000366211:  72%|████████████▉     | 18/25 [00:01<00:00, 12.31it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5494726300239563:  40%|█████▎       | 59/146 [00:08<00:14,  6.12it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5494726300239563:  41%|█████▎       | 60/146 [00:08<00:13,  6.37it/s]Epoch: 1, train for the 206-th batch, train loss: 0.5574461221694946:  85%|█████████▎ | 205/241 [00:46<00:05,  6.09it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5052459836006165:  83%|█████████▉  | 99/119 [00:13<00:03,  6.60it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5052459836006165:  84%|█████████▏ | 100/119 [00:13<00:02,  6.94it/s]evaluate for the 20-th batch, evaluate loss: 0.6904545426368713:  72%|████████████▉     | 18/25 [00:01<00:00, 12.31it/s]evaluate for the 20-th batch, evaluate loss: 0.6904545426368713:  80%|██████████████▍   | 20/25 [00:01<00:00, 12.65it/s]Epoch: 1, train for the 206-th batch, train loss: 0.5574461221694946:  85%|█████████▍ | 206/241 [00:46<00:07,  4.69it/s]Epoch: 1, train for the 168-th batch, train loss: 0.31487342715263367:  44%|████▎     | 167/383 [00:45<01:00,  3.59it/s]Epoch: 1, train for the 173-th batch, train loss: 0.49104735255241394:  73%|███████▎  | 172/237 [00:46<00:16,  3.89it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5820779800415039:  41%|█████▎       | 60/146 [00:08<00:13,  6.37it/s]Epoch: 1, train for the 173-th batch, train loss: 0.49104735255241394:  73%|███████▎  | 173/237 [00:46<00:16,  3.88it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5820779800415039:  42%|█████▍       | 61/146 [00:08<00:12,  6.76it/s]Epoch: 1, train for the 168-th batch, train loss: 0.31487342715263367:  44%|████▍     | 168/383 [00:45<01:06,  3.22it/s]evaluate for the 21-th batch, evaluate loss: 0.7122013568878174:  80%|██████████████▍   | 20/25 [00:01<00:00, 12.65it/s]Epoch: 2, train for the 101-th batch, train loss: 0.47044748067855835:  84%|████████▍ | 100/119 [00:13<00:02,  6.94it/s]Epoch: 2, train for the 101-th batch, train loss: 0.47044748067855835:  85%|████████▍ | 101/119 [00:13<00:02,  7.29it/s]Epoch: 1, train for the 207-th batch, train loss: 0.5644485354423523:  85%|█████████▍ | 206/241 [00:46<00:07,  4.69it/s]Epoch: 1, train for the 207-th batch, train loss: 0.5644485354423523:  86%|█████████▍ | 207/241 [00:46<00:06,  5.13it/s]evaluate for the 22-th batch, evaluate loss: 0.6705698370933533:  80%|██████████████▍   | 20/25 [00:01<00:00, 12.65it/s]evaluate for the 22-th batch, evaluate loss: 0.6705698370933533:  88%|███████████████▊  | 22/25 [00:01<00:00, 12.18it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5142436623573303:  42%|█████▍       | 61/146 [00:08<00:12,  6.76it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5142436623573303:  42%|█████▌       | 62/146 [00:08<00:12,  6.69it/s]Epoch: 1, train for the 174-th batch, train loss: 0.5495671033859253:  73%|████████   | 173/237 [00:46<00:16,  3.88it/s]Epoch: 2, train for the 102-th batch, train loss: 0.5058152079582214:  85%|█████████▎ | 101/119 [00:14<00:02,  7.29it/s]Epoch: 2, train for the 102-th batch, train loss: 0.5058152079582214:  86%|█████████▍ | 102/119 [00:14<00:02,  7.37it/s]evaluate for the 23-th batch, evaluate loss: 0.6687878966331482:  88%|███████████████▊  | 22/25 [00:01<00:00, 12.18it/s]Epoch: 1, train for the 169-th batch, train loss: 0.42022696137428284:  44%|████▍     | 168/383 [00:45<01:06,  3.22it/s]Epoch: 1, train for the 174-th batch, train loss: 0.5495671033859253:  73%|████████   | 174/237 [00:46<00:15,  4.07it/s]Epoch: 1, train for the 169-th batch, train loss: 0.42022696137428284:  44%|████▍     | 169/383 [00:45<01:02,  3.43it/s]evaluate for the 24-th batch, evaluate loss: 0.6858567595481873:  88%|███████████████▊  | 22/25 [00:01<00:00, 12.18it/s]evaluate for the 24-th batch, evaluate loss: 0.6858567595481873:  96%|█████████████████▎| 24/25 [00:01<00:00, 12.80it/s]Epoch: 2, train for the 63-th batch, train loss: 0.5557494759559631:  42%|█████▌       | 62/146 [00:08<00:12,  6.69it/s]Epoch: 2, train for the 63-th batch, train loss: 0.5557494759559631:  43%|█████▌       | 63/146 [00:08<00:11,  7.06it/s]Epoch: 2, train for the 103-th batch, train loss: 0.4944280982017517:  86%|█████████▍ | 102/119 [00:14<00:02,  7.37it/s]evaluate for the 25-th batch, evaluate loss: 0.6993444561958313:  96%|█████████████████▎| 24/25 [00:02<00:00, 12.80it/s]evaluate for the 25-th batch, evaluate loss: 0.6993444561958313: 100%|██████████████████| 25/25 [00:02<00:00, 12.38it/s]
Epoch: 2, train for the 103-th batch, train loss: 0.4944280982017517:  87%|█████████▌ | 103/119 [00:14<00:02,  7.49it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5549159646034241:  43%|█████▌       | 63/146 [00:08<00:11,  7.06it/s]Epoch: 1, train for the 208-th batch, train loss: 0.536864697933197:  86%|██████████▎ | 207/241 [00:46<00:06,  5.13it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5549159646034241:  44%|█████▋       | 64/146 [00:08<00:11,  7.45it/s]Epoch: 1, train for the 208-th batch, train loss: 0.536864697933197:  86%|██████████▎ | 208/241 [00:46<00:07,  4.31it/s]Epoch: 1, train for the 170-th batch, train loss: 0.354867160320282:  44%|█████▎      | 169/383 [00:45<01:02,  3.43it/s]Epoch: 2, train for the 104-th batch, train loss: 0.4750099778175354:  87%|█████████▌ | 103/119 [00:14<00:02,  7.49it/s]Epoch: 2, train for the 104-th batch, train loss: 0.4750099778175354:  87%|█████████▌ | 104/119 [00:14<00:02,  7.36it/s]Epoch: 1, train for the 175-th batch, train loss: 0.569813072681427:  73%|████████▊   | 174/237 [00:47<00:15,  4.07it/s]Epoch: 1, train for the 170-th batch, train loss: 0.354867160320282:  44%|█████▎      | 170/383 [00:45<00:58,  3.63it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5685595273971558:  44%|█████▋       | 64/146 [00:08<00:11,  7.45it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5685595273971558:  45%|█████▊       | 65/146 [00:08<00:10,  7.93it/s]Epoch: 1, train for the 175-th batch, train loss: 0.569813072681427:  74%|████████▊   | 175/237 [00:47<00:16,  3.78it/s]Epoch: 1, train for the 209-th batch, train loss: 0.5349070429801941:  86%|█████████▍ | 208/241 [00:47<00:07,  4.31it/s]Epoch: 1, train for the 209-th batch, train loss: 0.5349070429801941:  87%|█████████▌ | 209/241 [00:47<00:06,  4.95it/s]Epoch: 2, train for the 105-th batch, train loss: 0.46481016278266907:  87%|████████▋ | 104/119 [00:14<00:02,  7.36it/s]Epoch: 2, train for the 105-th batch, train loss: 0.46481016278266907:  88%|████████▊ | 105/119 [00:14<00:01,  7.20it/s]Epoch: 2, train for the 66-th batch, train loss: 0.5339938402175903:  45%|█████▊       | 65/146 [00:08<00:10,  7.93it/s]Epoch: 2, train for the 66-th batch, train loss: 0.5339938402175903:  45%|█████▉       | 66/146 [00:08<00:10,  7.57it/s]Epoch: 1, train for the 171-th batch, train loss: 0.4780336320400238:  44%|████▉      | 170/383 [00:45<00:58,  3.63it/s]INFO:root:Epoch: 1, learning rate: 0.0001, train loss: 0.6030
INFO:root:train average_precision, 0.7375
INFO:root:train roc_auc, 0.7153
INFO:root:validate loss: 0.5145
INFO:root:validate average_precision, 0.8396
INFO:root:validate roc_auc, 0.8294
INFO:root:new node validate loss: 0.7163
INFO:root:new node validate first_1_average_precision, 0.5398
INFO:root:new node validate first_1_roc_auc, 0.5172
INFO:root:new node validate first_3_average_precision, 0.5779
INFO:root:new node validate first_3_roc_auc, 0.5614
INFO:root:new node validate first_10_average_precision, 0.6325
INFO:root:new node validate first_10_roc_auc, 0.6231
INFO:root:new node validate average_precision, 0.6532
INFO:root:new node validate roc_auc, 0.6405
INFO:root:save model ./saved_models/TGN/ia-retweet-pol/TGN_seed0_dummy-o1xc9z6e/TGN_seed0_dummy-o1xc9z6e.pkl
Epoch: 1, train for the 171-th batch, train loss: 0.4780336320400238:  45%|████▉      | 171/383 [00:45<00:54,  3.92it/s]Epoch: 1, train for the 210-th batch, train loss: 0.46762797236442566:  87%|████████▋ | 209/241 [00:47<00:06,  4.95it/s]Epoch: 2, train for the 106-th batch, train loss: 0.4770564138889313:  88%|█████████▋ | 105/119 [00:14<00:01,  7.20it/s]Epoch: 2, train for the 106-th batch, train loss: 0.4770564138889313:  89%|█████████▊ | 106/119 [00:14<00:01,  7.33it/s]Epoch: 1, train for the 210-th batch, train loss: 0.46762797236442566:  87%|████████▋ | 210/241 [00:47<00:06,  5.02it/s]Epoch: 2, train for the 67-th batch, train loss: 0.5826697945594788:  45%|█████▉       | 66/146 [00:09<00:10,  7.57it/s]Epoch: 2, train for the 67-th batch, train loss: 0.5826697945594788:  46%|█████▉       | 67/146 [00:09<00:10,  7.52it/s]Epoch: 1, train for the 176-th batch, train loss: 0.5632839202880859:  74%|████████   | 175/237 [00:47<00:16,  3.78it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5076736807823181:  89%|█████████▊ | 106/119 [00:14<00:01,  7.33it/s]Epoch: 1, train for the 211-th batch, train loss: 0.3900584280490875:  87%|█████████▌ | 210/241 [00:47<00:06,  5.02it/s]Epoch: 1, train for the 176-th batch, train loss: 0.5632839202880859:  74%|████████▏  | 176/237 [00:47<00:17,  3.51it/s]Epoch: 1, train for the 172-th batch, train loss: 0.3464149832725525:  45%|████▉      | 171/383 [00:45<00:54,  3.92it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5076736807823181:  90%|█████████▉ | 107/119 [00:14<00:01,  7.51it/s]Epoch: 1, train for the 211-th batch, train loss: 0.3900584280490875:  88%|█████████▋ | 211/241 [00:47<00:05,  5.59it/s]Epoch: 1, train for the 172-th batch, train loss: 0.3464149832725525:  45%|████▉      | 172/383 [00:45<00:50,  4.18it/s]Epoch: 2, train for the 68-th batch, train loss: 0.533740758895874:  46%|██████▍       | 67/146 [00:09<00:10,  7.52it/s]Epoch: 2, train for the 68-th batch, train loss: 0.533740758895874:  47%|██████▌       | 68/146 [00:09<00:10,  7.20it/s]Epoch: 2, train for the 108-th batch, train loss: 0.40173789858818054:  90%|████████▉ | 107/119 [00:14<00:01,  7.51it/s]Epoch: 2, train for the 108-th batch, train loss: 0.40173789858818054:  91%|█████████ | 108/119 [00:14<00:01,  7.29it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5697896480560303:  47%|██████       | 68/146 [00:09<00:10,  7.20it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5697896480560303:  47%|██████▏      | 69/146 [00:09<00:10,  7.64it/s]Epoch: 1, train for the 173-th batch, train loss: 0.46764224767684937:  45%|████▍     | 172/383 [00:46<00:50,  4.18it/s]Epoch: 1, train for the 173-th batch, train loss: 0.46764224767684937:  45%|████▌     | 173/383 [00:46<00:47,  4.41it/s]Epoch: 2, train for the 109-th batch, train loss: 0.48677825927734375:  91%|█████████ | 108/119 [00:15<00:01,  7.29it/s]Epoch: 1, train for the 212-th batch, train loss: 0.5037074089050293:  88%|█████████▋ | 211/241 [00:47<00:05,  5.59it/s]Epoch: 2, train for the 109-th batch, train loss: 0.48677825927734375:  92%|█████████▏| 109/119 [00:15<00:01,  6.95it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5474132299423218:  47%|██████▏      | 69/146 [00:09<00:10,  7.64it/s]Epoch: 1, train for the 212-th batch, train loss: 0.5037074089050293:  88%|█████████▋ | 212/241 [00:47<00:06,  4.59it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5474132299423218:  48%|██████▏      | 70/146 [00:09<00:10,  7.49it/s]Epoch: 1, train for the 177-th batch, train loss: 0.5866276025772095:  74%|████████▏  | 176/237 [00:47<00:17,  3.51it/s]Epoch: 1, train for the 177-th batch, train loss: 0.5866276025772095:  75%|████████▏  | 177/237 [00:47<00:18,  3.23it/s]Epoch: 1, train for the 213-th batch, train loss: 0.5306148529052734:  88%|█████████▋ | 212/241 [00:47<00:06,  4.59it/s]Epoch: 1, train for the 174-th batch, train loss: 0.4953295886516571:  45%|████▉      | 173/383 [00:46<00:47,  4.41it/s]Epoch: 1, train for the 213-th batch, train loss: 0.5306148529052734:  88%|█████████▋ | 213/241 [00:47<00:05,  5.31it/s]Epoch: 2, train for the 110-th batch, train loss: 0.5343228578567505:  92%|██████████ | 109/119 [00:15<00:01,  6.95it/s]Epoch: 2, train for the 110-th batch, train loss: 0.5343228578567505:  92%|██████████▏| 110/119 [00:15<00:01,  6.77it/s]Epoch: 1, train for the 174-th batch, train loss: 0.4953295886516571:  45%|████▉      | 174/383 [00:46<00:47,  4.37it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5637586116790771:  48%|██████▏      | 70/146 [00:09<00:10,  7.49it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5637586116790771:  49%|██████▎      | 71/146 [00:09<00:10,  7.12it/s]Epoch: 1, train for the 178-th batch, train loss: 0.5789570212364197:  75%|████████▏  | 177/237 [00:47<00:18,  3.23it/s]Epoch: 1, train for the 214-th batch, train loss: 0.4890478551387787:  88%|█████████▋ | 213/241 [00:47<00:05,  5.31it/s]Epoch: 1, train for the 178-th batch, train loss: 0.5789570212364197:  75%|████████▎  | 178/237 [00:48<00:15,  3.69it/s]Epoch: 1, train for the 214-th batch, train loss: 0.4890478551387787:  89%|█████████▊ | 214/241 [00:47<00:04,  5.97it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5235039591789246:  92%|██████████▏| 110/119 [00:15<00:01,  6.77it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5235039591789246:  93%|██████████▎| 111/119 [00:15<00:01,  6.69it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5729657411575317:  49%|██████▎      | 71/146 [00:09<00:10,  7.12it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5729657411575317:  49%|██████▍      | 72/146 [00:09<00:10,  6.78it/s]Epoch: 1, train for the 215-th batch, train loss: 0.5102415084838867:  89%|█████████▊ | 214/241 [00:48<00:04,  5.97it/s]Epoch: 1, train for the 175-th batch, train loss: 0.414377361536026:  45%|█████▍      | 174/383 [00:46<00:47,  4.37it/s]Epoch: 1, train for the 215-th batch, train loss: 0.5102415084838867:  89%|█████████▊ | 215/241 [00:48<00:03,  6.54it/s]Epoch: 1, train for the 175-th batch, train loss: 0.414377361536026:  46%|█████▍      | 175/383 [00:46<00:47,  4.38it/s]Epoch: 2, train for the 112-th batch, train loss: 0.54405277967453:  93%|████████████▏| 111/119 [00:15<00:01,  6.69it/s]Epoch: 2, train for the 112-th batch, train loss: 0.54405277967453:  94%|████████████▏| 112/119 [00:15<00:01,  6.86it/s]Epoch: 1, train for the 216-th batch, train loss: 0.5855144262313843:  89%|█████████▊ | 215/241 [00:48<00:03,  6.54it/s]Epoch: 2, train for the 73-th batch, train loss: 0.5618500113487244:  49%|██████▍      | 72/146 [00:09<00:10,  6.78it/s]Epoch: 2, train for the 73-th batch, train loss: 0.5618500113487244:  50%|██████▌      | 73/146 [00:09<00:10,  6.81it/s]Epoch: 1, train for the 216-th batch, train loss: 0.5855144262313843:  90%|█████████▊ | 216/241 [00:48<00:03,  7.01it/s]Epoch: 2, train for the 113-th batch, train loss: 0.5043377876281738:  94%|██████████▎| 112/119 [00:15<00:01,  6.86it/s]Epoch: 2, train for the 113-th batch, train loss: 0.5043377876281738:  95%|██████████▍| 113/119 [00:15<00:00,  7.11it/s]Epoch: 1, train for the 217-th batch, train loss: 0.5069820880889893:  90%|█████████▊ | 216/241 [00:48<00:03,  7.01it/s]Epoch: 1, train for the 179-th batch, train loss: 0.5165451765060425:  75%|████████▎  | 178/237 [00:48<00:15,  3.69it/s]Epoch: 1, train for the 176-th batch, train loss: 0.4343128800392151:  46%|█████      | 175/383 [00:46<00:47,  4.38it/s]Epoch: 1, train for the 217-th batch, train loss: 0.5069820880889893:  90%|█████████▉ | 217/241 [00:48<00:03,  7.43it/s]Epoch: 2, train for the 74-th batch, train loss: 0.5334955453872681:  50%|██████▌      | 73/146 [00:10<00:10,  6.81it/s]Epoch: 2, train for the 74-th batch, train loss: 0.5334955453872681:  51%|██████▌      | 74/146 [00:10<00:10,  6.96it/s]Epoch: 1, train for the 179-th batch, train loss: 0.5165451765060425:  76%|████████▎  | 179/237 [00:48<00:17,  3.27it/s]Epoch: 1, train for the 176-th batch, train loss: 0.4343128800392151:  46%|█████      | 176/383 [00:46<00:49,  4.22it/s]Epoch: 2, train for the 114-th batch, train loss: 0.449602335691452:  95%|███████████▍| 113/119 [00:15<00:00,  7.11it/s]Epoch: 2, train for the 114-th batch, train loss: 0.449602335691452:  96%|███████████▍| 114/119 [00:15<00:00,  7.37it/s]Epoch: 1, train for the 218-th batch, train loss: 0.43540626764297485:  90%|█████████ | 217/241 [00:48<00:03,  7.43it/s]Epoch: 2, train for the 75-th batch, train loss: 0.5334137678146362:  51%|██████▌      | 74/146 [00:10<00:10,  6.96it/s]Epoch: 2, train for the 75-th batch, train loss: 0.5334137678146362:  51%|██████▋      | 75/146 [00:10<00:09,  7.24it/s]Epoch: 1, train for the 218-th batch, train loss: 0.43540626764297485:  90%|█████████ | 218/241 [00:48<00:03,  7.26it/s]Epoch: 1, train for the 180-th batch, train loss: 0.5839439034461975:  76%|████████▎  | 179/237 [00:48<00:17,  3.27it/s]Epoch: 2, train for the 115-th batch, train loss: 0.4461268484592438:  96%|██████████▌| 114/119 [00:15<00:00,  7.37it/s]Epoch: 2, train for the 115-th batch, train loss: 0.4461268484592438:  97%|██████████▋| 115/119 [00:15<00:00,  7.37it/s]Epoch: 1, train for the 180-th batch, train loss: 0.5839439034461975:  76%|████████▎  | 180/237 [00:48<00:15,  3.59it/s]Epoch: 1, train for the 177-th batch, train loss: 0.43046098947525024:  46%|████▌     | 176/383 [00:47<00:49,  4.22it/s]Epoch: 1, train for the 219-th batch, train loss: 0.4824831187725067:  90%|█████████▉ | 218/241 [00:48<00:03,  7.26it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5734553337097168:  51%|██████▋      | 75/146 [00:10<00:09,  7.24it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]Epoch: 1, train for the 177-th batch, train loss: 0.43046098947525024:  46%|████▌     | 177/383 [00:47<00:48,  4.23it/s]Epoch: 1, train for the 219-th batch, train loss: 0.4824831187725067:  91%|█████████▉ | 219/241 [00:48<00:02,  7.35it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5734553337097168:  52%|██████▊      | 76/146 [00:10<00:09,  7.16it/s]Epoch: 2, train for the 116-th batch, train loss: 0.4082227945327759:  97%|██████████▋| 115/119 [00:16<00:00,  7.37it/s]Epoch: 2, train for the 116-th batch, train loss: 0.4082227945327759:  97%|██████████▋| 116/119 [00:16<00:00,  7.63it/s]Epoch: 2, train for the 1-th batch, train loss: 0.7310431003570557:   0%|                       | 0/151 [00:00<?, ?it/s]Epoch: 2, train for the 1-th batch, train loss: 0.7310431003570557:   1%|               | 1/151 [00:00<00:15,  9.78it/s]Epoch: 2, train for the 77-th batch, train loss: 0.6152286529541016:  52%|██████▊      | 76/146 [00:10<00:09,  7.16it/s]Epoch: 2, train for the 77-th batch, train loss: 0.6152286529541016:  53%|██████▊      | 77/146 [00:10<00:09,  7.13it/s]Epoch: 1, train for the 181-th batch, train loss: 0.5455543994903564:  76%|████████▎  | 180/237 [00:48<00:15,  3.59it/s]Epoch: 1, train for the 220-th batch, train loss: 0.573455274105072:  91%|██████████▉ | 219/241 [00:48<00:02,  7.35it/s]Epoch: 2, train for the 2-th batch, train loss: 0.7095738053321838:   1%|               | 1/151 [00:00<00:15,  9.78it/s]Epoch: 1, train for the 181-th batch, train loss: 0.5455543994903564:  76%|████████▍  | 181/237 [00:48<00:14,  3.76it/s]Epoch: 2, train for the 117-th batch, train loss: 0.43467849493026733:  97%|█████████▋| 116/119 [00:16<00:00,  7.63it/s]Epoch: 1, train for the 178-th batch, train loss: 0.5184969305992126:  46%|█████      | 177/383 [00:47<00:48,  4.23it/s]Epoch: 2, train for the 117-th batch, train loss: 0.43467849493026733:  98%|█████████▊| 117/119 [00:16<00:00,  7.59it/s]Epoch: 1, train for the 220-th batch, train loss: 0.573455274105072:  91%|██████████▉ | 220/241 [00:48<00:03,  6.38it/s]Epoch: 1, train for the 178-th batch, train loss: 0.5184969305992126:  46%|█████      | 178/383 [00:47<00:49,  4.12it/s]Epoch: 2, train for the 78-th batch, train loss: 0.5656686425209045:  53%|██████▊      | 77/146 [00:10<00:09,  7.13it/s]Epoch: 2, train for the 78-th batch, train loss: 0.5656686425209045:  53%|██████▉      | 78/146 [00:10<00:09,  7.21it/s]Epoch: 2, train for the 3-th batch, train loss: 0.6928303241729736:   1%|               | 1/151 [00:00<00:15,  9.78it/s]Epoch: 2, train for the 3-th batch, train loss: 0.6928303241729736:   2%|▎              | 3/151 [00:00<00:15,  9.29it/s]Epoch: 2, train for the 118-th batch, train loss: 0.4459052085876465:  98%|██████████▊| 117/119 [00:16<00:00,  7.59it/s]Epoch: 2, train for the 118-th batch, train loss: 0.4459052085876465:  99%|██████████▉| 118/119 [00:16<00:00,  7.23it/s]Epoch: 1, train for the 221-th batch, train loss: 0.5583108067512512:  91%|██████████ | 220/241 [00:48<00:03,  6.38it/s]Epoch: 1, train for the 221-th batch, train loss: 0.5583108067512512:  92%|██████████ | 221/241 [00:49<00:03,  5.98it/s]Epoch: 2, train for the 4-th batch, train loss: 0.6715083122253418:   2%|▎              | 3/151 [00:00<00:15,  9.29it/s]Epoch: 1, train for the 182-th batch, train loss: 0.5860403180122375:  76%|████████▍  | 181/237 [00:49<00:14,  3.76it/s]Epoch: 2, train for the 4-th batch, train loss: 0.6715083122253418:   3%|▍              | 4/151 [00:00<00:15,  9.42it/s]Epoch: 2, train for the 79-th batch, train loss: 0.5895265936851501:  53%|██████▉      | 78/146 [00:10<00:09,  7.21it/s]Epoch: 2, train for the 79-th batch, train loss: 0.5895265936851501:  54%|███████      | 79/146 [00:10<00:09,  6.72it/s]Epoch: 1, train for the 182-th batch, train loss: 0.5860403180122375:  77%|████████▍  | 182/237 [00:49<00:14,  3.77it/s]Epoch: 1, train for the 179-th batch, train loss: 0.4268099069595337:  46%|█████      | 178/383 [00:47<00:49,  4.12it/s]Epoch: 2, train for the 119-th batch, train loss: 0.4257115125656128:  99%|██████████▉| 118/119 [00:16<00:00,  7.23it/s]Epoch: 2, train for the 119-th batch, train loss: 0.4257115125656128: 100%|███████████| 119/119 [00:16<00:00,  7.73it/s]Epoch: 2, train for the 119-th batch, train loss: 0.4257115125656128: 100%|███████████| 119/119 [00:16<00:00,  7.24it/s]
Epoch: 1, train for the 179-th batch, train loss: 0.4268099069595337:  47%|█████▏     | 179/383 [00:47<00:50,  4.07it/s]Epoch: 1, train for the 222-th batch, train loss: 0.494455486536026:  92%|███████████ | 221/241 [00:49<00:03,  5.98it/s]Epoch: 2, train for the 80-th batch, train loss: 0.5697998404502869:  54%|███████      | 79/146 [00:10<00:09,  6.72it/s]Epoch: 2, train for the 5-th batch, train loss: 0.6748997569084167:   3%|▍              | 4/151 [00:00<00:15,  9.42it/s]Epoch: 2, train for the 80-th batch, train loss: 0.5697998404502869:  55%|███████      | 80/146 [00:10<00:09,  7.31it/s]Epoch: 1, train for the 222-th batch, train loss: 0.494455486536026:  92%|███████████ | 222/241 [00:49<00:03,  5.97it/s]Epoch: 2, train for the 5-th batch, train loss: 0.6748997569084167:   3%|▍              | 5/151 [00:00<00:17,  8.37it/s]Epoch: 1, train for the 183-th batch, train loss: 0.5929039716720581:  77%|████████▍  | 182/237 [00:49<00:14,  3.77it/s]  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 1, train for the 183-th batch, train loss: 0.5929039716720581:  77%|████████▍  | 183/237 [00:49<00:13,  3.98it/s]Epoch: 2, train for the 81-th batch, train loss: 0.5345857739448547:  55%|███████      | 80/146 [00:11<00:09,  7.31it/s]Epoch: 2, train for the 81-th batch, train loss: 0.5345857739448547:  55%|███████▏     | 81/146 [00:11<00:08,  7.57it/s]evaluate for the 1-th batch, evaluate loss: 0.4876997172832489:   0%|                            | 0/40 [00:00<?, ?it/s]Epoch: 2, train for the 6-th batch, train loss: 0.7067857384681702:   3%|▍              | 5/151 [00:00<00:17,  8.37it/s]Epoch: 2, train for the 6-th batch, train loss: 0.7067857384681702:   4%|▌              | 6/151 [00:00<00:17,  8.14it/s]Epoch: 1, train for the 180-th batch, train loss: 0.5059813261032104:  47%|█████▏     | 179/383 [00:47<00:50,  4.07it/s]Epoch: 1, train for the 223-th batch, train loss: 0.5001270771026611:  92%|██████████▏| 222/241 [00:49<00:03,  5.97it/s]evaluate for the 2-th batch, evaluate loss: 0.5106490850448608:   0%|                            | 0/40 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5106490850448608:   5%|█                   | 2/40 [00:00<00:02, 18.81it/s]Epoch: 1, train for the 223-th batch, train loss: 0.5001270771026611:  93%|██████████▏| 223/241 [00:49<00:03,  5.59it/s]Epoch: 1, train for the 180-th batch, train loss: 0.5059813261032104:  47%|█████▏     | 180/383 [00:47<00:51,  3.98it/s]Epoch: 2, train for the 82-th batch, train loss: 0.544971227645874:  55%|███████▊      | 81/146 [00:11<00:08,  7.57it/s]evaluate for the 3-th batch, evaluate loss: 0.5074435472488403:   5%|█                   | 2/40 [00:00<00:02, 18.81it/s]Epoch: 2, train for the 82-th batch, train loss: 0.544971227645874:  56%|███████▊      | 82/146 [00:11<00:08,  7.40it/s]Epoch: 2, train for the 7-th batch, train loss: 0.6877369284629822:   4%|▌              | 6/151 [00:00<00:17,  8.14it/s]Epoch: 2, train for the 7-th batch, train loss: 0.6877369284629822:   5%|▋              | 7/151 [00:00<00:18,  7.62it/s]Epoch: 1, train for the 184-th batch, train loss: 0.5617883801460266:  77%|████████▍  | 183/237 [00:49<00:13,  3.98it/s]evaluate for the 4-th batch, evaluate loss: 0.5747424364089966:   5%|█                   | 2/40 [00:00<00:02, 18.81it/s]evaluate for the 4-th batch, evaluate loss: 0.5747424364089966:  10%|██                  | 4/40 [00:00<00:02, 14.27it/s]Epoch: 1, train for the 184-th batch, train loss: 0.5617883801460266:  78%|████████▌  | 184/237 [00:49<00:13,  4.06it/s]Epoch: 1, train for the 224-th batch, train loss: 0.5105549097061157:  93%|██████████▏| 223/241 [00:49<00:03,  5.59it/s]Epoch: 1, train for the 224-th batch, train loss: 0.5105549097061157:  93%|██████████▏| 224/241 [00:49<00:02,  5.69it/s]Epoch: 2, train for the 8-th batch, train loss: 0.6962659358978271:   5%|▋              | 7/151 [00:00<00:18,  7.62it/s]Epoch: 2, train for the 8-th batch, train loss: 0.6962659358978271:   5%|▊              | 8/151 [00:00<00:18,  7.53it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5526943802833557:  56%|███████▎     | 82/146 [00:11<00:08,  7.40it/s]evaluate for the 5-th batch, evaluate loss: 0.5372250080108643:  10%|██                  | 4/40 [00:00<00:02, 14.27it/s]Epoch: 1, train for the 181-th batch, train loss: 0.416147917509079:  47%|█████▋      | 180/383 [00:48<00:51,  3.98it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5526943802833557:  57%|███████▍     | 83/146 [00:11<00:09,  6.75it/s]Epoch: 1, train for the 181-th batch, train loss: 0.416147917509079:  47%|█████▋      | 181/383 [00:48<00:52,  3.87it/s]evaluate for the 6-th batch, evaluate loss: 0.5203524827957153:  10%|██                  | 4/40 [00:00<00:02, 14.27it/s]evaluate for the 6-th batch, evaluate loss: 0.5203524827957153:  15%|███                 | 6/40 [00:00<00:02, 13.70it/s]Epoch: 1, train for the 185-th batch, train loss: 0.5545803308486938:  78%|████████▌  | 184/237 [00:49<00:13,  4.06it/s]Epoch: 1, train for the 225-th batch, train loss: 0.5413540601730347:  93%|██████████▏| 224/241 [00:49<00:02,  5.69it/s]Epoch: 1, train for the 225-th batch, train loss: 0.5413540601730347:  93%|██████████▎| 225/241 [00:49<00:02,  5.80it/s]Epoch: 1, train for the 185-th batch, train loss: 0.5545803308486938:  78%|████████▌  | 185/237 [00:49<00:12,  4.29it/s]Epoch: 2, train for the 9-th batch, train loss: 0.7094062566757202:   5%|▊              | 8/151 [00:01<00:18,  7.53it/s]Epoch: 2, train for the 9-th batch, train loss: 0.7094062566757202:   6%|▉              | 9/151 [00:01<00:19,  7.47it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5313555598258972:  57%|███████▍     | 83/146 [00:11<00:09,  6.75it/s]evaluate for the 7-th batch, evaluate loss: 0.5254155397415161:  15%|███                 | 6/40 [00:00<00:02, 13.70it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5313555598258972:  58%|███████▍     | 84/146 [00:11<00:09,  6.62it/s]evaluate for the 8-th batch, evaluate loss: 0.5003117918968201:  15%|███                 | 6/40 [00:00<00:02, 13.70it/s]evaluate for the 8-th batch, evaluate loss: 0.5003117918968201:  20%|████                | 8/40 [00:00<00:02, 13.61it/s]Epoch: 2, train for the 10-th batch, train loss: 0.6708323359489441:   6%|▊             | 9/151 [00:01<00:19,  7.47it/s]Epoch: 2, train for the 10-th batch, train loss: 0.6708323359489441:   7%|▊            | 10/151 [00:01<00:19,  7.09it/s]Epoch: 1, train for the 186-th batch, train loss: 0.5271430611610413:  78%|████████▌  | 185/237 [00:49<00:12,  4.29it/s]evaluate for the 9-th batch, evaluate loss: 0.5290707945823669:  20%|████                | 8/40 [00:00<00:02, 13.61it/s]Epoch: 1, train for the 226-th batch, train loss: 0.5487245917320251:  93%|██████████▎| 225/241 [00:49<00:02,  5.80it/s]Epoch: 2, train for the 85-th batch, train loss: 0.5232264399528503:  58%|███████▍     | 84/146 [00:11<00:09,  6.62it/s]Epoch: 2, train for the 85-th batch, train loss: 0.5232264399528503:  58%|███████▌     | 85/146 [00:11<00:09,  6.70it/s]Epoch: 1, train for the 186-th batch, train loss: 0.5271430611610413:  78%|████████▋  | 186/237 [00:49<00:11,  4.45it/s]Epoch: 1, train for the 226-th batch, train loss: 0.5487245917320251:  94%|██████████▎| 226/241 [00:49<00:02,  5.40it/s]evaluate for the 10-th batch, evaluate loss: 0.5689586400985718:  20%|███▊               | 8/40 [00:00<00:02, 13.61it/s]evaluate for the 10-th batch, evaluate loss: 0.5689586400985718:  25%|████▌             | 10/40 [00:00<00:02, 14.19it/s]Epoch: 1, train for the 182-th batch, train loss: 0.5250232815742493:  47%|█████▏     | 181/383 [00:48<00:52,  3.87it/s]Epoch: 2, train for the 11-th batch, train loss: 0.6645218729972839:   7%|▊            | 10/151 [00:01<00:19,  7.09it/s]Epoch: 2, train for the 11-th batch, train loss: 0.6645218729972839:   7%|▉            | 11/151 [00:01<00:18,  7.38it/s]Epoch: 1, train for the 182-th batch, train loss: 0.5250232815742493:  48%|█████▏     | 182/383 [00:48<00:59,  3.36it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5573253035545349:  58%|███████▌     | 85/146 [00:11<00:09,  6.70it/s]evaluate for the 11-th batch, evaluate loss: 0.5201146602630615:  25%|████▌             | 10/40 [00:00<00:02, 14.19it/s]Epoch: 1, train for the 227-th batch, train loss: 0.5783560276031494:  94%|██████████▎| 226/241 [00:50<00:02,  5.40it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5573253035545349:  59%|███████▋     | 86/146 [00:11<00:09,  6.52it/s]Epoch: 1, train for the 227-th batch, train loss: 0.5783560276031494:  94%|██████████▎| 227/241 [00:50<00:02,  5.55it/s]Epoch: 1, train for the 187-th batch, train loss: 0.5708672404289246:  78%|████████▋  | 186/237 [00:50<00:11,  4.45it/s]Epoch: 2, train for the 12-th batch, train loss: 0.6842121481895447:   7%|▉            | 11/151 [00:01<00:18,  7.38it/s]evaluate for the 12-th batch, evaluate loss: 0.5345136523246765:  25%|████▌             | 10/40 [00:00<00:02, 14.19it/s]evaluate for the 12-th batch, evaluate loss: 0.5345136523246765:  30%|█████▍            | 12/40 [00:00<00:02, 13.21it/s]Epoch: 2, train for the 12-th batch, train loss: 0.6842121481895447:   8%|█            | 12/151 [00:01<00:18,  7.71it/s]Epoch: 1, train for the 187-th batch, train loss: 0.5708672404289246:  79%|████████▋  | 187/237 [00:50<00:11,  4.40it/s]evaluate for the 13-th batch, evaluate loss: 0.5287290215492249:  30%|█████▍            | 12/40 [00:00<00:02, 13.21it/s]Epoch: 2, train for the 87-th batch, train loss: 0.54954594373703:  59%|████████▊      | 86/146 [00:11<00:09,  6.52it/s]Epoch: 2, train for the 87-th batch, train loss: 0.54954594373703:  60%|████████▉      | 87/146 [00:11<00:09,  6.54it/s]evaluate for the 14-th batch, evaluate loss: 0.5144273042678833:  30%|█████▍            | 12/40 [00:01<00:02, 13.21it/s]evaluate for the 14-th batch, evaluate loss: 0.5144273042678833:  35%|██████▎           | 14/40 [00:01<00:01, 13.40it/s]Epoch: 1, train for the 183-th batch, train loss: 0.4140101969242096:  48%|█████▏     | 182/383 [00:48<00:59,  3.36it/s]Epoch: 1, train for the 228-th batch, train loss: 0.5276396870613098:  94%|██████████▎| 227/241 [00:50<00:02,  5.55it/s]Epoch: 2, train for the 13-th batch, train loss: 0.6832220554351807:   8%|█            | 12/151 [00:01<00:18,  7.71it/s]Epoch: 2, train for the 13-th batch, train loss: 0.6832220554351807:   9%|█            | 13/151 [00:01<00:20,  6.84it/s]Epoch: 1, train for the 228-th batch, train loss: 0.5276396870613098:  95%|██████████▍| 228/241 [00:50<00:02,  5.22it/s]Epoch: 1, train for the 183-th batch, train loss: 0.4140101969242096:  48%|█████▎     | 183/383 [00:48<00:58,  3.44it/s]Epoch: 1, train for the 188-th batch, train loss: 0.5965394973754883:  79%|████████▋  | 187/237 [00:50<00:11,  4.40it/s]evaluate for the 15-th batch, evaluate loss: 0.533208966255188:  35%|██████▋            | 14/40 [00:01<00:01, 13.40it/s]Epoch: 1, train for the 188-th batch, train loss: 0.5965394973754883:  79%|████████▋  | 188/237 [00:50<00:10,  4.47it/s]Epoch: 2, train for the 88-th batch, train loss: 0.570324718952179:  60%|████████▎     | 87/146 [00:12<00:09,  6.54it/s]Epoch: 2, train for the 88-th batch, train loss: 0.570324718952179:  60%|████████▍     | 88/146 [00:12<00:08,  6.45it/s]evaluate for the 16-th batch, evaluate loss: 0.5148674249649048:  35%|██████▎           | 14/40 [00:01<00:01, 13.40it/s]evaluate for the 16-th batch, evaluate loss: 0.5148674249649048:  40%|███████▏          | 16/40 [00:01<00:01, 13.99it/s]Epoch: 2, train for the 14-th batch, train loss: 0.6921051144599915:   9%|█            | 13/151 [00:01<00:20,  6.84it/s]Epoch: 2, train for the 14-th batch, train loss: 0.6921051144599915:   9%|█▏           | 14/151 [00:01<00:20,  6.82it/s]evaluate for the 17-th batch, evaluate loss: 0.5489605665206909:  40%|███████▏          | 16/40 [00:01<00:01, 13.99it/s]Epoch: 1, train for the 229-th batch, train loss: 0.5334540605545044:  95%|██████████▍| 228/241 [00:50<00:02,  5.22it/s]Epoch: 1, train for the 229-th batch, train loss: 0.5334540605545044:  95%|██████████▍| 229/241 [00:50<00:02,  5.39it/s]Epoch: 1, train for the 184-th batch, train loss: 0.42520540952682495:  48%|████▊     | 183/383 [00:49<00:58,  3.44it/s]Epoch: 2, train for the 89-th batch, train loss: 0.511876106262207:  60%|████████▍     | 88/146 [00:12<00:08,  6.45it/s]evaluate for the 18-th batch, evaluate loss: 0.5389527678489685:  40%|███████▏          | 16/40 [00:01<00:01, 13.99it/s]evaluate for the 18-th batch, evaluate loss: 0.5389527678489685:  45%|████████          | 18/40 [00:01<00:01, 14.04it/s]Epoch: 2, train for the 89-th batch, train loss: 0.511876106262207:  61%|████████▌     | 89/146 [00:12<00:08,  6.54it/s]Epoch: 1, train for the 184-th batch, train loss: 0.42520540952682495:  48%|████▊     | 184/383 [00:49<00:53,  3.70it/s]Epoch: 1, train for the 189-th batch, train loss: 0.5929443836212158:  79%|████████▋  | 188/237 [00:50<00:10,  4.47it/s]Epoch: 2, train for the 15-th batch, train loss: 0.7105401158332825:   9%|█▏           | 14/151 [00:01<00:20,  6.82it/s]Epoch: 2, train for the 15-th batch, train loss: 0.7105401158332825:  10%|█▎           | 15/151 [00:01<00:18,  7.22it/s]Epoch: 1, train for the 189-th batch, train loss: 0.5929443836212158:  80%|████████▊  | 189/237 [00:50<00:10,  4.51it/s]evaluate for the 19-th batch, evaluate loss: 0.5288803577423096:  45%|████████          | 18/40 [00:01<00:01, 14.04it/s]Epoch: 1, train for the 230-th batch, train loss: 0.49976491928100586:  95%|█████████▌| 229/241 [00:50<00:02,  5.39it/s]Epoch: 1, train for the 230-th batch, train loss: 0.49976491928100586:  95%|█████████▌| 230/241 [00:50<00:01,  5.55it/s]Epoch: 2, train for the 90-th batch, train loss: 0.5383849143981934:  61%|███████▉     | 89/146 [00:12<00:08,  6.54it/s]evaluate for the 20-th batch, evaluate loss: 0.518967866897583:  45%|████████▌          | 18/40 [00:01<00:01, 14.04it/s]evaluate for the 20-th batch, evaluate loss: 0.518967866897583:  50%|█████████▌         | 20/40 [00:01<00:01, 13.43it/s]Epoch: 2, train for the 90-th batch, train loss: 0.5383849143981934:  62%|████████     | 90/146 [00:12<00:08,  6.33it/s]Epoch: 2, train for the 16-th batch, train loss: 0.6836891174316406:  10%|█▎           | 15/151 [00:02<00:18,  7.22it/s]Epoch: 2, train for the 16-th batch, train loss: 0.6836891174316406:  11%|█▍           | 16/151 [00:02<00:19,  6.94it/s]evaluate for the 21-th batch, evaluate loss: 0.5405011773109436:  50%|█████████         | 20/40 [00:01<00:01, 13.43it/s]Epoch: 1, train for the 185-th batch, train loss: 0.31399616599082947:  48%|████▊     | 184/383 [00:49<00:53,  3.70it/s]Epoch: 1, train for the 190-th batch, train loss: 0.5351415872573853:  80%|████████▊  | 189/237 [00:50<00:10,  4.51it/s]Epoch: 1, train for the 185-th batch, train loss: 0.31399616599082947:  48%|████▊     | 185/383 [00:49<00:54,  3.64it/s]Epoch: 2, train for the 17-th batch, train loss: 0.686825692653656:  11%|█▍            | 16/151 [00:02<00:19,  6.94it/s]Epoch: 2, train for the 17-th batch, train loss: 0.686825692653656:  11%|█▌            | 17/151 [00:02<00:17,  7.60it/s]Epoch: 1, train for the 190-th batch, train loss: 0.5351415872573853:  80%|████████▊  | 190/237 [00:50<00:10,  4.31it/s]evaluate for the 22-th batch, evaluate loss: 0.5020196437835693:  50%|█████████         | 20/40 [00:01<00:01, 13.43it/s]evaluate for the 22-th batch, evaluate loss: 0.5020196437835693:  55%|█████████▉        | 22/40 [00:01<00:01, 13.12it/s]Epoch: 2, train for the 91-th batch, train loss: 0.5555600523948669:  62%|████████     | 90/146 [00:12<00:08,  6.33it/s]Epoch: 2, train for the 91-th batch, train loss: 0.5555600523948669:  62%|████████     | 91/146 [00:12<00:08,  6.25it/s]Epoch: 1, train for the 231-th batch, train loss: 0.5492992401123047:  95%|██████████▍| 230/241 [00:50<00:01,  5.55it/s]evaluate for the 23-th batch, evaluate loss: 0.4638969302177429:  55%|█████████▉        | 22/40 [00:01<00:01, 13.12it/s]Epoch: 1, train for the 231-th batch, train loss: 0.5492992401123047:  96%|██████████▌| 231/241 [00:50<00:02,  4.71it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5208290815353394:  62%|████████     | 91/146 [00:12<00:08,  6.25it/s]evaluate for the 24-th batch, evaluate loss: 0.512791097164154:  55%|██████████▍        | 22/40 [00:01<00:01, 13.12it/s]evaluate for the 24-th batch, evaluate loss: 0.512791097164154:  60%|███████████▍       | 24/40 [00:01<00:01, 12.86it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5208290815353394:  63%|████████▏    | 92/146 [00:12<00:08,  6.31it/s]Epoch: 1, train for the 191-th batch, train loss: 0.5496093034744263:  80%|████████▊  | 190/237 [00:51<00:10,  4.31it/s]Epoch: 1, train for the 232-th batch, train loss: 0.5672383904457092:  96%|██████████▌| 231/241 [00:51<00:02,  4.71it/s]Epoch: 1, train for the 191-th batch, train loss: 0.5496093034744263:  81%|████████▊  | 191/237 [00:51<00:10,  4.39it/s]Epoch: 2, train for the 18-th batch, train loss: 0.6801897287368774:  11%|█▍           | 17/151 [00:02<00:17,  7.60it/s]Epoch: 1, train for the 232-th batch, train loss: 0.5672383904457092:  96%|██████████▌| 232/241 [00:51<00:01,  5.26it/s]evaluate for the 25-th batch, evaluate loss: 0.5716477632522583:  60%|██████████▊       | 24/40 [00:01<00:01, 12.86it/s]Epoch: 2, train for the 18-th batch, train loss: 0.6801897287368774:  12%|█▌           | 18/151 [00:02<00:22,  5.94it/s]Epoch: 2, train for the 93-th batch, train loss: 0.5506223440170288:  63%|████████▏    | 92/146 [00:12<00:08,  6.31it/s]Epoch: 1, train for the 186-th batch, train loss: 0.40871796011924744:  48%|████▊     | 185/383 [00:49<00:54,  3.64it/s]evaluate for the 26-th batch, evaluate loss: 0.48242372274398804:  60%|██████████▏      | 24/40 [00:01<00:01, 12.86it/s]evaluate for the 26-th batch, evaluate loss: 0.48242372274398804:  65%|███████████      | 26/40 [00:01<00:01, 13.30it/s]Epoch: 2, train for the 93-th batch, train loss: 0.5506223440170288:  64%|████████▎    | 93/146 [00:12<00:08,  6.56it/s]Epoch: 1, train for the 186-th batch, train loss: 0.40871796011924744:  49%|████▊     | 186/383 [00:49<01:00,  3.24it/s]evaluate for the 27-th batch, evaluate loss: 0.5006499290466309:  65%|███████████▋      | 26/40 [00:01<00:01, 13.30it/s]Epoch: 1, train for the 192-th batch, train loss: 0.5771214365959167:  81%|████████▊  | 191/237 [00:51<00:10,  4.39it/s]Epoch: 1, train for the 233-th batch, train loss: 0.5141889452934265:  96%|██████████▌| 232/241 [00:51<00:01,  5.26it/s]Epoch: 2, train for the 19-th batch, train loss: 0.6794077754020691:  12%|█▌           | 18/151 [00:02<00:22,  5.94it/s]Epoch: 2, train for the 19-th batch, train loss: 0.6794077754020691:  13%|█▋           | 19/151 [00:02<00:23,  5.73it/s]Epoch: 1, train for the 192-th batch, train loss: 0.5771214365959167:  81%|████████▉  | 192/237 [00:51<00:10,  4.46it/s]Epoch: 1, train for the 233-th batch, train loss: 0.5141889452934265:  97%|██████████▋| 233/241 [00:51<00:01,  5.09it/s]Epoch: 2, train for the 94-th batch, train loss: 0.5234575271606445:  64%|████████▎    | 93/146 [00:13<00:08,  6.56it/s]evaluate for the 28-th batch, evaluate loss: 0.4847092032432556:  65%|███████████▋      | 26/40 [00:02<00:01, 13.30it/s]evaluate for the 28-th batch, evaluate loss: 0.4847092032432556:  70%|████████████▌     | 28/40 [00:02<00:00, 13.10it/s]Epoch: 2, train for the 94-th batch, train loss: 0.5234575271606445:  64%|████████▎    | 94/146 [00:13<00:07,  6.50it/s]evaluate for the 29-th batch, evaluate loss: 0.5086449384689331:  70%|████████████▌     | 28/40 [00:02<00:00, 13.10it/s]Epoch: 1, train for the 187-th batch, train loss: 0.35348930954933167:  49%|████▊     | 186/383 [00:49<01:00,  3.24it/s]Epoch: 2, train for the 20-th batch, train loss: 0.6607031226158142:  13%|█▋           | 19/151 [00:02<00:23,  5.73it/s]Epoch: 2, train for the 20-th batch, train loss: 0.6607031226158142:  13%|█▋           | 20/151 [00:02<00:22,  5.89it/s]Epoch: 1, train for the 187-th batch, train loss: 0.35348930954933167:  49%|████▉     | 187/383 [00:49<00:55,  3.53it/s]evaluate for the 30-th batch, evaluate loss: 0.5119838714599609:  70%|████████████▌     | 28/40 [00:02<00:00, 13.10it/s]evaluate for the 30-th batch, evaluate loss: 0.5119838714599609:  75%|█████████████▌    | 30/40 [00:02<00:00, 13.69it/s]Epoch: 1, train for the 193-th batch, train loss: 0.5822269320487976:  81%|████████▉  | 192/237 [00:51<00:10,  4.46it/s]Epoch: 1, train for the 234-th batch, train loss: 0.577099621295929:  97%|███████████▌| 233/241 [00:51<00:01,  5.09it/s]Epoch: 2, train for the 95-th batch, train loss: 0.519199788570404:  64%|█████████     | 94/146 [00:13<00:07,  6.50it/s]Epoch: 2, train for the 95-th batch, train loss: 0.519199788570404:  65%|█████████     | 95/146 [00:13<00:08,  6.30it/s]evaluate for the 31-th batch, evaluate loss: 0.4924524128437042:  75%|█████████████▌    | 30/40 [00:02<00:00, 13.69it/s]Epoch: 1, train for the 193-th batch, train loss: 0.5822269320487976:  81%|████████▉  | 193/237 [00:51<00:09,  4.44it/s]Epoch: 1, train for the 234-th batch, train loss: 0.577099621295929:  97%|███████████▋| 234/241 [00:51<00:01,  4.81it/s]evaluate for the 32-th batch, evaluate loss: 0.523730456829071:  75%|██████████████▎    | 30/40 [00:02<00:00, 13.69it/s]evaluate for the 32-th batch, evaluate loss: 0.523730456829071:  80%|███████████████▏   | 32/40 [00:02<00:00, 14.50it/s]Epoch: 2, train for the 21-th batch, train loss: 0.6681881546974182:  13%|█▋           | 20/151 [00:02<00:22,  5.89it/s]Epoch: 2, train for the 21-th batch, train loss: 0.6681881546974182:  14%|█▊           | 21/151 [00:02<00:21,  6.04it/s]Epoch: 2, train for the 96-th batch, train loss: 0.5578497052192688:  65%|████████▍    | 95/146 [00:13<00:08,  6.30it/s]Epoch: 2, train for the 96-th batch, train loss: 0.5578497052192688:  66%|████████▌    | 96/146 [00:13<00:07,  6.52it/s]evaluate for the 33-th batch, evaluate loss: 0.5036048889160156:  80%|██████████████▍   | 32/40 [00:02<00:00, 14.50it/s]Epoch: 1, train for the 188-th batch, train loss: 0.4863692820072174:  49%|█████▎     | 187/383 [00:50<00:55,  3.53it/s]Epoch: 1, train for the 188-th batch, train loss: 0.4863692820072174:  49%|█████▍     | 188/383 [00:50<00:53,  3.65it/s]Epoch: 1, train for the 194-th batch, train loss: 0.5546745657920837:  81%|████████▉  | 193/237 [00:51<00:09,  4.44it/s]evaluate for the 34-th batch, evaluate loss: 0.5721049904823303:  80%|██████████████▍   | 32/40 [00:02<00:00, 14.50it/s]evaluate for the 34-th batch, evaluate loss: 0.5721049904823303:  85%|███████████████▎  | 34/40 [00:02<00:00, 14.92it/s]Epoch: 1, train for the 235-th batch, train loss: 0.6072200536727905:  97%|██████████▋| 234/241 [00:51<00:01,  4.81it/s]Epoch: 1, train for the 194-th batch, train loss: 0.5546745657920837:  82%|█████████  | 194/237 [00:51<00:09,  4.49it/s]Epoch: 1, train for the 235-th batch, train loss: 0.6072200536727905:  98%|██████████▋| 235/241 [00:51<00:01,  4.81it/s]Epoch: 2, train for the 22-th batch, train loss: 0.642177939414978:  14%|█▉            | 21/151 [00:03<00:21,  6.04it/s]evaluate for the 35-th batch, evaluate loss: 0.5343067049980164:  85%|███████████████▎  | 34/40 [00:02<00:00, 14.92it/s]Epoch: 2, train for the 22-th batch, train loss: 0.642177939414978:  15%|██            | 22/151 [00:03<00:20,  6.19it/s]evaluate for the 36-th batch, evaluate loss: 0.5230780839920044:  85%|███████████████▎  | 34/40 [00:02<00:00, 14.92it/s]evaluate for the 36-th batch, evaluate loss: 0.5230780839920044:  90%|████████████████▏ | 36/40 [00:02<00:00, 15.04it/s]Epoch: 1, train for the 189-th batch, train loss: 0.4685613214969635:  49%|█████▍     | 188/383 [00:50<00:53,  3.65it/s]evaluate for the 37-th batch, evaluate loss: 0.5517300963401794:  90%|████████████████▏ | 36/40 [00:02<00:00, 15.04it/s]Epoch: 2, train for the 97-th batch, train loss: 0.5160313844680786:  66%|████████▌    | 96/146 [00:13<00:07,  6.52it/s]Epoch: 2, train for the 23-th batch, train loss: 0.6532241702079773:  15%|█▉           | 22/151 [00:03<00:20,  6.19it/s]Epoch: 1, train for the 236-th batch, train loss: 0.5613763928413391:  98%|██████████▋| 235/241 [00:51<00:01,  4.81it/s]Epoch: 2, train for the 97-th batch, train loss: 0.5160313844680786:  66%|████████▋    | 97/146 [00:13<00:09,  5.13it/s]Epoch: 2, train for the 23-th batch, train loss: 0.6532241702079773:  15%|█▉           | 23/151 [00:03<00:21,  5.92it/s]Epoch: 1, train for the 236-th batch, train loss: 0.5613763928413391:  98%|██████████▊| 236/241 [00:51<00:01,  4.77it/s]Epoch: 1, train for the 195-th batch, train loss: 0.5538004040718079:  82%|█████████  | 194/237 [00:51<00:09,  4.49it/s]Epoch: 1, train for the 189-th batch, train loss: 0.4685613214969635:  49%|█████▍     | 189/383 [00:50<00:53,  3.64it/s]evaluate for the 38-th batch, evaluate loss: 0.5832729935646057:  90%|████████████████▏ | 36/40 [00:02<00:00, 15.04it/s]evaluate for the 38-th batch, evaluate loss: 0.5832729935646057:  95%|█████████████████ | 38/40 [00:02<00:00, 14.25it/s]Epoch: 1, train for the 195-th batch, train loss: 0.5538004040718079:  82%|█████████  | 195/237 [00:52<00:09,  4.24it/s]Epoch: 2, train for the 98-th batch, train loss: 0.5058899521827698:  66%|████████▋    | 97/146 [00:13<00:09,  5.13it/s]evaluate for the 39-th batch, evaluate loss: 0.5696980953216553:  95%|█████████████████ | 38/40 [00:02<00:00, 14.25it/s]Epoch: 2, train for the 98-th batch, train loss: 0.5058899521827698:  67%|████████▋    | 98/146 [00:13<00:08,  5.76it/s]Epoch: 2, train for the 24-th batch, train loss: 0.646979033946991:  15%|██▏           | 23/151 [00:03<00:21,  5.92it/s]Epoch: 2, train for the 24-th batch, train loss: 0.646979033946991:  16%|██▏           | 24/151 [00:03<00:21,  5.99it/s]Epoch: 1, train for the 237-th batch, train loss: 0.5224588513374329:  98%|██████████▊| 236/241 [00:52<00:01,  4.77it/s]evaluate for the 40-th batch, evaluate loss: 0.6687790155410767:  95%|█████████████████ | 38/40 [00:02<00:00, 14.25it/s]evaluate for the 40-th batch, evaluate loss: 0.6687790155410767: 100%|██████████████████| 40/40 [00:02<00:00, 14.06it/s]evaluate for the 40-th batch, evaluate loss: 0.6687790155410767: 100%|██████████████████| 40/40 [00:02<00:00, 13.88it/s]
Epoch: 1, train for the 237-th batch, train loss: 0.5224588513374329:  98%|██████████▊| 237/241 [00:52<00:00,  4.94it/s]Epoch: 2, train for the 25-th batch, train loss: 0.6752147674560547:  16%|██           | 24/151 [00:03<00:21,  5.99it/s]Epoch: 2, train for the 25-th batch, train loss: 0.6752147674560547:  17%|██▏          | 25/151 [00:03<00:19,  6.30it/s]  0%|                                                                                            | 0/21 [00:00<?, ?it/s]Epoch: 2, train for the 99-th batch, train loss: 0.5563065409660339:  67%|████████▋    | 98/146 [00:14<00:08,  5.76it/s]Epoch: 1, train for the 196-th batch, train loss: 0.5610881447792053:  82%|█████████  | 195/237 [00:52<00:09,  4.24it/s]Epoch: 1, train for the 190-th batch, train loss: 0.5035965442657471:  49%|█████▍     | 189/383 [00:50<00:53,  3.64it/s]Epoch: 1, train for the 238-th batch, train loss: 0.537007212638855:  98%|███████████▊| 237/241 [00:52<00:00,  4.94it/s]Epoch: 2, train for the 99-th batch, train loss: 0.5563065409660339:  68%|████████▊    | 99/146 [00:14<00:08,  5.37it/s]Epoch: 1, train for the 238-th batch, train loss: 0.537007212638855:  99%|███████████▊| 238/241 [00:52<00:00,  5.30it/s]Epoch: 1, train for the 196-th batch, train loss: 0.5610881447792053:  83%|█████████  | 196/237 [00:52<00:10,  3.85it/s]Epoch: 1, train for the 190-th batch, train loss: 0.5035965442657471:  50%|█████▍     | 190/383 [00:50<00:57,  3.38it/s]evaluate for the 1-th batch, evaluate loss: 0.6462143063545227:   0%|                            | 0/21 [00:00<?, ?it/s]Epoch: 2, train for the 26-th batch, train loss: 0.6558927893638611:  17%|██▏          | 25/151 [00:03<00:19,  6.30it/s]Epoch: 2, train for the 26-th batch, train loss: 0.6558927893638611:  17%|██▏          | 26/151 [00:03<00:19,  6.39it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5803607702255249:  68%|████████▏   | 99/146 [00:14<00:08,  5.37it/s]evaluate for the 2-th batch, evaluate loss: 0.7224066257476807:   0%|                            | 0/21 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.7224066257476807:  10%|█▉                  | 2/21 [00:00<00:01, 12.75it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5803607702255249:  68%|███████▌   | 100/146 [00:14<00:08,  5.74it/s]Epoch: 1, train for the 239-th batch, train loss: 0.5802589058876038:  99%|██████████▊| 238/241 [00:52<00:00,  5.30it/s]Epoch: 1, train for the 239-th batch, train loss: 0.5802589058876038:  99%|██████████▉| 239/241 [00:52<00:00,  5.41it/s]evaluate for the 3-th batch, evaluate loss: 0.707548201084137:  10%|██                   | 2/21 [00:00<00:01, 12.75it/s]Epoch: 1, train for the 197-th batch, train loss: 0.5298115611076355:  83%|█████████  | 196/237 [00:52<00:10,  3.85it/s]Epoch: 2, train for the 27-th batch, train loss: 0.6702093482017517:  17%|██▏          | 26/151 [00:03<00:19,  6.39it/s]Epoch: 2, train for the 27-th batch, train loss: 0.6702093482017517:  18%|██▎          | 27/151 [00:03<00:19,  6.40it/s]evaluate for the 4-th batch, evaluate loss: 0.6428334712982178:  10%|█▉                  | 2/21 [00:00<00:01, 12.75it/s]evaluate for the 4-th batch, evaluate loss: 0.6428334712982178:  19%|███▊                | 4/21 [00:00<00:01, 13.41it/s]Epoch: 1, train for the 191-th batch, train loss: 0.41240569949150085:  50%|████▉     | 190/383 [00:51<00:57,  3.38it/s]Epoch: 1, train for the 197-th batch, train loss: 0.5298115611076355:  83%|█████████▏ | 197/237 [00:52<00:10,  3.90it/s]Epoch: 2, train for the 101-th batch, train loss: 0.49983251094818115:  68%|██████▊   | 100/146 [00:14<00:08,  5.74it/s]Epoch: 2, train for the 101-th batch, train loss: 0.49983251094818115:  69%|██████▉   | 101/146 [00:14<00:07,  5.78it/s]Epoch: 1, train for the 240-th batch, train loss: 0.47433122992515564:  99%|█████████▉| 239/241 [00:52<00:00,  5.41it/s]Epoch: 1, train for the 191-th batch, train loss: 0.41240569949150085:  50%|████▉     | 191/383 [00:51<00:56,  3.40it/s]Epoch: 1, train for the 240-th batch, train loss: 0.47433122992515564: 100%|█████████▉| 240/241 [00:52<00:00,  5.68it/s]evaluate for the 5-th batch, evaluate loss: 0.7243524193763733:  19%|███▊                | 4/21 [00:00<00:01, 13.41it/s]Epoch: 2, train for the 28-th batch, train loss: 0.6549530625343323:  18%|██▎          | 27/151 [00:04<00:19,  6.40it/s]Epoch: 2, train for the 28-th batch, train loss: 0.6549530625343323:  19%|██▍          | 28/151 [00:04<00:18,  6.82it/s]Epoch: 1, train for the 198-th batch, train loss: 0.5191940665245056:  83%|█████████▏ | 197/237 [00:52<00:10,  3.90it/s]Epoch: 2, train for the 102-th batch, train loss: 0.5378231406211853:  69%|███████▌   | 101/146 [00:14<00:07,  5.78it/s]evaluate for the 6-th batch, evaluate loss: 0.7185804843902588:  19%|███▊                | 4/21 [00:00<00:01, 13.41it/s]evaluate for the 6-th batch, evaluate loss: 0.7185804843902588:  29%|█████▋              | 6/21 [00:00<00:01, 11.89it/s]Epoch: 1, train for the 198-th batch, train loss: 0.5191940665245056:  84%|█████████▏ | 198/237 [00:52<00:09,  4.16it/s]Epoch: 2, train for the 102-th batch, train loss: 0.5378231406211853:  70%|███████▋   | 102/146 [00:14<00:07,  5.76it/s]Epoch: 1, train for the 241-th batch, train loss: 0.5371871590614319: 100%|██████████▉| 240/241 [00:52<00:00,  5.68it/s]evaluate for the 7-th batch, evaluate loss: 0.6662619113922119:  29%|█████▋              | 6/21 [00:00<00:01, 11.89it/s]Epoch: 2, train for the 29-th batch, train loss: 0.667041540145874:  19%|██▌           | 28/151 [00:04<00:18,  6.82it/s]Epoch: 1, train for the 241-th batch, train loss: 0.5371871590614319: 100%|███████████| 241/241 [00:52<00:00,  5.50it/s]Epoch: 1, train for the 241-th batch, train loss: 0.5371871590614319: 100%|███████████| 241/241 [00:52<00:00,  4.56it/s]
Epoch: 2, train for the 29-th batch, train loss: 0.667041540145874:  19%|██▋           | 29/151 [00:04<00:18,  6.65it/s]Epoch: 1, train for the 192-th batch, train loss: 0.5118545293807983:  50%|█████▍     | 191/383 [00:51<00:56,  3.40it/s]evaluate for the 8-th batch, evaluate loss: 0.7062931060791016:  29%|█████▋              | 6/21 [00:00<00:01, 11.89it/s]evaluate for the 8-th batch, evaluate loss: 0.7062931060791016:  38%|███████▌            | 8/21 [00:00<00:00, 14.06it/s]Epoch: 1, train for the 192-th batch, train loss: 0.5118545293807983:  50%|█████▌     | 192/383 [00:51<00:54,  3.49it/s]Epoch: 2, train for the 103-th batch, train loss: 0.534458577632904:  70%|████████▍   | 102/146 [00:14<00:07,  5.76it/s]Epoch: 2, train for the 103-th batch, train loss: 0.534458577632904:  71%|████████▍   | 103/146 [00:14<00:07,  6.14it/s]evaluate for the 9-th batch, evaluate loss: 0.7086637616157532:  38%|███████▌            | 8/21 [00:00<00:00, 14.06it/s]Epoch: 1, train for the 199-th batch, train loss: 0.5209378004074097:  84%|█████████▏ | 198/237 [00:53<00:09,  4.16it/s]evaluate for the 10-th batch, evaluate loss: 0.718591570854187:  38%|███████▌            | 8/21 [00:00<00:00, 14.06it/s]evaluate for the 10-th batch, evaluate loss: 0.718591570854187:  48%|█████████          | 10/21 [00:00<00:00, 14.64it/s]  0%|                                                                                            | 0/72 [00:00<?, ?it/s]Epoch: 2, train for the 30-th batch, train loss: 0.6446875333786011:  19%|██▍          | 29/151 [00:04<00:18,  6.65it/s]Epoch: 2, train for the 30-th batch, train loss: 0.6446875333786011:  20%|██▌          | 30/151 [00:04<00:19,  6.29it/s]Epoch: 1, train for the 199-th batch, train loss: 0.5209378004074097:  84%|█████████▏ | 199/237 [00:53<00:09,  4.12it/s]evaluate for the 1-th batch, evaluate loss: 0.5283472537994385:   0%|                            | 0/72 [00:00<?, ?it/s]Epoch: 2, train for the 104-th batch, train loss: 0.5155928134918213:  71%|███████▊   | 103/146 [00:14<00:07,  6.14it/s]evaluate for the 11-th batch, evaluate loss: 0.7106266617774963:  48%|████████▌         | 10/21 [00:00<00:00, 14.64it/s]Epoch: 1, train for the 193-th batch, train loss: 0.5138523578643799:  50%|█████▌     | 192/383 [00:51<00:54,  3.49it/s]Epoch: 2, train for the 104-th batch, train loss: 0.5155928134918213:  71%|███████▊   | 104/146 [00:14<00:06,  6.17it/s]evaluate for the 2-th batch, evaluate loss: 0.5307163000106812:   0%|                            | 0/72 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5307163000106812:   3%|▌                   | 2/72 [00:00<00:04, 15.91it/s]Epoch: 1, train for the 193-th batch, train loss: 0.5138523578643799:  50%|█████▌     | 193/383 [00:51<00:51,  3.71it/s]evaluate for the 12-th batch, evaluate loss: 0.691292405128479:  48%|█████████          | 10/21 [00:00<00:00, 14.64it/s]Epoch: 2, train for the 31-th batch, train loss: 0.641277551651001:  20%|██▊           | 30/151 [00:04<00:19,  6.29it/s]evaluate for the 12-th batch, evaluate loss: 0.691292405128479:  57%|██████████▊        | 12/21 [00:00<00:00, 13.96it/s]Epoch: 2, train for the 31-th batch, train loss: 0.641277551651001:  21%|██▊           | 31/151 [00:04<00:18,  6.45it/s]evaluate for the 3-th batch, evaluate loss: 0.490312784910202:   3%|▌                    | 2/72 [00:00<00:04, 15.91it/s]Epoch: 2, train for the 105-th batch, train loss: 0.5326889157295227:  71%|███████▊   | 104/146 [00:14<00:06,  6.17it/s]evaluate for the 13-th batch, evaluate loss: 0.6838787198066711:  57%|██████████▎       | 12/21 [00:00<00:00, 13.96it/s]Epoch: 2, train for the 105-th batch, train loss: 0.5326889157295227:  72%|███████▉   | 105/146 [00:14<00:06,  6.35it/s]Epoch: 1, train for the 200-th batch, train loss: 0.5619010329246521:  84%|█████████▏ | 199/237 [00:53<00:09,  4.12it/s]evaluate for the 4-th batch, evaluate loss: 0.5527970790863037:   3%|▌                   | 2/72 [00:00<00:04, 15.91it/s]evaluate for the 4-th batch, evaluate loss: 0.5527970790863037:   6%|█                   | 4/72 [00:00<00:04, 14.40it/s]Epoch: 1, train for the 200-th batch, train loss: 0.5619010329246521:  84%|█████████▎ | 200/237 [00:53<00:09,  4.04it/s]Epoch: 2, train for the 32-th batch, train loss: 0.6720320582389832:  21%|██▋          | 31/151 [00:04<00:18,  6.45it/s]Epoch: 2, train for the 32-th batch, train loss: 0.6720320582389832:  21%|██▊          | 32/151 [00:04<00:18,  6.47it/s]Epoch: 2, train for the 106-th batch, train loss: 0.5153672695159912:  72%|███████▉   | 105/146 [00:15<00:06,  6.35it/s]evaluate for the 5-th batch, evaluate loss: 0.4906710088253021:   6%|█                   | 4/72 [00:00<00:04, 14.40it/s]Epoch: 2, train for the 106-th batch, train loss: 0.5153672695159912:  73%|███████▉   | 106/146 [00:15<00:05,  6.71it/s]Epoch: 1, train for the 194-th batch, train loss: 0.47177818417549133:  50%|█████     | 193/383 [00:51<00:51,  3.71it/s]evaluate for the 14-th batch, evaluate loss: 0.6630352735519409:  57%|██████████▎       | 12/21 [00:01<00:00, 13.96it/s]evaluate for the 14-th batch, evaluate loss: 0.6630352735519409:  67%|████████████      | 14/21 [00:01<00:00, 10.75it/s]Epoch: 1, train for the 194-th batch, train loss: 0.47177818417549133:  51%|█████     | 194/383 [00:51<00:52,  3.59it/s]evaluate for the 6-th batch, evaluate loss: 0.5053858160972595:   6%|█                   | 4/72 [00:00<00:04, 14.40it/s]evaluate for the 6-th batch, evaluate loss: 0.5053858160972595:   8%|█▋                  | 6/72 [00:00<00:04, 13.54it/s]Epoch: 2, train for the 33-th batch, train loss: 0.6425572037696838:  21%|██▊          | 32/151 [00:04<00:18,  6.47it/s]Epoch: 2, train for the 33-th batch, train loss: 0.6425572037696838:  22%|██▊          | 33/151 [00:04<00:17,  6.59it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5556888580322266:  73%|███████▉   | 106/146 [00:15<00:05,  6.71it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5556888580322266:  73%|████████   | 107/146 [00:15<00:05,  6.98it/s]Epoch: 1, train for the 201-th batch, train loss: 0.5471658110618591:  84%|█████████▎ | 200/237 [00:53<00:09,  4.04it/s]evaluate for the 7-th batch, evaluate loss: 0.556420087814331:   8%|█▊                   | 6/72 [00:00<00:04, 13.54it/s]evaluate for the 15-th batch, evaluate loss: 0.7029443383216858:  67%|████████████      | 14/21 [00:01<00:00, 10.75it/s]Epoch: 1, train for the 201-th batch, train loss: 0.5471658110618591:  85%|█████████▎ | 201/237 [00:53<00:09,  3.89it/s]evaluate for the 8-th batch, evaluate loss: 0.5502867102622986:   8%|█▋                  | 6/72 [00:00<00:04, 13.54it/s]evaluate for the 8-th batch, evaluate loss: 0.5502867102622986:  11%|██▏                 | 8/72 [00:00<00:04, 13.58it/s]evaluate for the 16-th batch, evaluate loss: 0.6830068230628967:  67%|████████████      | 14/21 [00:01<00:00, 10.75it/s]evaluate for the 16-th batch, evaluate loss: 0.6830068230628967:  76%|█████████████▋    | 16/21 [00:01<00:00, 10.73it/s]Epoch: 1, train for the 195-th batch, train loss: 0.4943501949310303:  51%|█████▌     | 194/383 [00:52<00:52,  3.59it/s]Epoch: 2, train for the 108-th batch, train loss: 0.523688793182373:  73%|████████▊   | 107/146 [00:15<00:05,  6.98it/s]Epoch: 2, train for the 34-th batch, train loss: 0.6396344900131226:  22%|██▊          | 33/151 [00:05<00:17,  6.59it/s]Epoch: 2, train for the 108-th batch, train loss: 0.523688793182373:  74%|████████▉   | 108/146 [00:15<00:05,  6.74it/s]evaluate for the 9-th batch, evaluate loss: 0.5111753940582275:  11%|██▏                 | 8/72 [00:00<00:04, 13.58it/s]Epoch: 2, train for the 34-th batch, train loss: 0.6396344900131226:  23%|██▉          | 34/151 [00:05<00:19,  6.15it/s]Epoch: 1, train for the 195-th batch, train loss: 0.4943501949310303:  51%|█████▌     | 195/383 [00:52<00:49,  3.77it/s]evaluate for the 17-th batch, evaluate loss: 0.6183031797409058:  76%|█████████████▋    | 16/21 [00:01<00:00, 10.73it/s]evaluate for the 10-th batch, evaluate loss: 0.5026861429214478:  11%|██                 | 8/72 [00:00<00:04, 13.58it/s]evaluate for the 10-th batch, evaluate loss: 0.5026861429214478:  14%|██▌               | 10/72 [00:00<00:04, 13.07it/s]evaluate for the 18-th batch, evaluate loss: 0.6650334000587463:  76%|█████████████▋    | 16/21 [00:01<00:00, 10.73it/s]evaluate for the 18-th batch, evaluate loss: 0.6650334000587463:  86%|███████████████▍  | 18/21 [00:01<00:00, 11.41it/s]Epoch: 2, train for the 109-th batch, train loss: 0.5734155178070068:  74%|████████▏  | 108/146 [00:15<00:05,  6.74it/s]Epoch: 1, train for the 202-th batch, train loss: 0.5567277669906616:  85%|█████████▎ | 201/237 [00:53<00:09,  3.89it/s]Epoch: 2, train for the 109-th batch, train loss: 0.5734155178070068:  75%|████████▏  | 109/146 [00:15<00:05,  6.47it/s]Epoch: 2, train for the 35-th batch, train loss: 0.635875940322876:  23%|███▏          | 34/151 [00:05<00:19,  6.15it/s]evaluate for the 11-th batch, evaluate loss: 0.5178644061088562:  14%|██▌               | 10/72 [00:00<00:04, 13.07it/s]evaluate for the 19-th batch, evaluate loss: 0.6904818415641785:  86%|███████████████▍  | 18/21 [00:01<00:00, 11.41it/s]Epoch: 2, train for the 35-th batch, train loss: 0.635875940322876:  23%|███▏          | 35/151 [00:05<00:19,  6.03it/s]Epoch: 1, train for the 202-th batch, train loss: 0.5567277669906616:  85%|█████████▍ | 202/237 [00:53<00:09,  3.77it/s]Epoch: 1, train for the 196-th batch, train loss: 0.5011903047561646:  51%|█████▌     | 195/383 [00:52<00:49,  3.77it/s]evaluate for the 20-th batch, evaluate loss: 0.691872239112854:  86%|████████████████▎  | 18/21 [00:01<00:00, 11.41it/s]evaluate for the 20-th batch, evaluate loss: 0.691872239112854:  95%|██████████████████ | 20/21 [00:01<00:00, 12.41it/s]evaluate for the 12-th batch, evaluate loss: 0.5010867714881897:  14%|██▌               | 10/72 [00:00<00:04, 13.07it/s]evaluate for the 12-th batch, evaluate loss: 0.5010867714881897:  17%|███               | 12/72 [00:00<00:04, 12.59it/s]Epoch: 1, train for the 196-th batch, train loss: 0.5011903047561646:  51%|█████▋     | 196/383 [00:52<00:50,  3.72it/s]evaluate for the 21-th batch, evaluate loss: 0.8491703867912292:  95%|█████████████████▏| 20/21 [00:01<00:00, 12.41it/s]evaluate for the 21-th batch, evaluate loss: 0.8491703867912292: 100%|██████████████████| 21/21 [00:01<00:00, 12.53it/s]
Epoch: 2, train for the 110-th batch, train loss: 0.4761500358581543:  75%|████████▏  | 109/146 [00:15<00:05,  6.47it/s]Epoch: 2, train for the 36-th batch, train loss: 0.6490918397903442:  23%|███          | 35/151 [00:05<00:19,  6.03it/s]evaluate for the 13-th batch, evaluate loss: 0.45294398069381714:  17%|██▊              | 12/72 [00:00<00:04, 12.59it/s]Epoch: 2, train for the 110-th batch, train loss: 0.4761500358581543:  75%|████████▎  | 110/146 [00:15<00:05,  6.26it/s]Epoch: 2, train for the 36-th batch, train loss: 0.6490918397903442:  24%|███          | 36/151 [00:05<00:18,  6.12it/s]evaluate for the 14-th batch, evaluate loss: 0.42200374603271484:  17%|██▊              | 12/72 [00:01<00:04, 12.59it/s]evaluate for the 14-th batch, evaluate loss: 0.42200374603271484:  19%|███▎             | 14/72 [00:01<00:04, 13.52it/s]Epoch: 1, train for the 203-th batch, train loss: 0.5343500971794128:  85%|█████████▍ | 202/237 [00:54<00:09,  3.77it/s]Epoch: 1, train for the 203-th batch, train loss: 0.5343500971794128:  86%|█████████▍ | 203/237 [00:54<00:08,  3.88it/s]INFO:root:Epoch: 2, learning rate: 0.0001, train loss: 0.5208
INFO:root:train average_precision, 0.8305
INFO:root:train roc_auc, 0.8234
INFO:root:validate loss: 0.5286
INFO:root:validate average_precision, 0.8329
INFO:root:validate roc_auc, 0.8208
INFO:root:new node validate loss: 0.6958
INFO:root:new node validate first_1_average_precision, 0.7068
INFO:root:new node validate first_1_roc_auc, 0.6877
INFO:root:new node validate first_3_average_precision, 0.7163
INFO:root:new node validate first_3_roc_auc, 0.7062
INFO:root:new node validate first_10_average_precision, 0.7105
INFO:root:new node validate first_10_roc_auc, 0.7108
INFO:root:new node validate average_precision, 0.6801
INFO:root:new node validate roc_auc, 0.6859
INFO:root:save model ./saved_models/TGN/ia-reality-call/TGN_seed0_dummy-yehxa035/TGN_seed0_dummy-yehxa035.pkl
evaluate for the 15-th batch, evaluate loss: 0.46153444051742554:  19%|███▎             | 14/72 [00:01<00:04, 13.52it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5058441162109375:  75%|████████▎  | 110/146 [00:15<00:05,  6.26it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5058441162109375:  76%|████████▎  | 111/146 [00:15<00:05,  6.28it/s]Epoch: 2, train for the 37-th batch, train loss: 0.6634983420372009:  24%|███          | 36/151 [00:05<00:18,  6.12it/s]Epoch: 1, train for the 197-th batch, train loss: 0.5026404857635498:  51%|█████▋     | 196/383 [00:52<00:50,  3.72it/s]evaluate for the 16-th batch, evaluate loss: 0.4812317192554474:  19%|███▌              | 14/72 [00:01<00:04, 13.52it/s]evaluate for the 16-th batch, evaluate loss: 0.4812317192554474:  22%|████              | 16/72 [00:01<00:04, 13.81it/s]Epoch: 2, train for the 37-th batch, train loss: 0.6634983420372009:  25%|███▏         | 37/151 [00:05<00:19,  5.86it/s]Epoch: 1, train for the 197-th batch, train loss: 0.5026404857635498:  51%|█████▋     | 197/383 [00:52<00:50,  3.66it/s]evaluate for the 17-th batch, evaluate loss: 0.4420366585254669:  22%|████              | 16/72 [00:01<00:04, 13.81it/s]Epoch: 2, train for the 112-th batch, train loss: 0.5537280440330505:  76%|████████▎  | 111/146 [00:15<00:05,  6.28it/s]Epoch: 1, train for the 204-th batch, train loss: 0.5447887778282166:  86%|█████████▍ | 203/237 [00:54<00:08,  3.88it/s]Epoch: 2, train for the 112-th batch, train loss: 0.5537280440330505:  77%|████████▍  | 112/146 [00:16<00:05,  6.65it/s]Epoch: 2, train for the 38-th batch, train loss: 0.6496607661247253:  25%|███▏         | 37/151 [00:05<00:19,  5.86it/s]evaluate for the 18-th batch, evaluate loss: 0.4277898371219635:  22%|████              | 16/72 [00:01<00:04, 13.81it/s]evaluate for the 18-th batch, evaluate loss: 0.4277898371219635:  25%|████▌             | 18/72 [00:01<00:03, 13.75it/s]Epoch: 2, train for the 38-th batch, train loss: 0.6496607661247253:  25%|███▎         | 38/151 [00:05<00:18,  6.06it/s]Epoch: 1, train for the 204-th batch, train loss: 0.5447887778282166:  86%|█████████▍ | 204/237 [00:54<00:08,  3.97it/s]Epoch: 2, train for the 113-th batch, train loss: 0.4903824031352997:  77%|████████▍  | 112/146 [00:16<00:05,  6.65it/s]Epoch: 2, train for the 113-th batch, train loss: 0.4903824031352997:  77%|████████▌  | 113/146 [00:16<00:04,  7.20it/s]evaluate for the 19-th batch, evaluate loss: 0.47352004051208496:  25%|████▎            | 18/72 [00:01<00:03, 13.75it/s]Epoch: 1, train for the 198-th batch, train loss: 0.46497583389282227:  51%|█████▏    | 197/383 [00:52<00:50,  3.66it/s]Epoch: 1, train for the 198-th batch, train loss: 0.46497583389282227:  52%|█████▏    | 198/383 [00:52<00:48,  3.84it/s]evaluate for the 20-th batch, evaluate loss: 0.5067405104637146:  25%|████▌             | 18/72 [00:01<00:03, 13.75it/s]evaluate for the 20-th batch, evaluate loss: 0.5067405104637146:  28%|█████             | 20/72 [00:01<00:03, 13.94it/s]Epoch: 2, train for the 39-th batch, train loss: 0.6258552670478821:  25%|███▎         | 38/151 [00:05<00:18,  6.06it/s]Epoch: 2, train for the 39-th batch, train loss: 0.6258552670478821:  26%|███▎         | 39/151 [00:05<00:18,  6.02it/s]Epoch: 2, train for the 114-th batch, train loss: 0.5112654566764832:  77%|████████▌  | 113/146 [00:16<00:04,  7.20it/s]Epoch: 2, train for the 114-th batch, train loss: 0.5112654566764832:  78%|████████▌  | 114/146 [00:16<00:04,  7.31it/s]evaluate for the 21-th batch, evaluate loss: 0.5243375897407532:  28%|█████             | 20/72 [00:01<00:03, 13.94it/s]Epoch: 1, train for the 205-th batch, train loss: 0.5440464615821838:  86%|█████████▍ | 204/237 [00:54<00:08,  3.97it/s]Epoch: 1, train for the 205-th batch, train loss: 0.5440464615821838:  86%|█████████▌ | 205/237 [00:54<00:07,  4.00it/s]evaluate for the 22-th batch, evaluate loss: 0.4664340019226074:  28%|█████             | 20/72 [00:01<00:03, 13.94it/s]evaluate for the 22-th batch, evaluate loss: 0.4664340019226074:  31%|█████▌            | 22/72 [00:01<00:03, 13.92it/s]Epoch: 2, train for the 40-th batch, train loss: 0.6277545690536499:  26%|███▎         | 39/151 [00:06<00:18,  6.02it/s]Epoch: 1, train for the 199-th batch, train loss: 0.4663788378238678:  52%|█████▋     | 198/383 [00:53<00:48,  3.84it/s]Epoch: 2, train for the 115-th batch, train loss: 0.5379375219345093:  78%|████████▌  | 114/146 [00:16<00:04,  7.31it/s]Epoch: 2, train for the 40-th batch, train loss: 0.6277545690536499:  26%|███▍         | 40/151 [00:06<00:17,  6.23it/s]Epoch: 2, train for the 115-th batch, train loss: 0.5379375219345093:  79%|████████▋  | 115/146 [00:16<00:04,  7.42it/s]evaluate for the 23-th batch, evaluate loss: 0.4764538109302521:  31%|█████▌            | 22/72 [00:01<00:03, 13.92it/s]Epoch: 1, train for the 199-th batch, train loss: 0.4663788378238678:  52%|█████▋     | 199/383 [00:53<00:46,  3.99it/s]Epoch: 2, train for the 116-th batch, train loss: 0.5261589884757996:  79%|████████▋  | 115/146 [00:16<00:04,  7.42it/s]evaluate for the 24-th batch, evaluate loss: 0.4713389575481415:  31%|█████▌            | 22/72 [00:01<00:03, 13.92it/s]evaluate for the 24-th batch, evaluate loss: 0.4713389575481415:  33%|██████            | 24/72 [00:01<00:03, 13.37it/s]Epoch: 1, train for the 206-th batch, train loss: 0.5444071888923645:  86%|█████████▌ | 205/237 [00:54<00:07,  4.00it/s]Epoch: 2, train for the 116-th batch, train loss: 0.5261589884757996:  79%|████████▋  | 116/146 [00:16<00:03,  7.60it/s]Epoch: 2, train for the 41-th batch, train loss: 0.6355079412460327:  26%|███▍         | 40/151 [00:06<00:17,  6.23it/s]Epoch: 2, train for the 41-th batch, train loss: 0.6355079412460327:  27%|███▌         | 41/151 [00:06<00:17,  6.25it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]Epoch: 1, train for the 206-th batch, train loss: 0.5444071888923645:  87%|█████████▌ | 206/237 [00:54<00:07,  4.02it/s]evaluate for the 25-th batch, evaluate loss: 0.5869686603546143:  33%|██████            | 24/72 [00:01<00:03, 13.37it/s]Epoch: 1, train for the 200-th batch, train loss: 0.5600077509880066:  52%|█████▋     | 199/383 [00:53<00:46,  3.99it/s]Epoch: 2, train for the 117-th batch, train loss: 0.5722150802612305:  79%|████████▋  | 116/146 [00:16<00:03,  7.60it/s]Epoch: 2, train for the 117-th batch, train loss: 0.5722150802612305:  80%|████████▊  | 117/146 [00:16<00:03,  7.25it/s]evaluate for the 26-th batch, evaluate loss: 0.42798933386802673:  33%|█████▋           | 24/72 [00:01<00:03, 13.37it/s]evaluate for the 26-th batch, evaluate loss: 0.42798933386802673:  36%|██████▏          | 26/72 [00:01<00:03, 12.95it/s]Epoch: 2, train for the 42-th batch, train loss: 0.6118024587631226:  27%|███▌         | 41/151 [00:06<00:17,  6.25it/s]Epoch: 2, train for the 42-th batch, train loss: 0.6118024587631226:  28%|███▌         | 42/151 [00:06<00:17,  6.39it/s]Epoch: 1, train for the 200-th batch, train loss: 0.5600077509880066:  52%|█████▋     | 200/383 [00:53<00:47,  3.87it/s]evaluate for the 27-th batch, evaluate loss: 0.4035443365573883:  36%|██████▌           | 26/72 [00:01<00:03, 12.95it/s]Epoch: 2, train for the 118-th batch, train loss: 0.5496639013290405:  80%|████████▊  | 117/146 [00:16<00:03,  7.25it/s]evaluate for the 28-th batch, evaluate loss: 0.4376388490200043:  36%|██████▌           | 26/72 [00:02<00:03, 12.95it/s]evaluate for the 28-th batch, evaluate loss: 0.4376388490200043:  39%|███████           | 28/72 [00:02<00:03, 13.56it/s]Epoch: 3, train for the 1-th batch, train loss: 0.6629377603530884:   0%|                       | 0/119 [00:00<?, ?it/s]Epoch: 2, train for the 118-th batch, train loss: 0.5496639013290405:  81%|████████▉  | 118/146 [00:16<00:03,  7.18it/s]Epoch: 3, train for the 1-th batch, train loss: 0.6629377603530884:   1%|▏              | 1/119 [00:00<00:30,  3.82it/s]Epoch: 2, train for the 43-th batch, train loss: 0.644077718257904:  28%|███▉          | 42/151 [00:06<00:17,  6.39it/s]Epoch: 2, train for the 43-th batch, train loss: 0.644077718257904:  28%|███▉          | 43/151 [00:06<00:16,  6.44it/s]evaluate for the 29-th batch, evaluate loss: 0.5031639933586121:  39%|███████           | 28/72 [00:02<00:03, 13.56it/s]evaluate for the 30-th batch, evaluate loss: 0.4857739210128784:  39%|███████           | 28/72 [00:02<00:03, 13.56it/s]evaluate for the 30-th batch, evaluate loss: 0.4857739210128784:  42%|███████▌          | 30/72 [00:02<00:02, 14.04it/s]Epoch: 1, train for the 201-th batch, train loss: 0.5202324390411377:  52%|█████▋     | 200/383 [00:53<00:47,  3.87it/s]Epoch: 1, train for the 207-th batch, train loss: 0.579749584197998:  87%|██████████▍ | 206/237 [00:55<00:07,  4.02it/s]Epoch: 3, train for the 2-th batch, train loss: 0.8083557486534119:   1%|▏              | 1/119 [00:00<00:30,  3.82it/s]Epoch: 3, train for the 2-th batch, train loss: 0.8083557486534119:   2%|▎              | 2/119 [00:00<00:22,  5.22it/s]Epoch: 1, train for the 201-th batch, train loss: 0.5202324390411377:  52%|█████▊     | 201/383 [00:53<00:48,  3.75it/s]Epoch: 1, train for the 207-th batch, train loss: 0.579749584197998:  87%|██████████▍ | 207/237 [00:55<00:08,  3.34it/s]Epoch: 2, train for the 119-th batch, train loss: 0.507504403591156:  81%|█████████▋  | 118/146 [00:16<00:03,  7.18it/s]Epoch: 2, train for the 119-th batch, train loss: 0.507504403591156:  82%|█████████▊  | 119/146 [00:16<00:04,  6.52it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5823308229446411:  28%|███▋         | 43/151 [00:06<00:16,  6.44it/s]evaluate for the 31-th batch, evaluate loss: 0.5234043598175049:  42%|███████▌          | 30/72 [00:02<00:02, 14.04it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5823308229446411:  29%|███▊         | 44/151 [00:06<00:17,  6.06it/s]Epoch: 3, train for the 3-th batch, train loss: 0.8496397137641907:   2%|▎              | 2/119 [00:00<00:22,  5.22it/s]Epoch: 3, train for the 3-th batch, train loss: 0.8496397137641907:   3%|▍              | 3/119 [00:00<00:18,  6.32it/s]evaluate for the 32-th batch, evaluate loss: 0.5684638023376465:  42%|███████▌          | 30/72 [00:02<00:02, 14.04it/s]evaluate for the 32-th batch, evaluate loss: 0.5684638023376465:  44%|████████          | 32/72 [00:02<00:02, 13.70it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5130597949028015:  82%|████████▉  | 119/146 [00:17<00:04,  6.52it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5130597949028015:  82%|█████████  | 120/146 [00:17<00:03,  6.54it/s]evaluate for the 33-th batch, evaluate loss: 0.502152681350708:  44%|████████▍          | 32/72 [00:02<00:02, 13.70it/s]Epoch: 2, train for the 45-th batch, train loss: 0.6002478003501892:  29%|███▊         | 44/151 [00:06<00:17,  6.06it/s]Epoch: 3, train for the 4-th batch, train loss: 0.7766056656837463:   3%|▍              | 3/119 [00:00<00:18,  6.32it/s]Epoch: 1, train for the 208-th batch, train loss: 0.5454913377761841:  87%|█████████▌ | 207/237 [00:55<00:08,  3.34it/s]Epoch: 3, train for the 4-th batch, train loss: 0.7766056656837463:   3%|▌              | 4/119 [00:00<00:17,  6.68it/s]Epoch: 2, train for the 45-th batch, train loss: 0.6002478003501892:  30%|███▊         | 45/151 [00:06<00:18,  5.84it/s]evaluate for the 34-th batch, evaluate loss: 0.5489398241043091:  44%|████████          | 32/72 [00:02<00:02, 13.70it/s]evaluate for the 34-th batch, evaluate loss: 0.5489398241043091:  47%|████████▌         | 34/72 [00:02<00:02, 14.04it/s]Epoch: 1, train for the 202-th batch, train loss: 0.4845622777938843:  52%|█████▊     | 201/383 [00:54<00:48,  3.75it/s]Epoch: 1, train for the 208-th batch, train loss: 0.5454913377761841:  88%|█████████▋ | 208/237 [00:55<00:08,  3.46it/s]Epoch: 2, train for the 121-th batch, train loss: 0.5382938981056213:  82%|█████████  | 120/146 [00:17<00:03,  6.54it/s]Epoch: 2, train for the 121-th batch, train loss: 0.5382938981056213:  83%|█████████  | 121/146 [00:17<00:03,  6.73it/s]evaluate for the 35-th batch, evaluate loss: 0.46520376205444336:  47%|████████         | 34/72 [00:02<00:02, 14.04it/s]Epoch: 1, train for the 202-th batch, train loss: 0.4845622777938843:  53%|█████▊     | 202/383 [00:54<00:51,  3.51it/s]Epoch: 3, train for the 5-th batch, train loss: 0.637365460395813:   3%|▌               | 4/119 [00:00<00:17,  6.68it/s]Epoch: 3, train for the 5-th batch, train loss: 0.637365460395813:   4%|▋               | 5/119 [00:00<00:16,  7.03it/s]Epoch: 2, train for the 46-th batch, train loss: 0.6094002723693848:  30%|███▊         | 45/151 [00:06<00:18,  5.84it/s]evaluate for the 36-th batch, evaluate loss: 0.508226215839386:  47%|████████▉          | 34/72 [00:02<00:02, 14.04it/s]evaluate for the 36-th batch, evaluate loss: 0.508226215839386:  50%|█████████▌         | 36/72 [00:02<00:02, 14.42it/s]Epoch: 2, train for the 46-th batch, train loss: 0.6094002723693848:  30%|███▉         | 46/151 [00:06<00:17,  6.17it/s]evaluate for the 37-th batch, evaluate loss: 0.49487051367759705:  50%|████████▌        | 36/72 [00:02<00:02, 14.42it/s]Epoch: 2, train for the 122-th batch, train loss: 0.5479132533073425:  83%|█████████  | 121/146 [00:17<00:03,  6.73it/s]Epoch: 1, train for the 209-th batch, train loss: 0.5530941486358643:  88%|█████████▋ | 208/237 [00:55<00:08,  3.46it/s]Epoch: 2, train for the 122-th batch, train loss: 0.5479132533073425:  84%|█████████▏ | 122/146 [00:17<00:03,  6.81it/s]Epoch: 1, train for the 209-th batch, train loss: 0.5530941486358643:  88%|█████████▋ | 209/237 [00:55<00:07,  3.77it/s]Epoch: 3, train for the 6-th batch, train loss: 0.5420011281967163:   4%|▋              | 5/119 [00:00<00:16,  7.03it/s]evaluate for the 38-th batch, evaluate loss: 0.4098457098007202:  50%|█████████         | 36/72 [00:02<00:02, 14.42it/s]evaluate for the 38-th batch, evaluate loss: 0.4098457098007202:  53%|█████████▌        | 38/72 [00:02<00:02, 14.59it/s]Epoch: 3, train for the 6-th batch, train loss: 0.5420011281967163:   5%|▊              | 6/119 [00:00<00:16,  6.90it/s]Epoch: 2, train for the 47-th batch, train loss: 0.6149533987045288:  30%|███▉         | 46/151 [00:07<00:17,  6.17it/s]Epoch: 2, train for the 47-th batch, train loss: 0.6149533987045288:  31%|████         | 47/151 [00:07<00:16,  6.19it/s]Epoch: 1, train for the 203-th batch, train loss: 0.4435536861419678:  53%|█████▊     | 202/383 [00:54<00:51,  3.51it/s]evaluate for the 39-th batch, evaluate loss: 0.4335821866989136:  53%|█████████▌        | 38/72 [00:02<00:02, 14.59it/s]Epoch: 2, train for the 123-th batch, train loss: 0.5700250864028931:  84%|█████████▏ | 122/146 [00:17<00:03,  6.81it/s]Epoch: 2, train for the 123-th batch, train loss: 0.5700250864028931:  84%|█████████▎ | 123/146 [00:17<00:03,  6.94it/s]Epoch: 3, train for the 7-th batch, train loss: 0.5006040334701538:   5%|▊              | 6/119 [00:01<00:16,  6.90it/s]Epoch: 3, train for the 7-th batch, train loss: 0.5006040334701538:   6%|▉              | 7/119 [00:01<00:14,  7.52it/s]Epoch: 1, train for the 203-th batch, train loss: 0.4435536861419678:  53%|█████▊     | 203/383 [00:54<00:51,  3.48it/s]evaluate for the 40-th batch, evaluate loss: 0.4915093779563904:  53%|█████████▌        | 38/72 [00:02<00:02, 14.59it/s]evaluate for the 40-th batch, evaluate loss: 0.4915093779563904:  56%|██████████        | 40/72 [00:02<00:02, 15.05it/s]Epoch: 1, train for the 210-th batch, train loss: 0.5765330195426941:  88%|█████████▋ | 209/237 [00:55<00:07,  3.77it/s]Epoch: 2, train for the 48-th batch, train loss: 0.5720542669296265:  31%|████         | 47/151 [00:07<00:16,  6.19it/s]Epoch: 1, train for the 210-th batch, train loss: 0.5765330195426941:  89%|█████████▋ | 210/237 [00:55<00:06,  3.88it/s]evaluate for the 41-th batch, evaluate loss: 0.4957638084888458:  56%|██████████        | 40/72 [00:02<00:02, 15.05it/s]Epoch: 2, train for the 48-th batch, train loss: 0.5720542669296265:  32%|████▏        | 48/151 [00:07<00:17,  5.85it/s]Epoch: 2, train for the 124-th batch, train loss: 0.5514225363731384:  84%|█████████▎ | 123/146 [00:17<00:03,  6.94it/s]Epoch: 3, train for the 8-th batch, train loss: 0.4669640362262726:   6%|▉              | 7/119 [00:01<00:14,  7.52it/s]Epoch: 2, train for the 124-th batch, train loss: 0.5514225363731384:  85%|█████████▎ | 124/146 [00:17<00:03,  6.49it/s]evaluate for the 42-th batch, evaluate loss: 0.4906662702560425:  56%|██████████        | 40/72 [00:03<00:02, 15.05it/s]evaluate for the 42-th batch, evaluate loss: 0.4906662702560425:  58%|██████████▌       | 42/72 [00:03<00:02, 14.91it/s]Epoch: 3, train for the 8-th batch, train loss: 0.4669640362262726:   7%|█              | 8/119 [00:01<00:15,  7.20it/s]Epoch: 1, train for the 204-th batch, train loss: 0.32856056094169617:  53%|█████▎    | 203/383 [00:54<00:51,  3.48it/s]evaluate for the 43-th batch, evaluate loss: 0.4957664906978607:  58%|██████████▌       | 42/72 [00:03<00:02, 14.91it/s]Epoch: 1, train for the 204-th batch, train loss: 0.32856056094169617:  53%|█████▎    | 204/383 [00:54<00:48,  3.65it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5963925123214722:  32%|████▏        | 48/151 [00:07<00:17,  5.85it/s]Epoch: 1, train for the 211-th batch, train loss: 0.5307116508483887:  89%|█████████▋ | 210/237 [00:56<00:06,  3.88it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5963925123214722:  32%|████▏        | 49/151 [00:07<00:17,  5.87it/s]Epoch: 2, train for the 125-th batch, train loss: 0.5236768126487732:  85%|█████████▎ | 124/146 [00:17<00:03,  6.49it/s]Epoch: 2, train for the 125-th batch, train loss: 0.5236768126487732:  86%|█████████▍ | 125/146 [00:17<00:03,  6.45it/s]evaluate for the 44-th batch, evaluate loss: 0.49310624599456787:  58%|█████████▉       | 42/72 [00:03<00:02, 14.91it/s]evaluate for the 44-th batch, evaluate loss: 0.49310624599456787:  61%|██████████▍      | 44/72 [00:03<00:01, 14.21it/s]Epoch: 1, train for the 211-th batch, train loss: 0.5307116508483887:  89%|█████████▊ | 211/237 [00:56<00:06,  4.10it/s]Epoch: 3, train for the 9-th batch, train loss: 0.49613329768180847:   7%|▉             | 8/119 [00:01<00:15,  7.20it/s]Epoch: 3, train for the 9-th batch, train loss: 0.49613329768180847:   8%|█             | 9/119 [00:01<00:16,  6.74it/s]evaluate for the 45-th batch, evaluate loss: 0.48344069719314575:  61%|██████████▍      | 44/72 [00:03<00:01, 14.21it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5728152394294739:  32%|████▏        | 49/151 [00:07<00:17,  5.87it/s]Epoch: 1, train for the 205-th batch, train loss: 0.4237113893032074:  53%|█████▊     | 204/383 [00:54<00:48,  3.65it/s]Epoch: 2, train for the 126-th batch, train loss: 0.5618872046470642:  86%|█████████▍ | 125/146 [00:18<00:03,  6.45it/s]evaluate for the 46-th batch, evaluate loss: 0.5348596572875977:  61%|███████████       | 44/72 [00:03<00:01, 14.21it/s]evaluate for the 46-th batch, evaluate loss: 0.5348596572875977:  64%|███████████▌      | 46/72 [00:03<00:01, 13.57it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5728152394294739:  33%|████▎        | 50/151 [00:07<00:17,  5.71it/s]Epoch: 2, train for the 126-th batch, train loss: 0.5618872046470642:  86%|█████████▍ | 126/146 [00:18<00:03,  6.27it/s]Epoch: 3, train for the 10-th batch, train loss: 0.5043856501579285:   8%|█             | 9/119 [00:01<00:16,  6.74it/s]Epoch: 3, train for the 10-th batch, train loss: 0.5043856501579285:   8%|█            | 10/119 [00:01<00:16,  6.58it/s]Epoch: 1, train for the 212-th batch, train loss: 0.5401461124420166:  89%|█████████▊ | 211/237 [00:56<00:06,  4.10it/s]Epoch: 1, train for the 205-th batch, train loss: 0.4237113893032074:  54%|█████▉     | 205/383 [00:54<00:49,  3.61it/s]evaluate for the 47-th batch, evaluate loss: 0.367186039686203:  64%|████████████▏      | 46/72 [00:03<00:01, 13.57it/s]Epoch: 1, train for the 212-th batch, train loss: 0.5401461124420166:  89%|█████████▊ | 212/237 [00:56<00:06,  4.08it/s]evaluate for the 48-th batch, evaluate loss: 0.3245944082736969:  64%|███████████▌      | 46/72 [00:03<00:01, 13.57it/s]evaluate for the 48-th batch, evaluate loss: 0.3245944082736969:  67%|████████████      | 48/72 [00:03<00:01, 14.14it/s]Epoch: 2, train for the 51-th batch, train loss: 0.6179882884025574:  33%|████▎        | 50/151 [00:07<00:17,  5.71it/s]Epoch: 2, train for the 51-th batch, train loss: 0.6179882884025574:  34%|████▍        | 51/151 [00:07<00:16,  5.95it/s]Epoch: 2, train for the 127-th batch, train loss: 0.5100157856941223:  86%|█████████▍ | 126/146 [00:18<00:03,  6.27it/s]Epoch: 3, train for the 11-th batch, train loss: 0.5052957534790039:   8%|█            | 10/119 [00:01<00:16,  6.58it/s]Epoch: 2, train for the 127-th batch, train loss: 0.5100157856941223:  87%|█████████▌ | 127/146 [00:18<00:03,  6.03it/s]evaluate for the 49-th batch, evaluate loss: 0.4483538866043091:  67%|████████████      | 48/72 [00:03<00:01, 14.14it/s]Epoch: 3, train for the 11-th batch, train loss: 0.5052957534790039:   9%|█▏           | 11/119 [00:01<00:17,  6.28it/s]evaluate for the 50-th batch, evaluate loss: 0.40112003684043884:  67%|███████████▎     | 48/72 [00:03<00:01, 14.14it/s]evaluate for the 50-th batch, evaluate loss: 0.40112003684043884:  69%|███████████▊     | 50/72 [00:03<00:01, 14.11it/s]Epoch: 2, train for the 52-th batch, train loss: 0.6304534077644348:  34%|████▍        | 51/151 [00:07<00:16,  5.95it/s]Epoch: 2, train for the 52-th batch, train loss: 0.6304534077644348:  34%|████▍        | 52/151 [00:08<00:16,  6.00it/s]evaluate for the 51-th batch, evaluate loss: 0.4992128610610962:  69%|████████████▌     | 50/72 [00:03<00:01, 14.11it/s]Epoch: 1, train for the 213-th batch, train loss: 0.5547690391540527:  89%|█████████▊ | 212/237 [00:56<00:06,  4.08it/s]Epoch: 1, train for the 206-th batch, train loss: 0.46673640608787537:  54%|█████▎    | 205/383 [00:55<00:49,  3.61it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5411290526390076:  87%|█████████▌ | 127/146 [00:18<00:03,  6.03it/s]Epoch: 3, train for the 12-th batch, train loss: 0.5006164908409119:   9%|█▏           | 11/119 [00:01<00:17,  6.28it/s]Epoch: 3, train for the 12-th batch, train loss: 0.5006164908409119:  10%|█▎           | 12/119 [00:01<00:18,  5.91it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5411290526390076:  88%|█████████▋ | 128/146 [00:18<00:03,  5.66it/s]Epoch: 1, train for the 213-th batch, train loss: 0.5547690391540527:  90%|█████████▉ | 213/237 [00:56<00:06,  3.83it/s]evaluate for the 52-th batch, evaluate loss: 0.4436614215373993:  69%|████████████▌     | 50/72 [00:03<00:01, 14.11it/s]evaluate for the 52-th batch, evaluate loss: 0.4436614215373993:  72%|█████████████     | 52/72 [00:03<00:01, 14.38it/s]Epoch: 1, train for the 206-th batch, train loss: 0.46673640608787537:  54%|█████▍    | 206/383 [00:55<00:52,  3.37it/s]Epoch: 2, train for the 53-th batch, train loss: 0.6121940612792969:  34%|████▍        | 52/151 [00:08<00:16,  6.00it/s]Epoch: 2, train for the 53-th batch, train loss: 0.6121940612792969:  35%|████▌        | 53/151 [00:08<00:15,  6.21it/s]evaluate for the 53-th batch, evaluate loss: 0.4762301445007324:  72%|█████████████     | 52/72 [00:03<00:01, 14.38it/s]Epoch: 3, train for the 13-th batch, train loss: 0.4262200891971588:  10%|█▎           | 12/119 [00:02<00:18,  5.91it/s]Epoch: 3, train for the 13-th batch, train loss: 0.4262200891971588:  11%|█▍           | 13/119 [00:02<00:16,  6.55it/s]Epoch: 2, train for the 129-th batch, train loss: 0.5454136729240417:  88%|█████████▋ | 128/146 [00:18<00:03,  5.66it/s]evaluate for the 54-th batch, evaluate loss: 0.5208272337913513:  72%|█████████████     | 52/72 [00:03<00:01, 14.38it/s]evaluate for the 54-th batch, evaluate loss: 0.5208272337913513:  75%|█████████████▌    | 54/72 [00:03<00:01, 14.51it/s]Epoch: 2, train for the 129-th batch, train loss: 0.5454136729240417:  88%|█████████▋ | 129/146 [00:18<00:02,  5.83it/s]Epoch: 2, train for the 54-th batch, train loss: 0.5817694664001465:  35%|████▌        | 53/151 [00:08<00:15,  6.21it/s]Epoch: 2, train for the 54-th batch, train loss: 0.5817694664001465:  36%|████▋        | 54/151 [00:08<00:14,  6.56it/s]evaluate for the 55-th batch, evaluate loss: 0.4872833490371704:  75%|█████████████▌    | 54/72 [00:03<00:01, 14.51it/s]Epoch: 3, train for the 14-th batch, train loss: 0.4924883246421814:  11%|█▍           | 13/119 [00:02<00:16,  6.55it/s]Epoch: 3, train for the 14-th batch, train loss: 0.4924883246421814:  12%|█▌           | 14/119 [00:02<00:15,  6.86it/s]Epoch: 1, train for the 214-th batch, train loss: 0.5961904525756836:  90%|█████████▉ | 213/237 [00:56<00:06,  3.83it/s]Epoch: 1, train for the 207-th batch, train loss: 0.4700649082660675:  54%|█████▉     | 206/383 [00:55<00:52,  3.37it/s]evaluate for the 56-th batch, evaluate loss: 0.49224281311035156:  75%|████████████▊    | 54/72 [00:03<00:01, 14.51it/s]evaluate for the 56-th batch, evaluate loss: 0.49224281311035156:  78%|█████████████▏   | 56/72 [00:03<00:01, 14.92it/s]Epoch: 2, train for the 130-th batch, train loss: 0.4771327078342438:  88%|█████████▋ | 129/146 [00:18<00:02,  5.83it/s]Epoch: 1, train for the 214-th batch, train loss: 0.5961904525756836:  90%|█████████▉ | 214/237 [00:57<00:06,  3.73it/s]Epoch: 2, train for the 130-th batch, train loss: 0.4771327078342438:  89%|█████████▊ | 130/146 [00:18<00:02,  6.24it/s]Epoch: 1, train for the 207-th batch, train loss: 0.4700649082660675:  54%|█████▉     | 207/383 [00:55<00:51,  3.39it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5659384727478027:  36%|████▋        | 54/151 [00:08<00:14,  6.56it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5659384727478027:  36%|████▋        | 55/151 [00:08<00:13,  6.99it/s]evaluate for the 57-th batch, evaluate loss: 0.4410130977630615:  78%|██████████████    | 56/72 [00:04<00:01, 14.92it/s]Epoch: 3, train for the 15-th batch, train loss: 0.5401661396026611:  12%|█▌           | 14/119 [00:02<00:15,  6.86it/s]Epoch: 3, train for the 15-th batch, train loss: 0.5401661396026611:  13%|█▋           | 15/119 [00:02<00:15,  6.65it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5421926975250244:  89%|█████████▊ | 130/146 [00:18<00:02,  6.24it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5421926975250244:  90%|█████████▊ | 131/146 [00:18<00:02,  6.42it/s]evaluate for the 58-th batch, evaluate loss: 0.5023624897003174:  78%|██████████████    | 56/72 [00:04<00:01, 14.92it/s]evaluate for the 58-th batch, evaluate loss: 0.5023624897003174:  81%|██████████████▌   | 58/72 [00:04<00:01, 13.73it/s]Epoch: 1, train for the 215-th batch, train loss: 0.5621259808540344:  90%|█████████▉ | 214/237 [00:57<00:06,  3.73it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5278415083885193:  36%|████▋        | 55/151 [00:08<00:13,  6.99it/s]evaluate for the 59-th batch, evaluate loss: 0.4755411446094513:  81%|██████████████▌   | 58/72 [00:04<00:01, 13.73it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5278415083885193:  37%|████▊        | 56/151 [00:08<00:14,  6.34it/s]Epoch: 1, train for the 208-th batch, train loss: 0.5035117864608765:  54%|█████▉     | 207/383 [00:55<00:51,  3.39it/s]Epoch: 3, train for the 16-th batch, train loss: 0.5008660554885864:  13%|█▋           | 15/119 [00:02<00:15,  6.65it/s]Epoch: 3, train for the 16-th batch, train loss: 0.5008660554885864:  13%|█▋           | 16/119 [00:02<00:15,  6.84it/s]Epoch: 1, train for the 215-th batch, train loss: 0.5621259808540344:  91%|█████████▉ | 215/237 [00:57<00:05,  3.75it/s]Epoch: 1, train for the 208-th batch, train loss: 0.5035117864608765:  54%|█████▉     | 208/383 [00:55<00:49,  3.50it/s]Epoch: 2, train for the 132-th batch, train loss: 0.4842144548892975:  90%|█████████▊ | 131/146 [00:19<00:02,  6.42it/s]Epoch: 2, train for the 132-th batch, train loss: 0.4842144548892975:  90%|█████████▉ | 132/146 [00:19<00:02,  6.46it/s]evaluate for the 60-th batch, evaluate loss: 0.4844411313533783:  81%|██████████████▌   | 58/72 [00:04<00:01, 13.73it/s]evaluate for the 60-th batch, evaluate loss: 0.4844411313533783:  83%|███████████████   | 60/72 [00:04<00:00, 13.76it/s]Epoch: 2, train for the 57-th batch, train loss: 0.5642173886299133:  37%|████▊        | 56/151 [00:08<00:14,  6.34it/s]Epoch: 2, train for the 57-th batch, train loss: 0.5642173886299133:  38%|████▉        | 57/151 [00:08<00:14,  6.65it/s]Epoch: 3, train for the 17-th batch, train loss: 0.4690794348716736:  13%|█▋           | 16/119 [00:02<00:15,  6.84it/s]Epoch: 3, train for the 17-th batch, train loss: 0.4690794348716736:  14%|█▊           | 17/119 [00:02<00:15,  6.73it/s]evaluate for the 61-th batch, evaluate loss: 0.45263800024986267:  83%|██████████████▏  | 60/72 [00:04<00:00, 13.76it/s]Epoch: 2, train for the 133-th batch, train loss: 0.5384830832481384:  90%|█████████▉ | 132/146 [00:19<00:02,  6.46it/s]Epoch: 2, train for the 133-th batch, train loss: 0.5384830832481384:  91%|██████████ | 133/146 [00:19<00:02,  6.40it/s]evaluate for the 62-th batch, evaluate loss: 0.4515817165374756:  83%|███████████████   | 60/72 [00:04<00:00, 13.76it/s]evaluate for the 62-th batch, evaluate loss: 0.4515817165374756:  86%|███████████████▌  | 62/72 [00:04<00:00, 12.65it/s]Epoch: 1, train for the 216-th batch, train loss: 0.4841785728931427:  91%|█████████▉ | 215/237 [00:57<00:05,  3.75it/s]Epoch: 3, train for the 18-th batch, train loss: 0.47682130336761475:  14%|█▋          | 17/119 [00:02<00:15,  6.73it/s]Epoch: 3, train for the 18-th batch, train loss: 0.47682130336761475:  15%|█▊          | 18/119 [00:02<00:14,  7.08it/s]Epoch: 2, train for the 58-th batch, train loss: 0.5459532141685486:  38%|████▉        | 57/151 [00:08<00:14,  6.65it/s]Epoch: 1, train for the 209-th batch, train loss: 0.4292042553424835:  54%|█████▉     | 208/383 [00:56<00:49,  3.50it/s]Epoch: 2, train for the 58-th batch, train loss: 0.5459532141685486:  38%|████▉        | 58/151 [00:08<00:14,  6.24it/s]evaluate for the 63-th batch, evaluate loss: 0.4286191463470459:  86%|███████████████▌  | 62/72 [00:04<00:00, 12.65it/s]Epoch: 1, train for the 209-th batch, train loss: 0.4292042553424835:  55%|██████     | 209/383 [00:56<00:50,  3.48it/s]Epoch: 1, train for the 216-th batch, train loss: 0.4841785728931427:  91%|██████████ | 216/237 [00:57<00:05,  3.55it/s]Epoch: 2, train for the 134-th batch, train loss: 0.4732184410095215:  91%|██████████ | 133/146 [00:19<00:02,  6.40it/s]Epoch: 2, train for the 134-th batch, train loss: 0.4732184410095215:  92%|██████████ | 134/146 [00:19<00:01,  6.68it/s]evaluate for the 64-th batch, evaluate loss: 0.5278446078300476:  86%|███████████████▌  | 62/72 [00:04<00:00, 12.65it/s]evaluate for the 64-th batch, evaluate loss: 0.5278446078300476:  89%|████████████████  | 64/72 [00:04<00:00, 13.49it/s]Epoch: 3, train for the 19-th batch, train loss: 0.4967101812362671:  15%|█▉           | 18/119 [00:02<00:14,  7.08it/s]Epoch: 3, train for the 19-th batch, train loss: 0.4967101812362671:  16%|██           | 19/119 [00:02<00:13,  7.45it/s]evaluate for the 65-th batch, evaluate loss: 0.5138256549835205:  89%|████████████████  | 64/72 [00:04<00:00, 13.49it/s]Epoch: 2, train for the 59-th batch, train loss: 0.5512830018997192:  38%|████▉        | 58/151 [00:09<00:14,  6.24it/s]Epoch: 2, train for the 59-th batch, train loss: 0.5512830018997192:  39%|█████        | 59/151 [00:09<00:15,  5.94it/s]Epoch: 2, train for the 135-th batch, train loss: 0.5177382826805115:  92%|██████████ | 134/146 [00:19<00:01,  6.68it/s]evaluate for the 66-th batch, evaluate loss: 0.4481455981731415:  89%|████████████████  | 64/72 [00:04<00:00, 13.49it/s]evaluate for the 66-th batch, evaluate loss: 0.4481455981731415:  92%|████████████████▌ | 66/72 [00:04<00:00, 12.97it/s]Epoch: 2, train for the 135-th batch, train loss: 0.5177382826805115:  92%|██████████▏| 135/146 [00:19<00:01,  6.19it/s]Epoch: 3, train for the 20-th batch, train loss: 0.4830796420574188:  16%|██           | 19/119 [00:02<00:13,  7.45it/s]Epoch: 3, train for the 20-th batch, train loss: 0.4830796420574188:  17%|██▏          | 20/119 [00:02<00:13,  7.19it/s]Epoch: 1, train for the 217-th batch, train loss: 0.4845639765262604:  91%|██████████ | 216/237 [00:57<00:05,  3.55it/s]Epoch: 1, train for the 210-th batch, train loss: 0.4782048761844635:  55%|██████     | 209/383 [00:56<00:50,  3.48it/s]Epoch: 1, train for the 217-th batch, train loss: 0.4845639765262604:  92%|██████████ | 217/237 [00:57<00:05,  3.55it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5452784895896912:  39%|█████        | 59/151 [00:09<00:15,  5.94it/s]evaluate for the 67-th batch, evaluate loss: 0.4605470895767212:  92%|████████████████▌ | 66/72 [00:04<00:00, 12.97it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5452784895896912:  40%|█████▏       | 60/151 [00:09<00:14,  6.23it/s]Epoch: 1, train for the 210-th batch, train loss: 0.4782048761844635:  55%|██████     | 210/383 [00:56<00:50,  3.40it/s]evaluate for the 68-th batch, evaluate loss: 0.4549979269504547:  92%|████████████████▌ | 66/72 [00:04<00:00, 12.97it/s]evaluate for the 68-th batch, evaluate loss: 0.4549979269504547:  94%|█████████████████ | 68/72 [00:04<00:00, 13.34it/s]Epoch: 2, train for the 136-th batch, train loss: 0.4946359097957611:  92%|██████████▏| 135/146 [00:19<00:01,  6.19it/s]Epoch: 3, train for the 21-th batch, train loss: 0.498718798160553:  17%|██▎           | 20/119 [00:03<00:13,  7.19it/s]Epoch: 2, train for the 136-th batch, train loss: 0.4946359097957611:  93%|██████████▏| 136/146 [00:19<00:01,  5.84it/s]Epoch: 3, train for the 21-th batch, train loss: 0.498718798160553:  18%|██▍           | 21/119 [00:03<00:15,  6.49it/s]evaluate for the 69-th batch, evaluate loss: 0.46951788663864136:  94%|████████████████ | 68/72 [00:05<00:00, 13.34it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5651264786720276:  40%|█████▏       | 60/151 [00:09<00:14,  6.23it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5651264786720276:  40%|█████▎       | 61/151 [00:09<00:15,  5.98it/s]evaluate for the 70-th batch, evaluate loss: 0.4670318365097046:  94%|█████████████████ | 68/72 [00:05<00:00, 13.34it/s]evaluate for the 70-th batch, evaluate loss: 0.4670318365097046:  97%|█████████████████▌| 70/72 [00:05<00:00, 13.18it/s]Epoch: 1, train for the 218-th batch, train loss: 0.5635077357292175:  92%|██████████ | 217/237 [00:58<00:05,  3.55it/s]Epoch: 1, train for the 211-th batch, train loss: 0.43062788248062134:  55%|█████▍    | 210/383 [00:56<00:50,  3.40it/s]Epoch: 1, train for the 218-th batch, train loss: 0.5635077357292175:  92%|██████████ | 218/237 [00:58<00:05,  3.60it/s]evaluate for the 71-th batch, evaluate loss: 0.5101353526115417:  97%|█████████████████▌| 70/72 [00:05<00:00, 13.18it/s]Epoch: 1, train for the 211-th batch, train loss: 0.43062788248062134:  55%|█████▌    | 211/383 [00:56<00:49,  3.51it/s]Epoch: 2, train for the 137-th batch, train loss: 0.4970313012599945:  93%|██████████▏| 136/146 [00:19<00:01,  5.84it/s]Epoch: 3, train for the 22-th batch, train loss: 0.45853328704833984:  18%|██          | 21/119 [00:03<00:15,  6.49it/s]Epoch: 3, train for the 22-th batch, train loss: 0.45853328704833984:  18%|██▏         | 22/119 [00:03<00:16,  6.00it/s]Epoch: 2, train for the 137-th batch, train loss: 0.4970313012599945:  94%|██████████▎| 137/146 [00:19<00:01,  5.52it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5830893516540527:  40%|█████▎       | 61/151 [00:09<00:15,  5.98it/s]evaluate for the 72-th batch, evaluate loss: 0.5602645874023438:  97%|█████████████████▌| 70/72 [00:05<00:00, 13.18it/s]evaluate for the 72-th batch, evaluate loss: 0.5602645874023438: 100%|██████████████████| 72/72 [00:05<00:00, 13.62it/s]evaluate for the 72-th batch, evaluate loss: 0.5602645874023438: 100%|██████████████████| 72/72 [00:05<00:00, 13.79it/s]
Epoch: 2, train for the 62-th batch, train loss: 0.5830893516540527:  41%|█████▎       | 62/151 [00:09<00:14,  6.11it/s]Epoch: 3, train for the 23-th batch, train loss: 0.5313903093338013:  18%|██▍          | 22/119 [00:03<00:16,  6.00it/s]Epoch: 1, train for the 219-th batch, train loss: 0.5554378628730774:  92%|██████████ | 218/237 [00:58<00:05,  3.60it/s]Epoch: 3, train for the 23-th batch, train loss: 0.5313903093338013:  19%|██▌          | 23/119 [00:03<00:15,  6.30it/s]Epoch: 2, train for the 63-th batch, train loss: 0.6220855712890625:  41%|█████▎       | 62/151 [00:09<00:14,  6.11it/s]Epoch: 2, train for the 63-th batch, train loss: 0.6220855712890625:  42%|█████▍       | 63/151 [00:09<00:13,  6.49it/s]Epoch: 2, train for the 138-th batch, train loss: 0.5101413130760193:  94%|██████████▎| 137/146 [00:20<00:01,  5.52it/s]Epoch: 1, train for the 219-th batch, train loss: 0.5554378628730774:  92%|██████████▏| 219/237 [00:58<00:04,  3.80it/s]Epoch: 1, train for the 212-th batch, train loss: 0.5645337104797363:  55%|██████     | 211/383 [00:56<00:49,  3.51it/s]Epoch: 2, train for the 138-th batch, train loss: 0.5101413130760193:  95%|██████████▍| 138/146 [00:20<00:01,  5.51it/s]Epoch: 3, train for the 24-th batch, train loss: 0.3888600170612335:  19%|██▌          | 23/119 [00:03<00:15,  6.30it/s]Epoch: 3, train for the 24-th batch, train loss: 0.3888600170612335:  20%|██▌          | 24/119 [00:03<00:14,  6.79it/s]Epoch: 1, train for the 212-th batch, train loss: 0.5645337104797363:  55%|██████     | 212/383 [00:56<00:49,  3.49it/s]Epoch: 2, train for the 139-th batch, train loss: 0.5468839406967163:  95%|██████████▍| 138/146 [00:20<00:01,  5.51it/s]Epoch: 2, train for the 139-th batch, train loss: 0.5468839406967163:  95%|██████████▍| 139/146 [00:20<00:01,  5.80it/s]Epoch: 1, train for the 220-th batch, train loss: 0.5795475244522095:  92%|██████████▏| 219/237 [00:58<00:04,  3.80it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5627022981643677:  42%|█████▍       | 63/151 [00:09<00:13,  6.49it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5627022981643677:  42%|█████▌       | 64/151 [00:09<00:14,  5.89it/s]Epoch: 3, train for the 25-th batch, train loss: 0.46415388584136963:  20%|██▍         | 24/119 [00:03<00:14,  6.79it/s]Epoch: 3, train for the 25-th batch, train loss: 0.46415388584136963:  21%|██▌         | 25/119 [00:03<00:13,  7.10it/s]Epoch: 1, train for the 220-th batch, train loss: 0.5795475244522095:  93%|██████████▏| 220/237 [00:58<00:04,  3.93it/s]Epoch: 1, train for the 213-th batch, train loss: 0.545289933681488:  55%|██████▋     | 212/383 [00:57<00:49,  3.49it/s]Epoch: 2, train for the 140-th batch, train loss: 0.4800986647605896:  95%|██████████▍| 139/146 [00:20<00:01,  5.80it/s]Epoch: 2, train for the 140-th batch, train loss: 0.4800986647605896:  96%|██████████▌| 140/146 [00:20<00:01,  5.85it/s]Epoch: 3, train for the 26-th batch, train loss: 0.39894965291023254:  21%|██▌         | 25/119 [00:03<00:13,  7.10it/s]Epoch: 1, train for the 213-th batch, train loss: 0.545289933681488:  56%|██████▋     | 213/383 [00:57<00:46,  3.63it/s]Epoch: 3, train for the 26-th batch, train loss: 0.39894965291023254:  22%|██▌         | 26/119 [00:03<00:12,  7.30it/s]  0%|                                                                                            | 0/34 [00:00<?, ?it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5177029967308044:  42%|█████▌       | 64/151 [00:10<00:14,  5.89it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5177029967308044:  43%|█████▌       | 65/151 [00:10<00:15,  5.66it/s]Epoch: 1, train for the 221-th batch, train loss: 0.5578527450561523:  93%|██████████▏| 220/237 [00:58<00:04,  3.93it/s]evaluate for the 1-th batch, evaluate loss: 0.6148017644882202:   0%|                            | 0/34 [00:00<?, ?it/s]Epoch: 1, train for the 221-th batch, train loss: 0.5578527450561523:  93%|██████████▎| 221/237 [00:58<00:03,  4.06it/s]Epoch: 2, train for the 141-th batch, train loss: 0.5257065296173096:  96%|██████████▌| 140/146 [00:20<00:01,  5.85it/s]Epoch: 3, train for the 27-th batch, train loss: 0.5172939300537109:  22%|██▊          | 26/119 [00:04<00:12,  7.30it/s]Epoch: 2, train for the 141-th batch, train loss: 0.5257065296173096:  97%|██████████▌| 141/146 [00:20<00:00,  5.90it/s]Epoch: 3, train for the 27-th batch, train loss: 0.5172939300537109:  23%|██▉          | 27/119 [00:04<00:13,  6.89it/s]evaluate for the 2-th batch, evaluate loss: 0.6786507964134216:   0%|                            | 0/34 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.6786507964134216:   6%|█▏                  | 2/34 [00:00<00:02, 11.60it/s]Epoch: 2, train for the 66-th batch, train loss: 0.5547478199005127:  43%|█████▌       | 65/151 [00:10<00:15,  5.66it/s]Epoch: 1, train for the 214-th batch, train loss: 0.4641771614551544:  56%|██████     | 213/383 [00:57<00:46,  3.63it/s]Epoch: 2, train for the 66-th batch, train loss: 0.5547478199005127:  44%|█████▋       | 66/151 [00:10<00:14,  5.78it/s]Epoch: 1, train for the 214-th batch, train loss: 0.4641771614551544:  56%|██████▏    | 214/383 [00:57<00:45,  3.70it/s]evaluate for the 3-th batch, evaluate loss: 0.6942522525787354:   6%|█▏                  | 2/34 [00:00<00:02, 11.60it/s]Epoch: 2, train for the 142-th batch, train loss: 0.44742169976234436:  97%|█████████▋| 141/146 [00:20<00:00,  5.90it/s]Epoch: 1, train for the 222-th batch, train loss: 0.5672695636749268:  93%|██████████▎| 221/237 [00:59<00:03,  4.06it/s]Epoch: 3, train for the 28-th batch, train loss: 0.4969887435436249:  23%|██▉          | 27/119 [00:04<00:13,  6.89it/s]Epoch: 2, train for the 142-th batch, train loss: 0.44742169976234436:  97%|█████████▋| 142/146 [00:20<00:00,  5.94it/s]Epoch: 3, train for the 28-th batch, train loss: 0.4969887435436249:  24%|███          | 28/119 [00:04<00:13,  6.65it/s]evaluate for the 4-th batch, evaluate loss: 0.6619051694869995:   6%|█▏                  | 2/34 [00:00<00:02, 11.60it/s]evaluate for the 4-th batch, evaluate loss: 0.6619051694869995:  12%|██▎                 | 4/34 [00:00<00:02, 12.09it/s]Epoch: 1, train for the 222-th batch, train loss: 0.5672695636749268:  94%|██████████▎| 222/237 [00:59<00:03,  4.13it/s]Epoch: 2, train for the 67-th batch, train loss: 0.5442878007888794:  44%|█████▋       | 66/151 [00:10<00:14,  5.78it/s]Epoch: 2, train for the 67-th batch, train loss: 0.5442878007888794:  44%|█████▊       | 67/151 [00:10<00:14,  5.85it/s]evaluate for the 5-th batch, evaluate loss: 0.655005931854248:  12%|██▍                  | 4/34 [00:00<00:02, 12.09it/s]Epoch: 1, train for the 215-th batch, train loss: 0.3953937590122223:  56%|██████▏    | 214/383 [00:57<00:45,  3.70it/s]Epoch: 3, train for the 29-th batch, train loss: 0.475806325674057:  24%|███▎          | 28/119 [00:04<00:13,  6.65it/s]Epoch: 3, train for the 29-th batch, train loss: 0.475806325674057:  24%|███▍          | 29/119 [00:04<00:13,  6.74it/s]evaluate for the 6-th batch, evaluate loss: 0.6024686694145203:  12%|██▎                 | 4/34 [00:00<00:02, 12.09it/s]evaluate for the 6-th batch, evaluate loss: 0.6024686694145203:  18%|███▌                | 6/34 [00:00<00:02, 12.82it/s]Epoch: 1, train for the 215-th batch, train loss: 0.3953937590122223:  56%|██████▏    | 215/383 [00:57<00:43,  3.86it/s]Epoch: 2, train for the 143-th batch, train loss: 0.5374248623847961:  97%|██████████▋| 142/146 [00:20<00:00,  5.94it/s]Epoch: 2, train for the 143-th batch, train loss: 0.5374248623847961:  98%|██████████▊| 143/146 [00:20<00:00,  5.65it/s]Epoch: 2, train for the 68-th batch, train loss: 0.5425888299942017:  44%|█████▊       | 67/151 [00:10<00:14,  5.85it/s]Epoch: 2, train for the 68-th batch, train loss: 0.5425888299942017:  45%|█████▊       | 68/151 [00:10<00:13,  5.94it/s]evaluate for the 7-th batch, evaluate loss: 0.5673010349273682:  18%|███▌                | 6/34 [00:00<00:02, 12.82it/s]Epoch: 1, train for the 223-th batch, train loss: 0.5584854483604431:  94%|██████████▎| 222/237 [00:59<00:03,  4.13it/s]Epoch: 3, train for the 30-th batch, train loss: 0.46409764885902405:  24%|██▉         | 29/119 [00:04<00:13,  6.74it/s]Epoch: 3, train for the 30-th batch, train loss: 0.46409764885902405:  25%|███         | 30/119 [00:04<00:12,  7.17it/s]Epoch: 1, train for the 223-th batch, train loss: 0.5584854483604431:  94%|██████████▎| 223/237 [00:59<00:03,  4.06it/s]evaluate for the 8-th batch, evaluate loss: 0.679881751537323:  18%|███▋                 | 6/34 [00:00<00:02, 12.82it/s]evaluate for the 8-th batch, evaluate loss: 0.679881751537323:  24%|████▉                | 8/34 [00:00<00:02, 12.81it/s]Epoch: 2, train for the 144-th batch, train loss: 0.47848406434059143:  98%|█████████▊| 143/146 [00:21<00:00,  5.65it/s]Epoch: 2, train for the 144-th batch, train loss: 0.47848406434059143:  99%|█████████▊| 144/146 [00:21<00:00,  5.70it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5320342183113098:  45%|█████▊       | 68/151 [00:10<00:13,  5.94it/s]Epoch: 1, train for the 216-th batch, train loss: 0.453571617603302:  56%|██████▋     | 215/383 [00:57<00:43,  3.86it/s]evaluate for the 9-th batch, evaluate loss: 0.5876103639602661:  24%|████▋               | 8/34 [00:00<00:02, 12.81it/s]Epoch: 3, train for the 31-th batch, train loss: 0.47293493151664734:  25%|███         | 30/119 [00:04<00:12,  7.17it/s]Epoch: 3, train for the 31-th batch, train loss: 0.47293493151664734:  26%|███▏        | 31/119 [00:04<00:12,  7.17it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5320342183113098:  46%|█████▉       | 69/151 [00:10<00:14,  5.77it/s]Epoch: 1, train for the 216-th batch, train loss: 0.453571617603302:  56%|██████▊     | 216/383 [00:57<00:43,  3.87it/s]evaluate for the 10-th batch, evaluate loss: 0.6381195783615112:  24%|████▍              | 8/34 [00:00<00:02, 12.81it/s]evaluate for the 10-th batch, evaluate loss: 0.6381195783615112:  29%|█████▎            | 10/34 [00:00<00:01, 13.53it/s]Epoch: 1, train for the 224-th batch, train loss: 0.5847759246826172:  94%|██████████▎| 223/237 [00:59<00:03,  4.06it/s]Epoch: 2, train for the 145-th batch, train loss: 0.514867901802063:  99%|███████████▊| 144/146 [00:21<00:00,  5.70it/s]Epoch: 2, train for the 145-th batch, train loss: 0.514867901802063:  99%|███████████▉| 145/146 [00:21<00:00,  6.04it/s]Epoch: 3, train for the 32-th batch, train loss: 0.4312589168548584:  26%|███▍         | 31/119 [00:04<00:12,  7.17it/s]Epoch: 3, train for the 32-th batch, train loss: 0.4312589168548584:  27%|███▍         | 32/119 [00:04<00:12,  7.17it/s]Epoch: 1, train for the 224-th batch, train loss: 0.5847759246826172:  95%|██████████▍| 224/237 [00:59<00:03,  4.00it/s]evaluate for the 11-th batch, evaluate loss: 0.6662580966949463:  29%|█████▎            | 10/34 [00:00<00:01, 13.53it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5646324157714844:  46%|█████▉       | 69/151 [00:10<00:14,  5.77it/s]Epoch: 1, train for the 217-th batch, train loss: 0.4752725064754486:  56%|██████▏    | 216/383 [00:58<00:43,  3.87it/s]evaluate for the 12-th batch, evaluate loss: 0.6150602102279663:  29%|█████▎            | 10/34 [00:00<00:01, 13.53it/s]evaluate for the 12-th batch, evaluate loss: 0.6150602102279663:  35%|██████▎           | 12/34 [00:00<00:01, 13.17it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5646324157714844:  46%|██████       | 70/151 [00:11<00:14,  5.49it/s]Epoch: 2, train for the 146-th batch, train loss: 0.4708483815193176:  99%|██████████▉| 145/146 [00:21<00:00,  6.04it/s]Epoch: 2, train for the 146-th batch, train loss: 0.4708483815193176: 100%|███████████| 146/146 [00:21<00:00,  6.68it/s]Epoch: 2, train for the 146-th batch, train loss: 0.4708483815193176: 100%|███████████| 146/146 [00:21<00:00,  6.83it/s]
Epoch: 1, train for the 217-th batch, train loss: 0.4752725064754486:  57%|██████▏    | 217/383 [00:58<00:41,  4.03it/s]evaluate for the 13-th batch, evaluate loss: 0.5686484575271606:  35%|██████▎           | 12/34 [00:00<00:01, 13.17it/s]Epoch: 3, train for the 33-th batch, train loss: 0.5009949207305908:  27%|███▍         | 32/119 [00:04<00:12,  7.17it/s]Epoch: 3, train for the 33-th batch, train loss: 0.5009949207305908:  28%|███▌         | 33/119 [00:04<00:12,  6.68it/s]Epoch: 1, train for the 225-th batch, train loss: 0.5740287899971008:  95%|██████████▍| 224/237 [00:59<00:03,  4.00it/s]evaluate for the 14-th batch, evaluate loss: 0.5523385405540466:  35%|██████▎           | 12/34 [00:01<00:01, 13.17it/s]evaluate for the 14-th batch, evaluate loss: 0.5523385405540466:  41%|███████▍          | 14/34 [00:01<00:01, 13.64it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5814248919487:  46%|███████▍        | 70/151 [00:11<00:14,  5.49it/s]Epoch: 1, train for the 225-th batch, train loss: 0.5740287899971008:  95%|██████████▍| 225/237 [00:59<00:02,  4.10it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5814248919487:  47%|███████▌        | 71/151 [00:11<00:14,  5.46it/s]evaluate for the 15-th batch, evaluate loss: 0.6734853982925415:  41%|███████▍          | 14/34 [00:01<00:01, 13.64it/s]Epoch: 3, train for the 34-th batch, train loss: 0.46228280663490295:  28%|███▎        | 33/119 [00:05<00:12,  6.68it/s]Epoch: 3, train for the 34-th batch, train loss: 0.46228280663490295:  29%|███▍        | 34/119 [00:05<00:11,  7.27it/s]Epoch: 1, train for the 218-th batch, train loss: 0.4984152913093567:  57%|██████▏    | 217/383 [00:58<00:41,  4.03it/s]  0%|                                                                                            | 0/38 [00:00<?, ?it/s]evaluate for the 16-th batch, evaluate loss: 0.6111915707588196:  41%|███████▍          | 14/34 [00:01<00:01, 13.64it/s]evaluate for the 16-th batch, evaluate loss: 0.6111915707588196:  47%|████████▍         | 16/34 [00:01<00:01, 13.80it/s]Epoch: 1, train for the 218-th batch, train loss: 0.4984152913093567:  57%|██████▎    | 218/383 [00:58<00:41,  3.99it/s]evaluate for the 1-th batch, evaluate loss: 0.4739793539047241:   0%|                            | 0/38 [00:00<?, ?it/s]Epoch: 2, train for the 72-th batch, train loss: 0.527527928352356:  47%|██████▌       | 71/151 [00:11<00:14,  5.46it/s]Epoch: 1, train for the 226-th batch, train loss: 0.591086745262146:  95%|███████████▍| 225/237 [01:00<00:02,  4.10it/s]evaluate for the 17-th batch, evaluate loss: 0.6513832211494446:  47%|████████▍         | 16/34 [00:01<00:01, 13.80it/s]Epoch: 2, train for the 72-th batch, train loss: 0.527527928352356:  48%|██████▋       | 72/151 [00:11<00:14,  5.29it/s]Epoch: 3, train for the 35-th batch, train loss: 0.454731285572052:  29%|████          | 34/119 [00:05<00:11,  7.27it/s]evaluate for the 2-th batch, evaluate loss: 0.49809733033180237:   0%|                           | 0/38 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.49809733033180237:   5%|█                  | 2/38 [00:00<00:02, 12.15it/s]Epoch: 3, train for the 35-th batch, train loss: 0.454731285572052:  29%|████          | 35/119 [00:05<00:13,  6.42it/s]Epoch: 1, train for the 226-th batch, train loss: 0.591086745262146:  95%|███████████▍| 226/237 [01:00<00:02,  3.99it/s]evaluate for the 18-th batch, evaluate loss: 0.5851770043373108:  47%|████████▍         | 16/34 [00:01<00:01, 13.80it/s]evaluate for the 18-th batch, evaluate loss: 0.5851770043373108:  53%|█████████▌        | 18/34 [00:01<00:01, 13.05it/s]evaluate for the 3-th batch, evaluate loss: 0.47214311361312866:   5%|█                  | 2/38 [00:00<00:02, 12.15it/s]Epoch: 1, train for the 219-th batch, train loss: 0.3343454599380493:  57%|██████▎    | 218/383 [00:58<00:41,  3.99it/s]Epoch: 1, train for the 219-th batch, train loss: 0.3343454599380493:  57%|██████▎    | 219/383 [00:58<00:39,  4.14it/s]evaluate for the 4-th batch, evaluate loss: 0.48698362708091736:   5%|█                  | 2/38 [00:00<00:02, 12.15it/s]evaluate for the 4-th batch, evaluate loss: 0.48698362708091736:  11%|██                 | 4/38 [00:00<00:02, 14.06it/s]evaluate for the 19-th batch, evaluate loss: 0.5616434216499329:  53%|█████████▌        | 18/34 [00:01<00:01, 13.05it/s]Epoch: 3, train for the 36-th batch, train loss: 0.4497966170310974:  29%|███▊         | 35/119 [00:05<00:13,  6.42it/s]Epoch: 2, train for the 73-th batch, train loss: 0.5409986972808838:  48%|██████▏      | 72/151 [00:11<00:14,  5.29it/s]Epoch: 3, train for the 36-th batch, train loss: 0.4497966170310974:  30%|███▉         | 36/119 [00:05<00:12,  6.41it/s]evaluate for the 5-th batch, evaluate loss: 0.5361703634262085:  11%|██                  | 4/38 [00:00<00:02, 14.06it/s]Epoch: 2, train for the 73-th batch, train loss: 0.5409986972808838:  48%|██████▎      | 73/151 [00:11<00:14,  5.24it/s]evaluate for the 20-th batch, evaluate loss: 0.620074987411499:  53%|██████████         | 18/34 [00:01<00:01, 13.05it/s]evaluate for the 20-th batch, evaluate loss: 0.620074987411499:  59%|███████████▏       | 20/34 [00:01<00:01, 12.86it/s]Epoch: 1, train for the 227-th batch, train loss: 0.5469340085983276:  95%|██████████▍| 226/237 [01:00<00:02,  3.99it/s]evaluate for the 6-th batch, evaluate loss: 0.4832804799079895:  11%|██                  | 4/38 [00:00<00:02, 14.06it/s]evaluate for the 6-th batch, evaluate loss: 0.4832804799079895:  16%|███▏                | 6/38 [00:00<00:02, 14.86it/s]Epoch: 1, train for the 227-th batch, train loss: 0.5469340085983276:  96%|██████████▌| 227/237 [01:00<00:02,  3.97it/s]evaluate for the 21-th batch, evaluate loss: 0.6178986430168152:  59%|██████████▌       | 20/34 [00:01<00:01, 12.86it/s]Epoch: 1, train for the 220-th batch, train loss: 0.39868563413619995:  57%|█████▋    | 219/383 [00:58<00:39,  4.14it/s]Epoch: 3, train for the 37-th batch, train loss: 0.5071945190429688:  30%|███▉         | 36/119 [00:05<00:12,  6.41it/s]evaluate for the 7-th batch, evaluate loss: 0.43847328424453735:  16%|███                | 6/38 [00:00<00:02, 14.86it/s]Epoch: 2, train for the 74-th batch, train loss: 0.4900231957435608:  48%|██████▎      | 73/151 [00:11<00:14,  5.24it/s]Epoch: 3, train for the 37-th batch, train loss: 0.5071945190429688:  31%|████         | 37/119 [00:05<00:13,  6.13it/s]evaluate for the 22-th batch, evaluate loss: 0.4067954421043396:  59%|██████████▌       | 20/34 [00:01<00:01, 12.86it/s]evaluate for the 22-th batch, evaluate loss: 0.4067954421043396:  65%|███████████▋      | 22/34 [00:01<00:00, 13.04it/s]Epoch: 2, train for the 74-th batch, train loss: 0.4900231957435608:  49%|██████▎      | 74/151 [00:11<00:14,  5.27it/s]Epoch: 1, train for the 220-th batch, train loss: 0.39868563413619995:  57%|█████▋    | 220/383 [00:58<00:41,  3.96it/s]evaluate for the 8-th batch, evaluate loss: 0.46098607778549194:  16%|███                | 6/38 [00:00<00:02, 14.86it/s]evaluate for the 8-th batch, evaluate loss: 0.46098607778549194:  21%|████               | 8/38 [00:00<00:02, 13.50it/s]evaluate for the 23-th batch, evaluate loss: 0.3646613359451294:  65%|███████████▋      | 22/34 [00:01<00:00, 13.04it/s]Epoch: 1, train for the 228-th batch, train loss: 0.5394229888916016:  96%|██████████▌| 227/237 [01:00<00:02,  3.97it/s]Epoch: 3, train for the 38-th batch, train loss: 0.4374157190322876:  31%|████         | 37/119 [00:05<00:13,  6.13it/s]evaluate for the 24-th batch, evaluate loss: 0.5249968767166138:  65%|███████████▋      | 22/34 [00:01<00:00, 13.04it/s]evaluate for the 24-th batch, evaluate loss: 0.5249968767166138:  71%|████████████▋     | 24/34 [00:01<00:00, 13.47it/s]evaluate for the 9-th batch, evaluate loss: 0.4834119975566864:  21%|████▏               | 8/38 [00:00<00:02, 13.50it/s]Epoch: 3, train for the 38-th batch, train loss: 0.4374157190322876:  32%|████▏        | 38/119 [00:05<00:12,  6.26it/s]Epoch: 1, train for the 228-th batch, train loss: 0.5394229888916016:  96%|██████████▌| 228/237 [01:00<00:02,  4.13it/s]Epoch: 2, train for the 75-th batch, train loss: 0.541479229927063:  49%|██████▊       | 74/151 [00:11<00:14,  5.27it/s]Epoch: 2, train for the 75-th batch, train loss: 0.541479229927063:  50%|██████▉       | 75/151 [00:11<00:14,  5.38it/s]evaluate for the 10-th batch, evaluate loss: 0.5132730603218079:  21%|████               | 8/38 [00:00<00:02, 13.50it/s]evaluate for the 10-th batch, evaluate loss: 0.5132730603218079:  26%|████▋             | 10/38 [00:00<00:02, 13.93it/s]evaluate for the 25-th batch, evaluate loss: 0.5233505368232727:  71%|████████████▋     | 24/34 [00:01<00:00, 13.47it/s]Epoch: 1, train for the 221-th batch, train loss: 0.4477849006652832:  57%|██████▎    | 220/383 [00:59<00:41,  3.96it/s]evaluate for the 11-th batch, evaluate loss: 0.45872828364372253:  26%|████▍            | 10/38 [00:00<00:02, 13.93it/s]Epoch: 1, train for the 221-th batch, train loss: 0.4477849006652832:  58%|██████▎    | 221/383 [00:59<00:41,  3.88it/s]Epoch: 3, train for the 39-th batch, train loss: 0.4839370548725128:  32%|████▏        | 38/119 [00:05<00:12,  6.26it/s]Epoch: 3, train for the 39-th batch, train loss: 0.4839370548725128:  33%|████▎        | 39/119 [00:05<00:12,  6.21it/s]evaluate for the 26-th batch, evaluate loss: 0.622381329536438:  71%|█████████████▍     | 24/34 [00:02<00:00, 13.47it/s]evaluate for the 26-th batch, evaluate loss: 0.622381329536438:  76%|██████████████▌    | 26/34 [00:02<00:00, 12.66it/s]Epoch: 1, train for the 229-th batch, train loss: 0.612689733505249:  96%|███████████▌| 228/237 [01:00<00:02,  4.13it/s]evaluate for the 12-th batch, evaluate loss: 0.533649742603302:  26%|█████              | 10/38 [00:00<00:02, 13.93it/s]evaluate for the 12-th batch, evaluate loss: 0.533649742603302:  32%|██████             | 12/38 [00:00<00:01, 13.64it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5440883040428162:  50%|██████▍      | 75/151 [00:12<00:14,  5.38it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5440883040428162:  50%|██████▌      | 76/151 [00:12<00:13,  5.47it/s]Epoch: 1, train for the 229-th batch, train loss: 0.612689733505249:  97%|███████████▌| 229/237 [01:00<00:01,  4.15it/s]evaluate for the 13-th batch, evaluate loss: 0.4960498809814453:  32%|█████▋            | 12/38 [00:00<00:01, 13.64it/s]evaluate for the 27-th batch, evaluate loss: 0.6387868523597717:  76%|█████████████▊    | 26/34 [00:02<00:00, 12.66it/s]Epoch: 3, train for the 40-th batch, train loss: 0.4695214629173279:  33%|████▎        | 39/119 [00:06<00:12,  6.21it/s]Epoch: 3, train for the 40-th batch, train loss: 0.4695214629173279:  34%|████▎        | 40/119 [00:06<00:12,  6.36it/s]evaluate for the 14-th batch, evaluate loss: 0.43076422810554504:  32%|█████▎           | 12/38 [00:00<00:01, 13.64it/s]evaluate for the 14-th batch, evaluate loss: 0.43076422810554504:  37%|██████▎          | 14/38 [00:00<00:01, 14.57it/s]evaluate for the 28-th batch, evaluate loss: 0.6069062948226929:  76%|█████████████▊    | 26/34 [00:02<00:00, 12.66it/s]evaluate for the 28-th batch, evaluate loss: 0.6069062948226929:  82%|██████████████▊   | 28/34 [00:02<00:00, 11.62it/s]Epoch: 1, train for the 222-th batch, train loss: 0.49385568499565125:  58%|█████▊    | 221/383 [00:59<00:41,  3.88it/s]evaluate for the 15-th batch, evaluate loss: 0.4516344666481018:  37%|██████▋           | 14/38 [00:01<00:01, 14.57it/s]Epoch: 2, train for the 77-th batch, train loss: 0.4999077618122101:  50%|██████▌      | 76/151 [00:12<00:13,  5.47it/s]Epoch: 2, train for the 77-th batch, train loss: 0.4999077618122101:  51%|██████▋      | 77/151 [00:12<00:13,  5.37it/s]Epoch: 1, train for the 222-th batch, train loss: 0.49385568499565125:  58%|█████▊    | 222/383 [00:59<00:42,  3.81it/s]Epoch: 1, train for the 230-th batch, train loss: 0.5509843826293945:  97%|██████████▋| 229/237 [01:00<00:01,  4.15it/s]evaluate for the 29-th batch, evaluate loss: 0.5821056962013245:  82%|██████████████▊   | 28/34 [00:02<00:00, 11.62it/s]Epoch: 3, train for the 41-th batch, train loss: 0.4723705053329468:  34%|████▎        | 40/119 [00:06<00:12,  6.36it/s]evaluate for the 16-th batch, evaluate loss: 0.5177401304244995:  37%|██████▋           | 14/38 [00:01<00:01, 14.57it/s]evaluate for the 16-th batch, evaluate loss: 0.5177401304244995:  42%|███████▌          | 16/38 [00:01<00:01, 14.39it/s]Epoch: 3, train for the 41-th batch, train loss: 0.4723705053329468:  34%|████▍        | 41/119 [00:06<00:12,  6.39it/s]Epoch: 1, train for the 230-th batch, train loss: 0.5509843826293945:  97%|██████████▋| 230/237 [01:01<00:01,  3.92it/s]evaluate for the 30-th batch, evaluate loss: 0.5633889436721802:  82%|██████████████▊   | 28/34 [00:02<00:00, 11.62it/s]evaluate for the 30-th batch, evaluate loss: 0.5633889436721802:  88%|███████████████▉  | 30/34 [00:02<00:00, 11.91it/s]evaluate for the 17-th batch, evaluate loss: 0.4709429144859314:  42%|███████▌          | 16/38 [00:01<00:01, 14.39it/s]Epoch: 2, train for the 78-th batch, train loss: 0.503480076789856:  51%|███████▏      | 77/151 [00:12<00:13,  5.37it/s]Epoch: 2, train for the 78-th batch, train loss: 0.503480076789856:  52%|███████▏      | 78/151 [00:12<00:13,  5.31it/s]Epoch: 3, train for the 42-th batch, train loss: 0.507745623588562:  34%|████▊         | 41/119 [00:06<00:12,  6.39it/s]evaluate for the 18-th batch, evaluate loss: 0.5077822804450989:  42%|███████▌          | 16/38 [00:01<00:01, 14.39it/s]evaluate for the 18-th batch, evaluate loss: 0.5077822804450989:  47%|████████▌         | 18/38 [00:01<00:01, 13.66it/s]Epoch: 3, train for the 42-th batch, train loss: 0.507745623588562:  35%|████▉         | 42/119 [00:06<00:12,  6.36it/s]Epoch: 1, train for the 223-th batch, train loss: 0.43366050720214844:  58%|█████▊    | 222/383 [00:59<00:42,  3.81it/s]evaluate for the 19-th batch, evaluate loss: 0.49207350611686707:  47%|████████         | 18/38 [00:01<00:01, 13.66it/s]Epoch: 1, train for the 223-th batch, train loss: 0.43366050720214844:  58%|█████▊    | 223/383 [00:59<00:43,  3.70it/s]Epoch: 1, train for the 231-th batch, train loss: 0.5708793997764587:  97%|██████████▋| 230/237 [01:01<00:01,  3.92it/s]Epoch: 2, train for the 79-th batch, train loss: 0.5878108739852905:  52%|██████▋      | 78/151 [00:12<00:13,  5.31it/s]evaluate for the 20-th batch, evaluate loss: 0.4286554753780365:  47%|████████▌         | 18/38 [00:01<00:01, 13.66it/s]evaluate for the 20-th batch, evaluate loss: 0.4286554753780365:  53%|█████████▍        | 20/38 [00:01<00:01, 14.95it/s]Epoch: 2, train for the 79-th batch, train loss: 0.5878108739852905:  52%|██████▊      | 79/151 [00:12<00:12,  5.80it/s]evaluate for the 31-th batch, evaluate loss: 0.6383236646652222:  88%|███████████████▉  | 30/34 [00:02<00:00, 11.91it/s]Epoch: 1, train for the 231-th batch, train loss: 0.5708793997764587:  97%|██████████▋| 231/237 [01:01<00:01,  4.04it/s]Epoch: 3, train for the 43-th batch, train loss: 0.4835120737552643:  35%|████▌        | 42/119 [00:06<00:12,  6.36it/s]evaluate for the 21-th batch, evaluate loss: 0.44088828563690186:  53%|████████▉        | 20/38 [00:01<00:01, 14.95it/s]Epoch: 3, train for the 43-th batch, train loss: 0.4835120737552643:  36%|████▋        | 43/119 [00:06<00:12,  6.30it/s]evaluate for the 32-th batch, evaluate loss: 0.6512294411659241:  88%|███████████████▉  | 30/34 [00:02<00:00, 11.91it/s]evaluate for the 32-th batch, evaluate loss: 0.6512294411659241:  94%|████████████████▉ | 32/34 [00:02<00:00,  9.49it/s]evaluate for the 22-th batch, evaluate loss: 0.48511791229248047:  53%|████████▉        | 20/38 [00:01<00:01, 14.95it/s]evaluate for the 22-th batch, evaluate loss: 0.48511791229248047:  58%|█████████▊       | 22/38 [00:01<00:01, 14.31it/s]Epoch: 2, train for the 80-th batch, train loss: 0.5452489852905273:  52%|██████▊      | 79/151 [00:12<00:12,  5.80it/s]Epoch: 2, train for the 80-th batch, train loss: 0.5452489852905273:  53%|██████▉      | 80/151 [00:12<00:12,  5.79it/s]evaluate for the 33-th batch, evaluate loss: 0.6550988554954529:  94%|████████████████▉ | 32/34 [00:02<00:00,  9.49it/s]Epoch: 3, train for the 44-th batch, train loss: 0.4343799948692322:  36%|████▋        | 43/119 [00:06<00:12,  6.30it/s]evaluate for the 23-th batch, evaluate loss: 0.47494253516197205:  58%|█████████▊       | 22/38 [00:01<00:01, 14.31it/s]Epoch: 3, train for the 44-th batch, train loss: 0.4343799948692322:  37%|████▊        | 44/119 [00:06<00:12,  5.78it/s]evaluate for the 34-th batch, evaluate loss: 0.7062582969665527:  94%|████████████████▉ | 32/34 [00:02<00:00,  9.49it/s]evaluate for the 34-th batch, evaluate loss: 0.7062582969665527: 100%|██████████████████| 34/34 [00:02<00:00, 10.33it/s]evaluate for the 34-th batch, evaluate loss: 0.7062582969665527: 100%|██████████████████| 34/34 [00:02<00:00, 12.02it/s]
evaluate for the 24-th batch, evaluate loss: 0.4552414119243622:  58%|██████████▍       | 22/38 [00:01<00:01, 14.31it/s]evaluate for the 24-th batch, evaluate loss: 0.4552414119243622:  63%|███████████▎      | 24/38 [00:01<00:01, 13.85it/s]Epoch: 1, train for the 224-th batch, train loss: 0.5070871114730835:  58%|██████▍    | 223/383 [01:00<00:43,  3.70it/s]Epoch: 1, train for the 232-th batch, train loss: 0.586107611656189:  97%|███████████▋| 231/237 [01:01<00:01,  4.04it/s]Epoch: 1, train for the 232-th batch, train loss: 0.586107611656189:  98%|███████████▋| 232/237 [01:01<00:01,  3.66it/s]Epoch: 2, train for the 81-th batch, train loss: 0.5216326713562012:  53%|██████▉      | 80/151 [00:13<00:12,  5.79it/s]Epoch: 1, train for the 224-th batch, train loss: 0.5070871114730835:  58%|██████▍    | 224/383 [01:00<00:48,  3.25it/s]Epoch: 2, train for the 81-th batch, train loss: 0.5216326713562012:  54%|██████▉      | 81/151 [00:13<00:12,  5.41it/s]Epoch: 3, train for the 45-th batch, train loss: 0.45123225450515747:  37%|████▍       | 44/119 [00:06<00:12,  5.78it/s]evaluate for the 25-th batch, evaluate loss: 0.49168500304222107:  63%|██████████▋      | 24/38 [00:01<00:01, 13.85it/s]Epoch: 3, train for the 45-th batch, train loss: 0.45123225450515747:  38%|████▌       | 45/119 [00:06<00:12,  6.07it/s]evaluate for the 26-th batch, evaluate loss: 0.4791783094406128:  63%|███████████▎      | 24/38 [00:01<00:01, 13.85it/s]evaluate for the 26-th batch, evaluate loss: 0.4791783094406128:  68%|████████████▎     | 26/38 [00:01<00:00, 12.76it/s]Epoch: 1, train for the 233-th batch, train loss: 0.5831287503242493:  98%|██████████▊| 232/237 [01:01<00:01,  3.66it/s]INFO:root:Epoch: 1, learning rate: 0.0001, train loss: 0.5264
INFO:root:train average_precision, 0.8354
INFO:root:train roc_auc, 0.8026
INFO:root:validate loss: 0.4825
INFO:root:validate average_precision, 0.8735
INFO:root:validate roc_auc, 0.8549
INFO:root:new node validate loss: 0.6055
INFO:root:new node validate first_1_average_precision, 0.5085
INFO:root:new node validate first_1_roc_auc, 0.5018
INFO:root:new node validate first_3_average_precision, 0.6000
INFO:root:new node validate first_3_roc_auc, 0.5844
INFO:root:new node validate first_10_average_precision, 0.6988
INFO:root:new node validate first_10_roc_auc, 0.6772
INFO:root:new node validate average_precision, 0.7565
INFO:root:new node validate roc_auc, 0.7239
INFO:root:save model ./saved_models/TGN/ia-movielens-user2tags-10m/TGN_seed0_dummy-n0qq8qen/TGN_seed0_dummy-n0qq8qen.pkl
Epoch: 3, train for the 46-th batch, train loss: 0.4771568179130554:  38%|████▉        | 45/119 [00:07<00:12,  6.07it/s]Epoch: 1, train for the 233-th batch, train loss: 0.5831287503242493:  98%|██████████▊| 233/237 [01:01<00:01,  3.91it/s]evaluate for the 27-th batch, evaluate loss: 0.4777672588825226:  68%|████████████▎     | 26/38 [00:01<00:00, 12.76it/s]Epoch: 3, train for the 46-th batch, train loss: 0.4771568179130554:  39%|█████        | 46/119 [00:07<00:12,  5.90it/s]Epoch: 2, train for the 82-th batch, train loss: 0.5206708312034607:  54%|██████▉      | 81/151 [00:13<00:12,  5.41it/s]Epoch: 2, train for the 82-th batch, train loss: 0.5206708312034607:  54%|███████      | 82/151 [00:13<00:13,  5.16it/s]Epoch: 1, train for the 225-th batch, train loss: 0.43153947591781616:  58%|█████▊    | 224/383 [01:00<00:48,  3.25it/s]evaluate for the 28-th batch, evaluate loss: 0.4993583858013153:  68%|████████████▎     | 26/38 [00:02<00:00, 12.76it/s]evaluate for the 28-th batch, evaluate loss: 0.4993583858013153:  74%|█████████████▎    | 28/38 [00:02<00:00, 13.19it/s]Epoch: 1, train for the 225-th batch, train loss: 0.43153947591781616:  59%|█████▊    | 225/383 [01:00<00:48,  3.29it/s]evaluate for the 29-th batch, evaluate loss: 0.46868133544921875:  74%|████████████▌    | 28/38 [00:02<00:00, 13.19it/s]Epoch: 3, train for the 47-th batch, train loss: 0.5537055134773254:  39%|█████        | 46/119 [00:07<00:12,  5.90it/s]Epoch: 3, train for the 47-th batch, train loss: 0.5537055134773254:  39%|█████▏       | 47/119 [00:07<00:11,  6.07it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5260310769081116:  54%|███████      | 82/151 [00:13<00:13,  5.16it/s]evaluate for the 30-th batch, evaluate loss: 0.5008180737495422:  74%|█████████████▎    | 28/38 [00:02<00:00, 13.19it/s]evaluate for the 30-th batch, evaluate loss: 0.5008180737495422:  79%|██████████████▏   | 30/38 [00:02<00:00, 13.75it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5260310769081116:  55%|███████▏     | 83/151 [00:13<00:12,  5.45it/s]Epoch: 1, train for the 234-th batch, train loss: 0.5562204718589783:  98%|██████████▊| 233/237 [01:02<00:01,  3.91it/s]evaluate for the 31-th batch, evaluate loss: 0.47175654768943787:  79%|█████████████▍   | 30/38 [00:02<00:00, 13.75it/s]Epoch: 1, train for the 234-th batch, train loss: 0.5562204718589783:  99%|██████████▊| 234/237 [01:02<00:00,  3.75it/s]Epoch: 3, train for the 48-th batch, train loss: 0.50596684217453:  39%|█████▉         | 47/119 [00:07<00:11,  6.07it/s]Epoch: 3, train for the 48-th batch, train loss: 0.50596684217453:  40%|██████         | 48/119 [00:07<00:11,  6.40it/s]evaluate for the 32-th batch, evaluate loss: 0.4418356716632843:  79%|██████████████▏   | 30/38 [00:02<00:00, 13.75it/s]evaluate for the 32-th batch, evaluate loss: 0.4418356716632843:  84%|███████████████▏  | 32/38 [00:02<00:00, 14.53it/s]Epoch: 1, train for the 226-th batch, train loss: 0.48993074893951416:  59%|█████▊    | 225/383 [01:00<00:48,  3.29it/s]evaluate for the 33-th batch, evaluate loss: 0.47322332859039307:  84%|██████████████▎  | 32/38 [00:02<00:00, 14.53it/s]Epoch: 1, train for the 226-th batch, train loss: 0.48993074893951416:  59%|█████▉    | 226/383 [01:00<00:45,  3.42it/s]Epoch: 3, train for the 49-th batch, train loss: 0.4746231734752655:  40%|█████▏       | 48/119 [00:07<00:11,  6.40it/s]evaluate for the 34-th batch, evaluate loss: 0.4824378490447998:  84%|███████████████▏  | 32/38 [00:02<00:00, 14.53it/s]evaluate for the 34-th batch, evaluate loss: 0.4824378490447998:  89%|████████████████  | 34/38 [00:02<00:00, 14.26it/s]Epoch: 3, train for the 49-th batch, train loss: 0.4746231734752655:  41%|█████▎       | 49/119 [00:07<00:10,  6.44it/s]Epoch: 2, train for the 84-th batch, train loss: 0.565078854560852:  55%|███████▋      | 83/151 [00:13<00:12,  5.45it/s]Epoch: 2, train for the 84-th batch, train loss: 0.565078854560852:  56%|███████▊      | 84/151 [00:13<00:14,  4.62it/s]evaluate for the 35-th batch, evaluate loss: 0.49359405040740967:  89%|███████████████▏ | 34/38 [00:02<00:00, 14.26it/s]Epoch: 1, train for the 235-th batch, train loss: 0.5333093404769897:  99%|██████████▊| 234/237 [01:02<00:00,  3.75it/s]Epoch: 1, train for the 227-th batch, train loss: 0.5265859961509705:  59%|██████▍    | 226/383 [01:00<00:45,  3.42it/s]Epoch: 1, train for the 235-th batch, train loss: 0.5333093404769897:  99%|██████████▉| 235/237 [01:02<00:00,  3.69it/s]Epoch: 3, train for the 50-th batch, train loss: 0.510267972946167:  41%|█████▊        | 49/119 [00:07<00:10,  6.44it/s]evaluate for the 36-th batch, evaluate loss: 0.508038341999054:  89%|█████████████████  | 34/38 [00:02<00:00, 14.26it/s]evaluate for the 36-th batch, evaluate loss: 0.508038341999054:  95%|██████████████████ | 36/38 [00:02<00:00, 14.25it/s]Epoch: 3, train for the 50-th batch, train loss: 0.510267972946167:  42%|█████▉        | 50/119 [00:07<00:10,  6.67it/s]Epoch: 2, train for the 85-th batch, train loss: 0.4900067448616028:  56%|███████▏     | 84/151 [00:13<00:14,  4.62it/s]Epoch: 1, train for the 227-th batch, train loss: 0.5265859961509705:  59%|██████▌    | 227/383 [01:00<00:43,  3.57it/s]Epoch: 2, train for the 85-th batch, train loss: 0.4900067448616028:  56%|███████▎     | 85/151 [00:13<00:12,  5.16it/s]evaluate for the 37-th batch, evaluate loss: 0.4372161030769348:  95%|█████████████████ | 36/38 [00:02<00:00, 14.25it/s]Epoch: 3, train for the 51-th batch, train loss: 0.5041484832763672:  42%|█████▍       | 50/119 [00:07<00:10,  6.67it/s]Epoch: 3, train for the 51-th batch, train loss: 0.5041484832763672:  43%|█████▌       | 51/119 [00:07<00:10,  6.77it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5082495212554932:  56%|███████▎     | 85/151 [00:13<00:12,  5.16it/s]Epoch: 1, train for the 236-th batch, train loss: 0.5419920086860657:  99%|██████████▉| 235/237 [01:02<00:00,  3.69it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5082495212554932:  57%|███████▍     | 86/151 [00:13<00:11,  5.72it/s]evaluate for the 38-th batch, evaluate loss: 0.4659723937511444:  95%|█████████████████ | 36/38 [00:02<00:00, 14.25it/s]evaluate for the 38-th batch, evaluate loss: 0.4659723937511444: 100%|██████████████████| 38/38 [00:02<00:00, 13.38it/s]evaluate for the 38-th batch, evaluate loss: 0.4659723937511444: 100%|██████████████████| 38/38 [00:02<00:00, 13.85it/s]
Epoch: 1, train for the 236-th batch, train loss: 0.5419920086860657: 100%|██████████▉| 236/237 [01:02<00:00,  3.95it/s]Epoch: 1, train for the 228-th batch, train loss: 0.4266708195209503:  59%|██████▌    | 227/383 [01:01<00:43,  3.57it/s]Epoch: 2, train for the 87-th batch, train loss: 0.4942648708820343:  57%|███████▍     | 86/151 [00:14<00:11,  5.72it/s]Epoch: 2, train for the 87-th batch, train loss: 0.4942648708820343:  58%|███████▍     | 87/151 [00:14<00:10,  6.35it/s]Epoch: 1, train for the 228-th batch, train loss: 0.4266708195209503:  60%|██████▌    | 228/383 [01:01<00:42,  3.64it/s]Epoch: 3, train for the 52-th batch, train loss: 0.528115451335907:  43%|██████        | 51/119 [00:07<00:10,  6.77it/s]Epoch: 3, train for the 52-th batch, train loss: 0.528115451335907:  44%|██████        | 52/119 [00:07<00:10,  6.41it/s]Epoch: 1, train for the 237-th batch, train loss: 0.5096003413200378: 100%|██████████▉| 236/237 [01:02<00:00,  3.95it/s]  0%|                                                                                            | 0/20 [00:00<?, ?it/s]Epoch: 1, train for the 237-th batch, train loss: 0.5096003413200378: 100%|███████████| 237/237 [01:02<00:00,  4.30it/s]Epoch: 1, train for the 237-th batch, train loss: 0.5096003413200378: 100%|███████████| 237/237 [01:02<00:00,  3.77it/s]
Epoch: 2, train for the 88-th batch, train loss: 0.5455291271209717:  58%|███████▍     | 87/151 [00:14<00:10,  6.35it/s]Epoch: 2, train for the 88-th batch, train loss: 0.5455291271209717:  58%|███████▌     | 88/151 [00:14<00:09,  6.91it/s]evaluate for the 1-th batch, evaluate loss: 0.7298118472099304:   0%|                            | 0/20 [00:00<?, ?it/s]Epoch: 3, train for the 53-th batch, train loss: 0.47606557607650757:  44%|█████▏      | 52/119 [00:08<00:10,  6.41it/s]Epoch: 3, train for the 53-th batch, train loss: 0.47606557607650757:  45%|█████▎      | 53/119 [00:08<00:09,  6.70it/s]evaluate for the 2-th batch, evaluate loss: 0.7649415135383606:   0%|                            | 0/20 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.7649415135383606:  10%|██                  | 2/20 [00:00<00:00, 18.13it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5305291414260864:  58%|███████▌     | 88/151 [00:14<00:09,  6.91it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5305291414260864:  59%|███████▋     | 89/151 [00:14<00:08,  7.32it/s]evaluate for the 3-th batch, evaluate loss: 0.6344153881072998:  10%|██                  | 2/20 [00:00<00:00, 18.13it/s]Epoch: 1, train for the 229-th batch, train loss: 0.37258657813072205:  60%|█████▉    | 228/383 [01:01<00:42,  3.64it/s]Epoch: 3, train for the 54-th batch, train loss: 0.4455098509788513:  45%|█████▊       | 53/119 [00:08<00:09,  6.70it/s]Epoch: 3, train for the 54-th batch, train loss: 0.4455098509788513:  45%|█████▉       | 54/119 [00:08<00:09,  7.06it/s]evaluate for the 4-th batch, evaluate loss: 0.6685462594032288:  10%|██                  | 2/20 [00:00<00:00, 18.13it/s]evaluate for the 4-th batch, evaluate loss: 0.6685462594032288:  20%|████                | 4/20 [00:00<00:00, 16.58it/s]Epoch: 2, train for the 90-th batch, train loss: 0.49822998046875:  59%|████████▊      | 89/151 [00:14<00:08,  7.32it/s]Epoch: 2, train for the 90-th batch, train loss: 0.49822998046875:  60%|████████▉      | 90/151 [00:14<00:08,  7.55it/s]evaluate for the 5-th batch, evaluate loss: 0.6594778895378113:  20%|████                | 4/20 [00:00<00:00, 16.58it/s]Epoch: 1, train for the 229-th batch, train loss: 0.37258657813072205:  60%|█████▉    | 229/383 [01:01<00:46,  3.29it/s]Epoch: 3, train for the 55-th batch, train loss: 0.4735327661037445:  45%|█████▉       | 54/119 [00:08<00:09,  7.06it/s]Epoch: 3, train for the 55-th batch, train loss: 0.4735327661037445:  46%|██████       | 55/119 [00:08<00:08,  7.34it/s]evaluate for the 6-th batch, evaluate loss: 0.7248685956001282:  20%|████                | 4/20 [00:00<00:00, 16.58it/s]evaluate for the 6-th batch, evaluate loss: 0.7248685956001282:  30%|██████              | 6/20 [00:00<00:00, 16.88it/s]Epoch: 2, train for the 91-th batch, train loss: 0.49393004179000854:  60%|███████▏    | 90/151 [00:14<00:08,  7.55it/s]Epoch: 2, train for the 91-th batch, train loss: 0.49393004179000854:  60%|███████▏    | 91/151 [00:14<00:07,  7.73it/s]evaluate for the 7-th batch, evaluate loss: 0.7444047331809998:  30%|██████              | 6/20 [00:00<00:00, 16.88it/s]  0%|                                                                                            | 0/66 [00:00<?, ?it/s]Epoch: 3, train for the 56-th batch, train loss: 0.47183603048324585:  46%|█████▌      | 55/119 [00:08<00:08,  7.34it/s]Epoch: 3, train for the 56-th batch, train loss: 0.47183603048324585:  47%|█████▋      | 56/119 [00:08<00:08,  7.04it/s]evaluate for the 8-th batch, evaluate loss: 0.6952328085899353:  30%|██████              | 6/20 [00:00<00:00, 16.88it/s]evaluate for the 8-th batch, evaluate loss: 0.6952328085899353:  40%|████████            | 8/20 [00:00<00:00, 14.93it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5383563041687012:  60%|███████▊     | 91/151 [00:14<00:07,  7.73it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5383563041687012:  61%|███████▉     | 92/151 [00:14<00:07,  7.38it/s]Epoch: 1, train for the 230-th batch, train loss: 0.44990670680999756:  60%|█████▉    | 229/383 [01:01<00:46,  3.29it/s]evaluate for the 1-th batch, evaluate loss: 0.5623487830162048:   0%|                            | 0/66 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.5623487830162048:   2%|▎                   | 1/66 [00:00<00:09,  7.22it/s]evaluate for the 9-th batch, evaluate loss: 0.6371906399726868:  40%|████████            | 8/20 [00:00<00:00, 14.93it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/241 [00:00<?, ?it/s]Epoch: 1, train for the 230-th batch, train loss: 0.44990670680999756:  60%|██████    | 230/383 [01:01<00:46,  3.26it/s]Epoch: 3, train for the 57-th batch, train loss: 0.42285552620887756:  47%|█████▋      | 56/119 [00:08<00:08,  7.04it/s]Epoch: 3, train for the 57-th batch, train loss: 0.42285552620887756:  48%|█████▋      | 57/119 [00:08<00:08,  7.15it/s]evaluate for the 10-th batch, evaluate loss: 0.6405945420265198:  40%|███████▌           | 8/20 [00:00<00:00, 14.93it/s]evaluate for the 10-th batch, evaluate loss: 0.6405945420265198:  50%|█████████         | 10/20 [00:00<00:00, 14.34it/s]evaluate for the 2-th batch, evaluate loss: 0.5165041089057922:   2%|▎                   | 1/66 [00:00<00:09,  7.22it/s]evaluate for the 2-th batch, evaluate loss: 0.5165041089057922:   3%|▌                   | 2/66 [00:00<00:07,  8.61it/s]Epoch: 2, train for the 1-th batch, train loss: 0.5754666924476624:   0%|                       | 0/241 [00:00<?, ?it/s]Epoch: 2, train for the 1-th batch, train loss: 0.5754666924476624:   0%|               | 1/241 [00:00<00:26,  9.04it/s]evaluate for the 11-th batch, evaluate loss: 0.6337828040122986:  50%|█████████         | 10/20 [00:00<00:00, 14.34it/s]Epoch: 2, train for the 93-th batch, train loss: 0.5619528889656067:  61%|███████▉     | 92/151 [00:14<00:07,  7.38it/s]Epoch: 2, train for the 93-th batch, train loss: 0.5619528889656067:  62%|████████     | 93/151 [00:14<00:08,  6.60it/s]Epoch: 3, train for the 58-th batch, train loss: 0.46624454855918884:  48%|█████▋      | 57/119 [00:08<00:08,  7.15it/s]Epoch: 3, train for the 58-th batch, train loss: 0.46624454855918884:  49%|█████▊      | 58/119 [00:08<00:08,  7.42it/s]evaluate for the 12-th batch, evaluate loss: 0.7026283144950867:  50%|█████████         | 10/20 [00:00<00:00, 14.34it/s]evaluate for the 12-th batch, evaluate loss: 0.7026283144950867:  60%|██████████▊       | 12/20 [00:00<00:00, 14.34it/s]Epoch: 2, train for the 2-th batch, train loss: 0.4254573583602905:   0%|               | 1/241 [00:00<00:26,  9.04it/s]Epoch: 2, train for the 2-th batch, train loss: 0.4254573583602905:   1%|               | 2/241 [00:00<00:29,  8.15it/s]Epoch: 1, train for the 231-th batch, train loss: 0.46301043033599854:  60%|██████    | 230/383 [01:02<00:46,  3.26it/s]evaluate for the 3-th batch, evaluate loss: 0.560097336769104:   3%|▋                    | 2/66 [00:00<00:07,  8.61it/s]evaluate for the 3-th batch, evaluate loss: 0.560097336769104:   5%|▉                    | 3/66 [00:00<00:09,  6.79it/s]evaluate for the 13-th batch, evaluate loss: 0.7181476950645447:  60%|██████████▊       | 12/20 [00:00<00:00, 14.34it/s]Epoch: 2, train for the 94-th batch, train loss: 0.5392810106277466:  62%|████████     | 93/151 [00:15<00:08,  6.60it/s]Epoch: 1, train for the 231-th batch, train loss: 0.46301043033599854:  60%|██████    | 231/383 [01:02<00:45,  3.33it/s]Epoch: 3, train for the 59-th batch, train loss: 0.47235575318336487:  49%|█████▊      | 58/119 [00:08<00:08,  7.42it/s]Epoch: 2, train for the 94-th batch, train loss: 0.5392810106277466:  62%|████████     | 94/151 [00:15<00:08,  6.42it/s]Epoch: 3, train for the 59-th batch, train loss: 0.47235575318336487:  50%|█████▉      | 59/119 [00:08<00:08,  7.07it/s]Epoch: 2, train for the 3-th batch, train loss: 0.470688134431839:   1%|▏               | 2/241 [00:00<00:29,  8.15it/s]evaluate for the 14-th batch, evaluate loss: 0.7334429025650024:  60%|██████████▊       | 12/20 [00:00<00:00, 14.34it/s]evaluate for the 14-th batch, evaluate loss: 0.7334429025650024:  70%|████████████▌     | 14/20 [00:00<00:00, 14.60it/s]evaluate for the 4-th batch, evaluate loss: 0.5265436172485352:   5%|▉                   | 3/66 [00:00<00:09,  6.79it/s]evaluate for the 4-th batch, evaluate loss: 0.5265436172485352:   6%|█▏                  | 4/66 [00:00<00:09,  6.59it/s]evaluate for the 15-th batch, evaluate loss: 0.7253328561782837:  70%|████████████▌     | 14/20 [00:01<00:00, 14.60it/s]Epoch: 3, train for the 60-th batch, train loss: 0.4411449432373047:  50%|██████▍      | 59/119 [00:09<00:08,  7.07it/s]Epoch: 3, train for the 60-th batch, train loss: 0.4411449432373047:  50%|██████▌      | 60/119 [00:09<00:08,  6.70it/s]evaluate for the 16-th batch, evaluate loss: 0.6645557880401611:  70%|████████████▌     | 14/20 [00:01<00:00, 14.60it/s]evaluate for the 16-th batch, evaluate loss: 0.6645557880401611:  80%|██████████████▍   | 16/20 [00:01<00:00, 13.97it/s]Epoch: 2, train for the 4-th batch, train loss: 0.4408891201019287:   1%|               | 2/241 [00:00<00:29,  8.15it/s]Epoch: 2, train for the 4-th batch, train loss: 0.4408891201019287:   2%|▏              | 4/241 [00:00<00:32,  7.20it/s]Epoch: 2, train for the 95-th batch, train loss: 0.47811439633369446:  62%|███████▍    | 94/151 [00:15<00:08,  6.42it/s]evaluate for the 17-th batch, evaluate loss: 0.7060871720314026:  80%|██████████████▍   | 16/20 [00:01<00:00, 13.97it/s]Epoch: 2, train for the 95-th batch, train loss: 0.47811439633369446:  63%|███████▌    | 95/151 [00:15<00:10,  5.55it/s]evaluate for the 5-th batch, evaluate loss: 0.5481334328651428:   6%|█▏                  | 4/66 [00:00<00:09,  6.59it/s]evaluate for the 5-th batch, evaluate loss: 0.5481334328651428:   8%|█▌                  | 5/66 [00:00<00:09,  6.61it/s]Epoch: 1, train for the 232-th batch, train loss: 0.38863417506217957:  60%|██████    | 231/383 [01:02<00:45,  3.33it/s]Epoch: 1, train for the 232-th batch, train loss: 0.38863417506217957:  61%|██████    | 232/383 [01:02<00:46,  3.27it/s]Epoch: 2, train for the 5-th batch, train loss: 0.47035202383995056:   2%|▏             | 4/241 [00:00<00:32,  7.20it/s]Epoch: 2, train for the 5-th batch, train loss: 0.47035202383995056:   2%|▎             | 5/241 [00:00<00:30,  7.65it/s]Epoch: 3, train for the 61-th batch, train loss: 0.4698502719402313:  50%|██████▌      | 60/119 [00:09<00:08,  6.70it/s]Epoch: 3, train for the 61-th batch, train loss: 0.4698502719402313:  51%|██████▋      | 61/119 [00:09<00:08,  6.59it/s]evaluate for the 18-th batch, evaluate loss: 0.693644642829895:  80%|███████████████▏   | 16/20 [00:01<00:00, 13.97it/s]evaluate for the 18-th batch, evaluate loss: 0.693644642829895:  90%|█████████████████  | 18/20 [00:01<00:00, 13.47it/s]Epoch: 2, train for the 96-th batch, train loss: 0.534807562828064:  63%|████████▊     | 95/151 [00:15<00:10,  5.55it/s]Epoch: 2, train for the 96-th batch, train loss: 0.534807562828064:  64%|████████▉     | 96/151 [00:15<00:09,  5.64it/s]evaluate for the 6-th batch, evaluate loss: 0.5725370049476624:   8%|█▌                  | 5/66 [00:00<00:09,  6.61it/s]evaluate for the 6-th batch, evaluate loss: 0.5725370049476624:   9%|█▊                  | 6/66 [00:00<00:09,  6.52it/s]evaluate for the 19-th batch, evaluate loss: 0.7387946844100952:  90%|████████████████▏ | 18/20 [00:01<00:00, 13.47it/s]Epoch: 2, train for the 6-th batch, train loss: 0.619594156742096:   2%|▎               | 5/241 [00:00<00:30,  7.65it/s]Epoch: 2, train for the 6-th batch, train loss: 0.619594156742096:   2%|▍               | 6/241 [00:00<00:30,  7.66it/s]Epoch: 3, train for the 62-th batch, train loss: 0.5367815494537354:  51%|██████▋      | 61/119 [00:09<00:08,  6.59it/s]evaluate for the 20-th batch, evaluate loss: 0.7612534761428833:  90%|████████████████▏ | 18/20 [00:01<00:00, 13.47it/s]evaluate for the 20-th batch, evaluate loss: 0.7612534761428833: 100%|██████████████████| 20/20 [00:01<00:00, 13.46it/s]evaluate for the 20-th batch, evaluate loss: 0.7612534761428833: 100%|██████████████████| 20/20 [00:01<00:00, 14.31it/s]
Epoch: 3, train for the 62-th batch, train loss: 0.5367815494537354:  52%|██████▊      | 62/119 [00:09<00:08,  6.54it/s]Epoch: 1, train for the 233-th batch, train loss: 0.4147518575191498:  61%|██████▋    | 232/383 [01:02<00:46,  3.27it/s]evaluate for the 7-th batch, evaluate loss: 0.5444924235343933:   9%|█▊                  | 6/66 [00:01<00:09,  6.52it/s]evaluate for the 7-th batch, evaluate loss: 0.5444924235343933:  11%|██                  | 7/66 [00:01<00:08,  6.90it/s]Epoch: 1, train for the 233-th batch, train loss: 0.4147518575191498:  61%|██████▋    | 233/383 [01:02<00:43,  3.42it/s]Epoch: 2, train for the 97-th batch, train loss: 0.5343385934829712:  64%|████████▎    | 96/151 [00:15<00:09,  5.64it/s]Epoch: 2, train for the 7-th batch, train loss: 0.4113599956035614:   2%|▎              | 6/241 [00:00<00:30,  7.66it/s]Epoch: 2, train for the 97-th batch, train loss: 0.5343385934829712:  64%|████████▎    | 97/151 [00:15<00:09,  5.58it/s]Epoch: 2, train for the 7-th batch, train loss: 0.4113599956035614:   3%|▍              | 7/241 [00:00<00:31,  7.44it/s]Epoch: 3, train for the 63-th batch, train loss: 0.49468687176704407:  52%|██████▎     | 62/119 [00:09<00:08,  6.54it/s]evaluate for the 8-th batch, evaluate loss: 0.5374395847320557:  11%|██                  | 7/66 [00:01<00:08,  6.90it/s]Epoch: 3, train for the 63-th batch, train loss: 0.49468687176704407:  53%|██████▎     | 63/119 [00:09<00:08,  6.59it/s]Epoch: 2, train for the 8-th batch, train loss: 0.657996654510498:   3%|▍               | 7/241 [00:01<00:31,  7.44it/s]Epoch: 2, train for the 8-th batch, train loss: 0.657996654510498:   3%|▌               | 8/241 [00:01<00:30,  7.65it/s]evaluate for the 9-th batch, evaluate loss: 0.5087752938270569:  11%|██                  | 7/66 [00:01<00:08,  6.90it/s]evaluate for the 9-th batch, evaluate loss: 0.5087752938270569:  14%|██▋                 | 9/66 [00:01<00:06,  8.35it/s]Epoch: 2, train for the 98-th batch, train loss: 0.5078312754631042:  64%|████████▎    | 97/151 [00:15<00:09,  5.58it/s]Epoch: 2, train for the 98-th batch, train loss: 0.5078312754631042:  65%|████████▍    | 98/151 [00:15<00:09,  5.52it/s]INFO:root:Epoch: 2, learning rate: 0.0001, train loss: 0.5589
INFO:root:train average_precision, 0.8064
INFO:root:train roc_auc, 0.7859
INFO:root:validate loss: 0.4785
INFO:root:validate average_precision, 0.8549
INFO:root:validate roc_auc, 0.8504
INFO:root:new node validate loss: 0.6989
INFO:root:new node validate first_1_average_precision, 0.5705
INFO:root:new node validate first_1_roc_auc, 0.5093
INFO:root:new node validate first_3_average_precision, 0.5977
INFO:root:new node validate first_3_roc_auc, 0.5676
INFO:root:new node validate first_10_average_precision, 0.6207
INFO:root:new node validate first_10_roc_auc, 0.6190
INFO:root:new node validate average_precision, 0.6665
INFO:root:new node validate roc_auc, 0.6688
INFO:root:save model ./saved_models/TGN/ia-escorts-dynamic/TGN_seed0_dummy-4ge5z0wz/TGN_seed0_dummy-4ge5z0wz.pkl
Epoch: 3, train for the 64-th batch, train loss: 0.4995497763156891:  53%|██████▉      | 63/119 [00:09<00:08,  6.59it/s]Epoch: 1, train for the 234-th batch, train loss: 0.4283345639705658:  61%|██████▋    | 233/383 [01:03<00:43,  3.42it/s]Epoch: 3, train for the 64-th batch, train loss: 0.4995497763156891:  54%|██████▉      | 64/119 [00:09<00:08,  6.41it/s]evaluate for the 10-th batch, evaluate loss: 0.5108892917633057:  14%|██▌                | 9/66 [00:01<00:06,  8.35it/s]Epoch: 2, train for the 9-th batch, train loss: 0.5961138010025024:   3%|▍              | 8/241 [00:01<00:30,  7.65it/s]Epoch: 1, train for the 234-th batch, train loss: 0.4283345639705658:  61%|██████▋    | 234/383 [01:03<00:42,  3.51it/s]Epoch: 3, train for the 65-th batch, train loss: 0.4953654408454895:  54%|██████▉      | 64/119 [00:09<00:08,  6.41it/s]Epoch: 3, train for the 65-th batch, train loss: 0.4953654408454895:  55%|███████      | 65/119 [00:09<00:07,  7.08it/s]evaluate for the 11-th batch, evaluate loss: 0.5152700543403625:  14%|██▌                | 9/66 [00:01<00:06,  8.35it/s]evaluate for the 11-th batch, evaluate loss: 0.5152700543403625:  17%|███               | 11/66 [00:01<00:05,  9.18it/s]Epoch: 2, train for the 99-th batch, train loss: 0.5303806662559509:  65%|████████▍    | 98/151 [00:15<00:09,  5.52it/s]Epoch: 2, train for the 99-th batch, train loss: 0.5303806662559509:  66%|████████▌    | 99/151 [00:16<00:09,  5.67it/s]Epoch: 2, train for the 10-th batch, train loss: 0.5377066135406494:   3%|▍             | 8/241 [00:01<00:30,  7.65it/s]Epoch: 2, train for the 10-th batch, train loss: 0.5377066135406494:   4%|▌            | 10/241 [00:01<00:29,  7.85it/s]Epoch: 3, train for the 66-th batch, train loss: 0.4754682183265686:  55%|███████      | 65/119 [00:09<00:07,  7.08it/s]Epoch: 3, train for the 66-th batch, train loss: 0.4754682183265686:  55%|███████▏     | 66/119 [00:09<00:07,  7.41it/s]Epoch: 1, train for the 235-th batch, train loss: 0.37194350361824036:  61%|██████    | 234/383 [01:03<00:42,  3.51it/s]evaluate for the 12-th batch, evaluate loss: 0.5375550985336304:  17%|███               | 11/66 [00:01<00:05,  9.18it/s]evaluate for the 12-th batch, evaluate loss: 0.5375550985336304:  18%|███▎              | 12/66 [00:01<00:06,  8.09it/s]Epoch: 2, train for the 11-th batch, train loss: 0.3670446574687958:   4%|▌            | 10/241 [00:01<00:29,  7.85it/s]Epoch: 2, train for the 11-th batch, train loss: 0.3670446574687958:   5%|▌            | 11/241 [00:01<00:30,  7.64it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5727910995483398:  66%|███████▊    | 99/151 [00:16<00:09,  5.67it/s]Epoch: 1, train for the 235-th batch, train loss: 0.37194350361824036:  61%|██████▏   | 235/383 [01:03<00:42,  3.48it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5727910995483398:  66%|███████▎   | 100/151 [00:16<00:09,  5.41it/s]evaluate for the 13-th batch, evaluate loss: 0.5495876669883728:  18%|███▎              | 12/66 [00:01<00:06,  8.09it/s]evaluate for the 13-th batch, evaluate loss: 0.5495876669883728:  20%|███▌              | 13/66 [00:01<00:06,  8.40it/s]Epoch: 2, train for the 12-th batch, train loss: 0.5409549474716187:   5%|▌            | 11/241 [00:01<00:30,  7.64it/s]Epoch: 2, train for the 12-th batch, train loss: 0.5409549474716187:   5%|▋            | 12/241 [00:01<00:28,  8.03it/s]Epoch: 3, train for the 67-th batch, train loss: 0.5304713845252991:  55%|███████▏     | 66/119 [00:10<00:07,  7.41it/s]Epoch: 3, train for the 67-th batch, train loss: 0.5304713845252991:  56%|███████▎     | 67/119 [00:10<00:08,  6.16it/s]evaluate for the 14-th batch, evaluate loss: 0.5395950078964233:  20%|███▌              | 13/66 [00:01<00:06,  8.40it/s]evaluate for the 14-th batch, evaluate loss: 0.5395950078964233:  21%|███▊              | 14/66 [00:01<00:06,  8.48it/s]Epoch: 2, train for the 101-th batch, train loss: 0.5991700887680054:  66%|███████▎   | 100/151 [00:16<00:09,  5.41it/s]Epoch: 2, train for the 13-th batch, train loss: 0.5240763425827026:   5%|▋            | 12/241 [00:01<00:28,  8.03it/s]Epoch: 2, train for the 13-th batch, train loss: 0.5240763425827026:   5%|▋            | 13/241 [00:01<00:28,  8.07it/s]Epoch: 2, train for the 101-th batch, train loss: 0.5991700887680054:  67%|███████▎   | 101/151 [00:16<00:09,  5.36it/s]Epoch: 3, train for the 68-th batch, train loss: 0.4530329704284668:  56%|███████▎     | 67/119 [00:10<00:08,  6.16it/s]Epoch: 1, train for the 236-th batch, train loss: 0.4181285798549652:  61%|██████▋    | 235/383 [01:03<00:42,  3.48it/s]evaluate for the 15-th batch, evaluate loss: 0.522092878818512:  21%|████               | 14/66 [00:01<00:06,  8.48it/s]Epoch: 1, train for the 236-th batch, train loss: 0.4181285798549652:  62%|██████▊    | 236/383 [01:03<00:41,  3.51it/s]Epoch: 2, train for the 14-th batch, train loss: 0.5978098511695862:   5%|▋            | 13/241 [00:01<00:28,  8.07it/s]Epoch: 2, train for the 14-th batch, train loss: 0.5978098511695862:   6%|▊            | 14/241 [00:01<00:27,  8.28it/s]Epoch: 3, train for the 69-th batch, train loss: 0.48136815428733826:  56%|██████▊     | 67/119 [00:10<00:08,  6.16it/s]Epoch: 3, train for the 69-th batch, train loss: 0.48136815428733826:  58%|██████▉     | 69/119 [00:10<00:06,  7.36it/s]Epoch: 2, train for the 102-th batch, train loss: 0.49475032091140747:  67%|██████▋   | 101/151 [00:16<00:09,  5.36it/s]evaluate for the 16-th batch, evaluate loss: 0.5010406374931335:  21%|███▊              | 14/66 [00:01<00:06,  8.48it/s]evaluate for the 16-th batch, evaluate loss: 0.5010406374931335:  24%|████▎             | 16/66 [00:01<00:05,  9.36it/s]Epoch: 2, train for the 102-th batch, train loss: 0.49475032091140747:  68%|██████▊   | 102/151 [00:16<00:08,  5.61it/s]Epoch: 3, train for the 70-th batch, train loss: 0.4128774106502533:  58%|███████▌     | 69/119 [00:10<00:06,  7.36it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]Epoch: 3, train for the 70-th batch, train loss: 0.4128774106502533:  59%|███████▋     | 70/119 [00:10<00:06,  7.22it/s]Epoch: 2, train for the 103-th batch, train loss: 0.5829381346702576:  68%|███████▍   | 102/151 [00:16<00:08,  5.61it/s]Epoch: 2, train for the 103-th batch, train loss: 0.5829381346702576:  68%|███████▌   | 103/151 [00:16<00:08,  5.95it/s]evaluate for the 17-th batch, evaluate loss: 0.5204443335533142:  24%|████▎             | 16/66 [00:02<00:05,  9.36it/s]evaluate for the 17-th batch, evaluate loss: 0.5204443335533142:  26%|████▋             | 17/66 [00:02<00:05,  8.34it/s]Epoch: 1, train for the 237-th batch, train loss: 0.4205228388309479:  62%|██████▊    | 236/383 [01:03<00:41,  3.51it/s]Epoch: 3, train for the 1-th batch, train loss: 0.8794859647750854:   0%|                       | 0/146 [00:00<?, ?it/s]Epoch: 2, train for the 15-th batch, train loss: 0.5303505063056946:   6%|▊            | 14/241 [00:02<00:27,  8.28it/s]Epoch: 2, train for the 15-th batch, train loss: 0.5303505063056946:   6%|▊            | 15/241 [00:02<00:37,  5.98it/s]Epoch: 1, train for the 237-th batch, train loss: 0.4205228388309479:  62%|██████▊    | 237/383 [01:03<00:43,  3.38it/s]Epoch: 3, train for the 71-th batch, train loss: 0.4546615481376648:  59%|███████▋     | 70/119 [00:10<00:06,  7.22it/s]Epoch: 3, train for the 71-th batch, train loss: 0.4546615481376648:  60%|███████▊     | 71/119 [00:10<00:06,  7.11it/s]evaluate for the 18-th batch, evaluate loss: 0.522256076335907:  26%|████▉              | 17/66 [00:02<00:05,  8.34it/s]evaluate for the 18-th batch, evaluate loss: 0.522256076335907:  27%|█████▏             | 18/66 [00:02<00:05,  8.58it/s]Epoch: 2, train for the 104-th batch, train loss: 0.5778915286064148:  68%|███████▌   | 103/151 [00:16<00:08,  5.95it/s]Epoch: 2, train for the 104-th batch, train loss: 0.5778915286064148:  69%|███████▌   | 104/151 [00:16<00:07,  5.88it/s]Epoch: 2, train for the 16-th batch, train loss: 0.45302897691726685:   6%|▋           | 15/241 [00:02<00:37,  5.98it/s]Epoch: 2, train for the 16-th batch, train loss: 0.45302897691726685:   7%|▊           | 16/241 [00:02<00:34,  6.60it/s]Epoch: 3, train for the 72-th batch, train loss: 0.5123577117919922:  60%|███████▊     | 71/119 [00:10<00:06,  7.11it/s]evaluate for the 19-th batch, evaluate loss: 0.4968700408935547:  27%|████▉             | 18/66 [00:02<00:05,  8.58it/s]evaluate for the 19-th batch, evaluate loss: 0.4968700408935547:  29%|█████▏            | 19/66 [00:02<00:05,  8.72it/s]Epoch: 3, train for the 72-th batch, train loss: 0.5123577117919922:  61%|███████▊     | 72/119 [00:10<00:06,  7.54it/s]Epoch: 3, train for the 2-th batch, train loss: 0.8259812593460083:   0%|                       | 0/146 [00:00<?, ?it/s]Epoch: 3, train for the 2-th batch, train loss: 0.8259812593460083:   1%|▏              | 2/146 [00:00<00:22,  6.40it/s]Epoch: 2, train for the 17-th batch, train loss: 0.41582372784614563:   7%|▊           | 16/241 [00:02<00:34,  6.60it/s]Epoch: 1, train for the 238-th batch, train loss: 0.36260348558425903:  62%|██████▏   | 237/383 [01:04<00:43,  3.38it/s]Epoch: 2, train for the 17-th batch, train loss: 0.41582372784614563:   7%|▊           | 17/241 [00:02<00:32,  6.90it/s]evaluate for the 20-th batch, evaluate loss: 0.5591505169868469:  29%|█████▏            | 19/66 [00:02<00:05,  8.72it/s]evaluate for the 20-th batch, evaluate loss: 0.5591505169868469:  30%|█████▍            | 20/66 [00:02<00:05,  8.78it/s]Epoch: 3, train for the 73-th batch, train loss: 0.47052574157714844:  61%|███████▎    | 72/119 [00:10<00:06,  7.54it/s]Epoch: 2, train for the 105-th batch, train loss: 0.5503044724464417:  69%|███████▌   | 104/151 [00:17<00:07,  5.88it/s]Epoch: 3, train for the 73-th batch, train loss: 0.47052574157714844:  61%|███████▎    | 73/119 [00:10<00:06,  7.30it/s]Epoch: 1, train for the 238-th batch, train loss: 0.36260348558425903:  62%|██████▏   | 238/383 [01:04<00:42,  3.38it/s]Epoch: 3, train for the 3-th batch, train loss: 0.7896302342414856:   1%|▏              | 2/146 [00:00<00:22,  6.40it/s]Epoch: 3, train for the 3-th batch, train loss: 0.7896302342414856:   2%|▎              | 3/146 [00:00<00:20,  7.05it/s]Epoch: 2, train for the 105-th batch, train loss: 0.5503044724464417:  70%|███████▋   | 105/151 [00:17<00:08,  5.42it/s]Epoch: 2, train for the 18-th batch, train loss: 0.47852298617362976:   7%|▊           | 17/241 [00:02<00:32,  6.90it/s]Epoch: 2, train for the 18-th batch, train loss: 0.47852298617362976:   7%|▉           | 18/241 [00:02<00:29,  7.49it/s]evaluate for the 21-th batch, evaluate loss: 0.5540809631347656:  30%|█████▍            | 20/66 [00:02<00:05,  8.78it/s]evaluate for the 21-th batch, evaluate loss: 0.5540809631347656:  32%|█████▋            | 21/66 [00:02<00:05,  8.58it/s]Epoch: 3, train for the 74-th batch, train loss: 0.5073919296264648:  61%|███████▉     | 73/119 [00:11<00:06,  7.30it/s]Epoch: 3, train for the 74-th batch, train loss: 0.5073919296264648:  62%|████████     | 74/119 [00:11<00:06,  7.02it/s]Epoch: 3, train for the 4-th batch, train loss: 0.7629114389419556:   2%|▎              | 3/146 [00:00<00:20,  7.05it/s]Epoch: 3, train for the 4-th batch, train loss: 0.7629114389419556:   3%|▍              | 4/146 [00:00<00:20,  6.77it/s]Epoch: 2, train for the 106-th batch, train loss: 0.5039320588111877:  70%|███████▋   | 105/151 [00:17<00:08,  5.42it/s]Epoch: 2, train for the 19-th batch, train loss: 0.48145487904548645:   7%|▉           | 18/241 [00:02<00:29,  7.49it/s]Epoch: 2, train for the 19-th batch, train loss: 0.48145487904548645:   8%|▉           | 19/241 [00:02<00:32,  6.75it/s]Epoch: 2, train for the 106-th batch, train loss: 0.5039320588111877:  70%|███████▋   | 106/151 [00:17<00:08,  5.06it/s]Epoch: 1, train for the 239-th batch, train loss: 0.44148504734039307:  62%|██████▏   | 238/383 [01:04<00:42,  3.38it/s]evaluate for the 22-th batch, evaluate loss: 0.5370494723320007:  32%|█████▋            | 21/66 [00:02<00:05,  8.58it/s]evaluate for the 22-th batch, evaluate loss: 0.5370494723320007:  33%|██████            | 22/66 [00:02<00:06,  7.30it/s]Epoch: 3, train for the 75-th batch, train loss: 0.4656561613082886:  62%|████████     | 74/119 [00:11<00:06,  7.02it/s]Epoch: 3, train for the 75-th batch, train loss: 0.4656561613082886:  63%|████████▏    | 75/119 [00:11<00:06,  6.74it/s]Epoch: 1, train for the 239-th batch, train loss: 0.44148504734039307:  62%|██████▏   | 239/383 [01:04<00:43,  3.33it/s]Epoch: 3, train for the 5-th batch, train loss: 0.7253842949867249:   3%|▍              | 4/146 [00:00<00:20,  6.77it/s]Epoch: 3, train for the 5-th batch, train loss: 0.7253842949867249:   3%|▌              | 5/146 [00:00<00:21,  6.71it/s]Epoch: 2, train for the 20-th batch, train loss: 0.38682904839515686:   8%|▉           | 19/241 [00:02<00:32,  6.75it/s]Epoch: 2, train for the 20-th batch, train loss: 0.38682904839515686:   8%|▉           | 20/241 [00:02<00:29,  7.46it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5267803072929382:  70%|███████▋   | 106/151 [00:17<00:08,  5.06it/s]evaluate for the 23-th batch, evaluate loss: 0.5436128973960876:  33%|██████            | 22/66 [00:02<00:06,  7.30it/s]evaluate for the 23-th batch, evaluate loss: 0.5436128973960876:  35%|██████▎           | 23/66 [00:02<00:05,  7.60it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5267803072929382:  71%|███████▊   | 107/151 [00:17<00:08,  5.38it/s]Epoch: 3, train for the 6-th batch, train loss: 0.6869081258773804:   3%|▌              | 5/146 [00:00<00:21,  6.71it/s]Epoch: 2, train for the 21-th batch, train loss: 0.627295196056366:   8%|█▏            | 20/241 [00:02<00:29,  7.46it/s]Epoch: 3, train for the 6-th batch, train loss: 0.6869081258773804:   4%|▌              | 6/146 [00:00<00:20,  6.69it/s]Epoch: 3, train for the 76-th batch, train loss: 0.5018787384033203:  63%|████████▏    | 75/119 [00:11<00:06,  6.74it/s]Epoch: 2, train for the 21-th batch, train loss: 0.627295196056366:   9%|█▏            | 21/241 [00:02<00:29,  7.37it/s]Epoch: 3, train for the 76-th batch, train loss: 0.5018787384033203:  64%|████████▎    | 76/119 [00:11<00:06,  6.44it/s]evaluate for the 24-th batch, evaluate loss: 0.5310010313987732:  35%|██████▎           | 23/66 [00:03<00:05,  7.60it/s]evaluate for the 24-th batch, evaluate loss: 0.5310010313987732:  36%|██████▌           | 24/66 [00:03<00:05,  7.68it/s]Epoch: 2, train for the 108-th batch, train loss: 0.5370217561721802:  71%|███████▊   | 107/151 [00:17<00:08,  5.38it/s]Epoch: 1, train for the 240-th batch, train loss: 0.5183424353599548:  62%|██████▊    | 239/383 [01:04<00:43,  3.33it/s]Epoch: 2, train for the 108-th batch, train loss: 0.5370217561721802:  72%|███████▊   | 108/151 [00:17<00:07,  5.63it/s]Epoch: 2, train for the 22-th batch, train loss: 0.5317378640174866:   9%|█▏           | 21/241 [00:02<00:29,  7.37it/s]Epoch: 2, train for the 22-th batch, train loss: 0.5317378640174866:   9%|█▏           | 22/241 [00:02<00:28,  7.77it/s]Epoch: 3, train for the 7-th batch, train loss: 0.6779837012290955:   4%|▌              | 6/146 [00:01<00:20,  6.69it/s]evaluate for the 25-th batch, evaluate loss: 0.5439012050628662:  36%|██████▌           | 24/66 [00:03<00:05,  7.68it/s]Epoch: 1, train for the 240-th batch, train loss: 0.5183424353599548:  63%|██████▉    | 240/383 [01:04<00:42,  3.40it/s]Epoch: 3, train for the 7-th batch, train loss: 0.6779837012290955:   5%|▋              | 7/146 [00:01<00:19,  6.99it/s]Epoch: 3, train for the 77-th batch, train loss: 0.4842485785484314:  64%|████████▎    | 76/119 [00:11<00:06,  6.44it/s]Epoch: 3, train for the 77-th batch, train loss: 0.4842485785484314:  65%|████████▍    | 77/119 [00:11<00:06,  6.53it/s]Epoch: 2, train for the 109-th batch, train loss: 0.4979501962661743:  72%|███████▊   | 108/151 [00:17<00:07,  5.63it/s]Epoch: 2, train for the 23-th batch, train loss: 0.5062803030014038:   9%|█▏           | 22/241 [00:03<00:28,  7.77it/s]Epoch: 2, train for the 109-th batch, train loss: 0.4979501962661743:  72%|███████▉   | 109/151 [00:17<00:07,  5.85it/s]Epoch: 2, train for the 23-th batch, train loss: 0.5062803030014038:  10%|█▏           | 23/241 [00:03<00:28,  7.77it/s]evaluate for the 26-th batch, evaluate loss: 0.5540415644645691:  36%|██████▌           | 24/66 [00:03<00:05,  7.68it/s]evaluate for the 26-th batch, evaluate loss: 0.5540415644645691:  39%|███████           | 26/66 [00:03<00:04,  8.13it/s]Epoch: 3, train for the 8-th batch, train loss: 0.6655279397964478:   5%|▋              | 7/146 [00:01<00:19,  6.99it/s]Epoch: 3, train for the 8-th batch, train loss: 0.6655279397964478:   5%|▊              | 8/146 [00:01<00:20,  6.70it/s]Epoch: 3, train for the 78-th batch, train loss: 0.5480877757072449:  65%|████████▍    | 77/119 [00:11<00:06,  6.53it/s]Epoch: 3, train for the 78-th batch, train loss: 0.5480877757072449:  66%|████████▌    | 78/119 [00:11<00:06,  6.40it/s]Epoch: 2, train for the 24-th batch, train loss: 0.5334284901618958:  10%|█▏           | 23/241 [00:03<00:28,  7.77it/s]Epoch: 1, train for the 241-th batch, train loss: 0.5662246346473694:  63%|██████▉    | 240/383 [01:05<00:42,  3.40it/s]Epoch: 2, train for the 24-th batch, train loss: 0.5334284901618958:  10%|█▎           | 24/241 [00:03<00:27,  7.75it/s]evaluate for the 27-th batch, evaluate loss: 0.5422071814537048:  39%|███████           | 26/66 [00:03<00:04,  8.13it/s]evaluate for the 27-th batch, evaluate loss: 0.5422071814537048:  41%|███████▎          | 27/66 [00:03<00:04,  8.25it/s]Epoch: 3, train for the 9-th batch, train loss: 0.6439936757087708:   5%|▊              | 8/146 [00:01<00:20,  6.70it/s]Epoch: 3, train for the 9-th batch, train loss: 0.6439936757087708:   6%|▉              | 9/146 [00:01<00:18,  7.38it/s]Epoch: 2, train for the 110-th batch, train loss: 0.5486568212509155:  72%|███████▉   | 109/151 [00:17<00:07,  5.85it/s]Epoch: 1, train for the 241-th batch, train loss: 0.5662246346473694:  63%|██████▉    | 241/383 [01:05<00:42,  3.37it/s]Epoch: 2, train for the 110-th batch, train loss: 0.5486568212509155:  73%|████████   | 110/151 [00:17<00:07,  5.62it/s]Epoch: 3, train for the 79-th batch, train loss: 0.5019758343696594:  66%|████████▌    | 78/119 [00:11<00:06,  6.40it/s]Epoch: 3, train for the 79-th batch, train loss: 0.5019758343696594:  66%|████████▋    | 79/119 [00:11<00:06,  6.66it/s]Epoch: 2, train for the 25-th batch, train loss: 0.5928179621696472:  10%|█▎           | 24/241 [00:03<00:27,  7.75it/s]evaluate for the 28-th batch, evaluate loss: 0.5933530330657959:  41%|███████▎          | 27/66 [00:03<00:04,  8.25it/s]Epoch: 2, train for the 25-th batch, train loss: 0.5928179621696472:  10%|█▎           | 25/241 [00:03<00:26,  8.13it/s]Epoch: 3, train for the 10-th batch, train loss: 0.6473503112792969:   6%|▊             | 9/146 [00:01<00:18,  7.38it/s]Epoch: 3, train for the 10-th batch, train loss: 0.6473503112792969:   7%|▉            | 10/146 [00:01<00:17,  7.72it/s]evaluate for the 29-th batch, evaluate loss: 0.5077495574951172:  41%|███████▎          | 27/66 [00:03<00:04,  8.25it/s]evaluate for the 29-th batch, evaluate loss: 0.5077495574951172:  44%|███████▉          | 29/66 [00:03<00:04,  8.64it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5413274765014648:  73%|████████   | 110/151 [00:18<00:07,  5.62it/s]Epoch: 3, train for the 80-th batch, train loss: 0.5339195132255554:  66%|████████▋    | 79/119 [00:12<00:06,  6.66it/s]Epoch: 3, train for the 80-th batch, train loss: 0.5339195132255554:  67%|████████▋    | 80/119 [00:12<00:06,  6.37it/s]Epoch: 2, train for the 26-th batch, train loss: 0.642297625541687:  10%|█▍            | 25/241 [00:03<00:26,  8.13it/s]Epoch: 2, train for the 26-th batch, train loss: 0.642297625541687:  11%|█▌            | 26/241 [00:03<00:28,  7.50it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5413274765014648:  74%|████████   | 111/151 [00:18<00:07,  5.25it/s]Epoch: 3, train for the 11-th batch, train loss: 0.6443718075752258:   7%|▉            | 10/146 [00:01<00:17,  7.72it/s]Epoch: 1, train for the 242-th batch, train loss: 0.474206805229187:  63%|███████▌    | 241/383 [01:05<00:42,  3.37it/s]Epoch: 3, train for the 11-th batch, train loss: 0.6443718075752258:   8%|▉            | 11/146 [00:01<00:18,  7.37it/s]evaluate for the 30-th batch, evaluate loss: 0.5570970773696899:  44%|███████▉          | 29/66 [00:03<00:04,  8.64it/s]evaluate for the 30-th batch, evaluate loss: 0.5570970773696899:  45%|████████▏         | 30/66 [00:03<00:04,  8.72it/s]Epoch: 1, train for the 242-th batch, train loss: 0.474206805229187:  63%|███████▌    | 242/383 [01:05<00:42,  3.34it/s]Epoch: 2, train for the 27-th batch, train loss: 0.6033129096031189:  11%|█▍           | 26/241 [00:03<00:28,  7.50it/s]Epoch: 2, train for the 27-th batch, train loss: 0.6033129096031189:  11%|█▍           | 27/241 [00:03<00:28,  7.60it/s]Epoch: 3, train for the 81-th batch, train loss: 0.42589065432548523:  67%|████████    | 80/119 [00:12<00:06,  6.37it/s]evaluate for the 31-th batch, evaluate loss: 0.544266939163208:  45%|████████▋          | 30/66 [00:03<00:04,  8.72it/s]Epoch: 3, train for the 12-th batch, train loss: 0.6438437700271606:   8%|▉            | 11/146 [00:01<00:18,  7.37it/s]Epoch: 3, train for the 81-th batch, train loss: 0.42589065432548523:  68%|████████▏   | 81/119 [00:12<00:06,  6.11it/s]Epoch: 3, train for the 12-th batch, train loss: 0.6438437700271606:   8%|█            | 12/146 [00:01<00:18,  7.09it/s]Epoch: 2, train for the 112-th batch, train loss: 0.4736495018005371:  74%|████████   | 111/151 [00:18<00:07,  5.25it/s]Epoch: 2, train for the 112-th batch, train loss: 0.4736495018005371:  74%|████████▏  | 112/151 [00:18<00:07,  5.23it/s]Epoch: 2, train for the 28-th batch, train loss: 0.45345932245254517:  11%|█▎          | 27/241 [00:03<00:28,  7.60it/s]Epoch: 2, train for the 28-th batch, train loss: 0.45345932245254517:  12%|█▍          | 28/241 [00:03<00:26,  7.95it/s]evaluate for the 32-th batch, evaluate loss: 0.5554093718528748:  45%|████████▏         | 30/66 [00:03<00:04,  8.72it/s]evaluate for the 32-th batch, evaluate loss: 0.5554093718528748:  48%|████████▋         | 32/66 [00:03<00:04,  8.36it/s]Epoch: 3, train for the 13-th batch, train loss: 0.6526846289634705:   8%|█            | 12/146 [00:01<00:18,  7.09it/s]Epoch: 3, train for the 13-th batch, train loss: 0.6526846289634705:   9%|█▏           | 13/146 [00:01<00:19,  6.89it/s]Epoch: 1, train for the 243-th batch, train loss: 0.4748341143131256:  63%|██████▉    | 242/383 [01:05<00:42,  3.34it/s]Epoch: 3, train for the 82-th batch, train loss: 0.4968263804912567:  68%|████████▊    | 81/119 [00:12<00:06,  6.11it/s]Epoch: 3, train for the 82-th batch, train loss: 0.4968263804912567:  69%|████████▉    | 82/119 [00:12<00:06,  5.95it/s]Epoch: 2, train for the 29-th batch, train loss: 0.5691343545913696:  12%|█▌           | 28/241 [00:03<00:26,  7.95it/s]Epoch: 1, train for the 243-th batch, train loss: 0.4748341143131256:  63%|██████▉    | 243/383 [01:05<00:42,  3.31it/s]evaluate for the 33-th batch, evaluate loss: 0.5824039578437805:  48%|████████▋         | 32/66 [00:04<00:04,  8.36it/s]Epoch: 2, train for the 113-th batch, train loss: 0.531204879283905:  74%|████████▉   | 112/151 [00:18<00:07,  5.23it/s]Epoch: 2, train for the 29-th batch, train loss: 0.5691343545913696:  12%|█▌           | 29/241 [00:03<00:29,  7.25it/s]Epoch: 2, train for the 113-th batch, train loss: 0.531204879283905:  75%|████████▉   | 113/151 [00:18<00:07,  4.94it/s]Epoch: 3, train for the 14-th batch, train loss: 0.6417328715324402:   9%|█▏           | 13/146 [00:01<00:19,  6.89it/s]Epoch: 3, train for the 14-th batch, train loss: 0.6417328715324402:  10%|█▏           | 14/146 [00:01<00:17,  7.41it/s]Epoch: 3, train for the 83-th batch, train loss: 0.4701012670993805:  69%|████████▉    | 82/119 [00:12<00:06,  5.95it/s]Epoch: 3, train for the 83-th batch, train loss: 0.4701012670993805:  70%|█████████    | 83/119 [00:12<00:05,  6.35it/s]evaluate for the 34-th batch, evaluate loss: 0.5436246991157532:  48%|████████▋         | 32/66 [00:04<00:04,  8.36it/s]evaluate for the 34-th batch, evaluate loss: 0.5436246991157532:  52%|█████████▎        | 34/66 [00:04<00:03,  9.07it/s]Epoch: 2, train for the 30-th batch, train loss: 0.6454010009765625:  12%|█▌           | 29/241 [00:03<00:29,  7.25it/s]Epoch: 2, train for the 30-th batch, train loss: 0.6454010009765625:  12%|█▌           | 30/241 [00:04<00:28,  7.39it/s]Epoch: 3, train for the 15-th batch, train loss: 0.6534122824668884:  10%|█▏           | 14/146 [00:02<00:17,  7.41it/s]Epoch: 3, train for the 15-th batch, train loss: 0.6534122824668884:  10%|█▎           | 15/146 [00:02<00:17,  7.55it/s]Epoch: 2, train for the 114-th batch, train loss: 0.4957793653011322:  75%|████████▏  | 113/151 [00:18<00:07,  4.94it/s]Epoch: 3, train for the 84-th batch, train loss: 0.49987441301345825:  70%|████████▎   | 83/119 [00:12<00:05,  6.35it/s]Epoch: 2, train for the 114-th batch, train loss: 0.4957793653011322:  75%|████████▎  | 114/151 [00:18<00:07,  5.04it/s]Epoch: 3, train for the 84-th batch, train loss: 0.49987441301345825:  71%|████████▍   | 84/119 [00:12<00:05,  6.48it/s]Epoch: 1, train for the 244-th batch, train loss: 0.37976571917533875:  63%|██████▎   | 243/383 [01:05<00:42,  3.31it/s]Epoch: 2, train for the 31-th batch, train loss: 0.4681731164455414:  12%|█▌           | 30/241 [00:04<00:28,  7.39it/s]evaluate for the 35-th batch, evaluate loss: 0.5697755217552185:  52%|█████████▎        | 34/66 [00:04<00:03,  9.07it/s]evaluate for the 35-th batch, evaluate loss: 0.5697755217552185:  53%|█████████▌        | 35/66 [00:04<00:03,  8.68it/s]Epoch: 2, train for the 31-th batch, train loss: 0.4681731164455414:  13%|█▋           | 31/241 [00:04<00:26,  7.79it/s]Epoch: 3, train for the 16-th batch, train loss: 0.6568410992622375:  10%|█▎           | 15/146 [00:02<00:17,  7.55it/s]Epoch: 3, train for the 16-th batch, train loss: 0.6568410992622375:  11%|█▍           | 16/146 [00:02<00:16,  7.78it/s]Epoch: 1, train for the 244-th batch, train loss: 0.37976571917533875:  64%|██████▎   | 244/383 [01:06<00:41,  3.31it/s]evaluate for the 36-th batch, evaluate loss: 0.5628170371055603:  53%|█████████▌        | 35/66 [00:04<00:03,  8.68it/s]evaluate for the 36-th batch, evaluate loss: 0.5628170371055603:  55%|█████████▊        | 36/66 [00:04<00:03,  8.70it/s]Epoch: 3, train for the 85-th batch, train loss: 0.5089042782783508:  71%|█████████▏   | 84/119 [00:12<00:05,  6.48it/s]Epoch: 3, train for the 85-th batch, train loss: 0.5089042782783508:  71%|█████████▎   | 85/119 [00:12<00:05,  6.53it/s]Epoch: 3, train for the 17-th batch, train loss: 0.6375287771224976:  11%|█▍           | 16/146 [00:02<00:16,  7.78it/s]Epoch: 3, train for the 17-th batch, train loss: 0.6375287771224976:  12%|█▌           | 17/146 [00:02<00:16,  8.01it/s]Epoch: 2, train for the 32-th batch, train loss: 0.43482697010040283:  13%|█▌          | 31/241 [00:04<00:26,  7.79it/s]Epoch: 2, train for the 115-th batch, train loss: 0.4596244692802429:  75%|████████▎  | 114/151 [00:19<00:07,  5.04it/s]Epoch: 2, train for the 32-th batch, train loss: 0.43482697010040283:  13%|█▌          | 32/241 [00:04<00:29,  6.98it/s]Epoch: 2, train for the 115-th batch, train loss: 0.4596244692802429:  76%|████████▍  | 115/151 [00:19<00:07,  4.78it/s]Epoch: 1, train for the 245-th batch, train loss: 0.4357622563838959:  64%|███████    | 244/383 [01:06<00:41,  3.31it/s]evaluate for the 37-th batch, evaluate loss: 0.567152202129364:  55%|██████████▎        | 36/66 [00:04<00:03,  8.70it/s]evaluate for the 37-th batch, evaluate loss: 0.567152202129364:  56%|██████████▋        | 37/66 [00:04<00:03,  7.84it/s]Epoch: 3, train for the 86-th batch, train loss: 0.5414735674858093:  71%|█████████▎   | 85/119 [00:12<00:05,  6.53it/s]Epoch: 3, train for the 86-th batch, train loss: 0.5414735674858093:  72%|█████████▍   | 86/119 [00:12<00:05,  6.45it/s]Epoch: 2, train for the 33-th batch, train loss: 0.6164114475250244:  13%|█▋           | 32/241 [00:04<00:29,  6.98it/s]Epoch: 3, train for the 18-th batch, train loss: 0.6247045993804932:  12%|█▌           | 17/146 [00:02<00:16,  8.01it/s]Epoch: 3, train for the 18-th batch, train loss: 0.6247045993804932:  12%|█▌           | 18/146 [00:02<00:16,  7.57it/s]Epoch: 2, train for the 33-th batch, train loss: 0.6164114475250244:  14%|█▊           | 33/241 [00:04<00:28,  7.24it/s]Epoch: 1, train for the 245-th batch, train loss: 0.4357622563838959:  64%|███████    | 245/383 [01:06<00:40,  3.40it/s]Epoch: 2, train for the 116-th batch, train loss: 0.4480911195278168:  76%|████████▍  | 115/151 [00:19<00:07,  4.78it/s]Epoch: 2, train for the 116-th batch, train loss: 0.4480911195278168:  77%|████████▍  | 116/151 [00:19<00:06,  5.21it/s]evaluate for the 38-th batch, evaluate loss: 0.520214855670929:  56%|██████████▋        | 37/66 [00:04<00:03,  7.84it/s]evaluate for the 38-th batch, evaluate loss: 0.520214855670929:  58%|██████████▉        | 38/66 [00:04<00:03,  7.78it/s]Epoch: 3, train for the 87-th batch, train loss: 0.49463754892349243:  72%|████████▋   | 86/119 [00:13<00:05,  6.45it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5562545657157898:  14%|█▊           | 33/241 [00:04<00:28,  7.24it/s]Epoch: 3, train for the 87-th batch, train loss: 0.49463754892349243:  73%|████████▊   | 87/119 [00:13<00:04,  6.57it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5562545657157898:  14%|█▊           | 34/241 [00:04<00:28,  7.30it/s]Epoch: 3, train for the 19-th batch, train loss: 0.6271634697914124:  12%|█▌           | 18/146 [00:02<00:16,  7.57it/s]Epoch: 3, train for the 19-th batch, train loss: 0.6271634697914124:  13%|█▋           | 19/146 [00:02<00:17,  7.39it/s]Epoch: 2, train for the 117-th batch, train loss: 0.49870237708091736:  77%|███████▋  | 116/151 [00:19<00:06,  5.21it/s]Epoch: 2, train for the 117-th batch, train loss: 0.49870237708091736:  77%|███████▋  | 117/151 [00:19<00:06,  5.37it/s]Epoch: 1, train for the 246-th batch, train loss: 0.47825637459754944:  64%|██████▍   | 245/383 [01:06<00:40,  3.40it/s]Epoch: 2, train for the 35-th batch, train loss: 0.5744152069091797:  14%|█▊           | 34/241 [00:04<00:28,  7.30it/s]evaluate for the 39-th batch, evaluate loss: 0.5577815771102905:  58%|██████████▎       | 38/66 [00:04<00:03,  7.78it/s]evaluate for the 39-th batch, evaluate loss: 0.5577815771102905:  59%|██████████▋       | 39/66 [00:04<00:03,  7.64it/s]Epoch: 2, train for the 35-th batch, train loss: 0.5744152069091797:  15%|█▉           | 35/241 [00:04<00:26,  7.73it/s]Epoch: 3, train for the 88-th batch, train loss: 0.5279262661933899:  73%|█████████▌   | 87/119 [00:13<00:04,  6.57it/s]Epoch: 3, train for the 88-th batch, train loss: 0.5279262661933899:  74%|█████████▌   | 88/119 [00:13<00:04,  6.81it/s]Epoch: 3, train for the 20-th batch, train loss: 0.6119135022163391:  13%|█▋           | 19/146 [00:02<00:17,  7.39it/s]Epoch: 1, train for the 246-th batch, train loss: 0.47825637459754944:  64%|██████▍   | 246/383 [01:06<00:39,  3.46it/s]Epoch: 3, train for the 20-th batch, train loss: 0.6119135022163391:  14%|█▊           | 20/146 [00:02<00:18,  6.97it/s]evaluate for the 40-th batch, evaluate loss: 0.5399909615516663:  59%|██████████▋       | 39/66 [00:04<00:03,  7.64it/s]Epoch: 2, train for the 36-th batch, train loss: 0.5682212114334106:  15%|█▉           | 35/241 [00:04<00:26,  7.73it/s]Epoch: 2, train for the 36-th batch, train loss: 0.5682212114334106:  15%|█▉           | 36/241 [00:04<00:25,  8.10it/s]Epoch: 3, train for the 89-th batch, train loss: 0.5223645567893982:  74%|█████████▌   | 88/119 [00:13<00:04,  6.81it/s]Epoch: 3, train for the 89-th batch, train loss: 0.5223645567893982:  75%|█████████▋   | 89/119 [00:13<00:04,  6.91it/s]Epoch: 2, train for the 118-th batch, train loss: 0.4652823507785797:  77%|████████▌  | 117/151 [00:19<00:06,  5.37it/s]Epoch: 3, train for the 21-th batch, train loss: 0.5995029807090759:  14%|█▊           | 20/146 [00:02<00:18,  6.97it/s]Epoch: 2, train for the 118-th batch, train loss: 0.4652823507785797:  78%|████████▌  | 118/151 [00:19<00:06,  5.16it/s]Epoch: 3, train for the 21-th batch, train loss: 0.5995029807090759:  14%|█▊           | 21/146 [00:02<00:17,  7.02it/s]evaluate for the 41-th batch, evaluate loss: 0.5027954578399658:  59%|██████████▋       | 39/66 [00:05<00:03,  7.64it/s]evaluate for the 41-th batch, evaluate loss: 0.5027954578399658:  62%|███████████▏      | 41/66 [00:05<00:02,  8.46it/s]Epoch: 2, train for the 37-th batch, train loss: 0.4746567904949188:  15%|█▉           | 36/241 [00:04<00:25,  8.10it/s]Epoch: 2, train for the 37-th batch, train loss: 0.4746567904949188:  15%|█▉           | 37/241 [00:04<00:26,  7.62it/s]Epoch: 1, train for the 247-th batch, train loss: 0.3669063448905945:  64%|███████    | 246/383 [01:06<00:39,  3.46it/s]Epoch: 3, train for the 90-th batch, train loss: 0.4883621037006378:  75%|█████████▋   | 89/119 [00:13<00:04,  6.91it/s]Epoch: 3, train for the 90-th batch, train loss: 0.4883621037006378:  76%|█████████▊   | 90/119 [00:13<00:04,  6.82it/s]Epoch: 1, train for the 247-th batch, train loss: 0.3669063448905945:  64%|███████    | 247/383 [01:06<00:38,  3.53it/s]Epoch: 3, train for the 22-th batch, train loss: 0.6049817800521851:  14%|█▊           | 21/146 [00:03<00:17,  7.02it/s]evaluate for the 42-th batch, evaluate loss: 0.5392428040504456:  62%|███████████▏      | 41/66 [00:05<00:02,  8.46it/s]evaluate for the 42-th batch, evaluate loss: 0.5392428040504456:  64%|███████████▍      | 42/66 [00:05<00:02,  8.20it/s]Epoch: 3, train for the 22-th batch, train loss: 0.6049817800521851:  15%|█▉           | 22/146 [00:03<00:17,  7.10it/s]Epoch: 2, train for the 119-th batch, train loss: 0.5070009231567383:  78%|████████▌  | 118/151 [00:19<00:06,  5.16it/s]Epoch: 2, train for the 38-th batch, train loss: 0.5553897619247437:  15%|█▉           | 37/241 [00:05<00:26,  7.62it/s]Epoch: 2, train for the 38-th batch, train loss: 0.5553897619247437:  16%|██           | 38/241 [00:05<00:27,  7.43it/s]Epoch: 2, train for the 119-th batch, train loss: 0.5070009231567383:  79%|████████▋  | 119/151 [00:19<00:06,  4.92it/s]evaluate for the 43-th batch, evaluate loss: 0.5213682055473328:  64%|███████████▍      | 42/66 [00:05<00:02,  8.20it/s]evaluate for the 43-th batch, evaluate loss: 0.5213682055473328:  65%|███████████▋      | 43/66 [00:05<00:02,  8.07it/s]Epoch: 3, train for the 91-th batch, train loss: 0.47547683119773865:  76%|█████████   | 90/119 [00:13<00:04,  6.82it/s]Epoch: 3, train for the 23-th batch, train loss: 0.6105760931968689:  15%|█▉           | 22/146 [00:03<00:17,  7.10it/s]Epoch: 3, train for the 23-th batch, train loss: 0.6105760931968689:  16%|██           | 23/146 [00:03<00:17,  7.10it/s]Epoch: 3, train for the 91-th batch, train loss: 0.47547683119773865:  76%|█████████▏  | 91/119 [00:13<00:04,  6.45it/s]Epoch: 2, train for the 39-th batch, train loss: 0.6030449271202087:  16%|██           | 38/241 [00:05<00:27,  7.43it/s]Epoch: 2, train for the 39-th batch, train loss: 0.6030449271202087:  16%|██           | 39/241 [00:05<00:27,  7.28it/s]evaluate for the 44-th batch, evaluate loss: 0.543524980545044:  65%|████████████▍      | 43/66 [00:05<00:02,  8.07it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5226854085922241:  79%|████████▋  | 119/151 [00:19<00:06,  4.92it/s]Epoch: 3, train for the 24-th batch, train loss: 0.5956180691719055:  16%|██           | 23/146 [00:03<00:17,  7.10it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5226854085922241:  79%|████████▋  | 120/151 [00:20<00:06,  5.06it/s]Epoch: 3, train for the 24-th batch, train loss: 0.5956180691719055:  16%|██▏          | 24/146 [00:03<00:16,  7.20it/s]Epoch: 3, train for the 92-th batch, train loss: 0.5216765403747559:  76%|█████████▉   | 91/119 [00:13<00:04,  6.45it/s]Epoch: 3, train for the 92-th batch, train loss: 0.5216765403747559:  77%|██████████   | 92/119 [00:13<00:04,  6.25it/s]Epoch: 2, train for the 40-th batch, train loss: 0.6250225901603699:  16%|██           | 39/241 [00:05<00:27,  7.28it/s]Epoch: 2, train for the 40-th batch, train loss: 0.6250225901603699:  17%|██▏          | 40/241 [00:05<00:27,  7.19it/s]Epoch: 1, train for the 248-th batch, train loss: 0.36249762773513794:  64%|██████▍   | 247/383 [01:07<00:38,  3.53it/s]evaluate for the 45-th batch, evaluate loss: 0.5180914402008057:  65%|███████████▋      | 43/66 [00:05<00:02,  8.07it/s]evaluate for the 45-th batch, evaluate loss: 0.5180914402008057:  68%|████████████▎     | 45/66 [00:05<00:02,  7.94it/s]Epoch: 3, train for the 25-th batch, train loss: 0.5924100875854492:  16%|██▏          | 24/146 [00:03<00:16,  7.20it/s]Epoch: 3, train for the 25-th batch, train loss: 0.5924100875854492:  17%|██▏          | 25/146 [00:03<00:16,  7.47it/s]Epoch: 2, train for the 121-th batch, train loss: 0.5081379413604736:  79%|████████▋  | 120/151 [00:20<00:06,  5.06it/s]Epoch: 1, train for the 248-th batch, train loss: 0.36249762773513794:  65%|██████▍   | 248/383 [01:07<00:44,  3.02it/s]Epoch: 2, train for the 121-th batch, train loss: 0.5081379413604736:  80%|████████▊  | 121/151 [00:20<00:05,  5.24it/s]Epoch: 3, train for the 93-th batch, train loss: 0.4688750207424164:  77%|██████████   | 92/119 [00:14<00:04,  6.25it/s]Epoch: 3, train for the 93-th batch, train loss: 0.4688750207424164:  78%|██████████▏  | 93/119 [00:14<00:03,  6.52it/s]Epoch: 2, train for the 41-th batch, train loss: 0.54038405418396:  17%|██▍            | 40/241 [00:05<00:27,  7.19it/s]Epoch: 2, train for the 41-th batch, train loss: 0.54038405418396:  17%|██▌            | 41/241 [00:05<00:27,  7.19it/s]Epoch: 3, train for the 26-th batch, train loss: 0.5467756390571594:  17%|██▏          | 25/146 [00:03<00:16,  7.47it/s]Epoch: 3, train for the 26-th batch, train loss: 0.5467756390571594:  18%|██▎          | 26/146 [00:03<00:15,  7.91it/s]evaluate for the 46-th batch, evaluate loss: 0.5717538595199585:  68%|████████████▎     | 45/66 [00:05<00:02,  7.94it/s]evaluate for the 46-th batch, evaluate loss: 0.5717538595199585:  70%|████████████▌     | 46/66 [00:05<00:02,  8.02it/s]Epoch: 3, train for the 94-th batch, train loss: 0.4435599446296692:  78%|██████████▏  | 93/119 [00:14<00:03,  6.52it/s]Epoch: 3, train for the 94-th batch, train loss: 0.4435599446296692:  79%|██████████▎  | 94/119 [00:14<00:03,  6.57it/s]Epoch: 2, train for the 122-th batch, train loss: 0.47214263677597046:  80%|████████  | 121/151 [00:20<00:05,  5.24it/s]Epoch: 3, train for the 27-th batch, train loss: 0.5704354047775269:  18%|██▎          | 26/146 [00:03<00:15,  7.91it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5691432356834412:  17%|██▏          | 41/241 [00:05<00:27,  7.19it/s]Epoch: 2, train for the 122-th batch, train loss: 0.47214263677597046:  81%|████████  | 122/151 [00:20<00:05,  5.19it/s]Epoch: 3, train for the 27-th batch, train loss: 0.5704354047775269:  18%|██▍          | 27/146 [00:03<00:15,  7.68it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5691432356834412:  17%|██▎          | 42/241 [00:05<00:28,  6.92it/s]Epoch: 1, train for the 249-th batch, train loss: 0.43439429998397827:  65%|██████▍   | 248/383 [01:07<00:44,  3.02it/s]evaluate for the 47-th batch, evaluate loss: 0.5594823956489563:  70%|████████████▌     | 46/66 [00:05<00:02,  8.02it/s]evaluate for the 47-th batch, evaluate loss: 0.5594823956489563:  71%|████████████▊     | 47/66 [00:05<00:02,  7.27it/s]Epoch: 3, train for the 95-th batch, train loss: 0.43893107771873474:  79%|█████████▍  | 94/119 [00:14<00:03,  6.57it/s]Epoch: 1, train for the 249-th batch, train loss: 0.43439429998397827:  65%|██████▌   | 249/383 [01:07<00:43,  3.11it/s]Epoch: 3, train for the 95-th batch, train loss: 0.43893107771873474:  80%|█████████▌  | 95/119 [00:14<00:03,  6.70it/s]Epoch: 3, train for the 28-th batch, train loss: 0.5458813905715942:  18%|██▍          | 27/146 [00:03<00:15,  7.68it/s]Epoch: 2, train for the 43-th batch, train loss: 0.6079319715499878:  17%|██▎          | 42/241 [00:05<00:28,  6.92it/s]Epoch: 3, train for the 28-th batch, train loss: 0.5458813905715942:  19%|██▍          | 28/146 [00:03<00:15,  7.70it/s]Epoch: 2, train for the 43-th batch, train loss: 0.6079319715499878:  18%|██▎          | 43/241 [00:05<00:27,  7.13it/s]evaluate for the 48-th batch, evaluate loss: 0.5477479696273804:  71%|████████████▊     | 47/66 [00:05<00:02,  7.27it/s]evaluate for the 48-th batch, evaluate loss: 0.5477479696273804:  73%|█████████████     | 48/66 [00:05<00:02,  7.69it/s]Epoch: 2, train for the 123-th batch, train loss: 0.4906825125217438:  81%|████████▉  | 122/151 [00:20<00:05,  5.19it/s]Epoch: 2, train for the 123-th batch, train loss: 0.4906825125217438:  81%|████████▉  | 123/151 [00:20<00:05,  5.18it/s]Epoch: 3, train for the 96-th batch, train loss: 0.45970916748046875:  80%|█████████▌  | 95/119 [00:14<00:03,  6.70it/s]Epoch: 2, train for the 44-th batch, train loss: 0.6376184821128845:  18%|██▎          | 43/241 [00:05<00:27,  7.13it/s]Epoch: 2, train for the 44-th batch, train loss: 0.6376184821128845:  18%|██▎          | 44/241 [00:05<00:26,  7.31it/s]Epoch: 3, train for the 96-th batch, train loss: 0.45970916748046875:  81%|█████████▋  | 96/119 [00:14<00:03,  6.56it/s]Epoch: 3, train for the 29-th batch, train loss: 0.5161439776420593:  19%|██▍          | 28/146 [00:03<00:15,  7.70it/s]evaluate for the 49-th batch, evaluate loss: 0.568743109703064:  73%|█████████████▊     | 48/66 [00:06<00:02,  7.69it/s]evaluate for the 49-th batch, evaluate loss: 0.568743109703064:  74%|██████████████     | 49/66 [00:06<00:02,  7.79it/s]Epoch: 3, train for the 29-th batch, train loss: 0.5161439776420593:  20%|██▌          | 29/146 [00:03<00:15,  7.45it/s]Epoch: 1, train for the 250-th batch, train loss: 0.4284803569316864:  65%|███████▏   | 249/383 [01:07<00:43,  3.11it/s]Epoch: 2, train for the 124-th batch, train loss: 0.5140572190284729:  81%|████████▉  | 123/151 [00:20<00:05,  5.18it/s]evaluate for the 50-th batch, evaluate loss: 0.5932238101959229:  74%|█████████████▎    | 49/66 [00:06<00:02,  7.79it/s]Epoch: 2, train for the 45-th batch, train loss: 0.46930983662605286:  18%|██▏         | 44/241 [00:06<00:26,  7.31it/s]Epoch: 2, train for the 124-th batch, train loss: 0.5140572190284729:  82%|█████████  | 124/151 [00:20<00:05,  5.22it/s]Epoch: 2, train for the 45-th batch, train loss: 0.46930983662605286:  19%|██▏         | 45/241 [00:06<00:25,  7.60it/s]Epoch: 1, train for the 250-th batch, train loss: 0.4284803569316864:  65%|███████▏   | 250/383 [01:07<00:41,  3.18it/s]Epoch: 3, train for the 30-th batch, train loss: 0.5078016519546509:  20%|██▌          | 29/146 [00:04<00:15,  7.45it/s]Epoch: 3, train for the 30-th batch, train loss: 0.5078016519546509:  21%|██▋          | 30/146 [00:04<00:15,  7.60it/s]Epoch: 3, train for the 97-th batch, train loss: 0.48144954442977905:  81%|█████████▋  | 96/119 [00:14<00:03,  6.56it/s]Epoch: 3, train for the 97-th batch, train loss: 0.48144954442977905:  82%|█████████▊  | 97/119 [00:14<00:03,  6.18it/s]evaluate for the 51-th batch, evaluate loss: 0.5570501685142517:  74%|█████████████▎    | 49/66 [00:06<00:02,  7.79it/s]evaluate for the 51-th batch, evaluate loss: 0.5570501685142517:  77%|█████████████▉    | 51/66 [00:06<00:01,  8.76it/s]Epoch: 3, train for the 31-th batch, train loss: 0.5610783696174622:  21%|██▋          | 30/146 [00:04<00:15,  7.60it/s]Epoch: 3, train for the 31-th batch, train loss: 0.5610783696174622:  21%|██▊          | 31/146 [00:04<00:15,  7.39it/s]Epoch: 2, train for the 46-th batch, train loss: 0.5250030159950256:  19%|██▍          | 45/241 [00:06<00:25,  7.60it/s]evaluate for the 52-th batch, evaluate loss: 0.5472056269645691:  77%|█████████████▉    | 51/66 [00:06<00:01,  8.76it/s]Epoch: 2, train for the 46-th batch, train loss: 0.5250030159950256:  19%|██▍          | 46/241 [00:06<00:28,  6.87it/s]Epoch: 2, train for the 125-th batch, train loss: 0.5249612331390381:  82%|█████████  | 124/151 [00:20<00:05,  5.22it/s]Epoch: 3, train for the 98-th batch, train loss: 0.44616079330444336:  82%|█████████▊  | 97/119 [00:14<00:03,  6.18it/s]Epoch: 2, train for the 125-th batch, train loss: 0.5249612331390381:  83%|█████████  | 125/151 [00:20<00:05,  4.90it/s]Epoch: 1, train for the 251-th batch, train loss: 0.4377201497554779:  65%|███████▏   | 250/383 [01:08<00:41,  3.18it/s]Epoch: 3, train for the 98-th batch, train loss: 0.44616079330444336:  82%|█████████▉  | 98/119 [00:14<00:03,  6.15it/s]Epoch: 3, train for the 32-th batch, train loss: 0.5129687190055847:  21%|██▊          | 31/146 [00:04<00:15,  7.39it/s]Epoch: 3, train for the 32-th batch, train loss: 0.5129687190055847:  22%|██▊          | 32/146 [00:04<00:14,  7.92it/s]evaluate for the 53-th batch, evaluate loss: 0.5231936573982239:  77%|█████████████▉    | 51/66 [00:06<00:01,  8.76it/s]evaluate for the 53-th batch, evaluate loss: 0.5231936573982239:  80%|██████████████▍   | 53/66 [00:06<00:01,  9.36it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5795833468437195:  19%|██▍          | 46/241 [00:06<00:28,  6.87it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5795833468437195:  20%|██▌          | 47/241 [00:06<00:27,  6.99it/s]Epoch: 1, train for the 251-th batch, train loss: 0.4377201497554779:  66%|███████▏   | 251/383 [01:08<00:42,  3.14it/s]evaluate for the 54-th batch, evaluate loss: 0.563590943813324:  80%|███████████████▎   | 53/66 [00:06<00:01,  9.36it/s]evaluate for the 54-th batch, evaluate loss: 0.563590943813324:  82%|███████████████▌   | 54/66 [00:06<00:01,  9.21it/s]Epoch: 3, train for the 99-th batch, train loss: 0.47078028321266174:  82%|█████████▉  | 98/119 [00:14<00:03,  6.15it/s]Epoch: 2, train for the 126-th batch, train loss: 0.5133641958236694:  83%|█████████  | 125/151 [00:21<00:05,  4.90it/s]Epoch: 3, train for the 99-th batch, train loss: 0.47078028321266174:  83%|█████████▉  | 99/119 [00:14<00:03,  6.04it/s]Epoch: 3, train for the 33-th batch, train loss: 0.5023857951164246:  22%|██▊          | 32/146 [00:04<00:14,  7.92it/s]Epoch: 2, train for the 126-th batch, train loss: 0.5133641958236694:  83%|█████████▏ | 126/151 [00:21<00:05,  4.96it/s]Epoch: 3, train for the 33-th batch, train loss: 0.5023857951164246:  23%|██▉          | 33/146 [00:04<00:15,  7.28it/s]Epoch: 2, train for the 48-th batch, train loss: 0.5588926076889038:  20%|██▌          | 47/241 [00:06<00:27,  6.99it/s]Epoch: 2, train for the 48-th batch, train loss: 0.5588926076889038:  20%|██▌          | 48/241 [00:06<00:27,  6.97it/s]evaluate for the 55-th batch, evaluate loss: 0.5062798261642456:  82%|██████████████▋   | 54/66 [00:06<00:01,  9.21it/s]evaluate for the 55-th batch, evaluate loss: 0.5062798261642456:  83%|███████████████   | 55/66 [00:06<00:01,  9.23it/s]Epoch: 3, train for the 100-th batch, train loss: 0.485340416431427:  83%|██████████▊  | 99/119 [00:15<00:03,  6.04it/s]Epoch: 3, train for the 100-th batch, train loss: 0.485340416431427:  84%|██████████  | 100/119 [00:15<00:03,  6.22it/s]Epoch: 1, train for the 252-th batch, train loss: 0.31654423475265503:  66%|██████▌   | 251/383 [01:08<00:42,  3.14it/s]Epoch: 3, train for the 34-th batch, train loss: 0.5442779660224915:  23%|██▉          | 33/146 [00:04<00:15,  7.28it/s]Epoch: 3, train for the 34-th batch, train loss: 0.5442779660224915:  23%|███          | 34/146 [00:04<00:16,  6.91it/s]evaluate for the 56-th batch, evaluate loss: 0.5336557030677795:  83%|███████████████   | 55/66 [00:06<00:01,  9.23it/s]Epoch: 1, train for the 252-th batch, train loss: 0.31654423475265503:  66%|██████▌   | 252/383 [01:08<00:40,  3.24it/s]Epoch: 2, train for the 127-th batch, train loss: 0.4831368923187256:  83%|█████████▏ | 126/151 [00:21<00:05,  4.96it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5538373589515686:  20%|██▌          | 48/241 [00:06<00:27,  6.97it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5538373589515686:  20%|██▋          | 49/241 [00:06<00:30,  6.38it/s]Epoch: 2, train for the 127-th batch, train loss: 0.4831368923187256:  84%|█████████▎ | 127/151 [00:21<00:05,  4.80it/s]Epoch: 3, train for the 101-th batch, train loss: 0.46957841515541077:  84%|████████▍ | 100/119 [00:15<00:03,  6.22it/s]evaluate for the 57-th batch, evaluate loss: 0.5983740091323853:  83%|███████████████   | 55/66 [00:06<00:01,  9.23it/s]evaluate for the 57-th batch, evaluate loss: 0.5983740091323853:  86%|███████████████▌  | 57/66 [00:06<00:00,  9.46it/s]Epoch: 3, train for the 101-th batch, train loss: 0.46957841515541077:  85%|████████▍ | 101/119 [00:15<00:02,  6.33it/s]Epoch: 3, train for the 35-th batch, train loss: 0.4625253975391388:  23%|███          | 34/146 [00:04<00:16,  6.91it/s]Epoch: 3, train for the 35-th batch, train loss: 0.4625253975391388:  24%|███          | 35/146 [00:04<00:15,  7.20it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5555009245872498:  20%|██▋          | 49/241 [00:06<00:30,  6.38it/s]evaluate for the 58-th batch, evaluate loss: 0.5566906332969666:  86%|███████████████▌  | 57/66 [00:06<00:00,  9.46it/s]evaluate for the 58-th batch, evaluate loss: 0.5566906332969666:  88%|███████████████▊  | 58/66 [00:06<00:00,  9.41it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5555009245872498:  21%|██▋          | 50/241 [00:06<00:29,  6.38it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5466787815093994:  84%|█████████▎ | 127/151 [00:21<00:05,  4.80it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5466787815093994:  85%|█████████▎ | 128/151 [00:21<00:04,  4.74it/s]Epoch: 1, train for the 253-th batch, train loss: 0.3192070424556732:  66%|███████▏   | 252/383 [01:08<00:40,  3.24it/s]Epoch: 3, train for the 102-th batch, train loss: 0.5379666686058044:  85%|█████████▎ | 101/119 [00:15<00:02,  6.33it/s]Epoch: 3, train for the 36-th batch, train loss: 0.46826478838920593:  24%|██▉         | 35/146 [00:05<00:15,  7.20it/s]evaluate for the 59-th batch, evaluate loss: 0.5694695115089417:  88%|███████████████▊  | 58/66 [00:07<00:00,  9.41it/s]evaluate for the 59-th batch, evaluate loss: 0.5694695115089417:  89%|████████████████  | 59/66 [00:07<00:00,  9.36it/s]Epoch: 2, train for the 51-th batch, train loss: 0.5416958332061768:  21%|██▋          | 50/241 [00:06<00:29,  6.38it/s]Epoch: 3, train for the 36-th batch, train loss: 0.46826478838920593:  25%|██▉         | 36/146 [00:05<00:17,  6.35it/s]Epoch: 3, train for the 102-th batch, train loss: 0.5379666686058044:  86%|█████████▍ | 102/119 [00:15<00:02,  5.68it/s]Epoch: 2, train for the 51-th batch, train loss: 0.5416958332061768:  21%|██▊          | 51/241 [00:06<00:27,  6.84it/s]Epoch: 1, train for the 253-th batch, train loss: 0.3192070424556732:  66%|███████▎   | 253/383 [01:08<00:41,  3.14it/s]Epoch: 2, train for the 129-th batch, train loss: 0.5586571097373962:  85%|█████████▎ | 128/151 [00:21<00:04,  4.74it/s]evaluate for the 60-th batch, evaluate loss: 0.5708930492401123:  89%|████████████████  | 59/66 [00:07<00:00,  9.36it/s]evaluate for the 60-th batch, evaluate loss: 0.5708930492401123:  91%|████████████████▎ | 60/66 [00:07<00:00,  9.24it/s]Epoch: 2, train for the 129-th batch, train loss: 0.5586571097373962:  85%|█████████▍ | 129/151 [00:21<00:04,  5.09it/s]Epoch: 2, train for the 52-th batch, train loss: 0.5925047397613525:  21%|██▊          | 51/241 [00:07<00:27,  6.84it/s]Epoch: 3, train for the 37-th batch, train loss: 0.5189825892448425:  25%|███▏         | 36/146 [00:05<00:17,  6.35it/s]Epoch: 2, train for the 52-th batch, train loss: 0.5925047397613525:  22%|██▊          | 52/241 [00:07<00:26,  7.10it/s]Epoch: 3, train for the 37-th batch, train loss: 0.5189825892448425:  25%|███▎         | 37/146 [00:05<00:16,  6.56it/s]Epoch: 3, train for the 103-th batch, train loss: 0.4607124328613281:  86%|█████████▍ | 102/119 [00:15<00:02,  5.68it/s]Epoch: 3, train for the 103-th batch, train loss: 0.4607124328613281:  87%|█████████▌ | 103/119 [00:15<00:02,  5.74it/s]evaluate for the 61-th batch, evaluate loss: 0.5766717195510864:  91%|████████████████▎ | 60/66 [00:07<00:00,  9.24it/s]evaluate for the 61-th batch, evaluate loss: 0.5766717195510864:  92%|████████████████▋ | 61/66 [00:07<00:00,  8.37it/s]Epoch: 3, train for the 38-th batch, train loss: 0.5271244049072266:  25%|███▎         | 37/146 [00:05<00:16,  6.56it/s]Epoch: 3, train for the 38-th batch, train loss: 0.5271244049072266:  26%|███▍         | 38/146 [00:05<00:16,  6.70it/s]Epoch: 2, train for the 130-th batch, train loss: 0.510281503200531:  85%|██████████▎ | 129/151 [00:21<00:04,  5.09it/s]Epoch: 1, train for the 254-th batch, train loss: 0.4541417062282562:  66%|███████▎   | 253/383 [01:09<00:41,  3.14it/s]Epoch: 2, train for the 53-th batch, train loss: 0.419532835483551:  22%|███           | 52/241 [00:07<00:26,  7.10it/s]Epoch: 3, train for the 104-th batch, train loss: 0.48726263642311096:  87%|████████▋ | 103/119 [00:15<00:02,  5.74it/s]Epoch: 2, train for the 53-th batch, train loss: 0.419532835483551:  22%|███           | 53/241 [00:07<00:28,  6.55it/s]Epoch: 2, train for the 130-th batch, train loss: 0.510281503200531:  86%|██████████▎ | 130/151 [00:22<00:04,  4.96it/s]Epoch: 3, train for the 104-th batch, train loss: 0.48726263642311096:  87%|████████▋ | 104/119 [00:15<00:02,  5.97it/s]Epoch: 1, train for the 254-th batch, train loss: 0.4541417062282562:  66%|███████▎   | 254/383 [01:09<00:41,  3.14it/s]evaluate for the 62-th batch, evaluate loss: 0.5941420793533325:  92%|████████████████▋ | 61/66 [00:07<00:00,  8.37it/s]evaluate for the 62-th batch, evaluate loss: 0.5941420793533325:  94%|████████████████▉ | 62/66 [00:07<00:00,  8.70it/s]Epoch: 3, train for the 39-th batch, train loss: 0.4923107922077179:  26%|███▍         | 38/146 [00:05<00:16,  6.70it/s]Epoch: 3, train for the 39-th batch, train loss: 0.4923107922077179:  27%|███▍         | 39/146 [00:05<00:14,  7.14it/s]Epoch: 2, train for the 54-th batch, train loss: 0.372342973947525:  22%|███           | 53/241 [00:07<00:28,  6.55it/s]Epoch: 2, train for the 54-th batch, train loss: 0.372342973947525:  22%|███▏          | 54/241 [00:07<00:27,  6.89it/s]Epoch: 3, train for the 105-th batch, train loss: 0.4809093475341797:  87%|█████████▌ | 104/119 [00:15<00:02,  5.97it/s]Epoch: 3, train for the 105-th batch, train loss: 0.4809093475341797:  88%|█████████▋ | 105/119 [00:16<00:02,  5.93it/s]evaluate for the 63-th batch, evaluate loss: 0.548812985420227:  94%|█████████████████▊ | 62/66 [00:07<00:00,  8.70it/s]evaluate for the 63-th batch, evaluate loss: 0.548812985420227:  95%|██████████████████▏| 63/66 [00:07<00:00,  8.16it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5310304164886475:  86%|█████████▍ | 130/151 [00:22<00:04,  4.96it/s]Epoch: 3, train for the 40-th batch, train loss: 0.4920322299003601:  27%|███▍         | 39/146 [00:05<00:14,  7.14it/s]Epoch: 3, train for the 40-th batch, train loss: 0.4920322299003601:  27%|███▌         | 40/146 [00:05<00:14,  7.25it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5310304164886475:  87%|█████████▌ | 131/151 [00:22<00:04,  4.87it/s]Epoch: 2, train for the 55-th batch, train loss: 0.3649090528488159:  22%|██▉          | 54/241 [00:07<00:27,  6.89it/s]Epoch: 2, train for the 55-th batch, train loss: 0.3649090528488159:  23%|██▉          | 55/241 [00:07<00:25,  7.21it/s]Epoch: 1, train for the 255-th batch, train loss: 0.40289387106895447:  66%|██████▋   | 254/383 [01:09<00:41,  3.14it/s]Epoch: 3, train for the 106-th batch, train loss: 0.4981396496295929:  88%|█████████▋ | 105/119 [00:16<00:02,  5.93it/s]Epoch: 3, train for the 106-th batch, train loss: 0.4981396496295929:  89%|█████████▊ | 106/119 [00:16<00:02,  6.45it/s]evaluate for the 64-th batch, evaluate loss: 0.5458177328109741:  95%|█████████████████▏| 63/66 [00:07<00:00,  8.16it/s]evaluate for the 64-th batch, evaluate loss: 0.5458177328109741:  97%|█████████████████▍| 64/66 [00:07<00:00,  8.00it/s]Epoch: 1, train for the 255-th batch, train loss: 0.40289387106895447:  67%|██████▋   | 255/383 [01:09<00:39,  3.20it/s]Epoch: 3, train for the 41-th batch, train loss: 0.4918499290943146:  27%|███▌         | 40/146 [00:05<00:14,  7.25it/s]Epoch: 3, train for the 41-th batch, train loss: 0.4918499290943146:  28%|███▋         | 41/146 [00:05<00:14,  7.25it/s]Epoch: 2, train for the 56-th batch, train loss: 0.3880050480365753:  23%|██▉          | 55/241 [00:07<00:25,  7.21it/s]Epoch: 2, train for the 56-th batch, train loss: 0.3880050480365753:  23%|███          | 56/241 [00:07<00:26,  6.92it/s]evaluate for the 65-th batch, evaluate loss: 0.5882555842399597:  97%|█████████████████▍| 64/66 [00:07<00:00,  8.00it/s]evaluate for the 65-th batch, evaluate loss: 0.5882555842399597:  98%|█████████████████▋| 65/66 [00:07<00:00,  8.49it/s]Epoch: 3, train for the 107-th batch, train loss: 0.519353449344635:  89%|██████████▋ | 106/119 [00:16<00:02,  6.45it/s]Epoch: 2, train for the 132-th batch, train loss: 0.503052830696106:  87%|██████████▍ | 131/151 [00:22<00:04,  4.87it/s]Epoch: 3, train for the 107-th batch, train loss: 0.519353449344635:  90%|██████████▊ | 107/119 [00:16<00:01,  6.77it/s]Epoch: 2, train for the 132-th batch, train loss: 0.503052830696106:  87%|██████████▍ | 132/151 [00:22<00:04,  4.62it/s]Epoch: 3, train for the 42-th batch, train loss: 0.49738067388534546:  28%|███▎        | 41/146 [00:05<00:14,  7.25it/s]Epoch: 3, train for the 42-th batch, train loss: 0.49738067388534546:  29%|███▍        | 42/146 [00:05<00:13,  7.58it/s]Epoch: 2, train for the 57-th batch, train loss: 0.4342736303806305:  23%|███          | 56/241 [00:07<00:26,  6.92it/s]Epoch: 2, train for the 57-th batch, train loss: 0.4342736303806305:  24%|███          | 57/241 [00:07<00:26,  6.99it/s]evaluate for the 66-th batch, evaluate loss: 0.5423487424850464:  98%|█████████████████▋| 65/66 [00:07<00:00,  8.49it/s]evaluate for the 66-th batch, evaluate loss: 0.5423487424850464: 100%|██████████████████| 66/66 [00:07<00:00,  7.53it/s]evaluate for the 66-th batch, evaluate loss: 0.5423487424850464: 100%|██████████████████| 66/66 [00:07<00:00,  8.25it/s]
Epoch: 1, train for the 256-th batch, train loss: 0.3441716730594635:  67%|███████▎   | 255/383 [01:09<00:39,  3.20it/s]Epoch: 2, train for the 133-th batch, train loss: 0.4536759853363037:  87%|█████████▌ | 132/151 [00:22<00:04,  4.62it/s]Epoch: 3, train for the 43-th batch, train loss: 0.5001527667045593:  29%|███▋         | 42/146 [00:05<00:13,  7.58it/s]Epoch: 3, train for the 108-th batch, train loss: 0.40743833780288696:  90%|████████▉ | 107/119 [00:16<00:01,  6.77it/s]Epoch: 1, train for the 256-th batch, train loss: 0.3441716730594635:  67%|███████▎   | 256/383 [01:09<00:39,  3.24it/s]Epoch: 3, train for the 43-th batch, train loss: 0.5001527667045593:  29%|███▊         | 43/146 [00:05<00:14,  6.94it/s]Epoch: 3, train for the 108-th batch, train loss: 0.40743833780288696:  91%|█████████ | 108/119 [00:16<00:01,  5.96it/s]Epoch: 2, train for the 58-th batch, train loss: 0.3375948369503021:  24%|███          | 57/241 [00:07<00:26,  6.99it/s]Epoch: 2, train for the 133-th batch, train loss: 0.4536759853363037:  88%|█████████▋ | 133/151 [00:22<00:03,  4.74it/s]Epoch: 2, train for the 58-th batch, train loss: 0.3375948369503021:  24%|███▏         | 58/241 [00:07<00:24,  7.42it/s]Epoch: 3, train for the 44-th batch, train loss: 0.4757614731788635:  29%|███▊         | 43/146 [00:06<00:14,  6.94it/s]Epoch: 3, train for the 44-th batch, train loss: 0.4757614731788635:  30%|███▉         | 44/146 [00:06<00:15,  6.71it/s]Epoch: 3, train for the 109-th batch, train loss: 0.4858565330505371:  91%|█████████▉ | 108/119 [00:16<00:01,  5.96it/s]Epoch: 1, train for the 257-th batch, train loss: 0.4948965311050415:  67%|███████▎   | 256/383 [01:09<00:39,  3.24it/s]Epoch: 3, train for the 109-th batch, train loss: 0.4858565330505371:  92%|██████████ | 109/119 [00:16<00:01,  5.95it/s]Epoch: 2, train for the 59-th batch, train loss: 0.607185423374176:  24%|███▎          | 58/241 [00:08<00:24,  7.42it/s]Epoch: 2, train for the 59-th batch, train loss: 0.607185423374176:  24%|███▍          | 59/241 [00:08<00:26,  6.85it/s]Epoch: 2, train for the 134-th batch, train loss: 0.48614194989204407:  88%|████████▊ | 133/151 [00:22<00:03,  4.74it/s]Epoch: 1, train for the 257-th batch, train loss: 0.4948965311050415:  67%|███████▍   | 257/383 [01:10<00:35,  3.52it/s]Epoch: 2, train for the 134-th batch, train loss: 0.48614194989204407:  89%|████████▊ | 134/151 [00:22<00:03,  4.64it/s]Epoch: 3, train for the 45-th batch, train loss: 0.4985802471637726:  30%|███▉         | 44/146 [00:06<00:15,  6.71it/s]Epoch: 3, train for the 45-th batch, train loss: 0.4985802471637726:  31%|████         | 45/146 [00:06<00:14,  7.08it/s]Epoch: 3, train for the 110-th batch, train loss: 0.5082389712333679:  92%|██████████ | 109/119 [00:16<00:01,  5.95it/s]Epoch: 3, train for the 110-th batch, train loss: 0.5082389712333679:  92%|██████████▏| 110/119 [00:16<00:01,  6.13it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5922206044197083:  24%|███▏         | 59/241 [00:08<00:26,  6.85it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5922206044197083:  25%|███▏         | 60/241 [00:08<00:26,  6.76it/s]Epoch: 3, train for the 46-th batch, train loss: 0.48799577355384827:  31%|███▋        | 45/146 [00:06<00:14,  7.08it/s]Epoch: 3, train for the 46-th batch, train loss: 0.48799577355384827:  32%|███▊        | 46/146 [00:06<00:13,  7.34it/s]Epoch: 2, train for the 135-th batch, train loss: 0.4615485966205597:  89%|█████████▊ | 134/151 [00:23<00:03,  4.64it/s]Epoch: 2, train for the 135-th batch, train loss: 0.4615485966205597:  89%|█████████▊ | 135/151 [00:23<00:03,  4.78it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5573652386665344:  25%|███▏         | 60/241 [00:08<00:26,  6.76it/s]Epoch: 3, train for the 111-th batch, train loss: 0.5618881583213806:  92%|██████████▏| 110/119 [00:16<00:01,  6.13it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5573652386665344:  25%|███▎         | 61/241 [00:08<00:24,  7.27it/s]Epoch: 3, train for the 111-th batch, train loss: 0.5618881583213806:  93%|██████████▎| 111/119 [00:16<00:01,  6.38it/s]Epoch: 1, train for the 258-th batch, train loss: 0.339862197637558:  67%|████████    | 257/383 [01:10<00:35,  3.52it/s]Epoch: 3, train for the 47-th batch, train loss: 0.5119710564613342:  32%|████         | 46/146 [00:06<00:13,  7.34it/s]Epoch: 3, train for the 47-th batch, train loss: 0.5119710564613342:  32%|████▏        | 47/146 [00:06<00:12,  7.69it/s]Epoch: 1, train for the 258-th batch, train loss: 0.339862197637558:  67%|████████    | 258/383 [01:10<00:36,  3.41it/s]Epoch: 3, train for the 112-th batch, train loss: 0.49315351247787476:  93%|█████████▎| 111/119 [00:17<00:01,  6.38it/s]Epoch: 3, train for the 112-th batch, train loss: 0.49315351247787476:  94%|█████████▍| 112/119 [00:17<00:01,  6.55it/s]Epoch: 2, train for the 136-th batch, train loss: 0.5144394040107727:  89%|█████████▊ | 135/151 [00:23<00:03,  4.78it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5766907930374146:  25%|███▎         | 61/241 [00:08<00:24,  7.27it/s]Epoch: 3, train for the 48-th batch, train loss: 0.4752136468887329:  32%|████▏        | 47/146 [00:06<00:12,  7.69it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5766907930374146:  26%|███▎         | 62/241 [00:08<00:26,  6.69it/s]Epoch: 3, train for the 48-th batch, train loss: 0.4752136468887329:  33%|████▎        | 48/146 [00:06<00:12,  7.99it/s]Epoch: 2, train for the 136-th batch, train loss: 0.5144394040107727:  90%|█████████▉ | 136/151 [00:23<00:03,  4.80it/s]Epoch: 1, train for the 259-th batch, train loss: 0.4262837767601013:  67%|███████▍   | 258/383 [01:10<00:36,  3.41it/s]Epoch: 2, train for the 63-th batch, train loss: 0.7252328395843506:  26%|███▎         | 62/241 [00:08<00:26,  6.69it/s]Epoch: 2, train for the 63-th batch, train loss: 0.7252328395843506:  26%|███▍         | 63/241 [00:08<00:26,  6.81it/s]Epoch: 1, train for the 259-th batch, train loss: 0.4262837767601013:  68%|███████▍   | 259/383 [01:10<00:34,  3.60it/s]Epoch: 3, train for the 113-th batch, train loss: 0.4673846364021301:  94%|██████████▎| 112/119 [00:17<00:01,  6.55it/s]Epoch: 3, train for the 49-th batch, train loss: 0.49714770913124084:  33%|███▉        | 48/146 [00:06<00:12,  7.99it/s]Epoch: 3, train for the 49-th batch, train loss: 0.49714770913124084:  34%|████        | 49/146 [00:06<00:13,  7.16it/s]Epoch: 2, train for the 137-th batch, train loss: 0.5548707842826843:  90%|█████████▉ | 136/151 [00:23<00:03,  4.80it/s]Epoch: 3, train for the 113-th batch, train loss: 0.4673846364021301:  95%|██████████▍| 113/119 [00:17<00:01,  5.97it/s]  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 2, train for the 137-th batch, train loss: 0.5548707842826843:  91%|█████████▉ | 137/151 [00:23<00:02,  4.87it/s]Epoch: 2, train for the 64-th batch, train loss: 0.3122641146183014:  26%|███▍         | 63/241 [00:08<00:26,  6.81it/s]Epoch: 2, train for the 64-th batch, train loss: 0.3122641146183014:  27%|███▍         | 64/241 [00:08<00:26,  6.81it/s]evaluate for the 1-th batch, evaluate loss: 0.6643599271774292:   0%|                            | 0/40 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6643599271774292:   2%|▌                   | 1/40 [00:00<00:04,  9.43it/s]Epoch: 3, train for the 50-th batch, train loss: 0.5164098739624023:  34%|████▎        | 49/146 [00:06<00:13,  7.16it/s]Epoch: 3, train for the 50-th batch, train loss: 0.5164098739624023:  34%|████▍        | 50/146 [00:06<00:13,  7.00it/s]Epoch: 3, train for the 114-th batch, train loss: 0.4640843868255615:  95%|██████████▍| 113/119 [00:17<00:01,  5.97it/s]Epoch: 3, train for the 114-th batch, train loss: 0.4640843868255615:  96%|██████████▌| 114/119 [00:17<00:00,  5.89it/s]Epoch: 2, train for the 138-th batch, train loss: 0.5293325185775757:  91%|█████████▉ | 137/151 [00:23<00:02,  4.87it/s]Epoch: 2, train for the 138-th batch, train loss: 0.5293325185775757:  91%|██████████ | 138/151 [00:23<00:02,  5.01it/s]Epoch: 2, train for the 65-th batch, train loss: 0.36476776003837585:  27%|███▏        | 64/241 [00:08<00:26,  6.81it/s]Epoch: 2, train for the 65-th batch, train loss: 0.36476776003837585:  27%|███▏        | 65/241 [00:08<00:26,  6.71it/s]Epoch: 3, train for the 51-th batch, train loss: 0.51421058177948:  34%|█████▏         | 50/146 [00:07<00:13,  7.00it/s]Epoch: 3, train for the 51-th batch, train loss: 0.51421058177948:  35%|█████▏         | 51/146 [00:07<00:13,  7.12it/s]Epoch: 1, train for the 260-th batch, train loss: 0.3396034836769104:  68%|███████▍   | 259/383 [01:10<00:34,  3.60it/s]evaluate for the 2-th batch, evaluate loss: 0.6477555632591248:   2%|▌                   | 1/40 [00:00<00:04,  9.43it/s]evaluate for the 2-th batch, evaluate loss: 0.6477555632591248:   5%|█                   | 2/40 [00:00<00:05,  6.89it/s]Epoch: 3, train for the 115-th batch, train loss: 0.46966707706451416:  96%|█████████▌| 114/119 [00:17<00:00,  5.89it/s]Epoch: 3, train for the 115-th batch, train loss: 0.46966707706451416:  97%|█████████▋| 115/119 [00:17<00:00,  6.13it/s]Epoch: 1, train for the 260-th batch, train loss: 0.3396034836769104:  68%|███████▍   | 260/383 [01:10<00:39,  3.13it/s]Epoch: 3, train for the 52-th batch, train loss: 0.48297521471977234:  35%|████▏       | 51/146 [00:07<00:13,  7.12it/s]evaluate for the 3-th batch, evaluate loss: 0.629162609577179:   5%|█                    | 2/40 [00:00<00:05,  6.89it/s]Epoch: 3, train for the 52-th batch, train loss: 0.48297521471977234:  36%|████▎       | 52/146 [00:07<00:12,  7.51it/s]Epoch: 2, train for the 139-th batch, train loss: 0.5000372529029846:  91%|██████████ | 138/151 [00:23<00:02,  5.01it/s]Epoch: 2, train for the 66-th batch, train loss: 0.307229220867157:  27%|███▊          | 65/241 [00:09<00:26,  6.71it/s]Epoch: 2, train for the 66-th batch, train loss: 0.307229220867157:  27%|███▊          | 66/241 [00:09<00:26,  6.65it/s]Epoch: 2, train for the 139-th batch, train loss: 0.5000372529029846:  92%|██████████▏| 139/151 [00:23<00:02,  4.92it/s]Epoch: 3, train for the 116-th batch, train loss: 0.4814591109752655:  97%|██████████▋| 115/119 [00:17<00:00,  6.13it/s]Epoch: 3, train for the 116-th batch, train loss: 0.4814591109752655:  97%|██████████▋| 116/119 [00:17<00:00,  6.57it/s]evaluate for the 4-th batch, evaluate loss: 0.6422981023788452:   5%|█                   | 2/40 [00:00<00:05,  6.89it/s]evaluate for the 4-th batch, evaluate loss: 0.6422981023788452:  10%|██                  | 4/40 [00:00<00:04,  8.17it/s]Epoch: 3, train for the 53-th batch, train loss: 0.488252729177475:  36%|████▉         | 52/146 [00:07<00:12,  7.51it/s]Epoch: 3, train for the 53-th batch, train loss: 0.488252729177475:  36%|█████         | 53/146 [00:07<00:12,  7.48it/s]Epoch: 2, train for the 67-th batch, train loss: 0.417894572019577:  27%|███▊          | 66/241 [00:09<00:26,  6.65it/s]Epoch: 2, train for the 67-th batch, train loss: 0.417894572019577:  28%|███▉          | 67/241 [00:09<00:25,  6.76it/s]Epoch: 3, train for the 117-th batch, train loss: 0.44421473145484924:  97%|█████████▋| 116/119 [00:17<00:00,  6.57it/s]Epoch: 3, train for the 117-th batch, train loss: 0.44421473145484924:  98%|█████████▊| 117/119 [00:17<00:00,  6.59it/s]Epoch: 2, train for the 140-th batch, train loss: 0.4961692690849304:  92%|██████████▏| 139/151 [00:24<00:02,  4.92it/s]Epoch: 3, train for the 54-th batch, train loss: 0.5404327511787415:  36%|████▋        | 53/146 [00:07<00:12,  7.48it/s]Epoch: 2, train for the 140-th batch, train loss: 0.4961692690849304:  93%|██████████▏| 140/151 [00:24<00:02,  4.79it/s]Epoch: 3, train for the 54-th batch, train loss: 0.5404327511787415:  37%|████▊        | 54/146 [00:07<00:11,  7.78it/s]Epoch: 2, train for the 68-th batch, train loss: 0.5061108469963074:  28%|███▌         | 67/241 [00:09<00:25,  6.76it/s]Epoch: 1, train for the 261-th batch, train loss: 0.33841079473495483:  68%|██████▊   | 260/383 [01:11<00:39,  3.13it/s]evaluate for the 5-th batch, evaluate loss: 0.643778383731842:  10%|██                   | 4/40 [00:00<00:04,  8.17it/s]evaluate for the 5-th batch, evaluate loss: 0.643778383731842:  12%|██▋                  | 5/40 [00:00<00:04,  7.27it/s]Epoch: 2, train for the 68-th batch, train loss: 0.5061108469963074:  28%|███▋         | 68/241 [00:09<00:24,  7.06it/s]Epoch: 3, train for the 118-th batch, train loss: 0.444762259721756:  98%|███████████▊| 117/119 [00:17<00:00,  6.59it/s]Epoch: 3, train for the 118-th batch, train loss: 0.444762259721756:  99%|███████████▉| 118/119 [00:18<00:00,  6.97it/s]Epoch: 1, train for the 261-th batch, train loss: 0.33841079473495483:  68%|██████▊   | 261/383 [01:11<00:40,  3.01it/s]Epoch: 3, train for the 55-th batch, train loss: 0.5628901720046997:  37%|████▊        | 54/146 [00:07<00:11,  7.78it/s]evaluate for the 6-th batch, evaluate loss: 0.6089058518409729:  12%|██▌                 | 5/40 [00:00<00:04,  7.27it/s]Epoch: 3, train for the 55-th batch, train loss: 0.5628901720046997:  38%|████▉        | 55/146 [00:07<00:11,  7.88it/s]Epoch: 3, train for the 119-th batch, train loss: 0.4816211462020874:  99%|██████████▉| 118/119 [00:18<00:00,  6.97it/s]Epoch: 2, train for the 141-th batch, train loss: 0.5210807919502258:  93%|██████████▏| 140/151 [00:24<00:02,  4.79it/s]Epoch: 3, train for the 119-th batch, train loss: 0.4816211462020874: 100%|███████████| 119/119 [00:18<00:00,  7.57it/s]Epoch: 3, train for the 119-th batch, train loss: 0.4816211462020874: 100%|███████████| 119/119 [00:18<00:00,  6.57it/s]
Epoch: 2, train for the 69-th batch, train loss: 0.6152483820915222:  28%|███▋         | 68/241 [00:09<00:24,  7.06it/s]Epoch: 2, train for the 141-th batch, train loss: 0.5210807919502258:  93%|██████████▎| 141/151 [00:24<00:02,  4.83it/s]Epoch: 2, train for the 69-th batch, train loss: 0.6152483820915222:  29%|███▋         | 69/241 [00:09<00:26,  6.56it/s]evaluate for the 7-th batch, evaluate loss: 0.6580036878585815:  12%|██▌                 | 5/40 [00:00<00:04,  7.27it/s]evaluate for the 7-th batch, evaluate loss: 0.6580036878585815:  18%|███▌                | 7/40 [00:00<00:03,  8.56it/s]Epoch: 3, train for the 56-th batch, train loss: 0.5156106352806091:  38%|████▉        | 55/146 [00:07<00:11,  7.88it/s]Epoch: 3, train for the 56-th batch, train loss: 0.5156106352806091:  38%|████▉        | 56/146 [00:07<00:11,  7.88it/s]Epoch: 1, train for the 262-th batch, train loss: 0.3621711730957031:  68%|███████▍   | 261/383 [01:11<00:40,  3.01it/s]evaluate for the 8-th batch, evaluate loss: 0.6778658628463745:  18%|███▌                | 7/40 [00:00<00:03,  8.56it/s]evaluate for the 8-th batch, evaluate loss: 0.6778658628463745:  20%|████                | 8/40 [00:00<00:03,  8.48it/s]Epoch: 1, train for the 262-th batch, train loss: 0.3621711730957031:  68%|███████▌   | 262/383 [01:11<00:37,  3.23it/s]Epoch: 3, train for the 57-th batch, train loss: 0.5330153703689575:  38%|████▉        | 56/146 [00:07<00:11,  7.88it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5105582475662231:  29%|███▋         | 69/241 [00:09<00:26,  6.56it/s]Epoch: 3, train for the 57-th batch, train loss: 0.5330153703689575:  39%|█████        | 57/146 [00:07<00:10,  8.15it/s]  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5105582475662231:  29%|███▊         | 70/241 [00:09<00:26,  6.44it/s]Epoch: 2, train for the 142-th batch, train loss: 0.49056312441825867:  93%|█████████▎| 141/151 [00:24<00:02,  4.83it/s]Epoch: 2, train for the 142-th batch, train loss: 0.49056312441825867:  94%|█████████▍| 142/151 [00:24<00:01,  4.81it/s]evaluate for the 1-th batch, evaluate loss: 0.4714388847351074:   0%|                            | 0/40 [00:00<?, ?it/s]evaluate for the 9-th batch, evaluate loss: 0.6888523697853088:  20%|████                | 8/40 [00:01<00:03,  8.48it/s]evaluate for the 9-th batch, evaluate loss: 0.6888523697853088:  22%|████▌               | 9/40 [00:01<00:04,  7.62it/s]Epoch: 3, train for the 58-th batch, train loss: 0.495415061712265:  39%|█████▍        | 57/146 [00:07<00:10,  8.15it/s]evaluate for the 2-th batch, evaluate loss: 0.48747336864471436:   0%|                           | 0/40 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.48747336864471436:   5%|▉                  | 2/40 [00:00<00:03, 12.49it/s]Epoch: 3, train for the 58-th batch, train loss: 0.495415061712265:  40%|█████▌        | 58/146 [00:07<00:12,  7.20it/s]Epoch: 2, train for the 71-th batch, train loss: 0.6041399240493774:  29%|███▊         | 70/241 [00:09<00:26,  6.44it/s]Epoch: 2, train for the 71-th batch, train loss: 0.6041399240493774:  29%|███▊         | 71/241 [00:09<00:29,  5.80it/s]evaluate for the 3-th batch, evaluate loss: 0.48390695452690125:   5%|▉                  | 2/40 [00:00<00:03, 12.49it/s]Epoch: 2, train for the 143-th batch, train loss: 0.47545862197875977:  94%|█████████▍| 142/151 [00:24<00:01,  4.81it/s]Epoch: 1, train for the 263-th batch, train loss: 0.4162989854812622:  68%|███████▌   | 262/383 [01:11<00:37,  3.23it/s]Epoch: 2, train for the 143-th batch, train loss: 0.47545862197875977:  95%|█████████▍| 143/151 [00:24<00:01,  4.78it/s]evaluate for the 10-th batch, evaluate loss: 0.6498392820358276:  22%|████▎              | 9/40 [00:01<00:04,  7.62it/s]evaluate for the 10-th batch, evaluate loss: 0.6498392820358276:  25%|████▌             | 10/40 [00:01<00:03,  7.84it/s]Epoch: 3, train for the 59-th batch, train loss: 0.4675368666648865:  40%|█████▏       | 58/146 [00:08<00:12,  7.20it/s]Epoch: 3, train for the 59-th batch, train loss: 0.4675368666648865:  40%|█████▎       | 59/146 [00:08<00:11,  7.57it/s]evaluate for the 4-th batch, evaluate loss: 0.5477294325828552:   5%|█                   | 2/40 [00:00<00:03, 12.49it/s]evaluate for the 4-th batch, evaluate loss: 0.5477294325828552:  10%|██                  | 4/40 [00:00<00:02, 14.18it/s]Epoch: 1, train for the 263-th batch, train loss: 0.4162989854812622:  69%|███████▌   | 263/383 [01:11<00:39,  3.06it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5591802000999451:  29%|███▊         | 71/241 [00:10<00:29,  5.80it/s]evaluate for the 11-th batch, evaluate loss: 0.6647495627403259:  25%|████▌             | 10/40 [00:01<00:03,  7.84it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5591802000999451:  30%|███▉         | 72/241 [00:10<00:28,  5.97it/s]evaluate for the 5-th batch, evaluate loss: 0.5214782357215881:  10%|██                  | 4/40 [00:00<00:02, 14.18it/s]Epoch: 2, train for the 144-th batch, train loss: 0.4634160101413727:  95%|██████████▍| 143/151 [00:24<00:01,  4.78it/s]Epoch: 3, train for the 60-th batch, train loss: 0.5224500894546509:  40%|█████▎       | 59/146 [00:08<00:11,  7.57it/s]Epoch: 3, train for the 60-th batch, train loss: 0.5224500894546509:  41%|█████▎       | 60/146 [00:08<00:11,  7.45it/s]Epoch: 2, train for the 144-th batch, train loss: 0.4634160101413727:  95%|██████████▍| 144/151 [00:24<00:01,  5.02it/s]evaluate for the 6-th batch, evaluate loss: 0.4895223081111908:  10%|██                  | 4/40 [00:00<00:02, 14.18it/s]evaluate for the 6-th batch, evaluate loss: 0.4895223081111908:  15%|███                 | 6/40 [00:00<00:02, 12.06it/s]Epoch: 2, train for the 73-th batch, train loss: 0.49075889587402344:  30%|███▌        | 72/241 [00:10<00:28,  5.97it/s]evaluate for the 12-th batch, evaluate loss: 0.7030419111251831:  25%|████▌             | 10/40 [00:01<00:03,  7.84it/s]evaluate for the 12-th batch, evaluate loss: 0.7030419111251831:  30%|█████▍            | 12/40 [00:01<00:03,  8.00it/s]Epoch: 2, train for the 73-th batch, train loss: 0.49075889587402344:  30%|███▋        | 73/241 [00:10<00:26,  6.23it/s]evaluate for the 7-th batch, evaluate loss: 0.5086190104484558:  15%|███                 | 6/40 [00:00<00:02, 12.06it/s]Epoch: 1, train for the 264-th batch, train loss: 0.32858288288116455:  69%|██████▊   | 263/383 [01:12<00:39,  3.06it/s]Epoch: 3, train for the 61-th batch, train loss: 0.532418966293335:  41%|█████▊        | 60/146 [00:08<00:11,  7.45it/s]Epoch: 3, train for the 61-th batch, train loss: 0.532418966293335:  42%|█████▊        | 61/146 [00:08<00:12,  6.71it/s]evaluate for the 13-th batch, evaluate loss: 0.6504276990890503:  30%|█████▍            | 12/40 [00:01<00:03,  8.00it/s]evaluate for the 13-th batch, evaluate loss: 0.6504276990890503:  32%|█████▊            | 13/40 [00:01<00:03,  8.24it/s]evaluate for the 8-th batch, evaluate loss: 0.4611503481864929:  15%|███                 | 6/40 [00:00<00:02, 12.06it/s]evaluate for the 8-th batch, evaluate loss: 0.4611503481864929:  20%|████                | 8/40 [00:00<00:02, 13.17it/s]Epoch: 2, train for the 145-th batch, train loss: 0.4764906167984009:  95%|██████████▍| 144/151 [00:25<00:01,  5.02it/s]Epoch: 2, train for the 145-th batch, train loss: 0.4764906167984009:  96%|██████████▌| 145/151 [00:25<00:01,  4.91it/s]Epoch: 2, train for the 74-th batch, train loss: 0.4337140917778015:  30%|███▉         | 73/241 [00:10<00:26,  6.23it/s]Epoch: 1, train for the 264-th batch, train loss: 0.32858288288116455:  69%|██████▉   | 264/383 [01:12<00:38,  3.10it/s]evaluate for the 9-th batch, evaluate loss: 0.5112544894218445:  20%|████                | 8/40 [00:00<00:02, 13.17it/s]Epoch: 2, train for the 74-th batch, train loss: 0.4337140917778015:  31%|███▉         | 74/241 [00:10<00:26,  6.31it/s]evaluate for the 10-th batch, evaluate loss: 0.5438299179077148:  20%|███▊               | 8/40 [00:00<00:02, 13.17it/s]evaluate for the 10-th batch, evaluate loss: 0.5438299179077148:  25%|████▌             | 10/40 [00:00<00:02, 14.29it/s]Epoch: 3, train for the 62-th batch, train loss: 0.4954913854598999:  42%|█████▍       | 61/146 [00:08<00:12,  6.71it/s]evaluate for the 14-th batch, evaluate loss: 0.7121089100837708:  32%|█████▊            | 13/40 [00:01<00:03,  8.24it/s]evaluate for the 14-th batch, evaluate loss: 0.7121089100837708:  35%|██████▎           | 14/40 [00:01<00:03,  7.83it/s]Epoch: 3, train for the 62-th batch, train loss: 0.4954913854598999:  42%|█████▌       | 62/146 [00:08<00:12,  6.64it/s]evaluate for the 11-th batch, evaluate loss: 0.48872748017311096:  25%|████▎            | 10/40 [00:00<00:02, 14.29it/s]Epoch: 2, train for the 146-th batch, train loss: 0.4516265392303467:  96%|██████████▌| 145/151 [00:25<00:01,  4.91it/s]Epoch: 2, train for the 75-th batch, train loss: 0.3863186538219452:  31%|███▉         | 74/241 [00:10<00:26,  6.31it/s]evaluate for the 12-th batch, evaluate loss: 0.5057942867279053:  25%|████▌             | 10/40 [00:00<00:02, 14.29it/s]evaluate for the 12-th batch, evaluate loss: 0.5057942867279053:  30%|█████▍            | 12/40 [00:00<00:01, 15.69it/s]Epoch: 2, train for the 75-th batch, train loss: 0.3863186538219452:  31%|████         | 75/241 [00:10<00:26,  6.16it/s]Epoch: 2, train for the 146-th batch, train loss: 0.4516265392303467:  97%|██████████▋| 146/151 [00:25<00:01,  4.80it/s]evaluate for the 15-th batch, evaluate loss: 0.6760442852973938:  35%|██████▎           | 14/40 [00:01<00:03,  7.83it/s]evaluate for the 15-th batch, evaluate loss: 0.6760442852973938:  38%|██████▊           | 15/40 [00:01<00:03,  7.58it/s]Epoch: 3, train for the 63-th batch, train loss: 0.4926058351993561:  42%|█████▌       | 62/146 [00:08<00:12,  6.64it/s]evaluate for the 13-th batch, evaluate loss: 0.47893819212913513:  30%|█████            | 12/40 [00:00<00:01, 15.69it/s]Epoch: 3, train for the 63-th batch, train loss: 0.4926058351993561:  43%|█████▌       | 63/146 [00:08<00:12,  6.43it/s]Epoch: 1, train for the 265-th batch, train loss: 0.35486018657684326:  69%|██████▉   | 264/383 [01:12<00:38,  3.10it/s]Epoch: 2, train for the 76-th batch, train loss: 0.30086272954940796:  31%|███▋        | 75/241 [00:10<00:26,  6.16it/s]Epoch: 2, train for the 76-th batch, train loss: 0.30086272954940796:  32%|███▊        | 76/241 [00:10<00:24,  6.61it/s]evaluate for the 14-th batch, evaluate loss: 0.49391859769821167:  30%|█████            | 12/40 [00:00<00:01, 15.69it/s]evaluate for the 14-th batch, evaluate loss: 0.49391859769821167:  35%|█████▉           | 14/40 [00:00<00:01, 15.49it/s]Epoch: 2, train for the 147-th batch, train loss: 0.5023481845855713:  97%|██████████▋| 146/151 [00:25<00:01,  4.80it/s]evaluate for the 16-th batch, evaluate loss: 0.641417920589447:  38%|███████▏           | 15/40 [00:02<00:03,  7.58it/s]evaluate for the 16-th batch, evaluate loss: 0.641417920589447:  40%|███████▌           | 16/40 [00:02<00:03,  7.93it/s]Epoch: 1, train for the 265-th batch, train loss: 0.35486018657684326:  69%|██████▉   | 265/383 [01:12<00:39,  3.01it/s]Epoch: 2, train for the 147-th batch, train loss: 0.5023481845855713:  97%|██████████▋| 147/151 [00:25<00:00,  5.13it/s]Epoch: 3, train for the 64-th batch, train loss: 0.5387434959411621:  43%|█████▌       | 63/146 [00:08<00:12,  6.43it/s]evaluate for the 15-th batch, evaluate loss: 0.5124675631523132:  35%|██████▎           | 14/40 [00:01<00:01, 15.49it/s]Epoch: 3, train for the 64-th batch, train loss: 0.5387434959411621:  44%|█████▋       | 64/146 [00:08<00:12,  6.63it/s]Epoch: 2, train for the 77-th batch, train loss: 0.3613721430301666:  32%|████         | 76/241 [00:10<00:24,  6.61it/s]Epoch: 2, train for the 77-th batch, train loss: 0.3613721430301666:  32%|████▏        | 77/241 [00:10<00:24,  6.70it/s]evaluate for the 16-th batch, evaluate loss: 0.48798996210098267:  35%|█████▉           | 14/40 [00:01<00:01, 15.49it/s]evaluate for the 16-th batch, evaluate loss: 0.48798996210098267:  40%|██████▊          | 16/40 [00:01<00:01, 14.17it/s]evaluate for the 17-th batch, evaluate loss: 0.6543675065040588:  40%|███████▏          | 16/40 [00:02<00:03,  7.93it/s]evaluate for the 17-th batch, evaluate loss: 0.6543675065040588:  42%|███████▋          | 17/40 [00:02<00:02,  7.67it/s]Epoch: 2, train for the 148-th batch, train loss: 0.47021135687828064:  97%|█████████▋| 147/151 [00:25<00:00,  5.13it/s]Epoch: 3, train for the 65-th batch, train loss: 0.5482070446014404:  44%|█████▋       | 64/146 [00:09<00:12,  6.63it/s]evaluate for the 17-th batch, evaluate loss: 0.5005019903182983:  40%|███████▏          | 16/40 [00:01<00:01, 14.17it/s]Epoch: 3, train for the 65-th batch, train loss: 0.5482070446014404:  45%|█████▊       | 65/146 [00:09<00:12,  6.73it/s]Epoch: 2, train for the 148-th batch, train loss: 0.47021135687828064:  98%|█████████▊| 148/151 [00:25<00:00,  5.19it/s]Epoch: 2, train for the 78-th batch, train loss: 0.5436450242996216:  32%|████▏        | 77/241 [00:10<00:24,  6.70it/s]Epoch: 2, train for the 78-th batch, train loss: 0.5436450242996216:  32%|████▏        | 78/241 [00:11<00:24,  6.68it/s]Epoch: 1, train for the 266-th batch, train loss: 0.3981578052043915:  69%|███████▌   | 265/383 [01:12<00:39,  3.01it/s]evaluate for the 18-th batch, evaluate loss: 0.49598148465156555:  40%|██████▊          | 16/40 [00:01<00:01, 14.17it/s]evaluate for the 18-th batch, evaluate loss: 0.49598148465156555:  45%|███████▋         | 18/40 [00:01<00:01, 14.01it/s]evaluate for the 18-th batch, evaluate loss: 0.6390926837921143:  42%|███████▋          | 17/40 [00:02<00:02,  7.67it/s]evaluate for the 18-th batch, evaluate loss: 0.6390926837921143:  45%|████████          | 18/40 [00:02<00:02,  7.61it/s]Epoch: 1, train for the 266-th batch, train loss: 0.3981578052043915:  69%|███████▋   | 266/383 [01:12<00:38,  3.05it/s]Epoch: 3, train for the 66-th batch, train loss: 0.5400233864784241:  45%|█████▊       | 65/146 [00:09<00:12,  6.73it/s]evaluate for the 19-th batch, evaluate loss: 0.49603572487831116:  45%|███████▋         | 18/40 [00:01<00:01, 14.01it/s]Epoch: 3, train for the 66-th batch, train loss: 0.5400233864784241:  45%|█████▉       | 66/146 [00:09<00:11,  6.88it/s]Epoch: 2, train for the 149-th batch, train loss: 0.4668307900428772:  98%|██████████▊| 148/151 [00:25<00:00,  5.19it/s]evaluate for the 19-th batch, evaluate loss: 0.6970041990280151:  45%|████████          | 18/40 [00:02<00:02,  7.61it/s]evaluate for the 19-th batch, evaluate loss: 0.6970041990280151:  48%|████████▌         | 19/40 [00:02<00:02,  7.78it/s]evaluate for the 20-th batch, evaluate loss: 0.4862697422504425:  45%|████████          | 18/40 [00:01<00:01, 14.01it/s]evaluate for the 20-th batch, evaluate loss: 0.4862697422504425:  50%|█████████         | 20/40 [00:01<00:01, 14.48it/s]Epoch: 2, train for the 79-th batch, train loss: 0.4575292468070984:  32%|████▏        | 78/241 [00:11<00:24,  6.68it/s]Epoch: 2, train for the 149-th batch, train loss: 0.4668307900428772:  99%|██████████▊| 149/151 [00:25<00:00,  5.07it/s]Epoch: 2, train for the 79-th batch, train loss: 0.4575292468070984:  33%|████▎        | 79/241 [00:11<00:24,  6.52it/s]Epoch: 3, train for the 67-th batch, train loss: 0.5542246699333191:  45%|█████▉       | 66/146 [00:09<00:11,  6.88it/s]evaluate for the 21-th batch, evaluate loss: 0.49994781613349915:  50%|████████▌        | 20/40 [00:01<00:01, 14.48it/s]Epoch: 3, train for the 67-th batch, train loss: 0.5542246699333191:  46%|█████▉       | 67/146 [00:09<00:11,  7.02it/s]Epoch: 2, train for the 80-th batch, train loss: 0.3383462727069855:  33%|████▎        | 79/241 [00:11<00:24,  6.52it/s]evaluate for the 22-th batch, evaluate loss: 0.48475417494773865:  50%|████████▌        | 20/40 [00:01<00:01, 14.48it/s]evaluate for the 22-th batch, evaluate loss: 0.48475417494773865:  55%|█████████▎       | 22/40 [00:01<00:01, 14.09it/s]evaluate for the 20-th batch, evaluate loss: 0.669583797454834:  48%|█████████          | 19/40 [00:02<00:02,  7.78it/s]evaluate for the 20-th batch, evaluate loss: 0.669583797454834:  50%|█████████▌         | 20/40 [00:02<00:02,  7.23it/s]Epoch: 2, train for the 80-th batch, train loss: 0.3383462727069855:  33%|████▎        | 80/241 [00:11<00:24,  6.60it/s]Epoch: 2, train for the 150-th batch, train loss: 0.4865837097167969:  99%|██████████▊| 149/151 [00:26<00:00,  5.07it/s]Epoch: 1, train for the 267-th batch, train loss: 0.33084937930107117:  69%|██████▉   | 266/383 [01:13<00:38,  3.05it/s]Epoch: 2, train for the 150-th batch, train loss: 0.4865837097167969:  99%|██████████▉| 150/151 [00:26<00:00,  4.98it/s]Epoch: 3, train for the 68-th batch, train loss: 0.4968988001346588:  46%|█████▉       | 67/146 [00:09<00:11,  7.02it/s]evaluate for the 23-th batch, evaluate loss: 0.4279032051563263:  55%|█████████▉        | 22/40 [00:01<00:01, 14.09it/s]Epoch: 3, train for the 68-th batch, train loss: 0.4968988001346588:  47%|██████       | 68/146 [00:09<00:11,  6.64it/s]evaluate for the 21-th batch, evaluate loss: 0.6879003047943115:  50%|█████████         | 20/40 [00:02<00:02,  7.23it/s]Epoch: 1, train for the 267-th batch, train loss: 0.33084937930107117:  70%|██████▉   | 267/383 [01:13<00:38,  3.01it/s]Epoch: 2, train for the 81-th batch, train loss: 0.3783896565437317:  33%|████▎        | 80/241 [00:11<00:24,  6.60it/s]evaluate for the 24-th batch, evaluate loss: 0.4943893849849701:  55%|█████████▉        | 22/40 [00:01<00:01, 14.09it/s]evaluate for the 24-th batch, evaluate loss: 0.4943893849849701:  60%|██████████▊       | 24/40 [00:01<00:01, 14.20it/s]Epoch: 2, train for the 81-th batch, train loss: 0.3783896565437317:  34%|████▎        | 81/241 [00:11<00:24,  6.63it/s]evaluate for the 25-th batch, evaluate loss: 0.5259433388710022:  60%|██████████▊       | 24/40 [00:01<00:01, 14.20it/s]evaluate for the 22-th batch, evaluate loss: 0.7089506983757019:  50%|█████████         | 20/40 [00:02<00:02,  7.23it/s]evaluate for the 22-th batch, evaluate loss: 0.7089506983757019:  55%|█████████▉        | 22/40 [00:02<00:02,  7.68it/s]evaluate for the 26-th batch, evaluate loss: 0.46255895495414734:  60%|██████████▏      | 24/40 [00:01<00:01, 14.20it/s]evaluate for the 26-th batch, evaluate loss: 0.46255895495414734:  65%|███████████      | 26/40 [00:01<00:00, 14.90it/s]Epoch: 2, train for the 82-th batch, train loss: 0.4313181936740875:  34%|████▎        | 81/241 [00:11<00:24,  6.63it/s]Epoch: 2, train for the 82-th batch, train loss: 0.4313181936740875:  34%|████▍        | 82/241 [00:11<00:23,  6.73it/s]evaluate for the 27-th batch, evaluate loss: 0.473743200302124:  65%|████████████▎      | 26/40 [00:01<00:00, 14.90it/s]Epoch: 3, train for the 69-th batch, train loss: 0.5156975388526917:  47%|██████       | 68/146 [00:09<00:11,  6.64it/s]Epoch: 2, train for the 151-th batch, train loss: 0.5455724596977234:  99%|██████████▉| 150/151 [00:26<00:00,  4.98it/s]evaluate for the 28-th batch, evaluate loss: 0.45162132382392883:  65%|███████████      | 26/40 [00:01<00:00, 14.90it/s]evaluate for the 28-th batch, evaluate loss: 0.45162132382392883:  70%|███████████▉     | 28/40 [00:01<00:00, 15.27it/s]Epoch: 3, train for the 69-th batch, train loss: 0.5156975388526917:  47%|██████▏      | 69/146 [00:09<00:14,  5.20it/s]Epoch: 1, train for the 268-th batch, train loss: 0.35304197669029236:  70%|██████▉   | 267/383 [01:13<00:38,  3.01it/s]Epoch: 2, train for the 151-th batch, train loss: 0.5455724596977234: 100%|███████████| 151/151 [00:26<00:00,  4.12it/s]Epoch: 2, train for the 151-th batch, train loss: 0.5455724596977234: 100%|███████████| 151/151 [00:26<00:00,  5.71it/s]
evaluate for the 23-th batch, evaluate loss: 0.6682906746864319:  55%|█████████▉        | 22/40 [00:02<00:02,  7.68it/s]evaluate for the 23-th batch, evaluate loss: 0.6682906746864319:  57%|██████████▎       | 23/40 [00:02<00:02,  7.08it/s]evaluate for the 29-th batch, evaluate loss: 0.49005958437919617:  70%|███████████▉     | 28/40 [00:02<00:00, 15.27it/s]Epoch: 1, train for the 268-th batch, train loss: 0.35304197669029236:  70%|██████▉   | 268/383 [01:13<00:38,  3.00it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5144837498664856:  34%|████▍        | 82/241 [00:11<00:23,  6.73it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5144837498664856:  34%|████▍        | 83/241 [00:11<00:25,  6.26it/s]Epoch: 3, train for the 70-th batch, train loss: 0.5347307324409485:  47%|██████▏      | 69/146 [00:09<00:14,  5.20it/s]evaluate for the 30-th batch, evaluate loss: 0.47049033641815186:  70%|███████████▉     | 28/40 [00:02<00:00, 15.27it/s]evaluate for the 30-th batch, evaluate loss: 0.47049033641815186:  75%|████████████▊    | 30/40 [00:02<00:00, 15.50it/s]Epoch: 3, train for the 70-th batch, train loss: 0.5347307324409485:  48%|██████▏      | 70/146 [00:09<00:13,  5.74it/s]evaluate for the 24-th batch, evaluate loss: 0.6954322457313538:  57%|██████████▎       | 23/40 [00:03<00:02,  7.08it/s]evaluate for the 24-th batch, evaluate loss: 0.6954322457313538:  60%|██████████▊       | 24/40 [00:03<00:02,  7.16it/s]evaluate for the 31-th batch, evaluate loss: 0.4621371328830719:  75%|█████████████▌    | 30/40 [00:02<00:00, 15.50it/s]  0%|                                                                                            | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.5389662981033325:   0%|                            | 0/46 [00:00<?, ?it/s]Epoch: 3, train for the 71-th batch, train loss: 0.5455289483070374:  48%|██████▏      | 70/146 [00:10<00:13,  5.74it/s]evaluate for the 32-th batch, evaluate loss: 0.48277992010116577:  75%|████████████▊    | 30/40 [00:02<00:00, 15.50it/s]evaluate for the 32-th batch, evaluate loss: 0.48277992010116577:  80%|█████████████▌   | 32/40 [00:02<00:00, 14.09it/s]Epoch: 3, train for the 71-th batch, train loss: 0.5455289483070374:  49%|██████▎      | 71/146 [00:10<00:12,  5.79it/s]Epoch: 1, train for the 269-th batch, train loss: 0.4242227375507355:  70%|███████▋   | 268/383 [01:13<00:38,  3.00it/s]evaluate for the 25-th batch, evaluate loss: 0.6894747614860535:  60%|██████████▊       | 24/40 [00:03<00:02,  7.16it/s]evaluate for the 25-th batch, evaluate loss: 0.6894747614860535:  62%|███████████▎      | 25/40 [00:03<00:02,  7.14it/s]evaluate for the 2-th batch, evaluate loss: 0.5619748830795288:   0%|                            | 0/46 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5619748830795288:   4%|▊                   | 2/46 [00:00<00:03, 14.38it/s]evaluate for the 33-th batch, evaluate loss: 0.46770262718200684:  80%|█████████████▌   | 32/40 [00:02<00:00, 14.09it/s]Epoch: 1, train for the 269-th batch, train loss: 0.4242227375507355:  70%|███████▋   | 269/383 [01:13<00:37,  3.04it/s]Epoch: 3, train for the 72-th batch, train loss: 0.540087103843689:  49%|██████▊       | 71/146 [00:10<00:12,  5.79it/s]evaluate for the 26-th batch, evaluate loss: 0.7010865807533264:  62%|███████████▎      | 25/40 [00:03<00:02,  7.14it/s]evaluate for the 26-th batch, evaluate loss: 0.7010865807533264:  65%|███████████▋      | 26/40 [00:03<00:01,  7.71it/s]Epoch: 3, train for the 72-th batch, train loss: 0.540087103843689:  49%|██████▉       | 72/146 [00:10<00:11,  6.36it/s]evaluate for the 34-th batch, evaluate loss: 0.5049221515655518:  80%|██████████████▍   | 32/40 [00:02<00:00, 14.09it/s]evaluate for the 34-th batch, evaluate loss: 0.5049221515655518:  85%|███████████████▎  | 34/40 [00:02<00:00, 13.98it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5947088003158569:  34%|████▍        | 83/241 [00:12<00:25,  6.26it/s]evaluate for the 3-th batch, evaluate loss: 0.549861490726471:   4%|▉                    | 2/46 [00:00<00:03, 14.38it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5947088003158569:  35%|████▌        | 84/241 [00:12<00:34,  4.54it/s]evaluate for the 35-th batch, evaluate loss: 0.5064852833747864:  85%|███████████████▎  | 34/40 [00:02<00:00, 13.98it/s]evaluate for the 27-th batch, evaluate loss: 0.6575634479522705:  65%|███████████▋      | 26/40 [00:03<00:01,  7.71it/s]evaluate for the 27-th batch, evaluate loss: 0.6575634479522705:  68%|████████████▏     | 27/40 [00:03<00:01,  7.73it/s]Epoch: 3, train for the 73-th batch, train loss: 0.538429319858551:  49%|██████▉       | 72/146 [00:10<00:11,  6.36it/s]evaluate for the 4-th batch, evaluate loss: 0.5315114259719849:   4%|▊                   | 2/46 [00:00<00:03, 14.38it/s]evaluate for the 4-th batch, evaluate loss: 0.5315114259719849:   9%|█▋                  | 4/46 [00:00<00:03, 10.70it/s]Epoch: 3, train for the 73-th batch, train loss: 0.538429319858551:  50%|███████       | 73/146 [00:10<00:11,  6.57it/s]Epoch: 1, train for the 270-th batch, train loss: 0.4111390709877014:  70%|███████▋   | 269/383 [01:14<00:37,  3.04it/s]evaluate for the 36-th batch, evaluate loss: 0.4922044277191162:  85%|███████████████▎  | 34/40 [00:02<00:00, 13.98it/s]evaluate for the 36-th batch, evaluate loss: 0.4922044277191162:  90%|████████████████▏ | 36/40 [00:02<00:00, 12.99it/s]Epoch: 2, train for the 85-th batch, train loss: 0.4261438548564911:  35%|████▌        | 84/241 [00:12<00:34,  4.54it/s]evaluate for the 5-th batch, evaluate loss: 0.5256012678146362:   9%|█▋                  | 4/46 [00:00<00:03, 10.70it/s]Epoch: 2, train for the 85-th batch, train loss: 0.4261438548564911:  35%|████▌        | 85/241 [00:12<00:32,  4.80it/s]evaluate for the 28-th batch, evaluate loss: 0.7186083793640137:  68%|████████████▏     | 27/40 [00:03<00:01,  7.73it/s]evaluate for the 28-th batch, evaluate loss: 0.7186083793640137:  70%|████████████▌     | 28/40 [00:03<00:01,  8.10it/s]Epoch: 1, train for the 270-th batch, train loss: 0.4111390709877014:  70%|███████▊   | 270/383 [01:14<00:35,  3.19it/s]Epoch: 3, train for the 74-th batch, train loss: 0.5203306078910828:  50%|██████▌      | 73/146 [00:10<00:11,  6.57it/s]evaluate for the 37-th batch, evaluate loss: 0.49600034952163696:  90%|███████████████▎ | 36/40 [00:02<00:00, 12.99it/s]Epoch: 3, train for the 74-th batch, train loss: 0.5203306078910828:  51%|██████▌      | 74/146 [00:10<00:10,  6.56it/s]evaluate for the 6-th batch, evaluate loss: 0.5312737822532654:   9%|█▋                  | 4/46 [00:00<00:03, 10.70it/s]evaluate for the 6-th batch, evaluate loss: 0.5312737822532654:  13%|██▌                 | 6/46 [00:00<00:03, 10.97it/s]evaluate for the 38-th batch, evaluate loss: 0.5378350615501404:  90%|████████████████▏ | 36/40 [00:02<00:00, 12.99it/s]evaluate for the 38-th batch, evaluate loss: 0.5378350615501404:  95%|█████████████████ | 38/40 [00:02<00:00, 13.37it/s]Epoch: 2, train for the 86-th batch, train loss: 0.499723345041275:  35%|████▉         | 85/241 [00:12<00:32,  4.80it/s]Epoch: 2, train for the 86-th batch, train loss: 0.499723345041275:  36%|████▉         | 86/241 [00:12<00:29,  5.21it/s]evaluate for the 29-th batch, evaluate loss: 0.7311914563179016:  70%|████████████▌     | 28/40 [00:03<00:01,  8.10it/s]evaluate for the 29-th batch, evaluate loss: 0.7311914563179016:  72%|█████████████     | 29/40 [00:03<00:01,  7.51it/s]evaluate for the 7-th batch, evaluate loss: 0.5388068556785583:  13%|██▌                 | 6/46 [00:00<00:03, 10.97it/s]Epoch: 3, train for the 75-th batch, train loss: 0.5521169900894165:  51%|██████▌      | 74/146 [00:10<00:10,  6.56it/s]evaluate for the 39-th batch, evaluate loss: 0.5279907584190369:  95%|█████████████████ | 38/40 [00:02<00:00, 13.37it/s]Epoch: 3, train for the 75-th batch, train loss: 0.5521169900894165:  51%|██████▋      | 75/146 [00:10<00:10,  6.52it/s]evaluate for the 40-th batch, evaluate loss: 0.41441085934638977:  95%|████████████████▏| 38/40 [00:02<00:00, 13.37it/s]evaluate for the 40-th batch, evaluate loss: 0.41441085934638977: 100%|█████████████████| 40/40 [00:02<00:00, 13.27it/s]evaluate for the 40-th batch, evaluate loss: 0.41441085934638977: 100%|█████████████████| 40/40 [00:02<00:00, 14.01it/s]
Epoch: 1, train for the 271-th batch, train loss: 0.36010831594467163:  70%|███████   | 270/383 [01:14<00:35,  3.19it/s]Epoch: 2, train for the 87-th batch, train loss: 0.5651711225509644:  36%|████▋        | 86/241 [00:12<00:29,  5.21it/s]evaluate for the 8-th batch, evaluate loss: 0.5572790503501892:  13%|██▌                 | 6/46 [00:00<00:03, 10.97it/s]evaluate for the 8-th batch, evaluate loss: 0.5572790503501892:  17%|███▍                | 8/46 [00:00<00:03, 10.45it/s]evaluate for the 30-th batch, evaluate loss: 0.7357420325279236:  72%|█████████████     | 29/40 [00:03<00:01,  7.51it/s]evaluate for the 30-th batch, evaluate loss: 0.7357420325279236:  75%|█████████████▌    | 30/40 [00:03<00:01,  7.58it/s]Epoch: 2, train for the 87-th batch, train loss: 0.5651711225509644:  36%|████▋        | 87/241 [00:12<00:27,  5.59it/s]Epoch: 1, train for the 271-th batch, train loss: 0.36010831594467163:  71%|███████   | 271/383 [01:14<00:35,  3.12it/s]evaluate for the 9-th batch, evaluate loss: 0.539147675037384:  17%|███▋                 | 8/46 [00:00<00:03, 10.45it/s]evaluate for the 31-th batch, evaluate loss: 0.6707990765571594:  75%|█████████████▌    | 30/40 [00:03<00:01,  7.58it/s]Epoch: 3, train for the 76-th batch, train loss: 0.5219060182571411:  51%|██████▋      | 75/146 [00:10<00:10,  6.52it/s]  0%|                                                                                            | 0/21 [00:00<?, ?it/s]Epoch: 3, train for the 76-th batch, train loss: 0.5219060182571411:  52%|██████▊      | 76/146 [00:10<00:11,  6.18it/s]evaluate for the 1-th batch, evaluate loss: 0.5980305075645447:   0%|                            | 0/21 [00:00<?, ?it/s]Epoch: 2, train for the 88-th batch, train loss: 0.48815304040908813:  36%|████▎       | 87/241 [00:12<00:27,  5.59it/s]evaluate for the 10-th batch, evaluate loss: 0.5165344476699829:  17%|███▎               | 8/46 [00:00<00:03, 10.45it/s]evaluate for the 10-th batch, evaluate loss: 0.5165344476699829:  22%|███▉              | 10/46 [00:00<00:03, 10.95it/s]Epoch: 2, train for the 88-th batch, train loss: 0.48815304040908813:  37%|████▍       | 88/241 [00:12<00:27,  5.49it/s]evaluate for the 32-th batch, evaluate loss: 0.7015894055366516:  75%|█████████████▌    | 30/40 [00:04<00:01,  7.58it/s]evaluate for the 32-th batch, evaluate loss: 0.7015894055366516:  80%|██████████████▍   | 32/40 [00:04<00:00,  8.12it/s]evaluate for the 11-th batch, evaluate loss: 0.5649591088294983:  22%|███▉              | 10/46 [00:00<00:03, 10.95it/s]Epoch: 3, train for the 77-th batch, train loss: 0.5847030878067017:  52%|██████▊      | 76/146 [00:10<00:11,  6.18it/s]evaluate for the 2-th batch, evaluate loss: 0.677558958530426:   0%|                             | 0/21 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.677558958530426:  10%|██                   | 2/21 [00:00<00:01, 12.66it/s]Epoch: 3, train for the 77-th batch, train loss: 0.5847030878067017:  53%|██████▊      | 77/146 [00:10<00:10,  6.30it/s]Epoch: 1, train for the 272-th batch, train loss: 0.32466864585876465:  71%|███████   | 271/383 [01:14<00:35,  3.12it/s]evaluate for the 3-th batch, evaluate loss: 0.6679789423942566:  10%|█▉                  | 2/21 [00:00<00:01, 12.66it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5403512120246887:  37%|████▋        | 88/241 [00:12<00:27,  5.49it/s]evaluate for the 12-th batch, evaluate loss: 0.50725919008255:  22%|████▎               | 10/46 [00:01<00:03, 10.95it/s]evaluate for the 12-th batch, evaluate loss: 0.50725919008255:  26%|█████▏              | 12/46 [00:01<00:03, 10.96it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5403512120246887:  37%|████▊        | 89/241 [00:12<00:26,  5.73it/s]evaluate for the 33-th batch, evaluate loss: 0.7097773551940918:  80%|██████████████▍   | 32/40 [00:04<00:00,  8.12it/s]evaluate for the 33-th batch, evaluate loss: 0.7097773551940918:  82%|██████████████▊   | 33/40 [00:04<00:00,  7.97it/s]Epoch: 1, train for the 272-th batch, train loss: 0.32466864585876465:  71%|███████   | 272/383 [01:14<00:35,  3.13it/s]Epoch: 3, train for the 78-th batch, train loss: 0.5279092788696289:  53%|██████▊      | 77/146 [00:11<00:10,  6.30it/s]Epoch: 3, train for the 78-th batch, train loss: 0.5279092788696289:  53%|██████▉      | 78/146 [00:11<00:10,  6.74it/s]evaluate for the 4-th batch, evaluate loss: 0.5988377332687378:  10%|█▉                  | 2/21 [00:00<00:01, 12.66it/s]evaluate for the 4-th batch, evaluate loss: 0.5988377332687378:  19%|███▊                | 4/21 [00:00<00:01, 13.08it/s]evaluate for the 13-th batch, evaluate loss: 0.5182321071624756:  26%|████▋             | 12/46 [00:01<00:03, 10.96it/s]evaluate for the 34-th batch, evaluate loss: 0.7032666802406311:  82%|██████████████▊   | 33/40 [00:04<00:00,  7.97it/s]evaluate for the 34-th batch, evaluate loss: 0.7032666802406311:  85%|███████████████▎  | 34/40 [00:04<00:00,  7.90it/s]evaluate for the 5-th batch, evaluate loss: 0.6904036402702332:  19%|███▊                | 4/21 [00:00<00:01, 13.08it/s]Epoch: 2, train for the 90-th batch, train loss: 0.4959860146045685:  37%|████▊        | 89/241 [00:13<00:26,  5.73it/s]evaluate for the 14-th batch, evaluate loss: 0.5617892146110535:  26%|████▋             | 12/46 [00:01<00:03, 10.96it/s]evaluate for the 14-th batch, evaluate loss: 0.5617892146110535:  30%|█████▍            | 14/46 [00:01<00:02, 11.45it/s]Epoch: 2, train for the 90-th batch, train loss: 0.4959860146045685:  37%|████▊        | 90/241 [00:13<00:25,  5.83it/s]Epoch: 3, train for the 79-th batch, train loss: 0.5628228783607483:  53%|██████▉      | 78/146 [00:11<00:10,  6.74it/s]Epoch: 3, train for the 79-th batch, train loss: 0.5628228783607483:  54%|███████      | 79/146 [00:11<00:10,  6.53it/s]evaluate for the 6-th batch, evaluate loss: 0.6724750995635986:  19%|███▊                | 4/21 [00:00<00:01, 13.08it/s]evaluate for the 6-th batch, evaluate loss: 0.6724750995635986:  29%|█████▋              | 6/21 [00:00<00:01, 12.89it/s]evaluate for the 15-th batch, evaluate loss: 0.5473436117172241:  30%|█████▍            | 14/46 [00:01<00:02, 11.45it/s]evaluate for the 7-th batch, evaluate loss: 0.641196608543396:  29%|██████               | 6/21 [00:00<00:01, 12.89it/s]Epoch: 1, train for the 273-th batch, train loss: 0.42240074276924133:  71%|███████   | 272/383 [01:15<00:35,  3.13it/s]evaluate for the 35-th batch, evaluate loss: 0.7289982438087463:  85%|███████████████▎  | 34/40 [00:04<00:00,  7.90it/s]evaluate for the 35-th batch, evaluate loss: 0.7289982438087463:  88%|███████████████▊  | 35/40 [00:04<00:00,  7.04it/s]Epoch: 3, train for the 80-th batch, train loss: 0.5444847941398621:  54%|███████      | 79/146 [00:11<00:10,  6.53it/s]Epoch: 3, train for the 80-th batch, train loss: 0.5444847941398621:  55%|███████      | 80/146 [00:11<00:09,  6.72it/s]Epoch: 2, train for the 91-th batch, train loss: 0.4877093732357025:  37%|████▊        | 90/241 [00:13<00:25,  5.83it/s]evaluate for the 16-th batch, evaluate loss: 0.5270416140556335:  30%|█████▍            | 14/46 [00:01<00:02, 11.45it/s]evaluate for the 16-th batch, evaluate loss: 0.5270416140556335:  35%|██████▎           | 16/46 [00:01<00:02, 10.83it/s]Epoch: 2, train for the 91-th batch, train loss: 0.4877093732357025:  38%|████▉        | 91/241 [00:13<00:27,  5.55it/s]evaluate for the 8-th batch, evaluate loss: 0.6568841934204102:  29%|█████▋              | 6/21 [00:00<00:01, 12.89it/s]evaluate for the 8-th batch, evaluate loss: 0.6568841934204102:  38%|███████▌            | 8/21 [00:00<00:01, 12.90it/s]Epoch: 1, train for the 273-th batch, train loss: 0.42240074276924133:  71%|███████▏  | 273/383 [01:15<00:36,  3.03it/s]evaluate for the 36-th batch, evaluate loss: 0.754380464553833:  88%|████████████████▋  | 35/40 [00:04<00:00,  7.04it/s]evaluate for the 36-th batch, evaluate loss: 0.754380464553833:  90%|█████████████████  | 36/40 [00:04<00:00,  7.45it/s]evaluate for the 17-th batch, evaluate loss: 0.4638710618019104:  35%|██████▎           | 16/46 [00:01<00:02, 10.83it/s]Epoch: 3, train for the 81-th batch, train loss: 0.5092657804489136:  55%|███████      | 80/146 [00:11<00:09,  6.72it/s]evaluate for the 9-th batch, evaluate loss: 0.6389965415000916:  38%|███████▌            | 8/21 [00:00<00:01, 12.90it/s]Epoch: 3, train for the 81-th batch, train loss: 0.5092657804489136:  55%|███████▏     | 81/146 [00:11<00:09,  6.53it/s]Epoch: 2, train for the 92-th batch, train loss: 0.40552741289138794:  38%|████▌       | 91/241 [00:13<00:27,  5.55it/s]Epoch: 2, train for the 92-th batch, train loss: 0.40552741289138794:  38%|████▌       | 92/241 [00:13<00:26,  5.59it/s]evaluate for the 18-th batch, evaluate loss: 0.5197245478630066:  35%|██████▎           | 16/46 [00:01<00:02, 10.83it/s]evaluate for the 18-th batch, evaluate loss: 0.5197245478630066:  39%|███████           | 18/46 [00:01<00:02, 10.72it/s]evaluate for the 37-th batch, evaluate loss: 0.7367238998413086:  90%|████████████████▏ | 36/40 [00:04<00:00,  7.45it/s]evaluate for the 37-th batch, evaluate loss: 0.7367238998413086:  92%|████████████████▋ | 37/40 [00:04<00:00,  7.80it/s]evaluate for the 10-th batch, evaluate loss: 0.6704601645469666:  38%|███████▏           | 8/21 [00:00<00:01, 12.90it/s]evaluate for the 10-th batch, evaluate loss: 0.6704601645469666:  48%|████████▌         | 10/21 [00:00<00:00, 11.41it/s]Epoch: 1, train for the 274-th batch, train loss: 0.30328333377838135:  71%|███████▏  | 273/383 [01:15<00:36,  3.03it/s]evaluate for the 19-th batch, evaluate loss: 0.5657148361206055:  39%|███████           | 18/46 [00:01<00:02, 10.72it/s]evaluate for the 38-th batch, evaluate loss: 0.7166805267333984:  92%|████████████████▋ | 37/40 [00:04<00:00,  7.80it/s]evaluate for the 38-th batch, evaluate loss: 0.7166805267333984:  95%|█████████████████ | 38/40 [00:04<00:00,  8.11it/s]Epoch: 3, train for the 82-th batch, train loss: 0.5270357131958008:  55%|███████▏     | 81/146 [00:11<00:09,  6.53it/s]evaluate for the 11-th batch, evaluate loss: 0.6607153415679932:  48%|████████▌         | 10/21 [00:00<00:00, 11.41it/s]Epoch: 1, train for the 274-th batch, train loss: 0.30328333377838135:  72%|███████▏  | 274/383 [01:15<00:34,  3.12it/s]Epoch: 3, train for the 82-th batch, train loss: 0.5270357131958008:  56%|███████▎     | 82/146 [00:11<00:10,  6.24it/s]Epoch: 2, train for the 93-th batch, train loss: 0.43453001976013184:  38%|████▌       | 92/241 [00:13<00:26,  5.59it/s]evaluate for the 20-th batch, evaluate loss: 0.516948938369751:  39%|███████▍           | 18/46 [00:01<00:02, 10.72it/s]evaluate for the 20-th batch, evaluate loss: 0.516948938369751:  43%|████████▎          | 20/46 [00:01<00:02, 11.45it/s]Epoch: 2, train for the 93-th batch, train loss: 0.43453001976013184:  39%|████▋       | 93/241 [00:13<00:25,  5.73it/s]evaluate for the 12-th batch, evaluate loss: 0.6507742404937744:  48%|████████▌         | 10/21 [00:01<00:00, 11.41it/s]evaluate for the 12-th batch, evaluate loss: 0.6507742404937744:  57%|██████████▎       | 12/21 [00:01<00:00, 11.15it/s]evaluate for the 39-th batch, evaluate loss: 0.7312609553337097:  95%|█████████████████ | 38/40 [00:05<00:00,  8.11it/s]evaluate for the 39-th batch, evaluate loss: 0.7312609553337097:  98%|█████████████████▌| 39/40 [00:05<00:00,  7.88it/s]evaluate for the 21-th batch, evaluate loss: 0.5680469870567322:  43%|███████▊          | 20/46 [00:01<00:02, 11.45it/s]Epoch: 3, train for the 83-th batch, train loss: 0.5217382907867432:  56%|███████▎     | 82/146 [00:11<00:10,  6.24it/s]Epoch: 3, train for the 83-th batch, train loss: 0.5217382907867432:  57%|███████▍     | 83/146 [00:11<00:10,  6.29it/s]evaluate for the 13-th batch, evaluate loss: 0.6396015882492065:  57%|██████████▎       | 12/21 [00:01<00:00, 11.15it/s]Epoch: 2, train for the 94-th batch, train loss: 0.3932878077030182:  39%|█████        | 93/241 [00:13<00:25,  5.73it/s]evaluate for the 22-th batch, evaluate loss: 0.5192095637321472:  43%|███████▊          | 20/46 [00:01<00:02, 11.45it/s]evaluate for the 22-th batch, evaluate loss: 0.5192095637321472:  48%|████████▌         | 22/46 [00:01<00:02, 11.23it/s]Epoch: 2, train for the 94-th batch, train loss: 0.3932878077030182:  39%|█████        | 94/241 [00:13<00:26,  5.62it/s]evaluate for the 14-th batch, evaluate loss: 0.6216006278991699:  57%|██████████▎       | 12/21 [00:01<00:00, 11.15it/s]evaluate for the 14-th batch, evaluate loss: 0.6216006278991699:  67%|████████████      | 14/21 [00:01<00:00, 11.94it/s]Epoch: 1, train for the 275-th batch, train loss: 0.43764960765838623:  72%|███████▏  | 274/383 [01:15<00:34,  3.12it/s]Epoch: 3, train for the 84-th batch, train loss: 0.4687947630882263:  57%|███████▍     | 83/146 [00:11<00:10,  6.29it/s]evaluate for the 40-th batch, evaluate loss: 0.7529128193855286:  98%|█████████████████▌| 39/40 [00:05<00:00,  7.88it/s]evaluate for the 40-th batch, evaluate loss: 0.7529128193855286: 100%|██████████████████| 40/40 [00:05<00:00,  7.51it/s]evaluate for the 40-th batch, evaluate loss: 0.7529128193855286: 100%|██████████████████| 40/40 [00:05<00:00,  7.71it/s]
Epoch: 3, train for the 84-th batch, train loss: 0.4687947630882263:  58%|███████▍     | 84/146 [00:12<00:09,  6.67it/s]evaluate for the 23-th batch, evaluate loss: 0.47790926694869995:  48%|████████▏        | 22/46 [00:02<00:02, 11.23it/s]evaluate for the 15-th batch, evaluate loss: 0.6725153923034668:  67%|████████████      | 14/21 [00:01<00:00, 11.94it/s]Epoch: 1, train for the 275-th batch, train loss: 0.43764960765838623:  72%|███████▏  | 275/383 [01:15<00:34,  3.12it/s]Epoch: 2, train for the 95-th batch, train loss: 0.4415546655654907:  39%|█████        | 94/241 [00:14<00:26,  5.62it/s]evaluate for the 24-th batch, evaluate loss: 0.48818662762641907:  48%|████████▏        | 22/46 [00:02<00:02, 11.23it/s]evaluate for the 24-th batch, evaluate loss: 0.48818662762641907:  52%|████████▊        | 24/46 [00:02<00:01, 11.55it/s]Epoch: 2, train for the 95-th batch, train loss: 0.4415546655654907:  39%|█████        | 95/241 [00:14<00:24,  5.85it/s]evaluate for the 16-th batch, evaluate loss: 0.6470302939414978:  67%|████████████      | 14/21 [00:01<00:00, 11.94it/s]evaluate for the 16-th batch, evaluate loss: 0.6470302939414978:  76%|█████████████▋    | 16/21 [00:01<00:00, 12.59it/s]Epoch: 3, train for the 85-th batch, train loss: 0.5183924436569214:  58%|███████▍     | 84/146 [00:12<00:09,  6.67it/s]Epoch: 3, train for the 85-th batch, train loss: 0.5183924436569214:  58%|███████▌     | 85/146 [00:12<00:08,  6.91it/s]evaluate for the 17-th batch, evaluate loss: 0.5822120308876038:  76%|█████████████▋    | 16/21 [00:01<00:00, 12.59it/s]evaluate for the 25-th batch, evaluate loss: 0.5149992108345032:  52%|█████████▍        | 24/46 [00:02<00:01, 11.55it/s]Epoch: 2, train for the 96-th batch, train loss: 0.5343490242958069:  39%|█████        | 95/241 [00:14<00:24,  5.85it/s]Epoch: 2, train for the 96-th batch, train loss: 0.5343490242958069:  40%|█████▏       | 96/241 [00:14<00:24,  5.84it/s]evaluate for the 18-th batch, evaluate loss: 0.6524078249931335:  76%|█████████████▋    | 16/21 [00:01<00:00, 12.59it/s]evaluate for the 18-th batch, evaluate loss: 0.6524078249931335:  86%|███████████████▍  | 18/21 [00:01<00:00, 12.15it/s]Epoch: 3, train for the 86-th batch, train loss: 0.5039905905723572:  58%|███████▌     | 85/146 [00:12<00:08,  6.91it/s]Epoch: 3, train for the 86-th batch, train loss: 0.5039905905723572:  59%|███████▋     | 86/146 [00:12<00:08,  6.73it/s]evaluate for the 26-th batch, evaluate loss: 0.5468533039093018:  52%|█████████▍        | 24/46 [00:02<00:01, 11.55it/s]evaluate for the 26-th batch, evaluate loss: 0.5468533039093018:  57%|██████████▏       | 26/46 [00:02<00:01, 10.72it/s]evaluate for the 19-th batch, evaluate loss: 0.6464575529098511:  86%|███████████████▍  | 18/21 [00:01<00:00, 12.15it/s]INFO:root:Epoch: 1, learning rate: 0.0001, train loss: 0.5879
INFO:root:train average_precision, 0.7874
INFO:root:train roc_auc, 0.7608
INFO:root:validate loss: 0.5462
INFO:root:validate average_precision, 0.8161
Epoch: 1, train for the 276-th batch, train loss: 0.6500811576843262:  72%|███████▉   | 275/383 [01:16<00:34,  3.12it/s]INFO:root:validate roc_auc, 0.8005
INFO:root:new node validate loss: 0.6855
INFO:root:new node validate first_1_average_precision, 0.6116
INFO:root:new node validate first_1_roc_auc, 0.5372
INFO:root:new node validate first_3_average_precision, 0.6351
INFO:root:new node validate first_3_roc_auc, 0.5685
INFO:root:new node validate first_10_average_precision, 0.6683
INFO:root:new node validate first_10_roc_auc, 0.6151
INFO:root:new node validate average_precision, 0.6767
INFO:root:new node validate roc_auc, 0.6308
INFO:root:save model ./saved_models/TGN/ia-digg-reply/TGN_seed0_dummy-s7jpz2dn/TGN_seed0_dummy-s7jpz2dn.pkl
Epoch: 1, train for the 276-th batch, train loss: 0.6500811576843262:  72%|███████▉   | 276/383 [01:16<00:35,  2.98it/s]evaluate for the 20-th batch, evaluate loss: 0.6380402445793152:  86%|███████████████▍  | 18/21 [00:01<00:00, 12.15it/s]evaluate for the 20-th batch, evaluate loss: 0.6380402445793152:  95%|█████████████████▏| 20/21 [00:01<00:00, 12.62it/s]Epoch: 2, train for the 97-th batch, train loss: 0.3924658000469208:  40%|█████▏       | 96/241 [00:14<00:24,  5.84it/s]evaluate for the 27-th batch, evaluate loss: 0.5376086831092834:  57%|██████████▏       | 26/46 [00:02<00:01, 10.72it/s]Epoch: 2, train for the 97-th batch, train loss: 0.3924658000469208:  40%|█████▏       | 97/241 [00:14<00:25,  5.72it/s]Epoch: 3, train for the 87-th batch, train loss: 0.5173691511154175:  59%|███████▋     | 86/146 [00:12<00:08,  6.73it/s]evaluate for the 21-th batch, evaluate loss: 0.5951682925224304:  95%|█████████████████▏| 20/21 [00:01<00:00, 12.62it/s]evaluate for the 21-th batch, evaluate loss: 0.5951682925224304: 100%|██████████████████| 21/21 [00:01<00:00, 12.57it/s]
Epoch: 3, train for the 87-th batch, train loss: 0.5173691511154175:  60%|███████▋     | 87/146 [00:12<00:09,  6.42it/s]evaluate for the 28-th batch, evaluate loss: 0.5427236557006836:  57%|██████████▏       | 26/46 [00:02<00:01, 10.72it/s]evaluate for the 28-th batch, evaluate loss: 0.5427236557006836:  61%|██████████▉       | 28/46 [00:02<00:01, 10.01it/s]Epoch: 3, train for the 88-th batch, train loss: 0.5486451387405396:  60%|███████▋     | 87/146 [00:12<00:09,  6.42it/s]Epoch: 2, train for the 98-th batch, train loss: 0.47432151436805725:  40%|████▊       | 97/241 [00:14<00:25,  5.72it/s]Epoch: 3, train for the 88-th batch, train loss: 0.5486451387405396:  60%|███████▊     | 88/146 [00:12<00:08,  6.72it/s]Epoch: 1, train for the 277-th batch, train loss: 0.4422000050544739:  72%|███████▉   | 276/383 [01:16<00:35,  2.98it/s]evaluate for the 29-th batch, evaluate loss: 0.5124839544296265:  61%|██████████▉       | 28/46 [00:02<00:01, 10.01it/s]Epoch: 2, train for the 98-th batch, train loss: 0.47432151436805725:  41%|████▉       | 98/241 [00:14<00:24,  5.79it/s]Epoch: 1, train for the 277-th batch, train loss: 0.4422000050544739:  72%|███████▉   | 277/383 [01:16<00:33,  3.19it/s]evaluate for the 30-th batch, evaluate loss: 0.49805372953414917:  61%|██████████▎      | 28/46 [00:02<00:01, 10.01it/s]evaluate for the 30-th batch, evaluate loss: 0.49805372953414917:  65%|███████████      | 30/46 [00:02<00:01, 10.78it/s]INFO:root:Epoch: 3, learning rate: 0.0001, train loss: 0.4936
INFO:root:train average_precision, 0.8520
INFO:root:train roc_auc, 0.8415
INFO:root:validate loss: 0.4912
INFO:root:validate average_precision, 0.8645
INFO:root:validate roc_auc, 0.8597
INFO:root:new node validate loss: 0.6438
INFO:root:new node validate first_1_average_precision, 0.6990
INFO:root:new node validate first_1_roc_auc, 0.6784
INFO:root:new node validate first_3_average_precision, 0.7293
INFO:root:new node validate first_3_roc_auc, 0.7180
INFO:root:new node validate first_10_average_precision, 0.7319
INFO:root:new node validate first_10_roc_auc, 0.7301
INFO:root:new node validate average_precision, 0.7234
INFO:root:new node validate roc_auc, 0.7367
INFO:root:save model ./saved_models/TGN/ia-reality-call/TGN_seed0_dummy-yehxa035/TGN_seed0_dummy-yehxa035.pkl
Epoch: 2, train for the 99-th batch, train loss: 0.29842981696128845:  41%|████▉       | 98/241 [00:14<00:24,  5.79it/s]Epoch: 3, train for the 89-th batch, train loss: 0.5082913041114807:  60%|███████▊     | 88/146 [00:12<00:08,  6.72it/s]Epoch: 2, train for the 99-th batch, train loss: 0.29842981696128845:  41%|████▉       | 99/241 [00:14<00:22,  6.24it/s]Epoch: 3, train for the 89-th batch, train loss: 0.5082913041114807:  61%|███████▉     | 89/146 [00:12<00:08,  6.71it/s]evaluate for the 31-th batch, evaluate loss: 0.4504653811454773:  65%|███████████▋      | 30/46 [00:02<00:01, 10.78it/s]evaluate for the 32-th batch, evaluate loss: 0.4839799404144287:  65%|███████████▋      | 30/46 [00:02<00:01, 10.78it/s]evaluate for the 32-th batch, evaluate loss: 0.4839799404144287:  70%|████████████▌     | 32/46 [00:02<00:01, 11.62it/s]Epoch: 3, train for the 90-th batch, train loss: 0.5239881873130798:  61%|███████▉     | 89/146 [00:12<00:08,  6.71it/s]Epoch: 3, train for the 90-th batch, train loss: 0.5239881873130798:  62%|████████     | 90/146 [00:12<00:07,  7.32it/s]Epoch: 1, train for the 278-th batch, train loss: 0.36418280005455017:  72%|███████▏  | 277/383 [01:16<00:33,  3.19it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5127045512199402:  41%|████▉       | 99/241 [00:14<00:22,  6.24it/s]evaluate for the 33-th batch, evaluate loss: 0.49549636244773865:  70%|███████████▊     | 32/46 [00:02<00:01, 11.62it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5127045512199402:  41%|████▌      | 100/241 [00:14<00:22,  6.29it/s]Epoch: 1, train for the 278-th batch, train loss: 0.36418280005455017:  73%|███████▎  | 278/383 [01:16<00:31,  3.36it/s]Epoch: 3, train for the 91-th batch, train loss: 0.5271058678627014:  62%|████████     | 90/146 [00:12<00:07,  7.32it/s]evaluate for the 34-th batch, evaluate loss: 0.45625045895576477:  70%|███████████▊     | 32/46 [00:03<00:01, 11.62it/s]evaluate for the 34-th batch, evaluate loss: 0.45625045895576477:  74%|████████████▌    | 34/46 [00:03<00:00, 12.27it/s]Epoch: 2, train for the 101-th batch, train loss: 0.5095757842063904:  41%|████▌      | 100/241 [00:14<00:22,  6.29it/s]Epoch: 3, train for the 92-th batch, train loss: 0.5844324231147766:  62%|████████     | 90/146 [00:13<00:07,  7.32it/s]Epoch: 2, train for the 101-th batch, train loss: 0.5095757842063904:  42%|████▌      | 101/241 [00:14<00:21,  6.46it/s]Epoch: 3, train for the 92-th batch, train loss: 0.5844324231147766:  63%|████████▏    | 92/146 [00:13<00:06,  8.36it/s]evaluate for the 35-th batch, evaluate loss: 0.5149385929107666:  74%|█████████████▎    | 34/46 [00:03<00:00, 12.27it/s]Epoch: 1, train for the 279-th batch, train loss: 0.5112327337265015:  73%|███████▉   | 278/383 [01:16<00:31,  3.36it/s]Epoch: 3, train for the 93-th batch, train loss: 0.5324280261993408:  63%|████████▏    | 92/146 [00:13<00:06,  8.36it/s]evaluate for the 36-th batch, evaluate loss: 0.48127955198287964:  74%|████████████▌    | 34/46 [00:03<00:00, 12.27it/s]evaluate for the 36-th batch, evaluate loss: 0.48127955198287964:  78%|█████████████▎   | 36/46 [00:03<00:00, 11.68it/s]Epoch: 3, train for the 93-th batch, train loss: 0.5324280261993408:  64%|████████▎    | 93/146 [00:13<00:06,  8.61it/s]Epoch: 1, train for the 279-th batch, train loss: 0.5112327337265015:  73%|████████   | 279/383 [01:17<00:29,  3.52it/s]Epoch: 2, train for the 102-th batch, train loss: 0.5363232493400574:  42%|████▌      | 101/241 [00:15<00:21,  6.46it/s]Epoch: 2, train for the 102-th batch, train loss: 0.5363232493400574:  42%|████▋      | 102/241 [00:15<00:22,  6.31it/s]evaluate for the 37-th batch, evaluate loss: 0.5175586342811584:  78%|██████████████    | 36/46 [00:03<00:00, 11.68it/s]Epoch: 3, train for the 94-th batch, train loss: 0.5091179013252258:  64%|████████▎    | 93/146 [00:13<00:06,  8.61it/s]Epoch: 3, train for the 94-th batch, train loss: 0.5091179013252258:  64%|████████▎    | 94/146 [00:13<00:05,  8.86it/s]evaluate for the 38-th batch, evaluate loss: 0.5049522519111633:  78%|██████████████    | 36/46 [00:03<00:00, 11.68it/s]evaluate for the 38-th batch, evaluate loss: 0.5049522519111633:  83%|██████████████▊   | 38/46 [00:03<00:00, 11.58it/s]Epoch: 2, train for the 103-th batch, train loss: 0.5007354617118835:  42%|████▋      | 102/241 [00:15<00:22,  6.31it/s]Epoch: 3, train for the 95-th batch, train loss: 0.5441136956214905:  64%|████████▎    | 94/146 [00:13<00:05,  8.86it/s]Epoch: 2, train for the 103-th batch, train loss: 0.5007354617118835:  43%|████▋      | 103/241 [00:15<00:21,  6.38it/s]Epoch: 3, train for the 95-th batch, train loss: 0.5441136956214905:  65%|████████▍    | 95/146 [00:13<00:05,  9.00it/s]Epoch: 1, train for the 280-th batch, train loss: 0.3352120816707611:  73%|████████   | 279/383 [01:17<00:29,  3.52it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]evaluate for the 39-th batch, evaluate loss: 0.4891488254070282:  83%|██████████████▊   | 38/46 [00:03<00:00, 11.58it/s]Epoch: 1, train for the 280-th batch, train loss: 0.3352120816707611:  73%|████████   | 280/383 [01:17<00:28,  3.56it/s]Epoch: 2, train for the 104-th batch, train loss: 0.5227836966514587:  43%|████▋      | 103/241 [00:15<00:21,  6.38it/s]Epoch: 3, train for the 96-th batch, train loss: 0.50775146484375:  65%|█████████▊     | 95/146 [00:13<00:05,  9.00it/s]evaluate for the 40-th batch, evaluate loss: 0.47917136549949646:  83%|██████████████   | 38/46 [00:03<00:00, 11.58it/s]evaluate for the 40-th batch, evaluate loss: 0.47917136549949646:  87%|██████████████▊  | 40/46 [00:03<00:00, 11.80it/s]Epoch: 3, train for the 96-th batch, train loss: 0.50775146484375:  66%|█████████▊     | 96/146 [00:13<00:05,  8.65it/s]Epoch: 2, train for the 104-th batch, train loss: 0.5227836966514587:  43%|████▋      | 104/241 [00:15<00:20,  6.65it/s]evaluate for the 41-th batch, evaluate loss: 0.4821186363697052:  87%|███████████████▋  | 40/46 [00:03<00:00, 11.80it/s]Epoch: 2, train for the 105-th batch, train loss: 0.44358932971954346:  43%|████▎     | 104/241 [00:15<00:20,  6.65it/s]Epoch: 3, train for the 97-th batch, train loss: 0.5158457159996033:  66%|████████▌    | 96/146 [00:13<00:05,  8.65it/s]evaluate for the 42-th batch, evaluate loss: 0.46462148427963257:  87%|██████████████▊  | 40/46 [00:03<00:00, 11.80it/s]evaluate for the 42-th batch, evaluate loss: 0.46462148427963257:  91%|███████████████▌ | 42/46 [00:03<00:00, 11.85it/s]Epoch: 4, train for the 1-th batch, train loss: 0.6429162621498108:   0%|                       | 0/119 [00:00<?, ?it/s]Epoch: 3, train for the 97-th batch, train loss: 0.5158457159996033:  66%|████████▋    | 97/146 [00:13<00:06,  7.64it/s]Epoch: 4, train for the 1-th batch, train loss: 0.6429162621498108:   1%|▏              | 1/119 [00:00<00:29,  3.95it/s]Epoch: 2, train for the 105-th batch, train loss: 0.44358932971954346:  44%|████▎     | 105/241 [00:15<00:21,  6.36it/s]Epoch: 1, train for the 281-th batch, train loss: 0.3917493224143982:  73%|████████   | 280/383 [01:17<00:28,  3.56it/s]evaluate for the 43-th batch, evaluate loss: 0.5521680116653442:  91%|████████████████▍ | 42/46 [00:03<00:00, 11.85it/s]Epoch: 1, train for the 281-th batch, train loss: 0.3917493224143982:  73%|████████   | 281/383 [01:17<00:29,  3.40it/s]Epoch: 3, train for the 98-th batch, train loss: 0.5030288696289062:  66%|████████▋    | 97/146 [00:13<00:06,  7.64it/s]Epoch: 4, train for the 2-th batch, train loss: 0.7204592823982239:   1%|▏              | 1/119 [00:00<00:29,  3.95it/s]Epoch: 4, train for the 2-th batch, train loss: 0.7204592823982239:   2%|▎              | 2/119 [00:00<00:22,  5.10it/s]Epoch: 3, train for the 98-th batch, train loss: 0.5030288696289062:  67%|████████▋    | 98/146 [00:13<00:06,  7.17it/s]Epoch: 2, train for the 106-th batch, train loss: 0.4894562363624573:  44%|████▊      | 105/241 [00:15<00:21,  6.36it/s]evaluate for the 44-th batch, evaluate loss: 0.4992782771587372:  91%|████████████████▍ | 42/46 [00:03<00:00, 11.85it/s]evaluate for the 44-th batch, evaluate loss: 0.4992782771587372:  96%|█████████████████▏| 44/46 [00:03<00:00, 11.70it/s]Epoch: 2, train for the 106-th batch, train loss: 0.4894562363624573:  44%|████▊      | 106/241 [00:15<00:22,  6.09it/s]evaluate for the 45-th batch, evaluate loss: 0.4601041376590729:  96%|█████████████████▏| 44/46 [00:03<00:00, 11.70it/s]Epoch: 4, train for the 3-th batch, train loss: 0.754620373249054:   2%|▎               | 2/119 [00:00<00:22,  5.10it/s]Epoch: 1, train for the 282-th batch, train loss: 0.4823122024536133:  73%|████████   | 281/383 [01:17<00:29,  3.40it/s]Epoch: 4, train for the 3-th batch, train loss: 0.754620373249054:   3%|▍               | 3/119 [00:00<00:20,  5.55it/s]evaluate for the 46-th batch, evaluate loss: 0.45824453234672546:  96%|████████████████▎| 44/46 [00:04<00:00, 11.70it/s]evaluate for the 46-th batch, evaluate loss: 0.45824453234672546: 100%|█████████████████| 46/46 [00:04<00:00, 11.82it/s]evaluate for the 46-th batch, evaluate loss: 0.45824453234672546: 100%|█████████████████| 46/46 [00:04<00:00, 11.32it/s]
Epoch: 3, train for the 99-th batch, train loss: 0.5448336005210876:  67%|████████▋    | 98/146 [00:14<00:06,  7.17it/s]Epoch: 3, train for the 99-th batch, train loss: 0.5448336005210876:  68%|████████▊    | 99/146 [00:14<00:07,  6.37it/s]Epoch: 1, train for the 282-th batch, train loss: 0.4823122024536133:  74%|████████   | 282/383 [01:17<00:28,  3.50it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5577095150947571:  44%|████▊      | 106/241 [00:15<00:22,  6.09it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5577095150947571:  44%|████▉      | 107/241 [00:16<00:24,  5.51it/s]Epoch: 4, train for the 4-th batch, train loss: 0.6890935897827148:   3%|▍              | 3/119 [00:00<00:20,  5.55it/s]Epoch: 3, train for the 100-th batch, train loss: 0.5980488657951355:  68%|████████▏   | 99/146 [00:14<00:07,  6.37it/s]Epoch: 3, train for the 100-th batch, train loss: 0.5980488657951355:  68%|███████▌   | 100/146 [00:14<00:06,  6.79it/s]Epoch: 2, train for the 108-th batch, train loss: 0.4986405074596405:  44%|████▉      | 107/241 [00:16<00:24,  5.51it/s]Epoch: 2, train for the 108-th batch, train loss: 0.4986405074596405:  45%|████▉      | 108/241 [00:16<00:22,  6.03it/s]Epoch: 4, train for the 5-th batch, train loss: 0.6338078379631042:   3%|▍              | 3/119 [00:00<00:20,  5.55it/s]Epoch: 4, train for the 5-th batch, train loss: 0.6338078379631042:   4%|▋              | 5/119 [00:00<00:16,  6.99it/s]Epoch: 1, train for the 283-th batch, train loss: 0.48388031125068665:  74%|███████▎  | 282/383 [01:18<00:28,  3.50it/s]Epoch: 3, train for the 101-th batch, train loss: 0.5345770716667175:  68%|███████▌   | 100/146 [00:14<00:06,  6.79it/s]Epoch: 3, train for the 101-th batch, train loss: 0.5345770716667175:  69%|███████▌   | 101/146 [00:14<00:06,  7.14it/s]Epoch: 1, train for the 283-th batch, train loss: 0.48388031125068665:  74%|███████▍  | 283/383 [01:18<00:27,  3.64it/s]Epoch: 4, train for the 6-th batch, train loss: 0.5304841995239258:   4%|▋              | 5/119 [00:00<00:16,  6.99it/s]Epoch: 2, train for the 109-th batch, train loss: 0.6500328183174133:  45%|████▉      | 108/241 [00:16<00:22,  6.03it/s]Epoch: 4, train for the 6-th batch, train loss: 0.5304841995239258:   5%|▊              | 6/119 [00:00<00:15,  7.39it/s]Epoch: 2, train for the 109-th batch, train loss: 0.6500328183174133:  45%|████▉      | 109/241 [00:16<00:21,  6.23it/s]Epoch: 3, train for the 102-th batch, train loss: 0.5335643887519836:  69%|███████▌   | 101/146 [00:14<00:06,  7.14it/s]Epoch: 3, train for the 102-th batch, train loss: 0.5335643887519836:  70%|███████▋   | 102/146 [00:14<00:06,  7.30it/s]Epoch: 4, train for the 7-th batch, train loss: 0.5041926503181458:   5%|▊              | 6/119 [00:01<00:15,  7.39it/s]  0%|                                                                                            | 0/25 [00:00<?, ?it/s]Epoch: 4, train for the 7-th batch, train loss: 0.5041926503181458:   6%|▉              | 7/119 [00:01<00:14,  7.78it/s]Epoch: 2, train for the 110-th batch, train loss: 0.551443338394165:  45%|█████▍      | 109/241 [00:16<00:21,  6.23it/s]Epoch: 1, train for the 284-th batch, train loss: 0.5172209739685059:  74%|████████▏  | 283/383 [01:18<00:27,  3.64it/s]Epoch: 2, train for the 110-th batch, train loss: 0.551443338394165:  46%|█████▍      | 110/241 [00:16<00:20,  6.34it/s]Epoch: 3, train for the 103-th batch, train loss: 0.4929910898208618:  70%|███████▋   | 102/146 [00:14<00:06,  7.30it/s]evaluate for the 1-th batch, evaluate loss: 0.730764627456665:   0%|                             | 0/25 [00:00<?, ?it/s]Epoch: 3, train for the 103-th batch, train loss: 0.4929910898208618:  71%|███████▊   | 103/146 [00:14<00:05,  7.44it/s]Epoch: 4, train for the 8-th batch, train loss: 0.4421442449092865:   6%|▉              | 7/119 [00:01<00:14,  7.78it/s]Epoch: 4, train for the 8-th batch, train loss: 0.4421442449092865:   7%|█              | 8/119 [00:01<00:13,  7.94it/s]Epoch: 1, train for the 284-th batch, train loss: 0.5172209739685059:  74%|████████▏  | 284/383 [01:18<00:27,  3.64it/s]Epoch: 2, train for the 111-th batch, train loss: 0.23122569918632507:  46%|████▌     | 110/241 [00:16<00:20,  6.34it/s]evaluate for the 2-th batch, evaluate loss: 0.7481207251548767:   0%|                            | 0/25 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.7481207251548767:   8%|█▌                  | 2/25 [00:00<00:02,  9.39it/s]Epoch: 2, train for the 111-th batch, train loss: 0.23122569918632507:  46%|████▌     | 111/241 [00:16<00:20,  6.35it/s]Epoch: 3, train for the 104-th batch, train loss: 0.5407503247261047:  71%|███████▊   | 103/146 [00:14<00:05,  7.44it/s]Epoch: 4, train for the 9-th batch, train loss: 0.44713881611824036:   7%|▉             | 8/119 [00:01<00:13,  7.94it/s]Epoch: 3, train for the 104-th batch, train loss: 0.5407503247261047:  71%|███████▊   | 104/146 [00:14<00:06,  6.92it/s]Epoch: 4, train for the 9-th batch, train loss: 0.44713881611824036:   8%|█             | 9/119 [00:01<00:14,  7.71it/s]evaluate for the 3-th batch, evaluate loss: 0.7557125687599182:   8%|█▌                  | 2/25 [00:00<00:02,  9.39it/s]Epoch: 1, train for the 285-th batch, train loss: 0.43942663073539734:  74%|███████▍  | 284/383 [01:18<00:27,  3.64it/s]Epoch: 1, train for the 285-th batch, train loss: 0.43942663073539734:  74%|███████▍  | 285/383 [01:18<00:26,  3.68it/s]Epoch: 2, train for the 112-th batch, train loss: 0.3209111988544464:  46%|█████      | 111/241 [00:16<00:20,  6.35it/s]evaluate for the 4-th batch, evaluate loss: 0.7105231881141663:   8%|█▌                  | 2/25 [00:00<00:02,  9.39it/s]evaluate for the 4-th batch, evaluate loss: 0.7105231881141663:  16%|███▏                | 4/25 [00:00<00:02,  9.93it/s]Epoch: 2, train for the 112-th batch, train loss: 0.3209111988544464:  46%|█████      | 112/241 [00:16<00:21,  5.91it/s]Epoch: 4, train for the 10-th batch, train loss: 0.4675839841365814:   8%|█             | 9/119 [00:01<00:14,  7.71it/s]Epoch: 4, train for the 10-th batch, train loss: 0.4675839841365814:   8%|█            | 10/119 [00:01<00:15,  7.11it/s]Epoch: 3, train for the 105-th batch, train loss: 0.547935426235199:  71%|████████▌   | 104/146 [00:14<00:06,  6.92it/s]Epoch: 3, train for the 105-th batch, train loss: 0.547935426235199:  72%|████████▋   | 105/146 [00:14<00:06,  6.25it/s]evaluate for the 5-th batch, evaluate loss: 0.7364242672920227:  16%|███▏                | 4/25 [00:00<00:02,  9.93it/s]Epoch: 4, train for the 11-th batch, train loss: 0.4667794406414032:   8%|█            | 10/119 [00:01<00:15,  7.11it/s]Epoch: 2, train for the 113-th batch, train loss: 0.38682350516319275:  46%|████▋     | 112/241 [00:16<00:21,  5.91it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/237 [00:00<?, ?it/s]Epoch: 4, train for the 11-th batch, train loss: 0.4667794406414032:   9%|█▏           | 11/119 [00:01<00:15,  6.96it/s]evaluate for the 6-th batch, evaluate loss: 0.736129879951477:  16%|███▎                 | 4/25 [00:00<00:02,  9.93it/s]evaluate for the 6-th batch, evaluate loss: 0.736129879951477:  24%|█████                | 6/25 [00:00<00:01, 10.43it/s]Epoch: 2, train for the 113-th batch, train loss: 0.38682350516319275:  47%|████▋     | 113/241 [00:16<00:22,  5.76it/s]Epoch: 1, train for the 286-th batch, train loss: 0.38055419921875:  74%|█████████▋   | 285/383 [01:18<00:26,  3.68it/s]Epoch: 3, train for the 106-th batch, train loss: 0.5193089842796326:  72%|███████▉   | 105/146 [00:15<00:06,  6.25it/s]Epoch: 3, train for the 106-th batch, train loss: 0.5193089842796326:  73%|███████▉   | 106/146 [00:15<00:06,  5.84it/s]evaluate for the 7-th batch, evaluate loss: 0.751879870891571:  24%|█████                | 6/25 [00:00<00:01, 10.43it/s]Epoch: 2, train for the 1-th batch, train loss: 0.6841727495193481:   0%|                       | 0/237 [00:00<?, ?it/s]Epoch: 1, train for the 286-th batch, train loss: 0.38055419921875:  75%|█████████▋   | 286/383 [01:18<00:27,  3.58it/s]Epoch: 2, train for the 1-th batch, train loss: 0.6841727495193481:   0%|               | 1/237 [00:00<00:26,  9.06it/s]Epoch: 4, train for the 12-th batch, train loss: 0.4449059069156647:   9%|█▏           | 11/119 [00:01<00:15,  6.96it/s]Epoch: 4, train for the 12-th batch, train loss: 0.4449059069156647:  10%|█▎           | 12/119 [00:01<00:15,  7.04it/s]Epoch: 2, train for the 114-th batch, train loss: 0.4005940854549408:  47%|█████▏     | 113/241 [00:17<00:22,  5.76it/s]Epoch: 2, train for the 114-th batch, train loss: 0.4005940854549408:  47%|█████▏     | 114/241 [00:17<00:21,  5.83it/s]evaluate for the 8-th batch, evaluate loss: 0.7257017493247986:  24%|████▊               | 6/25 [00:00<00:01, 10.43it/s]evaluate for the 8-th batch, evaluate loss: 0.7257017493247986:  32%|██████▍             | 8/25 [00:00<00:01, 10.67it/s]Epoch: 3, train for the 107-th batch, train loss: 0.5323843359947205:  73%|███████▉   | 106/146 [00:15<00:06,  5.84it/s]Epoch: 3, train for the 107-th batch, train loss: 0.5323843359947205:  73%|████████   | 107/146 [00:15<00:06,  6.17it/s]Epoch: 2, train for the 2-th batch, train loss: 0.6401811242103577:   0%|               | 1/237 [00:00<00:26,  9.06it/s]Epoch: 2, train for the 2-th batch, train loss: 0.6401811242103577:   1%|▏              | 2/237 [00:00<00:28,  8.36it/s]Epoch: 4, train for the 13-th batch, train loss: 0.47263234853744507:  10%|█▏          | 12/119 [00:01<00:15,  7.04it/s]evaluate for the 9-th batch, evaluate loss: 0.7021897435188293:  32%|██████▍             | 8/25 [00:00<00:01, 10.67it/s]Epoch: 4, train for the 13-th batch, train loss: 0.47263234853744507:  11%|█▎          | 13/119 [00:01<00:15,  6.99it/s]Epoch: 1, train for the 287-th batch, train loss: 0.41712504625320435:  75%|███████▍  | 286/383 [01:19<00:27,  3.58it/s]Epoch: 2, train for the 115-th batch, train loss: 0.42298343777656555:  47%|████▋     | 114/241 [00:17<00:21,  5.83it/s]Epoch: 3, train for the 108-th batch, train loss: 0.5369417667388916:  73%|████████   | 107/146 [00:15<00:06,  6.17it/s]Epoch: 2, train for the 115-th batch, train loss: 0.42298343777656555:  48%|████▊     | 115/241 [00:17<00:21,  5.88it/s]Epoch: 2, train for the 3-th batch, train loss: 0.6119080185890198:   1%|▏              | 2/237 [00:00<00:28,  8.36it/s]Epoch: 3, train for the 108-th batch, train loss: 0.5369417667388916:  74%|████████▏  | 108/146 [00:15<00:05,  6.37it/s]Epoch: 2, train for the 3-th batch, train loss: 0.6119080185890198:   1%|▏              | 3/237 [00:00<00:29,  8.05it/s]evaluate for the 10-th batch, evaluate loss: 0.7599585652351379:  32%|██████             | 8/25 [00:00<00:01, 10.67it/s]evaluate for the 10-th batch, evaluate loss: 0.7599585652351379:  40%|███████▏          | 10/25 [00:00<00:01, 10.88it/s]Epoch: 1, train for the 287-th batch, train loss: 0.41712504625320435:  75%|███████▍  | 287/383 [01:19<00:26,  3.59it/s]Epoch: 4, train for the 14-th batch, train loss: 0.48227161169052124:  11%|█▎          | 13/119 [00:02<00:15,  6.99it/s]Epoch: 4, train for the 14-th batch, train loss: 0.48227161169052124:  12%|█▍          | 14/119 [00:02<00:14,  7.09it/s]Epoch: 2, train for the 4-th batch, train loss: 0.5518508553504944:   1%|▏              | 3/237 [00:00<00:29,  8.05it/s]evaluate for the 11-th batch, evaluate loss: 0.741755485534668:  40%|███████▌           | 10/25 [00:01<00:01, 10.88it/s]Epoch: 2, train for the 4-th batch, train loss: 0.5518508553504944:   2%|▎              | 4/237 [00:00<00:29,  8.01it/s]Epoch: 2, train for the 116-th batch, train loss: 0.519000768661499:  48%|█████▋      | 115/241 [00:17<00:21,  5.88it/s]Epoch: 3, train for the 109-th batch, train loss: 0.5190967917442322:  74%|████████▏  | 108/146 [00:15<00:05,  6.37it/s]Epoch: 2, train for the 116-th batch, train loss: 0.519000768661499:  48%|█████▊      | 116/241 [00:17<00:21,  5.94it/s]Epoch: 3, train for the 109-th batch, train loss: 0.5190967917442322:  75%|████████▏  | 109/146 [00:15<00:05,  6.20it/s]Epoch: 4, train for the 15-th batch, train loss: 0.5043811202049255:  12%|█▌           | 14/119 [00:02<00:14,  7.09it/s]Epoch: 4, train for the 15-th batch, train loss: 0.5043811202049255:  13%|█▋           | 15/119 [00:02<00:14,  7.32it/s]evaluate for the 12-th batch, evaluate loss: 0.6530053615570068:  40%|███████▏          | 10/25 [00:01<00:01, 10.88it/s]evaluate for the 12-th batch, evaluate loss: 0.6530053615570068:  48%|████████▋         | 12/25 [00:01<00:01, 10.12it/s]Epoch: 2, train for the 5-th batch, train loss: 0.6180022954940796:   2%|▎              | 4/237 [00:00<00:29,  8.01it/s]Epoch: 2, train for the 5-th batch, train loss: 0.6180022954940796:   2%|▎              | 5/237 [00:00<00:33,  6.98it/s]Epoch: 2, train for the 117-th batch, train loss: 0.6117357015609741:  48%|█████▎     | 116/241 [00:17<00:21,  5.94it/s]Epoch: 1, train for the 288-th batch, train loss: 0.47171714901924133:  75%|███████▍  | 287/383 [01:19<00:26,  3.59it/s]evaluate for the 13-th batch, evaluate loss: 0.6679919958114624:  48%|████████▋         | 12/25 [00:01<00:01, 10.12it/s]Epoch: 2, train for the 117-th batch, train loss: 0.6117357015609741:  49%|█████▎     | 117/241 [00:17<00:21,  5.80it/s]Epoch: 3, train for the 110-th batch, train loss: 0.46243947744369507:  75%|███████▍  | 109/146 [00:15<00:05,  6.20it/s]Epoch: 4, train for the 16-th batch, train loss: 0.4869731664657593:  13%|█▋           | 15/119 [00:02<00:14,  7.32it/s]Epoch: 3, train for the 110-th batch, train loss: 0.46243947744369507:  75%|███████▌  | 110/146 [00:15<00:06,  5.96it/s]Epoch: 4, train for the 16-th batch, train loss: 0.4869731664657593:  13%|█▋           | 16/119 [00:02<00:15,  6.80it/s]Epoch: 1, train for the 288-th batch, train loss: 0.47171714901924133:  75%|███████▌  | 288/383 [01:19<00:28,  3.28it/s]Epoch: 2, train for the 6-th batch, train loss: 0.6311212182044983:   2%|▎              | 5/237 [00:00<00:33,  6.98it/s]evaluate for the 14-th batch, evaluate loss: 0.7068435549736023:  48%|████████▋         | 12/25 [00:01<00:01, 10.12it/s]evaluate for the 14-th batch, evaluate loss: 0.7068435549736023:  56%|██████████        | 14/25 [00:01<00:01, 10.20it/s]Epoch: 2, train for the 118-th batch, train loss: 0.4906315505504608:  49%|█████▎     | 117/241 [00:17<00:21,  5.80it/s]evaluate for the 15-th batch, evaluate loss: 0.7424377799034119:  56%|██████████        | 14/25 [00:01<00:01, 10.20it/s]Epoch: 2, train for the 118-th batch, train loss: 0.4906315505504608:  49%|█████▍     | 118/241 [00:17<00:21,  5.74it/s]Epoch: 4, train for the 17-th batch, train loss: 0.48388469219207764:  13%|█▌          | 16/119 [00:02<00:15,  6.80it/s]Epoch: 2, train for the 7-th batch, train loss: 0.6190931797027588:   2%|▎              | 5/237 [00:00<00:33,  6.98it/s]Epoch: 3, train for the 111-th batch, train loss: 0.5171053409576416:  75%|████████▎  | 110/146 [00:15<00:06,  5.96it/s]Epoch: 4, train for the 17-th batch, train loss: 0.48388469219207764:  14%|█▋          | 17/119 [00:02<00:16,  6.23it/s]Epoch: 2, train for the 7-th batch, train loss: 0.6190931797027588:   3%|▍              | 7/237 [00:00<00:30,  7.49it/s]Epoch: 3, train for the 111-th batch, train loss: 0.5171053409576416:  76%|████████▎  | 111/146 [00:15<00:06,  5.60it/s]evaluate for the 16-th batch, evaluate loss: 0.6656367182731628:  56%|██████████        | 14/25 [00:01<00:01, 10.20it/s]evaluate for the 16-th batch, evaluate loss: 0.6656367182731628:  64%|███████████▌      | 16/25 [00:01<00:00, 10.71it/s]Epoch: 1, train for the 289-th batch, train loss: 0.46312299370765686:  75%|███████▌  | 288/383 [01:19<00:28,  3.28it/s]Epoch: 4, train for the 18-th batch, train loss: 0.45835447311401367:  14%|█▋          | 17/119 [00:02<00:16,  6.23it/s]Epoch: 4, train for the 18-th batch, train loss: 0.45835447311401367:  15%|█▊          | 18/119 [00:02<00:15,  6.54it/s]Epoch: 2, train for the 8-th batch, train loss: 0.5961912274360657:   3%|▍              | 7/237 [00:01<00:30,  7.49it/s]Epoch: 2, train for the 8-th batch, train loss: 0.5961912274360657:   3%|▌              | 8/237 [00:01<00:31,  7.29it/s]Epoch: 2, train for the 119-th batch, train loss: 0.35419997572898865:  49%|████▉     | 118/241 [00:18<00:21,  5.74it/s]evaluate for the 17-th batch, evaluate loss: 0.6570833325386047:  64%|███████████▌      | 16/25 [00:01<00:00, 10.71it/s]Epoch: 1, train for the 289-th batch, train loss: 0.46312299370765686:  75%|███████▌  | 289/383 [01:19<00:29,  3.19it/s]Epoch: 2, train for the 119-th batch, train loss: 0.35419997572898865:  49%|████▉     | 119/241 [00:18<00:22,  5.43it/s]Epoch: 3, train for the 112-th batch, train loss: 0.5639323592185974:  76%|████████▎  | 111/146 [00:16<00:06,  5.60it/s]Epoch: 3, train for the 112-th batch, train loss: 0.5639323592185974:  77%|████████▍  | 112/146 [00:16<00:06,  5.56it/s]Epoch: 4, train for the 19-th batch, train loss: 0.4554925858974457:  15%|█▉           | 18/119 [00:02<00:15,  6.54it/s]Epoch: 2, train for the 9-th batch, train loss: 0.5883664488792419:   3%|▌              | 8/237 [00:01<00:31,  7.29it/s]Epoch: 4, train for the 19-th batch, train loss: 0.4554925858974457:  16%|██           | 19/119 [00:02<00:14,  6.86it/s]Epoch: 2, train for the 9-th batch, train loss: 0.5883664488792419:   4%|▌              | 9/237 [00:01<00:29,  7.61it/s]evaluate for the 18-th batch, evaluate loss: 0.6338362693786621:  64%|███████████▌      | 16/25 [00:01<00:00, 10.71it/s]evaluate for the 18-th batch, evaluate loss: 0.6338362693786621:  72%|████████████▉     | 18/25 [00:01<00:00,  9.92it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5913216471672058:  49%|█████▍     | 119/241 [00:18<00:22,  5.43it/s]Epoch: 3, train for the 113-th batch, train loss: 0.4779597222805023:  77%|████████▍  | 112/146 [00:16<00:06,  5.56it/s]Epoch: 3, train for the 113-th batch, train loss: 0.4779597222805023:  77%|████████▌  | 113/146 [00:16<00:05,  5.79it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5913216471672058:  50%|█████▍     | 120/241 [00:18<00:21,  5.54it/s]Epoch: 4, train for the 20-th batch, train loss: 0.4168764352798462:  16%|██           | 19/119 [00:02<00:14,  6.86it/s]Epoch: 4, train for the 20-th batch, train loss: 0.4168764352798462:  17%|██▏          | 20/119 [00:02<00:13,  7.40it/s]evaluate for the 19-th batch, evaluate loss: 0.6188982129096985:  72%|████████████▉     | 18/25 [00:01<00:00,  9.92it/s]Epoch: 2, train for the 10-th batch, train loss: 0.6234917044639587:   4%|▌             | 9/237 [00:01<00:29,  7.61it/s]Epoch: 2, train for the 10-th batch, train loss: 0.6234917044639587:   4%|▌            | 10/237 [00:01<00:30,  7.43it/s]Epoch: 3, train for the 114-th batch, train loss: 0.5222189426422119:  77%|████████▌  | 113/146 [00:16<00:05,  5.79it/s]evaluate for the 20-th batch, evaluate loss: 0.6741529107093811:  72%|████████████▉     | 18/25 [00:01<00:00,  9.92it/s]evaluate for the 20-th batch, evaluate loss: 0.6741529107093811:  80%|██████████████▍   | 20/25 [00:01<00:00,  9.74it/s]Epoch: 3, train for the 114-th batch, train loss: 0.5222189426422119:  78%|████████▌  | 114/146 [00:16<00:05,  6.09it/s]Epoch: 4, train for the 21-th batch, train loss: 0.4953645169734955:  17%|██▏          | 20/119 [00:03<00:13,  7.40it/s]Epoch: 2, train for the 11-th batch, train loss: 0.6464329361915588:   4%|▌            | 10/237 [00:01<00:30,  7.43it/s]Epoch: 2, train for the 121-th batch, train loss: 0.5466699004173279:  50%|█████▍     | 120/241 [00:18<00:21,  5.54it/s]Epoch: 4, train for the 21-th batch, train loss: 0.4953645169734955:  18%|██▎          | 21/119 [00:03<00:13,  7.32it/s]Epoch: 2, train for the 11-th batch, train loss: 0.6464329361915588:   5%|▌            | 11/237 [00:01<00:28,  7.88it/s]Epoch: 2, train for the 121-th batch, train loss: 0.5466699004173279:  50%|█████▌     | 121/241 [00:18<00:21,  5.52it/s]Epoch: 1, train for the 290-th batch, train loss: 0.44636276364326477:  75%|███████▌  | 289/383 [01:20<00:29,  3.19it/s]evaluate for the 21-th batch, evaluate loss: 0.6863308548927307:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.74it/s]Epoch: 2, train for the 12-th batch, train loss: 0.6402501463890076:   5%|▌            | 11/237 [00:01<00:28,  7.88it/s]Epoch: 1, train for the 290-th batch, train loss: 0.44636276364326477:  76%|███████▌  | 290/383 [01:20<00:33,  2.78it/s]Epoch: 2, train for the 12-th batch, train loss: 0.6402501463890076:   5%|▋            | 12/237 [00:01<00:28,  7.92it/s]Epoch: 3, train for the 115-th batch, train loss: 0.5218079686164856:  78%|████████▌  | 114/146 [00:16<00:05,  6.09it/s]Epoch: 3, train for the 115-th batch, train loss: 0.5218079686164856:  79%|████████▋  | 115/146 [00:16<00:05,  6.07it/s]Epoch: 4, train for the 22-th batch, train loss: 0.5275049209594727:  18%|██▎          | 21/119 [00:03<00:13,  7.32it/s]Epoch: 4, train for the 22-th batch, train loss: 0.5275049209594727:  18%|██▍          | 22/119 [00:03<00:13,  6.98it/s]evaluate for the 22-th batch, evaluate loss: 0.6394984126091003:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.74it/s]evaluate for the 22-th batch, evaluate loss: 0.6394984126091003:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.85it/s]Epoch: 2, train for the 122-th batch, train loss: 0.4457295536994934:  50%|█████▌     | 121/241 [00:18<00:21,  5.52it/s]Epoch: 2, train for the 122-th batch, train loss: 0.4457295536994934:  51%|█████▌     | 122/241 [00:18<00:21,  5.53it/s]Epoch: 2, train for the 13-th batch, train loss: 0.6483268737792969:   5%|▋            | 12/237 [00:01<00:28,  7.92it/s]evaluate for the 23-th batch, evaluate loss: 0.647540271282196:  88%|████████████████▋  | 22/25 [00:02<00:00,  9.85it/s]Epoch: 2, train for the 13-th batch, train loss: 0.6483268737792969:   5%|▋            | 13/237 [00:01<00:28,  7.79it/s]Epoch: 3, train for the 116-th batch, train loss: 0.5216354131698608:  79%|████████▋  | 115/146 [00:16<00:05,  6.07it/s]Epoch: 3, train for the 116-th batch, train loss: 0.5216354131698608:  79%|████████▋  | 116/146 [00:16<00:04,  6.19it/s]Epoch: 4, train for the 23-th batch, train loss: 0.49633651971817017:  18%|██▏         | 22/119 [00:03<00:13,  6.98it/s]Epoch: 4, train for the 23-th batch, train loss: 0.49633651971817017:  19%|██▎         | 23/119 [00:03<00:14,  6.64it/s]Epoch: 2, train for the 123-th batch, train loss: 0.6822340488433838:  51%|█████▌     | 122/241 [00:18<00:21,  5.53it/s]evaluate for the 24-th batch, evaluate loss: 0.6682232022285461:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.85it/s]evaluate for the 24-th batch, evaluate loss: 0.6682232022285461:  96%|█████████████████▎| 24/25 [00:02<00:00, 10.04it/s]Epoch: 2, train for the 14-th batch, train loss: 0.6730731725692749:   5%|▋            | 13/237 [00:01<00:28,  7.79it/s]Epoch: 2, train for the 123-th batch, train loss: 0.6822340488433838:  51%|█████▌     | 123/241 [00:18<00:21,  5.60it/s]Epoch: 2, train for the 14-th batch, train loss: 0.6730731725692749:   6%|▊            | 14/237 [00:01<00:27,  8.15it/s]Epoch: 3, train for the 117-th batch, train loss: 0.5373309254646301:  79%|████████▋  | 116/146 [00:16<00:04,  6.19it/s]Epoch: 3, train for the 117-th batch, train loss: 0.5373309254646301:  80%|████████▊  | 117/146 [00:16<00:04,  6.34it/s]evaluate for the 25-th batch, evaluate loss: 0.6337143778800964:  96%|█████████████████▎| 24/25 [00:02<00:00, 10.04it/s]evaluate for the 25-th batch, evaluate loss: 0.6337143778800964: 100%|██████████████████| 25/25 [00:02<00:00, 10.20it/s]
Epoch: 4, train for the 24-th batch, train loss: 0.46320781111717224:  19%|██▎         | 23/119 [00:03<00:14,  6.64it/s]Epoch: 4, train for the 24-th batch, train loss: 0.46320781111717224:  20%|██▍         | 24/119 [00:03<00:13,  6.80it/s]Epoch: 2, train for the 15-th batch, train loss: 0.7132546901702881:   6%|▊            | 14/237 [00:01<00:27,  8.15it/s]Epoch: 2, train for the 15-th batch, train loss: 0.7132546901702881:   6%|▊            | 15/237 [00:01<00:27,  8.09it/s]Epoch: 1, train for the 291-th batch, train loss: 0.35345977544784546:  76%|███████▌  | 290/383 [01:20<00:33,  2.78it/s]Epoch: 2, train for the 124-th batch, train loss: 0.3843042552471161:  51%|█████▌     | 123/241 [00:18<00:21,  5.60it/s]Epoch: 2, train for the 124-th batch, train loss: 0.3843042552471161:  51%|█████▋     | 124/241 [00:18<00:20,  5.74it/s]Epoch: 1, train for the 291-th batch, train loss: 0.35345977544784546:  76%|███████▌  | 291/383 [01:20<00:35,  2.61it/s]Epoch: 3, train for the 118-th batch, train loss: 0.50376957654953:  80%|██████████▍  | 117/146 [00:17<00:04,  6.34it/s]Epoch: 3, train for the 118-th batch, train loss: 0.50376957654953:  81%|██████████▌  | 118/146 [00:17<00:04,  6.57it/s]Epoch: 2, train for the 16-th batch, train loss: 0.6354606747627258:   6%|▊            | 15/237 [00:02<00:27,  8.09it/s]Epoch: 4, train for the 25-th batch, train loss: 0.4545454680919647:  20%|██▌          | 24/119 [00:03<00:13,  6.80it/s]Epoch: 4, train for the 25-th batch, train loss: 0.4545454680919647:  21%|██▋          | 25/119 [00:03<00:13,  6.81it/s]Epoch: 2, train for the 125-th batch, train loss: 0.49113568663597107:  51%|█████▏    | 124/241 [00:19<00:20,  5.74it/s]Epoch: 2, train for the 125-th batch, train loss: 0.49113568663597107:  52%|█████▏    | 125/241 [00:19<00:19,  6.01it/s]Epoch: 3, train for the 119-th batch, train loss: 0.5105953812599182:  81%|████████▉  | 118/146 [00:17<00:04,  6.57it/s]Epoch: 3, train for the 119-th batch, train loss: 0.5105953812599182:  82%|████████▉  | 119/146 [00:17<00:03,  6.86it/s]Epoch: 2, train for the 17-th batch, train loss: 0.6183775663375854:   6%|▊            | 15/237 [00:02<00:27,  8.09it/s]Epoch: 4, train for the 26-th batch, train loss: 0.43561574816703796:  21%|██▌         | 25/119 [00:03<00:13,  6.81it/s]Epoch: 2, train for the 17-th batch, train loss: 0.6183775663375854:   7%|▉            | 17/237 [00:02<00:27,  8.09it/s]Epoch: 4, train for the 26-th batch, train loss: 0.43561574816703796:  22%|██▌         | 26/119 [00:03<00:13,  6.97it/s]Epoch: 2, train for the 126-th batch, train loss: 0.4334717392921448:  52%|█████▋     | 125/241 [00:19<00:19,  6.01it/s]Epoch: 2, train for the 126-th batch, train loss: 0.4334717392921448:  52%|█████▊     | 126/241 [00:19<00:16,  6.79it/s]Epoch: 1, train for the 292-th batch, train loss: 0.47808825969696045:  76%|███████▌  | 291/383 [01:21<00:35,  2.61it/s]Epoch: 3, train for the 120-th batch, train loss: 0.4875258803367615:  82%|████████▉  | 119/146 [00:17<00:03,  6.86it/s]Epoch: 3, train for the 120-th batch, train loss: 0.4875258803367615:  82%|█████████  | 120/146 [00:17<00:03,  7.16it/s]INFO:root:Epoch: 2, learning rate: 0.0001, train loss: 0.5688
INFO:root:train average_precision, 0.7681
INFO:root:train roc_auc, 0.7448
INFO:root:validate loss: 0.5148
INFO:root:validate average_precision, 0.8380
INFO:root:validate roc_auc, 0.8330
INFO:root:new node validate loss: 0.6958
INFO:root:new node validate first_1_average_precision, 0.5545
INFO:root:new node validate first_1_roc_auc, 0.5621
INFO:root:new node validate first_3_average_precision, 0.5908
INFO:root:new node validate first_3_roc_auc, 0.5948
INFO:root:new node validate first_10_average_precision, 0.6438
INFO:root:new node validate first_10_roc_auc, 0.6495
INFO:root:new node validate average_precision, 0.6582
INFO:root:new node validate roc_auc, 0.6584
Epoch: 1, train for the 292-th batch, train loss: 0.47808825969696045:  76%|███████▌  | 292/383 [01:21<00:32,  2.81it/s]Epoch: 2, train for the 18-th batch, train loss: 0.6362608671188354:   7%|▉            | 17/237 [00:02<00:27,  8.09it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]Epoch: 4, train for the 27-th batch, train loss: 0.5271111130714417:  22%|██▊          | 26/119 [00:03<00:13,  6.97it/s]Epoch: 2, train for the 18-th batch, train loss: 0.6362608671188354:   8%|▉            | 18/237 [00:02<00:27,  7.92it/s]Epoch: 4, train for the 27-th batch, train loss: 0.5271111130714417:  23%|██▉          | 27/119 [00:03<00:13,  7.07it/s]Epoch: 2, train for the 127-th batch, train loss: 0.5103904604911804:  52%|█████▊     | 126/241 [00:19<00:16,  6.79it/s]Epoch: 2, train for the 127-th batch, train loss: 0.5103904604911804:  53%|█████▊     | 127/241 [00:19<00:16,  6.75it/s]Epoch: 3, train for the 121-th batch, train loss: 0.4962882995605469:  82%|█████████  | 120/146 [00:17<00:03,  7.16it/s]Epoch: 3, train for the 121-th batch, train loss: 0.4962882995605469:  83%|█████████  | 121/146 [00:17<00:03,  7.42it/s]Epoch: 3, train for the 1-th batch, train loss: 0.9261704683303833:   0%|                       | 0/151 [00:00<?, ?it/s]Epoch: 3, train for the 1-th batch, train loss: 0.9261704683303833:   1%|               | 1/151 [00:00<00:16,  9.06it/s]Epoch: 4, train for the 28-th batch, train loss: 0.5003051161766052:  23%|██▉          | 27/119 [00:04<00:13,  7.07it/s]Epoch: 2, train for the 19-th batch, train loss: 0.6161009073257446:   8%|▉            | 18/237 [00:02<00:27,  7.92it/s]Epoch: 4, train for the 28-th batch, train loss: 0.5003051161766052:  24%|███          | 28/119 [00:04<00:13,  6.88it/s]Epoch: 2, train for the 19-th batch, train loss: 0.6161009073257446:   8%|█            | 19/237 [00:02<00:29,  7.28it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5321503281593323:  53%|█████▊     | 127/241 [00:19<00:16,  6.75it/s]Epoch: 3, train for the 122-th batch, train loss: 0.5467263460159302:  83%|█████████  | 121/146 [00:17<00:03,  7.42it/s]Epoch: 3, train for the 122-th batch, train loss: 0.5467263460159302:  84%|█████████▏ | 122/146 [00:17<00:03,  7.20it/s]Epoch: 3, train for the 2-th batch, train loss: 0.8888307213783264:   1%|               | 1/151 [00:00<00:16,  9.06it/s]Epoch: 3, train for the 2-th batch, train loss: 0.8888307213783264:   1%|▏              | 2/151 [00:00<00:19,  7.83it/s]Epoch: 1, train for the 293-th batch, train loss: 0.45766347646713257:  76%|███████▌  | 292/383 [01:21<00:32,  2.81it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5321503281593323:  53%|█████▊     | 128/241 [00:19<00:18,  6.10it/s]Epoch: 4, train for the 29-th batch, train loss: 0.45320990681648254:  24%|██▊         | 28/119 [00:04<00:13,  6.88it/s]Epoch: 2, train for the 20-th batch, train loss: 0.6606379747390747:   8%|█            | 19/237 [00:02<00:29,  7.28it/s]Epoch: 4, train for the 29-th batch, train loss: 0.45320990681648254:  24%|██▉         | 29/119 [00:04<00:12,  7.04it/s]Epoch: 2, train for the 20-th batch, train loss: 0.6606379747390747:   8%|█            | 20/237 [00:02<00:29,  7.32it/s]Epoch: 1, train for the 293-th batch, train loss: 0.45766347646713257:  77%|███████▋  | 293/383 [01:21<00:31,  2.84it/s]Epoch: 3, train for the 3-th batch, train loss: 0.8444020748138428:   1%|▏              | 2/151 [00:00<00:19,  7.83it/s]Epoch: 3, train for the 123-th batch, train loss: 0.535309374332428:  84%|██████████  | 122/146 [00:17<00:03,  7.20it/s]Epoch: 3, train for the 3-th batch, train loss: 0.8444020748138428:   2%|▎              | 3/151 [00:00<00:18,  8.00it/s]Epoch: 3, train for the 123-th batch, train loss: 0.535309374332428:  84%|██████████  | 123/146 [00:17<00:03,  7.23it/s]Epoch: 2, train for the 129-th batch, train loss: 0.45108795166015625:  53%|█████▎    | 128/241 [00:19<00:18,  6.10it/s]Epoch: 4, train for the 30-th batch, train loss: 0.41416046023368835:  24%|██▉         | 29/119 [00:04<00:12,  7.04it/s]Epoch: 4, train for the 30-th batch, train loss: 0.41416046023368835:  25%|███         | 30/119 [00:04<00:12,  7.39it/s]Epoch: 2, train for the 21-th batch, train loss: 0.6154043674468994:   8%|█            | 20/237 [00:02<00:29,  7.32it/s]Epoch: 2, train for the 129-th batch, train loss: 0.45108795166015625:  54%|█████▎    | 129/241 [00:19<00:18,  6.07it/s]Epoch: 2, train for the 21-th batch, train loss: 0.6154043674468994:   9%|█▏           | 21/237 [00:02<00:28,  7.48it/s]Epoch: 3, train for the 4-th batch, train loss: 0.7658795118331909:   2%|▎              | 3/151 [00:00<00:18,  8.00it/s]Epoch: 3, train for the 4-th batch, train loss: 0.7658795118331909:   3%|▍              | 4/151 [00:00<00:18,  7.77it/s]Epoch: 3, train for the 124-th batch, train loss: 0.5289469361305237:  84%|█████████▎ | 123/146 [00:17<00:03,  7.23it/s]Epoch: 3, train for the 124-th batch, train loss: 0.5289469361305237:  85%|█████████▎ | 124/146 [00:17<00:03,  7.13it/s]Epoch: 4, train for the 31-th batch, train loss: 0.49748265743255615:  25%|███         | 30/119 [00:04<00:12,  7.39it/s]Epoch: 4, train for the 31-th batch, train loss: 0.49748265743255615:  26%|███▏        | 31/119 [00:04<00:12,  7.20it/s]Epoch: 2, train for the 130-th batch, train loss: 0.5002803206443787:  54%|█████▉     | 129/241 [00:19<00:18,  6.07it/s]Epoch: 2, train for the 130-th batch, train loss: 0.5002803206443787:  54%|█████▉     | 130/241 [00:19<00:18,  6.17it/s]Epoch: 1, train for the 294-th batch, train loss: 0.2992326319217682:  77%|████████▍  | 293/383 [01:21<00:31,  2.84it/s]Epoch: 3, train for the 5-th batch, train loss: 0.7399771809577942:   3%|▍              | 4/151 [00:00<00:18,  7.77it/s]Epoch: 3, train for the 5-th batch, train loss: 0.7399771809577942:   3%|▍              | 5/151 [00:00<00:17,  8.33it/s]Epoch: 2, train for the 22-th batch, train loss: 0.6097609400749207:   9%|█▏           | 21/237 [00:02<00:28,  7.48it/s]Epoch: 2, train for the 22-th batch, train loss: 0.6097609400749207:   9%|█▏           | 22/237 [00:02<00:33,  6.50it/s]Epoch: 3, train for the 125-th batch, train loss: 0.49835968017578125:  85%|████████▍ | 124/146 [00:17<00:03,  7.13it/s]Epoch: 1, train for the 294-th batch, train loss: 0.2992326319217682:  77%|████████▍  | 294/383 [01:21<00:30,  2.89it/s]Epoch: 4, train for the 32-th batch, train loss: 0.451835960149765:  26%|███▋          | 31/119 [00:04<00:12,  7.20it/s]Epoch: 3, train for the 125-th batch, train loss: 0.49835968017578125:  86%|████████▌ | 125/146 [00:18<00:03,  6.91it/s]Epoch: 4, train for the 32-th batch, train loss: 0.451835960149765:  27%|███▊          | 32/119 [00:04<00:11,  7.62it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5253267288208008:  54%|█████▉     | 130/241 [00:19<00:18,  6.17it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5253267288208008:  54%|█████▉     | 131/241 [00:19<00:17,  6.29it/s]Epoch: 3, train for the 6-th batch, train loss: 0.7124764919281006:   3%|▍              | 5/151 [00:00<00:17,  8.33it/s]Epoch: 3, train for the 6-th batch, train loss: 0.7124764919281006:   4%|▌              | 6/151 [00:00<00:18,  7.78it/s]Epoch: 4, train for the 33-th batch, train loss: 0.4814692437648773:  27%|███▍         | 32/119 [00:04<00:11,  7.62it/s]Epoch: 4, train for the 33-th batch, train loss: 0.4814692437648773:  28%|███▌         | 33/119 [00:04<00:11,  7.38it/s]Epoch: 3, train for the 126-th batch, train loss: 0.5655356645584106:  86%|█████████▍ | 125/146 [00:18<00:03,  6.91it/s]Epoch: 3, train for the 126-th batch, train loss: 0.5655356645584106:  86%|█████████▍ | 126/146 [00:18<00:03,  6.30it/s]Epoch: 2, train for the 132-th batch, train loss: 0.4940745532512665:  54%|█████▉     | 131/241 [00:20<00:17,  6.29it/s]Epoch: 2, train for the 132-th batch, train loss: 0.4940745532512665:  55%|██████     | 132/241 [00:20<00:17,  6.35it/s]Epoch: 3, train for the 7-th batch, train loss: 0.6679197549819946:   4%|▌              | 6/151 [00:00<00:18,  7.78it/s]Epoch: 3, train for the 7-th batch, train loss: 0.6679197549819946:   5%|▋              | 7/151 [00:00<00:19,  7.21it/s]Epoch: 2, train for the 23-th batch, train loss: 0.5998514890670776:   9%|█▏           | 22/237 [00:03<00:33,  6.50it/s]Epoch: 4, train for the 34-th batch, train loss: 0.44182249903678894:  28%|███▎        | 33/119 [00:04<00:11,  7.38it/s]Epoch: 2, train for the 23-th batch, train loss: 0.5998514890670776:  10%|█▎           | 23/237 [00:03<00:41,  5.18it/s]Epoch: 4, train for the 34-th batch, train loss: 0.44182249903678894:  29%|███▍        | 34/119 [00:04<00:10,  7.91it/s]Epoch: 3, train for the 127-th batch, train loss: 0.519902765750885:  86%|██████████▎ | 126/146 [00:18<00:03,  6.30it/s]Epoch: 3, train for the 127-th batch, train loss: 0.519902765750885:  87%|██████████▍ | 127/146 [00:18<00:02,  6.64it/s]Epoch: 1, train for the 295-th batch, train loss: 0.3560599088668823:  77%|████████▍  | 294/383 [01:22<00:30,  2.89it/s]Epoch: 2, train for the 133-th batch, train loss: 0.41042792797088623:  55%|█████▍    | 132/241 [00:20<00:17,  6.35it/s]Epoch: 4, train for the 35-th batch, train loss: 0.45105278491973877:  29%|███▍        | 34/119 [00:04<00:10,  7.91it/s]Epoch: 4, train for the 35-th batch, train loss: 0.45105278491973877:  29%|███▌        | 35/119 [00:04<00:10,  7.90it/s]Epoch: 2, train for the 133-th batch, train loss: 0.41042792797088623:  55%|█████▌    | 133/241 [00:20<00:17,  6.17it/s]Epoch: 3, train for the 8-th batch, train loss: 0.6886253952980042:   5%|▋              | 7/151 [00:01<00:19,  7.21it/s]Epoch: 2, train for the 24-th batch, train loss: 0.5673229694366455:  10%|█▎           | 23/237 [00:03<00:41,  5.18it/s]Epoch: 3, train for the 8-th batch, train loss: 0.6886253952980042:   5%|▊              | 8/151 [00:01<00:21,  6.80it/s]Epoch: 2, train for the 24-th batch, train loss: 0.5673229694366455:  10%|█▎           | 24/237 [00:03<00:39,  5.37it/s]Epoch: 1, train for the 295-th batch, train loss: 0.3560599088668823:  77%|████████▍  | 295/383 [01:22<00:33,  2.64it/s]Epoch: 3, train for the 128-th batch, train loss: 0.5172062516212463:  87%|█████████▌ | 127/146 [00:18<00:02,  6.64it/s]Epoch: 3, train for the 128-th batch, train loss: 0.5172062516212463:  88%|█████████▋ | 128/146 [00:18<00:02,  6.85it/s]Epoch: 4, train for the 36-th batch, train loss: 0.42462339997291565:  29%|███▌        | 35/119 [00:05<00:10,  7.90it/s]Epoch: 4, train for the 36-th batch, train loss: 0.42462339997291565:  30%|███▋        | 36/119 [00:05<00:10,  7.66it/s]Epoch: 2, train for the 134-th batch, train loss: 0.47607579827308655:  55%|█████▌    | 133/241 [00:20<00:17,  6.17it/s]Epoch: 3, train for the 9-th batch, train loss: 0.6818659901618958:   5%|▊              | 8/151 [00:01<00:21,  6.80it/s]Epoch: 3, train for the 9-th batch, train loss: 0.6818659901618958:   6%|▉              | 9/151 [00:01<00:21,  6.49it/s]Epoch: 2, train for the 134-th batch, train loss: 0.47607579827308655:  56%|█████▌    | 134/241 [00:20<00:18,  5.90it/s]Epoch: 3, train for the 129-th batch, train loss: 0.5288314819335938:  88%|█████████▋ | 128/146 [00:18<00:02,  6.85it/s]Epoch: 2, train for the 25-th batch, train loss: 0.580605685710907:  10%|█▍            | 24/237 [00:03<00:39,  5.37it/s]Epoch: 3, train for the 129-th batch, train loss: 0.5288314819335938:  88%|█████████▋ | 129/146 [00:18<00:02,  7.04it/s]Epoch: 2, train for the 25-th batch, train loss: 0.580605685710907:  11%|█▍            | 25/237 [00:03<00:38,  5.45it/s]Epoch: 4, train for the 37-th batch, train loss: 0.47747474908828735:  30%|███▋        | 36/119 [00:05<00:10,  7.66it/s]Epoch: 4, train for the 37-th batch, train loss: 0.47747474908828735:  31%|███▋        | 37/119 [00:05<00:10,  7.78it/s]Epoch: 1, train for the 296-th batch, train loss: 0.2984262704849243:  77%|████████▍  | 295/383 [01:22<00:33,  2.64it/s]Epoch: 1, train for the 296-th batch, train loss: 0.2984262704849243:  77%|████████▌  | 296/383 [01:22<00:30,  2.89it/s]Epoch: 3, train for the 130-th batch, train loss: 0.45874273777008057:  88%|████████▊ | 129/146 [00:18<00:02,  7.04it/s]Epoch: 2, train for the 26-th batch, train loss: 0.5587019920349121:  11%|█▎           | 25/237 [00:03<00:38,  5.45it/s]Epoch: 3, train for the 130-th batch, train loss: 0.45874273777008057:  89%|████████▉ | 130/146 [00:18<00:02,  7.02it/s]Epoch: 2, train for the 26-th batch, train loss: 0.5587019920349121:  11%|█▍           | 26/237 [00:03<00:35,  5.90it/s]Epoch: 2, train for the 135-th batch, train loss: 0.4011287987232208:  56%|██████     | 134/241 [00:20<00:18,  5.90it/s]Epoch: 3, train for the 10-th batch, train loss: 0.6895861625671387:   6%|▊             | 9/151 [00:01<00:21,  6.49it/s]Epoch: 4, train for the 38-th batch, train loss: 0.43162989616394043:  31%|███▋        | 37/119 [00:05<00:10,  7.78it/s]Epoch: 3, train for the 10-th batch, train loss: 0.6895861625671387:   7%|▊            | 10/151 [00:01<00:23,  6.05it/s]Epoch: 4, train for the 38-th batch, train loss: 0.43162989616394043:  32%|███▊        | 38/119 [00:05<00:10,  7.97it/s]Epoch: 2, train for the 135-th batch, train loss: 0.4011287987232208:  56%|██████▏    | 135/241 [00:20<00:19,  5.54it/s]Epoch: 2, train for the 27-th batch, train loss: 0.5826640129089355:  11%|█▍           | 26/237 [00:03<00:35,  5.90it/s]Epoch: 2, train for the 27-th batch, train loss: 0.5826640129089355:  11%|█▍           | 27/237 [00:03<00:33,  6.22it/s]Epoch: 3, train for the 131-th batch, train loss: 0.5448330044746399:  89%|█████████▊ | 130/146 [00:18<00:02,  7.02it/s]Epoch: 3, train for the 131-th batch, train loss: 0.5448330044746399:  90%|█████████▊ | 131/146 [00:18<00:02,  6.83it/s]Epoch: 4, train for the 39-th batch, train loss: 0.47538992762565613:  32%|███▊        | 38/119 [00:05<00:10,  7.97it/s]Epoch: 3, train for the 11-th batch, train loss: 0.7053595781326294:   7%|▊            | 10/151 [00:01<00:23,  6.05it/s]Epoch: 4, train for the 39-th batch, train loss: 0.47538992762565613:  33%|███▉        | 39/119 [00:05<00:10,  7.29it/s]Epoch: 3, train for the 11-th batch, train loss: 0.7053595781326294:   7%|▉            | 11/151 [00:01<00:23,  6.01it/s]Epoch: 1, train for the 297-th batch, train loss: 0.4975837469100952:  77%|████████▌  | 296/383 [01:22<00:30,  2.89it/s]Epoch: 2, train for the 136-th batch, train loss: 0.4478197693824768:  56%|██████▏    | 135/241 [00:20<00:19,  5.54it/s]Epoch: 2, train for the 136-th batch, train loss: 0.4478197693824768:  56%|██████▏    | 136/241 [00:20<00:19,  5.47it/s]Epoch: 2, train for the 28-th batch, train loss: 0.6547105312347412:  11%|█▍           | 27/237 [00:03<00:33,  6.22it/s]Epoch: 3, train for the 132-th batch, train loss: 0.49573859572410583:  90%|████████▉ | 131/146 [00:19<00:02,  6.83it/s]Epoch: 2, train for the 28-th batch, train loss: 0.6547105312347412:  12%|█▌           | 28/237 [00:03<00:31,  6.65it/s]Epoch: 1, train for the 297-th batch, train loss: 0.4975837469100952:  78%|████████▌  | 297/383 [01:22<00:28,  3.00it/s]Epoch: 3, train for the 132-th batch, train loss: 0.49573859572410583:  90%|█████████ | 132/146 [00:19<00:01,  7.20it/s]Epoch: 4, train for the 40-th batch, train loss: 0.4410010874271393:  33%|████▎        | 39/119 [00:05<00:10,  7.29it/s]Epoch: 4, train for the 40-th batch, train loss: 0.4410010874271393:  34%|████▎        | 40/119 [00:05<00:10,  7.28it/s]Epoch: 3, train for the 12-th batch, train loss: 0.7145780920982361:   7%|▉            | 11/151 [00:01<00:23,  6.01it/s]Epoch: 3, train for the 12-th batch, train loss: 0.7145780920982361:   8%|█            | 12/151 [00:01<00:22,  6.28it/s]Epoch: 2, train for the 137-th batch, train loss: 0.41658228635787964:  56%|█████▋    | 136/241 [00:21<00:19,  5.47it/s]Epoch: 2, train for the 29-th batch, train loss: 0.5868431925773621:  12%|█▌           | 28/237 [00:04<00:31,  6.65it/s]Epoch: 2, train for the 29-th batch, train loss: 0.5868431925773621:  12%|█▌           | 29/237 [00:04<00:29,  7.12it/s]Epoch: 2, train for the 137-th batch, train loss: 0.41658228635787964:  57%|█████▋    | 137/241 [00:21<00:18,  5.62it/s]Epoch: 3, train for the 133-th batch, train loss: 0.4761946499347687:  90%|█████████▉ | 132/146 [00:19<00:01,  7.20it/s]Epoch: 3, train for the 133-th batch, train loss: 0.4761946499347687:  91%|██████████ | 133/146 [00:19<00:01,  7.10it/s]Epoch: 3, train for the 13-th batch, train loss: 0.6969523429870605:   8%|█            | 12/151 [00:01<00:22,  6.28it/s]Epoch: 3, train for the 13-th batch, train loss: 0.6969523429870605:   9%|█            | 13/151 [00:01<00:20,  6.84it/s]Epoch: 4, train for the 41-th batch, train loss: 0.4955834448337555:  34%|████▎        | 40/119 [00:05<00:10,  7.28it/s]Epoch: 4, train for the 41-th batch, train loss: 0.4955834448337555:  34%|████▍        | 41/119 [00:05<00:10,  7.25it/s]Epoch: 2, train for the 138-th batch, train loss: 0.41160833835601807:  57%|█████▋    | 137/241 [00:21<00:18,  5.62it/s]Epoch: 3, train for the 134-th batch, train loss: 0.5055732727050781:  91%|██████████ | 133/146 [00:19<00:01,  7.10it/s]Epoch: 2, train for the 138-th batch, train loss: 0.41160833835601807:  57%|█████▋    | 138/241 [00:21<00:17,  5.82it/s]Epoch: 1, train for the 298-th batch, train loss: 0.4764516353607178:  78%|████████▌  | 297/383 [01:23<00:28,  3.00it/s]Epoch: 3, train for the 134-th batch, train loss: 0.5055732727050781:  92%|██████████ | 134/146 [00:19<00:01,  7.08it/s]Epoch: 2, train for the 30-th batch, train loss: 0.6759980916976929:  12%|█▌           | 29/237 [00:04<00:29,  7.12it/s]Epoch: 3, train for the 14-th batch, train loss: 0.7085193991661072:   9%|█            | 13/151 [00:01<00:20,  6.84it/s]Epoch: 3, train for the 14-th batch, train loss: 0.7085193991661072:   9%|█▏           | 14/151 [00:01<00:19,  7.13it/s]Epoch: 2, train for the 30-th batch, train loss: 0.6759980916976929:  13%|█▋           | 30/237 [00:04<00:33,  6.27it/s]Epoch: 4, train for the 42-th batch, train loss: 0.4820862114429474:  34%|████▍        | 41/119 [00:05<00:10,  7.25it/s]Epoch: 4, train for the 42-th batch, train loss: 0.4820862114429474:  35%|████▌        | 42/119 [00:05<00:10,  7.34it/s]Epoch: 1, train for the 298-th batch, train loss: 0.4764516353607178:  78%|████████▌  | 298/383 [01:23<00:29,  2.86it/s]Epoch: 4, train for the 43-th batch, train loss: 0.49258673191070557:  35%|████▏       | 42/119 [00:06<00:10,  7.34it/s]Epoch: 2, train for the 31-th batch, train loss: 0.5825467109680176:  13%|█▋           | 30/237 [00:04<00:33,  6.27it/s]Epoch: 4, train for the 43-th batch, train loss: 0.49258673191070557:  36%|████▎       | 43/119 [00:06<00:10,  7.54it/s]Epoch: 2, train for the 31-th batch, train loss: 0.5825467109680176:  13%|█▋           | 31/237 [00:04<00:32,  6.29it/s]Epoch: 3, train for the 15-th batch, train loss: 0.6912907361984253:   9%|█▏           | 14/151 [00:02<00:19,  7.13it/s]Epoch: 2, train for the 139-th batch, train loss: 0.33677712082862854:  57%|█████▋    | 138/241 [00:21<00:17,  5.82it/s]Epoch: 3, train for the 15-th batch, train loss: 0.6912907361984253:  10%|█▎           | 15/151 [00:02<00:21,  6.33it/s]Epoch: 2, train for the 139-th batch, train loss: 0.33677712082862854:  58%|█████▊    | 139/241 [00:21<00:20,  5.10it/s]Epoch: 3, train for the 135-th batch, train loss: 0.5129527449607849:  92%|██████████ | 134/146 [00:19<00:01,  7.08it/s]Epoch: 3, train for the 135-th batch, train loss: 0.5129527449607849:  92%|██████████▏| 135/146 [00:19<00:02,  5.49it/s]Epoch: 4, train for the 44-th batch, train loss: 0.43668991327285767:  36%|████▎       | 43/119 [00:06<00:10,  7.54it/s]Epoch: 2, train for the 32-th batch, train loss: 0.6009909510612488:  13%|█▋           | 31/237 [00:04<00:32,  6.29it/s]Epoch: 4, train for the 44-th batch, train loss: 0.43668991327285767:  37%|████▍       | 44/119 [00:06<00:10,  7.23it/s]Epoch: 2, train for the 32-th batch, train loss: 0.6009909510612488:  14%|█▊           | 32/237 [00:04<00:31,  6.53it/s]Epoch: 3, train for the 16-th batch, train loss: 0.7051923274993896:  10%|█▎           | 15/151 [00:02<00:21,  6.33it/s]Epoch: 3, train for the 16-th batch, train loss: 0.7051923274993896:  11%|█▍           | 16/151 [00:02<00:20,  6.58it/s]Epoch: 2, train for the 140-th batch, train loss: 0.3763583302497864:  58%|██████▎    | 139/241 [00:21<00:20,  5.10it/s]Epoch: 2, train for the 140-th batch, train loss: 0.3763583302497864:  58%|██████▍    | 140/241 [00:21<00:18,  5.44it/s]Epoch: 2, train for the 33-th batch, train loss: 0.5893802642822266:  14%|█▊           | 32/237 [00:04<00:31,  6.53it/s]Epoch: 2, train for the 33-th batch, train loss: 0.5893802642822266:  14%|█▊           | 33/237 [00:04<00:28,  7.06it/s]Epoch: 3, train for the 136-th batch, train loss: 0.4849422872066498:  92%|██████████▏| 135/146 [00:19<00:02,  5.49it/s]Epoch: 1, train for the 299-th batch, train loss: 0.5232885479927063:  78%|████████▌  | 298/383 [01:23<00:29,  2.86it/s]Epoch: 4, train for the 45-th batch, train loss: 0.40645095705986023:  37%|████▍       | 44/119 [00:06<00:10,  7.23it/s]Epoch: 3, train for the 17-th batch, train loss: 0.6933881044387817:  11%|█▍           | 16/151 [00:02<00:20,  6.58it/s]Epoch: 3, train for the 136-th batch, train loss: 0.4849422872066498:  93%|██████████▏| 136/146 [00:19<00:01,  5.42it/s]Epoch: 3, train for the 17-th batch, train loss: 0.6933881044387817:  11%|█▍           | 17/151 [00:02<00:18,  7.07it/s]Epoch: 4, train for the 45-th batch, train loss: 0.40645095705986023:  38%|████▌       | 45/119 [00:06<00:10,  6.92it/s]Epoch: 2, train for the 141-th batch, train loss: 0.47863146662712097:  58%|█████▊    | 140/241 [00:21<00:18,  5.44it/s]Epoch: 1, train for the 299-th batch, train loss: 0.5232885479927063:  78%|████████▌  | 299/383 [01:23<00:31,  2.68it/s]Epoch: 2, train for the 141-th batch, train loss: 0.47863146662712097:  59%|█████▊    | 141/241 [00:21<00:17,  5.84it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5515627264976501:  14%|█▊           | 33/237 [00:04<00:28,  7.06it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5515627264976501:  14%|█▊           | 34/237 [00:04<00:28,  7.19it/s]Epoch: 3, train for the 18-th batch, train loss: 0.6839314103126526:  11%|█▍           | 17/151 [00:02<00:18,  7.07it/s]Epoch: 4, train for the 46-th batch, train loss: 0.4333510398864746:  38%|████▉        | 45/119 [00:06<00:10,  6.92it/s]Epoch: 4, train for the 46-th batch, train loss: 0.4333510398864746:  39%|█████        | 46/119 [00:06<00:10,  6.79it/s]Epoch: 3, train for the 18-th batch, train loss: 0.6839314103126526:  12%|█▌           | 18/151 [00:02<00:19,  6.84it/s]Epoch: 3, train for the 137-th batch, train loss: 0.4604951739311218:  93%|██████████▏| 136/146 [00:19<00:01,  5.42it/s]Epoch: 3, train for the 137-th batch, train loss: 0.4604951739311218:  94%|██████████▎| 137/146 [00:19<00:01,  5.43it/s]Epoch: 2, train for the 142-th batch, train loss: 0.45943981409072876:  59%|█████▊    | 141/241 [00:21<00:17,  5.84it/s]Epoch: 2, train for the 142-th batch, train loss: 0.45943981409072876:  59%|█████▉    | 142/241 [00:21<00:16,  5.99it/s]Epoch: 2, train for the 35-th batch, train loss: 0.542073667049408:  14%|██            | 34/237 [00:04<00:28,  7.19it/s]Epoch: 2, train for the 35-th batch, train loss: 0.542073667049408:  15%|██            | 35/237 [00:05<00:30,  6.72it/s]Epoch: 3, train for the 19-th batch, train loss: 0.6732864379882812:  12%|█▌           | 18/151 [00:02<00:19,  6.84it/s]Epoch: 4, train for the 47-th batch, train loss: 0.49213477969169617:  39%|████▋       | 46/119 [00:06<00:10,  6.79it/s]Epoch: 3, train for the 19-th batch, train loss: 0.6732864379882812:  13%|█▋           | 19/151 [00:02<00:18,  6.95it/s]Epoch: 1, train for the 300-th batch, train loss: 0.5331469774246216:  78%|████████▌  | 299/383 [01:23<00:31,  2.68it/s]Epoch: 4, train for the 47-th batch, train loss: 0.49213477969169617:  39%|████▋       | 47/119 [00:06<00:10,  6.76it/s]Epoch: 3, train for the 138-th batch, train loss: 0.5175061821937561:  94%|██████████▎| 137/146 [00:20<00:01,  5.43it/s]Epoch: 3, train for the 138-th batch, train loss: 0.5175061821937561:  95%|██████████▍| 138/146 [00:20<00:01,  5.57it/s]Epoch: 2, train for the 143-th batch, train loss: 0.4828454554080963:  59%|██████▍    | 142/241 [00:22<00:16,  5.99it/s]Epoch: 1, train for the 300-th batch, train loss: 0.5331469774246216:  78%|████████▌  | 300/383 [01:23<00:29,  2.80it/s]Epoch: 2, train for the 143-th batch, train loss: 0.4828454554080963:  59%|██████▌    | 143/241 [00:22<00:16,  6.10it/s]Epoch: 2, train for the 36-th batch, train loss: 0.5738164186477661:  15%|█▉           | 35/237 [00:05<00:30,  6.72it/s]Epoch: 2, train for the 36-th batch, train loss: 0.5738164186477661:  15%|█▉           | 36/237 [00:05<00:29,  6.85it/s]Epoch: 3, train for the 20-th batch, train loss: 0.6632960438728333:  13%|█▋           | 19/151 [00:02<00:18,  6.95it/s]Epoch: 3, train for the 20-th batch, train loss: 0.6632960438728333:  13%|█▋           | 20/151 [00:02<00:17,  7.28it/s]Epoch: 4, train for the 48-th batch, train loss: 0.4625406563282013:  39%|█████▏       | 47/119 [00:06<00:10,  6.76it/s]Epoch: 4, train for the 48-th batch, train loss: 0.4625406563282013:  40%|█████▏       | 48/119 [00:06<00:10,  6.65it/s]Epoch: 3, train for the 139-th batch, train loss: 0.5231308937072754:  95%|██████████▍| 138/146 [00:20<00:01,  5.57it/s]Epoch: 3, train for the 139-th batch, train loss: 0.5231308937072754:  95%|██████████▍| 139/146 [00:20<00:01,  5.89it/s]Epoch: 2, train for the 144-th batch, train loss: 0.5730239152908325:  59%|██████▌    | 143/241 [00:22<00:16,  6.10it/s]Epoch: 3, train for the 21-th batch, train loss: 0.6743670701980591:  13%|█▋           | 20/151 [00:03<00:17,  7.28it/s]Epoch: 3, train for the 21-th batch, train loss: 0.6743670701980591:  14%|█▊           | 21/151 [00:03<00:18,  7.19it/s]Epoch: 2, train for the 144-th batch, train loss: 0.5730239152908325:  60%|██████▌    | 144/241 [00:22<00:16,  5.94it/s]Epoch: 2, train for the 37-th batch, train loss: 0.6126217246055603:  15%|█▉           | 36/237 [00:05<00:29,  6.85it/s]Epoch: 2, train for the 37-th batch, train loss: 0.6126217246055603:  16%|██           | 37/237 [00:05<00:31,  6.35it/s]Epoch: 4, train for the 49-th batch, train loss: 0.45664680004119873:  40%|████▊       | 48/119 [00:06<00:10,  6.65it/s]Epoch: 4, train for the 49-th batch, train loss: 0.45664680004119873:  41%|████▉       | 49/119 [00:06<00:10,  6.63it/s]Epoch: 1, train for the 301-th batch, train loss: 0.32474344968795776:  78%|███████▊  | 300/383 [01:24<00:29,  2.80it/s]Epoch: 3, train for the 140-th batch, train loss: 0.48900023102760315:  95%|█████████▌| 139/146 [00:20<00:01,  5.89it/s]Epoch: 3, train for the 140-th batch, train loss: 0.48900023102760315:  96%|█████████▌| 140/146 [00:20<00:01,  5.92it/s]Epoch: 1, train for the 301-th batch, train loss: 0.32474344968795776:  79%|███████▊  | 301/383 [01:24<00:28,  2.90it/s]Epoch: 2, train for the 38-th batch, train loss: 0.5634621977806091:  16%|██           | 37/237 [00:05<00:31,  6.35it/s]Epoch: 2, train for the 38-th batch, train loss: 0.5634621977806091:  16%|██           | 38/237 [00:05<00:29,  6.80it/s]Epoch: 4, train for the 50-th batch, train loss: 0.4245431423187256:  41%|█████▎       | 49/119 [00:07<00:10,  6.63it/s]Epoch: 3, train for the 22-th batch, train loss: 0.6553230285644531:  14%|█▊           | 21/151 [00:03<00:18,  7.19it/s]Epoch: 2, train for the 145-th batch, train loss: 0.5870024561882019:  60%|██████▌    | 144/241 [00:22<00:16,  5.94it/s]Epoch: 4, train for the 50-th batch, train loss: 0.4245431423187256:  42%|█████▍       | 50/119 [00:07<00:09,  6.99it/s]Epoch: 3, train for the 22-th batch, train loss: 0.6553230285644531:  15%|█▉           | 22/151 [00:03<00:19,  6.61it/s]Epoch: 2, train for the 145-th batch, train loss: 0.5870024561882019:  60%|██████▌    | 145/241 [00:22<00:16,  5.69it/s]Epoch: 3, train for the 141-th batch, train loss: 0.5091857314109802:  96%|██████████▌| 140/146 [00:20<00:01,  5.92it/s]Epoch: 3, train for the 141-th batch, train loss: 0.5091857314109802:  97%|██████████▌| 141/146 [00:20<00:00,  6.12it/s]Epoch: 2, train for the 39-th batch, train loss: 0.6099120378494263:  16%|██           | 38/237 [00:05<00:29,  6.80it/s]Epoch: 2, train for the 39-th batch, train loss: 0.6099120378494263:  16%|██▏          | 39/237 [00:05<00:28,  6.85it/s]Epoch: 4, train for the 51-th batch, train loss: 0.4878077208995819:  42%|█████▍       | 50/119 [00:07<00:09,  6.99it/s]Epoch: 4, train for the 51-th batch, train loss: 0.4878077208995819:  43%|█████▌       | 51/119 [00:07<00:09,  7.16it/s]Epoch: 3, train for the 23-th batch, train loss: 0.6584157943725586:  15%|█▉           | 22/151 [00:03<00:19,  6.61it/s]Epoch: 3, train for the 23-th batch, train loss: 0.6584157943725586:  15%|█▉           | 23/151 [00:03<00:20,  6.35it/s]Epoch: 1, train for the 302-th batch, train loss: 0.3443494439125061:  79%|████████▋  | 301/383 [01:24<00:28,  2.90it/s]Epoch: 2, train for the 146-th batch, train loss: 0.5935592651367188:  60%|██████▌    | 145/241 [00:22<00:16,  5.69it/s]Epoch: 3, train for the 142-th batch, train loss: 0.46342241764068604:  97%|█████████▋| 141/146 [00:20<00:00,  6.12it/s]Epoch: 3, train for the 142-th batch, train loss: 0.46342241764068604:  97%|█████████▋| 142/146 [00:20<00:00,  6.32it/s]Epoch: 2, train for the 146-th batch, train loss: 0.5935592651367188:  61%|██████▋    | 146/241 [00:22<00:17,  5.41it/s]Epoch: 1, train for the 302-th batch, train loss: 0.3443494439125061:  79%|████████▋  | 302/383 [01:24<00:26,  3.07it/s]Epoch: 2, train for the 40-th batch, train loss: 0.5769863724708557:  16%|██▏          | 39/237 [00:05<00:28,  6.85it/s]Epoch: 4, train for the 52-th batch, train loss: 0.4763370454311371:  43%|█████▌       | 51/119 [00:07<00:09,  7.16it/s]Epoch: 2, train for the 40-th batch, train loss: 0.5769863724708557:  17%|██▏          | 40/237 [00:05<00:27,  7.09it/s]Epoch: 4, train for the 52-th batch, train loss: 0.4763370454311371:  44%|█████▋       | 52/119 [00:07<00:09,  7.42it/s]Epoch: 3, train for the 24-th batch, train loss: 0.6661038994789124:  15%|█▉           | 23/151 [00:03<00:20,  6.35it/s]Epoch: 3, train for the 24-th batch, train loss: 0.6661038994789124:  16%|██           | 24/151 [00:03<00:19,  6.57it/s]Epoch: 3, train for the 143-th batch, train loss: 0.4942900836467743:  97%|██████████▋| 142/146 [00:20<00:00,  6.32it/s]Epoch: 2, train for the 147-th batch, train loss: 0.650847315788269:  61%|███████▎    | 146/241 [00:22<00:17,  5.41it/s]Epoch: 3, train for the 143-th batch, train loss: 0.4942900836467743:  98%|██████████▊| 143/146 [00:20<00:00,  6.21it/s]Epoch: 2, train for the 41-th batch, train loss: 0.576763927936554:  17%|██▎           | 40/237 [00:05<00:27,  7.09it/s]Epoch: 2, train for the 147-th batch, train loss: 0.650847315788269:  61%|███████▎    | 147/241 [00:22<00:17,  5.50it/s]Epoch: 2, train for the 41-th batch, train loss: 0.576763927936554:  17%|██▍           | 41/237 [00:05<00:28,  6.92it/s]Epoch: 4, train for the 53-th batch, train loss: 0.441262423992157:  44%|██████        | 52/119 [00:07<00:09,  7.42it/s]Epoch: 4, train for the 53-th batch, train loss: 0.441262423992157:  45%|██████▏       | 53/119 [00:07<00:09,  7.01it/s]Epoch: 3, train for the 25-th batch, train loss: 0.724923312664032:  16%|██▏           | 24/151 [00:03<00:19,  6.57it/s]Epoch: 3, train for the 25-th batch, train loss: 0.724923312664032:  17%|██▎           | 25/151 [00:03<00:18,  6.72it/s]Epoch: 1, train for the 303-th batch, train loss: 0.39758339524269104:  79%|███████▉  | 302/383 [01:24<00:26,  3.07it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5971786975860596:  17%|██▏          | 41/237 [00:06<00:28,  6.92it/s]Epoch: 3, train for the 144-th batch, train loss: 0.4349207878112793:  98%|██████████▊| 143/146 [00:21<00:00,  6.21it/s]Epoch: 2, train for the 148-th batch, train loss: 0.506406843662262:  61%|███████▎    | 147/241 [00:22<00:17,  5.50it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5971786975860596:  18%|██▎          | 42/237 [00:06<00:27,  7.03it/s]Epoch: 3, train for the 144-th batch, train loss: 0.4349207878112793:  99%|██████████▊| 144/146 [00:21<00:00,  6.18it/s]Epoch: 4, train for the 54-th batch, train loss: 0.4129641652107239:  45%|█████▊       | 53/119 [00:07<00:09,  7.01it/s]Epoch: 1, train for the 303-th batch, train loss: 0.39758339524269104:  79%|███████▉  | 303/383 [01:24<00:26,  3.07it/s]Epoch: 2, train for the 148-th batch, train loss: 0.506406843662262:  61%|███████▎    | 148/241 [00:23<00:16,  5.64it/s]Epoch: 3, train for the 26-th batch, train loss: 0.6711363196372986:  17%|██▏          | 25/151 [00:03<00:18,  6.72it/s]Epoch: 4, train for the 54-th batch, train loss: 0.4129641652107239:  45%|█████▉       | 54/119 [00:07<00:09,  6.84it/s]Epoch: 3, train for the 26-th batch, train loss: 0.6711363196372986:  17%|██▏          | 26/151 [00:03<00:17,  7.16it/s]Epoch: 2, train for the 43-th batch, train loss: 0.5608952045440674:  18%|██▎          | 42/237 [00:06<00:27,  7.03it/s]Epoch: 2, train for the 43-th batch, train loss: 0.5608952045440674:  18%|██▎          | 43/237 [00:06<00:28,  6.70it/s]Epoch: 3, train for the 145-th batch, train loss: 0.5060510039329529:  99%|██████████▊| 144/146 [00:21<00:00,  6.18it/s]Epoch: 4, train for the 55-th batch, train loss: 0.43421661853790283:  45%|█████▍      | 54/119 [00:07<00:09,  6.84it/s]Epoch: 3, train for the 145-th batch, train loss: 0.5060510039329529:  99%|██████████▉| 145/146 [00:21<00:00,  5.85it/s]Epoch: 4, train for the 55-th batch, train loss: 0.43421661853790283:  46%|█████▌      | 55/119 [00:07<00:09,  6.43it/s]Epoch: 2, train for the 149-th batch, train loss: 0.4688992202281952:  61%|██████▊    | 148/241 [00:23<00:16,  5.64it/s]Epoch: 3, train for the 27-th batch, train loss: 0.673242449760437:  17%|██▍           | 26/151 [00:03<00:17,  7.16it/s]Epoch: 3, train for the 27-th batch, train loss: 0.673242449760437:  18%|██▌           | 27/151 [00:03<00:19,  6.23it/s]Epoch: 2, train for the 149-th batch, train loss: 0.4688992202281952:  62%|██████▊    | 149/241 [00:23<00:17,  5.25it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5821405649185181:  18%|██▎          | 43/237 [00:06<00:28,  6.70it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5821405649185181:  19%|██▍          | 44/237 [00:06<00:27,  6.96it/s]Epoch: 4, train for the 56-th batch, train loss: 0.44288626313209534:  46%|█████▌      | 55/119 [00:08<00:09,  6.43it/s]Epoch: 3, train for the 146-th batch, train loss: 0.42035043239593506:  99%|█████████▉| 145/146 [00:21<00:00,  5.85it/s]Epoch: 4, train for the 56-th batch, train loss: 0.44288626313209534:  47%|█████▋      | 56/119 [00:08<00:10,  6.10it/s]Epoch: 2, train for the 45-th batch, train loss: 0.6059392690658569:  19%|██▍          | 44/237 [00:06<00:27,  6.96it/s]Epoch: 3, train for the 146-th batch, train loss: 0.42035043239593506: 100%|██████████| 146/146 [00:21<00:00,  5.63it/s]Epoch: 3, train for the 146-th batch, train loss: 0.42035043239593506: 100%|██████████| 146/146 [00:21<00:00,  6.81it/s]
Epoch: 2, train for the 45-th batch, train loss: 0.6059392690658569:  19%|██▍          | 45/237 [00:06<00:25,  7.40it/s]Epoch: 2, train for the 150-th batch, train loss: 0.45783400535583496:  62%|██████▏   | 149/241 [00:23<00:17,  5.25it/s]Epoch: 3, train for the 28-th batch, train loss: 0.678781270980835:  18%|██▌           | 27/151 [00:04<00:19,  6.23it/s]Epoch: 3, train for the 28-th batch, train loss: 0.678781270980835:  19%|██▌           | 28/151 [00:04<00:20,  5.98it/s]Epoch: 2, train for the 150-th batch, train loss: 0.45783400535583496:  62%|██████▏   | 150/241 [00:23<00:17,  5.32it/s]  0%|                                                                                            | 0/38 [00:00<?, ?it/s]Epoch: 4, train for the 57-th batch, train loss: 0.44131964445114136:  47%|█████▋      | 56/119 [00:08<00:10,  6.10it/s]Epoch: 4, train for the 57-th batch, train loss: 0.44131964445114136:  48%|█████▋      | 57/119 [00:08<00:09,  6.60it/s]Epoch: 2, train for the 46-th batch, train loss: 0.5678643584251404:  19%|██▍          | 45/237 [00:06<00:25,  7.40it/s]evaluate for the 1-th batch, evaluate loss: 0.4852919578552246:   0%|                            | 0/38 [00:00<?, ?it/s]Epoch: 2, train for the 46-th batch, train loss: 0.5678643584251404:  19%|██▌          | 46/237 [00:06<00:25,  7.41it/s]Epoch: 1, train for the 304-th batch, train loss: 0.4301636517047882:  79%|████████▋  | 303/383 [01:25<00:26,  3.07it/s]Epoch: 3, train for the 29-th batch, train loss: 0.6919398307800293:  19%|██▍          | 28/151 [00:04<00:20,  5.98it/s]Epoch: 2, train for the 151-th batch, train loss: 0.38247960805892944:  62%|██████▏   | 150/241 [00:23<00:17,  5.32it/s]Epoch: 3, train for the 29-th batch, train loss: 0.6919398307800293:  19%|██▍          | 29/151 [00:04<00:20,  5.97it/s]Epoch: 2, train for the 151-th batch, train loss: 0.38247960805892944:  63%|██████▎   | 151/241 [00:23<00:16,  5.46it/s]evaluate for the 2-th batch, evaluate loss: 0.4865632653236389:   0%|                            | 0/38 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.4865632653236389:   5%|█                   | 2/38 [00:00<00:02, 15.94it/s]Epoch: 1, train for the 304-th batch, train loss: 0.4301636517047882:  79%|████████▋  | 304/383 [01:25<00:32,  2.43it/s]Epoch: 4, train for the 58-th batch, train loss: 0.4266674816608429:  48%|██████▏      | 57/119 [00:08<00:09,  6.60it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5663759112358093:  19%|██▌          | 46/237 [00:06<00:25,  7.41it/s]Epoch: 4, train for the 58-th batch, train loss: 0.4266674816608429:  49%|██████▎      | 58/119 [00:08<00:09,  6.77it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5663759112358093:  20%|██▌          | 47/237 [00:06<00:24,  7.66it/s]evaluate for the 3-th batch, evaluate loss: 0.47385725378990173:   5%|█                  | 2/38 [00:00<00:02, 15.94it/s]evaluate for the 4-th batch, evaluate loss: 0.4785766005516052:   5%|█                   | 2/38 [00:00<00:02, 15.94it/s]evaluate for the 4-th batch, evaluate loss: 0.4785766005516052:  11%|██                  | 4/38 [00:00<00:02, 16.68it/s]Epoch: 3, train for the 30-th batch, train loss: 0.6918909549713135:  19%|██▍          | 29/151 [00:04<00:20,  5.97it/s]Epoch: 2, train for the 152-th batch, train loss: 0.3405417501926422:  63%|██████▉    | 151/241 [00:23<00:16,  5.46it/s]Epoch: 3, train for the 30-th batch, train loss: 0.6918909549713135:  20%|██▌          | 30/151 [00:04<00:20,  5.99it/s]Epoch: 2, train for the 152-th batch, train loss: 0.3405417501926422:  63%|██████▉    | 152/241 [00:23<00:16,  5.51it/s]Epoch: 4, train for the 59-th batch, train loss: 0.4655393362045288:  49%|██████▎      | 58/119 [00:08<00:09,  6.77it/s]Epoch: 4, train for the 59-th batch, train loss: 0.4655393362045288:  50%|██████▍      | 59/119 [00:08<00:08,  6.83it/s]evaluate for the 5-th batch, evaluate loss: 0.5452811121940613:  11%|██                  | 4/38 [00:00<00:02, 16.68it/s]Epoch: 2, train for the 48-th batch, train loss: 0.559366762638092:  20%|██▊           | 47/237 [00:06<00:24,  7.66it/s]Epoch: 2, train for the 48-th batch, train loss: 0.559366762638092:  20%|██▊           | 48/237 [00:06<00:26,  7.04it/s]evaluate for the 6-th batch, evaluate loss: 0.48626577854156494:  11%|██                 | 4/38 [00:00<00:02, 16.68it/s]evaluate for the 6-th batch, evaluate loss: 0.48626577854156494:  16%|███                | 6/38 [00:00<00:02, 15.53it/s]Epoch: 3, train for the 31-th batch, train loss: 0.6894561648368835:  20%|██▌          | 30/151 [00:04<00:20,  5.99it/s]Epoch: 3, train for the 31-th batch, train loss: 0.6894561648368835:  21%|██▋          | 31/151 [00:04<00:19,  6.31it/s]Epoch: 2, train for the 153-th batch, train loss: 0.43596595525741577:  63%|██████▎   | 152/241 [00:23<00:16,  5.51it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5292361378669739:  20%|██▋          | 48/237 [00:06<00:26,  7.04it/s]Epoch: 2, train for the 153-th batch, train loss: 0.43596595525741577:  63%|██████▎   | 153/241 [00:23<00:15,  5.64it/s]Epoch: 4, train for the 60-th batch, train loss: 0.4549216330051422:  50%|██████▍      | 59/119 [00:08<00:08,  6.83it/s]evaluate for the 7-th batch, evaluate loss: 0.43782341480255127:  16%|███                | 6/38 [00:00<00:02, 15.53it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5292361378669739:  21%|██▋          | 49/237 [00:06<00:25,  7.37it/s]Epoch: 4, train for the 60-th batch, train loss: 0.4549216330051422:  50%|██████▌      | 60/119 [00:08<00:08,  6.60it/s]True
  0%|          | 0/52049 [00:00<?, ?it/s]True
  0%|          | 0/50631 [00:00<?, ?it/s]True
  0%|          | 0/61156 [00:00<?, ?it/s]True
  0%|          | 0/95577 [00:00<?, ?it/s]True
  0%|          | 0/140777 [00:00<?, ?it/s]True
  0%|          | 0/87626 [00:00<?, ?it/s] 10%|█         | 5297/52049 [00:00<00:00, 52965.76it/s]  9%|▉         | 4640/50631 [00:00<00:01, 42478.14it/s]  9%|▊         | 5203/61156 [00:00<00:01, 50201.20it/s]  8%|▊         | 7737/95577 [00:00<00:01, 77363.80it/s]  4%|▍         | 6048/140777 [00:00<00:02, 60474.43it/s]  8%|▊         | 7312/87626 [00:00<00:01, 73116.76it/s] 19%|█▉        | 9799/50631 [00:00<00:00, 47639.85it/s] 17%|█▋        | 16155/95577 [00:00<00:00, 81372.79it/s] 17%|█▋        | 10224/61156 [00:00<00:01, 47575.35it/s] 20%|██        | 10594/52049 [00:00<00:00, 44379.91it/s] 10%|█         | 14261/140777 [00:00<00:01, 73208.76it/s] 29%|██▉       | 14583/50631 [00:00<00:00, 46680.44it/s] 29%|██▉       | 15120/52049 [00:00<00:00, 44492.37it/s] 16%|█▌        | 22785/140777 [00:00<00:01, 78701.22it/s] 25%|██▌       | 15332/61156 [00:00<00:01, 43038.60it/s] 17%|█▋        | 14624/87626 [00:00<00:01, 46427.70it/s] 38%|███▊      | 19263/50631 [00:00<00:00, 43992.44it/s] 38%|███▊      | 19837/52049 [00:00<00:00, 42993.56it/s] 32%|███▏      | 19697/61156 [00:00<00:00, 43254.29it/s] 23%|██▎       | 19810/87626 [00:00<00:01, 48140.10it/s] 22%|██▏       | 30656/140777 [00:00<00:01, 61932.93it/s] 49%|████▉     | 24780/50631 [00:00<00:00, 47802.56it/s] 25%|██▌       | 24293/95577 [00:00<00:01, 39691.38it/s] 40%|████      | 24733/61156 [00:00<00:00, 43344.52it/s] 48%|████▊     | 24902/52049 [00:00<00:00, 41732.87it/s] 28%|██▊       | 24972/87626 [00:00<00:01, 44304.23it/s] 58%|█████▊    | 29598/50631 [00:00<00:00, 43593.10it/s] 31%|███       | 29841/95577 [00:00<00:01, 40264.37it/s] 57%|█████▋    | 29543/52049 [00:00<00:00, 41537.86it/s] 49%|████▉     | 30270/61156 [00:00<00:00, 43909.27it/s] 34%|███▍      | 29617/87626 [00:00<00:01, 44537.01it/s] 69%|██████▊   | 34700/50631 [00:00<00:00, 45808.71it/s] 26%|██▋       | 37282/140777 [00:00<00:02, 40658.22it/s] 40%|███▉      | 34976/87626 [00:00<00:01, 47223.78it/s] 66%|██████▋   | 34528/52049 [00:00<00:00, 42120.39it/s] 37%|███▋      | 35278/95577 [00:00<00:01, 40923.43it/s] 57%|█████▋    | 34672/61156 [00:00<00:00, 41462.53it/s] 32%|███▏      | 45610/140777 [00:00<00:01, 50037.81it/s] 78%|███████▊  | 39354/50631 [00:00<00:00, 40899.29it/s] 43%|████▎     | 40964/95577 [00:00<00:01, 44821.12it/s] 75%|███████▍  | 38944/52049 [00:00<00:00, 41895.58it/s] 65%|██████▍   | 39601/61156 [00:00<00:00, 42777.36it/s] 45%|████▌     | 39831/87626 [00:00<00:01, 44749.30it/s] 88%|████████▊ | 44573/50631 [00:01<00:00, 44004.19it/s] 49%|████▉     | 47037/95577 [00:00<00:00, 48940.55it/s] 83%|████████▎ | 43414/52049 [00:01<00:00, 42698.97it/s] 72%|███████▏  | 43896/61156 [00:01<00:00, 42015.59it/s] 51%|█████     | 44832/87626 [00:01<00:01, 41180.00it/s] 98%|█████████▊| 49629/50631 [00:01<00:00, 45853.23it/s] 37%|███▋      | 52126/140777 [00:01<00:02, 41622.97it/s] 92%|█████████▏| 47697/52049 [00:01<00:00, 40797.98it/s] 79%|███████▉  | 48509/61156 [00:01<00:00, 42612.06it/s]100%|█████████▉| 50628/50631 [00:01<00:00, 43715.74it/s]
 57%|█████▋    | 49794/87626 [00:01<00:00, 43428.73it/s] 55%|█████▍    | 52426/95577 [00:01<00:01, 42814.23it/s] 43%|████▎     | 60446/140777 [00:01<00:01, 50335.09it/s]100%|█████████▉| 52048/52049 [00:01<00:00, 42136.45it/s]
 87%|████████▋ | 53002/61156 [00:01<00:00, 42100.99it/s] 62%|██████▏   | 54718/87626 [00:01<00:00, 45030.94it/s] 60%|██████    | 57371/95577 [00:01<00:00, 44449.16it/s]  0%|          | 0/50631 [00:00<?, ?it/s]100%|██████████| 50631/50631 [00:00<00:00, 1862969.93it/s]
 49%|████▉     | 68863/140777 [00:01<00:01, 58129.11it/s] 95%|█████████▌| 58380/61156 [00:01<00:00, 42294.60it/s] 68%|██████▊   | 59321/87626 [00:01<00:00, 43523.95it/s] 65%|██████▌   | 62158/95577 [00:01<00:00, 44207.33it/s]  0%|          | 0/52049 [00:00<?, ?it/s]100%|██████████| 52049/52049 [00:00<00:00, 1977081.41it/s]
100%|█████████▉| 61155/61156 [00:01<00:00, 42951.21it/s]
 70%|███████   | 66985/95577 [00:01<00:00, 45289.83it/s] 73%|███████▎  | 63745/87626 [00:01<00:00, 41608.14it/s] 54%|█████▍    | 75716/140777 [00:01<00:01, 44280.01it/s] 79%|███████▉  | 69610/87626 [00:01<00:00, 46298.12it/s]  0%|          | 0/61156 [00:00<?, ?it/s]100%|██████████| 61156/61156 [00:00<00:00, 1101247.00it/s]
 75%|███████▌  | 71693/95577 [00:01<00:00, 36851.27it/s] 58%|█████▊    | 81670/140777 [00:01<00:01, 47409.20it/s] 85%|████████▍ | 74326/87626 [00:01<00:00, 37102.51it/s] 81%|████████  | 76944/95577 [00:01<00:00, 37666.29it/s] 91%|█████████ | 79491/87626 [00:01<00:00, 40641.02it/s] 85%|████████▌ | 81457/95577 [00:01<00:00, 36156.51it/s] 63%|██████▎   | 89180/140777 [00:01<00:01, 34014.34it/s] 96%|█████████▌| 83871/87626 [00:01<00:00, 37130.43it/s] 91%|█████████ | 86681/95577 [00:02<00:00, 40022.56it/s]INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-escorts-dynamic', batch_size=200, model_name='TGN', gpu=0, num_neighbors=10, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=1, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='lincorrect-time-linear', use_ROPe=False, t1_factor_of_t2=1, use_init_method=True, emb_proj=False, init_weights='time-linear', clip=1.0, device='cuda:0', seed=0, save_model_name='TGN_seed0_dummy-55wl6je1')
100%|█████████▉| 87625/87626 [00:02<00:00, 42664.39it/s]
 69%|██████▉   | 97509/140777 [00:02<00:01, 42400.84it/s] 95%|█████████▌| 90904/95577 [00:02<00:00, 37962.47it/s] 99%|█████████▉| 95092/95577 [00:02<00:00, 38958.87it/s]100%|█████████▉| 95576/95577 [00:02<00:00, 41785.52it/s]
INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-reality-call', batch_size=200, model_name='TGN', gpu=0, num_neighbors=10, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=1, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='lincorrect-time-linear', use_ROPe=False, t1_factor_of_t2=1, use_init_method=True, emb_proj=False, init_weights='time-linear', clip=1.0, device='cuda:0', seed=0, save_model_name='TGN_seed0_dummy-e41ajx4p')
  0%|          | 0/87626 [00:00<?, ?it/s] 73%|███████▎  | 103244/140777 [00:02<00:01, 32417.98it/s]100%|██████████| 87626/87626 [00:00<00:00, 1939984.60it/s]
 79%|███████▉  | 111590/140777 [00:02<00:00, 40919.36it/s]  0%|          | 0/95577 [00:00<?, ?it/s]100%|██████████| 95577/95577 [00:00<00:00, 1986339.14it/s]
 85%|████████▌ | 119952/140777 [00:02<00:00, 49126.70it/s]INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-retweet-pol', batch_size=200, model_name='TGN', gpu=2, num_neighbors=10, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=1, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='lincorrect-time-linear', use_ROPe=False, t1_factor_of_t2=1, use_init_method=True, emb_proj=False, init_weights='time-linear', clip=1.0, device='cuda:2', seed=0, save_model_name='TGN_seed0_dummy-lhwm9lqt')
 90%|████████▉ | 126461/140777 [00:02<00:00, 41357.65it/s] 94%|█████████▎| 131880/140777 [00:03<00:00, 29690.03it/s]100%|█████████▉| 140246/140777 [00:03<00:00, 38249.78it/s]100%|█████████▉| 140776/140777 [00:03<00:00, 41783.35it/s]
INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-digg-reply', batch_size=200, model_name='TGN', gpu=1, num_neighbors=10, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=1, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='lincorrect-time-linear', use_ROPe=False, t1_factor_of_t2=1, use_init_method=True, emb_proj=False, init_weights='time-linear', clip=1.0, device='cuda:1', seed=0, save_model_name='TGN_seed0_dummy-bceprsuq')
INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-movielens-user2tags-10m', batch_size=200, model_name='TGN', gpu=2, num_neighbors=10, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=1, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='lincorrect-time-linear', use_ROPe=False, t1_factor_of_t2=1, use_init_method=True, emb_proj=False, init_weights='time-linear', clip=1.0, device='cuda:2', seed=0, save_model_name='TGN_seed0_dummy-s2o65ldo')
  0%|          | 0/140777 [00:00<?, ?it/s]100%|██████████| 140777/140777 [00:00<00:00, 1890051.49it/s]
INFO:root:model -> Sequential(
  (0): MemoryModel(
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (emb_proj): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
      (1): ReLU()
    )
    (time_transformation_for_init): TimeInitTransformLinear(
      (lin): Linear(in_features=1, out_features=1, bias=True)
    )
    (message_aggregator): MessageAggregator()
    (memory_bank): MemoryBank(num_nodes=10108, memory_dim=172)
    (memory_updater): GRUMemoryUpdater(
      (memory_bank): MemoryBank(num_nodes=10108, memory_dim=172)
      (memory_updater): GRUCell(616, 172)
    )
    (embedding_module): GraphAttentionEmbedding(
      (time_encoder): TimeEncoder(
        (w): Linear(in_features=1, out_features=100, bias=True)
      )
      (temporal_conv_layers): ModuleList(
        (0): MultiHeadAttention(
          (query_projection): Linear(in_features=272, out_features=272, bias=False)
          (key_projection): Linear(in_features=444, out_features=272, bias=False)
          (value_projection): Linear(in_features=444, out_features=272, bias=False)
          (layer_norm): LayerNorm((272,), eps=1e-05, elementwise_affine=True)
          (residual_fc): Linear(in_features=272, out_features=272, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (merge_layers): ModuleList(
        (0): MergeLayer(
          (fc1): Linear(in_features=444, out_features=172, bias=True)
          (fc2): Linear(in_features=172, out_features=172, bias=True)
          (act): ReLU()
        )
      )
    )
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: TGN, #parameters: 3974904 B, 3881.7421875 KB, 3.7907638549804688 MB.
The dataset has 50631 interactions, involving 10106 different nodes
The training dataset has 29100 interactions, involving 7154 different nodes
The validation dataset has 7596 interactions, involving 4118 different nodes
The test dataset has 7577 interactions, involving 4144 different nodes
The new node validation dataset has 3845 interactions, involving 2930 different nodes
The new node test dataset has 4829 interactions, involving 3346 different nodes
1010 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-slashdot-reply-dir', batch_size=200, model_name='TGN', gpu=1, num_neighbors=10, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=1, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='lincorrect-time-linear', use_ROPe=False, t1_factor_of_t2=1, use_init_method=True, emb_proj=False, init_weights='time-linear', clip=1.0, device='cuda:1', seed=0, save_model_name='TGN_seed0_dummy-it4vxx9j')
INFO:root:model -> Sequential(
  (0): MemoryModel(
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (emb_proj): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
      (1): ReLU()
    )
    (time_transformation_for_init): TimeInitTransformLinear(
      (lin): Linear(in_features=1, out_features=1, bias=True)
    )
    (message_aggregator): MessageAggregator()
    (memory_bank): MemoryBank(num_nodes=27046, memory_dim=172)
    (memory_updater): GRUMemoryUpdater(
      (memory_bank): MemoryBank(num_nodes=27046, memory_dim=172)
      (memory_updater): GRUCell(616, 172)
    )
    (embedding_module): GraphAttentionEmbedding(
      (time_encoder): TimeEncoder(
        (w): Linear(in_features=1, out_features=100, bias=True)
      )
      (temporal_conv_layers): ModuleList(
        (0): MultiHeadAttention(
          (query_projection): Linear(in_features=272, out_features=272, bias=False)
          (key_projection): Linear(in_features=444, out_features=272, bias=False)
          (value_projection): Linear(in_features=444, out_features=272, bias=False)
          (layer_norm): LayerNorm((272,), eps=1e-05, elementwise_affine=True)
          (residual_fc): Linear(in_features=272, out_features=272, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (merge_layers): ModuleList(
        (0): MergeLayer(
          (fc1): Linear(in_features=444, out_features=172, bias=True)
          (fc2): Linear(in_features=172, out_features=172, bias=True)
          (act): ReLU()
        )
      )
    )
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: TGN, #parameters: 3974904 B, 3881.7421875 KB, 3.7907638549804688 MB.
The dataset has 52049 interactions, involving 6809 different nodes
The training dataset has 23625 interactions, involving 3838 different nodes
The validation dataset has 7807 interactions, involving 1715 different nodes
The test dataset has 7808 interactions, involving 1937 different nodes
The new node validation dataset has 4011 interactions, involving 1185 different nodes
The new node test dataset has 4611 interactions, involving 1531 different nodes
680 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]INFO:root:model -> Sequential(
  (0): MemoryModel(
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (emb_proj): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
      (1): ReLU()
    )
    (time_transformation_for_init): TimeInitTransformLinear(
      (lin): Linear(in_features=1, out_features=1, bias=True)
    )
    (message_aggregator): MessageAggregator()
    (memory_bank): MemoryBank(num_nodes=18471, memory_dim=172)
    (memory_updater): GRUMemoryUpdater(
      (memory_bank): MemoryBank(num_nodes=18471, memory_dim=172)
      (memory_updater): GRUCell(616, 172)
    )
    (embedding_module): GraphAttentionEmbedding(
      (time_encoder): TimeEncoder(
        (w): Linear(in_features=1, out_features=100, bias=True)
      )
      (temporal_conv_layers): ModuleList(
        (0): MultiHeadAttention(
          (query_projection): Linear(in_features=272, out_features=272, bias=False)
          (key_projection): Linear(in_features=444, out_features=272, bias=False)
          (value_projection): Linear(in_features=444, out_features=272, bias=False)
          (layer_norm): LayerNorm((272,), eps=1e-05, elementwise_affine=True)
          (residual_fc): Linear(in_features=272, out_features=272, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (merge_layers): ModuleList(
        (0): MergeLayer(
          (fc1): Linear(in_features=444, out_features=172, bias=True)
          (fc2): Linear(in_features=172, out_features=172, bias=True)
          (act): ReLU()
        )
      )
    )
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: TGN, #parameters: 3974904 B, 3881.7421875 KB, 3.7907638549804688 MB.
The dataset has 61156 interactions, involving 18470 different nodes
The training dataset has 30070 interactions, involving 12678 different nodes
The validation dataset has 9173 interactions, involving 5479 different nodes
The test dataset has 9174 interactions, involving 5328 different nodes
The new node validation dataset has 4957 interactions, involving 4196 different nodes
The new node test dataset has 5073 interactions, involving 4153 different nodes
1847 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]INFO:root:model -> Sequential(
  (0): MemoryModel(
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (emb_proj): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
      (1): ReLU()
    )
    (time_transformation_for_init): TimeInitTransformLinear(
      (lin): Linear(in_features=1, out_features=1, bias=True)
    )
    (message_aggregator): MessageAggregator()
    (memory_bank): MemoryBank(num_nodes=30400, memory_dim=172)
    (memory_updater): GRUMemoryUpdater(
      (memory_bank): MemoryBank(num_nodes=30400, memory_dim=172)
      (memory_updater): GRUCell(616, 172)
    )
    (embedding_module): GraphAttentionEmbedding(
      (time_encoder): TimeEncoder(
        (w): Linear(in_features=1, out_features=100, bias=True)
      )
      (temporal_conv_layers): ModuleList(
        (0): MultiHeadAttention(
          (query_projection): Linear(in_features=272, out_features=272, bias=False)
          (key_projection): Linear(in_features=444, out_features=272, bias=False)
          (value_projection): Linear(in_features=444, out_features=272, bias=False)
          (layer_norm): LayerNorm((272,), eps=1e-05, elementwise_affine=True)
          (residual_fc): Linear(in_features=272, out_features=272, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (merge_layers): ModuleList(
        (0): MergeLayer(
          (fc1): Linear(in_features=444, out_features=172, bias=True)
          (fc2): Linear(in_features=172, out_features=172, bias=True)
          (act): ReLU()
        )
      )
    )
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: TGN, #parameters: 3974904 B, 3881.7421875 KB, 3.7907638549804688 MB.
The dataset has 87626 interactions, involving 30398 different nodes
The training dataset has 47297 interactions, involving 21540 different nodes
The validation dataset has 13144 interactions, involving 9241 different nodes
The test dataset has 13144 interactions, involving 9511 different nodes
The new node validation dataset has 7995 interactions, involving 7321 different nodes
The new node test dataset has 8239 interactions, involving 7732 different nodes
3039 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/237 [00:00<?, ?it/s]INFO:root:model -> Sequential(
  (0): MemoryModel(
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (emb_proj): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
      (1): ReLU()
    )
    (time_transformation_for_init): TimeInitTransformLinear(
      (lin): Linear(in_features=1, out_features=1, bias=True)
    )
    (message_aggregator): MessageAggregator()
    (memory_bank): MemoryBank(num_nodes=16530, memory_dim=172)
    (memory_updater): GRUMemoryUpdater(
      (memory_bank): MemoryBank(num_nodes=16530, memory_dim=172)
      (memory_updater): GRUCell(616, 172)
    )
    (embedding_module): GraphAttentionEmbedding(
      (time_encoder): TimeEncoder(
        (w): Linear(in_features=1, out_features=100, bias=True)
      )
      (temporal_conv_layers): ModuleList(
        (0): MultiHeadAttention(
          (query_projection): Linear(in_features=272, out_features=272, bias=False)
          (key_projection): Linear(in_features=444, out_features=272, bias=False)
          (value_projection): Linear(in_features=444, out_features=272, bias=False)
          (layer_norm): LayerNorm((272,), eps=1e-05, elementwise_affine=True)
          (residual_fc): Linear(in_features=272, out_features=272, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (merge_layers): ModuleList(
        (0): MergeLayer(
          (fc1): Linear(in_features=444, out_features=172, bias=True)
          (fc2): Linear(in_features=172, out_features=172, bias=True)
          (act): ReLU()
        )
      )
    )
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: TGN, #parameters: 3974904 B, 3881.7421875 KB, 3.7907638549804688 MB.
The dataset has 95577 interactions, involving 16527 different nodes
The training dataset has 48189 interactions, involving 10566 different nodes
The validation dataset has 14336 interactions, involving 5209 different nodes
The test dataset has 14337 interactions, involving 5377 different nodes
The new node validation dataset has 6745 interactions, involving 3518 different nodes
The new node test dataset has 7521 interactions, involving 3885 different nodes
1652 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/241 [00:00<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6933676600456238:   0%|                       | 0/146 [00:02<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6933676600456238:   1%|               | 1/146 [00:02<05:57,  2.46s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6931648254394531:   1%|               | 1/146 [00:02<05:57,  2.46s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6932362914085388:   1%|               | 1/146 [00:02<05:57,  2.46s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6932362914085388:   2%|▎              | 3/146 [00:02<01:54,  1.25it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6934496760368347:   0%|                       | 0/151 [00:02<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6934496760368347:   1%|               | 1/151 [00:02<06:03,  2.42s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6931430101394653:   1%|               | 1/151 [00:02<06:03,  2.42s/it]Epoch: 1, train for the 1-th batch, train loss: 0.6951776742935181:   0%|                       | 0/119 [00:02<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6951776742935181:   1%|▏              | 1/119 [00:02<05:00,  2.55s/it]Epoch: 1, train for the 4-th batch, train loss: 0.6933436393737793:   2%|▎              | 3/146 [00:03<01:54,  1.25it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6933436393737793:   3%|▍              | 4/146 [00:03<01:25,  1.66it/s]Epoch: 1, train for the 3-th batch, train loss: 0.6925732493400574:   1%|               | 1/151 [00:02<06:03,  2.42s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6925732493400574:   2%|▎              | 3/151 [00:02<01:56,  1.27it/s]INFO:root:model -> Sequential(
  (0): MemoryModel(
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (emb_proj): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
      (1): ReLU()
    )
    (time_transformation_for_init): TimeInitTransformLinear(
      (lin): Linear(in_features=1, out_features=1, bias=True)
    )
    (message_aggregator): MessageAggregator()
    (memory_bank): MemoryBank(num_nodes=51085, memory_dim=172)
    (memory_updater): GRUMemoryUpdater(
      (memory_bank): MemoryBank(num_nodes=51085, memory_dim=172)
      (memory_updater): GRUCell(616, 172)
    )
    (embedding_module): GraphAttentionEmbedding(
      (time_encoder): TimeEncoder(
        (w): Linear(in_features=1, out_features=100, bias=True)
      )
      (temporal_conv_layers): ModuleList(
        (0): MultiHeadAttention(
          (query_projection): Linear(in_features=272, out_features=272, bias=False)
          (key_projection): Linear(in_features=444, out_features=272, bias=False)
          (value_projection): Linear(in_features=444, out_features=272, bias=False)
          (layer_norm): LayerNorm((272,), eps=1e-05, elementwise_affine=True)
          (residual_fc): Linear(in_features=272, out_features=272, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (merge_layers): ModuleList(
        (0): MergeLayer(
          (fc1): Linear(in_features=444, out_features=172, bias=True)
          (fc2): Linear(in_features=172, out_features=172, bias=True)
          (act): ReLU()
        )
      )
    )
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: TGN, #parameters: 3974904 B, 3881.7421875 KB, 3.7907638549804688 MB.
Epoch: 1, train for the 5-th batch, train loss: 0.6927593946456909:   3%|▍              | 4/146 [00:03<01:25,  1.66it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6927593946456909:   3%|▌              | 5/146 [00:03<01:09,  2.01it/s]The dataset has 140777 interactions, involving 51083 different nodes
The training dataset has 76599 interactions, involving 34496 different nodes
The validation dataset has 21116 interactions, involving 10542 different nodes
The test dataset has 21117 interactions, involving 10424 different nodes
The new node validation dataset has 15534 interactions, involving 9790 different nodes
The new node test dataset has 16568 interactions, involving 9911 different nodes
5108 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/383 [00:00<?, ?it/s]Epoch: 1, train for the 2-th batch, train loss: 0.6877005696296692:   1%|▏              | 1/119 [00:02<05:00,  2.55s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6877005696296692:   2%|▎              | 2/119 [00:02<02:28,  1.27s/it]Epoch: 1, train for the 4-th batch, train loss: 0.6931124925613403:   2%|▎              | 3/151 [00:02<01:56,  1.27it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6931124925613403:   3%|▍              | 4/151 [00:02<01:21,  1.80it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6923458576202393:   3%|▌              | 5/146 [00:03<01:09,  2.01it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6923458576202393:   4%|▌              | 6/146 [00:03<00:51,  2.70it/s]Epoch: 1, train for the 3-th batch, train loss: 0.6864984631538391:   2%|▎              | 2/119 [00:03<02:28,  1.27s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6864984631538391:   3%|▍              | 3/119 [00:03<01:27,  1.32it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6923882961273193:   3%|▍              | 4/151 [00:03<01:21,  1.80it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6926030516624451:   4%|▌              | 6/146 [00:03<00:51,  2.70it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6926030516624451:   5%|▋              | 7/146 [00:03<00:39,  3.49it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6920763850212097:   5%|▋              | 7/146 [00:03<00:39,  3.49it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6785052418708801:   3%|▍              | 3/119 [00:03<01:27,  1.32it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6785052418708801:   3%|▌              | 4/119 [00:03<01:00,  1.89it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6927867531776428:   3%|▍              | 4/151 [00:03<01:21,  1.80it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6927867531776428:   4%|▌              | 6/151 [00:03<00:52,  2.76it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6932621598243713:   0%|                       | 0/237 [00:02<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6932621598243713:   0%|               | 1/237 [00:02<09:18,  2.37s/it]Epoch: 1, train for the 9-th batch, train loss: 0.6919276118278503:   5%|▋              | 7/146 [00:03<00:39,  3.49it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6919276118278503:   6%|▉              | 9/146 [00:03<00:29,  4.69it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6748557686805725:   3%|▌              | 4/119 [00:03<01:00,  1.89it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6748557686805725:   4%|▋              | 5/119 [00:03<00:44,  2.56it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6919369101524353:   4%|▌              | 6/151 [00:03<00:52,  2.76it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6919369101524353:   5%|▋              | 7/151 [00:03<00:42,  3.38it/s]Epoch: 1, train for the 2-th batch, train loss: 0.6913686990737915:   0%|               | 1/237 [00:02<09:18,  2.37s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6913686990737915:   1%|▏              | 2/237 [00:02<04:03,  1.04s/it]Epoch: 1, train for the 10-th batch, train loss: 0.6921049356460571:   6%|▊             | 9/146 [00:03<00:29,  4.69it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6921049356460571:   7%|▉            | 10/146 [00:03<00:25,  5.31it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6625056266784668:   4%|▋              | 5/119 [00:03<00:44,  2.56it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6625056266784668:   5%|▊              | 6/119 [00:03<00:37,  3.04it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6922738552093506:   5%|▋              | 7/151 [00:03<00:42,  3.38it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6922738552093506:   5%|▊              | 8/151 [00:03<00:40,  3.53it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6543843150138855:   5%|▊              | 6/119 [00:03<00:37,  3.04it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6925178170204163:   7%|▉            | 10/146 [00:04<00:25,  5.31it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6925178170204163:   8%|▉            | 11/146 [00:04<00:27,  4.88it/s]Epoch: 1, train for the 9-th batch, train loss: 0.692197859287262:   5%|▊               | 8/151 [00:03<00:40,  3.53it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6929596066474915:   0%|                       | 0/241 [00:02<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6929596066474915:   0%|               | 1/241 [00:02<09:27,  2.36s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6914303302764893:   1%|▏              | 2/237 [00:02<04:03,  1.04s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6914303302764893:   1%|▏              | 3/237 [00:02<02:49,  1.38it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6434661746025085:   5%|▊              | 6/119 [00:03<00:37,  3.04it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6434661746025085:   7%|█              | 8/119 [00:03<00:25,  4.35it/s]Epoch: 1, train for the 12-th batch, train loss: 0.692290723323822:   8%|█             | 11/146 [00:04<00:27,  4.88it/s]Epoch: 1, train for the 12-th batch, train loss: 0.692290723323822:   8%|█▏            | 12/146 [00:04<00:25,  5.31it/s]Epoch: 1, train for the 2-th batch, train loss: 0.6955755352973938:   0%|               | 1/241 [00:02<09:27,  2.36s/it]Epoch: 1, train for the 10-th batch, train loss: 0.6921953558921814:   5%|▋             | 8/151 [00:03<00:40,  3.53it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6906660199165344:   1%|▏              | 3/237 [00:02<02:49,  1.38it/s]Epoch: 1, train for the 2-th batch, train loss: 0.6955755352973938:   1%|               | 2/241 [00:02<04:12,  1.06s/it]Epoch: 1, train for the 10-th batch, train loss: 0.6921953558921814:   7%|▊            | 10/151 [00:03<00:30,  4.67it/s]Epoch: 1, train for the 9-th batch, train loss: 0.641822099685669:   7%|█               | 8/119 [00:03<00:25,  4.35it/s]Epoch: 1, train for the 9-th batch, train loss: 0.641822099685669:   8%|█▏              | 9/119 [00:03<00:21,  5.08it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6919694542884827:   8%|█            | 12/146 [00:04<00:25,  5.31it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6919694542884827:   9%|█▏           | 13/146 [00:04<00:22,  5.96it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6884830594062805:   1%|▏              | 3/237 [00:03<02:49,  1.38it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6884830594062805:   2%|▎              | 5/237 [00:03<01:26,  2.69it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6922411918640137:   7%|▊            | 10/151 [00:04<00:30,  4.67it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6922411918640137:   7%|▉            | 11/151 [00:04<00:26,  5.24it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6317307353019714:   8%|█             | 9/119 [00:04<00:21,  5.08it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6317307353019714:   8%|█            | 10/119 [00:04<00:19,  5.70it/s]Epoch: 1, train for the 14-th batch, train loss: 0.69222092628479:   9%|█▎             | 13/146 [00:04<00:22,  5.96it/s]Epoch: 1, train for the 14-th batch, train loss: 0.69222092628479:  10%|█▍             | 14/146 [00:04<00:20,  6.55it/s]Epoch: 1, train for the 6-th batch, train loss: 0.688471794128418:   2%|▎               | 5/237 [00:03<01:26,  2.69it/s]Epoch: 1, train for the 12-th batch, train loss: 0.69290691614151:   7%|█              | 11/151 [00:04<00:26,  5.24it/s]Epoch: 1, train for the 6-th batch, train loss: 0.688471794128418:   3%|▍               | 6/237 [00:03<01:08,  3.36it/s]Epoch: 1, train for the 12-th batch, train loss: 0.69290691614151:   8%|█▏             | 12/151 [00:04<00:23,  5.95it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6268618106842041:   8%|█            | 10/119 [00:04<00:19,  5.70it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6268618106842041:   9%|█▏           | 11/119 [00:04<00:16,  6.40it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6915716528892517:  10%|█▏           | 14/146 [00:04<00:20,  6.55it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6915716528892517:  10%|█▎           | 15/146 [00:04<00:18,  6.92it/s]Epoch: 1, train for the 12-th batch, train loss: 0.620071530342102:   9%|█▎            | 11/119 [00:04<00:16,  6.40it/s]Epoch: 1, train for the 12-th batch, train loss: 0.620071530342102:  10%|█▍            | 12/119 [00:04<00:15,  6.95it/s]Epoch: 1, train for the 3-th batch, train loss: 0.6889529228210449:   1%|               | 2/241 [00:02<04:12,  1.06s/it]Epoch: 1, train for the 13-th batch, train loss: 0.6933109760284424:   8%|█            | 12/151 [00:04<00:23,  5.95it/s]Epoch: 1, train for the 3-th batch, train loss: 0.6889529228210449:   1%|▏              | 3/241 [00:02<03:02,  1.30it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6933109760284424:   9%|█            | 13/151 [00:04<00:24,  5.67it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6906705498695374:  10%|█▎           | 15/146 [00:04<00:18,  6.92it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6906705498695374:  11%|█▍           | 16/146 [00:04<00:17,  7.49it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6902485489845276:   3%|▍              | 6/237 [00:03<01:08,  3.36it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6902485489845276:   3%|▍              | 7/237 [00:03<01:04,  3.56it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6043916344642639:  10%|█▎           | 12/119 [00:04<00:15,  6.95it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6043916344642639:  11%|█▍           | 13/119 [00:04<00:14,  7.43it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6907978653907776:  11%|█▍           | 16/146 [00:04<00:17,  7.49it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6907978653907776:  12%|█▌           | 17/146 [00:04<00:16,  8.02it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6883886456489563:   1%|▏              | 3/241 [00:03<03:02,  1.30it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6883886456489563:   2%|▏              | 4/241 [00:03<02:02,  1.94it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6931553483009338:   9%|█            | 13/151 [00:04<00:24,  5.67it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6931553483009338:   9%|█▏           | 14/151 [00:04<00:23,  5.73it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6904875040054321:  12%|█▌           | 17/146 [00:05<00:16,  8.02it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6879867911338806:   2%|▏              | 4/241 [00:03<02:02,  1.94it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6909773349761963:  12%|█▌           | 17/146 [00:05<00:16,  8.02it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6909773349761963:  13%|█▋           | 19/146 [00:05<00:13,  9.58it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6022683382034302:  11%|█▍           | 13/119 [00:04<00:14,  7.43it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6022683382034302:  12%|█▌           | 14/119 [00:04<00:17,  5.89it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6877738833427429:   3%|▍              | 7/237 [00:03<01:04,  3.56it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6901561617851257:  13%|█▋           | 19/146 [00:05<00:13,  9.58it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6877738833427429:   3%|▌              | 8/237 [00:03<01:08,  3.34it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6901561617851257:  14%|█▊           | 20/146 [00:05<00:13,  9.44it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6898705959320068:   2%|▏              | 4/241 [00:03<02:02,  1.94it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6898705959320068:   2%|▎              | 6/241 [00:03<01:13,  3.19it/s]Epoch: 1, train for the 15-th batch, train loss: 0.5914003849029541:  12%|█▌           | 14/119 [00:04<00:17,  5.89it/s]Epoch: 1, train for the 15-th batch, train loss: 0.5914003849029541:  13%|█▋           | 15/119 [00:04<00:16,  6.38it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6857671737670898:   2%|▎              | 6/241 [00:03<01:13,  3.19it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6866384744644165:   3%|▌              | 8/237 [00:03<01:08,  3.34it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6910127401351929:  14%|█▊           | 20/146 [00:05<00:13,  9.44it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6866384744644165:   4%|▌              | 9/237 [00:03<00:55,  4.08it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6910127401351929:  14%|█▊           | 21/146 [00:05<00:13,  9.18it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6918540000915527:   9%|█▏           | 14/151 [00:04<00:23,  5.73it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6918540000915527:  10%|█▎           | 15/151 [00:04<00:31,  4.27it/s]Epoch: 1, train for the 16-th batch, train loss: 0.5962814092636108:  13%|█▋           | 15/119 [00:04<00:16,  6.38it/s]Epoch: 1, train for the 16-th batch, train loss: 0.5962814092636108:  13%|█▋           | 16/119 [00:04<00:14,  6.95it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6899009346961975:   2%|▎              | 6/241 [00:03<01:13,  3.19it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6899009346961975:   3%|▍              | 8/241 [00:03<00:51,  4.52it/s]Epoch: 1, train for the 10-th batch, train loss: 0.685300886631012:   4%|▌              | 9/237 [00:03<00:55,  4.08it/s]Epoch: 1, train for the 10-th batch, train loss: 0.685300886631012:   4%|▌             | 10/237 [00:03<00:46,  4.91it/s]Epoch: 1, train for the 17-th batch, train loss: 0.5664502382278442:  13%|█▋           | 16/119 [00:05<00:14,  6.95it/s]Epoch: 1, train for the 17-th batch, train loss: 0.5664502382278442:  14%|█▊           | 17/119 [00:05<00:13,  7.57it/s]Epoch: 1, train for the 16-th batch, train loss: 0.692074716091156:  10%|█▍            | 15/151 [00:05<00:31,  4.27it/s]Epoch: 1, train for the 16-th batch, train loss: 0.692074716091156:  11%|█▍            | 16/151 [00:05<00:27,  4.94it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6851930618286133:   4%|▌            | 10/237 [00:04<00:46,  4.91it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6851930618286133:   5%|▌            | 11/237 [00:04<00:39,  5.77it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6873307824134827:   3%|▍              | 8/241 [00:03<00:51,  4.52it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6873307824134827:   4%|▌              | 9/241 [00:03<00:45,  5.04it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6852676868438721:   4%|▌             | 9/241 [00:03<00:45,  5.04it/s]Epoch: 1, train for the 18-th batch, train loss: 0.5653783082962036:  14%|█▊           | 17/119 [00:05<00:13,  7.57it/s]Epoch: 1, train for the 18-th batch, train loss: 0.5653783082962036:  15%|█▉           | 18/119 [00:05<00:13,  7.22it/s]Epoch: 1, train for the 12-th batch, train loss: 0.686743438243866:   5%|▋             | 11/237 [00:04<00:39,  5.77it/s]Epoch: 1, train for the 12-th batch, train loss: 0.686743438243866:   5%|▋             | 12/237 [00:04<00:36,  6.22it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6906999945640564:  14%|█▊           | 21/146 [00:05<00:13,  9.18it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6906999945640564:  15%|█▉           | 22/146 [00:05<00:22,  5.62it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6918354630470276:  11%|█▍           | 16/151 [00:05<00:27,  4.94it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6918354630470276:  11%|█▍           | 17/151 [00:05<00:27,  4.93it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6795193552970886:   4%|▌             | 9/241 [00:03<00:45,  5.04it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6795193552970886:   5%|▌            | 11/241 [00:03<00:35,  6.39it/s]Epoch: 1, train for the 19-th batch, train loss: 0.555121660232544:  15%|██            | 18/119 [00:05<00:13,  7.22it/s]Epoch: 1, train for the 19-th batch, train loss: 0.555121660232544:  16%|██▏           | 19/119 [00:05<00:13,  7.46it/s]Epoch: 1, train for the 23-th batch, train loss: 0.689591646194458:  15%|██            | 22/146 [00:05<00:22,  5.62it/s]Epoch: 1, train for the 23-th batch, train loss: 0.689591646194458:  16%|██▏           | 23/146 [00:05<00:20,  6.15it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6875860691070557:   5%|▋            | 12/237 [00:04<00:36,  6.22it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6875860691070557:   5%|▋            | 13/237 [00:04<00:36,  6.19it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6925706267356873:  11%|█▍           | 17/151 [00:05<00:27,  4.93it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6820532083511353:   5%|▌            | 11/241 [00:03<00:35,  6.39it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6820532083511353:   5%|▋            | 12/241 [00:03<00:35,  6.53it/s]Epoch: 1, train for the 20-th batch, train loss: 0.5542899370193481:  16%|██           | 19/119 [00:05<00:13,  7.46it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6925706267356873:  12%|█▌           | 18/151 [00:05<00:25,  5.19it/s]Epoch: 1, train for the 20-th batch, train loss: 0.5542899370193481:  17%|██▏          | 20/119 [00:05<00:12,  7.88it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6889598965644836:   5%|▋            | 13/237 [00:04<00:36,  6.19it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6896719336509705:  16%|██           | 23/146 [00:05<00:20,  6.15it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6889598965644836:   6%|▊            | 14/237 [00:04<00:33,  6.66it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6896719336509705:  16%|██▏          | 24/146 [00:05<00:19,  6.38it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6908954381942749:  12%|█▌           | 18/151 [00:05<00:25,  5.19it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6908954381942749:  13%|█▋           | 19/151 [00:05<00:22,  5.86it/s]Epoch: 1, train for the 21-th batch, train loss: 0.5579237937927246:  17%|██▏          | 20/119 [00:05<00:12,  7.88it/s]Epoch: 1, train for the 21-th batch, train loss: 0.5579237937927246:  18%|██▎          | 21/119 [00:05<00:13,  7.51it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6903837323188782:   6%|▊            | 14/237 [00:04<00:33,  6.66it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6903837323188782:   6%|▊            | 15/237 [00:04<00:30,  7.35it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6808117032051086:   5%|▋            | 12/241 [00:04<00:35,  6.53it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6808117032051086:   5%|▋            | 13/241 [00:04<00:36,  6.18it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6954643130302429:   0%|                       | 0/383 [00:02<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6954643130302429:   0%|               | 1/383 [00:02<17:08,  2.69s/it]Epoch: 1, train for the 20-th batch, train loss: 0.6908237934112549:  13%|█▋           | 19/151 [00:05<00:22,  5.86it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6908237934112549:  13%|█▋           | 20/151 [00:05<00:20,  6.29it/s]Epoch: 1, train for the 22-th batch, train loss: 0.5364669561386108:  18%|██▎          | 21/119 [00:05<00:13,  7.51it/s]Epoch: 1, train for the 22-th batch, train loss: 0.5364669561386108:  18%|██▍          | 22/119 [00:05<00:12,  7.66it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6850526332855225:   5%|▋            | 13/241 [00:04<00:36,  6.18it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6836215257644653:   6%|▊            | 15/237 [00:04<00:30,  7.35it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6850526332855225:   6%|▊            | 14/241 [00:04<00:33,  6.68it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6900964379310608:  16%|██▏          | 24/146 [00:06<00:19,  6.38it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6836215257644653:   7%|▉            | 16/237 [00:04<00:29,  7.47it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6900964379310608:  17%|██▏          | 25/146 [00:06<00:21,  5.56it/s]Epoch: 1, train for the 2-th batch, train loss: 0.6908218264579773:   0%|               | 1/383 [00:02<17:08,  2.69s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6908218264579773:   1%|               | 2/383 [00:02<07:32,  1.19s/it]Epoch: 1, train for the 23-th batch, train loss: 0.5390170812606812:  18%|██▍          | 22/119 [00:05<00:12,  7.66it/s]Epoch: 1, train for the 23-th batch, train loss: 0.5390170812606812:  19%|██▌          | 23/119 [00:05<00:12,  7.71it/s]Epoch: 1, train for the 17-th batch, train loss: 0.683716356754303:   7%|▉             | 16/237 [00:04<00:29,  7.47it/s]Epoch: 1, train for the 17-th batch, train loss: 0.683716356754303:   7%|█             | 17/237 [00:04<00:29,  7.40it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6893233060836792:  17%|██▏          | 25/146 [00:06<00:21,  5.56it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6893233060836792:  18%|██▎          | 26/146 [00:06<00:20,  5.95it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6798098683357239:   6%|▊            | 14/241 [00:04<00:33,  6.68it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6905553936958313:  13%|█▋           | 20/151 [00:05<00:20,  6.29it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6798098683357239:   6%|▊            | 15/241 [00:04<00:34,  6.54it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6905553936958313:  14%|█▊           | 21/151 [00:05<00:22,  5.69it/s]Epoch: 1, train for the 24-th batch, train loss: 0.5423991084098816:  19%|██▌          | 23/119 [00:05<00:12,  7.71it/s]Epoch: 1, train for the 24-th batch, train loss: 0.5423991084098816:  20%|██▌          | 24/119 [00:05<00:11,  7.97it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6896017789840698:  14%|█▊           | 21/151 [00:05<00:22,  5.69it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6896017789840698:  15%|█▉           | 22/151 [00:05<00:20,  6.41it/s]Epoch: 1, train for the 25-th batch, train loss: 0.5115311145782471:  20%|██▌          | 24/119 [00:05<00:11,  7.97it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6756879091262817:   7%|▉            | 17/237 [00:05<00:29,  7.40it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6756879091262817:   8%|▉            | 18/237 [00:05<00:37,  5.79it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6883431673049927:  18%|██▎          | 26/146 [00:06<00:20,  5.95it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6883431673049927:  18%|██▍          | 27/146 [00:06<00:24,  4.87it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6904237866401672:  15%|█▉           | 22/151 [00:06<00:20,  6.41it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6904237866401672:  15%|█▉           | 23/151 [00:06<00:21,  6.07it/s]Epoch: 1, train for the 3-th batch, train loss: 0.693851888179779:   1%|                | 2/383 [00:03<07:32,  1.19s/it]Epoch: 1, train for the 16-th batch, train loss: 0.6715750694274902:   6%|▊            | 15/241 [00:04<00:34,  6.54it/s]Epoch: 1, train for the 26-th batch, train loss: 0.5211037397384644:  20%|██▌          | 24/119 [00:06<00:11,  7.97it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6715750694274902:   7%|▊            | 16/241 [00:04<00:44,  5.06it/s]Epoch: 1, train for the 3-th batch, train loss: 0.693851888179779:   1%|▏               | 3/383 [00:03<05:19,  1.19it/s]Epoch: 1, train for the 26-th batch, train loss: 0.5211037397384644:  22%|██▊          | 26/119 [00:06<00:11,  7.79it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6790826320648193:   8%|▉            | 18/237 [00:05<00:37,  5.79it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6790826320648193:   8%|█            | 19/237 [00:05<00:34,  6.39it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6874198913574219:  18%|██▍          | 27/146 [00:06<00:24,  4.87it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6908940076828003:  15%|█▉           | 23/151 [00:06<00:21,  6.07it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6874198913574219:  19%|██▍          | 28/146 [00:06<00:21,  5.56it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6908940076828003:  16%|██           | 24/151 [00:06<00:18,  6.68it/s]Epoch: 1, train for the 27-th batch, train loss: 0.5441939234733582:  22%|██▊          | 26/119 [00:06<00:11,  7.79it/s]Epoch: 1, train for the 27-th batch, train loss: 0.5441939234733582:  23%|██▉          | 27/119 [00:06<00:12,  7.48it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6833633780479431:   8%|█            | 19/237 [00:05<00:34,  6.39it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6833633780479431:   8%|█            | 20/237 [00:05<00:33,  6.55it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6917375326156616:  16%|██           | 24/151 [00:06<00:18,  6.68it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6905811429023743:   1%|               | 3/383 [00:03<05:19,  1.19it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6917375326156616:  17%|██▏          | 25/151 [00:06<00:17,  7.12it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6905811429023743:   1%|▏              | 4/383 [00:03<03:45,  1.68it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6902743577957153:  17%|██▏          | 25/151 [00:06<00:17,  7.12it/s]Epoch: 1, train for the 29-th batch, train loss: 0.688101589679718:  19%|██▋           | 28/146 [00:06<00:21,  5.56it/s]Epoch: 1, train for the 29-th batch, train loss: 0.688101589679718:  20%|██▊           | 29/146 [00:06<00:22,  5.20it/s]Epoch: 1, train for the 21-th batch, train loss: 0.683721661567688:   8%|█▏            | 20/237 [00:05<00:33,  6.55it/s]Epoch: 1, train for the 21-th batch, train loss: 0.683721661567688:   9%|█▏            | 21/237 [00:05<00:34,  6.34it/s]Epoch: 1, train for the 28-th batch, train loss: 0.5517826080322266:  23%|██▉          | 27/119 [00:06<00:12,  7.48it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6672145128250122:   7%|▊            | 16/241 [00:05<00:44,  5.06it/s]Epoch: 1, train for the 28-th batch, train loss: 0.5517826080322266:  24%|███          | 28/119 [00:06<00:14,  6.43it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6672145128250122:   7%|▉            | 17/241 [00:05<00:55,  4.04it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6907283067703247:   1%|▏              | 4/383 [00:03<03:45,  1.68it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6907283067703247:   1%|▏              | 5/383 [00:03<02:46,  2.27it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6896946430206299:  20%|██▌          | 29/146 [00:07<00:22,  5.20it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6896946430206299:  21%|██▋          | 30/146 [00:07<00:19,  6.07it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6903045773506165:  17%|██▏          | 25/151 [00:06<00:17,  7.12it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6903045773506165:  18%|██▎          | 27/151 [00:06<00:15,  7.86it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6650307178497314:   7%|▉            | 17/241 [00:05<00:55,  4.04it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6785888671875:   9%|█▍              | 21/237 [00:05<00:34,  6.34it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6650307178497314:   7%|▉            | 18/241 [00:05<00:47,  4.74it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6785888671875:   9%|█▍              | 22/237 [00:05<00:32,  6.52it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6888416409492493:   1%|▏              | 5/383 [00:03<02:46,  2.27it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6888416409492493:   2%|▏              | 6/383 [00:03<02:09,  2.92it/s]Epoch: 1, train for the 29-th batch, train loss: 0.5080628991127014:  24%|███          | 28/119 [00:06<00:14,  6.43it/s]Epoch: 1, train for the 29-th batch, train loss: 0.5080628991127014:  24%|███▏         | 29/119 [00:06<00:14,  6.19it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6911153793334961:  18%|██▎          | 27/151 [00:06<00:15,  7.86it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6911153793334961:  19%|██▍          | 28/151 [00:06<00:15,  7.82it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6705319285392761:   7%|▉            | 18/241 [00:05<00:47,  4.74it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6705319285392761:   8%|█            | 19/241 [00:05<00:40,  5.52it/s]Epoch: 1, train for the 30-th batch, train loss: 0.5010730028152466:  24%|███▏         | 29/119 [00:06<00:14,  6.19it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6901940107345581:   2%|▏              | 6/383 [00:03<02:09,  2.92it/s]Epoch: 1, train for the 30-th batch, train loss: 0.5010730028152466:  25%|███▎         | 30/119 [00:06<00:12,  6.85it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6901940107345581:   2%|▎              | 7/383 [00:03<01:41,  3.70it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6915433406829834:  19%|██▍          | 28/151 [00:06<00:15,  7.82it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6915433406829834:  19%|██▍          | 29/151 [00:06<00:15,  8.11it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6866953372955322:  21%|██▋          | 30/146 [00:07<00:19,  6.07it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6866953372955322:  21%|██▊          | 31/146 [00:07<00:25,  4.57it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6842291355133057:   2%|▎              | 7/383 [00:04<01:41,  3.70it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6842291355133057:   2%|▎              | 8/383 [00:04<01:22,  4.57it/s]Epoch: 1, train for the 31-th batch, train loss: 0.4996492266654968:  25%|███▎         | 30/119 [00:06<00:12,  6.85it/s]Epoch: 1, train for the 31-th batch, train loss: 0.4996492266654968:  26%|███▍         | 31/119 [00:06<00:12,  6.84it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6772826910018921:   9%|█▏           | 22/237 [00:06<00:32,  6.52it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6772826910018921:  10%|█▎           | 23/237 [00:06<00:45,  4.69it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6873688101768494:  21%|██▊          | 31/146 [00:07<00:25,  4.57it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6906538009643555:  19%|██▍          | 29/151 [00:07<00:15,  8.11it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6873688101768494:  22%|██▊          | 32/146 [00:07<00:21,  5.37it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6906538009643555:  20%|██▌          | 30/151 [00:07<00:17,  7.00it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6553226113319397:   8%|█            | 19/241 [00:05<00:40,  5.52it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6553226113319397:   8%|█            | 20/241 [00:05<00:46,  4.74it/s]Epoch: 1, train for the 32-th batch, train loss: 0.503924548625946:  26%|███▋          | 31/119 [00:07<00:12,  6.84it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6901273727416992:   2%|▎              | 8/383 [00:04<01:22,  4.57it/s]Epoch: 1, train for the 32-th batch, train loss: 0.503924548625946:  27%|███▊          | 32/119 [00:07<00:11,  7.37it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6901273727416992:   2%|▎              | 9/383 [00:04<01:13,  5.08it/s]Epoch: 1, train for the 33-th batch, train loss: 0.687063992023468:  22%|███           | 32/146 [00:07<00:21,  5.37it/s]Epoch: 1, train for the 33-th batch, train loss: 0.687063992023468:  23%|███▏          | 33/146 [00:07<00:19,  5.84it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6785188317298889:  10%|█▎           | 23/237 [00:06<00:45,  4.69it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6908412575721741:  20%|██▌          | 30/151 [00:07<00:17,  7.00it/s]Epoch: 1, train for the 33-th batch, train loss: 0.5442340970039368:  27%|███▍         | 32/119 [00:07<00:11,  7.37it/s]Epoch: 1, train for the 33-th batch, train loss: 0.5442340970039368:  28%|███▌         | 33/119 [00:07<00:11,  7.27it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6908412575721741:  21%|██▋          | 31/151 [00:07<00:18,  6.54it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6785188317298889:  10%|█▎           | 24/237 [00:06<00:44,  4.76it/s]Epoch: 1, train for the 34-th batch, train loss: 0.687424898147583:  23%|███▏          | 33/146 [00:07<00:19,  5.84it/s]Epoch: 1, train for the 34-th batch, train loss: 0.687424898147583:  23%|███▎          | 34/146 [00:07<00:17,  6.56it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6739967465400696:  10%|█▎           | 24/237 [00:06<00:44,  4.76it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6739967465400696:  11%|█▎           | 25/237 [00:06<00:38,  5.51it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6865142583847046:  23%|███          | 34/146 [00:07<00:17,  6.56it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6856140494346619:   8%|█            | 20/241 [00:05<00:46,  4.74it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6856140494346619:   9%|█▏           | 21/241 [00:05<00:53,  4.14it/s]Epoch: 1, train for the 10-th batch, train loss: 0.685843288898468:   2%|▎              | 9/383 [00:04<01:13,  5.08it/s]Epoch: 1, train for the 10-th batch, train loss: 0.685843288898468:   3%|▎             | 10/383 [00:04<01:27,  4.27it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6689152121543884:   9%|█▏           | 21/241 [00:06<00:53,  4.14it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6887381672859192:  21%|██▋          | 31/151 [00:07<00:18,  6.54it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6887381672859192:  21%|██▊          | 32/151 [00:07<00:23,  5.16it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6844889521598816:  23%|███          | 34/146 [00:08<00:17,  6.56it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6844889521598816:  25%|███▏         | 36/146 [00:08<00:15,  7.06it/s]Epoch: 1, train for the 34-th batch, train loss: 0.5116332173347473:  28%|███▌         | 33/119 [00:07<00:11,  7.27it/s]Epoch: 1, train for the 34-th batch, train loss: 0.5116332173347473:  29%|███▋         | 34/119 [00:07<00:16,  5.06it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6654079556465149:   9%|█▏           | 21/241 [00:06<00:53,  4.14it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6654079556465149:  10%|█▏           | 23/241 [00:06<00:38,  5.61it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6619676947593689:  11%|█▎           | 25/237 [00:06<00:38,  5.51it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6815216541290283:   3%|▎            | 10/383 [00:04<01:27,  4.27it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6815216541290283:   3%|▎            | 11/383 [00:04<01:22,  4.50it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6619676947593689:  11%|█▍           | 26/237 [00:06<00:43,  4.86it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6886614561080933:  21%|██▊          | 32/151 [00:07<00:23,  5.16it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6886614561080933:  22%|██▊          | 33/151 [00:07<00:20,  5.74it/s]Epoch: 1, train for the 24-th batch, train loss: 0.67650306224823:  10%|█▍             | 23/241 [00:06<00:38,  5.61it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6850486397743225:  25%|███▏         | 36/146 [00:08<00:15,  7.06it/s]Epoch: 1, train for the 24-th batch, train loss: 0.67650306224823:  10%|█▍             | 24/241 [00:06<00:35,  6.15it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6850486397743225:  25%|███▎         | 37/146 [00:08<00:15,  6.94it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6750326156616211:   3%|▎            | 11/383 [00:04<01:22,  4.50it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6750326156616211:   3%|▍            | 12/383 [00:04<01:16,  4.84it/s]Epoch: 1, train for the 38-th batch, train loss: 0.6845793128013611:  25%|███▎         | 37/146 [00:08<00:15,  6.94it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6886085271835327:  22%|██▊          | 33/151 [00:07<00:20,  5.74it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6886085271835327:  23%|██▉          | 34/151 [00:07<00:19,  5.98it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6798183917999268:   3%|▍            | 12/383 [00:04<01:16,  4.84it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6798183917999268:   3%|▍            | 13/383 [00:04<01:09,  5.35it/s]Epoch: 1, train for the 35-th batch, train loss: 0.5099931955337524:  29%|███▋         | 34/119 [00:07<00:16,  5.06it/s]Epoch: 1, train for the 35-th batch, train loss: 0.5099931955337524:  29%|███▊         | 35/119 [00:07<00:21,  4.00it/s]Epoch: 1, train for the 27-th batch, train loss: 0.671576976776123:  11%|█▌            | 26/237 [00:06<00:43,  4.86it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6748461723327637:  10%|█▎           | 24/241 [00:06<00:35,  6.15it/s]Epoch: 1, train for the 27-th batch, train loss: 0.671576976776123:  11%|█▌            | 27/237 [00:06<00:52,  3.99it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6748461723327637:  10%|█▎           | 25/241 [00:06<00:41,  5.20it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6844448447227478:  25%|███▎         | 37/146 [00:08<00:15,  6.94it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6844448447227478:  27%|███▍         | 39/146 [00:08<00:15,  6.82it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6878610849380493:  23%|██▉          | 34/151 [00:07<00:19,  5.98it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6878610849380493:  23%|███          | 35/151 [00:08<00:20,  5.54it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6861404180526733:   3%|▍            | 13/383 [00:05<01:09,  5.35it/s]Epoch: 1, train for the 36-th batch, train loss: 0.5025727152824402:  29%|███▊         | 35/119 [00:08<00:21,  4.00it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6861404180526733:   4%|▍            | 14/383 [00:05<01:08,  5.36it/s]Epoch: 1, train for the 36-th batch, train loss: 0.5025727152824402:  30%|███▉         | 36/119 [00:08<00:18,  4.44it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6800395250320435:  10%|█▎           | 25/241 [00:06<00:41,  5.20it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6775943040847778:  11%|█▍           | 27/237 [00:07<00:52,  3.99it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6800395250320435:  11%|█▍           | 26/241 [00:06<00:38,  5.56it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6775943040847778:  12%|█▌           | 28/237 [00:07<00:46,  4.45it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6844692826271057:  27%|███▍         | 39/146 [00:08<00:15,  6.82it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6844692826271057:  27%|███▌         | 40/146 [00:08<00:15,  6.91it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6888717412948608:  23%|███          | 35/151 [00:08<00:20,  5.54it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6888717412948608:  24%|███          | 36/151 [00:08<00:19,  5.86it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6681458950042725:  11%|█▍           | 26/241 [00:06<00:38,  5.56it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6681458950042725:  11%|█▍           | 27/241 [00:06<00:38,  5.61it/s]Epoch: 1, train for the 37-th batch, train loss: 0.5140180587768555:  30%|███▉         | 36/119 [00:08<00:18,  4.44it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6908580660820007:  24%|███          | 36/151 [00:08<00:19,  5.86it/s]Epoch: 1, train for the 37-th batch, train loss: 0.5140180587768555:  31%|████         | 37/119 [00:08<00:18,  4.49it/s]Epoch: 1, train for the 29-th batch, train loss: 0.661996066570282:  12%|█▋            | 28/237 [00:07<00:46,  4.45it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6908580660820007:  25%|███▏         | 37/151 [00:08<00:19,  5.87it/s]Epoch: 1, train for the 29-th batch, train loss: 0.661996066570282:  12%|█▋            | 29/237 [00:07<00:46,  4.52it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6815719604492188:  27%|███▌         | 40/146 [00:08<00:15,  6.91it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6748449802398682:   4%|▍            | 14/383 [00:05<01:08,  5.36it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6815719604492188:  28%|███▋         | 41/146 [00:08<00:17,  5.86it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6748449802398682:   4%|▌            | 15/383 [00:05<01:20,  4.57it/s]Epoch: 1, train for the 38-th batch, train loss: 0.505577564239502:  31%|████▎         | 37/119 [00:08<00:18,  4.49it/s]Epoch: 1, train for the 38-th batch, train loss: 0.505577564239502:  32%|████▍         | 38/119 [00:08<00:16,  5.05it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6408511996269226:  11%|█▍           | 27/241 [00:07<00:38,  5.61it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6578777432441711:  12%|█▌           | 29/237 [00:07<00:46,  4.52it/s]Epoch: 1, train for the 38-th batch, train loss: 0.6888128519058228:  25%|███▏         | 37/151 [00:08<00:19,  5.87it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6408511996269226:  12%|█▌           | 28/241 [00:07<00:38,  5.52it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6578777432441711:  13%|█▋           | 30/237 [00:07<00:41,  5.03it/s]Epoch: 1, train for the 38-th batch, train loss: 0.6888128519058228:  25%|███▎         | 38/151 [00:08<00:19,  5.89it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6827685236930847:  28%|███▋         | 41/146 [00:08<00:17,  5.86it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6827685236930847:  29%|███▋         | 42/146 [00:09<00:16,  6.24it/s]Epoch: 1, train for the 16-th batch, train loss: 0.686430811882019:   4%|▌             | 15/383 [00:05<01:20,  4.57it/s]Epoch: 1, train for the 16-th batch, train loss: 0.686430811882019:   4%|▌             | 16/383 [00:05<01:14,  4.96it/s]Epoch: 1, train for the 39-th batch, train loss: 0.5215393900871277:  32%|████▏        | 38/119 [00:08<00:16,  5.05it/s]Epoch: 1, train for the 39-th batch, train loss: 0.5215393900871277:  33%|████▎        | 39/119 [00:08<00:13,  5.75it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6718084812164307:  12%|█▌           | 28/241 [00:07<00:38,  5.52it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6718084812164307:  12%|█▌           | 29/241 [00:07<00:34,  6.11it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6799586415290833:  29%|███▋         | 42/146 [00:09<00:16,  6.24it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6799586415290833:  29%|███▊         | 43/146 [00:09<00:15,  6.62it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6864159107208252:  25%|███▎         | 38/151 [00:08<00:19,  5.89it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6864159107208252:  26%|███▎         | 39/151 [00:08<00:18,  5.99it/s]Epoch: 1, train for the 40-th batch, train loss: 0.5490800738334656:  33%|████▎        | 39/119 [00:08<00:13,  5.75it/s]Epoch: 1, train for the 40-th batch, train loss: 0.5490800738334656:  34%|████▎        | 40/119 [00:08<00:12,  6.42it/s]Epoch: 1, train for the 41-th batch, train loss: 0.5577796101570129:  34%|████▎        | 40/119 [00:08<00:12,  6.42it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6878945827484131:  12%|█▌           | 29/241 [00:07<00:34,  6.11it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6878945827484131:  12%|█▌           | 30/241 [00:07<00:37,  5.56it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6612372994422913:  13%|█▋           | 30/237 [00:07<00:41,  5.03it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6612372994422913:  13%|█▋           | 31/237 [00:07<00:50,  4.11it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6826464533805847:   4%|▌            | 16/383 [00:05<01:14,  4.96it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6826464533805847:   4%|▌            | 17/383 [00:05<01:26,  4.22it/s]Epoch: 1, train for the 42-th batch, train loss: 0.5644547343254089:  34%|████▎        | 40/119 [00:08<00:12,  6.42it/s]Epoch: 1, train for the 42-th batch, train loss: 0.5644547343254089:  35%|████▌        | 42/119 [00:08<00:10,  7.68it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6844112873077393:  26%|███▎         | 39/151 [00:08<00:18,  5.99it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6844112873077393:  26%|███▍         | 40/151 [00:08<00:21,  5.21it/s]Epoch: 1, train for the 31-th batch, train loss: 0.641399621963501:  12%|█▋            | 30/241 [00:07<00:37,  5.56it/s]Epoch: 1, train for the 44-th batch, train loss: 0.6779223680496216:  29%|███▊         | 43/146 [00:09<00:15,  6.62it/s]Epoch: 1, train for the 31-th batch, train loss: 0.641399621963501:  13%|█▊            | 31/241 [00:07<00:34,  6.10it/s]Epoch: 1, train for the 44-th batch, train loss: 0.6779223680496216:  30%|███▉         | 44/146 [00:09<00:19,  5.12it/s]Epoch: 1, train for the 32-th batch, train loss: 0.677121102809906:  13%|█▊            | 31/237 [00:07<00:50,  4.11it/s]Epoch: 1, train for the 32-th batch, train loss: 0.677121102809906:  14%|█▉            | 32/237 [00:07<00:43,  4.69it/s]Epoch: 1, train for the 43-th batch, train loss: 0.5235655903816223:  35%|████▌        | 42/119 [00:09<00:10,  7.68it/s]Epoch: 1, train for the 43-th batch, train loss: 0.5235655903816223:  36%|████▋        | 43/119 [00:09<00:10,  7.44it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6843906044960022:   4%|▌            | 17/383 [00:06<01:26,  4.22it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6843906044960022:   5%|▌            | 18/383 [00:06<01:20,  4.52it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6861284971237183:  26%|███▍         | 40/151 [00:09<00:21,  5.21it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6819929480552673:  30%|███▉         | 44/146 [00:09<00:19,  5.12it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6294784545898438:  13%|█▋           | 31/241 [00:07<00:34,  6.10it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6861284971237183:  27%|███▌         | 41/151 [00:09<00:20,  5.34it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6819929480552673:  31%|████         | 45/146 [00:09<00:18,  5.59it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6294784545898438:  13%|█▋           | 32/241 [00:07<00:33,  6.26it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6565531492233276:  14%|█▊           | 32/237 [00:08<00:43,  4.69it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6565531492233276:  14%|█▊           | 33/237 [00:08<00:39,  5.11it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6746638417243958:  31%|████         | 45/146 [00:09<00:18,  5.59it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6585701107978821:   5%|▌            | 18/383 [00:06<01:20,  4.52it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6746638417243958:  32%|████         | 46/146 [00:09<00:16,  6.12it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6585701107978821:   5%|▋            | 19/383 [00:06<01:13,  4.93it/s]Epoch: 1, train for the 44-th batch, train loss: 0.4983387887477875:  36%|████▋        | 43/119 [00:09<00:10,  7.44it/s]Epoch: 1, train for the 44-th batch, train loss: 0.4983387887477875:  37%|████▊        | 44/119 [00:09<00:11,  6.27it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6441068053245544:  14%|█▊           | 33/237 [00:08<00:39,  5.11it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6850364804267883:  27%|███▌         | 41/151 [00:09<00:20,  5.34it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6441068053245544:  14%|█▊           | 34/237 [00:08<00:42,  4.73it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6743389964103699:  13%|█▋           | 32/241 [00:07<00:33,  6.26it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6850364804267883:  28%|███▌         | 42/151 [00:09<00:23,  4.54it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6743389964103699:  14%|█▊           | 33/241 [00:07<00:41,  5.01it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6641821265220642:   5%|▋            | 19/383 [00:06<01:13,  4.93it/s]Epoch: 1, train for the 45-th batch, train loss: 0.47387441992759705:  37%|████▍       | 44/119 [00:09<00:11,  6.27it/s]Epoch: 1, train for the 45-th batch, train loss: 0.47387441992759705:  38%|████▌       | 45/119 [00:09<00:11,  6.26it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6783212423324585:  32%|████         | 46/146 [00:09<00:16,  6.12it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6641821265220642:   5%|▋            | 20/383 [00:06<01:14,  4.84it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6783212423324585:  32%|████▏        | 47/146 [00:09<00:18,  5.46it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6613900661468506:  14%|█▊           | 34/237 [00:08<00:42,  4.73it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6613900661468506:  15%|█▉           | 35/237 [00:08<00:39,  5.17it/s]Epoch: 1, train for the 48-th batch, train loss: 0.6777830123901367:  32%|████▏        | 47/146 [00:10<00:18,  5.46it/s]Epoch: 1, train for the 48-th batch, train loss: 0.6777830123901367:  33%|████▎        | 48/146 [00:10<00:15,  6.19it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6413671970367432:  14%|█▊           | 33/241 [00:08<00:41,  5.01it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6848234534263611:  28%|███▌         | 42/151 [00:09<00:23,  4.54it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6413671970367432:  14%|█▊           | 34/241 [00:08<00:40,  5.11it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6848234534263611:  28%|███▋         | 43/151 [00:09<00:23,  4.62it/s]Epoch: 1, train for the 44-th batch, train loss: 0.680027425289154:  28%|███▉          | 43/151 [00:09<00:23,  4.62it/s]Epoch: 1, train for the 46-th batch, train loss: 0.5446685552597046:  38%|████▉        | 45/119 [00:09<00:11,  6.26it/s]Epoch: 1, train for the 44-th batch, train loss: 0.680027425289154:  29%|████          | 44/151 [00:09<00:20,  5.10it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6611121892929077:  14%|█▊           | 34/241 [00:08<00:40,  5.11it/s]Epoch: 1, train for the 46-th batch, train loss: 0.5446685552597046:  39%|█████        | 46/119 [00:09<00:14,  4.95it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6611121892929077:  15%|█▉           | 35/241 [00:08<00:39,  5.23it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6570900082588196:  15%|█▉           | 35/237 [00:08<00:39,  5.17it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6725098490715027:   5%|▋            | 20/383 [00:06<01:14,  4.84it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6747239828109741:  33%|████▎        | 48/146 [00:10<00:15,  6.19it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6570900082588196:  15%|█▉           | 36/237 [00:08<00:42,  4.74it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6747239828109741:  34%|████▎        | 49/146 [00:10<00:17,  5.51it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6725098490715027:   5%|▋            | 21/383 [00:06<01:30,  4.00it/s]Epoch: 1, train for the 47-th batch, train loss: 0.5927447080612183:  39%|█████        | 46/119 [00:09<00:14,  4.95it/s]Epoch: 1, train for the 47-th batch, train loss: 0.5927447080612183:  39%|█████▏       | 47/119 [00:09<00:13,  5.46it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6789929270744324:  29%|███▊         | 44/151 [00:09<00:20,  5.10it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6789929270744324:  30%|███▊         | 45/151 [00:09<00:19,  5.40it/s]Epoch: 1, train for the 36-th batch, train loss: 0.660404622554779:  15%|██            | 35/241 [00:08<00:39,  5.23it/s]Epoch: 1, train for the 36-th batch, train loss: 0.660404622554779:  15%|██            | 36/241 [00:08<00:37,  5.42it/s]Epoch: 1, train for the 50-th batch, train loss: 0.6723211407661438:  34%|████▎        | 49/146 [00:10<00:17,  5.51it/s]Epoch: 1, train for the 50-th batch, train loss: 0.6723211407661438:  34%|████▍        | 50/146 [00:10<00:16,  5.79it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6788504719734192:   5%|▋            | 21/383 [00:07<01:30,  4.00it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6658052206039429:  15%|█▉           | 36/237 [00:08<00:42,  4.74it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6788504719734192:   6%|▋            | 22/383 [00:07<01:24,  4.25it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6658052206039429:  16%|██           | 37/237 [00:08<00:42,  4.76it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6818314790725708:  30%|███▊         | 45/151 [00:10<00:19,  5.40it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6818314790725708:  30%|███▉         | 46/151 [00:10<00:18,  5.74it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6167357563972473:  15%|█▉           | 36/241 [00:08<00:37,  5.42it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6167357563972473:  15%|█▉           | 37/241 [00:08<00:36,  5.65it/s]Epoch: 1, train for the 38-th batch, train loss: 0.6554185152053833:  16%|██           | 37/237 [00:09<00:42,  4.76it/s]Epoch: 1, train for the 48-th batch, train loss: 0.532701849937439:  39%|█████▌        | 47/119 [00:10<00:13,  5.46it/s]Epoch: 1, train for the 48-th batch, train loss: 0.532701849937439:  40%|█████▋        | 48/119 [00:10<00:13,  5.08it/s]Epoch: 1, train for the 38-th batch, train loss: 0.6554185152053833:  16%|██           | 38/237 [00:09<00:36,  5.41it/s]Epoch: 1, train for the 51-th batch, train loss: 0.6750605702400208:  34%|████▍        | 50/146 [00:10<00:16,  5.79it/s]Epoch: 1, train for the 51-th batch, train loss: 0.6750605702400208:  35%|████▌        | 51/146 [00:10<00:17,  5.35it/s]Epoch: 1, train for the 38-th batch, train loss: 0.6381710767745972:  15%|█▉           | 37/241 [00:08<00:36,  5.65it/s]Epoch: 1, train for the 38-th batch, train loss: 0.6381710767745972:  16%|██           | 38/241 [00:08<00:31,  6.43it/s]Epoch: 1, train for the 49-th batch, train loss: 0.582549512386322:  40%|█████▋        | 48/119 [00:10<00:13,  5.08it/s]Epoch: 1, train for the 49-th batch, train loss: 0.582549512386322:  41%|█████▊        | 49/119 [00:10<00:12,  5.79it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6718463897705078:  35%|████▌        | 51/146 [00:10<00:17,  5.35it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6751935482025146:  16%|██           | 38/237 [00:09<00:36,  5.41it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6718463897705078:  36%|████▋        | 52/146 [00:10<00:15,  5.93it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6789559721946716:   6%|▋            | 22/383 [00:07<01:24,  4.25it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6751935482025146:  16%|██▏          | 39/237 [00:09<00:36,  5.48it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6789559721946716:   6%|▊            | 23/383 [00:07<01:33,  3.87it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6835538148880005:  30%|███▉         | 46/151 [00:10<00:18,  5.74it/s]Epoch: 1, train for the 39-th batch, train loss: 0.658586859703064:  16%|██▏           | 38/241 [00:08<00:31,  6.43it/s]Epoch: 1, train for the 50-th batch, train loss: 0.551712691783905:  41%|█████▊        | 49/119 [00:10<00:12,  5.79it/s]Epoch: 1, train for the 39-th batch, train loss: 0.658586859703064:  16%|██▎           | 39/241 [00:08<00:31,  6.51it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6835538148880005:  31%|████         | 47/151 [00:10<00:21,  4.77it/s]Epoch: 1, train for the 50-th batch, train loss: 0.551712691783905:  42%|█████▉        | 50/119 [00:10<00:10,  6.42it/s]Epoch: 1, train for the 53-th batch, train loss: 0.6722132563591003:  36%|████▋        | 52/146 [00:10<00:15,  5.93it/s]Epoch: 1, train for the 53-th batch, train loss: 0.6722132563591003:  36%|████▋        | 53/146 [00:10<00:14,  6.27it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6460064649581909:  16%|██▏          | 39/237 [00:09<00:36,  5.48it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6460064649581909:  17%|██▏          | 40/237 [00:09<00:34,  5.79it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6698030233383179:  16%|██           | 39/241 [00:09<00:31,  6.51it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6519227623939514:   6%|▊            | 23/383 [00:07<01:33,  3.87it/s]Epoch: 1, train for the 51-th batch, train loss: 0.5718179941177368:  42%|█████▍       | 50/119 [00:10<00:10,  6.42it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6698030233383179:  17%|██▏          | 40/241 [00:09<00:31,  6.47it/s]Epoch: 1, train for the 51-th batch, train loss: 0.5718179941177368:  43%|█████▌       | 51/119 [00:10<00:10,  6.39it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6519227623939514:   6%|▊            | 24/383 [00:07<01:26,  4.14it/s]Epoch: 1, train for the 54-th batch, train loss: 0.6683135628700256:  36%|████▋        | 53/146 [00:11<00:14,  6.27it/s]Epoch: 1, train for the 54-th batch, train loss: 0.6683135628700256:  37%|████▊        | 54/146 [00:11<00:13,  6.82it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6298208236694336:  17%|██▏          | 40/241 [00:09<00:31,  6.47it/s]Epoch: 1, train for the 25-th batch, train loss: 0.672733724117279:   6%|▉             | 24/383 [00:07<01:26,  4.14it/s]Epoch: 1, train for the 48-th batch, train loss: 0.6748902797698975:  31%|████         | 47/151 [00:10<00:21,  4.77it/s]Epoch: 1, train for the 25-th batch, train loss: 0.672733724117279:   7%|▉             | 25/383 [00:07<01:15,  4.77it/s]Epoch: 1, train for the 48-th batch, train loss: 0.6748902797698975:  32%|████▏        | 48/151 [00:10<00:24,  4.20it/s]Epoch: 1, train for the 52-th batch, train loss: 0.5599741339683533:  43%|█████▌       | 51/119 [00:10<00:10,  6.39it/s]Epoch: 1, train for the 52-th batch, train loss: 0.5599741339683533:  44%|█████▋       | 52/119 [00:10<00:10,  6.32it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6346965432167053:  17%|██▏          | 40/241 [00:09<00:31,  6.47it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6346965432167053:  17%|██▎          | 42/241 [00:09<00:27,  7.17it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6513422727584839:  17%|██▏          | 40/237 [00:09<00:34,  5.79it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6513422727584839:  17%|██▏          | 41/237 [00:09<00:43,  4.51it/s]Epoch: 1, train for the 55-th batch, train loss: 0.6734001636505127:  37%|████▊        | 54/146 [00:11<00:13,  6.82it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6813427805900574:  32%|████▏        | 48/151 [00:10<00:24,  4.20it/s]Epoch: 1, train for the 55-th batch, train loss: 0.6734001636505127:  38%|████▉        | 55/146 [00:11<00:16,  5.48it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6813427805900574:  32%|████▏        | 49/151 [00:10<00:21,  4.65it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6714299917221069:   7%|▊            | 25/383 [00:07<01:15,  4.77it/s]Epoch: 1, train for the 53-th batch, train loss: 0.5153856873512268:  44%|█████▋       | 52/119 [00:10<00:10,  6.32it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6714299917221069:   7%|▉            | 26/383 [00:07<01:11,  4.96it/s]Epoch: 1, train for the 53-th batch, train loss: 0.5153856873512268:  45%|█████▊       | 53/119 [00:10<00:10,  6.26it/s]Epoch: 1, train for the 43-th batch, train loss: 0.656545877456665:  17%|██▍           | 42/241 [00:09<00:27,  7.17it/s]Epoch: 1, train for the 43-th batch, train loss: 0.656545877456665:  18%|██▍           | 43/241 [00:09<00:27,  7.15it/s]Epoch: 1, train for the 56-th batch, train loss: 0.6633338928222656:  38%|████▉        | 55/146 [00:11<00:16,  5.48it/s]Epoch: 1, train for the 56-th batch, train loss: 0.6633338928222656:  38%|████▉        | 56/146 [00:11<00:15,  5.93it/s]Epoch: 1, train for the 50-th batch, train loss: 0.6717955470085144:  32%|████▏        | 49/151 [00:10<00:21,  4.65it/s]Epoch: 1, train for the 50-th batch, train loss: 0.6717955470085144:  33%|████▎        | 50/151 [00:10<00:20,  5.04it/s]Epoch: 1, train for the 54-th batch, train loss: 0.5644269585609436:  45%|█████▊       | 53/119 [00:10<00:10,  6.26it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6440620422363281:   7%|▉            | 26/383 [00:08<01:11,  4.96it/s]Epoch: 1, train for the 54-th batch, train loss: 0.5644269585609436:  45%|█████▉       | 54/119 [00:10<00:10,  6.27it/s]Epoch: 1, train for the 44-th batch, train loss: 0.6726495027542114:  18%|██▎          | 43/241 [00:09<00:27,  7.15it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6440620422363281:   7%|▉            | 27/383 [00:08<01:08,  5.17it/s]Epoch: 1, train for the 44-th batch, train loss: 0.6726495027542114:  18%|██▎          | 44/241 [00:09<00:26,  7.40it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6654972434043884:  17%|██▏          | 41/237 [00:10<00:43,  4.51it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6654972434043884:  18%|██▎          | 42/237 [00:10<00:49,  3.92it/s]Epoch: 1, train for the 55-th batch, train loss: 0.5098733305931091:  45%|█████▉       | 54/119 [00:11<00:10,  6.27it/s]Epoch: 1, train for the 55-th batch, train loss: 0.5098733305931091:  46%|██████       | 55/119 [00:11<00:11,  5.61it/s]Epoch: 1, train for the 51-th batch, train loss: 0.6782882809638977:  33%|████▎        | 50/151 [00:11<00:20,  5.04it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6033957004547119:  18%|██▎          | 44/241 [00:09<00:26,  7.40it/s]Epoch: 1, train for the 51-th batch, train loss: 0.6782882809638977:  34%|████▍        | 51/151 [00:11<00:21,  4.62it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6033957004547119:  19%|██▍          | 45/241 [00:09<00:31,  6.25it/s]Epoch: 1, train for the 57-th batch, train loss: 0.6611773371696472:  38%|████▉        | 56/146 [00:11<00:15,  5.93it/s]Epoch: 1, train for the 57-th batch, train loss: 0.6611773371696472:  39%|█████        | 57/146 [00:11<00:18,  4.73it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6571954488754272:  18%|██▎          | 42/237 [00:10<00:49,  3.92it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6289169192314148:   7%|▉            | 27/383 [00:08<01:08,  5.17it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6571954488754272:  18%|██▎          | 43/237 [00:10<00:46,  4.20it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6289169192314148:   7%|▉            | 28/383 [00:08<01:22,  4.33it/s]Epoch: 1, train for the 56-th batch, train loss: 0.5329126715660095:  46%|██████       | 55/119 [00:11<00:11,  5.61it/s]Epoch: 1, train for the 56-th batch, train loss: 0.5329126715660095:  47%|██████       | 56/119 [00:11<00:10,  5.85it/s]Epoch: 1, train for the 58-th batch, train loss: 0.6616426110267639:  39%|█████        | 57/146 [00:11<00:18,  4.73it/s]Epoch: 1, train for the 58-th batch, train loss: 0.6616426110267639:  40%|█████▏       | 58/146 [00:11<00:16,  5.26it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6828905940055847:  34%|████▍        | 51/151 [00:11<00:21,  4.62it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6100395321846008:  19%|██▍          | 45/241 [00:09<00:31,  6.25it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6100395321846008:  19%|██▍          | 46/241 [00:10<00:32,  5.99it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6828905940055847:  34%|████▍        | 52/151 [00:11<00:20,  4.78it/s]Epoch: 1, train for the 44-th batch, train loss: 0.6596606969833374:  18%|██▎          | 43/237 [00:10<00:46,  4.20it/s]Epoch: 1, train for the 44-th batch, train loss: 0.6596606969833374:  19%|██▍          | 44/237 [00:10<00:40,  4.73it/s]Epoch: 1, train for the 59-th batch, train loss: 0.6436948180198669:  40%|█████▏       | 58/146 [00:11<00:16,  5.26it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6174017786979675:   7%|▉            | 28/383 [00:08<01:22,  4.33it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6174017786979675:   8%|▉            | 29/383 [00:08<01:27,  4.06it/s]Epoch: 1, train for the 53-th batch, train loss: 0.6794619560241699:  34%|████▍        | 52/151 [00:11<00:20,  4.78it/s]Epoch: 1, train for the 53-th batch, train loss: 0.6794619560241699:  35%|████▌        | 53/151 [00:11<00:21,  4.57it/s]Epoch: 1, train for the 57-th batch, train loss: 0.5236488580703735:  47%|██████       | 56/119 [00:11<00:10,  5.85it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6590309143066406:  19%|██▍          | 44/237 [00:10<00:40,  4.73it/s]Epoch: 1, train for the 57-th batch, train loss: 0.5236488580703735:  48%|██████▏      | 57/119 [00:11<00:13,  4.73it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6590309143066406:  19%|██▍          | 45/237 [00:10<00:40,  4.71it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6519995927810669:  19%|██▍          | 46/241 [00:10<00:32,  5.99it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6519995927810669:  20%|██▌          | 47/241 [00:10<00:38,  5.02it/s]Epoch: 1, train for the 60-th batch, train loss: 0.6453628540039062:  40%|█████▏       | 58/146 [00:12<00:16,  5.26it/s]Epoch: 1, train for the 60-th batch, train loss: 0.6453628540039062:  41%|█████▎       | 60/146 [00:12<00:15,  5.67it/s]Epoch: 1, train for the 30-th batch, train loss: 0.615598201751709:   8%|█             | 29/383 [00:08<01:27,  4.06it/s]Epoch: 1, train for the 30-th batch, train loss: 0.615598201751709:   8%|█             | 30/383 [00:08<01:17,  4.57it/s]Epoch: 1, train for the 54-th batch, train loss: 0.6655824780464172:  35%|████▌        | 53/151 [00:11<00:21,  4.57it/s]Epoch: 1, train for the 58-th batch, train loss: 0.537956178188324:  48%|██████▋       | 57/119 [00:11<00:13,  4.73it/s]Epoch: 1, train for the 54-th batch, train loss: 0.6655824780464172:  36%|████▋        | 54/151 [00:11<00:19,  5.05it/s]Epoch: 1, train for the 58-th batch, train loss: 0.537956178188324:  49%|██████▊       | 58/119 [00:11<00:11,  5.20it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6193863153457642:  19%|██▍          | 45/237 [00:10<00:40,  4.71it/s]Epoch: 1, train for the 48-th batch, train loss: 0.6310949921607971:  20%|██▌          | 47/241 [00:10<00:38,  5.02it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6193863153457642:  19%|██▌          | 46/237 [00:10<00:37,  5.08it/s]Epoch: 1, train for the 48-th batch, train loss: 0.6310949921607971:  20%|██▌          | 48/241 [00:10<00:35,  5.41it/s]Epoch: 1, train for the 61-th batch, train loss: 0.6507822275161743:  41%|█████▎       | 60/146 [00:12<00:15,  5.67it/s]Epoch: 1, train for the 61-th batch, train loss: 0.6507822275161743:  42%|█████▍       | 61/146 [00:12<00:14,  6.00it/s]Epoch: 1, train for the 55-th batch, train loss: 0.6679431796073914:  36%|████▋        | 54/151 [00:11<00:19,  5.05it/s]Epoch: 1, train for the 55-th batch, train loss: 0.6679431796073914:  36%|████▋        | 55/151 [00:11<00:16,  5.65it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6417057514190674:  19%|██▌          | 46/237 [00:10<00:37,  5.08it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6417057514190674:  20%|██▌          | 47/237 [00:10<00:33,  5.72it/s]Epoch: 1, train for the 56-th batch, train loss: 0.6332443952560425:  36%|████▋        | 55/151 [00:12<00:16,  5.65it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6190183758735657:   8%|█            | 30/383 [00:09<01:17,  4.57it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6190183758735657:   8%|█            | 31/383 [00:09<01:24,  4.17it/s]Epoch: 1, train for the 59-th batch, train loss: 0.5665449500083923:  49%|██████▎      | 58/119 [00:12<00:11,  5.20it/s]Epoch: 1, train for the 48-th batch, train loss: 0.6366458535194397:  20%|██▌          | 47/237 [00:11<00:33,  5.72it/s]Epoch: 1, train for the 59-th batch, train loss: 0.5665449500083923:  50%|██████▍      | 59/119 [00:12<00:14,  4.28it/s]Epoch: 1, train for the 48-th batch, train loss: 0.6366458535194397:  20%|██▋          | 48/237 [00:11<00:34,  5.53it/s]Epoch: 1, train for the 57-th batch, train loss: 0.6485725045204163:  36%|████▋        | 55/151 [00:12<00:16,  5.65it/s]Epoch: 1, train for the 57-th batch, train loss: 0.6485725045204163:  38%|████▉        | 57/151 [00:12<00:14,  6.65it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6254380941390991:  42%|█████▍       | 61/146 [00:12<00:14,  6.00it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6589623093605042:   8%|█            | 31/383 [00:09<01:24,  4.17it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6254380941390991:  42%|█████▌       | 62/146 [00:12<00:17,  4.72it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6312721967697144:  20%|██▌          | 48/241 [00:10<00:35,  5.41it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6589623093605042:   8%|█            | 32/383 [00:09<01:15,  4.64it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6312721967697144:  20%|██▋          | 49/241 [00:10<00:45,  4.22it/s]Epoch: 1, train for the 60-th batch, train loss: 0.5349931120872498:  50%|██████▍      | 59/119 [00:12<00:14,  4.28it/s]Epoch: 1, train for the 60-th batch, train loss: 0.5349931120872498:  50%|██████▌      | 60/119 [00:12<00:11,  5.01it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6369451284408569:  20%|██▋          | 48/237 [00:11<00:34,  5.53it/s]Epoch: 1, train for the 63-th batch, train loss: 0.6322049498558044:  42%|█████▌       | 62/146 [00:12<00:17,  4.72it/s]Epoch: 1, train for the 63-th batch, train loss: 0.6322049498558044:  43%|█████▌       | 63/146 [00:12<00:15,  5.21it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6369451284408569:  21%|██▋          | 49/237 [00:11<00:33,  5.60it/s]Epoch: 1, train for the 58-th batch, train loss: 0.6487414240837097:  38%|████▉        | 57/151 [00:12<00:14,  6.65it/s]Epoch: 1, train for the 58-th batch, train loss: 0.6487414240837097:  38%|████▉        | 58/151 [00:12<00:15,  6.17it/s]Epoch: 1, train for the 50-th batch, train loss: 0.638617753982544:  21%|██▉           | 49/237 [00:11<00:33,  5.60it/s]Epoch: 1, train for the 50-th batch, train loss: 0.638617753982544:  21%|██▉           | 50/237 [00:11<00:29,  6.31it/s]Epoch: 1, train for the 50-th batch, train loss: 0.6156671047210693:  20%|██▋          | 49/241 [00:11<00:45,  4.22it/s]Epoch: 1, train for the 59-th batch, train loss: 0.6528686285018921:  38%|████▉        | 58/151 [00:12<00:15,  6.17it/s]Epoch: 1, train for the 50-th batch, train loss: 0.6156671047210693:  21%|██▋          | 50/241 [00:11<00:49,  3.85it/s]Epoch: 1, train for the 59-th batch, train loss: 0.6528686285018921:  39%|█████        | 59/151 [00:12<00:14,  6.19it/s]Epoch: 1, train for the 64-th batch, train loss: 0.6354463696479797:  43%|█████▌       | 63/146 [00:13<00:15,  5.21it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6634975075721741:   8%|█            | 32/383 [00:09<01:15,  4.64it/s]Epoch: 1, train for the 64-th batch, train loss: 0.6354463696479797:  44%|█████▋       | 64/146 [00:13<00:16,  4.99it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6634975075721741:   9%|█            | 33/383 [00:09<01:30,  3.87it/s]Epoch: 1, train for the 61-th batch, train loss: 0.5429494976997375:  50%|██████▌      | 60/119 [00:12<00:11,  5.01it/s]Epoch: 1, train for the 61-th batch, train loss: 0.5429494976997375:  51%|██████▋      | 61/119 [00:12<00:14,  3.99it/s]Epoch: 1, train for the 51-th batch, train loss: 0.629958987236023:  21%|██▉           | 50/237 [00:11<00:29,  6.31it/s]Epoch: 1, train for the 51-th batch, train loss: 0.629958987236023:  22%|███           | 51/237 [00:11<00:32,  5.70it/s]Epoch: 1, train for the 51-th batch, train loss: 0.6178447008132935:  21%|██▋          | 50/241 [00:11<00:49,  3.85it/s]Epoch: 1, train for the 65-th batch, train loss: 0.6453993320465088:  44%|█████▋       | 64/146 [00:13<00:16,  4.99it/s]Epoch: 1, train for the 65-th batch, train loss: 0.6453993320465088:  45%|█████▊       | 65/146 [00:13<00:14,  5.49it/s]Epoch: 1, train for the 51-th batch, train loss: 0.6178447008132935:  21%|██▊          | 51/241 [00:11<00:44,  4.28it/s]Epoch: 1, train for the 60-th batch, train loss: 0.6574066281318665:  39%|█████        | 59/151 [00:12<00:14,  6.19it/s]Epoch: 1, train for the 34-th batch, train loss: 0.632737398147583:   9%|█▏            | 33/383 [00:09<01:30,  3.87it/s]Epoch: 1, train for the 34-th batch, train loss: 0.632737398147583:   9%|█▏            | 34/383 [00:09<01:19,  4.39it/s]Epoch: 1, train for the 60-th batch, train loss: 0.6574066281318665:  40%|█████▏       | 60/151 [00:12<00:15,  5.95it/s]Epoch: 1, train for the 62-th batch, train loss: 0.5732836723327637:  51%|██████▋      | 61/119 [00:12<00:14,  3.99it/s]Epoch: 1, train for the 62-th batch, train loss: 0.5732836723327637:  52%|██████▊      | 62/119 [00:12<00:12,  4.52it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6207266449928284:  21%|██▊          | 51/241 [00:11<00:44,  4.28it/s]Epoch: 1, train for the 66-th batch, train loss: 0.6219077706336975:  45%|█████▊       | 65/146 [00:13<00:14,  5.49it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6207266449928284:  22%|██▊          | 52/241 [00:11<00:38,  4.94it/s]Epoch: 1, train for the 66-th batch, train loss: 0.6219077706336975:  45%|█████▉       | 66/146 [00:13<00:13,  5.91it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6082712411880493:  22%|██▊          | 51/237 [00:11<00:32,  5.70it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6082712411880493:  22%|██▊          | 52/237 [00:11<00:34,  5.36it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6354662179946899:   9%|█▏           | 34/383 [00:09<01:19,  4.39it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6354662179946899:   9%|█▏           | 35/383 [00:09<01:13,  4.73it/s]Epoch: 1, train for the 53-th batch, train loss: 0.5097892880439758:  22%|██▊          | 52/241 [00:11<00:38,  4.94it/s]Epoch: 1, train for the 61-th batch, train loss: 0.6462089419364929:  40%|█████▏       | 60/151 [00:12<00:15,  5.95it/s]Epoch: 1, train for the 61-th batch, train loss: 0.6462089419364929:  40%|█████▎       | 61/151 [00:12<00:17,  5.01it/s]Epoch: 1, train for the 63-th batch, train loss: 0.5407807230949402:  52%|██████▊      | 62/119 [00:12<00:12,  4.52it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6270061135292053:   9%|█▏           | 35/383 [00:10<01:13,  4.73it/s]Epoch: 1, train for the 63-th batch, train loss: 0.5407807230949402:  53%|██████▉      | 63/119 [00:13<00:12,  4.54it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6270061135292053:   9%|█▏           | 36/383 [00:10<01:04,  5.38it/s]Epoch: 1, train for the 67-th batch, train loss: 0.6289806962013245:  45%|█████▉       | 66/146 [00:13<00:13,  5.91it/s]Epoch: 1, train for the 67-th batch, train loss: 0.6289806962013245:  46%|█████▉       | 67/146 [00:13<00:14,  5.33it/s]Epoch: 1, train for the 53-th batch, train loss: 0.6178497672080994:  22%|██▊          | 52/237 [00:12<00:34,  5.36it/s]Epoch: 1, train for the 53-th batch, train loss: 0.6178497672080994:  22%|██▉          | 53/237 [00:12<00:37,  4.90it/s]Epoch: 1, train for the 54-th batch, train loss: 0.4708826243877411:  22%|██▊          | 52/241 [00:11<00:38,  4.94it/s]Epoch: 1, train for the 54-th batch, train loss: 0.4708826243877411:  22%|██▉          | 54/241 [00:11<00:33,  5.57it/s]Epoch: 1, train for the 64-th batch, train loss: 0.5465591549873352:  53%|██████▉      | 63/119 [00:13<00:12,  4.54it/s]Epoch: 1, train for the 64-th batch, train loss: 0.5465591549873352:  54%|██████▉      | 64/119 [00:13<00:10,  5.18it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6538587212562561:  40%|█████▎       | 61/151 [00:13<00:17,  5.01it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6082735061645508:   9%|█▏           | 36/383 [00:10<01:04,  5.38it/s]Epoch: 1, train for the 68-th batch, train loss: 0.6231057643890381:  46%|█████▉       | 67/146 [00:13<00:14,  5.33it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6538587212562561:  41%|█████▎       | 62/151 [00:13<00:17,  5.23it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6082735061645508:  10%|█▎           | 37/383 [00:10<01:01,  5.60it/s]Epoch: 1, train for the 68-th batch, train loss: 0.6231057643890381:  47%|██████       | 68/146 [00:13<00:13,  5.93it/s]Epoch: 1, train for the 55-th batch, train loss: 0.4667423963546753:  22%|██▉          | 54/241 [00:11<00:33,  5.57it/s]Epoch: 1, train for the 55-th batch, train loss: 0.4667423963546753:  23%|██▉          | 55/241 [00:11<00:30,  6.04it/s]Epoch: 1, train for the 65-th batch, train loss: 0.5258755087852478:  54%|██████▉      | 64/119 [00:13<00:10,  5.18it/s]Epoch: 1, train for the 54-th batch, train loss: 0.5721738338470459:  22%|██▉          | 53/237 [00:12<00:37,  4.90it/s]Epoch: 1, train for the 65-th batch, train loss: 0.5258755087852478:  55%|███████      | 65/119 [00:13<00:09,  5.65it/s]Epoch: 1, train for the 54-th batch, train loss: 0.5721738338470459:  23%|██▉          | 54/237 [00:12<00:35,  5.18it/s]Epoch: 1, train for the 69-th batch, train loss: 0.611284613609314:  47%|██████▌       | 68/146 [00:13<00:13,  5.93it/s]Epoch: 1, train for the 38-th batch, train loss: 0.630377471446991:  10%|█▎            | 37/383 [00:10<01:01,  5.60it/s]Epoch: 1, train for the 69-th batch, train loss: 0.611284613609314:  47%|██████▌       | 69/146 [00:13<00:12,  6.35it/s]Epoch: 1, train for the 63-th batch, train loss: 0.6438454389572144:  41%|█████▎       | 62/151 [00:13<00:17,  5.23it/s]Epoch: 1, train for the 38-th batch, train loss: 0.630377471446991:  10%|█▍            | 38/383 [00:10<00:57,  5.95it/s]Epoch: 1, train for the 63-th batch, train loss: 0.6438454389572144:  42%|█████▍       | 63/151 [00:13<00:15,  5.51it/s]Epoch: 1, train for the 66-th batch, train loss: 0.5316542983055115:  55%|███████      | 65/119 [00:13<00:09,  5.65it/s]Epoch: 1, train for the 66-th batch, train loss: 0.5316542983055115:  55%|███████▏     | 66/119 [00:13<00:09,  5.85it/s]Epoch: 1, train for the 70-th batch, train loss: 0.6228346824645996:  47%|██████▏      | 69/146 [00:13<00:12,  6.35it/s]Epoch: 1, train for the 64-th batch, train loss: 0.6326923370361328:  42%|█████▍       | 63/151 [00:13<00:15,  5.51it/s]Epoch: 1, train for the 70-th batch, train loss: 0.6228346824645996:  48%|██████▏      | 70/146 [00:13<00:11,  6.34it/s]Epoch: 1, train for the 56-th batch, train loss: 0.49917060136795044:  23%|██▋         | 55/241 [00:12<00:30,  6.04it/s]Epoch: 1, train for the 64-th batch, train loss: 0.6326923370361328:  42%|█████▌       | 64/151 [00:13<00:14,  5.81it/s]Epoch: 1, train for the 56-th batch, train loss: 0.49917060136795044:  23%|██▊         | 56/241 [00:12<00:34,  5.41it/s]Epoch: 1, train for the 67-th batch, train loss: 0.6120514273643494:  55%|███████▏     | 66/119 [00:13<00:09,  5.85it/s]Epoch: 1, train for the 67-th batch, train loss: 0.6120514273643494:  56%|███████▎     | 67/119 [00:13<00:08,  6.45it/s]Epoch: 1, train for the 39-th batch, train loss: 0.5764803886413574:  10%|█▎           | 38/383 [00:10<00:57,  5.95it/s]Epoch: 1, train for the 39-th batch, train loss: 0.5764803886413574:  10%|█▎           | 39/383 [00:10<01:06,  5.19it/s]Epoch: 1, train for the 55-th batch, train loss: 0.6270222663879395:  23%|██▉          | 54/237 [00:12<00:35,  5.18it/s]Epoch: 1, train for the 55-th batch, train loss: 0.6270222663879395:  23%|███          | 55/237 [00:12<00:42,  4.27it/s]Epoch: 1, train for the 57-th batch, train loss: 0.5152565836906433:  23%|███          | 56/241 [00:12<00:34,  5.41it/s]Epoch: 1, train for the 65-th batch, train loss: 0.6068964004516602:  42%|█████▌       | 64/151 [00:13<00:14,  5.81it/s]Epoch: 1, train for the 57-th batch, train loss: 0.5152565836906433:  24%|███          | 57/241 [00:12<00:33,  5.47it/s]Epoch: 1, train for the 65-th batch, train loss: 0.6068964004516602:  43%|█████▌       | 65/151 [00:13<00:15,  5.60it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5424759387969971:  56%|███████▎     | 67/119 [00:13<00:08,  6.45it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5424759387969971:  57%|███████▍     | 68/119 [00:13<00:07,  6.65it/s]Epoch: 1, train for the 40-th batch, train loss: 0.5951055884361267:  10%|█▎           | 39/383 [00:10<01:06,  5.19it/s]Epoch: 1, train for the 71-th batch, train loss: 0.588312029838562:  48%|██████▋       | 70/146 [00:14<00:11,  6.34it/s]Epoch: 1, train for the 71-th batch, train loss: 0.588312029838562:  49%|██████▊       | 71/146 [00:14<00:13,  5.38it/s]Epoch: 1, train for the 40-th batch, train loss: 0.5951055884361267:  10%|█▎           | 40/383 [00:10<01:02,  5.51it/s]Epoch: 1, train for the 41-th batch, train loss: 0.5473180413246155:  10%|█▎           | 40/383 [00:10<01:02,  5.51it/s]Epoch: 1, train for the 41-th batch, train loss: 0.5473180413246155:  11%|█▍           | 41/383 [00:10<00:55,  6.13it/s]Epoch: 1, train for the 58-th batch, train loss: 0.4373061954975128:  24%|███          | 57/241 [00:12<00:33,  5.47it/s]Epoch: 1, train for the 58-th batch, train loss: 0.4373061954975128:  24%|███▏         | 58/241 [00:12<00:37,  4.87it/s]Epoch: 1, train for the 69-th batch, train loss: 0.6258057951927185:  57%|███████▍     | 68/119 [00:13<00:07,  6.65it/s]Epoch: 1, train for the 69-th batch, train loss: 0.6258057951927185:  58%|███████▌     | 69/119 [00:13<00:09,  5.52it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5932059288024902:  49%|██████▎      | 71/146 [00:14<00:13,  5.38it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5932059288024902:  49%|██████▍      | 72/146 [00:14<00:15,  4.79it/s]Epoch: 1, train for the 42-th batch, train loss: 0.5926491618156433:  11%|█▍           | 41/383 [00:11<00:55,  6.13it/s]Epoch: 1, train for the 59-th batch, train loss: 0.641417920589447:  24%|███▎          | 58/241 [00:12<00:37,  4.87it/s]Epoch: 1, train for the 56-th batch, train loss: 0.6267480850219727:  23%|███          | 55/237 [00:13<00:42,  4.27it/s]Epoch: 1, train for the 42-th batch, train loss: 0.5926491618156433:  11%|█▍           | 42/383 [00:11<00:59,  5.69it/s]Epoch: 1, train for the 59-th batch, train loss: 0.641417920589447:  24%|███▍          | 59/241 [00:12<00:33,  5.45it/s]Epoch: 1, train for the 56-th batch, train loss: 0.6267480850219727:  24%|███          | 56/237 [00:13<00:54,  3.32it/s]Epoch: 1, train for the 70-th batch, train loss: 0.554372251033783:  58%|████████      | 69/119 [00:14<00:09,  5.52it/s]Epoch: 1, train for the 66-th batch, train loss: 0.6338199973106384:  43%|█████▌       | 65/151 [00:14<00:15,  5.60it/s]Epoch: 1, train for the 70-th batch, train loss: 0.554372251033783:  59%|████████▏     | 70/119 [00:14<00:08,  5.85it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5911335349082947:  49%|██████▍      | 72/146 [00:14<00:15,  4.79it/s]Epoch: 1, train for the 66-th batch, train loss: 0.6338199973106384:  44%|█████▋       | 66/151 [00:14<00:21,  3.91it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5911335349082947:  50%|██████▌      | 73/146 [00:14<00:13,  5.32it/s]Epoch: 1, train for the 67-th batch, train loss: 0.6432865858078003:  44%|█████▋       | 66/151 [00:14<00:21,  3.91it/s]Epoch: 1, train for the 71-th batch, train loss: 0.5448659658432007:  59%|███████▋     | 70/119 [00:14<00:08,  5.85it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6448236703872681:  11%|█▍           | 42/383 [00:11<00:59,  5.69it/s]Epoch: 1, train for the 71-th batch, train loss: 0.5448659658432007:  60%|███████▊     | 71/119 [00:14<00:07,  6.17it/s]Epoch: 1, train for the 67-th batch, train loss: 0.6432865858078003:  44%|█████▊       | 67/151 [00:14<00:18,  4.61it/s]Epoch: 1, train for the 57-th batch, train loss: 0.600898027420044:  24%|███▎          | 56/237 [00:13<00:54,  3.32it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6448236703872681:  11%|█▍           | 43/383 [00:11<01:02,  5.43it/s]Epoch: 1, train for the 57-th batch, train loss: 0.600898027420044:  24%|███▎          | 57/237 [00:13<00:48,  3.71it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5794523358345032:  50%|██████▌      | 73/146 [00:14<00:13,  5.32it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5794523358345032:  51%|██████▌      | 74/146 [00:14<00:13,  5.45it/s]Epoch: 1, train for the 58-th batch, train loss: 0.6182196140289307:  24%|███▏         | 57/237 [00:13<00:48,  3.71it/s]Epoch: 1, train for the 58-th batch, train loss: 0.6182196140289307:  24%|███▏         | 58/237 [00:13<00:42,  4.19it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5625973343849182:  60%|███████▊     | 71/119 [00:14<00:07,  6.17it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5625973343849182:  61%|███████▊     | 72/119 [00:14<00:08,  5.34it/s]Epoch: 1, train for the 60-th batch, train loss: 0.6828291416168213:  24%|███▏         | 59/241 [00:13<00:33,  5.45it/s]Epoch: 1, train for the 44-th batch, train loss: 0.5698503851890564:  11%|█▍           | 43/383 [00:11<01:02,  5.43it/s]Epoch: 1, train for the 60-th batch, train loss: 0.6828291416168213:  25%|███▏         | 60/241 [00:13<00:47,  3.80it/s]Epoch: 1, train for the 44-th batch, train loss: 0.5698503851890564:  11%|█▍           | 44/383 [00:11<01:10,  4.82it/s]Epoch: 1, train for the 75-th batch, train loss: 0.5891141891479492:  51%|██████▌      | 74/146 [00:15<00:13,  5.45it/s]Epoch: 1, train for the 75-th batch, train loss: 0.5891141891479492:  51%|██████▋      | 75/146 [00:15<00:14,  5.05it/s]Epoch: 1, train for the 68-th batch, train loss: 0.6267120838165283:  44%|█████▊       | 67/151 [00:14<00:18,  4.61it/s]Epoch: 1, train for the 59-th batch, train loss: 0.6299830675125122:  24%|███▏         | 58/237 [00:13<00:42,  4.19it/s]Epoch: 1, train for the 68-th batch, train loss: 0.6267120838165283:  45%|█████▊       | 68/151 [00:14<00:20,  4.01it/s]Epoch: 1, train for the 59-th batch, train loss: 0.6299830675125122:  25%|███▏         | 59/237 [00:13<00:37,  4.76it/s]Epoch: 1, train for the 76-th batch, train loss: 0.5697534084320068:  51%|██████▋      | 75/146 [00:15<00:14,  5.05it/s]Epoch: 1, train for the 61-th batch, train loss: 0.6229292154312134:  25%|███▏         | 60/241 [00:13<00:47,  3.80it/s]Epoch: 1, train for the 76-th batch, train loss: 0.5697534084320068:  52%|██████▊      | 76/146 [00:15<00:11,  5.86it/s]Epoch: 1, train for the 61-th batch, train loss: 0.6229292154312134:  25%|███▎         | 61/241 [00:13<00:40,  4.49it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5208116769790649:  61%|███████▊     | 72/119 [00:14<00:08,  5.34it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5208116769790649:  61%|███████▉     | 73/119 [00:14<00:09,  5.05it/s]Epoch: 1, train for the 45-th batch, train loss: 0.5698237419128418:  11%|█▍           | 44/383 [00:11<01:10,  4.82it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6506713628768921:  25%|███▎         | 61/241 [00:13<00:40,  4.49it/s]Epoch: 1, train for the 45-th batch, train loss: 0.5698237419128418:  12%|█▌           | 45/383 [00:11<01:10,  4.81it/s]Epoch: 1, train for the 60-th batch, train loss: 0.5917412638664246:  25%|███▏         | 59/237 [00:13<00:37,  4.76it/s]Epoch: 1, train for the 60-th batch, train loss: 0.5917412638664246:  25%|███▎         | 60/237 [00:13<00:35,  4.93it/s]Epoch: 1, train for the 63-th batch, train loss: 0.7171078324317932:  25%|███▎         | 61/241 [00:13<00:40,  4.49it/s]Epoch: 1, train for the 63-th batch, train loss: 0.7171078324317932:  26%|███▍         | 63/241 [00:13<00:30,  5.84it/s]Epoch: 1, train for the 61-th batch, train loss: 0.6098294854164124:  25%|███▎         | 60/237 [00:13<00:35,  4.93it/s]Epoch: 1, train for the 69-th batch, train loss: 0.6242634654045105:  45%|█████▊       | 68/151 [00:14<00:20,  4.01it/s]Epoch: 1, train for the 61-th batch, train loss: 0.6098294854164124:  26%|███▎         | 61/237 [00:13<00:31,  5.59it/s]Epoch: 1, train for the 69-th batch, train loss: 0.6242634654045105:  46%|█████▉       | 69/151 [00:14<00:22,  3.66it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5432369709014893:  61%|███████▉     | 73/119 [00:14<00:09,  5.05it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5432369709014893:  62%|████████     | 74/119 [00:14<00:09,  4.76it/s]Epoch: 1, train for the 46-th batch, train loss: 0.5992761850357056:  12%|█▌           | 45/383 [00:12<01:10,  4.81it/s]Epoch: 1, train for the 64-th batch, train loss: 0.3755732774734497:  26%|███▍         | 63/241 [00:13<00:30,  5.84it/s]Epoch: 1, train for the 46-th batch, train loss: 0.5992761850357056:  12%|█▌           | 46/383 [00:12<01:14,  4.53it/s]Epoch: 1, train for the 64-th batch, train loss: 0.3755732774734497:  27%|███▍         | 64/241 [00:13<00:28,  6.23it/s]Epoch: 1, train for the 77-th batch, train loss: 0.5909823179244995:  52%|██████▊      | 76/146 [00:15<00:11,  5.86it/s]Epoch: 1, train for the 77-th batch, train loss: 0.5909823179244995:  53%|██████▊      | 77/146 [00:15<00:15,  4.34it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6068528890609741:  26%|███▎         | 61/237 [00:14<00:31,  5.59it/s]Epoch: 1, train for the 70-th batch, train loss: 0.6118218898773193:  46%|█████▉       | 69/151 [00:15<00:22,  3.66it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6068528890609741:  26%|███▍         | 62/237 [00:14<00:29,  5.95it/s]Epoch: 1, train for the 70-th batch, train loss: 0.6118218898773193:  46%|██████       | 70/151 [00:15<00:18,  4.27it/s]Epoch: 1, train for the 75-th batch, train loss: 0.5366492867469788:  62%|████████     | 74/119 [00:15<00:09,  4.76it/s]Epoch: 1, train for the 75-th batch, train loss: 0.5366492867469788:  63%|████████▏    | 75/119 [00:15<00:08,  5.19it/s]Epoch: 1, train for the 47-th batch, train loss: 0.503240704536438:  12%|█▋            | 46/383 [00:12<01:14,  4.53it/s]Epoch: 1, train for the 65-th batch, train loss: 0.41893213987350464:  27%|███▏        | 64/241 [00:13<00:28,  6.23it/s]Epoch: 1, train for the 65-th batch, train loss: 0.41893213987350464:  27%|███▏        | 65/241 [00:13<00:27,  6.37it/s]Epoch: 1, train for the 47-th batch, train loss: 0.503240704536438:  12%|█▋            | 47/383 [00:12<01:08,  4.93it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5892764329910278:  53%|██████▊      | 77/146 [00:15<00:15,  4.34it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5892764329910278:  53%|██████▉      | 78/146 [00:15<00:14,  4.83it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5711650848388672:  53%|██████▉      | 78/146 [00:15<00:14,  4.83it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5711650848388672:  54%|███████      | 79/146 [00:15<00:11,  5.70it/s]Epoch: 1, train for the 76-th batch, train loss: 0.569945216178894:  63%|████████▊     | 75/119 [00:15<00:08,  5.19it/s]Epoch: 1, train for the 66-th batch, train loss: 0.38540950417518616:  27%|███▏        | 65/241 [00:13<00:27,  6.37it/s]Epoch: 1, train for the 76-th batch, train loss: 0.569945216178894:  64%|████████▉     | 76/119 [00:15<00:08,  4.81it/s]Epoch: 1, train for the 66-th batch, train loss: 0.38540950417518616:  27%|███▎        | 66/241 [00:13<00:29,  5.91it/s]Epoch: 1, train for the 63-th batch, train loss: 0.6352366209030151:  26%|███▍         | 62/237 [00:14<00:29,  5.95it/s]Epoch: 1, train for the 63-th batch, train loss: 0.6352366209030151:  27%|███▍         | 63/237 [00:14<00:37,  4.60it/s]Epoch: 1, train for the 80-th batch, train loss: 0.5789366960525513:  54%|███████      | 79/146 [00:15<00:11,  5.70it/s]Epoch: 1, train for the 80-th batch, train loss: 0.5789366960525513:  55%|███████      | 80/146 [00:15<00:10,  6.13it/s]Epoch: 1, train for the 48-th batch, train loss: 0.5422139763832092:  12%|█▌           | 47/383 [00:12<01:08,  4.93it/s]Epoch: 1, train for the 77-th batch, train loss: 0.5098796486854553:  64%|████████▎    | 76/119 [00:15<00:08,  4.81it/s]Epoch: 1, train for the 48-th batch, train loss: 0.5422139763832092:  13%|█▋           | 48/383 [00:12<01:19,  4.20it/s]Epoch: 1, train for the 77-th batch, train loss: 0.5098796486854553:  65%|████████▍    | 77/119 [00:15<00:07,  5.48it/s]Epoch: 1, train for the 67-th batch, train loss: 0.47519469261169434:  27%|███▎        | 66/241 [00:14<00:29,  5.91it/s]Epoch: 1, train for the 67-th batch, train loss: 0.47519469261169434:  28%|███▎        | 67/241 [00:14<00:28,  6.18it/s]Epoch: 1, train for the 64-th batch, train loss: 0.6067391633987427:  27%|███▍         | 63/237 [00:14<00:37,  4.60it/s]Epoch: 1, train for the 71-th batch, train loss: 0.6289486289024353:  46%|██████       | 70/151 [00:15<00:18,  4.27it/s]Epoch: 1, train for the 64-th batch, train loss: 0.6067391633987427:  27%|███▌         | 64/237 [00:14<00:33,  5.12it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5360358953475952:  55%|███████      | 80/146 [00:15<00:10,  6.13it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5360358953475952:  55%|███████▏     | 81/146 [00:16<00:09,  6.67it/s]Epoch: 1, train for the 71-th batch, train loss: 0.6289486289024353:  47%|██████       | 71/151 [00:15<00:25,  3.19it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5454642176628113:  65%|████████▍    | 77/119 [00:15<00:07,  5.48it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5454642176628113:  66%|████████▌    | 78/119 [00:15<00:06,  5.98it/s]Epoch: 1, train for the 65-th batch, train loss: 0.6031576991081238:  27%|███▌         | 64/237 [00:14<00:33,  5.12it/s]Epoch: 1, train for the 65-th batch, train loss: 0.6031576991081238:  27%|███▌         | 65/237 [00:14<00:31,  5.52it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5368129014968872:  66%|████████▌    | 78/119 [00:15<00:06,  5.98it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5757192373275757:  55%|███████▏     | 81/146 [00:16<00:09,  6.67it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5757192373275757:  56%|███████▎     | 82/146 [00:16<00:10,  6.19it/s]Epoch: 1, train for the 68-th batch, train loss: 0.565457284450531:  28%|███▉          | 67/241 [00:14<00:28,  6.18it/s]Epoch: 1, train for the 68-th batch, train loss: 0.565457284450531:  28%|███▉          | 68/241 [00:14<00:31,  5.43it/s]Epoch: 1, train for the 69-th batch, train loss: 0.681678056716919:  28%|███▉          | 68/241 [00:14<00:31,  5.43it/s]Epoch: 1, train for the 80-th batch, train loss: 0.549684464931488:  66%|█████████▏    | 78/119 [00:15<00:06,  5.98it/s]Epoch: 1, train for the 49-th batch, train loss: 0.5887061953544617:  13%|█▋           | 48/383 [00:12<01:19,  4.20it/s]Epoch: 1, train for the 80-th batch, train loss: 0.549684464931488:  67%|█████████▍    | 80/119 [00:15<00:06,  6.35it/s]Epoch: 1, train for the 49-th batch, train loss: 0.5887061953544617:  13%|█▋           | 49/383 [00:12<01:39,  3.34it/s]Epoch: 1, train for the 72-th batch, train loss: 0.6048929691314697:  47%|██████       | 71/151 [00:15<00:25,  3.19it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5534009337425232:  56%|███████▎     | 82/146 [00:16<00:10,  6.19it/s]Epoch: 1, train for the 72-th batch, train loss: 0.6048929691314697:  48%|██████▏      | 72/151 [00:15<00:26,  2.96it/s]Epoch: 1, train for the 70-th batch, train loss: 0.540681004524231:  28%|███▉          | 68/241 [00:14<00:31,  5.43it/s]Epoch: 1, train for the 66-th batch, train loss: 0.6038360595703125:  27%|███▌         | 65/237 [00:14<00:31,  5.52it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5534009337425232:  57%|███████▍     | 83/146 [00:16<00:11,  5.46it/s]Epoch: 1, train for the 70-th batch, train loss: 0.540681004524231:  29%|████          | 70/241 [00:14<00:25,  6.61it/s]Epoch: 1, train for the 66-th batch, train loss: 0.6038360595703125:  28%|███▌         | 66/237 [00:14<00:36,  4.63it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5212770104408264:  67%|████████▋    | 80/119 [00:16<00:06,  6.35it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5212770104408264:  68%|████████▊    | 81/119 [00:16<00:05,  6.51it/s]Epoch: 1, train for the 84-th batch, train loss: 0.503982424736023:  57%|███████▉      | 83/146 [00:16<00:11,  5.46it/s]Epoch: 1, train for the 50-th batch, train loss: 0.41983896493911743:  13%|█▌          | 49/383 [00:13<01:39,  3.34it/s]Epoch: 1, train for the 84-th batch, train loss: 0.503982424736023:  58%|████████      | 84/146 [00:16<00:10,  5.86it/s]Epoch: 1, train for the 50-th batch, train loss: 0.41983896493911743:  13%|█▌          | 50/383 [00:13<01:29,  3.73it/s]Epoch: 1, train for the 71-th batch, train loss: 0.6439575552940369:  29%|███▊         | 70/241 [00:14<00:25,  6.61it/s]Epoch: 1, train for the 67-th batch, train loss: 0.5496861338615417:  28%|███▌         | 66/237 [00:15<00:36,  4.63it/s]Epoch: 1, train for the 71-th batch, train loss: 0.6439575552940369:  29%|███▊         | 71/241 [00:14<00:29,  5.73it/s]Epoch: 1, train for the 73-th batch, train loss: 0.60843425989151:  48%|███████▏       | 72/151 [00:16<00:26,  2.96it/s]Epoch: 1, train for the 67-th batch, train loss: 0.5496861338615417:  28%|███▋         | 67/237 [00:15<00:38,  4.42it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5200584530830383:  58%|███████▍     | 84/146 [00:16<00:10,  5.86it/s]Epoch: 1, train for the 73-th batch, train loss: 0.60843425989151:  48%|███████▎       | 73/151 [00:16<00:25,  3.12it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5200584530830383:  58%|███████▌     | 85/146 [00:16<00:09,  6.30it/s]Epoch: 1, train for the 51-th batch, train loss: 0.44454941153526306:  13%|█▌          | 50/383 [00:13<01:29,  3.73it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5760660171508789:  68%|████████▊    | 81/119 [00:16<00:05,  6.51it/s]Epoch: 1, train for the 51-th batch, train loss: 0.44454941153526306:  13%|█▌          | 51/383 [00:13<01:18,  4.24it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5760660171508789:  69%|████████▉    | 82/119 [00:16<00:06,  5.68it/s]Epoch: 1, train for the 72-th batch, train loss: 0.6155356168746948:  29%|███▊         | 71/241 [00:14<00:29,  5.73it/s]Epoch: 1, train for the 72-th batch, train loss: 0.6155356168746948:  30%|███▉         | 72/241 [00:14<00:28,  5.99it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5842668414115906:  48%|██████▎      | 73/151 [00:16<00:25,  3.12it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5541237592697144:  28%|███▋         | 67/237 [00:15<00:38,  4.42it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5842668414115906:  49%|██████▎      | 74/151 [00:16<00:21,  3.64it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5541237592697144:  29%|███▋         | 68/237 [00:15<00:36,  4.57it/s]Epoch: 1, train for the 52-th batch, train loss: 0.46969637274742126:  13%|█▌          | 51/383 [00:13<01:18,  4.24it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5779644846916199:  30%|███▉         | 72/241 [00:15<00:28,  5.99it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5303536653518677:  69%|████████▉    | 82/119 [00:16<00:06,  5.68it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5779644846916199:  30%|███▉         | 73/241 [00:15<00:25,  6.50it/s]Epoch: 1, train for the 86-th batch, train loss: 0.557216465473175:  58%|████████▏     | 85/146 [00:16<00:09,  6.30it/s]Epoch: 1, train for the 52-th batch, train loss: 0.46969637274742126:  14%|█▋          | 52/383 [00:13<01:14,  4.45it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5303536653518677:  70%|█████████    | 83/119 [00:16<00:06,  5.51it/s]Epoch: 1, train for the 86-th batch, train loss: 0.557216465473175:  59%|████████▏     | 86/146 [00:16<00:11,  5.34it/s]Epoch: 1, train for the 75-th batch, train loss: 0.5812357068061829:  49%|██████▎      | 74/151 [00:16<00:21,  3.64it/s]Epoch: 1, train for the 75-th batch, train loss: 0.5812357068061829:  50%|██████▍      | 75/151 [00:16<00:18,  4.18it/s]Epoch: 1, train for the 69-th batch, train loss: 0.590044379234314:  29%|████          | 68/237 [00:15<00:36,  4.57it/s]Epoch: 1, train for the 69-th batch, train loss: 0.590044379234314:  29%|████          | 69/237 [00:15<00:34,  4.91it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5394949913024902:  30%|███▉         | 73/241 [00:15<00:25,  6.50it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5394949913024902:  31%|███▉         | 74/241 [00:15<00:25,  6.66it/s]Epoch: 1, train for the 53-th batch, train loss: 0.5474222302436829:  14%|█▊           | 52/383 [00:13<01:14,  4.45it/s]Epoch: 1, train for the 87-th batch, train loss: 0.6272739171981812:  59%|███████▋     | 86/146 [00:17<00:11,  5.34it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5062366724014282:  70%|█████████    | 83/119 [00:16<00:06,  5.51it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5062366724014282:  71%|█████████▏   | 84/119 [00:16<00:06,  5.37it/s]Epoch: 1, train for the 87-th batch, train loss: 0.6272739171981812:  60%|███████▋     | 87/146 [00:17<00:11,  5.22it/s]Epoch: 1, train for the 53-th batch, train loss: 0.5474222302436829:  14%|█▊           | 53/383 [00:13<01:13,  4.47it/s]Epoch: 1, train for the 70-th batch, train loss: 0.5722857713699341:  29%|███▊         | 69/237 [00:15<00:34,  4.91it/s]Epoch: 1, train for the 70-th batch, train loss: 0.5722857713699341:  30%|███▊         | 70/237 [00:15<00:30,  5.40it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5327691435813904:  71%|█████████▏   | 84/119 [00:16<00:06,  5.37it/s]Epoch: 1, train for the 76-th batch, train loss: 0.6122475862503052:  50%|██████▍      | 75/151 [00:16<00:18,  4.18it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5327691435813904:  71%|█████████▎   | 85/119 [00:16<00:05,  6.15it/s]Epoch: 1, train for the 76-th batch, train loss: 0.6122475862503052:  50%|██████▌      | 76/151 [00:16<00:17,  4.18it/s]Epoch: 1, train for the 75-th batch, train loss: 0.4348814785480499:  31%|███▉         | 74/241 [00:15<00:25,  6.66it/s]Epoch: 1, train for the 54-th batch, train loss: 0.46734747290611267:  14%|█▋          | 53/383 [00:13<01:13,  4.47it/s]Epoch: 1, train for the 75-th batch, train loss: 0.4348814785480499:  31%|████         | 75/241 [00:15<00:28,  5.79it/s]Epoch: 1, train for the 54-th batch, train loss: 0.46734747290611267:  14%|█▋          | 54/383 [00:13<01:07,  4.85it/s]Epoch: 1, train for the 88-th batch, train loss: 0.6152317523956299:  60%|███████▋     | 87/146 [00:17<00:11,  5.22it/s]Epoch: 1, train for the 71-th batch, train loss: 0.5464173555374146:  30%|███▊         | 70/237 [00:15<00:30,  5.40it/s]Epoch: 1, train for the 88-th batch, train loss: 0.6152317523956299:  60%|███████▊     | 88/146 [00:17<00:11,  5.01it/s]Epoch: 1, train for the 77-th batch, train loss: 0.5884057283401489:  50%|██████▌      | 76/151 [00:16<00:17,  4.18it/s]Epoch: 1, train for the 71-th batch, train loss: 0.5464173555374146:  30%|███▉         | 71/237 [00:15<00:30,  5.38it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5471494793891907:  71%|█████████▎   | 85/119 [00:16<00:05,  6.15it/s]Epoch: 1, train for the 77-th batch, train loss: 0.5884057283401489:  51%|██████▋      | 77/151 [00:16<00:15,  4.78it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5471494793891907:  72%|█████████▍   | 86/119 [00:16<00:05,  6.30it/s]Epoch: 1, train for the 76-th batch, train loss: 0.2982289493083954:  31%|████         | 75/241 [00:15<00:28,  5.79it/s]Epoch: 1, train for the 76-th batch, train loss: 0.2982289493083954:  32%|████         | 76/241 [00:15<00:26,  6.24it/s]Epoch: 1, train for the 55-th batch, train loss: 0.699060320854187:  14%|█▉            | 54/383 [00:14<01:07,  4.85it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5162262320518494:  60%|███████▊     | 88/146 [00:17<00:11,  5.01it/s]Epoch: 1, train for the 55-th batch, train loss: 0.699060320854187:  14%|██            | 55/383 [00:14<01:02,  5.21it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5162262320518494:  61%|███████▉     | 89/146 [00:17<00:10,  5.63it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5347462892532349:  30%|███▉         | 71/237 [00:16<00:30,  5.38it/s]Epoch: 1, train for the 87-th batch, train loss: 0.530057430267334:  72%|██████████    | 86/119 [00:17<00:05,  6.30it/s]Epoch: 1, train for the 87-th batch, train loss: 0.530057430267334:  73%|██████████▏   | 87/119 [00:17<00:04,  6.46it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5347462892532349:  30%|███▉         | 72/237 [00:16<00:29,  5.56it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5753517150878906:  51%|██████▋      | 77/151 [00:17<00:15,  4.78it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5753517150878906:  52%|██████▋      | 78/151 [00:17<00:15,  4.78it/s]Epoch: 1, train for the 77-th batch, train loss: 0.4069525897502899:  32%|████         | 76/241 [00:15<00:26,  6.24it/s]Epoch: 1, train for the 88-th batch, train loss: 0.542693018913269:  73%|██████████▏   | 87/119 [00:17<00:04,  6.46it/s]Epoch: 1, train for the 56-th batch, train loss: 0.5674936771392822:  14%|█▊           | 55/383 [00:14<01:02,  5.21it/s]Epoch: 1, train for the 88-th batch, train loss: 0.542693018913269:  74%|██████████▎   | 88/119 [00:17<00:04,  6.73it/s]Epoch: 1, train for the 77-th batch, train loss: 0.4069525897502899:  32%|████▏        | 77/241 [00:15<00:29,  5.48it/s]Epoch: 1, train for the 56-th batch, train loss: 0.5674936771392822:  15%|█▉           | 56/383 [00:14<01:04,  5.03it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5500583052635193:  61%|███████▉     | 89/146 [00:17<00:10,  5.63it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5500583052635193:  62%|████████     | 90/146 [00:17<00:10,  5.13it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5460023283958435:  30%|███▉         | 72/237 [00:16<00:29,  5.56it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5557475686073303:  74%|█████████▌   | 88/119 [00:17<00:04,  6.73it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5460023283958435:  31%|████         | 73/237 [00:16<00:33,  4.93it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5557475686073303:  75%|█████████▋   | 89/119 [00:17<00:04,  6.92it/s]Epoch: 1, train for the 78-th batch, train loss: 0.639079749584198:  32%|████▍         | 77/241 [00:15<00:29,  5.48it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5974181890487671:  52%|██████▋      | 78/151 [00:17<00:15,  4.78it/s]Epoch: 1, train for the 78-th batch, train loss: 0.639079749584198:  32%|████▌         | 78/241 [00:15<00:28,  5.66it/s]Epoch: 1, train for the 57-th batch, train loss: 0.49230265617370605:  15%|█▊          | 56/383 [00:14<01:04,  5.03it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5362824201583862:  62%|████████     | 90/146 [00:17<00:10,  5.13it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5974181890487671:  52%|██████▊      | 79/151 [00:17<00:15,  4.53it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5362824201583862:  62%|████████     | 91/146 [00:17<00:09,  5.71it/s]Epoch: 1, train for the 57-th batch, train loss: 0.49230265617370605:  15%|█▊          | 57/383 [00:14<01:01,  5.31it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5506706833839417:  75%|█████████▋   | 89/119 [00:17<00:04,  6.92it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5506706833839417:  76%|█████████▊   | 90/119 [00:17<00:04,  7.10it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5088465213775635:  62%|████████     | 91/146 [00:18<00:09,  5.71it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5088465213775635:  63%|████████▏    | 92/146 [00:18<00:09,  5.96it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5271807909011841:  31%|████         | 73/237 [00:16<00:33,  4.93it/s]Epoch: 1, train for the 91-th batch, train loss: 0.513165295124054:  76%|██████████▌   | 90/119 [00:17<00:04,  7.10it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5271807909011841:  31%|████         | 74/237 [00:16<00:35,  4.55it/s]Epoch: 1, train for the 91-th batch, train loss: 0.513165295124054:  76%|██████████▋   | 91/119 [00:17<00:03,  7.30it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5954767465591431:  63%|████████▏    | 92/146 [00:18<00:09,  5.96it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5954767465591431:  64%|████████▎    | 93/146 [00:18<00:08,  6.61it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5323934555053711:  76%|█████████▉   | 91/119 [00:17<00:03,  7.30it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5323934555053711:  77%|██████████   | 92/119 [00:17<00:04,  6.73it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5514017939567566:  32%|████▏        | 78/241 [00:16<00:28,  5.66it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5514017939567566:  33%|████▎        | 79/241 [00:16<00:40,  3.95it/s]Epoch: 1, train for the 80-th batch, train loss: 0.562232494354248:  52%|███████▎      | 79/151 [00:17<00:15,  4.53it/s]Epoch: 1, train for the 58-th batch, train loss: 0.42987388372421265:  15%|█▊          | 57/383 [00:14<01:01,  5.31it/s]Epoch: 1, train for the 94-th batch, train loss: 0.5571396350860596:  64%|████████▎    | 93/146 [00:18<00:08,  6.61it/s]Epoch: 1, train for the 75-th batch, train loss: 0.5789150595664978:  31%|████         | 74/237 [00:16<00:35,  4.55it/s]Epoch: 1, train for the 80-th batch, train loss: 0.562232494354248:  53%|███████▍      | 80/151 [00:17<00:20,  3.49it/s]Epoch: 1, train for the 94-th batch, train loss: 0.5571396350860596:  64%|████████▎    | 94/146 [00:18<00:08,  6.33it/s]Epoch: 1, train for the 58-th batch, train loss: 0.42987388372421265:  15%|█▊          | 58/383 [00:14<01:24,  3.83it/s]Epoch: 1, train for the 75-th batch, train loss: 0.5789150595664978:  32%|████         | 75/237 [00:16<00:36,  4.43it/s]Epoch: 1, train for the 93-th batch, train loss: 0.48266515135765076:  77%|█████████▎  | 92/119 [00:17<00:04,  6.73it/s]Epoch: 1, train for the 93-th batch, train loss: 0.48266515135765076:  78%|█████████▍  | 93/119 [00:17<00:03,  7.14it/s]Epoch: 1, train for the 80-th batch, train loss: 0.4439174234867096:  33%|████▎        | 79/241 [00:16<00:40,  3.95it/s]Epoch: 1, train for the 80-th batch, train loss: 0.4439174234867096:  33%|████▎        | 80/241 [00:16<00:34,  4.62it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5345334410667419:  64%|████████▎    | 94/146 [00:18<00:08,  6.33it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5345334410667419:  65%|████████▍    | 95/146 [00:18<00:07,  6.67it/s]Epoch: 1, train for the 59-th batch, train loss: 0.4890451729297638:  15%|█▉           | 58/383 [00:15<01:24,  3.83it/s]Epoch: 1, train for the 59-th batch, train loss: 0.4890451729297638:  15%|██           | 59/383 [00:15<01:17,  4.20it/s]Epoch: 1, train for the 94-th batch, train loss: 0.5216314196586609:  78%|██████████▏  | 93/119 [00:18<00:03,  7.14it/s]Epoch: 1, train for the 94-th batch, train loss: 0.5216314196586609:  79%|██████████▎  | 94/119 [00:18<00:04,  5.64it/s]Epoch: 1, train for the 76-th batch, train loss: 0.5670226216316223:  32%|████         | 75/237 [00:17<00:36,  4.43it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5870996713638306:  53%|██████▉      | 80/151 [00:18<00:20,  3.49it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5673754811286926:  33%|████▎        | 80/241 [00:16<00:34,  4.62it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5673754811286926:  34%|████▎        | 81/241 [00:16<00:36,  4.35it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5870996713638306:  54%|██████▉      | 81/151 [00:18<00:21,  3.20it/s]Epoch: 1, train for the 76-th batch, train loss: 0.5670226216316223:  32%|████▏        | 76/237 [00:17<00:42,  3.74it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5044481754302979:  79%|██████████▎  | 94/119 [00:18<00:04,  5.64it/s]Epoch: 1, train for the 60-th batch, train loss: 0.396964967250824:  15%|██▏           | 59/383 [00:15<01:17,  4.20it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5044481754302979:  80%|██████████▍  | 95/119 [00:18<00:03,  6.22it/s]Epoch: 1, train for the 60-th batch, train loss: 0.396964967250824:  16%|██▏           | 60/383 [00:15<01:22,  3.94it/s]Epoch: 1, train for the 96-th batch, train loss: 0.558555543422699:  65%|█████████     | 95/146 [00:18<00:07,  6.67it/s]Epoch: 1, train for the 96-th batch, train loss: 0.558555543422699:  66%|█████████▏    | 96/146 [00:18<00:10,  4.61it/s]Epoch: 1, train for the 77-th batch, train loss: 0.6099724173545837:  32%|████▏        | 76/237 [00:17<00:42,  3.74it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5669779181480408:  54%|██████▉      | 81/151 [00:18<00:21,  3.20it/s]Epoch: 1, train for the 77-th batch, train loss: 0.6099724173545837:  32%|████▏        | 77/237 [00:17<00:37,  4.28it/s]Epoch: 1, train for the 82-th batch, train loss: 0.4890891909599304:  34%|████▎        | 81/241 [00:16<00:36,  4.35it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5669779181480408:  54%|███████      | 82/151 [00:18<00:18,  3.68it/s]Epoch: 1, train for the 82-th batch, train loss: 0.4890891909599304:  34%|████▍        | 82/241 [00:16<00:34,  4.61it/s]Epoch: 1, train for the 96-th batch, train loss: 0.4917323887348175:  80%|██████████▍  | 95/119 [00:18<00:03,  6.22it/s]Epoch: 1, train for the 96-th batch, train loss: 0.4917323887348175:  81%|██████████▍  | 96/119 [00:18<00:03,  6.37it/s]Epoch: 1, train for the 61-th batch, train loss: 0.4362547993659973:  16%|██           | 60/383 [00:15<01:22,  3.94it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5728368759155273:  66%|████████▌    | 96/146 [00:18<00:10,  4.61it/s]Epoch: 1, train for the 61-th batch, train loss: 0.4362547993659973:  16%|██           | 61/383 [00:15<01:12,  4.42it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5728368759155273:  66%|████████▋    | 97/146 [00:18<00:09,  5.16it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5413872599601746:  34%|████▍        | 82/241 [00:17<00:34,  4.61it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5795004367828369:  32%|████▏        | 77/237 [00:17<00:37,  4.28it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5413872599601746:  34%|████▍        | 83/241 [00:17<00:30,  5.16it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5795004367828369:  33%|████▎        | 78/237 [00:17<00:34,  4.58it/s]Epoch: 1, train for the 62-th batch, train loss: 0.535203218460083:  16%|██▏           | 61/383 [00:15<01:12,  4.42it/s]Epoch: 1, train for the 62-th batch, train loss: 0.535203218460083:  16%|██▎           | 62/383 [00:15<01:07,  4.76it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5036324858665466:  81%|██████████▍  | 96/119 [00:18<00:03,  6.37it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5036324858665466:  82%|██████████▌  | 97/119 [00:18<00:03,  5.68it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5802026391029358:  33%|████▎        | 78/237 [00:17<00:34,  4.58it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5802026391029358:  33%|████▎        | 79/237 [00:17<00:32,  4.91it/s]Epoch: 1, train for the 63-th batch, train loss: 0.5364278554916382:  16%|██           | 62/383 [00:15<01:07,  4.76it/s]Epoch: 1, train for the 98-th batch, train loss: 0.4583163261413574:  82%|██████████▌  | 97/119 [00:18<00:03,  5.68it/s]Epoch: 1, train for the 84-th batch, train loss: 0.6636819243431091:  34%|████▍        | 83/241 [00:17<00:30,  5.16it/s]Epoch: 1, train for the 98-th batch, train loss: 0.4583163261413574:  82%|██████████▋  | 98/119 [00:18<00:03,  6.09it/s]Epoch: 1, train for the 63-th batch, train loss: 0.5364278554916382:  16%|██▏          | 63/383 [00:15<01:02,  5.16it/s]Epoch: 1, train for the 84-th batch, train loss: 0.6636819243431091:  35%|████▌        | 84/241 [00:17<00:34,  4.55it/s]Epoch: 1, train for the 98-th batch, train loss: 0.557198166847229:  66%|█████████▎    | 97/146 [00:19<00:09,  5.16it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5375866293907166:  54%|███████      | 82/151 [00:18<00:18,  3.68it/s]Epoch: 1, train for the 98-th batch, train loss: 0.557198166847229:  67%|█████████▍    | 98/146 [00:19<00:11,  4.08it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5375866293907166:  55%|███████▏     | 83/151 [00:18<00:22,  3.03it/s]Epoch: 1, train for the 80-th batch, train loss: 0.587378203868866:  33%|████▋         | 79/237 [00:17<00:32,  4.91it/s]Epoch: 1, train for the 80-th batch, train loss: 0.587378203868866:  34%|████▋         | 80/237 [00:17<00:30,  5.08it/s]Epoch: 1, train for the 99-th batch, train loss: 0.5024319887161255:  82%|██████████▋  | 98/119 [00:18<00:03,  6.09it/s]Epoch: 1, train for the 99-th batch, train loss: 0.5024319887161255:  83%|██████████▊  | 99/119 [00:18<00:03,  6.47it/s]Epoch: 1, train for the 64-th batch, train loss: 0.49880996346473694:  16%|█▉          | 63/383 [00:16<01:02,  5.16it/s]Epoch: 1, train for the 64-th batch, train loss: 0.49880996346473694:  17%|██          | 64/383 [00:16<01:01,  5.17it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5102514028549194:  83%|█████████▉  | 99/119 [00:18<00:03,  6.47it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5849555134773254:  55%|███████▏     | 83/151 [00:19<00:22,  3.03it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5849555134773254:  56%|███████▏     | 84/151 [00:19<00:21,  3.19it/s]Epoch: 1, train for the 99-th batch, train loss: 0.570904016494751:  67%|█████████▍    | 98/146 [00:19<00:11,  4.08it/s]Epoch: 1, train for the 99-th batch, train loss: 0.570904016494751:  68%|█████████▍    | 99/146 [00:19<00:12,  3.78it/s]Epoch: 1, train for the 65-th batch, train loss: 0.4262860119342804:  17%|██▏          | 64/383 [00:16<01:01,  5.17it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5140362977981567:  35%|████▌        | 84/241 [00:17<00:34,  4.55it/s]Epoch: 1, train for the 65-th batch, train loss: 0.4262860119342804:  17%|██▏          | 65/383 [00:16<01:05,  4.87it/s]Epoch: 1, train for the 101-th batch, train loss: 0.49469611048698425:  83%|█████████▏ | 99/119 [00:19<00:03,  6.47it/s]Epoch: 1, train for the 101-th batch, train loss: 0.49469611048698425:  85%|████████▍ | 101/119 [00:19<00:02,  6.50it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5140362977981567:  35%|████▌        | 85/241 [00:17<00:44,  3.53it/s]Epoch: 1, train for the 81-th batch, train loss: 0.6046919822692871:  34%|████▍        | 80/237 [00:18<00:30,  5.08it/s]Epoch: 1, train for the 85-th batch, train loss: 0.4950545132160187:  56%|███████▏     | 84/151 [00:19<00:21,  3.19it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5977514982223511:  68%|████████▏   | 99/146 [00:19<00:12,  3.78it/s]Epoch: 1, train for the 85-th batch, train loss: 0.4950545132160187:  56%|███████▎     | 85/151 [00:19<00:17,  3.76it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5977514982223511:  68%|███████▌   | 100/146 [00:19<00:10,  4.42it/s]Epoch: 1, train for the 81-th batch, train loss: 0.6046919822692871:  34%|████▍        | 81/237 [00:18<00:40,  3.84it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5164303183555603:  85%|█████████▎ | 101/119 [00:19<00:02,  6.50it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5164303183555603:  86%|█████████▍ | 102/119 [00:19<00:02,  6.74it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5204839706420898:  35%|████▌        | 85/241 [00:17<00:44,  3.53it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5204839706420898:  36%|████▋        | 86/241 [00:17<00:37,  4.14it/s]Epoch: 1, train for the 66-th batch, train loss: 0.5011810064315796:  17%|██▏          | 65/383 [00:16<01:05,  4.87it/s]Epoch: 1, train for the 66-th batch, train loss: 0.5011810064315796:  17%|██▏          | 66/383 [00:16<01:02,  5.04it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5274222493171692:  68%|███████▌   | 100/146 [00:19<00:10,  4.42it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5274222493171692:  69%|███████▌   | 101/146 [00:19<00:08,  5.07it/s]Epoch: 1, train for the 82-th batch, train loss: 0.6144390106201172:  34%|████▍        | 81/237 [00:18<00:40,  3.84it/s]Epoch: 1, train for the 82-th batch, train loss: 0.6144390106201172:  35%|████▍        | 82/237 [00:18<00:36,  4.22it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5451557040214539:  69%|███████▌   | 101/146 [00:19<00:08,  5.07it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5451557040214539:  70%|███████▋   | 102/146 [00:20<00:07,  5.75it/s]Epoch: 1, train for the 103-th batch, train loss: 0.4678305685520172:  86%|█████████▍ | 102/119 [00:19<00:02,  6.74it/s]Epoch: 1, train for the 103-th batch, train loss: 0.4678305685520172:  87%|█████████▌ | 103/119 [00:19<00:02,  5.82it/s]Epoch: 1, train for the 87-th batch, train loss: 0.5489810109138489:  36%|████▋        | 86/241 [00:18<00:37,  4.14it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5750247836112976:  56%|███████▎     | 85/151 [00:19<00:17,  3.76it/s]Epoch: 1, train for the 87-th batch, train loss: 0.5489810109138489:  36%|████▋        | 87/241 [00:18<00:37,  4.15it/s]Epoch: 1, train for the 83-th batch, train loss: 0.6300649046897888:  35%|████▍        | 82/237 [00:18<00:36,  4.22it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5750247836112976:  57%|███████▍     | 86/151 [00:19<00:18,  3.42it/s]Epoch: 1, train for the 83-th batch, train loss: 0.6300649046897888:  35%|████▌        | 83/237 [00:18<00:33,  4.65it/s]Epoch: 1, train for the 67-th batch, train loss: 0.5267484784126282:  17%|██▏          | 66/383 [00:16<01:02,  5.04it/s]Epoch: 1, train for the 67-th batch, train loss: 0.5267484784126282:  17%|██▎          | 67/383 [00:16<01:12,  4.36it/s]Epoch: 1, train for the 104-th batch, train loss: 0.48183250427246094:  87%|████████▋ | 103/119 [00:19<00:02,  5.82it/s]Epoch: 1, train for the 104-th batch, train loss: 0.48183250427246094:  87%|████████▋ | 104/119 [00:19<00:02,  6.20it/s]Epoch: 1, train for the 103-th batch, train loss: 0.5386489033699036:  70%|███████▋   | 102/146 [00:20<00:07,  5.75it/s]Epoch: 1, train for the 103-th batch, train loss: 0.5386489033699036:  71%|███████▊   | 103/146 [00:20<00:08,  5.08it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5837658047676086:  35%|████▌        | 83/237 [00:18<00:33,  4.65it/s]Epoch: 1, train for the 87-th batch, train loss: 0.544333279132843:  57%|███████▉      | 86/151 [00:19<00:18,  3.42it/s]Epoch: 1, train for the 88-th batch, train loss: 0.5699504613876343:  36%|████▋        | 87/241 [00:18<00:37,  4.15it/s]Epoch: 1, train for the 87-th batch, train loss: 0.544333279132843:  58%|████████      | 87/151 [00:19<00:16,  3.88it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5837658047676086:  35%|████▌        | 84/237 [00:18<00:30,  5.03it/s]Epoch: 1, train for the 88-th batch, train loss: 0.5699504613876343:  37%|████▋        | 88/241 [00:18<00:34,  4.41it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5303194522857666:  17%|██▎          | 67/383 [00:16<01:12,  4.36it/s]Epoch: 1, train for the 105-th batch, train loss: 0.4925609827041626:  87%|█████████▌ | 104/119 [00:19<00:02,  6.20it/s]Epoch: 1, train for the 105-th batch, train loss: 0.4925609827041626:  88%|█████████▋ | 105/119 [00:19<00:02,  6.56it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5303194522857666:  18%|██▎          | 68/383 [00:16<01:06,  4.76it/s]Epoch: 1, train for the 104-th batch, train loss: 0.546558678150177:  71%|████████▍   | 103/146 [00:20<00:08,  5.08it/s]Epoch: 1, train for the 104-th batch, train loss: 0.546558678150177:  71%|████████▌   | 104/146 [00:20<00:07,  5.67it/s]Epoch: 1, train for the 88-th batch, train loss: 0.6445066928863525:  58%|███████▍     | 87/151 [00:19<00:16,  3.88it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5462905764579773:  37%|████▋        | 88/241 [00:18<00:34,  4.41it/s]Epoch: 1, train for the 106-th batch, train loss: 0.45696333050727844:  88%|████████▊ | 105/119 [00:19<00:02,  6.56it/s]Epoch: 1, train for the 88-th batch, train loss: 0.6445066928863525:  58%|███████▌     | 88/151 [00:19<00:14,  4.29it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5462905764579773:  37%|████▊        | 89/241 [00:18<00:31,  4.80it/s]Epoch: 1, train for the 106-th batch, train loss: 0.45696333050727844:  89%|████████▉ | 106/119 [00:19<00:01,  6.97it/s]Epoch: 1, train for the 107-th batch, train loss: 0.5252687335014343:  89%|█████████▊ | 106/119 [00:20<00:01,  6.97it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5298826694488525:  35%|████▌        | 84/237 [00:19<00:30,  5.03it/s]Epoch: 1, train for the 69-th batch, train loss: 0.38000959157943726:  18%|██▏         | 68/383 [00:17<01:06,  4.76it/s]Epoch: 1, train for the 107-th batch, train loss: 0.5252687335014343:  90%|█████████▉ | 107/119 [00:20<00:01,  6.45it/s]Epoch: 1, train for the 69-th batch, train loss: 0.38000959157943726:  18%|██▏         | 69/383 [00:17<01:15,  4.14it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5298826694488525:  36%|████▋        | 85/237 [00:19<00:38,  3.94it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5447748303413391:  37%|████▊        | 89/241 [00:18<00:31,  4.80it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5756533145904541:  58%|███████▌     | 88/151 [00:20<00:14,  4.29it/s]Epoch: 1, train for the 105-th batch, train loss: 0.5715831518173218:  71%|███████▊   | 104/146 [00:20<00:07,  5.67it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5447748303413391:  37%|████▊        | 90/241 [00:18<00:32,  4.59it/s]Epoch: 1, train for the 105-th batch, train loss: 0.5715831518173218:  72%|███████▉   | 105/146 [00:20<00:08,  4.63it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5756533145904541:  59%|███████▋     | 89/151 [00:20<00:14,  4.21it/s]Epoch: 1, train for the 108-th batch, train loss: 0.41277560591697693:  90%|████████▉ | 107/119 [00:20<00:01,  6.45it/s]Epoch: 1, train for the 108-th batch, train loss: 0.41277560591697693:  91%|█████████ | 108/119 [00:20<00:01,  6.76it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5992617607116699:  36%|████▋        | 85/237 [00:19<00:38,  3.94it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5624107122421265:  72%|███████▉   | 105/146 [00:20<00:08,  4.63it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5992617607116699:  36%|████▋        | 86/237 [00:19<00:35,  4.29it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5624107122421265:  73%|███████▉   | 106/146 [00:20<00:07,  5.05it/s]Epoch: 1, train for the 70-th batch, train loss: 0.4984152019023895:  18%|██▎          | 69/383 [00:17<01:15,  4.14it/s]Epoch: 1, train for the 70-th batch, train loss: 0.4984152019023895:  18%|██▍          | 70/383 [00:17<01:14,  4.22it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5452502369880676:  37%|████▊        | 90/241 [00:19<00:32,  4.59it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5452502369880676:  38%|████▉        | 91/241 [00:19<00:33,  4.48it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5561639070510864:  59%|███████▋     | 89/151 [00:20<00:14,  4.21it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5561639070510864:  60%|███████▋     | 90/151 [00:20<00:15,  4.05it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5007890462875366:  38%|████▉        | 91/241 [00:19<00:33,  4.48it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5007890462875366:  38%|████▉        | 92/241 [00:19<00:29,  5.06it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5330666899681091:  60%|███████▋     | 90/151 [00:20<00:15,  4.05it/s]Epoch: 1, train for the 71-th batch, train loss: 0.594245970249176:  18%|██▌           | 70/383 [00:17<01:14,  4.22it/s]Epoch: 1, train for the 109-th batch, train loss: 0.48937729001045227:  91%|█████████ | 108/119 [00:20<00:01,  6.76it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5330666899681091:  60%|███████▊     | 91/151 [00:20<00:12,  4.66it/s]Epoch: 1, train for the 109-th batch, train loss: 0.48937729001045227:  92%|█████████▏| 109/119 [00:20<00:02,  4.87it/s]Epoch: 1, train for the 87-th batch, train loss: 0.6156049966812134:  36%|████▋        | 86/237 [00:19<00:35,  4.29it/s]Epoch: 1, train for the 71-th batch, train loss: 0.594245970249176:  19%|██▌           | 71/383 [00:17<01:14,  4.20it/s]Epoch: 1, train for the 107-th batch, train loss: 0.542926549911499:  73%|████████▋   | 106/146 [00:21<00:07,  5.05it/s]Epoch: 1, train for the 87-th batch, train loss: 0.6156049966812134:  37%|████▊        | 87/237 [00:19<00:38,  3.94it/s]Epoch: 1, train for the 107-th batch, train loss: 0.542926549911499:  73%|████████▊   | 107/146 [00:21<00:09,  4.32it/s]Epoch: 1, train for the 93-th batch, train loss: 0.48729291558265686:  38%|████▌       | 92/241 [00:19<00:29,  5.06it/s]Epoch: 1, train for the 93-th batch, train loss: 0.48729291558265686:  39%|████▋       | 93/241 [00:19<00:26,  5.62it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5602899789810181:  60%|███████▊     | 91/151 [00:20<00:12,  4.66it/s]Epoch: 1, train for the 110-th batch, train loss: 0.48082074522972107:  92%|█████████▏| 109/119 [00:20<00:02,  4.87it/s]Epoch: 1, train for the 110-th batch, train loss: 0.48082074522972107:  92%|█████████▏| 110/119 [00:20<00:01,  5.44it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5602899789810181:  61%|███████▉     | 92/151 [00:20<00:11,  5.17it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5677455067634583:  73%|████████   | 107/146 [00:21<00:09,  4.32it/s]Epoch: 1, train for the 94-th batch, train loss: 0.479531854391098:  39%|█████▍        | 93/241 [00:19<00:26,  5.62it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5677455067634583:  74%|████████▏  | 108/146 [00:21<00:07,  4.83it/s]Epoch: 1, train for the 88-th batch, train loss: 0.6097177267074585:  37%|████▊        | 87/237 [00:19<00:38,  3.94it/s]Epoch: 1, train for the 94-th batch, train loss: 0.479531854391098:  39%|█████▍        | 94/241 [00:19<00:23,  6.14it/s]Epoch: 1, train for the 88-th batch, train loss: 0.6097177267074585:  37%|████▊        | 88/237 [00:19<00:35,  4.24it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5093748569488525:  39%|█████        | 94/241 [00:19<00:23,  6.14it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5093748569488525:  39%|█████        | 95/241 [00:19<00:21,  6.91it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5669605135917664:  19%|██▍          | 71/383 [00:18<01:14,  4.20it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5397023558616638:  92%|██████████▏| 110/119 [00:20<00:01,  5.44it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5397023558616638:  93%|██████████▎| 111/119 [00:20<00:01,  5.10it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5669605135917664:  19%|██▍          | 72/383 [00:18<01:24,  3.67it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5761400461196899:  37%|████▊        | 88/237 [00:20<00:35,  4.24it/s]Epoch: 1, train for the 96-th batch, train loss: 0.5988832116127014:  39%|█████        | 95/241 [00:19<00:21,  6.91it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5788098573684692:  61%|███████▉     | 92/151 [00:21<00:11,  5.17it/s]Epoch: 1, train for the 96-th batch, train loss: 0.5988832116127014:  40%|█████▏       | 96/241 [00:19<00:23,  6.20it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5788098573684692:  62%|████████     | 93/151 [00:21<00:14,  4.01it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5761400461196899:  38%|████▉        | 89/237 [00:20<00:37,  3.98it/s]Epoch: 1, train for the 112-th batch, train loss: 0.4978034496307373:  93%|██████████▎| 111/119 [00:21<00:01,  5.10it/s]Epoch: 1, train for the 73-th batch, train loss: 0.40655890107154846:  19%|██▎         | 72/383 [00:18<01:24,  3.67it/s]Epoch: 1, train for the 109-th batch, train loss: 0.5359202027320862:  74%|████████▏  | 108/146 [00:21<00:07,  4.83it/s]Epoch: 1, train for the 112-th batch, train loss: 0.4978034496307373:  94%|██████████▎| 112/119 [00:21<00:01,  5.23it/s]Epoch: 1, train for the 109-th batch, train loss: 0.5359202027320862:  75%|████████▏  | 109/146 [00:21<00:09,  3.97it/s]Epoch: 1, train for the 73-th batch, train loss: 0.40655890107154846:  19%|██▎         | 73/383 [00:18<01:17,  3.98it/s]Epoch: 1, train for the 97-th batch, train loss: 0.42022979259490967:  40%|████▊       | 96/241 [00:19<00:23,  6.20it/s]Epoch: 1, train for the 97-th batch, train loss: 0.42022979259490967:  40%|████▊       | 97/241 [00:19<00:22,  6.36it/s]Epoch: 1, train for the 94-th batch, train loss: 0.5490396022796631:  62%|████████     | 93/151 [00:21<00:14,  4.01it/s]Epoch: 1, train for the 110-th batch, train loss: 0.5223888158798218:  75%|████████▏  | 109/146 [00:21<00:09,  3.97it/s]Epoch: 1, train for the 110-th batch, train loss: 0.5223888158798218:  75%|████████▎  | 110/146 [00:21<00:07,  4.67it/s]Epoch: 1, train for the 94-th batch, train loss: 0.5490396022796631:  62%|████████     | 94/151 [00:21<00:12,  4.47it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5763640999794006:  38%|████▉        | 89/237 [00:20<00:37,  3.98it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5763640999794006:  38%|████▉        | 90/237 [00:20<00:35,  4.19it/s]Epoch: 1, train for the 74-th batch, train loss: 0.2756749987602234:  19%|██▍          | 73/383 [00:18<01:17,  3.98it/s]Epoch: 1, train for the 74-th batch, train loss: 0.2756749987602234:  19%|██▌          | 74/383 [00:18<01:11,  4.33it/s]Epoch: 1, train for the 113-th batch, train loss: 0.463116317987442:  94%|███████████▎| 112/119 [00:21<00:01,  5.23it/s]Epoch: 1, train for the 113-th batch, train loss: 0.463116317987442:  95%|███████████▍| 113/119 [00:21<00:01,  4.52it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5430164337158203:  38%|████▉        | 90/237 [00:20<00:35,  4.19it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5633687376976013:  62%|████████     | 94/151 [00:21<00:12,  4.47it/s]Epoch: 1, train for the 98-th batch, train loss: 0.544162392616272:  40%|█████▋        | 97/241 [00:20<00:22,  6.36it/s]Epoch: 1, train for the 114-th batch, train loss: 0.4877932667732239:  95%|██████████▍| 113/119 [00:21<00:01,  4.52it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5430164337158203:  38%|████▉        | 91/237 [00:20<00:34,  4.18it/s]Epoch: 1, train for the 114-th batch, train loss: 0.4877932667732239:  96%|██████████▌| 114/119 [00:21<00:00,  5.05it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5633687376976013:  63%|████████▏    | 95/151 [00:21<00:13,  4.09it/s]Epoch: 1, train for the 98-th batch, train loss: 0.544162392616272:  41%|█████▋        | 98/241 [00:20<00:29,  4.87it/s]Epoch: 1, train for the 111-th batch, train loss: 0.53082275390625:  75%|█████████▊   | 110/146 [00:22<00:07,  4.67it/s]Epoch: 1, train for the 75-th batch, train loss: 0.4202210009098053:  19%|██▌          | 74/383 [00:18<01:11,  4.33it/s]Epoch: 1, train for the 111-th batch, train loss: 0.53082275390625:  76%|█████████▉   | 111/146 [00:22<00:08,  4.08it/s]Epoch: 1, train for the 75-th batch, train loss: 0.4202210009098053:  20%|██▌          | 75/383 [00:18<01:16,  4.05it/s]Epoch: 1, train for the 112-th batch, train loss: 0.571032702922821:  76%|█████████   | 111/146 [00:22<00:08,  4.08it/s]Epoch: 1, train for the 112-th batch, train loss: 0.571032702922821:  77%|█████████▏  | 112/146 [00:22<00:07,  4.71it/s]Epoch: 1, train for the 99-th batch, train loss: 0.3610519766807556:  41%|█████▎       | 98/241 [00:20<00:29,  4.87it/s]Epoch: 1, train for the 96-th batch, train loss: 0.5732941031455994:  63%|████████▏    | 95/151 [00:21<00:13,  4.09it/s]Epoch: 1, train for the 99-th batch, train loss: 0.3610519766807556:  41%|█████▎       | 99/241 [00:20<00:29,  4.88it/s]Epoch: 1, train for the 96-th batch, train loss: 0.5732941031455994:  64%|████████▎    | 96/151 [00:21<00:12,  4.26it/s]Epoch: 1, train for the 113-th batch, train loss: 0.5310996174812317:  77%|████████▍  | 112/146 [00:22<00:07,  4.71it/s]Epoch: 1, train for the 113-th batch, train loss: 0.5310996174812317:  77%|████████▌  | 113/146 [00:22<00:05,  5.60it/s]Epoch: 1, train for the 115-th batch, train loss: 0.4696401059627533:  96%|██████████▌| 114/119 [00:21<00:00,  5.05it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5622450113296509:  64%|████████▎    | 96/151 [00:21<00:12,  4.26it/s]Epoch: 1, train for the 115-th batch, train loss: 0.4696401059627533:  97%|██████████▋| 115/119 [00:21<00:00,  4.22it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5622450113296509:  64%|████████▎    | 97/151 [00:21<00:10,  4.99it/s]Epoch: 1, train for the 76-th batch, train loss: 0.5323862433433533:  20%|██▌          | 75/383 [00:19<01:16,  4.05it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5589314699172974:  38%|████▉        | 91/237 [00:20<00:34,  4.18it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5589314699172974:  39%|█████        | 92/237 [00:20<00:41,  3.48it/s]Epoch: 1, train for the 76-th batch, train loss: 0.5323862433433533:  20%|██▌          | 76/383 [00:19<01:23,  3.67it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5647287964820862:  41%|████▉       | 99/241 [00:20<00:29,  4.88it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5647287964820862:  41%|████▌      | 100/241 [00:20<00:29,  4.81it/s]Epoch: 1, train for the 98-th batch, train loss: 0.5277490019798279:  64%|████████▎    | 97/151 [00:22<00:10,  4.99it/s]Epoch: 1, train for the 98-th batch, train loss: 0.5277490019798279:  65%|████████▍    | 98/151 [00:22<00:09,  5.59it/s]Epoch: 1, train for the 116-th batch, train loss: 0.42344930768013:  97%|████████████▌| 115/119 [00:22<00:00,  4.22it/s]Epoch: 1, train for the 114-th batch, train loss: 0.5490903854370117:  77%|████████▌  | 113/146 [00:22<00:05,  5.60it/s]Epoch: 1, train for the 114-th batch, train loss: 0.5490903854370117:  78%|████████▌  | 114/146 [00:22<00:06,  4.95it/s]Epoch: 1, train for the 116-th batch, train loss: 0.42344930768013:  97%|████████████▋| 116/119 [00:22<00:00,  4.48it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5414935350418091:  39%|█████        | 92/237 [00:21<00:41,  3.48it/s]Epoch: 1, train for the 101-th batch, train loss: 0.592289924621582:  41%|████▉       | 100/241 [00:20<00:29,  4.81it/s]Epoch: 1, train for the 101-th batch, train loss: 0.592289924621582:  42%|█████       | 101/241 [00:20<00:26,  5.29it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5414935350418091:  39%|█████        | 93/237 [00:21<00:36,  3.93it/s]Epoch: 1, train for the 77-th batch, train loss: 0.4945094883441925:  20%|██▌          | 76/383 [00:19<01:23,  3.67it/s]Epoch: 1, train for the 99-th batch, train loss: 0.5457153916358948:  65%|████████▍    | 98/151 [00:22<00:09,  5.59it/s]Epoch: 1, train for the 77-th batch, train loss: 0.4945094883441925:  20%|██▌          | 77/383 [00:19<01:17,  3.94it/s]Epoch: 1, train for the 99-th batch, train loss: 0.5457153916358948:  66%|████████▌    | 99/151 [00:22<00:08,  5.88it/s]Epoch: 1, train for the 94-th batch, train loss: 0.6057150363922119:  39%|█████        | 93/237 [00:21<00:36,  3.93it/s]Epoch: 1, train for the 117-th batch, train loss: 0.4187329113483429:  97%|██████████▋| 116/119 [00:22<00:00,  4.48it/s]Epoch: 1, train for the 117-th batch, train loss: 0.4187329113483429:  98%|██████████▊| 117/119 [00:22<00:00,  4.34it/s]Epoch: 1, train for the 94-th batch, train loss: 0.6057150363922119:  40%|█████▏       | 94/237 [00:21<00:33,  4.21it/s]Epoch: 1, train for the 102-th batch, train loss: 0.6069274544715881:  42%|████▌      | 101/241 [00:20<00:26,  5.29it/s]Epoch: 1, train for the 78-th batch, train loss: 0.45981451869010925:  20%|██▍         | 77/383 [00:19<01:17,  3.94it/s]Epoch: 1, train for the 102-th batch, train loss: 0.6069274544715881:  42%|████▋      | 102/241 [00:20<00:28,  4.95it/s]Epoch: 1, train for the 78-th batch, train loss: 0.45981451869010925:  20%|██▍         | 78/383 [00:19<01:13,  4.16it/s]Epoch: 1, train for the 115-th batch, train loss: 0.5597318410873413:  78%|████████▌  | 114/146 [00:22<00:06,  4.95it/s]Epoch: 1, train for the 115-th batch, train loss: 0.5597318410873413:  79%|████████▋  | 115/146 [00:22<00:07,  4.01it/s]Epoch: 1, train for the 118-th batch, train loss: 0.44954004883766174:  98%|█████████▊| 117/119 [00:22<00:00,  4.34it/s]Epoch: 1, train for the 118-th batch, train loss: 0.44954004883766174:  99%|█████████▉| 118/119 [00:22<00:00,  4.95it/s]Epoch: 1, train for the 100-th batch, train loss: 0.6110520362854004:  66%|███████▊    | 99/151 [00:22<00:08,  5.88it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5494676828384399:  40%|█████▏       | 94/237 [00:21<00:33,  4.21it/s]Epoch: 1, train for the 100-th batch, train loss: 0.6110520362854004:  66%|███████▎   | 100/151 [00:22<00:11,  4.58it/s]Epoch: 1, train for the 103-th batch, train loss: 0.5515527129173279:  42%|████▋      | 102/241 [00:21<00:28,  4.95it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5494676828384399:  40%|█████▏       | 95/237 [00:21<00:31,  4.49it/s]Epoch: 1, train for the 103-th batch, train loss: 0.5515527129173279:  43%|████▋      | 103/241 [00:21<00:26,  5.30it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5638460516929626:  20%|██▋          | 78/383 [00:19<01:13,  4.16it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5638460516929626:  21%|██▋          | 79/383 [00:19<01:08,  4.44it/s]Epoch: 1, train for the 116-th batch, train loss: 0.5622720718383789:  79%|████████▋  | 115/146 [00:23<00:07,  4.01it/s]Epoch: 1, train for the 119-th batch, train loss: 0.36144521832466125:  99%|█████████▉| 118/119 [00:22<00:00,  4.95it/s]Epoch: 1, train for the 116-th batch, train loss: 0.5622720718383789:  79%|████████▋  | 116/146 [00:23<00:06,  4.44it/s]Epoch: 1, train for the 119-th batch, train loss: 0.36144521832466125: 100%|██████████| 119/119 [00:22<00:00,  5.40it/s]Epoch: 1, train for the 119-th batch, train loss: 0.36144521832466125: 100%|██████████| 119/119 [00:22<00:00,  5.26it/s]
Epoch: 1, train for the 96-th batch, train loss: 0.538425862789154:  40%|█████▌        | 95/237 [00:21<00:31,  4.49it/s]Epoch: 1, train for the 96-th batch, train loss: 0.538425862789154:  41%|█████▋        | 96/237 [00:21<00:28,  4.92it/s]Epoch: 1, train for the 80-th batch, train loss: 0.3438730835914612:  21%|██▋          | 79/383 [00:19<01:08,  4.44it/s]Epoch: 1, train for the 80-th batch, train loss: 0.3438730835914612:  21%|██▋          | 80/383 [00:19<01:02,  4.86it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5982942581176758:  66%|███████▎   | 100/151 [00:22<00:11,  4.58it/s]Epoch: 1, train for the 117-th batch, train loss: 0.5829050540924072:  79%|████████▋  | 116/146 [00:23<00:06,  4.44it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5712301135063171:  43%|████▋      | 103/241 [00:21<00:26,  5.30it/s]Epoch: 1, train for the 117-th batch, train loss: 0.5829050540924072:  80%|████████▊  | 117/146 [00:23<00:05,  5.07it/s]  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5982942581176758:  67%|███████▎   | 101/151 [00:22<00:11,  4.43it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5712301135063171:  43%|████▋      | 104/241 [00:21<00:27,  4.94it/s]evaluate for the 1-th batch, evaluate loss: 0.5207842588424683:   0%|                            | 0/40 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5458422899246216:   0%|                            | 0/40 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5458422899246216:   5%|█                   | 2/40 [00:00<00:02, 16.03it/s]Epoch: 1, train for the 81-th batch, train loss: 0.3177242577075958:  21%|██▋          | 80/383 [00:20<01:02,  4.86it/s]Epoch: 1, train for the 81-th batch, train loss: 0.3177242577075958:  21%|██▋          | 81/383 [00:20<01:01,  4.88it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5461913347244263:  41%|█████▎       | 96/237 [00:21<00:28,  4.92it/s]Epoch: 1, train for the 118-th batch, train loss: 0.5294780731201172:  80%|████████▊  | 117/146 [00:23<00:05,  5.07it/s]evaluate for the 3-th batch, evaluate loss: 0.5496700406074524:   5%|█                   | 2/40 [00:00<00:02, 16.03it/s]Epoch: 1, train for the 105-th batch, train loss: 0.4609363079071045:  43%|████▋      | 104/241 [00:21<00:27,  4.94it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5413416028022766:  67%|███████▎   | 101/151 [00:22<00:11,  4.43it/s]Epoch: 1, train for the 118-th batch, train loss: 0.5294780731201172:  81%|████████▉  | 118/146 [00:23<00:05,  4.84it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5461913347244263:  41%|█████▎       | 97/237 [00:21<00:32,  4.33it/s]Epoch: 1, train for the 105-th batch, train loss: 0.4609363079071045:  44%|████▊      | 105/241 [00:21<00:28,  4.78it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5413416028022766:  68%|███████▍   | 102/151 [00:23<00:11,  4.35it/s]evaluate for the 4-th batch, evaluate loss: 0.6230064034461975:   5%|█                   | 2/40 [00:00<00:02, 16.03it/s]evaluate for the 4-th batch, evaluate loss: 0.6230064034461975:  10%|██                  | 4/40 [00:00<00:02, 13.61it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5666933655738831:  21%|██▋          | 81/383 [00:20<01:01,  4.88it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5666933655738831:  21%|██▊          | 82/383 [00:20<00:59,  5.08it/s]Epoch: 1, train for the 119-th batch, train loss: 0.5305728912353516:  81%|████████▉  | 118/146 [00:23<00:05,  4.84it/s]evaluate for the 5-th batch, evaluate loss: 0.5970845818519592:  10%|██                  | 4/40 [00:00<00:02, 13.61it/s]Epoch: 1, train for the 119-th batch, train loss: 0.5305728912353516:  82%|████████▉  | 119/146 [00:23<00:05,  5.09it/s]evaluate for the 6-th batch, evaluate loss: 0.5665887594223022:  10%|██                  | 4/40 [00:00<00:02, 13.61it/s]evaluate for the 6-th batch, evaluate loss: 0.5665887594223022:  15%|███                 | 6/40 [00:00<00:02, 12.76it/s]Epoch: 1, train for the 103-th batch, train loss: 0.6350986957550049:  68%|███████▍   | 102/151 [00:23<00:11,  4.35it/s]evaluate for the 7-th batch, evaluate loss: 0.5890670418739319:  15%|███                 | 6/40 [00:00<00:02, 12.76it/s]Epoch: 1, train for the 103-th batch, train loss: 0.6350986957550049:  68%|███████▌   | 103/151 [00:23<00:12,  3.98it/s]evaluate for the 8-th batch, evaluate loss: 0.5578437447547913:  15%|███                 | 6/40 [00:00<00:02, 12.76it/s]evaluate for the 8-th batch, evaluate loss: 0.5578437447547913:  20%|████                | 8/40 [00:00<00:02, 14.15it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5181053280830383:  21%|██▊          | 82/383 [00:20<00:59,  5.08it/s]Epoch: 1, train for the 98-th batch, train loss: 0.5492209792137146:  41%|█████▎       | 97/237 [00:22<00:32,  4.33it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5181053280830383:  22%|██▊          | 83/383 [00:20<01:04,  4.62it/s]Epoch: 1, train for the 98-th batch, train loss: 0.5492209792137146:  41%|█████▍       | 98/237 [00:22<00:39,  3.55it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5311922430992126:  82%|████████▉  | 119/146 [00:23<00:05,  5.09it/s]evaluate for the 9-th batch, evaluate loss: 0.5802997350692749:  20%|████                | 8/40 [00:00<00:02, 14.15it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5311922430992126:  82%|█████████  | 120/146 [00:23<00:05,  4.72it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5441854596138:  44%|██████        | 105/241 [00:22<00:28,  4.78it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5441854596138:  44%|██████▏       | 106/241 [00:22<00:38,  3.55it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5707578659057617:  68%|███████▌   | 103/151 [00:23<00:12,  3.98it/s]evaluate for the 10-th batch, evaluate loss: 0.6123245358467102:  20%|███▊               | 8/40 [00:00<00:02, 14.15it/s]evaluate for the 10-th batch, evaluate loss: 0.6123245358467102:  25%|████▌             | 10/40 [00:00<00:02, 14.45it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5707578659057617:  69%|███████▌   | 104/151 [00:23<00:11,  4.25it/s]Epoch: 1, train for the 99-th batch, train loss: 0.5541556477546692:  41%|█████▍       | 98/237 [00:22<00:39,  3.55it/s]Epoch: 1, train for the 99-th batch, train loss: 0.5541556477546692:  42%|█████▍       | 99/237 [00:22<00:34,  3.99it/s]evaluate for the 11-th batch, evaluate loss: 0.567829966545105:  25%|████▊              | 10/40 [00:00<00:02, 14.45it/s]Epoch: 1, train for the 121-th batch, train loss: 0.5337089896202087:  82%|█████████  | 120/146 [00:24<00:05,  4.72it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5132094621658325:  22%|██▊          | 83/383 [00:20<01:04,  4.62it/s]Epoch: 1, train for the 121-th batch, train loss: 0.5337089896202087:  83%|█████████  | 121/146 [00:24<00:05,  4.83it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5132094621658325:  22%|██▊          | 84/383 [00:20<01:07,  4.41it/s]evaluate for the 12-th batch, evaluate loss: 0.5987159609794617:  25%|████▌             | 10/40 [00:00<00:02, 14.45it/s]evaluate for the 12-th batch, evaluate loss: 0.5987159609794617:  30%|█████▍            | 12/40 [00:00<00:02, 13.43it/s]evaluate for the 13-th batch, evaluate loss: 0.6104764342308044:  30%|█████▍            | 12/40 [00:00<00:02, 13.43it/s]Epoch: 1, train for the 107-th batch, train loss: 0.6218324303627014:  44%|████▊      | 106/241 [00:22<00:38,  3.55it/s]Epoch: 1, train for the 107-th batch, train loss: 0.6218324303627014:  44%|████▉      | 107/241 [00:22<00:39,  3.36it/s]evaluate for the 14-th batch, evaluate loss: 0.5869560241699219:  30%|█████▍            | 12/40 [00:01<00:02, 13.43it/s]evaluate for the 14-th batch, evaluate loss: 0.5869560241699219:  35%|██████▎           | 14/40 [00:01<00:02, 12.50it/s]Epoch: 1, train for the 122-th batch, train loss: 0.5695372819900513:  83%|█████████  | 121/146 [00:24<00:05,  4.83it/s]Epoch: 1, train for the 122-th batch, train loss: 0.5695372819900513:  84%|█████████▏ | 122/146 [00:24<00:05,  4.54it/s]evaluate for the 15-th batch, evaluate loss: 0.6064330339431763:  35%|██████▎           | 14/40 [00:01<00:02, 12.50it/s]Epoch: 1, train for the 105-th batch, train loss: 0.5391702055931091:  69%|███████▌   | 104/151 [00:23<00:11,  4.25it/s]Epoch: 1, train for the 105-th batch, train loss: 0.5391702055931091:  70%|███████▋   | 105/151 [00:23<00:13,  3.52it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5618818402290344:  44%|████▉      | 107/241 [00:22<00:39,  3.36it/s]evaluate for the 16-th batch, evaluate loss: 0.5922871232032776:  35%|██████▎           | 14/40 [00:01<00:02, 12.50it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5080456137657166:  22%|██▊          | 84/383 [00:21<01:07,  4.41it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5401297807693481:  42%|█████       | 99/237 [00:22<00:34,  3.99it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5618818402290344:  45%|████▉      | 108/241 [00:22<00:33,  4.03it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5080456137657166:  22%|██▉          | 85/383 [00:21<01:14,  3.98it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5401297807693481:  42%|████▋      | 100/237 [00:22<00:39,  3.46it/s]Epoch: 1, train for the 123-th batch, train loss: 0.5952675342559814:  84%|█████████▏ | 122/146 [00:24<00:05,  4.54it/s]evaluate for the 17-th batch, evaluate loss: 0.6529441475868225:  35%|██████▎           | 14/40 [00:01<00:02, 12.50it/s]evaluate for the 17-th batch, evaluate loss: 0.6529441475868225:  42%|███████▋          | 17/40 [00:01<00:01, 14.12it/s]Epoch: 1, train for the 123-th batch, train loss: 0.5952675342559814:  84%|█████████▎ | 123/146 [00:24<00:04,  4.99it/s]evaluate for the 18-th batch, evaluate loss: 0.6152696013450623:  42%|███████▋          | 17/40 [00:01<00:01, 14.12it/s]Epoch: 1, train for the 109-th batch, train loss: 0.6443257927894592:  45%|████▉      | 108/241 [00:22<00:33,  4.03it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5618669986724854:  70%|███████▋   | 105/151 [00:24<00:13,  3.52it/s]Epoch: 1, train for the 109-th batch, train loss: 0.6443257927894592:  45%|████▉      | 109/241 [00:22<00:29,  4.41it/s]Epoch: 1, train for the 86-th batch, train loss: 0.42958712577819824:  22%|██▋         | 85/383 [00:21<01:14,  3.98it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5618669986724854:  70%|███████▋   | 106/151 [00:24<00:11,  3.80it/s]Epoch: 1, train for the 86-th batch, train loss: 0.42958712577819824:  22%|██▋         | 86/383 [00:21<01:09,  4.29it/s]Epoch: 1, train for the 101-th batch, train loss: 0.595850944519043:  42%|█████       | 100/237 [00:23<00:39,  3.46it/s]Epoch: 1, train for the 101-th batch, train loss: 0.595850944519043:  43%|█████       | 101/237 [00:23<00:39,  3.42it/s]Epoch: 1, train for the 124-th batch, train loss: 0.5353869795799255:  84%|█████████▎ | 123/146 [00:24<00:04,  4.99it/s]evaluate for the 19-th batch, evaluate loss: 0.6236129403114319:  42%|███████▋          | 17/40 [00:01<00:01, 14.12it/s]evaluate for the 19-th batch, evaluate loss: 0.6236129403114319:  48%|████████▌         | 19/40 [00:01<00:01, 11.49it/s]Epoch: 1, train for the 124-th batch, train loss: 0.5353869795799255:  85%|█████████▎ | 124/146 [00:24<00:04,  4.61it/s]evaluate for the 20-th batch, evaluate loss: 0.6019224524497986:  48%|████████▌         | 19/40 [00:01<00:01, 11.49it/s]Epoch: 1, train for the 107-th batch, train loss: 0.5606757402420044:  70%|███████▋   | 106/151 [00:24<00:11,  3.80it/s]Epoch: 1, train for the 110-th batch, train loss: 0.6162704229354858:  45%|████▉      | 109/241 [00:22<00:29,  4.41it/s]evaluate for the 21-th batch, evaluate loss: 0.6316078305244446:  48%|████████▌         | 19/40 [00:01<00:01, 11.49it/s]evaluate for the 21-th batch, evaluate loss: 0.6316078305244446:  52%|█████████▍        | 21/40 [00:01<00:01, 12.08it/s]Epoch: 1, train for the 107-th batch, train loss: 0.5606757402420044:  71%|███████▊   | 107/151 [00:24<00:11,  3.68it/s]Epoch: 1, train for the 125-th batch, train loss: 0.5382401347160339:  85%|█████████▎ | 124/146 [00:24<00:04,  4.61it/s]Epoch: 1, train for the 110-th batch, train loss: 0.6162704229354858:  46%|█████      | 110/241 [00:23<00:33,  3.94it/s]Epoch: 1, train for the 125-th batch, train loss: 0.5382401347160339:  86%|█████████▍ | 125/146 [00:24<00:04,  5.03it/s]Epoch: 1, train for the 87-th batch, train loss: 0.49093931913375854:  22%|██▋         | 86/383 [00:21<01:09,  4.29it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5799055695533752:  43%|████▋      | 101/237 [00:23<00:39,  3.42it/s]evaluate for the 22-th batch, evaluate loss: 0.5787143707275391:  52%|█████████▍        | 21/40 [00:01<00:01, 12.08it/s]Epoch: 1, train for the 87-th batch, train loss: 0.49093931913375854:  23%|██▋         | 87/383 [00:21<01:16,  3.86it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5799055695533752:  43%|████▋      | 102/237 [00:23<00:37,  3.63it/s]evaluate for the 23-th batch, evaluate loss: 0.5463576316833496:  52%|█████████▍        | 21/40 [00:01<00:01, 12.08it/s]evaluate for the 23-th batch, evaluate loss: 0.5463576316833496:  57%|██████████▎       | 23/40 [00:01<00:01, 12.44it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5560036897659302:  86%|█████████▍ | 125/146 [00:25<00:04,  5.03it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5560036897659302:  86%|█████████▍ | 126/146 [00:25<00:03,  5.21it/s]evaluate for the 24-th batch, evaluate loss: 0.600719153881073:  57%|██████████▉        | 23/40 [00:01<00:01, 12.44it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5237101912498474:  71%|███████▊   | 107/151 [00:24<00:11,  3.68it/s]Epoch: 1, train for the 111-th batch, train loss: 0.32591983675956726:  46%|████▌     | 110/241 [00:23<00:33,  3.94it/s]Epoch: 1, train for the 111-th batch, train loss: 0.32591983675956726:  46%|████▌     | 111/241 [00:23<00:31,  4.13it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5237101912498474:  72%|███████▊   | 108/151 [00:24<00:11,  3.90it/s]Epoch: 1, train for the 103-th batch, train loss: 0.5986170768737793:  43%|████▋      | 102/237 [00:23<00:37,  3.63it/s]Epoch: 1, train for the 103-th batch, train loss: 0.5986170768737793:  43%|████▊      | 103/237 [00:23<00:32,  4.10it/s]Epoch: 1, train for the 88-th batch, train loss: 0.4985826909542084:  23%|██▉          | 87/383 [00:21<01:16,  3.86it/s]Epoch: 1, train for the 88-th batch, train loss: 0.4985826909542084:  23%|██▉          | 88/383 [00:21<01:19,  3.73it/s]Epoch: 1, train for the 109-th batch, train loss: 0.5306887030601501:  72%|███████▊   | 108/151 [00:24<00:11,  3.90it/s]Epoch: 1, train for the 109-th batch, train loss: 0.5306887030601501:  72%|███████▉   | 109/151 [00:24<00:09,  4.43it/s]Epoch: 1, train for the 112-th batch, train loss: 0.34507179260253906:  46%|████▌     | 111/241 [00:23<00:31,  4.13it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5499180555343628:  43%|████▊      | 103/237 [00:23<00:32,  4.10it/s]evaluate for the 25-th batch, evaluate loss: 0.628785252571106:  57%|██████████▉        | 23/40 [00:02<00:01, 12.44it/s]evaluate for the 25-th batch, evaluate loss: 0.628785252571106:  62%|███████████▉       | 25/40 [00:02<00:01, 10.84it/s]Epoch: 1, train for the 112-th batch, train loss: 0.34507179260253906:  46%|████▋     | 112/241 [00:23<00:28,  4.47it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5499180555343628:  44%|████▊      | 104/237 [00:23<00:29,  4.51it/s]evaluate for the 26-th batch, evaluate loss: 0.5681367516517639:  62%|███████████▎      | 25/40 [00:02<00:01, 10.84it/s]Epoch: 1, train for the 127-th batch, train loss: 0.5009449124336243:  86%|█████████▍ | 126/146 [00:25<00:03,  5.21it/s]Epoch: 1, train for the 89-th batch, train loss: 0.48080968856811523:  23%|██▊         | 88/383 [00:21<01:19,  3.73it/s]Epoch: 1, train for the 127-th batch, train loss: 0.5009449124336243:  87%|█████████▌ | 127/146 [00:25<00:04,  4.51it/s]evaluate for the 27-th batch, evaluate loss: 0.624008297920227:  62%|███████████▉       | 25/40 [00:02<00:01, 10.84it/s]evaluate for the 27-th batch, evaluate loss: 0.624008297920227:  68%|████████████▊      | 27/40 [00:02<00:01, 12.04it/s]Epoch: 1, train for the 89-th batch, train loss: 0.48080968856811523:  23%|██▊         | 89/383 [00:22<01:10,  4.18it/s]Epoch: 1, train for the 110-th batch, train loss: 0.5721877813339233:  72%|███████▉   | 109/151 [00:24<00:09,  4.43it/s]Epoch: 1, train for the 113-th batch, train loss: 0.3645919859409332:  46%|█████      | 112/241 [00:23<00:28,  4.47it/s]evaluate for the 28-th batch, evaluate loss: 0.5683823823928833:  68%|████████████▏     | 27/40 [00:02<00:01, 12.04it/s]Epoch: 1, train for the 110-th batch, train loss: 0.5721877813339233:  73%|████████   | 110/151 [00:24<00:08,  4.65it/s]Epoch: 1, train for the 113-th batch, train loss: 0.3645919859409332:  47%|█████▏     | 113/241 [00:23<00:26,  4.84it/s]Epoch: 1, train for the 105-th batch, train loss: 0.5459133982658386:  44%|████▊      | 104/237 [00:24<00:29,  4.51it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5697698593139648:  87%|█████████▌ | 127/146 [00:25<00:04,  4.51it/s]Epoch: 1, train for the 105-th batch, train loss: 0.5459133982658386:  44%|████▊      | 105/237 [00:24<00:28,  4.59it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5697698593139648:  88%|█████████▋ | 128/146 [00:25<00:03,  5.02it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5471494793891907:  73%|████████   | 110/151 [00:25<00:08,  4.65it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5471494793891907:  74%|████████   | 111/151 [00:25<00:07,  5.08it/s]Epoch: 1, train for the 106-th batch, train loss: 0.58347487449646:  44%|█████▊       | 105/237 [00:24<00:28,  4.59it/s]Epoch: 1, train for the 106-th batch, train loss: 0.58347487449646:  45%|█████▊       | 106/237 [00:24<00:25,  5.13it/s]evaluate for the 29-th batch, evaluate loss: 0.6123726963996887:  68%|████████████▏     | 27/40 [00:02<00:01, 12.04it/s]evaluate for the 29-th batch, evaluate loss: 0.6123726963996887:  72%|█████████████     | 29/40 [00:02<00:01, 10.15it/s]Epoch: 1, train for the 114-th batch, train loss: 0.36950454115867615:  47%|████▋     | 113/241 [00:23<00:26,  4.84it/s]Epoch: 1, train for the 114-th batch, train loss: 0.36950454115867615:  47%|████▋     | 114/241 [00:23<00:27,  4.69it/s]evaluate for the 30-th batch, evaluate loss: 0.6227919459342957:  72%|█████████████     | 29/40 [00:02<00:01, 10.15it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5641709566116333:  88%|█████████▋ | 128/146 [00:25<00:03,  5.02it/s]Epoch: 1, train for the 112-th batch, train loss: 0.4888744354248047:  74%|████████   | 111/151 [00:25<00:07,  5.08it/s]Epoch: 1, train for the 90-th batch, train loss: 0.48088598251342773:  23%|██▊         | 89/383 [00:22<01:10,  4.18it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5641709566116333:  88%|█████████▋ | 129/146 [00:25<00:03,  4.75it/s]Epoch: 1, train for the 112-th batch, train loss: 0.4888744354248047:  74%|████████▏  | 112/151 [00:25<00:07,  5.33it/s]Epoch: 1, train for the 90-th batch, train loss: 0.48088598251342773:  23%|██▊         | 90/383 [00:22<01:22,  3.56it/s]evaluate for the 31-th batch, evaluate loss: 0.5842358469963074:  72%|█████████████     | 29/40 [00:02<00:01, 10.15it/s]evaluate for the 31-th batch, evaluate loss: 0.5842358469963074:  78%|█████████████▉    | 31/40 [00:02<00:00, 11.60it/s]Epoch: 1, train for the 115-th batch, train loss: 0.474239706993103:  47%|█████▋      | 114/241 [00:23<00:27,  4.69it/s]Epoch: 1, train for the 107-th batch, train loss: 0.5627509355545044:  45%|████▉      | 106/237 [00:24<00:25,  5.13it/s]Epoch: 1, train for the 115-th batch, train loss: 0.474239706993103:  48%|█████▋      | 115/241 [00:23<00:23,  5.41it/s]Epoch: 1, train for the 107-th batch, train loss: 0.5627509355545044:  45%|████▉      | 107/237 [00:24<00:25,  5.12it/s]evaluate for the 32-th batch, evaluate loss: 0.627192497253418:  78%|██████████████▋    | 31/40 [00:02<00:00, 11.60it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5254855751991272:  88%|█████████▋ | 129/146 [00:25<00:03,  4.75it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5254855751991272:  89%|█████████▊ | 130/146 [00:25<00:03,  5.11it/s]evaluate for the 33-th batch, evaluate loss: 0.6012834310531616:  78%|█████████████▉    | 31/40 [00:02<00:00, 11.60it/s]evaluate for the 33-th batch, evaluate loss: 0.6012834310531616:  82%|██████████████▊   | 33/40 [00:02<00:00, 12.13it/s]Epoch: 1, train for the 113-th batch, train loss: 0.55353844165802:  74%|█████████▋   | 112/151 [00:25<00:07,  5.33it/s]Epoch: 1, train for the 91-th batch, train loss: 0.3934343755245209:  23%|███          | 90/383 [00:22<01:22,  3.56it/s]Epoch: 1, train for the 113-th batch, train loss: 0.55353844165802:  75%|█████████▋   | 113/151 [00:25<00:07,  5.28it/s]evaluate for the 34-th batch, evaluate loss: 0.6718365550041199:  82%|██████████████▊   | 33/40 [00:02<00:00, 12.13it/s]Epoch: 1, train for the 91-th batch, train loss: 0.3934343755245209:  24%|███          | 91/383 [00:22<01:16,  3.81it/s]Epoch: 1, train for the 116-th batch, train loss: 0.6107048392295837:  48%|█████▏     | 115/241 [00:24<00:23,  5.41it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5856931805610657:  45%|████▉      | 107/237 [00:24<00:25,  5.12it/s]Epoch: 1, train for the 116-th batch, train loss: 0.6107048392295837:  48%|█████▎     | 116/241 [00:24<00:28,  4.37it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5856931805610657:  46%|█████      | 108/237 [00:24<00:29,  4.42it/s]Epoch: 1, train for the 131-th batch, train loss: 0.5423017144203186:  89%|█████████▊ | 130/146 [00:26<00:03,  5.11it/s]Epoch: 1, train for the 131-th batch, train loss: 0.5423017144203186:  90%|█████████▊ | 131/146 [00:26<00:03,  4.40it/s]evaluate for the 35-th batch, evaluate loss: 0.6368436217308044:  82%|██████████████▊   | 33/40 [00:02<00:00, 12.13it/s]evaluate for the 35-th batch, evaluate loss: 0.6368436217308044:  88%|███████████████▊  | 35/40 [00:02<00:00,  9.48it/s]Epoch: 1, train for the 114-th batch, train loss: 0.4968711733818054:  75%|████████▏  | 113/151 [00:25<00:07,  5.28it/s]Epoch: 1, train for the 117-th batch, train loss: 0.6977013945579529:  48%|█████▎     | 116/241 [00:24<00:28,  4.37it/s]Epoch: 1, train for the 114-th batch, train loss: 0.4968711733818054:  75%|████████▎  | 114/151 [00:25<00:08,  4.40it/s]Epoch: 1, train for the 117-th batch, train loss: 0.6977013945579529:  49%|█████▎     | 117/241 [00:24<00:26,  4.74it/s]evaluate for the 36-th batch, evaluate loss: 0.6590726971626282:  88%|███████████████▊  | 35/40 [00:03<00:00,  9.48it/s]Epoch: 1, train for the 109-th batch, train loss: 0.6136753559112549:  46%|█████      | 108/237 [00:24<00:29,  4.42it/s]Epoch: 1, train for the 92-th batch, train loss: 0.42096349596977234:  24%|██▊         | 91/383 [00:22<01:16,  3.81it/s]Epoch: 1, train for the 109-th batch, train loss: 0.6136753559112549:  46%|█████      | 109/237 [00:24<00:28,  4.56it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5173352956771851:  90%|█████████▊ | 131/146 [00:26<00:03,  4.40it/s]evaluate for the 37-th batch, evaluate loss: 0.6612765192985535:  88%|███████████████▊  | 35/40 [00:03<00:00,  9.48it/s]evaluate for the 37-th batch, evaluate loss: 0.6612765192985535:  92%|████████████████▋ | 37/40 [00:03<00:00, 10.68it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5173352956771851:  90%|█████████▉ | 132/146 [00:26<00:02,  4.80it/s]Epoch: 1, train for the 92-th batch, train loss: 0.42096349596977234:  24%|██▉         | 92/383 [00:22<01:27,  3.32it/s]Epoch: 1, train for the 115-th batch, train loss: 0.4868803322315216:  75%|████████▎  | 114/151 [00:25<00:08,  4.40it/s]evaluate for the 38-th batch, evaluate loss: 0.6779365539550781:  92%|████████████████▋ | 37/40 [00:03<00:00, 10.68it/s]Epoch: 1, train for the 115-th batch, train loss: 0.4868803322315216:  76%|████████▍  | 115/151 [00:25<00:07,  4.87it/s]Epoch: 1, train for the 110-th batch, train loss: 0.577354371547699:  46%|█████▌      | 109/237 [00:25<00:28,  4.56it/s]Epoch: 1, train for the 110-th batch, train loss: 0.577354371547699:  46%|█████▌      | 110/237 [00:25<00:26,  4.85it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5273492932319641:  24%|███          | 92/383 [00:23<01:27,  3.32it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5273492932319641:  24%|███▏         | 93/383 [00:23<01:16,  3.80it/s]evaluate for the 39-th batch, evaluate loss: 0.6763301491737366:  92%|████████████████▋ | 37/40 [00:03<00:00, 10.68it/s]evaluate for the 39-th batch, evaluate loss: 0.6763301491737366:  98%|█████████████████▌| 39/40 [00:03<00:00,  8.98it/s]Epoch: 1, train for the 118-th batch, train loss: 0.5792137980461121:  49%|█████▎     | 117/241 [00:24<00:26,  4.74it/s]Epoch: 1, train for the 118-th batch, train loss: 0.5792137980461121:  49%|█████▍     | 118/241 [00:24<00:33,  3.71it/s]evaluate for the 40-th batch, evaluate loss: 0.856203019618988:  98%|██████████████████▌| 39/40 [00:03<00:00,  8.98it/s]evaluate for the 40-th batch, evaluate loss: 0.856203019618988: 100%|███████████████████| 40/40 [00:03<00:00, 11.50it/s]
Epoch: 1, train for the 94-th batch, train loss: 0.4026309847831726:  24%|███▏         | 93/383 [00:23<01:16,  3.80it/s]Epoch: 1, train for the 116-th batch, train loss: 0.47056517004966736:  76%|███████▌  | 115/151 [00:26<00:07,  4.87it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5693441033363342:  46%|█████      | 110/237 [00:25<00:26,  4.85it/s]Epoch: 1, train for the 133-th batch, train loss: 0.4888966381549835:  90%|█████████▉ | 132/146 [00:26<00:02,  4.80it/s]Epoch: 1, train for the 133-th batch, train loss: 0.4888966381549835:  91%|██████████ | 133/146 [00:26<00:03,  3.85it/s]Epoch: 1, train for the 116-th batch, train loss: 0.47056517004966736:  77%|███████▋  | 116/151 [00:26<00:08,  4.17it/s]Epoch: 1, train for the 94-th batch, train loss: 0.4026309847831726:  25%|███▏         | 94/383 [00:23<01:10,  4.09it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5693441033363342:  47%|█████▏     | 111/237 [00:25<00:27,  4.56it/s]Epoch: 1, train for the 119-th batch, train loss: 0.43812644481658936:  49%|████▉     | 118/241 [00:24<00:33,  3.71it/s]Epoch: 1, train for the 119-th batch, train loss: 0.43812644481658936:  49%|████▉     | 119/241 [00:24<00:28,  4.24it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5451300144195557:  91%|██████████ | 133/146 [00:26<00:03,  3.85it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5451300144195557:  92%|██████████ | 134/146 [00:26<00:02,  4.47it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5332282781600952:  25%|███▏         | 94/383 [00:23<01:10,  4.09it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5332282781600952:  25%|███▏         | 95/383 [00:23<01:03,  4.50it/s]  0%|                                                                                            | 0/21 [00:00<?, ?it/s]Epoch: 1, train for the 120-th batch, train loss: 0.6383547186851501:  49%|█████▍     | 119/241 [00:25<00:28,  4.24it/s]Epoch: 1, train for the 120-th batch, train loss: 0.6383547186851501:  50%|█████▍     | 120/241 [00:25<00:24,  4.91it/s]Epoch: 1, train for the 135-th batch, train loss: 0.546449601650238:  92%|███████████ | 134/146 [00:27<00:02,  4.47it/s]Epoch: 1, train for the 135-th batch, train loss: 0.546449601650238:  92%|███████████ | 135/146 [00:27<00:02,  5.19it/s]evaluate for the 1-th batch, evaluate loss: 0.7362448573112488:   0%|                            | 0/21 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.7538712620735168:   0%|                            | 0/21 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.7538712620735168:  10%|█▉                  | 2/21 [00:00<00:01, 16.16it/s]evaluate for the 3-th batch, evaluate loss: 0.7421914339065552:  10%|█▉                  | 2/21 [00:00<00:01, 16.16it/s]Epoch: 1, train for the 117-th batch, train loss: 0.5315738320350647:  77%|████████▍  | 116/151 [00:26<00:08,  4.17it/s]Epoch: 1, train for the 121-th batch, train loss: 0.6254822611808777:  50%|█████▍     | 120/241 [00:25<00:24,  4.91it/s]Epoch: 1, train for the 117-th batch, train loss: 0.5315738320350647:  77%|████████▌  | 117/151 [00:26<00:10,  3.37it/s]Epoch: 1, train for the 96-th batch, train loss: 0.4719903767108917:  25%|███▏         | 95/383 [00:23<01:03,  4.50it/s]Epoch: 1, train for the 121-th batch, train loss: 0.6254822611808777:  50%|█████▌     | 121/241 [00:25<00:24,  4.92it/s]Epoch: 1, train for the 136-th batch, train loss: 0.5186399817466736:  92%|██████████▏| 135/146 [00:27<00:02,  5.19it/s]evaluate for the 4-th batch, evaluate loss: 0.7068189978599548:  10%|█▉                  | 2/21 [00:00<00:01, 16.16it/s]evaluate for the 4-th batch, evaluate loss: 0.7068189978599548:  19%|███▊                | 4/21 [00:00<00:01, 15.67it/s]Epoch: 1, train for the 96-th batch, train loss: 0.4719903767108917:  25%|███▎         | 96/383 [00:23<01:09,  4.13it/s]Epoch: 1, train for the 112-th batch, train loss: 0.5816563367843628:  47%|█████▏     | 111/237 [00:25<00:27,  4.56it/s]Epoch: 1, train for the 136-th batch, train loss: 0.5186399817466736:  93%|██████████▏| 136/146 [00:27<00:01,  5.01it/s]Epoch: 1, train for the 112-th batch, train loss: 0.5816563367843628:  47%|█████▏     | 112/237 [00:25<00:36,  3.39it/s]evaluate for the 5-th batch, evaluate loss: 0.7720149159431458:  19%|███▊                | 4/21 [00:00<00:01, 15.67it/s]Epoch: 1, train for the 118-th batch, train loss: 0.47892242670059204:  77%|███████▋  | 117/151 [00:26<00:10,  3.37it/s]Epoch: 1, train for the 118-th batch, train loss: 0.47892242670059204:  78%|███████▊  | 118/151 [00:26<00:08,  3.80it/s]evaluate for the 6-th batch, evaluate loss: 0.7704247832298279:  19%|███▊                | 4/21 [00:00<00:01, 15.67it/s]evaluate for the 6-th batch, evaluate loss: 0.7704247832298279:  29%|█████▋              | 6/21 [00:00<00:01, 14.16it/s]evaluate for the 7-th batch, evaluate loss: 0.7221032381057739:  29%|█████▋              | 6/21 [00:00<00:01, 14.16it/s]Epoch: 1, train for the 113-th batch, train loss: 0.5853753685951233:  47%|█████▏     | 112/237 [00:25<00:36,  3.39it/s]Epoch: 1, train for the 97-th batch, train loss: 0.3784199357032776:  25%|███▎         | 96/383 [00:24<01:09,  4.13it/s]Epoch: 1, train for the 113-th batch, train loss: 0.5853753685951233:  48%|█████▏     | 113/237 [00:26<00:34,  3.64it/s]evaluate for the 8-th batch, evaluate loss: 0.7453360557556152:  29%|█████▋              | 6/21 [00:00<00:01, 14.16it/s]evaluate for the 8-th batch, evaluate loss: 0.7453360557556152:  38%|███████▌            | 8/21 [00:00<00:00, 15.67it/s]Epoch: 1, train for the 97-th batch, train loss: 0.3784199357032776:  25%|███▎         | 97/383 [00:24<01:11,  4.00it/s]Epoch: 1, train for the 122-th batch, train loss: 0.5507530570030212:  50%|█████▌     | 121/241 [00:25<00:24,  4.92it/s]evaluate for the 9-th batch, evaluate loss: 0.7625836133956909:  38%|███████▌            | 8/21 [00:00<00:00, 15.67it/s]Epoch: 1, train for the 122-th batch, train loss: 0.5507530570030212:  51%|█████▌     | 122/241 [00:25<00:29,  4.05it/s]evaluate for the 10-th batch, evaluate loss: 0.7649845480918884:  38%|███████▏           | 8/21 [00:00<00:00, 15.67it/s]evaluate for the 10-th batch, evaluate loss: 0.7649845480918884:  48%|████████▌         | 10/21 [00:00<00:00, 16.96it/s]Epoch: 1, train for the 137-th batch, train loss: 0.5028939843177795:  93%|██████████▏| 136/146 [00:27<00:01,  5.01it/s]Epoch: 1, train for the 119-th batch, train loss: 0.5224573016166687:  78%|████████▌  | 118/151 [00:27<00:08,  3.80it/s]Epoch: 1, train for the 137-th batch, train loss: 0.5028939843177795:  94%|██████████▎| 137/146 [00:27<00:02,  3.88it/s]Epoch: 1, train for the 114-th batch, train loss: 0.5501583218574524:  48%|█████▏     | 113/237 [00:26<00:34,  3.64it/s]Epoch: 1, train for the 119-th batch, train loss: 0.5224573016166687:  79%|████████▋  | 119/151 [00:27<00:08,  3.77it/s]evaluate for the 11-th batch, evaluate loss: 0.7719883322715759:  48%|████████▌         | 10/21 [00:00<00:00, 16.96it/s]Epoch: 1, train for the 114-th batch, train loss: 0.5501583218574524:  48%|█████▎     | 114/237 [00:26<00:30,  4.02it/s]Epoch: 1, train for the 123-th batch, train loss: 0.6536162495613098:  51%|█████▌     | 122/241 [00:25<00:29,  4.05it/s]Epoch: 1, train for the 98-th batch, train loss: 0.339263916015625:  25%|███▌          | 97/383 [00:24<01:11,  4.00it/s]Epoch: 1, train for the 123-th batch, train loss: 0.6536162495613098:  51%|█████▌     | 123/241 [00:25<00:25,  4.67it/s]evaluate for the 12-th batch, evaluate loss: 0.7234304547309875:  48%|████████▌         | 10/21 [00:00<00:00, 16.96it/s]evaluate for the 12-th batch, evaluate loss: 0.7234304547309875:  57%|██████████▎       | 12/21 [00:00<00:00, 16.27it/s]Epoch: 1, train for the 98-th batch, train loss: 0.339263916015625:  26%|███▌          | 98/383 [00:24<01:09,  4.07it/s]Epoch: 1, train for the 138-th batch, train loss: 0.5482919812202454:  94%|██████████▎| 137/146 [00:27<00:02,  3.88it/s]Epoch: 1, train for the 138-th batch, train loss: 0.5482919812202454:  95%|██████████▍| 138/146 [00:27<00:01,  4.45it/s]evaluate for the 13-th batch, evaluate loss: 0.7180419564247131:  57%|██████████▎       | 12/21 [00:00<00:00, 16.27it/s]Epoch: 1, train for the 115-th batch, train loss: 0.5339283347129822:  48%|█████▎     | 114/237 [00:26<00:30,  4.02it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5380920767784119:  79%|████████▋  | 119/151 [00:27<00:08,  3.77it/s]Epoch: 1, train for the 124-th batch, train loss: 0.42669403553009033:  51%|█████     | 123/241 [00:25<00:25,  4.67it/s]evaluate for the 14-th batch, evaluate loss: 0.6981451511383057:  57%|██████████▎       | 12/21 [00:00<00:00, 16.27it/s]evaluate for the 14-th batch, evaluate loss: 0.6981451511383057:  67%|████████████      | 14/21 [00:00<00:00, 15.90it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5380920767784119:  79%|████████▋  | 120/151 [00:27<00:07,  4.00it/s]Epoch: 1, train for the 115-th batch, train loss: 0.5339283347129822:  49%|█████▎     | 115/237 [00:26<00:28,  4.32it/s]Epoch: 1, train for the 124-th batch, train loss: 0.42669403553009033:  51%|█████▏    | 124/241 [00:25<00:23,  4.92it/s]Epoch: 1, train for the 99-th batch, train loss: 0.4966159760951996:  26%|███▎         | 98/383 [00:24<01:09,  4.07it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5531509518623352:  95%|██████████▍| 138/146 [00:27<00:01,  4.45it/s]Epoch: 1, train for the 99-th batch, train loss: 0.4966159760951996:  26%|███▎         | 99/383 [00:24<01:05,  4.31it/s]evaluate for the 15-th batch, evaluate loss: 0.7243577241897583:  67%|████████████      | 14/21 [00:00<00:00, 15.90it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5531509518623352:  95%|██████████▍| 139/146 [00:27<00:01,  4.87it/s]evaluate for the 16-th batch, evaluate loss: 0.69526606798172:  67%|█████████████▎      | 14/21 [00:01<00:00, 15.90it/s]evaluate for the 16-th batch, evaluate loss: 0.69526606798172:  76%|███████████████▏    | 16/21 [00:01<00:00, 14.62it/s]evaluate for the 17-th batch, evaluate loss: 0.6697567701339722:  76%|█████████████▋    | 16/21 [00:01<00:00, 14.62it/s]evaluate for the 18-th batch, evaluate loss: 0.6955434083938599:  76%|█████████████▋    | 16/21 [00:01<00:00, 14.62it/s]Epoch: 1, train for the 121-th batch, train loss: 0.5072742104530334:  79%|████████▋  | 120/151 [00:27<00:07,  4.00it/s]Epoch: 1, train for the 125-th batch, train loss: 0.5534836649894714:  51%|█████▋     | 124/241 [00:26<00:23,  4.92it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5667879581451416:  26%|███         | 99/383 [00:24<01:05,  4.31it/s]Epoch: 1, train for the 116-th batch, train loss: 0.62882000207901:  49%|██████▎      | 115/237 [00:26<00:28,  4.32it/s]Epoch: 1, train for the 121-th batch, train loss: 0.5072742104530334:  80%|████████▊  | 121/151 [00:27<00:07,  3.86it/s]Epoch: 1, train for the 125-th batch, train loss: 0.5534836649894714:  52%|█████▋     | 125/241 [00:26<00:26,  4.38it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5667879581451416:  26%|██▊        | 100/383 [00:24<01:05,  4.33it/s]Epoch: 1, train for the 140-th batch, train loss: 0.523707926273346:  95%|███████████▍| 139/146 [00:28<00:01,  4.87it/s]Epoch: 1, train for the 116-th batch, train loss: 0.62882000207901:  49%|██████▎      | 116/237 [00:26<00:30,  3.98it/s]evaluate for the 19-th batch, evaluate loss: 0.7325589656829834:  76%|█████████████▋    | 16/21 [00:01<00:00, 14.62it/s]evaluate for the 19-th batch, evaluate loss: 0.7325589656829834:  90%|████████████████▎ | 19/21 [00:01<00:00, 16.23it/s]Epoch: 1, train for the 140-th batch, train loss: 0.523707926273346:  96%|███████████▌| 140/146 [00:28<00:01,  4.69it/s]evaluate for the 20-th batch, evaluate loss: 0.7092244029045105:  90%|████████████████▎ | 19/21 [00:01<00:00, 16.23it/s]Epoch: 1, train for the 122-th batch, train loss: 0.5007823705673218:  80%|████████▊  | 121/151 [00:27<00:07,  3.86it/s]evaluate for the 21-th batch, evaluate loss: 0.9094957113265991:  90%|████████████████▎ | 19/21 [00:01<00:00, 16.23it/s]evaluate for the 21-th batch, evaluate loss: 0.9094957113265991: 100%|██████████████████| 21/21 [00:01<00:00, 15.33it/s]evaluate for the 21-th batch, evaluate loss: 0.9094957113265991: 100%|██████████████████| 21/21 [00:01<00:00, 15.58it/s]
Epoch: 1, train for the 122-th batch, train loss: 0.5007823705673218:  81%|████████▉  | 122/151 [00:27<00:06,  4.22it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5020073652267456:  52%|█████▋     | 125/241 [00:26<00:26,  4.38it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5020073652267456:  52%|█████▊     | 126/241 [00:26<00:25,  4.60it/s]Epoch: 1, train for the 117-th batch, train loss: 0.5673620700836182:  49%|█████▍     | 116/237 [00:26<00:30,  3.98it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5345271825790405:  26%|██▊        | 100/383 [00:25<01:05,  4.33it/s]Epoch: 1, train for the 117-th batch, train loss: 0.5673620700836182:  49%|█████▍     | 117/237 [00:26<00:30,  3.98it/s]INFO:root:Epoch: 1, learning rate: 0.0001, train loss: 0.5396
INFO:root:train average_precision, 0.8224
INFO:root:train roc_auc, 0.8278
INFO:root:validate loss: 0.6108
INFO:root:validate average_precision, 0.7557
INFO:root:validate roc_auc, 0.7419
INFO:root:new node validate loss: 0.7393
INFO:root:new node validate first_1_average_precision, 0.6850
INFO:root:new node validate first_1_roc_auc, 0.6604
INFO:root:new node validate first_3_average_precision, 0.6721
INFO:root:new node validate first_3_roc_auc, 0.6458
INFO:root:new node validate first_10_average_precision, 0.6424
INFO:root:new node validate first_10_roc_auc, 0.6223
INFO:root:new node validate average_precision, 0.6049
INFO:root:new node validate roc_auc, 0.5865
INFO:root:save model ./saved_models/TGN/ia-reality-call/TGN_seed0_dummy-e41ajx4p/TGN_seed0_dummy-e41ajx4p.pkl
Epoch: 1, train for the 101-th batch, train loss: 0.5345271825790405:  26%|██▉        | 101/383 [00:25<01:08,  4.10it/s]Epoch: 1, train for the 141-th batch, train loss: 0.5348172783851624:  96%|██████████▌| 140/146 [00:28<00:01,  4.69it/s]Epoch: 1, train for the 141-th batch, train loss: 0.5348172783851624:  97%|██████████▌| 141/146 [00:28<00:01,  3.76it/s]Epoch: 1, train for the 118-th batch, train loss: 0.562131404876709:  49%|█████▉      | 117/237 [00:27<00:30,  3.98it/s]Epoch: 1, train for the 102-th batch, train loss: 0.484028160572052:  26%|███▏        | 101/383 [00:25<01:08,  4.10it/s]Epoch: 1, train for the 118-th batch, train loss: 0.562131404876709:  50%|█████▉      | 118/237 [00:27<00:28,  4.23it/s]Epoch: 1, train for the 127-th batch, train loss: 0.6135058403015137:  52%|█████▊     | 126/241 [00:26<00:25,  4.60it/s]Epoch: 1, train for the 123-th batch, train loss: 0.5660075545310974:  81%|████████▉  | 122/151 [00:28<00:06,  4.22it/s]Epoch: 1, train for the 102-th batch, train loss: 0.484028160572052:  27%|███▏        | 102/383 [00:25<01:04,  4.38it/s]Epoch: 1, train for the 127-th batch, train loss: 0.6135058403015137:  53%|█████▊     | 127/241 [00:26<00:27,  4.22it/s]Epoch: 1, train for the 123-th batch, train loss: 0.5660075545310974:  81%|████████▉  | 123/151 [00:28<00:07,  3.88it/s]Epoch: 1, train for the 142-th batch, train loss: 0.5070449113845825:  97%|██████████▌| 141/146 [00:28<00:01,  3.76it/s]Epoch: 1, train for the 142-th batch, train loss: 0.5070449113845825:  97%|██████████▋| 142/146 [00:28<00:00,  4.53it/s]Epoch: 1, train for the 119-th batch, train loss: 0.5364187359809875:  50%|█████▍     | 118/237 [00:27<00:28,  4.23it/s]Epoch: 1, train for the 119-th batch, train loss: 0.5364187359809875:  50%|█████▌     | 119/237 [00:27<00:25,  4.71it/s]Epoch: 1, train for the 124-th batch, train loss: 0.503486692905426:  81%|█████████▊  | 123/151 [00:28<00:07,  3.88it/s]Epoch: 1, train for the 143-th batch, train loss: 0.5283958911895752:  97%|██████████▋| 142/146 [00:28<00:00,  4.53it/s]Epoch: 1, train for the 143-th batch, train loss: 0.5283958911895752:  98%|██████████▊| 143/146 [00:28<00:00,  5.18it/s]Epoch: 1, train for the 103-th batch, train loss: 0.3282330334186554:  27%|██▉        | 102/383 [00:25<01:04,  4.38it/s]Epoch: 1, train for the 124-th batch, train loss: 0.503486692905426:  82%|█████████▊  | 124/151 [00:28<00:06,  4.27it/s]Epoch: 1, train for the 103-th batch, train loss: 0.3282330334186554:  27%|██▉        | 103/383 [00:25<01:03,  4.41it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5548347234725952:  50%|█████▌     | 119/237 [00:27<00:25,  4.71it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5839263200759888:  53%|█████▊     | 127/241 [00:27<00:27,  4.22it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5548347234725952:  51%|█████▌     | 120/237 [00:27<00:23,  4.94it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5839263200759888:  53%|█████▊     | 128/241 [00:27<00:30,  3.75it/s]Epoch: 1, train for the 144-th batch, train loss: 0.4893139600753784:  98%|██████████▊| 143/146 [00:29<00:00,  5.18it/s]Epoch: 1, train for the 144-th batch, train loss: 0.4893139600753784:  99%|██████████▊| 144/146 [00:29<00:00,  4.99it/s]Epoch: 1, train for the 104-th batch, train loss: 0.4231724739074707:  27%|██▉        | 103/383 [00:25<01:03,  4.41it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]Epoch: 1, train for the 125-th batch, train loss: 0.550220251083374:  82%|█████████▊  | 124/151 [00:28<00:06,  4.27it/s]Epoch: 1, train for the 125-th batch, train loss: 0.550220251083374:  83%|█████████▉  | 125/151 [00:28<00:06,  4.24it/s]Epoch: 1, train for the 104-th batch, train loss: 0.4231724739074707:  27%|██▉        | 104/383 [00:25<01:01,  4.51it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5111778378486633:  53%|█████▊     | 128/241 [00:27<00:30,  3.75it/s]Epoch: 1, train for the 121-th batch, train loss: 0.5877308249473572:  51%|█████▌     | 120/237 [00:27<00:23,  4.94it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5111778378486633:  54%|█████▉     | 129/241 [00:27<00:25,  4.38it/s]Epoch: 1, train for the 121-th batch, train loss: 0.5877308249473572:  51%|█████▌     | 121/237 [00:27<00:22,  5.11it/s]Epoch: 1, train for the 145-th batch, train loss: 0.5050992369651794:  99%|██████████▊| 144/146 [00:29<00:00,  4.99it/s]Epoch: 1, train for the 145-th batch, train loss: 0.5050992369651794:  99%|██████████▉| 145/146 [00:29<00:00,  5.61it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5255498886108398:  83%|█████████  | 125/151 [00:28<00:06,  4.24it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5255498886108398:  83%|█████████▏ | 126/151 [00:28<00:05,  4.80it/s]Epoch: 2, train for the 1-th batch, train loss: 0.9548770785331726:   0%|                       | 0/119 [00:00<?, ?it/s]Epoch: 2, train for the 1-th batch, train loss: 0.9548770785331726:   1%|▏              | 1/119 [00:00<00:25,  4.70it/s]Epoch: 1, train for the 146-th batch, train loss: 0.47363531589508057:  99%|█████████▉| 145/146 [00:29<00:00,  5.61it/s]Epoch: 1, train for the 146-th batch, train loss: 0.47363531589508057: 100%|██████████| 146/146 [00:29<00:00,  5.80it/s]Epoch: 1, train for the 146-th batch, train loss: 0.47363531589508057: 100%|██████████| 146/146 [00:29<00:00,  4.98it/s]
Epoch: 1, train for the 105-th batch, train loss: 0.3255462944507599:  27%|██▉        | 104/383 [00:25<01:01,  4.51it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5059067606925964:  54%|█████▉     | 129/241 [00:27<00:25,  4.38it/s]Epoch: 1, train for the 122-th batch, train loss: 0.550767183303833:  51%|██████▏     | 121/237 [00:27<00:22,  5.11it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5059067606925964:  54%|█████▉     | 130/241 [00:27<00:25,  4.39it/s]Epoch: 1, train for the 105-th batch, train loss: 0.3255462944507599:  27%|███        | 105/383 [00:25<01:06,  4.19it/s]Epoch: 1, train for the 122-th batch, train loss: 0.550767183303833:  51%|██████▏     | 122/237 [00:27<00:23,  4.90it/s]  0%|                                                                                            | 0/38 [00:00<?, ?it/s]Epoch: 2, train for the 2-th batch, train loss: 1.0112791061401367:   1%|▏              | 1/119 [00:00<00:25,  4.70it/s]Epoch: 2, train for the 2-th batch, train loss: 1.0112791061401367:   2%|▎              | 2/119 [00:00<00:19,  5.92it/s]Epoch: 1, train for the 131-th batch, train loss: 0.5548768043518066:  54%|█████▉     | 130/241 [00:27<00:25,  4.39it/s]evaluate for the 1-th batch, evaluate loss: 0.5034996867179871:   0%|                            | 0/38 [00:00<?, ?it/s]Epoch: 1, train for the 131-th batch, train loss: 0.5548768043518066:  54%|█████▉     | 131/241 [00:27<00:21,  5.03it/s]Epoch: 1, train for the 127-th batch, train loss: 0.49602389335632324:  83%|████████▎ | 126/151 [00:28<00:05,  4.80it/s]Epoch: 1, train for the 127-th batch, train loss: 0.49602389335632324:  84%|████████▍ | 127/151 [00:29<00:05,  4.25it/s]Epoch: 2, train for the 3-th batch, train loss: 0.9263918995857239:   2%|▎              | 2/119 [00:00<00:19,  5.92it/s]Epoch: 2, train for the 3-th batch, train loss: 0.9263918995857239:   3%|▍              | 3/119 [00:00<00:18,  6.23it/s]evaluate for the 2-th batch, evaluate loss: 0.5165563225746155:   0%|                            | 0/38 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5165563225746155:   5%|█                   | 2/38 [00:00<00:02, 12.84it/s]Epoch: 1, train for the 123-th batch, train loss: 0.5189517736434937:  51%|█████▋     | 122/237 [00:28<00:23,  4.90it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5002352595329285:  54%|█████▉     | 131/241 [00:27<00:21,  5.03it/s]evaluate for the 3-th batch, evaluate loss: 0.49356168508529663:   5%|█                  | 2/38 [00:00<00:02, 12.84it/s]Epoch: 1, train for the 106-th batch, train loss: 0.3200826048851013:  27%|███        | 105/383 [00:26<01:06,  4.19it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5002352595329285:  55%|██████     | 132/241 [00:27<00:19,  5.53it/s]Epoch: 1, train for the 123-th batch, train loss: 0.5189517736434937:  52%|█████▋     | 123/237 [00:28<00:25,  4.52it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5332163572311401:  84%|█████████▎ | 127/151 [00:29<00:05,  4.25it/s]Epoch: 1, train for the 106-th batch, train loss: 0.3200826048851013:  28%|███        | 106/383 [00:26<01:10,  3.93it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5332163572311401:  85%|█████████▎ | 128/151 [00:29<00:04,  4.81it/s]evaluate for the 4-th batch, evaluate loss: 0.5055515170097351:   5%|█                   | 2/38 [00:00<00:02, 12.84it/s]evaluate for the 4-th batch, evaluate loss: 0.5055515170097351:  11%|██                  | 4/38 [00:00<00:02, 15.08it/s]Epoch: 2, train for the 4-th batch, train loss: 0.7424671649932861:   3%|▍              | 3/119 [00:00<00:18,  6.23it/s]Epoch: 2, train for the 4-th batch, train loss: 0.7424671649932861:   3%|▌              | 4/119 [00:00<00:18,  6.32it/s]evaluate for the 5-th batch, evaluate loss: 0.5422025918960571:  11%|██                  | 4/38 [00:00<00:02, 15.08it/s]Epoch: 1, train for the 107-th batch, train loss: 0.47771093249320984:  28%|██▊       | 106/383 [00:26<01:10,  3.93it/s]evaluate for the 6-th batch, evaluate loss: 0.5264851450920105:  11%|██                  | 4/38 [00:00<00:02, 15.08it/s]evaluate for the 6-th batch, evaluate loss: 0.5264851450920105:  16%|███▏                | 6/38 [00:00<00:02, 14.18it/s]Epoch: 1, train for the 107-th batch, train loss: 0.47771093249320984:  28%|██▊       | 107/383 [00:26<01:05,  4.18it/s]Epoch: 1, train for the 133-th batch, train loss: 0.44692283868789673:  55%|█████▍    | 132/241 [00:27<00:19,  5.53it/s]Epoch: 2, train for the 5-th batch, train loss: 0.5919407606124878:   3%|▌              | 4/119 [00:00<00:18,  6.32it/s]Epoch: 2, train for the 5-th batch, train loss: 0.5919407606124878:   4%|▋              | 5/119 [00:00<00:17,  6.39it/s]evaluate for the 7-th batch, evaluate loss: 0.4648680090904236:  16%|███▏                | 6/38 [00:00<00:02, 14.18it/s]Epoch: 1, train for the 124-th batch, train loss: 0.5103901028633118:  52%|█████▋     | 123/237 [00:28<00:25,  4.52it/s]Epoch: 1, train for the 133-th batch, train loss: 0.44692283868789673:  55%|█████▌    | 133/241 [00:27<00:22,  4.88it/s]Epoch: 1, train for the 124-th batch, train loss: 0.5103901028633118:  52%|█████▊     | 124/237 [00:28<00:26,  4.22it/s]evaluate for the 8-th batch, evaluate loss: 0.49175140261650085:  16%|███                | 6/38 [00:00<00:02, 14.18it/s]evaluate for the 8-th batch, evaluate loss: 0.49175140261650085:  21%|████               | 8/38 [00:00<00:02, 14.88it/s]Epoch: 2, train for the 6-th batch, train loss: 0.510254442691803:   4%|▋               | 5/119 [00:00<00:17,  6.39it/s]Epoch: 2, train for the 6-th batch, train loss: 0.510254442691803:   5%|▊               | 6/119 [00:00<00:16,  6.97it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5248304605484009:  85%|█████████▎ | 128/151 [00:29<00:04,  4.81it/s]evaluate for the 9-th batch, evaluate loss: 0.5112788081169128:  21%|████▏               | 8/38 [00:00<00:02, 14.88it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5248304605484009:  85%|█████████▍ | 129/151 [00:29<00:05,  3.91it/s]Epoch: 1, train for the 108-th batch, train loss: 0.43677765130996704:  28%|██▊       | 107/383 [00:26<01:05,  4.18it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5360750555992126:  55%|██████     | 133/241 [00:28<00:22,  4.88it/s]Epoch: 1, train for the 108-th batch, train loss: 0.43677765130996704:  28%|██▊       | 108/383 [00:26<01:02,  4.37it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5360750555992126:  56%|██████     | 134/241 [00:28<00:21,  5.06it/s]Epoch: 1, train for the 125-th batch, train loss: 0.5004327893257141:  52%|█████▊     | 124/237 [00:28<00:26,  4.22it/s]evaluate for the 10-th batch, evaluate loss: 0.5387439131736755:  21%|████               | 8/38 [00:00<00:02, 14.88it/s]evaluate for the 10-th batch, evaluate loss: 0.5387439131736755:  26%|████▋             | 10/38 [00:00<00:01, 14.78it/s]Epoch: 1, train for the 125-th batch, train loss: 0.5004327893257141:  53%|█████▊     | 125/237 [00:28<00:25,  4.45it/s]Epoch: 2, train for the 7-th batch, train loss: 0.546607255935669:   5%|▊               | 6/119 [00:01<00:16,  6.97it/s]Epoch: 2, train for the 7-th batch, train loss: 0.546607255935669:   6%|▉               | 7/119 [00:01<00:16,  6.93it/s]evaluate for the 11-th batch, evaluate loss: 0.491984099149704:  26%|█████              | 10/38 [00:00<00:01, 14.78it/s]Epoch: 1, train for the 109-th batch, train loss: 0.43023955821990967:  28%|██▊       | 108/383 [00:26<01:02,  4.37it/s]Epoch: 2, train for the 8-th batch, train loss: 0.4826032221317291:   6%|▉              | 7/119 [00:01<00:16,  6.93it/s]Epoch: 1, train for the 109-th batch, train loss: 0.43023955821990967:  28%|██▊       | 109/383 [00:26<00:59,  4.61it/s]Epoch: 2, train for the 9-th batch, train loss: 0.4983171820640564:   6%|▉              | 7/119 [00:01<00:16,  6.93it/s]Epoch: 2, train for the 9-th batch, train loss: 0.4983171820640564:   8%|█▏             | 9/119 [00:01<00:13,  8.46it/s]Epoch: 1, train for the 135-th batch, train loss: 0.40312379598617554:  56%|█████▌    | 134/241 [00:28<00:21,  5.06it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5366311073303223:  85%|█████████▍ | 129/151 [00:29<00:05,  3.91it/s]Epoch: 1, train for the 135-th batch, train loss: 0.40312379598617554:  56%|█████▌    | 135/241 [00:28<00:24,  4.30it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5366311073303223:  86%|█████████▍ | 130/151 [00:29<00:05,  3.51it/s]Epoch: 2, train for the 10-th batch, train loss: 0.5434032082557678:   8%|█             | 9/119 [00:01<00:13,  8.46it/s]Epoch: 1, train for the 110-th batch, train loss: 0.3788513243198395:  28%|███▏       | 109/383 [00:27<00:59,  4.61it/s]evaluate for the 12-th batch, evaluate loss: 0.5552001595497131:  26%|████▋             | 10/38 [00:01<00:01, 14.78it/s]evaluate for the 12-th batch, evaluate loss: 0.5552001595497131:  32%|█████▋            | 12/38 [00:01<00:02,  9.32it/s]Epoch: 1, train for the 110-th batch, train loss: 0.3788513243198395:  29%|███▏       | 110/383 [00:27<01:00,  4.49it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5468834042549133:  53%|█████▊     | 125/237 [00:28<00:25,  4.45it/s]Epoch: 1, train for the 136-th batch, train loss: 0.4489826560020447:  56%|██████▏    | 135/241 [00:28<00:24,  4.30it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5468834042549133:  53%|█████▊     | 126/237 [00:29<00:31,  3.52it/s]Epoch: 1, train for the 136-th batch, train loss: 0.4489826560020447:  56%|██████▏    | 136/241 [00:28<00:21,  4.80it/s]evaluate for the 13-th batch, evaluate loss: 0.5162437558174133:  32%|█████▋            | 12/38 [00:01<00:02,  9.32it/s]Epoch: 2, train for the 11-th batch, train loss: 0.49111098051071167:   8%|▉            | 9/119 [00:01<00:13,  8.46it/s]Epoch: 1, train for the 131-th batch, train loss: 0.4580836296081543:  86%|█████████▍ | 130/151 [00:30<00:05,  3.51it/s]Epoch: 2, train for the 11-th batch, train loss: 0.49111098051071167:   9%|█           | 11/119 [00:01<00:13,  8.17it/s]Epoch: 1, train for the 131-th batch, train loss: 0.4580836296081543:  87%|█████████▌ | 131/151 [00:30<00:05,  3.80it/s]evaluate for the 14-th batch, evaluate loss: 0.4634144604206085:  32%|█████▋            | 12/38 [00:01<00:02,  9.32it/s]evaluate for the 14-th batch, evaluate loss: 0.4634144604206085:  37%|██████▋           | 14/38 [00:01<00:02, 10.23it/s]evaluate for the 15-th batch, evaluate loss: 0.49955496191978455:  37%|██████▎          | 14/38 [00:01<00:02, 10.23it/s]Epoch: 2, train for the 12-th batch, train loss: 0.5245625972747803:   9%|█▏           | 11/119 [00:01<00:13,  8.17it/s]Epoch: 2, train for the 12-th batch, train loss: 0.5245625972747803:  10%|█▎           | 12/119 [00:01<00:13,  7.84it/s]Epoch: 2, train for the 13-th batch, train loss: 0.4521298110485077:  10%|█▎           | 12/119 [00:01<00:13,  7.84it/s]Epoch: 1, train for the 137-th batch, train loss: 0.4791136682033539:  56%|██████▏    | 136/241 [00:28<00:21,  4.80it/s]Epoch: 1, train for the 137-th batch, train loss: 0.4791136682033539:  57%|██████▎    | 137/241 [00:28<00:24,  4.31it/s]Epoch: 1, train for the 111-th batch, train loss: 0.4722537398338318:  29%|███▏       | 110/383 [00:27<01:00,  4.49it/s]Epoch: 1, train for the 127-th batch, train loss: 0.533156156539917:  53%|██████▍     | 126/237 [00:29<00:31,  3.52it/s]Epoch: 1, train for the 111-th batch, train loss: 0.4722537398338318:  29%|███▏       | 111/383 [00:27<01:12,  3.77it/s]Epoch: 1, train for the 127-th batch, train loss: 0.533156156539917:  54%|██████▍     | 127/237 [00:29<00:32,  3.38it/s]Epoch: 2, train for the 14-th batch, train loss: 0.4672505557537079:  10%|█▎           | 12/119 [00:01<00:13,  7.84it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5263385772705078:  87%|█████████▌ | 131/151 [00:30<00:05,  3.80it/s]Epoch: 2, train for the 14-th batch, train loss: 0.4672505557537079:  12%|█▌           | 14/119 [00:01<00:12,  8.75it/s]evaluate for the 16-th batch, evaluate loss: 0.5220211148262024:  37%|██████▋           | 14/38 [00:01<00:02, 10.23it/s]evaluate for the 16-th batch, evaluate loss: 0.5220211148262024:  42%|███████▌          | 16/38 [00:01<00:02,  8.84it/s]Epoch: 1, train for the 138-th batch, train loss: 0.39337316155433655:  57%|█████▋    | 137/241 [00:29<00:24,  4.31it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5263385772705078:  87%|█████████▌ | 132/151 [00:30<00:05,  3.49it/s]Epoch: 1, train for the 138-th batch, train loss: 0.39337316155433655:  57%|█████▋    | 138/241 [00:29<00:20,  4.91it/s]evaluate for the 17-th batch, evaluate loss: 0.5053645968437195:  42%|███████▌          | 16/38 [00:01<00:02,  8.84it/s]Epoch: 2, train for the 15-th batch, train loss: 0.4745097756385803:  12%|█▌           | 14/119 [00:01<00:12,  8.75it/s]Epoch: 2, train for the 15-th batch, train loss: 0.4745097756385803:  13%|█▋           | 15/119 [00:01<00:12,  8.42it/s]evaluate for the 18-th batch, evaluate loss: 0.5184860825538635:  42%|███████▌          | 16/38 [00:01<00:02,  8.84it/s]evaluate for the 18-th batch, evaluate loss: 0.5184860825538635:  47%|████████▌         | 18/38 [00:01<00:01, 10.12it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5368092060089111:  54%|█████▉     | 127/237 [00:29<00:32,  3.38it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5368092060089111:  54%|█████▉     | 128/237 [00:29<00:29,  3.67it/s]Epoch: 1, train for the 112-th batch, train loss: 0.4246317446231842:  29%|███▏       | 111/383 [00:27<01:12,  3.77it/s]evaluate for the 19-th batch, evaluate loss: 0.48643678426742554:  47%|████████         | 18/38 [00:01<00:01, 10.12it/s]Epoch: 1, train for the 133-th batch, train loss: 0.5172570943832397:  87%|█████████▌ | 132/151 [00:30<00:05,  3.49it/s]Epoch: 1, train for the 112-th batch, train loss: 0.4246317446231842:  29%|███▏       | 112/383 [00:27<01:11,  3.77it/s]Epoch: 2, train for the 16-th batch, train loss: 0.5198491811752319:  13%|█▋           | 15/119 [00:02<00:12,  8.42it/s]Epoch: 1, train for the 133-th batch, train loss: 0.5172570943832397:  88%|█████████▋ | 133/151 [00:30<00:04,  3.85it/s]Epoch: 2, train for the 16-th batch, train loss: 0.5198491811752319:  13%|█▋           | 16/119 [00:02<00:11,  8.72it/s]Epoch: 2, train for the 17-th batch, train loss: 0.43761682510375977:  13%|█▌          | 16/119 [00:02<00:11,  8.72it/s]Epoch: 1, train for the 139-th batch, train loss: 0.35510265827178955:  57%|█████▋    | 138/241 [00:29<00:20,  4.91it/s]evaluate for the 20-th batch, evaluate loss: 0.4408365786075592:  47%|████████▌         | 18/38 [00:01<00:01, 10.12it/s]evaluate for the 20-th batch, evaluate loss: 0.4408365786075592:  53%|█████████▍        | 20/38 [00:01<00:01,  9.57it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5020995140075684:  88%|█████████▋ | 133/151 [00:30<00:04,  3.85it/s]Epoch: 1, train for the 139-th batch, train loss: 0.35510265827178955:  58%|█████▊    | 139/241 [00:29<00:24,  4.12it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5020995140075684:  89%|█████████▊ | 134/151 [00:30<00:04,  4.25it/s]Epoch: 2, train for the 18-th batch, train loss: 0.5145494937896729:  13%|█▋           | 16/119 [00:02<00:11,  8.72it/s]Epoch: 2, train for the 18-th batch, train loss: 0.5145494937896729:  15%|█▉           | 18/119 [00:02<00:10,  9.25it/s]evaluate for the 21-th batch, evaluate loss: 0.44806671142578125:  53%|████████▉        | 20/38 [00:01<00:01,  9.57it/s]Epoch: 1, train for the 113-th batch, train loss: 0.4281674921512604:  29%|███▏       | 112/383 [00:27<01:11,  3.77it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5572707653045654:  54%|█████▉     | 128/237 [00:29<00:29,  3.67it/s]evaluate for the 22-th batch, evaluate loss: 0.508347749710083:  53%|██████████         | 20/38 [00:01<00:01,  9.57it/s]evaluate for the 22-th batch, evaluate loss: 0.508347749710083:  58%|███████████        | 22/38 [00:01<00:01, 10.93it/s]Epoch: 1, train for the 140-th batch, train loss: 0.37465253472328186:  58%|█████▊    | 139/241 [00:29<00:24,  4.12it/s]Epoch: 1, train for the 140-th batch, train loss: 0.37465253472328186:  58%|█████▊    | 140/241 [00:29<00:20,  4.82it/s]Epoch: 1, train for the 113-th batch, train loss: 0.4281674921512604:  30%|███▏       | 113/383 [00:28<01:15,  3.56it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5572707653045654:  54%|█████▉     | 129/237 [00:29<00:32,  3.33it/s]Epoch: 1, train for the 135-th batch, train loss: 0.49167028069496155:  89%|████████▊ | 134/151 [00:30<00:04,  4.25it/s]Epoch: 1, train for the 135-th batch, train loss: 0.49167028069496155:  89%|████████▉ | 135/151 [00:30<00:03,  4.75it/s]evaluate for the 23-th batch, evaluate loss: 0.5052527785301208:  58%|██████████▍       | 22/38 [00:02<00:01, 10.93it/s]Epoch: 2, train for the 19-th batch, train loss: 0.5500885248184204:  15%|█▉           | 18/119 [00:02<00:10,  9.25it/s]Epoch: 2, train for the 19-th batch, train loss: 0.5500885248184204:  16%|██           | 19/119 [00:02<00:11,  8.56it/s]evaluate for the 24-th batch, evaluate loss: 0.46923017501831055:  58%|█████████▊       | 22/38 [00:02<00:01, 10.93it/s]evaluate for the 24-th batch, evaluate loss: 0.46923017501831055:  63%|██████████▋      | 24/38 [00:02<00:01, 11.88it/s]Epoch: 2, train for the 20-th batch, train loss: 0.44783759117126465:  16%|█▉          | 19/119 [00:02<00:11,  8.56it/s]Epoch: 2, train for the 20-th batch, train loss: 0.44783759117126465:  17%|██          | 20/119 [00:02<00:11,  8.33it/s]Epoch: 2, train for the 21-th batch, train loss: 0.5033110976219177:  17%|██▏          | 20/119 [00:02<00:11,  8.33it/s]Epoch: 1, train for the 114-th batch, train loss: 0.3919547200202942:  30%|███▏       | 113/383 [00:28<01:15,  3.56it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5404796600341797:  54%|█████▉     | 129/237 [00:30<00:32,  3.33it/s]evaluate for the 25-th batch, evaluate loss: 0.5199058055877686:  63%|███████████▎      | 24/38 [00:02<00:01, 11.88it/s]Epoch: 1, train for the 141-th batch, train loss: 0.4962709844112396:  58%|██████▍    | 140/241 [00:29<00:20,  4.82it/s]Epoch: 1, train for the 114-th batch, train loss: 0.3919547200202942:  30%|███▎       | 114/383 [00:28<01:16,  3.53it/s]Epoch: 1, train for the 141-th batch, train loss: 0.4962709844112396:  59%|██████▍    | 141/241 [00:29<00:23,  4.21it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5404796600341797:  55%|██████     | 130/237 [00:30<00:31,  3.36it/s]evaluate for the 26-th batch, evaluate loss: 0.47888895869255066:  63%|██████████▋      | 24/38 [00:02<00:01, 11.88it/s]evaluate for the 26-th batch, evaluate loss: 0.47888895869255066:  68%|███████████▋     | 26/38 [00:02<00:01, 10.54it/s]Epoch: 2, train for the 22-th batch, train loss: 0.5245932936668396:  17%|██▏          | 20/119 [00:02<00:11,  8.33it/s]Epoch: 2, train for the 22-th batch, train loss: 0.5245932936668396:  18%|██▍          | 22/119 [00:02<00:11,  8.65it/s]evaluate for the 27-th batch, evaluate loss: 0.5028800368309021:  68%|████████████▎     | 26/38 [00:02<00:01, 10.54it/s]Epoch: 1, train for the 136-th batch, train loss: 0.5487919449806213:  89%|█████████▊ | 135/151 [00:31<00:03,  4.75it/s]Epoch: 1, train for the 142-th batch, train loss: 0.45848333835601807:  59%|█████▊    | 141/241 [00:29<00:23,  4.21it/s]evaluate for the 28-th batch, evaluate loss: 0.5024950504302979:  68%|████████████▎     | 26/38 [00:02<00:01, 10.54it/s]evaluate for the 28-th batch, evaluate loss: 0.5024950504302979:  74%|█████████████▎    | 28/38 [00:02<00:00, 11.65it/s]Epoch: 1, train for the 131-th batch, train loss: 0.51349937915802:  55%|███████▏     | 130/237 [00:30<00:31,  3.36it/s]Epoch: 1, train for the 136-th batch, train loss: 0.5487919449806213:  90%|█████████▉ | 136/151 [00:31<00:04,  3.58it/s]Epoch: 1, train for the 142-th batch, train loss: 0.45848333835601807:  59%|█████▉    | 142/241 [00:30<00:22,  4.47it/s]Epoch: 1, train for the 131-th batch, train loss: 0.51349937915802:  55%|███████▏     | 131/237 [00:30<00:28,  3.70it/s]Epoch: 2, train for the 23-th batch, train loss: 0.5494613647460938:  18%|██▍          | 22/119 [00:02<00:11,  8.65it/s]Epoch: 2, train for the 23-th batch, train loss: 0.5494613647460938:  19%|██▌          | 23/119 [00:02<00:11,  8.51it/s]evaluate for the 29-th batch, evaluate loss: 0.48382309079170227:  74%|████████████▌    | 28/38 [00:02<00:00, 11.65it/s]evaluate for the 30-th batch, evaluate loss: 0.5059493184089661:  74%|█████████████▎    | 28/38 [00:02<00:00, 11.65it/s]evaluate for the 30-th batch, evaluate loss: 0.5059493184089661:  79%|██████████████▏   | 30/38 [00:02<00:00, 12.73it/s]Epoch: 1, train for the 115-th batch, train loss: 0.3554070293903351:  30%|███▎       | 114/383 [00:28<01:16,  3.53it/s]Epoch: 1, train for the 137-th batch, train loss: 0.5392698049545288:  90%|█████████▉ | 136/151 [00:31<00:04,  3.58it/s]Epoch: 2, train for the 24-th batch, train loss: 0.4792023301124573:  19%|██▌          | 23/119 [00:03<00:11,  8.51it/s]Epoch: 1, train for the 115-th batch, train loss: 0.3554070293903351:  30%|███▎       | 115/383 [00:28<01:21,  3.30it/s]Epoch: 2, train for the 24-th batch, train loss: 0.4792023301124573:  20%|██▌          | 24/119 [00:03<00:11,  8.40it/s]Epoch: 1, train for the 137-th batch, train loss: 0.5392698049545288:  91%|█████████▉ | 137/151 [00:31<00:03,  3.99it/s]Epoch: 1, train for the 132-th batch, train loss: 0.563211977481842:  55%|██████▋     | 131/237 [00:30<00:28,  3.70it/s]Epoch: 1, train for the 132-th batch, train loss: 0.563211977481842:  56%|██████▋     | 132/237 [00:30<00:26,  4.03it/s]Epoch: 2, train for the 25-th batch, train loss: 0.4623056650161743:  20%|██▌          | 24/119 [00:03<00:11,  8.40it/s]Epoch: 1, train for the 143-th batch, train loss: 0.4588488042354584:  59%|██████▍    | 142/241 [00:30<00:22,  4.47it/s]Epoch: 1, train for the 143-th batch, train loss: 0.4588488042354584:  59%|██████▌    | 143/241 [00:30<00:24,  4.08it/s]Epoch: 2, train for the 26-th batch, train loss: 0.4669523239135742:  20%|██▌          | 24/119 [00:03<00:11,  8.40it/s]evaluate for the 31-th batch, evaluate loss: 0.5028449892997742:  79%|██████████████▏   | 30/38 [00:02<00:00, 12.73it/s]Epoch: 2, train for the 26-th batch, train loss: 0.4669523239135742:  22%|██▊          | 26/119 [00:03<00:10,  9.06it/s]Epoch: 1, train for the 133-th batch, train loss: 0.5132474899291992:  56%|██████▏    | 132/237 [00:30<00:26,  4.03it/s]Epoch: 1, train for the 138-th batch, train loss: 0.5292426943778992:  91%|█████████▉ | 137/151 [00:31<00:03,  3.99it/s]Epoch: 1, train for the 116-th batch, train loss: 0.35152286291122437:  30%|███       | 115/383 [00:28<01:21,  3.30it/s]Epoch: 1, train for the 138-th batch, train loss: 0.5292426943778992:  91%|██████████ | 138/151 [00:31<00:03,  4.10it/s]Epoch: 1, train for the 133-th batch, train loss: 0.5132474899291992:  56%|██████▏    | 133/237 [00:30<00:23,  4.34it/s]evaluate for the 32-th batch, evaluate loss: 0.4867396950721741:  79%|██████████████▏   | 30/38 [00:02<00:00, 12.73it/s]evaluate for the 32-th batch, evaluate loss: 0.4867396950721741:  84%|███████████████▏  | 32/38 [00:02<00:00, 10.00it/s]Epoch: 1, train for the 144-th batch, train loss: 0.5888596773147583:  59%|██████▌    | 143/241 [00:30<00:24,  4.08it/s]Epoch: 1, train for the 116-th batch, train loss: 0.35152286291122437:  30%|███       | 116/383 [00:28<01:18,  3.41it/s]Epoch: 1, train for the 144-th batch, train loss: 0.5888596773147583:  60%|██████▌    | 144/241 [00:30<00:20,  4.68it/s]Epoch: 2, train for the 27-th batch, train loss: 0.5676084756851196:  22%|██▊          | 26/119 [00:03<00:10,  9.06it/s]evaluate for the 33-th batch, evaluate loss: 0.4842696189880371:  84%|███████████████▏  | 32/38 [00:03<00:00, 10.00it/s]Epoch: 2, train for the 27-th batch, train loss: 0.5676084756851196:  23%|██▉          | 27/119 [00:03<00:11,  8.33it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5138490796089172:  91%|██████████ | 138/151 [00:31<00:03,  4.10it/s]evaluate for the 34-th batch, evaluate loss: 0.49846792221069336:  84%|██████████████▎  | 32/38 [00:03<00:00, 10.00it/s]evaluate for the 34-th batch, evaluate loss: 0.49846792221069336:  89%|███████████████▏ | 34/38 [00:03<00:00, 10.62it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5138490796089172:  92%|██████████▏| 139/151 [00:31<00:02,  4.48it/s]Epoch: 2, train for the 28-th batch, train loss: 0.5518131852149963:  23%|██▉          | 27/119 [00:03<00:11,  8.33it/s]Epoch: 2, train for the 28-th batch, train loss: 0.5518131852149963:  24%|███          | 28/119 [00:03<00:10,  8.44it/s]Epoch: 2, train for the 29-th batch, train loss: 0.5034507513046265:  24%|███          | 28/119 [00:03<00:10,  8.44it/s]Epoch: 1, train for the 117-th batch, train loss: 0.3302624523639679:  30%|███▎       | 116/383 [00:29<01:18,  3.41it/s]Epoch: 1, train for the 117-th batch, train loss: 0.3302624523639679:  31%|███▎       | 117/383 [00:29<01:22,  3.23it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5686830282211304:  56%|██████▏    | 133/237 [00:31<00:23,  4.34it/s]Epoch: 1, train for the 140-th batch, train loss: 0.47064945101737976:  92%|█████████▏| 139/151 [00:32<00:02,  4.48it/s]Epoch: 1, train for the 145-th batch, train loss: 0.6293120980262756:  60%|██████▌    | 144/241 [00:30<00:20,  4.68it/s]Epoch: 2, train for the 30-th batch, train loss: 0.528130054473877:  24%|███▎          | 28/119 [00:03<00:10,  8.44it/s]evaluate for the 35-th batch, evaluate loss: 0.5315691232681274:  89%|████████████████  | 34/38 [00:03<00:00, 10.62it/s]Epoch: 2, train for the 30-th batch, train loss: 0.528130054473877:  25%|███▌          | 30/119 [00:03<00:10,  8.33it/s]Epoch: 1, train for the 140-th batch, train loss: 0.47064945101737976:  93%|█████████▎| 140/151 [00:32<00:02,  4.05it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5686830282211304:  57%|██████▏    | 134/237 [00:31<00:31,  3.27it/s]Epoch: 1, train for the 145-th batch, train loss: 0.6293120980262756:  60%|██████▌    | 145/241 [00:30<00:27,  3.46it/s]evaluate for the 36-th batch, evaluate loss: 0.5328254699707031:  89%|████████████████  | 34/38 [00:03<00:00, 10.62it/s]evaluate for the 36-th batch, evaluate loss: 0.5328254699707031:  95%|█████████████████ | 36/38 [00:03<00:00,  8.20it/s]Epoch: 1, train for the 118-th batch, train loss: 0.32719793915748596:  31%|███       | 117/383 [00:29<01:22,  3.23it/s]Epoch: 2, train for the 31-th batch, train loss: 0.5093749761581421:  25%|███▎         | 30/119 [00:03<00:10,  8.33it/s]Epoch: 2, train for the 31-th batch, train loss: 0.5093749761581421:  26%|███▍         | 31/119 [00:03<00:10,  8.29it/s]Epoch: 1, train for the 118-th batch, train loss: 0.32719793915748596:  31%|███       | 118/383 [00:29<01:15,  3.51it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5610001087188721:  60%|██████▌    | 145/241 [00:31<00:27,  3.46it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5610001087188721:  61%|██████▋    | 146/241 [00:31<00:23,  3.98it/s]Epoch: 2, train for the 32-th batch, train loss: 0.43710288405418396:  26%|███▏        | 31/119 [00:03<00:10,  8.29it/s]Epoch: 2, train for the 33-th batch, train loss: 0.5292007327079773:  26%|███▍         | 31/119 [00:04<00:10,  8.29it/s]Epoch: 2, train for the 33-th batch, train loss: 0.5292007327079773:  28%|███▌         | 33/119 [00:04<00:09,  9.35it/s]Epoch: 1, train for the 141-th batch, train loss: 0.49475064873695374:  93%|█████████▎| 140/151 [00:32<00:02,  4.05it/s]evaluate for the 37-th batch, evaluate loss: 0.4603431522846222:  95%|█████████████████ | 36/38 [00:03<00:00,  8.20it/s]evaluate for the 37-th batch, evaluate loss: 0.4603431522846222:  97%|█████████████████▌| 37/38 [00:03<00:00,  6.99it/s]Epoch: 1, train for the 135-th batch, train loss: 0.5992160439491272:  57%|██████▏    | 134/237 [00:31<00:31,  3.27it/s]Epoch: 1, train for the 141-th batch, train loss: 0.49475064873695374:  93%|█████████▎| 141/151 [00:32<00:02,  3.70it/s]evaluate for the 38-th batch, evaluate loss: 0.4990321099758148:  97%|█████████████████▌| 37/38 [00:03<00:00,  6.99it/s]evaluate for the 38-th batch, evaluate loss: 0.4990321099758148: 100%|██████████████████| 38/38 [00:03<00:00, 10.13it/s]
Epoch: 1, train for the 135-th batch, train loss: 0.5992160439491272:  57%|██████▎    | 135/237 [00:31<00:32,  3.11it/s]Epoch: 1, train for the 119-th batch, train loss: 0.381992369890213:  31%|███▋        | 118/383 [00:29<01:15,  3.51it/s]Epoch: 1, train for the 147-th batch, train loss: 0.557532787322998:  61%|███████▎    | 146/241 [00:31<00:23,  3.98it/s]Epoch: 1, train for the 147-th batch, train loss: 0.557532787322998:  61%|███████▎    | 147/241 [00:31<00:23,  3.98it/s]Epoch: 1, train for the 119-th batch, train loss: 0.381992369890213:  31%|███▋        | 119/383 [00:29<01:17,  3.39it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5014763474464417:  28%|███▌         | 33/119 [00:04<00:09,  9.35it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5014763474464417:  29%|███▋         | 34/119 [00:04<00:10,  8.36it/s]Epoch: 1, train for the 142-th batch, train loss: 0.4949652850627899:  93%|██████████▎| 141/151 [00:32<00:02,  3.70it/s]Epoch: 1, train for the 142-th batch, train loss: 0.4949652850627899:  94%|██████████▎| 142/151 [00:32<00:02,  4.14it/s]Epoch: 1, train for the 136-th batch, train loss: 0.599542498588562:  57%|██████▊     | 135/237 [00:31<00:32,  3.11it/s]Epoch: 2, train for the 35-th batch, train loss: 0.4734361171722412:  29%|███▋         | 34/119 [00:04<00:10,  8.36it/s]Epoch: 1, train for the 136-th batch, train loss: 0.599542498588562:  57%|██████▉     | 136/237 [00:31<00:28,  3.58it/s]Epoch: 1, train for the 148-th batch, train loss: 0.5100642442703247:  61%|██████▋    | 147/241 [00:31<00:23,  3.98it/s]Epoch: 1, train for the 148-th batch, train loss: 0.5100642442703247:  61%|██████▊    | 148/241 [00:31<00:20,  4.58it/s]Epoch: 1, train for the 120-th batch, train loss: 0.41779136657714844:  31%|███       | 119/383 [00:29<01:17,  3.39it/s]  0%|                                                                                            | 0/20 [00:00<?, ?it/s]Epoch: 1, train for the 120-th batch, train loss: 0.41779136657714844:  31%|███▏      | 120/383 [00:30<01:10,  3.73it/s]Epoch: 1, train for the 149-th batch, train loss: 0.3807913362979889:  61%|██████▊    | 148/241 [00:31<00:20,  4.58it/s]Epoch: 2, train for the 36-th batch, train loss: 0.5214536786079407:  29%|███▋         | 34/119 [00:04<00:10,  8.36it/s]Epoch: 1, train for the 149-th batch, train loss: 0.3807913362979889:  62%|██████▊    | 149/241 [00:31<00:17,  5.37it/s]Epoch: 2, train for the 36-th batch, train loss: 0.5214536786079407:  30%|███▉         | 36/119 [00:04<00:09,  8.35it/s]Epoch: 1, train for the 137-th batch, train loss: 0.5652818083763123:  57%|██████▎    | 136/237 [00:31<00:28,  3.58it/s]evaluate for the 1-th batch, evaluate loss: 0.7204946875572205:   0%|                            | 0/20 [00:00<?, ?it/s]Epoch: 1, train for the 137-th batch, train loss: 0.5652818083763123:  58%|██████▎    | 137/237 [00:32<00:24,  4.03it/s]evaluate for the 2-th batch, evaluate loss: 0.7608678340911865:   0%|                            | 0/20 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.7608678340911865:  10%|██                  | 2/20 [00:00<00:01, 13.47it/s]Epoch: 1, train for the 143-th batch, train loss: 0.4429550766944885:  94%|██████████▎| 142/151 [00:33<00:02,  4.14it/s]Epoch: 2, train for the 37-th batch, train loss: 0.5048972964286804:  30%|███▉         | 36/119 [00:04<00:09,  8.35it/s]Epoch: 2, train for the 37-th batch, train loss: 0.5048972964286804:  31%|████         | 37/119 [00:04<00:10,  8.06it/s]Epoch: 1, train for the 143-th batch, train loss: 0.4429550766944885:  95%|██████████▍| 143/151 [00:33<00:02,  3.73it/s]evaluate for the 3-th batch, evaluate loss: 0.6634619235992432:  10%|██                  | 2/20 [00:00<00:01, 13.47it/s]Epoch: 1, train for the 121-th batch, train loss: 0.43217992782592773:  31%|███▏      | 120/383 [00:30<01:10,  3.73it/s]Epoch: 1, train for the 150-th batch, train loss: 0.38003119826316833:  62%|██████▏   | 149/241 [00:31<00:17,  5.37it/s]evaluate for the 4-th batch, evaluate loss: 0.6826355457305908:  10%|██                  | 2/20 [00:00<00:01, 13.47it/s]evaluate for the 4-th batch, evaluate loss: 0.6826355457305908:  20%|████                | 4/20 [00:00<00:01, 14.72it/s]Epoch: 1, train for the 121-th batch, train loss: 0.43217992782592773:  32%|███▏      | 121/383 [00:30<01:09,  3.76it/s]Epoch: 1, train for the 138-th batch, train loss: 0.6011638641357422:  58%|██████▎    | 137/237 [00:32<00:24,  4.03it/s]Epoch: 1, train for the 150-th batch, train loss: 0.38003119826316833:  62%|██████▏   | 150/241 [00:31<00:18,  5.00it/s]Epoch: 1, train for the 138-th batch, train loss: 0.6011638641357422:  58%|██████▍    | 138/237 [00:32<00:23,  4.15it/s]Epoch: 2, train for the 38-th batch, train loss: 0.4999670386314392:  31%|████         | 37/119 [00:04<00:10,  8.06it/s]Epoch: 1, train for the 144-th batch, train loss: 0.4637793302536011:  95%|██████████▍| 143/151 [00:33<00:02,  3.73it/s]Epoch: 2, train for the 38-th batch, train loss: 0.4999670386314392:  32%|████▏        | 38/119 [00:04<00:10,  8.03it/s]evaluate for the 5-th batch, evaluate loss: 0.6897907853126526:  20%|████                | 4/20 [00:00<00:01, 14.72it/s]Epoch: 1, train for the 144-th batch, train loss: 0.4637793302536011:  95%|██████████▍| 144/151 [00:33<00:01,  4.32it/s]evaluate for the 6-th batch, evaluate loss: 0.7120781540870667:  20%|████                | 4/20 [00:00<00:01, 14.72it/s]evaluate for the 6-th batch, evaluate loss: 0.7120781540870667:  30%|██████              | 6/20 [00:00<00:00, 14.11it/s]Epoch: 2, train for the 39-th batch, train loss: 0.5366382002830505:  32%|████▏        | 38/119 [00:04<00:10,  8.03it/s]Epoch: 1, train for the 151-th batch, train loss: 0.38788923621177673:  62%|██████▏   | 150/241 [00:32<00:18,  5.00it/s]Epoch: 2, train for the 39-th batch, train loss: 0.5366382002830505:  33%|████▎        | 39/119 [00:04<00:10,  7.29it/s]evaluate for the 7-th batch, evaluate loss: 0.7532599568367004:  30%|██████              | 6/20 [00:00<00:00, 14.11it/s]Epoch: 1, train for the 151-th batch, train loss: 0.38788923621177673:  63%|██████▎   | 151/241 [00:32<00:18,  4.79it/s]evaluate for the 8-th batch, evaluate loss: 0.692454993724823:  30%|██████▎              | 6/20 [00:00<00:00, 14.11it/s]evaluate for the 8-th batch, evaluate loss: 0.692454993724823:  40%|████████▍            | 8/20 [00:00<00:00, 13.98it/s]Epoch: 2, train for the 40-th batch, train loss: 0.5047042369842529:  33%|████▎        | 39/119 [00:04<00:10,  7.29it/s]Epoch: 2, train for the 40-th batch, train loss: 0.5047042369842529:  34%|████▎        | 40/119 [00:04<00:10,  7.36it/s]evaluate for the 9-th batch, evaluate loss: 0.6365994215011597:  40%|████████            | 8/20 [00:00<00:00, 13.98it/s]Epoch: 1, train for the 145-th batch, train loss: 0.4968518614768982:  95%|██████████▍| 144/151 [00:33<00:01,  4.32it/s]Epoch: 1, train for the 145-th batch, train loss: 0.4968518614768982:  96%|██████████▌| 145/151 [00:33<00:01,  3.87it/s]evaluate for the 10-th batch, evaluate loss: 0.656836986541748:  40%|████████            | 8/20 [00:00<00:00, 13.98it/s]evaluate for the 10-th batch, evaluate loss: 0.656836986541748:  50%|█████████▌         | 10/20 [00:00<00:00, 14.51it/s]Epoch: 1, train for the 122-th batch, train loss: 0.4998129904270172:  32%|███▍       | 121/383 [00:30<01:09,  3.76it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5813487768173218:  58%|██████▍    | 138/237 [00:32<00:23,  4.15it/s]Epoch: 1, train for the 122-th batch, train loss: 0.4998129904270172:  32%|███▌       | 122/383 [00:30<01:24,  3.08it/s]Epoch: 1, train for the 152-th batch, train loss: 0.31361234188079834:  63%|██████▎   | 151/241 [00:32<00:18,  4.79it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5813487768173218:  59%|██████▍    | 139/237 [00:32<00:29,  3.36it/s]Epoch: 2, train for the 41-th batch, train loss: 0.5051063299179077:  34%|████▎        | 40/119 [00:05<00:10,  7.36it/s]Epoch: 2, train for the 41-th batch, train loss: 0.5051063299179077:  34%|████▍        | 41/119 [00:05<00:10,  7.57it/s]evaluate for the 11-th batch, evaluate loss: 0.6142393350601196:  50%|█████████         | 10/20 [00:00<00:00, 14.51it/s]Epoch: 1, train for the 152-th batch, train loss: 0.31361234188079834:  63%|██████▎   | 152/241 [00:32<00:19,  4.57it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5075656771659851:  96%|██████████▌| 145/151 [00:33<00:01,  3.87it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5075656771659851:  97%|██████████▋| 146/151 [00:33<00:01,  4.35it/s]evaluate for the 12-th batch, evaluate loss: 0.6732004880905151:  50%|█████████         | 10/20 [00:00<00:00, 14.51it/s]evaluate for the 12-th batch, evaluate loss: 0.6732004880905151:  60%|██████████▊       | 12/20 [00:00<00:00, 14.58it/s]Epoch: 1, train for the 153-th batch, train loss: 0.36292633414268494:  63%|██████▎   | 152/241 [00:32<00:19,  4.57it/s]Epoch: 1, train for the 123-th batch, train loss: 0.41002753376960754:  32%|███▏      | 122/383 [00:30<01:24,  3.08it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5310019850730896:  34%|████▍        | 41/119 [00:05<00:10,  7.57it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5310019850730896:  35%|████▌        | 42/119 [00:05<00:11,  6.82it/s]evaluate for the 13-th batch, evaluate loss: 0.6961327791213989:  60%|██████████▊       | 12/20 [00:00<00:00, 14.58it/s]Epoch: 1, train for the 153-th batch, train loss: 0.36292633414268494:  63%|██████▎   | 153/241 [00:32<00:18,  4.85it/s]Epoch: 1, train for the 123-th batch, train loss: 0.41002753376960754:  32%|███▏      | 123/383 [00:30<01:16,  3.40it/s]Epoch: 1, train for the 147-th batch, train loss: 0.49810275435447693:  97%|█████████▋| 146/151 [00:33<00:01,  4.35it/s]Epoch: 1, train for the 147-th batch, train loss: 0.49810275435447693:  97%|█████████▋| 147/151 [00:33<00:00,  4.63it/s]evaluate for the 14-th batch, evaluate loss: 0.707684338092804:  60%|███████████▍       | 12/20 [00:01<00:00, 14.58it/s]evaluate for the 14-th batch, evaluate loss: 0.707684338092804:  70%|█████████████▎     | 14/20 [00:01<00:00, 12.76it/s]Epoch: 1, train for the 140-th batch, train loss: 0.5538479685783386:  59%|██████▍    | 139/237 [00:33<00:29,  3.36it/s]Epoch: 1, train for the 140-th batch, train loss: 0.5538479685783386:  59%|██████▍    | 140/237 [00:33<00:30,  3.15it/s]Epoch: 2, train for the 43-th batch, train loss: 0.5341609716415405:  35%|████▌        | 42/119 [00:05<00:11,  6.82it/s]Epoch: 2, train for the 43-th batch, train loss: 0.5341609716415405:  36%|████▋        | 43/119 [00:05<00:11,  6.44it/s]evaluate for the 15-th batch, evaluate loss: 0.688606858253479:  70%|█████████████▎     | 14/20 [00:01<00:00, 12.76it/s]evaluate for the 16-th batch, evaluate loss: 0.6563220620155334:  70%|████████████▌     | 14/20 [00:01<00:00, 12.76it/s]evaluate for the 16-th batch, evaluate loss: 0.6563220620155334:  80%|██████████████▍   | 16/20 [00:01<00:00, 12.25it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5075398683547974:  36%|████▋        | 43/119 [00:05<00:11,  6.44it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5075398683547974:  37%|████▊        | 44/119 [00:05<00:10,  6.86it/s]Epoch: 1, train for the 124-th batch, train loss: 0.46126577258110046:  32%|███▏      | 123/383 [00:31<01:16,  3.40it/s]Epoch: 1, train for the 154-th batch, train loss: 0.2757873237133026:  63%|██████▉    | 153/241 [00:32<00:18,  4.85it/s]evaluate for the 17-th batch, evaluate loss: 0.6917728781700134:  80%|██████████████▍   | 16/20 [00:01<00:00, 12.25it/s]Epoch: 1, train for the 154-th batch, train loss: 0.2757873237133026:  64%|███████    | 154/241 [00:32<00:21,  4.06it/s]Epoch: 1, train for the 124-th batch, train loss: 0.46126577258110046:  32%|███▏      | 124/383 [00:31<01:18,  3.30it/s]Epoch: 1, train for the 148-th batch, train loss: 0.47104963660240173:  97%|█████████▋| 147/151 [00:34<00:00,  4.63it/s]Epoch: 1, train for the 141-th batch, train loss: 0.5946485996246338:  59%|██████▍    | 140/237 [00:33<00:30,  3.15it/s]Epoch: 1, train for the 148-th batch, train loss: 0.47104963660240173:  98%|█████████▊| 148/151 [00:34<00:00,  4.14it/s]Epoch: 1, train for the 141-th batch, train loss: 0.5946485996246338:  59%|██████▌    | 141/237 [00:33<00:27,  3.49it/s]Epoch: 2, train for the 45-th batch, train loss: 0.5293031930923462:  37%|████▊        | 44/119 [00:05<00:10,  6.86it/s]Epoch: 2, train for the 45-th batch, train loss: 0.5293031930923462:  38%|████▉        | 45/119 [00:05<00:10,  7.15it/s]evaluate for the 18-th batch, evaluate loss: 0.6811991930007935:  80%|██████████████▍   | 16/20 [00:01<00:00, 12.25it/s]evaluate for the 18-th batch, evaluate loss: 0.6811991930007935:  90%|████████████████▏ | 18/20 [00:01<00:00, 12.34it/s]Epoch: 1, train for the 155-th batch, train loss: 0.29781538248062134:  64%|██████▍   | 154/241 [00:32<00:21,  4.06it/s]Epoch: 1, train for the 155-th batch, train loss: 0.29781538248062134:  64%|██████▍   | 155/241 [00:32<00:18,  4.53it/s]evaluate for the 19-th batch, evaluate loss: 0.7252517342567444:  90%|████████████████▏ | 18/20 [00:01<00:00, 12.34it/s]Epoch: 1, train for the 125-th batch, train loss: 0.4230802059173584:  32%|███▌       | 124/383 [00:31<01:18,  3.30it/s]Epoch: 1, train for the 125-th batch, train loss: 0.4230802059173584:  33%|███▌       | 125/383 [00:31<01:12,  3.56it/s]Epoch: 2, train for the 46-th batch, train loss: 0.5140731930732727:  38%|████▉        | 45/119 [00:05<00:10,  7.15it/s]evaluate for the 20-th batch, evaluate loss: 0.7017879486083984:  90%|████████████████▏ | 18/20 [00:01<00:00, 12.34it/s]evaluate for the 20-th batch, evaluate loss: 0.7017879486083984: 100%|██████████████████| 20/20 [00:01<00:00, 12.47it/s]evaluate for the 20-th batch, evaluate loss: 0.7017879486083984: 100%|██████████████████| 20/20 [00:01<00:00, 13.12it/s]
Epoch: 2, train for the 46-th batch, train loss: 0.5140731930732727:  39%|█████        | 46/119 [00:05<00:10,  6.76it/s]Epoch: 1, train for the 142-th batch, train loss: 0.597907304763794:  59%|███████▏    | 141/237 [00:33<00:27,  3.49it/s]INFO:root:Epoch: 1, learning rate: 0.0001, train loss: 0.6099
INFO:root:train average_precision, 0.7527
INFO:root:train roc_auc, 0.7485
INFO:root:validate loss: 0.5004
INFO:root:validate average_precision, 0.8423
INFO:root:validate roc_auc, 0.8405
INFO:root:new node validate loss: 0.6902
INFO:root:new node validate first_1_average_precision, 0.5810
INFO:root:new node validate first_1_roc_auc, 0.5085
INFO:root:new node validate first_3_average_precision, 0.5955
INFO:root:new node validate first_3_roc_auc, 0.5565
INFO:root:new node validate first_10_average_precision, 0.6061
INFO:root:new node validate first_10_roc_auc, 0.6000
INFO:root:new node validate average_precision, 0.6525
INFO:root:new node validate roc_auc, 0.6558
INFO:root:save model ./saved_models/TGN/ia-escorts-dynamic/TGN_seed0_dummy-55wl6je1/TGN_seed0_dummy-55wl6je1.pkl
Epoch: 2, train for the 47-th batch, train loss: 0.5603384971618652:  39%|█████        | 46/119 [00:06<00:10,  6.76it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5603384971618652:  39%|█████▏       | 47/119 [00:06<00:10,  6.63it/s]Epoch: 1, train for the 142-th batch, train loss: 0.597907304763794:  60%|███████▏    | 142/237 [00:33<00:29,  3.20it/s]Epoch: 1, train for the 126-th batch, train loss: 0.3664429783821106:  33%|███▌       | 125/383 [00:31<01:12,  3.56it/s]Epoch: 1, train for the 126-th batch, train loss: 0.3664429783821106:  33%|███▌       | 126/383 [00:31<01:06,  3.85it/s]Epoch: 1, train for the 149-th batch, train loss: 0.4409342110157013:  98%|██████████▊| 148/151 [00:34<00:00,  4.14it/s]Epoch: 1, train for the 156-th batch, train loss: 0.21976043283939362:  64%|██████▍   | 155/241 [00:33<00:18,  4.53it/s]Epoch: 1, train for the 156-th batch, train loss: 0.21976043283939362:  65%|██████▍   | 156/241 [00:33<00:21,  3.92it/s]Epoch: 1, train for the 149-th batch, train loss: 0.4409342110157013:  99%|██████████▊| 149/151 [00:34<00:00,  3.25it/s]Epoch: 2, train for the 48-th batch, train loss: 0.549763560295105:  39%|█████▌        | 47/119 [00:06<00:10,  6.63it/s]Epoch: 1, train for the 143-th batch, train loss: 0.5411774516105652:  60%|██████▌    | 142/237 [00:33<00:29,  3.20it/s]Epoch: 1, train for the 143-th batch, train loss: 0.5411774516105652:  60%|██████▋    | 143/237 [00:33<00:25,  3.64it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5143571496009827:  39%|█████▏       | 47/119 [00:06<00:10,  6.63it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5143571496009827:  41%|█████▎       | 49/119 [00:06<00:09,  7.71it/s]Epoch: 1, train for the 127-th batch, train loss: 0.3635709285736084:  33%|███▌       | 126/383 [00:31<01:06,  3.85it/s]Epoch: 1, train for the 157-th batch, train loss: 0.19115354120731354:  65%|██████▍   | 156/241 [00:33<00:21,  3.92it/s]Epoch: 1, train for the 157-th batch, train loss: 0.19115354120731354:  65%|██████▌   | 157/241 [00:33<00:19,  4.41it/s]Epoch: 1, train for the 127-th batch, train loss: 0.3635709285736084:  33%|███▋       | 127/383 [00:31<01:03,  4.04it/s]Epoch: 2, train for the 50-th batch, train loss: 0.4920198321342468:  41%|█████▎       | 49/119 [00:06<00:09,  7.71it/s]Epoch: 1, train for the 144-th batch, train loss: 0.5597597360610962:  60%|██████▋    | 143/237 [00:33<00:25,  3.64it/s]Epoch: 1, train for the 144-th batch, train loss: 0.5597597360610962:  61%|██████▋    | 144/237 [00:33<00:22,  4.07it/s]Epoch: 1, train for the 150-th batch, train loss: 0.5312080383300781:  99%|██████████▊| 149/151 [00:34<00:00,  3.25it/s]Epoch: 2, train for the 51-th batch, train loss: 0.5690444707870483:  41%|█████▎       | 49/119 [00:06<00:09,  7.71it/s]Epoch: 2, train for the 51-th batch, train loss: 0.5690444707870483:  43%|█████▌       | 51/119 [00:06<00:07,  8.61it/s]Epoch: 1, train for the 150-th batch, train loss: 0.5312080383300781:  99%|██████████▉| 150/151 [00:34<00:00,  3.23it/s]Epoch: 1, train for the 158-th batch, train loss: 0.20901130139827728:  65%|██████▌   | 157/241 [00:33<00:19,  4.41it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5227372646331787:  33%|███▋       | 127/383 [00:32<01:03,  4.04it/s]Epoch: 2, train for the 52-th batch, train loss: 0.55396568775177:  43%|██████▍        | 51/119 [00:06<00:07,  8.61it/s]Epoch: 1, train for the 158-th batch, train loss: 0.20901130139827728:  66%|██████▌   | 158/241 [00:33<00:19,  4.34it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5227372646331787:  33%|███▋       | 128/383 [00:32<01:03,  3.99it/s]Epoch: 1, train for the 145-th batch, train loss: 0.5678267478942871:  61%|██████▋    | 144/237 [00:34<00:22,  4.07it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5263885259628296:  99%|██████████▉| 150/151 [00:35<00:00,  3.23it/s]Epoch: 1, train for the 145-th batch, train loss: 0.5678267478942871:  61%|██████▋    | 145/237 [00:34<00:20,  4.40it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5263885259628296: 100%|███████████| 151/151 [00:35<00:00,  3.76it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5263885259628296: 100%|███████████| 151/151 [00:35<00:00,  4.29it/s]
Epoch: 2, train for the 53-th batch, train loss: 0.4906877875328064:  43%|█████▌       | 51/119 [00:06<00:07,  8.61it/s]Epoch: 2, train for the 53-th batch, train loss: 0.4906877875328064:  45%|█████▊       | 53/119 [00:06<00:07,  9.24it/s]Epoch: 2, train for the 54-th batch, train loss: 0.5342762470245361:  45%|█████▊       | 53/119 [00:06<00:07,  9.24it/s]  0%|                                                                                            | 0/46 [00:00<?, ?it/s]Epoch: 1, train for the 129-th batch, train loss: 0.383878231048584:  33%|████        | 128/383 [00:32<01:03,  3.99it/s]Epoch: 1, train for the 159-th batch, train loss: 0.12286126613616943:  66%|██████▌   | 158/241 [00:33<00:19,  4.34it/s]Epoch: 1, train for the 159-th batch, train loss: 0.12286126613616943:  66%|██████▌   | 159/241 [00:33<00:19,  4.14it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5731651782989502:  61%|██████▋    | 145/237 [00:34<00:20,  4.40it/s]Epoch: 1, train for the 129-th batch, train loss: 0.383878231048584:  34%|████        | 129/383 [00:32<01:03,  3.99it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5007867813110352:  45%|█████▊       | 53/119 [00:06<00:07,  9.24it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5007867813110352:  46%|██████       | 55/119 [00:06<00:06,  9.66it/s]evaluate for the 1-th batch, evaluate loss: 0.5525270104408264:   0%|                            | 0/46 [00:00<?, ?it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5731651782989502:  62%|██████▊    | 146/237 [00:34<00:20,  4.41it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5419986844062805:   0%|                            | 0/46 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5419986844062805:   4%|▊                   | 2/46 [00:00<00:03, 11.82it/s]Epoch: 2, train for the 1-th batch, train loss: 0.9383929967880249:   0%|                       | 0/146 [00:00<?, ?it/s]Epoch: 2, train for the 1-th batch, train loss: 0.9383929967880249:   1%|               | 1/146 [00:00<00:19,  7.37it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5339435338973999:  46%|██████       | 55/119 [00:07<00:06,  9.66it/s]Epoch: 1, train for the 160-th batch, train loss: 0.6486974954605103:  66%|███████▎   | 159/241 [00:34<00:19,  4.14it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5339435338973999:  47%|██████       | 56/119 [00:07<00:07,  8.22it/s]evaluate for the 3-th batch, evaluate loss: 0.5298249125480652:   4%|▊                   | 2/46 [00:00<00:03, 11.82it/s]Epoch: 1, train for the 160-th batch, train loss: 0.6486974954605103:  66%|███████▎   | 160/241 [00:34<00:19,  4.17it/s]Epoch: 2, train for the 2-th batch, train loss: 0.8632838726043701:   1%|               | 1/146 [00:00<00:19,  7.37it/s]Epoch: 2, train for the 2-th batch, train loss: 0.8632838726043701:   1%|▏              | 2/146 [00:00<00:16,  8.63it/s]Epoch: 1, train for the 130-th batch, train loss: 0.3864068388938904:  34%|███▋       | 129/383 [00:32<01:03,  3.99it/s]Epoch: 1, train for the 147-th batch, train loss: 0.5833940505981445:  62%|██████▊    | 146/237 [00:34<00:20,  4.41it/s]evaluate for the 4-th batch, evaluate loss: 0.529204249382019:   4%|▉                    | 2/46 [00:00<00:03, 11.82it/s]evaluate for the 4-th batch, evaluate loss: 0.529204249382019:   9%|█▊                   | 4/46 [00:00<00:03, 11.68it/s]Epoch: 1, train for the 130-th batch, train loss: 0.3864068388938904:  34%|███▋       | 130/383 [00:32<01:08,  3.69it/s]Epoch: 1, train for the 147-th batch, train loss: 0.5833940505981445:  62%|██████▊    | 147/237 [00:34<00:22,  4.01it/s]Epoch: 1, train for the 161-th batch, train loss: 0.3511649966239929:  66%|███████▎   | 160/241 [00:34<00:19,  4.17it/s]Epoch: 2, train for the 57-th batch, train loss: 0.47230789065361023:  47%|█████▋      | 56/119 [00:07<00:07,  8.22it/s]Epoch: 1, train for the 161-th batch, train loss: 0.3511649966239929:  67%|███████▎   | 161/241 [00:34<00:16,  4.88it/s]Epoch: 2, train for the 57-th batch, train loss: 0.47230789065361023:  48%|█████▋      | 57/119 [00:07<00:07,  7.95it/s]Epoch: 2, train for the 3-th batch, train loss: 0.7930164337158203:   1%|▏              | 2/146 [00:00<00:16,  8.63it/s]Epoch: 2, train for the 3-th batch, train loss: 0.7930164337158203:   2%|▎              | 3/146 [00:00<00:15,  9.09it/s]evaluate for the 5-th batch, evaluate loss: 0.5466919541358948:   9%|█▋                  | 4/46 [00:00<00:03, 11.68it/s]Epoch: 1, train for the 162-th batch, train loss: 0.1364031732082367:  67%|███████▎   | 161/241 [00:34<00:16,  4.88it/s]evaluate for the 6-th batch, evaluate loss: 0.5347265005111694:   9%|█▋                  | 4/46 [00:00<00:03, 11.68it/s]evaluate for the 6-th batch, evaluate loss: 0.5347265005111694:  13%|██▌                 | 6/46 [00:00<00:03, 10.83it/s]Epoch: 1, train for the 162-th batch, train loss: 0.1364031732082367:  67%|███████▍   | 162/241 [00:34<00:14,  5.42it/s]Epoch: 1, train for the 131-th batch, train loss: 0.4103919267654419:  34%|███▋       | 130/383 [00:32<01:08,  3.69it/s]Epoch: 2, train for the 4-th batch, train loss: 0.7280970811843872:   2%|▎              | 3/146 [00:00<00:15,  9.09it/s]Epoch: 2, train for the 4-th batch, train loss: 0.7280970811843872:   3%|▍              | 4/146 [00:00<00:18,  7.74it/s]Epoch: 2, train for the 58-th batch, train loss: 0.5148397088050842:  48%|██████▏      | 57/119 [00:07<00:07,  7.95it/s]Epoch: 2, train for the 58-th batch, train loss: 0.5148397088050842:  49%|██████▎      | 58/119 [00:07<00:08,  7.01it/s]Epoch: 1, train for the 131-th batch, train loss: 0.4103919267654419:  34%|███▊       | 131/383 [00:33<01:05,  3.85it/s]evaluate for the 7-th batch, evaluate loss: 0.5631925463676453:  13%|██▌                 | 6/46 [00:00<00:03, 10.83it/s]Epoch: 1, train for the 148-th batch, train loss: 0.5371211171150208:  62%|██████▊    | 147/237 [00:34<00:22,  4.01it/s]Epoch: 1, train for the 148-th batch, train loss: 0.5371211171150208:  62%|██████▊    | 148/237 [00:34<00:23,  3.78it/s]Epoch: 2, train for the 5-th batch, train loss: 0.6911277770996094:   3%|▍              | 4/146 [00:00<00:18,  7.74it/s]Epoch: 2, train for the 5-th batch, train loss: 0.6911277770996094:   3%|▌              | 5/146 [00:00<00:16,  8.34it/s]Epoch: 1, train for the 163-th batch, train loss: 0.14016205072402954:  67%|██████▋   | 162/241 [00:34<00:14,  5.42it/s]evaluate for the 8-th batch, evaluate loss: 0.5503429770469666:  13%|██▌                 | 6/46 [00:00<00:03, 10.83it/s]evaluate for the 8-th batch, evaluate loss: 0.5503429770469666:  17%|███▍                | 8/46 [00:00<00:03, 10.70it/s]Epoch: 2, train for the 59-th batch, train loss: 0.5453755855560303:  49%|██████▎      | 58/119 [00:07<00:08,  7.01it/s]Epoch: 1, train for the 163-th batch, train loss: 0.14016205072402954:  68%|██████▊   | 163/241 [00:34<00:14,  5.25it/s]Epoch: 2, train for the 59-th batch, train loss: 0.5453755855560303:  50%|██████▍      | 59/119 [00:07<00:08,  6.98it/s]Epoch: 2, train for the 6-th batch, train loss: 0.673663318157196:   3%|▌               | 5/146 [00:00<00:16,  8.34it/s]evaluate for the 9-th batch, evaluate loss: 0.5704501867294312:  17%|███▍                | 8/46 [00:00<00:03, 10.70it/s]Epoch: 2, train for the 6-th batch, train loss: 0.673663318157196:   4%|▋               | 6/146 [00:00<00:17,  8.12it/s]Epoch: 1, train for the 164-th batch, train loss: 0.11291497945785522:  68%|██████▊   | 163/241 [00:34<00:14,  5.25it/s]evaluate for the 10-th batch, evaluate loss: 0.5143699049949646:  17%|███▎               | 8/46 [00:00<00:03, 10.70it/s]evaluate for the 10-th batch, evaluate loss: 0.5143699049949646:  22%|███▉              | 10/46 [00:00<00:03, 11.48it/s]Epoch: 1, train for the 164-th batch, train loss: 0.11291497945785522:  68%|██████▊   | 164/241 [00:34<00:13,  5.70it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5203247666358948:  50%|██████▍      | 59/119 [00:07<00:08,  6.98it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5203247666358948:  50%|██████▌      | 60/119 [00:07<00:08,  6.60it/s]Epoch: 1, train for the 132-th batch, train loss: 0.4523219168186188:  34%|███▊       | 131/383 [00:33<01:05,  3.85it/s]Epoch: 2, train for the 7-th batch, train loss: 0.6536770462989807:   4%|▌              | 6/146 [00:00<00:17,  8.12it/s]evaluate for the 11-th batch, evaluate loss: 0.5638437271118164:  22%|███▉              | 10/46 [00:00<00:03, 11.48it/s]Epoch: 2, train for the 7-th batch, train loss: 0.6536770462989807:   5%|▋              | 7/146 [00:00<00:17,  7.83it/s]Epoch: 1, train for the 132-th batch, train loss: 0.4523219168186188:  34%|███▊       | 132/383 [00:33<01:13,  3.39it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5247575044631958:  50%|██████▌      | 60/119 [00:07<00:08,  6.60it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5247575044631958:  51%|██████▋      | 61/119 [00:07<00:08,  6.68it/s]Epoch: 2, train for the 8-th batch, train loss: 0.6671852469444275:   5%|▋              | 7/146 [00:01<00:17,  7.83it/s]Epoch: 2, train for the 8-th batch, train loss: 0.6671852469444275:   5%|▊              | 8/146 [00:01<00:18,  7.63it/s]Epoch: 1, train for the 133-th batch, train loss: 0.48992034792900085:  34%|███▍      | 132/383 [00:33<01:13,  3.39it/s]Epoch: 1, train for the 133-th batch, train loss: 0.48992034792900085:  35%|███▍      | 133/383 [00:33<01:05,  3.83it/s]Epoch: 1, train for the 165-th batch, train loss: 0.2000318467617035:  68%|███████▍   | 164/241 [00:35<00:13,  5.70it/s]evaluate for the 12-th batch, evaluate loss: 0.5157150030136108:  22%|███▉              | 10/46 [00:01<00:03, 11.48it/s]evaluate for the 12-th batch, evaluate loss: 0.5157150030136108:  26%|████▋             | 12/46 [00:01<00:03,  9.16it/s]Epoch: 1, train for the 149-th batch, train loss: 0.5424304008483887:  62%|██████▊    | 148/237 [00:35<00:23,  3.78it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5735296010971069:  51%|██████▋      | 61/119 [00:07<00:08,  6.68it/s]Epoch: 1, train for the 165-th batch, train loss: 0.2000318467617035:  68%|███████▌   | 165/241 [00:35<00:16,  4.62it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5735296010971069:  52%|██████▊      | 62/119 [00:07<00:08,  6.84it/s]Epoch: 1, train for the 149-th batch, train loss: 0.5424304008483887:  63%|██████▉    | 149/237 [00:35<00:30,  2.86it/s]Epoch: 2, train for the 9-th batch, train loss: 0.685126006603241:   5%|▉               | 8/146 [00:01<00:18,  7.63it/s]Epoch: 2, train for the 9-th batch, train loss: 0.685126006603241:   6%|▉               | 9/146 [00:01<00:18,  7.49it/s]evaluate for the 13-th batch, evaluate loss: 0.5263801217079163:  26%|████▋             | 12/46 [00:01<00:03,  9.16it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5213264226913452:  35%|███▊       | 133/383 [00:33<01:05,  3.83it/s]Epoch: 2, train for the 10-th batch, train loss: 0.6910932660102844:   6%|▊             | 9/146 [00:01<00:18,  7.49it/s]Epoch: 2, train for the 10-th batch, train loss: 0.6910932660102844:   7%|▉            | 10/146 [00:01<00:18,  7.29it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5213264226913452:  35%|███▊       | 134/383 [00:33<01:02,  4.02it/s]Epoch: 1, train for the 166-th batch, train loss: 0.2419765740633011:  68%|███████▌   | 165/241 [00:35<00:16,  4.62it/s]evaluate for the 14-th batch, evaluate loss: 0.5680684447288513:  26%|████▋             | 12/46 [00:01<00:03,  9.16it/s]evaluate for the 14-th batch, evaluate loss: 0.5680684447288513:  30%|█████▍            | 14/46 [00:01<00:03,  9.26it/s]Epoch: 1, train for the 150-th batch, train loss: 0.5289872884750366:  63%|██████▉    | 149/237 [00:35<00:30,  2.86it/s]Epoch: 1, train for the 166-th batch, train loss: 0.2419765740633011:  69%|███████▌   | 166/241 [00:35<00:16,  4.53it/s]evaluate for the 15-th batch, evaluate loss: 0.5515518188476562:  30%|█████▍            | 14/46 [00:01<00:03,  9.26it/s]Epoch: 1, train for the 150-th batch, train loss: 0.5289872884750366:  63%|██████▉    | 150/237 [00:35<00:27,  3.13it/s]Epoch: 2, train for the 63-th batch, train loss: 0.5167733430862427:  52%|██████▊      | 62/119 [00:08<00:08,  6.84it/s]Epoch: 2, train for the 63-th batch, train loss: 0.5167733430862427:  53%|██████▉      | 63/119 [00:08<00:10,  5.26it/s]Epoch: 2, train for the 11-th batch, train loss: 0.7068465352058411:   7%|▉            | 10/146 [00:01<00:18,  7.29it/s]Epoch: 2, train for the 11-th batch, train loss: 0.7068465352058411:   8%|▉            | 11/146 [00:01<00:18,  7.34it/s]Epoch: 1, train for the 135-th batch, train loss: 0.4611488878726959:  35%|███▊       | 134/383 [00:33<01:02,  4.02it/s]Epoch: 1, train for the 135-th batch, train loss: 0.4611488878726959:  35%|███▉       | 135/383 [00:34<00:59,  4.16it/s]Epoch: 1, train for the 167-th batch, train loss: 0.11371707916259766:  69%|██████▉   | 166/241 [00:35<00:16,  4.53it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5192066431045532:  53%|██████▉      | 63/119 [00:08<00:10,  5.26it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5192066431045532:  54%|██████▉      | 64/119 [00:08<00:09,  5.58it/s]Epoch: 2, train for the 12-th batch, train loss: 0.6906097531318665:   8%|▉            | 11/146 [00:01<00:18,  7.34it/s]Epoch: 2, train for the 12-th batch, train loss: 0.6906097531318665:   8%|█            | 12/146 [00:01<00:18,  7.08it/s]evaluate for the 16-th batch, evaluate loss: 0.5150614976882935:  30%|█████▍            | 14/46 [00:01<00:03,  9.26it/s]evaluate for the 16-th batch, evaluate loss: 0.5150614976882935:  35%|██████▎           | 16/46 [00:01<00:03,  8.53it/s]Epoch: 1, train for the 167-th batch, train loss: 0.11371707916259766:  69%|██████▉   | 167/241 [00:35<00:16,  4.39it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5489057302474976:  63%|██████▉    | 150/237 [00:36<00:27,  3.13it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5489057302474976:  64%|███████    | 151/237 [00:36<00:26,  3.30it/s]evaluate for the 17-th batch, evaluate loss: 0.4572535455226898:  35%|██████▎           | 16/46 [00:01<00:03,  8.53it/s]evaluate for the 17-th batch, evaluate loss: 0.4572535455226898:  37%|██████▋           | 17/46 [00:01<00:03,  8.76it/s]Epoch: 1, train for the 136-th batch, train loss: 0.39225876331329346:  35%|███▌      | 135/383 [00:34<00:59,  4.16it/s]Epoch: 1, train for the 136-th batch, train loss: 0.39225876331329346:  36%|███▌      | 136/383 [00:34<00:56,  4.35it/s]Epoch: 1, train for the 168-th batch, train loss: 0.1149299368262291:  69%|███████▌   | 167/241 [00:35<00:16,  4.39it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5261978507041931:  54%|██████▉      | 64/119 [00:08<00:09,  5.58it/s]Epoch: 2, train for the 13-th batch, train loss: 0.6855831146240234:   8%|█            | 12/146 [00:01<00:18,  7.08it/s]evaluate for the 18-th batch, evaluate loss: 0.5077501535415649:  37%|██████▋           | 17/46 [00:01<00:03,  8.76it/s]Epoch: 1, train for the 168-th batch, train loss: 0.1149299368262291:  70%|███████▋   | 168/241 [00:35<00:15,  4.70it/s]Epoch: 2, train for the 13-th batch, train loss: 0.6855831146240234:   9%|█▏           | 13/146 [00:01<00:20,  6.43it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5261978507041931:  55%|███████      | 65/119 [00:08<00:10,  5.38it/s]Epoch: 1, train for the 152-th batch, train loss: 0.521547257900238:  64%|███████▋    | 151/237 [00:36<00:26,  3.30it/s]evaluate for the 19-th batch, evaluate loss: 0.5778334736824036:  37%|██████▋           | 17/46 [00:01<00:03,  8.76it/s]evaluate for the 19-th batch, evaluate loss: 0.5778334736824036:  41%|███████▍          | 19/46 [00:01<00:02,  9.46it/s]Epoch: 1, train for the 152-th batch, train loss: 0.521547257900238:  64%|███████▋    | 152/237 [00:36<00:24,  3.54it/s]Epoch: 1, train for the 137-th batch, train loss: 0.47383788228034973:  36%|███▌      | 136/383 [00:34<00:56,  4.35it/s]Epoch: 2, train for the 14-th batch, train loss: 0.6755132675170898:   9%|█▏           | 13/146 [00:01<00:20,  6.43it/s]Epoch: 2, train for the 14-th batch, train loss: 0.6755132675170898:  10%|█▏           | 14/146 [00:01<00:20,  6.31it/s]Epoch: 2, train for the 66-th batch, train loss: 0.5492238402366638:  55%|███████      | 65/119 [00:08<00:10,  5.38it/s]Epoch: 1, train for the 137-th batch, train loss: 0.47383788228034973:  36%|███▌      | 137/383 [00:34<00:56,  4.36it/s]Epoch: 2, train for the 66-th batch, train loss: 0.5492238402366638:  55%|███████▏     | 66/119 [00:08<00:09,  5.38it/s]Epoch: 1, train for the 169-th batch, train loss: 0.6220358610153198:  70%|███████▋   | 168/241 [00:36<00:15,  4.70it/s]Epoch: 1, train for the 169-th batch, train loss: 0.6220358610153198:  70%|███████▋   | 169/241 [00:36<00:16,  4.27it/s]evaluate for the 20-th batch, evaluate loss: 0.5220330953598022:  41%|███████▍          | 19/46 [00:02<00:02,  9.46it/s]evaluate for the 20-th batch, evaluate loss: 0.5220330953598022:  43%|███████▊          | 20/46 [00:02<00:03,  8.25it/s]Epoch: 1, train for the 153-th batch, train loss: 0.5341013073921204:  64%|███████    | 152/237 [00:36<00:24,  3.54it/s]Epoch: 2, train for the 15-th batch, train loss: 0.6568822860717773:  10%|█▏           | 14/146 [00:02<00:20,  6.31it/s]Epoch: 2, train for the 15-th batch, train loss: 0.6568822860717773:  10%|█▎           | 15/146 [00:02<00:20,  6.43it/s]Epoch: 2, train for the 67-th batch, train loss: 0.5768430233001709:  55%|███████▏     | 66/119 [00:08<00:09,  5.38it/s]Epoch: 1, train for the 153-th batch, train loss: 0.5341013073921204:  65%|███████    | 153/237 [00:36<00:22,  3.78it/s]Epoch: 2, train for the 67-th batch, train loss: 0.5768430233001709:  56%|███████▎     | 67/119 [00:08<00:09,  5.66it/s]Epoch: 1, train for the 138-th batch, train loss: 0.2824225127696991:  36%|███▉       | 137/383 [00:34<00:56,  4.36it/s]evaluate for the 21-th batch, evaluate loss: 0.5569994449615479:  43%|███████▊          | 20/46 [00:02<00:03,  8.25it/s]Epoch: 1, train for the 138-th batch, train loss: 0.2824225127696991:  36%|███▉       | 138/383 [00:34<00:53,  4.57it/s]Epoch: 1, train for the 170-th batch, train loss: 0.7401524186134338:  70%|███████▋   | 169/241 [00:36<00:16,  4.27it/s]evaluate for the 22-th batch, evaluate loss: 0.5276358127593994:  43%|███████▊          | 20/46 [00:02<00:03,  8.25it/s]evaluate for the 22-th batch, evaluate loss: 0.5276358127593994:  48%|████████▌         | 22/46 [00:02<00:02,  9.64it/s]Epoch: 1, train for the 170-th batch, train loss: 0.7401524186134338:  71%|███████▊   | 170/241 [00:36<00:14,  4.78it/s]Epoch: 2, train for the 16-th batch, train loss: 0.6276257634162903:  10%|█▎           | 15/146 [00:02<00:20,  6.43it/s]Epoch: 2, train for the 16-th batch, train loss: 0.6276257634162903:  11%|█▍           | 16/146 [00:02<00:19,  6.75it/s]Epoch: 2, train for the 68-th batch, train loss: 0.49620845913887024:  56%|██████▊     | 67/119 [00:09<00:09,  5.66it/s]Epoch: 2, train for the 68-th batch, train loss: 0.49620845913887024:  57%|██████▊     | 68/119 [00:09<00:08,  5.89it/s]evaluate for the 23-th batch, evaluate loss: 0.4751131236553192:  48%|████████▌         | 22/46 [00:02<00:02,  9.64it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5215911269187927:  36%|███▉       | 138/383 [00:34<00:53,  4.57it/s]Epoch: 2, train for the 17-th batch, train loss: 0.6244209408760071:  11%|█▍           | 16/146 [00:02<00:19,  6.75it/s]Epoch: 2, train for the 17-th batch, train loss: 0.6244209408760071:  12%|█▌           | 17/146 [00:02<00:18,  7.09it/s]Epoch: 1, train for the 171-th batch, train loss: 0.607357382774353:  71%|████████▍   | 170/241 [00:36<00:14,  4.78it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5215911269187927:  36%|███▉       | 139/383 [00:34<00:51,  4.70it/s]Epoch: 1, train for the 171-th batch, train loss: 0.607357382774353:  71%|████████▌   | 171/241 [00:36<00:13,  5.19it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5822079181671143:  57%|███████▍     | 68/119 [00:09<00:08,  5.89it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5822079181671143:  58%|███████▌     | 69/119 [00:09<00:07,  6.48it/s]Epoch: 1, train for the 154-th batch, train loss: 0.5561736822128296:  65%|███████    | 153/237 [00:36<00:22,  3.78it/s]Epoch: 1, train for the 154-th batch, train loss: 0.5561736822128296:  65%|███████▏   | 154/237 [00:36<00:23,  3.47it/s]Epoch: 2, train for the 18-th batch, train loss: 0.6057220101356506:  12%|█▌           | 17/146 [00:02<00:18,  7.09it/s]Epoch: 2, train for the 18-th batch, train loss: 0.6057220101356506:  12%|█▌           | 18/146 [00:02<00:17,  7.37it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5114386081695557:  58%|███████▌     | 69/119 [00:09<00:07,  6.48it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5114386081695557:  59%|███████▋     | 70/119 [00:09<00:07,  6.80it/s]Epoch: 1, train for the 140-th batch, train loss: 0.43425196409225464:  36%|███▋      | 139/383 [00:35<00:51,  4.70it/s]Epoch: 1, train for the 140-th batch, train loss: 0.43425196409225464:  37%|███▋      | 140/383 [00:35<00:51,  4.74it/s]Epoch: 2, train for the 19-th batch, train loss: 0.6052159070968628:  12%|█▌           | 18/146 [00:02<00:17,  7.37it/s]evaluate for the 24-th batch, evaluate loss: 0.5040609836578369:  48%|████████▌         | 22/46 [00:02<00:02,  9.64it/s]evaluate for the 24-th batch, evaluate loss: 0.5040609836578369:  52%|█████████▍        | 24/46 [00:02<00:02,  7.53it/s]Epoch: 1, train for the 172-th batch, train loss: 0.5359270572662354:  71%|███████▊   | 171/241 [00:36<00:13,  5.19it/s]Epoch: 2, train for the 19-th batch, train loss: 0.6052159070968628:  13%|█▋           | 19/146 [00:02<00:16,  7.66it/s]Epoch: 1, train for the 172-th batch, train loss: 0.5359270572662354:  71%|███████▊   | 172/241 [00:36<00:14,  4.85it/s]Epoch: 1, train for the 155-th batch, train loss: 0.5671120882034302:  65%|███████▏   | 154/237 [00:37<00:23,  3.47it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5172886848449707:  59%|███████▋     | 70/119 [00:09<00:07,  6.80it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5172886848449707:  60%|███████▊     | 71/119 [00:09<00:06,  7.03it/s]Epoch: 1, train for the 155-th batch, train loss: 0.5671120882034302:  65%|███████▏   | 155/237 [00:37<00:21,  3.75it/s]evaluate for the 25-th batch, evaluate loss: 0.5053480267524719:  52%|█████████▍        | 24/46 [00:02<00:02,  7.53it/s]Epoch: 2, train for the 20-th batch, train loss: 0.5941686034202576:  13%|█▋           | 19/146 [00:02<00:16,  7.66it/s]Epoch: 2, train for the 20-th batch, train loss: 0.5941686034202576:  14%|█▊           | 20/146 [00:02<00:16,  7.73it/s]Epoch: 1, train for the 141-th batch, train loss: 0.43568092584609985:  37%|███▋      | 140/383 [00:35<00:51,  4.74it/s]Epoch: 1, train for the 141-th batch, train loss: 0.43568092584609985:  37%|███▋      | 141/383 [00:35<00:52,  4.58it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5447099208831787:  60%|███████▊     | 71/119 [00:09<00:06,  7.03it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5447099208831787:  61%|███████▊     | 72/119 [00:09<00:07,  6.64it/s]Epoch: 2, train for the 21-th batch, train loss: 0.59737628698349:  14%|██             | 20/146 [00:02<00:16,  7.73it/s]Epoch: 1, train for the 156-th batch, train loss: 0.5536715984344482:  65%|███████▏   | 155/237 [00:37<00:21,  3.75it/s]Epoch: 2, train for the 21-th batch, train loss: 0.59737628698349:  14%|██▏            | 21/146 [00:02<00:16,  7.52it/s]Epoch: 1, train for the 156-th batch, train loss: 0.5536715984344482:  66%|███████▏   | 156/237 [00:37<00:20,  3.94it/s]evaluate for the 26-th batch, evaluate loss: 0.5494024753570557:  52%|█████████▍        | 24/46 [00:02<00:02,  7.53it/s]evaluate for the 26-th batch, evaluate loss: 0.5494024753570557:  57%|██████████▏       | 26/46 [00:02<00:02,  7.00it/s]Epoch: 2, train for the 73-th batch, train loss: 0.4775363504886627:  61%|███████▊     | 72/119 [00:09<00:07,  6.64it/s]Epoch: 1, train for the 142-th batch, train loss: 0.46589764952659607:  37%|███▋      | 141/383 [00:35<00:52,  4.58it/s]Epoch: 2, train for the 22-th batch, train loss: 0.6010497212409973:  14%|█▊           | 21/146 [00:02<00:16,  7.52it/s]Epoch: 2, train for the 22-th batch, train loss: 0.6010497212409973:  15%|█▉           | 22/146 [00:03<00:17,  7.14it/s]Epoch: 2, train for the 73-th batch, train loss: 0.4775363504886627:  61%|███████▉     | 73/119 [00:09<00:07,  6.17it/s]Epoch: 1, train for the 173-th batch, train loss: 0.33227404952049255:  71%|███████▏  | 172/241 [00:37<00:14,  4.85it/s]evaluate for the 27-th batch, evaluate loss: 0.522483229637146:  57%|██████████▋        | 26/46 [00:03<00:02,  7.00it/s]evaluate for the 27-th batch, evaluate loss: 0.522483229637146:  59%|███████████▏       | 27/46 [00:03<00:02,  7.29it/s]Epoch: 1, train for the 142-th batch, train loss: 0.46589764952659607:  37%|███▋      | 142/383 [00:35<00:54,  4.45it/s]Epoch: 1, train for the 173-th batch, train loss: 0.33227404952049255:  72%|███████▏  | 173/241 [00:37<00:18,  3.61it/s]Epoch: 1, train for the 157-th batch, train loss: 0.5797247290611267:  66%|███████▏   | 156/237 [00:37<00:20,  3.94it/s]evaluate for the 28-th batch, evaluate loss: 0.5335626006126404:  59%|██████████▌       | 27/46 [00:03<00:02,  7.29it/s]Epoch: 1, train for the 157-th batch, train loss: 0.5797247290611267:  66%|███████▎   | 157/237 [00:37<00:19,  4.19it/s]Epoch: 2, train for the 23-th batch, train loss: 0.6038644313812256:  15%|█▉           | 22/146 [00:03<00:17,  7.14it/s]Epoch: 2, train for the 23-th batch, train loss: 0.6038644313812256:  16%|██           | 23/146 [00:03<00:16,  7.33it/s]Epoch: 2, train for the 74-th batch, train loss: 0.5683024525642395:  61%|███████▉     | 73/119 [00:10<00:07,  6.17it/s]Epoch: 2, train for the 74-th batch, train loss: 0.5683024525642395:  62%|████████     | 74/119 [00:10<00:07,  6.11it/s]Epoch: 1, train for the 174-th batch, train loss: 0.42134714126586914:  72%|███████▏  | 173/241 [00:37<00:18,  3.61it/s]evaluate for the 29-th batch, evaluate loss: 0.49129608273506165:  59%|█████████▉       | 27/46 [00:03<00:02,  7.29it/s]evaluate for the 29-th batch, evaluate loss: 0.49129608273506165:  63%|██████████▋      | 29/46 [00:03<00:02,  8.28it/s]Epoch: 1, train for the 143-th batch, train loss: 0.3802817463874817:  37%|████       | 142/383 [00:35<00:54,  4.45it/s]Epoch: 1, train for the 174-th batch, train loss: 0.42134714126586914:  72%|███████▏  | 174/241 [00:37<00:16,  4.06it/s]Epoch: 2, train for the 24-th batch, train loss: 0.6006824970245361:  16%|██           | 23/146 [00:03<00:16,  7.33it/s]Epoch: 2, train for the 24-th batch, train loss: 0.6006824970245361:  16%|██▏          | 24/146 [00:03<00:15,  7.84it/s]Epoch: 1, train for the 143-th batch, train loss: 0.3802817463874817:  37%|████       | 143/383 [00:35<00:53,  4.47it/s]Epoch: 2, train for the 75-th batch, train loss: 0.5221253037452698:  62%|████████     | 74/119 [00:10<00:07,  6.11it/s]Epoch: 2, train for the 75-th batch, train loss: 0.5221253037452698:  63%|████████▏    | 75/119 [00:10<00:06,  6.57it/s]Epoch: 2, train for the 25-th batch, train loss: 0.5853460431098938:  16%|██▏          | 24/146 [00:03<00:15,  7.84it/s]Epoch: 2, train for the 25-th batch, train loss: 0.5853460431098938:  17%|██▏          | 25/146 [00:03<00:15,  7.91it/s]evaluate for the 30-th batch, evaluate loss: 0.5101996064186096:  63%|███████████▎      | 29/46 [00:03<00:02,  8.28it/s]evaluate for the 30-th batch, evaluate loss: 0.5101996064186096:  65%|███████████▋      | 30/46 [00:03<00:02,  7.58it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5374413728713989:  63%|████████▏    | 75/119 [00:10<00:06,  6.57it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5374413728713989:  64%|████████▎    | 76/119 [00:10<00:06,  6.92it/s]evaluate for the 31-th batch, evaluate loss: 0.4504021406173706:  65%|███████████▋      | 30/46 [00:03<00:02,  7.58it/s]Epoch: 1, train for the 175-th batch, train loss: 0.2736906409263611:  72%|███████▉   | 174/241 [00:37<00:16,  4.06it/s]Epoch: 2, train for the 26-th batch, train loss: 0.5494393110275269:  17%|██▏          | 25/146 [00:03<00:15,  7.91it/s]Epoch: 2, train for the 26-th batch, train loss: 0.5494393110275269:  18%|██▎          | 26/146 [00:03<00:14,  8.10it/s]Epoch: 1, train for the 175-th batch, train loss: 0.2736906409263611:  73%|███████▉   | 175/241 [00:37<00:16,  3.96it/s]Epoch: 1, train for the 158-th batch, train loss: 0.5180110931396484:  66%|███████▎   | 157/237 [00:37<00:19,  4.19it/s]Epoch: 1, train for the 144-th batch, train loss: 0.33795738220214844:  37%|███▋      | 143/383 [00:35<00:53,  4.47it/s]evaluate for the 32-th batch, evaluate loss: 0.48835447430610657:  65%|███████████      | 30/46 [00:03<00:02,  7.58it/s]evaluate for the 32-th batch, evaluate loss: 0.48835447430610657:  70%|███████████▊     | 32/46 [00:03<00:01,  9.38it/s]Epoch: 1, train for the 158-th batch, train loss: 0.5180110931396484:  67%|███████▎   | 158/237 [00:37<00:23,  3.43it/s]Epoch: 2, train for the 77-th batch, train loss: 0.4846796989440918:  64%|████████▎    | 76/119 [00:10<00:06,  6.92it/s]Epoch: 1, train for the 144-th batch, train loss: 0.33795738220214844:  38%|███▊      | 144/383 [00:36<00:57,  4.15it/s]Epoch: 2, train for the 77-th batch, train loss: 0.4846796989440918:  65%|████████▍    | 77/119 [00:10<00:05,  7.34it/s]Epoch: 2, train for the 27-th batch, train loss: 0.5791457891464233:  18%|██▎          | 26/146 [00:03<00:14,  8.10it/s]Epoch: 2, train for the 27-th batch, train loss: 0.5791457891464233:  18%|██▍          | 27/146 [00:03<00:14,  8.33it/s]Epoch: 1, train for the 176-th batch, train loss: 0.3708098530769348:  73%|███████▉   | 175/241 [00:37<00:16,  3.96it/s]evaluate for the 33-th batch, evaluate loss: 0.4948784410953522:  70%|████████████▌     | 32/46 [00:03<00:01,  9.38it/s]Epoch: 1, train for the 176-th batch, train loss: 0.3708098530769348:  73%|████████   | 176/241 [00:37<00:14,  4.42it/s]Epoch: 2, train for the 78-th batch, train loss: 0.4980475902557373:  65%|████████▍    | 77/119 [00:10<00:05,  7.34it/s]Epoch: 2, train for the 78-th batch, train loss: 0.4980475902557373:  66%|████████▌    | 78/119 [00:10<00:05,  7.22it/s]Epoch: 2, train for the 28-th batch, train loss: 0.5614284873008728:  18%|██▍          | 27/146 [00:03<00:14,  8.33it/s]Epoch: 1, train for the 145-th batch, train loss: 0.4610745906829834:  38%|████▏      | 144/383 [00:36<00:57,  4.15it/s]Epoch: 2, train for the 28-th batch, train loss: 0.5614284873008728:  19%|██▍          | 28/146 [00:03<00:14,  8.26it/s]Epoch: 1, train for the 145-th batch, train loss: 0.4610745906829834:  38%|████▏      | 145/383 [00:36<00:56,  4.22it/s]Epoch: 2, train for the 29-th batch, train loss: 0.5326244831085205:  19%|██▍          | 28/146 [00:03<00:14,  8.26it/s]Epoch: 1, train for the 177-th batch, train loss: 0.28567054867744446:  73%|███████▎  | 176/241 [00:37<00:14,  4.42it/s]Epoch: 1, train for the 159-th batch, train loss: 0.574356198310852:  67%|████████    | 158/237 [00:38<00:23,  3.43it/s]evaluate for the 34-th batch, evaluate loss: 0.4588474929332733:  70%|████████████▌     | 32/46 [00:03<00:01,  9.38it/s]evaluate for the 34-th batch, evaluate loss: 0.4588474929332733:  74%|█████████████▎    | 34/46 [00:03<00:01,  7.64it/s]Epoch: 2, train for the 79-th batch, train loss: 0.4887402355670929:  66%|████████▌    | 78/119 [00:10<00:05,  7.22it/s]Epoch: 1, train for the 177-th batch, train loss: 0.28567054867744446:  73%|███████▎  | 177/241 [00:37<00:14,  4.48it/s]Epoch: 2, train for the 79-th batch, train loss: 0.4887402355670929:  66%|████████▋    | 79/119 [00:10<00:06,  6.62it/s]Epoch: 1, train for the 159-th batch, train loss: 0.574356198310852:  67%|████████    | 159/237 [00:38<00:24,  3.18it/s]Epoch: 1, train for the 146-th batch, train loss: 0.3628544807434082:  38%|████▏      | 145/383 [00:36<00:56,  4.22it/s]evaluate for the 35-th batch, evaluate loss: 0.5130842328071594:  74%|█████████████▎    | 34/46 [00:04<00:01,  7.64it/s]Epoch: 1, train for the 146-th batch, train loss: 0.3628544807434082:  38%|████▏      | 146/383 [00:36<00:53,  4.42it/s]Epoch: 2, train for the 80-th batch, train loss: 0.522345781326294:  66%|█████████▎    | 79/119 [00:10<00:06,  6.62it/s]Epoch: 1, train for the 178-th batch, train loss: 0.4558003842830658:  73%|████████   | 177/241 [00:37<00:14,  4.48it/s]Epoch: 2, train for the 80-th batch, train loss: 0.522345781326294:  67%|█████████▍    | 80/119 [00:10<00:05,  7.00it/s]Epoch: 1, train for the 178-th batch, train loss: 0.4558003842830658:  74%|████████   | 178/241 [00:37<00:12,  5.02it/s]evaluate for the 36-th batch, evaluate loss: 0.4651683568954468:  74%|█████████████▎    | 34/46 [00:04<00:01,  7.64it/s]evaluate for the 36-th batch, evaluate loss: 0.4651683568954468:  78%|██████████████    | 36/46 [00:04<00:01,  8.75it/s]Epoch: 2, train for the 30-th batch, train loss: 0.5390334725379944:  19%|██▍          | 28/146 [00:04<00:14,  8.26it/s]Epoch: 2, train for the 30-th batch, train loss: 0.5390334725379944:  21%|██▋          | 30/146 [00:04<00:16,  7.20it/s]Epoch: 1, train for the 160-th batch, train loss: 0.5735011696815491:  67%|███████▍   | 159/237 [00:38<00:24,  3.18it/s]Epoch: 1, train for the 160-th batch, train loss: 0.5735011696815491:  68%|███████▍   | 160/237 [00:38<00:21,  3.52it/s]evaluate for the 37-th batch, evaluate loss: 0.5177112221717834:  78%|██████████████    | 36/46 [00:04<00:01,  8.75it/s]Epoch: 2, train for the 81-th batch, train loss: 0.523911714553833:  67%|█████████▍    | 80/119 [00:10<00:05,  7.00it/s]Epoch: 2, train for the 81-th batch, train loss: 0.523911714553833:  68%|█████████▌    | 81/119 [00:10<00:05,  6.95it/s]Epoch: 1, train for the 147-th batch, train loss: 0.3207530379295349:  38%|████▏      | 146/383 [00:36<00:53,  4.42it/s]Epoch: 2, train for the 31-th batch, train loss: 0.5528125762939453:  21%|██▋          | 30/146 [00:04<00:16,  7.20it/s]Epoch: 2, train for the 31-th batch, train loss: 0.5528125762939453:  21%|██▊          | 31/146 [00:04<00:15,  7.23it/s]Epoch: 1, train for the 147-th batch, train loss: 0.3207530379295349:  38%|████▏      | 147/383 [00:36<00:53,  4.44it/s]Epoch: 2, train for the 82-th batch, train loss: 0.5312684774398804:  68%|████████▊    | 81/119 [00:11<00:05,  6.95it/s]Epoch: 2, train for the 82-th batch, train loss: 0.5312684774398804:  69%|████████▉    | 82/119 [00:11<00:05,  7.21it/s]Epoch: 2, train for the 32-th batch, train loss: 0.5468427538871765:  21%|██▊          | 31/146 [00:04<00:15,  7.23it/s]Epoch: 1, train for the 179-th batch, train loss: 0.4105081856250763:  74%|████████   | 178/241 [00:38<00:12,  5.02it/s]Epoch: 2, train for the 32-th batch, train loss: 0.5468427538871765:  22%|██▊          | 32/146 [00:04<00:15,  7.31it/s]Epoch: 1, train for the 179-th batch, train loss: 0.4105081856250763:  74%|████████▏  | 179/241 [00:38<00:14,  4.28it/s]evaluate for the 38-th batch, evaluate loss: 0.48637330532073975:  78%|█████████████▎   | 36/46 [00:04<00:01,  8.75it/s]evaluate for the 38-th batch, evaluate loss: 0.48637330532073975:  83%|██████████████   | 38/46 [00:04<00:01,  7.26it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5230951309204102:  69%|████████▉    | 82/119 [00:11<00:05,  7.21it/s]Epoch: 1, train for the 161-th batch, train loss: 0.5893889665603638:  68%|███████▍   | 160/237 [00:38<00:21,  3.52it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5230951309204102:  70%|█████████    | 83/119 [00:11<00:05,  7.13it/s]Epoch: 1, train for the 148-th batch, train loss: 0.3172281086444855:  38%|████▏      | 147/383 [00:36<00:53,  4.44it/s]Epoch: 2, train for the 33-th batch, train loss: 0.5476083159446716:  22%|██▊          | 32/146 [00:04<00:15,  7.31it/s]Epoch: 2, train for the 33-th batch, train loss: 0.5476083159446716:  23%|██▉          | 33/146 [00:04<00:15,  7.31it/s]Epoch: 1, train for the 161-th batch, train loss: 0.5893889665603638:  68%|███████▍   | 161/237 [00:38<00:23,  3.30it/s]Epoch: 1, train for the 180-th batch, train loss: 0.5552153587341309:  74%|████████▏  | 179/241 [00:38<00:14,  4.28it/s]Epoch: 1, train for the 148-th batch, train loss: 0.3172281086444855:  39%|████▎      | 148/383 [00:36<00:56,  4.20it/s]evaluate for the 39-th batch, evaluate loss: 0.4836418330669403:  83%|██████████████▊   | 38/46 [00:04<00:01,  7.26it/s]Epoch: 1, train for the 180-th batch, train loss: 0.5552153587341309:  75%|████████▏  | 180/241 [00:38<00:12,  4.79it/s]evaluate for the 40-th batch, evaluate loss: 0.47941192984580994:  83%|██████████████   | 38/46 [00:04<00:01,  7.26it/s]evaluate for the 40-th batch, evaluate loss: 0.47941192984580994:  87%|██████████████▊  | 40/46 [00:04<00:00,  8.48it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5036148428916931:  70%|█████████    | 83/119 [00:11<00:05,  7.13it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5036148428916931:  71%|█████████▏   | 84/119 [00:11<00:05,  6.91it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5572420954704285:  23%|██▉          | 33/146 [00:04<00:15,  7.31it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5572420954704285:  23%|███          | 34/146 [00:04<00:15,  7.10it/s]Epoch: 1, train for the 181-th batch, train loss: 0.7217243909835815:  75%|████████▏  | 180/241 [00:38<00:12,  4.79it/s]evaluate for the 41-th batch, evaluate loss: 0.4885358214378357:  87%|███████████████▋  | 40/46 [00:04<00:00,  8.48it/s]Epoch: 1, train for the 149-th batch, train loss: 0.398099809885025:  39%|████▋       | 148/383 [00:37<00:56,  4.20it/s]Epoch: 1, train for the 181-th batch, train loss: 0.7217243909835815:  75%|████████▎  | 181/241 [00:38<00:12,  4.97it/s]Epoch: 2, train for the 85-th batch, train loss: 0.544805109500885:  71%|█████████▉    | 84/119 [00:11<00:05,  6.91it/s]Epoch: 1, train for the 149-th batch, train loss: 0.398099809885025:  39%|████▋       | 149/383 [00:37<00:55,  4.22it/s]Epoch: 2, train for the 85-th batch, train loss: 0.544805109500885:  71%|██████████    | 85/119 [00:11<00:04,  7.00it/s]Epoch: 2, train for the 35-th batch, train loss: 0.506269633769989:  23%|███▎          | 34/146 [00:04<00:15,  7.10it/s]Epoch: 2, train for the 35-th batch, train loss: 0.506269633769989:  24%|███▎          | 35/146 [00:04<00:15,  7.03it/s]Epoch: 1, train for the 182-th batch, train loss: 0.6561182737350464:  75%|████████▎  | 181/241 [00:38<00:12,  4.97it/s]Epoch: 1, train for the 182-th batch, train loss: 0.6561182737350464:  76%|████████▎  | 182/241 [00:38<00:10,  5.74it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5542107820510864:  71%|█████████▎   | 85/119 [00:11<00:04,  7.00it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5542107820510864:  72%|█████████▍   | 86/119 [00:11<00:04,  7.19it/s]evaluate for the 42-th batch, evaluate loss: 0.46123817563056946:  87%|██████████████▊  | 40/46 [00:04<00:00,  8.48it/s]evaluate for the 42-th batch, evaluate loss: 0.46123817563056946:  91%|███████████████▌ | 42/46 [00:04<00:00,  7.68it/s]Epoch: 2, train for the 36-th batch, train loss: 0.5118353366851807:  24%|███          | 35/146 [00:04<00:15,  7.03it/s]Epoch: 2, train for the 36-th batch, train loss: 0.5118353366851807:  25%|███▏         | 36/146 [00:04<00:15,  7.18it/s]Epoch: 1, train for the 183-th batch, train loss: 0.5074894428253174:  76%|████████▎  | 182/241 [00:38<00:10,  5.74it/s]Epoch: 1, train for the 162-th batch, train loss: 0.5909925699234009:  68%|███████▍   | 161/237 [00:39<00:23,  3.30it/s]Epoch: 1, train for the 183-th batch, train loss: 0.5074894428253174:  76%|████████▎  | 183/241 [00:38<00:09,  6.06it/s]Epoch: 1, train for the 150-th batch, train loss: 0.5757564902305603:  39%|████▎      | 149/383 [00:37<00:55,  4.22it/s]evaluate for the 43-th batch, evaluate loss: 0.529386043548584:  91%|█████████████████▎ | 42/46 [00:05<00:00,  7.68it/s]Epoch: 1, train for the 162-th batch, train loss: 0.5909925699234009:  68%|███████▌   | 162/237 [00:39<00:27,  2.76it/s]Epoch: 1, train for the 150-th batch, train loss: 0.5757564902305603:  39%|████▎      | 150/383 [00:37<00:57,  4.07it/s]Epoch: 2, train for the 87-th batch, train loss: 0.5232794880867004:  72%|█████████▍   | 86/119 [00:11<00:04,  7.19it/s]Epoch: 2, train for the 87-th batch, train loss: 0.5232794880867004:  73%|█████████▌   | 87/119 [00:11<00:04,  7.11it/s]evaluate for the 44-th batch, evaluate loss: 0.47845718264579773:  91%|███████████████▌ | 42/46 [00:05<00:00,  7.68it/s]evaluate for the 44-th batch, evaluate loss: 0.47845718264579773:  96%|████████████████▎| 44/46 [00:05<00:00,  8.97it/s]Epoch: 2, train for the 37-th batch, train loss: 0.5483188629150391:  25%|███▏         | 36/146 [00:04<00:15,  7.18it/s]Epoch: 2, train for the 37-th batch, train loss: 0.5483188629150391:  25%|███▎         | 37/146 [00:05<00:15,  7.16it/s]Epoch: 2, train for the 88-th batch, train loss: 0.5560383200645447:  73%|█████████▌   | 87/119 [00:11<00:04,  7.11it/s]Epoch: 2, train for the 88-th batch, train loss: 0.5560383200645447:  74%|█████████▌   | 88/119 [00:11<00:04,  6.74it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5412315726280212:  39%|████▎      | 150/383 [00:37<00:57,  4.07it/s]Epoch: 2, train for the 38-th batch, train loss: 0.5106731653213501:  25%|███▎         | 37/146 [00:05<00:15,  7.16it/s]Epoch: 2, train for the 38-th batch, train loss: 0.5106731653213501:  26%|███▍         | 38/146 [00:05<00:15,  6.77it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5412315726280212:  39%|████▎      | 151/383 [00:37<00:56,  4.09it/s]Epoch: 1, train for the 184-th batch, train loss: 0.5852745175361633:  76%|████████▎  | 183/241 [00:39<00:09,  6.06it/s]Epoch: 1, train for the 163-th batch, train loss: 0.5894217491149902:  68%|███████▌   | 162/237 [00:39<00:27,  2.76it/s]Epoch: 1, train for the 184-th batch, train loss: 0.5852745175361633:  76%|████████▍  | 184/241 [00:39<00:12,  4.70it/s]Epoch: 1, train for the 163-th batch, train loss: 0.5894217491149902:  69%|███████▌   | 163/237 [00:39<00:25,  2.87it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5017502307891846:  74%|█████████▌   | 88/119 [00:12<00:04,  6.74it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5017502307891846:  75%|█████████▋   | 89/119 [00:12<00:04,  6.84it/s]Epoch: 2, train for the 39-th batch, train loss: 0.5533263087272644:  26%|███▍         | 38/146 [00:05<00:15,  6.77it/s]Epoch: 2, train for the 39-th batch, train loss: 0.5533263087272644:  27%|███▍         | 39/146 [00:05<00:15,  6.81it/s]evaluate for the 45-th batch, evaluate loss: 0.48292258381843567:  96%|████████████████▎| 44/46 [00:05<00:00,  8.97it/s]Epoch: 1, train for the 152-th batch, train loss: 0.4309472143650055:  39%|████▎      | 151/383 [00:37<00:56,  4.09it/s]Epoch: 1, train for the 185-th batch, train loss: 0.6705052852630615:  76%|████████▍  | 184/241 [00:39<00:12,  4.70it/s]Epoch: 1, train for the 152-th batch, train loss: 0.4309472143650055:  40%|████▎      | 152/383 [00:37<00:53,  4.33it/s]Epoch: 1, train for the 185-th batch, train loss: 0.6705052852630615:  77%|████████▍  | 185/241 [00:39<00:11,  5.01it/s]evaluate for the 46-th batch, evaluate loss: 0.4751458168029785:  96%|█████████████████▏| 44/46 [00:05<00:00,  8.97it/s]evaluate for the 46-th batch, evaluate loss: 0.4751458168029785: 100%|██████████████████| 46/46 [00:05<00:00,  7.03it/s]evaluate for the 46-th batch, evaluate loss: 0.4751458168029785: 100%|██████████████████| 46/46 [00:05<00:00,  8.36it/s]
Epoch: 2, train for the 90-th batch, train loss: 0.5249991416931152:  75%|█████████▋   | 89/119 [00:12<00:04,  6.84it/s]Epoch: 1, train for the 164-th batch, train loss: 0.5979090929031372:  69%|███████▌   | 163/237 [00:39<00:25,  2.87it/s]Epoch: 2, train for the 90-th batch, train loss: 0.5249991416931152:  76%|█████████▊   | 90/119 [00:12<00:04,  6.69it/s]Epoch: 2, train for the 40-th batch, train loss: 0.517948567867279:  27%|███▋          | 39/146 [00:05<00:15,  6.81it/s]Epoch: 1, train for the 164-th batch, train loss: 0.5979090929031372:  69%|███████▌   | 164/237 [00:39<00:22,  3.30it/s]Epoch: 2, train for the 40-th batch, train loss: 0.517948567867279:  27%|███▊          | 40/146 [00:05<00:15,  6.94it/s]Epoch: 1, train for the 186-th batch, train loss: 0.6141391396522522:  77%|████████▍  | 185/241 [00:39<00:11,  5.01it/s]Epoch: 1, train for the 186-th batch, train loss: 0.6141391396522522:  77%|████████▍  | 186/241 [00:39<00:09,  5.60it/s]Epoch: 2, train for the 91-th batch, train loss: 0.5007261037826538:  76%|█████████▊   | 90/119 [00:12<00:04,  6.69it/s]Epoch: 1, train for the 153-th batch, train loss: 0.4919721186161041:  40%|████▎      | 152/383 [00:38<00:53,  4.33it/s]Epoch: 2, train for the 91-th batch, train loss: 0.5007261037826538:  76%|█████████▉   | 91/119 [00:12<00:04,  6.81it/s]Epoch: 2, train for the 41-th batch, train loss: 0.4998771548271179:  27%|███▌         | 40/146 [00:05<00:15,  6.94it/s]Epoch: 2, train for the 41-th batch, train loss: 0.4998771548271179:  28%|███▋         | 41/146 [00:05<00:15,  6.77it/s]Epoch: 1, train for the 153-th batch, train loss: 0.4919721186161041:  40%|████▍      | 153/383 [00:38<00:52,  4.37it/s]Epoch: 1, train for the 165-th batch, train loss: 0.55038982629776:  69%|████████▉    | 164/237 [00:40<00:22,  3.30it/s]Epoch: 1, train for the 165-th batch, train loss: 0.55038982629776:  70%|█████████    | 165/237 [00:40<00:19,  3.63it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5117054581642151:  76%|█████████▉   | 91/119 [00:12<00:04,  6.81it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5117054581642151:  77%|██████████   | 92/119 [00:12<00:03,  7.00it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5054788589477539:  28%|███▋         | 41/146 [00:05<00:15,  6.77it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5054788589477539:  29%|███▋         | 42/146 [00:05<00:15,  6.82it/s]Epoch: 1, train for the 154-th batch, train loss: 0.44828471541404724:  40%|███▉      | 153/383 [00:38<00:52,  4.37it/s]Epoch: 2, train for the 93-th batch, train loss: 0.4753125309944153:  77%|██████████   | 92/119 [00:12<00:03,  7.00it/s]Epoch: 2, train for the 93-th batch, train loss: 0.4753125309944153:  78%|██████████▏  | 93/119 [00:12<00:03,  7.42it/s]Epoch: 1, train for the 187-th batch, train loss: 0.659774899482727:  77%|█████████▎  | 186/241 [00:39<00:09,  5.60it/s]Epoch: 1, train for the 154-th batch, train loss: 0.44828471541404724:  40%|████      | 154/383 [00:38<00:51,  4.46it/s]Epoch: 1, train for the 187-th batch, train loss: 0.659774899482727:  78%|█████████▎  | 187/241 [00:39<00:12,  4.49it/s]Epoch: 2, train for the 43-th batch, train loss: 0.5126060247421265:  29%|███▋         | 42/146 [00:05<00:15,  6.82it/s]Epoch: 2, train for the 43-th batch, train loss: 0.5126060247421265:  29%|███▊         | 43/146 [00:05<00:14,  7.30it/s]Epoch: 1, train for the 166-th batch, train loss: 0.5252636671066284:  70%|███████▋   | 165/237 [00:40<00:19,  3.63it/s]Epoch: 1, train for the 166-th batch, train loss: 0.5252636671066284:  70%|███████▋   | 166/237 [00:40<00:19,  3.63it/s]Epoch: 2, train for the 94-th batch, train loss: 0.495636522769928:  78%|██████████▉   | 93/119 [00:12<00:03,  7.42it/s]Epoch: 2, train for the 94-th batch, train loss: 0.495636522769928:  79%|███████████   | 94/119 [00:12<00:03,  7.64it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5091040134429932:  29%|███▊         | 43/146 [00:05<00:14,  7.30it/s]  0%|                                                                                            | 0/25 [00:00<?, ?it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5091040134429932:  30%|███▉         | 44/146 [00:06<00:13,  7.31it/s]Epoch: 1, train for the 155-th batch, train loss: 0.32818397879600525:  40%|████      | 154/383 [00:38<00:51,  4.46it/s]Epoch: 1, train for the 188-th batch, train loss: 0.4906390905380249:  78%|████████▌  | 187/241 [00:40<00:12,  4.49it/s]Epoch: 1, train for the 188-th batch, train loss: 0.4906390905380249:  78%|████████▌  | 188/241 [00:40<00:11,  4.70it/s]Epoch: 1, train for the 155-th batch, train loss: 0.32818397879600525:  40%|████      | 155/383 [00:38<00:51,  4.46it/s]Epoch: 2, train for the 95-th batch, train loss: 0.46162134408950806:  79%|█████████▍  | 94/119 [00:12<00:03,  7.64it/s]Epoch: 2, train for the 95-th batch, train loss: 0.46162134408950806:  80%|█████████▌  | 95/119 [00:12<00:03,  7.68it/s]evaluate for the 1-th batch, evaluate loss: 0.7607200741767883:   0%|                            | 0/25 [00:00<?, ?it/s]Epoch: 1, train for the 167-th batch, train loss: 0.5593066811561584:  70%|███████▋   | 166/237 [00:40<00:19,  3.63it/s]Epoch: 2, train for the 45-th batch, train loss: 0.512839674949646:  30%|████▏         | 44/146 [00:06<00:13,  7.31it/s]Epoch: 2, train for the 45-th batch, train loss: 0.512839674949646:  31%|████▎         | 45/146 [00:06<00:13,  7.33it/s]evaluate for the 2-th batch, evaluate loss: 0.752081573009491:   0%|                             | 0/25 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.752081573009491:   8%|█▋                   | 2/25 [00:00<00:01, 13.34it/s]Epoch: 1, train for the 167-th batch, train loss: 0.5593066811561584:  70%|███████▊   | 167/237 [00:40<00:18,  3.85it/s]Epoch: 2, train for the 96-th batch, train loss: 0.4408777058124542:  80%|██████████▍  | 95/119 [00:13<00:03,  7.68it/s]Epoch: 2, train for the 96-th batch, train loss: 0.4408777058124542:  81%|██████████▍  | 96/119 [00:13<00:03,  6.99it/s]Epoch: 1, train for the 156-th batch, train loss: 0.37007230520248413:  40%|████      | 155/383 [00:38<00:51,  4.46it/s]Epoch: 1, train for the 189-th batch, train loss: 0.41970932483673096:  78%|███████▊  | 188/241 [00:40<00:11,  4.70it/s]evaluate for the 3-th batch, evaluate loss: 0.7955380082130432:   8%|█▌                  | 2/25 [00:00<00:01, 13.34it/s]Epoch: 2, train for the 46-th batch, train loss: 0.5144016742706299:  31%|████         | 45/146 [00:06<00:13,  7.33it/s]Epoch: 2, train for the 46-th batch, train loss: 0.5144016742706299:  32%|████         | 46/146 [00:06<00:14,  7.09it/s]Epoch: 1, train for the 189-th batch, train loss: 0.41970932483673096:  78%|███████▊  | 189/241 [00:40<00:11,  4.38it/s]Epoch: 1, train for the 156-th batch, train loss: 0.37007230520248413:  41%|████      | 156/383 [00:38<00:53,  4.25it/s]Epoch: 1, train for the 168-th batch, train loss: 0.5810009241104126:  70%|███████▊   | 167/237 [00:40<00:18,  3.85it/s]Epoch: 1, train for the 168-th batch, train loss: 0.5810009241104126:  71%|███████▊   | 168/237 [00:40<00:16,  4.10it/s]Epoch: 2, train for the 97-th batch, train loss: 0.5054072141647339:  81%|██████████▍  | 96/119 [00:13<00:03,  6.99it/s]evaluate for the 4-th batch, evaluate loss: 0.7239419221878052:   8%|█▌                  | 2/25 [00:00<00:01, 13.34it/s]evaluate for the 4-th batch, evaluate loss: 0.7239419221878052:  16%|███▏                | 4/25 [00:00<00:02, 10.00it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5716802477836609:  32%|████         | 46/146 [00:06<00:14,  7.09it/s]Epoch: 2, train for the 97-th batch, train loss: 0.5054072141647339:  82%|██████████▌  | 97/119 [00:13<00:03,  6.59it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5716802477836609:  32%|████▏        | 47/146 [00:06<00:13,  7.24it/s]Epoch: 1, train for the 190-th batch, train loss: 0.4108918011188507:  78%|████████▋  | 189/241 [00:40<00:11,  4.38it/s]Epoch: 1, train for the 190-th batch, train loss: 0.4108918011188507:  79%|████████▋  | 190/241 [00:40<00:10,  4.84it/s]evaluate for the 5-th batch, evaluate loss: 0.746324896812439:  16%|███▎                 | 4/25 [00:00<00:02, 10.00it/s]Epoch: 1, train for the 157-th batch, train loss: 0.4457762539386749:  41%|████▍      | 156/383 [00:39<00:53,  4.25it/s]evaluate for the 6-th batch, evaluate loss: 0.745023787021637:  16%|███▎                 | 4/25 [00:00<00:02, 10.00it/s]evaluate for the 6-th batch, evaluate loss: 0.745023787021637:  24%|█████                | 6/25 [00:00<00:01, 10.68it/s]Epoch: 2, train for the 48-th batch, train loss: 0.5454549193382263:  32%|████▏        | 47/146 [00:06<00:13,  7.24it/s]Epoch: 1, train for the 157-th batch, train loss: 0.4457762539386749:  41%|████▌      | 157/383 [00:39<00:55,  4.05it/s]Epoch: 2, train for the 48-th batch, train loss: 0.5454549193382263:  33%|████▎        | 48/146 [00:06<00:14,  6.82it/s]Epoch: 2, train for the 98-th batch, train loss: 0.46071067452430725:  82%|█████████▊  | 97/119 [00:13<00:03,  6.59it/s]Epoch: 2, train for the 98-th batch, train loss: 0.46071067452430725:  82%|█████████▉  | 98/119 [00:13<00:03,  6.23it/s]evaluate for the 7-th batch, evaluate loss: 0.7708937525749207:  24%|████▊               | 6/25 [00:00<00:01, 10.68it/s]Epoch: 1, train for the 169-th batch, train loss: 0.5674533843994141:  71%|███████▊   | 168/237 [00:41<00:16,  4.10it/s]Epoch: 1, train for the 191-th batch, train loss: 0.3736124336719513:  79%|████████▋  | 190/241 [00:40<00:10,  4.84it/s]evaluate for the 8-th batch, evaluate loss: 0.747109591960907:  24%|█████                | 6/25 [00:00<00:01, 10.68it/s]evaluate for the 8-th batch, evaluate loss: 0.747109591960907:  32%|██████▋              | 8/25 [00:00<00:01, 11.11it/s]Epoch: 1, train for the 191-th batch, train loss: 0.3736124336719513:  79%|████████▋  | 191/241 [00:40<00:11,  4.41it/s]Epoch: 1, train for the 169-th batch, train loss: 0.5674533843994141:  71%|███████▊   | 169/237 [00:41<00:19,  3.53it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5124608278274536:  33%|████▎        | 48/146 [00:06<00:14,  6.82it/s]Epoch: 1, train for the 158-th batch, train loss: 0.4112628102302551:  41%|████▌      | 157/383 [00:39<00:55,  4.05it/s]Epoch: 2, train for the 99-th batch, train loss: 0.48017188906669617:  82%|█████████▉  | 98/119 [00:13<00:03,  6.23it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5124608278274536:  34%|████▎        | 49/146 [00:06<00:14,  6.57it/s]Epoch: 2, train for the 99-th batch, train loss: 0.48017188906669617:  83%|█████████▉  | 99/119 [00:13<00:03,  6.22it/s]Epoch: 1, train for the 158-th batch, train loss: 0.4112628102302551:  41%|████▌      | 158/383 [00:39<00:52,  4.27it/s]evaluate for the 9-th batch, evaluate loss: 0.7171643376350403:  32%|██████▍             | 8/25 [00:00<00:01, 11.11it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5701977610588074:  34%|████▎        | 49/146 [00:06<00:14,  6.57it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5701977610588074:  34%|████▍        | 50/146 [00:06<00:14,  6.65it/s]Epoch: 1, train for the 192-th batch, train loss: 0.335750013589859:  79%|█████████▌  | 191/241 [00:40<00:11,  4.41it/s]evaluate for the 10-th batch, evaluate loss: 0.769996166229248:  32%|██████▍             | 8/25 [00:00<00:01, 11.11it/s]evaluate for the 10-th batch, evaluate loss: 0.769996166229248:  40%|███████▌           | 10/25 [00:00<00:01, 10.96it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5052459836006165:  83%|█████████▉  | 99/119 [00:13<00:03,  6.22it/s]Epoch: 1, train for the 192-th batch, train loss: 0.335750013589859:  80%|█████████▌  | 192/241 [00:40<00:10,  4.62it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5052459836006165:  84%|█████████▏ | 100/119 [00:13<00:03,  6.14it/s]Epoch: 1, train for the 170-th batch, train loss: 0.5628708004951477:  71%|███████▊   | 169/237 [00:41<00:19,  3.53it/s]evaluate for the 11-th batch, evaluate loss: 0.7546212673187256:  40%|███████▏          | 10/25 [00:00<00:01, 10.96it/s]Epoch: 1, train for the 170-th batch, train loss: 0.5628708004951477:  72%|███████▉   | 170/237 [00:41<00:18,  3.62it/s]Epoch: 2, train for the 51-th batch, train loss: 0.5307953357696533:  34%|████▍        | 50/146 [00:07<00:14,  6.65it/s]Epoch: 2, train for the 51-th batch, train loss: 0.5307953357696533:  35%|████▌        | 51/146 [00:07<00:13,  7.05it/s]Epoch: 2, train for the 101-th batch, train loss: 0.47044748067855835:  84%|████████▍ | 100/119 [00:13<00:03,  6.14it/s]evaluate for the 12-th batch, evaluate loss: 0.6704286336898804:  40%|███████▏          | 10/25 [00:01<00:01, 10.96it/s]evaluate for the 12-th batch, evaluate loss: 0.6704286336898804:  48%|████████▋         | 12/25 [00:01<00:01, 11.78it/s]Epoch: 2, train for the 101-th batch, train loss: 0.47044748067855835:  85%|████████▍ | 101/119 [00:13<00:02,  6.46it/s]evaluate for the 13-th batch, evaluate loss: 0.6797254681587219:  48%|████████▋         | 12/25 [00:01<00:01, 11.78it/s]Epoch: 2, train for the 52-th batch, train loss: 0.537417471408844:  35%|████▉         | 51/146 [00:07<00:13,  7.05it/s]Epoch: 2, train for the 52-th batch, train loss: 0.537417471408844:  36%|████▉         | 52/146 [00:07<00:12,  7.34it/s]Epoch: 1, train for the 193-th batch, train loss: 0.3388043940067291:  80%|████████▊  | 192/241 [00:41<00:10,  4.62it/s]Epoch: 2, train for the 102-th batch, train loss: 0.5058152079582214:  85%|█████████▎ | 101/119 [00:14<00:02,  6.46it/s]Epoch: 2, train for the 102-th batch, train loss: 0.5058152079582214:  86%|█████████▍ | 102/119 [00:14<00:02,  6.87it/s]Epoch: 1, train for the 159-th batch, train loss: 0.4666665196418762:  41%|████▌      | 158/383 [00:39<00:52,  4.27it/s]Epoch: 1, train for the 171-th batch, train loss: 0.5650452375411987:  72%|███████▉   | 170/237 [00:41<00:18,  3.62it/s]Epoch: 1, train for the 193-th batch, train loss: 0.3388043940067291:  80%|████████▊  | 193/241 [00:41<00:11,  4.30it/s]Epoch: 1, train for the 171-th batch, train loss: 0.5650452375411987:  72%|███████▉   | 171/237 [00:41<00:17,  3.71it/s]Epoch: 1, train for the 159-th batch, train loss: 0.4666665196418762:  42%|████▌      | 159/383 [00:39<01:08,  3.29it/s]evaluate for the 14-th batch, evaluate loss: 0.7292921543121338:  48%|████████▋         | 12/25 [00:01<00:01, 11.78it/s]evaluate for the 14-th batch, evaluate loss: 0.7292921543121338:  56%|██████████        | 14/25 [00:01<00:00, 11.14it/s]evaluate for the 15-th batch, evaluate loss: 0.7698735594749451:  56%|██████████        | 14/25 [00:01<00:00, 11.14it/s]Epoch: 2, train for the 53-th batch, train loss: 0.5464988350868225:  36%|████▋        | 52/146 [00:07<00:12,  7.34it/s]Epoch: 2, train for the 103-th batch, train loss: 0.4944280982017517:  86%|█████████▍ | 102/119 [00:14<00:02,  6.87it/s]Epoch: 2, train for the 53-th batch, train loss: 0.5464988350868225:  36%|████▋        | 53/146 [00:07<00:15,  5.88it/s]Epoch: 2, train for the 103-th batch, train loss: 0.4944280982017517:  87%|█████████▌ | 103/119 [00:14<00:02,  6.08it/s]evaluate for the 16-th batch, evaluate loss: 0.688006579875946:  56%|██████████▋        | 14/25 [00:01<00:00, 11.14it/s]evaluate for the 16-th batch, evaluate loss: 0.688006579875946:  64%|████████████▏      | 16/25 [00:01<00:00, 11.51it/s]Epoch: 1, train for the 172-th batch, train loss: 0.5297314524650574:  72%|███████▉   | 171/237 [00:41<00:17,  3.71it/s]evaluate for the 17-th batch, evaluate loss: 0.6807570457458496:  64%|███████████▌      | 16/25 [00:01<00:00, 11.51it/s]Epoch: 1, train for the 160-th batch, train loss: 0.49953779578208923:  42%|████▏     | 159/383 [00:39<01:08,  3.29it/s]Epoch: 1, train for the 172-th batch, train loss: 0.5297314524650574:  73%|███████▉   | 172/237 [00:41<00:16,  3.83it/s]evaluate for the 18-th batch, evaluate loss: 0.6496114730834961:  64%|███████████▌      | 16/25 [00:01<00:00, 11.51it/s]evaluate for the 18-th batch, evaluate loss: 0.6496114730834961:  72%|████████████▉     | 18/25 [00:01<00:00, 12.67it/s]Epoch: 1, train for the 160-th batch, train loss: 0.49953779578208923:  42%|████▏     | 160/383 [00:40<01:07,  3.29it/s]Epoch: 2, train for the 54-th batch, train loss: 0.5584245324134827:  36%|████▋        | 53/146 [00:07<00:15,  5.88it/s]Epoch: 2, train for the 104-th batch, train loss: 0.4750099778175354:  87%|█████████▌ | 103/119 [00:14<00:02,  6.08it/s]Epoch: 2, train for the 54-th batch, train loss: 0.5584245324134827:  37%|████▊        | 54/146 [00:07<00:15,  5.76it/s]Epoch: 2, train for the 104-th batch, train loss: 0.4750099778175354:  87%|█████████▌ | 104/119 [00:14<00:02,  5.90it/s]Epoch: 1, train for the 194-th batch, train loss: 0.3430800437927246:  80%|████████▊  | 193/241 [00:41<00:11,  4.30it/s]evaluate for the 19-th batch, evaluate loss: 0.6298666000366211:  72%|████████████▉     | 18/25 [00:01<00:00, 12.67it/s]Epoch: 1, train for the 194-th batch, train loss: 0.3430800437927246:  80%|████████▊  | 194/241 [00:41<00:13,  3.40it/s]Epoch: 1, train for the 173-th batch, train loss: 0.49104735255241394:  73%|███████▎  | 172/237 [00:42<00:16,  3.83it/s]Epoch: 1, train for the 173-th batch, train loss: 0.49104735255241394:  73%|███████▎  | 173/237 [00:42<00:15,  4.14it/s]evaluate for the 20-th batch, evaluate loss: 0.6904545426368713:  72%|████████████▉     | 18/25 [00:01<00:00, 12.67it/s]evaluate for the 20-th batch, evaluate loss: 0.6904545426368713:  80%|██████████████▍   | 20/25 [00:01<00:00, 12.64it/s]Epoch: 2, train for the 105-th batch, train loss: 0.46481016278266907:  87%|████████▋ | 104/119 [00:14<00:02,  5.90it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5630533695220947:  37%|████▊        | 54/146 [00:07<00:15,  5.76it/s]Epoch: 2, train for the 105-th batch, train loss: 0.46481016278266907:  88%|████████▊ | 105/119 [00:14<00:02,  5.79it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5630533695220947:  38%|████▉        | 55/146 [00:07<00:16,  5.59it/s]Epoch: 1, train for the 161-th batch, train loss: 0.45372098684310913:  42%|████▏     | 160/383 [00:40<01:07,  3.29it/s]Epoch: 1, train for the 195-th batch, train loss: 0.595585823059082:  80%|█████████▋  | 194/241 [00:41<00:13,  3.40it/s]evaluate for the 21-th batch, evaluate loss: 0.7122013568878174:  80%|██████████████▍   | 20/25 [00:01<00:00, 12.64it/s]Epoch: 1, train for the 195-th batch, train loss: 0.595585823059082:  81%|█████████▋  | 195/241 [00:41<00:12,  3.72it/s]Epoch: 1, train for the 161-th batch, train loss: 0.45372098684310913:  42%|████▏     | 161/383 [00:40<01:06,  3.35it/s]Epoch: 1, train for the 174-th batch, train loss: 0.5495671033859253:  73%|████████   | 173/237 [00:42<00:15,  4.14it/s]evaluate for the 22-th batch, evaluate loss: 0.6705698370933533:  80%|██████████████▍   | 20/25 [00:01<00:00, 12.64it/s]evaluate for the 22-th batch, evaluate loss: 0.6705698370933533:  88%|███████████████▊  | 22/25 [00:01<00:00, 11.34it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5286783576011658:  38%|████▉        | 55/146 [00:07<00:16,  5.59it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5286783576011658:  38%|████▉        | 56/146 [00:07<00:15,  5.73it/s]Epoch: 1, train for the 174-th batch, train loss: 0.5495671033859253:  73%|████████   | 174/237 [00:42<00:15,  4.11it/s]Epoch: 2, train for the 106-th batch, train loss: 0.4770564138889313:  88%|█████████▋ | 105/119 [00:14<00:02,  5.79it/s]Epoch: 2, train for the 106-th batch, train loss: 0.4770564138889313:  89%|█████████▊ | 106/119 [00:14<00:02,  5.41it/s]Epoch: 1, train for the 196-th batch, train loss: 0.5231695175170898:  81%|████████▉  | 195/241 [00:41<00:12,  3.72it/s]evaluate for the 23-th batch, evaluate loss: 0.6687878966331482:  88%|███████████████▊  | 22/25 [00:02<00:00, 11.34it/s]Epoch: 1, train for the 196-th batch, train loss: 0.5231695175170898:  81%|████████▉  | 196/241 [00:42<00:10,  4.17it/s]Epoch: 2, train for the 57-th batch, train loss: 0.5491928458213806:  38%|████▉        | 56/146 [00:08<00:15,  5.73it/s]Epoch: 2, train for the 57-th batch, train loss: 0.5491928458213806:  39%|█████        | 57/146 [00:08<00:14,  6.28it/s]evaluate for the 24-th batch, evaluate loss: 0.6858567595481873:  88%|███████████████▊  | 22/25 [00:02<00:00, 11.34it/s]evaluate for the 24-th batch, evaluate loss: 0.6858567595481873:  96%|█████████████████▎| 24/25 [00:02<00:00, 11.23it/s]Epoch: 1, train for the 162-th batch, train loss: 0.4583529531955719:  42%|████▌      | 161/383 [00:40<01:06,  3.35it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5076736807823181:  89%|█████████▊ | 106/119 [00:14<00:02,  5.41it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5076736807823181:  90%|█████████▉ | 107/119 [00:14<00:02,  5.74it/s]Epoch: 1, train for the 197-th batch, train loss: 0.5607028603553772:  81%|████████▉  | 196/241 [00:42<00:10,  4.17it/s]Epoch: 1, train for the 162-th batch, train loss: 0.4583529531955719:  42%|████▋      | 162/383 [00:40<01:06,  3.32it/s]Epoch: 1, train for the 197-th batch, train loss: 0.5607028603553772:  82%|████████▉  | 197/241 [00:42<00:09,  4.75it/s]Epoch: 2, train for the 58-th batch, train loss: 0.5309150218963623:  39%|█████        | 57/146 [00:08<00:14,  6.28it/s]evaluate for the 25-th batch, evaluate loss: 0.6993444561958313:  96%|█████████████████▎| 24/25 [00:02<00:00, 11.23it/s]evaluate for the 25-th batch, evaluate loss: 0.6993444561958313: 100%|██████████████████| 25/25 [00:02<00:00, 11.51it/s]
Epoch: 2, train for the 58-th batch, train loss: 0.5309150218963623:  40%|█████▏       | 58/146 [00:08<00:12,  6.79it/s]Epoch: 1, train for the 175-th batch, train loss: 0.569813072681427:  73%|████████▊   | 174/237 [00:42<00:15,  4.11it/s]Epoch: 1, train for the 175-th batch, train loss: 0.569813072681427:  74%|████████▊   | 175/237 [00:42<00:15,  3.91it/s]Epoch: 2, train for the 108-th batch, train loss: 0.40173789858818054:  90%|████████▉ | 107/119 [00:15<00:02,  5.74it/s]Epoch: 2, train for the 108-th batch, train loss: 0.40173789858818054:  91%|█████████ | 108/119 [00:15<00:01,  6.11it/s]Epoch: 2, train for the 59-th batch, train loss: 0.5100013017654419:  40%|█████▏       | 58/146 [00:08<00:12,  6.79it/s]Epoch: 2, train for the 59-th batch, train loss: 0.5100013017654419:  40%|█████▎       | 59/146 [00:08<00:12,  7.21it/s]Epoch: 1, train for the 198-th batch, train loss: 0.5251973867416382:  82%|████████▉  | 197/241 [00:42<00:09,  4.75it/s]Epoch: 1, train for the 198-th batch, train loss: 0.5251973867416382:  82%|█████████  | 198/241 [00:42<00:08,  5.11it/s]Epoch: 1, train for the 163-th batch, train loss: 0.3853325843811035:  42%|████▋      | 162/383 [00:40<01:06,  3.32it/s]Epoch: 2, train for the 109-th batch, train loss: 0.48677825927734375:  91%|█████████ | 108/119 [00:15<00:01,  6.11it/s]Epoch: 1, train for the 163-th batch, train loss: 0.3853325843811035:  43%|████▋      | 163/383 [00:40<01:02,  3.52it/s]Epoch: 2, train for the 109-th batch, train loss: 0.48677825927734375:  92%|█████████▏| 109/119 [00:15<00:01,  6.34it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5494726300239563:  40%|█████▎       | 59/146 [00:08<00:12,  7.21it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5494726300239563:  41%|█████▎       | 60/146 [00:08<00:12,  7.09it/s]INFO:root:Epoch: 1, learning rate: 0.0001, train loss: 0.6030
INFO:root:train average_precision, 0.7375
INFO:root:train roc_auc, 0.7153
INFO:root:validate loss: 0.5145
INFO:root:validate average_precision, 0.8396
INFO:root:validate roc_auc, 0.8294
INFO:root:new node validate loss: 0.7163
INFO:root:new node validate first_1_average_precision, 0.5398
INFO:root:new node validate first_1_roc_auc, 0.5172
INFO:root:new node validate first_3_average_precision, 0.5779
INFO:root:new node validate first_3_roc_auc, 0.5614
INFO:root:new node validate first_10_average_precision, 0.6325
INFO:root:new node validate first_10_roc_auc, 0.6231
INFO:root:new node validate average_precision, 0.6532
INFO:root:new node validate roc_auc, 0.6405
INFO:root:save model ./saved_models/TGN/ia-retweet-pol/TGN_seed0_dummy-lhwm9lqt/TGN_seed0_dummy-lhwm9lqt.pkl
Epoch: 1, train for the 199-th batch, train loss: 0.35681700706481934:  82%|████████▏ | 198/241 [00:42<00:08,  5.11it/s]Epoch: 2, train for the 110-th batch, train loss: 0.5343228578567505:  92%|██████████ | 109/119 [00:15<00:01,  6.34it/s]Epoch: 2, train for the 110-th batch, train loss: 0.5343228578567505:  92%|██████████▏| 110/119 [00:15<00:01,  6.53it/s]Epoch: 1, train for the 199-th batch, train loss: 0.35681700706481934:  83%|████████▎ | 199/241 [00:42<00:08,  4.79it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5820779800415039:  41%|█████▎       | 60/146 [00:08<00:12,  7.09it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5820779800415039:  42%|█████▍       | 61/146 [00:08<00:11,  7.09it/s]Epoch: 1, train for the 176-th batch, train loss: 0.5632839202880859:  74%|████████   | 175/237 [00:42<00:15,  3.91it/s]Epoch: 1, train for the 164-th batch, train loss: 0.36999619007110596:  43%|████▎     | 163/383 [00:41<01:02,  3.52it/s]Epoch: 1, train for the 176-th batch, train loss: 0.5632839202880859:  74%|████████▏  | 176/237 [00:43<00:18,  3.36it/s]Epoch: 1, train for the 164-th batch, train loss: 0.36999619007110596:  43%|████▎     | 164/383 [00:41<00:59,  3.71it/s]Epoch: 1, train for the 200-th batch, train loss: 0.44890546798706055:  83%|████████▎ | 199/241 [00:42<00:08,  4.79it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5235039591789246:  92%|██████████▏| 110/119 [00:15<00:01,  6.53it/s]Epoch: 1, train for the 200-th batch, train loss: 0.44890546798706055:  83%|████████▎ | 200/241 [00:42<00:07,  5.40it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5235039591789246:  93%|██████████▎| 111/119 [00:15<00:01,  6.65it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5142436623573303:  42%|█████▍       | 61/146 [00:08<00:11,  7.09it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5142436623573303:  42%|█████▌       | 62/146 [00:08<00:11,  7.02it/s]Epoch: 1, train for the 165-th batch, train loss: 0.333903431892395:  43%|█████▏      | 164/383 [00:41<00:59,  3.71it/s]Epoch: 1, train for the 201-th batch, train loss: 0.4889516830444336:  83%|█████████▏ | 200/241 [00:42<00:07,  5.40it/s]Epoch: 1, train for the 201-th batch, train loss: 0.4889516830444336:  83%|█████████▏ | 201/241 [00:42<00:06,  5.98it/s]Epoch: 2, train for the 112-th batch, train loss: 0.54405277967453:  93%|████████████▏| 111/119 [00:15<00:01,  6.65it/s]Epoch: 1, train for the 165-th batch, train loss: 0.333903431892395:  43%|█████▏      | 165/383 [00:41<00:54,  4.02it/s]Epoch: 2, train for the 112-th batch, train loss: 0.54405277967453:  94%|████████████▏| 112/119 [00:15<00:01,  6.71it/s]Epoch: 2, train for the 63-th batch, train loss: 0.5557494759559631:  42%|█████▌       | 62/146 [00:08<00:11,  7.02it/s]Epoch: 2, train for the 63-th batch, train loss: 0.5557494759559631:  43%|█████▌       | 63/146 [00:08<00:11,  6.97it/s]Epoch: 1, train for the 177-th batch, train loss: 0.5866276025772095:  74%|████████▏  | 176/237 [00:43<00:18,  3.36it/s]Epoch: 1, train for the 177-th batch, train loss: 0.5866276025772095:  75%|████████▏  | 177/237 [00:43<00:18,  3.22it/s]Epoch: 2, train for the 113-th batch, train loss: 0.5043377876281738:  94%|██████████▎| 112/119 [00:15<00:01,  6.71it/s]Epoch: 2, train for the 113-th batch, train loss: 0.5043377876281738:  95%|██████████▍| 113/119 [00:15<00:00,  6.84it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5549159646034241:  43%|█████▌       | 63/146 [00:09<00:11,  6.97it/s]Epoch: 1, train for the 202-th batch, train loss: 0.25810402631759644:  83%|████████▎ | 201/241 [00:43<00:06,  5.98it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5549159646034241:  44%|█████▋       | 64/146 [00:09<00:11,  7.05it/s]Epoch: 1, train for the 166-th batch, train loss: 0.4653647541999817:  43%|████▋      | 165/383 [00:41<00:54,  4.02it/s]Epoch: 1, train for the 202-th batch, train loss: 0.25810402631759644:  84%|████████▍ | 202/241 [00:43<00:07,  5.45it/s]Epoch: 1, train for the 166-th batch, train loss: 0.4653647541999817:  43%|████▊      | 166/383 [00:41<00:54,  3.97it/s]Epoch: 1, train for the 178-th batch, train loss: 0.5789570212364197:  75%|████████▏  | 177/237 [00:43<00:18,  3.22it/s]Epoch: 2, train for the 114-th batch, train loss: 0.449602335691452:  95%|███████████▍| 113/119 [00:15<00:00,  6.84it/s]Epoch: 2, train for the 114-th batch, train loss: 0.449602335691452:  96%|███████████▍| 114/119 [00:15<00:00,  6.80it/s]Epoch: 1, train for the 203-th batch, train loss: 0.585810661315918:  84%|██████████  | 202/241 [00:43<00:07,  5.45it/s]Epoch: 1, train for the 178-th batch, train loss: 0.5789570212364197:  75%|████████▎  | 178/237 [00:43<00:15,  3.71it/s]Epoch: 1, train for the 203-th batch, train loss: 0.585810661315918:  84%|██████████  | 203/241 [00:43<00:06,  6.13it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5685595273971558:  44%|█████▋       | 64/146 [00:09<00:11,  7.05it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5685595273971558:  45%|█████▊       | 65/146 [00:09<00:11,  6.93it/s]Epoch: 2, train for the 115-th batch, train loss: 0.4461268484592438:  96%|██████████▌| 114/119 [00:16<00:00,  6.80it/s]Epoch: 2, train for the 115-th batch, train loss: 0.4461268484592438:  97%|██████████▋| 115/119 [00:16<00:00,  6.74it/s]Epoch: 2, train for the 66-th batch, train loss: 0.5339938402175903:  45%|█████▊       | 65/146 [00:09<00:11,  6.93it/s]Epoch: 2, train for the 66-th batch, train loss: 0.5339938402175903:  45%|█████▉       | 66/146 [00:09<00:11,  6.87it/s]Epoch: 1, train for the 167-th batch, train loss: 0.43072909116744995:  43%|████▎     | 166/383 [00:41<00:54,  3.97it/s]Epoch: 1, train for the 167-th batch, train loss: 0.43072909116744995:  44%|████▎     | 167/383 [00:41<00:55,  3.92it/s]Epoch: 1, train for the 204-th batch, train loss: 0.4791724383831024:  84%|█████████▎ | 203/241 [00:43<00:06,  6.13it/s]Epoch: 1, train for the 204-th batch, train loss: 0.4791724383831024:  85%|█████████▎ | 204/241 [00:43<00:07,  5.25it/s]Epoch: 2, train for the 116-th batch, train loss: 0.4082227945327759:  97%|██████████▋| 115/119 [00:16<00:00,  6.74it/s]Epoch: 2, train for the 116-th batch, train loss: 0.4082227945327759:  97%|██████████▋| 116/119 [00:16<00:00,  6.89it/s]Epoch: 2, train for the 67-th batch, train loss: 0.5826697945594788:  45%|█████▉       | 66/146 [00:09<00:11,  6.87it/s]Epoch: 2, train for the 67-th batch, train loss: 0.5826697945594788:  46%|█████▉       | 67/146 [00:09<00:11,  6.92it/s]Epoch: 1, train for the 179-th batch, train loss: 0.5165451765060425:  75%|████████▎  | 178/237 [00:43<00:15,  3.71it/s]Epoch: 1, train for the 179-th batch, train loss: 0.5165451765060425:  76%|████████▎  | 179/237 [00:43<00:17,  3.32it/s]Epoch: 1, train for the 205-th batch, train loss: 0.4824499487876892:  85%|█████████▎ | 204/241 [00:43<00:07,  5.25it/s]Epoch: 1, train for the 168-th batch, train loss: 0.31487342715263367:  44%|████▎     | 167/383 [00:42<00:55,  3.92it/s]Epoch: 1, train for the 205-th batch, train loss: 0.4824499487876892:  85%|█████████▎ | 205/241 [00:43<00:06,  5.60it/s]Epoch: 2, train for the 117-th batch, train loss: 0.43467849493026733:  97%|█████████▋| 116/119 [00:16<00:00,  6.89it/s]Epoch: 2, train for the 68-th batch, train loss: 0.533740758895874:  46%|██████▍       | 67/146 [00:09<00:11,  6.92it/s]Epoch: 1, train for the 168-th batch, train loss: 0.31487342715263367:  44%|████▍     | 168/383 [00:42<00:54,  3.93it/s]Epoch: 2, train for the 117-th batch, train loss: 0.43467849493026733:  98%|█████████▊| 117/119 [00:16<00:00,  6.22it/s]Epoch: 2, train for the 68-th batch, train loss: 0.533740758895874:  47%|██████▌       | 68/146 [00:09<00:11,  6.68it/s]Epoch: 1, train for the 180-th batch, train loss: 0.5839439034461975:  76%|████████▎  | 179/237 [00:44<00:17,  3.32it/s]Epoch: 1, train for the 206-th batch, train loss: 0.5574461221694946:  85%|█████████▎ | 205/241 [00:43<00:06,  5.60it/s]Epoch: 1, train for the 206-th batch, train loss: 0.5574461221694946:  85%|█████████▍ | 206/241 [00:43<00:05,  6.04it/s]Epoch: 1, train for the 180-th batch, train loss: 0.5839439034461975:  76%|████████▎  | 180/237 [00:44<00:15,  3.77it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5697896480560303:  47%|██████       | 68/146 [00:09<00:11,  6.68it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5697896480560303:  47%|██████▏      | 69/146 [00:09<00:10,  7.16it/s]Epoch: 2, train for the 118-th batch, train loss: 0.4459052085876465:  98%|██████████▊| 117/119 [00:16<00:00,  6.22it/s]Epoch: 2, train for the 118-th batch, train loss: 0.4459052085876465:  99%|██████████▉| 118/119 [00:16<00:00,  6.15it/s]Epoch: 1, train for the 207-th batch, train loss: 0.5644485354423523:  85%|█████████▍ | 206/241 [00:43<00:05,  6.04it/s]Epoch: 1, train for the 207-th batch, train loss: 0.5644485354423523:  86%|█████████▍ | 207/241 [00:43<00:05,  6.46it/s]Epoch: 1, train for the 169-th batch, train loss: 0.42022696137428284:  44%|████▍     | 168/383 [00:42<00:54,  3.93it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5474132299423218:  47%|██████▏      | 69/146 [00:09<00:10,  7.16it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5474132299423218:  48%|██████▏      | 70/146 [00:09<00:10,  7.16it/s]Epoch: 1, train for the 169-th batch, train loss: 0.42022696137428284:  44%|████▍     | 169/383 [00:42<00:55,  3.86it/s]Epoch: 2, train for the 119-th batch, train loss: 0.4257115125656128:  99%|██████████▉| 118/119 [00:16<00:00,  6.15it/s]Epoch: 2, train for the 119-th batch, train loss: 0.4257115125656128: 100%|███████████| 119/119 [00:16<00:00,  6.79it/s]Epoch: 2, train for the 119-th batch, train loss: 0.4257115125656128: 100%|███████████| 119/119 [00:16<00:00,  7.11it/s]
Epoch: 1, train for the 181-th batch, train loss: 0.5455543994903564:  76%|████████▎  | 180/237 [00:44<00:15,  3.77it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5637586116790771:  48%|██████▏      | 70/146 [00:09<00:10,  7.16it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5637586116790771:  49%|██████▎      | 71/146 [00:09<00:09,  7.69it/s]Epoch: 1, train for the 208-th batch, train loss: 0.536864697933197:  86%|██████████▎ | 207/241 [00:43<00:05,  6.46it/s]Epoch: 2, train for the 1-th batch, train loss: 0.7310431003570557:   0%|                       | 0/151 [00:00<?, ?it/s]Epoch: 1, train for the 181-th batch, train loss: 0.5455543994903564:  76%|████████▍  | 181/237 [00:44<00:15,  3.63it/s]Epoch: 2, train for the 1-th batch, train loss: 0.7310431003570557:   1%|               | 1/151 [00:00<00:19,  7.78it/s]Epoch: 1, train for the 208-th batch, train loss: 0.536864697933197:  86%|██████████▎ | 208/241 [00:44<00:05,  6.02it/s]Epoch: 1, train for the 170-th batch, train loss: 0.354867160320282:  44%|█████▎      | 169/383 [00:42<00:55,  3.86it/s]  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5729657411575317:  49%|██████▎      | 71/146 [00:10<00:09,  7.69it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5729657411575317:  49%|██████▍      | 72/146 [00:10<00:09,  7.82it/s]Epoch: 2, train for the 2-th batch, train loss: 0.7095738053321838:   1%|               | 1/151 [00:00<00:19,  7.78it/s]Epoch: 2, train for the 2-th batch, train loss: 0.7095738053321838:   1%|▏              | 2/151 [00:00<00:17,  8.48it/s]evaluate for the 1-th batch, evaluate loss: 0.4876997172832489:   0%|                            | 0/40 [00:00<?, ?it/s]Epoch: 1, train for the 170-th batch, train loss: 0.354867160320282:  44%|█████▎      | 170/383 [00:42<00:54,  3.93it/s]Epoch: 1, train for the 209-th batch, train loss: 0.5349070429801941:  86%|█████████▍ | 208/241 [00:44<00:05,  6.02it/s]Epoch: 1, train for the 182-th batch, train loss: 0.5860403180122375:  76%|████████▍  | 181/237 [00:44<00:15,  3.63it/s]Epoch: 1, train for the 209-th batch, train loss: 0.5349070429801941:  87%|█████████▌ | 209/241 [00:44<00:05,  6.03it/s]evaluate for the 2-th batch, evaluate loss: 0.5106490850448608:   0%|                            | 0/40 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5106490850448608:   5%|█                   | 2/40 [00:00<00:02, 16.62it/s]Epoch: 1, train for the 182-th batch, train loss: 0.5860403180122375:  77%|████████▍  | 182/237 [00:44<00:14,  3.91it/s]Epoch: 2, train for the 3-th batch, train loss: 0.6928303241729736:   1%|▏              | 2/151 [00:00<00:17,  8.48it/s]Epoch: 2, train for the 3-th batch, train loss: 0.6928303241729736:   2%|▎              | 3/151 [00:00<00:16,  8.79it/s]Epoch: 2, train for the 73-th batch, train loss: 0.5618500113487244:  49%|██████▍      | 72/146 [00:10<00:09,  7.82it/s]Epoch: 2, train for the 73-th batch, train loss: 0.5618500113487244:  50%|██████▌      | 73/146 [00:10<00:09,  7.56it/s]evaluate for the 3-th batch, evaluate loss: 0.5074435472488403:   5%|█                   | 2/40 [00:00<00:02, 16.62it/s]Epoch: 1, train for the 210-th batch, train loss: 0.46762797236442566:  87%|████████▋ | 209/241 [00:44<00:05,  6.03it/s]evaluate for the 4-th batch, evaluate loss: 0.5747424364089966:   5%|█                   | 2/40 [00:00<00:02, 16.62it/s]evaluate for the 4-th batch, evaluate loss: 0.5747424364089966:  10%|██                  | 4/40 [00:00<00:02, 14.38it/s]Epoch: 1, train for the 210-th batch, train loss: 0.46762797236442566:  87%|████████▋ | 210/241 [00:44<00:05,  5.93it/s]Epoch: 1, train for the 171-th batch, train loss: 0.4780336320400238:  44%|████▉      | 170/383 [00:42<00:54,  3.93it/s]Epoch: 2, train for the 4-th batch, train loss: 0.6715083122253418:   2%|▎              | 3/151 [00:00<00:16,  8.79it/s]Epoch: 2, train for the 4-th batch, train loss: 0.6715083122253418:   3%|▍              | 4/151 [00:00<00:19,  7.65it/s]Epoch: 2, train for the 74-th batch, train loss: 0.5334955453872681:  50%|██████▌      | 73/146 [00:10<00:09,  7.56it/s]evaluate for the 5-th batch, evaluate loss: 0.5372250080108643:  10%|██                  | 4/40 [00:00<00:02, 14.38it/s]Epoch: 2, train for the 74-th batch, train loss: 0.5334955453872681:  51%|██████▌      | 74/146 [00:10<00:09,  7.21it/s]Epoch: 1, train for the 183-th batch, train loss: 0.5929039716720581:  77%|████████▍  | 182/237 [00:44<00:14,  3.91it/s]Epoch: 1, train for the 171-th batch, train loss: 0.4780336320400238:  45%|████▉      | 171/383 [00:42<00:56,  3.76it/s]Epoch: 1, train for the 183-th batch, train loss: 0.5929039716720581:  77%|████████▍  | 183/237 [00:44<00:13,  3.96it/s]evaluate for the 6-th batch, evaluate loss: 0.5203524827957153:  10%|██                  | 4/40 [00:00<00:02, 14.38it/s]evaluate for the 6-th batch, evaluate loss: 0.5203524827957153:  15%|███                 | 6/40 [00:00<00:02, 15.11it/s]evaluate for the 7-th batch, evaluate loss: 0.5254155397415161:  15%|███                 | 6/40 [00:00<00:02, 15.11it/s]Epoch: 2, train for the 75-th batch, train loss: 0.5334137678146362:  51%|██████▌      | 74/146 [00:10<00:09,  7.21it/s]Epoch: 2, train for the 75-th batch, train loss: 0.5334137678146362:  51%|██████▋      | 75/146 [00:10<00:10,  6.98it/s]Epoch: 1, train for the 211-th batch, train loss: 0.3900584280490875:  87%|█████████▌ | 210/241 [00:44<00:05,  5.93it/s]Epoch: 2, train for the 5-th batch, train loss: 0.6748997569084167:   3%|▍              | 4/151 [00:00<00:19,  7.65it/s]Epoch: 2, train for the 5-th batch, train loss: 0.6748997569084167:   3%|▍              | 5/151 [00:00<00:22,  6.51it/s]evaluate for the 8-th batch, evaluate loss: 0.5003117918968201:  15%|███                 | 6/40 [00:00<00:02, 15.11it/s]evaluate for the 8-th batch, evaluate loss: 0.5003117918968201:  20%|████                | 8/40 [00:00<00:02, 15.55it/s]Epoch: 1, train for the 211-th batch, train loss: 0.3900584280490875:  88%|█████████▋ | 211/241 [00:44<00:05,  5.31it/s]Epoch: 1, train for the 184-th batch, train loss: 0.5617883801460266:  77%|████████▍  | 183/237 [00:45<00:13,  3.96it/s]evaluate for the 9-th batch, evaluate loss: 0.5290707945823669:  20%|████                | 8/40 [00:00<00:02, 15.55it/s]Epoch: 1, train for the 184-th batch, train loss: 0.5617883801460266:  78%|████████▌  | 184/237 [00:45<00:12,  4.11it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5734553337097168:  51%|██████▋      | 75/146 [00:10<00:10,  6.98it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5734553337097168:  52%|██████▊      | 76/146 [00:10<00:09,  7.06it/s]evaluate for the 10-th batch, evaluate loss: 0.5689586400985718:  20%|███▊               | 8/40 [00:00<00:02, 15.55it/s]evaluate for the 10-th batch, evaluate loss: 0.5689586400985718:  25%|████▌             | 10/40 [00:00<00:01, 16.52it/s]Epoch: 2, train for the 6-th batch, train loss: 0.7067857384681702:   3%|▍              | 5/151 [00:00<00:22,  6.51it/s]Epoch: 2, train for the 6-th batch, train loss: 0.7067857384681702:   4%|▌              | 6/151 [00:00<00:21,  6.66it/s]evaluate for the 11-th batch, evaluate loss: 0.5201146602630615:  25%|████▌             | 10/40 [00:00<00:01, 16.52it/s]Epoch: 2, train for the 7-th batch, train loss: 0.6877369284629822:   4%|▌              | 6/151 [00:00<00:21,  6.66it/s]Epoch: 2, train for the 77-th batch, train loss: 0.6152286529541016:  52%|██████▊      | 76/146 [00:10<00:09,  7.06it/s]evaluate for the 12-th batch, evaluate loss: 0.5345136523246765:  25%|████▌             | 10/40 [00:00<00:01, 16.52it/s]evaluate for the 12-th batch, evaluate loss: 0.5345136523246765:  30%|█████▍            | 12/40 [00:00<00:01, 15.67it/s]Epoch: 2, train for the 77-th batch, train loss: 0.6152286529541016:  53%|██████▊      | 77/146 [00:10<00:09,  6.94it/s]Epoch: 1, train for the 212-th batch, train loss: 0.5037074089050293:  88%|█████████▋ | 211/241 [00:44<00:05,  5.31it/s]Epoch: 1, train for the 172-th batch, train loss: 0.3464149832725525:  45%|████▉      | 171/383 [00:43<00:56,  3.76it/s]Epoch: 1, train for the 212-th batch, train loss: 0.5037074089050293:  88%|█████████▋ | 212/241 [00:44<00:06,  4.50it/s]evaluate for the 13-th batch, evaluate loss: 0.5287290215492249:  30%|█████▍            | 12/40 [00:00<00:01, 15.67it/s]Epoch: 2, train for the 8-th batch, train loss: 0.6962659358978271:   4%|▌              | 6/151 [00:01<00:21,  6.66it/s]Epoch: 2, train for the 8-th batch, train loss: 0.6962659358978271:   5%|▊              | 8/151 [00:01<00:17,  7.96it/s]Epoch: 1, train for the 172-th batch, train loss: 0.3464149832725525:  45%|████▉      | 172/383 [00:43<01:12,  2.91it/s]Epoch: 2, train for the 78-th batch, train loss: 0.5656686425209045:  53%|██████▊      | 77/146 [00:10<00:09,  6.94it/s]evaluate for the 14-th batch, evaluate loss: 0.5144273042678833:  30%|█████▍            | 12/40 [00:00<00:01, 15.67it/s]evaluate for the 14-th batch, evaluate loss: 0.5144273042678833:  35%|██████▎           | 14/40 [00:00<00:01, 14.65it/s]Epoch: 2, train for the 78-th batch, train loss: 0.5656686425209045:  53%|██████▉      | 78/146 [00:10<00:09,  6.82it/s]Epoch: 1, train for the 185-th batch, train loss: 0.5545803308486938:  78%|████████▌  | 184/237 [00:45<00:12,  4.11it/s]Epoch: 2, train for the 9-th batch, train loss: 0.7094062566757202:   5%|▊              | 8/151 [00:01<00:17,  7.96it/s]Epoch: 1, train for the 185-th batch, train loss: 0.5545803308486938:  78%|████████▌  | 185/237 [00:45<00:14,  3.53it/s]evaluate for the 15-th batch, evaluate loss: 0.533208966255188:  35%|██████▋            | 14/40 [00:00<00:01, 14.65it/s]Epoch: 2, train for the 9-th batch, train loss: 0.7094062566757202:   6%|▉              | 9/151 [00:01<00:18,  7.71it/s]Epoch: 1, train for the 213-th batch, train loss: 0.5306148529052734:  88%|█████████▋ | 212/241 [00:45<00:06,  4.50it/s]Epoch: 1, train for the 213-th batch, train loss: 0.5306148529052734:  88%|█████████▋ | 213/241 [00:45<00:06,  4.59it/s]Epoch: 2, train for the 79-th batch, train loss: 0.5895265936851501:  53%|██████▉      | 78/146 [00:11<00:09,  6.82it/s]evaluate for the 16-th batch, evaluate loss: 0.5148674249649048:  35%|██████▎           | 14/40 [00:01<00:01, 14.65it/s]evaluate for the 16-th batch, evaluate loss: 0.5148674249649048:  40%|███████▏          | 16/40 [00:01<00:01, 14.50it/s]Epoch: 2, train for the 79-th batch, train loss: 0.5895265936851501:  54%|███████      | 79/146 [00:11<00:09,  6.89it/s]Epoch: 1, train for the 173-th batch, train loss: 0.46764224767684937:  45%|████▍     | 172/383 [00:43<01:12,  2.91it/s]Epoch: 2, train for the 10-th batch, train loss: 0.6708323359489441:   6%|▊             | 9/151 [00:01<00:18,  7.71it/s]Epoch: 2, train for the 10-th batch, train loss: 0.6708323359489441:   7%|▊            | 10/151 [00:01<00:18,  7.83it/s]Epoch: 1, train for the 173-th batch, train loss: 0.46764224767684937:  45%|████▌     | 173/383 [00:43<01:05,  3.20it/s]evaluate for the 17-th batch, evaluate loss: 0.5489605665206909:  40%|███████▏          | 16/40 [00:01<00:01, 14.50it/s]evaluate for the 18-th batch, evaluate loss: 0.5389527678489685:  40%|███████▏          | 16/40 [00:01<00:01, 14.50it/s]evaluate for the 18-th batch, evaluate loss: 0.5389527678489685:  45%|████████          | 18/40 [00:01<00:01, 14.38it/s]Epoch: 2, train for the 11-th batch, train loss: 0.6645218729972839:   7%|▊            | 10/151 [00:01<00:18,  7.83it/s]Epoch: 2, train for the 80-th batch, train loss: 0.5697998404502869:  54%|███████      | 79/146 [00:11<00:09,  6.89it/s]Epoch: 2, train for the 11-th batch, train loss: 0.6645218729972839:   7%|▉            | 11/151 [00:01<00:17,  8.11it/s]Epoch: 2, train for the 80-th batch, train loss: 0.5697998404502869:  55%|███████      | 80/146 [00:11<00:09,  6.72it/s]evaluate for the 19-th batch, evaluate loss: 0.5288803577423096:  45%|████████          | 18/40 [00:01<00:01, 14.38it/s]Epoch: 1, train for the 214-th batch, train loss: 0.4890478551387787:  88%|█████████▋ | 213/241 [00:45<00:06,  4.59it/s]Epoch: 1, train for the 214-th batch, train loss: 0.4890478551387787:  89%|█████████▊ | 214/241 [00:45<00:06,  4.34it/s]Epoch: 1, train for the 174-th batch, train loss: 0.4953295886516571:  45%|████▉      | 173/383 [00:43<01:05,  3.20it/s]evaluate for the 20-th batch, evaluate loss: 0.518967866897583:  45%|████████▌          | 18/40 [00:01<00:01, 14.38it/s]evaluate for the 20-th batch, evaluate loss: 0.518967866897583:  50%|█████████▌         | 20/40 [00:01<00:01, 14.70it/s]Epoch: 1, train for the 174-th batch, train loss: 0.4953295886516571:  45%|████▉      | 174/383 [00:43<01:00,  3.47it/s]Epoch: 2, train for the 12-th batch, train loss: 0.6842121481895447:   7%|▉            | 11/151 [00:01<00:17,  8.11it/s]Epoch: 1, train for the 186-th batch, train loss: 0.5271430611610413:  78%|████████▌  | 185/237 [00:45<00:14,  3.53it/s]Epoch: 2, train for the 12-th batch, train loss: 0.6842121481895447:   8%|█            | 12/151 [00:01<00:17,  7.94it/s]Epoch: 2, train for the 81-th batch, train loss: 0.5345857739448547:  55%|███████      | 80/146 [00:11<00:09,  6.72it/s]Epoch: 2, train for the 81-th batch, train loss: 0.5345857739448547:  55%|███████▏     | 81/146 [00:11<00:09,  6.67it/s]evaluate for the 21-th batch, evaluate loss: 0.5405011773109436:  50%|█████████         | 20/40 [00:01<00:01, 14.70it/s]Epoch: 1, train for the 186-th batch, train loss: 0.5271430611610413:  78%|████████▋  | 186/237 [00:45<00:16,  3.11it/s]Epoch: 1, train for the 215-th batch, train loss: 0.5102415084838867:  89%|█████████▊ | 214/241 [00:45<00:06,  4.34it/s]Epoch: 1, train for the 215-th batch, train loss: 0.5102415084838867:  89%|█████████▊ | 215/241 [00:45<00:05,  4.80it/s]evaluate for the 22-th batch, evaluate loss: 0.5020196437835693:  50%|█████████         | 20/40 [00:01<00:01, 14.70it/s]evaluate for the 22-th batch, evaluate loss: 0.5020196437835693:  55%|█████████▉        | 22/40 [00:01<00:01, 14.84it/s]Epoch: 2, train for the 13-th batch, train loss: 0.6832220554351807:   8%|█            | 12/151 [00:01<00:17,  7.94it/s]Epoch: 2, train for the 13-th batch, train loss: 0.6832220554351807:   9%|█            | 13/151 [00:01<00:17,  7.79it/s]Epoch: 2, train for the 82-th batch, train loss: 0.544971227645874:  55%|███████▊      | 81/146 [00:11<00:09,  6.67it/s]evaluate for the 23-th batch, evaluate loss: 0.4638969302177429:  55%|█████████▉        | 22/40 [00:01<00:01, 14.84it/s]Epoch: 2, train for the 82-th batch, train loss: 0.544971227645874:  56%|███████▊      | 82/146 [00:11<00:09,  6.61it/s]Epoch: 1, train for the 216-th batch, train loss: 0.5855144262313843:  89%|█████████▊ | 215/241 [00:45<00:05,  4.80it/s]Epoch: 1, train for the 175-th batch, train loss: 0.414377361536026:  45%|█████▍      | 174/383 [00:44<01:00,  3.47it/s]Epoch: 1, train for the 216-th batch, train loss: 0.5855144262313843:  90%|█████████▊ | 216/241 [00:45<00:04,  5.24it/s]Epoch: 2, train for the 14-th batch, train loss: 0.6921051144599915:   9%|█            | 13/151 [00:01<00:17,  7.79it/s]Epoch: 2, train for the 14-th batch, train loss: 0.6921051144599915:   9%|█▏           | 14/151 [00:01<00:18,  7.60it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5526943802833557:  56%|███████▎     | 82/146 [00:11<00:09,  6.61it/s]Epoch: 1, train for the 175-th batch, train loss: 0.414377361536026:  46%|█████▍      | 175/383 [00:44<01:00,  3.47it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5526943802833557:  57%|███████▍     | 83/146 [00:11<00:08,  7.22it/s]Epoch: 1, train for the 187-th batch, train loss: 0.5708672404289246:  78%|████████▋  | 186/237 [00:46<00:16,  3.11it/s]Epoch: 2, train for the 15-th batch, train loss: 0.7105401158332825:   9%|█▏           | 14/151 [00:01<00:18,  7.60it/s]Epoch: 2, train for the 15-th batch, train loss: 0.7105401158332825:  10%|█▎           | 15/151 [00:01<00:18,  7.44it/s]Epoch: 1, train for the 217-th batch, train loss: 0.5069820880889893:  90%|█████████▊ | 216/241 [00:45<00:04,  5.24it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5313555598258972:  57%|███████▍     | 83/146 [00:11<00:08,  7.22it/s]evaluate for the 24-th batch, evaluate loss: 0.512791097164154:  55%|██████████▍        | 22/40 [00:01<00:01, 14.84it/s]evaluate for the 24-th batch, evaluate loss: 0.512791097164154:  60%|███████████▍       | 24/40 [00:01<00:01, 10.51it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5313555598258972:  58%|███████▍     | 84/146 [00:11<00:08,  7.07it/s]Epoch: 1, train for the 187-th batch, train loss: 0.5708672404289246:  79%|████████▋  | 187/237 [00:46<00:17,  2.89it/s]Epoch: 1, train for the 217-th batch, train loss: 0.5069820880889893:  90%|█████████▉ | 217/241 [00:45<00:04,  5.21it/s]Epoch: 1, train for the 176-th batch, train loss: 0.4343128800392151:  46%|█████      | 175/383 [00:44<01:00,  3.47it/s]Epoch: 1, train for the 176-th batch, train loss: 0.4343128800392151:  46%|█████      | 176/383 [00:44<00:55,  3.71it/s]evaluate for the 25-th batch, evaluate loss: 0.5716477632522583:  60%|██████████▊       | 24/40 [00:01<00:01, 10.51it/s]Epoch: 2, train for the 16-th batch, train loss: 0.6836891174316406:  10%|█▎           | 15/151 [00:02<00:18,  7.44it/s]Epoch: 2, train for the 16-th batch, train loss: 0.6836891174316406:  11%|█▍           | 16/151 [00:02<00:17,  7.55it/s]Epoch: 2, train for the 85-th batch, train loss: 0.5232264399528503:  58%|███████▍     | 84/146 [00:11<00:08,  7.07it/s]Epoch: 2, train for the 85-th batch, train loss: 0.5232264399528503:  58%|███████▌     | 85/146 [00:11<00:08,  7.23it/s]Epoch: 1, train for the 218-th batch, train loss: 0.43540626764297485:  90%|█████████ | 217/241 [00:46<00:04,  5.21it/s]Epoch: 1, train for the 188-th batch, train loss: 0.5965394973754883:  79%|████████▋  | 187/237 [00:46<00:17,  2.89it/s]Epoch: 1, train for the 218-th batch, train loss: 0.43540626764297485:  90%|█████████ | 218/241 [00:46<00:04,  5.24it/s]Epoch: 2, train for the 17-th batch, train loss: 0.686825692653656:  11%|█▍            | 16/151 [00:02<00:17,  7.55it/s]Epoch: 2, train for the 17-th batch, train loss: 0.686825692653656:  11%|█▌            | 17/151 [00:02<00:17,  7.81it/s]Epoch: 1, train for the 188-th batch, train loss: 0.5965394973754883:  79%|████████▋  | 188/237 [00:46<00:15,  3.24it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5573253035545349:  58%|███████▌     | 85/146 [00:12<00:08,  7.23it/s]Epoch: 1, train for the 177-th batch, train loss: 0.43046098947525024:  46%|████▌     | 176/383 [00:44<00:55,  3.71it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5573253035545349:  59%|███████▋     | 86/146 [00:12<00:07,  7.65it/s]Epoch: 1, train for the 177-th batch, train loss: 0.43046098947525024:  46%|████▌     | 177/383 [00:44<00:52,  3.96it/s]Epoch: 2, train for the 87-th batch, train loss: 0.54954594373703:  59%|████████▊      | 86/146 [00:12<00:07,  7.65it/s]Epoch: 2, train for the 87-th batch, train loss: 0.54954594373703:  60%|████████▉      | 87/146 [00:12<00:07,  8.13it/s]Epoch: 1, train for the 189-th batch, train loss: 0.5929443836212158:  79%|████████▋  | 188/237 [00:46<00:15,  3.24it/s]Epoch: 1, train for the 219-th batch, train loss: 0.4824831187725067:  90%|█████████▉ | 218/241 [00:46<00:04,  5.24it/s]Epoch: 1, train for the 189-th batch, train loss: 0.5929443836212158:  80%|████████▊  | 189/237 [00:46<00:13,  3.66it/s]Epoch: 1, train for the 219-th batch, train loss: 0.4824831187725067:  91%|█████████▉ | 219/241 [00:46<00:04,  4.94it/s]evaluate for the 26-th batch, evaluate loss: 0.48242372274398804:  60%|██████████▏      | 24/40 [00:02<00:01, 10.51it/s]evaluate for the 26-th batch, evaluate loss: 0.48242372274398804:  65%|███████████      | 26/40 [00:02<00:01,  7.54it/s]Epoch: 1, train for the 178-th batch, train loss: 0.5184969305992126:  46%|█████      | 177/383 [00:44<00:52,  3.96it/s]Epoch: 2, train for the 18-th batch, train loss: 0.6801897287368774:  11%|█▍           | 17/151 [00:02<00:17,  7.81it/s]Epoch: 2, train for the 18-th batch, train loss: 0.6801897287368774:  12%|█▌           | 18/151 [00:02<00:21,  6.23it/s]Epoch: 2, train for the 88-th batch, train loss: 0.570324718952179:  60%|████████▎     | 87/146 [00:12<00:07,  8.13it/s]Epoch: 2, train for the 88-th batch, train loss: 0.570324718952179:  60%|████████▍     | 88/146 [00:12<00:07,  7.89it/s]evaluate for the 27-th batch, evaluate loss: 0.5006499290466309:  65%|███████████▋      | 26/40 [00:02<00:01,  7.54it/s]Epoch: 1, train for the 178-th batch, train loss: 0.5184969305992126:  46%|█████      | 178/383 [00:44<00:49,  4.12it/s]evaluate for the 28-th batch, evaluate loss: 0.4847092032432556:  65%|███████████▋      | 26/40 [00:02<00:01,  7.54it/s]evaluate for the 28-th batch, evaluate loss: 0.4847092032432556:  70%|████████████▌     | 28/40 [00:02<00:01,  8.96it/s]Epoch: 1, train for the 220-th batch, train loss: 0.573455274105072:  91%|██████████▉ | 219/241 [00:46<00:04,  4.94it/s]Epoch: 1, train for the 220-th batch, train loss: 0.573455274105072:  91%|██████████▉ | 220/241 [00:46<00:04,  5.19it/s]Epoch: 2, train for the 19-th batch, train loss: 0.6794077754020691:  12%|█▌           | 18/151 [00:02<00:21,  6.23it/s]Epoch: 1, train for the 190-th batch, train loss: 0.5351415872573853:  80%|████████▊  | 189/237 [00:46<00:13,  3.66it/s]Epoch: 2, train for the 19-th batch, train loss: 0.6794077754020691:  13%|█▋           | 19/151 [00:02<00:20,  6.37it/s]Epoch: 2, train for the 89-th batch, train loss: 0.511876106262207:  60%|████████▍     | 88/146 [00:12<00:07,  7.89it/s]evaluate for the 29-th batch, evaluate loss: 0.5086449384689331:  70%|████████████▌     | 28/40 [00:02<00:01,  8.96it/s]Epoch: 2, train for the 89-th batch, train loss: 0.511876106262207:  61%|████████▌     | 89/146 [00:12<00:07,  7.49it/s]Epoch: 1, train for the 190-th batch, train loss: 0.5351415872573853:  80%|████████▊  | 190/237 [00:46<00:12,  3.82it/s]Epoch: 1, train for the 179-th batch, train loss: 0.4268099069595337:  46%|█████      | 178/383 [00:45<00:49,  4.12it/s]Epoch: 2, train for the 90-th batch, train loss: 0.5383849143981934:  61%|███████▉     | 89/146 [00:12<00:07,  7.49it/s]Epoch: 1, train for the 179-th batch, train loss: 0.4268099069595337:  47%|█████▏     | 179/383 [00:45<00:49,  4.16it/s]Epoch: 2, train for the 90-th batch, train loss: 0.5383849143981934:  62%|████████     | 90/146 [00:12<00:07,  7.88it/s]Epoch: 1, train for the 221-th batch, train loss: 0.5583108067512512:  91%|██████████ | 220/241 [00:46<00:04,  5.19it/s]Epoch: 2, train for the 20-th batch, train loss: 0.6607031226158142:  13%|█▋           | 19/151 [00:02<00:20,  6.37it/s]Epoch: 2, train for the 20-th batch, train loss: 0.6607031226158142:  13%|█▋           | 20/151 [00:02<00:21,  5.99it/s]evaluate for the 30-th batch, evaluate loss: 0.5119838714599609:  70%|████████████▌     | 28/40 [00:02<00:01,  8.96it/s]evaluate for the 30-th batch, evaluate loss: 0.5119838714599609:  75%|█████████████▌    | 30/40 [00:02<00:01,  8.68it/s]Epoch: 1, train for the 221-th batch, train loss: 0.5583108067512512:  92%|██████████ | 221/241 [00:46<00:03,  5.02it/s]Epoch: 2, train for the 91-th batch, train loss: 0.5555600523948669:  62%|████████     | 90/146 [00:12<00:07,  7.88it/s]Epoch: 2, train for the 91-th batch, train loss: 0.5555600523948669:  62%|████████     | 91/146 [00:12<00:06,  7.90it/s]evaluate for the 31-th batch, evaluate loss: 0.4924524128437042:  75%|█████████████▌    | 30/40 [00:02<00:01,  8.68it/s]Epoch: 1, train for the 191-th batch, train loss: 0.5496093034744263:  80%|████████▊  | 190/237 [00:47<00:12,  3.82it/s]Epoch: 2, train for the 21-th batch, train loss: 0.6681881546974182:  13%|█▋           | 20/151 [00:02<00:21,  5.99it/s]evaluate for the 32-th batch, evaluate loss: 0.523730456829071:  75%|██████████████▎    | 30/40 [00:02<00:01,  8.68it/s]evaluate for the 32-th batch, evaluate loss: 0.523730456829071:  80%|███████████████▏   | 32/40 [00:02<00:00,  9.95it/s]Epoch: 2, train for the 21-th batch, train loss: 0.6681881546974182:  14%|█▊           | 21/151 [00:02<00:20,  6.27it/s]Epoch: 1, train for the 191-th batch, train loss: 0.5496093034744263:  81%|████████▊  | 191/237 [00:47<00:12,  3.65it/s]Epoch: 1, train for the 222-th batch, train loss: 0.494455486536026:  92%|███████████ | 221/241 [00:46<00:03,  5.02it/s]Epoch: 1, train for the 180-th batch, train loss: 0.5059813261032104:  47%|█████▏     | 179/383 [00:45<00:49,  4.16it/s]Epoch: 1, train for the 222-th batch, train loss: 0.494455486536026:  92%|███████████ | 222/241 [00:46<00:03,  5.17it/s]Epoch: 1, train for the 180-th batch, train loss: 0.5059813261032104:  47%|█████▏     | 180/383 [00:45<00:50,  3.99it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5208290815353394:  62%|████████     | 91/146 [00:12<00:06,  7.90it/s]evaluate for the 33-th batch, evaluate loss: 0.5036048889160156:  80%|██████████████▍   | 32/40 [00:02<00:00,  9.95it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5208290815353394:  63%|████████▏    | 92/146 [00:12<00:07,  7.33it/s]Epoch: 2, train for the 22-th batch, train loss: 0.642177939414978:  14%|█▉            | 21/151 [00:03<00:20,  6.27it/s]Epoch: 2, train for the 22-th batch, train loss: 0.642177939414978:  15%|██            | 22/151 [00:03<00:20,  6.18it/s]Epoch: 1, train for the 223-th batch, train loss: 0.5001270771026611:  92%|██████████▏| 222/241 [00:46<00:03,  5.17it/s]Epoch: 2, train for the 93-th batch, train loss: 0.5506223440170288:  63%|████████▏    | 92/146 [00:12<00:07,  7.33it/s]Epoch: 2, train for the 93-th batch, train loss: 0.5506223440170288:  64%|████████▎    | 93/146 [00:13<00:06,  7.59it/s]Epoch: 1, train for the 223-th batch, train loss: 0.5001270771026611:  93%|██████████▏| 223/241 [00:47<00:03,  5.39it/s]Epoch: 1, train for the 181-th batch, train loss: 0.416147917509079:  47%|█████▋      | 180/383 [00:45<00:50,  3.99it/s]Epoch: 2, train for the 23-th batch, train loss: 0.6532241702079773:  15%|█▉           | 22/151 [00:03<00:20,  6.18it/s]Epoch: 1, train for the 181-th batch, train loss: 0.416147917509079:  47%|█████▋      | 181/383 [00:45<00:48,  4.18it/s]Epoch: 2, train for the 23-th batch, train loss: 0.6532241702079773:  15%|█▉           | 23/151 [00:03<00:18,  6.74it/s]evaluate for the 34-th batch, evaluate loss: 0.5721049904823303:  80%|██████████████▍   | 32/40 [00:03<00:00,  9.95it/s]evaluate for the 34-th batch, evaluate loss: 0.5721049904823303:  85%|███████████████▎  | 34/40 [00:03<00:00,  8.21it/s]Epoch: 2, train for the 94-th batch, train loss: 0.5234575271606445:  64%|████████▎    | 93/146 [00:13<00:06,  7.59it/s]Epoch: 2, train for the 94-th batch, train loss: 0.5234575271606445:  64%|████████▎    | 94/146 [00:13<00:06,  7.46it/s]Epoch: 1, train for the 192-th batch, train loss: 0.5771214365959167:  81%|████████▊  | 191/237 [00:47<00:12,  3.65it/s]evaluate for the 35-th batch, evaluate loss: 0.5343067049980164:  85%|███████████████▎  | 34/40 [00:03<00:00,  8.21it/s]Epoch: 1, train for the 192-th batch, train loss: 0.5771214365959167:  81%|████████▉  | 192/237 [00:47<00:14,  3.15it/s]Epoch: 2, train for the 24-th batch, train loss: 0.646979033946991:  15%|██▏           | 23/151 [00:03<00:18,  6.74it/s]Epoch: 1, train for the 224-th batch, train loss: 0.5105549097061157:  93%|██████████▏| 223/241 [00:47<00:03,  5.39it/s]Epoch: 2, train for the 24-th batch, train loss: 0.646979033946991:  16%|██▏           | 24/151 [00:03<00:19,  6.62it/s]Epoch: 1, train for the 224-th batch, train loss: 0.5105549097061157:  93%|██████████▏| 224/241 [00:47<00:03,  4.92it/s]evaluate for the 36-th batch, evaluate loss: 0.5230780839920044:  85%|███████████████▎  | 34/40 [00:03<00:00,  8.21it/s]evaluate for the 36-th batch, evaluate loss: 0.5230780839920044:  90%|████████████████▏ | 36/40 [00:03<00:00,  9.31it/s]Epoch: 2, train for the 95-th batch, train loss: 0.519199788570404:  64%|█████████     | 94/146 [00:13<00:06,  7.46it/s]Epoch: 2, train for the 95-th batch, train loss: 0.519199788570404:  65%|█████████     | 95/146 [00:13<00:07,  7.06it/s]evaluate for the 37-th batch, evaluate loss: 0.5517300963401794:  90%|████████████████▏ | 36/40 [00:03<00:00,  9.31it/s]Epoch: 1, train for the 182-th batch, train loss: 0.5250232815742493:  47%|█████▏     | 181/383 [00:45<00:48,  4.18it/s]Epoch: 2, train for the 96-th batch, train loss: 0.5578497052192688:  65%|████████▍    | 95/146 [00:13<00:07,  7.06it/s]Epoch: 2, train for the 96-th batch, train loss: 0.5578497052192688:  66%|████████▌    | 96/146 [00:13<00:06,  7.57it/s]Epoch: 1, train for the 182-th batch, train loss: 0.5250232815742493:  48%|█████▏     | 182/383 [00:45<00:54,  3.69it/s]Epoch: 1, train for the 193-th batch, train loss: 0.5822269320487976:  81%|████████▉  | 192/237 [00:47<00:14,  3.15it/s]Epoch: 2, train for the 25-th batch, train loss: 0.6752147674560547:  16%|██           | 24/151 [00:03<00:19,  6.62it/s]Epoch: 1, train for the 225-th batch, train loss: 0.5413540601730347:  93%|██████████▏| 224/241 [00:47<00:03,  4.92it/s]Epoch: 2, train for the 25-th batch, train loss: 0.6752147674560547:  17%|██▏          | 25/151 [00:03<00:21,  5.96it/s]Epoch: 1, train for the 225-th batch, train loss: 0.5413540601730347:  93%|██████████▎| 225/241 [00:47<00:03,  4.88it/s]Epoch: 1, train for the 193-th batch, train loss: 0.5822269320487976:  81%|████████▉  | 193/237 [00:47<00:13,  3.37it/s]evaluate for the 38-th batch, evaluate loss: 0.5832729935646057:  90%|████████████████▏ | 36/40 [00:03<00:00,  9.31it/s]evaluate for the 38-th batch, evaluate loss: 0.5832729935646057:  95%|█████████████████ | 38/40 [00:03<00:00,  8.63it/s]Epoch: 1, train for the 183-th batch, train loss: 0.4140101969242096:  48%|█████▏     | 182/383 [00:46<00:54,  3.69it/s]Epoch: 2, train for the 26-th batch, train loss: 0.6558927893638611:  17%|██▏          | 25/151 [00:03<00:21,  5.96it/s]Epoch: 2, train for the 26-th batch, train loss: 0.6558927893638611:  17%|██▏          | 26/151 [00:03<00:19,  6.29it/s]Epoch: 1, train for the 183-th batch, train loss: 0.4140101969242096:  48%|█████▎     | 183/383 [00:46<00:49,  4.07it/s]evaluate for the 39-th batch, evaluate loss: 0.5696980953216553:  95%|█████████████████ | 38/40 [00:03<00:00,  8.63it/s]Epoch: 2, train for the 97-th batch, train loss: 0.5160313844680786:  66%|████████▌    | 96/146 [00:13<00:06,  7.57it/s]Epoch: 2, train for the 97-th batch, train loss: 0.5160313844680786:  66%|████████▋    | 97/146 [00:13<00:07,  6.18it/s]Epoch: 1, train for the 226-th batch, train loss: 0.5487245917320251:  93%|██████████▎| 225/241 [00:47<00:03,  4.88it/s]evaluate for the 40-th batch, evaluate loss: 0.6687790155410767:  95%|█████████████████ | 38/40 [00:03<00:00,  8.63it/s]evaluate for the 40-th batch, evaluate loss: 0.6687790155410767: 100%|██████████████████| 40/40 [00:03<00:00,  9.95it/s]evaluate for the 40-th batch, evaluate loss: 0.6687790155410767: 100%|██████████████████| 40/40 [00:03<00:00, 11.05it/s]
Epoch: 1, train for the 226-th batch, train loss: 0.5487245917320251:  94%|██████████▎| 226/241 [00:47<00:03,  4.74it/s]Epoch: 2, train for the 27-th batch, train loss: 0.6702093482017517:  17%|██▏          | 26/151 [00:03<00:19,  6.29it/s]Epoch: 2, train for the 27-th batch, train loss: 0.6702093482017517:  18%|██▎          | 27/151 [00:03<00:18,  6.84it/s]Epoch: 1, train for the 194-th batch, train loss: 0.5546745657920837:  81%|████████▉  | 193/237 [00:48<00:13,  3.37it/s]Epoch: 1, train for the 194-th batch, train loss: 0.5546745657920837:  82%|█████████  | 194/237 [00:48<00:13,  3.28it/s]Epoch: 1, train for the 184-th batch, train loss: 0.42520540952682495:  48%|████▊     | 183/383 [00:46<00:49,  4.07it/s]  0%|                                                                                            | 0/21 [00:00<?, ?it/s]Epoch: 2, train for the 98-th batch, train loss: 0.5058899521827698:  66%|████████▋    | 97/146 [00:13<00:07,  6.18it/s]Epoch: 2, train for the 98-th batch, train loss: 0.5058899521827698:  67%|████████▋    | 98/146 [00:13<00:08,  5.95it/s]Epoch: 1, train for the 227-th batch, train loss: 0.5783560276031494:  94%|██████████▎| 226/241 [00:47<00:03,  4.74it/s]Epoch: 2, train for the 28-th batch, train loss: 0.6549530625343323:  18%|██▎          | 27/151 [00:03<00:18,  6.84it/s]Epoch: 1, train for the 184-th batch, train loss: 0.42520540952682495:  48%|████▊     | 184/383 [00:46<00:49,  4.02it/s]Epoch: 1, train for the 227-th batch, train loss: 0.5783560276031494:  94%|██████████▎| 227/241 [00:47<00:02,  4.91it/s]evaluate for the 1-th batch, evaluate loss: 0.6462143063545227:   0%|                            | 0/21 [00:00<?, ?it/s]Epoch: 2, train for the 28-th batch, train loss: 0.6549530625343323:  19%|██▍          | 28/151 [00:04<00:19,  6.41it/s]Epoch: 2, train for the 99-th batch, train loss: 0.5563065409660339:  67%|████████▋    | 98/146 [00:13<00:08,  5.95it/s]evaluate for the 2-th batch, evaluate loss: 0.7224066257476807:   0%|                            | 0/21 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.7224066257476807:  10%|█▉                  | 2/21 [00:00<00:01, 12.77it/s]Epoch: 2, train for the 99-th batch, train loss: 0.5563065409660339:  68%|████████▊    | 99/146 [00:13<00:07,  6.15it/s]Epoch: 2, train for the 29-th batch, train loss: 0.667041540145874:  19%|██▌           | 28/151 [00:04<00:19,  6.41it/s]Epoch: 2, train for the 29-th batch, train loss: 0.667041540145874:  19%|██▋           | 29/151 [00:04<00:18,  6.56it/s]evaluate for the 3-th batch, evaluate loss: 0.707548201084137:  10%|██                   | 2/21 [00:00<00:01, 12.77it/s]Epoch: 1, train for the 185-th batch, train loss: 0.31399616599082947:  48%|████▊     | 184/383 [00:46<00:49,  4.02it/s]Epoch: 1, train for the 185-th batch, train loss: 0.31399616599082947:  48%|████▊     | 185/383 [00:46<00:47,  4.17it/s]evaluate for the 4-th batch, evaluate loss: 0.6428334712982178:  10%|█▉                  | 2/21 [00:00<00:01, 12.77it/s]evaluate for the 4-th batch, evaluate loss: 0.6428334712982178:  19%|███▊                | 4/21 [00:00<00:01, 14.35it/s]Epoch: 1, train for the 195-th batch, train loss: 0.5538004040718079:  82%|█████████  | 194/237 [00:48<00:13,  3.28it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5803607702255249:  68%|████████▏   | 99/146 [00:14<00:07,  6.15it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5803607702255249:  68%|███████▌   | 100/146 [00:14<00:07,  6.24it/s]Epoch: 1, train for the 195-th batch, train loss: 0.5538004040718079:  82%|█████████  | 195/237 [00:48<00:13,  3.15it/s]Epoch: 1, train for the 228-th batch, train loss: 0.5276396870613098:  94%|██████████▎| 227/241 [00:48<00:02,  4.91it/s]evaluate for the 5-th batch, evaluate loss: 0.7243524193763733:  19%|███▊                | 4/21 [00:00<00:01, 14.35it/s]Epoch: 2, train for the 30-th batch, train loss: 0.6446875333786011:  19%|██▍          | 29/151 [00:04<00:18,  6.56it/s]Epoch: 1, train for the 228-th batch, train loss: 0.5276396870613098:  95%|██████████▍| 228/241 [00:48<00:03,  4.28it/s]Epoch: 2, train for the 30-th batch, train loss: 0.6446875333786011:  20%|██▌          | 30/151 [00:04<00:18,  6.53it/s]evaluate for the 6-th batch, evaluate loss: 0.7185804843902588:  19%|███▊                | 4/21 [00:00<00:01, 14.35it/s]evaluate for the 6-th batch, evaluate loss: 0.7185804843902588:  29%|█████▋              | 6/21 [00:00<00:01, 13.26it/s]Epoch: 2, train for the 101-th batch, train loss: 0.49983251094818115:  68%|██████▊   | 100/146 [00:14<00:07,  6.24it/s]Epoch: 2, train for the 101-th batch, train loss: 0.49983251094818115:  69%|██████▉   | 101/146 [00:14<00:07,  6.05it/s]evaluate for the 7-th batch, evaluate loss: 0.6662619113922119:  29%|█████▋              | 6/21 [00:00<00:01, 13.26it/s]Epoch: 2, train for the 31-th batch, train loss: 0.641277551651001:  20%|██▊           | 30/151 [00:04<00:18,  6.53it/s]Epoch: 2, train for the 31-th batch, train loss: 0.641277551651001:  21%|██▊           | 31/151 [00:04<00:19,  6.30it/s]Epoch: 1, train for the 229-th batch, train loss: 0.5334540605545044:  95%|██████████▍| 228/241 [00:48<00:03,  4.28it/s]Epoch: 1, train for the 186-th batch, train loss: 0.40871796011924744:  48%|████▊     | 185/383 [00:46<00:47,  4.17it/s]evaluate for the 8-th batch, evaluate loss: 0.7062931060791016:  29%|█████▋              | 6/21 [00:00<00:01, 13.26it/s]evaluate for the 8-th batch, evaluate loss: 0.7062931060791016:  38%|███████▌            | 8/21 [00:00<00:00, 14.87it/s]Epoch: 1, train for the 229-th batch, train loss: 0.5334540605545044:  95%|██████████▍| 229/241 [00:48<00:02,  4.44it/s]Epoch: 1, train for the 186-th batch, train loss: 0.40871796011924744:  49%|████▊     | 186/383 [00:46<00:52,  3.76it/s]Epoch: 2, train for the 102-th batch, train loss: 0.5378231406211853:  69%|███████▌   | 101/146 [00:14<00:07,  6.05it/s]Epoch: 2, train for the 102-th batch, train loss: 0.5378231406211853:  70%|███████▋   | 102/146 [00:14<00:06,  6.54it/s]evaluate for the 9-th batch, evaluate loss: 0.7086637616157532:  38%|███████▌            | 8/21 [00:00<00:00, 14.87it/s]Epoch: 2, train for the 32-th batch, train loss: 0.6720320582389832:  21%|██▋          | 31/151 [00:04<00:19,  6.30it/s]Epoch: 2, train for the 32-th batch, train loss: 0.6720320582389832:  21%|██▊          | 32/151 [00:04<00:18,  6.42it/s]evaluate for the 10-th batch, evaluate loss: 0.718591570854187:  38%|███████▌            | 8/21 [00:00<00:00, 14.87it/s]evaluate for the 10-th batch, evaluate loss: 0.718591570854187:  48%|█████████          | 10/21 [00:00<00:00, 15.37it/s]Epoch: 2, train for the 103-th batch, train loss: 0.534458577632904:  70%|████████▍   | 102/146 [00:14<00:06,  6.54it/s]evaluate for the 11-th batch, evaluate loss: 0.7106266617774963:  48%|████████▌         | 10/21 [00:00<00:00, 15.37it/s]Epoch: 2, train for the 103-th batch, train loss: 0.534458577632904:  71%|████████▍   | 103/146 [00:14<00:06,  6.49it/s]Epoch: 1, train for the 230-th batch, train loss: 0.49976491928100586:  95%|█████████▌| 229/241 [00:48<00:02,  4.44it/s]Epoch: 1, train for the 196-th batch, train loss: 0.5610881447792053:  82%|█████████  | 195/237 [00:49<00:13,  3.15it/s]Epoch: 1, train for the 230-th batch, train loss: 0.49976491928100586:  95%|█████████▌| 230/241 [00:48<00:02,  4.38it/s]Epoch: 2, train for the 33-th batch, train loss: 0.6425572037696838:  21%|██▊          | 32/151 [00:04<00:18,  6.42it/s]Epoch: 1, train for the 187-th batch, train loss: 0.35348930954933167:  49%|████▊     | 186/383 [00:47<00:52,  3.76it/s]Epoch: 2, train for the 33-th batch, train loss: 0.6425572037696838:  22%|██▊          | 33/151 [00:04<00:17,  6.63it/s]Epoch: 1, train for the 196-th batch, train loss: 0.5610881447792053:  83%|█████████  | 196/237 [00:49<00:15,  2.69it/s]evaluate for the 12-th batch, evaluate loss: 0.691292405128479:  48%|█████████          | 10/21 [00:00<00:00, 15.37it/s]evaluate for the 12-th batch, evaluate loss: 0.691292405128479:  57%|██████████▊        | 12/21 [00:00<00:00, 14.13it/s]Epoch: 1, train for the 187-th batch, train loss: 0.35348930954933167:  49%|████▉     | 187/383 [00:47<00:53,  3.66it/s]Epoch: 2, train for the 104-th batch, train loss: 0.5155928134918213:  71%|███████▊   | 103/146 [00:14<00:06,  6.49it/s]evaluate for the 13-th batch, evaluate loss: 0.6838787198066711:  57%|██████████▎       | 12/21 [00:00<00:00, 14.13it/s]Epoch: 2, train for the 104-th batch, train loss: 0.5155928134918213:  71%|███████▊   | 104/146 [00:14<00:06,  6.52it/s]Epoch: 2, train for the 34-th batch, train loss: 0.6396344900131226:  22%|██▊          | 33/151 [00:04<00:17,  6.63it/s]Epoch: 2, train for the 34-th batch, train loss: 0.6396344900131226:  23%|██▉          | 34/151 [00:04<00:17,  6.64it/s]Epoch: 1, train for the 197-th batch, train loss: 0.5298115611076355:  83%|█████████  | 196/237 [00:49<00:15,  2.69it/s]Epoch: 2, train for the 105-th batch, train loss: 0.5326889157295227:  71%|███████▊   | 104/146 [00:14<00:06,  6.52it/s]Epoch: 2, train for the 105-th batch, train loss: 0.5326889157295227:  72%|███████▉   | 105/146 [00:14<00:05,  6.95it/s]Epoch: 1, train for the 197-th batch, train loss: 0.5298115611076355:  83%|█████████▏ | 197/237 [00:49<00:13,  3.06it/s]Epoch: 1, train for the 231-th batch, train loss: 0.5492992401123047:  95%|██████████▍| 230/241 [00:48<00:02,  4.38it/s]Epoch: 1, train for the 231-th batch, train loss: 0.5492992401123047:  96%|██████████▌| 231/241 [00:48<00:02,  4.11it/s]Epoch: 2, train for the 35-th batch, train loss: 0.635875940322876:  23%|███▏          | 34/151 [00:05<00:17,  6.64it/s]Epoch: 1, train for the 188-th batch, train loss: 0.4863692820072174:  49%|█████▎     | 187/383 [00:47<00:53,  3.66it/s]Epoch: 2, train for the 35-th batch, train loss: 0.635875940322876:  23%|███▏          | 35/151 [00:05<00:17,  6.71it/s]evaluate for the 14-th batch, evaluate loss: 0.6630352735519409:  57%|██████████▎       | 12/21 [00:01<00:00, 14.13it/s]evaluate for the 14-th batch, evaluate loss: 0.6630352735519409:  67%|████████████      | 14/21 [00:01<00:00, 10.33it/s]Epoch: 1, train for the 188-th batch, train loss: 0.4863692820072174:  49%|█████▍     | 188/383 [00:47<00:53,  3.65it/s]Epoch: 2, train for the 106-th batch, train loss: 0.5153672695159912:  72%|███████▉   | 105/146 [00:14<00:05,  6.95it/s]Epoch: 2, train for the 106-th batch, train loss: 0.5153672695159912:  73%|███████▉   | 106/146 [00:15<00:05,  7.09it/s]evaluate for the 15-th batch, evaluate loss: 0.7029443383216858:  67%|████████████      | 14/21 [00:01<00:00, 10.33it/s]Epoch: 1, train for the 198-th batch, train loss: 0.5191940665245056:  83%|█████████▏ | 197/237 [00:49<00:13,  3.06it/s]Epoch: 1, train for the 232-th batch, train loss: 0.5672383904457092:  96%|██████████▌| 231/241 [00:49<00:02,  4.11it/s]Epoch: 2, train for the 36-th batch, train loss: 0.6490918397903442:  23%|███          | 35/151 [00:05<00:17,  6.71it/s]Epoch: 2, train for the 36-th batch, train loss: 0.6490918397903442:  24%|███          | 36/151 [00:05<00:17,  6.61it/s]Epoch: 1, train for the 232-th batch, train loss: 0.5672383904457092:  96%|██████████▌| 232/241 [00:49<00:02,  4.34it/s]Epoch: 1, train for the 198-th batch, train loss: 0.5191940665245056:  84%|█████████▏ | 198/237 [00:49<00:11,  3.33it/s]evaluate for the 16-th batch, evaluate loss: 0.6830068230628967:  67%|████████████      | 14/21 [00:01<00:00, 10.33it/s]evaluate for the 16-th batch, evaluate loss: 0.6830068230628967:  76%|█████████████▋    | 16/21 [00:01<00:00, 11.08it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5556888580322266:  73%|███████▉   | 106/146 [00:15<00:05,  7.09it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5556888580322266:  73%|████████   | 107/146 [00:15<00:05,  7.12it/s]Epoch: 1, train for the 189-th batch, train loss: 0.4685613214969635:  49%|█████▍     | 188/383 [00:47<00:53,  3.65it/s]evaluate for the 17-th batch, evaluate loss: 0.6183031797409058:  76%|█████████████▋    | 16/21 [00:01<00:00, 11.08it/s]Epoch: 1, train for the 189-th batch, train loss: 0.4685613214969635:  49%|█████▍     | 189/383 [00:47<00:50,  3.82it/s]evaluate for the 18-th batch, evaluate loss: 0.6650334000587463:  76%|█████████████▋    | 16/21 [00:01<00:00, 11.08it/s]evaluate for the 18-th batch, evaluate loss: 0.6650334000587463:  86%|███████████████▍  | 18/21 [00:01<00:00, 12.24it/s]Epoch: 1, train for the 233-th batch, train loss: 0.5141889452934265:  96%|██████████▌| 232/241 [00:49<00:02,  4.34it/s]Epoch: 2, train for the 37-th batch, train loss: 0.6634983420372009:  24%|███          | 36/151 [00:05<00:17,  6.61it/s]Epoch: 2, train for the 37-th batch, train loss: 0.6634983420372009:  25%|███▏         | 37/151 [00:05<00:19,  5.93it/s]Epoch: 2, train for the 108-th batch, train loss: 0.523688793182373:  73%|████████▊   | 107/146 [00:15<00:05,  7.12it/s]Epoch: 1, train for the 233-th batch, train loss: 0.5141889452934265:  97%|██████████▋| 233/241 [00:49<00:01,  4.43it/s]evaluate for the 19-th batch, evaluate loss: 0.6904818415641785:  86%|███████████████▍  | 18/21 [00:01<00:00, 12.24it/s]Epoch: 2, train for the 108-th batch, train loss: 0.523688793182373:  74%|████████▉   | 108/146 [00:15<00:05,  6.60it/s]Epoch: 1, train for the 190-th batch, train loss: 0.5035965442657471:  49%|█████▍     | 189/383 [00:47<00:50,  3.82it/s]evaluate for the 20-th batch, evaluate loss: 0.691872239112854:  86%|████████████████▎  | 18/21 [00:01<00:00, 12.24it/s]evaluate for the 20-th batch, evaluate loss: 0.691872239112854:  95%|██████████████████ | 20/21 [00:01<00:00, 12.90it/s]Epoch: 1, train for the 199-th batch, train loss: 0.5209378004074097:  84%|█████████▏ | 198/237 [00:49<00:11,  3.33it/s]Epoch: 1, train for the 190-th batch, train loss: 0.5035965442657471:  50%|█████▍     | 190/383 [00:47<00:47,  4.10it/s]evaluate for the 21-th batch, evaluate loss: 0.8491703867912292:  95%|█████████████████▏| 20/21 [00:01<00:00, 12.90it/s]evaluate for the 21-th batch, evaluate loss: 0.8491703867912292: 100%|██████████████████| 21/21 [00:01<00:00, 13.12it/s]
Epoch: 1, train for the 199-th batch, train loss: 0.5209378004074097:  84%|█████████▏ | 199/237 [00:49<00:11,  3.19it/s]Epoch: 2, train for the 38-th batch, train loss: 0.6496607661247253:  25%|███▏         | 37/151 [00:05<00:19,  5.93it/s]Epoch: 2, train for the 38-th batch, train loss: 0.6496607661247253:  25%|███▎         | 38/151 [00:05<00:19,  5.94it/s]Epoch: 1, train for the 234-th batch, train loss: 0.577099621295929:  97%|███████████▌| 233/241 [00:49<00:01,  4.43it/s]Epoch: 1, train for the 234-th batch, train loss: 0.577099621295929:  97%|███████████▋| 234/241 [00:49<00:01,  4.68it/s]Epoch: 2, train for the 109-th batch, train loss: 0.5734155178070068:  74%|████████▏  | 108/146 [00:15<00:05,  6.60it/s]Epoch: 2, train for the 109-th batch, train loss: 0.5734155178070068:  75%|████████▏  | 109/146 [00:15<00:06,  6.02it/s]INFO:root:Epoch: 2, learning rate: 0.0001, train loss: 0.5208
INFO:root:train average_precision, 0.8305
INFO:root:train roc_auc, 0.8234
INFO:root:validate loss: 0.5286
INFO:root:validate average_precision, 0.8329
INFO:root:validate roc_auc, 0.8208
INFO:root:new node validate loss: 0.6958
INFO:root:new node validate first_1_average_precision, 0.7068
INFO:root:new node validate first_1_roc_auc, 0.6877
INFO:root:new node validate first_3_average_precision, 0.7163
INFO:root:new node validate first_3_roc_auc, 0.7062
INFO:root:new node validate first_10_average_precision, 0.7105
INFO:root:new node validate first_10_roc_auc, 0.7108
INFO:root:new node validate average_precision, 0.6801
INFO:root:new node validate roc_auc, 0.6859
INFO:root:save model ./saved_models/TGN/ia-reality-call/TGN_seed0_dummy-e41ajx4p/TGN_seed0_dummy-e41ajx4p.pkl
Epoch: 2, train for the 39-th batch, train loss: 0.6258552670478821:  25%|███▎         | 38/151 [00:05<00:19,  5.94it/s]Epoch: 1, train for the 191-th batch, train loss: 0.41240569949150085:  50%|████▉     | 190/383 [00:48<00:47,  4.10it/s]Epoch: 2, train for the 39-th batch, train loss: 0.6258552670478821:  26%|███▎         | 39/151 [00:05<00:18,  6.14it/s]Epoch: 1, train for the 235-th batch, train loss: 0.6072200536727905:  97%|██████████▋| 234/241 [00:49<00:01,  4.68it/s]Epoch: 2, train for the 110-th batch, train loss: 0.4761500358581543:  75%|████████▏  | 109/146 [00:15<00:06,  6.02it/s]Epoch: 2, train for the 110-th batch, train loss: 0.4761500358581543:  75%|████████▎  | 110/146 [00:15<00:05,  6.39it/s]Epoch: 1, train for the 191-th batch, train loss: 0.41240569949150085:  50%|████▉     | 191/383 [00:48<00:46,  4.14it/s]Epoch: 1, train for the 235-th batch, train loss: 0.6072200536727905:  98%|██████████▋| 235/241 [00:49<00:01,  5.06it/s]Epoch: 1, train for the 200-th batch, train loss: 0.5619010329246521:  84%|█████████▏ | 199/237 [00:50<00:11,  3.19it/s]Epoch: 1, train for the 200-th batch, train loss: 0.5619010329246521:  84%|█████████▎ | 200/237 [00:50<00:11,  3.34it/s]Epoch: 2, train for the 40-th batch, train loss: 0.6277545690536499:  26%|███▎         | 39/151 [00:05<00:18,  6.14it/s]Epoch: 2, train for the 40-th batch, train loss: 0.6277545690536499:  26%|███▍         | 40/151 [00:05<00:16,  6.55it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5058441162109375:  75%|████████▎  | 110/146 [00:15<00:05,  6.39it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5058441162109375:  76%|████████▎  | 111/146 [00:15<00:05,  6.99it/s]Epoch: 1, train for the 236-th batch, train loss: 0.5613763928413391:  98%|██████████▋| 235/241 [00:49<00:01,  5.06it/s]Epoch: 1, train for the 236-th batch, train loss: 0.5613763928413391:  98%|██████████▊| 236/241 [00:49<00:00,  5.33it/s]Epoch: 2, train for the 112-th batch, train loss: 0.5537280440330505:  76%|████████▎  | 111/146 [00:15<00:05,  6.99it/s]Epoch: 2, train for the 112-th batch, train loss: 0.5537280440330505:  77%|████████▍  | 112/146 [00:15<00:04,  7.52it/s]Epoch: 2, train for the 41-th batch, train loss: 0.6355079412460327:  26%|███▍         | 40/151 [00:06<00:16,  6.55it/s]Epoch: 2, train for the 41-th batch, train loss: 0.6355079412460327:  27%|███▌         | 41/151 [00:06<00:16,  6.58it/s]Epoch: 1, train for the 237-th batch, train loss: 0.5224588513374329:  98%|██████████▊| 236/241 [00:49<00:00,  5.33it/s]Epoch: 2, train for the 113-th batch, train loss: 0.4903824031352997:  77%|████████▍  | 112/146 [00:15<00:04,  7.52it/s]Epoch: 2, train for the 113-th batch, train loss: 0.4903824031352997:  77%|████████▌  | 113/146 [00:15<00:04,  8.06it/s]Epoch: 1, train for the 237-th batch, train loss: 0.5224588513374329:  98%|██████████▊| 237/241 [00:49<00:00,  5.64it/s]Epoch: 2, train for the 42-th batch, train loss: 0.6118024587631226:  27%|███▌         | 41/151 [00:06<00:16,  6.58it/s]Epoch: 2, train for the 42-th batch, train loss: 0.6118024587631226:  28%|███▌         | 42/151 [00:06<00:16,  6.67it/s]Epoch: 1, train for the 192-th batch, train loss: 0.5118545293807983:  50%|█████▍     | 191/383 [00:48<00:46,  4.14it/s]Epoch: 1, train for the 201-th batch, train loss: 0.5471658110618591:  84%|█████████▎ | 200/237 [00:50<00:11,  3.34it/s]Epoch: 2, train for the 114-th batch, train loss: 0.5112654566764832:  77%|████████▌  | 113/146 [00:16<00:04,  8.06it/s]Epoch: 2, train for the 114-th batch, train loss: 0.5112654566764832:  78%|████████▌  | 114/146 [00:16<00:03,  8.55it/s]Epoch: 1, train for the 201-th batch, train loss: 0.5471658110618591:  85%|█████████▎ | 201/237 [00:50<00:11,  3.14it/s]Epoch: 1, train for the 192-th batch, train loss: 0.5118545293807983:  50%|█████▌     | 192/383 [00:48<00:57,  3.31it/s]Epoch: 1, train for the 238-th batch, train loss: 0.537007212638855:  98%|███████████▊| 237/241 [00:50<00:00,  5.64it/s]Epoch: 1, train for the 238-th batch, train loss: 0.537007212638855:  99%|███████████▊| 238/241 [00:50<00:00,  5.86it/s]Epoch: 2, train for the 115-th batch, train loss: 0.5379375219345093:  78%|████████▌  | 114/146 [00:16<00:03,  8.55it/s]Epoch: 2, train for the 43-th batch, train loss: 0.644077718257904:  28%|███▉          | 42/151 [00:06<00:16,  6.67it/s]Epoch: 2, train for the 115-th batch, train loss: 0.5379375219345093:  79%|████████▋  | 115/146 [00:16<00:03,  8.79it/s]Epoch: 2, train for the 43-th batch, train loss: 0.644077718257904:  28%|███▉          | 43/151 [00:06<00:16,  6.68it/s]Epoch: 1, train for the 239-th batch, train loss: 0.5802589058876038:  99%|██████████▊| 238/241 [00:50<00:00,  5.86it/s]Epoch: 2, train for the 116-th batch, train loss: 0.5261589884757996:  79%|████████▋  | 115/146 [00:16<00:03,  8.79it/s]Epoch: 1, train for the 193-th batch, train loss: 0.5138523578643799:  50%|█████▌     | 192/383 [00:48<00:57,  3.31it/s]Epoch: 1, train for the 239-th batch, train loss: 0.5802589058876038:  99%|██████████▉| 239/241 [00:50<00:00,  6.03it/s]Epoch: 2, train for the 116-th batch, train loss: 0.5261589884757996:  79%|████████▋  | 116/146 [00:16<00:03,  8.91it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5823308229446411:  28%|███▋         | 43/151 [00:06<00:16,  6.68it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5823308229446411:  29%|███▊         | 44/151 [00:06<00:15,  6.99it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]Epoch: 1, train for the 193-th batch, train loss: 0.5138523578643799:  50%|█████▌     | 193/383 [00:48<00:53,  3.54it/s]Epoch: 2, train for the 117-th batch, train loss: 0.5722150802612305:  79%|████████▋  | 116/146 [00:16<00:03,  8.91it/s]Epoch: 2, train for the 117-th batch, train loss: 0.5722150802612305:  80%|████████▊  | 117/146 [00:16<00:03,  8.73it/s]Epoch: 1, train for the 240-th batch, train loss: 0.47433122992515564:  99%|█████████▉| 239/241 [00:50<00:00,  6.03it/s]Epoch: 1, train for the 202-th batch, train loss: 0.5567277669906616:  85%|█████████▎ | 201/237 [00:50<00:11,  3.14it/s]Epoch: 1, train for the 240-th batch, train loss: 0.47433122992515564: 100%|█████████▉| 240/241 [00:50<00:00,  5.89it/s]Epoch: 2, train for the 45-th batch, train loss: 0.6002478003501892:  29%|███▊         | 44/151 [00:06<00:15,  6.99it/s]Epoch: 1, train for the 202-th batch, train loss: 0.5567277669906616:  85%|█████████▍ | 202/237 [00:50<00:12,  2.91it/s]Epoch: 2, train for the 45-th batch, train loss: 0.6002478003501892:  30%|███▊         | 45/151 [00:06<00:15,  6.68it/s]Epoch: 1, train for the 194-th batch, train loss: 0.47177818417549133:  50%|█████     | 193/383 [00:49<00:53,  3.54it/s]Epoch: 2, train for the 118-th batch, train loss: 0.5496639013290405:  80%|████████▊  | 117/146 [00:16<00:03,  8.73it/s]Epoch: 3, train for the 1-th batch, train loss: 0.6629377603530884:   0%|                       | 0/119 [00:00<?, ?it/s]Epoch: 2, train for the 118-th batch, train loss: 0.5496639013290405:  81%|████████▉  | 118/146 [00:16<00:03,  8.33it/s]Epoch: 3, train for the 1-th batch, train loss: 0.6629377603530884:   1%|▏              | 1/119 [00:00<00:26,  4.48it/s]Epoch: 1, train for the 194-th batch, train loss: 0.47177818417549133:  51%|█████     | 194/383 [00:49<00:49,  3.80it/s]Epoch: 1, train for the 241-th batch, train loss: 0.5371871590614319: 100%|██████████▉| 240/241 [00:50<00:00,  5.89it/s]Epoch: 1, train for the 241-th batch, train loss: 0.5371871590614319: 100%|███████████| 241/241 [00:50<00:00,  5.93it/s]Epoch: 1, train for the 241-th batch, train loss: 0.5371871590614319: 100%|███████████| 241/241 [00:50<00:00,  4.76it/s]
Epoch: 2, train for the 46-th batch, train loss: 0.6094002723693848:  30%|███▊         | 45/151 [00:06<00:15,  6.68it/s]Epoch: 2, train for the 46-th batch, train loss: 0.6094002723693848:  30%|███▉         | 46/151 [00:06<00:16,  6.43it/s]Epoch: 1, train for the 203-th batch, train loss: 0.5343500971794128:  85%|█████████▍ | 202/237 [00:51<00:12,  2.91it/s]Epoch: 3, train for the 2-th batch, train loss: 0.8083557486534119:   1%|▏              | 1/119 [00:00<00:26,  4.48it/s]Epoch: 3, train for the 2-th batch, train loss: 0.8083557486534119:   2%|▎              | 2/119 [00:00<00:20,  5.81it/s]Epoch: 2, train for the 119-th batch, train loss: 0.507504403591156:  81%|█████████▋  | 118/146 [00:16<00:03,  8.33it/s]Epoch: 2, train for the 119-th batch, train loss: 0.507504403591156:  82%|█████████▊  | 119/146 [00:16<00:03,  7.42it/s]Epoch: 1, train for the 203-th batch, train loss: 0.5343500971794128:  86%|█████████▍ | 203/237 [00:51<00:10,  3.24it/s]Epoch: 1, train for the 195-th batch, train loss: 0.4943501949310303:  51%|█████▌     | 194/383 [00:49<00:49,  3.80it/s]Epoch: 1, train for the 195-th batch, train loss: 0.4943501949310303:  51%|█████▌     | 195/383 [00:49<00:46,  4.06it/s]Epoch: 2, train for the 47-th batch, train loss: 0.6149533987045288:  30%|███▉         | 46/151 [00:06<00:16,  6.43it/s]Epoch: 2, train for the 47-th batch, train loss: 0.6149533987045288:  31%|████         | 47/151 [00:06<00:15,  6.73it/s]Epoch: 3, train for the 3-th batch, train loss: 0.8496397137641907:   2%|▎              | 2/119 [00:00<00:20,  5.81it/s]Epoch: 3, train for the 3-th batch, train loss: 0.8496397137641907:   3%|▍              | 3/119 [00:00<00:18,  6.39it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5130597949028015:  82%|████████▉  | 119/146 [00:16<00:03,  7.42it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5130597949028015:  82%|█████████  | 120/146 [00:16<00:03,  7.06it/s]Epoch: 2, train for the 48-th batch, train loss: 0.5720542669296265:  31%|████         | 47/151 [00:07<00:15,  6.73it/s]Epoch: 2, train for the 48-th batch, train loss: 0.5720542669296265:  32%|████▏        | 48/151 [00:07<00:14,  7.14it/s]  0%|                                                                                            | 0/72 [00:00<?, ?it/s]Epoch: 3, train for the 4-th batch, train loss: 0.7766056656837463:   3%|▍              | 3/119 [00:00<00:18,  6.39it/s]Epoch: 3, train for the 4-th batch, train loss: 0.7766056656837463:   3%|▌              | 4/119 [00:00<00:15,  7.29it/s]Epoch: 1, train for the 204-th batch, train loss: 0.5447887778282166:  86%|█████████▍ | 203/237 [00:51<00:10,  3.24it/s]evaluate for the 1-th batch, evaluate loss: 0.5283472537994385:   0%|                            | 0/72 [00:00<?, ?it/s]Epoch: 1, train for the 204-th batch, train loss: 0.5447887778282166:  86%|█████████▍ | 204/237 [00:51<00:09,  3.37it/s]Epoch: 1, train for the 196-th batch, train loss: 0.5011903047561646:  51%|█████▌     | 195/383 [00:49<00:46,  4.06it/s]Epoch: 2, train for the 121-th batch, train loss: 0.5382938981056213:  82%|█████████  | 120/146 [00:16<00:03,  7.06it/s]Epoch: 2, train for the 121-th batch, train loss: 0.5382938981056213:  83%|█████████  | 121/146 [00:16<00:03,  7.30it/s]Epoch: 1, train for the 196-th batch, train loss: 0.5011903047561646:  51%|█████▋     | 196/383 [00:49<00:47,  3.90it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5963925123214722:  32%|████▏        | 48/151 [00:07<00:14,  7.14it/s]Epoch: 3, train for the 5-th batch, train loss: 0.637365460395813:   3%|▌               | 4/119 [00:00<00:15,  7.29it/s]evaluate for the 2-th batch, evaluate loss: 0.5307163000106812:   0%|                            | 0/72 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5307163000106812:   3%|▌                   | 2/72 [00:00<00:05, 13.63it/s]Epoch: 3, train for the 5-th batch, train loss: 0.637365460395813:   4%|▋               | 5/119 [00:00<00:15,  7.37it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5963925123214722:  32%|████▏        | 49/151 [00:07<00:14,  6.88it/s]evaluate for the 3-th batch, evaluate loss: 0.490312784910202:   3%|▌                    | 2/72 [00:00<00:05, 13.63it/s]Epoch: 2, train for the 122-th batch, train loss: 0.5479132533073425:  83%|█████████  | 121/146 [00:17<00:03,  7.30it/s]Epoch: 2, train for the 122-th batch, train loss: 0.5479132533073425:  84%|█████████▏ | 122/146 [00:17<00:03,  6.95it/s]Epoch: 3, train for the 6-th batch, train loss: 0.5420011281967163:   4%|▋              | 5/119 [00:00<00:15,  7.37it/s]Epoch: 3, train for the 6-th batch, train loss: 0.5420011281967163:   5%|▊              | 6/119 [00:00<00:16,  6.99it/s]evaluate for the 4-th batch, evaluate loss: 0.5527970790863037:   3%|▌                   | 2/72 [00:00<00:05, 13.63it/s]evaluate for the 4-th batch, evaluate loss: 0.5527970790863037:   6%|█                   | 4/72 [00:00<00:05, 12.02it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5728152394294739:  32%|████▏        | 49/151 [00:07<00:14,  6.88it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5728152394294739:  33%|████▎        | 50/151 [00:07<00:15,  6.39it/s]Epoch: 2, train for the 123-th batch, train loss: 0.5700250864028931:  84%|█████████▏ | 122/146 [00:17<00:03,  6.95it/s]Epoch: 1, train for the 197-th batch, train loss: 0.5026404857635498:  51%|█████▋     | 196/383 [00:49<00:47,  3.90it/s]Epoch: 1, train for the 205-th batch, train loss: 0.5440464615821838:  86%|█████████▍ | 204/237 [00:51<00:09,  3.37it/s]Epoch: 2, train for the 123-th batch, train loss: 0.5700250864028931:  84%|█████████▎ | 123/146 [00:17<00:03,  6.73it/s]Epoch: 3, train for the 7-th batch, train loss: 0.5006040334701538:   5%|▊              | 6/119 [00:01<00:16,  6.99it/s]Epoch: 3, train for the 7-th batch, train loss: 0.5006040334701538:   6%|▉              | 7/119 [00:01<00:15,  7.30it/s]Epoch: 1, train for the 205-th batch, train loss: 0.5440464615821838:  86%|█████████▌ | 205/237 [00:51<00:10,  3.15it/s]Epoch: 1, train for the 197-th batch, train loss: 0.5026404857635498:  51%|█████▋     | 197/383 [00:49<00:50,  3.65it/s]Epoch: 2, train for the 51-th batch, train loss: 0.6179882884025574:  33%|████▎        | 50/151 [00:07<00:15,  6.39it/s]evaluate for the 5-th batch, evaluate loss: 0.4906710088253021:   6%|█                   | 4/72 [00:00<00:05, 12.02it/s]Epoch: 2, train for the 51-th batch, train loss: 0.6179882884025574:  34%|████▍        | 51/151 [00:07<00:14,  6.75it/s]Epoch: 2, train for the 124-th batch, train loss: 0.5514225363731384:  84%|█████████▎ | 123/146 [00:17<00:03,  6.73it/s]Epoch: 2, train for the 124-th batch, train loss: 0.5514225363731384:  85%|█████████▎ | 124/146 [00:17<00:03,  6.71it/s]evaluate for the 6-th batch, evaluate loss: 0.5053858160972595:   6%|█                   | 4/72 [00:00<00:05, 12.02it/s]evaluate for the 6-th batch, evaluate loss: 0.5053858160972595:   8%|█▋                  | 6/72 [00:00<00:06, 10.14it/s]Epoch: 3, train for the 8-th batch, train loss: 0.4669640362262726:   6%|▉              | 7/119 [00:01<00:15,  7.30it/s]Epoch: 3, train for the 8-th batch, train loss: 0.4669640362262726:   7%|█              | 8/119 [00:01<00:15,  7.12it/s]Epoch: 2, train for the 52-th batch, train loss: 0.6304534077644348:  34%|████▍        | 51/151 [00:07<00:14,  6.75it/s]evaluate for the 7-th batch, evaluate loss: 0.556420087814331:   8%|█▊                   | 6/72 [00:00<00:06, 10.14it/s]Epoch: 2, train for the 52-th batch, train loss: 0.6304534077644348:  34%|████▍        | 52/151 [00:07<00:15,  6.53it/s]Epoch: 1, train for the 206-th batch, train loss: 0.5444071888923645:  86%|█████████▌ | 205/237 [00:51<00:10,  3.15it/s]Epoch: 1, train for the 206-th batch, train loss: 0.5444071888923645:  87%|█████████▌ | 206/237 [00:51<00:09,  3.41it/s]Epoch: 2, train for the 125-th batch, train loss: 0.5236768126487732:  85%|█████████▎ | 124/146 [00:17<00:03,  6.71it/s]Epoch: 2, train for the 53-th batch, train loss: 0.6121940612792969:  34%|████▍        | 52/151 [00:07<00:15,  6.53it/s]Epoch: 2, train for the 125-th batch, train loss: 0.5236768126487732:  86%|█████████▍ | 125/146 [00:17<00:03,  6.34it/s]Epoch: 3, train for the 9-th batch, train loss: 0.49613329768180847:   7%|▉             | 8/119 [00:01<00:15,  7.12it/s]Epoch: 2, train for the 53-th batch, train loss: 0.6121940612792969:  35%|████▌        | 53/151 [00:07<00:13,  7.17it/s]Epoch: 3, train for the 9-th batch, train loss: 0.49613329768180847:   8%|█             | 9/119 [00:01<00:15,  6.89it/s]Epoch: 1, train for the 198-th batch, train loss: 0.46497583389282227:  51%|█████▏    | 197/383 [00:50<00:50,  3.65it/s]Epoch: 2, train for the 54-th batch, train loss: 0.5817694664001465:  35%|████▌        | 53/151 [00:07<00:13,  7.17it/s]Epoch: 2, train for the 54-th batch, train loss: 0.5817694664001465:  36%|████▋        | 54/151 [00:07<00:12,  7.56it/s]Epoch: 1, train for the 198-th batch, train loss: 0.46497583389282227:  52%|█████▏    | 198/383 [00:50<00:59,  3.10it/s]Epoch: 3, train for the 10-th batch, train loss: 0.5043856501579285:   8%|█             | 9/119 [00:01<00:15,  6.89it/s]Epoch: 2, train for the 126-th batch, train loss: 0.5618872046470642:  86%|█████████▍ | 125/146 [00:17<00:03,  6.34it/s]Epoch: 3, train for the 10-th batch, train loss: 0.5043856501579285:   8%|█            | 10/119 [00:01<00:16,  6.41it/s]evaluate for the 8-th batch, evaluate loss: 0.5502867102622986:   8%|█▋                  | 6/72 [00:00<00:06, 10.14it/s]evaluate for the 8-th batch, evaluate loss: 0.5502867102622986:  11%|██▏                 | 8/72 [00:00<00:08,  7.61it/s]Epoch: 2, train for the 126-th batch, train loss: 0.5618872046470642:  86%|█████████▍ | 126/146 [00:17<00:03,  5.98it/s]Epoch: 1, train for the 207-th batch, train loss: 0.579749584197998:  87%|██████████▍ | 206/237 [00:52<00:09,  3.41it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5659384727478027:  36%|████▋        | 54/151 [00:08<00:12,  7.56it/s]Epoch: 1, train for the 207-th batch, train loss: 0.579749584197998:  87%|██████████▍ | 207/237 [00:52<00:08,  3.38it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5659384727478027:  36%|████▋        | 55/151 [00:08<00:12,  7.54it/s]evaluate for the 9-th batch, evaluate loss: 0.5111753940582275:  11%|██▏                 | 8/72 [00:00<00:08,  7.61it/s]evaluate for the 10-th batch, evaluate loss: 0.5026861429214478:  11%|██                 | 8/72 [00:01<00:08,  7.61it/s]evaluate for the 10-th batch, evaluate loss: 0.5026861429214478:  14%|██▌               | 10/72 [00:01<00:06,  9.28it/s]Epoch: 3, train for the 11-th batch, train loss: 0.5052957534790039:   8%|█            | 10/119 [00:01<00:16,  6.41it/s]Epoch: 2, train for the 127-th batch, train loss: 0.5100157856941223:  86%|█████████▍ | 126/146 [00:18<00:03,  5.98it/s]Epoch: 3, train for the 11-th batch, train loss: 0.5052957534790039:   9%|█▏           | 11/119 [00:01<00:17,  6.03it/s]Epoch: 2, train for the 127-th batch, train loss: 0.5100157856941223:  87%|█████████▌ | 127/146 [00:18<00:03,  5.74it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5278415083885193:  36%|████▋        | 55/151 [00:08<00:12,  7.54it/s]Epoch: 1, train for the 199-th batch, train loss: 0.4663788378238678:  52%|█████▋     | 198/383 [00:50<00:59,  3.10it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5278415083885193:  37%|████▊        | 56/151 [00:08<00:12,  7.36it/s]Epoch: 1, train for the 199-th batch, train loss: 0.4663788378238678:  52%|█████▋     | 199/383 [00:50<00:57,  3.19it/s]Epoch: 2, train for the 57-th batch, train loss: 0.5642173886299133:  37%|████▊        | 56/151 [00:08<00:12,  7.36it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5411290526390076:  87%|█████████▌ | 127/146 [00:18<00:03,  5.74it/s]Epoch: 2, train for the 57-th batch, train loss: 0.5642173886299133:  38%|████▉        | 57/151 [00:08<00:12,  7.50it/s]Epoch: 3, train for the 12-th batch, train loss: 0.5006164908409119:   9%|█▏           | 11/119 [00:01<00:17,  6.03it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5411290526390076:  88%|█████████▋ | 128/146 [00:18<00:03,  5.95it/s]Epoch: 3, train for the 12-th batch, train loss: 0.5006164908409119:  10%|█▎           | 12/119 [00:01<00:17,  6.04it/s]evaluate for the 11-th batch, evaluate loss: 0.5178644061088562:  14%|██▌               | 10/72 [00:01<00:06,  9.28it/s]evaluate for the 12-th batch, evaluate loss: 0.5010867714881897:  14%|██▌               | 10/72 [00:01<00:06,  9.28it/s]evaluate for the 12-th batch, evaluate loss: 0.5010867714881897:  17%|███               | 12/72 [00:01<00:07,  7.60it/s]Epoch: 2, train for the 58-th batch, train loss: 0.5459532141685486:  38%|████▉        | 57/151 [00:08<00:12,  7.50it/s]Epoch: 1, train for the 208-th batch, train loss: 0.5454913377761841:  87%|█████████▌ | 207/237 [00:52<00:08,  3.38it/s]Epoch: 2, train for the 58-th batch, train loss: 0.5459532141685486:  38%|████▉        | 58/151 [00:08<00:13,  7.02it/s]Epoch: 1, train for the 200-th batch, train loss: 0.5600077509880066:  52%|█████▋     | 199/383 [00:50<00:57,  3.19it/s]Epoch: 2, train for the 129-th batch, train loss: 0.5454136729240417:  88%|█████████▋ | 128/146 [00:18<00:03,  5.95it/s]Epoch: 3, train for the 13-th batch, train loss: 0.4262200891971588:  10%|█▎           | 12/119 [00:02<00:17,  6.04it/s]Epoch: 3, train for the 13-th batch, train loss: 0.4262200891971588:  11%|█▍           | 13/119 [00:02<00:17,  5.95it/s]Epoch: 1, train for the 208-th batch, train loss: 0.5454913377761841:  88%|█████████▋ | 208/237 [00:52<00:10,  2.86it/s]Epoch: 2, train for the 129-th batch, train loss: 0.5454136729240417:  88%|█████████▋ | 129/146 [00:18<00:02,  5.77it/s]evaluate for the 13-th batch, evaluate loss: 0.45294398069381714:  17%|██▊              | 12/72 [00:01<00:07,  7.60it/s]Epoch: 1, train for the 200-th batch, train loss: 0.5600077509880066:  52%|█████▋     | 200/383 [00:50<00:58,  3.12it/s]evaluate for the 14-th batch, evaluate loss: 0.42200374603271484:  17%|██▊              | 12/72 [00:01<00:07,  7.60it/s]evaluate for the 14-th batch, evaluate loss: 0.42200374603271484:  19%|███▎             | 14/72 [00:01<00:06,  9.14it/s]Epoch: 2, train for the 59-th batch, train loss: 0.5512830018997192:  38%|████▉        | 58/151 [00:08<00:13,  7.02it/s]Epoch: 2, train for the 59-th batch, train loss: 0.5512830018997192:  39%|█████        | 59/151 [00:08<00:13,  6.75it/s]Epoch: 3, train for the 14-th batch, train loss: 0.4924883246421814:  11%|█▍           | 13/119 [00:02<00:17,  5.95it/s]Epoch: 3, train for the 14-th batch, train loss: 0.4924883246421814:  12%|█▌           | 14/119 [00:02<00:17,  6.02it/s]Epoch: 2, train for the 130-th batch, train loss: 0.4771327078342438:  88%|█████████▋ | 129/146 [00:18<00:02,  5.77it/s]Epoch: 2, train for the 130-th batch, train loss: 0.4771327078342438:  89%|█████████▊ | 130/146 [00:18<00:02,  5.65it/s]Epoch: 1, train for the 209-th batch, train loss: 0.5530941486358643:  88%|█████████▋ | 208/237 [00:52<00:10,  2.86it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5452784895896912:  39%|█████        | 59/151 [00:08<00:13,  6.75it/s]Epoch: 1, train for the 201-th batch, train loss: 0.5202324390411377:  52%|█████▋     | 200/383 [00:51<00:58,  3.12it/s]Epoch: 1, train for the 209-th batch, train loss: 0.5530941486358643:  88%|█████████▋ | 209/237 [00:53<00:08,  3.12it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5452784895896912:  40%|█████▏       | 60/151 [00:08<00:12,  7.02it/s]Epoch: 3, train for the 15-th batch, train loss: 0.5401661396026611:  12%|█▌           | 14/119 [00:02<00:17,  6.02it/s]Epoch: 3, train for the 15-th batch, train loss: 0.5401661396026611:  13%|█▋           | 15/119 [00:02<00:16,  6.43it/s]Epoch: 1, train for the 201-th batch, train loss: 0.5202324390411377:  52%|█████▊     | 201/383 [00:51<00:54,  3.37it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5421926975250244:  89%|█████████▊ | 130/146 [00:18<00:02,  5.65it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5421926975250244:  90%|█████████▊ | 131/146 [00:18<00:02,  5.79it/s]evaluate for the 15-th batch, evaluate loss: 0.46153444051742554:  19%|███▎             | 14/72 [00:01<00:06,  9.14it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5651264786720276:  40%|█████▏       | 60/151 [00:08<00:12,  7.02it/s]Epoch: 3, train for the 16-th batch, train loss: 0.5008660554885864:  13%|█▋           | 15/119 [00:02<00:16,  6.43it/s]Epoch: 3, train for the 16-th batch, train loss: 0.5008660554885864:  13%|█▋           | 16/119 [00:02<00:15,  6.70it/s]evaluate for the 16-th batch, evaluate loss: 0.4812317192554474:  19%|███▌              | 14/72 [00:01<00:06,  9.14it/s]evaluate for the 16-th batch, evaluate loss: 0.4812317192554474:  22%|████              | 16/72 [00:01<00:07,  7.75it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5651264786720276:  40%|█████▎       | 61/151 [00:08<00:13,  6.67it/s]Epoch: 1, train for the 210-th batch, train loss: 0.5765330195426941:  88%|█████████▋ | 209/237 [00:53<00:08,  3.12it/s]evaluate for the 17-th batch, evaluate loss: 0.4420366585254669:  22%|████              | 16/72 [00:01<00:07,  7.75it/s]Epoch: 2, train for the 132-th batch, train loss: 0.4842144548892975:  90%|█████████▊ | 131/146 [00:18<00:02,  5.79it/s]Epoch: 1, train for the 210-th batch, train loss: 0.5765330195426941:  89%|█████████▋ | 210/237 [00:53<00:07,  3.38it/s]Epoch: 2, train for the 132-th batch, train loss: 0.4842144548892975:  90%|█████████▉ | 132/146 [00:18<00:02,  5.97it/s]Epoch: 1, train for the 202-th batch, train loss: 0.4845622777938843:  52%|█████▊     | 201/383 [00:51<00:54,  3.37it/s]Epoch: 3, train for the 17-th batch, train loss: 0.4690794348716736:  13%|█▋           | 16/119 [00:02<00:15,  6.70it/s]evaluate for the 18-th batch, evaluate loss: 0.4277898371219635:  22%|████              | 16/72 [00:01<00:07,  7.75it/s]evaluate for the 18-th batch, evaluate loss: 0.4277898371219635:  25%|████▌             | 18/72 [00:01<00:05,  9.38it/s]Epoch: 3, train for the 17-th batch, train loss: 0.4690794348716736:  14%|█▊           | 17/119 [00:02<00:14,  7.05it/s]Epoch: 1, train for the 202-th batch, train loss: 0.4845622777938843:  53%|█████▊     | 202/383 [00:51<00:51,  3.52it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5830893516540527:  40%|█████▎       | 61/151 [00:09<00:13,  6.67it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5830893516540527:  41%|█████▎       | 62/151 [00:09<00:14,  6.24it/s]evaluate for the 19-th batch, evaluate loss: 0.47352004051208496:  25%|████▎            | 18/72 [00:02<00:05,  9.38it/s]Epoch: 2, train for the 133-th batch, train loss: 0.5384830832481384:  90%|█████████▉ | 132/146 [00:18<00:02,  5.97it/s]Epoch: 2, train for the 133-th batch, train loss: 0.5384830832481384:  91%|██████████ | 133/146 [00:18<00:02,  6.39it/s]evaluate for the 20-th batch, evaluate loss: 0.5067405104637146:  25%|████▌             | 18/72 [00:02<00:05,  9.38it/s]evaluate for the 20-th batch, evaluate loss: 0.5067405104637146:  28%|█████             | 20/72 [00:02<00:05, 10.38it/s]Epoch: 3, train for the 18-th batch, train loss: 0.47682130336761475:  14%|█▋          | 17/119 [00:02<00:14,  7.05it/s]Epoch: 1, train for the 211-th batch, train loss: 0.5307116508483887:  89%|█████████▋ | 210/237 [00:53<00:07,  3.38it/s]Epoch: 3, train for the 18-th batch, train loss: 0.47682130336761475:  15%|█▊          | 18/119 [00:02<00:14,  6.83it/s]Epoch: 2, train for the 63-th batch, train loss: 0.6220855712890625:  41%|█████▎       | 62/151 [00:09<00:14,  6.24it/s]Epoch: 1, train for the 211-th batch, train loss: 0.5307116508483887:  89%|█████████▊ | 211/237 [00:53<00:07,  3.55it/s]Epoch: 2, train for the 63-th batch, train loss: 0.6220855712890625:  42%|█████▍       | 63/151 [00:09<00:13,  6.43it/s]Epoch: 2, train for the 134-th batch, train loss: 0.4732184410095215:  91%|██████████ | 133/146 [00:19<00:02,  6.39it/s]Epoch: 2, train for the 134-th batch, train loss: 0.4732184410095215:  92%|██████████ | 134/146 [00:19<00:01,  6.56it/s]Epoch: 3, train for the 19-th batch, train loss: 0.4967101812362671:  15%|█▉           | 18/119 [00:02<00:14,  6.83it/s]Epoch: 3, train for the 19-th batch, train loss: 0.4967101812362671:  16%|██           | 19/119 [00:02<00:13,  7.31it/s]Epoch: 1, train for the 203-th batch, train loss: 0.4435536861419678:  53%|█████▊     | 202/383 [00:51<00:51,  3.52it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5627022981643677:  42%|█████▍       | 63/151 [00:09<00:13,  6.43it/s]Epoch: 1, train for the 203-th batch, train loss: 0.4435536861419678:  53%|█████▊     | 203/383 [00:51<00:53,  3.36it/s]evaluate for the 21-th batch, evaluate loss: 0.5243375897407532:  28%|█████             | 20/72 [00:02<00:05, 10.38it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5627022981643677:  42%|█████▌       | 64/151 [00:09<00:12,  6.72it/s]Epoch: 2, train for the 135-th batch, train loss: 0.5177382826805115:  92%|██████████ | 134/146 [00:19<00:01,  6.56it/s]Epoch: 1, train for the 212-th batch, train loss: 0.5401461124420166:  89%|█████████▊ | 211/237 [00:53<00:07,  3.55it/s]evaluate for the 22-th batch, evaluate loss: 0.4664340019226074:  28%|█████             | 20/72 [00:02<00:05, 10.38it/s]evaluate for the 22-th batch, evaluate loss: 0.4664340019226074:  31%|█████▌            | 22/72 [00:02<00:05,  9.58it/s]Epoch: 3, train for the 20-th batch, train loss: 0.4830796420574188:  16%|██           | 19/119 [00:02<00:13,  7.31it/s]Epoch: 2, train for the 135-th batch, train loss: 0.5177382826805115:  92%|██████████▏| 135/146 [00:19<00:01,  6.32it/s]Epoch: 3, train for the 20-th batch, train loss: 0.4830796420574188:  17%|██▏          | 20/119 [00:02<00:13,  7.32it/s]Epoch: 1, train for the 212-th batch, train loss: 0.5401461124420166:  89%|█████████▊ | 212/237 [00:53<00:06,  3.74it/s]evaluate for the 23-th batch, evaluate loss: 0.4764538109302521:  31%|█████▌            | 22/72 [00:02<00:05,  9.58it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5177029967308044:  42%|█████▌       | 64/151 [00:09<00:12,  6.72it/s]evaluate for the 24-th batch, evaluate loss: 0.4713389575481415:  31%|█████▌            | 22/72 [00:02<00:05,  9.58it/s]evaluate for the 24-th batch, evaluate loss: 0.4713389575481415:  33%|██████            | 24/72 [00:02<00:04, 10.65it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5177029967308044:  43%|█████▌       | 65/151 [00:09<00:13,  6.19it/s]Epoch: 3, train for the 21-th batch, train loss: 0.498718798160553:  17%|██▎           | 20/119 [00:03<00:13,  7.32it/s]Epoch: 3, train for the 21-th batch, train loss: 0.498718798160553:  18%|██▍           | 21/119 [00:03<00:14,  6.54it/s]Epoch: 2, train for the 136-th batch, train loss: 0.4946359097957611:  92%|██████████▏| 135/146 [00:19<00:01,  6.32it/s]Epoch: 2, train for the 136-th batch, train loss: 0.4946359097957611:  93%|██████████▏| 136/146 [00:19<00:01,  5.67it/s]Epoch: 2, train for the 66-th batch, train loss: 0.5547478199005127:  43%|█████▌       | 65/151 [00:09<00:13,  6.19it/s]Epoch: 2, train for the 66-th batch, train loss: 0.5547478199005127:  44%|█████▋       | 66/151 [00:09<00:12,  6.92it/s]Epoch: 3, train for the 22-th batch, train loss: 0.45853328704833984:  18%|██          | 21/119 [00:03<00:14,  6.54it/s]Epoch: 3, train for the 22-th batch, train loss: 0.45853328704833984:  18%|██▏         | 22/119 [00:03<00:13,  7.25it/s]Epoch: 1, train for the 213-th batch, train loss: 0.5547690391540527:  89%|█████████▊ | 212/237 [00:54<00:06,  3.74it/s]Epoch: 2, train for the 67-th batch, train loss: 0.5442878007888794:  44%|█████▋       | 66/151 [00:09<00:12,  6.92it/s]Epoch: 2, train for the 67-th batch, train loss: 0.5442878007888794:  44%|█████▊       | 67/151 [00:09<00:11,  7.47it/s]Epoch: 2, train for the 137-th batch, train loss: 0.4970313012599945:  93%|██████████▏| 136/146 [00:19<00:01,  5.67it/s]Epoch: 2, train for the 137-th batch, train loss: 0.4970313012599945:  94%|██████████▎| 137/146 [00:19<00:01,  6.02it/s]Epoch: 1, train for the 213-th batch, train loss: 0.5547690391540527:  90%|█████████▉ | 213/237 [00:54<00:06,  3.47it/s]Epoch: 3, train for the 23-th batch, train loss: 0.5313903093338013:  18%|██▍          | 22/119 [00:03<00:13,  7.25it/s]evaluate for the 25-th batch, evaluate loss: 0.5869686603546143:  33%|██████            | 24/72 [00:02<00:04, 10.65it/s]Epoch: 1, train for the 204-th batch, train loss: 0.32856056094169617:  53%|█████▎    | 203/383 [00:52<00:53,  3.36it/s]Epoch: 3, train for the 23-th batch, train loss: 0.5313903093338013:  19%|██▌          | 23/119 [00:03<00:12,  7.64it/s]Epoch: 1, train for the 204-th batch, train loss: 0.32856056094169617:  53%|█████▎    | 204/383 [00:52<01:05,  2.71it/s]Epoch: 2, train for the 68-th batch, train loss: 0.5425888299942017:  44%|█████▊       | 67/151 [00:09<00:11,  7.47it/s]evaluate for the 26-th batch, evaluate loss: 0.42798933386802673:  33%|█████▋           | 24/72 [00:02<00:04, 10.65it/s]evaluate for the 26-th batch, evaluate loss: 0.42798933386802673:  36%|██████▏          | 26/72 [00:02<00:05,  8.27it/s]Epoch: 2, train for the 68-th batch, train loss: 0.5425888299942017:  45%|█████▊       | 68/151 [00:09<00:11,  7.19it/s]Epoch: 2, train for the 138-th batch, train loss: 0.5101413130760193:  94%|██████████▎| 137/146 [00:19<00:01,  6.02it/s]Epoch: 2, train for the 138-th batch, train loss: 0.5101413130760193:  95%|██████████▍| 138/146 [00:19<00:01,  6.10it/s]Epoch: 3, train for the 24-th batch, train loss: 0.3888600170612335:  19%|██▌          | 23/119 [00:03<00:12,  7.64it/s]evaluate for the 27-th batch, evaluate loss: 0.4035443365573883:  36%|██████▌           | 26/72 [00:02<00:05,  8.27it/s]Epoch: 1, train for the 214-th batch, train loss: 0.5961904525756836:  90%|█████████▉ | 213/237 [00:54<00:06,  3.47it/s]Epoch: 3, train for the 24-th batch, train loss: 0.3888600170612335:  20%|██▌          | 24/119 [00:03<00:12,  7.39it/s]Epoch: 1, train for the 214-th batch, train loss: 0.5961904525756836:  90%|█████████▉ | 214/237 [00:54<00:06,  3.63it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5320342183113098:  45%|█████▊       | 68/151 [00:10<00:11,  7.19it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5320342183113098:  46%|█████▉       | 69/151 [00:10<00:11,  6.93it/s]Epoch: 2, train for the 139-th batch, train loss: 0.5468839406967163:  95%|██████████▍| 138/146 [00:20<00:01,  6.10it/s]Epoch: 2, train for the 139-th batch, train loss: 0.5468839406967163:  95%|██████████▍| 139/146 [00:20<00:01,  5.84it/s]Epoch: 3, train for the 25-th batch, train loss: 0.46415388584136963:  20%|██▍         | 24/119 [00:03<00:12,  7.39it/s]Epoch: 3, train for the 25-th batch, train loss: 0.46415388584136963:  21%|██▌         | 25/119 [00:03<00:13,  6.88it/s]Epoch: 1, train for the 215-th batch, train loss: 0.5621259808540344:  90%|█████████▉ | 214/237 [00:54<00:06,  3.63it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5646324157714844:  46%|█████▉       | 69/151 [00:10<00:11,  6.93it/s]evaluate for the 28-th batch, evaluate loss: 0.4376388490200043:  36%|██████▌           | 26/72 [00:03<00:05,  8.27it/s]evaluate for the 28-th batch, evaluate loss: 0.4376388490200043:  39%|███████           | 28/72 [00:03<00:05,  7.84it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5646324157714844:  46%|██████       | 70/151 [00:10<00:11,  7.23it/s]Epoch: 1, train for the 215-th batch, train loss: 0.5621259808540344:  91%|█████████▉ | 215/237 [00:54<00:05,  4.09it/s]evaluate for the 29-th batch, evaluate loss: 0.5031639933586121:  39%|███████           | 28/72 [00:03<00:05,  7.84it/s]Epoch: 2, train for the 140-th batch, train loss: 0.4800986647605896:  95%|██████████▍| 139/146 [00:20<00:01,  5.84it/s]Epoch: 1, train for the 205-th batch, train loss: 0.4237113893032074:  53%|█████▊     | 204/383 [00:52<01:05,  2.71it/s]Epoch: 2, train for the 140-th batch, train loss: 0.4800986647605896:  96%|██████████▌| 140/146 [00:20<00:00,  6.03it/s]Epoch: 3, train for the 26-th batch, train loss: 0.39894965291023254:  21%|██▌         | 25/119 [00:03<00:13,  6.88it/s]Epoch: 3, train for the 26-th batch, train loss: 0.39894965291023254:  22%|██▌         | 26/119 [00:03<00:13,  6.66it/s]evaluate for the 30-th batch, evaluate loss: 0.4857739210128784:  39%|███████           | 28/72 [00:03<00:05,  7.84it/s]evaluate for the 30-th batch, evaluate loss: 0.4857739210128784:  42%|███████▌          | 30/72 [00:03<00:04,  8.96it/s]Epoch: 1, train for the 205-th batch, train loss: 0.4237113893032074:  54%|█████▉     | 205/383 [00:52<01:10,  2.53it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5814248919487:  46%|███████▍        | 70/151 [00:10<00:11,  7.23it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5814248919487:  47%|███████▌        | 71/151 [00:10<00:11,  6.85it/s]Epoch: 1, train for the 216-th batch, train loss: 0.4841785728931427:  91%|█████████▉ | 215/237 [00:54<00:05,  4.09it/s]evaluate for the 31-th batch, evaluate loss: 0.5234043598175049:  42%|███████▌          | 30/72 [00:03<00:04,  8.96it/s]Epoch: 2, train for the 141-th batch, train loss: 0.5257065296173096:  96%|██████████▌| 140/146 [00:20<00:00,  6.03it/s]Epoch: 2, train for the 141-th batch, train loss: 0.5257065296173096:  97%|██████████▌| 141/146 [00:20<00:00,  6.14it/s]Epoch: 3, train for the 27-th batch, train loss: 0.5172939300537109:  22%|██▊          | 26/119 [00:04<00:13,  6.66it/s]Epoch: 3, train for the 27-th batch, train loss: 0.5172939300537109:  23%|██▉          | 27/119 [00:04<00:13,  6.66it/s]Epoch: 1, train for the 216-th batch, train loss: 0.4841785728931427:  91%|██████████ | 216/237 [00:54<00:05,  4.01it/s]evaluate for the 32-th batch, evaluate loss: 0.5684638023376465:  42%|███████▌          | 30/72 [00:03<00:04,  8.96it/s]evaluate for the 32-th batch, evaluate loss: 0.5684638023376465:  44%|████████          | 32/72 [00:03<00:04,  9.76it/s]Epoch: 2, train for the 72-th batch, train loss: 0.527527928352356:  47%|██████▌       | 71/151 [00:10<00:11,  6.85it/s]Epoch: 2, train for the 72-th batch, train loss: 0.527527928352356:  48%|██████▋       | 72/151 [00:10<00:12,  6.50it/s]Epoch: 1, train for the 206-th batch, train loss: 0.46673640608787537:  54%|█████▎    | 205/383 [00:52<01:10,  2.53it/s]evaluate for the 33-th batch, evaluate loss: 0.502152681350708:  44%|████████▍          | 32/72 [00:03<00:04,  9.76it/s]Epoch: 2, train for the 142-th batch, train loss: 0.44742169976234436:  97%|█████████▋| 141/146 [00:20<00:00,  6.14it/s]Epoch: 3, train for the 28-th batch, train loss: 0.4969887435436249:  23%|██▉          | 27/119 [00:04<00:13,  6.66it/s]Epoch: 1, train for the 206-th batch, train loss: 0.46673640608787537:  54%|█████▍    | 206/383 [00:52<01:03,  2.78it/s]Epoch: 3, train for the 28-th batch, train loss: 0.4969887435436249:  24%|███          | 28/119 [00:04<00:13,  6.51it/s]Epoch: 2, train for the 142-th batch, train loss: 0.44742169976234436:  97%|█████████▋| 142/146 [00:20<00:00,  5.89it/s]evaluate for the 34-th batch, evaluate loss: 0.5489398241043091:  44%|████████          | 32/72 [00:03<00:04,  9.76it/s]evaluate for the 34-th batch, evaluate loss: 0.5489398241043091:  47%|████████▌         | 34/72 [00:03<00:03, 10.55it/s]Epoch: 2, train for the 73-th batch, train loss: 0.5409986972808838:  48%|██████▏      | 72/151 [00:10<00:12,  6.50it/s]Epoch: 2, train for the 73-th batch, train loss: 0.5409986972808838:  48%|██████▎      | 73/151 [00:10<00:12,  6.21it/s]Epoch: 3, train for the 29-th batch, train loss: 0.475806325674057:  24%|███▎          | 28/119 [00:04<00:13,  6.51it/s]Epoch: 3, train for the 29-th batch, train loss: 0.475806325674057:  24%|███▍          | 29/119 [00:04<00:12,  7.06it/s]Epoch: 1, train for the 217-th batch, train loss: 0.4845639765262604:  91%|██████████ | 216/237 [00:55<00:05,  4.01it/s]evaluate for the 35-th batch, evaluate loss: 0.46520376205444336:  47%|████████         | 34/72 [00:03<00:03, 10.55it/s]Epoch: 2, train for the 143-th batch, train loss: 0.5374248623847961:  97%|██████████▋| 142/146 [00:20<00:00,  5.89it/s]Epoch: 2, train for the 143-th batch, train loss: 0.5374248623847961:  98%|██████████▊| 143/146 [00:20<00:00,  5.89it/s]Epoch: 1, train for the 217-th batch, train loss: 0.4845639765262604:  92%|██████████ | 217/237 [00:55<00:05,  3.59it/s]Epoch: 1, train for the 207-th batch, train loss: 0.4700649082660675:  54%|█████▉     | 206/383 [00:53<01:03,  2.78it/s]Epoch: 3, train for the 30-th batch, train loss: 0.46409764885902405:  24%|██▉         | 29/119 [00:04<00:12,  7.06it/s]Epoch: 2, train for the 74-th batch, train loss: 0.4900231957435608:  48%|██████▎      | 73/151 [00:10<00:12,  6.21it/s]Epoch: 3, train for the 30-th batch, train loss: 0.46409764885902405:  25%|███         | 30/119 [00:04<00:12,  7.38it/s]evaluate for the 36-th batch, evaluate loss: 0.508226215839386:  47%|████████▉          | 34/72 [00:03<00:03, 10.55it/s]evaluate for the 36-th batch, evaluate loss: 0.508226215839386:  50%|█████████▌         | 36/72 [00:03<00:03, 10.41it/s]Epoch: 2, train for the 74-th batch, train loss: 0.4900231957435608:  49%|██████▎      | 74/151 [00:10<00:12,  6.27it/s]Epoch: 1, train for the 207-th batch, train loss: 0.4700649082660675:  54%|█████▉     | 207/383 [00:53<00:59,  2.98it/s]evaluate for the 37-th batch, evaluate loss: 0.49487051367759705:  50%|████████▌        | 36/72 [00:03<00:03, 10.41it/s]Epoch: 2, train for the 144-th batch, train loss: 0.47848406434059143:  98%|█████████▊| 143/146 [00:20<00:00,  5.89it/s]Epoch: 2, train for the 144-th batch, train loss: 0.47848406434059143:  99%|█████████▊| 144/146 [00:20<00:00,  6.17it/s]Epoch: 3, train for the 31-th batch, train loss: 0.47293493151664734:  25%|███         | 30/119 [00:04<00:12,  7.38it/s]Epoch: 3, train for the 31-th batch, train loss: 0.47293493151664734:  26%|███▏        | 31/119 [00:04<00:12,  7.28it/s]evaluate for the 38-th batch, evaluate loss: 0.4098457098007202:  50%|█████████         | 36/72 [00:03<00:03, 10.41it/s]evaluate for the 38-th batch, evaluate loss: 0.4098457098007202:  53%|█████████▌        | 38/72 [00:03<00:03, 11.20it/s]Epoch: 1, train for the 218-th batch, train loss: 0.5635077357292175:  92%|██████████ | 217/237 [00:55<00:05,  3.59it/s]Epoch: 2, train for the 75-th batch, train loss: 0.541479229927063:  49%|██████▊       | 74/151 [00:11<00:12,  6.27it/s]Epoch: 1, train for the 218-th batch, train loss: 0.5635077357292175:  92%|██████████ | 218/237 [00:55<00:05,  3.72it/s]Epoch: 2, train for the 75-th batch, train loss: 0.541479229927063:  50%|██████▉       | 75/151 [00:11<00:12,  5.95it/s]evaluate for the 39-th batch, evaluate loss: 0.4335821866989136:  53%|█████████▌        | 38/72 [00:04<00:03, 11.20it/s]Epoch: 2, train for the 145-th batch, train loss: 0.514867901802063:  99%|███████████▊| 144/146 [00:20<00:00,  6.17it/s]Epoch: 2, train for the 145-th batch, train loss: 0.514867901802063:  99%|███████████▉| 145/146 [00:20<00:00,  6.42it/s]evaluate for the 40-th batch, evaluate loss: 0.4915093779563904:  53%|█████████▌        | 38/72 [00:04<00:03, 11.20it/s]evaluate for the 40-th batch, evaluate loss: 0.4915093779563904:  56%|██████████        | 40/72 [00:04<00:02, 12.37it/s]Epoch: 3, train for the 32-th batch, train loss: 0.4312589168548584:  26%|███▍         | 31/119 [00:04<00:12,  7.28it/s]Epoch: 1, train for the 208-th batch, train loss: 0.5035117864608765:  54%|█████▉     | 207/383 [00:53<00:59,  2.98it/s]Epoch: 3, train for the 32-th batch, train loss: 0.4312589168548584:  27%|███▍         | 32/119 [00:04<00:12,  7.20it/s]Epoch: 1, train for the 208-th batch, train loss: 0.5035117864608765:  54%|█████▉     | 208/383 [00:53<00:55,  3.14it/s]Epoch: 2, train for the 146-th batch, train loss: 0.4708483815193176:  99%|██████████▉| 145/146 [00:21<00:00,  6.42it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5440883040428162:  50%|██████▍      | 75/151 [00:11<00:12,  5.95it/s]Epoch: 2, train for the 146-th batch, train loss: 0.4708483815193176: 100%|███████████| 146/146 [00:21<00:00,  6.78it/s]Epoch: 2, train for the 146-th batch, train loss: 0.4708483815193176: 100%|███████████| 146/146 [00:21<00:00,  6.92it/s]
evaluate for the 41-th batch, evaluate loss: 0.4957638084888458:  56%|██████████        | 40/72 [00:04<00:02, 12.37it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5440883040428162:  50%|██████▌      | 76/151 [00:11<00:12,  6.02it/s]Epoch: 1, train for the 219-th batch, train loss: 0.5554378628730774:  92%|██████████ | 218/237 [00:55<00:05,  3.72it/s]Epoch: 1, train for the 219-th batch, train loss: 0.5554378628730774:  92%|██████████▏| 219/237 [00:55<00:04,  3.98it/s]evaluate for the 42-th batch, evaluate loss: 0.4906662702560425:  56%|██████████        | 40/72 [00:04<00:02, 12.37it/s]evaluate for the 42-th batch, evaluate loss: 0.4906662702560425:  58%|██████████▌       | 42/72 [00:04<00:02, 12.70it/s]Epoch: 3, train for the 33-th batch, train loss: 0.5009949207305908:  27%|███▍         | 32/119 [00:04<00:12,  7.20it/s]Epoch: 3, train for the 33-th batch, train loss: 0.5009949207305908:  28%|███▌         | 33/119 [00:04<00:12,  6.86it/s]Epoch: 2, train for the 77-th batch, train loss: 0.4999077618122101:  50%|██████▌      | 76/151 [00:11<00:12,  6.02it/s]evaluate for the 43-th batch, evaluate loss: 0.4957664906978607:  58%|██████████▌       | 42/72 [00:04<00:02, 12.70it/s]Epoch: 2, train for the 77-th batch, train loss: 0.4999077618122101:  51%|██████▋      | 77/151 [00:11<00:11,  6.28it/s]Epoch: 3, train for the 34-th batch, train loss: 0.46228280663490295:  28%|███▎        | 33/119 [00:04<00:12,  6.86it/s]evaluate for the 44-th batch, evaluate loss: 0.49310624599456787:  58%|█████████▉       | 42/72 [00:04<00:02, 12.70it/s]evaluate for the 44-th batch, evaluate loss: 0.49310624599456787:  61%|██████████▍      | 44/72 [00:04<00:02, 13.54it/s]Epoch: 1, train for the 209-th batch, train loss: 0.4292042553424835:  54%|█████▉     | 208/383 [00:53<00:55,  3.14it/s]  0%|                                                                                            | 0/38 [00:00<?, ?it/s]Epoch: 1, train for the 220-th batch, train loss: 0.5795475244522095:  92%|██████████▏| 219/237 [00:55<00:04,  3.98it/s]Epoch: 1, train for the 209-th batch, train loss: 0.4292042553424835:  55%|██████     | 209/383 [00:53<00:53,  3.23it/s]evaluate for the 1-th batch, evaluate loss: 0.4739793539047241:   0%|                            | 0/38 [00:00<?, ?it/s]evaluate for the 45-th batch, evaluate loss: 0.48344069719314575:  61%|██████████▍      | 44/72 [00:04<00:02, 13.54it/s]Epoch: 1, train for the 220-th batch, train loss: 0.5795475244522095:  93%|██████████▏| 220/237 [00:55<00:04,  4.10it/s]Epoch: 3, train for the 35-th batch, train loss: 0.454731285572052:  28%|███▉          | 33/119 [00:05<00:12,  6.86it/s]Epoch: 2, train for the 78-th batch, train loss: 0.503480076789856:  51%|███████▏      | 77/151 [00:11<00:11,  6.28it/s]Epoch: 3, train for the 35-th batch, train loss: 0.454731285572052:  29%|████          | 35/119 [00:05<00:11,  7.52it/s]evaluate for the 2-th batch, evaluate loss: 0.49809733033180237:   0%|                           | 0/38 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.49809733033180237:   5%|█                  | 2/38 [00:00<00:02, 15.59it/s]evaluate for the 46-th batch, evaluate loss: 0.5348596572875977:  61%|███████████       | 44/72 [00:04<00:02, 13.54it/s]evaluate for the 46-th batch, evaluate loss: 0.5348596572875977:  64%|███████████▌      | 46/72 [00:04<00:02, 12.92it/s]Epoch: 2, train for the 78-th batch, train loss: 0.503480076789856:  52%|███████▏      | 78/151 [00:11<00:12,  5.66it/s]evaluate for the 3-th batch, evaluate loss: 0.47214311361312866:   5%|█                  | 2/38 [00:00<00:02, 15.59it/s]evaluate for the 47-th batch, evaluate loss: 0.367186039686203:  64%|████████████▏      | 46/72 [00:04<00:02, 12.92it/s]Epoch: 3, train for the 36-th batch, train loss: 0.4497966170310974:  29%|███▊         | 35/119 [00:05<00:11,  7.52it/s]Epoch: 1, train for the 221-th batch, train loss: 0.5578527450561523:  93%|██████████▏| 220/237 [00:55<00:04,  4.10it/s]evaluate for the 4-th batch, evaluate loss: 0.48698362708091736:   5%|█                  | 2/38 [00:00<00:02, 15.59it/s]evaluate for the 4-th batch, evaluate loss: 0.48698362708091736:  11%|██                 | 4/38 [00:00<00:02, 13.37it/s]Epoch: 3, train for the 36-th batch, train loss: 0.4497966170310974:  30%|███▉         | 36/119 [00:05<00:11,  6.99it/s]evaluate for the 48-th batch, evaluate loss: 0.3245944082736969:  64%|███████████▌      | 46/72 [00:04<00:02, 12.92it/s]evaluate for the 48-th batch, evaluate loss: 0.3245944082736969:  67%|████████████      | 48/72 [00:04<00:01, 13.30it/s]Epoch: 1, train for the 221-th batch, train loss: 0.5578527450561523:  93%|██████████▎| 221/237 [00:56<00:03,  4.06it/s]Epoch: 1, train for the 210-th batch, train loss: 0.4782048761844635:  55%|██████     | 209/383 [00:54<00:53,  3.23it/s]Epoch: 2, train for the 79-th batch, train loss: 0.5878108739852905:  52%|██████▋      | 78/151 [00:11<00:12,  5.66it/s]evaluate for the 5-th batch, evaluate loss: 0.5361703634262085:  11%|██                  | 4/38 [00:00<00:02, 13.37it/s]evaluate for the 49-th batch, evaluate loss: 0.4483538866043091:  67%|████████████      | 48/72 [00:04<00:01, 13.30it/s]Epoch: 2, train for the 79-th batch, train loss: 0.5878108739852905:  52%|██████▊      | 79/151 [00:11<00:13,  5.44it/s]Epoch: 1, train for the 210-th batch, train loss: 0.4782048761844635:  55%|██████     | 210/383 [00:54<00:55,  3.13it/s]Epoch: 3, train for the 37-th batch, train loss: 0.5071945190429688:  30%|███▉         | 36/119 [00:05<00:11,  6.99it/s]Epoch: 3, train for the 37-th batch, train loss: 0.5071945190429688:  31%|████         | 37/119 [00:05<00:11,  7.37it/s]evaluate for the 50-th batch, evaluate loss: 0.40112003684043884:  67%|███████████▎     | 48/72 [00:04<00:01, 13.30it/s]evaluate for the 50-th batch, evaluate loss: 0.40112003684043884:  69%|███████████▊     | 50/72 [00:04<00:01, 13.59it/s]evaluate for the 6-th batch, evaluate loss: 0.4832804799079895:  11%|██                  | 4/38 [00:00<00:02, 13.37it/s]evaluate for the 6-th batch, evaluate loss: 0.4832804799079895:  16%|███▏                | 6/38 [00:00<00:02, 13.20it/s]evaluate for the 7-th batch, evaluate loss: 0.43847328424453735:  16%|███                | 6/38 [00:00<00:02, 13.20it/s]Epoch: 2, train for the 80-th batch, train loss: 0.5452489852905273:  52%|██████▊      | 79/151 [00:11<00:13,  5.44it/s]evaluate for the 51-th batch, evaluate loss: 0.4992128610610962:  69%|████████████▌     | 50/72 [00:04<00:01, 13.59it/s]Epoch: 1, train for the 222-th batch, train loss: 0.5672695636749268:  93%|██████████▎| 221/237 [00:56<00:03,  4.06it/s]Epoch: 2, train for the 80-th batch, train loss: 0.5452489852905273:  53%|██████▉      | 80/151 [00:11<00:12,  5.50it/s]Epoch: 3, train for the 38-th batch, train loss: 0.4374157190322876:  31%|████         | 37/119 [00:05<00:11,  7.37it/s]Epoch: 3, train for the 38-th batch, train loss: 0.4374157190322876:  32%|████▏        | 38/119 [00:05<00:11,  7.32it/s]evaluate for the 8-th batch, evaluate loss: 0.46098607778549194:  16%|███                | 6/38 [00:00<00:02, 13.20it/s]evaluate for the 8-th batch, evaluate loss: 0.46098607778549194:  21%|████               | 8/38 [00:00<00:02, 14.27it/s]evaluate for the 52-th batch, evaluate loss: 0.4436614215373993:  69%|████████████▌     | 50/72 [00:04<00:01, 13.59it/s]evaluate for the 52-th batch, evaluate loss: 0.4436614215373993:  72%|█████████████     | 52/72 [00:04<00:01, 13.43it/s]Epoch: 1, train for the 222-th batch, train loss: 0.5672695636749268:  94%|██████████▎| 222/237 [00:56<00:03,  3.91it/s]evaluate for the 9-th batch, evaluate loss: 0.4834119975566864:  21%|████▏               | 8/38 [00:00<00:02, 14.27it/s]Epoch: 1, train for the 211-th batch, train loss: 0.43062788248062134:  55%|█████▍    | 210/383 [00:54<00:55,  3.13it/s]Epoch: 3, train for the 39-th batch, train loss: 0.4839370548725128:  32%|████▏        | 38/119 [00:05<00:11,  7.32it/s]evaluate for the 53-th batch, evaluate loss: 0.4762301445007324:  72%|█████████████     | 52/72 [00:05<00:01, 13.43it/s]Epoch: 3, train for the 39-th batch, train loss: 0.4839370548725128:  33%|████▎        | 39/119 [00:05<00:11,  7.16it/s]evaluate for the 10-th batch, evaluate loss: 0.5132730603218079:  21%|████               | 8/38 [00:00<00:02, 14.27it/s]evaluate for the 10-th batch, evaluate loss: 0.5132730603218079:  26%|████▋             | 10/38 [00:00<00:01, 14.57it/s]Epoch: 1, train for the 211-th batch, train loss: 0.43062788248062134:  55%|█████▌    | 211/383 [00:54<00:54,  3.13it/s]Epoch: 2, train for the 81-th batch, train loss: 0.5216326713562012:  53%|██████▉      | 80/151 [00:12<00:12,  5.50it/s]Epoch: 2, train for the 81-th batch, train loss: 0.5216326713562012:  54%|██████▉      | 81/151 [00:12<00:13,  5.37it/s]evaluate for the 54-th batch, evaluate loss: 0.5208272337913513:  72%|█████████████     | 52/72 [00:05<00:01, 13.43it/s]evaluate for the 54-th batch, evaluate loss: 0.5208272337913513:  75%|█████████████▌    | 54/72 [00:05<00:01, 12.79it/s]Epoch: 1, train for the 223-th batch, train loss: 0.5584854483604431:  94%|██████████▎| 222/237 [00:56<00:03,  3.91it/s]evaluate for the 11-th batch, evaluate loss: 0.45872828364372253:  26%|████▍            | 10/38 [00:00<00:01, 14.57it/s]Epoch: 3, train for the 40-th batch, train loss: 0.4695214629173279:  33%|████▎        | 39/119 [00:05<00:11,  7.16it/s]Epoch: 1, train for the 223-th batch, train loss: 0.5584854483604431:  94%|██████████▎| 223/237 [00:56<00:03,  3.99it/s]Epoch: 3, train for the 40-th batch, train loss: 0.4695214629173279:  34%|████▎        | 40/119 [00:05<00:11,  7.04it/s]evaluate for the 55-th batch, evaluate loss: 0.4872833490371704:  75%|█████████████▌    | 54/72 [00:05<00:01, 12.79it/s]evaluate for the 12-th batch, evaluate loss: 0.533649742603302:  26%|█████              | 10/38 [00:00<00:01, 14.57it/s]evaluate for the 12-th batch, evaluate loss: 0.533649742603302:  32%|██████             | 12/38 [00:00<00:01, 13.69it/s]Epoch: 2, train for the 82-th batch, train loss: 0.5206708312034607:  54%|██████▉      | 81/151 [00:12<00:13,  5.37it/s]evaluate for the 13-th batch, evaluate loss: 0.4960498809814453:  32%|█████▋            | 12/38 [00:00<00:01, 13.69it/s]evaluate for the 56-th batch, evaluate loss: 0.49224281311035156:  75%|████████████▊    | 54/72 [00:05<00:01, 12.79it/s]evaluate for the 56-th batch, evaluate loss: 0.49224281311035156:  78%|█████████████▏   | 56/72 [00:05<00:01, 12.40it/s]Epoch: 2, train for the 82-th batch, train loss: 0.5206708312034607:  54%|███████      | 82/151 [00:12<00:13,  5.17it/s]Epoch: 1, train for the 212-th batch, train loss: 0.5645337104797363:  55%|██████     | 211/383 [00:54<00:54,  3.13it/s]Epoch: 3, train for the 41-th batch, train loss: 0.4723705053329468:  34%|████▎        | 40/119 [00:05<00:11,  7.04it/s]Epoch: 3, train for the 41-th batch, train loss: 0.4723705053329468:  34%|████▍        | 41/119 [00:05<00:10,  7.11it/s]evaluate for the 14-th batch, evaluate loss: 0.43076422810554504:  32%|█████▎           | 12/38 [00:00<00:01, 13.69it/s]evaluate for the 14-th batch, evaluate loss: 0.43076422810554504:  37%|██████▎          | 14/38 [00:00<00:01, 14.06it/s]evaluate for the 57-th batch, evaluate loss: 0.4410130977630615:  78%|██████████████    | 56/72 [00:05<00:01, 12.40it/s]Epoch: 1, train for the 212-th batch, train loss: 0.5645337104797363:  55%|██████     | 212/383 [00:54<00:53,  3.19it/s]Epoch: 1, train for the 224-th batch, train loss: 0.5847759246826172:  94%|██████████▎| 223/237 [00:56<00:03,  3.99it/s]evaluate for the 15-th batch, evaluate loss: 0.4516344666481018:  37%|██████▋           | 14/38 [00:01<00:01, 14.06it/s]Epoch: 1, train for the 224-th batch, train loss: 0.5847759246826172:  95%|██████████▍| 224/237 [00:56<00:03,  3.97it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5260310769081116:  54%|███████      | 82/151 [00:12<00:13,  5.17it/s]evaluate for the 58-th batch, evaluate loss: 0.5023624897003174:  78%|██████████████    | 56/72 [00:05<00:01, 12.40it/s]evaluate for the 58-th batch, evaluate loss: 0.5023624897003174:  81%|██████████████▌   | 58/72 [00:05<00:01, 12.22it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5260310769081116:  55%|███████▏     | 83/151 [00:12<00:12,  5.34it/s]Epoch: 3, train for the 42-th batch, train loss: 0.507745623588562:  34%|████▊         | 41/119 [00:06<00:10,  7.11it/s]evaluate for the 16-th batch, evaluate loss: 0.5177401304244995:  37%|██████▋           | 14/38 [00:01<00:01, 14.06it/s]evaluate for the 16-th batch, evaluate loss: 0.5177401304244995:  42%|███████▌          | 16/38 [00:01<00:01, 14.01it/s]Epoch: 3, train for the 42-th batch, train loss: 0.507745623588562:  35%|████▉         | 42/119 [00:06<00:11,  6.76it/s]evaluate for the 59-th batch, evaluate loss: 0.4755411446094513:  81%|██████████████▌   | 58/72 [00:05<00:01, 12.22it/s]Epoch: 1, train for the 213-th batch, train loss: 0.545289933681488:  55%|██████▋     | 212/383 [00:55<00:53,  3.19it/s]evaluate for the 17-th batch, evaluate loss: 0.4709429144859314:  42%|███████▌          | 16/38 [00:01<00:01, 14.01it/s]evaluate for the 60-th batch, evaluate loss: 0.4844411313533783:  81%|██████████████▌   | 58/72 [00:05<00:01, 12.22it/s]evaluate for the 60-th batch, evaluate loss: 0.4844411313533783:  83%|███████████████   | 60/72 [00:05<00:00, 12.43it/s]Epoch: 3, train for the 43-th batch, train loss: 0.4835120737552643:  35%|████▌        | 42/119 [00:06<00:11,  6.76it/s]Epoch: 3, train for the 43-th batch, train loss: 0.4835120737552643:  36%|████▋        | 43/119 [00:06<00:10,  6.92it/s]evaluate for the 18-th batch, evaluate loss: 0.5077822804450989:  42%|███████▌          | 16/38 [00:01<00:01, 14.01it/s]evaluate for the 18-th batch, evaluate loss: 0.5077822804450989:  47%|████████▌         | 18/38 [00:01<00:01, 14.02it/s]Epoch: 1, train for the 213-th batch, train loss: 0.545289933681488:  56%|██████▋     | 213/383 [00:55<00:51,  3.29it/s]Epoch: 1, train for the 225-th batch, train loss: 0.5740287899971008:  95%|██████████▍| 224/237 [00:57<00:03,  3.97it/s]evaluate for the 61-th batch, evaluate loss: 0.45263800024986267:  83%|██████████████▏  | 60/72 [00:05<00:00, 12.43it/s]Epoch: 1, train for the 225-th batch, train loss: 0.5740287899971008:  95%|██████████▍| 225/237 [00:57<00:03,  3.98it/s]evaluate for the 19-th batch, evaluate loss: 0.49207350611686707:  47%|████████         | 18/38 [00:01<00:01, 14.02it/s]Epoch: 3, train for the 44-th batch, train loss: 0.4343799948692322:  36%|████▋        | 43/119 [00:06<00:10,  6.92it/s]Epoch: 3, train for the 44-th batch, train loss: 0.4343799948692322:  37%|████▊        | 44/119 [00:06<00:10,  7.15it/s]evaluate for the 20-th batch, evaluate loss: 0.4286554753780365:  47%|████████▌         | 18/38 [00:01<00:01, 14.02it/s]evaluate for the 20-th batch, evaluate loss: 0.4286554753780365:  53%|█████████▍        | 20/38 [00:01<00:01, 14.42it/s]evaluate for the 62-th batch, evaluate loss: 0.4515817165374756:  83%|███████████████   | 60/72 [00:05<00:00, 12.43it/s]evaluate for the 62-th batch, evaluate loss: 0.4515817165374756:  86%|███████████████▌  | 62/72 [00:05<00:00, 12.34it/s]Epoch: 2, train for the 84-th batch, train loss: 0.565078854560852:  55%|███████▋      | 83/151 [00:12<00:12,  5.34it/s]Epoch: 2, train for the 84-th batch, train loss: 0.565078854560852:  56%|███████▊      | 84/151 [00:12<00:15,  4.27it/s]evaluate for the 21-th batch, evaluate loss: 0.44088828563690186:  53%|████████▉        | 20/38 [00:01<00:01, 14.42it/s]evaluate for the 63-th batch, evaluate loss: 0.4286191463470459:  86%|███████████████▌  | 62/72 [00:05<00:00, 12.34it/s]evaluate for the 64-th batch, evaluate loss: 0.5278446078300476:  86%|███████████████▌  | 62/72 [00:05<00:00, 12.34it/s]evaluate for the 64-th batch, evaluate loss: 0.5278446078300476:  89%|████████████████  | 64/72 [00:05<00:00, 13.08it/s]Epoch: 3, train for the 45-th batch, train loss: 0.45123225450515747:  37%|████▍       | 44/119 [00:06<00:10,  7.15it/s]Epoch: 1, train for the 214-th batch, train loss: 0.4641771614551544:  56%|██████     | 213/383 [00:55<00:51,  3.29it/s]Epoch: 3, train for the 45-th batch, train loss: 0.45123225450515747:  38%|████▌       | 45/119 [00:06<00:10,  6.96it/s]evaluate for the 22-th batch, evaluate loss: 0.48511791229248047:  53%|████████▉        | 20/38 [00:01<00:01, 14.42it/s]evaluate for the 22-th batch, evaluate loss: 0.48511791229248047:  58%|█████████▊       | 22/38 [00:01<00:01, 14.10it/s]Epoch: 1, train for the 226-th batch, train loss: 0.591086745262146:  95%|███████████▍| 225/237 [00:57<00:03,  3.98it/s]Epoch: 2, train for the 85-th batch, train loss: 0.4900067448616028:  56%|███████▏     | 84/151 [00:13<00:15,  4.27it/s]evaluate for the 65-th batch, evaluate loss: 0.5138256549835205:  89%|████████████████  | 64/72 [00:06<00:00, 13.08it/s]Epoch: 1, train for the 214-th batch, train loss: 0.4641771614551544:  56%|██████▏    | 214/383 [00:55<00:52,  3.21it/s]Epoch: 2, train for the 85-th batch, train loss: 0.4900067448616028:  56%|███████▎     | 85/151 [00:13<00:14,  4.66it/s]evaluate for the 23-th batch, evaluate loss: 0.47494253516197205:  58%|█████████▊       | 22/38 [00:01<00:01, 14.10it/s]Epoch: 1, train for the 226-th batch, train loss: 0.591086745262146:  95%|███████████▍| 226/237 [00:57<00:03,  3.64it/s]Epoch: 3, train for the 46-th batch, train loss: 0.4771568179130554:  38%|████▉        | 45/119 [00:06<00:10,  6.96it/s]evaluate for the 66-th batch, evaluate loss: 0.4481455981731415:  89%|████████████████  | 64/72 [00:06<00:00, 13.08it/s]evaluate for the 66-th batch, evaluate loss: 0.4481455981731415:  92%|████████████████▌ | 66/72 [00:06<00:00, 13.88it/s]Epoch: 3, train for the 46-th batch, train loss: 0.4771568179130554:  39%|█████        | 46/119 [00:06<00:10,  7.29it/s]evaluate for the 24-th batch, evaluate loss: 0.4552414119243622:  58%|██████████▍       | 22/38 [00:01<00:01, 14.10it/s]evaluate for the 24-th batch, evaluate loss: 0.4552414119243622:  63%|███████████▎      | 24/38 [00:01<00:00, 14.24it/s]evaluate for the 67-th batch, evaluate loss: 0.4605470895767212:  92%|████████████████▌ | 66/72 [00:06<00:00, 13.88it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5082495212554932:  56%|███████▎     | 85/151 [00:13<00:14,  4.66it/s]evaluate for the 25-th batch, evaluate loss: 0.49168500304222107:  63%|██████████▋      | 24/38 [00:01<00:00, 14.24it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5082495212554932:  57%|███████▍     | 86/151 [00:13<00:13,  4.74it/s]Epoch: 3, train for the 47-th batch, train loss: 0.5537055134773254:  39%|█████        | 46/119 [00:06<00:10,  7.29it/s]evaluate for the 68-th batch, evaluate loss: 0.4549979269504547:  92%|████████████████▌ | 66/72 [00:06<00:00, 13.88it/s]evaluate for the 68-th batch, evaluate loss: 0.4549979269504547:  94%|█████████████████ | 68/72 [00:06<00:00, 13.32it/s]Epoch: 3, train for the 47-th batch, train loss: 0.5537055134773254:  39%|█████▏       | 47/119 [00:06<00:10,  6.95it/s]evaluate for the 26-th batch, evaluate loss: 0.4791783094406128:  63%|███████████▎      | 24/38 [00:01<00:00, 14.24it/s]evaluate for the 26-th batch, evaluate loss: 0.4791783094406128:  68%|████████████▎     | 26/38 [00:01<00:00, 12.92it/s]evaluate for the 69-th batch, evaluate loss: 0.46951788663864136:  94%|████████████████ | 68/72 [00:06<00:00, 13.32it/s]Epoch: 1, train for the 215-th batch, train loss: 0.3953937590122223:  56%|██████▏    | 214/383 [00:55<00:52,  3.21it/s]Epoch: 1, train for the 227-th batch, train loss: 0.5469340085983276:  95%|██████████▍| 226/237 [00:57<00:03,  3.64it/s]Epoch: 1, train for the 215-th batch, train loss: 0.3953937590122223:  56%|██████▏    | 215/383 [00:55<00:54,  3.11it/s]evaluate for the 27-th batch, evaluate loss: 0.4777672588825226:  68%|████████████▎     | 26/38 [00:01<00:00, 12.92it/s]Epoch: 2, train for the 87-th batch, train loss: 0.4942648708820343:  57%|███████▍     | 86/151 [00:13<00:13,  4.74it/s]Epoch: 1, train for the 227-th batch, train loss: 0.5469340085983276:  96%|██████████▌| 227/237 [00:57<00:02,  3.50it/s]evaluate for the 70-th batch, evaluate loss: 0.4670318365097046:  94%|█████████████████ | 68/72 [00:06<00:00, 13.32it/s]evaluate for the 70-th batch, evaluate loss: 0.4670318365097046:  97%|█████████████████▌| 70/72 [00:06<00:00, 13.39it/s]Epoch: 3, train for the 48-th batch, train loss: 0.50596684217453:  39%|█████▉         | 47/119 [00:06<00:10,  6.95it/s]Epoch: 2, train for the 87-th batch, train loss: 0.4942648708820343:  58%|███████▍     | 87/151 [00:13<00:12,  4.99it/s]Epoch: 3, train for the 48-th batch, train loss: 0.50596684217453:  40%|██████         | 48/119 [00:06<00:10,  6.66it/s]evaluate for the 28-th batch, evaluate loss: 0.4993583858013153:  68%|████████████▎     | 26/38 [00:02<00:00, 12.92it/s]evaluate for the 28-th batch, evaluate loss: 0.4993583858013153:  74%|█████████████▎    | 28/38 [00:02<00:00, 13.50it/s]evaluate for the 71-th batch, evaluate loss: 0.5101353526115417:  97%|█████████████████▌| 70/72 [00:06<00:00, 13.39it/s]evaluate for the 29-th batch, evaluate loss: 0.46868133544921875:  74%|████████████▌    | 28/38 [00:02<00:00, 13.50it/s]Epoch: 3, train for the 49-th batch, train loss: 0.4746231734752655:  40%|█████▏       | 48/119 [00:07<00:10,  6.66it/s]Epoch: 3, train for the 49-th batch, train loss: 0.4746231734752655:  41%|█████▎       | 49/119 [00:07<00:10,  6.71it/s]evaluate for the 72-th batch, evaluate loss: 0.5602645874023438:  97%|█████████████████▌| 70/72 [00:06<00:00, 13.39it/s]evaluate for the 72-th batch, evaluate loss: 0.5602645874023438: 100%|██████████████████| 72/72 [00:06<00:00, 12.50it/s]evaluate for the 72-th batch, evaluate loss: 0.5602645874023438: 100%|██████████████████| 72/72 [00:06<00:00, 10.95it/s]
evaluate for the 30-th batch, evaluate loss: 0.5008180737495422:  74%|█████████████▎    | 28/38 [00:02<00:00, 13.50it/s]evaluate for the 30-th batch, evaluate loss: 0.5008180737495422:  79%|██████████████▏   | 30/38 [00:02<00:00, 13.11it/s]Epoch: 1, train for the 216-th batch, train loss: 0.453571617603302:  56%|██████▋     | 215/383 [00:56<00:54,  3.11it/s]Epoch: 2, train for the 88-th batch, train loss: 0.5455291271209717:  58%|███████▍     | 87/151 [00:13<00:12,  4.99it/s]Epoch: 2, train for the 88-th batch, train loss: 0.5455291271209717:  58%|███████▌     | 88/151 [00:13<00:13,  4.69it/s]Epoch: 1, train for the 228-th batch, train loss: 0.5394229888916016:  96%|██████████▌| 227/237 [00:57<00:02,  3.50it/s]evaluate for the 31-th batch, evaluate loss: 0.47175654768943787:  79%|█████████████▍   | 30/38 [00:02<00:00, 13.11it/s]Epoch: 1, train for the 216-th batch, train loss: 0.453571617603302:  56%|██████▊     | 216/383 [00:56<00:53,  3.12it/s]Epoch: 3, train for the 50-th batch, train loss: 0.510267972946167:  41%|█████▊        | 49/119 [00:07<00:10,  6.71it/s]Epoch: 3, train for the 50-th batch, train loss: 0.510267972946167:  42%|█████▉        | 50/119 [00:07<00:10,  6.78it/s]Epoch: 1, train for the 228-th batch, train loss: 0.5394229888916016:  96%|██████████▌| 228/237 [00:58<00:02,  3.38it/s]evaluate for the 32-th batch, evaluate loss: 0.4418356716632843:  79%|██████████████▏   | 30/38 [00:02<00:00, 13.11it/s]evaluate for the 32-th batch, evaluate loss: 0.4418356716632843:  84%|███████████████▏  | 32/38 [00:02<00:00, 12.99it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5305291414260864:  58%|███████▌     | 88/151 [00:13<00:13,  4.69it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5305291414260864:  59%|███████▋     | 89/151 [00:13<00:12,  4.97it/s]Epoch: 3, train for the 51-th batch, train loss: 0.5041484832763672:  42%|█████▍       | 50/119 [00:07<00:10,  6.78it/s]evaluate for the 33-th batch, evaluate loss: 0.47322332859039307:  84%|██████████████▎  | 32/38 [00:02<00:00, 12.99it/s]Epoch: 3, train for the 51-th batch, train loss: 0.5041484832763672:  43%|█████▌       | 51/119 [00:07<00:09,  6.89it/s]evaluate for the 34-th batch, evaluate loss: 0.4824378490447998:  84%|███████████████▏  | 32/38 [00:02<00:00, 12.99it/s]evaluate for the 34-th batch, evaluate loss: 0.4824378490447998:  89%|████████████████  | 34/38 [00:02<00:00, 12.74it/s]Epoch: 1, train for the 217-th batch, train loss: 0.4752725064754486:  56%|██████▏    | 216/383 [00:56<00:53,  3.12it/s]Epoch: 1, train for the 229-th batch, train loss: 0.612689733505249:  96%|███████████▌| 228/237 [00:58<00:02,  3.38it/s]Epoch: 3, train for the 52-th batch, train loss: 0.528115451335907:  43%|██████        | 51/119 [00:07<00:09,  6.89it/s]evaluate for the 35-th batch, evaluate loss: 0.49359405040740967:  89%|███████████████▏ | 34/38 [00:02<00:00, 12.74it/s]Epoch: 3, train for the 52-th batch, train loss: 0.528115451335907:  44%|██████        | 52/119 [00:07<00:09,  7.06it/s]Epoch: 1, train for the 217-th batch, train loss: 0.4752725064754486:  57%|██████▏    | 217/383 [00:56<00:52,  3.17it/s]Epoch: 1, train for the 229-th batch, train loss: 0.612689733505249:  97%|███████████▌| 229/237 [00:58<00:02,  3.39it/s]  0%|                                                                                            | 0/34 [00:00<?, ?it/s]Epoch: 2, train for the 90-th batch, train loss: 0.49822998046875:  59%|████████▊      | 89/151 [00:14<00:12,  4.97it/s]Epoch: 2, train for the 90-th batch, train loss: 0.49822998046875:  60%|████████▉      | 90/151 [00:14<00:12,  4.80it/s]evaluate for the 1-th batch, evaluate loss: 0.6148017644882202:   0%|                            | 0/34 [00:00<?, ?it/s]evaluate for the 36-th batch, evaluate loss: 0.508038341999054:  89%|█████████████████  | 34/38 [00:02<00:00, 12.74it/s]evaluate for the 36-th batch, evaluate loss: 0.508038341999054:  95%|██████████████████ | 36/38 [00:02<00:00, 12.14it/s]Epoch: 3, train for the 53-th batch, train loss: 0.47606557607650757:  44%|█████▏      | 52/119 [00:07<00:09,  7.06it/s]Epoch: 3, train for the 53-th batch, train loss: 0.47606557607650757:  45%|█████▎      | 53/119 [00:07<00:09,  6.89it/s]evaluate for the 2-th batch, evaluate loss: 0.6786507964134216:   0%|                            | 0/34 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.6786507964134216:   6%|█▏                  | 2/34 [00:00<00:02, 13.02it/s]evaluate for the 37-th batch, evaluate loss: 0.4372161030769348:  95%|█████████████████ | 36/38 [00:02<00:00, 12.14it/s]Epoch: 2, train for the 91-th batch, train loss: 0.49393004179000854:  60%|███████▏    | 90/151 [00:14<00:12,  4.80it/s]evaluate for the 38-th batch, evaluate loss: 0.4659723937511444:  95%|█████████████████ | 36/38 [00:02<00:00, 12.14it/s]evaluate for the 38-th batch, evaluate loss: 0.4659723937511444: 100%|██████████████████| 38/38 [00:02<00:00, 12.90it/s]evaluate for the 38-th batch, evaluate loss: 0.4659723937511444: 100%|██████████████████| 38/38 [00:02<00:00, 13.47it/s]
evaluate for the 3-th batch, evaluate loss: 0.6942522525787354:   6%|█▏                  | 2/34 [00:00<00:02, 13.02it/s]Epoch: 2, train for the 91-th batch, train loss: 0.49393004179000854:  60%|███████▏    | 91/151 [00:14<00:12,  4.96it/s]evaluate for the 4-th batch, evaluate loss: 0.6619051694869995:   6%|█▏                  | 2/34 [00:00<00:02, 13.02it/s]evaluate for the 4-th batch, evaluate loss: 0.6619051694869995:  12%|██▎                 | 4/34 [00:00<00:02, 13.88it/s]Epoch: 1, train for the 230-th batch, train loss: 0.5509843826293945:  97%|██████████▋| 229/237 [00:58<00:02,  3.39it/s]Epoch: 1, train for the 218-th batch, train loss: 0.4984152913093567:  57%|██████▏    | 217/383 [00:56<00:52,  3.17it/s]Epoch: 3, train for the 54-th batch, train loss: 0.4455098509788513:  45%|█████▊       | 53/119 [00:07<00:09,  6.89it/s]Epoch: 3, train for the 54-th batch, train loss: 0.4455098509788513:  45%|█████▉       | 54/119 [00:07<00:10,  6.31it/s]Epoch: 1, train for the 230-th batch, train loss: 0.5509843826293945:  97%|██████████▋| 230/237 [00:58<00:02,  3.21it/s]Epoch: 1, train for the 218-th batch, train loss: 0.4984152913093567:  57%|██████▎    | 218/383 [00:56<00:54,  3.03it/s]  0%|                                                                                            | 0/20 [00:00<?, ?it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5383563041687012:  60%|███████▊     | 91/151 [00:14<00:12,  4.96it/s]evaluate for the 5-th batch, evaluate loss: 0.655005931854248:  12%|██▍                  | 4/34 [00:00<00:02, 13.88it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5383563041687012:  61%|███████▉     | 92/151 [00:14<00:11,  5.21it/s]Epoch: 3, train for the 55-th batch, train loss: 0.4735327661037445:  45%|█████▉       | 54/119 [00:08<00:10,  6.31it/s]evaluate for the 1-th batch, evaluate loss: 0.7298118472099304:   0%|                            | 0/20 [00:00<?, ?it/s]evaluate for the 6-th batch, evaluate loss: 0.6024686694145203:  12%|██▎                 | 4/34 [00:00<00:02, 13.88it/s]evaluate for the 6-th batch, evaluate loss: 0.6024686694145203:  18%|███▌                | 6/34 [00:00<00:02, 13.26it/s]Epoch: 3, train for the 55-th batch, train loss: 0.4735327661037445:  46%|██████       | 55/119 [00:08<00:09,  6.56it/s]evaluate for the 2-th batch, evaluate loss: 0.7649415135383606:   0%|                            | 0/20 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.7649415135383606:  10%|██                  | 2/20 [00:00<00:01, 12.66it/s]evaluate for the 7-th batch, evaluate loss: 0.5673010349273682:  18%|███▌                | 6/34 [00:00<00:02, 13.26it/s]Epoch: 1, train for the 231-th batch, train loss: 0.5708793997764587:  97%|██████████▋| 230/237 [00:58<00:02,  3.21it/s]Epoch: 2, train for the 93-th batch, train loss: 0.5619528889656067:  61%|███████▉     | 92/151 [00:14<00:11,  5.21it/s]Epoch: 1, train for the 231-th batch, train loss: 0.5708793997764587:  97%|██████████▋| 231/237 [00:58<00:01,  3.42it/s]Epoch: 1, train for the 219-th batch, train loss: 0.3343454599380493:  57%|██████▎    | 218/383 [00:56<00:54,  3.03it/s]Epoch: 2, train for the 93-th batch, train loss: 0.5619528889656067:  62%|████████     | 93/151 [00:14<00:11,  5.12it/s]evaluate for the 8-th batch, evaluate loss: 0.679881751537323:  18%|███▋                 | 6/34 [00:00<00:02, 13.26it/s]evaluate for the 8-th batch, evaluate loss: 0.679881751537323:  24%|████▉                | 8/34 [00:00<00:01, 13.06it/s]evaluate for the 3-th batch, evaluate loss: 0.6344153881072998:  10%|██                  | 2/20 [00:00<00:01, 12.66it/s]Epoch: 3, train for the 56-th batch, train loss: 0.47183603048324585:  46%|█████▌      | 55/119 [00:08<00:09,  6.56it/s]Epoch: 3, train for the 56-th batch, train loss: 0.47183603048324585:  47%|█████▋      | 56/119 [00:08<00:10,  6.15it/s]Epoch: 1, train for the 219-th batch, train loss: 0.3343454599380493:  57%|██████▎    | 219/383 [00:57<00:53,  3.07it/s]evaluate for the 4-th batch, evaluate loss: 0.6685462594032288:  10%|██                  | 2/20 [00:00<00:01, 12.66it/s]evaluate for the 4-th batch, evaluate loss: 0.6685462594032288:  20%|████                | 4/20 [00:00<00:01, 12.65it/s]evaluate for the 9-th batch, evaluate loss: 0.5876103639602661:  24%|████▋               | 8/34 [00:00<00:01, 13.06it/s]Epoch: 2, train for the 94-th batch, train loss: 0.5392810106277466:  62%|████████     | 93/151 [00:14<00:11,  5.12it/s]Epoch: 1, train for the 232-th batch, train loss: 0.586107611656189:  97%|███████████▋| 231/237 [00:59<00:01,  3.42it/s]Epoch: 3, train for the 57-th batch, train loss: 0.42285552620887756:  47%|█████▋      | 56/119 [00:08<00:10,  6.15it/s]Epoch: 2, train for the 94-th batch, train loss: 0.5392810106277466:  62%|████████     | 94/151 [00:14<00:11,  5.12it/s]evaluate for the 5-th batch, evaluate loss: 0.6594778895378113:  20%|████                | 4/20 [00:00<00:01, 12.65it/s]evaluate for the 10-th batch, evaluate loss: 0.6381195783615112:  24%|████▍              | 8/34 [00:00<00:01, 13.06it/s]evaluate for the 10-th batch, evaluate loss: 0.6381195783615112:  29%|█████▎            | 10/34 [00:00<00:02, 11.76it/s]Epoch: 3, train for the 57-th batch, train loss: 0.42285552620887756:  48%|█████▋      | 57/119 [00:08<00:10,  6.04it/s]Epoch: 1, train for the 232-th batch, train loss: 0.586107611656189:  98%|███████████▋| 232/237 [00:59<00:01,  3.68it/s]evaluate for the 6-th batch, evaluate loss: 0.7248685956001282:  20%|████                | 4/20 [00:00<00:01, 12.65it/s]evaluate for the 6-th batch, evaluate loss: 0.7248685956001282:  30%|██████              | 6/20 [00:00<00:01, 12.26it/s]Epoch: 1, train for the 220-th batch, train loss: 0.39868563413619995:  57%|█████▋    | 219/383 [00:57<00:53,  3.07it/s]evaluate for the 11-th batch, evaluate loss: 0.6662580966949463:  29%|█████▎            | 10/34 [00:00<00:02, 11.76it/s]Epoch: 1, train for the 220-th batch, train loss: 0.39868563413619995:  57%|█████▋    | 220/383 [00:57<00:50,  3.24it/s]Epoch: 2, train for the 95-th batch, train loss: 0.47811439633369446:  62%|███████▍    | 94/151 [00:14<00:11,  5.12it/s]Epoch: 3, train for the 58-th batch, train loss: 0.46624454855918884:  48%|█████▋      | 57/119 [00:08<00:10,  6.04it/s]evaluate for the 12-th batch, evaluate loss: 0.6150602102279663:  29%|█████▎            | 10/34 [00:00<00:02, 11.76it/s]Epoch: 3, train for the 58-th batch, train loss: 0.46624454855918884:  49%|█████▊      | 58/119 [00:08<00:09,  6.12it/s]evaluate for the 12-th batch, evaluate loss: 0.6150602102279663:  35%|██████▎           | 12/34 [00:00<00:01, 11.97it/s]evaluate for the 7-th batch, evaluate loss: 0.7444047331809998:  30%|██████              | 6/20 [00:00<00:01, 12.26it/s]Epoch: 2, train for the 95-th batch, train loss: 0.47811439633369446:  63%|███████▌    | 95/151 [00:15<00:10,  5.26it/s]Epoch: 1, train for the 233-th batch, train loss: 0.5831287503242493:  98%|██████████▊| 232/237 [00:59<00:01,  3.68it/s]evaluate for the 13-th batch, evaluate loss: 0.5686484575271606:  35%|██████▎           | 12/34 [00:01<00:01, 11.97it/s]evaluate for the 8-th batch, evaluate loss: 0.6952328085899353:  30%|██████              | 6/20 [00:00<00:01, 12.26it/s]evaluate for the 8-th batch, evaluate loss: 0.6952328085899353:  40%|████████            | 8/20 [00:00<00:01, 11.55it/s]Epoch: 1, train for the 233-th batch, train loss: 0.5831287503242493:  98%|██████████▊| 233/237 [00:59<00:01,  3.73it/s]evaluate for the 14-th batch, evaluate loss: 0.5523385405540466:  35%|██████▎           | 12/34 [00:01<00:01, 11.97it/s]evaluate for the 14-th batch, evaluate loss: 0.5523385405540466:  41%|███████▍          | 14/34 [00:01<00:01, 12.91it/s]Epoch: 3, train for the 59-th batch, train loss: 0.47235575318336487:  49%|█████▊      | 58/119 [00:08<00:09,  6.12it/s]Epoch: 3, train for the 59-th batch, train loss: 0.47235575318336487:  50%|█████▉      | 59/119 [00:08<00:09,  6.20it/s]evaluate for the 9-th batch, evaluate loss: 0.6371906399726868:  40%|████████            | 8/20 [00:00<00:01, 11.55it/s]Epoch: 2, train for the 96-th batch, train loss: 0.534807562828064:  63%|████████▊     | 95/151 [00:15<00:10,  5.26it/s]Epoch: 1, train for the 221-th batch, train loss: 0.4477849006652832:  57%|██████▎    | 220/383 [00:57<00:50,  3.24it/s]evaluate for the 15-th batch, evaluate loss: 0.6734853982925415:  41%|███████▍          | 14/34 [00:01<00:01, 12.91it/s]Epoch: 2, train for the 96-th batch, train loss: 0.534807562828064:  64%|████████▉     | 96/151 [00:15<00:10,  5.24it/s]evaluate for the 10-th batch, evaluate loss: 0.6405945420265198:  40%|███████▌           | 8/20 [00:00<00:01, 11.55it/s]evaluate for the 10-th batch, evaluate loss: 0.6405945420265198:  50%|█████████         | 10/20 [00:00<00:00, 12.14it/s]Epoch: 1, train for the 221-th batch, train loss: 0.4477849006652832:  58%|██████▎    | 221/383 [00:57<00:48,  3.36it/s]evaluate for the 16-th batch, evaluate loss: 0.6111915707588196:  41%|███████▍          | 14/34 [00:01<00:01, 12.91it/s]evaluate for the 16-th batch, evaluate loss: 0.6111915707588196:  47%|████████▍         | 16/34 [00:01<00:01, 13.17it/s]Epoch: 1, train for the 234-th batch, train loss: 0.5562204718589783:  98%|██████████▊| 233/237 [00:59<00:01,  3.73it/s]Epoch: 3, train for the 60-th batch, train loss: 0.4411449432373047:  50%|██████▍      | 59/119 [00:08<00:09,  6.20it/s]Epoch: 3, train for the 60-th batch, train loss: 0.4411449432373047:  50%|██████▌      | 60/119 [00:08<00:09,  6.40it/s]evaluate for the 11-th batch, evaluate loss: 0.6337828040122986:  50%|█████████         | 10/20 [00:00<00:00, 12.14it/s]Epoch: 1, train for the 234-th batch, train loss: 0.5562204718589783:  99%|██████████▊| 234/237 [00:59<00:00,  3.96it/s]Epoch: 2, train for the 97-th batch, train loss: 0.5343385934829712:  64%|████████▎    | 96/151 [00:15<00:10,  5.24it/s]evaluate for the 17-th batch, evaluate loss: 0.6513832211494446:  47%|████████▍         | 16/34 [00:01<00:01, 13.17it/s]Epoch: 2, train for the 97-th batch, train loss: 0.5343385934829712:  64%|████████▎    | 97/151 [00:15<00:09,  5.44it/s]evaluate for the 12-th batch, evaluate loss: 0.7026283144950867:  50%|█████████         | 10/20 [00:00<00:00, 12.14it/s]evaluate for the 12-th batch, evaluate loss: 0.7026283144950867:  60%|██████████▊       | 12/20 [00:00<00:00, 12.85it/s]Epoch: 3, train for the 61-th batch, train loss: 0.4698502719402313:  50%|██████▌      | 60/119 [00:08<00:09,  6.40it/s]evaluate for the 18-th batch, evaluate loss: 0.5851770043373108:  47%|████████▍         | 16/34 [00:01<00:01, 13.17it/s]evaluate for the 18-th batch, evaluate loss: 0.5851770043373108:  53%|█████████▌        | 18/34 [00:01<00:01, 12.70it/s]Epoch: 3, train for the 61-th batch, train loss: 0.4698502719402313:  51%|██████▋      | 61/119 [00:09<00:08,  6.55it/s]evaluate for the 13-th batch, evaluate loss: 0.7181476950645447:  60%|██████████▊       | 12/20 [00:01<00:00, 12.85it/s]Epoch: 1, train for the 222-th batch, train loss: 0.49385568499565125:  58%|█████▊    | 221/383 [00:57<00:48,  3.36it/s]Epoch: 2, train for the 98-th batch, train loss: 0.5078312754631042:  64%|████████▎    | 97/151 [00:15<00:09,  5.44it/s]Epoch: 1, train for the 235-th batch, train loss: 0.5333093404769897:  99%|██████████▊| 234/237 [00:59<00:00,  3.96it/s]Epoch: 1, train for the 222-th batch, train loss: 0.49385568499565125:  58%|█████▊    | 222/383 [00:57<00:46,  3.43it/s]Epoch: 2, train for the 98-th batch, train loss: 0.5078312754631042:  65%|████████▍    | 98/151 [00:15<00:09,  5.81it/s]evaluate for the 19-th batch, evaluate loss: 0.5616434216499329:  53%|█████████▌        | 18/34 [00:01<00:01, 12.70it/s]evaluate for the 14-th batch, evaluate loss: 0.7334429025650024:  60%|██████████▊       | 12/20 [00:01<00:00, 12.85it/s]evaluate for the 14-th batch, evaluate loss: 0.7334429025650024:  70%|████████████▌     | 14/20 [00:01<00:00, 12.87it/s]Epoch: 1, train for the 235-th batch, train loss: 0.5333093404769897:  99%|██████████▉| 235/237 [00:59<00:00,  4.07it/s]Epoch: 3, train for the 62-th batch, train loss: 0.5367815494537354:  51%|██████▋      | 61/119 [00:09<00:08,  6.55it/s]Epoch: 3, train for the 62-th batch, train loss: 0.5367815494537354:  52%|██████▊      | 62/119 [00:09<00:08,  6.90it/s]evaluate for the 20-th batch, evaluate loss: 0.620074987411499:  53%|██████████         | 18/34 [00:01<00:01, 12.70it/s]evaluate for the 20-th batch, evaluate loss: 0.620074987411499:  59%|███████████▏       | 20/34 [00:01<00:01, 13.01it/s]evaluate for the 15-th batch, evaluate loss: 0.7253328561782837:  70%|████████████▌     | 14/20 [00:01<00:00, 12.87it/s]Epoch: 2, train for the 99-th batch, train loss: 0.5303806662559509:  65%|████████▍    | 98/151 [00:15<00:09,  5.81it/s]Epoch: 2, train for the 99-th batch, train loss: 0.5303806662559509:  66%|████████▌    | 99/151 [00:15<00:08,  5.95it/s]evaluate for the 21-th batch, evaluate loss: 0.6178986430168152:  59%|██████████▌       | 20/34 [00:01<00:01, 13.01it/s]Epoch: 1, train for the 223-th batch, train loss: 0.43366050720214844:  58%|█████▊    | 222/383 [00:58<00:46,  3.43it/s]evaluate for the 16-th batch, evaluate loss: 0.6645557880401611:  70%|████████████▌     | 14/20 [00:01<00:00, 12.87it/s]evaluate for the 16-th batch, evaluate loss: 0.6645557880401611:  80%|██████████████▍   | 16/20 [00:01<00:00, 11.88it/s]Epoch: 3, train for the 63-th batch, train loss: 0.49468687176704407:  52%|██████▎     | 62/119 [00:09<00:08,  6.90it/s]Epoch: 3, train for the 63-th batch, train loss: 0.49468687176704407:  53%|██████▎     | 63/119 [00:09<00:08,  6.70it/s]evaluate for the 22-th batch, evaluate loss: 0.4067954421043396:  59%|██████████▌       | 20/34 [00:01<00:01, 13.01it/s]evaluate for the 22-th batch, evaluate loss: 0.4067954421043396:  65%|███████████▋      | 22/34 [00:01<00:00, 13.18it/s]Epoch: 1, train for the 236-th batch, train loss: 0.5419920086860657:  99%|██████████▉| 235/237 [01:00<00:00,  4.07it/s]Epoch: 1, train for the 223-th batch, train loss: 0.43366050720214844:  58%|█████▊    | 223/383 [00:58<00:44,  3.57it/s]evaluate for the 17-th batch, evaluate loss: 0.7060871720314026:  80%|██████████████▍   | 16/20 [00:01<00:00, 11.88it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5727910995483398:  66%|███████▊    | 99/151 [00:15<00:08,  5.95it/s]evaluate for the 23-th batch, evaluate loss: 0.3646613359451294:  65%|███████████▋      | 22/34 [00:01<00:00, 13.18it/s]Epoch: 1, train for the 236-th batch, train loss: 0.5419920086860657: 100%|██████████▉| 236/237 [01:00<00:00,  3.96it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5727910995483398:  66%|███████▎   | 100/151 [00:15<00:08,  6.17it/s]Epoch: 3, train for the 64-th batch, train loss: 0.4995497763156891:  53%|██████▉      | 63/119 [00:09<00:08,  6.70it/s]evaluate for the 18-th batch, evaluate loss: 0.693644642829895:  80%|███████████████▏   | 16/20 [00:01<00:00, 11.88it/s]evaluate for the 18-th batch, evaluate loss: 0.693644642829895:  90%|█████████████████  | 18/20 [00:01<00:00, 12.43it/s]Epoch: 3, train for the 64-th batch, train loss: 0.4995497763156891:  54%|██████▉      | 64/119 [00:09<00:08,  6.80it/s]evaluate for the 24-th batch, evaluate loss: 0.5249968767166138:  65%|███████████▋      | 22/34 [00:01<00:00, 13.18it/s]evaluate for the 24-th batch, evaluate loss: 0.5249968767166138:  71%|████████████▋     | 24/34 [00:01<00:00, 12.58it/s]evaluate for the 19-th batch, evaluate loss: 0.7387946844100952:  90%|████████████████▏ | 18/20 [00:01<00:00, 12.43it/s]Epoch: 1, train for the 224-th batch, train loss: 0.5070871114730835:  58%|██████▍    | 223/383 [00:58<00:44,  3.57it/s]Epoch: 3, train for the 65-th batch, train loss: 0.4953654408454895:  54%|██████▉      | 64/119 [00:09<00:08,  6.80it/s]Epoch: 3, train for the 65-th batch, train loss: 0.4953654408454895:  55%|███████      | 65/119 [00:09<00:07,  7.09it/s]Epoch: 2, train for the 101-th batch, train loss: 0.5991700887680054:  66%|███████▎   | 100/151 [00:16<00:08,  6.17it/s]Epoch: 1, train for the 224-th batch, train loss: 0.5070871114730835:  58%|██████▍    | 224/383 [00:58<00:42,  3.71it/s]evaluate for the 20-th batch, evaluate loss: 0.7612534761428833:  90%|████████████████▏ | 18/20 [00:01<00:00, 12.43it/s]evaluate for the 25-th batch, evaluate loss: 0.5233505368232727:  71%|████████████▋     | 24/34 [00:01<00:00, 12.58it/s]evaluate for the 20-th batch, evaluate loss: 0.7612534761428833: 100%|██████████████████| 20/20 [00:01<00:00, 12.78it/s]evaluate for the 20-th batch, evaluate loss: 0.7612534761428833: 100%|██████████████████| 20/20 [00:01<00:00, 12.47it/s]
Epoch: 1, train for the 237-th batch, train loss: 0.5096003413200378: 100%|██████████▉| 236/237 [01:00<00:00,  3.96it/s]Epoch: 2, train for the 101-th batch, train loss: 0.5991700887680054:  67%|███████▎   | 101/151 [00:16<00:08,  5.73it/s]Epoch: 1, train for the 237-th batch, train loss: 0.5096003413200378: 100%|███████████| 237/237 [01:00<00:00,  4.07it/s]Epoch: 1, train for the 237-th batch, train loss: 0.5096003413200378: 100%|███████████| 237/237 [01:00<00:00,  3.93it/s]
evaluate for the 26-th batch, evaluate loss: 0.622381329536438:  71%|█████████████▍     | 24/34 [00:02<00:00, 12.58it/s]evaluate for the 26-th batch, evaluate loss: 0.622381329536438:  76%|██████████████▌    | 26/34 [00:02<00:00, 12.33it/s]Epoch: 3, train for the 66-th batch, train loss: 0.4754682183265686:  55%|███████      | 65/119 [00:09<00:07,  7.09it/s]Epoch: 3, train for the 66-th batch, train loss: 0.4754682183265686:  55%|███████▏     | 66/119 [00:09<00:07,  7.14it/s]evaluate for the 27-th batch, evaluate loss: 0.6387868523597717:  76%|█████████████▊    | 26/34 [00:02<00:00, 12.33it/s]Epoch: 2, train for the 102-th batch, train loss: 0.49475032091140747:  67%|██████▋   | 101/151 [00:16<00:08,  5.73it/s]Epoch: 2, train for the 102-th batch, train loss: 0.49475032091140747:  68%|██████▊   | 102/151 [00:16<00:08,  5.50it/s]evaluate for the 28-th batch, evaluate loss: 0.6069062948226929:  76%|█████████████▊    | 26/34 [00:02<00:00, 12.33it/s]evaluate for the 28-th batch, evaluate loss: 0.6069062948226929:  82%|██████████████▊   | 28/34 [00:02<00:00, 12.81it/s]Epoch: 1, train for the 225-th batch, train loss: 0.43153947591781616:  58%|█████▊    | 224/383 [00:58<00:42,  3.71it/s]evaluate for the 29-th batch, evaluate loss: 0.5821056962013245:  82%|██████████████▊   | 28/34 [00:02<00:00, 12.81it/s]INFO:root:Epoch: 2, learning rate: 0.0001, train loss: 0.5589
INFO:root:train average_precision, 0.8064
INFO:root:train roc_auc, 0.7859
INFO:root:validate loss: 0.4785
INFO:root:validate average_precision, 0.8549
INFO:root:validate roc_auc, 0.8504
INFO:root:new node validate loss: 0.6989
INFO:root:new node validate first_1_average_precision, 0.5705
INFO:root:new node validate first_1_roc_auc, 0.5093
INFO:root:new node validate first_3_average_precision, 0.5977
INFO:root:new node validate first_3_roc_auc, 0.5676
INFO:root:new node validate first_10_average_precision, 0.6207
INFO:root:new node validate first_10_roc_auc, 0.6190
INFO:root:new node validate average_precision, 0.6665
INFO:root:new node validate roc_auc, 0.6688
INFO:root:save model ./saved_models/TGN/ia-escorts-dynamic/TGN_seed0_dummy-55wl6je1/TGN_seed0_dummy-55wl6je1.pkl
Epoch: 1, train for the 225-th batch, train loss: 0.43153947591781616:  59%|█████▊    | 225/383 [00:58<00:45,  3.49it/s]evaluate for the 30-th batch, evaluate loss: 0.5633889436721802:  82%|██████████████▊   | 28/34 [00:02<00:00, 12.81it/s]evaluate for the 30-th batch, evaluate loss: 0.5633889436721802:  88%|███████████████▉  | 30/34 [00:02<00:00, 13.80it/s]Epoch: 3, train for the 67-th batch, train loss: 0.5304713845252991:  55%|███████▏     | 66/119 [00:09<00:07,  7.14it/s]Epoch: 3, train for the 67-th batch, train loss: 0.5304713845252991:  56%|███████▎     | 67/119 [00:09<00:08,  6.04it/s]Epoch: 2, train for the 103-th batch, train loss: 0.5829381346702576:  68%|███████▍   | 102/151 [00:16<00:08,  5.50it/s]Epoch: 2, train for the 103-th batch, train loss: 0.5829381346702576:  68%|███████▌   | 103/151 [00:16<00:08,  5.41it/s]  0%|                                                                                            | 0/66 [00:00<?, ?it/s]Epoch: 3, train for the 68-th batch, train loss: 0.4530329704284668:  56%|███████▎     | 67/119 [00:10<00:08,  6.04it/s]Epoch: 2, train for the 104-th batch, train loss: 0.5778915286064148:  68%|███████▌   | 103/151 [00:16<00:08,  5.41it/s]Epoch: 3, train for the 69-th batch, train loss: 0.48136815428733826:  56%|██████▊     | 67/119 [00:10<00:08,  6.04it/s]Epoch: 2, train for the 104-th batch, train loss: 0.5778915286064148:  69%|███████▌   | 104/151 [00:16<00:07,  5.88it/s]Epoch: 3, train for the 69-th batch, train loss: 0.48136815428733826:  58%|██████▉     | 69/119 [00:10<00:06,  7.44it/s]evaluate for the 31-th batch, evaluate loss: 0.6383236646652222:  88%|███████████████▉  | 30/34 [00:02<00:00, 13.80it/s]Epoch: 1, train for the 226-th batch, train loss: 0.48993074893951416:  59%|█████▊    | 225/383 [00:58<00:45,  3.49it/s]evaluate for the 1-th batch, evaluate loss: 0.5623487830162048:   0%|                            | 0/66 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.5623487830162048:   2%|▎                   | 1/66 [00:00<00:08,  7.61it/s]Epoch: 1, train for the 226-th batch, train loss: 0.48993074893951416:  59%|█████▉    | 226/383 [00:58<00:45,  3.48it/s]evaluate for the 32-th batch, evaluate loss: 0.6512294411659241:  88%|███████████████▉  | 30/34 [00:02<00:00, 13.80it/s]evaluate for the 32-th batch, evaluate loss: 0.6512294411659241:  94%|████████████████▉ | 32/34 [00:02<00:00, 10.60it/s]Epoch: 3, train for the 70-th batch, train loss: 0.4128774106502533:  58%|███████▌     | 69/119 [00:10<00:06,  7.44it/s]Epoch: 3, train for the 70-th batch, train loss: 0.4128774106502533:  59%|███████▋     | 70/119 [00:10<00:06,  7.65it/s]evaluate for the 2-th batch, evaluate loss: 0.5165041089057922:   2%|▎                   | 1/66 [00:00<00:08,  7.61it/s]evaluate for the 2-th batch, evaluate loss: 0.5165041089057922:   3%|▌                   | 2/66 [00:00<00:07,  8.56it/s]Epoch: 2, train for the 105-th batch, train loss: 0.5503044724464417:  69%|███████▌   | 104/151 [00:16<00:07,  5.88it/s]evaluate for the 33-th batch, evaluate loss: 0.6550988554954529:  94%|████████████████▉ | 32/34 [00:02<00:00, 10.60it/s]Epoch: 2, train for the 105-th batch, train loss: 0.5503044724464417:  70%|███████▋   | 105/151 [00:16<00:07,  5.81it/s]evaluate for the 3-th batch, evaluate loss: 0.560097336769104:   3%|▋                    | 2/66 [00:00<00:07,  8.56it/s]Epoch: 3, train for the 71-th batch, train loss: 0.4546615481376648:  59%|███████▋     | 70/119 [00:10<00:06,  7.65it/s]Epoch: 3, train for the 71-th batch, train loss: 0.4546615481376648:  60%|███████▊     | 71/119 [00:10<00:06,  7.89it/s]evaluate for the 34-th batch, evaluate loss: 0.7062582969665527:  94%|████████████████▉ | 32/34 [00:02<00:00, 10.60it/s]evaluate for the 34-th batch, evaluate loss: 0.7062582969665527: 100%|██████████████████| 34/34 [00:02<00:00, 10.96it/s]evaluate for the 34-th batch, evaluate loss: 0.7062582969665527: 100%|██████████████████| 34/34 [00:02<00:00, 12.27it/s]
Epoch: 1, train for the 227-th batch, train loss: 0.5265859961509705:  59%|██████▍    | 226/383 [00:59<00:45,  3.48it/s]evaluate for the 4-th batch, evaluate loss: 0.5265436172485352:   3%|▌                   | 2/66 [00:00<00:07,  8.56it/s]evaluate for the 4-th batch, evaluate loss: 0.5265436172485352:   6%|█▏                  | 4/66 [00:00<00:06, 10.29it/s]Epoch: 3, train for the 72-th batch, train loss: 0.5123577117919922:  60%|███████▊     | 71/119 [00:10<00:06,  7.89it/s]Epoch: 1, train for the 227-th batch, train loss: 0.5265859961509705:  59%|██████▌    | 227/383 [00:59<00:43,  3.57it/s]Epoch: 3, train for the 72-th batch, train loss: 0.5123577117919922:  61%|███████▊     | 72/119 [00:10<00:05,  8.36it/s]Epoch: 2, train for the 106-th batch, train loss: 0.5039320588111877:  70%|███████▋   | 105/151 [00:16<00:07,  5.81it/s]Epoch: 2, train for the 106-th batch, train loss: 0.5039320588111877:  70%|███████▋   | 106/151 [00:16<00:08,  5.40it/s]evaluate for the 5-th batch, evaluate loss: 0.5481334328651428:   6%|█▏                  | 4/66 [00:00<00:06, 10.29it/s]Epoch: 3, train for the 73-th batch, train loss: 0.47052574157714844:  61%|███████▎    | 72/119 [00:10<00:05,  8.36it/s]Epoch: 3, train for the 73-th batch, train loss: 0.47052574157714844:  61%|███████▎    | 73/119 [00:10<00:05,  8.55it/s]evaluate for the 6-th batch, evaluate loss: 0.5725370049476624:   6%|█▏                  | 4/66 [00:00<00:06, 10.29it/s]evaluate for the 6-th batch, evaluate loss: 0.5725370049476624:   9%|█▊                  | 6/66 [00:00<00:06,  9.78it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]INFO:root:Epoch: 1, learning rate: 0.0001, train loss: 0.5264
INFO:root:train average_precision, 0.8354
INFO:root:train roc_auc, 0.8026
INFO:root:validate loss: 0.4825
INFO:root:validate average_precision, 0.8735
INFO:root:validate roc_auc, 0.8549
INFO:root:new node validate loss: 0.6055
INFO:root:new node validate first_1_average_precision, 0.5085
INFO:root:new node validate first_1_roc_auc, 0.5018
INFO:root:new node validate first_3_average_precision, 0.6000
INFO:root:new node validate first_3_roc_auc, 0.5844
INFO:root:new node validate first_10_average_precision, 0.6988
INFO:root:new node validate first_10_roc_auc, 0.6772
INFO:root:new node validate average_precision, 0.7565
INFO:root:new node validate roc_auc, 0.7239
INFO:root:save model ./saved_models/TGN/ia-movielens-user2tags-10m/TGN_seed0_dummy-s2o65ldo/TGN_seed0_dummy-s2o65ldo.pkl
Epoch: 3, train for the 74-th batch, train loss: 0.5073919296264648:  61%|███████▉     | 73/119 [00:10<00:05,  8.55it/s]Epoch: 3, train for the 74-th batch, train loss: 0.5073919296264648:  62%|████████     | 74/119 [00:10<00:05,  8.91it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5267803072929382:  70%|███████▋   | 106/151 [00:17<00:08,  5.40it/s]Epoch: 1, train for the 228-th batch, train loss: 0.4266708195209503:  59%|██████▌    | 227/383 [00:59<00:43,  3.57it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5267803072929382:  71%|███████▊   | 107/151 [00:17<00:08,  5.37it/s]Epoch: 3, train for the 1-th batch, train loss: 0.8794859647750854:   0%|                       | 0/146 [00:00<?, ?it/s]evaluate for the 7-th batch, evaluate loss: 0.5444924235343933:   9%|█▊                  | 6/66 [00:00<00:06,  9.78it/s]Epoch: 1, train for the 228-th batch, train loss: 0.4266708195209503:  60%|██████▌    | 228/383 [00:59<00:44,  3.47it/s]Epoch: 3, train for the 75-th batch, train loss: 0.4656561613082886:  62%|████████     | 74/119 [00:10<00:05,  8.91it/s]Epoch: 3, train for the 75-th batch, train loss: 0.4656561613082886:  63%|████████▏    | 75/119 [00:10<00:05,  8.71it/s]Epoch: 2, train for the 108-th batch, train loss: 0.5370217561721802:  71%|███████▊   | 107/151 [00:17<00:08,  5.37it/s]Epoch: 2, train for the 108-th batch, train loss: 0.5370217561721802:  72%|███████▊   | 108/151 [00:17<00:07,  5.89it/s]evaluate for the 8-th batch, evaluate loss: 0.5374395847320557:   9%|█▊                  | 6/66 [00:00<00:06,  9.78it/s]evaluate for the 8-th batch, evaluate loss: 0.5374395847320557:  12%|██▍                 | 8/66 [00:00<00:05,  9.67it/s]Epoch: 3, train for the 76-th batch, train loss: 0.5018787384033203:  63%|████████▏    | 75/119 [00:10<00:05,  8.71it/s]Epoch: 3, train for the 76-th batch, train loss: 0.5018787384033203:  64%|████████▎    | 76/119 [00:10<00:04,  8.74it/s]Epoch: 3, train for the 2-th batch, train loss: 0.8259812593460083:   0%|                       | 0/146 [00:00<?, ?it/s]Epoch: 2, train for the 109-th batch, train loss: 0.4979501962661743:  72%|███████▊   | 108/151 [00:17<00:07,  5.89it/s]Epoch: 3, train for the 2-th batch, train loss: 0.8259812593460083:   1%|▏              | 2/146 [00:00<00:21,  6.63it/s]Epoch: 2, train for the 109-th batch, train loss: 0.4979501962661743:  72%|███████▉   | 109/151 [00:17<00:06,  6.28it/s]Epoch: 1, train for the 229-th batch, train loss: 0.37258657813072205:  60%|█████▉    | 228/383 [00:59<00:44,  3.47it/s]evaluate for the 9-th batch, evaluate loss: 0.5087752938270569:  12%|██▍                 | 8/66 [00:00<00:05,  9.67it/s]evaluate for the 9-th batch, evaluate loss: 0.5087752938270569:  14%|██▋                 | 9/66 [00:00<00:06,  8.62it/s]Epoch: 3, train for the 77-th batch, train loss: 0.4842485785484314:  64%|████████▎    | 76/119 [00:11<00:04,  8.74it/s]Epoch: 3, train for the 77-th batch, train loss: 0.4842485785484314:  65%|████████▍    | 77/119 [00:11<00:05,  8.16it/s]Epoch: 1, train for the 229-th batch, train loss: 0.37258657813072205:  60%|█████▉    | 229/383 [00:59<00:45,  3.42it/s]Epoch: 2, train for the 110-th batch, train loss: 0.5486568212509155:  72%|███████▉   | 109/151 [00:17<00:06,  6.28it/s]Epoch: 3, train for the 3-th batch, train loss: 0.7896302342414856:   1%|▏              | 2/146 [00:00<00:21,  6.63it/s]Epoch: 3, train for the 3-th batch, train loss: 0.7896302342414856:   2%|▎              | 3/146 [00:00<00:20,  7.13it/s]Epoch: 2, train for the 110-th batch, train loss: 0.5486568212509155:  73%|████████   | 110/151 [00:17<00:06,  6.76it/s]evaluate for the 10-th batch, evaluate loss: 0.5108892917633057:  14%|██▌                | 9/66 [00:01<00:06,  8.62it/s]Epoch: 3, train for the 78-th batch, train loss: 0.5480877757072449:  65%|████████▍    | 77/119 [00:11<00:05,  8.16it/s]Epoch: 3, train for the 78-th batch, train loss: 0.5480877757072449:  66%|████████▌    | 78/119 [00:11<00:05,  7.64it/s]evaluate for the 11-th batch, evaluate loss: 0.5152700543403625:  14%|██▌                | 9/66 [00:01<00:06,  8.62it/s]evaluate for the 11-th batch, evaluate loss: 0.5152700543403625:  17%|███               | 11/66 [00:01<00:05,  9.25it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5413274765014648:  73%|████████   | 110/151 [00:17<00:06,  6.76it/s]Epoch: 3, train for the 4-th batch, train loss: 0.7629114389419556:   2%|▎              | 3/146 [00:00<00:20,  7.13it/s]Epoch: 3, train for the 4-th batch, train loss: 0.7629114389419556:   3%|▍              | 4/146 [00:00<00:20,  6.96it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5413274765014648:  74%|████████   | 111/151 [00:17<00:05,  6.91it/s]Epoch: 1, train for the 230-th batch, train loss: 0.44990670680999756:  60%|█████▉    | 229/383 [01:00<00:45,  3.42it/s]evaluate for the 12-th batch, evaluate loss: 0.5375550985336304:  17%|███               | 11/66 [00:01<00:05,  9.25it/s]evaluate for the 12-th batch, evaluate loss: 0.5375550985336304:  18%|███▎              | 12/66 [00:01<00:06,  8.97it/s]Epoch: 3, train for the 79-th batch, train loss: 0.5019758343696594:  66%|████████▌    | 78/119 [00:11<00:05,  7.64it/s]Epoch: 2, train for the 112-th batch, train loss: 0.4736495018005371:  74%|████████   | 111/151 [00:17<00:05,  6.91it/s]Epoch: 3, train for the 79-th batch, train loss: 0.5019758343696594:  66%|████████▋    | 79/119 [00:11<00:05,  7.45it/s]Epoch: 2, train for the 112-th batch, train loss: 0.4736495018005371:  74%|████████▏  | 112/151 [00:17<00:05,  7.19it/s]Epoch: 3, train for the 5-th batch, train loss: 0.7253842949867249:   3%|▍              | 4/146 [00:00<00:20,  6.96it/s]Epoch: 3, train for the 5-th batch, train loss: 0.7253842949867249:   3%|▌              | 5/146 [00:00<00:20,  6.90it/s]Epoch: 1, train for the 230-th batch, train loss: 0.44990670680999756:  60%|██████    | 230/383 [01:00<00:47,  3.25it/s]evaluate for the 13-th batch, evaluate loss: 0.5495876669883728:  18%|███▎              | 12/66 [00:01<00:06,  8.97it/s]Epoch: 3, train for the 80-th batch, train loss: 0.5339195132255554:  66%|████████▋    | 79/119 [00:11<00:05,  7.45it/s]Epoch: 2, train for the 113-th batch, train loss: 0.531204879283905:  74%|████████▉   | 112/151 [00:17<00:05,  7.19it/s]Epoch: 3, train for the 80-th batch, train loss: 0.5339195132255554:  67%|████████▋    | 80/119 [00:11<00:05,  7.29it/s]Epoch: 2, train for the 113-th batch, train loss: 0.531204879283905:  75%|████████▉   | 113/151 [00:17<00:05,  7.02it/s]Epoch: 3, train for the 6-th batch, train loss: 0.6869081258773804:   3%|▌              | 5/146 [00:00<00:20,  6.90it/s]evaluate for the 14-th batch, evaluate loss: 0.5395950078964233:  18%|███▎              | 12/66 [00:01<00:06,  8.97it/s]evaluate for the 14-th batch, evaluate loss: 0.5395950078964233:  21%|███▊              | 14/66 [00:01<00:05,  9.63it/s]Epoch: 3, train for the 6-th batch, train loss: 0.6869081258773804:   4%|▌              | 6/146 [00:00<00:20,  6.96it/s]evaluate for the 15-th batch, evaluate loss: 0.522092878818512:  21%|████               | 14/66 [00:01<00:05,  9.63it/s]evaluate for the 15-th batch, evaluate loss: 0.522092878818512:  23%|████▎              | 15/66 [00:01<00:05,  9.12it/s]Epoch: 2, train for the 114-th batch, train loss: 0.4957793653011322:  75%|████████▏  | 113/151 [00:18<00:05,  7.02it/s]Epoch: 2, train for the 114-th batch, train loss: 0.4957793653011322:  75%|████████▎  | 114/151 [00:18<00:05,  6.78it/s]Epoch: 3, train for the 7-th batch, train loss: 0.6779837012290955:   4%|▌              | 6/146 [00:01<00:20,  6.96it/s]Epoch: 3, train for the 7-th batch, train loss: 0.6779837012290955:   5%|▋              | 7/146 [00:01<00:20,  6.78it/s]Epoch: 3, train for the 81-th batch, train loss: 0.42589065432548523:  67%|████████    | 80/119 [00:11<00:05,  7.29it/s]Epoch: 3, train for the 81-th batch, train loss: 0.42589065432548523:  68%|████████▏   | 81/119 [00:11<00:05,  6.45it/s]Epoch: 1, train for the 231-th batch, train loss: 0.46301043033599854:  60%|██████    | 230/383 [01:00<00:47,  3.25it/s]evaluate for the 16-th batch, evaluate loss: 0.5010406374931335:  23%|████              | 15/66 [00:01<00:05,  9.12it/s]evaluate for the 16-th batch, evaluate loss: 0.5010406374931335:  24%|████▎             | 16/66 [00:01<00:05,  9.26it/s]Epoch: 1, train for the 231-th batch, train loss: 0.46301043033599854:  60%|██████    | 231/383 [01:00<00:48,  3.12it/s]Epoch: 2, train for the 115-th batch, train loss: 0.4596244692802429:  75%|████████▎  | 114/151 [00:18<00:05,  6.78it/s]Epoch: 3, train for the 8-th batch, train loss: 0.6655279397964478:   5%|▋              | 7/146 [00:01<00:20,  6.78it/s]Epoch: 3, train for the 8-th batch, train loss: 0.6655279397964478:   5%|▊              | 8/146 [00:01<00:19,  6.99it/s]Epoch: 2, train for the 115-th batch, train loss: 0.4596244692802429:  76%|████████▍  | 115/151 [00:18<00:05,  6.84it/s]Epoch: 3, train for the 82-th batch, train loss: 0.4968263804912567:  68%|████████▊    | 81/119 [00:11<00:05,  6.45it/s]Epoch: 3, train for the 82-th batch, train loss: 0.4968263804912567:  69%|████████▉    | 82/119 [00:11<00:05,  6.32it/s]evaluate for the 17-th batch, evaluate loss: 0.5204443335533142:  24%|████▎             | 16/66 [00:01<00:05,  9.26it/s]evaluate for the 17-th batch, evaluate loss: 0.5204443335533142:  26%|████▋             | 17/66 [00:01<00:05,  9.30it/s]Epoch: 3, train for the 9-th batch, train loss: 0.6439936757087708:   5%|▊              | 8/146 [00:01<00:19,  6.99it/s]Epoch: 3, train for the 9-th batch, train loss: 0.6439936757087708:   6%|▉              | 9/146 [00:01<00:18,  7.29it/s]Epoch: 2, train for the 116-th batch, train loss: 0.4480911195278168:  76%|████████▍  | 115/151 [00:18<00:05,  6.84it/s]Epoch: 2, train for the 116-th batch, train loss: 0.4480911195278168:  77%|████████▍  | 116/151 [00:18<00:05,  6.89it/s]evaluate for the 18-th batch, evaluate loss: 0.522256076335907:  26%|████▉              | 17/66 [00:01<00:05,  9.30it/s]evaluate for the 18-th batch, evaluate loss: 0.522256076335907:  27%|█████▏             | 18/66 [00:01<00:05,  9.21it/s]Epoch: 3, train for the 83-th batch, train loss: 0.4701012670993805:  69%|████████▉    | 82/119 [00:11<00:05,  6.32it/s]Epoch: 3, train for the 83-th batch, train loss: 0.4701012670993805:  70%|█████████    | 83/119 [00:11<00:05,  6.47it/s]Epoch: 1, train for the 232-th batch, train loss: 0.38863417506217957:  60%|██████    | 231/383 [01:00<00:48,  3.12it/s]Epoch: 3, train for the 10-th batch, train loss: 0.6473503112792969:   6%|▊             | 9/146 [00:01<00:18,  7.29it/s]Epoch: 3, train for the 10-th batch, train loss: 0.6473503112792969:   7%|▉            | 10/146 [00:01<00:17,  7.86it/s]evaluate for the 19-th batch, evaluate loss: 0.4968700408935547:  27%|████▉             | 18/66 [00:02<00:05,  9.21it/s]Epoch: 2, train for the 117-th batch, train loss: 0.49870237708091736:  77%|███████▋  | 116/151 [00:18<00:05,  6.89it/s]Epoch: 2, train for the 117-th batch, train loss: 0.49870237708091736:  77%|███████▋  | 117/151 [00:18<00:04,  7.04it/s]Epoch: 1, train for the 232-th batch, train loss: 0.38863417506217957:  61%|██████    | 232/383 [01:00<00:48,  3.12it/s]Epoch: 3, train for the 84-th batch, train loss: 0.49987441301345825:  70%|████████▎   | 83/119 [00:12<00:05,  6.47it/s]evaluate for the 20-th batch, evaluate loss: 0.5591505169868469:  27%|████▉             | 18/66 [00:02<00:05,  9.21it/s]evaluate for the 20-th batch, evaluate loss: 0.5591505169868469:  30%|█████▍            | 20/66 [00:02<00:04, 10.01it/s]Epoch: 3, train for the 84-th batch, train loss: 0.49987441301345825:  71%|████████▍   | 84/119 [00:12<00:05,  6.58it/s]Epoch: 3, train for the 11-th batch, train loss: 0.6443718075752258:   7%|▉            | 10/146 [00:01<00:17,  7.86it/s]Epoch: 3, train for the 11-th batch, train loss: 0.6443718075752258:   8%|▉            | 11/146 [00:01<00:17,  7.71it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/241 [00:00<?, ?it/s]Epoch: 2, train for the 118-th batch, train loss: 0.4652823507785797:  77%|████████▌  | 117/151 [00:18<00:04,  7.04it/s]Epoch: 2, train for the 118-th batch, train loss: 0.4652823507785797:  78%|████████▌  | 118/151 [00:18<00:04,  6.94it/s]evaluate for the 21-th batch, evaluate loss: 0.5540809631347656:  30%|█████▍            | 20/66 [00:02<00:04, 10.01it/s]evaluate for the 21-th batch, evaluate loss: 0.5540809631347656:  32%|█████▋            | 21/66 [00:02<00:04,  9.69it/s]Epoch: 3, train for the 85-th batch, train loss: 0.5089042782783508:  71%|█████████▏   | 84/119 [00:12<00:05,  6.58it/s]Epoch: 2, train for the 1-th batch, train loss: 0.5754666924476624:   0%|                       | 0/241 [00:00<?, ?it/s]Epoch: 3, train for the 85-th batch, train loss: 0.5089042782783508:  71%|█████████▎   | 85/119 [00:12<00:05,  6.59it/s]Epoch: 3, train for the 12-th batch, train loss: 0.6438437700271606:   8%|▉            | 11/146 [00:01<00:17,  7.71it/s]Epoch: 3, train for the 12-th batch, train loss: 0.6438437700271606:   8%|█            | 12/146 [00:01<00:18,  7.21it/s]Epoch: 1, train for the 233-th batch, train loss: 0.4147518575191498:  61%|██████▋    | 232/383 [01:01<00:48,  3.12it/s]evaluate for the 22-th batch, evaluate loss: 0.5370494723320007:  32%|█████▋            | 21/66 [00:02<00:04,  9.69it/s]evaluate for the 22-th batch, evaluate loss: 0.5370494723320007:  33%|██████            | 22/66 [00:02<00:04,  9.17it/s]Epoch: 3, train for the 86-th batch, train loss: 0.5414735674858093:  71%|█████████▎   | 85/119 [00:12<00:05,  6.59it/s]Epoch: 2, train for the 2-th batch, train loss: 0.4254573583602905:   0%|                       | 0/241 [00:00<?, ?it/s]Epoch: 2, train for the 119-th batch, train loss: 0.5070009231567383:  78%|████████▌  | 118/151 [00:18<00:04,  6.94it/s]Epoch: 2, train for the 2-th batch, train loss: 0.4254573583602905:   1%|               | 2/241 [00:00<00:28,  8.39it/s]Epoch: 3, train for the 86-th batch, train loss: 0.5414735674858093:  72%|█████████▍   | 86/119 [00:12<00:04,  6.78it/s]Epoch: 1, train for the 233-th batch, train loss: 0.4147518575191498:  61%|██████▋    | 233/383 [01:01<00:49,  3.04it/s]Epoch: 2, train for the 119-th batch, train loss: 0.5070009231567383:  79%|████████▋  | 119/151 [00:18<00:05,  5.99it/s]Epoch: 3, train for the 13-th batch, train loss: 0.6526846289634705:   8%|█            | 12/146 [00:01<00:18,  7.21it/s]Epoch: 3, train for the 13-th batch, train loss: 0.6526846289634705:   9%|█▏           | 13/146 [00:01<00:18,  7.23it/s]evaluate for the 23-th batch, evaluate loss: 0.5436128973960876:  33%|██████            | 22/66 [00:02<00:04,  9.17it/s]Epoch: 2, train for the 3-th batch, train loss: 0.470688134431839:   1%|▏               | 2/241 [00:00<00:28,  8.39it/s]Epoch: 2, train for the 3-th batch, train loss: 0.470688134431839:   1%|▏               | 3/241 [00:00<00:28,  8.34it/s]Epoch: 3, train for the 87-th batch, train loss: 0.49463754892349243:  72%|████████▋   | 86/119 [00:12<00:04,  6.78it/s]Epoch: 3, train for the 87-th batch, train loss: 0.49463754892349243:  73%|████████▊   | 87/119 [00:12<00:04,  6.59it/s]evaluate for the 24-th batch, evaluate loss: 0.5310010313987732:  33%|██████            | 22/66 [00:02<00:04,  9.17it/s]evaluate for the 24-th batch, evaluate loss: 0.5310010313987732:  36%|██████▌           | 24/66 [00:02<00:04,  9.12it/s]Epoch: 3, train for the 14-th batch, train loss: 0.6417328715324402:   9%|█▏           | 13/146 [00:01<00:18,  7.23it/s]Epoch: 3, train for the 14-th batch, train loss: 0.6417328715324402:  10%|█▏           | 14/146 [00:01<00:18,  7.20it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5226854085922241:  79%|████████▋  | 119/151 [00:19<00:05,  5.99it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5226854085922241:  79%|████████▋  | 120/151 [00:19<00:05,  5.81it/s]Epoch: 2, train for the 4-th batch, train loss: 0.4408891201019287:   1%|▏              | 3/241 [00:00<00:28,  8.34it/s]Epoch: 1, train for the 234-th batch, train loss: 0.4283345639705658:  61%|██████▋    | 233/383 [01:01<00:49,  3.04it/s]evaluate for the 25-th batch, evaluate loss: 0.5439012050628662:  36%|██████▌           | 24/66 [00:02<00:04,  9.12it/s]evaluate for the 25-th batch, evaluate loss: 0.5439012050628662:  38%|██████▊           | 25/66 [00:02<00:04,  9.02it/s]Epoch: 3, train for the 88-th batch, train loss: 0.5279262661933899:  73%|█████████▌   | 87/119 [00:12<00:04,  6.59it/s]Epoch: 3, train for the 88-th batch, train loss: 0.5279262661933899:  74%|█████████▌   | 88/119 [00:12<00:04,  6.61it/s]Epoch: 1, train for the 234-th batch, train loss: 0.4283345639705658:  61%|██████▋    | 234/383 [01:01<00:48,  3.08it/s]Epoch: 3, train for the 15-th batch, train loss: 0.6534122824668884:  10%|█▏           | 14/146 [00:02<00:18,  7.20it/s]Epoch: 3, train for the 15-th batch, train loss: 0.6534122824668884:  10%|█▎           | 15/146 [00:02<00:19,  6.88it/s]Epoch: 2, train for the 5-th batch, train loss: 0.47035202383995056:   1%|▏             | 3/241 [00:00<00:28,  8.34it/s]Epoch: 2, train for the 5-th batch, train loss: 0.47035202383995056:   2%|▎             | 5/241 [00:00<00:27,  8.62it/s]Epoch: 2, train for the 121-th batch, train loss: 0.5081379413604736:  79%|████████▋  | 120/151 [00:19<00:05,  5.81it/s]evaluate for the 26-th batch, evaluate loss: 0.5540415644645691:  38%|██████▊           | 25/66 [00:02<00:04,  9.02it/s]evaluate for the 26-th batch, evaluate loss: 0.5540415644645691:  39%|███████           | 26/66 [00:02<00:04,  8.75it/s]Epoch: 2, train for the 121-th batch, train loss: 0.5081379413604736:  80%|████████▊  | 121/151 [00:19<00:05,  5.44it/s]Epoch: 3, train for the 89-th batch, train loss: 0.5223645567893982:  74%|█████████▌   | 88/119 [00:12<00:04,  6.61it/s]Epoch: 3, train for the 89-th batch, train loss: 0.5223645567893982:  75%|█████████▋   | 89/119 [00:12<00:04,  6.55it/s]Epoch: 2, train for the 6-th batch, train loss: 0.619594156742096:   2%|▎               | 5/241 [00:00<00:27,  8.62it/s]Epoch: 2, train for the 6-th batch, train loss: 0.619594156742096:   2%|▍               | 6/241 [00:00<00:28,  8.20it/s]Epoch: 3, train for the 16-th batch, train loss: 0.6568410992622375:  10%|█▎           | 15/146 [00:02<00:19,  6.88it/s]Epoch: 3, train for the 16-th batch, train loss: 0.6568410992622375:  11%|█▍           | 16/146 [00:02<00:19,  6.67it/s]evaluate for the 27-th batch, evaluate loss: 0.5422071814537048:  39%|███████           | 26/66 [00:02<00:04,  8.75it/s]evaluate for the 27-th batch, evaluate loss: 0.5422071814537048:  41%|███████▎          | 27/66 [00:02<00:04,  8.53it/s]Epoch: 2, train for the 122-th batch, train loss: 0.47214263677597046:  80%|████████  | 121/151 [00:19<00:05,  5.44it/s]Epoch: 2, train for the 122-th batch, train loss: 0.47214263677597046:  81%|████████  | 122/151 [00:19<00:05,  5.78it/s]Epoch: 1, train for the 235-th batch, train loss: 0.37194350361824036:  61%|██████    | 234/383 [01:01<00:48,  3.08it/s]Epoch: 2, train for the 7-th batch, train loss: 0.4113599956035614:   2%|▎              | 6/241 [00:00<00:28,  8.20it/s]Epoch: 2, train for the 7-th batch, train loss: 0.4113599956035614:   3%|▍              | 7/241 [00:00<00:28,  8.24it/s]Epoch: 3, train for the 90-th batch, train loss: 0.4883621037006378:  75%|█████████▋   | 89/119 [00:13<00:04,  6.55it/s]Epoch: 3, train for the 90-th batch, train loss: 0.4883621037006378:  76%|█████████▊   | 90/119 [00:13<00:04,  6.32it/s]Epoch: 3, train for the 17-th batch, train loss: 0.6375287771224976:  11%|█▍           | 16/146 [00:02<00:19,  6.67it/s]evaluate for the 28-th batch, evaluate loss: 0.5933530330657959:  41%|███████▎          | 27/66 [00:03<00:04,  8.53it/s]evaluate for the 28-th batch, evaluate loss: 0.5933530330657959:  42%|███████▋          | 28/66 [00:03<00:04,  8.70it/s]Epoch: 3, train for the 17-th batch, train loss: 0.6375287771224976:  12%|█▌           | 17/146 [00:02<00:19,  6.66it/s]Epoch: 1, train for the 235-th batch, train loss: 0.37194350361824036:  61%|██████▏   | 235/383 [01:01<00:48,  3.06it/s]Epoch: 2, train for the 123-th batch, train loss: 0.4906825125217438:  81%|████████▉  | 122/151 [00:19<00:05,  5.78it/s]Epoch: 2, train for the 123-th batch, train loss: 0.4906825125217438:  81%|████████▉  | 123/151 [00:19<00:04,  5.99it/s]Epoch: 2, train for the 8-th batch, train loss: 0.657996654510498:   3%|▍               | 7/241 [00:00<00:28,  8.24it/s]Epoch: 2, train for the 8-th batch, train loss: 0.657996654510498:   3%|▌               | 8/241 [00:00<00:28,  8.12it/s]evaluate for the 29-th batch, evaluate loss: 0.5077495574951172:  42%|███████▋          | 28/66 [00:03<00:04,  8.70it/s]Epoch: 3, train for the 18-th batch, train loss: 0.6247045993804932:  12%|█▌           | 17/146 [00:02<00:19,  6.66it/s]Epoch: 3, train for the 18-th batch, train loss: 0.6247045993804932:  12%|█▌           | 18/146 [00:02<00:18,  6.77it/s]Epoch: 3, train for the 91-th batch, train loss: 0.47547683119773865:  76%|█████████   | 90/119 [00:13<00:04,  6.32it/s]Epoch: 3, train for the 91-th batch, train loss: 0.47547683119773865:  76%|█████████▏  | 91/119 [00:13<00:04,  6.10it/s]Epoch: 2, train for the 9-th batch, train loss: 0.5961138010025024:   3%|▍              | 8/241 [00:01<00:28,  8.12it/s]Epoch: 2, train for the 9-th batch, train loss: 0.5961138010025024:   4%|▌              | 9/241 [00:01<00:27,  8.42it/s]Epoch: 2, train for the 124-th batch, train loss: 0.5140572190284729:  81%|████████▉  | 123/151 [00:19<00:04,  5.99it/s]Epoch: 3, train for the 19-th batch, train loss: 0.6271634697914124:  12%|█▌           | 18/146 [00:02<00:18,  6.77it/s]Epoch: 3, train for the 19-th batch, train loss: 0.6271634697914124:  13%|█▋           | 19/146 [00:02<00:17,  7.24it/s]Epoch: 2, train for the 124-th batch, train loss: 0.5140572190284729:  82%|█████████  | 124/151 [00:19<00:04,  5.69it/s]Epoch: 1, train for the 236-th batch, train loss: 0.4181285798549652:  61%|██████▋    | 235/383 [01:02<00:48,  3.06it/s]Epoch: 2, train for the 10-th batch, train loss: 0.5377066135406494:   4%|▌             | 9/241 [00:01<00:27,  8.42it/s]evaluate for the 30-th batch, evaluate loss: 0.5570970773696899:  42%|███████▋          | 28/66 [00:03<00:04,  8.70it/s]evaluate for the 30-th batch, evaluate loss: 0.5570970773696899:  45%|████████▏         | 30/66 [00:03<00:04,  7.79it/s]Epoch: 3, train for the 92-th batch, train loss: 0.5216765403747559:  76%|█████████▉   | 91/119 [00:13<00:04,  6.10it/s]Epoch: 3, train for the 92-th batch, train loss: 0.5216765403747559:  77%|██████████   | 92/119 [00:13<00:04,  6.26it/s]Epoch: 1, train for the 236-th batch, train loss: 0.4181285798549652:  62%|██████▊    | 236/383 [01:02<00:48,  3.01it/s]Epoch: 3, train for the 20-th batch, train loss: 0.6119135022163391:  13%|█▋           | 19/146 [00:02<00:17,  7.24it/s]Epoch: 3, train for the 20-th batch, train loss: 0.6119135022163391:  14%|█▊           | 20/146 [00:02<00:16,  7.48it/s]evaluate for the 31-th batch, evaluate loss: 0.544266939163208:  45%|████████▋          | 30/66 [00:03<00:04,  7.79it/s]evaluate for the 31-th batch, evaluate loss: 0.544266939163208:  47%|████████▉          | 31/66 [00:03<00:04,  7.86it/s]Epoch: 2, train for the 125-th batch, train loss: 0.5249612331390381:  82%|█████████  | 124/151 [00:19<00:04,  5.69it/s]Epoch: 2, train for the 11-th batch, train loss: 0.3670446574687958:   4%|▌             | 9/241 [00:01<00:27,  8.42it/s]Epoch: 3, train for the 93-th batch, train loss: 0.4688750207424164:  77%|██████████   | 92/119 [00:13<00:04,  6.26it/s]Epoch: 2, train for the 11-th batch, train loss: 0.3670446574687958:   5%|▌            | 11/241 [00:01<00:28,  8.09it/s]Epoch: 3, train for the 93-th batch, train loss: 0.4688750207424164:  78%|██████████▏  | 93/119 [00:13<00:03,  6.56it/s]Epoch: 2, train for the 125-th batch, train loss: 0.5249612331390381:  83%|█████████  | 125/151 [00:19<00:04,  5.51it/s]Epoch: 3, train for the 21-th batch, train loss: 0.5995029807090759:  14%|█▊           | 20/146 [00:02<00:16,  7.48it/s]Epoch: 3, train for the 21-th batch, train loss: 0.5995029807090759:  14%|█▊           | 21/146 [00:02<00:16,  7.64it/s]evaluate for the 32-th batch, evaluate loss: 0.5554093718528748:  47%|████████▍         | 31/66 [00:03<00:04,  7.86it/s]evaluate for the 32-th batch, evaluate loss: 0.5554093718528748:  48%|████████▋         | 32/66 [00:03<00:04,  8.01it/s]Epoch: 2, train for the 12-th batch, train loss: 0.5409549474716187:   5%|▌            | 11/241 [00:01<00:28,  8.09it/s]Epoch: 2, train for the 12-th batch, train loss: 0.5409549474716187:   5%|▋            | 12/241 [00:01<00:28,  8.09it/s]Epoch: 3, train for the 94-th batch, train loss: 0.4435599446296692:  78%|██████████▏  | 93/119 [00:13<00:03,  6.56it/s]Epoch: 3, train for the 94-th batch, train loss: 0.4435599446296692:  79%|██████████▎  | 94/119 [00:13<00:03,  6.38it/s]Epoch: 1, train for the 237-th batch, train loss: 0.4205228388309479:  62%|██████▊    | 236/383 [01:02<00:48,  3.01it/s]Epoch: 3, train for the 22-th batch, train loss: 0.6049817800521851:  14%|█▊           | 21/146 [00:03<00:16,  7.64it/s]Epoch: 2, train for the 126-th batch, train loss: 0.5133641958236694:  83%|█████████  | 125/151 [00:20<00:04,  5.51it/s]Epoch: 3, train for the 22-th batch, train loss: 0.6049817800521851:  15%|█▉           | 22/146 [00:03<00:16,  7.49it/s]evaluate for the 33-th batch, evaluate loss: 0.5824039578437805:  48%|████████▋         | 32/66 [00:03<00:04,  8.01it/s]evaluate for the 33-th batch, evaluate loss: 0.5824039578437805:  50%|█████████         | 33/66 [00:03<00:04,  8.03it/s]Epoch: 2, train for the 13-th batch, train loss: 0.5240763425827026:   5%|▋            | 12/241 [00:01<00:28,  8.09it/s]Epoch: 2, train for the 13-th batch, train loss: 0.5240763425827026:   5%|▋            | 13/241 [00:01<00:27,  8.43it/s]Epoch: 2, train for the 126-th batch, train loss: 0.5133641958236694:  83%|█████████▏ | 126/151 [00:20<00:04,  5.18it/s]Epoch: 1, train for the 237-th batch, train loss: 0.4205228388309479:  62%|██████▊    | 237/383 [01:02<00:49,  2.95it/s]evaluate for the 34-th batch, evaluate loss: 0.5436246991157532:  50%|█████████         | 33/66 [00:03<00:04,  8.03it/s]evaluate for the 34-th batch, evaluate loss: 0.5436246991157532:  52%|█████████▎        | 34/66 [00:03<00:03,  8.16it/s]Epoch: 2, train for the 14-th batch, train loss: 0.5978098511695862:   5%|▋            | 13/241 [00:01<00:27,  8.43it/s]Epoch: 3, train for the 95-th batch, train loss: 0.43893107771873474:  79%|█████████▍  | 94/119 [00:13<00:03,  6.38it/s]Epoch: 2, train for the 14-th batch, train loss: 0.5978098511695862:   6%|▊            | 14/241 [00:01<00:25,  8.74it/s]Epoch: 3, train for the 23-th batch, train loss: 0.6105760931968689:  15%|█▉           | 22/146 [00:03<00:16,  7.49it/s]Epoch: 3, train for the 23-th batch, train loss: 0.6105760931968689:  16%|██           | 23/146 [00:03<00:17,  7.20it/s]Epoch: 3, train for the 95-th batch, train loss: 0.43893107771873474:  80%|█████████▌  | 95/119 [00:13<00:03,  6.15it/s]Epoch: 2, train for the 127-th batch, train loss: 0.4831368923187256:  83%|█████████▏ | 126/151 [00:20<00:04,  5.18it/s]Epoch: 2, train for the 127-th batch, train loss: 0.4831368923187256:  84%|█████████▎ | 127/151 [00:20<00:04,  5.31it/s]evaluate for the 35-th batch, evaluate loss: 0.5697755217552185:  52%|█████████▎        | 34/66 [00:03<00:03,  8.16it/s]evaluate for the 35-th batch, evaluate loss: 0.5697755217552185:  53%|█████████▌        | 35/66 [00:03<00:03,  7.78it/s]Epoch: 3, train for the 24-th batch, train loss: 0.5956180691719055:  16%|██           | 23/146 [00:03<00:17,  7.20it/s]Epoch: 3, train for the 96-th batch, train loss: 0.45970916748046875:  80%|█████████▌  | 95/119 [00:14<00:03,  6.15it/s]Epoch: 1, train for the 238-th batch, train loss: 0.36260348558425903:  62%|██████▏   | 237/383 [01:02<00:49,  2.95it/s]Epoch: 3, train for the 24-th batch, train loss: 0.5956180691719055:  16%|██▏          | 24/146 [00:03<00:18,  6.77it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5466787815093994:  84%|█████████▎ | 127/151 [00:20<00:04,  5.31it/s]Epoch: 3, train for the 96-th batch, train loss: 0.45970916748046875:  81%|█████████▋  | 96/119 [00:14<00:03,  6.05it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5466787815093994:  85%|█████████▎ | 128/151 [00:20<00:03,  5.90it/s]evaluate for the 36-th batch, evaluate loss: 0.5628170371055603:  53%|█████████▌        | 35/66 [00:04<00:03,  7.78it/s]Epoch: 2, train for the 15-th batch, train loss: 0.5303505063056946:   6%|▊            | 14/241 [00:01<00:25,  8.74it/s]Epoch: 2, train for the 15-th batch, train loss: 0.5303505063056946:   6%|▊            | 15/241 [00:01<00:33,  6.71it/s]Epoch: 1, train for the 238-th batch, train loss: 0.36260348558425903:  62%|██████▏   | 238/383 [01:02<00:49,  2.96it/s]evaluate for the 37-th batch, evaluate loss: 0.567152202129364:  53%|██████████         | 35/66 [00:04<00:03,  7.78it/s]evaluate for the 37-th batch, evaluate loss: 0.567152202129364:  56%|██████████▋        | 37/66 [00:04<00:03,  8.73it/s]Epoch: 3, train for the 25-th batch, train loss: 0.5924100875854492:  16%|██▏          | 24/146 [00:03<00:18,  6.77it/s]Epoch: 3, train for the 25-th batch, train loss: 0.5924100875854492:  17%|██▏          | 25/146 [00:03<00:18,  6.64it/s]Epoch: 3, train for the 97-th batch, train loss: 0.48144954442977905:  81%|█████████▋  | 96/119 [00:14<00:03,  6.05it/s]Epoch: 3, train for the 97-th batch, train loss: 0.48144954442977905:  82%|█████████▊  | 97/119 [00:14<00:03,  6.07it/s]Epoch: 2, train for the 129-th batch, train loss: 0.5586571097373962:  85%|█████████▎ | 128/151 [00:20<00:03,  5.90it/s]Epoch: 2, train for the 16-th batch, train loss: 0.45302897691726685:   6%|▋           | 15/241 [00:02<00:33,  6.71it/s]Epoch: 2, train for the 129-th batch, train loss: 0.5586571097373962:  85%|█████████▍ | 129/151 [00:20<00:03,  5.84it/s]Epoch: 2, train for the 16-th batch, train loss: 0.45302897691726685:   7%|▊           | 16/241 [00:02<00:33,  6.78it/s]evaluate for the 38-th batch, evaluate loss: 0.520214855670929:  56%|██████████▋        | 37/66 [00:04<00:03,  8.73it/s]evaluate for the 38-th batch, evaluate loss: 0.520214855670929:  58%|██████████▉        | 38/66 [00:04<00:03,  8.35it/s]Epoch: 3, train for the 26-th batch, train loss: 0.5467756390571594:  17%|██▏          | 25/146 [00:03<00:18,  6.64it/s]Epoch: 3, train for the 26-th batch, train loss: 0.5467756390571594:  18%|██▎          | 26/146 [00:03<00:17,  6.69it/s]Epoch: 1, train for the 239-th batch, train loss: 0.44148504734039307:  62%|██████▏   | 238/383 [01:03<00:49,  2.96it/s]Epoch: 3, train for the 98-th batch, train loss: 0.44616079330444336:  82%|█████████▊  | 97/119 [00:14<00:03,  6.07it/s]Epoch: 2, train for the 17-th batch, train loss: 0.41582372784614563:   7%|▊           | 16/241 [00:02<00:33,  6.78it/s]Epoch: 3, train for the 98-th batch, train loss: 0.44616079330444336:  82%|█████████▉  | 98/119 [00:14<00:03,  5.95it/s]Epoch: 2, train for the 17-th batch, train loss: 0.41582372784614563:   7%|▊           | 17/241 [00:02<00:33,  6.72it/s]Epoch: 1, train for the 239-th batch, train loss: 0.44148504734039307:  62%|██████▏   | 239/383 [01:03<00:46,  3.08it/s]evaluate for the 39-th batch, evaluate loss: 0.5577815771102905:  58%|██████████▎       | 38/66 [00:04<00:03,  8.35it/s]evaluate for the 39-th batch, evaluate loss: 0.5577815771102905:  59%|██████████▋       | 39/66 [00:04<00:03,  8.67it/s]Epoch: 2, train for the 130-th batch, train loss: 0.510281503200531:  85%|██████████▎ | 129/151 [00:20<00:03,  5.84it/s]Epoch: 3, train for the 27-th batch, train loss: 0.5704354047775269:  18%|██▎          | 26/146 [00:03<00:17,  6.69it/s]Epoch: 2, train for the 130-th batch, train loss: 0.510281503200531:  86%|██████████▎ | 130/151 [00:20<00:03,  5.39it/s]Epoch: 3, train for the 27-th batch, train loss: 0.5704354047775269:  18%|██▍          | 27/146 [00:03<00:16,  7.26it/s]Epoch: 2, train for the 18-th batch, train loss: 0.47852298617362976:   7%|▊           | 17/241 [00:02<00:33,  6.72it/s]Epoch: 2, train for the 18-th batch, train loss: 0.47852298617362976:   7%|▉           | 18/241 [00:02<00:30,  7.21it/s]Epoch: 3, train for the 99-th batch, train loss: 0.47078028321266174:  82%|█████████▉  | 98/119 [00:14<00:03,  5.95it/s]Epoch: 3, train for the 99-th batch, train loss: 0.47078028321266174:  83%|█████████▉  | 99/119 [00:14<00:03,  6.24it/s]evaluate for the 40-th batch, evaluate loss: 0.5399909615516663:  59%|██████████▋       | 39/66 [00:04<00:03,  8.67it/s]evaluate for the 40-th batch, evaluate loss: 0.5399909615516663:  61%|██████████▉       | 40/66 [00:04<00:03,  8.64it/s]Epoch: 3, train for the 28-th batch, train loss: 0.5458813905715942:  18%|██▍          | 27/146 [00:03<00:16,  7.26it/s]Epoch: 3, train for the 28-th batch, train loss: 0.5458813905715942:  19%|██▍          | 28/146 [00:03<00:16,  7.02it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5310304164886475:  86%|█████████▍ | 130/151 [00:21<00:03,  5.39it/s]Epoch: 2, train for the 19-th batch, train loss: 0.48145487904548645:   7%|▉           | 18/241 [00:02<00:30,  7.21it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5310304164886475:  87%|█████████▌ | 131/151 [00:21<00:03,  5.26it/s]Epoch: 2, train for the 19-th batch, train loss: 0.48145487904548645:   8%|▉           | 19/241 [00:02<00:31,  7.02it/s]Epoch: 1, train for the 240-th batch, train loss: 0.5183424353599548:  62%|██████▊    | 239/383 [01:03<00:46,  3.08it/s]Epoch: 3, train for the 100-th batch, train loss: 0.485340416431427:  83%|██████████▊  | 99/119 [00:14<00:03,  6.24it/s]evaluate for the 41-th batch, evaluate loss: 0.5027954578399658:  61%|██████████▉       | 40/66 [00:04<00:03,  8.64it/s]evaluate for the 41-th batch, evaluate loss: 0.5027954578399658:  62%|███████████▏      | 41/66 [00:04<00:02,  8.37it/s]Epoch: 3, train for the 100-th batch, train loss: 0.485340416431427:  84%|██████████  | 100/119 [00:14<00:03,  6.32it/s]Epoch: 3, train for the 29-th batch, train loss: 0.5161439776420593:  19%|██▍          | 28/146 [00:04<00:16,  7.02it/s]Epoch: 3, train for the 29-th batch, train loss: 0.5161439776420593:  20%|██▌          | 29/146 [00:04<00:15,  7.49it/s]Epoch: 1, train for the 240-th batch, train loss: 0.5183424353599548:  63%|██████▉    | 240/383 [01:03<00:46,  3.10it/s]Epoch: 2, train for the 20-th batch, train loss: 0.38682904839515686:   8%|▉           | 19/241 [00:02<00:31,  7.02it/s]evaluate for the 42-th batch, evaluate loss: 0.5392428040504456:  62%|███████████▏      | 41/66 [00:04<00:02,  8.37it/s]evaluate for the 42-th batch, evaluate loss: 0.5392428040504456:  64%|███████████▍      | 42/66 [00:04<00:02,  8.54it/s]Epoch: 2, train for the 20-th batch, train loss: 0.38682904839515686:   8%|▉           | 20/241 [00:02<00:30,  7.23it/s]Epoch: 3, train for the 101-th batch, train loss: 0.46957841515541077:  84%|████████▍ | 100/119 [00:14<00:03,  6.32it/s]Epoch: 3, train for the 101-th batch, train loss: 0.46957841515541077:  85%|████████▍ | 101/119 [00:14<00:02,  6.45it/s]Epoch: 2, train for the 132-th batch, train loss: 0.503052830696106:  87%|██████████▍ | 131/151 [00:21<00:03,  5.26it/s]Epoch: 3, train for the 30-th batch, train loss: 0.5078016519546509:  20%|██▌          | 29/146 [00:04<00:15,  7.49it/s]Epoch: 3, train for the 30-th batch, train loss: 0.5078016519546509:  21%|██▋          | 30/146 [00:04<00:15,  7.43it/s]Epoch: 2, train for the 132-th batch, train loss: 0.503052830696106:  87%|██████████▍ | 132/151 [00:21<00:03,  5.13it/s]Epoch: 2, train for the 21-th batch, train loss: 0.627295196056366:   8%|█▏            | 20/241 [00:02<00:30,  7.23it/s]Epoch: 2, train for the 21-th batch, train loss: 0.627295196056366:   9%|█▏            | 21/241 [00:02<00:29,  7.48it/s]evaluate for the 43-th batch, evaluate loss: 0.5213682055473328:  64%|███████████▍      | 42/66 [00:04<00:02,  8.54it/s]evaluate for the 43-th batch, evaluate loss: 0.5213682055473328:  65%|███████████▋      | 43/66 [00:04<00:03,  7.53it/s]Epoch: 1, train for the 241-th batch, train loss: 0.5662246346473694:  63%|██████▉    | 240/383 [01:03<00:46,  3.10it/s]Epoch: 3, train for the 102-th batch, train loss: 0.5379666686058044:  85%|█████████▎ | 101/119 [00:14<00:02,  6.45it/s]Epoch: 1, train for the 241-th batch, train loss: 0.5662246346473694:  63%|██████▉    | 241/383 [01:03<00:44,  3.22it/s]Epoch: 3, train for the 31-th batch, train loss: 0.5610783696174622:  21%|██▋          | 30/146 [00:04<00:15,  7.43it/s]Epoch: 3, train for the 102-th batch, train loss: 0.5379666686058044:  86%|█████████▍ | 102/119 [00:15<00:02,  6.06it/s]Epoch: 2, train for the 133-th batch, train loss: 0.4536759853363037:  87%|█████████▌ | 132/151 [00:21<00:03,  5.13it/s]Epoch: 3, train for the 31-th batch, train loss: 0.5610783696174622:  21%|██▊          | 31/146 [00:04<00:16,  7.03it/s]Epoch: 2, train for the 22-th batch, train loss: 0.5317378640174866:   9%|█▏           | 21/241 [00:02<00:29,  7.48it/s]Epoch: 2, train for the 133-th batch, train loss: 0.4536759853363037:  88%|█████████▋ | 133/151 [00:21<00:03,  5.23it/s]Epoch: 2, train for the 22-th batch, train loss: 0.5317378640174866:   9%|█▏           | 22/241 [00:02<00:29,  7.43it/s]evaluate for the 44-th batch, evaluate loss: 0.543524980545044:  65%|████████████▍      | 43/66 [00:05<00:03,  7.53it/s]evaluate for the 44-th batch, evaluate loss: 0.543524980545044:  67%|████████████▋      | 44/66 [00:05<00:02,  7.86it/s]Epoch: 3, train for the 32-th batch, train loss: 0.5129687190055847:  21%|██▊          | 31/146 [00:04<00:16,  7.03it/s]Epoch: 3, train for the 32-th batch, train loss: 0.5129687190055847:  22%|██▊          | 32/146 [00:04<00:15,  7.39it/s]Epoch: 2, train for the 23-th batch, train loss: 0.5062803030014038:   9%|█▏           | 22/241 [00:02<00:29,  7.43it/s]Epoch: 3, train for the 103-th batch, train loss: 0.4607124328613281:  86%|█████████▍ | 102/119 [00:15<00:02,  6.06it/s]Epoch: 2, train for the 23-th batch, train loss: 0.5062803030014038:  10%|█▏           | 23/241 [00:02<00:29,  7.32it/s]Epoch: 3, train for the 103-th batch, train loss: 0.4607124328613281:  87%|█████████▌ | 103/119 [00:15<00:02,  5.99it/s]evaluate for the 45-th batch, evaluate loss: 0.5180914402008057:  67%|████████████      | 44/66 [00:05<00:02,  7.86it/s]evaluate for the 45-th batch, evaluate loss: 0.5180914402008057:  68%|████████████▎     | 45/66 [00:05<00:02,  7.91it/s]Epoch: 2, train for the 134-th batch, train loss: 0.48614194989204407:  88%|████████▊ | 133/151 [00:21<00:03,  5.23it/s]Epoch: 2, train for the 134-th batch, train loss: 0.48614194989204407:  89%|████████▊ | 134/151 [00:21<00:03,  5.20it/s]Epoch: 3, train for the 33-th batch, train loss: 0.5023857951164246:  22%|██▊          | 32/146 [00:04<00:15,  7.39it/s]Epoch: 1, train for the 242-th batch, train loss: 0.474206805229187:  63%|███████▌    | 241/383 [01:04<00:44,  3.22it/s]Epoch: 3, train for the 33-th batch, train loss: 0.5023857951164246:  23%|██▉          | 33/146 [00:04<00:14,  7.65it/s]evaluate for the 46-th batch, evaluate loss: 0.5717538595199585:  68%|████████████▎     | 45/66 [00:05<00:02,  7.91it/s]evaluate for the 46-th batch, evaluate loss: 0.5717538595199585:  70%|████████████▌     | 46/66 [00:05<00:02,  8.23it/s]Epoch: 3, train for the 104-th batch, train loss: 0.48726263642311096:  87%|████████▋ | 103/119 [00:15<00:02,  5.99it/s]Epoch: 2, train for the 24-th batch, train loss: 0.5334284901618958:  10%|█▏           | 23/241 [00:03<00:29,  7.32it/s]Epoch: 3, train for the 104-th batch, train loss: 0.48726263642311096:  87%|████████▋ | 104/119 [00:15<00:02,  6.49it/s]Epoch: 2, train for the 24-th batch, train loss: 0.5334284901618958:  10%|█▎           | 24/241 [00:03<00:29,  7.45it/s]Epoch: 1, train for the 242-th batch, train loss: 0.474206805229187:  63%|███████▌    | 242/383 [01:04<00:43,  3.22it/s]Epoch: 2, train for the 135-th batch, train loss: 0.4615485966205597:  89%|█████████▊ | 134/151 [00:21<00:03,  5.20it/s]Epoch: 3, train for the 34-th batch, train loss: 0.5442779660224915:  23%|██▉          | 33/146 [00:04<00:14,  7.65it/s]Epoch: 2, train for the 135-th batch, train loss: 0.4615485966205597:  89%|█████████▊ | 135/151 [00:21<00:02,  5.65it/s]Epoch: 3, train for the 34-th batch, train loss: 0.5442779660224915:  23%|███          | 34/146 [00:04<00:14,  7.78it/s]Epoch: 3, train for the 105-th batch, train loss: 0.4809093475341797:  87%|█████████▌ | 104/119 [00:15<00:02,  6.49it/s]evaluate for the 47-th batch, evaluate loss: 0.5594823956489563:  70%|████████████▌     | 46/66 [00:05<00:02,  8.23it/s]evaluate for the 47-th batch, evaluate loss: 0.5594823956489563:  71%|████████████▊     | 47/66 [00:05<00:02,  7.93it/s]Epoch: 2, train for the 25-th batch, train loss: 0.5928179621696472:  10%|█▎           | 24/241 [00:03<00:29,  7.45it/s]Epoch: 2, train for the 25-th batch, train loss: 0.5928179621696472:  10%|█▎           | 25/241 [00:03<00:29,  7.42it/s]Epoch: 3, train for the 105-th batch, train loss: 0.4809093475341797:  88%|█████████▋ | 105/119 [00:15<00:02,  6.63it/s]Epoch: 3, train for the 35-th batch, train loss: 0.4625253975391388:  23%|███          | 34/146 [00:04<00:14,  7.78it/s]Epoch: 3, train for the 35-th batch, train loss: 0.4625253975391388:  24%|███          | 35/146 [00:04<00:14,  7.69it/s]Epoch: 2, train for the 136-th batch, train loss: 0.5144394040107727:  89%|█████████▊ | 135/151 [00:21<00:02,  5.65it/s]Epoch: 1, train for the 243-th batch, train loss: 0.4748341143131256:  63%|██████▉    | 242/383 [01:04<00:43,  3.22it/s]evaluate for the 48-th batch, evaluate loss: 0.5477479696273804:  71%|████████████▊     | 47/66 [00:05<00:02,  7.93it/s]evaluate for the 48-th batch, evaluate loss: 0.5477479696273804:  73%|█████████████     | 48/66 [00:05<00:02,  8.06it/s]Epoch: 2, train for the 26-th batch, train loss: 0.642297625541687:  10%|█▍            | 25/241 [00:03<00:29,  7.42it/s]Epoch: 2, train for the 136-th batch, train loss: 0.5144394040107727:  90%|█████████▉ | 136/151 [00:22<00:02,  5.52it/s]Epoch: 2, train for the 26-th batch, train loss: 0.642297625541687:  11%|█▌            | 26/241 [00:03<00:28,  7.65it/s]Epoch: 3, train for the 106-th batch, train loss: 0.4981396496295929:  88%|█████████▋ | 105/119 [00:15<00:02,  6.63it/s]Epoch: 3, train for the 106-th batch, train loss: 0.4981396496295929:  89%|█████████▊ | 106/119 [00:15<00:01,  6.92it/s]Epoch: 1, train for the 243-th batch, train loss: 0.4748341143131256:  63%|██████▉    | 243/383 [01:04<00:42,  3.31it/s]Epoch: 3, train for the 36-th batch, train loss: 0.46826478838920593:  24%|██▉         | 35/146 [00:04<00:14,  7.69it/s]Epoch: 3, train for the 36-th batch, train loss: 0.46826478838920593:  25%|██▉         | 36/146 [00:04<00:13,  8.15it/s]Epoch: 2, train for the 27-th batch, train loss: 0.6033129096031189:  11%|█▍           | 26/241 [00:03<00:28,  7.65it/s]evaluate for the 49-th batch, evaluate loss: 0.568743109703064:  73%|█████████████▊     | 48/66 [00:05<00:02,  8.06it/s]evaluate for the 49-th batch, evaluate loss: 0.568743109703064:  74%|██████████████     | 49/66 [00:05<00:02,  8.05it/s]Epoch: 2, train for the 27-th batch, train loss: 0.6033129096031189:  11%|█▍           | 27/241 [00:03<00:26,  8.06it/s]Epoch: 3, train for the 107-th batch, train loss: 0.519353449344635:  89%|██████████▋ | 106/119 [00:15<00:01,  6.92it/s]Epoch: 3, train for the 107-th batch, train loss: 0.519353449344635:  90%|██████████▊ | 107/119 [00:15<00:01,  6.80it/s]Epoch: 2, train for the 137-th batch, train loss: 0.5548707842826843:  90%|█████████▉ | 136/151 [00:22<00:02,  5.52it/s]Epoch: 3, train for the 37-th batch, train loss: 0.5189825892448425:  25%|███▏         | 36/146 [00:05<00:13,  8.15it/s]Epoch: 3, train for the 37-th batch, train loss: 0.5189825892448425:  25%|███▎         | 37/146 [00:05<00:14,  7.48it/s]Epoch: 2, train for the 28-th batch, train loss: 0.45345932245254517:  11%|█▎          | 27/241 [00:03<00:26,  8.06it/s]Epoch: 2, train for the 137-th batch, train loss: 0.5548707842826843:  91%|█████████▉ | 137/151 [00:22<00:02,  5.23it/s]Epoch: 2, train for the 28-th batch, train loss: 0.45345932245254517:  12%|█▍          | 28/241 [00:03<00:25,  8.40it/s]evaluate for the 50-th batch, evaluate loss: 0.5932238101959229:  74%|█████████████▎    | 49/66 [00:05<00:02,  8.05it/s]evaluate for the 50-th batch, evaluate loss: 0.5932238101959229:  76%|█████████████▋    | 50/66 [00:05<00:01,  8.01it/s]Epoch: 1, train for the 244-th batch, train loss: 0.37976571917533875:  63%|██████▎   | 243/383 [01:04<00:42,  3.31it/s]Epoch: 3, train for the 108-th batch, train loss: 0.40743833780288696:  90%|████████▉ | 107/119 [00:15<00:01,  6.80it/s]Epoch: 3, train for the 108-th batch, train loss: 0.40743833780288696:  91%|█████████ | 108/119 [00:15<00:01,  7.02it/s]Epoch: 1, train for the 244-th batch, train loss: 0.37976571917533875:  64%|██████▎   | 244/383 [01:04<00:41,  3.34it/s]Epoch: 2, train for the 29-th batch, train loss: 0.5691343545913696:  12%|█▌           | 28/241 [00:03<00:25,  8.40it/s]Epoch: 3, train for the 38-th batch, train loss: 0.5271244049072266:  25%|███▎         | 37/146 [00:05<00:14,  7.48it/s]evaluate for the 51-th batch, evaluate loss: 0.5570501685142517:  76%|█████████████▋    | 50/66 [00:05<00:01,  8.01it/s]Epoch: 2, train for the 29-th batch, train loss: 0.5691343545913696:  12%|█▌           | 29/241 [00:03<00:25,  8.39it/s]evaluate for the 51-th batch, evaluate loss: 0.5570501685142517:  77%|█████████████▉    | 51/66 [00:05<00:01,  8.49it/s]Epoch: 3, train for the 38-th batch, train loss: 0.5271244049072266:  26%|███▍         | 38/146 [00:05<00:14,  7.50it/s]Epoch: 2, train for the 138-th batch, train loss: 0.5293325185775757:  91%|█████████▉ | 137/151 [00:22<00:02,  5.23it/s]Epoch: 2, train for the 138-th batch, train loss: 0.5293325185775757:  91%|██████████ | 138/151 [00:22<00:02,  5.18it/s]Epoch: 3, train for the 109-th batch, train loss: 0.4858565330505371:  91%|█████████▉ | 108/119 [00:15<00:01,  7.02it/s]Epoch: 3, train for the 109-th batch, train loss: 0.4858565330505371:  92%|██████████ | 109/119 [00:16<00:01,  6.93it/s]Epoch: 2, train for the 30-th batch, train loss: 0.6454010009765625:  12%|█▌           | 29/241 [00:03<00:25,  8.39it/s]Epoch: 2, train for the 30-th batch, train loss: 0.6454010009765625:  12%|█▌           | 30/241 [00:03<00:25,  8.41it/s]evaluate for the 52-th batch, evaluate loss: 0.5472056269645691:  77%|█████████████▉    | 51/66 [00:06<00:01,  8.49it/s]evaluate for the 52-th batch, evaluate loss: 0.5472056269645691:  79%|██████████████▏   | 52/66 [00:06<00:01,  8.26it/s]Epoch: 3, train for the 39-th batch, train loss: 0.4923107922077179:  26%|███▍         | 38/146 [00:05<00:14,  7.50it/s]Epoch: 3, train for the 39-th batch, train loss: 0.4923107922077179:  27%|███▍         | 39/146 [00:05<00:14,  7.19it/s]Epoch: 3, train for the 110-th batch, train loss: 0.5082389712333679:  92%|██████████ | 109/119 [00:16<00:01,  6.93it/s]Epoch: 3, train for the 110-th batch, train loss: 0.5082389712333679:  92%|██████████▏| 110/119 [00:16<00:01,  6.99it/s]Epoch: 1, train for the 245-th batch, train loss: 0.4357622563838959:  64%|███████    | 244/383 [01:04<00:41,  3.34it/s]Epoch: 2, train for the 31-th batch, train loss: 0.4681731164455414:  12%|█▌           | 30/241 [00:03<00:25,  8.41it/s]Epoch: 3, train for the 40-th batch, train loss: 0.4920322299003601:  27%|███▍         | 39/146 [00:05<00:14,  7.19it/s]evaluate for the 53-th batch, evaluate loss: 0.5231936573982239:  79%|██████████████▏   | 52/66 [00:06<00:01,  8.26it/s]evaluate for the 53-th batch, evaluate loss: 0.5231936573982239:  80%|██████████████▍   | 53/66 [00:06<00:01,  7.69it/s]Epoch: 2, train for the 31-th batch, train loss: 0.4681731164455414:  13%|█▋           | 31/241 [00:04<00:27,  7.56it/s]Epoch: 3, train for the 40-th batch, train loss: 0.4920322299003601:  27%|███▌         | 40/146 [00:05<00:14,  7.38it/s]Epoch: 2, train for the 139-th batch, train loss: 0.5000372529029846:  91%|██████████ | 138/151 [00:22<00:02,  5.18it/s]Epoch: 2, train for the 139-th batch, train loss: 0.5000372529029846:  92%|██████████▏| 139/151 [00:22<00:02,  4.77it/s]Epoch: 1, train for the 245-th batch, train loss: 0.4357622563838959:  64%|███████    | 245/383 [01:05<00:43,  3.14it/s]Epoch: 3, train for the 111-th batch, train loss: 0.5618881583213806:  92%|██████████▏| 110/119 [00:16<00:01,  6.99it/s]Epoch: 2, train for the 32-th batch, train loss: 0.43482697010040283:  13%|█▌          | 31/241 [00:04<00:27,  7.56it/s]Epoch: 3, train for the 111-th batch, train loss: 0.5618881583213806:  93%|██████████▎| 111/119 [00:16<00:01,  6.95it/s]evaluate for the 54-th batch, evaluate loss: 0.563590943813324:  80%|███████████████▎   | 53/66 [00:06<00:01,  7.69it/s]evaluate for the 54-th batch, evaluate loss: 0.563590943813324:  82%|███████████████▌   | 54/66 [00:06<00:01,  7.89it/s]Epoch: 2, train for the 32-th batch, train loss: 0.43482697010040283:  13%|█▌          | 32/241 [00:04<00:26,  7.74it/s]Epoch: 3, train for the 41-th batch, train loss: 0.4918499290943146:  27%|███▌         | 40/146 [00:05<00:14,  7.38it/s]Epoch: 3, train for the 41-th batch, train loss: 0.4918499290943146:  28%|███▋         | 41/146 [00:05<00:14,  7.26it/s]Epoch: 2, train for the 140-th batch, train loss: 0.4961692690849304:  92%|██████████▏| 139/151 [00:22<00:02,  4.77it/s]Epoch: 2, train for the 140-th batch, train loss: 0.4961692690849304:  93%|██████████▏| 140/151 [00:22<00:02,  5.13it/s]evaluate for the 55-th batch, evaluate loss: 0.5062798261642456:  82%|██████████████▋   | 54/66 [00:06<00:01,  7.89it/s]evaluate for the 55-th batch, evaluate loss: 0.5062798261642456:  83%|███████████████   | 55/66 [00:06<00:01,  7.92it/s]Epoch: 2, train for the 33-th batch, train loss: 0.6164114475250244:  13%|█▋           | 32/241 [00:04<00:26,  7.74it/s]Epoch: 2, train for the 33-th batch, train loss: 0.6164114475250244:  14%|█▊           | 33/241 [00:04<00:28,  7.39it/s]Epoch: 1, train for the 246-th batch, train loss: 0.47825637459754944:  64%|██████▍   | 245/383 [01:05<00:43,  3.14it/s]Epoch: 3, train for the 112-th batch, train loss: 0.49315351247787476:  93%|█████████▎| 111/119 [00:16<00:01,  6.95it/s]Epoch: 3, train for the 42-th batch, train loss: 0.49738067388534546:  28%|███▎        | 41/146 [00:05<00:14,  7.26it/s]Epoch: 3, train for the 42-th batch, train loss: 0.49738067388534546:  29%|███▍        | 42/146 [00:05<00:15,  6.64it/s]Epoch: 3, train for the 112-th batch, train loss: 0.49315351247787476:  94%|█████████▍| 112/119 [00:16<00:01,  6.07it/s]Epoch: 2, train for the 141-th batch, train loss: 0.5210807919502258:  93%|██████████▏| 140/151 [00:22<00:02,  5.13it/s]Epoch: 2, train for the 141-th batch, train loss: 0.5210807919502258:  93%|██████████▎| 141/151 [00:22<00:01,  5.46it/s]evaluate for the 56-th batch, evaluate loss: 0.5336557030677795:  83%|███████████████   | 55/66 [00:06<00:01,  7.92it/s]evaluate for the 56-th batch, evaluate loss: 0.5336557030677795:  85%|███████████████▎  | 56/66 [00:06<00:01,  8.06it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5562545657157898:  14%|█▊           | 33/241 [00:04<00:28,  7.39it/s]Epoch: 1, train for the 246-th batch, train loss: 0.47825637459754944:  64%|██████▍   | 246/383 [01:05<00:43,  3.15it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5562545657157898:  14%|█▊           | 34/241 [00:04<00:26,  7.82it/s]Epoch: 3, train for the 43-th batch, train loss: 0.5001527667045593:  29%|███▋         | 42/146 [00:06<00:15,  6.64it/s]Epoch: 3, train for the 43-th batch, train loss: 0.5001527667045593:  29%|███▊         | 43/146 [00:06<00:15,  6.77it/s]evaluate for the 57-th batch, evaluate loss: 0.5983740091323853:  85%|███████████████▎  | 56/66 [00:06<00:01,  8.06it/s]evaluate for the 57-th batch, evaluate loss: 0.5983740091323853:  86%|███████████████▌  | 57/66 [00:06<00:01,  8.20it/s]Epoch: 3, train for the 113-th batch, train loss: 0.4673846364021301:  94%|██████████▎| 112/119 [00:16<00:01,  6.07it/s]Epoch: 3, train for the 113-th batch, train loss: 0.4673846364021301:  95%|██████████▍| 113/119 [00:16<00:00,  6.04it/s]Epoch: 2, train for the 35-th batch, train loss: 0.5744152069091797:  14%|█▊           | 34/241 [00:04<00:26,  7.82it/s]Epoch: 2, train for the 142-th batch, train loss: 0.49056312441825867:  93%|█████████▎| 141/151 [00:23<00:01,  5.46it/s]Epoch: 2, train for the 35-th batch, train loss: 0.5744152069091797:  15%|█▉           | 35/241 [00:04<00:28,  7.19it/s]Epoch: 2, train for the 142-th batch, train loss: 0.49056312441825867:  94%|█████████▍| 142/151 [00:23<00:01,  5.31it/s]Epoch: 3, train for the 44-th batch, train loss: 0.4757614731788635:  29%|███▊         | 43/146 [00:06<00:15,  6.77it/s]Epoch: 3, train for the 44-th batch, train loss: 0.4757614731788635:  30%|███▉         | 44/146 [00:06<00:14,  7.16it/s]Epoch: 1, train for the 247-th batch, train loss: 0.3669063448905945:  64%|███████    | 246/383 [01:05<00:43,  3.15it/s]Epoch: 3, train for the 114-th batch, train loss: 0.4640843868255615:  95%|██████████▍| 113/119 [00:16<00:00,  6.04it/s]evaluate for the 58-th batch, evaluate loss: 0.5566906332969666:  86%|███████████████▌  | 57/66 [00:06<00:01,  8.20it/s]evaluate for the 58-th batch, evaluate loss: 0.5566906332969666:  88%|███████████████▊  | 58/66 [00:06<00:01,  7.79it/s]Epoch: 3, train for the 114-th batch, train loss: 0.4640843868255615:  96%|██████████▌| 114/119 [00:16<00:00,  6.37it/s]Epoch: 2, train for the 36-th batch, train loss: 0.5682212114334106:  15%|█▉           | 35/241 [00:04<00:28,  7.19it/s]Epoch: 2, train for the 36-th batch, train loss: 0.5682212114334106:  15%|█▉           | 36/241 [00:04<00:28,  7.26it/s]Epoch: 1, train for the 247-th batch, train loss: 0.3669063448905945:  64%|███████    | 247/383 [01:05<00:42,  3.19it/s]Epoch: 3, train for the 45-th batch, train loss: 0.4985802471637726:  30%|███▉         | 44/146 [00:06<00:14,  7.16it/s]Epoch: 3, train for the 45-th batch, train loss: 0.4985802471637726:  31%|████         | 45/146 [00:06<00:13,  7.47it/s]Epoch: 2, train for the 143-th batch, train loss: 0.47545862197875977:  94%|█████████▍| 142/151 [00:23<00:01,  5.31it/s]evaluate for the 59-th batch, evaluate loss: 0.5694695115089417:  88%|███████████████▊  | 58/66 [00:06<00:01,  7.79it/s]evaluate for the 59-th batch, evaluate loss: 0.5694695115089417:  89%|████████████████  | 59/66 [00:06<00:00,  8.05it/s]Epoch: 2, train for the 143-th batch, train loss: 0.47545862197875977:  95%|█████████▍| 143/151 [00:23<00:01,  5.28it/s]Epoch: 3, train for the 115-th batch, train loss: 0.46966707706451416:  96%|█████████▌| 114/119 [00:16<00:00,  6.37it/s]Epoch: 3, train for the 115-th batch, train loss: 0.46966707706451416:  97%|█████████▋| 115/119 [00:16<00:00,  6.52it/s]Epoch: 2, train for the 37-th batch, train loss: 0.4746567904949188:  15%|█▉           | 36/241 [00:04<00:28,  7.26it/s]Epoch: 3, train for the 46-th batch, train loss: 0.48799577355384827:  31%|███▋        | 45/146 [00:06<00:13,  7.47it/s]Epoch: 2, train for the 37-th batch, train loss: 0.4746567904949188:  15%|█▉           | 37/241 [00:04<00:27,  7.40it/s]Epoch: 3, train for the 46-th batch, train loss: 0.48799577355384827:  32%|███▊        | 46/146 [00:06<00:12,  7.90it/s]evaluate for the 60-th batch, evaluate loss: 0.5708930492401123:  89%|████████████████  | 59/66 [00:07<00:00,  8.05it/s]evaluate for the 60-th batch, evaluate loss: 0.5708930492401123:  91%|████████████████▎ | 60/66 [00:07<00:00,  7.98it/s]Epoch: 3, train for the 116-th batch, train loss: 0.4814591109752655:  97%|██████████▋| 115/119 [00:17<00:00,  6.52it/s]Epoch: 2, train for the 144-th batch, train loss: 0.4634160101413727:  95%|██████████▍| 143/151 [00:23<00:01,  5.28it/s]Epoch: 3, train for the 116-th batch, train loss: 0.4814591109752655:  97%|██████████▋| 116/119 [00:17<00:00,  6.39it/s]Epoch: 2, train for the 38-th batch, train loss: 0.5553897619247437:  15%|█▉           | 37/241 [00:04<00:27,  7.40it/s]Epoch: 2, train for the 144-th batch, train loss: 0.4634160101413727:  95%|██████████▍| 144/151 [00:23<00:01,  5.10it/s]evaluate for the 61-th batch, evaluate loss: 0.5766717195510864:  91%|████████████████▎ | 60/66 [00:07<00:00,  7.98it/s]evaluate for the 61-th batch, evaluate loss: 0.5766717195510864:  92%|████████████████▋ | 61/66 [00:07<00:00,  8.46it/s]Epoch: 3, train for the 47-th batch, train loss: 0.5119710564613342:  32%|████         | 46/146 [00:06<00:12,  7.90it/s]Epoch: 2, train for the 38-th batch, train loss: 0.5553897619247437:  16%|██           | 38/241 [00:04<00:28,  7.00it/s]Epoch: 3, train for the 47-th batch, train loss: 0.5119710564613342:  32%|████▏        | 47/146 [00:06<00:13,  7.33it/s]Epoch: 3, train for the 117-th batch, train loss: 0.44421473145484924:  97%|█████████▋| 116/119 [00:17<00:00,  6.39it/s]Epoch: 3, train for the 117-th batch, train loss: 0.44421473145484924:  98%|█████████▊| 117/119 [00:17<00:00,  6.75it/s]Epoch: 1, train for the 248-th batch, train loss: 0.36249762773513794:  64%|██████▍   | 247/383 [01:06<00:42,  3.19it/s]evaluate for the 62-th batch, evaluate loss: 0.5941420793533325:  92%|████████████████▋ | 61/66 [00:07<00:00,  8.46it/s]evaluate for the 62-th batch, evaluate loss: 0.5941420793533325:  94%|████████████████▉ | 62/66 [00:07<00:00,  8.49it/s]Epoch: 3, train for the 48-th batch, train loss: 0.4752136468887329:  32%|████▏        | 47/146 [00:06<00:13,  7.33it/s]Epoch: 3, train for the 48-th batch, train loss: 0.4752136468887329:  33%|████▎        | 48/146 [00:06<00:13,  7.33it/s]Epoch: 1, train for the 248-th batch, train loss: 0.36249762773513794:  65%|██████▍   | 248/383 [01:06<00:47,  2.84it/s]Epoch: 2, train for the 145-th batch, train loss: 0.4764906167984009:  95%|██████████▍| 144/151 [00:23<00:01,  5.10it/s]Epoch: 2, train for the 39-th batch, train loss: 0.6030449271202087:  16%|██           | 38/241 [00:05<00:28,  7.00it/s]Epoch: 2, train for the 39-th batch, train loss: 0.6030449271202087:  16%|██           | 39/241 [00:05<00:31,  6.40it/s]Epoch: 2, train for the 145-th batch, train loss: 0.4764906167984009:  96%|██████████▌| 145/151 [00:23<00:01,  5.06it/s]evaluate for the 63-th batch, evaluate loss: 0.548812985420227:  94%|█████████████████▊ | 62/66 [00:07<00:00,  8.49it/s]evaluate for the 63-th batch, evaluate loss: 0.548812985420227:  95%|██████████████████▏| 63/66 [00:07<00:00,  8.46it/s]Epoch: 3, train for the 118-th batch, train loss: 0.444762259721756:  98%|███████████▊| 117/119 [00:17<00:00,  6.75it/s]Epoch: 3, train for the 118-th batch, train loss: 0.444762259721756:  99%|███████████▉| 118/119 [00:17<00:00,  6.61it/s]Epoch: 3, train for the 49-th batch, train loss: 0.49714770913124084:  33%|███▉        | 48/146 [00:06<00:13,  7.33it/s]Epoch: 3, train for the 49-th batch, train loss: 0.49714770913124084:  34%|████        | 49/146 [00:06<00:12,  7.53it/s]Epoch: 2, train for the 40-th batch, train loss: 0.6250225901603699:  16%|██           | 39/241 [00:05<00:31,  6.40it/s]Epoch: 2, train for the 40-th batch, train loss: 0.6250225901603699:  17%|██▏          | 40/241 [00:05<00:30,  6.60it/s]Epoch: 2, train for the 146-th batch, train loss: 0.4516265392303467:  96%|██████████▌| 145/151 [00:23<00:01,  5.06it/s]Epoch: 2, train for the 146-th batch, train loss: 0.4516265392303467:  97%|██████████▋| 146/151 [00:23<00:00,  5.13it/s]Epoch: 3, train for the 119-th batch, train loss: 0.4816211462020874:  99%|██████████▉| 118/119 [00:17<00:00,  6.61it/s]Epoch: 3, train for the 119-th batch, train loss: 0.4816211462020874: 100%|███████████| 119/119 [00:17<00:00,  6.66it/s]Epoch: 3, train for the 119-th batch, train loss: 0.4816211462020874: 100%|███████████| 119/119 [00:17<00:00,  6.78it/s]
Epoch: 1, train for the 249-th batch, train loss: 0.43439429998397827:  65%|██████▍   | 248/383 [01:06<00:47,  2.84it/s]evaluate for the 64-th batch, evaluate loss: 0.5458177328109741:  95%|█████████████████▏| 63/66 [00:07<00:00,  8.46it/s]evaluate for the 64-th batch, evaluate loss: 0.5458177328109741:  97%|█████████████████▍| 64/66 [00:07<00:00,  6.95it/s]Epoch: 3, train for the 50-th batch, train loss: 0.5164098739624023:  34%|████▎        | 49/146 [00:06<00:12,  7.53it/s]Epoch: 3, train for the 50-th batch, train loss: 0.5164098739624023:  34%|████▍        | 50/146 [00:06<00:14,  6.83it/s]Epoch: 2, train for the 41-th batch, train loss: 0.54038405418396:  17%|██▍            | 40/241 [00:05<00:30,  6.60it/s]Epoch: 2, train for the 41-th batch, train loss: 0.54038405418396:  17%|██▌            | 41/241 [00:05<00:28,  6.90it/s]Epoch: 1, train for the 249-th batch, train loss: 0.43439429998397827:  65%|██████▌   | 249/383 [01:06<00:46,  2.88it/s]Epoch: 2, train for the 147-th batch, train loss: 0.5023481845855713:  97%|██████████▋| 146/151 [00:24<00:00,  5.13it/s]Epoch: 3, train for the 51-th batch, train loss: 0.51421058177948:  34%|█████▏         | 50/146 [00:07<00:14,  6.83it/s]Epoch: 2, train for the 147-th batch, train loss: 0.5023481845855713:  97%|██████████▋| 147/151 [00:24<00:00,  5.47it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5691432356834412:  17%|██▏          | 41/241 [00:05<00:28,  6.90it/s]evaluate for the 65-th batch, evaluate loss: 0.5882555842399597:  97%|█████████████████▍| 64/66 [00:07<00:00,  6.95it/s]evaluate for the 65-th batch, evaluate loss: 0.5882555842399597:  98%|█████████████████▋| 65/66 [00:07<00:00,  6.94it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5691432356834412:  17%|██▎          | 42/241 [00:05<00:27,  7.20it/s]  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 3, train for the 52-th batch, train loss: 0.48297521471977234:  34%|████        | 50/146 [00:07<00:14,  6.83it/s]evaluate for the 1-th batch, evaluate loss: 0.4714388847351074:   0%|                            | 0/40 [00:00<?, ?it/s]Epoch: 3, train for the 52-th batch, train loss: 0.48297521471977234:  36%|████▎       | 52/146 [00:07<00:12,  7.46it/s]Epoch: 2, train for the 148-th batch, train loss: 0.47021135687828064:  97%|█████████▋| 147/151 [00:24<00:00,  5.47it/s]Epoch: 2, train for the 148-th batch, train loss: 0.47021135687828064:  98%|█████████▊| 148/151 [00:24<00:00,  5.54it/s]evaluate for the 66-th batch, evaluate loss: 0.5423487424850464:  98%|█████████████████▋| 65/66 [00:07<00:00,  6.94it/s]evaluate for the 66-th batch, evaluate loss: 0.5423487424850464: 100%|██████████████████| 66/66 [00:07<00:00,  7.12it/s]Epoch: 2, train for the 43-th batch, train loss: 0.6079319715499878:  17%|██▎          | 42/241 [00:05<00:27,  7.20it/s]evaluate for the 66-th batch, evaluate loss: 0.5423487424850464: 100%|██████████████████| 66/66 [00:07<00:00,  8.40it/s]
Epoch: 2, train for the 43-th batch, train loss: 0.6079319715499878:  18%|██▎          | 43/241 [00:05<00:27,  7.25it/s]evaluate for the 2-th batch, evaluate loss: 0.48747336864471436:   0%|                           | 0/40 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.48747336864471436:   5%|▉                  | 2/40 [00:00<00:02, 14.88it/s]Epoch: 1, train for the 250-th batch, train loss: 0.4284803569316864:  65%|███████▏   | 249/383 [01:06<00:46,  2.88it/s]evaluate for the 3-th batch, evaluate loss: 0.48390695452690125:   5%|▉                  | 2/40 [00:00<00:02, 14.88it/s]Epoch: 3, train for the 53-th batch, train loss: 0.488252729177475:  36%|████▉         | 52/146 [00:07<00:12,  7.46it/s]Epoch: 1, train for the 250-th batch, train loss: 0.4284803569316864:  65%|███████▏   | 250/383 [01:06<00:47,  2.82it/s]evaluate for the 4-th batch, evaluate loss: 0.5477294325828552:   5%|█                   | 2/40 [00:00<00:02, 14.88it/s]evaluate for the 4-th batch, evaluate loss: 0.5477294325828552:  10%|██                  | 4/40 [00:00<00:02, 14.93it/s]Epoch: 3, train for the 53-th batch, train loss: 0.488252729177475:  36%|█████         | 53/146 [00:07<00:13,  6.82it/s]Epoch: 2, train for the 149-th batch, train loss: 0.4668307900428772:  98%|██████████▊| 148/151 [00:24<00:00,  5.54it/s]Epoch: 2, train for the 44-th batch, train loss: 0.6376184821128845:  18%|██▎          | 43/241 [00:05<00:27,  7.25it/s]Epoch: 2, train for the 149-th batch, train loss: 0.4668307900428772:  99%|██████████▊| 149/151 [00:24<00:00,  5.53it/s]Epoch: 2, train for the 44-th batch, train loss: 0.6376184821128845:  18%|██▎          | 44/241 [00:05<00:29,  6.70it/s]evaluate for the 5-th batch, evaluate loss: 0.5214782357215881:  10%|██                  | 4/40 [00:00<00:02, 14.93it/s]Epoch: 3, train for the 54-th batch, train loss: 0.5404327511787415:  36%|████▋        | 53/146 [00:07<00:13,  6.82it/s]Epoch: 3, train for the 54-th batch, train loss: 0.5404327511787415:  37%|████▊        | 54/146 [00:07<00:13,  7.03it/s]evaluate for the 6-th batch, evaluate loss: 0.4895223081111908:  10%|██                  | 4/40 [00:00<00:02, 14.93it/s]evaluate for the 6-th batch, evaluate loss: 0.4895223081111908:  15%|███                 | 6/40 [00:00<00:02, 14.70it/s]Epoch: 2, train for the 45-th batch, train loss: 0.46930983662605286:  18%|██▏         | 44/241 [00:05<00:29,  6.70it/s]Epoch: 2, train for the 45-th batch, train loss: 0.46930983662605286:  19%|██▏         | 45/241 [00:05<00:28,  6.96it/s]evaluate for the 7-th batch, evaluate loss: 0.5086190104484558:  15%|███                 | 6/40 [00:00<00:02, 14.70it/s]Epoch: 2, train for the 150-th batch, train loss: 0.4865837097167969:  99%|██████████▊| 149/151 [00:24<00:00,  5.53it/s]Epoch: 2, train for the 150-th batch, train loss: 0.4865837097167969:  99%|██████████▉| 150/151 [00:24<00:00,  5.06it/s]Epoch: 2, train for the 46-th batch, train loss: 0.5250030159950256:  19%|██▍          | 45/241 [00:06<00:28,  6.96it/s]Epoch: 3, train for the 55-th batch, train loss: 0.5628901720046997:  37%|████▊        | 54/146 [00:07<00:13,  7.03it/s]evaluate for the 8-th batch, evaluate loss: 0.4611503481864929:  15%|███                 | 6/40 [00:00<00:02, 14.70it/s]evaluate for the 8-th batch, evaluate loss: 0.4611503481864929:  20%|████                | 8/40 [00:00<00:02, 14.92it/s]Epoch: 2, train for the 46-th batch, train loss: 0.5250030159950256:  19%|██▍          | 46/241 [00:06<00:26,  7.47it/s]Epoch: 3, train for the 55-th batch, train loss: 0.5628901720046997:  38%|████▉        | 55/146 [00:07<00:12,  7.01it/s]evaluate for the 9-th batch, evaluate loss: 0.5112544894218445:  20%|████                | 8/40 [00:00<00:02, 14.92it/s]Epoch: 1, train for the 251-th batch, train loss: 0.4377201497554779:  65%|███████▏   | 250/383 [01:07<00:47,  2.82it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5795833468437195:  19%|██▍          | 46/241 [00:06<00:26,  7.47it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5795833468437195:  20%|██▌          | 47/241 [00:06<00:25,  7.58it/s]evaluate for the 10-th batch, evaluate loss: 0.5438299179077148:  20%|███▊               | 8/40 [00:00<00:02, 14.92it/s]evaluate for the 10-th batch, evaluate loss: 0.5438299179077148:  25%|████▌             | 10/40 [00:00<00:02, 14.66it/s]Epoch: 1, train for the 251-th batch, train loss: 0.4377201497554779:  66%|███████▏   | 251/383 [01:07<00:49,  2.69it/s]Epoch: 3, train for the 56-th batch, train loss: 0.5156106352806091:  38%|████▉        | 55/146 [00:07<00:12,  7.01it/s]Epoch: 3, train for the 56-th batch, train loss: 0.5156106352806091:  38%|████▉        | 56/146 [00:07<00:12,  6.93it/s]evaluate for the 11-th batch, evaluate loss: 0.48872748017311096:  25%|████▎            | 10/40 [00:00<00:02, 14.66it/s]Epoch: 2, train for the 48-th batch, train loss: 0.5588926076889038:  20%|██▌          | 47/241 [00:06<00:25,  7.58it/s]Epoch: 2, train for the 48-th batch, train loss: 0.5588926076889038:  20%|██▌          | 48/241 [00:06<00:23,  8.15it/s]evaluate for the 12-th batch, evaluate loss: 0.5057942867279053:  25%|████▌             | 10/40 [00:00<00:02, 14.66it/s]evaluate for the 12-th batch, evaluate loss: 0.5057942867279053:  30%|█████▍            | 12/40 [00:00<00:01, 15.41it/s]Epoch: 3, train for the 57-th batch, train loss: 0.5330153703689575:  38%|████▉        | 56/146 [00:07<00:12,  6.93it/s]Epoch: 3, train for the 57-th batch, train loss: 0.5330153703689575:  39%|█████        | 57/146 [00:07<00:12,  7.05it/s]Epoch: 2, train for the 151-th batch, train loss: 0.5455724596977234:  99%|██████████▉| 150/151 [00:25<00:00,  5.06it/s]evaluate for the 13-th batch, evaluate loss: 0.47893819212913513:  30%|█████            | 12/40 [00:00<00:01, 15.41it/s]Epoch: 2, train for the 151-th batch, train loss: 0.5455724596977234: 100%|███████████| 151/151 [00:25<00:00,  4.26it/s]Epoch: 2, train for the 151-th batch, train loss: 0.5455724596977234: 100%|███████████| 151/151 [00:25<00:00,  6.03it/s]
Epoch: 2, train for the 49-th batch, train loss: 0.5538373589515686:  20%|██▌          | 48/241 [00:06<00:23,  8.15it/s]evaluate for the 14-th batch, evaluate loss: 0.49391859769821167:  30%|█████            | 12/40 [00:00<00:01, 15.41it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5538373589515686:  20%|██▋          | 49/241 [00:06<00:23,  8.04it/s]Epoch: 1, train for the 252-th batch, train loss: 0.31654423475265503:  66%|██████▌   | 251/383 [01:07<00:49,  2.69it/s]Epoch: 3, train for the 58-th batch, train loss: 0.495415061712265:  39%|█████▍        | 57/146 [00:08<00:12,  7.05it/s]evaluate for the 15-th batch, evaluate loss: 0.5124675631523132:  30%|█████▍            | 12/40 [00:00<00:01, 15.41it/s]evaluate for the 15-th batch, evaluate loss: 0.5124675631523132:  38%|██████▊           | 15/40 [00:00<00:01, 16.70it/s]Epoch: 3, train for the 58-th batch, train loss: 0.495415061712265:  40%|█████▌        | 58/146 [00:08<00:12,  7.25it/s]Epoch: 1, train for the 252-th batch, train loss: 0.31654423475265503:  66%|██████▌   | 252/383 [01:07<00:45,  2.90it/s]evaluate for the 16-th batch, evaluate loss: 0.48798996210098267:  38%|██████▍          | 15/40 [00:01<00:01, 16.70it/s]  0%|                                                                                            | 0/46 [00:00<?, ?it/s]  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5555009245872498:  20%|██▋          | 49/241 [00:06<00:23,  8.04it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5555009245872498:  21%|██▋          | 50/241 [00:06<00:24,  7.83it/s]Epoch: 3, train for the 59-th batch, train loss: 0.4675368666648865:  40%|█████▏       | 58/146 [00:08<00:12,  7.25it/s]evaluate for the 17-th batch, evaluate loss: 0.5005019903182983:  38%|██████▊           | 15/40 [00:01<00:01, 16.70it/s]evaluate for the 17-th batch, evaluate loss: 0.5005019903182983:  42%|███████▋          | 17/40 [00:01<00:01, 16.40it/s]Epoch: 3, train for the 59-th batch, train loss: 0.4675368666648865:  40%|█████▎       | 59/146 [00:08<00:11,  7.31it/s]evaluate for the 1-th batch, evaluate loss: 0.5389662981033325:   0%|                            | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.5389662981033325:   2%|▍                   | 1/46 [00:00<00:04,  9.74it/s]evaluate for the 1-th batch, evaluate loss: 0.6643599271774292:   0%|                            | 0/40 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6643599271774292:   2%|▌                   | 1/40 [00:00<00:04,  8.37it/s]evaluate for the 18-th batch, evaluate loss: 0.49598148465156555:  42%|███████▏         | 17/40 [00:01<00:01, 16.40it/s]Epoch: 2, train for the 51-th batch, train loss: 0.5416958332061768:  21%|██▋          | 50/241 [00:06<00:24,  7.83it/s]Epoch: 2, train for the 51-th batch, train loss: 0.5416958332061768:  21%|██▊          | 51/241 [00:06<00:25,  7.50it/s]evaluate for the 2-th batch, evaluate loss: 0.5619748830795288:   2%|▍                   | 1/46 [00:00<00:04,  9.74it/s]Epoch: 1, train for the 253-th batch, train loss: 0.3192070424556732:  66%|███████▏   | 252/383 [01:07<00:45,  2.90it/s]evaluate for the 19-th batch, evaluate loss: 0.49603572487831116:  42%|███████▏         | 17/40 [00:01<00:01, 16.40it/s]evaluate for the 19-th batch, evaluate loss: 0.49603572487831116:  48%|████████         | 19/40 [00:01<00:01, 15.58it/s]Epoch: 3, train for the 60-th batch, train loss: 0.5224500894546509:  40%|█████▎       | 59/146 [00:08<00:11,  7.31it/s]evaluate for the 2-th batch, evaluate loss: 0.6477555632591248:   2%|▌                   | 1/40 [00:00<00:04,  8.37it/s]evaluate for the 2-th batch, evaluate loss: 0.6477555632591248:   5%|█                   | 2/40 [00:00<00:04,  9.17it/s]Epoch: 3, train for the 60-th batch, train loss: 0.5224500894546509:  41%|█████▎       | 60/146 [00:08<00:12,  6.97it/s]evaluate for the 3-th batch, evaluate loss: 0.549861490726471:   2%|▍                    | 1/46 [00:00<00:04,  9.74it/s]evaluate for the 3-th batch, evaluate loss: 0.549861490726471:   7%|█▎                   | 3/46 [00:00<00:03, 11.89it/s]Epoch: 1, train for the 253-th batch, train loss: 0.3192070424556732:  66%|███████▎   | 253/383 [01:07<00:43,  2.96it/s]evaluate for the 20-th batch, evaluate loss: 0.4862697422504425:  48%|████████▌         | 19/40 [00:01<00:01, 15.58it/s]Epoch: 2, train for the 52-th batch, train loss: 0.5925047397613525:  21%|██▊          | 51/241 [00:06<00:25,  7.50it/s]Epoch: 2, train for the 52-th batch, train loss: 0.5925047397613525:  22%|██▊          | 52/241 [00:06<00:26,  7.04it/s]evaluate for the 4-th batch, evaluate loss: 0.5315114259719849:   7%|█▎                  | 3/46 [00:00<00:03, 11.89it/s]evaluate for the 3-th batch, evaluate loss: 0.629162609577179:   5%|█                    | 2/40 [00:00<00:04,  9.17it/s]evaluate for the 3-th batch, evaluate loss: 0.629162609577179:   8%|█▌                   | 3/40 [00:00<00:04,  8.21it/s]Epoch: 3, train for the 61-th batch, train loss: 0.532418966293335:  41%|█████▊        | 60/146 [00:08<00:12,  6.97it/s]evaluate for the 21-th batch, evaluate loss: 0.49994781613349915:  48%|████████         | 19/40 [00:01<00:01, 15.58it/s]evaluate for the 21-th batch, evaluate loss: 0.49994781613349915:  52%|████████▉        | 21/40 [00:01<00:01, 14.74it/s]Epoch: 3, train for the 61-th batch, train loss: 0.532418966293335:  42%|█████▊        | 61/146 [00:08<00:11,  7.16it/s]evaluate for the 4-th batch, evaluate loss: 0.6422981023788452:   8%|█▌                  | 3/40 [00:00<00:04,  8.21it/s]evaluate for the 4-th batch, evaluate loss: 0.6422981023788452:  10%|██                  | 4/40 [00:00<00:04,  8.29it/s]evaluate for the 22-th batch, evaluate loss: 0.48475417494773865:  52%|████████▉        | 21/40 [00:01<00:01, 14.74it/s]Epoch: 2, train for the 53-th batch, train loss: 0.419532835483551:  22%|███           | 52/241 [00:07<00:26,  7.04it/s]evaluate for the 5-th batch, evaluate loss: 0.5256012678146362:   7%|█▎                  | 3/46 [00:00<00:03, 11.89it/s]evaluate for the 5-th batch, evaluate loss: 0.5256012678146362:  11%|██▏                 | 5/46 [00:00<00:04,  9.96it/s]Epoch: 2, train for the 53-th batch, train loss: 0.419532835483551:  22%|███           | 53/241 [00:07<00:28,  6.68it/s]Epoch: 3, train for the 62-th batch, train loss: 0.4954913854598999:  42%|█████▍       | 61/146 [00:08<00:11,  7.16it/s]Epoch: 3, train for the 62-th batch, train loss: 0.4954913854598999:  42%|█████▌       | 62/146 [00:08<00:12,  6.89it/s]evaluate for the 23-th batch, evaluate loss: 0.4279032051563263:  52%|█████████▍        | 21/40 [00:01<00:01, 14.74it/s]evaluate for the 23-th batch, evaluate loss: 0.4279032051563263:  57%|██████████▎       | 23/40 [00:01<00:01, 13.54it/s]Epoch: 1, train for the 254-th batch, train loss: 0.4541417062282562:  66%|███████▎   | 253/383 [01:08<00:43,  2.96it/s]evaluate for the 6-th batch, evaluate loss: 0.5312737822532654:  11%|██▏                 | 5/46 [00:00<00:04,  9.96it/s]evaluate for the 5-th batch, evaluate loss: 0.643778383731842:  10%|██                   | 4/40 [00:00<00:04,  8.29it/s]evaluate for the 5-th batch, evaluate loss: 0.643778383731842:  12%|██▋                  | 5/40 [00:00<00:04,  8.33it/s]Epoch: 2, train for the 54-th batch, train loss: 0.372342973947525:  22%|███           | 53/241 [00:07<00:28,  6.68it/s]Epoch: 1, train for the 254-th batch, train loss: 0.4541417062282562:  66%|███████▎   | 254/383 [01:08<00:43,  2.94it/s]Epoch: 2, train for the 54-th batch, train loss: 0.372342973947525:  22%|███▏          | 54/241 [00:07<00:26,  7.01it/s]evaluate for the 24-th batch, evaluate loss: 0.4943893849849701:  57%|██████████▎       | 23/40 [00:01<00:01, 13.54it/s]evaluate for the 7-th batch, evaluate loss: 0.5388068556785583:  11%|██▏                 | 5/46 [00:00<00:04,  9.96it/s]evaluate for the 7-th batch, evaluate loss: 0.5388068556785583:  15%|███                 | 7/46 [00:00<00:03, 10.50it/s]Epoch: 3, train for the 63-th batch, train loss: 0.4926058351993561:  42%|█████▌       | 62/146 [00:08<00:12,  6.89it/s]Epoch: 3, train for the 63-th batch, train loss: 0.4926058351993561:  43%|█████▌       | 63/146 [00:08<00:12,  6.73it/s]evaluate for the 25-th batch, evaluate loss: 0.5259433388710022:  57%|██████████▎       | 23/40 [00:01<00:01, 13.54it/s]evaluate for the 25-th batch, evaluate loss: 0.5259433388710022:  62%|███████████▎      | 25/40 [00:01<00:01, 12.41it/s]Epoch: 2, train for the 55-th batch, train loss: 0.3649090528488159:  22%|██▉          | 54/241 [00:07<00:26,  7.01it/s]evaluate for the 6-th batch, evaluate loss: 0.6089058518409729:  12%|██▌                 | 5/40 [00:00<00:04,  8.33it/s]evaluate for the 6-th batch, evaluate loss: 0.6089058518409729:  15%|███                 | 6/40 [00:00<00:04,  7.73it/s]evaluate for the 8-th batch, evaluate loss: 0.5572790503501892:  15%|███                 | 7/46 [00:00<00:03, 10.50it/s]Epoch: 2, train for the 55-th batch, train loss: 0.3649090528488159:  23%|██▉          | 55/241 [00:07<00:26,  7.10it/s]evaluate for the 26-th batch, evaluate loss: 0.46255895495414734:  62%|██████████▋      | 25/40 [00:01<00:01, 12.41it/s]evaluate for the 9-th batch, evaluate loss: 0.539147675037384:  15%|███▏                 | 7/46 [00:00<00:03, 10.50it/s]evaluate for the 9-th batch, evaluate loss: 0.539147675037384:  20%|████                 | 9/46 [00:00<00:03, 11.07it/s]Epoch: 3, train for the 64-th batch, train loss: 0.5387434959411621:  43%|█████▌       | 63/146 [00:08<00:12,  6.73it/s]Epoch: 3, train for the 64-th batch, train loss: 0.5387434959411621:  44%|█████▋       | 64/146 [00:08<00:12,  6.50it/s]Epoch: 2, train for the 56-th batch, train loss: 0.3880050480365753:  23%|██▉          | 55/241 [00:07<00:26,  7.10it/s]evaluate for the 27-th batch, evaluate loss: 0.473743200302124:  62%|███████████▉       | 25/40 [00:01<00:01, 12.41it/s]evaluate for the 27-th batch, evaluate loss: 0.473743200302124:  68%|████████████▊      | 27/40 [00:01<00:01, 12.83it/s]Epoch: 2, train for the 56-th batch, train loss: 0.3880050480365753:  23%|███          | 56/241 [00:07<00:25,  7.30it/s]evaluate for the 10-th batch, evaluate loss: 0.5165344476699829:  20%|███▋               | 9/46 [00:00<00:03, 11.07it/s]Epoch: 1, train for the 255-th batch, train loss: 0.40289387106895447:  66%|██████▋   | 254/383 [01:08<00:43,  2.94it/s]evaluate for the 7-th batch, evaluate loss: 0.6580036878585815:  15%|███                 | 6/40 [00:00<00:04,  7.73it/s]evaluate for the 7-th batch, evaluate loss: 0.6580036878585815:  18%|███▌                | 7/40 [00:00<00:04,  7.00it/s]evaluate for the 11-th batch, evaluate loss: 0.5649591088294983:  20%|███▋               | 9/46 [00:00<00:03, 11.07it/s]evaluate for the 11-th batch, evaluate loss: 0.5649591088294983:  24%|████▎             | 11/46 [00:00<00:02, 12.23it/s]Epoch: 1, train for the 255-th batch, train loss: 0.40289387106895447:  67%|██████▋   | 255/383 [01:08<00:44,  2.88it/s]evaluate for the 28-th batch, evaluate loss: 0.45162132382392883:  68%|███████████▍     | 27/40 [00:01<00:01, 12.83it/s]Epoch: 3, train for the 65-th batch, train loss: 0.5482070446014404:  44%|█████▋       | 64/146 [00:09<00:12,  6.50it/s]Epoch: 3, train for the 65-th batch, train loss: 0.5482070446014404:  45%|█████▊       | 65/146 [00:09<00:12,  6.61it/s]Epoch: 2, train for the 57-th batch, train loss: 0.4342736303806305:  23%|███          | 56/241 [00:07<00:25,  7.30it/s]evaluate for the 8-th batch, evaluate loss: 0.6778658628463745:  18%|███▌                | 7/40 [00:01<00:04,  7.00it/s]Epoch: 2, train for the 57-th batch, train loss: 0.4342736303806305:  24%|███          | 57/241 [00:07<00:24,  7.40it/s]evaluate for the 12-th batch, evaluate loss: 0.50725919008255:  24%|████▊               | 11/46 [00:01<00:02, 12.23it/s]evaluate for the 29-th batch, evaluate loss: 0.49005958437919617:  68%|███████████▍     | 27/40 [00:02<00:01, 12.83it/s]evaluate for the 29-th batch, evaluate loss: 0.49005958437919617:  72%|████████████▎    | 29/40 [00:02<00:00, 12.29it/s]evaluate for the 13-th batch, evaluate loss: 0.5182321071624756:  24%|████▎             | 11/46 [00:01<00:02, 12.23it/s]evaluate for the 13-th batch, evaluate loss: 0.5182321071624756:  28%|█████             | 13/46 [00:01<00:02, 12.23it/s]Epoch: 3, train for the 66-th batch, train loss: 0.5400233864784241:  45%|█████▊       | 65/146 [00:09<00:12,  6.61it/s]evaluate for the 30-th batch, evaluate loss: 0.47049033641815186:  72%|████████████▎    | 29/40 [00:02<00:00, 12.29it/s]Epoch: 3, train for the 66-th batch, train loss: 0.5400233864784241:  45%|█████▉       | 66/146 [00:09<00:12,  6.56it/s]evaluate for the 9-th batch, evaluate loss: 0.6888523697853088:  18%|███▌                | 7/40 [00:01<00:04,  7.00it/s]evaluate for the 9-th batch, evaluate loss: 0.6888523697853088:  22%|████▌               | 9/40 [00:01<00:04,  7.58it/s]Epoch: 2, train for the 58-th batch, train loss: 0.3375948369503021:  24%|███          | 57/241 [00:07<00:24,  7.40it/s]Epoch: 2, train for the 58-th batch, train loss: 0.3375948369503021:  24%|███▏         | 58/241 [00:07<00:26,  6.99it/s]evaluate for the 14-th batch, evaluate loss: 0.5617892146110535:  28%|█████             | 13/46 [00:01<00:02, 12.23it/s]evaluate for the 31-th batch, evaluate loss: 0.4621371328830719:  72%|█████████████     | 29/40 [00:02<00:00, 12.29it/s]Epoch: 1, train for the 256-th batch, train loss: 0.3441716730594635:  67%|███████▎   | 255/383 [01:08<00:44,  2.88it/s]evaluate for the 31-th batch, evaluate loss: 0.4621371328830719:  78%|█████████████▉    | 31/40 [00:02<00:00, 12.11it/s]evaluate for the 10-th batch, evaluate loss: 0.6498392820358276:  22%|████▎              | 9/40 [00:01<00:04,  7.58it/s]evaluate for the 10-th batch, evaluate loss: 0.6498392820358276:  25%|████▌             | 10/40 [00:01<00:03,  7.71it/s]Epoch: 3, train for the 67-th batch, train loss: 0.5542246699333191:  45%|█████▉       | 66/146 [00:09<00:12,  6.56it/s]Epoch: 1, train for the 256-th batch, train loss: 0.3441716730594635:  67%|███████▎   | 256/383 [01:08<00:42,  2.98it/s]Epoch: 3, train for the 67-th batch, train loss: 0.5542246699333191:  46%|█████▉       | 67/146 [00:09<00:11,  6.73it/s]evaluate for the 32-th batch, evaluate loss: 0.48277992010116577:  78%|█████████████▏   | 31/40 [00:02<00:00, 12.11it/s]Epoch: 2, train for the 59-th batch, train loss: 0.607185423374176:  24%|███▎          | 58/241 [00:07<00:26,  6.99it/s]evaluate for the 15-th batch, evaluate loss: 0.5473436117172241:  28%|█████             | 13/46 [00:01<00:02, 12.23it/s]evaluate for the 15-th batch, evaluate loss: 0.5473436117172241:  33%|█████▊            | 15/46 [00:01<00:02, 11.54it/s]Epoch: 2, train for the 59-th batch, train loss: 0.607185423374176:  24%|███▍          | 59/241 [00:07<00:26,  6.98it/s]evaluate for the 33-th batch, evaluate loss: 0.46770262718200684:  78%|█████████████▏   | 31/40 [00:02<00:00, 12.11it/s]evaluate for the 33-th batch, evaluate loss: 0.46770262718200684:  82%|██████████████   | 33/40 [00:02<00:00, 13.41it/s]evaluate for the 11-th batch, evaluate loss: 0.6647495627403259:  25%|████▌             | 10/40 [00:01<00:03,  7.71it/s]evaluate for the 11-th batch, evaluate loss: 0.6647495627403259:  28%|████▉             | 11/40 [00:01<00:03,  7.89it/s]Epoch: 3, train for the 68-th batch, train loss: 0.4968988001346588:  46%|█████▉       | 67/146 [00:09<00:11,  6.73it/s]evaluate for the 34-th batch, evaluate loss: 0.5049221515655518:  82%|██████████████▊   | 33/40 [00:02<00:00, 13.41it/s]Epoch: 3, train for the 68-th batch, train loss: 0.4968988001346588:  47%|██████       | 68/146 [00:09<00:11,  6.68it/s]evaluate for the 16-th batch, evaluate loss: 0.5270416140556335:  33%|█████▊            | 15/46 [00:01<00:02, 11.54it/s]evaluate for the 35-th batch, evaluate loss: 0.5064852833747864:  82%|██████████████▊   | 33/40 [00:02<00:00, 13.41it/s]evaluate for the 35-th batch, evaluate loss: 0.5064852833747864:  88%|███████████████▊  | 35/40 [00:02<00:00, 13.64it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5922206044197083:  24%|███▏         | 59/241 [00:08<00:26,  6.98it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5922206044197083:  25%|███▏         | 60/241 [00:08<00:28,  6.26it/s]evaluate for the 36-th batch, evaluate loss: 0.4922044277191162:  88%|███████████████▊  | 35/40 [00:02<00:00, 13.64it/s]evaluate for the 17-th batch, evaluate loss: 0.4638710618019104:  33%|█████▊            | 15/46 [00:01<00:02, 11.54it/s]evaluate for the 17-th batch, evaluate loss: 0.4638710618019104:  37%|██████▋           | 17/46 [00:01<00:02, 10.50it/s]Epoch: 1, train for the 257-th batch, train loss: 0.4948965311050415:  67%|███████▎   | 256/383 [01:09<00:42,  2.98it/s]evaluate for the 12-th batch, evaluate loss: 0.7030419111251831:  28%|████▉             | 11/40 [00:01<00:03,  7.89it/s]evaluate for the 12-th batch, evaluate loss: 0.7030419111251831:  30%|█████▍            | 12/40 [00:01<00:04,  6.99it/s]evaluate for the 37-th batch, evaluate loss: 0.49600034952163696:  88%|██████████████▉  | 35/40 [00:02<00:00, 13.64it/s]evaluate for the 37-th batch, evaluate loss: 0.49600034952163696:  92%|███████████████▋ | 37/40 [00:02<00:00, 14.62it/s]Epoch: 1, train for the 257-th batch, train loss: 0.4948965311050415:  67%|███████▍   | 257/383 [01:09<00:43,  2.91it/s]evaluate for the 38-th batch, evaluate loss: 0.5378350615501404:  92%|████████████████▋ | 37/40 [00:02<00:00, 14.62it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5573652386665344:  25%|███▏         | 60/241 [00:08<00:28,  6.26it/s]evaluate for the 18-th batch, evaluate loss: 0.5197245478630066:  37%|██████▋           | 17/46 [00:01<00:02, 10.50it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5573652386665344:  25%|███▎         | 61/241 [00:08<00:28,  6.33it/s]evaluate for the 13-th batch, evaluate loss: 0.6504276990890503:  30%|█████▍            | 12/40 [00:01<00:04,  6.99it/s]evaluate for the 13-th batch, evaluate loss: 0.6504276990890503:  32%|█████▊            | 13/40 [00:01<00:03,  7.42it/s]Epoch: 3, train for the 69-th batch, train loss: 0.5156975388526917:  47%|██████       | 68/146 [00:09<00:11,  6.68it/s]Epoch: 3, train for the 69-th batch, train loss: 0.5156975388526917:  47%|██████▏      | 69/146 [00:09<00:14,  5.36it/s]evaluate for the 39-th batch, evaluate loss: 0.5279907584190369:  92%|████████████████▋ | 37/40 [00:02<00:00, 14.62it/s]evaluate for the 39-th batch, evaluate loss: 0.5279907584190369:  98%|█████████████████▌| 39/40 [00:02<00:00, 15.17it/s]evaluate for the 40-th batch, evaluate loss: 0.41441085934638977:  98%|████████████████▌| 39/40 [00:02<00:00, 15.17it/s]evaluate for the 40-th batch, evaluate loss: 0.41441085934638977: 100%|█████████████████| 40/40 [00:02<00:00, 14.28it/s]
evaluate for the 19-th batch, evaluate loss: 0.5657148361206055:  37%|██████▋           | 17/46 [00:01<00:02, 10.50it/s]evaluate for the 19-th batch, evaluate loss: 0.5657148361206055:  41%|███████▍          | 19/46 [00:01<00:02,  9.46it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5766907930374146:  25%|███▎         | 61/241 [00:08<00:28,  6.33it/s]evaluate for the 14-th batch, evaluate loss: 0.7121089100837708:  32%|█████▊            | 13/40 [00:01<00:03,  7.42it/s]evaluate for the 14-th batch, evaluate loss: 0.7121089100837708:  35%|██████▎           | 14/40 [00:01<00:03,  7.58it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5766907930374146:  26%|███▎         | 62/241 [00:08<00:28,  6.36it/s]Epoch: 1, train for the 258-th batch, train loss: 0.339862197637558:  67%|████████    | 257/383 [01:09<00:43,  2.91it/s]evaluate for the 20-th batch, evaluate loss: 0.516948938369751:  41%|███████▊           | 19/46 [00:01<00:02,  9.46it/s]Epoch: 3, train for the 70-th batch, train loss: 0.5347307324409485:  47%|██████▏      | 69/146 [00:10<00:14,  5.36it/s]Epoch: 3, train for the 70-th batch, train loss: 0.5347307324409485:  48%|██████▏      | 70/146 [00:10<00:14,  5.28it/s]evaluate for the 15-th batch, evaluate loss: 0.6760442852973938:  35%|██████▎           | 14/40 [00:01<00:03,  7.58it/s]Epoch: 1, train for the 258-th batch, train loss: 0.339862197637558:  67%|████████    | 258/383 [01:09<00:40,  3.11it/s]  0%|                                                                                            | 0/21 [00:00<?, ?it/s]Epoch: 2, train for the 63-th batch, train loss: 0.7252328395843506:  26%|███▎         | 62/241 [00:08<00:28,  6.36it/s]evaluate for the 21-th batch, evaluate loss: 0.5680469870567322:  41%|███████▍          | 19/46 [00:01<00:02,  9.46it/s]evaluate for the 21-th batch, evaluate loss: 0.5680469870567322:  46%|████████▏         | 21/46 [00:01<00:02, 10.18it/s]Epoch: 2, train for the 63-th batch, train loss: 0.7252328395843506:  26%|███▍         | 63/241 [00:08<00:27,  6.50it/s]evaluate for the 1-th batch, evaluate loss: 0.5980305075645447:   0%|                            | 0/21 [00:00<?, ?it/s]evaluate for the 16-th batch, evaluate loss: 0.641417920589447:  35%|██████▋            | 14/40 [00:02<00:03,  7.58it/s]evaluate for the 16-th batch, evaluate loss: 0.641417920589447:  40%|███████▌           | 16/40 [00:02<00:02,  8.25it/s]Epoch: 3, train for the 71-th batch, train loss: 0.5455289483070374:  48%|██████▏      | 70/146 [00:10<00:14,  5.28it/s]Epoch: 3, train for the 71-th batch, train loss: 0.5455289483070374:  49%|██████▎      | 71/146 [00:10<00:13,  5.56it/s]evaluate for the 22-th batch, evaluate loss: 0.5192095637321472:  46%|████████▏         | 21/46 [00:02<00:02, 10.18it/s]evaluate for the 2-th batch, evaluate loss: 0.677558958530426:   0%|                             | 0/21 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.677558958530426:  10%|██                   | 2/21 [00:00<00:01, 14.23it/s]Epoch: 2, train for the 64-th batch, train loss: 0.3122641146183014:  26%|███▍         | 63/241 [00:08<00:27,  6.50it/s]evaluate for the 3-th batch, evaluate loss: 0.6679789423942566:  10%|█▉                  | 2/21 [00:00<00:01, 14.23it/s]Epoch: 2, train for the 64-th batch, train loss: 0.3122641146183014:  27%|███▍         | 64/241 [00:08<00:28,  6.27it/s]evaluate for the 23-th batch, evaluate loss: 0.47790926694869995:  46%|███████▊         | 21/46 [00:02<00:02, 10.18it/s]evaluate for the 23-th batch, evaluate loss: 0.47790926694869995:  50%|████████▌        | 23/46 [00:02<00:02, 10.35it/s]Epoch: 1, train for the 259-th batch, train loss: 0.4262837767601013:  67%|███████▍   | 258/383 [01:09<00:40,  3.11it/s]evaluate for the 17-th batch, evaluate loss: 0.6543675065040588:  40%|███████▏          | 16/40 [00:02<00:02,  8.25it/s]evaluate for the 17-th batch, evaluate loss: 0.6543675065040588:  42%|███████▋          | 17/40 [00:02<00:02,  7.71it/s]Epoch: 3, train for the 72-th batch, train loss: 0.540087103843689:  49%|██████▊       | 71/146 [00:10<00:13,  5.56it/s]evaluate for the 4-th batch, evaluate loss: 0.5988377332687378:  10%|█▉                  | 2/21 [00:00<00:01, 14.23it/s]evaluate for the 4-th batch, evaluate loss: 0.5988377332687378:  19%|███▊                | 4/21 [00:00<00:01, 13.66it/s]Epoch: 3, train for the 72-th batch, train loss: 0.540087103843689:  49%|██████▉       | 72/146 [00:10<00:12,  5.71it/s]evaluate for the 24-th batch, evaluate loss: 0.48818662762641907:  50%|████████▌        | 23/46 [00:02<00:02, 10.35it/s]Epoch: 1, train for the 259-th batch, train loss: 0.4262837767601013:  68%|███████▍   | 259/383 [01:09<00:40,  3.05it/s]Epoch: 2, train for the 65-th batch, train loss: 0.36476776003837585:  27%|███▏        | 64/241 [00:08<00:28,  6.27it/s]evaluate for the 5-th batch, evaluate loss: 0.6904036402702332:  19%|███▊                | 4/21 [00:00<00:01, 13.66it/s]Epoch: 2, train for the 65-th batch, train loss: 0.36476776003837585:  27%|███▏        | 65/241 [00:08<00:27,  6.51it/s]evaluate for the 18-th batch, evaluate loss: 0.6390926837921143:  42%|███████▋          | 17/40 [00:02<00:02,  7.71it/s]evaluate for the 18-th batch, evaluate loss: 0.6390926837921143:  45%|████████          | 18/40 [00:02<00:02,  7.96it/s]evaluate for the 25-th batch, evaluate loss: 0.5149992108345032:  50%|█████████         | 23/46 [00:02<00:02, 10.35it/s]evaluate for the 25-th batch, evaluate loss: 0.5149992108345032:  54%|█████████▊        | 25/46 [00:02<00:02, 10.19it/s]Epoch: 3, train for the 73-th batch, train loss: 0.538429319858551:  49%|██████▉       | 72/146 [00:10<00:12,  5.71it/s]Epoch: 3, train for the 73-th batch, train loss: 0.538429319858551:  50%|███████       | 73/146 [00:10<00:12,  5.89it/s]evaluate for the 6-th batch, evaluate loss: 0.6724750995635986:  19%|███▊                | 4/21 [00:00<00:01, 13.66it/s]evaluate for the 6-th batch, evaluate loss: 0.6724750995635986:  29%|█████▋              | 6/21 [00:00<00:01, 13.03it/s]Epoch: 2, train for the 66-th batch, train loss: 0.307229220867157:  27%|███▊          | 65/241 [00:08<00:27,  6.51it/s]evaluate for the 26-th batch, evaluate loss: 0.5468533039093018:  54%|█████████▊        | 25/46 [00:02<00:02, 10.19it/s]evaluate for the 19-th batch, evaluate loss: 0.6970041990280151:  45%|████████          | 18/40 [00:02<00:02,  7.96it/s]evaluate for the 19-th batch, evaluate loss: 0.6970041990280151:  48%|████████▌         | 19/40 [00:02<00:02,  7.96it/s]Epoch: 2, train for the 66-th batch, train loss: 0.307229220867157:  27%|███▊          | 66/241 [00:09<00:26,  6.59it/s]evaluate for the 7-th batch, evaluate loss: 0.641196608543396:  29%|██████               | 6/21 [00:00<00:01, 13.03it/s]Epoch: 1, train for the 260-th batch, train loss: 0.3396034836769104:  68%|███████▍   | 259/383 [01:10<00:40,  3.05it/s]evaluate for the 27-th batch, evaluate loss: 0.5376086831092834:  54%|█████████▊        | 25/46 [00:02<00:02, 10.19it/s]evaluate for the 27-th batch, evaluate loss: 0.5376086831092834:  59%|██████████▌       | 27/46 [00:02<00:01, 10.84it/s]Epoch: 3, train for the 74-th batch, train loss: 0.5203306078910828:  50%|██████▌      | 73/146 [00:10<00:12,  5.89it/s]evaluate for the 8-th batch, evaluate loss: 0.6568841934204102:  29%|█████▋              | 6/21 [00:00<00:01, 13.03it/s]evaluate for the 8-th batch, evaluate loss: 0.6568841934204102:  38%|███████▌            | 8/21 [00:00<00:00, 13.21it/s]Epoch: 3, train for the 74-th batch, train loss: 0.5203306078910828:  51%|██████▌      | 74/146 [00:10<00:11,  6.11it/s]evaluate for the 20-th batch, evaluate loss: 0.669583797454834:  48%|█████████          | 19/40 [00:02<00:02,  7.96it/s]evaluate for the 20-th batch, evaluate loss: 0.669583797454834:  50%|█████████▌         | 20/40 [00:02<00:02,  8.28it/s]Epoch: 1, train for the 260-th batch, train loss: 0.3396034836769104:  68%|███████▍   | 260/383 [01:10<00:39,  3.12it/s]Epoch: 2, train for the 67-th batch, train loss: 0.417894572019577:  27%|███▊          | 66/241 [00:09<00:26,  6.59it/s]Epoch: 2, train for the 67-th batch, train loss: 0.417894572019577:  28%|███▉          | 67/241 [00:09<00:26,  6.67it/s]evaluate for the 28-th batch, evaluate loss: 0.5427236557006836:  59%|██████████▌       | 27/46 [00:02<00:01, 10.84it/s]evaluate for the 9-th batch, evaluate loss: 0.6389965415000916:  38%|███████▌            | 8/21 [00:00<00:00, 13.21it/s]Epoch: 3, train for the 75-th batch, train loss: 0.5521169900894165:  51%|██████▌      | 74/146 [00:10<00:11,  6.11it/s]evaluate for the 21-th batch, evaluate loss: 0.6879003047943115:  50%|█████████         | 20/40 [00:02<00:02,  8.28it/s]evaluate for the 21-th batch, evaluate loss: 0.6879003047943115:  52%|█████████▍        | 21/40 [00:02<00:02,  8.23it/s]Epoch: 3, train for the 75-th batch, train loss: 0.5521169900894165:  51%|██████▋      | 75/146 [00:10<00:10,  6.46it/s]evaluate for the 10-th batch, evaluate loss: 0.6704601645469666:  38%|███████▏           | 8/21 [00:00<00:00, 13.21it/s]evaluate for the 10-th batch, evaluate loss: 0.6704601645469666:  48%|████████▌         | 10/21 [00:00<00:00, 13.26it/s]evaluate for the 29-th batch, evaluate loss: 0.5124839544296265:  59%|██████████▌       | 27/46 [00:02<00:01, 10.84it/s]evaluate for the 29-th batch, evaluate loss: 0.5124839544296265:  63%|███████████▎      | 29/46 [00:02<00:01, 10.78it/s]Epoch: 2, train for the 68-th batch, train loss: 0.5061108469963074:  28%|███▌         | 67/241 [00:09<00:26,  6.67it/s]Epoch: 2, train for the 68-th batch, train loss: 0.5061108469963074:  28%|███▋         | 68/241 [00:09<00:26,  6.41it/s]evaluate for the 11-th batch, evaluate loss: 0.6607153415679932:  48%|████████▌         | 10/21 [00:00<00:00, 13.26it/s]Epoch: 3, train for the 76-th batch, train loss: 0.5219060182571411:  51%|██████▋      | 75/146 [00:10<00:10,  6.46it/s]Epoch: 3, train for the 76-th batch, train loss: 0.5219060182571411:  52%|██████▊      | 76/146 [00:10<00:10,  6.72it/s]evaluate for the 30-th batch, evaluate loss: 0.49805372953414917:  63%|██████████▋      | 29/46 [00:02<00:01, 10.78it/s]evaluate for the 22-th batch, evaluate loss: 0.7089506983757019:  52%|█████████▍        | 21/40 [00:02<00:02,  8.23it/s]evaluate for the 22-th batch, evaluate loss: 0.7089506983757019:  55%|█████████▉        | 22/40 [00:02<00:02,  7.48it/s]Epoch: 1, train for the 261-th batch, train loss: 0.33841079473495483:  68%|██████▊   | 260/383 [01:10<00:39,  3.12it/s]evaluate for the 12-th batch, evaluate loss: 0.6507742404937744:  48%|████████▌         | 10/21 [00:00<00:00, 13.26it/s]evaluate for the 12-th batch, evaluate loss: 0.6507742404937744:  57%|██████████▎       | 12/21 [00:00<00:00, 13.04it/s]Epoch: 2, train for the 69-th batch, train loss: 0.6152483820915222:  28%|███▋         | 68/241 [00:09<00:26,  6.41it/s]Epoch: 1, train for the 261-th batch, train loss: 0.33841079473495483:  68%|██████▊   | 261/383 [01:10<00:39,  3.07it/s]evaluate for the 31-th batch, evaluate loss: 0.4504653811454773:  63%|███████████▎      | 29/46 [00:02<00:01, 10.78it/s]evaluate for the 31-th batch, evaluate loss: 0.4504653811454773:  67%|████████████▏     | 31/46 [00:02<00:01, 10.47it/s]Epoch: 2, train for the 69-th batch, train loss: 0.6152483820915222:  29%|███▋         | 69/241 [00:09<00:27,  6.36it/s]evaluate for the 23-th batch, evaluate loss: 0.6682906746864319:  55%|█████████▉        | 22/40 [00:02<00:02,  7.48it/s]evaluate for the 23-th batch, evaluate loss: 0.6682906746864319:  57%|██████████▎       | 23/40 [00:02<00:02,  8.05it/s]Epoch: 3, train for the 77-th batch, train loss: 0.5847030878067017:  52%|██████▊      | 76/146 [00:11<00:10,  6.72it/s]evaluate for the 13-th batch, evaluate loss: 0.6396015882492065:  57%|██████████▎       | 12/21 [00:01<00:00, 13.04it/s]Epoch: 3, train for the 77-th batch, train loss: 0.5847030878067017:  53%|██████▊      | 77/146 [00:11<00:10,  6.71it/s]evaluate for the 32-th batch, evaluate loss: 0.4839799404144287:  67%|████████████▏     | 31/46 [00:02<00:01, 10.47it/s]evaluate for the 14-th batch, evaluate loss: 0.6216006278991699:  57%|██████████▎       | 12/21 [00:01<00:00, 13.04it/s]evaluate for the 14-th batch, evaluate loss: 0.6216006278991699:  67%|████████████      | 14/21 [00:01<00:00, 12.05it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5105582475662231:  29%|███▋         | 69/241 [00:09<00:27,  6.36it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5105582475662231:  29%|███▊         | 70/241 [00:09<00:26,  6.51it/s]evaluate for the 24-th batch, evaluate loss: 0.6954322457313538:  57%|██████████▎       | 23/40 [00:03<00:02,  8.05it/s]evaluate for the 24-th batch, evaluate loss: 0.6954322457313538:  60%|██████████▊       | 24/40 [00:03<00:02,  7.98it/s]evaluate for the 33-th batch, evaluate loss: 0.49549636244773865:  67%|███████████▍     | 31/46 [00:03<00:01, 10.47it/s]evaluate for the 33-th batch, evaluate loss: 0.49549636244773865:  72%|████████████▏    | 33/46 [00:03<00:01, 10.67it/s]Epoch: 3, train for the 78-th batch, train loss: 0.5279092788696289:  53%|██████▊      | 77/146 [00:11<00:10,  6.71it/s]evaluate for the 15-th batch, evaluate loss: 0.6725153923034668:  67%|████████████      | 14/21 [00:01<00:00, 12.05it/s]Epoch: 3, train for the 78-th batch, train loss: 0.5279092788696289:  53%|██████▉      | 78/146 [00:11<00:10,  6.50it/s]Epoch: 1, train for the 262-th batch, train loss: 0.3621711730957031:  68%|███████▍   | 261/383 [01:10<00:39,  3.07it/s]evaluate for the 25-th batch, evaluate loss: 0.6894747614860535:  60%|██████████▊       | 24/40 [00:03<00:02,  7.98it/s]evaluate for the 25-th batch, evaluate loss: 0.6894747614860535:  62%|███████████▎      | 25/40 [00:03<00:01,  8.17it/s]evaluate for the 16-th batch, evaluate loss: 0.6470302939414978:  67%|████████████      | 14/21 [00:01<00:00, 12.05it/s]evaluate for the 16-th batch, evaluate loss: 0.6470302939414978:  76%|█████████████▋    | 16/21 [00:01<00:00, 12.11it/s]Epoch: 2, train for the 71-th batch, train loss: 0.6041399240493774:  29%|███▊         | 70/241 [00:09<00:26,  6.51it/s]evaluate for the 34-th batch, evaluate loss: 0.45625045895576477:  72%|████████████▏    | 33/46 [00:03<00:01, 10.67it/s]Epoch: 1, train for the 262-th batch, train loss: 0.3621711730957031:  68%|███████▌   | 262/383 [01:10<00:38,  3.12it/s]Epoch: 2, train for the 71-th batch, train loss: 0.6041399240493774:  29%|███▊         | 71/241 [00:09<00:26,  6.41it/s]Epoch: 3, train for the 79-th batch, train loss: 0.5628228783607483:  53%|██████▉      | 78/146 [00:11<00:10,  6.50it/s]evaluate for the 17-th batch, evaluate loss: 0.5822120308876038:  76%|█████████████▋    | 16/21 [00:01<00:00, 12.11it/s]Epoch: 3, train for the 79-th batch, train loss: 0.5628228783607483:  54%|███████      | 79/146 [00:11<00:10,  6.38it/s]evaluate for the 35-th batch, evaluate loss: 0.5149385929107666:  72%|████████████▉     | 33/46 [00:03<00:01, 10.67it/s]evaluate for the 35-th batch, evaluate loss: 0.5149385929107666:  76%|█████████████▋    | 35/46 [00:03<00:01, 10.14it/s]evaluate for the 26-th batch, evaluate loss: 0.7010865807533264:  62%|███████████▎      | 25/40 [00:03<00:01,  8.17it/s]evaluate for the 26-th batch, evaluate loss: 0.7010865807533264:  65%|███████████▋      | 26/40 [00:03<00:01,  7.95it/s]evaluate for the 18-th batch, evaluate loss: 0.6524078249931335:  76%|█████████████▋    | 16/21 [00:01<00:00, 12.11it/s]evaluate for the 18-th batch, evaluate loss: 0.6524078249931335:  86%|███████████████▍  | 18/21 [00:01<00:00, 12.77it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5591802000999451:  29%|███▊         | 71/241 [00:09<00:26,  6.41it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5591802000999451:  30%|███▉         | 72/241 [00:09<00:26,  6.45it/s]evaluate for the 36-th batch, evaluate loss: 0.48127955198287964:  76%|████████████▉    | 35/46 [00:03<00:01, 10.14it/s]Epoch: 3, train for the 80-th batch, train loss: 0.5444847941398621:  54%|███████      | 79/146 [00:11<00:10,  6.38it/s]Epoch: 3, train for the 80-th batch, train loss: 0.5444847941398621:  55%|███████      | 80/146 [00:11<00:09,  6.61it/s]evaluate for the 19-th batch, evaluate loss: 0.6464575529098511:  86%|███████████████▍  | 18/21 [00:01<00:00, 12.77it/s]evaluate for the 27-th batch, evaluate loss: 0.6575634479522705:  65%|███████████▋      | 26/40 [00:03<00:01,  7.95it/s]evaluate for the 27-th batch, evaluate loss: 0.6575634479522705:  68%|████████████▏     | 27/40 [00:03<00:01,  7.32it/s]evaluate for the 20-th batch, evaluate loss: 0.6380402445793152:  86%|███████████████▍  | 18/21 [00:01<00:00, 12.77it/s]evaluate for the 20-th batch, evaluate loss: 0.6380402445793152:  95%|█████████████████▏| 20/21 [00:01<00:00, 12.53it/s]Epoch: 1, train for the 263-th batch, train loss: 0.4162989854812622:  68%|███████▌   | 262/383 [01:11<00:38,  3.12it/s]Epoch: 2, train for the 73-th batch, train loss: 0.49075889587402344:  30%|███▌        | 72/241 [00:10<00:26,  6.45it/s]evaluate for the 37-th batch, evaluate loss: 0.5175586342811584:  76%|█████████████▋    | 35/46 [00:03<00:01, 10.14it/s]evaluate for the 37-th batch, evaluate loss: 0.5175586342811584:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.74it/s]Epoch: 2, train for the 73-th batch, train loss: 0.49075889587402344:  30%|███▋        | 73/241 [00:10<00:26,  6.24it/s]evaluate for the 21-th batch, evaluate loss: 0.5951682925224304:  95%|█████████████████▏| 20/21 [00:01<00:00, 12.53it/s]evaluate for the 21-th batch, evaluate loss: 0.5951682925224304: 100%|██████████████████| 21/21 [00:01<00:00, 12.96it/s]
Epoch: 1, train for the 263-th batch, train loss: 0.4162989854812622:  69%|███████▌   | 263/383 [01:11<00:39,  3.01it/s]evaluate for the 28-th batch, evaluate loss: 0.7186083793640137:  68%|████████████▏     | 27/40 [00:03<00:01,  7.32it/s]evaluate for the 28-th batch, evaluate loss: 0.7186083793640137:  70%|████████████▌     | 28/40 [00:03<00:01,  7.89it/s]Epoch: 3, train for the 81-th batch, train loss: 0.5092657804489136:  55%|███████      | 80/146 [00:11<00:09,  6.61it/s]Epoch: 3, train for the 81-th batch, train loss: 0.5092657804489136:  55%|███████▏     | 81/146 [00:11<00:10,  6.44it/s]evaluate for the 38-th batch, evaluate loss: 0.5049522519111633:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.74it/s]Epoch: 2, train for the 74-th batch, train loss: 0.4337140917778015:  30%|███▉         | 73/241 [00:10<00:26,  6.24it/s]Epoch: 2, train for the 74-th batch, train loss: 0.4337140917778015:  31%|███▉         | 74/241 [00:10<00:24,  6.68it/s]Epoch: 3, train for the 82-th batch, train loss: 0.5270357131958008:  55%|███████▏     | 81/146 [00:11<00:10,  6.44it/s]Epoch: 3, train for the 82-th batch, train loss: 0.5270357131958008:  56%|███████▎     | 82/146 [00:11<00:09,  6.93it/s]evaluate for the 29-th batch, evaluate loss: 0.7311914563179016:  70%|████████████▌     | 28/40 [00:03<00:01,  7.89it/s]evaluate for the 29-th batch, evaluate loss: 0.7311914563179016:  72%|█████████████     | 29/40 [00:03<00:01,  7.52it/s]evaluate for the 39-th batch, evaluate loss: 0.4891488254070282:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.74it/s]evaluate for the 39-th batch, evaluate loss: 0.4891488254070282:  85%|███████████████▎  | 39/46 [00:03<00:00,  9.79it/s]Epoch: 2, train for the 75-th batch, train loss: 0.3863186538219452:  31%|███▉         | 74/241 [00:10<00:24,  6.68it/s]Epoch: 1, train for the 264-th batch, train loss: 0.32858288288116455:  69%|██████▊   | 263/383 [01:11<00:39,  3.01it/s]evaluate for the 40-th batch, evaluate loss: 0.47917136549949646:  85%|██████████████▍  | 39/46 [00:03<00:00,  9.79it/s]Epoch: 2, train for the 75-th batch, train loss: 0.3863186538219452:  31%|████         | 75/241 [00:10<00:23,  6.96it/s]Epoch: 3, train for the 83-th batch, train loss: 0.5217382907867432:  56%|███████▎     | 82/146 [00:11<00:09,  6.93it/s]evaluate for the 30-th batch, evaluate loss: 0.7357420325279236:  72%|█████████████     | 29/40 [00:03<00:01,  7.52it/s]evaluate for the 30-th batch, evaluate loss: 0.7357420325279236:  75%|█████████████▌    | 30/40 [00:03<00:01,  7.84it/s]INFO:root:Epoch: 3, learning rate: 0.0001, train loss: 0.4936
INFO:root:train average_precision, 0.8520
INFO:root:train roc_auc, 0.8415
INFO:root:validate loss: 0.4912
INFO:root:validate average_precision, 0.8645
INFO:root:validate roc_auc, 0.8597
INFO:root:new node validate loss: 0.6438
INFO:root:new node validate first_1_average_precision, 0.6990
INFO:root:new node validate first_1_roc_auc, 0.6784
INFO:root:new node validate first_3_average_precision, 0.7293
INFO:root:new node validate first_3_roc_auc, 0.7180
INFO:root:new node validate first_10_average_precision, 0.7319
INFO:root:new node validate first_10_roc_auc, 0.7301
INFO:root:new node validate average_precision, 0.7234
INFO:root:new node validate roc_auc, 0.7367
INFO:root:save model ./saved_models/TGN/ia-reality-call/TGN_seed0_dummy-e41ajx4p/TGN_seed0_dummy-e41ajx4p.pkl
Epoch: 3, train for the 83-th batch, train loss: 0.5217382907867432:  57%|███████▍     | 83/146 [00:11<00:08,  7.09it/s]Epoch: 1, train for the 264-th batch, train loss: 0.32858288288116455:  69%|██████▉   | 264/383 [01:11<00:37,  3.18it/s]evaluate for the 41-th batch, evaluate loss: 0.4821186363697052:  85%|███████████████▎  | 39/46 [00:03<00:00,  9.79it/s]evaluate for the 41-th batch, evaluate loss: 0.4821186363697052:  89%|████████████████  | 41/46 [00:03<00:00, 10.79it/s]Epoch: 2, train for the 76-th batch, train loss: 0.30086272954940796:  31%|███▋        | 75/241 [00:10<00:23,  6.96it/s]Epoch: 2, train for the 76-th batch, train loss: 0.30086272954940796:  32%|███▊        | 76/241 [00:10<00:22,  7.20it/s]Epoch: 3, train for the 84-th batch, train loss: 0.4687947630882263:  57%|███████▍     | 83/146 [00:12<00:08,  7.09it/s]evaluate for the 42-th batch, evaluate loss: 0.46462148427963257:  89%|███████████████▏ | 41/46 [00:03<00:00, 10.79it/s]Epoch: 3, train for the 84-th batch, train loss: 0.4687947630882263:  58%|███████▍     | 84/146 [00:12<00:08,  7.14it/s]evaluate for the 31-th batch, evaluate loss: 0.6707990765571594:  75%|█████████████▌    | 30/40 [00:03<00:01,  7.84it/s]evaluate for the 31-th batch, evaluate loss: 0.6707990765571594:  78%|█████████████▉    | 31/40 [00:03<00:01,  7.54it/s]Epoch: 3, train for the 85-th batch, train loss: 0.5183924436569214:  58%|███████▍     | 84/146 [00:12<00:08,  7.14it/s]Epoch: 3, train for the 85-th batch, train loss: 0.5183924436569214:  58%|███████▌     | 85/146 [00:12<00:07,  7.79it/s]Epoch: 2, train for the 77-th batch, train loss: 0.3613721430301666:  32%|████         | 76/241 [00:10<00:22,  7.20it/s]evaluate for the 43-th batch, evaluate loss: 0.5521680116653442:  89%|████████████████  | 41/46 [00:04<00:00, 10.79it/s]evaluate for the 43-th batch, evaluate loss: 0.5521680116653442:  93%|████████████████▊ | 43/46 [00:04<00:00, 10.18it/s]Epoch: 1, train for the 265-th batch, train loss: 0.35486018657684326:  69%|██████▉   | 264/383 [01:11<00:37,  3.18it/s]evaluate for the 32-th batch, evaluate loss: 0.7015894055366516:  78%|█████████████▉    | 31/40 [00:04<00:01,  7.54it/s]evaluate for the 32-th batch, evaluate loss: 0.7015894055366516:  80%|██████████████▍   | 32/40 [00:04<00:01,  7.64it/s]Epoch: 2, train for the 77-th batch, train loss: 0.3613721430301666:  32%|████▏        | 77/241 [00:10<00:25,  6.54it/s]Epoch: 1, train for the 265-th batch, train loss: 0.35486018657684326:  69%|██████▉   | 265/383 [01:11<00:36,  3.20it/s]evaluate for the 44-th batch, evaluate loss: 0.4992782771587372:  93%|████████████████▊ | 43/46 [00:04<00:00, 10.18it/s]Epoch: 3, train for the 86-th batch, train loss: 0.5039905905723572:  58%|███████▌     | 85/146 [00:12<00:07,  7.79it/s]Epoch: 3, train for the 86-th batch, train loss: 0.5039905905723572:  59%|███████▋     | 86/146 [00:12<00:07,  8.21it/s]evaluate for the 33-th batch, evaluate loss: 0.7097773551940918:  80%|██████████████▍   | 32/40 [00:04<00:01,  7.64it/s]evaluate for the 33-th batch, evaluate loss: 0.7097773551940918:  82%|██████████████▊   | 33/40 [00:04<00:00,  7.76it/s]Epoch: 2, train for the 78-th batch, train loss: 0.5436450242996216:  32%|████▏        | 77/241 [00:10<00:25,  6.54it/s]evaluate for the 45-th batch, evaluate loss: 0.4601041376590729:  93%|████████████████▊ | 43/46 [00:04<00:00, 10.18it/s]evaluate for the 45-th batch, evaluate loss: 0.4601041376590729:  98%|█████████████████▌| 45/46 [00:04<00:00, 10.92it/s]Epoch: 2, train for the 78-th batch, train loss: 0.5436450242996216:  32%|████▏        | 78/241 [00:10<00:24,  6.73it/s]Epoch: 3, train for the 87-th batch, train loss: 0.5173691511154175:  59%|███████▋     | 86/146 [00:12<00:07,  8.21it/s]Epoch: 3, train for the 87-th batch, train loss: 0.5173691511154175:  60%|███████▋     | 87/146 [00:12<00:07,  8.05it/s]evaluate for the 46-th batch, evaluate loss: 0.45824453234672546:  98%|████████████████▋| 45/46 [00:04<00:00, 10.92it/s]evaluate for the 46-th batch, evaluate loss: 0.45824453234672546: 100%|█████████████████| 46/46 [00:04<00:00, 10.65it/s]
evaluate for the 34-th batch, evaluate loss: 0.7032666802406311:  82%|██████████████▊   | 33/40 [00:04<00:00,  7.76it/s]evaluate for the 34-th batch, evaluate loss: 0.7032666802406311:  85%|███████████████▎  | 34/40 [00:04<00:00,  7.77it/s]Epoch: 2, train for the 79-th batch, train loss: 0.4575292468070984:  32%|████▏        | 78/241 [00:10<00:24,  6.73it/s]Epoch: 3, train for the 88-th batch, train loss: 0.5486451387405396:  60%|███████▋     | 87/146 [00:12<00:07,  8.05it/s]Epoch: 2, train for the 79-th batch, train loss: 0.4575292468070984:  33%|████▎        | 79/241 [00:10<00:24,  6.64it/s]Epoch: 1, train for the 266-th batch, train loss: 0.3981578052043915:  69%|███████▌   | 265/383 [01:11<00:36,  3.20it/s]evaluate for the 35-th batch, evaluate loss: 0.7289982438087463:  85%|███████████████▎  | 34/40 [00:04<00:00,  7.77it/s]evaluate for the 35-th batch, evaluate loss: 0.7289982438087463:  88%|███████████████▊  | 35/40 [00:04<00:00,  8.30it/s]Epoch: 1, train for the 266-th batch, train loss: 0.3981578052043915:  69%|███████▋   | 266/383 [01:12<00:36,  3.19it/s]Epoch: 2, train for the 80-th batch, train loss: 0.3383462727069855:  33%|████▎        | 79/241 [00:11<00:24,  6.64it/s]Epoch: 3, train for the 89-th batch, train loss: 0.5082913041114807:  60%|███████▋     | 87/146 [00:12<00:07,  8.05it/s]Epoch: 2, train for the 80-th batch, train loss: 0.3383462727069855:  33%|████▎        | 80/241 [00:11<00:22,  7.19it/s]Epoch: 3, train for the 89-th batch, train loss: 0.5082913041114807:  61%|███████▉     | 89/146 [00:12<00:06,  8.76it/s]evaluate for the 36-th batch, evaluate loss: 0.754380464553833:  88%|████████████████▋  | 35/40 [00:04<00:00,  8.30it/s]evaluate for the 36-th batch, evaluate loss: 0.754380464553833:  90%|█████████████████  | 36/40 [00:04<00:00,  8.33it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]Epoch: 3, train for the 90-th batch, train loss: 0.5239881873130798:  61%|███████▉     | 89/146 [00:12<00:06,  8.76it/s]Epoch: 3, train for the 90-th batch, train loss: 0.5239881873130798:  62%|████████     | 90/146 [00:12<00:06,  8.76it/s]Epoch: 2, train for the 81-th batch, train loss: 0.3783896565437317:  33%|████▎        | 80/241 [00:11<00:22,  7.19it/s]Epoch: 2, train for the 81-th batch, train loss: 0.3783896565437317:  34%|████▎        | 81/241 [00:11<00:22,  7.07it/s]Epoch: 1, train for the 267-th batch, train loss: 0.33084937930107117:  69%|██████▉   | 266/383 [01:12<00:36,  3.19it/s]evaluate for the 37-th batch, evaluate loss: 0.7367238998413086:  90%|████████████████▏ | 36/40 [00:04<00:00,  8.33it/s]evaluate for the 37-th batch, evaluate loss: 0.7367238998413086:  92%|████████████████▋ | 37/40 [00:04<00:00,  8.31it/s]Epoch: 3, train for the 91-th batch, train loss: 0.5271058678627014:  62%|████████     | 90/146 [00:12<00:06,  8.76it/s]Epoch: 3, train for the 91-th batch, train loss: 0.5271058678627014:  62%|████████     | 91/146 [00:12<00:06,  8.94it/s]Epoch: 1, train for the 267-th batch, train loss: 0.33084937930107117:  70%|██████▉   | 267/383 [01:12<00:35,  3.30it/s]  0%|                                                                                            | 0/25 [00:00<?, ?it/s]Epoch: 4, train for the 1-th batch, train loss: 0.6429162621498108:   0%|                       | 0/119 [00:00<?, ?it/s]Epoch: 4, train for the 1-th batch, train loss: 0.6429162621498108:   1%|▏              | 1/119 [00:00<00:24,  4.78it/s]Epoch: 2, train for the 82-th batch, train loss: 0.4313181936740875:  34%|████▎        | 81/241 [00:11<00:22,  7.07it/s]evaluate for the 38-th batch, evaluate loss: 0.7166805267333984:  92%|████████████████▋ | 37/40 [00:04<00:00,  8.31it/s]evaluate for the 38-th batch, evaluate loss: 0.7166805267333984:  95%|█████████████████ | 38/40 [00:04<00:00,  8.45it/s]Epoch: 2, train for the 82-th batch, train loss: 0.4313181936740875:  34%|████▍        | 82/241 [00:11<00:23,  6.75it/s]evaluate for the 1-th batch, evaluate loss: 0.730764627456665:   0%|                             | 0/25 [00:00<?, ?it/s]Epoch: 3, train for the 92-th batch, train loss: 0.5844324231147766:  62%|████████     | 91/146 [00:12<00:06,  8.94it/s]Epoch: 3, train for the 92-th batch, train loss: 0.5844324231147766:  63%|████████▏    | 92/146 [00:12<00:06,  8.42it/s]Epoch: 4, train for the 2-th batch, train loss: 0.7204592823982239:   1%|▏              | 1/119 [00:00<00:24,  4.78it/s]Epoch: 4, train for the 2-th batch, train loss: 0.7204592823982239:   2%|▎              | 2/119 [00:00<00:19,  6.02it/s]evaluate for the 2-th batch, evaluate loss: 0.7481207251548767:   0%|                            | 0/25 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.7481207251548767:   8%|█▌                  | 2/25 [00:00<00:02, 11.11it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5144837498664856:  34%|████▍        | 82/241 [00:11<00:23,  6.75it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5144837498664856:  34%|████▍        | 83/241 [00:11<00:24,  6.48it/s]evaluate for the 39-th batch, evaluate loss: 0.7312609553337097:  95%|█████████████████ | 38/40 [00:05<00:00,  8.45it/s]evaluate for the 39-th batch, evaluate loss: 0.7312609553337097:  98%|█████████████████▌| 39/40 [00:05<00:00,  7.13it/s]evaluate for the 3-th batch, evaluate loss: 0.7557125687599182:   8%|█▌                  | 2/25 [00:00<00:02, 11.11it/s]Epoch: 3, train for the 93-th batch, train loss: 0.5324280261993408:  63%|████████▏    | 92/146 [00:13<00:06,  8.42it/s]Epoch: 1, train for the 268-th batch, train loss: 0.35304197669029236:  70%|██████▉   | 267/383 [01:12<00:35,  3.30it/s]Epoch: 4, train for the 3-th batch, train loss: 0.754620373249054:   2%|▎               | 2/119 [00:00<00:19,  6.02it/s]Epoch: 4, train for the 3-th batch, train loss: 0.754620373249054:   3%|▍               | 3/119 [00:00<00:17,  6.49it/s]Epoch: 3, train for the 93-th batch, train loss: 0.5324280261993408:  64%|████████▎    | 93/146 [00:13<00:07,  7.08it/s]evaluate for the 4-th batch, evaluate loss: 0.7105231881141663:   8%|█▌                  | 2/25 [00:00<00:02, 11.11it/s]evaluate for the 4-th batch, evaluate loss: 0.7105231881141663:  16%|███▏                | 4/25 [00:00<00:01, 11.98it/s]Epoch: 1, train for the 268-th batch, train loss: 0.35304197669029236:  70%|██████▉   | 268/383 [01:12<00:37,  3.06it/s]evaluate for the 40-th batch, evaluate loss: 0.7529128193855286:  98%|█████████████████▌| 39/40 [00:05<00:00,  7.13it/s]evaluate for the 40-th batch, evaluate loss: 0.7529128193855286: 100%|██████████████████| 40/40 [00:05<00:00,  7.35it/s]evaluate for the 40-th batch, evaluate loss: 0.7529128193855286: 100%|██████████████████| 40/40 [00:05<00:00,  7.80it/s]
evaluate for the 5-th batch, evaluate loss: 0.7364242672920227:  16%|███▏                | 4/25 [00:00<00:01, 11.98it/s]Epoch: 4, train for the 4-th batch, train loss: 0.6890935897827148:   3%|▍              | 3/119 [00:00<00:17,  6.49it/s]Epoch: 4, train for the 4-th batch, train loss: 0.6890935897827148:   3%|▌              | 4/119 [00:00<00:18,  6.38it/s]Epoch: 3, train for the 94-th batch, train loss: 0.5091179013252258:  64%|████████▎    | 93/146 [00:13<00:07,  7.08it/s]Epoch: 3, train for the 94-th batch, train loss: 0.5091179013252258:  64%|████████▎    | 94/146 [00:13<00:07,  6.59it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5947088003158569:  34%|████▍        | 83/241 [00:11<00:24,  6.48it/s]evaluate for the 6-th batch, evaluate loss: 0.736129879951477:  16%|███▎                 | 4/25 [00:00<00:01, 11.98it/s]evaluate for the 6-th batch, evaluate loss: 0.736129879951477:  24%|█████                | 6/25 [00:00<00:01, 11.64it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5947088003158569:  35%|████▌        | 84/241 [00:11<00:31,  5.03it/s]evaluate for the 7-th batch, evaluate loss: 0.751879870891571:  24%|█████                | 6/25 [00:00<00:01, 11.64it/s]Epoch: 4, train for the 5-th batch, train loss: 0.6338078379631042:   3%|▌              | 4/119 [00:00<00:18,  6.38it/s]Epoch: 4, train for the 5-th batch, train loss: 0.6338078379631042:   4%|▋              | 5/119 [00:00<00:17,  6.40it/s]Epoch: 3, train for the 95-th batch, train loss: 0.5441136956214905:  64%|████████▎    | 94/146 [00:13<00:07,  6.59it/s]Epoch: 3, train for the 95-th batch, train loss: 0.5441136956214905:  65%|████████▍    | 95/146 [00:13<00:07,  6.47it/s]Epoch: 1, train for the 269-th batch, train loss: 0.4242227375507355:  70%|███████▋   | 268/383 [01:12<00:37,  3.06it/s]Epoch: 2, train for the 85-th batch, train loss: 0.4261438548564911:  35%|████▌        | 84/241 [00:12<00:31,  5.03it/s]evaluate for the 8-th batch, evaluate loss: 0.7257017493247986:  24%|████▊               | 6/25 [00:00<00:01, 11.64it/s]evaluate for the 8-th batch, evaluate loss: 0.7257017493247986:  32%|██████▍             | 8/25 [00:00<00:01, 12.30it/s]Epoch: 2, train for the 85-th batch, train loss: 0.4261438548564911:  35%|████▌        | 85/241 [00:12<00:28,  5.39it/s]INFO:root:Epoch: 1, learning rate: 0.0001, train loss: 0.5879
INFO:root:train average_precision, 0.7874
INFO:root:train roc_auc, 0.7608
INFO:root:validate loss: 0.5462
INFO:root:validate average_precision, 0.8161
INFO:root:validate roc_auc, 0.8005
INFO:root:new node validate loss: 0.6855
INFO:root:new node validate first_1_average_precision, 0.6116
INFO:root:new node validate first_1_roc_auc, 0.5372
INFO:root:new node validate first_3_average_precision, 0.6351
INFO:root:new node validate first_3_roc_auc, 0.5685
INFO:root:new node validate first_10_average_precision, 0.6683
INFO:root:new node validate first_10_roc_auc, 0.6151
INFO:root:new node validate average_precision, 0.6767
INFO:root:new node validate roc_auc, 0.6308
INFO:root:save model ./saved_models/TGN/ia-digg-reply/TGN_seed0_dummy-bceprsuq/TGN_seed0_dummy-bceprsuq.pkl
Epoch: 1, train for the 269-th batch, train loss: 0.4242227375507355:  70%|███████▋   | 269/383 [01:13<00:38,  2.98it/s]Epoch: 4, train for the 6-th batch, train loss: 0.5304841995239258:   4%|▋              | 5/119 [00:00<00:17,  6.40it/s]evaluate for the 9-th batch, evaluate loss: 0.7021897435188293:  32%|██████▍             | 8/25 [00:00<00:01, 12.30it/s]Epoch: 4, train for the 6-th batch, train loss: 0.5304841995239258:   5%|▊              | 6/119 [00:00<00:16,  6.69it/s]Epoch: 3, train for the 96-th batch, train loss: 0.50775146484375:  65%|█████████▊     | 95/146 [00:13<00:07,  6.47it/s]Epoch: 3, train for the 96-th batch, train loss: 0.50775146484375:  66%|█████████▊     | 96/146 [00:13<00:07,  6.46it/s]Epoch: 2, train for the 86-th batch, train loss: 0.499723345041275:  35%|████▉         | 85/241 [00:12<00:28,  5.39it/s]evaluate for the 10-th batch, evaluate loss: 0.7599585652351379:  32%|██████             | 8/25 [00:00<00:01, 12.30it/s]evaluate for the 10-th batch, evaluate loss: 0.7599585652351379:  40%|███████▏          | 10/25 [00:00<00:01, 12.29it/s]Epoch: 2, train for the 86-th batch, train loss: 0.499723345041275:  36%|████▉         | 86/241 [00:12<00:27,  5.66it/s]Epoch: 4, train for the 7-th batch, train loss: 0.5041926503181458:   5%|▊              | 6/119 [00:01<00:16,  6.69it/s]Epoch: 4, train for the 7-th batch, train loss: 0.5041926503181458:   6%|▉              | 7/119 [00:01<00:16,  6.91it/s]Epoch: 1, train for the 270-th batch, train loss: 0.4111390709877014:  70%|███████▋   | 269/383 [01:13<00:38,  2.98it/s]evaluate for the 11-th batch, evaluate loss: 0.741755485534668:  40%|███████▌           | 10/25 [00:00<00:01, 12.29it/s]Epoch: 2, train for the 87-th batch, train loss: 0.5651711225509644:  36%|████▋        | 86/241 [00:12<00:27,  5.66it/s]Epoch: 3, train for the 97-th batch, train loss: 0.5158457159996033:  66%|████████▌    | 96/146 [00:13<00:07,  6.46it/s]Epoch: 1, train for the 270-th batch, train loss: 0.4111390709877014:  70%|███████▊   | 270/383 [01:13<00:34,  3.24it/s]Epoch: 2, train for the 87-th batch, train loss: 0.5651711225509644:  36%|████▋        | 87/241 [00:12<00:24,  6.25it/s]Epoch: 3, train for the 97-th batch, train loss: 0.5158457159996033:  66%|████████▋    | 97/146 [00:13<00:07,  6.30it/s]Epoch: 4, train for the 8-th batch, train loss: 0.4421442449092865:   6%|▉              | 7/119 [00:01<00:16,  6.91it/s]evaluate for the 12-th batch, evaluate loss: 0.6530053615570068:  40%|███████▏          | 10/25 [00:00<00:01, 12.29it/s]evaluate for the 12-th batch, evaluate loss: 0.6530053615570068:  48%|████████▋         | 12/25 [00:00<00:01, 12.18it/s]Epoch: 4, train for the 8-th batch, train loss: 0.4421442449092865:   7%|█              | 8/119 [00:01<00:15,  7.36it/s]Epoch: 2, train for the 88-th batch, train loss: 0.48815304040908813:  36%|████▎       | 87/241 [00:12<00:24,  6.25it/s]evaluate for the 13-th batch, evaluate loss: 0.6679919958114624:  48%|████████▋         | 12/25 [00:01<00:01, 12.18it/s]Epoch: 2, train for the 88-th batch, train loss: 0.48815304040908813:  37%|████▍       | 88/241 [00:12<00:23,  6.41it/s]Epoch: 3, train for the 98-th batch, train loss: 0.5030288696289062:  66%|████████▋    | 97/146 [00:14<00:07,  6.30it/s]Epoch: 3, train for the 98-th batch, train loss: 0.5030288696289062:  67%|████████▋    | 98/146 [00:14<00:07,  6.22it/s]Epoch: 4, train for the 9-th batch, train loss: 0.44713881611824036:   7%|▉             | 8/119 [00:01<00:15,  7.36it/s]Epoch: 4, train for the 9-th batch, train loss: 0.44713881611824036:   8%|█             | 9/119 [00:01<00:15,  7.02it/s]Epoch: 1, train for the 271-th batch, train loss: 0.36010831594467163:  70%|███████   | 270/383 [01:13<00:34,  3.24it/s]evaluate for the 14-th batch, evaluate loss: 0.7068435549736023:  48%|████████▋         | 12/25 [00:01<00:01, 12.18it/s]evaluate for the 14-th batch, evaluate loss: 0.7068435549736023:  56%|██████████        | 14/25 [00:01<00:00, 11.89it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5403512120246887:  37%|████▋        | 88/241 [00:12<00:23,  6.41it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5403512120246887:  37%|████▊        | 89/241 [00:12<00:22,  6.73it/s]Epoch: 1, train for the 271-th batch, train loss: 0.36010831594467163:  71%|███████   | 271/383 [01:13<00:33,  3.32it/s]evaluate for the 15-th batch, evaluate loss: 0.7424377799034119:  56%|██████████        | 14/25 [00:01<00:00, 11.89it/s]Epoch: 3, train for the 99-th batch, train loss: 0.5448336005210876:  67%|████████▋    | 98/146 [00:14<00:07,  6.22it/s]Epoch: 3, train for the 99-th batch, train loss: 0.5448336005210876:  68%|████████▊    | 99/146 [00:14<00:07,  6.31it/s]Epoch: 4, train for the 10-th batch, train loss: 0.4675839841365814:   8%|█             | 9/119 [00:01<00:15,  7.02it/s]Epoch: 4, train for the 10-th batch, train loss: 0.4675839841365814:   8%|█            | 10/119 [00:01<00:15,  7.10it/s]evaluate for the 16-th batch, evaluate loss: 0.6656367182731628:  56%|██████████        | 14/25 [00:01<00:00, 11.89it/s]evaluate for the 16-th batch, evaluate loss: 0.6656367182731628:  64%|███████████▌      | 16/25 [00:01<00:00, 12.72it/s]Epoch: 2, train for the 90-th batch, train loss: 0.4959860146045685:  37%|████▊        | 89/241 [00:12<00:22,  6.73it/s]Epoch: 2, train for the 90-th batch, train loss: 0.4959860146045685:  37%|████▊        | 90/241 [00:12<00:21,  6.92it/s]evaluate for the 17-th batch, evaluate loss: 0.6570833325386047:  64%|███████████▌      | 16/25 [00:01<00:00, 12.72it/s]Epoch: 1, train for the 272-th batch, train loss: 0.32466864585876465:  71%|███████   | 271/383 [01:13<00:33,  3.32it/s]Epoch: 4, train for the 11-th batch, train loss: 0.4667794406414032:   8%|█            | 10/119 [00:01<00:15,  7.10it/s]Epoch: 3, train for the 100-th batch, train loss: 0.5980488657951355:  68%|████████▏   | 99/146 [00:14<00:07,  6.31it/s]Epoch: 1, train for the 272-th batch, train loss: 0.32466864585876465:  71%|███████   | 272/383 [01:13<00:31,  3.54it/s]Epoch: 4, train for the 11-th batch, train loss: 0.4667794406414032:   9%|█▏           | 11/119 [00:01<00:16,  6.45it/s]Epoch: 3, train for the 100-th batch, train loss: 0.5980488657951355:  68%|███████▌   | 100/146 [00:14<00:07,  5.79it/s]evaluate for the 18-th batch, evaluate loss: 0.6338362693786621:  64%|███████████▌      | 16/25 [00:01<00:00, 12.72it/s]evaluate for the 18-th batch, evaluate loss: 0.6338362693786621:  72%|████████████▉     | 18/25 [00:01<00:00, 12.12it/s]Epoch: 2, train for the 91-th batch, train loss: 0.4877093732357025:  37%|████▊        | 90/241 [00:12<00:21,  6.92it/s]Epoch: 2, train for the 91-th batch, train loss: 0.4877093732357025:  38%|████▉        | 91/241 [00:12<00:22,  6.76it/s]Epoch: 4, train for the 12-th batch, train loss: 0.4449059069156647:   9%|█▏           | 11/119 [00:01<00:16,  6.45it/s]evaluate for the 19-th batch, evaluate loss: 0.6188982129096985:  72%|████████████▉     | 18/25 [00:01<00:00, 12.12it/s]Epoch: 2, train for the 92-th batch, train loss: 0.40552741289138794:  38%|████▌       | 91/241 [00:12<00:22,  6.76it/s]Epoch: 3, train for the 101-th batch, train loss: 0.5345770716667175:  68%|███████▌   | 100/146 [00:14<00:07,  5.79it/s]Epoch: 2, train for the 92-th batch, train loss: 0.40552741289138794:  38%|████▌       | 92/241 [00:13<00:21,  6.99it/s]Epoch: 1, train for the 273-th batch, train loss: 0.42240074276924133:  71%|███████   | 272/383 [01:14<00:31,  3.54it/s]Epoch: 3, train for the 101-th batch, train loss: 0.5345770716667175:  69%|███████▌   | 101/146 [00:14<00:07,  5.78it/s]Epoch: 4, train for the 13-th batch, train loss: 0.47263234853744507:   9%|█           | 11/119 [00:01<00:16,  6.45it/s]Epoch: 4, train for the 13-th batch, train loss: 0.47263234853744507:  11%|█▎          | 13/119 [00:01<00:13,  7.78it/s]evaluate for the 20-th batch, evaluate loss: 0.6741529107093811:  72%|████████████▉     | 18/25 [00:01<00:00, 12.12it/s]evaluate for the 20-th batch, evaluate loss: 0.6741529107093811:  80%|██████████████▍   | 20/25 [00:01<00:00, 11.36it/s]Epoch: 1, train for the 273-th batch, train loss: 0.42240074276924133:  71%|███████▏  | 273/383 [01:14<00:29,  3.70it/s]evaluate for the 21-th batch, evaluate loss: 0.6863308548927307:  80%|██████████████▍   | 20/25 [00:01<00:00, 11.36it/s]Epoch: 2, train for the 93-th batch, train loss: 0.43453001976013184:  38%|████▌       | 92/241 [00:13<00:21,  6.99it/s]Epoch: 3, train for the 102-th batch, train loss: 0.5335643887519836:  69%|███████▌   | 101/146 [00:14<00:07,  5.78it/s]Epoch: 2, train for the 93-th batch, train loss: 0.43453001976013184:  39%|████▋       | 93/241 [00:13<00:22,  6.68it/s]Epoch: 4, train for the 14-th batch, train loss: 0.48227161169052124:  11%|█▎          | 13/119 [00:02<00:13,  7.78it/s]Epoch: 3, train for the 102-th batch, train loss: 0.5335643887519836:  70%|███████▋   | 102/146 [00:14<00:07,  5.84it/s]Epoch: 4, train for the 14-th batch, train loss: 0.48227161169052124:  12%|█▍          | 14/119 [00:02<00:14,  7.43it/s]evaluate for the 22-th batch, evaluate loss: 0.6394984126091003:  80%|██████████████▍   | 20/25 [00:01<00:00, 11.36it/s]evaluate for the 22-th batch, evaluate loss: 0.6394984126091003:  88%|███████████████▊  | 22/25 [00:01<00:00, 10.85it/s]Epoch: 1, train for the 274-th batch, train loss: 0.30328333377838135:  71%|███████▏  | 273/383 [01:14<00:29,  3.70it/s]Epoch: 2, train for the 94-th batch, train loss: 0.3932878077030182:  39%|█████        | 93/241 [00:13<00:22,  6.68it/s]Epoch: 2, train for the 94-th batch, train loss: 0.3932878077030182:  39%|█████        | 94/241 [00:13<00:22,  6.66it/s]evaluate for the 23-th batch, evaluate loss: 0.647540271282196:  88%|████████████████▋  | 22/25 [00:01<00:00, 10.85it/s]Epoch: 1, train for the 274-th batch, train loss: 0.30328333377838135:  72%|███████▏  | 274/383 [01:14<00:29,  3.74it/s]Epoch: 4, train for the 15-th batch, train loss: 0.5043811202049255:  12%|█▌           | 14/119 [00:02<00:14,  7.43it/s]Epoch: 4, train for the 15-th batch, train loss: 0.5043811202049255:  13%|█▋           | 15/119 [00:02<00:15,  6.79it/s]Epoch: 3, train for the 103-th batch, train loss: 0.4929910898208618:  70%|███████▋   | 102/146 [00:14<00:07,  5.84it/s]Epoch: 3, train for the 103-th batch, train loss: 0.4929910898208618:  71%|███████▊   | 103/146 [00:14<00:07,  5.53it/s]Epoch: 2, train for the 95-th batch, train loss: 0.4415546655654907:  39%|█████        | 94/241 [00:13<00:22,  6.66it/s]evaluate for the 24-th batch, evaluate loss: 0.6682232022285461:  88%|███████████████▊  | 22/25 [00:02<00:00, 10.85it/s]evaluate for the 24-th batch, evaluate loss: 0.6682232022285461:  96%|█████████████████▎| 24/25 [00:02<00:00, 10.13it/s]Epoch: 2, train for the 95-th batch, train loss: 0.4415546655654907:  39%|█████        | 95/241 [00:13<00:22,  6.58it/s]Epoch: 4, train for the 16-th batch, train loss: 0.4869731664657593:  13%|█▋           | 15/119 [00:02<00:15,  6.79it/s]Epoch: 4, train for the 16-th batch, train loss: 0.4869731664657593:  13%|█▋           | 16/119 [00:02<00:15,  6.71it/s]Epoch: 3, train for the 104-th batch, train loss: 0.5407503247261047:  71%|███████▊   | 103/146 [00:15<00:07,  5.53it/s]evaluate for the 25-th batch, evaluate loss: 0.6337143778800964:  96%|█████████████████▎| 24/25 [00:02<00:00, 10.13it/s]evaluate for the 25-th batch, evaluate loss: 0.6337143778800964: 100%|██████████████████| 25/25 [00:02<00:00, 11.43it/s]
Epoch: 3, train for the 104-th batch, train loss: 0.5407503247261047:  71%|███████▊   | 104/146 [00:15<00:07,  5.63it/s]Epoch: 1, train for the 275-th batch, train loss: 0.43764960765838623:  72%|███████▏  | 274/383 [01:14<00:29,  3.74it/s]Epoch: 4, train for the 17-th batch, train loss: 0.48388469219207764:  13%|█▌          | 16/119 [00:02<00:15,  6.71it/s]Epoch: 1, train for the 275-th batch, train loss: 0.43764960765838623:  72%|███████▏  | 275/383 [01:14<00:29,  3.63it/s]Epoch: 4, train for the 17-th batch, train loss: 0.48388469219207764:  14%|█▋          | 17/119 [00:02<00:14,  7.25it/s]Epoch: 2, train for the 96-th batch, train loss: 0.5343490242958069:  39%|█████        | 95/241 [00:13<00:22,  6.58it/s]Epoch: 2, train for the 96-th batch, train loss: 0.5343490242958069:  40%|█████▏       | 96/241 [00:13<00:23,  6.18it/s]Epoch: 3, train for the 105-th batch, train loss: 0.547935426235199:  71%|████████▌   | 104/146 [00:15<00:07,  5.63it/s]Epoch: 3, train for the 105-th batch, train loss: 0.547935426235199:  72%|████████▋   | 105/146 [00:15<00:07,  5.85it/s]Epoch: 4, train for the 18-th batch, train loss: 0.45835447311401367:  14%|█▋          | 17/119 [00:02<00:14,  7.25it/s]Epoch: 4, train for the 18-th batch, train loss: 0.45835447311401367:  15%|█▊          | 18/119 [00:02<00:13,  7.73it/s]Epoch: 2, train for the 97-th batch, train loss: 0.3924658000469208:  40%|█████▏       | 96/241 [00:13<00:23,  6.18it/s]Epoch: 2, train for the 97-th batch, train loss: 0.3924658000469208:  40%|█████▏       | 97/241 [00:13<00:21,  6.69it/s]Epoch: 1, train for the 276-th batch, train loss: 0.6500811576843262:  72%|███████▉   | 275/383 [01:14<00:29,  3.63it/s]Epoch: 3, train for the 106-th batch, train loss: 0.5193089842796326:  72%|███████▉   | 105/146 [00:15<00:07,  5.85it/s]Epoch: 3, train for the 106-th batch, train loss: 0.5193089842796326:  73%|███████▉   | 106/146 [00:15<00:06,  6.07it/s]Epoch: 4, train for the 19-th batch, train loss: 0.4554925858974457:  15%|█▉           | 18/119 [00:02<00:13,  7.73it/s]Epoch: 4, train for the 19-th batch, train loss: 0.4554925858974457:  16%|██           | 19/119 [00:02<00:13,  7.54it/s]Epoch: 1, train for the 276-th batch, train loss: 0.6500811576843262:  72%|███████▉   | 276/383 [01:14<00:29,  3.66it/s]Epoch: 2, train for the 98-th batch, train loss: 0.47432151436805725:  40%|████▊       | 97/241 [00:13<00:21,  6.69it/s]Epoch: 2, train for the 98-th batch, train loss: 0.47432151436805725:  41%|████▉       | 98/241 [00:13<00:20,  6.88it/s]INFO:root:Epoch: 2, learning rate: 0.0001, train loss: 0.5688
INFO:root:train average_precision, 0.7681
INFO:root:train roc_auc, 0.7448
INFO:root:validate loss: 0.5148
INFO:root:validate average_precision, 0.8380
INFO:root:validate roc_auc, 0.8330
INFO:root:new node validate loss: 0.6958
INFO:root:new node validate first_1_average_precision, 0.5545
INFO:root:new node validate first_1_roc_auc, 0.5621
INFO:root:new node validate first_3_average_precision, 0.5908
INFO:root:new node validate first_3_roc_auc, 0.5948
INFO:root:new node validate first_10_average_precision, 0.6438
INFO:root:new node validate first_10_roc_auc, 0.6495
INFO:root:new node validate average_precision, 0.6582
INFO:root:new node validate roc_auc, 0.6584
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]Epoch: 3, train for the 107-th batch, train loss: 0.5323843359947205:  73%|███████▉   | 106/146 [00:15<00:06,  6.07it/s]Epoch: 4, train for the 20-th batch, train loss: 0.4168764352798462:  16%|██           | 19/119 [00:02<00:13,  7.54it/s]Epoch: 2, train for the 99-th batch, train loss: 0.29842981696128845:  41%|████▉       | 98/241 [00:14<00:20,  6.88it/s]Epoch: 3, train for the 107-th batch, train loss: 0.5323843359947205:  73%|████████   | 107/146 [00:15<00:06,  6.06it/s]Epoch: 4, train for the 20-th batch, train loss: 0.4168764352798462:  17%|██▏          | 20/119 [00:02<00:13,  7.27it/s]Epoch: 2, train for the 99-th batch, train loss: 0.29842981696128845:  41%|████▉       | 99/241 [00:14<00:19,  7.30it/s]Epoch: 3, train for the 1-th batch, train loss: 0.9261704683303833:   0%|                       | 0/151 [00:00<?, ?it/s]Epoch: 1, train for the 277-th batch, train loss: 0.4422000050544739:  72%|███████▉   | 276/383 [01:15<00:29,  3.66it/s]Epoch: 1, train for the 277-th batch, train loss: 0.4422000050544739:  72%|███████▉   | 277/383 [01:15<00:28,  3.73it/s]Epoch: 3, train for the 108-th batch, train loss: 0.5369417667388916:  73%|████████   | 107/146 [00:15<00:06,  6.06it/s]Epoch: 3, train for the 108-th batch, train loss: 0.5369417667388916:  74%|████████▏  | 108/146 [00:15<00:06,  6.10it/s]Epoch: 4, train for the 21-th batch, train loss: 0.4953645169734955:  17%|██▏          | 20/119 [00:03<00:13,  7.27it/s]Epoch: 4, train for the 21-th batch, train loss: 0.4953645169734955:  18%|██▎          | 21/119 [00:03<00:14,  6.81it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5127045512199402:  41%|████▉       | 99/241 [00:14<00:19,  7.30it/s]Epoch: 3, train for the 2-th batch, train loss: 0.8888307213783264:   0%|                       | 0/151 [00:00<?, ?it/s]Epoch: 3, train for the 2-th batch, train loss: 0.8888307213783264:   1%|▏              | 2/151 [00:00<00:19,  7.70it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5127045512199402:  41%|████▌      | 100/241 [00:14<00:21,  6.52it/s]Epoch: 4, train for the 22-th batch, train loss: 0.5275049209594727:  18%|██▎          | 21/119 [00:03<00:14,  6.81it/s]Epoch: 4, train for the 22-th batch, train loss: 0.5275049209594727:  18%|██▍          | 22/119 [00:03<00:14,  6.49it/s]Epoch: 1, train for the 278-th batch, train loss: 0.36418280005455017:  72%|███████▏  | 277/383 [01:15<00:28,  3.73it/s]Epoch: 2, train for the 101-th batch, train loss: 0.5095757842063904:  41%|████▌      | 100/241 [00:14<00:21,  6.52it/s]Epoch: 3, train for the 109-th batch, train loss: 0.5190967917442322:  74%|████████▏  | 108/146 [00:15<00:06,  6.10it/s]Epoch: 3, train for the 109-th batch, train loss: 0.5190967917442322:  75%|████████▏  | 109/146 [00:15<00:06,  5.72it/s]Epoch: 2, train for the 101-th batch, train loss: 0.5095757842063904:  42%|████▌      | 101/241 [00:14<00:21,  6.46it/s]Epoch: 3, train for the 3-th batch, train loss: 0.8444020748138428:   1%|▏              | 2/151 [00:00<00:19,  7.70it/s]Epoch: 3, train for the 3-th batch, train loss: 0.8444020748138428:   2%|▎              | 3/151 [00:00<00:23,  6.28it/s]Epoch: 1, train for the 278-th batch, train loss: 0.36418280005455017:  73%|███████▎  | 278/383 [01:15<00:28,  3.65it/s]Epoch: 4, train for the 23-th batch, train loss: 0.49633651971817017:  18%|██▏         | 22/119 [00:03<00:14,  6.49it/s]Epoch: 4, train for the 23-th batch, train loss: 0.49633651971817017:  19%|██▎         | 23/119 [00:03<00:14,  6.69it/s]Epoch: 2, train for the 102-th batch, train loss: 0.5363232493400574:  42%|████▌      | 101/241 [00:14<00:21,  6.46it/s]Epoch: 3, train for the 110-th batch, train loss: 0.46243947744369507:  75%|███████▍  | 109/146 [00:16<00:06,  5.72it/s]Epoch: 2, train for the 102-th batch, train loss: 0.5363232493400574:  42%|████▋      | 102/241 [00:14<00:21,  6.51it/s]Epoch: 3, train for the 110-th batch, train loss: 0.46243947744369507:  75%|███████▌  | 110/146 [00:16<00:06,  5.86it/s]Epoch: 3, train for the 4-th batch, train loss: 0.7658795118331909:   2%|▎              | 3/151 [00:00<00:23,  6.28it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/237 [00:00<?, ?it/s]Epoch: 3, train for the 4-th batch, train loss: 0.7658795118331909:   3%|▍              | 4/151 [00:00<00:22,  6.40it/s]Epoch: 4, train for the 24-th batch, train loss: 0.46320781111717224:  19%|██▎         | 23/119 [00:03<00:14,  6.69it/s]Epoch: 4, train for the 24-th batch, train loss: 0.46320781111717224:  20%|██▍         | 24/119 [00:03<00:13,  6.84it/s]Epoch: 1, train for the 279-th batch, train loss: 0.5112327337265015:  73%|███████▉   | 278/383 [01:15<00:28,  3.65it/s]Epoch: 3, train for the 111-th batch, train loss: 0.5171053409576416:  75%|████████▎  | 110/146 [00:16<00:06,  5.86it/s]Epoch: 2, train for the 103-th batch, train loss: 0.5007354617118835:  42%|████▋      | 102/241 [00:14<00:21,  6.51it/s]Epoch: 2, train for the 1-th batch, train loss: 0.6841727495193481:   0%|                       | 0/237 [00:00<?, ?it/s]Epoch: 2, train for the 1-th batch, train loss: 0.6841727495193481:   0%|               | 1/237 [00:00<00:28,  8.16it/s]Epoch: 3, train for the 111-th batch, train loss: 0.5171053409576416:  76%|████████▎  | 111/146 [00:16<00:05,  6.11it/s]Epoch: 2, train for the 103-th batch, train loss: 0.5007354617118835:  43%|████▋      | 103/241 [00:14<00:21,  6.36it/s]Epoch: 3, train for the 5-th batch, train loss: 0.7399771809577942:   3%|▍              | 4/151 [00:00<00:22,  6.40it/s]Epoch: 3, train for the 5-th batch, train loss: 0.7399771809577942:   3%|▍              | 5/151 [00:00<00:22,  6.37it/s]Epoch: 1, train for the 279-th batch, train loss: 0.5112327337265015:  73%|████████   | 279/383 [01:15<00:29,  3.53it/s]Epoch: 4, train for the 25-th batch, train loss: 0.4545454680919647:  20%|██▌          | 24/119 [00:03<00:13,  6.84it/s]Epoch: 4, train for the 25-th batch, train loss: 0.4545454680919647:  21%|██▋          | 25/119 [00:03<00:13,  7.12it/s]Epoch: 2, train for the 2-th batch, train loss: 0.6401811242103577:   0%|               | 1/237 [00:00<00:28,  8.16it/s]Epoch: 3, train for the 112-th batch, train loss: 0.5639323592185974:  76%|████████▎  | 111/146 [00:16<00:05,  6.11it/s]Epoch: 2, train for the 104-th batch, train loss: 0.5227836966514587:  43%|████▋      | 103/241 [00:14<00:21,  6.36it/s]Epoch: 3, train for the 112-th batch, train loss: 0.5639323592185974:  77%|████████▍  | 112/146 [00:16<00:05,  6.22it/s]Epoch: 2, train for the 104-th batch, train loss: 0.5227836966514587:  43%|████▋      | 104/241 [00:14<00:21,  6.49it/s]Epoch: 3, train for the 6-th batch, train loss: 0.7124764919281006:   3%|▍              | 5/151 [00:00<00:22,  6.37it/s]Epoch: 3, train for the 6-th batch, train loss: 0.7124764919281006:   4%|▌              | 6/151 [00:00<00:22,  6.53it/s]Epoch: 4, train for the 26-th batch, train loss: 0.43561574816703796:  21%|██▌         | 25/119 [00:03<00:13,  7.12it/s]Epoch: 4, train for the 26-th batch, train loss: 0.43561574816703796:  22%|██▌         | 26/119 [00:03<00:12,  7.38it/s]Epoch: 2, train for the 3-th batch, train loss: 0.6119080185890198:   0%|               | 1/237 [00:00<00:28,  8.16it/s]Epoch: 2, train for the 3-th batch, train loss: 0.6119080185890198:   1%|▏              | 3/237 [00:00<00:25,  9.05it/s]Epoch: 1, train for the 280-th batch, train loss: 0.3352120816707611:  73%|████████   | 279/383 [01:16<00:29,  3.53it/s]Epoch: 3, train for the 113-th batch, train loss: 0.4779597222805023:  77%|████████▍  | 112/146 [00:16<00:05,  6.22it/s]Epoch: 3, train for the 113-th batch, train loss: 0.4779597222805023:  77%|████████▌  | 113/146 [00:16<00:05,  6.05it/s]Epoch: 3, train for the 7-th batch, train loss: 0.6679197549819946:   4%|▌              | 6/151 [00:01<00:22,  6.53it/s]Epoch: 2, train for the 105-th batch, train loss: 0.44358932971954346:  43%|████▎     | 104/241 [00:15<00:21,  6.49it/s]Epoch: 3, train for the 7-th batch, train loss: 0.6679197549819946:   5%|▋              | 7/151 [00:01<00:22,  6.47it/s]Epoch: 2, train for the 4-th batch, train loss: 0.5518508553504944:   1%|▏              | 3/237 [00:00<00:25,  9.05it/s]Epoch: 4, train for the 27-th batch, train loss: 0.5271111130714417:  22%|██▊          | 26/119 [00:03<00:12,  7.38it/s]Epoch: 2, train for the 105-th batch, train loss: 0.44358932971954346:  44%|████▎     | 105/241 [00:15<00:22,  5.94it/s]Epoch: 2, train for the 4-th batch, train loss: 0.5518508553504944:   2%|▎              | 4/237 [00:00<00:28,  8.06it/s]Epoch: 4, train for the 27-th batch, train loss: 0.5271111130714417:  23%|██▉          | 27/119 [00:03<00:13,  6.91it/s]Epoch: 1, train for the 280-th batch, train loss: 0.3352120816707611:  73%|████████   | 280/383 [01:16<00:31,  3.29it/s]Epoch: 3, train for the 114-th batch, train loss: 0.5222189426422119:  77%|████████▌  | 113/146 [00:16<00:05,  6.05it/s]Epoch: 3, train for the 8-th batch, train loss: 0.6886253952980042:   5%|▋              | 7/151 [00:01<00:22,  6.47it/s]Epoch: 3, train for the 114-th batch, train loss: 0.5222189426422119:  78%|████████▌  | 114/146 [00:16<00:04,  6.50it/s]Epoch: 3, train for the 8-th batch, train loss: 0.6886253952980042:   5%|▊              | 8/151 [00:01<00:20,  6.91it/s]Epoch: 2, train for the 5-th batch, train loss: 0.6180022954940796:   2%|▎              | 4/237 [00:00<00:28,  8.06it/s]Epoch: 2, train for the 5-th batch, train loss: 0.6180022954940796:   2%|▎              | 5/237 [00:00<00:29,  7.85it/s]Epoch: 4, train for the 28-th batch, train loss: 0.5003051161766052:  23%|██▉          | 27/119 [00:04<00:13,  6.91it/s]Epoch: 4, train for the 28-th batch, train loss: 0.5003051161766052:  24%|███          | 28/119 [00:04<00:13,  6.83it/s]Epoch: 2, train for the 106-th batch, train loss: 0.4894562363624573:  44%|████▊      | 105/241 [00:15<00:22,  5.94it/s]Epoch: 2, train for the 106-th batch, train loss: 0.4894562363624573:  44%|████▊      | 106/241 [00:15<00:23,  5.73it/s]Epoch: 3, train for the 115-th batch, train loss: 0.5218079686164856:  78%|████████▌  | 114/146 [00:16<00:04,  6.50it/s]Epoch: 3, train for the 9-th batch, train loss: 0.6818659901618958:   5%|▊              | 8/151 [00:01<00:20,  6.91it/s]Epoch: 3, train for the 9-th batch, train loss: 0.6818659901618958:   6%|▉              | 9/151 [00:01<00:19,  7.13it/s]Epoch: 3, train for the 115-th batch, train loss: 0.5218079686164856:  79%|████████▋  | 115/146 [00:16<00:04,  6.66it/s]Epoch: 4, train for the 29-th batch, train loss: 0.45320990681648254:  24%|██▊         | 28/119 [00:04<00:13,  6.83it/s]Epoch: 4, train for the 29-th batch, train loss: 0.45320990681648254:  24%|██▉         | 29/119 [00:04<00:12,  7.23it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5577095150947571:  44%|████▊      | 106/241 [00:15<00:23,  5.73it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5577095150947571:  44%|████▉      | 107/241 [00:15<00:21,  6.14it/s]Epoch: 3, train for the 10-th batch, train loss: 0.6895861625671387:   6%|▊             | 9/151 [00:01<00:19,  7.13it/s]Epoch: 3, train for the 10-th batch, train loss: 0.6895861625671387:   7%|▊            | 10/151 [00:01<00:18,  7.51it/s]Epoch: 1, train for the 281-th batch, train loss: 0.3917493224143982:  73%|████████   | 280/383 [01:16<00:31,  3.29it/s]Epoch: 2, train for the 6-th batch, train loss: 0.6311212182044983:   2%|▎              | 5/237 [00:00<00:29,  7.85it/s]Epoch: 2, train for the 6-th batch, train loss: 0.6311212182044983:   3%|▍              | 6/237 [00:00<00:36,  6.25it/s]Epoch: 3, train for the 116-th batch, train loss: 0.5216354131698608:  79%|████████▋  | 115/146 [00:16<00:04,  6.66it/s]Epoch: 3, train for the 116-th batch, train loss: 0.5216354131698608:  79%|████████▋  | 116/146 [00:16<00:04,  6.67it/s]Epoch: 4, train for the 30-th batch, train loss: 0.41416046023368835:  24%|██▉         | 29/119 [00:04<00:12,  7.23it/s]Epoch: 4, train for the 30-th batch, train loss: 0.41416046023368835:  25%|███         | 30/119 [00:04<00:12,  7.24it/s]Epoch: 1, train for the 281-th batch, train loss: 0.3917493224143982:  73%|████████   | 281/383 [01:16<00:34,  2.99it/s]Epoch: 2, train for the 7-th batch, train loss: 0.6190931797027588:   3%|▍              | 6/237 [00:00<00:36,  6.25it/s]Epoch: 2, train for the 7-th batch, train loss: 0.6190931797027588:   3%|▍              | 7/237 [00:00<00:32,  7.02it/s]Epoch: 3, train for the 11-th batch, train loss: 0.7053595781326294:   7%|▊            | 10/151 [00:01<00:18,  7.51it/s]Epoch: 3, train for the 11-th batch, train loss: 0.7053595781326294:   7%|▉            | 11/151 [00:01<00:19,  7.28it/s]Epoch: 2, train for the 108-th batch, train loss: 0.4986405074596405:  44%|████▉      | 107/241 [00:15<00:21,  6.14it/s]Epoch: 2, train for the 108-th batch, train loss: 0.4986405074596405:  45%|████▉      | 108/241 [00:15<00:22,  5.80it/s]Epoch: 3, train for the 117-th batch, train loss: 0.5373309254646301:  79%|████████▋  | 116/146 [00:17<00:04,  6.67it/s]Epoch: 3, train for the 117-th batch, train loss: 0.5373309254646301:  80%|████████▊  | 117/146 [00:17<00:04,  6.69it/s]Epoch: 4, train for the 31-th batch, train loss: 0.49748265743255615:  25%|███         | 30/119 [00:04<00:12,  7.24it/s]Epoch: 4, train for the 31-th batch, train loss: 0.49748265743255615:  26%|███▏        | 31/119 [00:04<00:12,  7.12it/s]Epoch: 2, train for the 8-th batch, train loss: 0.5961912274360657:   3%|▍              | 7/237 [00:01<00:32,  7.02it/s]Epoch: 3, train for the 12-th batch, train loss: 0.7145780920982361:   7%|▉            | 11/151 [00:01<00:19,  7.28it/s]Epoch: 3, train for the 12-th batch, train loss: 0.7145780920982361:   8%|█            | 12/151 [00:01<00:17,  7.80it/s]Epoch: 2, train for the 8-th batch, train loss: 0.5961912274360657:   3%|▌              | 8/237 [00:01<00:32,  7.04it/s]Epoch: 2, train for the 109-th batch, train loss: 0.6500328183174133:  45%|████▉      | 108/241 [00:15<00:22,  5.80it/s]Epoch: 3, train for the 118-th batch, train loss: 0.50376957654953:  80%|██████████▍  | 117/146 [00:17<00:04,  6.69it/s]Epoch: 2, train for the 109-th batch, train loss: 0.6500328183174133:  45%|████▉      | 109/241 [00:15<00:22,  5.86it/s]Epoch: 3, train for the 118-th batch, train loss: 0.50376957654953:  81%|██████████▌  | 118/146 [00:17<00:04,  6.67it/s]Epoch: 1, train for the 282-th batch, train loss: 0.4823122024536133:  73%|████████   | 281/383 [01:16<00:34,  2.99it/s]Epoch: 3, train for the 13-th batch, train loss: 0.6969523429870605:   8%|█            | 12/151 [00:01<00:17,  7.80it/s]Epoch: 3, train for the 13-th batch, train loss: 0.6969523429870605:   9%|█            | 13/151 [00:01<00:16,  8.13it/s]Epoch: 4, train for the 32-th batch, train loss: 0.451835960149765:  26%|███▋          | 31/119 [00:04<00:12,  7.12it/s]Epoch: 4, train for the 32-th batch, train loss: 0.451835960149765:  27%|███▊          | 32/119 [00:04<00:12,  6.71it/s]Epoch: 2, train for the 9-th batch, train loss: 0.5883664488792419:   3%|▌              | 8/237 [00:01<00:32,  7.04it/s]Epoch: 2, train for the 9-th batch, train loss: 0.5883664488792419:   4%|▌              | 9/237 [00:01<00:33,  6.88it/s]Epoch: 1, train for the 282-th batch, train loss: 0.4823122024536133:  74%|████████   | 282/383 [01:16<00:33,  3.04it/s]Epoch: 3, train for the 119-th batch, train loss: 0.5105953812599182:  81%|████████▉  | 118/146 [00:17<00:04,  6.67it/s]Epoch: 3, train for the 119-th batch, train loss: 0.5105953812599182:  82%|████████▉  | 119/146 [00:17<00:04,  6.73it/s]Epoch: 2, train for the 110-th batch, train loss: 0.551443338394165:  45%|█████▍      | 109/241 [00:15<00:22,  5.86it/s]Epoch: 4, train for the 33-th batch, train loss: 0.4814692437648773:  27%|███▍         | 32/119 [00:04<00:12,  6.71it/s]Epoch: 3, train for the 14-th batch, train loss: 0.7085193991661072:   9%|█            | 13/151 [00:01<00:16,  8.13it/s]Epoch: 4, train for the 33-th batch, train loss: 0.4814692437648773:  28%|███▌         | 33/119 [00:04<00:12,  6.85it/s]Epoch: 2, train for the 110-th batch, train loss: 0.551443338394165:  46%|█████▍      | 110/241 [00:15<00:23,  5.69it/s]Epoch: 3, train for the 14-th batch, train loss: 0.7085193991661072:   9%|█▏           | 14/151 [00:01<00:18,  7.46it/s]Epoch: 2, train for the 10-th batch, train loss: 0.6234917044639587:   4%|▌             | 9/237 [00:01<00:33,  6.88it/s]Epoch: 2, train for the 10-th batch, train loss: 0.6234917044639587:   4%|▌            | 10/237 [00:01<00:33,  6.78it/s]Epoch: 3, train for the 120-th batch, train loss: 0.4875258803367615:  82%|████████▉  | 119/146 [00:17<00:04,  6.73it/s]Epoch: 3, train for the 120-th batch, train loss: 0.4875258803367615:  82%|█████████  | 120/146 [00:17<00:03,  6.71it/s]Epoch: 1, train for the 283-th batch, train loss: 0.48388031125068665:  74%|███████▎  | 282/383 [01:17<00:33,  3.04it/s]Epoch: 4, train for the 34-th batch, train loss: 0.44182249903678894:  28%|███▎        | 33/119 [00:04<00:12,  6.85it/s]Epoch: 4, train for the 34-th batch, train loss: 0.44182249903678894:  29%|███▍        | 34/119 [00:04<00:12,  6.84it/s]Epoch: 2, train for the 11-th batch, train loss: 0.6464329361915588:   4%|▌            | 10/237 [00:01<00:33,  6.78it/s]Epoch: 2, train for the 111-th batch, train loss: 0.23122569918632507:  46%|████▌     | 110/241 [00:16<00:23,  5.69it/s]Epoch: 2, train for the 11-th batch, train loss: 0.6464329361915588:   5%|▌            | 11/237 [00:01<00:31,  7.08it/s]Epoch: 1, train for the 283-th batch, train loss: 0.48388031125068665:  74%|███████▍  | 283/383 [01:17<00:31,  3.17it/s]Epoch: 3, train for the 15-th batch, train loss: 0.6912907361984253:   9%|█▏           | 14/151 [00:02<00:18,  7.46it/s]Epoch: 2, train for the 111-th batch, train loss: 0.23122569918632507:  46%|████▌     | 111/241 [00:16<00:23,  5.57it/s]Epoch: 3, train for the 15-th batch, train loss: 0.6912907361984253:  10%|█▎           | 15/151 [00:02<00:20,  6.65it/s]Epoch: 3, train for the 121-th batch, train loss: 0.4962882995605469:  82%|█████████  | 120/146 [00:17<00:03,  6.71it/s]Epoch: 3, train for the 121-th batch, train loss: 0.4962882995605469:  83%|█████████  | 121/146 [00:17<00:03,  6.74it/s]Epoch: 4, train for the 35-th batch, train loss: 0.45105278491973877:  29%|███▍        | 34/119 [00:05<00:12,  6.84it/s]Epoch: 4, train for the 35-th batch, train loss: 0.45105278491973877:  29%|███▌        | 35/119 [00:05<00:12,  6.97it/s]Epoch: 2, train for the 12-th batch, train loss: 0.6402501463890076:   5%|▌            | 11/237 [00:01<00:31,  7.08it/s]Epoch: 2, train for the 12-th batch, train loss: 0.6402501463890076:   5%|▋            | 12/237 [00:01<00:31,  7.15it/s]Epoch: 3, train for the 16-th batch, train loss: 0.7051923274993896:  10%|█▎           | 15/151 [00:02<00:20,  6.65it/s]Epoch: 3, train for the 16-th batch, train loss: 0.7051923274993896:  11%|█▍           | 16/151 [00:02<00:20,  6.52it/s]Epoch: 2, train for the 112-th batch, train loss: 0.3209111988544464:  46%|█████      | 111/241 [00:16<00:23,  5.57it/s]Epoch: 2, train for the 112-th batch, train loss: 0.3209111988544464:  46%|█████      | 112/241 [00:16<00:23,  5.54it/s]Epoch: 1, train for the 284-th batch, train loss: 0.5172209739685059:  74%|████████▏  | 283/383 [01:17<00:31,  3.17it/s]Epoch: 3, train for the 122-th batch, train loss: 0.5467263460159302:  83%|█████████  | 121/146 [00:17<00:03,  6.74it/s]Epoch: 4, train for the 36-th batch, train loss: 0.42462339997291565:  29%|███▌        | 35/119 [00:05<00:12,  6.97it/s]Epoch: 2, train for the 13-th batch, train loss: 0.6483268737792969:   5%|▋            | 12/237 [00:01<00:31,  7.15it/s]Epoch: 4, train for the 36-th batch, train loss: 0.42462339997291565:  30%|███▋        | 36/119 [00:05<00:12,  6.70it/s]Epoch: 2, train for the 13-th batch, train loss: 0.6483268737792969:   5%|▋            | 13/237 [00:01<00:31,  7.18it/s]Epoch: 3, train for the 122-th batch, train loss: 0.5467263460159302:  84%|█████████▏ | 122/146 [00:17<00:03,  6.26it/s]Epoch: 1, train for the 284-th batch, train loss: 0.5172209739685059:  74%|████████▏  | 284/383 [01:17<00:30,  3.25it/s]Epoch: 3, train for the 17-th batch, train loss: 0.6933881044387817:  11%|█▍           | 16/151 [00:02<00:20,  6.52it/s]Epoch: 3, train for the 17-th batch, train loss: 0.6933881044387817:  11%|█▍           | 17/151 [00:02<00:19,  6.81it/s]Epoch: 2, train for the 113-th batch, train loss: 0.38682350516319275:  46%|████▋     | 112/241 [00:16<00:23,  5.54it/s]Epoch: 2, train for the 113-th batch, train loss: 0.38682350516319275:  47%|████▋     | 113/241 [00:16<00:21,  5.92it/s]Epoch: 4, train for the 37-th batch, train loss: 0.47747474908828735:  30%|███▋        | 36/119 [00:05<00:12,  6.70it/s]Epoch: 4, train for the 37-th batch, train loss: 0.47747474908828735:  31%|███▋        | 37/119 [00:05<00:11,  6.98it/s]Epoch: 2, train for the 14-th batch, train loss: 0.6730731725692749:   5%|▋            | 13/237 [00:01<00:31,  7.18it/s]Epoch: 2, train for the 14-th batch, train loss: 0.6730731725692749:   6%|▊            | 14/237 [00:01<00:32,  6.94it/s]Epoch: 3, train for the 123-th batch, train loss: 0.535309374332428:  84%|██████████  | 122/146 [00:18<00:03,  6.26it/s]Epoch: 3, train for the 123-th batch, train loss: 0.535309374332428:  84%|██████████  | 123/146 [00:18<00:03,  6.12it/s]Epoch: 3, train for the 18-th batch, train loss: 0.6839314103126526:  11%|█▍           | 17/151 [00:02<00:19,  6.81it/s]Epoch: 3, train for the 18-th batch, train loss: 0.6839314103126526:  12%|█▌           | 18/151 [00:02<00:19,  6.87it/s]Epoch: 2, train for the 114-th batch, train loss: 0.4005940854549408:  47%|█████▏     | 113/241 [00:16<00:21,  5.92it/s]Epoch: 2, train for the 114-th batch, train loss: 0.4005940854549408:  47%|█████▏     | 114/241 [00:16<00:20,  6.14it/s]Epoch: 4, train for the 38-th batch, train loss: 0.43162989616394043:  31%|███▋        | 37/119 [00:05<00:11,  6.98it/s]Epoch: 4, train for the 38-th batch, train loss: 0.43162989616394043:  32%|███▊        | 38/119 [00:05<00:11,  7.05it/s]Epoch: 1, train for the 285-th batch, train loss: 0.43942663073539734:  74%|███████▍  | 284/383 [01:17<00:30,  3.25it/s]Epoch: 2, train for the 15-th batch, train loss: 0.7132546901702881:   6%|▊            | 14/237 [00:02<00:32,  6.94it/s]Epoch: 3, train for the 19-th batch, train loss: 0.6732864379882812:  12%|█▌           | 18/151 [00:02<00:19,  6.87it/s]Epoch: 3, train for the 124-th batch, train loss: 0.5289469361305237:  84%|█████████▎ | 123/146 [00:18<00:03,  6.12it/s]Epoch: 3, train for the 19-th batch, train loss: 0.6732864379882812:  13%|█▋           | 19/151 [00:02<00:19,  6.93it/s]Epoch: 2, train for the 15-th batch, train loss: 0.7132546901702881:   6%|▊            | 15/237 [00:02<00:33,  6.60it/s]Epoch: 3, train for the 124-th batch, train loss: 0.5289469361305237:  85%|█████████▎ | 124/146 [00:18<00:03,  6.21it/s]Epoch: 2, train for the 115-th batch, train loss: 0.42298343777656555:  47%|████▋     | 114/241 [00:16<00:20,  6.14it/s]Epoch: 1, train for the 285-th batch, train loss: 0.43942663073539734:  74%|███████▍  | 285/383 [01:17<00:31,  3.16it/s]Epoch: 2, train for the 115-th batch, train loss: 0.42298343777656555:  48%|████▊     | 115/241 [00:16<00:20,  6.19it/s]Epoch: 4, train for the 39-th batch, train loss: 0.47538992762565613:  32%|███▊        | 38/119 [00:05<00:11,  7.05it/s]Epoch: 4, train for the 39-th batch, train loss: 0.47538992762565613:  33%|███▉        | 39/119 [00:05<00:10,  7.34it/s]Epoch: 3, train for the 20-th batch, train loss: 0.6632960438728333:  13%|█▋           | 19/151 [00:02<00:19,  6.93it/s]Epoch: 2, train for the 16-th batch, train loss: 0.6354606747627258:   6%|▊            | 15/237 [00:02<00:33,  6.60it/s]Epoch: 3, train for the 20-th batch, train loss: 0.6632960438728333:  13%|█▋           | 20/151 [00:02<00:17,  7.28it/s]Epoch: 2, train for the 16-th batch, train loss: 0.6354606747627258:   7%|▉            | 16/237 [00:02<00:31,  6.94it/s]Epoch: 3, train for the 125-th batch, train loss: 0.49835968017578125:  85%|████████▍ | 124/146 [00:18<00:03,  6.21it/s]Epoch: 3, train for the 125-th batch, train loss: 0.49835968017578125:  86%|████████▌ | 125/146 [00:18<00:03,  6.54it/s]Epoch: 4, train for the 40-th batch, train loss: 0.4410010874271393:  33%|████▎        | 39/119 [00:05<00:10,  7.34it/s]Epoch: 4, train for the 40-th batch, train loss: 0.4410010874271393:  34%|████▎        | 40/119 [00:05<00:10,  7.86it/s]Epoch: 2, train for the 116-th batch, train loss: 0.519000768661499:  48%|█████▋      | 115/241 [00:16<00:20,  6.19it/s]Epoch: 2, train for the 116-th batch, train loss: 0.519000768661499:  48%|█████▊      | 116/241 [00:16<00:20,  6.13it/s]Epoch: 3, train for the 21-th batch, train loss: 0.6743670701980591:  13%|█▋           | 20/151 [00:02<00:17,  7.28it/s]Epoch: 3, train for the 21-th batch, train loss: 0.6743670701980591:  14%|█▊           | 21/151 [00:02<00:17,  7.29it/s]Epoch: 3, train for the 126-th batch, train loss: 0.5655356645584106:  86%|█████████▍ | 125/146 [00:18<00:03,  6.54it/s]Epoch: 3, train for the 126-th batch, train loss: 0.5655356645584106:  86%|█████████▍ | 126/146 [00:18<00:03,  6.52it/s]Epoch: 2, train for the 17-th batch, train loss: 0.6183775663375854:   7%|▉            | 16/237 [00:02<00:31,  6.94it/s]Epoch: 2, train for the 17-th batch, train loss: 0.6183775663375854:   7%|▉            | 17/237 [00:02<00:33,  6.48it/s]Epoch: 4, train for the 41-th batch, train loss: 0.4955834448337555:  34%|████▎        | 40/119 [00:05<00:10,  7.86it/s]Epoch: 4, train for the 41-th batch, train loss: 0.4955834448337555:  34%|████▍        | 41/119 [00:05<00:10,  7.34it/s]Epoch: 2, train for the 117-th batch, train loss: 0.6117357015609741:  48%|█████▎     | 116/241 [00:17<00:20,  6.13it/s]Epoch: 2, train for the 117-th batch, train loss: 0.6117357015609741:  49%|█████▎     | 117/241 [00:17<00:19,  6.21it/s]Epoch: 3, train for the 22-th batch, train loss: 0.6553230285644531:  14%|█▊           | 21/151 [00:03<00:17,  7.29it/s]Epoch: 2, train for the 18-th batch, train loss: 0.6362608671188354:   7%|▉            | 17/237 [00:02<00:33,  6.48it/s]Epoch: 3, train for the 22-th batch, train loss: 0.6553230285644531:  15%|█▉           | 22/151 [00:03<00:18,  7.16it/s]Epoch: 2, train for the 18-th batch, train loss: 0.6362608671188354:   8%|▉            | 18/237 [00:02<00:30,  7.20it/s]Epoch: 3, train for the 127-th batch, train loss: 0.519902765750885:  86%|██████████▎ | 126/146 [00:18<00:03,  6.52it/s]Epoch: 3, train for the 127-th batch, train loss: 0.519902765750885:  87%|██████████▍ | 127/146 [00:18<00:02,  6.76it/s]Epoch: 4, train for the 42-th batch, train loss: 0.4820862114429474:  34%|████▍        | 41/119 [00:05<00:10,  7.34it/s]Epoch: 4, train for the 42-th batch, train loss: 0.4820862114429474:  35%|████▌        | 42/119 [00:05<00:10,  7.32it/s]Epoch: 2, train for the 118-th batch, train loss: 0.4906315505504608:  49%|█████▎     | 117/241 [00:17<00:19,  6.21it/s]Epoch: 2, train for the 19-th batch, train loss: 0.6161009073257446:   8%|▉            | 18/237 [00:02<00:30,  7.20it/s]Epoch: 2, train for the 19-th batch, train loss: 0.6161009073257446:   8%|█            | 19/237 [00:02<00:28,  7.70it/s]Epoch: 2, train for the 118-th batch, train loss: 0.4906315505504608:  49%|█████▍     | 118/241 [00:17<00:19,  6.37it/s]Epoch: 3, train for the 23-th batch, train loss: 0.6584157943725586:  15%|█▉           | 22/151 [00:03<00:18,  7.16it/s]Epoch: 3, train for the 23-th batch, train loss: 0.6584157943725586:  15%|█▉           | 23/151 [00:03<00:18,  7.11it/s]Epoch: 3, train for the 128-th batch, train loss: 0.5172062516212463:  87%|█████████▌ | 127/146 [00:18<00:02,  6.76it/s]Epoch: 1, train for the 286-th batch, train loss: 0.38055419921875:  74%|█████████▋   | 285/383 [01:18<00:31,  3.16it/s]Epoch: 3, train for the 128-th batch, train loss: 0.5172062516212463:  88%|█████████▋ | 128/146 [00:18<00:02,  6.56it/s]Epoch: 4, train for the 43-th batch, train loss: 0.49258673191070557:  35%|████▏       | 42/119 [00:06<00:10,  7.32it/s]Epoch: 4, train for the 43-th batch, train loss: 0.49258673191070557:  36%|████▎       | 43/119 [00:06<00:10,  7.20it/s]Epoch: 2, train for the 20-th batch, train loss: 0.6606379747390747:   8%|█            | 19/237 [00:02<00:28,  7.70it/s]Epoch: 2, train for the 20-th batch, train loss: 0.6606379747390747:   8%|█            | 20/237 [00:02<00:29,  7.35it/s]Epoch: 2, train for the 119-th batch, train loss: 0.35419997572898865:  49%|████▉     | 118/241 [00:17<00:19,  6.37it/s]Epoch: 1, train for the 286-th batch, train loss: 0.38055419921875:  75%|█████████▋   | 286/383 [01:18<00:40,  2.41it/s]Epoch: 2, train for the 119-th batch, train loss: 0.35419997572898865:  49%|████▉     | 119/241 [00:17<00:19,  6.28it/s]Epoch: 3, train for the 24-th batch, train loss: 0.6661038994789124:  15%|█▉           | 23/151 [00:03<00:18,  7.11it/s]Epoch: 3, train for the 24-th batch, train loss: 0.6661038994789124:  16%|██           | 24/151 [00:03<00:18,  6.71it/s]Epoch: 3, train for the 129-th batch, train loss: 0.5288314819335938:  88%|█████████▋ | 128/146 [00:18<00:02,  6.56it/s]Epoch: 4, train for the 44-th batch, train loss: 0.43668991327285767:  36%|████▎       | 43/119 [00:06<00:10,  7.20it/s]Epoch: 3, train for the 129-th batch, train loss: 0.5288314819335938:  88%|█████████▋ | 129/146 [00:18<00:02,  6.39it/s]Epoch: 4, train for the 44-th batch, train loss: 0.43668991327285767:  37%|████▍       | 44/119 [00:06<00:10,  7.00it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5913216471672058:  49%|█████▍     | 119/241 [00:17<00:19,  6.28it/s]Epoch: 2, train for the 21-th batch, train loss: 0.6154043674468994:   8%|█            | 20/237 [00:02<00:29,  7.35it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5913216471672058:  50%|█████▍     | 120/241 [00:17<00:19,  6.29it/s]Epoch: 2, train for the 21-th batch, train loss: 0.6154043674468994:   9%|█▏           | 21/237 [00:02<00:32,  6.66it/s]Epoch: 3, train for the 25-th batch, train loss: 0.724923312664032:  16%|██▏           | 24/151 [00:03<00:18,  6.71it/s]Epoch: 3, train for the 25-th batch, train loss: 0.724923312664032:  17%|██▎           | 25/151 [00:03<00:19,  6.55it/s]Epoch: 3, train for the 130-th batch, train loss: 0.45874273777008057:  88%|████████▊ | 129/146 [00:19<00:02,  6.39it/s]Epoch: 4, train for the 45-th batch, train loss: 0.40645095705986023:  37%|████▍       | 44/119 [00:06<00:10,  7.00it/s]Epoch: 3, train for the 130-th batch, train loss: 0.45874273777008057:  89%|████████▉ | 130/146 [00:19<00:02,  6.31it/s]Epoch: 4, train for the 45-th batch, train loss: 0.40645095705986023:  38%|████▌       | 45/119 [00:06<00:10,  6.73it/s]Epoch: 2, train for the 22-th batch, train loss: 0.6097609400749207:   9%|█▏           | 21/237 [00:03<00:32,  6.66it/s]Epoch: 2, train for the 22-th batch, train loss: 0.6097609400749207:   9%|█▏           | 22/237 [00:03<00:30,  6.98it/s]Epoch: 2, train for the 121-th batch, train loss: 0.5466699004173279:  50%|█████▍     | 120/241 [00:17<00:19,  6.29it/s]Epoch: 2, train for the 121-th batch, train loss: 0.5466699004173279:  50%|█████▌     | 121/241 [00:17<00:18,  6.37it/s]Epoch: 3, train for the 26-th batch, train loss: 0.6711363196372986:  17%|██▏          | 25/151 [00:03<00:19,  6.55it/s]Epoch: 3, train for the 26-th batch, train loss: 0.6711363196372986:  17%|██▏          | 26/151 [00:03<00:18,  6.61it/s]Epoch: 1, train for the 287-th batch, train loss: 0.41712504625320435:  75%|███████▍  | 286/383 [01:18<00:40,  2.41it/s]Epoch: 3, train for the 131-th batch, train loss: 0.5448330044746399:  89%|█████████▊ | 130/146 [00:19<00:02,  6.31it/s]Epoch: 4, train for the 46-th batch, train loss: 0.4333510398864746:  38%|████▉        | 45/119 [00:06<00:10,  6.73it/s]Epoch: 3, train for the 131-th batch, train loss: 0.5448330044746399:  90%|█████████▊ | 131/146 [00:19<00:02,  6.37it/s]Epoch: 4, train for the 46-th batch, train loss: 0.4333510398864746:  39%|█████        | 46/119 [00:06<00:10,  6.68it/s]Epoch: 1, train for the 287-th batch, train loss: 0.41712504625320435:  75%|███████▍  | 287/383 [01:18<00:40,  2.38it/s]Epoch: 2, train for the 122-th batch, train loss: 0.4457295536994934:  50%|█████▌     | 121/241 [00:17<00:18,  6.37it/s]Epoch: 2, train for the 122-th batch, train loss: 0.4457295536994934:  51%|█████▌     | 122/241 [00:17<00:18,  6.46it/s]Epoch: 3, train for the 27-th batch, train loss: 0.673242449760437:  17%|██▍           | 26/151 [00:03<00:18,  6.61it/s]Epoch: 3, train for the 27-th batch, train loss: 0.673242449760437:  18%|██▌           | 27/151 [00:03<00:18,  6.61it/s]Epoch: 3, train for the 132-th batch, train loss: 0.49573859572410583:  90%|████████▉ | 131/146 [00:19<00:02,  6.37it/s]Epoch: 4, train for the 47-th batch, train loss: 0.49213477969169617:  39%|████▋       | 46/119 [00:06<00:10,  6.68it/s]Epoch: 3, train for the 132-th batch, train loss: 0.49573859572410583:  90%|█████████ | 132/146 [00:19<00:02,  6.37it/s]Epoch: 4, train for the 47-th batch, train loss: 0.49213477969169617:  39%|████▋       | 47/119 [00:06<00:10,  6.61it/s]Epoch: 2, train for the 23-th batch, train loss: 0.5998514890670776:   9%|█▏           | 22/237 [00:03<00:30,  6.98it/s]Epoch: 2, train for the 23-th batch, train loss: 0.5998514890670776:  10%|█▎           | 23/237 [00:03<00:40,  5.35it/s]Epoch: 2, train for the 123-th batch, train loss: 0.6822340488433838:  51%|█████▌     | 122/241 [00:17<00:18,  6.46it/s]Epoch: 2, train for the 123-th batch, train loss: 0.6822340488433838:  51%|█████▌     | 123/241 [00:17<00:17,  6.66it/s]Epoch: 3, train for the 28-th batch, train loss: 0.678781270980835:  18%|██▌           | 27/151 [00:04<00:18,  6.61it/s]Epoch: 3, train for the 28-th batch, train loss: 0.678781270980835:  19%|██▌           | 28/151 [00:04<00:18,  6.66it/s]Epoch: 2, train for the 24-th batch, train loss: 0.5673229694366455:  10%|█▎           | 23/237 [00:03<00:40,  5.35it/s]Epoch: 2, train for the 24-th batch, train loss: 0.5673229694366455:  10%|█▎           | 24/237 [00:03<00:34,  6.13it/s]Epoch: 4, train for the 48-th batch, train loss: 0.4625406563282013:  39%|█████▏       | 47/119 [00:06<00:10,  6.61it/s]Epoch: 3, train for the 133-th batch, train loss: 0.4761946499347687:  90%|█████████▉ | 132/146 [00:19<00:02,  6.37it/s]Epoch: 4, train for the 48-th batch, train loss: 0.4625406563282013:  40%|█████▏       | 48/119 [00:06<00:11,  6.03it/s]Epoch: 3, train for the 133-th batch, train loss: 0.4761946499347687:  91%|██████████ | 133/146 [00:19<00:02,  5.66it/s]Epoch: 2, train for the 124-th batch, train loss: 0.3843042552471161:  51%|█████▌     | 123/241 [00:18<00:17,  6.66it/s]Epoch: 3, train for the 29-th batch, train loss: 0.6919398307800293:  19%|██▍          | 28/151 [00:04<00:18,  6.66it/s]Epoch: 2, train for the 25-th batch, train loss: 0.580605685710907:  10%|█▍            | 24/237 [00:03<00:34,  6.13it/s]Epoch: 2, train for the 124-th batch, train loss: 0.3843042552471161:  51%|█████▋     | 124/241 [00:18<00:19,  6.07it/s]Epoch: 3, train for the 29-th batch, train loss: 0.6919398307800293:  19%|██▍          | 29/151 [00:04<00:19,  6.40it/s]Epoch: 2, train for the 25-th batch, train loss: 0.580605685710907:  11%|█▍            | 25/237 [00:03<00:31,  6.65it/s]Epoch: 1, train for the 288-th batch, train loss: 0.47171714901924133:  75%|███████▍  | 287/383 [01:19<00:40,  2.38it/s]Epoch: 4, train for the 49-th batch, train loss: 0.45664680004119873:  40%|████▊       | 48/119 [00:07<00:11,  6.03it/s]Epoch: 4, train for the 49-th batch, train loss: 0.45664680004119873:  41%|████▉       | 49/119 [00:07<00:10,  6.55it/s]Epoch: 1, train for the 288-th batch, train loss: 0.47171714901924133:  75%|███████▌  | 288/383 [01:19<00:41,  2.28it/s]Epoch: 3, train for the 134-th batch, train loss: 0.5055732727050781:  91%|██████████ | 133/146 [00:19<00:02,  5.66it/s]Epoch: 2, train for the 26-th batch, train loss: 0.5587019920349121:  11%|█▎           | 25/237 [00:03<00:31,  6.65it/s]Epoch: 3, train for the 134-th batch, train loss: 0.5055732727050781:  92%|██████████ | 134/146 [00:19<00:02,  5.92it/s]Epoch: 2, train for the 26-th batch, train loss: 0.5587019920349121:  11%|█▍           | 26/237 [00:03<00:29,  7.14it/s]Epoch: 2, train for the 125-th batch, train loss: 0.49113568663597107:  51%|█████▏    | 124/241 [00:18<00:19,  6.07it/s]Epoch: 2, train for the 125-th batch, train loss: 0.49113568663597107:  52%|█████▏    | 125/241 [00:18<00:19,  6.06it/s]Epoch: 4, train for the 50-th batch, train loss: 0.4245431423187256:  41%|█████▎       | 49/119 [00:07<00:10,  6.55it/s]Epoch: 3, train for the 30-th batch, train loss: 0.6918909549713135:  19%|██▍          | 29/151 [00:04<00:19,  6.40it/s]Epoch: 4, train for the 50-th batch, train loss: 0.4245431423187256:  42%|█████▍       | 50/119 [00:07<00:09,  7.20it/s]Epoch: 3, train for the 30-th batch, train loss: 0.6918909549713135:  20%|██▌          | 30/151 [00:04<00:19,  6.13it/s]Epoch: 2, train for the 27-th batch, train loss: 0.5826640129089355:  11%|█▍           | 26/237 [00:03<00:29,  7.14it/s]Epoch: 4, train for the 51-th batch, train loss: 0.4878077208995819:  42%|█████▍       | 50/119 [00:07<00:09,  7.20it/s]Epoch: 2, train for the 126-th batch, train loss: 0.4334717392921448:  52%|█████▋     | 125/241 [00:18<00:19,  6.06it/s]Epoch: 4, train for the 51-th batch, train loss: 0.4878077208995819:  43%|█████▌       | 51/119 [00:07<00:09,  7.34it/s]Epoch: 2, train for the 27-th batch, train loss: 0.5826640129089355:  11%|█▍           | 27/237 [00:03<00:32,  6.52it/s]Epoch: 2, train for the 126-th batch, train loss: 0.4334717392921448:  52%|█████▊     | 126/241 [00:18<00:18,  6.25it/s]Epoch: 3, train for the 31-th batch, train loss: 0.6894561648368835:  20%|██▌          | 30/151 [00:04<00:19,  6.13it/s]Epoch: 3, train for the 31-th batch, train loss: 0.6894561648368835:  21%|██▋          | 31/151 [00:04<00:20,  5.96it/s]Epoch: 1, train for the 289-th batch, train loss: 0.46312299370765686:  75%|███████▌  | 288/383 [01:19<00:41,  2.28it/s]Epoch: 3, train for the 135-th batch, train loss: 0.5129527449607849:  92%|██████████ | 134/146 [00:20<00:02,  5.92it/s]Epoch: 3, train for the 135-th batch, train loss: 0.5129527449607849:  92%|██████████▏| 135/146 [00:20<00:02,  4.87it/s]Epoch: 4, train for the 52-th batch, train loss: 0.4763370454311371:  43%|█████▌       | 51/119 [00:07<00:09,  7.34it/s]Epoch: 2, train for the 127-th batch, train loss: 0.5103904604911804:  52%|█████▊     | 126/241 [00:18<00:18,  6.25it/s]Epoch: 1, train for the 289-th batch, train loss: 0.46312299370765686:  75%|███████▌  | 289/383 [01:19<00:38,  2.47it/s]Epoch: 4, train for the 52-th batch, train loss: 0.4763370454311371:  44%|█████▋       | 52/119 [00:07<00:09,  7.41it/s]Epoch: 2, train for the 28-th batch, train loss: 0.6547105312347412:  11%|█▍           | 27/237 [00:04<00:32,  6.52it/s]Epoch: 2, train for the 127-th batch, train loss: 0.5103904604911804:  53%|█████▊     | 127/241 [00:18<00:17,  6.55it/s]Epoch: 2, train for the 28-th batch, train loss: 0.6547105312347412:  12%|█▌           | 28/237 [00:04<00:31,  6.59it/s]Epoch: 3, train for the 32-th batch, train loss: 0.6736396551132202:  21%|██▋          | 31/151 [00:04<00:20,  5.96it/s]Epoch: 3, train for the 32-th batch, train loss: 0.6736396551132202:  21%|██▊          | 32/151 [00:04<00:18,  6.47it/s]Epoch: 3, train for the 136-th batch, train loss: 0.4849422872066498:  92%|██████████▏| 135/146 [00:20<00:02,  4.87it/s]Epoch: 3, train for the 136-th batch, train loss: 0.4849422872066498:  93%|██████████▏| 136/146 [00:20<00:01,  5.23it/s]Epoch: 4, train for the 53-th batch, train loss: 0.441262423992157:  44%|██████        | 52/119 [00:07<00:09,  7.41it/s]Epoch: 2, train for the 29-th batch, train loss: 0.5868431925773621:  12%|█▌           | 28/237 [00:04<00:31,  6.59it/s]Epoch: 4, train for the 53-th batch, train loss: 0.441262423992157:  45%|██████▏       | 53/119 [00:07<00:09,  6.98it/s]Epoch: 2, train for the 29-th batch, train loss: 0.5868431925773621:  12%|█▌           | 29/237 [00:04<00:31,  6.57it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5321503281593323:  53%|█████▊     | 127/241 [00:18<00:17,  6.55it/s]Epoch: 3, train for the 33-th batch, train loss: 0.6637704968452454:  21%|██▊          | 32/151 [00:04<00:18,  6.47it/s]Epoch: 3, train for the 33-th batch, train loss: 0.6637704968452454:  22%|██▊          | 33/151 [00:04<00:19,  5.96it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5321503281593323:  53%|█████▊     | 128/241 [00:18<00:20,  5.50it/s]Epoch: 3, train for the 137-th batch, train loss: 0.4604951739311218:  93%|██████████▏| 136/146 [00:20<00:01,  5.23it/s]Epoch: 3, train for the 137-th batch, train loss: 0.4604951739311218:  94%|██████████▎| 137/146 [00:20<00:01,  5.59it/s]Epoch: 2, train for the 30-th batch, train loss: 0.6759980916976929:  12%|█▌           | 29/237 [00:04<00:31,  6.57it/s]Epoch: 4, train for the 54-th batch, train loss: 0.4129641652107239:  45%|█████▊       | 53/119 [00:07<00:09,  6.98it/s]Epoch: 2, train for the 30-th batch, train loss: 0.6759980916976929:  13%|█▋           | 30/237 [00:04<00:30,  6.80it/s]Epoch: 4, train for the 54-th batch, train loss: 0.4129641652107239:  45%|█████▉       | 54/119 [00:07<00:09,  6.76it/s]Epoch: 3, train for the 138-th batch, train loss: 0.5175061821937561:  94%|██████████▎| 137/146 [00:20<00:01,  5.59it/s]Epoch: 2, train for the 31-th batch, train loss: 0.5825467109680176:  13%|█▋           | 30/237 [00:04<00:30,  6.80it/s]Epoch: 3, train for the 34-th batch, train loss: 0.6607468128204346:  22%|██▊          | 33/151 [00:05<00:19,  5.96it/s]Epoch: 2, train for the 129-th batch, train loss: 0.45108795166015625:  53%|█████▎    | 128/241 [00:19<00:20,  5.50it/s]Epoch: 2, train for the 31-th batch, train loss: 0.5825467109680176:  13%|█▋           | 31/237 [00:04<00:28,  7.31it/s]Epoch: 3, train for the 138-th batch, train loss: 0.5175061821937561:  95%|██████████▍| 138/146 [00:20<00:01,  5.94it/s]Epoch: 3, train for the 34-th batch, train loss: 0.6607468128204346:  23%|██▉          | 34/151 [00:05<00:20,  5.85it/s]Epoch: 2, train for the 129-th batch, train loss: 0.45108795166015625:  54%|█████▎    | 129/241 [00:19<00:19,  5.62it/s]Epoch: 4, train for the 55-th batch, train loss: 0.43421661853790283:  45%|█████▍      | 54/119 [00:07<00:09,  6.76it/s]Epoch: 4, train for the 55-th batch, train loss: 0.43421661853790283:  46%|█████▌      | 55/119 [00:07<00:09,  6.76it/s]Epoch: 1, train for the 290-th batch, train loss: 0.44636276364326477:  75%|███████▌  | 289/383 [01:20<00:38,  2.47it/s]Epoch: 2, train for the 32-th batch, train loss: 0.6009909510612488:  13%|█▋           | 31/237 [00:04<00:28,  7.31it/s]Epoch: 2, train for the 130-th batch, train loss: 0.5002803206443787:  54%|█████▉     | 129/241 [00:19<00:19,  5.62it/s]Epoch: 2, train for the 32-th batch, train loss: 0.6009909510612488:  14%|█▊           | 32/237 [00:04<00:27,  7.46it/s]Epoch: 3, train for the 139-th batch, train loss: 0.5231308937072754:  95%|██████████▍| 138/146 [00:20<00:01,  5.94it/s]Epoch: 1, train for the 290-th batch, train loss: 0.44636276364326477:  76%|███████▌  | 290/383 [01:20<00:42,  2.20it/s]Epoch: 2, train for the 130-th batch, train loss: 0.5002803206443787:  54%|█████▉     | 130/241 [00:19<00:18,  6.01it/s]Epoch: 3, train for the 139-th batch, train loss: 0.5231308937072754:  95%|██████████▍| 139/146 [00:20<00:01,  6.15it/s]Epoch: 4, train for the 56-th batch, train loss: 0.44288626313209534:  46%|█████▌      | 55/119 [00:08<00:09,  6.76it/s]Epoch: 4, train for the 56-th batch, train loss: 0.44288626313209534:  47%|█████▋      | 56/119 [00:08<00:09,  6.67it/s]Epoch: 2, train for the 33-th batch, train loss: 0.5893802642822266:  14%|█▊           | 32/237 [00:04<00:27,  7.46it/s]Epoch: 2, train for the 33-th batch, train loss: 0.5893802642822266:  14%|█▊           | 33/237 [00:04<00:26,  7.69it/s]Epoch: 3, train for the 140-th batch, train loss: 0.48900023102760315:  95%|█████████▌| 139/146 [00:20<00:01,  6.15it/s]Epoch: 3, train for the 140-th batch, train loss: 0.48900023102760315:  96%|█████████▌| 140/146 [00:20<00:00,  6.19it/s]Epoch: 3, train for the 35-th batch, train loss: 0.6520553231239319:  23%|██▉          | 34/151 [00:05<00:20,  5.85it/s]Epoch: 4, train for the 57-th batch, train loss: 0.44131964445114136:  47%|█████▋      | 56/119 [00:08<00:09,  6.67it/s]Epoch: 3, train for the 35-th batch, train loss: 0.6520553231239319:  23%|███          | 35/151 [00:05<00:24,  4.65it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5253267288208008:  54%|█████▉     | 130/241 [00:19<00:18,  6.01it/s]Epoch: 4, train for the 57-th batch, train loss: 0.44131964445114136:  48%|█████▋      | 57/119 [00:08<00:08,  6.95it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5253267288208008:  54%|█████▉     | 131/241 [00:19<00:19,  5.71it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5515627264976501:  14%|█▊           | 33/237 [00:04<00:26,  7.69it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5515627264976501:  14%|█▊           | 34/237 [00:04<00:28,  7.24it/s]Epoch: 3, train for the 36-th batch, train loss: 0.6800703406333923:  23%|███          | 35/151 [00:05<00:24,  4.65it/s]Epoch: 3, train for the 141-th batch, train loss: 0.5091857314109802:  96%|██████████▌| 140/146 [00:21<00:00,  6.19it/s]Epoch: 3, train for the 36-th batch, train loss: 0.6800703406333923:  24%|███          | 36/151 [00:05<00:22,  5.18it/s]Epoch: 3, train for the 141-th batch, train loss: 0.5091857314109802:  97%|██████████▌| 141/146 [00:21<00:00,  6.21it/s]Epoch: 2, train for the 132-th batch, train loss: 0.4940745532512665:  54%|█████▉     | 131/241 [00:19<00:19,  5.71it/s]Epoch: 4, train for the 58-th batch, train loss: 0.4266674816608429:  48%|██████▏      | 57/119 [00:08<00:08,  6.95it/s]Epoch: 4, train for the 58-th batch, train loss: 0.4266674816608429:  49%|██████▎      | 58/119 [00:08<00:09,  6.74it/s]Epoch: 2, train for the 132-th batch, train loss: 0.4940745532512665:  55%|██████     | 132/241 [00:19<00:18,  5.93it/s]Epoch: 2, train for the 35-th batch, train loss: 0.542073667049408:  14%|██            | 34/237 [00:04<00:28,  7.24it/s]Epoch: 2, train for the 35-th batch, train loss: 0.542073667049408:  15%|██            | 35/237 [00:04<00:26,  7.76it/s]Epoch: 1, train for the 291-th batch, train loss: 0.35345977544784546:  76%|███████▌  | 290/383 [01:20<00:42,  2.20it/s]Epoch: 3, train for the 37-th batch, train loss: 0.6660438179969788:  24%|███          | 36/151 [00:05<00:22,  5.18it/s]Epoch: 1, train for the 291-th batch, train loss: 0.35345977544784546:  76%|███████▌  | 291/383 [01:20<00:41,  2.19it/s]Epoch: 3, train for the 142-th batch, train loss: 0.46342241764068604:  97%|█████████▋| 141/146 [00:21<00:00,  6.21it/s]Epoch: 3, train for the 37-th batch, train loss: 0.6660438179969788:  25%|███▏         | 37/151 [00:05<00:20,  5.64it/s]Epoch: 3, train for the 142-th batch, train loss: 0.46342241764068604:  97%|█████████▋| 142/146 [00:21<00:00,  6.39it/s]Epoch: 2, train for the 36-th batch, train loss: 0.5738164186477661:  15%|█▉           | 35/237 [00:05<00:26,  7.76it/s]Epoch: 2, train for the 133-th batch, train loss: 0.41042792797088623:  55%|█████▍    | 132/241 [00:19<00:18,  5.93it/s]Epoch: 4, train for the 59-th batch, train loss: 0.4655393362045288:  49%|██████▎      | 58/119 [00:08<00:09,  6.74it/s]Epoch: 2, train for the 36-th batch, train loss: 0.5738164186477661:  15%|█▉           | 36/237 [00:05<00:25,  7.77it/s]Epoch: 2, train for the 133-th batch, train loss: 0.41042792797088623:  55%|█████▌    | 133/241 [00:19<00:17,  6.13it/s]Epoch: 4, train for the 59-th batch, train loss: 0.4655393362045288:  50%|██████▍      | 59/119 [00:08<00:09,  6.51it/s]Epoch: 3, train for the 38-th batch, train loss: 0.6759693622589111:  25%|███▏         | 37/151 [00:05<00:20,  5.64it/s]Epoch: 3, train for the 38-th batch, train loss: 0.6759693622589111:  25%|███▎         | 38/151 [00:05<00:19,  5.82it/s]Epoch: 3, train for the 143-th batch, train loss: 0.4942900836467743:  97%|██████████▋| 142/146 [00:21<00:00,  6.39it/s]Epoch: 2, train for the 37-th batch, train loss: 0.6126217246055603:  15%|█▉           | 36/237 [00:05<00:25,  7.77it/s]Epoch: 3, train for the 143-th batch, train loss: 0.4942900836467743:  98%|██████████▊| 143/146 [00:21<00:00,  6.17it/s]Epoch: 2, train for the 37-th batch, train loss: 0.6126217246055603:  16%|██           | 37/237 [00:05<00:27,  7.37it/s]Epoch: 2, train for the 134-th batch, train loss: 0.47607579827308655:  55%|█████▌    | 133/241 [00:19<00:17,  6.13it/s]Epoch: 4, train for the 60-th batch, train loss: 0.4549216330051422:  50%|██████▍      | 59/119 [00:08<00:09,  6.51it/s]Epoch: 4, train for the 60-th batch, train loss: 0.4549216330051422:  50%|██████▌      | 60/119 [00:08<00:09,  6.44it/s]Epoch: 2, train for the 134-th batch, train loss: 0.47607579827308655:  56%|█████▌    | 134/241 [00:19<00:17,  6.03it/s]Epoch: 1, train for the 292-th batch, train loss: 0.47808825969696045:  76%|███████▌  | 291/383 [01:20<00:41,  2.19it/s]Epoch: 3, train for the 39-th batch, train loss: 0.6616945266723633:  25%|███▎         | 38/151 [00:05<00:19,  5.82it/s]Epoch: 3, train for the 39-th batch, train loss: 0.6616945266723633:  26%|███▎         | 39/151 [00:05<00:18,  6.18it/s]Epoch: 2, train for the 38-th batch, train loss: 0.5634621977806091:  16%|██           | 37/237 [00:05<00:27,  7.37it/s]Epoch: 1, train for the 292-th batch, train loss: 0.47808825969696045:  76%|███████▌  | 292/383 [01:20<00:38,  2.39it/s]Epoch: 2, train for the 38-th batch, train loss: 0.5634621977806091:  16%|██           | 38/237 [00:05<00:27,  7.24it/s]Epoch: 2, train for the 135-th batch, train loss: 0.4011287987232208:  56%|██████     | 134/241 [00:19<00:17,  6.03it/s]Epoch: 3, train for the 144-th batch, train loss: 0.4349207878112793:  98%|██████████▊| 143/146 [00:21<00:00,  6.17it/s]Epoch: 3, train for the 144-th batch, train loss: 0.4349207878112793:  99%|██████████▊| 144/146 [00:21<00:00,  6.18it/s]Epoch: 2, train for the 135-th batch, train loss: 0.4011287987232208:  56%|██████▏    | 135/241 [00:20<00:16,  6.30it/s]Epoch: 4, train for the 61-th batch, train loss: 0.46265989542007446:  50%|██████      | 60/119 [00:08<00:09,  6.44it/s]Epoch: 4, train for the 61-th batch, train loss: 0.46265989542007446:  51%|██████▏     | 61/119 [00:08<00:09,  6.41it/s]Epoch: 3, train for the 40-th batch, train loss: 0.6563020944595337:  26%|███▎         | 39/151 [00:06<00:18,  6.18it/s]Epoch: 3, train for the 40-th batch, train loss: 0.6563020944595337:  26%|███▍         | 40/151 [00:06<00:17,  6.28it/s]Epoch: 2, train for the 39-th batch, train loss: 0.6099120378494263:  16%|██           | 38/237 [00:05<00:27,  7.24it/s]Epoch: 2, train for the 39-th batch, train loss: 0.6099120378494263:  16%|██▏          | 39/237 [00:05<00:28,  6.93it/s]Epoch: 3, train for the 145-th batch, train loss: 0.5060510039329529:  99%|██████████▊| 144/146 [00:21<00:00,  6.18it/s]Epoch: 3, train for the 145-th batch, train loss: 0.5060510039329529:  99%|██████████▉| 145/146 [00:21<00:00,  6.09it/s]Epoch: 2, train for the 136-th batch, train loss: 0.4478197693824768:  56%|██████▏    | 135/241 [00:20<00:16,  6.30it/s]Epoch: 4, train for the 62-th batch, train loss: 0.460269570350647:  51%|███████▏      | 61/119 [00:09<00:09,  6.41it/s]Epoch: 4, train for the 62-th batch, train loss: 0.460269570350647:  52%|███████▎      | 62/119 [00:09<00:09,  6.21it/s]Epoch: 2, train for the 136-th batch, train loss: 0.4478197693824768:  56%|██████▏    | 136/241 [00:20<00:17,  6.01it/s]Epoch: 1, train for the 293-th batch, train loss: 0.45766347646713257:  76%|███████▌  | 292/383 [01:21<00:38,  2.39it/s]Epoch: 3, train for the 41-th batch, train loss: 0.6638232469558716:  26%|███▍         | 40/151 [00:06<00:17,  6.28it/s]Epoch: 3, train for the 41-th batch, train loss: 0.6638232469558716:  27%|███▌         | 41/151 [00:06<00:16,  6.53it/s]Epoch: 2, train for the 40-th batch, train loss: 0.5769863724708557:  16%|██▏          | 39/237 [00:05<00:28,  6.93it/s]Epoch: 3, train for the 146-th batch, train loss: 0.42035043239593506:  99%|█████████▉| 145/146 [00:21<00:00,  6.09it/s]Epoch: 2, train for the 40-th batch, train loss: 0.5769863724708557:  17%|██▏          | 40/237 [00:05<00:28,  6.86it/s]Epoch: 1, train for the 293-th batch, train loss: 0.45766347646713257:  77%|███████▋  | 293/383 [01:21<00:34,  2.59it/s]Epoch: 3, train for the 146-th batch, train loss: 0.42035043239593506: 100%|██████████| 146/146 [00:21<00:00,  6.58it/s]Epoch: 3, train for the 146-th batch, train loss: 0.42035043239593506: 100%|██████████| 146/146 [00:21<00:00,  6.69it/s]
Epoch: 4, train for the 63-th batch, train loss: 0.44397979974746704:  52%|██████▎     | 62/119 [00:09<00:09,  6.21it/s]Epoch: 2, train for the 137-th batch, train loss: 0.41658228635787964:  56%|█████▋    | 136/241 [00:20<00:17,  6.01it/s]Epoch: 4, train for the 63-th batch, train loss: 0.44397979974746704:  53%|██████▎     | 63/119 [00:09<00:09,  6.22it/s]  0%|                                                                                            | 0/38 [00:00<?, ?it/s]Epoch: 2, train for the 137-th batch, train loss: 0.41658228635787964:  57%|█████▋    | 137/241 [00:20<00:17,  5.90it/s]Epoch: 3, train for the 42-th batch, train loss: 0.6556246876716614:  27%|███▌         | 41/151 [00:06<00:16,  6.53it/s]Epoch: 3, train for the 42-th batch, train loss: 0.6556246876716614:  28%|███▌         | 42/151 [00:06<00:16,  6.54it/s]Epoch: 2, train for the 41-th batch, train loss: 0.576763927936554:  17%|██▎           | 40/237 [00:05<00:28,  6.86it/s]evaluate for the 1-th batch, evaluate loss: 0.4852919578552246:   0%|                            | 0/38 [00:00<?, ?it/s]Epoch: 2, train for the 41-th batch, train loss: 0.576763927936554:  17%|██▍           | 41/237 [00:05<00:28,  6.97it/s]Epoch: 4, train for the 64-th batch, train loss: 0.4494856894016266:  53%|██████▉      | 63/119 [00:09<00:09,  6.22it/s]evaluate for the 2-th batch, evaluate loss: 0.4865632653236389:   0%|                            | 0/38 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.4865632653236389:   5%|█                   | 2/38 [00:00<00:02, 15.15it/s]Epoch: 4, train for the 64-th batch, train loss: 0.4494856894016266:  54%|██████▉      | 64/119 [00:09<00:08,  6.37it/s]Epoch: 2, train for the 138-th batch, train loss: 0.41160833835601807:  57%|█████▋    | 137/241 [00:20<00:17,  5.90it/s]Epoch: 2, train for the 138-th batch, train loss: 0.41160833835601807:  57%|█████▋    | 138/241 [00:20<00:17,  5.96it/s]Epoch: 3, train for the 43-th batch, train loss: 0.6697917580604553:  28%|███▌         | 42/151 [00:06<00:16,  6.54it/s]Epoch: 1, train for the 294-th batch, train loss: 0.2992326319217682:  77%|████████▍  | 293/383 [01:21<00:34,  2.59it/s]Epoch: 3, train for the 43-th batch, train loss: 0.6697917580604553:  28%|███▋         | 43/151 [00:06<00:16,  6.50it/s]evaluate for the 3-th batch, evaluate loss: 0.47385725378990173:   5%|█                  | 2/38 [00:00<00:02, 15.15it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5971786975860596:  17%|██▏          | 41/237 [00:05<00:28,  6.97it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5971786975860596:  18%|██▎          | 42/237 [00:06<00:29,  6.70it/s]Epoch: 1, train for the 294-th batch, train loss: 0.2992326319217682:  77%|████████▍  | 294/383 [01:21<00:32,  2.76it/s]evaluate for the 4-th batch, evaluate loss: 0.4785766005516052:   5%|█                   | 2/38 [00:00<00:02, 15.15it/s]evaluate for the 4-th batch, evaluate loss: 0.4785766005516052:  11%|██                  | 4/38 [00:00<00:02, 15.28it/s]evaluate for the 5-th batch, evaluate loss: 0.5452811121940613:  11%|██                  | 4/38 [00:00<00:02, 15.28it/s]Epoch: 2, train for the 139-th batch, train loss: 0.33677712082862854:  57%|█████▋    | 138/241 [00:20<00:17,  5.96it/s]evaluate for the 6-th batch, evaluate loss: 0.48626577854156494:  11%|██                 | 4/38 [00:00<00:02, 15.28it/s]Epoch: 3, train for the 44-th batch, train loss: 0.6428032517433167:  28%|███▋         | 43/151 [00:06<00:16,  6.50it/s]Epoch: 2, train for the 139-th batch, train loss: 0.33677712082862854:  58%|█████▊    | 139/241 [00:20<00:17,  5.68it/s]Epoch: 3, train for the 44-th batch, train loss: 0.6428032517433167:  29%|███▊         | 44/151 [00:06<00:17,  6.19it/s]Epoch: 2, train for the 43-th batch, train loss: 0.5608952045440674:  18%|██▎          | 42/237 [00:06<00:29,  6.70it/s]Epoch: 2, train for the 43-th batch, train loss: 0.5608952045440674:  18%|██▎          | 43/237 [00:06<00:30,  6.32it/s]evaluate for the 7-th batch, evaluate loss: 0.43782341480255127:  11%|██                 | 4/38 [00:00<00:02, 15.28it/s]evaluate for the 7-th batch, evaluate loss: 0.43782341480255127:  18%|███▌               | 7/38 [00:00<00:01, 16.28it/s]Epoch: 4, train for the 65-th batch, train loss: 0.45312875509262085:  54%|██████▍     | 64/119 [00:09<00:08,  6.37it/s]Epoch: 4, train for the 65-th batch, train loss: 0.45312875509262085:  55%|██████▌     | 65/119 [00:09<00:11,  4.81it/s]evaluate for the 8-th batch, evaluate loss: 0.4606817662715912:  18%|███▋                | 7/38 [00:00<00:01, 16.28it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5821405649185181:  18%|██▎          | 43/237 [00:06<00:30,  6.32it/s]Epoch: 2, train for the 140-th batch, train loss: 0.3763583302497864:  58%|██████▎    | 139/241 [00:20<00:17,  5.68it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5821405649185181:  19%|██▍          | 44/237 [00:06<00:28,  6.77it/s]Epoch: 3, train for the 45-th batch, train loss: 0.6409904360771179:  29%|███▊         | 44/151 [00:06<00:17,  6.19it/s]Epoch: 2, train for the 140-th batch, train loss: 0.3763583302497864:  58%|██████▍    | 140/241 [00:20<00:17,  5.70it/s]Epoch: 3, train for the 45-th batch, train loss: 0.6409904360771179:  30%|███▊         | 45/151 [00:06<00:17,  5.96it/s]evaluate for the 9-th batch, evaluate loss: 0.47693151235580444:  18%|███▌               | 7/38 [00:00<00:01, 16.28it/s]evaluate for the 9-th batch, evaluate loss: 0.47693151235580444:  24%|████▌              | 9/38 [00:00<00:01, 16.04it/s]Epoch: 4, train for the 66-th batch, train loss: 0.4489331841468811:  55%|███████      | 65/119 [00:09<00:11,  4.81it/s]Epoch: 4, train for the 66-th batch, train loss: 0.4489331841468811:  55%|███████▏     | 66/119 [00:09<00:10,  5.26it/s]Epoch: 1, train for the 295-th batch, train loss: 0.3560599088668823:  77%|████████▍  | 294/383 [01:21<00:32,  2.76it/s]evaluate for the 10-th batch, evaluate loss: 0.5018643140792847:  24%|████▌              | 9/38 [00:00<00:01, 16.04it/s]Epoch: 2, train for the 45-th batch, train loss: 0.6059392690658569:  19%|██▍          | 44/237 [00:06<00:28,  6.77it/s]Epoch: 2, train for the 141-th batch, train loss: 0.47863146662712097:  58%|█████▊    | 140/241 [00:21<00:17,  5.70it/s]Epoch: 2, train for the 45-th batch, train loss: 0.6059392690658569:  19%|██▍          | 45/237 [00:06<00:28,  6.63it/s]Epoch: 1, train for the 295-th batch, train loss: 0.3560599088668823:  77%|████████▍  | 295/383 [01:22<00:34,  2.56it/s]Epoch: 2, train for the 141-th batch, train loss: 0.47863146662712097:  59%|█████▊    | 141/241 [00:21<00:17,  5.85it/s]Epoch: 3, train for the 46-th batch, train loss: 0.6442735195159912:  30%|███▊         | 45/151 [00:07<00:17,  5.96it/s]evaluate for the 11-th batch, evaluate loss: 0.44785842299461365:  24%|████▎             | 9/38 [00:00<00:01, 16.04it/s]evaluate for the 11-th batch, evaluate loss: 0.44785842299461365:  29%|████▉            | 11/38 [00:00<00:01, 15.33it/s]Epoch: 3, train for the 46-th batch, train loss: 0.6442735195159912:  30%|███▉         | 46/151 [00:07<00:17,  5.93it/s]Epoch: 4, train for the 67-th batch, train loss: 0.5310571193695068:  55%|███████▏     | 66/119 [00:09<00:10,  5.26it/s]Epoch: 4, train for the 67-th batch, train loss: 0.5310571193695068:  56%|███████▎     | 67/119 [00:09<00:09,  5.72it/s]evaluate for the 12-th batch, evaluate loss: 0.5224941968917847:  29%|█████▏            | 11/38 [00:00<00:01, 15.33it/s]Epoch: 2, train for the 142-th batch, train loss: 0.45943981409072876:  59%|█████▊    | 141/241 [00:21<00:17,  5.85it/s]Epoch: 2, train for the 46-th batch, train loss: 0.5678643584251404:  19%|██▍          | 45/237 [00:06<00:28,  6.63it/s]Epoch: 2, train for the 142-th batch, train loss: 0.45943981409072876:  59%|█████▉    | 142/241 [00:21<00:16,  5.96it/s]evaluate for the 13-th batch, evaluate loss: 0.4901295304298401:  29%|█████▏            | 11/38 [00:00<00:01, 15.33it/s]evaluate for the 13-th batch, evaluate loss: 0.4901295304298401:  34%|██████▏           | 13/38 [00:00<00:01, 14.34it/s]Epoch: 2, train for the 46-th batch, train loss: 0.5678643584251404:  19%|██▌          | 46/237 [00:06<00:30,  6.25it/s]Epoch: 3, train for the 47-th batch, train loss: 0.647031843662262:  30%|████▎         | 46/151 [00:07<00:17,  5.93it/s]Epoch: 3, train for the 47-th batch, train loss: 0.647031843662262:  31%|████▎         | 47/151 [00:07<00:17,  5.81it/s]Epoch: 4, train for the 68-th batch, train loss: 0.43481048941612244:  56%|██████▊     | 67/119 [00:10<00:09,  5.72it/s]Epoch: 4, train for the 68-th batch, train loss: 0.43481048941612244:  57%|██████▊     | 68/119 [00:10<00:08,  5.82it/s]evaluate for the 14-th batch, evaluate loss: 0.43092992901802063:  34%|█████▊           | 13/38 [00:00<00:01, 14.34it/s]Epoch: 1, train for the 296-th batch, train loss: 0.2984262704849243:  77%|████████▍  | 295/383 [01:22<00:34,  2.56it/s]evaluate for the 15-th batch, evaluate loss: 0.4456600844860077:  34%|██████▏           | 13/38 [00:00<00:01, 14.34it/s]evaluate for the 15-th batch, evaluate loss: 0.4456600844860077:  39%|███████           | 15/38 [00:00<00:01, 14.72it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5663759112358093:  19%|██▌          | 46/237 [00:06<00:30,  6.25it/s]Epoch: 2, train for the 143-th batch, train loss: 0.4828454554080963:  59%|██████▍    | 142/241 [00:21<00:16,  5.96it/s]Epoch: 1, train for the 296-th batch, train loss: 0.2984262704849243:  77%|████████▌  | 296/383 [01:22<00:32,  2.70it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5663759112358093:  20%|██▌          | 47/237 [00:06<00:29,  6.38it/s]Epoch: 2, train for the 143-th batch, train loss: 0.4828454554080963:  59%|██████▌    | 143/241 [00:21<00:16,  5.96it/s]Epoch: 3, train for the 48-th batch, train loss: 0.6208099722862244:  31%|████         | 47/151 [00:07<00:17,  5.81it/s]Epoch: 3, train for the 48-th batch, train loss: 0.6208099722862244:  32%|████▏        | 48/151 [00:07<00:17,  5.98it/s]Epoch: 4, train for the 69-th batch, train loss: 0.42832863330841064:  57%|██████▊     | 68/119 [00:10<00:08,  5.82it/s]evaluate for the 16-th batch, evaluate loss: 0.5159270167350769:  39%|███████           | 15/38 [00:01<00:01, 14.72it/s]Epoch: 4, train for the 69-th batch, train loss: 0.42832863330841064:  58%|██████▉     | 69/119 [00:10<00:08,  5.79it/s]evaluate for the 17-th batch, evaluate loss: 0.4670972228050232:  39%|███████           | 15/38 [00:01<00:01, 14.72it/s]evaluate for the 17-th batch, evaluate loss: 0.4670972228050232:  45%|████████          | 17/38 [00:01<00:01, 14.12it/s]Epoch: 2, train for the 48-th batch, train loss: 0.559366762638092:  20%|██▊           | 47/237 [00:06<00:29,  6.38it/s]Epoch: 2, train for the 48-th batch, train loss: 0.559366762638092:  20%|██▊           | 48/237 [00:06<00:31,  6.08it/s]Epoch: 2, train for the 144-th batch, train loss: 0.5730239152908325:  59%|██████▌    | 143/241 [00:21<00:16,  5.96it/s]Epoch: 2, train for the 144-th batch, train loss: 0.5730239152908325:  60%|██████▌    | 144/241 [00:21<00:17,  5.56it/s]evaluate for the 18-th batch, evaluate loss: 0.5045432448387146:  45%|████████          | 17/38 [00:01<00:01, 14.12it/s]Epoch: 3, train for the 49-th batch, train loss: 0.6490702033042908:  32%|████▏        | 48/151 [00:07<00:17,  5.98it/s]Epoch: 4, train for the 70-th batch, train loss: 0.4063485562801361:  58%|███████▌     | 69/119 [00:10<00:08,  5.79it/s]Epoch: 3, train for the 49-th batch, train loss: 0.6490702033042908:  32%|████▏        | 49/151 [00:07<00:18,  5.62it/s]Epoch: 1, train for the 297-th batch, train loss: 0.4975837469100952:  77%|████████▌  | 296/383 [01:22<00:32,  2.70it/s]Epoch: 4, train for the 70-th batch, train loss: 0.4063485562801361:  59%|███████▋     | 70/119 [00:10<00:08,  5.77it/s]evaluate for the 19-th batch, evaluate loss: 0.49051931500434875:  45%|███████▌         | 17/38 [00:01<00:01, 14.12it/s]evaluate for the 19-th batch, evaluate loss: 0.49051931500434875:  50%|████████▌        | 19/38 [00:01<00:01, 13.76it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5292361378669739:  20%|██▋          | 48/237 [00:07<00:31,  6.08it/s]Epoch: 1, train for the 297-th batch, train loss: 0.4975837469100952:  78%|████████▌  | 297/383 [01:22<00:31,  2.77it/s]evaluate for the 20-th batch, evaluate loss: 0.40044310688972473:  50%|████████▌        | 19/38 [00:01<00:01, 13.76it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5292361378669739:  21%|██▋          | 49/237 [00:07<00:31,  6.01it/s]Epoch: 2, train for the 145-th batch, train loss: 0.5870024561882019:  60%|██████▌    | 144/241 [00:21<00:17,  5.56it/s]Epoch: 2, train for the 145-th batch, train loss: 0.5870024561882019:  60%|██████▌    | 145/241 [00:21<00:17,  5.49it/s]Epoch: 3, train for the 50-th batch, train loss: 0.6349306106567383:  32%|████▏        | 49/151 [00:07<00:18,  5.62it/s]Epoch: 4, train for the 71-th batch, train loss: 0.43220072984695435:  59%|███████     | 70/119 [00:10<00:08,  5.77it/s]evaluate for the 21-th batch, evaluate loss: 0.4368050992488861:  50%|█████████         | 19/38 [00:01<00:01, 13.76it/s]evaluate for the 21-th batch, evaluate loss: 0.4368050992488861:  55%|█████████▉        | 21/38 [00:01<00:01, 13.94it/s]Epoch: 4, train for the 71-th batch, train loss: 0.43220072984695435:  60%|███████▏    | 71/119 [00:10<00:08,  5.70it/s]Epoch: 3, train for the 50-th batch, train loss: 0.6349306106567383:  33%|████▎        | 50/151 [00:07<00:18,  5.52it/s]Epoch: 2, train for the 50-th batch, train loss: 0.6029039621353149:  21%|██▋          | 49/237 [00:07<00:31,  6.01it/s]evaluate for the 22-th batch, evaluate loss: 0.46796634793281555:  55%|█████████▍       | 21/38 [00:01<00:01, 13.94it/s]Epoch: 2, train for the 50-th batch, train loss: 0.6029039621353149:  21%|██▋          | 50/237 [00:07<00:31,  5.91it/s]Epoch: 2, train for the 146-th batch, train loss: 0.5935592651367188:  60%|██████▌    | 145/241 [00:21<00:17,  5.49it/s]Epoch: 4, train for the 72-th batch, train loss: 0.45891204476356506:  60%|███████▏    | 71/119 [00:10<00:08,  5.70it/s]Epoch: 1, train for the 298-th batch, train loss: 0.4764516353607178:  78%|████████▌  | 297/383 [01:22<00:31,  2.77it/s]Epoch: 4, train for the 72-th batch, train loss: 0.45891204476356506:  61%|███████▎    | 72/119 [00:10<00:08,  5.84it/s]evaluate for the 23-th batch, evaluate loss: 0.46785399317741394:  55%|█████████▍       | 21/38 [00:01<00:01, 13.94it/s]evaluate for the 23-th batch, evaluate loss: 0.46785399317741394:  61%|██████████▎      | 23/38 [00:01<00:01, 13.36it/s]Epoch: 3, train for the 51-th batch, train loss: 0.6422373056411743:  33%|████▎        | 50/151 [00:08<00:18,  5.52it/s]Epoch: 2, train for the 146-th batch, train loss: 0.5935592651367188:  61%|██████▋    | 146/241 [00:21<00:17,  5.45it/s]Epoch: 3, train for the 51-th batch, train loss: 0.6422373056411743:  34%|████▍        | 51/151 [00:08<00:18,  5.54it/s]Epoch: 2, train for the 51-th batch, train loss: 0.5902327299118042:  21%|██▋          | 50/237 [00:07<00:31,  5.91it/s]evaluate for the 24-th batch, evaluate loss: 0.4528958797454834:  61%|██████████▉       | 23/38 [00:01<00:01, 13.36it/s]Epoch: 2, train for the 51-th batch, train loss: 0.5902327299118042:  22%|██▊          | 51/237 [00:07<00:29,  6.21it/s]Epoch: 1, train for the 298-th batch, train loss: 0.4764516353607178:  78%|████████▌  | 298/383 [01:23<00:30,  2.80it/s]Epoch: 4, train for the 73-th batch, train loss: 0.45123404264450073:  61%|███████▎    | 72/119 [00:10<00:08,  5.84it/s]Epoch: 4, train for the 73-th batch, train loss: 0.45123404264450073:  61%|███████▎    | 73/119 [00:10<00:07,  6.32it/s]evaluate for the 25-th batch, evaluate loss: 0.49149298667907715:  61%|██████████▎      | 23/38 [00:01<00:01, 13.36it/s]evaluate for the 25-th batch, evaluate loss: 0.49149298667907715:  66%|███████████▏     | 25/38 [00:01<00:00, 13.88it/s]Epoch: 2, train for the 147-th batch, train loss: 0.650847315788269:  61%|███████▎    | 146/241 [00:22<00:17,  5.45it/s]Epoch: 3, train for the 52-th batch, train loss: 0.6465492248535156:  34%|████▍        | 51/151 [00:08<00:18,  5.54it/s]Epoch: 4, train for the 74-th batch, train loss: 0.4325835704803467:  61%|███████▉     | 73/119 [00:11<00:07,  6.32it/s]Epoch: 3, train for the 52-th batch, train loss: 0.6465492248535156:  34%|████▍        | 52/151 [00:08<00:18,  5.29it/s]Epoch: 2, train for the 147-th batch, train loss: 0.650847315788269:  61%|███████▎    | 147/241 [00:22<00:18,  5.08it/s]Epoch: 2, train for the 52-th batch, train loss: 0.576582133769989:  22%|███           | 51/237 [00:07<00:29,  6.21it/s]Epoch: 2, train for the 52-th batch, train loss: 0.576582133769989:  22%|███           | 52/237 [00:07<00:30,  6.02it/s]Epoch: 4, train for the 75-th batch, train loss: 0.4351576566696167:  61%|███████▉     | 73/119 [00:11<00:07,  6.32it/s]Epoch: 2, train for the 148-th batch, train loss: 0.506406843662262:  61%|███████▎    | 147/241 [00:22<00:18,  5.08it/s]evaluate for the 26-th batch, evaluate loss: 0.47082236409187317:  66%|███████████▏     | 25/38 [00:01<00:00, 13.88it/s]Epoch: 4, train for the 75-th batch, train loss: 0.4351576566696167:  63%|████████▏    | 75/119 [00:11<00:06,  7.08it/s]Epoch: 2, train for the 53-th batch, train loss: 0.5818487405776978:  22%|██▊          | 52/237 [00:07<00:30,  6.02it/s]Epoch: 2, train for the 148-th batch, train loss: 0.506406843662262:  61%|███████▎    | 148/241 [00:22<00:17,  5.42it/s]Epoch: 2, train for the 53-th batch, train loss: 0.5818487405776978:  22%|██▉          | 53/237 [00:07<00:29,  6.34it/s]Epoch: 3, train for the 53-th batch, train loss: 0.6454545259475708:  34%|████▍        | 52/151 [00:08<00:18,  5.29it/s]Epoch: 3, train for the 53-th batch, train loss: 0.6454545259475708:  35%|████▌        | 53/151 [00:08<00:18,  5.30it/s]evaluate for the 27-th batch, evaluate loss: 0.46514320373535156:  66%|███████████▏     | 25/38 [00:02<00:00, 13.88it/s]evaluate for the 27-th batch, evaluate loss: 0.46514320373535156:  71%|████████████     | 27/38 [00:02<00:01, 10.57it/s]Epoch: 1, train for the 299-th batch, train loss: 0.5232885479927063:  78%|████████▌  | 298/383 [01:23<00:30,  2.80it/s]Epoch: 4, train for the 76-th batch, train loss: 0.4626818001270294:  63%|████████▏    | 75/119 [00:11<00:06,  7.08it/s]Epoch: 1, train for the 299-th batch, train loss: 0.5232885479927063:  78%|████████▌  | 299/383 [01:23<00:31,  2.66it/s]evaluate for the 28-th batch, evaluate loss: 0.47626692056655884:  71%|████████████     | 27/38 [00:02<00:01, 10.57it/s]Epoch: 4, train for the 76-th batch, train loss: 0.4626818001270294:  64%|████████▎    | 76/119 [00:11<00:06,  6.98it/s]Epoch: 2, train for the 54-th batch, train loss: 0.5083178877830505:  22%|██▉          | 53/237 [00:07<00:29,  6.34it/s]evaluate for the 29-th batch, evaluate loss: 0.4715721011161804:  71%|████████████▊     | 27/38 [00:02<00:01, 10.57it/s]evaluate for the 29-th batch, evaluate loss: 0.4715721011161804:  76%|█████████████▋    | 29/38 [00:02<00:00, 11.42it/s]Epoch: 2, train for the 149-th batch, train loss: 0.4688992202281952:  61%|██████▊    | 148/241 [00:22<00:17,  5.42it/s]Epoch: 2, train for the 54-th batch, train loss: 0.5083178877830505:  23%|██▉          | 54/237 [00:07<00:30,  6.00it/s]Epoch: 3, train for the 54-th batch, train loss: 0.6090536117553711:  35%|████▌        | 53/151 [00:08<00:18,  5.30it/s]Epoch: 2, train for the 149-th batch, train loss: 0.4688992202281952:  62%|██████▊    | 149/241 [00:22<00:17,  5.20it/s]Epoch: 3, train for the 54-th batch, train loss: 0.6090536117553711:  36%|████▋        | 54/151 [00:08<00:18,  5.26it/s]evaluate for the 30-th batch, evaluate loss: 0.49720999598503113:  76%|████████████▉    | 29/38 [00:02<00:00, 11.42it/s]Epoch: 4, train for the 77-th batch, train loss: 0.4339278042316437:  64%|████████▎    | 76/119 [00:11<00:06,  6.98it/s]Epoch: 4, train for the 77-th batch, train loss: 0.4339278042316437:  65%|████████▍    | 77/119 [00:11<00:06,  6.97it/s]Epoch: 4, train for the 78-th batch, train loss: 0.5030044317245483:  65%|████████▍    | 77/119 [00:11<00:06,  6.97it/s]Epoch: 4, train for the 78-th batch, train loss: 0.5030044317245483:  66%|████████▌    | 78/119 [00:11<00:05,  7.57it/s]Epoch: 2, train for the 150-th batch, train loss: 0.45783400535583496:  62%|██████▏   | 149/241 [00:22<00:17,  5.20it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5648866295814514:  23%|██▉          | 54/237 [00:08<00:30,  6.00it/s]evaluate for the 31-th batch, evaluate loss: 0.47432997822761536:  76%|████████████▉    | 29/38 [00:02<00:00, 11.42it/s]evaluate for the 31-th batch, evaluate loss: 0.47432997822761536:  82%|█████████████▊   | 31/38 [00:02<00:00, 10.54it/s]Epoch: 3, train for the 55-th batch, train loss: 0.615001916885376:  36%|█████         | 54/151 [00:08<00:18,  5.26it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5648866295814514:  23%|███          | 55/237 [00:08<00:32,  5.53it/s]Epoch: 2, train for the 150-th batch, train loss: 0.45783400535583496:  62%|██████▏   | 150/241 [00:22<00:17,  5.11it/s]Epoch: 1, train for the 300-th batch, train loss: 0.5331469774246216:  78%|████████▌  | 299/383 [01:23<00:31,  2.66it/s]Epoch: 3, train for the 55-th batch, train loss: 0.615001916885376:  36%|█████         | 55/151 [00:08<00:18,  5.20it/s]evaluate for the 32-th batch, evaluate loss: 0.4325360059738159:  82%|██████████████▋   | 31/38 [00:02<00:00, 10.54it/s]Epoch: 1, train for the 300-th batch, train loss: 0.5331469774246216:  78%|████████▌  | 300/383 [01:23<00:31,  2.66it/s]Epoch: 4, train for the 79-th batch, train loss: 0.45043912529945374:  66%|███████▊    | 78/119 [00:11<00:05,  7.57it/s]Epoch: 4, train for the 79-th batch, train loss: 0.45043912529945374:  66%|███████▉    | 79/119 [00:11<00:05,  7.27it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5180818438529968:  23%|███          | 55/237 [00:08<00:32,  5.53it/s]evaluate for the 33-th batch, evaluate loss: 0.46912384033203125:  82%|█████████████▊   | 31/38 [00:02<00:00, 10.54it/s]evaluate for the 33-th batch, evaluate loss: 0.46912384033203125:  87%|██████████████▊  | 33/38 [00:02<00:00, 11.65it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5180818438529968:  24%|███          | 56/237 [00:08<00:30,  5.91it/s]Epoch: 2, train for the 151-th batch, train loss: 0.38247960805892944:  62%|██████▏   | 150/241 [00:22<00:17,  5.11it/s]Epoch: 3, train for the 56-th batch, train loss: 0.5557880401611328:  36%|████▋        | 55/151 [00:08<00:18,  5.20it/s]Epoch: 2, train for the 151-th batch, train loss: 0.38247960805892944:  63%|██████▎   | 151/241 [00:22<00:16,  5.35it/s]evaluate for the 34-th batch, evaluate loss: 0.4958138167858124:  87%|███████████████▋  | 33/38 [00:02<00:00, 11.65it/s]Epoch: 3, train for the 56-th batch, train loss: 0.5557880401611328:  37%|████▊        | 56/151 [00:08<00:17,  5.37it/s]Epoch: 4, train for the 80-th batch, train loss: 0.4945410490036011:  66%|████████▋    | 79/119 [00:11<00:05,  7.27it/s]evaluate for the 35-th batch, evaluate loss: 0.4856780767440796:  87%|███████████████▋  | 33/38 [00:02<00:00, 11.65it/s]evaluate for the 35-th batch, evaluate loss: 0.4856780767440796:  92%|████████████████▌ | 35/38 [00:02<00:00, 12.44it/s]Epoch: 4, train for the 80-th batch, train loss: 0.4945410490036011:  67%|████████▋    | 80/119 [00:11<00:05,  7.14it/s]Epoch: 2, train for the 57-th batch, train loss: 0.578187108039856:  24%|███▎          | 56/237 [00:08<00:30,  5.91it/s]Epoch: 2, train for the 57-th batch, train loss: 0.578187108039856:  24%|███▎          | 57/237 [00:08<00:29,  6.16it/s]evaluate for the 36-th batch, evaluate loss: 0.4984147548675537:  92%|████████████████▌ | 35/38 [00:02<00:00, 12.44it/s]Epoch: 2, train for the 152-th batch, train loss: 0.3405417501926422:  63%|██████▉    | 151/241 [00:23<00:16,  5.35it/s]Epoch: 3, train for the 57-th batch, train loss: 0.5920371413230896:  37%|████▊        | 56/151 [00:09<00:17,  5.37it/s]Epoch: 2, train for the 152-th batch, train loss: 0.3405417501926422:  63%|██████▉    | 152/241 [00:23<00:17,  5.23it/s]Epoch: 4, train for the 81-th batch, train loss: 0.4297907054424286:  67%|████████▋    | 80/119 [00:11<00:05,  7.14it/s]Epoch: 3, train for the 57-th batch, train loss: 0.5920371413230896:  38%|████▉        | 57/151 [00:09<00:17,  5.23it/s]Epoch: 4, train for the 81-th batch, train loss: 0.4297907054424286:  68%|████████▊    | 81/119 [00:11<00:05,  7.44it/s]Epoch: 2, train for the 58-th batch, train loss: 0.6157507300376892:  24%|███▏         | 57/237 [00:08<00:29,  6.16it/s]Epoch: 2, train for the 58-th batch, train loss: 0.6157507300376892:  24%|███▏         | 58/237 [00:08<00:28,  6.30it/s]Epoch: 1, train for the 301-th batch, train loss: 0.32474344968795776:  78%|███████▊  | 300/383 [01:24<00:31,  2.66it/s]Epoch: 4, train for the 82-th batch, train loss: 0.476284921169281:  68%|█████████▌    | 81/119 [00:12<00:05,  7.44it/s]Epoch: 4, train for the 82-th batch, train loss: 0.476284921169281:  69%|█████████▋    | 82/119 [00:12<00:04,  7.87it/s]Epoch: 2, train for the 153-th batch, train loss: 0.43596595525741577:  63%|██████▎   | 152/241 [00:23<00:17,  5.23it/s]Epoch: 3, train for the 58-th batch, train loss: 0.5848140716552734:  38%|████▉        | 57/151 [00:09<00:17,  5.23it/s]Epoch: 2, train for the 153-th batch, train loss: 0.43596595525741577:  63%|██████▎   | 153/241 [00:23<00:16,  5.41it/s]Epoch: 1, train for the 301-th batch, train loss: 0.32474344968795776:  79%|███████▊  | 301/383 [01:24<00:32,  2.52it/s]Epoch: 3, train for the 58-th batch, train loss: 0.5848140716552734:  38%|████▉        | 58/151 [00:09<00:16,  5.48it/s]evaluate for the 37-th batch, evaluate loss: 0.42840656638145447:  92%|███████████████▋ | 35/38 [00:02<00:00, 12.44it/s]evaluate for the 37-th batch, evaluate loss: 0.42840656638145447:  97%|████████████████▌| 37/38 [00:02<00:00,  9.97it/s]Epoch: 2, train for the 59-th batch, train loss: 0.6180776357650757:  24%|███▏         | 58/237 [00:08<00:28,  6.30it/s]Epoch: 2, train for the 59-th batch, train loss: 0.6180776357650757:  25%|███▏         | 59/237 [00:08<00:27,  6.54it/s]Epoch: 4, train for the 83-th batch, train loss: 0.48881906270980835:  69%|████████▎   | 82/119 [00:12<00:04,  7.87it/s]Epoch: 4, train for the 83-th batch, train loss: 0.48881906270980835:  70%|████████▎   | 83/119 [00:12<00:04,  7.80it/s]evaluate for the 38-th batch, evaluate loss: 0.4625215232372284:  97%|█████████████████▌| 37/38 [00:03<00:00,  9.97it/s]evaluate for the 38-th batch, evaluate loss: 0.4625215232372284: 100%|██████████████████| 38/38 [00:03<00:00, 12.54it/s]
Epoch: 2, train for the 154-th batch, train loss: 0.3479454815387726:  63%|██████▉    | 153/241 [00:23<00:16,  5.41it/s]Epoch: 2, train for the 154-th batch, train loss: 0.3479454815387726:  64%|███████    | 154/241 [00:23<00:15,  5.74it/s]Epoch: 3, train for the 59-th batch, train loss: 0.5694531798362732:  38%|████▉        | 58/151 [00:09<00:16,  5.48it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5365003347396851:  25%|███▏         | 59/237 [00:08<00:27,  6.54it/s]Epoch: 3, train for the 59-th batch, train loss: 0.5694531798362732:  39%|█████        | 59/151 [00:09<00:16,  5.44it/s]Epoch: 4, train for the 84-th batch, train loss: 0.4810035526752472:  70%|█████████    | 83/119 [00:12<00:04,  7.80it/s]Epoch: 4, train for the 84-th batch, train loss: 0.4810035526752472:  71%|█████████▏   | 84/119 [00:12<00:04,  7.82it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5365003347396851:  25%|███▎         | 60/237 [00:08<00:28,  6.22it/s]Epoch: 1, train for the 302-th batch, train loss: 0.3443494439125061:  79%|████████▋  | 301/383 [01:24<00:32,  2.52it/s]Epoch: 2, train for the 155-th batch, train loss: 0.3945944607257843:  64%|███████    | 154/241 [00:23<00:15,  5.74it/s]Epoch: 2, train for the 155-th batch, train loss: 0.3945944607257843:  64%|███████    | 155/241 [00:23<00:13,  6.16it/s]Epoch: 4, train for the 85-th batch, train loss: 0.4638270139694214:  71%|█████████▏   | 84/119 [00:12<00:04,  7.82it/s]Epoch: 3, train for the 60-th batch, train loss: 0.5717186331748962:  39%|█████        | 59/151 [00:09<00:16,  5.44it/s]Epoch: 4, train for the 85-th batch, train loss: 0.4638270139694214:  71%|█████████▎   | 85/119 [00:12<00:04,  8.27it/s]Epoch: 1, train for the 302-th batch, train loss: 0.3443494439125061:  79%|████████▋  | 302/383 [01:24<00:30,  2.66it/s]Epoch: 3, train for the 60-th batch, train loss: 0.5717186331748962:  40%|█████▏       | 60/151 [00:09<00:15,  5.91it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5758841633796692:  25%|███▎         | 60/237 [00:09<00:28,  6.22it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5758841633796692:  26%|███▎         | 61/237 [00:09<00:28,  6.24it/s]  0%|                                                                                            | 0/20 [00:00<?, ?it/s]Epoch: 3, train for the 61-th batch, train loss: 0.5532821416854858:  40%|█████▏       | 60/151 [00:09<00:15,  5.91it/s]evaluate for the 1-th batch, evaluate loss: 0.7581588625907898:   0%|                            | 0/20 [00:00<?, ?it/s]Epoch: 3, train for the 61-th batch, train loss: 0.5532821416854858:  40%|█████▎       | 61/151 [00:09<00:14,  6.27it/s]Epoch: 4, train for the 86-th batch, train loss: 0.4854070246219635:  71%|█████████▎   | 85/119 [00:12<00:04,  8.27it/s]Epoch: 4, train for the 86-th batch, train loss: 0.4854070246219635:  72%|█████████▍   | 86/119 [00:12<00:04,  7.48it/s]evaluate for the 2-th batch, evaluate loss: 0.7781121730804443:   0%|                            | 0/20 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.7781121730804443:  10%|██                  | 2/20 [00:00<00:01, 14.53it/s]Epoch: 2, train for the 156-th batch, train loss: 0.27312108874320984:  64%|██████▍   | 155/241 [00:23<00:13,  6.16it/s]Epoch: 2, train for the 156-th batch, train loss: 0.27312108874320984:  65%|██████▍   | 156/241 [00:23<00:16,  5.10it/s]evaluate for the 3-th batch, evaluate loss: 0.6422799825668335:  10%|██                  | 2/20 [00:00<00:01, 14.53it/s]Epoch: 3, train for the 62-th batch, train loss: 0.6061137914657593:  40%|█████▎       | 61/151 [00:09<00:14,  6.27it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5670934319496155:  26%|███▎         | 61/237 [00:09<00:28,  6.24it/s]Epoch: 1, train for the 303-th batch, train loss: 0.39758339524269104:  79%|███████▉  | 302/383 [01:24<00:30,  2.66it/s]Epoch: 3, train for the 62-th batch, train loss: 0.6061137914657593:  41%|█████▎       | 62/151 [00:09<00:14,  6.22it/s]Epoch: 4, train for the 87-th batch, train loss: 0.4923454821109772:  72%|█████████▍   | 86/119 [00:12<00:04,  7.48it/s]Epoch: 4, train for the 87-th batch, train loss: 0.4923454821109772:  73%|█████████▌   | 87/119 [00:12<00:04,  7.03it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5670934319496155:  26%|███▍         | 62/237 [00:09<00:33,  5.16it/s]evaluate for the 4-th batch, evaluate loss: 0.6661568880081177:  10%|██                  | 2/20 [00:00<00:01, 14.53it/s]evaluate for the 4-th batch, evaluate loss: 0.6661568880081177:  20%|████                | 4/20 [00:00<00:01, 14.63it/s]Epoch: 2, train for the 157-th batch, train loss: 0.25593090057373047:  65%|██████▍   | 156/241 [00:23<00:16,  5.10it/s]Epoch: 1, train for the 303-th batch, train loss: 0.39758339524269104:  79%|███████▉  | 303/383 [01:24<00:29,  2.67it/s]Epoch: 2, train for the 157-th batch, train loss: 0.25593090057373047:  65%|██████▌   | 157/241 [00:24<00:15,  5.55it/s]evaluate for the 5-th batch, evaluate loss: 0.6778591275215149:  20%|████                | 4/20 [00:00<00:01, 14.63it/s]Epoch: 4, train for the 88-th batch, train loss: 0.5056887865066528:  73%|█████████▌   | 87/119 [00:12<00:04,  7.03it/s]Epoch: 4, train for the 88-th batch, train loss: 0.5056887865066528:  74%|█████████▌   | 88/119 [00:12<00:04,  7.04it/s]Epoch: 3, train for the 63-th batch, train loss: 0.5759111046791077:  41%|█████▎       | 62/151 [00:10<00:14,  6.22it/s]Epoch: 2, train for the 63-th batch, train loss: 0.6023382544517517:  26%|███▍         | 62/237 [00:09<00:33,  5.16it/s]evaluate for the 6-th batch, evaluate loss: 0.7435463070869446:  20%|████                | 4/20 [00:00<00:01, 14.63it/s]evaluate for the 6-th batch, evaluate loss: 0.7435463070869446:  30%|██████              | 6/20 [00:00<00:01, 13.68it/s]Epoch: 3, train for the 63-th batch, train loss: 0.5759111046791077:  42%|█████▍       | 63/151 [00:10<00:15,  5.85it/s]Epoch: 2, train for the 158-th batch, train loss: 0.28664472699165344:  65%|██████▌   | 157/241 [00:24<00:15,  5.55it/s]Epoch: 2, train for the 63-th batch, train loss: 0.6023382544517517:  27%|███▍         | 63/237 [00:09<00:33,  5.17it/s]Epoch: 2, train for the 158-th batch, train loss: 0.28664472699165344:  66%|██████▌   | 158/241 [00:24<00:14,  5.68it/s]evaluate for the 7-th batch, evaluate loss: 0.7586250901222229:  30%|██████              | 6/20 [00:00<00:01, 13.68it/s]Epoch: 4, train for the 89-th batch, train loss: 0.493791788816452:  74%|██████████▎   | 88/119 [00:13<00:04,  7.04it/s]evaluate for the 8-th batch, evaluate loss: 0.6859025359153748:  30%|██████              | 6/20 [00:00<00:01, 13.68it/s]evaluate for the 8-th batch, evaluate loss: 0.6859025359153748:  40%|████████            | 8/20 [00:00<00:00, 14.20it/s]Epoch: 4, train for the 89-th batch, train loss: 0.493791788816452:  75%|██████████▍   | 89/119 [00:13<00:04,  6.70it/s]Epoch: 3, train for the 64-th batch, train loss: 0.5611578822135925:  42%|█████▍       | 63/151 [00:10<00:15,  5.85it/s]Epoch: 3, train for the 64-th batch, train loss: 0.5611578822135925:  42%|█████▌       | 64/151 [00:10<00:14,  5.89it/s]Epoch: 1, train for the 304-th batch, train loss: 0.4301636517047882:  79%|████████▋  | 303/383 [01:25<00:29,  2.67it/s]Epoch: 2, train for the 159-th batch, train loss: 0.2520017921924591:  66%|███████▏   | 158/241 [00:24<00:14,  5.68it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5767071843147278:  27%|███▍         | 63/237 [00:09<00:33,  5.17it/s]evaluate for the 9-th batch, evaluate loss: 0.6441683769226074:  40%|████████            | 8/20 [00:00<00:00, 14.20it/s]Epoch: 2, train for the 159-th batch, train loss: 0.2520017921924591:  66%|███████▎   | 159/241 [00:24<00:14,  5.78it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5767071843147278:  27%|███▌         | 64/237 [00:09<00:34,  5.04it/s]Epoch: 1, train for the 304-th batch, train loss: 0.4301636517047882:  79%|████████▋  | 304/383 [01:25<00:29,  2.65it/s]Epoch: 4, train for the 90-th batch, train loss: 0.4855045974254608:  75%|█████████▋   | 89/119 [00:13<00:04,  6.70it/s]Epoch: 4, train for the 90-th batch, train loss: 0.4855045974254608:  76%|█████████▊   | 90/119 [00:13<00:04,  6.79it/s]evaluate for the 10-th batch, evaluate loss: 0.6397775411605835:  40%|███████▌           | 8/20 [00:00<00:00, 14.20it/s]evaluate for the 10-th batch, evaluate loss: 0.6397775411605835:  50%|█████████         | 10/20 [00:00<00:00, 13.90it/s]Epoch: 3, train for the 65-th batch, train loss: 0.5248706340789795:  42%|█████▌       | 64/151 [00:10<00:14,  5.89it/s]Epoch: 3, train for the 65-th batch, train loss: 0.5248706340789795:  43%|█████▌       | 65/151 [00:10<00:14,  6.04it/s]Epoch: 2, train for the 160-th batch, train loss: 0.5478062033653259:  66%|███████▎   | 159/241 [00:24<00:14,  5.78it/s]evaluate for the 11-th batch, evaluate loss: 0.6386928558349609:  50%|█████████         | 10/20 [00:00<00:00, 13.90it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5441724061965942:  27%|███▌         | 64/237 [00:09<00:34,  5.04it/s]Epoch: 2, train for the 160-th batch, train loss: 0.5478062033653259:  66%|███████▎   | 160/241 [00:24<00:13,  5.83it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5441724061965942:  27%|███▌         | 65/237 [00:09<00:32,  5.28it/s]Epoch: 4, train for the 91-th batch, train loss: 0.43320024013519287:  76%|█████████   | 90/119 [00:13<00:04,  6.79it/s]Epoch: 4, train for the 91-th batch, train loss: 0.43320024013519287:  76%|█████████▏  | 91/119 [00:13<00:04,  6.60it/s]evaluate for the 12-th batch, evaluate loss: 0.6946438550949097:  50%|█████████         | 10/20 [00:00<00:00, 13.90it/s]evaluate for the 12-th batch, evaluate loss: 0.6946438550949097:  60%|██████████▊       | 12/20 [00:00<00:00, 13.48it/s]Epoch: 3, train for the 66-th batch, train loss: 0.5623283982276917:  43%|█████▌       | 65/151 [00:10<00:14,  6.04it/s]Epoch: 3, train for the 66-th batch, train loss: 0.5623283982276917:  44%|█████▋       | 66/151 [00:10<00:14,  6.02it/s]evaluate for the 13-th batch, evaluate loss: 0.7128588557243347:  60%|██████████▊       | 12/20 [00:00<00:00, 13.48it/s]Epoch: 2, train for the 66-th batch, train loss: 0.5581912994384766:  27%|███▌         | 65/237 [00:10<00:32,  5.28it/s]Epoch: 2, train for the 161-th batch, train loss: 0.3318207859992981:  66%|███████▎   | 160/241 [00:24<00:13,  5.83it/s]Epoch: 2, train for the 161-th batch, train loss: 0.3318207859992981:  67%|███████▎   | 161/241 [00:24<00:13,  5.76it/s]Epoch: 2, train for the 66-th batch, train loss: 0.5581912994384766:  28%|███▌         | 66/237 [00:10<00:31,  5.48it/s]Epoch: 4, train for the 92-th batch, train loss: 0.4651455283164978:  76%|█████████▉   | 91/119 [00:13<00:04,  6.60it/s]Epoch: 4, train for the 92-th batch, train loss: 0.4651455283164978:  77%|██████████   | 92/119 [00:13<00:04,  6.66it/s]evaluate for the 14-th batch, evaluate loss: 0.7255194187164307:  60%|██████████▊       | 12/20 [00:01<00:00, 13.48it/s]evaluate for the 14-th batch, evaluate loss: 0.7255194187164307:  70%|████████████▌     | 14/20 [00:01<00:00, 13.47it/s]Epoch: 3, train for the 67-th batch, train loss: 0.5661746859550476:  44%|█████▋       | 66/151 [00:10<00:14,  6.02it/s]Epoch: 1, train for the 305-th batch, train loss: 0.5167338252067566:  79%|████████▋  | 304/383 [01:25<00:29,  2.65it/s]Epoch: 3, train for the 67-th batch, train loss: 0.5661746859550476:  44%|█████▊       | 67/151 [00:10<00:13,  6.19it/s]evaluate for the 15-th batch, evaluate loss: 0.7215598821640015:  70%|████████████▌     | 14/20 [00:01<00:00, 13.47it/s]Epoch: 2, train for the 162-th batch, train loss: 0.2088650017976761:  67%|███████▎   | 161/241 [00:24<00:13,  5.76it/s]Epoch: 2, train for the 162-th batch, train loss: 0.2088650017976761:  67%|███████▍   | 162/241 [00:24<00:12,  6.19it/s]Epoch: 1, train for the 305-th batch, train loss: 0.5167338252067566:  80%|████████▊  | 305/383 [01:25<00:30,  2.53it/s]Epoch: 2, train for the 67-th batch, train loss: 0.510882556438446:  28%|███▉          | 66/237 [00:10<00:31,  5.48it/s]Epoch: 2, train for the 67-th batch, train loss: 0.510882556438446:  28%|███▉          | 67/237 [00:10<00:30,  5.63it/s]Epoch: 4, train for the 93-th batch, train loss: 0.420205682516098:  77%|██████████▊   | 92/119 [00:13<00:04,  6.66it/s]evaluate for the 16-th batch, evaluate loss: 0.6691808104515076:  70%|████████████▌     | 14/20 [00:01<00:00, 13.47it/s]evaluate for the 16-th batch, evaluate loss: 0.6691808104515076:  80%|██████████████▍   | 16/20 [00:01<00:00, 12.96it/s]Epoch: 4, train for the 93-th batch, train loss: 0.420205682516098:  78%|██████████▉   | 93/119 [00:13<00:04,  6.34it/s]Epoch: 3, train for the 68-th batch, train loss: 0.5524830222129822:  44%|█████▊       | 67/151 [00:10<00:13,  6.19it/s]Epoch: 3, train for the 68-th batch, train loss: 0.5524830222129822:  45%|█████▊       | 68/151 [00:10<00:13,  6.30it/s]evaluate for the 17-th batch, evaluate loss: 0.7158437967300415:  80%|██████████████▍   | 16/20 [00:01<00:00, 12.96it/s]Epoch: 2, train for the 163-th batch, train loss: 0.17277498543262482:  67%|██████▋   | 162/241 [00:24<00:12,  6.19it/s]Epoch: 2, train for the 163-th batch, train loss: 0.17277498543262482:  68%|██████▊   | 163/241 [00:24<00:12,  6.35it/s]Epoch: 2, train for the 68-th batch, train loss: 0.5051898956298828:  28%|███▋         | 67/237 [00:10<00:30,  5.63it/s]Epoch: 2, train for the 68-th batch, train loss: 0.5051898956298828:  29%|███▋         | 68/237 [00:10<00:28,  5.94it/s]evaluate for the 18-th batch, evaluate loss: 0.6812471151351929:  80%|██████████████▍   | 16/20 [00:01<00:00, 12.96it/s]evaluate for the 18-th batch, evaluate loss: 0.6812471151351929:  90%|████████████████▏ | 18/20 [00:01<00:00, 12.73it/s]Epoch: 4, train for the 94-th batch, train loss: 0.4531528353691101:  78%|██████████▏  | 93/119 [00:13<00:04,  6.34it/s]Epoch: 4, train for the 94-th batch, train loss: 0.4531528353691101:  79%|██████████▎  | 94/119 [00:13<00:04,  6.05it/s]Epoch: 3, train for the 69-th batch, train loss: 0.5537518262863159:  45%|█████▊       | 68/151 [00:11<00:13,  6.30it/s]Epoch: 3, train for the 69-th batch, train loss: 0.5537518262863159:  46%|█████▉       | 69/151 [00:11<00:13,  6.23it/s]Epoch: 1, train for the 306-th batch, train loss: 0.355823814868927:  80%|█████████▌  | 305/383 [01:26<00:30,  2.53it/s]Epoch: 2, train for the 164-th batch, train loss: 0.16852961480617523:  68%|██████▊   | 163/241 [00:25<00:12,  6.35it/s]evaluate for the 19-th batch, evaluate loss: 0.7402421236038208:  90%|████████████████▏ | 18/20 [00:01<00:00, 12.73it/s]Epoch: 2, train for the 164-th batch, train loss: 0.16852961480617523:  68%|██████▊   | 164/241 [00:25<00:12,  6.38it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5408304929733276:  29%|███▋         | 68/237 [00:10<00:28,  5.94it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5408304929733276:  29%|███▊         | 69/237 [00:10<00:27,  6.09it/s]Epoch: 1, train for the 306-th batch, train loss: 0.355823814868927:  80%|█████████▌  | 306/383 [01:26<00:29,  2.61it/s]evaluate for the 20-th batch, evaluate loss: 0.7217866778373718:  90%|████████████████▏ | 18/20 [00:01<00:00, 12.73it/s]evaluate for the 20-th batch, evaluate loss: 0.7217866778373718: 100%|██████████████████| 20/20 [00:01<00:00, 13.11it/s]evaluate for the 20-th batch, evaluate loss: 0.7217866778373718: 100%|██████████████████| 20/20 [00:01<00:00, 13.41it/s]
Epoch: 4, train for the 95-th batch, train loss: 0.40194475650787354:  79%|█████████▍  | 94/119 [00:14<00:04,  6.05it/s]Epoch: 4, train for the 95-th batch, train loss: 0.40194475650787354:  80%|█████████▌  | 95/119 [00:14<00:03,  6.24it/s]Epoch: 3, train for the 70-th batch, train loss: 0.5645229816436768:  46%|█████▉       | 69/151 [00:11<00:13,  6.23it/s]Epoch: 2, train for the 165-th batch, train loss: 0.26539453864097595:  68%|██████▊   | 164/241 [00:25<00:12,  6.38it/s]Epoch: 3, train for the 70-th batch, train loss: 0.5645229816436768:  46%|██████       | 70/151 [00:11<00:13,  6.21it/s]Epoch: 2, train for the 165-th batch, train loss: 0.26539453864097595:  68%|██████▊   | 165/241 [00:25<00:11,  6.61it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5463013052940369:  29%|███▊         | 69/237 [00:10<00:27,  6.09it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5463013052940369:  30%|███▊         | 70/237 [00:10<00:27,  5.99it/s]INFO:root:Epoch: 3, learning rate: 0.0001, train loss: 0.5422
INFO:root:train average_precision, 0.8312
INFO:root:train roc_auc, 0.8162
INFO:root:validate loss: 0.4735
INFO:root:validate average_precision, 0.8610
INFO:root:validate roc_auc, 0.8538
INFO:root:new node validate loss: 0.7008
INFO:root:new node validate first_1_average_precision, 0.5696
INFO:root:new node validate first_1_roc_auc, 0.5172
INFO:root:new node validate first_3_average_precision, 0.6080
INFO:root:new node validate first_3_roc_auc, 0.5694
INFO:root:new node validate first_10_average_precision, 0.6326
INFO:root:new node validate first_10_roc_auc, 0.6192
INFO:root:new node validate average_precision, 0.6784
INFO:root:new node validate roc_auc, 0.6733
INFO:root:save model ./saved_models/TGN/ia-escorts-dynamic/TGN_seed0_dummy-55wl6je1/TGN_seed0_dummy-55wl6je1.pkl
Epoch: 4, train for the 96-th batch, train loss: 0.4468814432621002:  80%|██████████▍  | 95/119 [00:14<00:03,  6.24it/s]Epoch: 4, train for the 96-th batch, train loss: 0.4468814432621002:  81%|██████████▍  | 96/119 [00:14<00:03,  6.22it/s]Epoch: 2, train for the 166-th batch, train loss: 0.2523685097694397:  68%|███████▌   | 165/241 [00:25<00:11,  6.61it/s]Epoch: 3, train for the 71-th batch, train loss: 0.5881144404411316:  46%|██████       | 70/151 [00:11<00:13,  6.21it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5430227518081665:  30%|███▊         | 70/237 [00:10<00:27,  5.99it/s]Epoch: 4, train for the 97-th batch, train loss: 0.4627925753593445:  81%|██████████▍  | 96/119 [00:14<00:03,  6.22it/s]Epoch: 3, train for the 71-th batch, train loss: 0.5881144404411316:  47%|██████       | 71/151 [00:11<00:14,  5.65it/s]Epoch: 2, train for the 166-th batch, train loss: 0.2523685097694397:  69%|███████▌   | 166/241 [00:25<00:12,  5.98it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5430227518081665:  30%|███▉         | 71/237 [00:10<00:26,  6.30it/s]Epoch: 4, train for the 98-th batch, train loss: 0.43851760029792786:  81%|█████████▋  | 96/119 [00:14<00:03,  6.22it/s]Epoch: 4, train for the 98-th batch, train loss: 0.43851760029792786:  82%|█████████▉  | 98/119 [00:14<00:02,  7.53it/s]Epoch: 1, train for the 307-th batch, train loss: 0.35199013352394104:  80%|███████▉  | 306/383 [01:26<00:29,  2.61it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5559120774269104:  30%|███▉         | 71/237 [00:11<00:26,  6.30it/s]Epoch: 2, train for the 167-th batch, train loss: 0.17203623056411743:  69%|██████▉   | 166/241 [00:25<00:12,  5.98it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5559120774269104:  30%|███▉         | 72/237 [00:11<00:26,  6.20it/s]Epoch: 3, train for the 72-th batch, train loss: 0.5101786851882935:  47%|██████       | 71/151 [00:11<00:14,  5.65it/s]Epoch: 2, train for the 167-th batch, train loss: 0.17203623056411743:  69%|██████▉   | 167/241 [00:25<00:12,  5.75it/s]Epoch: 1, train for the 307-th batch, train loss: 0.35199013352394104:  80%|████████  | 307/383 [01:26<00:31,  2.42it/s]Epoch: 4, train for the 99-th batch, train loss: 0.43999403715133667:  82%|█████████▉  | 98/119 [00:14<00:02,  7.53it/s]Epoch: 3, train for the 72-th batch, train loss: 0.5101786851882935:  48%|██████▏      | 72/151 [00:11<00:14,  5.41it/s]Epoch: 4, train for the 100-th batch, train loss: 0.47270575165748596:  82%|█████████  | 98/119 [00:14<00:02,  7.53it/s]Epoch: 4, train for the 100-th batch, train loss: 0.47270575165748596:  84%|████████▍ | 100/119 [00:14<00:02,  8.27it/s]Epoch: 2, train for the 168-th batch, train loss: 0.14424780011177063:  69%|██████▉   | 167/241 [00:25<00:12,  5.75it/s]Epoch: 2, train for the 73-th batch, train loss: 0.5160836577415466:  30%|███▉         | 72/237 [00:11<00:26,  6.20it/s]Epoch: 2, train for the 73-th batch, train loss: 0.5160836577415466:  31%|████         | 73/237 [00:11<00:27,  6.02it/s]Epoch: 2, train for the 168-th batch, train loss: 0.14424780011177063:  70%|██████▉   | 168/241 [00:25<00:12,  5.85it/s]Epoch: 3, train for the 73-th batch, train loss: 0.5578495264053345:  48%|██████▏      | 72/151 [00:11<00:14,  5.41it/s]Epoch: 3, train for the 73-th batch, train loss: 0.5578495264053345:  48%|██████▎      | 73/151 [00:11<00:14,  5.45it/s]Epoch: 4, train for the 101-th batch, train loss: 0.45593950152397156:  84%|████████▍ | 100/119 [00:14<00:02,  8.27it/s]Epoch: 4, train for the 101-th batch, train loss: 0.45593950152397156:  85%|████████▍ | 101/119 [00:14<00:02,  8.56it/s]Epoch: 1, train for the 308-th batch, train loss: 0.41577085852622986:  80%|████████  | 307/383 [01:26<00:31,  2.42it/s]Epoch: 2, train for the 74-th batch, train loss: 0.5204188227653503:  31%|████         | 73/237 [00:11<00:27,  6.02it/s]Epoch: 2, train for the 169-th batch, train loss: 0.5854713916778564:  70%|███████▋   | 168/241 [00:25<00:12,  5.85it/s]Epoch: 4, train for the 102-th batch, train loss: 0.48316535353660583:  85%|████████▍ | 101/119 [00:14<00:02,  8.56it/s]Epoch: 2, train for the 74-th batch, train loss: 0.5204188227653503:  31%|████         | 74/237 [00:11<00:27,  6.04it/s]Epoch: 4, train for the 102-th batch, train loss: 0.48316535353660583:  86%|████████▌ | 102/119 [00:14<00:01,  8.86it/s]Epoch: 3, train for the 74-th batch, train loss: 0.499671071767807:  48%|██████▊       | 73/151 [00:12<00:14,  5.45it/s]Epoch: 2, train for the 169-th batch, train loss: 0.5854713916778564:  70%|███████▋   | 169/241 [00:26<00:12,  5.68it/s]Epoch: 1, train for the 308-th batch, train loss: 0.41577085852622986:  80%|████████  | 308/383 [01:27<00:29,  2.54it/s]Epoch: 3, train for the 74-th batch, train loss: 0.499671071767807:  49%|██████▊       | 74/151 [00:12<00:13,  5.51it/s]Epoch: 4, train for the 103-th batch, train loss: 0.44901564717292786:  86%|████████▌ | 102/119 [00:14<00:01,  8.86it/s]Epoch: 4, train for the 103-th batch, train loss: 0.44901564717292786:  87%|████████▋ | 103/119 [00:14<00:01,  9.10it/s]Epoch: 2, train for the 75-th batch, train loss: 0.527065098285675:  31%|████▎         | 74/237 [00:11<00:27,  6.04it/s]Epoch: 2, train for the 170-th batch, train loss: 0.6993421912193298:  70%|███████▋   | 169/241 [00:26<00:12,  5.68it/s]Epoch: 2, train for the 75-th batch, train loss: 0.527065098285675:  32%|████▍         | 75/237 [00:11<00:27,  5.98it/s]Epoch: 2, train for the 170-th batch, train loss: 0.6993421912193298:  71%|███████▊   | 170/241 [00:26<00:11,  5.94it/s]Epoch: 3, train for the 75-th batch, train loss: 0.5365890264511108:  49%|██████▎      | 74/151 [00:12<00:13,  5.51it/s]Epoch: 4, train for the 104-th batch, train loss: 0.44300684332847595:  87%|████████▋ | 103/119 [00:15<00:01,  9.10it/s]Epoch: 3, train for the 75-th batch, train loss: 0.5365890264511108:  50%|██████▍      | 75/151 [00:12<00:13,  5.62it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]Epoch: 1, train for the 309-th batch, train loss: 0.41382312774658203:  80%|████████  | 308/383 [01:27<00:29,  2.54it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5651605129241943:  32%|████         | 75/237 [00:11<00:27,  5.98it/s]Epoch: 2, train for the 171-th batch, train loss: 0.5910019278526306:  71%|███████▊   | 170/241 [00:26<00:11,  5.94it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5651605129241943:  32%|████▏        | 76/237 [00:11<00:26,  6.09it/s]True
  0%|          | 0/95577 [00:00<?, ?it/s]True
  0%|          | 0/140777 [00:00<?, ?it/s]True
  0%|          | 0/50631 [00:00<?, ?it/s]True
  0%|          | 0/87626 [00:00<?, ?it/s]  8%|▊         | 7600/95577 [00:00<00:01, 75992.10it/s]  3%|▎         | 4906/140777 [00:00<00:02, 49057.12it/s] 10%|▉         | 5007/50631 [00:00<00:00, 50068.38it/s]  8%|▊         | 7103/87626 [00:00<00:01, 71026.68it/s] 17%|█▋        | 16223/95577 [00:00<00:00, 82013.49it/s]  9%|▉         | 13130/140777 [00:00<00:01, 68575.84it/s] 21%|██        | 10402/50631 [00:00<00:00, 46556.89it/s] 15%|█▌        | 21683/140777 [00:00<00:01, 76313.81it/s] 16%|█▌        | 14206/87626 [00:00<00:01, 45083.96it/s]True
  0%|          | 0/52049 [00:00<?, ?it/s]True
  0%|          | 0/61156 [00:00<?, ?it/s] 31%|███       | 15720/50631 [00:00<00:00, 45627.54it/s] 22%|██▏       | 19243/87626 [00:00<00:01, 46485.21it/s]  9%|▉         | 5584/61156 [00:00<00:00, 55837.26it/s] 11%|█         | 5698/52049 [00:00<00:00, 48635.99it/s] 21%|██        | 29315/140777 [00:00<00:01, 60433.70it/s] 43%|████▎     | 21558/50631 [00:00<00:00, 45439.75it/s] 26%|██▌       | 24425/95577 [00:00<00:01, 40296.32it/s] 18%|█▊        | 11168/61156 [00:00<00:00, 52209.92it/s] 20%|██        | 10562/52049 [00:00<00:00, 47766.55it/s] 28%|██▊       | 24859/87626 [00:00<00:01, 43671.96it/s] 54%|█████▍    | 27254/50631 [00:00<00:00, 49101.76it/s] 31%|███▏      | 30028/95577 [00:00<00:01, 40877.70it/s] 29%|██▉       | 15334/52049 [00:00<00:00, 47313.86it/s] 34%|███▎      | 29436/87626 [00:00<00:01, 43758.73it/s] 27%|██▋       | 16405/61156 [00:00<00:00, 46335.08it/s] 64%|██████▎   | 32221/50631 [00:00<00:00, 44519.21it/s] 26%|██▋       | 37282/140777 [00:00<00:02, 41454.27it/s] 37%|███▋      | 35278/95577 [00:00<00:01, 41381.35it/s] 40%|███▉      | 34665/87626 [00:00<00:01, 46252.75it/s] 39%|███▊      | 20062/52049 [00:00<00:00, 44715.02it/s] 35%|███▍      | 21102/61156 [00:00<00:00, 46180.08it/s] 74%|███████▎  | 37311/50631 [00:00<00:00, 46351.37it/s] 32%|███▏      | 45707/140777 [00:00<00:01, 50696.54it/s] 43%|████▎     | 41032/95577 [00:00<00:01, 45380.28it/s] 42%|████▏     | 25758/61156 [00:00<00:00, 45234.30it/s] 45%|████▍     | 39421/87626 [00:00<00:01, 43776.76it/s] 48%|████▊     | 24902/52049 [00:00<00:00, 42655.24it/s] 83%|████████▎ | 42270/50631 [00:00<00:00, 47293.37it/s] 49%|████▉     | 47246/95577 [00:00<00:00, 49757.91it/s] 50%|████▉     | 30303/61156 [00:00<00:00, 44253.89it/s] 57%|█████▋    | 29543/52049 [00:00<00:00, 42463.71it/s] 51%|█████     | 44832/87626 [00:01<00:01, 40919.65it/s] 93%|█████████▎| 47064/50631 [00:01<00:00, 43502.52it/s] 37%|███▋      | 52126/140777 [00:01<00:02, 42237.15it/s] 57%|█████▋    | 34742/61156 [00:00<00:00, 42002.80it/s] 66%|██████▋   | 34528/52049 [00:00<00:00, 42922.68it/s] 55%|█████▌    | 52715/95577 [00:01<00:00, 43652.65it/s]100%|█████████▉| 50628/50631 [00:01<00:00, 45310.49it/s]
 57%|█████▋    | 49647/87626 [00:01<00:00, 42811.86it/s] 43%|████▎     | 60602/140777 [00:01<00:01, 51119.25it/s] 65%|██████▍   | 39601/61156 [00:00<00:00, 43571.70it/s] 75%|███████▍  | 38944/52049 [00:00<00:00, 42696.41it/s] 60%|██████    | 57756/95577 [00:01<00:00, 45334.84it/s] 62%|██████▏   | 54461/87626 [00:01<00:00, 44261.63it/s]  0%|          | 0/50631 [00:00<?, ?it/s] 49%|████▉     | 69141/140777 [00:01<00:01, 58994.59it/s]100%|██████████| 50631/50631 [00:00<00:00, 1888298.32it/s]
 72%|███████▏  | 43981/61156 [00:00<00:00, 43077.00it/s] 84%|████████▎ | 43553/52049 [00:00<00:00, 43685.91it/s] 66%|██████▌   | 62630/95577 [00:01<00:00, 45051.85it/s] 67%|██████▋   | 58983/87626 [00:01<00:00, 42759.53it/s] 79%|███████▉  | 48509/61156 [00:01<00:00, 43651.76it/s] 71%|███████   | 67540/95577 [00:01<00:00, 46133.96it/s] 92%|█████████▏| 47932/52049 [00:01<00:00, 41843.00it/s] 72%|███████▏  | 63327/87626 [00:01<00:00, 40886.55it/s] 54%|█████▍    | 76080/140777 [00:01<00:01, 45015.93it/s] 87%|████████▋ | 53002/61156 [00:01<00:00, 43118.20it/s]100%|█████████▉| 52048/52049 [00:01<00:00, 43067.44it/s]
 79%|███████▊  | 68860/87626 [00:01<00:00, 44841.41it/s] 76%|███████▌  | 72331/95577 [00:01<00:00, 38583.97it/s] 59%|█████▉    | 82918/140777 [00:01<00:01, 49808.46it/s] 94%|█████████▎| 57324/61156 [00:01<00:00, 42383.40it/s] 84%|████████▍ | 73422/87626 [00:01<00:00, 41053.75it/s] 81%|████████  | 76944/95577 [00:01<00:00, 37184.94it/s]100%|█████████▉| 61155/61156 [00:01<00:00, 43348.13it/s]
  0%|          | 0/52049 [00:00<?, ?it/s]100%|██████████| 52049/52049 [00:00<00:00, 2018560.43it/s]
 89%|████████▊ | 77630/87626 [00:01<00:00, 41015.83it/s] 85%|████████▌ | 81457/95577 [00:01<00:00, 37441.54it/s]  0%|          | 0/61156 [00:00<?, ?it/s] 63%|██████▎   | 89180/140777 [00:01<00:01, 35600.81it/s]100%|██████████| 61156/61156 [00:00<00:00, 1862145.77it/s]
 94%|█████████▍| 82553/87626 [00:01<00:00, 37423.79it/s] 91%|█████████▏| 87295/95577 [00:01<00:00, 41166.60it/s] 69%|██████▉   | 97583/140777 [00:02<00:00, 44291.56it/s]100%|█████████▉| 87625/87626 [00:02<00:00, 42780.42it/s]
 97%|█████████▋| 92261/95577 [00:02<00:00, 40919.70it/s]100%|█████████▉| 95576/95577 [00:02<00:00, 43608.75it/s]
 74%|███████▎  | 103474/140777 [00:02<00:01, 33189.75it/s]  0%|          | 0/87626 [00:00<?, ?it/s]100%|██████████| 87626/87626 [00:00<00:00, 1946354.30it/s]
  0%|          | 0/95577 [00:00<?, ?it/s] 79%|███████▉  | 111849/140777 [00:02<00:00, 41778.91it/s]100%|██████████| 95577/95577 [00:00<00:00, 1158295.13it/s]
 84%|████████▎ | 117649/140777 [00:02<00:00, 44523.73it/s] 88%|████████▊ | 123368/140777 [00:02<00:00, 37789.23it/s]wandb: Currently logged in as: singhayush4499 (fb-graph-proj). Use `wandb login --relogin` to force relogin
 93%|█████████▎| 130231/140777 [00:03<00:00, 29727.88it/s] 98%|█████████▊| 138593/140777 [00:03<00:00, 38400.31it/s]100%|█████████▉| 140776/140777 [00:03<00:00, 42156.47it/s]
wandb: Currently logged in as: singhayush4499 (fb-graph-proj). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: singhayush4499 (fb-graph-proj). Use `wandb login --relogin` to force relogin
  0%|          | 0/140777 [00:00<?, ?it/s]100%|██████████| 140777/140777 [00:00<00:00, 1866086.63it/s]
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/ayush/DyGLib/wandb/run-20240320_223526-wlvncoob
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tgn-ia-escorts-dynamic-lincorrect-time-linear
wandb: ⭐️ View project at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib
wandb: 🚀 View run at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib/runs/wlvncoob
INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-escorts-dynamic', batch_size=200, model_name='TGN', gpu=0, num_neighbors=10, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=1, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='lincorrect-time-linear', use_ROPe=False, t1_factor_of_t2=1, use_init_method=True, emb_proj=False, init_weights='time-linear', clip=1.0, device='cuda:0', seed=0, save_model_name='TGN_seed0_tgn-ia-escorts-dynamic-lincorrect-time-linear')
wandb: Currently logged in as: singhayush4499 (fb-graph-proj). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: singhayush4499 (fb-graph-proj). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/ayush/DyGLib/wandb/run-20240320_223527-ho3lkcfr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tgn-ia-reality-call-lincorrect-time-linear
wandb: ⭐️ View project at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib
wandb: 🚀 View run at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib/runs/ho3lkcfr
INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-reality-call', batch_size=200, model_name='TGN', gpu=0, num_neighbors=10, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=1, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='lincorrect-time-linear', use_ROPe=False, t1_factor_of_t2=1, use_init_method=True, emb_proj=False, init_weights='time-linear', clip=1.0, device='cuda:0', seed=0, save_model_name='TGN_seed0_tgn-ia-reality-call-lincorrect-time-linear')
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/ayush/DyGLib/wandb/run-20240320_223527-kw0m2rb7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tgn-ia-retweet-pol-lincorrect-time-linear
wandb: ⭐️ View project at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib
wandb: 🚀 View run at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib/runs/kw0m2rb7
INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-retweet-pol', batch_size=200, model_name='TGN', gpu=2, num_neighbors=10, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=1, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='lincorrect-time-linear', use_ROPe=False, t1_factor_of_t2=1, use_init_method=True, emb_proj=False, init_weights='time-linear', clip=1.0, device='cuda:2', seed=0, save_model_name='TGN_seed0_tgn-ia-retweet-pol-lincorrect-time-linear')
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/ayush/DyGLib/wandb/run-20240320_223528-6na6bg76
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tgn-ia-movielens-user2tags-10m-lincorrect-time-linear
wandb: ⭐️ View project at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib
wandb: 🚀 View run at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib/runs/6na6bg76
INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-movielens-user2tags-10m', batch_size=200, model_name='TGN', gpu=2, num_neighbors=10, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=1, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='lincorrect-time-linear', use_ROPe=False, t1_factor_of_t2=1, use_init_method=True, emb_proj=False, init_weights='time-linear', clip=1.0, device='cuda:2', seed=0, save_model_name='TGN_seed0_tgn-ia-movielens-user2tags-10m-lincorrect-time-linear')
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/ayush/DyGLib/wandb/run-20240320_223528-bmvvi4cy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tgn-ia-digg-reply-lincorrect-time-linear
wandb: ⭐️ View project at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib
wandb: 🚀 View run at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib/runs/bmvvi4cy
INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-digg-reply', batch_size=200, model_name='TGN', gpu=1, num_neighbors=10, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=1, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='lincorrect-time-linear', use_ROPe=False, t1_factor_of_t2=1, use_init_method=True, emb_proj=False, init_weights='time-linear', clip=1.0, device='cuda:1', seed=0, save_model_name='TGN_seed0_tgn-ia-digg-reply-lincorrect-time-linear')
wandb: Currently logged in as: singhayush4499 (fb-graph-proj). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/ayush/DyGLib/wandb/run-20240320_223529-lns5fomn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tgn-ia-slashdot-reply-dir-lincorrect-time-linear
wandb: ⭐️ View project at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib
wandb: 🚀 View run at https://wandb.ai/fb-graph-proj/fb-graph-proj-dyglib/runs/lns5fomn
INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='ia-slashdot-reply-dir', batch_size=200, model_name='TGN', gpu=1, num_neighbors=10, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=1, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='AdamW', weight_decay=0.0, patience=150, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=True, use_wandb='lincorrect-time-linear', use_ROPe=False, t1_factor_of_t2=1, use_init_method=True, emb_proj=False, init_weights='time-linear', clip=1.0, device='cuda:1', seed=0, save_model_name='TGN_seed0_tgn-ia-slashdot-reply-dir-lincorrect-time-linear')
INFO:root:model -> Sequential(
  (0): MemoryModel(
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (emb_proj): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
      (1): ReLU()
    )
    (time_transformation_for_init): TimeInitTransformLinear(
      (lin): Linear(in_features=1, out_features=1, bias=True)
    )
    (message_aggregator): MessageAggregator()
    (memory_bank): MemoryBank(num_nodes=10108, memory_dim=172)
    (memory_updater): GRUMemoryUpdater(
      (memory_bank): MemoryBank(num_nodes=10108, memory_dim=172)
      (memory_updater): GRUCell(616, 172)
    )
    (embedding_module): GraphAttentionEmbedding(
      (time_encoder): TimeEncoder(
        (w): Linear(in_features=1, out_features=100, bias=True)
      )
      (temporal_conv_layers): ModuleList(
        (0): MultiHeadAttention(
          (query_projection): Linear(in_features=272, out_features=272, bias=False)
          (key_projection): Linear(in_features=444, out_features=272, bias=False)
          (value_projection): Linear(in_features=444, out_features=272, bias=False)
          (layer_norm): LayerNorm((272,), eps=1e-05, elementwise_affine=True)
          (residual_fc): Linear(in_features=272, out_features=272, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (merge_layers): ModuleList(
        (0): MergeLayer(
          (fc1): Linear(in_features=444, out_features=172, bias=True)
          (fc2): Linear(in_features=172, out_features=172, bias=True)
          (act): ReLU()
        )
      )
    )
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: TGN, #parameters: 3974904 B, 3881.7421875 KB, 3.7907638549804688 MB.
The dataset has 50631 interactions, involving 10106 different nodes
The training dataset has 29100 interactions, involving 7154 different nodes
The validation dataset has 7596 interactions, involving 4118 different nodes
The test dataset has 7577 interactions, involving 4144 different nodes
The new node validation dataset has 3845 interactions, involving 2930 different nodes
The new node test dataset has 4829 interactions, involving 3346 different nodes
1010 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]INFO:root:model -> Sequential(
  (0): MemoryModel(
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (emb_proj): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
      (1): ReLU()
    )
    (time_transformation_for_init): TimeInitTransformLinear(
      (lin): Linear(in_features=1, out_features=1, bias=True)
    )
    (message_aggregator): MessageAggregator()
    (memory_bank): MemoryBank(num_nodes=27046, memory_dim=172)
    (memory_updater): GRUMemoryUpdater(
      (memory_bank): MemoryBank(num_nodes=27046, memory_dim=172)
      (memory_updater): GRUCell(616, 172)
    )
    (embedding_module): GraphAttentionEmbedding(
      (time_encoder): TimeEncoder(
        (w): Linear(in_features=1, out_features=100, bias=True)
      )
      (temporal_conv_layers): ModuleList(
        (0): MultiHeadAttention(
          (query_projection): Linear(in_features=272, out_features=272, bias=False)
          (key_projection): Linear(in_features=444, out_features=272, bias=False)
          (value_projection): Linear(in_features=444, out_features=272, bias=False)
          (layer_norm): LayerNorm((272,), eps=1e-05, elementwise_affine=True)
          (residual_fc): Linear(in_features=272, out_features=272, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (merge_layers): ModuleList(
        (0): MergeLayer(
          (fc1): Linear(in_features=444, out_features=172, bias=True)
          (fc2): Linear(in_features=172, out_features=172, bias=True)
          (act): ReLU()
        )
      )
    )
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: TGN, #parameters: 3974904 B, 3881.7421875 KB, 3.7907638549804688 MB.
The dataset has 52049 interactions, involving 6809 different nodes
The training dataset has 23625 interactions, involving 3838 different nodes
The validation dataset has 7807 interactions, involving 1715 different nodes
The test dataset has 7808 interactions, involving 1937 different nodes
The new node validation dataset has 4011 interactions, involving 1185 different nodes
The new node test dataset has 4611 interactions, involving 1531 different nodes
680 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]INFO:root:model -> Sequential(
  (0): MemoryModel(
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (emb_proj): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
      (1): ReLU()
    )
    (time_transformation_for_init): TimeInitTransformLinear(
      (lin): Linear(in_features=1, out_features=1, bias=True)
    )
    (message_aggregator): MessageAggregator()
    (memory_bank): MemoryBank(num_nodes=18471, memory_dim=172)
    (memory_updater): GRUMemoryUpdater(
      (memory_bank): MemoryBank(num_nodes=18471, memory_dim=172)
      (memory_updater): GRUCell(616, 172)
    )
    (embedding_module): GraphAttentionEmbedding(
      (time_encoder): TimeEncoder(
        (w): Linear(in_features=1, out_features=100, bias=True)
      )
      (temporal_conv_layers): ModuleList(
        (0): MultiHeadAttention(
          (query_projection): Linear(in_features=272, out_features=272, bias=False)
          (key_projection): Linear(in_features=444, out_features=272, bias=False)
          (value_projection): Linear(in_features=444, out_features=272, bias=False)
          (layer_norm): LayerNorm((272,), eps=1e-05, elementwise_affine=True)
          (residual_fc): Linear(in_features=272, out_features=272, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (merge_layers): ModuleList(
        (0): MergeLayer(
          (fc1): Linear(in_features=444, out_features=172, bias=True)
          (fc2): Linear(in_features=172, out_features=172, bias=True)
          (act): ReLU()
        )
      )
    )
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: TGN, #parameters: 3974904 B, 3881.7421875 KB, 3.7907638549804688 MB.
The dataset has 61156 interactions, involving 18470 different nodes
The training dataset has 30070 interactions, involving 12678 different nodes
The validation dataset has 9173 interactions, involving 5479 different nodes
The test dataset has 9174 interactions, involving 5328 different nodes
The new node validation dataset has 4957 interactions, involving 4196 different nodes
The new node test dataset has 5073 interactions, involving 4153 different nodes
1847 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]INFO:root:model -> Sequential(
  (0): MemoryModel(
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (emb_proj): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
      (1): ReLU()
    )
    (time_transformation_for_init): TimeInitTransformLinear(
      (lin): Linear(in_features=1, out_features=1, bias=True)
    )
    (message_aggregator): MessageAggregator()
    (memory_bank): MemoryBank(num_nodes=16530, memory_dim=172)
    (memory_updater): GRUMemoryUpdater(
      (memory_bank): MemoryBank(num_nodes=16530, memory_dim=172)
      (memory_updater): GRUCell(616, 172)
    )
    (embedding_module): GraphAttentionEmbedding(
      (time_encoder): TimeEncoder(
        (w): Linear(in_features=1, out_features=100, bias=True)
      )
      (temporal_conv_layers): ModuleList(
        (0): MultiHeadAttention(
          (query_projection): Linear(in_features=272, out_features=272, bias=False)
          (key_projection): Linear(in_features=444, out_features=272, bias=False)
          (value_projection): Linear(in_features=444, out_features=272, bias=False)
          (layer_norm): LayerNorm((272,), eps=1e-05, elementwise_affine=True)
          (residual_fc): Linear(in_features=272, out_features=272, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (merge_layers): ModuleList(
        (0): MergeLayer(
          (fc1): Linear(in_features=444, out_features=172, bias=True)
          (fc2): Linear(in_features=172, out_features=172, bias=True)
          (act): ReLU()
        )
      )
    )
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: TGN, #parameters: 3974904 B, 3881.7421875 KB, 3.7907638549804688 MB.
The dataset has 95577 interactions, involving 16527 different nodes
The training dataset has 48189 interactions, involving 10566 different nodes
The validation dataset has 14336 interactions, involving 5209 different nodes
The test dataset has 14337 interactions, involving 5377 different nodes
The new node validation dataset has 6745 interactions, involving 3518 different nodes
The new node test dataset has 7521 interactions, involving 3885 different nodes
1652 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/241 [00:00<?, ?it/s]INFO:root:model -> Sequential(
  (0): MemoryModel(
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (emb_proj): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
      (1): ReLU()
    )
    (time_transformation_for_init): TimeInitTransformLinear(
      (lin): Linear(in_features=1, out_features=1, bias=True)
    )
    (message_aggregator): MessageAggregator()
    (memory_bank): MemoryBank(num_nodes=30400, memory_dim=172)
    (memory_updater): GRUMemoryUpdater(
      (memory_bank): MemoryBank(num_nodes=30400, memory_dim=172)
      (memory_updater): GRUCell(616, 172)
    )
    (embedding_module): GraphAttentionEmbedding(
      (time_encoder): TimeEncoder(
        (w): Linear(in_features=1, out_features=100, bias=True)
      )
      (temporal_conv_layers): ModuleList(
        (0): MultiHeadAttention(
          (query_projection): Linear(in_features=272, out_features=272, bias=False)
          (key_projection): Linear(in_features=444, out_features=272, bias=False)
          (value_projection): Linear(in_features=444, out_features=272, bias=False)
          (layer_norm): LayerNorm((272,), eps=1e-05, elementwise_affine=True)
          (residual_fc): Linear(in_features=272, out_features=272, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (merge_layers): ModuleList(
        (0): MergeLayer(
          (fc1): Linear(in_features=444, out_features=172, bias=True)
          (fc2): Linear(in_features=172, out_features=172, bias=True)
          (act): ReLU()
        )
      )
    )
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: TGN, #parameters: 3974904 B, 3881.7421875 KB, 3.7907638549804688 MB.
The dataset has 87626 interactions, involving 30398 different nodes
The training dataset has 47297 interactions, involving 21540 different nodes
The validation dataset has 13144 interactions, involving 9241 different nodes
The test dataset has 13144 interactions, involving 9511 different nodes
The new node validation dataset has 7995 interactions, involving 7321 different nodes
The new node test dataset has 8239 interactions, involving 7732 different nodes
3039 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/237 [00:00<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6933676600456238:   0%|                       | 0/146 [00:02<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6933676600456238:   1%|               | 1/146 [00:02<06:43,  2.78s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6931648254394531:   1%|               | 1/146 [00:02<06:43,  2.78s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6932362914085388:   1%|               | 1/146 [00:02<06:43,  2.78s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6932362914085388:   2%|▎              | 3/146 [00:02<01:51,  1.28it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6933436393737793:   2%|▎              | 3/146 [00:03<01:51,  1.28it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6934496760368347:   0%|                       | 0/151 [00:02<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6934496760368347:   1%|               | 1/151 [00:02<05:59,  2.39s/it]Epoch: 1, train for the 1-th batch, train loss: 0.6951776742935181:   0%|                       | 0/119 [00:02<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6951776742935181:   1%|▏              | 1/119 [00:02<04:46,  2.43s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6877005696296692:   1%|▏              | 1/119 [00:02<04:46,  2.43s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6931430101394653:   1%|               | 1/151 [00:02<05:59,  2.39s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6931430101394653:   1%|▏              | 2/151 [00:02<02:35,  1.05s/it]INFO:root:model -> Sequential(
  (0): MemoryModel(
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (emb_proj): Sequential(
      (0): Linear(in_features=172, out_features=172, bias=True)
      (1): ReLU()
    )
    (time_transformation_for_init): TimeInitTransformLinear(
      (lin): Linear(in_features=1, out_features=1, bias=True)
    )
    (message_aggregator): MessageAggregator()
    (memory_bank): MemoryBank(num_nodes=51085, memory_dim=172)
    (memory_updater): GRUMemoryUpdater(
      (memory_bank): MemoryBank(num_nodes=51085, memory_dim=172)
      (memory_updater): GRUCell(616, 172)
    )
    (embedding_module): GraphAttentionEmbedding(
      (time_encoder): TimeEncoder(
        (w): Linear(in_features=1, out_features=100, bias=True)
      )
      (temporal_conv_layers): ModuleList(
        (0): MultiHeadAttention(
          (query_projection): Linear(in_features=272, out_features=272, bias=False)
          (key_projection): Linear(in_features=444, out_features=272, bias=False)
          (value_projection): Linear(in_features=444, out_features=272, bias=False)
          (layer_norm): LayerNorm((272,), eps=1e-05, elementwise_affine=True)
          (residual_fc): Linear(in_features=272, out_features=272, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (merge_layers): ModuleList(
        (0): MergeLayer(
          (fc1): Linear(in_features=444, out_features=172, bias=True)
          (fc2): Linear(in_features=172, out_features=172, bias=True)
          (act): ReLU()
        )
      )
    )
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: TGN, #parameters: 3974904 B, 3881.7421875 KB, 3.7907638549804688 MB.
Epoch: 1, train for the 5-th batch, train loss: 0.6927593946456909:   2%|▎              | 3/146 [00:03<01:51,  1.28it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6927593946456909:   3%|▌              | 5/146 [00:03<01:13,  1.92it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6923458576202393:   3%|▌              | 5/146 [00:03<01:13,  1.92it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6926030516624451:   3%|▌              | 5/146 [00:03<01:13,  1.92it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6926030516624451:   5%|▋              | 7/146 [00:03<00:45,  3.03it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6920763850212097:   5%|▋              | 7/146 [00:03<00:45,  3.03it/s]The dataset has 140777 interactions, involving 51083 different nodes
The training dataset has 76599 interactions, involving 34496 different nodes
The validation dataset has 21116 interactions, involving 10542 different nodes
The test dataset has 21117 interactions, involving 10424 different nodes
The new node validation dataset has 15534 interactions, involving 9790 different nodes
The new node test dataset has 16568 interactions, involving 9911 different nodes
5108 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/383 [00:00<?, ?it/s]Epoch: 1, train for the 3-th batch, train loss: 0.6925732493400574:   1%|▏              | 2/151 [00:02<02:35,  1.05s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6925732493400574:   2%|▎              | 3/151 [00:02<01:49,  1.35it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6929596066474915:   0%|                       | 0/241 [00:02<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6929596066474915:   0%|               | 1/241 [00:02<09:31,  2.38s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6864984631538391:   1%|▏              | 1/119 [00:02<04:46,  2.43s/it]Epoch: 1, train for the 9-th batch, train loss: 0.6919276118278503:   5%|▋              | 7/146 [00:03<00:45,  3.03it/s]Epoch: 1, train for the 3-th batch, train loss: 0.6864984631538391:   3%|▍              | 3/119 [00:02<01:35,  1.22it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6919276118278503:   6%|▉              | 9/146 [00:03<00:34,  4.00it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6931124925613403:   2%|▎              | 3/151 [00:02<01:49,  1.35it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6931124925613403:   3%|▍              | 4/151 [00:02<01:13,  2.01it/s]Epoch: 1, train for the 2-th batch, train loss: 0.6955755352973938:   0%|               | 1/241 [00:02<09:31,  2.38s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6955755352973938:   1%|               | 2/241 [00:02<04:10,  1.05s/it]Epoch: 1, train for the 10-th batch, train loss: 0.6921049356460571:   6%|▊             | 9/146 [00:04<00:34,  4.00it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6921049356460571:   7%|▉            | 10/146 [00:04<00:30,  4.41it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6785052418708801:   3%|▍              | 3/119 [00:03<01:35,  1.22it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6785052418708801:   3%|▌              | 4/119 [00:03<01:08,  1.69it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6923882961273193:   3%|▍              | 4/151 [00:03<01:13,  2.01it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6923882961273193:   3%|▍              | 5/151 [00:03<00:52,  2.80it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6748557686805725:   3%|▌              | 4/119 [00:03<01:08,  1.69it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6932621598243713:   0%|                       | 0/237 [00:02<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6932621598243713:   0%|               | 1/237 [00:02<09:18,  2.37s/it]Epoch: 1, train for the 11-th batch, train loss: 0.6925178170204163:   7%|▉            | 10/146 [00:04<00:30,  4.41it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6925178170204163:   8%|▉            | 11/146 [00:04<00:32,  4.15it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6927867531776428:   3%|▍              | 5/151 [00:03<00:52,  2.80it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6927867531776428:   4%|▌              | 6/151 [00:03<00:47,  3.04it/s]Epoch: 1, train for the 3-th batch, train loss: 0.6889529228210449:   1%|               | 2/241 [00:02<04:10,  1.05s/it]Epoch: 1, train for the 6-th batch, train loss: 0.6625056266784668:   3%|▌              | 4/119 [00:03<01:08,  1.69it/s]Epoch: 1, train for the 3-th batch, train loss: 0.6889529228210449:   1%|▏              | 3/241 [00:02<02:55,  1.36it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6625056266784668:   5%|▊              | 6/119 [00:03<00:42,  2.68it/s]Epoch: 1, train for the 2-th batch, train loss: 0.6913686990737915:   0%|               | 1/237 [00:02<09:18,  2.37s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6913686990737915:   1%|▏              | 2/237 [00:02<04:04,  1.04s/it]Epoch: 1, train for the 12-th batch, train loss: 0.692290723323822:   8%|█             | 11/146 [00:04<00:32,  4.15it/s]Epoch: 1, train for the 12-th batch, train loss: 0.692290723323822:   8%|█▏            | 12/146 [00:04<00:27,  4.81it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6883886456489563:   1%|▏              | 3/241 [00:02<02:55,  1.36it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6883886456489563:   2%|▏              | 4/241 [00:02<01:57,  2.02it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6879867911338806:   2%|▏              | 4/241 [00:03<01:57,  2.02it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6543843150138855:   5%|▊              | 6/119 [00:03<00:42,  2.68it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6543843150138855:   6%|▉              | 7/119 [00:03<00:37,  3.02it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6919369101524353:   4%|▌              | 6/151 [00:03<00:47,  3.04it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6919369101524353:   5%|▋              | 7/151 [00:03<00:50,  2.83it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6919694542884827:   8%|█            | 12/146 [00:04<00:27,  4.81it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6919694542884827:   9%|█▏           | 13/146 [00:04<00:32,  4.07it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6898705959320068:   2%|▏              | 4/241 [00:03<01:57,  2.02it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6898705959320068:   2%|▎              | 6/241 [00:03<01:14,  3.16it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6434661746025085:   6%|▉              | 7/119 [00:03<00:37,  3.02it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6434661746025085:   7%|█              | 8/119 [00:03<00:34,  3.26it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6922738552093506:   5%|▋              | 7/151 [00:03<00:50,  2.83it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6922738552093506:   5%|▊              | 8/151 [00:03<00:39,  3.59it/s]Epoch: 1, train for the 3-th batch, train loss: 0.6914303302764893:   1%|▏              | 2/237 [00:02<04:04,  1.04s/it]Epoch: 1, train for the 3-th batch, train loss: 0.6914303302764893:   1%|▏              | 3/237 [00:02<03:03,  1.28it/s]Epoch: 1, train for the 14-th batch, train loss: 0.69222092628479:   9%|█▎             | 13/146 [00:04<00:32,  4.07it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6857671737670898:   2%|▎              | 6/241 [00:03<01:14,  3.16it/s]Epoch: 1, train for the 14-th batch, train loss: 0.69222092628479:  10%|█▍             | 14/146 [00:04<00:28,  4.65it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6857671737670898:   3%|▍              | 7/241 [00:03<01:01,  3.78it/s]Epoch: 1, train for the 9-th batch, train loss: 0.641822099685669:   7%|█               | 8/119 [00:03<00:34,  3.26it/s]Epoch: 1, train for the 9-th batch, train loss: 0.641822099685669:   8%|█▏              | 9/119 [00:04<00:28,  3.83it/s]Epoch: 1, train for the 9-th batch, train loss: 0.692197859287262:   5%|▊               | 8/151 [00:04<00:39,  3.59it/s]Epoch: 1, train for the 9-th batch, train loss: 0.692197859287262:   6%|▉               | 9/151 [00:04<00:32,  4.36it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6915716528892517:  10%|█▏           | 14/146 [00:04<00:28,  4.65it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6915716528892517:  10%|█▎           | 15/146 [00:05<00:24,  5.38it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6899009346961975:   3%|▍              | 7/241 [00:03<01:01,  3.78it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6899009346961975:   3%|▍              | 8/241 [00:03<00:51,  4.50it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6317307353019714:   8%|█             | 9/119 [00:04<00:28,  3.83it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6317307353019714:   8%|█            | 10/119 [00:04<00:23,  4.55it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6921953558921814:   6%|▊             | 9/151 [00:04<00:32,  4.36it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6921953558921814:   7%|▊            | 10/151 [00:04<00:27,  5.11it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6873307824134827:   3%|▍              | 8/241 [00:03<00:51,  4.50it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6873307824134827:   4%|▌              | 9/241 [00:03<00:44,  5.26it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6268618106842041:   8%|█            | 10/119 [00:04<00:23,  4.55it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6906660199165344:   1%|▏              | 3/237 [00:03<03:03,  1.28it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6906660199165344:   2%|▎              | 4/237 [00:03<02:20,  1.66it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6922411918640137:   7%|▊            | 10/151 [00:04<00:27,  5.11it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6906705498695374:  10%|█▎           | 15/146 [00:05<00:24,  5.38it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6922411918640137:   7%|▉            | 11/151 [00:04<00:24,  5.76it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6906705498695374:  11%|█▍           | 16/146 [00:05<00:25,  5.19it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6852676868438721:   4%|▌             | 9/241 [00:03<00:44,  5.26it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6852676868438721:   4%|▌            | 10/241 [00:03<00:39,  5.89it/s]Epoch: 1, train for the 12-th batch, train loss: 0.620071530342102:   8%|█▏            | 10/119 [00:04<00:23,  4.55it/s]Epoch: 1, train for the 12-th batch, train loss: 0.620071530342102:  10%|█▍            | 12/119 [00:04<00:18,  5.91it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6884830594062805:   2%|▎              | 4/237 [00:03<02:20,  1.66it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6884830594062805:   2%|▎              | 5/237 [00:03<01:38,  2.36it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6907978653907776:  11%|█▍           | 16/146 [00:05<00:25,  5.19it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6907978653907776:  12%|█▌           | 17/146 [00:05<00:21,  5.87it/s]Epoch: 1, train for the 12-th batch, train loss: 0.69290691614151:   7%|█              | 11/151 [00:04<00:24,  5.76it/s]Epoch: 1, train for the 12-th batch, train loss: 0.69290691614151:   8%|█▏             | 12/151 [00:04<00:23,  5.96it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6795193552970886:   4%|▌            | 10/241 [00:03<00:39,  5.89it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6795193552970886:   5%|▌            | 11/241 [00:03<00:36,  6.32it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6043916344642639:  10%|█▎           | 12/119 [00:04<00:18,  5.91it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6043916344642639:  11%|█▍           | 13/119 [00:04<00:16,  6.29it/s]Epoch: 1, train for the 6-th batch, train loss: 0.688471794128418:   2%|▎               | 5/237 [00:03<01:38,  2.36it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6904875040054321:  12%|█▌           | 17/146 [00:05<00:21,  5.87it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6904875040054321:  12%|█▌           | 18/146 [00:05<00:19,  6.48it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6820532083511353:   5%|▌            | 11/241 [00:03<00:36,  6.32it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6909773349761963:  12%|█▌           | 18/146 [00:05<00:19,  6.48it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6901561617851257:  12%|█▌           | 18/146 [00:05<00:19,  6.48it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6901561617851257:  14%|█▊           | 20/146 [00:05<00:15,  8.36it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6933109760284424:   8%|█            | 12/151 [00:04<00:23,  5.96it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6933109760284424:   9%|█            | 13/151 [00:04<00:28,  4.84it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6902485489845276:   2%|▎              | 5/237 [00:03<01:38,  2.36it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6902485489845276:   3%|▍              | 7/237 [00:03<01:09,  3.31it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6808117032051086:   5%|▌            | 11/241 [00:04<00:36,  6.32it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6808117032051086:   5%|▋            | 13/241 [00:04<00:36,  6.25it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6022683382034302:  11%|█▍           | 13/119 [00:04<00:16,  6.29it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6022683382034302:  12%|█▌           | 14/119 [00:04<00:21,  4.98it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6910127401351929:  14%|█▊           | 20/146 [00:05<00:15,  8.36it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6910127401351929:  14%|█▊           | 21/146 [00:05<00:15,  7.90it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6931553483009338:   9%|█            | 13/151 [00:04<00:28,  4.84it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6931553483009338:   9%|█▏           | 14/151 [00:04<00:24,  5.62it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6877738833427429:   3%|▍              | 7/237 [00:03<01:09,  3.31it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6877738833427429:   3%|▌              | 8/237 [00:03<00:57,  3.96it/s]Epoch: 1, train for the 15-th batch, train loss: 0.5914003849029541:  12%|█▌           | 14/119 [00:04<00:21,  4.98it/s]Epoch: 1, train for the 15-th batch, train loss: 0.5914003849029541:  13%|█▋           | 15/119 [00:04<00:18,  5.64it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6850526332855225:   5%|▋            | 13/241 [00:04<00:36,  6.25it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6850526332855225:   6%|▊            | 14/241 [00:04<00:34,  6.51it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6906999945640564:  14%|█▊           | 21/146 [00:05<00:15,  7.90it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6906999945640564:  15%|█▉           | 22/146 [00:05<00:15,  8.10it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6866384744644165:   3%|▌              | 8/237 [00:03<00:57,  3.96it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6866384744644165:   4%|▌              | 9/237 [00:03<00:48,  4.74it/s]Epoch: 1, train for the 10-th batch, train loss: 0.685300886631012:   4%|▌              | 9/237 [00:04<00:48,  4.74it/s]Epoch: 1, train for the 10-th batch, train loss: 0.685300886631012:   4%|▌             | 10/237 [00:04<00:41,  5.50it/s]Epoch: 1, train for the 23-th batch, train loss: 0.689591646194458:  15%|██            | 22/146 [00:06<00:15,  8.10it/s]Epoch: 1, train for the 23-th batch, train loss: 0.689591646194458:  16%|██▏           | 23/146 [00:06<00:15,  7.73it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6918540000915527:   9%|█▏           | 14/151 [00:05<00:24,  5.62it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6798098683357239:   6%|▊            | 14/241 [00:04<00:34,  6.51it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6798098683357239:   6%|▊            | 15/241 [00:04<00:36,  6.15it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6918540000915527:  10%|█▎           | 15/151 [00:05<00:27,  5.04it/s]Epoch: 1, train for the 16-th batch, train loss: 0.5962814092636108:  13%|█▋           | 15/119 [00:05<00:18,  5.64it/s]Epoch: 1, train for the 16-th batch, train loss: 0.5962814092636108:  13%|█▋           | 16/119 [00:05<00:19,  5.42it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6851930618286133:   4%|▌            | 10/237 [00:04<00:41,  5.50it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6851930618286133:   5%|▌            | 11/237 [00:04<00:36,  6.11it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6896719336509705:  16%|██           | 23/146 [00:06<00:15,  7.73it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6896719336509705:  16%|██▏          | 24/146 [00:06<00:15,  7.66it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6715750694274902:   6%|▊            | 15/241 [00:04<00:36,  6.15it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6715750694274902:   7%|▊            | 16/241 [00:04<00:35,  6.41it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6954643130302429:   0%|                       | 0/383 [00:02<?, ?it/s]Epoch: 1, train for the 17-th batch, train loss: 0.5664502382278442:  13%|█▋           | 16/119 [00:05<00:19,  5.42it/s]Epoch: 1, train for the 17-th batch, train loss: 0.5664502382278442:  14%|█▊           | 17/119 [00:05<00:18,  5.66it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6954643130302429:   0%|               | 1/383 [00:02<15:28,  2.43s/it]Epoch: 1, train for the 12-th batch, train loss: 0.686743438243866:   5%|▋             | 11/237 [00:04<00:36,  6.11it/s]Epoch: 1, train for the 12-th batch, train loss: 0.686743438243866:   5%|▋             | 12/237 [00:04<00:35,  6.40it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6900964379310608:  16%|██▏          | 24/146 [00:06<00:15,  7.66it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6900964379310608:  17%|██▏          | 25/146 [00:06<00:16,  7.43it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6672145128250122:   7%|▊            | 16/241 [00:04<00:35,  6.41it/s]Epoch: 1, train for the 16-th batch, train loss: 0.692074716091156:  10%|█▍            | 15/151 [00:05<00:27,  5.04it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6672145128250122:   7%|▉            | 17/241 [00:04<00:34,  6.45it/s]Epoch: 1, train for the 2-th batch, train loss: 0.6908218264579773:   0%|               | 1/383 [00:02<15:28,  2.43s/it]Epoch: 1, train for the 16-th batch, train loss: 0.692074716091156:  11%|█▍            | 16/151 [00:05<00:30,  4.38it/s]Epoch: 1, train for the 2-th batch, train loss: 0.6908218264579773:   1%|               | 2/383 [00:02<06:52,  1.08s/it]Epoch: 1, train for the 13-th batch, train loss: 0.6875860691070557:   5%|▋            | 12/237 [00:04<00:35,  6.40it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6875860691070557:   5%|▋            | 13/237 [00:04<00:32,  6.98it/s]Epoch: 1, train for the 18-th batch, train loss: 0.5653783082962036:  14%|█▊           | 17/119 [00:05<00:18,  5.66it/s]Epoch: 1, train for the 18-th batch, train loss: 0.5653783082962036:  15%|█▉           | 18/119 [00:05<00:18,  5.36it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6918354630470276:  11%|█▍           | 16/151 [00:05<00:30,  4.38it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6918354630470276:  11%|█▍           | 17/151 [00:05<00:26,  5.13it/s]Epoch: 1, train for the 19-th batch, train loss: 0.555121660232544:  15%|██            | 18/119 [00:05<00:18,  5.36it/s]Epoch: 1, train for the 19-th batch, train loss: 0.555121660232544:  16%|██▏           | 19/119 [00:05<00:16,  6.12it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6893233060836792:  17%|██▏          | 25/146 [00:06<00:16,  7.43it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6650307178497314:   7%|▉            | 17/241 [00:05<00:34,  6.45it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6893233060836792:  18%|██▎          | 26/146 [00:06<00:20,  5.87it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6650307178497314:   7%|▉            | 18/241 [00:05<00:39,  5.60it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6889598965644836:   5%|▋            | 13/237 [00:04<00:32,  6.98it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6889598965644836:   6%|▊            | 14/237 [00:04<00:37,  5.87it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6925706267356873:  11%|█▍           | 17/151 [00:05<00:26,  5.13it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6925706267356873:  12%|█▌           | 18/151 [00:05<00:25,  5.14it/s]Epoch: 1, train for the 3-th batch, train loss: 0.693851888179779:   1%|                | 2/383 [00:02<06:52,  1.08s/it]Epoch: 1, train for the 19-th batch, train loss: 0.6705319285392761:   7%|▉            | 18/241 [00:05<00:39,  5.60it/s]Epoch: 1, train for the 20-th batch, train loss: 0.5542899370193481:  16%|██           | 19/119 [00:05<00:16,  6.12it/s]Epoch: 1, train for the 3-th batch, train loss: 0.693851888179779:   1%|▏               | 3/383 [00:02<04:42,  1.34it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6705319285392761:   8%|█            | 19/241 [00:05<00:35,  6.23it/s]Epoch: 1, train for the 20-th batch, train loss: 0.5542899370193481:  17%|██▏          | 20/119 [00:05<00:16,  6.05it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6883431673049927:  18%|██▎          | 26/146 [00:06<00:20,  5.87it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6883431673049927:  18%|██▍          | 27/146 [00:06<00:19,  5.97it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6903837323188782:   6%|▊            | 14/237 [00:04<00:37,  5.87it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6903837323188782:   6%|▊            | 15/237 [00:04<00:35,  6.31it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6908954381942749:  12%|█▌           | 18/151 [00:05<00:25,  5.14it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6908954381942749:  13%|█▋           | 19/151 [00:05<00:24,  5.36it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6553226113319397:   8%|█            | 19/241 [00:05<00:35,  6.23it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6905811429023743:   1%|               | 3/383 [00:03<04:42,  1.34it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6553226113319397:   8%|█            | 20/241 [00:05<00:34,  6.46it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6905811429023743:   1%|▏              | 4/383 [00:03<03:12,  1.96it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6907283067703247:   1%|▏              | 4/383 [00:03<03:12,  1.96it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6907283067703247:   1%|▏              | 5/383 [00:03<02:17,  2.74it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6908237934112549:  13%|█▋           | 19/151 [00:06<00:24,  5.36it/s]Epoch: 1, train for the 21-th batch, train loss: 0.5579237937927246:  17%|██▏          | 20/119 [00:06<00:16,  6.05it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6908237934112549:  13%|█▋           | 20/151 [00:06<00:23,  5.53it/s]Epoch: 1, train for the 21-th batch, train loss: 0.5579237937927246:  18%|██▎          | 21/119 [00:06<00:20,  4.88it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6856140494346619:   8%|█            | 20/241 [00:05<00:34,  6.46it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6856140494346619:   9%|█▏           | 21/241 [00:05<00:41,  5.26it/s]Epoch: 1, train for the 22-th batch, train loss: 0.5364669561386108:  18%|██▎          | 21/119 [00:06<00:20,  4.88it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6905553936958313:  13%|█▋           | 20/151 [00:06<00:23,  5.53it/s]Epoch: 1, train for the 22-th batch, train loss: 0.5364669561386108:  18%|██▍          | 22/119 [00:06<00:17,  5.58it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6905553936958313:  14%|█▊           | 21/151 [00:06<00:21,  6.06it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6874198913574219:  18%|██▍          | 27/146 [00:07<00:19,  5.97it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6874198913574219:  19%|██▍          | 28/146 [00:07<00:28,  4.07it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6888416409492493:   1%|▏              | 5/383 [00:03<02:17,  2.74it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6836215257644653:   6%|▊            | 15/237 [00:05<00:35,  6.31it/s]Epoch: 1, train for the 16-th batch, train loss: 0.6836215257644653:   7%|▉            | 16/237 [00:05<00:53,  4.16it/s]Epoch: 1, train for the 6-th batch, train loss: 0.6888416409492493:   2%|▏              | 6/383 [00:03<02:02,  3.08it/s]Epoch: 1, train for the 23-th batch, train loss: 0.5390170812606812:  18%|██▍          | 22/119 [00:06<00:17,  5.58it/s]Epoch: 1, train for the 23-th batch, train loss: 0.5390170812606812:  19%|██▌          | 23/119 [00:06<00:15,  6.31it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6689152121543884:   9%|█▏           | 21/241 [00:05<00:41,  5.26it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6689152121543884:   9%|█▏           | 22/241 [00:05<00:37,  5.77it/s]Epoch: 1, train for the 29-th batch, train loss: 0.688101589679718:  19%|██▋           | 28/146 [00:07<00:28,  4.07it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6896017789840698:  14%|█▊           | 21/151 [00:06<00:21,  6.06it/s]Epoch: 1, train for the 29-th batch, train loss: 0.688101589679718:  20%|██▊           | 29/146 [00:07<00:24,  4.86it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6896017789840698:  15%|█▉           | 22/151 [00:06<00:21,  6.01it/s]Epoch: 1, train for the 17-th batch, train loss: 0.683716356754303:   7%|▉             | 16/237 [00:05<00:53,  4.16it/s]Epoch: 1, train for the 17-th batch, train loss: 0.683716356754303:   7%|█             | 17/237 [00:05<00:46,  4.77it/s]Epoch: 1, train for the 24-th batch, train loss: 0.5423991084098816:  19%|██▌          | 23/119 [00:06<00:15,  6.31it/s]Epoch: 1, train for the 24-th batch, train loss: 0.5423991084098816:  20%|██▌          | 24/119 [00:06<00:14,  6.71it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6654079556465149:   9%|█▏           | 22/241 [00:05<00:37,  5.77it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6901940107345581:   2%|▏              | 6/383 [00:03<02:02,  3.08it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6654079556465149:  10%|█▏           | 23/241 [00:05<00:34,  6.24it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6901940107345581:   2%|▎              | 7/383 [00:03<01:44,  3.61it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6896946430206299:  20%|██▌          | 29/146 [00:07<00:24,  4.86it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6896946430206299:  21%|██▋          | 30/146 [00:07<00:21,  5.41it/s]Epoch: 1, train for the 25-th batch, train loss: 0.5115311145782471:  20%|██▌          | 24/119 [00:06<00:14,  6.71it/s]Epoch: 1, train for the 25-th batch, train loss: 0.5115311145782471:  21%|██▋          | 25/119 [00:06<00:13,  7.09it/s]Epoch: 1, train for the 24-th batch, train loss: 0.67650306224823:  10%|█▍             | 23/241 [00:05<00:34,  6.24it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6842291355133057:   2%|▎              | 7/383 [00:03<01:44,  3.61it/s]Epoch: 1, train for the 24-th batch, train loss: 0.67650306224823:  10%|█▍             | 24/241 [00:05<00:31,  6.80it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6842291355133057:   2%|▎              | 8/383 [00:03<01:24,  4.43it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6866953372955322:  21%|██▋          | 30/146 [00:07<00:21,  5.41it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6866953372955322:  21%|██▊          | 31/146 [00:07<00:19,  5.81it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6756879091262817:   7%|▉            | 17/237 [00:05<00:46,  4.77it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6756879091262817:   8%|▉            | 18/237 [00:05<00:48,  4.48it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6904237866401672:  15%|█▉           | 22/151 [00:06<00:21,  6.01it/s]Epoch: 1, train for the 26-th batch, train loss: 0.5211037397384644:  21%|██▋          | 25/119 [00:06<00:13,  7.09it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6904237866401672:  15%|█▉           | 23/151 [00:06<00:27,  4.69it/s]Epoch: 1, train for the 26-th batch, train loss: 0.5211037397384644:  22%|██▊          | 26/119 [00:06<00:13,  7.11it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6901273727416992:   2%|▎              | 8/383 [00:03<01:24,  4.43it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6748461723327637:  10%|█▎           | 24/241 [00:06<00:31,  6.80it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6901273727416992:   2%|▎              | 9/383 [00:03<01:13,  5.07it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6748461723327637:  10%|█▎           | 25/241 [00:06<00:31,  6.91it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6873688101768494:  21%|██▊          | 31/146 [00:07<00:19,  5.81it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6873688101768494:  22%|██▊          | 32/146 [00:07<00:17,  6.46it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6790826320648193:   8%|▉            | 18/237 [00:05<00:48,  4.48it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6908940076828003:  15%|█▉           | 23/151 [00:06<00:27,  4.69it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6790826320648193:   8%|█            | 19/237 [00:05<00:44,  4.91it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6908940076828003:  16%|██           | 24/151 [00:06<00:24,  5.24it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6800395250320435:  10%|█▎           | 25/241 [00:06<00:31,  6.91it/s]Epoch: 1, train for the 27-th batch, train loss: 0.5441939234733582:  22%|██▊          | 26/119 [00:06<00:13,  7.11it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6800395250320435:  11%|█▍           | 26/241 [00:06<00:31,  6.79it/s]Epoch: 1, train for the 27-th batch, train loss: 0.5441939234733582:  23%|██▉          | 27/119 [00:06<00:13,  6.69it/s]Epoch: 1, train for the 33-th batch, train loss: 0.687063992023468:  22%|███           | 32/146 [00:07<00:17,  6.46it/s]Epoch: 1, train for the 33-th batch, train loss: 0.687063992023468:  23%|███▏          | 33/146 [00:07<00:17,  6.39it/s]Epoch: 1, train for the 10-th batch, train loss: 0.685843288898468:   2%|▎              | 9/383 [00:04<01:13,  5.07it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6917375326156616:  16%|██           | 24/151 [00:06<00:24,  5.24it/s]Epoch: 1, train for the 10-th batch, train loss: 0.685843288898468:   3%|▎             | 10/383 [00:04<01:20,  4.65it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6917375326156616:  17%|██▏          | 25/151 [00:06<00:22,  5.70it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6681458950042725:  11%|█▍           | 26/241 [00:06<00:31,  6.79it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6681458950042725:  11%|█▍           | 27/241 [00:06<00:31,  6.74it/s]Epoch: 1, train for the 34-th batch, train loss: 0.687424898147583:  23%|███▏          | 33/146 [00:07<00:17,  6.39it/s]Epoch: 1, train for the 34-th batch, train loss: 0.687424898147583:  23%|███▎          | 34/146 [00:07<00:16,  6.97it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6902743577957153:  17%|██▏          | 25/151 [00:07<00:22,  5.70it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6902743577957153:  17%|██▏          | 26/151 [00:07<00:19,  6.27it/s]Epoch: 1, train for the 28-th batch, train loss: 0.5517826080322266:  23%|██▉          | 27/119 [00:07<00:13,  6.69it/s]Epoch: 1, train for the 28-th batch, train loss: 0.5517826080322266:  24%|███          | 28/119 [00:07<00:16,  5.56it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6815216541290283:   3%|▎            | 10/383 [00:04<01:20,  4.65it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6833633780479431:   8%|█            | 19/237 [00:06<00:44,  4.91it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6408511996269226:  11%|█▍           | 27/241 [00:06<00:31,  6.74it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6815216541290283:   3%|▎            | 11/383 [00:04<01:15,  4.91it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6833633780479431:   8%|█            | 20/237 [00:06<00:51,  4.19it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6865142583847046:  23%|███          | 34/146 [00:08<00:16,  6.97it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6408511996269226:  12%|█▌           | 28/241 [00:06<00:31,  6.83it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6865142583847046:  24%|███          | 35/146 [00:08<00:15,  7.12it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6903045773506165:  17%|██▏          | 26/151 [00:07<00:19,  6.27it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6903045773506165:  18%|██▎          | 27/151 [00:07<00:19,  6.41it/s]Epoch: 1, train for the 29-th batch, train loss: 0.5080628991127014:  24%|███          | 28/119 [00:07<00:16,  5.56it/s]Epoch: 1, train for the 29-th batch, train loss: 0.5080628991127014:  24%|███▏         | 29/119 [00:07<00:14,  6.03it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6718084812164307:  12%|█▌           | 28/241 [00:06<00:31,  6.83it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6844889521598816:  24%|███          | 35/146 [00:08<00:15,  7.12it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6750326156616211:   3%|▎            | 11/383 [00:04<01:15,  4.91it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6718084812164307:  12%|█▌           | 29/241 [00:06<00:31,  6.80it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6844889521598816:  25%|███▏         | 36/146 [00:08<00:15,  7.03it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6750326156616211:   3%|▍            | 12/383 [00:04<01:11,  5.17it/s]Epoch: 1, train for the 21-th batch, train loss: 0.683721661567688:   8%|█▏            | 20/237 [00:06<00:51,  4.19it/s]Epoch: 1, train for the 21-th batch, train loss: 0.683721661567688:   9%|█▏            | 21/237 [00:06<00:49,  4.41it/s]Epoch: 1, train for the 30-th batch, train loss: 0.5010730028152466:  24%|███▏         | 29/119 [00:07<00:14,  6.03it/s]Epoch: 1, train for the 30-th batch, train loss: 0.5010730028152466:  25%|███▎         | 30/119 [00:07<00:13,  6.53it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6798183917999268:   3%|▍            | 12/383 [00:04<01:11,  5.17it/s]Epoch: 1, train for the 31-th batch, train loss: 0.4996492266654968:  25%|███▎         | 30/119 [00:07<00:13,  6.53it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6798183917999268:   3%|▍            | 13/383 [00:04<01:07,  5.44it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6911153793334961:  18%|██▎          | 27/151 [00:07<00:19,  6.41it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6911153793334961:  19%|██▍          | 28/151 [00:07<00:23,  5.29it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6878945827484131:  12%|█▌           | 29/241 [00:06<00:31,  6.80it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6878945827484131:  12%|█▌           | 30/241 [00:06<00:36,  5.73it/s]Epoch: 1, train for the 32-th batch, train loss: 0.503924548625946:  25%|███▌          | 30/119 [00:07<00:13,  6.53it/s]Epoch: 1, train for the 32-th batch, train loss: 0.503924548625946:  27%|███▊          | 32/119 [00:07<00:11,  7.79it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6861404180526733:   3%|▍            | 13/383 [00:04<01:07,  5.44it/s]Epoch: 1, train for the 14-th batch, train loss: 0.6861404180526733:   4%|▍            | 14/383 [00:04<01:04,  5.76it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6785888671875:   9%|█▍              | 21/237 [00:06<00:49,  4.41it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6915433406829834:  19%|██▍          | 28/151 [00:07<00:23,  5.29it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6850486397743225:  25%|███▏         | 36/146 [00:08<00:15,  7.03it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6915433406829834:  19%|██▍          | 29/151 [00:07<00:20,  5.88it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6850486397743225:  25%|███▎         | 37/146 [00:08<00:21,  5.05it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6785888671875:   9%|█▍              | 22/237 [00:06<00:52,  4.07it/s]Epoch: 1, train for the 31-th batch, train loss: 0.641399621963501:  12%|█▋            | 30/241 [00:07<00:36,  5.73it/s]Epoch: 1, train for the 31-th batch, train loss: 0.641399621963501:  13%|█▊            | 31/241 [00:07<00:33,  6.22it/s]Epoch: 1, train for the 33-th batch, train loss: 0.5442340970039368:  27%|███▍         | 32/119 [00:07<00:11,  7.79it/s]Epoch: 1, train for the 33-th batch, train loss: 0.5442340970039368:  28%|███▌         | 33/119 [00:07<00:10,  7.82it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6906538009643555:  19%|██▍          | 29/151 [00:07<00:20,  5.88it/s]Epoch: 1, train for the 38-th batch, train loss: 0.6845793128013611:  25%|███▎         | 37/146 [00:08<00:21,  5.05it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6748449802398682:   4%|▍            | 14/383 [00:04<01:04,  5.76it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6906538009643555:  20%|██▌          | 30/151 [00:07<00:20,  5.99it/s]Epoch: 1, train for the 38-th batch, train loss: 0.6845793128013611:  26%|███▍         | 38/146 [00:08<00:20,  5.37it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6748449802398682:   4%|▌            | 15/383 [00:04<01:04,  5.74it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6294784545898438:  13%|█▋           | 31/241 [00:07<00:33,  6.22it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6294784545898438:  13%|█▋           | 32/241 [00:07<00:32,  6.46it/s]Epoch: 1, train for the 34-th batch, train loss: 0.5116332173347473:  28%|███▌         | 33/119 [00:07<00:10,  7.82it/s]Epoch: 1, train for the 34-th batch, train loss: 0.5116332173347473:  29%|███▋         | 34/119 [00:07<00:11,  7.68it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6908412575721741:  20%|██▌          | 30/151 [00:07<00:20,  5.99it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6908412575721741:  21%|██▋          | 31/151 [00:07<00:19,  6.25it/s]Epoch: 1, train for the 16-th batch, train loss: 0.686430811882019:   4%|▌             | 15/383 [00:05<01:04,  5.74it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6772826910018921:   9%|█▏           | 22/237 [00:06<00:52,  4.07it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6772826910018921:  10%|█▎           | 23/237 [00:06<00:58,  3.64it/s]Epoch: 1, train for the 16-th batch, train loss: 0.686430811882019:   4%|▌             | 16/383 [00:05<01:04,  5.65it/s]Epoch: 1, train for the 35-th batch, train loss: 0.5099931955337524:  29%|███▋         | 34/119 [00:08<00:11,  7.68it/s]Epoch: 1, train for the 35-th batch, train loss: 0.5099931955337524:  29%|███▊         | 35/119 [00:08<00:12,  6.63it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6743389964103699:  13%|█▋           | 32/241 [00:07<00:32,  6.46it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6743389964103699:  14%|█▊           | 33/241 [00:07<00:39,  5.32it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6844448447227478:  26%|███▍         | 38/146 [00:08<00:20,  5.37it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6844448447227478:  27%|███▍         | 39/146 [00:08<00:23,  4.51it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6887381672859192:  21%|██▋          | 31/151 [00:08<00:19,  6.25it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6887381672859192:  21%|██▊          | 32/151 [00:08<00:19,  5.98it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6785188317298889:  10%|█▎           | 23/237 [00:07<00:58,  3.64it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6826464533805847:   4%|▌            | 16/383 [00:05<01:04,  5.65it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6785188317298889:  10%|█▎           | 24/237 [00:07<00:51,  4.13it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6826464533805847:   4%|▌            | 17/383 [00:05<01:04,  5.68it/s]Epoch: 1, train for the 36-th batch, train loss: 0.5025727152824402:  29%|███▊         | 35/119 [00:08<00:12,  6.63it/s]Epoch: 1, train for the 36-th batch, train loss: 0.5025727152824402:  30%|███▉         | 36/119 [00:08<00:12,  6.87it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6413671970367432:  14%|█▊           | 33/241 [00:07<00:39,  5.32it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6413671970367432:  14%|█▊           | 34/241 [00:07<00:34,  5.94it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6844692826271057:  27%|███▍         | 39/146 [00:09<00:23,  4.51it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6844692826271057:  27%|███▌         | 40/146 [00:09<00:20,  5.09it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6886614561080933:  21%|██▊          | 32/151 [00:08<00:19,  5.98it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6886614561080933:  22%|██▊          | 33/151 [00:08<00:18,  6.40it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6739967465400696:  10%|█▎           | 24/237 [00:07<00:51,  4.13it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6739967465400696:  11%|█▎           | 25/237 [00:07<00:44,  4.77it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6886085271835327:  22%|██▊          | 33/151 [00:08<00:18,  6.40it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6886085271835327:  23%|██▉          | 34/151 [00:08<00:16,  7.13it/s]Epoch: 1, train for the 37-th batch, train loss: 0.5140180587768555:  30%|███▉         | 36/119 [00:08<00:12,  6.87it/s]Epoch: 1, train for the 37-th batch, train loss: 0.5140180587768555:  31%|████         | 37/119 [00:08<00:14,  5.74it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6611121892929077:  14%|█▊           | 34/241 [00:07<00:34,  5.94it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6611121892929077:  15%|█▉           | 35/241 [00:07<00:38,  5.31it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6843906044960022:   4%|▌            | 17/383 [00:05<01:04,  5.68it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6815719604492188:  27%|███▌         | 40/146 [00:09<00:20,  5.09it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6815719604492188:  28%|███▋         | 41/146 [00:09<00:21,  4.78it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6843906044960022:   5%|▌            | 18/383 [00:05<01:20,  4.54it/s]Epoch: 1, train for the 38-th batch, train loss: 0.505577564239502:  31%|████▎         | 37/119 [00:08<00:14,  5.74it/s]Epoch: 1, train for the 38-th batch, train loss: 0.505577564239502:  32%|████▍         | 38/119 [00:08<00:12,  6.27it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6619676947593689:  11%|█▎           | 25/237 [00:07<00:44,  4.77it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6619676947593689:  11%|█▍           | 26/237 [00:07<00:50,  4.21it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6827685236930847:  28%|███▋         | 41/146 [00:09<00:21,  4.78it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6878610849380493:  23%|██▉          | 34/151 [00:08<00:16,  7.13it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6827685236930847:  29%|███▋         | 42/146 [00:09<00:18,  5.48it/s]Epoch: 1, train for the 36-th batch, train loss: 0.660404622554779:  15%|██            | 35/241 [00:08<00:38,  5.31it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6878610849380493:  23%|███          | 35/151 [00:08<00:20,  5.77it/s]Epoch: 1, train for the 36-th batch, train loss: 0.660404622554779:  15%|██            | 36/241 [00:08<00:38,  5.39it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6585701107978821:   5%|▌            | 18/383 [00:05<01:20,  4.54it/s]Epoch: 1, train for the 19-th batch, train loss: 0.6585701107978821:   5%|▋            | 19/383 [00:05<01:13,  4.96it/s]Epoch: 1, train for the 39-th batch, train loss: 0.5215393900871277:  32%|████▏        | 38/119 [00:08<00:12,  6.27it/s]Epoch: 1, train for the 39-th batch, train loss: 0.5215393900871277:  33%|████▎        | 39/119 [00:08<00:12,  6.63it/s]Epoch: 1, train for the 27-th batch, train loss: 0.671576976776123:  11%|█▌            | 26/237 [00:07<00:50,  4.21it/s]Epoch: 1, train for the 27-th batch, train loss: 0.671576976776123:  11%|█▌            | 27/237 [00:07<00:44,  4.76it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6799586415290833:  29%|███▋         | 42/146 [00:09<00:18,  5.48it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6799586415290833:  29%|███▊         | 43/146 [00:09<00:17,  5.83it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6888717412948608:  23%|███          | 35/151 [00:08<00:20,  5.77it/s]Epoch: 1, train for the 40-th batch, train loss: 0.5490800738334656:  33%|████▎        | 39/119 [00:08<00:12,  6.63it/s]Epoch: 1, train for the 40-th batch, train loss: 0.5490800738334656:  34%|████▎        | 40/119 [00:08<00:10,  7.21it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6888717412948608:  24%|███          | 36/151 [00:08<00:20,  5.70it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6775943040847778:  11%|█▍           | 27/237 [00:07<00:44,  4.76it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6775943040847778:  12%|█▌           | 28/237 [00:07<00:37,  5.59it/s]Epoch: 1, train for the 41-th batch, train loss: 0.5577796101570129:  34%|████▎        | 40/119 [00:08<00:10,  7.21it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6167357563972473:  15%|█▉           | 36/241 [00:08<00:38,  5.39it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6908580660820007:  24%|███          | 36/151 [00:08<00:20,  5.70it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6167357563972473:  15%|█▉           | 37/241 [00:08<00:43,  4.69it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6908580660820007:  25%|███▏         | 37/151 [00:08<00:18,  6.33it/s]Epoch: 1, train for the 42-th batch, train loss: 0.5644547343254089:  34%|████▎        | 40/119 [00:08<00:10,  7.21it/s]Epoch: 1, train for the 42-th batch, train loss: 0.5644547343254089:  35%|████▌        | 42/119 [00:08<00:09,  8.00it/s]Epoch: 1, train for the 38-th batch, train loss: 0.6381710767745972:  15%|█▉           | 37/241 [00:08<00:43,  4.69it/s]Epoch: 1, train for the 44-th batch, train loss: 0.6779223680496216:  29%|███▊         | 43/146 [00:09<00:17,  5.83it/s]Epoch: 1, train for the 44-th batch, train loss: 0.6779223680496216:  30%|███▉         | 44/146 [00:09<00:21,  4.76it/s]Epoch: 1, train for the 38-th batch, train loss: 0.6381710767745972:  16%|██           | 38/241 [00:08<00:39,  5.13it/s]Epoch: 1, train for the 38-th batch, train loss: 0.6888128519058228:  25%|███▏         | 37/151 [00:09<00:18,  6.33it/s]Epoch: 1, train for the 29-th batch, train loss: 0.661996066570282:  12%|█▋            | 28/237 [00:08<00:37,  5.59it/s]Epoch: 1, train for the 38-th batch, train loss: 0.6888128519058228:  25%|███▎         | 38/151 [00:09<00:18,  6.22it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6641821265220642:   5%|▋            | 19/383 [00:06<01:13,  4.96it/s]Epoch: 1, train for the 29-th batch, train loss: 0.661996066570282:  12%|█▋            | 29/237 [00:08<00:41,  5.03it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6641821265220642:   5%|▋            | 20/383 [00:06<01:40,  3.61it/s]Epoch: 1, train for the 43-th batch, train loss: 0.5235655903816223:  35%|████▌        | 42/119 [00:09<00:09,  8.00it/s]Epoch: 1, train for the 43-th batch, train loss: 0.5235655903816223:  36%|████▋        | 43/119 [00:09<00:09,  8.22it/s]Epoch: 1, train for the 39-th batch, train loss: 0.658586859703064:  16%|██▏           | 38/241 [00:08<00:39,  5.13it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6819929480552673:  30%|███▉         | 44/146 [00:10<00:21,  4.76it/s]Epoch: 1, train for the 39-th batch, train loss: 0.658586859703064:  16%|██▎           | 39/241 [00:08<00:34,  5.80it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6819929480552673:  31%|████         | 45/146 [00:10<00:18,  5.44it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6864159107208252:  25%|███▎         | 38/151 [00:09<00:18,  6.22it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6864159107208252:  26%|███▎         | 39/151 [00:09<00:17,  6.39it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6578777432441711:  12%|█▌           | 29/237 [00:08<00:41,  5.03it/s]Epoch: 1, train for the 30-th batch, train loss: 0.6578777432441711:  13%|█▋           | 30/237 [00:08<00:38,  5.40it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6746638417243958:  31%|████         | 45/146 [00:10<00:18,  5.44it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6746638417243958:  32%|████         | 46/146 [00:10<00:16,  6.17it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6612372994422913:  13%|█▋           | 30/237 [00:08<00:38,  5.40it/s]Epoch: 1, train for the 44-th batch, train loss: 0.4983387887477875:  36%|████▋        | 43/119 [00:09<00:09,  8.22it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6612372994422913:  13%|█▋           | 31/237 [00:08<00:33,  6.08it/s]Epoch: 1, train for the 44-th batch, train loss: 0.4983387887477875:  37%|████▊        | 44/119 [00:09<00:11,  6.51it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6725098490715027:   5%|▋            | 20/383 [00:06<01:40,  3.61it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6725098490715027:   5%|▋            | 21/383 [00:06<01:41,  3.55it/s]Epoch: 1, train for the 45-th batch, train loss: 0.47387441992759705:  37%|████▍       | 44/119 [00:09<00:11,  6.51it/s]Epoch: 1, train for the 45-th batch, train loss: 0.47387441992759705:  38%|████▌       | 45/119 [00:09<00:10,  6.77it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6844112873077393:  26%|███▎         | 39/151 [00:09<00:17,  6.39it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6698030233383179:  16%|██           | 39/241 [00:08<00:34,  5.80it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6698030233383179:  17%|██▏          | 40/241 [00:08<00:45,  4.44it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6844112873077393:  26%|███▍         | 40/151 [00:09<00:21,  5.05it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6783212423324585:  32%|████         | 46/146 [00:10<00:16,  6.17it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6783212423324585:  32%|████▏        | 47/146 [00:10<00:18,  5.34it/s]Epoch: 1, train for the 32-th batch, train loss: 0.677121102809906:  13%|█▊            | 31/237 [00:08<00:33,  6.08it/s]Epoch: 1, train for the 32-th batch, train loss: 0.677121102809906:  14%|█▉            | 32/237 [00:08<00:35,  5.74it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6788504719734192:   5%|▋            | 21/383 [00:06<01:41,  3.55it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6788504719734192:   6%|▋            | 22/383 [00:06<01:32,  3.91it/s]Epoch: 1, train for the 46-th batch, train loss: 0.5446685552597046:  38%|████▉        | 45/119 [00:09<00:10,  6.77it/s]Epoch: 1, train for the 46-th batch, train loss: 0.5446685552597046:  39%|█████        | 46/119 [00:09<00:10,  7.05it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6298208236694336:  17%|██▏          | 40/241 [00:09<00:45,  4.44it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6298208236694336:  17%|██▏          | 41/241 [00:09<00:39,  5.03it/s]Epoch: 1, train for the 48-th batch, train loss: 0.6777830123901367:  32%|████▏        | 47/146 [00:10<00:18,  5.34it/s]Epoch: 1, train for the 48-th batch, train loss: 0.6777830123901367:  33%|████▎        | 48/146 [00:10<00:17,  5.71it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6565531492233276:  14%|█▊           | 32/237 [00:08<00:35,  5.74it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6565531492233276:  14%|█▊           | 33/237 [00:08<00:34,  5.87it/s]Epoch: 1, train for the 47-th batch, train loss: 0.5927447080612183:  39%|█████        | 46/119 [00:09<00:10,  7.05it/s]Epoch: 1, train for the 47-th batch, train loss: 0.5927447080612183:  39%|█████▏       | 47/119 [00:09<00:09,  7.39it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6861284971237183:  26%|███▍         | 40/151 [00:09<00:21,  5.05it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6861284971237183:  27%|███▌         | 41/151 [00:09<00:23,  4.71it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6789559721946716:   6%|▋            | 22/383 [00:06<01:32,  3.91it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6789559721946716:   6%|▊            | 23/383 [00:07<01:35,  3.78it/s]Epoch: 1, train for the 48-th batch, train loss: 0.532701849937439:  39%|█████▌        | 47/119 [00:09<00:09,  7.39it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6441068053245544:  14%|█▊           | 33/237 [00:08<00:34,  5.87it/s]Epoch: 1, train for the 48-th batch, train loss: 0.532701849937439:  40%|█████▋        | 48/119 [00:09<00:10,  6.57it/s]Epoch: 1, train for the 34-th batch, train loss: 0.6441068053245544:  14%|█▊           | 34/237 [00:08<00:38,  5.31it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6850364804267883:  27%|███▌         | 41/151 [00:09<00:23,  4.71it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6346965432167053:  17%|██▏          | 41/241 [00:09<00:39,  5.03it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6850364804267883:  28%|███▌         | 42/151 [00:09<00:22,  4.84it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6346965432167053:  17%|██▎          | 42/241 [00:09<00:46,  4.32it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6747239828109741:  33%|████▎        | 48/146 [00:10<00:17,  5.71it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6747239828109741:  34%|████▎        | 49/146 [00:10<00:20,  4.63it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6519227623939514:   6%|▊            | 23/383 [00:07<01:35,  3.78it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6519227623939514:   6%|▊            | 24/383 [00:07<01:21,  4.40it/s]Epoch: 1, train for the 49-th batch, train loss: 0.582549512386322:  40%|█████▋        | 48/119 [00:10<00:10,  6.57it/s]Epoch: 1, train for the 49-th batch, train loss: 0.582549512386322:  41%|█████▊        | 49/119 [00:10<00:10,  6.99it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6613900661468506:  14%|█▊           | 34/237 [00:09<00:38,  5.31it/s]Epoch: 1, train for the 43-th batch, train loss: 0.656545877456665:  17%|██▍           | 42/241 [00:09<00:46,  4.32it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6613900661468506:  15%|█▉           | 35/237 [00:09<00:36,  5.55it/s]Epoch: 1, train for the 43-th batch, train loss: 0.656545877456665:  18%|██▍           | 43/241 [00:09<00:40,  4.88it/s]Epoch: 1, train for the 50-th batch, train loss: 0.6723211407661438:  34%|████▎        | 49/146 [00:10<00:20,  4.63it/s]Epoch: 1, train for the 50-th batch, train loss: 0.6723211407661438:  34%|████▍        | 50/146 [00:11<00:18,  5.15it/s]Epoch: 1, train for the 25-th batch, train loss: 0.672733724117279:   6%|▉             | 24/383 [00:07<01:21,  4.40it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6848234534263611:  28%|███▌         | 42/151 [00:10<00:22,  4.84it/s]Epoch: 1, train for the 25-th batch, train loss: 0.672733724117279:   7%|▉             | 25/383 [00:07<01:12,  4.95it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6848234534263611:  28%|███▋         | 43/151 [00:10<00:22,  4.90it/s]Epoch: 1, train for the 44-th batch, train loss: 0.6726495027542114:  18%|██▎          | 43/241 [00:09<00:40,  4.88it/s]Epoch: 1, train for the 44-th batch, train loss: 0.6726495027542114:  18%|██▎          | 44/241 [00:09<00:35,  5.58it/s]Epoch: 1, train for the 50-th batch, train loss: 0.551712691783905:  41%|█████▊        | 49/119 [00:10<00:10,  6.99it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6570900082588196:  15%|█▉           | 35/237 [00:09<00:36,  5.55it/s]Epoch: 1, train for the 50-th batch, train loss: 0.551712691783905:  42%|█████▉        | 50/119 [00:10<00:12,  5.57it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6714299917221069:   7%|▊            | 25/383 [00:07<01:12,  4.95it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6570900082588196:  15%|█▉           | 36/237 [00:09<00:39,  5.15it/s]Epoch: 1, train for the 26-th batch, train loss: 0.6714299917221069:   7%|▉            | 26/383 [00:07<01:10,  5.05it/s]Epoch: 1, train for the 51-th batch, train loss: 0.6750605702400208:  34%|████▍        | 50/146 [00:11<00:18,  5.15it/s]Epoch: 1, train for the 51-th batch, train loss: 0.6750605702400208:  35%|████▌        | 51/146 [00:11<00:19,  4.85it/s]Epoch: 1, train for the 44-th batch, train loss: 0.680027425289154:  28%|███▉          | 43/151 [00:10<00:22,  4.90it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6033957004547119:  18%|██▎          | 44/241 [00:09<00:35,  5.58it/s]Epoch: 1, train for the 44-th batch, train loss: 0.680027425289154:  29%|████          | 44/151 [00:10<00:23,  4.46it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6033957004547119:  19%|██▍          | 45/241 [00:09<00:36,  5.35it/s]Epoch: 1, train for the 51-th batch, train loss: 0.5718179941177368:  42%|█████▍       | 50/119 [00:10<00:12,  5.57it/s]Epoch: 1, train for the 51-th batch, train loss: 0.5718179941177368:  43%|█████▌       | 51/119 [00:10<00:11,  6.01it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6658052206039429:  15%|█▉           | 36/237 [00:09<00:39,  5.15it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6658052206039429:  16%|██           | 37/237 [00:09<00:36,  5.54it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6718463897705078:  35%|████▌        | 51/146 [00:11<00:19,  4.85it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6718463897705078:  36%|████▋        | 52/146 [00:11<00:17,  5.36it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6440620422363281:   7%|▉            | 26/383 [00:07<01:10,  5.05it/s]Epoch: 1, train for the 27-th batch, train loss: 0.6440620422363281:   7%|▉            | 27/383 [00:07<01:08,  5.17it/s]Epoch: 1, train for the 52-th batch, train loss: 0.5599741339683533:  43%|█████▌       | 51/119 [00:10<00:11,  6.01it/s]Epoch: 1, train for the 52-th batch, train loss: 0.5599741339683533:  44%|█████▋       | 52/119 [00:10<00:10,  6.66it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6100395321846008:  19%|██▍          | 45/241 [00:10<00:36,  5.35it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6789929270744324:  29%|███▊         | 44/151 [00:10<00:23,  4.46it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6100395321846008:  19%|██▍          | 46/241 [00:10<00:35,  5.42it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6789929270744324:  30%|███▊         | 45/151 [00:10<00:22,  4.68it/s]Epoch: 1, train for the 53-th batch, train loss: 0.6722132563591003:  36%|████▋        | 52/146 [00:11<00:17,  5.36it/s]Epoch: 1, train for the 53-th batch, train loss: 0.6722132563591003:  36%|████▋        | 53/146 [00:11<00:15,  5.99it/s]Epoch: 1, train for the 53-th batch, train loss: 0.5153856873512268:  44%|█████▋       | 52/119 [00:10<00:10,  6.66it/s]Epoch: 1, train for the 53-th batch, train loss: 0.5153856873512268:  45%|█████▊       | 53/119 [00:10<00:09,  7.08it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6818314790725708:  30%|███▊         | 45/151 [00:10<00:22,  4.68it/s]Epoch: 1, train for the 54-th batch, train loss: 0.6683135628700256:  36%|████▋        | 53/146 [00:11<00:15,  5.99it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6818314790725708:  30%|███▉         | 46/151 [00:10<00:19,  5.35it/s]Epoch: 1, train for the 54-th batch, train loss: 0.6683135628700256:  37%|████▊        | 54/146 [00:11<00:14,  6.57it/s]Epoch: 1, train for the 38-th batch, train loss: 0.6554185152053833:  16%|██           | 37/237 [00:09<00:36,  5.54it/s]Epoch: 1, train for the 38-th batch, train loss: 0.6554185152053833:  16%|██           | 38/237 [00:09<00:41,  4.78it/s]Epoch: 1, train for the 54-th batch, train loss: 0.5644269585609436:  45%|█████▊       | 53/119 [00:10<00:09,  7.08it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6519995927810669:  19%|██▍          | 46/241 [00:10<00:35,  5.42it/s]Epoch: 1, train for the 54-th batch, train loss: 0.5644269585609436:  45%|█████▉       | 54/119 [00:10<00:08,  7.52it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6519995927810669:  20%|██▌          | 47/241 [00:10<00:36,  5.27it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6835538148880005:  30%|███▉         | 46/151 [00:10<00:19,  5.35it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6751935482025146:  16%|██           | 38/237 [00:09<00:41,  4.78it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6835538148880005:  31%|████         | 47/151 [00:10<00:18,  5.70it/s]Epoch: 1, train for the 39-th batch, train loss: 0.6751935482025146:  16%|██▏          | 39/237 [00:09<00:37,  5.31it/s]Epoch: 1, train for the 55-th batch, train loss: 0.6734001636505127:  37%|████▊        | 54/146 [00:11<00:14,  6.57it/s]Epoch: 1, train for the 55-th batch, train loss: 0.6734001636505127:  38%|████▉        | 55/146 [00:11<00:14,  6.34it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6289169192314148:   7%|▉            | 27/383 [00:08<01:08,  5.17it/s]Epoch: 1, train for the 48-th batch, train loss: 0.6310949921607971:  20%|██▌          | 47/241 [00:10<00:36,  5.27it/s]Epoch: 1, train for the 55-th batch, train loss: 0.5098733305931091:  45%|█████▉       | 54/119 [00:10<00:08,  7.52it/s]Epoch: 1, train for the 55-th batch, train loss: 0.5098733305931091:  46%|██████       | 55/119 [00:10<00:08,  7.18it/s]Epoch: 1, train for the 28-th batch, train loss: 0.6289169192314148:   7%|▉            | 28/383 [00:08<01:34,  3.75it/s]Epoch: 1, train for the 48-th batch, train loss: 0.6310949921607971:  20%|██▌          | 48/241 [00:10<00:34,  5.59it/s]Epoch: 1, train for the 48-th batch, train loss: 0.6748902797698975:  31%|████         | 47/151 [00:10<00:18,  5.70it/s]Epoch: 1, train for the 48-th batch, train loss: 0.6748902797698975:  32%|████▏        | 48/151 [00:10<00:17,  6.04it/s]Epoch: 1, train for the 56-th batch, train loss: 0.6633338928222656:  38%|████▉        | 55/146 [00:11<00:14,  6.34it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6460064649581909:  16%|██▏          | 39/237 [00:10<00:37,  5.31it/s]Epoch: 1, train for the 56-th batch, train loss: 0.6633338928222656:  38%|████▉        | 56/146 [00:11<00:13,  6.67it/s]Epoch: 1, train for the 40-th batch, train loss: 0.6460064649581909:  17%|██▏          | 40/237 [00:10<00:36,  5.47it/s]Epoch: 1, train for the 56-th batch, train loss: 0.5329126715660095:  46%|██████       | 55/119 [00:11<00:08,  7.18it/s]Epoch: 1, train for the 56-th batch, train loss: 0.5329126715660095:  47%|██████       | 56/119 [00:11<00:08,  7.11it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6813427805900574:  32%|████▏        | 48/151 [00:11<00:17,  6.04it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6312721967697144:  20%|██▌          | 48/241 [00:10<00:34,  5.59it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6813427805900574:  32%|████▏        | 49/151 [00:11<00:18,  5.53it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6312721967697144:  20%|██▋          | 49/241 [00:10<00:40,  4.70it/s]Epoch: 1, train for the 57-th batch, train loss: 0.6611773371696472:  38%|████▉        | 56/146 [00:12<00:13,  6.67it/s]Epoch: 1, train for the 57-th batch, train loss: 0.5236488580703735:  47%|██████       | 56/119 [00:11<00:08,  7.11it/s]Epoch: 1, train for the 57-th batch, train loss: 0.6611773371696472:  39%|█████        | 57/146 [00:12<00:17,  5.18it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6513422727584839:  17%|██▏          | 40/237 [00:10<00:36,  5.47it/s]Epoch: 1, train for the 57-th batch, train loss: 0.5236488580703735:  48%|██████▏      | 57/119 [00:11<00:10,  5.82it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6174017786979675:   7%|▉            | 28/383 [00:08<01:34,  3.75it/s]Epoch: 1, train for the 41-th batch, train loss: 0.6513422727584839:  17%|██▏          | 41/237 [00:10<00:41,  4.67it/s]Epoch: 1, train for the 29-th batch, train loss: 0.6174017786979675:   8%|▉            | 29/383 [00:08<01:49,  3.24it/s]Epoch: 1, train for the 50-th batch, train loss: 0.6717955470085144:  32%|████▏        | 49/151 [00:11<00:18,  5.53it/s]Epoch: 1, train for the 50-th batch, train loss: 0.6156671047210693:  20%|██▋          | 49/241 [00:10<00:40,  4.70it/s]Epoch: 1, train for the 50-th batch, train loss: 0.6156671047210693:  21%|██▋          | 50/241 [00:10<00:38,  4.94it/s]Epoch: 1, train for the 50-th batch, train loss: 0.6717955470085144:  33%|████▎        | 50/151 [00:11<00:18,  5.44it/s]Epoch: 1, train for the 58-th batch, train loss: 0.6616426110267639:  39%|█████        | 57/146 [00:12<00:17,  5.18it/s]Epoch: 1, train for the 58-th batch, train loss: 0.6616426110267639:  40%|█████▏       | 58/146 [00:12<00:15,  5.59it/s]Epoch: 1, train for the 58-th batch, train loss: 0.537956178188324:  48%|██████▋       | 57/119 [00:11<00:10,  5.82it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6654972434043884:  17%|██▏          | 41/237 [00:10<00:41,  4.67it/s]Epoch: 1, train for the 58-th batch, train loss: 0.537956178188324:  49%|██████▊       | 58/119 [00:11<00:10,  5.95it/s]Epoch: 1, train for the 42-th batch, train loss: 0.6654972434043884:  18%|██▎          | 42/237 [00:10<00:38,  5.08it/s]Epoch: 1, train for the 30-th batch, train loss: 0.615598201751709:   8%|█             | 29/383 [00:08<01:49,  3.24it/s]Epoch: 1, train for the 30-th batch, train loss: 0.615598201751709:   8%|█             | 30/383 [00:08<01:36,  3.67it/s]Epoch: 1, train for the 51-th batch, train loss: 0.6178447008132935:  21%|██▋          | 50/241 [00:10<00:38,  4.94it/s]Epoch: 1, train for the 51-th batch, train loss: 0.6178447008132935:  21%|██▊          | 51/241 [00:10<00:34,  5.44it/s]Epoch: 1, train for the 59-th batch, train loss: 0.6436948180198669:  40%|█████▏       | 58/146 [00:12<00:15,  5.59it/s]Epoch: 1, train for the 59-th batch, train loss: 0.6436948180198669:  40%|█████▎       | 59/146 [00:12<00:14,  6.08it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6207266449928284:  21%|██▊          | 51/241 [00:11<00:34,  5.44it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6207266449928284:  22%|██▊          | 52/241 [00:11<00:31,  5.95it/s]Epoch: 1, train for the 59-th batch, train loss: 0.5665449500083923:  49%|██████▎      | 58/119 [00:11<00:10,  5.95it/s]Epoch: 1, train for the 59-th batch, train loss: 0.5665449500083923:  50%|██████▍      | 59/119 [00:11<00:10,  5.51it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6571954488754272:  18%|██▎          | 42/237 [00:10<00:38,  5.08it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6190183758735657:   8%|█            | 30/383 [00:08<01:36,  3.67it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6571954488754272:  18%|██▎          | 43/237 [00:10<00:40,  4.84it/s]Epoch: 1, train for the 31-th batch, train loss: 0.6190183758735657:   8%|█            | 31/383 [00:08<01:26,  4.06it/s]Epoch: 1, train for the 60-th batch, train loss: 0.6453628540039062:  40%|█████▎       | 59/146 [00:12<00:14,  6.08it/s]Epoch: 1, train for the 60-th batch, train loss: 0.6453628540039062:  41%|█████▎       | 60/146 [00:12<00:13,  6.30it/s]Epoch: 1, train for the 51-th batch, train loss: 0.6782882809638977:  33%|████▎        | 50/151 [00:11<00:18,  5.44it/s]Epoch: 1, train for the 53-th batch, train loss: 0.5097892880439758:  22%|██▊          | 52/241 [00:11<00:31,  5.95it/s]Epoch: 1, train for the 51-th batch, train loss: 0.6782882809638977:  34%|████▍        | 51/151 [00:11<00:24,  4.06it/s]Epoch: 1, train for the 60-th batch, train loss: 0.5349931120872498:  50%|██████▍      | 59/119 [00:11<00:10,  5.51it/s]Epoch: 1, train for the 53-th batch, train loss: 0.5097892880439758:  22%|██▊          | 53/241 [00:11<00:29,  6.36it/s]Epoch: 1, train for the 60-th batch, train loss: 0.5349931120872498:  50%|██████▌      | 60/119 [00:11<00:09,  6.11it/s]Epoch: 1, train for the 61-th batch, train loss: 0.6507822275161743:  41%|█████▎       | 60/146 [00:12<00:13,  6.30it/s]Epoch: 1, train for the 61-th batch, train loss: 0.6507822275161743:  42%|█████▍       | 61/146 [00:12<00:12,  6.54it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6589623093605042:   8%|█            | 31/383 [00:09<01:26,  4.06it/s]Epoch: 1, train for the 44-th batch, train loss: 0.6596606969833374:  18%|██▎          | 43/237 [00:10<00:40,  4.84it/s]Epoch: 1, train for the 32-th batch, train loss: 0.6589623093605042:   8%|█            | 32/383 [00:09<01:22,  4.23it/s]Epoch: 1, train for the 61-th batch, train loss: 0.5429494976997375:  50%|██████▌      | 60/119 [00:11<00:09,  6.11it/s]Epoch: 1, train for the 61-th batch, train loss: 0.5429494976997375:  51%|██████▋      | 61/119 [00:11<00:08,  6.54it/s]Epoch: 1, train for the 44-th batch, train loss: 0.6596606969833374:  19%|██▍          | 44/237 [00:10<00:41,  4.66it/s]Epoch: 1, train for the 54-th batch, train loss: 0.4708826243877411:  22%|██▊          | 53/241 [00:11<00:29,  6.36it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6828905940055847:  34%|████▍        | 51/151 [00:11<00:24,  4.06it/s]Epoch: 1, train for the 54-th batch, train loss: 0.4708826243877411:  22%|██▉          | 54/241 [00:11<00:30,  6.23it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6254380941390991:  42%|█████▍       | 61/146 [00:12<00:12,  6.54it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6828905940055847:  34%|████▍        | 52/151 [00:11<00:22,  4.39it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6254380941390991:  42%|█████▌       | 62/146 [00:12<00:12,  6.95it/s]Epoch: 1, train for the 62-th batch, train loss: 0.5732836723327637:  51%|██████▋      | 61/119 [00:12<00:08,  6.54it/s]Epoch: 1, train for the 62-th batch, train loss: 0.5732836723327637:  52%|██████▊      | 62/119 [00:12<00:08,  6.81it/s]Epoch: 1, train for the 55-th batch, train loss: 0.4667423963546753:  22%|██▉          | 54/241 [00:11<00:30,  6.23it/s]Epoch: 1, train for the 63-th batch, train loss: 0.6322049498558044:  42%|█████▌       | 62/146 [00:13<00:12,  6.95it/s]Epoch: 1, train for the 55-th batch, train loss: 0.4667423963546753:  23%|██▉          | 55/241 [00:11<00:29,  6.40it/s]Epoch: 1, train for the 63-th batch, train loss: 0.6322049498558044:  43%|█████▌       | 63/146 [00:13<00:11,  7.14it/s]Epoch: 1, train for the 63-th batch, train loss: 0.5407807230949402:  52%|██████▊      | 62/119 [00:12<00:08,  6.81it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6590309143066406:  19%|██▍          | 44/237 [00:11<00:41,  4.66it/s]Epoch: 1, train for the 63-th batch, train loss: 0.5407807230949402:  53%|██████▉      | 63/119 [00:12<00:08,  6.51it/s]Epoch: 1, train for the 53-th batch, train loss: 0.6794619560241699:  34%|████▍        | 52/151 [00:12<00:22,  4.39it/s]Epoch: 1, train for the 45-th batch, train loss: 0.6590309143066406:  19%|██▍          | 45/237 [00:11<00:46,  4.13it/s]Epoch: 1, train for the 56-th batch, train loss: 0.49917060136795044:  23%|██▋         | 55/241 [00:11<00:29,  6.40it/s]Epoch: 1, train for the 53-th batch, train loss: 0.6794619560241699:  35%|████▌        | 53/151 [00:12<00:23,  4.12it/s]Epoch: 1, train for the 56-th batch, train loss: 0.49917060136795044:  23%|██▊         | 56/241 [00:11<00:28,  6.51it/s]Epoch: 1, train for the 64-th batch, train loss: 0.6354463696479797:  43%|█████▌       | 63/146 [00:13<00:11,  7.14it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6634975075721741:   8%|█            | 32/383 [00:09<01:22,  4.23it/s]Epoch: 1, train for the 64-th batch, train loss: 0.6354463696479797:  44%|█████▋       | 64/146 [00:13<00:11,  6.90it/s]Epoch: 1, train for the 33-th batch, train loss: 0.6634975075721741:   9%|█            | 33/383 [00:09<01:38,  3.56it/s]Epoch: 1, train for the 64-th batch, train loss: 0.5465591549873352:  53%|██████▉      | 63/119 [00:12<00:08,  6.51it/s]Epoch: 1, train for the 64-th batch, train loss: 0.5465591549873352:  54%|██████▉      | 64/119 [00:12<00:08,  6.74it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6193863153457642:  19%|██▍          | 45/237 [00:11<00:46,  4.13it/s]Epoch: 1, train for the 65-th batch, train loss: 0.6453993320465088:  44%|█████▋       | 64/146 [00:13<00:11,  6.90it/s]Epoch: 1, train for the 46-th batch, train loss: 0.6193863153457642:  19%|██▌          | 46/237 [00:11<00:41,  4.55it/s]Epoch: 1, train for the 65-th batch, train loss: 0.6453993320465088:  45%|█████▊       | 65/146 [00:13<00:11,  6.91it/s]Epoch: 1, train for the 57-th batch, train loss: 0.5152565836906433:  23%|███          | 56/241 [00:11<00:28,  6.51it/s]Epoch: 1, train for the 57-th batch, train loss: 0.5152565836906433:  24%|███          | 57/241 [00:11<00:29,  6.27it/s]Epoch: 1, train for the 54-th batch, train loss: 0.6655824780464172:  35%|████▌        | 53/151 [00:12<00:23,  4.12it/s]Epoch: 1, train for the 54-th batch, train loss: 0.6655824780464172:  36%|████▋        | 54/151 [00:12<00:22,  4.28it/s]Epoch: 1, train for the 66-th batch, train loss: 0.6219077706336975:  45%|█████▊       | 65/146 [00:13<00:11,  6.91it/s]Epoch: 1, train for the 66-th batch, train loss: 0.6219077706336975:  45%|█████▉       | 66/146 [00:13<00:10,  7.32it/s]Epoch: 1, train for the 55-th batch, train loss: 0.6679431796073914:  36%|████▋        | 54/151 [00:12<00:22,  4.28it/s]Epoch: 1, train for the 55-th batch, train loss: 0.6679431796073914:  36%|████▋        | 55/151 [00:12<00:19,  5.03it/s]Epoch: 1, train for the 56-th batch, train loss: 0.6332443952560425:  36%|████▋        | 55/151 [00:12<00:19,  5.03it/s]Epoch: 1, train for the 56-th batch, train loss: 0.6332443952560425:  37%|████▊        | 56/151 [00:12<00:16,  5.72it/s]Epoch: 1, train for the 65-th batch, train loss: 0.5258755087852478:  54%|██████▉      | 64/119 [00:12<00:08,  6.74it/s]Epoch: 1, train for the 58-th batch, train loss: 0.4373061954975128:  24%|███          | 57/241 [00:12<00:29,  6.27it/s]Epoch: 1, train for the 65-th batch, train loss: 0.5258755087852478:  55%|███████      | 65/119 [00:12<00:11,  4.81it/s]Epoch: 1, train for the 58-th batch, train loss: 0.4373061954975128:  24%|███▏         | 58/241 [00:12<00:36,  5.07it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6417057514190674:  19%|██▌          | 46/237 [00:11<00:41,  4.55it/s]Epoch: 1, train for the 34-th batch, train loss: 0.632737398147583:   9%|█▏            | 33/383 [00:09<01:38,  3.56it/s]Epoch: 1, train for the 47-th batch, train loss: 0.6417057514190674:  20%|██▌          | 47/237 [00:11<00:48,  3.95it/s]Epoch: 1, train for the 34-th batch, train loss: 0.632737398147583:   9%|█▏            | 34/383 [00:09<01:54,  3.04it/s]Epoch: 1, train for the 67-th batch, train loss: 0.6289806962013245:  45%|█████▉       | 66/146 [00:13<00:10,  7.32it/s]Epoch: 1, train for the 67-th batch, train loss: 0.6289806962013245:  46%|█████▉       | 67/146 [00:13<00:12,  6.12it/s]Epoch: 1, train for the 57-th batch, train loss: 0.6485725045204163:  37%|████▊        | 56/151 [00:12<00:16,  5.72it/s]Epoch: 1, train for the 66-th batch, train loss: 0.5316542983055115:  55%|███████      | 65/119 [00:12<00:11,  4.81it/s]Epoch: 1, train for the 57-th batch, train loss: 0.6485725045204163:  38%|████▉        | 57/151 [00:12<00:16,  5.86it/s]Epoch: 1, train for the 66-th batch, train loss: 0.5316542983055115:  55%|███████▏     | 66/119 [00:12<00:10,  5.20it/s]Epoch: 1, train for the 59-th batch, train loss: 0.641417920589447:  24%|███▎          | 58/241 [00:12<00:36,  5.07it/s]Epoch: 1, train for the 48-th batch, train loss: 0.6366458535194397:  20%|██▌          | 47/237 [00:11<00:48,  3.95it/s]Epoch: 1, train for the 59-th batch, train loss: 0.641417920589447:  24%|███▍          | 59/241 [00:12<00:34,  5.21it/s]Epoch: 1, train for the 68-th batch, train loss: 0.6231057643890381:  46%|█████▉       | 67/146 [00:13<00:12,  6.12it/s]Epoch: 1, train for the 48-th batch, train loss: 0.6366458535194397:  20%|██▋          | 48/237 [00:11<00:43,  4.38it/s]Epoch: 1, train for the 68-th batch, train loss: 0.6231057643890381:  47%|██████       | 68/146 [00:13<00:12,  6.22it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6354662179946899:   9%|█▏           | 34/383 [00:10<01:54,  3.04it/s]Epoch: 1, train for the 35-th batch, train loss: 0.6354662179946899:   9%|█▏           | 35/383 [00:10<01:42,  3.40it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6270061135292053:   9%|█▏           | 35/383 [00:10<01:42,  3.40it/s]Epoch: 1, train for the 67-th batch, train loss: 0.6120514273643494:  55%|███████▏     | 66/119 [00:13<00:10,  5.20it/s]Epoch: 1, train for the 67-th batch, train loss: 0.6120514273643494:  56%|███████▎     | 67/119 [00:13<00:10,  5.03it/s]Epoch: 1, train for the 36-th batch, train loss: 0.6270061135292053:   9%|█▏           | 36/383 [00:10<01:25,  4.06it/s]Epoch: 1, train for the 58-th batch, train loss: 0.6487414240837097:  38%|████▉        | 57/151 [00:13<00:16,  5.86it/s]Epoch: 1, train for the 58-th batch, train loss: 0.6487414240837097:  38%|████▉        | 58/151 [00:13<00:18,  5.07it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6369451284408569:  20%|██▋          | 48/237 [00:12<00:43,  4.38it/s]Epoch: 1, train for the 49-th batch, train loss: 0.6369451284408569:  21%|██▋          | 49/237 [00:12<00:45,  4.10it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5424759387969971:  56%|███████▎     | 67/119 [00:13<00:10,  5.03it/s]Epoch: 1, train for the 60-th batch, train loss: 0.6828291416168213:  24%|███▏         | 59/241 [00:12<00:34,  5.21it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5424759387969971:  57%|███████▍     | 68/119 [00:13<00:09,  5.59it/s]Epoch: 1, train for the 60-th batch, train loss: 0.6828291416168213:  25%|███▏         | 60/241 [00:12<00:42,  4.31it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6082735061645508:   9%|█▏           | 36/383 [00:10<01:25,  4.06it/s]Epoch: 1, train for the 69-th batch, train loss: 0.611284613609314:  47%|██████▌       | 68/146 [00:14<00:12,  6.22it/s]Epoch: 1, train for the 69-th batch, train loss: 0.611284613609314:  47%|██████▌       | 69/146 [00:14<00:16,  4.66it/s]Epoch: 1, train for the 37-th batch, train loss: 0.6082735061645508:  10%|█▎           | 37/383 [00:10<01:17,  4.48it/s]Epoch: 1, train for the 59-th batch, train loss: 0.6528686285018921:  38%|████▉        | 58/151 [00:13<00:18,  5.07it/s]Epoch: 1, train for the 69-th batch, train loss: 0.6258057951927185:  57%|███████▍     | 68/119 [00:13<00:09,  5.59it/s]Epoch: 1, train for the 59-th batch, train loss: 0.6528686285018921:  39%|█████        | 59/151 [00:13<00:18,  5.03it/s]Epoch: 1, train for the 69-th batch, train loss: 0.6258057951927185:  58%|███████▌     | 69/119 [00:13<00:07,  6.29it/s]Epoch: 1, train for the 50-th batch, train loss: 0.638617753982544:  21%|██▉           | 49/237 [00:12<00:45,  4.10it/s]Epoch: 1, train for the 50-th batch, train loss: 0.638617753982544:  21%|██▉           | 50/237 [00:12<00:41,  4.52it/s]Epoch: 1, train for the 70-th batch, train loss: 0.6228346824645996:  47%|██████▏      | 69/146 [00:14<00:16,  4.66it/s]Epoch: 1, train for the 70-th batch, train loss: 0.6228346824645996:  48%|██████▏      | 70/146 [00:14<00:14,  5.26it/s]Epoch: 1, train for the 38-th batch, train loss: 0.630377471446991:  10%|█▎            | 37/383 [00:10<01:17,  4.48it/s]Epoch: 1, train for the 60-th batch, train loss: 0.6574066281318665:  39%|█████        | 59/151 [00:13<00:18,  5.03it/s]Epoch: 1, train for the 38-th batch, train loss: 0.630377471446991:  10%|█▍            | 38/383 [00:10<01:11,  4.79it/s]Epoch: 1, train for the 60-th batch, train loss: 0.6574066281318665:  40%|█████▏       | 60/151 [00:13<00:15,  5.72it/s]Epoch: 1, train for the 61-th batch, train loss: 0.6229292154312134:  25%|███▏         | 60/241 [00:13<00:42,  4.31it/s]Epoch: 1, train for the 61-th batch, train loss: 0.6229292154312134:  25%|███▎         | 61/241 [00:13<00:47,  3.79it/s]Epoch: 1, train for the 70-th batch, train loss: 0.554372251033783:  58%|████████      | 69/119 [00:13<00:07,  6.29it/s]Epoch: 1, train for the 70-th batch, train loss: 0.554372251033783:  59%|████████▏     | 70/119 [00:13<00:09,  4.96it/s]Epoch: 1, train for the 61-th batch, train loss: 0.6462089419364929:  40%|█████▏       | 60/151 [00:13<00:15,  5.72it/s]Epoch: 1, train for the 61-th batch, train loss: 0.6462089419364929:  40%|█████▎       | 61/151 [00:13<00:16,  5.41it/s]Epoch: 1, train for the 51-th batch, train loss: 0.629958987236023:  21%|██▉           | 50/237 [00:12<00:41,  4.52it/s]Epoch: 1, train for the 39-th batch, train loss: 0.5764803886413574:  10%|█▎           | 38/383 [00:10<01:11,  4.79it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6506713628768921:  25%|███▎         | 61/241 [00:13<00:47,  3.79it/s]Epoch: 1, train for the 51-th batch, train loss: 0.629958987236023:  22%|███           | 51/237 [00:12<00:46,  4.02it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6506713628768921:  26%|███▎         | 62/241 [00:13<00:39,  4.53it/s]Epoch: 1, train for the 39-th batch, train loss: 0.5764803886413574:  10%|█▎           | 39/383 [00:10<01:17,  4.44it/s]Epoch: 1, train for the 71-th batch, train loss: 0.5448659658432007:  59%|███████▋     | 70/119 [00:13<00:09,  4.96it/s]Epoch: 1, train for the 71-th batch, train loss: 0.588312029838562:  48%|██████▋       | 70/146 [00:14<00:14,  5.26it/s]Epoch: 1, train for the 71-th batch, train loss: 0.588312029838562:  49%|██████▊       | 71/146 [00:14<00:19,  3.93it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6082712411880493:  22%|██▊          | 51/237 [00:12<00:46,  4.02it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6538587212562561:  40%|█████▎       | 61/151 [00:13<00:16,  5.41it/s]Epoch: 1, train for the 52-th batch, train loss: 0.6082712411880493:  22%|██▊          | 52/237 [00:12<00:40,  4.58it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6538587212562561:  41%|█████▎       | 62/151 [00:13<00:16,  5.51it/s]Epoch: 1, train for the 63-th batch, train loss: 0.7171078324317932:  26%|███▎         | 62/241 [00:13<00:39,  4.53it/s]Epoch: 1, train for the 63-th batch, train loss: 0.7171078324317932:  26%|███▍         | 63/241 [00:13<00:36,  4.91it/s]Epoch: 1, train for the 63-th batch, train loss: 0.6438454389572144:  41%|█████▎       | 62/151 [00:13<00:16,  5.51it/s]Epoch: 1, train for the 63-th batch, train loss: 0.6438454389572144:  42%|█████▍       | 63/151 [00:13<00:14,  6.05it/s]Epoch: 1, train for the 64-th batch, train loss: 0.6326923370361328:  42%|█████▍       | 63/151 [00:14<00:14,  6.05it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5625973343849182:  59%|███████▋     | 70/119 [00:14<00:09,  4.96it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5625973343849182:  61%|███████▊     | 72/119 [00:14<00:10,  4.65it/s]Epoch: 1, train for the 64-th batch, train loss: 0.3755732774734497:  26%|███▍         | 63/241 [00:13<00:36,  4.91it/s]Epoch: 1, train for the 64-th batch, train loss: 0.3755732774734497:  27%|███▍         | 64/241 [00:13<00:39,  4.47it/s]Epoch: 1, train for the 40-th batch, train loss: 0.5951055884361267:  10%|█▎           | 39/383 [00:11<01:17,  4.44it/s]Epoch: 1, train for the 53-th batch, train loss: 0.6178497672080994:  22%|██▊          | 52/237 [00:13<00:40,  4.58it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5932059288024902:  49%|██████▎      | 71/146 [00:15<00:19,  3.93it/s]Epoch: 1, train for the 53-th batch, train loss: 0.6178497672080994:  22%|██▉          | 53/237 [00:13<00:45,  4.01it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5932059288024902:  49%|██████▍      | 72/146 [00:15<00:21,  3.49it/s]Epoch: 1, train for the 40-th batch, train loss: 0.5951055884361267:  10%|█▎           | 40/383 [00:11<01:41,  3.38it/s]Epoch: 1, train for the 65-th batch, train loss: 0.6068964004516602:  42%|█████▍       | 63/151 [00:14<00:14,  6.05it/s]Epoch: 1, train for the 65-th batch, train loss: 0.6068964004516602:  43%|█████▌       | 65/151 [00:14<00:12,  6.80it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5208116769790649:  61%|███████▊     | 72/119 [00:14<00:10,  4.65it/s]Epoch: 1, train for the 65-th batch, train loss: 0.41893213987350464:  27%|███▏        | 64/241 [00:13<00:39,  4.47it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5208116769790649:  61%|███████▉     | 73/119 [00:14<00:09,  5.11it/s]Epoch: 1, train for the 65-th batch, train loss: 0.41893213987350464:  27%|███▏        | 65/241 [00:13<00:34,  5.13it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5911335349082947:  49%|██████▍      | 72/146 [00:15<00:21,  3.49it/s]Epoch: 1, train for the 54-th batch, train loss: 0.5721738338470459:  22%|██▉          | 53/237 [00:13<00:45,  4.01it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5911335349082947:  50%|██████▌      | 73/146 [00:15<00:17,  4.15it/s]Epoch: 1, train for the 54-th batch, train loss: 0.5721738338470459:  23%|██▉          | 54/237 [00:13<00:40,  4.48it/s]Epoch: 1, train for the 41-th batch, train loss: 0.5473180413246155:  10%|█▎           | 40/383 [00:11<01:41,  3.38it/s]Epoch: 1, train for the 41-th batch, train loss: 0.5473180413246155:  11%|█▍           | 41/383 [00:11<01:30,  3.77it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5794523358345032:  50%|██████▌      | 73/146 [00:15<00:17,  4.15it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5432369709014893:  61%|███████▉     | 73/119 [00:14<00:09,  5.11it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5432369709014893:  62%|████████     | 74/119 [00:14<00:09,  4.78it/s]Epoch: 1, train for the 66-th batch, train loss: 0.38540950417518616:  27%|███▏        | 65/241 [00:13<00:34,  5.13it/s]Epoch: 1, train for the 75-th batch, train loss: 0.5891141891479492:  50%|██████▌      | 73/146 [00:15<00:17,  4.15it/s]Epoch: 1, train for the 75-th batch, train loss: 0.5891141891479492:  51%|██████▋      | 75/146 [00:15<00:13,  5.42it/s]Epoch: 1, train for the 66-th batch, train loss: 0.38540950417518616:  27%|███▎        | 66/241 [00:13<00:38,  4.52it/s]Epoch: 1, train for the 66-th batch, train loss: 0.6338199973106384:  43%|█████▌       | 65/151 [00:14<00:12,  6.80it/s]Epoch: 1, train for the 66-th batch, train loss: 0.6338199973106384:  44%|█████▋       | 66/151 [00:14<00:17,  4.89it/s]Epoch: 1, train for the 75-th batch, train loss: 0.5366492867469788:  62%|████████     | 74/119 [00:14<00:09,  4.78it/s]Epoch: 1, train for the 75-th batch, train loss: 0.5366492867469788:  63%|████████▏    | 75/119 [00:14<00:08,  5.41it/s]Epoch: 1, train for the 42-th batch, train loss: 0.5926491618156433:  11%|█▍           | 41/383 [00:11<01:30,  3.77it/s]Epoch: 1, train for the 55-th batch, train loss: 0.6270222663879395:  23%|██▉          | 54/237 [00:13<00:40,  4.48it/s]Epoch: 1, train for the 42-th batch, train loss: 0.5926491618156433:  11%|█▍           | 42/383 [00:11<01:30,  3.77it/s]Epoch: 1, train for the 55-th batch, train loss: 0.6270222663879395:  23%|███          | 55/237 [00:13<00:46,  3.94it/s]Epoch: 1, train for the 76-th batch, train loss: 0.5697534084320068:  51%|██████▋      | 75/146 [00:15<00:13,  5.42it/s]Epoch: 1, train for the 76-th batch, train loss: 0.5697534084320068:  52%|██████▊      | 76/146 [00:15<00:11,  5.94it/s]Epoch: 1, train for the 67-th batch, train loss: 0.47519469261169434:  27%|███▎        | 66/241 [00:14<00:38,  4.52it/s]Epoch: 1, train for the 67-th batch, train loss: 0.47519469261169434:  28%|███▎        | 67/241 [00:14<00:34,  5.05it/s]Epoch: 1, train for the 67-th batch, train loss: 0.6432865858078003:  44%|█████▋       | 66/151 [00:14<00:17,  4.89it/s]Epoch: 1, train for the 76-th batch, train loss: 0.569945216178894:  63%|████████▊     | 75/119 [00:14<00:08,  5.41it/s]Epoch: 1, train for the 67-th batch, train loss: 0.6432865858078003:  44%|█████▊       | 67/151 [00:14<00:15,  5.38it/s]Epoch: 1, train for the 76-th batch, train loss: 0.569945216178894:  64%|████████▉     | 76/119 [00:14<00:07,  5.94it/s]Epoch: 1, train for the 77-th batch, train loss: 0.5909823179244995:  52%|██████▊      | 76/146 [00:15<00:11,  5.94it/s]Epoch: 1, train for the 77-th batch, train loss: 0.5909823179244995:  53%|██████▊      | 77/146 [00:15<00:11,  6.25it/s]Epoch: 1, train for the 56-th batch, train loss: 0.6267480850219727:  23%|███          | 55/237 [00:13<00:46,  3.94it/s]Epoch: 1, train for the 56-th batch, train loss: 0.6267480850219727:  24%|███          | 56/237 [00:13<00:41,  4.37it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6448236703872681:  11%|█▍           | 42/383 [00:11<01:30,  3.77it/s]Epoch: 1, train for the 43-th batch, train loss: 0.6448236703872681:  11%|█▍           | 43/383 [00:12<01:26,  3.94it/s]Epoch: 1, train for the 77-th batch, train loss: 0.5098796486854553:  64%|████████▎    | 76/119 [00:14<00:07,  5.94it/s]Epoch: 1, train for the 77-th batch, train loss: 0.5098796486854553:  65%|████████▍    | 77/119 [00:14<00:06,  6.30it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5892764329910278:  53%|██████▊      | 77/146 [00:15<00:11,  6.25it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5892764329910278:  53%|██████▉      | 78/146 [00:15<00:10,  6.76it/s]Epoch: 1, train for the 68-th batch, train loss: 0.565457284450531:  28%|███▉          | 67/241 [00:14<00:34,  5.05it/s]Epoch: 1, train for the 68-th batch, train loss: 0.565457284450531:  28%|███▉          | 68/241 [00:14<00:40,  4.31it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5711650848388672:  53%|██████▉      | 78/146 [00:15<00:10,  6.76it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5711650848388672:  54%|███████      | 79/146 [00:15<00:09,  7.30it/s]Epoch: 1, train for the 68-th batch, train loss: 0.6267120838165283:  44%|█████▊       | 67/151 [00:14<00:15,  5.38it/s]Epoch: 1, train for the 68-th batch, train loss: 0.6267120838165283:  45%|█████▊       | 68/151 [00:15<00:18,  4.57it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5454642176628113:  65%|████████▍    | 77/119 [00:15<00:06,  6.30it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5454642176628113:  66%|████████▌    | 78/119 [00:15<00:06,  5.94it/s]Epoch: 1, train for the 57-th batch, train loss: 0.600898027420044:  24%|███▎          | 56/237 [00:14<00:41,  4.37it/s]Epoch: 1, train for the 69-th batch, train loss: 0.681678056716919:  28%|███▉          | 68/241 [00:14<00:40,  4.31it/s]Epoch: 1, train for the 44-th batch, train loss: 0.5698503851890564:  11%|█▍           | 43/383 [00:12<01:26,  3.94it/s]Epoch: 1, train for the 57-th batch, train loss: 0.600898027420044:  24%|███▎          | 57/237 [00:14<00:45,  3.98it/s]Epoch: 1, train for the 69-th batch, train loss: 0.681678056716919:  29%|████          | 69/241 [00:14<00:34,  5.02it/s]Epoch: 1, train for the 44-th batch, train loss: 0.5698503851890564:  11%|█▍           | 44/383 [00:12<01:28,  3.84it/s]Epoch: 1, train for the 80-th batch, train loss: 0.5789366960525513:  54%|███████      | 79/146 [00:16<00:09,  7.30it/s]Epoch: 1, train for the 80-th batch, train loss: 0.5789366960525513:  55%|███████      | 80/146 [00:16<00:08,  7.49it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5368129014968872:  66%|████████▌    | 78/119 [00:15<00:06,  5.94it/s]Epoch: 1, train for the 69-th batch, train loss: 0.6242634654045105:  45%|█████▊       | 68/151 [00:15<00:18,  4.57it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5368129014968872:  66%|████████▋    | 79/119 [00:15<00:05,  6.67it/s]Epoch: 1, train for the 69-th batch, train loss: 0.6242634654045105:  46%|█████▉       | 69/151 [00:15<00:16,  5.10it/s]Epoch: 1, train for the 58-th batch, train loss: 0.6182196140289307:  24%|███▏         | 57/237 [00:14<00:45,  3.98it/s]Epoch: 1, train for the 58-th batch, train loss: 0.6182196140289307:  24%|███▏         | 58/237 [00:14<00:41,  4.31it/s]Epoch: 1, train for the 70-th batch, train loss: 0.6118218898773193:  46%|█████▉       | 69/151 [00:15<00:16,  5.10it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5360358953475952:  55%|███████      | 80/146 [00:16<00:08,  7.49it/s]Epoch: 1, train for the 45-th batch, train loss: 0.5698237419128418:  11%|█▍           | 44/383 [00:12<01:28,  3.84it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5360358953475952:  55%|███████▏     | 81/146 [00:16<00:09,  6.73it/s]Epoch: 1, train for the 70-th batch, train loss: 0.6118218898773193:  46%|██████       | 70/151 [00:15<00:15,  5.40it/s]Epoch: 1, train for the 45-th batch, train loss: 0.5698237419128418:  12%|█▌           | 45/383 [00:12<01:22,  4.09it/s]Epoch: 1, train for the 70-th batch, train loss: 0.540681004524231:  29%|████          | 69/241 [00:14<00:34,  5.02it/s]Epoch: 1, train for the 70-th batch, train loss: 0.540681004524231:  29%|████          | 70/241 [00:14<00:36,  4.72it/s]Epoch: 1, train for the 80-th batch, train loss: 0.549684464931488:  66%|█████████▎    | 79/119 [00:15<00:05,  6.67it/s]Epoch: 1, train for the 80-th batch, train loss: 0.549684464931488:  67%|█████████▍    | 80/119 [00:15<00:08,  4.73it/s]Epoch: 1, train for the 71-th batch, train loss: 0.6439575552940369:  29%|███▊         | 70/241 [00:14<00:36,  4.72it/s]Epoch: 1, train for the 71-th batch, train loss: 0.6439575552940369:  29%|███▊         | 71/241 [00:14<00:35,  4.84it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5757192373275757:  55%|███████▏     | 81/146 [00:16<00:09,  6.73it/s]Epoch: 1, train for the 71-th batch, train loss: 0.6289486289024353:  46%|██████       | 70/151 [00:15<00:15,  5.40it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5757192373275757:  56%|███████▎     | 82/146 [00:16<00:11,  5.59it/s]Epoch: 1, train for the 59-th batch, train loss: 0.6299830675125122:  24%|███▏         | 58/237 [00:14<00:41,  4.31it/s]Epoch: 1, train for the 46-th batch, train loss: 0.5992761850357056:  12%|█▌           | 45/383 [00:12<01:22,  4.09it/s]Epoch: 1, train for the 71-th batch, train loss: 0.6289486289024353:  47%|██████       | 71/151 [00:15<00:16,  4.76it/s]Epoch: 1, train for the 46-th batch, train loss: 0.5992761850357056:  12%|█▌           | 46/383 [00:12<01:24,  3.98it/s]Epoch: 1, train for the 59-th batch, train loss: 0.6299830675125122:  25%|███▏         | 59/237 [00:14<00:45,  3.93it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5212770104408264:  67%|████████▋    | 80/119 [00:15<00:08,  4.73it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5212770104408264:  68%|████████▊    | 81/119 [00:15<00:07,  5.36it/s]Epoch: 1, train for the 72-th batch, train loss: 0.6155356168746948:  29%|███▊         | 71/241 [00:15<00:35,  4.84it/s]Epoch: 1, train for the 72-th batch, train loss: 0.6155356168746948:  30%|███▉         | 72/241 [00:15<00:31,  5.33it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5534009337425232:  56%|███████▎     | 82/146 [00:16<00:11,  5.59it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5534009337425232:  57%|███████▍     | 83/146 [00:16<00:10,  6.07it/s]Epoch: 1, train for the 47-th batch, train loss: 0.503240704536438:  12%|█▋            | 46/383 [00:12<01:24,  3.98it/s]Epoch: 1, train for the 47-th batch, train loss: 0.503240704536438:  12%|█▋            | 47/383 [00:12<01:16,  4.39it/s]Epoch: 1, train for the 60-th batch, train loss: 0.5917412638664246:  25%|███▏         | 59/237 [00:14<00:45,  3.93it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5779644846916199:  30%|███▉         | 72/241 [00:15<00:31,  5.33it/s]Epoch: 1, train for the 84-th batch, train loss: 0.503982424736023:  57%|███████▉      | 83/146 [00:16<00:10,  6.07it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5760660171508789:  68%|████████▊    | 81/119 [00:15<00:07,  5.36it/s]Epoch: 1, train for the 60-th batch, train loss: 0.5917412638664246:  25%|███▎         | 60/237 [00:14<00:43,  4.05it/s]Epoch: 1, train for the 84-th batch, train loss: 0.503982424736023:  58%|████████      | 84/146 [00:16<00:09,  6.49it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5760660171508789:  69%|████████▉    | 82/119 [00:15<00:06,  5.33it/s]Epoch: 1, train for the 72-th batch, train loss: 0.6048929691314697:  47%|██████       | 71/151 [00:15<00:16,  4.76it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5779644846916199:  30%|███▉         | 73/241 [00:15<00:29,  5.68it/s]Epoch: 1, train for the 72-th batch, train loss: 0.6048929691314697:  48%|██████▏      | 72/151 [00:15<00:17,  4.46it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5303536653518677:  69%|████████▉    | 82/119 [00:15<00:06,  5.33it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5303536653518677:  70%|█████████    | 83/119 [00:15<00:06,  5.97it/s]Epoch: 1, train for the 61-th batch, train loss: 0.6098294854164124:  25%|███▎         | 60/237 [00:14<00:43,  4.05it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5394949913024902:  30%|███▉         | 73/241 [00:15<00:29,  5.68it/s]Epoch: 1, train for the 61-th batch, train loss: 0.6098294854164124:  26%|███▎         | 61/237 [00:14<00:39,  4.46it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5394949913024902:  31%|███▉         | 74/241 [00:15<00:29,  5.75it/s]Epoch: 1, train for the 73-th batch, train loss: 0.60843425989151:  48%|███████▏       | 72/151 [00:16<00:17,  4.46it/s]Epoch: 1, train for the 48-th batch, train loss: 0.5422139763832092:  12%|█▌           | 47/383 [00:13<01:16,  4.39it/s]Epoch: 1, train for the 73-th batch, train loss: 0.60843425989151:  48%|███████▎       | 73/151 [00:16<00:16,  4.61it/s]Epoch: 1, train for the 48-th batch, train loss: 0.5422139763832092:  13%|█▋           | 48/383 [00:13<01:22,  4.05it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5200584530830383:  58%|███████▍     | 84/146 [00:16<00:09,  6.49it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5200584530830383:  58%|███████▌     | 85/146 [00:16<00:11,  5.49it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5062366724014282:  70%|█████████    | 83/119 [00:16<00:06,  5.97it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5062366724014282:  71%|█████████▏   | 84/119 [00:16<00:05,  6.27it/s]Epoch: 1, train for the 75-th batch, train loss: 0.4348814785480499:  31%|███▉         | 74/241 [00:15<00:29,  5.75it/s]Epoch: 1, train for the 75-th batch, train loss: 0.4348814785480499:  31%|████         | 75/241 [00:15<00:26,  6.34it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6068528890609741:  26%|███▎         | 61/237 [00:15<00:39,  4.46it/s]Epoch: 1, train for the 62-th batch, train loss: 0.6068528890609741:  26%|███▍         | 62/237 [00:15<00:36,  4.86it/s]Epoch: 1, train for the 76-th batch, train loss: 0.2982289493083954:  31%|████         | 75/241 [00:15<00:26,  6.34it/s]Epoch: 1, train for the 76-th batch, train loss: 0.2982289493083954:  32%|████         | 76/241 [00:15<00:23,  6.94it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5327691435813904:  71%|█████████▏   | 84/119 [00:16<00:05,  6.27it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5327691435813904:  71%|█████████▎   | 85/119 [00:16<00:05,  6.28it/s]Epoch: 1, train for the 63-th batch, train loss: 0.6352366209030151:  26%|███▍         | 62/237 [00:15<00:36,  4.86it/s]Epoch: 1, train for the 63-th batch, train loss: 0.6352366209030151:  27%|███▍         | 63/237 [00:15<00:32,  5.41it/s]Epoch: 1, train for the 86-th batch, train loss: 0.557216465473175:  58%|████████▏     | 85/146 [00:17<00:11,  5.49it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5842668414115906:  48%|██████▎      | 73/151 [00:16<00:16,  4.61it/s]Epoch: 1, train for the 86-th batch, train loss: 0.557216465473175:  59%|████████▏     | 86/146 [00:17<00:12,  4.73it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5842668414115906:  49%|██████▎      | 74/151 [00:16<00:18,  4.07it/s]Epoch: 1, train for the 49-th batch, train loss: 0.5887061953544617:  13%|█▋           | 48/383 [00:13<01:22,  4.05it/s]Epoch: 1, train for the 49-th batch, train loss: 0.5887061953544617:  13%|█▋           | 49/383 [00:13<01:31,  3.64it/s]Epoch: 1, train for the 77-th batch, train loss: 0.4069525897502899:  32%|████         | 76/241 [00:15<00:23,  6.94it/s]Epoch: 1, train for the 64-th batch, train loss: 0.6067391633987427:  27%|███▍         | 63/237 [00:15<00:32,  5.41it/s]Epoch: 1, train for the 77-th batch, train loss: 0.4069525897502899:  32%|████▏        | 77/241 [00:15<00:26,  6.13it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5471494793891907:  71%|█████████▎   | 85/119 [00:16<00:05,  6.28it/s]Epoch: 1, train for the 64-th batch, train loss: 0.6067391633987427:  27%|███▌         | 64/237 [00:15<00:31,  5.57it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5471494793891907:  72%|█████████▍   | 86/119 [00:16<00:05,  5.59it/s]Epoch: 1, train for the 75-th batch, train loss: 0.5812357068061829:  49%|██████▎      | 74/151 [00:16<00:18,  4.07it/s]Epoch: 1, train for the 87-th batch, train loss: 0.6272739171981812:  59%|███████▋     | 86/146 [00:17<00:12,  4.73it/s]Epoch: 1, train for the 75-th batch, train loss: 0.5812357068061829:  50%|██████▍      | 75/151 [00:16<00:16,  4.66it/s]Epoch: 1, train for the 87-th batch, train loss: 0.6272739171981812:  60%|███████▋     | 87/146 [00:17<00:11,  5.18it/s]Epoch: 1, train for the 78-th batch, train loss: 0.639079749584198:  32%|████▍         | 77/241 [00:16<00:26,  6.13it/s]Epoch: 1, train for the 50-th batch, train loss: 0.41983896493911743:  13%|█▌          | 49/383 [00:13<01:31,  3.64it/s]Epoch: 1, train for the 78-th batch, train loss: 0.639079749584198:  32%|████▌         | 78/241 [00:16<00:25,  6.41it/s]Epoch: 1, train for the 50-th batch, train loss: 0.41983896493911743:  13%|█▌          | 50/383 [00:13<01:23,  3.96it/s]Epoch: 1, train for the 65-th batch, train loss: 0.6031576991081238:  27%|███▌         | 64/237 [00:15<00:31,  5.57it/s]Epoch: 1, train for the 65-th batch, train loss: 0.6031576991081238:  27%|███▌         | 65/237 [00:15<00:29,  5.76it/s]Epoch: 1, train for the 76-th batch, train loss: 0.6122475862503052:  50%|██████▍      | 75/151 [00:16<00:16,  4.66it/s]Epoch: 1, train for the 76-th batch, train loss: 0.6122475862503052:  50%|██████▌      | 76/151 [00:16<00:14,  5.03it/s]Epoch: 1, train for the 87-th batch, train loss: 0.530057430267334:  72%|██████████    | 86/119 [00:16<00:05,  5.59it/s]Epoch: 1, train for the 88-th batch, train loss: 0.6152317523956299:  60%|███████▋     | 87/146 [00:17<00:11,  5.18it/s]Epoch: 1, train for the 87-th batch, train loss: 0.530057430267334:  73%|██████████▏   | 87/119 [00:16<00:06,  5.33it/s]Epoch: 1, train for the 88-th batch, train loss: 0.6152317523956299:  60%|███████▊     | 88/146 [00:17<00:10,  5.30it/s]Epoch: 1, train for the 51-th batch, train loss: 0.44454941153526306:  13%|█▌          | 50/383 [00:13<01:23,  3.96it/s]Epoch: 1, train for the 51-th batch, train loss: 0.44454941153526306:  13%|█▌          | 51/383 [00:13<01:15,  4.42it/s]Epoch: 1, train for the 88-th batch, train loss: 0.542693018913269:  73%|██████████▏   | 87/119 [00:16<00:06,  5.33it/s]Epoch: 1, train for the 88-th batch, train loss: 0.542693018913269:  74%|██████████▎   | 88/119 [00:16<00:05,  5.79it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5514017939567566:  32%|████▏        | 78/241 [00:16<00:25,  6.41it/s]Epoch: 1, train for the 66-th batch, train loss: 0.6038360595703125:  27%|███▌         | 65/237 [00:15<00:29,  5.76it/s]Epoch: 1, train for the 77-th batch, train loss: 0.5884057283401489:  50%|██████▌      | 76/151 [00:16<00:14,  5.03it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5514017939567566:  33%|████▎        | 79/241 [00:16<00:30,  5.30it/s]Epoch: 1, train for the 66-th batch, train loss: 0.6038360595703125:  28%|███▌         | 66/237 [00:15<00:32,  5.30it/s]Epoch: 1, train for the 77-th batch, train loss: 0.5884057283401489:  51%|██████▋      | 77/151 [00:16<00:14,  5.06it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5162262320518494:  60%|███████▊     | 88/146 [00:17<00:10,  5.30it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5162262320518494:  61%|███████▉     | 89/146 [00:17<00:10,  5.30it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5557475686073303:  74%|█████████▌   | 88/119 [00:16<00:05,  5.79it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5557475686073303:  75%|█████████▋   | 89/119 [00:16<00:04,  6.18it/s]Epoch: 1, train for the 80-th batch, train loss: 0.4439174234867096:  33%|████▎        | 79/241 [00:16<00:30,  5.30it/s]Epoch: 1, train for the 80-th batch, train loss: 0.4439174234867096:  33%|████▎        | 80/241 [00:16<00:28,  5.74it/s]Epoch: 1, train for the 52-th batch, train loss: 0.46969637274742126:  13%|█▌          | 51/383 [00:14<01:15,  4.42it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5500583052635193:  61%|███████▉     | 89/146 [00:17<00:10,  5.30it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5753517150878906:  51%|██████▋      | 77/151 [00:16<00:14,  5.06it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5500583052635193:  62%|████████     | 90/146 [00:17<00:09,  5.65it/s]Epoch: 1, train for the 67-th batch, train loss: 0.5496861338615417:  28%|███▌         | 66/237 [00:16<00:32,  5.30it/s]Epoch: 1, train for the 52-th batch, train loss: 0.46969637274742126:  14%|█▋          | 52/383 [00:14<01:19,  4.18it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5753517150878906:  52%|██████▋      | 78/151 [00:17<00:13,  5.21it/s]Epoch: 1, train for the 67-th batch, train loss: 0.5496861338615417:  28%|███▋         | 67/237 [00:16<00:32,  5.18it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5506706833839417:  75%|█████████▋   | 89/119 [00:17<00:04,  6.18it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5506706833839417:  76%|█████████▊   | 90/119 [00:17<00:04,  6.58it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5362824201583862:  62%|████████     | 90/146 [00:18<00:09,  5.65it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5362824201583862:  62%|████████     | 91/146 [00:18<00:09,  5.99it/s]Epoch: 1, train for the 91-th batch, train loss: 0.513165295124054:  76%|██████████▌   | 90/119 [00:17<00:04,  6.58it/s]Epoch: 1, train for the 91-th batch, train loss: 0.513165295124054:  76%|██████████▋   | 91/119 [00:17<00:04,  6.74it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5673754811286926:  33%|████▎        | 80/241 [00:16<00:28,  5.74it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5974181890487671:  52%|██████▋      | 78/151 [00:17<00:13,  5.21it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5541237592697144:  28%|███▋         | 67/237 [00:16<00:32,  5.18it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5673754811286926:  34%|████▎        | 81/241 [00:16<00:34,  4.58it/s]Epoch: 1, train for the 53-th batch, train loss: 0.5474222302436829:  14%|█▊           | 52/383 [00:14<01:19,  4.18it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5974181890487671:  52%|██████▊      | 79/151 [00:17<00:15,  4.56it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5541237592697144:  29%|███▋         | 68/237 [00:16<00:36,  4.60it/s]Epoch: 1, train for the 53-th batch, train loss: 0.5474222302436829:  14%|█▊           | 53/383 [00:14<01:25,  3.86it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5088465213775635:  62%|████████     | 91/146 [00:18<00:09,  5.99it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5088465213775635:  63%|████████▏    | 92/146 [00:18<00:09,  5.80it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5323934555053711:  76%|█████████▉   | 91/119 [00:17<00:04,  6.74it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5323934555053711:  77%|██████████   | 92/119 [00:17<00:04,  6.60it/s]Epoch: 1, train for the 82-th batch, train loss: 0.4890891909599304:  34%|████▎        | 81/241 [00:16<00:34,  4.58it/s]Epoch: 1, train for the 54-th batch, train loss: 0.46734747290611267:  14%|█▋          | 53/383 [00:14<01:25,  3.86it/s]Epoch: 1, train for the 82-th batch, train loss: 0.4890891909599304:  34%|████▍        | 82/241 [00:16<00:32,  4.85it/s]Epoch: 1, train for the 80-th batch, train loss: 0.562232494354248:  52%|███████▎      | 79/151 [00:17<00:15,  4.56it/s]Epoch: 1, train for the 54-th batch, train loss: 0.46734747290611267:  14%|█▋          | 54/383 [00:14<01:15,  4.35it/s]Epoch: 1, train for the 80-th batch, train loss: 0.562232494354248:  53%|███████▍      | 80/151 [00:17<00:14,  4.76it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5954767465591431:  63%|████████▏    | 92/146 [00:18<00:09,  5.80it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5954767465591431:  64%|████████▎    | 93/146 [00:18<00:09,  5.74it/s]Epoch: 1, train for the 93-th batch, train loss: 0.48266515135765076:  77%|█████████▎  | 92/119 [00:17<00:04,  6.60it/s]Epoch: 1, train for the 93-th batch, train loss: 0.48266515135765076:  78%|█████████▍  | 93/119 [00:17<00:04,  6.36it/s]Epoch: 1, train for the 69-th batch, train loss: 0.590044379234314:  29%|████          | 68/237 [00:16<00:36,  4.60it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5413872599601746:  34%|████▍        | 82/241 [00:17<00:32,  4.85it/s]Epoch: 1, train for the 69-th batch, train loss: 0.590044379234314:  29%|████          | 69/237 [00:16<00:39,  4.25it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5413872599601746:  34%|████▍        | 83/241 [00:17<00:28,  5.47it/s]Epoch: 1, train for the 70-th batch, train loss: 0.5722857713699341:  29%|███▊         | 69/237 [00:16<00:39,  4.25it/s]Epoch: 1, train for the 70-th batch, train loss: 0.5722857713699341:  30%|███▊         | 70/237 [00:16<00:33,  4.96it/s]Epoch: 1, train for the 94-th batch, train loss: 0.5216314196586609:  78%|██████████▏  | 93/119 [00:17<00:04,  6.36it/s]Epoch: 1, train for the 94-th batch, train loss: 0.5216314196586609:  79%|██████████▎  | 94/119 [00:17<00:04,  6.05it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5870996713638306:  53%|██████▉      | 80/151 [00:17<00:14,  4.76it/s]Epoch: 1, train for the 81-th batch, train loss: 0.5870996713638306:  54%|██████▉      | 81/151 [00:17<00:15,  4.44it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5044481754302979:  79%|██████████▎  | 94/119 [00:17<00:04,  6.05it/s]Epoch: 1, train for the 71-th batch, train loss: 0.5464173555374146:  30%|███▊         | 70/237 [00:16<00:33,  4.96it/s]Epoch: 1, train for the 84-th batch, train loss: 0.6636819243431091:  34%|████▍        | 83/241 [00:17<00:28,  5.47it/s]Epoch: 1, train for the 71-th batch, train loss: 0.5464173555374146:  30%|███▉         | 71/237 [00:16<00:30,  5.44it/s]Epoch: 1, train for the 84-th batch, train loss: 0.6636819243431091:  35%|████▌        | 84/241 [00:17<00:32,  4.87it/s]Epoch: 1, train for the 94-th batch, train loss: 0.5571396350860596:  64%|████████▎    | 93/146 [00:18<00:09,  5.74it/s]Epoch: 1, train for the 55-th batch, train loss: 0.699060320854187:  14%|█▉            | 54/383 [00:15<01:15,  4.35it/s]Epoch: 1, train for the 94-th batch, train loss: 0.5571396350860596:  64%|████████▎    | 94/146 [00:18<00:12,  4.24it/s]Epoch: 1, train for the 55-th batch, train loss: 0.699060320854187:  14%|██            | 55/383 [00:15<01:35,  3.43it/s]Epoch: 1, train for the 96-th batch, train loss: 0.4917323887348175:  79%|██████████▎  | 94/119 [00:17<00:04,  6.05it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5140362977981567:  35%|████▌        | 84/241 [00:17<00:32,  4.87it/s]Epoch: 1, train for the 96-th batch, train loss: 0.4917323887348175:  81%|██████████▍  | 96/119 [00:17<00:03,  6.85it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5140362977981567:  35%|████▌        | 85/241 [00:17<00:28,  5.50it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5669779181480408:  54%|██████▉      | 81/151 [00:18<00:15,  4.44it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5345334410667419:  64%|████████▎    | 94/146 [00:18<00:12,  4.24it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5669779181480408:  54%|███████      | 82/151 [00:18<00:16,  4.10it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5345334410667419:  65%|████████▍    | 95/146 [00:18<00:10,  4.69it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5204839706420898:  35%|████▌        | 85/241 [00:17<00:28,  5.50it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5204839706420898:  36%|████▋        | 86/241 [00:17<00:26,  5.91it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5347462892532349:  30%|███▉         | 71/237 [00:17<00:30,  5.44it/s]Epoch: 1, train for the 56-th batch, train loss: 0.5674936771392822:  14%|█▊           | 55/383 [00:15<01:35,  3.43it/s]Epoch: 1, train for the 56-th batch, train loss: 0.5674936771392822:  15%|█▉           | 56/383 [00:15<01:37,  3.34it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5347462892532349:  30%|███▉         | 72/237 [00:17<00:40,  4.07it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5036324858665466:  81%|██████████▍  | 96/119 [00:18<00:03,  6.85it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5375866293907166:  54%|███████      | 82/151 [00:18<00:16,  4.10it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5036324858665466:  82%|██████████▌  | 97/119 [00:18<00:04,  5.24it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5375866293907166:  55%|███████▏     | 83/151 [00:18<00:17,  3.97it/s]Epoch: 1, train for the 96-th batch, train loss: 0.558555543422699:  65%|█████████     | 95/146 [00:19<00:10,  4.69it/s]Epoch: 1, train for the 87-th batch, train loss: 0.5489810109138489:  36%|████▋        | 86/241 [00:17<00:26,  5.91it/s]Epoch: 1, train for the 96-th batch, train loss: 0.558555543422699:  66%|█████████▏    | 96/146 [00:19<00:11,  4.27it/s]Epoch: 1, train for the 87-th batch, train loss: 0.5489810109138489:  36%|████▋        | 87/241 [00:17<00:28,  5.32it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5460023283958435:  30%|███▉         | 72/237 [00:17<00:40,  4.07it/s]Epoch: 1, train for the 73-th batch, train loss: 0.5460023283958435:  31%|████         | 73/237 [00:17<00:36,  4.53it/s]Epoch: 1, train for the 57-th batch, train loss: 0.49230265617370605:  15%|█▊          | 56/383 [00:15<01:37,  3.34it/s]Epoch: 1, train for the 98-th batch, train loss: 0.4583163261413574:  82%|██████████▌  | 97/119 [00:18<00:04,  5.24it/s]Epoch: 1, train for the 98-th batch, train loss: 0.4583163261413574:  82%|██████████▋  | 98/119 [00:18<00:03,  5.65it/s]Epoch: 1, train for the 57-th batch, train loss: 0.49230265617370605:  15%|█▊          | 57/383 [00:15<01:28,  3.67it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5728368759155273:  66%|████████▌    | 96/146 [00:19<00:11,  4.27it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5849555134773254:  55%|███████▏     | 83/151 [00:18<00:17,  3.97it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5728368759155273:  66%|████████▋    | 97/146 [00:19<00:10,  4.83it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5849555134773254:  56%|███████▏     | 84/151 [00:18<00:15,  4.39it/s]Epoch: 1, train for the 88-th batch, train loss: 0.5699504613876343:  36%|████▋        | 87/241 [00:17<00:28,  5.32it/s]Epoch: 1, train for the 88-th batch, train loss: 0.5699504613876343:  37%|████▋        | 88/241 [00:17<00:27,  5.53it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5271807909011841:  31%|████         | 73/237 [00:17<00:36,  4.53it/s]Epoch: 1, train for the 99-th batch, train loss: 0.5024319887161255:  82%|██████████▋  | 98/119 [00:18<00:03,  5.65it/s]Epoch: 1, train for the 99-th batch, train loss: 0.5024319887161255:  83%|██████████▊  | 99/119 [00:18<00:03,  5.37it/s]Epoch: 1, train for the 74-th batch, train loss: 0.5271807909011841:  31%|████         | 74/237 [00:17<00:37,  4.39it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5462905764579773:  37%|████▋        | 88/241 [00:18<00:27,  5.53it/s]Epoch: 1, train for the 85-th batch, train loss: 0.4950545132160187:  56%|███████▏     | 84/151 [00:18<00:15,  4.39it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5462905764579773:  37%|████▊        | 89/241 [00:18<00:27,  5.47it/s]Epoch: 1, train for the 85-th batch, train loss: 0.4950545132160187:  56%|███████▎     | 85/151 [00:18<00:15,  4.37it/s]Epoch: 1, train for the 58-th batch, train loss: 0.42987388372421265:  15%|█▊          | 57/383 [00:15<01:28,  3.67it/s]Epoch: 1, train for the 58-th batch, train loss: 0.42987388372421265:  15%|█▊          | 58/383 [00:15<01:32,  3.53it/s]Epoch: 1, train for the 75-th batch, train loss: 0.5789150595664978:  31%|████         | 74/237 [00:17<00:37,  4.39it/s]Epoch: 1, train for the 75-th batch, train loss: 0.5789150595664978:  32%|████         | 75/237 [00:17<00:32,  4.93it/s]Epoch: 1, train for the 98-th batch, train loss: 0.557198166847229:  66%|█████████▎    | 97/146 [00:19<00:10,  4.83it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5447748303413391:  37%|████▊        | 89/241 [00:18<00:27,  5.47it/s]Epoch: 1, train for the 98-th batch, train loss: 0.557198166847229:  67%|█████████▍    | 98/146 [00:19<00:11,  4.07it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5102514028549194:  83%|█████████▉  | 99/119 [00:18<00:03,  5.37it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5447748303413391:  37%|████▊        | 90/241 [00:18<00:25,  6.02it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5102514028549194:  84%|█████████▏ | 100/119 [00:18<00:03,  5.30it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5750247836112976:  56%|███████▎     | 85/151 [00:18<00:15,  4.37it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5750247836112976:  57%|███████▍     | 86/151 [00:18<00:13,  4.76it/s]Epoch: 1, train for the 59-th batch, train loss: 0.4890451729297638:  15%|█▉           | 58/383 [00:16<01:32,  3.53it/s]Epoch: 1, train for the 99-th batch, train loss: 0.570904016494751:  67%|█████████▍    | 98/146 [00:19<00:11,  4.07it/s]Epoch: 1, train for the 59-th batch, train loss: 0.4890451729297638:  15%|██           | 59/383 [00:16<01:22,  3.93it/s]Epoch: 1, train for the 99-th batch, train loss: 0.570904016494751:  68%|█████████▍    | 99/146 [00:19<00:09,  4.73it/s]Epoch: 1, train for the 76-th batch, train loss: 0.5670226216316223:  32%|████         | 75/237 [00:18<00:32,  4.93it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5452502369880676:  37%|████▊        | 90/241 [00:18<00:25,  6.02it/s]Epoch: 1, train for the 76-th batch, train loss: 0.5670226216316223:  32%|████▏        | 76/237 [00:18<00:38,  4.16it/s]Epoch: 1, train for the 87-th batch, train loss: 0.544333279132843:  57%|███████▉      | 86/151 [00:19<00:13,  4.76it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5452502369880676:  38%|████▉        | 91/241 [00:18<00:30,  4.90it/s]Epoch: 1, train for the 87-th batch, train loss: 0.544333279132843:  58%|████████      | 87/151 [00:19<00:14,  4.46it/s]Epoch: 1, train for the 101-th batch, train loss: 0.49469611048698425:  84%|████████▍ | 100/119 [00:19<00:03,  5.30it/s]Epoch: 1, train for the 101-th batch, train loss: 0.49469611048698425:  85%|████████▍ | 101/119 [00:19<00:04,  4.17it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5977514982223511:  68%|████████▏   | 99/146 [00:20<00:09,  4.73it/s]Epoch: 1, train for the 60-th batch, train loss: 0.396964967250824:  15%|██▏           | 59/383 [00:16<01:22,  3.93it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5977514982223511:  68%|███████▌   | 100/146 [00:20<00:10,  4.38it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5007890462875366:  38%|████▉        | 91/241 [00:18<00:30,  4.90it/s]Epoch: 1, train for the 60-th batch, train loss: 0.396964967250824:  16%|██▏           | 60/383 [00:16<01:25,  3.77it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5007890462875366:  38%|████▉        | 92/241 [00:18<00:26,  5.55it/s]Epoch: 1, train for the 77-th batch, train loss: 0.6099724173545837:  32%|████▏        | 76/237 [00:18<00:38,  4.16it/s]Epoch: 1, train for the 88-th batch, train loss: 0.6445066928863525:  58%|███████▍     | 87/151 [00:19<00:14,  4.46it/s]Epoch: 1, train for the 88-th batch, train loss: 0.6445066928863525:  58%|███████▌     | 88/151 [00:19<00:12,  4.96it/s]Epoch: 1, train for the 77-th batch, train loss: 0.6099724173545837:  32%|████▏        | 77/237 [00:18<00:35,  4.54it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5164303183555603:  85%|█████████▎ | 101/119 [00:19<00:04,  4.17it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5164303183555603:  86%|█████████▍ | 102/119 [00:19<00:03,  4.69it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5795004367828369:  32%|████▏        | 77/237 [00:18<00:35,  4.54it/s]Epoch: 1, train for the 78-th batch, train loss: 0.5795004367828369:  33%|████▎        | 78/237 [00:18<00:31,  4.98it/s]Epoch: 1, train for the 103-th batch, train loss: 0.4678305685520172:  86%|█████████▍ | 102/119 [00:19<00:03,  4.69it/s]Epoch: 1, train for the 103-th batch, train loss: 0.4678305685520172:  87%|█████████▌ | 103/119 [00:19<00:03,  4.69it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5802026391029358:  33%|████▎        | 78/237 [00:18<00:31,  4.98it/s]Epoch: 1, train for the 61-th batch, train loss: 0.4362547993659973:  16%|██           | 60/383 [00:16<01:25,  3.77it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5802026391029358:  33%|████▎        | 79/237 [00:18<00:29,  5.35it/s]Epoch: 1, train for the 61-th batch, train loss: 0.4362547993659973:  16%|██           | 61/383 [00:16<01:36,  3.35it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5274222493171692:  68%|███████▌   | 100/146 [00:20<00:10,  4.38it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5274222493171692:  69%|███████▌   | 101/146 [00:20<00:13,  3.44it/s]Epoch: 1, train for the 104-th batch, train loss: 0.48183250427246094:  87%|████████▋ | 103/119 [00:19<00:03,  4.69it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5756533145904541:  58%|███████▌     | 88/151 [00:19<00:12,  4.96it/s]Epoch: 1, train for the 93-th batch, train loss: 0.48729291558265686:  38%|████▌       | 92/241 [00:19<00:26,  5.55it/s]Epoch: 1, train for the 104-th batch, train loss: 0.48183250427246094:  87%|████████▋ | 104/119 [00:19<00:02,  5.49it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5756533145904541:  59%|███████▋     | 89/151 [00:19<00:16,  3.82it/s]Epoch: 1, train for the 93-th batch, train loss: 0.48729291558265686:  39%|████▋       | 93/241 [00:19<00:38,  3.82it/s]Epoch: 1, train for the 80-th batch, train loss: 0.587378203868866:  33%|████▋         | 79/237 [00:18<00:29,  5.35it/s]Epoch: 1, train for the 80-th batch, train loss: 0.587378203868866:  34%|████▋         | 80/237 [00:18<00:27,  5.68it/s]Epoch: 1, train for the 62-th batch, train loss: 0.535203218460083:  16%|██▏           | 61/383 [00:16<01:36,  3.35it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5451557040214539:  69%|███████▌   | 101/146 [00:20<00:13,  3.44it/s]Epoch: 1, train for the 62-th batch, train loss: 0.535203218460083:  16%|██▎           | 62/383 [00:16<01:24,  3.78it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5451557040214539:  70%|███████▋   | 102/146 [00:20<00:10,  4.02it/s]Epoch: 1, train for the 105-th batch, train loss: 0.4925609827041626:  87%|█████████▌ | 104/119 [00:19<00:02,  5.49it/s]Epoch: 1, train for the 105-th batch, train loss: 0.4925609827041626:  88%|█████████▋ | 105/119 [00:19<00:02,  5.69it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5561639070510864:  59%|███████▋     | 89/151 [00:19<00:16,  3.82it/s]Epoch: 1, train for the 94-th batch, train loss: 0.479531854391098:  39%|█████▍        | 93/241 [00:19<00:38,  3.82it/s]Epoch: 1, train for the 94-th batch, train loss: 0.479531854391098:  39%|█████▍        | 94/241 [00:19<00:35,  4.19it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5561639070510864:  60%|███████▋     | 90/151 [00:19<00:14,  4.12it/s]Epoch: 1, train for the 106-th batch, train loss: 0.45696333050727844:  88%|████████▊ | 105/119 [00:19<00:02,  5.69it/s]Epoch: 1, train for the 106-th batch, train loss: 0.45696333050727844:  89%|████████▉ | 106/119 [00:19<00:02,  6.28it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5093748569488525:  39%|█████        | 94/241 [00:19<00:35,  4.19it/s]Epoch: 1, train for the 63-th batch, train loss: 0.5364278554916382:  16%|██           | 62/383 [00:17<01:24,  3.78it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5093748569488525:  39%|█████        | 95/241 [00:19<00:31,  4.67it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5330666899681091:  60%|███████▋     | 90/151 [00:20<00:14,  4.12it/s]Epoch: 1, train for the 103-th batch, train loss: 0.5386489033699036:  70%|███████▋   | 102/146 [00:20<00:10,  4.02it/s]Epoch: 1, train for the 63-th batch, train loss: 0.5364278554916382:  16%|██▏          | 63/383 [00:17<01:25,  3.76it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5330666899681091:  60%|███████▊     | 91/151 [00:20<00:13,  4.49it/s]Epoch: 1, train for the 103-th batch, train loss: 0.5386489033699036:  71%|███████▊   | 103/146 [00:21<00:10,  3.95it/s]Epoch: 1, train for the 81-th batch, train loss: 0.6046919822692871:  34%|████▍        | 80/237 [00:19<00:27,  5.68it/s]Epoch: 1, train for the 81-th batch, train loss: 0.6046919822692871:  34%|████▍        | 81/237 [00:19<00:37,  4.20it/s]Epoch: 1, train for the 107-th batch, train loss: 0.5252687335014343:  89%|█████████▊ | 106/119 [00:20<00:02,  6.28it/s]Epoch: 1, train for the 107-th batch, train loss: 0.5252687335014343:  90%|█████████▉ | 107/119 [00:20<00:02,  5.86it/s]Epoch: 1, train for the 96-th batch, train loss: 0.5988832116127014:  39%|█████        | 95/241 [00:19<00:31,  4.67it/s]Epoch: 1, train for the 96-th batch, train loss: 0.5988832116127014:  40%|█████▏       | 96/241 [00:19<00:28,  5.02it/s]Epoch: 1, train for the 104-th batch, train loss: 0.546558678150177:  71%|████████▍   | 103/146 [00:21<00:10,  3.95it/s]Epoch: 1, train for the 104-th batch, train loss: 0.546558678150177:  71%|████████▌   | 104/146 [00:21<00:09,  4.46it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5602899789810181:  60%|███████▊     | 91/151 [00:20<00:13,  4.49it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5602899789810181:  61%|███████▉     | 92/151 [00:20<00:12,  4.80it/s]Epoch: 1, train for the 64-th batch, train loss: 0.49880996346473694:  16%|█▉          | 63/383 [00:17<01:25,  3.76it/s]Epoch: 1, train for the 64-th batch, train loss: 0.49880996346473694:  17%|██          | 64/383 [00:17<01:19,  4.01it/s]Epoch: 1, train for the 108-th batch, train loss: 0.41277560591697693:  90%|████████▉ | 107/119 [00:20<00:02,  5.86it/s]Epoch: 1, train for the 82-th batch, train loss: 0.6144390106201172:  34%|████▍        | 81/237 [00:19<00:37,  4.20it/s]Epoch: 1, train for the 108-th batch, train loss: 0.41277560591697693:  91%|█████████ | 108/119 [00:20<00:01,  6.21it/s]Epoch: 1, train for the 82-th batch, train loss: 0.6144390106201172:  35%|████▍        | 82/237 [00:19<00:34,  4.55it/s]Epoch: 1, train for the 97-th batch, train loss: 0.42022979259490967:  40%|████▊       | 96/241 [00:19<00:28,  5.02it/s]Epoch: 1, train for the 105-th batch, train loss: 0.5715831518173218:  71%|███████▊   | 104/146 [00:21<00:09,  4.46it/s]Epoch: 1, train for the 97-th batch, train loss: 0.42022979259490967:  40%|████▊       | 97/241 [00:19<00:26,  5.45it/s]Epoch: 1, train for the 105-th batch, train loss: 0.5715831518173218:  72%|███████▉   | 105/146 [00:21<00:08,  5.05it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5624107122421265:  72%|███████▉   | 105/146 [00:21<00:08,  5.05it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5624107122421265:  73%|███████▉   | 106/146 [00:21<00:06,  5.90it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5788098573684692:  61%|███████▉     | 92/151 [00:20<00:12,  4.80it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5788098573684692:  62%|████████     | 93/151 [00:20<00:13,  4.40it/s]Epoch: 1, train for the 65-th batch, train loss: 0.4262860119342804:  17%|██▏          | 64/383 [00:17<01:19,  4.01it/s]Epoch: 1, train for the 109-th batch, train loss: 0.48937729001045227:  91%|█████████ | 108/119 [00:20<00:01,  6.21it/s]Epoch: 1, train for the 109-th batch, train loss: 0.48937729001045227:  92%|█████████▏| 109/119 [00:20<00:02,  4.81it/s]Epoch: 1, train for the 98-th batch, train loss: 0.544162392616272:  40%|█████▋        | 97/241 [00:20<00:26,  5.45it/s]Epoch: 1, train for the 65-th batch, train loss: 0.4262860119342804:  17%|██▏          | 65/383 [00:17<01:27,  3.62it/s]Epoch: 1, train for the 83-th batch, train loss: 0.6300649046897888:  35%|████▍        | 82/237 [00:19<00:34,  4.55it/s]Epoch: 1, train for the 98-th batch, train loss: 0.544162392616272:  41%|█████▋        | 98/241 [00:20<00:30,  4.75it/s]Epoch: 1, train for the 83-th batch, train loss: 0.6300649046897888:  35%|████▌        | 83/237 [00:19<00:38,  3.97it/s]Epoch: 1, train for the 107-th batch, train loss: 0.542926549911499:  73%|████████▋   | 106/146 [00:21<00:06,  5.90it/s]Epoch: 1, train for the 94-th batch, train loss: 0.5490396022796631:  62%|████████     | 93/151 [00:20<00:13,  4.40it/s]Epoch: 1, train for the 107-th batch, train loss: 0.542926549911499:  73%|████████▊   | 107/146 [00:21<00:06,  5.65it/s]Epoch: 1, train for the 94-th batch, train loss: 0.5490396022796631:  62%|████████     | 94/151 [00:20<00:11,  4.77it/s]Epoch: 1, train for the 110-th batch, train loss: 0.48082074522972107:  92%|█████████▏| 109/119 [00:20<00:02,  4.81it/s]Epoch: 1, train for the 110-th batch, train loss: 0.48082074522972107:  92%|█████████▏| 110/119 [00:20<00:01,  5.35it/s]Epoch: 1, train for the 99-th batch, train loss: 0.3610519766807556:  41%|█████▎       | 98/241 [00:20<00:30,  4.75it/s]Epoch: 1, train for the 99-th batch, train loss: 0.3610519766807556:  41%|█████▎       | 99/241 [00:20<00:27,  5.23it/s]Epoch: 1, train for the 66-th batch, train loss: 0.5011810064315796:  17%|██▏          | 65/383 [00:17<01:27,  3.62it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5677455067634583:  73%|████████   | 107/146 [00:21<00:06,  5.65it/s]Epoch: 1, train for the 66-th batch, train loss: 0.5011810064315796:  17%|██▏          | 66/383 [00:18<01:21,  3.90it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5677455067634583:  74%|████████▏  | 108/146 [00:21<00:06,  5.74it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5837658047676086:  35%|████▌        | 83/237 [00:19<00:38,  3.97it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5837658047676086:  35%|████▌        | 84/237 [00:19<00:36,  4.17it/s]Epoch: 1, train for the 67-th batch, train loss: 0.5267484784126282:  17%|██▏          | 66/383 [00:18<01:21,  3.90it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5397023558616638:  92%|██████████▏| 110/119 [00:20<00:01,  5.35it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5397023558616638:  93%|██████████▎| 111/119 [00:20<00:01,  5.05it/s]Epoch: 1, train for the 67-th batch, train loss: 0.5267484784126282:  17%|██▎          | 67/383 [00:18<01:10,  4.47it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5298826694488525:  35%|████▌        | 84/237 [00:20<00:36,  4.17it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5298826694488525:  36%|████▋        | 85/237 [00:20<00:36,  4.18it/s]Epoch: 1, train for the 112-th batch, train loss: 0.4978034496307373:  93%|██████████▎| 111/119 [00:21<00:01,  5.05it/s]Epoch: 1, train for the 112-th batch, train loss: 0.4978034496307373:  94%|██████████▎| 112/119 [00:21<00:01,  5.60it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5633687376976013:  62%|████████     | 94/151 [00:21<00:11,  4.77it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5647287964820862:  41%|████▉       | 99/241 [00:20<00:27,  5.23it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5303194522857666:  17%|██▎          | 67/383 [00:18<01:10,  4.47it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5647287964820862:  41%|████▌      | 100/241 [00:20<00:33,  4.15it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5633687376976013:  63%|████████▏    | 95/151 [00:21<00:15,  3.54it/s]Epoch: 1, train for the 109-th batch, train loss: 0.5359202027320862:  74%|████████▏  | 108/146 [00:22<00:06,  5.74it/s]Epoch: 1, train for the 68-th batch, train loss: 0.5303194522857666:  18%|██▎          | 68/383 [00:18<01:05,  4.83it/s]Epoch: 1, train for the 109-th batch, train loss: 0.5359202027320862:  75%|████████▏  | 109/146 [00:22<00:08,  4.57it/s]Epoch: 1, train for the 113-th batch, train loss: 0.463116317987442:  94%|███████████▎| 112/119 [00:21<00:01,  5.60it/s]Epoch: 1, train for the 113-th batch, train loss: 0.463116317987442:  95%|███████████▍| 113/119 [00:21<00:01,  5.92it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5992617607116699:  36%|████▋        | 85/237 [00:20<00:36,  4.18it/s]Epoch: 1, train for the 86-th batch, train loss: 0.5992617607116699:  36%|████▋        | 86/237 [00:20<00:33,  4.48it/s]Epoch: 1, train for the 110-th batch, train loss: 0.5223888158798218:  75%|████████▏  | 109/146 [00:22<00:08,  4.57it/s]Epoch: 1, train for the 96-th batch, train loss: 0.5732941031455994:  63%|████████▏    | 95/151 [00:21<00:15,  3.54it/s]Epoch: 1, train for the 110-th batch, train loss: 0.5223888158798218:  75%|████████▎  | 110/146 [00:22<00:07,  5.03it/s]Epoch: 1, train for the 96-th batch, train loss: 0.5732941031455994:  64%|████████▎    | 96/151 [00:21<00:13,  4.00it/s]Epoch: 1, train for the 114-th batch, train loss: 0.4877932667732239:  95%|██████████▍| 113/119 [00:21<00:01,  5.92it/s]Epoch: 1, train for the 114-th batch, train loss: 0.4877932667732239:  96%|██████████▌| 114/119 [00:21<00:00,  6.49it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5622450113296509:  64%|████████▎    | 96/151 [00:21<00:13,  4.00it/s]Epoch: 1, train for the 69-th batch, train loss: 0.38000959157943726:  18%|██▏         | 68/383 [00:18<01:05,  4.83it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5622450113296509:  64%|████████▎    | 97/151 [00:21<00:11,  4.72it/s]Epoch: 1, train for the 69-th batch, train loss: 0.38000959157943726:  18%|██▏         | 69/383 [00:18<01:14,  4.19it/s]Epoch: 1, train for the 101-th batch, train loss: 0.592289924621582:  41%|████▉       | 100/241 [00:20<00:33,  4.15it/s]Epoch: 1, train for the 101-th batch, train loss: 0.592289924621582:  42%|█████       | 101/241 [00:21<00:40,  3.45it/s]Epoch: 1, train for the 115-th batch, train loss: 0.4696401059627533:  96%|██████████▌| 114/119 [00:21<00:00,  6.49it/s]Epoch: 1, train for the 98-th batch, train loss: 0.5277490019798279:  64%|████████▎    | 97/151 [00:21<00:11,  4.72it/s]Epoch: 1, train for the 115-th batch, train loss: 0.4696401059627533:  97%|██████████▋| 115/119 [00:21<00:00,  6.15it/s]Epoch: 1, train for the 87-th batch, train loss: 0.6156049966812134:  36%|████▋        | 86/237 [00:20<00:33,  4.48it/s]Epoch: 1, train for the 98-th batch, train loss: 0.5277490019798279:  65%|████████▍    | 98/151 [00:21<00:10,  5.26it/s]Epoch: 1, train for the 87-th batch, train loss: 0.6156049966812134:  37%|████▊        | 87/237 [00:20<00:37,  4.02it/s]Epoch: 1, train for the 111-th batch, train loss: 0.53082275390625:  75%|█████████▊   | 110/146 [00:22<00:07,  5.03it/s]Epoch: 1, train for the 70-th batch, train loss: 0.4984152019023895:  18%|██▎          | 69/383 [00:18<01:14,  4.19it/s]Epoch: 1, train for the 111-th batch, train loss: 0.53082275390625:  76%|█████████▉   | 111/146 [00:22<00:08,  4.36it/s]Epoch: 1, train for the 70-th batch, train loss: 0.4984152019023895:  18%|██▍          | 70/383 [00:18<01:08,  4.60it/s]Epoch: 1, train for the 102-th batch, train loss: 0.6069274544715881:  42%|████▌      | 101/241 [00:21<00:40,  3.45it/s]Epoch: 1, train for the 116-th batch, train loss: 0.42344930768013:  97%|████████████▌| 115/119 [00:21<00:00,  6.15it/s]Epoch: 1, train for the 116-th batch, train loss: 0.42344930768013:  97%|████████████▋| 116/119 [00:21<00:00,  6.55it/s]Epoch: 1, train for the 102-th batch, train loss: 0.6069274544715881:  42%|████▋      | 102/241 [00:21<00:34,  4.04it/s]Epoch: 1, train for the 99-th batch, train loss: 0.5457153916358948:  65%|████████▍    | 98/151 [00:21<00:10,  5.26it/s]Epoch: 1, train for the 99-th batch, train loss: 0.5457153916358948:  66%|████████▌    | 99/151 [00:21<00:09,  5.35it/s]Epoch: 1, train for the 112-th batch, train loss: 0.571032702922821:  76%|█████████   | 111/146 [00:22<00:08,  4.36it/s]Epoch: 1, train for the 88-th batch, train loss: 0.6097177267074585:  37%|████▊        | 87/237 [00:20<00:37,  4.02it/s]Epoch: 1, train for the 112-th batch, train loss: 0.571032702922821:  77%|█████████▏  | 112/146 [00:22<00:07,  4.59it/s]Epoch: 1, train for the 103-th batch, train loss: 0.5515527129173279:  42%|████▋      | 102/241 [00:21<00:34,  4.04it/s]Epoch: 1, train for the 88-th batch, train loss: 0.6097177267074585:  37%|████▊        | 88/237 [00:20<00:36,  4.12it/s]Epoch: 1, train for the 103-th batch, train loss: 0.5515527129173279:  43%|████▋      | 103/241 [00:21<00:29,  4.69it/s]Epoch: 1, train for the 71-th batch, train loss: 0.594245970249176:  18%|██▌           | 70/383 [00:19<01:08,  4.60it/s]Epoch: 1, train for the 71-th batch, train loss: 0.594245970249176:  19%|██▌           | 71/383 [00:19<01:15,  4.16it/s]Epoch: 1, train for the 117-th batch, train loss: 0.4187329113483429:  97%|██████████▋| 116/119 [00:21<00:00,  6.55it/s]Epoch: 1, train for the 113-th batch, train loss: 0.5310996174812317:  77%|████████▍  | 112/146 [00:22<00:07,  4.59it/s]Epoch: 1, train for the 117-th batch, train loss: 0.4187329113483429:  98%|██████████▊| 117/119 [00:21<00:00,  5.28it/s]Epoch: 1, train for the 113-th batch, train loss: 0.5310996174812317:  77%|████████▌  | 113/146 [00:22<00:06,  4.90it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5712301135063171:  43%|████▋      | 103/241 [00:21<00:29,  4.69it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5761400461196899:  37%|████▊        | 88/237 [00:21<00:36,  4.12it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5712301135063171:  43%|████▋      | 104/241 [00:21<00:28,  4.79it/s]Epoch: 1, train for the 89-th batch, train loss: 0.5761400461196899:  38%|████▉        | 89/237 [00:21<00:35,  4.18it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5669605135917664:  19%|██▍          | 71/383 [00:19<01:15,  4.16it/s]Epoch: 1, train for the 100-th batch, train loss: 0.6110520362854004:  66%|███████▊    | 99/151 [00:22<00:09,  5.35it/s]Epoch: 1, train for the 72-th batch, train loss: 0.5669605135917664:  19%|██▍          | 72/383 [00:19<01:07,  4.64it/s]Epoch: 1, train for the 100-th batch, train loss: 0.6110520362854004:  66%|███████▎   | 100/151 [00:22<00:11,  4.33it/s]Epoch: 1, train for the 118-th batch, train loss: 0.44954004883766174:  98%|█████████▊| 117/119 [00:22<00:00,  5.28it/s]Epoch: 1, train for the 114-th batch, train loss: 0.5490903854370117:  77%|████████▌  | 113/146 [00:23<00:06,  4.90it/s]Epoch: 1, train for the 118-th batch, train loss: 0.44954004883766174:  99%|█████████▉| 118/119 [00:22<00:00,  5.45it/s]Epoch: 1, train for the 105-th batch, train loss: 0.4609363079071045:  43%|████▋      | 104/241 [00:21<00:28,  4.79it/s]Epoch: 1, train for the 114-th batch, train loss: 0.5490903854370117:  78%|████████▌  | 114/146 [00:23<00:06,  5.12it/s]Epoch: 1, train for the 105-th batch, train loss: 0.4609363079071045:  44%|████▊      | 105/241 [00:21<00:26,  5.19it/s]Epoch: 1, train for the 73-th batch, train loss: 0.40655890107154846:  19%|██▎         | 72/383 [00:19<01:07,  4.64it/s]Epoch: 1, train for the 73-th batch, train loss: 0.40655890107154846:  19%|██▎         | 73/383 [00:19<01:01,  5.04it/s]Epoch: 1, train for the 74-th batch, train loss: 0.2756749987602234:  19%|██▍          | 73/383 [00:19<01:01,  5.04it/s]Epoch: 1, train for the 115-th batch, train loss: 0.5597318410873413:  78%|████████▌  | 114/146 [00:23<00:06,  5.12it/s]Epoch: 1, train for the 115-th batch, train loss: 0.5597318410873413:  79%|████████▋  | 115/146 [00:23<00:06,  4.96it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5441854596138:  44%|██████        | 105/241 [00:21<00:26,  5.19it/s]Epoch: 1, train for the 74-th batch, train loss: 0.2756749987602234:  19%|██▌          | 74/383 [00:19<00:55,  5.61it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5441854596138:  44%|██████▏       | 106/241 [00:21<00:26,  5.12it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5982942581176758:  66%|███████▎   | 100/151 [00:22<00:11,  4.33it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5982942581176758:  67%|███████▎   | 101/151 [00:22<00:14,  3.56it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5763640999794006:  38%|████▉        | 89/237 [00:21<00:35,  4.18it/s]Epoch: 1, train for the 107-th batch, train loss: 0.6218324303627014:  44%|████▊      | 106/241 [00:21<00:26,  5.12it/s]Epoch: 1, train for the 116-th batch, train loss: 0.5622720718383789:  79%|████████▋  | 115/146 [00:23<00:06,  4.96it/s]Epoch: 1, train for the 107-th batch, train loss: 0.6218324303627014:  44%|████▉      | 107/241 [00:21<00:23,  5.66it/s]Epoch: 1, train for the 119-th batch, train loss: 0.36144521832466125:  99%|█████████▉| 118/119 [00:22<00:00,  5.45it/s]Epoch: 1, train for the 90-th batch, train loss: 0.5763640999794006:  38%|████▉        | 90/237 [00:21<00:44,  3.27it/s]Epoch: 1, train for the 116-th batch, train loss: 0.5622720718383789:  79%|████████▋  | 116/146 [00:23<00:05,  5.33it/s]Epoch: 1, train for the 119-th batch, train loss: 0.36144521832466125: 100%|██████████| 119/119 [00:22<00:00,  4.02it/s]Epoch: 1, train for the 119-th batch, train loss: 0.36144521832466125: 100%|██████████| 119/119 [00:22<00:00,  5.28it/s]
Epoch: 1, train for the 75-th batch, train loss: 0.4202210009098053:  19%|██▌          | 74/383 [00:19<00:55,  5.61it/s]Epoch: 1, train for the 75-th batch, train loss: 0.4202210009098053:  20%|██▌          | 75/383 [00:19<00:57,  5.35it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5413416028022766:  67%|███████▎   | 101/151 [00:22<00:14,  3.56it/s]Epoch: 1, train for the 117-th batch, train loss: 0.5829050540924072:  79%|████████▋  | 116/146 [00:23<00:05,  5.33it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5413416028022766:  68%|███████▍   | 102/151 [00:22<00:11,  4.09it/s]Epoch: 1, train for the 117-th batch, train loss: 0.5829050540924072:  80%|████████▊  | 117/146 [00:23<00:04,  5.86it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5618818402290344:  44%|████▉      | 107/241 [00:22<00:23,  5.66it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5618818402290344:  45%|████▉      | 108/241 [00:22<00:22,  5.80it/s]  0%|                                                                                            | 0/40 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.5207842588424683:   0%|                            | 0/40 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5458422899246216:   0%|                            | 0/40 [00:00<?, ?it/s]Epoch: 1, train for the 109-th batch, train loss: 0.6443257927894592:  45%|████▉      | 108/241 [00:22<00:22,  5.80it/s]Epoch: 1, train for the 109-th batch, train loss: 0.6443257927894592:  45%|████▉      | 109/241 [00:22<00:21,  6.13it/s]evaluate for the 3-th batch, evaluate loss: 0.5496700406074524:   0%|                            | 0/40 [00:00<?, ?it/s]evaluate for the 3-th batch, evaluate loss: 0.5496700406074524:   8%|█▌                  | 3/40 [00:00<00:01, 24.39it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5430164337158203:  38%|████▉        | 90/237 [00:21<00:44,  3.27it/s]Epoch: 1, train for the 91-th batch, train loss: 0.5430164337158203:  38%|████▉        | 91/237 [00:21<00:46,  3.15it/s]Epoch: 1, train for the 118-th batch, train loss: 0.5294780731201172:  80%|████████▊  | 117/146 [00:23<00:04,  5.86it/s]evaluate for the 4-th batch, evaluate loss: 0.6230064034461975:   8%|█▌                  | 3/40 [00:00<00:01, 24.39it/s]Epoch: 1, train for the 118-th batch, train loss: 0.5294780731201172:  81%|████████▉  | 118/146 [00:23<00:05,  5.01it/s]Epoch: 1, train for the 76-th batch, train loss: 0.5323862433433533:  20%|██▌          | 75/383 [00:20<00:57,  5.35it/s]Epoch: 1, train for the 103-th batch, train loss: 0.6350986957550049:  68%|███████▍   | 102/151 [00:22<00:11,  4.09it/s]evaluate for the 5-th batch, evaluate loss: 0.5970845818519592:   8%|█▌                  | 3/40 [00:00<00:01, 24.39it/s]Epoch: 1, train for the 110-th batch, train loss: 0.6162704229354858:  45%|████▉      | 109/241 [00:22<00:21,  6.13it/s]Epoch: 1, train for the 76-th batch, train loss: 0.5323862433433533:  20%|██▌          | 76/383 [00:20<01:16,  4.03it/s]Epoch: 1, train for the 103-th batch, train loss: 0.6350986957550049:  68%|███████▌   | 103/151 [00:22<00:13,  3.68it/s]Epoch: 1, train for the 110-th batch, train loss: 0.6162704229354858:  46%|█████      | 110/241 [00:22<00:21,  6.06it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5589314699172974:  38%|████▉        | 91/237 [00:22<00:46,  3.15it/s]Epoch: 1, train for the 92-th batch, train loss: 0.5589314699172974:  39%|█████        | 92/237 [00:22<00:39,  3.65it/s]Epoch: 1, train for the 119-th batch, train loss: 0.5305728912353516:  81%|████████▉  | 118/146 [00:23<00:05,  5.01it/s]evaluate for the 6-th batch, evaluate loss: 0.5665887594223022:   8%|█▌                  | 3/40 [00:00<00:01, 24.39it/s]evaluate for the 6-th batch, evaluate loss: 0.5665887594223022:  15%|███                 | 6/40 [00:00<00:02, 15.48it/s]Epoch: 1, train for the 119-th batch, train loss: 0.5305728912353516:  82%|████████▉  | 119/146 [00:24<00:05,  5.32it/s]evaluate for the 7-th batch, evaluate loss: 0.5890670418739319:  15%|███                 | 6/40 [00:00<00:02, 15.48it/s]Epoch: 1, train for the 77-th batch, train loss: 0.4945094883441925:  20%|██▌          | 76/383 [00:20<01:16,  4.03it/s]Epoch: 1, train for the 77-th batch, train loss: 0.4945094883441925:  20%|██▌          | 77/383 [00:20<01:10,  4.35it/s]Epoch: 1, train for the 111-th batch, train loss: 0.32591983675956726:  46%|████▌     | 110/241 [00:22<00:21,  6.06it/s]evaluate for the 8-th batch, evaluate loss: 0.5578437447547913:  15%|███                 | 6/40 [00:00<00:02, 15.48it/s]evaluate for the 8-th batch, evaluate loss: 0.5578437447547913:  20%|████                | 8/40 [00:00<00:02, 15.98it/s]Epoch: 1, train for the 111-th batch, train loss: 0.32591983675956726:  46%|████▌     | 111/241 [00:22<00:23,  5.65it/s]Epoch: 1, train for the 78-th batch, train loss: 0.45981451869010925:  20%|██▍         | 77/383 [00:20<01:10,  4.35it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5707578659057617:  68%|███████▌   | 103/151 [00:23<00:13,  3.68it/s]Epoch: 1, train for the 78-th batch, train loss: 0.45981451869010925:  20%|██▍         | 78/383 [00:20<01:03,  4.83it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5707578659057617:  69%|███████▌   | 104/151 [00:23<00:13,  3.36it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5311922430992126:  82%|████████▉  | 119/146 [00:24<00:05,  5.32it/s]Epoch: 1, train for the 112-th batch, train loss: 0.34507179260253906:  46%|████▌     | 111/241 [00:22<00:23,  5.65it/s]evaluate for the 9-th batch, evaluate loss: 0.5802997350692749:  20%|████                | 8/40 [00:00<00:02, 15.98it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5311922430992126:  82%|█████████  | 120/146 [00:24<00:06,  4.32it/s]Epoch: 1, train for the 112-th batch, train loss: 0.34507179260253906:  46%|████▋     | 112/241 [00:22<00:24,  5.17it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5414935350418091:  39%|█████        | 92/237 [00:22<00:39,  3.65it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5638460516929626:  20%|██▋          | 78/383 [00:20<01:03,  4.83it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5414935350418091:  39%|█████        | 93/237 [00:22<00:46,  3.10it/s]evaluate for the 10-th batch, evaluate loss: 0.6123245358467102:  20%|███▊               | 8/40 [00:00<00:02, 15.98it/s]evaluate for the 10-th batch, evaluate loss: 0.6123245358467102:  25%|████▌             | 10/40 [00:00<00:02, 11.20it/s]Epoch: 1, train for the 105-th batch, train loss: 0.5391702055931091:  69%|███████▌   | 104/151 [00:23<00:13,  3.36it/s]Epoch: 1, train for the 79-th batch, train loss: 0.5638460516929626:  21%|██▋          | 79/383 [00:20<01:00,  5.03it/s]Epoch: 1, train for the 105-th batch, train loss: 0.5391702055931091:  70%|███████▋   | 105/151 [00:23<00:11,  3.89it/s]evaluate for the 11-th batch, evaluate loss: 0.567829966545105:  25%|████▊              | 10/40 [00:00<00:02, 11.20it/s]Epoch: 1, train for the 113-th batch, train loss: 0.3645919859409332:  46%|█████      | 112/241 [00:23<00:24,  5.17it/s]Epoch: 1, train for the 121-th batch, train loss: 0.5337089896202087:  82%|█████████  | 120/146 [00:24<00:06,  4.32it/s]Epoch: 1, train for the 113-th batch, train loss: 0.3645919859409332:  47%|█████▏     | 113/241 [00:23<00:22,  5.65it/s]Epoch: 1, train for the 121-th batch, train loss: 0.5337089896202087:  83%|█████████  | 121/146 [00:24<00:05,  4.76it/s]evaluate for the 12-th batch, evaluate loss: 0.5987159609794617:  25%|████▌             | 10/40 [00:00<00:02, 11.20it/s]evaluate for the 12-th batch, evaluate loss: 0.5987159609794617:  30%|█████▍            | 12/40 [00:00<00:02, 12.30it/s]Epoch: 1, train for the 94-th batch, train loss: 0.6057150363922119:  39%|█████        | 93/237 [00:22<00:46,  3.10it/s]Epoch: 1, train for the 94-th batch, train loss: 0.6057150363922119:  40%|█████▏       | 94/237 [00:22<00:39,  3.61it/s]Epoch: 1, train for the 80-th batch, train loss: 0.3438730835914612:  21%|██▋          | 79/383 [00:20<01:00,  5.03it/s]Epoch: 1, train for the 80-th batch, train loss: 0.3438730835914612:  21%|██▋          | 80/383 [00:20<01:00,  5.02it/s]evaluate for the 13-th batch, evaluate loss: 0.6104764342308044:  30%|█████▍            | 12/40 [00:01<00:02, 12.30it/s]Epoch: 1, train for the 114-th batch, train loss: 0.36950454115867615:  47%|████▋     | 113/241 [00:23<00:22,  5.65it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5494676828384399:  40%|█████▏       | 94/237 [00:22<00:39,  3.61it/s]evaluate for the 14-th batch, evaluate loss: 0.5869560241699219:  30%|█████▍            | 12/40 [00:01<00:02, 12.30it/s]evaluate for the 14-th batch, evaluate loss: 0.5869560241699219:  35%|██████▎           | 14/40 [00:01<00:02, 11.46it/s]Epoch: 1, train for the 114-th batch, train loss: 0.36950454115867615:  47%|████▋     | 114/241 [00:23<00:24,  5.10it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5494676828384399:  40%|█████▏       | 95/237 [00:22<00:34,  4.10it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5618669986724854:  70%|███████▋   | 105/151 [00:23<00:11,  3.89it/s]Epoch: 1, train for the 106-th batch, train loss: 0.5618669986724854:  70%|███████▋   | 106/151 [00:23<00:12,  3.54it/s]Epoch: 1, train for the 81-th batch, train loss: 0.3177242577075958:  21%|██▋          | 80/383 [00:21<01:00,  5.02it/s]evaluate for the 15-th batch, evaluate loss: 0.6064330339431763:  35%|██████▎           | 14/40 [00:01<00:02, 11.46it/s]Epoch: 1, train for the 81-th batch, train loss: 0.3177242577075958:  21%|██▋          | 81/383 [00:21<01:01,  4.87it/s]Epoch: 1, train for the 115-th batch, train loss: 0.474239706993103:  47%|█████▋      | 114/241 [00:23<00:24,  5.10it/s]Epoch: 1, train for the 122-th batch, train loss: 0.5695372819900513:  83%|█████████  | 121/146 [00:24<00:05,  4.76it/s]Epoch: 1, train for the 96-th batch, train loss: 0.538425862789154:  40%|█████▌        | 95/237 [00:22<00:34,  4.10it/s]Epoch: 1, train for the 122-th batch, train loss: 0.5695372819900513:  84%|█████████▏ | 122/146 [00:24<00:06,  3.84it/s]Epoch: 1, train for the 115-th batch, train loss: 0.474239706993103:  48%|█████▋      | 115/241 [00:23<00:23,  5.47it/s]evaluate for the 16-th batch, evaluate loss: 0.5922871232032776:  35%|██████▎           | 14/40 [00:01<00:02, 11.46it/s]evaluate for the 16-th batch, evaluate loss: 0.5922871232032776:  40%|███████▏          | 16/40 [00:01<00:02, 11.86it/s]Epoch: 1, train for the 96-th batch, train loss: 0.538425862789154:  41%|█████▋        | 96/237 [00:22<00:30,  4.61it/s]Epoch: 1, train for the 107-th batch, train loss: 0.5606757402420044:  70%|███████▋   | 106/151 [00:23<00:12,  3.54it/s]Epoch: 1, train for the 107-th batch, train loss: 0.5606757402420044:  71%|███████▊   | 107/151 [00:23<00:10,  4.05it/s]evaluate for the 17-th batch, evaluate loss: 0.6529441475868225:  40%|███████▏          | 16/40 [00:01<00:02, 11.86it/s]Epoch: 1, train for the 123-th batch, train loss: 0.5952675342559814:  84%|█████████▏ | 122/146 [00:25<00:06,  3.84it/s]evaluate for the 18-th batch, evaluate loss: 0.6152696013450623:  40%|███████▏          | 16/40 [00:01<00:02, 11.86it/s]evaluate for the 18-th batch, evaluate loss: 0.6152696013450623:  45%|████████          | 18/40 [00:01<00:01, 12.74it/s]Epoch: 1, train for the 123-th batch, train loss: 0.5952675342559814:  84%|█████████▎ | 123/146 [00:25<00:05,  4.48it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5666933655738831:  21%|██▋          | 81/383 [00:21<01:01,  4.87it/s]Epoch: 1, train for the 82-th batch, train loss: 0.5666933655738831:  21%|██▊          | 82/383 [00:21<01:03,  4.75it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5237101912498474:  71%|███████▊   | 107/151 [00:24<00:10,  4.05it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5237101912498474:  72%|███████▊   | 108/151 [00:24<00:09,  4.58it/s]evaluate for the 19-th batch, evaluate loss: 0.6236129403114319:  45%|████████          | 18/40 [00:01<00:01, 12.74it/s]evaluate for the 20-th batch, evaluate loss: 0.6019224524497986:  45%|████████          | 18/40 [00:01<00:01, 12.74it/s]evaluate for the 20-th batch, evaluate loss: 0.6019224524497986:  50%|█████████         | 20/40 [00:01<00:01, 12.11it/s]Epoch: 1, train for the 116-th batch, train loss: 0.6107048392295837:  48%|█████▏     | 115/241 [00:23<00:23,  5.47it/s]Epoch: 1, train for the 116-th batch, train loss: 0.6107048392295837:  48%|█████▎     | 116/241 [00:23<00:28,  4.39it/s]Epoch: 1, train for the 109-th batch, train loss: 0.5306887030601501:  72%|███████▊   | 108/151 [00:24<00:09,  4.58it/s]Epoch: 1, train for the 109-th batch, train loss: 0.5306887030601501:  72%|███████▉   | 109/151 [00:24<00:08,  4.93it/s]Epoch: 1, train for the 124-th batch, train loss: 0.5353869795799255:  84%|█████████▎ | 123/146 [00:25<00:05,  4.48it/s]evaluate for the 21-th batch, evaluate loss: 0.6316078305244446:  50%|█████████         | 20/40 [00:01<00:01, 12.11it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5461913347244263:  41%|█████▎       | 96/237 [00:23<00:30,  4.61it/s]Epoch: 1, train for the 124-th batch, train loss: 0.5353869795799255:  85%|█████████▎ | 124/146 [00:25<00:05,  4.20it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5181053280830383:  21%|██▊          | 82/383 [00:21<01:03,  4.75it/s]Epoch: 1, train for the 97-th batch, train loss: 0.5461913347244263:  41%|█████▎       | 97/237 [00:23<00:38,  3.61it/s]Epoch: 1, train for the 83-th batch, train loss: 0.5181053280830383:  22%|██▊          | 83/383 [00:21<01:08,  4.37it/s]Epoch: 1, train for the 117-th batch, train loss: 0.6977013945579529:  48%|█████▎     | 116/241 [00:23<00:28,  4.39it/s]evaluate for the 22-th batch, evaluate loss: 0.5787143707275391:  50%|█████████         | 20/40 [00:01<00:01, 12.11it/s]evaluate for the 22-th batch, evaluate loss: 0.5787143707275391:  55%|█████████▉        | 22/40 [00:01<00:01, 12.66it/s]Epoch: 1, train for the 117-th batch, train loss: 0.6977013945579529:  49%|█████▎     | 117/241 [00:23<00:24,  5.00it/s]Epoch: 1, train for the 110-th batch, train loss: 0.5721877813339233:  72%|███████▉   | 109/151 [00:24<00:08,  4.93it/s]Epoch: 1, train for the 110-th batch, train loss: 0.5721877813339233:  73%|████████   | 110/151 [00:24<00:07,  5.18it/s]evaluate for the 23-th batch, evaluate loss: 0.5463576316833496:  55%|█████████▉        | 22/40 [00:01<00:01, 12.66it/s]Epoch: 1, train for the 125-th batch, train loss: 0.5382401347160339:  85%|█████████▎ | 124/146 [00:25<00:05,  4.20it/s]Epoch: 1, train for the 125-th batch, train loss: 0.5382401347160339:  86%|█████████▍ | 125/146 [00:25<00:04,  4.59it/s]Epoch: 1, train for the 98-th batch, train loss: 0.5492209792137146:  41%|█████▎       | 97/237 [00:23<00:38,  3.61it/s]evaluate for the 24-th batch, evaluate loss: 0.600719153881073:  55%|██████████▍        | 22/40 [00:01<00:01, 12.66it/s]evaluate for the 24-th batch, evaluate loss: 0.600719153881073:  60%|███████████▍       | 24/40 [00:01<00:01, 12.90it/s]Epoch: 1, train for the 98-th batch, train loss: 0.5492209792137146:  41%|█████▍       | 98/237 [00:23<00:34,  3.99it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5132094621658325:  22%|██▊          | 83/383 [00:21<01:08,  4.37it/s]Epoch: 1, train for the 84-th batch, train loss: 0.5132094621658325:  22%|██▊          | 84/383 [00:21<01:05,  4.53it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5471494793891907:  73%|████████   | 110/151 [00:24<00:07,  5.18it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5471494793891907:  74%|████████   | 111/151 [00:24<00:06,  5.80it/s]evaluate for the 25-th batch, evaluate loss: 0.628785252571106:  60%|███████████▍       | 24/40 [00:02<00:01, 12.90it/s]Epoch: 1, train for the 118-th batch, train loss: 0.5792137980461121:  49%|█████▎     | 117/241 [00:24<00:24,  5.00it/s]Epoch: 1, train for the 118-th batch, train loss: 0.5792137980461121:  49%|█████▍     | 118/241 [00:24<00:30,  4.01it/s]evaluate for the 26-th batch, evaluate loss: 0.5681367516517639:  60%|██████████▊       | 24/40 [00:02<00:01, 12.90it/s]evaluate for the 26-th batch, evaluate loss: 0.5681367516517639:  65%|███████████▋      | 26/40 [00:02<00:01, 11.09it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5560036897659302:  86%|█████████▍ | 125/146 [00:25<00:04,  4.59it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5560036897659302:  86%|█████████▍ | 126/146 [00:25<00:04,  4.22it/s]Epoch: 1, train for the 112-th batch, train loss: 0.4888744354248047:  74%|████████   | 111/151 [00:24<00:06,  5.80it/s]Epoch: 1, train for the 112-th batch, train loss: 0.4888744354248047:  74%|████████▏  | 112/151 [00:24<00:07,  5.44it/s]evaluate for the 27-th batch, evaluate loss: 0.624008297920227:  65%|████████████▎      | 26/40 [00:02<00:01, 11.09it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5080456137657166:  22%|██▊          | 84/383 [00:22<01:05,  4.53it/s]Epoch: 1, train for the 99-th batch, train loss: 0.5541556477546692:  41%|█████▍       | 98/237 [00:23<00:34,  3.99it/s]Epoch: 1, train for the 119-th batch, train loss: 0.43812644481658936:  49%|████▉     | 118/241 [00:24<00:30,  4.01it/s]Epoch: 1, train for the 85-th batch, train loss: 0.5080456137657166:  22%|██▉          | 85/383 [00:22<01:12,  4.10it/s]Epoch: 1, train for the 99-th batch, train loss: 0.5541556477546692:  42%|█████▍       | 99/237 [00:23<00:38,  3.61it/s]Epoch: 1, train for the 119-th batch, train loss: 0.43812644481658936:  49%|████▉     | 119/241 [00:24<00:26,  4.67it/s]evaluate for the 28-th batch, evaluate loss: 0.5683823823928833:  65%|███████████▋      | 26/40 [00:02<00:01, 11.09it/s]evaluate for the 28-th batch, evaluate loss: 0.5683823823928833:  70%|████████████▌     | 28/40 [00:02<00:00, 12.09it/s]Epoch: 1, train for the 113-th batch, train loss: 0.55353844165802:  74%|█████████▋   | 112/151 [00:24<00:07,  5.44it/s]Epoch: 1, train for the 127-th batch, train loss: 0.5009449124336243:  86%|█████████▍ | 126/146 [00:25<00:04,  4.22it/s]evaluate for the 29-th batch, evaluate loss: 0.6123726963996887:  70%|████████████▌     | 28/40 [00:02<00:00, 12.09it/s]Epoch: 1, train for the 127-th batch, train loss: 0.5009449124336243:  87%|█████████▌ | 127/146 [00:25<00:04,  4.53it/s]Epoch: 1, train for the 113-th batch, train loss: 0.55353844165802:  75%|█████████▋   | 113/151 [00:24<00:06,  5.59it/s]Epoch: 1, train for the 120-th batch, train loss: 0.6383547186851501:  49%|█████▍     | 119/241 [00:24<00:26,  4.67it/s]evaluate for the 30-th batch, evaluate loss: 0.6227919459342957:  70%|████████████▌     | 28/40 [00:02<00:00, 12.09it/s]evaluate for the 30-th batch, evaluate loss: 0.6227919459342957:  75%|█████████████▌    | 30/40 [00:02<00:00, 12.86it/s]Epoch: 1, train for the 86-th batch, train loss: 0.42958712577819824:  22%|██▋         | 85/383 [00:22<01:12,  4.10it/s]Epoch: 1, train for the 120-th batch, train loss: 0.6383547186851501:  50%|█████▍     | 120/241 [00:24<00:24,  5.03it/s]Epoch: 1, train for the 86-th batch, train loss: 0.42958712577819824:  22%|██▋         | 86/383 [00:22<01:08,  4.31it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5697698593139648:  87%|█████████▌ | 127/146 [00:26<00:04,  4.53it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5697698593139648:  88%|█████████▋ | 128/146 [00:26<00:03,  5.13it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5641709566116333:  88%|█████████▋ | 128/146 [00:26<00:03,  5.13it/s]evaluate for the 31-th batch, evaluate loss: 0.5842358469963074:  75%|█████████████▌    | 30/40 [00:02<00:00, 12.86it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5641709566116333:  88%|█████████▋ | 129/146 [00:26<00:03,  5.33it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5401297807693481:  42%|█████       | 99/237 [00:24<00:38,  3.61it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5401297807693481:  42%|████▋      | 100/237 [00:24<00:44,  3.10it/s]Epoch: 1, train for the 87-th batch, train loss: 0.49093931913375854:  22%|██▋         | 86/383 [00:22<01:08,  4.31it/s]Epoch: 1, train for the 114-th batch, train loss: 0.4968711733818054:  75%|████████▏  | 113/151 [00:25<00:06,  5.59it/s]Epoch: 1, train for the 121-th batch, train loss: 0.6254822611808777:  50%|█████▍     | 120/241 [00:24<00:24,  5.03it/s]evaluate for the 32-th batch, evaluate loss: 0.627192497253418:  75%|██████████████▎    | 30/40 [00:02<00:00, 12.86it/s]evaluate for the 32-th batch, evaluate loss: 0.627192497253418:  80%|███████████████▏   | 32/40 [00:02<00:00, 10.09it/s]Epoch: 1, train for the 114-th batch, train loss: 0.4968711733818054:  75%|████████▎  | 114/151 [00:25<00:08,  4.27it/s]Epoch: 1, train for the 121-th batch, train loss: 0.6254822611808777:  50%|█████▌     | 121/241 [00:24<00:26,  4.47it/s]Epoch: 1, train for the 87-th batch, train loss: 0.49093931913375854:  23%|██▋         | 87/383 [00:22<01:11,  4.15it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5254855751991272:  88%|█████████▋ | 129/146 [00:26<00:03,  5.33it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5254855751991272:  89%|█████████▊ | 130/146 [00:26<00:02,  5.83it/s]evaluate for the 33-th batch, evaluate loss: 0.6012834310531616:  80%|██████████████▍   | 32/40 [00:02<00:00, 10.09it/s]evaluate for the 34-th batch, evaluate loss: 0.6718365550041199:  80%|██████████████▍   | 32/40 [00:02<00:00, 10.09it/s]evaluate for the 34-th batch, evaluate loss: 0.6718365550041199:  85%|███████████████▎  | 34/40 [00:02<00:00, 10.85it/s]Epoch: 1, train for the 115-th batch, train loss: 0.4868803322315216:  75%|████████▎  | 114/151 [00:25<00:08,  4.27it/s]Epoch: 1, train for the 115-th batch, train loss: 0.4868803322315216:  76%|████████▍  | 115/151 [00:25<00:07,  4.66it/s]Epoch: 1, train for the 122-th batch, train loss: 0.5507530570030212:  50%|█████▌     | 121/241 [00:25<00:26,  4.47it/s]Epoch: 1, train for the 122-th batch, train loss: 0.5507530570030212:  51%|█████▌     | 122/241 [00:25<00:29,  3.97it/s]evaluate for the 35-th batch, evaluate loss: 0.6368436217308044:  85%|███████████████▎  | 34/40 [00:02<00:00, 10.85it/s]Epoch: 1, train for the 88-th batch, train loss: 0.4985826909542084:  23%|██▉          | 87/383 [00:22<01:11,  4.15it/s]Epoch: 1, train for the 101-th batch, train loss: 0.595850944519043:  42%|█████       | 100/237 [00:24<00:44,  3.10it/s]evaluate for the 36-th batch, evaluate loss: 0.6590726971626282:  85%|███████████████▎  | 34/40 [00:03<00:00, 10.85it/s]evaluate for the 36-th batch, evaluate loss: 0.6590726971626282:  90%|████████████████▏ | 36/40 [00:03<00:00, 10.14it/s]Epoch: 1, train for the 88-th batch, train loss: 0.4985826909542084:  23%|██▉          | 88/383 [00:22<01:23,  3.54it/s]Epoch: 1, train for the 131-th batch, train loss: 0.5423017144203186:  89%|█████████▊ | 130/146 [00:26<00:02,  5.83it/s]Epoch: 1, train for the 101-th batch, train loss: 0.595850944519043:  43%|█████       | 101/237 [00:24<00:48,  2.82it/s]Epoch: 1, train for the 131-th batch, train loss: 0.5423017144203186:  90%|█████████▊ | 131/146 [00:26<00:03,  4.56it/s]evaluate for the 37-th batch, evaluate loss: 0.6612765192985535:  90%|████████████████▏ | 36/40 [00:03<00:00, 10.14it/s]evaluate for the 38-th batch, evaluate loss: 0.6779365539550781:  90%|████████████████▏ | 36/40 [00:03<00:00, 10.14it/s]evaluate for the 38-th batch, evaluate loss: 0.6779365539550781:  95%|█████████████████ | 38/40 [00:03<00:00, 11.47it/s]Epoch: 1, train for the 123-th batch, train loss: 0.6536162495613098:  51%|█████▌     | 122/241 [00:25<00:29,  3.97it/s]Epoch: 1, train for the 116-th batch, train loss: 0.47056517004966736:  76%|███████▌  | 115/151 [00:25<00:07,  4.66it/s]Epoch: 1, train for the 123-th batch, train loss: 0.6536162495613098:  51%|█████▌     | 123/241 [00:25<00:27,  4.24it/s]Epoch: 1, train for the 116-th batch, train loss: 0.47056517004966736:  77%|███████▋  | 116/151 [00:25<00:08,  3.92it/s]Epoch: 1, train for the 89-th batch, train loss: 0.48080968856811523:  23%|██▊         | 88/383 [00:23<01:23,  3.54it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5173352956771851:  90%|█████████▊ | 131/146 [00:26<00:03,  4.56it/s]evaluate for the 39-th batch, evaluate loss: 0.6763301491737366:  95%|█████████████████ | 38/40 [00:03<00:00, 11.47it/s]Epoch: 1, train for the 89-th batch, train loss: 0.48080968856811523:  23%|██▊         | 89/383 [00:23<01:14,  3.96it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5173352956771851:  90%|█████████▉ | 132/146 [00:26<00:02,  4.86it/s]evaluate for the 40-th batch, evaluate loss: 0.856203019618988:  95%|██████████████████ | 38/40 [00:03<00:00, 11.47it/s]evaluate for the 40-th batch, evaluate loss: 0.856203019618988: 100%|███████████████████| 40/40 [00:03<00:00, 12.92it/s]evaluate for the 40-th batch, evaluate loss: 0.856203019618988: 100%|███████████████████| 40/40 [00:03<00:00, 12.25it/s]
Epoch: 1, train for the 124-th batch, train loss: 0.42669403553009033:  51%|█████     | 123/241 [00:25<00:27,  4.24it/s]Epoch: 1, train for the 124-th batch, train loss: 0.42669403553009033:  51%|█████▏    | 124/241 [00:25<00:25,  4.51it/s]Epoch: 1, train for the 117-th batch, train loss: 0.5315738320350647:  77%|████████▍  | 116/151 [00:26<00:08,  3.92it/s]Epoch: 1, train for the 117-th batch, train loss: 0.5315738320350647:  77%|████████▌  | 117/151 [00:26<00:08,  3.84it/s]Epoch: 1, train for the 133-th batch, train loss: 0.4888966381549835:  90%|█████████▉ | 132/146 [00:27<00:02,  4.86it/s]Epoch: 1, train for the 133-th batch, train loss: 0.4888966381549835:  91%|██████████ | 133/146 [00:27<00:02,  4.69it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5799055695533752:  43%|████▋      | 101/237 [00:25<00:48,  2.82it/s]Epoch: 1, train for the 90-th batch, train loss: 0.48088598251342773:  23%|██▊         | 89/383 [00:23<01:14,  3.96it/s]Epoch: 1, train for the 125-th batch, train loss: 0.5534836649894714:  51%|█████▋     | 124/241 [00:25<00:25,  4.51it/s]Epoch: 1, train for the 102-th batch, train loss: 0.5799055695533752:  43%|████▋      | 102/237 [00:25<00:52,  2.56it/s]Epoch: 1, train for the 125-th batch, train loss: 0.5534836649894714:  52%|█████▋     | 125/241 [00:25<00:24,  4.81it/s]Epoch: 1, train for the 90-th batch, train loss: 0.48088598251342773:  23%|██▊         | 90/383 [00:23<01:19,  3.69it/s]  0%|                                                                                            | 0/21 [00:00<?, ?it/s]Epoch: 1, train for the 118-th batch, train loss: 0.47892242670059204:  77%|███████▋  | 117/151 [00:26<00:08,  3.84it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5451300144195557:  91%|██████████ | 133/146 [00:27<00:02,  4.69it/s]Epoch: 1, train for the 118-th batch, train loss: 0.47892242670059204:  78%|███████▊  | 118/151 [00:26<00:07,  4.33it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5451300144195557:  92%|██████████ | 134/146 [00:27<00:02,  5.08it/s]Epoch: 1, train for the 103-th batch, train loss: 0.5986170768737793:  43%|████▋      | 102/237 [00:25<00:52,  2.56it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5020073652267456:  52%|█████▋     | 125/241 [00:25<00:24,  4.81it/s]Epoch: 1, train for the 103-th batch, train loss: 0.5986170768737793:  43%|████▊      | 103/237 [00:25<00:43,  3.06it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5020073652267456:  52%|█████▊     | 126/241 [00:25<00:22,  5.09it/s]Epoch: 1, train for the 91-th batch, train loss: 0.3934343755245209:  23%|███          | 90/383 [00:23<01:19,  3.69it/s]Epoch: 1, train for the 135-th batch, train loss: 0.546449601650238:  92%|███████████ | 134/146 [00:27<00:02,  5.08it/s]Epoch: 1, train for the 135-th batch, train loss: 0.546449601650238:  92%|███████████ | 135/146 [00:27<00:01,  5.65it/s]Epoch: 1, train for the 91-th batch, train loss: 0.3934343755245209:  24%|███          | 91/383 [00:23<01:16,  3.81it/s]evaluate for the 1-th batch, evaluate loss: 0.7362448573112488:   0%|                            | 0/21 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.7362448573112488:   5%|▉                   | 1/21 [00:00<00:04,  4.65it/s]evaluate for the 2-th batch, evaluate loss: 0.7538712620735168:   5%|▉                   | 1/21 [00:00<00:04,  4.65it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5499180555343628:  43%|████▊      | 103/237 [00:25<00:43,  3.06it/s]Epoch: 1, train for the 104-th batch, train loss: 0.5499180555343628:  44%|████▊      | 104/237 [00:25<00:37,  3.59it/s]Epoch: 1, train for the 119-th batch, train loss: 0.5224573016166687:  78%|████████▌  | 118/151 [00:26<00:07,  4.33it/s]evaluate for the 3-th batch, evaluate loss: 0.7421914339065552:   5%|▉                   | 1/21 [00:00<00:04,  4.65it/s]evaluate for the 3-th batch, evaluate loss: 0.7421914339065552:  14%|██▊                 | 3/21 [00:00<00:01, 10.59it/s]Epoch: 1, train for the 119-th batch, train loss: 0.5224573016166687:  79%|████████▋  | 119/151 [00:26<00:07,  4.02it/s]evaluate for the 4-th batch, evaluate loss: 0.7068189978599548:  14%|██▊                 | 3/21 [00:00<00:01, 10.59it/s]Epoch: 1, train for the 127-th batch, train loss: 0.6135058403015137:  52%|█████▊     | 126/241 [00:26<00:22,  5.09it/s]Epoch: 1, train for the 127-th batch, train loss: 0.6135058403015137:  53%|█████▊     | 127/241 [00:26<00:26,  4.37it/s]evaluate for the 5-th batch, evaluate loss: 0.7720149159431458:  14%|██▊                 | 3/21 [00:00<00:01, 10.59it/s]evaluate for the 5-th batch, evaluate loss: 0.7720149159431458:  24%|████▊               | 5/21 [00:00<00:01, 12.93it/s]Epoch: 1, train for the 136-th batch, train loss: 0.5186399817466736:  92%|██████████▏| 135/146 [00:27<00:01,  5.65it/s]Epoch: 1, train for the 105-th batch, train loss: 0.5459133982658386:  44%|████▊      | 104/237 [00:25<00:37,  3.59it/s]Epoch: 1, train for the 136-th batch, train loss: 0.5186399817466736:  93%|██████████▏| 136/146 [00:27<00:02,  4.72it/s]Epoch: 1, train for the 92-th batch, train loss: 0.42096349596977234:  24%|██▊         | 91/383 [00:23<01:16,  3.81it/s]evaluate for the 6-th batch, evaluate loss: 0.7704247832298279:  24%|████▊               | 5/21 [00:00<00:01, 12.93it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5380920767784119:  79%|████████▋  | 119/151 [00:26<00:07,  4.02it/s]Epoch: 1, train for the 105-th batch, train loss: 0.5459133982658386:  44%|████▊      | 105/237 [00:25<00:33,  3.92it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5380920767784119:  79%|████████▋  | 120/151 [00:26<00:07,  4.27it/s]Epoch: 1, train for the 92-th batch, train loss: 0.42096349596977234:  24%|██▉         | 92/383 [00:23<01:22,  3.54it/s]evaluate for the 7-th batch, evaluate loss: 0.7221032381057739:  24%|████▊               | 5/21 [00:00<00:01, 12.93it/s]evaluate for the 7-th batch, evaluate loss: 0.7221032381057739:  33%|██████▋             | 7/21 [00:00<00:00, 14.50it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5839263200759888:  53%|█████▊     | 127/241 [00:26<00:26,  4.37it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5839263200759888:  53%|█████▊     | 128/241 [00:26<00:23,  4.74it/s]evaluate for the 8-th batch, evaluate loss: 0.7453360557556152:  33%|██████▋             | 7/21 [00:00<00:00, 14.50it/s]Epoch: 1, train for the 106-th batch, train loss: 0.58347487449646:  44%|█████▊       | 105/237 [00:25<00:33,  3.92it/s]Epoch: 1, train for the 106-th batch, train loss: 0.58347487449646:  45%|█████▊       | 106/237 [00:25<00:30,  4.30it/s]evaluate for the 9-th batch, evaluate loss: 0.7625836133956909:  33%|██████▋             | 7/21 [00:00<00:00, 14.50it/s]evaluate for the 9-th batch, evaluate loss: 0.7625836133956909:  43%|████████▌           | 9/21 [00:00<00:00, 14.78it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5111778378486633:  53%|█████▊     | 128/241 [00:26<00:23,  4.74it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5111778378486633:  54%|█████▉     | 129/241 [00:26<00:20,  5.42it/s]evaluate for the 10-th batch, evaluate loss: 0.7649845480918884:  43%|████████▏          | 9/21 [00:00<00:00, 14.78it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5273492932319641:  24%|███          | 92/383 [00:24<01:22,  3.54it/s]Epoch: 1, train for the 93-th batch, train loss: 0.5273492932319641:  24%|███▏         | 93/383 [00:24<01:17,  3.72it/s]Epoch: 1, train for the 137-th batch, train loss: 0.5028939843177795:  93%|██████████▏| 136/146 [00:27<00:02,  4.72it/s]Epoch: 1, train for the 137-th batch, train loss: 0.5028939843177795:  94%|██████████▎| 137/146 [00:27<00:02,  4.09it/s]evaluate for the 11-th batch, evaluate loss: 0.7719883322715759:  43%|████████▏          | 9/21 [00:00<00:00, 14.78it/s]evaluate for the 11-th batch, evaluate loss: 0.7719883322715759:  52%|█████████▍        | 11/21 [00:00<00:00, 15.73it/s]evaluate for the 12-th batch, evaluate loss: 0.7234304547309875:  52%|█████████▍        | 11/21 [00:00<00:00, 15.73it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5059067606925964:  54%|█████▉     | 129/241 [00:26<00:20,  5.42it/s]Epoch: 1, train for the 121-th batch, train loss: 0.5072742104530334:  79%|████████▋  | 120/151 [00:27<00:07,  4.27it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5059067606925964:  54%|█████▉     | 130/241 [00:26<00:20,  5.45it/s]evaluate for the 13-th batch, evaluate loss: 0.7180419564247131:  52%|█████████▍        | 11/21 [00:00<00:00, 15.73it/s]evaluate for the 13-th batch, evaluate loss: 0.7180419564247131:  62%|███████████▏      | 13/21 [00:00<00:00, 15.54it/s]Epoch: 1, train for the 121-th batch, train loss: 0.5072742104530334:  80%|████████▊  | 121/151 [00:27<00:08,  3.50it/s]Epoch: 1, train for the 107-th batch, train loss: 0.5627509355545044:  45%|████▉      | 106/237 [00:26<00:30,  4.30it/s]Epoch: 1, train for the 107-th batch, train loss: 0.5627509355545044:  45%|████▉      | 107/237 [00:26<00:32,  3.98it/s]Epoch: 1, train for the 94-th batch, train loss: 0.4026309847831726:  24%|███▏         | 93/383 [00:24<01:17,  3.72it/s]evaluate for the 14-th batch, evaluate loss: 0.6981451511383057:  62%|███████████▏      | 13/21 [00:00<00:00, 15.54it/s]Epoch: 1, train for the 94-th batch, train loss: 0.4026309847831726:  25%|███▏         | 94/383 [00:24<01:15,  3.85it/s]Epoch: 1, train for the 131-th batch, train loss: 0.5548768043518066:  54%|█████▉     | 130/241 [00:26<00:20,  5.45it/s]evaluate for the 15-th batch, evaluate loss: 0.7243577241897583:  62%|███████████▏      | 13/21 [00:01<00:00, 15.54it/s]evaluate for the 15-th batch, evaluate loss: 0.7243577241897583:  71%|████████████▊     | 15/21 [00:01<00:00, 15.16it/s]Epoch: 1, train for the 138-th batch, train loss: 0.5482919812202454:  94%|██████████▎| 137/146 [00:28<00:02,  4.09it/s]Epoch: 1, train for the 138-th batch, train loss: 0.5482919812202454:  95%|██████████▍| 138/146 [00:28<00:02,  3.87it/s]Epoch: 1, train for the 131-th batch, train loss: 0.5548768043518066:  54%|█████▉     | 131/241 [00:26<00:19,  5.52it/s]Epoch: 1, train for the 122-th batch, train loss: 0.5007823705673218:  80%|████████▊  | 121/151 [00:27<00:08,  3.50it/s]Epoch: 1, train for the 122-th batch, train loss: 0.5007823705673218:  81%|████████▉  | 122/151 [00:27<00:07,  3.89it/s]evaluate for the 16-th batch, evaluate loss: 0.69526606798172:  71%|██████████████▎     | 15/21 [00:01<00:00, 15.16it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5332282781600952:  25%|███▏         | 94/383 [00:24<01:15,  3.85it/s]Epoch: 1, train for the 95-th batch, train loss: 0.5332282781600952:  25%|███▏         | 95/383 [00:24<01:08,  4.19it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5531509518623352:  95%|██████████▍| 138/146 [00:28<00:02,  3.87it/s]evaluate for the 17-th batch, evaluate loss: 0.6697567701339722:  71%|████████████▊     | 15/21 [00:01<00:00, 15.16it/s]evaluate for the 17-th batch, evaluate loss: 0.6697567701339722:  81%|██████████████▌   | 17/21 [00:01<00:00, 13.72it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5002352595329285:  54%|█████▉     | 131/241 [00:26<00:19,  5.52it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5531509518623352:  95%|██████████▍| 139/146 [00:28<00:01,  4.36it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5002352595329285:  55%|██████     | 132/241 [00:27<00:19,  5.50it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5856931805610657:  45%|████▉      | 107/237 [00:26<00:32,  3.98it/s]evaluate for the 18-th batch, evaluate loss: 0.6955434083938599:  81%|██████████████▌   | 17/21 [00:01<00:00, 13.72it/s]Epoch: 1, train for the 108-th batch, train loss: 0.5856931805610657:  46%|█████      | 108/237 [00:26<00:36,  3.54it/s]Epoch: 1, train for the 96-th batch, train loss: 0.4719903767108917:  25%|███▏         | 95/383 [00:24<01:08,  4.19it/s]Epoch: 1, train for the 123-th batch, train loss: 0.5660075545310974:  81%|████████▉  | 122/151 [00:27<00:07,  3.89it/s]Epoch: 1, train for the 96-th batch, train loss: 0.4719903767108917:  25%|███▎         | 96/383 [00:24<01:02,  4.59it/s]Epoch: 1, train for the 133-th batch, train loss: 0.44692283868789673:  55%|█████▍    | 132/241 [00:27<00:19,  5.50it/s]Epoch: 1, train for the 123-th batch, train loss: 0.5660075545310974:  81%|████████▉  | 123/151 [00:27<00:07,  3.84it/s]Epoch: 1, train for the 133-th batch, train loss: 0.44692283868789673:  55%|█████▌    | 133/241 [00:27<00:18,  5.95it/s]Epoch: 1, train for the 140-th batch, train loss: 0.523707926273346:  95%|███████████▍| 139/146 [00:28<00:01,  4.36it/s]evaluate for the 19-th batch, evaluate loss: 0.7325589656829834:  81%|██████████████▌   | 17/21 [00:01<00:00, 13.72it/s]evaluate for the 19-th batch, evaluate loss: 0.7325589656829834:  90%|████████████████▎ | 19/21 [00:01<00:00, 12.78it/s]Epoch: 1, train for the 140-th batch, train loss: 0.523707926273346:  96%|███████████▌| 140/146 [00:28<00:01,  4.64it/s]Epoch: 1, train for the 109-th batch, train loss: 0.6136753559112549:  46%|█████      | 108/237 [00:26<00:36,  3.54it/s]Epoch: 1, train for the 109-th batch, train loss: 0.6136753559112549:  46%|█████      | 109/237 [00:26<00:31,  4.02it/s]evaluate for the 20-th batch, evaluate loss: 0.7092244029045105:  90%|████████████████▎ | 19/21 [00:01<00:00, 12.78it/s]evaluate for the 21-th batch, evaluate loss: 0.9094957113265991:  90%|████████████████▎ | 19/21 [00:01<00:00, 12.78it/s]evaluate for the 21-th batch, evaluate loss: 0.9094957113265991: 100%|██████████████████| 21/21 [00:01<00:00, 13.28it/s]evaluate for the 21-th batch, evaluate loss: 0.9094957113265991: 100%|██████████████████| 21/21 [00:01<00:00, 13.43it/s]
Epoch: 1, train for the 134-th batch, train loss: 0.5360750555992126:  55%|██████     | 133/241 [00:27<00:18,  5.95it/s]Epoch: 1, train for the 97-th batch, train loss: 0.3784199357032776:  25%|███▎         | 96/383 [00:25<01:02,  4.59it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5360750555992126:  56%|██████     | 134/241 [00:27<00:18,  5.80it/s]Epoch: 1, train for the 124-th batch, train loss: 0.503486692905426:  81%|█████████▊  | 123/151 [00:27<00:07,  3.84it/s]Epoch: 1, train for the 97-th batch, train loss: 0.3784199357032776:  25%|███▎         | 97/383 [00:25<01:03,  4.52it/s]Epoch: 1, train for the 124-th batch, train loss: 0.503486692905426:  82%|█████████▊  | 124/151 [00:27<00:06,  4.02it/s]INFO:root:Epoch: 1, learning rate: 0.0001, train loss: 0.5396
INFO:root:train average_precision, 0.8224
INFO:root:train roc_auc, 0.8278
INFO:root:validate loss: 0.6108
INFO:root:validate average_precision, 0.7557
INFO:root:validate roc_auc, 0.7419
INFO:root:new node validate loss: 0.7393
INFO:root:new node validate first_1_average_precision, 0.6850
INFO:root:new node validate first_1_roc_auc, 0.6604
INFO:root:new node validate first_3_average_precision, 0.6721
INFO:root:new node validate first_3_roc_auc, 0.6458
INFO:root:new node validate first_10_average_precision, 0.6424
INFO:root:new node validate first_10_roc_auc, 0.6223
INFO:root:new node validate average_precision, 0.6049
INFO:root:new node validate roc_auc, 0.5865
INFO:root:save model ./saved_models/TGN/ia-reality-call/TGN_seed0_tgn-ia-reality-call-lincorrect-time-linear/TGN_seed0_tgn-ia-reality-call-lincorrect-time-linear.pkl
Epoch: 1, train for the 141-th batch, train loss: 0.5348172783851624:  96%|██████████▌| 140/146 [00:29<00:01,  4.64it/s]Epoch: 1, train for the 141-th batch, train loss: 0.5348172783851624:  97%|██████████▌| 141/146 [00:29<00:01,  3.71it/s]Epoch: 1, train for the 135-th batch, train loss: 0.40312379598617554:  56%|█████▌    | 134/241 [00:27<00:18,  5.80it/s]Epoch: 1, train for the 98-th batch, train loss: 0.339263916015625:  25%|███▌          | 97/383 [00:25<01:03,  4.52it/s]Epoch: 1, train for the 125-th batch, train loss: 0.550220251083374:  82%|█████████▊  | 124/151 [00:28<00:06,  4.02it/s]Epoch: 1, train for the 135-th batch, train loss: 0.40312379598617554:  56%|█████▌    | 135/241 [00:27<00:21,  4.94it/s]Epoch: 1, train for the 110-th batch, train loss: 0.577354371547699:  46%|█████▌      | 109/237 [00:27<00:31,  4.02it/s]Epoch: 1, train for the 125-th batch, train loss: 0.550220251083374:  83%|█████████▉  | 125/151 [00:28<00:06,  3.95it/s]Epoch: 1, train for the 98-th batch, train loss: 0.339263916015625:  26%|███▌          | 98/383 [00:25<01:07,  4.19it/s]Epoch: 1, train for the 110-th batch, train loss: 0.577354371547699:  46%|█████▌      | 110/237 [00:27<00:36,  3.44it/s]Epoch: 1, train for the 142-th batch, train loss: 0.5070449113845825:  97%|██████████▌| 141/146 [00:29<00:01,  3.71it/s]Epoch: 1, train for the 142-th batch, train loss: 0.5070449113845825:  97%|██████████▋| 142/146 [00:29<00:00,  4.50it/s]Epoch: 1, train for the 136-th batch, train loss: 0.4489826560020447:  56%|██████▏    | 135/241 [00:27<00:21,  4.94it/s]Epoch: 1, train for the 136-th batch, train loss: 0.4489826560020447:  56%|██████▏    | 136/241 [00:27<00:20,  5.10it/s]Epoch: 1, train for the 143-th batch, train loss: 0.5283958911895752:  97%|██████████▋| 142/146 [00:29<00:00,  4.50it/s]Epoch: 1, train for the 143-th batch, train loss: 0.5283958911895752:  98%|██████████▊| 143/146 [00:29<00:00,  5.22it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5255498886108398:  83%|█████████  | 125/151 [00:28<00:06,  3.95it/s]Epoch: 1, train for the 99-th batch, train loss: 0.4966159760951996:  26%|███▎         | 98/383 [00:25<01:07,  4.19it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5693441033363342:  46%|█████      | 110/237 [00:27<00:36,  3.44it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5255498886108398:  83%|█████████▏ | 126/151 [00:28<00:06,  4.10it/s]Epoch: 1, train for the 99-th batch, train loss: 0.4966159760951996:  26%|███▎         | 99/383 [00:25<01:06,  4.28it/s]Epoch: 1, train for the 111-th batch, train loss: 0.5693441033363342:  47%|█████▏     | 111/237 [00:27<00:34,  3.66it/s]Epoch: 1, train for the 144-th batch, train loss: 0.4893139600753784:  98%|██████████▊| 143/146 [00:29<00:00,  5.22it/s]Epoch: 1, train for the 144-th batch, train loss: 0.4893139600753784:  99%|██████████▊| 144/146 [00:29<00:00,  4.94it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]Epoch: 1, train for the 127-th batch, train loss: 0.49602389335632324:  83%|████████▎ | 126/151 [00:28<00:06,  4.10it/s]Epoch: 1, train for the 137-th batch, train loss: 0.4791136682033539:  56%|██████▏    | 136/241 [00:28<00:20,  5.10it/s]Epoch: 1, train for the 127-th batch, train loss: 0.49602389335632324:  84%|████████▍ | 127/151 [00:28<00:05,  4.18it/s]Epoch: 1, train for the 137-th batch, train loss: 0.4791136682033539:  57%|██████▎    | 137/241 [00:28<00:23,  4.43it/s]Epoch: 1, train for the 112-th batch, train loss: 0.5816563367843628:  47%|█████▏     | 111/237 [00:27<00:34,  3.66it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5667879581451416:  26%|███         | 99/383 [00:25<01:06,  4.28it/s]Epoch: 1, train for the 145-th batch, train loss: 0.5050992369651794:  99%|██████████▊| 144/146 [00:29<00:00,  4.94it/s]Epoch: 1, train for the 112-th batch, train loss: 0.5816563367843628:  47%|█████▏     | 112/237 [00:27<00:33,  3.74it/s]Epoch: 1, train for the 100-th batch, train loss: 0.5667879581451416:  26%|██▊        | 100/383 [00:25<01:10,  4.03it/s]Epoch: 1, train for the 145-th batch, train loss: 0.5050992369651794:  99%|██████████▉| 145/146 [00:29<00:00,  5.73it/s]Epoch: 1, train for the 138-th batch, train loss: 0.39337316155433655:  57%|█████▋    | 137/241 [00:28<00:23,  4.43it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5332163572311401:  84%|█████████▎ | 127/151 [00:28<00:05,  4.18it/s]Epoch: 1, train for the 138-th batch, train loss: 0.39337316155433655:  57%|█████▋    | 138/241 [00:28<00:22,  4.60it/s]Epoch: 2, train for the 1-th batch, train loss: 0.9548770785331726:   0%|                       | 0/119 [00:00<?, ?it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5332163572311401:  85%|█████████▎ | 128/151 [00:28<00:05,  4.34it/s]Epoch: 2, train for the 1-th batch, train loss: 0.9548770785331726:   1%|▏              | 1/119 [00:00<00:29,  4.06it/s]Epoch: 1, train for the 113-th batch, train loss: 0.5853753685951233:  47%|█████▏     | 112/237 [00:27<00:33,  3.74it/s]Epoch: 1, train for the 113-th batch, train loss: 0.5853753685951233:  48%|█████▏     | 113/237 [00:27<00:31,  3.91it/s]Epoch: 2, train for the 2-th batch, train loss: 1.0112791061401367:   1%|▏              | 1/119 [00:00<00:29,  4.06it/s]Epoch: 1, train for the 146-th batch, train loss: 0.47363531589508057:  99%|█████████▉| 145/146 [00:29<00:00,  5.73it/s]Epoch: 1, train for the 146-th batch, train loss: 0.47363531589508057: 100%|██████████| 146/146 [00:29<00:00,  4.84it/s]Epoch: 1, train for the 146-th batch, train loss: 0.47363531589508057: 100%|██████████| 146/146 [00:29<00:00,  4.89it/s]
Epoch: 1, train for the 129-th batch, train loss: 0.5248304605484009:  85%|█████████▎ | 128/151 [00:28<00:05,  4.34it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5248304605484009:  85%|█████████▍ | 129/151 [00:28<00:04,  4.74it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5345271825790405:  26%|██▊        | 100/383 [00:26<01:10,  4.03it/s]  0%|                                                                                            | 0/38 [00:00<?, ?it/s]Epoch: 1, train for the 139-th batch, train loss: 0.35510265827178955:  57%|█████▋    | 138/241 [00:28<00:22,  4.60it/s]Epoch: 1, train for the 114-th batch, train loss: 0.5501583218574524:  48%|█████▏     | 113/237 [00:28<00:31,  3.91it/s]Epoch: 2, train for the 3-th batch, train loss: 0.9263918995857239:   1%|▏              | 1/119 [00:00<00:29,  4.06it/s]Epoch: 1, train for the 101-th batch, train loss: 0.5345271825790405:  26%|██▉        | 101/383 [00:26<01:21,  3.44it/s]Epoch: 2, train for the 3-th batch, train loss: 0.9263918995857239:   3%|▍              | 3/119 [00:00<00:17,  6.64it/s]Epoch: 1, train for the 139-th batch, train loss: 0.35510265827178955:  58%|█████▊    | 139/241 [00:28<00:23,  4.40it/s]evaluate for the 1-th batch, evaluate loss: 0.5034996867179871:   0%|                            | 0/38 [00:00<?, ?it/s]Epoch: 1, train for the 114-th batch, train loss: 0.5501583218574524:  48%|█████▎     | 114/237 [00:28<00:29,  4.16it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5366311073303223:  85%|█████████▍ | 129/151 [00:29<00:04,  4.74it/s]evaluate for the 2-th batch, evaluate loss: 0.5165563225746155:   0%|                            | 0/38 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5165563225746155:   5%|█                   | 2/38 [00:00<00:02, 15.73it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5366311073303223:  86%|█████████▍ | 130/151 [00:29<00:04,  5.01it/s]Epoch: 2, train for the 4-th batch, train loss: 0.7424671649932861:   3%|▍              | 3/119 [00:00<00:17,  6.64it/s]Epoch: 2, train for the 4-th batch, train loss: 0.7424671649932861:   3%|▌              | 4/119 [00:00<00:17,  6.71it/s]evaluate for the 3-th batch, evaluate loss: 0.49356168508529663:   5%|█                  | 2/38 [00:00<00:02, 15.73it/s]Epoch: 1, train for the 140-th batch, train loss: 0.37465253472328186:  58%|█████▊    | 139/241 [00:28<00:23,  4.40it/s]Epoch: 1, train for the 140-th batch, train loss: 0.37465253472328186:  58%|█████▊    | 140/241 [00:28<00:21,  4.71it/s]evaluate for the 4-th batch, evaluate loss: 0.5055515170097351:   5%|█                   | 2/38 [00:00<00:02, 15.73it/s]evaluate for the 4-th batch, evaluate loss: 0.5055515170097351:  11%|██                  | 4/38 [00:00<00:02, 15.88it/s]Epoch: 1, train for the 115-th batch, train loss: 0.5339283347129822:  48%|█████▎     | 114/237 [00:28<00:29,  4.16it/s]Epoch: 1, train for the 115-th batch, train loss: 0.5339283347129822:  49%|█████▎     | 115/237 [00:28<00:28,  4.29it/s]Epoch: 1, train for the 102-th batch, train loss: 0.484028160572052:  26%|███▏        | 101/383 [00:26<01:21,  3.44it/s]Epoch: 2, train for the 5-th batch, train loss: 0.5919407606124878:   3%|▌              | 4/119 [00:00<00:17,  6.71it/s]Epoch: 1, train for the 131-th batch, train loss: 0.4580836296081543:  86%|█████████▍ | 130/151 [00:29<00:04,  5.01it/s]Epoch: 2, train for the 5-th batch, train loss: 0.5919407606124878:   4%|▋              | 5/119 [00:00<00:15,  7.17it/s]evaluate for the 5-th batch, evaluate loss: 0.5422025918960571:  11%|██                  | 4/38 [00:00<00:02, 15.88it/s]Epoch: 1, train for the 102-th batch, train loss: 0.484028160572052:  27%|███▏        | 102/383 [00:26<01:22,  3.42it/s]Epoch: 1, train for the 131-th batch, train loss: 0.4580836296081543:  87%|█████████▌ | 131/151 [00:29<00:03,  5.00it/s]evaluate for the 6-th batch, evaluate loss: 0.5264851450920105:  11%|██                  | 4/38 [00:00<00:02, 15.88it/s]evaluate for the 6-th batch, evaluate loss: 0.5264851450920105:  16%|███▏                | 6/38 [00:00<00:02, 15.91it/s]Epoch: 1, train for the 141-th batch, train loss: 0.4962709844112396:  58%|██████▍    | 140/241 [00:28<00:21,  4.71it/s]Epoch: 2, train for the 6-th batch, train loss: 0.510254442691803:   4%|▋               | 5/119 [00:00<00:15,  7.17it/s]Epoch: 1, train for the 141-th batch, train loss: 0.4962709844112396:  59%|██████▍    | 141/241 [00:28<00:21,  4.75it/s]Epoch: 2, train for the 6-th batch, train loss: 0.510254442691803:   5%|▊               | 6/119 [00:00<00:15,  7.33it/s]evaluate for the 7-th batch, evaluate loss: 0.4648680090904236:  16%|███▏                | 6/38 [00:00<00:02, 15.91it/s]evaluate for the 8-th batch, evaluate loss: 0.49175140261650085:  16%|███                | 6/38 [00:00<00:02, 15.91it/s]evaluate for the 8-th batch, evaluate loss: 0.49175140261650085:  21%|████               | 8/38 [00:00<00:01, 16.21it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5263385772705078:  87%|█████████▌ | 131/151 [00:29<00:03,  5.00it/s]Epoch: 1, train for the 103-th batch, train loss: 0.3282330334186554:  27%|██▉        | 102/383 [00:26<01:22,  3.42it/s]Epoch: 1, train for the 132-th batch, train loss: 0.5263385772705078:  87%|█████████▌ | 132/151 [00:29<00:03,  4.92it/s]Epoch: 1, train for the 142-th batch, train loss: 0.45848333835601807:  59%|█████▊    | 141/241 [00:29<00:21,  4.75it/s]Epoch: 1, train for the 116-th batch, train loss: 0.62882000207901:  49%|██████▎      | 115/237 [00:28<00:28,  4.29it/s]Epoch: 1, train for the 142-th batch, train loss: 0.45848333835601807:  59%|█████▉    | 142/241 [00:29<00:18,  5.32it/s]Epoch: 2, train for the 7-th batch, train loss: 0.546607255935669:   5%|▊               | 6/119 [00:01<00:15,  7.33it/s]evaluate for the 9-th batch, evaluate loss: 0.5112788081169128:  21%|████▏               | 8/38 [00:00<00:01, 16.21it/s]Epoch: 1, train for the 103-th batch, train loss: 0.3282330334186554:  27%|██▉        | 103/383 [00:26<01:18,  3.59it/s]Epoch: 2, train for the 7-th batch, train loss: 0.546607255935669:   6%|▉               | 7/119 [00:01<00:15,  7.19it/s]Epoch: 1, train for the 116-th batch, train loss: 0.62882000207901:  49%|██████▎      | 116/237 [00:28<00:30,  3.95it/s]evaluate for the 10-th batch, evaluate loss: 0.5387439131736755:  21%|████               | 8/38 [00:00<00:01, 16.21it/s]evaluate for the 10-th batch, evaluate loss: 0.5387439131736755:  26%|████▋             | 10/38 [00:00<00:02, 13.83it/s]Epoch: 1, train for the 133-th batch, train loss: 0.5172570943832397:  87%|█████████▌ | 132/151 [00:29<00:03,  4.92it/s]Epoch: 2, train for the 8-th batch, train loss: 0.4826032221317291:   6%|▉              | 7/119 [00:01<00:15,  7.19it/s]Epoch: 2, train for the 8-th batch, train loss: 0.4826032221317291:   7%|█              | 8/119 [00:01<00:16,  6.74it/s]evaluate for the 11-th batch, evaluate loss: 0.491984099149704:  26%|█████              | 10/38 [00:00<00:02, 13.83it/s]Epoch: 1, train for the 133-th batch, train loss: 0.5172570943832397:  88%|█████████▋ | 133/151 [00:29<00:03,  4.78it/s]Epoch: 1, train for the 117-th batch, train loss: 0.5673620700836182:  49%|█████▍     | 116/237 [00:28<00:30,  3.95it/s]Epoch: 1, train for the 117-th batch, train loss: 0.5673620700836182:  49%|█████▍     | 117/237 [00:28<00:30,  3.95it/s]Epoch: 2, train for the 9-th batch, train loss: 0.4983171820640564:   7%|█              | 8/119 [00:01<00:16,  6.74it/s]Epoch: 2, train for the 9-th batch, train loss: 0.4983171820640564:   8%|█▏             | 9/119 [00:01<00:14,  7.36it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5020995140075684:  88%|█████████▋ | 133/151 [00:29<00:03,  4.78it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5020995140075684:  89%|█████████▊ | 134/151 [00:29<00:03,  5.44it/s]Epoch: 2, train for the 10-th batch, train loss: 0.5434032082557678:   8%|█             | 9/119 [00:01<00:14,  7.36it/s]Epoch: 1, train for the 143-th batch, train loss: 0.4588488042354584:  59%|██████▍    | 142/241 [00:29<00:18,  5.32it/s]Epoch: 1, train for the 104-th batch, train loss: 0.4231724739074707:  27%|██▉        | 103/383 [00:27<01:18,  3.59it/s]Epoch: 1, train for the 143-th batch, train loss: 0.4588488042354584:  59%|██████▌    | 143/241 [00:29<00:25,  3.83it/s]evaluate for the 12-th batch, evaluate loss: 0.5552001595497131:  26%|████▋             | 10/38 [00:01<00:02, 13.83it/s]evaluate for the 12-th batch, evaluate loss: 0.5552001595497131:  32%|█████▋            | 12/38 [00:01<00:02,  9.69it/s]Epoch: 1, train for the 104-th batch, train loss: 0.4231724739074707:  27%|██▉        | 104/383 [00:27<01:30,  3.08it/s]Epoch: 1, train for the 135-th batch, train loss: 0.49167028069496155:  89%|████████▊ | 134/151 [00:30<00:03,  5.44it/s]Epoch: 1, train for the 118-th batch, train loss: 0.562131404876709:  49%|█████▉      | 117/237 [00:29<00:30,  3.95it/s]Epoch: 2, train for the 11-th batch, train loss: 0.49111098051071167:   8%|▉            | 9/119 [00:01<00:14,  7.36it/s]Epoch: 1, train for the 135-th batch, train loss: 0.49167028069496155:  89%|████████▉ | 135/151 [00:30<00:02,  5.55it/s]Epoch: 2, train for the 11-th batch, train loss: 0.49111098051071167:   9%|█           | 11/119 [00:01<00:13,  8.12it/s]evaluate for the 13-th batch, evaluate loss: 0.5162437558174133:  32%|█████▋            | 12/38 [00:01<00:02,  9.69it/s]Epoch: 1, train for the 118-th batch, train loss: 0.562131404876709:  50%|█████▉      | 118/237 [00:29<00:29,  3.98it/s]Epoch: 1, train for the 144-th batch, train loss: 0.5888596773147583:  59%|██████▌    | 143/241 [00:29<00:25,  3.83it/s]evaluate for the 14-th batch, evaluate loss: 0.4634144604206085:  32%|█████▋            | 12/38 [00:01<00:02,  9.69it/s]evaluate for the 14-th batch, evaluate loss: 0.4634144604206085:  37%|██████▋           | 14/38 [00:01<00:02, 10.73it/s]Epoch: 1, train for the 144-th batch, train loss: 0.5888596773147583:  60%|██████▌    | 144/241 [00:29<00:22,  4.32it/s]Epoch: 2, train for the 12-th batch, train loss: 0.5245625972747803:   9%|█▏           | 11/119 [00:01<00:13,  8.12it/s]Epoch: 2, train for the 12-th batch, train loss: 0.5245625972747803:  10%|█▎           | 12/119 [00:01<00:13,  7.80it/s]evaluate for the 15-th batch, evaluate loss: 0.49955496191978455:  37%|██████▎          | 14/38 [00:01<00:02, 10.73it/s]Epoch: 2, train for the 13-th batch, train loss: 0.4521298110485077:  10%|█▎           | 12/119 [00:01<00:13,  7.80it/s]Epoch: 1, train for the 105-th batch, train loss: 0.3255462944507599:  27%|██▉        | 104/383 [00:27<01:30,  3.08it/s]evaluate for the 16-th batch, evaluate loss: 0.5220211148262024:  37%|██████▋           | 14/38 [00:01<00:02, 10.73it/s]evaluate for the 16-th batch, evaluate loss: 0.5220211148262024:  42%|███████▌          | 16/38 [00:01<00:02, 10.28it/s]Epoch: 1, train for the 105-th batch, train loss: 0.3255462944507599:  27%|███        | 105/383 [00:27<01:32,  2.99it/s]Epoch: 2, train for the 14-th batch, train loss: 0.4672505557537079:  10%|█▎           | 12/119 [00:01<00:13,  7.80it/s]Epoch: 2, train for the 14-th batch, train loss: 0.4672505557537079:  12%|█▌           | 14/119 [00:01<00:12,  8.73it/s]evaluate for the 17-th batch, evaluate loss: 0.5053645968437195:  42%|███████▌          | 16/38 [00:01<00:02, 10.28it/s]Epoch: 1, train for the 145-th batch, train loss: 0.6293120980262756:  60%|██████▌    | 144/241 [00:29<00:22,  4.32it/s]Epoch: 1, train for the 119-th batch, train loss: 0.5364187359809875:  50%|█████▍     | 118/237 [00:29<00:29,  3.98it/s]Epoch: 1, train for the 145-th batch, train loss: 0.6293120980262756:  60%|██████▌    | 145/241 [00:29<00:24,  3.91it/s]Epoch: 1, train for the 119-th batch, train loss: 0.5364187359809875:  50%|█████▌     | 119/237 [00:29<00:35,  3.36it/s]Epoch: 1, train for the 136-th batch, train loss: 0.5487919449806213:  89%|█████████▊ | 135/151 [00:30<00:02,  5.55it/s]Epoch: 1, train for the 136-th batch, train loss: 0.5487919449806213:  90%|█████████▉ | 136/151 [00:30<00:03,  3.79it/s]evaluate for the 18-th batch, evaluate loss: 0.5184860825538635:  42%|███████▌          | 16/38 [00:01<00:02, 10.28it/s]evaluate for the 18-th batch, evaluate loss: 0.5184860825538635:  47%|████████▌         | 18/38 [00:01<00:01, 10.63it/s]Epoch: 2, train for the 15-th batch, train loss: 0.4745097756385803:  12%|█▌           | 14/119 [00:01<00:12,  8.73it/s]Epoch: 2, train for the 15-th batch, train loss: 0.4745097756385803:  13%|█▋           | 15/119 [00:01<00:12,  8.26it/s]Epoch: 1, train for the 106-th batch, train loss: 0.3200826048851013:  27%|███        | 105/383 [00:27<01:32,  2.99it/s]evaluate for the 19-th batch, evaluate loss: 0.48643678426742554:  47%|████████         | 18/38 [00:01<00:01, 10.63it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5610001087188721:  60%|██████▌    | 145/241 [00:30<00:24,  3.91it/s]Epoch: 1, train for the 106-th batch, train loss: 0.3200826048851013:  28%|███        | 106/383 [00:27<01:25,  3.26it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5610001087188721:  61%|██████▋    | 146/241 [00:30<00:21,  4.43it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5548347234725952:  50%|█████▌     | 119/237 [00:29<00:35,  3.36it/s]Epoch: 1, train for the 120-th batch, train loss: 0.5548347234725952:  51%|█████▌     | 120/237 [00:29<00:30,  3.86it/s]Epoch: 2, train for the 16-th batch, train loss: 0.5198491811752319:  13%|█▋           | 15/119 [00:02<00:12,  8.26it/s]evaluate for the 20-th batch, evaluate loss: 0.4408365786075592:  47%|████████▌         | 18/38 [00:01<00:01, 10.63it/s]evaluate for the 20-th batch, evaluate loss: 0.4408365786075592:  53%|█████████▍        | 20/38 [00:01<00:01, 11.37it/s]Epoch: 2, train for the 16-th batch, train loss: 0.5198491811752319:  13%|█▋           | 16/119 [00:02<00:13,  7.83it/s]Epoch: 1, train for the 137-th batch, train loss: 0.5392698049545288:  90%|█████████▉ | 136/151 [00:30<00:03,  3.79it/s]Epoch: 1, train for the 137-th batch, train loss: 0.5392698049545288:  91%|█████████▉ | 137/151 [00:30<00:03,  4.10it/s]Epoch: 1, train for the 107-th batch, train loss: 0.47771093249320984:  28%|██▊       | 106/383 [00:27<01:25,  3.26it/s]Epoch: 2, train for the 17-th batch, train loss: 0.43761682510375977:  13%|█▌          | 16/119 [00:02<00:13,  7.83it/s]Epoch: 2, train for the 17-th batch, train loss: 0.43761682510375977:  14%|█▋          | 17/119 [00:02<00:12,  8.19it/s]Epoch: 1, train for the 107-th batch, train loss: 0.47771093249320984:  28%|██▊       | 107/383 [00:28<01:15,  3.64it/s]Epoch: 2, train for the 18-th batch, train loss: 0.5145494937896729:  14%|█▊           | 17/119 [00:02<00:12,  8.19it/s]evaluate for the 21-th batch, evaluate loss: 0.44806671142578125:  53%|████████▉        | 20/38 [00:01<00:01, 11.37it/s]Epoch: 1, train for the 147-th batch, train loss: 0.557532787322998:  61%|███████▎    | 146/241 [00:30<00:21,  4.43it/s]evaluate for the 22-th batch, evaluate loss: 0.508347749710083:  53%|██████████         | 20/38 [00:01<00:01, 11.37it/s]evaluate for the 22-th batch, evaluate loss: 0.508347749710083:  58%|███████████        | 22/38 [00:01<00:01,  9.57it/s]Epoch: 1, train for the 138-th batch, train loss: 0.5292426943778992:  91%|█████████▉ | 137/151 [00:30<00:03,  4.10it/s]Epoch: 1, train for the 121-th batch, train loss: 0.5877308249473572:  51%|█████▌     | 120/237 [00:30<00:30,  3.86it/s]Epoch: 1, train for the 147-th batch, train loss: 0.557532787322998:  61%|███████▎    | 147/241 [00:30<00:24,  3.76it/s]Epoch: 1, train for the 108-th batch, train loss: 0.43677765130996704:  28%|██▊       | 107/383 [00:28<01:15,  3.64it/s]Epoch: 1, train for the 138-th batch, train loss: 0.5292426943778992:  91%|██████████ | 138/151 [00:31<00:03,  3.95it/s]Epoch: 1, train for the 121-th batch, train loss: 0.5877308249473572:  51%|█████▌     | 121/237 [00:30<00:33,  3.49it/s]Epoch: 1, train for the 108-th batch, train loss: 0.43677765130996704:  28%|██▊       | 108/383 [00:28<01:11,  3.87it/s]Epoch: 2, train for the 19-th batch, train loss: 0.5500885248184204:  14%|█▊           | 17/119 [00:02<00:12,  8.19it/s]evaluate for the 23-th batch, evaluate loss: 0.5052527785301208:  58%|██████████▍       | 22/38 [00:02<00:01,  9.57it/s]Epoch: 2, train for the 19-th batch, train loss: 0.5500885248184204:  16%|██           | 19/119 [00:02<00:12,  8.11it/s]evaluate for the 24-th batch, evaluate loss: 0.46923017501831055:  58%|█████████▊       | 22/38 [00:02<00:01,  9.57it/s]evaluate for the 24-th batch, evaluate loss: 0.46923017501831055:  63%|██████████▋      | 24/38 [00:02<00:01, 10.74it/s]Epoch: 1, train for the 148-th batch, train loss: 0.5100642442703247:  61%|██████▋    | 147/241 [00:30<00:24,  3.76it/s]Epoch: 1, train for the 148-th batch, train loss: 0.5100642442703247:  61%|██████▊    | 148/241 [00:30<00:21,  4.41it/s]Epoch: 2, train for the 20-th batch, train loss: 0.44783759117126465:  16%|█▉          | 19/119 [00:02<00:12,  8.11it/s]Epoch: 2, train for the 20-th batch, train loss: 0.44783759117126465:  17%|██          | 20/119 [00:02<00:12,  8.03it/s]Epoch: 1, train for the 149-th batch, train loss: 0.3807913362979889:  61%|██████▊    | 148/241 [00:30<00:21,  4.41it/s]Epoch: 1, train for the 149-th batch, train loss: 0.3807913362979889:  62%|██████▊    | 149/241 [00:30<00:18,  5.09it/s]Epoch: 2, train for the 21-th batch, train loss: 0.5033110976219177:  17%|██▏          | 20/119 [00:02<00:12,  8.03it/s]Epoch: 1, train for the 122-th batch, train loss: 0.550767183303833:  51%|██████▏     | 121/237 [00:30<00:33,  3.49it/s]Epoch: 1, train for the 109-th batch, train loss: 0.43023955821990967:  28%|██▊       | 108/383 [00:28<01:11,  3.87it/s]Epoch: 1, train for the 109-th batch, train loss: 0.43023955821990967:  28%|██▊       | 109/383 [00:28<01:12,  3.77it/s]Epoch: 1, train for the 122-th batch, train loss: 0.550767183303833:  51%|██████▏     | 122/237 [00:30<00:33,  3.45it/s]Epoch: 2, train for the 22-th batch, train loss: 0.5245932936668396:  17%|██▏          | 20/119 [00:02<00:12,  8.03it/s]Epoch: 2, train for the 22-th batch, train loss: 0.5245932936668396:  18%|██▍          | 22/119 [00:02<00:10,  9.22it/s]evaluate for the 25-th batch, evaluate loss: 0.5199058055877686:  63%|███████████▎      | 24/38 [00:02<00:01, 10.74it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5138490796089172:  91%|██████████ | 138/151 [00:31<00:03,  3.95it/s]Epoch: 1, train for the 150-th batch, train loss: 0.38003119826316833:  62%|██████▏   | 149/241 [00:30<00:18,  5.09it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5138490796089172:  92%|██████████▏| 139/151 [00:31<00:03,  3.20it/s]Epoch: 1, train for the 150-th batch, train loss: 0.38003119826316833:  62%|██████▏   | 150/241 [00:30<00:18,  5.04it/s]Epoch: 1, train for the 123-th batch, train loss: 0.5189517736434937:  51%|█████▋     | 122/237 [00:30<00:33,  3.45it/s]Epoch: 2, train for the 23-th batch, train loss: 0.5494613647460938:  18%|██▍          | 22/119 [00:02<00:10,  9.22it/s]evaluate for the 26-th batch, evaluate loss: 0.47888895869255066:  63%|██████████▋      | 24/38 [00:02<00:01, 10.74it/s]evaluate for the 26-th batch, evaluate loss: 0.47888895869255066:  68%|███████████▋     | 26/38 [00:02<00:01,  8.02it/s]Epoch: 1, train for the 123-th batch, train loss: 0.5189517736434937:  52%|█████▋     | 123/237 [00:30<00:29,  3.86it/s]Epoch: 2, train for the 23-th batch, train loss: 0.5494613647460938:  19%|██▌          | 23/119 [00:02<00:11,  8.42it/s]evaluate for the 27-th batch, evaluate loss: 0.5028800368309021:  68%|████████████▎     | 26/38 [00:02<00:01,  8.02it/s]Epoch: 1, train for the 151-th batch, train loss: 0.38788923621177673:  62%|██████▏   | 150/241 [00:31<00:18,  5.04it/s]Epoch: 1, train for the 151-th batch, train loss: 0.38788923621177673:  63%|██████▎   | 151/241 [00:31<00:16,  5.52it/s]Epoch: 2, train for the 24-th batch, train loss: 0.4792023301124573:  19%|██▌          | 23/119 [00:03<00:11,  8.42it/s]Epoch: 1, train for the 124-th batch, train loss: 0.5103901028633118:  52%|█████▋     | 123/237 [00:30<00:29,  3.86it/s]Epoch: 2, train for the 24-th batch, train loss: 0.4792023301124573:  20%|██▌          | 24/119 [00:03<00:11,  7.98it/s]evaluate for the 28-th batch, evaluate loss: 0.5024950504302979:  68%|████████████▎     | 26/38 [00:02<00:01,  8.02it/s]evaluate for the 28-th batch, evaluate loss: 0.5024950504302979:  74%|█████████████▎    | 28/38 [00:02<00:01,  9.05it/s]Epoch: 1, train for the 124-th batch, train loss: 0.5103901028633118:  52%|█████▊     | 124/237 [00:30<00:26,  4.21it/s]evaluate for the 29-th batch, evaluate loss: 0.48382309079170227:  74%|████████████▌    | 28/38 [00:02<00:01,  9.05it/s]Epoch: 2, train for the 25-th batch, train loss: 0.4623056650161743:  20%|██▌          | 24/119 [00:03<00:11,  7.98it/s]Epoch: 2, train for the 25-th batch, train loss: 0.4623056650161743:  21%|██▋          | 25/119 [00:03<00:11,  8.32it/s]Epoch: 1, train for the 140-th batch, train loss: 0.47064945101737976:  92%|█████████▏| 139/151 [00:31<00:03,  3.20it/s]Epoch: 1, train for the 140-th batch, train loss: 0.47064945101737976:  93%|█████████▎| 140/151 [00:31<00:03,  3.15it/s]Epoch: 2, train for the 26-th batch, train loss: 0.4669523239135742:  21%|██▋          | 25/119 [00:03<00:11,  8.32it/s]Epoch: 1, train for the 152-th batch, train loss: 0.31361234188079834:  63%|██████▎   | 151/241 [00:31<00:16,  5.52it/s]evaluate for the 30-th batch, evaluate loss: 0.5059493184089661:  74%|█████████████▎    | 28/38 [00:02<00:01,  9.05it/s]evaluate for the 30-th batch, evaluate loss: 0.5059493184089661:  79%|██████████████▏   | 30/38 [00:02<00:00,  8.93it/s]Epoch: 1, train for the 152-th batch, train loss: 0.31361234188079834:  63%|██████▎   | 152/241 [00:31<00:19,  4.52it/s]Epoch: 1, train for the 125-th batch, train loss: 0.5004327893257141:  52%|█████▊     | 124/237 [00:30<00:26,  4.21it/s]Epoch: 1, train for the 110-th batch, train loss: 0.3788513243198395:  28%|███▏       | 109/383 [00:29<01:12,  3.77it/s]Epoch: 1, train for the 141-th batch, train loss: 0.49475064873695374:  93%|█████████▎| 140/151 [00:31<00:03,  3.15it/s]Epoch: 1, train for the 125-th batch, train loss: 0.5004327893257141:  53%|█████▊     | 125/237 [00:30<00:27,  4.14it/s]Epoch: 1, train for the 141-th batch, train loss: 0.49475064873695374:  93%|█████████▎| 141/151 [00:31<00:02,  3.70it/s]Epoch: 1, train for the 110-th batch, train loss: 0.3788513243198395:  29%|███▏       | 110/383 [00:29<01:42,  2.65it/s]evaluate for the 31-th batch, evaluate loss: 0.5028449892997742:  79%|██████████████▏   | 30/38 [00:02<00:00,  8.93it/s]Epoch: 2, train for the 27-th batch, train loss: 0.5676084756851196:  21%|██▋          | 25/119 [00:03<00:11,  8.32it/s]Epoch: 2, train for the 27-th batch, train loss: 0.5676084756851196:  23%|██▉          | 27/119 [00:03<00:10,  8.45it/s]evaluate for the 32-th batch, evaluate loss: 0.4867396950721741:  79%|██████████████▏   | 30/38 [00:03<00:00,  8.93it/s]evaluate for the 32-th batch, evaluate loss: 0.4867396950721741:  84%|███████████████▏  | 32/38 [00:03<00:00, 10.31it/s]Epoch: 1, train for the 153-th batch, train loss: 0.36292633414268494:  63%|██████▎   | 152/241 [00:31<00:19,  4.52it/s]Epoch: 1, train for the 153-th batch, train loss: 0.36292633414268494:  63%|██████▎   | 153/241 [00:31<00:17,  5.15it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5468834042549133:  53%|█████▊     | 125/237 [00:31<00:27,  4.14it/s]Epoch: 2, train for the 28-th batch, train loss: 0.5518131852149963:  23%|██▉          | 27/119 [00:03<00:10,  8.45it/s]Epoch: 1, train for the 126-th batch, train loss: 0.5468834042549133:  53%|█████▊     | 126/237 [00:31<00:24,  4.55it/s]Epoch: 2, train for the 28-th batch, train loss: 0.5518131852149963:  24%|███          | 28/119 [00:03<00:10,  8.40it/s]Epoch: 1, train for the 142-th batch, train loss: 0.4949652850627899:  93%|██████████▎| 141/151 [00:32<00:02,  3.70it/s]Epoch: 1, train for the 142-th batch, train loss: 0.4949652850627899:  94%|██████████▎| 142/151 [00:32<00:02,  3.98it/s]Epoch: 2, train for the 29-th batch, train loss: 0.5034507513046265:  24%|███          | 28/119 [00:03<00:10,  8.40it/s]Epoch: 1, train for the 111-th batch, train loss: 0.4722537398338318:  29%|███▏       | 110/383 [00:29<01:42,  2.65it/s]Epoch: 1, train for the 111-th batch, train loss: 0.4722537398338318:  29%|███▏       | 111/383 [00:29<01:36,  2.81it/s]Epoch: 2, train for the 30-th batch, train loss: 0.528130054473877:  24%|███▎          | 28/119 [00:03<00:10,  8.40it/s]Epoch: 2, train for the 30-th batch, train loss: 0.528130054473877:  25%|███▌          | 30/119 [00:03<00:09,  9.38it/s]evaluate for the 33-th batch, evaluate loss: 0.4842696189880371:  84%|███████████████▏  | 32/38 [00:03<00:00, 10.31it/s]Epoch: 1, train for the 127-th batch, train loss: 0.533156156539917:  53%|██████▍     | 126/237 [00:31<00:24,  4.55it/s]Epoch: 1, train for the 154-th batch, train loss: 0.2757873237133026:  63%|██████▉    | 153/241 [00:31<00:17,  5.15it/s]Epoch: 1, train for the 127-th batch, train loss: 0.533156156539917:  54%|██████▍     | 127/237 [00:31<00:25,  4.24it/s]Epoch: 1, train for the 143-th batch, train loss: 0.4429550766944885:  94%|██████████▎| 142/151 [00:32<00:02,  3.98it/s]Epoch: 1, train for the 154-th batch, train loss: 0.2757873237133026:  64%|███████    | 154/241 [00:31<00:20,  4.15it/s]Epoch: 1, train for the 143-th batch, train loss: 0.4429550766944885:  95%|██████████▍| 143/151 [00:32<00:02,  3.97it/s]evaluate for the 34-th batch, evaluate loss: 0.49846792221069336:  84%|██████████████▎  | 32/38 [00:03<00:00, 10.31it/s]evaluate for the 34-th batch, evaluate loss: 0.49846792221069336:  89%|███████████████▏ | 34/38 [00:03<00:00,  7.79it/s]Epoch: 1, train for the 112-th batch, train loss: 0.4246317446231842:  29%|███▏       | 111/383 [00:29<01:36,  2.81it/s]Epoch: 2, train for the 31-th batch, train loss: 0.5093749761581421:  25%|███▎         | 30/119 [00:03<00:09,  9.38it/s]Epoch: 2, train for the 31-th batch, train loss: 0.5093749761581421:  26%|███▍         | 31/119 [00:03<00:10,  8.47it/s]Epoch: 1, train for the 112-th batch, train loss: 0.4246317446231842:  29%|███▏       | 112/383 [00:29<01:24,  3.19it/s]evaluate for the 35-th batch, evaluate loss: 0.5315691232681274:  89%|████████████████  | 34/38 [00:03<00:00,  7.79it/s]evaluate for the 36-th batch, evaluate loss: 0.5328254699707031:  89%|████████████████  | 34/38 [00:03<00:00,  7.79it/s]evaluate for the 36-th batch, evaluate loss: 0.5328254699707031:  95%|█████████████████ | 36/38 [00:03<00:00,  9.21it/s]Epoch: 1, train for the 144-th batch, train loss: 0.4637793302536011:  95%|██████████▍| 143/151 [00:32<00:02,  3.97it/s]Epoch: 1, train for the 155-th batch, train loss: 0.29781538248062134:  64%|██████▍   | 154/241 [00:32<00:20,  4.15it/s]Epoch: 1, train for the 144-th batch, train loss: 0.4637793302536011:  95%|██████████▍| 144/151 [00:32<00:01,  4.33it/s]Epoch: 2, train for the 32-th batch, train loss: 0.43710288405418396:  26%|███▏        | 31/119 [00:04<00:10,  8.47it/s]Epoch: 2, train for the 32-th batch, train loss: 0.43710288405418396:  27%|███▏        | 32/119 [00:04<00:10,  8.02it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5368092060089111:  54%|█████▉     | 127/237 [00:31<00:25,  4.24it/s]Epoch: 1, train for the 155-th batch, train loss: 0.29781538248062134:  64%|██████▍   | 155/241 [00:32<00:20,  4.26it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5368092060089111:  54%|█████▉     | 128/237 [00:31<00:26,  4.18it/s]Epoch: 2, train for the 33-th batch, train loss: 0.5292007327079773:  27%|███▍         | 32/119 [00:04<00:10,  8.02it/s]Epoch: 2, train for the 33-th batch, train loss: 0.5292007327079773:  28%|███▌         | 33/119 [00:04<00:10,  8.23it/s]evaluate for the 37-th batch, evaluate loss: 0.4603431522846222:  95%|█████████████████ | 36/38 [00:03<00:00,  9.21it/s]evaluate for the 38-th batch, evaluate loss: 0.4990321099758148:  95%|█████████████████ | 36/38 [00:03<00:00,  9.21it/s]evaluate for the 38-th batch, evaluate loss: 0.4990321099758148: 100%|██████████████████| 38/38 [00:03<00:00,  9.08it/s]evaluate for the 38-th batch, evaluate loss: 0.4990321099758148: 100%|██████████████████| 38/38 [00:03<00:00, 10.09it/s]
Epoch: 1, train for the 113-th batch, train loss: 0.4281674921512604:  29%|███▏       | 112/383 [00:30<01:24,  3.19it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5572707653045654:  54%|█████▉     | 128/237 [00:31<00:26,  4.18it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5014763474464417:  28%|███▌         | 33/119 [00:04<00:10,  8.23it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5014763474464417:  29%|███▋         | 34/119 [00:04<00:10,  7.73it/s]Epoch: 1, train for the 145-th batch, train loss: 0.4968518614768982:  95%|██████████▍| 144/151 [00:32<00:01,  4.33it/s]Epoch: 1, train for the 156-th batch, train loss: 0.21976043283939362:  64%|██████▍   | 155/241 [00:32<00:20,  4.26it/s]Epoch: 1, train for the 113-th batch, train loss: 0.4281674921512604:  30%|███▏       | 113/383 [00:30<01:31,  2.97it/s]Epoch: 1, train for the 129-th batch, train loss: 0.5572707653045654:  54%|█████▉     | 129/237 [00:31<00:25,  4.17it/s]Epoch: 1, train for the 145-th batch, train loss: 0.4968518614768982:  96%|██████████▌| 145/151 [00:32<00:01,  3.96it/s]Epoch: 1, train for the 156-th batch, train loss: 0.21976043283939362:  65%|██████▍   | 156/241 [00:32<00:21,  4.01it/s]Epoch: 2, train for the 35-th batch, train loss: 0.4734361171722412:  29%|███▋         | 34/119 [00:04<00:10,  7.73it/s]Epoch: 2, train for the 35-th batch, train loss: 0.4734361171722412:  29%|███▊         | 35/119 [00:04<00:10,  7.91it/s]  0%|                                                                                            | 0/20 [00:00<?, ?it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5075656771659851:  96%|██████████▌| 145/151 [00:33<00:01,  3.96it/s]Epoch: 1, train for the 157-th batch, train loss: 0.19115354120731354:  65%|██████▍   | 156/241 [00:32<00:21,  4.01it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5075656771659851:  97%|██████████▋| 146/151 [00:33<00:01,  4.14it/s]Epoch: 1, train for the 157-th batch, train loss: 0.19115354120731354:  65%|██████▌   | 157/241 [00:32<00:20,  4.17it/s]Epoch: 2, train for the 36-th batch, train loss: 0.5214536786079407:  29%|███▊         | 35/119 [00:04<00:10,  7.91it/s]Epoch: 2, train for the 36-th batch, train loss: 0.5214536786079407:  30%|███▉         | 36/119 [00:04<00:10,  7.61it/s]evaluate for the 1-th batch, evaluate loss: 0.7204946875572205:   0%|                            | 0/20 [00:00<?, ?it/s]Epoch: 1, train for the 114-th batch, train loss: 0.3919547200202942:  30%|███▏       | 113/383 [00:30<01:31,  2.97it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5404796600341797:  54%|█████▉     | 129/237 [00:32<00:25,  4.17it/s]Epoch: 1, train for the 114-th batch, train loss: 0.3919547200202942:  30%|███▎       | 114/383 [00:30<01:28,  3.04it/s]Epoch: 1, train for the 130-th batch, train loss: 0.5404796600341797:  55%|██████     | 130/237 [00:32<00:28,  3.80it/s]evaluate for the 2-th batch, evaluate loss: 0.7608678340911865:   0%|                            | 0/20 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.7608678340911865:  10%|██                  | 2/20 [00:00<00:01, 14.39it/s]Epoch: 2, train for the 37-th batch, train loss: 0.5048972964286804:  30%|███▉         | 36/119 [00:04<00:10,  7.61it/s]Epoch: 2, train for the 37-th batch, train loss: 0.5048972964286804:  31%|████         | 37/119 [00:04<00:10,  7.86it/s]evaluate for the 3-th batch, evaluate loss: 0.6634619235992432:  10%|██                  | 2/20 [00:00<00:01, 14.39it/s]Epoch: 1, train for the 158-th batch, train loss: 0.20901130139827728:  65%|██████▌   | 157/241 [00:32<00:20,  4.17it/s]Epoch: 1, train for the 147-th batch, train loss: 0.49810275435447693:  97%|█████████▋| 146/151 [00:33<00:01,  4.14it/s]Epoch: 1, train for the 158-th batch, train loss: 0.20901130139827728:  66%|██████▌   | 158/241 [00:32<00:18,  4.37it/s]Epoch: 1, train for the 147-th batch, train loss: 0.49810275435447693:  97%|█████████▋| 147/151 [00:33<00:00,  4.15it/s]evaluate for the 4-th batch, evaluate loss: 0.6826355457305908:  10%|██                  | 2/20 [00:00<00:01, 14.39it/s]evaluate for the 4-th batch, evaluate loss: 0.6826355457305908:  20%|████                | 4/20 [00:00<00:01, 14.13it/s]Epoch: 1, train for the 115-th batch, train loss: 0.3554070293903351:  30%|███▎       | 114/383 [00:30<01:28,  3.04it/s]Epoch: 2, train for the 38-th batch, train loss: 0.4999670386314392:  31%|████         | 37/119 [00:04<00:10,  7.86it/s]Epoch: 1, train for the 115-th batch, train loss: 0.3554070293903351:  30%|███▎       | 115/383 [00:30<01:18,  3.40it/s]Epoch: 1, train for the 131-th batch, train loss: 0.51349937915802:  55%|███████▏     | 130/237 [00:32<00:28,  3.80it/s]Epoch: 2, train for the 38-th batch, train loss: 0.4999670386314392:  32%|████▏        | 38/119 [00:04<00:11,  7.30it/s]evaluate for the 5-th batch, evaluate loss: 0.6897907853126526:  20%|████                | 4/20 [00:00<00:01, 14.13it/s]Epoch: 1, train for the 159-th batch, train loss: 0.12286126613616943:  66%|██████▌   | 158/241 [00:32<00:18,  4.37it/s]Epoch: 1, train for the 131-th batch, train loss: 0.51349937915802:  55%|███████▏     | 131/237 [00:32<00:27,  3.82it/s]Epoch: 1, train for the 159-th batch, train loss: 0.12286126613616943:  66%|██████▌   | 159/241 [00:32<00:17,  4.82it/s]evaluate for the 6-th batch, evaluate loss: 0.7120781540870667:  20%|████                | 4/20 [00:00<00:01, 14.13it/s]evaluate for the 6-th batch, evaluate loss: 0.7120781540870667:  30%|██████              | 6/20 [00:00<00:00, 14.66it/s]Epoch: 2, train for the 39-th batch, train loss: 0.5366382002830505:  32%|████▏        | 38/119 [00:04<00:11,  7.30it/s]Epoch: 2, train for the 39-th batch, train loss: 0.5366382002830505:  33%|████▎        | 39/119 [00:04<00:10,  7.46it/s]evaluate for the 7-th batch, evaluate loss: 0.7532599568367004:  30%|██████              | 6/20 [00:00<00:00, 14.66it/s]evaluate for the 8-th batch, evaluate loss: 0.692454993724823:  30%|██████▎              | 6/20 [00:00<00:00, 14.66it/s]evaluate for the 8-th batch, evaluate loss: 0.692454993724823:  40%|████████▍            | 8/20 [00:00<00:00, 14.57it/s]Epoch: 1, train for the 132-th batch, train loss: 0.563211977481842:  55%|██████▋     | 131/237 [00:32<00:27,  3.82it/s]Epoch: 1, train for the 132-th batch, train loss: 0.563211977481842:  56%|██████▋     | 132/237 [00:32<00:25,  4.11it/s]Epoch: 2, train for the 40-th batch, train loss: 0.5047042369842529:  33%|████▎        | 39/119 [00:05<00:10,  7.46it/s]Epoch: 2, train for the 40-th batch, train loss: 0.5047042369842529:  34%|████▎        | 40/119 [00:05<00:10,  7.49it/s]evaluate for the 9-th batch, evaluate loss: 0.6365994215011597:  40%|████████            | 8/20 [00:00<00:00, 14.57it/s]Epoch: 1, train for the 148-th batch, train loss: 0.47104963660240173:  97%|█████████▋| 147/151 [00:33<00:00,  4.15it/s]Epoch: 1, train for the 148-th batch, train loss: 0.47104963660240173:  98%|█████████▊| 148/151 [00:33<00:00,  3.57it/s]evaluate for the 10-th batch, evaluate loss: 0.656836986541748:  40%|████████            | 8/20 [00:00<00:00, 14.57it/s]evaluate for the 10-th batch, evaluate loss: 0.656836986541748:  50%|█████████▌         | 10/20 [00:00<00:00, 15.39it/s]Epoch: 1, train for the 160-th batch, train loss: 0.6486974954605103:  66%|███████▎   | 159/241 [00:33<00:17,  4.82it/s]Epoch: 2, train for the 41-th batch, train loss: 0.5051063299179077:  34%|████▎        | 40/119 [00:05<00:10,  7.49it/s]Epoch: 1, train for the 116-th batch, train loss: 0.35152286291122437:  30%|███       | 115/383 [00:30<01:18,  3.40it/s]Epoch: 2, train for the 41-th batch, train loss: 0.5051063299179077:  34%|████▍        | 41/119 [00:05<00:09,  7.92it/s]Epoch: 1, train for the 160-th batch, train loss: 0.6486974954605103:  66%|███████▎   | 160/241 [00:33<00:19,  4.24it/s]Epoch: 1, train for the 116-th batch, train loss: 0.35152286291122437:  30%|███       | 116/383 [00:30<01:28,  3.03it/s]evaluate for the 11-th batch, evaluate loss: 0.6142393350601196:  50%|█████████         | 10/20 [00:00<00:00, 15.39it/s]Epoch: 1, train for the 133-th batch, train loss: 0.5132474899291992:  56%|██████▏    | 132/237 [00:32<00:25,  4.11it/s]Epoch: 1, train for the 133-th batch, train loss: 0.5132474899291992:  56%|██████▏    | 133/237 [00:32<00:24,  4.30it/s]Epoch: 1, train for the 161-th batch, train loss: 0.3511649966239929:  66%|███████▎   | 160/241 [00:33<00:19,  4.24it/s]evaluate for the 12-th batch, evaluate loss: 0.6732004880905151:  50%|█████████         | 10/20 [00:00<00:00, 15.39it/s]evaluate for the 12-th batch, evaluate loss: 0.6732004880905151:  60%|██████████▊       | 12/20 [00:00<00:00, 13.78it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5310019850730896:  34%|████▍        | 41/119 [00:05<00:09,  7.92it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5310019850730896:  35%|████▌        | 42/119 [00:05<00:10,  7.41it/s]Epoch: 1, train for the 161-th batch, train loss: 0.3511649966239929:  67%|███████▎   | 161/241 [00:33<00:16,  4.72it/s]evaluate for the 13-th batch, evaluate loss: 0.6961327791213989:  60%|██████████▊       | 12/20 [00:00<00:00, 13.78it/s]Epoch: 1, train for the 162-th batch, train loss: 0.1364031732082367:  67%|███████▎   | 161/241 [00:33<00:16,  4.72it/s]Epoch: 1, train for the 162-th batch, train loss: 0.1364031732082367:  67%|███████▍   | 162/241 [00:33<00:14,  5.37it/s]Epoch: 2, train for the 43-th batch, train loss: 0.5341609716415405:  35%|████▌        | 42/119 [00:05<00:10,  7.41it/s]Epoch: 2, train for the 43-th batch, train loss: 0.5341609716415405:  36%|████▋        | 43/119 [00:05<00:10,  7.26it/s]evaluate for the 14-th batch, evaluate loss: 0.707684338092804:  60%|███████████▍       | 12/20 [00:01<00:00, 13.78it/s]evaluate for the 14-th batch, evaluate loss: 0.707684338092804:  70%|█████████████▎     | 14/20 [00:01<00:00, 13.22it/s]Epoch: 1, train for the 117-th batch, train loss: 0.3302624523639679:  30%|███▎       | 116/383 [00:31<01:28,  3.03it/s]evaluate for the 15-th batch, evaluate loss: 0.688606858253479:  70%|█████████████▎     | 14/20 [00:01<00:00, 13.22it/s]Epoch: 1, train for the 117-th batch, train loss: 0.3302624523639679:  31%|███▎       | 117/383 [00:31<01:29,  2.98it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5075398683547974:  36%|████▋        | 43/119 [00:05<00:10,  7.26it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5075398683547974:  37%|████▊        | 44/119 [00:05<00:10,  7.17it/s]evaluate for the 16-th batch, evaluate loss: 0.6563220620155334:  70%|████████████▌     | 14/20 [00:01<00:00, 13.22it/s]evaluate for the 16-th batch, evaluate loss: 0.6563220620155334:  80%|██████████████▍   | 16/20 [00:01<00:00, 13.42it/s]Epoch: 1, train for the 163-th batch, train loss: 0.14016205072402954:  67%|██████▋   | 162/241 [00:33<00:14,  5.37it/s]Epoch: 1, train for the 149-th batch, train loss: 0.4409342110157013:  98%|██████████▊| 148/151 [00:34<00:00,  3.57it/s]Epoch: 1, train for the 163-th batch, train loss: 0.14016205072402954:  68%|██████▊   | 163/241 [00:33<00:14,  5.26it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5686830282211304:  56%|██████▏    | 133/237 [00:33<00:24,  4.30it/s]Epoch: 1, train for the 149-th batch, train loss: 0.4409342110157013:  99%|██████████▊| 149/151 [00:34<00:00,  2.75it/s]evaluate for the 17-th batch, evaluate loss: 0.6917728781700134:  80%|██████████████▍   | 16/20 [00:01<00:00, 13.42it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5686830282211304:  57%|██████▏    | 134/237 [00:33<00:30,  3.38it/s]Epoch: 1, train for the 118-th batch, train loss: 0.32719793915748596:  31%|███       | 117/383 [00:31<01:29,  2.98it/s]Epoch: 2, train for the 45-th batch, train loss: 0.5293031930923462:  37%|████▊        | 44/119 [00:05<00:10,  7.17it/s]Epoch: 2, train for the 45-th batch, train loss: 0.5293031930923462:  38%|████▉        | 45/119 [00:05<00:09,  7.53it/s]evaluate for the 18-th batch, evaluate loss: 0.6811991930007935:  80%|██████████████▍   | 16/20 [00:01<00:00, 13.42it/s]evaluate for the 18-th batch, evaluate loss: 0.6811991930007935:  90%|████████████████▏ | 18/20 [00:01<00:00, 13.73it/s]Epoch: 1, train for the 118-th batch, train loss: 0.32719793915748596:  31%|███       | 118/383 [00:31<01:18,  3.37it/s]Epoch: 1, train for the 164-th batch, train loss: 0.11291497945785522:  68%|██████▊   | 163/241 [00:33<00:14,  5.26it/s]Epoch: 1, train for the 164-th batch, train loss: 0.11291497945785522:  68%|██████▊   | 164/241 [00:33<00:14,  5.46it/s]Epoch: 2, train for the 46-th batch, train loss: 0.5140731930732727:  38%|████▉        | 45/119 [00:05<00:09,  7.53it/s]evaluate for the 19-th batch, evaluate loss: 0.7252517342567444:  90%|████████████████▏ | 18/20 [00:01<00:00, 13.73it/s]Epoch: 2, train for the 46-th batch, train loss: 0.5140731930732727:  39%|█████        | 46/119 [00:05<00:09,  7.44it/s]evaluate for the 20-th batch, evaluate loss: 0.7017879486083984:  90%|████████████████▏ | 18/20 [00:01<00:00, 13.73it/s]evaluate for the 20-th batch, evaluate loss: 0.7017879486083984: 100%|██████████████████| 20/20 [00:01<00:00, 13.52it/s]evaluate for the 20-th batch, evaluate loss: 0.7017879486083984: 100%|██████████████████| 20/20 [00:01<00:00, 13.86it/s]
Epoch: 1, train for the 150-th batch, train loss: 0.5312080383300781:  99%|██████████▊| 149/151 [00:34<00:00,  2.75it/s]Epoch: 1, train for the 150-th batch, train loss: 0.5312080383300781:  99%|██████████▉| 150/151 [00:34<00:00,  2.80it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5603384971618652:  39%|█████        | 46/119 [00:06<00:09,  7.44it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5603384971618652:  39%|█████▏       | 47/119 [00:06<00:10,  6.88it/s]INFO:root:Epoch: 1, learning rate: 0.0001, train loss: 0.6099
INFO:root:train average_precision, 0.7527
INFO:root:train roc_auc, 0.7485
INFO:root:validate loss: 0.5004
INFO:root:validate average_precision, 0.8423
INFO:root:validate roc_auc, 0.8405
INFO:root:new node validate loss: 0.6902
INFO:root:new node validate first_1_average_precision, 0.5810
INFO:root:new node validate first_1_roc_auc, 0.5085
INFO:root:new node validate first_3_average_precision, 0.5955
INFO:root:new node validate first_3_roc_auc, 0.5565
INFO:root:new node validate first_10_average_precision, 0.6061
INFO:root:new node validate first_10_roc_auc, 0.6000
INFO:root:new node validate average_precision, 0.6525
INFO:root:new node validate roc_auc, 0.6558
INFO:root:save model ./saved_models/TGN/ia-escorts-dynamic/TGN_seed0_tgn-ia-escorts-dynamic-lincorrect-time-linear/TGN_seed0_tgn-ia-escorts-dynamic-lincorrect-time-linear.pkl
Epoch: 1, train for the 165-th batch, train loss: 0.2000318467617035:  68%|███████▍   | 164/241 [00:34<00:14,  5.46it/s]Epoch: 1, train for the 119-th batch, train loss: 0.381992369890213:  31%|███▋        | 118/383 [00:31<01:18,  3.37it/s]Epoch: 1, train for the 165-th batch, train loss: 0.2000318467617035:  68%|███████▌   | 165/241 [00:34<00:16,  4.58it/s]Epoch: 1, train for the 135-th batch, train loss: 0.5992160439491272:  57%|██████▏    | 134/237 [00:33<00:30,  3.38it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5263885259628296:  99%|██████████▉| 150/151 [00:34<00:00,  2.80it/s]Epoch: 2, train for the 48-th batch, train loss: 0.549763560295105:  39%|█████▌        | 47/119 [00:06<00:10,  6.88it/s]Epoch: 1, train for the 119-th batch, train loss: 0.381992369890213:  31%|███▋        | 119/383 [00:31<01:25,  3.07it/s]Epoch: 2, train for the 48-th batch, train loss: 0.549763560295105:  40%|█████▋        | 48/119 [00:06<00:09,  7.26it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5263885259628296: 100%|███████████| 151/151 [00:34<00:00,  3.41it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5263885259628296: 100%|███████████| 151/151 [00:34<00:00,  4.34it/s]
Epoch: 1, train for the 135-th batch, train loss: 0.5992160439491272:  57%|██████▎    | 135/237 [00:33<00:35,  2.89it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5143571496009827:  40%|█████▏       | 48/119 [00:06<00:09,  7.26it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5143571496009827:  41%|█████▎       | 49/119 [00:06<00:09,  7.31it/s]Epoch: 1, train for the 166-th batch, train loss: 0.2419765740633011:  68%|███████▌   | 165/241 [00:34<00:16,  4.58it/s]  0%|                                                                                            | 0/46 [00:00<?, ?it/s]Epoch: 1, train for the 166-th batch, train loss: 0.2419765740633011:  69%|███████▌   | 166/241 [00:34<00:16,  4.61it/s]Epoch: 1, train for the 136-th batch, train loss: 0.599542498588562:  57%|██████▊     | 135/237 [00:33<00:35,  2.89it/s]Epoch: 1, train for the 120-th batch, train loss: 0.41779136657714844:  31%|███       | 119/383 [00:32<01:25,  3.07it/s]Epoch: 2, train for the 50-th batch, train loss: 0.4920198321342468:  41%|█████▎       | 49/119 [00:06<00:09,  7.31it/s]evaluate for the 1-th batch, evaluate loss: 0.5525270104408264:   0%|                            | 0/46 [00:00<?, ?it/s]Epoch: 2, train for the 50-th batch, train loss: 0.4920198321342468:  42%|█████▍       | 50/119 [00:06<00:08,  7.93it/s]Epoch: 1, train for the 136-th batch, train loss: 0.599542498588562:  57%|██████▉     | 136/237 [00:34<00:31,  3.19it/s]Epoch: 1, train for the 120-th batch, train loss: 0.41779136657714844:  31%|███▏      | 120/383 [00:32<01:21,  3.25it/s]evaluate for the 2-th batch, evaluate loss: 0.5419986844062805:   0%|                            | 0/46 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5419986844062805:   4%|▊                   | 2/46 [00:00<00:03, 14.53it/s]Epoch: 2, train for the 51-th batch, train loss: 0.5690444707870483:  42%|█████▍       | 50/119 [00:06<00:08,  7.93it/s]Epoch: 1, train for the 167-th batch, train loss: 0.11371707916259766:  69%|██████▉   | 166/241 [00:34<00:16,  4.61it/s]evaluate for the 3-th batch, evaluate loss: 0.5298249125480652:   4%|▊                   | 2/46 [00:00<00:03, 14.53it/s]Epoch: 2, train for the 52-th batch, train loss: 0.55396568775177:  42%|██████▎        | 50/119 [00:06<00:08,  7.93it/s]Epoch: 2, train for the 52-th batch, train loss: 0.55396568775177:  44%|██████▌        | 52/119 [00:06<00:07,  8.87it/s]Epoch: 1, train for the 167-th batch, train loss: 0.11371707916259766:  69%|██████▉   | 167/241 [00:34<00:16,  4.42it/s]Epoch: 1, train for the 137-th batch, train loss: 0.5652818083763123:  57%|██████▎    | 136/237 [00:34<00:31,  3.19it/s]Epoch: 1, train for the 121-th batch, train loss: 0.43217992782592773:  31%|███▏      | 120/383 [00:32<01:21,  3.25it/s]Epoch: 1, train for the 137-th batch, train loss: 0.5652818083763123:  58%|██████▎    | 137/237 [00:34<00:28,  3.48it/s]evaluate for the 4-th batch, evaluate loss: 0.529204249382019:   4%|▉                    | 2/46 [00:00<00:03, 14.53it/s]evaluate for the 4-th batch, evaluate loss: 0.529204249382019:   9%|█▊                   | 4/46 [00:00<00:03, 11.90it/s]Epoch: 1, train for the 121-th batch, train loss: 0.43217992782592773:  32%|███▏      | 121/383 [00:32<01:16,  3.43it/s]Epoch: 2, train for the 53-th batch, train loss: 0.4906877875328064:  44%|█████▋       | 52/119 [00:06<00:07,  8.87it/s]Epoch: 2, train for the 53-th batch, train loss: 0.4906877875328064:  45%|█████▊       | 53/119 [00:06<00:07,  8.96it/s]evaluate for the 5-th batch, evaluate loss: 0.5466919541358948:   9%|█▋                  | 4/46 [00:00<00:03, 11.90it/s]Epoch: 1, train for the 168-th batch, train loss: 0.1149299368262291:  69%|███████▌   | 167/241 [00:34<00:16,  4.42it/s]Epoch: 1, train for the 138-th batch, train loss: 0.6011638641357422:  58%|██████▎    | 137/237 [00:34<00:28,  3.48it/s]Epoch: 1, train for the 168-th batch, train loss: 0.1149299368262291:  70%|███████▋   | 168/241 [00:34<00:15,  4.61it/s]Epoch: 2, train for the 54-th batch, train loss: 0.5342762470245361:  45%|█████▊       | 53/119 [00:06<00:07,  8.96it/s]Epoch: 2, train for the 54-th batch, train loss: 0.5342762470245361:  45%|█████▉       | 54/119 [00:06<00:07,  8.88it/s]Epoch: 1, train for the 138-th batch, train loss: 0.6011638641357422:  58%|██████▍    | 138/237 [00:34<00:25,  3.88it/s]evaluate for the 6-th batch, evaluate loss: 0.5347265005111694:   9%|█▋                  | 4/46 [00:00<00:03, 11.90it/s]evaluate for the 6-th batch, evaluate loss: 0.5347265005111694:  13%|██▌                 | 6/46 [00:00<00:03, 11.47it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5007867813110352:  45%|█████▉       | 54/119 [00:06<00:07,  8.88it/s]evaluate for the 7-th batch, evaluate loss: 0.5631925463676453:  13%|██▌                 | 6/46 [00:00<00:03, 11.47it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5007867813110352:  46%|██████       | 55/119 [00:06<00:07,  8.94it/s]Epoch: 1, train for the 169-th batch, train loss: 0.6220358610153198:  70%|███████▋   | 168/241 [00:35<00:15,  4.61it/s]Epoch: 1, train for the 122-th batch, train loss: 0.4998129904270172:  32%|███▍       | 121/383 [00:32<01:16,  3.43it/s]Epoch: 1, train for the 169-th batch, train loss: 0.6220358610153198:  70%|███████▋   | 169/241 [00:35<00:14,  4.92it/s]Epoch: 2, train for the 1-th batch, train loss: 0.9383929967880249:   0%|                       | 0/146 [00:00<?, ?it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5813487768173218:  58%|██████▍    | 138/237 [00:34<00:25,  3.88it/s]Epoch: 1, train for the 122-th batch, train loss: 0.4998129904270172:  32%|███▌       | 122/383 [00:32<01:19,  3.27it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5813487768173218:  59%|██████▍    | 139/237 [00:34<00:23,  4.13it/s]evaluate for the 8-th batch, evaluate loss: 0.5503429770469666:  13%|██▌                 | 6/46 [00:00<00:03, 11.47it/s]evaluate for the 8-th batch, evaluate loss: 0.5503429770469666:  17%|███▍                | 8/46 [00:00<00:03, 10.25it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5339435338973999:  46%|██████       | 55/119 [00:07<00:07,  8.94it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5339435338973999:  47%|██████       | 56/119 [00:07<00:07,  7.95it/s]Epoch: 1, train for the 170-th batch, train loss: 0.7401524186134338:  70%|███████▋   | 169/241 [00:35<00:14,  4.92it/s]Epoch: 2, train for the 2-th batch, train loss: 0.8632838726043701:   0%|                       | 0/146 [00:00<?, ?it/s]Epoch: 2, train for the 2-th batch, train loss: 0.8632838726043701:   1%|▏              | 2/146 [00:00<00:13, 10.34it/s]Epoch: 1, train for the 170-th batch, train loss: 0.7401524186134338:  71%|███████▊   | 170/241 [00:35<00:13,  5.25it/s]evaluate for the 9-th batch, evaluate loss: 0.5704501867294312:  17%|███▍                | 8/46 [00:00<00:03, 10.25it/s]evaluate for the 10-th batch, evaluate loss: 0.5143699049949646:  17%|███▎               | 8/46 [00:00<00:03, 10.25it/s]evaluate for the 10-th batch, evaluate loss: 0.5143699049949646:  22%|███▉              | 10/46 [00:00<00:03, 10.36it/s]Epoch: 1, train for the 140-th batch, train loss: 0.5538479685783386:  59%|██████▍    | 139/237 [00:34<00:23,  4.13it/s]Epoch: 2, train for the 3-th batch, train loss: 0.7930164337158203:   1%|▏              | 2/146 [00:00<00:13, 10.34it/s]Epoch: 1, train for the 123-th batch, train loss: 0.41002753376960754:  32%|███▏      | 122/383 [00:33<01:19,  3.27it/s]Epoch: 2, train for the 57-th batch, train loss: 0.47230789065361023:  47%|█████▋      | 56/119 [00:07<00:07,  7.95it/s]Epoch: 2, train for the 57-th batch, train loss: 0.47230789065361023:  48%|█████▋      | 57/119 [00:07<00:08,  6.98it/s]Epoch: 1, train for the 171-th batch, train loss: 0.607357382774353:  71%|████████▍   | 170/241 [00:35<00:13,  5.25it/s]Epoch: 1, train for the 140-th batch, train loss: 0.5538479685783386:  59%|██████▍    | 140/237 [00:34<00:23,  4.04it/s]Epoch: 1, train for the 123-th batch, train loss: 0.41002753376960754:  32%|███▏      | 123/383 [00:33<01:17,  3.35it/s]Epoch: 1, train for the 171-th batch, train loss: 0.607357382774353:  71%|████████▌   | 171/241 [00:35<00:13,  5.24it/s]evaluate for the 11-th batch, evaluate loss: 0.5638437271118164:  22%|███▉              | 10/46 [00:00<00:03, 10.36it/s]Epoch: 2, train for the 4-th batch, train loss: 0.7280970811843872:   1%|▏              | 2/146 [00:00<00:13, 10.34it/s]Epoch: 2, train for the 4-th batch, train loss: 0.7280970811843872:   3%|▍              | 4/146 [00:00<00:17,  8.11it/s]Epoch: 2, train for the 58-th batch, train loss: 0.5148397088050842:  48%|██████▏      | 57/119 [00:07<00:08,  6.98it/s]Epoch: 2, train for the 58-th batch, train loss: 0.5148397088050842:  49%|██████▎      | 58/119 [00:07<00:09,  6.75it/s]Epoch: 2, train for the 5-th batch, train loss: 0.6911277770996094:   3%|▍              | 4/146 [00:00<00:17,  8.11it/s]Epoch: 2, train for the 5-th batch, train loss: 0.6911277770996094:   3%|▌              | 5/146 [00:00<00:17,  8.13it/s]evaluate for the 12-th batch, evaluate loss: 0.5157150030136108:  22%|███▉              | 10/46 [00:01<00:03, 10.36it/s]evaluate for the 12-th batch, evaluate loss: 0.5157150030136108:  26%|████▋             | 12/46 [00:01<00:03,  8.70it/s]Epoch: 2, train for the 59-th batch, train loss: 0.5453755855560303:  49%|██████▎      | 58/119 [00:07<00:09,  6.75it/s]Epoch: 2, train for the 59-th batch, train loss: 0.5453755855560303:  50%|██████▍      | 59/119 [00:07<00:08,  7.00it/s]Epoch: 1, train for the 172-th batch, train loss: 0.5359270572662354:  71%|███████▊   | 171/241 [00:35<00:13,  5.24it/s]Epoch: 1, train for the 124-th batch, train loss: 0.46126577258110046:  32%|███▏      | 123/383 [00:33<01:17,  3.35it/s]Epoch: 1, train for the 141-th batch, train loss: 0.5946485996246338:  59%|██████▍    | 140/237 [00:35<00:23,  4.04it/s]Epoch: 1, train for the 172-th batch, train loss: 0.5359270572662354:  71%|███████▊   | 172/241 [00:35<00:15,  4.59it/s]Epoch: 1, train for the 124-th batch, train loss: 0.46126577258110046:  32%|███▏      | 124/383 [00:33<01:18,  3.28it/s]Epoch: 1, train for the 141-th batch, train loss: 0.5946485996246338:  59%|██████▌    | 141/237 [00:35<00:26,  3.65it/s]Epoch: 2, train for the 6-th batch, train loss: 0.673663318157196:   3%|▌               | 5/146 [00:00<00:17,  8.13it/s]evaluate for the 13-th batch, evaluate loss: 0.5263801217079163:  26%|████▋             | 12/46 [00:01<00:03,  8.70it/s]Epoch: 2, train for the 6-th batch, train loss: 0.673663318157196:   4%|▋               | 6/146 [00:00<00:16,  8.29it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5203247666358948:  50%|██████▍      | 59/119 [00:07<00:08,  7.00it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5203247666358948:  50%|██████▌      | 60/119 [00:07<00:08,  7.33it/s]evaluate for the 14-th batch, evaluate loss: 0.5680684447288513:  26%|████▋             | 12/46 [00:01<00:03,  8.70it/s]evaluate for the 14-th batch, evaluate loss: 0.5680684447288513:  30%|█████▍            | 14/46 [00:01<00:03,  9.64it/s]Epoch: 2, train for the 7-th batch, train loss: 0.6536770462989807:   4%|▌              | 6/146 [00:00<00:16,  8.29it/s]Epoch: 2, train for the 7-th batch, train loss: 0.6536770462989807:   5%|▋              | 7/146 [00:00<00:17,  8.09it/s]evaluate for the 15-th batch, evaluate loss: 0.5515518188476562:  30%|█████▍            | 14/46 [00:01<00:03,  9.64it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5247575044631958:  50%|██████▌      | 60/119 [00:07<00:08,  7.33it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5247575044631958:  51%|██████▋      | 61/119 [00:07<00:07,  7.36it/s]Epoch: 1, train for the 125-th batch, train loss: 0.4230802059173584:  32%|███▌       | 124/383 [00:33<01:18,  3.28it/s]Epoch: 1, train for the 142-th batch, train loss: 0.597907304763794:  59%|███████▏    | 141/237 [00:35<00:26,  3.65it/s]Epoch: 1, train for the 125-th batch, train loss: 0.4230802059173584:  33%|███▌       | 125/383 [00:33<01:13,  3.53it/s]Epoch: 2, train for the 8-th batch, train loss: 0.6671852469444275:   5%|▋              | 7/146 [00:00<00:17,  8.09it/s]Epoch: 2, train for the 8-th batch, train loss: 0.6671852469444275:   5%|▊              | 8/146 [00:00<00:16,  8.26it/s]Epoch: 1, train for the 142-th batch, train loss: 0.597907304763794:  60%|███████▏    | 142/237 [00:35<00:25,  3.74it/s]Epoch: 1, train for the 173-th batch, train loss: 0.33227404952049255:  71%|███████▏  | 172/241 [00:35<00:15,  4.59it/s]Epoch: 1, train for the 173-th batch, train loss: 0.33227404952049255:  72%|███████▏  | 173/241 [00:35<00:16,  4.07it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5735296010971069:  51%|██████▋      | 61/119 [00:07<00:07,  7.36it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5735296010971069:  52%|██████▊      | 62/119 [00:07<00:07,  7.42it/s]Epoch: 2, train for the 9-th batch, train loss: 0.685126006603241:   5%|▉               | 8/146 [00:01<00:16,  8.26it/s]Epoch: 2, train for the 9-th batch, train loss: 0.685126006603241:   6%|▉               | 9/146 [00:01<00:15,  8.71it/s]evaluate for the 16-th batch, evaluate loss: 0.5150614976882935:  30%|█████▍            | 14/46 [00:01<00:03,  9.64it/s]evaluate for the 16-th batch, evaluate loss: 0.5150614976882935:  35%|██████▎           | 16/46 [00:01<00:03,  8.64it/s]Epoch: 1, train for the 126-th batch, train loss: 0.3664429783821106:  33%|███▌       | 125/383 [00:33<01:13,  3.53it/s]Epoch: 1, train for the 126-th batch, train loss: 0.3664429783821106:  33%|███▌       | 126/383 [00:33<01:05,  3.93it/s]Epoch: 1, train for the 143-th batch, train loss: 0.5411774516105652:  60%|██████▌    | 142/237 [00:35<00:25,  3.74it/s]Epoch: 1, train for the 174-th batch, train loss: 0.42134714126586914:  72%|███████▏  | 173/241 [00:36<00:16,  4.07it/s]evaluate for the 17-th batch, evaluate loss: 0.4572535455226898:  35%|██████▎           | 16/46 [00:01<00:03,  8.64it/s]Epoch: 1, train for the 174-th batch, train loss: 0.42134714126586914:  72%|███████▏  | 174/241 [00:36<00:14,  4.50it/s]Epoch: 1, train for the 143-th batch, train loss: 0.5411774516105652:  60%|██████▋    | 143/237 [00:35<00:23,  3.97it/s]Epoch: 2, train for the 63-th batch, train loss: 0.5167733430862427:  52%|██████▊      | 62/119 [00:08<00:07,  7.42it/s]Epoch: 2, train for the 10-th batch, train loss: 0.6910932660102844:   6%|▊             | 9/146 [00:01<00:15,  8.71it/s]Epoch: 2, train for the 10-th batch, train loss: 0.6910932660102844:   7%|▉            | 10/146 [00:01<00:17,  7.81it/s]Epoch: 2, train for the 63-th batch, train loss: 0.5167733430862427:  53%|██████▉      | 63/119 [00:08<00:08,  6.74it/s]evaluate for the 18-th batch, evaluate loss: 0.5077501535415649:  35%|██████▎           | 16/46 [00:01<00:03,  8.64it/s]evaluate for the 18-th batch, evaluate loss: 0.5077501535415649:  39%|███████           | 18/46 [00:01<00:02,  9.68it/s]evaluate for the 19-th batch, evaluate loss: 0.5778334736824036:  39%|███████           | 18/46 [00:01<00:02,  9.68it/s]Epoch: 1, train for the 127-th batch, train loss: 0.3635709285736084:  33%|███▌       | 126/383 [00:34<01:05,  3.93it/s]Epoch: 2, train for the 11-th batch, train loss: 0.7068465352058411:   7%|▉            | 10/146 [00:01<00:17,  7.81it/s]Epoch: 2, train for the 11-th batch, train loss: 0.7068465352058411:   8%|▉            | 11/146 [00:01<00:16,  8.08it/s]Epoch: 1, train for the 127-th batch, train loss: 0.3635709285736084:  33%|███▋       | 127/383 [00:34<01:03,  4.06it/s]Epoch: 2, train for the 12-th batch, train loss: 0.6906097531318665:   8%|▉            | 11/146 [00:01<00:16,  8.08it/s]evaluate for the 20-th batch, evaluate loss: 0.5220330953598022:  39%|███████           | 18/46 [00:02<00:02,  9.68it/s]evaluate for the 20-th batch, evaluate loss: 0.5220330953598022:  43%|███████▊          | 20/46 [00:02<00:02,  9.27it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5192066431045532:  53%|██████▉      | 63/119 [00:08<00:08,  6.74it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5192066431045532:  54%|██████▉      | 64/119 [00:08<00:10,  5.33it/s]Epoch: 2, train for the 13-th batch, train loss: 0.6855831146240234:   8%|▉            | 11/146 [00:01<00:16,  8.08it/s]Epoch: 2, train for the 13-th batch, train loss: 0.6855831146240234:   9%|█▏           | 13/146 [00:01<00:14,  8.89it/s]Epoch: 1, train for the 144-th batch, train loss: 0.5597597360610962:  60%|██████▋    | 143/237 [00:36<00:23,  3.97it/s]Epoch: 1, train for the 175-th batch, train loss: 0.2736906409263611:  72%|███████▉   | 174/241 [00:36<00:14,  4.50it/s]evaluate for the 21-th batch, evaluate loss: 0.5569994449615479:  43%|███████▊          | 20/46 [00:02<00:02,  9.27it/s]evaluate for the 21-th batch, evaluate loss: 0.5569994449615479:  46%|████████▏         | 21/46 [00:02<00:02,  9.02it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5227372646331787:  33%|███▋       | 127/383 [00:34<01:03,  4.06it/s]Epoch: 1, train for the 175-th batch, train loss: 0.2736906409263611:  73%|███████▉   | 175/241 [00:36<00:19,  3.44it/s]Epoch: 1, train for the 144-th batch, train loss: 0.5597597360610962:  61%|██████▋    | 144/237 [00:36<00:28,  3.25it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5261978507041931:  54%|██████▉      | 64/119 [00:08<00:10,  5.33it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5261978507041931:  55%|███████      | 65/119 [00:08<00:09,  5.80it/s]Epoch: 1, train for the 128-th batch, train loss: 0.5227372646331787:  33%|███▋       | 128/383 [00:34<01:05,  3.88it/s]Epoch: 2, train for the 14-th batch, train loss: 0.6755132675170898:   9%|█▏           | 13/146 [00:01<00:14,  8.89it/s]Epoch: 2, train for the 14-th batch, train loss: 0.6755132675170898:  10%|█▏           | 14/146 [00:01<00:15,  8.47it/s]evaluate for the 22-th batch, evaluate loss: 0.5276358127593994:  46%|████████▏         | 21/46 [00:02<00:02,  9.02it/s]Epoch: 1, train for the 176-th batch, train loss: 0.3708098530769348:  73%|███████▉   | 175/241 [00:36<00:19,  3.44it/s]evaluate for the 23-th batch, evaluate loss: 0.4751131236553192:  46%|████████▏         | 21/46 [00:02<00:02,  9.02it/s]evaluate for the 23-th batch, evaluate loss: 0.4751131236553192:  50%|█████████         | 23/46 [00:02<00:02,  9.65it/s]Epoch: 1, train for the 176-th batch, train loss: 0.3708098530769348:  73%|████████   | 176/241 [00:36<00:16,  3.91it/s]Epoch: 2, train for the 66-th batch, train loss: 0.5492238402366638:  55%|███████      | 65/119 [00:08<00:09,  5.80it/s]Epoch: 1, train for the 145-th batch, train loss: 0.5678267478942871:  61%|██████▋    | 144/237 [00:36<00:28,  3.25it/s]Epoch: 2, train for the 66-th batch, train loss: 0.5492238402366638:  55%|███████▏     | 66/119 [00:08<00:09,  5.80it/s]Epoch: 2, train for the 15-th batch, train loss: 0.6568822860717773:  10%|█▏           | 14/146 [00:01<00:15,  8.47it/s]Epoch: 2, train for the 15-th batch, train loss: 0.6568822860717773:  10%|█▎           | 15/146 [00:01<00:16,  7.80it/s]Epoch: 1, train for the 145-th batch, train loss: 0.5678267478942871:  61%|██████▋    | 145/237 [00:36<00:25,  3.56it/s]Epoch: 1, train for the 129-th batch, train loss: 0.383878231048584:  33%|████        | 128/383 [00:34<01:05,  3.88it/s]Epoch: 1, train for the 129-th batch, train loss: 0.383878231048584:  34%|████        | 129/383 [00:34<01:04,  3.95it/s]evaluate for the 24-th batch, evaluate loss: 0.5040609836578369:  50%|█████████         | 23/46 [00:02<00:02,  9.65it/s]evaluate for the 24-th batch, evaluate loss: 0.5040609836578369:  52%|█████████▍        | 24/46 [00:02<00:02,  8.78it/s]Epoch: 2, train for the 67-th batch, train loss: 0.5768430233001709:  55%|███████▏     | 66/119 [00:08<00:09,  5.80it/s]Epoch: 2, train for the 67-th batch, train loss: 0.5768430233001709:  56%|███████▎     | 67/119 [00:08<00:08,  5.96it/s]Epoch: 2, train for the 16-th batch, train loss: 0.6276257634162903:  10%|█▎           | 15/146 [00:01<00:16,  7.80it/s]Epoch: 2, train for the 16-th batch, train loss: 0.6276257634162903:  11%|█▍           | 16/146 [00:01<00:17,  7.44it/s]evaluate for the 25-th batch, evaluate loss: 0.5053480267524719:  52%|█████████▍        | 24/46 [00:02<00:02,  8.78it/s]Epoch: 1, train for the 177-th batch, train loss: 0.28567054867744446:  73%|███████▎  | 176/241 [00:37<00:16,  3.91it/s]evaluate for the 26-th batch, evaluate loss: 0.5494024753570557:  52%|█████████▍        | 24/46 [00:02<00:02,  8.78it/s]evaluate for the 26-th batch, evaluate loss: 0.5494024753570557:  57%|██████████▏       | 26/46 [00:02<00:01, 10.61it/s]Epoch: 1, train for the 177-th batch, train loss: 0.28567054867744446:  73%|███████▎  | 177/241 [00:37<00:16,  3.86it/s]Epoch: 1, train for the 130-th batch, train loss: 0.3864068388938904:  34%|███▋       | 129/383 [00:34<01:04,  3.95it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5731651782989502:  61%|██████▋    | 145/237 [00:36<00:25,  3.56it/s]evaluate for the 27-th batch, evaluate loss: 0.522483229637146:  57%|██████████▋        | 26/46 [00:02<00:01, 10.61it/s]Epoch: 1, train for the 146-th batch, train loss: 0.5731651782989502:  62%|██████▊    | 146/237 [00:36<00:25,  3.55it/s]Epoch: 1, train for the 130-th batch, train loss: 0.3864068388938904:  34%|███▋       | 130/383 [00:34<01:03,  4.00it/s]Epoch: 2, train for the 68-th batch, train loss: 0.49620845913887024:  56%|██████▊     | 67/119 [00:09<00:08,  5.96it/s]Epoch: 2, train for the 17-th batch, train loss: 0.6244209408760071:  11%|█▍           | 16/146 [00:02<00:17,  7.44it/s]Epoch: 2, train for the 17-th batch, train loss: 0.6244209408760071:  12%|█▌           | 17/146 [00:02<00:18,  7.00it/s]Epoch: 2, train for the 68-th batch, train loss: 0.49620845913887024:  57%|██████▊     | 68/119 [00:09<00:08,  5.86it/s]Epoch: 1, train for the 178-th batch, train loss: 0.4558003842830658:  73%|████████   | 177/241 [00:37<00:16,  3.86it/s]evaluate for the 28-th batch, evaluate loss: 0.5335626006126404:  57%|██████████▏       | 26/46 [00:02<00:01, 10.61it/s]evaluate for the 28-th batch, evaluate loss: 0.5335626006126404:  61%|██████████▉       | 28/46 [00:02<00:01, 11.43it/s]Epoch: 1, train for the 178-th batch, train loss: 0.4558003842830658:  74%|████████   | 178/241 [00:37<00:14,  4.36it/s]Epoch: 2, train for the 18-th batch, train loss: 0.6057220101356506:  12%|█▌           | 17/146 [00:02<00:18,  7.00it/s]Epoch: 2, train for the 18-th batch, train loss: 0.6057220101356506:  12%|█▌           | 18/146 [00:02<00:17,  7.29it/s]evaluate for the 29-th batch, evaluate loss: 0.49129608273506165:  61%|██████████▎      | 28/46 [00:02<00:01, 11.43it/s]Epoch: 1, train for the 147-th batch, train loss: 0.5833940505981445:  62%|██████▊    | 146/237 [00:36<00:25,  3.55it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5822079181671143:  57%|███████▍     | 68/119 [00:09<00:08,  5.86it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5822079181671143:  58%|███████▌     | 69/119 [00:09<00:08,  5.81it/s]Epoch: 1, train for the 147-th batch, train loss: 0.5833940505981445:  62%|██████▊    | 147/237 [00:36<00:23,  3.85it/s]Epoch: 1, train for the 131-th batch, train loss: 0.4103919267654419:  34%|███▋       | 130/383 [00:35<01:03,  4.00it/s]Epoch: 1, train for the 131-th batch, train loss: 0.4103919267654419:  34%|███▊       | 131/383 [00:35<01:02,  4.04it/s]Epoch: 2, train for the 19-th batch, train loss: 0.6052159070968628:  12%|█▌           | 18/146 [00:02<00:17,  7.29it/s]Epoch: 2, train for the 19-th batch, train loss: 0.6052159070968628:  13%|█▋           | 19/146 [00:02<00:16,  7.64it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5114386081695557:  58%|███████▌     | 69/119 [00:09<00:08,  5.81it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5114386081695557:  59%|███████▋     | 70/119 [00:09<00:07,  6.35it/s]Epoch: 2, train for the 20-th batch, train loss: 0.5941686034202576:  13%|█▋           | 19/146 [00:02<00:16,  7.64it/s]Epoch: 2, train for the 20-th batch, train loss: 0.5941686034202576:  14%|█▊           | 20/146 [00:02<00:15,  7.89it/s]Epoch: 1, train for the 148-th batch, train loss: 0.5371211171150208:  62%|██████▊    | 147/237 [00:37<00:23,  3.85it/s]Epoch: 1, train for the 148-th batch, train loss: 0.5371211171150208:  62%|██████▊    | 148/237 [00:37<00:22,  4.04it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5172886848449707:  59%|███████▋     | 70/119 [00:09<00:07,  6.35it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5172886848449707:  60%|███████▊     | 71/119 [00:09<00:06,  6.90it/s]evaluate for the 30-th batch, evaluate loss: 0.5101996064186096:  61%|██████████▉       | 28/46 [00:03<00:01, 11.43it/s]evaluate for the 30-th batch, evaluate loss: 0.5101996064186096:  65%|███████████▋      | 30/46 [00:03<00:01,  8.45it/s]Epoch: 1, train for the 179-th batch, train loss: 0.4105081856250763:  74%|████████   | 178/241 [00:37<00:14,  4.36it/s]Epoch: 1, train for the 179-th batch, train loss: 0.4105081856250763:  74%|████████▏  | 179/241 [00:37<00:16,  3.68it/s]Epoch: 2, train for the 21-th batch, train loss: 0.59737628698349:  14%|██             | 20/146 [00:02<00:15,  7.89it/s]Epoch: 2, train for the 21-th batch, train loss: 0.59737628698349:  14%|██▏            | 21/146 [00:02<00:15,  7.85it/s]evaluate for the 31-th batch, evaluate loss: 0.4504021406173706:  65%|███████████▋      | 30/46 [00:03<00:01,  8.45it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5447099208831787:  60%|███████▊     | 71/119 [00:09<00:06,  6.90it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5447099208831787:  61%|███████▊     | 72/119 [00:09<00:06,  7.19it/s]Epoch: 1, train for the 132-th batch, train loss: 0.4523219168186188:  34%|███▊       | 131/383 [00:35<01:02,  4.04it/s]evaluate for the 32-th batch, evaluate loss: 0.48835447430610657:  65%|███████████      | 30/46 [00:03<00:01,  8.45it/s]evaluate for the 32-th batch, evaluate loss: 0.48835447430610657:  70%|███████████▊     | 32/46 [00:03<00:01,  9.52it/s]Epoch: 1, train for the 149-th batch, train loss: 0.5424304008483887:  62%|██████▊    | 148/237 [00:37<00:22,  4.04it/s]Epoch: 2, train for the 22-th batch, train loss: 0.6010497212409973:  14%|█▊           | 21/146 [00:02<00:15,  7.85it/s]Epoch: 1, train for the 132-th batch, train loss: 0.4523219168186188:  34%|███▊       | 132/383 [00:35<01:12,  3.45it/s]Epoch: 2, train for the 22-th batch, train loss: 0.6010497212409973:  15%|█▉           | 22/146 [00:02<00:16,  7.73it/s]Epoch: 1, train for the 149-th batch, train loss: 0.5424304008483887:  63%|██████▉    | 149/237 [00:37<00:21,  4.10it/s]evaluate for the 33-th batch, evaluate loss: 0.4948784410953522:  70%|████████████▌     | 32/46 [00:03<00:01,  9.52it/s]Epoch: 2, train for the 73-th batch, train loss: 0.4775363504886627:  61%|███████▊     | 72/119 [00:09<00:06,  7.19it/s]Epoch: 2, train for the 73-th batch, train loss: 0.4775363504886627:  61%|███████▉     | 73/119 [00:09<00:06,  7.29it/s]Epoch: 2, train for the 23-th batch, train loss: 0.6038644313812256:  15%|█▉           | 22/146 [00:02<00:16,  7.73it/s]Epoch: 2, train for the 23-th batch, train loss: 0.6038644313812256:  16%|██           | 23/146 [00:02<00:15,  7.80it/s]Epoch: 2, train for the 74-th batch, train loss: 0.5683024525642395:  61%|███████▉     | 73/119 [00:09<00:06,  7.29it/s]Epoch: 2, train for the 74-th batch, train loss: 0.5683024525642395:  62%|████████     | 74/119 [00:09<00:05,  7.58it/s]Epoch: 1, train for the 133-th batch, train loss: 0.48992034792900085:  34%|███▍      | 132/383 [00:35<01:12,  3.45it/s]Epoch: 1, train for the 180-th batch, train loss: 0.5552153587341309:  74%|████████▏  | 179/241 [00:37<00:16,  3.68it/s]Epoch: 1, train for the 133-th batch, train loss: 0.48992034792900085:  35%|███▍      | 133/383 [00:35<01:07,  3.68it/s]Epoch: 1, train for the 150-th batch, train loss: 0.5289872884750366:  63%|██████▉    | 149/237 [00:37<00:21,  4.10it/s]evaluate for the 34-th batch, evaluate loss: 0.4588474929332733:  70%|████████████▌     | 32/46 [00:03<00:01,  9.52it/s]evaluate for the 34-th batch, evaluate loss: 0.4588474929332733:  74%|█████████████▎    | 34/46 [00:03<00:01,  8.64it/s]Epoch: 2, train for the 24-th batch, train loss: 0.6006824970245361:  16%|██           | 23/146 [00:02<00:15,  7.80it/s]Epoch: 1, train for the 180-th batch, train loss: 0.5552153587341309:  75%|████████▏  | 180/241 [00:37<00:19,  3.17it/s]Epoch: 2, train for the 24-th batch, train loss: 0.6006824970245361:  16%|██▏          | 24/146 [00:02<00:15,  8.02it/s]Epoch: 1, train for the 150-th batch, train loss: 0.5289872884750366:  63%|██████▉    | 150/237 [00:37<00:21,  4.10it/s]Epoch: 2, train for the 75-th batch, train loss: 0.5221253037452698:  62%|████████     | 74/119 [00:10<00:05,  7.58it/s]Epoch: 2, train for the 75-th batch, train loss: 0.5221253037452698:  63%|████████▏    | 75/119 [00:10<00:05,  7.44it/s]evaluate for the 35-th batch, evaluate loss: 0.5130842328071594:  74%|█████████████▎    | 34/46 [00:03<00:01,  8.64it/s]evaluate for the 35-th batch, evaluate loss: 0.5130842328071594:  76%|█████████████▋    | 35/46 [00:03<00:01,  8.71it/s]Epoch: 2, train for the 25-th batch, train loss: 0.5853460431098938:  16%|██▏          | 24/146 [00:03<00:15,  8.02it/s]Epoch: 2, train for the 25-th batch, train loss: 0.5853460431098938:  17%|██▏          | 25/146 [00:03<00:15,  7.60it/s]Epoch: 1, train for the 181-th batch, train loss: 0.7217243909835815:  75%|████████▏  | 180/241 [00:38<00:19,  3.17it/s]evaluate for the 36-th batch, evaluate loss: 0.4651683568954468:  76%|█████████████▋    | 35/46 [00:03<00:01,  8.71it/s]Epoch: 1, train for the 181-th batch, train loss: 0.7217243909835815:  75%|████████▎  | 181/241 [00:38<00:16,  3.63it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5374413728713989:  63%|████████▏    | 75/119 [00:10<00:05,  7.44it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5489057302474976:  63%|██████▉    | 150/237 [00:37<00:21,  4.10it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5374413728713989:  64%|████████▎    | 76/119 [00:10<00:05,  7.32it/s]Epoch: 2, train for the 26-th batch, train loss: 0.5494393110275269:  17%|██▏          | 25/146 [00:03<00:15,  7.60it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5489057302474976:  64%|███████    | 151/237 [00:37<00:20,  4.20it/s]Epoch: 2, train for the 26-th batch, train loss: 0.5494393110275269:  18%|██▎          | 26/146 [00:03<00:14,  8.07it/s]evaluate for the 37-th batch, evaluate loss: 0.5177112221717834:  76%|█████████████▋    | 35/46 [00:03<00:01,  8.71it/s]evaluate for the 37-th batch, evaluate loss: 0.5177112221717834:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.69it/s]Epoch: 1, train for the 182-th batch, train loss: 0.6561182737350464:  75%|████████▎  | 181/241 [00:38<00:16,  3.63it/s]Epoch: 2, train for the 77-th batch, train loss: 0.4846796989440918:  64%|████████▎    | 76/119 [00:10<00:05,  7.32it/s]Epoch: 1, train for the 182-th batch, train loss: 0.6561182737350464:  76%|████████▎  | 182/241 [00:38<00:14,  4.14it/s]Epoch: 2, train for the 77-th batch, train loss: 0.4846796989440918:  65%|████████▍    | 77/119 [00:10<00:05,  7.46it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5213264226913452:  35%|███▊       | 133/383 [00:36<01:07,  3.68it/s]Epoch: 2, train for the 27-th batch, train loss: 0.5791457891464233:  18%|██▎          | 26/146 [00:03<00:14,  8.07it/s]Epoch: 2, train for the 27-th batch, train loss: 0.5791457891464233:  18%|██▍          | 27/146 [00:03<00:14,  7.96it/s]Epoch: 1, train for the 134-th batch, train loss: 0.5213264226913452:  35%|███▊       | 134/383 [00:36<01:18,  3.16it/s]Epoch: 1, train for the 183-th batch, train loss: 0.5074894428253174:  76%|████████▎  | 182/241 [00:38<00:14,  4.14it/s]Epoch: 1, train for the 152-th batch, train loss: 0.521547257900238:  64%|███████▋    | 151/237 [00:37<00:20,  4.20it/s]evaluate for the 38-th batch, evaluate loss: 0.48637330532073975:  80%|█████████████▋   | 37/46 [00:04<00:00,  9.69it/s]Epoch: 1, train for the 183-th batch, train loss: 0.5074894428253174:  76%|████████▎  | 183/241 [00:38<00:12,  4.73it/s]Epoch: 1, train for the 152-th batch, train loss: 0.521547257900238:  64%|███████▋    | 152/237 [00:38<00:20,  4.15it/s]Epoch: 2, train for the 78-th batch, train loss: 0.4980475902557373:  65%|████████▍    | 77/119 [00:10<00:05,  7.46it/s]Epoch: 2, train for the 78-th batch, train loss: 0.4980475902557373:  66%|████████▌    | 78/119 [00:10<00:05,  6.92it/s]Epoch: 2, train for the 28-th batch, train loss: 0.5614284873008728:  18%|██▍          | 27/146 [00:03<00:14,  7.96it/s]Epoch: 2, train for the 28-th batch, train loss: 0.5614284873008728:  19%|██▍          | 28/146 [00:03<00:16,  7.37it/s]evaluate for the 39-th batch, evaluate loss: 0.4836418330669403:  80%|██████████████▍   | 37/46 [00:04<00:00,  9.69it/s]evaluate for the 39-th batch, evaluate loss: 0.4836418330669403:  85%|███████████████▎  | 39/46 [00:04<00:00,  8.49it/s]Epoch: 1, train for the 135-th batch, train loss: 0.4611488878726959:  35%|███▊       | 134/383 [00:36<01:18,  3.16it/s]Epoch: 1, train for the 184-th batch, train loss: 0.5852745175361633:  76%|████████▎  | 183/241 [00:38<00:12,  4.73it/s]Epoch: 1, train for the 135-th batch, train loss: 0.4611488878726959:  35%|███▉       | 135/383 [00:36<01:10,  3.51it/s]Epoch: 1, train for the 184-th batch, train loss: 0.5852745175361633:  76%|████████▍  | 184/241 [00:38<00:10,  5.20it/s]evaluate for the 40-th batch, evaluate loss: 0.47941192984580994:  85%|██████████████▍  | 39/46 [00:04<00:00,  8.49it/s]Epoch: 2, train for the 79-th batch, train loss: 0.4887402355670929:  66%|████████▌    | 78/119 [00:10<00:05,  6.92it/s]Epoch: 1, train for the 153-th batch, train loss: 0.5341013073921204:  64%|███████    | 152/237 [00:38<00:20,  4.15it/s]Epoch: 2, train for the 79-th batch, train loss: 0.4887402355670929:  66%|████████▋    | 79/119 [00:10<00:06,  6.54it/s]Epoch: 2, train for the 29-th batch, train loss: 0.5326244831085205:  19%|██▍          | 28/146 [00:03<00:16,  7.37it/s]Epoch: 2, train for the 29-th batch, train loss: 0.5326244831085205:  20%|██▌          | 29/146 [00:03<00:17,  6.84it/s]evaluate for the 41-th batch, evaluate loss: 0.4885358214378357:  85%|███████████████▎  | 39/46 [00:04<00:00,  8.49it/s]evaluate for the 41-th batch, evaluate loss: 0.4885358214378357:  89%|████████████████  | 41/46 [00:04<00:00,  9.33it/s]Epoch: 1, train for the 153-th batch, train loss: 0.5341013073921204:  65%|███████    | 153/237 [00:38<00:20,  4.09it/s]Epoch: 1, train for the 185-th batch, train loss: 0.6705052852630615:  76%|████████▍  | 184/241 [00:38<00:10,  5.20it/s]Epoch: 1, train for the 185-th batch, train loss: 0.6705052852630615:  77%|████████▍  | 185/241 [00:38<00:10,  5.35it/s]Epoch: 2, train for the 80-th batch, train loss: 0.522345781326294:  66%|█████████▎    | 79/119 [00:10<00:06,  6.54it/s]Epoch: 2, train for the 80-th batch, train loss: 0.522345781326294:  67%|█████████▍    | 80/119 [00:10<00:06,  6.32it/s]Epoch: 2, train for the 30-th batch, train loss: 0.5390334725379944:  20%|██▌          | 29/146 [00:03<00:17,  6.84it/s]Epoch: 2, train for the 30-th batch, train loss: 0.5390334725379944:  21%|██▋          | 30/146 [00:03<00:17,  6.72it/s]Epoch: 1, train for the 186-th batch, train loss: 0.6141391396522522:  77%|████████▍  | 185/241 [00:38<00:10,  5.35it/s]evaluate for the 42-th batch, evaluate loss: 0.46123817563056946:  89%|███████████████▏ | 41/46 [00:04<00:00,  9.33it/s]Epoch: 1, train for the 186-th batch, train loss: 0.6141391396522522:  77%|████████▍  | 186/241 [00:38<00:09,  5.70it/s]Epoch: 2, train for the 81-th batch, train loss: 0.523911714553833:  67%|█████████▍    | 80/119 [00:10<00:06,  6.32it/s]Epoch: 2, train for the 81-th batch, train loss: 0.523911714553833:  68%|█████████▌    | 81/119 [00:10<00:05,  6.98it/s]Epoch: 1, train for the 154-th batch, train loss: 0.5561736822128296:  65%|███████    | 153/237 [00:38<00:20,  4.09it/s]Epoch: 1, train for the 136-th batch, train loss: 0.39225876331329346:  35%|███▌      | 135/383 [00:36<01:10,  3.51it/s]Epoch: 1, train for the 154-th batch, train loss: 0.5561736822128296:  65%|███████▏   | 154/237 [00:38<00:20,  4.05it/s]Epoch: 1, train for the 136-th batch, train loss: 0.39225876331329346:  36%|███▌      | 136/383 [00:36<01:18,  3.14it/s]evaluate for the 43-th batch, evaluate loss: 0.529386043548584:  89%|████████████████▉  | 41/46 [00:04<00:00,  9.33it/s]evaluate for the 43-th batch, evaluate loss: 0.529386043548584:  93%|█████████████████▊ | 43/46 [00:04<00:00,  8.22it/s]Epoch: 1, train for the 187-th batch, train loss: 0.659774899482727:  77%|█████████▎  | 186/241 [00:39<00:09,  5.70it/s]Epoch: 2, train for the 31-th batch, train loss: 0.5528125762939453:  21%|██▋          | 30/146 [00:04<00:17,  6.72it/s]Epoch: 2, train for the 31-th batch, train loss: 0.5528125762939453:  21%|██▊          | 31/146 [00:04<00:19,  5.79it/s]Epoch: 1, train for the 187-th batch, train loss: 0.659774899482727:  78%|█████████▎  | 187/241 [00:39<00:09,  5.99it/s]Epoch: 2, train for the 82-th batch, train loss: 0.5312684774398804:  68%|████████▊    | 81/119 [00:11<00:05,  6.98it/s]Epoch: 2, train for the 82-th batch, train loss: 0.5312684774398804:  69%|████████▉    | 82/119 [00:11<00:05,  6.53it/s]Epoch: 1, train for the 137-th batch, train loss: 0.47383788228034973:  36%|███▌      | 136/383 [00:36<01:18,  3.14it/s]Epoch: 2, train for the 32-th batch, train loss: 0.5468427538871765:  21%|██▊          | 31/146 [00:04<00:19,  5.79it/s]Epoch: 2, train for the 32-th batch, train loss: 0.5468427538871765:  22%|██▊          | 32/146 [00:04<00:17,  6.41it/s]Epoch: 1, train for the 137-th batch, train loss: 0.47383788228034973:  36%|███▌      | 137/383 [00:36<01:10,  3.50it/s]Epoch: 1, train for the 188-th batch, train loss: 0.4906390905380249:  78%|████████▌  | 187/241 [00:39<00:09,  5.99it/s]evaluate for the 44-th batch, evaluate loss: 0.47845718264579773:  93%|███████████████▉ | 43/46 [00:04<00:00,  8.22it/s]evaluate for the 44-th batch, evaluate loss: 0.47845718264579773:  96%|████████████████▎| 44/46 [00:04<00:00,  7.20it/s]Epoch: 1, train for the 188-th batch, train loss: 0.4906390905380249:  78%|████████▌  | 188/241 [00:39<00:08,  5.99it/s]Epoch: 1, train for the 155-th batch, train loss: 0.5671120882034302:  65%|███████▏   | 154/237 [00:38<00:20,  4.05it/s]Epoch: 1, train for the 155-th batch, train loss: 0.5671120882034302:  65%|███████▏   | 155/237 [00:38<00:21,  3.78it/s]Epoch: 2, train for the 33-th batch, train loss: 0.5476083159446716:  22%|██▊          | 32/146 [00:04<00:17,  6.41it/s]Epoch: 2, train for the 33-th batch, train loss: 0.5476083159446716:  23%|██▉          | 33/146 [00:04<00:16,  7.03it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5230951309204102:  69%|████████▉    | 82/119 [00:11<00:05,  6.53it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5230951309204102:  70%|█████████    | 83/119 [00:11<00:06,  5.89it/s]Epoch: 1, train for the 138-th batch, train loss: 0.2824225127696991:  36%|███▉       | 137/383 [00:37<01:10,  3.50it/s]Epoch: 1, train for the 138-th batch, train loss: 0.2824225127696991:  36%|███▉       | 138/383 [00:37<01:04,  3.78it/s]Epoch: 1, train for the 189-th batch, train loss: 0.41970932483673096:  78%|███████▊  | 188/241 [00:39<00:08,  5.99it/s]evaluate for the 45-th batch, evaluate loss: 0.48292258381843567:  96%|████████████████▎| 44/46 [00:05<00:00,  7.20it/s]evaluate for the 45-th batch, evaluate loss: 0.48292258381843567:  98%|████████████████▋| 45/46 [00:05<00:00,  6.66it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5572420954704285:  23%|██▉          | 33/146 [00:04<00:16,  7.03it/s]Epoch: 1, train for the 189-th batch, train loss: 0.41970932483673096:  78%|███████▊  | 189/241 [00:39<00:08,  5.79it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5572420954704285:  23%|███          | 34/146 [00:04<00:15,  7.10it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5036148428916931:  70%|█████████    | 83/119 [00:11<00:06,  5.89it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5036148428916931:  71%|█████████▏   | 84/119 [00:11<00:05,  6.11it/s]Epoch: 1, train for the 156-th batch, train loss: 0.5536715984344482:  65%|███████▏   | 155/237 [00:39<00:21,  3.78it/s]evaluate for the 46-th batch, evaluate loss: 0.4751458168029785:  98%|█████████████████▌| 45/46 [00:05<00:00,  6.66it/s]evaluate for the 46-th batch, evaluate loss: 0.4751458168029785: 100%|██████████████████| 46/46 [00:05<00:00,  7.04it/s]evaluate for the 46-th batch, evaluate loss: 0.4751458168029785: 100%|██████████████████| 46/46 [00:05<00:00,  8.93it/s]
Epoch: 2, train for the 35-th batch, train loss: 0.506269633769989:  23%|███▎          | 34/146 [00:04<00:15,  7.10it/s]Epoch: 2, train for the 35-th batch, train loss: 0.506269633769989:  24%|███▎          | 35/146 [00:04<00:15,  7.40it/s]Epoch: 1, train for the 156-th batch, train loss: 0.5536715984344482:  66%|███████▏   | 156/237 [00:39<00:21,  3.70it/s]Epoch: 1, train for the 190-th batch, train loss: 0.4108918011188507:  78%|████████▋  | 189/241 [00:39<00:08,  5.79it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5215911269187927:  36%|███▉       | 138/383 [00:37<01:04,  3.78it/s]Epoch: 1, train for the 190-th batch, train loss: 0.4108918011188507:  79%|████████▋  | 190/241 [00:39<00:09,  5.57it/s]Epoch: 1, train for the 139-th batch, train loss: 0.5215911269187927:  36%|███▉       | 139/383 [00:37<01:03,  3.86it/s]Epoch: 2, train for the 36-th batch, train loss: 0.5118353366851807:  24%|███          | 35/146 [00:04<00:15,  7.40it/s]Epoch: 2, train for the 36-th batch, train loss: 0.5118353366851807:  25%|███▏         | 36/146 [00:04<00:14,  7.70it/s]Epoch: 2, train for the 85-th batch, train loss: 0.544805109500885:  71%|█████████▉    | 84/119 [00:11<00:05,  6.11it/s]Epoch: 2, train for the 85-th batch, train loss: 0.544805109500885:  71%|██████████    | 85/119 [00:11<00:06,  5.42it/s]Epoch: 2, train for the 37-th batch, train loss: 0.5483188629150391:  25%|███▏         | 36/146 [00:04<00:14,  7.70it/s]Epoch: 2, train for the 37-th batch, train loss: 0.5483188629150391:  25%|███▎         | 37/146 [00:04<00:13,  7.88it/s]Epoch: 1, train for the 191-th batch, train loss: 0.3736124336719513:  79%|████████▋  | 190/241 [00:39<00:09,  5.57it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5542107820510864:  71%|█████████▎   | 85/119 [00:11<00:06,  5.42it/s]Epoch: 1, train for the 157-th batch, train loss: 0.5797247290611267:  66%|███████▏   | 156/237 [00:39<00:21,  3.70it/s]Epoch: 1, train for the 191-th batch, train loss: 0.3736124336719513:  79%|████████▋  | 191/241 [00:39<00:09,  5.20it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5542107820510864:  72%|█████████▍   | 86/119 [00:11<00:05,  5.78it/s]Epoch: 1, train for the 140-th batch, train loss: 0.43425196409225464:  36%|███▋      | 139/383 [00:37<01:03,  3.86it/s]Epoch: 1, train for the 157-th batch, train loss: 0.5797247290611267:  66%|███████▎   | 157/237 [00:39<00:22,  3.50it/s]Epoch: 2, train for the 38-th batch, train loss: 0.5106731653213501:  25%|███▎         | 37/146 [00:04<00:13,  7.88it/s]Epoch: 1, train for the 140-th batch, train loss: 0.43425196409225464:  37%|███▋      | 140/383 [00:37<01:01,  3.95it/s]Epoch: 2, train for the 38-th batch, train loss: 0.5106731653213501:  26%|███▍         | 38/146 [00:04<00:12,  8.35it/s]Epoch: 1, train for the 192-th batch, train loss: 0.335750013589859:  79%|█████████▌  | 191/241 [00:39<00:09,  5.20it/s]Epoch: 1, train for the 192-th batch, train loss: 0.335750013589859:  80%|█████████▌  | 192/241 [00:40<00:08,  5.58it/s]Epoch: 2, train for the 87-th batch, train loss: 0.5232794880867004:  72%|█████████▍   | 86/119 [00:11<00:05,  5.78it/s]Epoch: 2, train for the 87-th batch, train loss: 0.5232794880867004:  73%|█████████▌   | 87/119 [00:11<00:05,  5.88it/s]Epoch: 2, train for the 39-th batch, train loss: 0.5533263087272644:  26%|███▍         | 38/146 [00:05<00:12,  8.35it/s]Epoch: 2, train for the 39-th batch, train loss: 0.5533263087272644:  27%|███▍         | 39/146 [00:05<00:13,  7.71it/s]Epoch: 1, train for the 158-th batch, train loss: 0.5180110931396484:  66%|███████▎   | 157/237 [00:39<00:22,  3.50it/s]Epoch: 1, train for the 158-th batch, train loss: 0.5180110931396484:  67%|███████▎   | 158/237 [00:39<00:20,  3.84it/s]Epoch: 1, train for the 141-th batch, train loss: 0.43568092584609985:  37%|███▋      | 140/383 [00:37<01:01,  3.95it/s]Epoch: 1, train for the 141-th batch, train loss: 0.43568092584609985:  37%|███▋      | 141/383 [00:37<01:00,  4.02it/s]Epoch: 2, train for the 88-th batch, train loss: 0.5560383200645447:  73%|█████████▌   | 87/119 [00:12<00:05,  5.88it/s]Epoch: 2, train for the 88-th batch, train loss: 0.5560383200645447:  74%|█████████▌   | 88/119 [00:12<00:05,  6.16it/s]Epoch: 2, train for the 40-th batch, train loss: 0.517948567867279:  27%|███▋          | 39/146 [00:05<00:13,  7.71it/s]Epoch: 1, train for the 193-th batch, train loss: 0.3388043940067291:  80%|████████▊  | 192/241 [00:40<00:08,  5.58it/s]Epoch: 2, train for the 40-th batch, train loss: 0.517948567867279:  27%|███▊          | 40/146 [00:05<00:14,  7.40it/s]Epoch: 1, train for the 193-th batch, train loss: 0.3388043940067291:  80%|████████▊  | 193/241 [00:40<00:09,  5.18it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5017502307891846:  74%|█████████▌   | 88/119 [00:12<00:05,  6.16it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5017502307891846:  75%|█████████▋   | 89/119 [00:12<00:04,  6.36it/s]Epoch: 2, train for the 41-th batch, train loss: 0.4998771548271179:  27%|███▌         | 40/146 [00:05<00:14,  7.40it/s]Epoch: 2, train for the 41-th batch, train loss: 0.4998771548271179:  28%|███▋         | 41/146 [00:05<00:14,  7.37it/s]Epoch: 1, train for the 142-th batch, train loss: 0.46589764952659607:  37%|███▋      | 141/383 [00:38<01:00,  4.02it/s]Epoch: 1, train for the 159-th batch, train loss: 0.574356198310852:  67%|████████    | 158/237 [00:39<00:20,  3.84it/s]Epoch: 1, train for the 194-th batch, train loss: 0.3430800437927246:  80%|████████▊  | 193/241 [00:40<00:09,  5.18it/s]Epoch: 1, train for the 194-th batch, train loss: 0.3430800437927246:  80%|████████▊  | 194/241 [00:40<00:08,  5.41it/s]Epoch: 1, train for the 159-th batch, train loss: 0.574356198310852:  67%|████████    | 159/237 [00:39<00:21,  3.68it/s]Epoch: 1, train for the 142-th batch, train loss: 0.46589764952659607:  37%|███▋      | 142/383 [00:38<01:00,  3.97it/s]Epoch: 2, train for the 90-th batch, train loss: 0.5249991416931152:  75%|█████████▋   | 89/119 [00:12<00:04,  6.36it/s]Epoch: 2, train for the 90-th batch, train loss: 0.5249991416931152:  76%|█████████▊   | 90/119 [00:12<00:04,  6.55it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5054788589477539:  28%|███▋         | 41/146 [00:05<00:14,  7.37it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5054788589477539:  29%|███▋         | 42/146 [00:05<00:14,  7.35it/s]  0%|                                                                                            | 0/25 [00:00<?, ?it/s]Epoch: 1, train for the 195-th batch, train loss: 0.595585823059082:  80%|█████████▋  | 194/241 [00:40<00:08,  5.41it/s]Epoch: 1, train for the 195-th batch, train loss: 0.595585823059082:  81%|█████████▋  | 195/241 [00:40<00:08,  5.57it/s]evaluate for the 1-th batch, evaluate loss: 0.7607200741767883:   0%|                            | 0/25 [00:00<?, ?it/s]Epoch: 1, train for the 143-th batch, train loss: 0.3802817463874817:  37%|████       | 142/383 [00:38<01:00,  3.97it/s]Epoch: 2, train for the 91-th batch, train loss: 0.5007261037826538:  76%|█████████▊   | 90/119 [00:12<00:04,  6.55it/s]Epoch: 2, train for the 91-th batch, train loss: 0.5007261037826538:  76%|█████████▉   | 91/119 [00:12<00:04,  6.37it/s]Epoch: 1, train for the 143-th batch, train loss: 0.3802817463874817:  37%|████       | 143/383 [00:38<00:58,  4.07it/s]evaluate for the 2-th batch, evaluate loss: 0.752081573009491:   0%|                             | 0/25 [00:00<?, ?it/s]Epoch: 2, train for the 43-th batch, train loss: 0.5126060247421265:  29%|███▋         | 42/146 [00:05<00:14,  7.35it/s]evaluate for the 2-th batch, evaluate loss: 0.752081573009491:   8%|█▋                   | 2/25 [00:00<00:01, 13.85it/s]Epoch: 2, train for the 43-th batch, train loss: 0.5126060247421265:  29%|███▊         | 43/146 [00:05<00:14,  6.89it/s]Epoch: 1, train for the 196-th batch, train loss: 0.5231695175170898:  81%|████████▉  | 195/241 [00:40<00:08,  5.57it/s]Epoch: 1, train for the 196-th batch, train loss: 0.5231695175170898:  81%|████████▉  | 196/241 [00:40<00:07,  5.86it/s]evaluate for the 3-th batch, evaluate loss: 0.7955380082130432:   8%|█▌                  | 2/25 [00:00<00:01, 13.85it/s]Epoch: 1, train for the 160-th batch, train loss: 0.5735011696815491:  67%|███████▍   | 159/237 [00:40<00:21,  3.68it/s]Epoch: 1, train for the 160-th batch, train loss: 0.5735011696815491:  68%|███████▍   | 160/237 [00:40<00:23,  3.23it/s]Epoch: 1, train for the 144-th batch, train loss: 0.33795738220214844:  37%|███▋      | 143/383 [00:38<00:58,  4.07it/s]evaluate for the 4-th batch, evaluate loss: 0.7239419221878052:   8%|█▌                  | 2/25 [00:00<00:01, 13.85it/s]evaluate for the 4-th batch, evaluate loss: 0.7239419221878052:  16%|███▏                | 4/25 [00:00<00:01, 12.61it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5117054581642151:  76%|█████████▉   | 91/119 [00:12<00:04,  6.37it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5091040134429932:  29%|███▊         | 43/146 [00:05<00:14,  6.89it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5091040134429932:  30%|███▉         | 44/146 [00:05<00:15,  6.50it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5117054581642151:  77%|██████████   | 92/119 [00:12<00:04,  5.87it/s]Epoch: 1, train for the 144-th batch, train loss: 0.33795738220214844:  38%|███▊      | 144/383 [00:38<00:55,  4.29it/s]Epoch: 1, train for the 197-th batch, train loss: 0.5607028603553772:  81%|████████▉  | 196/241 [00:40<00:07,  5.86it/s]evaluate for the 5-th batch, evaluate loss: 0.746324896812439:  16%|███▎                 | 4/25 [00:00<00:01, 12.61it/s]Epoch: 1, train for the 197-th batch, train loss: 0.5607028603553772:  82%|████████▉  | 197/241 [00:40<00:07,  5.61it/s]Epoch: 2, train for the 45-th batch, train loss: 0.512839674949646:  30%|████▏         | 44/146 [00:05<00:15,  6.50it/s]Epoch: 2, train for the 45-th batch, train loss: 0.512839674949646:  31%|████▎         | 45/146 [00:05<00:15,  6.72it/s]evaluate for the 6-th batch, evaluate loss: 0.745023787021637:  16%|███▎                 | 4/25 [00:00<00:01, 12.61it/s]evaluate for the 6-th batch, evaluate loss: 0.745023787021637:  24%|█████                | 6/25 [00:00<00:01, 12.69it/s]Epoch: 1, train for the 161-th batch, train loss: 0.5893889665603638:  68%|███████▍   | 160/237 [00:40<00:23,  3.23it/s]Epoch: 2, train for the 93-th batch, train loss: 0.4753125309944153:  77%|██████████   | 92/119 [00:12<00:04,  5.87it/s]Epoch: 2, train for the 93-th batch, train loss: 0.4753125309944153:  78%|██████████▏  | 93/119 [00:12<00:04,  5.82it/s]Epoch: 1, train for the 161-th batch, train loss: 0.5893889665603638:  68%|███████▍   | 161/237 [00:40<00:21,  3.57it/s]Epoch: 1, train for the 145-th batch, train loss: 0.4610745906829834:  38%|████▏      | 144/383 [00:38<00:55,  4.29it/s]Epoch: 1, train for the 145-th batch, train loss: 0.4610745906829834:  38%|████▏      | 145/383 [00:38<00:54,  4.37it/s]Epoch: 1, train for the 198-th batch, train loss: 0.5251973867416382:  82%|████████▉  | 197/241 [00:41<00:07,  5.61it/s]Epoch: 2, train for the 46-th batch, train loss: 0.5144016742706299:  31%|████         | 45/146 [00:06<00:15,  6.72it/s]Epoch: 2, train for the 46-th batch, train loss: 0.5144016742706299:  32%|████         | 46/146 [00:06<00:13,  7.19it/s]evaluate for the 7-th batch, evaluate loss: 0.7708937525749207:  24%|████▊               | 6/25 [00:00<00:01, 12.69it/s]Epoch: 1, train for the 198-th batch, train loss: 0.5251973867416382:  82%|█████████  | 198/241 [00:41<00:07,  5.56it/s]Epoch: 2, train for the 94-th batch, train loss: 0.495636522769928:  78%|██████████▉   | 93/119 [00:13<00:04,  5.82it/s]Epoch: 2, train for the 94-th batch, train loss: 0.495636522769928:  79%|███████████   | 94/119 [00:13<00:04,  6.04it/s]evaluate for the 8-th batch, evaluate loss: 0.747109591960907:  24%|█████                | 6/25 [00:00<00:01, 12.69it/s]evaluate for the 8-th batch, evaluate loss: 0.747109591960907:  32%|██████▋              | 8/25 [00:00<00:01, 11.31it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5716802477836609:  32%|████         | 46/146 [00:06<00:13,  7.19it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5716802477836609:  32%|████▏        | 47/146 [00:06<00:14,  6.96it/s]Epoch: 1, train for the 199-th batch, train loss: 0.35681700706481934:  82%|████████▏ | 198/241 [00:41<00:07,  5.56it/s]evaluate for the 9-th batch, evaluate loss: 0.7171643376350403:  32%|██████▍             | 8/25 [00:00<00:01, 11.31it/s]Epoch: 1, train for the 199-th batch, train loss: 0.35681700706481934:  83%|████████▎ | 199/241 [00:41<00:07,  5.53it/s]Epoch: 2, train for the 95-th batch, train loss: 0.46162134408950806:  79%|█████████▍  | 94/119 [00:13<00:04,  6.04it/s]Epoch: 2, train for the 95-th batch, train loss: 0.46162134408950806:  80%|█████████▌  | 95/119 [00:13<00:03,  6.40it/s]Epoch: 1, train for the 162-th batch, train loss: 0.5909925699234009:  68%|███████▍   | 161/237 [00:40<00:21,  3.57it/s]Epoch: 1, train for the 146-th batch, train loss: 0.3628544807434082:  38%|████▏      | 145/383 [00:39<00:54,  4.37it/s]evaluate for the 10-th batch, evaluate loss: 0.769996166229248:  32%|██████▍             | 8/25 [00:00<00:01, 11.31it/s]evaluate for the 10-th batch, evaluate loss: 0.769996166229248:  40%|███████▌           | 10/25 [00:00<00:01, 11.45it/s]Epoch: 1, train for the 162-th batch, train loss: 0.5909925699234009:  68%|███████▌   | 162/237 [00:40<00:22,  3.34it/s]Epoch: 2, train for the 48-th batch, train loss: 0.5454549193382263:  32%|████▏        | 47/146 [00:06<00:14,  6.96it/s]Epoch: 1, train for the 146-th batch, train loss: 0.3628544807434082:  38%|████▏      | 146/383 [00:39<00:59,  4.02it/s]Epoch: 2, train for the 48-th batch, train loss: 0.5454549193382263:  33%|████▎        | 48/146 [00:06<00:13,  7.23it/s]Epoch: 2, train for the 96-th batch, train loss: 0.4408777058124542:  80%|██████████▍  | 95/119 [00:13<00:03,  6.40it/s]evaluate for the 11-th batch, evaluate loss: 0.7546212673187256:  40%|███████▏          | 10/25 [00:00<00:01, 11.45it/s]Epoch: 2, train for the 96-th batch, train loss: 0.4408777058124542:  81%|██████████▍  | 96/119 [00:13<00:03,  6.66it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5124608278274536:  33%|████▎        | 48/146 [00:06<00:13,  7.23it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5124608278274536:  34%|████▎        | 49/146 [00:06<00:12,  7.50it/s]evaluate for the 12-th batch, evaluate loss: 0.6704286336898804:  40%|███████▏          | 10/25 [00:00<00:01, 11.45it/s]evaluate for the 12-th batch, evaluate loss: 0.6704286336898804:  48%|████████▋         | 12/25 [00:00<00:01, 11.99it/s]evaluate for the 13-th batch, evaluate loss: 0.6797254681587219:  48%|████████▋         | 12/25 [00:01<00:01, 11.99it/s]Epoch: 1, train for the 147-th batch, train loss: 0.3207530379295349:  38%|████▏      | 146/383 [00:39<00:59,  4.02it/s]Epoch: 2, train for the 97-th batch, train loss: 0.5054072141647339:  81%|██████████▍  | 96/119 [00:13<00:03,  6.66it/s]Epoch: 2, train for the 97-th batch, train loss: 0.5054072141647339:  82%|██████████▌  | 97/119 [00:13<00:03,  6.75it/s]Epoch: 1, train for the 147-th batch, train loss: 0.3207530379295349:  38%|████▏      | 147/383 [00:39<00:58,  4.06it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5701977610588074:  34%|████▎        | 49/146 [00:06<00:12,  7.50it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5701977610588074:  34%|████▍        | 50/146 [00:06<00:12,  7.63it/s]evaluate for the 14-th batch, evaluate loss: 0.7292921543121338:  48%|████████▋         | 12/25 [00:01<00:01, 11.99it/s]evaluate for the 14-th batch, evaluate loss: 0.7292921543121338:  56%|██████████        | 14/25 [00:01<00:00, 13.34it/s]Epoch: 2, train for the 98-th batch, train loss: 0.46071067452430725:  82%|█████████▊  | 97/119 [00:13<00:03,  6.75it/s]Epoch: 1, train for the 200-th batch, train loss: 0.44890546798706055:  83%|████████▎ | 199/241 [00:41<00:07,  5.53it/s]Epoch: 2, train for the 98-th batch, train loss: 0.46071067452430725:  82%|█████████▉  | 98/119 [00:13<00:03,  6.95it/s]evaluate for the 15-th batch, evaluate loss: 0.7698735594749451:  56%|██████████        | 14/25 [00:01<00:00, 13.34it/s]Epoch: 1, train for the 200-th batch, train loss: 0.44890546798706055:  83%|████████▎ | 200/241 [00:41<00:10,  3.85it/s]Epoch: 1, train for the 163-th batch, train loss: 0.5894217491149902:  68%|███████▌   | 162/237 [00:41<00:22,  3.34it/s]Epoch: 2, train for the 51-th batch, train loss: 0.5307953357696533:  34%|████▍        | 50/146 [00:06<00:12,  7.63it/s]Epoch: 2, train for the 51-th batch, train loss: 0.5307953357696533:  35%|████▌        | 51/146 [00:06<00:12,  7.45it/s]Epoch: 1, train for the 163-th batch, train loss: 0.5894217491149902:  69%|███████▌   | 163/237 [00:41<00:24,  3.01it/s]Epoch: 1, train for the 148-th batch, train loss: 0.3172281086444855:  38%|████▏      | 147/383 [00:39<00:58,  4.06it/s]evaluate for the 16-th batch, evaluate loss: 0.688006579875946:  56%|██████████▋        | 14/25 [00:01<00:00, 13.34it/s]evaluate for the 16-th batch, evaluate loss: 0.688006579875946:  64%|████████████▏      | 16/25 [00:01<00:00, 11.95it/s]Epoch: 1, train for the 148-th batch, train loss: 0.3172281086444855:  39%|████▎      | 148/383 [00:39<00:56,  4.18it/s]Epoch: 2, train for the 99-th batch, train loss: 0.48017188906669617:  82%|█████████▉  | 98/119 [00:13<00:03,  6.95it/s]Epoch: 2, train for the 99-th batch, train loss: 0.48017188906669617:  83%|█████████▉  | 99/119 [00:13<00:02,  6.75it/s]Epoch: 2, train for the 52-th batch, train loss: 0.537417471408844:  35%|████▉         | 51/146 [00:06<00:12,  7.45it/s]Epoch: 2, train for the 52-th batch, train loss: 0.537417471408844:  36%|████▉         | 52/146 [00:06<00:12,  7.28it/s]evaluate for the 17-th batch, evaluate loss: 0.6807570457458496:  64%|███████████▌      | 16/25 [00:01<00:00, 11.95it/s]evaluate for the 18-th batch, evaluate loss: 0.6496114730834961:  64%|███████████▌      | 16/25 [00:01<00:00, 11.95it/s]evaluate for the 18-th batch, evaluate loss: 0.6496114730834961:  72%|████████████▉     | 18/25 [00:01<00:00, 12.24it/s]Epoch: 1, train for the 149-th batch, train loss: 0.398099809885025:  39%|████▋       | 148/383 [00:39<00:56,  4.18it/s]Epoch: 1, train for the 149-th batch, train loss: 0.398099809885025:  39%|████▋       | 149/383 [00:39<00:53,  4.41it/s]evaluate for the 19-th batch, evaluate loss: 0.6298666000366211:  72%|████████████▉     | 18/25 [00:01<00:00, 12.24it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5052459836006165:  83%|█████████▉  | 99/119 [00:13<00:02,  6.75it/s]Epoch: 2, train for the 53-th batch, train loss: 0.5464988350868225:  36%|████▋        | 52/146 [00:07<00:12,  7.28it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5052459836006165:  84%|█████████▏ | 100/119 [00:14<00:03,  6.25it/s]Epoch: 2, train for the 53-th batch, train loss: 0.5464988350868225:  36%|████▋        | 53/146 [00:07<00:13,  6.88it/s]evaluate for the 20-th batch, evaluate loss: 0.6904545426368713:  72%|████████████▉     | 18/25 [00:01<00:00, 12.24it/s]evaluate for the 20-th batch, evaluate loss: 0.6904545426368713:  80%|██████████████▍   | 20/25 [00:01<00:00, 13.12it/s]Epoch: 1, train for the 201-th batch, train loss: 0.4889516830444336:  83%|█████████▏ | 200/241 [00:42<00:10,  3.85it/s]evaluate for the 21-th batch, evaluate loss: 0.7122013568878174:  80%|██████████████▍   | 20/25 [00:01<00:00, 13.12it/s]Epoch: 1, train for the 201-th batch, train loss: 0.4889516830444336:  83%|█████████▏ | 201/241 [00:42<00:12,  3.15it/s]Epoch: 1, train for the 164-th batch, train loss: 0.5979090929031372:  69%|███████▌   | 163/237 [00:41<00:24,  3.01it/s]Epoch: 1, train for the 150-th batch, train loss: 0.5757564902305603:  39%|████▎      | 149/383 [00:39<00:53,  4.41it/s]Epoch: 2, train for the 101-th batch, train loss: 0.47044748067855835:  84%|████████▍ | 100/119 [00:14<00:03,  6.25it/s]Epoch: 2, train for the 54-th batch, train loss: 0.5584245324134827:  36%|████▋        | 53/146 [00:07<00:13,  6.88it/s]Epoch: 2, train for the 54-th batch, train loss: 0.5584245324134827:  37%|████▊        | 54/146 [00:07<00:14,  6.56it/s]Epoch: 2, train for the 101-th batch, train loss: 0.47044748067855835:  85%|████████▍ | 101/119 [00:14<00:02,  6.02it/s]Epoch: 1, train for the 164-th batch, train loss: 0.5979090929031372:  69%|███████▌   | 164/237 [00:41<00:27,  2.67it/s]evaluate for the 22-th batch, evaluate loss: 0.6705698370933533:  80%|██████████████▍   | 20/25 [00:01<00:00, 13.12it/s]evaluate for the 22-th batch, evaluate loss: 0.6705698370933533:  88%|███████████████▊  | 22/25 [00:01<00:00, 13.09it/s]Epoch: 1, train for the 150-th batch, train loss: 0.5757564902305603:  39%|████▎      | 150/383 [00:39<00:54,  4.30it/s]Epoch: 1, train for the 202-th batch, train loss: 0.25810402631759644:  83%|████████▎ | 201/241 [00:42<00:12,  3.15it/s]evaluate for the 23-th batch, evaluate loss: 0.6687878966331482:  88%|███████████████▊  | 22/25 [00:01<00:00, 13.09it/s]Epoch: 1, train for the 202-th batch, train loss: 0.25810402631759644:  84%|████████▍ | 202/241 [00:42<00:10,  3.58it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5630533695220947:  37%|████▊        | 54/146 [00:07<00:14,  6.56it/s]Epoch: 2, train for the 102-th batch, train loss: 0.5058152079582214:  85%|█████████▎ | 101/119 [00:14<00:02,  6.02it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5630533695220947:  38%|████▉        | 55/146 [00:07<00:14,  6.27it/s]Epoch: 2, train for the 102-th batch, train loss: 0.5058152079582214:  86%|█████████▍ | 102/119 [00:14<00:02,  5.95it/s]evaluate for the 24-th batch, evaluate loss: 0.6858567595481873:  88%|███████████████▊  | 22/25 [00:01<00:00, 13.09it/s]evaluate for the 24-th batch, evaluate loss: 0.6858567595481873:  96%|█████████████████▎| 24/25 [00:01<00:00, 12.88it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5412315726280212:  39%|████▎      | 150/383 [00:40<00:54,  4.30it/s]Epoch: 1, train for the 165-th batch, train loss: 0.55038982629776:  69%|████████▉    | 164/237 [00:42<00:27,  2.67it/s]Epoch: 1, train for the 151-th batch, train loss: 0.5412315726280212:  39%|████▎      | 151/383 [00:40<00:54,  4.28it/s]Epoch: 1, train for the 165-th batch, train loss: 0.55038982629776:  70%|█████████    | 165/237 [00:42<00:25,  2.88it/s]Epoch: 1, train for the 203-th batch, train loss: 0.585810661315918:  84%|██████████  | 202/241 [00:42<00:10,  3.58it/s]evaluate for the 25-th batch, evaluate loss: 0.6993444561958313:  96%|█████████████████▎| 24/25 [00:02<00:00, 12.88it/s]evaluate for the 25-th batch, evaluate loss: 0.6993444561958313: 100%|██████████████████| 25/25 [00:02<00:00, 12.31it/s]
Epoch: 1, train for the 203-th batch, train loss: 0.585810661315918:  84%|██████████  | 203/241 [00:42<00:09,  4.04it/s]Epoch: 2, train for the 103-th batch, train loss: 0.4944280982017517:  86%|█████████▍ | 102/119 [00:14<00:02,  5.95it/s]Epoch: 2, train for the 103-th batch, train loss: 0.4944280982017517:  87%|█████████▌ | 103/119 [00:14<00:02,  5.96it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5286783576011658:  38%|████▉        | 55/146 [00:07<00:14,  6.27it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5286783576011658:  38%|████▉        | 56/146 [00:07<00:16,  5.52it/s]Epoch: 1, train for the 152-th batch, train loss: 0.4309472143650055:  39%|████▎      | 151/383 [00:40<00:54,  4.28it/s]Epoch: 1, train for the 204-th batch, train loss: 0.4791724383831024:  84%|█████████▎ | 203/241 [00:42<00:09,  4.04it/s]Epoch: 1, train for the 152-th batch, train loss: 0.4309472143650055:  40%|████▎      | 152/383 [00:40<00:50,  4.55it/s]Epoch: 2, train for the 104-th batch, train loss: 0.4750099778175354:  87%|█████████▌ | 103/119 [00:14<00:02,  5.96it/s]Epoch: 1, train for the 204-th batch, train loss: 0.4791724383831024:  85%|█████████▎ | 204/241 [00:42<00:08,  4.53it/s]Epoch: 2, train for the 104-th batch, train loss: 0.4750099778175354:  87%|█████████▌ | 104/119 [00:14<00:02,  6.41it/s]Epoch: 2, train for the 57-th batch, train loss: 0.5491928458213806:  38%|████▉        | 56/146 [00:07<00:16,  5.52it/s]Epoch: 2, train for the 57-th batch, train loss: 0.5491928458213806:  39%|█████        | 57/146 [00:07<00:14,  5.99it/s]Epoch: 1, train for the 166-th batch, train loss: 0.5252636671066284:  70%|███████▋   | 165/237 [00:42<00:25,  2.88it/s]Epoch: 1, train for the 166-th batch, train loss: 0.5252636671066284:  70%|███████▋   | 166/237 [00:42<00:23,  3.02it/s]Epoch: 2, train for the 105-th batch, train loss: 0.46481016278266907:  87%|████████▋ | 104/119 [00:14<00:02,  6.41it/s]Epoch: 2, train for the 105-th batch, train loss: 0.46481016278266907:  88%|████████▊ | 105/119 [00:14<00:02,  6.62it/s]INFO:root:Epoch: 1, learning rate: 0.0001, train loss: 0.6030
INFO:root:train average_precision, 0.7375
INFO:root:train roc_auc, 0.7153
INFO:root:validate loss: 0.5145
INFO:root:validate average_precision, 0.8396
INFO:root:validate roc_auc, 0.8294
INFO:root:new node validate loss: 0.7163
INFO:root:new node validate first_1_average_precision, 0.5398
INFO:root:new node validate first_1_roc_auc, 0.5172
INFO:root:new node validate first_3_average_precision, 0.5779
INFO:root:new node validate first_3_roc_auc, 0.5614
INFO:root:new node validate first_10_average_precision, 0.6325
INFO:root:new node validate first_10_roc_auc, 0.6231
INFO:root:new node validate average_precision, 0.6532
INFO:root:new node validate roc_auc, 0.6405
INFO:root:save model ./saved_models/TGN/ia-retweet-pol/TGN_seed0_tgn-ia-retweet-pol-lincorrect-time-linear/TGN_seed0_tgn-ia-retweet-pol-lincorrect-time-linear.pkl
Epoch: 1, train for the 205-th batch, train loss: 0.4824499487876892:  85%|█████████▎ | 204/241 [00:42<00:08,  4.53it/s]Epoch: 1, train for the 153-th batch, train loss: 0.4919721186161041:  40%|████▎      | 152/383 [00:40<00:50,  4.55it/s]Epoch: 1, train for the 205-th batch, train loss: 0.4824499487876892:  85%|█████████▎ | 205/241 [00:42<00:07,  4.63it/s]Epoch: 2, train for the 58-th batch, train loss: 0.5309150218963623:  39%|█████        | 57/146 [00:07<00:14,  5.99it/s]Epoch: 2, train for the 58-th batch, train loss: 0.5309150218963623:  40%|█████▏       | 58/146 [00:07<00:14,  6.27it/s]Epoch: 1, train for the 153-th batch, train loss: 0.4919721186161041:  40%|████▍      | 153/383 [00:40<00:51,  4.45it/s]Epoch: 2, train for the 106-th batch, train loss: 0.4770564138889313:  88%|█████████▋ | 105/119 [00:14<00:02,  6.62it/s]Epoch: 2, train for the 106-th batch, train loss: 0.4770564138889313:  89%|█████████▊ | 106/119 [00:14<00:01,  6.81it/s]Epoch: 2, train for the 59-th batch, train loss: 0.5100013017654419:  40%|█████▏       | 58/146 [00:08<00:14,  6.27it/s]Epoch: 2, train for the 59-th batch, train loss: 0.5100013017654419:  40%|█████▎       | 59/146 [00:08<00:13,  6.56it/s]Epoch: 1, train for the 206-th batch, train loss: 0.5574461221694946:  85%|█████████▎ | 205/241 [00:43<00:07,  4.63it/s]Epoch: 1, train for the 206-th batch, train loss: 0.5574461221694946:  85%|█████████▍ | 206/241 [00:43<00:07,  4.56it/s]Epoch: 1, train for the 167-th batch, train loss: 0.5593066811561584:  70%|███████▋   | 166/237 [00:42<00:23,  3.02it/s]Epoch: 1, train for the 154-th batch, train loss: 0.44828471541404724:  40%|███▉      | 153/383 [00:40<00:51,  4.45it/s]Epoch: 1, train for the 167-th batch, train loss: 0.5593066811561584:  70%|███████▊   | 167/237 [00:42<00:23,  2.94it/s]Epoch: 1, train for the 154-th batch, train loss: 0.44828471541404724:  40%|████      | 154/383 [00:40<00:54,  4.18it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5076736807823181:  89%|█████████▊ | 106/119 [00:15<00:01,  6.81it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5494726300239563:  40%|█████▎       | 59/146 [00:08<00:13,  6.56it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5076736807823181:  90%|█████████▉ | 107/119 [00:15<00:02,  5.73it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5494726300239563:  41%|█████▎       | 60/146 [00:08<00:13,  6.28it/s]Epoch: 1, train for the 207-th batch, train loss: 0.5644485354423523:  85%|█████████▍ | 206/241 [00:43<00:07,  4.56it/s]Epoch: 1, train for the 207-th batch, train loss: 0.5644485354423523:  86%|█████████▍ | 207/241 [00:43<00:06,  5.29it/s]Epoch: 1, train for the 168-th batch, train loss: 0.5810009241104126:  70%|███████▊   | 167/237 [00:42<00:23,  2.94it/s]Epoch: 2, train for the 108-th batch, train loss: 0.40173789858818054:  90%|████████▉ | 107/119 [00:15<00:02,  5.73it/s]Epoch: 1, train for the 168-th batch, train loss: 0.5810009241104126:  71%|███████▊   | 168/237 [00:42<00:21,  3.28it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5820779800415039:  41%|█████▎       | 60/146 [00:08<00:13,  6.28it/s]Epoch: 2, train for the 108-th batch, train loss: 0.40173789858818054:  91%|█████████ | 108/119 [00:15<00:01,  5.59it/s]Epoch: 1, train for the 155-th batch, train loss: 0.32818397879600525:  40%|████      | 154/383 [00:41<00:54,  4.18it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5820779800415039:  42%|█████▍       | 61/146 [00:08<00:14,  5.95it/s]Epoch: 1, train for the 155-th batch, train loss: 0.32818397879600525:  40%|████      | 155/383 [00:41<00:56,  4.07it/s]Epoch: 1, train for the 208-th batch, train loss: 0.536864697933197:  86%|██████████▎ | 207/241 [00:43<00:06,  5.29it/s]Epoch: 1, train for the 208-th batch, train loss: 0.536864697933197:  86%|██████████▎ | 208/241 [00:43<00:06,  4.74it/s]Epoch: 2, train for the 109-th batch, train loss: 0.48677825927734375:  91%|█████████ | 108/119 [00:15<00:01,  5.59it/s]Epoch: 2, train for the 109-th batch, train loss: 0.48677825927734375:  92%|█████████▏| 109/119 [00:15<00:01,  5.89it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5142436623573303:  42%|█████▍       | 61/146 [00:08<00:14,  5.95it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5142436623573303:  42%|█████▌       | 62/146 [00:08<00:14,  5.99it/s]Epoch: 1, train for the 169-th batch, train loss: 0.5674533843994141:  71%|███████▊   | 168/237 [00:43<00:21,  3.28it/s]Epoch: 1, train for the 169-th batch, train loss: 0.5674533843994141:  71%|███████▊   | 169/237 [00:43<00:19,  3.54it/s]Epoch: 1, train for the 156-th batch, train loss: 0.37007230520248413:  40%|████      | 155/383 [00:41<00:56,  4.07it/s]Epoch: 1, train for the 209-th batch, train loss: 0.5349070429801941:  86%|█████████▍ | 208/241 [00:43<00:06,  4.74it/s]Epoch: 1, train for the 209-th batch, train loss: 0.5349070429801941:  87%|█████████▌ | 209/241 [00:43<00:06,  5.26it/s]Epoch: 1, train for the 156-th batch, train loss: 0.37007230520248413:  41%|████      | 156/383 [00:41<00:53,  4.28it/s]Epoch: 2, train for the 110-th batch, train loss: 0.5343228578567505:  92%|██████████ | 109/119 [00:15<00:01,  5.89it/s]Epoch: 2, train for the 63-th batch, train loss: 0.5557494759559631:  42%|█████▌       | 62/146 [00:08<00:14,  5.99it/s]Epoch: 2, train for the 110-th batch, train loss: 0.5343228578567505:  92%|██████████▏| 110/119 [00:15<00:01,  5.85it/s]Epoch: 2, train for the 63-th batch, train loss: 0.5557494759559631:  43%|█████▌       | 63/146 [00:08<00:13,  6.09it/s]Epoch: 1, train for the 210-th batch, train loss: 0.46762797236442566:  87%|████████▋ | 209/241 [00:43<00:06,  5.26it/s]Epoch: 1, train for the 170-th batch, train loss: 0.5628708004951477:  71%|███████▊   | 169/237 [00:43<00:19,  3.54it/s]Epoch: 1, train for the 210-th batch, train loss: 0.46762797236442566:  87%|████████▋ | 210/241 [00:43<00:05,  5.57it/s]Epoch: 1, train for the 170-th batch, train loss: 0.5628708004951477:  72%|███████▉   | 170/237 [00:43<00:17,  3.85it/s]Epoch: 1, train for the 157-th batch, train loss: 0.4457762539386749:  41%|████▍      | 156/383 [00:41<00:53,  4.28it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5549159646034241:  43%|█████▌       | 63/146 [00:08<00:13,  6.09it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5549159646034241:  44%|█████▋       | 64/146 [00:08<00:13,  6.29it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5235039591789246:  92%|██████████▏| 110/119 [00:15<00:01,  5.85it/s]Epoch: 1, train for the 157-th batch, train loss: 0.4457762539386749:  41%|████▌      | 157/383 [00:41<00:52,  4.34it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5235039591789246:  93%|██████████▎| 111/119 [00:15<00:01,  5.92it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5685595273971558:  44%|█████▋       | 64/146 [00:09<00:13,  6.29it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5685595273971558:  45%|█████▊       | 65/146 [00:09<00:12,  6.38it/s]Epoch: 1, train for the 211-th batch, train loss: 0.3900584280490875:  87%|█████████▌ | 210/241 [00:44<00:05,  5.57it/s]Epoch: 2, train for the 112-th batch, train loss: 0.54405277967453:  93%|████████████▏| 111/119 [00:16<00:01,  5.92it/s]Epoch: 2, train for the 112-th batch, train loss: 0.54405277967453:  94%|████████████▏| 112/119 [00:16<00:01,  5.97it/s]Epoch: 1, train for the 211-th batch, train loss: 0.3900584280490875:  88%|█████████▋ | 211/241 [00:44<00:06,  4.93it/s]Epoch: 1, train for the 171-th batch, train loss: 0.5650452375411987:  72%|███████▉   | 170/237 [00:43<00:17,  3.85it/s]Epoch: 1, train for the 158-th batch, train loss: 0.4112628102302551:  41%|████▌      | 157/383 [00:41<00:52,  4.34it/s]Epoch: 1, train for the 171-th batch, train loss: 0.5650452375411987:  72%|███████▉   | 171/237 [00:43<00:17,  3.79it/s]Epoch: 1, train for the 158-th batch, train loss: 0.4112628102302551:  41%|████▌      | 158/383 [00:41<00:53,  4.18it/s]Epoch: 1, train for the 212-th batch, train loss: 0.5037074089050293:  88%|█████████▋ | 211/241 [00:44<00:06,  4.93it/s]Epoch: 2, train for the 66-th batch, train loss: 0.5339938402175903:  45%|█████▊       | 65/146 [00:09<00:12,  6.38it/s]Epoch: 1, train for the 212-th batch, train loss: 0.5037074089050293:  88%|█████████▋ | 212/241 [00:44<00:05,  5.54it/s]Epoch: 2, train for the 113-th batch, train loss: 0.5043377876281738:  94%|██████████▎| 112/119 [00:16<00:01,  5.97it/s]Epoch: 2, train for the 66-th batch, train loss: 0.5339938402175903:  45%|█████▉       | 66/146 [00:09<00:12,  6.21it/s]Epoch: 2, train for the 113-th batch, train loss: 0.5043377876281738:  95%|██████████▍| 113/119 [00:16<00:00,  6.05it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]Epoch: 1, train for the 172-th batch, train loss: 0.5297314524650574:  72%|███████▉   | 171/237 [00:43<00:17,  3.79it/s]Epoch: 1, train for the 172-th batch, train loss: 0.5297314524650574:  73%|███████▉   | 172/237 [00:43<00:15,  4.07it/s]Epoch: 2, train for the 67-th batch, train loss: 0.5826697945594788:  45%|█████▉       | 66/146 [00:09<00:12,  6.21it/s]Epoch: 2, train for the 67-th batch, train loss: 0.5826697945594788:  46%|█████▉       | 67/146 [00:09<00:12,  6.38it/s]Epoch: 1, train for the 213-th batch, train loss: 0.5306148529052734:  88%|█████████▋ | 212/241 [00:44<00:05,  5.54it/s]Epoch: 2, train for the 114-th batch, train loss: 0.449602335691452:  95%|███████████▍| 113/119 [00:16<00:00,  6.05it/s]Epoch: 2, train for the 1-th batch, train loss: 0.7310431003570557:   0%|                       | 0/151 [00:00<?, ?it/s]Epoch: 2, train for the 114-th batch, train loss: 0.449602335691452:  96%|███████████▍| 114/119 [00:16<00:00,  5.91it/s]Epoch: 2, train for the 1-th batch, train loss: 0.7310431003570557:   1%|               | 1/151 [00:00<00:23,  6.27it/s]Epoch: 1, train for the 213-th batch, train loss: 0.5306148529052734:  88%|█████████▋ | 213/241 [00:44<00:05,  5.33it/s]Epoch: 2, train for the 68-th batch, train loss: 0.533740758895874:  46%|██████▍       | 67/146 [00:09<00:12,  6.38it/s]Epoch: 2, train for the 68-th batch, train loss: 0.533740758895874:  47%|██████▌       | 68/146 [00:09<00:11,  6.89it/s]Epoch: 2, train for the 115-th batch, train loss: 0.4461268484592438:  96%|██████████▌| 114/119 [00:16<00:00,  5.91it/s]Epoch: 2, train for the 115-th batch, train loss: 0.4461268484592438:  97%|██████████▋| 115/119 [00:16<00:00,  6.36it/s]Epoch: 1, train for the 173-th batch, train loss: 0.49104735255241394:  73%|███████▎  | 172/237 [00:44<00:15,  4.07it/s]Epoch: 1, train for the 159-th batch, train loss: 0.4666665196418762:  41%|████▌      | 158/383 [00:42<00:53,  4.18it/s]Epoch: 2, train for the 2-th batch, train loss: 0.7095738053321838:   1%|               | 1/151 [00:00<00:23,  6.27it/s]Epoch: 2, train for the 2-th batch, train loss: 0.7095738053321838:   1%|▏              | 2/151 [00:00<00:24,  6.03it/s]Epoch: 1, train for the 214-th batch, train loss: 0.4890478551387787:  88%|█████████▋ | 213/241 [00:44<00:05,  5.33it/s]Epoch: 1, train for the 173-th batch, train loss: 0.49104735255241394:  73%|███████▎  | 173/237 [00:44<00:16,  3.96it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5697896480560303:  47%|██████       | 68/146 [00:09<00:11,  6.89it/s]Epoch: 1, train for the 214-th batch, train loss: 0.4890478551387787:  89%|█████████▊ | 214/241 [00:44<00:05,  5.28it/s]Epoch: 1, train for the 159-th batch, train loss: 0.4666665196418762:  42%|████▌      | 159/383 [00:42<01:07,  3.31it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5697896480560303:  47%|██████▏      | 69/146 [00:09<00:10,  7.14it/s]Epoch: 2, train for the 116-th batch, train loss: 0.4082227945327759:  97%|██████████▋| 115/119 [00:16<00:00,  6.36it/s]Epoch: 2, train for the 3-th batch, train loss: 0.6928303241729736:   1%|▏              | 2/151 [00:00<00:24,  6.03it/s]Epoch: 2, train for the 116-th batch, train loss: 0.4082227945327759:  97%|██████████▋| 116/119 [00:16<00:00,  6.46it/s]Epoch: 2, train for the 3-th batch, train loss: 0.6928303241729736:   2%|▎              | 3/151 [00:00<00:21,  7.04it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5474132299423218:  47%|██████▏      | 69/146 [00:09<00:10,  7.14it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5474132299423218:  48%|██████▏      | 70/146 [00:09<00:10,  7.13it/s]Epoch: 1, train for the 215-th batch, train loss: 0.5102415084838867:  89%|█████████▊ | 214/241 [00:44<00:05,  5.28it/s]Epoch: 1, train for the 174-th batch, train loss: 0.5495671033859253:  73%|████████   | 173/237 [00:44<00:16,  3.96it/s]Epoch: 2, train for the 117-th batch, train loss: 0.43467849493026733:  97%|█████████▋| 116/119 [00:16<00:00,  6.46it/s]Epoch: 2, train for the 4-th batch, train loss: 0.6715083122253418:   2%|▎              | 3/151 [00:00<00:21,  7.04it/s]Epoch: 1, train for the 215-th batch, train loss: 0.5102415084838867:  89%|█████████▊ | 215/241 [00:44<00:05,  5.12it/s]Epoch: 2, train for the 117-th batch, train loss: 0.43467849493026733:  98%|█████████▊| 117/119 [00:16<00:00,  6.81it/s]Epoch: 2, train for the 4-th batch, train loss: 0.6715083122253418:   3%|▍              | 4/151 [00:00<00:19,  7.39it/s]Epoch: 1, train for the 160-th batch, train loss: 0.49953779578208923:  42%|████▏     | 159/383 [00:42<01:07,  3.31it/s]Epoch: 1, train for the 174-th batch, train loss: 0.5495671033859253:  73%|████████   | 174/237 [00:44<00:15,  3.98it/s]Epoch: 1, train for the 160-th batch, train loss: 0.49953779578208923:  42%|████▏     | 160/383 [00:42<01:05,  3.42it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5637586116790771:  48%|██████▏      | 70/146 [00:09<00:10,  7.13it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5637586116790771:  49%|██████▎      | 71/146 [00:09<00:10,  7.14it/s]Epoch: 2, train for the 118-th batch, train loss: 0.4459052085876465:  98%|██████████▊| 117/119 [00:16<00:00,  6.81it/s]Epoch: 2, train for the 118-th batch, train loss: 0.4459052085876465:  99%|██████████▉| 118/119 [00:16<00:00,  7.14it/s]Epoch: 2, train for the 5-th batch, train loss: 0.6748997569084167:   3%|▍              | 4/151 [00:00<00:19,  7.39it/s]Epoch: 2, train for the 5-th batch, train loss: 0.6748997569084167:   3%|▍              | 5/151 [00:00<00:19,  7.31it/s]Epoch: 1, train for the 216-th batch, train loss: 0.5855144262313843:  89%|█████████▊ | 215/241 [00:44<00:05,  5.12it/s]Epoch: 1, train for the 216-th batch, train loss: 0.5855144262313843:  90%|█████████▊ | 216/241 [00:44<00:04,  5.14it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5729657411575317:  49%|██████▎      | 71/146 [00:10<00:10,  7.14it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5729657411575317:  49%|██████▍      | 72/146 [00:10<00:10,  7.14it/s]Epoch: 2, train for the 119-th batch, train loss: 0.4257115125656128:  99%|██████████▉| 118/119 [00:16<00:00,  7.14it/s]Epoch: 1, train for the 161-th batch, train loss: 0.45372098684310913:  42%|████▏     | 160/383 [00:42<01:05,  3.42it/s]Epoch: 2, train for the 119-th batch, train loss: 0.4257115125656128: 100%|███████████| 119/119 [00:17<00:00,  7.43it/s]Epoch: 2, train for the 119-th batch, train loss: 0.4257115125656128: 100%|███████████| 119/119 [00:17<00:00,  7.00it/s]
Epoch: 2, train for the 6-th batch, train loss: 0.7067857384681702:   3%|▍              | 5/151 [00:00<00:19,  7.31it/s]Epoch: 2, train for the 6-th batch, train loss: 0.7067857384681702:   4%|▌              | 6/151 [00:00<00:19,  7.61it/s]Epoch: 1, train for the 161-th batch, train loss: 0.45372098684310913:  42%|████▏     | 161/383 [00:42<00:59,  3.71it/s]Epoch: 1, train for the 175-th batch, train loss: 0.569813072681427:  73%|████████▊   | 174/237 [00:44<00:15,  3.98it/s]Epoch: 2, train for the 73-th batch, train loss: 0.5618500113487244:  49%|██████▍      | 72/146 [00:10<00:10,  7.14it/s]Epoch: 1, train for the 217-th batch, train loss: 0.5069820880889893:  90%|█████████▊ | 216/241 [00:45<00:04,  5.14it/s]Epoch: 2, train for the 73-th batch, train loss: 0.5618500113487244:  50%|██████▌      | 73/146 [00:10<00:09,  7.68it/s]Epoch: 1, train for the 175-th batch, train loss: 0.569813072681427:  74%|████████▊   | 175/237 [00:44<00:16,  3.72it/s]Epoch: 1, train for the 217-th batch, train loss: 0.5069820880889893:  90%|█████████▉ | 217/241 [00:45<00:04,  5.55it/s]Epoch: 2, train for the 7-th batch, train loss: 0.6877369284629822:   4%|▌              | 6/151 [00:00<00:19,  7.61it/s]Epoch: 2, train for the 7-th batch, train loss: 0.6877369284629822:   5%|▋              | 7/151 [00:00<00:18,  7.81it/s]  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 2, train for the 74-th batch, train loss: 0.5334955453872681:  50%|██████▌      | 73/146 [00:10<00:09,  7.68it/s]Epoch: 2, train for the 74-th batch, train loss: 0.5334955453872681:  51%|██████▌      | 74/146 [00:10<00:09,  7.62it/s]evaluate for the 1-th batch, evaluate loss: 0.4876997172832489:   0%|                            | 0/40 [00:00<?, ?it/s]Epoch: 1, train for the 176-th batch, train loss: 0.5632839202880859:  74%|████████   | 175/237 [00:44<00:16,  3.72it/s]Epoch: 1, train for the 218-th batch, train loss: 0.43540626764297485:  90%|█████████ | 217/241 [00:45<00:04,  5.55it/s]Epoch: 2, train for the 8-th batch, train loss: 0.6962659358978271:   5%|▋              | 7/151 [00:01<00:18,  7.81it/s]Epoch: 1, train for the 176-th batch, train loss: 0.5632839202880859:  74%|████████▏  | 176/237 [00:44<00:15,  4.05it/s]Epoch: 2, train for the 8-th batch, train loss: 0.6962659358978271:   5%|▊              | 8/151 [00:01<00:19,  7.40it/s]Epoch: 1, train for the 218-th batch, train loss: 0.43540626764297485:  90%|█████████ | 218/241 [00:45<00:04,  5.41it/s]Epoch: 1, train for the 162-th batch, train loss: 0.4583529531955719:  42%|████▌      | 161/383 [00:43<00:59,  3.71it/s]evaluate for the 2-th batch, evaluate loss: 0.5106490850448608:   0%|                            | 0/40 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5106490850448608:   5%|█                   | 2/40 [00:00<00:02, 15.01it/s]Epoch: 2, train for the 75-th batch, train loss: 0.5334137678146362:  51%|██████▌      | 74/146 [00:10<00:09,  7.62it/s]Epoch: 2, train for the 75-th batch, train loss: 0.5334137678146362:  51%|██████▋      | 75/146 [00:10<00:09,  7.52it/s]Epoch: 1, train for the 162-th batch, train loss: 0.4583529531955719:  42%|████▋      | 162/383 [00:43<01:03,  3.46it/s]Epoch: 2, train for the 9-th batch, train loss: 0.7094062566757202:   5%|▊              | 8/151 [00:01<00:19,  7.40it/s]evaluate for the 3-th batch, evaluate loss: 0.5074435472488403:   5%|█                   | 2/40 [00:00<00:02, 15.01it/s]Epoch: 2, train for the 9-th batch, train loss: 0.7094062566757202:   6%|▉              | 9/151 [00:01<00:17,  8.00it/s]Epoch: 1, train for the 219-th batch, train loss: 0.4824831187725067:  90%|█████████▉ | 218/241 [00:45<00:04,  5.41it/s]Epoch: 1, train for the 219-th batch, train loss: 0.4824831187725067:  91%|█████████▉ | 219/241 [00:45<00:03,  5.58it/s]evaluate for the 4-th batch, evaluate loss: 0.5747424364089966:   5%|█                   | 2/40 [00:00<00:02, 15.01it/s]evaluate for the 4-th batch, evaluate loss: 0.5747424364089966:  10%|██                  | 4/40 [00:00<00:02, 15.26it/s]Epoch: 1, train for the 177-th batch, train loss: 0.5866276025772095:  74%|████████▏  | 176/237 [00:45<00:15,  4.05it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5734553337097168:  51%|██████▋      | 75/146 [00:10<00:09,  7.52it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5734553337097168:  52%|██████▊      | 76/146 [00:10<00:09,  7.02it/s]Epoch: 1, train for the 177-th batch, train loss: 0.5866276025772095:  75%|████████▏  | 177/237 [00:45<00:14,  4.13it/s]evaluate for the 5-th batch, evaluate loss: 0.5372250080108643:  10%|██                  | 4/40 [00:00<00:02, 15.26it/s]Epoch: 2, train for the 10-th batch, train loss: 0.6708323359489441:   6%|▊             | 9/151 [00:01<00:17,  8.00it/s]Epoch: 2, train for the 10-th batch, train loss: 0.6708323359489441:   7%|▊            | 10/151 [00:01<00:18,  7.66it/s]Epoch: 1, train for the 163-th batch, train loss: 0.3853325843811035:  42%|████▋      | 162/383 [00:43<01:03,  3.46it/s]evaluate for the 6-th batch, evaluate loss: 0.5203524827957153:  10%|██                  | 4/40 [00:00<00:02, 15.26it/s]evaluate for the 6-th batch, evaluate loss: 0.5203524827957153:  15%|███                 | 6/40 [00:00<00:02, 15.76it/s]Epoch: 1, train for the 220-th batch, train loss: 0.573455274105072:  91%|██████████▉ | 219/241 [00:45<00:03,  5.58it/s]Epoch: 1, train for the 163-th batch, train loss: 0.3853325843811035:  43%|████▋      | 163/383 [00:43<00:59,  3.72it/s]Epoch: 1, train for the 220-th batch, train loss: 0.573455274105072:  91%|██████████▉ | 220/241 [00:45<00:03,  5.82it/s]Epoch: 2, train for the 77-th batch, train loss: 0.6152286529541016:  52%|██████▊      | 76/146 [00:10<00:09,  7.02it/s]evaluate for the 7-th batch, evaluate loss: 0.5254155397415161:  15%|███                 | 6/40 [00:00<00:02, 15.76it/s]Epoch: 2, train for the 77-th batch, train loss: 0.6152286529541016:  53%|██████▊      | 77/146 [00:10<00:10,  6.83it/s]Epoch: 2, train for the 11-th batch, train loss: 0.6645218729972839:   7%|▊            | 10/151 [00:01<00:18,  7.66it/s]Epoch: 2, train for the 11-th batch, train loss: 0.6645218729972839:   7%|▉            | 11/151 [00:01<00:19,  7.24it/s]evaluate for the 8-th batch, evaluate loss: 0.5003117918968201:  15%|███                 | 6/40 [00:00<00:02, 15.76it/s]evaluate for the 8-th batch, evaluate loss: 0.5003117918968201:  20%|████                | 8/40 [00:00<00:02, 15.59it/s]Epoch: 1, train for the 221-th batch, train loss: 0.5583108067512512:  91%|██████████ | 220/241 [00:45<00:03,  5.82it/s]evaluate for the 9-th batch, evaluate loss: 0.5290707945823669:  20%|████                | 8/40 [00:00<00:02, 15.59it/s]Epoch: 1, train for the 221-th batch, train loss: 0.5583108067512512:  92%|██████████ | 221/241 [00:45<00:03,  5.82it/s]Epoch: 2, train for the 12-th batch, train loss: 0.6842121481895447:   7%|▉            | 11/151 [00:01<00:19,  7.24it/s]Epoch: 2, train for the 78-th batch, train loss: 0.5656686425209045:  53%|██████▊      | 77/146 [00:10<00:10,  6.83it/s]evaluate for the 10-th batch, evaluate loss: 0.5689586400985718:  20%|███▊               | 8/40 [00:00<00:02, 15.59it/s]evaluate for the 10-th batch, evaluate loss: 0.5689586400985718:  25%|████▌             | 10/40 [00:00<00:01, 15.18it/s]Epoch: 2, train for the 12-th batch, train loss: 0.6842121481895447:   8%|█            | 12/151 [00:01<00:20,  6.86it/s]Epoch: 2, train for the 78-th batch, train loss: 0.5656686425209045:  53%|██████▉      | 78/146 [00:10<00:10,  6.24it/s]Epoch: 1, train for the 178-th batch, train loss: 0.5789570212364197:  75%|████████▏  | 177/237 [00:45<00:14,  4.13it/s]Epoch: 1, train for the 222-th batch, train loss: 0.494455486536026:  92%|███████████ | 221/241 [00:45<00:03,  5.82it/s]evaluate for the 11-th batch, evaluate loss: 0.5201146602630615:  25%|████▌             | 10/40 [00:00<00:01, 15.18it/s]Epoch: 1, train for the 164-th batch, train loss: 0.36999619007110596:  43%|████▎     | 163/383 [00:43<00:59,  3.72it/s]Epoch: 1, train for the 222-th batch, train loss: 0.494455486536026:  92%|███████████ | 222/241 [00:45<00:03,  6.11it/s]Epoch: 1, train for the 178-th batch, train loss: 0.5789570212364197:  75%|████████▎  | 178/237 [00:45<00:17,  3.42it/s]Epoch: 1, train for the 164-th batch, train loss: 0.36999619007110596:  43%|████▎     | 164/383 [00:43<01:04,  3.37it/s]evaluate for the 12-th batch, evaluate loss: 0.5345136523246765:  25%|████▌             | 10/40 [00:00<00:01, 15.18it/s]evaluate for the 12-th batch, evaluate loss: 0.5345136523246765:  30%|█████▍            | 12/40 [00:00<00:01, 15.16it/s]Epoch: 2, train for the 13-th batch, train loss: 0.6832220554351807:   8%|█            | 12/151 [00:01<00:20,  6.86it/s]Epoch: 2, train for the 79-th batch, train loss: 0.5895265936851501:  53%|██████▉      | 78/146 [00:11<00:10,  6.24it/s]Epoch: 2, train for the 13-th batch, train loss: 0.6832220554351807:   9%|█            | 13/151 [00:01<00:20,  6.90it/s]Epoch: 2, train for the 79-th batch, train loss: 0.5895265936851501:  54%|███████      | 79/146 [00:11<00:10,  6.45it/s]evaluate for the 13-th batch, evaluate loss: 0.5287290215492249:  30%|█████▍            | 12/40 [00:00<00:01, 15.16it/s]Epoch: 1, train for the 223-th batch, train loss: 0.5001270771026611:  92%|██████████▏| 222/241 [00:46<00:03,  6.11it/s]Epoch: 1, train for the 223-th batch, train loss: 0.5001270771026611:  93%|██████████▏| 223/241 [00:46<00:02,  6.13it/s]Epoch: 1, train for the 179-th batch, train loss: 0.5165451765060425:  75%|████████▎  | 178/237 [00:45<00:17,  3.42it/s]Epoch: 2, train for the 14-th batch, train loss: 0.6921051144599915:   9%|█            | 13/151 [00:01<00:20,  6.90it/s]evaluate for the 14-th batch, evaluate loss: 0.5144273042678833:  30%|█████▍            | 12/40 [00:00<00:01, 15.16it/s]evaluate for the 14-th batch, evaluate loss: 0.5144273042678833:  35%|██████▎           | 14/40 [00:00<00:01, 14.81it/s]Epoch: 1, train for the 179-th batch, train loss: 0.5165451765060425:  76%|████████▎  | 179/237 [00:45<00:15,  3.79it/s]Epoch: 2, train for the 14-th batch, train loss: 0.6921051144599915:   9%|█▏           | 14/151 [00:01<00:19,  7.02it/s]Epoch: 1, train for the 165-th batch, train loss: 0.333903431892395:  43%|█████▏      | 164/383 [00:43<01:04,  3.37it/s]Epoch: 2, train for the 80-th batch, train loss: 0.5697998404502869:  54%|███████      | 79/146 [00:11<00:10,  6.45it/s]Epoch: 2, train for the 80-th batch, train loss: 0.5697998404502869:  55%|███████      | 80/146 [00:11<00:10,  6.40it/s]Epoch: 1, train for the 165-th batch, train loss: 0.333903431892395:  43%|█████▏      | 165/383 [00:43<01:00,  3.61it/s]evaluate for the 15-th batch, evaluate loss: 0.533208966255188:  35%|██████▋            | 14/40 [00:00<00:01, 14.81it/s]Epoch: 2, train for the 15-th batch, train loss: 0.7105401158332825:   9%|█▏           | 14/151 [00:02<00:19,  7.02it/s]Epoch: 2, train for the 15-th batch, train loss: 0.7105401158332825:  10%|█▎           | 15/151 [00:02<00:18,  7.55it/s]evaluate for the 16-th batch, evaluate loss: 0.5148674249649048:  35%|██████▎           | 14/40 [00:01<00:01, 14.81it/s]evaluate for the 16-th batch, evaluate loss: 0.5148674249649048:  40%|███████▏          | 16/40 [00:01<00:01, 15.34it/s]Epoch: 2, train for the 81-th batch, train loss: 0.5345857739448547:  55%|███████      | 80/146 [00:11<00:10,  6.40it/s]evaluate for the 17-th batch, evaluate loss: 0.5489605665206909:  40%|███████▏          | 16/40 [00:01<00:01, 15.34it/s]Epoch: 2, train for the 81-th batch, train loss: 0.5345857739448547:  55%|███████▏     | 81/146 [00:11<00:10,  6.48it/s]Epoch: 1, train for the 180-th batch, train loss: 0.5839439034461975:  76%|████████▎  | 179/237 [00:45<00:15,  3.79it/s]evaluate for the 18-th batch, evaluate loss: 0.5389527678489685:  40%|███████▏          | 16/40 [00:01<00:01, 15.34it/s]evaluate for the 18-th batch, evaluate loss: 0.5389527678489685:  45%|████████          | 18/40 [00:01<00:01, 15.50it/s]Epoch: 1, train for the 224-th batch, train loss: 0.5105549097061157:  93%|██████████▏| 223/241 [00:46<00:02,  6.13it/s]Epoch: 1, train for the 166-th batch, train loss: 0.4653647541999817:  43%|████▋      | 165/383 [00:44<01:00,  3.61it/s]Epoch: 2, train for the 16-th batch, train loss: 0.6836891174316406:  10%|█▎           | 15/151 [00:02<00:18,  7.55it/s]Epoch: 1, train for the 224-th batch, train loss: 0.5105549097061157:  93%|██████████▏| 224/241 [00:46<00:03,  4.81it/s]Epoch: 2, train for the 16-th batch, train loss: 0.6836891174316406:  11%|█▍           | 16/151 [00:02<00:19,  7.03it/s]Epoch: 1, train for the 180-th batch, train loss: 0.5839439034461975:  76%|████████▎  | 180/237 [00:46<00:15,  3.67it/s]Epoch: 1, train for the 166-th batch, train loss: 0.4653647541999817:  43%|████▊      | 166/383 [00:44<00:57,  3.75it/s]Epoch: 2, train for the 82-th batch, train loss: 0.544971227645874:  55%|███████▊      | 81/146 [00:11<00:10,  6.48it/s]evaluate for the 19-th batch, evaluate loss: 0.5288803577423096:  45%|████████          | 18/40 [00:01<00:01, 15.50it/s]Epoch: 2, train for the 82-th batch, train loss: 0.544971227645874:  56%|███████▊      | 82/146 [00:11<00:09,  6.84it/s]evaluate for the 20-th batch, evaluate loss: 0.518967866897583:  45%|████████▌          | 18/40 [00:01<00:01, 15.50it/s]evaluate for the 20-th batch, evaluate loss: 0.518967866897583:  50%|█████████▌         | 20/40 [00:01<00:01, 14.61it/s]Epoch: 1, train for the 225-th batch, train loss: 0.5413540601730347:  93%|██████████▏| 224/241 [00:46<00:03,  4.81it/s]Epoch: 1, train for the 225-th batch, train loss: 0.5413540601730347:  93%|██████████▎| 225/241 [00:46<00:03,  4.85it/s]Epoch: 2, train for the 17-th batch, train loss: 0.686825692653656:  11%|█▍            | 16/151 [00:02<00:19,  7.03it/s]evaluate for the 21-th batch, evaluate loss: 0.5405011773109436:  50%|█████████         | 20/40 [00:01<00:01, 14.61it/s]Epoch: 2, train for the 17-th batch, train loss: 0.686825692653656:  11%|█▌            | 17/151 [00:02<00:21,  6.18it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5526943802833557:  56%|███████▎     | 82/146 [00:11<00:09,  6.84it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5526943802833557:  57%|███████▍     | 83/146 [00:11<00:10,  6.23it/s]Epoch: 1, train for the 167-th batch, train loss: 0.43072909116744995:  43%|████▎     | 166/383 [00:44<00:57,  3.75it/s]Epoch: 1, train for the 181-th batch, train loss: 0.5455543994903564:  76%|████████▎  | 180/237 [00:46<00:15,  3.67it/s]Epoch: 1, train for the 167-th batch, train loss: 0.43072909116744995:  44%|████▎     | 167/383 [00:44<00:58,  3.68it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5313555598258972:  57%|███████▍     | 83/146 [00:11<00:10,  6.23it/s]Epoch: 1, train for the 181-th batch, train loss: 0.5455543994903564:  76%|████████▍  | 181/237 [00:46<00:15,  3.51it/s]Epoch: 2, train for the 18-th batch, train loss: 0.6801897287368774:  11%|█▍           | 17/151 [00:02<00:21,  6.18it/s]Epoch: 2, train for the 18-th batch, train loss: 0.6801897287368774:  12%|█▌           | 18/151 [00:02<00:20,  6.56it/s]Epoch: 1, train for the 226-th batch, train loss: 0.5487245917320251:  93%|██████████▎| 225/241 [00:46<00:03,  4.85it/s]Epoch: 1, train for the 226-th batch, train loss: 0.5487245917320251:  94%|██████████▎| 226/241 [00:46<00:03,  4.67it/s]evaluate for the 22-th batch, evaluate loss: 0.5020196437835693:  50%|█████████         | 20/40 [00:01<00:01, 14.61it/s]evaluate for the 22-th batch, evaluate loss: 0.5020196437835693:  55%|█████████▉        | 22/40 [00:01<00:01, 10.17it/s]Epoch: 2, train for the 85-th batch, train loss: 0.5232264399528503:  57%|███████▍     | 83/146 [00:11<00:10,  6.23it/s]Epoch: 2, train for the 19-th batch, train loss: 0.6794077754020691:  12%|█▌           | 18/151 [00:02<00:20,  6.56it/s]Epoch: 2, train for the 85-th batch, train loss: 0.5232264399528503:  58%|███████▌     | 85/146 [00:11<00:08,  7.06it/s]Epoch: 2, train for the 19-th batch, train loss: 0.6794077754020691:  13%|█▋           | 19/151 [00:02<00:19,  6.91it/s]Epoch: 1, train for the 168-th batch, train loss: 0.31487342715263367:  44%|████▎     | 167/383 [00:44<00:58,  3.68it/s]evaluate for the 23-th batch, evaluate loss: 0.4638969302177429:  55%|█████████▉        | 22/40 [00:01<00:01, 10.17it/s]Epoch: 1, train for the 182-th batch, train loss: 0.5860403180122375:  76%|████████▍  | 181/237 [00:46<00:15,  3.51it/s]Epoch: 1, train for the 168-th batch, train loss: 0.31487342715263367:  44%|████▍     | 168/383 [00:44<00:57,  3.77it/s]Epoch: 1, train for the 227-th batch, train loss: 0.5783560276031494:  94%|██████████▎| 226/241 [00:47<00:03,  4.67it/s]Epoch: 1, train for the 182-th batch, train loss: 0.5860403180122375:  77%|████████▍  | 182/237 [00:46<00:15,  3.66it/s]Epoch: 1, train for the 227-th batch, train loss: 0.5783560276031494:  94%|██████████▎| 227/241 [00:47<00:02,  5.05it/s]Epoch: 2, train for the 20-th batch, train loss: 0.6607031226158142:  13%|█▋           | 19/151 [00:02<00:19,  6.91it/s]Epoch: 2, train for the 20-th batch, train loss: 0.6607031226158142:  13%|█▋           | 20/151 [00:02<00:18,  6.95it/s]evaluate for the 24-th batch, evaluate loss: 0.512791097164154:  55%|██████████▍        | 22/40 [00:01<00:01, 10.17it/s]evaluate for the 24-th batch, evaluate loss: 0.512791097164154:  60%|███████████▍       | 24/40 [00:01<00:01, 10.81it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5573253035545349:  58%|███████▌     | 85/146 [00:12<00:08,  7.06it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5573253035545349:  59%|███████▋     | 86/146 [00:12<00:08,  6.68it/s]evaluate for the 25-th batch, evaluate loss: 0.5716477632522583:  60%|██████████▊       | 24/40 [00:01<00:01, 10.81it/s]Epoch: 2, train for the 21-th batch, train loss: 0.6681881546974182:  13%|█▋           | 20/151 [00:02<00:18,  6.95it/s]Epoch: 2, train for the 21-th batch, train loss: 0.6681881546974182:  14%|█▊           | 21/151 [00:02<00:18,  6.97it/s]Epoch: 2, train for the 87-th batch, train loss: 0.54954594373703:  59%|████████▊      | 86/146 [00:12<00:08,  6.68it/s]Epoch: 1, train for the 183-th batch, train loss: 0.5929039716720581:  77%|████████▍  | 182/237 [00:46<00:15,  3.66it/s]Epoch: 2, train for the 87-th batch, train loss: 0.54954594373703:  60%|████████▉      | 87/146 [00:12<00:08,  6.90it/s]Epoch: 1, train for the 169-th batch, train loss: 0.42022696137428284:  44%|████▍     | 168/383 [00:44<00:57,  3.77it/s]Epoch: 1, train for the 183-th batch, train loss: 0.5929039716720581:  77%|████████▍  | 183/237 [00:46<00:14,  3.86it/s]Epoch: 1, train for the 169-th batch, train loss: 0.42022696137428284:  44%|████▍     | 169/383 [00:44<00:57,  3.71it/s]Epoch: 2, train for the 22-th batch, train loss: 0.642177939414978:  14%|█▉            | 21/151 [00:03<00:18,  6.97it/s]Epoch: 2, train for the 88-th batch, train loss: 0.570324718952179:  60%|████████▎     | 87/146 [00:12<00:08,  6.90it/s]Epoch: 1, train for the 228-th batch, train loss: 0.5276396870613098:  94%|██████████▎| 227/241 [00:47<00:02,  5.05it/s]evaluate for the 26-th batch, evaluate loss: 0.48242372274398804:  60%|██████████▏      | 24/40 [00:02<00:01, 10.81it/s]evaluate for the 26-th batch, evaluate loss: 0.48242372274398804:  65%|███████████      | 26/40 [00:02<00:01,  9.47it/s]Epoch: 2, train for the 88-th batch, train loss: 0.570324718952179:  60%|████████▍     | 88/146 [00:12<00:07,  7.36it/s]Epoch: 1, train for the 228-th batch, train loss: 0.5276396870613098:  95%|██████████▍| 228/241 [00:47<00:03,  4.27it/s]evaluate for the 27-th batch, evaluate loss: 0.5006499290466309:  65%|███████████▋      | 26/40 [00:02<00:01,  9.47it/s]Epoch: 2, train for the 23-th batch, train loss: 0.6532241702079773:  14%|█▊           | 21/151 [00:03<00:18,  6.97it/s]Epoch: 2, train for the 23-th batch, train loss: 0.6532241702079773:  15%|█▉           | 23/151 [00:03<00:16,  7.83it/s]Epoch: 1, train for the 184-th batch, train loss: 0.5617883801460266:  77%|████████▍  | 183/237 [00:46<00:14,  3.86it/s]Epoch: 1, train for the 184-th batch, train loss: 0.5617883801460266:  78%|████████▌  | 184/237 [00:47<00:13,  4.05it/s]Epoch: 1, train for the 170-th batch, train loss: 0.354867160320282:  44%|█████▎      | 169/383 [00:45<00:57,  3.71it/s]Epoch: 2, train for the 89-th batch, train loss: 0.511876106262207:  60%|████████▍     | 88/146 [00:12<00:07,  7.36it/s]evaluate for the 28-th batch, evaluate loss: 0.4847092032432556:  65%|███████████▋      | 26/40 [00:02<00:01,  9.47it/s]evaluate for the 28-th batch, evaluate loss: 0.4847092032432556:  70%|████████████▌     | 28/40 [00:02<00:01, 10.18it/s]Epoch: 2, train for the 89-th batch, train loss: 0.511876106262207:  61%|████████▌     | 89/146 [00:12<00:08,  6.90it/s]Epoch: 1, train for the 170-th batch, train loss: 0.354867160320282:  44%|█████▎      | 170/383 [00:45<00:56,  3.79it/s]Epoch: 1, train for the 229-th batch, train loss: 0.5334540605545044:  95%|██████████▍| 228/241 [00:47<00:03,  4.27it/s]evaluate for the 29-th batch, evaluate loss: 0.5086449384689331:  70%|████████████▌     | 28/40 [00:02<00:01, 10.18it/s]Epoch: 2, train for the 24-th batch, train loss: 0.646979033946991:  15%|██▏           | 23/151 [00:03<00:16,  7.83it/s]Epoch: 1, train for the 229-th batch, train loss: 0.5334540605545044:  95%|██████████▍| 229/241 [00:47<00:02,  4.28it/s]Epoch: 2, train for the 24-th batch, train loss: 0.646979033946991:  16%|██▏           | 24/151 [00:03<00:17,  7.19it/s]Epoch: 2, train for the 90-th batch, train loss: 0.5383849143981934:  61%|███████▉     | 89/146 [00:12<00:08,  6.90it/s]Epoch: 2, train for the 90-th batch, train loss: 0.5383849143981934:  62%|████████     | 90/146 [00:12<00:07,  7.18it/s]Epoch: 1, train for the 171-th batch, train loss: 0.4780336320400238:  44%|████▉      | 170/383 [00:45<00:56,  3.79it/s]Epoch: 2, train for the 91-th batch, train loss: 0.5555600523948669:  62%|████████     | 90/146 [00:12<00:07,  7.18it/s]Epoch: 1, train for the 171-th batch, train loss: 0.4780336320400238:  45%|████▉      | 171/383 [00:45<00:52,  4.07it/s]Epoch: 2, train for the 91-th batch, train loss: 0.5555600523948669:  62%|████████     | 91/146 [00:12<00:07,  7.60it/s]Epoch: 1, train for the 230-th batch, train loss: 0.49976491928100586:  95%|█████████▌| 229/241 [00:47<00:02,  4.28it/s]evaluate for the 30-th batch, evaluate loss: 0.5119838714599609:  70%|████████████▌     | 28/40 [00:02<00:01, 10.18it/s]evaluate for the 30-th batch, evaluate loss: 0.5119838714599609:  75%|█████████████▌    | 30/40 [00:02<00:01,  8.71it/s]Epoch: 1, train for the 230-th batch, train loss: 0.49976491928100586:  95%|█████████▌| 230/241 [00:47<00:02,  4.36it/s]Epoch: 2, train for the 25-th batch, train loss: 0.6752147674560547:  16%|██           | 24/151 [00:03<00:17,  7.19it/s]Epoch: 2, train for the 25-th batch, train loss: 0.6752147674560547:  17%|██▏          | 25/151 [00:03<00:21,  5.85it/s]Epoch: 1, train for the 185-th batch, train loss: 0.5545803308486938:  78%|████████▌  | 184/237 [00:47<00:13,  4.05it/s]evaluate for the 31-th batch, evaluate loss: 0.4924524128437042:  75%|█████████████▌    | 30/40 [00:02<00:01,  8.71it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5208290815353394:  62%|████████     | 91/146 [00:12<00:07,  7.60it/s]Epoch: 1, train for the 185-th batch, train loss: 0.5545803308486938:  78%|████████▌  | 185/237 [00:47<00:15,  3.29it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5208290815353394:  63%|████████▏    | 92/146 [00:12<00:07,  7.04it/s]evaluate for the 32-th batch, evaluate loss: 0.523730456829071:  75%|██████████████▎    | 30/40 [00:02<00:01,  8.71it/s]evaluate for the 32-th batch, evaluate loss: 0.523730456829071:  80%|███████████████▏   | 32/40 [00:02<00:00,  9.90it/s]Epoch: 1, train for the 172-th batch, train loss: 0.3464149832725525:  45%|████▉      | 171/383 [00:45<00:52,  4.07it/s]Epoch: 1, train for the 231-th batch, train loss: 0.5492992401123047:  95%|██████████▍| 230/241 [00:47<00:02,  4.36it/s]Epoch: 2, train for the 26-th batch, train loss: 0.6558927893638611:  17%|██▏          | 25/151 [00:03<00:21,  5.85it/s]Epoch: 1, train for the 231-th batch, train loss: 0.5492992401123047:  96%|██████████▌| 231/241 [00:47<00:02,  4.59it/s]Epoch: 1, train for the 172-th batch, train loss: 0.3464149832725525:  45%|████▉      | 172/383 [00:45<00:53,  3.95it/s]Epoch: 2, train for the 26-th batch, train loss: 0.6558927893638611:  17%|██▏          | 26/151 [00:03<00:20,  6.03it/s]evaluate for the 33-th batch, evaluate loss: 0.5036048889160156:  80%|██████████████▍   | 32/40 [00:02<00:00,  9.90it/s]Epoch: 2, train for the 93-th batch, train loss: 0.5506223440170288:  63%|████████▏    | 92/146 [00:13<00:07,  7.04it/s]Epoch: 2, train for the 93-th batch, train loss: 0.5506223440170288:  64%|████████▎    | 93/146 [00:13<00:07,  6.96it/s]Epoch: 2, train for the 27-th batch, train loss: 0.6702093482017517:  17%|██▏          | 26/151 [00:03<00:20,  6.03it/s]Epoch: 2, train for the 27-th batch, train loss: 0.6702093482017517:  18%|██▎          | 27/151 [00:03<00:19,  6.43it/s]Epoch: 2, train for the 94-th batch, train loss: 0.5234575271606445:  64%|████████▎    | 93/146 [00:13<00:07,  6.96it/s]Epoch: 2, train for the 94-th batch, train loss: 0.5234575271606445:  64%|████████▎    | 94/146 [00:13<00:06,  7.48it/s]Epoch: 1, train for the 173-th batch, train loss: 0.46764224767684937:  45%|████▍     | 172/383 [00:45<00:53,  3.95it/s]Epoch: 1, train for the 173-th batch, train loss: 0.46764224767684937:  45%|████▌     | 173/383 [00:45<00:51,  4.09it/s]Epoch: 1, train for the 186-th batch, train loss: 0.5271430611610413:  78%|████████▌  | 185/237 [00:47<00:15,  3.29it/s]evaluate for the 34-th batch, evaluate loss: 0.5721049904823303:  80%|██████████████▍   | 32/40 [00:03<00:00,  9.90it/s]evaluate for the 34-th batch, evaluate loss: 0.5721049904823303:  85%|███████████████▎  | 34/40 [00:03<00:00,  8.28it/s]Epoch: 1, train for the 232-th batch, train loss: 0.5672383904457092:  96%|██████████▌| 231/241 [00:48<00:02,  4.59it/s]Epoch: 2, train for the 95-th batch, train loss: 0.519199788570404:  64%|█████████     | 94/146 [00:13<00:06,  7.48it/s]Epoch: 2, train for the 28-th batch, train loss: 0.6549530625343323:  18%|██▎          | 27/151 [00:04<00:19,  6.43it/s]Epoch: 2, train for the 95-th batch, train loss: 0.519199788570404:  65%|█████████     | 95/146 [00:13<00:06,  7.51it/s]Epoch: 1, train for the 186-th batch, train loss: 0.5271430611610413:  78%|████████▋  | 186/237 [00:47<00:16,  3.02it/s]Epoch: 2, train for the 28-th batch, train loss: 0.6549530625343323:  19%|██▍          | 28/151 [00:04<00:19,  6.27it/s]Epoch: 1, train for the 232-th batch, train loss: 0.5672383904457092:  96%|██████████▌| 232/241 [00:48<00:02,  4.06it/s]evaluate for the 35-th batch, evaluate loss: 0.5343067049980164:  85%|███████████████▎  | 34/40 [00:03<00:00,  8.28it/s]evaluate for the 36-th batch, evaluate loss: 0.5230780839920044:  85%|███████████████▎  | 34/40 [00:03<00:00,  8.28it/s]evaluate for the 36-th batch, evaluate loss: 0.5230780839920044:  90%|████████████████▏ | 36/40 [00:03<00:00,  9.57it/s]Epoch: 1, train for the 174-th batch, train loss: 0.4953295886516571:  45%|████▉      | 173/383 [00:46<00:51,  4.09it/s]Epoch: 2, train for the 96-th batch, train loss: 0.5578497052192688:  65%|████████▍    | 95/146 [00:13<00:06,  7.51it/s]Epoch: 2, train for the 96-th batch, train loss: 0.5578497052192688:  66%|████████▌    | 96/146 [00:13<00:07,  6.99it/s]evaluate for the 37-th batch, evaluate loss: 0.5517300963401794:  90%|████████████████▏ | 36/40 [00:03<00:00,  9.57it/s]Epoch: 2, train for the 29-th batch, train loss: 0.667041540145874:  19%|██▌           | 28/151 [00:04<00:19,  6.27it/s]Epoch: 1, train for the 174-th batch, train loss: 0.4953295886516571:  45%|████▉      | 174/383 [00:46<00:52,  4.00it/s]Epoch: 2, train for the 29-th batch, train loss: 0.667041540145874:  19%|██▋           | 29/151 [00:04<00:20,  5.93it/s]Epoch: 1, train for the 233-th batch, train loss: 0.5141889452934265:  96%|██████████▌| 232/241 [00:48<00:02,  4.06it/s]Epoch: 1, train for the 233-th batch, train loss: 0.5141889452934265:  97%|██████████▋| 233/241 [00:48<00:01,  4.18it/s]Epoch: 2, train for the 30-th batch, train loss: 0.6446875333786011:  19%|██▍          | 29/151 [00:04<00:20,  5.93it/s]Epoch: 2, train for the 30-th batch, train loss: 0.6446875333786011:  20%|██▌          | 30/151 [00:04<00:19,  6.37it/s]evaluate for the 38-th batch, evaluate loss: 0.5832729935646057:  90%|████████████████▏ | 36/40 [00:03<00:00,  9.57it/s]evaluate for the 38-th batch, evaluate loss: 0.5832729935646057:  95%|█████████████████ | 38/40 [00:03<00:00,  9.43it/s]Epoch: 1, train for the 234-th batch, train loss: 0.577099621295929:  97%|███████████▌| 233/241 [00:48<00:01,  4.18it/s]Epoch: 1, train for the 187-th batch, train loss: 0.5708672404289246:  78%|████████▋  | 186/237 [00:48<00:16,  3.02it/s]Epoch: 2, train for the 97-th batch, train loss: 0.5160313844680786:  66%|████████▌    | 96/146 [00:13<00:07,  6.99it/s]evaluate for the 39-th batch, evaluate loss: 0.5696980953216553:  95%|█████████████████ | 38/40 [00:03<00:00,  9.43it/s]Epoch: 1, train for the 175-th batch, train loss: 0.414377361536026:  45%|█████▍      | 174/383 [00:46<00:52,  4.00it/s]Epoch: 2, train for the 97-th batch, train loss: 0.5160313844680786:  66%|████████▋    | 97/146 [00:13<00:08,  5.75it/s]Epoch: 1, train for the 234-th batch, train loss: 0.577099621295929:  97%|███████████▋| 234/241 [00:48<00:01,  4.55it/s]Epoch: 1, train for the 187-th batch, train loss: 0.5708672404289246:  79%|████████▋  | 187/237 [00:48<00:18,  2.78it/s]Epoch: 2, train for the 31-th batch, train loss: 0.641277551651001:  20%|██▊           | 30/151 [00:04<00:19,  6.37it/s]Epoch: 1, train for the 175-th batch, train loss: 0.414377361536026:  46%|█████▍      | 175/383 [00:46<00:52,  3.99it/s]Epoch: 2, train for the 31-th batch, train loss: 0.641277551651001:  21%|██▊           | 31/151 [00:04<00:18,  6.65it/s]evaluate for the 40-th batch, evaluate loss: 0.6687790155410767:  95%|█████████████████ | 38/40 [00:03<00:00,  9.43it/s]evaluate for the 40-th batch, evaluate loss: 0.6687790155410767: 100%|██████████████████| 40/40 [00:03<00:00, 10.41it/s]evaluate for the 40-th batch, evaluate loss: 0.6687790155410767: 100%|██████████████████| 40/40 [00:03<00:00, 11.34it/s]
Epoch: 1, train for the 235-th batch, train loss: 0.6072200536727905:  97%|██████████▋| 234/241 [00:48<00:01,  4.55it/s]Epoch: 1, train for the 188-th batch, train loss: 0.5965394973754883:  79%|████████▋  | 187/237 [00:48<00:18,  2.78it/s]  0%|                                                                                            | 0/21 [00:00<?, ?it/s]Epoch: 2, train for the 98-th batch, train loss: 0.5058899521827698:  66%|████████▋    | 97/146 [00:13<00:08,  5.75it/s]Epoch: 2, train for the 98-th batch, train loss: 0.5058899521827698:  67%|████████▋    | 98/146 [00:13<00:08,  5.35it/s]Epoch: 2, train for the 32-th batch, train loss: 0.6720320582389832:  21%|██▋          | 31/151 [00:04<00:18,  6.65it/s]Epoch: 1, train for the 235-th batch, train loss: 0.6072200536727905:  98%|██████████▋| 235/241 [00:48<00:01,  4.49it/s]Epoch: 2, train for the 32-th batch, train loss: 0.6720320582389832:  21%|██▊          | 32/151 [00:04<00:19,  6.26it/s]Epoch: 1, train for the 188-th batch, train loss: 0.5965394973754883:  79%|████████▋  | 188/237 [00:48<00:15,  3.12it/s]evaluate for the 1-th batch, evaluate loss: 0.6462143063545227:   0%|                            | 0/21 [00:00<?, ?it/s]Epoch: 1, train for the 176-th batch, train loss: 0.4343128800392151:  46%|█████      | 175/383 [00:46<00:52,  3.99it/s]evaluate for the 2-th batch, evaluate loss: 0.7224066257476807:   0%|                            | 0/21 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.7224066257476807:  10%|█▉                  | 2/21 [00:00<00:01, 14.09it/s]Epoch: 1, train for the 176-th batch, train loss: 0.4343128800392151:  46%|█████      | 176/383 [00:46<00:56,  3.69it/s]Epoch: 2, train for the 33-th batch, train loss: 0.6425572037696838:  21%|██▊          | 32/151 [00:04<00:19,  6.26it/s]Epoch: 2, train for the 99-th batch, train loss: 0.5563065409660339:  67%|████████▋    | 98/146 [00:14<00:08,  5.35it/s]Epoch: 2, train for the 33-th batch, train loss: 0.6425572037696838:  22%|██▊          | 33/151 [00:04<00:18,  6.51it/s]Epoch: 2, train for the 99-th batch, train loss: 0.5563065409660339:  68%|████████▊    | 99/146 [00:14<00:08,  5.62it/s]evaluate for the 3-th batch, evaluate loss: 0.707548201084137:  10%|██                   | 2/21 [00:00<00:01, 14.09it/s]Epoch: 1, train for the 236-th batch, train loss: 0.5613763928413391:  98%|██████████▋| 235/241 [00:49<00:01,  4.49it/s]Epoch: 1, train for the 189-th batch, train loss: 0.5929443836212158:  79%|████████▋  | 188/237 [00:48<00:15,  3.12it/s]Epoch: 1, train for the 236-th batch, train loss: 0.5613763928413391:  98%|██████████▊| 236/241 [00:49<00:01,  4.55it/s]Epoch: 1, train for the 189-th batch, train loss: 0.5929443836212158:  80%|████████▊  | 189/237 [00:48<00:14,  3.37it/s]Epoch: 2, train for the 34-th batch, train loss: 0.6396344900131226:  22%|██▊          | 33/151 [00:04<00:18,  6.51it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5803607702255249:  68%|████████▏   | 99/146 [00:14<00:08,  5.62it/s]evaluate for the 4-th batch, evaluate loss: 0.6428334712982178:  10%|█▉                  | 2/21 [00:00<00:01, 14.09it/s]evaluate for the 4-th batch, evaluate loss: 0.6428334712982178:  19%|███▊                | 4/21 [00:00<00:01, 12.16it/s]Epoch: 2, train for the 34-th batch, train loss: 0.6396344900131226:  23%|██▉          | 34/151 [00:05<00:18,  6.42it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5803607702255249:  68%|███████▌   | 100/146 [00:14<00:08,  5.75it/s]Epoch: 1, train for the 237-th batch, train loss: 0.5224588513374329:  98%|██████████▊| 236/241 [00:49<00:01,  4.55it/s]evaluate for the 5-th batch, evaluate loss: 0.7243524193763733:  19%|███▊                | 4/21 [00:00<00:01, 12.16it/s]Epoch: 1, train for the 237-th batch, train loss: 0.5224588513374329:  98%|██████████▊| 237/241 [00:49<00:00,  4.84it/s]Epoch: 2, train for the 35-th batch, train loss: 0.635875940322876:  23%|███▏          | 34/151 [00:05<00:18,  6.42it/s]Epoch: 2, train for the 35-th batch, train loss: 0.635875940322876:  23%|███▏          | 35/151 [00:05<00:17,  6.65it/s]Epoch: 2, train for the 101-th batch, train loss: 0.49983251094818115:  68%|██████▊   | 100/146 [00:14<00:08,  5.75it/s]evaluate for the 6-th batch, evaluate loss: 0.7185804843902588:  19%|███▊                | 4/21 [00:00<00:01, 12.16it/s]evaluate for the 6-th batch, evaluate loss: 0.7185804843902588:  29%|█████▋              | 6/21 [00:00<00:01, 11.89it/s]Epoch: 2, train for the 101-th batch, train loss: 0.49983251094818115:  69%|██████▉   | 101/146 [00:14<00:07,  5.84it/s]Epoch: 1, train for the 190-th batch, train loss: 0.5351415872573853:  80%|████████▊  | 189/237 [00:48<00:14,  3.37it/s]Epoch: 1, train for the 177-th batch, train loss: 0.43046098947525024:  46%|████▌     | 176/383 [00:47<00:56,  3.69it/s]Epoch: 1, train for the 190-th batch, train loss: 0.5351415872573853:  80%|████████▊  | 190/237 [00:49<00:13,  3.46it/s]Epoch: 1, train for the 238-th batch, train loss: 0.537007212638855:  98%|███████████▊| 237/241 [00:49<00:00,  4.84it/s]evaluate for the 7-th batch, evaluate loss: 0.6662619113922119:  29%|█████▋              | 6/21 [00:00<00:01, 11.89it/s]Epoch: 1, train for the 238-th batch, train loss: 0.537007212638855:  99%|███████████▊| 238/241 [00:49<00:00,  5.18it/s]Epoch: 1, train for the 177-th batch, train loss: 0.43046098947525024:  46%|████▌     | 177/383 [00:47<01:05,  3.14it/s]Epoch: 2, train for the 36-th batch, train loss: 0.6490918397903442:  23%|███          | 35/151 [00:05<00:17,  6.65it/s]Epoch: 2, train for the 36-th batch, train loss: 0.6490918397903442:  24%|███          | 36/151 [00:05<00:16,  6.91it/s]Epoch: 2, train for the 102-th batch, train loss: 0.5378231406211853:  69%|███████▌   | 101/146 [00:14<00:07,  5.84it/s]Epoch: 2, train for the 102-th batch, train loss: 0.5378231406211853:  70%|███████▋   | 102/146 [00:14<00:07,  6.15it/s]evaluate for the 8-th batch, evaluate loss: 0.7062931060791016:  29%|█████▋              | 6/21 [00:00<00:01, 11.89it/s]evaluate for the 8-th batch, evaluate loss: 0.7062931060791016:  38%|███████▌            | 8/21 [00:00<00:01, 12.43it/s]evaluate for the 9-th batch, evaluate loss: 0.7086637616157532:  38%|███████▌            | 8/21 [00:00<00:01, 12.43it/s]Epoch: 1, train for the 239-th batch, train loss: 0.5802589058876038:  99%|██████████▊| 238/241 [00:49<00:00,  5.18it/s]Epoch: 1, train for the 239-th batch, train loss: 0.5802589058876038:  99%|██████████▉| 239/241 [00:49<00:00,  5.31it/s]Epoch: 2, train for the 37-th batch, train loss: 0.6634983420372009:  24%|███          | 36/151 [00:05<00:16,  6.91it/s]Epoch: 2, train for the 37-th batch, train loss: 0.6634983420372009:  25%|███▏         | 37/151 [00:05<00:17,  6.49it/s]Epoch: 1, train for the 191-th batch, train loss: 0.5496093034744263:  80%|████████▊  | 190/237 [00:49<00:13,  3.46it/s]Epoch: 2, train for the 103-th batch, train loss: 0.534458577632904:  70%|████████▍   | 102/146 [00:14<00:07,  6.15it/s]evaluate for the 10-th batch, evaluate loss: 0.718591570854187:  38%|███████▌            | 8/21 [00:00<00:01, 12.43it/s]evaluate for the 10-th batch, evaluate loss: 0.718591570854187:  48%|█████████          | 10/21 [00:00<00:00, 12.66it/s]Epoch: 2, train for the 103-th batch, train loss: 0.534458577632904:  71%|████████▍   | 103/146 [00:14<00:06,  6.20it/s]Epoch: 1, train for the 191-th batch, train loss: 0.5496093034744263:  81%|████████▊  | 191/237 [00:49<00:12,  3.55it/s]Epoch: 1, train for the 178-th batch, train loss: 0.5184969305992126:  46%|█████      | 177/383 [00:47<01:05,  3.14it/s]evaluate for the 11-th batch, evaluate loss: 0.7106266617774963:  48%|████████▌         | 10/21 [00:00<00:00, 12.66it/s]Epoch: 1, train for the 178-th batch, train loss: 0.5184969305992126:  46%|█████      | 178/383 [00:47<01:04,  3.19it/s]Epoch: 1, train for the 240-th batch, train loss: 0.47433122992515564:  99%|█████████▉| 239/241 [00:49<00:00,  5.31it/s]Epoch: 2, train for the 104-th batch, train loss: 0.5155928134918213:  71%|███████▊   | 103/146 [00:14<00:06,  6.20it/s]Epoch: 2, train for the 104-th batch, train loss: 0.5155928134918213:  71%|███████▊   | 104/146 [00:14<00:06,  6.46it/s]evaluate for the 12-th batch, evaluate loss: 0.691292405128479:  48%|█████████          | 10/21 [00:00<00:00, 12.66it/s]evaluate for the 12-th batch, evaluate loss: 0.691292405128479:  57%|██████████▊        | 12/21 [00:00<00:00, 13.13it/s]Epoch: 2, train for the 38-th batch, train loss: 0.6496607661247253:  25%|███▏         | 37/151 [00:05<00:17,  6.49it/s]Epoch: 1, train for the 240-th batch, train loss: 0.47433122992515564: 100%|█████████▉| 240/241 [00:49<00:00,  5.26it/s]Epoch: 2, train for the 38-th batch, train loss: 0.6496607661247253:  25%|███▎         | 38/151 [00:05<00:18,  6.11it/s]evaluate for the 13-th batch, evaluate loss: 0.6838787198066711:  57%|██████████▎       | 12/21 [00:01<00:00, 13.13it/s]Epoch: 1, train for the 179-th batch, train loss: 0.4268099069595337:  46%|█████      | 178/383 [00:47<01:04,  3.19it/s]Epoch: 1, train for the 179-th batch, train loss: 0.4268099069595337:  47%|█████▏     | 179/383 [00:47<00:57,  3.55it/s]Epoch: 2, train for the 39-th batch, train loss: 0.6258552670478821:  25%|███▎         | 38/151 [00:05<00:18,  6.11it/s]Epoch: 2, train for the 105-th batch, train loss: 0.5326889157295227:  71%|███████▊   | 104/146 [00:15<00:06,  6.46it/s]Epoch: 2, train for the 39-th batch, train loss: 0.6258552670478821:  26%|███▎         | 39/151 [00:05<00:18,  6.20it/s]evaluate for the 14-th batch, evaluate loss: 0.6630352735519409:  57%|██████████▎       | 12/21 [00:01<00:00, 13.13it/s]evaluate for the 14-th batch, evaluate loss: 0.6630352735519409:  67%|████████████      | 14/21 [00:01<00:00, 12.50it/s]Epoch: 2, train for the 105-th batch, train loss: 0.5326889157295227:  72%|███████▉   | 105/146 [00:15<00:06,  6.15it/s]Epoch: 1, train for the 192-th batch, train loss: 0.5771214365959167:  81%|████████▊  | 191/237 [00:49<00:12,  3.55it/s]Epoch: 1, train for the 192-th batch, train loss: 0.5771214365959167:  81%|████████▉  | 192/237 [00:49<00:13,  3.23it/s]evaluate for the 15-th batch, evaluate loss: 0.7029443383216858:  67%|████████████      | 14/21 [00:01<00:00, 12.50it/s]Epoch: 1, train for the 241-th batch, train loss: 0.5371871590614319: 100%|██████████▉| 240/241 [00:50<00:00,  5.26it/s]Epoch: 2, train for the 106-th batch, train loss: 0.5153672695159912:  72%|███████▉   | 105/146 [00:15<00:06,  6.15it/s]Epoch: 2, train for the 40-th batch, train loss: 0.6277545690536499:  26%|███▎         | 39/151 [00:05<00:18,  6.20it/s]evaluate for the 16-th batch, evaluate loss: 0.6830068230628967:  67%|████████████      | 14/21 [00:01<00:00, 12.50it/s]evaluate for the 16-th batch, evaluate loss: 0.6830068230628967:  76%|█████████████▋    | 16/21 [00:01<00:00, 12.30it/s]Epoch: 1, train for the 241-th batch, train loss: 0.5371871590614319: 100%|███████████| 241/241 [00:50<00:00,  4.26it/s]Epoch: 1, train for the 241-th batch, train loss: 0.5371871590614319: 100%|███████████| 241/241 [00:50<00:00,  4.80it/s]
Epoch: 1, train for the 180-th batch, train loss: 0.5059813261032104:  47%|█████▏     | 179/383 [00:47<00:57,  3.55it/s]Epoch: 2, train for the 40-th batch, train loss: 0.6277545690536499:  26%|███▍         | 40/151 [00:05<00:18,  5.98it/s]Epoch: 2, train for the 106-th batch, train loss: 0.5153672695159912:  73%|███████▉   | 106/146 [00:15<00:06,  6.00it/s]Epoch: 1, train for the 193-th batch, train loss: 0.5822269320487976:  81%|████████▉  | 192/237 [00:49<00:13,  3.23it/s]Epoch: 1, train for the 180-th batch, train loss: 0.5059813261032104:  47%|█████▏     | 180/383 [00:47<00:55,  3.66it/s]evaluate for the 17-th batch, evaluate loss: 0.6183031797409058:  76%|█████████████▋    | 16/21 [00:01<00:00, 12.30it/s]Epoch: 1, train for the 193-th batch, train loss: 0.5822269320487976:  81%|████████▉  | 193/237 [00:49<00:11,  3.69it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5556888580322266:  73%|███████▉   | 106/146 [00:15<00:06,  6.00it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5556888580322266:  73%|████████   | 107/146 [00:15<00:06,  6.41it/s]Epoch: 2, train for the 41-th batch, train loss: 0.6355079412460327:  26%|███▍         | 40/151 [00:06<00:18,  5.98it/s]Epoch: 2, train for the 41-th batch, train loss: 0.6355079412460327:  27%|███▌         | 41/151 [00:06<00:18,  5.89it/s]Epoch: 2, train for the 108-th batch, train loss: 0.523688793182373:  73%|████████▊   | 107/146 [00:15<00:06,  6.41it/s]Epoch: 2, train for the 108-th batch, train loss: 0.523688793182373:  74%|████████▉   | 108/146 [00:15<00:05,  7.15it/s]Epoch: 1, train for the 181-th batch, train loss: 0.416147917509079:  47%|█████▋      | 180/383 [00:48<00:55,  3.66it/s]evaluate for the 18-th batch, evaluate loss: 0.6650334000587463:  76%|█████████████▋    | 16/21 [00:01<00:00, 12.30it/s]evaluate for the 18-th batch, evaluate loss: 0.6650334000587463:  86%|███████████████▍  | 18/21 [00:01<00:00, 10.12it/s]Epoch: 2, train for the 42-th batch, train loss: 0.6118024587631226:  27%|███▌         | 41/151 [00:06<00:18,  5.89it/s]Epoch: 2, train for the 42-th batch, train loss: 0.6118024587631226:  28%|███▌         | 42/151 [00:06<00:16,  6.69it/s]Epoch: 1, train for the 181-th batch, train loss: 0.416147917509079:  47%|█████▋      | 181/383 [00:48<00:52,  3.82it/s]Epoch: 2, train for the 109-th batch, train loss: 0.5734155178070068:  74%|████████▏  | 108/146 [00:15<00:05,  7.15it/s]evaluate for the 19-th batch, evaluate loss: 0.6904818415641785:  86%|███████████████▍  | 18/21 [00:01<00:00, 10.12it/s]Epoch: 2, train for the 109-th batch, train loss: 0.5734155178070068:  75%|████████▏  | 109/146 [00:15<00:05,  7.16it/s]Epoch: 1, train for the 194-th batch, train loss: 0.5546745657920837:  81%|████████▉  | 193/237 [00:50<00:11,  3.69it/s]Epoch: 2, train for the 43-th batch, train loss: 0.644077718257904:  28%|███▉          | 42/151 [00:06<00:16,  6.69it/s]  0%|                                                                                            | 0/72 [00:00<?, ?it/s]Epoch: 2, train for the 43-th batch, train loss: 0.644077718257904:  28%|███▉          | 43/151 [00:06<00:15,  6.76it/s]Epoch: 1, train for the 194-th batch, train loss: 0.5546745657920837:  82%|█████████  | 194/237 [00:50<00:12,  3.38it/s]evaluate for the 20-th batch, evaluate loss: 0.691872239112854:  86%|████████████████▎  | 18/21 [00:01<00:00, 10.12it/s]evaluate for the 20-th batch, evaluate loss: 0.691872239112854:  95%|██████████████████ | 20/21 [00:01<00:00, 10.62it/s]Epoch: 1, train for the 182-th batch, train loss: 0.5250232815742493:  47%|█████▏     | 181/383 [00:48<00:52,  3.82it/s]evaluate for the 1-th batch, evaluate loss: 0.5283472537994385:   0%|                            | 0/72 [00:00<?, ?it/s]Epoch: 2, train for the 110-th batch, train loss: 0.4761500358581543:  75%|████████▏  | 109/146 [00:15<00:05,  7.16it/s]evaluate for the 21-th batch, evaluate loss: 0.8491703867912292:  95%|█████████████████▏| 20/21 [00:01<00:00, 10.62it/s]evaluate for the 21-th batch, evaluate loss: 0.8491703867912292: 100%|██████████████████| 21/21 [00:01<00:00, 11.67it/s]
Epoch: 2, train for the 110-th batch, train loss: 0.4761500358581543:  75%|████████▎  | 110/146 [00:15<00:05,  7.16it/s]Epoch: 1, train for the 182-th batch, train loss: 0.5250232815742493:  48%|█████▏     | 182/383 [00:48<00:52,  3.80it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5823308229446411:  28%|███▋         | 43/151 [00:06<00:15,  6.76it/s]evaluate for the 2-th batch, evaluate loss: 0.5307163000106812:   0%|                            | 0/72 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5307163000106812:   3%|▌                   | 2/72 [00:00<00:05, 12.30it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5823308229446411:  29%|███▊         | 44/151 [00:06<00:16,  6.49it/s]evaluate for the 3-th batch, evaluate loss: 0.490312784910202:   3%|▌                    | 2/72 [00:00<00:05, 12.30it/s]INFO:root:Epoch: 2, learning rate: 0.0001, train loss: 0.5208
INFO:root:train average_precision, 0.8305
INFO:root:train roc_auc, 0.8234
INFO:root:validate loss: 0.5286
INFO:root:validate average_precision, 0.8329
INFO:root:validate roc_auc, 0.8208
INFO:root:new node validate loss: 0.6958
INFO:root:new node validate first_1_average_precision, 0.7068
INFO:root:new node validate first_1_roc_auc, 0.6877
INFO:root:new node validate first_3_average_precision, 0.7163
INFO:root:new node validate first_3_roc_auc, 0.7062
INFO:root:new node validate first_10_average_precision, 0.7105
INFO:root:new node validate first_10_roc_auc, 0.7108
INFO:root:new node validate average_precision, 0.6801
INFO:root:new node validate roc_auc, 0.6859
INFO:root:save model ./saved_models/TGN/ia-reality-call/TGN_seed0_tgn-ia-reality-call-lincorrect-time-linear/TGN_seed0_tgn-ia-reality-call-lincorrect-time-linear.pkl
Epoch: 2, train for the 111-th batch, train loss: 0.5058441162109375:  75%|████████▎  | 110/146 [00:15<00:05,  7.16it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5058441162109375:  76%|████████▎  | 111/146 [00:15<00:05,  6.75it/s]Epoch: 1, train for the 183-th batch, train loss: 0.4140101969242096:  48%|█████▏     | 182/383 [00:48<00:52,  3.80it/s]Epoch: 2, train for the 45-th batch, train loss: 0.6002478003501892:  29%|███▊         | 44/151 [00:06<00:16,  6.49it/s]evaluate for the 4-th batch, evaluate loss: 0.5527970790863037:   3%|▌                   | 2/72 [00:00<00:05, 12.30it/s]evaluate for the 4-th batch, evaluate loss: 0.5527970790863037:   6%|█                   | 4/72 [00:00<00:05, 12.79it/s]Epoch: 2, train for the 45-th batch, train loss: 0.6002478003501892:  30%|███▊         | 45/151 [00:06<00:16,  6.53it/s]Epoch: 1, train for the 183-th batch, train loss: 0.4140101969242096:  48%|█████▎     | 183/383 [00:48<00:49,  4.07it/s]Epoch: 2, train for the 112-th batch, train loss: 0.5537280440330505:  76%|████████▎  | 111/146 [00:16<00:05,  6.75it/s]Epoch: 2, train for the 112-th batch, train loss: 0.5537280440330505:  77%|████████▍  | 112/146 [00:16<00:04,  7.24it/s]Epoch: 1, train for the 195-th batch, train loss: 0.5538004040718079:  82%|█████████  | 194/237 [00:50<00:12,  3.38it/s]Epoch: 2, train for the 46-th batch, train loss: 0.6094002723693848:  30%|███▊         | 45/151 [00:06<00:16,  6.53it/s]Epoch: 2, train for the 46-th batch, train loss: 0.6094002723693848:  30%|███▉         | 46/151 [00:06<00:14,  7.17it/s]Epoch: 1, train for the 195-th batch, train loss: 0.5538004040718079:  82%|█████████  | 195/237 [00:50<00:14,  2.98it/s]evaluate for the 5-th batch, evaluate loss: 0.4906710088253021:   6%|█                   | 4/72 [00:00<00:05, 12.79it/s]Epoch: 2, train for the 113-th batch, train loss: 0.4903824031352997:  77%|████████▍  | 112/146 [00:16<00:04,  7.24it/s]Epoch: 2, train for the 113-th batch, train loss: 0.4903824031352997:  77%|████████▌  | 113/146 [00:16<00:04,  7.85it/s]Epoch: 1, train for the 184-th batch, train loss: 0.42520540952682495:  48%|████▊     | 183/383 [00:48<00:49,  4.07it/s]evaluate for the 6-th batch, evaluate loss: 0.5053858160972595:   6%|█                   | 4/72 [00:00<00:05, 12.79it/s]evaluate for the 6-th batch, evaluate loss: 0.5053858160972595:   8%|█▋                  | 6/72 [00:00<00:06, 10.36it/s]Epoch: 1, train for the 184-th batch, train loss: 0.42520540952682495:  48%|████▊     | 184/383 [00:48<00:48,  4.09it/s]Epoch: 2, train for the 114-th batch, train loss: 0.5112654566764832:  77%|████████▌  | 113/146 [00:16<00:04,  7.85it/s]Epoch: 2, train for the 114-th batch, train loss: 0.5112654566764832:  78%|████████▌  | 114/146 [00:16<00:03,  8.25it/s]Epoch: 2, train for the 47-th batch, train loss: 0.6149533987045288:  30%|███▉         | 46/151 [00:06<00:14,  7.17it/s]evaluate for the 7-th batch, evaluate loss: 0.556420087814331:   8%|█▊                   | 6/72 [00:00<00:06, 10.36it/s]Epoch: 2, train for the 47-th batch, train loss: 0.6149533987045288:  31%|████         | 47/151 [00:06<00:15,  6.67it/s]Epoch: 1, train for the 196-th batch, train loss: 0.5610881447792053:  82%|█████████  | 195/237 [00:50<00:14,  2.98it/s]Epoch: 1, train for the 196-th batch, train loss: 0.5610881447792053:  83%|█████████  | 196/237 [00:50<00:12,  3.27it/s]Epoch: 2, train for the 115-th batch, train loss: 0.5379375219345093:  78%|████████▌  | 114/146 [00:16<00:03,  8.25it/s]Epoch: 2, train for the 115-th batch, train loss: 0.5379375219345093:  79%|████████▋  | 115/146 [00:16<00:03,  8.10it/s]Epoch: 2, train for the 48-th batch, train loss: 0.5720542669296265:  31%|████         | 47/151 [00:07<00:15,  6.67it/s]Epoch: 2, train for the 48-th batch, train loss: 0.5720542669296265:  32%|████▏        | 48/151 [00:07<00:14,  6.94it/s]Epoch: 1, train for the 185-th batch, train loss: 0.31399616599082947:  48%|████▊     | 184/383 [00:49<00:48,  4.09it/s]Epoch: 2, train for the 116-th batch, train loss: 0.5261589884757996:  79%|████████▋  | 115/146 [00:16<00:03,  8.10it/s]Epoch: 2, train for the 116-th batch, train loss: 0.5261589884757996:  79%|████████▋  | 116/146 [00:16<00:03,  8.46it/s]Epoch: 1, train for the 185-th batch, train loss: 0.31399616599082947:  48%|████▊     | 185/383 [00:49<00:49,  3.97it/s]evaluate for the 8-th batch, evaluate loss: 0.5502867102622986:   8%|█▋                  | 6/72 [00:00<00:06, 10.36it/s]evaluate for the 8-th batch, evaluate loss: 0.5502867102622986:  11%|██▏                 | 8/72 [00:00<00:07,  8.54it/s]Epoch: 1, train for the 197-th batch, train loss: 0.5298115611076355:  83%|█████████  | 196/237 [00:51<00:12,  3.27it/s]evaluate for the 9-th batch, evaluate loss: 0.5111753940582275:  11%|██▏                 | 8/72 [00:00<00:07,  8.54it/s]Epoch: 1, train for the 197-th batch, train loss: 0.5298115611076355:  83%|█████████▏ | 197/237 [00:51<00:11,  3.49it/s]Epoch: 2, train for the 117-th batch, train loss: 0.5722150802612305:  79%|████████▋  | 116/146 [00:16<00:03,  8.46it/s]Epoch: 2, train for the 117-th batch, train loss: 0.5722150802612305:  80%|████████▊  | 117/146 [00:16<00:03,  8.23it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5963925123214722:  32%|████▏        | 48/151 [00:07<00:14,  6.94it/s]evaluate for the 10-th batch, evaluate loss: 0.5026861429214478:  11%|██                 | 8/72 [00:00<00:07,  8.54it/s]evaluate for the 10-th batch, evaluate loss: 0.5026861429214478:  14%|██▌               | 10/72 [00:00<00:05, 10.35it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5963925123214722:  32%|████▏        | 49/151 [00:07<00:17,  5.71it/s]evaluate for the 11-th batch, evaluate loss: 0.5178644061088562:  14%|██▌               | 10/72 [00:01<00:05, 10.35it/s]Epoch: 3, train for the 1-th batch, train loss: 0.6629377603530884:   0%|                       | 0/119 [00:00<?, ?it/s]Epoch: 1, train for the 186-th batch, train loss: 0.40871796011924744:  48%|████▊     | 185/383 [00:49<00:49,  3.97it/s]Epoch: 3, train for the 1-th batch, train loss: 0.6629377603530884:   1%|▏              | 1/119 [00:00<00:16,  7.08it/s]evaluate for the 12-th batch, evaluate loss: 0.5010867714881897:  14%|██▌               | 10/72 [00:01<00:05, 10.35it/s]evaluate for the 12-th batch, evaluate loss: 0.5010867714881897:  17%|███               | 12/72 [00:01<00:05, 11.54it/s]Epoch: 2, train for the 118-th batch, train loss: 0.5496639013290405:  80%|████████▊  | 117/146 [00:16<00:03,  8.23it/s]Epoch: 1, train for the 186-th batch, train loss: 0.40871796011924744:  49%|████▊     | 186/383 [00:49<00:52,  3.73it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5728152394294739:  32%|████▏        | 49/151 [00:07<00:17,  5.71it/s]Epoch: 2, train for the 118-th batch, train loss: 0.5496639013290405:  81%|████████▉  | 118/146 [00:16<00:04,  6.83it/s]evaluate for the 13-th batch, evaluate loss: 0.45294398069381714:  17%|██▊              | 12/72 [00:01<00:05, 11.54it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5728152394294739:  33%|████▎        | 50/151 [00:07<00:18,  5.60it/s]evaluate for the 14-th batch, evaluate loss: 0.42200374603271484:  17%|██▊              | 12/72 [00:01<00:05, 11.54it/s]evaluate for the 14-th batch, evaluate loss: 0.42200374603271484:  19%|███▎             | 14/72 [00:01<00:04, 12.88it/s]Epoch: 2, train for the 119-th batch, train loss: 0.507504403591156:  81%|█████████▋  | 118/146 [00:16<00:04,  6.83it/s]Epoch: 2, train for the 119-th batch, train loss: 0.507504403591156:  82%|█████████▊  | 119/146 [00:16<00:03,  7.29it/s]Epoch: 3, train for the 2-th batch, train loss: 0.8083557486534119:   1%|▏              | 1/119 [00:00<00:16,  7.08it/s]Epoch: 3, train for the 2-th batch, train loss: 0.8083557486534119:   2%|▎              | 2/119 [00:00<00:22,  5.31it/s]evaluate for the 15-th batch, evaluate loss: 0.46153444051742554:  19%|███▎             | 14/72 [00:01<00:04, 12.88it/s]Epoch: 1, train for the 198-th batch, train loss: 0.5191940665245056:  83%|█████████▏ | 197/237 [00:51<00:11,  3.49it/s]evaluate for the 16-th batch, evaluate loss: 0.4812317192554474:  19%|███▌              | 14/72 [00:01<00:04, 12.88it/s]evaluate for the 16-th batch, evaluate loss: 0.4812317192554474:  22%|████              | 16/72 [00:01<00:04, 12.78it/s]Epoch: 2, train for the 51-th batch, train loss: 0.6179882884025574:  33%|████▎        | 50/151 [00:07<00:18,  5.60it/s]Epoch: 1, train for the 198-th batch, train loss: 0.5191940665245056:  84%|█████████▏ | 198/237 [00:51<00:13,  2.93it/s]Epoch: 2, train for the 51-th batch, train loss: 0.6179882884025574:  34%|████▍        | 51/151 [00:07<00:19,  5.15it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5130597949028015:  82%|████████▉  | 119/146 [00:17<00:03,  7.29it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5130597949028015:  82%|█████████  | 120/146 [00:17<00:03,  7.05it/s]Epoch: 1, train for the 187-th batch, train loss: 0.35348930954933167:  49%|████▊     | 186/383 [00:49<00:52,  3.73it/s]Epoch: 3, train for the 3-th batch, train loss: 0.8496397137641907:   2%|▎              | 2/119 [00:00<00:22,  5.31it/s]Epoch: 3, train for the 3-th batch, train loss: 0.8496397137641907:   3%|▍              | 3/119 [00:00<00:18,  6.39it/s]evaluate for the 17-th batch, evaluate loss: 0.4420366585254669:  22%|████              | 16/72 [00:01<00:04, 12.78it/s]Epoch: 1, train for the 187-th batch, train loss: 0.35348930954933167:  49%|████▉     | 187/383 [00:49<00:56,  3.49it/s]Epoch: 2, train for the 52-th batch, train loss: 0.6304534077644348:  34%|████▍        | 51/151 [00:07<00:19,  5.15it/s]evaluate for the 18-th batch, evaluate loss: 0.4277898371219635:  22%|████              | 16/72 [00:01<00:04, 12.78it/s]evaluate for the 18-th batch, evaluate loss: 0.4277898371219635:  25%|████▌             | 18/72 [00:01<00:04, 12.63it/s]Epoch: 2, train for the 52-th batch, train loss: 0.6304534077644348:  34%|████▍        | 52/151 [00:07<00:17,  5.51it/s]Epoch: 1, train for the 199-th batch, train loss: 0.5209378004074097:  84%|█████████▏ | 198/237 [00:51<00:13,  2.93it/s]Epoch: 2, train for the 121-th batch, train loss: 0.5382938981056213:  82%|█████████  | 120/146 [00:17<00:03,  7.05it/s]evaluate for the 19-th batch, evaluate loss: 0.47352004051208496:  25%|████▎            | 18/72 [00:01<00:04, 12.63it/s]Epoch: 3, train for the 4-th batch, train loss: 0.7766056656837463:   3%|▍              | 3/119 [00:00<00:18,  6.39it/s]Epoch: 2, train for the 121-th batch, train loss: 0.5382938981056213:  83%|█████████  | 121/146 [00:17<00:03,  6.30it/s]Epoch: 3, train for the 4-th batch, train loss: 0.7766056656837463:   3%|▌              | 4/119 [00:00<00:19,  5.95it/s]Epoch: 1, train for the 199-th batch, train loss: 0.5209378004074097:  84%|█████████▏ | 199/237 [00:51<00:11,  3.20it/s]Epoch: 1, train for the 188-th batch, train loss: 0.4863692820072174:  49%|█████▎     | 187/383 [00:49<00:56,  3.49it/s]evaluate for the 20-th batch, evaluate loss: 0.5067405104637146:  25%|████▌             | 18/72 [00:01<00:04, 12.63it/s]evaluate for the 20-th batch, evaluate loss: 0.5067405104637146:  28%|█████             | 20/72 [00:01<00:03, 13.46it/s]Epoch: 1, train for the 188-th batch, train loss: 0.4863692820072174:  49%|█████▍     | 188/383 [00:50<00:52,  3.70it/s]Epoch: 2, train for the 53-th batch, train loss: 0.6121940612792969:  34%|████▍        | 52/151 [00:08<00:17,  5.51it/s]Epoch: 2, train for the 122-th batch, train loss: 0.5479132533073425:  83%|█████████  | 121/146 [00:17<00:03,  6.30it/s]Epoch: 2, train for the 122-th batch, train loss: 0.5479132533073425:  84%|█████████▏ | 122/146 [00:17<00:03,  6.24it/s]Epoch: 2, train for the 53-th batch, train loss: 0.6121940612792969:  35%|████▌        | 53/151 [00:08<00:19,  5.04it/s]Epoch: 3, train for the 5-th batch, train loss: 0.637365460395813:   3%|▌               | 4/119 [00:00<00:19,  5.95it/s]Epoch: 3, train for the 5-th batch, train loss: 0.637365460395813:   4%|▋               | 5/119 [00:00<00:19,  5.81it/s]evaluate for the 21-th batch, evaluate loss: 0.5243375897407532:  28%|█████             | 20/72 [00:01<00:03, 13.46it/s]Epoch: 1, train for the 189-th batch, train loss: 0.4685613214969635:  49%|█████▍     | 188/383 [00:50<00:52,  3.70it/s]Epoch: 2, train for the 123-th batch, train loss: 0.5700250864028931:  84%|█████████▏ | 122/146 [00:17<00:03,  6.24it/s]Epoch: 2, train for the 123-th batch, train loss: 0.5700250864028931:  84%|█████████▎ | 123/146 [00:17<00:03,  6.24it/s]Epoch: 3, train for the 6-th batch, train loss: 0.5420011281967163:   4%|▋              | 5/119 [00:00<00:19,  5.81it/s]Epoch: 2, train for the 54-th batch, train loss: 0.5817694664001465:  35%|████▌        | 53/151 [00:08<00:19,  5.04it/s]Epoch: 3, train for the 6-th batch, train loss: 0.5420011281967163:   5%|▊              | 6/119 [00:01<00:18,  5.95it/s]evaluate for the 22-th batch, evaluate loss: 0.4664340019226074:  28%|█████             | 20/72 [00:01<00:03, 13.46it/s]evaluate for the 22-th batch, evaluate loss: 0.4664340019226074:  31%|█████▌            | 22/72 [00:01<00:04, 10.33it/s]Epoch: 1, train for the 189-th batch, train loss: 0.4685613214969635:  49%|█████▍     | 189/383 [00:50<00:51,  3.74it/s]Epoch: 2, train for the 54-th batch, train loss: 0.5817694664001465:  36%|████▋        | 54/151 [00:08<00:18,  5.12it/s]Epoch: 1, train for the 200-th batch, train loss: 0.5619010329246521:  84%|█████████▏ | 199/237 [00:52<00:11,  3.20it/s]evaluate for the 23-th batch, evaluate loss: 0.4764538109302521:  31%|█████▌            | 22/72 [00:02<00:04, 10.33it/s]evaluate for the 24-th batch, evaluate loss: 0.4713389575481415:  31%|█████▌            | 22/72 [00:02<00:04, 10.33it/s]evaluate for the 24-th batch, evaluate loss: 0.4713389575481415:  33%|██████            | 24/72 [00:02<00:03, 12.02it/s]Epoch: 1, train for the 200-th batch, train loss: 0.5619010329246521:  84%|█████████▎ | 200/237 [00:52<00:12,  2.85it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5659384727478027:  36%|████▋        | 54/151 [00:08<00:18,  5.12it/s]Epoch: 2, train for the 124-th batch, train loss: 0.5514225363731384:  84%|█████████▎ | 123/146 [00:17<00:03,  6.24it/s]Epoch: 3, train for the 7-th batch, train loss: 0.5006040334701538:   5%|▊              | 6/119 [00:01<00:18,  5.95it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5659384727478027:  36%|████▋        | 55/151 [00:08<00:17,  5.50it/s]Epoch: 3, train for the 7-th batch, train loss: 0.5006040334701538:   6%|▉              | 7/119 [00:01<00:18,  5.93it/s]Epoch: 2, train for the 124-th batch, train loss: 0.5514225363731384:  85%|█████████▎ | 124/146 [00:17<00:03,  5.91it/s]Epoch: 1, train for the 190-th batch, train loss: 0.5035965442657471:  49%|█████▍     | 189/383 [00:50<00:51,  3.74it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5278415083885193:  36%|████▋        | 55/151 [00:08<00:17,  5.50it/s]Epoch: 1, train for the 190-th batch, train loss: 0.5035965442657471:  50%|█████▍     | 190/383 [00:50<00:52,  3.68it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5278415083885193:  37%|████▊        | 56/151 [00:08<00:16,  5.92it/s]Epoch: 3, train for the 8-th batch, train loss: 0.4669640362262726:   6%|▉              | 7/119 [00:01<00:18,  5.93it/s]Epoch: 3, train for the 8-th batch, train loss: 0.4669640362262726:   7%|█              | 8/119 [00:01<00:17,  6.21it/s]evaluate for the 25-th batch, evaluate loss: 0.5869686603546143:  33%|██████            | 24/72 [00:02<00:03, 12.02it/s]Epoch: 2, train for the 125-th batch, train loss: 0.5236768126487732:  85%|█████████▎ | 124/146 [00:17<00:03,  5.91it/s]Epoch: 2, train for the 125-th batch, train loss: 0.5236768126487732:  86%|█████████▍ | 125/146 [00:17<00:03,  5.77it/s]evaluate for the 26-th batch, evaluate loss: 0.42798933386802673:  33%|█████▋           | 24/72 [00:02<00:03, 12.02it/s]evaluate for the 26-th batch, evaluate loss: 0.42798933386802673:  36%|██████▏          | 26/72 [00:02<00:04,  9.60it/s]Epoch: 3, train for the 9-th batch, train loss: 0.49613329768180847:   7%|▉             | 8/119 [00:01<00:17,  6.21it/s]Epoch: 3, train for the 9-th batch, train loss: 0.49613329768180847:   8%|█             | 9/119 [00:01<00:15,  6.88it/s]Epoch: 1, train for the 201-th batch, train loss: 0.5471658110618591:  84%|█████████▎ | 200/237 [00:52<00:12,  2.85it/s]Epoch: 1, train for the 191-th batch, train loss: 0.41240569949150085:  50%|████▉     | 190/383 [00:50<00:52,  3.68it/s]Epoch: 1, train for the 201-th batch, train loss: 0.5471658110618591:  85%|█████████▎ | 201/237 [00:52<00:12,  2.80it/s]Epoch: 2, train for the 57-th batch, train loss: 0.5642173886299133:  37%|████▊        | 56/151 [00:08<00:16,  5.92it/s]evaluate for the 27-th batch, evaluate loss: 0.4035443365573883:  36%|██████▌           | 26/72 [00:02<00:04,  9.60it/s]Epoch: 2, train for the 126-th batch, train loss: 0.5618872046470642:  86%|█████████▍ | 125/146 [00:18<00:03,  5.77it/s]Epoch: 2, train for the 57-th batch, train loss: 0.5642173886299133:  38%|████▉        | 57/151 [00:08<00:16,  5.55it/s]Epoch: 2, train for the 126-th batch, train loss: 0.5618872046470642:  86%|█████████▍ | 126/146 [00:18<00:03,  5.83it/s]Epoch: 1, train for the 191-th batch, train loss: 0.41240569949150085:  50%|████▉     | 191/383 [00:50<00:50,  3.83it/s]Epoch: 3, train for the 10-th batch, train loss: 0.5043856501579285:   8%|█             | 9/119 [00:01<00:15,  6.88it/s]evaluate for the 28-th batch, evaluate loss: 0.4376388490200043:  36%|██████▌           | 26/72 [00:02<00:04,  9.60it/s]evaluate for the 28-th batch, evaluate loss: 0.4376388490200043:  39%|███████           | 28/72 [00:02<00:04, 10.81it/s]Epoch: 3, train for the 10-th batch, train loss: 0.5043856501579285:   8%|█            | 10/119 [00:01<00:15,  7.26it/s]evaluate for the 29-th batch, evaluate loss: 0.5031639933586121:  39%|███████           | 28/72 [00:02<00:04, 10.81it/s]Epoch: 2, train for the 127-th batch, train loss: 0.5100157856941223:  86%|█████████▍ | 126/146 [00:18<00:03,  5.83it/s]Epoch: 2, train for the 58-th batch, train loss: 0.5459532141685486:  38%|████▉        | 57/151 [00:09<00:16,  5.55it/s]Epoch: 2, train for the 127-th batch, train loss: 0.5100157856941223:  87%|█████████▌ | 127/146 [00:18<00:03,  5.99it/s]Epoch: 2, train for the 58-th batch, train loss: 0.5459532141685486:  38%|████▉        | 58/151 [00:09<00:16,  5.62it/s]Epoch: 3, train for the 11-th batch, train loss: 0.5052957534790039:   8%|█            | 10/119 [00:01<00:15,  7.26it/s]Epoch: 3, train for the 11-th batch, train loss: 0.5052957534790039:   9%|█▏           | 11/119 [00:01<00:15,  6.93it/s]Epoch: 2, train for the 59-th batch, train loss: 0.5512830018997192:  38%|████▉        | 58/151 [00:09<00:16,  5.62it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5411290526390076:  87%|█████████▌ | 127/146 [00:18<00:03,  5.99it/s]evaluate for the 30-th batch, evaluate loss: 0.4857739210128784:  39%|███████           | 28/72 [00:02<00:04, 10.81it/s]evaluate for the 30-th batch, evaluate loss: 0.4857739210128784:  42%|███████▌          | 30/72 [00:02<00:04,  9.34it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5411290526390076:  88%|█████████▋ | 128/146 [00:18<00:02,  6.15it/s]Epoch: 2, train for the 59-th batch, train loss: 0.5512830018997192:  39%|█████        | 59/151 [00:09<00:15,  5.91it/s]Epoch: 3, train for the 12-th batch, train loss: 0.5006164908409119:   9%|█▏           | 11/119 [00:01<00:15,  6.93it/s]Epoch: 3, train for the 12-th batch, train loss: 0.5006164908409119:  10%|█▎           | 12/119 [00:01<00:15,  6.95it/s]evaluate for the 31-th batch, evaluate loss: 0.5234043598175049:  42%|███████▌          | 30/72 [00:02<00:04,  9.34it/s]Epoch: 1, train for the 202-th batch, train loss: 0.5567277669906616:  85%|█████████▎ | 201/237 [00:53<00:12,  2.80it/s]Epoch: 1, train for the 192-th batch, train loss: 0.5118545293807983:  50%|█████▍     | 191/383 [00:51<00:50,  3.83it/s]Epoch: 1, train for the 202-th batch, train loss: 0.5567277669906616:  85%|█████████▍ | 202/237 [00:53<00:14,  2.50it/s]evaluate for the 32-th batch, evaluate loss: 0.5684638023376465:  42%|███████▌          | 30/72 [00:02<00:04,  9.34it/s]evaluate for the 32-th batch, evaluate loss: 0.5684638023376465:  44%|████████          | 32/72 [00:02<00:03, 10.01it/s]Epoch: 1, train for the 192-th batch, train loss: 0.5118545293807983:  50%|█████▌     | 192/383 [00:51<01:01,  3.11it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5452784895896912:  39%|█████        | 59/151 [00:09<00:15,  5.91it/s]Epoch: 3, train for the 13-th batch, train loss: 0.4262200891971588:  10%|█▎           | 12/119 [00:02<00:15,  6.95it/s]Epoch: 3, train for the 13-th batch, train loss: 0.4262200891971588:  11%|█▍           | 13/119 [00:02<00:16,  6.54it/s]Epoch: 2, train for the 129-th batch, train loss: 0.5454136729240417:  88%|█████████▋ | 128/146 [00:18<00:02,  6.15it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5452784895896912:  40%|█████▏       | 60/151 [00:09<00:16,  5.66it/s]Epoch: 2, train for the 129-th batch, train loss: 0.5454136729240417:  88%|█████████▋ | 129/146 [00:18<00:02,  5.68it/s]evaluate for the 33-th batch, evaluate loss: 0.502152681350708:  44%|████████▍          | 32/72 [00:03<00:03, 10.01it/s]Epoch: 3, train for the 14-th batch, train loss: 0.4924883246421814:  11%|█▍           | 13/119 [00:02<00:16,  6.54it/s]Epoch: 3, train for the 14-th batch, train loss: 0.4924883246421814:  12%|█▌           | 14/119 [00:02<00:14,  7.02it/s]Epoch: 2, train for the 130-th batch, train loss: 0.4771327078342438:  88%|█████████▋ | 129/146 [00:18<00:02,  5.68it/s]evaluate for the 34-th batch, evaluate loss: 0.5489398241043091:  44%|████████          | 32/72 [00:03<00:03, 10.01it/s]evaluate for the 34-th batch, evaluate loss: 0.5489398241043091:  47%|████████▌         | 34/72 [00:03<00:03, 10.09it/s]Epoch: 2, train for the 130-th batch, train loss: 0.4771327078342438:  89%|█████████▊ | 130/146 [00:18<00:02,  5.90it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5651264786720276:  40%|█████▏       | 60/151 [00:09<00:16,  5.66it/s]Epoch: 1, train for the 203-th batch, train loss: 0.5343500971794128:  85%|█████████▍ | 202/237 [00:53<00:14,  2.50it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5651264786720276:  40%|█████▎       | 61/151 [00:09<00:16,  5.36it/s]Epoch: 3, train for the 15-th batch, train loss: 0.5401661396026611:  12%|█▌           | 14/119 [00:02<00:14,  7.02it/s]Epoch: 1, train for the 193-th batch, train loss: 0.5138523578643799:  50%|█████▌     | 192/383 [00:51<01:01,  3.11it/s]Epoch: 3, train for the 15-th batch, train loss: 0.5401661396026611:  13%|█▋           | 15/119 [00:02<00:14,  7.30it/s]Epoch: 1, train for the 203-th batch, train loss: 0.5343500971794128:  86%|█████████▍ | 203/237 [00:53<00:12,  2.72it/s]Epoch: 1, train for the 193-th batch, train loss: 0.5138523578643799:  50%|█████▌     | 193/383 [00:51<01:00,  3.13it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5421926975250244:  89%|█████████▊ | 130/146 [00:18<00:02,  5.90it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5421926975250244:  90%|█████████▊ | 131/146 [00:18<00:02,  6.25it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5830893516540527:  40%|█████▎       | 61/151 [00:09<00:16,  5.36it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5830893516540527:  41%|█████▎       | 62/151 [00:09<00:14,  6.09it/s]Epoch: 3, train for the 16-th batch, train loss: 0.5008660554885864:  13%|█▋           | 15/119 [00:02<00:14,  7.30it/s]Epoch: 3, train for the 16-th batch, train loss: 0.5008660554885864:  13%|█▋           | 16/119 [00:02<00:14,  7.35it/s]evaluate for the 35-th batch, evaluate loss: 0.46520376205444336:  47%|████████         | 34/72 [00:03<00:03, 10.09it/s]Epoch: 1, train for the 204-th batch, train loss: 0.5447887778282166:  86%|█████████▍ | 203/237 [00:53<00:12,  2.72it/s]Epoch: 2, train for the 132-th batch, train loss: 0.4842144548892975:  90%|█████████▊ | 131/146 [00:19<00:02,  6.25it/s]evaluate for the 36-th batch, evaluate loss: 0.508226215839386:  47%|████████▉          | 34/72 [00:03<00:03, 10.09it/s]evaluate for the 36-th batch, evaluate loss: 0.508226215839386:  50%|█████████▌         | 36/72 [00:03<00:04,  8.85it/s]Epoch: 1, train for the 204-th batch, train loss: 0.5447887778282166:  86%|█████████▍ | 204/237 [00:53<00:10,  3.12it/s]Epoch: 2, train for the 132-th batch, train loss: 0.4842144548892975:  90%|█████████▉ | 132/146 [00:19<00:02,  6.31it/s]Epoch: 2, train for the 63-th batch, train loss: 0.6220855712890625:  41%|█████▎       | 62/151 [00:09<00:14,  6.09it/s]Epoch: 3, train for the 17-th batch, train loss: 0.4690794348716736:  13%|█▋           | 16/119 [00:02<00:14,  7.35it/s]Epoch: 1, train for the 194-th batch, train loss: 0.47177818417549133:  50%|█████     | 193/383 [00:51<01:00,  3.13it/s]Epoch: 2, train for the 63-th batch, train loss: 0.6220855712890625:  42%|█████▍       | 63/151 [00:09<00:14,  5.98it/s]Epoch: 3, train for the 17-th batch, train loss: 0.4690794348716736:  14%|█▊           | 17/119 [00:02<00:13,  7.49it/s]evaluate for the 37-th batch, evaluate loss: 0.49487051367759705:  50%|████████▌        | 36/72 [00:03<00:04,  8.85it/s]Epoch: 1, train for the 194-th batch, train loss: 0.47177818417549133:  51%|█████     | 194/383 [00:51<00:58,  3.22it/s]evaluate for the 38-th batch, evaluate loss: 0.4098457098007202:  50%|█████████         | 36/72 [00:03<00:04,  8.85it/s]evaluate for the 38-th batch, evaluate loss: 0.4098457098007202:  53%|█████████▌        | 38/72 [00:03<00:03, 10.08it/s]Epoch: 2, train for the 133-th batch, train loss: 0.5384830832481384:  90%|█████████▉ | 132/146 [00:19<00:02,  6.31it/s]Epoch: 1, train for the 205-th batch, train loss: 0.5440464615821838:  86%|█████████▍ | 204/237 [00:53<00:10,  3.12it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5627022981643677:  42%|█████▍       | 63/151 [00:10<00:14,  5.98it/s]Epoch: 3, train for the 18-th batch, train loss: 0.47682130336761475:  14%|█▋          | 17/119 [00:02<00:13,  7.49it/s]Epoch: 2, train for the 133-th batch, train loss: 0.5384830832481384:  91%|██████████ | 133/146 [00:19<00:02,  5.90it/s]evaluate for the 39-th batch, evaluate loss: 0.4335821866989136:  53%|█████████▌        | 38/72 [00:03<00:03, 10.08it/s]Epoch: 3, train for the 18-th batch, train loss: 0.47682130336761475:  15%|█▊          | 18/119 [00:02<00:14,  7.04it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5627022981643677:  42%|█████▌       | 64/151 [00:10<00:14,  5.84it/s]Epoch: 1, train for the 205-th batch, train loss: 0.5440464615821838:  86%|█████████▌ | 205/237 [00:53<00:09,  3.37it/s]evaluate for the 40-th batch, evaluate loss: 0.4915093779563904:  53%|█████████▌        | 38/72 [00:03<00:03, 10.08it/s]evaluate for the 40-th batch, evaluate loss: 0.4915093779563904:  56%|██████████        | 40/72 [00:03<00:02, 10.75it/s]Epoch: 1, train for the 195-th batch, train loss: 0.4943501949310303:  51%|█████▌     | 194/383 [00:52<00:58,  3.22it/s]Epoch: 2, train for the 134-th batch, train loss: 0.4732184410095215:  91%|██████████ | 133/146 [00:19<00:02,  5.90it/s]Epoch: 3, train for the 19-th batch, train loss: 0.4967101812362671:  15%|█▉           | 18/119 [00:02<00:14,  7.04it/s]Epoch: 2, train for the 134-th batch, train loss: 0.4732184410095215:  92%|██████████ | 134/146 [00:19<00:02,  5.85it/s]evaluate for the 41-th batch, evaluate loss: 0.4957638084888458:  56%|██████████        | 40/72 [00:03<00:02, 10.75it/s]Epoch: 3, train for the 19-th batch, train loss: 0.4967101812362671:  16%|██           | 19/119 [00:02<00:15,  6.58it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5177029967308044:  42%|█████▌       | 64/151 [00:10<00:14,  5.84it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5177029967308044:  43%|█████▌       | 65/151 [00:10<00:15,  5.72it/s]Epoch: 1, train for the 195-th batch, train loss: 0.4943501949310303:  51%|█████▌     | 195/383 [00:52<00:57,  3.27it/s]Epoch: 1, train for the 206-th batch, train loss: 0.5444071888923645:  86%|█████████▌ | 205/237 [00:54<00:09,  3.37it/s]evaluate for the 42-th batch, evaluate loss: 0.4906662702560425:  56%|██████████        | 40/72 [00:03<00:02, 10.75it/s]evaluate for the 42-th batch, evaluate loss: 0.4906662702560425:  58%|██████████▌       | 42/72 [00:03<00:02, 11.09it/s]Epoch: 1, train for the 206-th batch, train loss: 0.5444071888923645:  87%|█████████▌ | 206/237 [00:54<00:08,  3.57it/s]evaluate for the 43-th batch, evaluate loss: 0.4957664906978607:  58%|██████████▌       | 42/72 [00:03<00:02, 11.09it/s]Epoch: 3, train for the 20-th batch, train loss: 0.4830796420574188:  16%|██           | 19/119 [00:03<00:15,  6.58it/s]Epoch: 2, train for the 66-th batch, train loss: 0.5547478199005127:  43%|█████▌       | 65/151 [00:10<00:15,  5.72it/s]Epoch: 3, train for the 20-th batch, train loss: 0.4830796420574188:  17%|██▏          | 20/119 [00:03<00:16,  6.16it/s]Epoch: 2, train for the 135-th batch, train loss: 0.5177382826805115:  92%|██████████ | 134/146 [00:19<00:02,  5.85it/s]Epoch: 2, train for the 66-th batch, train loss: 0.5547478199005127:  44%|█████▋       | 66/151 [00:10<00:14,  5.72it/s]Epoch: 2, train for the 135-th batch, train loss: 0.5177382826805115:  92%|██████████▏| 135/146 [00:19<00:02,  5.41it/s]Epoch: 2, train for the 67-th batch, train loss: 0.5442878007888794:  44%|█████▋       | 66/151 [00:10<00:14,  5.72it/s]Epoch: 3, train for the 21-th batch, train loss: 0.498718798160553:  17%|██▎           | 20/119 [00:03<00:16,  6.16it/s]Epoch: 2, train for the 67-th batch, train loss: 0.5442878007888794:  44%|█████▊       | 67/151 [00:10<00:13,  6.38it/s]Epoch: 3, train for the 21-th batch, train loss: 0.498718798160553:  18%|██▍           | 21/119 [00:03<00:14,  6.55it/s]Epoch: 2, train for the 136-th batch, train loss: 0.4946359097957611:  92%|██████████▏| 135/146 [00:19<00:02,  5.41it/s]evaluate for the 44-th batch, evaluate loss: 0.49310624599456787:  58%|█████████▉       | 42/72 [00:04<00:02, 11.09it/s]evaluate for the 44-th batch, evaluate loss: 0.49310624599456787:  61%|██████████▍      | 44/72 [00:04<00:02,  9.35it/s]Epoch: 2, train for the 136-th batch, train loss: 0.4946359097957611:  93%|██████████▏| 136/146 [00:19<00:01,  5.68it/s]Epoch: 1, train for the 196-th batch, train loss: 0.5011903047561646:  51%|█████▌     | 195/383 [00:52<00:57,  3.27it/s]Epoch: 1, train for the 207-th batch, train loss: 0.579749584197998:  87%|██████████▍ | 206/237 [00:54<00:08,  3.57it/s]Epoch: 3, train for the 22-th batch, train loss: 0.45853328704833984:  18%|██          | 21/119 [00:03<00:14,  6.55it/s]Epoch: 2, train for the 68-th batch, train loss: 0.5425888299942017:  44%|█████▊       | 67/151 [00:10<00:13,  6.38it/s]Epoch: 3, train for the 22-th batch, train loss: 0.45853328704833984:  18%|██▏         | 22/119 [00:03<00:14,  6.87it/s]evaluate for the 45-th batch, evaluate loss: 0.48344069719314575:  61%|██████████▍      | 44/72 [00:04<00:02,  9.35it/s]Epoch: 2, train for the 68-th batch, train loss: 0.5425888299942017:  45%|█████▊       | 68/151 [00:10<00:12,  6.58it/s]Epoch: 1, train for the 207-th batch, train loss: 0.579749584197998:  87%|██████████▍ | 207/237 [00:54<00:09,  3.30it/s]Epoch: 1, train for the 196-th batch, train loss: 0.5011903047561646:  51%|█████▋     | 196/383 [00:52<01:05,  2.87it/s]Epoch: 2, train for the 137-th batch, train loss: 0.4970313012599945:  93%|██████████▏| 136/146 [00:19<00:01,  5.68it/s]Epoch: 2, train for the 137-th batch, train loss: 0.4970313012599945:  94%|██████████▎| 137/146 [00:19<00:01,  6.06it/s]evaluate for the 46-th batch, evaluate loss: 0.5348596572875977:  61%|███████████       | 44/72 [00:04<00:02,  9.35it/s]evaluate for the 46-th batch, evaluate loss: 0.5348596572875977:  64%|███████████▌      | 46/72 [00:04<00:02, 10.24it/s]Epoch: 3, train for the 23-th batch, train loss: 0.5313903093338013:  18%|██▍          | 22/119 [00:03<00:14,  6.87it/s]Epoch: 3, train for the 23-th batch, train loss: 0.5313903093338013:  19%|██▌          | 23/119 [00:03<00:13,  7.02it/s]evaluate for the 47-th batch, evaluate loss: 0.367186039686203:  64%|████████████▏      | 46/72 [00:04<00:02, 10.24it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5320342183113098:  45%|█████▊       | 68/151 [00:10<00:12,  6.58it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5320342183113098:  46%|█████▉       | 69/151 [00:10<00:12,  6.36it/s]Epoch: 1, train for the 208-th batch, train loss: 0.5454913377761841:  87%|█████████▌ | 207/237 [00:54<00:09,  3.30it/s]Epoch: 2, train for the 138-th batch, train loss: 0.5101413130760193:  94%|██████████▎| 137/146 [00:20<00:01,  6.06it/s]Epoch: 2, train for the 138-th batch, train loss: 0.5101413130760193:  95%|██████████▍| 138/146 [00:20<00:01,  6.18it/s]Epoch: 1, train for the 208-th batch, train loss: 0.5454913377761841:  88%|█████████▋ | 208/237 [00:54<00:08,  3.61it/s]Epoch: 1, train for the 197-th batch, train loss: 0.5026404857635498:  51%|█████▋     | 196/383 [00:52<01:05,  2.87it/s]Epoch: 3, train for the 24-th batch, train loss: 0.3888600170612335:  19%|██▌          | 23/119 [00:03<00:13,  7.02it/s]Epoch: 3, train for the 24-th batch, train loss: 0.3888600170612335:  20%|██▌          | 24/119 [00:03<00:13,  6.83it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5646324157714844:  46%|█████▉       | 69/151 [00:10<00:12,  6.36it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5646324157714844:  46%|██████       | 70/151 [00:10<00:12,  6.53it/s]Epoch: 1, train for the 197-th batch, train loss: 0.5026404857635498:  51%|█████▋     | 197/383 [00:52<01:01,  3.00it/s]Epoch: 2, train for the 139-th batch, train loss: 0.5468839406967163:  95%|██████████▍| 138/146 [00:20<00:01,  6.18it/s]Epoch: 2, train for the 139-th batch, train loss: 0.5468839406967163:  95%|██████████▍| 139/146 [00:20<00:01,  6.27it/s]evaluate for the 48-th batch, evaluate loss: 0.3245944082736969:  64%|███████████▌      | 46/72 [00:04<00:02, 10.24it/s]evaluate for the 48-th batch, evaluate loss: 0.3245944082736969:  67%|████████████      | 48/72 [00:04<00:02,  8.67it/s]Epoch: 3, train for the 25-th batch, train loss: 0.46415388584136963:  20%|██▍         | 24/119 [00:03<00:13,  6.83it/s]Epoch: 3, train for the 25-th batch, train loss: 0.46415388584136963:  21%|██▌         | 25/119 [00:03<00:13,  6.98it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5814248919487:  46%|███████▍        | 70/151 [00:11<00:12,  6.53it/s]evaluate for the 49-th batch, evaluate loss: 0.4483538866043091:  67%|████████████      | 48/72 [00:04<00:02,  8.67it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5814248919487:  47%|███████▌        | 71/151 [00:11<00:12,  6.54it/s]Epoch: 1, train for the 209-th batch, train loss: 0.5530941486358643:  88%|█████████▋ | 208/237 [00:54<00:08,  3.61it/s]Epoch: 2, train for the 140-th batch, train loss: 0.4800986647605896:  95%|██████████▍| 139/146 [00:20<00:01,  6.27it/s]Epoch: 2, train for the 140-th batch, train loss: 0.4800986647605896:  96%|██████████▌| 140/146 [00:20<00:00,  6.42it/s]Epoch: 1, train for the 209-th batch, train loss: 0.5530941486358643:  88%|█████████▋ | 209/237 [00:54<00:07,  3.54it/s]Epoch: 1, train for the 198-th batch, train loss: 0.46497583389282227:  51%|█████▏    | 197/383 [00:53<01:01,  3.00it/s]evaluate for the 50-th batch, evaluate loss: 0.40112003684043884:  67%|███████████▎     | 48/72 [00:04<00:02,  8.67it/s]evaluate for the 50-th batch, evaluate loss: 0.40112003684043884:  69%|███████████▊     | 50/72 [00:04<00:02,  9.66it/s]Epoch: 3, train for the 26-th batch, train loss: 0.39894965291023254:  21%|██▌         | 25/119 [00:03<00:13,  6.98it/s]Epoch: 3, train for the 26-th batch, train loss: 0.39894965291023254:  22%|██▌         | 26/119 [00:03<00:12,  7.20it/s]Epoch: 1, train for the 198-th batch, train loss: 0.46497583389282227:  52%|█████▏    | 198/383 [00:53<00:57,  3.22it/s]Epoch: 2, train for the 72-th batch, train loss: 0.527527928352356:  47%|██████▌       | 71/151 [00:11<00:12,  6.54it/s]evaluate for the 51-th batch, evaluate loss: 0.4992128610610962:  69%|████████████▌     | 50/72 [00:04<00:02,  9.66it/s]Epoch: 2, train for the 72-th batch, train loss: 0.527527928352356:  48%|██████▋       | 72/151 [00:11<00:12,  6.38it/s]Epoch: 2, train for the 141-th batch, train loss: 0.5257065296173096:  96%|██████████▌| 140/146 [00:20<00:00,  6.42it/s]evaluate for the 52-th batch, evaluate loss: 0.4436614215373993:  69%|████████████▌     | 50/72 [00:04<00:02,  9.66it/s]evaluate for the 52-th batch, evaluate loss: 0.4436614215373993:  72%|█████████████     | 52/72 [00:04<00:01, 10.74it/s]Epoch: 2, train for the 141-th batch, train loss: 0.5257065296173096:  97%|██████████▌| 141/146 [00:20<00:00,  6.31it/s]Epoch: 3, train for the 27-th batch, train loss: 0.5172939300537109:  22%|██▊          | 26/119 [00:04<00:12,  7.20it/s]Epoch: 3, train for the 27-th batch, train loss: 0.5172939300537109:  23%|██▉          | 27/119 [00:04<00:12,  7.08it/s]Epoch: 1, train for the 210-th batch, train loss: 0.5765330195426941:  88%|█████████▋ | 209/237 [00:55<00:07,  3.54it/s]evaluate for the 53-th batch, evaluate loss: 0.4762301445007324:  72%|█████████████     | 52/72 [00:05<00:01, 10.74it/s]Epoch: 1, train for the 210-th batch, train loss: 0.5765330195426941:  89%|█████████▋ | 210/237 [00:55<00:07,  3.70it/s]Epoch: 1, train for the 199-th batch, train loss: 0.4663788378238678:  52%|█████▋     | 198/383 [00:53<00:57,  3.22it/s]Epoch: 2, train for the 73-th batch, train loss: 0.5409986972808838:  48%|██████▏      | 72/151 [00:11<00:12,  6.38it/s]Epoch: 2, train for the 73-th batch, train loss: 0.5409986972808838:  48%|██████▎      | 73/151 [00:11<00:12,  6.36it/s]Epoch: 1, train for the 199-th batch, train loss: 0.4663788378238678:  52%|█████▋     | 199/383 [00:53<00:52,  3.50it/s]Epoch: 2, train for the 142-th batch, train loss: 0.44742169976234436:  97%|█████████▋| 141/146 [00:20<00:00,  6.31it/s]Epoch: 3, train for the 28-th batch, train loss: 0.4969887435436249:  23%|██▉          | 27/119 [00:04<00:12,  7.08it/s]Epoch: 2, train for the 142-th batch, train loss: 0.44742169976234436:  97%|█████████▋| 142/146 [00:20<00:00,  6.06it/s]Epoch: 3, train for the 28-th batch, train loss: 0.4969887435436249:  24%|███          | 28/119 [00:04<00:13,  6.77it/s]Epoch: 2, train for the 74-th batch, train loss: 0.4900231957435608:  48%|██████▎      | 73/151 [00:11<00:12,  6.36it/s]Epoch: 2, train for the 74-th batch, train loss: 0.4900231957435608:  49%|██████▎      | 74/151 [00:11<00:11,  6.95it/s]evaluate for the 54-th batch, evaluate loss: 0.5208272337913513:  72%|█████████████     | 52/72 [00:05<00:01, 10.74it/s]evaluate for the 54-th batch, evaluate loss: 0.5208272337913513:  75%|█████████████▌    | 54/72 [00:05<00:01,  9.54it/s]Epoch: 1, train for the 211-th batch, train loss: 0.5307116508483887:  89%|█████████▋ | 210/237 [00:55<00:07,  3.70it/s]Epoch: 2, train for the 143-th batch, train loss: 0.5374248623847961:  97%|██████████▋| 142/146 [00:20<00:00,  6.06it/s]Epoch: 2, train for the 143-th batch, train loss: 0.5374248623847961:  98%|██████████▊| 143/146 [00:20<00:00,  6.50it/s]evaluate for the 55-th batch, evaluate loss: 0.4872833490371704:  75%|█████████████▌    | 54/72 [00:05<00:01,  9.54it/s]Epoch: 1, train for the 211-th batch, train loss: 0.5307116508483887:  89%|█████████▊ | 211/237 [00:55<00:06,  3.90it/s]Epoch: 3, train for the 29-th batch, train loss: 0.475806325674057:  24%|███▎          | 28/119 [00:04<00:13,  6.77it/s]Epoch: 1, train for the 200-th batch, train loss: 0.5600077509880066:  52%|█████▋     | 199/383 [00:53<00:52,  3.50it/s]Epoch: 3, train for the 29-th batch, train loss: 0.475806325674057:  24%|███▍          | 29/119 [00:04<00:13,  6.67it/s]Epoch: 2, train for the 75-th batch, train loss: 0.541479229927063:  49%|██████▊       | 74/151 [00:11<00:11,  6.95it/s]Epoch: 1, train for the 200-th batch, train loss: 0.5600077509880066:  52%|█████▋     | 200/383 [00:53<00:51,  3.56it/s]evaluate for the 56-th batch, evaluate loss: 0.49224281311035156:  75%|████████████▊    | 54/72 [00:05<00:01,  9.54it/s]evaluate for the 56-th batch, evaluate loss: 0.49224281311035156:  78%|█████████████▏   | 56/72 [00:05<00:01, 10.43it/s]Epoch: 2, train for the 75-th batch, train loss: 0.541479229927063:  50%|██████▉       | 75/151 [00:11<00:12,  6.28it/s]Epoch: 2, train for the 144-th batch, train loss: 0.47848406434059143:  98%|█████████▊| 143/146 [00:21<00:00,  6.50it/s]Epoch: 2, train for the 144-th batch, train loss: 0.47848406434059143:  99%|█████████▊| 144/146 [00:21<00:00,  6.56it/s]evaluate for the 57-th batch, evaluate loss: 0.4410130977630615:  78%|██████████████    | 56/72 [00:05<00:01, 10.43it/s]Epoch: 3, train for the 30-th batch, train loss: 0.46409764885902405:  24%|██▉         | 29/119 [00:04<00:13,  6.67it/s]Epoch: 3, train for the 30-th batch, train loss: 0.46409764885902405:  25%|███         | 30/119 [00:04<00:13,  6.69it/s]evaluate for the 58-th batch, evaluate loss: 0.5023624897003174:  78%|██████████████    | 56/72 [00:05<00:01, 10.43it/s]evaluate for the 58-th batch, evaluate loss: 0.5023624897003174:  81%|██████████████▌   | 58/72 [00:05<00:01, 11.31it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5440883040428162:  50%|██████▍      | 75/151 [00:11<00:12,  6.28it/s]Epoch: 2, train for the 145-th batch, train loss: 0.514867901802063:  99%|███████████▊| 144/146 [00:21<00:00,  6.56it/s]evaluate for the 59-th batch, evaluate loss: 0.4755411446094513:  81%|██████████████▌   | 58/72 [00:05<00:01, 11.31it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5440883040428162:  50%|██████▌      | 76/151 [00:11<00:12,  5.82it/s]Epoch: 2, train for the 145-th batch, train loss: 0.514867901802063:  99%|███████████▉| 145/146 [00:21<00:00,  6.33it/s]Epoch: 3, train for the 31-th batch, train loss: 0.47293493151664734:  25%|███         | 30/119 [00:04<00:13,  6.69it/s]Epoch: 3, train for the 31-th batch, train loss: 0.47293493151664734:  26%|███▏        | 31/119 [00:04<00:13,  6.53it/s]Epoch: 1, train for the 201-th batch, train loss: 0.5202324390411377:  52%|█████▋     | 200/383 [00:53<00:51,  3.56it/s]evaluate for the 60-th batch, evaluate loss: 0.4844411313533783:  81%|██████████████▌   | 58/72 [00:05<00:01, 11.31it/s]evaluate for the 60-th batch, evaluate loss: 0.4844411313533783:  83%|███████████████   | 60/72 [00:05<00:01, 11.60it/s]Epoch: 1, train for the 212-th batch, train loss: 0.5401461124420166:  89%|█████████▊ | 211/237 [00:55<00:06,  3.90it/s]Epoch: 1, train for the 201-th batch, train loss: 0.5202324390411377:  52%|█████▊     | 201/383 [00:54<00:53,  3.38it/s]Epoch: 2, train for the 146-th batch, train loss: 0.4708483815193176:  99%|██████████▉| 145/146 [00:21<00:00,  6.33it/s]Epoch: 2, train for the 146-th batch, train loss: 0.4708483815193176: 100%|███████████| 146/146 [00:21<00:00,  6.83it/s]Epoch: 2, train for the 146-th batch, train loss: 0.4708483815193176: 100%|███████████| 146/146 [00:21<00:00,  6.84it/s]
evaluate for the 61-th batch, evaluate loss: 0.45263800024986267:  83%|██████████████▏  | 60/72 [00:05<00:01, 11.60it/s]Epoch: 1, train for the 212-th batch, train loss: 0.5401461124420166:  89%|█████████▊ | 212/237 [00:55<00:08,  3.11it/s]Epoch: 2, train for the 77-th batch, train loss: 0.4999077618122101:  50%|██████▌      | 76/151 [00:12<00:12,  5.82it/s]Epoch: 2, train for the 77-th batch, train loss: 0.4999077618122101:  51%|██████▋      | 77/151 [00:12<00:12,  5.78it/s]Epoch: 3, train for the 32-th batch, train loss: 0.4312589168548584:  26%|███▍         | 31/119 [00:04<00:13,  6.53it/s]evaluate for the 62-th batch, evaluate loss: 0.4515817165374756:  83%|███████████████   | 60/72 [00:05<00:01, 11.60it/s]evaluate for the 62-th batch, evaluate loss: 0.4515817165374756:  86%|███████████████▌  | 62/72 [00:05<00:00, 12.62it/s]Epoch: 3, train for the 32-th batch, train loss: 0.4312589168548584:  27%|███▍         | 32/119 [00:04<00:14,  6.10it/s]evaluate for the 63-th batch, evaluate loss: 0.4286191463470459:  86%|███████████████▌  | 62/72 [00:05<00:00, 12.62it/s]  0%|                                                                                            | 0/38 [00:00<?, ?it/s]Epoch: 3, train for the 33-th batch, train loss: 0.5009949207305908:  27%|███▍         | 32/119 [00:04<00:14,  6.10it/s]Epoch: 3, train for the 33-th batch, train loss: 0.5009949207305908:  28%|███▌         | 33/119 [00:04<00:13,  6.38it/s]Epoch: 2, train for the 78-th batch, train loss: 0.503480076789856:  51%|███████▏      | 77/151 [00:12<00:12,  5.78it/s]evaluate for the 64-th batch, evaluate loss: 0.5278446078300476:  86%|███████████████▌  | 62/72 [00:05<00:00, 12.62it/s]evaluate for the 64-th batch, evaluate loss: 0.5278446078300476:  89%|████████████████  | 64/72 [00:05<00:00, 12.86it/s]Epoch: 2, train for the 78-th batch, train loss: 0.503480076789856:  52%|███████▏      | 78/151 [00:12<00:13,  5.43it/s]evaluate for the 1-th batch, evaluate loss: 0.4739793539047241:   0%|                            | 0/38 [00:00<?, ?it/s]Epoch: 1, train for the 213-th batch, train loss: 0.5547690391540527:  89%|█████████▊ | 212/237 [00:56<00:08,  3.11it/s]Epoch: 3, train for the 34-th batch, train loss: 0.46228280663490295:  28%|███▎        | 33/119 [00:05<00:13,  6.38it/s]evaluate for the 2-th batch, evaluate loss: 0.49809733033180237:   0%|                           | 0/38 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.49809733033180237:   5%|█                  | 2/38 [00:00<00:02, 12.68it/s]Epoch: 3, train for the 34-th batch, train loss: 0.46228280663490295:  29%|███▍        | 34/119 [00:05<00:12,  6.66it/s]Epoch: 2, train for the 79-th batch, train loss: 0.5878108739852905:  52%|██████▋      | 78/151 [00:12<00:13,  5.43it/s]Epoch: 1, train for the 213-th batch, train loss: 0.5547690391540527:  90%|█████████▉ | 213/237 [00:56<00:07,  3.03it/s]Epoch: 2, train for the 79-th batch, train loss: 0.5878108739852905:  52%|██████▊      | 79/151 [00:12<00:12,  5.93it/s]evaluate for the 3-th batch, evaluate loss: 0.47214311361312866:   5%|█                  | 2/38 [00:00<00:02, 12.68it/s]Epoch: 1, train for the 202-th batch, train loss: 0.4845622777938843:  52%|█████▊     | 201/383 [00:54<00:53,  3.38it/s]evaluate for the 65-th batch, evaluate loss: 0.5138256549835205:  89%|████████████████  | 64/72 [00:06<00:00, 12.86it/s]Epoch: 3, train for the 35-th batch, train loss: 0.454731285572052:  29%|████          | 34/119 [00:05<00:12,  6.66it/s]Epoch: 1, train for the 202-th batch, train loss: 0.4845622777938843:  53%|█████▊     | 202/383 [00:54<01:05,  2.77it/s]Epoch: 3, train for the 35-th batch, train loss: 0.454731285572052:  29%|████          | 35/119 [00:05<00:12,  6.80it/s]evaluate for the 4-th batch, evaluate loss: 0.48698362708091736:   5%|█                  | 2/38 [00:00<00:02, 12.68it/s]evaluate for the 4-th batch, evaluate loss: 0.48698362708091736:  11%|██                 | 4/38 [00:00<00:02, 13.48it/s]evaluate for the 66-th batch, evaluate loss: 0.4481455981731415:  89%|████████████████  | 64/72 [00:06<00:00, 12.86it/s]evaluate for the 66-th batch, evaluate loss: 0.4481455981731415:  92%|████████████████▌ | 66/72 [00:06<00:00,  9.84it/s]Epoch: 2, train for the 80-th batch, train loss: 0.5452489852905273:  52%|██████▊      | 79/151 [00:12<00:12,  5.93it/s]evaluate for the 5-th batch, evaluate loss: 0.5361703634262085:  11%|██                  | 4/38 [00:00<00:02, 13.48it/s]Epoch: 1, train for the 214-th batch, train loss: 0.5961904525756836:  90%|█████████▉ | 213/237 [00:56<00:07,  3.03it/s]Epoch: 2, train for the 80-th batch, train loss: 0.5452489852905273:  53%|██████▉      | 80/151 [00:12<00:12,  5.69it/s]evaluate for the 67-th batch, evaluate loss: 0.4605470895767212:  92%|████████████████▌ | 66/72 [00:06<00:00,  9.84it/s]Epoch: 3, train for the 36-th batch, train loss: 0.4497966170310974:  29%|███▊         | 35/119 [00:05<00:12,  6.80it/s]Epoch: 1, train for the 214-th batch, train loss: 0.5961904525756836:  90%|█████████▉ | 214/237 [00:56<00:07,  3.23it/s]evaluate for the 6-th batch, evaluate loss: 0.4832804799079895:  11%|██                  | 4/38 [00:00<00:02, 13.48it/s]evaluate for the 6-th batch, evaluate loss: 0.4832804799079895:  16%|███▏                | 6/38 [00:00<00:02, 13.63it/s]Epoch: 3, train for the 36-th batch, train loss: 0.4497966170310974:  30%|███▉         | 36/119 [00:05<00:12,  6.70it/s]evaluate for the 68-th batch, evaluate loss: 0.4549979269504547:  92%|████████████████▌ | 66/72 [00:06<00:00,  9.84it/s]evaluate for the 68-th batch, evaluate loss: 0.4549979269504547:  94%|█████████████████ | 68/72 [00:06<00:00, 10.41it/s]evaluate for the 7-th batch, evaluate loss: 0.43847328424453735:  16%|███                | 6/38 [00:00<00:02, 13.63it/s]Epoch: 1, train for the 203-th batch, train loss: 0.4435536861419678:  53%|█████▊     | 202/383 [00:54<01:05,  2.77it/s]Epoch: 2, train for the 81-th batch, train loss: 0.5216326713562012:  53%|██████▉      | 80/151 [00:12<00:12,  5.69it/s]Epoch: 2, train for the 81-th batch, train loss: 0.5216326713562012:  54%|██████▉      | 81/151 [00:12<00:12,  5.63it/s]Epoch: 1, train for the 203-th batch, train loss: 0.4435536861419678:  53%|█████▊     | 203/383 [00:54<01:01,  2.94it/s]Epoch: 3, train for the 37-th batch, train loss: 0.5071945190429688:  30%|███▉         | 36/119 [00:05<00:12,  6.70it/s]evaluate for the 8-th batch, evaluate loss: 0.46098607778549194:  16%|███                | 6/38 [00:00<00:02, 13.63it/s]evaluate for the 8-th batch, evaluate loss: 0.46098607778549194:  21%|████               | 8/38 [00:00<00:02, 13.06it/s]Epoch: 3, train for the 37-th batch, train loss: 0.5071945190429688:  31%|████         | 37/119 [00:05<00:12,  6.58it/s]evaluate for the 9-th batch, evaluate loss: 0.4834119975566864:  21%|████▏               | 8/38 [00:00<00:02, 13.06it/s]Epoch: 2, train for the 82-th batch, train loss: 0.5206708312034607:  54%|██████▉      | 81/151 [00:12<00:12,  5.63it/s]evaluate for the 69-th batch, evaluate loss: 0.46951788663864136:  94%|████████████████ | 68/72 [00:06<00:00, 10.41it/s]Epoch: 2, train for the 82-th batch, train loss: 0.5206708312034607:  54%|███████      | 82/151 [00:13<00:11,  5.96it/s]Epoch: 1, train for the 215-th batch, train loss: 0.5621259808540344:  90%|█████████▉ | 214/237 [00:56<00:07,  3.23it/s]Epoch: 3, train for the 38-th batch, train loss: 0.4374157190322876:  31%|████         | 37/119 [00:05<00:12,  6.58it/s]evaluate for the 10-th batch, evaluate loss: 0.5132730603218079:  21%|████               | 8/38 [00:00<00:02, 13.06it/s]evaluate for the 10-th batch, evaluate loss: 0.5132730603218079:  26%|████▋             | 10/38 [00:00<00:02, 13.09it/s]Epoch: 3, train for the 38-th batch, train loss: 0.4374157190322876:  32%|████▏        | 38/119 [00:05<00:12,  6.57it/s]Epoch: 1, train for the 215-th batch, train loss: 0.5621259808540344:  91%|█████████▉ | 215/237 [00:56<00:06,  3.16it/s]evaluate for the 70-th batch, evaluate loss: 0.4670318365097046:  94%|█████████████████ | 68/72 [00:06<00:00, 10.41it/s]evaluate for the 70-th batch, evaluate loss: 0.4670318365097046:  97%|█████████████████▌| 70/72 [00:06<00:00,  9.12it/s]evaluate for the 11-th batch, evaluate loss: 0.45872828364372253:  26%|████▍            | 10/38 [00:00<00:02, 13.09it/s]Epoch: 1, train for the 204-th batch, train loss: 0.32856056094169617:  53%|█████▎    | 203/383 [00:55<01:01,  2.94it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5260310769081116:  54%|███████      | 82/151 [00:13<00:11,  5.96it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5260310769081116:  55%|███████▏     | 83/151 [00:13<00:11,  5.94it/s]evaluate for the 71-th batch, evaluate loss: 0.5101353526115417:  97%|█████████████████▌| 70/72 [00:06<00:00,  9.12it/s]Epoch: 1, train for the 204-th batch, train loss: 0.32856056094169617:  53%|█████▎    | 204/383 [00:55<00:58,  3.04it/s]evaluate for the 12-th batch, evaluate loss: 0.533649742603302:  26%|█████              | 10/38 [00:00<00:02, 13.09it/s]evaluate for the 12-th batch, evaluate loss: 0.533649742603302:  32%|██████             | 12/38 [00:00<00:02, 12.56it/s]Epoch: 3, train for the 39-th batch, train loss: 0.4839370548725128:  32%|████▏        | 38/119 [00:05<00:12,  6.57it/s]Epoch: 3, train for the 39-th batch, train loss: 0.4839370548725128:  33%|████▎        | 39/119 [00:05<00:12,  6.20it/s]evaluate for the 72-th batch, evaluate loss: 0.5602645874023438:  97%|█████████████████▌| 70/72 [00:06<00:00,  9.12it/s]evaluate for the 72-th batch, evaluate loss: 0.5602645874023438: 100%|██████████████████| 72/72 [00:06<00:00,  9.53it/s]evaluate for the 72-th batch, evaluate loss: 0.5602645874023438: 100%|██████████████████| 72/72 [00:06<00:00, 10.45it/s]
Epoch: 1, train for the 216-th batch, train loss: 0.4841785728931427:  91%|█████████▉ | 215/237 [00:57<00:06,  3.16it/s]evaluate for the 13-th batch, evaluate loss: 0.4960498809814453:  32%|█████▋            | 12/38 [00:00<00:02, 12.56it/s]Epoch: 2, train for the 84-th batch, train loss: 0.565078854560852:  55%|███████▋      | 83/151 [00:13<00:11,  5.94it/s]Epoch: 1, train for the 216-th batch, train loss: 0.4841785728931427:  91%|██████████ | 216/237 [00:57<00:06,  3.31it/s]Epoch: 2, train for the 84-th batch, train loss: 0.565078854560852:  56%|███████▊      | 84/151 [00:13<00:11,  5.87it/s]evaluate for the 14-th batch, evaluate loss: 0.43076422810554504:  32%|█████▎           | 12/38 [00:01<00:02, 12.56it/s]evaluate for the 14-th batch, evaluate loss: 0.43076422810554504:  37%|██████▎          | 14/38 [00:01<00:01, 13.51it/s]Epoch: 3, train for the 40-th batch, train loss: 0.4695214629173279:  33%|████▎        | 39/119 [00:06<00:12,  6.20it/s]Epoch: 3, train for the 40-th batch, train loss: 0.4695214629173279:  34%|████▎        | 40/119 [00:06<00:12,  6.38it/s]evaluate for the 15-th batch, evaluate loss: 0.4516344666481018:  37%|██████▋           | 14/38 [00:01<00:01, 13.51it/s]Epoch: 1, train for the 205-th batch, train loss: 0.4237113893032074:  53%|█████▊     | 204/383 [00:55<00:58,  3.04it/s]Epoch: 2, train for the 85-th batch, train loss: 0.4900067448616028:  56%|███████▏     | 84/151 [00:13<00:11,  5.87it/s]Epoch: 2, train for the 85-th batch, train loss: 0.4900067448616028:  56%|███████▎     | 85/151 [00:13<00:10,  6.12it/s]Epoch: 1, train for the 217-th batch, train loss: 0.4845639765262604:  91%|██████████ | 216/237 [00:57<00:06,  3.31it/s]evaluate for the 16-th batch, evaluate loss: 0.5177401304244995:  37%|██████▋           | 14/38 [00:01<00:01, 13.51it/s]evaluate for the 16-th batch, evaluate loss: 0.5177401304244995:  42%|███████▌          | 16/38 [00:01<00:01, 12.92it/s]Epoch: 1, train for the 205-th batch, train loss: 0.4237113893032074:  54%|█████▉     | 205/383 [00:55<00:59,  3.01it/s]Epoch: 3, train for the 41-th batch, train loss: 0.4723705053329468:  34%|████▎        | 40/119 [00:06<00:12,  6.38it/s]Epoch: 3, train for the 41-th batch, train loss: 0.4723705053329468:  34%|████▍        | 41/119 [00:06<00:12,  6.36it/s]Epoch: 1, train for the 217-th batch, train loss: 0.4845639765262604:  92%|██████████ | 217/237 [00:57<00:05,  3.62it/s]evaluate for the 17-th batch, evaluate loss: 0.4709429144859314:  42%|███████▌          | 16/38 [00:01<00:01, 12.92it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5082495212554932:  56%|███████▎     | 85/151 [00:13<00:10,  6.12it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5082495212554932:  57%|███████▍     | 86/151 [00:13<00:10,  6.33it/s]Epoch: 3, train for the 42-th batch, train loss: 0.507745623588562:  34%|████▊         | 41/119 [00:06<00:12,  6.36it/s]evaluate for the 18-th batch, evaluate loss: 0.5077822804450989:  42%|███████▌          | 16/38 [00:01<00:01, 12.92it/s]evaluate for the 18-th batch, evaluate loss: 0.5077822804450989:  47%|████████▌         | 18/38 [00:01<00:01, 11.90it/s]Epoch: 3, train for the 42-th batch, train loss: 0.507745623588562:  35%|████▉         | 42/119 [00:06<00:12,  6.09it/s]Epoch: 1, train for the 206-th batch, train loss: 0.46673640608787537:  54%|█████▎    | 205/383 [00:55<00:59,  3.01it/s]Epoch: 2, train for the 87-th batch, train loss: 0.4942648708820343:  57%|███████▍     | 86/151 [00:13<00:10,  6.33it/s]Epoch: 2, train for the 87-th batch, train loss: 0.4942648708820343:  58%|███████▍     | 87/151 [00:13<00:09,  6.69it/s]evaluate for the 19-th batch, evaluate loss: 0.49207350611686707:  47%|████████         | 18/38 [00:01<00:01, 11.90it/s]Epoch: 1, train for the 218-th batch, train loss: 0.5635077357292175:  92%|██████████ | 217/237 [00:57<00:05,  3.62it/s]Epoch: 1, train for the 206-th batch, train loss: 0.46673640608787537:  54%|█████▍    | 206/383 [00:55<00:54,  3.22it/s]Epoch: 1, train for the 218-th batch, train loss: 0.5635077357292175:  92%|██████████ | 218/237 [00:57<00:05,  3.63it/s]evaluate for the 20-th batch, evaluate loss: 0.4286554753780365:  47%|████████▌         | 18/38 [00:01<00:01, 11.90it/s]evaluate for the 20-th batch, evaluate loss: 0.4286554753780365:  53%|█████████▍        | 20/38 [00:01<00:01, 12.50it/s]Epoch: 3, train for the 43-th batch, train loss: 0.4835120737552643:  35%|████▌        | 42/119 [00:06<00:12,  6.09it/s]Epoch: 3, train for the 43-th batch, train loss: 0.4835120737552643:  36%|████▋        | 43/119 [00:06<00:12,  6.24it/s]Epoch: 2, train for the 88-th batch, train loss: 0.5455291271209717:  58%|███████▍     | 87/151 [00:13<00:09,  6.69it/s]evaluate for the 21-th batch, evaluate loss: 0.44088828563690186:  53%|████████▉        | 20/38 [00:01<00:01, 12.50it/s]Epoch: 2, train for the 88-th batch, train loss: 0.5455291271209717:  58%|███████▌     | 88/151 [00:13<00:09,  6.61it/s]evaluate for the 22-th batch, evaluate loss: 0.48511791229248047:  53%|████████▉        | 20/38 [00:01<00:01, 12.50it/s]evaluate for the 22-th batch, evaluate loss: 0.48511791229248047:  58%|█████████▊       | 22/38 [00:01<00:01, 13.45it/s]Epoch: 1, train for the 219-th batch, train loss: 0.5554378628730774:  92%|██████████ | 218/237 [00:57<00:05,  3.63it/s]Epoch: 1, train for the 207-th batch, train loss: 0.4700649082660675:  54%|█████▉     | 206/383 [00:56<00:54,  3.22it/s]Epoch: 3, train for the 44-th batch, train loss: 0.4343799948692322:  36%|████▋        | 43/119 [00:06<00:12,  6.24it/s]Epoch: 1, train for the 219-th batch, train loss: 0.5554378628730774:  92%|██████████▏| 219/237 [00:57<00:04,  3.74it/s]evaluate for the 23-th batch, evaluate loss: 0.47494253516197205:  58%|█████████▊       | 22/38 [00:01<00:01, 13.45it/s]Epoch: 3, train for the 44-th batch, train loss: 0.4343799948692322:  37%|████▊        | 44/119 [00:06<00:12,  5.77it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5305291414260864:  58%|███████▌     | 88/151 [00:14<00:09,  6.61it/s]Epoch: 1, train for the 207-th batch, train loss: 0.4700649082660675:  54%|█████▉     | 207/383 [00:56<00:55,  3.20it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5305291414260864:  59%|███████▋     | 89/151 [00:14<00:10,  6.11it/s]evaluate for the 24-th batch, evaluate loss: 0.4552414119243622:  58%|██████████▍       | 22/38 [00:01<00:01, 13.45it/s]evaluate for the 24-th batch, evaluate loss: 0.4552414119243622:  63%|███████████▎      | 24/38 [00:01<00:01, 13.33it/s]  0%|                                                                                            | 0/34 [00:00<?, ?it/s]Epoch: 3, train for the 45-th batch, train loss: 0.45123225450515747:  37%|████▍       | 44/119 [00:06<00:12,  5.77it/s]Epoch: 3, train for the 45-th batch, train loss: 0.45123225450515747:  38%|████▌       | 45/119 [00:06<00:12,  6.07it/s]evaluate for the 1-th batch, evaluate loss: 0.6148017644882202:   0%|                            | 0/34 [00:00<?, ?it/s]Epoch: 2, train for the 90-th batch, train loss: 0.49822998046875:  59%|████████▊      | 89/151 [00:14<00:10,  6.11it/s]Epoch: 2, train for the 90-th batch, train loss: 0.49822998046875:  60%|████████▉      | 90/151 [00:14<00:10,  5.83it/s]Epoch: 3, train for the 46-th batch, train loss: 0.4771568179130554:  38%|████▉        | 45/119 [00:06<00:12,  6.07it/s]evaluate for the 2-th batch, evaluate loss: 0.6786507964134216:   0%|                            | 0/34 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.6786507964134216:   6%|█▏                  | 2/34 [00:00<00:02, 14.49it/s]Epoch: 3, train for the 46-th batch, train loss: 0.4771568179130554:  39%|█████        | 46/119 [00:06<00:10,  6.79it/s]evaluate for the 25-th batch, evaluate loss: 0.49168500304222107:  63%|██████████▋      | 24/38 [00:02<00:01, 13.33it/s]evaluate for the 3-th batch, evaluate loss: 0.6942522525787354:   6%|█▏                  | 2/34 [00:00<00:02, 14.49it/s]Epoch: 1, train for the 208-th batch, train loss: 0.5035117864608765:  54%|█████▉     | 207/383 [00:56<00:55,  3.20it/s]evaluate for the 4-th batch, evaluate loss: 0.6619051694869995:   6%|█▏                  | 2/34 [00:00<00:02, 14.49it/s]evaluate for the 4-th batch, evaluate loss: 0.6619051694869995:  12%|██▎                 | 4/34 [00:00<00:01, 15.05it/s]evaluate for the 26-th batch, evaluate loss: 0.4791783094406128:  63%|███████████▎      | 24/38 [00:02<00:01, 13.33it/s]evaluate for the 26-th batch, evaluate loss: 0.4791783094406128:  68%|████████████▎     | 26/38 [00:02<00:01,  9.78it/s]Epoch: 1, train for the 220-th batch, train loss: 0.5795475244522095:  92%|██████████▏| 219/237 [00:58<00:04,  3.74it/s]Epoch: 3, train for the 47-th batch, train loss: 0.5537055134773254:  39%|█████        | 46/119 [00:07<00:10,  6.79it/s]Epoch: 1, train for the 208-th batch, train loss: 0.5035117864608765:  54%|█████▉     | 208/383 [00:56<00:58,  2.99it/s]Epoch: 3, train for the 47-th batch, train loss: 0.5537055134773254:  39%|█████▏       | 47/119 [00:07<00:10,  6.59it/s]evaluate for the 5-th batch, evaluate loss: 0.655005931854248:  12%|██▍                  | 4/34 [00:00<00:01, 15.05it/s]evaluate for the 27-th batch, evaluate loss: 0.4777672588825226:  68%|████████████▎     | 26/38 [00:02<00:01,  9.78it/s]Epoch: 1, train for the 220-th batch, train loss: 0.5795475244522095:  93%|██████████▏| 220/237 [00:58<00:05,  3.10it/s]Epoch: 2, train for the 91-th batch, train loss: 0.49393004179000854:  60%|███████▏    | 90/151 [00:14<00:10,  5.83it/s]evaluate for the 6-th batch, evaluate loss: 0.6024686694145203:  12%|██▎                 | 4/34 [00:00<00:01, 15.05it/s]evaluate for the 6-th batch, evaluate loss: 0.6024686694145203:  18%|███▌                | 6/34 [00:00<00:01, 14.04it/s]Epoch: 2, train for the 91-th batch, train loss: 0.49393004179000854:  60%|███████▏    | 91/151 [00:14<00:12,  4.67it/s]evaluate for the 28-th batch, evaluate loss: 0.4993583858013153:  68%|████████████▎     | 26/38 [00:02<00:01,  9.78it/s]evaluate for the 28-th batch, evaluate loss: 0.4993583858013153:  74%|█████████████▎    | 28/38 [00:02<00:00, 10.47it/s]Epoch: 3, train for the 48-th batch, train loss: 0.50596684217453:  39%|█████▉         | 47/119 [00:07<00:10,  6.59it/s]Epoch: 3, train for the 48-th batch, train loss: 0.50596684217453:  40%|██████         | 48/119 [00:07<00:10,  6.54it/s]evaluate for the 7-th batch, evaluate loss: 0.5673010349273682:  18%|███▌                | 6/34 [00:00<00:01, 14.04it/s]Epoch: 1, train for the 209-th batch, train loss: 0.4292042553424835:  54%|█████▉     | 208/383 [00:56<00:58,  2.99it/s]Epoch: 1, train for the 221-th batch, train loss: 0.5578527450561523:  93%|██████████▏| 220/237 [00:58<00:05,  3.10it/s]Epoch: 1, train for the 209-th batch, train loss: 0.4292042553424835:  55%|██████     | 209/383 [00:56<00:54,  3.21it/s]Epoch: 3, train for the 49-th batch, train loss: 0.4746231734752655:  40%|█████▏       | 48/119 [00:07<00:10,  6.54it/s]Epoch: 3, train for the 49-th batch, train loss: 0.4746231734752655:  41%|█████▎       | 49/119 [00:07<00:09,  7.08it/s]evaluate for the 8-th batch, evaluate loss: 0.679881751537323:  18%|███▋                 | 6/34 [00:00<00:01, 14.04it/s]evaluate for the 8-th batch, evaluate loss: 0.679881751537323:  24%|████▉                | 8/34 [00:00<00:01, 13.50it/s]Epoch: 1, train for the 221-th batch, train loss: 0.5578527450561523:  93%|██████████▎| 221/237 [00:58<00:04,  3.33it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5383563041687012:  60%|███████▊     | 91/151 [00:14<00:12,  4.67it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5383563041687012:  61%|███████▉     | 92/151 [00:14<00:12,  4.80it/s]evaluate for the 9-th batch, evaluate loss: 0.5876103639602661:  24%|████▋               | 8/34 [00:00<00:01, 13.50it/s]evaluate for the 29-th batch, evaluate loss: 0.46868133544921875:  74%|████████████▌    | 28/38 [00:02<00:00, 10.47it/s]Epoch: 3, train for the 50-th batch, train loss: 0.510267972946167:  41%|█████▊        | 49/119 [00:07<00:09,  7.08it/s]Epoch: 3, train for the 50-th batch, train loss: 0.510267972946167:  42%|█████▉        | 50/119 [00:07<00:09,  7.25it/s]evaluate for the 10-th batch, evaluate loss: 0.6381195783615112:  24%|████▍              | 8/34 [00:00<00:01, 13.50it/s]evaluate for the 10-th batch, evaluate loss: 0.6381195783615112:  29%|█████▎            | 10/34 [00:00<00:01, 13.59it/s]evaluate for the 30-th batch, evaluate loss: 0.5008180737495422:  74%|█████████████▎    | 28/38 [00:02<00:00, 10.47it/s]evaluate for the 30-th batch, evaluate loss: 0.5008180737495422:  79%|██████████████▏   | 30/38 [00:02<00:00,  8.88it/s]Epoch: 2, train for the 93-th batch, train loss: 0.5619528889656067:  61%|███████▉     | 92/151 [00:14<00:12,  4.80it/s]evaluate for the 11-th batch, evaluate loss: 0.6662580966949463:  29%|█████▎            | 10/34 [00:00<00:01, 13.59it/s]Epoch: 1, train for the 210-th batch, train loss: 0.4782048761844635:  55%|██████     | 209/383 [00:56<00:54,  3.21it/s]Epoch: 2, train for the 93-th batch, train loss: 0.5619528889656067:  62%|████████     | 93/151 [00:15<00:11,  4.97it/s]evaluate for the 31-th batch, evaluate loss: 0.47175654768943787:  79%|█████████████▍   | 30/38 [00:02<00:00,  8.88it/s]Epoch: 1, train for the 222-th batch, train loss: 0.5672695636749268:  93%|██████████▎| 221/237 [00:58<00:04,  3.33it/s]Epoch: 3, train for the 51-th batch, train loss: 0.5041484832763672:  42%|█████▍       | 50/119 [00:07<00:09,  7.25it/s]Epoch: 3, train for the 51-th batch, train loss: 0.5041484832763672:  43%|█████▌       | 51/119 [00:07<00:09,  7.01it/s]Epoch: 1, train for the 222-th batch, train loss: 0.5672695636749268:  94%|██████████▎| 222/237 [00:58<00:04,  3.39it/s]Epoch: 1, train for the 210-th batch, train loss: 0.4782048761844635:  55%|██████     | 210/383 [00:57<00:54,  3.20it/s]evaluate for the 12-th batch, evaluate loss: 0.6150602102279663:  29%|█████▎            | 10/34 [00:00<00:01, 13.59it/s]evaluate for the 12-th batch, evaluate loss: 0.6150602102279663:  35%|██████▎           | 12/34 [00:00<00:01, 13.49it/s]evaluate for the 32-th batch, evaluate loss: 0.4418356716632843:  79%|██████████████▏   | 30/38 [00:02<00:00,  8.88it/s]evaluate for the 32-th batch, evaluate loss: 0.4418356716632843:  84%|███████████████▏  | 32/38 [00:02<00:00, 10.12it/s]Epoch: 2, train for the 94-th batch, train loss: 0.5392810106277466:  62%|████████     | 93/151 [00:15<00:11,  4.97it/s]evaluate for the 13-th batch, evaluate loss: 0.5686484575271606:  35%|██████▎           | 12/34 [00:00<00:01, 13.49it/s]Epoch: 3, train for the 52-th batch, train loss: 0.528115451335907:  43%|██████        | 51/119 [00:07<00:09,  7.01it/s]Epoch: 2, train for the 94-th batch, train loss: 0.5392810106277466:  62%|████████     | 94/151 [00:15<00:11,  5.06it/s]Epoch: 3, train for the 52-th batch, train loss: 0.528115451335907:  44%|██████        | 52/119 [00:07<00:09,  6.96it/s]evaluate for the 14-th batch, evaluate loss: 0.5523385405540466:  35%|██████▎           | 12/34 [00:01<00:01, 13.49it/s]evaluate for the 14-th batch, evaluate loss: 0.5523385405540466:  41%|███████▍          | 14/34 [00:01<00:01, 13.27it/s]Epoch: 1, train for the 211-th batch, train loss: 0.43062788248062134:  55%|█████▍    | 210/383 [00:57<00:54,  3.20it/s]evaluate for the 15-th batch, evaluate loss: 0.6734853982925415:  41%|███████▍          | 14/34 [00:01<00:01, 13.27it/s]Epoch: 1, train for the 211-th batch, train loss: 0.43062788248062134:  55%|█████▌    | 211/383 [00:57<00:51,  3.36it/s]Epoch: 1, train for the 223-th batch, train loss: 0.5584854483604431:  94%|██████████▎| 222/237 [00:59<00:04,  3.39it/s]Epoch: 3, train for the 53-th batch, train loss: 0.47606557607650757:  44%|█████▏      | 52/119 [00:07<00:09,  6.96it/s]evaluate for the 33-th batch, evaluate loss: 0.47322332859039307:  84%|██████████████▎  | 32/38 [00:03<00:00, 10.12it/s]Epoch: 3, train for the 53-th batch, train loss: 0.47606557607650757:  45%|█████▎      | 53/119 [00:08<00:09,  6.84it/s]Epoch: 2, train for the 95-th batch, train loss: 0.47811439633369446:  62%|███████▍    | 94/151 [00:15<00:11,  5.06it/s]evaluate for the 16-th batch, evaluate loss: 0.6111915707588196:  41%|███████▍          | 14/34 [00:01<00:01, 13.27it/s]evaluate for the 16-th batch, evaluate loss: 0.6111915707588196:  47%|████████▍         | 16/34 [00:01<00:01, 13.23it/s]evaluate for the 34-th batch, evaluate loss: 0.4824378490447998:  84%|███████████████▏  | 32/38 [00:03<00:00, 10.12it/s]evaluate for the 34-th batch, evaluate loss: 0.4824378490447998:  89%|████████████████  | 34/38 [00:03<00:00,  8.47it/s]Epoch: 1, train for the 223-th batch, train loss: 0.5584854483604431:  94%|██████████▎| 223/237 [00:59<00:04,  3.27it/s]Epoch: 2, train for the 95-th batch, train loss: 0.47811439633369446:  63%|███████▌    | 95/151 [00:15<00:11,  5.00it/s]evaluate for the 35-th batch, evaluate loss: 0.49359405040740967:  89%|███████████████▏ | 34/38 [00:03<00:00,  8.47it/s]evaluate for the 17-th batch, evaluate loss: 0.6513832211494446:  47%|████████▍         | 16/34 [00:01<00:01, 13.23it/s]Epoch: 3, train for the 54-th batch, train loss: 0.4455098509788513:  45%|█████▊       | 53/119 [00:08<00:09,  6.84it/s]Epoch: 3, train for the 54-th batch, train loss: 0.4455098509788513:  45%|█████▉       | 54/119 [00:08<00:10,  6.39it/s]evaluate for the 18-th batch, evaluate loss: 0.5851770043373108:  47%|████████▍         | 16/34 [00:01<00:01, 13.23it/s]evaluate for the 18-th batch, evaluate loss: 0.5851770043373108:  53%|█████████▌        | 18/34 [00:01<00:01, 11.95it/s]Epoch: 2, train for the 96-th batch, train loss: 0.534807562828064:  63%|████████▊     | 95/151 [00:15<00:11,  5.00it/s]Epoch: 1, train for the 224-th batch, train loss: 0.5847759246826172:  94%|██████████▎| 223/237 [00:59<00:04,  3.27it/s]Epoch: 2, train for the 96-th batch, train loss: 0.534807562828064:  64%|████████▉     | 96/151 [00:15<00:11,  4.84it/s]Epoch: 3, train for the 55-th batch, train loss: 0.4735327661037445:  45%|█████▉       | 54/119 [00:08<00:10,  6.39it/s]evaluate for the 19-th batch, evaluate loss: 0.5616434216499329:  53%|█████████▌        | 18/34 [00:01<00:01, 11.95it/s]Epoch: 3, train for the 55-th batch, train loss: 0.4735327661037445:  46%|██████       | 55/119 [00:08<00:09,  6.99it/s]Epoch: 1, train for the 224-th batch, train loss: 0.5847759246826172:  95%|██████████▍| 224/237 [00:59<00:03,  3.46it/s]evaluate for the 20-th batch, evaluate loss: 0.620074987411499:  53%|██████████         | 18/34 [00:01<00:01, 11.95it/s]evaluate for the 20-th batch, evaluate loss: 0.620074987411499:  59%|███████████▏       | 20/34 [00:01<00:01, 12.85it/s]Epoch: 3, train for the 56-th batch, train loss: 0.47183603048324585:  46%|█████▌      | 55/119 [00:08<00:09,  6.99it/s]Epoch: 3, train for the 56-th batch, train loss: 0.47183603048324585:  47%|█████▋      | 56/119 [00:08<00:08,  7.50it/s]Epoch: 2, train for the 97-th batch, train loss: 0.5343385934829712:  64%|████████▎    | 96/151 [00:15<00:11,  4.84it/s]evaluate for the 36-th batch, evaluate loss: 0.508038341999054:  89%|█████████████████  | 34/38 [00:03<00:00,  8.47it/s]evaluate for the 36-th batch, evaluate loss: 0.508038341999054:  95%|██████████████████ | 36/38 [00:03<00:00,  7.05it/s]evaluate for the 21-th batch, evaluate loss: 0.6178986430168152:  59%|██████████▌       | 20/34 [00:01<00:01, 12.85it/s]Epoch: 2, train for the 97-th batch, train loss: 0.5343385934829712:  64%|████████▎    | 97/151 [00:15<00:10,  4.98it/s]Epoch: 1, train for the 212-th batch, train loss: 0.5645337104797363:  55%|██████     | 211/383 [00:57<00:51,  3.36it/s]evaluate for the 22-th batch, evaluate loss: 0.4067954421043396:  59%|██████████▌       | 20/34 [00:01<00:01, 12.85it/s]evaluate for the 22-th batch, evaluate loss: 0.4067954421043396:  65%|███████████▋      | 22/34 [00:01<00:00, 12.85it/s]evaluate for the 37-th batch, evaluate loss: 0.4372161030769348:  95%|█████████████████ | 36/38 [00:03<00:00,  7.05it/s]Epoch: 1, train for the 225-th batch, train loss: 0.5740287899971008:  95%|██████████▍| 224/237 [00:59<00:03,  3.46it/s]Epoch: 1, train for the 212-th batch, train loss: 0.5645337104797363:  55%|██████     | 212/383 [00:57<01:04,  2.67it/s]Epoch: 3, train for the 57-th batch, train loss: 0.42285552620887756:  47%|█████▋      | 56/119 [00:08<00:08,  7.50it/s]Epoch: 3, train for the 57-th batch, train loss: 0.42285552620887756:  48%|█████▋      | 57/119 [00:08<00:08,  7.28it/s]evaluate for the 38-th batch, evaluate loss: 0.4659723937511444:  95%|█████████████████ | 36/38 [00:03<00:00,  7.05it/s]evaluate for the 38-th batch, evaluate loss: 0.4659723937511444: 100%|██████████████████| 38/38 [00:03<00:00,  8.38it/s]evaluate for the 38-th batch, evaluate loss: 0.4659723937511444: 100%|██████████████████| 38/38 [00:03<00:00, 10.49it/s]
Epoch: 1, train for the 225-th batch, train loss: 0.5740287899971008:  95%|██████████▍| 225/237 [00:59<00:03,  3.48it/s]evaluate for the 23-th batch, evaluate loss: 0.3646613359451294:  65%|███████████▋      | 22/34 [00:01<00:00, 12.85it/s]Epoch: 2, train for the 98-th batch, train loss: 0.5078312754631042:  64%|████████▎    | 97/151 [00:15<00:10,  4.98it/s]Epoch: 2, train for the 98-th batch, train loss: 0.5078312754631042:  65%|████████▍    | 98/151 [00:15<00:10,  5.19it/s]evaluate for the 24-th batch, evaluate loss: 0.5249968767166138:  65%|███████████▋      | 22/34 [00:01<00:00, 12.85it/s]evaluate for the 24-th batch, evaluate loss: 0.5249968767166138:  71%|████████████▋     | 24/34 [00:01<00:00, 12.51it/s]Epoch: 3, train for the 58-th batch, train loss: 0.46624454855918884:  48%|█████▋      | 57/119 [00:08<00:08,  7.28it/s]Epoch: 3, train for the 58-th batch, train loss: 0.46624454855918884:  49%|█████▊      | 58/119 [00:08<00:08,  6.78it/s]Epoch: 1, train for the 213-th batch, train loss: 0.545289933681488:  55%|██████▋     | 212/383 [00:58<01:04,  2.67it/s]  0%|                                                                                            | 0/20 [00:00<?, ?it/s]Epoch: 2, train for the 99-th batch, train loss: 0.5303806662559509:  65%|████████▍    | 98/151 [00:16<00:10,  5.19it/s]evaluate for the 25-th batch, evaluate loss: 0.5233505368232727:  71%|████████████▋     | 24/34 [00:01<00:00, 12.51it/s]Epoch: 2, train for the 99-th batch, train loss: 0.5303806662559509:  66%|████████▌    | 99/151 [00:16<00:09,  5.49it/s]evaluate for the 1-th batch, evaluate loss: 0.7298118472099304:   0%|                            | 0/20 [00:00<?, ?it/s]Epoch: 1, train for the 226-th batch, train loss: 0.591086745262146:  95%|███████████▍| 225/237 [00:59<00:03,  3.48it/s]Epoch: 1, train for the 213-th batch, train loss: 0.545289933681488:  56%|██████▋     | 213/383 [00:58<01:00,  2.83it/s]Epoch: 3, train for the 59-th batch, train loss: 0.47235575318336487:  49%|█████▊      | 58/119 [00:08<00:08,  6.78it/s]evaluate for the 26-th batch, evaluate loss: 0.622381329536438:  71%|█████████████▍     | 24/34 [00:02<00:00, 12.51it/s]evaluate for the 26-th batch, evaluate loss: 0.622381329536438:  76%|██████████████▌    | 26/34 [00:02<00:00, 12.29it/s]Epoch: 3, train for the 59-th batch, train loss: 0.47235575318336487:  50%|█████▉      | 59/119 [00:08<00:08,  7.02it/s]evaluate for the 2-th batch, evaluate loss: 0.7649415135383606:   0%|                            | 0/20 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.7649415135383606:  10%|██                  | 2/20 [00:00<00:01, 15.58it/s]Epoch: 1, train for the 226-th batch, train loss: 0.591086745262146:  95%|███████████▍| 226/237 [01:00<00:03,  3.35it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5727910995483398:  66%|███████▊    | 99/151 [00:16<00:09,  5.49it/s]evaluate for the 27-th batch, evaluate loss: 0.6387868523597717:  76%|█████████████▊    | 26/34 [00:02<00:00, 12.29it/s]evaluate for the 3-th batch, evaluate loss: 0.6344153881072998:  10%|██                  | 2/20 [00:00<00:01, 15.58it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5727910995483398:  66%|███████▎   | 100/151 [00:16<00:09,  5.47it/s]Epoch: 3, train for the 60-th batch, train loss: 0.4411449432373047:  50%|██████▍      | 59/119 [00:08<00:08,  7.02it/s]Epoch: 3, train for the 60-th batch, train loss: 0.4411449432373047:  50%|██████▌      | 60/119 [00:09<00:08,  6.90it/s]evaluate for the 4-th batch, evaluate loss: 0.6685462594032288:  10%|██                  | 2/20 [00:00<00:01, 15.58it/s]evaluate for the 4-th batch, evaluate loss: 0.6685462594032288:  20%|████                | 4/20 [00:00<00:01, 13.77it/s]Epoch: 1, train for the 214-th batch, train loss: 0.4641771614551544:  56%|██████     | 213/383 [00:58<01:00,  2.83it/s]evaluate for the 28-th batch, evaluate loss: 0.6069062948226929:  76%|█████████████▊    | 26/34 [00:02<00:00, 12.29it/s]evaluate for the 28-th batch, evaluate loss: 0.6069062948226929:  82%|██████████████▊   | 28/34 [00:02<00:00, 11.53it/s]Epoch: 1, train for the 214-th batch, train loss: 0.4641771614551544:  56%|██████▏    | 214/383 [00:58<00:55,  3.06it/s]Epoch: 1, train for the 227-th batch, train loss: 0.5469340085983276:  95%|██████████▍| 226/237 [01:00<00:03,  3.35it/s]evaluate for the 5-th batch, evaluate loss: 0.6594778895378113:  20%|████                | 4/20 [00:00<00:01, 13.77it/s]Epoch: 1, train for the 227-th batch, train loss: 0.5469340085983276:  96%|██████████▌| 227/237 [01:00<00:02,  3.54it/s]evaluate for the 29-th batch, evaluate loss: 0.5821056962013245:  82%|██████████████▊   | 28/34 [00:02<00:00, 11.53it/s]Epoch: 2, train for the 101-th batch, train loss: 0.5991700887680054:  66%|███████▎   | 100/151 [00:16<00:09,  5.47it/s]Epoch: 3, train for the 61-th batch, train loss: 0.4698502719402313:  50%|██████▌      | 60/119 [00:09<00:08,  6.90it/s]Epoch: 3, train for the 61-th batch, train loss: 0.4698502719402313:  51%|██████▋      | 61/119 [00:09<00:08,  6.58it/s]Epoch: 2, train for the 101-th batch, train loss: 0.5991700887680054:  67%|███████▎   | 101/151 [00:16<00:09,  5.24it/s]evaluate for the 6-th batch, evaluate loss: 0.7248685956001282:  20%|████                | 4/20 [00:00<00:01, 13.77it/s]evaluate for the 6-th batch, evaluate loss: 0.7248685956001282:  30%|██████              | 6/20 [00:00<00:01, 13.23it/s]evaluate for the 30-th batch, evaluate loss: 0.5633889436721802:  82%|██████████████▊   | 28/34 [00:02<00:00, 11.53it/s]evaluate for the 30-th batch, evaluate loss: 0.5633889436721802:  88%|███████████████▉  | 30/34 [00:02<00:00, 11.26it/s]evaluate for the 7-th batch, evaluate loss: 0.7444047331809998:  30%|██████              | 6/20 [00:00<00:01, 13.23it/s]evaluate for the 8-th batch, evaluate loss: 0.6952328085899353:  30%|██████              | 6/20 [00:00<00:01, 13.23it/s]evaluate for the 8-th batch, evaluate loss: 0.6952328085899353:  40%|████████            | 8/20 [00:00<00:00, 13.95it/s]evaluate for the 31-th batch, evaluate loss: 0.6383236646652222:  88%|███████████████▉  | 30/34 [00:02<00:00, 11.26it/s]Epoch: 1, train for the 215-th batch, train loss: 0.3953937590122223:  56%|██████▏    | 214/383 [00:58<00:55,  3.06it/s]evaluate for the 9-th batch, evaluate loss: 0.6371906399726868:  40%|████████            | 8/20 [00:00<00:00, 13.95it/s]Epoch: 2, train for the 102-th batch, train loss: 0.49475032091140747:  67%|██████▋   | 101/151 [00:16<00:09,  5.24it/s]Epoch: 2, train for the 102-th batch, train loss: 0.49475032091140747:  68%|██████▊   | 102/151 [00:16<00:09,  5.13it/s]evaluate for the 32-th batch, evaluate loss: 0.6512294411659241:  88%|███████████████▉  | 30/34 [00:02<00:00, 11.26it/s]evaluate for the 32-th batch, evaluate loss: 0.6512294411659241:  94%|████████████████▉ | 32/34 [00:02<00:00, 11.55it/s]Epoch: 1, train for the 228-th batch, train loss: 0.5394229888916016:  96%|██████████▌| 227/237 [01:00<00:02,  3.54it/s]Epoch: 3, train for the 62-th batch, train loss: 0.5367815494537354:  51%|██████▋      | 61/119 [00:09<00:08,  6.58it/s]Epoch: 1, train for the 215-th batch, train loss: 0.3953937590122223:  56%|██████▏    | 215/383 [00:58<00:54,  3.08it/s]Epoch: 3, train for the 62-th batch, train loss: 0.5367815494537354:  52%|██████▊      | 62/119 [00:09<00:10,  5.50it/s]evaluate for the 10-th batch, evaluate loss: 0.6405945420265198:  40%|███████▌           | 8/20 [00:00<00:00, 13.95it/s]evaluate for the 10-th batch, evaluate loss: 0.6405945420265198:  50%|█████████         | 10/20 [00:00<00:00, 15.34it/s]Epoch: 1, train for the 228-th batch, train loss: 0.5394229888916016:  96%|██████████▌| 228/237 [01:00<00:02,  3.42it/s]evaluate for the 11-th batch, evaluate loss: 0.6337828040122986:  50%|█████████         | 10/20 [00:00<00:00, 15.34it/s]Epoch: 2, train for the 103-th batch, train loss: 0.5829381346702576:  68%|███████▍   | 102/151 [00:16<00:09,  5.13it/s]Epoch: 3, train for the 63-th batch, train loss: 0.49468687176704407:  52%|██████▎     | 62/119 [00:09<00:10,  5.50it/s]Epoch: 3, train for the 63-th batch, train loss: 0.49468687176704407:  53%|██████▎     | 63/119 [00:09<00:09,  5.88it/s]Epoch: 2, train for the 103-th batch, train loss: 0.5829381346702576:  68%|███████▌   | 103/151 [00:16<00:09,  5.25it/s]evaluate for the 12-th batch, evaluate loss: 0.7026283144950867:  50%|█████████         | 10/20 [00:00<00:00, 15.34it/s]evaluate for the 12-th batch, evaluate loss: 0.7026283144950867:  60%|██████████▊       | 12/20 [00:00<00:00, 13.82it/s]evaluate for the 33-th batch, evaluate loss: 0.6550988554954529:  94%|████████████████▉ | 32/34 [00:02<00:00, 11.55it/s]evaluate for the 13-th batch, evaluate loss: 0.7181476950645447:  60%|██████████▊       | 12/20 [00:00<00:00, 13.82it/s]Epoch: 3, train for the 64-th batch, train loss: 0.4995497763156891:  53%|██████▉      | 63/119 [00:09<00:09,  5.88it/s]Epoch: 3, train for the 64-th batch, train loss: 0.4995497763156891:  54%|██████▉      | 64/119 [00:09<00:08,  6.30it/s]evaluate for the 34-th batch, evaluate loss: 0.7062582969665527:  94%|████████████████▉ | 32/34 [00:02<00:00, 11.55it/s]evaluate for the 34-th batch, evaluate loss: 0.7062582969665527: 100%|██████████████████| 34/34 [00:02<00:00,  9.34it/s]evaluate for the 34-th batch, evaluate loss: 0.7062582969665527: 100%|██████████████████| 34/34 [00:02<00:00, 11.86it/s]
evaluate for the 14-th batch, evaluate loss: 0.7334429025650024:  60%|██████████▊       | 12/20 [00:00<00:00, 13.82it/s]evaluate for the 14-th batch, evaluate loss: 0.7334429025650024:  70%|████████████▌     | 14/20 [00:00<00:00, 14.41it/s]Epoch: 2, train for the 104-th batch, train loss: 0.5778915286064148:  68%|███████▌   | 103/151 [00:17<00:09,  5.25it/s]Epoch: 2, train for the 104-th batch, train loss: 0.5778915286064148:  69%|███████▌   | 104/151 [00:17<00:09,  5.00it/s]evaluate for the 15-th batch, evaluate loss: 0.7253328561782837:  70%|████████████▌     | 14/20 [00:01<00:00, 14.41it/s]Epoch: 3, train for the 65-th batch, train loss: 0.4953654408454895:  54%|██████▉      | 64/119 [00:09<00:08,  6.30it/s]Epoch: 3, train for the 65-th batch, train loss: 0.4953654408454895:  55%|███████      | 65/119 [00:09<00:08,  6.20it/s]evaluate for the 16-th batch, evaluate loss: 0.6645557880401611:  70%|████████████▌     | 14/20 [00:01<00:00, 14.41it/s]evaluate for the 16-th batch, evaluate loss: 0.6645557880401611:  80%|██████████████▍   | 16/20 [00:01<00:00, 14.44it/s]evaluate for the 17-th batch, evaluate loss: 0.7060871720314026:  80%|██████████████▍   | 16/20 [00:01<00:00, 14.44it/s]Epoch: 1, train for the 216-th batch, train loss: 0.453571617603302:  56%|██████▋     | 215/383 [00:59<00:54,  3.08it/s]Epoch: 1, train for the 229-th batch, train loss: 0.612689733505249:  96%|███████████▌| 228/237 [01:01<00:02,  3.42it/s]INFO:root:Epoch: 1, learning rate: 0.0001, train loss: 0.5264
INFO:root:train average_precision, 0.8354
INFO:root:train roc_auc, 0.8026
INFO:root:validate loss: 0.4825
INFO:root:validate average_precision, 0.8735
INFO:root:validate roc_auc, 0.8549
INFO:root:new node validate loss: 0.6055
INFO:root:new node validate first_1_average_precision, 0.5085
INFO:root:new node validate first_1_roc_auc, 0.5018
INFO:root:new node validate first_3_average_precision, 0.6000
INFO:root:new node validate first_3_roc_auc, 0.5844
INFO:root:new node validate first_10_average_precision, 0.6988
INFO:root:new node validate first_10_roc_auc, 0.6772
INFO:root:new node validate average_precision, 0.7565
INFO:root:new node validate roc_auc, 0.7239
INFO:root:save model ./saved_models/TGN/ia-movielens-user2tags-10m/TGN_seed0_tgn-ia-movielens-user2tags-10m-lincorrect-time-linear/TGN_seed0_tgn-ia-movielens-user2tags-10m-lincorrect-time-linear.pkl
Epoch: 2, train for the 105-th batch, train loss: 0.5503044724464417:  69%|███████▌   | 104/151 [00:17<00:09,  5.00it/s]Epoch: 1, train for the 229-th batch, train loss: 0.612689733505249:  97%|███████████▌| 229/237 [01:01<00:02,  2.74it/s]Epoch: 1, train for the 216-th batch, train loss: 0.453571617603302:  56%|██████▊     | 216/383 [00:59<01:07,  2.49it/s]Epoch: 3, train for the 66-th batch, train loss: 0.4754682183265686:  55%|███████      | 65/119 [00:10<00:08,  6.20it/s]Epoch: 2, train for the 105-th batch, train loss: 0.5503044724464417:  70%|███████▋   | 105/151 [00:17<00:09,  4.88it/s]Epoch: 3, train for the 66-th batch, train loss: 0.4754682183265686:  55%|███████▏     | 66/119 [00:10<00:08,  6.34it/s]evaluate for the 18-th batch, evaluate loss: 0.693644642829895:  80%|███████████████▏   | 16/20 [00:01<00:00, 14.44it/s]evaluate for the 18-th batch, evaluate loss: 0.693644642829895:  90%|█████████████████  | 18/20 [00:01<00:00, 14.06it/s]evaluate for the 19-th batch, evaluate loss: 0.7387946844100952:  90%|████████████████▏ | 18/20 [00:01<00:00, 14.06it/s]Epoch: 2, train for the 106-th batch, train loss: 0.5039320588111877:  70%|███████▋   | 105/151 [00:17<00:09,  4.88it/s]Epoch: 2, train for the 106-th batch, train loss: 0.5039320588111877:  70%|███████▋   | 106/151 [00:17<00:08,  5.30it/s]Epoch: 3, train for the 67-th batch, train loss: 0.5304713845252991:  55%|███████▏     | 66/119 [00:10<00:08,  6.34it/s]evaluate for the 20-th batch, evaluate loss: 0.7612534761428833:  90%|████████████████▏ | 18/20 [00:01<00:00, 14.06it/s]evaluate for the 20-th batch, evaluate loss: 0.7612534761428833: 100%|██████████████████| 20/20 [00:01<00:00, 13.73it/s]evaluate for the 20-th batch, evaluate loss: 0.7612534761428833: 100%|██████████████████| 20/20 [00:01<00:00, 14.05it/s]
Epoch: 3, train for the 67-th batch, train loss: 0.5304713845252991:  56%|███████▎     | 67/119 [00:10<00:08,  6.27it/s]Epoch: 1, train for the 230-th batch, train loss: 0.5509843826293945:  97%|██████████▋| 229/237 [01:01<00:02,  2.74it/s]Epoch: 1, train for the 230-th batch, train loss: 0.5509843826293945:  97%|██████████▋| 230/237 [01:01<00:02,  3.05it/s]Epoch: 1, train for the 217-th batch, train loss: 0.4752725064754486:  56%|██████▏    | 216/383 [00:59<01:07,  2.49it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5267803072929382:  70%|███████▋   | 106/151 [00:17<00:08,  5.30it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5267803072929382:  71%|███████▊   | 107/151 [00:17<00:07,  5.91it/s]Epoch: 1, train for the 217-th batch, train loss: 0.4752725064754486:  57%|██████▏    | 217/383 [00:59<01:01,  2.70it/s]Epoch: 3, train for the 68-th batch, train loss: 0.4530329704284668:  56%|███████▎     | 67/119 [00:10<00:08,  6.27it/s]Epoch: 3, train for the 68-th batch, train loss: 0.4530329704284668:  57%|███████▍     | 68/119 [00:10<00:07,  6.50it/s]Epoch: 1, train for the 231-th batch, train loss: 0.5708793997764587:  97%|██████████▋| 230/237 [01:01<00:02,  3.05it/s]Epoch: 2, train for the 108-th batch, train loss: 0.5370217561721802:  71%|███████▊   | 107/151 [00:17<00:07,  5.91it/s]INFO:root:Epoch: 2, learning rate: 0.0001, train loss: 0.5589
INFO:root:train average_precision, 0.8064
INFO:root:train roc_auc, 0.7859
INFO:root:validate loss: 0.4785
INFO:root:validate average_precision, 0.8549
INFO:root:validate roc_auc, 0.8504
INFO:root:new node validate loss: 0.6989
INFO:root:new node validate first_1_average_precision, 0.5705
INFO:root:new node validate first_1_roc_auc, 0.5093
INFO:root:new node validate first_3_average_precision, 0.5977
INFO:root:new node validate first_3_roc_auc, 0.5676
INFO:root:new node validate first_10_average_precision, 0.6207
INFO:root:new node validate first_10_roc_auc, 0.6190
INFO:root:new node validate average_precision, 0.6665
INFO:root:new node validate roc_auc, 0.6688
INFO:root:save model ./saved_models/TGN/ia-escorts-dynamic/TGN_seed0_tgn-ia-escorts-dynamic-lincorrect-time-linear/TGN_seed0_tgn-ia-escorts-dynamic-lincorrect-time-linear.pkl
Epoch: 2, train for the 108-th batch, train loss: 0.5370217561721802:  72%|███████▊   | 108/151 [00:17<00:06,  6.21it/s]Epoch: 1, train for the 231-th batch, train loss: 0.5708793997764587:  97%|██████████▋| 231/237 [01:01<00:01,  3.45it/s]Epoch: 1, train for the 218-th batch, train loss: 0.4984152913093567:  57%|██████▏    | 217/383 [00:59<01:01,  2.70it/s]Epoch: 2, train for the 109-th batch, train loss: 0.4979501962661743:  72%|███████▊   | 108/151 [00:17<00:06,  6.21it/s]Epoch: 3, train for the 69-th batch, train loss: 0.48136815428733826:  57%|██████▊     | 68/119 [00:10<00:07,  6.50it/s]Epoch: 3, train for the 69-th batch, train loss: 0.48136815428733826:  58%|██████▉     | 69/119 [00:10<00:08,  5.65it/s]Epoch: 2, train for the 109-th batch, train loss: 0.4979501962661743:  72%|███████▉   | 109/151 [00:17<00:06,  6.56it/s]Epoch: 1, train for the 218-th batch, train loss: 0.4984152913093567:  57%|██████▎    | 218/383 [00:59<00:57,  2.87it/s]Epoch: 1, train for the 232-th batch, train loss: 0.586107611656189:  97%|███████████▋| 231/237 [01:01<00:01,  3.45it/s]Epoch: 3, train for the 70-th batch, train loss: 0.4128774106502533:  58%|███████▌     | 69/119 [00:10<00:08,  5.65it/s]Epoch: 1, train for the 232-th batch, train loss: 0.586107611656189:  98%|███████████▋| 232/237 [01:01<00:01,  3.84it/s]Epoch: 2, train for the 110-th batch, train loss: 0.5486568212509155:  72%|███████▉   | 109/151 [00:18<00:06,  6.56it/s]Epoch: 2, train for the 110-th batch, train loss: 0.5486568212509155:  73%|████████   | 110/151 [00:18<00:05,  6.93it/s]Epoch: 3, train for the 71-th batch, train loss: 0.4546615481376648:  58%|███████▌     | 69/119 [00:10<00:08,  5.65it/s]Epoch: 3, train for the 71-th batch, train loss: 0.4546615481376648:  60%|███████▊     | 71/119 [00:10<00:06,  7.28it/s]Epoch: 1, train for the 219-th batch, train loss: 0.3343454599380493:  57%|██████▎    | 218/383 [01:00<00:57,  2.87it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5413274765014648:  73%|████████   | 110/151 [00:18<00:05,  6.93it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5413274765014648:  74%|████████   | 111/151 [00:18<00:05,  7.30it/s]Epoch: 1, train for the 219-th batch, train loss: 0.3343454599380493:  57%|██████▎    | 219/383 [01:00<00:50,  3.25it/s]Epoch: 3, train for the 72-th batch, train loss: 0.5123577117919922:  60%|███████▊     | 71/119 [00:10<00:06,  7.28it/s]Epoch: 1, train for the 233-th batch, train loss: 0.5831287503242493:  98%|██████████▊| 232/237 [01:01<00:01,  3.84it/s]Epoch: 1, train for the 233-th batch, train loss: 0.5831287503242493:  98%|██████████▊| 233/237 [01:02<00:01,  3.81it/s]Epoch: 3, train for the 73-th batch, train loss: 0.47052574157714844:  60%|███████▏    | 71/119 [00:10<00:06,  7.28it/s]Epoch: 2, train for the 112-th batch, train loss: 0.4736495018005371:  74%|████████   | 111/151 [00:18<00:05,  7.30it/s]Epoch: 3, train for the 73-th batch, train loss: 0.47052574157714844:  61%|███████▎    | 73/119 [00:10<00:05,  8.35it/s]Epoch: 2, train for the 112-th batch, train loss: 0.4736495018005371:  74%|████████▏  | 112/151 [00:18<00:05,  7.40it/s]Epoch: 3, train for the 74-th batch, train loss: 0.5073919296264648:  61%|███████▉     | 73/119 [00:11<00:05,  8.35it/s]Epoch: 3, train for the 74-th batch, train loss: 0.5073919296264648:  62%|████████     | 74/119 [00:11<00:05,  8.58it/s]Epoch: 2, train for the 113-th batch, train loss: 0.531204879283905:  74%|████████▉   | 112/151 [00:18<00:05,  7.40it/s]Epoch: 2, train for the 113-th batch, train loss: 0.531204879283905:  75%|████████▉   | 113/151 [00:18<00:05,  7.21it/s]Epoch: 1, train for the 234-th batch, train loss: 0.5562204718589783:  98%|██████████▊| 233/237 [01:02<00:01,  3.81it/s]Epoch: 3, train for the 75-th batch, train loss: 0.4656561613082886:  62%|████████     | 74/119 [00:11<00:05,  8.58it/s]Epoch: 1, train for the 220-th batch, train loss: 0.39868563413619995:  57%|█████▋    | 219/383 [01:00<00:50,  3.25it/s]Epoch: 1, train for the 234-th batch, train loss: 0.5562204718589783:  99%|██████████▊| 234/237 [01:02<00:00,  3.92it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]Epoch: 1, train for the 220-th batch, train loss: 0.39868563413619995:  57%|█████▋    | 220/383 [01:00<00:53,  3.07it/s]Epoch: 2, train for the 114-th batch, train loss: 0.4957793653011322:  75%|████████▏  | 113/151 [00:18<00:05,  7.21it/s]Epoch: 2, train for the 114-th batch, train loss: 0.4957793653011322:  75%|████████▎  | 114/151 [00:18<00:05,  7.10it/s]Epoch: 3, train for the 76-th batch, train loss: 0.5018787384033203:  62%|████████     | 74/119 [00:11<00:05,  8.58it/s]Epoch: 3, train for the 76-th batch, train loss: 0.5018787384033203:  64%|████████▎    | 76/119 [00:11<00:05,  8.36it/s]Epoch: 3, train for the 1-th batch, train loss: 0.8794859647750854:   0%|                       | 0/146 [00:00<?, ?it/s]Epoch: 3, train for the 1-th batch, train loss: 0.8794859647750854:   1%|               | 1/146 [00:00<00:17,  8.19it/s]Epoch: 1, train for the 235-th batch, train loss: 0.5333093404769897:  99%|██████████▊| 234/237 [01:02<00:00,  3.92it/s]Epoch: 1, train for the 235-th batch, train loss: 0.5333093404769897:  99%|██████████▉| 235/237 [01:02<00:00,  4.08it/s]Epoch: 2, train for the 115-th batch, train loss: 0.4596244692802429:  75%|████████▎  | 114/151 [00:18<00:05,  7.10it/s]Epoch: 1, train for the 221-th batch, train loss: 0.4477849006652832:  57%|██████▎    | 220/383 [01:00<00:53,  3.07it/s]Epoch: 3, train for the 2-th batch, train loss: 0.8259812593460083:   1%|               | 1/146 [00:00<00:17,  8.19it/s]Epoch: 3, train for the 2-th batch, train loss: 0.8259812593460083:   1%|▏              | 2/146 [00:00<00:16,  8.54it/s]Epoch: 2, train for the 115-th batch, train loss: 0.4596244692802429:  76%|████████▍  | 115/151 [00:18<00:05,  6.72it/s]Epoch: 3, train for the 77-th batch, train loss: 0.4842485785484314:  64%|████████▎    | 76/119 [00:11<00:05,  8.36it/s]Epoch: 1, train for the 221-th batch, train loss: 0.4477849006652832:  58%|██████▎    | 221/383 [01:00<00:49,  3.26it/s]Epoch: 3, train for the 77-th batch, train loss: 0.4842485785484314:  65%|████████▍    | 77/119 [00:11<00:05,  7.70it/s]Epoch: 2, train for the 116-th batch, train loss: 0.4480911195278168:  76%|████████▍  | 115/151 [00:18<00:05,  6.72it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/241 [00:00<?, ?it/s]Epoch: 2, train for the 116-th batch, train loss: 0.4480911195278168:  77%|████████▍  | 116/151 [00:18<00:05,  6.88it/s]Epoch: 3, train for the 78-th batch, train loss: 0.5480877757072449:  65%|████████▍    | 77/119 [00:11<00:05,  7.70it/s]Epoch: 3, train for the 78-th batch, train loss: 0.5480877757072449:  66%|████████▌    | 78/119 [00:11<00:05,  7.76it/s]Epoch: 3, train for the 3-th batch, train loss: 0.7896302342414856:   1%|▏              | 2/146 [00:00<00:16,  8.54it/s]Epoch: 3, train for the 3-th batch, train loss: 0.7896302342414856:   2%|▎              | 3/146 [00:00<00:23,  6.09it/s]Epoch: 2, train for the 1-th batch, train loss: 0.5754666924476624:   0%|                       | 0/241 [00:00<?, ?it/s]Epoch: 1, train for the 236-th batch, train loss: 0.5419920086860657:  99%|██████████▉| 235/237 [01:02<00:00,  4.08it/s]Epoch: 2, train for the 117-th batch, train loss: 0.49870237708091736:  77%|███████▋  | 116/151 [00:19<00:05,  6.88it/s]Epoch: 3, train for the 79-th batch, train loss: 0.5019758343696594:  66%|████████▌    | 78/119 [00:11<00:05,  7.76it/s]Epoch: 2, train for the 117-th batch, train loss: 0.49870237708091736:  77%|███████▋  | 117/151 [00:19<00:05,  6.71it/s]Epoch: 3, train for the 79-th batch, train loss: 0.5019758343696594:  66%|████████▋    | 79/119 [00:11<00:05,  7.81it/s]Epoch: 1, train for the 236-th batch, train loss: 0.5419920086860657: 100%|██████████▉| 236/237 [01:02<00:00,  3.62it/s]Epoch: 2, train for the 2-th batch, train loss: 0.4254573583602905:   0%|                       | 0/241 [00:00<?, ?it/s]Epoch: 2, train for the 2-th batch, train loss: 0.4254573583602905:   1%|               | 2/241 [00:00<00:22, 10.86it/s]Epoch: 3, train for the 4-th batch, train loss: 0.7629114389419556:   2%|▎              | 3/146 [00:00<00:23,  6.09it/s]Epoch: 3, train for the 4-th batch, train loss: 0.7629114389419556:   3%|▍              | 4/146 [00:00<00:20,  6.76it/s]Epoch: 1, train for the 222-th batch, train loss: 0.49385568499565125:  58%|█████▊    | 221/383 [01:01<00:49,  3.26it/s]Epoch: 3, train for the 80-th batch, train loss: 0.5339195132255554:  66%|████████▋    | 79/119 [00:11<00:05,  7.81it/s]Epoch: 3, train for the 80-th batch, train loss: 0.5339195132255554:  67%|████████▋    | 80/119 [00:11<00:05,  7.54it/s]Epoch: 2, train for the 3-th batch, train loss: 0.470688134431839:   1%|▏               | 2/241 [00:00<00:22, 10.86it/s]Epoch: 1, train for the 222-th batch, train loss: 0.49385568499565125:  58%|█████▊    | 222/383 [01:01<00:54,  2.93it/s]Epoch: 1, train for the 237-th batch, train loss: 0.5096003413200378: 100%|██████████▉| 236/237 [01:02<00:00,  3.62it/s]Epoch: 3, train for the 5-th batch, train loss: 0.7253842949867249:   3%|▍              | 4/146 [00:00<00:20,  6.76it/s]Epoch: 3, train for the 5-th batch, train loss: 0.7253842949867249:   3%|▌              | 5/146 [00:00<00:20,  6.86it/s]Epoch: 2, train for the 118-th batch, train loss: 0.4652823507785797:  77%|████████▌  | 117/151 [00:19<00:05,  6.71it/s]Epoch: 1, train for the 237-th batch, train loss: 0.5096003413200378: 100%|███████████| 237/237 [01:03<00:00,  3.92it/s]Epoch: 1, train for the 237-th batch, train loss: 0.5096003413200378: 100%|███████████| 237/237 [01:03<00:00,  3.76it/s]
Epoch: 2, train for the 118-th batch, train loss: 0.4652823507785797:  78%|████████▌  | 118/151 [00:19<00:05,  5.76it/s]Epoch: 3, train for the 81-th batch, train loss: 0.42589065432548523:  67%|████████    | 80/119 [00:11<00:05,  7.54it/s]Epoch: 2, train for the 4-th batch, train loss: 0.4408891201019287:   1%|               | 2/241 [00:00<00:22, 10.86it/s]Epoch: 2, train for the 4-th batch, train loss: 0.4408891201019287:   2%|▏              | 4/241 [00:00<00:26,  9.10it/s]Epoch: 3, train for the 81-th batch, train loss: 0.42589065432548523:  68%|████████▏   | 81/119 [00:11<00:05,  7.46it/s]Epoch: 3, train for the 6-th batch, train loss: 0.6869081258773804:   3%|▌              | 5/146 [00:00<00:20,  6.86it/s]Epoch: 3, train for the 6-th batch, train loss: 0.6869081258773804:   4%|▌              | 6/146 [00:00<00:18,  7.44it/s]Epoch: 2, train for the 119-th batch, train loss: 0.5070009231567383:  78%|████████▌  | 118/151 [00:19<00:05,  5.76it/s]Epoch: 2, train for the 119-th batch, train loss: 0.5070009231567383:  79%|████████▋  | 119/151 [00:19<00:05,  5.95it/s]Epoch: 2, train for the 5-th batch, train loss: 0.47035202383995056:   2%|▏             | 4/241 [00:00<00:26,  9.10it/s]Epoch: 2, train for the 5-th batch, train loss: 0.47035202383995056:   2%|▎             | 5/241 [00:00<00:27,  8.70it/s]Epoch: 1, train for the 223-th batch, train loss: 0.43366050720214844:  58%|█████▊    | 222/383 [01:01<00:54,  2.93it/s]Epoch: 3, train for the 82-th batch, train loss: 0.4968263804912567:  68%|████████▊    | 81/119 [00:12<00:05,  7.46it/s]Epoch: 3, train for the 82-th batch, train loss: 0.4968263804912567:  69%|████████▉    | 82/119 [00:12<00:05,  7.11it/s]Epoch: 1, train for the 223-th batch, train loss: 0.43366050720214844:  58%|█████▊    | 223/383 [01:01<00:51,  3.12it/s]Epoch: 3, train for the 7-th batch, train loss: 0.6779837012290955:   4%|▌              | 6/146 [00:00<00:18,  7.44it/s]Epoch: 3, train for the 7-th batch, train loss: 0.6779837012290955:   5%|▋              | 7/146 [00:00<00:20,  6.93it/s]Epoch: 2, train for the 6-th batch, train loss: 0.619594156742096:   2%|▎               | 5/241 [00:00<00:27,  8.70it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5226854085922241:  79%|████████▋  | 119/151 [00:19<00:05,  5.95it/s]Epoch: 2, train for the 6-th batch, train loss: 0.619594156742096:   2%|▍               | 6/241 [00:00<00:31,  7.51it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5226854085922241:  79%|████████▋  | 120/151 [00:19<00:05,  5.61it/s]  0%|                                                                                            | 0/66 [00:00<?, ?it/s]Epoch: 3, train for the 83-th batch, train loss: 0.4701012670993805:  69%|████████▉    | 82/119 [00:12<00:05,  7.11it/s]Epoch: 3, train for the 83-th batch, train loss: 0.4701012670993805:  70%|█████████    | 83/119 [00:12<00:05,  6.69it/s]Epoch: 3, train for the 8-th batch, train loss: 0.6655279397964478:   5%|▋              | 7/146 [00:01<00:20,  6.93it/s]Epoch: 3, train for the 8-th batch, train loss: 0.6655279397964478:   5%|▊              | 8/146 [00:01<00:20,  6.71it/s]Epoch: 1, train for the 224-th batch, train loss: 0.5070871114730835:  58%|██████▍    | 223/383 [01:01<00:51,  3.12it/s]Epoch: 2, train for the 7-th batch, train loss: 0.4113599956035614:   2%|▎              | 6/241 [00:00<00:31,  7.51it/s]Epoch: 2, train for the 7-th batch, train loss: 0.4113599956035614:   3%|▍              | 7/241 [00:00<00:29,  8.05it/s]evaluate for the 1-th batch, evaluate loss: 0.5623487830162048:   0%|                            | 0/66 [00:00<?, ?it/s]Epoch: 1, train for the 224-th batch, train loss: 0.5070871114730835:  58%|██████▍    | 224/383 [01:01<00:48,  3.25it/s]Epoch: 2, train for the 121-th batch, train loss: 0.5081379413604736:  79%|████████▋  | 120/151 [00:19<00:05,  5.61it/s]Epoch: 3, train for the 84-th batch, train loss: 0.49987441301345825:  70%|████████▎   | 83/119 [00:12<00:05,  6.69it/s]Epoch: 2, train for the 121-th batch, train loss: 0.5081379413604736:  80%|████████▊  | 121/151 [00:19<00:05,  5.64it/s]Epoch: 3, train for the 84-th batch, train loss: 0.49987441301345825:  71%|████████▍   | 84/119 [00:12<00:05,  6.68it/s]Epoch: 2, train for the 8-th batch, train loss: 0.657996654510498:   3%|▍               | 7/241 [00:00<00:29,  8.05it/s]Epoch: 3, train for the 9-th batch, train loss: 0.6439936757087708:   5%|▊              | 8/146 [00:01<00:20,  6.71it/s]Epoch: 2, train for the 8-th batch, train loss: 0.657996654510498:   3%|▌               | 8/241 [00:00<00:27,  8.33it/s]Epoch: 3, train for the 9-th batch, train loss: 0.6439936757087708:   6%|▉              | 9/146 [00:01<00:21,  6.46it/s]Epoch: 3, train for the 85-th batch, train loss: 0.5089042782783508:  71%|█████████▏   | 84/119 [00:12<00:05,  6.68it/s]Epoch: 3, train for the 85-th batch, train loss: 0.5089042782783508:  71%|█████████▎   | 85/119 [00:12<00:05,  6.51it/s]Epoch: 2, train for the 122-th batch, train loss: 0.47214263677597046:  80%|████████  | 121/151 [00:19<00:05,  5.64it/s]Epoch: 3, train for the 10-th batch, train loss: 0.6473503112792969:   6%|▊             | 9/146 [00:01<00:21,  6.46it/s]Epoch: 2, train for the 9-th batch, train loss: 0.5961138010025024:   3%|▍              | 8/241 [00:01<00:27,  8.33it/s]Epoch: 3, train for the 10-th batch, train loss: 0.6473503112792969:   7%|▉            | 10/146 [00:01<00:21,  6.39it/s]Epoch: 2, train for the 122-th batch, train loss: 0.47214263677597046:  81%|████████  | 122/151 [00:19<00:05,  5.53it/s]Epoch: 2, train for the 9-th batch, train loss: 0.5961138010025024:   4%|▌              | 9/241 [00:01<00:31,  7.45it/s]Epoch: 1, train for the 225-th batch, train loss: 0.43153947591781616:  58%|█████▊    | 224/383 [01:01<00:48,  3.25it/s]evaluate for the 2-th batch, evaluate loss: 0.5165041089057922:   0%|                            | 0/66 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5165041089057922:   3%|▌                   | 2/66 [00:00<00:12,  5.17it/s]Epoch: 1, train for the 225-th batch, train loss: 0.43153947591781616:  59%|█████▊    | 225/383 [01:02<00:49,  3.21it/s]Epoch: 3, train for the 86-th batch, train loss: 0.5414735674858093:  71%|█████████▎   | 85/119 [00:12<00:05,  6.51it/s]Epoch: 3, train for the 86-th batch, train loss: 0.5414735674858093:  72%|█████████▍   | 86/119 [00:12<00:04,  6.74it/s]evaluate for the 3-th batch, evaluate loss: 0.560097336769104:   3%|▋                    | 2/66 [00:00<00:12,  5.17it/s]Epoch: 2, train for the 10-th batch, train loss: 0.5377066135406494:   4%|▌             | 9/241 [00:01<00:31,  7.45it/s]Epoch: 3, train for the 11-th batch, train loss: 0.6443718075752258:   7%|▉            | 10/146 [00:01<00:21,  6.39it/s]Epoch: 2, train for the 10-th batch, train loss: 0.5377066135406494:   4%|▌            | 10/241 [00:01<00:31,  7.35it/s]Epoch: 3, train for the 11-th batch, train loss: 0.6443718075752258:   8%|▉            | 11/146 [00:01<00:20,  6.49it/s]Epoch: 2, train for the 123-th batch, train loss: 0.4906825125217438:  81%|████████▉  | 122/151 [00:20<00:05,  5.53it/s]Epoch: 2, train for the 123-th batch, train loss: 0.4906825125217438:  81%|████████▉  | 123/151 [00:20<00:05,  5.23it/s]evaluate for the 4-th batch, evaluate loss: 0.5265436172485352:   3%|▌                   | 2/66 [00:00<00:12,  5.17it/s]evaluate for the 4-th batch, evaluate loss: 0.5265436172485352:   6%|█▏                  | 4/66 [00:00<00:08,  7.23it/s]Epoch: 3, train for the 87-th batch, train loss: 0.49463754892349243:  72%|████████▋   | 86/119 [00:12<00:04,  6.74it/s]Epoch: 3, train for the 87-th batch, train loss: 0.49463754892349243:  73%|████████▊   | 87/119 [00:12<00:04,  6.71it/s]Epoch: 2, train for the 11-th batch, train loss: 0.3670446574687958:   4%|▌            | 10/241 [00:01<00:31,  7.35it/s]Epoch: 2, train for the 11-th batch, train loss: 0.3670446574687958:   5%|▌            | 11/241 [00:01<00:31,  7.31it/s]Epoch: 3, train for the 12-th batch, train loss: 0.6438437700271606:   8%|▉            | 11/146 [00:01<00:20,  6.49it/s]Epoch: 3, train for the 12-th batch, train loss: 0.6438437700271606:   8%|█            | 12/146 [00:01<00:20,  6.57it/s]evaluate for the 5-th batch, evaluate loss: 0.5481334328651428:   6%|█▏                  | 4/66 [00:00<00:08,  7.23it/s]evaluate for the 5-th batch, evaluate loss: 0.5481334328651428:   8%|█▌                  | 5/66 [00:00<00:08,  7.26it/s]Epoch: 2, train for the 124-th batch, train loss: 0.5140572190284729:  81%|████████▉  | 123/151 [00:20<00:05,  5.23it/s]Epoch: 2, train for the 12-th batch, train loss: 0.5409549474716187:   5%|▌            | 11/241 [00:01<00:31,  7.31it/s]Epoch: 2, train for the 124-th batch, train loss: 0.5140572190284729:  82%|█████████  | 124/151 [00:20<00:05,  5.06it/s]Epoch: 3, train for the 88-th batch, train loss: 0.5279262661933899:  73%|█████████▌   | 87/119 [00:13<00:04,  6.71it/s]Epoch: 2, train for the 12-th batch, train loss: 0.5409549474716187:   5%|▋            | 12/241 [00:01<00:32,  7.13it/s]Epoch: 3, train for the 88-th batch, train loss: 0.5279262661933899:  74%|█████████▌   | 88/119 [00:13<00:04,  6.34it/s]Epoch: 3, train for the 13-th batch, train loss: 0.6526846289634705:   8%|█            | 12/146 [00:01<00:20,  6.57it/s]Epoch: 1, train for the 226-th batch, train loss: 0.48993074893951416:  59%|█████▊    | 225/383 [01:02<00:49,  3.21it/s]Epoch: 3, train for the 13-th batch, train loss: 0.6526846289634705:   9%|█▏           | 13/146 [00:01<00:20,  6.50it/s]Epoch: 1, train for the 226-th batch, train loss: 0.48993074893951416:  59%|█████▉    | 226/383 [01:02<00:53,  2.91it/s]evaluate for the 6-th batch, evaluate loss: 0.5725370049476624:   8%|█▌                  | 5/66 [00:00<00:08,  7.26it/s]evaluate for the 6-th batch, evaluate loss: 0.5725370049476624:   9%|█▊                  | 6/66 [00:00<00:09,  6.41it/s]Epoch: 2, train for the 13-th batch, train loss: 0.5240763425827026:   5%|▋            | 12/241 [00:01<00:32,  7.13it/s]Epoch: 2, train for the 13-th batch, train loss: 0.5240763425827026:   5%|▋            | 13/241 [00:01<00:33,  6.89it/s]Epoch: 2, train for the 125-th batch, train loss: 0.5249612331390381:  82%|█████████  | 124/151 [00:20<00:05,  5.06it/s]Epoch: 3, train for the 89-th batch, train loss: 0.5223645567893982:  74%|█████████▌   | 88/119 [00:13<00:04,  6.34it/s]Epoch: 3, train for the 14-th batch, train loss: 0.6417328715324402:   9%|█▏           | 13/146 [00:02<00:20,  6.50it/s]Epoch: 3, train for the 14-th batch, train loss: 0.6417328715324402:  10%|█▏           | 14/146 [00:02<00:21,  6.27it/s]Epoch: 3, train for the 89-th batch, train loss: 0.5223645567893982:  75%|█████████▋   | 89/119 [00:13<00:05,  5.92it/s]Epoch: 2, train for the 125-th batch, train loss: 0.5249612331390381:  83%|█████████  | 125/151 [00:20<00:05,  4.97it/s]evaluate for the 7-th batch, evaluate loss: 0.5444924235343933:   9%|█▊                  | 6/66 [00:01<00:09,  6.41it/s]evaluate for the 7-th batch, evaluate loss: 0.5444924235343933:  11%|██                  | 7/66 [00:01<00:08,  6.62it/s]Epoch: 2, train for the 14-th batch, train loss: 0.5978098511695862:   5%|▋            | 13/241 [00:01<00:33,  6.89it/s]Epoch: 2, train for the 14-th batch, train loss: 0.5978098511695862:   6%|▊            | 14/241 [00:01<00:32,  7.04it/s]Epoch: 3, train for the 15-th batch, train loss: 0.6534122824668884:  10%|█▏           | 14/146 [00:02<00:21,  6.27it/s]Epoch: 3, train for the 15-th batch, train loss: 0.6534122824668884:  10%|█▎           | 15/146 [00:02<00:18,  7.03it/s]Epoch: 3, train for the 90-th batch, train loss: 0.4883621037006378:  75%|█████████▋   | 89/119 [00:13<00:05,  5.92it/s]Epoch: 2, train for the 126-th batch, train loss: 0.5133641958236694:  83%|█████████  | 125/151 [00:20<00:05,  4.97it/s]Epoch: 3, train for the 90-th batch, train loss: 0.4883621037006378:  76%|█████████▊   | 90/119 [00:13<00:04,  6.00it/s]Epoch: 2, train for the 126-th batch, train loss: 0.5133641958236694:  83%|█████████▏ | 126/151 [00:20<00:04,  5.24it/s]Epoch: 2, train for the 15-th batch, train loss: 0.5303505063056946:   6%|▊            | 14/241 [00:01<00:32,  7.04it/s]Epoch: 2, train for the 15-th batch, train loss: 0.5303505063056946:   6%|▊            | 15/241 [00:01<00:30,  7.53it/s]Epoch: 3, train for the 16-th batch, train loss: 0.6568410992622375:  10%|█▎           | 15/146 [00:02<00:18,  7.03it/s]Epoch: 3, train for the 16-th batch, train loss: 0.6568410992622375:  11%|█▍           | 16/146 [00:02<00:17,  7.42it/s]Epoch: 3, train for the 91-th batch, train loss: 0.47547683119773865:  76%|█████████   | 90/119 [00:13<00:04,  6.00it/s]Epoch: 3, train for the 91-th batch, train loss: 0.47547683119773865:  76%|█████████▏  | 91/119 [00:13<00:04,  6.53it/s]Epoch: 2, train for the 127-th batch, train loss: 0.4831368923187256:  83%|█████████▏ | 126/151 [00:20<00:04,  5.24it/s]Epoch: 2, train for the 127-th batch, train loss: 0.4831368923187256:  84%|█████████▎ | 127/151 [00:20<00:04,  5.47it/s]Epoch: 3, train for the 17-th batch, train loss: 0.6375287771224976:  11%|█▍           | 16/146 [00:02<00:17,  7.42it/s]Epoch: 3, train for the 17-th batch, train loss: 0.6375287771224976:  12%|█▌           | 17/146 [00:02<00:17,  7.48it/s]Epoch: 1, train for the 227-th batch, train loss: 0.5265859961509705:  59%|██████▍    | 226/383 [01:02<00:53,  2.91it/s]evaluate for the 8-th batch, evaluate loss: 0.5374395847320557:  11%|██                  | 7/66 [00:01<00:08,  6.62it/s]evaluate for the 8-th batch, evaluate loss: 0.5374395847320557:  12%|██▍                 | 8/66 [00:01<00:11,  4.93it/s]Epoch: 3, train for the 92-th batch, train loss: 0.5216765403747559:  76%|█████████▉   | 91/119 [00:13<00:04,  6.53it/s]Epoch: 3, train for the 92-th batch, train loss: 0.5216765403747559:  77%|██████████   | 92/119 [00:13<00:03,  6.91it/s]Epoch: 2, train for the 16-th batch, train loss: 0.45302897691726685:   6%|▋           | 15/241 [00:02<00:30,  7.53it/s]Epoch: 1, train for the 227-th batch, train loss: 0.5265859961509705:  59%|██████▌    | 227/383 [01:03<01:03,  2.45it/s]Epoch: 2, train for the 16-th batch, train loss: 0.45302897691726685:   7%|▊           | 16/241 [00:02<00:36,  6.14it/s]Epoch: 3, train for the 18-th batch, train loss: 0.6247045993804932:  12%|█▌           | 17/146 [00:02<00:17,  7.48it/s]Epoch: 3, train for the 18-th batch, train loss: 0.6247045993804932:  12%|█▌           | 18/146 [00:02<00:16,  7.86it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5466787815093994:  84%|█████████▎ | 127/151 [00:21<00:04,  5.47it/s]evaluate for the 9-th batch, evaluate loss: 0.5087752938270569:  12%|██▍                 | 8/66 [00:01<00:11,  4.93it/s]evaluate for the 9-th batch, evaluate loss: 0.5087752938270569:  14%|██▋                 | 9/66 [00:01<00:09,  5.71it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5466787815093994:  85%|█████████▎ | 128/151 [00:21<00:04,  5.51it/s]Epoch: 3, train for the 93-th batch, train loss: 0.4688750207424164:  77%|██████████   | 92/119 [00:13<00:03,  6.91it/s]Epoch: 3, train for the 93-th batch, train loss: 0.4688750207424164:  78%|██████████▏  | 93/119 [00:13<00:03,  6.89it/s]Epoch: 2, train for the 17-th batch, train loss: 0.41582372784614563:   7%|▊           | 16/241 [00:02<00:36,  6.14it/s]Epoch: 2, train for the 17-th batch, train loss: 0.41582372784614563:   7%|▊           | 17/241 [00:02<00:35,  6.36it/s]Epoch: 3, train for the 19-th batch, train loss: 0.6271634697914124:  12%|█▌           | 18/146 [00:02<00:16,  7.86it/s]Epoch: 3, train for the 19-th batch, train loss: 0.6271634697914124:  13%|█▋           | 19/146 [00:02<00:16,  7.65it/s]Epoch: 1, train for the 228-th batch, train loss: 0.4266708195209503:  59%|██████▌    | 227/383 [01:03<01:03,  2.45it/s]Epoch: 2, train for the 129-th batch, train loss: 0.5586571097373962:  85%|█████████▎ | 128/151 [00:21<00:04,  5.51it/s]Epoch: 1, train for the 228-th batch, train loss: 0.4266708195209503:  60%|██████▌    | 228/383 [01:03<00:56,  2.76it/s]Epoch: 2, train for the 129-th batch, train loss: 0.5586571097373962:  85%|█████████▍ | 129/151 [00:21<00:03,  5.56it/s]Epoch: 2, train for the 18-th batch, train loss: 0.47852298617362976:   7%|▊           | 17/241 [00:02<00:35,  6.36it/s]Epoch: 3, train for the 94-th batch, train loss: 0.4435599446296692:  78%|██████████▏  | 93/119 [00:13<00:03,  6.89it/s]Epoch: 2, train for the 18-th batch, train loss: 0.47852298617362976:   7%|▉           | 18/241 [00:02<00:32,  6.81it/s]Epoch: 3, train for the 94-th batch, train loss: 0.4435599446296692:  79%|██████████▎  | 94/119 [00:13<00:03,  6.84it/s]Epoch: 3, train for the 20-th batch, train loss: 0.6119135022163391:  13%|█▋           | 19/146 [00:02<00:16,  7.65it/s]Epoch: 3, train for the 20-th batch, train loss: 0.6119135022163391:  14%|█▊           | 20/146 [00:02<00:17,  7.38it/s]evaluate for the 10-th batch, evaluate loss: 0.5108892917633057:  14%|██▌                | 9/66 [00:01<00:09,  5.71it/s]evaluate for the 10-th batch, evaluate loss: 0.5108892917633057:  15%|██▋               | 10/66 [00:01<00:12,  4.61it/s]Epoch: 2, train for the 19-th batch, train loss: 0.48145487904548645:   7%|▉           | 18/241 [00:02<00:32,  6.81it/s]Epoch: 2, train for the 19-th batch, train loss: 0.48145487904548645:   8%|▉           | 19/241 [00:02<00:32,  6.80it/s]Epoch: 3, train for the 95-th batch, train loss: 0.43893107771873474:  79%|█████████▍  | 94/119 [00:14<00:03,  6.84it/s]Epoch: 2, train for the 130-th batch, train loss: 0.510281503200531:  85%|██████████▎ | 129/151 [00:21<00:03,  5.56it/s]Epoch: 3, train for the 95-th batch, train loss: 0.43893107771873474:  80%|█████████▌  | 95/119 [00:14<00:03,  6.30it/s]Epoch: 3, train for the 21-th batch, train loss: 0.5995029807090759:  14%|█▊           | 20/146 [00:03<00:17,  7.38it/s]Epoch: 3, train for the 21-th batch, train loss: 0.5995029807090759:  14%|█▊           | 21/146 [00:03<00:18,  6.93it/s]Epoch: 2, train for the 130-th batch, train loss: 0.510281503200531:  86%|██████████▎ | 130/151 [00:21<00:04,  5.22it/s]Epoch: 1, train for the 229-th batch, train loss: 0.37258657813072205:  60%|█████▉    | 228/383 [01:03<00:56,  2.76it/s]evaluate for the 11-th batch, evaluate loss: 0.5152700543403625:  15%|██▋               | 10/66 [00:01<00:12,  4.61it/s]evaluate for the 11-th batch, evaluate loss: 0.5152700543403625:  17%|███               | 11/66 [00:01<00:10,  5.38it/s]Epoch: 2, train for the 20-th batch, train loss: 0.38682904839515686:   8%|▉           | 19/241 [00:02<00:32,  6.80it/s]Epoch: 2, train for the 20-th batch, train loss: 0.38682904839515686:   8%|▉           | 20/241 [00:02<00:29,  7.37it/s]Epoch: 1, train for the 229-th batch, train loss: 0.37258657813072205:  60%|█████▉    | 229/383 [01:03<00:53,  2.87it/s]evaluate for the 12-th batch, evaluate loss: 0.5375550985336304:  17%|███               | 11/66 [00:02<00:10,  5.38it/s]evaluate for the 12-th batch, evaluate loss: 0.5375550985336304:  18%|███▎              | 12/66 [00:02<00:08,  6.23it/s]Epoch: 3, train for the 22-th batch, train loss: 0.6049817800521851:  14%|█▊           | 21/146 [00:03<00:18,  6.93it/s]Epoch: 3, train for the 22-th batch, train loss: 0.6049817800521851:  15%|█▉           | 22/146 [00:03<00:17,  7.01it/s]Epoch: 3, train for the 96-th batch, train loss: 0.45970916748046875:  80%|█████████▌  | 95/119 [00:14<00:03,  6.30it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5310304164886475:  86%|█████████▍ | 130/151 [00:21<00:04,  5.22it/s]Epoch: 3, train for the 96-th batch, train loss: 0.45970916748046875:  81%|█████████▋  | 96/119 [00:14<00:03,  6.01it/s]Epoch: 2, train for the 21-th batch, train loss: 0.627295196056366:   8%|█▏            | 20/241 [00:02<00:29,  7.37it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5310304164886475:  87%|█████████▌ | 131/151 [00:21<00:03,  5.24it/s]Epoch: 2, train for the 21-th batch, train loss: 0.627295196056366:   9%|█▏            | 21/241 [00:02<00:30,  7.10it/s]evaluate for the 13-th batch, evaluate loss: 0.5495876669883728:  18%|███▎              | 12/66 [00:02<00:08,  6.23it/s]evaluate for the 13-th batch, evaluate loss: 0.5495876669883728:  20%|███▌              | 13/66 [00:02<00:07,  7.01it/s]Epoch: 3, train for the 23-th batch, train loss: 0.6105760931968689:  15%|█▉           | 22/146 [00:03<00:17,  7.01it/s]Epoch: 3, train for the 23-th batch, train loss: 0.6105760931968689:  16%|██           | 23/146 [00:03<00:16,  7.57it/s]Epoch: 3, train for the 97-th batch, train loss: 0.48144954442977905:  81%|█████████▋  | 96/119 [00:14<00:03,  6.01it/s]Epoch: 3, train for the 97-th batch, train loss: 0.48144954442977905:  82%|█████████▊  | 97/119 [00:14<00:03,  6.02it/s]Epoch: 2, train for the 22-th batch, train loss: 0.5317378640174866:   9%|█▏           | 21/241 [00:02<00:30,  7.10it/s]Epoch: 3, train for the 24-th batch, train loss: 0.5956180691719055:  16%|██           | 23/146 [00:03<00:16,  7.57it/s]Epoch: 3, train for the 24-th batch, train loss: 0.5956180691719055:  16%|██▏          | 24/146 [00:03<00:15,  7.93it/s]Epoch: 2, train for the 22-th batch, train loss: 0.5317378640174866:   9%|█▏           | 22/241 [00:03<00:31,  6.88it/s]Epoch: 2, train for the 132-th batch, train loss: 0.503052830696106:  87%|██████████▍ | 131/151 [00:21<00:03,  5.24it/s]Epoch: 2, train for the 132-th batch, train loss: 0.503052830696106:  87%|██████████▍ | 132/151 [00:21<00:03,  4.89it/s]Epoch: 1, train for the 230-th batch, train loss: 0.44990670680999756:  60%|█████▉    | 229/383 [01:03<00:53,  2.87it/s]evaluate for the 14-th batch, evaluate loss: 0.5395950078964233:  20%|███▌              | 13/66 [00:02<00:07,  7.01it/s]evaluate for the 14-th batch, evaluate loss: 0.5395950078964233:  21%|███▊              | 14/66 [00:02<00:08,  5.89it/s]Epoch: 2, train for the 23-th batch, train loss: 0.5062803030014038:   9%|█▏           | 22/241 [00:03<00:31,  6.88it/s]Epoch: 2, train for the 23-th batch, train loss: 0.5062803030014038:  10%|█▏           | 23/241 [00:03<00:29,  7.34it/s]Epoch: 3, train for the 98-th batch, train loss: 0.44616079330444336:  82%|█████████▊  | 97/119 [00:14<00:03,  6.02it/s]Epoch: 3, train for the 98-th batch, train loss: 0.44616079330444336:  82%|█████████▉  | 98/119 [00:14<00:03,  6.09it/s]Epoch: 3, train for the 25-th batch, train loss: 0.5924100875854492:  16%|██▏          | 24/146 [00:03<00:15,  7.93it/s]Epoch: 1, train for the 230-th batch, train loss: 0.44990670680999756:  60%|██████    | 230/383 [01:03<00:56,  2.72it/s]Epoch: 3, train for the 25-th batch, train loss: 0.5924100875854492:  17%|██▏          | 25/146 [00:03<00:16,  7.39it/s]evaluate for the 15-th batch, evaluate loss: 0.522092878818512:  21%|████               | 14/66 [00:02<00:08,  5.89it/s]Epoch: 2, train for the 133-th batch, train loss: 0.4536759853363037:  87%|█████████▌ | 132/151 [00:22<00:03,  4.89it/s]Epoch: 2, train for the 24-th batch, train loss: 0.5334284901618958:  10%|█▏           | 23/241 [00:03<00:29,  7.34it/s]Epoch: 2, train for the 133-th batch, train loss: 0.4536759853363037:  88%|█████████▋ | 133/151 [00:22<00:03,  4.99it/s]Epoch: 2, train for the 24-th batch, train loss: 0.5334284901618958:  10%|█▎           | 24/241 [00:03<00:30,  7.15it/s]Epoch: 3, train for the 99-th batch, train loss: 0.47078028321266174:  82%|█████████▉  | 98/119 [00:14<00:03,  6.09it/s]Epoch: 3, train for the 99-th batch, train loss: 0.47078028321266174:  83%|█████████▉  | 99/119 [00:14<00:03,  6.19it/s]Epoch: 3, train for the 26-th batch, train loss: 0.5467756390571594:  17%|██▏          | 25/146 [00:03<00:16,  7.39it/s]Epoch: 3, train for the 26-th batch, train loss: 0.5467756390571594:  18%|██▎          | 26/146 [00:03<00:17,  7.01it/s]Epoch: 1, train for the 231-th batch, train loss: 0.46301043033599854:  60%|██████    | 230/383 [01:04<00:56,  2.72it/s]evaluate for the 16-th batch, evaluate loss: 0.5010406374931335:  21%|███▊              | 14/66 [00:02<00:08,  5.89it/s]evaluate for the 16-th batch, evaluate loss: 0.5010406374931335:  24%|████▎             | 16/66 [00:02<00:07,  6.49it/s]Epoch: 1, train for the 231-th batch, train loss: 0.46301043033599854:  60%|██████    | 231/383 [01:04<00:50,  3.00it/s]Epoch: 2, train for the 25-th batch, train loss: 0.5928179621696472:  10%|█▎           | 24/241 [00:03<00:30,  7.15it/s]Epoch: 2, train for the 25-th batch, train loss: 0.5928179621696472:  10%|█▎           | 25/241 [00:03<00:31,  6.92it/s]Epoch: 2, train for the 134-th batch, train loss: 0.48614194989204407:  88%|████████▊ | 133/151 [00:22<00:03,  4.99it/s]evaluate for the 17-th batch, evaluate loss: 0.5204443335533142:  24%|████▎             | 16/66 [00:02<00:07,  6.49it/s]Epoch: 2, train for the 134-th batch, train loss: 0.48614194989204407:  89%|████████▊ | 134/151 [00:22<00:03,  4.86it/s]Epoch: 3, train for the 27-th batch, train loss: 0.5704354047775269:  18%|██▎          | 26/146 [00:03<00:17,  7.01it/s]Epoch: 3, train for the 100-th batch, train loss: 0.485340416431427:  83%|██████████▊  | 99/119 [00:15<00:03,  6.19it/s]Epoch: 3, train for the 27-th batch, train loss: 0.5704354047775269:  18%|██▍          | 27/146 [00:03<00:18,  6.37it/s]Epoch: 3, train for the 100-th batch, train loss: 0.485340416431427:  84%|██████████  | 100/119 [00:15<00:03,  5.58it/s]Epoch: 2, train for the 26-th batch, train loss: 0.642297625541687:  10%|█▍            | 25/241 [00:03<00:31,  6.92it/s]Epoch: 2, train for the 26-th batch, train loss: 0.642297625541687:  11%|█▌            | 26/241 [00:03<00:29,  7.20it/s]evaluate for the 18-th batch, evaluate loss: 0.522256076335907:  24%|████▌              | 16/66 [00:02<00:07,  6.49it/s]evaluate for the 18-th batch, evaluate loss: 0.522256076335907:  27%|█████▏             | 18/66 [00:02<00:06,  6.97it/s]Epoch: 3, train for the 28-th batch, train loss: 0.5458813905715942:  18%|██▍          | 27/146 [00:04<00:18,  6.37it/s]Epoch: 1, train for the 232-th batch, train loss: 0.38863417506217957:  60%|██████    | 231/383 [01:04<00:50,  3.00it/s]Epoch: 3, train for the 28-th batch, train loss: 0.5458813905715942:  19%|██▍          | 28/146 [00:04<00:17,  6.62it/s]Epoch: 2, train for the 135-th batch, train loss: 0.4615485966205597:  89%|█████████▊ | 134/151 [00:22<00:03,  4.86it/s]Epoch: 2, train for the 27-th batch, train loss: 0.6033129096031189:  11%|█▍           | 26/241 [00:03<00:29,  7.20it/s]Epoch: 3, train for the 101-th batch, train loss: 0.46957841515541077:  84%|████████▍ | 100/119 [00:15<00:03,  5.58it/s]Epoch: 2, train for the 27-th batch, train loss: 0.6033129096031189:  11%|█▍           | 27/241 [00:03<00:30,  7.08it/s]Epoch: 2, train for the 135-th batch, train loss: 0.4615485966205597:  89%|█████████▊ | 135/151 [00:22<00:03,  4.81it/s]Epoch: 1, train for the 232-th batch, train loss: 0.38863417506217957:  61%|██████    | 232/383 [01:04<00:48,  3.12it/s]Epoch: 3, train for the 101-th batch, train loss: 0.46957841515541077:  85%|████████▍ | 101/119 [00:15<00:03,  5.54it/s]evaluate for the 19-th batch, evaluate loss: 0.4968700408935547:  27%|████▉             | 18/66 [00:02<00:06,  6.97it/s]Epoch: 3, train for the 29-th batch, train loss: 0.5161439776420593:  19%|██▍          | 28/146 [00:04<00:17,  6.62it/s]Epoch: 3, train for the 29-th batch, train loss: 0.5161439776420593:  20%|██▌          | 29/146 [00:04<00:16,  7.10it/s]Epoch: 3, train for the 102-th batch, train loss: 0.5379666686058044:  85%|█████████▎ | 101/119 [00:15<00:03,  5.54it/s]Epoch: 2, train for the 28-th batch, train loss: 0.45345932245254517:  11%|█▎          | 27/241 [00:03<00:30,  7.08it/s]Epoch: 2, train for the 28-th batch, train loss: 0.45345932245254517:  12%|█▍          | 28/241 [00:03<00:31,  6.74it/s]Epoch: 3, train for the 102-th batch, train loss: 0.5379666686058044:  86%|█████████▍ | 102/119 [00:15<00:02,  5.75it/s]Epoch: 3, train for the 30-th batch, train loss: 0.5078016519546509:  20%|██▌          | 29/146 [00:04<00:16,  7.10it/s]Epoch: 2, train for the 136-th batch, train loss: 0.5144394040107727:  89%|█████████▊ | 135/151 [00:22<00:03,  4.81it/s]Epoch: 3, train for the 30-th batch, train loss: 0.5078016519546509:  21%|██▋          | 30/146 [00:04<00:15,  7.29it/s]Epoch: 1, train for the 233-th batch, train loss: 0.4147518575191498:  61%|██████▋    | 232/383 [01:04<00:48,  3.12it/s]Epoch: 2, train for the 136-th batch, train loss: 0.5144394040107727:  90%|█████████▉ | 136/151 [00:22<00:03,  4.75it/s]evaluate for the 20-th batch, evaluate loss: 0.5591505169868469:  27%|████▉             | 18/66 [00:03<00:06,  6.97it/s]evaluate for the 20-th batch, evaluate loss: 0.5591505169868469:  30%|█████▍            | 20/66 [00:03<00:06,  6.62it/s]Epoch: 1, train for the 233-th batch, train loss: 0.4147518575191498:  61%|██████▋    | 233/383 [01:04<00:45,  3.28it/s]Epoch: 2, train for the 29-th batch, train loss: 0.5691343545913696:  12%|█▌           | 28/241 [00:03<00:31,  6.74it/s]Epoch: 3, train for the 103-th batch, train loss: 0.4607124328613281:  86%|█████████▍ | 102/119 [00:15<00:02,  5.75it/s]Epoch: 3, train for the 103-th batch, train loss: 0.4607124328613281:  87%|█████████▌ | 103/119 [00:15<00:02,  6.33it/s]Epoch: 2, train for the 29-th batch, train loss: 0.5691343545913696:  12%|█▌           | 29/241 [00:03<00:29,  7.13it/s]Epoch: 3, train for the 31-th batch, train loss: 0.5610783696174622:  21%|██▋          | 30/146 [00:04<00:15,  7.29it/s]Epoch: 3, train for the 31-th batch, train loss: 0.5610783696174622:  21%|██▊          | 31/146 [00:04<00:15,  7.43it/s]Epoch: 2, train for the 137-th batch, train loss: 0.5548707842826843:  90%|█████████▉ | 136/151 [00:22<00:03,  4.75it/s]Epoch: 2, train for the 137-th batch, train loss: 0.5548707842826843:  91%|█████████▉ | 137/151 [00:22<00:02,  5.09it/s]evaluate for the 21-th batch, evaluate loss: 0.5540809631347656:  30%|█████▍            | 20/66 [00:03<00:06,  6.62it/s]evaluate for the 21-th batch, evaluate loss: 0.5540809631347656:  32%|█████▋            | 21/66 [00:03<00:06,  6.85it/s]Epoch: 2, train for the 30-th batch, train loss: 0.6454010009765625:  12%|█▌           | 29/241 [00:04<00:29,  7.13it/s]Epoch: 3, train for the 104-th batch, train loss: 0.48726263642311096:  87%|████████▋ | 103/119 [00:15<00:02,  6.33it/s]Epoch: 2, train for the 30-th batch, train loss: 0.6454010009765625:  12%|█▌           | 30/241 [00:04<00:28,  7.30it/s]Epoch: 3, train for the 104-th batch, train loss: 0.48726263642311096:  87%|████████▋ | 104/119 [00:15<00:02,  6.60it/s]Epoch: 3, train for the 32-th batch, train loss: 0.5129687190055847:  21%|██▊          | 31/146 [00:04<00:15,  7.43it/s]Epoch: 3, train for the 32-th batch, train loss: 0.5129687190055847:  22%|██▊          | 32/146 [00:04<00:15,  7.55it/s]Epoch: 1, train for the 234-th batch, train loss: 0.4283345639705658:  61%|██████▋    | 233/383 [01:05<00:45,  3.28it/s]evaluate for the 22-th batch, evaluate loss: 0.5370494723320007:  32%|█████▋            | 21/66 [00:03<00:06,  6.85it/s]evaluate for the 22-th batch, evaluate loss: 0.5370494723320007:  33%|██████            | 22/66 [00:03<00:06,  6.86it/s]Epoch: 2, train for the 138-th batch, train loss: 0.5293325185775757:  91%|█████████▉ | 137/151 [00:23<00:02,  5.09it/s]Epoch: 2, train for the 31-th batch, train loss: 0.4681731164455414:  12%|█▌           | 30/241 [00:04<00:28,  7.30it/s]Epoch: 3, train for the 33-th batch, train loss: 0.5023857951164246:  22%|██▊          | 32/146 [00:04<00:15,  7.55it/s]Epoch: 1, train for the 234-th batch, train loss: 0.4283345639705658:  61%|██████▋    | 234/383 [01:05<00:45,  3.24it/s]Epoch: 3, train for the 33-th batch, train loss: 0.5023857951164246:  23%|██▉          | 33/146 [00:04<00:15,  7.41it/s]Epoch: 2, train for the 31-th batch, train loss: 0.4681731164455414:  13%|█▋           | 31/241 [00:04<00:31,  6.67it/s]Epoch: 2, train for the 138-th batch, train loss: 0.5293325185775757:  91%|██████████ | 138/151 [00:23<00:02,  4.94it/s]Epoch: 3, train for the 105-th batch, train loss: 0.4809093475341797:  87%|█████████▌ | 104/119 [00:15<00:02,  6.60it/s]evaluate for the 23-th batch, evaluate loss: 0.5436128973960876:  33%|██████            | 22/66 [00:03<00:06,  6.86it/s]Epoch: 3, train for the 105-th batch, train loss: 0.4809093475341797:  88%|█████████▋ | 105/119 [00:15<00:02,  5.98it/s]Epoch: 3, train for the 34-th batch, train loss: 0.5442779660224915:  23%|██▉          | 33/146 [00:04<00:15,  7.41it/s]Epoch: 3, train for the 34-th batch, train loss: 0.5442779660224915:  23%|███          | 34/146 [00:04<00:15,  7.43it/s]Epoch: 2, train for the 32-th batch, train loss: 0.43482697010040283:  13%|█▌          | 31/241 [00:04<00:31,  6.67it/s]Epoch: 2, train for the 32-th batch, train loss: 0.43482697010040283:  13%|█▌          | 32/241 [00:04<00:31,  6.60it/s]Epoch: 3, train for the 106-th batch, train loss: 0.4981396496295929:  88%|█████████▋ | 105/119 [00:16<00:02,  5.98it/s]Epoch: 2, train for the 139-th batch, train loss: 0.5000372529029846:  91%|██████████ | 138/151 [00:23<00:02,  4.94it/s]Epoch: 3, train for the 106-th batch, train loss: 0.4981396496295929:  89%|█████████▊ | 106/119 [00:16<00:02,  5.92it/s]Epoch: 2, train for the 139-th batch, train loss: 0.5000372529029846:  92%|██████████▏| 139/151 [00:23<00:02,  4.85it/s]Epoch: 1, train for the 235-th batch, train loss: 0.37194350361824036:  61%|██████    | 234/383 [01:05<00:45,  3.24it/s]Epoch: 3, train for the 35-th batch, train loss: 0.4625253975391388:  23%|███          | 34/146 [00:04<00:15,  7.43it/s]Epoch: 3, train for the 35-th batch, train loss: 0.4625253975391388:  24%|███          | 35/146 [00:04<00:14,  7.92it/s]evaluate for the 24-th batch, evaluate loss: 0.5310010313987732:  33%|██████            | 22/66 [00:03<00:06,  6.86it/s]evaluate for the 24-th batch, evaluate loss: 0.5310010313987732:  36%|██████▌           | 24/66 [00:03<00:06,  6.77it/s]Epoch: 2, train for the 33-th batch, train loss: 0.6164114475250244:  13%|█▋           | 32/241 [00:04<00:31,  6.60it/s]Epoch: 2, train for the 33-th batch, train loss: 0.6164114475250244:  14%|█▊           | 33/241 [00:04<00:29,  6.99it/s]Epoch: 1, train for the 235-th batch, train loss: 0.37194350361824036:  61%|██████▏   | 235/383 [01:05<00:46,  3.19it/s]evaluate for the 25-th batch, evaluate loss: 0.5439012050628662:  36%|██████▌           | 24/66 [00:03<00:06,  6.77it/s]evaluate for the 25-th batch, evaluate loss: 0.5439012050628662:  38%|██████▊           | 25/66 [00:03<00:05,  7.26it/s]Epoch: 3, train for the 107-th batch, train loss: 0.519353449344635:  89%|██████████▋ | 106/119 [00:16<00:02,  5.92it/s]Epoch: 3, train for the 107-th batch, train loss: 0.519353449344635:  90%|██████████▊ | 107/119 [00:16<00:02,  5.95it/s]Epoch: 2, train for the 140-th batch, train loss: 0.4961692690849304:  92%|██████████▏| 139/151 [00:23<00:02,  4.85it/s]Epoch: 3, train for the 36-th batch, train loss: 0.46826478838920593:  24%|██▉         | 35/146 [00:05<00:14,  7.92it/s]Epoch: 3, train for the 36-th batch, train loss: 0.46826478838920593:  25%|██▉         | 36/146 [00:05<00:14,  7.46it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5562545657157898:  14%|█▊           | 33/241 [00:04<00:29,  6.99it/s]Epoch: 2, train for the 140-th batch, train loss: 0.4961692690849304:  93%|██████████▏| 140/151 [00:23<00:02,  5.02it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5562545657157898:  14%|█▊           | 34/241 [00:04<00:28,  7.20it/s]Epoch: 1, train for the 236-th batch, train loss: 0.4181285798549652:  61%|██████▋    | 235/383 [01:05<00:46,  3.19it/s]Epoch: 2, train for the 35-th batch, train loss: 0.5744152069091797:  14%|█▊           | 34/241 [00:04<00:28,  7.20it/s]Epoch: 1, train for the 236-th batch, train loss: 0.4181285798549652:  62%|██████▊    | 236/383 [01:05<00:42,  3.42it/s]Epoch: 2, train for the 35-th batch, train loss: 0.5744152069091797:  15%|█▉           | 35/241 [00:04<00:30,  6.87it/s]Epoch: 3, train for the 108-th batch, train loss: 0.40743833780288696:  90%|████████▉ | 107/119 [00:16<00:02,  5.95it/s]Epoch: 3, train for the 37-th batch, train loss: 0.5189825892448425:  25%|███▏         | 36/146 [00:05<00:14,  7.46it/s]Epoch: 2, train for the 141-th batch, train loss: 0.5210807919502258:  93%|██████████▏| 140/151 [00:23<00:02,  5.02it/s]Epoch: 3, train for the 37-th batch, train loss: 0.5189825892448425:  25%|███▎         | 37/146 [00:05<00:16,  6.56it/s]Epoch: 3, train for the 108-th batch, train loss: 0.40743833780288696:  91%|█████████ | 108/119 [00:16<00:02,  5.44it/s]evaluate for the 26-th batch, evaluate loss: 0.5540415644645691:  38%|██████▊           | 25/66 [00:04<00:05,  7.26it/s]evaluate for the 26-th batch, evaluate loss: 0.5540415644645691:  39%|███████           | 26/66 [00:04<00:06,  6.06it/s]Epoch: 2, train for the 141-th batch, train loss: 0.5210807919502258:  93%|██████████▎| 141/151 [00:23<00:02,  4.95it/s]Epoch: 2, train for the 36-th batch, train loss: 0.5682212114334106:  15%|█▉           | 35/241 [00:05<00:30,  6.87it/s]Epoch: 2, train for the 36-th batch, train loss: 0.5682212114334106:  15%|█▉           | 36/241 [00:05<00:29,  6.88it/s]evaluate for the 27-th batch, evaluate loss: 0.5422071814537048:  39%|███████           | 26/66 [00:04<00:06,  6.06it/s]evaluate for the 27-th batch, evaluate loss: 0.5422071814537048:  41%|███████▎          | 27/66 [00:04<00:06,  6.46it/s]Epoch: 3, train for the 38-th batch, train loss: 0.5271244049072266:  25%|███▎         | 37/146 [00:05<00:16,  6.56it/s]Epoch: 3, train for the 38-th batch, train loss: 0.5271244049072266:  26%|███▍         | 38/146 [00:05<00:16,  6.70it/s]Epoch: 3, train for the 109-th batch, train loss: 0.4858565330505371:  91%|█████████▉ | 108/119 [00:16<00:02,  5.44it/s]Epoch: 3, train for the 109-th batch, train loss: 0.4858565330505371:  92%|██████████ | 109/119 [00:16<00:01,  5.54it/s]Epoch: 1, train for the 237-th batch, train loss: 0.4205228388309479:  62%|██████▊    | 236/383 [01:05<00:42,  3.42it/s]Epoch: 2, train for the 142-th batch, train loss: 0.49056312441825867:  93%|█████████▎| 141/151 [00:23<00:02,  4.95it/s]evaluate for the 28-th batch, evaluate loss: 0.5933530330657959:  41%|███████▎          | 27/66 [00:04<00:06,  6.46it/s]Epoch: 2, train for the 37-th batch, train loss: 0.4746567904949188:  15%|█▉           | 36/241 [00:05<00:29,  6.88it/s]Epoch: 2, train for the 142-th batch, train loss: 0.49056312441825867:  94%|█████████▍| 142/151 [00:24<00:01,  4.83it/s]Epoch: 2, train for the 37-th batch, train loss: 0.4746567904949188:  15%|█▉           | 37/241 [00:05<00:27,  7.30it/s]Epoch: 1, train for the 237-th batch, train loss: 0.4205228388309479:  62%|██████▊    | 237/383 [01:05<00:41,  3.48it/s]Epoch: 3, train for the 39-th batch, train loss: 0.4923107922077179:  26%|███▍         | 38/146 [00:05<00:16,  6.70it/s]Epoch: 3, train for the 39-th batch, train loss: 0.4923107922077179:  27%|███▍         | 39/146 [00:05<00:15,  6.87it/s]Epoch: 3, train for the 110-th batch, train loss: 0.5082389712333679:  92%|██████████ | 109/119 [00:16<00:01,  5.54it/s]evaluate for the 29-th batch, evaluate loss: 0.5077495574951172:  41%|███████▎          | 27/66 [00:04<00:06,  6.46it/s]evaluate for the 29-th batch, evaluate loss: 0.5077495574951172:  44%|███████▉          | 29/66 [00:04<00:04,  7.73it/s]Epoch: 3, train for the 110-th batch, train loss: 0.5082389712333679:  92%|██████████▏| 110/119 [00:16<00:01,  5.80it/s]Epoch: 2, train for the 38-th batch, train loss: 0.5553897619247437:  15%|█▉           | 37/241 [00:05<00:27,  7.30it/s]Epoch: 3, train for the 40-th batch, train loss: 0.4920322299003601:  27%|███▍         | 39/146 [00:05<00:15,  6.87it/s]Epoch: 2, train for the 38-th batch, train loss: 0.5553897619247437:  16%|██           | 38/241 [00:05<00:29,  6.97it/s]Epoch: 3, train for the 40-th batch, train loss: 0.4920322299003601:  27%|███▌         | 40/146 [00:05<00:14,  7.15it/s]Epoch: 2, train for the 143-th batch, train loss: 0.47545862197875977:  94%|█████████▍| 142/151 [00:24<00:01,  4.83it/s]Epoch: 3, train for the 111-th batch, train loss: 0.5618881583213806:  92%|██████████▏| 110/119 [00:16<00:01,  5.80it/s]Epoch: 2, train for the 143-th batch, train loss: 0.47545862197875977:  95%|█████████▍| 143/151 [00:24<00:01,  4.81it/s]Epoch: 3, train for the 111-th batch, train loss: 0.5618881583213806:  93%|██████████▎| 111/119 [00:16<00:01,  6.23it/s]Epoch: 2, train for the 39-th batch, train loss: 0.6030449271202087:  16%|██           | 38/241 [00:05<00:29,  6.97it/s]Epoch: 3, train for the 41-th batch, train loss: 0.4918499290943146:  27%|███▌         | 40/146 [00:05<00:14,  7.15it/s]Epoch: 2, train for the 39-th batch, train loss: 0.6030449271202087:  16%|██           | 39/241 [00:05<00:27,  7.28it/s]Epoch: 3, train for the 41-th batch, train loss: 0.4918499290943146:  28%|███▋         | 41/146 [00:05<00:14,  7.40it/s]Epoch: 1, train for the 238-th batch, train loss: 0.36260348558425903:  62%|██████▏   | 237/383 [01:06<00:41,  3.48it/s]evaluate for the 30-th batch, evaluate loss: 0.5570970773696899:  44%|███████▉          | 29/66 [00:04<00:04,  7.73it/s]evaluate for the 30-th batch, evaluate loss: 0.5570970773696899:  45%|████████▏         | 30/66 [00:04<00:05,  6.25it/s]Epoch: 3, train for the 112-th batch, train loss: 0.49315351247787476:  93%|█████████▎| 111/119 [00:17<00:01,  6.23it/s]Epoch: 2, train for the 144-th batch, train loss: 0.4634160101413727:  95%|██████████▍| 143/151 [00:24<00:01,  4.81it/s]Epoch: 3, train for the 112-th batch, train loss: 0.49315351247787476:  94%|█████████▍| 112/119 [00:17<00:01,  6.50it/s]Epoch: 1, train for the 238-th batch, train loss: 0.36260348558425903:  62%|██████▏   | 238/383 [01:06<00:44,  3.24it/s]Epoch: 2, train for the 144-th batch, train loss: 0.4634160101413727:  95%|██████████▍| 144/151 [00:24<00:01,  5.08it/s]Epoch: 2, train for the 40-th batch, train loss: 0.6250225901603699:  16%|██           | 39/241 [00:05<00:27,  7.28it/s]Epoch: 2, train for the 40-th batch, train loss: 0.6250225901603699:  17%|██▏          | 40/241 [00:05<00:25,  7.80it/s]Epoch: 3, train for the 42-th batch, train loss: 0.49738067388534546:  28%|███▎        | 41/146 [00:05<00:14,  7.40it/s]Epoch: 3, train for the 42-th batch, train loss: 0.49738067388534546:  29%|███▍        | 42/146 [00:05<00:13,  7.73it/s]evaluate for the 31-th batch, evaluate loss: 0.544266939163208:  45%|████████▋          | 30/66 [00:04<00:05,  6.25it/s]evaluate for the 31-th batch, evaluate loss: 0.544266939163208:  47%|████████▉          | 31/66 [00:04<00:05,  6.71it/s]Epoch: 3, train for the 113-th batch, train loss: 0.4673846364021301:  94%|██████████▎| 112/119 [00:17<00:01,  6.50it/s]Epoch: 3, train for the 113-th batch, train loss: 0.4673846364021301:  95%|██████████▍| 113/119 [00:17<00:00,  6.51it/s]Epoch: 2, train for the 41-th batch, train loss: 0.54038405418396:  17%|██▍            | 40/241 [00:05<00:25,  7.80it/s]Epoch: 2, train for the 41-th batch, train loss: 0.54038405418396:  17%|██▌            | 41/241 [00:05<00:26,  7.49it/s]Epoch: 3, train for the 43-th batch, train loss: 0.5001527667045593:  29%|███▋         | 42/146 [00:06<00:13,  7.73it/s]Epoch: 3, train for the 43-th batch, train loss: 0.5001527667045593:  29%|███▊         | 43/146 [00:06<00:14,  7.24it/s]Epoch: 2, train for the 145-th batch, train loss: 0.4764906167984009:  95%|██████████▍| 144/151 [00:24<00:01,  5.08it/s]Epoch: 2, train for the 145-th batch, train loss: 0.4764906167984009:  96%|██████████▌| 145/151 [00:24<00:01,  4.91it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5691432356834412:  17%|██▏          | 41/241 [00:05<00:26,  7.49it/s]Epoch: 3, train for the 114-th batch, train loss: 0.4640843868255615:  95%|██████████▍| 113/119 [00:17<00:00,  6.51it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5691432356834412:  17%|██▎          | 42/241 [00:05<00:27,  7.33it/s]Epoch: 3, train for the 114-th batch, train loss: 0.4640843868255615:  96%|██████████▌| 114/119 [00:17<00:00,  6.24it/s]Epoch: 3, train for the 44-th batch, train loss: 0.4757614731788635:  29%|███▊         | 43/146 [00:06<00:14,  7.24it/s]Epoch: 3, train for the 44-th batch, train loss: 0.4757614731788635:  30%|███▉         | 44/146 [00:06<00:14,  6.95it/s]Epoch: 2, train for the 146-th batch, train loss: 0.4516265392303467:  96%|██████████▌| 145/151 [00:24<00:01,  4.91it/s]Epoch: 2, train for the 146-th batch, train loss: 0.4516265392303467:  97%|██████████▋| 146/151 [00:24<00:00,  5.29it/s]Epoch: 2, train for the 43-th batch, train loss: 0.6079319715499878:  17%|██▎          | 42/241 [00:05<00:27,  7.33it/s]Epoch: 2, train for the 43-th batch, train loss: 0.6079319715499878:  18%|██▎          | 43/241 [00:05<00:25,  7.64it/s]Epoch: 1, train for the 239-th batch, train loss: 0.44148504734039307:  62%|██████▏   | 238/383 [01:06<00:44,  3.24it/s]evaluate for the 32-th batch, evaluate loss: 0.5554093718528748:  47%|████████▍         | 31/66 [00:05<00:05,  6.71it/s]evaluate for the 32-th batch, evaluate loss: 0.5554093718528748:  48%|████████▋         | 32/66 [00:05<00:06,  4.89it/s]Epoch: 3, train for the 115-th batch, train loss: 0.46966707706451416:  96%|█████████▌| 114/119 [00:17<00:00,  6.24it/s]Epoch: 3, train for the 115-th batch, train loss: 0.46966707706451416:  97%|█████████▋| 115/119 [00:17<00:00,  6.40it/s]Epoch: 3, train for the 45-th batch, train loss: 0.4985802471637726:  30%|███▉         | 44/146 [00:06<00:14,  6.95it/s]Epoch: 3, train for the 45-th batch, train loss: 0.4985802471637726:  31%|████         | 45/146 [00:06<00:14,  6.95it/s]Epoch: 1, train for the 239-th batch, train loss: 0.44148504734039307:  62%|██████▏   | 239/383 [01:06<00:53,  2.71it/s]evaluate for the 33-th batch, evaluate loss: 0.5824039578437805:  48%|████████▋         | 32/66 [00:05<00:06,  4.89it/s]Epoch: 2, train for the 147-th batch, train loss: 0.5023481845855713:  97%|██████████▋| 146/151 [00:24<00:00,  5.29it/s]Epoch: 2, train for the 44-th batch, train loss: 0.6376184821128845:  18%|██▎          | 43/241 [00:06<00:25,  7.64it/s]Epoch: 2, train for the 147-th batch, train loss: 0.5023481845855713:  97%|██████████▋| 147/151 [00:24<00:00,  5.38it/s]Epoch: 2, train for the 44-th batch, train loss: 0.6376184821128845:  18%|██▎          | 44/241 [00:06<00:26,  7.48it/s]Epoch: 3, train for the 116-th batch, train loss: 0.4814591109752655:  97%|██████████▋| 115/119 [00:17<00:00,  6.40it/s]Epoch: 3, train for the 116-th batch, train loss: 0.4814591109752655:  97%|██████████▋| 116/119 [00:17<00:00,  6.48it/s]Epoch: 3, train for the 46-th batch, train loss: 0.48799577355384827:  31%|███▋        | 45/146 [00:06<00:14,  6.95it/s]Epoch: 3, train for the 46-th batch, train loss: 0.48799577355384827:  32%|███▊        | 46/146 [00:06<00:14,  6.78it/s]Epoch: 2, train for the 45-th batch, train loss: 0.46930983662605286:  18%|██▏         | 44/241 [00:06<00:26,  7.48it/s]Epoch: 2, train for the 45-th batch, train loss: 0.46930983662605286:  19%|██▏         | 45/241 [00:06<00:26,  7.46it/s]Epoch: 2, train for the 148-th batch, train loss: 0.47021135687828064:  97%|█████████▋| 147/151 [00:25<00:00,  5.38it/s]Epoch: 1, train for the 240-th batch, train loss: 0.5183424353599548:  62%|██████▊    | 239/383 [01:07<00:53,  2.71it/s]Epoch: 3, train for the 117-th batch, train loss: 0.44421473145484924:  97%|█████████▋| 116/119 [00:17<00:00,  6.48it/s]Epoch: 3, train for the 117-th batch, train loss: 0.44421473145484924:  98%|█████████▊| 117/119 [00:17<00:00,  6.41it/s]Epoch: 2, train for the 148-th batch, train loss: 0.47021135687828064:  98%|█████████▊| 148/151 [00:25<00:00,  5.13it/s]Epoch: 3, train for the 47-th batch, train loss: 0.5119710564613342:  32%|████         | 46/146 [00:06<00:14,  6.78it/s]Epoch: 1, train for the 240-th batch, train loss: 0.5183424353599548:  63%|██████▉    | 240/383 [01:07<00:49,  2.88it/s]Epoch: 2, train for the 46-th batch, train loss: 0.5250030159950256:  19%|██▍          | 45/241 [00:06<00:26,  7.46it/s]Epoch: 3, train for the 47-th batch, train loss: 0.5119710564613342:  32%|████▏        | 47/146 [00:06<00:14,  6.64it/s]Epoch: 2, train for the 46-th batch, train loss: 0.5250030159950256:  19%|██▍          | 46/241 [00:06<00:25,  7.77it/s]evaluate for the 34-th batch, evaluate loss: 0.5436246991157532:  48%|████████▋         | 32/66 [00:05<00:06,  4.89it/s]evaluate for the 34-th batch, evaluate loss: 0.5436246991157532:  52%|█████████▎        | 34/66 [00:05<00:06,  5.05it/s]Epoch: 3, train for the 118-th batch, train loss: 0.444762259721756:  98%|███████████▊| 117/119 [00:17<00:00,  6.41it/s]evaluate for the 35-th batch, evaluate loss: 0.5697755217552185:  52%|█████████▎        | 34/66 [00:05<00:06,  5.05it/s]evaluate for the 35-th batch, evaluate loss: 0.5697755217552185:  53%|█████████▌        | 35/66 [00:05<00:05,  5.57it/s]Epoch: 3, train for the 118-th batch, train loss: 0.444762259721756:  99%|███████████▉| 118/119 [00:17<00:00,  6.16it/s]Epoch: 3, train for the 48-th batch, train loss: 0.4752136468887329:  32%|████▏        | 47/146 [00:06<00:14,  6.64it/s]Epoch: 2, train for the 149-th batch, train loss: 0.4668307900428772:  98%|██████████▊| 148/151 [00:25<00:00,  5.13it/s]Epoch: 3, train for the 48-th batch, train loss: 0.4752136468887329:  33%|████▎        | 48/146 [00:06<00:15,  6.44it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5795833468437195:  19%|██▍          | 46/241 [00:06<00:25,  7.77it/s]Epoch: 2, train for the 149-th batch, train loss: 0.4668307900428772:  99%|██████████▊| 149/151 [00:25<00:00,  4.98it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5795833468437195:  20%|██▌          | 47/241 [00:06<00:27,  6.93it/s]Epoch: 3, train for the 119-th batch, train loss: 0.4816211462020874:  99%|██████████▉| 118/119 [00:18<00:00,  6.16it/s]Epoch: 1, train for the 241-th batch, train loss: 0.5662246346473694:  63%|██████▉    | 240/383 [01:07<00:49,  2.88it/s]Epoch: 3, train for the 119-th batch, train loss: 0.4816211462020874: 100%|███████████| 119/119 [00:18<00:00,  6.68it/s]Epoch: 3, train for the 119-th batch, train loss: 0.4816211462020874: 100%|███████████| 119/119 [00:18<00:00,  6.58it/s]
evaluate for the 36-th batch, evaluate loss: 0.5628170371055603:  53%|█████████▌        | 35/66 [00:05<00:05,  5.57it/s]evaluate for the 36-th batch, evaluate loss: 0.5628170371055603:  55%|█████████▊        | 36/66 [00:05<00:04,  6.01it/s]Epoch: 3, train for the 49-th batch, train loss: 0.49714770913124084:  33%|███▉        | 48/146 [00:06<00:15,  6.44it/s]Epoch: 3, train for the 49-th batch, train loss: 0.49714770913124084:  34%|████        | 49/146 [00:06<00:14,  6.86it/s]Epoch: 1, train for the 241-th batch, train loss: 0.5662246346473694:  63%|██████▉    | 241/383 [01:07<00:48,  2.90it/s]evaluate for the 37-th batch, evaluate loss: 0.567152202129364:  55%|██████████▎        | 36/66 [00:05<00:04,  6.01it/s]Epoch: 3, train for the 50-th batch, train loss: 0.5164098739624023:  34%|████▎        | 49/146 [00:07<00:14,  6.86it/s]Epoch: 3, train for the 50-th batch, train loss: 0.5164098739624023:  34%|████▍        | 50/146 [00:07<00:12,  7.54it/s]Epoch: 2, train for the 150-th batch, train loss: 0.4865837097167969:  99%|██████████▊| 149/151 [00:25<00:00,  4.98it/s]Epoch: 2, train for the 48-th batch, train loss: 0.5588926076889038:  20%|██▌          | 47/241 [00:06<00:27,  6.93it/s]Epoch: 2, train for the 48-th batch, train loss: 0.5588926076889038:  20%|██▌          | 48/241 [00:06<00:32,  5.94it/s]Epoch: 2, train for the 150-th batch, train loss: 0.4865837097167969:  99%|██████████▉| 150/151 [00:25<00:00,  4.60it/s]  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 3, train for the 51-th batch, train loss: 0.51421058177948:  34%|█████▏         | 50/146 [00:07<00:12,  7.54it/s]Epoch: 3, train for the 51-th batch, train loss: 0.51421058177948:  35%|█████▏         | 51/146 [00:07<00:11,  7.94it/s]evaluate for the 1-th batch, evaluate loss: 0.4714388847351074:   0%|                            | 0/40 [00:00<?, ?it/s]Epoch: 1, train for the 242-th batch, train loss: 0.474206805229187:  63%|███████▌    | 241/383 [01:07<00:48,  2.90it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5538373589515686:  20%|██▌          | 48/241 [00:06<00:32,  5.94it/s]evaluate for the 2-th batch, evaluate loss: 0.48747336864471436:   0%|                           | 0/40 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.48747336864471436:   5%|▉                  | 2/40 [00:00<00:02, 17.75it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5538373589515686:  20%|██▋          | 49/241 [00:06<00:31,  6.02it/s]evaluate for the 38-th batch, evaluate loss: 0.520214855670929:  55%|██████████▎        | 36/66 [00:06<00:04,  6.01it/s]evaluate for the 38-th batch, evaluate loss: 0.520214855670929:  58%|██████████▉        | 38/66 [00:06<00:04,  6.15it/s]Epoch: 2, train for the 151-th batch, train loss: 0.5455724596977234:  99%|██████████▉| 150/151 [00:25<00:00,  4.60it/s]Epoch: 1, train for the 242-th batch, train loss: 0.474206805229187:  63%|███████▌    | 242/383 [01:07<00:45,  3.11it/s]Epoch: 2, train for the 151-th batch, train loss: 0.5455724596977234: 100%|███████████| 151/151 [00:25<00:00,  4.82it/s]Epoch: 2, train for the 151-th batch, train loss: 0.5455724596977234: 100%|███████████| 151/151 [00:25<00:00,  5.85it/s]
Epoch: 3, train for the 52-th batch, train loss: 0.48297521471977234:  35%|████▏       | 51/146 [00:07<00:11,  7.94it/s]evaluate for the 3-th batch, evaluate loss: 0.48390695452690125:   5%|▉                  | 2/40 [00:00<00:02, 17.75it/s]Epoch: 3, train for the 52-th batch, train loss: 0.48297521471977234:  36%|████▎       | 52/146 [00:07<00:12,  7.71it/s]evaluate for the 39-th batch, evaluate loss: 0.5577815771102905:  58%|██████████▎       | 38/66 [00:06<00:04,  6.15it/s]evaluate for the 39-th batch, evaluate loss: 0.5577815771102905:  59%|██████████▋       | 39/66 [00:06<00:04,  6.75it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5555009245872498:  20%|██▋          | 49/241 [00:06<00:31,  6.02it/s]evaluate for the 4-th batch, evaluate loss: 0.5477294325828552:   5%|█                   | 2/40 [00:00<00:02, 17.75it/s]evaluate for the 4-th batch, evaluate loss: 0.5477294325828552:  10%|██                  | 4/40 [00:00<00:02, 16.23it/s]Epoch: 2, train for the 50-th batch, train loss: 0.5555009245872498:  21%|██▋          | 50/241 [00:07<00:30,  6.25it/s]Epoch: 3, train for the 53-th batch, train loss: 0.488252729177475:  36%|████▉         | 52/146 [00:07<00:12,  7.71it/s]Epoch: 3, train for the 53-th batch, train loss: 0.488252729177475:  36%|█████         | 53/146 [00:07<00:11,  7.87it/s]evaluate for the 5-th batch, evaluate loss: 0.5214782357215881:  10%|██                  | 4/40 [00:00<00:02, 16.23it/s]evaluate for the 6-th batch, evaluate loss: 0.4895223081111908:  10%|██                  | 4/40 [00:00<00:02, 16.23it/s]evaluate for the 6-th batch, evaluate loss: 0.4895223081111908:  15%|███                 | 6/40 [00:00<00:02, 13.99it/s]Epoch: 2, train for the 51-th batch, train loss: 0.5416958332061768:  21%|██▋          | 50/241 [00:07<00:30,  6.25it/s]Epoch: 2, train for the 51-th batch, train loss: 0.5416958332061768:  21%|██▊          | 51/241 [00:07<00:30,  6.18it/s]Epoch: 3, train for the 54-th batch, train loss: 0.5404327511787415:  36%|████▋        | 53/146 [00:07<00:11,  7.87it/s]Epoch: 3, train for the 54-th batch, train loss: 0.5404327511787415:  37%|████▊        | 54/146 [00:07<00:12,  7.46it/s]  0%|                                                                                            | 0/46 [00:00<?, ?it/s]evaluate for the 7-th batch, evaluate loss: 0.5086190104484558:  15%|███                 | 6/40 [00:00<00:02, 13.99it/s]evaluate for the 8-th batch, evaluate loss: 0.4611503481864929:  15%|███                 | 6/40 [00:00<00:02, 13.99it/s]evaluate for the 8-th batch, evaluate loss: 0.4611503481864929:  20%|████                | 8/40 [00:00<00:02, 15.43it/s]Epoch: 1, train for the 243-th batch, train loss: 0.4748341143131256:  63%|██████▉    | 242/383 [01:08<00:45,  3.11it/s]Epoch: 2, train for the 52-th batch, train loss: 0.5925047397613525:  21%|██▊          | 51/241 [00:07<00:30,  6.18it/s]evaluate for the 1-th batch, evaluate loss: 0.5389662981033325:   0%|                            | 0/46 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.5389662981033325:   2%|▍                   | 1/46 [00:00<00:05,  7.59it/s]evaluate for the 40-th batch, evaluate loss: 0.5399909615516663:  59%|██████████▋       | 39/66 [00:06<00:04,  6.75it/s]evaluate for the 40-th batch, evaluate loss: 0.5399909615516663:  61%|██████████▉       | 40/66 [00:06<00:05,  4.90it/s]Epoch: 3, train for the 55-th batch, train loss: 0.5628901720046997:  37%|████▊        | 54/146 [00:07<00:12,  7.46it/s]Epoch: 2, train for the 52-th batch, train loss: 0.5925047397613525:  22%|██▊          | 52/241 [00:07<00:30,  6.10it/s]evaluate for the 9-th batch, evaluate loss: 0.5112544894218445:  20%|████                | 8/40 [00:00<00:02, 15.43it/s]Epoch: 3, train for the 55-th batch, train loss: 0.5628901720046997:  38%|████▉        | 55/146 [00:07<00:12,  7.15it/s]Epoch: 1, train for the 243-th batch, train loss: 0.4748341143131256:  63%|██████▉    | 243/383 [01:08<00:53,  2.64it/s]evaluate for the 2-th batch, evaluate loss: 0.5619748830795288:   2%|▍                   | 1/46 [00:00<00:05,  7.59it/s]evaluate for the 10-th batch, evaluate loss: 0.5438299179077148:  20%|███▊               | 8/40 [00:00<00:02, 15.43it/s]evaluate for the 10-th batch, evaluate loss: 0.5438299179077148:  25%|████▌             | 10/40 [00:00<00:02, 14.04it/s]evaluate for the 41-th batch, evaluate loss: 0.5027954578399658:  61%|██████████▉       | 40/66 [00:06<00:05,  4.90it/s]evaluate for the 41-th batch, evaluate loss: 0.5027954578399658:  62%|███████████▏      | 41/66 [00:06<00:04,  5.36it/s]Epoch: 2, train for the 53-th batch, train loss: 0.419532835483551:  22%|███           | 52/241 [00:07<00:30,  6.10it/s]evaluate for the 3-th batch, evaluate loss: 0.549861490726471:   2%|▍                    | 1/46 [00:00<00:05,  7.59it/s]Epoch: 3, train for the 56-th batch, train loss: 0.5156106352806091:  38%|████▉        | 55/146 [00:07<00:12,  7.15it/s]evaluate for the 3-th batch, evaluate loss: 0.549861490726471:   7%|█▎                   | 3/46 [00:00<00:04, 10.38it/s]Epoch: 2, train for the 53-th batch, train loss: 0.419532835483551:  22%|███           | 53/241 [00:07<00:30,  6.19it/s]evaluate for the 11-th batch, evaluate loss: 0.48872748017311096:  25%|████▎            | 10/40 [00:00<00:02, 14.04it/s]Epoch: 3, train for the 56-th batch, train loss: 0.5156106352806091:  38%|████▉        | 56/146 [00:07<00:13,  6.89it/s]evaluate for the 12-th batch, evaluate loss: 0.5057942867279053:  25%|████▌             | 10/40 [00:00<00:02, 14.04it/s]evaluate for the 12-th batch, evaluate loss: 0.5057942867279053:  30%|█████▍            | 12/40 [00:00<00:02, 13.14it/s]evaluate for the 4-th batch, evaluate loss: 0.5315114259719849:   7%|█▎                  | 3/46 [00:00<00:04, 10.38it/s]Epoch: 2, train for the 54-th batch, train loss: 0.372342973947525:  22%|███           | 53/241 [00:07<00:30,  6.19it/s]Epoch: 1, train for the 244-th batch, train loss: 0.37976571917533875:  63%|██████▎   | 243/383 [01:08<00:53,  2.64it/s]Epoch: 2, train for the 54-th batch, train loss: 0.372342973947525:  22%|███▏          | 54/241 [00:07<00:29,  6.36it/s]evaluate for the 13-th batch, evaluate loss: 0.47893819212913513:  30%|█████            | 12/40 [00:00<00:02, 13.14it/s]Epoch: 3, train for the 57-th batch, train loss: 0.5330153703689575:  38%|████▉        | 56/146 [00:08<00:13,  6.89it/s]Epoch: 3, train for the 57-th batch, train loss: 0.5330153703689575:  39%|█████        | 57/146 [00:08<00:13,  6.45it/s]evaluate for the 5-th batch, evaluate loss: 0.5256012678146362:   7%|█▎                  | 3/46 [00:00<00:04, 10.38it/s]evaluate for the 5-th batch, evaluate loss: 0.5256012678146362:  11%|██▏                 | 5/46 [00:00<00:03, 10.34it/s]Epoch: 1, train for the 244-th batch, train loss: 0.37976571917533875:  64%|██████▎   | 244/383 [01:08<00:49,  2.80it/s]evaluate for the 14-th batch, evaluate loss: 0.49391859769821167:  30%|█████            | 12/40 [00:00<00:02, 13.14it/s]evaluate for the 14-th batch, evaluate loss: 0.49391859769821167:  35%|█████▉           | 14/40 [00:00<00:01, 13.61it/s]Epoch: 2, train for the 55-th batch, train loss: 0.3649090528488159:  22%|██▉          | 54/241 [00:07<00:29,  6.36it/s]Epoch: 2, train for the 55-th batch, train loss: 0.3649090528488159:  23%|██▉          | 55/241 [00:07<00:27,  6.77it/s]evaluate for the 6-th batch, evaluate loss: 0.5312737822532654:  11%|██▏                 | 5/46 [00:00<00:03, 10.34it/s]Epoch: 3, train for the 58-th batch, train loss: 0.495415061712265:  39%|█████▍        | 57/146 [00:08<00:13,  6.45it/s]evaluate for the 15-th batch, evaluate loss: 0.5124675631523132:  35%|██████▎           | 14/40 [00:01<00:01, 13.61it/s]Epoch: 3, train for the 58-th batch, train loss: 0.495415061712265:  40%|█████▌        | 58/146 [00:08<00:13,  6.35it/s]evaluate for the 42-th batch, evaluate loss: 0.5392428040504456:  62%|███████████▏      | 41/66 [00:07<00:04,  5.36it/s]evaluate for the 42-th batch, evaluate loss: 0.5392428040504456:  64%|███████████▍      | 42/66 [00:07<00:05,  4.15it/s]evaluate for the 16-th batch, evaluate loss: 0.48798996210098267:  35%|█████▉           | 14/40 [00:01<00:01, 13.61it/s]evaluate for the 16-th batch, evaluate loss: 0.48798996210098267:  40%|██████▊          | 16/40 [00:01<00:01, 12.53it/s]evaluate for the 7-th batch, evaluate loss: 0.5388068556785583:  11%|██▏                 | 5/46 [00:00<00:03, 10.34it/s]evaluate for the 7-th batch, evaluate loss: 0.5388068556785583:  15%|███                 | 7/46 [00:00<00:04,  9.51it/s]Epoch: 2, train for the 56-th batch, train loss: 0.3880050480365753:  23%|██▉          | 55/241 [00:07<00:27,  6.77it/s]Epoch: 2, train for the 56-th batch, train loss: 0.3880050480365753:  23%|███          | 56/241 [00:07<00:28,  6.46it/s]Epoch: 1, train for the 245-th batch, train loss: 0.4357622563838959:  64%|███████    | 244/383 [01:08<00:49,  2.80it/s]Epoch: 3, train for the 59-th batch, train loss: 0.4675368666648865:  40%|█████▏       | 58/146 [00:08<00:13,  6.35it/s]evaluate for the 17-th batch, evaluate loss: 0.5005019903182983:  40%|███████▏          | 16/40 [00:01<00:01, 12.53it/s]evaluate for the 43-th batch, evaluate loss: 0.5213682055473328:  64%|███████████▍      | 42/66 [00:07<00:05,  4.15it/s]evaluate for the 43-th batch, evaluate loss: 0.5213682055473328:  65%|███████████▋      | 43/66 [00:07<00:04,  4.74it/s]Epoch: 3, train for the 59-th batch, train loss: 0.4675368666648865:  40%|█████▎       | 59/146 [00:08<00:13,  6.40it/s]evaluate for the 8-th batch, evaluate loss: 0.5572790503501892:  15%|███                 | 7/46 [00:00<00:04,  9.51it/s]evaluate for the 18-th batch, evaluate loss: 0.49598148465156555:  40%|██████▊          | 16/40 [00:01<00:01, 12.53it/s]evaluate for the 18-th batch, evaluate loss: 0.49598148465156555:  45%|███████▋         | 18/40 [00:01<00:01, 13.39it/s]Epoch: 2, train for the 57-th batch, train loss: 0.4342736303806305:  23%|███          | 56/241 [00:08<00:28,  6.46it/s]Epoch: 1, train for the 245-th batch, train loss: 0.4357622563838959:  64%|███████    | 245/383 [01:08<00:49,  2.79it/s]Epoch: 2, train for the 57-th batch, train loss: 0.4342736303806305:  24%|███          | 57/241 [00:08<00:26,  6.85it/s]evaluate for the 19-th batch, evaluate loss: 0.49603572487831116:  45%|███████▋         | 18/40 [00:01<00:01, 13.39it/s]evaluate for the 9-th batch, evaluate loss: 0.539147675037384:  15%|███▏                 | 7/46 [00:00<00:04,  9.51it/s]evaluate for the 9-th batch, evaluate loss: 0.539147675037384:  20%|████                 | 9/46 [00:00<00:03,  9.83it/s]Epoch: 3, train for the 60-th batch, train loss: 0.5224500894546509:  40%|█████▎       | 59/146 [00:08<00:13,  6.40it/s]Epoch: 3, train for the 60-th batch, train loss: 0.5224500894546509:  41%|█████▎       | 60/146 [00:08<00:13,  6.53it/s]evaluate for the 20-th batch, evaluate loss: 0.4862697422504425:  45%|████████          | 18/40 [00:01<00:01, 13.39it/s]evaluate for the 20-th batch, evaluate loss: 0.4862697422504425:  50%|█████████         | 20/40 [00:01<00:01, 13.83it/s]Epoch: 2, train for the 58-th batch, train loss: 0.3375948369503021:  24%|███          | 57/241 [00:08<00:26,  6.85it/s]evaluate for the 10-th batch, evaluate loss: 0.5165344476699829:  20%|███▋               | 9/46 [00:01<00:03,  9.83it/s]Epoch: 2, train for the 58-th batch, train loss: 0.3375948369503021:  24%|███▏         | 58/241 [00:08<00:26,  6.79it/s]evaluate for the 44-th batch, evaluate loss: 0.543524980545044:  65%|████████████▍      | 43/66 [00:07<00:04,  4.74it/s]evaluate for the 44-th batch, evaluate loss: 0.543524980545044:  67%|████████████▋      | 44/66 [00:07<00:04,  4.62it/s]evaluate for the 21-th batch, evaluate loss: 0.49994781613349915:  50%|████████▌        | 20/40 [00:01<00:01, 13.83it/s]evaluate for the 11-th batch, evaluate loss: 0.5649591088294983:  20%|███▋               | 9/46 [00:01<00:03,  9.83it/s]evaluate for the 11-th batch, evaluate loss: 0.5649591088294983:  24%|████▎             | 11/46 [00:01<00:03, 10.06it/s]Epoch: 3, train for the 61-th batch, train loss: 0.532418966293335:  41%|█████▊        | 60/146 [00:08<00:13,  6.53it/s]Epoch: 3, train for the 61-th batch, train loss: 0.532418966293335:  42%|█████▊        | 61/146 [00:08<00:13,  6.22it/s]evaluate for the 22-th batch, evaluate loss: 0.48475417494773865:  50%|████████▌        | 20/40 [00:01<00:01, 13.83it/s]evaluate for the 22-th batch, evaluate loss: 0.48475417494773865:  55%|█████████▎       | 22/40 [00:01<00:01, 12.91it/s]evaluate for the 45-th batch, evaluate loss: 0.5180914402008057:  67%|████████████      | 44/66 [00:07<00:04,  4.62it/s]evaluate for the 45-th batch, evaluate loss: 0.5180914402008057:  68%|████████████▎     | 45/66 [00:07<00:04,  5.04it/s]Epoch: 1, train for the 246-th batch, train loss: 0.47825637459754944:  64%|██████▍   | 245/383 [01:09<00:49,  2.79it/s]Epoch: 2, train for the 59-th batch, train loss: 0.607185423374176:  24%|███▎          | 58/241 [00:08<00:26,  6.79it/s]evaluate for the 12-th batch, evaluate loss: 0.50725919008255:  24%|████▊               | 11/46 [00:01<00:03, 10.06it/s]Epoch: 2, train for the 59-th batch, train loss: 0.607185423374176:  24%|███▍          | 59/241 [00:08<00:29,  6.10it/s]evaluate for the 23-th batch, evaluate loss: 0.4279032051563263:  55%|█████████▉        | 22/40 [00:01<00:01, 12.91it/s]evaluate for the 13-th batch, evaluate loss: 0.5182321071624756:  24%|████▎             | 11/46 [00:01<00:03, 10.06it/s]evaluate for the 13-th batch, evaluate loss: 0.5182321071624756:  28%|█████             | 13/46 [00:01<00:03, 10.72it/s]Epoch: 1, train for the 246-th batch, train loss: 0.47825637459754944:  64%|██████▍   | 246/383 [01:09<00:51,  2.68it/s]evaluate for the 46-th batch, evaluate loss: 0.5717538595199585:  68%|████████████▎     | 45/66 [00:07<00:04,  5.04it/s]Epoch: 3, train for the 62-th batch, train loss: 0.4954913854598999:  42%|█████▍       | 61/146 [00:08<00:13,  6.22it/s]Epoch: 3, train for the 62-th batch, train loss: 0.4954913854598999:  42%|█████▌       | 62/146 [00:08<00:13,  6.18it/s]evaluate for the 24-th batch, evaluate loss: 0.4943893849849701:  55%|█████████▉        | 22/40 [00:01<00:01, 12.91it/s]evaluate for the 24-th batch, evaluate loss: 0.4943893849849701:  60%|██████████▊       | 24/40 [00:01<00:01, 12.56it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5922206044197083:  24%|███▏         | 59/241 [00:08<00:29,  6.10it/s]evaluate for the 47-th batch, evaluate loss: 0.5594823956489563:  68%|████████████▎     | 45/66 [00:07<00:04,  5.04it/s]evaluate for the 47-th batch, evaluate loss: 0.5594823956489563:  71%|████████████▊     | 47/66 [00:07<00:03,  6.29it/s]evaluate for the 14-th batch, evaluate loss: 0.5617892146110535:  28%|█████             | 13/46 [00:01<00:03, 10.72it/s]evaluate for the 25-th batch, evaluate loss: 0.5259433388710022:  60%|██████████▊       | 24/40 [00:01<00:01, 12.56it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5922206044197083:  25%|███▏         | 60/241 [00:08<00:31,  5.72it/s]Epoch: 3, train for the 63-th batch, train loss: 0.4926058351993561:  42%|█████▌       | 62/146 [00:09<00:13,  6.18it/s]Epoch: 3, train for the 63-th batch, train loss: 0.4926058351993561:  43%|█████▌       | 63/146 [00:09<00:13,  6.05it/s]evaluate for the 26-th batch, evaluate loss: 0.46255895495414734:  60%|██████████▏      | 24/40 [00:01<00:01, 12.56it/s]evaluate for the 26-th batch, evaluate loss: 0.46255895495414734:  65%|███████████      | 26/40 [00:01<00:01, 12.47it/s]evaluate for the 27-th batch, evaluate loss: 0.473743200302124:  65%|████████████▎      | 26/40 [00:01<00:01, 12.47it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5573652386665344:  25%|███▏         | 60/241 [00:08<00:31,  5.72it/s]Epoch: 1, train for the 247-th batch, train loss: 0.3669063448905945:  64%|███████    | 246/383 [01:09<00:51,  2.68it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5573652386665344:  25%|███▎         | 61/241 [00:08<00:30,  5.92it/s]evaluate for the 15-th batch, evaluate loss: 0.5473436117172241:  28%|█████             | 13/46 [00:01<00:03, 10.72it/s]evaluate for the 15-th batch, evaluate loss: 0.5473436117172241:  33%|█████▊            | 15/46 [00:01<00:03,  8.86it/s]evaluate for the 48-th batch, evaluate loss: 0.5477479696273804:  71%|████████████▊     | 47/66 [00:08<00:03,  6.29it/s]evaluate for the 48-th batch, evaluate loss: 0.5477479696273804:  73%|█████████████     | 48/66 [00:08<00:02,  6.03it/s]Epoch: 3, train for the 64-th batch, train loss: 0.5387434959411621:  43%|█████▌       | 63/146 [00:09<00:13,  6.05it/s]evaluate for the 28-th batch, evaluate loss: 0.45162132382392883:  65%|███████████      | 26/40 [00:02<00:01, 12.47it/s]evaluate for the 28-th batch, evaluate loss: 0.45162132382392883:  70%|███████████▉     | 28/40 [00:02<00:00, 13.47it/s]Epoch: 3, train for the 64-th batch, train loss: 0.5387434959411621:  44%|█████▋       | 64/146 [00:09<00:13,  6.23it/s]Epoch: 1, train for the 247-th batch, train loss: 0.3669063448905945:  64%|███████    | 247/383 [01:09<00:50,  2.70it/s]evaluate for the 29-th batch, evaluate loss: 0.49005958437919617:  70%|███████████▉     | 28/40 [00:02<00:00, 13.47it/s]evaluate for the 16-th batch, evaluate loss: 0.5270416140556335:  33%|█████▊            | 15/46 [00:01<00:03,  8.86it/s]evaluate for the 16-th batch, evaluate loss: 0.5270416140556335:  35%|██████▎           | 16/46 [00:01<00:03,  8.39it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5766907930374146:  25%|███▎         | 61/241 [00:08<00:30,  5.92it/s]Epoch: 3, train for the 65-th batch, train loss: 0.5482070446014404:  44%|█████▋       | 64/146 [00:09<00:13,  6.23it/s]evaluate for the 49-th batch, evaluate loss: 0.568743109703064:  73%|█████████████▊     | 48/66 [00:08<00:02,  6.03it/s]evaluate for the 49-th batch, evaluate loss: 0.568743109703064:  74%|██████████████     | 49/66 [00:08<00:02,  6.27it/s]Epoch: 3, train for the 65-th batch, train loss: 0.5482070446014404:  45%|█████▊       | 65/146 [00:09<00:12,  6.64it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5766907930374146:  26%|███▎         | 62/241 [00:08<00:30,  5.96it/s]evaluate for the 30-th batch, evaluate loss: 0.47049033641815186:  70%|███████████▉     | 28/40 [00:02<00:00, 13.47it/s]evaluate for the 30-th batch, evaluate loss: 0.47049033641815186:  75%|████████████▊    | 30/40 [00:02<00:00, 13.51it/s]Epoch: 1, train for the 248-th batch, train loss: 0.36249762773513794:  64%|██████▍   | 247/383 [01:09<00:50,  2.70it/s]evaluate for the 31-th batch, evaluate loss: 0.4621371328830719:  75%|█████████████▌    | 30/40 [00:02<00:00, 13.51it/s]Epoch: 3, train for the 66-th batch, train loss: 0.5400233864784241:  45%|█████▊       | 65/146 [00:09<00:12,  6.64it/s]Epoch: 3, train for the 66-th batch, train loss: 0.5400233864784241:  45%|█████▉       | 66/146 [00:09<00:11,  6.69it/s]Epoch: 1, train for the 248-th batch, train loss: 0.36249762773513794:  65%|██████▍   | 248/383 [01:09<00:45,  2.98it/s]evaluate for the 32-th batch, evaluate loss: 0.48277992010116577:  75%|████████████▊    | 30/40 [00:02<00:00, 13.51it/s]evaluate for the 32-th batch, evaluate loss: 0.48277992010116577:  80%|█████████████▌   | 32/40 [00:02<00:00, 13.66it/s]Epoch: 2, train for the 63-th batch, train loss: 0.7252328395843506:  26%|███▎         | 62/241 [00:09<00:30,  5.96it/s]Epoch: 2, train for the 63-th batch, train loss: 0.7252328395843506:  26%|███▍         | 63/241 [00:09<00:30,  5.85it/s]evaluate for the 33-th batch, evaluate loss: 0.46770262718200684:  80%|█████████████▌   | 32/40 [00:02<00:00, 13.66it/s]Epoch: 3, train for the 67-th batch, train loss: 0.5542246699333191:  45%|█████▉       | 66/146 [00:09<00:11,  6.69it/s]evaluate for the 34-th batch, evaluate loss: 0.5049221515655518:  80%|██████████████▍   | 32/40 [00:02<00:00, 13.66it/s]evaluate for the 34-th batch, evaluate loss: 0.5049221515655518:  85%|███████████████▎  | 34/40 [00:02<00:00, 14.02it/s]Epoch: 3, train for the 67-th batch, train loss: 0.5542246699333191:  46%|█████▉       | 67/146 [00:09<00:11,  6.72it/s]Epoch: 2, train for the 64-th batch, train loss: 0.3122641146183014:  26%|███▍         | 63/241 [00:09<00:30,  5.85it/s]evaluate for the 17-th batch, evaluate loss: 0.4638710618019104:  35%|██████▎           | 16/46 [00:02<00:03,  8.39it/s]evaluate for the 17-th batch, evaluate loss: 0.4638710618019104:  37%|██████▋           | 17/46 [00:02<00:04,  6.04it/s]Epoch: 2, train for the 64-th batch, train loss: 0.3122641146183014:  27%|███▍         | 64/241 [00:09<00:29,  6.07it/s]evaluate for the 50-th batch, evaluate loss: 0.5932238101959229:  74%|█████████████▎    | 49/66 [00:08<00:02,  6.27it/s]evaluate for the 50-th batch, evaluate loss: 0.5932238101959229:  76%|█████████████▋    | 50/66 [00:08<00:03,  4.63it/s]evaluate for the 35-th batch, evaluate loss: 0.5064852833747864:  85%|███████████████▎  | 34/40 [00:02<00:00, 14.02it/s]Epoch: 1, train for the 249-th batch, train loss: 0.43439429998397827:  65%|██████▍   | 248/383 [01:10<00:45,  2.98it/s]evaluate for the 18-th batch, evaluate loss: 0.5197245478630066:  37%|██████▋           | 17/46 [00:02<00:04,  6.04it/s]Epoch: 3, train for the 68-th batch, train loss: 0.4968988001346588:  46%|█████▉       | 67/146 [00:09<00:11,  6.72it/s]Epoch: 3, train for the 68-th batch, train loss: 0.4968988001346588:  47%|██████       | 68/146 [00:09<00:11,  6.91it/s]evaluate for the 36-th batch, evaluate loss: 0.4922044277191162:  85%|███████████████▎  | 34/40 [00:02<00:00, 14.02it/s]evaluate for the 36-th batch, evaluate loss: 0.4922044277191162:  90%|████████████████▏ | 36/40 [00:02<00:00, 13.99it/s]evaluate for the 51-th batch, evaluate loss: 0.5570501685142517:  76%|█████████████▋    | 50/66 [00:08<00:03,  4.63it/s]evaluate for the 51-th batch, evaluate loss: 0.5570501685142517:  77%|█████████████▉    | 51/66 [00:08<00:02,  5.38it/s]Epoch: 1, train for the 249-th batch, train loss: 0.43439429998397827:  65%|██████▌   | 249/383 [01:10<00:44,  3.02it/s]Epoch: 2, train for the 65-th batch, train loss: 0.36476776003837585:  27%|███▏        | 64/241 [00:09<00:29,  6.07it/s]evaluate for the 19-th batch, evaluate loss: 0.5657148361206055:  37%|██████▋           | 17/46 [00:02<00:04,  6.04it/s]evaluate for the 19-th batch, evaluate loss: 0.5657148361206055:  41%|███████▍          | 19/46 [00:02<00:03,  7.39it/s]Epoch: 2, train for the 65-th batch, train loss: 0.36476776003837585:  27%|███▏        | 65/241 [00:09<00:29,  6.02it/s]evaluate for the 37-th batch, evaluate loss: 0.49600034952163696:  90%|███████████████▎ | 36/40 [00:02<00:00, 13.99it/s]evaluate for the 20-th batch, evaluate loss: 0.516948938369751:  41%|███████▊           | 19/46 [00:02<00:03,  7.39it/s]evaluate for the 38-th batch, evaluate loss: 0.5378350615501404:  90%|████████████████▏ | 36/40 [00:02<00:00, 13.99it/s]evaluate for the 38-th batch, evaluate loss: 0.5378350615501404:  95%|█████████████████ | 38/40 [00:02<00:00, 14.05it/s]Epoch: 2, train for the 66-th batch, train loss: 0.307229220867157:  27%|███▊          | 65/241 [00:09<00:29,  6.02it/s]Epoch: 2, train for the 66-th batch, train loss: 0.307229220867157:  27%|███▊          | 66/241 [00:09<00:28,  6.23it/s]evaluate for the 39-th batch, evaluate loss: 0.5279907584190369:  95%|█████████████████ | 38/40 [00:02<00:00, 14.05it/s]Epoch: 1, train for the 250-th batch, train loss: 0.4284803569316864:  65%|███████▏   | 249/383 [01:10<00:44,  3.02it/s]Epoch: 3, train for the 69-th batch, train loss: 0.5156975388526917:  47%|██████       | 68/146 [00:10<00:11,  6.91it/s]evaluate for the 40-th batch, evaluate loss: 0.41441085934638977:  95%|████████████████▏| 38/40 [00:02<00:00, 14.05it/s]evaluate for the 40-th batch, evaluate loss: 0.41441085934638977: 100%|█████████████████| 40/40 [00:02<00:00, 14.47it/s]evaluate for the 40-th batch, evaluate loss: 0.41441085934638977: 100%|█████████████████| 40/40 [00:02<00:00, 13.76it/s]
Epoch: 3, train for the 69-th batch, train loss: 0.5156975388526917:  47%|██████▏      | 69/146 [00:10<00:14,  5.37it/s]evaluate for the 52-th batch, evaluate loss: 0.5472056269645691:  77%|█████████████▉    | 51/66 [00:08<00:02,  5.38it/s]evaluate for the 52-th batch, evaluate loss: 0.5472056269645691:  79%|██████████████▏   | 52/66 [00:08<00:02,  4.93it/s]Epoch: 2, train for the 67-th batch, train loss: 0.417894572019577:  27%|███▊          | 66/241 [00:09<00:28,  6.23it/s]Epoch: 1, train for the 250-th batch, train loss: 0.4284803569316864:  65%|███████▏   | 250/383 [01:10<00:41,  3.19it/s]Epoch: 2, train for the 67-th batch, train loss: 0.417894572019577:  28%|███▉          | 67/241 [00:09<00:25,  6.96it/s]evaluate for the 21-th batch, evaluate loss: 0.5680469870567322:  41%|███████▍          | 19/46 [00:02<00:03,  7.39it/s]evaluate for the 21-th batch, evaluate loss: 0.5680469870567322:  46%|████████▏         | 21/46 [00:02<00:03,  7.14it/s]  0%|                                                                                            | 0/21 [00:00<?, ?it/s]evaluate for the 53-th batch, evaluate loss: 0.5231936573982239:  79%|██████████████▏   | 52/66 [00:09<00:02,  4.93it/s]evaluate for the 53-th batch, evaluate loss: 0.5231936573982239:  80%|██████████████▍   | 53/66 [00:09<00:02,  5.44it/s]Epoch: 3, train for the 70-th batch, train loss: 0.5347307324409485:  47%|██████▏      | 69/146 [00:10<00:14,  5.37it/s]Epoch: 3, train for the 70-th batch, train loss: 0.5347307324409485:  48%|██████▏      | 70/146 [00:10<00:13,  5.57it/s]evaluate for the 22-th batch, evaluate loss: 0.5192095637321472:  46%|████████▏         | 21/46 [00:02<00:03,  7.14it/s]evaluate for the 22-th batch, evaluate loss: 0.5192095637321472:  48%|████████▌         | 22/46 [00:02<00:03,  7.55it/s]Epoch: 2, train for the 68-th batch, train loss: 0.5061108469963074:  28%|███▌         | 67/241 [00:09<00:25,  6.96it/s]Epoch: 2, train for the 68-th batch, train loss: 0.5061108469963074:  28%|███▋         | 68/241 [00:09<00:25,  6.81it/s]evaluate for the 1-th batch, evaluate loss: 0.5980305075645447:   0%|                            | 0/21 [00:00<?, ?it/s]evaluate for the 54-th batch, evaluate loss: 0.563590943813324:  80%|███████████████▎   | 53/66 [00:09<00:02,  5.44it/s]evaluate for the 54-th batch, evaluate loss: 0.563590943813324:  82%|███████████████▌   | 54/66 [00:09<00:01,  6.10it/s]evaluate for the 23-th batch, evaluate loss: 0.47790926694869995:  48%|████████▏        | 22/46 [00:02<00:03,  7.55it/s]Epoch: 3, train for the 71-th batch, train loss: 0.5455289483070374:  48%|██████▏      | 70/146 [00:10<00:13,  5.57it/s]evaluate for the 2-th batch, evaluate loss: 0.677558958530426:   0%|                             | 0/21 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.677558958530426:  10%|██                   | 2/21 [00:00<00:01, 10.30it/s]Epoch: 3, train for the 71-th batch, train loss: 0.5455289483070374:  49%|██████▎      | 71/146 [00:10<00:13,  5.61it/s]Epoch: 2, train for the 69-th batch, train loss: 0.6152483820915222:  28%|███▋         | 68/241 [00:10<00:25,  6.81it/s]evaluate for the 55-th batch, evaluate loss: 0.5062798261642456:  82%|██████████████▋   | 54/66 [00:09<00:01,  6.10it/s]evaluate for the 55-th batch, evaluate loss: 0.5062798261642456:  83%|███████████████   | 55/66 [00:09<00:01,  6.79it/s]evaluate for the 24-th batch, evaluate loss: 0.48818662762641907:  48%|████████▏        | 22/46 [00:02<00:03,  7.55it/s]evaluate for the 24-th batch, evaluate loss: 0.48818662762641907:  52%|████████▊        | 24/46 [00:02<00:02,  8.46it/s]Epoch: 2, train for the 69-th batch, train loss: 0.6152483820915222:  29%|███▋         | 69/241 [00:10<00:26,  6.43it/s]evaluate for the 3-th batch, evaluate loss: 0.6679789423942566:  10%|█▉                  | 2/21 [00:00<00:01, 10.30it/s]Epoch: 1, train for the 251-th batch, train loss: 0.4377201497554779:  65%|███████▏   | 250/383 [01:10<00:41,  3.19it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5105582475662231:  29%|███▋         | 69/241 [00:10<00:26,  6.43it/s]Epoch: 3, train for the 72-th batch, train loss: 0.540087103843689:  49%|██████▊       | 71/146 [00:10<00:13,  5.61it/s]Epoch: 3, train for the 72-th batch, train loss: 0.540087103843689:  49%|██████▉       | 72/146 [00:10<00:12,  5.94it/s]evaluate for the 4-th batch, evaluate loss: 0.5988377332687378:  10%|█▉                  | 2/21 [00:00<00:01, 10.30it/s]evaluate for the 4-th batch, evaluate loss: 0.5988377332687378:  19%|███▊                | 4/21 [00:00<00:01, 11.41it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5105582475662231:  29%|███▊         | 70/241 [00:10<00:24,  6.88it/s]Epoch: 1, train for the 251-th batch, train loss: 0.4377201497554779:  66%|███████▏   | 251/383 [01:10<00:47,  2.80it/s]evaluate for the 25-th batch, evaluate loss: 0.5149992108345032:  52%|█████████▍        | 24/46 [00:02<00:02,  8.46it/s]evaluate for the 25-th batch, evaluate loss: 0.5149992108345032:  54%|█████████▊        | 25/46 [00:02<00:02,  7.73it/s]evaluate for the 5-th batch, evaluate loss: 0.6904036402702332:  19%|███▊                | 4/21 [00:00<00:01, 11.41it/s]evaluate for the 56-th batch, evaluate loss: 0.5336557030677795:  83%|███████████████   | 55/66 [00:09<00:01,  6.79it/s]evaluate for the 56-th batch, evaluate loss: 0.5336557030677795:  85%|███████████████▎  | 56/66 [00:09<00:01,  6.07it/s]Epoch: 3, train for the 73-th batch, train loss: 0.538429319858551:  49%|██████▉       | 72/146 [00:10<00:12,  5.94it/s]Epoch: 3, train for the 73-th batch, train loss: 0.538429319858551:  50%|███████       | 73/146 [00:10<00:11,  6.37it/s]evaluate for the 26-th batch, evaluate loss: 0.5468533039093018:  54%|█████████▊        | 25/46 [00:03<00:02,  7.73it/s]evaluate for the 6-th batch, evaluate loss: 0.6724750995635986:  19%|███▊                | 4/21 [00:00<00:01, 11.41it/s]evaluate for the 6-th batch, evaluate loss: 0.6724750995635986:  29%|█████▋              | 6/21 [00:00<00:01, 11.92it/s]Epoch: 2, train for the 71-th batch, train loss: 0.6041399240493774:  29%|███▊         | 70/241 [00:10<00:24,  6.88it/s]Epoch: 2, train for the 71-th batch, train loss: 0.6041399240493774:  29%|███▊         | 71/241 [00:10<00:26,  6.30it/s]evaluate for the 57-th batch, evaluate loss: 0.5983740091323853:  85%|███████████████▎  | 56/66 [00:09<00:01,  6.07it/s]evaluate for the 57-th batch, evaluate loss: 0.5983740091323853:  86%|███████████████▌  | 57/66 [00:09<00:01,  6.46it/s]evaluate for the 27-th batch, evaluate loss: 0.5376086831092834:  54%|█████████▊        | 25/46 [00:03<00:02,  7.73it/s]evaluate for the 27-th batch, evaluate loss: 0.5376086831092834:  59%|██████████▌       | 27/46 [00:03<00:02,  8.76it/s]evaluate for the 7-th batch, evaluate loss: 0.641196608543396:  29%|██████               | 6/21 [00:00<00:01, 11.92it/s]Epoch: 3, train for the 74-th batch, train loss: 0.5203306078910828:  50%|██████▌      | 73/146 [00:10<00:11,  6.37it/s]Epoch: 3, train for the 74-th batch, train loss: 0.5203306078910828:  51%|██████▌      | 74/146 [00:10<00:11,  6.24it/s]evaluate for the 28-th batch, evaluate loss: 0.5427236557006836:  59%|██████████▌       | 27/46 [00:03<00:02,  8.76it/s]evaluate for the 8-th batch, evaluate loss: 0.6568841934204102:  29%|█████▋              | 6/21 [00:00<00:01, 11.92it/s]evaluate for the 8-th batch, evaluate loss: 0.6568841934204102:  38%|███████▌            | 8/21 [00:00<00:01, 11.57it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5591802000999451:  29%|███▊         | 71/241 [00:10<00:26,  6.30it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5591802000999451:  30%|███▉         | 72/241 [00:10<00:28,  6.02it/s]Epoch: 3, train for the 75-th batch, train loss: 0.5521169900894165:  51%|██████▌      | 74/146 [00:10<00:11,  6.24it/s]evaluate for the 9-th batch, evaluate loss: 0.6389965415000916:  38%|███████▌            | 8/21 [00:00<00:01, 11.57it/s]Epoch: 3, train for the 75-th batch, train loss: 0.5521169900894165:  51%|██████▋      | 75/146 [00:10<00:10,  6.48it/s]Epoch: 2, train for the 73-th batch, train loss: 0.49075889587402344:  30%|███▌        | 72/241 [00:10<00:28,  6.02it/s]evaluate for the 10-th batch, evaluate loss: 0.6704601645469666:  38%|███████▏           | 8/21 [00:00<00:01, 11.57it/s]evaluate for the 10-th batch, evaluate loss: 0.6704601645469666:  48%|████████▌         | 10/21 [00:00<00:00, 11.74it/s]Epoch: 1, train for the 252-th batch, train loss: 0.31654423475265503:  66%|██████▌   | 251/383 [01:11<00:47,  2.80it/s]evaluate for the 58-th batch, evaluate loss: 0.5566906332969666:  86%|███████████████▌  | 57/66 [00:09<00:01,  6.46it/s]evaluate for the 58-th batch, evaluate loss: 0.5566906332969666:  88%|███████████████▊  | 58/66 [00:09<00:01,  4.91it/s]evaluate for the 29-th batch, evaluate loss: 0.5124839544296265:  59%|██████████▌       | 27/46 [00:03<00:02,  8.76it/s]evaluate for the 29-th batch, evaluate loss: 0.5124839544296265:  63%|███████████▎      | 29/46 [00:03<00:02,  7.84it/s]Epoch: 3, train for the 76-th batch, train loss: 0.5219060182571411:  51%|██████▋      | 75/146 [00:11<00:10,  6.48it/s]Epoch: 3, train for the 76-th batch, train loss: 0.5219060182571411:  52%|██████▊      | 76/146 [00:11<00:10,  6.75it/s]evaluate for the 11-th batch, evaluate loss: 0.6607153415679932:  48%|████████▌         | 10/21 [00:00<00:00, 11.74it/s]Epoch: 1, train for the 252-th batch, train loss: 0.31654423475265503:  66%|██████▌   | 252/383 [01:11<00:56,  2.32it/s]Epoch: 2, train for the 74-th batch, train loss: 0.4337140917778015:  30%|███▉         | 72/241 [00:10<00:28,  6.02it/s]evaluate for the 30-th batch, evaluate loss: 0.49805372953414917:  63%|██████████▋      | 29/46 [00:03<00:02,  7.84it/s]evaluate for the 59-th batch, evaluate loss: 0.5694695115089417:  88%|███████████████▊  | 58/66 [00:10<00:01,  4.91it/s]evaluate for the 59-th batch, evaluate loss: 0.5694695115089417:  89%|████████████████  | 59/66 [00:10<00:01,  5.74it/s]evaluate for the 12-th batch, evaluate loss: 0.6507742404937744:  48%|████████▌         | 10/21 [00:01<00:00, 11.74it/s]evaluate for the 12-th batch, evaluate loss: 0.6507742404937744:  57%|██████████▎       | 12/21 [00:01<00:00, 12.41it/s]Epoch: 2, train for the 74-th batch, train loss: 0.4337140917778015:  31%|███▉         | 74/241 [00:10<00:25,  6.57it/s]Epoch: 3, train for the 77-th batch, train loss: 0.5847030878067017:  52%|██████▊      | 76/146 [00:11<00:10,  6.75it/s]Epoch: 3, train for the 77-th batch, train loss: 0.5847030878067017:  53%|██████▊      | 77/146 [00:11<00:09,  6.99it/s]evaluate for the 31-th batch, evaluate loss: 0.4504653811454773:  63%|███████████▎      | 29/46 [00:03<00:02,  7.84it/s]evaluate for the 31-th batch, evaluate loss: 0.4504653811454773:  67%|████████████▏     | 31/46 [00:03<00:01,  8.80it/s]evaluate for the 13-th batch, evaluate loss: 0.6396015882492065:  57%|██████████▎       | 12/21 [00:01<00:00, 12.41it/s]evaluate for the 14-th batch, evaluate loss: 0.6216006278991699:  57%|██████████▎       | 12/21 [00:01<00:00, 12.41it/s]evaluate for the 14-th batch, evaluate loss: 0.6216006278991699:  67%|████████████      | 14/21 [00:01<00:00, 12.87it/s]Epoch: 2, train for the 75-th batch, train loss: 0.3863186538219452:  31%|███▉         | 74/241 [00:10<00:25,  6.57it/s]evaluate for the 32-th batch, evaluate loss: 0.4839799404144287:  67%|████████████▏     | 31/46 [00:03<00:01,  8.80it/s]Epoch: 2, train for the 75-th batch, train loss: 0.3863186538219452:  31%|████         | 75/241 [00:10<00:25,  6.50it/s]Epoch: 3, train for the 78-th batch, train loss: 0.5279092788696289:  53%|██████▊      | 77/146 [00:11<00:09,  6.99it/s]Epoch: 1, train for the 253-th batch, train loss: 0.3192070424556732:  66%|███████▏   | 252/383 [01:11<00:56,  2.32it/s]Epoch: 3, train for the 78-th batch, train loss: 0.5279092788696289:  53%|██████▉      | 78/146 [00:11<00:09,  7.05it/s]evaluate for the 15-th batch, evaluate loss: 0.6725153923034668:  67%|████████████      | 14/21 [00:01<00:00, 12.87it/s]Epoch: 1, train for the 253-th batch, train loss: 0.3192070424556732:  66%|███████▎   | 253/383 [01:11<00:50,  2.60it/s]Epoch: 2, train for the 76-th batch, train loss: 0.30086272954940796:  31%|███▋        | 75/241 [00:11<00:25,  6.50it/s]Epoch: 2, train for the 76-th batch, train loss: 0.30086272954940796:  32%|███▊        | 76/241 [00:11<00:23,  7.07it/s]evaluate for the 16-th batch, evaluate loss: 0.6470302939414978:  67%|████████████      | 14/21 [00:01<00:00, 12.87it/s]evaluate for the 16-th batch, evaluate loss: 0.6470302939414978:  76%|█████████████▋    | 16/21 [00:01<00:00, 12.70it/s]Epoch: 3, train for the 79-th batch, train loss: 0.5628228783607483:  53%|██████▉      | 78/146 [00:11<00:09,  7.05it/s]Epoch: 3, train for the 79-th batch, train loss: 0.5628228783607483:  54%|███████      | 79/146 [00:11<00:09,  6.94it/s]Epoch: 2, train for the 77-th batch, train loss: 0.3613721430301666:  32%|████         | 76/241 [00:11<00:23,  7.07it/s]evaluate for the 33-th batch, evaluate loss: 0.49549636244773865:  67%|███████████▍     | 31/46 [00:03<00:01,  8.80it/s]evaluate for the 33-th batch, evaluate loss: 0.49549636244773865:  72%|████████████▏    | 33/46 [00:03<00:01,  7.70it/s]evaluate for the 17-th batch, evaluate loss: 0.5822120308876038:  76%|█████████████▋    | 16/21 [00:01<00:00, 12.70it/s]evaluate for the 60-th batch, evaluate loss: 0.5708930492401123:  89%|████████████████  | 59/66 [00:10<00:01,  5.74it/s]evaluate for the 60-th batch, evaluate loss: 0.5708930492401123:  91%|████████████████▎ | 60/66 [00:10<00:01,  4.14it/s]Epoch: 2, train for the 77-th batch, train loss: 0.3613721430301666:  32%|████▏        | 77/241 [00:11<00:22,  7.29it/s]Epoch: 3, train for the 80-th batch, train loss: 0.5444847941398621:  54%|███████      | 79/146 [00:11<00:09,  6.94it/s]Epoch: 1, train for the 254-th batch, train loss: 0.4541417062282562:  66%|███████▎   | 253/383 [01:12<00:50,  2.60it/s]Epoch: 3, train for the 80-th batch, train loss: 0.5444847941398621:  55%|███████      | 80/146 [00:11<00:08,  7.34it/s]evaluate for the 34-th batch, evaluate loss: 0.45625045895576477:  72%|████████████▏    | 33/46 [00:04<00:01,  7.70it/s]evaluate for the 18-th batch, evaluate loss: 0.6524078249931335:  76%|█████████████▋    | 16/21 [00:01<00:00, 12.70it/s]evaluate for the 18-th batch, evaluate loss: 0.6524078249931335:  86%|███████████████▍  | 18/21 [00:01<00:00, 12.43it/s]evaluate for the 61-th batch, evaluate loss: 0.5766717195510864:  91%|████████████████▎ | 60/66 [00:10<00:01,  4.14it/s]evaluate for the 61-th batch, evaluate loss: 0.5766717195510864:  92%|████████████████▋ | 61/66 [00:10<00:01,  4.96it/s]Epoch: 1, train for the 254-th batch, train loss: 0.4541417062282562:  66%|███████▎   | 254/383 [01:12<00:45,  2.83it/s]Epoch: 2, train for the 78-th batch, train loss: 0.5436450242996216:  32%|████▏        | 77/241 [00:11<00:22,  7.29it/s]Epoch: 2, train for the 78-th batch, train loss: 0.5436450242996216:  32%|████▏        | 78/241 [00:11<00:23,  7.06it/s]evaluate for the 19-th batch, evaluate loss: 0.6464575529098511:  86%|███████████████▍  | 18/21 [00:01<00:00, 12.43it/s]evaluate for the 35-th batch, evaluate loss: 0.5149385929107666:  72%|████████████▉     | 33/46 [00:04<00:01,  7.70it/s]evaluate for the 35-th batch, evaluate loss: 0.5149385929107666:  76%|█████████████▋    | 35/46 [00:04<00:01,  8.71it/s]Epoch: 3, train for the 81-th batch, train loss: 0.5092657804489136:  55%|███████      | 80/146 [00:11<00:08,  7.34it/s]evaluate for the 20-th batch, evaluate loss: 0.6380402445793152:  86%|███████████████▍  | 18/21 [00:01<00:00, 12.43it/s]evaluate for the 20-th batch, evaluate loss: 0.6380402445793152:  95%|█████████████████▏| 20/21 [00:01<00:00, 12.94it/s]Epoch: 3, train for the 81-th batch, train loss: 0.5092657804489136:  55%|███████▏     | 81/146 [00:11<00:09,  7.07it/s]evaluate for the 36-th batch, evaluate loss: 0.48127955198287964:  76%|████████████▉    | 35/46 [00:04<00:01,  8.71it/s]evaluate for the 21-th batch, evaluate loss: 0.5951682925224304:  95%|█████████████████▏| 20/21 [00:01<00:00, 12.94it/s]Epoch: 2, train for the 79-th batch, train loss: 0.4575292468070984:  32%|████▏        | 78/241 [00:11<00:23,  7.06it/s]evaluate for the 21-th batch, evaluate loss: 0.5951682925224304: 100%|██████████████████| 21/21 [00:01<00:00, 12.42it/s]
Epoch: 2, train for the 79-th batch, train loss: 0.4575292468070984:  33%|████▎        | 79/241 [00:11<00:23,  6.87it/s]Epoch: 2, train for the 80-th batch, train loss: 0.3383462727069855:  33%|████▎        | 79/241 [00:11<00:23,  6.87it/s]evaluate for the 37-th batch, evaluate loss: 0.5175586342811584:  76%|█████████████▋    | 35/46 [00:04<00:01,  8.71it/s]evaluate for the 37-th batch, evaluate loss: 0.5175586342811584:  80%|██████████████▍   | 37/46 [00:04<00:01,  8.13it/s]Epoch: 3, train for the 82-th batch, train loss: 0.5270357131958008:  55%|███████▏     | 81/146 [00:12<00:09,  7.07it/s]Epoch: 3, train for the 82-th batch, train loss: 0.5270357131958008:  56%|███████▎     | 82/146 [00:12<00:10,  5.90it/s]Epoch: 1, train for the 255-th batch, train loss: 0.40289387106895447:  66%|██████▋   | 254/383 [01:12<00:45,  2.83it/s]evaluate for the 62-th batch, evaluate loss: 0.5941420793533325:  92%|████████████████▋ | 61/66 [00:10<00:01,  4.96it/s]evaluate for the 62-th batch, evaluate loss: 0.5941420793533325:  94%|████████████████▉ | 62/66 [00:10<00:01,  3.97it/s]Epoch: 1, train for the 255-th batch, train loss: 0.40289387106895447:  67%|██████▋   | 255/383 [01:12<00:47,  2.67it/s]Epoch: 2, train for the 81-th batch, train loss: 0.3783896565437317:  33%|████▎        | 79/241 [00:11<00:23,  6.87it/s]evaluate for the 38-th batch, evaluate loss: 0.5049522519111633:  80%|██████████████▍   | 37/46 [00:04<00:01,  8.13it/s]evaluate for the 38-th batch, evaluate loss: 0.5049522519111633:  83%|██████████████▊   | 38/46 [00:04<00:00,  8.30it/s]Epoch: 2, train for the 81-th batch, train loss: 0.3783896565437317:  34%|████▎        | 81/241 [00:11<00:22,  7.27it/s]Epoch: 3, train for the 83-th batch, train loss: 0.5217382907867432:  56%|███████▎     | 82/146 [00:12<00:10,  5.90it/s]evaluate for the 63-th batch, evaluate loss: 0.548812985420227:  94%|█████████████████▊ | 62/66 [00:11<00:01,  3.97it/s]Epoch: 3, train for the 83-th batch, train loss: 0.5217382907867432:  57%|███████▍     | 83/146 [00:12<00:09,  6.45it/s]evaluate for the 39-th batch, evaluate loss: 0.4891488254070282:  83%|██████████████▊   | 38/46 [00:04<00:00,  8.30it/s]INFO:root:Epoch: 3, learning rate: 0.0001, train loss: 0.4936
INFO:root:train average_precision, 0.8520
INFO:root:train roc_auc, 0.8415
INFO:root:validate loss: 0.4912
INFO:root:validate average_precision, 0.8645
INFO:root:validate roc_auc, 0.8597
INFO:root:new node validate loss: 0.6438
INFO:root:new node validate first_1_average_precision, 0.6990
INFO:root:new node validate first_1_roc_auc, 0.6784
INFO:root:new node validate first_3_average_precision, 0.7293
INFO:root:new node validate first_3_roc_auc, 0.7180
INFO:root:new node validate first_10_average_precision, 0.7319
INFO:root:new node validate first_10_roc_auc, 0.7301
INFO:root:new node validate average_precision, 0.7234
INFO:root:new node validate roc_auc, 0.7367
INFO:root:save model ./saved_models/TGN/ia-reality-call/TGN_seed0_tgn-ia-reality-call-lincorrect-time-linear/TGN_seed0_tgn-ia-reality-call-lincorrect-time-linear.pkl
Epoch: 3, train for the 84-th batch, train loss: 0.4687947630882263:  57%|███████▍     | 83/146 [00:12<00:09,  6.45it/s]Epoch: 3, train for the 84-th batch, train loss: 0.4687947630882263:  58%|███████▍     | 84/146 [00:12<00:08,  6.90it/s]Epoch: 2, train for the 82-th batch, train loss: 0.4313181936740875:  34%|████▎        | 81/241 [00:11<00:22,  7.27it/s]evaluate for the 40-th batch, evaluate loss: 0.47917136549949646:  83%|██████████████   | 38/46 [00:04<00:00,  8.30it/s]evaluate for the 40-th batch, evaluate loss: 0.47917136549949646:  87%|██████████████▊  | 40/46 [00:04<00:00,  9.35it/s]Epoch: 2, train for the 82-th batch, train loss: 0.4313181936740875:  34%|████▍        | 82/241 [00:11<00:22,  7.06it/s]Epoch: 1, train for the 256-th batch, train loss: 0.3441716730594635:  67%|███████▎   | 255/383 [01:12<00:47,  2.67it/s]Epoch: 3, train for the 85-th batch, train loss: 0.5183924436569214:  58%|███████▍     | 84/146 [00:12<00:08,  6.90it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5144837498664856:  34%|████▍        | 82/241 [00:12<00:22,  7.06it/s]Epoch: 1, train for the 256-th batch, train loss: 0.3441716730594635:  67%|███████▎   | 256/383 [01:12<00:43,  2.92it/s]Epoch: 2, train for the 83-th batch, train loss: 0.5144837498664856:  34%|████▍        | 83/241 [00:12<00:20,  7.53it/s]evaluate for the 64-th batch, evaluate loss: 0.5458177328109741:  94%|████████████████▉ | 62/66 [00:11<00:01,  3.97it/s]evaluate for the 64-th batch, evaluate loss: 0.5458177328109741:  97%|█████████████████▍| 64/66 [00:11<00:00,  4.57it/s]Epoch: 3, train for the 86-th batch, train loss: 0.5039905905723572:  58%|███████▍     | 84/146 [00:12<00:08,  6.90it/s]Epoch: 3, train for the 86-th batch, train loss: 0.5039905905723572:  59%|███████▋     | 86/146 [00:12<00:07,  8.01it/s]evaluate for the 41-th batch, evaluate loss: 0.4821186363697052:  87%|███████████████▋  | 40/46 [00:04<00:00,  9.35it/s]evaluate for the 41-th batch, evaluate loss: 0.4821186363697052:  89%|████████████████  | 41/46 [00:04<00:00,  7.91it/s]evaluate for the 65-th batch, evaluate loss: 0.5882555842399597:  97%|█████████████████▍| 64/66 [00:11<00:00,  4.57it/s]evaluate for the 65-th batch, evaluate loss: 0.5882555842399597:  98%|█████████████████▋| 65/66 [00:11<00:00,  5.18it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5947088003158569:  34%|████▍        | 83/241 [00:12<00:20,  7.53it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5947088003158569:  35%|████▌        | 84/241 [00:12<00:22,  7.05it/s]Epoch: 3, train for the 87-th batch, train loss: 0.5173691511154175:  59%|███████▋     | 86/146 [00:12<00:07,  8.01it/s]Epoch: 3, train for the 87-th batch, train loss: 0.5173691511154175:  60%|███████▋     | 87/146 [00:12<00:07,  8.28it/s]evaluate for the 42-th batch, evaluate loss: 0.46462148427963257:  89%|███████████████▏ | 41/46 [00:04<00:00,  7.91it/s]evaluate for the 43-th batch, evaluate loss: 0.5521680116653442:  89%|████████████████  | 41/46 [00:05<00:00,  7.91it/s]evaluate for the 43-th batch, evaluate loss: 0.5521680116653442:  93%|████████████████▊ | 43/46 [00:05<00:00,  9.10it/s]Epoch: 1, train for the 257-th batch, train loss: 0.4948965311050415:  67%|███████▎   | 256/383 [01:13<00:43,  2.92it/s]evaluate for the 66-th batch, evaluate loss: 0.5423487424850464:  98%|█████████████████▋| 65/66 [00:11<00:00,  5.18it/s]evaluate for the 66-th batch, evaluate loss: 0.5423487424850464: 100%|██████████████████| 66/66 [00:11<00:00,  5.63it/s]evaluate for the 66-th batch, evaluate loss: 0.5423487424850464: 100%|██████████████████| 66/66 [00:11<00:00,  5.72it/s]
Epoch: 3, train for the 88-th batch, train loss: 0.5486451387405396:  60%|███████▋     | 87/146 [00:12<00:07,  8.28it/s]Epoch: 3, train for the 88-th batch, train loss: 0.5486451387405396:  60%|███████▊     | 88/146 [00:12<00:06,  8.63it/s]Epoch: 1, train for the 257-th batch, train loss: 0.4948965311050415:  67%|███████▍   | 257/383 [01:13<00:42,  2.97it/s]Epoch: 3, train for the 89-th batch, train loss: 0.5082913041114807:  60%|███████▊     | 88/146 [00:12<00:06,  8.63it/s]Epoch: 2, train for the 85-th batch, train loss: 0.4261438548564911:  35%|████▌        | 84/241 [00:12<00:22,  7.05it/s]Epoch: 2, train for the 85-th batch, train loss: 0.4261438548564911:  35%|████▌        | 85/241 [00:12<00:26,  5.80it/s]evaluate for the 44-th batch, evaluate loss: 0.4992782771587372:  93%|████████████████▊ | 43/46 [00:05<00:00,  9.10it/s]evaluate for the 44-th batch, evaluate loss: 0.4992782771587372:  96%|█████████████████▏| 44/46 [00:05<00:00,  7.95it/s]Epoch: 1, train for the 258-th batch, train loss: 0.339862197637558:  67%|████████    | 257/383 [01:13<00:42,  2.97it/s]Epoch: 3, train for the 90-th batch, train loss: 0.5239881873130798:  60%|███████▊     | 88/146 [00:12<00:06,  8.63it/s]Epoch: 3, train for the 90-th batch, train loss: 0.5239881873130798:  62%|████████     | 90/146 [00:12<00:06,  8.83it/s]evaluate for the 45-th batch, evaluate loss: 0.4601041376590729:  96%|█████████████████▏| 44/46 [00:05<00:00,  7.95it/s]Epoch: 1, train for the 258-th batch, train loss: 0.339862197637558:  67%|████████    | 258/383 [01:13<00:37,  3.30it/s]Epoch: 2, train for the 86-th batch, train loss: 0.499723345041275:  35%|████▉         | 85/241 [00:12<00:26,  5.80it/s]Epoch: 2, train for the 86-th batch, train loss: 0.499723345041275:  36%|████▉         | 86/241 [00:12<00:25,  6.06it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]Epoch: 3, train for the 91-th batch, train loss: 0.5271058678627014:  62%|████████     | 90/146 [00:12<00:06,  8.83it/s]evaluate for the 46-th batch, evaluate loss: 0.45824453234672546:  96%|████████████████▎| 44/46 [00:05<00:00,  7.95it/s]evaluate for the 46-th batch, evaluate loss: 0.45824453234672546: 100%|█████████████████| 46/46 [00:05<00:00,  9.12it/s]evaluate for the 46-th batch, evaluate loss: 0.45824453234672546: 100%|█████████████████| 46/46 [00:05<00:00,  8.50it/s]
Epoch: 3, train for the 91-th batch, train loss: 0.5271058678627014:  62%|████████     | 91/146 [00:13<00:06,  9.05it/s]Epoch: 3, train for the 92-th batch, train loss: 0.5844324231147766:  62%|████████     | 91/146 [00:13<00:06,  9.05it/s]Epoch: 3, train for the 92-th batch, train loss: 0.5844324231147766:  63%|████████▏    | 92/146 [00:13<00:05,  9.01it/s]Epoch: 1, train for the 259-th batch, train loss: 0.4262837767601013:  67%|███████▍   | 258/383 [01:13<00:37,  3.30it/s]Epoch: 2, train for the 87-th batch, train loss: 0.5651711225509644:  36%|████▋        | 86/241 [00:12<00:25,  6.06it/s]Epoch: 2, train for the 87-th batch, train loss: 0.5651711225509644:  36%|████▋        | 87/241 [00:12<00:27,  5.63it/s]Epoch: 4, train for the 1-th batch, train loss: 0.6429162621498108:   0%|                       | 0/119 [00:00<?, ?it/s]Epoch: 4, train for the 1-th batch, train loss: 0.6429162621498108:   1%|▏              | 1/119 [00:00<00:24,  4.80it/s]Epoch: 1, train for the 259-th batch, train loss: 0.4262837767601013:  68%|███████▍   | 259/383 [01:13<00:35,  3.46it/s]Epoch: 3, train for the 93-th batch, train loss: 0.5324280261993408:  63%|████████▏    | 92/146 [00:13<00:05,  9.01it/s]Epoch: 3, train for the 93-th batch, train loss: 0.5324280261993408:  64%|████████▎    | 93/146 [00:13<00:06,  8.78it/s]Epoch: 2, train for the 88-th batch, train loss: 0.48815304040908813:  36%|████▎       | 87/241 [00:12<00:27,  5.63it/s]Epoch: 2, train for the 88-th batch, train loss: 0.48815304040908813:  37%|████▍       | 88/241 [00:12<00:23,  6.42it/s]Epoch: 4, train for the 2-th batch, train loss: 0.7204592823982239:   1%|▏              | 1/119 [00:00<00:24,  4.80it/s]Epoch: 4, train for the 2-th batch, train loss: 0.7204592823982239:   2%|▎              | 2/119 [00:00<00:18,  6.50it/s]Epoch: 3, train for the 94-th batch, train loss: 0.5091179013252258:  64%|████████▎    | 93/146 [00:13<00:06,  8.78it/s]Epoch: 3, train for the 94-th batch, train loss: 0.5091179013252258:  64%|████████▎    | 94/146 [00:13<00:06,  8.47it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5403512120246887:  37%|████▋        | 88/241 [00:12<00:23,  6.42it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5403512120246887:  37%|████▊        | 89/241 [00:13<00:21,  6.99it/s]Epoch: 1, train for the 260-th batch, train loss: 0.3396034836769104:  68%|███████▍   | 259/383 [01:13<00:35,  3.46it/s]Epoch: 4, train for the 3-th batch, train loss: 0.754620373249054:   2%|▎               | 2/119 [00:00<00:18,  6.50it/s]Epoch: 4, train for the 3-th batch, train loss: 0.754620373249054:   3%|▍               | 3/119 [00:00<00:15,  7.42it/s]Epoch: 1, train for the 260-th batch, train loss: 0.3396034836769104:  68%|███████▍   | 260/383 [01:13<00:34,  3.54it/s]Epoch: 2, train for the 90-th batch, train loss: 0.4959860146045685:  37%|████▊        | 89/241 [00:13<00:21,  6.99it/s]Epoch: 2, train for the 90-th batch, train loss: 0.4959860146045685:  37%|████▊        | 90/241 [00:13<00:20,  7.45it/s]Epoch: 3, train for the 95-th batch, train loss: 0.5441136956214905:  64%|████████▎    | 94/146 [00:13<00:06,  8.47it/s]Epoch: 3, train for the 95-th batch, train loss: 0.5441136956214905:  65%|████████▍    | 95/146 [00:13<00:06,  7.74it/s]Epoch: 4, train for the 4-th batch, train loss: 0.6890935897827148:   3%|▍              | 3/119 [00:00<00:15,  7.42it/s]Epoch: 4, train for the 4-th batch, train loss: 0.6890935897827148:   3%|▌              | 4/119 [00:00<00:15,  7.43it/s]Epoch: 3, train for the 96-th batch, train loss: 0.50775146484375:  65%|█████████▊     | 95/146 [00:13<00:06,  7.74it/s]Epoch: 3, train for the 96-th batch, train loss: 0.50775146484375:  66%|█████████▊     | 96/146 [00:13<00:06,  7.49it/s]Epoch: 2, train for the 91-th batch, train loss: 0.4877093732357025:  37%|████▊        | 90/241 [00:13<00:20,  7.45it/s]  0%|                                                                                            | 0/25 [00:00<?, ?it/s]Epoch: 4, train for the 5-th batch, train loss: 0.6338078379631042:   3%|▌              | 4/119 [00:00<00:15,  7.43it/s]Epoch: 4, train for the 5-th batch, train loss: 0.6338078379631042:   4%|▋              | 5/119 [00:00<00:16,  7.11it/s]Epoch: 2, train for the 91-th batch, train loss: 0.4877093732357025:  38%|████▉        | 91/241 [00:13<00:22,  6.53it/s]evaluate for the 1-th batch, evaluate loss: 0.730764627456665:   0%|                             | 0/25 [00:00<?, ?it/s]Epoch: 3, train for the 97-th batch, train loss: 0.5158457159996033:  66%|████████▌    | 96/146 [00:13<00:06,  7.49it/s]Epoch: 3, train for the 97-th batch, train loss: 0.5158457159996033:  66%|████████▋    | 97/146 [00:13<00:06,  7.31it/s]Epoch: 4, train for the 6-th batch, train loss: 0.5304841995239258:   4%|▋              | 5/119 [00:00<00:16,  7.11it/s]Epoch: 2, train for the 92-th batch, train loss: 0.40552741289138794:  38%|████▌       | 91/241 [00:13<00:22,  6.53it/s]Epoch: 4, train for the 6-th batch, train loss: 0.5304841995239258:   5%|▊              | 6/119 [00:00<00:16,  7.02it/s]evaluate for the 2-th batch, evaluate loss: 0.7481207251548767:   0%|                            | 0/25 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.7481207251548767:   8%|█▌                  | 2/25 [00:00<00:01, 12.10it/s]Epoch: 2, train for the 92-th batch, train loss: 0.40552741289138794:  38%|████▌       | 92/241 [00:13<00:22,  6.58it/s]Epoch: 1, train for the 261-th batch, train loss: 0.33841079473495483:  68%|██████▊   | 260/383 [01:14<00:34,  3.54it/s]evaluate for the 3-th batch, evaluate loss: 0.7557125687599182:   8%|█▌                  | 2/25 [00:00<00:01, 12.10it/s]Epoch: 3, train for the 98-th batch, train loss: 0.5030288696289062:  66%|████████▋    | 97/146 [00:13<00:06,  7.31it/s]Epoch: 3, train for the 98-th batch, train loss: 0.5030288696289062:  67%|████████▋    | 98/146 [00:13<00:06,  7.22it/s]Epoch: 2, train for the 93-th batch, train loss: 0.43453001976013184:  38%|████▌       | 92/241 [00:13<00:22,  6.58it/s]Epoch: 4, train for the 7-th batch, train loss: 0.5041926503181458:   5%|▊              | 6/119 [00:01<00:16,  7.02it/s]Epoch: 4, train for the 7-th batch, train loss: 0.5041926503181458:   6%|▉              | 7/119 [00:01<00:16,  6.95it/s]Epoch: 1, train for the 261-th batch, train loss: 0.33841079473495483:  68%|██████▊   | 261/383 [01:14<00:43,  2.82it/s]Epoch: 2, train for the 93-th batch, train loss: 0.43453001976013184:  39%|████▋       | 93/241 [00:13<00:21,  6.74it/s]evaluate for the 4-th batch, evaluate loss: 0.7105231881141663:   8%|█▌                  | 2/25 [00:00<00:01, 12.10it/s]evaluate for the 4-th batch, evaluate loss: 0.7105231881141663:  16%|███▏                | 4/25 [00:00<00:01, 12.59it/s]evaluate for the 5-th batch, evaluate loss: 0.7364242672920227:  16%|███▏                | 4/25 [00:00<00:01, 12.59it/s]Epoch: 4, train for the 8-th batch, train loss: 0.4421442449092865:   6%|▉              | 7/119 [00:01<00:16,  6.95it/s]Epoch: 4, train for the 8-th batch, train loss: 0.4421442449092865:   7%|█              | 8/119 [00:01<00:15,  7.11it/s]Epoch: 2, train for the 94-th batch, train loss: 0.3932878077030182:  39%|█████        | 93/241 [00:13<00:21,  6.74it/s]Epoch: 2, train for the 94-th batch, train loss: 0.3932878077030182:  39%|█████        | 94/241 [00:13<00:22,  6.67it/s]Epoch: 3, train for the 99-th batch, train loss: 0.5448336005210876:  67%|████████▋    | 98/146 [00:14<00:06,  7.22it/s]evaluate for the 6-th batch, evaluate loss: 0.736129879951477:  16%|███▎                 | 4/25 [00:00<00:01, 12.59it/s]evaluate for the 6-th batch, evaluate loss: 0.736129879951477:  24%|█████                | 6/25 [00:00<00:01, 12.44it/s]Epoch: 3, train for the 99-th batch, train loss: 0.5448336005210876:  68%|████████▊    | 99/146 [00:14<00:07,  6.28it/s]Epoch: 1, train for the 262-th batch, train loss: 0.3621711730957031:  68%|███████▍   | 261/383 [01:14<00:43,  2.82it/s]Epoch: 4, train for the 9-th batch, train loss: 0.44713881611824036:   7%|▉             | 8/119 [00:01<00:15,  7.11it/s]Epoch: 4, train for the 9-th batch, train loss: 0.44713881611824036:   8%|█             | 9/119 [00:01<00:14,  7.51it/s]Epoch: 1, train for the 262-th batch, train loss: 0.3621711730957031:  68%|███████▌   | 262/383 [01:14<00:39,  3.07it/s]evaluate for the 7-th batch, evaluate loss: 0.751879870891571:  24%|█████                | 6/25 [00:00<00:01, 12.44it/s]Epoch: 2, train for the 95-th batch, train loss: 0.4415546655654907:  39%|█████        | 94/241 [00:13<00:22,  6.67it/s]  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 3, train for the 100-th batch, train loss: 0.5980488657951355:  68%|████████▏   | 99/146 [00:14<00:07,  6.28it/s]Epoch: 2, train for the 95-th batch, train loss: 0.4415546655654907:  39%|█████        | 95/241 [00:13<00:22,  6.59it/s]Epoch: 3, train for the 100-th batch, train loss: 0.5980488657951355:  68%|███████▌   | 100/146 [00:14<00:07,  6.52it/s]evaluate for the 8-th batch, evaluate loss: 0.7257017493247986:  24%|████▊               | 6/25 [00:00<00:01, 12.44it/s]evaluate for the 8-th batch, evaluate loss: 0.7257017493247986:  32%|██████▍             | 8/25 [00:00<00:01, 11.87it/s]Epoch: 4, train for the 10-th batch, train loss: 0.4675839841365814:   8%|█             | 9/119 [00:01<00:14,  7.51it/s]Epoch: 4, train for the 10-th batch, train loss: 0.4675839841365814:   8%|█            | 10/119 [00:01<00:14,  7.50it/s]evaluate for the 9-th batch, evaluate loss: 0.7021897435188293:  32%|██████▍             | 8/25 [00:00<00:01, 11.87it/s]Epoch: 3, train for the 101-th batch, train loss: 0.5345770716667175:  68%|███████▌   | 100/146 [00:14<00:07,  6.52it/s]Epoch: 3, train for the 101-th batch, train loss: 0.5345770716667175:  69%|███████▌   | 101/146 [00:14<00:06,  6.47it/s]Epoch: 2, train for the 96-th batch, train loss: 0.5343490242958069:  39%|█████        | 95/241 [00:14<00:22,  6.59it/s]Epoch: 1, train for the 263-th batch, train loss: 0.4162989854812622:  68%|███████▌   | 262/383 [01:14<00:39,  3.07it/s]evaluate for the 1-th batch, evaluate loss: 0.6643599271774292:   0%|                            | 0/40 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6643599271774292:   2%|▌                   | 1/40 [00:00<00:07,  5.19it/s]Epoch: 4, train for the 11-th batch, train loss: 0.4667794406414032:   8%|█            | 10/119 [00:01<00:14,  7.50it/s]Epoch: 2, train for the 96-th batch, train loss: 0.5343490242958069:  40%|█████▏       | 96/241 [00:14<00:23,  6.09it/s]Epoch: 4, train for the 11-th batch, train loss: 0.4667794406414032:   9%|█▏           | 11/119 [00:01<00:14,  7.65it/s]evaluate for the 10-th batch, evaluate loss: 0.7599585652351379:  32%|██████             | 8/25 [00:00<00:01, 11.87it/s]evaluate for the 10-th batch, evaluate loss: 0.7599585652351379:  40%|███████▏          | 10/25 [00:00<00:01, 11.77it/s]Epoch: 1, train for the 263-th batch, train loss: 0.4162989854812622:  69%|███████▌   | 263/383 [01:14<00:37,  3.19it/s]Epoch: 3, train for the 102-th batch, train loss: 0.5335643887519836:  69%|███████▌   | 101/146 [00:14<00:06,  6.47it/s]evaluate for the 2-th batch, evaluate loss: 0.6477555632591248:   2%|▌                   | 1/40 [00:00<00:07,  5.19it/s]Epoch: 3, train for the 102-th batch, train loss: 0.5335643887519836:  70%|███████▋   | 102/146 [00:14<00:06,  6.66it/s]evaluate for the 2-th batch, evaluate loss: 0.6477555632591248:   5%|█                   | 2/40 [00:00<00:05,  6.62it/s]Epoch: 4, train for the 12-th batch, train loss: 0.4449059069156647:   9%|█▏           | 11/119 [00:01<00:14,  7.65it/s]Epoch: 4, train for the 12-th batch, train loss: 0.4449059069156647:  10%|█▎           | 12/119 [00:01<00:14,  7.42it/s]evaluate for the 11-th batch, evaluate loss: 0.741755485534668:  40%|███████▌           | 10/25 [00:00<00:01, 11.77it/s]Epoch: 2, train for the 97-th batch, train loss: 0.3924658000469208:  40%|█████▏       | 96/241 [00:14<00:23,  6.09it/s]Epoch: 2, train for the 97-th batch, train loss: 0.3924658000469208:  40%|█████▏       | 97/241 [00:14<00:24,  5.88it/s]Epoch: 3, train for the 103-th batch, train loss: 0.4929910898208618:  70%|███████▋   | 102/146 [00:14<00:06,  6.66it/s]evaluate for the 3-th batch, evaluate loss: 0.629162609577179:   5%|█                    | 2/40 [00:00<00:05,  6.62it/s]evaluate for the 3-th batch, evaluate loss: 0.629162609577179:   8%|█▌                   | 3/40 [00:00<00:05,  7.22it/s]Epoch: 3, train for the 103-th batch, train loss: 0.4929910898208618:  71%|███████▊   | 103/146 [00:14<00:06,  6.89it/s]evaluate for the 12-th batch, evaluate loss: 0.6530053615570068:  40%|███████▏          | 10/25 [00:01<00:01, 11.77it/s]evaluate for the 12-th batch, evaluate loss: 0.6530053615570068:  48%|████████▋         | 12/25 [00:01<00:01, 10.47it/s]Epoch: 2, train for the 98-th batch, train loss: 0.47432151436805725:  40%|████▊       | 97/241 [00:14<00:24,  5.88it/s]Epoch: 4, train for the 13-th batch, train loss: 0.47263234853744507:  10%|█▏          | 12/119 [00:01<00:14,  7.42it/s]Epoch: 4, train for the 13-th batch, train loss: 0.47263234853744507:  11%|█▎          | 13/119 [00:01<00:15,  6.95it/s]Epoch: 2, train for the 98-th batch, train loss: 0.47432151436805725:  41%|████▉       | 98/241 [00:14<00:22,  6.28it/s]evaluate for the 13-th batch, evaluate loss: 0.6679919958114624:  48%|████████▋         | 12/25 [00:01<00:01, 10.47it/s]Epoch: 1, train for the 264-th batch, train loss: 0.32858288288116455:  69%|██████▊   | 263/383 [01:15<00:37,  3.19it/s]evaluate for the 4-th batch, evaluate loss: 0.6422981023788452:   8%|█▌                  | 3/40 [00:00<00:05,  7.22it/s]evaluate for the 4-th batch, evaluate loss: 0.6422981023788452:  10%|██                  | 4/40 [00:00<00:05,  7.20it/s]Epoch: 3, train for the 104-th batch, train loss: 0.5407503247261047:  71%|███████▊   | 103/146 [00:14<00:06,  6.89it/s]Epoch: 3, train for the 104-th batch, train loss: 0.5407503247261047:  71%|███████▊   | 104/146 [00:14<00:06,  6.86it/s]Epoch: 1, train for the 264-th batch, train loss: 0.32858288288116455:  69%|██████▉   | 264/383 [01:15<00:40,  2.96it/s]Epoch: 4, train for the 14-th batch, train loss: 0.48227161169052124:  11%|█▎          | 13/119 [00:01<00:15,  6.95it/s]evaluate for the 14-th batch, evaluate loss: 0.7068435549736023:  48%|████████▋         | 12/25 [00:01<00:01, 10.47it/s]evaluate for the 14-th batch, evaluate loss: 0.7068435549736023:  56%|██████████        | 14/25 [00:01<00:01, 10.69it/s]Epoch: 4, train for the 14-th batch, train loss: 0.48227161169052124:  12%|█▍          | 14/119 [00:01<00:14,  7.17it/s]Epoch: 2, train for the 99-th batch, train loss: 0.29842981696128845:  41%|████▉       | 98/241 [00:14<00:22,  6.28it/s]Epoch: 2, train for the 99-th batch, train loss: 0.29842981696128845:  41%|████▉       | 99/241 [00:14<00:22,  6.33it/s]evaluate for the 5-th batch, evaluate loss: 0.643778383731842:  10%|██                   | 4/40 [00:00<00:05,  7.20it/s]evaluate for the 5-th batch, evaluate loss: 0.643778383731842:  12%|██▋                  | 5/40 [00:00<00:04,  7.70it/s]Epoch: 3, train for the 105-th batch, train loss: 0.547935426235199:  71%|████████▌   | 104/146 [00:15<00:06,  6.86it/s]evaluate for the 15-th batch, evaluate loss: 0.7424377799034119:  56%|██████████        | 14/25 [00:01<00:01, 10.69it/s]Epoch: 3, train for the 105-th batch, train loss: 0.547935426235199:  72%|████████▋   | 105/146 [00:15<00:05,  6.93it/s]Epoch: 4, train for the 15-th batch, train loss: 0.5043811202049255:  12%|█▌           | 14/119 [00:02<00:14,  7.17it/s]Epoch: 4, train for the 15-th batch, train loss: 0.5043811202049255:  13%|█▋           | 15/119 [00:02<00:13,  7.47it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5127045512199402:  41%|████▉       | 99/241 [00:14<00:22,  6.33it/s]evaluate for the 6-th batch, evaluate loss: 0.6089058518409729:  12%|██▌                 | 5/40 [00:00<00:04,  7.70it/s]evaluate for the 6-th batch, evaluate loss: 0.6089058518409729:  15%|███                 | 6/40 [00:00<00:04,  7.71it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5127045512199402:  41%|████▌      | 100/241 [00:14<00:22,  6.39it/s]evaluate for the 16-th batch, evaluate loss: 0.6656367182731628:  56%|██████████        | 14/25 [00:01<00:01, 10.69it/s]evaluate for the 16-th batch, evaluate loss: 0.6656367182731628:  64%|███████████▌      | 16/25 [00:01<00:00, 10.55it/s]evaluate for the 7-th batch, evaluate loss: 0.6580036878585815:  15%|███                 | 6/40 [00:00<00:04,  7.71it/s]evaluate for the 7-th batch, evaluate loss: 0.6580036878585815:  18%|███▌                | 7/40 [00:00<00:04,  8.09it/s]Epoch: 4, train for the 16-th batch, train loss: 0.4869731664657593:  13%|█▋           | 15/119 [00:02<00:13,  7.47it/s]Epoch: 3, train for the 106-th batch, train loss: 0.5193089842796326:  72%|███████▉   | 105/146 [00:15<00:05,  6.93it/s]Epoch: 4, train for the 16-th batch, train loss: 0.4869731664657593:  13%|█▋           | 16/119 [00:02<00:15,  6.54it/s]Epoch: 3, train for the 106-th batch, train loss: 0.5193089842796326:  73%|███████▉   | 106/146 [00:15<00:06,  5.89it/s]Epoch: 2, train for the 101-th batch, train loss: 0.5095757842063904:  41%|████▌      | 100/241 [00:14<00:22,  6.39it/s]evaluate for the 17-th batch, evaluate loss: 0.6570833325386047:  64%|███████████▌      | 16/25 [00:01<00:00, 10.55it/s]Epoch: 2, train for the 101-th batch, train loss: 0.5095757842063904:  42%|████▌      | 101/241 [00:14<00:21,  6.36it/s]Epoch: 1, train for the 265-th batch, train loss: 0.35486018657684326:  69%|██████▉   | 264/383 [01:15<00:40,  2.96it/s]evaluate for the 18-th batch, evaluate loss: 0.6338362693786621:  64%|███████████▌      | 16/25 [00:01<00:00, 10.55it/s]evaluate for the 18-th batch, evaluate loss: 0.6338362693786621:  72%|████████████▉     | 18/25 [00:01<00:00,  9.70it/s]evaluate for the 8-th batch, evaluate loss: 0.6778658628463745:  18%|███▌                | 7/40 [00:01<00:04,  8.09it/s]evaluate for the 8-th batch, evaluate loss: 0.6778658628463745:  20%|████                | 8/40 [00:01<00:04,  7.55it/s]Epoch: 4, train for the 17-th batch, train loss: 0.48388469219207764:  13%|█▌          | 16/119 [00:02<00:15,  6.54it/s]Epoch: 4, train for the 17-th batch, train loss: 0.48388469219207764:  14%|█▋          | 17/119 [00:02<00:15,  6.62it/s]Epoch: 2, train for the 102-th batch, train loss: 0.5363232493400574:  42%|████▌      | 101/241 [00:15<00:21,  6.36it/s]Epoch: 1, train for the 265-th batch, train loss: 0.35486018657684326:  69%|██████▉   | 265/383 [01:15<00:45,  2.59it/s]Epoch: 2, train for the 102-th batch, train loss: 0.5363232493400574:  42%|████▋      | 102/241 [00:15<00:21,  6.42it/s]Epoch: 3, train for the 107-th batch, train loss: 0.5323843359947205:  73%|███████▉   | 106/146 [00:15<00:06,  5.89it/s]Epoch: 3, train for the 107-th batch, train loss: 0.5323843359947205:  73%|████████   | 107/146 [00:15<00:06,  5.59it/s]evaluate for the 19-th batch, evaluate loss: 0.6188982129096985:  72%|████████████▉     | 18/25 [00:01<00:00,  9.70it/s]evaluate for the 9-th batch, evaluate loss: 0.6888523697853088:  20%|████                | 8/40 [00:01<00:04,  7.55it/s]evaluate for the 9-th batch, evaluate loss: 0.6888523697853088:  22%|████▌               | 9/40 [00:01<00:03,  7.88it/s]Epoch: 4, train for the 18-th batch, train loss: 0.45835447311401367:  14%|█▋          | 17/119 [00:02<00:15,  6.62it/s]Epoch: 4, train for the 18-th batch, train loss: 0.45835447311401367:  15%|█▊          | 18/119 [00:02<00:15,  6.38it/s]evaluate for the 20-th batch, evaluate loss: 0.6741529107093811:  72%|████████████▉     | 18/25 [00:01<00:00,  9.70it/s]evaluate for the 20-th batch, evaluate loss: 0.6741529107093811:  80%|██████████████▍   | 20/25 [00:01<00:00,  9.43it/s]Epoch: 2, train for the 103-th batch, train loss: 0.5007354617118835:  42%|████▋      | 102/241 [00:15<00:21,  6.42it/s]evaluate for the 10-th batch, evaluate loss: 0.6498392820358276:  22%|████▎              | 9/40 [00:01<00:03,  7.88it/s]Epoch: 2, train for the 103-th batch, train loss: 0.5007354617118835:  43%|████▋      | 103/241 [00:15<00:22,  6.03it/s]evaluate for the 10-th batch, evaluate loss: 0.6498392820358276:  25%|████▌             | 10/40 [00:01<00:03,  7.85it/s]Epoch: 4, train for the 19-th batch, train loss: 0.4554925858974457:  15%|█▉           | 18/119 [00:02<00:15,  6.38it/s]evaluate for the 21-th batch, evaluate loss: 0.6863308548927307:  80%|██████████████▍   | 20/25 [00:01<00:00,  9.43it/s]evaluate for the 11-th batch, evaluate loss: 0.6647495627403259:  25%|████▌             | 10/40 [00:01<00:03,  7.85it/s]evaluate for the 11-th batch, evaluate loss: 0.6647495627403259:  28%|████▉             | 11/40 [00:01<00:03,  8.33it/s]Epoch: 2, train for the 104-th batch, train loss: 0.5227836966514587:  43%|████▋      | 103/241 [00:15<00:22,  6.03it/s]evaluate for the 22-th batch, evaluate loss: 0.6394984126091003:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.43it/s]evaluate for the 22-th batch, evaluate loss: 0.6394984126091003:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.88it/s]Epoch: 2, train for the 104-th batch, train loss: 0.5227836966514587:  43%|████▋      | 104/241 [00:15<00:22,  6.14it/s]Epoch: 4, train for the 20-th batch, train loss: 0.4168764352798462:  15%|█▉           | 18/119 [00:02<00:15,  6.38it/s]Epoch: 4, train for the 20-th batch, train loss: 0.4168764352798462:  17%|██▏          | 20/119 [00:02<00:13,  7.43it/s]Epoch: 1, train for the 266-th batch, train loss: 0.3981578052043915:  69%|███████▌   | 265/383 [01:16<00:45,  2.59it/s]Epoch: 3, train for the 108-th batch, train loss: 0.5369417667388916:  73%|████████   | 107/146 [00:15<00:06,  5.59it/s]Epoch: 3, train for the 108-th batch, train loss: 0.5369417667388916:  74%|████████▏  | 108/146 [00:15<00:08,  4.25it/s]evaluate for the 12-th batch, evaluate loss: 0.7030419111251831:  28%|████▉             | 11/40 [00:01<00:03,  8.33it/s]evaluate for the 12-th batch, evaluate loss: 0.7030419111251831:  30%|█████▍            | 12/40 [00:01<00:03,  8.68it/s]evaluate for the 23-th batch, evaluate loss: 0.647540271282196:  88%|████████████████▋  | 22/25 [00:02<00:00,  9.88it/s]Epoch: 1, train for the 266-th batch, train loss: 0.3981578052043915:  69%|███████▋   | 266/383 [01:16<00:47,  2.44it/s]Epoch: 2, train for the 105-th batch, train loss: 0.44358932971954346:  43%|████▎     | 104/241 [00:15<00:22,  6.14it/s]Epoch: 4, train for the 21-th batch, train loss: 0.4953645169734955:  17%|██▏          | 20/119 [00:02<00:13,  7.43it/s]Epoch: 4, train for the 21-th batch, train loss: 0.4953645169734955:  18%|██▎          | 21/119 [00:02<00:13,  7.43it/s]evaluate for the 13-th batch, evaluate loss: 0.6504276990890503:  30%|█████▍            | 12/40 [00:01<00:03,  8.68it/s]evaluate for the 13-th batch, evaluate loss: 0.6504276990890503:  32%|█████▊            | 13/40 [00:01<00:03,  8.99it/s]Epoch: 2, train for the 105-th batch, train loss: 0.44358932971954346:  44%|████▎     | 105/241 [00:15<00:22,  6.16it/s]evaluate for the 24-th batch, evaluate loss: 0.6682232022285461:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.88it/s]evaluate for the 24-th batch, evaluate loss: 0.6682232022285461:  96%|█████████████████▎| 24/25 [00:02<00:00, 10.14it/s]Epoch: 3, train for the 109-th batch, train loss: 0.5190967917442322:  74%|████████▏  | 108/146 [00:15<00:08,  4.25it/s]Epoch: 3, train for the 109-th batch, train loss: 0.5190967917442322:  75%|████████▏  | 109/146 [00:15<00:08,  4.61it/s]evaluate for the 14-th batch, evaluate loss: 0.7121089100837708:  32%|█████▊            | 13/40 [00:01<00:03,  8.99it/s]evaluate for the 14-th batch, evaluate loss: 0.7121089100837708:  35%|██████▎           | 14/40 [00:01<00:02,  8.76it/s]Epoch: 4, train for the 22-th batch, train loss: 0.5275049209594727:  18%|██▎          | 21/119 [00:03<00:13,  7.43it/s]Epoch: 4, train for the 22-th batch, train loss: 0.5275049209594727:  18%|██▍          | 22/119 [00:03<00:13,  7.29it/s]Epoch: 2, train for the 106-th batch, train loss: 0.4894562363624573:  44%|████▊      | 105/241 [00:15<00:22,  6.16it/s]evaluate for the 25-th batch, evaluate loss: 0.6337143778800964:  96%|█████████████████▎| 24/25 [00:02<00:00, 10.14it/s]evaluate for the 25-th batch, evaluate loss: 0.6337143778800964: 100%|██████████████████| 25/25 [00:02<00:00, 10.37it/s]
Epoch: 2, train for the 106-th batch, train loss: 0.4894562363624573:  44%|████▊      | 106/241 [00:15<00:21,  6.15it/s]Epoch: 3, train for the 110-th batch, train loss: 0.46243947744369507:  75%|███████▍  | 109/146 [00:16<00:08,  4.61it/s]Epoch: 3, train for the 110-th batch, train loss: 0.46243947744369507:  75%|███████▌  | 110/146 [00:16<00:07,  5.12it/s]Epoch: 1, train for the 267-th batch, train loss: 0.33084937930107117:  69%|██████▉   | 266/383 [01:16<00:47,  2.44it/s]Epoch: 4, train for the 23-th batch, train loss: 0.49633651971817017:  18%|██▏         | 22/119 [00:03<00:13,  7.29it/s]Epoch: 4, train for the 23-th batch, train loss: 0.49633651971817017:  19%|██▎         | 23/119 [00:03<00:12,  7.73it/s]evaluate for the 15-th batch, evaluate loss: 0.6760442852973938:  35%|██████▎           | 14/40 [00:01<00:02,  8.76it/s]evaluate for the 15-th batch, evaluate loss: 0.6760442852973938:  38%|██████▊           | 15/40 [00:01<00:02,  8.52it/s]Epoch: 1, train for the 267-th batch, train loss: 0.33084937930107117:  70%|██████▉   | 267/383 [01:16<00:45,  2.57it/s]evaluate for the 16-th batch, evaluate loss: 0.641417920589447:  38%|███████▏           | 15/40 [00:01<00:02,  8.52it/s]Epoch: 3, train for the 111-th batch, train loss: 0.5171053409576416:  75%|████████▎  | 110/146 [00:16<00:07,  5.12it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5577095150947571:  44%|████▊      | 106/241 [00:15<00:21,  6.15it/s]Epoch: 3, train for the 111-th batch, train loss: 0.5171053409576416:  76%|████████▎  | 111/146 [00:16<00:06,  5.50it/s]Epoch: 4, train for the 24-th batch, train loss: 0.46320781111717224:  19%|██▎         | 23/119 [00:03<00:12,  7.73it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5577095150947571:  44%|████▉      | 107/241 [00:15<00:23,  5.67it/s]Epoch: 4, train for the 24-th batch, train loss: 0.46320781111717224:  20%|██▍         | 24/119 [00:03<00:12,  7.49it/s]evaluate for the 17-th batch, evaluate loss: 0.6543675065040588:  38%|██████▊           | 15/40 [00:02<00:02,  8.52it/s]evaluate for the 17-th batch, evaluate loss: 0.6543675065040588:  42%|███████▋          | 17/40 [00:02<00:02,  8.70it/s]Epoch: 2, train for the 108-th batch, train loss: 0.4986405074596405:  44%|████▉      | 107/241 [00:16<00:23,  5.67it/s]Epoch: 3, train for the 112-th batch, train loss: 0.5639323592185974:  76%|████████▎  | 111/146 [00:16<00:06,  5.50it/s]Epoch: 3, train for the 112-th batch, train loss: 0.5639323592185974:  77%|████████▍  | 112/146 [00:16<00:05,  5.74it/s]Epoch: 2, train for the 108-th batch, train loss: 0.4986405074596405:  45%|████▉      | 108/241 [00:16<00:22,  5.92it/s]Epoch: 4, train for the 25-th batch, train loss: 0.4545454680919647:  20%|██▌          | 24/119 [00:03<00:12,  7.49it/s]INFO:root:Epoch: 2, learning rate: 0.0001, train loss: 0.5688
INFO:root:train average_precision, 0.7681
INFO:root:train roc_auc, 0.7448
INFO:root:validate loss: 0.5148
INFO:root:validate average_precision, 0.8380
INFO:root:validate roc_auc, 0.8330
INFO:root:new node validate loss: 0.6958
Epoch: 4, train for the 25-th batch, train loss: 0.4545454680919647:  21%|██▋          | 25/119 [00:03<00:13,  6.98it/s]INFO:root:new node validate first_1_average_precision, 0.5545
INFO:root:new node validate first_1_roc_auc, 0.5621
INFO:root:new node validate first_3_average_precision, 0.5908
INFO:root:new node validate first_3_roc_auc, 0.5948
INFO:root:new node validate first_10_average_precision, 0.6438
INFO:root:new node validate first_10_roc_auc, 0.6495
INFO:root:new node validate average_precision, 0.6582
INFO:root:new node validate roc_auc, 0.6584
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]evaluate for the 18-th batch, evaluate loss: 0.6390926837921143:  42%|███████▋          | 17/40 [00:02<00:02,  8.70it/s]evaluate for the 18-th batch, evaluate loss: 0.6390926837921143:  45%|████████          | 18/40 [00:02<00:02,  8.70it/s]Epoch: 3, train for the 113-th batch, train loss: 0.4779597222805023:  77%|████████▍  | 112/146 [00:16<00:05,  5.74it/s]Epoch: 3, train for the 113-th batch, train loss: 0.4779597222805023:  77%|████████▌  | 113/146 [00:16<00:05,  5.99it/s]Epoch: 2, train for the 109-th batch, train loss: 0.6500328183174133:  45%|████▉      | 108/241 [00:16<00:22,  5.92it/s]evaluate for the 19-th batch, evaluate loss: 0.6970041990280151:  45%|████████          | 18/40 [00:02<00:02,  8.70it/s]Epoch: 4, train for the 26-th batch, train loss: 0.43561574816703796:  21%|██▌         | 25/119 [00:03<00:13,  6.98it/s]Epoch: 4, train for the 26-th batch, train loss: 0.43561574816703796:  22%|██▌         | 26/119 [00:03<00:13,  7.05it/s]Epoch: 2, train for the 109-th batch, train loss: 0.6500328183174133:  45%|████▉      | 109/241 [00:16<00:22,  5.97it/s]Epoch: 3, train for the 1-th batch, train loss: 0.9261704683303833:   0%|                       | 0/151 [00:00<?, ?it/s]Epoch: 3, train for the 1-th batch, train loss: 0.9261704683303833:   1%|               | 1/151 [00:00<00:18,  8.14it/s]Epoch: 3, train for the 114-th batch, train loss: 0.5222189426422119:  77%|████████▌  | 113/146 [00:16<00:05,  5.99it/s]Epoch: 1, train for the 268-th batch, train loss: 0.35304197669029236:  70%|██████▉   | 267/383 [01:17<00:45,  2.57it/s]Epoch: 3, train for the 114-th batch, train loss: 0.5222189426422119:  78%|████████▌  | 114/146 [00:16<00:05,  6.34it/s]evaluate for the 20-th batch, evaluate loss: 0.669583797454834:  45%|████████▌          | 18/40 [00:02<00:02,  8.70it/s]evaluate for the 20-th batch, evaluate loss: 0.669583797454834:  50%|█████████▌         | 20/40 [00:02<00:02,  8.52it/s]Epoch: 4, train for the 27-th batch, train loss: 0.5271111130714417:  22%|██▊          | 26/119 [00:03<00:13,  7.05it/s]Epoch: 4, train for the 27-th batch, train loss: 0.5271111130714417:  23%|██▉          | 27/119 [00:03<00:13,  7.04it/s]Epoch: 3, train for the 2-th batch, train loss: 0.8888307213783264:   1%|               | 1/151 [00:00<00:18,  8.14it/s]Epoch: 3, train for the 2-th batch, train loss: 0.8888307213783264:   1%|▏              | 2/151 [00:00<00:19,  7.61it/s]Epoch: 2, train for the 110-th batch, train loss: 0.551443338394165:  45%|█████▍      | 109/241 [00:16<00:22,  5.97it/s]Epoch: 1, train for the 268-th batch, train loss: 0.35304197669029236:  70%|██████▉   | 268/383 [01:17<00:51,  2.25it/s]Epoch: 2, train for the 110-th batch, train loss: 0.551443338394165:  46%|█████▍      | 110/241 [00:16<00:22,  5.79it/s]evaluate for the 21-th batch, evaluate loss: 0.6879003047943115:  50%|█████████         | 20/40 [00:02<00:02,  8.52it/s]evaluate for the 21-th batch, evaluate loss: 0.6879003047943115:  52%|█████████▍        | 21/40 [00:02<00:02,  8.60it/s]Epoch: 3, train for the 115-th batch, train loss: 0.5218079686164856:  78%|████████▌  | 114/146 [00:16<00:05,  6.34it/s]Epoch: 3, train for the 115-th batch, train loss: 0.5218079686164856:  79%|████████▋  | 115/146 [00:16<00:04,  6.49it/s]Epoch: 3, train for the 3-th batch, train loss: 0.8444020748138428:   1%|▏              | 2/151 [00:00<00:19,  7.61it/s]Epoch: 3, train for the 3-th batch, train loss: 0.8444020748138428:   2%|▎              | 3/151 [00:00<00:18,  7.87it/s]Epoch: 4, train for the 28-th batch, train loss: 0.5003051161766052:  23%|██▉          | 27/119 [00:03<00:13,  7.04it/s]Epoch: 4, train for the 28-th batch, train loss: 0.5003051161766052:  24%|███          | 28/119 [00:03<00:12,  7.07it/s]Epoch: 2, train for the 111-th batch, train loss: 0.23122569918632507:  46%|████▌     | 110/241 [00:16<00:22,  5.79it/s]Epoch: 2, train for the 111-th batch, train loss: 0.23122569918632507:  46%|████▌     | 111/241 [00:16<00:21,  6.06it/s]evaluate for the 22-th batch, evaluate loss: 0.7089506983757019:  52%|█████████▍        | 21/40 [00:02<00:02,  8.60it/s]evaluate for the 22-th batch, evaluate loss: 0.7089506983757019:  55%|█████████▉        | 22/40 [00:02<00:02,  8.49it/s]Epoch: 3, train for the 116-th batch, train loss: 0.5216354131698608:  79%|████████▋  | 115/146 [00:17<00:04,  6.49it/s]Epoch: 3, train for the 116-th batch, train loss: 0.5216354131698608:  79%|████████▋  | 116/146 [00:17<00:04,  6.32it/s]Epoch: 4, train for the 29-th batch, train loss: 0.45320990681648254:  24%|██▊         | 28/119 [00:04<00:12,  7.07it/s]Epoch: 4, train for the 29-th batch, train loss: 0.45320990681648254:  24%|██▉         | 29/119 [00:04<00:13,  6.78it/s]Epoch: 2, train for the 112-th batch, train loss: 0.3209111988544464:  46%|█████      | 111/241 [00:16<00:21,  6.06it/s]evaluate for the 23-th batch, evaluate loss: 0.6682906746864319:  55%|█████████▉        | 22/40 [00:02<00:02,  8.49it/s]evaluate for the 23-th batch, evaluate loss: 0.6682906746864319:  57%|██████████▎       | 23/40 [00:02<00:01,  8.80it/s]Epoch: 3, train for the 4-th batch, train loss: 0.7658795118331909:   2%|▎              | 3/151 [00:00<00:18,  7.87it/s]Epoch: 2, train for the 112-th batch, train loss: 0.3209111988544464:  46%|█████      | 112/241 [00:16<00:20,  6.25it/s]Epoch: 3, train for the 4-th batch, train loss: 0.7658795118331909:   3%|▍              | 4/151 [00:00<00:23,  6.24it/s]Epoch: 1, train for the 269-th batch, train loss: 0.4242227375507355:  70%|███████▋   | 268/383 [01:17<00:51,  2.25it/s]evaluate for the 24-th batch, evaluate loss: 0.6954322457313538:  57%|██████████▎       | 23/40 [00:02<00:01,  8.80it/s]evaluate for the 24-th batch, evaluate loss: 0.6954322457313538:  60%|██████████▊       | 24/40 [00:02<00:01,  8.58it/s]Epoch: 3, train for the 117-th batch, train loss: 0.5373309254646301:  79%|████████▋  | 116/146 [00:17<00:04,  6.32it/s]Epoch: 4, train for the 30-th batch, train loss: 0.41416046023368835:  24%|██▉         | 29/119 [00:04<00:13,  6.78it/s]Epoch: 1, train for the 269-th batch, train loss: 0.4242227375507355:  70%|███████▋   | 269/383 [01:17<00:50,  2.25it/s]Epoch: 4, train for the 30-th batch, train loss: 0.41416046023368835:  25%|███         | 30/119 [00:04<00:13,  6.40it/s]Epoch: 3, train for the 117-th batch, train loss: 0.5373309254646301:  80%|████████▊  | 117/146 [00:17<00:04,  5.87it/s]Epoch: 3, train for the 5-th batch, train loss: 0.7399771809577942:   3%|▍              | 4/151 [00:00<00:23,  6.24it/s]Epoch: 2, train for the 113-th batch, train loss: 0.38682350516319275:  46%|████▋     | 112/241 [00:16<00:20,  6.25it/s]Epoch: 3, train for the 5-th batch, train loss: 0.7399771809577942:   3%|▍              | 5/151 [00:00<00:24,  5.87it/s]Epoch: 2, train for the 113-th batch, train loss: 0.38682350516319275:  47%|████▋     | 113/241 [00:16<00:22,  5.75it/s]evaluate for the 25-th batch, evaluate loss: 0.6894747614860535:  60%|██████████▊       | 24/40 [00:03<00:01,  8.58it/s]evaluate for the 25-th batch, evaluate loss: 0.6894747614860535:  62%|███████████▎      | 25/40 [00:03<00:01,  8.84it/s]Epoch: 4, train for the 31-th batch, train loss: 0.49748265743255615:  25%|███         | 30/119 [00:04<00:13,  6.40it/s]Epoch: 4, train for the 31-th batch, train loss: 0.49748265743255615:  26%|███▏        | 31/119 [00:04<00:13,  6.35it/s]Epoch: 3, train for the 6-th batch, train loss: 0.7124764919281006:   3%|▍              | 5/151 [00:00<00:24,  5.87it/s]evaluate for the 26-th batch, evaluate loss: 0.7010865807533264:  62%|███████████▎      | 25/40 [00:03<00:01,  8.84it/s]evaluate for the 26-th batch, evaluate loss: 0.7010865807533264:  65%|███████████▋      | 26/40 [00:03<00:01,  8.81it/s]Epoch: 3, train for the 6-th batch, train loss: 0.7124764919281006:   4%|▌              | 6/151 [00:00<00:23,  6.19it/s]Epoch: 3, train for the 118-th batch, train loss: 0.50376957654953:  80%|██████████▍  | 117/146 [00:17<00:04,  5.87it/s]Epoch: 3, train for the 118-th batch, train loss: 0.50376957654953:  81%|██████████▌  | 118/146 [00:17<00:05,  5.51it/s]Epoch: 2, train for the 114-th batch, train loss: 0.4005940854549408:  47%|█████▏     | 113/241 [00:17<00:22,  5.75it/s]Epoch: 2, train for the 114-th batch, train loss: 0.4005940854549408:  47%|█████▏     | 114/241 [00:17<00:22,  5.75it/s]Epoch: 1, train for the 270-th batch, train loss: 0.4111390709877014:  70%|███████▋   | 269/383 [01:17<00:50,  2.25it/s]Epoch: 4, train for the 32-th batch, train loss: 0.451835960149765:  26%|███▋          | 31/119 [00:04<00:13,  6.35it/s]Epoch: 4, train for the 32-th batch, train loss: 0.451835960149765:  27%|███▊          | 32/119 [00:04<00:12,  6.78it/s]evaluate for the 27-th batch, evaluate loss: 0.6575634479522705:  65%|███████████▋      | 26/40 [00:03<00:01,  8.81it/s]evaluate for the 27-th batch, evaluate loss: 0.6575634479522705:  68%|████████████▏     | 27/40 [00:03<00:01,  8.90it/s]Epoch: 1, train for the 270-th batch, train loss: 0.4111390709877014:  70%|███████▊   | 270/383 [01:17<00:45,  2.48it/s]Epoch: 3, train for the 7-th batch, train loss: 0.6679197549819946:   4%|▌              | 6/151 [00:01<00:23,  6.19it/s]Epoch: 3, train for the 7-th batch, train loss: 0.6679197549819946:   5%|▋              | 7/151 [00:01<00:21,  6.73it/s]Epoch: 3, train for the 119-th batch, train loss: 0.5105953812599182:  81%|████████▉  | 118/146 [00:17<00:05,  5.51it/s]Epoch: 3, train for the 119-th batch, train loss: 0.5105953812599182:  82%|████████▉  | 119/146 [00:17<00:04,  5.84it/s]Epoch: 2, train for the 115-th batch, train loss: 0.42298343777656555:  47%|████▋     | 114/241 [00:17<00:22,  5.75it/s]Epoch: 2, train for the 115-th batch, train loss: 0.42298343777656555:  48%|████▊     | 115/241 [00:17<00:21,  5.99it/s]Epoch: 4, train for the 33-th batch, train loss: 0.4814692437648773:  27%|███▍         | 32/119 [00:04<00:12,  6.78it/s]Epoch: 4, train for the 33-th batch, train loss: 0.4814692437648773:  28%|███▌         | 33/119 [00:04<00:12,  7.03it/s]evaluate for the 28-th batch, evaluate loss: 0.7186083793640137:  68%|████████████▏     | 27/40 [00:03<00:01,  8.90it/s]evaluate for the 28-th batch, evaluate loss: 0.7186083793640137:  70%|████████████▌     | 28/40 [00:03<00:01,  8.66it/s]Epoch: 3, train for the 8-th batch, train loss: 0.6886253952980042:   5%|▋              | 7/151 [00:01<00:21,  6.73it/s]Epoch: 3, train for the 8-th batch, train loss: 0.6886253952980042:   5%|▊              | 8/151 [00:01<00:20,  7.08it/s]Epoch: 2, train for the 116-th batch, train loss: 0.519000768661499:  48%|█████▋      | 115/241 [00:17<00:21,  5.99it/s]Epoch: 3, train for the 120-th batch, train loss: 0.4875258803367615:  82%|████████▉  | 119/146 [00:17<00:04,  5.84it/s]Epoch: 2, train for the 116-th batch, train loss: 0.519000768661499:  48%|█████▊      | 116/241 [00:17<00:19,  6.29it/s]Epoch: 4, train for the 34-th batch, train loss: 0.44182249903678894:  28%|███▎        | 33/119 [00:04<00:12,  7.03it/s]Epoch: 4, train for the 34-th batch, train loss: 0.44182249903678894:  29%|███▍        | 34/119 [00:04<00:11,  7.22it/s]Epoch: 3, train for the 120-th batch, train loss: 0.4875258803367615:  82%|█████████  | 120/146 [00:17<00:04,  5.69it/s]Epoch: 1, train for the 271-th batch, train loss: 0.36010831594467163:  70%|███████   | 270/383 [01:18<00:45,  2.48it/s]evaluate for the 29-th batch, evaluate loss: 0.7311914563179016:  70%|████████████▌     | 28/40 [00:03<00:01,  8.66it/s]evaluate for the 29-th batch, evaluate loss: 0.7311914563179016:  72%|█████████████     | 29/40 [00:03<00:01,  7.97it/s]Epoch: 3, train for the 9-th batch, train loss: 0.6818659901618958:   5%|▊              | 8/151 [00:01<00:20,  7.08it/s]Epoch: 3, train for the 9-th batch, train loss: 0.6818659901618958:   6%|▉              | 9/151 [00:01<00:20,  6.83it/s]Epoch: 1, train for the 271-th batch, train loss: 0.36010831594467163:  71%|███████   | 271/383 [01:18<00:43,  2.60it/s]Epoch: 2, train for the 117-th batch, train loss: 0.6117357015609741:  48%|█████▎     | 116/241 [00:17<00:19,  6.29it/s]evaluate for the 30-th batch, evaluate loss: 0.7357420325279236:  72%|█████████████     | 29/40 [00:03<00:01,  7.97it/s]Epoch: 4, train for the 35-th batch, train loss: 0.45105278491973877:  29%|███▍        | 34/119 [00:04<00:11,  7.22it/s]Epoch: 4, train for the 35-th batch, train loss: 0.45105278491973877:  29%|███▌        | 35/119 [00:04<00:11,  7.35it/s]Epoch: 2, train for the 117-th batch, train loss: 0.6117357015609741:  49%|█████▎     | 117/241 [00:17<00:19,  6.41it/s]Epoch: 3, train for the 121-th batch, train loss: 0.4962882995605469:  82%|█████████  | 120/146 [00:17<00:04,  5.69it/s]Epoch: 3, train for the 121-th batch, train loss: 0.4962882995605469:  83%|█████████  | 121/146 [00:17<00:04,  5.61it/s]Epoch: 3, train for the 10-th batch, train loss: 0.6895861625671387:   6%|▊             | 9/151 [00:01<00:20,  6.83it/s]Epoch: 3, train for the 10-th batch, train loss: 0.6895861625671387:   7%|▊            | 10/151 [00:01<00:21,  6.67it/s]evaluate for the 31-th batch, evaluate loss: 0.6707990765571594:  72%|█████████████     | 29/40 [00:03<00:01,  7.97it/s]evaluate for the 31-th batch, evaluate loss: 0.6707990765571594:  78%|█████████████▉    | 31/40 [00:03<00:01,  8.51it/s]Epoch: 4, train for the 36-th batch, train loss: 0.42462339997291565:  29%|███▌        | 35/119 [00:05<00:11,  7.35it/s]Epoch: 2, train for the 118-th batch, train loss: 0.4906315505504608:  49%|█████▎     | 117/241 [00:17<00:19,  6.41it/s]Epoch: 4, train for the 36-th batch, train loss: 0.42462339997291565:  30%|███▋        | 36/119 [00:05<00:11,  7.42it/s]Epoch: 2, train for the 118-th batch, train loss: 0.4906315505504608:  49%|█████▍     | 118/241 [00:17<00:18,  6.63it/s]Epoch: 1, train for the 272-th batch, train loss: 0.32466864585876465:  71%|███████   | 271/383 [01:18<00:43,  2.60it/s]Epoch: 3, train for the 11-th batch, train loss: 0.7053595781326294:   7%|▊            | 10/151 [00:01<00:21,  6.67it/s]Epoch: 3, train for the 122-th batch, train loss: 0.5467263460159302:  83%|█████████  | 121/146 [00:18<00:04,  5.61it/s]Epoch: 3, train for the 11-th batch, train loss: 0.7053595781326294:   7%|▉            | 11/151 [00:01<00:20,  6.95it/s]Epoch: 3, train for the 122-th batch, train loss: 0.5467263460159302:  84%|█████████▏ | 122/146 [00:18<00:04,  5.59it/s]Epoch: 4, train for the 37-th batch, train loss: 0.47747474908828735:  30%|███▋        | 36/119 [00:05<00:11,  7.42it/s]evaluate for the 32-th batch, evaluate loss: 0.7015894055366516:  78%|█████████████▉    | 31/40 [00:03<00:01,  8.51it/s]evaluate for the 32-th batch, evaluate loss: 0.7015894055366516:  80%|██████████████▍   | 32/40 [00:03<00:00,  8.33it/s]Epoch: 4, train for the 37-th batch, train loss: 0.47747474908828735:  31%|███▋        | 37/119 [00:05<00:10,  7.63it/s]Epoch: 1, train for the 272-th batch, train loss: 0.32466864585876465:  71%|███████   | 272/383 [01:18<00:39,  2.81it/s]Epoch: 2, train for the 119-th batch, train loss: 0.35419997572898865:  49%|████▉     | 118/241 [00:17<00:18,  6.63it/s]Epoch: 2, train for the 119-th batch, train loss: 0.35419997572898865:  49%|████▉     | 119/241 [00:17<00:17,  6.79it/s]Epoch: 3, train for the 12-th batch, train loss: 0.7145780920982361:   7%|▉            | 11/151 [00:01<00:20,  6.95it/s]Epoch: 3, train for the 12-th batch, train loss: 0.7145780920982361:   8%|█            | 12/151 [00:01<00:19,  7.12it/s]evaluate for the 33-th batch, evaluate loss: 0.7097773551940918:  80%|██████████████▍   | 32/40 [00:03<00:00,  8.33it/s]evaluate for the 33-th batch, evaluate loss: 0.7097773551940918:  82%|██████████████▊   | 33/40 [00:03<00:00,  8.50it/s]Epoch: 3, train for the 123-th batch, train loss: 0.535309374332428:  84%|██████████  | 122/146 [00:18<00:04,  5.59it/s]Epoch: 4, train for the 38-th batch, train loss: 0.43162989616394043:  31%|███▋        | 37/119 [00:05<00:10,  7.63it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5913216471672058:  49%|█████▍     | 119/241 [00:17<00:17,  6.79it/s]Epoch: 3, train for the 123-th batch, train loss: 0.535309374332428:  84%|██████████  | 123/146 [00:18<00:04,  5.50it/s]Epoch: 4, train for the 38-th batch, train loss: 0.43162989616394043:  32%|███▊        | 38/119 [00:05<00:11,  6.91it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5913216471672058:  50%|█████▍     | 120/241 [00:17<00:18,  6.62it/s]Epoch: 3, train for the 13-th batch, train loss: 0.6969523429870605:   8%|█            | 12/151 [00:01<00:19,  7.12it/s]Epoch: 3, train for the 13-th batch, train loss: 0.6969523429870605:   9%|█            | 13/151 [00:01<00:18,  7.42it/s]evaluate for the 34-th batch, evaluate loss: 0.7032666802406311:  82%|██████████████▊   | 33/40 [00:04<00:00,  8.50it/s]evaluate for the 34-th batch, evaluate loss: 0.7032666802406311:  85%|███████████████▎  | 34/40 [00:04<00:00,  8.44it/s]Epoch: 3, train for the 124-th batch, train loss: 0.5289469361305237:  84%|█████████▎ | 123/146 [00:18<00:04,  5.50it/s]Epoch: 3, train for the 124-th batch, train loss: 0.5289469361305237:  85%|█████████▎ | 124/146 [00:18<00:03,  5.84it/s]evaluate for the 35-th batch, evaluate loss: 0.7289982438087463:  85%|███████████████▎  | 34/40 [00:04<00:00,  8.44it/s]evaluate for the 35-th batch, evaluate loss: 0.7289982438087463:  88%|███████████████▊  | 35/40 [00:04<00:00,  8.79it/s]Epoch: 4, train for the 39-th batch, train loss: 0.47538992762565613:  32%|███▊        | 38/119 [00:05<00:11,  6.91it/s]Epoch: 4, train for the 39-th batch, train loss: 0.47538992762565613:  33%|███▉        | 39/119 [00:05<00:12,  6.50it/s]Epoch: 2, train for the 121-th batch, train loss: 0.5466699004173279:  50%|█████▍     | 120/241 [00:18<00:18,  6.62it/s]Epoch: 3, train for the 14-th batch, train loss: 0.7085193991661072:   9%|█            | 13/151 [00:02<00:18,  7.42it/s]Epoch: 3, train for the 14-th batch, train loss: 0.7085193991661072:   9%|█▏           | 14/151 [00:02<00:20,  6.81it/s]Epoch: 2, train for the 121-th batch, train loss: 0.5466699004173279:  50%|█████▌     | 121/241 [00:18<00:19,  6.04it/s]Epoch: 1, train for the 273-th batch, train loss: 0.42240074276924133:  71%|███████   | 272/383 [01:19<00:39,  2.81it/s]Epoch: 3, train for the 125-th batch, train loss: 0.49835968017578125:  85%|████████▍ | 124/146 [00:18<00:03,  5.84it/s]evaluate for the 36-th batch, evaluate loss: 0.754380464553833:  88%|████████████████▋  | 35/40 [00:04<00:00,  8.79it/s]evaluate for the 36-th batch, evaluate loss: 0.754380464553833:  90%|█████████████████  | 36/40 [00:04<00:00,  8.86it/s]Epoch: 3, train for the 125-th batch, train loss: 0.49835968017578125:  86%|████████▌ | 125/146 [00:18<00:03,  6.35it/s]Epoch: 4, train for the 40-th batch, train loss: 0.4410010874271393:  33%|████▎        | 39/119 [00:05<00:12,  6.50it/s]Epoch: 1, train for the 273-th batch, train loss: 0.42240074276924133:  71%|███████▏  | 273/383 [01:19<00:43,  2.54it/s]Epoch: 4, train for the 40-th batch, train loss: 0.4410010874271393:  34%|████▎        | 40/119 [00:05<00:11,  6.66it/s]Epoch: 2, train for the 122-th batch, train loss: 0.4457295536994934:  50%|█████▌     | 121/241 [00:18<00:19,  6.04it/s]Epoch: 2, train for the 122-th batch, train loss: 0.4457295536994934:  51%|█████▌     | 122/241 [00:18<00:19,  6.19it/s]evaluate for the 37-th batch, evaluate loss: 0.7367238998413086:  90%|████████████████▏ | 36/40 [00:04<00:00,  8.86it/s]evaluate for the 37-th batch, evaluate loss: 0.7367238998413086:  92%|████████████████▋ | 37/40 [00:04<00:00,  8.83it/s]Epoch: 3, train for the 15-th batch, train loss: 0.6912907361984253:   9%|█▏           | 14/151 [00:02<00:20,  6.81it/s]Epoch: 3, train for the 15-th batch, train loss: 0.6912907361984253:  10%|█▎           | 15/151 [00:02<00:21,  6.35it/s]Epoch: 3, train for the 126-th batch, train loss: 0.5655356645584106:  86%|█████████▍ | 125/146 [00:18<00:03,  6.35it/s]Epoch: 3, train for the 126-th batch, train loss: 0.5655356645584106:  86%|█████████▍ | 126/146 [00:18<00:03,  6.59it/s]Epoch: 4, train for the 41-th batch, train loss: 0.4955834448337555:  34%|████▎        | 40/119 [00:05<00:11,  6.66it/s]Epoch: 4, train for the 41-th batch, train loss: 0.4955834448337555:  34%|████▍        | 41/119 [00:05<00:11,  6.76it/s]evaluate for the 38-th batch, evaluate loss: 0.7166805267333984:  92%|████████████████▋ | 37/40 [00:04<00:00,  8.83it/s]evaluate for the 38-th batch, evaluate loss: 0.7166805267333984:  95%|█████████████████ | 38/40 [00:04<00:00,  8.48it/s]Epoch: 2, train for the 123-th batch, train loss: 0.6822340488433838:  51%|█████▌     | 122/241 [00:18<00:19,  6.19it/s]Epoch: 3, train for the 127-th batch, train loss: 0.519902765750885:  86%|██████████▎ | 126/146 [00:18<00:03,  6.59it/s]Epoch: 2, train for the 123-th batch, train loss: 0.6822340488433838:  51%|█████▌     | 123/241 [00:18<00:19,  5.93it/s]Epoch: 3, train for the 16-th batch, train loss: 0.7051923274993896:  10%|█▎           | 15/151 [00:02<00:21,  6.35it/s]Epoch: 1, train for the 274-th batch, train loss: 0.30328333377838135:  71%|███████▏  | 273/383 [01:19<00:43,  2.54it/s]Epoch: 3, train for the 127-th batch, train loss: 0.519902765750885:  87%|██████████▍ | 127/146 [00:18<00:02,  6.52it/s]Epoch: 3, train for the 16-th batch, train loss: 0.7051923274993896:  11%|█▍           | 16/151 [00:02<00:22,  6.10it/s]Epoch: 4, train for the 42-th batch, train loss: 0.4820862114429474:  34%|████▍        | 41/119 [00:05<00:11,  6.76it/s]Epoch: 4, train for the 42-th batch, train loss: 0.4820862114429474:  35%|████▌        | 42/119 [00:05<00:11,  6.92it/s]Epoch: 1, train for the 274-th batch, train loss: 0.30328333377838135:  72%|███████▏  | 274/383 [01:19<00:40,  2.68it/s]evaluate for the 39-th batch, evaluate loss: 0.7312609553337097:  95%|█████████████████ | 38/40 [00:04<00:00,  8.48it/s]evaluate for the 39-th batch, evaluate loss: 0.7312609553337097:  98%|█████████████████▌| 39/40 [00:04<00:00,  7.99it/s]Epoch: 2, train for the 124-th batch, train loss: 0.3843042552471161:  51%|█████▌     | 123/241 [00:18<00:19,  5.93it/s]Epoch: 2, train for the 124-th batch, train loss: 0.3843042552471161:  51%|█████▋     | 124/241 [00:18<00:18,  6.17it/s]Epoch: 3, train for the 128-th batch, train loss: 0.5172062516212463:  87%|█████████▌ | 127/146 [00:19<00:02,  6.52it/s]Epoch: 3, train for the 17-th batch, train loss: 0.6933881044387817:  11%|█▍           | 16/151 [00:02<00:22,  6.10it/s]Epoch: 3, train for the 128-th batch, train loss: 0.5172062516212463:  88%|█████████▋ | 128/146 [00:19<00:02,  6.28it/s]Epoch: 3, train for the 17-th batch, train loss: 0.6933881044387817:  11%|█▍           | 17/151 [00:02<00:22,  6.01it/s]evaluate for the 40-th batch, evaluate loss: 0.7529128193855286:  98%|█████████████████▌| 39/40 [00:04<00:00,  7.99it/s]evaluate for the 40-th batch, evaluate loss: 0.7529128193855286: 100%|██████████████████| 40/40 [00:04<00:00,  7.86it/s]evaluate for the 40-th batch, evaluate loss: 0.7529128193855286: 100%|██████████████████| 40/40 [00:04<00:00,  8.28it/s]
Epoch: 4, train for the 43-th batch, train loss: 0.49258673191070557:  35%|████▏       | 42/119 [00:06<00:11,  6.92it/s]Epoch: 4, train for the 43-th batch, train loss: 0.49258673191070557:  36%|████▎       | 43/119 [00:06<00:12,  6.25it/s]Epoch: 2, train for the 125-th batch, train loss: 0.49113568663597107:  51%|█████▏    | 124/241 [00:18<00:18,  6.17it/s]Epoch: 2, train for the 125-th batch, train loss: 0.49113568663597107:  52%|█████▏    | 125/241 [00:18<00:18,  6.41it/s]Epoch: 3, train for the 129-th batch, train loss: 0.5288314819335938:  88%|█████████▋ | 128/146 [00:19<00:02,  6.28it/s]Epoch: 3, train for the 18-th batch, train loss: 0.6839314103126526:  11%|█▍           | 17/151 [00:02<00:22,  6.01it/s]Epoch: 3, train for the 129-th batch, train loss: 0.5288314819335938:  88%|█████████▋ | 129/146 [00:19<00:02,  6.53it/s]Epoch: 1, train for the 275-th batch, train loss: 0.43764960765838623:  72%|███████▏  | 274/383 [01:19<00:40,  2.68it/s]Epoch: 3, train for the 18-th batch, train loss: 0.6839314103126526:  12%|█▌           | 18/151 [00:02<00:20,  6.35it/s]Epoch: 4, train for the 44-th batch, train loss: 0.43668991327285767:  36%|████▎       | 43/119 [00:06<00:12,  6.25it/s]Epoch: 4, train for the 44-th batch, train loss: 0.43668991327285767:  37%|████▍       | 44/119 [00:06<00:10,  6.84it/s]Epoch: 1, train for the 275-th batch, train loss: 0.43764960765838623:  72%|███████▏  | 275/383 [01:19<00:37,  2.86it/s]Epoch: 2, train for the 126-th batch, train loss: 0.4334717392921448:  52%|█████▋     | 125/241 [00:18<00:18,  6.41it/s]Epoch: 3, train for the 130-th batch, train loss: 0.45874273777008057:  88%|████████▊ | 129/146 [00:19<00:02,  6.53it/s]Epoch: 3, train for the 130-th batch, train loss: 0.45874273777008057:  89%|████████▉ | 130/146 [00:19<00:02,  6.78it/s]Epoch: 3, train for the 19-th batch, train loss: 0.6732864379882812:  12%|█▌           | 18/151 [00:02<00:20,  6.35it/s]Epoch: 2, train for the 126-th batch, train loss: 0.4334717392921448:  52%|█████▊     | 126/241 [00:18<00:18,  6.14it/s]Epoch: 3, train for the 19-th batch, train loss: 0.6732864379882812:  13%|█▋           | 19/151 [00:02<00:20,  6.50it/s]Epoch: 4, train for the 45-th batch, train loss: 0.40645095705986023:  37%|████▍       | 44/119 [00:06<00:10,  6.84it/s]Epoch: 4, train for the 45-th batch, train loss: 0.40645095705986023:  38%|████▌       | 45/119 [00:06<00:10,  6.96it/s]Epoch: 3, train for the 131-th batch, train loss: 0.5448330044746399:  89%|█████████▊ | 130/146 [00:19<00:02,  6.78it/s]Epoch: 3, train for the 131-th batch, train loss: 0.5448330044746399:  90%|█████████▊ | 131/146 [00:19<00:02,  6.87it/s]Epoch: 2, train for the 127-th batch, train loss: 0.5103904604911804:  52%|█████▊     | 126/241 [00:19<00:18,  6.14it/s]Epoch: 1, train for the 276-th batch, train loss: 0.6500811576843262:  72%|███████▉   | 275/383 [01:19<00:37,  2.86it/s]Epoch: 2, train for the 127-th batch, train loss: 0.5103904604911804:  53%|█████▊     | 127/241 [00:19<00:18,  6.30it/s]Epoch: 3, train for the 20-th batch, train loss: 0.6632960438728333:  13%|█▋           | 19/151 [00:03<00:20,  6.50it/s]Epoch: 4, train for the 46-th batch, train loss: 0.4333510398864746:  38%|████▉        | 45/119 [00:06<00:10,  6.96it/s]Epoch: 4, train for the 46-th batch, train loss: 0.4333510398864746:  39%|█████        | 46/119 [00:06<00:10,  6.89it/s]Epoch: 3, train for the 20-th batch, train loss: 0.6632960438728333:  13%|█▋           | 20/151 [00:03<00:20,  6.33it/s]Epoch: 1, train for the 276-th batch, train loss: 0.6500811576843262:  72%|███████▉   | 276/383 [01:20<00:34,  3.06it/s]Epoch: 3, train for the 132-th batch, train loss: 0.49573859572410583:  90%|████████▉ | 131/146 [00:19<00:02,  6.87it/s]INFO:root:Epoch: 1, learning rate: 0.0001, train loss: 0.5879
INFO:root:train average_precision, 0.7874
INFO:root:train roc_auc, 0.7608
INFO:root:validate loss: 0.5462
INFO:root:validate average_precision, 0.8161
INFO:root:validate roc_auc, 0.8005
INFO:root:new node validate loss: 0.6855
INFO:root:new node validate first_1_average_precision, 0.6116
INFO:root:new node validate first_1_roc_auc, 0.5372
INFO:root:new node validate first_3_average_precision, 0.6351
INFO:root:new node validate first_3_roc_auc, 0.5685
INFO:root:new node validate first_10_average_precision, 0.6683
INFO:root:new node validate first_10_roc_auc, 0.6151
INFO:root:new node validate average_precision, 0.6767
INFO:root:new node validate roc_auc, 0.6308
Epoch: 3, train for the 132-th batch, train loss: 0.49573859572410583:  90%|█████████ | 132/146 [00:19<00:01,  7.05it/s]INFO:root:save model ./saved_models/TGN/ia-digg-reply/TGN_seed0_tgn-ia-digg-reply-lincorrect-time-linear/TGN_seed0_tgn-ia-digg-reply-lincorrect-time-linear.pkl
Epoch: 2, train for the 128-th batch, train loss: 0.5321503281593323:  53%|█████▊     | 127/241 [00:19<00:18,  6.30it/s]Epoch: 2, train for the 128-th batch, train loss: 0.5321503281593323:  53%|█████▊     | 128/241 [00:19<00:17,  6.49it/s]Epoch: 4, train for the 47-th batch, train loss: 0.49213477969169617:  39%|████▋       | 46/119 [00:06<00:10,  6.89it/s]Epoch: 3, train for the 21-th batch, train loss: 0.6743670701980591:  13%|█▋           | 20/151 [00:03<00:20,  6.33it/s]Epoch: 4, train for the 47-th batch, train loss: 0.49213477969169617:  39%|████▋       | 47/119 [00:06<00:10,  6.88it/s]Epoch: 3, train for the 21-th batch, train loss: 0.6743670701980591:  14%|█▊           | 21/151 [00:03<00:20,  6.41it/s]Epoch: 1, train for the 277-th batch, train loss: 0.4422000050544739:  72%|███████▉   | 276/383 [01:20<00:34,  3.06it/s]Epoch: 3, train for the 133-th batch, train loss: 0.4761946499347687:  90%|█████████▉ | 132/146 [00:19<00:01,  7.05it/s]Epoch: 3, train for the 133-th batch, train loss: 0.4761946499347687:  91%|██████████ | 133/146 [00:19<00:01,  7.07it/s]Epoch: 1, train for the 277-th batch, train loss: 0.4422000050544739:  72%|███████▉   | 277/383 [01:20<00:31,  3.37it/s]Epoch: 2, train for the 129-th batch, train loss: 0.45108795166015625:  53%|█████▎    | 128/241 [00:19<00:17,  6.49it/s]Epoch: 2, train for the 129-th batch, train loss: 0.45108795166015625:  54%|█████▎    | 129/241 [00:19<00:16,  6.65it/s]Epoch: 4, train for the 48-th batch, train loss: 0.4625406563282013:  39%|█████▏       | 47/119 [00:06<00:10,  6.88it/s]Epoch: 4, train for the 48-th batch, train loss: 0.4625406563282013:  40%|█████▏       | 48/119 [00:06<00:09,  7.18it/s]Epoch: 3, train for the 22-th batch, train loss: 0.6553230285644531:  14%|█▊           | 21/151 [00:03<00:20,  6.41it/s]Epoch: 3, train for the 22-th batch, train loss: 0.6553230285644531:  15%|█▉           | 22/151 [00:03<00:20,  6.44it/s]Epoch: 3, train for the 134-th batch, train loss: 0.5055732727050781:  91%|██████████ | 133/146 [00:19<00:01,  7.07it/s]Epoch: 2, train for the 130-th batch, train loss: 0.5002803206443787:  54%|█████▉     | 129/241 [00:19<00:16,  6.65it/s]Epoch: 3, train for the 134-th batch, train loss: 0.5055732727050781:  92%|██████████ | 134/146 [00:19<00:01,  6.76it/s]Epoch: 2, train for the 130-th batch, train loss: 0.5002803206443787:  54%|█████▉     | 130/241 [00:19<00:16,  6.82it/s]Epoch: 4, train for the 49-th batch, train loss: 0.45664680004119873:  40%|████▊       | 48/119 [00:06<00:09,  7.18it/s]Epoch: 4, train for the 49-th batch, train loss: 0.45664680004119873:  41%|████▉       | 49/119 [00:06<00:09,  7.02it/s]Epoch: 1, train for the 278-th batch, train loss: 0.36418280005455017:  72%|███████▏  | 277/383 [01:20<00:31,  3.37it/s]Epoch: 3, train for the 23-th batch, train loss: 0.6584157943725586:  15%|█▉           | 22/151 [00:03<00:20,  6.44it/s]Epoch: 3, train for the 23-th batch, train loss: 0.6584157943725586:  15%|█▉           | 23/151 [00:03<00:19,  6.56it/s]Epoch: 1, train for the 278-th batch, train loss: 0.36418280005455017:  73%|███████▎  | 278/383 [01:20<00:29,  3.57it/s]Epoch: 3, train for the 135-th batch, train loss: 0.5129527449607849:  92%|██████████ | 134/146 [00:20<00:01,  6.76it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5253267288208008:  54%|█████▉     | 130/241 [00:19<00:16,  6.82it/s]Epoch: 3, train for the 135-th batch, train loss: 0.5129527449607849:  92%|██████████▏| 135/146 [00:20<00:01,  6.78it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5253267288208008:  54%|█████▉     | 131/241 [00:19<00:16,  6.73it/s]Epoch: 4, train for the 50-th batch, train loss: 0.4245431423187256:  41%|█████▎       | 49/119 [00:07<00:09,  7.02it/s]Epoch: 3, train for the 24-th batch, train loss: 0.6661038994789124:  15%|█▉           | 23/151 [00:03<00:19,  6.56it/s]Epoch: 4, train for the 50-th batch, train loss: 0.4245431423187256:  42%|█████▍       | 50/119 [00:07<00:10,  6.85it/s]Epoch: 3, train for the 24-th batch, train loss: 0.6661038994789124:  16%|██           | 24/151 [00:03<00:18,  6.70it/s]Epoch: 1, train for the 279-th batch, train loss: 0.5112327337265015:  73%|███████▉   | 278/383 [01:20<00:29,  3.57it/s]Epoch: 2, train for the 132-th batch, train loss: 0.4940745532512665:  54%|█████▉     | 131/241 [00:19<00:16,  6.73it/s]Epoch: 2, train for the 132-th batch, train loss: 0.4940745532512665:  55%|██████     | 132/241 [00:19<00:17,  6.32it/s]Epoch: 3, train for the 25-th batch, train loss: 0.724923312664032:  16%|██▏           | 24/151 [00:03<00:18,  6.70it/s]Epoch: 3, train for the 25-th batch, train loss: 0.724923312664032:  17%|██▎           | 25/151 [00:03<00:18,  6.69it/s]Epoch: 1, train for the 279-th batch, train loss: 0.5112327337265015:  73%|████████   | 279/383 [01:20<00:28,  3.64it/s]Epoch: 3, train for the 136-th batch, train loss: 0.4849422872066498:  92%|██████████▏| 135/146 [00:20<00:01,  6.78it/s]Epoch: 4, train for the 51-th batch, train loss: 0.4878077208995819:  42%|█████▍       | 50/119 [00:07<00:10,  6.85it/s]Epoch: 3, train for the 136-th batch, train loss: 0.4849422872066498:  93%|██████████▏| 136/146 [00:20<00:01,  5.65it/s]Epoch: 4, train for the 51-th batch, train loss: 0.4878077208995819:  43%|█████▌       | 51/119 [00:07<00:11,  6.02it/s]Epoch: 2, train for the 133-th batch, train loss: 0.41042792797088623:  55%|█████▍    | 132/241 [00:20<00:17,  6.32it/s]Epoch: 2, train for the 133-th batch, train loss: 0.41042792797088623:  55%|█████▌    | 133/241 [00:20<00:16,  6.49it/s]Epoch: 3, train for the 26-th batch, train loss: 0.6711363196372986:  17%|██▏          | 25/151 [00:03<00:18,  6.69it/s]Epoch: 3, train for the 26-th batch, train loss: 0.6711363196372986:  17%|██▏          | 26/151 [00:03<00:18,  6.60it/s]Epoch: 4, train for the 52-th batch, train loss: 0.4763370454311371:  43%|█████▌       | 51/119 [00:07<00:11,  6.02it/s]Epoch: 4, train for the 52-th batch, train loss: 0.4763370454311371:  44%|█████▋       | 52/119 [00:07<00:11,  6.05it/s]Epoch: 1, train for the 280-th batch, train loss: 0.3352120816707611:  73%|████████   | 279/383 [01:20<00:28,  3.64it/s]Epoch: 3, train for the 137-th batch, train loss: 0.4604951739311218:  93%|██████████▏| 136/146 [00:20<00:01,  5.65it/s]Epoch: 3, train for the 137-th batch, train loss: 0.4604951739311218:  94%|██████████▎| 137/146 [00:20<00:01,  5.48it/s]Epoch: 2, train for the 134-th batch, train loss: 0.47607579827308655:  55%|█████▌    | 133/241 [00:20<00:16,  6.49it/s]Epoch: 1, train for the 280-th batch, train loss: 0.3352120816707611:  73%|████████   | 280/383 [01:21<00:28,  3.65it/s]Epoch: 2, train for the 134-th batch, train loss: 0.47607579827308655:  56%|█████▌    | 134/241 [00:20<00:16,  6.43it/s]Epoch: 3, train for the 27-th batch, train loss: 0.673242449760437:  17%|██▍           | 26/151 [00:04<00:18,  6.60it/s]Epoch: 3, train for the 27-th batch, train loss: 0.673242449760437:  18%|██▌           | 27/151 [00:04<00:19,  6.42it/s]Epoch: 4, train for the 53-th batch, train loss: 0.441262423992157:  44%|██████        | 52/119 [00:07<00:11,  6.05it/s]Epoch: 4, train for the 53-th batch, train loss: 0.441262423992157:  45%|██████▏       | 53/119 [00:07<00:10,  6.18it/s]Epoch: 3, train for the 138-th batch, train loss: 0.5175061821937561:  94%|██████████▎| 137/146 [00:20<00:01,  5.48it/s]Epoch: 3, train for the 138-th batch, train loss: 0.5175061821937561:  95%|██████████▍| 138/146 [00:20<00:01,  5.65it/s]Epoch: 2, train for the 135-th batch, train loss: 0.4011287987232208:  56%|██████     | 134/241 [00:20<00:16,  6.43it/s]Epoch: 2, train for the 135-th batch, train loss: 0.4011287987232208:  56%|██████▏    | 135/241 [00:20<00:16,  6.51it/s]Epoch: 3, train for the 28-th batch, train loss: 0.678781270980835:  18%|██▌           | 27/151 [00:04<00:19,  6.42it/s]Epoch: 3, train for the 28-th batch, train loss: 0.678781270980835:  19%|██▌           | 28/151 [00:04<00:18,  6.61it/s]Epoch: 4, train for the 54-th batch, train loss: 0.4129641652107239:  45%|█████▊       | 53/119 [00:07<00:10,  6.18it/s]Epoch: 4, train for the 54-th batch, train loss: 0.4129641652107239:  45%|█████▉       | 54/119 [00:07<00:10,  6.32it/s]Epoch: 3, train for the 139-th batch, train loss: 0.5231308937072754:  95%|██████████▍| 138/146 [00:20<00:01,  5.65it/s]Epoch: 3, train for the 139-th batch, train loss: 0.5231308937072754:  95%|██████████▍| 139/146 [00:20<00:01,  5.84it/s]Epoch: 2, train for the 136-th batch, train loss: 0.4478197693824768:  56%|██████▏    | 135/241 [00:20<00:16,  6.51it/s]Epoch: 3, train for the 29-th batch, train loss: 0.6919398307800293:  19%|██▍          | 28/151 [00:04<00:18,  6.61it/s]Epoch: 2, train for the 136-th batch, train loss: 0.4478197693824768:  56%|██████▏    | 136/241 [00:20<00:16,  6.32it/s]Epoch: 3, train for the 29-th batch, train loss: 0.6919398307800293:  19%|██▍          | 29/151 [00:04<00:18,  6.59it/s]Epoch: 4, train for the 55-th batch, train loss: 0.43421661853790283:  45%|█████▍      | 54/119 [00:07<00:10,  6.32it/s]Epoch: 1, train for the 281-th batch, train loss: 0.3917493224143982:  73%|████████   | 280/383 [01:21<00:28,  3.65it/s]Epoch: 4, train for the 55-th batch, train loss: 0.43421661853790283:  46%|█████▌      | 55/119 [00:07<00:09,  6.87it/s]Epoch: 3, train for the 140-th batch, train loss: 0.48900023102760315:  95%|█████████▌| 139/146 [00:20<00:01,  5.84it/s]Epoch: 1, train for the 281-th batch, train loss: 0.3917493224143982:  73%|████████   | 281/383 [01:21<00:31,  3.19it/s]Epoch: 3, train for the 140-th batch, train loss: 0.48900023102760315:  96%|█████████▌| 140/146 [00:20<00:00,  6.34it/s]Epoch: 4, train for the 56-th batch, train loss: 0.44288626313209534:  46%|█████▌      | 55/119 [00:08<00:09,  6.87it/s]Epoch: 4, train for the 56-th batch, train loss: 0.44288626313209534:  47%|█████▋      | 56/119 [00:08<00:08,  7.19it/s]Epoch: 2, train for the 137-th batch, train loss: 0.41658228635787964:  56%|█████▋    | 136/241 [00:20<00:16,  6.32it/s]Epoch: 3, train for the 30-th batch, train loss: 0.6918909549713135:  19%|██▍          | 29/151 [00:04<00:18,  6.59it/s]Epoch: 2, train for the 137-th batch, train loss: 0.41658228635787964:  57%|█████▋    | 137/241 [00:20<00:17,  5.95it/s]Epoch: 3, train for the 30-th batch, train loss: 0.6918909549713135:  20%|██▌          | 30/151 [00:04<00:19,  6.18it/s]Epoch: 3, train for the 141-th batch, train loss: 0.5091857314109802:  96%|██████████▌| 140/146 [00:21<00:00,  6.34it/s]Epoch: 3, train for the 141-th batch, train loss: 0.5091857314109802:  97%|██████████▌| 141/146 [00:21<00:00,  6.54it/s]Epoch: 1, train for the 282-th batch, train loss: 0.4823122024536133:  73%|████████   | 281/383 [01:21<00:31,  3.19it/s]Epoch: 4, train for the 57-th batch, train loss: 0.44131964445114136:  47%|█████▋      | 56/119 [00:08<00:08,  7.19it/s]Epoch: 4, train for the 57-th batch, train loss: 0.44131964445114136:  48%|█████▋      | 57/119 [00:08<00:08,  7.44it/s]Epoch: 1, train for the 282-th batch, train loss: 0.4823122024536133:  74%|████████   | 282/383 [01:21<00:29,  3.48it/s]Epoch: 2, train for the 138-th batch, train loss: 0.41160833835601807:  57%|█████▋    | 137/241 [00:20<00:17,  5.95it/s]Epoch: 3, train for the 31-th batch, train loss: 0.6894561648368835:  20%|██▌          | 30/151 [00:04<00:19,  6.18it/s]Epoch: 2, train for the 138-th batch, train loss: 0.41160833835601807:  57%|█████▋    | 138/241 [00:20<00:17,  5.74it/s]Epoch: 3, train for the 142-th batch, train loss: 0.46342241764068604:  97%|█████████▋| 141/146 [00:21<00:00,  6.54it/s]Epoch: 3, train for the 31-th batch, train loss: 0.6894561648368835:  21%|██▋          | 31/151 [00:04<00:20,  5.85it/s]Epoch: 3, train for the 142-th batch, train loss: 0.46342241764068604:  97%|█████████▋| 142/146 [00:21<00:00,  6.19it/s]Epoch: 4, train for the 58-th batch, train loss: 0.4266674816608429:  48%|██████▏      | 57/119 [00:08<00:08,  7.44it/s]Epoch: 4, train for the 58-th batch, train loss: 0.4266674816608429:  49%|██████▎      | 58/119 [00:08<00:08,  7.30it/s]Epoch: 2, train for the 139-th batch, train loss: 0.33677712082862854:  57%|█████▋    | 138/241 [00:21<00:17,  5.74it/s]Epoch: 2, train for the 139-th batch, train loss: 0.33677712082862854:  58%|█████▊    | 139/241 [00:21<00:16,  6.02it/s]Epoch: 1, train for the 283-th batch, train loss: 0.48388031125068665:  74%|███████▎  | 282/383 [01:21<00:29,  3.48it/s]Epoch: 3, train for the 143-th batch, train loss: 0.4942900836467743:  97%|██████████▋| 142/146 [00:21<00:00,  6.19it/s]Epoch: 3, train for the 32-th batch, train loss: 0.6736396551132202:  21%|██▋          | 31/151 [00:04<00:20,  5.85it/s]Epoch: 3, train for the 32-th batch, train loss: 0.6736396551132202:  21%|██▊          | 32/151 [00:04<00:20,  5.85it/s]Epoch: 3, train for the 143-th batch, train loss: 0.4942900836467743:  98%|██████████▊| 143/146 [00:21<00:00,  6.24it/s]Epoch: 4, train for the 59-th batch, train loss: 0.4655393362045288:  49%|██████▎      | 58/119 [00:08<00:08,  7.30it/s]Epoch: 1, train for the 283-th batch, train loss: 0.48388031125068665:  74%|███████▍  | 283/383 [01:21<00:28,  3.48it/s]Epoch: 4, train for the 59-th batch, train loss: 0.4655393362045288:  50%|██████▍      | 59/119 [00:08<00:08,  6.72it/s]Epoch: 2, train for the 140-th batch, train loss: 0.3763583302497864:  58%|██████▎    | 139/241 [00:21<00:16,  6.02it/s]Epoch: 2, train for the 140-th batch, train loss: 0.3763583302497864:  58%|██████▍    | 140/241 [00:21<00:15,  6.43it/s]Epoch: 3, train for the 33-th batch, train loss: 0.6637704968452454:  21%|██▊          | 32/151 [00:05<00:20,  5.85it/s]Epoch: 3, train for the 33-th batch, train loss: 0.6637704968452454:  22%|██▊          | 33/151 [00:05<00:19,  6.16it/s]Epoch: 3, train for the 144-th batch, train loss: 0.4349207878112793:  98%|██████████▊| 143/146 [00:21<00:00,  6.24it/s]Epoch: 3, train for the 144-th batch, train loss: 0.4349207878112793:  99%|██████████▊| 144/146 [00:21<00:00,  6.02it/s]Epoch: 4, train for the 60-th batch, train loss: 0.4549216330051422:  50%|██████▍      | 59/119 [00:08<00:08,  6.72it/s]Epoch: 4, train for the 60-th batch, train loss: 0.4549216330051422:  50%|██████▌      | 60/119 [00:08<00:09,  6.55it/s]Epoch: 2, train for the 141-th batch, train loss: 0.47863146662712097:  58%|█████▊    | 140/241 [00:21<00:15,  6.43it/s]Epoch: 2, train for the 141-th batch, train loss: 0.47863146662712097:  59%|█████▊    | 141/241 [00:21<00:15,  6.65it/s]Epoch: 3, train for the 34-th batch, train loss: 0.6607468128204346:  22%|██▊          | 33/151 [00:05<00:19,  6.16it/s]Epoch: 3, train for the 34-th batch, train loss: 0.6607468128204346:  23%|██▉          | 34/151 [00:05<00:17,  6.50it/s]Epoch: 3, train for the 145-th batch, train loss: 0.5060510039329529:  99%|██████████▊| 144/146 [00:21<00:00,  6.02it/s]Epoch: 4, train for the 61-th batch, train loss: 0.46265989542007446:  50%|██████      | 60/119 [00:08<00:09,  6.55it/s]Epoch: 4, train for the 61-th batch, train loss: 0.46265989542007446:  51%|██████▏     | 61/119 [00:08<00:08,  6.46it/s]Epoch: 3, train for the 145-th batch, train loss: 0.5060510039329529:  99%|██████████▉| 145/146 [00:21<00:00,  5.94it/s]Epoch: 2, train for the 142-th batch, train loss: 0.45943981409072876:  59%|█████▊    | 141/241 [00:21<00:15,  6.65it/s]Epoch: 2, train for the 142-th batch, train loss: 0.45943981409072876:  59%|█████▉    | 142/241 [00:21<00:15,  6.51it/s]Epoch: 3, train for the 35-th batch, train loss: 0.6520553231239319:  23%|██▉          | 34/151 [00:05<00:17,  6.50it/s]Epoch: 3, train for the 35-th batch, train loss: 0.6520553231239319:  23%|███          | 35/151 [00:05<00:17,  6.63it/s]Epoch: 1, train for the 284-th batch, train loss: 0.5172209739685059:  74%|████████▏  | 283/383 [01:22<00:28,  3.48it/s]Epoch: 4, train for the 62-th batch, train loss: 0.460269570350647:  51%|███████▏      | 61/119 [00:09<00:08,  6.46it/s]Epoch: 3, train for the 146-th batch, train loss: 0.42035043239593506:  99%|█████████▉| 145/146 [00:21<00:00,  5.94it/s]Epoch: 4, train for the 62-th batch, train loss: 0.460269570350647:  52%|███████▎      | 62/119 [00:09<00:09,  6.17it/s]Epoch: 3, train for the 146-th batch, train loss: 0.42035043239593506: 100%|██████████| 146/146 [00:21<00:00,  5.79it/s]Epoch: 3, train for the 146-th batch, train loss: 0.42035043239593506: 100%|██████████| 146/146 [00:21<00:00,  6.64it/s]
Epoch: 1, train for the 284-th batch, train loss: 0.5172209739685059:  74%|████████▏  | 284/383 [01:22<00:35,  2.81it/s]Epoch: 3, train for the 36-th batch, train loss: 0.6800703406333923:  23%|███          | 35/151 [00:05<00:17,  6.63it/s]Epoch: 2, train for the 143-th batch, train loss: 0.4828454554080963:  59%|██████▍    | 142/241 [00:21<00:15,  6.51it/s]Epoch: 3, train for the 36-th batch, train loss: 0.6800703406333923:  24%|███          | 36/151 [00:05<00:18,  6.38it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/237 [00:00<?, ?it/s]Epoch: 2, train for the 143-th batch, train loss: 0.4828454554080963:  59%|██████▌    | 143/241 [00:21<00:16,  5.97it/s]  0%|                                                                                            | 0/38 [00:00<?, ?it/s]Epoch: 4, train for the 63-th batch, train loss: 0.44397979974746704:  52%|██████▎     | 62/119 [00:09<00:09,  6.17it/s]Epoch: 4, train for the 63-th batch, train loss: 0.44397979974746704:  53%|██████▎     | 63/119 [00:09<00:08,  6.59it/s]Epoch: 2, train for the 1-th batch, train loss: 0.6841727495193481:   0%|                       | 0/237 [00:00<?, ?it/s]Epoch: 2, train for the 1-th batch, train loss: 0.6841727495193481:   0%|               | 1/237 [00:00<00:26,  8.98it/s]evaluate for the 1-th batch, evaluate loss: 0.4852919578552246:   0%|                            | 0/38 [00:00<?, ?it/s]Epoch: 3, train for the 37-th batch, train loss: 0.6660438179969788:  24%|███          | 36/151 [00:05<00:18,  6.38it/s]Epoch: 3, train for the 37-th batch, train loss: 0.6660438179969788:  25%|███▏         | 37/151 [00:05<00:17,  6.42it/s]Epoch: 2, train for the 144-th batch, train loss: 0.5730239152908325:  59%|██████▌    | 143/241 [00:21<00:16,  5.97it/s]Epoch: 4, train for the 64-th batch, train loss: 0.4494856894016266:  53%|██████▉      | 63/119 [00:09<00:08,  6.59it/s]evaluate for the 2-th batch, evaluate loss: 0.4865632653236389:   0%|                            | 0/38 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.4865632653236389:   5%|█                   | 2/38 [00:00<00:03, 11.91it/s]Epoch: 4, train for the 64-th batch, train loss: 0.4494856894016266:  54%|██████▉      | 64/119 [00:09<00:07,  6.93it/s]Epoch: 2, train for the 144-th batch, train loss: 0.5730239152908325:  60%|██████▌    | 144/241 [00:21<00:16,  5.76it/s]Epoch: 1, train for the 285-th batch, train loss: 0.43942663073539734:  74%|███████▍  | 284/383 [01:22<00:35,  2.81it/s]Epoch: 2, train for the 2-th batch, train loss: 0.6401811242103577:   0%|               | 1/237 [00:00<00:26,  8.98it/s]Epoch: 2, train for the 2-th batch, train loss: 0.6401811242103577:   1%|▏              | 2/237 [00:00<00:29,  8.00it/s]evaluate for the 3-th batch, evaluate loss: 0.47385725378990173:   5%|█                  | 2/38 [00:00<00:03, 11.91it/s]Epoch: 3, train for the 38-th batch, train loss: 0.6759693622589111:  25%|███▏         | 37/151 [00:05<00:17,  6.42it/s]Epoch: 1, train for the 285-th batch, train loss: 0.43942663073539734:  74%|███████▍  | 285/383 [01:22<00:33,  2.90it/s]Epoch: 3, train for the 38-th batch, train loss: 0.6759693622589111:  25%|███▎         | 38/151 [00:05<00:16,  6.73it/s]Epoch: 4, train for the 65-th batch, train loss: 0.45312875509262085:  54%|██████▍     | 64/119 [00:09<00:07,  6.93it/s]evaluate for the 4-th batch, evaluate loss: 0.4785766005516052:   5%|█                   | 2/38 [00:00<00:03, 11.91it/s]Epoch: 4, train for the 65-th batch, train loss: 0.45312875509262085:  55%|██████▌     | 65/119 [00:09<00:07,  7.06it/s]evaluate for the 4-th batch, evaluate loss: 0.4785766005516052:  11%|██                  | 4/38 [00:00<00:02, 13.42it/s]Epoch: 2, train for the 145-th batch, train loss: 0.5870024561882019:  60%|██████▌    | 144/241 [00:21<00:16,  5.76it/s]Epoch: 2, train for the 3-th batch, train loss: 0.6119080185890198:   1%|▏              | 2/237 [00:00<00:29,  8.00it/s]Epoch: 2, train for the 145-th batch, train loss: 0.5870024561882019:  60%|██████▌    | 145/241 [00:22<00:16,  5.77it/s]Epoch: 2, train for the 3-th batch, train loss: 0.6119080185890198:   1%|▏              | 3/237 [00:00<00:29,  7.97it/s]evaluate for the 5-th batch, evaluate loss: 0.5452811121940613:  11%|██                  | 4/38 [00:00<00:02, 13.42it/s]Epoch: 3, train for the 39-th batch, train loss: 0.6616945266723633:  25%|███▎         | 38/151 [00:05<00:16,  6.73it/s]Epoch: 4, train for the 66-th batch, train loss: 0.4489331841468811:  55%|███████      | 65/119 [00:09<00:07,  7.06it/s]Epoch: 3, train for the 39-th batch, train loss: 0.6616945266723633:  26%|███▎         | 39/151 [00:05<00:17,  6.59it/s]evaluate for the 6-th batch, evaluate loss: 0.48626577854156494:  11%|██                 | 4/38 [00:00<00:02, 13.42it/s]Epoch: 4, train for the 66-th batch, train loss: 0.4489331841468811:  55%|███████▏     | 66/119 [00:09<00:07,  7.24it/s]evaluate for the 6-th batch, evaluate loss: 0.48626577854156494:  16%|███                | 6/38 [00:00<00:02, 14.24it/s]Epoch: 1, train for the 286-th batch, train loss: 0.38055419921875:  74%|█████████▋   | 285/383 [01:22<00:33,  2.90it/s]Epoch: 2, train for the 146-th batch, train loss: 0.5935592651367188:  60%|██████▌    | 145/241 [00:22<00:16,  5.77it/s]evaluate for the 7-th batch, evaluate loss: 0.43782341480255127:  16%|███                | 6/38 [00:00<00:02, 14.24it/s]Epoch: 2, train for the 146-th batch, train loss: 0.5935592651367188:  61%|██████▋    | 146/241 [00:22<00:16,  5.81it/s]Epoch: 2, train for the 4-th batch, train loss: 0.5518508553504944:   1%|▏              | 3/237 [00:00<00:29,  7.97it/s]Epoch: 2, train for the 4-th batch, train loss: 0.5518508553504944:   2%|▎              | 4/237 [00:00<00:33,  6.90it/s]evaluate for the 8-th batch, evaluate loss: 0.4606817662715912:  16%|███▏                | 6/38 [00:00<00:02, 14.24it/s]evaluate for the 8-th batch, evaluate loss: 0.4606817662715912:  21%|████▏               | 8/38 [00:00<00:01, 15.58it/s]Epoch: 3, train for the 40-th batch, train loss: 0.6563020944595337:  26%|███▎         | 39/151 [00:06<00:17,  6.59it/s]Epoch: 1, train for the 286-th batch, train loss: 0.38055419921875:  75%|█████████▋   | 286/383 [01:23<00:32,  3.02it/s]Epoch: 3, train for the 40-th batch, train loss: 0.6563020944595337:  26%|███▍         | 40/151 [00:06<00:16,  6.76it/s]evaluate for the 9-th batch, evaluate loss: 0.47693151235580444:  21%|████               | 8/38 [00:00<00:01, 15.58it/s]evaluate for the 10-th batch, evaluate loss: 0.5018643140792847:  21%|████               | 8/38 [00:00<00:01, 15.58it/s]evaluate for the 10-th batch, evaluate loss: 0.5018643140792847:  26%|████▋             | 10/38 [00:00<00:01, 16.47it/s]Epoch: 2, train for the 5-th batch, train loss: 0.6180022954940796:   2%|▎              | 4/237 [00:00<00:33,  6.90it/s]Epoch: 2, train for the 5-th batch, train loss: 0.6180022954940796:   2%|▎              | 5/237 [00:00<00:35,  6.56it/s]Epoch: 2, train for the 147-th batch, train loss: 0.650847315788269:  61%|███████▎    | 146/241 [00:22<00:16,  5.81it/s]Epoch: 4, train for the 67-th batch, train loss: 0.5310571193695068:  55%|███████▏     | 66/119 [00:09<00:07,  7.24it/s]evaluate for the 11-th batch, evaluate loss: 0.44785842299461365:  26%|████▍            | 10/38 [00:00<00:01, 16.47it/s]Epoch: 3, train for the 41-th batch, train loss: 0.6638232469558716:  26%|███▍         | 40/151 [00:06<00:16,  6.76it/s]Epoch: 4, train for the 67-th batch, train loss: 0.5310571193695068:  56%|███████▎     | 67/119 [00:09<00:10,  5.19it/s]Epoch: 2, train for the 147-th batch, train loss: 0.650847315788269:  61%|███████▎    | 147/241 [00:22<00:18,  5.16it/s]Epoch: 3, train for the 41-th batch, train loss: 0.6638232469558716:  27%|███▌         | 41/151 [00:06<00:18,  5.95it/s]evaluate for the 12-th batch, evaluate loss: 0.5224941968917847:  26%|████▋             | 10/38 [00:00<00:01, 16.47it/s]evaluate for the 12-th batch, evaluate loss: 0.5224941968917847:  32%|█████▋            | 12/38 [00:00<00:01, 15.61it/s]Epoch: 2, train for the 6-th batch, train loss: 0.6311212182044983:   2%|▎              | 5/237 [00:00<00:35,  6.56it/s]Epoch: 2, train for the 6-th batch, train loss: 0.6311212182044983:   3%|▍              | 6/237 [00:00<00:34,  6.64it/s]Epoch: 1, train for the 287-th batch, train loss: 0.41712504625320435:  75%|███████▍  | 286/383 [01:23<00:32,  3.02it/s]evaluate for the 13-th batch, evaluate loss: 0.4901295304298401:  32%|█████▋            | 12/38 [00:00<00:01, 15.61it/s]Epoch: 4, train for the 68-th batch, train loss: 0.43481048941612244:  56%|██████▊     | 67/119 [00:09<00:10,  5.19it/s]evaluate for the 14-th batch, evaluate loss: 0.43092992901802063:  32%|█████▎           | 12/38 [00:00<00:01, 15.61it/s]evaluate for the 14-th batch, evaluate loss: 0.43092992901802063:  37%|██████▎          | 14/38 [00:00<00:01, 16.27it/s]Epoch: 4, train for the 68-th batch, train loss: 0.43481048941612244:  57%|██████▊     | 68/119 [00:10<00:09,  5.53it/s]Epoch: 1, train for the 287-th batch, train loss: 0.41712504625320435:  75%|███████▍  | 287/383 [01:23<00:32,  2.92it/s]Epoch: 2, train for the 148-th batch, train loss: 0.506406843662262:  61%|███████▎    | 147/241 [00:22<00:18,  5.16it/s]Epoch: 3, train for the 42-th batch, train loss: 0.6556246876716614:  27%|███▌         | 41/151 [00:06<00:18,  5.95it/s]Epoch: 2, train for the 148-th batch, train loss: 0.506406843662262:  61%|███████▎    | 148/241 [00:22<00:17,  5.17it/s]evaluate for the 15-th batch, evaluate loss: 0.4456600844860077:  37%|██████▋           | 14/38 [00:00<00:01, 16.27it/s]Epoch: 2, train for the 7-th batch, train loss: 0.6190931797027588:   3%|▍              | 6/237 [00:00<00:34,  6.64it/s]Epoch: 3, train for the 42-th batch, train loss: 0.6556246876716614:  28%|███▌         | 42/151 [00:06<00:19,  5.67it/s]Epoch: 2, train for the 7-th batch, train loss: 0.6190931797027588:   3%|▍              | 7/237 [00:00<00:33,  6.81it/s]evaluate for the 16-th batch, evaluate loss: 0.5159270167350769:  37%|██████▋           | 14/38 [00:01<00:01, 16.27it/s]evaluate for the 16-th batch, evaluate loss: 0.5159270167350769:  42%|███████▌          | 16/38 [00:01<00:01, 15.58it/s]Epoch: 4, train for the 69-th batch, train loss: 0.42832863330841064:  57%|██████▊     | 68/119 [00:10<00:09,  5.53it/s]Epoch: 4, train for the 69-th batch, train loss: 0.42832863330841064:  58%|██████▉     | 69/119 [00:10<00:08,  5.63it/s]evaluate for the 17-th batch, evaluate loss: 0.4670972228050232:  42%|███████▌          | 16/38 [00:01<00:01, 15.58it/s]Epoch: 2, train for the 8-th batch, train loss: 0.5961912274360657:   3%|▍              | 7/237 [00:01<00:33,  6.81it/s]Epoch: 2, train for the 8-th batch, train loss: 0.5961912274360657:   3%|▌              | 8/237 [00:01<00:33,  6.79it/s]Epoch: 2, train for the 149-th batch, train loss: 0.4688992202281952:  61%|██████▊    | 148/241 [00:22<00:17,  5.17it/s]Epoch: 3, train for the 43-th batch, train loss: 0.6697917580604553:  28%|███▌         | 42/151 [00:06<00:19,  5.67it/s]Epoch: 2, train for the 149-th batch, train loss: 0.4688992202281952:  62%|██████▊    | 149/241 [00:22<00:17,  5.12it/s]evaluate for the 18-th batch, evaluate loss: 0.5045432448387146:  42%|███████▌          | 16/38 [00:01<00:01, 15.58it/s]evaluate for the 18-th batch, evaluate loss: 0.5045432448387146:  47%|████████▌         | 18/38 [00:01<00:01, 16.50it/s]Epoch: 3, train for the 43-th batch, train loss: 0.6697917580604553:  28%|███▋         | 43/151 [00:06<00:19,  5.46it/s]Epoch: 4, train for the 70-th batch, train loss: 0.4063485562801361:  58%|███████▌     | 69/119 [00:10<00:08,  5.63it/s]evaluate for the 19-th batch, evaluate loss: 0.49051931500434875:  47%|████████         | 18/38 [00:01<00:01, 16.50it/s]Epoch: 4, train for the 70-th batch, train loss: 0.4063485562801361:  59%|███████▋     | 70/119 [00:10<00:08,  5.91it/s]Epoch: 2, train for the 9-th batch, train loss: 0.5883664488792419:   3%|▌              | 8/237 [00:01<00:33,  6.79it/s]Epoch: 2, train for the 9-th batch, train loss: 0.5883664488792419:   4%|▌              | 9/237 [00:01<00:33,  6.82it/s]evaluate for the 20-th batch, evaluate loss: 0.40044310688972473:  47%|████████         | 18/38 [00:01<00:01, 16.50it/s]evaluate for the 20-th batch, evaluate loss: 0.40044310688972473:  53%|████████▉        | 20/38 [00:01<00:01, 16.09it/s]Epoch: 1, train for the 288-th batch, train loss: 0.47171714901924133:  75%|███████▍  | 287/383 [01:23<00:32,  2.92it/s]Epoch: 2, train for the 150-th batch, train loss: 0.45783400535583496:  62%|██████▏   | 149/241 [00:22<00:17,  5.12it/s]Epoch: 2, train for the 150-th batch, train loss: 0.45783400535583496:  62%|██████▏   | 150/241 [00:23<00:17,  5.25it/s]Epoch: 3, train for the 44-th batch, train loss: 0.6428032517433167:  28%|███▋         | 43/151 [00:06<00:19,  5.46it/s]Epoch: 3, train for the 44-th batch, train loss: 0.6428032517433167:  29%|███▊         | 44/151 [00:06<00:19,  5.52it/s]Epoch: 1, train for the 288-th batch, train loss: 0.47171714901924133:  75%|███████▌  | 288/383 [01:23<00:35,  2.71it/s]Epoch: 4, train for the 71-th batch, train loss: 0.43220072984695435:  59%|███████     | 70/119 [00:10<00:08,  5.91it/s]evaluate for the 21-th batch, evaluate loss: 0.4368050992488861:  53%|█████████▍        | 20/38 [00:01<00:01, 16.09it/s]Epoch: 4, train for the 71-th batch, train loss: 0.43220072984695435:  60%|███████▏    | 71/119 [00:10<00:07,  6.22it/s]Epoch: 2, train for the 10-th batch, train loss: 0.6234917044639587:   4%|▌             | 9/237 [00:01<00:33,  6.82it/s]Epoch: 2, train for the 10-th batch, train loss: 0.6234917044639587:   4%|▌            | 10/237 [00:01<00:31,  7.13it/s]evaluate for the 22-th batch, evaluate loss: 0.46796634793281555:  53%|████████▉        | 20/38 [00:01<00:01, 16.09it/s]evaluate for the 22-th batch, evaluate loss: 0.46796634793281555:  58%|█████████▊       | 22/38 [00:01<00:01, 14.42it/s]Epoch: 2, train for the 151-th batch, train loss: 0.38247960805892944:  62%|██████▏   | 150/241 [00:23<00:17,  5.25it/s]Epoch: 2, train for the 151-th batch, train loss: 0.38247960805892944:  63%|██████▎   | 151/241 [00:23<00:16,  5.62it/s]Epoch: 3, train for the 45-th batch, train loss: 0.6409904360771179:  29%|███▊         | 44/151 [00:07<00:19,  5.52it/s]Epoch: 3, train for the 45-th batch, train loss: 0.6409904360771179:  30%|███▊         | 45/151 [00:07<00:18,  5.65it/s]Epoch: 2, train for the 11-th batch, train loss: 0.6464329361915588:   4%|▌            | 10/237 [00:01<00:31,  7.13it/s]Epoch: 4, train for the 72-th batch, train loss: 0.45891204476356506:  60%|███████▏    | 71/119 [00:10<00:07,  6.22it/s]Epoch: 2, train for the 11-th batch, train loss: 0.6464329361915588:   5%|▌            | 11/237 [00:01<00:31,  7.26it/s]evaluate for the 23-th batch, evaluate loss: 0.46785399317741394:  58%|█████████▊       | 22/38 [00:01<00:01, 14.42it/s]Epoch: 4, train for the 72-th batch, train loss: 0.45891204476356506:  61%|███████▎    | 72/119 [00:10<00:07,  6.13it/s]Epoch: 1, train for the 289-th batch, train loss: 0.46312299370765686:  75%|███████▌  | 288/383 [01:24<00:35,  2.71it/s]evaluate for the 24-th batch, evaluate loss: 0.4528958797454834:  58%|██████████▍       | 22/38 [00:01<00:01, 14.42it/s]evaluate for the 24-th batch, evaluate loss: 0.4528958797454834:  63%|███████████▎      | 24/38 [00:01<00:01, 13.71it/s]Epoch: 2, train for the 12-th batch, train loss: 0.6402501463890076:   5%|▌            | 11/237 [00:01<00:31,  7.26it/s]Epoch: 1, train for the 289-th batch, train loss: 0.46312299370765686:  75%|███████▌  | 289/383 [01:24<00:32,  2.87it/s]Epoch: 2, train for the 152-th batch, train loss: 0.3405417501926422:  63%|██████▉    | 151/241 [00:23<00:16,  5.62it/s]Epoch: 2, train for the 12-th batch, train loss: 0.6402501463890076:   5%|▋            | 12/237 [00:01<00:30,  7.39it/s]Epoch: 2, train for the 152-th batch, train loss: 0.3405417501926422:  63%|██████▉    | 152/241 [00:23<00:16,  5.43it/s]Epoch: 3, train for the 46-th batch, train loss: 0.6442735195159912:  30%|███▊         | 45/151 [00:07<00:18,  5.65it/s]Epoch: 4, train for the 73-th batch, train loss: 0.45123404264450073:  61%|███████▎    | 72/119 [00:10<00:07,  6.13it/s]Epoch: 3, train for the 46-th batch, train loss: 0.6442735195159912:  30%|███▉         | 46/151 [00:07<00:19,  5.50it/s]evaluate for the 25-th batch, evaluate loss: 0.49149298667907715:  63%|██████████▋      | 24/38 [00:01<00:01, 13.71it/s]Epoch: 4, train for the 73-th batch, train loss: 0.45123404264450073:  61%|███████▎    | 73/119 [00:10<00:07,  6.05it/s]Epoch: 2, train for the 153-th batch, train loss: 0.43596595525741577:  63%|██████▎   | 152/241 [00:23<00:16,  5.43it/s]Epoch: 2, train for the 13-th batch, train loss: 0.6483268737792969:   5%|▋            | 12/237 [00:01<00:30,  7.39it/s]evaluate for the 26-th batch, evaluate loss: 0.47082236409187317:  63%|██████████▋      | 24/38 [00:01<00:01, 13.71it/s]evaluate for the 26-th batch, evaluate loss: 0.47082236409187317:  68%|███████████▋     | 26/38 [00:01<00:00, 12.79it/s]Epoch: 2, train for the 13-th batch, train loss: 0.6483268737792969:   5%|▋            | 13/237 [00:01<00:31,  7.16it/s]Epoch: 2, train for the 153-th batch, train loss: 0.43596595525741577:  63%|██████▎   | 153/241 [00:23<00:14,  5.87it/s]Epoch: 4, train for the 74-th batch, train loss: 0.4325835704803467:  61%|███████▉     | 73/119 [00:10<00:07,  6.05it/s]evaluate for the 27-th batch, evaluate loss: 0.46514320373535156:  68%|███████████▋     | 26/38 [00:01<00:00, 12.79it/s]Epoch: 4, train for the 74-th batch, train loss: 0.4325835704803467:  62%|████████     | 74/119 [00:10<00:07,  6.18it/s]evaluate for the 28-th batch, evaluate loss: 0.47626692056655884:  68%|███████████▋     | 26/38 [00:01<00:00, 12.79it/s]evaluate for the 28-th batch, evaluate loss: 0.47626692056655884:  74%|████████████▌    | 28/38 [00:01<00:00, 13.66it/s]Epoch: 1, train for the 290-th batch, train loss: 0.44636276364326477:  75%|███████▌  | 289/383 [01:24<00:32,  2.87it/s]Epoch: 2, train for the 14-th batch, train loss: 0.6730731725692749:   5%|▋            | 13/237 [00:01<00:31,  7.16it/s]Epoch: 2, train for the 154-th batch, train loss: 0.3479454815387726:  63%|██████▉    | 153/241 [00:23<00:14,  5.87it/s]Epoch: 2, train for the 14-th batch, train loss: 0.6730731725692749:   6%|▊            | 14/237 [00:02<00:33,  6.67it/s]Epoch: 4, train for the 75-th batch, train loss: 0.4351576566696167:  62%|████████     | 74/119 [00:11<00:07,  6.18it/s]Epoch: 2, train for the 154-th batch, train loss: 0.3479454815387726:  64%|███████    | 154/241 [00:23<00:14,  5.83it/s]Epoch: 4, train for the 75-th batch, train loss: 0.4351576566696167:  63%|████████▏    | 75/119 [00:11<00:06,  6.59it/s]Epoch: 3, train for the 47-th batch, train loss: 0.647031843662262:  30%|████▎         | 46/151 [00:07<00:19,  5.50it/s]Epoch: 1, train for the 290-th batch, train loss: 0.44636276364326477:  76%|███████▌  | 290/383 [01:24<00:33,  2.81it/s]Epoch: 3, train for the 47-th batch, train loss: 0.647031843662262:  31%|████▎         | 47/151 [00:07<00:23,  4.46it/s]Epoch: 4, train for the 76-th batch, train loss: 0.4626818001270294:  63%|████████▏    | 75/119 [00:11<00:06,  6.59it/s]Epoch: 2, train for the 15-th batch, train loss: 0.7132546901702881:   6%|▊            | 14/237 [00:02<00:33,  6.67it/s]Epoch: 4, train for the 76-th batch, train loss: 0.4626818001270294:  64%|████████▎    | 76/119 [00:11<00:06,  7.14it/s]Epoch: 2, train for the 15-th batch, train loss: 0.7132546901702881:   6%|▊            | 15/237 [00:02<00:31,  7.02it/s]Epoch: 2, train for the 155-th batch, train loss: 0.3945944607257843:  64%|███████    | 154/241 [00:23<00:14,  5.83it/s]Epoch: 2, train for the 155-th batch, train loss: 0.3945944607257843:  64%|███████    | 155/241 [00:23<00:13,  6.19it/s]Epoch: 3, train for the 48-th batch, train loss: 0.6208099722862244:  31%|████         | 47/151 [00:07<00:23,  4.46it/s]Epoch: 3, train for the 48-th batch, train loss: 0.6208099722862244:  32%|████▏        | 48/151 [00:07<00:21,  4.80it/s]evaluate for the 29-th batch, evaluate loss: 0.4715721011161804:  74%|█████████████▎    | 28/38 [00:02<00:00, 13.66it/s]Epoch: 4, train for the 77-th batch, train loss: 0.4339278042316437:  64%|████████▎    | 76/119 [00:11<00:06,  7.14it/s]Epoch: 4, train for the 77-th batch, train loss: 0.4339278042316437:  65%|████████▍    | 77/119 [00:11<00:05,  7.02it/s]evaluate for the 30-th batch, evaluate loss: 0.49720999598503113:  74%|████████████▌    | 28/38 [00:02<00:00, 13.66it/s]evaluate for the 30-th batch, evaluate loss: 0.49720999598503113:  79%|█████████████▍   | 30/38 [00:02<00:00,  9.84it/s]Epoch: 2, train for the 156-th batch, train loss: 0.27312108874320984:  64%|██████▍   | 155/241 [00:23<00:13,  6.19it/s]Epoch: 2, train for the 16-th batch, train loss: 0.6354606747627258:   6%|▊            | 15/237 [00:02<00:31,  7.02it/s]Epoch: 2, train for the 16-th batch, train loss: 0.6354606747627258:   7%|▉            | 16/237 [00:02<00:33,  6.53it/s]Epoch: 2, train for the 156-th batch, train loss: 0.27312108874320984:  65%|██████▍   | 156/241 [00:23<00:13,  6.15it/s]evaluate for the 31-th batch, evaluate loss: 0.47432997822761536:  79%|█████████████▍   | 30/38 [00:02<00:00,  9.84it/s]Epoch: 3, train for the 49-th batch, train loss: 0.6490702033042908:  32%|████▏        | 48/151 [00:07<00:21,  4.80it/s]Epoch: 1, train for the 291-th batch, train loss: 0.35345977544784546:  76%|███████▌  | 290/383 [01:24<00:33,  2.81it/s]Epoch: 3, train for the 49-th batch, train loss: 0.6490702033042908:  32%|████▏        | 49/151 [00:07<00:19,  5.33it/s]Epoch: 4, train for the 78-th batch, train loss: 0.5030044317245483:  65%|████████▍    | 77/119 [00:11<00:05,  7.02it/s]evaluate for the 32-th batch, evaluate loss: 0.4325360059738159:  79%|██████████████▏   | 30/38 [00:02<00:00,  9.84it/s]evaluate for the 32-th batch, evaluate loss: 0.4325360059738159:  84%|███████████████▏  | 32/38 [00:02<00:00, 10.99it/s]Epoch: 4, train for the 78-th batch, train loss: 0.5030044317245483:  66%|████████▌    | 78/119 [00:11<00:05,  7.07it/s]Epoch: 1, train for the 291-th batch, train loss: 0.35345977544784546:  76%|███████▌  | 291/383 [01:24<00:33,  2.77it/s]Epoch: 2, train for the 17-th batch, train loss: 0.6183775663375854:   7%|▉            | 16/237 [00:02<00:33,  6.53it/s]Epoch: 2, train for the 17-th batch, train loss: 0.6183775663375854:   7%|▉            | 17/237 [00:02<00:32,  6.86it/s]Epoch: 3, train for the 50-th batch, train loss: 0.6349306106567383:  32%|████▏        | 49/151 [00:08<00:19,  5.33it/s]Epoch: 3, train for the 50-th batch, train loss: 0.6349306106567383:  33%|████▎        | 50/151 [00:08<00:17,  5.81it/s]Epoch: 4, train for the 79-th batch, train loss: 0.45043912529945374:  66%|███████▊    | 78/119 [00:11<00:05,  7.07it/s]Epoch: 4, train for the 79-th batch, train loss: 0.45043912529945374:  66%|███████▉    | 79/119 [00:11<00:05,  7.56it/s]Epoch: 2, train for the 157-th batch, train loss: 0.25593090057373047:  65%|██████▍   | 156/241 [00:24<00:13,  6.15it/s]evaluate for the 33-th batch, evaluate loss: 0.46912384033203125:  84%|██████████████▎  | 32/38 [00:02<00:00, 10.99it/s]Epoch: 2, train for the 157-th batch, train loss: 0.25593090057373047:  65%|██████▌   | 157/241 [00:24<00:16,  5.00it/s]Epoch: 3, train for the 51-th batch, train loss: 0.6422373056411743:  33%|████▎        | 50/151 [00:08<00:17,  5.81it/s]Epoch: 3, train for the 51-th batch, train loss: 0.6422373056411743:  34%|████▍        | 51/151 [00:08<00:16,  6.19it/s]Epoch: 4, train for the 80-th batch, train loss: 0.4945410490036011:  66%|████████▋    | 79/119 [00:11<00:05,  7.56it/s]Epoch: 4, train for the 80-th batch, train loss: 0.4945410490036011:  67%|████████▋    | 80/119 [00:11<00:05,  7.75it/s]evaluate for the 34-th batch, evaluate loss: 0.4958138167858124:  84%|███████████████▏  | 32/38 [00:02<00:00, 10.99it/s]evaluate for the 34-th batch, evaluate loss: 0.4958138167858124:  89%|████████████████  | 34/38 [00:02<00:00, 10.09it/s]Epoch: 1, train for the 292-th batch, train loss: 0.47808825969696045:  76%|███████▌  | 291/383 [01:25<00:33,  2.77it/s]Epoch: 2, train for the 18-th batch, train loss: 0.6362608671188354:   7%|▉            | 17/237 [00:02<00:32,  6.86it/s]Epoch: 2, train for the 18-th batch, train loss: 0.6362608671188354:   8%|▉            | 18/237 [00:02<00:40,  5.42it/s]evaluate for the 35-th batch, evaluate loss: 0.4856780767440796:  89%|████████████████  | 34/38 [00:02<00:00, 10.09it/s]Epoch: 2, train for the 158-th batch, train loss: 0.28664472699165344:  65%|██████▌   | 157/241 [00:24<00:16,  5.00it/s]Epoch: 2, train for the 158-th batch, train loss: 0.28664472699165344:  66%|██████▌   | 158/241 [00:24<00:14,  5.55it/s]Epoch: 1, train for the 292-th batch, train loss: 0.47808825969696045:  76%|███████▌  | 292/383 [01:25<00:31,  2.87it/s]Epoch: 3, train for the 52-th batch, train loss: 0.6465492248535156:  34%|████▍        | 51/151 [00:08<00:16,  6.19it/s]Epoch: 3, train for the 52-th batch, train loss: 0.6465492248535156:  34%|████▍        | 52/151 [00:08<00:15,  6.32it/s]Epoch: 4, train for the 81-th batch, train loss: 0.4297907054424286:  67%|████████▋    | 80/119 [00:11<00:05,  7.75it/s]evaluate for the 36-th batch, evaluate loss: 0.4984147548675537:  89%|████████████████  | 34/38 [00:02<00:00, 10.09it/s]evaluate for the 36-th batch, evaluate loss: 0.4984147548675537:  95%|█████████████████ | 36/38 [00:02<00:00, 10.95it/s]Epoch: 4, train for the 81-th batch, train loss: 0.4297907054424286:  68%|████████▊    | 81/119 [00:11<00:05,  7.35it/s]Epoch: 2, train for the 19-th batch, train loss: 0.6161009073257446:   8%|▉            | 18/237 [00:02<00:40,  5.42it/s]Epoch: 2, train for the 19-th batch, train loss: 0.6161009073257446:   8%|█            | 19/237 [00:02<00:37,  5.88it/s]evaluate for the 37-th batch, evaluate loss: 0.42840656638145447:  95%|████████████████ | 36/38 [00:02<00:00, 10.95it/s]Epoch: 3, train for the 53-th batch, train loss: 0.6454545259475708:  34%|████▍        | 52/151 [00:08<00:15,  6.32it/s]Epoch: 4, train for the 82-th batch, train loss: 0.476284921169281:  68%|█████████▌    | 81/119 [00:11<00:05,  7.35it/s]Epoch: 4, train for the 82-th batch, train loss: 0.476284921169281:  69%|█████████▋    | 82/119 [00:12<00:05,  7.29it/s]Epoch: 3, train for the 53-th batch, train loss: 0.6454545259475708:  35%|████▌        | 53/151 [00:08<00:15,  6.37it/s]evaluate for the 38-th batch, evaluate loss: 0.4625215232372284:  95%|█████████████████ | 36/38 [00:02<00:00, 10.95it/s]evaluate for the 38-th batch, evaluate loss: 0.4625215232372284: 100%|██████████████████| 38/38 [00:02<00:00, 10.95it/s]evaluate for the 38-th batch, evaluate loss: 0.4625215232372284: 100%|██████████████████| 38/38 [00:02<00:00, 12.87it/s]
Epoch: 1, train for the 293-th batch, train loss: 0.45766347646713257:  76%|███████▌  | 292/383 [01:25<00:31,  2.87it/s]Epoch: 2, train for the 20-th batch, train loss: 0.6606379747390747:   8%|█            | 19/237 [00:03<00:37,  5.88it/s]Epoch: 2, train for the 20-th batch, train loss: 0.6606379747390747:   8%|█            | 20/237 [00:03<00:38,  5.68it/s]Epoch: 3, train for the 54-th batch, train loss: 0.6090536117553711:  35%|████▌        | 53/151 [00:08<00:15,  6.37it/s]Epoch: 3, train for the 54-th batch, train loss: 0.6090536117553711:  36%|████▋        | 54/151 [00:08<00:14,  6.88it/s]Epoch: 4, train for the 83-th batch, train loss: 0.48881906270980835:  69%|████████▎   | 82/119 [00:12<00:05,  7.29it/s]Epoch: 1, train for the 293-th batch, train loss: 0.45766347646713257:  77%|███████▋  | 293/383 [01:25<00:31,  2.89it/s]Epoch: 4, train for the 83-th batch, train loss: 0.48881906270980835:  70%|████████▎   | 83/119 [00:12<00:05,  7.10it/s]Epoch: 2, train for the 21-th batch, train loss: 0.6154043674468994:   8%|█            | 20/237 [00:03<00:38,  5.68it/s]Epoch: 4, train for the 84-th batch, train loss: 0.4810035526752472:  70%|█████████    | 83/119 [00:12<00:05,  7.10it/s]Epoch: 2, train for the 21-th batch, train loss: 0.6154043674468994:   9%|█▏           | 21/237 [00:03<00:36,  5.93it/s]Epoch: 4, train for the 84-th batch, train loss: 0.4810035526752472:  71%|█████████▏   | 84/119 [00:12<00:04,  7.67it/s]Epoch: 2, train for the 159-th batch, train loss: 0.2520017921924591:  66%|███████▏   | 158/241 [00:24<00:14,  5.55it/s]Epoch: 3, train for the 55-th batch, train loss: 0.615001916885376:  36%|█████         | 54/151 [00:08<00:14,  6.88it/s]Epoch: 3, train for the 55-th batch, train loss: 0.615001916885376:  36%|█████         | 55/151 [00:08<00:14,  6.68it/s]Epoch: 2, train for the 159-th batch, train loss: 0.2520017921924591:  66%|███████▎   | 159/241 [00:24<00:22,  3.69it/s]Epoch: 1, train for the 294-th batch, train loss: 0.2992326319217682:  77%|████████▍  | 293/383 [01:25<00:31,  2.89it/s]  0%|                                                                                            | 0/20 [00:00<?, ?it/s]Epoch: 2, train for the 22-th batch, train loss: 0.6097609400749207:   9%|█▏           | 21/237 [00:03<00:36,  5.93it/s]Epoch: 2, train for the 22-th batch, train loss: 0.6097609400749207:   9%|█▏           | 22/237 [00:03<00:35,  6.14it/s]Epoch: 1, train for the 294-th batch, train loss: 0.2992326319217682:  77%|████████▍  | 294/383 [01:25<00:28,  3.10it/s]Epoch: 3, train for the 56-th batch, train loss: 0.5557880401611328:  36%|████▋        | 55/151 [00:08<00:14,  6.68it/s]Epoch: 4, train for the 85-th batch, train loss: 0.4638270139694214:  71%|█████████▏   | 84/119 [00:12<00:04,  7.67it/s]Epoch: 4, train for the 85-th batch, train loss: 0.4638270139694214:  71%|█████████▎   | 85/119 [00:12<00:04,  7.08it/s]Epoch: 3, train for the 56-th batch, train loss: 0.5557880401611328:  37%|████▊        | 56/151 [00:08<00:14,  6.74it/s]evaluate for the 1-th batch, evaluate loss: 0.7581588625907898:   0%|                            | 0/20 [00:00<?, ?it/s]Epoch: 2, train for the 160-th batch, train loss: 0.5478062033653259:  66%|███████▎   | 159/241 [00:25<00:22,  3.69it/s]Epoch: 2, train for the 160-th batch, train loss: 0.5478062033653259:  66%|███████▎   | 160/241 [00:25<00:19,  4.08it/s]evaluate for the 2-th batch, evaluate loss: 0.7781121730804443:   0%|                            | 0/20 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.7781121730804443:  10%|██                  | 2/20 [00:00<00:01, 13.51it/s]Epoch: 2, train for the 23-th batch, train loss: 0.5998514890670776:   9%|█▏           | 22/237 [00:03<00:35,  6.14it/s]Epoch: 2, train for the 23-th batch, train loss: 0.5998514890670776:  10%|█▎           | 23/237 [00:03<00:34,  6.25it/s]Epoch: 4, train for the 86-th batch, train loss: 0.4854070246219635:  71%|█████████▎   | 85/119 [00:12<00:04,  7.08it/s]Epoch: 3, train for the 57-th batch, train loss: 0.5920371413230896:  37%|████▊        | 56/151 [00:09<00:14,  6.74it/s]Epoch: 4, train for the 86-th batch, train loss: 0.4854070246219635:  72%|█████████▍   | 86/119 [00:12<00:04,  6.77it/s]evaluate for the 3-th batch, evaluate loss: 0.6422799825668335:  10%|██                  | 2/20 [00:00<00:01, 13.51it/s]Epoch: 3, train for the 57-th batch, train loss: 0.5920371413230896:  38%|████▉        | 57/151 [00:09<00:14,  6.54it/s]Epoch: 2, train for the 161-th batch, train loss: 0.3318207859992981:  66%|███████▎   | 160/241 [00:25<00:19,  4.08it/s]evaluate for the 4-th batch, evaluate loss: 0.6661568880081177:  10%|██                  | 2/20 [00:00<00:01, 13.51it/s]evaluate for the 4-th batch, evaluate loss: 0.6661568880081177:  20%|████                | 4/20 [00:00<00:01, 12.37it/s]Epoch: 2, train for the 161-th batch, train loss: 0.3318207859992981:  67%|███████▎   | 161/241 [00:25<00:19,  4.17it/s]Epoch: 2, train for the 24-th batch, train loss: 0.5673229694366455:  10%|█▎           | 23/237 [00:03<00:34,  6.25it/s]Epoch: 1, train for the 295-th batch, train loss: 0.3560599088668823:  77%|████████▍  | 294/383 [01:26<00:28,  3.10it/s]Epoch: 2, train for the 24-th batch, train loss: 0.5673229694366455:  10%|█▎           | 24/237 [00:03<00:34,  6.12it/s]Epoch: 3, train for the 58-th batch, train loss: 0.5848140716552734:  38%|████▉        | 57/151 [00:09<00:14,  6.54it/s]Epoch: 3, train for the 58-th batch, train loss: 0.5848140716552734:  38%|████▉        | 58/151 [00:09<00:14,  6.48it/s]Epoch: 4, train for the 87-th batch, train loss: 0.4923454821109772:  72%|█████████▍   | 86/119 [00:12<00:04,  6.77it/s]evaluate for the 5-th batch, evaluate loss: 0.6778591275215149:  20%|████                | 4/20 [00:00<00:01, 12.37it/s]Epoch: 4, train for the 87-th batch, train loss: 0.4923454821109772:  73%|█████████▌   | 87/119 [00:12<00:05,  6.22it/s]Epoch: 1, train for the 295-th batch, train loss: 0.3560599088668823:  77%|████████▍  | 295/383 [01:26<00:30,  2.92it/s]Epoch: 2, train for the 162-th batch, train loss: 0.2088650017976761:  67%|███████▎   | 161/241 [00:25<00:19,  4.17it/s]Epoch: 2, train for the 162-th batch, train loss: 0.2088650017976761:  67%|███████▍   | 162/241 [00:25<00:16,  4.75it/s]Epoch: 2, train for the 25-th batch, train loss: 0.580605685710907:  10%|█▍            | 24/237 [00:03<00:34,  6.12it/s]evaluate for the 6-th batch, evaluate loss: 0.7435463070869446:  20%|████                | 4/20 [00:00<00:01, 12.37it/s]evaluate for the 6-th batch, evaluate loss: 0.7435463070869446:  30%|██████              | 6/20 [00:00<00:01, 12.09it/s]Epoch: 2, train for the 25-th batch, train loss: 0.580605685710907:  11%|█▍            | 25/237 [00:03<00:32,  6.55it/s]Epoch: 3, train for the 59-th batch, train loss: 0.5694531798362732:  38%|████▉        | 58/151 [00:09<00:14,  6.48it/s]Epoch: 3, train for the 59-th batch, train loss: 0.5694531798362732:  39%|█████        | 59/151 [00:09<00:14,  6.50it/s]Epoch: 4, train for the 88-th batch, train loss: 0.5056887865066528:  73%|█████████▌   | 87/119 [00:12<00:05,  6.22it/s]Epoch: 4, train for the 88-th batch, train loss: 0.5056887865066528:  74%|█████████▌   | 88/119 [00:12<00:04,  6.38it/s]evaluate for the 7-th batch, evaluate loss: 0.7586250901222229:  30%|██████              | 6/20 [00:00<00:01, 12.09it/s]Epoch: 2, train for the 163-th batch, train loss: 0.17277498543262482:  67%|██████▋   | 162/241 [00:25<00:16,  4.75it/s]Epoch: 2, train for the 26-th batch, train loss: 0.5587019920349121:  11%|█▎           | 25/237 [00:03<00:32,  6.55it/s]Epoch: 2, train for the 163-th batch, train loss: 0.17277498543262482:  68%|██████▊   | 163/241 [00:25<00:15,  5.09it/s]evaluate for the 8-th batch, evaluate loss: 0.6859025359153748:  30%|██████              | 6/20 [00:00<00:01, 12.09it/s]evaluate for the 8-th batch, evaluate loss: 0.6859025359153748:  40%|████████            | 8/20 [00:00<00:00, 12.37it/s]Epoch: 2, train for the 26-th batch, train loss: 0.5587019920349121:  11%|█▍           | 26/237 [00:03<00:32,  6.48it/s]Epoch: 3, train for the 60-th batch, train loss: 0.5717186331748962:  39%|█████        | 59/151 [00:09<00:14,  6.50it/s]Epoch: 3, train for the 60-th batch, train loss: 0.5717186331748962:  40%|█████▏       | 60/151 [00:09<00:14,  6.42it/s]Epoch: 1, train for the 296-th batch, train loss: 0.2984262704849243:  77%|████████▍  | 295/383 [01:26<00:30,  2.92it/s]Epoch: 4, train for the 89-th batch, train loss: 0.493791788816452:  74%|██████████▎   | 88/119 [00:13<00:04,  6.38it/s]evaluate for the 9-th batch, evaluate loss: 0.6441683769226074:  40%|████████            | 8/20 [00:00<00:00, 12.37it/s]Epoch: 4, train for the 89-th batch, train loss: 0.493791788816452:  75%|██████████▍   | 89/119 [00:13<00:04,  6.27it/s]Epoch: 2, train for the 164-th batch, train loss: 0.16852961480617523:  68%|██████▊   | 163/241 [00:25<00:15,  5.09it/s]Epoch: 1, train for the 296-th batch, train loss: 0.2984262704849243:  77%|████████▌  | 296/383 [01:26<00:29,  2.90it/s]evaluate for the 10-th batch, evaluate loss: 0.6397775411605835:  40%|███████▌           | 8/20 [00:00<00:00, 12.37it/s]evaluate for the 10-th batch, evaluate loss: 0.6397775411605835:  50%|█████████         | 10/20 [00:00<00:00, 12.87it/s]Epoch: 2, train for the 164-th batch, train loss: 0.16852961480617523:  68%|██████▊   | 164/241 [00:25<00:14,  5.46it/s]Epoch: 2, train for the 27-th batch, train loss: 0.5826640129089355:  11%|█▍           | 26/237 [00:04<00:32,  6.48it/s]Epoch: 2, train for the 27-th batch, train loss: 0.5826640129089355:  11%|█▍           | 27/237 [00:04<00:32,  6.46it/s]Epoch: 3, train for the 61-th batch, train loss: 0.5532821416854858:  40%|█████▏       | 60/151 [00:09<00:14,  6.42it/s]Epoch: 3, train for the 61-th batch, train loss: 0.5532821416854858:  40%|█████▎       | 61/151 [00:09<00:13,  6.81it/s]Epoch: 4, train for the 90-th batch, train loss: 0.4855045974254608:  75%|█████████▋   | 89/119 [00:13<00:04,  6.27it/s]Epoch: 4, train for the 90-th batch, train loss: 0.4855045974254608:  76%|█████████▊   | 90/119 [00:13<00:04,  6.55it/s]evaluate for the 11-th batch, evaluate loss: 0.6386928558349609:  50%|█████████         | 10/20 [00:00<00:00, 12.87it/s]Epoch: 2, train for the 165-th batch, train loss: 0.26539453864097595:  68%|██████▊   | 164/241 [00:25<00:14,  5.46it/s]evaluate for the 12-th batch, evaluate loss: 0.6946438550949097:  50%|█████████         | 10/20 [00:00<00:00, 12.87it/s]evaluate for the 12-th batch, evaluate loss: 0.6946438550949097:  60%|██████████▊       | 12/20 [00:00<00:00, 12.67it/s]Epoch: 2, train for the 28-th batch, train loss: 0.6547105312347412:  11%|█▍           | 27/237 [00:04<00:32,  6.46it/s]Epoch: 2, train for the 165-th batch, train loss: 0.26539453864097595:  68%|██████▊   | 165/241 [00:25<00:13,  5.60it/s]Epoch: 2, train for the 28-th batch, train loss: 0.6547105312347412:  12%|█▌           | 28/237 [00:04<00:33,  6.29it/s]Epoch: 3, train for the 62-th batch, train loss: 0.6061137914657593:  40%|█████▎       | 61/151 [00:09<00:13,  6.81it/s]Epoch: 3, train for the 62-th batch, train loss: 0.6061137914657593:  41%|█████▎       | 62/151 [00:09<00:13,  6.43it/s]Epoch: 4, train for the 91-th batch, train loss: 0.43320024013519287:  76%|█████████   | 90/119 [00:13<00:04,  6.55it/s]Epoch: 4, train for the 91-th batch, train loss: 0.43320024013519287:  76%|█████████▏  | 91/119 [00:13<00:04,  6.42it/s]evaluate for the 13-th batch, evaluate loss: 0.7128588557243347:  60%|██████████▊       | 12/20 [00:01<00:00, 12.67it/s]Epoch: 2, train for the 166-th batch, train loss: 0.2523685097694397:  68%|███████▌   | 165/241 [00:26<00:13,  5.60it/s]evaluate for the 14-th batch, evaluate loss: 0.7255194187164307:  60%|██████████▊       | 12/20 [00:01<00:00, 12.67it/s]evaluate for the 14-th batch, evaluate loss: 0.7255194187164307:  70%|████████████▌     | 14/20 [00:01<00:00, 13.21it/s]Epoch: 2, train for the 29-th batch, train loss: 0.5868431925773621:  12%|█▌           | 28/237 [00:04<00:33,  6.29it/s]Epoch: 2, train for the 29-th batch, train loss: 0.5868431925773621:  12%|█▌           | 29/237 [00:04<00:31,  6.62it/s]Epoch: 2, train for the 166-th batch, train loss: 0.2523685097694397:  69%|███████▌   | 166/241 [00:26<00:12,  5.92it/s]Epoch: 3, train for the 63-th batch, train loss: 0.5759111046791077:  41%|█████▎       | 62/151 [00:09<00:13,  6.43it/s]Epoch: 3, train for the 63-th batch, train loss: 0.5759111046791077:  42%|█████▍       | 63/151 [00:09<00:13,  6.51it/s]Epoch: 4, train for the 92-th batch, train loss: 0.4651455283164978:  76%|█████████▉   | 91/119 [00:13<00:04,  6.42it/s]Epoch: 4, train for the 92-th batch, train loss: 0.4651455283164978:  77%|██████████   | 92/119 [00:13<00:04,  6.56it/s]evaluate for the 15-th batch, evaluate loss: 0.7215598821640015:  70%|████████████▌     | 14/20 [00:01<00:00, 13.21it/s]Epoch: 2, train for the 167-th batch, train loss: 0.17203623056411743:  69%|██████▉   | 166/241 [00:26<00:12,  5.92it/s]Epoch: 2, train for the 30-th batch, train loss: 0.6759980916976929:  12%|█▌           | 29/237 [00:04<00:31,  6.62it/s]evaluate for the 16-th batch, evaluate loss: 0.6691808104515076:  70%|████████████▌     | 14/20 [00:01<00:00, 13.21it/s]evaluate for the 16-th batch, evaluate loss: 0.6691808104515076:  80%|██████████████▍   | 16/20 [00:01<00:00, 13.02it/s]Epoch: 2, train for the 30-th batch, train loss: 0.6759980916976929:  13%|█▋           | 30/237 [00:04<00:30,  6.73it/s]Epoch: 2, train for the 167-th batch, train loss: 0.17203623056411743:  69%|██████▉   | 167/241 [00:26<00:12,  6.04it/s]Epoch: 3, train for the 64-th batch, train loss: 0.5611578822135925:  42%|█████▍       | 63/151 [00:10<00:13,  6.51it/s]Epoch: 3, train for the 64-th batch, train loss: 0.5611578822135925:  42%|█████▌       | 64/151 [00:10<00:13,  6.66it/s]Epoch: 4, train for the 93-th batch, train loss: 0.420205682516098:  77%|██████████▊   | 92/119 [00:13<00:04,  6.56it/s]Epoch: 1, train for the 297-th batch, train loss: 0.4975837469100952:  77%|████████▌  | 296/383 [01:27<00:29,  2.90it/s]Epoch: 4, train for the 93-th batch, train loss: 0.420205682516098:  78%|██████████▉   | 93/119 [00:13<00:03,  6.63it/s]evaluate for the 17-th batch, evaluate loss: 0.7158437967300415:  80%|██████████████▍   | 16/20 [00:01<00:00, 13.02it/s]Epoch: 2, train for the 31-th batch, train loss: 0.5825467109680176:  13%|█▋           | 30/237 [00:04<00:30,  6.73it/s]Epoch: 1, train for the 297-th batch, train loss: 0.4975837469100952:  78%|████████▌  | 297/383 [01:27<00:36,  2.37it/s]evaluate for the 18-th batch, evaluate loss: 0.6812471151351929:  80%|██████████████▍   | 16/20 [00:01<00:00, 13.02it/s]evaluate for the 18-th batch, evaluate loss: 0.6812471151351929:  90%|████████████████▏ | 18/20 [00:01<00:00, 13.09it/s]Epoch: 2, train for the 31-th batch, train loss: 0.5825467109680176:  13%|█▋           | 31/237 [00:04<00:31,  6.61it/s]Epoch: 2, train for the 168-th batch, train loss: 0.14424780011177063:  69%|██████▉   | 167/241 [00:26<00:12,  6.04it/s]Epoch: 3, train for the 65-th batch, train loss: 0.5248706340789795:  42%|█████▌       | 64/151 [00:10<00:13,  6.66it/s]Epoch: 2, train for the 168-th batch, train loss: 0.14424780011177063:  70%|██████▉   | 168/241 [00:26<00:12,  5.72it/s]Epoch: 3, train for the 65-th batch, train loss: 0.5248706340789795:  43%|█████▌       | 65/151 [00:10<00:13,  6.27it/s]Epoch: 4, train for the 94-th batch, train loss: 0.4531528353691101:  78%|██████████▏  | 93/119 [00:13<00:03,  6.63it/s]evaluate for the 19-th batch, evaluate loss: 0.7402421236038208:  90%|████████████████▏ | 18/20 [00:01<00:00, 13.09it/s]Epoch: 4, train for the 94-th batch, train loss: 0.4531528353691101:  79%|██████████▎  | 94/119 [00:13<00:03,  6.27it/s]Epoch: 2, train for the 32-th batch, train loss: 0.6009909510612488:  13%|█▋           | 31/237 [00:04<00:31,  6.61it/s]Epoch: 2, train for the 32-th batch, train loss: 0.6009909510612488:  14%|█▊           | 32/237 [00:04<00:30,  6.72it/s]evaluate for the 20-th batch, evaluate loss: 0.7217866778373718:  90%|████████████████▏ | 18/20 [00:01<00:00, 13.09it/s]evaluate for the 20-th batch, evaluate loss: 0.7217866778373718: 100%|██████████████████| 20/20 [00:01<00:00, 12.66it/s]evaluate for the 20-th batch, evaluate loss: 0.7217866778373718: 100%|██████████████████| 20/20 [00:01<00:00, 12.74it/s]
Epoch: 3, train for the 66-th batch, train loss: 0.5623283982276917:  43%|█████▌       | 65/151 [00:10<00:13,  6.27it/s]Epoch: 2, train for the 169-th batch, train loss: 0.5854713916778564:  70%|███████▋   | 168/241 [00:26<00:12,  5.72it/s]Epoch: 3, train for the 66-th batch, train loss: 0.5623283982276917:  44%|█████▋       | 66/151 [00:10<00:14,  5.92it/s]Epoch: 1, train for the 298-th batch, train loss: 0.4764516353607178:  78%|████████▌  | 297/383 [01:27<00:36,  2.37it/s]Epoch: 4, train for the 95-th batch, train loss: 0.40194475650787354:  79%|█████████▍  | 94/119 [00:14<00:03,  6.27it/s]Epoch: 4, train for the 95-th batch, train loss: 0.40194475650787354:  80%|█████████▌  | 95/119 [00:14<00:03,  6.07it/s]Epoch: 2, train for the 169-th batch, train loss: 0.5854713916778564:  70%|███████▋   | 169/241 [00:26<00:13,  5.26it/s]Epoch: 2, train for the 33-th batch, train loss: 0.5893802642822266:  14%|█▊           | 32/237 [00:04<00:30,  6.72it/s]Epoch: 2, train for the 33-th batch, train loss: 0.5893802642822266:  14%|█▊           | 33/237 [00:05<00:31,  6.56it/s]Epoch: 1, train for the 298-th batch, train loss: 0.4764516353607178:  78%|████████▌  | 298/383 [01:27<00:34,  2.49it/s]INFO:root:Epoch: 3, learning rate: 0.0001, train loss: 0.5422
INFO:root:train average_precision, 0.8312
INFO:root:train roc_auc, 0.8162
INFO:root:validate loss: 0.4735
INFO:root:validate average_precision, 0.8610
INFO:root:validate roc_auc, 0.8538
INFO:root:new node validate loss: 0.7008
INFO:root:new node validate first_1_average_precision, 0.5696
INFO:root:new node validate first_1_roc_auc, 0.5172
INFO:root:new node validate first_3_average_precision, 0.6080
INFO:root:new node validate first_3_roc_auc, 0.5694
INFO:root:new node validate first_10_average_precision, 0.6326
INFO:root:new node validate first_10_roc_auc, 0.6192
INFO:root:new node validate average_precision, 0.6784
INFO:root:new node validate roc_auc, 0.6733
INFO:root:save model ./saved_models/TGN/ia-escorts-dynamic/TGN_seed0_tgn-ia-escorts-dynamic-lincorrect-time-linear/TGN_seed0_tgn-ia-escorts-dynamic-lincorrect-time-linear.pkl
Epoch: 4, train for the 96-th batch, train loss: 0.4468814432621002:  80%|██████████▍  | 95/119 [00:14<00:03,  6.07it/s]Epoch: 4, train for the 96-th batch, train loss: 0.4468814432621002:  81%|██████████▍  | 96/119 [00:14<00:03,  6.43it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5515627264976501:  14%|█▊           | 33/237 [00:05<00:31,  6.56it/s]Epoch: 3, train for the 67-th batch, train loss: 0.5661746859550476:  44%|█████▋       | 66/151 [00:10<00:14,  5.92it/s]Epoch: 2, train for the 34-th batch, train loss: 0.5515627264976501:  14%|█▊           | 34/237 [00:05<00:29,  6.89it/s]Epoch: 3, train for the 67-th batch, train loss: 0.5661746859550476:  44%|█████▊       | 67/151 [00:10<00:14,  5.74it/s]Epoch: 2, train for the 170-th batch, train loss: 0.6993421912193298:  70%|███████▋   | 169/241 [00:26<00:13,  5.26it/s]Epoch: 2, train for the 170-th batch, train loss: 0.6993421912193298:  71%|███████▊   | 170/241 [00:26<00:13,  5.24it/s]Epoch: 4, train for the 97-th batch, train loss: 0.4627925753593445:  81%|██████████▍  | 96/119 [00:14<00:03,  6.43it/s]Epoch: 4, train for the 97-th batch, train loss: 0.4627925753593445:  82%|██████████▌  | 97/119 [00:14<00:03,  6.92it/s]Epoch: 3, train for the 68-th batch, train loss: 0.5524830222129822:  44%|█████▊       | 67/151 [00:10<00:14,  5.74it/s]Epoch: 2, train for the 35-th batch, train loss: 0.542073667049408:  14%|██            | 34/237 [00:05<00:29,  6.89it/s]Epoch: 3, train for the 68-th batch, train loss: 0.5524830222129822:  45%|█████▊       | 68/151 [00:10<00:13,  5.99it/s]Epoch: 2, train for the 35-th batch, train loss: 0.542073667049408:  15%|██            | 35/237 [00:05<00:30,  6.58it/s]Epoch: 4, train for the 98-th batch, train loss: 0.43851760029792786:  82%|█████████▊  | 97/119 [00:14<00:03,  6.92it/s]Epoch: 2, train for the 171-th batch, train loss: 0.5910019278526306:  71%|███████▊   | 170/241 [00:26<00:13,  5.24it/s]Epoch: 2, train for the 171-th batch, train loss: 0.5910019278526306:  71%|███████▊   | 171/241 [00:27<00:12,  5.49it/s]Epoch: 3, train for the 69-th batch, train loss: 0.5537518262863159:  45%|█████▊       | 68/151 [00:10<00:13,  5.99it/s]Epoch: 4, train for the 99-th batch, train loss: 0.43999403715133667:  82%|█████████▊  | 97/119 [00:14<00:03,  6.92it/s]Epoch: 4, train for the 99-th batch, train loss: 0.43999403715133667:  83%|█████████▉  | 99/119 [00:14<00:02,  8.02it/s]Epoch: 3, train for the 69-th batch, train loss: 0.5537518262863159:  46%|█████▉       | 69/151 [00:10<00:13,  6.30it/s]Epoch: 2, train for the 36-th batch, train loss: 0.5738164186477661:  15%|█▉           | 35/237 [00:05<00:30,  6.58it/s]Epoch: 1, train for the 299-th batch, train loss: 0.5232885479927063:  78%|████████▌  | 298/383 [01:27<00:34,  2.49it/s]Epoch: 2, train for the 172-th batch, train loss: 0.4963575601577759:  71%|███████▊   | 171/241 [00:27<00:12,  5.49it/s]Epoch: 2, train for the 36-th batch, train loss: 0.5738164186477661:  15%|█▉           | 36/237 [00:05<00:32,  6.15it/s]Epoch: 2, train for the 172-th batch, train loss: 0.4963575601577759:  71%|███████▊   | 172/241 [00:27<00:12,  5.67it/s]Epoch: 4, train for the 100-th batch, train loss: 0.47270575165748596:  83%|█████████▏ | 99/119 [00:14<00:02,  8.02it/s]Epoch: 1, train for the 299-th batch, train loss: 0.5232885479927063:  78%|████████▌  | 299/383 [01:28<00:36,  2.32it/s]Epoch: 3, train for the 70-th batch, train loss: 0.5645229816436768:  46%|█████▉       | 69/151 [00:11<00:13,  6.30it/s]Epoch: 3, train for the 70-th batch, train loss: 0.5645229816436768:  46%|██████       | 70/151 [00:11<00:12,  6.41it/s]Epoch: 2, train for the 37-th batch, train loss: 0.6126217246055603:  15%|█▉           | 36/237 [00:05<00:32,  6.15it/s]Epoch: 2, train for the 173-th batch, train loss: 0.35262271761894226:  71%|███████▏  | 172/241 [00:27<00:12,  5.67it/s]Epoch: 4, train for the 101-th batch, train loss: 0.45593950152397156:  83%|█████████▏ | 99/119 [00:14<00:02,  8.02it/s]Epoch: 2, train for the 37-th batch, train loss: 0.6126217246055603:  16%|██           | 37/237 [00:05<00:31,  6.37it/s]Epoch: 4, train for the 101-th batch, train loss: 0.45593950152397156:  85%|████████▍ | 101/119 [00:14<00:02,  8.46it/s]Epoch: 2, train for the 173-th batch, train loss: 0.35262271761894226:  72%|███████▏  | 173/241 [00:27<00:11,  5.93it/s]Epoch: 3, train for the 71-th batch, train loss: 0.5881144404411316:  46%|██████       | 70/151 [00:11<00:12,  6.41it/s]Epoch: 3, train for the 71-th batch, train loss: 0.5881144404411316:  47%|██████       | 71/151 [00:11<00:12,  6.50it/s]Epoch: 4, train for the 102-th batch, train loss: 0.48316535353660583:  85%|████████▍ | 101/119 [00:14<00:02,  8.46it/s]Epoch: 4, train for the 102-th batch, train loss: 0.48316535353660583:  86%|████████▌ | 102/119 [00:14<00:02,  8.37it/s]Epoch: 2, train for the 174-th batch, train loss: 0.38541921973228455:  72%|███████▏  | 173/241 [00:27<00:11,  5.93it/s]Epoch: 2, train for the 38-th batch, train loss: 0.5634621977806091:  16%|██           | 37/237 [00:05<00:31,  6.37it/s]Epoch: 2, train for the 174-th batch, train loss: 0.38541921973228455:  72%|███████▏  | 174/241 [00:27<00:11,  6.08it/s]Epoch: 2, train for the 38-th batch, train loss: 0.5634621977806091:  16%|██           | 38/237 [00:05<00:32,  6.18it/s]Epoch: 3, train for the 72-th batch, train loss: 0.5101786851882935:  47%|██████       | 71/151 [00:11<00:12,  6.50it/s]Epoch: 4, train for the 103-th batch, train loss: 0.44901564717292786:  86%|████████▌ | 102/119 [00:14<00:02,  8.37it/s]Epoch: 4, train for the 103-th batch, train loss: 0.44901564717292786:  87%|████████▋ | 103/119 [00:14<00:01,  8.63it/s]Epoch: 3, train for the 72-th batch, train loss: 0.5101786851882935:  48%|██████▏      | 72/151 [00:11<00:11,  6.64it/s]Epoch: 2, train for the 39-th batch, train loss: 0.6099120378494263:  16%|██           | 38/237 [00:05<00:32,  6.18it/s]Epoch: 2, train for the 175-th batch, train loss: 0.2742551565170288:  72%|███████▉   | 174/241 [00:27<00:11,  6.08it/s]Epoch: 2, train for the 39-th batch, train loss: 0.6099120378494263:  16%|██▏          | 39/237 [00:05<00:29,  6.64it/s]Epoch: 2, train for the 175-th batch, train loss: 0.2742551565170288:  73%|███████▉   | 175/241 [00:27<00:10,  6.31it/s]Epoch: 4, train for the 104-th batch, train loss: 0.44300684332847595:  87%|████████▋ | 103/119 [00:15<00:01,  8.63it/s]Epoch: 4, train for the 104-th batch, train loss: 0.44300684332847595:  87%|████████▋ | 104/119 [00:15<00:01,  8.71it/s]Epoch: 3, train for the 73-th batch, train loss: 0.5578495264053345:  48%|██████▏      | 72/151 [00:11<00:11,  6.64it/s]Epoch: 3, train for the 73-th batch, train loss: 0.5578495264053345:  48%|██████▎      | 73/151 [00:11<00:11,  6.80it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]Epoch: 1, train for the 300-th batch, train loss: 0.5331469774246216:  78%|████████▌  | 299/383 [01:28<00:36,  2.32it/s]Epoch: 2, train for the 176-th batch, train loss: 0.41409996151924133:  73%|███████▎  | 175/241 [00:27<00:10,  6.31it/s]Epoch: 2, train for the 40-th batch, train loss: 0.5769863724708557:  16%|██▏          | 39/237 [00:06<00:29,  6.64it/s]Epoch: 4, train for the 105-th batch, train loss: 0.4228721559047699:  87%|█████████▌ | 104/119 [00:15<00:01,  8.71it/s]Epoch: 2, train for the 176-th batch, train loss: 0.41409996151924133:  73%|███████▎  | 176/241 [00:27<00:09,  6.56it/s]Epoch: 2, train for the 40-th batch, train loss: 0.5769863724708557:  17%|██▏          | 40/237 [00:06<00:30,  6.49it/s]Epoch: 4, train for the 105-th batch, train loss: 0.4228721559047699:  88%|█████████▋ | 105/119 [00:15<00:01,  8.80it/s]Epoch: 1, train for the 300-th batch, train loss: 0.5331469774246216:  78%|████████▌  | 300/383 [01:28<00:40,  2.06it/s]Epoch: 4, train for the 1-th batch, train loss: 0.7696812152862549:   0%|                       | 0/146 [00:00<?, ?it/s]Epoch: 3, train for the 74-th batch, train loss: 0.499671071767807:  48%|██████▊       | 73/151 [00:11<00:11,  6.80it/s]Epoch: 4, train for the 1-th batch, train loss: 0.7696812152862549:   1%|               | 1/146 [00:00<00:15,  9.15it/s]Epoch: 3, train for the 74-th batch, train loss: 0.499671071767807:  49%|██████▊       | 74/151 [00:11<00:11,  6.57it/s]Epoch: 2, train for the 177-th batch, train loss: 0.42295414209365845:  73%|███████▎  | 176/241 [00:27<00:09,  6.56it/s]Epoch: 2, train for the 41-th batch, train loss: 0.576763927936554:  17%|██▎           | 40/237 [00:06<00:30,  6.49it/s]Epoch: 2, train for the 177-th batch, train loss: 0.42295414209365845:  73%|███████▎  | 177/241 [00:27<00:09,  6.42it/s]Epoch: 2, train for the 41-th batch, train loss: 0.576763927936554:  17%|██▍           | 41/237 [00:06<00:30,  6.37it/s]Epoch: 4, train for the 106-th batch, train loss: 0.43300214409828186:  88%|████████▊ | 105/119 [00:15<00:01,  8.80it/s]Epoch: 4, train for the 2-th batch, train loss: 0.7358599901199341:   1%|               | 1/146 [00:00<00:15,  9.15it/s]Epoch: 4, train for the 2-th batch, train loss: 0.7358599901199341:   1%|▏              | 2/146 [00:00<00:16,  8.67it/s]Epoch: 4, train for the 106-th batch, train loss: 0.43300214409828186:  89%|████████▉ | 106/119 [00:15<00:01,  7.50it/s]Epoch: 3, train for the 75-th batch, train loss: 0.5365890264511108:  49%|██████▎      | 74/151 [00:11<00:11,  6.57it/s]Epoch: 3, train for the 75-th batch, train loss: 0.5365890264511108:  50%|██████▍      | 75/151 [00:11<00:11,  6.64it/s]Epoch: 4, train for the 3-th batch, train loss: 0.7105230689048767:   1%|▏              | 2/146 [00:00<00:16,  8.67it/s]Epoch: 4, train for the 3-th batch, train loss: 0.7105230689048767:   2%|▎              | 3/146 [00:00<00:16,  8.72it/s]Epoch: 2, train for the 178-th batch, train loss: 0.45792898535728455:  73%|███████▎  | 177/241 [00:28<00:09,  6.42it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5971786975860596:  17%|██▏          | 41/237 [00:06<00:30,  6.37it/s]Epoch: 2, train for the 178-th batch, train loss: 0.45792898535728455:  74%|███████▍  | 178/241 [00:28<00:09,  6.39it/s]Epoch: 2, train for the 42-th batch, train loss: 0.5971786975860596:  18%|██▎          | 42/237 [00:06<00:30,  6.37it/s]Epoch: 4, train for the 107-th batch, train loss: 0.4847477972507477:  89%|█████████▊ | 106/119 [00:15<00:01,  7.50it/s]Epoch: 3, train for the 76-th batch, train loss: 0.5648908019065857:  50%|██████▍      | 75/151 [00:11<00:11,  6.64it/s]Epoch: 4, train for the 107-th batch, train loss: 0.4847477972507477:  90%|█████████▉ | 107/119 [00:15<00:01,  7.04it/s]Epoch: 3, train for the 76-th batch, train loss: 0.5648908019065857:  50%|██████▌      | 76/151 [00:11<00:10,  6.93it/s]Epoch: 4, train for the 4-th batch, train loss: 0.6989901661872864:   2%|▎              | 3/146 [00:00<00:16,  8.72it/s]Epoch: 4, train for the 4-th batch, train loss: 0.6989901661872864:   3%|▍              | 4/146 [00:00<00:16,  8.77it/s]Epoch: 1, train for the 301-th batch, train loss: 0.32474344968795776:  78%|███████▊  | 300/383 [01:28<00:40,  2.06it/s]Epoch: 2, train for the 43-th batch, train loss: 0.5608952045440674:  18%|██▎          | 42/237 [00:06<00:30,  6.37it/s]Epoch: 2, train for the 179-th batch, train loss: 0.43685972690582275:  74%|███████▍  | 178/241 [00:28<00:09,  6.39it/s]Epoch: 2, train for the 43-th batch, train loss: 0.5608952045440674:  18%|██▎          | 43/237 [00:06<00:29,  6.60it/s]Epoch: 2, train for the 179-th batch, train loss: 0.43685972690582275:  74%|███████▍  | 179/241 [00:28<00:09,  6.46it/s]Epoch: 1, train for the 301-th batch, train loss: 0.32474344968795776:  79%|███████▊  | 301/383 [01:29<00:38,  2.15it/s]Epoch: 4, train for the 108-th batch, train loss: 0.36481165885925293:  90%|████████▉ | 107/119 [00:15<00:01,  7.04it/s]Epoch: 4, train for the 108-th batch, train loss: 0.36481165885925293:  91%|█████████ | 108/119 [00:15<00:01,  7.15it/s]Epoch: 3, train for the 77-th batch, train loss: 0.49713951349258423:  50%|██████      | 76/151 [00:12<00:10,  6.93it/s]Epoch: 3, train for the 77-th batch, train loss: 0.49713951349258423:  51%|██████      | 77/151 [00:12<00:10,  6.94it/s]Epoch: 4, train for the 5-th batch, train loss: 0.6972410678863525:   3%|▍              | 4/146 [00:00<00:16,  8.77it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5821405649185181:  18%|██▎          | 43/237 [00:06<00:29,  6.60it/s]Epoch: 2, train for the 180-th batch, train loss: 0.5282502770423889:  74%|████████▏  | 179/241 [00:28<00:09,  6.46it/s]Epoch: 2, train for the 44-th batch, train loss: 0.5821405649185181:  19%|██▍          | 44/237 [00:06<00:29,  6.54it/s]Epoch: 2, train for the 180-th batch, train loss: 0.5282502770423889:  75%|████████▏  | 180/241 [00:28<00:09,  6.37it/s]Epoch: 4, train for the 6-th batch, train loss: 0.6863020062446594:   3%|▍              | 4/146 [00:00<00:16,  8.77it/s]Epoch: 4, train for the 6-th batch, train loss: 0.6863020062446594:   4%|▌              | 6/146 [00:00<00:16,  8.56it/s]Epoch: 3, train for the 78-th batch, train loss: 0.5100989937782288:  51%|██████▋      | 77/151 [00:12<00:10,  6.94it/s]Epoch: 4, train for the 109-th batch, train loss: 0.475350946187973:  91%|██████████▉ | 108/119 [00:15<00:01,  7.15it/s]Epoch: 3, train for the 78-th batch, train loss: 0.5100989937782288:  52%|██████▋      | 78/151 [00:12<00:11,  6.53it/s]Epoch: 4, train for the 109-th batch, train loss: 0.475350946187973:  92%|██████████▉ | 109/119 [00:15<00:01,  6.37it/s]Epoch: 2, train for the 45-th batch, train loss: 0.6059392690658569:  19%|██▍          | 44/237 [00:06<00:29,  6.54it/s]Epoch: 4, train for the 7-th batch, train loss: 0.6745002865791321:   4%|▌              | 6/146 [00:00<00:16,  8.56it/s]Epoch: 2, train for the 45-th batch, train loss: 0.6059392690658569:  19%|██▍          | 45/237 [00:06<00:28,  6.66it/s]Epoch: 1, train for the 302-th batch, train loss: 0.3443494439125061:  79%|████████▋  | 301/383 [01:29<00:38,  2.15it/s]Epoch: 4, train for the 7-th batch, train loss: 0.6745002865791321:   5%|▋              | 7/146 [00:00<00:15,  8.71it/s]Epoch: 2, train for the 181-th batch, train loss: 0.7962649464607239:  75%|████████▏  | 180/241 [00:28<00:09,  6.37it/s]Epoch: 4, train for the 110-th batch, train loss: 0.4854780435562134:  92%|██████████ | 109/119 [00:15<00:01,  6.37it/s]Epoch: 2, train for the 181-th batch, train loss: 0.7962649464607239:  75%|████████▎  | 181/241 [00:28<00:10,  5.99it/s]Epoch: 4, train for the 110-th batch, train loss: 0.4854780435562134:  92%|██████████▏| 110/119 [00:15<00:01,  6.59it/s]Epoch: 3, train for the 79-th batch, train loss: 0.5619927048683167:  52%|██████▋      | 78/151 [00:12<00:11,  6.53it/s]Epoch: 1, train for the 302-th batch, train loss: 0.3443494439125061:  79%|████████▋  | 302/383 [01:29<00:35,  2.31it/s]Epoch: 4, train for the 8-th batch, train loss: 0.6585898995399475:   5%|▋              | 7/146 [00:00<00:15,  8.71it/s]Epoch: 3, train for the 79-th batch, train loss: 0.5619927048683167:  52%|██████▊      | 79/151 [00:12<00:11,  6.15it/s]Epoch: 2, train for the 46-th batch, train loss: 0.5678643584251404:  19%|██▍          | 45/237 [00:06<00:28,  6.66it/s]Epoch: 4, train for the 8-th batch, train loss: 0.6585898995399475:   5%|▊              | 8/146 [00:00<00:15,  8.91it/s]Epoch: 2, train for the 46-th batch, train loss: 0.5678643584251404:  19%|██▌          | 46/237 [00:06<00:27,  7.03it/s]Epoch: 2, train for the 182-th batch, train loss: 0.6044217348098755:  75%|████████▎  | 181/241 [00:28<00:10,  5.99it/s]Epoch: 4, train for the 111-th batch, train loss: 0.5486820340156555:  92%|██████████▏| 110/119 [00:16<00:01,  6.59it/s]Epoch: 2, train for the 182-th batch, train loss: 0.6044217348098755:  76%|████████▎  | 182/241 [00:28<00:09,  6.33it/s]Epoch: 4, train for the 111-th batch, train loss: 0.5486820340156555:  93%|██████████▎| 111/119 [00:16<00:01,  6.76it/s]Epoch: 4, train for the 9-th batch, train loss: 0.636566698551178:   5%|▉               | 8/146 [00:01<00:15,  8.91it/s]Epoch: 4, train for the 9-th batch, train loss: 0.636566698551178:   6%|▉               | 9/146 [00:01<00:16,  8.35it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5663759112358093:  19%|██▌          | 46/237 [00:07<00:27,  7.03it/s]Epoch: 3, train for the 80-th batch, train loss: 0.5358003377914429:  52%|██████▊      | 79/151 [00:12<00:11,  6.15it/s]Epoch: 2, train for the 47-th batch, train loss: 0.5663759112358093:  20%|██▌          | 47/237 [00:07<00:28,  6.72it/s]Epoch: 2, train for the 183-th batch, train loss: 0.40096738934516907:  76%|███████▌  | 182/241 [00:28<00:09,  6.33it/s]Epoch: 3, train for the 80-th batch, train loss: 0.5358003377914429:  53%|██████▉      | 80/151 [00:12<00:12,  5.69it/s]Epoch: 2, train for the 183-th batch, train loss: 0.40096738934516907:  76%|███████▌  | 183/241 [00:28<00:08,  6.69it/s]Epoch: 1, train for the 303-th batch, train loss: 0.39758339524269104:  79%|███████▉  | 302/383 [01:29<00:35,  2.31it/s]Epoch: 4, train for the 112-th batch, train loss: 0.46242281794548035:  93%|█████████▎| 111/119 [00:16<00:01,  6.76it/s]Epoch: 4, train for the 112-th batch, train loss: 0.46242281794548035:  94%|█████████▍| 112/119 [00:16<00:01,  6.57it/s]Epoch: 4, train for the 10-th batch, train loss: 0.6363522410392761:   6%|▊             | 9/146 [00:01<00:16,  8.35it/s]Epoch: 4, train for the 10-th batch, train loss: 0.6363522410392761:   7%|▉            | 10/146 [00:01<00:16,  8.12it/s]Epoch: 2, train for the 48-th batch, train loss: 0.559366762638092:  20%|██▊           | 47/237 [00:07<00:28,  6.72it/s]Epoch: 1, train for the 303-th batch, train loss: 0.39758339524269104:  79%|███████▉  | 303/383 [01:29<00:32,  2.46it/s]Epoch: 2, train for the 48-th batch, train loss: 0.559366762638092:  20%|██▊           | 48/237 [00:07<00:27,  6.90it/s]Epoch: 3, train for the 81-th batch, train loss: 0.5182036757469177:  53%|██████▉      | 80/151 [00:12<00:12,  5.69it/s]Epoch: 2, train for the 184-th batch, train loss: 0.6102445721626282:  76%|████████▎  | 183/241 [00:29<00:08,  6.69it/s]Epoch: 3, train for the 81-th batch, train loss: 0.5182036757469177:  54%|██████▉      | 81/151 [00:12<00:12,  5.72it/s]Epoch: 2, train for the 184-th batch, train loss: 0.6102445721626282:  76%|████████▍  | 184/241 [00:29<00:08,  6.36it/s]Epoch: 4, train for the 11-th batch, train loss: 0.6429296135902405:   7%|▉            | 10/146 [00:01<00:16,  8.12it/s]Epoch: 4, train for the 11-th batch, train loss: 0.6429296135902405:   8%|▉            | 11/146 [00:01<00:17,  7.93it/s]Epoch: 4, train for the 113-th batch, train loss: 0.4525469243526459:  94%|██████████▎| 112/119 [00:16<00:01,  6.57it/s]Epoch: 4, train for the 113-th batch, train loss: 0.4525469243526459:  95%|██████████▍| 113/119 [00:16<00:00,  6.25it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5292361378669739:  20%|██▋          | 48/237 [00:07<00:27,  6.90it/s]Epoch: 2, train for the 49-th batch, train loss: 0.5292361378669739:  21%|██▋          | 49/237 [00:07<00:28,  6.54it/s]Epoch: 4, train for the 12-th batch, train loss: 0.6211340427398682:   8%|▉            | 11/146 [00:01<00:17,  7.93it/s]Epoch: 4, train for the 12-th batch, train loss: 0.6211340427398682:   8%|█            | 12/146 [00:01<00:16,  7.96it/s]Epoch: 1, train for the 304-th batch, train loss: 0.4301636517047882:  79%|████████▋  | 303/383 [01:30<00:32,  2.46it/s]Epoch: 4, train for the 114-th batch, train loss: 0.4343808591365814:  95%|██████████▍| 113/119 [00:16<00:00,  6.25it/s]Epoch: 4, train for the 114-th batch, train loss: 0.4343808591365814:  96%|██████████▌| 114/119 [00:16<00:00,  6.35it/s]Epoch: 2, train for the 185-th batch, train loss: 0.6411022543907166:  76%|████████▍  | 184/241 [00:29<00:08,  6.36it/s]Epoch: 3, train for the 82-th batch, train loss: 0.5516470074653625:  54%|██████▉      | 81/151 [00:13<00:12,  5.72it/s]Epoch: 3, train for the 82-th batch, train loss: 0.5516470074653625:  54%|███████      | 82/151 [00:13<00:13,  5.16it/s]Epoch: 2, train for the 185-th batch, train loss: 0.6411022543907166:  77%|████████▍  | 185/241 [00:29<00:09,  5.65it/s]Epoch: 1, train for the 304-th batch, train loss: 0.4301636517047882:  79%|████████▋  | 304/383 [01:30<00:29,  2.66it/s]Epoch: 2, train for the 50-th batch, train loss: 0.6029039621353149:  21%|██▋          | 49/237 [00:07<00:28,  6.54it/s]Epoch: 4, train for the 13-th batch, train loss: 0.6206568479537964:   8%|█            | 12/146 [00:01<00:16,  7.96it/s]Epoch: 4, train for the 13-th batch, train loss: 0.6206568479537964:   9%|█▏           | 13/146 [00:01<00:16,  7.94it/s]Epoch: 2, train for the 50-th batch, train loss: 0.6029039621353149:  21%|██▋          | 50/237 [00:07<00:29,  6.27it/s]Epoch: 4, train for the 115-th batch, train loss: 0.4391113817691803:  96%|██████████▌| 114/119 [00:16<00:00,  6.35it/s]Epoch: 4, train for the 115-th batch, train loss: 0.4391113817691803:  97%|██████████▋| 115/119 [00:16<00:00,  6.36it/s]Epoch: 4, train for the 14-th batch, train loss: 0.6101687550544739:   9%|█▏           | 13/146 [00:01<00:16,  7.94it/s]Epoch: 3, train for the 83-th batch, train loss: 0.5701751708984375:  54%|███████      | 82/151 [00:13<00:13,  5.16it/s]Epoch: 4, train for the 14-th batch, train loss: 0.6101687550544739:  10%|█▏           | 14/146 [00:01<00:16,  7.99it/s]Epoch: 2, train for the 186-th batch, train loss: 0.5110517740249634:  77%|████████▍  | 185/241 [00:29<00:09,  5.65it/s]Epoch: 3, train for the 83-th batch, train loss: 0.5701751708984375:  55%|███████▏     | 83/151 [00:13<00:12,  5.24it/s]Epoch: 2, train for the 51-th batch, train loss: 0.5902327299118042:  21%|██▋          | 50/237 [00:07<00:29,  6.27it/s]Epoch: 2, train for the 186-th batch, train loss: 0.5110517740249634:  77%|████████▍  | 186/241 [00:29<00:10,  5.42it/s]Epoch: 2, train for the 51-th batch, train loss: 0.5902327299118042:  22%|██▊          | 51/237 [00:07<00:30,  6.10it/s]Epoch: 4, train for the 116-th batch, train loss: 0.406167209148407:  97%|███████████▌| 115/119 [00:16<00:00,  6.36it/s]Epoch: 4, train for the 15-th batch, train loss: 0.6074892282485962:  10%|█▏           | 14/146 [00:01<00:16,  7.99it/s]Epoch: 4, train for the 15-th batch, train loss: 0.6074892282485962:  10%|█▎           | 15/146 [00:01<00:17,  7.53it/s]Epoch: 4, train for the 116-th batch, train loss: 0.406167209148407:  97%|███████████▋| 116/119 [00:16<00:00,  6.00it/s]Epoch: 1, train for the 305-th batch, train loss: 0.5167338252067566:  79%|████████▋  | 304/383 [01:30<00:29,  2.66it/s]Epoch: 3, train for the 84-th batch, train loss: 0.5425936579704285:  55%|███████▏     | 83/151 [00:13<00:12,  5.24it/s]Epoch: 2, train for the 187-th batch, train loss: 0.5913953185081482:  77%|████████▍  | 186/241 [00:29<00:10,  5.42it/s]Epoch: 3, train for the 84-th batch, train loss: 0.5425936579704285:  56%|███████▏     | 84/151 [00:13<00:13,  5.14it/s]Epoch: 2, train for the 187-th batch, train loss: 0.5913953185081482:  78%|████████▌  | 187/241 [00:29<00:09,  5.43it/s]Epoch: 2, train for the 52-th batch, train loss: 0.576582133769989:  22%|███           | 51/237 [00:07<00:30,  6.10it/s]Epoch: 2, train for the 52-th batch, train loss: 0.576582133769989:  22%|███           | 52/237 [00:07<00:32,  5.76it/s]Epoch: 1, train for the 305-th batch, train loss: 0.5167338252067566:  80%|████████▊  | 305/383 [01:30<00:30,  2.56it/s]Epoch: 4, train for the 16-th batch, train loss: 0.6115532517433167:  10%|█▎           | 15/146 [00:01<00:17,  7.53it/s]Epoch: 4, train for the 16-th batch, train loss: 0.6115532517433167:  11%|█▍           | 16/146 [00:01<00:17,  7.53it/s]Epoch: 4, train for the 117-th batch, train loss: 0.4396578073501587:  97%|██████████▋| 116/119 [00:17<00:00,  6.00it/s]Epoch: 4, train for the 117-th batch, train loss: 0.4396578073501587:  98%|██████████▊| 117/119 [00:17<00:00,  6.05it/s]Epoch: 4, train for the 17-th batch, train loss: 0.6008480191230774:  11%|█▍           | 16/146 [00:02<00:17,  7.53it/s]Epoch: 3, train for the 85-th batch, train loss: 0.4921281635761261:  56%|███████▏     | 84/151 [00:13<00:13,  5.14it/s]Epoch: 2, train for the 188-th batch, train loss: 0.41171830892562866:  78%|███████▊  | 187/241 [00:29<00:09,  5.43it/s]Epoch: 4, train for the 17-th batch, train loss: 0.6008480191230774:  12%|█▌           | 17/146 [00:02<00:17,  7.54it/s]Epoch: 2, train for the 53-th batch, train loss: 0.5818487405776978:  22%|██▊          | 52/237 [00:08<00:32,  5.76it/s]Epoch: 3, train for the 85-th batch, train loss: 0.4921281635761261:  56%|███████▎     | 85/151 [00:13<00:13,  5.04it/s]Epoch: 2, train for the 188-th batch, train loss: 0.41171830892562866:  78%|███████▊  | 188/241 [00:29<00:10,  5.22it/s]Epoch: 2, train for the 53-th batch, train loss: 0.5818487405776978:  22%|██▉          | 53/237 [00:08<00:33,  5.52it/s]Epoch: 4, train for the 118-th batch, train loss: 0.39995935559272766:  98%|█████████▊| 117/119 [00:17<00:00,  6.05it/s]Epoch: 4, train for the 118-th batch, train loss: 0.39995935559272766:  99%|█████████▉| 118/119 [00:17<00:00,  6.23it/s]Epoch: 1, train for the 306-th batch, train loss: 0.355823814868927:  80%|█████████▌  | 305/383 [01:30<00:30,  2.56it/s]Epoch: 4, train for the 18-th batch, train loss: 0.5762078762054443:  12%|█▌           | 17/146 [00:02<00:17,  7.54it/s]Epoch: 4, train for the 18-th batch, train loss: 0.5762078762054443:  12%|█▌           | 18/146 [00:02<00:16,  7.55it/s]Epoch: 1, train for the 306-th batch, train loss: 0.355823814868927:  80%|█████████▌  | 306/383 [01:30<00:27,  2.79it/s]Epoch: 3, train for the 86-th batch, train loss: 0.5173697471618652:  56%|███████▎     | 85/151 [00:13<00:13,  5.04it/s]Epoch: 4, train for the 119-th batch, train loss: 0.3563280403614044:  99%|██████████▉| 118/119 [00:17<00:00,  6.23it/s]Epoch: 4, train for the 119-th batch, train loss: 0.3563280403614044: 100%|███████████| 119/119 [00:17<00:00,  6.67it/s]Epoch: 4, train for the 119-th batch, train loss: 0.3563280403614044: 100%|███████████| 119/119 [00:17<00:00,  6.84it/s]
Epoch: 2, train for the 54-th batch, train loss: 0.5083178877830505:  22%|██▉          | 53/237 [00:08<00:33,  5.52it/s]Epoch: 3, train for the 86-th batch, train loss: 0.5173697471618652:  57%|███████▍     | 86/151 [00:13<00:12,  5.31it/s]Epoch: 2, train for the 189-th batch, train loss: 0.3772658705711365:  78%|████████▌  | 188/241 [00:29<00:10,  5.22it/s]Epoch: 2, train for the 54-th batch, train loss: 0.5083178877830505:  23%|██▉          | 54/237 [00:08<00:31,  5.78it/s]Epoch: 2, train for the 189-th batch, train loss: 0.3772658705711365:  78%|████████▋  | 189/241 [00:30<00:09,  5.25it/s]Epoch: 4, train for the 19-th batch, train loss: 0.5920495986938477:  12%|█▌           | 18/146 [00:02<00:16,  7.55it/s]Epoch: 4, train for the 19-th batch, train loss: 0.5920495986938477:  13%|█▋           | 19/146 [00:02<00:16,  7.67it/s]Epoch: 3, train for the 87-th batch, train loss: 0.5442030429840088:  57%|███████▍     | 86/151 [00:13<00:12,  5.31it/s]Epoch: 3, train for the 87-th batch, train loss: 0.5442030429840088:  58%|███████▍     | 87/151 [00:14<00:11,  5.73it/s]Epoch: 2, train for the 190-th batch, train loss: 0.33481332659721375:  78%|███████▊  | 189/241 [00:30<00:09,  5.25it/s]Epoch: 4, train for the 20-th batch, train loss: 0.5591946840286255:  13%|█▋           | 19/146 [00:02<00:16,  7.67it/s]Epoch: 4, train for the 20-th batch, train loss: 0.5591946840286255:  14%|█▊           | 20/146 [00:02<00:15,  8.22it/s]Epoch: 2, train for the 190-th batch, train loss: 0.33481332659721375:  79%|███████▉  | 190/241 [00:30<00:09,  5.57it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5648866295814514:  23%|██▉          | 54/237 [00:08<00:31,  5.78it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5648866295814514:  23%|███          | 55/237 [00:08<00:32,  5.63it/s]  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 1, train for the 307-th batch, train loss: 0.35199013352394104:  80%|███████▉  | 306/383 [01:31<00:27,  2.79it/s]Epoch: 3, train for the 88-th batch, train loss: 0.5766588449478149:  58%|███████▍     | 87/151 [00:14<00:11,  5.73it/s]Epoch: 4, train for the 21-th batch, train loss: 0.5515293478965759:  14%|█▊           | 20/146 [00:02<00:15,  8.22it/s]evaluate for the 1-th batch, evaluate loss: 0.4706662595272064:   0%|                            | 0/40 [00:00<?, ?it/s]Epoch: 1, train for the 307-th batch, train loss: 0.35199013352394104:  80%|████████  | 307/383 [01:31<00:27,  2.81it/s]Epoch: 4, train for the 21-th batch, train loss: 0.5515293478965759:  14%|█▊           | 21/146 [00:02<00:15,  8.14it/s]Epoch: 3, train for the 88-th batch, train loss: 0.5766588449478149:  58%|███████▌     | 88/151 [00:14<00:10,  5.81it/s]Epoch: 2, train for the 191-th batch, train loss: 0.3034099042415619:  79%|████████▋  | 190/241 [00:30<00:09,  5.57it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5180818438529968:  23%|███          | 55/237 [00:08<00:32,  5.63it/s]Epoch: 2, train for the 191-th batch, train loss: 0.3034099042415619:  79%|████████▋  | 191/241 [00:30<00:08,  5.71it/s]Epoch: 2, train for the 56-th batch, train loss: 0.5180818438529968:  24%|███          | 56/237 [00:08<00:31,  5.81it/s]evaluate for the 2-th batch, evaluate loss: 0.48477545380592346:   0%|                           | 0/40 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.48477545380592346:   5%|▉                  | 2/40 [00:00<00:03, 12.50it/s]Epoch: 4, train for the 22-th batch, train loss: 0.5597729086875916:  14%|█▊           | 21/146 [00:02<00:15,  8.14it/s]Epoch: 4, train for the 22-th batch, train loss: 0.5597729086875916:  15%|█▉           | 22/146 [00:02<00:15,  8.24it/s]evaluate for the 3-th batch, evaluate loss: 0.4645070433616638:   5%|█                   | 2/40 [00:00<00:03, 12.50it/s]Epoch: 3, train for the 89-th batch, train loss: 0.538827657699585:  58%|████████▏     | 88/151 [00:14<00:10,  5.81it/s]Epoch: 2, train for the 192-th batch, train loss: 0.30145516991615295:  79%|███████▉  | 191/241 [00:30<00:08,  5.71it/s]Epoch: 3, train for the 89-th batch, train loss: 0.538827657699585:  59%|████████▎     | 89/151 [00:14<00:10,  5.70it/s]Epoch: 2, train for the 192-th batch, train loss: 0.30145516991615295:  80%|███████▉  | 192/241 [00:30<00:08,  5.76it/s]Epoch: 2, train for the 57-th batch, train loss: 0.578187108039856:  24%|███▎          | 56/237 [00:08<00:31,  5.81it/s]Epoch: 2, train for the 57-th batch, train loss: 0.578187108039856:  24%|███▎          | 57/237 [00:08<00:30,  5.81it/s]evaluate for the 4-th batch, evaluate loss: 0.5403926372528076:   5%|█                   | 2/40 [00:00<00:03, 12.50it/s]evaluate for the 4-th batch, evaluate loss: 0.5403926372528076:  10%|██                  | 4/40 [00:00<00:03, 11.76it/s]Epoch: 4, train for the 23-th batch, train loss: 0.594256579875946:  15%|██            | 22/146 [00:02<00:15,  8.24it/s]Epoch: 4, train for the 23-th batch, train loss: 0.594256579875946:  16%|██▏           | 23/146 [00:02<00:16,  7.52it/s]Epoch: 1, train for the 308-th batch, train loss: 0.41577085852622986:  80%|████████  | 307/383 [01:31<00:27,  2.81it/s]evaluate for the 5-th batch, evaluate loss: 0.5214362144470215:  10%|██                  | 4/40 [00:00<00:03, 11.76it/s]Epoch: 3, train for the 90-th batch, train loss: 0.4944464862346649:  59%|███████▋     | 89/151 [00:14<00:10,  5.70it/s]Epoch: 3, train for the 90-th batch, train loss: 0.4944464862346649:  60%|███████▋     | 90/151 [00:14<00:10,  5.88it/s]Epoch: 2, train for the 193-th batch, train loss: 0.33623576164245605:  80%|███████▉  | 192/241 [00:30<00:08,  5.76it/s]Epoch: 2, train for the 58-th batch, train loss: 0.6157507300376892:  24%|███▏         | 57/237 [00:08<00:30,  5.81it/s]Epoch: 2, train for the 193-th batch, train loss: 0.33623576164245605:  80%|████████  | 193/241 [00:30<00:08,  5.93it/s]evaluate for the 6-th batch, evaluate loss: 0.47093963623046875:  10%|█▉                 | 4/40 [00:00<00:03, 11.76it/s]evaluate for the 6-th batch, evaluate loss: 0.47093963623046875:  15%|██▊                | 6/40 [00:00<00:02, 13.26it/s]Epoch: 2, train for the 58-th batch, train loss: 0.6157507300376892:  24%|███▏         | 58/237 [00:09<00:29,  6.12it/s]Epoch: 1, train for the 308-th batch, train loss: 0.41577085852622986:  80%|████████  | 308/383 [01:31<00:27,  2.71it/s]Epoch: 4, train for the 24-th batch, train loss: 0.5711302161216736:  16%|██           | 23/146 [00:02<00:16,  7.52it/s]Epoch: 4, train for the 24-th batch, train loss: 0.5711302161216736:  16%|██▏          | 24/146 [00:02<00:16,  7.61it/s]evaluate for the 7-th batch, evaluate loss: 0.4928014278411865:  15%|███                 | 6/40 [00:00<00:02, 13.26it/s]Epoch: 2, train for the 59-th batch, train loss: 0.6180776357650757:  24%|███▏         | 58/237 [00:09<00:29,  6.12it/s]evaluate for the 8-th batch, evaluate loss: 0.4453630745410919:  15%|███                 | 6/40 [00:00<00:02, 13.26it/s]evaluate for the 8-th batch, evaluate loss: 0.4453630745410919:  20%|████                | 8/40 [00:00<00:02, 12.14it/s]Epoch: 3, train for the 91-th batch, train loss: 0.47621235251426697:  60%|███████▏    | 90/151 [00:14<00:10,  5.88it/s]Epoch: 2, train for the 59-th batch, train loss: 0.6180776357650757:  25%|███▏         | 59/237 [00:09<00:30,  5.84it/s]Epoch: 4, train for the 25-th batch, train loss: 0.5600625276565552:  16%|██▏          | 24/146 [00:03<00:16,  7.61it/s]Epoch: 2, train for the 194-th batch, train loss: 0.3585747480392456:  80%|████████▊  | 193/241 [00:30<00:08,  5.93it/s]Epoch: 4, train for the 25-th batch, train loss: 0.5600625276565552:  17%|██▏          | 25/146 [00:03<00:17,  7.11it/s]Epoch: 3, train for the 91-th batch, train loss: 0.47621235251426697:  60%|███████▏    | 91/151 [00:14<00:11,  5.24it/s]Epoch: 2, train for the 194-th batch, train loss: 0.3585747480392456:  80%|████████▊  | 194/241 [00:30<00:08,  5.43it/s]evaluate for the 9-th batch, evaluate loss: 0.4995790719985962:  20%|████                | 8/40 [00:00<00:02, 12.14it/s]Epoch: 4, train for the 26-th batch, train loss: 0.5377147793769836:  17%|██▏          | 25/146 [00:03<00:17,  7.11it/s]Epoch: 4, train for the 26-th batch, train loss: 0.5377147793769836:  18%|██▎          | 26/146 [00:03<00:16,  7.16it/s]evaluate for the 10-th batch, evaluate loss: 0.5138522386550903:  20%|███▊               | 8/40 [00:00<00:02, 12.14it/s]evaluate for the 10-th batch, evaluate loss: 0.5138522386550903:  25%|████▌             | 10/40 [00:00<00:02, 12.26it/s]Epoch: 1, train for the 309-th batch, train loss: 0.41382312774658203:  80%|████████  | 308/383 [01:31<00:27,  2.71it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5365003347396851:  25%|███▏         | 59/237 [00:09<00:30,  5.84it/s]Epoch: 2, train for the 195-th batch, train loss: 0.5509289503097534:  80%|████████▊  | 194/241 [00:31<00:08,  5.43it/s]Epoch: 2, train for the 60-th batch, train loss: 0.5365003347396851:  25%|███▎         | 60/237 [00:09<00:32,  5.42it/s]Epoch: 2, train for the 195-th batch, train loss: 0.5509289503097534:  81%|████████▉  | 195/241 [00:31<00:08,  5.33it/s]evaluate for the 11-th batch, evaluate loss: 0.47061607241630554:  25%|████▎            | 10/40 [00:00<00:02, 12.26it/s]Epoch: 3, train for the 92-th batch, train loss: 0.582712709903717:  60%|████████▍     | 91/151 [00:14<00:11,  5.24it/s]Epoch: 1, train for the 309-th batch, train loss: 0.41382312774658203:  81%|████████  | 309/383 [01:31<00:28,  2.61it/s]Epoch: 3, train for the 92-th batch, train loss: 0.582712709903717:  61%|████████▌     | 92/151 [00:14<00:12,  4.90it/s]Epoch: 4, train for the 27-th batch, train loss: 0.5218861103057861:  18%|██▎          | 26/146 [00:03<00:16,  7.16it/s]Epoch: 4, train for the 27-th batch, train loss: 0.5218861103057861:  18%|██▍          | 27/146 [00:03<00:16,  7.41it/s]evaluate for the 12-th batch, evaluate loss: 0.4930257201194763:  25%|████▌             | 10/40 [00:00<00:02, 12.26it/s]evaluate for the 12-th batch, evaluate loss: 0.4930257201194763:  30%|█████▍            | 12/40 [00:00<00:02, 13.04it/s]evaluate for the 13-th batch, evaluate loss: 0.47177770733833313:  30%|█████            | 12/40 [00:01<00:02, 13.04it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5758841633796692:  25%|███▎         | 60/237 [00:09<00:32,  5.42it/s]Epoch: 2, train for the 196-th batch, train loss: 0.48259490728378296:  81%|████████  | 195/241 [00:31<00:08,  5.33it/s]Epoch: 2, train for the 61-th batch, train loss: 0.5758841633796692:  26%|███▎         | 61/237 [00:09<00:31,  5.59it/s]Epoch: 2, train for the 196-th batch, train loss: 0.48259490728378296:  81%|████████▏ | 196/241 [00:31<00:08,  5.39it/s]Epoch: 4, train for the 28-th batch, train loss: 0.540263831615448:  18%|██▌           | 27/146 [00:03<00:16,  7.41it/s]evaluate for the 14-th batch, evaluate loss: 0.47757551074028015:  30%|█████            | 12/40 [00:01<00:02, 13.04it/s]evaluate for the 14-th batch, evaluate loss: 0.47757551074028015:  35%|█████▉           | 14/40 [00:01<00:01, 13.80it/s]Epoch: 4, train for the 28-th batch, train loss: 0.540263831615448:  19%|██▋           | 28/146 [00:03<00:16,  7.10it/s]Epoch: 3, train for the 93-th batch, train loss: 0.5200676321983337:  61%|███████▉     | 92/151 [00:15<00:12,  4.90it/s]Epoch: 3, train for the 93-th batch, train loss: 0.5200676321983337:  62%|████████     | 93/151 [00:15<00:11,  4.99it/s]evaluate for the 15-th batch, evaluate loss: 0.48432403802871704:  35%|█████▉           | 14/40 [00:01<00:01, 13.80it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5670934319496155:  26%|███▎         | 61/237 [00:09<00:31,  5.59it/s]evaluate for the 16-th batch, evaluate loss: 0.46767541766166687:  35%|█████▉           | 14/40 [00:01<00:01, 13.80it/s]evaluate for the 16-th batch, evaluate loss: 0.46767541766166687:  40%|██████▊          | 16/40 [00:01<00:01, 14.42it/s]Epoch: 2, train for the 197-th batch, train loss: 0.514380156993866:  81%|█████████▊  | 196/241 [00:31<00:08,  5.39it/s]Epoch: 2, train for the 62-th batch, train loss: 0.5670934319496155:  26%|███▍         | 62/237 [00:09<00:31,  5.59it/s]evaluate for the 17-th batch, evaluate loss: 0.47827234864234924:  40%|██████▊          | 16/40 [00:01<00:01, 14.42it/s]Epoch: 2, train for the 197-th batch, train loss: 0.514380156993866:  82%|█████████▊  | 197/241 [00:31<00:08,  5.41it/s]Epoch: 3, train for the 94-th batch, train loss: 0.5012943744659424:  62%|████████     | 93/151 [00:15<00:11,  4.99it/s]Epoch: 3, train for the 94-th batch, train loss: 0.5012943744659424:  62%|████████     | 94/151 [00:15<00:10,  5.25it/s]evaluate for the 18-th batch, evaluate loss: 0.4725569188594818:  40%|███████▏          | 16/40 [00:01<00:01, 14.42it/s]Epoch: 1, train for the 310-th batch, train loss: 0.5010039210319519:  81%|████████▊  | 309/383 [01:32<00:28,  2.61it/s]Epoch: 4, train for the 29-th batch, train loss: 0.5158124566078186:  19%|██▍          | 28/146 [00:03<00:16,  7.10it/s]evaluate for the 19-th batch, evaluate loss: 0.48587337136268616:  40%|██████▊          | 16/40 [00:01<00:01, 14.42it/s]evaluate for the 19-th batch, evaluate loss: 0.48587337136268616:  48%|████████         | 19/40 [00:01<00:01, 16.08it/s]Epoch: 4, train for the 29-th batch, train loss: 0.5158124566078186:  20%|██▌          | 29/146 [00:03<00:21,  5.50it/s]Epoch: 2, train for the 198-th batch, train loss: 0.5172707438468933:  82%|████████▉  | 197/241 [00:31<00:08,  5.41it/s]Epoch: 2, train for the 63-th batch, train loss: 0.6023382544517517:  26%|███▍         | 62/237 [00:09<00:31,  5.59it/s]Epoch: 2, train for the 198-th batch, train loss: 0.5172707438468933:  82%|█████████  | 198/241 [00:31<00:07,  5.81it/s]Epoch: 2, train for the 63-th batch, train loss: 0.6023382544517517:  27%|███▍         | 63/237 [00:09<00:31,  5.53it/s]evaluate for the 20-th batch, evaluate loss: 0.46879905462265015:  48%|████████         | 19/40 [00:01<00:01, 16.08it/s]Epoch: 1, train for the 310-th batch, train loss: 0.5010039210319519:  81%|████████▉  | 310/383 [01:32<00:30,  2.39it/s]Epoch: 3, train for the 95-th batch, train loss: 0.5320258736610413:  62%|████████     | 94/151 [00:15<00:10,  5.25it/s]Epoch: 3, train for the 95-th batch, train loss: 0.5320258736610413:  63%|████████▏    | 95/151 [00:15<00:10,  5.48it/s]Epoch: 4, train for the 30-th batch, train loss: 0.4780816435813904:  20%|██▌          | 29/146 [00:03<00:21,  5.50it/s]Epoch: 2, train for the 199-th batch, train loss: 0.35644230246543884:  82%|████████▏ | 198/241 [00:31<00:07,  5.81it/s]evaluate for the 21-th batch, evaluate loss: 0.4935225546360016:  48%|████████▌         | 19/40 [00:01<00:01, 16.08it/s]evaluate for the 21-th batch, evaluate loss: 0.4935225546360016:  52%|█████████▍        | 21/40 [00:01<00:01, 14.73it/s]Epoch: 2, train for the 199-th batch, train loss: 0.35644230246543884:  83%|████████▎ | 199/241 [00:31<00:06,  6.05it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5767071843147278:  27%|███▍         | 63/237 [00:10<00:31,  5.53it/s]Epoch: 4, train for the 31-th batch, train loss: 0.5200031399726868:  20%|██▌          | 29/146 [00:04<00:21,  5.50it/s]Epoch: 2, train for the 64-th batch, train loss: 0.5767071843147278:  27%|███▌         | 64/237 [00:10<00:31,  5.54it/s]Epoch: 4, train for the 31-th batch, train loss: 0.5200031399726868:  21%|██▊          | 31/146 [00:04<00:17,  6.62it/s]evaluate for the 22-th batch, evaluate loss: 0.4705640971660614:  52%|█████████▍        | 21/40 [00:01<00:01, 14.73it/s]Epoch: 3, train for the 96-th batch, train loss: 0.5301889181137085:  63%|████████▏    | 95/151 [00:15<00:10,  5.48it/s]Epoch: 3, train for the 96-th batch, train loss: 0.5301889181137085:  64%|████████▎    | 96/151 [00:15<00:10,  5.44it/s]evaluate for the 23-th batch, evaluate loss: 0.41264086961746216:  52%|████████▉        | 21/40 [00:01<00:01, 14.73it/s]evaluate for the 23-th batch, evaluate loss: 0.41264086961746216:  57%|█████████▊       | 23/40 [00:01<00:01, 14.46it/s]Epoch: 2, train for the 200-th batch, train loss: 0.4357336461544037:  83%|█████████  | 199/241 [00:31<00:06,  6.05it/s]Epoch: 1, train for the 311-th batch, train loss: 0.5460425615310669:  81%|████████▉  | 310/383 [01:32<00:30,  2.39it/s]Epoch: 2, train for the 200-th batch, train loss: 0.4357336461544037:  83%|█████████▏ | 200/241 [00:31<00:06,  5.92it/s]Epoch: 4, train for the 32-th batch, train loss: 0.5106086134910583:  21%|██▊          | 31/146 [00:04<00:17,  6.62it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5441724061965942:  27%|███▌         | 64/237 [00:10<00:31,  5.54it/s]Epoch: 4, train for the 32-th batch, train loss: 0.5106086134910583:  22%|██▊          | 32/146 [00:04<00:16,  6.78it/s]evaluate for the 24-th batch, evaluate loss: 0.4764271080493927:  57%|██████████▎       | 23/40 [00:01<00:01, 14.46it/s]Epoch: 2, train for the 65-th batch, train loss: 0.5441724061965942:  27%|███▌         | 65/237 [00:10<00:30,  5.68it/s]Epoch: 3, train for the 97-th batch, train loss: 0.5493283867835999:  64%|████████▎    | 96/151 [00:15<00:10,  5.44it/s]Epoch: 1, train for the 311-th batch, train loss: 0.5460425615310669:  81%|████████▉  | 311/383 [01:32<00:28,  2.51it/s]Epoch: 3, train for the 97-th batch, train loss: 0.5493283867835999:  64%|████████▎    | 97/151 [00:15<00:09,  5.68it/s]evaluate for the 25-th batch, evaluate loss: 0.5031395554542542:  57%|██████████▎       | 23/40 [00:01<00:01, 14.46it/s]evaluate for the 25-th batch, evaluate loss: 0.5031395554542542:  62%|███████████▎      | 25/40 [00:01<00:01, 14.70it/s]Epoch: 4, train for the 33-th batch, train loss: 0.5313568711280823:  22%|██▊          | 32/146 [00:04<00:16,  6.78it/s]Epoch: 4, train for the 33-th batch, train loss: 0.5313568711280823:  23%|██▉          | 33/146 [00:04<00:16,  6.94it/s]Epoch: 2, train for the 201-th batch, train loss: 0.4515484571456909:  83%|█████████▏ | 200/241 [00:32<00:06,  5.92it/s]evaluate for the 26-th batch, evaluate loss: 0.43777433037757874:  62%|██████████▋      | 25/40 [00:01<00:01, 14.70it/s]Epoch: 2, train for the 201-th batch, train loss: 0.4515484571456909:  83%|█████████▏ | 201/241 [00:32<00:06,  5.91it/s]evaluate for the 27-th batch, evaluate loss: 0.45781025290489197:  62%|██████████▋      | 25/40 [00:01<00:01, 14.70it/s]evaluate for the 27-th batch, evaluate loss: 0.45781025290489197:  68%|███████████▍     | 27/40 [00:01<00:00, 15.27it/s]Epoch: 2, train for the 66-th batch, train loss: 0.5581912994384766:  27%|███▌         | 65/237 [00:10<00:30,  5.68it/s]Epoch: 2, train for the 66-th batch, train loss: 0.5581912994384766:  28%|███▌         | 66/237 [00:10<00:30,  5.56it/s]evaluate for the 28-th batch, evaluate loss: 0.436545729637146:  68%|████████████▊      | 27/40 [00:01<00:00, 15.27it/s]Epoch: 2, train for the 202-th batch, train loss: 0.24957095086574554:  83%|████████▎ | 201/241 [00:32<00:06,  5.91it/s]Epoch: 4, train for the 34-th batch, train loss: 0.5289226174354553:  23%|██▉          | 33/146 [00:04<00:16,  6.94it/s]Epoch: 4, train for the 34-th batch, train loss: 0.5289226174354553:  23%|███          | 34/146 [00:04<00:16,  6.86it/s]Epoch: 2, train for the 202-th batch, train loss: 0.24957095086574554:  84%|████████▍ | 202/241 [00:32<00:06,  6.36it/s]evaluate for the 29-th batch, evaluate loss: 0.4640547037124634:  68%|████████████▏     | 27/40 [00:02<00:00, 15.27it/s]evaluate for the 29-th batch, evaluate loss: 0.4640547037124634:  72%|█████████████     | 29/40 [00:02<00:00, 15.01it/s]Epoch: 2, train for the 67-th batch, train loss: 0.510882556438446:  28%|███▉          | 66/237 [00:10<00:30,  5.56it/s]Epoch: 3, train for the 98-th batch, train loss: 0.5161877870559692:  64%|████████▎    | 97/151 [00:16<00:09,  5.68it/s]evaluate for the 30-th batch, evaluate loss: 0.447477787733078:  72%|█████████████▊     | 29/40 [00:02<00:00, 15.01it/s]Epoch: 2, train for the 67-th batch, train loss: 0.510882556438446:  28%|███▉          | 67/237 [00:10<00:30,  5.48it/s]Epoch: 3, train for the 98-th batch, train loss: 0.5161877870559692:  65%|████████▍    | 98/151 [00:16<00:12,  4.37it/s]Epoch: 2, train for the 203-th batch, train loss: 0.579800009727478:  84%|██████████  | 202/241 [00:32<00:06,  6.36it/s]Epoch: 4, train for the 35-th batch, train loss: 0.48395758867263794:  23%|██▊         | 34/146 [00:04<00:16,  6.86it/s]Epoch: 4, train for the 35-th batch, train loss: 0.48395758867263794:  24%|██▉         | 35/146 [00:04<00:16,  6.80it/s]Epoch: 2, train for the 203-th batch, train loss: 0.579800009727478:  84%|██████████  | 203/241 [00:32<00:05,  6.36it/s]evaluate for the 31-th batch, evaluate loss: 0.4617447555065155:  72%|█████████████     | 29/40 [00:02<00:00, 15.01it/s]evaluate for the 31-th batch, evaluate loss: 0.4617447555065155:  78%|█████████████▉    | 31/40 [00:02<00:00, 15.41it/s]evaluate for the 32-th batch, evaluate loss: 0.4631074070930481:  78%|█████████████▉    | 31/40 [00:02<00:00, 15.41it/s]Epoch: 1, train for the 312-th batch, train loss: 0.30947619676589966:  81%|████████  | 311/383 [01:33<00:28,  2.51it/s]Epoch: 4, train for the 36-th batch, train loss: 0.4547492563724518:  24%|███          | 35/146 [00:04<00:16,  6.80it/s]Epoch: 3, train for the 99-th batch, train loss: 0.5633844137191772:  65%|████████▍    | 98/151 [00:16<00:12,  4.37it/s]evaluate for the 33-th batch, evaluate loss: 0.4527773857116699:  78%|█████████████▉    | 31/40 [00:02<00:00, 15.41it/s]evaluate for the 33-th batch, evaluate loss: 0.4527773857116699:  82%|██████████████▊   | 33/40 [00:02<00:00, 15.91it/s]Epoch: 4, train for the 36-th batch, train loss: 0.4547492563724518:  25%|███▏         | 36/146 [00:04<00:15,  7.00it/s]Epoch: 2, train for the 68-th batch, train loss: 0.5051898956298828:  28%|███▋         | 67/237 [00:10<00:30,  5.48it/s]Epoch: 3, train for the 99-th batch, train loss: 0.5633844137191772:  66%|████████▌    | 99/151 [00:16<00:11,  4.66it/s]Epoch: 2, train for the 204-th batch, train loss: 0.43893077969551086:  84%|████████▍ | 203/241 [00:32<00:05,  6.36it/s]Epoch: 2, train for the 68-th batch, train loss: 0.5051898956298828:  29%|███▋         | 68/237 [00:10<00:32,  5.24it/s]evaluate for the 34-th batch, evaluate loss: 0.4518238604068756:  82%|██████████████▊   | 33/40 [00:02<00:00, 15.91it/s]Epoch: 2, train for the 204-th batch, train loss: 0.43893077969551086:  85%|████████▍ | 204/241 [00:32<00:06,  6.07it/s]Epoch: 1, train for the 312-th batch, train loss: 0.30947619676589966:  81%|████████▏ | 312/383 [01:33<00:32,  2.20it/s]Epoch: 4, train for the 37-th batch, train loss: 0.505561113357544:  25%|███▍          | 36/146 [00:04<00:15,  7.00it/s]Epoch: 4, train for the 37-th batch, train loss: 0.505561113357544:  25%|███▌          | 37/146 [00:04<00:14,  7.55it/s]evaluate for the 35-th batch, evaluate loss: 0.49096137285232544:  82%|██████████████   | 33/40 [00:02<00:00, 15.91it/s]evaluate for the 35-th batch, evaluate loss: 0.49096137285232544:  88%|██████████████▉  | 35/40 [00:02<00:00, 15.57it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5408304929733276:  29%|███▋         | 68/237 [00:11<00:32,  5.24it/s]evaluate for the 36-th batch, evaluate loss: 0.4641202390193939:  88%|███████████████▊  | 35/40 [00:02<00:00, 15.57it/s]Epoch: 4, train for the 38-th batch, train loss: 0.5054075717926025:  25%|███▎         | 37/146 [00:05<00:14,  7.55it/s]Epoch: 3, train for the 100-th batch, train loss: 0.5964988470077515:  66%|███████▊    | 99/151 [00:16<00:11,  4.66it/s]Epoch: 2, train for the 69-th batch, train loss: 0.5408304929733276:  29%|███▊         | 69/237 [00:11<00:32,  5.12it/s]Epoch: 2, train for the 205-th batch, train loss: 0.48941290378570557:  85%|████████▍ | 204/241 [00:32<00:06,  6.07it/s]Epoch: 4, train for the 38-th batch, train loss: 0.5054075717926025:  26%|███▍         | 38/146 [00:05<00:14,  7.30it/s]Epoch: 2, train for the 205-th batch, train loss: 0.48941290378570557:  85%|████████▌ | 205/241 [00:32<00:06,  5.58it/s]Epoch: 3, train for the 100-th batch, train loss: 0.5964988470077515:  66%|███████▎   | 100/151 [00:16<00:11,  4.37it/s]evaluate for the 37-th batch, evaluate loss: 0.48084914684295654:  88%|██████████████▉  | 35/40 [00:02<00:00, 15.57it/s]evaluate for the 37-th batch, evaluate loss: 0.48084914684295654:  92%|███████████████▋ | 37/40 [00:02<00:00, 13.64it/s]Epoch: 4, train for the 39-th batch, train loss: 0.5043231844902039:  26%|███▍         | 38/146 [00:05<00:14,  7.30it/s]Epoch: 4, train for the 39-th batch, train loss: 0.5043231844902039:  27%|███▍         | 39/146 [00:05<00:13,  7.70it/s]evaluate for the 38-th batch, evaluate loss: 0.50159752368927:  92%|██████████████████▌ | 37/40 [00:02<00:00, 13.64it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5463013052940369:  29%|███▊         | 69/237 [00:11<00:32,  5.12it/s]Epoch: 2, train for the 70-th batch, train loss: 0.5463013052940369:  30%|███▊         | 70/237 [00:11<00:31,  5.27it/s]Epoch: 1, train for the 313-th batch, train loss: 0.5372474193572998:  81%|████████▉  | 312/383 [01:33<00:32,  2.20it/s]evaluate for the 39-th batch, evaluate loss: 0.5019676685333252:  92%|████████████████▋ | 37/40 [00:02<00:00, 13.64it/s]evaluate for the 39-th batch, evaluate loss: 0.5019676685333252:  98%|█████████████████▌| 39/40 [00:02<00:00, 13.93it/s]Epoch: 2, train for the 206-th batch, train loss: 0.56111741065979:  85%|███████████  | 205/241 [00:32<00:06,  5.58it/s]Epoch: 3, train for the 101-th batch, train loss: 0.5741519331932068:  66%|███████▎   | 100/151 [00:16<00:11,  4.37it/s]Epoch: 2, train for the 206-th batch, train loss: 0.56111741065979:  85%|███████████  | 206/241 [00:33<00:06,  5.12it/s]evaluate for the 40-th batch, evaluate loss: 0.3352709114551544:  98%|█████████████████▌| 39/40 [00:02<00:00, 13.93it/s]evaluate for the 40-th batch, evaluate loss: 0.3352709114551544: 100%|██████████████████| 40/40 [00:02<00:00, 14.29it/s]
Epoch: 3, train for the 101-th batch, train loss: 0.5741519331932068:  67%|███████▎   | 101/151 [00:16<00:11,  4.38it/s]Epoch: 4, train for the 40-th batch, train loss: 0.4895073473453522:  27%|███▍         | 39/146 [00:05<00:13,  7.70it/s]Epoch: 4, train for the 40-th batch, train loss: 0.4895073473453522:  27%|███▌         | 40/146 [00:05<00:14,  7.17it/s]Epoch: 1, train for the 313-th batch, train loss: 0.5372474193572998:  82%|████████▉  | 313/383 [01:33<00:32,  2.16it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5430227518081665:  30%|███▊         | 70/237 [00:11<00:31,  5.27it/s]Epoch: 2, train for the 71-th batch, train loss: 0.5430227518081665:  30%|███▉         | 71/237 [00:11<00:30,  5.53it/s]  0%|                                                                                            | 0/21 [00:00<?, ?it/s]Epoch: 4, train for the 41-th batch, train loss: 0.47930267453193665:  27%|███▎        | 40/146 [00:05<00:14,  7.17it/s]Epoch: 4, train for the 41-th batch, train loss: 0.47930267453193665:  28%|███▎        | 41/146 [00:05<00:15,  6.97it/s]evaluate for the 1-th batch, evaluate loss: 0.6308397650718689:   0%|                            | 0/21 [00:00<?, ?it/s]Epoch: 2, train for the 207-th batch, train loss: 0.5313324332237244:  85%|█████████▍ | 206/241 [00:33<00:06,  5.12it/s]Epoch: 3, train for the 102-th batch, train loss: 0.5290982127189636:  67%|███████▎   | 101/151 [00:17<00:11,  4.38it/s]Epoch: 3, train for the 102-th batch, train loss: 0.5290982127189636:  68%|███████▍   | 102/151 [00:17<00:11,  4.39it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5559120774269104:  30%|███▉         | 71/237 [00:11<00:30,  5.53it/s]Epoch: 2, train for the 207-th batch, train loss: 0.5313324332237244:  86%|█████████▍ | 207/241 [00:33<00:07,  4.81it/s]evaluate for the 2-th batch, evaluate loss: 0.6863411068916321:   0%|                            | 0/21 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.6863411068916321:  10%|█▉                  | 2/21 [00:00<00:01, 16.53it/s]Epoch: 2, train for the 72-th batch, train loss: 0.5559120774269104:  30%|███▉         | 72/237 [00:11<00:29,  5.58it/s]Epoch: 1, train for the 314-th batch, train loss: 0.46099159121513367:  82%|████████▏ | 313/383 [01:34<00:32,  2.16it/s]Epoch: 4, train for the 42-th batch, train loss: 0.5039809942245483:  28%|███▋         | 41/146 [00:05<00:15,  6.97it/s]Epoch: 4, train for the 42-th batch, train loss: 0.5039809942245483:  29%|███▋         | 42/146 [00:05<00:14,  7.15it/s]evaluate for the 3-th batch, evaluate loss: 0.6558233499526978:  10%|█▉                  | 2/21 [00:00<00:01, 16.53it/s]Epoch: 1, train for the 314-th batch, train loss: 0.46099159121513367:  82%|████████▏ | 314/383 [01:34<00:29,  2.33it/s]evaluate for the 4-th batch, evaluate loss: 0.5773518085479736:  10%|█▉                  | 2/21 [00:00<00:01, 16.53it/s]evaluate for the 4-th batch, evaluate loss: 0.5773518085479736:  19%|███▊                | 4/21 [00:00<00:01, 15.58it/s]Epoch: 2, train for the 73-th batch, train loss: 0.5160836577415466:  30%|███▉         | 72/237 [00:11<00:29,  5.58it/s]Epoch: 3, train for the 103-th batch, train loss: 0.55637526512146:  68%|████████▊    | 102/151 [00:17<00:11,  4.39it/s]Epoch: 2, train for the 73-th batch, train loss: 0.5160836577415466:  31%|████         | 73/237 [00:11<00:28,  5.75it/s]Epoch: 3, train for the 103-th batch, train loss: 0.55637526512146:  68%|████████▊    | 103/151 [00:17<00:10,  4.64it/s]Epoch: 2, train for the 208-th batch, train loss: 0.5147741436958313:  86%|█████████▍ | 207/241 [00:33<00:07,  4.81it/s]Epoch: 4, train for the 43-th batch, train loss: 0.4847163259983063:  29%|███▋         | 42/146 [00:05<00:14,  7.15it/s]Epoch: 4, train for the 43-th batch, train loss: 0.4847163259983063:  29%|███▊         | 43/146 [00:05<00:14,  7.18it/s]Epoch: 2, train for the 208-th batch, train loss: 0.5147741436958313:  86%|█████████▍ | 208/241 [00:33<00:06,  4.78it/s]evaluate for the 5-th batch, evaluate loss: 0.6885346174240112:  19%|███▊                | 4/21 [00:00<00:01, 15.58it/s]Epoch: 2, train for the 74-th batch, train loss: 0.5204188227653503:  31%|████         | 73/237 [00:11<00:28,  5.75it/s]evaluate for the 6-th batch, evaluate loss: 0.6587299108505249:  19%|███▊                | 4/21 [00:00<00:01, 15.58it/s]evaluate for the 6-th batch, evaluate loss: 0.6587299108505249:  29%|█████▋              | 6/21 [00:00<00:01, 12.42it/s]Epoch: 4, train for the 44-th batch, train loss: 0.47146546840667725:  29%|███▌        | 43/146 [00:05<00:14,  7.18it/s]Epoch: 2, train for the 74-th batch, train loss: 0.5204188227653503:  31%|████         | 74/237 [00:11<00:29,  5.59it/s]Epoch: 4, train for the 44-th batch, train loss: 0.47146546840667725:  30%|███▌        | 44/146 [00:05<00:14,  6.83it/s]Epoch: 2, train for the 209-th batch, train loss: 0.549443244934082:  86%|██████████▎ | 208/241 [00:33<00:06,  4.78it/s]evaluate for the 7-th batch, evaluate loss: 0.6325482726097107:  29%|█████▋              | 6/21 [00:00<00:01, 12.42it/s]Epoch: 2, train for the 209-th batch, train loss: 0.549443244934082:  87%|██████████▍ | 209/241 [00:33<00:06,  4.83it/s]Epoch: 3, train for the 104-th batch, train loss: 0.5666123628616333:  68%|███████▌   | 103/151 [00:17<00:10,  4.64it/s]Epoch: 3, train for the 104-th batch, train loss: 0.5666123628616333:  69%|███████▌   | 104/151 [00:17<00:11,  4.24it/s]evaluate for the 8-th batch, evaluate loss: 0.6316854357719421:  29%|█████▋              | 6/21 [00:00<00:01, 12.42it/s]evaluate for the 8-th batch, evaluate loss: 0.6316854357719421:  38%|███████▌            | 8/21 [00:00<00:01, 12.69it/s]Epoch: 2, train for the 75-th batch, train loss: 0.527065098285675:  31%|████▎         | 74/237 [00:12<00:29,  5.59it/s]Epoch: 4, train for the 45-th batch, train loss: 0.5151163935661316:  30%|███▉         | 44/146 [00:06<00:14,  6.83it/s]Epoch: 4, train for the 45-th batch, train loss: 0.5151163935661316:  31%|████         | 45/146 [00:06<00:15,  6.53it/s]Epoch: 2, train for the 75-th batch, train loss: 0.527065098285675:  32%|████▍         | 75/237 [00:12<00:28,  5.68it/s]Epoch: 1, train for the 315-th batch, train loss: 0.5531705617904663:  82%|█████████  | 314/383 [01:34<00:29,  2.33it/s]evaluate for the 9-th batch, evaluate loss: 0.6140874624252319:  38%|███████▌            | 8/21 [00:00<00:01, 12.69it/s]Epoch: 3, train for the 105-th batch, train loss: 0.5177686810493469:  69%|███████▌   | 104/151 [00:17<00:11,  4.24it/s]Epoch: 3, train for the 105-th batch, train loss: 0.5177686810493469:  70%|███████▋   | 105/151 [00:17<00:09,  4.80it/s]Epoch: 1, train for the 315-th batch, train loss: 0.5531705617904663:  82%|█████████  | 315/383 [01:34<00:30,  2.24it/s]evaluate for the 10-th batch, evaluate loss: 0.6432185769081116:  38%|███████▏           | 8/21 [00:00<00:01, 12.69it/s]evaluate for the 10-th batch, evaluate loss: 0.6432185769081116:  48%|████████▌         | 10/21 [00:00<00:00, 13.07it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5651605129241943:  32%|████         | 75/237 [00:12<00:28,  5.68it/s]Epoch: 2, train for the 210-th batch, train loss: 0.39874792098999023:  87%|████████▋ | 209/241 [00:33<00:06,  4.83it/s]Epoch: 4, train for the 46-th batch, train loss: 0.470558226108551:  31%|████▎         | 45/146 [00:06<00:15,  6.53it/s]Epoch: 2, train for the 76-th batch, train loss: 0.5651605129241943:  32%|████▏        | 76/237 [00:12<00:26,  6.00it/s]Epoch: 4, train for the 46-th batch, train loss: 0.470558226108551:  32%|████▍         | 46/146 [00:06<00:15,  6.59it/s]Epoch: 2, train for the 210-th batch, train loss: 0.39874792098999023:  87%|████████▋ | 210/241 [00:33<00:07,  4.37it/s]evaluate for the 11-th batch, evaluate loss: 0.6497332453727722:  48%|████████▌         | 10/21 [00:00<00:00, 13.07it/s]Epoch: 3, train for the 106-th batch, train loss: 0.5246933698654175:  70%|███████▋   | 105/151 [00:17<00:09,  4.80it/s]Epoch: 3, train for the 106-th batch, train loss: 0.5246933698654175:  70%|███████▋   | 106/151 [00:17<00:09,  4.95it/s]Epoch: 4, train for the 47-th batch, train loss: 0.522121250629425:  32%|████▍         | 46/146 [00:06<00:15,  6.59it/s]evaluate for the 12-th batch, evaluate loss: 0.653351902961731:  48%|█████████          | 10/21 [00:00<00:00, 13.07it/s]evaluate for the 12-th batch, evaluate loss: 0.653351902961731:  57%|██████████▊        | 12/21 [00:00<00:00, 12.27it/s]Epoch: 4, train for the 47-th batch, train loss: 0.522121250629425:  32%|████▌         | 47/146 [00:06<00:14,  6.70it/s]Epoch: 2, train for the 77-th batch, train loss: 0.5797470808029175:  32%|████▏        | 76/237 [00:12<00:26,  6.00it/s]Epoch: 2, train for the 211-th batch, train loss: 0.398465633392334:  87%|██████████▍ | 210/241 [00:34<00:07,  4.37it/s]Epoch: 2, train for the 77-th batch, train loss: 0.5797470808029175:  32%|████▏        | 77/237 [00:12<00:27,  5.91it/s]Epoch: 2, train for the 211-th batch, train loss: 0.398465633392334:  88%|██████████▌ | 211/241 [00:34<00:06,  4.68it/s]Epoch: 1, train for the 316-th batch, train loss: 0.46775346994400024:  82%|████████▏ | 315/383 [01:34<00:30,  2.24it/s]evaluate for the 13-th batch, evaluate loss: 0.6380211114883423:  57%|██████████▎       | 12/21 [00:01<00:00, 12.27it/s]Epoch: 3, train for the 107-th batch, train loss: 0.5108022689819336:  70%|███████▋   | 106/151 [00:18<00:09,  4.95it/s]Epoch: 3, train for the 107-th batch, train loss: 0.5108022689819336:  71%|███████▊   | 107/151 [00:18<00:08,  5.28it/s]Epoch: 4, train for the 48-th batch, train loss: 0.4647575914859772:  32%|████▏        | 47/146 [00:06<00:14,  6.70it/s]Epoch: 4, train for the 48-th batch, train loss: 0.4647575914859772:  33%|████▎        | 48/146 [00:06<00:14,  6.80it/s]evaluate for the 14-th batch, evaluate loss: 0.6162446737289429:  57%|██████████▎       | 12/21 [00:01<00:00, 12.27it/s]evaluate for the 14-th batch, evaluate loss: 0.6162446737289429:  67%|████████████      | 14/21 [00:01<00:00, 12.40it/s]Epoch: 1, train for the 316-th batch, train loss: 0.46775346994400024:  83%|████████▎ | 316/383 [01:35<00:28,  2.38it/s]Epoch: 2, train for the 78-th batch, train loss: 0.5787221193313599:  32%|████▏        | 77/237 [00:12<00:27,  5.91it/s]Epoch: 2, train for the 78-th batch, train loss: 0.5787221193313599:  33%|████▎        | 78/237 [00:12<00:27,  5.85it/s]Epoch: 2, train for the 212-th batch, train loss: 0.48138362169265747:  88%|████████▊ | 211/241 [00:34<00:06,  4.68it/s]evaluate for the 15-th batch, evaluate loss: 0.6366782188415527:  67%|████████████      | 14/21 [00:01<00:00, 12.40it/s]Epoch: 2, train for the 212-th batch, train loss: 0.48138362169265747:  88%|████████▊ | 212/241 [00:34<00:06,  4.79it/s]Epoch: 4, train for the 49-th batch, train loss: 0.5044179558753967:  33%|████▎        | 48/146 [00:06<00:14,  6.80it/s]Epoch: 3, train for the 108-th batch, train loss: 0.5219833254814148:  71%|███████▊   | 107/151 [00:18<00:08,  5.28it/s]Epoch: 4, train for the 49-th batch, train loss: 0.5044179558753967:  34%|████▎        | 49/146 [00:06<00:13,  7.24it/s]Epoch: 3, train for the 108-th batch, train loss: 0.5219833254814148:  72%|███████▊   | 108/151 [00:18<00:07,  5.66it/s]evaluate for the 16-th batch, evaluate loss: 0.6436256170272827:  67%|████████████      | 14/21 [00:01<00:00, 12.40it/s]evaluate for the 16-th batch, evaluate loss: 0.6436256170272827:  76%|█████████████▋    | 16/21 [00:01<00:00, 13.15it/s]Epoch: 2, train for the 79-th batch, train loss: 0.5643066167831421:  33%|████▎        | 78/237 [00:12<00:27,  5.85it/s]Epoch: 2, train for the 213-th batch, train loss: 0.505530834197998:  88%|██████████▌ | 212/241 [00:34<00:06,  4.79it/s]Epoch: 2, train for the 79-th batch, train loss: 0.5643066167831421:  33%|████▎        | 79/237 [00:12<00:28,  5.58it/s]evaluate for the 17-th batch, evaluate loss: 0.5690098404884338:  76%|█████████████▋    | 16/21 [00:01<00:00, 13.15it/s]Epoch: 4, train for the 50-th batch, train loss: 0.55064457654953:  34%|█████          | 49/146 [00:06<00:13,  7.24it/s]Epoch: 2, train for the 213-th batch, train loss: 0.505530834197998:  88%|██████████▌ | 213/241 [00:34<00:05,  5.04it/s]Epoch: 4, train for the 50-th batch, train loss: 0.55064457654953:  34%|█████▏         | 50/146 [00:06<00:13,  7.02it/s]Epoch: 1, train for the 317-th batch, train loss: 0.33660033345222473:  83%|████████▎ | 316/383 [01:35<00:28,  2.38it/s]Epoch: 3, train for the 109-th batch, train loss: 0.4923631250858307:  72%|███████▊   | 108/151 [00:18<00:07,  5.66it/s]Epoch: 3, train for the 109-th batch, train loss: 0.4923631250858307:  72%|███████▉   | 109/151 [00:18<00:07,  5.40it/s]evaluate for the 18-th batch, evaluate loss: 0.6337833404541016:  76%|█████████████▋    | 16/21 [00:01<00:00, 13.15it/s]evaluate for the 18-th batch, evaluate loss: 0.6337833404541016:  86%|███████████████▍  | 18/21 [00:01<00:00, 11.86it/s]Epoch: 1, train for the 317-th batch, train loss: 0.33660033345222473:  83%|████████▎ | 317/383 [01:35<00:26,  2.50it/s]Epoch: 4, train for the 51-th batch, train loss: 0.5182414054870605:  34%|████▍        | 50/146 [00:06<00:13,  7.02it/s]Epoch: 2, train for the 80-th batch, train loss: 0.5792010426521301:  33%|████▎        | 79/237 [00:12<00:28,  5.58it/s]evaluate for the 19-th batch, evaluate loss: 0.6278660297393799:  86%|███████████████▍  | 18/21 [00:01<00:00, 11.86it/s]Epoch: 4, train for the 51-th batch, train loss: 0.5182414054870605:  35%|████▌        | 51/146 [00:06<00:13,  6.86it/s]Epoch: 2, train for the 80-th batch, train loss: 0.5792010426521301:  34%|████▍        | 80/237 [00:12<00:28,  5.59it/s]Epoch: 2, train for the 214-th batch, train loss: 0.4474032521247864:  88%|█████████▋ | 213/241 [00:34<00:05,  5.04it/s]evaluate for the 20-th batch, evaluate loss: 0.593891978263855:  86%|████████████████▎  | 18/21 [00:01<00:00, 11.86it/s]evaluate for the 20-th batch, evaluate loss: 0.593891978263855:  95%|██████████████████ | 20/21 [00:01<00:00, 12.57it/s]Epoch: 3, train for the 110-th batch, train loss: 0.506775438785553:  72%|████████▋   | 109/151 [00:18<00:07,  5.40it/s]Epoch: 2, train for the 214-th batch, train loss: 0.4474032521247864:  89%|█████████▊ | 214/241 [00:34<00:05,  4.76it/s]Epoch: 3, train for the 110-th batch, train loss: 0.506775438785553:  73%|████████▋   | 110/151 [00:18<00:07,  5.36it/s]Epoch: 4, train for the 52-th batch, train loss: 0.49747857451438904:  35%|████▏       | 51/146 [00:07<00:13,  6.86it/s]evaluate for the 21-th batch, evaluate loss: 0.4928969740867615:  95%|█████████████████▏| 20/21 [00:01<00:00, 12.57it/s]evaluate for the 21-th batch, evaluate loss: 0.4928969740867615: 100%|██████████████████| 21/21 [00:01<00:00, 12.94it/s]
Epoch: 4, train for the 52-th batch, train loss: 0.49747857451438904:  36%|████▎       | 52/146 [00:07<00:13,  7.10it/s]Epoch: 2, train for the 81-th batch, train loss: 0.5626869201660156:  34%|████▍        | 80/237 [00:13<00:28,  5.59it/s]Epoch: 2, train for the 81-th batch, train loss: 0.5626869201660156:  34%|████▍        | 81/237 [00:13<00:29,  5.30it/s]Epoch: 1, train for the 318-th batch, train loss: 0.426583468914032:  83%|█████████▉  | 317/383 [01:35<00:26,  2.50it/s]Epoch: 2, train for the 215-th batch, train loss: 0.4731573760509491:  89%|█████████▊ | 214/241 [00:34<00:05,  4.76it/s]Epoch: 4, train for the 53-th batch, train loss: 0.518524169921875:  36%|████▉         | 52/146 [00:07<00:13,  7.10it/s]Epoch: 4, train for the 53-th batch, train loss: 0.518524169921875:  36%|█████         | 53/146 [00:07<00:12,  7.48it/s]Epoch: 2, train for the 215-th batch, train loss: 0.4731573760509491:  89%|█████████▊ | 215/241 [00:34<00:05,  5.09it/s]Epoch: 3, train for the 111-th batch, train loss: 0.5119155645370483:  73%|████████   | 110/151 [00:18<00:07,  5.36it/s]Epoch: 3, train for the 111-th batch, train loss: 0.5119155645370483:  74%|████████   | 111/151 [00:18<00:07,  5.34it/s]Epoch: 1, train for the 318-th batch, train loss: 0.426583468914032:  83%|█████████▉  | 318/383 [01:35<00:24,  2.61it/s]Epoch: 2, train for the 82-th batch, train loss: 0.6043939590454102:  34%|████▍        | 81/237 [00:13<00:29,  5.30it/s]Epoch: 2, train for the 216-th batch, train loss: 0.6053768992424011:  89%|█████████▊ | 215/241 [00:35<00:05,  5.09it/s]Epoch: 2, train for the 82-th batch, train loss: 0.6043939590454102:  35%|████▍        | 82/237 [00:13<00:27,  5.63it/s]Epoch: 4, train for the 54-th batch, train loss: 0.5175978541374207:  36%|████▋        | 53/146 [00:07<00:12,  7.48it/s]INFO:root:Epoch: 4, learning rate: 0.0001, train loss: 0.4677
INFO:root:train average_precision, 0.8720
INFO:root:train roc_auc, 0.8582
INFO:root:validate loss: 0.4720
INFO:root:validate average_precision, 0.8732
INFO:root:validate roc_auc, 0.8632
INFO:root:new node validate loss: 0.6273
INFO:root:new node validate first_1_average_precision, 0.6958
INFO:root:new node validate first_1_roc_auc, 0.6810
INFO:root:new node validate first_3_average_precision, 0.7319
INFO:root:new node validate first_3_roc_auc, 0.7152
INFO:root:new node validate first_10_average_precision, 0.7351
INFO:root:new node validate first_10_roc_auc, 0.7254
INFO:root:new node validate average_precision, 0.7378
INFO:root:new node validate roc_auc, 0.7440
Epoch: 4, train for the 54-th batch, train loss: 0.5175978541374207:  37%|████▊        | 54/146 [00:07<00:12,  7.37it/s]Epoch: 2, train for the 216-th batch, train loss: 0.6053768992424011:  90%|█████████▊ | 216/241 [00:35<00:04,  5.57it/s]INFO:root:save model ./saved_models/TGN/ia-reality-call/TGN_seed0_tgn-ia-reality-call-lincorrect-time-linear/TGN_seed0_tgn-ia-reality-call-lincorrect-time-linear.pkl
Epoch: 3, train for the 112-th batch, train loss: 0.4538508355617523:  74%|████████   | 111/151 [00:18<00:07,  5.34it/s]Epoch: 3, train for the 112-th batch, train loss: 0.4538508355617523:  74%|████████▏  | 112/151 [00:18<00:06,  5.64it/s]Epoch: 4, train for the 55-th batch, train loss: 0.5545787215232849:  37%|████▊        | 54/146 [00:07<00:12,  7.37it/s]Epoch: 4, train for the 55-th batch, train loss: 0.5545787215232849:  38%|████▉        | 55/146 [00:07<00:11,  7.86it/s]Epoch: 2, train for the 217-th batch, train loss: 0.5191190242767334:  90%|█████████▊ | 216/241 [00:35<00:04,  5.57it/s]Epoch: 2, train for the 83-th batch, train loss: 0.601483166217804:  35%|████▊         | 82/237 [00:13<00:27,  5.63it/s]Epoch: 2, train for the 217-th batch, train loss: 0.5191190242767334:  90%|█████████▉ | 217/241 [00:35<00:04,  5.84it/s]Epoch: 2, train for the 83-th batch, train loss: 0.601483166217804:  35%|████▉         | 83/237 [00:13<00:27,  5.55it/s]Epoch: 4, train for the 56-th batch, train loss: 0.5167190432548523:  38%|████▉        | 55/146 [00:07<00:11,  7.86it/s]Epoch: 3, train for the 113-th batch, train loss: 0.5409027934074402:  74%|████████▏  | 112/151 [00:19<00:06,  5.64it/s]Epoch: 3, train for the 113-th batch, train loss: 0.5409027934074402:  75%|████████▏  | 113/151 [00:19<00:06,  5.82it/s]Epoch: 4, train for the 57-th batch, train loss: 0.5407673716545105:  38%|████▉        | 55/146 [00:07<00:11,  7.86it/s]Epoch: 2, train for the 218-th batch, train loss: 0.45107001066207886:  90%|█████████ | 217/241 [00:35<00:04,  5.84it/s]Epoch: 4, train for the 57-th batch, train loss: 0.5407673716545105:  39%|█████        | 57/146 [00:07<00:09,  8.95it/s]Epoch: 1, train for the 319-th batch, train loss: 0.5352094173431396:  83%|█████████▏ | 318/383 [01:36<00:24,  2.61it/s]Epoch: 2, train for the 218-th batch, train loss: 0.45107001066207886:  90%|█████████ | 218/241 [00:35<00:03,  5.86it/s]Epoch: 1, train for the 319-th batch, train loss: 0.5352094173431396:  83%|█████████▏ | 319/383 [01:36<00:25,  2.46it/s]Epoch: 4, train for the 58-th batch, train loss: 0.4985651969909668:  39%|█████        | 57/146 [00:07<00:09,  8.95it/s]Epoch: 4, train for the 59-th batch, train loss: 0.48241469264030457:  39%|████▋       | 57/146 [00:07<00:09,  8.95it/s]Epoch: 4, train for the 59-th batch, train loss: 0.48241469264030457:  40%|████▊       | 59/146 [00:07<00:08,  9.72it/s]Epoch: 2, train for the 219-th batch, train loss: 0.46872618794441223:  90%|█████████ | 218/241 [00:35<00:03,  5.86it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5560935735702515:  35%|████▌        | 83/237 [00:13<00:27,  5.55it/s]Epoch: 2, train for the 219-th batch, train loss: 0.46872618794441223:  91%|█████████ | 219/241 [00:35<00:03,  5.69it/s]Epoch: 3, train for the 114-th batch, train loss: 0.4919677972793579:  75%|████████▏  | 113/151 [00:19<00:06,  5.82it/s]Epoch: 3, train for the 114-th batch, train loss: 0.4919677972793579:  75%|████████▎  | 114/151 [00:19<00:07,  4.67it/s]Epoch: 2, train for the 84-th batch, train loss: 0.5560935735702515:  35%|████▌        | 84/237 [00:13<00:36,  4.24it/s]Epoch: 4, train for the 60-th batch, train loss: 0.4892544448375702:  40%|█████▎       | 59/146 [00:07<00:08,  9.72it/s]Epoch: 1, train for the 320-th batch, train loss: 0.5723239779472351:  83%|█████████▏ | 319/383 [01:36<00:25,  2.46it/s]Epoch: 2, train for the 220-th batch, train loss: 0.5635946393013:  91%|████████████▋ | 219/241 [00:35<00:03,  5.69it/s]Epoch: 4, train for the 61-th batch, train loss: 0.5444551706314087:  40%|█████▎       | 59/146 [00:07<00:08,  9.72it/s]Epoch: 2, train for the 220-th batch, train loss: 0.5635946393013:  91%|████████████▊ | 220/241 [00:35<00:03,  5.87it/s]Epoch: 4, train for the 61-th batch, train loss: 0.5444551706314087:  42%|█████▍       | 61/146 [00:07<00:08,  9.93it/s]Epoch: 1, train for the 320-th batch, train loss: 0.5723239779472351:  84%|█████████▏ | 320/383 [01:36<00:23,  2.65it/s]Epoch: 3, train for the 115-th batch, train loss: 0.47683340311050415:  75%|███████▌  | 114/151 [00:19<00:07,  4.67it/s]Epoch: 2, train for the 85-th batch, train loss: 0.5442120432853699:  35%|████▌        | 84/237 [00:14<00:36,  4.24it/s]Epoch: 3, train for the 115-th batch, train loss: 0.47683340311050415:  76%|███████▌  | 115/151 [00:19<00:07,  4.92it/s]Epoch: 2, train for the 85-th batch, train loss: 0.5442120432853699:  36%|████▋        | 85/237 [00:14<00:32,  4.61it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]Epoch: 4, train for the 62-th batch, train loss: 0.4526512026786804:  42%|█████▍       | 61/146 [00:08<00:08,  9.93it/s]Epoch: 4, train for the 62-th batch, train loss: 0.4526512026786804:  42%|█████▌       | 62/146 [00:08<00:08,  9.76it/s]Epoch: 2, train for the 221-th batch, train loss: 0.5527318716049194:  91%|██████████ | 220/241 [00:35<00:03,  5.87it/s]Epoch: 2, train for the 221-th batch, train loss: 0.5527318716049194:  92%|██████████ | 221/241 [00:35<00:03,  5.97it/s]Epoch: 3, train for the 116-th batch, train loss: 0.4366980195045471:  76%|████████▍  | 115/151 [00:19<00:07,  4.92it/s]Epoch: 3, train for the 116-th batch, train loss: 0.4366980195045471:  77%|████████▍  | 116/151 [00:19<00:06,  5.28it/s]Epoch: 4, train for the 63-th batch, train loss: 0.5128377676010132:  42%|█████▌       | 62/146 [00:08<00:08,  9.76it/s]Epoch: 4, train for the 63-th batch, train loss: 0.5128377676010132:  43%|█████▌       | 63/146 [00:08<00:08,  9.62it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5699013471603394:  36%|████▋        | 85/237 [00:14<00:32,  4.61it/s]Epoch: 2, train for the 86-th batch, train loss: 0.5699013471603394:  36%|████▋        | 86/237 [00:14<00:33,  4.54it/s]Epoch: 2, train for the 222-th batch, train loss: 0.4620651602745056:  92%|██████████ | 221/241 [00:35<00:03,  5.97it/s]Epoch: 2, train for the 222-th batch, train loss: 0.4620651602745056:  92%|██████████▏| 222/241 [00:35<00:03,  6.27it/s]Epoch: 5, train for the 1-th batch, train loss: 0.698871910572052:   0%|                        | 0/119 [00:00<?, ?it/s]Epoch: 5, train for the 1-th batch, train loss: 0.698871910572052:   1%|▏               | 1/119 [00:00<00:30,  3.93it/s]Epoch: 3, train for the 117-th batch, train loss: 0.5038186311721802:  77%|████████▍  | 116/151 [00:19<00:06,  5.28it/s]Epoch: 4, train for the 64-th batch, train loss: 0.539818525314331:  43%|██████        | 63/146 [00:08<00:08,  9.62it/s]Epoch: 3, train for the 117-th batch, train loss: 0.5038186311721802:  77%|████████▌  | 117/151 [00:19<00:06,  5.54it/s]Epoch: 4, train for the 64-th batch, train loss: 0.539818525314331:  44%|██████▏       | 64/146 [00:08<00:09,  8.86it/s]Epoch: 2, train for the 87-th batch, train loss: 0.5797680020332336:  36%|████▋        | 86/237 [00:14<00:33,  4.54it/s]Epoch: 5, train for the 2-th batch, train loss: 0.7750958800315857:   1%|▏              | 1/119 [00:00<00:30,  3.93it/s]Epoch: 2, train for the 87-th batch, train loss: 0.5797680020332336:  37%|████▊        | 87/237 [00:14<00:29,  5.02it/s]Epoch: 5, train for the 2-th batch, train loss: 0.7750958800315857:   2%|▎              | 2/119 [00:00<00:20,  5.84it/s]Epoch: 2, train for the 223-th batch, train loss: 0.4782872200012207:  92%|██████████▏| 222/241 [00:36<00:03,  6.27it/s]Epoch: 4, train for the 65-th batch, train loss: 0.5448386073112488:  44%|█████▋       | 64/146 [00:08<00:09,  8.86it/s]Epoch: 4, train for the 65-th batch, train loss: 0.5448386073112488:  45%|█████▊       | 65/146 [00:08<00:09,  8.55it/s]Epoch: 2, train for the 223-th batch, train loss: 0.4782872200012207:  93%|██████████▏| 223/241 [00:36<00:03,  5.83it/s]Epoch: 3, train for the 118-th batch, train loss: 0.4467434883117676:  77%|████████▌  | 117/151 [00:20<00:06,  5.54it/s]Epoch: 3, train for the 118-th batch, train loss: 0.4467434883117676:  78%|████████▌  | 118/151 [00:20<00:05,  5.56it/s]Epoch: 5, train for the 3-th batch, train loss: 0.7544993162155151:   2%|▎              | 2/119 [00:00<00:20,  5.84it/s]Epoch: 5, train for the 3-th batch, train loss: 0.7544993162155151:   3%|▍              | 3/119 [00:00<00:17,  6.78it/s]Epoch: 2, train for the 88-th batch, train loss: 0.586400032043457:  37%|█████▏        | 87/237 [00:14<00:29,  5.02it/s]Epoch: 1, train for the 321-th batch, train loss: 0.5878126621246338:  84%|█████████▏ | 320/383 [01:37<00:23,  2.65it/s]Epoch: 2, train for the 88-th batch, train loss: 0.586400032043457:  37%|█████▏        | 88/237 [00:14<00:27,  5.46it/s]Epoch: 4, train for the 66-th batch, train loss: 0.54374760389328:  45%|██████▋        | 65/146 [00:08<00:09,  8.55it/s]Epoch: 4, train for the 66-th batch, train loss: 0.54374760389328:  45%|██████▊        | 66/146 [00:08<00:09,  8.38it/s]Epoch: 2, train for the 224-th batch, train loss: 0.5060252547264099:  93%|██████████▏| 223/241 [00:36<00:03,  5.83it/s]Epoch: 2, train for the 224-th batch, train loss: 0.5060252547264099:  93%|██████████▏| 224/241 [00:36<00:02,  5.99it/s]Epoch: 1, train for the 321-th batch, train loss: 0.5878126621246338:  84%|█████████▏ | 321/383 [01:37<00:28,  2.17it/s]Epoch: 5, train for the 4-th batch, train loss: 0.7063132524490356:   3%|▍              | 3/119 [00:00<00:17,  6.78it/s]Epoch: 5, train for the 4-th batch, train loss: 0.7063132524490356:   3%|▌              | 4/119 [00:00<00:16,  7.09it/s]Epoch: 3, train for the 119-th batch, train loss: 0.5207489132881165:  78%|████████▌  | 118/151 [00:20<00:05,  5.56it/s]Epoch: 3, train for the 119-th batch, train loss: 0.5207489132881165:  79%|████████▋  | 119/151 [00:20<00:05,  5.76it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5891880989074707:  37%|████▊        | 88/237 [00:14<00:27,  5.46it/s]Epoch: 4, train for the 67-th batch, train loss: 0.5453191995620728:  45%|█████▉       | 66/146 [00:08<00:09,  8.38it/s]Epoch: 4, train for the 67-th batch, train loss: 0.5453191995620728:  46%|█████▉       | 67/146 [00:08<00:09,  8.34it/s]Epoch: 2, train for the 89-th batch, train loss: 0.5891880989074707:  38%|████▉        | 89/237 [00:14<00:27,  5.42it/s]Epoch: 2, train for the 225-th batch, train loss: 0.5231907367706299:  93%|██████████▏| 224/241 [00:36<00:02,  5.99it/s]Epoch: 2, train for the 225-th batch, train loss: 0.5231907367706299:  93%|██████████▎| 225/241 [00:36<00:02,  6.21it/s]Epoch: 5, train for the 5-th batch, train loss: 0.6249599456787109:   3%|▌              | 4/119 [00:00<00:16,  7.09it/s]Epoch: 5, train for the 5-th batch, train loss: 0.6249599456787109:   4%|▋              | 5/119 [00:00<00:16,  6.92it/s]Epoch: 3, train for the 120-th batch, train loss: 0.52040696144104:  79%|██████████▏  | 119/151 [00:20<00:05,  5.76it/s]Epoch: 4, train for the 68-th batch, train loss: 0.4881100356578827:  46%|█████▉       | 67/146 [00:08<00:09,  8.34it/s]Epoch: 3, train for the 120-th batch, train loss: 0.52040696144104:  79%|██████████▎  | 120/151 [00:20<00:05,  5.78it/s]Epoch: 4, train for the 68-th batch, train loss: 0.4881100356578827:  47%|██████       | 68/146 [00:08<00:09,  8.05it/s]Epoch: 2, train for the 226-th batch, train loss: 0.568684458732605:  93%|███████████▏| 225/241 [00:36<00:02,  6.21it/s]Epoch: 2, train for the 226-th batch, train loss: 0.568684458732605:  94%|███████████▎| 226/241 [00:36<00:02,  6.36it/s]Epoch: 2, train for the 90-th batch, train loss: 0.583333432674408:  38%|█████▎        | 89/237 [00:14<00:27,  5.42it/s]Epoch: 5, train for the 6-th batch, train loss: 0.5307562351226807:   4%|▋              | 5/119 [00:00<00:16,  6.92it/s]Epoch: 5, train for the 6-th batch, train loss: 0.5307562351226807:   5%|▊              | 6/119 [00:00<00:16,  6.92it/s]Epoch: 1, train for the 322-th batch, train loss: 0.5664822459220886:  84%|█████████▏ | 321/383 [01:37<00:28,  2.17it/s]Epoch: 2, train for the 90-th batch, train loss: 0.583333432674408:  38%|█████▎        | 90/237 [00:15<00:28,  5.16it/s]Epoch: 3, train for the 121-th batch, train loss: 0.45726537704467773:  79%|███████▉  | 120/151 [00:20<00:05,  5.78it/s]Epoch: 4, train for the 69-th batch, train loss: 0.540890097618103:  47%|██████▌       | 68/146 [00:08<00:09,  8.05it/s]Epoch: 4, train for the 69-th batch, train loss: 0.540890097618103:  47%|██████▌       | 69/146 [00:08<00:10,  7.66it/s]Epoch: 3, train for the 121-th batch, train loss: 0.45726537704467773:  80%|████████  | 121/151 [00:20<00:05,  5.90it/s]Epoch: 1, train for the 322-th batch, train loss: 0.5664822459220886:  84%|█████████▏ | 322/383 [01:37<00:26,  2.27it/s]Epoch: 5, train for the 7-th batch, train loss: 0.5236092209815979:   5%|▊              | 6/119 [00:01<00:16,  6.92it/s]Epoch: 2, train for the 227-th batch, train loss: 0.5706417560577393:  94%|██████████▎| 226/241 [00:36<00:02,  6.36it/s]Epoch: 5, train for the 7-th batch, train loss: 0.5236092209815979:   6%|▉              | 7/119 [00:01<00:15,  7.28it/s]Epoch: 2, train for the 91-th batch, train loss: 0.5551568865776062:  38%|████▉        | 90/237 [00:15<00:28,  5.16it/s]Epoch: 2, train for the 227-th batch, train loss: 0.5706417560577393:  94%|██████████▎| 227/241 [00:36<00:02,  6.36it/s]Epoch: 2, train for the 91-th batch, train loss: 0.5551568865776062:  38%|████▉        | 91/237 [00:15<00:26,  5.51it/s]Epoch: 4, train for the 70-th batch, train loss: 0.5122402310371399:  47%|██████▏      | 69/146 [00:09<00:10,  7.66it/s]Epoch: 4, train for the 70-th batch, train loss: 0.5122402310371399:  48%|██████▏      | 70/146 [00:09<00:09,  7.69it/s]Epoch: 3, train for the 122-th batch, train loss: 0.5041348338127136:  80%|████████▊  | 121/151 [00:20<00:05,  5.90it/s]Epoch: 3, train for the 122-th batch, train loss: 0.5041348338127136:  81%|████████▉  | 122/151 [00:20<00:04,  5.88it/s]Epoch: 5, train for the 8-th batch, train loss: 0.46930959820747375:   6%|▊             | 7/119 [00:01<00:15,  7.28it/s]Epoch: 5, train for the 8-th batch, train loss: 0.46930959820747375:   7%|▉             | 8/119 [00:01<00:14,  7.43it/s]Epoch: 2, train for the 228-th batch, train loss: 0.5025190114974976:  94%|██████████▎| 227/241 [00:36<00:02,  6.36it/s]Epoch: 4, train for the 71-th batch, train loss: 0.6092209219932556:  48%|██████▏      | 70/146 [00:09<00:09,  7.69it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5325255393981934:  38%|████▉        | 91/237 [00:15<00:26,  5.51it/s]Epoch: 4, train for the 71-th batch, train loss: 0.6092209219932556:  49%|██████▎      | 71/146 [00:09<00:10,  7.45it/s]Epoch: 2, train for the 228-th batch, train loss: 0.5025190114974976:  95%|██████████▍| 228/241 [00:36<00:02,  5.99it/s]Epoch: 2, train for the 92-th batch, train loss: 0.5325255393981934:  39%|█████        | 92/237 [00:15<00:26,  5.48it/s]Epoch: 3, train for the 123-th batch, train loss: 0.5123110413551331:  81%|████████▉  | 122/151 [00:20<00:04,  5.88it/s]Epoch: 5, train for the 9-th batch, train loss: 0.4449646770954132:   7%|█              | 8/119 [00:01<00:14,  7.43it/s]Epoch: 5, train for the 9-th batch, train loss: 0.4449646770954132:   8%|█▏             | 9/119 [00:01<00:14,  7.61it/s]Epoch: 3, train for the 123-th batch, train loss: 0.5123110413551331:  81%|████████▉  | 123/151 [00:20<00:04,  6.07it/s]Epoch: 4, train for the 72-th batch, train loss: 0.5751655697822571:  49%|██████▎      | 71/146 [00:09<00:10,  7.45it/s]Epoch: 4, train for the 72-th batch, train loss: 0.5751655697822571:  49%|██████▍      | 72/146 [00:09<00:09,  7.52it/s]Epoch: 2, train for the 93-th batch, train loss: 0.5720376968383789:  39%|█████        | 92/237 [00:15<00:26,  5.48it/s]Epoch: 2, train for the 229-th batch, train loss: 0.4899198114871979:  95%|██████████▍| 228/241 [00:37<00:02,  5.99it/s]Epoch: 5, train for the 10-th batch, train loss: 0.4690576195716858:   8%|█             | 9/119 [00:01<00:14,  7.61it/s]Epoch: 2, train for the 93-th batch, train loss: 0.5720376968383789:  39%|█████        | 93/237 [00:15<00:25,  5.64it/s]Epoch: 5, train for the 10-th batch, train loss: 0.4690576195716858:   8%|█            | 10/119 [00:01<00:14,  7.58it/s]Epoch: 2, train for the 229-th batch, train loss: 0.4899198114871979:  95%|██████████▍| 229/241 [00:37<00:02,  5.71it/s]Epoch: 3, train for the 124-th batch, train loss: 0.4550999402999878:  81%|████████▉  | 123/151 [00:21<00:04,  6.07it/s]Epoch: 1, train for the 323-th batch, train loss: 0.48178592324256897:  84%|████████▍ | 322/383 [01:38<00:26,  2.27it/s]Epoch: 4, train for the 73-th batch, train loss: 0.520347535610199:  49%|██████▉       | 72/146 [00:09<00:09,  7.52it/s]Epoch: 3, train for the 124-th batch, train loss: 0.4550999402999878:  82%|█████████  | 124/151 [00:21<00:04,  5.85it/s]Epoch: 4, train for the 73-th batch, train loss: 0.520347535610199:  50%|███████       | 73/146 [00:09<00:09,  7.64it/s]Epoch: 5, train for the 11-th batch, train loss: 0.47714146971702576:   8%|█           | 10/119 [00:01<00:14,  7.58it/s]Epoch: 1, train for the 323-th batch, train loss: 0.48178592324256897:  84%|████████▍ | 323/383 [01:38<00:28,  2.11it/s]Epoch: 5, train for the 11-th batch, train loss: 0.47714146971702576:   9%|█           | 11/119 [00:01<00:14,  7.55it/s]Epoch: 2, train for the 94-th batch, train loss: 0.6321976184844971:  39%|█████        | 93/237 [00:15<00:25,  5.64it/s]Epoch: 2, train for the 94-th batch, train loss: 0.6321976184844971:  40%|█████▏       | 94/237 [00:15<00:25,  5.55it/s]Epoch: 3, train for the 125-th batch, train loss: 0.500986635684967:  82%|█████████▊  | 124/151 [00:21<00:04,  5.85it/s]Epoch: 4, train for the 74-th batch, train loss: 0.5326129198074341:  50%|██████▌      | 73/146 [00:09<00:09,  7.64it/s]Epoch: 4, train for the 74-th batch, train loss: 0.5326129198074341:  51%|██████▌      | 74/146 [00:09<00:09,  7.31it/s]Epoch: 3, train for the 125-th batch, train loss: 0.500986635684967:  83%|█████████▉  | 125/151 [00:21<00:04,  5.88it/s]Epoch: 5, train for the 12-th batch, train loss: 0.4538896381855011:   9%|█▏           | 11/119 [00:01<00:14,  7.55it/s]Epoch: 5, train for the 12-th batch, train loss: 0.4538896381855011:  10%|█▎           | 12/119 [00:01<00:13,  7.83it/s]Epoch: 2, train for the 230-th batch, train loss: 0.4928427040576935:  95%|██████████▍| 229/241 [00:37<00:02,  5.71it/s]Epoch: 2, train for the 230-th batch, train loss: 0.4928427040576935:  95%|██████████▍| 230/241 [00:37<00:02,  4.64it/s]Epoch: 2, train for the 95-th batch, train loss: 0.5440918207168579:  40%|█████▏       | 94/237 [00:15<00:25,  5.55it/s]Epoch: 2, train for the 95-th batch, train loss: 0.5440918207168579:  40%|█████▏       | 95/237 [00:15<00:25,  5.65it/s]Epoch: 4, train for the 75-th batch, train loss: 0.5337718725204468:  51%|██████▌      | 74/146 [00:09<00:09,  7.31it/s]Epoch: 4, train for the 75-th batch, train loss: 0.5337718725204468:  51%|██████▋      | 75/146 [00:09<00:10,  6.72it/s]Epoch: 1, train for the 324-th batch, train loss: 0.3482910394668579:  84%|█████████▎ | 323/383 [01:38<00:28,  2.11it/s]Epoch: 3, train for the 126-th batch, train loss: 0.4953676760196686:  83%|█████████  | 125/151 [00:21<00:04,  5.88it/s]Epoch: 5, train for the 13-th batch, train loss: 0.4378090500831604:  10%|█▎           | 12/119 [00:01<00:13,  7.83it/s]Epoch: 5, train for the 13-th batch, train loss: 0.4378090500831604:  11%|█▍           | 13/119 [00:01<00:14,  7.22it/s]Epoch: 3, train for the 126-th batch, train loss: 0.4953676760196686:  83%|█████████▏ | 126/151 [00:21<00:04,  5.65it/s]Epoch: 2, train for the 231-th batch, train loss: 0.5378876328468323:  95%|██████████▍| 230/241 [00:37<00:02,  4.64it/s]Epoch: 1, train for the 324-th batch, train loss: 0.3482910394668579:  85%|█████████▎ | 324/383 [01:38<00:25,  2.29it/s]Epoch: 2, train for the 231-th batch, train loss: 0.5378876328468323:  96%|██████████▌| 231/241 [00:37<00:01,  5.03it/s]Epoch: 2, train for the 96-th batch, train loss: 0.5919764637947083:  40%|█████▏       | 95/237 [00:16<00:25,  5.65it/s]Epoch: 2, train for the 96-th batch, train loss: 0.5919764637947083:  41%|█████▎       | 96/237 [00:16<00:24,  5.75it/s]Epoch: 4, train for the 76-th batch, train loss: 0.5064021348953247:  51%|██████▋      | 75/146 [00:09<00:10,  6.72it/s]Epoch: 4, train for the 76-th batch, train loss: 0.5064021348953247:  52%|██████▊      | 76/146 [00:09<00:10,  6.75it/s]Epoch: 5, train for the 14-th batch, train loss: 0.45830389857292175:  11%|█▎          | 13/119 [00:01<00:14,  7.22it/s]Epoch: 3, train for the 127-th batch, train loss: 0.47135740518569946:  83%|████████▎ | 126/151 [00:21<00:04,  5.65it/s]Epoch: 5, train for the 14-th batch, train loss: 0.45830389857292175:  12%|█▍          | 14/119 [00:01<00:14,  7.11it/s]Epoch: 3, train for the 127-th batch, train loss: 0.47135740518569946:  84%|████████▍ | 127/151 [00:21<00:04,  5.85it/s]Epoch: 2, train for the 232-th batch, train loss: 0.5484765768051147:  96%|██████████▌| 231/241 [00:37<00:01,  5.03it/s]Epoch: 2, train for the 232-th batch, train loss: 0.5484765768051147:  96%|██████████▌| 232/241 [00:37<00:01,  5.47it/s]Epoch: 2, train for the 97-th batch, train loss: 0.5654786825180054:  41%|█████▎       | 96/237 [00:16<00:24,  5.75it/s]Epoch: 4, train for the 77-th batch, train loss: 0.5776348114013672:  52%|██████▊      | 76/146 [00:10<00:10,  6.75it/s]Epoch: 4, train for the 77-th batch, train loss: 0.5776348114013672:  53%|██████▊      | 77/146 [00:10<00:10,  6.59it/s]Epoch: 2, train for the 97-th batch, train loss: 0.5654786825180054:  41%|█████▎       | 97/237 [00:16<00:24,  5.65it/s]Epoch: 5, train for the 15-th batch, train loss: 0.42105764150619507:  12%|█▍          | 14/119 [00:02<00:14,  7.11it/s]Epoch: 5, train for the 15-th batch, train loss: 0.42105764150619507:  13%|█▌          | 15/119 [00:02<00:15,  6.82it/s]Epoch: 3, train for the 128-th batch, train loss: 0.48507896065711975:  84%|████████▍ | 127/151 [00:21<00:04,  5.85it/s]Epoch: 3, train for the 128-th batch, train loss: 0.48507896065711975:  85%|████████▍ | 128/151 [00:21<00:04,  5.59it/s]Epoch: 2, train for the 233-th batch, train loss: 0.5051294565200806:  96%|██████████▌| 232/241 [00:37<00:01,  5.47it/s]Epoch: 2, train for the 233-th batch, train loss: 0.5051294565200806:  97%|██████████▋| 233/241 [00:37<00:01,  5.61it/s]Epoch: 2, train for the 98-th batch, train loss: 0.5293169021606445:  41%|█████▎       | 97/237 [00:16<00:24,  5.65it/s]Epoch: 4, train for the 78-th batch, train loss: 0.5557814836502075:  53%|██████▊      | 77/146 [00:10<00:10,  6.59it/s]Epoch: 4, train for the 78-th batch, train loss: 0.5557814836502075:  53%|██████▉      | 78/146 [00:10<00:09,  6.82it/s]Epoch: 5, train for the 16-th batch, train loss: 0.4610573351383209:  13%|█▋           | 15/119 [00:02<00:15,  6.82it/s]Epoch: 2, train for the 98-th batch, train loss: 0.5293169021606445:  41%|█████▍       | 98/237 [00:16<00:23,  5.87it/s]Epoch: 5, train for the 16-th batch, train loss: 0.4610573351383209:  13%|█▋           | 16/119 [00:02<00:14,  6.90it/s]Epoch: 3, train for the 129-th batch, train loss: 0.5482315421104431:  85%|█████████▎ | 128/151 [00:21<00:04,  5.59it/s]Epoch: 2, train for the 234-th batch, train loss: 0.5871757864952087:  97%|██████████▋| 233/241 [00:38<00:01,  5.61it/s]Epoch: 3, train for the 129-th batch, train loss: 0.5482315421104431:  85%|█████████▍ | 129/151 [00:21<00:03,  5.57it/s]Epoch: 4, train for the 79-th batch, train loss: 0.5612087249755859:  53%|██████▉      | 78/146 [00:10<00:09,  6.82it/s]Epoch: 1, train for the 325-th batch, train loss: 0.41824713349342346:  85%|████████▍ | 324/383 [01:38<00:25,  2.29it/s]Epoch: 2, train for the 234-th batch, train loss: 0.5871757864952087:  97%|██████████▋| 234/241 [00:38<00:01,  5.66it/s]Epoch: 4, train for the 79-th batch, train loss: 0.5612087249755859:  54%|███████      | 79/146 [00:10<00:09,  6.96it/s]Epoch: 5, train for the 17-th batch, train loss: 0.4623594582080841:  13%|█▋           | 16/119 [00:02<00:14,  6.90it/s]Epoch: 5, train for the 17-th batch, train loss: 0.4623594582080841:  14%|█▊           | 17/119 [00:02<00:14,  6.98it/s]Epoch: 2, train for the 99-th batch, train loss: 0.5603442788124084:  41%|█████▍       | 98/237 [00:16<00:23,  5.87it/s]Epoch: 1, train for the 325-th batch, train loss: 0.41824713349342346:  85%|████████▍ | 325/383 [01:39<00:27,  2.11it/s]Epoch: 2, train for the 99-th batch, train loss: 0.5603442788124084:  42%|█████▍       | 99/237 [00:16<00:24,  5.67it/s]Epoch: 3, train for the 130-th batch, train loss: 0.5143160223960876:  85%|█████████▍ | 129/151 [00:22<00:03,  5.57it/s]Epoch: 4, train for the 80-th batch, train loss: 0.5576248168945312:  54%|███████      | 79/146 [00:10<00:09,  6.96it/s]Epoch: 4, train for the 80-th batch, train loss: 0.5576248168945312:  55%|███████      | 80/146 [00:10<00:09,  6.87it/s]Epoch: 3, train for the 130-th batch, train loss: 0.5143160223960876:  86%|█████████▍ | 130/151 [00:22<00:03,  5.57it/s]Epoch: 2, train for the 235-th batch, train loss: 0.619558572769165:  97%|███████████▋| 234/241 [00:38<00:01,  5.66it/s]Epoch: 5, train for the 18-th batch, train loss: 0.4621538519859314:  14%|█▊           | 17/119 [00:02<00:14,  6.98it/s]Epoch: 5, train for the 18-th batch, train loss: 0.4621538519859314:  15%|█▉           | 18/119 [00:02<00:14,  6.98it/s]Epoch: 2, train for the 235-th batch, train loss: 0.619558572769165:  98%|███████████▋| 235/241 [00:38<00:01,  5.51it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5391521453857422:  42%|█████       | 99/237 [00:16<00:24,  5.67it/s]Epoch: 2, train for the 100-th batch, train loss: 0.5391521453857422:  42%|████▋      | 100/237 [00:16<00:23,  5.79it/s]Epoch: 4, train for the 81-th batch, train loss: 0.48735183477401733:  55%|██████▌     | 80/146 [00:10<00:09,  6.87it/s]Epoch: 4, train for the 81-th batch, train loss: 0.48735183477401733:  55%|██████▋     | 81/146 [00:10<00:09,  7.21it/s]Epoch: 5, train for the 19-th batch, train loss: 0.46379584074020386:  15%|█▊          | 18/119 [00:02<00:14,  6.98it/s]Epoch: 5, train for the 19-th batch, train loss: 0.46379584074020386:  16%|█▉          | 19/119 [00:02<00:14,  6.87it/s]Epoch: 3, train for the 131-th batch, train loss: 0.4908621907234192:  86%|█████████▍ | 130/151 [00:22<00:03,  5.57it/s]Epoch: 2, train for the 236-th batch, train loss: 0.5511226654052734:  98%|██████████▋| 235/241 [00:38<00:01,  5.51it/s]Epoch: 2, train for the 101-th batch, train loss: 0.5868447422981262:  42%|████▋      | 100/237 [00:16<00:23,  5.79it/s]Epoch: 3, train for the 131-th batch, train loss: 0.4908621907234192:  87%|█████████▌ | 131/151 [00:22<00:03,  5.31it/s]Epoch: 4, train for the 82-th batch, train loss: 0.5167353749275208:  55%|███████▏     | 81/146 [00:10<00:09,  7.21it/s]Epoch: 2, train for the 101-th batch, train loss: 0.5868447422981262:  43%|████▋      | 101/237 [00:16<00:22,  6.09it/s]Epoch: 2, train for the 236-th batch, train loss: 0.5511226654052734:  98%|██████████▊| 236/241 [00:38<00:00,  5.37it/s]Epoch: 4, train for the 82-th batch, train loss: 0.5167353749275208:  56%|███████▎     | 82/146 [00:10<00:08,  7.52it/s]Epoch: 5, train for the 20-th batch, train loss: 0.4256133437156677:  16%|██           | 19/119 [00:02<00:14,  6.87it/s]Epoch: 5, train for the 20-th batch, train loss: 0.4256133437156677:  17%|██▏          | 20/119 [00:02<00:13,  7.43it/s]Epoch: 1, train for the 326-th batch, train loss: 0.44572484493255615:  85%|████████▍ | 325/383 [01:39<00:27,  2.11it/s]Epoch: 4, train for the 83-th batch, train loss: 0.5124101042747498:  56%|███████▎     | 82/146 [00:10<00:08,  7.52it/s]Epoch: 4, train for the 83-th batch, train loss: 0.5124101042747498:  57%|███████▍     | 83/146 [00:10<00:08,  7.70it/s]Epoch: 3, train for the 132-th batch, train loss: 0.5175911784172058:  87%|█████████▌ | 131/151 [00:22<00:03,  5.31it/s]Epoch: 1, train for the 326-th batch, train loss: 0.44572484493255615:  85%|████████▌ | 326/383 [01:39<00:26,  2.13it/s]Epoch: 3, train for the 132-th batch, train loss: 0.5175911784172058:  87%|█████████▌ | 132/151 [00:22<00:03,  5.49it/s]Epoch: 2, train for the 102-th batch, train loss: 0.5942186713218689:  43%|████▋      | 101/237 [00:17<00:22,  6.09it/s]Epoch: 2, train for the 237-th batch, train loss: 0.5353971123695374:  98%|██████████▊| 236/241 [00:38<00:00,  5.37it/s]Epoch: 2, train for the 237-th batch, train loss: 0.5353971123695374:  98%|██████████▊| 237/241 [00:38<00:00,  5.45it/s]Epoch: 5, train for the 21-th batch, train loss: 0.5654763579368591:  17%|██▏          | 20/119 [00:02<00:13,  7.43it/s]Epoch: 2, train for the 102-th batch, train loss: 0.5942186713218689:  43%|████▋      | 102/237 [00:17<00:22,  5.92it/s]Epoch: 5, train for the 21-th batch, train loss: 0.5654763579368591:  18%|██▎          | 21/119 [00:02<00:13,  7.27it/s]Epoch: 4, train for the 84-th batch, train loss: 0.49201053380966187:  57%|██████▊     | 83/146 [00:11<00:08,  7.70it/s]Epoch: 4, train for the 84-th batch, train loss: 0.49201053380966187:  58%|██████▉     | 84/146 [00:11<00:08,  7.64it/s]Epoch: 3, train for the 133-th batch, train loss: 0.47723516821861267:  87%|████████▋ | 132/151 [00:22<00:03,  5.49it/s]Epoch: 5, train for the 22-th batch, train loss: 0.46794813871383667:  18%|██          | 21/119 [00:03<00:13,  7.27it/s]Epoch: 5, train for the 22-th batch, train loss: 0.46794813871383667:  18%|██▏         | 22/119 [00:03<00:13,  7.26it/s]Epoch: 3, train for the 133-th batch, train loss: 0.47723516821861267:  88%|████████▊ | 133/151 [00:22<00:03,  5.41it/s]Epoch: 2, train for the 238-th batch, train loss: 0.5244755148887634:  98%|██████████▊| 237/241 [00:38<00:00,  5.45it/s]Epoch: 2, train for the 103-th batch, train loss: 0.608587384223938:  43%|█████▏      | 102/237 [00:17<00:22,  5.92it/s]Epoch: 4, train for the 85-th batch, train loss: 0.5148603916168213:  58%|███████▍     | 84/146 [00:11<00:08,  7.64it/s]Epoch: 2, train for the 238-th batch, train loss: 0.5244755148887634:  99%|██████████▊| 238/241 [00:38<00:00,  5.36it/s]Epoch: 4, train for the 85-th batch, train loss: 0.5148603916168213:  58%|███████▌     | 85/146 [00:11<00:07,  7.84it/s]Epoch: 2, train for the 103-th batch, train loss: 0.608587384223938:  43%|█████▏      | 103/237 [00:17<00:24,  5.48it/s]Epoch: 1, train for the 327-th batch, train loss: 0.4924331307411194:  85%|█████████▎ | 326/383 [01:39<00:26,  2.13it/s]Epoch: 5, train for the 23-th batch, train loss: 0.49830031394958496:  18%|██▏         | 22/119 [00:03<00:13,  7.26it/s]Epoch: 5, train for the 23-th batch, train loss: 0.49830031394958496:  19%|██▎         | 23/119 [00:03<00:13,  7.38it/s]Epoch: 1, train for the 327-th batch, train loss: 0.4924331307411194:  85%|█████████▍ | 327/383 [01:39<00:24,  2.32it/s]Epoch: 3, train for the 134-th batch, train loss: 0.4758245348930359:  88%|█████████▋ | 133/151 [00:22<00:03,  5.41it/s]Epoch: 4, train for the 86-th batch, train loss: 0.5138418674468994:  58%|███████▌     | 85/146 [00:11<00:07,  7.84it/s]Epoch: 4, train for the 86-th batch, train loss: 0.5138418674468994:  59%|███████▋     | 86/146 [00:11<00:07,  7.69it/s]Epoch: 3, train for the 134-th batch, train loss: 0.4758245348930359:  89%|█████████▊ | 134/151 [00:22<00:03,  5.34it/s]Epoch: 2, train for the 239-th batch, train loss: 0.576420247554779:  99%|███████████▊| 238/241 [00:39<00:00,  5.36it/s]Epoch: 2, train for the 104-th batch, train loss: 0.5747581124305725:  43%|████▊      | 103/237 [00:17<00:24,  5.48it/s]Epoch: 2, train for the 239-th batch, train loss: 0.576420247554779:  99%|███████████▉| 239/241 [00:39<00:00,  5.31it/s]Epoch: 2, train for the 104-th batch, train loss: 0.5747581124305725:  44%|████▊      | 104/237 [00:17<00:23,  5.58it/s]Epoch: 5, train for the 24-th batch, train loss: 0.48006224632263184:  19%|██▎         | 23/119 [00:03<00:13,  7.38it/s]Epoch: 5, train for the 24-th batch, train loss: 0.48006224632263184:  20%|██▍         | 24/119 [00:03<00:12,  7.51it/s]Epoch: 4, train for the 87-th batch, train loss: 0.5262241363525391:  59%|███████▋     | 86/146 [00:11<00:07,  7.69it/s]Epoch: 4, train for the 87-th batch, train loss: 0.5262241363525391:  60%|███████▋     | 87/146 [00:11<00:07,  7.74it/s]Epoch: 2, train for the 240-th batch, train loss: 0.46409669518470764:  99%|█████████▉| 239/241 [00:39<00:00,  5.31it/s]Epoch: 3, train for the 135-th batch, train loss: 0.45770391821861267:  89%|████████▊ | 134/151 [00:23<00:03,  5.34it/s]Epoch: 5, train for the 25-th batch, train loss: 0.4497899115085602:  20%|██▌          | 24/119 [00:03<00:12,  7.51it/s]Epoch: 2, train for the 240-th batch, train loss: 0.46409669518470764: 100%|█████████▉| 240/241 [00:39<00:00,  5.40it/s]Epoch: 5, train for the 25-th batch, train loss: 0.4497899115085602:  21%|██▋          | 25/119 [00:03<00:13,  7.05it/s]Epoch: 3, train for the 135-th batch, train loss: 0.45770391821861267:  89%|████████▉ | 135/151 [00:23<00:03,  4.95it/s]Epoch: 4, train for the 88-th batch, train loss: 0.5684639811515808:  60%|███████▋     | 87/146 [00:11<00:07,  7.74it/s]Epoch: 4, train for the 88-th batch, train loss: 0.5684639811515808:  60%|███████▊     | 88/146 [00:11<00:07,  7.49it/s]Epoch: 1, train for the 328-th batch, train loss: 0.40109938383102417:  85%|████████▌ | 327/383 [01:40<00:24,  2.32it/s]Epoch: 2, train for the 105-th batch, train loss: 0.5505339503288269:  44%|████▊      | 104/237 [00:17<00:23,  5.58it/s]Epoch: 2, train for the 105-th batch, train loss: 0.5505339503288269:  44%|████▊      | 105/237 [00:17<00:27,  4.75it/s]Epoch: 5, train for the 26-th batch, train loss: 0.3703925311565399:  21%|██▋          | 25/119 [00:03<00:13,  7.05it/s]Epoch: 5, train for the 26-th batch, train loss: 0.3703925311565399:  22%|██▊          | 26/119 [00:03<00:12,  7.43it/s]Epoch: 1, train for the 328-th batch, train loss: 0.40109938383102417:  86%|████████▌ | 328/383 [01:40<00:23,  2.37it/s]Epoch: 2, train for the 241-th batch, train loss: 0.555003821849823: 100%|███████████▉| 240/241 [00:39<00:00,  5.40it/s]Epoch: 4, train for the 89-th batch, train loss: 0.5413697957992554:  60%|███████▊     | 88/146 [00:11<00:07,  7.49it/s]Epoch: 4, train for the 89-th batch, train loss: 0.5413697957992554:  61%|███████▉     | 89/146 [00:11<00:07,  7.38it/s]Epoch: 3, train for the 136-th batch, train loss: 0.52138352394104:  89%|███████████▌ | 135/151 [00:23<00:03,  4.95it/s]Epoch: 2, train for the 241-th batch, train loss: 0.555003821849823: 100%|████████████| 241/241 [00:39<00:00,  5.30it/s]Epoch: 2, train for the 241-th batch, train loss: 0.555003821849823: 100%|████████████| 241/241 [00:39<00:00,  6.11it/s]
Epoch: 3, train for the 136-th batch, train loss: 0.52138352394104:  90%|███████████▋ | 136/151 [00:23<00:02,  5.14it/s]Epoch: 5, train for the 27-th batch, train loss: 0.47680604457855225:  22%|██▌         | 26/119 [00:03<00:12,  7.43it/s]Epoch: 5, train for the 27-th batch, train loss: 0.47680604457855225:  23%|██▋         | 27/119 [00:03<00:12,  7.65it/s]Epoch: 2, train for the 106-th batch, train loss: 0.5846367478370667:  44%|████▊      | 105/237 [00:17<00:27,  4.75it/s]Epoch: 4, train for the 90-th batch, train loss: 0.5468162894248962:  61%|███████▉     | 89/146 [00:11<00:07,  7.38it/s]Epoch: 2, train for the 106-th batch, train loss: 0.5846367478370667:  45%|████▉      | 106/237 [00:17<00:27,  4.83it/s]Epoch: 4, train for the 90-th batch, train loss: 0.5468162894248962:  62%|████████     | 90/146 [00:11<00:07,  7.52it/s]Epoch: 5, train for the 28-th batch, train loss: 0.46301984786987305:  23%|██▋         | 27/119 [00:03<00:12,  7.65it/s]Epoch: 5, train for the 28-th batch, train loss: 0.46301984786987305:  24%|██▊         | 28/119 [00:03<00:11,  7.73it/s]Epoch: 1, train for the 329-th batch, train loss: 0.38760054111480713:  86%|████████▌ | 328/383 [01:40<00:23,  2.37it/s]  0%|                                                                                            | 0/72 [00:00<?, ?it/s]Epoch: 3, train for the 137-th batch, train loss: 0.5298426151275635:  90%|█████████▉ | 136/151 [00:23<00:02,  5.14it/s]Epoch: 3, train for the 137-th batch, train loss: 0.5298426151275635:  91%|█████████▉ | 137/151 [00:23<00:02,  5.08it/s]Epoch: 4, train for the 91-th batch, train loss: 0.5172170996665955:  62%|████████     | 90/146 [00:12<00:07,  7.52it/s]evaluate for the 1-th batch, evaluate loss: 0.5189846158027649:   0%|                            | 0/72 [00:00<?, ?it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5903162360191345:  45%|████▉      | 106/237 [00:18<00:27,  4.83it/s]Epoch: 1, train for the 329-th batch, train loss: 0.38760054111480713:  86%|████████▌ | 329/383 [01:40<00:21,  2.57it/s]Epoch: 4, train for the 91-th batch, train loss: 0.5172170996665955:  62%|████████     | 91/146 [00:12<00:07,  7.22it/s]Epoch: 2, train for the 107-th batch, train loss: 0.5903162360191345:  45%|████▉      | 107/237 [00:18<00:25,  5.13it/s]evaluate for the 2-th batch, evaluate loss: 0.5078707933425903:   0%|                            | 0/72 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5078707933425903:   3%|▌                   | 2/72 [00:00<00:04, 16.39it/s]Epoch: 4, train for the 92-th batch, train loss: 0.548954427242279:  62%|████████▋     | 91/146 [00:12<00:07,  7.22it/s]Epoch: 4, train for the 92-th batch, train loss: 0.548954427242279:  63%|████████▊     | 92/146 [00:12<00:07,  7.33it/s]Epoch: 3, train for the 138-th batch, train loss: 0.5409546494483948:  91%|█████████▉ | 137/151 [00:23<00:02,  5.08it/s]evaluate for the 3-th batch, evaluate loss: 0.4948221445083618:   3%|▌                   | 2/72 [00:00<00:04, 16.39it/s]Epoch: 3, train for the 138-th batch, train loss: 0.5409546494483948:  91%|██████████ | 138/151 [00:23<00:02,  4.93it/s]Epoch: 2, train for the 108-th batch, train loss: 0.6107149124145508:  45%|████▉      | 107/237 [00:18<00:25,  5.13it/s]evaluate for the 4-th batch, evaluate loss: 0.5494158864021301:   3%|▌                   | 2/72 [00:00<00:04, 16.39it/s]evaluate for the 4-th batch, evaluate loss: 0.5494158864021301:   6%|█                   | 4/72 [00:00<00:04, 14.80it/s]Epoch: 2, train for the 108-th batch, train loss: 0.6107149124145508:  46%|█████      | 108/237 [00:18<00:24,  5.18it/s]Epoch: 4, train for the 93-th batch, train loss: 0.5216746926307678:  63%|████████▏    | 92/146 [00:12<00:07,  7.33it/s]Epoch: 4, train for the 93-th batch, train loss: 0.5216746926307678:  64%|████████▎    | 93/146 [00:12<00:06,  7.84it/s]evaluate for the 5-th batch, evaluate loss: 0.49452269077301025:   6%|█                  | 4/72 [00:00<00:04, 14.80it/s]Epoch: 5, train for the 29-th batch, train loss: 0.4410659968852997:  24%|███          | 28/119 [00:04<00:11,  7.73it/s]Epoch: 5, train for the 29-th batch, train loss: 0.4410659968852997:  24%|███▏         | 29/119 [00:04<00:17,  5.17it/s]Epoch: 1, train for the 330-th batch, train loss: 0.44871869683265686:  86%|████████▌ | 329/383 [01:40<00:21,  2.57it/s]Epoch: 2, train for the 109-th batch, train loss: 0.5950254797935486:  46%|█████      | 108/237 [00:18<00:24,  5.18it/s]Epoch: 3, train for the 139-th batch, train loss: 0.5250626802444458:  91%|██████████ | 138/151 [00:23<00:02,  4.93it/s]evaluate for the 6-th batch, evaluate loss: 0.5275088548660278:   6%|█                   | 4/72 [00:00<00:04, 14.80it/s]evaluate for the 6-th batch, evaluate loss: 0.5275088548660278:   8%|█▋                  | 6/72 [00:00<00:04, 14.27it/s]Epoch: 3, train for the 139-th batch, train loss: 0.5250626802444458:  92%|██████████▏| 139/151 [00:23<00:02,  5.02it/s]Epoch: 2, train for the 109-th batch, train loss: 0.5950254797935486:  46%|█████      | 109/237 [00:18<00:23,  5.49it/s]Epoch: 1, train for the 330-th batch, train loss: 0.44871869683265686:  86%|████████▌ | 330/383 [01:40<00:20,  2.61it/s]evaluate for the 7-th batch, evaluate loss: 0.5403161644935608:   8%|█▋                  | 6/72 [00:00<00:04, 14.27it/s]Epoch: 4, train for the 94-th batch, train loss: 0.5272306799888611:  64%|████████▎    | 93/146 [00:12<00:06,  7.84it/s]Epoch: 5, train for the 30-th batch, train loss: 0.410643070936203:  24%|███▍          | 29/119 [00:04<00:17,  5.17it/s]Epoch: 5, train for the 30-th batch, train loss: 0.410643070936203:  25%|███▌          | 30/119 [00:04<00:16,  5.47it/s]Epoch: 4, train for the 94-th batch, train loss: 0.5272306799888611:  64%|████████▎    | 94/146 [00:12<00:07,  7.08it/s]evaluate for the 8-th batch, evaluate loss: 0.554551899433136:   8%|█▊                   | 6/72 [00:00<00:04, 14.27it/s]evaluate for the 8-th batch, evaluate loss: 0.554551899433136:  11%|██▎                  | 8/72 [00:00<00:04, 14.67it/s]Epoch: 2, train for the 110-th batch, train loss: 0.562973141670227:  46%|█████▌      | 109/237 [00:18<00:23,  5.49it/s]Epoch: 3, train for the 140-th batch, train loss: 0.5056357383728027:  92%|██████████▏| 139/151 [00:24<00:02,  5.02it/s]evaluate for the 9-th batch, evaluate loss: 0.5208155512809753:  11%|██▏                 | 8/72 [00:00<00:04, 14.67it/s]Epoch: 2, train for the 110-th batch, train loss: 0.562973141670227:  46%|█████▌      | 110/237 [00:18<00:23,  5.32it/s]Epoch: 3, train for the 140-th batch, train loss: 0.5056357383728027:  93%|██████████▏| 140/151 [00:24<00:02,  4.92it/s]Epoch: 5, train for the 31-th batch, train loss: 0.4680210053920746:  25%|███▎         | 30/119 [00:04<00:16,  5.47it/s]Epoch: 5, train for the 31-th batch, train loss: 0.4680210053920746:  26%|███▍         | 31/119 [00:04<00:16,  5.38it/s]Epoch: 4, train for the 95-th batch, train loss: 0.5258485674858093:  64%|████████▎    | 94/146 [00:12<00:07,  7.08it/s]evaluate for the 10-th batch, evaluate loss: 0.5089423060417175:  11%|██                 | 8/72 [00:00<00:04, 14.67it/s]evaluate for the 10-th batch, evaluate loss: 0.5089423060417175:  14%|██▌               | 10/72 [00:00<00:04, 14.48it/s]Epoch: 4, train for the 95-th batch, train loss: 0.5258485674858093:  65%|████████▍    | 95/146 [00:12<00:08,  5.98it/s]evaluate for the 11-th batch, evaluate loss: 0.5085877180099487:  14%|██▌               | 10/72 [00:00<00:04, 14.48it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5938394069671631:  46%|█████      | 110/237 [00:18<00:23,  5.32it/s]Epoch: 5, train for the 32-th batch, train loss: 0.41866132616996765:  26%|███▏        | 31/119 [00:04<00:16,  5.38it/s]Epoch: 2, train for the 111-th batch, train loss: 0.5938394069671631:  47%|█████▏     | 111/237 [00:18<00:23,  5.38it/s]Epoch: 5, train for the 32-th batch, train loss: 0.41866132616996765:  27%|███▏        | 32/119 [00:04<00:15,  5.79it/s]Epoch: 3, train for the 141-th batch, train loss: 0.512523353099823:  93%|███████████▏| 140/151 [00:24<00:02,  4.92it/s]evaluate for the 12-th batch, evaluate loss: 0.5132637023925781:  14%|██▌               | 10/72 [00:00<00:04, 14.48it/s]evaluate for the 12-th batch, evaluate loss: 0.5132637023925781:  17%|███               | 12/72 [00:00<00:04, 14.29it/s]Epoch: 3, train for the 141-th batch, train loss: 0.512523353099823:  93%|███████████▏| 141/151 [00:24<00:02,  4.87it/s]Epoch: 1, train for the 331-th batch, train loss: 0.47563543915748596:  86%|████████▌ | 330/383 [01:41<00:20,  2.61it/s]evaluate for the 13-th batch, evaluate loss: 0.4518396556377411:  17%|███               | 12/72 [00:00<00:04, 14.29it/s]Epoch: 5, train for the 33-th batch, train loss: 0.46253103017807007:  27%|███▏        | 32/119 [00:04<00:15,  5.79it/s]Epoch: 1, train for the 331-th batch, train loss: 0.47563543915748596:  86%|████████▋ | 331/383 [01:41<00:21,  2.37it/s]Epoch: 5, train for the 33-th batch, train loss: 0.46253103017807007:  28%|███▎        | 33/119 [00:04<00:13,  6.24it/s]evaluate for the 14-th batch, evaluate loss: 0.415506511926651:  17%|███▏               | 12/72 [00:00<00:04, 14.29it/s]evaluate for the 14-th batch, evaluate loss: 0.415506511926651:  19%|███▋               | 14/72 [00:00<00:04, 14.46it/s]Epoch: 2, train for the 112-th batch, train loss: 0.5816929936408997:  47%|█████▏     | 111/237 [00:18<00:23,  5.38it/s]Epoch: 4, train for the 96-th batch, train loss: 0.49812665581703186:  65%|███████▊    | 95/146 [00:12<00:08,  5.98it/s]Epoch: 4, train for the 96-th batch, train loss: 0.49812665581703186:  66%|███████▉    | 96/146 [00:12<00:10,  4.97it/s]Epoch: 2, train for the 112-th batch, train loss: 0.5816929936408997:  47%|█████▏     | 112/237 [00:18<00:23,  5.39it/s]Epoch: 3, train for the 142-th batch, train loss: 0.46771490573883057:  93%|█████████▎| 141/151 [00:24<00:02,  4.87it/s]evaluate for the 15-th batch, evaluate loss: 0.46639716625213623:  19%|███▎             | 14/72 [00:01<00:04, 14.46it/s]Epoch: 3, train for the 142-th batch, train loss: 0.46771490573883057:  94%|█████████▍| 142/151 [00:24<00:01,  4.90it/s]evaluate for the 16-th batch, evaluate loss: 0.4908592998981476:  19%|███▌              | 14/72 [00:01<00:04, 14.46it/s]evaluate for the 16-th batch, evaluate loss: 0.4908592998981476:  22%|████              | 16/72 [00:01<00:03, 14.79it/s]Epoch: 4, train for the 97-th batch, train loss: 0.5336958765983582:  66%|████████▌    | 96/146 [00:13<00:10,  4.97it/s]Epoch: 4, train for the 97-th batch, train loss: 0.5336958765983582:  66%|████████▋    | 97/146 [00:13<00:09,  5.36it/s]evaluate for the 17-th batch, evaluate loss: 0.46329933404922485:  22%|███▊             | 16/72 [00:01<00:03, 14.79it/s]Epoch: 5, train for the 34-th batch, train loss: 0.4533964693546295:  28%|███▌         | 33/119 [00:05<00:13,  6.24it/s]Epoch: 2, train for the 113-th batch, train loss: 0.593830406665802:  47%|█████▋      | 112/237 [00:19<00:23,  5.39it/s]Epoch: 5, train for the 34-th batch, train loss: 0.4533964693546295:  29%|███▋         | 34/119 [00:05<00:15,  5.44it/s]Epoch: 2, train for the 113-th batch, train loss: 0.593830406665802:  48%|█████▋      | 113/237 [00:19<00:24,  5.14it/s]evaluate for the 18-th batch, evaluate loss: 0.43032199144363403:  22%|███▊             | 16/72 [00:01<00:03, 14.79it/s]evaluate for the 18-th batch, evaluate loss: 0.43032199144363403:  25%|████▎            | 18/72 [00:01<00:03, 14.64it/s]Epoch: 3, train for the 143-th batch, train loss: 0.4368686079978943:  94%|██████████▎| 142/151 [00:24<00:01,  4.90it/s]evaluate for the 19-th batch, evaluate loss: 0.4775802493095398:  25%|████▌             | 18/72 [00:01<00:03, 14.64it/s]Epoch: 3, train for the 143-th batch, train loss: 0.4368686079978943:  95%|██████████▍| 143/151 [00:24<00:01,  4.71it/s]Epoch: 4, train for the 98-th batch, train loss: 0.5093643665313721:  66%|████████▋    | 97/146 [00:13<00:09,  5.36it/s]Epoch: 1, train for the 332-th batch, train loss: 0.4950055480003357:  86%|█████████▌ | 331/383 [01:41<00:21,  2.37it/s]Epoch: 4, train for the 98-th batch, train loss: 0.5093643665313721:  67%|████████▋    | 98/146 [00:13<00:08,  5.61it/s]Epoch: 5, train for the 35-th batch, train loss: 0.44316378235816956:  29%|███▍        | 34/119 [00:05<00:15,  5.44it/s]Epoch: 5, train for the 35-th batch, train loss: 0.44316378235816956:  29%|███▌        | 35/119 [00:05<00:14,  5.97it/s]Epoch: 2, train for the 114-th batch, train loss: 0.5634123682975769:  48%|█████▏     | 113/237 [00:19<00:24,  5.14it/s]evaluate for the 20-th batch, evaluate loss: 0.5000117421150208:  25%|████▌             | 18/72 [00:01<00:03, 14.64it/s]evaluate for the 20-th batch, evaluate loss: 0.5000117421150208:  28%|█████             | 20/72 [00:01<00:03, 14.53it/s]Epoch: 1, train for the 332-th batch, train loss: 0.4950055480003357:  87%|█████████▌ | 332/383 [01:41<00:21,  2.36it/s]Epoch: 2, train for the 114-th batch, train loss: 0.5634123682975769:  48%|█████▎     | 114/237 [00:19<00:23,  5.30it/s]Epoch: 4, train for the 99-th batch, train loss: 0.517841637134552:  67%|█████████▍    | 98/146 [00:13<00:08,  5.61it/s]Epoch: 4, train for the 99-th batch, train loss: 0.517841637134552:  68%|█████████▍    | 99/146 [00:13<00:08,  5.75it/s]Epoch: 5, train for the 36-th batch, train loss: 0.4223635792732239:  29%|███▊         | 35/119 [00:05<00:14,  5.97it/s]evaluate for the 21-th batch, evaluate loss: 0.5504881739616394:  28%|█████             | 20/72 [00:01<00:03, 14.53it/s]Epoch: 3, train for the 144-th batch, train loss: 0.4536634385585785:  95%|██████████▍| 143/151 [00:25<00:01,  4.71it/s]Epoch: 5, train for the 36-th batch, train loss: 0.4223635792732239:  30%|███▉         | 36/119 [00:05<00:13,  6.01it/s]Epoch: 3, train for the 144-th batch, train loss: 0.4536634385585785:  95%|██████████▍| 144/151 [00:25<00:01,  4.59it/s]evaluate for the 22-th batch, evaluate loss: 0.47101226449012756:  28%|████▋            | 20/72 [00:01<00:03, 14.53it/s]evaluate for the 22-th batch, evaluate loss: 0.47101226449012756:  31%|█████▏           | 22/72 [00:01<00:03, 13.10it/s]Epoch: 2, train for the 115-th batch, train loss: 0.5736050605773926:  48%|█████▎     | 114/237 [00:19<00:23,  5.30it/s]Epoch: 2, train for the 115-th batch, train loss: 0.5736050605773926:  49%|█████▎     | 115/237 [00:19<00:24,  5.07it/s]evaluate for the 23-th batch, evaluate loss: 0.5000098347663879:  31%|█████▌            | 22/72 [00:01<00:03, 13.10it/s]Epoch: 4, train for the 100-th batch, train loss: 0.5483262538909912:  68%|████████▏   | 99/146 [00:13<00:08,  5.75it/s]Epoch: 1, train for the 333-th batch, train loss: 0.4850718677043915:  87%|█████████▌ | 332/383 [01:42<00:21,  2.36it/s]Epoch: 4, train for the 100-th batch, train loss: 0.5483262538909912:  68%|███████▌   | 100/146 [00:13<00:07,  5.92it/s]Epoch: 5, train for the 37-th batch, train loss: 0.46131277084350586:  30%|███▋        | 36/119 [00:05<00:13,  6.01it/s]Epoch: 5, train for the 37-th batch, train loss: 0.46131277084350586:  31%|███▋        | 37/119 [00:05<00:13,  5.99it/s]Epoch: 3, train for the 145-th batch, train loss: 0.4806283116340637:  95%|██████████▍| 144/151 [00:25<00:01,  4.59it/s]evaluate for the 24-th batch, evaluate loss: 0.46741241216659546:  31%|█████▏           | 22/72 [00:01<00:03, 13.10it/s]evaluate for the 24-th batch, evaluate loss: 0.46741241216659546:  33%|█████▋           | 24/72 [00:01<00:03, 12.87it/s]Epoch: 1, train for the 333-th batch, train loss: 0.4850718677043915:  87%|█████████▌ | 333/383 [01:42<00:20,  2.48it/s]Epoch: 3, train for the 145-th batch, train loss: 0.4806283116340637:  96%|██████████▌| 145/151 [00:25<00:01,  4.50it/s]evaluate for the 25-th batch, evaluate loss: 0.5901476740837097:  33%|██████            | 24/72 [00:01<00:03, 12.87it/s]Epoch: 2, train for the 116-th batch, train loss: 0.6097560524940491:  49%|█████▎     | 115/237 [00:19<00:24,  5.07it/s]Epoch: 2, train for the 116-th batch, train loss: 0.6097560524940491:  49%|█████▍     | 116/237 [00:19<00:23,  5.13it/s]evaluate for the 26-th batch, evaluate loss: 0.4402758777141571:  33%|██████            | 24/72 [00:01<00:03, 12.87it/s]evaluate for the 26-th batch, evaluate loss: 0.4402758777141571:  36%|██████▌           | 26/72 [00:01<00:03, 14.42it/s]Epoch: 4, train for the 101-th batch, train loss: 0.5023654699325562:  68%|███████▌   | 100/146 [00:13<00:07,  5.92it/s]Epoch: 5, train for the 38-th batch, train loss: 0.41730010509490967:  31%|███▋        | 37/119 [00:05<00:13,  5.99it/s]Epoch: 5, train for the 38-th batch, train loss: 0.41730010509490967:  32%|███▊        | 38/119 [00:05<00:13,  5.82it/s]Epoch: 4, train for the 101-th batch, train loss: 0.5023654699325562:  69%|███████▌   | 101/146 [00:13<00:08,  5.48it/s]Epoch: 3, train for the 146-th batch, train loss: 0.467035710811615:  96%|███████████▌| 145/151 [00:25<00:01,  4.50it/s]evaluate for the 27-th batch, evaluate loss: 0.4155304729938507:  36%|██████▌           | 26/72 [00:01<00:03, 14.42it/s]Epoch: 3, train for the 146-th batch, train loss: 0.467035710811615:  97%|███████████▌| 146/151 [00:25<00:01,  4.70it/s]evaluate for the 28-th batch, evaluate loss: 0.4529842734336853:  36%|██████▌           | 26/72 [00:01<00:03, 14.42it/s]evaluate for the 28-th batch, evaluate loss: 0.4529842734336853:  39%|███████           | 28/72 [00:01<00:03, 14.28it/s]Epoch: 5, train for the 39-th batch, train loss: 0.48387351632118225:  32%|███▊        | 38/119 [00:05<00:13,  5.82it/s]Epoch: 5, train for the 39-th batch, train loss: 0.48387351632118225:  33%|███▉        | 39/119 [00:05<00:13,  5.93it/s]Epoch: 4, train for the 102-th batch, train loss: 0.464898943901062:  69%|████████▎   | 101/146 [00:13<00:08,  5.48it/s]Epoch: 2, train for the 117-th batch, train loss: 0.5474609136581421:  49%|█████▍     | 116/237 [00:20<00:23,  5.13it/s]evaluate for the 29-th batch, evaluate loss: 0.4909021556377411:  39%|███████           | 28/72 [00:02<00:03, 14.28it/s]Epoch: 4, train for the 102-th batch, train loss: 0.464898943901062:  70%|████████▍   | 102/146 [00:13<00:08,  5.50it/s]Epoch: 2, train for the 117-th batch, train loss: 0.5474609136581421:  49%|█████▍     | 117/237 [00:20<00:25,  4.74it/s]evaluate for the 30-th batch, evaluate loss: 0.4856899678707123:  39%|███████           | 28/72 [00:02<00:03, 14.28it/s]evaluate for the 30-th batch, evaluate loss: 0.4856899678707123:  42%|███████▌          | 30/72 [00:02<00:02, 14.22it/s]Epoch: 3, train for the 147-th batch, train loss: 0.47243189811706543:  97%|█████████▋| 146/151 [00:25<00:01,  4.70it/s]Epoch: 3, train for the 147-th batch, train loss: 0.47243189811706543:  97%|█████████▋| 147/151 [00:25<00:00,  4.87it/s]Epoch: 5, train for the 40-th batch, train loss: 0.4399627447128296:  33%|████▎        | 39/119 [00:06<00:13,  5.93it/s]Epoch: 5, train for the 40-th batch, train loss: 0.4399627447128296:  34%|████▎        | 40/119 [00:06<00:12,  6.13it/s]Epoch: 1, train for the 334-th batch, train loss: 0.4720137417316437:  87%|█████████▌ | 333/383 [01:42<00:20,  2.48it/s]evaluate for the 31-th batch, evaluate loss: 0.5398223400115967:  42%|███████▌          | 30/72 [00:02<00:02, 14.22it/s]Epoch: 4, train for the 103-th batch, train loss: 0.48083388805389404:  70%|██████▉   | 102/146 [00:14<00:08,  5.50it/s]Epoch: 2, train for the 118-th batch, train loss: 0.5436191558837891:  49%|█████▍     | 117/237 [00:20<00:25,  4.74it/s]Epoch: 4, train for the 103-th batch, train loss: 0.48083388805389404:  71%|███████   | 103/146 [00:14<00:07,  5.58it/s]evaluate for the 32-th batch, evaluate loss: 0.5941229462623596:  42%|███████▌          | 30/72 [00:02<00:02, 14.22it/s]evaluate for the 32-th batch, evaluate loss: 0.5941229462623596:  44%|████████          | 32/72 [00:02<00:02, 15.16it/s]Epoch: 2, train for the 118-th batch, train loss: 0.5436191558837891:  50%|█████▍     | 118/237 [00:20<00:23,  4.96it/s]Epoch: 1, train for the 334-th batch, train loss: 0.4720137417316437:  87%|█████████▌ | 334/383 [01:42<00:21,  2.32it/s]Epoch: 5, train for the 41-th batch, train loss: 0.4363066256046295:  34%|████▎        | 40/119 [00:06<00:12,  6.13it/s]Epoch: 5, train for the 41-th batch, train loss: 0.4363066256046295:  34%|████▍        | 41/119 [00:06<00:11,  6.66it/s]Epoch: 3, train for the 148-th batch, train loss: 0.4736814796924591:  97%|██████████▋| 147/151 [00:25<00:00,  4.87it/s]evaluate for the 33-th batch, evaluate loss: 0.5053139925003052:  44%|████████          | 32/72 [00:02<00:02, 15.16it/s]Epoch: 3, train for the 148-th batch, train loss: 0.4736814796924591:  98%|██████████▊| 148/151 [00:25<00:00,  5.15it/s]Epoch: 4, train for the 104-th batch, train loss: 0.5039611458778381:  71%|███████▊   | 103/146 [00:14<00:07,  5.58it/s]Epoch: 4, train for the 104-th batch, train loss: 0.5039611458778381:  71%|███████▊   | 104/146 [00:14<00:06,  6.00it/s]evaluate for the 34-th batch, evaluate loss: 0.5581550598144531:  44%|████████          | 32/72 [00:02<00:02, 15.16it/s]evaluate for the 34-th batch, evaluate loss: 0.5581550598144531:  47%|████████▌         | 34/72 [00:02<00:02, 15.32it/s]Epoch: 5, train for the 42-th batch, train loss: 0.5022300481796265:  34%|████▍        | 41/119 [00:06<00:11,  6.66it/s]Epoch: 2, train for the 119-th batch, train loss: 0.6000562310218811:  50%|█████▍     | 118/237 [00:20<00:23,  4.96it/s]Epoch: 5, train for the 42-th batch, train loss: 0.5022300481796265:  35%|████▌        | 42/119 [00:06<00:11,  6.79it/s]evaluate for the 35-th batch, evaluate loss: 0.48557162284851074:  47%|████████         | 34/72 [00:02<00:02, 15.32it/s]Epoch: 2, train for the 119-th batch, train loss: 0.6000562310218811:  50%|█████▌     | 119/237 [00:20<00:24,  4.91it/s]Epoch: 3, train for the 149-th batch, train loss: 0.4036724269390106:  98%|██████████▊| 148/151 [00:25<00:00,  5.15it/s]Epoch: 4, train for the 105-th batch, train loss: 0.5223158001899719:  71%|███████▊   | 104/146 [00:14<00:06,  6.00it/s]Epoch: 4, train for the 105-th batch, train loss: 0.5223158001899719:  72%|███████▉   | 105/146 [00:14<00:06,  6.18it/s]Epoch: 3, train for the 149-th batch, train loss: 0.4036724269390106:  99%|██████████▊| 149/151 [00:26<00:00,  5.20it/s]Epoch: 1, train for the 335-th batch, train loss: 0.5301662087440491:  87%|█████████▌ | 334/383 [01:42<00:21,  2.32it/s]Epoch: 5, train for the 43-th batch, train loss: 0.472711443901062:  35%|████▉         | 42/119 [00:06<00:11,  6.79it/s]Epoch: 5, train for the 43-th batch, train loss: 0.472711443901062:  36%|█████         | 43/119 [00:06<00:10,  7.21it/s]Epoch: 1, train for the 335-th batch, train loss: 0.5301662087440491:  87%|█████████▌ | 335/383 [01:43<00:19,  2.47it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5961354970932007:  50%|█████▌     | 119/237 [00:20<00:24,  4.91it/s]Epoch: 2, train for the 120-th batch, train loss: 0.5961354970932007:  51%|█████▌     | 120/237 [00:20<00:22,  5.14it/s]Epoch: 4, train for the 106-th batch, train loss: 0.47116920351982117:  72%|███████▏  | 105/146 [00:14<00:06,  6.18it/s]Epoch: 3, train for the 150-th batch, train loss: 0.5156528949737549:  99%|██████████▊| 149/151 [00:26<00:00,  5.20it/s]evaluate for the 36-th batch, evaluate loss: 0.514809250831604:  47%|████████▉          | 34/72 [00:02<00:02, 15.32it/s]evaluate for the 36-th batch, evaluate loss: 0.514809250831604:  50%|█████████▌         | 36/72 [00:02<00:03, 11.07it/s]Epoch: 4, train for the 106-th batch, train loss: 0.47116920351982117:  73%|███████▎  | 106/146 [00:14<00:06,  6.20it/s]Epoch: 3, train for the 150-th batch, train loss: 0.5156528949737549:  99%|██████████▉| 150/151 [00:26<00:00,  5.32it/s]Epoch: 5, train for the 44-th batch, train loss: 0.4212352931499481:  36%|████▋        | 43/119 [00:06<00:10,  7.21it/s]Epoch: 5, train for the 44-th batch, train loss: 0.4212352931499481:  37%|████▊        | 44/119 [00:06<00:10,  6.85it/s]evaluate for the 37-th batch, evaluate loss: 0.4926665723323822:  50%|█████████         | 36/72 [00:02<00:03, 11.07it/s]Epoch: 2, train for the 121-th batch, train loss: 0.595091700553894:  51%|██████      | 120/237 [00:20<00:22,  5.14it/s]Epoch: 2, train for the 121-th batch, train loss: 0.595091700553894:  51%|██████▏     | 121/237 [00:20<00:22,  5.24it/s]Epoch: 4, train for the 107-th batch, train loss: 0.5515889525413513:  73%|███████▉   | 106/146 [00:14<00:06,  6.20it/s]Epoch: 1, train for the 336-th batch, train loss: 0.4918365776538849:  87%|█████████▌ | 335/383 [01:43<00:19,  2.47it/s]evaluate for the 38-th batch, evaluate loss: 0.4093800187110901:  50%|█████████         | 36/72 [00:02<00:03, 11.07it/s]evaluate for the 38-th batch, evaluate loss: 0.4093800187110901:  53%|█████████▌        | 38/72 [00:02<00:03, 11.01it/s]Epoch: 4, train for the 107-th batch, train loss: 0.5515889525413513:  73%|████████   | 107/146 [00:14<00:06,  5.95it/s]Epoch: 5, train for the 45-th batch, train loss: 0.4104548990726471:  37%|████▊        | 44/119 [00:06<00:10,  6.85it/s]Epoch: 3, train for the 151-th batch, train loss: 0.4809565544128418:  99%|██████████▉| 150/151 [00:26<00:00,  5.32it/s]Epoch: 5, train for the 45-th batch, train loss: 0.4104548990726471:  38%|████▉        | 45/119 [00:06<00:10,  6.78it/s]Epoch: 3, train for the 151-th batch, train loss: 0.4809565544128418: 100%|███████████| 151/151 [00:26<00:00,  5.17it/s]Epoch: 3, train for the 151-th batch, train loss: 0.4809565544128418: 100%|███████████| 151/151 [00:26<00:00,  5.72it/s]
Epoch: 1, train for the 336-th batch, train loss: 0.4918365776538849:  88%|█████████▋ | 336/383 [01:43<00:17,  2.64it/s]evaluate for the 39-th batch, evaluate loss: 0.431010365486145:  53%|██████████         | 38/72 [00:02<00:03, 11.01it/s]Epoch: 2, train for the 122-th batch, train loss: 0.5703713893890381:  51%|█████▌     | 121/237 [00:20<00:22,  5.24it/s]Epoch: 2, train for the 122-th batch, train loss: 0.5703713893890381:  51%|█████▋     | 122/237 [00:20<00:21,  5.26it/s]Epoch: 4, train for the 108-th batch, train loss: 0.5088242888450623:  73%|████████   | 107/146 [00:14<00:06,  5.95it/s]Epoch: 4, train for the 108-th batch, train loss: 0.5088242888450623:  74%|████████▏  | 108/146 [00:14<00:06,  6.03it/s]Epoch: 5, train for the 46-th batch, train loss: 0.4406110942363739:  38%|████▉        | 45/119 [00:06<00:10,  6.78it/s]Epoch: 5, train for the 46-th batch, train loss: 0.4406110942363739:  39%|█████        | 46/119 [00:06<00:11,  6.46it/s]evaluate for the 40-th batch, evaluate loss: 0.5020065307617188:  53%|█████████▌        | 38/72 [00:03<00:03, 11.01it/s]evaluate for the 40-th batch, evaluate loss: 0.5020065307617188:  56%|██████████        | 40/72 [00:03<00:03,  9.46it/s]Epoch: 4, train for the 109-th batch, train loss: 0.5228656530380249:  74%|████████▏  | 108/146 [00:15<00:06,  6.03it/s]Epoch: 2, train for the 123-th batch, train loss: 0.55635005235672:  51%|██████▋      | 122/237 [00:21<00:21,  5.26it/s]Epoch: 4, train for the 109-th batch, train loss: 0.5228656530380249:  75%|████████▏  | 109/146 [00:15<00:05,  6.30it/s]Epoch: 2, train for the 123-th batch, train loss: 0.55635005235672:  52%|██████▋      | 123/237 [00:21<00:20,  5.46it/s]Epoch: 5, train for the 47-th batch, train loss: 0.4801647365093231:  39%|█████        | 46/119 [00:07<00:11,  6.46it/s]Epoch: 5, train for the 47-th batch, train loss: 0.4801647365093231:  39%|█████▏       | 47/119 [00:07<00:11,  6.49it/s]evaluate for the 41-th batch, evaluate loss: 0.5056845545768738:  56%|██████████        | 40/72 [00:03<00:03,  9.46it/s]Epoch: 1, train for the 337-th batch, train loss: 0.5736595392227173:  88%|█████████▋ | 336/383 [01:43<00:17,  2.64it/s]  0%|                                                                                            | 0/46 [00:00<?, ?it/s]evaluate for the 42-th batch, evaluate loss: 0.48244354128837585:  56%|█████████▍       | 40/72 [00:03<00:03,  9.46it/s]evaluate for the 42-th batch, evaluate loss: 0.48244354128837585:  58%|█████████▉       | 42/72 [00:03<00:02, 10.69it/s]Epoch: 1, train for the 337-th batch, train loss: 0.5736595392227173:  88%|█████████▋ | 337/383 [01:43<00:17,  2.67it/s]evaluate for the 1-th batch, evaluate loss: 0.5447002053260803:   0%|                            | 0/46 [00:00<?, ?it/s]Epoch: 4, train for the 110-th batch, train loss: 0.4673021733760834:  75%|████████▏  | 109/146 [00:15<00:05,  6.30it/s]Epoch: 4, train for the 110-th batch, train loss: 0.4673021733760834:  75%|████████▎  | 110/146 [00:15<00:05,  6.12it/s]Epoch: 5, train for the 48-th batch, train loss: 0.4534335136413574:  39%|█████▏       | 47/119 [00:07<00:11,  6.49it/s]Epoch: 5, train for the 48-th batch, train loss: 0.4534335136413574:  40%|█████▏       | 48/119 [00:07<00:11,  6.45it/s]Epoch: 2, train for the 124-th batch, train loss: 0.5194403529167175:  52%|█████▋     | 123/237 [00:21<00:20,  5.46it/s]evaluate for the 43-th batch, evaluate loss: 0.4882800579071045:  58%|██████████▌       | 42/72 [00:03<00:02, 10.69it/s]evaluate for the 2-th batch, evaluate loss: 0.5461058020591736:   0%|                            | 0/46 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5461058020591736:   4%|▊                   | 2/46 [00:00<00:03, 13.41it/s]Epoch: 2, train for the 124-th batch, train loss: 0.5194403529167175:  52%|█████▊     | 124/237 [00:21<00:23,  4.87it/s]evaluate for the 44-th batch, evaluate loss: 0.4798771142959595:  58%|██████████▌       | 42/72 [00:03<00:02, 10.69it/s]evaluate for the 44-th batch, evaluate loss: 0.4798771142959595:  61%|███████████       | 44/72 [00:03<00:02, 10.48it/s]Epoch: 4, train for the 111-th batch, train loss: 0.4849375784397125:  75%|████████▎  | 110/146 [00:15<00:05,  6.12it/s]evaluate for the 3-th batch, evaluate loss: 0.5439558029174805:   4%|▊                   | 2/46 [00:00<00:03, 13.41it/s]Epoch: 4, train for the 111-th batch, train loss: 0.4849375784397125:  76%|████████▎  | 111/146 [00:15<00:05,  6.22it/s]Epoch: 5, train for the 49-th batch, train loss: 0.4741632640361786:  40%|█████▏       | 48/119 [00:07<00:11,  6.45it/s]Epoch: 5, train for the 49-th batch, train loss: 0.4741632640361786:  41%|█████▎       | 49/119 [00:07<00:11,  6.36it/s]evaluate for the 4-th batch, evaluate loss: 0.5467556118965149:   4%|▊                   | 2/46 [00:00<00:03, 13.41it/s]evaluate for the 4-th batch, evaluate loss: 0.5467556118965149:   9%|█▋                  | 4/46 [00:00<00:03, 12.11it/s]Epoch: 2, train for the 125-th batch, train loss: 0.515495240688324:  52%|██████▎     | 124/237 [00:21<00:23,  4.87it/s]evaluate for the 5-th batch, evaluate loss: 0.5443625450134277:   9%|█▋                  | 4/46 [00:00<00:03, 12.11it/s]Epoch: 2, train for the 125-th batch, train loss: 0.515495240688324:  53%|██████▎     | 125/237 [00:21<00:22,  4.95it/s]evaluate for the 6-th batch, evaluate loss: 0.536718487739563:   9%|█▊                   | 4/46 [00:00<00:03, 12.11it/s]evaluate for the 6-th batch, evaluate loss: 0.536718487739563:  13%|██▋                  | 6/46 [00:00<00:02, 14.67it/s]Epoch: 4, train for the 112-th batch, train loss: 0.5406344532966614:  76%|████████▎  | 111/146 [00:15<00:05,  6.22it/s]Epoch: 5, train for the 50-th batch, train loss: 0.4390982687473297:  41%|█████▎       | 49/119 [00:07<00:11,  6.36it/s]Epoch: 4, train for the 112-th batch, train loss: 0.5406344532966614:  77%|████████▍  | 112/146 [00:15<00:05,  6.00it/s]Epoch: 5, train for the 50-th batch, train loss: 0.4390982687473297:  42%|█████▍       | 50/119 [00:07<00:10,  6.30it/s]Epoch: 1, train for the 338-th batch, train loss: 0.43489959836006165:  88%|████████▊ | 337/383 [01:44<00:17,  2.67it/s]evaluate for the 45-th batch, evaluate loss: 0.4880364239215851:  61%|███████████       | 44/72 [00:03<00:02, 10.48it/s]evaluate for the 7-th batch, evaluate loss: 0.557712197303772:  13%|██▋                  | 6/46 [00:00<00:02, 14.67it/s]Epoch: 2, train for the 126-th batch, train loss: 0.5672516226768494:  53%|█████▊     | 125/237 [00:21<00:22,  4.95it/s]evaluate for the 46-th batch, evaluate loss: 0.5583013892173767:  61%|███████████       | 44/72 [00:03<00:02, 10.48it/s]evaluate for the 46-th batch, evaluate loss: 0.5583013892173767:  64%|███████████▌      | 46/72 [00:03<00:03,  8.51it/s]evaluate for the 8-th batch, evaluate loss: 0.5710790753364563:  13%|██▌                 | 6/46 [00:00<00:02, 14.67it/s]evaluate for the 8-th batch, evaluate loss: 0.5710790753364563:  17%|███▍                | 8/46 [00:00<00:02, 13.78it/s]Epoch: 2, train for the 126-th batch, train loss: 0.5672516226768494:  53%|█████▊     | 126/237 [00:21<00:22,  5.04it/s]Epoch: 1, train for the 338-th batch, train loss: 0.43489959836006165:  88%|████████▊ | 338/383 [01:44<00:19,  2.36it/s]Epoch: 5, train for the 51-th batch, train loss: 0.4989388585090637:  42%|█████▍       | 50/119 [00:07<00:10,  6.30it/s]Epoch: 5, train for the 51-th batch, train loss: 0.4989388585090637:  43%|█████▌       | 51/119 [00:07<00:11,  6.17it/s]Epoch: 4, train for the 113-th batch, train loss: 0.45522063970565796:  77%|███████▋  | 112/146 [00:15<00:05,  6.00it/s]evaluate for the 47-th batch, evaluate loss: 0.32383400201797485:  64%|██████████▊      | 46/72 [00:03<00:03,  8.51it/s]Epoch: 4, train for the 113-th batch, train loss: 0.45522063970565796:  77%|███████▋  | 113/146 [00:15<00:05,  5.72it/s]evaluate for the 9-th batch, evaluate loss: 0.5331770777702332:  17%|███▍                | 8/46 [00:00<00:02, 13.78it/s]evaluate for the 48-th batch, evaluate loss: 0.3039708137512207:  64%|███████████▌      | 46/72 [00:03<00:03,  8.51it/s]evaluate for the 48-th batch, evaluate loss: 0.3039708137512207:  67%|████████████      | 48/72 [00:03<00:02,  9.81it/s]Epoch: 5, train for the 52-th batch, train loss: 0.4654225707054138:  43%|█████▌       | 51/119 [00:07<00:11,  6.17it/s]evaluate for the 10-th batch, evaluate loss: 0.5315570831298828:  17%|███▎               | 8/46 [00:00<00:02, 13.78it/s]evaluate for the 10-th batch, evaluate loss: 0.5315570831298828:  22%|███▉              | 10/46 [00:00<00:02, 12.26it/s]Epoch: 5, train for the 52-th batch, train loss: 0.4654225707054138:  44%|█████▋       | 52/119 [00:07<00:10,  6.11it/s]Epoch: 2, train for the 127-th batch, train loss: 0.5616601705551147:  53%|█████▊     | 126/237 [00:21<00:22,  5.04it/s]Epoch: 2, train for the 127-th batch, train loss: 0.5616601705551147:  54%|█████▉     | 127/237 [00:22<00:22,  4.83it/s]Epoch: 4, train for the 114-th batch, train loss: 0.5149046778678894:  77%|████████▌  | 113/146 [00:15<00:05,  5.72it/s]Epoch: 4, train for the 114-th batch, train loss: 0.5149046778678894:  78%|████████▌  | 114/146 [00:15<00:05,  5.52it/s]evaluate for the 11-th batch, evaluate loss: 0.5677199363708496:  22%|███▉              | 10/46 [00:00<00:02, 12.26it/s]evaluate for the 12-th batch, evaluate loss: 0.5095199346542358:  22%|███▉              | 10/46 [00:00<00:02, 12.26it/s]evaluate for the 12-th batch, evaluate loss: 0.5095199346542358:  26%|████▋             | 12/46 [00:00<00:02, 13.37it/s]Epoch: 1, train for the 339-th batch, train loss: 0.5120552182197571:  88%|█████████▋ | 338/383 [01:44<00:19,  2.36it/s]Epoch: 5, train for the 53-th batch, train loss: 0.4063970148563385:  44%|█████▋       | 52/119 [00:08<00:10,  6.11it/s]Epoch: 5, train for the 53-th batch, train loss: 0.4063970148563385:  45%|█████▊       | 53/119 [00:08<00:10,  6.46it/s]evaluate for the 49-th batch, evaluate loss: 0.43858984112739563:  67%|███████████▎     | 48/72 [00:04<00:02,  9.81it/s]Epoch: 4, train for the 115-th batch, train loss: 0.5162689685821533:  78%|████████▌  | 114/146 [00:16<00:05,  5.52it/s]Epoch: 4, train for the 115-th batch, train loss: 0.5162689685821533:  79%|████████▋  | 115/146 [00:16<00:05,  5.90it/s]Epoch: 2, train for the 128-th batch, train loss: 0.536062479019165:  54%|██████▍     | 127/237 [00:22<00:22,  4.83it/s]evaluate for the 13-th batch, evaluate loss: 0.5152584910392761:  26%|████▋             | 12/46 [00:00<00:02, 13.37it/s]Epoch: 1, train for the 339-th batch, train loss: 0.5120552182197571:  89%|█████████▋ | 339/383 [01:44<00:18,  2.41it/s]Epoch: 2, train for the 128-th batch, train loss: 0.536062479019165:  54%|██████▍     | 128/237 [00:22<00:22,  4.87it/s]evaluate for the 50-th batch, evaluate loss: 0.40411803126335144:  67%|███████████▎     | 48/72 [00:04<00:02,  9.81it/s]evaluate for the 50-th batch, evaluate loss: 0.40411803126335144:  69%|███████████▊     | 50/72 [00:04<00:02,  8.43it/s]Epoch: 5, train for the 54-th batch, train loss: 0.415693461894989:  45%|██████▏       | 53/119 [00:08<00:10,  6.46it/s]evaluate for the 14-th batch, evaluate loss: 0.5724574327468872:  26%|████▋             | 12/46 [00:01<00:02, 13.37it/s]evaluate for the 14-th batch, evaluate loss: 0.5724574327468872:  30%|█████▍            | 14/46 [00:01<00:02, 13.63it/s]Epoch: 5, train for the 54-th batch, train loss: 0.415693461894989:  45%|██████▎       | 54/119 [00:08<00:09,  6.74it/s]Epoch: 4, train for the 116-th batch, train loss: 0.5379694104194641:  79%|████████▋  | 115/146 [00:16<00:05,  5.90it/s]Epoch: 4, train for the 116-th batch, train loss: 0.5379694104194641:  79%|████████▋  | 116/146 [00:16<00:04,  6.24it/s]evaluate for the 51-th batch, evaluate loss: 0.48245397210121155:  69%|███████████▊     | 50/72 [00:04<00:02,  8.43it/s]evaluate for the 51-th batch, evaluate loss: 0.48245397210121155:  71%|████████████     | 51/72 [00:04<00:02,  8.45it/s]evaluate for the 15-th batch, evaluate loss: 0.5362846851348877:  30%|█████▍            | 14/46 [00:01<00:02, 13.63it/s]Epoch: 2, train for the 129-th batch, train loss: 0.5755710601806641:  54%|█████▉     | 128/237 [00:22<00:22,  4.87it/s]Epoch: 5, train for the 55-th batch, train loss: 0.41957706212997437:  45%|█████▍      | 54/119 [00:08<00:09,  6.74it/s]evaluate for the 16-th batch, evaluate loss: 0.530025839805603:  30%|█████▊             | 14/46 [00:01<00:02, 13.63it/s]evaluate for the 16-th batch, evaluate loss: 0.530025839805603:  35%|██████▌            | 16/46 [00:01<00:02, 13.12it/s]Epoch: 2, train for the 129-th batch, train loss: 0.5755710601806641:  54%|█████▉     | 129/237 [00:22<00:22,  4.85it/s]Epoch: 5, train for the 55-th batch, train loss: 0.41957706212997437:  46%|█████▌      | 55/119 [00:08<00:09,  6.45it/s]Epoch: 4, train for the 117-th batch, train loss: 0.5328592658042908:  79%|████████▋  | 116/146 [00:16<00:04,  6.24it/s]Epoch: 4, train for the 117-th batch, train loss: 0.5328592658042908:  80%|████████▊  | 117/146 [00:16<00:04,  6.42it/s]evaluate for the 17-th batch, evaluate loss: 0.4683069586753845:  35%|██████▎           | 16/46 [00:01<00:02, 13.12it/s]Epoch: 1, train for the 340-th batch, train loss: 0.46785101294517517:  89%|████████▊ | 339/383 [01:45<00:18,  2.41it/s]evaluate for the 52-th batch, evaluate loss: 0.42850497364997864:  71%|████████████     | 51/72 [00:04<00:02,  8.45it/s]evaluate for the 52-th batch, evaluate loss: 0.42850497364997864:  72%|████████████▎    | 52/72 [00:04<00:02,  7.45it/s]evaluate for the 18-th batch, evaluate loss: 0.5083822011947632:  35%|██████▎           | 16/46 [00:01<00:02, 13.12it/s]evaluate for the 18-th batch, evaluate loss: 0.5083822011947632:  39%|███████           | 18/46 [00:01<00:02, 13.46it/s]Epoch: 5, train for the 56-th batch, train loss: 0.448666512966156:  46%|██████▍       | 55/119 [00:08<00:09,  6.45it/s]Epoch: 5, train for the 56-th batch, train loss: 0.448666512966156:  47%|██████▌       | 56/119 [00:08<00:09,  6.72it/s]Epoch: 4, train for the 118-th batch, train loss: 0.502684473991394:  80%|█████████▌  | 117/146 [00:16<00:04,  6.42it/s]evaluate for the 53-th batch, evaluate loss: 0.4761834740638733:  72%|█████████████     | 52/72 [00:04<00:02,  7.45it/s]Epoch: 4, train for the 118-th batch, train loss: 0.502684473991394:  81%|█████████▋  | 118/146 [00:16<00:04,  6.61it/s]Epoch: 2, train for the 130-th batch, train loss: 0.5397478342056274:  54%|█████▉     | 129/237 [00:22<00:22,  4.85it/s]evaluate for the 19-th batch, evaluate loss: 0.581777811050415:  39%|███████▍           | 18/46 [00:01<00:02, 13.46it/s]Epoch: 1, train for the 340-th batch, train loss: 0.46785101294517517:  89%|████████▉ | 340/383 [01:45<00:18,  2.34it/s]Epoch: 2, train for the 130-th batch, train loss: 0.5397478342056274:  55%|██████     | 130/237 [00:22<00:22,  4.69it/s]evaluate for the 54-th batch, evaluate loss: 0.5164356827735901:  72%|█████████████     | 52/72 [00:04<00:02,  7.45it/s]evaluate for the 54-th batch, evaluate loss: 0.5164356827735901:  75%|█████████████▌    | 54/72 [00:04<00:01,  9.30it/s]Epoch: 5, train for the 57-th batch, train loss: 0.445272296667099:  47%|██████▌       | 56/119 [00:08<00:09,  6.72it/s]Epoch: 5, train for the 57-th batch, train loss: 0.445272296667099:  48%|██████▋       | 57/119 [00:08<00:09,  6.59it/s]evaluate for the 55-th batch, evaluate loss: 0.4886712431907654:  75%|█████████████▌    | 54/72 [00:04<00:01,  9.30it/s]Epoch: 4, train for the 119-th batch, train loss: 0.4950815737247467:  81%|████████▉  | 118/146 [00:16<00:04,  6.61it/s]Epoch: 4, train for the 119-th batch, train loss: 0.4950815737247467:  82%|████████▉  | 119/146 [00:16<00:04,  6.52it/s]evaluate for the 20-th batch, evaluate loss: 0.5223193764686584:  39%|███████           | 18/46 [00:01<00:02, 13.46it/s]evaluate for the 20-th batch, evaluate loss: 0.5223193764686584:  43%|███████▊          | 20/46 [00:01<00:02, 11.64it/s]evaluate for the 56-th batch, evaluate loss: 0.4932801425457001:  75%|█████████████▌    | 54/72 [00:04<00:01,  9.30it/s]evaluate for the 56-th batch, evaluate loss: 0.4932801425457001:  78%|██████████████    | 56/72 [00:04<00:01, 10.50it/s]evaluate for the 21-th batch, evaluate loss: 0.5554699301719666:  43%|███████▊          | 20/46 [00:01<00:02, 11.64it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5369510054588318:  55%|██████     | 130/237 [00:22<00:22,  4.69it/s]Epoch: 5, train for the 58-th batch, train loss: 0.4255513548851013:  48%|██████▏      | 57/119 [00:08<00:09,  6.59it/s]Epoch: 5, train for the 58-th batch, train loss: 0.4255513548851013:  49%|██████▎      | 58/119 [00:08<00:09,  6.41it/s]Epoch: 2, train for the 131-th batch, train loss: 0.5369510054588318:  55%|██████     | 131/237 [00:22<00:23,  4.55it/s]Epoch: 4, train for the 120-th batch, train loss: 0.49387863278388977:  82%|████████▏ | 119/146 [00:16<00:04,  6.52it/s]Epoch: 4, train for the 120-th batch, train loss: 0.49387863278388977:  82%|████████▏ | 120/146 [00:16<00:04,  6.44it/s]evaluate for the 22-th batch, evaluate loss: 0.525743842124939:  43%|████████▎          | 20/46 [00:01<00:02, 11.64it/s]evaluate for the 22-th batch, evaluate loss: 0.525743842124939:  48%|█████████          | 22/46 [00:01<00:01, 12.06it/s]Epoch: 1, train for the 341-th batch, train loss: 0.4382953643798828:  89%|█████████▊ | 340/383 [01:45<00:18,  2.34it/s]evaluate for the 23-th batch, evaluate loss: 0.4814206659793854:  48%|████████▌         | 22/46 [00:01<00:01, 12.06it/s]Epoch: 5, train for the 59-th batch, train loss: 0.4658300280570984:  49%|██████▎      | 58/119 [00:08<00:09,  6.41it/s]Epoch: 5, train for the 59-th batch, train loss: 0.4658300280570984:  50%|██████▍      | 59/119 [00:08<00:08,  6.77it/s]Epoch: 4, train for the 121-th batch, train loss: 0.4700715243816376:  82%|█████████  | 120/146 [00:16<00:04,  6.44it/s]Epoch: 1, train for the 341-th batch, train loss: 0.4382953643798828:  89%|█████████▊ | 341/383 [01:45<00:17,  2.38it/s]Epoch: 2, train for the 132-th batch, train loss: 0.5799863338470459:  55%|██████     | 131/237 [00:23<00:23,  4.55it/s]Epoch: 4, train for the 121-th batch, train loss: 0.4700715243816376:  83%|█████████  | 121/146 [00:17<00:03,  6.56it/s]evaluate for the 24-th batch, evaluate loss: 0.49097204208374023:  48%|████████▏        | 22/46 [00:01<00:01, 12.06it/s]evaluate for the 24-th batch, evaluate loss: 0.49097204208374023:  52%|████████▊        | 24/46 [00:01<00:01, 12.58it/s]evaluate for the 57-th batch, evaluate loss: 0.4532957673072815:  78%|██████████████    | 56/72 [00:05<00:01, 10.50it/s]Epoch: 2, train for the 132-th batch, train loss: 0.5799863338470459:  56%|██████▏    | 132/237 [00:23<00:22,  4.70it/s]Epoch: 5, train for the 60-th batch, train loss: 0.42986923456192017:  50%|█████▉      | 59/119 [00:09<00:08,  6.77it/s]Epoch: 5, train for the 60-th batch, train loss: 0.42986923456192017:  50%|██████      | 60/119 [00:09<00:08,  7.11it/s]evaluate for the 58-th batch, evaluate loss: 0.510489821434021:  78%|██████████████▊    | 56/72 [00:05<00:01, 10.50it/s]evaluate for the 58-th batch, evaluate loss: 0.510489821434021:  81%|███████████████▎   | 58/72 [00:05<00:01,  8.25it/s]evaluate for the 25-th batch, evaluate loss: 0.5278937220573425:  52%|█████████▍        | 24/46 [00:01<00:01, 12.58it/s]Epoch: 4, train for the 122-th batch, train loss: 0.5766832828521729:  83%|█████████  | 121/146 [00:17<00:03,  6.56it/s]Epoch: 4, train for the 122-th batch, train loss: 0.5766832828521729:  84%|█████████▏ | 122/146 [00:17<00:03,  6.61it/s]evaluate for the 59-th batch, evaluate loss: 0.48667898774147034:  81%|█████████████▋   | 58/72 [00:05<00:01,  8.25it/s]evaluate for the 26-th batch, evaluate loss: 0.5533479452133179:  52%|█████████▍        | 24/46 [00:02<00:01, 12.58it/s]evaluate for the 26-th batch, evaluate loss: 0.5533479452133179:  57%|██████████▏       | 26/46 [00:02<00:01, 11.88it/s]Epoch: 5, train for the 61-th batch, train loss: 0.42190584540367126:  50%|██████      | 60/119 [00:09<00:08,  7.11it/s]evaluate for the 60-th batch, evaluate loss: 0.49218863248825073:  81%|█████████████▋   | 58/72 [00:05<00:01,  8.25it/s]evaluate for the 60-th batch, evaluate loss: 0.49218863248825073:  83%|██████████████▏  | 60/72 [00:05<00:01,  9.56it/s]Epoch: 5, train for the 61-th batch, train loss: 0.42190584540367126:  51%|██████▏     | 61/119 [00:09<00:08,  6.77it/s]evaluate for the 27-th batch, evaluate loss: 0.5222444534301758:  57%|██████████▏       | 26/46 [00:02<00:01, 11.88it/s]Epoch: 2, train for the 133-th batch, train loss: 0.5223223567008972:  56%|██████▏    | 132/237 [00:23<00:22,  4.70it/s]Epoch: 4, train for the 123-th batch, train loss: 0.558609127998352:  84%|██████████  | 122/146 [00:17<00:03,  6.61it/s]Epoch: 4, train for the 123-th batch, train loss: 0.558609127998352:  84%|██████████  | 123/146 [00:17<00:03,  6.81it/s]Epoch: 2, train for the 133-th batch, train loss: 0.5223223567008972:  56%|██████▏    | 133/237 [00:23<00:24,  4.32it/s]evaluate for the 28-th batch, evaluate loss: 0.5222088694572449:  57%|██████████▏       | 26/46 [00:02<00:01, 11.88it/s]evaluate for the 28-th batch, evaluate loss: 0.5222088694572449:  61%|██████████▉       | 28/46 [00:02<00:01, 12.66it/s]Epoch: 5, train for the 62-th batch, train loss: 0.4800509214401245:  51%|██████▋      | 61/119 [00:09<00:08,  6.77it/s]Epoch: 5, train for the 62-th batch, train loss: 0.4800509214401245:  52%|██████▊      | 62/119 [00:09<00:08,  6.74it/s]Epoch: 1, train for the 342-th batch, train loss: 0.39891767501831055:  89%|████████▉ | 341/383 [01:45<00:17,  2.38it/s]evaluate for the 61-th batch, evaluate loss: 0.4435456097126007:  83%|███████████████   | 60/72 [00:05<00:01,  9.56it/s]Epoch: 4, train for the 124-th batch, train loss: 0.5072860717773438:  84%|█████████▎ | 123/146 [00:17<00:03,  6.81it/s]evaluate for the 29-th batch, evaluate loss: 0.4937670826911926:  61%|██████████▉       | 28/46 [00:02<00:01, 12.66it/s]Epoch: 4, train for the 124-th batch, train loss: 0.5072860717773438:  85%|█████████▎ | 124/146 [00:17<00:03,  6.99it/s]Epoch: 2, train for the 134-th batch, train loss: 0.5910821557044983:  56%|██████▏    | 133/237 [00:23<00:24,  4.32it/s]evaluate for the 62-th batch, evaluate loss: 0.4364749789237976:  83%|███████████████   | 60/72 [00:05<00:01,  9.56it/s]evaluate for the 62-th batch, evaluate loss: 0.4364749789237976:  86%|███████████████▌  | 62/72 [00:05<00:01,  9.04it/s]Epoch: 2, train for the 134-th batch, train loss: 0.5910821557044983:  57%|██████▏    | 134/237 [00:23<00:22,  4.60it/s]Epoch: 1, train for the 342-th batch, train loss: 0.39891767501831055:  89%|████████▉ | 342/383 [01:46<00:18,  2.22it/s]Epoch: 5, train for the 63-th batch, train loss: 0.4139934480190277:  52%|██████▊      | 62/119 [00:09<00:08,  6.74it/s]Epoch: 5, train for the 63-th batch, train loss: 0.4139934480190277:  53%|██████▉      | 63/119 [00:09<00:08,  6.59it/s]evaluate for the 30-th batch, evaluate loss: 0.5116434097290039:  61%|██████████▉       | 28/46 [00:02<00:01, 12.66it/s]evaluate for the 30-th batch, evaluate loss: 0.5116434097290039:  65%|███████████▋      | 30/46 [00:02<00:01, 11.28it/s]evaluate for the 63-th batch, evaluate loss: 0.4240279197692871:  86%|███████████████▌  | 62/72 [00:05<00:01,  9.04it/s]Epoch: 4, train for the 125-th batch, train loss: 0.49629247188568115:  85%|████████▍ | 124/146 [00:17<00:03,  6.99it/s]Epoch: 4, train for the 125-th batch, train loss: 0.49629247188568115:  86%|████████▌ | 125/146 [00:17<00:03,  6.67it/s]Epoch: 2, train for the 135-th batch, train loss: 0.5940988659858704:  57%|██████▏    | 134/237 [00:23<00:22,  4.60it/s]evaluate for the 64-th batch, evaluate loss: 0.5486668944358826:  86%|███████████████▌  | 62/72 [00:05<00:01,  9.04it/s]evaluate for the 64-th batch, evaluate loss: 0.5486668944358826:  89%|████████████████  | 64/72 [00:05<00:00,  9.96it/s]evaluate for the 31-th batch, evaluate loss: 0.4454876184463501:  65%|███████████▋      | 30/46 [00:02<00:01, 11.28it/s]Epoch: 2, train for the 135-th batch, train loss: 0.5940988659858704:  57%|██████▎    | 135/237 [00:23<00:20,  4.89it/s]Epoch: 5, train for the 64-th batch, train loss: 0.4699339270591736:  53%|██████▉      | 63/119 [00:09<00:08,  6.59it/s]Epoch: 5, train for the 64-th batch, train loss: 0.4699339270591736:  54%|██████▉      | 64/119 [00:09<00:08,  6.63it/s]evaluate for the 65-th batch, evaluate loss: 0.5322468280792236:  89%|████████████████  | 64/72 [00:05<00:00,  9.96it/s]Epoch: 4, train for the 126-th batch, train loss: 0.5138496160507202:  86%|█████████▍ | 125/146 [00:17<00:03,  6.67it/s]evaluate for the 32-th batch, evaluate loss: 0.47829803824424744:  65%|███████████      | 30/46 [00:02<00:01, 11.28it/s]evaluate for the 32-th batch, evaluate loss: 0.47829803824424744:  70%|███████████▊     | 32/46 [00:02<00:01, 11.42it/s]Epoch: 4, train for the 126-th batch, train loss: 0.5138496160507202:  86%|█████████▍ | 126/146 [00:17<00:02,  6.73it/s]Epoch: 1, train for the 343-th batch, train loss: 0.3905179500579834:  89%|█████████▊ | 342/383 [01:46<00:18,  2.22it/s]evaluate for the 66-th batch, evaluate loss: 0.43148475885391235:  89%|███████████████  | 64/72 [00:05<00:00,  9.96it/s]evaluate for the 66-th batch, evaluate loss: 0.43148475885391235:  92%|███████████████▌ | 66/72 [00:05<00:00, 10.76it/s]evaluate for the 33-th batch, evaluate loss: 0.4859064519405365:  70%|████████████▌     | 32/46 [00:02<00:01, 11.42it/s]Epoch: 4, train for the 127-th batch, train loss: 0.5219150185585022:  86%|█████████▍ | 126/146 [00:17<00:02,  6.73it/s]Epoch: 2, train for the 136-th batch, train loss: 0.6140806674957275:  57%|██████▎    | 135/237 [00:23<00:20,  4.89it/s]Epoch: 1, train for the 343-th batch, train loss: 0.3905179500579834:  90%|█████████▊ | 343/383 [01:46<00:16,  2.38it/s]Epoch: 4, train for the 127-th batch, train loss: 0.5219150185585022:  87%|█████████▌ | 127/146 [00:17<00:02,  7.00it/s]evaluate for the 34-th batch, evaluate loss: 0.46030065417289734:  70%|███████████▊     | 32/46 [00:02<00:01, 11.42it/s]evaluate for the 34-th batch, evaluate loss: 0.46030065417289734:  74%|████████████▌    | 34/46 [00:02<00:01, 11.79it/s]Epoch: 2, train for the 136-th batch, train loss: 0.6140806674957275:  57%|██████▎    | 136/237 [00:23<00:21,  4.71it/s]Epoch: 5, train for the 65-th batch, train loss: 0.4516525864601135:  54%|██████▉      | 64/119 [00:09<00:08,  6.63it/s]Epoch: 5, train for the 65-th batch, train loss: 0.4516525864601135:  55%|███████      | 65/119 [00:09<00:09,  5.46it/s]evaluate for the 35-th batch, evaluate loss: 0.5177209973335266:  74%|█████████████▎    | 34/46 [00:02<00:01, 11.79it/s]evaluate for the 67-th batch, evaluate loss: 0.47226253151893616:  92%|███████████████▌ | 66/72 [00:06<00:00, 10.76it/s]Epoch: 4, train for the 128-th batch, train loss: 0.49866896867752075:  87%|████████▋ | 127/146 [00:18<00:02,  7.00it/s]Epoch: 4, train for the 128-th batch, train loss: 0.49866896867752075:  88%|████████▊ | 128/146 [00:18<00:02,  6.64it/s]evaluate for the 68-th batch, evaluate loss: 0.46770045161247253:  92%|███████████████▌ | 66/72 [00:06<00:00, 10.76it/s]evaluate for the 68-th batch, evaluate loss: 0.46770045161247253:  94%|████████████████ | 68/72 [00:06<00:00,  9.09it/s]evaluate for the 36-th batch, evaluate loss: 0.4605635702610016:  74%|█████████████▎    | 34/46 [00:02<00:01, 11.79it/s]evaluate for the 36-th batch, evaluate loss: 0.4605635702610016:  78%|██████████████    | 36/46 [00:02<00:00, 10.88it/s]Epoch: 5, train for the 66-th batch, train loss: 0.4419086277484894:  55%|███████      | 65/119 [00:10<00:09,  5.46it/s]Epoch: 5, train for the 66-th batch, train loss: 0.4419086277484894:  55%|███████▏     | 66/119 [00:10<00:09,  5.72it/s]Epoch: 2, train for the 137-th batch, train loss: 0.6014447808265686:  57%|██████▎    | 136/237 [00:24<00:21,  4.71it/s]evaluate for the 69-th batch, evaluate loss: 0.47741052508354187:  94%|████████████████ | 68/72 [00:06<00:00,  9.09it/s]Epoch: 4, train for the 129-th batch, train loss: 0.519810676574707:  88%|██████████▌ | 128/146 [00:18<00:02,  6.64it/s]Epoch: 2, train for the 137-th batch, train loss: 0.6014447808265686:  58%|██████▎    | 137/237 [00:24<00:23,  4.22it/s]evaluate for the 37-th batch, evaluate loss: 0.5170981287956238:  78%|██████████████    | 36/46 [00:03<00:00, 10.88it/s]Epoch: 4, train for the 129-th batch, train loss: 0.519810676574707:  88%|██████████▌ | 129/146 [00:18<00:02,  6.53it/s]evaluate for the 70-th batch, evaluate loss: 0.4862840175628662:  94%|█████████████████ | 68/72 [00:06<00:00,  9.09it/s]evaluate for the 70-th batch, evaluate loss: 0.4862840175628662:  97%|█████████████████▌| 70/72 [00:06<00:00, 10.38it/s]Epoch: 5, train for the 67-th batch, train loss: 0.5248755216598511:  55%|███████▏     | 66/119 [00:10<00:09,  5.72it/s]Epoch: 5, train for the 67-th batch, train loss: 0.5248755216598511:  56%|███████▎     | 67/119 [00:10<00:08,  5.98it/s]evaluate for the 38-th batch, evaluate loss: 0.5112101435661316:  78%|██████████████    | 36/46 [00:03<00:00, 10.88it/s]evaluate for the 38-th batch, evaluate loss: 0.5112101435661316:  83%|██████████████▊   | 38/46 [00:03<00:00, 11.03it/s]evaluate for the 71-th batch, evaluate loss: 0.5244967341423035:  97%|█████████████████▌| 70/72 [00:06<00:00, 10.38it/s]Epoch: 4, train for the 130-th batch, train loss: 0.4316416084766388:  88%|█████████▋ | 129/146 [00:18<00:02,  6.53it/s]evaluate for the 39-th batch, evaluate loss: 0.4779932200908661:  83%|██████████████▊   | 38/46 [00:03<00:00, 11.03it/s]Epoch: 4, train for the 130-th batch, train loss: 0.4316416084766388:  89%|█████████▊ | 130/146 [00:18<00:02,  6.61it/s]Epoch: 2, train for the 138-th batch, train loss: 0.6290854811668396:  58%|██████▎    | 137/237 [00:24<00:23,  4.22it/s]evaluate for the 72-th batch, evaluate loss: 0.5436163544654846:  97%|█████████████████▌| 70/72 [00:06<00:00, 10.38it/s]evaluate for the 72-th batch, evaluate loss: 0.5436163544654846: 100%|██████████████████| 72/72 [00:06<00:00, 11.55it/s]evaluate for the 72-th batch, evaluate loss: 0.5436163544654846: 100%|██████████████████| 72/72 [00:06<00:00, 11.25it/s]
Epoch: 2, train for the 138-th batch, train loss: 0.6290854811668396:  58%|██████▍    | 138/237 [00:24<00:21,  4.51it/s]Epoch: 5, train for the 68-th batch, train loss: 0.40207594633102417:  56%|██████▊     | 67/119 [00:10<00:08,  5.98it/s]evaluate for the 40-th batch, evaluate loss: 0.4751840829849243:  83%|██████████████▊   | 38/46 [00:03<00:00, 11.03it/s]evaluate for the 40-th batch, evaluate loss: 0.4751840829849243:  87%|███████████████▋  | 40/46 [00:03<00:00, 12.00it/s]Epoch: 5, train for the 68-th batch, train loss: 0.40207594633102417:  57%|██████▊     | 68/119 [00:10<00:08,  6.22it/s]Epoch: 1, train for the 344-th batch, train loss: 0.4866117835044861:  90%|█████████▊ | 343/383 [01:46<00:16,  2.38it/s]Epoch: 4, train for the 131-th batch, train loss: 0.5318261384963989:  89%|█████████▊ | 130/146 [00:18<00:02,  6.61it/s]evaluate for the 41-th batch, evaluate loss: 0.5048283934593201:  87%|███████████████▋  | 40/46 [00:03<00:00, 12.00it/s]Epoch: 4, train for the 131-th batch, train loss: 0.5318261384963989:  90%|█████████▊ | 131/146 [00:18<00:02,  6.84it/s]Epoch: 1, train for the 344-th batch, train loss: 0.4866117835044861:  90%|█████████▉ | 344/383 [01:47<00:19,  2.04it/s]Epoch: 5, train for the 69-th batch, train loss: 0.4123070538043976:  57%|███████▍     | 68/119 [00:10<00:08,  6.22it/s]Epoch: 2, train for the 139-th batch, train loss: 0.6043278574943542:  58%|██████▍    | 138/237 [00:24<00:21,  4.51it/s]evaluate for the 42-th batch, evaluate loss: 0.4572836458683014:  87%|███████████████▋  | 40/46 [00:03<00:00, 12.00it/s]evaluate for the 42-th batch, evaluate loss: 0.4572836458683014:  91%|████████████████▍ | 42/46 [00:03<00:00, 12.92it/s]Epoch: 5, train for the 69-th batch, train loss: 0.4123070538043976:  58%|███████▌     | 69/119 [00:10<00:07,  6.63it/s]Epoch: 2, train for the 139-th batch, train loss: 0.6043278574943542:  59%|██████▍    | 139/237 [00:24<00:20,  4.74it/s]Epoch: 4, train for the 132-th batch, train loss: 0.47087687253952026:  90%|████████▉ | 131/146 [00:18<00:02,  6.84it/s]Epoch: 4, train for the 132-th batch, train loss: 0.47087687253952026:  90%|█████████ | 132/146 [00:18<00:02,  6.89it/s]evaluate for the 43-th batch, evaluate loss: 0.5410969257354736:  91%|████████████████▍ | 42/46 [00:03<00:00, 12.92it/s]Epoch: 5, train for the 70-th batch, train loss: 0.38154280185699463:  58%|██████▉     | 69/119 [00:10<00:07,  6.63it/s]Epoch: 5, train for the 70-th batch, train loss: 0.38154280185699463:  59%|███████     | 70/119 [00:10<00:07,  6.71it/s]evaluate for the 44-th batch, evaluate loss: 0.49340739846229553:  91%|███████████████▌ | 42/46 [00:03<00:00, 12.92it/s]evaluate for the 44-th batch, evaluate loss: 0.49340739846229553:  96%|████████████████▎| 44/46 [00:03<00:00, 12.77it/s]Epoch: 2, train for the 140-th batch, train loss: 0.5635374188423157:  59%|██████▍    | 139/237 [00:24<00:20,  4.74it/s]Epoch: 2, train for the 140-th batch, train loss: 0.5635374188423157:  59%|██████▍    | 140/237 [00:24<00:19,  4.97it/s]evaluate for the 45-th batch, evaluate loss: 0.47857439517974854:  96%|████████████████▎| 44/46 [00:03<00:00, 12.77it/s]Epoch: 4, train for the 133-th batch, train loss: 0.4714626967906952:  90%|█████████▉ | 132/146 [00:18<00:02,  6.89it/s]Epoch: 4, train for the 133-th batch, train loss: 0.4714626967906952:  91%|██████████ | 133/146 [00:18<00:01,  6.87it/s]evaluate for the 46-th batch, evaluate loss: 0.46656110882759094:  96%|████████████████▎| 44/46 [00:03<00:00, 12.77it/s]Epoch: 5, train for the 71-th batch, train loss: 0.43320196866989136:  59%|███████     | 70/119 [00:10<00:07,  6.71it/s]evaluate for the 46-th batch, evaluate loss: 0.46656110882759094: 100%|█████████████████| 46/46 [00:03<00:00, 13.81it/s]evaluate for the 46-th batch, evaluate loss: 0.46656110882759094: 100%|█████████████████| 46/46 [00:03<00:00, 12.52it/s]
Epoch: 5, train for the 71-th batch, train loss: 0.43320196866989136:  60%|███████▏    | 71/119 [00:10<00:07,  6.79it/s]Epoch: 2, train for the 141-th batch, train loss: 0.5946149826049805:  59%|██████▍    | 140/237 [00:24<00:19,  4.97it/s]Epoch: 2, train for the 141-th batch, train loss: 0.5946149826049805:  59%|██████▌    | 141/237 [00:24<00:18,  5.31it/s]Epoch: 1, train for the 345-th batch, train loss: 0.41078540682792664:  90%|████████▉ | 344/383 [01:47<00:19,  2.04it/s]Epoch: 4, train for the 134-th batch, train loss: 0.4687729775905609:  91%|██████████ | 133/146 [00:18<00:01,  6.87it/s]Epoch: 4, train for the 134-th batch, train loss: 0.4687729775905609:  92%|██████████ | 134/146 [00:18<00:01,  6.87it/s]Epoch: 5, train for the 72-th batch, train loss: 0.45494237542152405:  60%|███████▏    | 71/119 [00:10<00:07,  6.79it/s]Epoch: 5, train for the 72-th batch, train loss: 0.45494237542152405:  61%|███████▎    | 72/119 [00:10<00:06,  7.20it/s]Epoch: 1, train for the 345-th batch, train loss: 0.41078540682792664:  90%|█████████ | 345/383 [01:47<00:18,  2.09it/s]Epoch: 2, train for the 142-th batch, train loss: 0.6053609848022461:  59%|██████▌    | 141/237 [00:25<00:18,  5.31it/s]Epoch: 4, train for the 135-th batch, train loss: 0.5166450142860413:  92%|██████████ | 134/146 [00:19<00:01,  6.87it/s]Epoch: 2, train for the 142-th batch, train loss: 0.6053609848022461:  60%|██████▌    | 142/237 [00:25<00:17,  5.52it/s]Epoch: 4, train for the 135-th batch, train loss: 0.5166450142860413:  92%|██████████▏| 135/146 [00:19<00:01,  6.72it/s]Epoch: 5, train for the 73-th batch, train loss: 0.4287266731262207:  61%|███████▊     | 72/119 [00:11<00:06,  7.20it/s]Epoch: 5, train for the 73-th batch, train loss: 0.4287266731262207:  61%|███████▉     | 73/119 [00:11<00:06,  6.90it/s]  0%|                                                                                            | 0/34 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6134033799171448:   0%|                            | 0/34 [00:00<?, ?it/s]Epoch: 4, train for the 136-th batch, train loss: 0.4677596986293793:  92%|██████████▏| 135/146 [00:19<00:01,  6.72it/s]Epoch: 4, train for the 136-th batch, train loss: 0.4677596986293793:  93%|██████████▏| 136/146 [00:19<00:01,  6.98it/s]Epoch: 2, train for the 143-th batch, train loss: 0.5304219126701355:  60%|██████▌    | 142/237 [00:25<00:17,  5.52it/s]evaluate for the 2-th batch, evaluate loss: 0.6868547201156616:   0%|                            | 0/34 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.6868547201156616:   6%|█▏                  | 2/34 [00:00<00:01, 18.85it/s]Epoch: 2, train for the 143-th batch, train loss: 0.5304219126701355:  60%|██████▋    | 143/237 [00:25<00:16,  5.62it/s]  0%|                                                                                            | 0/25 [00:00<?, ?it/s]Epoch: 5, train for the 74-th batch, train loss: 0.4400685429573059:  61%|███████▉     | 73/119 [00:11<00:06,  6.90it/s]Epoch: 5, train for the 74-th batch, train loss: 0.4400685429573059:  62%|████████     | 74/119 [00:11<00:06,  6.73it/s]evaluate for the 3-th batch, evaluate loss: 0.6681601405143738:   6%|█▏                  | 2/34 [00:00<00:01, 18.85it/s]evaluate for the 1-th batch, evaluate loss: 0.7505276203155518:   0%|                            | 0/25 [00:00<?, ?it/s]Epoch: 4, train for the 137-th batch, train loss: 0.45931869745254517:  93%|█████████▎| 136/146 [00:19<00:01,  6.98it/s]Epoch: 4, train for the 137-th batch, train loss: 0.45931869745254517:  94%|█████████▍| 137/146 [00:19<00:01,  7.05it/s]evaluate for the 4-th batch, evaluate loss: 0.6599165797233582:   6%|█▏                  | 2/34 [00:00<00:01, 18.85it/s]evaluate for the 4-th batch, evaluate loss: 0.6599165797233582:  12%|██▎                 | 4/34 [00:00<00:01, 15.71it/s]evaluate for the 2-th batch, evaluate loss: 0.7683672308921814:   0%|                            | 0/25 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.7683672308921814:   8%|█▌                  | 2/25 [00:00<00:01, 13.36it/s]Epoch: 5, train for the 75-th batch, train loss: 0.4118436276912689:  62%|████████     | 74/119 [00:11<00:06,  6.73it/s]Epoch: 1, train for the 346-th batch, train loss: 0.4051578938961029:  90%|█████████▉ | 345/383 [01:47<00:18,  2.09it/s]Epoch: 5, train for the 75-th batch, train loss: 0.4118436276912689:  63%|████████▏    | 75/119 [00:11<00:06,  6.68it/s]Epoch: 2, train for the 144-th batch, train loss: 0.5841484069824219:  60%|██████▋    | 143/237 [00:25<00:16,  5.62it/s]evaluate for the 5-th batch, evaluate loss: 0.6464677453041077:  12%|██▎                 | 4/34 [00:00<00:01, 15.71it/s]Epoch: 2, train for the 144-th batch, train loss: 0.5841484069824219:  61%|██████▋    | 144/237 [00:25<00:17,  5.20it/s]evaluate for the 3-th batch, evaluate loss: 0.7717500925064087:   8%|█▌                  | 2/25 [00:00<00:01, 13.36it/s]Epoch: 1, train for the 346-th batch, train loss: 0.4051578938961029:  90%|█████████▉ | 346/383 [01:48<00:17,  2.06it/s]Epoch: 4, train for the 138-th batch, train loss: 0.5391435623168945:  94%|██████████▎| 137/146 [00:19<00:01,  7.05it/s]evaluate for the 6-th batch, evaluate loss: 0.6081632375717163:  12%|██▎                 | 4/34 [00:00<00:01, 15.71it/s]evaluate for the 6-th batch, evaluate loss: 0.6081632375717163:  18%|███▌                | 6/34 [00:00<00:01, 15.41it/s]Epoch: 4, train for the 138-th batch, train loss: 0.5391435623168945:  95%|██████████▍| 138/146 [00:19<00:01,  6.78it/s]Epoch: 5, train for the 76-th batch, train loss: 0.4482852816581726:  63%|████████▏    | 75/119 [00:11<00:06,  6.68it/s]Epoch: 5, train for the 76-th batch, train loss: 0.4482852816581726:  64%|████████▎    | 76/119 [00:11<00:06,  6.72it/s]evaluate for the 4-th batch, evaluate loss: 0.7453643679618835:   8%|█▌                  | 2/25 [00:00<00:01, 13.36it/s]evaluate for the 4-th batch, evaluate loss: 0.7453643679618835:  16%|███▏                | 4/25 [00:00<00:01, 12.09it/s]evaluate for the 7-th batch, evaluate loss: 0.5512115955352783:  18%|███▌                | 6/34 [00:00<00:01, 15.41it/s]Epoch: 2, train for the 145-th batch, train loss: 0.6028836965560913:  61%|██████▋    | 144/237 [00:25<00:17,  5.20it/s]Epoch: 4, train for the 139-th batch, train loss: 0.4999135732650757:  95%|██████████▍| 138/146 [00:19<00:01,  6.78it/s]Epoch: 4, train for the 139-th batch, train loss: 0.4999135732650757:  95%|██████████▍| 139/146 [00:19<00:01,  6.51it/s]Epoch: 2, train for the 145-th batch, train loss: 0.6028836965560913:  61%|██████▋    | 145/237 [00:25<00:18,  5.05it/s]evaluate for the 8-th batch, evaluate loss: 0.6848191618919373:  18%|███▌                | 6/34 [00:00<00:01, 15.41it/s]evaluate for the 8-th batch, evaluate loss: 0.6848191618919373:  24%|████▋               | 8/34 [00:00<00:01, 13.46it/s]evaluate for the 5-th batch, evaluate loss: 0.7493115067481995:  16%|███▏                | 4/25 [00:00<00:01, 12.09it/s]Epoch: 5, train for the 77-th batch, train loss: 0.42834120988845825:  64%|███████▋    | 76/119 [00:11<00:06,  6.72it/s]Epoch: 5, train for the 77-th batch, train loss: 0.42834120988845825:  65%|███████▊    | 77/119 [00:11<00:06,  6.87it/s]evaluate for the 9-th batch, evaluate loss: 0.5842709541320801:  24%|████▋               | 8/34 [00:00<00:01, 13.46it/s]Epoch: 1, train for the 347-th batch, train loss: 0.32101044058799744:  90%|█████████ | 346/383 [01:48<00:17,  2.06it/s]evaluate for the 6-th batch, evaluate loss: 0.7612496614456177:  16%|███▏                | 4/25 [00:00<00:01, 12.09it/s]evaluate for the 6-th batch, evaluate loss: 0.7612496614456177:  24%|████▊               | 6/25 [00:00<00:01, 10.67it/s]Epoch: 4, train for the 140-th batch, train loss: 0.4622727930545807:  95%|██████████▍| 139/146 [00:19<00:01,  6.51it/s]evaluate for the 10-th batch, evaluate loss: 0.6282384991645813:  24%|████▍              | 8/34 [00:00<00:01, 13.46it/s]evaluate for the 10-th batch, evaluate loss: 0.6282384991645813:  29%|█████▎            | 10/34 [00:00<00:01, 13.38it/s]Epoch: 4, train for the 140-th batch, train loss: 0.4622727930545807:  96%|██████████▌| 140/146 [00:19<00:00,  6.42it/s]Epoch: 2, train for the 146-th batch, train loss: 0.557086169719696:  61%|███████▎    | 145/237 [00:25<00:18,  5.05it/s]Epoch: 5, train for the 78-th batch, train loss: 0.46107932925224304:  65%|███████▊    | 77/119 [00:11<00:06,  6.87it/s]Epoch: 1, train for the 347-th batch, train loss: 0.32101044058799744:  91%|█████████ | 347/383 [01:48<00:16,  2.20it/s]Epoch: 5, train for the 78-th batch, train loss: 0.46107932925224304:  66%|███████▊    | 78/119 [00:11<00:06,  6.65it/s]Epoch: 2, train for the 146-th batch, train loss: 0.557086169719696:  62%|███████▍    | 146/237 [00:25<00:17,  5.06it/s]evaluate for the 7-th batch, evaluate loss: 0.7755997776985168:  24%|████▊               | 6/25 [00:00<00:01, 10.67it/s]evaluate for the 11-th batch, evaluate loss: 0.6679185628890991:  29%|█████▎            | 10/34 [00:00<00:01, 13.38it/s]Epoch: 4, train for the 141-th batch, train loss: 0.4949209988117218:  96%|██████████▌| 140/146 [00:19<00:00,  6.42it/s]Epoch: 4, train for the 141-th batch, train loss: 0.4949209988117218:  97%|██████████▌| 141/146 [00:19<00:00,  6.59it/s]evaluate for the 8-th batch, evaluate loss: 0.7483886480331421:  24%|████▊               | 6/25 [00:00<00:01, 10.67it/s]evaluate for the 8-th batch, evaluate loss: 0.7483886480331421:  32%|██████▍             | 8/25 [00:00<00:01, 10.59it/s]evaluate for the 12-th batch, evaluate loss: 0.6219976544380188:  29%|█████▎            | 10/34 [00:00<00:01, 13.38it/s]evaluate for the 12-th batch, evaluate loss: 0.6219976544380188:  35%|██████▎           | 12/34 [00:00<00:01, 13.01it/s]Epoch: 5, train for the 79-th batch, train loss: 0.4503774642944336:  66%|████████▌    | 78/119 [00:11<00:06,  6.65it/s]Epoch: 5, train for the 79-th batch, train loss: 0.4503774642944336:  66%|████████▋    | 79/119 [00:11<00:06,  6.54it/s]Epoch: 2, train for the 147-th batch, train loss: 0.6075528860092163:  62%|██████▊    | 146/237 [00:26<00:17,  5.06it/s]evaluate for the 13-th batch, evaluate loss: 0.5525277853012085:  35%|██████▎           | 12/34 [00:00<00:01, 13.01it/s]evaluate for the 9-th batch, evaluate loss: 0.7256349921226501:  32%|██████▍             | 8/25 [00:00<00:01, 10.59it/s]Epoch: 2, train for the 147-th batch, train loss: 0.6075528860092163:  62%|██████▊    | 147/237 [00:26<00:18,  4.92it/s]Epoch: 4, train for the 142-th batch, train loss: 0.4711095988750458:  97%|██████████▌| 141/146 [00:20<00:00,  6.59it/s]Epoch: 4, train for the 142-th batch, train loss: 0.4711095988750458:  97%|██████████▋| 142/146 [00:20<00:00,  6.53it/s]evaluate for the 14-th batch, evaluate loss: 0.5526677370071411:  35%|██████▎           | 12/34 [00:01<00:01, 13.01it/s]evaluate for the 14-th batch, evaluate loss: 0.5526677370071411:  41%|███████▍          | 14/34 [00:01<00:01, 13.04it/s]Epoch: 5, train for the 80-th batch, train loss: 0.493422269821167:  66%|█████████▎    | 79/119 [00:12<00:06,  6.54it/s]evaluate for the 10-th batch, evaluate loss: 0.7866183519363403:  32%|██████             | 8/25 [00:00<00:01, 10.59it/s]evaluate for the 10-th batch, evaluate loss: 0.7866183519363403:  40%|███████▏          | 10/25 [00:00<00:01, 10.63it/s]Epoch: 5, train for the 80-th batch, train loss: 0.493422269821167:  67%|█████████▍    | 80/119 [00:12<00:05,  6.58it/s]evaluate for the 15-th batch, evaluate loss: 0.6968234777450562:  41%|███████▍          | 14/34 [00:01<00:01, 13.04it/s]Epoch: 2, train for the 148-th batch, train loss: 0.5648682117462158:  62%|██████▊    | 147/237 [00:26<00:18,  4.92it/s]Epoch: 2, train for the 148-th batch, train loss: 0.5648682117462158:  62%|██████▊    | 148/237 [00:26<00:17,  5.02it/s]evaluate for the 11-th batch, evaluate loss: 0.7532511949539185:  40%|███████▏          | 10/25 [00:01<00:01, 10.63it/s]evaluate for the 16-th batch, evaluate loss: 0.6112918853759766:  41%|███████▍          | 14/34 [00:01<00:01, 13.04it/s]evaluate for the 16-th batch, evaluate loss: 0.6112918853759766:  47%|████████▍         | 16/34 [00:01<00:01, 13.06it/s]Epoch: 4, train for the 143-th batch, train loss: 0.5132378935813904:  97%|██████████▋| 142/146 [00:20<00:00,  6.53it/s]Epoch: 5, train for the 81-th batch, train loss: 0.3991377055644989:  67%|████████▋    | 80/119 [00:12<00:05,  6.58it/s]Epoch: 4, train for the 143-th batch, train loss: 0.5132378935813904:  98%|██████████▊| 143/146 [00:20<00:00,  6.02it/s]Epoch: 5, train for the 81-th batch, train loss: 0.3991377055644989:  68%|████████▊    | 81/119 [00:12<00:05,  6.50it/s]Epoch: 1, train for the 348-th batch, train loss: 0.47666457295417786:  91%|█████████ | 347/383 [01:48<00:16,  2.20it/s]evaluate for the 17-th batch, evaluate loss: 0.6438000202178955:  47%|████████▍         | 16/34 [00:01<00:01, 13.06it/s]evaluate for the 12-th batch, evaluate loss: 0.6821011304855347:  40%|███████▏          | 10/25 [00:01<00:01, 10.63it/s]evaluate for the 12-th batch, evaluate loss: 0.6821011304855347:  48%|████████▋         | 12/25 [00:01<00:01,  9.79it/s]Epoch: 1, train for the 348-th batch, train loss: 0.47666457295417786:  91%|█████████ | 348/383 [01:48<00:17,  2.02it/s]Epoch: 2, train for the 149-th batch, train loss: 0.5705177187919617:  62%|██████▊    | 148/237 [00:26<00:17,  5.02it/s]evaluate for the 18-th batch, evaluate loss: 0.5844962000846863:  47%|████████▍         | 16/34 [00:01<00:01, 13.06it/s]evaluate for the 18-th batch, evaluate loss: 0.5844962000846863:  53%|█████████▌        | 18/34 [00:01<00:01, 12.78it/s]evaluate for the 13-th batch, evaluate loss: 0.6908213496208191:  48%|████████▋         | 12/25 [00:01<00:01,  9.79it/s]Epoch: 2, train for the 149-th batch, train loss: 0.5705177187919617:  63%|██████▉    | 149/237 [00:26<00:17,  4.94it/s]Epoch: 4, train for the 144-th batch, train loss: 0.46837812662124634:  98%|█████████▊| 143/146 [00:20<00:00,  6.02it/s]Epoch: 4, train for the 144-th batch, train loss: 0.46837812662124634:  99%|█████████▊| 144/146 [00:20<00:00,  5.85it/s]Epoch: 5, train for the 82-th batch, train loss: 0.5115975737571716:  68%|████████▊    | 81/119 [00:12<00:05,  6.50it/s]Epoch: 5, train for the 82-th batch, train loss: 0.5115975737571716:  69%|████████▉    | 82/119 [00:12<00:06,  6.04it/s]evaluate for the 19-th batch, evaluate loss: 0.554244875907898:  53%|██████████         | 18/34 [00:01<00:01, 12.78it/s]evaluate for the 14-th batch, evaluate loss: 0.7327775359153748:  48%|████████▋         | 12/25 [00:01<00:01,  9.79it/s]evaluate for the 14-th batch, evaluate loss: 0.7327775359153748:  56%|██████████        | 14/25 [00:01<00:01, 10.48it/s]evaluate for the 20-th batch, evaluate loss: 0.6172016263008118:  53%|█████████▌        | 18/34 [00:01<00:01, 12.78it/s]evaluate for the 20-th batch, evaluate loss: 0.6172016263008118:  59%|██████████▌       | 20/34 [00:01<00:01, 12.11it/s]evaluate for the 15-th batch, evaluate loss: 0.7541983723640442:  56%|██████████        | 14/25 [00:01<00:01, 10.48it/s]Epoch: 4, train for the 145-th batch, train loss: 0.48741286993026733:  99%|█████████▊| 144/146 [00:20<00:00,  5.85it/s]Epoch: 4, train for the 145-th batch, train loss: 0.48741286993026733:  99%|█████████▉| 145/146 [00:20<00:00,  5.68it/s]Epoch: 5, train for the 83-th batch, train loss: 0.4457869529724121:  69%|████████▉    | 82/119 [00:12<00:06,  6.04it/s]Epoch: 5, train for the 83-th batch, train loss: 0.4457869529724121:  70%|█████████    | 83/119 [00:12<00:06,  5.74it/s]evaluate for the 21-th batch, evaluate loss: 0.6324266195297241:  59%|██████████▌       | 20/34 [00:01<00:01, 12.11it/s]evaluate for the 16-th batch, evaluate loss: 0.6937485933303833:  56%|██████████        | 14/25 [00:01<00:01, 10.48it/s]evaluate for the 16-th batch, evaluate loss: 0.6937485933303833:  64%|███████████▌      | 16/25 [00:01<00:00, 10.90it/s]Epoch: 1, train for the 349-th batch, train loss: 0.45465388894081116:  91%|█████████ | 348/383 [01:49<00:17,  2.02it/s]Epoch: 2, train for the 150-th batch, train loss: 0.539440929889679:  63%|███████▌    | 149/237 [00:26<00:17,  4.94it/s]evaluate for the 22-th batch, evaluate loss: 0.36333227157592773:  59%|██████████       | 20/34 [00:01<00:01, 12.11it/s]evaluate for the 22-th batch, evaluate loss: 0.36333227157592773:  65%|███████████      | 22/34 [00:01<00:00, 12.58it/s]Epoch: 2, train for the 150-th batch, train loss: 0.539440929889679:  63%|███████▌    | 150/237 [00:26<00:20,  4.21it/s]evaluate for the 17-th batch, evaluate loss: 0.6821891069412231:  64%|███████████▌      | 16/25 [00:01<00:00, 10.90it/s]Epoch: 4, train for the 146-th batch, train loss: 0.45653533935546875:  99%|█████████▉| 145/146 [00:20<00:00,  5.68it/s]Epoch: 1, train for the 349-th batch, train loss: 0.45465388894081116:  91%|█████████ | 349/383 [01:49<00:15,  2.17it/s]Epoch: 4, train for the 146-th batch, train loss: 0.45653533935546875: 100%|██████████| 146/146 [00:20<00:00,  6.09it/s]Epoch: 4, train for the 146-th batch, train loss: 0.45653533935546875: 100%|██████████| 146/146 [00:20<00:00,  7.01it/s]
evaluate for the 23-th batch, evaluate loss: 0.3312678039073944:  65%|███████████▋      | 22/34 [00:01<00:00, 12.58it/s]Epoch: 5, train for the 84-th batch, train loss: 0.4476584494113922:  70%|█████████    | 83/119 [00:12<00:06,  5.74it/s]Epoch: 5, train for the 84-th batch, train loss: 0.4476584494113922:  71%|█████████▏   | 84/119 [00:12<00:05,  5.98it/s]evaluate for the 24-th batch, evaluate loss: 0.5040654540061951:  65%|███████████▋      | 22/34 [00:01<00:00, 12.58it/s]evaluate for the 24-th batch, evaluate loss: 0.5040654540061951:  71%|████████████▋     | 24/34 [00:01<00:00, 12.68it/s]evaluate for the 18-th batch, evaluate loss: 0.6600977778434753:  64%|███████████▌      | 16/25 [00:01<00:00, 10.90it/s]evaluate for the 18-th batch, evaluate loss: 0.6600977778434753:  72%|████████████▉     | 18/25 [00:01<00:00, 10.27it/s]Epoch: 5, train for the 85-th batch, train loss: 0.4371538460254669:  71%|█████████▏   | 84/119 [00:12<00:05,  5.98it/s]Epoch: 5, train for the 85-th batch, train loss: 0.4371538460254669:  71%|█████████▎   | 85/119 [00:12<00:05,  6.18it/s]evaluate for the 25-th batch, evaluate loss: 0.501481831073761:  71%|█████████████▍     | 24/34 [00:01<00:00, 12.68it/s]evaluate for the 19-th batch, evaluate loss: 0.6501032710075378:  72%|████████████▉     | 18/25 [00:01<00:00, 10.27it/s]  0%|                                                                                            | 0/38 [00:00<?, ?it/s]Epoch: 2, train for the 151-th batch, train loss: 0.5882560014724731:  63%|██████▉    | 150/237 [00:27<00:20,  4.21it/s]evaluate for the 26-th batch, evaluate loss: 0.6170332431793213:  71%|████████████▋     | 24/34 [00:02<00:00, 12.68it/s]evaluate for the 26-th batch, evaluate loss: 0.6170332431793213:  76%|█████████████▊    | 26/34 [00:02<00:00, 12.15it/s]evaluate for the 20-th batch, evaluate loss: 0.6769581437110901:  72%|████████████▉     | 18/25 [00:01<00:00, 10.27it/s]evaluate for the 20-th batch, evaluate loss: 0.6769581437110901:  80%|██████████████▍   | 20/25 [00:01<00:00, 10.37it/s]Epoch: 2, train for the 151-th batch, train loss: 0.5882560014724731:  64%|███████    | 151/237 [00:27<00:23,  3.71it/s]Epoch: 5, train for the 86-th batch, train loss: 0.4445527493953705:  71%|█████████▎   | 85/119 [00:13<00:05,  6.18it/s]evaluate for the 1-th batch, evaluate loss: 0.4667387306690216:   0%|                            | 0/38 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.4667387306690216:   3%|▌                   | 1/38 [00:00<00:04,  8.49it/s]evaluate for the 27-th batch, evaluate loss: 0.6325197219848633:  76%|█████████████▊    | 26/34 [00:02<00:00, 12.15it/s]Epoch: 5, train for the 86-th batch, train loss: 0.4445527493953705:  72%|█████████▍   | 86/119 [00:13<00:05,  5.99it/s]evaluate for the 21-th batch, evaluate loss: 0.7134506106376648:  80%|██████████████▍   | 20/25 [00:01<00:00, 10.37it/s]Epoch: 1, train for the 350-th batch, train loss: 0.38621389865875244:  91%|█████████ | 349/383 [01:49<00:15,  2.17it/s]evaluate for the 2-th batch, evaluate loss: 0.48084643483161926:   3%|▌                  | 1/38 [00:00<00:04,  8.49it/s]evaluate for the 28-th batch, evaluate loss: 0.5756469368934631:  76%|█████████████▊    | 26/34 [00:02<00:00, 12.15it/s]evaluate for the 28-th batch, evaluate loss: 0.5756469368934631:  82%|██████████████▊   | 28/34 [00:02<00:00, 12.59it/s]Epoch: 1, train for the 350-th batch, train loss: 0.38621389865875244:  91%|█████████▏| 350/383 [01:49<00:15,  2.16it/s]evaluate for the 22-th batch, evaluate loss: 0.6560938358306885:  80%|██████████████▍   | 20/25 [00:02<00:00, 10.37it/s]evaluate for the 22-th batch, evaluate loss: 0.6560938358306885:  88%|███████████████▊  | 22/25 [00:02<00:00, 10.69it/s]Epoch: 2, train for the 152-th batch, train loss: 0.5465084910392761:  64%|███████    | 151/237 [00:27<00:23,  3.71it/s]evaluate for the 29-th batch, evaluate loss: 0.568164050579071:  82%|███████████████▋   | 28/34 [00:02<00:00, 12.59it/s]evaluate for the 3-th batch, evaluate loss: 0.45949533581733704:   3%|▌                  | 1/38 [00:00<00:04,  8.49it/s]evaluate for the 3-th batch, evaluate loss: 0.45949533581733704:   8%|█▌                 | 3/38 [00:00<00:03, 11.49it/s]Epoch: 2, train for the 152-th batch, train loss: 0.5465084910392761:  64%|███████    | 152/237 [00:27<00:21,  4.01it/s]Epoch: 5, train for the 87-th batch, train loss: 0.4665548503398895:  72%|█████████▍   | 86/119 [00:13<00:05,  5.99it/s]Epoch: 5, train for the 87-th batch, train loss: 0.4665548503398895:  73%|█████████▌   | 87/119 [00:13<00:05,  5.84it/s]evaluate for the 4-th batch, evaluate loss: 0.47114279866218567:   8%|█▌                 | 3/38 [00:00<00:03, 11.49it/s]evaluate for the 23-th batch, evaluate loss: 0.673468291759491:  88%|████████████████▋  | 22/25 [00:02<00:00, 10.69it/s]evaluate for the 30-th batch, evaluate loss: 0.5331071615219116:  82%|██████████████▊   | 28/34 [00:02<00:00, 12.59it/s]evaluate for the 30-th batch, evaluate loss: 0.5331071615219116:  88%|███████████████▉  | 30/34 [00:02<00:00, 13.18it/s]evaluate for the 5-th batch, evaluate loss: 0.515911340713501:   8%|█▋                   | 3/38 [00:00<00:03, 11.49it/s]evaluate for the 5-th batch, evaluate loss: 0.515911340713501:  13%|██▊                  | 5/38 [00:00<00:02, 12.24it/s]evaluate for the 24-th batch, evaluate loss: 0.6864351034164429:  88%|███████████████▊  | 22/25 [00:02<00:00, 10.69it/s]evaluate for the 24-th batch, evaluate loss: 0.6864351034164429:  96%|█████████████████▎| 24/25 [00:02<00:00, 10.60it/s]evaluate for the 31-th batch, evaluate loss: 0.6408581137657166:  88%|███████████████▉  | 30/34 [00:02<00:00, 13.18it/s]Epoch: 5, train for the 88-th batch, train loss: 0.47868266701698303:  73%|████████▊   | 87/119 [00:13<00:05,  5.84it/s]Epoch: 5, train for the 88-th batch, train loss: 0.47868266701698303:  74%|████████▊   | 88/119 [00:13<00:05,  5.90it/s]evaluate for the 6-th batch, evaluate loss: 0.46798795461654663:  13%|██▌                | 5/38 [00:00<00:02, 12.24it/s]evaluate for the 25-th batch, evaluate loss: 0.6553000211715698:  96%|█████████████████▎| 24/25 [00:02<00:00, 10.60it/s]evaluate for the 25-th batch, evaluate loss: 0.6553000211715698: 100%|██████████████████| 25/25 [00:02<00:00, 10.72it/s]
evaluate for the 7-th batch, evaluate loss: 0.43757864832878113:  13%|██▌                | 5/38 [00:00<00:02, 12.24it/s]evaluate for the 7-th batch, evaluate loss: 0.43757864832878113:  18%|███▌               | 7/38 [00:00<00:02, 13.45it/s]evaluate for the 32-th batch, evaluate loss: 0.6234452128410339:  88%|███████████████▉  | 30/34 [00:02<00:00, 13.18it/s]evaluate for the 32-th batch, evaluate loss: 0.6234452128410339:  94%|████████████████▉ | 32/34 [00:02<00:00, 11.71it/s]Epoch: 1, train for the 351-th batch, train loss: 0.4078419506549835:  91%|██████████ | 350/383 [01:50<00:15,  2.16it/s]Epoch: 2, train for the 153-th batch, train loss: 0.5411514043807983:  64%|███████    | 152/237 [00:27<00:21,  4.01it/s]Epoch: 2, train for the 153-th batch, train loss: 0.5411514043807983:  65%|███████    | 153/237 [00:27<00:23,  3.65it/s]Epoch: 5, train for the 89-th batch, train loss: 0.45693275332450867:  74%|████████▊   | 88/119 [00:13<00:05,  5.90it/s]evaluate for the 8-th batch, evaluate loss: 0.44845443964004517:  18%|███▌               | 7/38 [00:00<00:02, 13.45it/s]Epoch: 5, train for the 89-th batch, train loss: 0.45693275332450867:  75%|████████▉   | 89/119 [00:13<00:04,  6.08it/s]evaluate for the 33-th batch, evaluate loss: 0.6415780782699585:  94%|████████████████▉ | 32/34 [00:02<00:00, 11.71it/s]Epoch: 1, train for the 351-th batch, train loss: 0.4078419506549835:  92%|██████████ | 351/383 [01:50<00:14,  2.21it/s]evaluate for the 9-th batch, evaluate loss: 0.4625319838523865:  18%|███▋                | 7/38 [00:00<00:02, 13.45it/s]evaluate for the 9-th batch, evaluate loss: 0.4625319838523865:  24%|████▋               | 9/38 [00:00<00:02, 12.83it/s]evaluate for the 34-th batch, evaluate loss: 0.6878640055656433:  94%|████████████████▉ | 32/34 [00:02<00:00, 11.71it/s]evaluate for the 34-th batch, evaluate loss: 0.6878640055656433: 100%|██████████████████| 34/34 [00:02<00:00, 11.05it/s]evaluate for the 34-th batch, evaluate loss: 0.6878640055656433: 100%|██████████████████| 34/34 [00:02<00:00, 12.56it/s]
INFO:root:Epoch: 3, learning rate: 0.0001, train loss: 0.5799
INFO:root:train average_precision, 0.7632
INFO:root:train roc_auc, 0.7407
INFO:root:validate loss: 0.5157
INFO:root:validate average_precision, 0.8394
INFO:root:validate roc_auc, 0.8354
INFO:root:new node validate loss: 0.7178
INFO:root:new node validate first_1_average_precision, 0.5439
INFO:root:new node validate first_1_roc_auc, 0.5521
INFO:root:new node validate first_3_average_precision, 0.5815
INFO:root:new node validate first_3_roc_auc, 0.5878
INFO:root:new node validate first_10_average_precision, 0.6382
INFO:root:new node validate first_10_roc_auc, 0.6473
INFO:root:new node validate average_precision, 0.6526
INFO:root:new node validate roc_auc, 0.6567
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]Epoch: 5, train for the 90-th batch, train loss: 0.4465075731277466:  75%|█████████▋   | 89/119 [00:13<00:04,  6.08it/s]evaluate for the 10-th batch, evaluate loss: 0.49390920996665955:  24%|████▎             | 9/38 [00:00<00:02, 12.83it/s]Epoch: 5, train for the 90-th batch, train loss: 0.4465075731277466:  76%|█████████▊   | 90/119 [00:13<00:04,  6.01it/s]Epoch: 2, train for the 154-th batch, train loss: 0.5992535352706909:  65%|███████    | 153/237 [00:27<00:23,  3.65it/s]evaluate for the 11-th batch, evaluate loss: 0.4491596519947052:  24%|████▌              | 9/38 [00:00<00:02, 12.83it/s]evaluate for the 11-th batch, evaluate loss: 0.4491596519947052:  29%|█████▏            | 11/38 [00:00<00:02, 13.48it/s]Epoch: 2, train for the 154-th batch, train loss: 0.5992535352706909:  65%|███████▏   | 154/237 [00:27<00:22,  3.74it/s]evaluate for the 12-th batch, evaluate loss: 0.524057924747467:  29%|█████▌             | 11/38 [00:00<00:02, 13.48it/s]Epoch: 1, train for the 352-th batch, train loss: 0.34303024411201477:  92%|█████████▏| 351/383 [01:50<00:14,  2.21it/s]Epoch: 4, train for the 1-th batch, train loss: 0.7979804873466492:   0%|                       | 0/151 [00:00<?, ?it/s]Epoch: 4, train for the 1-th batch, train loss: 0.7979804873466492:   1%|               | 1/151 [00:00<00:19,  7.66it/s]Epoch: 5, train for the 91-th batch, train loss: 0.4269798994064331:  76%|█████████▊   | 90/119 [00:13<00:04,  6.01it/s]Epoch: 5, train for the 91-th batch, train loss: 0.4269798994064331:  76%|█████████▉   | 91/119 [00:13<00:04,  6.28it/s]evaluate for the 13-th batch, evaluate loss: 0.48601794242858887:  29%|████▉            | 11/38 [00:00<00:02, 13.48it/s]Epoch: 1, train for the 352-th batch, train loss: 0.34303024411201477:  92%|█████████▏| 352/383 [01:50<00:13,  2.38it/s]evaluate for the 14-th batch, evaluate loss: 0.42380285263061523:  29%|████▉            | 11/38 [00:01<00:02, 13.48it/s]evaluate for the 14-th batch, evaluate loss: 0.42380285263061523:  37%|██████▎          | 14/38 [00:01<00:01, 15.14it/s]Epoch: 4, train for the 2-th batch, train loss: 0.7916755676269531:   1%|               | 1/151 [00:00<00:19,  7.66it/s]Epoch: 2, train for the 155-th batch, train loss: 0.5900780558586121:  65%|███████▏   | 154/237 [00:28<00:22,  3.74it/s]Epoch: 2, train for the 155-th batch, train loss: 0.5900780558586121:  65%|███████▏   | 155/237 [00:28<00:19,  4.12it/s]Epoch: 5, train for the 92-th batch, train loss: 0.47148367762565613:  76%|█████████▏  | 91/119 [00:14<00:04,  6.28it/s]evaluate for the 15-th batch, evaluate loss: 0.4430517554283142:  37%|██████▋           | 14/38 [00:01<00:01, 15.14it/s]Epoch: 5, train for the 92-th batch, train loss: 0.47148367762565613:  77%|█████████▎  | 92/119 [00:14<00:04,  6.41it/s]evaluate for the 16-th batch, evaluate loss: 0.4972245693206787:  37%|██████▋           | 14/38 [00:01<00:01, 15.14it/s]evaluate for the 16-th batch, evaluate loss: 0.4972245693206787:  42%|███████▌          | 16/38 [00:01<00:01, 14.98it/s]INFO:root:Epoch: 2, learning rate: 0.0001, train loss: 0.4787
INFO:root:train average_precision, 0.8574
INFO:root:train roc_auc, 0.8267
INFO:root:validate loss: 0.4850
INFO:root:validate average_precision, 0.8677
INFO:root:validate roc_auc, 0.8486
INFO:root:new node validate loss: 0.5967
INFO:root:new node validate first_1_average_precision, 0.5275
INFO:root:new node validate first_1_roc_auc, 0.5337
INFO:root:new node validate first_3_average_precision, 0.6050
INFO:root:new node validate first_3_roc_auc, 0.5926
INFO:root:new node validate first_10_average_precision, 0.6909
INFO:root:new node validate first_10_roc_auc, 0.6717
INFO:root:new node validate average_precision, 0.7538
INFO:root:new node validate roc_auc, 0.7239
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/241 [00:00<?, ?it/s]Epoch: 4, train for the 3-th batch, train loss: 0.7947940826416016:   1%|               | 1/151 [00:00<00:19,  7.66it/s]Epoch: 4, train for the 3-th batch, train loss: 0.7947940826416016:   2%|▎              | 3/151 [00:00<00:19,  7.43it/s]Epoch: 2, train for the 156-th batch, train loss: 0.5714972019195557:  65%|███████▏   | 155/237 [00:28<00:19,  4.12it/s]Epoch: 5, train for the 93-th batch, train loss: 0.41556134819984436:  77%|█████████▎  | 92/119 [00:14<00:04,  6.41it/s]evaluate for the 17-th batch, evaluate loss: 0.4472479820251465:  42%|███████▌          | 16/38 [00:01<00:01, 14.98it/s]Epoch: 3, train for the 1-th batch, train loss: 0.6453409194946289:   0%|                       | 0/241 [00:00<?, ?it/s]Epoch: 5, train for the 93-th batch, train loss: 0.41556134819984436:  78%|█████████▍  | 93/119 [00:14<00:04,  6.10it/s]Epoch: 2, train for the 156-th batch, train loss: 0.5714972019195557:  66%|███████▏   | 156/237 [00:28<00:19,  4.17it/s]Epoch: 1, train for the 353-th batch, train loss: 0.47624334692955017:  92%|█████████▏| 352/383 [01:50<00:13,  2.38it/s]evaluate for the 18-th batch, evaluate loss: 0.49435481429100037:  42%|███████▏         | 16/38 [00:01<00:01, 14.98it/s]evaluate for the 18-th batch, evaluate loss: 0.49435481429100037:  47%|████████         | 18/38 [00:01<00:01, 13.79it/s]Epoch: 3, train for the 2-th batch, train loss: 0.47260141372680664:   0%|                      | 0/241 [00:00<?, ?it/s]Epoch: 3, train for the 2-th batch, train loss: 0.47260141372680664:   1%|              | 2/241 [00:00<00:22, 10.51it/s]Epoch: 1, train for the 353-th batch, train loss: 0.47624334692955017:  92%|█████████▏| 353/383 [01:50<00:12,  2.45it/s]Epoch: 5, train for the 94-th batch, train loss: 0.41715171933174133:  78%|█████████▍  | 93/119 [00:14<00:04,  6.10it/s]evaluate for the 19-th batch, evaluate loss: 0.46013450622558594:  47%|████████         | 18/38 [00:01<00:01, 13.79it/s]Epoch: 5, train for the 94-th batch, train loss: 0.41715171933174133:  79%|█████████▍  | 94/119 [00:14<00:04,  6.10it/s]Epoch: 2, train for the 157-th batch, train loss: 0.5856615900993347:  66%|███████▏   | 156/237 [00:28<00:19,  4.17it/s]evaluate for the 20-th batch, evaluate loss: 0.39918017387390137:  47%|████████         | 18/38 [00:01<00:01, 13.79it/s]evaluate for the 20-th batch, evaluate loss: 0.39918017387390137:  53%|████████▉        | 20/38 [00:01<00:01, 13.63it/s]Epoch: 4, train for the 4-th batch, train loss: 0.7419929504394531:   2%|▎              | 3/151 [00:00<00:19,  7.43it/s]Epoch: 4, train for the 4-th batch, train loss: 0.7419929504394531:   3%|▍              | 4/151 [00:00<00:27,  5.32it/s]Epoch: 3, train for the 3-th batch, train loss: 0.5003690719604492:   1%|               | 2/241 [00:00<00:22, 10.51it/s]Epoch: 2, train for the 157-th batch, train loss: 0.5856615900993347:  66%|███████▎   | 157/237 [00:28<00:18,  4.21it/s]evaluate for the 21-th batch, evaluate loss: 0.43167704343795776:  53%|████████▉        | 20/38 [00:01<00:01, 13.63it/s]Epoch: 5, train for the 95-th batch, train loss: 0.3950212895870209:  79%|██████████▎  | 94/119 [00:14<00:04,  6.10it/s]evaluate for the 22-th batch, evaluate loss: 0.4611915349960327:  53%|█████████▍        | 20/38 [00:01<00:01, 13.63it/s]evaluate for the 22-th batch, evaluate loss: 0.4611915349960327:  58%|██████████▍       | 22/38 [00:01<00:01, 13.56it/s]Epoch: 5, train for the 95-th batch, train loss: 0.3950212895870209:  80%|██████████▍  | 95/119 [00:14<00:04,  5.83it/s]Epoch: 3, train for the 4-th batch, train loss: 0.47336074709892273:   1%|              | 2/241 [00:00<00:22, 10.51it/s]Epoch: 3, train for the 4-th batch, train loss: 0.47336074709892273:   2%|▏             | 4/241 [00:00<00:28,  8.28it/s]Epoch: 4, train for the 5-th batch, train loss: 0.7317431569099426:   3%|▍              | 4/151 [00:00<00:27,  5.32it/s]Epoch: 4, train for the 5-th batch, train loss: 0.7317431569099426:   3%|▍              | 5/151 [00:00<00:27,  5.32it/s]evaluate for the 23-th batch, evaluate loss: 0.45330074429512024:  58%|█████████▊       | 22/38 [00:01<00:01, 13.56it/s]Epoch: 1, train for the 354-th batch, train loss: 0.3731249272823334:  92%|██████████▏| 353/383 [01:51<00:12,  2.45it/s]Epoch: 2, train for the 158-th batch, train loss: 0.539509117603302:  66%|███████▉    | 157/237 [00:28<00:18,  4.21it/s]Epoch: 1, train for the 354-th batch, train loss: 0.3731249272823334:  92%|██████████▏| 354/383 [01:51<00:11,  2.51it/s]Epoch: 2, train for the 158-th batch, train loss: 0.539509117603302:  67%|████████    | 158/237 [00:28<00:19,  4.15it/s]Epoch: 3, train for the 5-th batch, train loss: 0.5198110938072205:   2%|▏              | 4/241 [00:00<00:28,  8.28it/s]Epoch: 3, train for the 5-th batch, train loss: 0.5198110938072205:   2%|▎              | 5/241 [00:00<00:28,  8.15it/s]Epoch: 5, train for the 96-th batch, train loss: 0.3952072858810425:  80%|██████████▍  | 95/119 [00:14<00:04,  5.83it/s]evaluate for the 24-th batch, evaluate loss: 0.43051305413246155:  58%|█████████▊       | 22/38 [00:01<00:01, 13.56it/s]evaluate for the 24-th batch, evaluate loss: 0.43051305413246155:  63%|██████████▋      | 24/38 [00:01<00:01, 13.05it/s]Epoch: 5, train for the 96-th batch, train loss: 0.3952072858810425:  81%|██████████▍  | 96/119 [00:14<00:03,  5.85it/s]Epoch: 4, train for the 6-th batch, train loss: 0.7218374609947205:   3%|▍              | 5/151 [00:01<00:27,  5.32it/s]Epoch: 4, train for the 6-th batch, train loss: 0.7218374609947205:   4%|▌              | 6/151 [00:01<00:25,  5.67it/s]evaluate for the 25-th batch, evaluate loss: 0.47934672236442566:  63%|██████████▋      | 24/38 [00:01<00:01, 13.05it/s]Epoch: 3, train for the 6-th batch, train loss: 0.693174421787262:   2%|▎               | 5/241 [00:00<00:28,  8.15it/s]Epoch: 3, train for the 6-th batch, train loss: 0.693174421787262:   2%|▍               | 6/241 [00:00<00:28,  8.23it/s]Epoch: 2, train for the 159-th batch, train loss: 0.6045243144035339:  67%|███████▎   | 158/237 [00:29<00:19,  4.15it/s]Epoch: 5, train for the 97-th batch, train loss: 0.4256378710269928:  81%|██████████▍  | 96/119 [00:14<00:03,  5.85it/s]Epoch: 2, train for the 159-th batch, train loss: 0.6045243144035339:  67%|███████▍   | 159/237 [00:29<00:17,  4.40it/s]evaluate for the 26-th batch, evaluate loss: 0.4532012939453125:  63%|███████████▎      | 24/38 [00:01<00:01, 13.05it/s]evaluate for the 26-th batch, evaluate loss: 0.4532012939453125:  68%|████████████▎     | 26/38 [00:01<00:00, 12.59it/s]Epoch: 5, train for the 97-th batch, train loss: 0.4256378710269928:  82%|██████████▌  | 97/119 [00:14<00:03,  5.89it/s]Epoch: 4, train for the 7-th batch, train loss: 0.6900510787963867:   4%|▌              | 6/151 [00:01<00:25,  5.67it/s]Epoch: 4, train for the 7-th batch, train loss: 0.6900510787963867:   5%|▋              | 7/151 [00:01<00:24,  5.87it/s]Epoch: 3, train for the 7-th batch, train loss: 0.4328603744506836:   2%|▎              | 6/241 [00:00<00:28,  8.23it/s]Epoch: 1, train for the 355-th batch, train loss: 0.5103182196617126:  92%|██████████▏| 354/383 [01:51<00:11,  2.51it/s]Epoch: 3, train for the 7-th batch, train loss: 0.4328603744506836:   3%|▍              | 7/241 [00:00<00:27,  8.56it/s]evaluate for the 27-th batch, evaluate loss: 0.45770010352134705:  68%|███████████▋     | 26/38 [00:02<00:00, 12.59it/s]Epoch: 1, train for the 355-th batch, train loss: 0.5103182196617126:  93%|██████████▏| 355/383 [01:51<00:10,  2.65it/s]Epoch: 5, train for the 98-th batch, train loss: 0.4095601439476013:  82%|██████████▌  | 97/119 [00:15<00:03,  5.89it/s]evaluate for the 28-th batch, evaluate loss: 0.47943633794784546:  68%|███████████▋     | 26/38 [00:02<00:00, 12.59it/s]evaluate for the 28-th batch, evaluate loss: 0.47943633794784546:  74%|████████████▌    | 28/38 [00:02<00:00, 12.97it/s]Epoch: 5, train for the 98-th batch, train loss: 0.4095601439476013:  82%|██████████▋  | 98/119 [00:15<00:03,  6.20it/s]Epoch: 4, train for the 8-th batch, train loss: 0.6887977719306946:   5%|▋              | 7/151 [00:01<00:24,  5.87it/s]Epoch: 4, train for the 8-th batch, train loss: 0.6887977719306946:   5%|▊              | 8/151 [00:01<00:23,  6.14it/s]Epoch: 2, train for the 160-th batch, train loss: 0.5876363515853882:  67%|███████▍   | 159/237 [00:29<00:17,  4.40it/s]Epoch: 3, train for the 8-th batch, train loss: 0.7470554709434509:   3%|▍              | 7/241 [00:00<00:27,  8.56it/s]Epoch: 3, train for the 8-th batch, train loss: 0.7470554709434509:   3%|▍              | 8/241 [00:00<00:30,  7.73it/s]evaluate for the 29-th batch, evaluate loss: 0.4583348333835602:  74%|█████████████▎    | 28/38 [00:02<00:00, 12.97it/s]Epoch: 2, train for the 160-th batch, train loss: 0.5876363515853882:  68%|███████▍   | 160/237 [00:29<00:17,  4.33it/s]Epoch: 5, train for the 99-th batch, train loss: 0.41046664118766785:  82%|█████████▉  | 98/119 [00:15<00:03,  6.20it/s]evaluate for the 30-th batch, evaluate loss: 0.47365501523017883:  74%|████████████▌    | 28/38 [00:02<00:00, 12.97it/s]evaluate for the 30-th batch, evaluate loss: 0.47365501523017883:  79%|█████████████▍   | 30/38 [00:02<00:00, 13.19it/s]Epoch: 5, train for the 99-th batch, train loss: 0.41046664118766785:  83%|█████████▉  | 99/119 [00:15<00:03,  6.36it/s]Epoch: 4, train for the 9-th batch, train loss: 0.658051609992981:   5%|▊               | 8/151 [00:01<00:23,  6.14it/s]Epoch: 4, train for the 9-th batch, train loss: 0.658051609992981:   6%|▉               | 9/151 [00:01<00:22,  6.33it/s]Epoch: 3, train for the 9-th batch, train loss: 0.6357308030128479:   3%|▍              | 8/241 [00:01<00:30,  7.73it/s]Epoch: 3, train for the 9-th batch, train loss: 0.6357308030128479:   4%|▌              | 9/241 [00:01<00:30,  7.72it/s]evaluate for the 31-th batch, evaluate loss: 0.46973904967308044:  79%|█████████████▍   | 30/38 [00:02<00:00, 13.19it/s]Epoch: 2, train for the 161-th batch, train loss: 0.6146584749221802:  68%|███████▍   | 160/237 [00:29<00:17,  4.33it/s]Epoch: 5, train for the 100-th batch, train loss: 0.4372604191303253:  83%|█████████▉  | 99/119 [00:15<00:03,  6.36it/s]evaluate for the 32-th batch, evaluate loss: 0.42965689301490784:  79%|█████████████▍   | 30/38 [00:02<00:00, 13.19it/s]evaluate for the 32-th batch, evaluate loss: 0.42965689301490784:  84%|██████████████▎  | 32/38 [00:02<00:00, 13.06it/s]Epoch: 2, train for the 161-th batch, train loss: 0.6146584749221802:  68%|███████▍   | 161/237 [00:29<00:17,  4.45it/s]Epoch: 5, train for the 100-th batch, train loss: 0.4372604191303253:  84%|█████████▏ | 100/119 [00:15<00:02,  6.34it/s]Epoch: 4, train for the 10-th batch, train loss: 0.6494982838630676:   6%|▊             | 9/151 [00:01<00:22,  6.33it/s]Epoch: 3, train for the 10-th batch, train loss: 0.553432822227478:   4%|▌              | 9/241 [00:01<00:30,  7.72it/s]Epoch: 4, train for the 10-th batch, train loss: 0.6494982838630676:   7%|▊            | 10/151 [00:01<00:22,  6.14it/s]Epoch: 3, train for the 10-th batch, train loss: 0.553432822227478:   4%|▌             | 10/241 [00:01<00:32,  7.13it/s]evaluate for the 33-th batch, evaluate loss: 0.4765419661998749:  84%|███████████████▏  | 32/38 [00:02<00:00, 13.06it/s]Epoch: 5, train for the 101-th batch, train loss: 0.47296708822250366:  84%|████████▍ | 100/119 [00:15<00:02,  6.34it/s]evaluate for the 34-th batch, evaluate loss: 0.4790532886981964:  84%|███████████████▏  | 32/38 [00:02<00:00, 13.06it/s]Epoch: 5, train for the 101-th batch, train loss: 0.47296708822250366:  85%|████████▍ | 101/119 [00:15<00:02,  6.61it/s]evaluate for the 34-th batch, evaluate loss: 0.4790532886981964:  89%|████████████████  | 34/38 [00:02<00:00, 13.35it/s]Epoch: 2, train for the 162-th batch, train loss: 0.5807481408119202:  68%|███████▍   | 161/237 [00:29<00:17,  4.45it/s]Epoch: 2, train for the 162-th batch, train loss: 0.5807481408119202:  68%|███████▌   | 162/237 [00:29<00:15,  4.70it/s]evaluate for the 35-th batch, evaluate loss: 0.4800543785095215:  89%|████████████████  | 34/38 [00:02<00:00, 13.35it/s]Epoch: 4, train for the 11-th batch, train loss: 0.6721483469009399:   7%|▊            | 10/151 [00:01<00:22,  6.14it/s]Epoch: 1, train for the 356-th batch, train loss: 0.33674678206443787:  93%|█████████▎| 355/383 [01:52<00:10,  2.65it/s]Epoch: 3, train for the 11-th batch, train loss: 0.38051897287368774:   4%|▍           | 10/241 [00:01<00:32,  7.13it/s]Epoch: 4, train for the 11-th batch, train loss: 0.6721483469009399:   7%|▉            | 11/151 [00:01<00:23,  6.04it/s]Epoch: 3, train for the 11-th batch, train loss: 0.38051897287368774:   5%|▌           | 11/241 [00:01<00:34,  6.74it/s]Epoch: 5, train for the 102-th batch, train loss: 0.4598618447780609:  85%|█████████▎ | 101/119 [00:15<00:02,  6.61it/s]Epoch: 5, train for the 102-th batch, train loss: 0.4598618447780609:  86%|█████████▍ | 102/119 [00:15<00:02,  6.97it/s]evaluate for the 36-th batch, evaluate loss: 0.47752645611763:  89%|█████████████████▉  | 34/38 [00:02<00:00, 13.35it/s]evaluate for the 36-th batch, evaluate loss: 0.47752645611763:  95%|██████████████████▉ | 36/38 [00:02<00:00, 13.93it/s]Epoch: 1, train for the 356-th batch, train loss: 0.33674678206443787:  93%|█████████▎| 356/383 [01:52<00:12,  2.23it/s]evaluate for the 37-th batch, evaluate loss: 0.4240386188030243:  95%|█████████████████ | 36/38 [00:02<00:00, 13.93it/s]Epoch: 3, train for the 12-th batch, train loss: 0.5770924091339111:   5%|▌            | 11/241 [00:01<00:34,  6.74it/s]Epoch: 4, train for the 12-th batch, train loss: 0.6792262196540833:   7%|▉            | 11/151 [00:02<00:23,  6.04it/s]Epoch: 4, train for the 12-th batch, train loss: 0.6792262196540833:   8%|█            | 12/151 [00:02<00:23,  5.85it/s]Epoch: 3, train for the 12-th batch, train loss: 0.5770924091339111:   5%|▋            | 12/241 [00:01<00:35,  6.37it/s]Epoch: 5, train for the 103-th batch, train loss: 0.43961989879608154:  86%|████████▌ | 102/119 [00:15<00:02,  6.97it/s]evaluate for the 38-th batch, evaluate loss: 0.4557740390300751:  95%|█████████████████ | 36/38 [00:02<00:00, 13.93it/s]evaluate for the 38-th batch, evaluate loss: 0.4557740390300751: 100%|██████████████████| 38/38 [00:02<00:00, 14.06it/s]evaluate for the 38-th batch, evaluate loss: 0.4557740390300751: 100%|██████████████████| 38/38 [00:02<00:00, 13.45it/s]
Epoch: 5, train for the 103-th batch, train loss: 0.43961989879608154:  87%|████████▋ | 103/119 [00:15<00:02,  6.88it/s]Epoch: 4, train for the 13-th batch, train loss: 0.6888024806976318:   8%|█            | 12/151 [00:02<00:23,  5.85it/s]Epoch: 5, train for the 104-th batch, train loss: 0.4008753001689911:  87%|█████████▌ | 103/119 [00:15<00:02,  6.88it/s]Epoch: 3, train for the 13-th batch, train loss: 0.5372060537338257:   5%|▋            | 12/241 [00:01<00:35,  6.37it/s]Epoch: 4, train for the 13-th batch, train loss: 0.6888024806976318:   9%|█            | 13/151 [00:02<00:23,  5.81it/s]Epoch: 5, train for the 104-th batch, train loss: 0.4008753001689911:  87%|█████████▌ | 104/119 [00:16<00:02,  6.73it/s]Epoch: 3, train for the 13-th batch, train loss: 0.5372060537338257:   5%|▋            | 13/241 [00:01<00:37,  6.09it/s]  0%|                                                                                            | 0/20 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.7427001595497131:   0%|                            | 0/20 [00:00<?, ?it/s]Epoch: 1, train for the 357-th batch, train loss: 0.40967628359794617:  93%|█████████▎| 356/383 [01:52<00:12,  2.23it/s]Epoch: 2, train for the 163-th batch, train loss: 0.626265287399292:  68%|████████▏   | 162/237 [00:30<00:15,  4.70it/s]Epoch: 3, train for the 14-th batch, train loss: 0.5911222696304321:   5%|▋            | 13/241 [00:01<00:37,  6.09it/s]Epoch: 3, train for the 14-th batch, train loss: 0.5911222696304321:   6%|▊            | 14/241 [00:01<00:35,  6.41it/s]Epoch: 2, train for the 163-th batch, train loss: 0.626265287399292:  69%|████████▎   | 163/237 [00:30<00:22,  3.23it/s]Epoch: 5, train for the 105-th batch, train loss: 0.4392102062702179:  87%|█████████▌ | 104/119 [00:16<00:02,  6.73it/s]evaluate for the 2-th batch, evaluate loss: 0.7524234056472778:   0%|                            | 0/20 [00:00<?, ?it/s]Epoch: 4, train for the 14-th batch, train loss: 0.7105221152305603:   9%|█            | 13/151 [00:02<00:23,  5.81it/s]evaluate for the 2-th batch, evaluate loss: 0.7524234056472778:  10%|██                  | 2/20 [00:00<00:01, 12.65it/s]Epoch: 5, train for the 105-th batch, train loss: 0.4392102062702179:  88%|█████████▋ | 105/119 [00:16<00:02,  6.42it/s]Epoch: 1, train for the 357-th batch, train loss: 0.40967628359794617:  93%|█████████▎| 357/383 [01:52<00:11,  2.22it/s]Epoch: 4, train for the 14-th batch, train loss: 0.7105221152305603:   9%|█▏           | 14/151 [00:02<00:24,  5.65it/s]Epoch: 3, train for the 15-th batch, train loss: 0.5523543357849121:   6%|▊            | 14/241 [00:02<00:35,  6.41it/s]evaluate for the 3-th batch, evaluate loss: 0.6273123621940613:  10%|██                  | 2/20 [00:00<00:01, 12.65it/s]Epoch: 3, train for the 15-th batch, train loss: 0.5523543357849121:   6%|▊            | 15/241 [00:02<00:32,  6.93it/s]Epoch: 4, train for the 15-th batch, train loss: 0.6937746405601501:   9%|█▏           | 14/151 [00:02<00:24,  5.65it/s]Epoch: 4, train for the 15-th batch, train loss: 0.6937746405601501:  10%|█▎           | 15/151 [00:02<00:23,  5.84it/s]Epoch: 5, train for the 106-th batch, train loss: 0.4261768162250519:  88%|█████████▋ | 105/119 [00:16<00:02,  6.42it/s]evaluate for the 4-th batch, evaluate loss: 0.6683066487312317:  10%|██                  | 2/20 [00:00<00:01, 12.65it/s]evaluate for the 4-th batch, evaluate loss: 0.6683066487312317:  20%|████                | 4/20 [00:00<00:01, 11.45it/s]Epoch: 5, train for the 106-th batch, train loss: 0.4261768162250519:  89%|█████████▊ | 106/119 [00:16<00:02,  6.05it/s]Epoch: 2, train for the 164-th batch, train loss: 0.6142906546592712:  69%|███████▌   | 163/237 [00:30<00:22,  3.23it/s]Epoch: 3, train for the 16-th batch, train loss: 0.49599021673202515:   6%|▋           | 15/241 [00:02<00:32,  6.93it/s]Epoch: 3, train for the 16-th batch, train loss: 0.49599021673202515:   7%|▊           | 16/241 [00:02<00:30,  7.41it/s]Epoch: 2, train for the 164-th batch, train loss: 0.6142906546592712:  69%|███████▌   | 164/237 [00:30<00:21,  3.43it/s]evaluate for the 5-th batch, evaluate loss: 0.6598109006881714:  20%|████                | 4/20 [00:00<00:01, 11.45it/s]Epoch: 4, train for the 16-th batch, train loss: 0.7045642733573914:  10%|█▎           | 15/151 [00:02<00:23,  5.84it/s]Epoch: 4, train for the 16-th batch, train loss: 0.7045642733573914:  11%|█▍           | 16/151 [00:02<00:22,  5.88it/s]Epoch: 3, train for the 17-th batch, train loss: 0.4336969554424286:   7%|▊            | 16/241 [00:02<00:30,  7.41it/s]Epoch: 3, train for the 17-th batch, train loss: 0.4336969554424286:   7%|▉            | 17/241 [00:02<00:31,  7.11it/s]Epoch: 5, train for the 107-th batch, train loss: 0.4353167712688446:  89%|█████████▊ | 106/119 [00:16<00:02,  6.05it/s]evaluate for the 6-th batch, evaluate loss: 0.7357705235481262:  20%|████                | 4/20 [00:00<00:01, 11.45it/s]evaluate for the 6-th batch, evaluate loss: 0.7357705235481262:  30%|██████              | 6/20 [00:00<00:01, 11.42it/s]Epoch: 5, train for the 107-th batch, train loss: 0.4353167712688446:  90%|█████████▉ | 107/119 [00:16<00:02,  5.94it/s]Epoch: 2, train for the 165-th batch, train loss: 0.6026986837387085:  69%|███████▌   | 164/237 [00:30<00:21,  3.43it/s]evaluate for the 7-th batch, evaluate loss: 0.7406291961669922:  30%|██████              | 6/20 [00:00<00:01, 11.42it/s]Epoch: 1, train for the 358-th batch, train loss: 0.5683928728103638:  93%|██████████▎| 357/383 [01:53<00:11,  2.22it/s]Epoch: 2, train for the 165-th batch, train loss: 0.6026986837387085:  70%|███████▋   | 165/237 [00:30<00:19,  3.70it/s]Epoch: 4, train for the 17-th batch, train loss: 0.6848838925361633:  11%|█▍           | 16/151 [00:02<00:22,  5.88it/s]Epoch: 3, train for the 18-th batch, train loss: 0.4730677604675293:   7%|▉            | 17/241 [00:02<00:31,  7.11it/s]Epoch: 4, train for the 17-th batch, train loss: 0.6848838925361633:  11%|█▍           | 17/151 [00:02<00:22,  5.84it/s]Epoch: 1, train for the 358-th batch, train loss: 0.5683928728103638:  93%|██████████▎| 358/383 [01:53<00:11,  2.14it/s]Epoch: 3, train for the 18-th batch, train loss: 0.4730677604675293:   7%|▉            | 18/241 [00:02<00:33,  6.75it/s]Epoch: 5, train for the 108-th batch, train loss: 0.34002795815467834:  90%|████████▉ | 107/119 [00:16<00:02,  5.94it/s]evaluate for the 8-th batch, evaluate loss: 0.6672587394714355:  30%|██████              | 6/20 [00:00<00:01, 11.42it/s]evaluate for the 8-th batch, evaluate loss: 0.6672587394714355:  40%|████████            | 8/20 [00:00<00:01, 11.53it/s]Epoch: 5, train for the 108-th batch, train loss: 0.34002795815467834:  91%|█████████ | 108/119 [00:16<00:01,  5.87it/s]Epoch: 2, train for the 166-th batch, train loss: 0.5360543131828308:  70%|███████▋   | 165/237 [00:30<00:19,  3.70it/s]evaluate for the 9-th batch, evaluate loss: 0.6366037726402283:  40%|████████            | 8/20 [00:00<00:01, 11.53it/s]Epoch: 2, train for the 166-th batch, train loss: 0.5360543131828308:  70%|███████▋   | 166/237 [00:30<00:17,  4.05it/s]Epoch: 4, train for the 18-th batch, train loss: 0.6786718368530273:  11%|█▍           | 17/151 [00:03<00:22,  5.84it/s]Epoch: 3, train for the 19-th batch, train loss: 0.4981091320514679:   7%|▉            | 18/241 [00:02<00:33,  6.75it/s]Epoch: 3, train for the 19-th batch, train loss: 0.4981091320514679:   8%|█            | 19/241 [00:02<00:34,  6.44it/s]Epoch: 4, train for the 18-th batch, train loss: 0.6786718368530273:  12%|█▌           | 18/151 [00:03<00:23,  5.73it/s]Epoch: 5, train for the 109-th batch, train loss: 0.4451600909233093:  91%|█████████▉ | 108/119 [00:16<00:01,  5.87it/s]evaluate for the 10-th batch, evaluate loss: 0.6305059790611267:  40%|███████▌           | 8/20 [00:00<00:01, 11.53it/s]evaluate for the 10-th batch, evaluate loss: 0.6305059790611267:  50%|█████████         | 10/20 [00:00<00:00, 11.21it/s]Epoch: 5, train for the 109-th batch, train loss: 0.4451600909233093:  92%|██████████ | 109/119 [00:16<00:01,  5.68it/s]Epoch: 1, train for the 359-th batch, train loss: 0.5307519435882568:  93%|██████████▎| 358/383 [01:53<00:11,  2.14it/s]evaluate for the 11-th batch, evaluate loss: 0.6294685006141663:  50%|█████████         | 10/20 [00:00<00:00, 11.21it/s]Epoch: 3, train for the 20-th batch, train loss: 0.44532835483551025:   8%|▉           | 19/241 [00:02<00:34,  6.44it/s]Epoch: 3, train for the 20-th batch, train loss: 0.44532835483551025:   8%|▉           | 20/241 [00:02<00:31,  7.08it/s]Epoch: 1, train for the 359-th batch, train loss: 0.5307519435882568:  94%|██████████▎| 359/383 [01:53<00:10,  2.35it/s]Epoch: 4, train for the 19-th batch, train loss: 0.6635496616363525:  12%|█▌           | 18/151 [00:03<00:23,  5.73it/s]Epoch: 4, train for the 19-th batch, train loss: 0.6635496616363525:  13%|█▋           | 19/151 [00:03<00:22,  5.90it/s]Epoch: 2, train for the 167-th batch, train loss: 0.5711385011672974:  70%|███████▋   | 166/237 [00:31<00:17,  4.05it/s]Epoch: 5, train for the 110-th batch, train loss: 0.3996050953865051:  92%|██████████ | 109/119 [00:17<00:01,  5.68it/s]Epoch: 5, train for the 110-th batch, train loss: 0.3996050953865051:  92%|██████████▏| 110/119 [00:17<00:01,  5.98it/s]evaluate for the 12-th batch, evaluate loss: 0.6900915503501892:  50%|█████████         | 10/20 [00:01<00:00, 11.21it/s]evaluate for the 12-th batch, evaluate loss: 0.6900915503501892:  60%|██████████▊       | 12/20 [00:01<00:00, 11.68it/s]Epoch: 2, train for the 167-th batch, train loss: 0.5711385011672974:  70%|███████▊   | 167/237 [00:31<00:17,  4.00it/s]Epoch: 3, train for the 21-th batch, train loss: 0.6316677927970886:   8%|█            | 20/241 [00:02<00:31,  7.08it/s]Epoch: 3, train for the 21-th batch, train loss: 0.6316677927970886:   9%|█▏           | 21/241 [00:02<00:30,  7.26it/s]evaluate for the 13-th batch, evaluate loss: 0.7166728973388672:  60%|██████████▊       | 12/20 [00:01<00:00, 11.68it/s]Epoch: 4, train for the 20-th batch, train loss: 0.6521168947219849:  13%|█▋           | 19/151 [00:03<00:22,  5.90it/s]Epoch: 5, train for the 111-th batch, train loss: 0.4971139430999756:  92%|██████████▏| 110/119 [00:17<00:01,  5.98it/s]Epoch: 4, train for the 20-th batch, train loss: 0.6521168947219849:  13%|█▋           | 20/151 [00:03<00:21,  6.07it/s]Epoch: 5, train for the 111-th batch, train loss: 0.4971139430999756:  93%|██████████▎| 111/119 [00:17<00:01,  6.29it/s]Epoch: 3, train for the 22-th batch, train loss: 0.551942765712738:   9%|█▏            | 21/241 [00:03<00:30,  7.26it/s]evaluate for the 14-th batch, evaluate loss: 0.7139014005661011:  60%|██████████▊       | 12/20 [00:01<00:00, 11.68it/s]evaluate for the 14-th batch, evaluate loss: 0.7139014005661011:  70%|████████████▌     | 14/20 [00:01<00:00, 11.48it/s]Epoch: 3, train for the 22-th batch, train loss: 0.551942765712738:   9%|█▎            | 22/241 [00:03<00:30,  7.28it/s]Epoch: 4, train for the 21-th batch, train loss: 0.657612681388855:  13%|█▊            | 20/151 [00:03<00:21,  6.07it/s]Epoch: 1, train for the 360-th batch, train loss: 0.40730080008506775:  94%|█████████▎| 359/383 [01:53<00:10,  2.35it/s]Epoch: 4, train for the 21-th batch, train loss: 0.657612681388855:  14%|█▉            | 21/151 [00:03<00:21,  6.19it/s]evaluate for the 15-th batch, evaluate loss: 0.7088176012039185:  70%|████████████▌     | 14/20 [00:01<00:00, 11.48it/s]Epoch: 5, train for the 112-th batch, train loss: 0.39148038625717163:  93%|█████████▎| 111/119 [00:17<00:01,  6.29it/s]Epoch: 5, train for the 112-th batch, train loss: 0.39148038625717163:  94%|█████████▍| 112/119 [00:17<00:01,  6.16it/s]Epoch: 3, train for the 23-th batch, train loss: 0.4910721480846405:   9%|█▏           | 22/241 [00:03<00:30,  7.28it/s]Epoch: 2, train for the 168-th batch, train loss: 0.5890762209892273:  70%|███████▊   | 167/237 [00:31<00:17,  4.00it/s]Epoch: 3, train for the 23-th batch, train loss: 0.4910721480846405:  10%|█▏           | 23/241 [00:03<00:29,  7.35it/s]evaluate for the 16-th batch, evaluate loss: 0.6496568322181702:  70%|████████████▌     | 14/20 [00:01<00:00, 11.48it/s]evaluate for the 16-th batch, evaluate loss: 0.6496568322181702:  80%|██████████████▍   | 16/20 [00:01<00:00, 11.56it/s]Epoch: 1, train for the 360-th batch, train loss: 0.40730080008506775:  94%|█████████▍| 360/383 [01:53<00:09,  2.39it/s]Epoch: 2, train for the 168-th batch, train loss: 0.5890762209892273:  71%|███████▊   | 168/237 [00:31<00:19,  3.54it/s]evaluate for the 17-th batch, evaluate loss: 0.701181948184967:  80%|███████████████▏   | 16/20 [00:01<00:00, 11.56it/s]Epoch: 4, train for the 22-th batch, train loss: 0.6458941698074341:  14%|█▊           | 21/151 [00:03<00:21,  6.19it/s]Epoch: 5, train for the 113-th batch, train loss: 0.39353182911872864:  94%|█████████▍| 112/119 [00:17<00:01,  6.16it/s]Epoch: 4, train for the 22-th batch, train loss: 0.6458941698074341:  15%|█▉           | 22/151 [00:03<00:21,  6.11it/s]Epoch: 5, train for the 113-th batch, train loss: 0.39353182911872864:  95%|█████████▍| 113/119 [00:17<00:00,  6.37it/s]Epoch: 3, train for the 24-th batch, train loss: 0.5530521273612976:  10%|█▏           | 23/241 [00:03<00:29,  7.35it/s]Epoch: 3, train for the 24-th batch, train loss: 0.5530521273612976:  10%|█▎           | 24/241 [00:03<00:30,  7.19it/s]evaluate for the 18-th batch, evaluate loss: 0.665851891040802:  80%|███████████████▏   | 16/20 [00:01<00:00, 11.56it/s]evaluate for the 18-th batch, evaluate loss: 0.665851891040802:  90%|█████████████████  | 18/20 [00:01<00:00, 12.74it/s]evaluate for the 19-th batch, evaluate loss: 0.7152134776115417:  90%|████████████████▏ | 18/20 [00:01<00:00, 12.74it/s]Epoch: 2, train for the 169-th batch, train loss: 0.5808425545692444:  71%|███████▊   | 168/237 [00:31<00:19,  3.54it/s]Epoch: 2, train for the 169-th batch, train loss: 0.5808425545692444:  71%|███████▊   | 169/237 [00:31<00:18,  3.78it/s]Epoch: 5, train for the 114-th batch, train loss: 0.4366055130958557:  95%|██████████▍| 113/119 [00:17<00:00,  6.37it/s]evaluate for the 20-th batch, evaluate loss: 0.7058825492858887:  90%|████████████████▏ | 18/20 [00:01<00:00, 12.74it/s]evaluate for the 20-th batch, evaluate loss: 0.7058825492858887: 100%|██████████████████| 20/20 [00:01<00:00, 12.90it/s]evaluate for the 20-th batch, evaluate loss: 0.7058825492858887: 100%|██████████████████| 20/20 [00:01<00:00, 12.06it/s]
Epoch: 5, train for the 114-th batch, train loss: 0.4366055130958557:  96%|██████████▌| 114/119 [00:17<00:00,  6.13it/s]Epoch: 1, train for the 361-th batch, train loss: 0.43934980034828186:  94%|█████████▍| 360/383 [01:54<00:09,  2.39it/s]Epoch: 3, train for the 25-th batch, train loss: 0.5940656065940857:  10%|█▎           | 24/241 [00:03<00:30,  7.19it/s]Epoch: 4, train for the 23-th batch, train loss: 0.6435900926589966:  15%|█▉           | 22/151 [00:03<00:21,  6.11it/s]Epoch: 3, train for the 25-th batch, train loss: 0.5940656065940857:  10%|█▎           | 25/241 [00:03<00:34,  6.29it/s]Epoch: 4, train for the 23-th batch, train loss: 0.6435900926589966:  15%|█▉           | 23/151 [00:03<00:23,  5.48it/s]Epoch: 1, train for the 361-th batch, train loss: 0.43934980034828186:  94%|█████████▍| 361/383 [01:54<00:08,  2.49it/s]Epoch: 5, train for the 115-th batch, train loss: 0.39226552844047546:  96%|█████████▌| 114/119 [00:17<00:00,  6.13it/s]Epoch: 2, train for the 170-th batch, train loss: 0.5810306668281555:  71%|███████▊   | 169/237 [00:31<00:18,  3.78it/s]Epoch: 5, train for the 115-th batch, train loss: 0.39226552844047546:  97%|█████████▋| 115/119 [00:17<00:00,  6.66it/s]Epoch: 2, train for the 170-th batch, train loss: 0.5810306668281555:  72%|███████▉   | 170/237 [00:31<00:15,  4.22it/s]Epoch: 3, train for the 26-th batch, train loss: 0.616450846195221:  10%|█▍            | 25/241 [00:03<00:34,  6.29it/s]Epoch: 3, train for the 26-th batch, train loss: 0.616450846195221:  11%|█▌            | 26/241 [00:03<00:33,  6.47it/s]Epoch: 4, train for the 24-th batch, train loss: 0.6330037713050842:  15%|█▉           | 23/151 [00:04<00:23,  5.48it/s]Epoch: 4, train for the 24-th batch, train loss: 0.6330037713050842:  16%|██           | 24/151 [00:04<00:23,  5.51it/s]INFO:root:Epoch: 4, learning rate: 0.0001, train loss: 0.5321
INFO:root:train average_precision, 0.8365
INFO:root:train roc_auc, 0.8224
INFO:root:validate loss: 0.4631
INFO:root:validate average_precision, 0.8685
INFO:root:validate roc_auc, 0.8600
INFO:root:new node validate loss: 0.6879
INFO:root:new node validate first_1_average_precision, 0.5713
INFO:root:new node validate first_1_roc_auc, 0.5074
INFO:root:new node validate first_3_average_precision, 0.6134
INFO:root:new node validate first_3_roc_auc, 0.5655
INFO:root:new node validate first_10_average_precision, 0.6405
INFO:root:new node validate first_10_roc_auc, 0.6235
INFO:root:new node validate average_precision, 0.6874
INFO:root:new node validate roc_auc, 0.6788
INFO:root:save model ./saved_models/TGN/ia-escorts-dynamic/TGN_seed0_tgn-ia-escorts-dynamic-lincorrect-time-linear/TGN_seed0_tgn-ia-escorts-dynamic-lincorrect-time-linear.pkl
Epoch: 5, train for the 116-th batch, train loss: 0.38478705286979675:  97%|█████████▋| 115/119 [00:17<00:00,  6.66it/s]Epoch: 5, train for the 116-th batch, train loss: 0.38478705286979675:  97%|█████████▋| 116/119 [00:17<00:00,  6.65it/s]Epoch: 3, train for the 27-th batch, train loss: 0.6112997531890869:  11%|█▍           | 26/241 [00:03<00:33,  6.47it/s]Epoch: 3, train for the 27-th batch, train loss: 0.6112997531890869:  11%|█▍           | 27/241 [00:03<00:32,  6.64it/s]Epoch: 5, train for the 117-th batch, train loss: 0.3903762698173523:  97%|██████████▋| 116/119 [00:18<00:00,  6.65it/s]Epoch: 4, train for the 25-th batch, train loss: 0.7011216282844543:  16%|██           | 24/151 [00:04<00:23,  5.51it/s]Epoch: 1, train for the 362-th batch, train loss: 0.3866104483604431:  94%|██████████▎| 361/383 [01:54<00:08,  2.49it/s]Epoch: 5, train for the 117-th batch, train loss: 0.3903762698173523:  98%|██████████▊| 117/119 [00:18<00:00,  7.18it/s]Epoch: 4, train for the 25-th batch, train loss: 0.7011216282844543:  17%|██▏          | 25/151 [00:04<00:22,  5.70it/s]Epoch: 2, train for the 171-th batch, train loss: 0.5776216983795166:  72%|███████▉   | 170/237 [00:32<00:15,  4.22it/s]Epoch: 3, train for the 28-th batch, train loss: 0.4769640266895294:  11%|█▍           | 27/241 [00:03<00:32,  6.64it/s]Epoch: 2, train for the 171-th batch, train loss: 0.5776216983795166:  72%|███████▉   | 171/237 [00:32<00:16,  3.91it/s]Epoch: 3, train for the 28-th batch, train loss: 0.4769640266895294:  12%|█▌           | 28/241 [00:03<00:30,  6.97it/s]Epoch: 1, train for the 362-th batch, train loss: 0.3866104483604431:  95%|██████████▍| 362/383 [01:54<00:08,  2.57it/s]Epoch: 5, train for the 118-th batch, train loss: 0.3757106363773346:  98%|██████████▊| 117/119 [00:18<00:00,  7.18it/s]Epoch: 5, train for the 118-th batch, train loss: 0.3757106363773346:  99%|██████████▉| 118/119 [00:18<00:00,  7.83it/s]Epoch: 4, train for the 26-th batch, train loss: 0.6517570614814758:  17%|██▏          | 25/151 [00:04<00:22,  5.70it/s]Epoch: 4, train for the 26-th batch, train loss: 0.6517570614814758:  17%|██▏          | 26/151 [00:04<00:20,  6.04it/s]Epoch: 5, train for the 119-th batch, train loss: 0.2983071506023407:  99%|██████████▉| 118/119 [00:18<00:00,  7.83it/s]Epoch: 5, train for the 119-th batch, train loss: 0.2983071506023407: 100%|███████████| 119/119 [00:18<00:00,  8.25it/s]Epoch: 5, train for the 119-th batch, train loss: 0.2983071506023407: 100%|███████████| 119/119 [00:18<00:00,  6.51it/s]
Epoch: 3, train for the 29-th batch, train loss: 0.5930011868476868:  12%|█▌           | 28/241 [00:04<00:30,  6.97it/s]Epoch: 3, train for the 29-th batch, train loss: 0.5930011868476868:  12%|█▌           | 29/241 [00:04<00:31,  6.71it/s]Epoch: 2, train for the 172-th batch, train loss: 0.5535475015640259:  72%|███████▉   | 171/237 [00:32<00:16,  3.91it/s]Epoch: 4, train for the 27-th batch, train loss: 0.6647271513938904:  17%|██▏          | 26/151 [00:04<00:20,  6.04it/s]Epoch: 4, train for the 27-th batch, train loss: 0.6647271513938904:  18%|██▎          | 27/151 [00:04<00:19,  6.45it/s]Epoch: 2, train for the 172-th batch, train loss: 0.5535475015640259:  73%|███████▉   | 172/237 [00:32<00:16,  3.93it/s]Epoch: 4, train for the 28-th batch, train loss: 0.6634382605552673:  18%|██▎          | 27/151 [00:04<00:19,  6.45it/s]  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 1, train for the 363-th batch, train loss: 0.43669945001602173:  95%|█████████▍| 362/383 [01:55<00:08,  2.57it/s]evaluate for the 1-th batch, evaluate loss: 0.4698384702205658:   0%|                            | 0/40 [00:00<?, ?it/s]Epoch: 3, train for the 30-th batch, train loss: 0.6504064202308655:  12%|█▌           | 29/241 [00:04<00:31,  6.71it/s]Epoch: 3, train for the 30-th batch, train loss: 0.6504064202308655:  12%|█▌           | 30/241 [00:04<00:37,  5.58it/s]Epoch: 2, train for the 173-th batch, train loss: 0.5420631766319275:  73%|███████▉   | 172/237 [00:32<00:16,  3.93it/s]evaluate for the 2-th batch, evaluate loss: 0.48807811737060547:   0%|                           | 0/40 [00:00<?, ?it/s]Epoch: 4, train for the 29-th batch, train loss: 0.6973838806152344:  18%|██▎          | 27/151 [00:04<00:19,  6.45it/s]Epoch: 4, train for the 29-th batch, train loss: 0.6973838806152344:  19%|██▍          | 29/151 [00:04<00:16,  7.20it/s]Epoch: 1, train for the 363-th batch, train loss: 0.43669945001602173:  95%|█████████▍| 363/383 [01:55<00:08,  2.46it/s]Epoch: 2, train for the 173-th batch, train loss: 0.5420631766319275:  73%|████████   | 173/237 [00:32<00:15,  4.10it/s]evaluate for the 3-th batch, evaluate loss: 0.4846583306789398:   0%|                            | 0/40 [00:00<?, ?it/s]evaluate for the 3-th batch, evaluate loss: 0.4846583306789398:   8%|█▌                  | 3/40 [00:00<00:01, 24.34it/s]evaluate for the 4-th batch, evaluate loss: 0.560325026512146:   8%|█▌                   | 3/40 [00:00<00:01, 24.34it/s]Epoch: 3, train for the 31-th batch, train loss: 0.46472418308258057:  12%|█▍          | 30/241 [00:04<00:37,  5.58it/s]Epoch: 3, train for the 31-th batch, train loss: 0.46472418308258057:  13%|█▌          | 31/241 [00:04<00:35,  5.85it/s]evaluate for the 5-th batch, evaluate loss: 0.5457780361175537:   8%|█▌                  | 3/40 [00:00<00:01, 24.34it/s]Epoch: 4, train for the 30-th batch, train loss: 0.681617259979248:  19%|██▋           | 29/151 [00:04<00:16,  7.20it/s]Epoch: 4, train for the 30-th batch, train loss: 0.681617259979248:  20%|██▊           | 30/151 [00:04<00:17,  6.90it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]evaluate for the 6-th batch, evaluate loss: 0.4960389733314514:   8%|█▌                  | 3/40 [00:00<00:01, 24.34it/s]evaluate for the 6-th batch, evaluate loss: 0.4960389733314514:  15%|███                 | 6/40 [00:00<00:01, 19.37it/s]Epoch: 2, train for the 174-th batch, train loss: 0.5461047291755676:  73%|████████   | 173/237 [00:32<00:15,  4.10it/s]Epoch: 2, train for the 174-th batch, train loss: 0.5461047291755676:  73%|████████   | 174/237 [00:32<00:15,  4.12it/s]Epoch: 3, train for the 32-th batch, train loss: 0.43094298243522644:  13%|█▌          | 31/241 [00:04<00:35,  5.85it/s]Epoch: 3, train for the 32-th batch, train loss: 0.43094298243522644:  13%|█▌          | 32/241 [00:04<00:36,  5.79it/s]Epoch: 5, train for the 1-th batch, train loss: 0.7554413676261902:   0%|                       | 0/146 [00:00<?, ?it/s]Epoch: 5, train for the 1-th batch, train loss: 0.7554413676261902:   1%|               | 1/146 [00:00<00:16,  8.69it/s]evaluate for the 7-th batch, evaluate loss: 0.51253741979599:  15%|███▎                  | 6/40 [00:00<00:01, 19.37it/s]Epoch: 4, train for the 31-th batch, train loss: 0.6673979759216309:  20%|██▌          | 30/151 [00:05<00:17,  6.90it/s]Epoch: 4, train for the 31-th batch, train loss: 0.6673979759216309:  21%|██▋          | 31/151 [00:05<00:18,  6.59it/s]Epoch: 1, train for the 364-th batch, train loss: 0.388020396232605:  95%|███████████▎| 363/383 [01:55<00:08,  2.46it/s]evaluate for the 8-th batch, evaluate loss: 0.46146950125694275:  15%|██▊                | 6/40 [00:00<00:01, 19.37it/s]evaluate for the 8-th batch, evaluate loss: 0.46146950125694275:  20%|███▊               | 8/40 [00:00<00:02, 15.34it/s]Epoch: 2, train for the 175-th batch, train loss: 0.5796083807945251:  73%|████████   | 174/237 [00:33<00:15,  4.12it/s]Epoch: 5, train for the 2-th batch, train loss: 0.7245150804519653:   1%|               | 1/146 [00:00<00:16,  8.69it/s]Epoch: 5, train for the 2-th batch, train loss: 0.7245150804519653:   1%|▏              | 2/146 [00:00<00:18,  7.98it/s]Epoch: 3, train for the 33-th batch, train loss: 0.6353589296340942:  13%|█▋           | 32/241 [00:04<00:36,  5.79it/s]evaluate for the 9-th batch, evaluate loss: 0.506827712059021:  20%|████▏                | 8/40 [00:00<00:02, 15.34it/s]Epoch: 3, train for the 33-th batch, train loss: 0.6353589296340942:  14%|█▊           | 33/241 [00:04<00:34,  6.00it/s]Epoch: 1, train for the 364-th batch, train loss: 0.388020396232605:  95%|███████████▍| 364/383 [01:55<00:07,  2.40it/s]Epoch: 2, train for the 175-th batch, train loss: 0.5796083807945251:  74%|████████   | 175/237 [00:33<00:14,  4.30it/s]Epoch: 4, train for the 32-th batch, train loss: 0.6596615314483643:  21%|██▋          | 31/151 [00:05<00:18,  6.59it/s]Epoch: 4, train for the 32-th batch, train loss: 0.6596615314483643:  21%|██▊          | 32/151 [00:05<00:18,  6.33it/s]evaluate for the 10-th batch, evaluate loss: 0.5240451693534851:  20%|███▊               | 8/40 [00:00<00:02, 15.34it/s]evaluate for the 10-th batch, evaluate loss: 0.5240451693534851:  25%|████▌             | 10/40 [00:00<00:02, 14.90it/s]Epoch: 5, train for the 3-th batch, train loss: 0.7052354216575623:   1%|▏              | 2/146 [00:00<00:18,  7.98it/s]Epoch: 5, train for the 3-th batch, train loss: 0.7052354216575623:   2%|▎              | 3/146 [00:00<00:17,  8.36it/s]Epoch: 3, train for the 34-th batch, train loss: 0.53171706199646:  14%|██             | 33/241 [00:04<00:34,  6.00it/s]Epoch: 3, train for the 34-th batch, train loss: 0.53171706199646:  14%|██             | 34/241 [00:04<00:33,  6.16it/s]evaluate for the 11-th batch, evaluate loss: 0.4828093945980072:  25%|████▌             | 10/40 [00:00<00:02, 14.90it/s]Epoch: 4, train for the 33-th batch, train loss: 0.6478338241577148:  21%|██▊          | 32/151 [00:05<00:18,  6.33it/s]Epoch: 2, train for the 176-th batch, train loss: 0.5375301241874695:  74%|████████   | 175/237 [00:33<00:14,  4.30it/s]Epoch: 4, train for the 33-th batch, train loss: 0.6478338241577148:  22%|██▊          | 33/151 [00:05<00:18,  6.27it/s]Epoch: 5, train for the 4-th batch, train loss: 0.6919052004814148:   2%|▎              | 3/146 [00:00<00:17,  8.36it/s]evaluate for the 12-th batch, evaluate loss: 0.5180984735488892:  25%|████▌             | 10/40 [00:00<00:02, 14.90it/s]evaluate for the 12-th batch, evaluate loss: 0.5180984735488892:  30%|█████▍            | 12/40 [00:00<00:02, 13.37it/s]Epoch: 5, train for the 4-th batch, train loss: 0.6919052004814148:   3%|▍              | 4/146 [00:00<00:19,  7.45it/s]Epoch: 2, train for the 176-th batch, train loss: 0.5375301241874695:  74%|████████▏  | 176/237 [00:33<00:14,  4.27it/s]Epoch: 3, train for the 35-th batch, train loss: 0.5868479609489441:  14%|█▊           | 34/241 [00:05<00:33,  6.16it/s]Epoch: 3, train for the 35-th batch, train loss: 0.5868479609489441:  15%|█▉           | 35/241 [00:05<00:31,  6.55it/s]Epoch: 1, train for the 365-th batch, train loss: 0.36999645829200745:  95%|█████████▌| 364/383 [01:55<00:07,  2.40it/s]evaluate for the 13-th batch, evaluate loss: 0.47118449211120605:  30%|█████            | 12/40 [00:00<00:02, 13.37it/s]Epoch: 4, train for the 34-th batch, train loss: 0.6353464722633362:  22%|██▊          | 33/151 [00:05<00:18,  6.27it/s]Epoch: 4, train for the 34-th batch, train loss: 0.6353464722633362:  23%|██▉          | 34/151 [00:05<00:17,  6.54it/s]Epoch: 5, train for the 5-th batch, train loss: 0.6745133996009827:   3%|▍              | 4/146 [00:00<00:19,  7.45it/s]Epoch: 5, train for the 5-th batch, train loss: 0.6745133996009827:   3%|▌              | 5/146 [00:00<00:18,  7.80it/s]Epoch: 1, train for the 365-th batch, train loss: 0.36999645829200745:  95%|█████████▌| 365/383 [01:55<00:07,  2.47it/s]evaluate for the 14-th batch, evaluate loss: 0.4941926598548889:  30%|█████▍            | 12/40 [00:00<00:02, 13.37it/s]evaluate for the 14-th batch, evaluate loss: 0.4941926598548889:  35%|██████▎           | 14/40 [00:00<00:01, 13.64it/s]Epoch: 3, train for the 36-th batch, train loss: 0.5775893330574036:  15%|█▉           | 35/241 [00:05<00:31,  6.55it/s]Epoch: 2, train for the 177-th batch, train loss: 0.6080086827278137:  74%|████████▏  | 176/237 [00:33<00:14,  4.27it/s]Epoch: 3, train for the 36-th batch, train loss: 0.5775893330574036:  15%|█▉           | 36/241 [00:05<00:30,  6.62it/s]Epoch: 2, train for the 177-th batch, train loss: 0.6080086827278137:  75%|████████▏  | 177/237 [00:33<00:13,  4.43it/s]evaluate for the 15-th batch, evaluate loss: 0.49459853768348694:  35%|█████▉           | 14/40 [00:01<00:01, 13.64it/s]Epoch: 5, train for the 6-th batch, train loss: 0.6587674021720886:   3%|▌              | 5/146 [00:00<00:18,  7.80it/s]Epoch: 5, train for the 6-th batch, train loss: 0.6587674021720886:   4%|▌              | 6/146 [00:00<00:17,  7.93it/s]Epoch: 4, train for the 35-th batch, train loss: 0.6623408198356628:  23%|██▉          | 34/151 [00:05<00:17,  6.54it/s]Epoch: 4, train for the 35-th batch, train loss: 0.6623408198356628:  23%|███          | 35/151 [00:05<00:17,  6.63it/s]Epoch: 3, train for the 37-th batch, train loss: 0.49940261244773865:  15%|█▊          | 36/241 [00:05<00:30,  6.62it/s]evaluate for the 16-th batch, evaluate loss: 0.48927029967308044:  35%|█████▉           | 14/40 [00:01<00:01, 13.64it/s]evaluate for the 16-th batch, evaluate loss: 0.48927029967308044:  40%|██████▊          | 16/40 [00:01<00:01, 13.31it/s]Epoch: 3, train for the 37-th batch, train loss: 0.49940261244773865:  15%|█▊          | 37/241 [00:05<00:29,  6.86it/s]evaluate for the 17-th batch, evaluate loss: 0.5236445069313049:  40%|███████▏          | 16/40 [00:01<00:01, 13.31it/s]evaluate for the 18-th batch, evaluate loss: 0.45724111795425415:  40%|██████▊          | 16/40 [00:01<00:01, 13.31it/s]evaluate for the 18-th batch, evaluate loss: 0.45724111795425415:  45%|███████▋         | 18/40 [00:01<00:01, 14.71it/s]Epoch: 4, train for the 36-th batch, train loss: 0.6672818064689636:  23%|███          | 35/151 [00:05<00:17,  6.63it/s]Epoch: 2, train for the 178-th batch, train loss: 0.5643381476402283:  75%|████████▏  | 177/237 [00:33<00:13,  4.43it/s]Epoch: 4, train for the 36-th batch, train loss: 0.6672818064689636:  24%|███          | 36/151 [00:05<00:18,  6.22it/s]Epoch: 3, train for the 38-th batch, train loss: 0.5458752512931824:  15%|█▉           | 37/241 [00:05<00:29,  6.86it/s]Epoch: 3, train for the 38-th batch, train loss: 0.5458752512931824:  16%|██           | 38/241 [00:05<00:30,  6.77it/s]Epoch: 2, train for the 178-th batch, train loss: 0.5643381476402283:  75%|████████▎  | 178/237 [00:33<00:13,  4.28it/s]Epoch: 1, train for the 366-th batch, train loss: 0.535435140132904:  95%|███████████▍| 365/383 [01:56<00:07,  2.47it/s]evaluate for the 19-th batch, evaluate loss: 0.5096928477287292:  45%|████████          | 18/40 [00:01<00:01, 14.71it/s]Epoch: 5, train for the 7-th batch, train loss: 0.6479209065437317:   4%|▌              | 6/146 [00:01<00:17,  7.93it/s]Epoch: 5, train for the 7-th batch, train loss: 0.6479209065437317:   5%|▋              | 7/146 [00:01<00:24,  5.77it/s]evaluate for the 20-th batch, evaluate loss: 0.4686225652694702:  45%|████████          | 18/40 [00:01<00:01, 14.71it/s]evaluate for the 20-th batch, evaluate loss: 0.4686225652694702:  50%|█████████         | 20/40 [00:01<00:01, 14.40it/s]Epoch: 1, train for the 366-th batch, train loss: 0.535435140132904:  96%|███████████▍| 366/383 [01:56<00:07,  2.40it/s]Epoch: 4, train for the 37-th batch, train loss: 0.6590388417243958:  24%|███          | 36/151 [00:06<00:18,  6.22it/s]Epoch: 4, train for the 37-th batch, train loss: 0.6590388417243958:  25%|███▏         | 37/151 [00:06<00:17,  6.38it/s]Epoch: 3, train for the 39-th batch, train loss: 0.6012651920318604:  16%|██           | 38/241 [00:05<00:30,  6.77it/s]Epoch: 3, train for the 39-th batch, train loss: 0.6012651920318604:  16%|██           | 39/241 [00:05<00:30,  6.73it/s]evaluate for the 21-th batch, evaluate loss: 0.5322768688201904:  50%|█████████         | 20/40 [00:01<00:01, 14.40it/s]Epoch: 2, train for the 179-th batch, train loss: 0.5575603246688843:  75%|████████▎  | 178/237 [00:34<00:13,  4.28it/s]Epoch: 5, train for the 8-th batch, train loss: 0.6269494891166687:   5%|▋              | 7/146 [00:01<00:24,  5.77it/s]Epoch: 5, train for the 8-th batch, train loss: 0.6269494891166687:   5%|▊              | 8/146 [00:01<00:22,  6.05it/s]Epoch: 2, train for the 179-th batch, train loss: 0.5575603246688843:  76%|████████▎  | 179/237 [00:34<00:13,  4.35it/s]evaluate for the 22-th batch, evaluate loss: 0.4888398349285126:  50%|█████████         | 20/40 [00:01<00:01, 14.40it/s]evaluate for the 22-th batch, evaluate loss: 0.4888398349285126:  55%|█████████▉        | 22/40 [00:01<00:01, 14.06it/s]Epoch: 4, train for the 38-th batch, train loss: 0.6705074906349182:  25%|███▏         | 37/151 [00:06<00:17,  6.38it/s]Epoch: 4, train for the 38-th batch, train loss: 0.6705074906349182:  25%|███▎         | 38/151 [00:06<00:17,  6.35it/s]Epoch: 3, train for the 40-th batch, train loss: 0.6219452619552612:  16%|██           | 39/241 [00:05<00:30,  6.73it/s]Epoch: 3, train for the 40-th batch, train loss: 0.6219452619552612:  17%|██▏          | 40/241 [00:05<00:30,  6.65it/s]evaluate for the 23-th batch, evaluate loss: 0.41784346103668213:  55%|█████████▎       | 22/40 [00:01<00:01, 14.06it/s]Epoch: 5, train for the 9-th batch, train loss: 0.6121034026145935:   5%|▊              | 8/146 [00:01<00:22,  6.05it/s]Epoch: 5, train for the 9-th batch, train loss: 0.6121034026145935:   6%|▉              | 9/146 [00:01<00:21,  6.32it/s]evaluate for the 24-th batch, evaluate loss: 0.49695250391960144:  55%|█████████▎       | 22/40 [00:01<00:01, 14.06it/s]evaluate for the 24-th batch, evaluate loss: 0.49695250391960144:  60%|██████████▏      | 24/40 [00:01<00:01, 14.12it/s]Epoch: 2, train for the 180-th batch, train loss: 0.614651620388031:  76%|█████████   | 179/237 [00:34<00:13,  4.35it/s]Epoch: 2, train for the 180-th batch, train loss: 0.614651620388031:  76%|█████████   | 180/237 [00:34<00:12,  4.59it/s]evaluate for the 25-th batch, evaluate loss: 0.5258764624595642:  60%|██████████▊       | 24/40 [00:01<00:01, 14.12it/s]Epoch: 4, train for the 39-th batch, train loss: 0.6576374173164368:  25%|███▎         | 38/151 [00:06<00:17,  6.35it/s]Epoch: 3, train for the 41-th batch, train loss: 0.5462518334388733:  17%|██▏          | 40/241 [00:05<00:30,  6.65it/s]Epoch: 4, train for the 39-th batch, train loss: 0.6576374173164368:  26%|███▎         | 39/151 [00:06<00:18,  6.17it/s]Epoch: 3, train for the 41-th batch, train loss: 0.5462518334388733:  17%|██▏          | 41/241 [00:06<00:30,  6.51it/s]Epoch: 5, train for the 10-th batch, train loss: 0.6091504693031311:   6%|▊             | 9/146 [00:01<00:21,  6.32it/s]Epoch: 5, train for the 10-th batch, train loss: 0.6091504693031311:   7%|▉            | 10/146 [00:01<00:21,  6.37it/s]evaluate for the 26-th batch, evaluate loss: 0.4357443153858185:  60%|██████████▊       | 24/40 [00:01<00:01, 14.12it/s]evaluate for the 26-th batch, evaluate loss: 0.4357443153858185:  65%|███████████▋      | 26/40 [00:01<00:00, 14.49it/s]Epoch: 1, train for the 367-th batch, train loss: 0.3333941102027893:  96%|██████████▌| 366/383 [01:56<00:07,  2.40it/s]evaluate for the 27-th batch, evaluate loss: 0.4699966311454773:  65%|███████████▋      | 26/40 [00:01<00:00, 14.49it/s]Epoch: 2, train for the 181-th batch, train loss: 0.5505171418190002:  76%|████████▎  | 180/237 [00:34<00:12,  4.59it/s]Epoch: 3, train for the 42-th batch, train loss: 0.6005918979644775:  17%|██▏          | 41/241 [00:06<00:30,  6.51it/s]Epoch: 1, train for the 367-th batch, train loss: 0.3333941102027893:  96%|██████████▌| 367/383 [01:56<00:07,  2.23it/s]Epoch: 2, train for the 181-th batch, train loss: 0.5505171418190002:  76%|████████▍  | 181/237 [00:34<00:12,  4.60it/s]Epoch: 3, train for the 42-th batch, train loss: 0.6005918979644775:  17%|██▎          | 42/241 [00:06<00:31,  6.36it/s]Epoch: 4, train for the 40-th batch, train loss: 0.6352843046188354:  26%|███▎         | 39/151 [00:06<00:18,  6.17it/s]Epoch: 5, train for the 11-th batch, train loss: 0.614080011844635:   7%|▉             | 10/146 [00:01<00:21,  6.37it/s]evaluate for the 28-th batch, evaluate loss: 0.4390111565589905:  65%|███████████▋      | 26/40 [00:01<00:00, 14.49it/s]evaluate for the 28-th batch, evaluate loss: 0.4390111565589905:  70%|████████████▌     | 28/40 [00:01<00:00, 14.38it/s]Epoch: 5, train for the 11-th batch, train loss: 0.614080011844635:   8%|█             | 11/146 [00:01<00:21,  6.32it/s]Epoch: 4, train for the 40-th batch, train loss: 0.6352843046188354:  26%|███▍         | 40/151 [00:06<00:19,  5.72it/s]evaluate for the 29-th batch, evaluate loss: 0.4726208448410034:  70%|████████████▌     | 28/40 [00:01<00:00, 14.38it/s]Epoch: 3, train for the 43-th batch, train loss: 0.6223634481430054:  17%|██▎          | 42/241 [00:06<00:31,  6.36it/s]Epoch: 3, train for the 43-th batch, train loss: 0.6223634481430054:  18%|██▎          | 43/241 [00:06<00:30,  6.52it/s]Epoch: 5, train for the 12-th batch, train loss: 0.5991713404655457:   8%|▉            | 11/146 [00:01<00:21,  6.32it/s]Epoch: 5, train for the 12-th batch, train loss: 0.5991713404655457:   8%|█            | 12/146 [00:01<00:20,  6.51it/s]evaluate for the 30-th batch, evaluate loss: 0.45738208293914795:  70%|███████████▉     | 28/40 [00:02<00:00, 14.38it/s]evaluate for the 30-th batch, evaluate loss: 0.45738208293914795:  75%|████████████▊    | 30/40 [00:02<00:00, 13.56it/s]Epoch: 4, train for the 41-th batch, train loss: 0.6385654807090759:  26%|███▍         | 40/151 [00:06<00:19,  5.72it/s]Epoch: 4, train for the 41-th batch, train loss: 0.6385654807090759:  27%|███▌         | 41/151 [00:06<00:19,  5.78it/s]Epoch: 2, train for the 182-th batch, train loss: 0.5796176791191101:  76%|████████▍  | 181/237 [00:34<00:12,  4.60it/s]Epoch: 2, train for the 182-th batch, train loss: 0.5796176791191101:  77%|████████▍  | 182/237 [00:34<00:12,  4.37it/s]evaluate for the 31-th batch, evaluate loss: 0.4677196443080902:  75%|█████████████▌    | 30/40 [00:02<00:00, 13.56it/s]Epoch: 3, train for the 44-th batch, train loss: 0.6469807028770447:  18%|██▎          | 43/241 [00:06<00:30,  6.52it/s]Epoch: 3, train for the 44-th batch, train loss: 0.6469807028770447:  18%|██▎          | 44/241 [00:06<00:28,  6.86it/s]Epoch: 1, train for the 368-th batch, train loss: 0.5543637871742249:  96%|██████████▌| 367/383 [01:57<00:07,  2.23it/s]Epoch: 5, train for the 13-th batch, train loss: 0.5908504128456116:   8%|█            | 12/146 [00:01<00:20,  6.51it/s]Epoch: 5, train for the 13-th batch, train loss: 0.5908504128456116:   9%|█▏           | 13/146 [00:01<00:19,  6.97it/s]evaluate for the 32-th batch, evaluate loss: 0.4934316873550415:  75%|█████████████▌    | 30/40 [00:02<00:00, 13.56it/s]evaluate for the 32-th batch, evaluate loss: 0.4934316873550415:  80%|██████████████▍   | 32/40 [00:02<00:00, 14.02it/s]Epoch: 4, train for the 42-th batch, train loss: 0.6226528882980347:  27%|███▌         | 41/151 [00:06<00:19,  5.78it/s]Epoch: 4, train for the 42-th batch, train loss: 0.6226528882980347:  28%|███▌         | 42/151 [00:06<00:17,  6.12it/s]Epoch: 1, train for the 368-th batch, train loss: 0.5543637871742249:  96%|██████████▌| 368/383 [01:57<00:06,  2.38it/s]Epoch: 3, train for the 45-th batch, train loss: 0.5027292370796204:  18%|██▎          | 44/241 [00:06<00:28,  6.86it/s]evaluate for the 33-th batch, evaluate loss: 0.44903290271759033:  80%|█████████████▌   | 32/40 [00:02<00:00, 14.02it/s]Epoch: 3, train for the 45-th batch, train loss: 0.5027292370796204:  19%|██▍          | 45/241 [00:06<00:27,  7.09it/s]Epoch: 5, train for the 14-th batch, train loss: 0.5815245509147644:   9%|█▏           | 13/146 [00:02<00:19,  6.97it/s]Epoch: 2, train for the 183-th batch, train loss: 0.5817948579788208:  77%|████████▍  | 182/237 [00:34<00:12,  4.37it/s]Epoch: 5, train for the 14-th batch, train loss: 0.5815245509147644:  10%|█▏           | 14/146 [00:02<00:18,  7.03it/s]evaluate for the 34-th batch, evaluate loss: 0.458891361951828:  80%|███████████████▏   | 32/40 [00:02<00:00, 14.02it/s]evaluate for the 34-th batch, evaluate loss: 0.458891361951828:  85%|████████████████▏  | 34/40 [00:02<00:00, 14.15it/s]Epoch: 2, train for the 183-th batch, train loss: 0.5817948579788208:  77%|████████▍  | 183/237 [00:34<00:12,  4.48it/s]Epoch: 4, train for the 43-th batch, train loss: 0.6464555859565735:  28%|███▌         | 42/151 [00:07<00:17,  6.12it/s]Epoch: 4, train for the 43-th batch, train loss: 0.6464555859565735:  28%|███▋         | 43/151 [00:07<00:17,  6.22it/s]Epoch: 3, train for the 46-th batch, train loss: 0.5533580780029297:  19%|██▍          | 45/241 [00:06<00:27,  7.09it/s]Epoch: 5, train for the 15-th batch, train loss: 0.5696653127670288:  10%|█▏           | 14/146 [00:02<00:18,  7.03it/s]evaluate for the 35-th batch, evaluate loss: 0.5010548830032349:  85%|███████████████▎  | 34/40 [00:02<00:00, 14.15it/s]Epoch: 5, train for the 15-th batch, train loss: 0.5696653127670288:  10%|█▎           | 15/146 [00:02<00:18,  6.97it/s]Epoch: 3, train for the 46-th batch, train loss: 0.5533580780029297:  19%|██▍          | 46/241 [00:06<00:29,  6.63it/s]evaluate for the 36-th batch, evaluate loss: 0.4867064654827118:  85%|███████████████▎  | 34/40 [00:02<00:00, 14.15it/s]evaluate for the 36-th batch, evaluate loss: 0.4867064654827118:  90%|████████████████▏ | 36/40 [00:02<00:00, 13.00it/s]Epoch: 4, train for the 44-th batch, train loss: 0.5947075486183167:  28%|███▋         | 43/151 [00:07<00:17,  6.22it/s]Epoch: 1, train for the 369-th batch, train loss: 0.3271740674972534:  96%|██████████▌| 368/383 [01:57<00:06,  2.38it/s]Epoch: 4, train for the 44-th batch, train loss: 0.5947075486183167:  29%|███▊         | 44/151 [00:07<00:17,  6.25it/s]evaluate for the 37-th batch, evaluate loss: 0.467486172914505:  90%|█████████████████  | 36/40 [00:02<00:00, 13.00it/s]Epoch: 2, train for the 184-th batch, train loss: 0.5556532144546509:  77%|████████▍  | 183/237 [00:35<00:12,  4.48it/s]Epoch: 3, train for the 47-th batch, train loss: 0.6106688380241394:  19%|██▍          | 46/241 [00:06<00:29,  6.63it/s]Epoch: 3, train for the 47-th batch, train loss: 0.6106688380241394:  20%|██▌          | 47/241 [00:06<00:28,  6.91it/s]Epoch: 2, train for the 184-th batch, train loss: 0.5556532144546509:  78%|████████▌  | 184/237 [00:35<00:12,  4.22it/s]evaluate for the 38-th batch, evaluate loss: 0.5122576951980591:  90%|████████████████▏ | 36/40 [00:02<00:00, 13.00it/s]Epoch: 1, train for the 369-th batch, train loss: 0.3271740674972534:  96%|██████████▌| 369/383 [01:57<00:05,  2.42it/s]evaluate for the 39-th batch, evaluate loss: 0.5231814384460449:  90%|████████████████▏ | 36/40 [00:02<00:00, 13.00it/s]evaluate for the 39-th batch, evaluate loss: 0.5231814384460449:  98%|█████████████████▌| 39/40 [00:02<00:00, 15.32it/s]Epoch: 4, train for the 45-th batch, train loss: 0.5982336401939392:  29%|███▊         | 44/151 [00:07<00:17,  6.25it/s]Epoch: 4, train for the 45-th batch, train loss: 0.5982336401939392:  30%|███▊         | 45/151 [00:07<00:16,  6.50it/s]evaluate for the 40-th batch, evaluate loss: 0.39536529779434204:  98%|████████████████▌| 39/40 [00:02<00:00, 15.32it/s]evaluate for the 40-th batch, evaluate loss: 0.39536529779434204: 100%|█████████████████| 40/40 [00:02<00:00, 14.69it/s]
Epoch: 3, train for the 48-th batch, train loss: 0.5678899884223938:  20%|██▌          | 47/241 [00:07<00:28,  6.91it/s]Epoch: 3, train for the 48-th batch, train loss: 0.5678899884223938:  20%|██▌          | 48/241 [00:07<00:28,  6.82it/s]Epoch: 2, train for the 185-th batch, train loss: 0.5398189425468445:  78%|████████▌  | 184/237 [00:35<00:12,  4.22it/s]Epoch: 4, train for the 46-th batch, train loss: 0.618183970451355:  30%|████▏         | 45/151 [00:07<00:16,  6.50it/s]Epoch: 5, train for the 16-th batch, train loss: 0.5836414694786072:  10%|█▎           | 15/146 [00:02<00:18,  6.97it/s]Epoch: 2, train for the 185-th batch, train loss: 0.5398189425468445:  78%|████████▌  | 185/237 [00:35<00:11,  4.34it/s]  0%|                                                                                            | 0/21 [00:00<?, ?it/s]Epoch: 5, train for the 16-th batch, train loss: 0.5836414694786072:  11%|█▍           | 16/146 [00:02<00:27,  4.67it/s]Epoch: 4, train for the 46-th batch, train loss: 0.618183970451355:  30%|████▎         | 46/151 [00:07<00:16,  6.42it/s]Epoch: 3, train for the 49-th batch, train loss: 0.5969994068145752:  20%|██▌          | 48/241 [00:07<00:28,  6.82it/s]Epoch: 3, train for the 49-th batch, train loss: 0.5969994068145752:  20%|██▋          | 49/241 [00:07<00:27,  6.96it/s]evaluate for the 1-th batch, evaluate loss: 0.679046630859375:   0%|                             | 0/21 [00:00<?, ?it/s]Epoch: 1, train for the 370-th batch, train loss: 0.3792199492454529:  96%|██████████▌| 369/383 [01:57<00:05,  2.42it/s]Epoch: 1, train for the 370-th batch, train loss: 0.3792199492454529:  97%|██████████▋| 370/383 [01:58<00:05,  2.54it/s]Epoch: 5, train for the 17-th batch, train loss: 0.5804678201675415:  11%|█▍           | 16/146 [00:02<00:27,  4.67it/s]Epoch: 5, train for the 17-th batch, train loss: 0.5804678201675415:  12%|█▌           | 17/146 [00:02<00:25,  5.13it/s]evaluate for the 2-th batch, evaluate loss: 0.7400370836257935:   0%|                            | 0/21 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.7400370836257935:  10%|█▉                  | 2/21 [00:00<00:01, 12.77it/s]Epoch: 3, train for the 50-th batch, train loss: 0.5620054602622986:  20%|██▋          | 49/241 [00:07<00:27,  6.96it/s]Epoch: 3, train for the 50-th batch, train loss: 0.5620054602622986:  21%|██▋          | 50/241 [00:07<00:26,  7.27it/s]Epoch: 2, train for the 186-th batch, train loss: 0.5444775819778442:  78%|████████▌  | 185/237 [00:35<00:11,  4.34it/s]Epoch: 2, train for the 186-th batch, train loss: 0.5444775819778442:  78%|████████▋  | 186/237 [00:35<00:11,  4.39it/s]evaluate for the 3-th batch, evaluate loss: 0.722919762134552:  10%|██                   | 2/21 [00:00<00:01, 12.77it/s]Epoch: 5, train for the 18-th batch, train loss: 0.5449643731117249:  12%|█▌           | 17/146 [00:02<00:25,  5.13it/s]Epoch: 3, train for the 51-th batch, train loss: 0.560089111328125:  21%|██▉           | 50/241 [00:07<00:26,  7.27it/s]Epoch: 5, train for the 18-th batch, train loss: 0.5449643731117249:  12%|█▌           | 18/146 [00:02<00:22,  5.78it/s]Epoch: 3, train for the 51-th batch, train loss: 0.560089111328125:  21%|██▉           | 51/241 [00:07<00:25,  7.60it/s]evaluate for the 4-th batch, evaluate loss: 0.6242865324020386:  10%|█▉                  | 2/21 [00:00<00:01, 12.77it/s]evaluate for the 4-th batch, evaluate loss: 0.6242865324020386:  19%|███▊                | 4/21 [00:00<00:01, 11.27it/s]Epoch: 1, train for the 371-th batch, train loss: 0.3784681558609009:  97%|██████████▋| 370/383 [01:58<00:05,  2.54it/s]Epoch: 3, train for the 52-th batch, train loss: 0.5772403478622437:  21%|██▊          | 51/241 [00:07<00:25,  7.60it/s]Epoch: 3, train for the 52-th batch, train loss: 0.5772403478622437:  22%|██▊          | 52/241 [00:07<00:24,  7.74it/s]Epoch: 4, train for the 47-th batch, train loss: 0.6000242233276367:  30%|███▉         | 46/151 [00:07<00:16,  6.42it/s]Epoch: 5, train for the 19-th batch, train loss: 0.5483911633491516:  12%|█▌           | 18/146 [00:02<00:22,  5.78it/s]Epoch: 5, train for the 19-th batch, train loss: 0.5483911633491516:  13%|█▋           | 19/146 [00:02<00:21,  6.02it/s]evaluate for the 5-th batch, evaluate loss: 0.7321484088897705:  19%|███▊                | 4/21 [00:00<00:01, 11.27it/s]Epoch: 4, train for the 47-th batch, train loss: 0.6000242233276367:  31%|████         | 47/151 [00:07<00:24,  4.23it/s]Epoch: 2, train for the 187-th batch, train loss: 0.6019105315208435:  78%|████████▋  | 186/237 [00:35<00:11,  4.39it/s]Epoch: 1, train for the 371-th batch, train loss: 0.3784681558609009:  97%|██████████▋| 371/383 [01:58<00:04,  2.70it/s]Epoch: 2, train for the 187-th batch, train loss: 0.6019105315208435:  79%|████████▋  | 187/237 [00:35<00:11,  4.23it/s]evaluate for the 6-th batch, evaluate loss: 0.7169545292854309:  19%|███▊                | 4/21 [00:00<00:01, 11.27it/s]evaluate for the 6-th batch, evaluate loss: 0.7169545292854309:  29%|█████▋              | 6/21 [00:00<00:01, 12.11it/s]Epoch: 3, train for the 53-th batch, train loss: 0.4342643618583679:  22%|██▊          | 52/241 [00:07<00:24,  7.74it/s]Epoch: 3, train for the 53-th batch, train loss: 0.4342643618583679:  22%|██▊          | 53/241 [00:07<00:25,  7.34it/s]Epoch: 5, train for the 20-th batch, train loss: 0.5485600829124451:  13%|█▋           | 19/146 [00:03<00:21,  6.02it/s]evaluate for the 7-th batch, evaluate loss: 0.6461927890777588:  29%|█████▋              | 6/21 [00:00<00:01, 12.11it/s]Epoch: 5, train for the 20-th batch, train loss: 0.5485600829124451:  14%|█▊           | 20/146 [00:03<00:20,  6.11it/s]Epoch: 4, train for the 48-th batch, train loss: 0.5841151475906372:  31%|████         | 47/151 [00:08<00:24,  4.23it/s]Epoch: 4, train for the 48-th batch, train loss: 0.5841151475906372:  32%|████▏        | 48/151 [00:08<00:22,  4.50it/s]evaluate for the 8-th batch, evaluate loss: 0.6613780856132507:  29%|█████▋              | 6/21 [00:00<00:01, 12.11it/s]evaluate for the 8-th batch, evaluate loss: 0.6613780856132507:  38%|███████▌            | 8/21 [00:00<00:01, 11.81it/s]Epoch: 3, train for the 54-th batch, train loss: 0.3857460021972656:  22%|██▊          | 53/241 [00:07<00:25,  7.34it/s]Epoch: 2, train for the 188-th batch, train loss: 0.6166127324104309:  79%|████████▋  | 187/237 [00:36<00:11,  4.23it/s]Epoch: 3, train for the 54-th batch, train loss: 0.3857460021972656:  22%|██▉          | 54/241 [00:07<00:26,  7.14it/s]Epoch: 2, train for the 188-th batch, train loss: 0.6166127324104309:  79%|████████▋  | 188/237 [00:36<00:12,  4.06it/s]Epoch: 4, train for the 49-th batch, train loss: 0.5941230058670044:  32%|████▏        | 48/151 [00:08<00:22,  4.50it/s]evaluate for the 9-th batch, evaluate loss: 0.6674290895462036:  38%|███████▌            | 8/21 [00:00<00:01, 11.81it/s]Epoch: 4, train for the 49-th batch, train loss: 0.5941230058670044:  32%|████▏        | 49/151 [00:08<00:20,  5.01it/s]evaluate for the 10-th batch, evaluate loss: 0.6767709255218506:  38%|███████▏           | 8/21 [00:00<00:01, 11.81it/s]evaluate for the 10-th batch, evaluate loss: 0.6767709255218506:  48%|████████▌         | 10/21 [00:00<00:00, 12.73it/s]Epoch: 3, train for the 55-th batch, train loss: 0.373979777097702:  22%|███▏          | 54/241 [00:07<00:26,  7.14it/s]Epoch: 3, train for the 55-th batch, train loss: 0.373979777097702:  23%|███▏          | 55/241 [00:07<00:24,  7.49it/s]evaluate for the 11-th batch, evaluate loss: 0.7067207098007202:  48%|████████▌         | 10/21 [00:00<00:00, 12.73it/s]Epoch: 5, train for the 21-th batch, train loss: 0.5659468173980713:  14%|█▊           | 20/146 [00:03<00:20,  6.11it/s]Epoch: 5, train for the 21-th batch, train loss: 0.5659468173980713:  14%|█▊           | 21/146 [00:03<00:25,  4.97it/s]Epoch: 4, train for the 50-th batch, train loss: 0.5667962431907654:  32%|████▏        | 49/151 [00:08<00:20,  5.01it/s]Epoch: 4, train for the 50-th batch, train loss: 0.5667962431907654:  33%|████▎        | 50/151 [00:08<00:18,  5.42it/s]evaluate for the 12-th batch, evaluate loss: 0.6879971027374268:  48%|████████▌         | 10/21 [00:00<00:00, 12.73it/s]evaluate for the 12-th batch, evaluate loss: 0.6879971027374268:  57%|██████████▎       | 12/21 [00:00<00:00, 13.97it/s]Epoch: 1, train for the 372-th batch, train loss: 0.45707693696022034:  97%|█████████▋| 371/383 [01:58<00:04,  2.70it/s]Epoch: 2, train for the 189-th batch, train loss: 0.5958292484283447:  79%|████████▋  | 188/237 [00:36<00:12,  4.06it/s]Epoch: 3, train for the 56-th batch, train loss: 0.39874139428138733:  23%|██▋         | 55/241 [00:08<00:24,  7.49it/s]Epoch: 3, train for the 56-th batch, train loss: 0.39874139428138733:  23%|██▊         | 56/241 [00:08<00:24,  7.46it/s]Epoch: 2, train for the 189-th batch, train loss: 0.5958292484283447:  80%|████████▊  | 189/237 [00:36<00:11,  4.18it/s]Epoch: 5, train for the 22-th batch, train loss: 0.5383477807044983:  14%|█▊           | 21/146 [00:03<00:25,  4.97it/s]Epoch: 5, train for the 22-th batch, train loss: 0.5383477807044983:  15%|█▉           | 22/146 [00:03<00:22,  5.62it/s]evaluate for the 13-th batch, evaluate loss: 0.6770908832550049:  57%|██████████▎       | 12/21 [00:01<00:00, 13.97it/s]Epoch: 1, train for the 372-th batch, train loss: 0.45707693696022034:  97%|█████████▋| 372/383 [01:58<00:04,  2.36it/s]Epoch: 4, train for the 51-th batch, train loss: 0.6156523823738098:  33%|████▎        | 50/151 [00:08<00:18,  5.42it/s]Epoch: 4, train for the 51-th batch, train loss: 0.6156523823738098:  34%|████▍        | 51/151 [00:08<00:17,  5.85it/s]Epoch: 3, train for the 57-th batch, train loss: 0.439786434173584:  23%|███▎          | 56/241 [00:08<00:24,  7.46it/s]evaluate for the 14-th batch, evaluate loss: 0.64011150598526:  57%|███████████▍        | 12/21 [00:01<00:00, 13.97it/s]evaluate for the 14-th batch, evaluate loss: 0.64011150598526:  67%|█████████████▎      | 14/21 [00:01<00:00, 13.99it/s]Epoch: 3, train for the 57-th batch, train loss: 0.439786434173584:  24%|███▎          | 57/241 [00:08<00:23,  7.93it/s]Epoch: 5, train for the 23-th batch, train loss: 0.5561361908912659:  15%|█▉           | 22/146 [00:03<00:22,  5.62it/s]Epoch: 5, train for the 23-th batch, train loss: 0.5561361908912659:  16%|██           | 23/146 [00:03<00:19,  6.29it/s]Epoch: 2, train for the 190-th batch, train loss: 0.610265851020813:  80%|█████████▌  | 189/237 [00:36<00:11,  4.18it/s]evaluate for the 15-th batch, evaluate loss: 0.675701916217804:  67%|████████████▋      | 14/21 [00:01<00:00, 13.99it/s]Epoch: 2, train for the 190-th batch, train loss: 0.610265851020813:  80%|█████████▌  | 190/237 [00:36<00:10,  4.31it/s]Epoch: 4, train for the 52-th batch, train loss: 0.6123143434524536:  34%|████▍        | 51/151 [00:08<00:17,  5.85it/s]Epoch: 4, train for the 52-th batch, train loss: 0.6123143434524536:  34%|████▍        | 52/151 [00:08<00:16,  6.13it/s]Epoch: 3, train for the 58-th batch, train loss: 0.3452172875404358:  24%|███          | 57/241 [00:08<00:23,  7.93it/s]evaluate for the 16-th batch, evaluate loss: 0.6702142953872681:  67%|████████████      | 14/21 [00:01<00:00, 13.99it/s]evaluate for the 16-th batch, evaluate loss: 0.6702142953872681:  76%|█████████████▋    | 16/21 [00:01<00:00, 13.39it/s]Epoch: 3, train for the 58-th batch, train loss: 0.3452172875404358:  24%|███▏         | 58/241 [00:08<00:25,  7.27it/s]Epoch: 5, train for the 24-th batch, train loss: 0.5358719229698181:  16%|██           | 23/146 [00:03<00:19,  6.29it/s]Epoch: 5, train for the 24-th batch, train loss: 0.5358719229698181:  16%|██▏          | 24/146 [00:03<00:18,  6.44it/s]evaluate for the 17-th batch, evaluate loss: 0.5766680240631104:  76%|█████████████▋    | 16/21 [00:01<00:00, 13.39it/s]Epoch: 4, train for the 53-th batch, train loss: 0.6102581024169922:  34%|████▍        | 52/151 [00:08<00:16,  6.13it/s]Epoch: 4, train for the 53-th batch, train loss: 0.6102581024169922:  35%|████▌        | 53/151 [00:08<00:15,  6.15it/s]Epoch: 3, train for the 59-th batch, train loss: 0.6045876741409302:  24%|███▏         | 58/241 [00:08<00:25,  7.27it/s]Epoch: 3, train for the 59-th batch, train loss: 0.6045876741409302:  24%|███▏         | 59/241 [00:08<00:24,  7.32it/s]evaluate for the 18-th batch, evaluate loss: 0.6395645141601562:  76%|█████████████▋    | 16/21 [00:01<00:00, 13.39it/s]evaluate for the 18-th batch, evaluate loss: 0.6395645141601562:  86%|███████████████▍  | 18/21 [00:01<00:00, 13.60it/s]Epoch: 5, train for the 25-th batch, train loss: 0.5534379482269287:  16%|██▏          | 24/146 [00:03<00:18,  6.44it/s]Epoch: 5, train for the 25-th batch, train loss: 0.5534379482269287:  17%|██▏          | 25/146 [00:03<00:18,  6.48it/s]Epoch: 2, train for the 191-th batch, train loss: 0.5571397542953491:  80%|████████▊  | 190/237 [00:36<00:10,  4.31it/s]Epoch: 1, train for the 373-th batch, train loss: 0.5574931502342224:  97%|██████████▋| 372/383 [01:59<00:04,  2.36it/s]evaluate for the 19-th batch, evaluate loss: 0.6288927793502808:  86%|███████████████▍  | 18/21 [00:01<00:00, 13.60it/s]Epoch: 2, train for the 191-th batch, train loss: 0.5571397542953491:  81%|████████▊  | 191/237 [00:36<00:11,  4.11it/s]Epoch: 1, train for the 373-th batch, train loss: 0.5574931502342224:  97%|██████████▋| 373/383 [01:59<00:04,  2.27it/s]evaluate for the 20-th batch, evaluate loss: 0.6140893697738647:  86%|███████████████▍  | 18/21 [00:01<00:00, 13.60it/s]evaluate for the 20-th batch, evaluate loss: 0.6140893697738647:  95%|█████████████████▏| 20/21 [00:01<00:00, 13.57it/s]Epoch: 4, train for the 54-th batch, train loss: 0.5897099375724792:  35%|████▌        | 53/151 [00:09<00:15,  6.15it/s]Epoch: 3, train for the 60-th batch, train loss: 0.599448025226593:  24%|███▍          | 59/241 [00:08<00:24,  7.32it/s]Epoch: 4, train for the 54-th batch, train loss: 0.5897099375724792:  36%|████▋        | 54/151 [00:09<00:16,  5.86it/s]Epoch: 3, train for the 60-th batch, train loss: 0.599448025226593:  25%|███▍          | 60/241 [00:08<00:27,  6.60it/s]evaluate for the 21-th batch, evaluate loss: 0.5674523711204529:  95%|█████████████████▏| 20/21 [00:01<00:00, 13.57it/s]evaluate for the 21-th batch, evaluate loss: 0.5674523711204529: 100%|██████████████████| 21/21 [00:01<00:00, 13.26it/s]
Epoch: 2, train for the 192-th batch, train loss: 0.5855596661567688:  81%|████████▊  | 191/237 [00:37<00:11,  4.11it/s]Epoch: 5, train for the 26-th batch, train loss: 0.47829437255859375:  17%|██          | 25/146 [00:04<00:18,  6.48it/s]Epoch: 5, train for the 26-th batch, train loss: 0.47829437255859375:  18%|██▏         | 26/146 [00:04<00:21,  5.51it/s]Epoch: 2, train for the 192-th batch, train loss: 0.5855596661567688:  81%|████████▉  | 192/237 [00:37<00:10,  4.24it/s]Epoch: 3, train for the 61-th batch, train loss: 0.5661249756813049:  25%|███▏         | 60/241 [00:08<00:27,  6.60it/s]Epoch: 4, train for the 55-th batch, train loss: 0.567764163017273:  36%|█████         | 54/151 [00:09<00:16,  5.86it/s]Epoch: 3, train for the 61-th batch, train loss: 0.5661249756813049:  25%|███▎         | 61/241 [00:08<00:28,  6.39it/s]Epoch: 5, train for the 27-th batch, train loss: 0.5430733561515808:  18%|██▎          | 26/146 [00:04<00:21,  5.51it/s]Epoch: 4, train for the 55-th batch, train loss: 0.567764163017273:  36%|█████         | 55/151 [00:09<00:17,  5.61it/s]Epoch: 2, train for the 193-th batch, train loss: 0.5735847353935242:  81%|████████▉  | 192/237 [00:37<00:10,  4.24it/s]INFO:root:Epoch: 5, learning rate: 0.0001, train loss: 0.4536
INFO:root:train average_precision, 0.8793
INFO:root:train roc_auc, 0.8647
INFO:root:validate loss: 0.4863
INFO:root:validate average_precision, 0.8543
Epoch: 1, train for the 374-th batch, train loss: 0.3875684142112732:  97%|██████████▋| 373/383 [01:59<00:04,  2.27it/s]INFO:root:validate roc_auc, 0.8390
INFO:root:new node validate loss: 0.6644
INFO:root:new node validate first_1_average_precision, 0.6970
INFO:root:new node validate first_1_roc_auc, 0.6810
INFO:root:new node validate first_3_average_precision, 0.7126
INFO:root:new node validate first_3_roc_auc, 0.6896
INFO:root:new node validate first_10_average_precision, 0.7081
INFO:root:new node validate first_10_roc_auc, 0.6909
INFO:root:new node validate average_precision, 0.7154
INFO:root:new node validate roc_auc, 0.7083
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]Epoch: 2, train for the 193-th batch, train loss: 0.5735847353935242:  81%|████████▉  | 193/237 [00:37<00:10,  4.39it/s]Epoch: 5, train for the 28-th batch, train loss: 0.5265037417411804:  18%|██▎          | 26/146 [00:04<00:21,  5.51it/s]Epoch: 5, train for the 28-th batch, train loss: 0.5265037417411804:  19%|██▍          | 28/146 [00:04<00:18,  6.55it/s]Epoch: 3, train for the 62-th batch, train loss: 0.606099009513855:  25%|███▌          | 61/241 [00:09<00:28,  6.39it/s]Epoch: 4, train for the 56-th batch, train loss: 0.4868009388446808:  36%|████▋        | 55/151 [00:09<00:17,  5.61it/s]Epoch: 3, train for the 62-th batch, train loss: 0.606099009513855:  26%|███▌          | 62/241 [00:09<00:30,  5.84it/s]Epoch: 4, train for the 56-th batch, train loss: 0.4868009388446808:  37%|████▊        | 56/151 [00:09<00:17,  5.52it/s]Epoch: 1, train for the 374-th batch, train loss: 0.3875684142112732:  98%|██████████▋| 374/383 [01:59<00:04,  2.25it/s]Epoch: 6, train for the 1-th batch, train loss: 0.7736127972602844:   0%|                       | 0/119 [00:00<?, ?it/s]Epoch: 6, train for the 1-th batch, train loss: 0.7736127972602844:   1%|▏              | 1/119 [00:00<00:16,  7.09it/s]Epoch: 5, train for the 29-th batch, train loss: 0.48423394560813904:  19%|██▎         | 28/146 [00:04<00:18,  6.55it/s]Epoch: 5, train for the 29-th batch, train loss: 0.48423394560813904:  20%|██▍         | 29/146 [00:04<00:17,  6.72it/s]Epoch: 2, train for the 194-th batch, train loss: 0.5598801970481873:  81%|████████▉  | 193/237 [00:37<00:10,  4.39it/s]Epoch: 3, train for the 63-th batch, train loss: 0.7554571628570557:  26%|███▎         | 62/241 [00:09<00:30,  5.84it/s]Epoch: 3, train for the 63-th batch, train loss: 0.7554571628570557:  26%|███▍         | 63/241 [00:09<00:30,  5.87it/s]Epoch: 4, train for the 57-th batch, train loss: 0.5575475692749023:  37%|████▊        | 56/151 [00:09<00:17,  5.52it/s]Epoch: 2, train for the 194-th batch, train loss: 0.5598801970481873:  82%|█████████  | 194/237 [00:37<00:10,  4.25it/s]Epoch: 6, train for the 2-th batch, train loss: 0.8462594151496887:   1%|▏              | 1/119 [00:00<00:16,  7.09it/s]Epoch: 6, train for the 2-th batch, train loss: 0.8462594151496887:   2%|▎              | 2/119 [00:00<00:15,  7.55it/s]Epoch: 4, train for the 57-th batch, train loss: 0.5575475692749023:  38%|████▉        | 57/151 [00:09<00:17,  5.24it/s]Epoch: 5, train for the 30-th batch, train loss: 0.48389506340026855:  20%|██▍         | 29/146 [00:04<00:17,  6.72it/s]Epoch: 5, train for the 30-th batch, train loss: 0.48389506340026855:  21%|██▍         | 30/146 [00:04<00:17,  6.66it/s]Epoch: 1, train for the 375-th batch, train loss: 0.538127064704895:  98%|███████████▋| 374/383 [02:00<00:04,  2.25it/s]Epoch: 3, train for the 64-th batch, train loss: 0.3000999689102173:  26%|███▍         | 63/241 [00:09<00:30,  5.87it/s]Epoch: 3, train for the 64-th batch, train loss: 0.3000999689102173:  27%|███▍         | 64/241 [00:09<00:27,  6.40it/s]Epoch: 6, train for the 3-th batch, train loss: 0.8272474408149719:   2%|▎              | 2/119 [00:00<00:15,  7.55it/s]Epoch: 6, train for the 3-th batch, train loss: 0.8272474408149719:   3%|▍              | 3/119 [00:00<00:15,  7.45it/s]Epoch: 1, train for the 375-th batch, train loss: 0.538127064704895:  98%|███████████▋| 375/383 [02:00<00:03,  2.43it/s]Epoch: 4, train for the 58-th batch, train loss: 0.5487929582595825:  38%|████▉        | 57/151 [00:09<00:17,  5.24it/s]Epoch: 4, train for the 58-th batch, train loss: 0.5487929582595825:  38%|████▉        | 58/151 [00:09<00:16,  5.64it/s]Epoch: 2, train for the 195-th batch, train loss: 0.5487692356109619:  82%|█████████  | 194/237 [00:37<00:10,  4.25it/s]Epoch: 5, train for the 31-th batch, train loss: 0.5263906121253967:  21%|██▋          | 30/146 [00:04<00:17,  6.66it/s]Epoch: 5, train for the 31-th batch, train loss: 0.5263906121253967:  21%|██▊          | 31/146 [00:04<00:16,  6.78it/s]Epoch: 3, train for the 65-th batch, train loss: 0.3496168553829193:  27%|███▍         | 64/241 [00:09<00:27,  6.40it/s]Epoch: 3, train for the 65-th batch, train loss: 0.3496168553829193:  27%|███▌         | 65/241 [00:09<00:26,  6.74it/s]Epoch: 2, train for the 195-th batch, train loss: 0.5487692356109619:  82%|█████████  | 195/237 [00:37<00:09,  4.32it/s]Epoch: 6, train for the 4-th batch, train loss: 0.7440842986106873:   3%|▍              | 3/119 [00:00<00:15,  7.45it/s]Epoch: 6, train for the 4-th batch, train loss: 0.7440842986106873:   3%|▌              | 4/119 [00:00<00:15,  7.61it/s]Epoch: 4, train for the 59-th batch, train loss: 0.5462065935134888:  38%|████▉        | 58/151 [00:09<00:16,  5.64it/s]Epoch: 4, train for the 59-th batch, train loss: 0.5462065935134888:  39%|█████        | 59/151 [00:09<00:15,  5.92it/s]Epoch: 5, train for the 32-th batch, train loss: 0.49285802245140076:  21%|██▌         | 31/146 [00:05<00:16,  6.78it/s]Epoch: 5, train for the 32-th batch, train loss: 0.49285802245140076:  22%|██▋         | 32/146 [00:05<00:16,  6.85it/s]Epoch: 3, train for the 66-th batch, train loss: 0.2760680913925171:  27%|███▌         | 65/241 [00:09<00:26,  6.74it/s]Epoch: 3, train for the 66-th batch, train loss: 0.2760680913925171:  27%|███▌         | 66/241 [00:09<00:26,  6.71it/s]Epoch: 2, train for the 196-th batch, train loss: 0.576457142829895:  82%|█████████▊  | 195/237 [00:37<00:09,  4.32it/s]Epoch: 5, train for the 33-th batch, train loss: 0.4871586561203003:  22%|██▊          | 32/146 [00:05<00:16,  6.85it/s]Epoch: 4, train for the 60-th batch, train loss: 0.5201376080513:  39%|██████▎         | 59/151 [00:10<00:15,  5.92it/s]Epoch: 2, train for the 196-th batch, train loss: 0.576457142829895:  83%|█████████▉  | 196/237 [00:38<00:09,  4.29it/s]Epoch: 4, train for the 60-th batch, train loss: 0.5201376080513:  40%|██████▎         | 60/151 [00:10<00:15,  5.92it/s]Epoch: 3, train for the 67-th batch, train loss: 0.43237388134002686:  27%|███▎        | 66/241 [00:09<00:26,  6.71it/s]Epoch: 3, train for the 67-th batch, train loss: 0.43237388134002686:  28%|███▎        | 67/241 [00:09<00:24,  7.02it/s]Epoch: 5, train for the 34-th batch, train loss: 0.5058944821357727:  22%|██▊          | 32/146 [00:05<00:16,  6.85it/s]Epoch: 5, train for the 34-th batch, train loss: 0.5058944821357727:  23%|███          | 34/146 [00:05<00:13,  8.09it/s]Epoch: 5, train for the 35-th batch, train loss: 0.5008100867271423:  23%|███          | 34/146 [00:05<00:13,  8.09it/s]Epoch: 4, train for the 61-th batch, train loss: 0.5616717338562012:  40%|█████▏       | 60/151 [00:10<00:15,  5.92it/s]Epoch: 5, train for the 35-th batch, train loss: 0.5008100867271423:  24%|███          | 35/146 [00:05<00:13,  8.45it/s]Epoch: 4, train for the 61-th batch, train loss: 0.5616717338562012:  40%|█████▎       | 61/151 [00:10<00:14,  6.05it/s]Epoch: 3, train for the 68-th batch, train loss: 0.5040248036384583:  28%|███▌         | 67/241 [00:09<00:24,  7.02it/s]Epoch: 2, train for the 197-th batch, train loss: 0.5657555460929871:  83%|█████████  | 196/237 [00:38<00:09,  4.29it/s]Epoch: 3, train for the 68-th batch, train loss: 0.5040248036384583:  28%|███▋         | 68/241 [00:09<00:25,  6.80it/s]Epoch: 6, train for the 5-th batch, train loss: 0.6738707423210144:   3%|▌              | 4/119 [00:00<00:15,  7.61it/s]Epoch: 6, train for the 5-th batch, train loss: 0.6738707423210144:   4%|▋              | 5/119 [00:00<00:25,  4.39it/s]Epoch: 1, train for the 376-th batch, train loss: 0.3743170201778412:  98%|██████████▊| 375/383 [02:00<00:03,  2.43it/s]Epoch: 2, train for the 197-th batch, train loss: 0.5657555460929871:  83%|█████████▏ | 197/237 [00:38<00:09,  4.30it/s]Epoch: 5, train for the 36-th batch, train loss: 0.46619048714637756:  24%|██▉         | 35/146 [00:05<00:13,  8.45it/s]Epoch: 5, train for the 36-th batch, train loss: 0.46619048714637756:  25%|██▉         | 36/146 [00:05<00:13,  8.04it/s]Epoch: 1, train for the 376-th batch, train loss: 0.3743170201778412:  98%|██████████▊| 376/383 [02:00<00:03,  2.10it/s]Epoch: 6, train for the 6-th batch, train loss: 0.5799964666366577:   4%|▋              | 5/119 [00:01<00:25,  4.39it/s]Epoch: 4, train for the 62-th batch, train loss: 0.5745962262153625:  40%|█████▎       | 61/151 [00:10<00:14,  6.05it/s]Epoch: 6, train for the 6-th batch, train loss: 0.5799964666366577:   5%|▊              | 6/119 [00:01<00:22,  5.10it/s]Epoch: 3, train for the 69-th batch, train loss: 0.6269577741622925:  28%|███▋         | 68/241 [00:10<00:25,  6.80it/s]Epoch: 4, train for the 62-th batch, train loss: 0.5745962262153625:  41%|█████▎       | 62/151 [00:10<00:15,  5.83it/s]Epoch: 3, train for the 69-th batch, train loss: 0.6269577741622925:  29%|███▋         | 69/241 [00:10<00:26,  6.54it/s]Epoch: 2, train for the 198-th batch, train loss: 0.5501188635826111:  83%|█████████▏ | 197/237 [00:38<00:09,  4.30it/s]Epoch: 5, train for the 37-th batch, train loss: 0.49531158804893494:  25%|██▉         | 36/146 [00:05<00:13,  8.04it/s]Epoch: 5, train for the 37-th batch, train loss: 0.49531158804893494:  25%|███         | 37/146 [00:05<00:13,  8.14it/s]Epoch: 2, train for the 198-th batch, train loss: 0.5501188635826111:  84%|█████████▏ | 198/237 [00:38<00:08,  4.51it/s]Epoch: 6, train for the 7-th batch, train loss: 0.5616611242294312:   5%|▊              | 6/119 [00:01<00:22,  5.10it/s]Epoch: 6, train for the 7-th batch, train loss: 0.5616611242294312:   6%|▉              | 7/119 [00:01<00:19,  5.70it/s]Epoch: 3, train for the 70-th batch, train loss: 0.5450806021690369:  29%|███▋         | 69/241 [00:10<00:26,  6.54it/s]Epoch: 4, train for the 63-th batch, train loss: 0.5598790049552917:  41%|█████▎       | 62/151 [00:10<00:15,  5.83it/s]Epoch: 3, train for the 70-th batch, train loss: 0.5450806021690369:  29%|███▊         | 70/241 [00:10<00:27,  6.12it/s]Epoch: 4, train for the 63-th batch, train loss: 0.5598790049552917:  42%|█████▍       | 63/151 [00:10<00:15,  5.53it/s]Epoch: 5, train for the 38-th batch, train loss: 0.4998885989189148:  25%|███▎         | 37/146 [00:05<00:13,  8.14it/s]Epoch: 5, train for the 38-th batch, train loss: 0.4998885989189148:  26%|███▍         | 38/146 [00:05<00:14,  7.65it/s]Epoch: 1, train for the 377-th batch, train loss: 0.44720888137817383:  98%|█████████▊| 376/383 [02:01<00:03,  2.10it/s]Epoch: 6, train for the 8-th batch, train loss: 0.48144567012786865:   6%|▊             | 7/119 [00:01<00:19,  5.70it/s]Epoch: 6, train for the 8-th batch, train loss: 0.48144567012786865:   7%|▉             | 8/119 [00:01<00:17,  6.17it/s]Epoch: 2, train for the 199-th batch, train loss: 0.5678510665893555:  84%|█████████▏ | 198/237 [00:38<00:08,  4.51it/s]Epoch: 1, train for the 377-th batch, train loss: 0.44720888137817383:  98%|█████████▊| 377/383 [02:01<00:02,  2.26it/s]Epoch: 2, train for the 199-th batch, train loss: 0.5678510665893555:  84%|█████████▏ | 199/237 [00:38<00:08,  4.35it/s]Epoch: 5, train for the 39-th batch, train loss: 0.5351907014846802:  26%|███▍         | 38/146 [00:05<00:14,  7.65it/s]Epoch: 3, train for the 71-th batch, train loss: 0.6465768218040466:  29%|███▊         | 70/241 [00:10<00:27,  6.12it/s]Epoch: 5, train for the 39-th batch, train loss: 0.5351907014846802:  27%|███▍         | 39/146 [00:05<00:14,  7.43it/s]Epoch: 3, train for the 71-th batch, train loss: 0.6465768218040466:  29%|███▊         | 71/241 [00:10<00:28,  5.91it/s]Epoch: 4, train for the 64-th batch, train loss: 0.513177216053009:  42%|█████▊        | 63/151 [00:10<00:15,  5.53it/s]Epoch: 6, train for the 9-th batch, train loss: 0.48258689045906067:   7%|▉             | 8/119 [00:01<00:17,  6.17it/s]Epoch: 6, train for the 9-th batch, train loss: 0.48258689045906067:   8%|█             | 9/119 [00:01<00:17,  6.32it/s]Epoch: 4, train for the 64-th batch, train loss: 0.513177216053009:  42%|█████▉        | 64/151 [00:10<00:16,  5.38it/s]Epoch: 3, train for the 72-th batch, train loss: 0.5946491956710815:  29%|███▊         | 71/241 [00:10<00:28,  5.91it/s]Epoch: 5, train for the 40-th batch, train loss: 0.4560697078704834:  27%|███▍         | 39/146 [00:06<00:14,  7.43it/s]Epoch: 3, train for the 72-th batch, train loss: 0.5946491956710815:  30%|███▉         | 72/241 [00:10<00:27,  6.12it/s]Epoch: 2, train for the 200-th batch, train loss: 0.5884969830513:  84%|███████████▊  | 199/237 [00:38<00:08,  4.35it/s]Epoch: 5, train for the 40-th batch, train loss: 0.4560697078704834:  27%|███▌         | 40/146 [00:06<00:15,  6.94it/s]Epoch: 6, train for the 10-th batch, train loss: 0.45793089270591736:   8%|▉            | 9/119 [00:01<00:17,  6.32it/s]Epoch: 4, train for the 65-th batch, train loss: 0.4867976903915405:  42%|█████▌       | 64/151 [00:11<00:16,  5.38it/s]Epoch: 6, train for the 10-th batch, train loss: 0.45793089270591736:   8%|█           | 10/119 [00:01<00:16,  6.42it/s]Epoch: 4, train for the 65-th batch, train loss: 0.4867976903915405:  43%|█████▌       | 65/151 [00:11<00:15,  5.54it/s]Epoch: 2, train for the 200-th batch, train loss: 0.5884969830513:  84%|███████████▊  | 200/237 [00:38<00:08,  4.17it/s]Epoch: 3, train for the 73-th batch, train loss: 0.4932721257209778:  30%|███▉         | 72/241 [00:10<00:27,  6.12it/s]Epoch: 3, train for the 73-th batch, train loss: 0.4932721257209778:  30%|███▉         | 73/241 [00:10<00:25,  6.53it/s]Epoch: 5, train for the 41-th batch, train loss: 0.4664580523967743:  27%|███▌         | 40/146 [00:06<00:15,  6.94it/s]Epoch: 5, train for the 41-th batch, train loss: 0.4664580523967743:  28%|███▋         | 41/146 [00:06<00:15,  6.97it/s]Epoch: 4, train for the 66-th batch, train loss: 0.5493327379226685:  43%|█████▌       | 65/151 [00:11<00:15,  5.54it/s]Epoch: 1, train for the 378-th batch, train loss: 0.46304771304130554:  98%|█████████▊| 377/383 [02:01<00:02,  2.26it/s]Epoch: 4, train for the 66-th batch, train loss: 0.5493327379226685:  44%|█████▋       | 66/151 [00:11<00:14,  5.88it/s]Epoch: 2, train for the 201-th batch, train loss: 0.5606220364570618:  84%|█████████▎ | 200/237 [00:39<00:08,  4.17it/s]Epoch: 6, train for the 11-th batch, train loss: 0.4244547188282013:   8%|█            | 10/119 [00:01<00:16,  6.42it/s]Epoch: 5, train for the 42-th batch, train loss: 0.49142637848854065:  28%|███▎        | 41/146 [00:06<00:15,  6.97it/s]Epoch: 3, train for the 74-th batch, train loss: 0.4574587047100067:  30%|███▉         | 73/241 [00:10<00:25,  6.53it/s]Epoch: 5, train for the 42-th batch, train loss: 0.49142637848854065:  29%|███▍        | 42/146 [00:06<00:14,  7.42it/s]Epoch: 6, train for the 11-th batch, train loss: 0.4244547188282013:   9%|█▏           | 11/119 [00:01<00:19,  5.57it/s]Epoch: 3, train for the 74-th batch, train loss: 0.4574587047100067:  31%|███▉         | 74/241 [00:10<00:24,  6.71it/s]Epoch: 1, train for the 378-th batch, train loss: 0.46304771304130554:  99%|█████████▊| 378/383 [02:01<00:02,  2.20it/s]Epoch: 2, train for the 201-th batch, train loss: 0.5606220364570618:  85%|█████████▎ | 201/237 [00:39<00:08,  4.20it/s]Epoch: 4, train for the 67-th batch, train loss: 0.5578526854515076:  44%|█████▋       | 66/151 [00:11<00:14,  5.88it/s]Epoch: 4, train for the 67-th batch, train loss: 0.5578526854515076:  44%|█████▊       | 67/151 [00:11<00:13,  6.17it/s]Epoch: 5, train for the 43-th batch, train loss: 0.48318007588386536:  29%|███▍        | 42/146 [00:06<00:14,  7.42it/s]Epoch: 3, train for the 75-th batch, train loss: 0.3782338500022888:  31%|███▉         | 74/241 [00:10<00:24,  6.71it/s]Epoch: 5, train for the 43-th batch, train loss: 0.48318007588386536:  29%|███▌        | 43/146 [00:06<00:14,  7.25it/s]Epoch: 6, train for the 12-th batch, train loss: 0.43643251061439514:   9%|█           | 11/119 [00:02<00:19,  5.57it/s]Epoch: 6, train for the 12-th batch, train loss: 0.43643251061439514:  10%|█▏          | 12/119 [00:02<00:18,  5.77it/s]Epoch: 3, train for the 75-th batch, train loss: 0.3782338500022888:  31%|████         | 75/241 [00:11<00:24,  6.67it/s]Epoch: 2, train for the 202-th batch, train loss: 0.5784658193588257:  85%|█████████▎ | 201/237 [00:39<00:08,  4.20it/s]Epoch: 4, train for the 68-th batch, train loss: 0.5397847890853882:  44%|█████▊       | 67/151 [00:11<00:13,  6.17it/s]Epoch: 4, train for the 68-th batch, train loss: 0.5397847890853882:  45%|█████▊       | 68/151 [00:11<00:13,  6.04it/s]Epoch: 2, train for the 202-th batch, train loss: 0.5784658193588257:  85%|█████████▍ | 202/237 [00:39<00:08,  4.25it/s]Epoch: 3, train for the 76-th batch, train loss: 0.2841922640800476:  31%|████         | 75/241 [00:11<00:24,  6.67it/s]Epoch: 5, train for the 44-th batch, train loss: 0.5156541466712952:  29%|███▊         | 43/146 [00:06<00:14,  7.25it/s]Epoch: 5, train for the 44-th batch, train loss: 0.5156541466712952:  30%|███▉         | 44/146 [00:06<00:14,  7.10it/s]Epoch: 3, train for the 76-th batch, train loss: 0.2841922640800476:  32%|████         | 76/241 [00:11<00:23,  6.88it/s]Epoch: 6, train for the 13-th batch, train loss: 0.41776588559150696:  10%|█▏          | 12/119 [00:02<00:18,  5.77it/s]Epoch: 6, train for the 13-th batch, train loss: 0.41776588559150696:  11%|█▎          | 13/119 [00:02<00:17,  6.01it/s]Epoch: 1, train for the 379-th batch, train loss: 0.316598504781723:  99%|███████████▊| 378/383 [02:02<00:02,  2.20it/s]Epoch: 5, train for the 45-th batch, train loss: 0.4932469129562378:  30%|███▉         | 44/146 [00:06<00:14,  7.10it/s]Epoch: 5, train for the 45-th batch, train loss: 0.4932469129562378:  31%|████         | 45/146 [00:06<00:13,  7.22it/s]Epoch: 4, train for the 69-th batch, train loss: 0.5133596062660217:  45%|█████▊       | 68/151 [00:11<00:13,  6.04it/s]Epoch: 4, train for the 69-th batch, train loss: 0.5133596062660217:  46%|█████▉       | 69/151 [00:11<00:14,  5.75it/s]Epoch: 3, train for the 77-th batch, train loss: 0.3640228807926178:  32%|████         | 76/241 [00:11<00:23,  6.88it/s]Epoch: 2, train for the 203-th batch, train loss: 0.5637257695198059:  85%|█████████▍ | 202/237 [00:39<00:08,  4.25it/s]Epoch: 6, train for the 14-th batch, train loss: 0.5171910524368286:  11%|█▍           | 13/119 [00:02<00:17,  6.01it/s]Epoch: 3, train for the 77-th batch, train loss: 0.3640228807926178:  32%|████▏        | 77/241 [00:11<00:25,  6.48it/s]Epoch: 6, train for the 14-th batch, train loss: 0.5171910524368286:  12%|█▌           | 14/119 [00:02<00:17,  6.06it/s]Epoch: 2, train for the 203-th batch, train loss: 0.5637257695198059:  86%|█████████▍ | 203/237 [00:39<00:07,  4.28it/s]Epoch: 1, train for the 379-th batch, train loss: 0.316598504781723:  99%|███████████▊| 379/383 [02:02<00:01,  2.17it/s]Epoch: 5, train for the 46-th batch, train loss: 0.4475044906139374:  31%|████         | 45/146 [00:06<00:13,  7.22it/s]Epoch: 5, train for the 46-th batch, train loss: 0.4475044906139374:  32%|████         | 46/146 [00:06<00:13,  7.50it/s]Epoch: 6, train for the 15-th batch, train loss: 0.4104331135749817:  12%|█▌           | 14/119 [00:02<00:17,  6.06it/s]Epoch: 6, train for the 15-th batch, train loss: 0.4104331135749817:  13%|█▋           | 15/119 [00:02<00:16,  6.48it/s]Epoch: 4, train for the 70-th batch, train loss: 0.5425784587860107:  46%|█████▉       | 69/151 [00:11<00:14,  5.75it/s]Epoch: 3, train for the 78-th batch, train loss: 0.5563715100288391:  32%|████▏        | 77/241 [00:11<00:25,  6.48it/s]Epoch: 4, train for the 70-th batch, train loss: 0.5425784587860107:  46%|██████       | 70/151 [00:11<00:14,  5.50it/s]Epoch: 3, train for the 78-th batch, train loss: 0.5563715100288391:  32%|████▏        | 78/241 [00:11<00:26,  6.16it/s]Epoch: 5, train for the 47-th batch, train loss: 0.49559256434440613:  32%|███▊        | 46/146 [00:06<00:13,  7.50it/s]Epoch: 5, train for the 47-th batch, train loss: 0.49559256434440613:  32%|███▊        | 47/146 [00:06<00:12,  7.63it/s]Epoch: 2, train for the 204-th batch, train loss: 0.546210527420044:  86%|██████████▎ | 203/237 [00:39<00:07,  4.28it/s]Epoch: 2, train for the 204-th batch, train loss: 0.546210527420044:  86%|██████████▎ | 204/237 [00:39<00:07,  4.24it/s]Epoch: 6, train for the 16-th batch, train loss: 0.5103392004966736:  13%|█▋           | 15/119 [00:02<00:16,  6.48it/s]Epoch: 6, train for the 16-th batch, train loss: 0.5103392004966736:  13%|█▋           | 16/119 [00:02<00:15,  6.74it/s]Epoch: 1, train for the 380-th batch, train loss: 0.43347007036209106:  99%|█████████▉| 379/383 [02:02<00:01,  2.17it/s]Epoch: 3, train for the 79-th batch, train loss: 0.4701732099056244:  32%|████▏        | 78/241 [00:11<00:26,  6.16it/s]Epoch: 3, train for the 79-th batch, train loss: 0.4701732099056244:  33%|████▎        | 79/241 [00:11<00:25,  6.36it/s]Epoch: 5, train for the 48-th batch, train loss: 0.5150106549263:  32%|█████▏          | 47/146 [00:07<00:12,  7.63it/s]Epoch: 5, train for the 48-th batch, train loss: 0.5150106549263:  33%|█████▎          | 48/146 [00:07<00:12,  7.56it/s]Epoch: 1, train for the 380-th batch, train loss: 0.43347007036209106:  99%|█████████▉| 380/383 [02:02<00:01,  2.36it/s]Epoch: 2, train for the 205-th batch, train loss: 0.5329117178916931:  86%|█████████▍ | 204/237 [00:40<00:07,  4.24it/s]Epoch: 5, train for the 49-th batch, train loss: 0.47595450282096863:  33%|███▉        | 48/146 [00:07<00:12,  7.56it/s]Epoch: 5, train for the 49-th batch, train loss: 0.47595450282096863:  34%|████        | 49/146 [00:07<00:12,  8.07it/s]Epoch: 2, train for the 205-th batch, train loss: 0.5329117178916931:  86%|█████████▌ | 205/237 [00:40<00:07,  4.50it/s]Epoch: 6, train for the 17-th batch, train loss: 0.41492822766304016:  13%|█▌          | 16/119 [00:02<00:15,  6.74it/s]Epoch: 6, train for the 17-th batch, train loss: 0.41492822766304016:  14%|█▋          | 17/119 [00:02<00:16,  6.15it/s]Epoch: 4, train for the 71-th batch, train loss: 0.5757971405982971:  46%|██████       | 70/151 [00:12<00:14,  5.50it/s]Epoch: 3, train for the 80-th batch, train loss: 0.3568490445613861:  33%|████▎        | 79/241 [00:11<00:25,  6.36it/s]Epoch: 4, train for the 71-th batch, train loss: 0.5757971405982971:  47%|██████       | 71/151 [00:12<00:17,  4.50it/s]Epoch: 3, train for the 80-th batch, train loss: 0.3568490445613861:  33%|████▎        | 80/241 [00:11<00:25,  6.27it/s]Epoch: 5, train for the 50-th batch, train loss: 0.5250561237335205:  34%|████▎        | 49/146 [00:07<00:12,  8.07it/s]Epoch: 5, train for the 50-th batch, train loss: 0.5250561237335205:  34%|████▍        | 50/146 [00:07<00:13,  7.29it/s]Epoch: 6, train for the 18-th batch, train loss: 0.43314436078071594:  14%|█▋          | 17/119 [00:02<00:16,  6.15it/s]Epoch: 6, train for the 18-th batch, train loss: 0.43314436078071594:  15%|█▊          | 18/119 [00:02<00:16,  6.12it/s]Epoch: 3, train for the 81-th batch, train loss: 0.3684319257736206:  33%|████▎        | 80/241 [00:11<00:25,  6.27it/s]Epoch: 1, train for the 381-th batch, train loss: 0.47724097967147827:  99%|█████████▉| 380/383 [02:02<00:01,  2.36it/s]Epoch: 3, train for the 81-th batch, train loss: 0.3684319257736206:  34%|████▎        | 81/241 [00:12<00:26,  5.94it/s]Epoch: 4, train for the 72-th batch, train loss: 0.529883086681366:  47%|██████▌       | 71/151 [00:12<00:17,  4.50it/s]Epoch: 4, train for the 72-th batch, train loss: 0.529883086681366:  48%|██████▋       | 72/151 [00:12<00:17,  4.55it/s]Epoch: 2, train for the 206-th batch, train loss: 0.5624583959579468:  86%|█████████▌ | 205/237 [00:40<00:07,  4.50it/s]Epoch: 2, train for the 206-th batch, train loss: 0.5624583959579468:  87%|█████████▌ | 206/237 [00:40<00:07,  4.02it/s]Epoch: 5, train for the 51-th batch, train loss: 0.4654388427734375:  34%|████▍        | 50/146 [00:07<00:13,  7.29it/s]Epoch: 1, train for the 381-th batch, train loss: 0.47724097967147827:  99%|█████████▉| 381/383 [02:02<00:00,  2.37it/s]Epoch: 6, train for the 19-th batch, train loss: 0.45388635993003845:  15%|█▊          | 18/119 [00:03<00:16,  6.12it/s]Epoch: 6, train for the 19-th batch, train loss: 0.45388635993003845:  16%|█▉          | 19/119 [00:03<00:16,  6.25it/s]Epoch: 5, train for the 51-th batch, train loss: 0.4654388427734375:  35%|████▌        | 51/146 [00:07<00:13,  6.80it/s]Epoch: 3, train for the 82-th batch, train loss: 0.44198381900787354:  34%|████        | 81/241 [00:12<00:26,  5.94it/s]Epoch: 3, train for the 82-th batch, train loss: 0.44198381900787354:  34%|████        | 82/241 [00:12<00:26,  5.99it/s]Epoch: 4, train for the 73-th batch, train loss: 0.5655730962753296:  48%|██████▏      | 72/151 [00:12<00:17,  4.55it/s]Epoch: 4, train for the 73-th batch, train loss: 0.5655730962753296:  48%|██████▎      | 73/151 [00:12<00:15,  4.89it/s]Epoch: 6, train for the 20-th batch, train loss: 0.47767481207847595:  16%|█▉          | 19/119 [00:03<00:16,  6.25it/s]Epoch: 6, train for the 20-th batch, train loss: 0.47767481207847595:  17%|██          | 20/119 [00:03<00:15,  6.39it/s]Epoch: 5, train for the 52-th batch, train loss: 0.519761860370636:  35%|████▉         | 51/146 [00:07<00:13,  6.80it/s]Epoch: 3, train for the 83-th batch, train loss: 0.5120172500610352:  34%|████▍        | 82/241 [00:12<00:26,  5.99it/s]Epoch: 5, train for the 52-th batch, train loss: 0.519761860370636:  36%|████▉         | 52/146 [00:07<00:14,  6.31it/s]Epoch: 3, train for the 83-th batch, train loss: 0.5120172500610352:  34%|████▍        | 83/241 [00:12<00:24,  6.33it/s]Epoch: 2, train for the 207-th batch, train loss: 0.5946373343467712:  87%|█████████▌ | 206/237 [00:40<00:07,  4.02it/s]Epoch: 4, train for the 74-th batch, train loss: 0.509534478187561:  48%|██████▊       | 73/151 [00:12<00:15,  4.89it/s]Epoch: 2, train for the 207-th batch, train loss: 0.5946373343467712:  87%|█████████▌ | 207/237 [00:40<00:07,  3.92it/s]Epoch: 4, train for the 74-th batch, train loss: 0.509534478187561:  49%|██████▊       | 74/151 [00:12<00:14,  5.18it/s]Epoch: 6, train for the 21-th batch, train loss: 0.498995840549469:  17%|██▎           | 20/119 [00:03<00:15,  6.39it/s]Epoch: 6, train for the 21-th batch, train loss: 0.498995840549469:  18%|██▍           | 21/119 [00:03<00:14,  6.61it/s]Epoch: 1, train for the 382-th batch, train loss: 0.4751453995704651:  99%|██████████▉| 381/383 [02:03<00:00,  2.37it/s]Epoch: 5, train for the 53-th batch, train loss: 0.4808987081050873:  36%|████▋        | 52/146 [00:07<00:14,  6.31it/s]Epoch: 5, train for the 53-th batch, train loss: 0.4808987081050873:  36%|████▋        | 53/146 [00:07<00:14,  6.50it/s]Epoch: 3, train for the 84-th batch, train loss: 0.5616575479507446:  34%|████▍        | 83/241 [00:12<00:24,  6.33it/s]Epoch: 3, train for the 84-th batch, train loss: 0.5616575479507446:  35%|████▌        | 84/241 [00:12<00:25,  6.22it/s]Epoch: 1, train for the 382-th batch, train loss: 0.4751453995704651: 100%|██████████▉| 382/383 [02:03<00:00,  2.43it/s]Epoch: 6, train for the 22-th batch, train loss: 0.4276881217956543:  18%|██▎          | 21/119 [00:03<00:14,  6.61it/s]Epoch: 6, train for the 22-th batch, train loss: 0.4276881217956543:  18%|██▍          | 22/119 [00:03<00:13,  7.14it/s]Epoch: 4, train for the 75-th batch, train loss: 0.5611578226089478:  49%|██████▎      | 74/151 [00:12<00:14,  5.18it/s]Epoch: 4, train for the 75-th batch, train loss: 0.5611578226089478:  50%|██████▍      | 75/151 [00:12<00:14,  5.35it/s]Epoch: 2, train for the 208-th batch, train loss: 0.5845538973808289:  87%|█████████▌ | 207/237 [00:40<00:07,  3.92it/s]Epoch: 5, train for the 54-th batch, train loss: 0.504704475402832:  36%|█████         | 53/146 [00:07<00:14,  6.50it/s]Epoch: 5, train for the 54-th batch, train loss: 0.504704475402832:  37%|█████▏        | 54/146 [00:07<00:13,  6.96it/s]Epoch: 2, train for the 208-th batch, train loss: 0.5845538973808289:  88%|█████████▋ | 208/237 [00:40<00:07,  4.01it/s]Epoch: 3, train for the 85-th batch, train loss: 0.4521279036998749:  35%|████▌        | 84/241 [00:12<00:25,  6.22it/s]Epoch: 6, train for the 23-th batch, train loss: 0.4974244236946106:  18%|██▍          | 22/119 [00:03<00:13,  7.14it/s]Epoch: 3, train for the 85-th batch, train loss: 0.4521279036998749:  35%|████▌        | 85/241 [00:12<00:24,  6.34it/s]Epoch: 6, train for the 23-th batch, train loss: 0.4974244236946106:  19%|██▌          | 23/119 [00:03<00:12,  7.39it/s]Epoch: 4, train for the 76-th batch, train loss: 0.5413835644721985:  50%|██████▍      | 75/151 [00:13<00:14,  5.35it/s]Epoch: 5, train for the 55-th batch, train loss: 0.5516064167022705:  37%|████▊        | 54/146 [00:08<00:13,  6.96it/s]Epoch: 4, train for the 76-th batch, train loss: 0.5413835644721985:  50%|██████▌      | 76/151 [00:13<00:13,  5.61it/s]Epoch: 5, train for the 55-th batch, train loss: 0.5516064167022705:  38%|████▉        | 55/146 [00:08<00:13,  6.91it/s]Epoch: 1, train for the 383-th batch, train loss: 0.4338965117931366: 100%|██████████▉| 382/383 [02:03<00:00,  2.43it/s]Epoch: 6, train for the 24-th batch, train loss: 0.4201827943325043:  19%|██▌          | 23/119 [00:03<00:12,  7.39it/s]Epoch: 3, train for the 86-th batch, train loss: 0.47502797842025757:  35%|████▏       | 85/241 [00:12<00:24,  6.34it/s]Epoch: 6, train for the 24-th batch, train loss: 0.4201827943325043:  20%|██▌          | 24/119 [00:03<00:12,  7.32it/s]Epoch: 3, train for the 86-th batch, train loss: 0.47502797842025757:  36%|████▎       | 86/241 [00:12<00:24,  6.35it/s]Epoch: 2, train for the 209-th batch, train loss: 0.5530133843421936:  88%|█████████▋ | 208/237 [00:41<00:07,  4.01it/s]Epoch: 5, train for the 56-th batch, train loss: 0.5245877504348755:  38%|████▉        | 55/146 [00:08<00:13,  6.91it/s]Epoch: 5, train for the 56-th batch, train loss: 0.5245877504348755:  38%|████▉        | 56/146 [00:08<00:12,  7.25it/s]Epoch: 1, train for the 383-th batch, train loss: 0.4338965117931366: 100%|███████████| 383/383 [02:03<00:00,  2.53it/s]Epoch: 1, train for the 383-th batch, train loss: 0.4338965117931366: 100%|███████████| 383/383 [02:03<00:00,  3.10it/s]
Epoch: 2, train for the 209-th batch, train loss: 0.5530133843421936:  88%|█████████▋ | 209/237 [00:41<00:07,  3.99it/s]Epoch: 4, train for the 77-th batch, train loss: 0.523319661617279:  50%|███████       | 76/151 [00:13<00:13,  5.61it/s]Epoch: 4, train for the 77-th batch, train loss: 0.523319661617279:  51%|███████▏      | 77/151 [00:13<00:13,  5.66it/s]Epoch: 3, train for the 87-th batch, train loss: 0.5402814149856567:  36%|████▋        | 86/241 [00:12<00:24,  6.35it/s]Epoch: 6, train for the 25-th batch, train loss: 0.40877798199653625:  20%|██▍         | 24/119 [00:03<00:12,  7.32it/s]Epoch: 6, train for the 25-th batch, train loss: 0.40877798199653625:  21%|██▌         | 25/119 [00:03<00:13,  6.97it/s]Epoch: 3, train for the 87-th batch, train loss: 0.5402814149856567:  36%|████▋        | 87/241 [00:12<00:23,  6.45it/s]Epoch: 5, train for the 57-th batch, train loss: 0.554791271686554:  38%|█████▎        | 56/146 [00:08<00:12,  7.25it/s]Epoch: 5, train for the 57-th batch, train loss: 0.554791271686554:  39%|█████▍        | 57/146 [00:08<00:12,  7.07it/s]Epoch: 4, train for the 78-th batch, train loss: 0.501277506351471:  51%|███████▏      | 77/151 [00:13<00:13,  5.66it/s]Epoch: 4, train for the 78-th batch, train loss: 0.501277506351471:  52%|███████▏      | 78/151 [00:13<00:12,  6.07it/s]Epoch: 3, train for the 88-th batch, train loss: 0.4762405753135681:  36%|████▋        | 87/241 [00:13<00:23,  6.45it/s]Epoch: 3, train for the 88-th batch, train loss: 0.4762405753135681:  37%|████▋        | 88/241 [00:13<00:24,  6.33it/s]Epoch: 2, train for the 210-th batch, train loss: 0.5914061665534973:  88%|█████████▋ | 209/237 [00:41<00:07,  3.99it/s]Epoch: 6, train for the 26-th batch, train loss: 0.40058302879333496:  21%|██▌         | 25/119 [00:04<00:13,  6.97it/s]Epoch: 6, train for the 26-th batch, train loss: 0.40058302879333496:  22%|██▌         | 26/119 [00:04<00:14,  6.44it/s]Epoch: 5, train for the 58-th batch, train loss: 0.507221519947052:  39%|█████▍        | 57/146 [00:08<00:12,  7.07it/s]Epoch: 2, train for the 210-th batch, train loss: 0.5914061665534973:  89%|█████████▋ | 210/237 [00:41<00:07,  3.83it/s]Epoch: 5, train for the 58-th batch, train loss: 0.507221519947052:  40%|█████▌        | 58/146 [00:08<00:12,  6.77it/s]Epoch: 4, train for the 79-th batch, train loss: 0.5730498433113098:  52%|██████▋      | 78/151 [00:13<00:12,  6.07it/s]Epoch: 4, train for the 79-th batch, train loss: 0.5730498433113098:  52%|██████▊      | 79/151 [00:13<00:11,  6.11it/s]Epoch: 3, train for the 89-th batch, train loss: 0.5265724658966064:  37%|████▋        | 88/241 [00:13<00:24,  6.33it/s]Epoch: 3, train for the 89-th batch, train loss: 0.5265724658966064:  37%|████▊        | 89/241 [00:13<00:24,  6.33it/s]Epoch: 6, train for the 27-th batch, train loss: 0.4744821786880493:  22%|██▊          | 26/119 [00:04<00:14,  6.44it/s]Epoch: 6, train for the 27-th batch, train loss: 0.4744821786880493:  23%|██▉          | 27/119 [00:04<00:14,  6.43it/s]Epoch: 5, train for the 59-th batch, train loss: 0.42982977628707886:  40%|████▊       | 58/146 [00:08<00:12,  6.77it/s]Epoch: 4, train for the 80-th batch, train loss: 0.5092806220054626:  52%|██████▊      | 79/151 [00:13<00:11,  6.11it/s]Epoch: 5, train for the 59-th batch, train loss: 0.42982977628707886:  40%|████▊       | 59/146 [00:08<00:13,  6.62it/s]Epoch: 4, train for the 80-th batch, train loss: 0.5092806220054626:  53%|██████▉      | 80/151 [00:13<00:11,  6.30it/s]Epoch: 2, train for the 211-th batch, train loss: 0.5522961616516113:  89%|█████████▋ | 210/237 [00:41<00:07,  3.83it/s]Epoch: 2, train for the 211-th batch, train loss: 0.5522961616516113:  89%|█████████▊ | 211/237 [00:41<00:06,  3.98it/s]Epoch: 3, train for the 90-th batch, train loss: 0.4751704931259155:  37%|████▊        | 89/241 [00:13<00:24,  6.33it/s]Epoch: 5, train for the 60-th batch, train loss: 0.5132729411125183:  40%|█████▎       | 59/146 [00:08<00:13,  6.62it/s]Epoch: 3, train for the 90-th batch, train loss: 0.4751704931259155:  37%|████▊        | 90/241 [00:13<00:23,  6.48it/s]Epoch: 5, train for the 60-th batch, train loss: 0.5132729411125183:  41%|█████▎       | 60/146 [00:08<00:11,  7.24it/s]Epoch: 4, train for the 81-th batch, train loss: 0.502701461315155:  53%|███████▍      | 80/151 [00:13<00:11,  6.30it/s]Epoch: 4, train for the 81-th batch, train loss: 0.502701461315155:  54%|███████▌      | 81/151 [00:13<00:10,  6.45it/s]Epoch: 2, train for the 212-th batch, train loss: 0.5742133855819702:  89%|█████████▊ | 211/237 [00:41<00:06,  3.98it/s]Epoch: 3, train for the 91-th batch, train loss: 0.4883744418621063:  37%|████▊        | 90/241 [00:13<00:23,  6.48it/s]Epoch: 2, train for the 212-th batch, train loss: 0.5742133855819702:  89%|█████████▊ | 212/237 [00:41<00:05,  4.33it/s]Epoch: 3, train for the 91-th batch, train loss: 0.4883744418621063:  38%|████▉        | 91/241 [00:13<00:22,  6.56it/s]Epoch: 4, train for the 82-th batch, train loss: 0.5280587077140808:  54%|██████▉      | 81/151 [00:13<00:10,  6.45it/s]Epoch: 6, train for the 28-th batch, train loss: 0.46385592222213745:  23%|██▋         | 27/119 [00:04<00:14,  6.43it/s]Epoch: 5, train for the 61-th batch, train loss: 0.5399323105812073:  41%|█████▎       | 60/146 [00:09<00:11,  7.24it/s]Epoch: 6, train for the 28-th batch, train loss: 0.46385592222213745:  24%|██▊         | 28/119 [00:04<00:18,  4.86it/s]Epoch: 5, train for the 61-th batch, train loss: 0.5399323105812073:  42%|█████▍       | 61/146 [00:09<00:13,  6.50it/s]Epoch: 4, train for the 82-th batch, train loss: 0.5280587077140808:  54%|███████      | 82/151 [00:13<00:10,  6.72it/s]Epoch: 3, train for the 92-th batch, train loss: 0.37047910690307617:  38%|████▌       | 91/241 [00:13<00:22,  6.56it/s]Epoch: 3, train for the 92-th batch, train loss: 0.37047910690307617:  38%|████▌       | 92/241 [00:13<00:21,  6.97it/s]Epoch: 2, train for the 213-th batch, train loss: 0.5950502157211304:  89%|█████████▊ | 212/237 [00:42<00:05,  4.33it/s]Epoch: 4, train for the 83-th batch, train loss: 0.5215634703636169:  54%|███████      | 82/151 [00:14<00:10,  6.72it/s]Epoch: 5, train for the 62-th batch, train loss: 0.4710986614227295:  42%|█████▍       | 61/146 [00:09<00:13,  6.50it/s]Epoch: 6, train for the 29-th batch, train loss: 0.4307843744754791:  24%|███          | 28/119 [00:04<00:18,  4.86it/s]Epoch: 5, train for the 62-th batch, train loss: 0.4710986614227295:  42%|█████▌       | 62/146 [00:09<00:13,  6.36it/s]Epoch: 6, train for the 29-th batch, train loss: 0.4307843744754791:  24%|███▏         | 29/119 [00:04<00:17,  5.14it/s]Epoch: 4, train for the 83-th batch, train loss: 0.5215634703636169:  55%|███████▏     | 83/151 [00:14<00:10,  6.49it/s]Epoch: 3, train for the 93-th batch, train loss: 0.4099999666213989:  38%|████▉        | 92/241 [00:13<00:21,  6.97it/s]Epoch: 2, train for the 213-th batch, train loss: 0.5950502157211304:  90%|█████████▉ | 213/237 [00:42<00:05,  4.37it/s]Epoch: 3, train for the 93-th batch, train loss: 0.4099999666213989:  39%|█████        | 93/241 [00:13<00:19,  7.49it/s]  0%|                                                                                           | 0/106 [00:00<?, ?it/s]Epoch: 5, train for the 63-th batch, train loss: 0.5180467367172241:  42%|█████▌       | 62/146 [00:09<00:13,  6.36it/s]Epoch: 5, train for the 63-th batch, train loss: 0.5180467367172241:  43%|█████▌       | 63/146 [00:09<00:12,  6.69it/s]Epoch: 6, train for the 30-th batch, train loss: 0.38749998807907104:  24%|██▉         | 29/119 [00:04<00:17,  5.14it/s]Epoch: 6, train for the 30-th batch, train loss: 0.38749998807907104:  25%|███         | 30/119 [00:04<00:16,  5.48it/s]Epoch: 4, train for the 84-th batch, train loss: 0.5202386975288391:  55%|███████▏     | 83/151 [00:14<00:10,  6.49it/s]Epoch: 3, train for the 94-th batch, train loss: 0.36636099219322205:  39%|████▋       | 93/241 [00:13<00:19,  7.49it/s]Epoch: 2, train for the 214-th batch, train loss: 0.5955890417098999:  90%|█████████▉ | 213/237 [00:42<00:05,  4.37it/s]Epoch: 4, train for the 84-th batch, train loss: 0.5202386975288391:  56%|███████▏     | 84/151 [00:14<00:11,  5.92it/s]Epoch: 3, train for the 94-th batch, train loss: 0.36636099219322205:  39%|████▋       | 94/241 [00:13<00:21,  6.73it/s]Epoch: 2, train for the 214-th batch, train loss: 0.5955890417098999:  90%|█████████▉ | 214/237 [00:42<00:05,  4.40it/s]Epoch: 5, train for the 64-th batch, train loss: 0.5195809602737427:  43%|█████▌       | 63/146 [00:09<00:12,  6.69it/s]Epoch: 5, train for the 64-th batch, train loss: 0.5195809602737427:  44%|█████▋       | 64/146 [00:09<00:12,  6.71it/s]Epoch: 6, train for the 31-th batch, train loss: 0.430005818605423:  25%|███▌          | 30/119 [00:05<00:16,  5.48it/s]Epoch: 6, train for the 31-th batch, train loss: 0.430005818605423:  26%|███▋          | 31/119 [00:05<00:14,  5.96it/s]Epoch: 2, train for the 215-th batch, train loss: 0.5663447976112366:  90%|█████████▉ | 214/237 [00:42<00:05,  4.40it/s]Epoch: 3, train for the 95-th batch, train loss: 0.40953120589256287:  39%|████▋       | 94/241 [00:14<00:21,  6.73it/s]Epoch: 4, train for the 85-th batch, train loss: 0.4591625928878784:  56%|███████▏     | 84/151 [00:14<00:11,  5.92it/s]Epoch: 3, train for the 95-th batch, train loss: 0.40953120589256287:  39%|████▋       | 95/241 [00:14<00:23,  6.20it/s]Epoch: 4, train for the 85-th batch, train loss: 0.4591625928878784:  56%|███████▎     | 85/151 [00:14<00:11,  5.59it/s]Epoch: 2, train for the 215-th batch, train loss: 0.5663447976112366:  91%|█████████▉ | 215/237 [00:42<00:04,  4.71it/s]Epoch: 6, train for the 32-th batch, train loss: 0.4492321312427521:  26%|███▍         | 31/119 [00:05<00:14,  5.96it/s]Epoch: 5, train for the 65-th batch, train loss: 0.5260476469993591:  44%|█████▋       | 64/146 [00:09<00:12,  6.71it/s]Epoch: 6, train for the 32-th batch, train loss: 0.4492321312427521:  27%|███▍         | 32/119 [00:05<00:14,  5.93it/s]Epoch: 5, train for the 65-th batch, train loss: 0.5260476469993591:  45%|█████▊       | 65/146 [00:09<00:13,  6.16it/s]evaluate for the 1-th batch, evaluate loss: 0.34983184933662415:   0%|                          | 0/106 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.34983184933662415:   1%|▏                 | 1/106 [00:00<00:44,  2.35it/s]Epoch: 3, train for the 96-th batch, train loss: 0.5255059003829956:  39%|█████        | 95/241 [00:14<00:23,  6.20it/s]Epoch: 3, train for the 96-th batch, train loss: 0.5255059003829956:  40%|█████▏       | 96/241 [00:14<00:22,  6.37it/s]Epoch: 4, train for the 86-th batch, train loss: 0.48699983954429626:  56%|██████▊     | 85/151 [00:14<00:11,  5.59it/s]Epoch: 4, train for the 86-th batch, train loss: 0.48699983954429626:  57%|██████▊     | 86/151 [00:14<00:11,  5.63it/s]Epoch: 6, train for the 33-th batch, train loss: 0.43347442150115967:  27%|███▏        | 32/119 [00:05<00:14,  5.93it/s]Epoch: 6, train for the 33-th batch, train loss: 0.43347442150115967:  28%|███▎        | 33/119 [00:05<00:13,  6.31it/s]Epoch: 2, train for the 216-th batch, train loss: 0.5785335898399353:  91%|█████████▉ | 215/237 [00:42<00:04,  4.71it/s]Epoch: 5, train for the 66-th batch, train loss: 0.5364006161689758:  45%|█████▊       | 65/146 [00:09<00:13,  6.16it/s]Epoch: 2, train for the 216-th batch, train loss: 0.5785335898399353:  91%|██████████ | 216/237 [00:42<00:04,  4.49it/s]Epoch: 5, train for the 66-th batch, train loss: 0.5364006161689758:  45%|█████▉       | 66/146 [00:09<00:13,  5.91it/s]Epoch: 3, train for the 97-th batch, train loss: 0.37240415811538696:  40%|████▊       | 96/241 [00:14<00:22,  6.37it/s]evaluate for the 2-th batch, evaluate loss: 0.4784840941429138:   1%|▏                  | 1/106 [00:00<00:44,  2.35it/s]evaluate for the 2-th batch, evaluate loss: 0.4784840941429138:   2%|▎                  | 2/106 [00:00<00:28,  3.70it/s]Epoch: 3, train for the 97-th batch, train loss: 0.37240415811538696:  40%|████▊       | 97/241 [00:14<00:21,  6.59it/s]Epoch: 6, train for the 34-th batch, train loss: 0.4221230745315552:  28%|███▌         | 33/119 [00:05<00:13,  6.31it/s]Epoch: 6, train for the 34-th batch, train loss: 0.4221230745315552:  29%|███▋         | 34/119 [00:05<00:12,  6.87it/s]Epoch: 4, train for the 87-th batch, train loss: 0.49837538599967957:  57%|██████▊     | 86/151 [00:14<00:11,  5.63it/s]Epoch: 4, train for the 87-th batch, train loss: 0.49837538599967957:  58%|██████▉     | 87/151 [00:14<00:11,  5.42it/s]Epoch: 5, train for the 67-th batch, train loss: 0.5503564476966858:  45%|█████▉       | 66/146 [00:10<00:13,  5.91it/s]Epoch: 3, train for the 98-th batch, train loss: 0.4641759395599365:  40%|█████▏       | 97/241 [00:14<00:21,  6.59it/s]Epoch: 5, train for the 67-th batch, train loss: 0.5503564476966858:  46%|█████▉       | 67/146 [00:10<00:13,  6.04it/s]Epoch: 3, train for the 98-th batch, train loss: 0.4641759395599365:  41%|█████▎       | 98/241 [00:14<00:21,  6.69it/s]Epoch: 6, train for the 35-th batch, train loss: 0.4096880257129669:  29%|███▋         | 34/119 [00:05<00:12,  6.87it/s]Epoch: 6, train for the 35-th batch, train loss: 0.4096880257129669:  29%|███▊         | 35/119 [00:05<00:12,  6.81it/s]Epoch: 4, train for the 88-th batch, train loss: 0.5252630710601807:  58%|███████▍     | 87/151 [00:15<00:11,  5.42it/s]Epoch: 4, train for the 88-th batch, train loss: 0.5252630710601807:  58%|███████▌     | 88/151 [00:15<00:10,  5.79it/s]Epoch: 5, train for the 68-th batch, train loss: 0.5491149425506592:  46%|█████▉       | 67/146 [00:10<00:13,  6.04it/s]Epoch: 5, train for the 68-th batch, train loss: 0.5491149425506592:  47%|██████       | 68/146 [00:10<00:12,  6.44it/s]evaluate for the 3-th batch, evaluate loss: 0.5432260036468506:   2%|▎                  | 2/106 [00:00<00:28,  3.70it/s]evaluate for the 3-th batch, evaluate loss: 0.5432260036468506:   3%|▌                  | 3/106 [00:00<00:28,  3.65it/s]Epoch: 6, train for the 36-th batch, train loss: 0.4008767902851105:  29%|███▊         | 35/119 [00:05<00:12,  6.81it/s]Epoch: 6, train for the 36-th batch, train loss: 0.4008767902851105:  30%|███▉         | 36/119 [00:05<00:12,  6.78it/s]Epoch: 2, train for the 217-th batch, train loss: 0.5538845658302307:  91%|██████████ | 216/237 [00:43<00:04,  4.49it/s]Epoch: 4, train for the 89-th batch, train loss: 0.5093472599983215:  58%|███████▌     | 88/151 [00:15<00:10,  5.79it/s]Epoch: 2, train for the 217-th batch, train loss: 0.5538845658302307:  92%|██████████ | 217/237 [00:43<00:05,  3.60it/s]Epoch: 4, train for the 89-th batch, train loss: 0.5093472599983215:  59%|███████▋     | 89/151 [00:15<00:10,  6.13it/s]Epoch: 5, train for the 69-th batch, train loss: 0.5323392152786255:  47%|██████       | 68/146 [00:10<00:12,  6.44it/s]Epoch: 3, train for the 99-th batch, train loss: 0.3127003312110901:  41%|█████▎       | 98/241 [00:14<00:21,  6.69it/s]Epoch: 5, train for the 69-th batch, train loss: 0.5323392152786255:  47%|██████▏      | 69/146 [00:10<00:11,  6.69it/s]Epoch: 3, train for the 99-th batch, train loss: 0.3127003312110901:  41%|█████▎       | 99/241 [00:14<00:26,  5.39it/s]evaluate for the 4-th batch, evaluate loss: 0.4389072358608246:   3%|▌                  | 3/106 [00:01<00:28,  3.65it/s]evaluate for the 4-th batch, evaluate loss: 0.4389072358608246:   4%|▋                  | 4/106 [00:01<00:22,  4.45it/s]Epoch: 6, train for the 37-th batch, train loss: 0.44784095883369446:  30%|███▋        | 36/119 [00:05<00:12,  6.78it/s]Epoch: 6, train for the 37-th batch, train loss: 0.44784095883369446:  31%|███▋        | 37/119 [00:05<00:11,  6.86it/s]Epoch: 5, train for the 70-th batch, train loss: 0.5195151567459106:  47%|██████▏      | 69/146 [00:10<00:11,  6.69it/s]Epoch: 5, train for the 70-th batch, train loss: 0.5195151567459106:  48%|██████▏      | 70/146 [00:10<00:10,  7.10it/s]Epoch: 4, train for the 90-th batch, train loss: 0.5344968438148499:  59%|███████▋     | 89/151 [00:15<00:10,  6.13it/s]Epoch: 3, train for the 100-th batch, train loss: 0.5065845847129822:  41%|████▉       | 99/241 [00:15<00:26,  5.39it/s]Epoch: 4, train for the 90-th batch, train loss: 0.5344968438148499:  60%|███████▋     | 90/151 [00:15<00:10,  5.60it/s]Epoch: 3, train for the 100-th batch, train loss: 0.5065845847129822:  41%|████▌      | 100/241 [00:15<00:26,  5.39it/s]Epoch: 6, train for the 38-th batch, train loss: 0.41670477390289307:  31%|███▋        | 37/119 [00:06<00:11,  6.86it/s]Epoch: 2, train for the 218-th batch, train loss: 0.5596281290054321:  92%|██████████ | 217/237 [00:43<00:05,  3.60it/s]evaluate for the 5-th batch, evaluate loss: 0.42878928780555725:   4%|▋                 | 4/106 [00:01<00:22,  4.45it/s]evaluate for the 5-th batch, evaluate loss: 0.42878928780555725:   5%|▊                 | 5/106 [00:01<00:21,  4.63it/s]Epoch: 6, train for the 38-th batch, train loss: 0.41670477390289307:  32%|███▊        | 38/119 [00:06<00:12,  6.71it/s]Epoch: 5, train for the 71-th batch, train loss: 0.5505265593528748:  48%|██████▏      | 70/146 [00:10<00:10,  7.10it/s]Epoch: 2, train for the 218-th batch, train loss: 0.5596281290054321:  92%|██████████ | 218/237 [00:43<00:05,  3.61it/s]Epoch: 5, train for the 71-th batch, train loss: 0.5505265593528748:  49%|██████▎      | 71/146 [00:10<00:10,  7.16it/s]evaluate for the 6-th batch, evaluate loss: 0.46420469880104065:   5%|▊                 | 5/106 [00:01<00:21,  4.63it/s]evaluate for the 6-th batch, evaluate loss: 0.46420469880104065:   6%|█                 | 6/106 [00:01<00:19,  5.21it/s]Epoch: 4, train for the 91-th batch, train loss: 0.4837058186531067:  60%|███████▋     | 90/151 [00:15<00:10,  5.60it/s]Epoch: 6, train for the 39-th batch, train loss: 0.45500802993774414:  32%|███▊        | 38/119 [00:06<00:12,  6.71it/s]Epoch: 3, train for the 101-th batch, train loss: 0.4723668694496155:  41%|████▌      | 100/241 [00:15<00:26,  5.39it/s]Epoch: 6, train for the 39-th batch, train loss: 0.45500802993774414:  33%|███▉        | 39/119 [00:06<00:12,  6.44it/s]Epoch: 4, train for the 91-th batch, train loss: 0.4837058186531067:  60%|███████▊     | 91/151 [00:15<00:11,  5.42it/s]Epoch: 5, train for the 72-th batch, train loss: 0.5803642868995667:  49%|██████▎      | 71/146 [00:10<00:10,  7.16it/s]Epoch: 3, train for the 101-th batch, train loss: 0.4723668694496155:  42%|████▌      | 101/241 [00:15<00:27,  5.11it/s]Epoch: 5, train for the 72-th batch, train loss: 0.5803642868995667:  49%|██████▍      | 72/146 [00:10<00:10,  6.74it/s]Epoch: 2, train for the 219-th batch, train loss: 0.5776089429855347:  92%|██████████ | 218/237 [00:43<00:05,  3.61it/s]Epoch: 6, train for the 40-th batch, train loss: 0.43527284264564514:  33%|███▉        | 39/119 [00:06<00:12,  6.44it/s]evaluate for the 7-th batch, evaluate loss: 0.5228886008262634:   6%|█                  | 6/106 [00:01<00:19,  5.21it/s]evaluate for the 7-th batch, evaluate loss: 0.5228886008262634:   7%|█▎                 | 7/106 [00:01<00:17,  5.66it/s]Epoch: 6, train for the 40-th batch, train loss: 0.43527284264564514:  34%|████        | 40/119 [00:06<00:11,  6.99it/s]Epoch: 2, train for the 219-th batch, train loss: 0.5776089429855347:  92%|██████████▏| 219/237 [00:43<00:04,  3.67it/s]Epoch: 4, train for the 92-th batch, train loss: 0.5103219747543335:  60%|███████▊     | 91/151 [00:15<00:11,  5.42it/s]Epoch: 4, train for the 92-th batch, train loss: 0.5103219747543335:  61%|███████▉     | 92/151 [00:15<00:10,  5.63it/s]Epoch: 5, train for the 73-th batch, train loss: 0.5616721510887146:  49%|██████▍      | 72/146 [00:10<00:10,  6.74it/s]Epoch: 3, train for the 102-th batch, train loss: 0.5424910187721252:  42%|████▌      | 101/241 [00:15<00:27,  5.11it/s]Epoch: 5, train for the 73-th batch, train loss: 0.5616721510887146:  50%|██████▌      | 73/146 [00:10<00:11,  6.61it/s]Epoch: 3, train for the 102-th batch, train loss: 0.5424910187721252:  42%|████▋      | 102/241 [00:15<00:26,  5.22it/s]Epoch: 6, train for the 41-th batch, train loss: 0.42547622323036194:  34%|████        | 40/119 [00:06<00:11,  6.99it/s]Epoch: 6, train for the 41-th batch, train loss: 0.42547622323036194:  34%|████▏       | 41/119 [00:06<00:11,  7.02it/s]evaluate for the 8-th batch, evaluate loss: 0.3931196630001068:   7%|█▎                 | 7/106 [00:01<00:17,  5.66it/s]evaluate for the 8-th batch, evaluate loss: 0.3931196630001068:   8%|█▍                 | 8/106 [00:01<00:17,  5.52it/s]Epoch: 4, train for the 93-th batch, train loss: 0.5149245858192444:  61%|███████▉     | 92/151 [00:15<00:10,  5.63it/s]Epoch: 4, train for the 93-th batch, train loss: 0.5149245858192444:  62%|████████     | 93/151 [00:15<00:10,  5.80it/s]Epoch: 2, train for the 220-th batch, train loss: 0.583859384059906:  92%|███████████ | 219/237 [00:43<00:04,  3.67it/s]Epoch: 3, train for the 103-th batch, train loss: 0.47963178157806396:  42%|████▏     | 102/241 [00:15<00:26,  5.22it/s]Epoch: 6, train for the 42-th batch, train loss: 0.4659733474254608:  34%|████▍        | 41/119 [00:06<00:11,  7.02it/s]Epoch: 6, train for the 42-th batch, train loss: 0.4659733474254608:  35%|████▌        | 42/119 [00:06<00:10,  7.63it/s]Epoch: 3, train for the 103-th batch, train loss: 0.47963178157806396:  43%|████▎     | 103/241 [00:15<00:24,  5.53it/s]Epoch: 2, train for the 220-th batch, train loss: 0.583859384059906:  93%|███████████▏| 220/237 [00:43<00:04,  3.72it/s]Epoch: 5, train for the 74-th batch, train loss: 0.5247476696968079:  50%|██████▌      | 73/146 [00:11<00:11,  6.61it/s]Epoch: 5, train for the 74-th batch, train loss: 0.5247476696968079:  51%|██████▌      | 74/146 [00:11<00:12,  5.64it/s]Epoch: 4, train for the 94-th batch, train loss: 0.5125105977058411:  62%|████████     | 93/151 [00:16<00:10,  5.80it/s]evaluate for the 9-th batch, evaluate loss: 0.4599457383155823:   8%|█▍                 | 8/106 [00:01<00:17,  5.52it/s]evaluate for the 9-th batch, evaluate loss: 0.4599457383155823:   8%|█▌                 | 9/106 [00:01<00:16,  5.79it/s]Epoch: 4, train for the 94-th batch, train loss: 0.5125105977058411:  62%|████████     | 94/151 [00:16<00:09,  6.11it/s]Epoch: 6, train for the 43-th batch, train loss: 0.47063058614730835:  35%|████▏       | 42/119 [00:06<00:10,  7.63it/s]Epoch: 3, train for the 104-th batch, train loss: 0.5256226062774658:  43%|████▋      | 103/241 [00:15<00:24,  5.53it/s]Epoch: 6, train for the 43-th batch, train loss: 0.47063058614730835:  36%|████▎       | 43/119 [00:06<00:10,  7.29it/s]Epoch: 3, train for the 104-th batch, train loss: 0.5256226062774658:  43%|████▋      | 104/241 [00:15<00:24,  5.65it/s]Epoch: 5, train for the 75-th batch, train loss: 0.5287323594093323:  51%|██████▌      | 74/146 [00:11<00:12,  5.64it/s]Epoch: 5, train for the 75-th batch, train loss: 0.5287323594093323:  51%|██████▋      | 75/146 [00:11<00:11,  6.17it/s]Epoch: 2, train for the 221-th batch, train loss: 0.5681942701339722:  93%|██████████▏| 220/237 [00:44<00:04,  3.72it/s]Epoch: 4, train for the 95-th batch, train loss: 0.489644855260849:  62%|████████▋     | 94/151 [00:16<00:09,  6.11it/s]evaluate for the 10-th batch, evaluate loss: 0.3807533085346222:   8%|█▌                | 9/106 [00:02<00:16,  5.79it/s]evaluate for the 10-th batch, evaluate loss: 0.3807533085346222:   9%|█▌               | 10/106 [00:02<00:16,  5.98it/s]Epoch: 4, train for the 95-th batch, train loss: 0.489644855260849:  63%|████████▊     | 95/151 [00:16<00:09,  6.18it/s]Epoch: 2, train for the 221-th batch, train loss: 0.5681942701339722:  93%|██████████▎| 221/237 [00:44<00:04,  3.79it/s]Epoch: 6, train for the 44-th batch, train loss: 0.40594515204429626:  36%|████▎       | 43/119 [00:06<00:10,  7.29it/s]Epoch: 6, train for the 44-th batch, train loss: 0.40594515204429626:  37%|████▍       | 44/119 [00:06<00:10,  6.97it/s]Epoch: 3, train for the 105-th batch, train loss: 0.471789687871933:  43%|█████▏      | 104/241 [00:15<00:24,  5.65it/s]Epoch: 5, train for the 76-th batch, train loss: 0.5388264060020447:  51%|██████▋      | 75/146 [00:11<00:11,  6.17it/s]Epoch: 3, train for the 105-th batch, train loss: 0.471789687871933:  44%|█████▏      | 105/241 [00:15<00:23,  5.68it/s]Epoch: 5, train for the 76-th batch, train loss: 0.5388264060020447:  52%|██████▊      | 76/146 [00:11<00:11,  6.28it/s]Epoch: 4, train for the 96-th batch, train loss: 0.5446025133132935:  63%|████████▏    | 95/151 [00:16<00:09,  6.18it/s]Epoch: 6, train for the 45-th batch, train loss: 0.38984137773513794:  37%|████▍       | 44/119 [00:07<00:10,  6.97it/s]Epoch: 6, train for the 45-th batch, train loss: 0.38984137773513794:  38%|████▌       | 45/119 [00:07<00:10,  7.11it/s]Epoch: 4, train for the 96-th batch, train loss: 0.5446025133132935:  64%|████████▎    | 96/151 [00:16<00:09,  5.93it/s]Epoch: 3, train for the 106-th batch, train loss: 0.5278676748275757:  44%|████▊      | 105/241 [00:16<00:23,  5.68it/s]Epoch: 2, train for the 222-th batch, train loss: 0.6122770309448242:  93%|██████████▎| 221/237 [00:44<00:04,  3.79it/s]Epoch: 5, train for the 77-th batch, train loss: 0.5913552045822144:  52%|██████▊      | 76/146 [00:11<00:11,  6.28it/s]Epoch: 5, train for the 77-th batch, train loss: 0.5913552045822144:  53%|██████▊      | 77/146 [00:11<00:10,  6.36it/s]Epoch: 3, train for the 106-th batch, train loss: 0.5278676748275757:  44%|████▊      | 106/241 [00:16<00:22,  5.89it/s]evaluate for the 11-th batch, evaluate loss: 0.32860976457595825:   9%|█▌              | 10/106 [00:02<00:16,  5.98it/s]evaluate for the 11-th batch, evaluate loss: 0.32860976457595825:  10%|█▋              | 11/106 [00:02<00:18,  5.06it/s]Epoch: 2, train for the 222-th batch, train loss: 0.6122770309448242:  94%|██████████▎| 222/237 [00:44<00:03,  3.87it/s]Epoch: 6, train for the 46-th batch, train loss: 0.42505690455436707:  38%|████▌       | 45/119 [00:07<00:10,  7.11it/s]Epoch: 6, train for the 46-th batch, train loss: 0.42505690455436707:  39%|████▋       | 46/119 [00:07<00:09,  7.43it/s]Epoch: 4, train for the 97-th batch, train loss: 0.5530189275741577:  64%|████████▎    | 96/151 [00:16<00:09,  5.93it/s]Epoch: 4, train for the 97-th batch, train loss: 0.5530189275741577:  64%|████████▎    | 97/151 [00:16<00:09,  5.99it/s]Epoch: 5, train for the 78-th batch, train loss: 0.5721956491470337:  53%|██████▊      | 77/146 [00:11<00:10,  6.36it/s]Epoch: 5, train for the 78-th batch, train loss: 0.5721956491470337:  53%|██████▉      | 78/146 [00:11<00:10,  6.56it/s]Epoch: 3, train for the 107-th batch, train loss: 0.5757543444633484:  44%|████▊      | 106/241 [00:16<00:22,  5.89it/s]Epoch: 6, train for the 47-th batch, train loss: 0.473438024520874:  39%|█████▍        | 46/119 [00:07<00:09,  7.43it/s]Epoch: 3, train for the 107-th batch, train loss: 0.5757543444633484:  44%|████▉      | 107/241 [00:16<00:23,  5.76it/s]Epoch: 6, train for the 47-th batch, train loss: 0.473438024520874:  39%|█████▌        | 47/119 [00:07<00:09,  7.62it/s]evaluate for the 12-th batch, evaluate loss: 0.32402151823043823:  10%|█▋              | 11/106 [00:02<00:18,  5.06it/s]evaluate for the 12-th batch, evaluate loss: 0.32402151823043823:  11%|█▊              | 12/106 [00:02<00:18,  5.21it/s]Epoch: 2, train for the 223-th batch, train loss: 0.5429417490959167:  94%|██████████▎| 222/237 [00:44<00:03,  3.87it/s]Epoch: 4, train for the 98-th batch, train loss: 0.5191353559494019:  64%|████████▎    | 97/151 [00:16<00:09,  5.99it/s]Epoch: 5, train for the 79-th batch, train loss: 0.5541242361068726:  53%|██████▉      | 78/146 [00:11<00:10,  6.56it/s]Epoch: 4, train for the 98-th batch, train loss: 0.5191353559494019:  65%|████████▍    | 98/151 [00:16<00:08,  6.15it/s]Epoch: 5, train for the 79-th batch, train loss: 0.5541242361068726:  54%|███████      | 79/146 [00:11<00:09,  6.90it/s]Epoch: 2, train for the 223-th batch, train loss: 0.5429417490959167:  94%|██████████▎| 223/237 [00:44<00:03,  3.89it/s]Epoch: 3, train for the 108-th batch, train loss: 0.5153830051422119:  44%|████▉      | 107/241 [00:16<00:23,  5.76it/s]Epoch: 6, train for the 48-th batch, train loss: 0.4624091684818268:  39%|█████▏       | 47/119 [00:07<00:09,  7.62it/s]Epoch: 6, train for the 48-th batch, train loss: 0.4624091684818268:  40%|█████▏       | 48/119 [00:07<00:09,  7.51it/s]Epoch: 3, train for the 108-th batch, train loss: 0.5153830051422119:  45%|████▉      | 108/241 [00:16<00:21,  6.05it/s]Epoch: 5, train for the 80-th batch, train loss: 0.527209460735321:  54%|███████▌      | 79/146 [00:11<00:09,  6.90it/s]Epoch: 5, train for the 80-th batch, train loss: 0.527209460735321:  55%|███████▋      | 80/146 [00:11<00:09,  7.05it/s]Epoch: 4, train for the 99-th batch, train loss: 0.5509262084960938:  65%|████████▍    | 98/151 [00:16<00:08,  6.15it/s]Epoch: 2, train for the 224-th batch, train loss: 0.5766996145248413:  94%|██████████▎| 223/237 [00:44<00:03,  3.89it/s]Epoch: 4, train for the 99-th batch, train loss: 0.5509262084960938:  66%|████████▌    | 99/151 [00:16<00:08,  6.09it/s]Epoch: 3, train for the 109-th batch, train loss: 0.672789990901947:  45%|█████▍      | 108/241 [00:16<00:21,  6.05it/s]Epoch: 6, train for the 49-th batch, train loss: 0.42282015085220337:  40%|████▊       | 48/119 [00:07<00:09,  7.51it/s]Epoch: 2, train for the 224-th batch, train loss: 0.5766996145248413:  95%|██████████▍| 224/237 [00:44<00:03,  4.24it/s]Epoch: 3, train for the 109-th batch, train loss: 0.672789990901947:  45%|█████▍      | 109/241 [00:16<00:20,  6.38it/s]Epoch: 6, train for the 49-th batch, train loss: 0.42282015085220337:  41%|████▉       | 49/119 [00:07<00:09,  7.39it/s]evaluate for the 13-th batch, evaluate loss: 0.3319151997566223:  11%|█▉               | 12/106 [00:02<00:18,  5.21it/s]evaluate for the 13-th batch, evaluate loss: 0.3319151997566223:  12%|██               | 13/106 [00:02<00:21,  4.39it/s]Epoch: 5, train for the 81-th batch, train loss: 0.5105900168418884:  55%|███████      | 80/146 [00:12<00:09,  7.05it/s]Epoch: 5, train for the 81-th batch, train loss: 0.5105900168418884:  55%|███████▏     | 81/146 [00:12<00:08,  7.27it/s]Epoch: 6, train for the 50-th batch, train loss: 0.4111194908618927:  41%|█████▎       | 49/119 [00:07<00:09,  7.39it/s]Epoch: 4, train for the 100-th batch, train loss: 0.5913708209991455:  66%|███████▊    | 99/151 [00:17<00:08,  6.09it/s]Epoch: 6, train for the 50-th batch, train loss: 0.4111194908618927:  42%|█████▍       | 50/119 [00:07<00:09,  7.40it/s]Epoch: 4, train for the 100-th batch, train loss: 0.5913708209991455:  66%|███████▎   | 100/151 [00:17<00:08,  5.83it/s]Epoch: 3, train for the 110-th batch, train loss: 0.5697084665298462:  45%|████▉      | 109/241 [00:16<00:20,  6.38it/s]Epoch: 3, train for the 110-th batch, train loss: 0.5697084665298462:  46%|█████      | 110/241 [00:16<00:21,  6.09it/s]Epoch: 5, train for the 82-th batch, train loss: 0.5115804672241211:  55%|███████▏     | 81/146 [00:12<00:08,  7.27it/s]Epoch: 5, train for the 82-th batch, train loss: 0.5115804672241211:  56%|███████▎     | 82/146 [00:12<00:08,  7.31it/s]Epoch: 2, train for the 225-th batch, train loss: 0.5619296431541443:  95%|██████████▍| 224/237 [00:45<00:03,  4.24it/s]evaluate for the 14-th batch, evaluate loss: 0.5124692916870117:  12%|██               | 13/106 [00:02<00:21,  4.39it/s]evaluate for the 14-th batch, evaluate loss: 0.5124692916870117:  13%|██▏              | 14/106 [00:02<00:20,  4.58it/s]Epoch: 6, train for the 51-th batch, train loss: 0.49415725469589233:  42%|█████       | 50/119 [00:07<00:09,  7.40it/s]Epoch: 6, train for the 51-th batch, train loss: 0.49415725469589233:  43%|█████▏      | 51/119 [00:07<00:09,  7.34it/s]Epoch: 2, train for the 225-th batch, train loss: 0.5619296431541443:  95%|██████████▍| 225/237 [00:45<00:03,  3.96it/s]Epoch: 4, train for the 101-th batch, train loss: 0.5887046456336975:  66%|███████▎   | 100/151 [00:17<00:08,  5.83it/s]Epoch: 4, train for the 101-th batch, train loss: 0.5887046456336975:  67%|███████▎   | 101/151 [00:17<00:08,  5.91it/s]Epoch: 3, train for the 111-th batch, train loss: 0.230978325009346:  46%|█████▍      | 110/241 [00:16<00:21,  6.09it/s]Epoch: 5, train for the 83-th batch, train loss: 0.5259128212928772:  56%|███████▎     | 82/146 [00:12<00:08,  7.31it/s]Epoch: 5, train for the 83-th batch, train loss: 0.5259128212928772:  57%|███████▍     | 83/146 [00:12<00:08,  7.26it/s]Epoch: 3, train for the 111-th batch, train loss: 0.230978325009346:  46%|█████▌      | 111/241 [00:16<00:21,  6.12it/s]Epoch: 6, train for the 52-th batch, train loss: 0.4572383761405945:  43%|█████▌       | 51/119 [00:07<00:09,  7.34it/s]Epoch: 6, train for the 52-th batch, train loss: 0.4572383761405945:  44%|█████▋       | 52/119 [00:07<00:09,  7.27it/s]Epoch: 2, train for the 226-th batch, train loss: 0.5591664910316467:  95%|██████████▍| 225/237 [00:45<00:03,  3.96it/s]Epoch: 5, train for the 84-th batch, train loss: 0.5078186392784119:  57%|███████▍     | 83/146 [00:12<00:08,  7.26it/s]Epoch: 5, train for the 84-th batch, train loss: 0.5078186392784119:  58%|███████▍     | 84/146 [00:12<00:08,  7.14it/s]evaluate for the 15-th batch, evaluate loss: 0.47784727811813354:  13%|██              | 14/106 [00:03<00:20,  4.58it/s]evaluate for the 15-th batch, evaluate loss: 0.47784727811813354:  14%|██▎             | 15/106 [00:03<00:20,  4.39it/s]Epoch: 2, train for the 226-th batch, train loss: 0.5591664910316467:  95%|██████████▍| 226/237 [00:45<00:02,  4.14it/s]Epoch: 3, train for the 112-th batch, train loss: 0.29173603653907776:  46%|████▌     | 111/241 [00:17<00:21,  6.12it/s]Epoch: 4, train for the 102-th batch, train loss: 0.5136891603469849:  67%|███████▎   | 101/151 [00:17<00:08,  5.91it/s]Epoch: 3, train for the 112-th batch, train loss: 0.29173603653907776:  46%|████▋     | 112/241 [00:17<00:21,  5.96it/s]Epoch: 6, train for the 53-th batch, train loss: 0.4410531520843506:  44%|█████▋       | 52/119 [00:08<00:09,  7.27it/s]Epoch: 6, train for the 53-th batch, train loss: 0.4410531520843506:  45%|█████▊       | 53/119 [00:08<00:08,  7.45it/s]Epoch: 4, train for the 102-th batch, train loss: 0.5136891603469849:  68%|███████▍   | 102/151 [00:17<00:09,  5.32it/s]Epoch: 5, train for the 85-th batch, train loss: 0.5114266276359558:  58%|███████▍     | 84/146 [00:12<00:08,  7.14it/s]Epoch: 5, train for the 85-th batch, train loss: 0.5114266276359558:  58%|███████▌     | 85/146 [00:12<00:08,  7.10it/s]evaluate for the 16-th batch, evaluate loss: 0.43086084723472595:  14%|██▎             | 15/106 [00:03<00:20,  4.39it/s]evaluate for the 16-th batch, evaluate loss: 0.43086084723472595:  15%|██▍             | 16/106 [00:03<00:18,  4.84it/s]Epoch: 3, train for the 113-th batch, train loss: 0.40578535199165344:  46%|████▋     | 112/241 [00:17<00:21,  5.96it/s]Epoch: 3, train for the 113-th batch, train loss: 0.40578535199165344:  47%|████▋     | 113/241 [00:17<00:20,  6.10it/s]Epoch: 6, train for the 54-th batch, train loss: 0.41922110319137573:  45%|█████▎      | 53/119 [00:08<00:08,  7.45it/s]Epoch: 6, train for the 54-th batch, train loss: 0.41922110319137573:  45%|█████▍      | 54/119 [00:08<00:09,  7.02it/s]Epoch: 4, train for the 103-th batch, train loss: 0.5609650611877441:  68%|███████▍   | 102/151 [00:17<00:09,  5.32it/s]Epoch: 2, train for the 227-th batch, train loss: 0.5449387431144714:  95%|██████████▍| 226/237 [00:45<00:02,  4.14it/s]Epoch: 4, train for the 103-th batch, train loss: 0.5609650611877441:  68%|███████▌   | 103/151 [00:17<00:08,  5.36it/s]Epoch: 5, train for the 86-th batch, train loss: 0.527818500995636:  58%|████████▏     | 85/146 [00:12<00:08,  7.10it/s]Epoch: 5, train for the 86-th batch, train loss: 0.527818500995636:  59%|████████▏     | 86/146 [00:12<00:08,  7.21it/s]Epoch: 2, train for the 227-th batch, train loss: 0.5449387431144714:  96%|██████████▌| 227/237 [00:45<00:02,  3.94it/s]Epoch: 3, train for the 114-th batch, train loss: 0.36736708879470825:  47%|████▋     | 113/241 [00:17<00:20,  6.10it/s]Epoch: 6, train for the 55-th batch, train loss: 0.419354647397995:  45%|██████▎       | 54/119 [00:08<00:09,  7.02it/s]Epoch: 6, train for the 55-th batch, train loss: 0.419354647397995:  46%|██████▍       | 55/119 [00:08<00:08,  7.23it/s]Epoch: 3, train for the 114-th batch, train loss: 0.36736708879470825:  47%|████▋     | 114/241 [00:17<00:20,  6.15it/s]Epoch: 4, train for the 104-th batch, train loss: 0.5792697668075562:  68%|███████▌   | 103/151 [00:17<00:08,  5.36it/s]Epoch: 5, train for the 87-th batch, train loss: 0.5201208591461182:  59%|███████▋     | 86/146 [00:12<00:08,  7.21it/s]Epoch: 5, train for the 87-th batch, train loss: 0.5201208591461182:  60%|███████▋     | 87/146 [00:12<00:08,  7.23it/s]Epoch: 4, train for the 104-th batch, train loss: 0.5792697668075562:  69%|███████▌   | 104/151 [00:17<00:08,  5.56it/s]Epoch: 6, train for the 56-th batch, train loss: 0.43853071331977844:  46%|█████▌      | 55/119 [00:08<00:08,  7.23it/s]Epoch: 3, train for the 115-th batch, train loss: 0.38049596548080444:  47%|████▋     | 114/241 [00:17<00:20,  6.15it/s]Epoch: 6, train for the 56-th batch, train loss: 0.43853071331977844:  47%|█████▋      | 56/119 [00:08<00:08,  7.23it/s]Epoch: 3, train for the 115-th batch, train loss: 0.38049596548080444:  48%|████▊     | 115/241 [00:17<00:19,  6.35it/s]Epoch: 2, train for the 228-th batch, train loss: 0.5592979192733765:  96%|██████████▌| 227/237 [00:45<00:02,  3.94it/s]evaluate for the 17-th batch, evaluate loss: 0.40427812933921814:  15%|██▍             | 16/106 [00:03<00:18,  4.84it/s]evaluate for the 17-th batch, evaluate loss: 0.40427812933921814:  16%|██▌             | 17/106 [00:03<00:22,  3.93it/s]Epoch: 5, train for the 88-th batch, train loss: 0.535560131072998:  60%|████████▎     | 87/146 [00:13<00:08,  7.23it/s]Epoch: 4, train for the 105-th batch, train loss: 0.494977205991745:  69%|████████▎   | 104/151 [00:17<00:08,  5.56it/s]Epoch: 5, train for the 88-th batch, train loss: 0.535560131072998:  60%|████████▍     | 88/146 [00:13<00:08,  7.09it/s]Epoch: 2, train for the 228-th batch, train loss: 0.5592979192733765:  96%|██████████▌| 228/237 [00:45<00:02,  3.89it/s]Epoch: 4, train for the 105-th batch, train loss: 0.494977205991745:  70%|████████▎   | 105/151 [00:18<00:08,  5.72it/s]Epoch: 3, train for the 116-th batch, train loss: 0.5204628109931946:  48%|█████▏     | 115/241 [00:17<00:19,  6.35it/s]Epoch: 6, train for the 57-th batch, train loss: 0.40228354930877686:  47%|█████▋      | 56/119 [00:08<00:08,  7.23it/s]Epoch: 3, train for the 116-th batch, train loss: 0.5204628109931946:  48%|█████▎     | 116/241 [00:17<00:18,  6.67it/s]Epoch: 6, train for the 57-th batch, train loss: 0.40228354930877686:  48%|█████▋      | 57/119 [00:08<00:08,  7.12it/s]Epoch: 5, train for the 89-th batch, train loss: 0.5017791986465454:  60%|███████▊     | 88/146 [00:13<00:08,  7.09it/s]Epoch: 5, train for the 89-th batch, train loss: 0.5017791986465454:  61%|███████▉     | 89/146 [00:13<00:08,  6.89it/s]evaluate for the 18-th batch, evaluate loss: 0.4173525869846344:  16%|██▋              | 17/106 [00:03<00:22,  3.93it/s]evaluate for the 18-th batch, evaluate loss: 0.4173525869846344:  17%|██▉              | 18/106 [00:03<00:20,  4.25it/s]Epoch: 4, train for the 106-th batch, train loss: 0.5141814351081848:  70%|███████▋   | 105/151 [00:18<00:08,  5.72it/s]Epoch: 4, train for the 106-th batch, train loss: 0.5141814351081848:  70%|███████▋   | 106/151 [00:18<00:07,  5.74it/s]Epoch: 3, train for the 117-th batch, train loss: 0.6155242323875427:  48%|█████▎     | 116/241 [00:17<00:18,  6.67it/s]Epoch: 6, train for the 58-th batch, train loss: 0.40013211965560913:  48%|█████▋      | 57/119 [00:08<00:08,  7.12it/s]Epoch: 3, train for the 117-th batch, train loss: 0.6155242323875427:  49%|█████▎     | 117/241 [00:17<00:18,  6.73it/s]Epoch: 6, train for the 58-th batch, train loss: 0.40013211965560913:  49%|█████▊      | 58/119 [00:08<00:08,  6.98it/s]Epoch: 2, train for the 229-th batch, train loss: 0.5966957807540894:  96%|██████████▌| 228/237 [00:46<00:02,  3.89it/s]Epoch: 5, train for the 90-th batch, train loss: 0.5652095675468445:  61%|███████▉     | 89/146 [00:13<00:08,  6.89it/s]Epoch: 5, train for the 90-th batch, train loss: 0.5652095675468445:  62%|████████     | 90/146 [00:13<00:07,  7.23it/s]Epoch: 2, train for the 229-th batch, train loss: 0.5966957807540894:  97%|██████████▋| 229/237 [00:46<00:02,  3.82it/s]Epoch: 4, train for the 107-th batch, train loss: 0.4623538851737976:  70%|███████▋   | 106/151 [00:18<00:07,  5.74it/s]Epoch: 6, train for the 59-th batch, train loss: 0.4408542513847351:  49%|██████▎      | 58/119 [00:08<00:08,  6.98it/s]Epoch: 6, train for the 59-th batch, train loss: 0.4408542513847351:  50%|██████▍      | 59/119 [00:08<00:08,  6.92it/s]Epoch: 4, train for the 107-th batch, train loss: 0.4623538851737976:  71%|███████▊   | 107/151 [00:18<00:07,  5.70it/s]Epoch: 3, train for the 118-th batch, train loss: 0.4629523456096649:  49%|█████▎     | 117/241 [00:17<00:18,  6.73it/s]evaluate for the 19-th batch, evaluate loss: 0.2525497078895569:  17%|██▉              | 18/106 [00:04<00:20,  4.25it/s]evaluate for the 19-th batch, evaluate loss: 0.2525497078895569:  18%|███              | 19/106 [00:04<00:20,  4.26it/s]Epoch: 3, train for the 118-th batch, train loss: 0.4629523456096649:  49%|█████▍     | 118/241 [00:18<00:19,  6.26it/s]Epoch: 5, train for the 91-th batch, train loss: 0.5283727645874023:  62%|████████     | 90/146 [00:13<00:07,  7.23it/s]Epoch: 5, train for the 91-th batch, train loss: 0.5283727645874023:  62%|████████     | 91/146 [00:13<00:07,  7.06it/s]Epoch: 2, train for the 230-th batch, train loss: 0.5792164206504822:  97%|██████████▋| 229/237 [00:46<00:02,  3.82it/s]Epoch: 2, train for the 230-th batch, train loss: 0.5792164206504822:  97%|██████████▋| 230/237 [00:46<00:01,  4.10it/s]Epoch: 6, train for the 60-th batch, train loss: 0.41719871759414673:  50%|█████▉      | 59/119 [00:09<00:08,  6.92it/s]Epoch: 6, train for the 60-th batch, train loss: 0.41719871759414673:  50%|██████      | 60/119 [00:09<00:08,  7.02it/s]Epoch: 4, train for the 108-th batch, train loss: 0.4869595170021057:  71%|███████▊   | 107/151 [00:18<00:07,  5.70it/s]Epoch: 5, train for the 92-th batch, train loss: 0.5442472100257874:  62%|████████     | 91/146 [00:13<00:07,  7.06it/s]Epoch: 3, train for the 119-th batch, train loss: 0.36754852533340454:  49%|████▉     | 118/241 [00:18<00:19,  6.26it/s]Epoch: 4, train for the 108-th batch, train loss: 0.4869595170021057:  72%|███████▊   | 108/151 [00:18<00:07,  5.66it/s]Epoch: 5, train for the 92-th batch, train loss: 0.5442472100257874:  63%|████████▏    | 92/146 [00:13<00:07,  7.28it/s]evaluate for the 20-th batch, evaluate loss: 0.28207969665527344:  18%|██▊             | 19/106 [00:04<00:20,  4.26it/s]evaluate for the 20-th batch, evaluate loss: 0.28207969665527344:  19%|███             | 20/106 [00:04<00:18,  4.66it/s]Epoch: 3, train for the 119-th batch, train loss: 0.36754852533340454:  49%|████▉     | 119/241 [00:18<00:19,  6.12it/s]Epoch: 2, train for the 231-th batch, train loss: 0.6165115237236023:  97%|██████████▋| 230/237 [00:46<00:01,  4.10it/s]Epoch: 6, train for the 61-th batch, train loss: 0.4312889873981476:  50%|██████▌      | 60/119 [00:09<00:08,  7.02it/s]Epoch: 6, train for the 61-th batch, train loss: 0.4312889873981476:  51%|██████▋      | 61/119 [00:09<00:09,  6.43it/s]Epoch: 5, train for the 93-th batch, train loss: 0.5211366415023804:  63%|████████▏    | 92/146 [00:13<00:07,  7.28it/s]Epoch: 2, train for the 231-th batch, train loss: 0.6165115237236023:  97%|██████████▋| 231/237 [00:46<00:01,  4.11it/s]Epoch: 5, train for the 93-th batch, train loss: 0.5211366415023804:  64%|████████▎    | 93/146 [00:13<00:07,  6.78it/s]Epoch: 4, train for the 109-th batch, train loss: 0.4848571717739105:  72%|███████▊   | 108/151 [00:18<00:07,  5.66it/s]Epoch: 3, train for the 120-th batch, train loss: 0.5452138185501099:  49%|█████▍     | 119/241 [00:18<00:19,  6.12it/s]Epoch: 3, train for the 120-th batch, train loss: 0.5452138185501099:  50%|█████▍     | 120/241 [00:18<00:21,  5.72it/s]Epoch: 4, train for the 109-th batch, train loss: 0.4848571717739105:  72%|███████▉   | 109/151 [00:18<00:08,  5.20it/s]Epoch: 6, train for the 62-th batch, train loss: 0.44209468364715576:  51%|██████▏     | 61/119 [00:09<00:09,  6.43it/s]Epoch: 6, train for the 62-th batch, train loss: 0.44209468364715576:  52%|██████▎     | 62/119 [00:09<00:08,  6.46it/s]evaluate for the 21-th batch, evaluate loss: 0.2205129712820053:  19%|███▏             | 20/106 [00:04<00:18,  4.66it/s]evaluate for the 21-th batch, evaluate loss: 0.2205129712820053:  20%|███▎             | 21/106 [00:04<00:20,  4.15it/s]Epoch: 5, train for the 94-th batch, train loss: 0.5190535187721252:  64%|████████▎    | 93/146 [00:13<00:07,  6.78it/s]Epoch: 5, train for the 94-th batch, train loss: 0.5190535187721252:  64%|████████▎    | 94/146 [00:13<00:07,  6.78it/s]Epoch: 3, train for the 121-th batch, train loss: 0.5249912738800049:  50%|█████▍     | 120/241 [00:18<00:21,  5.72it/s]Epoch: 3, train for the 121-th batch, train loss: 0.5249912738800049:  50%|█████▌     | 121/241 [00:18<00:19,  6.10it/s]Epoch: 4, train for the 110-th batch, train loss: 0.5009031295776367:  72%|███████▉   | 109/151 [00:18<00:08,  5.20it/s]Epoch: 2, train for the 232-th batch, train loss: 0.5848650336265564:  97%|██████████▋| 231/237 [00:46<00:01,  4.11it/s]Epoch: 4, train for the 110-th batch, train loss: 0.5009031295776367:  73%|████████   | 110/151 [00:18<00:07,  5.35it/s]evaluate for the 22-th batch, evaluate loss: 0.3041169345378876:  20%|███▎             | 21/106 [00:04<00:20,  4.15it/s]evaluate for the 22-th batch, evaluate loss: 0.3041169345378876:  21%|███▌             | 22/106 [00:04<00:17,  4.78it/s]Epoch: 2, train for the 232-th batch, train loss: 0.5848650336265564:  98%|██████████▊| 232/237 [00:46<00:01,  3.95it/s]Epoch: 6, train for the 63-th batch, train loss: 0.4253529906272888:  52%|██████▊      | 62/119 [00:09<00:08,  6.46it/s]Epoch: 5, train for the 95-th batch, train loss: 0.5165573954582214:  64%|████████▎    | 94/146 [00:14<00:07,  6.78it/s]Epoch: 6, train for the 63-th batch, train loss: 0.4253529906272888:  53%|██████▉      | 63/119 [00:09<00:09,  6.09it/s]Epoch: 3, train for the 122-th batch, train loss: 0.4797583520412445:  50%|█████▌     | 121/241 [00:18<00:19,  6.10it/s]Epoch: 5, train for the 95-th batch, train loss: 0.5165573954582214:  65%|████████▍    | 95/146 [00:14<00:07,  6.59it/s]Epoch: 3, train for the 122-th batch, train loss: 0.4797583520412445:  51%|█████▌     | 122/241 [00:18<00:18,  6.30it/s]Epoch: 4, train for the 111-th batch, train loss: 0.4895175099372864:  73%|████████   | 110/151 [00:19<00:07,  5.35it/s]Epoch: 4, train for the 111-th batch, train loss: 0.4895175099372864:  74%|████████   | 111/151 [00:19<00:07,  5.51it/s]Epoch: 2, train for the 233-th batch, train loss: 0.6025999188423157:  98%|██████████▊| 232/237 [00:47<00:01,  3.95it/s]Epoch: 6, train for the 64-th batch, train loss: 0.43409299850463867:  53%|██████▎     | 63/119 [00:09<00:09,  6.09it/s]Epoch: 6, train for the 64-th batch, train loss: 0.43409299850463867:  54%|██████▍     | 64/119 [00:09<00:09,  6.08it/s]Epoch: 2, train for the 233-th batch, train loss: 0.6025999188423157:  98%|██████████▊| 233/237 [00:47<00:00,  4.20it/s]Epoch: 3, train for the 123-th batch, train loss: 0.7353764176368713:  51%|█████▌     | 122/241 [00:18<00:18,  6.30it/s]Epoch: 5, train for the 96-th batch, train loss: 0.49755820631980896:  65%|███████▊    | 95/146 [00:14<00:07,  6.59it/s]Epoch: 5, train for the 96-th batch, train loss: 0.49755820631980896:  66%|███████▉    | 96/146 [00:14<00:08,  6.18it/s]Epoch: 3, train for the 123-th batch, train loss: 0.7353764176368713:  51%|█████▌     | 123/241 [00:18<00:19,  6.17it/s]Epoch: 4, train for the 112-th batch, train loss: 0.46253493428230286:  74%|███████▎  | 111/151 [00:19<00:07,  5.51it/s]Epoch: 4, train for the 112-th batch, train loss: 0.46253493428230286:  74%|███████▍  | 112/151 [00:19<00:06,  5.82it/s]Epoch: 6, train for the 65-th batch, train loss: 0.4749366343021393:  54%|██████▉      | 64/119 [00:09<00:09,  6.08it/s]Epoch: 6, train for the 65-th batch, train loss: 0.4749366343021393:  55%|███████      | 65/119 [00:09<00:08,  6.33it/s]evaluate for the 23-th batch, evaluate loss: 0.38914549350738525:  21%|███▎            | 22/106 [00:05<00:17,  4.78it/s]evaluate for the 23-th batch, evaluate loss: 0.38914549350738525:  22%|███▍            | 23/106 [00:05<00:21,  3.95it/s]Epoch: 5, train for the 97-th batch, train loss: 0.5251211524009705:  66%|████████▌    | 96/146 [00:14<00:08,  6.18it/s]Epoch: 3, train for the 124-th batch, train loss: 0.3837979733943939:  51%|█████▌     | 123/241 [00:18<00:19,  6.17it/s]Epoch: 5, train for the 97-th batch, train loss: 0.5251211524009705:  66%|████████▋    | 97/146 [00:14<00:08,  6.11it/s]Epoch: 2, train for the 234-th batch, train loss: 0.5683068037033081:  98%|██████████▊| 233/237 [00:47<00:00,  4.20it/s]Epoch: 3, train for the 124-th batch, train loss: 0.3837979733943939:  51%|█████▋     | 124/241 [00:19<00:19,  5.98it/s]Epoch: 4, train for the 113-th batch, train loss: 0.5219442844390869:  74%|████████▏  | 112/151 [00:19<00:06,  5.82it/s]Epoch: 4, train for the 113-th batch, train loss: 0.5219442844390869:  75%|████████▏  | 113/151 [00:19<00:06,  5.78it/s]Epoch: 6, train for the 66-th batch, train loss: 0.46388620138168335:  55%|██████▌     | 65/119 [00:10<00:08,  6.33it/s]Epoch: 2, train for the 234-th batch, train loss: 0.5683068037033081:  99%|██████████▊| 234/237 [00:47<00:00,  4.12it/s]Epoch: 6, train for the 66-th batch, train loss: 0.46388620138168335:  55%|██████▋     | 66/119 [00:10<00:07,  6.82it/s]Epoch: 5, train for the 98-th batch, train loss: 0.5246829986572266:  66%|████████▋    | 97/146 [00:14<00:08,  6.11it/s]evaluate for the 24-th batch, evaluate loss: 0.30092355608940125:  22%|███▍            | 23/106 [00:05<00:21,  3.95it/s]evaluate for the 24-th batch, evaluate loss: 0.30092355608940125:  23%|███▌            | 24/106 [00:05<00:18,  4.32it/s]Epoch: 5, train for the 98-th batch, train loss: 0.5246829986572266:  67%|████████▋    | 98/146 [00:14<00:07,  6.28it/s]Epoch: 3, train for the 125-th batch, train loss: 0.47538527846336365:  51%|█████▏    | 124/241 [00:19<00:19,  5.98it/s]Epoch: 3, train for the 125-th batch, train loss: 0.47538527846336365:  52%|█████▏    | 125/241 [00:19<00:19,  6.07it/s]Epoch: 6, train for the 67-th batch, train loss: 0.47842222452163696:  55%|██████▋     | 66/119 [00:10<00:07,  6.82it/s]Epoch: 6, train for the 67-th batch, train loss: 0.47842222452163696:  56%|██████▊     | 67/119 [00:10<00:07,  6.74it/s]Epoch: 4, train for the 114-th batch, train loss: 0.4857443869113922:  75%|████████▏  | 113/151 [00:19<00:06,  5.78it/s]Epoch: 4, train for the 114-th batch, train loss: 0.4857443869113922:  75%|████████▎  | 114/151 [00:19<00:06,  5.55it/s]Epoch: 2, train for the 235-th batch, train loss: 0.5677858591079712:  99%|██████████▊| 234/237 [00:47<00:00,  4.12it/s]Epoch: 5, train for the 99-th batch, train loss: 0.525844931602478:  67%|█████████▍    | 98/146 [00:14<00:07,  6.28it/s]Epoch: 3, train for the 126-th batch, train loss: 0.432229220867157:  52%|██████▏     | 125/241 [00:19<00:19,  6.07it/s]Epoch: 5, train for the 99-th batch, train loss: 0.525844931602478:  68%|█████████▍    | 99/146 [00:14<00:07,  6.19it/s]Epoch: 3, train for the 126-th batch, train loss: 0.432229220867157:  52%|██████▎     | 126/241 [00:19<00:18,  6.32it/s]Epoch: 2, train for the 235-th batch, train loss: 0.5677858591079712:  99%|██████████▉| 235/237 [00:47<00:00,  4.04it/s]Epoch: 6, train for the 68-th batch, train loss: 0.3971901535987854:  56%|███████▎     | 67/119 [00:10<00:07,  6.74it/s]Epoch: 6, train for the 68-th batch, train loss: 0.3971901535987854:  57%|███████▍     | 68/119 [00:10<00:07,  7.06it/s]Epoch: 4, train for the 115-th batch, train loss: 0.44909238815307617:  75%|███████▌  | 114/151 [00:19<00:06,  5.55it/s]Epoch: 4, train for the 115-th batch, train loss: 0.44909238815307617:  76%|███████▌  | 115/151 [00:19<00:06,  5.71it/s]Epoch: 5, train for the 100-th batch, train loss: 0.5743498802185059:  68%|████████▏   | 99/146 [00:14<00:07,  6.19it/s]evaluate for the 25-th batch, evaluate loss: 0.3440142869949341:  23%|███▊             | 24/106 [00:05<00:18,  4.32it/s]evaluate for the 25-th batch, evaluate loss: 0.3440142869949341:  24%|████             | 25/106 [00:05<00:20,  4.03it/s]Epoch: 3, train for the 127-th batch, train loss: 0.5021342039108276:  52%|█████▊     | 126/241 [00:19<00:18,  6.32it/s]Epoch: 5, train for the 100-th batch, train loss: 0.5743498802185059:  68%|███████▌   | 100/146 [00:14<00:06,  6.67it/s]Epoch: 3, train for the 127-th batch, train loss: 0.5021342039108276:  53%|█████▊     | 127/241 [00:19<00:17,  6.55it/s]Epoch: 2, train for the 236-th batch, train loss: 0.558266818523407:  99%|███████████▉| 235/237 [00:47<00:00,  4.04it/s]Epoch: 6, train for the 69-th batch, train loss: 0.4170888364315033:  57%|███████▍     | 68/119 [00:10<00:07,  7.06it/s]Epoch: 6, train for the 69-th batch, train loss: 0.4170888364315033:  58%|███████▌     | 69/119 [00:10<00:07,  6.60it/s]Epoch: 5, train for the 101-th batch, train loss: 0.5039870738983154:  68%|███████▌   | 100/146 [00:14<00:06,  6.67it/s]Epoch: 5, train for the 101-th batch, train loss: 0.5039870738983154:  69%|███████▌   | 101/146 [00:14<00:06,  7.20it/s]Epoch: 2, train for the 236-th batch, train loss: 0.558266818523407: 100%|███████████▉| 236/237 [00:47<00:00,  4.13it/s]Epoch: 4, train for the 116-th batch, train loss: 0.41913336515426636:  76%|███████▌  | 115/151 [00:19<00:06,  5.71it/s]Epoch: 3, train for the 128-th batch, train loss: 0.5413522124290466:  53%|█████▊     | 127/241 [00:19<00:17,  6.55it/s]Epoch: 4, train for the 116-th batch, train loss: 0.41913336515426636:  77%|███████▋  | 116/151 [00:19<00:06,  5.52it/s]evaluate for the 26-th batch, evaluate loss: 0.43910902738571167:  24%|███▊            | 25/106 [00:05<00:20,  4.03it/s]evaluate for the 26-th batch, evaluate loss: 0.43910902738571167:  25%|███▉            | 26/106 [00:05<00:18,  4.33it/s]Epoch: 3, train for the 128-th batch, train loss: 0.5413522124290466:  53%|█████▊     | 128/241 [00:19<00:17,  6.42it/s]Epoch: 6, train for the 70-th batch, train loss: 0.3772135376930237:  58%|███████▌     | 69/119 [00:10<00:07,  6.60it/s]Epoch: 6, train for the 70-th batch, train loss: 0.3772135376930237:  59%|███████▋     | 70/119 [00:10<00:07,  6.36it/s]Epoch: 5, train for the 102-th batch, train loss: 0.4979396462440491:  69%|███████▌   | 101/146 [00:15<00:06,  7.20it/s]Epoch: 2, train for the 237-th batch, train loss: 0.5946425199508667: 100%|██████████▉| 236/237 [00:48<00:00,  4.13it/s]Epoch: 5, train for the 102-th batch, train loss: 0.4979396462440491:  70%|███████▋   | 102/146 [00:15<00:06,  6.57it/s]Epoch: 4, train for the 117-th batch, train loss: 0.48653677105903625:  77%|███████▋  | 116/151 [00:20<00:06,  5.52it/s]Epoch: 2, train for the 237-th batch, train loss: 0.5946425199508667: 100%|███████████| 237/237 [00:48<00:00,  4.27it/s]Epoch: 2, train for the 237-th batch, train loss: 0.5946425199508667: 100%|███████████| 237/237 [00:48<00:00,  4.93it/s]
Epoch: 4, train for the 117-th batch, train loss: 0.48653677105903625:  77%|███████▋  | 117/151 [00:20<00:05,  5.69it/s]Epoch: 3, train for the 129-th batch, train loss: 0.47943320870399475:  53%|█████▎    | 128/241 [00:19<00:17,  6.42it/s]Epoch: 3, train for the 129-th batch, train loss: 0.47943320870399475:  54%|█████▎    | 129/241 [00:19<00:17,  6.25it/s]Epoch: 6, train for the 71-th batch, train loss: 0.4113911986351013:  59%|███████▋     | 70/119 [00:10<00:07,  6.36it/s]Epoch: 6, train for the 71-th batch, train loss: 0.4113911986351013:  60%|███████▊     | 71/119 [00:10<00:07,  6.56it/s]Epoch: 5, train for the 103-th batch, train loss: 0.5063514113426208:  70%|███████▋   | 102/146 [00:15<00:06,  6.57it/s]Epoch: 5, train for the 103-th batch, train loss: 0.5063514113426208:  71%|███████▊   | 103/146 [00:15<00:06,  6.61it/s]evaluate for the 27-th batch, evaluate loss: 0.24400700628757477:  25%|███▉            | 26/106 [00:06<00:18,  4.33it/s]evaluate for the 27-th batch, evaluate loss: 0.24400700628757477:  25%|████            | 27/106 [00:06<00:20,  3.91it/s]Epoch: 6, train for the 72-th batch, train loss: 0.44480636715888977:  60%|███████▏    | 71/119 [00:10<00:07,  6.56it/s]Epoch: 4, train for the 118-th batch, train loss: 0.45963260531425476:  77%|███████▋  | 117/151 [00:20<00:05,  5.69it/s]Epoch: 6, train for the 72-th batch, train loss: 0.44480636715888977:  61%|███████▎    | 72/119 [00:10<00:06,  6.91it/s]Epoch: 3, train for the 130-th batch, train loss: 0.49481040239334106:  54%|█████▎    | 129/241 [00:19<00:17,  6.25it/s]Epoch: 4, train for the 118-th batch, train loss: 0.45963260531425476:  78%|███████▊  | 118/151 [00:20<00:06,  5.43it/s]Epoch: 3, train for the 130-th batch, train loss: 0.49481040239334106:  54%|█████▍    | 130/241 [00:19<00:18,  5.86it/s]Epoch: 5, train for the 104-th batch, train loss: 0.5183009505271912:  71%|███████▊   | 103/146 [00:15<00:06,  6.61it/s]Epoch: 5, train for the 104-th batch, train loss: 0.5183009505271912:  71%|███████▊   | 104/146 [00:15<00:06,  6.68it/s]evaluate for the 28-th batch, evaluate loss: 0.245997354388237:  25%|████▌             | 27/106 [00:06<00:20,  3.91it/s]evaluate for the 28-th batch, evaluate loss: 0.245997354388237:  26%|████▊             | 28/106 [00:06<00:17,  4.44it/s]Epoch: 6, train for the 73-th batch, train loss: 0.4182635545730591:  61%|███████▊     | 72/119 [00:11<00:06,  6.91it/s]Epoch: 6, train for the 73-th batch, train loss: 0.4182635545730591:  61%|███████▉     | 73/119 [00:11<00:06,  6.82it/s]Epoch: 4, train for the 119-th batch, train loss: 0.48916831612586975:  78%|███████▊  | 118/151 [00:20<00:06,  5.43it/s]Epoch: 3, train for the 131-th batch, train loss: 0.5549545884132385:  54%|█████▉     | 130/241 [00:20<00:18,  5.86it/s]Epoch: 5, train for the 105-th batch, train loss: 0.5135438442230225:  71%|███████▊   | 104/146 [00:15<00:06,  6.68it/s]Epoch: 4, train for the 119-th batch, train loss: 0.48916831612586975:  79%|███████▉  | 119/151 [00:20<00:06,  5.29it/s]Epoch: 5, train for the 105-th batch, train loss: 0.5135438442230225:  72%|███████▉   | 105/146 [00:15<00:06,  6.53it/s]Epoch: 3, train for the 131-th batch, train loss: 0.5549545884132385:  54%|█████▉     | 131/241 [00:20<00:19,  5.54it/s]Epoch: 6, train for the 74-th batch, train loss: 0.45537710189819336:  61%|███████▎    | 73/119 [00:11<00:06,  6.82it/s]Epoch: 6, train for the 74-th batch, train loss: 0.45537710189819336:  62%|███████▍    | 74/119 [00:11<00:06,  7.07it/s]  0%|                                                                                            | 0/66 [00:00<?, ?it/s]Epoch: 5, train for the 106-th batch, train loss: 0.5222446918487549:  72%|███████▉   | 105/146 [00:15<00:06,  6.53it/s]Epoch: 5, train for the 106-th batch, train loss: 0.5222446918487549:  73%|███████▉   | 106/146 [00:15<00:05,  6.77it/s]Epoch: 4, train for the 120-th batch, train loss: 0.5573672652244568:  79%|████████▋  | 119/151 [00:20<00:06,  5.29it/s]Epoch: 3, train for the 132-th batch, train loss: 0.47427961230278015:  54%|█████▍    | 131/241 [00:20<00:19,  5.54it/s]Epoch: 6, train for the 75-th batch, train loss: 0.43158069252967834:  62%|███████▍    | 74/119 [00:11<00:06,  7.07it/s]Epoch: 6, train for the 75-th batch, train loss: 0.43158069252967834:  63%|███████▌    | 75/119 [00:11<00:06,  7.30it/s]Epoch: 4, train for the 120-th batch, train loss: 0.5573672652244568:  79%|████████▋  | 120/151 [00:20<00:05,  5.24it/s]Epoch: 3, train for the 132-th batch, train loss: 0.47427961230278015:  55%|█████▍    | 132/241 [00:20<00:19,  5.50it/s]evaluate for the 29-th batch, evaluate loss: 0.305922269821167:  26%|████▊             | 28/106 [00:06<00:17,  4.44it/s]evaluate for the 29-th batch, evaluate loss: 0.305922269821167:  27%|████▉             | 29/106 [00:06<00:19,  4.04it/s]evaluate for the 1-th batch, evaluate loss: 0.5600490570068359:   0%|                            | 0/66 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.5600490570068359:   2%|▎                   | 1/66 [00:00<00:07,  8.90it/s]Epoch: 5, train for the 107-th batch, train loss: 0.4942662715911865:  73%|███████▉   | 106/146 [00:15<00:05,  6.77it/s]Epoch: 5, train for the 107-th batch, train loss: 0.4942662715911865:  73%|████████   | 107/146 [00:15<00:05,  6.95it/s]Epoch: 6, train for the 76-th batch, train loss: 0.46227961778640747:  63%|███████▌    | 75/119 [00:11<00:06,  7.30it/s]Epoch: 6, train for the 76-th batch, train loss: 0.46227961778640747:  64%|███████▋    | 76/119 [00:11<00:05,  7.27it/s]Epoch: 3, train for the 133-th batch, train loss: 0.4068852961063385:  55%|██████     | 132/241 [00:20<00:19,  5.50it/s]Epoch: 4, train for the 121-th batch, train loss: 0.4885568916797638:  79%|████████▋  | 120/151 [00:20<00:05,  5.24it/s]Epoch: 3, train for the 133-th batch, train loss: 0.4068852961063385:  55%|██████     | 133/241 [00:20<00:19,  5.42it/s]Epoch: 4, train for the 121-th batch, train loss: 0.4885568916797638:  80%|████████▊  | 121/151 [00:20<00:05,  5.10it/s]evaluate for the 2-th batch, evaluate loss: 0.5216060280799866:   2%|▎                   | 1/66 [00:00<00:07,  8.90it/s]evaluate for the 2-th batch, evaluate loss: 0.5216060280799866:   3%|▌                   | 2/66 [00:00<00:09,  6.86it/s]Epoch: 5, train for the 108-th batch, train loss: 0.4712331295013428:  73%|████████   | 107/146 [00:16<00:05,  6.95it/s]evaluate for the 30-th batch, evaluate loss: 0.36123159527778625:  27%|████▍           | 29/106 [00:06<00:19,  4.04it/s]evaluate for the 30-th batch, evaluate loss: 0.36123159527778625:  28%|████▌           | 30/106 [00:06<00:17,  4.27it/s]Epoch: 5, train for the 108-th batch, train loss: 0.4712331295013428:  74%|████████▏  | 108/146 [00:16<00:05,  7.04it/s]Epoch: 6, train for the 77-th batch, train loss: 0.420475035905838:  64%|████████▉     | 76/119 [00:11<00:05,  7.27it/s]Epoch: 3, train for the 134-th batch, train loss: 0.485696017742157:  55%|██████▌     | 133/241 [00:20<00:19,  5.42it/s]Epoch: 6, train for the 77-th batch, train loss: 0.420475035905838:  65%|█████████     | 77/119 [00:11<00:06,  6.67it/s]evaluate for the 3-th batch, evaluate loss: 0.5346730947494507:   3%|▌                   | 2/66 [00:00<00:09,  6.86it/s]evaluate for the 3-th batch, evaluate loss: 0.5346730947494507:   5%|▉                   | 3/66 [00:00<00:08,  7.69it/s]Epoch: 3, train for the 134-th batch, train loss: 0.485696017742157:  56%|██████▋     | 134/241 [00:20<00:18,  5.82it/s]Epoch: 5, train for the 109-th batch, train loss: 0.5038763284683228:  74%|████████▏  | 108/146 [00:16<00:05,  7.04it/s]Epoch: 5, train for the 109-th batch, train loss: 0.5038763284683228:  75%|████████▏  | 109/146 [00:16<00:05,  7.19it/s]Epoch: 4, train for the 122-th batch, train loss: 0.4886942505836487:  80%|████████▊  | 121/151 [00:21<00:05,  5.10it/s]Epoch: 4, train for the 122-th batch, train loss: 0.4886942505836487:  81%|████████▉  | 122/151 [00:21<00:05,  5.24it/s]evaluate for the 4-th batch, evaluate loss: 0.5237998366355896:   5%|▉                   | 3/66 [00:00<00:08,  7.69it/s]Epoch: 6, train for the 78-th batch, train loss: 0.4739118218421936:  65%|████████▍    | 77/119 [00:11<00:06,  6.67it/s]Epoch: 6, train for the 78-th batch, train loss: 0.4739118218421936:  66%|████████▌    | 78/119 [00:11<00:05,  6.84it/s]Epoch: 3, train for the 135-th batch, train loss: 0.40159299969673157:  56%|█████▌    | 134/241 [00:20<00:18,  5.82it/s]evaluate for the 31-th batch, evaluate loss: 0.3152308762073517:  28%|████▊            | 30/106 [00:07<00:17,  4.27it/s]evaluate for the 31-th batch, evaluate loss: 0.3152308762073517:  29%|████▉            | 31/106 [00:07<00:18,  4.12it/s]Epoch: 3, train for the 135-th batch, train loss: 0.40159299969673157:  56%|█████▌    | 135/241 [00:20<00:17,  6.05it/s]Epoch: 5, train for the 110-th batch, train loss: 0.4549933075904846:  75%|████████▏  | 109/146 [00:16<00:05,  7.19it/s]Epoch: 5, train for the 110-th batch, train loss: 0.4549933075904846:  75%|████████▎  | 110/146 [00:16<00:05,  7.07it/s]evaluate for the 5-th batch, evaluate loss: 0.5510582327842712:   5%|▉                   | 3/66 [00:00<00:08,  7.69it/s]evaluate for the 5-th batch, evaluate loss: 0.5510582327842712:   8%|█▌                  | 5/66 [00:00<00:06,  9.08it/s]Epoch: 4, train for the 123-th batch, train loss: 0.487659215927124:  81%|█████████▋  | 122/151 [00:21<00:05,  5.24it/s]Epoch: 6, train for the 79-th batch, train loss: 0.4443727731704712:  66%|████████▌    | 78/119 [00:11<00:05,  6.84it/s]Epoch: 6, train for the 79-th batch, train loss: 0.4443727731704712:  66%|████████▋    | 79/119 [00:11<00:05,  6.93it/s]Epoch: 4, train for the 123-th batch, train loss: 0.487659215927124:  81%|█████████▊  | 123/151 [00:21<00:05,  5.06it/s]Epoch: 3, train for the 136-th batch, train loss: 0.42891359329223633:  56%|█████▌    | 135/241 [00:20<00:17,  6.05it/s]Epoch: 3, train for the 136-th batch, train loss: 0.42891359329223633:  56%|█████▋    | 136/241 [00:21<00:16,  6.24it/s]evaluate for the 6-th batch, evaluate loss: 0.5834668874740601:   8%|█▌                  | 5/66 [00:00<00:06,  9.08it/s]evaluate for the 6-th batch, evaluate loss: 0.5834668874740601:   9%|█▊                  | 6/66 [00:00<00:07,  8.57it/s]Epoch: 5, train for the 111-th batch, train loss: 0.5102015733718872:  75%|████████▎  | 110/146 [00:16<00:05,  7.07it/s]Epoch: 5, train for the 111-th batch, train loss: 0.5102015733718872:  76%|████████▎  | 111/146 [00:16<00:05,  6.69it/s]evaluate for the 32-th batch, evaluate loss: 0.32674992084503174:  29%|████▋           | 31/106 [00:07<00:18,  4.12it/s]evaluate for the 32-th batch, evaluate loss: 0.32674992084503174:  30%|████▊           | 32/106 [00:07<00:16,  4.36it/s]Epoch: 6, train for the 80-th batch, train loss: 0.4437103271484375:  66%|████████▋    | 79/119 [00:12<00:05,  6.93it/s]Epoch: 6, train for the 80-th batch, train loss: 0.4437103271484375:  67%|████████▋    | 80/119 [00:12<00:05,  6.93it/s]evaluate for the 7-th batch, evaluate loss: 0.5407067537307739:   9%|█▊                  | 6/66 [00:00<00:07,  8.57it/s]evaluate for the 7-th batch, evaluate loss: 0.5407067537307739:  11%|██                  | 7/66 [00:00<00:06,  8.71it/s]Epoch: 5, train for the 112-th batch, train loss: 0.5082283616065979:  76%|████████▎  | 111/146 [00:16<00:05,  6.69it/s]Epoch: 4, train for the 124-th batch, train loss: 0.5246855616569519:  81%|████████▉  | 123/151 [00:21<00:05,  5.06it/s]Epoch: 5, train for the 112-th batch, train loss: 0.5082283616065979:  77%|████████▍  | 112/146 [00:16<00:05,  6.78it/s]Epoch: 3, train for the 137-th batch, train loss: 0.4472354054450989:  56%|██████▏    | 136/241 [00:21<00:16,  6.24it/s]Epoch: 4, train for the 124-th batch, train loss: 0.5246855616569519:  82%|█████████  | 124/151 [00:21<00:05,  4.82it/s]evaluate for the 8-th batch, evaluate loss: 0.5301147103309631:  11%|██                  | 7/66 [00:00<00:06,  8.71it/s]Epoch: 3, train for the 137-th batch, train loss: 0.4472354054450989:  57%|██████▎    | 137/241 [00:21<00:18,  5.75it/s]Epoch: 6, train for the 81-th batch, train loss: 0.402439147233963:  67%|█████████▍    | 80/119 [00:12<00:05,  6.93it/s]Epoch: 6, train for the 81-th batch, train loss: 0.402439147233963:  68%|█████████▌    | 81/119 [00:12<00:05,  7.23it/s]Epoch: 5, train for the 113-th batch, train loss: 0.44439056515693665:  77%|███████▋  | 112/146 [00:16<00:05,  6.78it/s]Epoch: 5, train for the 113-th batch, train loss: 0.44439056515693665:  77%|███████▋  | 113/146 [00:16<00:04,  6.79it/s]evaluate for the 9-th batch, evaluate loss: 0.47996795177459717:  11%|██                 | 7/66 [00:01<00:06,  8.71it/s]evaluate for the 9-th batch, evaluate loss: 0.47996795177459717:  14%|██▌                | 9/66 [00:01<00:06,  8.70it/s]evaluate for the 33-th batch, evaluate loss: 0.43156516551971436:  30%|████▊           | 32/106 [00:07<00:16,  4.36it/s]evaluate for the 33-th batch, evaluate loss: 0.43156516551971436:  31%|████▉           | 33/106 [00:07<00:18,  3.94it/s]Epoch: 6, train for the 82-th batch, train loss: 0.44050222635269165:  68%|████████▏   | 81/119 [00:12<00:05,  7.23it/s]Epoch: 6, train for the 82-th batch, train loss: 0.44050222635269165:  69%|████████▎   | 82/119 [00:12<00:05,  6.99it/s]Epoch: 3, train for the 138-th batch, train loss: 0.3929612338542938:  57%|██████▎    | 137/241 [00:21<00:18,  5.75it/s]Epoch: 3, train for the 138-th batch, train loss: 0.3929612338542938:  57%|██████▎    | 138/241 [00:21<00:18,  5.53it/s]Epoch: 4, train for the 125-th batch, train loss: 0.5340104699134827:  82%|█████████  | 124/151 [00:21<00:05,  4.82it/s]Epoch: 5, train for the 114-th batch, train loss: 0.49334585666656494:  77%|███████▋  | 113/146 [00:16<00:04,  6.79it/s]Epoch: 4, train for the 125-th batch, train loss: 0.5340104699134827:  83%|█████████  | 125/151 [00:21<00:05,  4.57it/s]Epoch: 5, train for the 114-th batch, train loss: 0.49334585666656494:  78%|███████▊  | 114/146 [00:16<00:04,  7.08it/s]evaluate for the 10-th batch, evaluate loss: 0.5216447114944458:  14%|██▌                | 9/66 [00:01<00:06,  8.70it/s]evaluate for the 10-th batch, evaluate loss: 0.5216447114944458:  15%|██▋               | 10/66 [00:01<00:06,  8.63it/s]Epoch: 3, train for the 139-th batch, train loss: 0.35403358936309814:  57%|█████▋    | 138/241 [00:21<00:18,  5.53it/s]evaluate for the 34-th batch, evaluate loss: 0.4108513295650482:  31%|█████▎           | 33/106 [00:07<00:18,  3.94it/s]evaluate for the 34-th batch, evaluate loss: 0.4108513295650482:  32%|█████▍           | 34/106 [00:07<00:16,  4.38it/s]Epoch: 6, train for the 83-th batch, train loss: 0.4645710587501526:  69%|████████▉    | 82/119 [00:12<00:05,  6.99it/s]Epoch: 3, train for the 139-th batch, train loss: 0.35403358936309814:  58%|█████▊    | 139/241 [00:21<00:16,  6.03it/s]Epoch: 6, train for the 83-th batch, train loss: 0.4645710587501526:  70%|█████████    | 83/119 [00:12<00:05,  6.75it/s]evaluate for the 11-th batch, evaluate loss: 0.5002641677856445:  15%|██▋               | 10/66 [00:01<00:06,  8.63it/s]evaluate for the 11-th batch, evaluate loss: 0.5002641677856445:  17%|███               | 11/66 [00:01<00:06,  8.69it/s]Epoch: 5, train for the 115-th batch, train loss: 0.5303986072540283:  78%|████████▌  | 114/146 [00:17<00:04,  7.08it/s]Epoch: 4, train for the 126-th batch, train loss: 0.46975573897361755:  83%|████████▎ | 125/151 [00:22<00:05,  4.57it/s]Epoch: 5, train for the 115-th batch, train loss: 0.5303986072540283:  79%|████████▋  | 115/146 [00:17<00:04,  6.77it/s]Epoch: 4, train for the 126-th batch, train loss: 0.46975573897361755:  83%|████████▎ | 126/151 [00:22<00:05,  4.75it/s]evaluate for the 12-th batch, evaluate loss: 0.5304473638534546:  17%|███               | 11/66 [00:01<00:06,  8.69it/s]Epoch: 3, train for the 140-th batch, train loss: 0.36849546432495117:  58%|█████▊    | 139/241 [00:21<00:16,  6.03it/s]Epoch: 6, train for the 84-th batch, train loss: 0.457471638917923:  70%|█████████▊    | 83/119 [00:12<00:05,  6.75it/s]Epoch: 6, train for the 84-th batch, train loss: 0.457471638917923:  71%|█████████▉    | 84/119 [00:12<00:05,  6.73it/s]Epoch: 3, train for the 140-th batch, train loss: 0.36849546432495117:  58%|█████▊    | 140/241 [00:21<00:16,  6.14it/s]Epoch: 4, train for the 127-th batch, train loss: 0.4738430380821228:  83%|█████████▏ | 126/151 [00:22<00:05,  4.75it/s]Epoch: 5, train for the 116-th batch, train loss: 0.5159575343132019:  79%|████████▋  | 115/146 [00:17<00:04,  6.77it/s]evaluate for the 13-th batch, evaluate loss: 0.5323200225830078:  17%|███               | 11/66 [00:01<00:06,  8.69it/s]evaluate for the 13-th batch, evaluate loss: 0.5323200225830078:  20%|███▌              | 13/66 [00:01<00:05,  9.51it/s]Epoch: 5, train for the 116-th batch, train loss: 0.5159575343132019:  79%|████████▋  | 116/146 [00:17<00:04,  6.56it/s]evaluate for the 35-th batch, evaluate loss: 0.2823053300380707:  32%|█████▍           | 34/106 [00:07<00:16,  4.38it/s]evaluate for the 35-th batch, evaluate loss: 0.2823053300380707:  33%|█████▌           | 35/106 [00:07<00:16,  4.21it/s]Epoch: 4, train for the 127-th batch, train loss: 0.4738430380821228:  84%|█████████▎ | 127/151 [00:22<00:04,  5.09it/s]Epoch: 3, train for the 141-th batch, train loss: 0.5040922164916992:  58%|██████▍    | 140/241 [00:21<00:16,  6.14it/s]Epoch: 6, train for the 85-th batch, train loss: 0.44221994280815125:  71%|████████▍   | 84/119 [00:12<00:05,  6.73it/s]Epoch: 3, train for the 141-th batch, train loss: 0.5040922164916992:  59%|██████▍    | 141/241 [00:21<00:16,  6.13it/s]Epoch: 6, train for the 85-th batch, train loss: 0.44221994280815125:  71%|████████▌   | 85/119 [00:12<00:05,  6.45it/s]evaluate for the 14-th batch, evaluate loss: 0.5198991894721985:  20%|███▌              | 13/66 [00:01<00:05,  9.51it/s]evaluate for the 14-th batch, evaluate loss: 0.5198991894721985:  21%|███▊              | 14/66 [00:01<00:05,  9.22it/s]Epoch: 5, train for the 117-th batch, train loss: 0.5628144145011902:  79%|████████▋  | 116/146 [00:17<00:04,  6.56it/s]Epoch: 5, train for the 117-th batch, train loss: 0.5628144145011902:  80%|████████▊  | 117/146 [00:17<00:04,  6.46it/s]Epoch: 4, train for the 128-th batch, train loss: 0.5070303082466125:  84%|█████████▎ | 127/151 [00:22<00:04,  5.09it/s]evaluate for the 36-th batch, evaluate loss: 0.4317401051521301:  33%|█████▌           | 35/106 [00:08<00:16,  4.21it/s]evaluate for the 36-th batch, evaluate loss: 0.4317401051521301:  34%|█████▊           | 36/106 [00:08<00:15,  4.57it/s]Epoch: 4, train for the 128-th batch, train loss: 0.5070303082466125:  85%|█████████▎ | 128/151 [00:22<00:04,  5.26it/s]Epoch: 3, train for the 142-th batch, train loss: 0.4484635591506958:  59%|██████▍    | 141/241 [00:21<00:16,  6.13it/s]Epoch: 6, train for the 86-th batch, train loss: 0.43979352712631226:  71%|████████▌   | 85/119 [00:13<00:05,  6.45it/s]Epoch: 3, train for the 142-th batch, train loss: 0.4484635591506958:  59%|██████▍    | 142/241 [00:22<00:15,  6.19it/s]Epoch: 6, train for the 86-th batch, train loss: 0.43979352712631226:  72%|████████▋   | 86/119 [00:13<00:05,  6.38it/s]Epoch: 5, train for the 118-th batch, train loss: 0.4659486413002014:  80%|████████▊  | 117/146 [00:17<00:04,  6.46it/s]Epoch: 5, train for the 118-th batch, train loss: 0.4659486413002014:  81%|████████▉  | 118/146 [00:17<00:04,  6.52it/s]Epoch: 4, train for the 129-th batch, train loss: 0.5099862813949585:  85%|█████████▎ | 128/151 [00:22<00:04,  5.26it/s]Epoch: 3, train for the 143-th batch, train loss: 0.45035088062286377:  59%|█████▉    | 142/241 [00:22<00:15,  6.19it/s]Epoch: 4, train for the 129-th batch, train loss: 0.5099862813949585:  85%|█████████▍ | 129/151 [00:22<00:04,  5.24it/s]Epoch: 6, train for the 87-th batch, train loss: 0.48757675290107727:  72%|████████▋   | 86/119 [00:13<00:05,  6.38it/s]evaluate for the 15-th batch, evaluate loss: 0.5204491019248962:  21%|███▊              | 14/66 [00:01<00:05,  9.22it/s]evaluate for the 15-th batch, evaluate loss: 0.5204491019248962:  23%|████              | 15/66 [00:01<00:07,  6.51it/s]Epoch: 6, train for the 87-th batch, train loss: 0.48757675290107727:  73%|████████▊   | 87/119 [00:13<00:05,  6.34it/s]Epoch: 3, train for the 143-th batch, train loss: 0.45035088062286377:  59%|█████▉    | 143/241 [00:22<00:16,  6.08it/s]Epoch: 5, train for the 119-th batch, train loss: 0.4860983192920685:  81%|████████▉  | 118/146 [00:17<00:04,  6.52it/s]Epoch: 5, train for the 119-th batch, train loss: 0.4860983192920685:  82%|████████▉  | 119/146 [00:17<00:04,  6.52it/s]evaluate for the 37-th batch, evaluate loss: 0.4682198166847229:  34%|█████▊           | 36/106 [00:08<00:15,  4.57it/s]evaluate for the 37-th batch, evaluate loss: 0.4682198166847229:  35%|█████▉           | 37/106 [00:08<00:18,  3.83it/s]evaluate for the 16-th batch, evaluate loss: 0.48717641830444336:  23%|███▊             | 15/66 [00:02<00:07,  6.51it/s]evaluate for the 16-th batch, evaluate loss: 0.48717641830444336:  24%|████             | 16/66 [00:02<00:07,  6.61it/s]Epoch: 3, train for the 144-th batch, train loss: 0.5409848690032959:  59%|██████▌    | 143/241 [00:22<00:16,  6.08it/s]Epoch: 6, train for the 88-th batch, train loss: 0.46910735964775085:  73%|████████▊   | 87/119 [00:13<00:05,  6.34it/s]Epoch: 6, train for the 88-th batch, train loss: 0.46910735964775085:  74%|████████▊   | 88/119 [00:13<00:05,  6.14it/s]Epoch: 3, train for the 144-th batch, train loss: 0.5409848690032959:  60%|██████▌    | 144/241 [00:22<00:16,  6.00it/s]Epoch: 4, train for the 130-th batch, train loss: 0.5186386704444885:  85%|█████████▍ | 129/151 [00:22<00:04,  5.24it/s]Epoch: 4, train for the 130-th batch, train loss: 0.5186386704444885:  86%|█████████▍ | 130/151 [00:22<00:04,  5.01it/s]Epoch: 5, train for the 120-th batch, train loss: 0.481452077627182:  82%|█████████▊  | 119/146 [00:17<00:04,  6.52it/s]Epoch: 5, train for the 120-th batch, train loss: 0.481452077627182:  82%|█████████▊  | 120/146 [00:17<00:04,  6.34it/s]Epoch: 3, train for the 145-th batch, train loss: 0.5927774906158447:  60%|██████▌    | 144/241 [00:22<00:16,  6.00it/s]Epoch: 3, train for the 145-th batch, train loss: 0.5927774906158447:  60%|██████▌    | 145/241 [00:22<00:15,  6.31it/s]evaluate for the 38-th batch, evaluate loss: 0.5542824268341064:  35%|█████▉           | 37/106 [00:08<00:18,  3.83it/s]evaluate for the 38-th batch, evaluate loss: 0.5542824268341064:  36%|██████           | 38/106 [00:08<00:16,  4.23it/s]Epoch: 6, train for the 89-th batch, train loss: 0.4828157424926758:  74%|█████████▌   | 88/119 [00:13<00:05,  6.14it/s]Epoch: 6, train for the 89-th batch, train loss: 0.4828157424926758:  75%|█████████▋   | 89/119 [00:13<00:05,  5.97it/s]Epoch: 4, train for the 131-th batch, train loss: 0.49420127272605896:  86%|████████▌ | 130/151 [00:22<00:04,  5.01it/s]evaluate for the 17-th batch, evaluate loss: 0.5185015201568604:  24%|████▎             | 16/66 [00:02<00:07,  6.61it/s]evaluate for the 17-th batch, evaluate loss: 0.5185015201568604:  26%|████▋             | 17/66 [00:02<00:08,  5.63it/s]Epoch: 4, train for the 131-th batch, train loss: 0.49420127272605896:  87%|████████▋ | 131/151 [00:22<00:04,  4.99it/s]Epoch: 3, train for the 146-th batch, train loss: 0.5708041191101074:  60%|██████▌    | 145/241 [00:22<00:15,  6.31it/s]Epoch: 5, train for the 121-th batch, train loss: 0.5022387504577637:  82%|█████████  | 120/146 [00:18<00:04,  6.34it/s]Epoch: 3, train for the 146-th batch, train loss: 0.5708041191101074:  61%|██████▋    | 146/241 [00:22<00:14,  6.40it/s]Epoch: 5, train for the 121-th batch, train loss: 0.5022387504577637:  83%|█████████  | 121/146 [00:18<00:04,  5.44it/s]Epoch: 6, train for the 90-th batch, train loss: 0.45817869901657104:  75%|████████▉   | 89/119 [00:13<00:05,  5.97it/s]evaluate for the 18-th batch, evaluate loss: 0.5264280438423157:  26%|████▋             | 17/66 [00:02<00:08,  5.63it/s]Epoch: 6, train for the 90-th batch, train loss: 0.45817869901657104:  76%|█████████   | 90/119 [00:13<00:04,  6.27it/s]evaluate for the 39-th batch, evaluate loss: 0.35186371207237244:  36%|█████▋          | 38/106 [00:08<00:16,  4.23it/s]evaluate for the 39-th batch, evaluate loss: 0.35186371207237244:  37%|█████▉          | 39/106 [00:08<00:15,  4.26it/s]Epoch: 4, train for the 132-th batch, train loss: 0.5025409460067749:  87%|█████████▌ | 131/151 [00:23<00:04,  4.99it/s]Epoch: 4, train for the 132-th batch, train loss: 0.5025409460067749:  87%|█████████▌ | 132/151 [00:23<00:03,  5.18it/s]evaluate for the 19-th batch, evaluate loss: 0.49502140283584595:  26%|████▍            | 17/66 [00:02<00:08,  5.63it/s]evaluate for the 19-th batch, evaluate loss: 0.49502140283584595:  29%|████▉            | 19/66 [00:02<00:07,  6.67it/s]Epoch: 3, train for the 147-th batch, train loss: 0.6093019247055054:  61%|██████▋    | 146/241 [00:22<00:14,  6.40it/s]Epoch: 5, train for the 122-th batch, train loss: 0.5331858992576599:  83%|█████████  | 121/146 [00:18<00:04,  5.44it/s]Epoch: 3, train for the 147-th batch, train loss: 0.6093019247055054:  61%|██████▋    | 147/241 [00:22<00:15,  5.99it/s]Epoch: 6, train for the 91-th batch, train loss: 0.4265642464160919:  76%|█████████▊   | 90/119 [00:13<00:04,  6.27it/s]Epoch: 5, train for the 122-th batch, train loss: 0.5331858992576599:  84%|█████████▏ | 122/146 [00:18<00:04,  5.40it/s]Epoch: 6, train for the 91-th batch, train loss: 0.4265642464160919:  76%|█████████▉   | 91/119 [00:13<00:04,  5.98it/s]evaluate for the 40-th batch, evaluate loss: 0.3360404074192047:  37%|██████▎          | 39/106 [00:09<00:15,  4.26it/s]evaluate for the 40-th batch, evaluate loss: 0.3360404074192047:  38%|██████▍          | 40/106 [00:09<00:13,  4.76it/s]Epoch: 4, train for the 133-th batch, train loss: 0.4801226556301117:  87%|█████████▌ | 132/151 [00:23<00:03,  5.18it/s]evaluate for the 20-th batch, evaluate loss: 0.5617955923080444:  29%|█████▏            | 19/66 [00:02<00:07,  6.67it/s]evaluate for the 20-th batch, evaluate loss: 0.5617955923080444:  30%|█████▍            | 20/66 [00:02<00:06,  6.87it/s]Epoch: 4, train for the 133-th batch, train loss: 0.4801226556301117:  88%|█████████▋ | 133/151 [00:23<00:03,  5.31it/s]Epoch: 3, train for the 148-th batch, train loss: 0.5151057839393616:  61%|██████▋    | 147/241 [00:22<00:15,  5.99it/s]Epoch: 5, train for the 123-th batch, train loss: 0.5283476114273071:  84%|█████████▏ | 122/146 [00:18<00:04,  5.40it/s]Epoch: 5, train for the 123-th batch, train loss: 0.5283476114273071:  84%|█████████▎ | 123/146 [00:18<00:04,  5.59it/s]Epoch: 3, train for the 148-th batch, train loss: 0.5151057839393616:  61%|██████▊    | 148/241 [00:23<00:15,  5.90it/s]Epoch: 6, train for the 92-th batch, train loss: 0.47610601782798767:  76%|█████████▏  | 91/119 [00:14<00:04,  5.98it/s]Epoch: 6, train for the 92-th batch, train loss: 0.47610601782798767:  77%|█████████▎  | 92/119 [00:14<00:04,  5.90it/s]evaluate for the 21-th batch, evaluate loss: 0.5494835376739502:  30%|█████▍            | 20/66 [00:02<00:06,  6.87it/s]evaluate for the 21-th batch, evaluate loss: 0.5494835376739502:  32%|█████▋            | 21/66 [00:02<00:06,  7.29it/s]Epoch: 4, train for the 134-th batch, train loss: 0.4908273220062256:  88%|█████████▋ | 133/151 [00:23<00:03,  5.31it/s]Epoch: 4, train for the 134-th batch, train loss: 0.4908273220062256:  89%|█████████▊ | 134/151 [00:23<00:03,  5.56it/s]Epoch: 3, train for the 149-th batch, train loss: 0.3779677152633667:  61%|██████▊    | 148/241 [00:23<00:15,  5.90it/s]Epoch: 3, train for the 149-th batch, train loss: 0.3779677152633667:  62%|██████▊    | 149/241 [00:23<00:15,  5.87it/s]evaluate for the 41-th batch, evaluate loss: 0.48019906878471375:  38%|██████          | 40/106 [00:09<00:13,  4.76it/s]evaluate for the 41-th batch, evaluate loss: 0.48019906878471375:  39%|██████▏         | 41/106 [00:09<00:15,  4.18it/s]evaluate for the 22-th batch, evaluate loss: 0.5397082567214966:  32%|█████▋            | 21/66 [00:02<00:06,  7.29it/s]evaluate for the 22-th batch, evaluate loss: 0.5397082567214966:  33%|██████            | 22/66 [00:02<00:06,  7.08it/s]Epoch: 6, train for the 93-th batch, train loss: 0.371230810880661:  77%|██████████▊   | 92/119 [00:14<00:04,  5.90it/s]Epoch: 5, train for the 124-th batch, train loss: 0.5449379682540894:  84%|█████████▎ | 123/146 [00:18<00:04,  5.59it/s]Epoch: 6, train for the 93-th batch, train loss: 0.371230810880661:  78%|██████████▉   | 93/119 [00:14<00:04,  5.67it/s]Epoch: 5, train for the 124-th batch, train loss: 0.5449379682540894:  85%|█████████▎ | 124/146 [00:18<00:04,  5.26it/s]Epoch: 4, train for the 135-th batch, train loss: 0.47653552889823914:  89%|████████▊ | 134/151 [00:23<00:03,  5.56it/s]Epoch: 4, train for the 135-th batch, train loss: 0.47653552889823914:  89%|████████▉ | 135/151 [00:23<00:02,  5.49it/s]Epoch: 3, train for the 150-th batch, train loss: 0.39767447113990784:  62%|██████▏   | 149/241 [00:23<00:15,  5.87it/s]Epoch: 3, train for the 150-th batch, train loss: 0.39767447113990784:  62%|██████▏   | 150/241 [00:23<00:14,  6.24it/s]evaluate for the 23-th batch, evaluate loss: 0.5362952351570129:  33%|██████            | 22/66 [00:03<00:06,  7.08it/s]evaluate for the 23-th batch, evaluate loss: 0.5362952351570129:  35%|██████▎           | 23/66 [00:03<00:06,  6.91it/s]evaluate for the 42-th batch, evaluate loss: 0.3930439054965973:  39%|██████▌          | 41/106 [00:09<00:15,  4.18it/s]evaluate for the 42-th batch, evaluate loss: 0.3930439054965973:  40%|██████▋          | 42/106 [00:09<00:13,  4.59it/s]Epoch: 5, train for the 125-th batch, train loss: 0.49012941122055054:  85%|████████▍ | 124/146 [00:18<00:04,  5.26it/s]Epoch: 5, train for the 125-th batch, train loss: 0.49012941122055054:  86%|████████▌ | 125/146 [00:18<00:03,  5.57it/s]Epoch: 3, train for the 151-th batch, train loss: 0.35322123765945435:  62%|██████▏   | 150/241 [00:23<00:14,  6.24it/s]Epoch: 4, train for the 136-th batch, train loss: 0.4655194878578186:  89%|█████████▊ | 135/151 [00:23<00:02,  5.49it/s]Epoch: 3, train for the 151-th batch, train loss: 0.35322123765945435:  63%|██████▎   | 151/241 [00:23<00:15,  5.78it/s]Epoch: 6, train for the 94-th batch, train loss: 0.4116993546485901:  78%|██████████▏  | 93/119 [00:14<00:04,  5.67it/s]Epoch: 4, train for the 136-th batch, train loss: 0.4655194878578186:  90%|█████████▉ | 136/151 [00:23<00:03,  4.97it/s]Epoch: 6, train for the 94-th batch, train loss: 0.4116993546485901:  79%|██████████▎  | 94/119 [00:14<00:05,  4.56it/s]Epoch: 5, train for the 126-th batch, train loss: 0.5126718878746033:  86%|█████████▍ | 125/146 [00:19<00:03,  5.57it/s]Epoch: 5, train for the 126-th batch, train loss: 0.5126718878746033:  86%|█████████▍ | 126/146 [00:19<00:03,  5.41it/s]Epoch: 3, train for the 152-th batch, train loss: 0.3221145570278168:  63%|██████▉    | 151/241 [00:23<00:15,  5.78it/s]Epoch: 3, train for the 152-th batch, train loss: 0.3221145570278168:  63%|██████▉    | 152/241 [00:23<00:14,  6.19it/s]evaluate for the 24-th batch, evaluate loss: 0.5492941737174988:  35%|██████▎           | 23/66 [00:03<00:06,  6.91it/s]evaluate for the 24-th batch, evaluate loss: 0.5492941737174988:  36%|██████▌           | 24/66 [00:03<00:08,  5.11it/s]evaluate for the 43-th batch, evaluate loss: 0.26981401443481445:  40%|██████▎         | 42/106 [00:09<00:13,  4.59it/s]evaluate for the 43-th batch, evaluate loss: 0.26981401443481445:  41%|██████▍         | 43/106 [00:09<00:15,  4.00it/s]Epoch: 6, train for the 95-th batch, train loss: 0.400948703289032:  79%|███████████   | 94/119 [00:14<00:05,  4.56it/s]Epoch: 6, train for the 95-th batch, train loss: 0.400948703289032:  80%|███████████▏  | 95/119 [00:14<00:05,  4.69it/s]Epoch: 5, train for the 127-th batch, train loss: 0.5124983191490173:  86%|█████████▍ | 126/146 [00:19<00:03,  5.41it/s]evaluate for the 25-th batch, evaluate loss: 0.5651630163192749:  36%|██████▌           | 24/66 [00:03<00:08,  5.11it/s]Epoch: 3, train for the 153-th batch, train loss: 0.37894073128700256:  63%|██████▎   | 152/241 [00:23<00:14,  6.19it/s]Epoch: 5, train for the 127-th batch, train loss: 0.5124983191490173:  87%|█████████▌ | 127/146 [00:19<00:03,  5.26it/s]Epoch: 3, train for the 153-th batch, train loss: 0.37894073128700256:  63%|██████▎   | 153/241 [00:23<00:13,  6.57it/s]Epoch: 4, train for the 137-th batch, train loss: 0.5207615494728088:  90%|█████████▉ | 136/151 [00:24<00:03,  4.97it/s]evaluate for the 44-th batch, evaluate loss: 0.3035585284233093:  41%|██████▉          | 43/106 [00:10<00:15,  4.00it/s]evaluate for the 44-th batch, evaluate loss: 0.3035585284233093:  42%|███████          | 44/106 [00:10<00:13,  4.44it/s]Epoch: 6, train for the 96-th batch, train loss: 0.444903165102005:  80%|███████████▏  | 95/119 [00:14<00:05,  4.69it/s]Epoch: 6, train for the 96-th batch, train loss: 0.444903165102005:  81%|███████████▎  | 96/119 [00:14<00:04,  5.36it/s]Epoch: 4, train for the 137-th batch, train loss: 0.5207615494728088:  91%|█████████▉ | 137/151 [00:24<00:03,  4.13it/s]Epoch: 3, train for the 154-th batch, train loss: 0.2774701416492462:  63%|██████▉    | 153/241 [00:23<00:13,  6.57it/s]Epoch: 5, train for the 128-th batch, train loss: 0.4821501076221466:  87%|█████████▌ | 127/146 [00:19<00:03,  5.26it/s]Epoch: 3, train for the 154-th batch, train loss: 0.2774701416492462:  64%|███████    | 154/241 [00:23<00:12,  6.73it/s]Epoch: 5, train for the 128-th batch, train loss: 0.4821501076221466:  88%|█████████▋ | 128/146 [00:19<00:03,  5.57it/s]Epoch: 6, train for the 97-th batch, train loss: 0.4487365186214447:  81%|██████████▍  | 96/119 [00:14<00:04,  5.36it/s]Epoch: 6, train for the 97-th batch, train loss: 0.4487365186214447:  82%|██████████▌  | 97/119 [00:15<00:03,  5.95it/s]evaluate for the 26-th batch, evaluate loss: 0.5630620718002319:  36%|██████▌           | 24/66 [00:03<00:08,  5.11it/s]evaluate for the 26-th batch, evaluate loss: 0.5630620718002319:  39%|███████           | 26/66 [00:03<00:07,  5.54it/s]Epoch: 4, train for the 138-th batch, train loss: 0.5485045909881592:  91%|█████████▉ | 137/151 [00:24<00:03,  4.13it/s]Epoch: 4, train for the 138-th batch, train loss: 0.5485045909881592:  91%|██████████ | 138/151 [00:24<00:02,  4.47it/s]Epoch: 3, train for the 155-th batch, train loss: 0.3104434311389923:  64%|███████    | 154/241 [00:24<00:12,  6.73it/s]Epoch: 5, train for the 129-th batch, train loss: 0.5516263246536255:  88%|█████████▋ | 128/146 [00:19<00:03,  5.57it/s]Epoch: 3, train for the 155-th batch, train loss: 0.3104434311389923:  64%|███████    | 155/241 [00:24<00:12,  6.74it/s]Epoch: 5, train for the 129-th batch, train loss: 0.5516263246536255:  88%|█████████▋ | 129/146 [00:19<00:02,  5.91it/s]evaluate for the 27-th batch, evaluate loss: 0.5667679905891418:  39%|███████           | 26/66 [00:03<00:07,  5.54it/s]Epoch: 6, train for the 98-th batch, train loss: 0.3789356052875519:  82%|██████████▌  | 97/119 [00:15<00:03,  5.95it/s]Epoch: 6, train for the 98-th batch, train loss: 0.3789356052875519:  82%|██████████▋  | 98/119 [00:15<00:03,  6.43it/s]evaluate for the 45-th batch, evaluate loss: 0.4271649122238159:  42%|███████          | 44/106 [00:10<00:13,  4.44it/s]evaluate for the 45-th batch, evaluate loss: 0.4271649122238159:  42%|███████▏         | 45/106 [00:10<00:14,  4.08it/s]Epoch: 4, train for the 139-th batch, train loss: 0.46927279233932495:  91%|█████████▏| 138/151 [00:24<00:02,  4.47it/s]evaluate for the 28-th batch, evaluate loss: 0.575816810131073:  39%|███████▍           | 26/66 [00:03<00:07,  5.54it/s]evaluate for the 28-th batch, evaluate loss: 0.575816810131073:  42%|████████           | 28/66 [00:03<00:05,  6.37it/s]Epoch: 3, train for the 156-th batch, train loss: 0.2639123201370239:  64%|███████    | 155/241 [00:24<00:12,  6.74it/s]Epoch: 5, train for the 130-th batch, train loss: 0.45666730403900146:  88%|████████▊ | 129/146 [00:19<00:02,  5.91it/s]Epoch: 4, train for the 139-th batch, train loss: 0.46927279233932495:  92%|█████████▏| 139/151 [00:24<00:02,  4.52it/s]Epoch: 5, train for the 130-th batch, train loss: 0.45666730403900146:  89%|████████▉ | 130/146 [00:19<00:02,  5.72it/s]Epoch: 6, train for the 99-th batch, train loss: 0.41912245750427246:  82%|█████████▉  | 98/119 [00:15<00:03,  6.43it/s]Epoch: 3, train for the 156-th batch, train loss: 0.2639123201370239:  65%|███████    | 156/241 [00:24<00:14,  6.03it/s]Epoch: 6, train for the 99-th batch, train loss: 0.41912245750427246:  83%|█████████▉  | 99/119 [00:15<00:03,  6.18it/s]evaluate for the 46-th batch, evaluate loss: 0.4421581029891968:  42%|███████▏         | 45/106 [00:10<00:14,  4.08it/s]evaluate for the 46-th batch, evaluate loss: 0.4421581029891968:  43%|███████▍         | 46/106 [00:10<00:15,  3.90it/s]evaluate for the 29-th batch, evaluate loss: 0.5089604258537292:  42%|███████▋          | 28/66 [00:04<00:05,  6.37it/s]evaluate for the 29-th batch, evaluate loss: 0.5089604258537292:  44%|███████▉          | 29/66 [00:04<00:06,  6.13it/s]Epoch: 4, train for the 140-th batch, train loss: 0.4140129089355469:  92%|██████████▏| 139/151 [00:24<00:02,  4.52it/s]Epoch: 3, train for the 157-th batch, train loss: 0.25084295868873596:  65%|██████▍   | 156/241 [00:24<00:14,  6.03it/s]Epoch: 4, train for the 140-th batch, train loss: 0.4140129089355469:  93%|██████████▏| 140/151 [00:24<00:02,  4.46it/s]Epoch: 3, train for the 157-th batch, train loss: 0.25084295868873596:  65%|██████▌   | 157/241 [00:24<00:15,  5.48it/s]Epoch: 6, train for the 100-th batch, train loss: 0.4339020550251007:  83%|█████████▉  | 99/119 [00:15<00:03,  6.18it/s]Epoch: 5, train for the 131-th batch, train loss: 0.5407232642173767:  89%|█████████▊ | 130/146 [00:19<00:02,  5.72it/s]Epoch: 6, train for the 100-th batch, train loss: 0.4339020550251007:  84%|█████████▏ | 100/119 [00:15<00:03,  5.41it/s]Epoch: 5, train for the 131-th batch, train loss: 0.5407232642173767:  90%|█████████▊ | 131/146 [00:19<00:03,  4.94it/s]evaluate for the 30-th batch, evaluate loss: 0.547029435634613:  44%|████████▎          | 29/66 [00:04<00:06,  6.13it/s]evaluate for the 30-th batch, evaluate loss: 0.547029435634613:  45%|████████▋          | 30/66 [00:04<00:06,  5.58it/s]evaluate for the 47-th batch, evaluate loss: 0.25725239515304565:  43%|██████▉         | 46/106 [00:10<00:15,  3.90it/s]evaluate for the 47-th batch, evaluate loss: 0.25725239515304565:  44%|███████         | 47/106 [00:10<00:14,  4.00it/s]Epoch: 3, train for the 158-th batch, train loss: 0.22371377050876617:  65%|██████▌   | 157/241 [00:24<00:15,  5.48it/s]Epoch: 4, train for the 141-th batch, train loss: 0.49941718578338623:  93%|█████████▎| 140/151 [00:25<00:02,  4.46it/s]Epoch: 5, train for the 132-th batch, train loss: 0.4584227204322815:  90%|█████████▊ | 131/146 [00:20<00:03,  4.94it/s]Epoch: 3, train for the 158-th batch, train loss: 0.22371377050876617:  66%|██████▌   | 158/241 [00:24<00:15,  5.24it/s]Epoch: 4, train for the 141-th batch, train loss: 0.49941718578338623:  93%|█████████▎| 141/151 [00:25<00:02,  4.51it/s]Epoch: 5, train for the 132-th batch, train loss: 0.4584227204322815:  90%|█████████▉ | 132/146 [00:20<00:02,  5.11it/s]Epoch: 6, train for the 101-th batch, train loss: 0.434701144695282:  84%|██████████  | 100/119 [00:15<00:03,  5.41it/s]Epoch: 6, train for the 101-th batch, train loss: 0.434701144695282:  85%|██████████▏ | 101/119 [00:15<00:03,  5.13it/s]evaluate for the 48-th batch, evaluate loss: 0.37391701340675354:  44%|███████         | 47/106 [00:10<00:14,  4.00it/s]evaluate for the 48-th batch, evaluate loss: 0.37391701340675354:  45%|███████▏        | 48/106 [00:10<00:13,  4.37it/s]Epoch: 5, train for the 133-th batch, train loss: 0.4960266649723053:  90%|█████████▉ | 132/146 [00:20<00:02,  5.11it/s]Epoch: 3, train for the 159-th batch, train loss: 0.22205355763435364:  66%|██████▌   | 158/241 [00:24<00:15,  5.24it/s]Epoch: 5, train for the 133-th batch, train loss: 0.4960266649723053:  91%|██████████ | 133/146 [00:20<00:02,  5.35it/s]Epoch: 6, train for the 102-th batch, train loss: 0.4439752995967865:  85%|█████████▎ | 101/119 [00:15<00:03,  5.13it/s]Epoch: 6, train for the 102-th batch, train loss: 0.4439752995967865:  86%|█████████▍ | 102/119 [00:15<00:03,  5.49it/s]Epoch: 4, train for the 142-th batch, train loss: 0.486785352230072:  93%|███████████▏| 141/151 [00:25<00:02,  4.51it/s]Epoch: 3, train for the 159-th batch, train loss: 0.22205355763435364:  66%|██████▌   | 159/241 [00:24<00:15,  5.17it/s]evaluate for the 31-th batch, evaluate loss: 0.5343669056892395:  45%|████████▏         | 30/66 [00:04<00:06,  5.58it/s]evaluate for the 31-th batch, evaluate loss: 0.5343669056892395:  47%|████████▍         | 31/66 [00:04<00:07,  4.98it/s]Epoch: 4, train for the 142-th batch, train loss: 0.486785352230072:  94%|███████████▎| 142/151 [00:25<00:02,  4.45it/s]evaluate for the 32-th batch, evaluate loss: 0.543559193611145:  47%|████████▉          | 31/66 [00:04<00:07,  4.98it/s]Epoch: 5, train for the 134-th batch, train loss: 0.5026218891143799:  91%|██████████ | 133/146 [00:20<00:02,  5.35it/s]Epoch: 3, train for the 160-th batch, train loss: 0.5733068585395813:  66%|███████▎   | 159/241 [00:25<00:15,  5.17it/s]Epoch: 5, train for the 134-th batch, train loss: 0.5026218891143799:  92%|██████████ | 134/146 [00:20<00:02,  5.49it/s]Epoch: 6, train for the 103-th batch, train loss: 0.4206184148788452:  86%|█████████▍ | 102/119 [00:16<00:03,  5.49it/s]Epoch: 6, train for the 103-th batch, train loss: 0.4206184148788452:  87%|█████████▌ | 103/119 [00:16<00:02,  5.67it/s]Epoch: 3, train for the 160-th batch, train loss: 0.5733068585395813:  66%|███████▎   | 160/241 [00:25<00:14,  5.43it/s]Epoch: 4, train for the 143-th batch, train loss: 0.4267488121986389:  94%|██████████▎| 142/151 [00:25<00:02,  4.45it/s]evaluate for the 49-th batch, evaluate loss: 0.4977045953273773:  45%|███████▋         | 48/106 [00:11<00:13,  4.37it/s]evaluate for the 49-th batch, evaluate loss: 0.4977045953273773:  46%|███████▊         | 49/106 [00:11<00:13,  4.23it/s]Epoch: 4, train for the 143-th batch, train loss: 0.4267488121986389:  95%|██████████▍| 143/151 [00:25<00:01,  4.82it/s]evaluate for the 33-th batch, evaluate loss: 0.56461101770401:  47%|█████████▍          | 31/66 [00:04<00:07,  4.98it/s]evaluate for the 33-th batch, evaluate loss: 0.56461101770401:  50%|██████████          | 33/66 [00:04<00:05,  6.15it/s]Epoch: 3, train for the 161-th batch, train loss: 0.31829991936683655:  66%|██████▋   | 160/241 [00:25<00:14,  5.43it/s]Epoch: 5, train for the 135-th batch, train loss: 0.4640001654624939:  92%|██████████ | 134/146 [00:20<00:02,  5.49it/s]evaluate for the 34-th batch, evaluate loss: 0.5181386470794678:  50%|█████████         | 33/66 [00:04<00:05,  6.15it/s]Epoch: 6, train for the 104-th batch, train loss: 0.44756460189819336:  87%|████████▋ | 103/119 [00:16<00:02,  5.67it/s]Epoch: 5, train for the 135-th batch, train loss: 0.4640001654624939:  92%|██████████▏| 135/146 [00:20<00:02,  5.46it/s]Epoch: 3, train for the 161-th batch, train loss: 0.31829991936683655:  67%|██████▋   | 161/241 [00:25<00:14,  5.64it/s]Epoch: 6, train for the 104-th batch, train loss: 0.44756460189819336:  87%|████████▋ | 104/119 [00:16<00:02,  5.65it/s]Epoch: 4, train for the 144-th batch, train loss: 0.4286207854747772:  95%|██████████▍| 143/151 [00:25<00:01,  4.82it/s]evaluate for the 50-th batch, evaluate loss: 0.3974023461341858:  46%|███████▊         | 49/106 [00:11<00:13,  4.23it/s]evaluate for the 50-th batch, evaluate loss: 0.3974023461341858:  47%|████████         | 50/106 [00:11<00:12,  4.32it/s]Epoch: 4, train for the 144-th batch, train loss: 0.4286207854747772:  95%|██████████▍| 144/151 [00:25<00:01,  4.85it/s]Epoch: 5, train for the 136-th batch, train loss: 0.4892769455909729:  92%|██████████▏| 135/146 [00:20<00:02,  5.46it/s]Epoch: 3, train for the 162-th batch, train loss: 0.17083869874477386:  67%|██████▋   | 161/241 [00:25<00:14,  5.64it/s]Epoch: 5, train for the 136-th batch, train loss: 0.4892769455909729:  93%|██████████▏| 136/146 [00:20<00:01,  5.73it/s]Epoch: 3, train for the 162-th batch, train loss: 0.17083869874477386:  67%|██████▋   | 162/241 [00:25<00:13,  5.72it/s]Epoch: 4, train for the 145-th batch, train loss: 0.4338493347167969:  95%|██████████▍| 144/151 [00:25<00:01,  4.85it/s]Epoch: 4, train for the 145-th batch, train loss: 0.4338493347167969:  96%|██████████▌| 145/151 [00:25<00:01,  5.23it/s]evaluate for the 35-th batch, evaluate loss: 0.560737669467926:  50%|█████████▌         | 33/66 [00:05<00:05,  6.15it/s]evaluate for the 35-th batch, evaluate loss: 0.560737669467926:  53%|██████████         | 35/66 [00:05<00:05,  6.03it/s]Epoch: 5, train for the 137-th batch, train loss: 0.4707121253013611:  93%|██████████▏| 136/146 [00:20<00:01,  5.73it/s]Epoch: 3, train for the 163-th batch, train loss: 0.16137240827083588:  67%|██████▋   | 162/241 [00:25<00:13,  5.72it/s]Epoch: 6, train for the 105-th batch, train loss: 0.42084014415740967:  87%|████████▋ | 104/119 [00:16<00:02,  5.65it/s]Epoch: 5, train for the 137-th batch, train loss: 0.4707121253013611:  94%|██████████▎| 137/146 [00:20<00:01,  5.91it/s]Epoch: 6, train for the 105-th batch, train loss: 0.42084014415740967:  88%|████████▊ | 105/119 [00:16<00:03,  4.65it/s]evaluate for the 51-th batch, evaluate loss: 0.44927704334259033:  47%|███████▌        | 50/106 [00:11<00:12,  4.32it/s]evaluate for the 51-th batch, evaluate loss: 0.44927704334259033:  48%|███████▋        | 51/106 [00:11<00:12,  4.25it/s]Epoch: 3, train for the 163-th batch, train loss: 0.16137240827083588:  68%|██████▊   | 163/241 [00:25<00:13,  5.88it/s]evaluate for the 36-th batch, evaluate loss: 0.5533750057220459:  53%|█████████▌        | 35/66 [00:05<00:05,  6.03it/s]evaluate for the 36-th batch, evaluate loss: 0.5533750057220459:  55%|█████████▊        | 36/66 [00:05<00:04,  6.37it/s]Epoch: 4, train for the 146-th batch, train loss: 0.5039127469062805:  96%|██████████▌| 145/151 [00:26<00:01,  5.23it/s]Epoch: 4, train for the 146-th batch, train loss: 0.5039127469062805:  97%|██████████▋| 146/151 [00:26<00:00,  5.44it/s]Epoch: 5, train for the 138-th batch, train loss: 0.5060195326805115:  94%|██████████▎| 137/146 [00:21<00:01,  5.91it/s]Epoch: 3, train for the 164-th batch, train loss: 0.14450015127658844:  68%|██████▊   | 163/241 [00:25<00:13,  5.88it/s]Epoch: 6, train for the 106-th batch, train loss: 0.43306976556777954:  88%|████████▊ | 105/119 [00:16<00:03,  4.65it/s]evaluate for the 52-th batch, evaluate loss: 0.377169132232666:  48%|████████▋         | 51/106 [00:11<00:12,  4.25it/s]evaluate for the 52-th batch, evaluate loss: 0.377169132232666:  49%|████████▊         | 52/106 [00:11<00:11,  4.63it/s]Epoch: 5, train for the 138-th batch, train loss: 0.5060195326805115:  95%|██████████▍| 138/146 [00:21<00:01,  5.76it/s]Epoch: 6, train for the 106-th batch, train loss: 0.43306976556777954:  89%|████████▉ | 106/119 [00:16<00:02,  4.86it/s]Epoch: 3, train for the 164-th batch, train loss: 0.14450015127658844:  68%|██████▊   | 164/241 [00:25<00:13,  5.86it/s]Epoch: 3, train for the 165-th batch, train loss: 0.2257177084684372:  68%|███████▍   | 164/241 [00:25<00:13,  5.86it/s]evaluate for the 37-th batch, evaluate loss: 0.5710495114326477:  55%|█████████▊        | 36/66 [00:05<00:04,  6.37it/s]evaluate for the 37-th batch, evaluate loss: 0.5710495114326477:  56%|██████████        | 37/66 [00:05<00:05,  5.53it/s]Epoch: 3, train for the 165-th batch, train loss: 0.2257177084684372:  68%|███████▌   | 165/241 [00:25<00:12,  6.33it/s]Epoch: 5, train for the 139-th batch, train loss: 0.5044616460800171:  95%|██████████▍| 138/146 [00:21<00:01,  5.76it/s]Epoch: 5, train for the 139-th batch, train loss: 0.5044616460800171:  95%|██████████▍| 139/146 [00:21<00:01,  6.07it/s]Epoch: 6, train for the 107-th batch, train loss: 0.48124825954437256:  89%|████████▉ | 106/119 [00:16<00:02,  4.86it/s]Epoch: 6, train for the 107-th batch, train loss: 0.48124825954437256:  90%|████████▉ | 107/119 [00:16<00:02,  5.06it/s]evaluate for the 38-th batch, evaluate loss: 0.5039767622947693:  56%|██████████        | 37/66 [00:05<00:05,  5.53it/s]Epoch: 6, train for the 108-th batch, train loss: 0.37916940450668335:  90%|████████▉ | 107/119 [00:17<00:02,  5.06it/s]Epoch: 3, train for the 166-th batch, train loss: 0.2062944769859314:  68%|███████▌   | 165/241 [00:26<00:12,  6.33it/s]Epoch: 4, train for the 147-th batch, train loss: 0.48541954159736633:  97%|█████████▋| 146/151 [00:26<00:00,  5.44it/s]evaluate for the 53-th batch, evaluate loss: 0.49903616309165955:  49%|███████▊        | 52/106 [00:12<00:11,  4.63it/s]evaluate for the 53-th batch, evaluate loss: 0.49903616309165955:  50%|████████        | 53/106 [00:12<00:12,  4.14it/s]Epoch: 6, train for the 108-th batch, train loss: 0.37916940450668335:  91%|█████████ | 108/119 [00:17<00:01,  5.71it/s]Epoch: 3, train for the 166-th batch, train loss: 0.2062944769859314:  69%|███████▌   | 166/241 [00:26<00:12,  6.20it/s]Epoch: 4, train for the 147-th batch, train loss: 0.48541954159736633:  97%|█████████▋| 147/151 [00:26<00:01,  3.98it/s]evaluate for the 39-th batch, evaluate loss: 0.5426813960075378:  56%|██████████        | 37/66 [00:05<00:05,  5.53it/s]evaluate for the 39-th batch, evaluate loss: 0.5426813960075378:  59%|██████████▋       | 39/66 [00:05<00:04,  6.52it/s]Epoch: 5, train for the 140-th batch, train loss: 0.4837648272514343:  95%|██████████▍| 139/146 [00:21<00:01,  6.07it/s]Epoch: 5, train for the 140-th batch, train loss: 0.4837648272514343:  96%|██████████▌| 140/146 [00:21<00:01,  5.16it/s]evaluate for the 54-th batch, evaluate loss: 0.33495932817459106:  50%|████████        | 53/106 [00:12<00:12,  4.14it/s]evaluate for the 54-th batch, evaluate loss: 0.33495932817459106:  51%|████████▏       | 54/106 [00:12<00:11,  4.55it/s]Epoch: 3, train for the 167-th batch, train loss: 0.14649224281311035:  69%|██████▉   | 166/241 [00:26<00:12,  6.20it/s]evaluate for the 40-th batch, evaluate loss: 0.5181211233139038:  59%|██████████▋       | 39/66 [00:05<00:04,  6.52it/s]evaluate for the 40-th batch, evaluate loss: 0.5181211233139038:  61%|██████████▉       | 40/66 [00:05<00:03,  6.84it/s]Epoch: 6, train for the 109-th batch, train loss: 0.4519364833831787:  91%|█████████▉ | 108/119 [00:17<00:01,  5.71it/s]Epoch: 6, train for the 109-th batch, train loss: 0.4519364833831787:  92%|██████████ | 109/119 [00:17<00:01,  5.59it/s]Epoch: 3, train for the 167-th batch, train loss: 0.14649224281311035:  69%|██████▉   | 167/241 [00:26<00:12,  5.88it/s]Epoch: 4, train for the 148-th batch, train loss: 0.4449428915977478:  97%|██████████▋| 147/151 [00:26<00:01,  3.98it/s]Epoch: 4, train for the 148-th batch, train loss: 0.4449428915977478:  98%|██████████▊| 148/151 [00:26<00:00,  4.13it/s]Epoch: 5, train for the 141-th batch, train loss: 0.4922541379928589:  96%|██████████▌| 140/146 [00:21<00:01,  5.16it/s]Epoch: 5, train for the 141-th batch, train loss: 0.4922541379928589:  97%|██████████▌| 141/146 [00:21<00:00,  5.52it/s]Epoch: 6, train for the 110-th batch, train loss: 0.41689765453338623:  92%|█████████▏| 109/119 [00:17<00:01,  5.59it/s]Epoch: 6, train for the 110-th batch, train loss: 0.41689765453338623:  92%|█████████▏| 110/119 [00:17<00:01,  5.94it/s]Epoch: 3, train for the 168-th batch, train loss: 0.10870780795812607:  69%|██████▉   | 167/241 [00:26<00:12,  5.88it/s]Epoch: 3, train for the 168-th batch, train loss: 0.10870780795812607:  70%|██████▉   | 168/241 [00:26<00:12,  5.96it/s]Epoch: 4, train for the 149-th batch, train loss: 0.4524170458316803:  98%|██████████▊| 148/151 [00:26<00:00,  4.13it/s]Epoch: 5, train for the 142-th batch, train loss: 0.46584925055503845:  97%|█████████▋| 141/146 [00:21<00:00,  5.52it/s]Epoch: 5, train for the 142-th batch, train loss: 0.46584925055503845:  97%|█████████▋| 142/146 [00:21<00:00,  5.87it/s]evaluate for the 41-th batch, evaluate loss: 0.498680979013443:  61%|███████████▌       | 40/66 [00:06<00:03,  6.84it/s]evaluate for the 41-th batch, evaluate loss: 0.498680979013443:  62%|███████████▊       | 41/66 [00:06<00:04,  5.99it/s]Epoch: 4, train for the 149-th batch, train loss: 0.4524170458316803:  99%|██████████▊| 149/151 [00:26<00:00,  4.53it/s]Epoch: 6, train for the 111-th batch, train loss: 0.5008755922317505:  92%|██████████▏| 110/119 [00:17<00:01,  5.94it/s]Epoch: 6, train for the 111-th batch, train loss: 0.5008755922317505:  93%|██████████▎| 111/119 [00:17<00:01,  6.12it/s]evaluate for the 55-th batch, evaluate loss: 0.24229024350643158:  51%|████████▏       | 54/106 [00:12<00:11,  4.55it/s]evaluate for the 55-th batch, evaluate loss: 0.24229024350643158:  52%|████████▎       | 55/106 [00:12<00:12,  3.95it/s]Epoch: 3, train for the 169-th batch, train loss: 0.5494769811630249:  70%|███████▋   | 168/241 [00:26<00:12,  5.96it/s]Epoch: 3, train for the 169-th batch, train loss: 0.5494769811630249:  70%|███████▋   | 169/241 [00:26<00:11,  6.03it/s]Epoch: 5, train for the 143-th batch, train loss: 0.49348974227905273:  97%|█████████▋| 142/146 [00:22<00:00,  5.87it/s]Epoch: 4, train for the 150-th batch, train loss: 0.4846380054950714:  99%|██████████▊| 149/151 [00:26<00:00,  4.53it/s]evaluate for the 42-th batch, evaluate loss: 0.5327110290527344:  62%|███████████▏      | 41/66 [00:06<00:04,  5.99it/s]evaluate for the 42-th batch, evaluate loss: 0.5327110290527344:  64%|███████████▍      | 42/66 [00:06<00:03,  6.13it/s]Epoch: 5, train for the 143-th batch, train loss: 0.49348974227905273:  98%|█████████▊| 143/146 [00:22<00:00,  5.96it/s]Epoch: 4, train for the 150-th batch, train loss: 0.4846380054950714:  99%|██████████▉| 150/151 [00:27<00:00,  4.77it/s]Epoch: 6, train for the 112-th batch, train loss: 0.4169706106185913:  93%|██████████▎| 111/119 [00:17<00:01,  6.12it/s]Epoch: 6, train for the 112-th batch, train loss: 0.4169706106185913:  94%|██████████▎| 112/119 [00:17<00:01,  6.25it/s]Epoch: 3, train for the 170-th batch, train loss: 0.6070725321769714:  70%|███████▋   | 169/241 [00:26<00:11,  6.03it/s]Epoch: 3, train for the 170-th batch, train loss: 0.6070725321769714:  71%|███████▊   | 170/241 [00:26<00:11,  6.08it/s]Epoch: 5, train for the 144-th batch, train loss: 0.44645950198173523:  98%|█████████▊| 143/146 [00:22<00:00,  5.96it/s]Epoch: 5, train for the 144-th batch, train loss: 0.44645950198173523:  99%|█████████▊| 144/146 [00:22<00:00,  6.34it/s]evaluate for the 56-th batch, evaluate loss: 0.3364255428314209:  52%|████████▊        | 55/106 [00:12<00:12,  3.95it/s]evaluate for the 56-th batch, evaluate loss: 0.3364255428314209:  53%|████████▉        | 56/106 [00:12<00:12,  4.16it/s]evaluate for the 43-th batch, evaluate loss: 0.49705103039741516:  64%|██████████▊      | 42/66 [00:06<00:03,  6.13it/s]evaluate for the 43-th batch, evaluate loss: 0.49705103039741516:  65%|███████████      | 43/66 [00:06<00:03,  6.31it/s]Epoch: 6, train for the 113-th batch, train loss: 0.4243709444999695:  94%|██████████▎| 112/119 [00:17<00:01,  6.25it/s]Epoch: 6, train for the 113-th batch, train loss: 0.4243709444999695:  95%|██████████▍| 113/119 [00:17<00:00,  6.38it/s]Epoch: 3, train for the 171-th batch, train loss: 0.5257371664047241:  71%|███████▊   | 170/241 [00:26<00:11,  6.08it/s]Epoch: 3, train for the 171-th batch, train loss: 0.5257371664047241:  71%|███████▊   | 171/241 [00:26<00:10,  6.44it/s]Epoch: 5, train for the 145-th batch, train loss: 0.5061080455780029:  99%|██████████▊| 144/146 [00:22<00:00,  6.34it/s]Epoch: 5, train for the 145-th batch, train loss: 0.5061080455780029:  99%|██████████▉| 145/146 [00:22<00:00,  6.55it/s]Epoch: 4, train for the 151-th batch, train loss: 0.5076050758361816:  99%|██████████▉| 150/151 [00:27<00:00,  4.77it/s]Epoch: 4, train for the 151-th batch, train loss: 0.5076050758361816: 100%|███████████| 151/151 [00:27<00:00,  4.22it/s]Epoch: 4, train for the 151-th batch, train loss: 0.5076050758361816: 100%|███████████| 151/151 [00:27<00:00,  5.53it/s]
Epoch: 6, train for the 114-th batch, train loss: 0.3974227011203766:  95%|██████████▍| 113/119 [00:17<00:00,  6.38it/s]Epoch: 6, train for the 114-th batch, train loss: 0.3974227011203766:  96%|██████████▌| 114/119 [00:17<00:00,  6.51it/s]Epoch: 3, train for the 172-th batch, train loss: 0.42387688159942627:  71%|███████   | 171/241 [00:26<00:10,  6.44it/s]Epoch: 5, train for the 146-th batch, train loss: 0.45698875188827515:  99%|█████████▉| 145/146 [00:22<00:00,  6.55it/s]evaluate for the 44-th batch, evaluate loss: 0.5220015048980713:  65%|███████████▋      | 43/66 [00:06<00:03,  6.31it/s]evaluate for the 44-th batch, evaluate loss: 0.5220015048980713:  67%|████████████      | 44/66 [00:06<00:04,  5.43it/s]Epoch: 3, train for the 172-th batch, train loss: 0.42387688159942627:  71%|███████▏  | 172/241 [00:26<00:10,  6.51it/s]evaluate for the 57-th batch, evaluate loss: 0.32573720812797546:  53%|████████▍       | 56/106 [00:13<00:12,  4.16it/s]evaluate for the 57-th batch, evaluate loss: 0.32573720812797546:  54%|████████▌       | 57/106 [00:13<00:12,  4.06it/s]Epoch: 5, train for the 146-th batch, train loss: 0.45698875188827515: 100%|██████████| 146/146 [00:22<00:00,  6.88it/s]Epoch: 5, train for the 146-th batch, train loss: 0.45698875188827515: 100%|██████████| 146/146 [00:22<00:00,  6.51it/s]
  0%|                                                                                            | 0/46 [00:00<?, ?it/s]  0%|                                                                                            | 0/38 [00:00<?, ?it/s]evaluate for the 45-th batch, evaluate loss: 0.5010999441146851:  67%|████████████      | 44/66 [00:06<00:04,  5.43it/s]evaluate for the 45-th batch, evaluate loss: 0.5010999441146851:  68%|████████████▎     | 45/66 [00:06<00:03,  6.13it/s]Epoch: 6, train for the 115-th batch, train loss: 0.39660125970840454:  96%|█████████▌| 114/119 [00:18<00:00,  6.51it/s]Epoch: 6, train for the 115-th batch, train loss: 0.39660125970840454:  97%|█████████▋| 115/119 [00:18<00:00,  6.38it/s]evaluate for the 1-th batch, evaluate loss: 0.46231868863105774:   0%|                           | 0/38 [00:00<?, ?it/s]Epoch: 3, train for the 173-th batch, train loss: 0.293704628944397:  71%|████████▌   | 172/241 [00:27<00:10,  6.51it/s]evaluate for the 1-th batch, evaluate loss: 0.5510849356651306:   0%|                            | 0/46 [00:00<?, ?it/s]Epoch: 3, train for the 173-th batch, train loss: 0.293704628944397:  72%|████████▌   | 173/241 [00:27<00:11,  6.16it/s]evaluate for the 58-th batch, evaluate loss: 0.4024500846862793:  54%|█████████▏       | 57/106 [00:13<00:12,  4.06it/s]evaluate for the 58-th batch, evaluate loss: 0.4024500846862793:  55%|█████████▎       | 58/106 [00:13<00:11,  4.21it/s]evaluate for the 46-th batch, evaluate loss: 0.5668250322341919:  68%|████████████▎     | 45/66 [00:06<00:03,  6.13it/s]evaluate for the 46-th batch, evaluate loss: 0.5668250322341919:  70%|████████████▌     | 46/66 [00:06<00:03,  6.48it/s]evaluate for the 2-th batch, evaluate loss: 0.4764796197414398:   0%|                            | 0/38 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.4764796197414398:   5%|█                   | 2/38 [00:00<00:02, 13.49it/s]evaluate for the 2-th batch, evaluate loss: 0.5547126531600952:   0%|                            | 0/46 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5547126531600952:   4%|▊                   | 2/46 [00:00<00:04, 10.83it/s]Epoch: 6, train for the 116-th batch, train loss: 0.3815217614173889:  97%|██████████▋| 115/119 [00:18<00:00,  6.38it/s]evaluate for the 3-th batch, evaluate loss: 0.45855143666267395:   5%|█                  | 2/38 [00:00<00:02, 13.49it/s]Epoch: 6, train for the 116-th batch, train loss: 0.3815217614173889:  97%|██████████▋| 116/119 [00:18<00:00,  5.93it/s]evaluate for the 3-th batch, evaluate loss: 0.5493475198745728:   4%|▊                   | 2/46 [00:00<00:04, 10.83it/s]evaluate for the 47-th batch, evaluate loss: 0.5456314086914062:  70%|████████████▌     | 46/66 [00:07<00:03,  6.48it/s]evaluate for the 47-th batch, evaluate loss: 0.5456314086914062:  71%|████████████▊     | 47/66 [00:07<00:02,  6.78it/s]evaluate for the 4-th batch, evaluate loss: 0.467700332403183:   5%|█                    | 2/38 [00:00<00:02, 13.49it/s]evaluate for the 4-th batch, evaluate loss: 0.467700332403183:  11%|██▏                  | 4/38 [00:00<00:02, 13.70it/s]evaluate for the 4-th batch, evaluate loss: 0.5410059094429016:   4%|▊                   | 2/46 [00:00<00:04, 10.83it/s]evaluate for the 4-th batch, evaluate loss: 0.5410059094429016:   9%|█▋                  | 4/46 [00:00<00:03, 12.06it/s]evaluate for the 5-th batch, evaluate loss: 0.5203084945678711:  11%|██                  | 4/38 [00:00<00:02, 13.70it/s]evaluate for the 59-th batch, evaluate loss: 0.3782994747161865:  55%|█████████▎       | 58/106 [00:13<00:11,  4.21it/s]evaluate for the 59-th batch, evaluate loss: 0.3782994747161865:  56%|█████████▍       | 59/106 [00:13<00:11,  4.12it/s]Epoch: 3, train for the 174-th batch, train loss: 0.35189494490623474:  72%|███████▏  | 173/241 [00:27<00:11,  6.16it/s]Epoch: 6, train for the 117-th batch, train loss: 0.3821240961551666:  97%|██████████▋| 116/119 [00:18<00:00,  5.93it/s]evaluate for the 6-th batch, evaluate loss: 0.46055784821510315:  11%|██                 | 4/38 [00:00<00:02, 13.70it/s]evaluate for the 6-th batch, evaluate loss: 0.46055784821510315:  16%|███                | 6/38 [00:00<00:02, 15.12it/s]Epoch: 6, train for the 117-th batch, train loss: 0.3821240961551666:  98%|██████████▊| 117/119 [00:18<00:00,  5.89it/s]evaluate for the 5-th batch, evaluate loss: 0.5339316725730896:   9%|█▋                  | 4/46 [00:00<00:03, 12.06it/s]Epoch: 3, train for the 174-th batch, train loss: 0.35189494490623474:  72%|███████▏  | 174/241 [00:27<00:14,  4.71it/s]evaluate for the 48-th batch, evaluate loss: 0.5368332266807556:  71%|████████████▊     | 47/66 [00:07<00:02,  6.78it/s]evaluate for the 48-th batch, evaluate loss: 0.5368332266807556:  73%|█████████████     | 48/66 [00:07<00:02,  6.82it/s]evaluate for the 7-th batch, evaluate loss: 0.4197724759578705:  16%|███▏                | 6/38 [00:00<00:02, 15.12it/s]evaluate for the 6-th batch, evaluate loss: 0.5342100262641907:   9%|█▋                  | 4/46 [00:00<00:03, 12.06it/s]evaluate for the 6-th batch, evaluate loss: 0.5342100262641907:  13%|██▌                 | 6/46 [00:00<00:03, 11.52it/s]evaluate for the 60-th batch, evaluate loss: 0.34767743945121765:  56%|████████▉       | 59/106 [00:13<00:11,  4.12it/s]evaluate for the 60-th batch, evaluate loss: 0.34767743945121765:  57%|█████████       | 60/106 [00:13<00:10,  4.44it/s]evaluate for the 8-th batch, evaluate loss: 0.43972480297088623:  16%|███                | 6/38 [00:00<00:02, 15.12it/s]evaluate for the 8-th batch, evaluate loss: 0.43972480297088623:  21%|████               | 8/38 [00:00<00:02, 13.60it/s]Epoch: 3, train for the 175-th batch, train loss: 0.253542959690094:  72%|████████▋   | 174/241 [00:27<00:14,  4.71it/s]evaluate for the 7-th batch, evaluate loss: 0.5426511764526367:  13%|██▌                 | 6/46 [00:00<00:03, 11.52it/s]Epoch: 6, train for the 118-th batch, train loss: 0.3747899532318115:  98%|██████████▊| 117/119 [00:18<00:00,  5.89it/s]Epoch: 3, train for the 175-th batch, train loss: 0.253542959690094:  73%|████████▋   | 175/241 [00:27<00:13,  4.94it/s]Epoch: 6, train for the 118-th batch, train loss: 0.3747899532318115:  99%|██████████▉| 118/119 [00:18<00:00,  5.71it/s]evaluate for the 9-th batch, evaluate loss: 0.4590906500816345:  21%|████▏               | 8/38 [00:00<00:02, 13.60it/s]evaluate for the 8-th batch, evaluate loss: 0.5684781670570374:  13%|██▌                 | 6/46 [00:00<00:03, 11.52it/s]evaluate for the 8-th batch, evaluate loss: 0.5684781670570374:  17%|███▍                | 8/46 [00:00<00:03, 11.87it/s]evaluate for the 10-th batch, evaluate loss: 0.48709312081336975:  21%|███▊              | 8/38 [00:00<00:02, 13.60it/s]evaluate for the 10-th batch, evaluate loss: 0.48709312081336975:  26%|████▍            | 10/38 [00:00<00:01, 14.42it/s]Epoch: 6, train for the 119-th batch, train loss: 0.353005588054657:  99%|███████████▉| 118/119 [00:18<00:00,  5.71it/s]evaluate for the 11-th batch, evaluate loss: 0.4372521638870239:  26%|████▋             | 10/38 [00:00<00:01, 14.42it/s]Epoch: 6, train for the 119-th batch, train loss: 0.353005588054657: 100%|████████████| 119/119 [00:18<00:00,  5.94it/s]Epoch: 6, train for the 119-th batch, train loss: 0.353005588054657: 100%|████████████| 119/119 [00:18<00:00,  6.31it/s]
Epoch: 3, train for the 176-th batch, train loss: 0.3531130850315094:  73%|███████▉   | 175/241 [00:27<00:13,  4.94it/s]evaluate for the 9-th batch, evaluate loss: 0.5208421349525452:  17%|███▍                | 8/46 [00:00<00:03, 11.87it/s]Epoch: 3, train for the 176-th batch, train loss: 0.3531130850315094:  73%|████████   | 176/241 [00:27<00:12,  5.14it/s]evaluate for the 49-th batch, evaluate loss: 0.5531893968582153:  73%|█████████████     | 48/66 [00:07<00:02,  6.82it/s]evaluate for the 49-th batch, evaluate loss: 0.5531893968582153:  74%|█████████████▎    | 49/66 [00:07<00:03,  4.77it/s]evaluate for the 10-th batch, evaluate loss: 0.5079120993614197:  17%|███▎               | 8/46 [00:00<00:03, 11.87it/s]evaluate for the 12-th batch, evaluate loss: 0.510871946811676:  26%|█████              | 10/38 [00:00<00:01, 14.42it/s]evaluate for the 10-th batch, evaluate loss: 0.5079120993614197:  22%|███▉              | 10/46 [00:00<00:02, 12.29it/s]evaluate for the 12-th batch, evaluate loss: 0.510871946811676:  32%|██████             | 12/38 [00:00<00:01, 14.91it/s]  0%|                                                                                            | 0/40 [00:00<?, ?it/s]evaluate for the 61-th batch, evaluate loss: 0.4537597596645355:  57%|█████████▌       | 60/106 [00:14<00:10,  4.44it/s]evaluate for the 61-th batch, evaluate loss: 0.4537597596645355:  58%|█████████▊       | 61/106 [00:14<00:11,  3.99it/s]evaluate for the 13-th batch, evaluate loss: 0.4841088652610779:  32%|█████▋            | 12/38 [00:00<00:01, 14.91it/s]Epoch: 3, train for the 177-th batch, train loss: 0.3398028612136841:  73%|████████   | 176/241 [00:28<00:12,  5.14it/s]evaluate for the 1-th batch, evaluate loss: 0.446523517370224:   0%|                             | 0/40 [00:00<?, ?it/s]evaluate for the 50-th batch, evaluate loss: 0.6002476215362549:  74%|█████████████▎    | 49/66 [00:07<00:03,  4.77it/s]evaluate for the 50-th batch, evaluate loss: 0.6002476215362549:  76%|█████████████▋    | 50/66 [00:07<00:03,  5.31it/s]evaluate for the 11-th batch, evaluate loss: 0.566895604133606:  22%|████▏              | 10/46 [00:00<00:02, 12.29it/s]Epoch: 3, train for the 177-th batch, train loss: 0.3398028612136841:  73%|████████   | 177/241 [00:28<00:11,  5.35it/s]evaluate for the 14-th batch, evaluate loss: 0.41525667905807495:  32%|█████▎           | 12/38 [00:00<00:01, 14.91it/s]evaluate for the 14-th batch, evaluate loss: 0.41525667905807495:  37%|██████▎          | 14/38 [00:00<00:01, 14.64it/s]evaluate for the 2-th batch, evaluate loss: 0.44533321261405945:   0%|                           | 0/40 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.44533321261405945:   5%|▉                  | 2/40 [00:00<00:03, 10.86it/s]evaluate for the 12-th batch, evaluate loss: 0.5160672664642334:  22%|███▉              | 10/46 [00:01<00:02, 12.29it/s]evaluate for the 12-th batch, evaluate loss: 0.5160672664642334:  26%|████▋             | 12/46 [00:01<00:03, 11.18it/s]evaluate for the 15-th batch, evaluate loss: 0.43127745389938354:  37%|██████▎          | 14/38 [00:01<00:01, 14.64it/s]evaluate for the 62-th batch, evaluate loss: 0.4104297459125519:  58%|█████████▊       | 61/106 [00:14<00:11,  3.99it/s]evaluate for the 62-th batch, evaluate loss: 0.4104297459125519:  58%|█████████▉       | 62/106 [00:14<00:10,  4.25it/s]Epoch: 3, train for the 178-th batch, train loss: 0.4326033592224121:  73%|████████   | 177/241 [00:28<00:11,  5.35it/s]evaluate for the 3-th batch, evaluate loss: 0.4506371319293976:   5%|█                   | 2/40 [00:00<00:03, 10.86it/s]Epoch: 3, train for the 178-th batch, train loss: 0.4326033592224121:  74%|████████   | 178/241 [00:28<00:11,  5.62it/s]evaluate for the 16-th batch, evaluate loss: 0.4855963885784149:  37%|██████▋           | 14/38 [00:01<00:01, 14.64it/s]evaluate for the 16-th batch, evaluate loss: 0.4855963885784149:  42%|███████▌          | 16/38 [00:01<00:01, 14.19it/s]evaluate for the 13-th batch, evaluate loss: 0.5155288577079773:  26%|████▋             | 12/46 [00:01<00:03, 11.18it/s]evaluate for the 51-th batch, evaluate loss: 0.5679706931114197:  76%|█████████████▋    | 50/66 [00:07<00:03,  5.31it/s]evaluate for the 51-th batch, evaluate loss: 0.5679706931114197:  77%|█████████████▉    | 51/66 [00:07<00:03,  4.79it/s]evaluate for the 4-th batch, evaluate loss: 0.517217218875885:   5%|█                    | 2/40 [00:00<00:03, 10.86it/s]evaluate for the 4-th batch, evaluate loss: 0.517217218875885:  10%|██                   | 4/40 [00:00<00:03, 11.32it/s]evaluate for the 17-th batch, evaluate loss: 0.44834962487220764:  42%|███████▏         | 16/38 [00:01<00:01, 14.19it/s]Epoch: 3, train for the 179-th batch, train loss: 0.3835161626338959:  74%|████████   | 178/241 [00:28<00:11,  5.62it/s]evaluate for the 5-th batch, evaluate loss: 0.5002590417861938:  10%|██                  | 4/40 [00:00<00:03, 11.32it/s]evaluate for the 14-th batch, evaluate loss: 0.5617079138755798:  26%|████▋             | 12/46 [00:01<00:03, 11.18it/s]evaluate for the 14-th batch, evaluate loss: 0.5617079138755798:  30%|█████▍            | 14/46 [00:01<00:03,  9.93it/s]evaluate for the 18-th batch, evaluate loss: 0.4963454008102417:  42%|███████▌          | 16/38 [00:01<00:01, 14.19it/s]evaluate for the 18-th batch, evaluate loss: 0.4963454008102417:  47%|████████▌         | 18/38 [00:01<00:01, 13.42it/s]Epoch: 3, train for the 179-th batch, train loss: 0.3835161626338959:  74%|████████▏  | 179/241 [00:28<00:11,  5.33it/s]evaluate for the 52-th batch, evaluate loss: 0.5372185111045837:  77%|█████████████▉    | 51/66 [00:08<00:03,  4.79it/s]evaluate for the 52-th batch, evaluate loss: 0.5372185111045837:  79%|██████████████▏   | 52/66 [00:08<00:02,  5.28it/s]evaluate for the 63-th batch, evaluate loss: 0.43766242265701294:  58%|█████████▎      | 62/106 [00:14<00:10,  4.25it/s]evaluate for the 63-th batch, evaluate loss: 0.43766242265701294:  59%|█████████▌      | 63/106 [00:14<00:10,  4.09it/s]evaluate for the 19-th batch, evaluate loss: 0.46998631954193115:  47%|████████         | 18/38 [00:01<00:01, 13.42it/s]evaluate for the 6-th batch, evaluate loss: 0.4647814929485321:  10%|██                  | 4/40 [00:00<00:03, 11.32it/s]evaluate for the 6-th batch, evaluate loss: 0.4647814929485321:  15%|███                 | 6/40 [00:00<00:03, 10.73it/s]evaluate for the 15-th batch, evaluate loss: 0.5415681600570679:  30%|█████▍            | 14/46 [00:01<00:03,  9.93it/s]evaluate for the 20-th batch, evaluate loss: 0.4010035991668701:  47%|████████▌         | 18/38 [00:01<00:01, 13.42it/s]evaluate for the 20-th batch, evaluate loss: 0.4010035991668701:  53%|█████████▍        | 20/38 [00:01<00:01, 13.32it/s]evaluate for the 7-th batch, evaluate loss: 0.46650809049606323:  15%|██▊                | 6/40 [00:00<00:03, 10.73it/s]Epoch: 3, train for the 180-th batch, train loss: 0.5589403510093689:  74%|████████▏  | 179/241 [00:28<00:11,  5.33it/s]evaluate for the 16-th batch, evaluate loss: 0.5285753607749939:  30%|█████▍            | 14/46 [00:01<00:03,  9.93it/s]evaluate for the 16-th batch, evaluate loss: 0.5285753607749939:  35%|██████▎           | 16/46 [00:01<00:03,  9.75it/s]evaluate for the 21-th batch, evaluate loss: 0.43265044689178467:  53%|████████▉        | 20/38 [00:01<00:01, 13.32it/s]Epoch: 3, train for the 180-th batch, train loss: 0.5589403510093689:  75%|████████▏  | 180/241 [00:28<00:11,  5.24it/s]evaluate for the 8-th batch, evaluate loss: 0.42626166343688965:  15%|██▊                | 6/40 [00:00<00:03, 10.73it/s]evaluate for the 8-th batch, evaluate loss: 0.42626166343688965:  20%|███▊               | 8/40 [00:00<00:02, 12.03it/s]evaluate for the 64-th batch, evaluate loss: 0.2787766456604004:  59%|██████████       | 63/106 [00:14<00:10,  4.09it/s]evaluate for the 64-th batch, evaluate loss: 0.2787766456604004:  60%|██████████▎      | 64/106 [00:14<00:10,  4.20it/s]evaluate for the 17-th batch, evaluate loss: 0.476317435503006:  35%|██████▌            | 16/46 [00:01<00:03,  9.75it/s]evaluate for the 22-th batch, evaluate loss: 0.459727019071579:  53%|██████████         | 20/38 [00:01<00:01, 13.32it/s]evaluate for the 22-th batch, evaluate loss: 0.459727019071579:  58%|███████████        | 22/38 [00:01<00:01, 12.89it/s]evaluate for the 9-th batch, evaluate loss: 0.4690483808517456:  20%|████                | 8/40 [00:00<00:02, 12.03it/s]evaluate for the 53-th batch, evaluate loss: 0.51081383228302:  79%|███████████████▊    | 52/66 [00:08<00:02,  5.28it/s]evaluate for the 53-th batch, evaluate loss: 0.51081383228302:  80%|████████████████    | 53/66 [00:08<00:03,  4.23it/s]Epoch: 3, train for the 181-th batch, train loss: 0.7439892292022705:  75%|████████▏  | 180/241 [00:28<00:11,  5.24it/s]evaluate for the 18-th batch, evaluate loss: 0.5027750134468079:  35%|██████▎           | 16/46 [00:01<00:03,  9.75it/s]evaluate for the 18-th batch, evaluate loss: 0.5027750134468079:  39%|███████           | 18/46 [00:01<00:02,  9.90it/s]Epoch: 3, train for the 181-th batch, train loss: 0.7439892292022705:  75%|████████▎  | 181/241 [00:28<00:11,  5.28it/s]evaluate for the 23-th batch, evaluate loss: 0.44728681445121765:  58%|█████████▊       | 22/38 [00:01<00:01, 12.89it/s]evaluate for the 10-th batch, evaluate loss: 0.503516435623169:  20%|████                | 8/40 [00:00<00:02, 12.03it/s]evaluate for the 10-th batch, evaluate loss: 0.503516435623169:  25%|████▊              | 10/40 [00:00<00:02, 11.31it/s]evaluate for the 19-th batch, evaluate loss: 0.5721402168273926:  39%|███████           | 18/46 [00:01<00:02,  9.90it/s]evaluate for the 24-th batch, evaluate loss: 0.445855051279068:  58%|███████████        | 22/38 [00:01<00:01, 12.89it/s]evaluate for the 24-th batch, evaluate loss: 0.445855051279068:  63%|████████████       | 24/38 [00:01<00:01, 12.14it/s]evaluate for the 54-th batch, evaluate loss: 0.5559658408164978:  80%|██████████████▍   | 53/66 [00:08<00:03,  4.23it/s]evaluate for the 54-th batch, evaluate loss: 0.5559658408164978:  82%|██████████████▋   | 54/66 [00:08<00:02,  4.84it/s]evaluate for the 11-th batch, evaluate loss: 0.4431173503398895:  25%|████▌             | 10/40 [00:00<00:02, 11.31it/s]evaluate for the 65-th batch, evaluate loss: 0.38640090823173523:  60%|█████████▋      | 64/106 [00:15<00:10,  4.20it/s]evaluate for the 65-th batch, evaluate loss: 0.38640090823173523:  61%|█████████▊      | 65/106 [00:15<00:10,  4.02it/s]Epoch: 3, train for the 182-th batch, train loss: 0.6174108982086182:  75%|████████▎  | 181/241 [00:28<00:11,  5.28it/s]evaluate for the 25-th batch, evaluate loss: 0.4963686168193817:  63%|███████████▎      | 24/38 [00:01<00:01, 12.14it/s]evaluate for the 20-th batch, evaluate loss: 0.5306864976882935:  39%|███████           | 18/46 [00:01<00:02,  9.90it/s]evaluate for the 20-th batch, evaluate loss: 0.5306864976882935:  43%|███████▊          | 20/46 [00:01<00:02, 10.22it/s]Epoch: 3, train for the 182-th batch, train loss: 0.6174108982086182:  76%|████████▎  | 182/241 [00:28<00:11,  5.30it/s]evaluate for the 12-th batch, evaluate loss: 0.46153968572616577:  25%|████▎            | 10/40 [00:01<00:02, 11.31it/s]evaluate for the 12-th batch, evaluate loss: 0.46153968572616577:  30%|█████            | 12/40 [00:01<00:02, 11.34it/s]evaluate for the 26-th batch, evaluate loss: 0.4519899785518646:  63%|███████████▎      | 24/38 [00:01<00:01, 12.14it/s]evaluate for the 26-th batch, evaluate loss: 0.4519899785518646:  68%|████████████▎     | 26/38 [00:01<00:00, 13.40it/s]evaluate for the 55-th batch, evaluate loss: 0.514665961265564:  82%|███████████████▌   | 54/66 [00:08<00:02,  4.84it/s]evaluate for the 55-th batch, evaluate loss: 0.514665961265564:  83%|███████████████▊   | 55/66 [00:08<00:02,  5.26it/s]evaluate for the 13-th batch, evaluate loss: 0.4617420434951782:  30%|█████▍            | 12/40 [00:01<00:02, 11.34it/s]evaluate for the 21-th batch, evaluate loss: 0.553914487361908:  43%|████████▎          | 20/46 [00:02<00:02, 10.22it/s]evaluate for the 27-th batch, evaluate loss: 0.4668608605861664:  68%|████████████▎     | 26/38 [00:01<00:00, 13.40it/s]Epoch: 3, train for the 183-th batch, train loss: 0.419843852519989:  76%|█████████   | 182/241 [00:29<00:11,  5.30it/s]evaluate for the 14-th batch, evaluate loss: 0.4595448076725006:  30%|█████▍            | 12/40 [00:01<00:02, 11.34it/s]evaluate for the 14-th batch, evaluate loss: 0.4595448076725006:  35%|██████▎           | 14/40 [00:01<00:02, 11.29it/s]Epoch: 3, train for the 183-th batch, train loss: 0.419843852519989:  76%|█████████   | 183/241 [00:29<00:10,  5.30it/s]evaluate for the 28-th batch, evaluate loss: 0.46683967113494873:  68%|███████████▋     | 26/38 [00:02<00:00, 13.40it/s]evaluate for the 28-th batch, evaluate loss: 0.46683967113494873:  74%|████████████▌    | 28/38 [00:02<00:00, 12.54it/s]evaluate for the 66-th batch, evaluate loss: 0.3961743414402008:  61%|██████████▍      | 65/106 [00:15<00:10,  4.02it/s]evaluate for the 66-th batch, evaluate loss: 0.3961743414402008:  62%|██████████▌      | 66/106 [00:15<00:10,  3.99it/s]evaluate for the 22-th batch, evaluate loss: 0.49900320172309875:  43%|███████▍         | 20/46 [00:02<00:02, 10.22it/s]evaluate for the 22-th batch, evaluate loss: 0.49900320172309875:  48%|████████▏        | 22/46 [00:02<00:02,  9.75it/s]evaluate for the 56-th batch, evaluate loss: 0.525126039981842:  83%|███████████████▊   | 55/66 [00:08<00:02,  5.26it/s]evaluate for the 56-th batch, evaluate loss: 0.525126039981842:  85%|████████████████   | 56/66 [00:08<00:01,  5.50it/s]evaluate for the 15-th batch, evaluate loss: 0.465391606092453:  35%|██████▋            | 14/40 [00:01<00:02, 11.29it/s]evaluate for the 29-th batch, evaluate loss: 0.4595983922481537:  74%|█████████████▎    | 28/38 [00:02<00:00, 12.54it/s]evaluate for the 16-th batch, evaluate loss: 0.454445481300354:  35%|██████▋            | 14/40 [00:01<00:02, 11.29it/s]evaluate for the 16-th batch, evaluate loss: 0.454445481300354:  40%|███████▌           | 16/40 [00:01<00:02, 11.69it/s]evaluate for the 57-th batch, evaluate loss: 0.5883941650390625:  85%|███████████████▎  | 56/66 [00:09<00:01,  5.50it/s]evaluate for the 57-th batch, evaluate loss: 0.5883941650390625:  86%|███████████████▌  | 57/66 [00:09<00:01,  5.87it/s]evaluate for the 30-th batch, evaluate loss: 0.4805475175380707:  74%|█████████████▎    | 28/38 [00:02<00:00, 12.54it/s]evaluate for the 30-th batch, evaluate loss: 0.4805475175380707:  79%|██████████████▏   | 30/38 [00:02<00:00, 12.17it/s]Epoch: 3, train for the 184-th batch, train loss: 0.5741827487945557:  76%|████████▎  | 183/241 [00:29<00:10,  5.30it/s]evaluate for the 23-th batch, evaluate loss: 0.4663161337375641:  48%|████████▌         | 22/46 [00:02<00:02,  9.75it/s]evaluate for the 23-th batch, evaluate loss: 0.4663161337375641:  50%|█████████         | 23/46 [00:02<00:02,  8.61it/s]evaluate for the 17-th batch, evaluate loss: 0.46236082911491394:  40%|██████▊          | 16/40 [00:01<00:02, 11.69it/s]Epoch: 3, train for the 184-th batch, train loss: 0.5741827487945557:  76%|████████▍  | 184/241 [00:29<00:11,  5.01it/s]evaluate for the 31-th batch, evaluate loss: 0.4613667130470276:  79%|██████████████▏   | 30/38 [00:02<00:00, 12.17it/s]evaluate for the 67-th batch, evaluate loss: 0.34905096888542175:  62%|█████████▉      | 66/106 [00:15<00:10,  3.99it/s]evaluate for the 67-th batch, evaluate loss: 0.34905096888542175:  63%|██████████      | 67/106 [00:15<00:10,  3.88it/s]evaluate for the 18-th batch, evaluate loss: 0.43562543392181396:  40%|██████▊          | 16/40 [00:01<00:02, 11.69it/s]evaluate for the 18-th batch, evaluate loss: 0.43562543392181396:  45%|███████▋         | 18/40 [00:01<00:01, 12.18it/s]evaluate for the 24-th batch, evaluate loss: 0.49958136677742004:  50%|████████▌        | 23/46 [00:02<00:02,  8.61it/s]evaluate for the 24-th batch, evaluate loss: 0.49958136677742004:  52%|████████▊        | 24/46 [00:02<00:02,  8.69it/s]evaluate for the 32-th batch, evaluate loss: 0.4117855727672577:  79%|██████████████▏   | 30/38 [00:02<00:00, 12.17it/s]evaluate for the 32-th batch, evaluate loss: 0.4117855727672577:  84%|███████████████▏  | 32/38 [00:02<00:00, 11.73it/s]evaluate for the 58-th batch, evaluate loss: 0.5664292573928833:  86%|███████████████▌  | 57/66 [00:09<00:01,  5.87it/s]evaluate for the 58-th batch, evaluate loss: 0.5664292573928833:  88%|███████████████▊  | 58/66 [00:09<00:01,  5.67it/s]Epoch: 3, train for the 185-th batch, train loss: 0.6331785321235657:  76%|████████▍  | 184/241 [00:29<00:11,  5.01it/s]evaluate for the 19-th batch, evaluate loss: 0.47578391432762146:  45%|███████▋         | 18/40 [00:01<00:01, 12.18it/s]evaluate for the 25-th batch, evaluate loss: 0.5298846364021301:  52%|█████████▍        | 24/46 [00:02<00:02,  8.69it/s]Epoch: 3, train for the 185-th batch, train loss: 0.6331785321235657:  77%|████████▍  | 185/241 [00:29<00:11,  4.94it/s]evaluate for the 33-th batch, evaluate loss: 0.4674730598926544:  84%|███████████████▏  | 32/38 [00:02<00:00, 11.73it/s]evaluate for the 20-th batch, evaluate loss: 0.44215190410614014:  45%|███████▋         | 18/40 [00:01<00:01, 12.18it/s]evaluate for the 20-th batch, evaluate loss: 0.44215190410614014:  50%|████████▌        | 20/40 [00:01<00:01, 11.92it/s]evaluate for the 68-th batch, evaluate loss: 0.41940048336982727:  63%|██████████      | 67/106 [00:15<00:10,  3.88it/s]evaluate for the 68-th batch, evaluate loss: 0.41940048336982727:  64%|██████████▎     | 68/106 [00:15<00:09,  4.04it/s]evaluate for the 26-th batch, evaluate loss: 0.542867124080658:  52%|█████████▉         | 24/46 [00:02<00:02,  8.69it/s]evaluate for the 26-th batch, evaluate loss: 0.542867124080658:  57%|██████████▋        | 26/46 [00:02<00:02,  8.79it/s]evaluate for the 34-th batch, evaluate loss: 0.4836212694644928:  84%|███████████████▏  | 32/38 [00:02<00:00, 11.73it/s]evaluate for the 34-th batch, evaluate loss: 0.4836212694644928:  89%|████████████████  | 34/38 [00:02<00:00, 11.32it/s]evaluate for the 21-th batch, evaluate loss: 0.466000497341156:  50%|█████████▌         | 20/40 [00:01<00:01, 11.92it/s]Epoch: 3, train for the 186-th batch, train loss: 0.5301166772842407:  77%|████████▍  | 185/241 [00:29<00:11,  4.94it/s]evaluate for the 27-th batch, evaluate loss: 0.524608314037323:  57%|██████████▋        | 26/46 [00:02<00:02,  8.79it/s]evaluate for the 59-th batch, evaluate loss: 0.5559388399124146:  88%|███████████████▊  | 58/66 [00:09<00:01,  5.67it/s]evaluate for the 59-th batch, evaluate loss: 0.5559388399124146:  89%|████████████████  | 59/66 [00:09<00:01,  4.99it/s]Epoch: 3, train for the 186-th batch, train loss: 0.5301166772842407:  77%|████████▍  | 186/241 [00:29<00:11,  4.94it/s]evaluate for the 35-th batch, evaluate loss: 0.4807257652282715:  89%|████████████████  | 34/38 [00:02<00:00, 11.32it/s]evaluate for the 22-th batch, evaluate loss: 0.44875839352607727:  50%|████████▌        | 20/40 [00:01<00:01, 11.92it/s]evaluate for the 22-th batch, evaluate loss: 0.44875839352607727:  55%|█████████▎       | 22/40 [00:01<00:01, 11.79it/s]evaluate for the 28-th batch, evaluate loss: 0.5244683027267456:  57%|██████████▏       | 26/46 [00:02<00:02,  8.79it/s]evaluate for the 28-th batch, evaluate loss: 0.5244683027267456:  61%|██████████▉       | 28/46 [00:02<00:02,  8.97it/s]evaluate for the 36-th batch, evaluate loss: 0.4805443584918976:  89%|████████████████  | 34/38 [00:02<00:00, 11.32it/s]evaluate for the 36-th batch, evaluate loss: 0.4805443584918976:  95%|█████████████████ | 36/38 [00:02<00:00, 11.03it/s]evaluate for the 69-th batch, evaluate loss: 0.4754555821418762:  64%|██████████▉      | 68/106 [00:16<00:09,  4.04it/s]evaluate for the 69-th batch, evaluate loss: 0.4754555821418762:  65%|███████████      | 69/106 [00:16<00:09,  4.07it/s]evaluate for the 23-th batch, evaluate loss: 0.4024728536605835:  55%|█████████▉        | 22/40 [00:02<00:01, 11.79it/s]evaluate for the 60-th batch, evaluate loss: 0.5699541568756104:  89%|████████████████  | 59/66 [00:09<00:01,  4.99it/s]evaluate for the 60-th batch, evaluate loss: 0.5699541568756104:  91%|████████████████▎ | 60/66 [00:09<00:01,  5.54it/s]Epoch: 3, train for the 187-th batch, train loss: 0.5922542810440063:  77%|████████▍  | 186/241 [00:29<00:11,  4.94it/s]Epoch: 3, train for the 187-th batch, train loss: 0.5922542810440063:  78%|████████▌  | 187/241 [00:29<00:10,  4.95it/s]evaluate for the 24-th batch, evaluate loss: 0.44065558910369873:  55%|█████████▎       | 22/40 [00:02<00:01, 11.79it/s]evaluate for the 24-th batch, evaluate loss: 0.44065558910369873:  60%|██████████▏      | 24/40 [00:02<00:01, 11.38it/s]evaluate for the 37-th batch, evaluate loss: 0.42637065052986145:  95%|████████████████ | 36/38 [00:02<00:00, 11.03it/s]evaluate for the 29-th batch, evaluate loss: 0.4997910261154175:  61%|██████████▉       | 28/46 [00:02<00:02,  8.97it/s]evaluate for the 25-th batch, evaluate loss: 0.4789264500141144:  60%|██████████▊       | 24/40 [00:02<00:01, 11.38it/s]evaluate for the 38-th batch, evaluate loss: 0.45798274874687195:  95%|████████████████ | 36/38 [00:03<00:00, 11.03it/s]evaluate for the 38-th batch, evaluate loss: 0.45798274874687195: 100%|█████████████████| 38/38 [00:03<00:00, 11.05it/s]evaluate for the 38-th batch, evaluate loss: 0.45798274874687195: 100%|█████████████████| 38/38 [00:03<00:00, 12.61it/s]
evaluate for the 70-th batch, evaluate loss: 0.3218163251876831:  65%|███████████      | 69/106 [00:16<00:09,  4.07it/s]evaluate for the 70-th batch, evaluate loss: 0.3218163251876831:  66%|███████████▏     | 70/106 [00:16<00:08,  4.25it/s]Epoch: 3, train for the 188-th batch, train loss: 0.4179829955101013:  78%|████████▌  | 187/241 [00:30<00:10,  4.95it/s]evaluate for the 26-th batch, evaluate loss: 0.4126167595386505:  60%|██████████▊       | 24/40 [00:02<00:01, 11.38it/s]evaluate for the 26-th batch, evaluate loss: 0.4126167595386505:  65%|███████████▋      | 26/40 [00:02<00:01, 11.49it/s]evaluate for the 61-th batch, evaluate loss: 0.5825033187866211:  91%|████████████████▎ | 60/66 [00:09<00:01,  5.54it/s]evaluate for the 61-th batch, evaluate loss: 0.5825033187866211:  92%|████████████████▋ | 61/66 [00:09<00:00,  5.02it/s]evaluate for the 30-th batch, evaluate loss: 0.489763468503952:  61%|███████████▌       | 28/46 [00:03<00:02,  8.97it/s]evaluate for the 30-th batch, evaluate loss: 0.489763468503952:  65%|████████████▍      | 30/46 [00:03<00:01,  8.48it/s]Epoch: 3, train for the 188-th batch, train loss: 0.4179829955101013:  78%|████████▌  | 188/241 [00:30<00:10,  5.04it/s]evaluate for the 27-th batch, evaluate loss: 0.42739903926849365:  65%|███████████      | 26/40 [00:02<00:01, 11.49it/s]evaluate for the 31-th batch, evaluate loss: 0.4233452379703522:  65%|███████████▋      | 30/46 [00:03<00:01,  8.48it/s]evaluate for the 28-th batch, evaluate loss: 0.4141518175601959:  65%|███████████▋      | 26/40 [00:02<00:01, 11.49it/s]evaluate for the 28-th batch, evaluate loss: 0.4141518175601959:  70%|████████████▌     | 28/40 [00:02<00:00, 12.50it/s]Epoch: 3, train for the 189-th batch, train loss: 0.3666570484638214:  78%|████████▌  | 188/241 [00:30<00:10,  5.04it/s]evaluate for the 32-th batch, evaluate loss: 0.46370697021484375:  65%|███████████      | 30/46 [00:03<00:01,  8.48it/s]evaluate for the 32-th batch, evaluate loss: 0.46370697021484375:  70%|███████████▊     | 32/46 [00:03<00:01,  9.06it/s]  0%|                                                                                            | 0/20 [00:00<?, ?it/s]evaluate for the 62-th batch, evaluate loss: 0.5940091609954834:  92%|████████████████▋ | 61/66 [00:10<00:00,  5.02it/s]evaluate for the 62-th batch, evaluate loss: 0.5940091609954834:  94%|████████████████▉ | 62/66 [00:10<00:00,  4.98it/s]Epoch: 3, train for the 189-th batch, train loss: 0.3666570484638214:  78%|████████▋  | 189/241 [00:30<00:10,  5.09it/s]evaluate for the 71-th batch, evaluate loss: 0.3374955952167511:  66%|███████████▏     | 70/106 [00:16<00:08,  4.25it/s]evaluate for the 71-th batch, evaluate loss: 0.3374955952167511:  67%|███████████▍     | 71/106 [00:16<00:08,  4.18it/s]evaluate for the 29-th batch, evaluate loss: 0.4392341375350952:  70%|████████████▌     | 28/40 [00:02<00:00, 12.50it/s]evaluate for the 30-th batch, evaluate loss: 0.4120698571205139:  70%|████████████▌     | 28/40 [00:02<00:00, 12.50it/s]evaluate for the 30-th batch, evaluate loss: 0.4120698571205139:  75%|█████████████▌    | 30/40 [00:02<00:00, 12.20it/s]evaluate for the 1-th batch, evaluate loss: 0.7317979335784912:   0%|                            | 0/20 [00:00<?, ?it/s]evaluate for the 33-th batch, evaluate loss: 0.48284271359443665:  70%|███████████▊     | 32/46 [00:03<00:01,  9.06it/s]evaluate for the 33-th batch, evaluate loss: 0.48284271359443665:  72%|████████████▏    | 33/46 [00:03<00:01,  8.85it/s]Epoch: 3, train for the 190-th batch, train loss: 0.3256179094314575:  78%|████████▋  | 189/241 [00:30<00:10,  5.09it/s]Epoch: 3, train for the 190-th batch, train loss: 0.3256179094314575:  79%|████████▋  | 190/241 [00:30<00:09,  5.39it/s]evaluate for the 2-th batch, evaluate loss: 0.7643412351608276:   0%|                            | 0/20 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.7643412351608276:  10%|██                  | 2/20 [00:00<00:01, 12.16it/s]evaluate for the 31-th batch, evaluate loss: 0.4299548864364624:  75%|█████████████▌    | 30/40 [00:02<00:00, 12.20it/s]evaluate for the 72-th batch, evaluate loss: 0.24350623786449432:  67%|██████████▋     | 71/106 [00:16<00:08,  4.18it/s]evaluate for the 72-th batch, evaluate loss: 0.24350623786449432:  68%|██████████▊     | 72/106 [00:16<00:07,  4.52it/s]evaluate for the 34-th batch, evaluate loss: 0.43537482619285583:  72%|████████████▏    | 33/46 [00:03<00:01,  8.85it/s]evaluate for the 34-th batch, evaluate loss: 0.43537482619285583:  74%|████████████▌    | 34/46 [00:03<00:01,  8.95it/s]evaluate for the 3-th batch, evaluate loss: 0.6224496960639954:  10%|██                  | 2/20 [00:00<00:01, 12.16it/s]evaluate for the 32-th batch, evaluate loss: 0.44217443466186523:  75%|████████████▊    | 30/40 [00:02<00:00, 12.20it/s]evaluate for the 32-th batch, evaluate loss: 0.44217443466186523:  80%|█████████████▌   | 32/40 [00:02<00:00, 12.66it/s]evaluate for the 63-th batch, evaluate loss: 0.5717262029647827:  94%|████████████████▉ | 62/66 [00:10<00:00,  4.98it/s]evaluate for the 63-th batch, evaluate loss: 0.5717262029647827:  95%|█████████████████▏| 63/66 [00:10<00:00,  4.45it/s]Epoch: 3, train for the 191-th batch, train loss: 0.3136751651763916:  79%|████████▋  | 190/241 [00:30<00:09,  5.39it/s]evaluate for the 35-th batch, evaluate loss: 0.49919095635414124:  74%|████████████▌    | 34/46 [00:03<00:01,  8.95it/s]evaluate for the 35-th batch, evaluate loss: 0.49919095635414124:  76%|████████████▉    | 35/46 [00:03<00:01,  9.05it/s]evaluate for the 33-th batch, evaluate loss: 0.4261915385723114:  80%|██████████████▍   | 32/40 [00:02<00:00, 12.66it/s]evaluate for the 4-th batch, evaluate loss: 0.6774065494537354:  10%|██                  | 2/20 [00:00<00:01, 12.16it/s]evaluate for the 4-th batch, evaluate loss: 0.6774065494537354:  20%|████                | 4/20 [00:00<00:01, 11.88it/s]Epoch: 3, train for the 191-th batch, train loss: 0.3136751651763916:  79%|████████▋  | 191/241 [00:30<00:09,  5.46it/s]evaluate for the 34-th batch, evaluate loss: 0.43331581354141235:  80%|█████████████▌   | 32/40 [00:02<00:00, 12.66it/s]evaluate for the 34-th batch, evaluate loss: 0.43331581354141235:  85%|██████████████▍  | 34/40 [00:02<00:00, 12.59it/s]evaluate for the 64-th batch, evaluate loss: 0.558074951171875:  95%|██████████████████▏| 63/66 [00:10<00:00,  4.45it/s]evaluate for the 64-th batch, evaluate loss: 0.558074951171875:  97%|██████████████████▍| 64/66 [00:10<00:00,  5.06it/s]evaluate for the 5-th batch, evaluate loss: 0.6533383131027222:  20%|████                | 4/20 [00:00<00:01, 11.88it/s]evaluate for the 36-th batch, evaluate loss: 0.4554801881313324:  76%|█████████████▋    | 35/46 [00:03<00:01,  9.05it/s]evaluate for the 36-th batch, evaluate loss: 0.4554801881313324:  78%|██████████████    | 36/46 [00:03<00:01,  9.21it/s]evaluate for the 73-th batch, evaluate loss: 0.29025790095329285:  68%|██████████▊     | 72/106 [00:16<00:07,  4.52it/s]evaluate for the 73-th batch, evaluate loss: 0.29025790095329285:  69%|███████████     | 73/106 [00:16<00:07,  4.21it/s]Epoch: 3, train for the 192-th batch, train loss: 0.27978840470314026:  79%|███████▉  | 191/241 [00:30<00:09,  5.46it/s]Epoch: 3, train for the 192-th batch, train loss: 0.27978840470314026:  80%|███████▉  | 192/241 [00:30<00:08,  5.71it/s]evaluate for the 37-th batch, evaluate loss: 0.5071243643760681:  78%|██████████████    | 36/46 [00:03<00:01,  9.21it/s]evaluate for the 35-th batch, evaluate loss: 0.4663598835468292:  85%|███████████████▎  | 34/40 [00:02<00:00, 12.59it/s]evaluate for the 6-th batch, evaluate loss: 0.7300860285758972:  20%|████                | 4/20 [00:00<00:01, 11.88it/s]evaluate for the 6-th batch, evaluate loss: 0.7300860285758972:  30%|██████              | 6/20 [00:00<00:01, 11.33it/s]evaluate for the 65-th batch, evaluate loss: 0.5802940726280212:  97%|█████████████████▍| 64/66 [00:10<00:00,  5.06it/s]evaluate for the 65-th batch, evaluate loss: 0.5802940726280212:  98%|█████████████████▋| 65/66 [00:10<00:00,  5.38it/s]evaluate for the 36-th batch, evaluate loss: 0.4351051151752472:  85%|███████████████▎  | 34/40 [00:03<00:00, 12.59it/s]evaluate for the 36-th batch, evaluate loss: 0.4351051151752472:  90%|████████████████▏ | 36/40 [00:03<00:00, 11.92it/s]evaluate for the 7-th batch, evaluate loss: 0.7384798526763916:  30%|██████              | 6/20 [00:00<00:01, 11.33it/s]evaluate for the 38-th batch, evaluate loss: 0.4878898561000824:  78%|██████████████    | 36/46 [00:03<00:01,  9.21it/s]evaluate for the 38-th batch, evaluate loss: 0.4878898561000824:  83%|██████████████▊   | 38/46 [00:03<00:00,  9.01it/s]Epoch: 3, train for the 193-th batch, train loss: 0.3427644968032837:  80%|████████▊  | 192/241 [00:31<00:08,  5.71it/s]evaluate for the 37-th batch, evaluate loss: 0.4541347324848175:  90%|████████████████▏ | 36/40 [00:03<00:00, 11.92it/s]evaluate for the 74-th batch, evaluate loss: 0.38182491064071655:  69%|███████████     | 73/106 [00:17<00:07,  4.21it/s]evaluate for the 74-th batch, evaluate loss: 0.38182491064071655:  70%|███████████▏    | 74/106 [00:17<00:07,  4.37it/s]Epoch: 3, train for the 193-th batch, train loss: 0.3427644968032837:  80%|████████▊  | 193/241 [00:31<00:08,  5.59it/s]evaluate for the 66-th batch, evaluate loss: 0.5628577470779419:  98%|█████████████████▋| 65/66 [00:10<00:00,  5.38it/s]evaluate for the 66-th batch, evaluate loss: 0.5628577470779419: 100%|██████████████████| 66/66 [00:10<00:00,  5.83it/s]evaluate for the 66-th batch, evaluate loss: 0.5628577470779419: 100%|██████████████████| 66/66 [00:10<00:00,  6.12it/s]
evaluate for the 8-th batch, evaluate loss: 0.6678118705749512:  30%|██████              | 6/20 [00:00<00:01, 11.33it/s]evaluate for the 8-th batch, evaluate loss: 0.6678118705749512:  40%|████████            | 8/20 [00:00<00:01, 10.70it/s]evaluate for the 38-th batch, evaluate loss: 0.4802039861679077:  90%|████████████████▏ | 36/40 [00:03<00:00, 11.92it/s]evaluate for the 38-th batch, evaluate loss: 0.4802039861679077:  95%|█████████████████ | 38/40 [00:03<00:00, 12.55it/s]evaluate for the 39-th batch, evaluate loss: 0.46934714913368225:  83%|██████████████   | 38/46 [00:04<00:00,  9.01it/s]evaluate for the 39-th batch, evaluate loss: 0.46934714913368225:  85%|██████████████▍  | 39/46 [00:04<00:00,  9.14it/s]evaluate for the 39-th batch, evaluate loss: 0.49132758378982544:  95%|████████████████▏| 38/40 [00:03<00:00, 12.55it/s]evaluate for the 9-th batch, evaluate loss: 0.6465505361557007:  40%|████████            | 8/20 [00:00<00:01, 10.70it/s]Epoch: 3, train for the 194-th batch, train loss: 0.3071230351924896:  80%|████████▊  | 193/241 [00:31<00:08,  5.59it/s]evaluate for the 40-th batch, evaluate loss: 0.466051310300827:  85%|████████████████   | 39/46 [00:04<00:00,  9.14it/s]evaluate for the 40-th batch, evaluate loss: 0.34626778960227966:  95%|████████████████▏| 38/40 [00:03<00:00, 12.55it/s]evaluate for the 40-th batch, evaluate loss: 0.34626778960227966: 100%|█████████████████| 40/40 [00:03<00:00, 13.51it/s]evaluate for the 40-th batch, evaluate loss: 0.34626778960227966: 100%|█████████████████| 40/40 [00:03<00:00, 12.08it/s]
Epoch: 3, train for the 194-th batch, train loss: 0.3071230351924896:  80%|████████▊  | 194/241 [00:31<00:08,  5.65it/s]evaluate for the 10-th batch, evaluate loss: 0.6309947967529297:  40%|███████▌           | 8/20 [00:00<00:01, 10.70it/s]evaluate for the 10-th batch, evaluate loss: 0.6309947967529297:  50%|█████████         | 10/20 [00:00<00:00, 11.26it/s]evaluate for the 41-th batch, evaluate loss: 0.48063838481903076:  85%|██████████████▍  | 39/46 [00:04<00:00,  9.14it/s]evaluate for the 41-th batch, evaluate loss: 0.48063838481903076:  89%|███████████████▏ | 41/46 [00:04<00:00, 10.13it/s]evaluate for the 11-th batch, evaluate loss: 0.6266770958900452:  50%|█████████         | 10/20 [00:00<00:00, 11.26it/s]evaluate for the 75-th batch, evaluate loss: 0.3269205689430237:  70%|███████████▊     | 74/106 [00:17<00:07,  4.37it/s]evaluate for the 75-th batch, evaluate loss: 0.3269205689430237:  71%|████████████     | 75/106 [00:17<00:07,  3.89it/s]evaluate for the 12-th batch, evaluate loss: 0.6925085186958313:  50%|█████████         | 10/20 [00:01<00:00, 11.26it/s]evaluate for the 12-th batch, evaluate loss: 0.6925085186958313:  60%|██████████▊       | 12/20 [00:01<00:00, 12.77it/s]Epoch: 3, train for the 195-th batch, train loss: 0.5515239834785461:  80%|████████▊  | 194/241 [00:31<00:08,  5.65it/s]evaluate for the 42-th batch, evaluate loss: 0.4417058825492859:  89%|████████████████  | 41/46 [00:04<00:00, 10.13it/s]Epoch: 3, train for the 195-th batch, train loss: 0.5515239834785461:  81%|████████▉  | 195/241 [00:31<00:08,  5.63it/s]evaluate for the 13-th batch, evaluate loss: 0.7018572688102722:  60%|██████████▊       | 12/20 [00:01<00:00, 12.77it/s]  0%|                                                                                            | 0/21 [00:00<?, ?it/s]evaluate for the 14-th batch, evaluate loss: 0.7074980139732361:  60%|██████████▊       | 12/20 [00:01<00:00, 12.77it/s]evaluate for the 14-th batch, evaluate loss: 0.7074980139732361:  70%|████████████▌     | 14/20 [00:01<00:00, 13.31it/s]evaluate for the 43-th batch, evaluate loss: 0.5481374859809875:  89%|████████████████  | 41/46 [00:04<00:00, 10.13it/s]evaluate for the 43-th batch, evaluate loss: 0.5481374859809875:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.76it/s]evaluate for the 76-th batch, evaluate loss: 0.3884434103965759:  71%|████████████     | 75/106 [00:17<00:07,  3.89it/s]evaluate for the 76-th batch, evaluate loss: 0.3884434103965759:  72%|████████████▏    | 76/106 [00:17<00:07,  4.21it/s]evaluate for the 1-th batch, evaluate loss: 0.6066064238548279:   0%|                            | 0/21 [00:00<?, ?it/s]evaluate for the 15-th batch, evaluate loss: 0.7123553156852722:  70%|████████████▌     | 14/20 [00:01<00:00, 13.31it/s]Epoch: 3, train for the 196-th batch, train loss: 0.4422334134578705:  81%|████████▉  | 195/241 [00:31<00:08,  5.63it/s]Epoch: 3, train for the 196-th batch, train loss: 0.4422334134578705:  81%|████████▉  | 196/241 [00:31<00:08,  5.42it/s]evaluate for the 2-th batch, evaluate loss: 0.6960766911506653:   0%|                            | 0/21 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.6960766911506653:  10%|█▉                  | 2/21 [00:00<00:01, 13.57it/s]evaluate for the 16-th batch, evaluate loss: 0.6610820889472961:  70%|████████████▌     | 14/20 [00:01<00:00, 13.31it/s]evaluate for the 16-th batch, evaluate loss: 0.6610820889472961:  80%|██████████████▍   | 16/20 [00:01<00:00, 13.44it/s]Epoch: 3, train for the 197-th batch, train loss: 0.5262102484703064:  81%|████████▉  | 196/241 [00:31<00:08,  5.42it/s]Epoch: 3, train for the 197-th batch, train loss: 0.5262102484703064:  82%|████████▉  | 197/241 [00:31<00:07,  5.86it/s]evaluate for the 17-th batch, evaluate loss: 0.697253942489624:  80%|███████████████▏   | 16/20 [00:01<00:00, 13.44it/s]evaluate for the 3-th batch, evaluate loss: 0.6738580465316772:  10%|█▉                  | 2/21 [00:00<00:01, 13.57it/s]evaluate for the 44-th batch, evaluate loss: 0.4855611026287079:  93%|████████████████▊ | 43/46 [00:04<00:00,  9.76it/s]evaluate for the 44-th batch, evaluate loss: 0.4855611026287079:  96%|█████████████████▏| 44/46 [00:04<00:00,  7.67it/s]evaluate for the 77-th batch, evaluate loss: 0.4202520251274109:  72%|████████████▏    | 76/106 [00:17<00:07,  4.21it/s]evaluate for the 77-th batch, evaluate loss: 0.4202520251274109:  73%|████████████▎    | 77/106 [00:17<00:07,  4.11it/s]  0%|                                                                                            | 0/40 [00:00<?, ?it/s]evaluate for the 45-th batch, evaluate loss: 0.4643806219100952:  96%|█████████████████▏| 44/46 [00:04<00:00,  7.67it/s]evaluate for the 4-th batch, evaluate loss: 0.5976730585098267:  10%|█▉                  | 2/21 [00:00<00:01, 13.57it/s]evaluate for the 4-th batch, evaluate loss: 0.5976730585098267:  19%|███▊                | 4/21 [00:00<00:01, 10.79it/s]evaluate for the 18-th batch, evaluate loss: 0.6756470203399658:  80%|██████████████▍   | 16/20 [00:01<00:00, 13.44it/s]evaluate for the 18-th batch, evaluate loss: 0.6756470203399658:  90%|████████████████▏ | 18/20 [00:01<00:00, 12.17it/s]Epoch: 3, train for the 198-th batch, train loss: 0.490966796875:  82%|████████████▎  | 197/241 [00:31<00:07,  5.86it/s]Epoch: 3, train for the 198-th batch, train loss: 0.490966796875:  82%|████████████▎  | 198/241 [00:31<00:07,  5.71it/s]evaluate for the 1-th batch, evaluate loss: 0.6492972373962402:   0%|                            | 0/40 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6492972373962402:   2%|▌                   | 1/40 [00:00<00:04,  8.66it/s]evaluate for the 46-th batch, evaluate loss: 0.44869914650917053:  96%|████████████████▎| 44/46 [00:04<00:00,  7.67it/s]evaluate for the 46-th batch, evaluate loss: 0.44869914650917053: 100%|█████████████████| 46/46 [00:04<00:00,  8.40it/s]evaluate for the 46-th batch, evaluate loss: 0.44869914650917053: 100%|█████████████████| 46/46 [00:04<00:00,  9.43it/s]
evaluate for the 5-th batch, evaluate loss: 0.6863576173782349:  19%|███▊                | 4/21 [00:00<00:01, 10.79it/s]evaluate for the 19-th batch, evaluate loss: 0.7247641086578369:  90%|████████████████▏ | 18/20 [00:01<00:00, 12.17it/s]evaluate for the 2-th batch, evaluate loss: 0.6297942996025085:   2%|▌                   | 1/40 [00:00<00:04,  8.66it/s]evaluate for the 2-th batch, evaluate loss: 0.6297942996025085:   5%|█                   | 2/40 [00:00<00:04,  9.36it/s]evaluate for the 20-th batch, evaluate loss: 0.6912842988967896:  90%|████████████████▏ | 18/20 [00:01<00:00, 12.17it/s]evaluate for the 20-th batch, evaluate loss: 0.6912842988967896: 100%|██████████████████| 20/20 [00:01<00:00, 11.37it/s]evaluate for the 20-th batch, evaluate loss: 0.6912842988967896: 100%|██████████████████| 20/20 [00:01<00:00, 11.87it/s]
evaluate for the 78-th batch, evaluate loss: 0.35287874937057495:  73%|███████████▌    | 77/106 [00:18<00:07,  4.11it/s]evaluate for the 78-th batch, evaluate loss: 0.35287874937057495:  74%|███████████▊    | 78/106 [00:18<00:07,  3.93it/s]evaluate for the 6-th batch, evaluate loss: 0.664029598236084:  19%|████                 | 4/21 [00:00<00:01, 10.79it/s]evaluate for the 6-th batch, evaluate loss: 0.664029598236084:  29%|██████               | 6/21 [00:00<00:01,  9.21it/s]Epoch: 3, train for the 199-th batch, train loss: 0.3339192271232605:  82%|█████████  | 198/241 [00:32<00:07,  5.71it/s]Epoch: 3, train for the 199-th batch, train loss: 0.3339192271232605:  83%|█████████  | 199/241 [00:32<00:07,  5.37it/s]evaluate for the 3-th batch, evaluate loss: 0.620964527130127:   5%|█                    | 2/40 [00:00<00:04,  9.36it/s]evaluate for the 7-th batch, evaluate loss: 0.6213513612747192:  29%|█████▋              | 6/21 [00:00<00:01,  9.21it/s]INFO:root:Epoch: 5, learning rate: 0.0001, train loss: 0.5262
INFO:root:train average_precision, 0.8412
INFO:root:train roc_auc, 0.8273
INFO:root:validate loss: 0.4608
INFO:root:validate average_precision, 0.8697
INFO:root:validate roc_auc, 0.8622
INFO:root:new node validate loss: 0.6877
INFO:root:new node validate first_1_average_precision, 0.5702
INFO:root:new node validate first_1_roc_auc, 0.5092
INFO:root:new node validate first_3_average_precision, 0.6131
INFO:root:new node validate first_3_roc_auc, 0.5657
INFO:root:new node validate first_10_average_precision, 0.6419
INFO:root:new node validate first_10_roc_auc, 0.6238
INFO:root:new node validate average_precision, 0.6869
INFO:root:new node validate roc_auc, 0.6783
INFO:root:save model ./saved_models/TGN/ia-escorts-dynamic/TGN_seed0_tgn-ia-escorts-dynamic-lincorrect-time-linear/TGN_seed0_tgn-ia-escorts-dynamic-lincorrect-time-linear.pkl
evaluate for the 8-th batch, evaluate loss: 0.6242208480834961:  29%|█████▋              | 6/21 [00:00<00:01,  9.21it/s]evaluate for the 8-th batch, evaluate loss: 0.6242208480834961:  38%|███████▌            | 8/21 [00:00<00:01, 10.20it/s]  0%|                                                                                            | 0/25 [00:00<?, ?it/s]evaluate for the 79-th batch, evaluate loss: 0.4011825919151306:  74%|████████████▌    | 78/106 [00:18<00:07,  3.93it/s]evaluate for the 79-th batch, evaluate loss: 0.4011825919151306:  75%|████████████▋    | 79/106 [00:18<00:06,  4.20it/s]evaluate for the 4-th batch, evaluate loss: 0.6367027759552002:   5%|█                   | 2/40 [00:00<00:04,  9.36it/s]evaluate for the 4-th batch, evaluate loss: 0.6367027759552002:  10%|██                  | 4/40 [00:00<00:04,  8.61it/s]Epoch: 3, train for the 200-th batch, train loss: 0.41377657651901245:  83%|████████▎ | 199/241 [00:32<00:07,  5.37it/s]evaluate for the 9-th batch, evaluate loss: 0.6153837442398071:  38%|███████▌            | 8/21 [00:00<00:01, 10.20it/s]Epoch: 3, train for the 200-th batch, train loss: 0.41377657651901245:  83%|████████▎ | 200/241 [00:32<00:07,  5.41it/s]evaluate for the 10-th batch, evaluate loss: 0.6448696255683899:  38%|███████▏           | 8/21 [00:00<00:01, 10.20it/s]evaluate for the 1-th batch, evaluate loss: 0.7508164644241333:   0%|                            | 0/25 [00:00<?, ?it/s]evaluate for the 11-th batch, evaluate loss: 0.6485860347747803:  38%|███████▏           | 8/21 [00:00<00:01, 10.20it/s]evaluate for the 11-th batch, evaluate loss: 0.6485860347747803:  52%|█████████▍        | 11/21 [00:00<00:00, 12.92it/s]evaluate for the 5-th batch, evaluate loss: 0.593591570854187:  10%|██                   | 4/40 [00:00<00:04,  8.61it/s]evaluate for the 5-th batch, evaluate loss: 0.593591570854187:  12%|██▋                  | 5/40 [00:00<00:04,  8.03it/s]evaluate for the 80-th batch, evaluate loss: 0.40524619817733765:  75%|███████████▉    | 79/106 [00:18<00:06,  4.20it/s]evaluate for the 80-th batch, evaluate loss: 0.40524619817733765:  75%|████████████    | 80/106 [00:18<00:05,  4.51it/s]Epoch: 3, train for the 201-th batch, train loss: 0.4607952833175659:  83%|█████████▏ | 200/241 [00:32<00:07,  5.41it/s]evaluate for the 12-th batch, evaluate loss: 0.6497653722763062:  52%|█████████▍        | 11/21 [00:01<00:00, 12.92it/s]evaluate for the 2-th batch, evaluate loss: 0.76417475938797:   0%|                              | 0/25 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.76417475938797:   8%|█▊                    | 2/25 [00:00<00:02,  9.61it/s]Epoch: 3, train for the 201-th batch, train loss: 0.4607952833175659:  83%|█████████▏ | 201/241 [00:32<00:07,  5.34it/s]evaluate for the 6-th batch, evaluate loss: 0.6044464111328125:  12%|██▌                 | 5/40 [00:00<00:04,  8.03it/s]evaluate for the 13-th batch, evaluate loss: 0.6346743702888489:  52%|█████████▍        | 11/21 [00:01<00:00, 12.92it/s]evaluate for the 13-th batch, evaluate loss: 0.6346743702888489:  62%|███████████▏      | 13/21 [00:01<00:00, 13.46it/s]evaluate for the 3-th batch, evaluate loss: 0.7836476564407349:   8%|█▌                  | 2/25 [00:00<00:02,  9.61it/s]evaluate for the 3-th batch, evaluate loss: 0.7836476564407349:  12%|██▍                 | 3/25 [00:00<00:02,  9.75it/s]evaluate for the 7-th batch, evaluate loss: 0.6333863735198975:  12%|██▌                 | 5/40 [00:00<00:04,  8.03it/s]evaluate for the 7-th batch, evaluate loss: 0.6333863735198975:  18%|███▌                | 7/40 [00:00<00:03,  8.91it/s]evaluate for the 14-th batch, evaluate loss: 0.6071053743362427:  62%|███████████▏      | 13/21 [00:01<00:00, 13.46it/s]Epoch: 3, train for the 202-th batch, train loss: 0.2452079951763153:  83%|█████████▏ | 201/241 [00:32<00:07,  5.34it/s]evaluate for the 15-th batch, evaluate loss: 0.6445562839508057:  62%|███████████▏      | 13/21 [00:01<00:00, 13.46it/s]evaluate for the 15-th batch, evaluate loss: 0.6445562839508057:  71%|████████████▊     | 15/21 [00:01<00:00, 14.32it/s]evaluate for the 4-th batch, evaluate loss: 0.7307031154632568:  12%|██▍                 | 3/25 [00:00<00:02,  9.75it/s]Epoch: 3, train for the 202-th batch, train loss: 0.2452079951763153:  84%|█████████▏ | 202/241 [00:32<00:07,  5.42it/s]evaluate for the 16-th batch, evaluate loss: 0.6380161046981812:  71%|████████████▊     | 15/21 [00:01<00:00, 14.32it/s]evaluate for the 81-th batch, evaluate loss: 0.4594483971595764:  75%|████████████▊    | 80/106 [00:18<00:05,  4.51it/s]evaluate for the 81-th batch, evaluate loss: 0.4594483971595764:  76%|████████████▉    | 81/106 [00:18<00:06,  3.98it/s]evaluate for the 8-th batch, evaluate loss: 0.6426605582237244:  18%|███▌                | 7/40 [00:00<00:03,  8.91it/s]evaluate for the 8-th batch, evaluate loss: 0.6426605582237244:  20%|████                | 8/40 [00:00<00:03,  8.02it/s]evaluate for the 5-th batch, evaluate loss: 0.7369292974472046:  12%|██▍                 | 3/25 [00:00<00:02,  9.75it/s]evaluate for the 5-th batch, evaluate loss: 0.7369292974472046:  20%|████                | 5/25 [00:00<00:02,  9.69it/s]evaluate for the 17-th batch, evaluate loss: 0.5482620000839233:  71%|████████████▊     | 15/21 [00:01<00:00, 14.32it/s]evaluate for the 17-th batch, evaluate loss: 0.5482620000839233:  81%|██████████████▌   | 17/21 [00:01<00:00, 14.58it/s]Epoch: 3, train for the 203-th batch, train loss: 0.5755848288536072:  84%|█████████▏ | 202/241 [00:32<00:07,  5.42it/s]Epoch: 3, train for the 203-th batch, train loss: 0.5755848288536072:  84%|█████████▎ | 203/241 [00:32<00:06,  5.49it/s]evaluate for the 6-th batch, evaluate loss: 0.7540473937988281:  20%|████                | 5/25 [00:00<00:02,  9.69it/s]evaluate for the 9-th batch, evaluate loss: 0.659869909286499:  20%|████▏                | 8/40 [00:01<00:03,  8.02it/s]evaluate for the 9-th batch, evaluate loss: 0.659869909286499:  22%|████▋                | 9/40 [00:01<00:03,  8.35it/s]evaluate for the 18-th batch, evaluate loss: 0.6211116909980774:  81%|██████████████▌   | 17/21 [00:01<00:00, 14.58it/s]evaluate for the 19-th batch, evaluate loss: 0.6239452958106995:  81%|██████████████▌   | 17/21 [00:01<00:00, 14.58it/s]evaluate for the 19-th batch, evaluate loss: 0.6239452958106995:  90%|████████████████▎ | 19/21 [00:01<00:00, 13.60it/s]evaluate for the 7-th batch, evaluate loss: 0.7575826644897461:  20%|████                | 5/25 [00:00<00:02,  9.69it/s]evaluate for the 7-th batch, evaluate loss: 0.7575826644897461:  28%|█████▌              | 7/25 [00:00<00:01, 10.01it/s]Epoch: 3, train for the 204-th batch, train loss: 0.4202386736869812:  84%|█████████▎ | 203/241 [00:33<00:06,  5.49it/s]evaluate for the 82-th batch, evaluate loss: 0.3885854184627533:  76%|████████████▉    | 81/106 [00:19<00:06,  3.98it/s]evaluate for the 82-th batch, evaluate loss: 0.3885854184627533:  77%|█████████████▏   | 82/106 [00:19<00:05,  4.08it/s]evaluate for the 10-th batch, evaluate loss: 0.6159284114837646:  22%|████▎              | 9/40 [00:01<00:03,  8.35it/s]evaluate for the 10-th batch, evaluate loss: 0.6159284114837646:  25%|████▌             | 10/40 [00:01<00:03,  8.29it/s]Epoch: 3, train for the 204-th batch, train loss: 0.4202386736869812:  85%|█████████▎ | 204/241 [00:33<00:06,  5.57it/s]evaluate for the 20-th batch, evaluate loss: 0.6142163276672363:  90%|████████████████▎ | 19/21 [00:01<00:00, 13.60it/s]evaluate for the 8-th batch, evaluate loss: 0.7204459309577942:  28%|█████▌              | 7/25 [00:00<00:01, 10.01it/s]evaluate for the 21-th batch, evaluate loss: 0.48584839701652527:  90%|███████████████▍ | 19/21 [00:01<00:00, 13.60it/s]evaluate for the 21-th batch, evaluate loss: 0.48584839701652527: 100%|█████████████████| 21/21 [00:01<00:00, 14.68it/s]evaluate for the 21-th batch, evaluate loss: 0.48584839701652527: 100%|█████████████████| 21/21 [00:01<00:00, 13.10it/s]
evaluate for the 11-th batch, evaluate loss: 0.6354253888130188:  25%|████▌             | 10/40 [00:01<00:03,  8.29it/s]evaluate for the 11-th batch, evaluate loss: 0.6354253888130188:  28%|████▉             | 11/40 [00:01<00:03,  8.41it/s]evaluate for the 83-th batch, evaluate loss: 0.319304883480072:  77%|█████████████▉    | 82/106 [00:19<00:05,  4.08it/s]evaluate for the 83-th batch, evaluate loss: 0.319304883480072:  78%|██████████████    | 83/106 [00:19<00:05,  4.44it/s]evaluate for the 9-th batch, evaluate loss: 0.7172895669937134:  28%|█████▌              | 7/25 [00:00<00:01, 10.01it/s]evaluate for the 9-th batch, evaluate loss: 0.7172895669937134:  36%|███████▏            | 9/25 [00:00<00:01,  9.83it/s]Epoch: 3, train for the 205-th batch, train loss: 0.45921018719673157:  85%|████████▍ | 204/241 [00:33<00:06,  5.57it/s]Epoch: 3, train for the 205-th batch, train loss: 0.45921018719673157:  85%|████████▌ | 205/241 [00:33<00:06,  5.55it/s]INFO:root:Epoch: 6, learning rate: 0.0001, train loss: 0.4519
INFO:root:train average_precision, 0.8748
INFO:root:train roc_auc, 0.8640
INFO:root:validate loss: 0.4500
INFO:root:validate average_precision, 0.8790
INFO:root:validate roc_auc, 0.8745
INFO:root:new node validate loss: 0.6260
INFO:root:new node validate first_1_average_precision, 0.6962
INFO:root:new node validate first_1_roc_auc, 0.6909
INFO:root:new node validate first_3_average_precision, 0.7262
INFO:root:new node validate first_3_roc_auc, 0.7257
INFO:root:new node validate first_10_average_precision, 0.7331
INFO:root:new node validate first_10_roc_auc, 0.7431
INFO:root:new node validate average_precision, 0.7356
INFO:root:new node validate roc_auc, 0.7602
INFO:root:save model ./saved_models/TGN/ia-reality-call/TGN_seed0_tgn-ia-reality-call-lincorrect-time-linear/TGN_seed0_tgn-ia-reality-call-lincorrect-time-linear.pkl
evaluate for the 12-th batch, evaluate loss: 0.7010229229927063:  28%|████▉             | 11/40 [00:01<00:03,  8.41it/s]evaluate for the 12-th batch, evaluate loss: 0.7010229229927063:  30%|█████▍            | 12/40 [00:01<00:03,  8.08it/s]evaluate for the 10-th batch, evaluate loss: 0.7504820227622986:  36%|██████▊            | 9/25 [00:00<00:01,  9.83it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
Epoch: 3, train for the 206-th batch, train loss: 0.5639731287956238:  85%|█████████▎ | 205/241 [00:33<00:06,  5.55it/s]  0%|                                                                                           | 0/146 [00:00<?, ?it/s]evaluate for the 10-th batch, evaluate loss: 0.7504820227622986:  40%|███████▏          | 10/25 [00:01<00:01,  8.81it/s]evaluate for the 84-th batch, evaluate loss: 0.39284515380859375:  78%|████████████▌   | 83/106 [00:19<00:05,  4.44it/s]evaluate for the 84-th batch, evaluate loss: 0.39284515380859375:  79%|████████████▋   | 84/106 [00:19<00:04,  4.73it/s]Epoch: 3, train for the 206-th batch, train loss: 0.5639731287956238:  85%|█████████▍ | 206/241 [00:33<00:06,  5.79it/s]evaluate for the 13-th batch, evaluate loss: 0.639255940914154:  30%|█████▋             | 12/40 [00:01<00:03,  8.08it/s]evaluate for the 13-th batch, evaluate loss: 0.639255940914154:  32%|██████▏            | 13/40 [00:01<00:03,  8.16it/s]Epoch: 6, train for the 1-th batch, train loss: 0.7178089618682861:   0%|                       | 0/146 [00:00<?, ?it/s]evaluate for the 11-th batch, evaluate loss: 0.7459564208984375:  40%|███████▏          | 10/25 [00:01<00:01,  8.81it/s]evaluate for the 14-th batch, evaluate loss: 0.6999057531356812:  32%|█████▊            | 13/40 [00:01<00:03,  8.16it/s]evaluate for the 14-th batch, evaluate loss: 0.6999057531356812:  35%|██████▎           | 14/40 [00:01<00:03,  8.28it/s]Epoch: 6, train for the 2-th batch, train loss: 0.6694633364677429:   0%|                       | 0/146 [00:00<?, ?it/s]Epoch: 6, train for the 2-th batch, train loss: 0.6694633364677429:   1%|▏              | 2/146 [00:00<00:12, 11.92it/s]Epoch: 3, train for the 207-th batch, train loss: 0.5556160807609558:  85%|█████████▍ | 206/241 [00:33<00:06,  5.79it/s]evaluate for the 12-th batch, evaluate loss: 0.6499804258346558:  40%|███████▏          | 10/25 [00:01<00:01,  8.81it/s]evaluate for the 12-th batch, evaluate loss: 0.6499804258346558:  48%|████████▋         | 12/25 [00:01<00:01,  9.39it/s]Epoch: 3, train for the 207-th batch, train loss: 0.5556160807609558:  86%|█████████▍ | 207/241 [00:33<00:05,  5.74it/s]evaluate for the 15-th batch, evaluate loss: 0.6708881855010986:  35%|██████▎           | 14/40 [00:01<00:03,  8.28it/s]Epoch: 6, train for the 3-th batch, train loss: 0.6571868062019348:   1%|▏              | 2/146 [00:00<00:12, 11.92it/s]evaluate for the 13-th batch, evaluate loss: 0.6819968223571777:  48%|████████▋         | 12/25 [00:01<00:01,  9.39it/s]evaluate for the 85-th batch, evaluate loss: 0.4488881826400757:  79%|█████████████▍   | 84/106 [00:19<00:04,  4.73it/s]evaluate for the 85-th batch, evaluate loss: 0.4488881826400757:  80%|█████████████▋   | 85/106 [00:19<00:05,  4.17it/s]Epoch: 6, train for the 4-th batch, train loss: 0.6529173254966736:   1%|▏              | 2/146 [00:00<00:12, 11.92it/s]Epoch: 6, train for the 4-th batch, train loss: 0.6529173254966736:   3%|▍              | 4/146 [00:00<00:11, 11.95it/s]Epoch: 3, train for the 208-th batch, train loss: 0.5333179235458374:  86%|█████████▍ | 207/241 [00:33<00:05,  5.74it/s]evaluate for the 16-th batch, evaluate loss: 0.6484521627426147:  35%|██████▎           | 14/40 [00:01<00:03,  8.28it/s]evaluate for the 16-th batch, evaluate loss: 0.6484521627426147:  40%|███████▏          | 16/40 [00:01<00:02,  8.99it/s]evaluate for the 14-th batch, evaluate loss: 0.7113545536994934:  48%|████████▋         | 12/25 [00:01<00:01,  9.39it/s]evaluate for the 14-th batch, evaluate loss: 0.7113545536994934:  56%|██████████        | 14/25 [00:01<00:01, 10.12it/s]Epoch: 3, train for the 208-th batch, train loss: 0.5333179235458374:  86%|█████████▍ | 208/241 [00:33<00:05,  5.81it/s]Epoch: 6, train for the 5-th batch, train loss: 0.6463714241981506:   3%|▍              | 4/146 [00:00<00:11, 11.95it/s]evaluate for the 15-th batch, evaluate loss: 0.7506894469261169:  56%|██████████        | 14/25 [00:01<00:01, 10.12it/s]evaluate for the 17-th batch, evaluate loss: 0.6171595454216003:  40%|███████▏          | 16/40 [00:02<00:02,  8.99it/s]evaluate for the 17-th batch, evaluate loss: 0.6171595454216003:  42%|███████▋          | 17/40 [00:02<00:02,  8.22it/s]evaluate for the 86-th batch, evaluate loss: 0.43683966994285583:  80%|████████████▊   | 85/106 [00:20<00:05,  4.17it/s]evaluate for the 86-th batch, evaluate loss: 0.43683966994285583:  81%|████████████▉   | 86/106 [00:20<00:04,  4.51it/s]Epoch: 6, train for the 6-th batch, train loss: 0.6367772221565247:   3%|▍              | 4/146 [00:00<00:11, 11.95it/s]Epoch: 6, train for the 6-th batch, train loss: 0.6367772221565247:   4%|▌              | 6/146 [00:00<00:12, 11.60it/s]Epoch: 3, train for the 209-th batch, train loss: 0.5289726257324219:  86%|█████████▍ | 208/241 [00:33<00:05,  5.81it/s]evaluate for the 16-th batch, evaluate loss: 0.6744809746742249:  56%|██████████        | 14/25 [00:01<00:01, 10.12it/s]evaluate for the 16-th batch, evaluate loss: 0.6744809746742249:  64%|███████████▌      | 16/25 [00:01<00:00, 10.37it/s]Epoch: 3, train for the 209-th batch, train loss: 0.5289726257324219:  87%|█████████▌ | 209/241 [00:33<00:05,  5.61it/s]Epoch: 6, train for the 7-th batch, train loss: 0.6150479316711426:   4%|▌              | 6/146 [00:00<00:12, 11.60it/s]evaluate for the 17-th batch, evaluate loss: 0.6530519127845764:  64%|███████████▌      | 16/25 [00:01<00:00, 10.37it/s]evaluate for the 18-th batch, evaluate loss: 0.6335228681564331:  42%|███████▋          | 17/40 [00:02<00:02,  8.22it/s]evaluate for the 18-th batch, evaluate loss: 0.6335228681564331:  45%|████████          | 18/40 [00:02<00:02,  8.23it/s]Epoch: 6, train for the 8-th batch, train loss: 0.5777250528335571:   4%|▌              | 6/146 [00:00<00:12, 11.60it/s]Epoch: 6, train for the 8-th batch, train loss: 0.5777250528335571:   5%|▊              | 8/146 [00:00<00:12, 11.36it/s]evaluate for the 87-th batch, evaluate loss: 0.3917178213596344:  81%|█████████████▊   | 86/106 [00:20<00:04,  4.51it/s]evaluate for the 87-th batch, evaluate loss: 0.3917178213596344:  82%|█████████████▉   | 87/106 [00:20<00:04,  4.67it/s]Epoch: 3, train for the 210-th batch, train loss: 0.40554291009902954:  87%|████████▋ | 209/241 [00:34<00:05,  5.61it/s]evaluate for the 19-th batch, evaluate loss: 0.6957200169563293:  45%|████████          | 18/40 [00:02<00:02,  8.23it/s]evaluate for the 19-th batch, evaluate loss: 0.6957200169563293:  48%|████████▌         | 19/40 [00:02<00:02,  8.19it/s]evaluate for the 18-th batch, evaluate loss: 0.6115533113479614:  64%|███████████▌      | 16/25 [00:01<00:00, 10.37it/s]evaluate for the 18-th batch, evaluate loss: 0.6115533113479614:  72%|████████████▉     | 18/25 [00:01<00:00, 10.14it/s]Epoch: 3, train for the 210-th batch, train loss: 0.40554291009902954:  87%|████████▋ | 210/241 [00:34<00:05,  5.42it/s]Epoch: 6, train for the 9-th batch, train loss: 0.575234591960907:   5%|▉               | 8/146 [00:00<00:12, 11.36it/s]Epoch: 6, train for the 10-th batch, train loss: 0.5743640065193176:   5%|▊             | 8/146 [00:00<00:12, 11.36it/s]Epoch: 6, train for the 10-th batch, train loss: 0.5743640065193176:   7%|▉            | 10/146 [00:00<00:12, 11.13it/s]evaluate for the 88-th batch, evaluate loss: 0.42178046703338623:  82%|█████████████▏  | 87/106 [00:20<00:04,  4.67it/s]Epoch: 3, train for the 211-th batch, train loss: 0.38884237408638:  87%|███████████▎ | 210/241 [00:34<00:05,  5.42it/s]evaluate for the 88-th batch, evaluate loss: 0.42178046703338623:  83%|█████████████▎  | 88/106 [00:20<00:03,  4.66it/s]evaluate for the 20-th batch, evaluate loss: 0.6556814312934875:  48%|████████▌         | 19/40 [00:02<00:02,  8.19it/s]evaluate for the 20-th batch, evaluate loss: 0.6556814312934875:  50%|█████████         | 20/40 [00:02<00:02,  7.30it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]Epoch: 3, train for the 211-th batch, train loss: 0.38884237408638:  88%|███████████▍ | 211/241 [00:34<00:05,  5.41it/s]Epoch: 6, train for the 11-th batch, train loss: 0.5895408391952515:   7%|▉            | 10/146 [00:00<00:12, 11.13it/s]evaluate for the 19-th batch, evaluate loss: 0.6171001195907593:  72%|████████████▉     | 18/25 [00:02<00:00, 10.14it/s]Epoch: 7, train for the 1-th batch, train loss: 0.6779261827468872:   0%|                       | 0/119 [00:00<?, ?it/s]evaluate for the 21-th batch, evaluate loss: 0.675980806350708:  50%|█████████▌         | 20/40 [00:02<00:02,  7.30it/s]evaluate for the 21-th batch, evaluate loss: 0.675980806350708:  52%|█████████▉         | 21/40 [00:02<00:02,  7.58it/s]Epoch: 7, train for the 1-th batch, train loss: 0.6779261827468872:   1%|▏              | 1/119 [00:00<00:12,  9.49it/s]evaluate for the 20-th batch, evaluate loss: 0.6637516021728516:  72%|████████████▉     | 18/25 [00:02<00:00, 10.14it/s]evaluate for the 20-th batch, evaluate loss: 0.6637516021728516:  80%|██████████████▍   | 20/25 [00:02<00:00,  8.59it/s]Epoch: 6, train for the 12-th batch, train loss: 0.5709819793701172:   7%|▉            | 10/146 [00:01<00:12, 11.13it/s]Epoch: 6, train for the 12-th batch, train loss: 0.5709819793701172:   8%|█            | 12/146 [00:01<00:12, 10.70it/s]Epoch: 3, train for the 212-th batch, train loss: 0.49036917090415955:  88%|████████▊ | 211/241 [00:34<00:05,  5.41it/s]Epoch: 3, train for the 212-th batch, train loss: 0.49036917090415955:  88%|████████▊ | 212/241 [00:34<00:05,  5.41it/s]evaluate for the 21-th batch, evaluate loss: 0.666012704372406:  80%|███████████████▏   | 20/25 [00:02<00:00,  8.59it/s]evaluate for the 22-th batch, evaluate loss: 0.6947525143623352:  52%|█████████▍        | 21/40 [00:02<00:02,  7.58it/s]Epoch: 7, train for the 2-th batch, train loss: 0.7636696696281433:   1%|▏              | 1/119 [00:00<00:12,  9.49it/s]Epoch: 7, train for the 2-th batch, train loss: 0.7636696696281433:   2%|▎              | 2/119 [00:00<00:13,  8.77it/s]Epoch: 6, train for the 13-th batch, train loss: 0.5715771317481995:   8%|█            | 12/146 [00:01<00:12, 10.70it/s]evaluate for the 22-th batch, evaluate loss: 0.6274561285972595:  80%|██████████████▍   | 20/25 [00:02<00:00,  8.59it/s]evaluate for the 22-th batch, evaluate loss: 0.6274561285972595:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.59it/s]Epoch: 3, train for the 213-th batch, train loss: 0.4938561022281647:  88%|█████████▋ | 212/241 [00:34<00:05,  5.41it/s]Epoch: 3, train for the 213-th batch, train loss: 0.4938561022281647:  88%|█████████▋ | 213/241 [00:34<00:04,  5.84it/s]evaluate for the 23-th batch, evaluate loss: 0.6466741561889648:  52%|█████████▍        | 21/40 [00:02<00:02,  7.58it/s]evaluate for the 23-th batch, evaluate loss: 0.6466741561889648:  57%|██████████▎       | 23/40 [00:02<00:02,  8.00it/s]Epoch: 7, train for the 3-th batch, train loss: 0.792662501335144:   2%|▎               | 2/119 [00:00<00:13,  8.77it/s]Epoch: 7, train for the 3-th batch, train loss: 0.792662501335144:   3%|▍               | 3/119 [00:00<00:13,  8.34it/s]evaluate for the 89-th batch, evaluate loss: 0.44059234857559204:  83%|█████████████▎  | 88/106 [00:20<00:03,  4.66it/s]evaluate for the 89-th batch, evaluate loss: 0.44059234857559204:  84%|█████████████▍  | 89/106 [00:20<00:04,  3.82it/s]evaluate for the 23-th batch, evaluate loss: 0.6430991291999817:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.59it/s]Epoch: 6, train for the 14-th batch, train loss: 0.5504302382469177:   8%|█            | 12/146 [00:01<00:12, 10.70it/s]Epoch: 6, train for the 14-th batch, train loss: 0.5504302382469177:  10%|█▏           | 14/146 [00:01<00:13,  9.76it/s]evaluate for the 24-th batch, evaluate loss: 0.6850584149360657:  57%|██████████▎       | 23/40 [00:02<00:02,  8.00it/s]evaluate for the 24-th batch, evaluate loss: 0.6850584149360657:  60%|██████████▊       | 24/40 [00:02<00:02,  7.78it/s]evaluate for the 24-th batch, evaluate loss: 0.6631952524185181:  88%|███████████████▊  | 22/25 [00:02<00:00,  9.59it/s]evaluate for the 24-th batch, evaluate loss: 0.6631952524185181:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.58it/s]Epoch: 3, train for the 214-th batch, train loss: 0.47787797451019287:  88%|████████▊ | 213/241 [00:34<00:04,  5.84it/s]Epoch: 3, train for the 214-th batch, train loss: 0.47787797451019287:  89%|████████▉ | 214/241 [00:34<00:04,  5.41it/s]Epoch: 7, train for the 4-th batch, train loss: 0.7207089066505432:   3%|▍              | 3/119 [00:00<00:13,  8.34it/s]evaluate for the 25-th batch, evaluate loss: 0.6337248682975769:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.58it/s]evaluate for the 25-th batch, evaluate loss: 0.6337248682975769: 100%|██████████████████| 25/25 [00:02<00:00,  9.74it/s]
Epoch: 7, train for the 4-th batch, train loss: 0.7207089066505432:   3%|▌              | 4/119 [00:00<00:17,  6.48it/s]Epoch: 6, train for the 15-th batch, train loss: 0.5431262850761414:  10%|█▏           | 14/146 [00:01<00:13,  9.76it/s]evaluate for the 90-th batch, evaluate loss: 0.31906208395957947:  84%|█████████████▍  | 89/106 [00:21<00:04,  3.82it/s]evaluate for the 90-th batch, evaluate loss: 0.31906208395957947:  85%|█████████████▌  | 90/106 [00:21<00:03,  4.05it/s]Epoch: 6, train for the 15-th batch, train loss: 0.5431262850761414:  10%|█▎           | 15/146 [00:01<00:15,  8.54it/s]evaluate for the 25-th batch, evaluate loss: 0.6543115377426147:  60%|██████████▊       | 24/40 [00:03<00:02,  7.78it/s]evaluate for the 25-th batch, evaluate loss: 0.6543115377426147:  62%|███████████▎      | 25/40 [00:03<00:02,  7.26it/s]Epoch: 7, train for the 5-th batch, train loss: 0.6386764049530029:   3%|▌              | 4/119 [00:00<00:17,  6.48it/s]Epoch: 7, train for the 5-th batch, train loss: 0.6386764049530029:   4%|▋              | 5/119 [00:00<00:16,  6.72it/s]Epoch: 6, train for the 16-th batch, train loss: 0.5494682788848877:  10%|█▎           | 15/146 [00:01<00:15,  8.54it/s]Epoch: 6, train for the 16-th batch, train loss: 0.5494682788848877:  11%|█▍           | 16/146 [00:01<00:16,  7.88it/s]evaluate for the 26-th batch, evaluate loss: 0.66298508644104:  62%|████████████▌       | 25/40 [00:03<00:02,  7.26it/s]Epoch: 3, train for the 215-th batch, train loss: 0.5023697018623352:  89%|█████████▊ | 214/241 [00:35<00:04,  5.41it/s]INFO:root:Epoch: 4, learning rate: 0.0001, train loss: 0.5642
INFO:root:train average_precision, 0.7745
INFO:root:train roc_auc, 0.7548
INFO:root:validate loss: 0.5088
INFO:root:validate average_precision, 0.8398
INFO:root:validate roc_auc, 0.8352
INFO:root:new node validate loss: 0.6984
INFO:root:new node validate first_1_average_precision, 0.5583
INFO:root:new node validate first_1_roc_auc, 0.5657
INFO:root:new node validate first_3_average_precision, 0.5870
INFO:root:new node validate first_3_roc_auc, 0.5885
INFO:root:new node validate first_10_average_precision, 0.6392
INFO:root:new node validate first_10_roc_auc, 0.6425
INFO:root:new node validate average_precision, 0.6580
INFO:root:new node validate roc_auc, 0.6599
INFO:root:save model ./saved_models/TGN/ia-retweet-pol/TGN_seed0_tgn-ia-retweet-pol-lincorrect-time-linear/TGN_seed0_tgn-ia-retweet-pol-lincorrect-time-linear.pkl
Epoch: 3, train for the 215-th batch, train loss: 0.5023697018623352:  89%|█████████▊ | 215/241 [00:35<00:05,  4.92it/s]Epoch: 7, train for the 6-th batch, train loss: 0.5222978591918945:   4%|▋              | 5/119 [00:00<00:16,  6.72it/s]Epoch: 7, train for the 6-th batch, train loss: 0.5222978591918945:   5%|▊              | 6/119 [00:00<00:16,  6.86it/s]Epoch: 6, train for the 17-th batch, train loss: 0.5514382719993591:  11%|█▍           | 16/146 [00:01<00:16,  7.88it/s]evaluate for the 91-th batch, evaluate loss: 0.31752461194992065:  85%|█████████████▌  | 90/106 [00:21<00:03,  4.05it/s]evaluate for the 91-th batch, evaluate loss: 0.31752461194992065:  86%|█████████████▋  | 91/106 [00:21<00:03,  3.85it/s]Epoch: 6, train for the 17-th batch, train loss: 0.5514382719993591:  12%|█▌           | 17/146 [00:01<00:16,  7.81it/s]evaluate for the 27-th batch, evaluate loss: 0.6482648849487305:  62%|███████████▎      | 25/40 [00:03<00:02,  7.26it/s]evaluate for the 27-th batch, evaluate loss: 0.6482648849487305:  68%|████████████▏     | 27/40 [00:03<00:01,  7.86it/s]Epoch: 3, train for the 216-th batch, train loss: 0.5712450742721558:  89%|█████████▊ | 215/241 [00:35<00:05,  4.92it/s]Epoch: 3, train for the 216-th batch, train loss: 0.5712450742721558:  90%|█████████▊ | 216/241 [00:35<00:04,  5.29it/s]evaluate for the 28-th batch, evaluate loss: 0.7048876881599426:  68%|████████████▏     | 27/40 [00:03<00:01,  7.86it/s]Epoch: 7, train for the 7-th batch, train loss: 0.5070084929466248:   5%|▊              | 6/119 [00:00<00:16,  6.86it/s]Epoch: 7, train for the 7-th batch, train loss: 0.5070084929466248:   6%|▉              | 7/119 [00:00<00:16,  6.89it/s]evaluate for the 92-th batch, evaluate loss: 0.34735873341560364:  86%|█████████████▋  | 91/106 [00:21<00:03,  3.85it/s]evaluate for the 92-th batch, evaluate loss: 0.34735873341560364:  87%|█████████████▉  | 92/106 [00:21<00:03,  4.35it/s]Epoch: 6, train for the 18-th batch, train loss: 0.5163141489028931:  12%|█▌           | 17/146 [00:01<00:16,  7.81it/s]Epoch: 6, train for the 18-th batch, train loss: 0.5163141489028931:  12%|█▌           | 18/146 [00:01<00:17,  7.30it/s]evaluate for the 29-th batch, evaluate loss: 0.7075521945953369:  68%|████████████▏     | 27/40 [00:03<00:01,  7.86it/s]evaluate for the 29-th batch, evaluate loss: 0.7075521945953369:  72%|█████████████     | 29/40 [00:03<00:01,  8.66it/s]Epoch: 3, train for the 217-th batch, train loss: 0.46854299306869507:  90%|████████▉ | 216/241 [00:35<00:04,  5.29it/s]Epoch: 3, train for the 217-th batch, train loss: 0.46854299306869507:  90%|█████████ | 217/241 [00:35<00:04,  5.68it/s]Epoch: 7, train for the 8-th batch, train loss: 0.4635055363178253:   6%|▉              | 7/119 [00:01<00:16,  6.89it/s]Epoch: 7, train for the 8-th batch, train loss: 0.4635055363178253:   7%|█              | 8/119 [00:01<00:15,  7.03it/s]evaluate for the 30-th batch, evaluate loss: 0.7318184971809387:  72%|█████████████     | 29/40 [00:03<00:01,  8.66it/s]Epoch: 6, train for the 19-th batch, train loss: 0.5165066123008728:  12%|█▌           | 18/146 [00:02<00:17,  7.30it/s]Epoch: 6, train for the 19-th batch, train loss: 0.5165066123008728:  13%|█▋           | 19/146 [00:02<00:17,  7.34it/s]evaluate for the 31-th batch, evaluate loss: 0.6420477628707886:  72%|█████████████     | 29/40 [00:03<00:01,  8.66it/s]evaluate for the 31-th batch, evaluate loss: 0.6420477628707886:  78%|█████████████▉    | 31/40 [00:03<00:00,  9.46it/s]Epoch: 3, train for the 218-th batch, train loss: 0.4678380489349365:  90%|█████████▉ | 217/241 [00:35<00:04,  5.68it/s]Epoch: 6, train for the 20-th batch, train loss: 0.5188212990760803:  13%|█▋           | 19/146 [00:02<00:17,  7.34it/s]Epoch: 3, train for the 218-th batch, train loss: 0.4678380489349365:  90%|█████████▉ | 218/241 [00:35<00:04,  5.74it/s]evaluate for the 93-th batch, evaluate loss: 0.3967669606208801:  87%|██████████████▊  | 92/106 [00:21<00:03,  4.35it/s]evaluate for the 93-th batch, evaluate loss: 0.3967669606208801:  88%|██████████████▉  | 93/106 [00:21<00:03,  3.97it/s]evaluate for the 32-th batch, evaluate loss: 0.6693161725997925:  78%|█████████████▉    | 31/40 [00:03<00:00,  9.46it/s]evaluate for the 32-th batch, evaluate loss: 0.6693161725997925:  80%|██████████████▍   | 32/40 [00:03<00:00,  8.72it/s]Epoch: 3, train for the 219-th batch, train loss: 0.48671361804008484:  90%|█████████ | 218/241 [00:35<00:04,  5.74it/s]Epoch: 7, train for the 9-th batch, train loss: 0.46696382761001587:   7%|▉             | 8/119 [00:01<00:15,  7.03it/s]Epoch: 7, train for the 9-th batch, train loss: 0.46696382761001587:   8%|█             | 9/119 [00:01<00:20,  5.41it/s]Epoch: 3, train for the 219-th batch, train loss: 0.48671361804008484:  91%|█████████ | 219/241 [00:35<00:03,  6.05it/s]Epoch: 6, train for the 21-th batch, train loss: 0.5092494487762451:  13%|█▋           | 19/146 [00:02<00:17,  7.34it/s]Epoch: 6, train for the 21-th batch, train loss: 0.5092494487762451:  14%|█▊           | 21/146 [00:02<00:16,  7.46it/s]Epoch: 3, train for the 220-th batch, train loss: 0.5114730596542358:  91%|█████████▉ | 219/241 [00:35<00:03,  6.05it/s]evaluate for the 94-th batch, evaluate loss: 0.4513692259788513:  88%|██████████████▉  | 93/106 [00:22<00:03,  3.97it/s]evaluate for the 94-th batch, evaluate loss: 0.4513692259788513:  89%|███████████████  | 94/106 [00:22<00:02,  4.09it/s]evaluate for the 33-th batch, evaluate loss: 0.6871893405914307:  80%|██████████████▍   | 32/40 [00:04<00:00,  8.72it/s]evaluate for the 33-th batch, evaluate loss: 0.6871893405914307:  82%|██████████████▊   | 33/40 [00:04<00:00,  7.92it/s]Epoch: 3, train for the 220-th batch, train loss: 0.5114730596542358:  91%|██████████ | 220/241 [00:35<00:03,  6.18it/s]Epoch: 7, train for the 10-th batch, train loss: 0.5052009224891663:   8%|█             | 9/119 [00:01<00:20,  5.41it/s]Epoch: 7, train for the 10-th batch, train loss: 0.5052009224891663:   8%|█            | 10/119 [00:01<00:19,  5.45it/s]Epoch: 6, train for the 22-th batch, train loss: 0.5225235819816589:  14%|█▊           | 21/146 [00:02<00:16,  7.46it/s]Epoch: 6, train for the 22-th batch, train loss: 0.5225235819816589:  15%|█▉           | 22/146 [00:02<00:17,  6.97it/s]Epoch: 3, train for the 221-th batch, train loss: 0.5072459578514099:  91%|██████████ | 220/241 [00:35<00:03,  6.18it/s]evaluate for the 34-th batch, evaluate loss: 0.6613990068435669:  82%|██████████████▊   | 33/40 [00:04<00:00,  7.92it/s]evaluate for the 34-th batch, evaluate loss: 0.6613990068435669:  85%|███████████████▎  | 34/40 [00:04<00:00,  7.81it/s]Epoch: 3, train for the 221-th batch, train loss: 0.5072459578514099:  92%|██████████ | 221/241 [00:36<00:03,  6.39it/s]evaluate for the 95-th batch, evaluate loss: 0.3615119457244873:  89%|███████████████  | 94/106 [00:22<00:02,  4.09it/s]evaluate for the 95-th batch, evaluate loss: 0.3615119457244873:  90%|███████████████▏ | 95/106 [00:22<00:02,  4.26it/s]Epoch: 7, train for the 11-th batch, train loss: 0.48182883858680725:   8%|█           | 10/119 [00:01<00:19,  5.45it/s]Epoch: 6, train for the 23-th batch, train loss: 0.5224565267562866:  15%|█▉           | 22/146 [00:02<00:17,  6.97it/s]Epoch: 6, train for the 23-th batch, train loss: 0.5224565267562866:  16%|██           | 23/146 [00:02<00:18,  6.61it/s]Epoch: 7, train for the 11-th batch, train loss: 0.48182883858680725:   9%|█           | 11/119 [00:01<00:20,  5.29it/s]evaluate for the 35-th batch, evaluate loss: 0.6980274319648743:  85%|███████████████▎  | 34/40 [00:04<00:00,  7.81it/s]evaluate for the 35-th batch, evaluate loss: 0.6980274319648743:  88%|███████████████▊  | 35/40 [00:04<00:00,  7.99it/s]Epoch: 3, train for the 222-th batch, train loss: 0.4819173812866211:  92%|██████████ | 221/241 [00:36<00:03,  6.39it/s]Epoch: 3, train for the 222-th batch, train loss: 0.4819173812866211:  92%|██████████▏| 222/241 [00:36<00:02,  6.47it/s]Epoch: 6, train for the 24-th batch, train loss: 0.5232884287834167:  16%|██           | 23/146 [00:02<00:18,  6.61it/s]Epoch: 6, train for the 24-th batch, train loss: 0.5232884287834167:  16%|██▏          | 24/146 [00:02<00:18,  6.61it/s]evaluate for the 36-th batch, evaluate loss: 0.7196257710456848:  88%|███████████████▊  | 35/40 [00:04<00:00,  7.99it/s]evaluate for the 36-th batch, evaluate loss: 0.7196257710456848:  90%|████████████████▏ | 36/40 [00:04<00:00,  7.97it/s]evaluate for the 96-th batch, evaluate loss: 0.3954904079437256:  90%|███████████████▏ | 95/106 [00:22<00:02,  4.26it/s]evaluate for the 96-th batch, evaluate loss: 0.3954904079437256:  91%|███████████████▍ | 96/106 [00:22<00:02,  4.63it/s]Epoch: 7, train for the 12-th batch, train loss: 0.5140684247016907:   9%|█▏           | 11/119 [00:01<00:20,  5.29it/s]Epoch: 7, train for the 12-th batch, train loss: 0.5140684247016907:  10%|█▎           | 12/119 [00:01<00:19,  5.49it/s]Epoch: 3, train for the 223-th batch, train loss: 0.46621173620224:  92%|███████████▉ | 222/241 [00:36<00:02,  6.47it/s]Epoch: 3, train for the 223-th batch, train loss: 0.46621173620224:  93%|████████████ | 223/241 [00:36<00:02,  6.35it/s]evaluate for the 37-th batch, evaluate loss: 0.7177239656448364:  90%|████████████████▏ | 36/40 [00:04<00:00,  7.97it/s]evaluate for the 37-th batch, evaluate loss: 0.7177239656448364:  92%|████████████████▋ | 37/40 [00:04<00:00,  8.44it/s]Epoch: 6, train for the 25-th batch, train loss: 0.5370141863822937:  16%|██▏          | 24/146 [00:02<00:18,  6.61it/s]Epoch: 6, train for the 25-th batch, train loss: 0.5370141863822937:  17%|██▏          | 25/146 [00:02<00:17,  6.90it/s]Epoch: 7, train for the 13-th batch, train loss: 0.4281628429889679:  10%|█▎           | 12/119 [00:02<00:19,  5.49it/s]Epoch: 7, train for the 13-th batch, train loss: 0.4281628429889679:  11%|█▍           | 13/119 [00:02<00:17,  5.97it/s]Epoch: 3, train for the 224-th batch, train loss: 0.4814596474170685:  93%|██████████▏| 223/241 [00:36<00:02,  6.35it/s]evaluate for the 38-th batch, evaluate loss: 0.7025198340415955:  92%|████████████████▋ | 37/40 [00:04<00:00,  8.44it/s]evaluate for the 38-th batch, evaluate loss: 0.7025198340415955:  95%|█████████████████ | 38/40 [00:04<00:00,  8.46it/s]Epoch: 3, train for the 224-th batch, train loss: 0.4814596474170685:  93%|██████████▏| 224/241 [00:36<00:02,  6.53it/s]Epoch: 6, train for the 26-th batch, train loss: 0.49501776695251465:  17%|██          | 25/146 [00:03<00:17,  6.90it/s]Epoch: 6, train for the 26-th batch, train loss: 0.49501776695251465:  18%|██▏         | 26/146 [00:03<00:17,  7.05it/s]Epoch: 7, train for the 14-th batch, train loss: 0.461250901222229:  11%|█▌            | 13/119 [00:02<00:17,  5.97it/s]Epoch: 7, train for the 14-th batch, train loss: 0.461250901222229:  12%|█▋            | 14/119 [00:02<00:16,  6.18it/s]evaluate for the 97-th batch, evaluate loss: 0.2974392771720886:  91%|███████████████▍ | 96/106 [00:22<00:02,  4.63it/s]evaluate for the 97-th batch, evaluate loss: 0.2974392771720886:  92%|███████████████▌ | 97/106 [00:22<00:02,  4.15it/s]Epoch: 3, train for the 225-th batch, train loss: 0.5337923169136047:  93%|██████████▏| 224/241 [00:36<00:02,  6.53it/s]evaluate for the 39-th batch, evaluate loss: 0.724915623664856:  95%|██████████████████ | 38/40 [00:04<00:00,  8.46it/s]evaluate for the 39-th batch, evaluate loss: 0.724915623664856:  98%|██████████████████▌| 39/40 [00:04<00:00,  7.72it/s]Epoch: 3, train for the 225-th batch, train loss: 0.5337923169136047:  93%|██████████▎| 225/241 [00:36<00:02,  6.48it/s]Epoch: 6, train for the 27-th batch, train loss: 0.5036051273345947:  18%|██▎          | 26/146 [00:03<00:17,  7.05it/s]Epoch: 6, train for the 27-th batch, train loss: 0.5036051273345947:  18%|██▍          | 27/146 [00:03<00:17,  6.99it/s]Epoch: 7, train for the 15-th batch, train loss: 0.46191954612731934:  12%|█▍          | 14/119 [00:02<00:16,  6.18it/s]Epoch: 7, train for the 15-th batch, train loss: 0.46191954612731934:  13%|█▌          | 15/119 [00:02<00:16,  6.46it/s]evaluate for the 98-th batch, evaluate loss: 0.3791941702365875:  92%|███████████████▌ | 97/106 [00:22<00:02,  4.15it/s]evaluate for the 98-th batch, evaluate loss: 0.3791941702365875:  92%|███████████████▋ | 98/106 [00:22<00:01,  4.56it/s]evaluate for the 40-th batch, evaluate loss: 0.721038818359375:  98%|██████████████████▌| 39/40 [00:04<00:00,  7.72it/s]evaluate for the 40-th batch, evaluate loss: 0.721038818359375: 100%|███████████████████| 40/40 [00:04<00:00,  7.57it/s]evaluate for the 40-th batch, evaluate loss: 0.721038818359375: 100%|███████████████████| 40/40 [00:04<00:00,  8.15it/s]
Epoch: 3, train for the 226-th batch, train loss: 0.5145677328109741:  93%|██████████▎| 225/241 [00:36<00:02,  6.48it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]Epoch: 7, train for the 16-th batch, train loss: 0.47048914432525635:  13%|█▌          | 15/119 [00:02<00:16,  6.46it/s]Epoch: 7, train for the 16-th batch, train loss: 0.47048914432525635:  13%|█▌          | 16/119 [00:02<00:14,  7.12it/s]Epoch: 3, train for the 226-th batch, train loss: 0.5145677328109741:  94%|██████████▎| 226/241 [00:36<00:02,  6.46it/s]Epoch: 5, train for the 1-th batch, train loss: 0.782814621925354:   0%|                        | 0/151 [00:00<?, ?it/s]Epoch: 6, train for the 28-th batch, train loss: 0.5042267441749573:  18%|██▍          | 27/146 [00:03<00:17,  6.99it/s]Epoch: 6, train for the 28-th batch, train loss: 0.5042267441749573:  19%|██▍          | 28/146 [00:03<00:21,  5.61it/s]Epoch: 7, train for the 17-th batch, train loss: 0.39536741375923157:  13%|█▌          | 16/119 [00:02<00:14,  7.12it/s]Epoch: 3, train for the 227-th batch, train loss: 0.5952947735786438:  94%|██████████▎| 226/241 [00:36<00:02,  6.46it/s]Epoch: 7, train for the 17-th batch, train loss: 0.39536741375923157:  14%|█▋          | 17/119 [00:02<00:14,  7.04it/s]Epoch: 3, train for the 227-th batch, train loss: 0.5952947735786438:  94%|██████████▎| 227/241 [00:36<00:02,  6.37it/s]Epoch: 5, train for the 2-th batch, train loss: 0.7883637547492981:   0%|                       | 0/151 [00:00<?, ?it/s]Epoch: 5, train for the 2-th batch, train loss: 0.7883637547492981:   1%|▏              | 2/151 [00:00<00:14, 10.41it/s]evaluate for the 99-th batch, evaluate loss: 0.342522531747818:  92%|████████████████▋ | 98/106 [00:23<00:01,  4.56it/s]evaluate for the 99-th batch, evaluate loss: 0.342522531747818:  93%|████████████████▊ | 99/106 [00:23<00:01,  4.15it/s]Epoch: 6, train for the 29-th batch, train loss: 0.46537452936172485:  19%|██▎         | 28/146 [00:03<00:21,  5.61it/s]Epoch: 6, train for the 29-th batch, train loss: 0.46537452936172485:  20%|██▍         | 29/146 [00:03<00:18,  6.31it/s]Epoch: 7, train for the 18-th batch, train loss: 0.4327929615974426:  14%|█▊           | 17/119 [00:02<00:14,  7.04it/s]Epoch: 7, train for the 18-th batch, train loss: 0.4327929615974426:  15%|█▉           | 18/119 [00:02<00:14,  6.90it/s]Epoch: 5, train for the 3-th batch, train loss: 0.7806968688964844:   1%|▏              | 2/151 [00:00<00:14, 10.41it/s]Epoch: 6, train for the 30-th batch, train loss: 0.4899006485939026:  20%|██▌          | 29/146 [00:03<00:18,  6.31it/s]Epoch: 6, train for the 30-th batch, train loss: 0.4899006485939026:  21%|██▋          | 30/146 [00:03<00:16,  6.88it/s]Epoch: 3, train for the 228-th batch, train loss: 0.49660539627075195:  94%|█████████▍| 227/241 [00:37<00:02,  6.37it/s]Epoch: 3, train for the 228-th batch, train loss: 0.49660539627075195:  95%|█████████▍| 228/241 [00:37<00:02,  5.68it/s]Epoch: 7, train for the 19-th batch, train loss: 0.41697409749031067:  15%|█▊          | 18/119 [00:02<00:14,  6.90it/s]Epoch: 7, train for the 19-th batch, train loss: 0.41697409749031067:  16%|█▉          | 19/119 [00:02<00:13,  7.38it/s]evaluate for the 100-th batch, evaluate loss: 0.3198472857475281:  93%|██████████████▉ | 99/106 [00:23<00:01,  4.15it/s]evaluate for the 100-th batch, evaluate loss: 0.3198472857475281:  94%|██████████████▏| 100/106 [00:23<00:01,  4.28it/s]Epoch: 5, train for the 4-th batch, train loss: 0.741104006767273:   1%|▏               | 2/151 [00:00<00:14, 10.41it/s]Epoch: 6, train for the 31-th batch, train loss: 0.5185727477073669:  21%|██▋          | 30/146 [00:03<00:16,  6.88it/s]Epoch: 6, train for the 31-th batch, train loss: 0.5185727477073669:  21%|██▊          | 31/146 [00:03<00:15,  7.25it/s]Epoch: 5, train for the 4-th batch, train loss: 0.741104006767273:   3%|▍               | 4/151 [00:00<00:18,  7.92it/s]INFO:root:Epoch: 2, learning rate: 0.0001, train loss: 0.5787
INFO:root:train average_precision, 0.7786
INFO:root:train roc_auc, 0.7498
INFO:root:validate loss: 0.5416
INFO:root:validate average_precision, 0.8149
INFO:root:validate roc_auc, 0.7953
INFO:root:new node validate loss: 0.6660
INFO:root:new node validate first_1_average_precision, 0.6112
INFO:root:new node validate first_1_roc_auc, 0.5372
INFO:root:new node validate first_3_average_precision, 0.6395
INFO:root:new node validate first_3_roc_auc, 0.5751
INFO:root:new node validate first_10_average_precision, 0.6673
INFO:root:new node validate first_10_roc_auc, 0.6181
INFO:root:new node validate average_precision, 0.6768
INFO:root:new node validate roc_auc, 0.6329
Epoch: 3, train for the 229-th batch, train loss: 0.4605688452720642:  95%|██████████▍| 228/241 [00:37<00:02,  5.68it/s]Epoch: 7, train for the 20-th batch, train loss: 0.4481853246688843:  16%|██           | 19/119 [00:02<00:13,  7.38it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/237 [00:00<?, ?it/s]Epoch: 7, train for the 20-th batch, train loss: 0.4481853246688843:  17%|██▏          | 20/119 [00:03<00:12,  7.68it/s]Epoch: 3, train for the 229-th batch, train loss: 0.4605688452720642:  95%|██████████▍| 229/241 [00:37<00:02,  5.83it/s]Epoch: 6, train for the 32-th batch, train loss: 0.4836346209049225:  21%|██▊          | 31/146 [00:03<00:15,  7.25it/s]Epoch: 5, train for the 5-th batch, train loss: 0.7566370964050293:   3%|▍              | 4/151 [00:00<00:18,  7.92it/s]Epoch: 6, train for the 32-th batch, train loss: 0.4836346209049225:  22%|██▊          | 32/146 [00:03<00:15,  7.57it/s]Epoch: 5, train for the 5-th batch, train loss: 0.7566370964050293:   3%|▍              | 5/151 [00:00<00:18,  7.91it/s]Epoch: 3, train for the 1-th batch, train loss: 0.8056336641311646:   0%|                       | 0/237 [00:00<?, ?it/s]Epoch: 7, train for the 21-th batch, train loss: 0.48177874088287354:  17%|██          | 20/119 [00:03<00:12,  7.68it/s]Epoch: 7, train for the 21-th batch, train loss: 0.48177874088287354:  18%|██          | 21/119 [00:03<00:12,  7.75it/s]Epoch: 3, train for the 230-th batch, train loss: 0.4975566864013672:  95%|██████████▍| 229/241 [00:37<00:02,  5.83it/s]evaluate for the 101-th batch, evaluate loss: 0.32624170184135437:  94%|█████████████▏| 100/106 [00:23<00:01,  4.28it/s]evaluate for the 101-th batch, evaluate loss: 0.32624170184135437:  95%|█████████████▎| 101/106 [00:23<00:01,  4.17it/s]Epoch: 6, train for the 33-th batch, train loss: 0.472979336977005:  22%|███           | 32/146 [00:04<00:15,  7.57it/s]Epoch: 6, train for the 33-th batch, train loss: 0.472979336977005:  23%|███▏          | 33/146 [00:04<00:14,  7.97it/s]Epoch: 3, train for the 230-th batch, train loss: 0.4975566864013672:  95%|██████████▍| 230/241 [00:37<00:01,  5.98it/s]Epoch: 5, train for the 6-th batch, train loss: 0.7545005679130554:   3%|▍              | 5/151 [00:00<00:18,  7.91it/s]Epoch: 5, train for the 6-th batch, train loss: 0.7545005679130554:   4%|▌              | 6/151 [00:00<00:18,  7.99it/s]Epoch: 3, train for the 2-th batch, train loss: 0.7907212376594543:   0%|                       | 0/237 [00:00<?, ?it/s]Epoch: 3, train for the 2-th batch, train loss: 0.7907212376594543:   1%|▏              | 2/237 [00:00<00:24,  9.52it/s]Epoch: 7, train for the 22-th batch, train loss: 0.42215120792388916:  18%|██          | 21/119 [00:03<00:12,  7.75it/s]Epoch: 7, train for the 22-th batch, train loss: 0.42215120792388916:  18%|██▏         | 22/119 [00:03<00:13,  7.27it/s]Epoch: 6, train for the 34-th batch, train loss: 0.512354850769043:  23%|███▏          | 33/146 [00:04<00:14,  7.97it/s]Epoch: 6, train for the 34-th batch, train loss: 0.512354850769043:  23%|███▎          | 34/146 [00:04<00:15,  7.35it/s]Epoch: 3, train for the 231-th batch, train loss: 0.5485268235206604:  95%|██████████▍| 230/241 [00:37<00:01,  5.98it/s]evaluate for the 102-th batch, evaluate loss: 0.3143051564693451:  95%|██████████████▎| 101/106 [00:23<00:01,  4.17it/s]evaluate for the 102-th batch, evaluate loss: 0.3143051564693451:  96%|██████████████▍| 102/106 [00:23<00:00,  4.42it/s]Epoch: 5, train for the 7-th batch, train loss: 0.703959047794342:   4%|▋               | 6/151 [00:00<00:18,  7.99it/s]Epoch: 3, train for the 231-th batch, train loss: 0.5485268235206604:  96%|██████████▌| 231/241 [00:37<00:01,  5.75it/s]Epoch: 5, train for the 7-th batch, train loss: 0.703959047794342:   5%|▋               | 7/151 [00:00<00:20,  6.94it/s]Epoch: 7, train for the 23-th batch, train loss: 0.488931268453598:  18%|██▌           | 22/119 [00:03<00:13,  7.27it/s]Epoch: 7, train for the 23-th batch, train loss: 0.488931268453598:  19%|██▋           | 23/119 [00:03<00:12,  7.46it/s]Epoch: 6, train for the 35-th batch, train loss: 0.48322004079818726:  23%|██▊         | 34/146 [00:04<00:15,  7.35it/s]Epoch: 6, train for the 35-th batch, train loss: 0.48322004079818726:  24%|██▉         | 35/146 [00:04<00:14,  7.50it/s]Epoch: 3, train for the 3-th batch, train loss: 0.7039884924888611:   1%|▏              | 2/237 [00:00<00:24,  9.52it/s]Epoch: 3, train for the 3-th batch, train loss: 0.7039884924888611:   1%|▏              | 3/237 [00:00<00:40,  5.71it/s]Epoch: 7, train for the 24-th batch, train loss: 0.434428334236145:  19%|██▋           | 23/119 [00:03<00:12,  7.46it/s]Epoch: 3, train for the 232-th batch, train loss: 0.5227094888687134:  96%|██████████▌| 231/241 [00:37<00:01,  5.75it/s]Epoch: 7, train for the 24-th batch, train loss: 0.434428334236145:  20%|██▊           | 24/119 [00:03<00:13,  7.10it/s]Epoch: 5, train for the 8-th batch, train loss: 0.7087281346321106:   5%|▋              | 7/151 [00:01<00:20,  6.94it/s]Epoch: 3, train for the 4-th batch, train loss: 0.6278751492500305:   1%|▏              | 3/237 [00:00<00:40,  5.71it/s]Epoch: 5, train for the 8-th batch, train loss: 0.7087281346321106:   5%|▊              | 8/151 [00:01<00:23,  6.22it/s]Epoch: 3, train for the 232-th batch, train loss: 0.5227094888687134:  96%|██████████▌| 232/241 [00:37<00:01,  5.32it/s]Epoch: 6, train for the 36-th batch, train loss: 0.4540397524833679:  24%|███          | 35/146 [00:04<00:14,  7.50it/s]Epoch: 6, train for the 36-th batch, train loss: 0.4540397524833679:  25%|███▏         | 36/146 [00:04<00:15,  7.15it/s]evaluate for the 103-th batch, evaluate loss: 0.5421101450920105:  96%|██████████████▍| 102/106 [00:24<00:00,  4.42it/s]evaluate for the 103-th batch, evaluate loss: 0.5421101450920105:  97%|██████████████▌| 103/106 [00:24<00:00,  3.96it/s]Epoch: 3, train for the 5-th batch, train loss: 0.6978170871734619:   1%|▏              | 3/237 [00:00<00:40,  5.71it/s]Epoch: 5, train for the 9-th batch, train loss: 0.6755562424659729:   5%|▊              | 8/151 [00:01<00:23,  6.22it/s]Epoch: 7, train for the 25-th batch, train loss: 0.47563353180885315:  20%|██▍         | 24/119 [00:03<00:13,  7.10it/s]Epoch: 3, train for the 5-th batch, train loss: 0.6978170871734619:   2%|▎              | 5/237 [00:00<00:34,  6.68it/s]Epoch: 5, train for the 9-th batch, train loss: 0.6755562424659729:   6%|▉              | 9/151 [00:01<00:23,  6.16it/s]Epoch: 7, train for the 25-th batch, train loss: 0.47563353180885315:  21%|██▌         | 25/119 [00:03<00:14,  6.56it/s]Epoch: 6, train for the 37-th batch, train loss: 0.49520957469940186:  25%|██▉         | 36/146 [00:04<00:15,  7.15it/s]Epoch: 3, train for the 233-th batch, train loss: 0.5043154358863831:  96%|██████████▌| 232/241 [00:38<00:01,  5.32it/s]Epoch: 6, train for the 37-th batch, train loss: 0.49520957469940186:  25%|███         | 37/146 [00:04<00:16,  6.63it/s]Epoch: 3, train for the 233-th batch, train loss: 0.5043154358863831:  97%|██████████▋| 233/241 [00:38<00:01,  4.98it/s]Epoch: 5, train for the 10-th batch, train loss: 0.6748616099357605:   6%|▊             | 9/151 [00:01<00:23,  6.16it/s]evaluate for the 104-th batch, evaluate loss: 0.3507731556892395:  97%|██████████████▌| 103/106 [00:24<00:00,  3.96it/s]evaluate for the 104-th batch, evaluate loss: 0.3507731556892395:  98%|██████████████▋| 104/106 [00:24<00:00,  4.13it/s]Epoch: 5, train for the 10-th batch, train loss: 0.6748616099357605:   7%|▊            | 10/151 [00:01<00:22,  6.40it/s]Epoch: 3, train for the 6-th batch, train loss: 0.6550291776657104:   2%|▎              | 5/237 [00:00<00:34,  6.68it/s]Epoch: 3, train for the 6-th batch, train loss: 0.6550291776657104:   3%|▍              | 6/237 [00:00<00:34,  6.66it/s]Epoch: 7, train for the 26-th batch, train loss: 0.3917838931083679:  21%|██▋          | 25/119 [00:03<00:14,  6.56it/s]Epoch: 7, train for the 26-th batch, train loss: 0.3917838931083679:  22%|██▊          | 26/119 [00:03<00:14,  6.43it/s]Epoch: 6, train for the 38-th batch, train loss: 0.48629164695739746:  25%|███         | 37/146 [00:04<00:16,  6.63it/s]Epoch: 6, train for the 38-th batch, train loss: 0.48629164695739746:  26%|███         | 38/146 [00:04<00:16,  6.61it/s]Epoch: 3, train for the 234-th batch, train loss: 0.6055691242218018:  97%|██████████▋| 233/241 [00:38<00:01,  4.98it/s]Epoch: 5, train for the 11-th batch, train loss: 0.6751798987388611:   7%|▊            | 10/151 [00:01<00:22,  6.40it/s]Epoch: 3, train for the 234-th batch, train loss: 0.6055691242218018:  97%|██████████▋| 234/241 [00:38<00:01,  5.20it/s]Epoch: 5, train for the 11-th batch, train loss: 0.6751798987388611:   7%|▉            | 11/151 [00:01<00:19,  7.05it/s]Epoch: 3, train for the 7-th batch, train loss: 0.6296782493591309:   3%|▍              | 6/237 [00:00<00:34,  6.66it/s]Epoch: 3, train for the 7-th batch, train loss: 0.6296782493591309:   3%|▍              | 7/237 [00:00<00:31,  7.23it/s]Epoch: 7, train for the 27-th batch, train loss: 0.4597871005535126:  22%|██▊          | 26/119 [00:04<00:14,  6.43it/s]Epoch: 6, train for the 39-th batch, train loss: 0.5028215646743774:  26%|███▍         | 38/146 [00:05<00:16,  6.61it/s]Epoch: 6, train for the 39-th batch, train loss: 0.5028215646743774:  27%|███▍         | 39/146 [00:05<00:16,  6.38it/s]Epoch: 7, train for the 27-th batch, train loss: 0.4597871005535126:  23%|██▉          | 27/119 [00:04<00:15,  5.92it/s]Epoch: 5, train for the 12-th batch, train loss: 0.6873148679733276:   7%|▉            | 11/151 [00:01<00:19,  7.05it/s]evaluate for the 105-th batch, evaluate loss: 0.3899689316749573:  98%|██████████████▋| 104/106 [00:24<00:00,  4.13it/s]evaluate for the 105-th batch, evaluate loss: 0.3899689316749573:  99%|██████████████▊| 105/106 [00:24<00:00,  4.15it/s]Epoch: 5, train for the 12-th batch, train loss: 0.6873148679733276:   8%|█            | 12/151 [00:01<00:19,  7.17it/s]Epoch: 3, train for the 8-th batch, train loss: 0.6159618496894836:   3%|▍              | 7/237 [00:01<00:31,  7.23it/s]Epoch: 3, train for the 8-th batch, train loss: 0.6159618496894836:   3%|▌              | 8/237 [00:01<00:32,  7.10it/s]Epoch: 3, train for the 235-th batch, train loss: 0.6109433174133301:  97%|██████████▋| 234/241 [00:38<00:01,  5.20it/s]Epoch: 3, train for the 235-th batch, train loss: 0.6109433174133301:  98%|██████████▋| 235/241 [00:38<00:01,  5.17it/s]Epoch: 6, train for the 40-th batch, train loss: 0.45150551199913025:  27%|███▏        | 39/146 [00:05<00:16,  6.38it/s]Epoch: 6, train for the 40-th batch, train loss: 0.45150551199913025:  27%|███▎        | 40/146 [00:05<00:15,  6.74it/s]Epoch: 5, train for the 13-th batch, train loss: 0.6799278855323792:   8%|█            | 12/151 [00:01<00:19,  7.17it/s]Epoch: 7, train for the 28-th batch, train loss: 0.4982658624649048:  23%|██▉          | 27/119 [00:04<00:15,  5.92it/s]Epoch: 5, train for the 13-th batch, train loss: 0.6799278855323792:   9%|█            | 13/151 [00:01<00:19,  7.18it/s]Epoch: 7, train for the 28-th batch, train loss: 0.4982658624649048:  24%|███          | 28/119 [00:04<00:15,  6.02it/s]Epoch: 3, train for the 9-th batch, train loss: 0.5840544104576111:   3%|▌              | 8/237 [00:01<00:32,  7.10it/s]evaluate for the 106-th batch, evaluate loss: 0.4311234652996063:  99%|██████████████▊| 105/106 [00:24<00:00,  4.15it/s]evaluate for the 106-th batch, evaluate loss: 0.4311234652996063: 100%|███████████████| 106/106 [00:24<00:00,  4.45it/s]evaluate for the 106-th batch, evaluate loss: 0.4311234652996063: 100%|███████████████| 106/106 [00:24<00:00,  4.28it/s]
Epoch: 3, train for the 9-th batch, train loss: 0.5840544104576111:   4%|▌              | 9/237 [00:01<00:34,  6.65it/s]Epoch: 3, train for the 236-th batch, train loss: 0.55631422996521:  98%|████████████▋| 235/241 [00:38<00:01,  5.17it/s]Epoch: 3, train for the 236-th batch, train loss: 0.55631422996521:  98%|████████████▋| 236/241 [00:38<00:00,  5.28it/s]Epoch: 5, train for the 14-th batch, train loss: 0.6907352805137634:   9%|█            | 13/151 [00:01<00:19,  7.18it/s]Epoch: 5, train for the 14-th batch, train loss: 0.6907352805137634:   9%|█▏           | 14/151 [00:01<00:18,  7.61it/s]Epoch: 6, train for the 41-th batch, train loss: 0.47473055124282837:  27%|███▎        | 40/146 [00:05<00:15,  6.74it/s]Epoch: 6, train for the 41-th batch, train loss: 0.47473055124282837:  28%|███▎        | 41/146 [00:05<00:15,  6.61it/s]Epoch: 7, train for the 29-th batch, train loss: 0.44481760263442993:  24%|██▊         | 28/119 [00:04<00:15,  6.02it/s]Epoch: 3, train for the 10-th batch, train loss: 0.642811119556427:   4%|▌              | 9/237 [00:01<00:34,  6.65it/s]Epoch: 7, train for the 29-th batch, train loss: 0.44481760263442993:  24%|██▉         | 29/119 [00:04<00:14,  6.06it/s]Epoch: 3, train for the 10-th batch, train loss: 0.642811119556427:   4%|▌             | 10/237 [00:01<00:31,  7.16it/s]Epoch: 3, train for the 237-th batch, train loss: 0.5157070755958557:  98%|██████████▊| 236/241 [00:38<00:00,  5.28it/s]Epoch: 6, train for the 42-th batch, train loss: 0.5175159573554993:  28%|███▋         | 41/146 [00:05<00:15,  6.61it/s]Epoch: 5, train for the 15-th batch, train loss: 0.6829484701156616:   9%|█▏           | 14/151 [00:02<00:18,  7.61it/s]Epoch: 3, train for the 11-th batch, train loss: 0.635425865650177:   4%|▌             | 10/237 [00:01<00:31,  7.16it/s]Epoch: 6, train for the 42-th batch, train loss: 0.5175159573554993:  29%|███▋         | 42/146 [00:05<00:15,  6.69it/s]Epoch: 3, train for the 11-th batch, train loss: 0.635425865650177:   5%|▋             | 11/237 [00:01<00:29,  7.60it/s]Epoch: 3, train for the 237-th batch, train loss: 0.5157070755958557:  98%|██████████▊| 237/241 [00:38<00:00,  5.35it/s]Epoch: 5, train for the 15-th batch, train loss: 0.6829484701156616:  10%|█▎           | 15/151 [00:02<00:19,  6.98it/s]Epoch: 7, train for the 30-th batch, train loss: 0.4206262230873108:  24%|███▏         | 29/119 [00:04<00:14,  6.06it/s]Epoch: 7, train for the 30-th batch, train loss: 0.4206262230873108:  25%|███▎         | 30/119 [00:04<00:14,  6.36it/s]Epoch: 3, train for the 12-th batch, train loss: 0.6495822072029114:   5%|▌            | 11/237 [00:01<00:29,  7.60it/s]Epoch: 6, train for the 43-th batch, train loss: 0.47823038697242737:  29%|███▍        | 42/146 [00:05<00:15,  6.69it/s]Epoch: 3, train for the 12-th batch, train loss: 0.6495822072029114:   5%|▋            | 12/237 [00:01<00:28,  7.81it/s]Epoch: 6, train for the 43-th batch, train loss: 0.47823038697242737:  29%|███▌        | 43/146 [00:05<00:14,  6.91it/s]Epoch: 7, train for the 31-th batch, train loss: 0.418515682220459:  25%|███▌          | 30/119 [00:04<00:14,  6.36it/s]Epoch: 7, train for the 31-th batch, train loss: 0.418515682220459:  26%|███▋          | 31/119 [00:04<00:13,  6.52it/s]Epoch: 5, train for the 16-th batch, train loss: 0.680785596370697:  10%|█▍            | 15/151 [00:02<00:19,  6.98it/s]Epoch: 3, train for the 238-th batch, train loss: 0.5426162481307983:  98%|██████████▊| 237/241 [00:39<00:00,  5.35it/s]Epoch: 5, train for the 16-th batch, train loss: 0.680785596370697:  11%|█▍            | 16/151 [00:02<00:21,  6.24it/s]Epoch: 3, train for the 13-th batch, train loss: 0.6549378037452698:   5%|▋            | 12/237 [00:01<00:28,  7.81it/s]Epoch: 3, train for the 238-th batch, train loss: 0.5426162481307983:  99%|██████████▊| 238/241 [00:39<00:00,  5.05it/s]Epoch: 3, train for the 13-th batch, train loss: 0.6549378037452698:   5%|▋            | 13/237 [00:01<00:27,  8.20it/s]Epoch: 6, train for the 44-th batch, train loss: 0.4506545662879944:  29%|███▊         | 43/146 [00:05<00:14,  6.91it/s]Epoch: 6, train for the 44-th batch, train loss: 0.4506545662879944:  30%|███▉         | 44/146 [00:05<00:14,  6.97it/s]Epoch: 7, train for the 32-th batch, train loss: 0.41814911365509033:  26%|███▏        | 31/119 [00:04<00:13,  6.52it/s]Epoch: 7, train for the 32-th batch, train loss: 0.41814911365509033:  27%|███▏        | 32/119 [00:04<00:12,  6.76it/s]Epoch: 5, train for the 17-th batch, train loss: 0.6896381974220276:  11%|█▍           | 16/151 [00:02<00:21,  6.24it/s]Epoch: 3, train for the 14-th batch, train loss: 0.66525799036026:   5%|▊              | 13/237 [00:01<00:27,  8.20it/s]Epoch: 5, train for the 17-th batch, train loss: 0.6896381974220276:  11%|█▍           | 17/151 [00:02<00:20,  6.63it/s]Epoch: 3, train for the 14-th batch, train loss: 0.66525799036026:   6%|▉              | 14/237 [00:01<00:26,  8.56it/s]Epoch: 3, train for the 239-th batch, train loss: 0.5606723427772522:  99%|██████████▊| 238/241 [00:39<00:00,  5.05it/s]Epoch: 6, train for the 45-th batch, train loss: 0.47588804364204407:  30%|███▌        | 44/146 [00:05<00:14,  6.97it/s]Epoch: 3, train for the 239-th batch, train loss: 0.5606723427772522:  99%|██████████▉| 239/241 [00:39<00:00,  5.11it/s]Epoch: 6, train for the 45-th batch, train loss: 0.47588804364204407:  31%|███▋        | 45/146 [00:05<00:14,  6.78it/s]Epoch: 7, train for the 33-th batch, train loss: 0.47989073395729065:  27%|███▏        | 32/119 [00:04<00:12,  6.76it/s]Epoch: 7, train for the 33-th batch, train loss: 0.47989073395729065:  28%|███▎        | 33/119 [00:04<00:12,  6.83it/s]Epoch: 5, train for the 18-th batch, train loss: 0.6817301511764526:  11%|█▍           | 17/151 [00:02<00:20,  6.63it/s]Epoch: 5, train for the 18-th batch, train loss: 0.6817301511764526:  12%|█▌           | 18/151 [00:02<00:19,  6.98it/s]Epoch: 3, train for the 15-th batch, train loss: 0.663398265838623:   6%|▊             | 14/237 [00:02<00:26,  8.56it/s]Epoch: 3, train for the 15-th batch, train loss: 0.663398265838623:   6%|▉             | 15/237 [00:02<00:28,  7.82it/s]Epoch: 3, train for the 240-th batch, train loss: 0.4465305209159851:  99%|██████████▉| 239/241 [00:39<00:00,  5.11it/s]Epoch: 6, train for the 46-th batch, train loss: 0.4578676223754883:  31%|████         | 45/146 [00:06<00:14,  6.78it/s]Epoch: 6, train for the 46-th batch, train loss: 0.4578676223754883:  32%|████         | 46/146 [00:06<00:14,  6.85it/s]Epoch: 3, train for the 240-th batch, train loss: 0.4465305209159851: 100%|██████████▉| 240/241 [00:39<00:00,  5.38it/s]Epoch: 5, train for the 19-th batch, train loss: 0.6808735728263855:  12%|█▌           | 18/151 [00:02<00:19,  6.98it/s]Epoch: 7, train for the 34-th batch, train loss: 0.40247929096221924:  28%|███▎        | 33/119 [00:05<00:12,  6.83it/s]Epoch: 5, train for the 19-th batch, train loss: 0.6808735728263855:  13%|█▋           | 19/151 [00:02<00:18,  7.12it/s]Epoch: 7, train for the 34-th batch, train loss: 0.40247929096221924:  29%|███▍        | 34/119 [00:05<00:12,  6.75it/s]Epoch: 3, train for the 16-th batch, train loss: 0.6848523020744324:   6%|▊            | 15/237 [00:02<00:28,  7.82it/s]Epoch: 3, train for the 16-th batch, train loss: 0.6848523020744324:   7%|▉            | 16/237 [00:02<00:30,  7.32it/s]Epoch: 6, train for the 47-th batch, train loss: 0.5304923057556152:  32%|████         | 46/146 [00:06<00:14,  6.85it/s]Epoch: 6, train for the 47-th batch, train loss: 0.5304923057556152:  32%|████▏        | 47/146 [00:06<00:13,  7.13it/s]Epoch: 7, train for the 35-th batch, train loss: 0.43069708347320557:  29%|███▍        | 34/119 [00:05<00:12,  6.75it/s]Epoch: 7, train for the 35-th batch, train loss: 0.43069708347320557:  29%|███▌        | 35/119 [00:05<00:12,  6.86it/s]Epoch: 3, train for the 17-th batch, train loss: 0.6252706050872803:   7%|▉            | 16/237 [00:02<00:30,  7.32it/s]Epoch: 5, train for the 20-th batch, train loss: 0.6742943525314331:  13%|█▋           | 19/151 [00:02<00:18,  7.12it/s]Epoch: 3, train for the 17-th batch, train loss: 0.6252706050872803:   7%|▉            | 17/237 [00:02<00:28,  7.71it/s]Epoch: 5, train for the 20-th batch, train loss: 0.6742943525314331:  13%|█▋           | 20/151 [00:02<00:19,  6.72it/s]Epoch: 3, train for the 241-th batch, train loss: 0.5333792567253113: 100%|██████████▉| 240/241 [00:39<00:00,  5.38it/s]Epoch: 3, train for the 241-th batch, train loss: 0.5333792567253113: 100%|███████████| 241/241 [00:39<00:00,  5.17it/s]Epoch: 3, train for the 241-th batch, train loss: 0.5333792567253113: 100%|███████████| 241/241 [00:39<00:00,  6.08it/s]
Epoch: 6, train for the 48-th batch, train loss: 0.46230942010879517:  32%|███▊        | 47/146 [00:06<00:13,  7.13it/s]Epoch: 6, train for the 48-th batch, train loss: 0.46230942010879517:  33%|███▉        | 48/146 [00:06<00:12,  7.58it/s]Epoch: 7, train for the 36-th batch, train loss: 0.42024075984954834:  29%|███▌        | 35/119 [00:05<00:12,  6.86it/s]Epoch: 7, train for the 36-th batch, train loss: 0.42024075984954834:  30%|███▋        | 36/119 [00:05<00:11,  7.48it/s]Epoch: 3, train for the 18-th batch, train loss: 0.6094731688499451:   7%|▉            | 17/237 [00:02<00:28,  7.71it/s]Epoch: 3, train for the 18-th batch, train loss: 0.6094731688499451:   8%|▉            | 18/237 [00:02<00:27,  7.95it/s]Epoch: 6, train for the 49-th batch, train loss: 0.48771703243255615:  33%|███▉        | 48/146 [00:06<00:12,  7.58it/s]Epoch: 6, train for the 49-th batch, train loss: 0.48771703243255615:  34%|████        | 49/146 [00:06<00:13,  7.34it/s]Epoch: 7, train for the 37-th batch, train loss: 0.43703001737594604:  30%|███▋        | 36/119 [00:05<00:11,  7.48it/s]Epoch: 7, train for the 37-th batch, train loss: 0.43703001737594604:  31%|███▋        | 37/119 [00:05<00:11,  7.33it/s]Epoch: 6, train for the 50-th batch, train loss: 0.4999459385871887:  34%|████▎        | 49/146 [00:06<00:13,  7.34it/s]Epoch: 6, train for the 50-th batch, train loss: 0.4999459385871887:  34%|████▍        | 50/146 [00:06<00:12,  7.52it/s]Epoch: 3, train for the 19-th batch, train loss: 0.5993799567222595:   8%|▉            | 18/237 [00:02<00:27,  7.95it/s]Epoch: 5, train for the 21-th batch, train loss: 0.6779006719589233:  13%|█▋           | 20/151 [00:03<00:19,  6.72it/s]Epoch: 3, train for the 19-th batch, train loss: 0.5993799567222595:   8%|█            | 19/237 [00:02<00:33,  6.45it/s]Epoch: 5, train for the 21-th batch, train loss: 0.6779006719589233:  14%|█▊           | 21/151 [00:03<00:26,  4.83it/s]  0%|                                                                                            | 0/72 [00:00<?, ?it/s]Epoch: 7, train for the 38-th batch, train loss: 0.4118134081363678:  31%|████         | 37/119 [00:05<00:11,  7.33it/s]Epoch: 7, train for the 38-th batch, train loss: 0.4118134081363678:  32%|████▏        | 38/119 [00:05<00:11,  7.35it/s]Epoch: 6, train for the 51-th batch, train loss: 0.4470757246017456:  34%|████▍        | 50/146 [00:06<00:12,  7.52it/s]evaluate for the 1-th batch, evaluate loss: 0.5222248435020447:   0%|                            | 0/72 [00:00<?, ?it/s]Epoch: 6, train for the 51-th batch, train loss: 0.4470757246017456:  35%|████▌        | 51/146 [00:06<00:12,  7.76it/s]Epoch: 3, train for the 20-th batch, train loss: 0.6068581938743591:   8%|█            | 19/237 [00:02<00:33,  6.45it/s]Epoch: 5, train for the 22-th batch, train loss: 0.6624029874801636:  14%|█▊           | 21/151 [00:03<00:26,  4.83it/s]Epoch: 3, train for the 20-th batch, train loss: 0.6068581938743591:   8%|█            | 20/237 [00:02<00:32,  6.61it/s]Epoch: 5, train for the 22-th batch, train loss: 0.6624029874801636:  15%|█▉           | 22/151 [00:03<00:24,  5.32it/s]evaluate for the 2-th batch, evaluate loss: 0.5221140384674072:   0%|                            | 0/72 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5221140384674072:   3%|▌                   | 2/72 [00:00<00:05, 13.67it/s]Epoch: 7, train for the 39-th batch, train loss: 0.441623717546463:  32%|████▍         | 38/119 [00:05<00:11,  7.35it/s]Epoch: 7, train for the 39-th batch, train loss: 0.441623717546463:  33%|████▌         | 39/119 [00:05<00:11,  7.22it/s]Epoch: 6, train for the 52-th batch, train loss: 0.48900991678237915:  35%|████▏       | 51/146 [00:06<00:12,  7.76it/s]Epoch: 6, train for the 52-th batch, train loss: 0.48900991678237915:  36%|████▎       | 52/146 [00:06<00:11,  8.03it/s]evaluate for the 3-th batch, evaluate loss: 0.49483388662338257:   3%|▌                  | 2/72 [00:00<00:05, 13.67it/s]Epoch: 3, train for the 21-th batch, train loss: 0.6131521463394165:   8%|█            | 20/237 [00:02<00:32,  6.61it/s]Epoch: 3, train for the 21-th batch, train loss: 0.6131521463394165:   9%|█▏           | 21/237 [00:02<00:31,  6.92it/s]Epoch: 5, train for the 23-th batch, train loss: 0.6708616614341736:  15%|█▉           | 22/151 [00:03<00:24,  5.32it/s]evaluate for the 4-th batch, evaluate loss: 0.544786274433136:   3%|▌                    | 2/72 [00:00<00:05, 13.67it/s]evaluate for the 4-th batch, evaluate loss: 0.544786274433136:   6%|█▏                   | 4/72 [00:00<00:05, 13.51it/s]Epoch: 5, train for the 23-th batch, train loss: 0.6708616614341736:  15%|█▉           | 23/151 [00:03<00:23,  5.54it/s]Epoch: 7, train for the 40-th batch, train loss: 0.40432342886924744:  33%|███▉        | 39/119 [00:05<00:11,  7.22it/s]Epoch: 7, train for the 40-th batch, train loss: 0.40432342886924744:  34%|████        | 40/119 [00:05<00:11,  6.95it/s]Epoch: 6, train for the 53-th batch, train loss: 0.5143547654151917:  36%|████▋        | 52/146 [00:06<00:11,  8.03it/s]Epoch: 6, train for the 53-th batch, train loss: 0.5143547654151917:  36%|████▋        | 53/146 [00:06<00:12,  7.61it/s]evaluate for the 5-th batch, evaluate loss: 0.49749326705932617:   6%|█                  | 4/72 [00:00<00:05, 13.51it/s]Epoch: 3, train for the 22-th batch, train loss: 0.5613824129104614:   9%|█▏           | 21/237 [00:03<00:31,  6.92it/s]Epoch: 3, train for the 22-th batch, train loss: 0.5613824129104614:   9%|█▏           | 22/237 [00:03<00:32,  6.70it/s]Epoch: 5, train for the 24-th batch, train loss: 0.6722173690795898:  15%|█▉           | 23/151 [00:03<00:23,  5.54it/s]evaluate for the 6-th batch, evaluate loss: 0.5283506512641907:   6%|█                   | 4/72 [00:00<00:05, 13.51it/s]evaluate for the 6-th batch, evaluate loss: 0.5283506512641907:   8%|█▋                  | 6/72 [00:00<00:04, 13.93it/s]Epoch: 5, train for the 24-th batch, train loss: 0.6722173690795898:  16%|██           | 24/151 [00:03<00:21,  5.98it/s]Epoch: 7, train for the 41-th batch, train loss: 0.4147205650806427:  34%|████▎        | 40/119 [00:06<00:11,  6.95it/s]Epoch: 7, train for the 41-th batch, train loss: 0.4147205650806427:  34%|████▍        | 41/119 [00:06<00:11,  6.85it/s]Epoch: 6, train for the 54-th batch, train loss: 0.4858090877532959:  36%|████▋        | 53/146 [00:07<00:12,  7.61it/s]Epoch: 6, train for the 54-th batch, train loss: 0.4858090877532959:  37%|████▊        | 54/146 [00:07<00:12,  7.20it/s]evaluate for the 7-th batch, evaluate loss: 0.5519528985023499:   8%|█▋                  | 6/72 [00:00<00:04, 13.93it/s]Epoch: 3, train for the 23-th batch, train loss: 0.5739490985870361:   9%|█▏           | 22/237 [00:03<00:32,  6.70it/s]Epoch: 3, train for the 23-th batch, train loss: 0.5739490985870361:  10%|█▎           | 23/237 [00:03<00:31,  6.84it/s]Epoch: 5, train for the 25-th batch, train loss: 0.6902111768722534:  16%|██           | 24/151 [00:03<00:21,  5.98it/s]evaluate for the 8-th batch, evaluate loss: 0.5506237149238586:   8%|█▋                  | 6/72 [00:00<00:04, 13.93it/s]evaluate for the 8-th batch, evaluate loss: 0.5506237149238586:  11%|██▏                 | 8/72 [00:00<00:04, 13.09it/s]Epoch: 5, train for the 25-th batch, train loss: 0.6902111768722534:  17%|██▏          | 25/151 [00:03<00:21,  5.95it/s]Epoch: 7, train for the 42-th batch, train loss: 0.44273027777671814:  34%|████▏       | 41/119 [00:06<00:11,  6.85it/s]Epoch: 7, train for the 42-th batch, train loss: 0.44273027777671814:  35%|████▏       | 42/119 [00:06<00:11,  6.46it/s]Epoch: 6, train for the 55-th batch, train loss: 0.5761593580245972:  37%|████▊        | 54/146 [00:07<00:12,  7.20it/s]Epoch: 6, train for the 55-th batch, train loss: 0.5761593580245972:  38%|████▉        | 55/146 [00:07<00:12,  7.13it/s]evaluate for the 9-th batch, evaluate loss: 0.5111082196235657:  11%|██▏                 | 8/72 [00:00<00:04, 13.09it/s]Epoch: 5, train for the 26-th batch, train loss: 0.6713464260101318:  17%|██▏          | 25/151 [00:03<00:21,  5.95it/s]evaluate for the 10-th batch, evaluate loss: 0.5007575750350952:  11%|██                 | 8/72 [00:00<00:04, 13.09it/s]Epoch: 5, train for the 26-th batch, train loss: 0.6713464260101318:  17%|██▏          | 26/151 [00:03<00:19,  6.31it/s]evaluate for the 10-th batch, evaluate loss: 0.5007575750350952:  14%|██▌               | 10/72 [00:00<00:04, 13.38it/s]Epoch: 3, train for the 24-th batch, train loss: 0.6003039479255676:  10%|█▎           | 23/237 [00:03<00:31,  6.84it/s]Epoch: 3, train for the 24-th batch, train loss: 0.6003039479255676:  10%|█▎           | 24/237 [00:03<00:35,  5.95it/s]Epoch: 6, train for the 56-th batch, train loss: 0.5067193508148193:  38%|████▉        | 55/146 [00:07<00:12,  7.13it/s]Epoch: 6, train for the 56-th batch, train loss: 0.5067193508148193:  38%|████▉        | 56/146 [00:07<00:12,  7.16it/s]Epoch: 7, train for the 43-th batch, train loss: 0.44978925585746765:  35%|████▏       | 42/119 [00:06<00:11,  6.46it/s]evaluate for the 11-th batch, evaluate loss: 0.5068185329437256:  14%|██▌               | 10/72 [00:00<00:04, 13.38it/s]Epoch: 7, train for the 43-th batch, train loss: 0.44978925585746765:  36%|████▎       | 43/119 [00:06<00:12,  6.21it/s]evaluate for the 12-th batch, evaluate loss: 0.5089482069015503:  14%|██▌               | 10/72 [00:00<00:04, 13.38it/s]evaluate for the 12-th batch, evaluate loss: 0.5089482069015503:  17%|███               | 12/72 [00:00<00:04, 13.64it/s]Epoch: 5, train for the 27-th batch, train loss: 0.6682673692703247:  17%|██▏          | 26/151 [00:04<00:19,  6.31it/s]Epoch: 5, train for the 27-th batch, train loss: 0.6682673692703247:  18%|██▎          | 27/151 [00:04<00:20,  6.01it/s]evaluate for the 13-th batch, evaluate loss: 0.4384865462779999:  17%|███               | 12/72 [00:00<00:04, 13.64it/s]Epoch: 6, train for the 57-th batch, train loss: 0.5773921012878418:  38%|████▉        | 56/146 [00:07<00:12,  7.16it/s]Epoch: 6, train for the 57-th batch, train loss: 0.5773921012878418:  39%|█████        | 57/146 [00:07<00:12,  7.02it/s]Epoch: 7, train for the 44-th batch, train loss: 0.4015064239501953:  36%|████▋        | 43/119 [00:06<00:12,  6.21it/s]  0%|                                                                                            | 0/78 [00:00<?, ?it/s]Epoch: 7, train for the 44-th batch, train loss: 0.4015064239501953:  37%|████▊        | 44/119 [00:06<00:11,  6.38it/s]Epoch: 3, train for the 25-th batch, train loss: 0.5778199434280396:  10%|█▎           | 24/237 [00:03<00:35,  5.95it/s]Epoch: 3, train for the 25-th batch, train loss: 0.5778199434280396:  11%|█▎           | 25/237 [00:03<00:38,  5.53it/s]evaluate for the 14-th batch, evaluate loss: 0.4155603051185608:  17%|███               | 12/72 [00:01<00:04, 13.64it/s]evaluate for the 14-th batch, evaluate loss: 0.4155603051185608:  19%|███▌              | 14/72 [00:01<00:04, 14.33it/s]Epoch: 6, train for the 58-th batch, train loss: 0.48658618330955505:  39%|████▋       | 57/146 [00:07<00:12,  7.02it/s]Epoch: 6, train for the 58-th batch, train loss: 0.48658618330955505:  40%|████▊       | 58/146 [00:07<00:11,  7.38it/s]Epoch: 5, train for the 28-th batch, train loss: 0.6784679889678955:  18%|██▎          | 27/151 [00:04<00:20,  6.01it/s]Epoch: 7, train for the 45-th batch, train loss: 0.4061727821826935:  37%|████▊        | 44/119 [00:06<00:11,  6.38it/s]evaluate for the 15-th batch, evaluate loss: 0.4614052474498749:  19%|███▌              | 14/72 [00:01<00:04, 14.33it/s]Epoch: 7, train for the 45-th batch, train loss: 0.4061727821826935:  38%|████▉        | 45/119 [00:06<00:11,  6.59it/s]Epoch: 5, train for the 28-th batch, train loss: 0.6784679889678955:  19%|██▍          | 28/151 [00:04<00:20,  5.91it/s]evaluate for the 1-th batch, evaluate loss: 0.4675723910331726:   0%|                            | 0/78 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.4675723910331726:   1%|▎                   | 1/78 [00:00<00:13,  5.57it/s]evaluate for the 16-th batch, evaluate loss: 0.5061336159706116:  19%|███▌              | 14/72 [00:01<00:04, 14.33it/s]evaluate for the 16-th batch, evaluate loss: 0.5061336159706116:  22%|████              | 16/72 [00:01<00:04, 13.37it/s]Epoch: 6, train for the 59-th batch, train loss: 0.4472190737724304:  40%|█████▏       | 58/146 [00:07<00:11,  7.38it/s]Epoch: 6, train for the 59-th batch, train loss: 0.4472190737724304:  40%|█████▎       | 59/146 [00:07<00:11,  7.36it/s]Epoch: 3, train for the 26-th batch, train loss: 0.5581089854240417:  11%|█▎           | 25/237 [00:03<00:38,  5.53it/s]Epoch: 3, train for the 26-th batch, train loss: 0.5581089854240417:  11%|█▍           | 26/237 [00:03<00:42,  4.96it/s]Epoch: 7, train for the 46-th batch, train loss: 0.4606763422489166:  38%|████▉        | 45/119 [00:06<00:11,  6.59it/s]Epoch: 5, train for the 29-th batch, train loss: 0.6768079996109009:  19%|██▍          | 28/151 [00:04<00:20,  5.91it/s]Epoch: 7, train for the 46-th batch, train loss: 0.4606763422489166:  39%|█████        | 46/119 [00:06<00:11,  6.54it/s]Epoch: 5, train for the 29-th batch, train loss: 0.6768079996109009:  19%|██▍          | 29/151 [00:04<00:20,  5.91it/s]evaluate for the 17-th batch, evaluate loss: 0.44900158047676086:  22%|███▊             | 16/72 [00:01<00:04, 13.37it/s]evaluate for the 2-th batch, evaluate loss: 0.6247542500495911:   1%|▎                   | 1/78 [00:00<00:13,  5.57it/s]evaluate for the 2-th batch, evaluate loss: 0.6247542500495911:   3%|▌                   | 2/78 [00:00<00:12,  6.07it/s]Epoch: 6, train for the 60-th batch, train loss: 0.5052087903022766:  40%|█████▎       | 59/146 [00:07<00:11,  7.36it/s]Epoch: 6, train for the 60-th batch, train loss: 0.5052087903022766:  41%|█████▎       | 60/146 [00:07<00:11,  7.21it/s]evaluate for the 18-th batch, evaluate loss: 0.4162995517253876:  22%|████              | 16/72 [00:01<00:04, 13.37it/s]evaluate for the 18-th batch, evaluate loss: 0.4162995517253876:  25%|████▌             | 18/72 [00:01<00:04, 12.58it/s]Epoch: 3, train for the 27-th batch, train loss: 0.583404004573822:  11%|█▌            | 26/237 [00:04<00:42,  4.96it/s]Epoch: 5, train for the 30-th batch, train loss: 0.6743273735046387:  19%|██▍          | 29/151 [00:04<00:20,  5.91it/s]Epoch: 7, train for the 47-th batch, train loss: 0.4699995219707489:  39%|█████        | 46/119 [00:07<00:11,  6.54it/s]Epoch: 3, train for the 27-th batch, train loss: 0.583404004573822:  11%|█▌            | 27/237 [00:04<00:40,  5.21it/s]Epoch: 5, train for the 30-th batch, train loss: 0.6743273735046387:  20%|██▌          | 30/151 [00:04<00:19,  6.25it/s]Epoch: 7, train for the 47-th batch, train loss: 0.4699995219707489:  39%|█████▏       | 47/119 [00:07<00:11,  6.43it/s]evaluate for the 19-th batch, evaluate loss: 0.4746667444705963:  25%|████▌             | 18/72 [00:01<00:04, 12.58it/s]Epoch: 6, train for the 61-th batch, train loss: 0.5454810261726379:  41%|█████▎       | 60/146 [00:08<00:11,  7.21it/s]Epoch: 6, train for the 61-th batch, train loss: 0.5454810261726379:  42%|█████▍       | 61/146 [00:08<00:11,  7.35it/s]evaluate for the 3-th batch, evaluate loss: 0.5135170817375183:   3%|▌                   | 2/78 [00:00<00:12,  6.07it/s]evaluate for the 3-th batch, evaluate loss: 0.5135170817375183:   4%|▊                   | 3/78 [00:00<00:13,  5.42it/s]Epoch: 5, train for the 31-th batch, train loss: 0.6766026020050049:  20%|██▌          | 30/151 [00:04<00:19,  6.25it/s]evaluate for the 20-th batch, evaluate loss: 0.4956561028957367:  25%|████▌             | 18/72 [00:01<00:04, 12.58it/s]evaluate for the 20-th batch, evaluate loss: 0.4956561028957367:  28%|█████             | 20/72 [00:01<00:04, 11.48it/s]Epoch: 3, train for the 28-th batch, train loss: 0.6088770031929016:  11%|█▍           | 27/237 [00:04<00:40,  5.21it/s]Epoch: 5, train for the 31-th batch, train loss: 0.6766026020050049:  21%|██▋          | 31/151 [00:04<00:19,  6.20it/s]Epoch: 3, train for the 28-th batch, train loss: 0.6088770031929016:  12%|█▌           | 28/237 [00:04<00:39,  5.27it/s]Epoch: 7, train for the 48-th batch, train loss: 0.4300847053527832:  39%|█████▏       | 47/119 [00:07<00:11,  6.43it/s]Epoch: 7, train for the 48-th batch, train loss: 0.4300847053527832:  40%|█████▏       | 48/119 [00:07<00:11,  6.09it/s]Epoch: 6, train for the 62-th batch, train loss: 0.4429823160171509:  42%|█████▍       | 61/146 [00:08<00:11,  7.35it/s]Epoch: 6, train for the 62-th batch, train loss: 0.4429823160171509:  42%|█████▌       | 62/146 [00:08<00:12,  6.85it/s]evaluate for the 4-th batch, evaluate loss: 0.513697624206543:   4%|▊                    | 3/78 [00:00<00:13,  5.42it/s]evaluate for the 4-th batch, evaluate loss: 0.513697624206543:   5%|█                    | 4/78 [00:00<00:12,  5.73it/s]evaluate for the 21-th batch, evaluate loss: 0.5332067012786865:  28%|█████             | 20/72 [00:01<00:04, 11.48it/s]Epoch: 5, train for the 32-th batch, train loss: 0.6726731657981873:  21%|██▋          | 31/151 [00:04<00:19,  6.20it/s]Epoch: 5, train for the 32-th batch, train loss: 0.6726731657981873:  21%|██▊          | 32/151 [00:04<00:19,  6.24it/s]evaluate for the 22-th batch, evaluate loss: 0.459521621465683:  28%|█████▎             | 20/72 [00:01<00:04, 11.48it/s]evaluate for the 22-th batch, evaluate loss: 0.459521621465683:  31%|█████▊             | 22/72 [00:01<00:04, 11.48it/s]Epoch: 3, train for the 29-th batch, train loss: 0.576471745967865:  12%|█▋            | 28/237 [00:04<00:39,  5.27it/s]Epoch: 3, train for the 29-th batch, train loss: 0.576471745967865:  12%|█▋            | 29/237 [00:04<00:38,  5.47it/s]Epoch: 7, train for the 49-th batch, train loss: 0.45318570733070374:  40%|████▊       | 48/119 [00:07<00:11,  6.09it/s]Epoch: 7, train for the 49-th batch, train loss: 0.45318570733070374:  41%|████▉       | 49/119 [00:07<00:11,  6.04it/s]Epoch: 6, train for the 63-th batch, train loss: 0.4893164336681366:  42%|█████▌       | 62/146 [00:08<00:12,  6.85it/s]Epoch: 6, train for the 63-th batch, train loss: 0.4893164336681366:  43%|█████▌       | 63/146 [00:08<00:12,  6.45it/s]evaluate for the 23-th batch, evaluate loss: 0.4872889518737793:  31%|█████▌            | 22/72 [00:01<00:04, 11.48it/s]evaluate for the 5-th batch, evaluate loss: 0.5489224195480347:   5%|█                   | 4/78 [00:00<00:12,  5.73it/s]evaluate for the 5-th batch, evaluate loss: 0.5489224195480347:   6%|█▎                  | 5/78 [00:00<00:13,  5.52it/s]Epoch: 7, train for the 50-th batch, train loss: 0.4158575236797333:  41%|█████▎       | 49/119 [00:07<00:11,  6.04it/s]Epoch: 3, train for the 30-th batch, train loss: 0.5842421650886536:  12%|█▌           | 29/237 [00:04<00:38,  5.47it/s]Epoch: 5, train for the 33-th batch, train loss: 0.6536644697189331:  21%|██▊          | 32/151 [00:05<00:19,  6.24it/s]evaluate for the 24-th batch, evaluate loss: 0.4657194912433624:  31%|█████▌            | 22/72 [00:01<00:04, 11.48it/s]evaluate for the 24-th batch, evaluate loss: 0.4657194912433624:  33%|██████            | 24/72 [00:01<00:04, 11.27it/s]Epoch: 7, train for the 50-th batch, train loss: 0.4158575236797333:  42%|█████▍       | 50/119 [00:07<00:11,  6.11it/s]Epoch: 3, train for the 30-th batch, train loss: 0.5842421650886536:  13%|█▋           | 30/237 [00:04<00:37,  5.53it/s]Epoch: 5, train for the 33-th batch, train loss: 0.6536644697189331:  22%|██▊          | 33/151 [00:05<00:20,  5.70it/s]Epoch: 6, train for the 64-th batch, train loss: 0.5027375817298889:  43%|█████▌       | 63/146 [00:08<00:12,  6.45it/s]Epoch: 6, train for the 64-th batch, train loss: 0.5027375817298889:  44%|█████▋       | 64/146 [00:08<00:12,  6.34it/s]evaluate for the 25-th batch, evaluate loss: 0.5894933938980103:  33%|██████            | 24/72 [00:02<00:04, 11.27it/s]evaluate for the 6-th batch, evaluate loss: 0.4776538014411926:   6%|█▎                  | 5/78 [00:01<00:13,  5.52it/s]evaluate for the 6-th batch, evaluate loss: 0.4776538014411926:   8%|█▌                  | 6/78 [00:01<00:13,  5.47it/s]evaluate for the 26-th batch, evaluate loss: 0.4224190413951874:  33%|██████            | 24/72 [00:02<00:04, 11.27it/s]evaluate for the 26-th batch, evaluate loss: 0.4224190413951874:  36%|██████▌           | 26/72 [00:02<00:04, 11.41it/s]Epoch: 3, train for the 31-th batch, train loss: 0.5789381861686707:  13%|█▋           | 30/237 [00:04<00:37,  5.53it/s]Epoch: 5, train for the 34-th batch, train loss: 0.6607818603515625:  22%|██▊          | 33/151 [00:05<00:20,  5.70it/s]Epoch: 7, train for the 51-th batch, train loss: 0.49665409326553345:  42%|█████       | 50/119 [00:07<00:11,  6.11it/s]Epoch: 3, train for the 31-th batch, train loss: 0.5789381861686707:  13%|█▋           | 31/237 [00:04<00:38,  5.39it/s]Epoch: 5, train for the 34-th batch, train loss: 0.6607818603515625:  23%|██▉          | 34/151 [00:05<00:20,  5.58it/s]Epoch: 7, train for the 51-th batch, train loss: 0.49665409326553345:  43%|█████▏      | 51/119 [00:07<00:11,  5.73it/s]Epoch: 6, train for the 65-th batch, train loss: 0.5058925151824951:  44%|█████▋       | 64/146 [00:08<00:12,  6.34it/s]Epoch: 6, train for the 65-th batch, train loss: 0.5058925151824951:  45%|█████▊       | 65/146 [00:08<00:13,  5.88it/s]evaluate for the 27-th batch, evaluate loss: 0.4004654586315155:  36%|██████▌           | 26/72 [00:02<00:04, 11.41it/s]evaluate for the 7-th batch, evaluate loss: 0.537307858467102:   8%|█▌                   | 6/78 [00:01<00:13,  5.47it/s]evaluate for the 7-th batch, evaluate loss: 0.537307858467102:   9%|█▉                   | 7/78 [00:01<00:12,  5.62it/s]Epoch: 3, train for the 32-th batch, train loss: 0.6360186338424683:  13%|█▋           | 31/237 [00:04<00:38,  5.39it/s]Epoch: 3, train for the 32-th batch, train loss: 0.6360186338424683:  14%|█▊           | 32/237 [00:04<00:36,  5.61it/s]evaluate for the 28-th batch, evaluate loss: 0.4459880590438843:  36%|██████▌           | 26/72 [00:02<00:04, 11.41it/s]evaluate for the 28-th batch, evaluate loss: 0.4459880590438843:  39%|███████           | 28/72 [00:02<00:04, 10.76it/s]Epoch: 5, train for the 35-th batch, train loss: 0.6580837965011597:  23%|██▉          | 34/151 [00:05<00:20,  5.58it/s]Epoch: 7, train for the 52-th batch, train loss: 0.42568260431289673:  43%|█████▏      | 51/119 [00:07<00:11,  5.73it/s]Epoch: 5, train for the 35-th batch, train loss: 0.6580837965011597:  23%|███          | 35/151 [00:05<00:21,  5.43it/s]Epoch: 7, train for the 52-th batch, train loss: 0.42568260431289673:  44%|█████▏      | 52/119 [00:07<00:12,  5.50it/s]Epoch: 6, train for the 66-th batch, train loss: 0.4918316900730133:  45%|█████▊       | 65/146 [00:08<00:13,  5.88it/s]Epoch: 6, train for the 66-th batch, train loss: 0.4918316900730133:  45%|█████▉       | 66/146 [00:08<00:14,  5.70it/s]evaluate for the 29-th batch, evaluate loss: 0.4835628271102905:  39%|███████           | 28/72 [00:02<00:04, 10.76it/s]evaluate for the 8-th batch, evaluate loss: 0.39556899666786194:   9%|█▋                 | 7/78 [00:01<00:12,  5.62it/s]evaluate for the 8-th batch, evaluate loss: 0.39556899666786194:  10%|█▉                 | 8/78 [00:01<00:12,  5.57it/s]Epoch: 3, train for the 33-th batch, train loss: 0.5746404528617859:  14%|█▊           | 32/237 [00:05<00:36,  5.61it/s]Epoch: 3, train for the 33-th batch, train loss: 0.5746404528617859:  14%|█▊           | 33/237 [00:05<00:35,  5.80it/s]evaluate for the 30-th batch, evaluate loss: 0.48306137323379517:  39%|██████▌          | 28/72 [00:02<00:04, 10.76it/s]evaluate for the 30-th batch, evaluate loss: 0.48306137323379517:  42%|███████          | 30/72 [00:02<00:03, 10.67it/s]Epoch: 5, train for the 36-th batch, train loss: 0.6758734583854675:  23%|███          | 35/151 [00:05<00:21,  5.43it/s]Epoch: 5, train for the 36-th batch, train loss: 0.6758734583854675:  24%|███          | 36/151 [00:05<00:21,  5.40it/s]Epoch: 6, train for the 67-th batch, train loss: 0.5374693870544434:  45%|█████▉       | 66/146 [00:09<00:14,  5.70it/s]Epoch: 6, train for the 67-th batch, train loss: 0.5374693870544434:  46%|█████▉       | 67/146 [00:09<00:13,  5.72it/s]Epoch: 7, train for the 53-th batch, train loss: 0.38249218463897705:  44%|█████▏      | 52/119 [00:08<00:12,  5.50it/s]Epoch: 7, train for the 53-th batch, train loss: 0.38249218463897705:  45%|█████▎      | 53/119 [00:08<00:12,  5.17it/s]evaluate for the 9-th batch, evaluate loss: 0.35959112644195557:  10%|█▉                 | 8/78 [00:01<00:12,  5.57it/s]evaluate for the 9-th batch, evaluate loss: 0.35959112644195557:  12%|██▏                | 9/78 [00:01<00:12,  5.38it/s]evaluate for the 31-th batch, evaluate loss: 0.5319920182228088:  42%|███████▌          | 30/72 [00:02<00:03, 10.67it/s]Epoch: 3, train for the 34-th batch, train loss: 0.5951555967330933:  14%|█▊           | 33/237 [00:05<00:35,  5.80it/s]Epoch: 3, train for the 34-th batch, train loss: 0.5951555967330933:  14%|█▊           | 34/237 [00:05<00:34,  5.86it/s]Epoch: 5, train for the 37-th batch, train loss: 0.6730282306671143:  24%|███          | 36/151 [00:05<00:21,  5.40it/s]Epoch: 6, train for the 68-th batch, train loss: 0.46294334530830383:  46%|█████▌      | 67/146 [00:09<00:13,  5.72it/s]evaluate for the 32-th batch, evaluate loss: 0.5931128263473511:  42%|███████▌          | 30/72 [00:02<00:03, 10.67it/s]evaluate for the 32-th batch, evaluate loss: 0.5931128263473511:  44%|████████          | 32/72 [00:02<00:03, 11.08it/s]Epoch: 6, train for the 68-th batch, train loss: 0.46294334530830383:  47%|█████▌      | 68/146 [00:09<00:12,  6.24it/s]Epoch: 5, train for the 37-th batch, train loss: 0.6730282306671143:  25%|███▏         | 37/151 [00:05<00:19,  5.73it/s]Epoch: 7, train for the 54-th batch, train loss: 0.3987707495689392:  45%|█████▊       | 53/119 [00:08<00:12,  5.17it/s]Epoch: 7, train for the 54-th batch, train loss: 0.3987707495689392:  45%|█████▉       | 54/119 [00:08<00:11,  5.58it/s]evaluate for the 33-th batch, evaluate loss: 0.5103474855422974:  44%|████████          | 32/72 [00:02<00:03, 11.08it/s]Epoch: 3, train for the 35-th batch, train loss: 0.5737236738204956:  14%|█▊           | 34/237 [00:05<00:34,  5.86it/s]Epoch: 6, train for the 69-th batch, train loss: 0.5191348195075989:  47%|██████       | 68/146 [00:09<00:12,  6.24it/s]evaluate for the 10-th batch, evaluate loss: 0.4656123220920563:  12%|██▏                | 9/78 [00:01<00:12,  5.38it/s]Epoch: 3, train for the 35-th batch, train loss: 0.5737236738204956:  15%|█▉           | 35/237 [00:05<00:34,  5.88it/s]evaluate for the 10-th batch, evaluate loss: 0.4656123220920563:  13%|██▎               | 10/78 [00:01<00:12,  5.27it/s]Epoch: 6, train for the 69-th batch, train loss: 0.5191348195075989:  47%|██████▏      | 69/146 [00:09<00:11,  6.60it/s]evaluate for the 34-th batch, evaluate loss: 0.5528605580329895:  44%|████████          | 32/72 [00:02<00:03, 11.08it/s]evaluate for the 34-th batch, evaluate loss: 0.5528605580329895:  47%|████████▌         | 34/72 [00:02<00:03, 11.22it/s]Epoch: 5, train for the 38-th batch, train loss: 0.6712216138839722:  25%|███▏         | 37/151 [00:06<00:19,  5.73it/s]Epoch: 7, train for the 55-th batch, train loss: 0.4427201747894287:  45%|█████▉       | 54/119 [00:08<00:11,  5.58it/s]Epoch: 7, train for the 55-th batch, train loss: 0.4427201747894287:  46%|██████       | 55/119 [00:08<00:10,  5.85it/s]Epoch: 5, train for the 38-th batch, train loss: 0.6712216138839722:  25%|███▎         | 38/151 [00:06<00:20,  5.49it/s]evaluate for the 35-th batch, evaluate loss: 0.4938770532608032:  47%|████████▌         | 34/72 [00:02<00:03, 11.22it/s]Epoch: 3, train for the 36-th batch, train loss: 0.5456479787826538:  15%|█▉           | 35/237 [00:05<00:34,  5.88it/s]evaluate for the 11-th batch, evaluate loss: 0.5478562712669373:  13%|██▎               | 10/78 [00:02<00:12,  5.27it/s]evaluate for the 11-th batch, evaluate loss: 0.5478562712669373:  14%|██▌               | 11/78 [00:02<00:12,  5.33it/s]Epoch: 3, train for the 36-th batch, train loss: 0.5456479787826538:  15%|█▉           | 36/237 [00:05<00:35,  5.73it/s]Epoch: 7, train for the 56-th batch, train loss: 0.42448118329048157:  46%|█████▌      | 55/119 [00:08<00:10,  5.85it/s]Epoch: 7, train for the 56-th batch, train loss: 0.42448118329048157:  47%|█████▋      | 56/119 [00:08<00:10,  6.23it/s]evaluate for the 36-th batch, evaluate loss: 0.5013529062271118:  47%|████████▌         | 34/72 [00:03<00:03, 11.22it/s]evaluate for the 36-th batch, evaluate loss: 0.5013529062271118:  50%|█████████         | 36/72 [00:03<00:03, 11.35it/s]Epoch: 5, train for the 39-th batch, train loss: 0.6494327187538147:  25%|███▎         | 38/151 [00:06<00:20,  5.49it/s]Epoch: 5, train for the 39-th batch, train loss: 0.6494327187538147:  26%|███▎         | 39/151 [00:06<00:20,  5.41it/s]evaluate for the 37-th batch, evaluate loss: 0.48966917395591736:  50%|████████▌        | 36/72 [00:03<00:03, 11.35it/s]Epoch: 7, train for the 57-th batch, train loss: 0.4351757764816284:  47%|██████       | 56/119 [00:08<00:10,  6.23it/s]Epoch: 3, train for the 37-th batch, train loss: 0.584371030330658:  15%|██▏           | 36/237 [00:05<00:35,  5.73it/s]Epoch: 6, train for the 70-th batch, train loss: 0.5255351662635803:  47%|██████▏      | 69/146 [00:09<00:11,  6.60it/s]Epoch: 7, train for the 57-th batch, train loss: 0.4351757764816284:  48%|██████▏      | 57/119 [00:08<00:10,  6.18it/s]Epoch: 6, train for the 70-th batch, train loss: 0.5255351662635803:  48%|██████▏      | 70/146 [00:09<00:16,  4.68it/s]evaluate for the 12-th batch, evaluate loss: 0.49469515681266785:  14%|██▍              | 11/78 [00:02<00:12,  5.33it/s]evaluate for the 12-th batch, evaluate loss: 0.49469515681266785:  15%|██▌              | 12/78 [00:02<00:12,  5.35it/s]Epoch: 3, train for the 37-th batch, train loss: 0.584371030330658:  16%|██▏           | 37/237 [00:05<00:35,  5.65it/s]evaluate for the 38-th batch, evaluate loss: 0.39490407705307007:  50%|████████▌        | 36/72 [00:03<00:03, 11.35it/s]evaluate for the 38-th batch, evaluate loss: 0.39490407705307007:  53%|████████▉        | 38/72 [00:03<00:02, 11.64it/s]Epoch: 5, train for the 40-th batch, train loss: 0.6520394086837769:  26%|███▎         | 39/151 [00:06<00:20,  5.41it/s]Epoch: 5, train for the 40-th batch, train loss: 0.6520394086837769:  26%|███▍         | 40/151 [00:06<00:20,  5.43it/s]evaluate for the 39-th batch, evaluate loss: 0.42960861325263977:  53%|████████▉        | 38/72 [00:03<00:02, 11.64it/s]Epoch: 7, train for the 58-th batch, train loss: 0.3840988576412201:  48%|██████▏      | 57/119 [00:08<00:10,  6.18it/s]Epoch: 7, train for the 58-th batch, train loss: 0.3840988576412201:  49%|██████▎      | 58/119 [00:08<00:10,  5.92it/s]evaluate for the 40-th batch, evaluate loss: 0.4991503059864044:  53%|█████████▌        | 38/72 [00:03<00:02, 11.64it/s]evaluate for the 40-th batch, evaluate loss: 0.4991503059864044:  56%|██████████        | 40/72 [00:03<00:02, 11.72it/s]Epoch: 3, train for the 38-th batch, train loss: 0.5952024459838867:  16%|██           | 37/237 [00:05<00:35,  5.65it/s]Epoch: 6, train for the 71-th batch, train loss: 0.5524812936782837:  48%|██████▏      | 70/146 [00:09<00:16,  4.68it/s]evaluate for the 13-th batch, evaluate loss: 0.530412495136261:  15%|██▉                | 12/78 [00:02<00:12,  5.35it/s]evaluate for the 13-th batch, evaluate loss: 0.530412495136261:  17%|███▏               | 13/78 [00:02<00:12,  5.26it/s]Epoch: 6, train for the 71-th batch, train loss: 0.5524812936782837:  49%|██████▎      | 71/146 [00:09<00:15,  4.78it/s]Epoch: 3, train for the 38-th batch, train loss: 0.5952024459838867:  16%|██           | 38/237 [00:06<00:36,  5.45it/s]Epoch: 5, train for the 41-th batch, train loss: 0.6536661982536316:  26%|███▍         | 40/151 [00:06<00:20,  5.43it/s]Epoch: 5, train for the 41-th batch, train loss: 0.6536661982536316:  27%|███▌         | 41/151 [00:06<00:19,  5.65it/s]evaluate for the 41-th batch, evaluate loss: 0.49077367782592773:  56%|█████████▍       | 40/72 [00:03<00:02, 11.72it/s]evaluate for the 42-th batch, evaluate loss: 0.48253336548805237:  56%|█████████▍       | 40/72 [00:03<00:02, 11.72it/s]evaluate for the 42-th batch, evaluate loss: 0.48253336548805237:  58%|█████████▉       | 42/72 [00:03<00:02, 12.17it/s]Epoch: 6, train for the 72-th batch, train loss: 0.5462202429771423:  49%|██████▎      | 71/146 [00:10<00:15,  4.78it/s]Epoch: 7, train for the 59-th batch, train loss: 0.410883367061615:  49%|██████▊       | 58/119 [00:09<00:10,  5.92it/s]Epoch: 6, train for the 72-th batch, train loss: 0.5462202429771423:  49%|██████▍      | 72/146 [00:10<00:14,  5.09it/s]Epoch: 5, train for the 42-th batch, train loss: 0.6418660879135132:  27%|███▌         | 41/151 [00:06<00:19,  5.65it/s]Epoch: 7, train for the 59-th batch, train loss: 0.410883367061615:  50%|██████▉       | 59/119 [00:09<00:10,  5.59it/s]Epoch: 5, train for the 42-th batch, train loss: 0.6418660879135132:  28%|███▌         | 42/151 [00:06<00:18,  6.01it/s]evaluate for the 43-th batch, evaluate loss: 0.48904669284820557:  58%|█████████▉       | 42/72 [00:03<00:02, 12.17it/s]Epoch: 3, train for the 39-th batch, train loss: 0.5941938757896423:  16%|██           | 38/237 [00:06<00:36,  5.45it/s]evaluate for the 14-th batch, evaluate loss: 0.355741024017334:  17%|███▏               | 13/78 [00:02<00:12,  5.26it/s]evaluate for the 14-th batch, evaluate loss: 0.355741024017334:  18%|███▍               | 14/78 [00:02<00:12,  5.13it/s]Epoch: 3, train for the 39-th batch, train loss: 0.5941938757896423:  16%|██▏          | 39/237 [00:06<00:37,  5.26it/s]Epoch: 6, train for the 73-th batch, train loss: 0.5794000029563904:  49%|██████▍      | 72/146 [00:10<00:14,  5.09it/s]Epoch: 6, train for the 73-th batch, train loss: 0.5794000029563904:  50%|██████▌      | 73/146 [00:10<00:12,  5.80it/s]evaluate for the 44-th batch, evaluate loss: 0.4840600788593292:  58%|██████████▌       | 42/72 [00:03<00:02, 12.17it/s]evaluate for the 44-th batch, evaluate loss: 0.4840600788593292:  61%|███████████       | 44/72 [00:03<00:02, 12.11it/s]Epoch: 7, train for the 60-th batch, train loss: 0.4276276230812073:  50%|██████▍      | 59/119 [00:09<00:10,  5.59it/s]Epoch: 5, train for the 43-th batch, train loss: 0.6520218253135681:  28%|███▌         | 42/151 [00:06<00:18,  6.01it/s]Epoch: 7, train for the 60-th batch, train loss: 0.4276276230812073:  50%|██████▌      | 60/119 [00:09<00:10,  5.78it/s]Epoch: 5, train for the 43-th batch, train loss: 0.6520218253135681:  28%|███▋         | 43/151 [00:06<00:18,  5.91it/s]Epoch: 3, train for the 40-th batch, train loss: 0.5954521894454956:  16%|██▏          | 39/237 [00:06<00:37,  5.26it/s]evaluate for the 45-th batch, evaluate loss: 0.4835626184940338:  61%|███████████       | 44/72 [00:03<00:02, 12.11it/s]evaluate for the 15-th batch, evaluate loss: 0.36018335819244385:  18%|███              | 14/78 [00:02<00:12,  5.13it/s]evaluate for the 15-th batch, evaluate loss: 0.36018335819244385:  19%|███▎             | 15/78 [00:02<00:11,  5.30it/s]Epoch: 3, train for the 40-th batch, train loss: 0.5954521894454956:  17%|██▏          | 40/237 [00:06<00:36,  5.35it/s]Epoch: 6, train for the 74-th batch, train loss: 0.522549569606781:  50%|███████       | 73/146 [00:10<00:12,  5.80it/s]Epoch: 6, train for the 74-th batch, train loss: 0.522549569606781:  51%|███████       | 74/146 [00:10<00:11,  6.14it/s]evaluate for the 46-th batch, evaluate loss: 0.5400416851043701:  61%|███████████       | 44/72 [00:03<00:02, 12.11it/s]evaluate for the 46-th batch, evaluate loss: 0.5400416851043701:  64%|███████████▌      | 46/72 [00:03<00:02, 12.69it/s]Epoch: 7, train for the 61-th batch, train loss: 0.43808382749557495:  50%|██████      | 60/119 [00:09<00:10,  5.78it/s]Epoch: 7, train for the 61-th batch, train loss: 0.43808382749557495:  51%|██████▏     | 61/119 [00:09<00:09,  6.34it/s]Epoch: 5, train for the 44-th batch, train loss: 0.6241140365600586:  28%|███▋         | 43/151 [00:07<00:18,  5.91it/s]evaluate for the 47-th batch, evaluate loss: 0.3140714466571808:  64%|███████████▌      | 46/72 [00:03<00:02, 12.69it/s]Epoch: 5, train for the 44-th batch, train loss: 0.6241140365600586:  29%|███▊         | 44/151 [00:07<00:18,  5.87it/s]evaluate for the 16-th batch, evaluate loss: 0.332241952419281:  19%|███▋               | 15/78 [00:02<00:11,  5.30it/s]evaluate for the 16-th batch, evaluate loss: 0.332241952419281:  21%|███▉               | 16/78 [00:02<00:11,  5.32it/s]Epoch: 3, train for the 41-th batch, train loss: 0.5792375802993774:  17%|██▏          | 40/237 [00:06<00:36,  5.35it/s]Epoch: 6, train for the 75-th batch, train loss: 0.5202315449714661:  51%|██████▌      | 74/146 [00:10<00:11,  6.14it/s]Epoch: 3, train for the 41-th batch, train loss: 0.5792375802993774:  17%|██▏          | 41/237 [00:06<00:37,  5.20it/s]Epoch: 6, train for the 75-th batch, train loss: 0.5202315449714661:  51%|██████▋      | 75/146 [00:10<00:11,  6.04it/s]evaluate for the 48-th batch, evaluate loss: 0.27928054332733154:  64%|██████████▊      | 46/72 [00:03<00:02, 12.69it/s]evaluate for the 48-th batch, evaluate loss: 0.27928054332733154:  67%|███████████▎     | 48/72 [00:03<00:01, 12.70it/s]Epoch: 7, train for the 62-th batch, train loss: 0.4860824942588806:  51%|██████▋      | 61/119 [00:09<00:09,  6.34it/s]Epoch: 7, train for the 62-th batch, train loss: 0.4860824942588806:  52%|██████▊      | 62/119 [00:09<00:08,  6.35it/s]Epoch: 5, train for the 45-th batch, train loss: 0.6208629608154297:  29%|███▊         | 44/151 [00:07<00:18,  5.87it/s]evaluate for the 49-th batch, evaluate loss: 0.44098055362701416:  67%|███████████▎     | 48/72 [00:04<00:01, 12.70it/s]Epoch: 5, train for the 45-th batch, train loss: 0.6208629608154297:  30%|███▊         | 45/151 [00:07<00:18,  5.82it/s]Epoch: 6, train for the 76-th batch, train loss: 0.5298832058906555:  51%|██████▋      | 75/146 [00:10<00:11,  6.04it/s]evaluate for the 17-th batch, evaluate loss: 0.4771413207054138:  21%|███▋              | 16/78 [00:03<00:11,  5.32it/s]evaluate for the 17-th batch, evaluate loss: 0.4771413207054138:  22%|███▉              | 17/78 [00:03<00:11,  5.41it/s]Epoch: 3, train for the 42-th batch, train loss: 0.6418814063072205:  17%|██▏          | 41/237 [00:06<00:37,  5.20it/s]evaluate for the 50-th batch, evaluate loss: 0.39548537135124207:  67%|███████████▎     | 48/72 [00:04<00:01, 12.70it/s]evaluate for the 50-th batch, evaluate loss: 0.39548537135124207:  69%|███████████▊     | 50/72 [00:04<00:01, 12.63it/s]Epoch: 6, train for the 76-th batch, train loss: 0.5298832058906555:  52%|██████▊      | 76/146 [00:10<00:11,  6.05it/s]Epoch: 7, train for the 63-th batch, train loss: 0.4278787076473236:  52%|██████▊      | 62/119 [00:09<00:08,  6.35it/s]Epoch: 3, train for the 42-th batch, train loss: 0.6418814063072205:  18%|██▎          | 42/237 [00:06<00:36,  5.32it/s]Epoch: 7, train for the 63-th batch, train loss: 0.4278787076473236:  53%|██████▉      | 63/119 [00:09<00:08,  6.40it/s]Epoch: 5, train for the 46-th batch, train loss: 0.6286420822143555:  30%|███▊         | 45/151 [00:07<00:18,  5.82it/s]evaluate for the 51-th batch, evaluate loss: 0.49193236231803894:  69%|███████████▊     | 50/72 [00:04<00:01, 12.63it/s]Epoch: 5, train for the 46-th batch, train loss: 0.6286420822143555:  30%|███▉         | 46/151 [00:07<00:17,  5.85it/s]Epoch: 6, train for the 77-th batch, train loss: 0.5579267740249634:  52%|██████▊      | 76/146 [00:10<00:11,  6.05it/s]Epoch: 6, train for the 77-th batch, train loss: 0.5579267740249634:  53%|██████▊      | 77/146 [00:10<00:11,  6.17it/s]evaluate for the 18-th batch, evaluate loss: 0.35528045892715454:  22%|███▋             | 17/78 [00:03<00:11,  5.41it/s]evaluate for the 18-th batch, evaluate loss: 0.35528045892715454:  23%|███▉             | 18/78 [00:03<00:11,  5.28it/s]Epoch: 7, train for the 64-th batch, train loss: 0.4594480097293854:  53%|██████▉      | 63/119 [00:09<00:08,  6.40it/s]Epoch: 3, train for the 43-th batch, train loss: 0.5871926546096802:  18%|██▎          | 42/237 [00:06<00:36,  5.32it/s]evaluate for the 52-th batch, evaluate loss: 0.4278744161128998:  69%|████████████▌     | 50/72 [00:04<00:01, 12.63it/s]evaluate for the 52-th batch, evaluate loss: 0.4278744161128998:  72%|█████████████     | 52/72 [00:04<00:01, 11.76it/s]Epoch: 7, train for the 64-th batch, train loss: 0.4594480097293854:  54%|██████▉      | 64/119 [00:09<00:09,  6.03it/s]Epoch: 3, train for the 43-th batch, train loss: 0.5871926546096802:  18%|██▎          | 43/237 [00:06<00:37,  5.19it/s]Epoch: 5, train for the 47-th batch, train loss: 0.6394447088241577:  30%|███▉         | 46/151 [00:07<00:17,  5.85it/s]Epoch: 5, train for the 47-th batch, train loss: 0.6394447088241577:  31%|████         | 47/151 [00:07<00:17,  6.02it/s]evaluate for the 53-th batch, evaluate loss: 0.4840162694454193:  72%|█████████████     | 52/72 [00:04<00:01, 11.76it/s]Epoch: 6, train for the 78-th batch, train loss: 0.5241113901138306:  53%|██████▊      | 77/146 [00:10<00:11,  6.17it/s]Epoch: 6, train for the 78-th batch, train loss: 0.5241113901138306:  53%|██████▉      | 78/146 [00:11<00:10,  6.36it/s]evaluate for the 54-th batch, evaluate loss: 0.5171882510185242:  72%|█████████████     | 52/72 [00:04<00:01, 11.76it/s]evaluate for the 54-th batch, evaluate loss: 0.5171882510185242:  75%|█████████████▌    | 54/72 [00:04<00:01, 11.78it/s]Epoch: 7, train for the 65-th batch, train loss: 0.4522816240787506:  54%|██████▉      | 64/119 [00:10<00:09,  6.03it/s]Epoch: 7, train for the 65-th batch, train loss: 0.4522816240787506:  55%|███████      | 65/119 [00:10<00:09,  5.96it/s]evaluate for the 19-th batch, evaluate loss: 0.576665997505188:  23%|████▍              | 18/78 [00:03<00:11,  5.28it/s]evaluate for the 19-th batch, evaluate loss: 0.576665997505188:  24%|████▋              | 19/78 [00:03<00:11,  5.22it/s]Epoch: 3, train for the 44-th batch, train loss: 0.5573540329933167:  18%|██▎          | 43/237 [00:07<00:37,  5.19it/s]Epoch: 3, train for the 44-th batch, train loss: 0.5573540329933167:  19%|██▍          | 44/237 [00:07<00:36,  5.22it/s]Epoch: 5, train for the 48-th batch, train loss: 0.6034652590751648:  31%|████         | 47/151 [00:07<00:17,  6.02it/s]Epoch: 6, train for the 79-th batch, train loss: 0.5368338823318481:  53%|██████▉      | 78/146 [00:11<00:10,  6.36it/s]evaluate for the 55-th batch, evaluate loss: 0.4934796690940857:  75%|█████████████▌    | 54/72 [00:04<00:01, 11.78it/s]Epoch: 6, train for the 79-th batch, train loss: 0.5368338823318481:  54%|███████      | 79/146 [00:11<00:10,  6.39it/s]Epoch: 5, train for the 48-th batch, train loss: 0.6034652590751648:  32%|████▏        | 48/151 [00:07<00:17,  5.87it/s]Epoch: 7, train for the 66-th batch, train loss: 0.4440423548221588:  55%|███████      | 65/119 [00:10<00:09,  5.96it/s]Epoch: 3, train for the 45-th batch, train loss: 0.5647438168525696:  19%|██▍          | 44/237 [00:07<00:36,  5.22it/s]Epoch: 7, train for the 66-th batch, train loss: 0.4440423548221588:  55%|███████▏     | 66/119 [00:10<00:08,  5.93it/s]evaluate for the 56-th batch, evaluate loss: 0.501680314540863:  75%|██████████████▎    | 54/72 [00:04<00:01, 11.78it/s]evaluate for the 56-th batch, evaluate loss: 0.501680314540863:  78%|██████████████▊    | 56/72 [00:04<00:01, 11.45it/s]Epoch: 3, train for the 45-th batch, train loss: 0.5647438168525696:  19%|██▍          | 45/237 [00:07<00:34,  5.54it/s]evaluate for the 20-th batch, evaluate loss: 0.3200705647468567:  24%|████▍             | 19/78 [00:03<00:11,  5.22it/s]evaluate for the 20-th batch, evaluate loss: 0.3200705647468567:  26%|████▌             | 20/78 [00:03<00:11,  5.22it/s]Epoch: 6, train for the 80-th batch, train loss: 0.5738141536712646:  54%|███████      | 79/146 [00:11<00:10,  6.39it/s]Epoch: 6, train for the 80-th batch, train loss: 0.5738141536712646:  55%|███████      | 80/146 [00:11<00:10,  6.50it/s]Epoch: 5, train for the 49-th batch, train loss: 0.6323952078819275:  32%|████▏        | 48/151 [00:07<00:17,  5.87it/s]evaluate for the 57-th batch, evaluate loss: 0.4543263018131256:  78%|██████████████    | 56/72 [00:04<00:01, 11.45it/s]Epoch: 5, train for the 49-th batch, train loss: 0.6323952078819275:  32%|████▏        | 49/151 [00:07<00:18,  5.55it/s]Epoch: 7, train for the 67-th batch, train loss: 0.48762190341949463:  55%|██████▋     | 66/119 [00:10<00:08,  5.93it/s]Epoch: 7, train for the 67-th batch, train loss: 0.48762190341949463:  56%|██████▊     | 67/119 [00:10<00:08,  6.35it/s]Epoch: 3, train for the 46-th batch, train loss: 0.603710949420929:  19%|██▋           | 45/237 [00:07<00:34,  5.54it/s]Epoch: 6, train for the 81-th batch, train loss: 0.47548362612724304:  55%|██████▌     | 80/146 [00:11<00:10,  6.50it/s]Epoch: 6, train for the 81-th batch, train loss: 0.47548362612724304:  55%|██████▋     | 81/146 [00:11<00:09,  6.76it/s]Epoch: 3, train for the 46-th batch, train loss: 0.603710949420929:  19%|██▋           | 46/237 [00:07<00:35,  5.42it/s]evaluate for the 58-th batch, evaluate loss: 0.512296736240387:  78%|██████████████▊    | 56/72 [00:04<00:01, 11.45it/s]evaluate for the 58-th batch, evaluate loss: 0.512296736240387:  81%|███████████████▎   | 58/72 [00:04<00:01, 10.94it/s]evaluate for the 21-th batch, evaluate loss: 0.31124722957611084:  26%|████▎            | 20/78 [00:03<00:11,  5.22it/s]evaluate for the 21-th batch, evaluate loss: 0.31124722957611084:  27%|████▌            | 21/78 [00:03<00:10,  5.28it/s]evaluate for the 59-th batch, evaluate loss: 0.47718992829322815:  81%|█████████████▋   | 58/72 [00:04<00:01, 10.94it/s]Epoch: 5, train for the 50-th batch, train loss: 0.5839349031448364:  32%|████▏        | 49/151 [00:08<00:18,  5.55it/s]Epoch: 7, train for the 68-th batch, train loss: 0.4187021553516388:  56%|███████▎     | 67/119 [00:10<00:08,  6.35it/s]Epoch: 7, train for the 68-th batch, train loss: 0.4187021553516388:  57%|███████▍     | 68/119 [00:10<00:08,  6.25it/s]Epoch: 5, train for the 50-th batch, train loss: 0.5839349031448364:  33%|████▎        | 50/151 [00:08<00:18,  5.44it/s]Epoch: 6, train for the 82-th batch, train loss: 0.5210422277450562:  55%|███████▏     | 81/146 [00:11<00:09,  6.76it/s]Epoch: 3, train for the 47-th batch, train loss: 0.5676453709602356:  19%|██▌          | 46/237 [00:07<00:35,  5.42it/s]evaluate for the 60-th batch, evaluate loss: 0.48510268330574036:  81%|█████████████▋   | 58/72 [00:05<00:01, 10.94it/s]evaluate for the 60-th batch, evaluate loss: 0.48510268330574036:  83%|██████████████▏  | 60/72 [00:05<00:01, 11.75it/s]Epoch: 6, train for the 82-th batch, train loss: 0.5210422277450562:  56%|███████▎     | 82/146 [00:11<00:09,  6.64it/s]evaluate for the 22-th batch, evaluate loss: 0.47787031531333923:  27%|████▌            | 21/78 [00:04<00:10,  5.28it/s]evaluate for the 22-th batch, evaluate loss: 0.47787031531333923:  28%|████▊            | 22/78 [00:04<00:10,  5.56it/s]Epoch: 3, train for the 47-th batch, train loss: 0.5676453709602356:  20%|██▌          | 47/237 [00:07<00:33,  5.60it/s]evaluate for the 61-th batch, evaluate loss: 0.44120916724205017:  83%|██████████████▏  | 60/72 [00:05<00:01, 11.75it/s]Epoch: 5, train for the 51-th batch, train loss: 0.6216970086097717:  33%|████▎        | 50/151 [00:08<00:18,  5.44it/s]Epoch: 5, train for the 51-th batch, train loss: 0.6216970086097717:  34%|████▍        | 51/151 [00:08<00:18,  5.54it/s]evaluate for the 62-th batch, evaluate loss: 0.45791229605674744:  83%|██████████████▏  | 60/72 [00:05<00:01, 11.75it/s]evaluate for the 62-th batch, evaluate loss: 0.45791229605674744:  86%|██████████████▋  | 62/72 [00:05<00:00, 12.59it/s]Epoch: 7, train for the 69-th batch, train loss: 0.41579535603523254:  57%|██████▊     | 68/119 [00:10<00:08,  6.25it/s]Epoch: 7, train for the 69-th batch, train loss: 0.41579535603523254:  58%|██████▉     | 69/119 [00:10<00:08,  5.78it/s]Epoch: 6, train for the 83-th batch, train loss: 0.5299896597862244:  56%|███████▎     | 82/146 [00:11<00:09,  6.64it/s]Epoch: 6, train for the 83-th batch, train loss: 0.5299896597862244:  57%|███████▍     | 83/146 [00:11<00:10,  6.17it/s]Epoch: 3, train for the 48-th batch, train loss: 0.5971230864524841:  20%|██▌          | 47/237 [00:07<00:33,  5.60it/s]evaluate for the 63-th batch, evaluate loss: 0.42587020993232727:  86%|██████████████▋  | 62/72 [00:05<00:00, 12.59it/s]evaluate for the 23-th batch, evaluate loss: 0.3735816776752472:  28%|█████             | 22/78 [00:04<00:10,  5.56it/s]evaluate for the 23-th batch, evaluate loss: 0.3735816776752472:  29%|█████▎            | 23/78 [00:04<00:10,  5.38it/s]Epoch: 3, train for the 48-th batch, train loss: 0.5971230864524841:  20%|██▋          | 48/237 [00:07<00:35,  5.29it/s]Epoch: 5, train for the 52-th batch, train loss: 0.6333717107772827:  34%|████▍        | 51/151 [00:08<00:18,  5.54it/s]evaluate for the 64-th batch, evaluate loss: 0.5502685308456421:  86%|███████████████▌  | 62/72 [00:05<00:00, 12.59it/s]evaluate for the 64-th batch, evaluate loss: 0.5502685308456421:  89%|████████████████  | 64/72 [00:05<00:00, 12.36it/s]Epoch: 5, train for the 52-th batch, train loss: 0.6333717107772827:  34%|████▍        | 52/151 [00:08<00:17,  5.50it/s]Epoch: 7, train for the 70-th batch, train loss: 0.3951437175273895:  58%|███████▌     | 69/119 [00:10<00:08,  5.78it/s]Epoch: 7, train for the 70-th batch, train loss: 0.3951437175273895:  59%|███████▋     | 70/119 [00:10<00:08,  5.74it/s]Epoch: 6, train for the 84-th batch, train loss: 0.5127145051956177:  57%|███████▍     | 83/146 [00:11<00:10,  6.17it/s]Epoch: 6, train for the 84-th batch, train loss: 0.5127145051956177:  58%|███████▍     | 84/146 [00:11<00:10,  6.18it/s]evaluate for the 65-th batch, evaluate loss: 0.5192902088165283:  89%|████████████████  | 64/72 [00:05<00:00, 12.36it/s]Epoch: 3, train for the 49-th batch, train loss: 0.5722896456718445:  20%|██▋          | 48/237 [00:08<00:35,  5.29it/s]evaluate for the 24-th batch, evaluate loss: 0.4633373022079468:  29%|█████▎            | 23/78 [00:04<00:10,  5.38it/s]evaluate for the 24-th batch, evaluate loss: 0.4633373022079468:  31%|█████▌            | 24/78 [00:04<00:10,  5.33it/s]Epoch: 3, train for the 49-th batch, train loss: 0.5722896456718445:  21%|██▋          | 49/237 [00:08<00:35,  5.31it/s]evaluate for the 66-th batch, evaluate loss: 0.43830177187919617:  89%|███████████████  | 64/72 [00:05<00:00, 12.36it/s]evaluate for the 66-th batch, evaluate loss: 0.43830177187919617:  92%|███████████████▌ | 66/72 [00:05<00:00, 13.25it/s]Epoch: 5, train for the 53-th batch, train loss: 0.6084999442100525:  34%|████▍        | 52/151 [00:08<00:17,  5.50it/s]Epoch: 7, train for the 71-th batch, train loss: 0.4515133500099182:  59%|███████▋     | 70/119 [00:11<00:08,  5.74it/s]evaluate for the 67-th batch, evaluate loss: 0.4690071642398834:  92%|████████████████▌ | 66/72 [00:05<00:00, 13.25it/s]Epoch: 5, train for the 53-th batch, train loss: 0.6084999442100525:  35%|████▌        | 53/151 [00:08<00:17,  5.53it/s]Epoch: 7, train for the 71-th batch, train loss: 0.4515133500099182:  60%|███████▊     | 71/119 [00:11<00:08,  5.83it/s]Epoch: 6, train for the 85-th batch, train loss: 0.47676488757133484:  58%|██████▉     | 84/146 [00:12<00:10,  6.18it/s]Epoch: 6, train for the 85-th batch, train loss: 0.47676488757133484:  58%|██████▉     | 85/146 [00:12<00:10,  5.69it/s]Epoch: 3, train for the 50-th batch, train loss: 0.5788733959197998:  21%|██▋          | 49/237 [00:08<00:35,  5.31it/s]evaluate for the 68-th batch, evaluate loss: 0.4661210775375366:  92%|████████████████▌ | 66/72 [00:05<00:00, 13.25it/s]evaluate for the 68-th batch, evaluate loss: 0.4661210775375366:  94%|█████████████████ | 68/72 [00:05<00:00, 13.09it/s]evaluate for the 25-th batch, evaluate loss: 0.48004063963890076:  31%|█████▏           | 24/78 [00:04<00:10,  5.33it/s]evaluate for the 25-th batch, evaluate loss: 0.48004063963890076:  32%|█████▍           | 25/78 [00:04<00:09,  5.43it/s]Epoch: 3, train for the 50-th batch, train loss: 0.5788733959197998:  21%|██▋          | 50/237 [00:08<00:34,  5.45it/s]Epoch: 7, train for the 72-th batch, train loss: 0.46863386034965515:  60%|███████▏    | 71/119 [00:11<00:08,  5.83it/s]Epoch: 7, train for the 72-th batch, train loss: 0.46863386034965515:  61%|███████▎    | 72/119 [00:11<00:07,  6.04it/s]Epoch: 5, train for the 54-th batch, train loss: 0.5948078632354736:  35%|████▌        | 53/151 [00:08<00:17,  5.53it/s]evaluate for the 69-th batch, evaluate loss: 0.4738556444644928:  94%|█████████████████ | 68/72 [00:05<00:00, 13.09it/s]Epoch: 6, train for the 86-th batch, train loss: 0.5042325854301453:  58%|███████▌     | 85/146 [00:12<00:10,  5.69it/s]Epoch: 6, train for the 86-th batch, train loss: 0.5042325854301453:  59%|███████▋     | 86/146 [00:12<00:09,  6.18it/s]Epoch: 5, train for the 54-th batch, train loss: 0.5948078632354736:  36%|████▋        | 54/151 [00:08<00:17,  5.41it/s]evaluate for the 70-th batch, evaluate loss: 0.4949255883693695:  94%|█████████████████ | 68/72 [00:05<00:00, 13.09it/s]evaluate for the 70-th batch, evaluate loss: 0.4949255883693695:  97%|█████████████████▌| 70/72 [00:05<00:00, 12.60it/s]Epoch: 3, train for the 51-th batch, train loss: 0.5540053248405457:  21%|██▋          | 50/237 [00:08<00:34,  5.45it/s]Epoch: 7, train for the 73-th batch, train loss: 0.4612758159637451:  61%|███████▊     | 72/119 [00:11<00:07,  6.04it/s]evaluate for the 26-th batch, evaluate loss: 0.4172632098197937:  32%|█████▊            | 25/78 [00:04<00:09,  5.43it/s]evaluate for the 26-th batch, evaluate loss: 0.4172632098197937:  33%|██████            | 26/78 [00:04<00:10,  5.17it/s]Epoch: 7, train for the 73-th batch, train loss: 0.4612758159637451:  61%|███████▉     | 73/119 [00:11<00:07,  6.04it/s]Epoch: 3, train for the 51-th batch, train loss: 0.5540053248405457:  22%|██▊          | 51/237 [00:08<00:36,  5.09it/s]Epoch: 6, train for the 87-th batch, train loss: 0.5233360528945923:  59%|███████▋     | 86/146 [00:12<00:09,  6.18it/s]Epoch: 6, train for the 87-th batch, train loss: 0.5233360528945923:  60%|███████▋     | 87/146 [00:12<00:09,  6.03it/s]evaluate for the 71-th batch, evaluate loss: 0.5314550995826721:  97%|█████████████████▌| 70/72 [00:05<00:00, 12.60it/s]Epoch: 5, train for the 55-th batch, train loss: 0.5843423008918762:  36%|████▋        | 54/151 [00:09<00:17,  5.41it/s]Epoch: 5, train for the 55-th batch, train loss: 0.5843423008918762:  36%|████▋        | 55/151 [00:09<00:18,  5.26it/s]Epoch: 7, train for the 74-th batch, train loss: 0.46882182359695435:  61%|███████▎    | 73/119 [00:11<00:07,  6.04it/s]evaluate for the 72-th batch, evaluate loss: 0.5663667321205139:  97%|█████████████████▌| 70/72 [00:05<00:00, 12.60it/s]evaluate for the 72-th batch, evaluate loss: 0.5663667321205139: 100%|██████████████████| 72/72 [00:05<00:00, 11.85it/s]evaluate for the 72-th batch, evaluate loss: 0.5663667321205139: 100%|██████████████████| 72/72 [00:05<00:00, 12.09it/s]
Epoch: 7, train for the 74-th batch, train loss: 0.46882182359695435:  62%|███████▍    | 74/119 [00:11<00:07,  6.37it/s]Epoch: 3, train for the 52-th batch, train loss: 0.6116269826889038:  22%|██▊          | 51/237 [00:08<00:36,  5.09it/s]evaluate for the 27-th batch, evaluate loss: 0.5925787091255188:  33%|██████            | 26/78 [00:05<00:10,  5.17it/s]evaluate for the 27-th batch, evaluate loss: 0.5925787091255188:  35%|██████▏           | 27/78 [00:05<00:09,  5.16it/s]Epoch: 3, train for the 52-th batch, train loss: 0.6116269826889038:  22%|██▊          | 52/237 [00:08<00:35,  5.21it/s]Epoch: 6, train for the 88-th batch, train loss: 0.5522369742393494:  60%|███████▋     | 87/146 [00:12<00:09,  6.03it/s]Epoch: 6, train for the 88-th batch, train loss: 0.5522369742393494:  60%|███████▊     | 88/146 [00:12<00:09,  5.87it/s]Epoch: 5, train for the 56-th batch, train loss: 0.5571591854095459:  36%|████▋        | 55/151 [00:09<00:18,  5.26it/s]Epoch: 5, train for the 56-th batch, train loss: 0.5571591854095459:  37%|████▊        | 56/151 [00:09<00:19,  4.95it/s]Epoch: 3, train for the 53-th batch, train loss: 0.5905840992927551:  22%|██▊          | 52/237 [00:08<00:35,  5.21it/s]Epoch: 3, train for the 53-th batch, train loss: 0.5905840992927551:  22%|██▉          | 53/237 [00:08<00:33,  5.45it/s]evaluate for the 28-th batch, evaluate loss: 0.6448746919631958:  35%|██████▏           | 27/78 [00:05<00:09,  5.16it/s]evaluate for the 28-th batch, evaluate loss: 0.6448746919631958:  36%|██████▍           | 28/78 [00:05<00:09,  5.33it/s]Epoch: 6, train for the 89-th batch, train loss: 0.4811001420021057:  60%|███████▊     | 88/146 [00:12<00:09,  5.87it/s]Epoch: 6, train for the 89-th batch, train loss: 0.4811001420021057:  61%|███████▉     | 89/146 [00:12<00:09,  6.16it/s]Epoch: 7, train for the 75-th batch, train loss: 0.4077255427837372:  62%|████████     | 74/119 [00:11<00:07,  6.37it/s]Epoch: 7, train for the 75-th batch, train loss: 0.4077255427837372:  63%|████████▏    | 75/119 [00:11<00:08,  5.32it/s]Epoch: 5, train for the 57-th batch, train loss: 0.5667648315429688:  37%|████▊        | 56/151 [00:09<00:19,  4.95it/s]Epoch: 5, train for the 57-th batch, train loss: 0.5667648315429688:  38%|████▉        | 57/151 [00:09<00:17,  5.46it/s]Epoch: 3, train for the 54-th batch, train loss: 0.5228900909423828:  22%|██▉          | 53/237 [00:08<00:33,  5.45it/s]Epoch: 3, train for the 54-th batch, train loss: 0.5228900909423828:  23%|██▉          | 54/237 [00:08<00:32,  5.64it/s]evaluate for the 29-th batch, evaluate loss: 0.4441232979297638:  36%|██████▍           | 28/78 [00:05<00:09,  5.33it/s]evaluate for the 29-th batch, evaluate loss: 0.4441232979297638:  37%|██████▋           | 29/78 [00:05<00:08,  5.53it/s]Epoch: 6, train for the 90-th batch, train loss: 0.5294279456138611:  61%|███████▉     | 89/146 [00:12<00:09,  6.16it/s]Epoch: 6, train for the 90-th batch, train loss: 0.5294279456138611:  62%|████████     | 90/146 [00:12<00:09,  6.09it/s]Epoch: 7, train for the 76-th batch, train loss: 0.41899970173835754:  63%|███████▌    | 75/119 [00:12<00:08,  5.32it/s]Epoch: 7, train for the 76-th batch, train loss: 0.41899970173835754:  64%|███████▋    | 76/119 [00:12<00:07,  5.55it/s]Epoch: 5, train for the 58-th batch, train loss: 0.5594578385353088:  38%|████▉        | 57/151 [00:09<00:17,  5.46it/s]Epoch: 3, train for the 55-th batch, train loss: 0.5604276657104492:  23%|██▉          | 54/237 [00:09<00:32,  5.64it/s]  0%|                                                                                            | 0/34 [00:00<?, ?it/s]Epoch: 5, train for the 58-th batch, train loss: 0.5594578385353088:  38%|████▉        | 58/151 [00:09<00:18,  5.08it/s]evaluate for the 30-th batch, evaluate loss: 0.6424517631530762:  37%|██████▋           | 29/78 [00:05<00:08,  5.53it/s]evaluate for the 30-th batch, evaluate loss: 0.6424517631530762:  38%|██████▉           | 30/78 [00:05<00:08,  5.73it/s]Epoch: 3, train for the 55-th batch, train loss: 0.5604276657104492:  23%|███          | 55/237 [00:09<00:31,  5.72it/s]Epoch: 6, train for the 91-th batch, train loss: 0.5473478436470032:  62%|████████     | 90/146 [00:13<00:09,  6.09it/s]Epoch: 7, train for the 77-th batch, train loss: 0.447914719581604:  64%|████████▉     | 76/119 [00:12<00:07,  5.55it/s]Epoch: 6, train for the 91-th batch, train loss: 0.5473478436470032:  62%|████████     | 91/146 [00:13<00:09,  6.00it/s]Epoch: 7, train for the 77-th batch, train loss: 0.447914719581604:  65%|█████████     | 77/119 [00:12<00:07,  5.73it/s]evaluate for the 1-th batch, evaluate loss: 0.6069936752319336:   0%|                            | 0/34 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.6746472716331482:   0%|                            | 0/34 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.6746472716331482:   6%|█▏                  | 2/34 [00:00<00:02, 10.71it/s]Epoch: 5, train for the 59-th batch, train loss: 0.5383036136627197:  38%|████▉        | 58/151 [00:09<00:18,  5.08it/s]evaluate for the 31-th batch, evaluate loss: 0.42518165707588196:  38%|██████▌          | 30/78 [00:05<00:08,  5.73it/s]evaluate for the 31-th batch, evaluate loss: 0.42518165707588196:  40%|██████▊          | 31/78 [00:05<00:08,  5.56it/s]Epoch: 3, train for the 56-th batch, train loss: 0.5593511462211609:  23%|███          | 55/237 [00:09<00:31,  5.72it/s]Epoch: 6, train for the 92-th batch, train loss: 0.5306969881057739:  62%|████████     | 91/146 [00:13<00:09,  6.00it/s]Epoch: 6, train for the 92-th batch, train loss: 0.5306969881057739:  63%|████████▏    | 92/146 [00:13<00:09,  5.91it/s]Epoch: 3, train for the 56-th batch, train loss: 0.5593511462211609:  24%|███          | 56/237 [00:09<00:33,  5.40it/s]Epoch: 5, train for the 59-th batch, train loss: 0.5383036136627197:  39%|█████        | 59/151 [00:09<00:18,  4.92it/s]Epoch: 7, train for the 78-th batch, train loss: 0.48760321736335754:  65%|███████▊    | 77/119 [00:12<00:07,  5.73it/s]Epoch: 7, train for the 78-th batch, train loss: 0.48760321736335754:  66%|███████▊    | 78/119 [00:12<00:07,  5.47it/s]evaluate for the 3-th batch, evaluate loss: 0.6749072074890137:   6%|█▏                  | 2/34 [00:00<00:02, 10.71it/s]Epoch: 6, train for the 93-th batch, train loss: 0.5265198349952698:  63%|████████▏    | 92/146 [00:13<00:09,  5.91it/s]Epoch: 6, train for the 93-th batch, train loss: 0.5265198349952698:  64%|████████▎    | 93/146 [00:13<00:08,  5.96it/s]Epoch: 5, train for the 60-th batch, train loss: 0.5751843452453613:  39%|█████        | 59/151 [00:10<00:18,  4.92it/s]Epoch: 3, train for the 57-th batch, train loss: 0.5425273180007935:  24%|███          | 56/237 [00:09<00:33,  5.40it/s]evaluate for the 4-th batch, evaluate loss: 0.6590614914894104:   6%|█▏                  | 2/34 [00:00<00:02, 10.71it/s]evaluate for the 4-th batch, evaluate loss: 0.6590614914894104:  12%|██▎                 | 4/34 [00:00<00:03,  9.81it/s]Epoch: 5, train for the 60-th batch, train loss: 0.5751843452453613:  40%|█████▏       | 60/151 [00:10<00:18,  5.04it/s]evaluate for the 32-th batch, evaluate loss: 0.38433554768562317:  40%|██████▊          | 31/78 [00:05<00:08,  5.56it/s]evaluate for the 32-th batch, evaluate loss: 0.38433554768562317:  41%|██████▉          | 32/78 [00:05<00:08,  5.30it/s]Epoch: 3, train for the 57-th batch, train loss: 0.5425273180007935:  24%|███▏         | 57/237 [00:09<00:33,  5.32it/s]Epoch: 7, train for the 79-th batch, train loss: 0.4534507393836975:  66%|████████▌    | 78/119 [00:12<00:07,  5.47it/s]Epoch: 7, train for the 79-th batch, train loss: 0.4534507393836975:  66%|████████▋    | 79/119 [00:12<00:07,  5.50it/s]evaluate for the 5-th batch, evaluate loss: 0.6397947669029236:  12%|██▎                 | 4/34 [00:00<00:03,  9.81it/s]Epoch: 7, train for the 80-th batch, train loss: 0.43894973397254944:  66%|███████▉    | 79/119 [00:12<00:07,  5.50it/s]Epoch: 7, train for the 80-th batch, train loss: 0.43894973397254944:  67%|████████    | 80/119 [00:12<00:06,  5.96it/s]Epoch: 5, train for the 61-th batch, train loss: 0.5417380928993225:  40%|█████▏       | 60/151 [00:10<00:18,  5.04it/s]evaluate for the 6-th batch, evaluate loss: 0.6070125102996826:  12%|██▎                 | 4/34 [00:00<00:03,  9.81it/s]evaluate for the 6-th batch, evaluate loss: 0.6070125102996826:  18%|███▌                | 6/34 [00:00<00:02, 10.26it/s]Epoch: 5, train for the 61-th batch, train loss: 0.5417380928993225:  40%|█████▎       | 61/151 [00:10<00:17,  5.05it/s]Epoch: 3, train for the 58-th batch, train loss: 0.5874924063682556:  24%|███▏         | 57/237 [00:09<00:33,  5.32it/s]evaluate for the 33-th batch, evaluate loss: 0.5244417786598206:  41%|███████▍          | 32/78 [00:06<00:08,  5.30it/s]evaluate for the 33-th batch, evaluate loss: 0.5244417786598206:  42%|███████▌          | 33/78 [00:06<00:08,  5.02it/s]Epoch: 6, train for the 94-th batch, train loss: 0.5267563462257385:  64%|████████▎    | 93/146 [00:13<00:08,  5.96it/s]evaluate for the 7-th batch, evaluate loss: 0.5677083134651184:  18%|███▌                | 6/34 [00:00<00:02, 10.26it/s]Epoch: 6, train for the 94-th batch, train loss: 0.5267563462257385:  64%|████████▎    | 94/146 [00:13<00:10,  5.06it/s]Epoch: 3, train for the 58-th batch, train loss: 0.5874924063682556:  24%|███▏         | 58/237 [00:09<00:36,  4.94it/s]Epoch: 7, train for the 81-th batch, train loss: 0.41967108845710754:  67%|████████    | 80/119 [00:12<00:06,  5.96it/s]Epoch: 7, train for the 81-th batch, train loss: 0.41967108845710754:  68%|████████▏   | 81/119 [00:12<00:05,  6.46it/s]Epoch: 5, train for the 62-th batch, train loss: 0.5928516983985901:  40%|█████▎       | 61/151 [00:10<00:17,  5.05it/s]evaluate for the 8-th batch, evaluate loss: 0.6729521155357361:  18%|███▌                | 6/34 [00:00<00:02, 10.26it/s]evaluate for the 8-th batch, evaluate loss: 0.6729521155357361:  24%|████▋               | 8/34 [00:00<00:02, 10.83it/s]Epoch: 5, train for the 62-th batch, train loss: 0.5928516983985901:  41%|█████▎       | 62/151 [00:10<00:16,  5.24it/s]Epoch: 6, train for the 95-th batch, train loss: 0.47509336471557617:  64%|███████▋    | 94/146 [00:13<00:10,  5.06it/s]Epoch: 6, train for the 95-th batch, train loss: 0.47509336471557617:  65%|███████▊    | 95/146 [00:13<00:09,  5.55it/s]Epoch: 7, train for the 82-th batch, train loss: 0.4659431278705597:  68%|████████▊    | 81/119 [00:12<00:05,  6.46it/s]evaluate for the 9-th batch, evaluate loss: 0.5832058787345886:  24%|████▋               | 8/34 [00:00<00:02, 10.83it/s]Epoch: 3, train for the 59-th batch, train loss: 0.5912290215492249:  24%|███▏         | 58/237 [00:09<00:36,  4.94it/s]evaluate for the 34-th batch, evaluate loss: 0.4383556544780731:  42%|███████▌          | 33/78 [00:06<00:08,  5.02it/s]evaluate for the 34-th batch, evaluate loss: 0.4383556544780731:  44%|███████▊          | 34/78 [00:06<00:08,  4.94it/s]Epoch: 7, train for the 82-th batch, train loss: 0.4659431278705597:  69%|████████▉    | 82/119 [00:12<00:05,  6.47it/s]Epoch: 3, train for the 59-th batch, train loss: 0.5912290215492249:  25%|███▏         | 59/237 [00:10<00:36,  4.93it/s]Epoch: 5, train for the 63-th batch, train loss: 0.55598384141922:  41%|██████▏        | 62/151 [00:10<00:16,  5.24it/s]evaluate for the 10-th batch, evaluate loss: 0.6364025473594666:  24%|████▍              | 8/34 [00:00<00:02, 10.83it/s]evaluate for the 10-th batch, evaluate loss: 0.6364025473594666:  29%|█████▎            | 10/34 [00:00<00:02, 11.28it/s]Epoch: 6, train for the 96-th batch, train loss: 0.47530651092529297:  65%|███████▊    | 95/146 [00:14<00:09,  5.55it/s]Epoch: 5, train for the 63-th batch, train loss: 0.55598384141922:  42%|██████▎        | 63/151 [00:10<00:15,  5.59it/s]Epoch: 6, train for the 96-th batch, train loss: 0.47530651092529297:  66%|███████▉    | 96/146 [00:14<00:08,  5.92it/s]evaluate for the 11-th batch, evaluate loss: 0.6638819575309753:  29%|█████▎            | 10/34 [00:00<00:02, 11.28it/s]Epoch: 7, train for the 83-th batch, train loss: 0.4928450286388397:  69%|████████▉    | 82/119 [00:13<00:05,  6.47it/s]Epoch: 7, train for the 83-th batch, train loss: 0.4928450286388397:  70%|█████████    | 83/119 [00:13<00:05,  6.34it/s]Epoch: 3, train for the 60-th batch, train loss: 0.5364719033241272:  25%|███▏         | 59/237 [00:10<00:36,  4.93it/s]evaluate for the 35-th batch, evaluate loss: 0.474841445684433:  44%|████████▎          | 34/78 [00:06<00:08,  4.94it/s]evaluate for the 35-th batch, evaluate loss: 0.474841445684433:  45%|████████▌          | 35/78 [00:06<00:08,  5.03it/s]evaluate for the 12-th batch, evaluate loss: 0.6211556196212769:  29%|█████▎            | 10/34 [00:01<00:02, 11.28it/s]evaluate for the 12-th batch, evaluate loss: 0.6211556196212769:  35%|██████▎           | 12/34 [00:01<00:01, 12.55it/s]Epoch: 3, train for the 60-th batch, train loss: 0.5364719033241272:  25%|███▎         | 60/237 [00:10<00:35,  4.94it/s]Epoch: 5, train for the 64-th batch, train loss: 0.5338739156723022:  42%|█████▍       | 63/151 [00:10<00:15,  5.59it/s]Epoch: 5, train for the 64-th batch, train loss: 0.5338739156723022:  42%|█████▌       | 64/151 [00:10<00:15,  5.60it/s]evaluate for the 13-th batch, evaluate loss: 0.5542492270469666:  35%|██████▎           | 12/34 [00:01<00:01, 12.55it/s]Epoch: 6, train for the 97-th batch, train loss: 0.5368531942367554:  66%|████████▌    | 96/146 [00:14<00:08,  5.92it/s]Epoch: 7, train for the 84-th batch, train loss: 0.451964795589447:  70%|█████████▊    | 83/119 [00:13<00:05,  6.34it/s]Epoch: 6, train for the 97-th batch, train loss: 0.5368531942367554:  66%|████████▋    | 97/146 [00:14<00:08,  5.53it/s]Epoch: 7, train for the 84-th batch, train loss: 0.451964795589447:  71%|█████████▉    | 84/119 [00:13<00:05,  6.56it/s]evaluate for the 14-th batch, evaluate loss: 0.5438799262046814:  35%|██████▎           | 12/34 [00:01<00:01, 12.55it/s]evaluate for the 14-th batch, evaluate loss: 0.5438799262046814:  41%|███████▍          | 14/34 [00:01<00:01, 13.25it/s]Epoch: 3, train for the 61-th batch, train loss: 0.6101758480072021:  25%|███▎         | 60/237 [00:10<00:35,  4.94it/s]evaluate for the 36-th batch, evaluate loss: 0.6314215660095215:  45%|████████          | 35/78 [00:06<00:08,  5.03it/s]evaluate for the 36-th batch, evaluate loss: 0.6314215660095215:  46%|████████▎         | 36/78 [00:06<00:08,  5.04it/s]Epoch: 3, train for the 61-th batch, train loss: 0.6101758480072021:  26%|███▎         | 61/237 [00:10<00:34,  5.06it/s]evaluate for the 15-th batch, evaluate loss: 0.6879907250404358:  41%|███████▍          | 14/34 [00:01<00:01, 13.25it/s]Epoch: 6, train for the 98-th batch, train loss: 0.5147560834884644:  66%|████████▋    | 97/146 [00:14<00:08,  5.53it/s]Epoch: 5, train for the 65-th batch, train loss: 0.5025115609169006:  42%|█████▌       | 64/151 [00:11<00:15,  5.60it/s]Epoch: 6, train for the 98-th batch, train loss: 0.5147560834884644:  67%|████████▋    | 98/146 [00:14<00:08,  5.57it/s]Epoch: 7, train for the 85-th batch, train loss: 0.46321889758110046:  71%|████████▍   | 84/119 [00:13<00:05,  6.56it/s]Epoch: 5, train for the 65-th batch, train loss: 0.5025115609169006:  43%|█████▌       | 65/151 [00:11<00:16,  5.22it/s]Epoch: 7, train for the 85-th batch, train loss: 0.46321889758110046:  71%|████████▌   | 85/119 [00:13<00:05,  6.03it/s]evaluate for the 16-th batch, evaluate loss: 0.6055759191513062:  41%|███████▍          | 14/34 [00:01<00:01, 13.25it/s]evaluate for the 16-th batch, evaluate loss: 0.6055759191513062:  47%|████████▍         | 16/34 [00:01<00:01, 12.33it/s]Epoch: 3, train for the 62-th batch, train loss: 0.5634922385215759:  26%|███▎         | 61/237 [00:10<00:34,  5.06it/s]evaluate for the 17-th batch, evaluate loss: 0.6296898722648621:  47%|████████▍         | 16/34 [00:01<00:01, 12.33it/s]evaluate for the 37-th batch, evaluate loss: 0.5657721757888794:  46%|████████▎         | 36/78 [00:06<00:08,  5.04it/s]evaluate for the 37-th batch, evaluate loss: 0.5657721757888794:  47%|████████▌         | 37/78 [00:06<00:08,  5.00it/s]Epoch: 3, train for the 62-th batch, train loss: 0.5634922385215759:  26%|███▍         | 62/237 [00:10<00:35,  4.98it/s]Epoch: 6, train for the 99-th batch, train loss: 0.5540675520896912:  67%|████████▋    | 98/146 [00:14<00:08,  5.57it/s]Epoch: 6, train for the 99-th batch, train loss: 0.5540675520896912:  68%|████████▊    | 99/146 [00:14<00:08,  5.77it/s]Epoch: 5, train for the 66-th batch, train loss: 0.5734890699386597:  43%|█████▌       | 65/151 [00:11<00:16,  5.22it/s]evaluate for the 18-th batch, evaluate loss: 0.5829328894615173:  47%|████████▍         | 16/34 [00:01<00:01, 12.33it/s]evaluate for the 18-th batch, evaluate loss: 0.5829328894615173:  53%|█████████▌        | 18/34 [00:01<00:01, 12.61it/s]Epoch: 5, train for the 66-th batch, train loss: 0.5734890699386597:  44%|█████▋       | 66/151 [00:11<00:16,  5.18it/s]Epoch: 7, train for the 86-th batch, train loss: 0.43180742859840393:  71%|████████▌   | 85/119 [00:13<00:05,  6.03it/s]Epoch: 7, train for the 86-th batch, train loss: 0.43180742859840393:  72%|████████▋   | 86/119 [00:13<00:05,  5.78it/s]Epoch: 6, train for the 100-th batch, train loss: 0.5538195967674255:  68%|████████▏   | 99/146 [00:14<00:08,  5.77it/s]evaluate for the 19-th batch, evaluate loss: 0.5479520559310913:  53%|█████████▌        | 18/34 [00:01<00:01, 12.61it/s]Epoch: 6, train for the 100-th batch, train loss: 0.5538195967674255:  68%|███████▌   | 100/146 [00:14<00:07,  6.22it/s]evaluate for the 38-th batch, evaluate loss: 0.47002676129341125:  47%|████████         | 37/78 [00:07<00:08,  5.00it/s]evaluate for the 38-th batch, evaluate loss: 0.47002676129341125:  49%|████████▎        | 38/78 [00:07<00:07,  5.07it/s]Epoch: 3, train for the 63-th batch, train loss: 0.6048469543457031:  26%|███▍         | 62/237 [00:10<00:35,  4.98it/s]Epoch: 3, train for the 63-th batch, train loss: 0.6048469543457031:  27%|███▍         | 63/237 [00:10<00:34,  4.98it/s]Epoch: 5, train for the 67-th batch, train loss: 0.5551086068153381:  44%|█████▋       | 66/151 [00:11<00:16,  5.18it/s]evaluate for the 20-th batch, evaluate loss: 0.6250032186508179:  53%|█████████▌        | 18/34 [00:01<00:01, 12.61it/s]evaluate for the 20-th batch, evaluate loss: 0.6250032186508179:  59%|██████████▌       | 20/34 [00:01<00:01, 12.02it/s]Epoch: 7, train for the 87-th batch, train loss: 0.4511122703552246:  72%|█████████▍   | 86/119 [00:13<00:05,  5.78it/s]Epoch: 5, train for the 67-th batch, train loss: 0.5551086068153381:  44%|█████▊       | 67/151 [00:11<00:16,  5.23it/s]Epoch: 7, train for the 87-th batch, train loss: 0.4511122703552246:  73%|█████████▌   | 87/119 [00:13<00:05,  5.72it/s]Epoch: 6, train for the 101-th batch, train loss: 0.48940420150756836:  68%|██████▊   | 100/146 [00:14<00:07,  6.22it/s]Epoch: 6, train for the 101-th batch, train loss: 0.48940420150756836:  69%|██████▉   | 101/146 [00:14<00:07,  6.20it/s]evaluate for the 21-th batch, evaluate loss: 0.628447413444519:  59%|███████████▏       | 20/34 [00:01<00:01, 12.02it/s]evaluate for the 39-th batch, evaluate loss: 0.5249791741371155:  49%|████████▊         | 38/78 [00:07<00:07,  5.07it/s]evaluate for the 39-th batch, evaluate loss: 0.5249791741371155:  50%|█████████         | 39/78 [00:07<00:07,  5.29it/s]Epoch: 3, train for the 64-th batch, train loss: 0.5588830709457397:  27%|███▍         | 63/237 [00:10<00:34,  4.98it/s]Epoch: 3, train for the 64-th batch, train loss: 0.5588830709457397:  27%|███▌         | 64/237 [00:11<00:35,  4.89it/s]evaluate for the 22-th batch, evaluate loss: 0.3570897579193115:  59%|██████████▌       | 20/34 [00:01<00:01, 12.02it/s]evaluate for the 22-th batch, evaluate loss: 0.3570897579193115:  65%|███████████▋      | 22/34 [00:01<00:01, 11.89it/s]Epoch: 5, train for the 68-th batch, train loss: 0.5736871361732483:  44%|█████▊       | 67/151 [00:11<00:16,  5.23it/s]Epoch: 7, train for the 88-th batch, train loss: 0.46132856607437134:  73%|████████▊   | 87/119 [00:14<00:05,  5.72it/s]Epoch: 5, train for the 68-th batch, train loss: 0.5736871361732483:  45%|█████▊       | 68/151 [00:11<00:16,  5.10it/s]Epoch: 7, train for the 88-th batch, train loss: 0.46132856607437134:  74%|████████▊   | 88/119 [00:14<00:05,  5.37it/s]evaluate for the 23-th batch, evaluate loss: 0.32644638419151306:  65%|███████████      | 22/34 [00:01<00:01, 11.89it/s]Epoch: 6, train for the 102-th batch, train loss: 0.5119609832763672:  69%|███████▌   | 101/146 [00:15<00:07,  6.20it/s]Epoch: 6, train for the 102-th batch, train loss: 0.5119609832763672:  70%|███████▋   | 102/146 [00:15<00:07,  5.91it/s]evaluate for the 24-th batch, evaluate loss: 0.5111312866210938:  65%|███████████▋      | 22/34 [00:02<00:01, 11.89it/s]evaluate for the 24-th batch, evaluate loss: 0.5111312866210938:  71%|████████████▋     | 24/34 [00:02<00:00, 12.38it/s]evaluate for the 40-th batch, evaluate loss: 0.42421820759773254:  50%|████████▌        | 39/78 [00:07<00:07,  5.29it/s]evaluate for the 40-th batch, evaluate loss: 0.42421820759773254:  51%|████████▋        | 40/78 [00:07<00:07,  4.99it/s]Epoch: 3, train for the 65-th batch, train loss: 0.5277813673019409:  27%|███▌         | 64/237 [00:11<00:35,  4.89it/s]Epoch: 3, train for the 65-th batch, train loss: 0.5277813673019409:  27%|███▌         | 65/237 [00:11<00:34,  5.02it/s]Epoch: 5, train for the 69-th batch, train loss: 0.5425401926040649:  45%|█████▊       | 68/151 [00:11<00:16,  5.10it/s]evaluate for the 25-th batch, evaluate loss: 0.5058857798576355:  71%|████████████▋     | 24/34 [00:02<00:00, 12.38it/s]Epoch: 5, train for the 69-th batch, train loss: 0.5425401926040649:  46%|█████▉       | 69/151 [00:11<00:15,  5.25it/s]Epoch: 7, train for the 89-th batch, train loss: 0.4286975860595703:  74%|█████████▌   | 88/119 [00:14<00:05,  5.37it/s]Epoch: 7, train for the 89-th batch, train loss: 0.4286975860595703:  75%|█████████▋   | 89/119 [00:14<00:05,  5.35it/s]Epoch: 6, train for the 103-th batch, train loss: 0.49141258001327515:  70%|██████▉   | 102/146 [00:15<00:07,  5.91it/s]Epoch: 6, train for the 103-th batch, train loss: 0.49141258001327515:  71%|███████   | 103/146 [00:15<00:07,  5.70it/s]evaluate for the 26-th batch, evaluate loss: 0.6044480204582214:  71%|████████████▋     | 24/34 [00:02<00:00, 12.38it/s]evaluate for the 26-th batch, evaluate loss: 0.6044480204582214:  76%|█████████████▊    | 26/34 [00:02<00:00, 12.50it/s]Epoch: 3, train for the 66-th batch, train loss: 0.5719656944274902:  27%|███▌         | 65/237 [00:11<00:34,  5.02it/s]Epoch: 3, train for the 66-th batch, train loss: 0.5719656944274902:  28%|███▌         | 66/237 [00:11<00:31,  5.34it/s]evaluate for the 41-th batch, evaluate loss: 0.4128633737564087:  51%|█████████▏        | 40/78 [00:07<00:07,  4.99it/s]evaluate for the 41-th batch, evaluate loss: 0.4128633737564087:  53%|█████████▍        | 41/78 [00:07<00:07,  5.03it/s]Epoch: 7, train for the 90-th batch, train loss: 0.45121100544929504:  75%|████████▉   | 89/119 [00:14<00:05,  5.35it/s]Epoch: 7, train for the 90-th batch, train loss: 0.45121100544929504:  76%|█████████   | 90/119 [00:14<00:05,  5.77it/s]Epoch: 5, train for the 70-th batch, train loss: 0.544554591178894:  46%|██████▍       | 69/151 [00:11<00:15,  5.25it/s]evaluate for the 27-th batch, evaluate loss: 0.6281431913375854:  76%|█████████████▊    | 26/34 [00:02<00:00, 12.50it/s]Epoch: 6, train for the 104-th batch, train loss: 0.4960726797580719:  71%|███████▊   | 103/146 [00:15<00:07,  5.70it/s]Epoch: 5, train for the 70-th batch, train loss: 0.544554591178894:  46%|██████▍       | 70/151 [00:12<00:16,  5.06it/s]Epoch: 6, train for the 104-th batch, train loss: 0.4960726797580719:  71%|███████▊   | 104/146 [00:15<00:07,  5.77it/s]evaluate for the 28-th batch, evaluate loss: 0.554267942905426:  76%|██████████████▌    | 26/34 [00:02<00:00, 12.50it/s]evaluate for the 28-th batch, evaluate loss: 0.554267942905426:  82%|███████████████▋   | 28/34 [00:02<00:00, 11.89it/s]Epoch: 3, train for the 67-th batch, train loss: 0.5564737319946289:  28%|███▌         | 66/237 [00:11<00:31,  5.34it/s]Epoch: 7, train for the 91-th batch, train loss: 0.4135023355484009:  76%|█████████▊   | 90/119 [00:14<00:05,  5.77it/s]Epoch: 3, train for the 67-th batch, train loss: 0.5564737319946289:  28%|███▋         | 67/237 [00:11<00:31,  5.36it/s]Epoch: 7, train for the 91-th batch, train loss: 0.4135023355484009:  76%|█████████▉   | 91/119 [00:14<00:04,  6.08it/s]evaluate for the 29-th batch, evaluate loss: 0.5530346632003784:  82%|██████████████▊   | 28/34 [00:02<00:00, 11.89it/s]Epoch: 6, train for the 105-th batch, train loss: 0.5326763987541199:  71%|███████▊   | 104/146 [00:15<00:07,  5.77it/s]Epoch: 6, train for the 105-th batch, train loss: 0.5326763987541199:  72%|███████▉   | 105/146 [00:15<00:07,  5.85it/s]Epoch: 5, train for the 71-th batch, train loss: 0.5691595673561096:  46%|██████       | 70/151 [00:12<00:16,  5.06it/s]Epoch: 5, train for the 71-th batch, train loss: 0.5691595673561096:  47%|██████       | 71/151 [00:12<00:15,  5.06it/s]evaluate for the 30-th batch, evaluate loss: 0.5350494384765625:  82%|██████████████▊   | 28/34 [00:02<00:00, 11.89it/s]evaluate for the 30-th batch, evaluate loss: 0.5350494384765625:  88%|███████████████▉  | 30/34 [00:02<00:00, 12.26it/s]Epoch: 7, train for the 92-th batch, train loss: 0.4792102575302124:  76%|█████████▉   | 91/119 [00:14<00:04,  6.08it/s]Epoch: 7, train for the 92-th batch, train loss: 0.4792102575302124:  77%|██████████   | 92/119 [00:14<00:04,  6.23it/s]evaluate for the 31-th batch, evaluate loss: 0.6245924830436707:  88%|███████████████▉  | 30/34 [00:02<00:00, 12.26it/s]Epoch: 6, train for the 106-th batch, train loss: 0.5322402715682983:  72%|███████▉   | 105/146 [00:15<00:07,  5.85it/s]Epoch: 6, train for the 106-th batch, train loss: 0.5322402715682983:  73%|███████▉   | 106/146 [00:15<00:06,  6.07it/s]evaluate for the 42-th batch, evaluate loss: 0.49001985788345337:  53%|████████▉        | 41/78 [00:08<00:07,  5.03it/s]evaluate for the 42-th batch, evaluate loss: 0.49001985788345337:  54%|█████████▏       | 42/78 [00:08<00:09,  3.79it/s]Epoch: 5, train for the 72-th batch, train loss: 0.56124347448349:  47%|███████        | 71/151 [00:12<00:15,  5.06it/s]evaluate for the 32-th batch, evaluate loss: 0.6143057942390442:  88%|███████████████▉  | 30/34 [00:02<00:00, 12.26it/s]evaluate for the 32-th batch, evaluate loss: 0.6143057942390442:  94%|████████████████▉ | 32/34 [00:02<00:00, 12.43it/s]Epoch: 5, train for the 72-th batch, train loss: 0.56124347448349:  48%|███████▏       | 72/151 [00:12<00:14,  5.34it/s]Epoch: 3, train for the 68-th batch, train loss: 0.5000291466712952:  28%|███▋         | 67/237 [00:11<00:31,  5.36it/s]Epoch: 3, train for the 68-th batch, train loss: 0.5000291466712952:  29%|███▋         | 68/237 [00:11<00:37,  4.47it/s]Epoch: 6, train for the 107-th batch, train loss: 0.4765441119670868:  73%|███████▉   | 106/146 [00:15<00:06,  6.07it/s]evaluate for the 33-th batch, evaluate loss: 0.6429672241210938:  94%|████████████████▉ | 32/34 [00:02<00:00, 12.43it/s]Epoch: 6, train for the 107-th batch, train loss: 0.4765441119670868:  73%|████████   | 107/146 [00:15<00:05,  6.63it/s]Epoch: 7, train for the 93-th batch, train loss: 0.37361472845077515:  77%|█████████▎  | 92/119 [00:14<00:04,  6.23it/s]Epoch: 7, train for the 93-th batch, train loss: 0.37361472845077515:  78%|█████████▍  | 93/119 [00:14<00:04,  5.47it/s]Epoch: 5, train for the 73-th batch, train loss: 0.543255090713501:  48%|██████▋       | 72/151 [00:12<00:14,  5.34it/s]evaluate for the 43-th batch, evaluate loss: 0.5036255717277527:  54%|█████████▋        | 42/78 [00:08<00:09,  3.79it/s]evaluate for the 43-th batch, evaluate loss: 0.5036255717277527:  55%|█████████▉        | 43/78 [00:08<00:08,  4.17it/s]Epoch: 5, train for the 73-th batch, train loss: 0.543255090713501:  48%|██████▊       | 73/151 [00:12<00:13,  5.66it/s]evaluate for the 34-th batch, evaluate loss: 0.681598961353302:  94%|█████████████████▉ | 32/34 [00:02<00:00, 12.43it/s]evaluate for the 34-th batch, evaluate loss: 0.681598961353302: 100%|███████████████████| 34/34 [00:02<00:00, 12.38it/s]evaluate for the 34-th batch, evaluate loss: 0.681598961353302: 100%|███████████████████| 34/34 [00:02<00:00, 12.01it/s]
Epoch: 3, train for the 69-th batch, train loss: 0.5417200326919556:  29%|███▋         | 68/237 [00:12<00:37,  4.47it/s]Epoch: 3, train for the 69-th batch, train loss: 0.5417200326919556:  29%|███▊         | 69/237 [00:12<00:34,  4.84it/s]Epoch: 6, train for the 108-th batch, train loss: 0.5132988691329956:  73%|████████   | 107/146 [00:16<00:05,  6.63it/s]Epoch: 6, train for the 108-th batch, train loss: 0.5132988691329956:  74%|████████▏  | 108/146 [00:16<00:05,  6.34it/s]Epoch: 7, train for the 94-th batch, train loss: 0.41311460733413696:  78%|█████████▍  | 93/119 [00:15<00:04,  5.47it/s]Epoch: 7, train for the 94-th batch, train loss: 0.41311460733413696:  79%|█████████▍  | 94/119 [00:15<00:04,  5.59it/s]Epoch: 5, train for the 74-th batch, train loss: 0.5302670001983643:  48%|██████▎      | 73/151 [00:12<00:13,  5.66it/s]evaluate for the 44-th batch, evaluate loss: 0.38009825348854065:  55%|█████████▎       | 43/78 [00:08<00:08,  4.17it/s]evaluate for the 44-th batch, evaluate loss: 0.38009825348854065:  56%|█████████▌       | 44/78 [00:08<00:07,  4.46it/s]Epoch: 3, train for the 70-th batch, train loss: 0.5566242933273315:  29%|███▊         | 69/237 [00:12<00:34,  4.84it/s]Epoch: 5, train for the 74-th batch, train loss: 0.5302670001983643:  49%|██████▎      | 74/151 [00:12<00:13,  5.52it/s]Epoch: 3, train for the 70-th batch, train loss: 0.5566242933273315:  30%|███▊         | 70/237 [00:12<00:32,  5.06it/s]Epoch: 6, train for the 109-th batch, train loss: 0.5143277049064636:  74%|████████▏  | 108/146 [00:16<00:05,  6.34it/s]Epoch: 6, train for the 109-th batch, train loss: 0.5143277049064636:  75%|████████▏  | 109/146 [00:16<00:05,  6.62it/s]Epoch: 7, train for the 95-th batch, train loss: 0.38233786821365356:  79%|█████████▍  | 94/119 [00:15<00:04,  5.59it/s]Epoch: 7, train for the 95-th batch, train loss: 0.38233786821365356:  80%|█████████▌  | 95/119 [00:15<00:04,  5.96it/s]Epoch: 5, train for the 75-th batch, train loss: 0.5449933409690857:  49%|██████▎      | 74/151 [00:12<00:13,  5.52it/s]Epoch: 5, train for the 75-th batch, train loss: 0.5449933409690857:  50%|██████▍      | 75/151 [00:12<00:12,  5.85it/s]evaluate for the 45-th batch, evaluate loss: 0.5561171174049377:  56%|██████████▏       | 44/78 [00:08<00:07,  4.46it/s]evaluate for the 45-th batch, evaluate loss: 0.5561171174049377:  58%|██████████▍       | 45/78 [00:08<00:06,  4.81it/s]Epoch: 6, train for the 110-th batch, train loss: 0.48069968819618225:  75%|███████▍  | 109/146 [00:16<00:05,  6.62it/s]Epoch: 6, train for the 110-th batch, train loss: 0.48069968819618225:  75%|███████▌  | 110/146 [00:16<00:05,  6.59it/s]INFO:root:Epoch: 3, learning rate: 0.0001, train loss: 0.4752
INFO:root:train average_precision, 0.8600
INFO:root:train roc_auc, 0.8312
INFO:root:validate loss: 0.4826
INFO:root:validate average_precision, 0.8688
INFO:root:validate roc_auc, 0.8525
INFO:root:new node validate loss: 0.5927
INFO:root:new node validate first_1_average_precision, 0.5264
INFO:root:new node validate first_1_roc_auc, 0.5291
INFO:root:new node validate first_3_average_precision, 0.6032
INFO:root:new node validate first_3_roc_auc, 0.5949
Epoch: 3, train for the 71-th batch, train loss: 0.5292002558708191:  30%|███▊         | 70/237 [00:12<00:32,  5.06it/s]INFO:root:new node validate first_10_average_precision, 0.6919
INFO:root:new node validate first_10_roc_auc, 0.6747
INFO:root:new node validate average_precision, 0.7569
INFO:root:new node validate roc_auc, 0.7293
Epoch: 7, train for the 96-th batch, train loss: 0.4004785418510437:  80%|██████████▍  | 95/119 [00:15<00:04,  5.96it/s]Epoch: 7, train for the 96-th batch, train loss: 0.4004785418510437:  81%|██████████▍  | 96/119 [00:15<00:03,  6.16it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/241 [00:00<?, ?it/s]Epoch: 3, train for the 71-th batch, train loss: 0.5292002558708191:  30%|███▉         | 71/237 [00:12<00:33,  4.97it/s]Epoch: 5, train for the 76-th batch, train loss: 0.5897666811943054:  50%|██████▍      | 75/151 [00:12<00:12,  5.85it/s]Epoch: 5, train for the 76-th batch, train loss: 0.5897666811943054:  50%|██████▌      | 76/151 [00:13<00:12,  6.16it/s]Epoch: 6, train for the 111-th batch, train loss: 0.5041080713272095:  75%|████████▎  | 110/146 [00:16<00:05,  6.59it/s]Epoch: 6, train for the 111-th batch, train loss: 0.5041080713272095:  76%|████████▎  | 111/146 [00:16<00:05,  6.93it/s]evaluate for the 46-th batch, evaluate loss: 0.54175865650177:  58%|███████████▌        | 45/78 [00:08<00:06,  4.81it/s]evaluate for the 46-th batch, evaluate loss: 0.54175865650177:  59%|███████████▊        | 46/78 [00:08<00:06,  5.07it/s]Epoch: 4, train for the 1-th batch, train loss: 0.6055124402046204:   0%|                       | 0/241 [00:00<?, ?it/s]Epoch: 7, train for the 97-th batch, train loss: 0.41435015201568604:  81%|█████████▋  | 96/119 [00:15<00:03,  6.16it/s]Epoch: 3, train for the 72-th batch, train loss: 0.5521054863929749:  30%|███▉         | 71/237 [00:12<00:33,  4.97it/s]Epoch: 7, train for the 97-th batch, train loss: 0.41435015201568604:  82%|█████████▊  | 97/119 [00:15<00:03,  6.33it/s]Epoch: 3, train for the 72-th batch, train loss: 0.5521054863929749:  30%|███▉         | 72/237 [00:12<00:30,  5.32it/s]Epoch: 5, train for the 77-th batch, train loss: 0.5230955481529236:  50%|██████▌      | 76/151 [00:13<00:12,  6.16it/s]Epoch: 6, train for the 112-th batch, train loss: 0.5372946262359619:  76%|████████▎  | 111/146 [00:16<00:05,  6.93it/s]Epoch: 4, train for the 2-th batch, train loss: 0.4592166841030121:   0%|                       | 0/241 [00:00<?, ?it/s]Epoch: 6, train for the 112-th batch, train loss: 0.5372946262359619:  77%|████████▍  | 112/146 [00:16<00:04,  7.01it/s]Epoch: 4, train for the 2-th batch, train loss: 0.4592166841030121:   1%|               | 2/241 [00:00<00:26,  9.03it/s]Epoch: 5, train for the 77-th batch, train loss: 0.5230955481529236:  51%|██████▋      | 77/151 [00:13<00:12,  5.99it/s]evaluate for the 47-th batch, evaluate loss: 0.3840962052345276:  59%|██████████▌       | 46/78 [00:09<00:06,  5.07it/s]evaluate for the 47-th batch, evaluate loss: 0.3840962052345276:  60%|██████████▊       | 47/78 [00:09<00:05,  5.27it/s]Epoch: 7, train for the 98-th batch, train loss: 0.4155741035938263:  82%|██████████▌  | 97/119 [00:15<00:03,  6.33it/s]Epoch: 7, train for the 98-th batch, train loss: 0.4155741035938263:  82%|██████████▋  | 98/119 [00:15<00:03,  6.58it/s]Epoch: 3, train for the 73-th batch, train loss: 0.5528228878974915:  30%|███▉         | 72/237 [00:12<00:30,  5.32it/s]Epoch: 3, train for the 73-th batch, train loss: 0.5528228878974915:  31%|████         | 73/237 [00:12<00:29,  5.60it/s]Epoch: 4, train for the 3-th batch, train loss: 0.4983079433441162:   1%|               | 2/241 [00:00<00:26,  9.03it/s]Epoch: 4, train for the 3-th batch, train loss: 0.4983079433441162:   1%|▏              | 3/241 [00:00<00:25,  9.33it/s]Epoch: 6, train for the 113-th batch, train loss: 0.4702407717704773:  77%|████████▍  | 112/146 [00:16<00:04,  7.01it/s]Epoch: 5, train for the 78-th batch, train loss: 0.5084800124168396:  51%|██████▋      | 77/151 [00:13<00:12,  5.99it/s]Epoch: 6, train for the 113-th batch, train loss: 0.4702407717704773:  77%|████████▌  | 113/146 [00:16<00:04,  6.88it/s]Epoch: 5, train for the 78-th batch, train loss: 0.5084800124168396:  52%|██████▋      | 78/151 [00:13<00:12,  6.04it/s]Epoch: 7, train for the 99-th batch, train loss: 0.4452020823955536:  82%|██████████▋  | 98/119 [00:15<00:03,  6.58it/s]Epoch: 7, train for the 99-th batch, train loss: 0.4452020823955536:  83%|██████████▊  | 99/119 [00:15<00:02,  6.87it/s]Epoch: 4, train for the 4-th batch, train loss: 0.45091482996940613:   1%|▏             | 3/241 [00:00<00:25,  9.33it/s]Epoch: 4, train for the 4-th batch, train loss: 0.45091482996940613:   2%|▏             | 4/241 [00:00<00:26,  9.09it/s]evaluate for the 48-th batch, evaluate loss: 0.5077282786369324:  60%|██████████▊       | 47/78 [00:09<00:05,  5.27it/s]evaluate for the 48-th batch, evaluate loss: 0.5077282786369324:  62%|███████████       | 48/78 [00:09<00:05,  5.25it/s]Epoch: 3, train for the 74-th batch, train loss: 0.5208442211151123:  31%|████         | 73/237 [00:12<00:29,  5.60it/s]Epoch: 3, train for the 74-th batch, train loss: 0.5208442211151123:  31%|████         | 74/237 [00:12<00:30,  5.35it/s]Epoch: 5, train for the 79-th batch, train loss: 0.5291611552238464:  52%|██████▋      | 78/151 [00:13<00:12,  6.04it/s]Epoch: 6, train for the 114-th batch, train loss: 0.49016693234443665:  77%|███████▋  | 113/146 [00:16<00:04,  6.88it/s]Epoch: 5, train for the 79-th batch, train loss: 0.5291611552238464:  52%|██████▊      | 79/151 [00:13<00:11,  6.24it/s]Epoch: 4, train for the 5-th batch, train loss: 0.48878416419029236:   2%|▏             | 4/241 [00:00<00:26,  9.09it/s]Epoch: 4, train for the 5-th batch, train loss: 0.48878416419029236:   2%|▎             | 5/241 [00:00<00:25,  9.13it/s]Epoch: 6, train for the 114-th batch, train loss: 0.49016693234443665:  78%|███████▊  | 114/146 [00:16<00:04,  6.48it/s]Epoch: 7, train for the 100-th batch, train loss: 0.4627044200897217:  83%|█████████▉  | 99/119 [00:15<00:02,  6.87it/s]Epoch: 7, train for the 100-th batch, train loss: 0.4627044200897217:  84%|█████████▏ | 100/119 [00:15<00:02,  6.67it/s]evaluate for the 49-th batch, evaluate loss: 0.49614962935447693:  62%|██████████▍      | 48/78 [00:09<00:05,  5.25it/s]evaluate for the 49-th batch, evaluate loss: 0.49614962935447693:  63%|██████████▋      | 49/78 [00:09<00:05,  4.97it/s]Epoch: 4, train for the 6-th batch, train loss: 0.6959566473960876:   2%|▎              | 5/241 [00:00<00:25,  9.13it/s]Epoch: 3, train for the 75-th batch, train loss: 0.5234242081642151:  31%|████         | 74/237 [00:13<00:30,  5.35it/s]Epoch: 4, train for the 6-th batch, train loss: 0.6959566473960876:   2%|▎              | 6/241 [00:00<00:30,  7.76it/s]Epoch: 5, train for the 80-th batch, train loss: 0.49220937490463257:  52%|██████▎     | 79/151 [00:13<00:11,  6.24it/s]Epoch: 6, train for the 115-th batch, train loss: 0.48843008279800415:  78%|███████▊  | 114/146 [00:17<00:04,  6.48it/s]Epoch: 3, train for the 75-th batch, train loss: 0.5234242081642151:  32%|████         | 75/237 [00:13<00:32,  5.03it/s]Epoch: 7, train for the 101-th batch, train loss: 0.4427148401737213:  84%|█████████▏ | 100/119 [00:16<00:02,  6.67it/s]Epoch: 5, train for the 80-th batch, train loss: 0.49220937490463257:  53%|██████▎     | 80/151 [00:13<00:12,  5.61it/s]Epoch: 6, train for the 115-th batch, train loss: 0.48843008279800415:  79%|███████▉  | 115/146 [00:17<00:05,  5.82it/s]Epoch: 7, train for the 101-th batch, train loss: 0.4427148401737213:  85%|█████████▎ | 101/119 [00:16<00:02,  6.08it/s]Epoch: 4, train for the 7-th batch, train loss: 0.42556074261665344:   2%|▎             | 6/241 [00:00<00:30,  7.76it/s]Epoch: 4, train for the 7-th batch, train loss: 0.42556074261665344:   3%|▍             | 7/241 [00:00<00:31,  7.44it/s]evaluate for the 50-th batch, evaluate loss: 0.5347277522087097:  63%|███████████▎      | 49/78 [00:09<00:05,  4.97it/s]evaluate for the 50-th batch, evaluate loss: 0.5347277522087097:  64%|███████████▌      | 50/78 [00:09<00:05,  4.99it/s]Epoch: 3, train for the 76-th batch, train loss: 0.5755779147148132:  32%|████         | 75/237 [00:13<00:32,  5.03it/s]Epoch: 5, train for the 81-th batch, train loss: 0.538428783416748:  53%|███████▍      | 80/151 [00:13<00:12,  5.61it/s]Epoch: 3, train for the 76-th batch, train loss: 0.5755779147148132:  32%|████▏        | 76/237 [00:13<00:31,  5.07it/s]Epoch: 5, train for the 81-th batch, train loss: 0.538428783416748:  54%|███████▌      | 81/151 [00:13<00:12,  5.45it/s]Epoch: 4, train for the 8-th batch, train loss: 0.7350418567657471:   3%|▍              | 7/241 [00:00<00:31,  7.44it/s]Epoch: 4, train for the 8-th batch, train loss: 0.7350418567657471:   3%|▍              | 8/241 [00:00<00:29,  7.86it/s]Epoch: 6, train for the 116-th batch, train loss: 0.501929759979248:  79%|█████████▍  | 115/146 [00:17<00:05,  5.82it/s]Epoch: 7, train for the 102-th batch, train loss: 0.470738023519516:  85%|██████████▏ | 101/119 [00:16<00:02,  6.08it/s]Epoch: 7, train for the 102-th batch, train loss: 0.470738023519516:  86%|██████████▎ | 102/119 [00:16<00:03,  5.52it/s]Epoch: 6, train for the 116-th batch, train loss: 0.501929759979248:  79%|█████████▌  | 116/146 [00:17<00:05,  5.31it/s]evaluate for the 51-th batch, evaluate loss: 0.5369300246238708:  64%|███████████▌      | 50/78 [00:09<00:05,  4.99it/s]evaluate for the 51-th batch, evaluate loss: 0.5369300246238708:  65%|███████████▊      | 51/78 [00:09<00:05,  4.74it/s]Epoch: 3, train for the 77-th batch, train loss: 0.5609028339385986:  32%|████▏        | 76/237 [00:13<00:31,  5.07it/s]Epoch: 5, train for the 82-th batch, train loss: 0.5180433392524719:  54%|██████▉      | 81/151 [00:14<00:12,  5.45it/s]Epoch: 4, train for the 9-th batch, train loss: 0.6339682340621948:   3%|▍              | 8/241 [00:01<00:29,  7.86it/s]Epoch: 4, train for the 9-th batch, train loss: 0.6339682340621948:   4%|▌              | 9/241 [00:01<00:33,  6.95it/s]Epoch: 7, train for the 103-th batch, train loss: 0.3920474648475647:  86%|█████████▍ | 102/119 [00:16<00:03,  5.52it/s]Epoch: 3, train for the 77-th batch, train loss: 0.5609028339385986:  32%|████▏        | 77/237 [00:13<00:32,  4.94it/s]Epoch: 7, train for the 103-th batch, train loss: 0.3920474648475647:  87%|█████████▌ | 103/119 [00:16<00:02,  5.55it/s]Epoch: 5, train for the 82-th batch, train loss: 0.5180433392524719:  54%|███████      | 82/151 [00:14<00:13,  5.14it/s]Epoch: 6, train for the 117-th batch, train loss: 0.5329074859619141:  79%|████████▋  | 116/146 [00:17<00:05,  5.31it/s]Epoch: 6, train for the 117-th batch, train loss: 0.5329074859619141:  80%|████████▊  | 117/146 [00:17<00:05,  4.97it/s]Epoch: 4, train for the 10-th batch, train loss: 0.5491032004356384:   4%|▌             | 9/241 [00:01<00:33,  6.95it/s]Epoch: 4, train for the 10-th batch, train loss: 0.5491032004356384:   4%|▌            | 10/241 [00:01<00:34,  6.79it/s]Epoch: 7, train for the 104-th batch, train loss: 0.4127004146575928:  87%|█████████▌ | 103/119 [00:16<00:02,  5.55it/s]Epoch: 3, train for the 78-th batch, train loss: 0.5489685535430908:  32%|████▏        | 77/237 [00:13<00:32,  4.94it/s]evaluate for the 52-th batch, evaluate loss: 0.4843795597553253:  65%|███████████▊      | 51/78 [00:10<00:05,  4.74it/s]evaluate for the 52-th batch, evaluate loss: 0.4843795597553253:  67%|████████████      | 52/78 [00:10<00:05,  4.69it/s]Epoch: 7, train for the 104-th batch, train loss: 0.4127004146575928:  87%|█████████▌ | 104/119 [00:16<00:02,  5.56it/s]Epoch: 3, train for the 78-th batch, train loss: 0.5489685535430908:  33%|████▎        | 78/237 [00:13<00:31,  4.99it/s]Epoch: 5, train for the 83-th batch, train loss: 0.5401539206504822:  54%|███████      | 82/151 [00:14<00:13,  5.14it/s]Epoch: 6, train for the 118-th batch, train loss: 0.49305614829063416:  80%|████████  | 117/146 [00:17<00:05,  4.97it/s]Epoch: 6, train for the 118-th batch, train loss: 0.49305614829063416:  81%|████████  | 118/146 [00:17<00:05,  5.34it/s]Epoch: 5, train for the 83-th batch, train loss: 0.5401539206504822:  55%|███████▏     | 83/151 [00:14<00:13,  5.06it/s]Epoch: 4, train for the 11-th batch, train loss: 0.3755878508090973:   4%|▌            | 10/241 [00:01<00:34,  6.79it/s]Epoch: 7, train for the 105-th batch, train loss: 0.4204864501953125:  87%|█████████▌ | 104/119 [00:16<00:02,  5.56it/s]Epoch: 7, train for the 105-th batch, train loss: 0.4204864501953125:  88%|█████████▋ | 105/119 [00:16<00:02,  5.86it/s]Epoch: 3, train for the 79-th batch, train loss: 0.5589598417282104:  33%|████▎        | 78/237 [00:13<00:31,  4.99it/s]Epoch: 3, train for the 79-th batch, train loss: 0.5589598417282104:  33%|████▎        | 79/237 [00:13<00:30,  5.23it/s]Epoch: 6, train for the 119-th batch, train loss: 0.4853883981704712:  81%|████████▉  | 118/146 [00:17<00:05,  5.34it/s]Epoch: 6, train for the 119-th batch, train loss: 0.4853883981704712:  82%|████████▉  | 119/146 [00:17<00:05,  5.38it/s]evaluate for the 53-th batch, evaluate loss: 0.33221644163131714:  67%|███████████▎     | 52/78 [00:10<00:05,  4.69it/s]evaluate for the 53-th batch, evaluate loss: 0.33221644163131714:  68%|███████████▌     | 53/78 [00:10<00:05,  4.62it/s]Epoch: 4, train for the 12-th batch, train loss: 0.5831982493400574:   4%|▌            | 10/241 [00:01<00:34,  6.79it/s]Epoch: 4, train for the 12-th batch, train loss: 0.5831982493400574:   5%|▋            | 12/241 [00:01<00:31,  7.20it/s]Epoch: 5, train for the 84-th batch, train loss: 0.4994216859340668:  55%|███████▏     | 83/151 [00:14<00:13,  5.06it/s]Epoch: 5, train for the 84-th batch, train loss: 0.4994216859340668:  56%|███████▏     | 84/151 [00:14<00:13,  4.88it/s]Epoch: 7, train for the 106-th batch, train loss: 0.4082691967487335:  88%|█████████▋ | 105/119 [00:17<00:02,  5.86it/s]Epoch: 7, train for the 106-th batch, train loss: 0.4082691967487335:  89%|█████████▊ | 106/119 [00:17<00:02,  5.97it/s]Epoch: 3, train for the 80-th batch, train loss: 0.5780066847801208:  33%|████▎        | 79/237 [00:14<00:30,  5.23it/s]Epoch: 6, train for the 120-th batch, train loss: 0.4948023557662964:  82%|████████▉  | 119/146 [00:18<00:05,  5.38it/s]Epoch: 6, train for the 120-th batch, train loss: 0.4948023557662964:  82%|█████████  | 120/146 [00:18<00:04,  5.72it/s]Epoch: 3, train for the 80-th batch, train loss: 0.5780066847801208:  34%|████▍        | 80/237 [00:14<00:30,  5.23it/s]Epoch: 4, train for the 13-th batch, train loss: 0.5357509255409241:   5%|▋            | 12/241 [00:01<00:31,  7.20it/s]evaluate for the 54-th batch, evaluate loss: 0.4909914433956146:  68%|████████████▏     | 53/78 [00:10<00:05,  4.62it/s]evaluate for the 54-th batch, evaluate loss: 0.4909914433956146:  69%|████████████▍     | 54/78 [00:10<00:04,  4.93it/s]Epoch: 4, train for the 13-th batch, train loss: 0.5357509255409241:   5%|▋            | 13/241 [00:01<00:33,  6.88it/s]Epoch: 5, train for the 85-th batch, train loss: 0.5004057288169861:  56%|███████▏     | 84/151 [00:14<00:13,  4.88it/s]Epoch: 5, train for the 85-th batch, train loss: 0.5004057288169861:  56%|███████▎     | 85/151 [00:14<00:13,  5.08it/s]Epoch: 7, train for the 107-th batch, train loss: 0.4487014710903168:  89%|█████████▊ | 106/119 [00:17<00:02,  5.97it/s]Epoch: 7, train for the 107-th batch, train loss: 0.4487014710903168:  90%|█████████▉ | 107/119 [00:17<00:01,  6.18it/s]Epoch: 6, train for the 121-th batch, train loss: 0.49826598167419434:  82%|████████▏ | 120/146 [00:18<00:04,  5.72it/s]Epoch: 3, train for the 81-th batch, train loss: 0.5878152847290039:  34%|████▍        | 80/237 [00:14<00:30,  5.23it/s]Epoch: 6, train for the 121-th batch, train loss: 0.49826598167419434:  83%|████████▎ | 121/146 [00:18<00:04,  6.09it/s]Epoch: 4, train for the 14-th batch, train loss: 0.5894308090209961:   5%|▋            | 13/241 [00:01<00:33,  6.88it/s]Epoch: 4, train for the 14-th batch, train loss: 0.5894308090209961:   6%|▊            | 14/241 [00:01<00:31,  7.12it/s]Epoch: 3, train for the 81-th batch, train loss: 0.5878152847290039:  34%|████▍        | 81/237 [00:14<00:28,  5.56it/s]evaluate for the 55-th batch, evaluate loss: 0.47311437129974365:  69%|███████████▊     | 54/78 [00:10<00:04,  4.93it/s]evaluate for the 55-th batch, evaluate loss: 0.47311437129974365:  71%|███████████▉     | 55/78 [00:10<00:04,  5.39it/s]Epoch: 5, train for the 86-th batch, train loss: 0.5272067189216614:  56%|███████▎     | 85/151 [00:14<00:13,  5.08it/s]Epoch: 5, train for the 86-th batch, train loss: 0.5272067189216614:  57%|███████▍     | 86/151 [00:14<00:12,  5.31it/s]Epoch: 7, train for the 108-th batch, train loss: 0.36716052889823914:  90%|████████▉ | 107/119 [00:17<00:01,  6.18it/s]Epoch: 4, train for the 15-th batch, train loss: 0.5420288443565369:   6%|▊            | 14/241 [00:01<00:31,  7.12it/s]Epoch: 4, train for the 15-th batch, train loss: 0.5420288443565369:   6%|▊            | 15/241 [00:01<00:30,  7.53it/s]Epoch: 7, train for the 108-th batch, train loss: 0.36716052889823914:  91%|█████████ | 108/119 [00:17<00:01,  6.03it/s]Epoch: 6, train for the 122-th batch, train loss: 0.5223873257637024:  83%|█████████  | 121/146 [00:18<00:04,  6.09it/s]Epoch: 6, train for the 122-th batch, train loss: 0.5223873257637024:  84%|█████████▏ | 122/146 [00:18<00:03,  6.12it/s]Epoch: 3, train for the 82-th batch, train loss: 0.5819510221481323:  34%|████▍        | 81/237 [00:14<00:28,  5.56it/s]Epoch: 3, train for the 82-th batch, train loss: 0.5819510221481323:  35%|████▍        | 82/237 [00:14<00:28,  5.44it/s]evaluate for the 56-th batch, evaluate loss: 0.5717502236366272:  71%|████████████▋     | 55/78 [00:10<00:04,  5.39it/s]evaluate for the 56-th batch, evaluate loss: 0.5717502236366272:  72%|████████████▉     | 56/78 [00:10<00:04,  5.26it/s]Epoch: 4, train for the 16-th batch, train loss: 0.490723192691803:   6%|▊             | 15/241 [00:02<00:30,  7.53it/s]Epoch: 4, train for the 16-th batch, train loss: 0.490723192691803:   7%|▉             | 16/241 [00:02<00:29,  7.60it/s]Epoch: 7, train for the 109-th batch, train loss: 0.4705776870250702:  91%|█████████▉ | 108/119 [00:17<00:01,  6.03it/s]Epoch: 4, train for the 17-th batch, train loss: 0.4337588846683502:   7%|▊            | 16/241 [00:02<00:29,  7.60it/s]Epoch: 6, train for the 123-th batch, train loss: 0.5093675255775452:  84%|█████████▏ | 122/146 [00:18<00:03,  6.12it/s]Epoch: 7, train for the 109-th batch, train loss: 0.4705776870250702:  92%|██████████ | 109/119 [00:17<00:01,  5.53it/s]Epoch: 3, train for the 83-th batch, train loss: 0.6167657971382141:  35%|████▍        | 82/237 [00:14<00:28,  5.44it/s]Epoch: 6, train for the 123-th batch, train loss: 0.5093675255775452:  84%|█████████▎ | 123/146 [00:18<00:04,  5.70it/s]Epoch: 3, train for the 83-th batch, train loss: 0.6167657971382141:  35%|████▌        | 83/237 [00:14<00:28,  5.46it/s]Epoch: 5, train for the 87-th batch, train loss: 0.521751344203949:  57%|███████▉      | 86/151 [00:15<00:12,  5.31it/s]evaluate for the 57-th batch, evaluate loss: 0.498616099357605:  72%|█████████████▋     | 56/78 [00:11<00:04,  5.26it/s]evaluate for the 57-th batch, evaluate loss: 0.498616099357605:  73%|█████████████▉     | 57/78 [00:11<00:04,  5.23it/s]Epoch: 5, train for the 87-th batch, train loss: 0.521751344203949:  58%|████████      | 87/151 [00:15<00:14,  4.33it/s]Epoch: 7, train for the 110-th batch, train loss: 0.43856802582740784:  92%|█████████▏| 109/119 [00:17<00:01,  5.53it/s]Epoch: 4, train for the 18-th batch, train loss: 0.47219452261924744:   7%|▊           | 16/241 [00:02<00:29,  7.60it/s]Epoch: 4, train for the 18-th batch, train loss: 0.47219452261924744:   7%|▉           | 18/241 [00:02<00:27,  7.97it/s]Epoch: 7, train for the 110-th batch, train loss: 0.43856802582740784:  92%|█████████▏| 110/119 [00:17<00:01,  5.89it/s]Epoch: 6, train for the 124-th batch, train loss: 0.515910267829895:  84%|██████████  | 123/146 [00:18<00:04,  5.70it/s]Epoch: 6, train for the 124-th batch, train loss: 0.515910267829895:  85%|██████████▏ | 124/146 [00:18<00:03,  5.76it/s]Epoch: 3, train for the 84-th batch, train loss: 0.5461564660072327:  35%|████▌        | 83/237 [00:14<00:28,  5.46it/s]Epoch: 5, train for the 88-th batch, train loss: 0.5534235239028931:  58%|███████▍     | 87/151 [00:15<00:14,  4.33it/s]evaluate for the 58-th batch, evaluate loss: 0.5483095645904541:  73%|█████████████▏    | 57/78 [00:11<00:04,  5.23it/s]evaluate for the 58-th batch, evaluate loss: 0.5483095645904541:  74%|█████████████▍    | 58/78 [00:11<00:03,  5.42it/s]Epoch: 3, train for the 84-th batch, train loss: 0.5461564660072327:  35%|████▌        | 84/237 [00:14<00:28,  5.31it/s]Epoch: 4, train for the 19-th batch, train loss: 0.49598681926727295:   7%|▉           | 18/241 [00:02<00:27,  7.97it/s]Epoch: 5, train for the 88-th batch, train loss: 0.5534235239028931:  58%|███████▌     | 88/151 [00:15<00:13,  4.67it/s]Epoch: 4, train for the 19-th batch, train loss: 0.49598681926727295:   8%|▉           | 19/241 [00:02<00:27,  8.05it/s]Epoch: 7, train for the 111-th batch, train loss: 0.5078101754188538:  92%|██████████▏| 110/119 [00:17<00:01,  5.89it/s]Epoch: 6, train for the 125-th batch, train loss: 0.49657806754112244:  85%|████████▍ | 124/146 [00:18<00:03,  5.76it/s]Epoch: 7, train for the 111-th batch, train loss: 0.5078101754188538:  93%|██████████▎| 111/119 [00:17<00:01,  5.61it/s]Epoch: 6, train for the 125-th batch, train loss: 0.49657806754112244:  86%|████████▌ | 125/146 [00:18<00:03,  5.80it/s]Epoch: 4, train for the 20-th batch, train loss: 0.4508320391178131:   8%|█            | 19/241 [00:02<00:27,  8.05it/s]Epoch: 4, train for the 20-th batch, train loss: 0.4508320391178131:   8%|█            | 20/241 [00:02<00:27,  8.12it/s]evaluate for the 59-th batch, evaluate loss: 0.5849744081497192:  74%|█████████████▍    | 58/78 [00:11<00:03,  5.42it/s]evaluate for the 59-th batch, evaluate loss: 0.5849744081497192:  76%|█████████████▌    | 59/78 [00:11<00:03,  5.45it/s]Epoch: 5, train for the 89-th batch, train loss: 0.5621615648269653:  58%|███████▌     | 88/151 [00:15<00:13,  4.67it/s]Epoch: 3, train for the 85-th batch, train loss: 0.5765567421913147:  35%|████▌        | 84/237 [00:15<00:28,  5.31it/s]Epoch: 5, train for the 89-th batch, train loss: 0.5621615648269653:  59%|███████▋     | 89/151 [00:15<00:12,  4.82it/s]Epoch: 3, train for the 85-th batch, train loss: 0.5765567421913147:  36%|████▋        | 85/237 [00:15<00:30,  5.07it/s]Epoch: 4, train for the 21-th batch, train loss: 0.6285011768341064:   8%|█            | 20/241 [00:02<00:27,  8.12it/s]Epoch: 4, train for the 21-th batch, train loss: 0.6285011768341064:   9%|█▏           | 21/241 [00:02<00:26,  8.27it/s]Epoch: 7, train for the 112-th batch, train loss: 0.43582701683044434:  93%|█████████▎| 111/119 [00:18<00:01,  5.61it/s]Epoch: 7, train for the 112-th batch, train loss: 0.43582701683044434:  94%|█████████▍| 112/119 [00:18<00:01,  5.55it/s]Epoch: 6, train for the 126-th batch, train loss: 0.5266854763031006:  86%|█████████▍ | 125/146 [00:19<00:03,  5.80it/s]Epoch: 6, train for the 126-th batch, train loss: 0.5266854763031006:  86%|█████████▍ | 126/146 [00:19<00:03,  5.46it/s]evaluate for the 60-th batch, evaluate loss: 0.545711100101471:  76%|██████████████▎    | 59/78 [00:11<00:03,  5.45it/s]evaluate for the 60-th batch, evaluate loss: 0.545711100101471:  77%|██████████████▌    | 60/78 [00:11<00:03,  5.61it/s]Epoch: 5, train for the 90-th batch, train loss: 0.5075939297676086:  59%|███████▋     | 89/151 [00:15<00:12,  4.82it/s]Epoch: 5, train for the 90-th batch, train loss: 0.5075939297676086:  60%|███████▋     | 90/151 [00:15<00:11,  5.11it/s]Epoch: 4, train for the 22-th batch, train loss: 0.566142737865448:   9%|█▏            | 21/241 [00:02<00:26,  8.27it/s]Epoch: 4, train for the 22-th batch, train loss: 0.566142737865448:   9%|█▎            | 22/241 [00:02<00:27,  8.00it/s]Epoch: 3, train for the 86-th batch, train loss: 0.5678781867027283:  36%|████▋        | 85/237 [00:15<00:30,  5.07it/s]Epoch: 3, train for the 86-th batch, train loss: 0.5678781867027283:  36%|████▋        | 86/237 [00:15<00:30,  4.88it/s]Epoch: 7, train for the 113-th batch, train loss: 0.41920924186706543:  94%|█████████▍| 112/119 [00:18<00:01,  5.55it/s]Epoch: 6, train for the 127-th batch, train loss: 0.4899364709854126:  86%|█████████▍ | 126/146 [00:19<00:03,  5.46it/s]Epoch: 7, train for the 113-th batch, train loss: 0.41920924186706543:  95%|█████████▍| 113/119 [00:18<00:01,  5.39it/s]Epoch: 6, train for the 127-th batch, train loss: 0.4899364709854126:  87%|█████████▌ | 127/146 [00:19<00:03,  5.50it/s]evaluate for the 61-th batch, evaluate loss: 0.4823186695575714:  77%|█████████████▊    | 60/78 [00:11<00:03,  5.61it/s]evaluate for the 61-th batch, evaluate loss: 0.4823186695575714:  78%|██████████████    | 61/78 [00:11<00:02,  5.68it/s]Epoch: 4, train for the 23-th batch, train loss: 0.5042228698730469:   9%|█▏           | 22/241 [00:02<00:27,  8.00it/s]Epoch: 4, train for the 23-th batch, train loss: 0.5042228698730469:  10%|█▏           | 23/241 [00:02<00:28,  7.65it/s]Epoch: 5, train for the 91-th batch, train loss: 0.4780370593070984:  60%|███████▋     | 90/151 [00:15<00:11,  5.11it/s]Epoch: 5, train for the 91-th batch, train loss: 0.4780370593070984:  60%|███████▊     | 91/151 [00:15<00:11,  5.11it/s]Epoch: 3, train for the 87-th batch, train loss: 0.5599356889724731:  36%|████▋        | 86/237 [00:15<00:30,  4.88it/s]Epoch: 3, train for the 87-th batch, train loss: 0.5599356889724731:  37%|████▊        | 87/237 [00:15<00:30,  4.94it/s]Epoch: 4, train for the 24-th batch, train loss: 0.5596469044685364:  10%|█▏           | 23/241 [00:03<00:28,  7.65it/s]Epoch: 4, train for the 24-th batch, train loss: 0.5596469044685364:  10%|█▎           | 24/241 [00:03<00:28,  7.62it/s]Epoch: 6, train for the 128-th batch, train loss: 0.49636220932006836:  87%|████████▋ | 127/146 [00:19<00:03,  5.50it/s]Epoch: 7, train for the 114-th batch, train loss: 0.4003888666629791:  95%|██████████▍| 113/119 [00:18<00:01,  5.39it/s]Epoch: 6, train for the 128-th batch, train loss: 0.49636220932006836:  88%|████████▊ | 128/146 [00:19<00:03,  5.34it/s]evaluate for the 62-th batch, evaluate loss: 0.6290099620819092:  78%|██████████████    | 61/78 [00:11<00:02,  5.68it/s]evaluate for the 62-th batch, evaluate loss: 0.6290099620819092:  79%|██████████████▎   | 62/78 [00:11<00:02,  5.62it/s]Epoch: 7, train for the 114-th batch, train loss: 0.4003888666629791:  96%|██████████▌| 114/119 [00:18<00:00,  5.06it/s]Epoch: 5, train for the 92-th batch, train loss: 0.5154535174369812:  60%|███████▊     | 91/151 [00:16<00:11,  5.11it/s]Epoch: 5, train for the 92-th batch, train loss: 0.5154535174369812:  61%|███████▉     | 92/151 [00:16<00:11,  5.03it/s]Epoch: 4, train for the 25-th batch, train loss: 0.6013018488883972:  10%|█▎           | 24/241 [00:03<00:28,  7.62it/s]Epoch: 4, train for the 25-th batch, train loss: 0.6013018488883972:  10%|█▎           | 25/241 [00:03<00:27,  7.77it/s]Epoch: 3, train for the 88-th batch, train loss: 0.6282972693443298:  37%|████▊        | 87/237 [00:15<00:30,  4.94it/s]Epoch: 6, train for the 129-th batch, train loss: 0.5276157259941101:  88%|█████████▋ | 128/146 [00:19<00:03,  5.34it/s]Epoch: 6, train for the 129-th batch, train loss: 0.5276157259941101:  88%|█████████▋ | 129/146 [00:19<00:03,  5.64it/s]Epoch: 7, train for the 115-th batch, train loss: 0.4171929359436035:  96%|██████████▌| 114/119 [00:18<00:00,  5.06it/s]Epoch: 3, train for the 88-th batch, train loss: 0.6282972693443298:  37%|████▊        | 88/237 [00:15<00:31,  4.80it/s]Epoch: 7, train for the 115-th batch, train loss: 0.4171929359436035:  97%|██████████▋| 115/119 [00:18<00:00,  5.31it/s]evaluate for the 63-th batch, evaluate loss: 0.5576143264770508:  79%|██████████████▎   | 62/78 [00:12<00:02,  5.62it/s]evaluate for the 63-th batch, evaluate loss: 0.5576143264770508:  81%|██████████████▌   | 63/78 [00:12<00:02,  5.60it/s]Epoch: 4, train for the 26-th batch, train loss: 0.6397492289543152:  10%|█▎           | 25/241 [00:03<00:27,  7.77it/s]Epoch: 4, train for the 26-th batch, train loss: 0.6397492289543152:  11%|█▍           | 26/241 [00:03<00:28,  7.62it/s]Epoch: 5, train for the 93-th batch, train loss: 0.49779805541038513:  61%|███████▎    | 92/151 [00:16<00:11,  5.03it/s]Epoch: 5, train for the 93-th batch, train loss: 0.49779805541038513:  62%|███████▍    | 93/151 [00:16<00:11,  4.94it/s]Epoch: 3, train for the 89-th batch, train loss: 0.5844265818595886:  37%|████▊        | 88/237 [00:15<00:31,  4.80it/s]Epoch: 3, train for the 89-th batch, train loss: 0.5844265818595886:  38%|████▉        | 89/237 [00:15<00:28,  5.26it/s]Epoch: 4, train for the 27-th batch, train loss: 0.6006487607955933:  11%|█▍           | 26/241 [00:03<00:28,  7.62it/s]Epoch: 4, train for the 27-th batch, train loss: 0.6006487607955933:  11%|█▍           | 27/241 [00:03<00:26,  8.06it/s]Epoch: 6, train for the 130-th batch, train loss: 0.4790377616882324:  88%|█████████▋ | 129/146 [00:19<00:03,  5.64it/s]Epoch: 7, train for the 116-th batch, train loss: 0.3704109191894531:  97%|██████████▋| 115/119 [00:18<00:00,  5.31it/s]Epoch: 7, train for the 116-th batch, train loss: 0.3704109191894531:  97%|██████████▋| 116/119 [00:18<00:00,  5.39it/s]evaluate for the 64-th batch, evaluate loss: 0.49658873677253723:  81%|█████████████▋   | 63/78 [00:12<00:02,  5.60it/s]evaluate for the 64-th batch, evaluate loss: 0.49658873677253723:  82%|█████████████▉   | 64/78 [00:12<00:02,  5.67it/s]Epoch: 6, train for the 130-th batch, train loss: 0.4790377616882324:  89%|█████████▊ | 130/146 [00:19<00:02,  5.43it/s]Epoch: 5, train for the 94-th batch, train loss: 0.5053326487541199:  62%|████████     | 93/151 [00:16<00:11,  4.94it/s]Epoch: 4, train for the 28-th batch, train loss: 0.4764482080936432:  11%|█▍           | 27/241 [00:03<00:26,  8.06it/s]Epoch: 5, train for the 94-th batch, train loss: 0.5053326487541199:  62%|████████     | 94/151 [00:16<00:11,  5.14it/s]Epoch: 3, train for the 90-th batch, train loss: 0.600452721118927:  38%|█████▎        | 89/237 [00:16<00:28,  5.26it/s]Epoch: 4, train for the 28-th batch, train loss: 0.4764482080936432:  12%|█▌           | 28/241 [00:03<00:27,  7.88it/s]Epoch: 3, train for the 90-th batch, train loss: 0.600452721118927:  38%|█████▎        | 90/237 [00:16<00:27,  5.32it/s]Epoch: 7, train for the 117-th batch, train loss: 0.3776009678840637:  97%|██████████▋| 116/119 [00:19<00:00,  5.39it/s]Epoch: 7, train for the 117-th batch, train loss: 0.3776009678840637:  98%|██████████▊| 117/119 [00:19<00:00,  5.55it/s]evaluate for the 65-th batch, evaluate loss: 0.6113734841346741:  82%|██████████████▊   | 64/78 [00:12<00:02,  5.67it/s]evaluate for the 65-th batch, evaluate loss: 0.6113734841346741:  83%|███████████████   | 65/78 [00:12<00:02,  5.66it/s]Epoch: 6, train for the 131-th batch, train loss: 0.5256930589675903:  89%|█████████▊ | 130/146 [00:20<00:02,  5.43it/s]Epoch: 6, train for the 131-th batch, train loss: 0.5256930589675903:  90%|█████████▊ | 131/146 [00:20<00:02,  5.33it/s]Epoch: 4, train for the 29-th batch, train loss: 0.5797726511955261:  12%|█▌           | 28/241 [00:03<00:27,  7.88it/s]Epoch: 4, train for the 29-th batch, train loss: 0.5797726511955261:  12%|█▌           | 29/241 [00:03<00:25,  8.16it/s]Epoch: 5, train for the 95-th batch, train loss: 0.5074713826179504:  62%|████████     | 94/151 [00:16<00:11,  5.14it/s]Epoch: 7, train for the 118-th batch, train loss: 0.36634448170661926:  98%|█████████▊| 117/119 [00:19<00:00,  5.55it/s]Epoch: 3, train for the 91-th batch, train loss: 0.5659400224685669:  38%|████▉        | 90/237 [00:16<00:27,  5.32it/s]Epoch: 7, train for the 118-th batch, train loss: 0.36634448170661926:  99%|█████████▉| 118/119 [00:19<00:00,  6.21it/s]Epoch: 5, train for the 95-th batch, train loss: 0.5074713826179504:  63%|████████▏    | 95/151 [00:16<00:10,  5.25it/s]Epoch: 3, train for the 91-th batch, train loss: 0.5659400224685669:  38%|████▉        | 91/237 [00:16<00:26,  5.56it/s]Epoch: 4, train for the 30-th batch, train loss: 0.6561868190765381:  12%|█▌           | 29/241 [00:03<00:25,  8.16it/s]Epoch: 6, train for the 132-th batch, train loss: 0.4713573455810547:  90%|█████████▊ | 131/146 [00:20<00:02,  5.33it/s]Epoch: 4, train for the 30-th batch, train loss: 0.6561868190765381:  12%|█▌           | 30/241 [00:03<00:26,  8.09it/s]Epoch: 6, train for the 132-th batch, train loss: 0.4713573455810547:  90%|█████████▉ | 132/146 [00:20<00:02,  5.53it/s]evaluate for the 66-th batch, evaluate loss: 0.41903913021087646:  83%|██████████████▏  | 65/78 [00:12<00:02,  5.66it/s]evaluate for the 66-th batch, evaluate loss: 0.41903913021087646:  85%|██████████████▍  | 66/78 [00:12<00:02,  5.55it/s]Epoch: 7, train for the 119-th batch, train loss: 0.3176968991756439:  99%|██████████▉| 118/119 [00:19<00:00,  6.21it/s]Epoch: 7, train for the 119-th batch, train loss: 0.3176968991756439: 100%|███████████| 119/119 [00:19<00:00,  6.91it/s]Epoch: 7, train for the 119-th batch, train loss: 0.3176968991756439: 100%|███████████| 119/119 [00:19<00:00,  6.17it/s]
Epoch: 5, train for the 96-th batch, train loss: 0.5389068722724915:  63%|████████▏    | 95/151 [00:16<00:10,  5.25it/s]Epoch: 6, train for the 133-th batch, train loss: 0.49534499645233154:  90%|█████████ | 132/146 [00:20<00:02,  5.53it/s]Epoch: 3, train for the 92-th batch, train loss: 0.5572103261947632:  38%|████▉        | 91/237 [00:16<00:26,  5.56it/s]Epoch: 4, train for the 31-th batch, train loss: 0.46352866291999817:  12%|█▍          | 30/241 [00:03<00:26,  8.09it/s]Epoch: 6, train for the 133-th batch, train loss: 0.49534499645233154:  91%|█████████ | 133/146 [00:20<00:02,  6.04it/s]Epoch: 5, train for the 96-th batch, train loss: 0.5389068722724915:  64%|████████▎    | 96/151 [00:16<00:10,  5.11it/s]Epoch: 4, train for the 31-th batch, train loss: 0.46352866291999817:  13%|█▌          | 31/241 [00:03<00:27,  7.60it/s]Epoch: 3, train for the 92-th batch, train loss: 0.5572103261947632:  39%|█████        | 92/237 [00:16<00:27,  5.35it/s]evaluate for the 67-th batch, evaluate loss: 0.48232710361480713:  85%|██████████████▍  | 66/78 [00:12<00:02,  5.55it/s]evaluate for the 67-th batch, evaluate loss: 0.48232710361480713:  86%|██████████████▌  | 67/78 [00:12<00:01,  5.78it/s]  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 6, train for the 134-th batch, train loss: 0.43521103262901306:  91%|█████████ | 133/146 [00:20<00:02,  6.04it/s]evaluate for the 1-th batch, evaluate loss: 0.4522017240524292:   0%|                            | 0/40 [00:00<?, ?it/s]Epoch: 6, train for the 134-th batch, train loss: 0.43521103262901306:  92%|█████████▏| 134/146 [00:20<00:01,  6.06it/s]Epoch: 3, train for the 93-th batch, train loss: 0.5366538763046265:  39%|█████        | 92/237 [00:16<00:27,  5.35it/s]Epoch: 5, train for the 97-th batch, train loss: 0.5668956637382507:  64%|████████▎    | 96/151 [00:17<00:10,  5.11it/s]Epoch: 4, train for the 32-th batch, train loss: 0.4320017993450165:  13%|█▋           | 31/241 [00:04<00:27,  7.60it/s]Epoch: 3, train for the 93-th batch, train loss: 0.5366538763046265:  39%|█████        | 93/237 [00:16<00:26,  5.41it/s]Epoch: 4, train for the 32-th batch, train loss: 0.4320017993450165:  13%|█▋           | 32/241 [00:04<00:31,  6.72it/s]evaluate for the 68-th batch, evaluate loss: 0.5013222694396973:  86%|███████████████▍  | 67/78 [00:12<00:01,  5.78it/s]evaluate for the 68-th batch, evaluate loss: 0.5013222694396973:  87%|███████████████▋  | 68/78 [00:12<00:01,  5.78it/s]Epoch: 5, train for the 97-th batch, train loss: 0.5668956637382507:  64%|████████▎    | 97/151 [00:17<00:10,  5.10it/s]evaluate for the 2-th batch, evaluate loss: 0.46393442153930664:   0%|                           | 0/40 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.46393442153930664:   5%|▉                  | 2/40 [00:00<00:02, 15.10it/s]evaluate for the 3-th batch, evaluate loss: 0.4576028287410736:   5%|█                   | 2/40 [00:00<00:02, 15.10it/s]Epoch: 6, train for the 135-th batch, train loss: 0.489434152841568:  92%|███████████ | 134/146 [00:20<00:01,  6.06it/s]Epoch: 6, train for the 135-th batch, train loss: 0.489434152841568:  92%|███████████ | 135/146 [00:20<00:01,  6.00it/s]evaluate for the 4-th batch, evaluate loss: 0.5132068991661072:   5%|█                   | 2/40 [00:00<00:02, 15.10it/s]evaluate for the 4-th batch, evaluate loss: 0.5132068991661072:  10%|██                  | 4/40 [00:00<00:02, 15.44it/s]Epoch: 4, train for the 33-th batch, train loss: 0.6283099055290222:  13%|█▋           | 32/241 [00:04<00:31,  6.72it/s]Epoch: 4, train for the 33-th batch, train loss: 0.6283099055290222:  14%|█▊           | 33/241 [00:04<00:31,  6.57it/s]Epoch: 5, train for the 98-th batch, train loss: 0.4955226480960846:  64%|████████▎    | 97/151 [00:17<00:10,  5.10it/s]Epoch: 3, train for the 94-th batch, train loss: 0.5922294855117798:  39%|█████        | 93/237 [00:16<00:26,  5.41it/s]Epoch: 5, train for the 98-th batch, train loss: 0.4955226480960846:  65%|████████▍    | 98/151 [00:17<00:10,  5.03it/s]evaluate for the 5-th batch, evaluate loss: 0.5128053426742554:  10%|██                  | 4/40 [00:00<00:02, 15.44it/s]evaluate for the 69-th batch, evaluate loss: 0.6238001585006714:  87%|███████████████▋  | 68/78 [00:13<00:01,  5.78it/s]evaluate for the 69-th batch, evaluate loss: 0.6238001585006714:  88%|███████████████▉  | 69/78 [00:13<00:01,  5.44it/s]Epoch: 3, train for the 94-th batch, train loss: 0.5922294855117798:  40%|█████▏       | 94/237 [00:16<00:28,  5.05it/s]Epoch: 6, train for the 136-th batch, train loss: 0.5064951181411743:  92%|██████████▏| 135/146 [00:20<00:01,  6.00it/s]evaluate for the 6-th batch, evaluate loss: 0.48417937755584717:  10%|█▉                 | 4/40 [00:00<00:02, 15.44it/s]evaluate for the 6-th batch, evaluate loss: 0.48417937755584717:  15%|██▊                | 6/40 [00:00<00:02, 14.44it/s]Epoch: 6, train for the 136-th batch, train loss: 0.5064951181411743:  93%|██████████▏| 136/146 [00:20<00:01,  6.06it/s]Epoch: 4, train for the 34-th batch, train loss: 0.5529049634933472:  14%|█▊           | 33/241 [00:04<00:31,  6.57it/s]Epoch: 4, train for the 34-th batch, train loss: 0.5529049634933472:  14%|█▊           | 34/241 [00:04<00:31,  6.63it/s]evaluate for the 7-th batch, evaluate loss: 0.4693695604801178:  15%|███                 | 6/40 [00:00<00:02, 14.44it/s]Epoch: 5, train for the 99-th batch, train loss: 0.5621255040168762:  65%|████████▍    | 98/151 [00:17<00:10,  5.03it/s]Epoch: 5, train for the 99-th batch, train loss: 0.5621255040168762:  66%|████████▌    | 99/151 [00:17<00:10,  5.05it/s]Epoch: 4, train for the 35-th batch, train loss: 0.5790507793426514:  14%|█▊           | 34/241 [00:04<00:31,  6.63it/s]Epoch: 4, train for the 35-th batch, train loss: 0.5790507793426514:  15%|█▉           | 35/241 [00:04<00:30,  6.86it/s]Epoch: 3, train for the 95-th batch, train loss: 0.5200416445732117:  40%|█████▏       | 94/237 [00:17<00:28,  5.05it/s]evaluate for the 70-th batch, evaluate loss: 0.5067160725593567:  88%|███████████████▉  | 69/78 [00:13<00:01,  5.44it/s]evaluate for the 70-th batch, evaluate loss: 0.5067160725593567:  90%|████████████████▏ | 70/78 [00:13<00:01,  4.89it/s]Epoch: 6, train for the 137-th batch, train loss: 0.47156620025634766:  93%|█████████▎| 136/146 [00:20<00:01,  6.06it/s]evaluate for the 8-th batch, evaluate loss: 0.4427608847618103:  15%|███                 | 6/40 [00:00<00:02, 14.44it/s]evaluate for the 8-th batch, evaluate loss: 0.4427608847618103:  20%|████                | 8/40 [00:00<00:02, 13.06it/s]Epoch: 6, train for the 137-th batch, train loss: 0.47156620025634766:  94%|█████████▍| 137/146 [00:21<00:01,  5.83it/s]Epoch: 3, train for the 95-th batch, train loss: 0.5200416445732117:  40%|█████▏       | 95/237 [00:17<00:30,  4.60it/s]evaluate for the 9-th batch, evaluate loss: 0.47700235247612:  20%|████▍                 | 8/40 [00:00<00:02, 13.06it/s]Epoch: 5, train for the 100-th batch, train loss: 0.5359472632408142:  66%|███████▊    | 99/151 [00:17<00:10,  5.05it/s]Epoch: 5, train for the 100-th batch, train loss: 0.5359472632408142:  66%|███████▎   | 100/151 [00:17<00:09,  5.29it/s]Epoch: 4, train for the 36-th batch, train loss: 0.6004890203475952:  15%|█▉           | 35/241 [00:04<00:30,  6.86it/s]Epoch: 4, train for the 36-th batch, train loss: 0.6004890203475952:  15%|█▉           | 36/241 [00:04<00:29,  6.94it/s]evaluate for the 10-th batch, evaluate loss: 0.5203325748443604:  20%|███▊               | 8/40 [00:00<00:02, 13.06it/s]evaluate for the 10-th batch, evaluate loss: 0.5203325748443604:  25%|████▌             | 10/40 [00:00<00:02, 13.73it/s]Epoch: 6, train for the 138-th batch, train loss: 0.49014097452163696:  94%|█████████▍| 137/146 [00:21<00:01,  5.83it/s]Epoch: 6, train for the 138-th batch, train loss: 0.49014097452163696:  95%|█████████▍| 138/146 [00:21<00:01,  5.97it/s]evaluate for the 11-th batch, evaluate loss: 0.4505523443222046:  25%|████▌             | 10/40 [00:00<00:02, 13.73it/s]Epoch: 3, train for the 96-th batch, train loss: 0.5731826424598694:  40%|█████▏       | 95/237 [00:17<00:30,  4.60it/s]evaluate for the 71-th batch, evaluate loss: 0.38471969962120056:  90%|███████████████▎ | 70/78 [00:13<00:01,  4.89it/s]evaluate for the 71-th batch, evaluate loss: 0.38471969962120056:  91%|███████████████▍ | 71/78 [00:13<00:01,  4.74it/s]Epoch: 3, train for the 96-th batch, train loss: 0.5731826424598694:  41%|█████▎       | 96/237 [00:17<00:31,  4.52it/s]evaluate for the 12-th batch, evaluate loss: 0.470703125:  25%|██████▎                  | 10/40 [00:00<00:02, 13.73it/s]evaluate for the 12-th batch, evaluate loss: 0.470703125:  30%|███████▌                 | 12/40 [00:00<00:01, 14.41it/s]Epoch: 4, train for the 37-th batch, train loss: 0.503764808177948:  15%|██            | 36/241 [00:04<00:29,  6.94it/s]Epoch: 4, train for the 37-th batch, train loss: 0.503764808177948:  15%|██▏           | 37/241 [00:04<00:31,  6.54it/s]Epoch: 6, train for the 139-th batch, train loss: 0.5530937910079956:  95%|██████████▍| 138/146 [00:21<00:01,  5.97it/s]evaluate for the 13-th batch, evaluate loss: 0.4769592881202698:  30%|█████▍            | 12/40 [00:00<00:01, 14.41it/s]Epoch: 6, train for the 139-th batch, train loss: 0.5530937910079956:  95%|██████████▍| 139/146 [00:21<00:01,  6.17it/s]evaluate for the 14-th batch, evaluate loss: 0.46120935678482056:  30%|█████            | 12/40 [00:00<00:01, 14.41it/s]evaluate for the 14-th batch, evaluate loss: 0.46120935678482056:  35%|█████▉           | 14/40 [00:00<00:01, 14.47it/s]Epoch: 4, train for the 38-th batch, train loss: 0.5653032064437866:  15%|█▉           | 37/241 [00:05<00:31,  6.54it/s]Epoch: 4, train for the 38-th batch, train loss: 0.5653032064437866:  16%|██           | 38/241 [00:05<00:28,  7.10it/s]evaluate for the 72-th batch, evaluate loss: 0.6250145435333252:  91%|████████████████▍ | 71/78 [00:13<00:01,  4.74it/s]evaluate for the 72-th batch, evaluate loss: 0.6250145435333252:  92%|████████████████▌ | 72/78 [00:13<00:01,  4.82it/s]Epoch: 3, train for the 97-th batch, train loss: 0.5591689348220825:  41%|█████▎       | 96/237 [00:17<00:31,  4.52it/s]Epoch: 6, train for the 140-th batch, train loss: 0.46368858218193054:  95%|█████████▌| 139/146 [00:21<00:01,  6.17it/s]Epoch: 3, train for the 97-th batch, train loss: 0.5591689348220825:  41%|█████▎       | 97/237 [00:17<00:30,  4.57it/s]evaluate for the 15-th batch, evaluate loss: 0.47855743765830994:  35%|█████▉           | 14/40 [00:01<00:01, 14.47it/s]Epoch: 6, train for the 140-th batch, train loss: 0.46368858218193054:  96%|█████████▌| 140/146 [00:21<00:00,  6.35it/s]Epoch: 4, train for the 39-th batch, train loss: 0.6014612317085266:  16%|██           | 38/241 [00:05<00:28,  7.10it/s]Epoch: 4, train for the 39-th batch, train loss: 0.6014612317085266:  16%|██           | 39/241 [00:05<00:26,  7.71it/s]evaluate for the 16-th batch, evaluate loss: 0.4667820334434509:  35%|██████▎           | 14/40 [00:01<00:01, 14.47it/s]evaluate for the 16-th batch, evaluate loss: 0.4667820334434509:  40%|███████▏          | 16/40 [00:01<00:01, 14.83it/s]Epoch: 5, train for the 101-th batch, train loss: 0.5567212700843811:  66%|███████▎   | 100/151 [00:18<00:09,  5.29it/s]Epoch: 5, train for the 101-th batch, train loss: 0.5567212700843811:  67%|███████▎   | 101/151 [00:18<00:13,  3.65it/s]evaluate for the 17-th batch, evaluate loss: 0.48016834259033203:  40%|██████▊          | 16/40 [00:01<00:01, 14.83it/s]evaluate for the 73-th batch, evaluate loss: 0.4720475673675537:  92%|████████████████▌ | 72/78 [00:14<00:01,  4.82it/s]evaluate for the 73-th batch, evaluate loss: 0.4720475673675537:  94%|████████████████▊ | 73/78 [00:14<00:01,  4.97it/s]Epoch: 6, train for the 141-th batch, train loss: 0.49888861179351807:  96%|█████████▌| 140/146 [00:21<00:00,  6.35it/s]Epoch: 4, train for the 40-th batch, train loss: 0.6452881693840027:  16%|██           | 39/241 [00:05<00:26,  7.71it/s]Epoch: 3, train for the 98-th batch, train loss: 0.5320244431495667:  41%|█████▎       | 97/237 [00:17<00:30,  4.57it/s]Epoch: 6, train for the 141-th batch, train loss: 0.49888861179351807:  97%|█████████▋| 141/146 [00:21<00:00,  6.03it/s]evaluate for the 18-th batch, evaluate loss: 0.4511544704437256:  40%|███████▏          | 16/40 [00:01<00:01, 14.83it/s]evaluate for the 18-th batch, evaluate loss: 0.4511544704437256:  45%|████████          | 18/40 [00:01<00:01, 14.85it/s]Epoch: 4, train for the 40-th batch, train loss: 0.6452881693840027:  17%|██▏          | 40/241 [00:05<00:27,  7.32it/s]Epoch: 3, train for the 98-th batch, train loss: 0.5320244431495667:  41%|█████▍       | 98/237 [00:17<00:30,  4.57it/s]Epoch: 5, train for the 102-th batch, train loss: 0.5202566981315613:  67%|███████▎   | 101/151 [00:18<00:13,  3.65it/s]evaluate for the 19-th batch, evaluate loss: 0.48515790700912476:  45%|███████▋         | 18/40 [00:01<00:01, 14.85it/s]Epoch: 5, train for the 102-th batch, train loss: 0.5202566981315613:  68%|███████▍   | 102/151 [00:18<00:11,  4.13it/s]Epoch: 4, train for the 41-th batch, train loss: 0.5364770293235779:  17%|██▏          | 40/241 [00:05<00:27,  7.32it/s]Epoch: 4, train for the 41-th batch, train loss: 0.5364770293235779:  17%|██▏          | 41/241 [00:05<00:27,  7.23it/s]evaluate for the 74-th batch, evaluate loss: 0.4435679018497467:  94%|████████████████▊ | 73/78 [00:14<00:01,  4.97it/s]evaluate for the 74-th batch, evaluate loss: 0.4435679018497467:  95%|█████████████████ | 74/78 [00:14<00:00,  4.93it/s]Epoch: 6, train for the 142-th batch, train loss: 0.4806961715221405:  97%|██████████▌| 141/146 [00:21<00:00,  6.03it/s]Epoch: 3, train for the 99-th batch, train loss: 0.5656588077545166:  41%|█████▍       | 98/237 [00:17<00:30,  4.57it/s]evaluate for the 20-th batch, evaluate loss: 0.45455968379974365:  45%|███████▋         | 18/40 [00:01<00:01, 14.85it/s]evaluate for the 20-th batch, evaluate loss: 0.45455968379974365:  50%|████████▌        | 20/40 [00:01<00:01, 13.21it/s]Epoch: 6, train for the 142-th batch, train loss: 0.4806961715221405:  97%|██████████▋| 142/146 [00:21<00:00,  5.64it/s]Epoch: 3, train for the 99-th batch, train loss: 0.5656588077545166:  42%|█████▍       | 99/237 [00:17<00:28,  4.78it/s]evaluate for the 21-th batch, evaluate loss: 0.48238319158554077:  50%|████████▌        | 20/40 [00:01<00:01, 13.21it/s]Epoch: 5, train for the 103-th batch, train loss: 0.5267724990844727:  68%|███████▍   | 102/151 [00:18<00:11,  4.13it/s]Epoch: 4, train for the 42-th batch, train loss: 0.5897340774536133:  17%|██▏          | 41/241 [00:05<00:27,  7.23it/s]Epoch: 5, train for the 103-th batch, train loss: 0.5267724990844727:  68%|███████▌   | 103/151 [00:18<00:10,  4.43it/s]Epoch: 4, train for the 42-th batch, train loss: 0.5897340774536133:  17%|██▎          | 42/241 [00:05<00:27,  7.20it/s]evaluate for the 22-th batch, evaluate loss: 0.44793272018432617:  50%|████████▌        | 20/40 [00:01<00:01, 13.21it/s]evaluate for the 22-th batch, evaluate loss: 0.44793272018432617:  55%|█████████▎       | 22/40 [00:01<00:01, 13.67it/s]evaluate for the 75-th batch, evaluate loss: 0.4439014196395874:  95%|█████████████████ | 74/78 [00:14<00:00,  4.93it/s]evaluate for the 75-th batch, evaluate loss: 0.4439014196395874:  96%|█████████████████▎| 75/78 [00:14<00:00,  5.04it/s]Epoch: 6, train for the 143-th batch, train loss: 0.48432672023773193:  97%|█████████▋| 142/146 [00:22<00:00,  5.64it/s]Epoch: 3, train for the 100-th batch, train loss: 0.5237189531326294:  42%|█████       | 99/237 [00:18<00:28,  4.78it/s]evaluate for the 23-th batch, evaluate loss: 0.404683917760849:  55%|██████████▍        | 22/40 [00:01<00:01, 13.67it/s]Epoch: 6, train for the 143-th batch, train loss: 0.48432672023773193:  98%|█████████▊| 143/146 [00:22<00:00,  5.48it/s]Epoch: 3, train for the 100-th batch, train loss: 0.5237189531326294:  42%|████▋      | 100/237 [00:18<00:28,  4.80it/s]Epoch: 5, train for the 104-th batch, train loss: 0.5494154691696167:  68%|███████▌   | 103/151 [00:18<00:10,  4.43it/s]evaluate for the 24-th batch, evaluate loss: 0.4468339681625366:  55%|█████████▉        | 22/40 [00:01<00:01, 13.67it/s]evaluate for the 24-th batch, evaluate loss: 0.4468339681625366:  60%|██████████▊       | 24/40 [00:01<00:01, 13.87it/s]Epoch: 4, train for the 43-th batch, train loss: 0.6178744435310364:  17%|██▎          | 42/241 [00:05<00:27,  7.20it/s]Epoch: 5, train for the 104-th batch, train loss: 0.5494154691696167:  69%|███████▌   | 104/151 [00:18<00:10,  4.56it/s]Epoch: 4, train for the 43-th batch, train loss: 0.6178744435310364:  18%|██▎          | 43/241 [00:05<00:31,  6.25it/s]evaluate for the 76-th batch, evaluate loss: 0.6126704812049866:  96%|█████████████████▎| 75/78 [00:14<00:00,  5.04it/s]evaluate for the 76-th batch, evaluate loss: 0.6126704812049866:  97%|█████████████████▌| 76/78 [00:14<00:00,  5.27it/s]Epoch: 6, train for the 144-th batch, train loss: 0.4612331688404083:  98%|██████████▊| 143/146 [00:22<00:00,  5.48it/s]evaluate for the 25-th batch, evaluate loss: 0.499784916639328:  60%|███████████▍       | 24/40 [00:01<00:01, 13.87it/s]Epoch: 6, train for the 144-th batch, train loss: 0.4612331688404083:  99%|██████████▊| 144/146 [00:22<00:00,  5.70it/s]Epoch: 3, train for the 101-th batch, train loss: 0.5471681356430054:  42%|████▋      | 100/237 [00:18<00:28,  4.80it/s]evaluate for the 26-th batch, evaluate loss: 0.43221473693847656:  60%|██████████▏      | 24/40 [00:01<00:01, 13.87it/s]evaluate for the 26-th batch, evaluate loss: 0.43221473693847656:  65%|███████████      | 26/40 [00:01<00:01, 13.99it/s]Epoch: 3, train for the 101-th batch, train loss: 0.5471681356430054:  43%|████▋      | 101/237 [00:18<00:28,  4.78it/s]Epoch: 5, train for the 105-th batch, train loss: 0.47600436210632324:  69%|██████▉   | 104/151 [00:18<00:10,  4.56it/s]Epoch: 4, train for the 44-th batch, train loss: 0.6603260636329651:  18%|██▎          | 43/241 [00:05<00:31,  6.25it/s]evaluate for the 27-th batch, evaluate loss: 0.4372081756591797:  65%|███████████▋      | 26/40 [00:01<00:01, 13.99it/s]Epoch: 5, train for the 105-th batch, train loss: 0.47600436210632324:  70%|██████▉   | 105/151 [00:18<00:09,  4.67it/s]Epoch: 4, train for the 44-th batch, train loss: 0.6603260636329651:  18%|██▎          | 44/241 [00:05<00:33,  5.89it/s]evaluate for the 77-th batch, evaluate loss: 0.5277911424636841:  97%|█████████████████▌| 76/78 [00:14<00:00,  5.27it/s]evaluate for the 77-th batch, evaluate loss: 0.5277911424636841:  99%|█████████████████▊| 77/78 [00:14<00:00,  5.33it/s]Epoch: 6, train for the 145-th batch, train loss: 0.45516669750213623:  99%|█████████▊| 144/146 [00:22<00:00,  5.70it/s]Epoch: 6, train for the 145-th batch, train loss: 0.45516669750213623:  99%|█████████▉| 145/146 [00:22<00:00,  5.78it/s]evaluate for the 28-th batch, evaluate loss: 0.4264993667602539:  65%|███████████▋      | 26/40 [00:01<00:01, 13.99it/s]evaluate for the 28-th batch, evaluate loss: 0.4264993667602539:  70%|████████████▌     | 28/40 [00:01<00:00, 13.79it/s]Epoch: 3, train for the 102-th batch, train loss: 0.5599345564842224:  43%|████▋      | 101/237 [00:18<00:28,  4.78it/s]Epoch: 4, train for the 45-th batch, train loss: 0.5048655867576599:  18%|██▎          | 44/241 [00:06<00:33,  5.89it/s]Epoch: 4, train for the 45-th batch, train loss: 0.5048655867576599:  19%|██▍          | 45/241 [00:06<00:30,  6.37it/s]Epoch: 3, train for the 102-th batch, train loss: 0.5599345564842224:  43%|████▋      | 102/237 [00:18<00:27,  4.92it/s]evaluate for the 29-th batch, evaluate loss: 0.4541005492210388:  70%|████████████▌     | 28/40 [00:02<00:00, 13.79it/s]Epoch: 5, train for the 106-th batch, train loss: 0.5558627843856812:  70%|███████▋   | 105/151 [00:19<00:09,  4.67it/s]Epoch: 6, train for the 146-th batch, train loss: 0.46279609203338623:  99%|█████████▉| 145/146 [00:22<00:00,  5.78it/s]Epoch: 6, train for the 146-th batch, train loss: 0.46279609203338623: 100%|██████████| 146/146 [00:22<00:00,  5.88it/s]Epoch: 6, train for the 146-th batch, train loss: 0.46279609203338623: 100%|██████████| 146/146 [00:22<00:00,  6.48it/s]
Epoch: 5, train for the 106-th batch, train loss: 0.5558627843856812:  70%|███████▋   | 106/151 [00:19<00:09,  4.64it/s]evaluate for the 78-th batch, evaluate loss: 0.5683456063270569:  99%|█████████████████▊| 77/78 [00:15<00:00,  5.33it/s]evaluate for the 78-th batch, evaluate loss: 0.5683456063270569: 100%|██████████████████| 78/78 [00:15<00:00,  5.16it/s]evaluate for the 78-th batch, evaluate loss: 0.5683456063270569: 100%|██████████████████| 78/78 [00:15<00:00,  5.20it/s]
evaluate for the 30-th batch, evaluate loss: 0.43697819113731384:  70%|███████████▉     | 28/40 [00:02<00:00, 13.79it/s]evaluate for the 30-th batch, evaluate loss: 0.43697819113731384:  75%|████████████▊    | 30/40 [00:02<00:00, 13.48it/s]Epoch: 4, train for the 46-th batch, train loss: 0.5583636164665222:  19%|██▍          | 45/241 [00:06<00:30,  6.37it/s]Epoch: 4, train for the 46-th batch, train loss: 0.5583636164665222:  19%|██▍          | 46/241 [00:06<00:29,  6.66it/s]evaluate for the 31-th batch, evaluate loss: 0.4363212585449219:  75%|█████████████▌    | 30/40 [00:02<00:00, 13.48it/s]Epoch: 3, train for the 103-th batch, train loss: 0.6191673278808594:  43%|████▋      | 102/237 [00:18<00:27,  4.92it/s]evaluate for the 32-th batch, evaluate loss: 0.4604073464870453:  75%|█████████████▌    | 30/40 [00:02<00:00, 13.48it/s]evaluate for the 32-th batch, evaluate loss: 0.4604073464870453:  80%|██████████████▍   | 32/40 [00:02<00:00, 14.49it/s]Epoch: 3, train for the 103-th batch, train loss: 0.6191673278808594:  43%|████▊      | 103/237 [00:18<00:28,  4.67it/s]Epoch: 5, train for the 107-th batch, train loss: 0.5083006024360657:  70%|███████▋   | 106/151 [00:19<00:09,  4.64it/s]evaluate for the 33-th batch, evaluate loss: 0.44557225704193115:  80%|█████████████▌   | 32/40 [00:02<00:00, 14.49it/s]Epoch: 4, train for the 47-th batch, train loss: 0.5952036380767822:  19%|██▍          | 46/241 [00:06<00:29,  6.66it/s]Epoch: 5, train for the 107-th batch, train loss: 0.5083006024360657:  71%|███████▊   | 107/151 [00:19<00:09,  4.87it/s]  0%|                                                                                            | 0/38 [00:00<?, ?it/s]Epoch: 4, train for the 47-th batch, train loss: 0.5952036380767822:  20%|██▌          | 47/241 [00:06<00:28,  6.69it/s]evaluate for the 34-th batch, evaluate loss: 0.44813621044158936:  80%|█████████████▌   | 32/40 [00:02<00:00, 14.49it/s]evaluate for the 34-th batch, evaluate loss: 0.44813621044158936:  85%|██████████████▍  | 34/40 [00:02<00:00, 14.70it/s]evaluate for the 1-th batch, evaluate loss: 0.47420042753219604:   0%|                           | 0/38 [00:00<?, ?it/s]evaluate for the 35-th batch, evaluate loss: 0.4775303602218628:  85%|███████████████▎  | 34/40 [00:02<00:00, 14.70it/s]evaluate for the 2-th batch, evaluate loss: 0.4807358682155609:   0%|                            | 0/38 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.4807358682155609:   5%|█                   | 2/38 [00:00<00:02, 14.02it/s]Epoch: 4, train for the 48-th batch, train loss: 0.5576063990592957:  20%|██▌          | 47/241 [00:06<00:28,  6.69it/s]evaluate for the 36-th batch, evaluate loss: 0.4507409334182739:  85%|███████████████▎  | 34/40 [00:02<00:00, 14.70it/s]evaluate for the 36-th batch, evaluate loss: 0.4507409334182739:  90%|████████████████▏ | 36/40 [00:02<00:00, 14.59it/s]evaluate for the 3-th batch, evaluate loss: 0.4668595492839813:   5%|█                   | 2/38 [00:00<00:02, 14.02it/s]Epoch: 4, train for the 48-th batch, train loss: 0.5576063990592957:  20%|██▌          | 48/241 [00:06<00:31,  6.03it/s]Epoch: 5, train for the 108-th batch, train loss: 0.5149849653244019:  71%|███████▊   | 107/151 [00:19<00:09,  4.87it/s]Epoch: 5, train for the 108-th batch, train loss: 0.5149849653244019:  72%|███████▊   | 108/151 [00:19<00:09,  4.63it/s]Epoch: 3, train for the 104-th batch, train loss: 0.5394437313079834:  43%|████▊      | 103/237 [00:19<00:28,  4.67it/s]evaluate for the 37-th batch, evaluate loss: 0.47443312406539917:  90%|███████████████▎ | 36/40 [00:02<00:00, 14.59it/s]Epoch: 3, train for the 104-th batch, train loss: 0.5394437313079834:  44%|████▊      | 104/237 [00:19<00:32,  4.09it/s]evaluate for the 4-th batch, evaluate loss: 0.4833792448043823:   5%|█                   | 2/38 [00:00<00:02, 14.02it/s]evaluate for the 4-th batch, evaluate loss: 0.4833792448043823:  11%|██                  | 4/38 [00:00<00:02, 13.78it/s]evaluate for the 38-th batch, evaluate loss: 0.48905059695243835:  90%|███████████████▎ | 36/40 [00:02<00:00, 14.59it/s]evaluate for the 38-th batch, evaluate loss: 0.48905059695243835:  95%|████████████████▏| 38/40 [00:02<00:00, 15.66it/s]Epoch: 4, train for the 49-th batch, train loss: 0.5894620418548584:  20%|██▌          | 48/241 [00:06<00:31,  6.03it/s]evaluate for the 5-th batch, evaluate loss: 0.5074791312217712:  11%|██                  | 4/38 [00:00<00:02, 13.78it/s]evaluate for the 39-th batch, evaluate loss: 0.5041625499725342:  95%|█████████████████ | 38/40 [00:02<00:00, 15.66it/s]Epoch: 4, train for the 49-th batch, train loss: 0.5894620418548584:  20%|██▋          | 49/241 [00:06<00:31,  6.15it/s]Epoch: 5, train for the 109-th batch, train loss: 0.500074565410614:  72%|████████▌   | 108/151 [00:19<00:09,  4.63it/s]evaluate for the 40-th batch, evaluate loss: 0.3678003251552582:  95%|█████████████████ | 38/40 [00:02<00:00, 15.66it/s]evaluate for the 40-th batch, evaluate loss: 0.3678003251552582: 100%|██████████████████| 40/40 [00:02<00:00, 14.59it/s]
Epoch: 5, train for the 109-th batch, train loss: 0.500074565410614:  72%|████████▋   | 109/151 [00:19<00:08,  4.87it/s]evaluate for the 6-th batch, evaluate loss: 0.47279098629951477:  11%|██                 | 4/38 [00:00<00:02, 13.78it/s]evaluate for the 6-th batch, evaluate loss: 0.47279098629951477:  16%|███                | 6/38 [00:00<00:02, 12.99it/s]Epoch: 4, train for the 50-th batch, train loss: 0.5641353726387024:  20%|██▋          | 49/241 [00:06<00:31,  6.15it/s]Epoch: 4, train for the 50-th batch, train loss: 0.5641353726387024:  21%|██▋          | 50/241 [00:06<00:29,  6.52it/s]INFO:root:Epoch: 1, learning rate: 0.0001, train loss: 0.4657
INFO:root:train average_precision, 0.8858
INFO:root:train roc_auc, 0.8684
INFO:root:validate loss: 0.3800
INFO:root:validate average_precision, 0.9215
INFO:root:validate roc_auc, 0.9093
INFO:root:new node validate loss: 0.4901
INFO:root:new node validate first_1_average_precision, 0.8745
INFO:root:new node validate first_1_roc_auc, 0.8418
INFO:root:new node validate first_3_average_precision, 0.8354
INFO:root:new node validate first_3_roc_auc, 0.7941
INFO:root:new node validate first_10_average_precision, 0.8422
INFO:root:new node validate first_10_roc_auc, 0.8071
INFO:root:new node validate average_precision, 0.8576
INFO:root:new node validate roc_auc, 0.8233
INFO:root:save model ./saved_models/TGN/ia-slashdot-reply-dir/TGN_seed0_tgn-ia-slashdot-reply-dir-lincorrect-time-linear/TGN_seed0_tgn-ia-slashdot-reply-dir-lincorrect-time-linear.pkl
Epoch: 3, train for the 105-th batch, train loss: 0.5474493503570557:  44%|████▊      | 104/237 [00:19<00:32,  4.09it/s]  0%|                                                                                            | 0/21 [00:00<?, ?it/s]evaluate for the 7-th batch, evaluate loss: 0.4279167056083679:  16%|███▏                | 6/38 [00:00<00:02, 12.99it/s]Epoch: 3, train for the 105-th batch, train loss: 0.5474493503570557:  44%|████▊      | 105/237 [00:19<00:33,  3.97it/s]Epoch: 5, train for the 110-th batch, train loss: 0.4964388608932495:  72%|███████▉   | 109/151 [00:19<00:08,  4.87it/s]Epoch: 5, train for the 110-th batch, train loss: 0.4964388608932495:  73%|████████   | 110/151 [00:19<00:08,  5.12it/s]evaluate for the 1-th batch, evaluate loss: 0.6365010738372803:   0%|                            | 0/21 [00:00<?, ?it/s]evaluate for the 8-th batch, evaluate loss: 0.4502527415752411:  16%|███▏                | 6/38 [00:00<00:02, 12.99it/s]Epoch: 4, train for the 51-th batch, train loss: 0.5661146640777588:  21%|██▋          | 50/241 [00:06<00:29,  6.52it/s]evaluate for the 8-th batch, evaluate loss: 0.4502527415752411:  21%|████▏               | 8/38 [00:00<00:02, 13.42it/s]Epoch: 4, train for the 51-th batch, train loss: 0.5661146640777588:  21%|██▊          | 51/241 [00:07<00:26,  7.09it/s]evaluate for the 2-th batch, evaluate loss: 0.6945073008537292:   0%|                            | 0/21 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.6945073008537292:  10%|█▉                  | 2/21 [00:00<00:01, 15.15it/s]evaluate for the 9-th batch, evaluate loss: 0.46974700689315796:  21%|████               | 8/38 [00:00<00:02, 13.42it/s]evaluate for the 10-th batch, evaluate loss: 0.49175065755844116:  21%|███▊              | 8/38 [00:00<00:02, 13.42it/s]evaluate for the 10-th batch, evaluate loss: 0.49175065755844116:  26%|████▍            | 10/38 [00:00<00:01, 14.41it/s]Epoch: 3, train for the 106-th batch, train loss: 0.5847012996673584:  44%|████▊      | 105/237 [00:19<00:33,  3.97it/s]evaluate for the 3-th batch, evaluate loss: 0.6847852468490601:  10%|█▉                  | 2/21 [00:00<00:01, 15.15it/s]Epoch: 3, train for the 106-th batch, train loss: 0.5847012996673584:  45%|████▉      | 106/237 [00:19<00:30,  4.26it/s]evaluate for the 11-th batch, evaluate loss: 0.4430004060268402:  26%|████▋             | 10/38 [00:00<00:01, 14.41it/s]Epoch: 4, train for the 52-th batch, train loss: 0.590260922908783:  21%|██▉           | 51/241 [00:07<00:26,  7.09it/s]Epoch: 4, train for the 52-th batch, train loss: 0.590260922908783:  22%|███           | 52/241 [00:07<00:28,  6.69it/s]Epoch: 5, train for the 111-th batch, train loss: 0.4965529441833496:  73%|████████   | 110/151 [00:20<00:08,  5.12it/s]Epoch: 5, train for the 111-th batch, train loss: 0.4965529441833496:  74%|████████   | 111/151 [00:20<00:08,  4.80it/s]evaluate for the 12-th batch, evaluate loss: 0.5194945335388184:  26%|████▋             | 10/38 [00:00<00:01, 14.41it/s]evaluate for the 12-th batch, evaluate loss: 0.5194945335388184:  32%|█████▋            | 12/38 [00:00<00:01, 14.50it/s]evaluate for the 4-th batch, evaluate loss: 0.604129433631897:  10%|██                   | 2/21 [00:00<00:01, 15.15it/s]evaluate for the 4-th batch, evaluate loss: 0.604129433631897:  19%|████                 | 4/21 [00:00<00:01, 11.91it/s]Epoch: 3, train for the 107-th batch, train loss: 0.5774229168891907:  45%|████▉      | 106/237 [00:19<00:30,  4.26it/s]evaluate for the 13-th batch, evaluate loss: 0.4961022138595581:  32%|█████▋            | 12/38 [00:00<00:01, 14.50it/s]Epoch: 4, train for the 53-th batch, train loss: 0.43023431301116943:  22%|██▌         | 52/241 [00:07<00:28,  6.69it/s]Epoch: 4, train for the 53-th batch, train loss: 0.43023431301116943:  22%|██▋         | 53/241 [00:07<00:28,  6.71it/s]evaluate for the 5-th batch, evaluate loss: 0.7099555730819702:  19%|███▊                | 4/21 [00:00<00:01, 11.91it/s]Epoch: 3, train for the 107-th batch, train loss: 0.5774229168891907:  45%|████▉      | 107/237 [00:19<00:28,  4.54it/s]evaluate for the 14-th batch, evaluate loss: 0.4292372167110443:  32%|█████▋            | 12/38 [00:00<00:01, 14.50it/s]Epoch: 5, train for the 112-th batch, train loss: 0.4805227518081665:  74%|████████   | 111/151 [00:20<00:08,  4.80it/s]evaluate for the 6-th batch, evaluate loss: 0.6759397983551025:  19%|███▊                | 4/21 [00:00<00:01, 11.91it/s]evaluate for the 6-th batch, evaluate loss: 0.6759397983551025:  29%|█████▋              | 6/21 [00:00<00:01, 12.70it/s]Epoch: 5, train for the 112-th batch, train loss: 0.4805227518081665:  74%|████████▏  | 112/151 [00:20<00:07,  5.04it/s]evaluate for the 15-th batch, evaluate loss: 0.4447261691093445:  32%|█████▋            | 12/38 [00:01<00:01, 14.50it/s]evaluate for the 15-th batch, evaluate loss: 0.4447261691093445:  39%|███████           | 15/38 [00:01<00:01, 16.27it/s]Epoch: 4, train for the 54-th batch, train loss: 0.4128202199935913:  22%|██▊          | 53/241 [00:07<00:28,  6.71it/s]Epoch: 3, train for the 108-th batch, train loss: 0.5882866978645325:  45%|████▉      | 107/237 [00:19<00:28,  4.54it/s]Epoch: 4, train for the 54-th batch, train loss: 0.4128202199935913:  22%|██▉          | 54/241 [00:07<00:27,  6.68it/s]evaluate for the 16-th batch, evaluate loss: 0.49623462557792664:  39%|██████▋          | 15/38 [00:01<00:01, 16.27it/s]evaluate for the 7-th batch, evaluate loss: 0.640582799911499:  29%|██████               | 6/21 [00:00<00:01, 12.70it/s]Epoch: 3, train for the 108-th batch, train loss: 0.5882866978645325:  46%|█████      | 108/237 [00:19<00:26,  4.93it/s]Epoch: 5, train for the 113-th batch, train loss: 0.5153213143348694:  74%|████████▏  | 112/151 [00:20<00:07,  5.04it/s]Epoch: 5, train for the 113-th batch, train loss: 0.5153213143348694:  75%|████████▏  | 113/151 [00:20<00:07,  5.34it/s]evaluate for the 17-th batch, evaluate loss: 0.46173644065856934:  39%|██████▋          | 15/38 [00:01<00:01, 16.27it/s]evaluate for the 17-th batch, evaluate loss: 0.46173644065856934:  45%|███████▌         | 17/38 [00:01<00:01, 14.70it/s]evaluate for the 8-th batch, evaluate loss: 0.6447942852973938:  29%|█████▋              | 6/21 [00:00<00:01, 12.70it/s]evaluate for the 8-th batch, evaluate loss: 0.6447942852973938:  38%|███████▌            | 8/21 [00:00<00:01, 11.95it/s]Epoch: 4, train for the 55-th batch, train loss: 0.3798186779022217:  22%|██▉          | 54/241 [00:07<00:27,  6.68it/s]Epoch: 4, train for the 55-th batch, train loss: 0.3798186779022217:  23%|██▉          | 55/241 [00:07<00:26,  7.11it/s]Epoch: 3, train for the 109-th batch, train loss: 0.5925257802009583:  46%|█████      | 108/237 [00:20<00:26,  4.93it/s]Epoch: 3, train for the 109-th batch, train loss: 0.5925257802009583:  46%|█████      | 109/237 [00:20<00:24,  5.20it/s]evaluate for the 18-th batch, evaluate loss: 0.49266448616981506:  45%|███████▌         | 17/38 [00:01<00:01, 14.70it/s]evaluate for the 9-th batch, evaluate loss: 0.6313907504081726:  38%|███████▌            | 8/21 [00:00<00:01, 11.95it/s]Epoch: 5, train for the 114-th batch, train loss: 0.4968849718570709:  75%|████████▏  | 113/151 [00:20<00:07,  5.34it/s]evaluate for the 19-th batch, evaluate loss: 0.4869650900363922:  45%|████████          | 17/38 [00:01<00:01, 14.70it/s]evaluate for the 19-th batch, evaluate loss: 0.4869650900363922:  50%|█████████         | 19/38 [00:01<00:01, 14.09it/s]Epoch: 5, train for the 114-th batch, train loss: 0.4968849718570709:  75%|████████▎  | 114/151 [00:20<00:06,  5.38it/s]evaluate for the 10-th batch, evaluate loss: 0.650524914264679:  38%|███████▌            | 8/21 [00:00<00:01, 11.95it/s]evaluate for the 10-th batch, evaluate loss: 0.650524914264679:  48%|█████████          | 10/21 [00:00<00:00, 12.05it/s]evaluate for the 20-th batch, evaluate loss: 0.40309450030326843:  50%|████████▌        | 19/38 [00:01<00:01, 14.09it/s]Epoch: 3, train for the 110-th batch, train loss: 0.5624359250068665:  46%|█████      | 109/237 [00:20<00:24,  5.20it/s]evaluate for the 11-th batch, evaluate loss: 0.6654113531112671:  48%|████████▌         | 10/21 [00:00<00:00, 12.05it/s]Epoch: 3, train for the 110-th batch, train loss: 0.5624359250068665:  46%|█████      | 110/237 [00:20<00:23,  5.32it/s]evaluate for the 21-th batch, evaluate loss: 0.4259732663631439:  50%|█████████         | 19/38 [00:01<00:01, 14.09it/s]evaluate for the 21-th batch, evaluate loss: 0.4259732663631439:  55%|█████████▉        | 21/38 [00:01<00:01, 14.97it/s]Epoch: 4, train for the 56-th batch, train loss: 0.4043973386287689:  23%|██▉          | 55/241 [00:07<00:26,  7.11it/s]Epoch: 4, train for the 56-th batch, train loss: 0.4043973386287689:  23%|███          | 56/241 [00:07<00:33,  5.51it/s]Epoch: 5, train for the 115-th batch, train loss: 0.4878406822681427:  75%|████████▎  | 114/151 [00:20<00:06,  5.38it/s]evaluate for the 12-th batch, evaluate loss: 0.6605590581893921:  48%|████████▌         | 10/21 [00:00<00:00, 12.05it/s]evaluate for the 12-th batch, evaluate loss: 0.6605590581893921:  57%|██████████▎       | 12/21 [00:00<00:00, 12.31it/s]evaluate for the 22-th batch, evaluate loss: 0.4724339246749878:  55%|█████████▉        | 21/38 [00:01<00:01, 14.97it/s]Epoch: 5, train for the 115-th batch, train loss: 0.4878406822681427:  76%|████████▍  | 115/151 [00:20<00:06,  5.44it/s]Epoch: 3, train for the 111-th batch, train loss: 0.5755725502967834:  46%|█████      | 110/237 [00:20<00:23,  5.32it/s]Epoch: 4, train for the 57-th batch, train loss: 0.4614787697792053:  23%|███          | 56/241 [00:07<00:33,  5.51it/s]evaluate for the 23-th batch, evaluate loss: 0.4662560820579529:  55%|█████████▉        | 21/38 [00:01<00:01, 14.97it/s]evaluate for the 23-th batch, evaluate loss: 0.4662560820579529:  61%|██████████▉       | 23/38 [00:01<00:01, 14.36it/s]Epoch: 4, train for the 57-th batch, train loss: 0.4614787697792053:  24%|███          | 57/241 [00:07<00:30,  6.04it/s]evaluate for the 13-th batch, evaluate loss: 0.6497190594673157:  57%|██████████▎       | 12/21 [00:01<00:00, 12.31it/s]Epoch: 3, train for the 111-th batch, train loss: 0.5755725502967834:  47%|█████▏     | 111/237 [00:20<00:22,  5.50it/s]Epoch: 5, train for the 116-th batch, train loss: 0.45532897114753723:  76%|███████▌  | 115/151 [00:21<00:06,  5.44it/s]evaluate for the 24-th batch, evaluate loss: 0.4436825215816498:  61%|██████████▉       | 23/38 [00:01<00:01, 14.36it/s]evaluate for the 14-th batch, evaluate loss: 0.6314522624015808:  57%|██████████▎       | 12/21 [00:01<00:00, 12.31it/s]evaluate for the 14-th batch, evaluate loss: 0.6314522624015808:  67%|████████████      | 14/21 [00:01<00:00, 11.30it/s]Epoch: 5, train for the 116-th batch, train loss: 0.45532897114753723:  77%|███████▋  | 116/151 [00:21<00:06,  5.29it/s]Epoch: 4, train for the 58-th batch, train loss: 0.3617175221443176:  24%|███          | 57/241 [00:08<00:30,  6.04it/s]Epoch: 4, train for the 58-th batch, train loss: 0.3617175221443176:  24%|███▏         | 58/241 [00:08<00:28,  6.34it/s]Epoch: 3, train for the 112-th batch, train loss: 0.6003469228744507:  47%|█████▏     | 111/237 [00:20<00:22,  5.50it/s]evaluate for the 25-th batch, evaluate loss: 0.49026548862457275:  61%|██████████▎      | 23/38 [00:01<00:01, 14.36it/s]Epoch: 3, train for the 112-th batch, train loss: 0.6003469228744507:  47%|█████▏     | 112/237 [00:20<00:22,  5.49it/s]evaluate for the 25-th batch, evaluate loss: 0.49026548862457275:  66%|███████████▏     | 25/38 [00:01<00:01, 12.85it/s]evaluate for the 15-th batch, evaluate loss: 0.6706669330596924:  67%|████████████      | 14/21 [00:01<00:00, 11.30it/s]evaluate for the 26-th batch, evaluate loss: 0.4645623564720154:  66%|███████████▊      | 25/38 [00:01<00:01, 12.85it/s]evaluate for the 16-th batch, evaluate loss: 0.6533764004707336:  67%|████████████      | 14/21 [00:01<00:00, 11.30it/s]evaluate for the 16-th batch, evaluate loss: 0.6533764004707336:  76%|█████████████▋    | 16/21 [00:01<00:00, 11.61it/s]Epoch: 4, train for the 59-th batch, train loss: 0.5812241435050964:  24%|███▏         | 58/241 [00:08<00:28,  6.34it/s]Epoch: 4, train for the 59-th batch, train loss: 0.5812241435050964:  24%|███▏         | 59/241 [00:08<00:29,  6.22it/s]Epoch: 5, train for the 117-th batch, train loss: 0.5002655386924744:  77%|████████▍  | 116/151 [00:21<00:06,  5.29it/s]Epoch: 5, train for the 117-th batch, train loss: 0.5002655386924744:  77%|████████▌  | 117/151 [00:21<00:06,  5.13it/s]evaluate for the 27-th batch, evaluate loss: 0.4806496202945709:  66%|███████████▊      | 25/38 [00:01<00:01, 12.85it/s]evaluate for the 27-th batch, evaluate loss: 0.4806496202945709:  71%|████████████▊     | 27/38 [00:01<00:00, 12.78it/s]Epoch: 3, train for the 113-th batch, train loss: 0.5999260544776917:  47%|█████▏     | 112/237 [00:20<00:22,  5.49it/s]evaluate for the 17-th batch, evaluate loss: 0.5681129097938538:  76%|█████████████▋    | 16/21 [00:01<00:00, 11.61it/s]Epoch: 3, train for the 113-th batch, train loss: 0.5999260544776917:  48%|█████▏     | 113/237 [00:20<00:23,  5.35it/s]evaluate for the 28-th batch, evaluate loss: 0.46427908539772034:  71%|████████████     | 27/38 [00:02<00:00, 12.78it/s]evaluate for the 18-th batch, evaluate loss: 0.642768383026123:  76%|██████████████▍    | 16/21 [00:01<00:00, 11.61it/s]evaluate for the 18-th batch, evaluate loss: 0.642768383026123:  86%|████████████████▎  | 18/21 [00:01<00:00, 11.88it/s]Epoch: 4, train for the 60-th batch, train loss: 0.5826337337493896:  24%|███▏         | 59/241 [00:08<00:29,  6.22it/s]Epoch: 4, train for the 60-th batch, train loss: 0.5826337337493896:  25%|███▏         | 60/241 [00:08<00:29,  6.20it/s]evaluate for the 29-th batch, evaluate loss: 0.46848273277282715:  71%|████████████     | 27/38 [00:02<00:00, 12.78it/s]evaluate for the 29-th batch, evaluate loss: 0.46848273277282715:  76%|████████████▉    | 29/38 [00:02<00:00, 13.26it/s]Epoch: 5, train for the 118-th batch, train loss: 0.4433354437351227:  77%|████████▌  | 117/151 [00:21<00:06,  5.13it/s]evaluate for the 19-th batch, evaluate loss: 0.640949547290802:  86%|████████████████▎  | 18/21 [00:01<00:00, 11.88it/s]Epoch: 5, train for the 118-th batch, train loss: 0.4433354437351227:  78%|████████▌  | 118/151 [00:21<00:06,  5.10it/s]Epoch: 3, train for the 114-th batch, train loss: 0.5502659678459167:  48%|█████▏     | 113/237 [00:20<00:23,  5.35it/s]Epoch: 3, train for the 114-th batch, train loss: 0.5502659678459167:  48%|█████▎     | 114/237 [00:20<00:22,  5.38it/s]evaluate for the 30-th batch, evaluate loss: 0.4797515869140625:  76%|█████████████▋    | 29/38 [00:02<00:00, 13.26it/s]evaluate for the 20-th batch, evaluate loss: 0.6291876435279846:  86%|███████████████▍  | 18/21 [00:01<00:00, 11.88it/s]evaluate for the 20-th batch, evaluate loss: 0.6291876435279846:  95%|█████████████████▏| 20/21 [00:01<00:00, 12.03it/s]Epoch: 4, train for the 61-th batch, train loss: 0.5596272945404053:  25%|███▏         | 60/241 [00:08<00:29,  6.20it/s]Epoch: 4, train for the 61-th batch, train loss: 0.5596272945404053:  25%|███▎         | 61/241 [00:08<00:28,  6.27it/s]evaluate for the 21-th batch, evaluate loss: 0.4994063079357147:  95%|█████████████████▏| 20/21 [00:01<00:00, 12.03it/s]evaluate for the 21-th batch, evaluate loss: 0.4994063079357147: 100%|██████████████████| 21/21 [00:01<00:00, 12.30it/s]
Epoch: 5, train for the 119-th batch, train loss: 0.536297082901001:  78%|█████████▍  | 118/151 [00:21<00:06,  5.10it/s]Epoch: 5, train for the 119-th batch, train loss: 0.536297082901001:  79%|█████████▍  | 119/151 [00:21<00:05,  5.35it/s]evaluate for the 31-th batch, evaluate loss: 0.46529898047447205:  76%|████████████▉    | 29/38 [00:02<00:00, 13.26it/s]evaluate for the 31-th batch, evaluate loss: 0.46529898047447205:  82%|█████████████▊   | 31/38 [00:02<00:00, 11.55it/s]Epoch: 3, train for the 115-th batch, train loss: 0.5377597212791443:  48%|█████▎     | 114/237 [00:21<00:22,  5.38it/s]Epoch: 4, train for the 62-th batch, train loss: 0.5819779634475708:  25%|███▎         | 61/241 [00:08<00:28,  6.27it/s]Epoch: 3, train for the 115-th batch, train loss: 0.5377597212791443:  49%|█████▎     | 115/237 [00:21<00:22,  5.34it/s]Epoch: 4, train for the 62-th batch, train loss: 0.5819779634475708:  26%|███▎         | 62/241 [00:08<00:27,  6.44it/s]evaluate for the 32-th batch, evaluate loss: 0.4386350214481354:  82%|██████████████▋   | 31/38 [00:02<00:00, 11.55it/s]evaluate for the 33-th batch, evaluate loss: 0.4857349395751953:  82%|██████████████▋   | 31/38 [00:02<00:00, 11.55it/s]evaluate for the 33-th batch, evaluate loss: 0.4857349395751953:  87%|███████████████▋  | 33/38 [00:02<00:00, 12.86it/s]Epoch: 5, train for the 120-th batch, train loss: 0.5095528364181519:  79%|████████▋  | 119/151 [00:21<00:05,  5.35it/s]Epoch: 5, train for the 120-th batch, train loss: 0.5095528364181519:  79%|████████▋  | 120/151 [00:21<00:05,  5.42it/s]Epoch: 4, train for the 63-th batch, train loss: 0.750273585319519:  26%|███▌          | 62/241 [00:08<00:27,  6.44it/s]Epoch: 4, train for the 63-th batch, train loss: 0.750273585319519:  26%|███▋          | 63/241 [00:08<00:25,  6.87it/s]Epoch: 3, train for the 116-th batch, train loss: 0.5997082591056824:  49%|█████▎     | 115/237 [00:21<00:22,  5.34it/s]INFO:root:Epoch: 7, learning rate: 0.0001, train loss: 0.4492
INFO:root:train average_precision, 0.8813
INFO:root:train roc_auc, 0.8705
INFO:root:validate loss: 0.4623
INFO:root:validate average_precision, 0.8697
INFO:root:validate roc_auc, 0.8647
INFO:root:new node validate loss: 0.6421
INFO:root:new node validate first_1_average_precision, 0.7013
INFO:root:new node validate first_1_roc_auc, 0.7016
INFO:root:new node validate first_3_average_precision, 0.7278
INFO:root:new node validate first_3_roc_auc, 0.7310
INFO:root:new node validate first_10_average_precision, 0.7295
INFO:root:new node validate first_10_roc_auc, 0.7416
INFO:root:new node validate average_precision, 0.7269
INFO:root:new node validate roc_auc, 0.7494
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]Epoch: 3, train for the 116-th batch, train loss: 0.5997082591056824:  49%|█████▍     | 116/237 [00:21<00:23,  5.20it/s]Epoch: 4, train for the 64-th batch, train loss: 0.3264944851398468:  26%|███▍         | 63/241 [00:09<00:25,  6.87it/s]evaluate for the 34-th batch, evaluate loss: 0.4854835867881775:  87%|███████████████▋  | 33/38 [00:02<00:00, 12.86it/s]Epoch: 4, train for the 64-th batch, train loss: 0.3264944851398468:  27%|███▍         | 64/241 [00:09<00:25,  6.86it/s]Epoch: 5, train for the 121-th batch, train loss: 0.47735053300857544:  79%|███████▉  | 120/151 [00:22<00:05,  5.42it/s]Epoch: 8, train for the 1-th batch, train loss: 0.7230903506278992:   0%|                       | 0/119 [00:00<?, ?it/s]Epoch: 8, train for the 1-th batch, train loss: 0.7230903506278992:   1%|▏              | 1/119 [00:00<00:12,  9.14it/s]Epoch: 5, train for the 121-th batch, train loss: 0.47735053300857544:  80%|████████  | 121/151 [00:22<00:05,  5.26it/s]evaluate for the 35-th batch, evaluate loss: 0.47765058279037476:  87%|██████████████▊  | 33/38 [00:02<00:00, 12.86it/s]evaluate for the 35-th batch, evaluate loss: 0.47765058279037476:  92%|███████████████▋ | 35/38 [00:02<00:00, 10.64it/s]Epoch: 3, train for the 117-th batch, train loss: 0.5259175896644592:  49%|█████▍     | 116/237 [00:21<00:23,  5.20it/s]Epoch: 3, train for the 117-th batch, train loss: 0.5259175896644592:  49%|█████▍     | 117/237 [00:21<00:22,  5.30it/s]Epoch: 4, train for the 65-th batch, train loss: 0.35109636187553406:  27%|███▏        | 64/241 [00:09<00:25,  6.86it/s]Epoch: 8, train for the 2-th batch, train loss: 0.7498673796653748:   1%|▏              | 1/119 [00:00<00:12,  9.14it/s]evaluate for the 36-th batch, evaluate loss: 0.4829610288143158:  92%|████████████████▌ | 35/38 [00:02<00:00, 10.64it/s]Epoch: 8, train for the 2-th batch, train loss: 0.7498673796653748:   2%|▎              | 2/119 [00:00<00:15,  7.64it/s]Epoch: 4, train for the 65-th batch, train loss: 0.35109636187553406:  27%|███▏        | 65/241 [00:09<00:27,  6.47it/s]Epoch: 5, train for the 122-th batch, train loss: 0.48848944902420044:  80%|████████  | 121/151 [00:22<00:05,  5.26it/s]Epoch: 5, train for the 122-th batch, train loss: 0.48848944902420044:  81%|████████  | 122/151 [00:22<00:05,  5.36it/s]evaluate for the 37-th batch, evaluate loss: 0.4361339211463928:  92%|████████████████▌ | 35/38 [00:02<00:00, 10.64it/s]evaluate for the 37-th batch, evaluate loss: 0.4361339211463928:  97%|█████████████████▌| 37/38 [00:02<00:00, 10.87it/s]Epoch: 3, train for the 118-th batch, train loss: 0.5254002213478088:  49%|█████▍     | 117/237 [00:21<00:22,  5.30it/s]Epoch: 3, train for the 118-th batch, train loss: 0.5254002213478088:  50%|█████▍     | 118/237 [00:21<00:21,  5.59it/s]Epoch: 8, train for the 3-th batch, train loss: 0.7956864833831787:   2%|▎              | 2/119 [00:00<00:15,  7.64it/s]Epoch: 8, train for the 3-th batch, train loss: 0.7956864833831787:   3%|▍              | 3/119 [00:00<00:14,  8.04it/s]Epoch: 4, train for the 66-th batch, train loss: 0.27849626541137695:  27%|███▏        | 65/241 [00:09<00:27,  6.47it/s]Epoch: 4, train for the 66-th batch, train loss: 0.27849626541137695:  27%|███▎        | 66/241 [00:09<00:26,  6.60it/s]Epoch: 8, train for the 4-th batch, train loss: 0.7087740302085876:   3%|▍              | 3/119 [00:00<00:14,  8.04it/s]Epoch: 5, train for the 123-th batch, train loss: 0.5044222474098206:  81%|████████▉  | 122/151 [00:22<00:05,  5.36it/s]Epoch: 5, train for the 123-th batch, train loss: 0.5044222474098206:  81%|████████▉  | 123/151 [00:22<00:05,  5.40it/s]Epoch: 3, train for the 119-th batch, train loss: 0.582997739315033:  50%|█████▉      | 118/237 [00:21<00:21,  5.59it/s]Epoch: 4, train for the 67-th batch, train loss: 0.43011870980262756:  27%|███▎        | 66/241 [00:09<00:26,  6.60it/s]Epoch: 3, train for the 119-th batch, train loss: 0.582997739315033:  50%|██████      | 119/237 [00:21<00:20,  5.63it/s]Epoch: 4, train for the 67-th batch, train loss: 0.43011870980262756:  28%|███▎        | 67/241 [00:09<00:24,  7.04it/s]evaluate for the 38-th batch, evaluate loss: 0.4673832356929779:  97%|█████████████████▌| 37/38 [00:03<00:00, 10.87it/s]evaluate for the 38-th batch, evaluate loss: 0.4673832356929779: 100%|██████████████████| 38/38 [00:03<00:00, 12.22it/s]
Epoch: 8, train for the 5-th batch, train loss: 0.65902179479599:   3%|▍                | 3/119 [00:00<00:14,  8.04it/s]Epoch: 8, train for the 5-th batch, train loss: 0.65902179479599:   4%|▋                | 5/119 [00:00<00:14,  7.84it/s]Epoch: 5, train for the 124-th batch, train loss: 0.47126197814941406:  81%|████████▏ | 123/151 [00:22<00:05,  5.40it/s]Epoch: 5, train for the 124-th batch, train loss: 0.47126197814941406:  82%|████████▏ | 124/151 [00:22<00:04,  5.59it/s]Epoch: 4, train for the 68-th batch, train loss: 0.5203447341918945:  28%|███▌         | 67/241 [00:09<00:24,  7.04it/s]Epoch: 3, train for the 120-th batch, train loss: 0.5534703731536865:  50%|█████▌     | 119/237 [00:22<00:20,  5.63it/s]Epoch: 4, train for the 68-th batch, train loss: 0.5203447341918945:  28%|███▋         | 68/241 [00:09<00:24,  7.00it/s]Epoch: 3, train for the 120-th batch, train loss: 0.5534703731536865:  51%|█████▌     | 120/237 [00:22<00:20,  5.80it/s]  0%|                                                                                            | 0/20 [00:00<?, ?it/s]Epoch: 8, train for the 6-th batch, train loss: 0.5576006770133972:   4%|▋              | 5/119 [00:00<00:14,  7.84it/s]Epoch: 8, train for the 6-th batch, train loss: 0.5576006770133972:   5%|▊              | 6/119 [00:00<00:14,  7.70it/s]Epoch: 5, train for the 125-th batch, train loss: 0.4916894733905792:  82%|█████████  | 124/151 [00:22<00:04,  5.59it/s]Epoch: 4, train for the 69-th batch, train loss: 0.640070378780365:  28%|███▉          | 68/241 [00:09<00:24,  7.00it/s]Epoch: 5, train for the 125-th batch, train loss: 0.4916894733905792:  83%|█████████  | 125/151 [00:22<00:04,  5.73it/s]evaluate for the 1-th batch, evaluate loss: 0.7364727854728699:   0%|                            | 0/20 [00:00<?, ?it/s]Epoch: 4, train for the 69-th batch, train loss: 0.640070378780365:  29%|████          | 69/241 [00:09<00:25,  6.84it/s]Epoch: 3, train for the 121-th batch, train loss: 0.5504390001296997:  51%|█████▌     | 120/237 [00:22<00:20,  5.80it/s]Epoch: 3, train for the 121-th batch, train loss: 0.5504390001296997:  51%|█████▌     | 121/237 [00:22<00:20,  5.54it/s]Epoch: 8, train for the 7-th batch, train loss: 0.5303133130073547:   5%|▊              | 6/119 [00:00<00:14,  7.70it/s]Epoch: 8, train for the 7-th batch, train loss: 0.5303133130073547:   6%|▉              | 7/119 [00:00<00:14,  7.75it/s]evaluate for the 2-th batch, evaluate loss: 0.7530679702758789:   0%|                            | 0/20 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.7530679702758789:  10%|██                  | 2/20 [00:00<00:01, 12.47it/s]Epoch: 5, train for the 126-th batch, train loss: 0.4785061478614807:  83%|█████████  | 125/151 [00:22<00:04,  5.73it/s]evaluate for the 3-th batch, evaluate loss: 0.6268318295478821:  10%|██                  | 2/20 [00:00<00:01, 12.47it/s]Epoch: 4, train for the 70-th batch, train loss: 0.5408278703689575:  29%|███▋         | 69/241 [00:09<00:25,  6.84it/s]Epoch: 4, train for the 70-th batch, train loss: 0.5408278703689575:  29%|███▊         | 70/241 [00:09<00:26,  6.34it/s]Epoch: 5, train for the 126-th batch, train loss: 0.4785061478614807:  83%|█████████▏ | 126/151 [00:22<00:04,  5.52it/s]Epoch: 8, train for the 8-th batch, train loss: 0.46197569370269775:   6%|▊             | 7/119 [00:01<00:14,  7.75it/s]Epoch: 8, train for the 8-th batch, train loss: 0.46197569370269775:   7%|▉             | 8/119 [00:01<00:14,  7.82it/s]Epoch: 3, train for the 122-th batch, train loss: 0.5710280537605286:  51%|█████▌     | 121/237 [00:22<00:20,  5.54it/s]Epoch: 3, train for the 122-th batch, train loss: 0.5710280537605286:  51%|█████▋     | 122/237 [00:22<00:20,  5.59it/s]evaluate for the 4-th batch, evaluate loss: 0.6707662343978882:  10%|██                  | 2/20 [00:00<00:01, 12.47it/s]evaluate for the 4-th batch, evaluate loss: 0.6707662343978882:  20%|████                | 4/20 [00:00<00:01, 12.18it/s]Epoch: 4, train for the 71-th batch, train loss: 0.6318390369415283:  29%|███▊         | 70/241 [00:10<00:26,  6.34it/s]Epoch: 8, train for the 9-th batch, train loss: 0.4792579412460327:   7%|█              | 8/119 [00:01<00:14,  7.82it/s]Epoch: 8, train for the 9-th batch, train loss: 0.4792579412460327:   8%|█▏             | 9/119 [00:01<00:14,  7.78it/s]Epoch: 4, train for the 71-th batch, train loss: 0.6318390369415283:  29%|███▊         | 71/241 [00:10<00:25,  6.56it/s]evaluate for the 5-th batch, evaluate loss: 0.6704587340354919:  20%|████                | 4/20 [00:00<00:01, 12.18it/s]Epoch: 5, train for the 127-th batch, train loss: 0.5294411182403564:  83%|█████████▏ | 126/151 [00:23<00:04,  5.52it/s]evaluate for the 6-th batch, evaluate loss: 0.7458938956260681:  20%|████                | 4/20 [00:00<00:01, 12.18it/s]evaluate for the 6-th batch, evaluate loss: 0.7458938956260681:  30%|██████              | 6/20 [00:00<00:01, 12.94it/s]Epoch: 3, train for the 123-th batch, train loss: 0.5350707769393921:  51%|█████▋     | 122/237 [00:22<00:20,  5.59it/s]Epoch: 5, train for the 127-th batch, train loss: 0.5294411182403564:  84%|█████████▎ | 127/151 [00:23<00:04,  5.33it/s]Epoch: 3, train for the 123-th batch, train loss: 0.5350707769393921:  52%|█████▋     | 123/237 [00:22<00:20,  5.54it/s]Epoch: 8, train for the 10-th batch, train loss: 0.4672513008117676:   8%|█             | 9/119 [00:01<00:14,  7.78it/s]Epoch: 4, train for the 72-th batch, train loss: 0.5859437584877014:  29%|███▊         | 71/241 [00:10<00:25,  6.56it/s]Epoch: 8, train for the 10-th batch, train loss: 0.4672513008117676:   8%|█            | 10/119 [00:01<00:13,  7.91it/s]evaluate for the 7-th batch, evaluate loss: 0.7434616088867188:  30%|██████              | 6/20 [00:00<00:01, 12.94it/s]Epoch: 4, train for the 72-th batch, train loss: 0.5859437584877014:  30%|███▉         | 72/241 [00:10<00:24,  6.80it/s]evaluate for the 8-th batch, evaluate loss: 0.6967604160308838:  30%|██████              | 6/20 [00:00<00:01, 12.94it/s]evaluate for the 8-th batch, evaluate loss: 0.6967604160308838:  40%|████████            | 8/20 [00:00<00:00, 13.39it/s]Epoch: 5, train for the 128-th batch, train loss: 0.5124841928482056:  84%|█████████▎ | 127/151 [00:23<00:04,  5.33it/s]Epoch: 5, train for the 128-th batch, train loss: 0.5124841928482056:  85%|█████████▎ | 128/151 [00:23<00:04,  5.43it/s]Epoch: 3, train for the 124-th batch, train loss: 0.49750104546546936:  52%|█████▏    | 123/237 [00:22<00:20,  5.54it/s]Epoch: 8, train for the 11-th batch, train loss: 0.43740278482437134:   8%|█           | 10/119 [00:01<00:13,  7.91it/s]Epoch: 8, train for the 11-th batch, train loss: 0.43740278482437134:   9%|█           | 11/119 [00:01<00:14,  7.64it/s]Epoch: 4, train for the 73-th batch, train loss: 0.4795285761356354:  30%|███▉         | 72/241 [00:10<00:24,  6.80it/s]evaluate for the 9-th batch, evaluate loss: 0.6520814299583435:  40%|████████            | 8/20 [00:00<00:00, 13.39it/s]Epoch: 4, train for the 73-th batch, train loss: 0.4795285761356354:  30%|███▉         | 73/241 [00:10<00:24,  6.84it/s]Epoch: 3, train for the 124-th batch, train loss: 0.49750104546546936:  52%|█████▏    | 124/237 [00:22<00:20,  5.45it/s]evaluate for the 10-th batch, evaluate loss: 0.6495918035507202:  40%|███████▌           | 8/20 [00:00<00:00, 13.39it/s]evaluate for the 10-th batch, evaluate loss: 0.6495918035507202:  50%|█████████         | 10/20 [00:00<00:00, 13.96it/s]Epoch: 8, train for the 12-th batch, train loss: 0.4592307209968567:   9%|█▏           | 11/119 [00:01<00:14,  7.64it/s]Epoch: 8, train for the 12-th batch, train loss: 0.4592307209968567:  10%|█▎           | 12/119 [00:01<00:14,  7.40it/s]evaluate for the 11-th batch, evaluate loss: 0.653134286403656:  50%|█████████▌         | 10/20 [00:00<00:00, 13.96it/s]Epoch: 5, train for the 129-th batch, train loss: 0.5275697112083435:  85%|█████████▎ | 128/151 [00:23<00:04,  5.43it/s]Epoch: 3, train for the 125-th batch, train loss: 0.5049632787704468:  52%|█████▊     | 124/237 [00:22<00:20,  5.45it/s]Epoch: 4, train for the 74-th batch, train loss: 0.46515482664108276:  30%|███▋        | 73/241 [00:10<00:24,  6.84it/s]Epoch: 5, train for the 129-th batch, train loss: 0.5275697112083435:  85%|█████████▍ | 129/151 [00:23<00:04,  5.18it/s]Epoch: 3, train for the 125-th batch, train loss: 0.5049632787704468:  53%|█████▊     | 125/237 [00:22<00:20,  5.40it/s]evaluate for the 12-th batch, evaluate loss: 0.7060258388519287:  50%|█████████         | 10/20 [00:00<00:00, 13.96it/s]evaluate for the 12-th batch, evaluate loss: 0.7060258388519287:  60%|██████████▊       | 12/20 [00:00<00:00, 14.12it/s]Epoch: 4, train for the 74-th batch, train loss: 0.46515482664108276:  31%|███▋        | 74/241 [00:10<00:26,  6.19it/s]Epoch: 8, train for the 13-th batch, train loss: 0.4572811722755432:  10%|█▎           | 12/119 [00:01<00:14,  7.40it/s]Epoch: 8, train for the 13-th batch, train loss: 0.4572811722755432:  11%|█▍           | 13/119 [00:01<00:13,  7.58it/s]evaluate for the 13-th batch, evaluate loss: 0.7083057761192322:  60%|██████████▊       | 12/20 [00:00<00:00, 14.12it/s]evaluate for the 14-th batch, evaluate loss: 0.7277679443359375:  60%|██████████▊       | 12/20 [00:01<00:00, 14.12it/s]evaluate for the 14-th batch, evaluate loss: 0.7277679443359375:  70%|████████████▌     | 14/20 [00:01<00:00, 14.53it/s]Epoch: 5, train for the 130-th batch, train loss: 0.48745086789131165:  85%|████████▌ | 129/151 [00:23<00:04,  5.18it/s]Epoch: 4, train for the 75-th batch, train loss: 0.382811039686203:  31%|████▎         | 74/241 [00:10<00:26,  6.19it/s]Epoch: 5, train for the 130-th batch, train loss: 0.48745086789131165:  86%|████████▌ | 130/151 [00:23<00:04,  5.01it/s]Epoch: 4, train for the 75-th batch, train loss: 0.382811039686203:  31%|████▎         | 75/241 [00:10<00:28,  5.83it/s]Epoch: 8, train for the 14-th batch, train loss: 0.4591500759124756:  11%|█▍           | 13/119 [00:01<00:13,  7.58it/s]Epoch: 8, train for the 14-th batch, train loss: 0.4591500759124756:  12%|█▌           | 14/119 [00:01<00:14,  7.18it/s]evaluate for the 15-th batch, evaluate loss: 0.7261361479759216:  70%|████████████▌     | 14/20 [00:01<00:00, 14.53it/s]evaluate for the 16-th batch, evaluate loss: 0.6730799674987793:  70%|████████████▌     | 14/20 [00:01<00:00, 14.53it/s]evaluate for the 16-th batch, evaluate loss: 0.6730799674987793:  80%|██████████████▍   | 16/20 [00:01<00:00, 13.77it/s]Epoch: 3, train for the 126-th batch, train loss: 0.5310660600662231:  53%|█████▊     | 125/237 [00:23<00:20,  5.40it/s]Epoch: 8, train for the 15-th batch, train loss: 0.4471296966075897:  12%|█▌           | 14/119 [00:01<00:14,  7.18it/s]Epoch: 5, train for the 131-th batch, train loss: 0.47977933287620544:  86%|████████▌ | 130/151 [00:23<00:04,  5.01it/s]Epoch: 8, train for the 15-th batch, train loss: 0.4471296966075897:  13%|█▋           | 15/119 [00:01<00:15,  6.91it/s]Epoch: 4, train for the 76-th batch, train loss: 0.2900677025318146:  31%|████         | 75/241 [00:10<00:28,  5.83it/s]Epoch: 3, train for the 126-th batch, train loss: 0.5310660600662231:  53%|█████▊     | 126/237 [00:23<00:26,  4.13it/s]evaluate for the 17-th batch, evaluate loss: 0.7248331904411316:  80%|██████████████▍   | 16/20 [00:01<00:00, 13.77it/s]Epoch: 5, train for the 131-th batch, train loss: 0.47977933287620544:  87%|████████▋ | 131/151 [00:23<00:03,  5.03it/s]Epoch: 4, train for the 76-th batch, train loss: 0.2900677025318146:  32%|████         | 76/241 [00:10<00:29,  5.58it/s]evaluate for the 18-th batch, evaluate loss: 0.6863608956336975:  80%|██████████████▍   | 16/20 [00:01<00:00, 13.77it/s]evaluate for the 18-th batch, evaluate loss: 0.6863608956336975:  90%|████████████████▏ | 18/20 [00:01<00:00, 13.50it/s]Epoch: 8, train for the 16-th batch, train loss: 0.4827582538127899:  13%|█▋           | 15/119 [00:02<00:15,  6.91it/s]Epoch: 8, train for the 16-th batch, train loss: 0.4827582538127899:  13%|█▋           | 16/119 [00:02<00:14,  7.01it/s]evaluate for the 19-th batch, evaluate loss: 0.744178295135498:  90%|█████████████████  | 18/20 [00:01<00:00, 13.50it/s]Epoch: 3, train for the 127-th batch, train loss: 0.5362353920936584:  53%|█████▊     | 126/237 [00:23<00:26,  4.13it/s]Epoch: 5, train for the 132-th batch, train loss: 0.514639139175415:  87%|██████████▍ | 131/151 [00:24<00:03,  5.03it/s]Epoch: 3, train for the 127-th batch, train loss: 0.5362353920936584:  54%|█████▉     | 127/237 [00:23<00:24,  4.49it/s]evaluate for the 20-th batch, evaluate loss: 0.7034838795661926:  90%|████████████████▏ | 18/20 [00:01<00:00, 13.50it/s]evaluate for the 20-th batch, evaluate loss: 0.7034838795661926: 100%|██████████████████| 20/20 [00:01<00:00, 14.37it/s]evaluate for the 20-th batch, evaluate loss: 0.7034838795661926: 100%|██████████████████| 20/20 [00:01<00:00, 13.82it/s]
Epoch: 5, train for the 132-th batch, train loss: 0.514639139175415:  87%|██████████▍ | 132/151 [00:24<00:03,  5.21it/s]Epoch: 8, train for the 17-th batch, train loss: 0.4153013527393341:  13%|█▋           | 16/119 [00:02<00:14,  7.01it/s]Epoch: 8, train for the 17-th batch, train loss: 0.4153013527393341:  14%|█▊           | 17/119 [00:02<00:15,  6.66it/s]Epoch: 5, train for the 133-th batch, train loss: 0.429869681596756:  87%|██████████▍ | 132/151 [00:24<00:03,  5.21it/s]Epoch: 3, train for the 128-th batch, train loss: 0.526704728603363:  54%|██████▍     | 127/237 [00:23<00:24,  4.49it/s]Epoch: 5, train for the 133-th batch, train loss: 0.429869681596756:  88%|██████████▌ | 133/151 [00:24<00:03,  5.67it/s]Epoch: 3, train for the 128-th batch, train loss: 0.526704728603363:  54%|██████▍     | 128/237 [00:23<00:22,  4.78it/s]Epoch: 8, train for the 18-th batch, train loss: 0.4322052001953125:  14%|█▊           | 17/119 [00:02<00:15,  6.66it/s]Epoch: 4, train for the 77-th batch, train loss: 0.35836562514305115:  32%|███▊        | 76/241 [00:11<00:29,  5.58it/s]Epoch: 4, train for the 77-th batch, train loss: 0.35836562514305115:  32%|███▊        | 77/241 [00:11<00:40,  4.02it/s]Epoch: 5, train for the 134-th batch, train loss: 0.45664846897125244:  88%|████████▊ | 133/151 [00:24<00:03,  5.67it/s]INFO:root:Epoch: 6, learning rate: 0.0001, train loss: 0.5155
INFO:root:train average_precision, 0.8475
INFO:root:train roc_auc, 0.8351
INFO:root:validate loss: 0.4683
INFO:root:validate average_precision, 0.8643
INFO:root:validate roc_auc, 0.8600
INFO:root:new node validate loss: 0.6999
INFO:root:new node validate first_1_average_precision, 0.5661
INFO:root:new node validate first_1_roc_auc, 0.5016
INFO:root:new node validate first_3_average_precision, 0.6007
INFO:root:new node validate first_3_roc_auc, 0.5581
INFO:root:new node validate first_10_average_precision, 0.6265
INFO:root:new node validate first_10_roc_auc, 0.6157
INFO:root:new node validate average_precision, 0.6765
INFO:root:new node validate roc_auc, 0.6725
Epoch: 5, train for the 134-th batch, train loss: 0.45664846897125244:  89%|████████▊ | 134/151 [00:24<00:02,  5.84it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]Epoch: 3, train for the 129-th batch, train loss: 0.5674059391021729:  54%|█████▉     | 128/237 [00:23<00:22,  4.78it/s]Epoch: 8, train for the 19-th batch, train loss: 0.42464667558670044:  14%|█▋          | 17/119 [00:02<00:15,  6.66it/s]Epoch: 8, train for the 19-th batch, train loss: 0.42464667558670044:  16%|█▉          | 19/119 [00:02<00:13,  7.55it/s]Epoch: 3, train for the 129-th batch, train loss: 0.5674059391021729:  54%|█████▉     | 129/237 [00:23<00:21,  5.02it/s]Epoch: 4, train for the 78-th batch, train loss: 0.5814698934555054:  32%|████▏        | 77/241 [00:11<00:40,  4.02it/s]Epoch: 4, train for the 78-th batch, train loss: 0.5814698934555054:  32%|████▏        | 78/241 [00:11<00:34,  4.74it/s]Epoch: 7, train for the 1-th batch, train loss: 0.6266597509384155:   0%|                       | 0/146 [00:00<?, ?it/s]Epoch: 8, train for the 20-th batch, train loss: 0.4256947338581085:  16%|██           | 19/119 [00:02<00:13,  7.55it/s]Epoch: 8, train for the 20-th batch, train loss: 0.4256947338581085:  17%|██▏          | 20/119 [00:02<00:12,  7.71it/s]Epoch: 5, train for the 135-th batch, train loss: 0.4863748550415039:  89%|█████████▊ | 134/151 [00:24<00:02,  5.84it/s]Epoch: 7, train for the 2-th batch, train loss: 0.5469359159469604:   0%|                       | 0/146 [00:00<?, ?it/s]Epoch: 7, train for the 2-th batch, train loss: 0.5469359159469604:   1%|▏              | 2/146 [00:00<00:12, 11.23it/s]Epoch: 5, train for the 135-th batch, train loss: 0.4863748550415039:  89%|█████████▊ | 135/151 [00:24<00:02,  5.63it/s]Epoch: 3, train for the 130-th batch, train loss: 0.540523111820221:  54%|██████▌     | 129/237 [00:24<00:21,  5.02it/s]Epoch: 4, train for the 79-th batch, train loss: 0.47493529319763184:  32%|███▉        | 78/241 [00:11<00:34,  4.74it/s]Epoch: 4, train for the 79-th batch, train loss: 0.47493529319763184:  33%|███▉        | 79/241 [00:11<00:31,  5.16it/s]Epoch: 3, train for the 130-th batch, train loss: 0.540523111820221:  55%|██████▌     | 130/237 [00:24<00:20,  5.12it/s]Epoch: 8, train for the 21-th batch, train loss: 0.5569852590560913:  17%|██▏          | 20/119 [00:02<00:12,  7.71it/s]Epoch: 8, train for the 21-th batch, train loss: 0.5569852590560913:  18%|██▎          | 21/119 [00:02<00:12,  7.58it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/383 [00:00<?, ?it/s]Epoch: 7, train for the 3-th batch, train loss: 0.5080335736274719:   1%|▏              | 2/146 [00:00<00:12, 11.23it/s]Epoch: 3, train for the 131-th batch, train loss: 0.5190297365188599:  55%|██████     | 130/237 [00:24<00:20,  5.12it/s]Epoch: 5, train for the 136-th batch, train loss: 0.5175313949584961:  89%|█████████▊ | 135/151 [00:24<00:02,  5.63it/s]Epoch: 4, train for the 80-th batch, train loss: 0.3937947452068329:  33%|████▎        | 79/241 [00:11<00:31,  5.16it/s]Epoch: 3, train for the 131-th batch, train loss: 0.5190297365188599:  55%|██████     | 131/237 [00:24<00:19,  5.40it/s]Epoch: 4, train for the 80-th batch, train loss: 0.3937947452068329:  33%|████▎        | 80/241 [00:11<00:30,  5.32it/s]Epoch: 5, train for the 136-th batch, train loss: 0.5175313949584961:  90%|█████████▉ | 136/151 [00:24<00:02,  5.43it/s]Epoch: 2, train for the 1-th batch, train loss: 0.48160260915756226:   0%|                      | 0/383 [00:00<?, ?it/s]Epoch: 8, train for the 22-th batch, train loss: 0.48263195157051086:  18%|██          | 21/119 [00:02<00:12,  7.58it/s]Epoch: 2, train for the 1-th batch, train loss: 0.48160260915756226:   0%|              | 1/383 [00:00<00:44,  8.68it/s]Epoch: 8, train for the 22-th batch, train loss: 0.48263195157051086:  18%|██▏         | 22/119 [00:02<00:13,  7.32it/s]Epoch: 7, train for the 4-th batch, train loss: 0.532755434513092:   1%|▏               | 2/146 [00:00<00:12, 11.23it/s]Epoch: 7, train for the 4-th batch, train loss: 0.532755434513092:   3%|▍               | 4/146 [00:00<00:17,  8.32it/s]Epoch: 4, train for the 81-th batch, train loss: 0.4140476882457733:  33%|████▎        | 80/241 [00:11<00:30,  5.32it/s]Epoch: 4, train for the 81-th batch, train loss: 0.4140476882457733:  34%|████▎        | 81/241 [00:11<00:27,  5.85it/s]Epoch: 5, train for the 137-th batch, train loss: 0.4814327657222748:  90%|█████████▉ | 136/151 [00:24<00:02,  5.43it/s]Epoch: 5, train for the 137-th batch, train loss: 0.4814327657222748:  91%|█████████▉ | 137/151 [00:24<00:02,  5.50it/s]Epoch: 3, train for the 132-th batch, train loss: 0.5181035399436951:  55%|██████     | 131/237 [00:24<00:19,  5.40it/s]Epoch: 2, train for the 2-th batch, train loss: 0.390541672706604:   0%|                | 1/383 [00:00<00:44,  8.68it/s]Epoch: 7, train for the 5-th batch, train loss: 0.5265472531318665:   3%|▍              | 4/146 [00:00<00:17,  8.32it/s]Epoch: 2, train for the 2-th batch, train loss: 0.390541672706604:   1%|                | 2/383 [00:00<00:57,  6.64it/s]Epoch: 7, train for the 5-th batch, train loss: 0.5265472531318665:   3%|▌              | 5/146 [00:00<00:17,  7.92it/s]Epoch: 3, train for the 132-th batch, train loss: 0.5181035399436951:  56%|██████▏    | 132/237 [00:24<00:20,  5.08it/s]Epoch: 8, train for the 23-th batch, train loss: 0.5174127817153931:  18%|██▍          | 22/119 [00:03<00:13,  7.32it/s]Epoch: 8, train for the 23-th batch, train loss: 0.5174127817153931:  19%|██▌          | 23/119 [00:03<00:14,  6.78it/s]Epoch: 4, train for the 82-th batch, train loss: 0.422799289226532:  34%|████▋         | 81/241 [00:12<00:27,  5.85it/s]Epoch: 4, train for the 82-th batch, train loss: 0.422799289226532:  34%|████▊         | 82/241 [00:12<00:25,  6.13it/s]Epoch: 7, train for the 6-th batch, train loss: 0.5515868067741394:   3%|▌              | 5/146 [00:00<00:17,  7.92it/s]Epoch: 7, train for the 6-th batch, train loss: 0.5515868067741394:   4%|▌              | 6/146 [00:00<00:17,  8.03it/s]Epoch: 5, train for the 138-th batch, train loss: 0.5094433426856995:  91%|█████████▉ | 137/151 [00:25<00:02,  5.50it/s]Epoch: 5, train for the 138-th batch, train loss: 0.5094433426856995:  91%|██████████ | 138/151 [00:25<00:02,  5.44it/s]Epoch: 8, train for the 24-th batch, train loss: 0.4179685413837433:  19%|██▌          | 23/119 [00:03<00:14,  6.78it/s]Epoch: 2, train for the 3-th batch, train loss: 0.6615683436393738:   1%|               | 2/383 [00:00<00:57,  6.64it/s]Epoch: 2, train for the 3-th batch, train loss: 0.6615683436393738:   1%|               | 3/383 [00:00<01:01,  6.16it/s]Epoch: 8, train for the 24-th batch, train loss: 0.4179685413837433:  20%|██▌          | 24/119 [00:03<00:14,  6.46it/s]Epoch: 4, train for the 83-th batch, train loss: 0.5187658071517944:  34%|████▍        | 82/241 [00:12<00:25,  6.13it/s]Epoch: 4, train for the 83-th batch, train loss: 0.5187658071517944:  34%|████▍        | 83/241 [00:12<00:24,  6.37it/s]Epoch: 3, train for the 133-th batch, train loss: 0.5149787664413452:  56%|██████▏    | 132/237 [00:24<00:20,  5.08it/s]Epoch: 7, train for the 7-th batch, train loss: 0.528814971446991:   4%|▋               | 6/146 [00:00<00:17,  8.03it/s]Epoch: 7, train for the 7-th batch, train loss: 0.528814971446991:   5%|▊               | 7/146 [00:00<00:16,  8.40it/s]Epoch: 3, train for the 133-th batch, train loss: 0.5149787664413452:  56%|██████▏    | 133/237 [00:24<00:21,  4.73it/s]Epoch: 2, train for the 4-th batch, train loss: 0.21175803244113922:   1%|              | 3/383 [00:00<01:01,  6.16it/s]Epoch: 5, train for the 139-th batch, train loss: 0.5216007232666016:  91%|██████████ | 138/151 [00:25<00:02,  5.44it/s]Epoch: 2, train for the 4-th batch, train loss: 0.21175803244113922:   1%|▏             | 4/383 [00:00<01:01,  6.14it/s]Epoch: 8, train for the 25-th batch, train loss: 0.5312171578407288:  20%|██▌          | 24/119 [00:03<00:14,  6.46it/s]Epoch: 8, train for the 25-th batch, train loss: 0.5312171578407288:  21%|██▋          | 25/119 [00:03<00:14,  6.34it/s]Epoch: 5, train for the 139-th batch, train loss: 0.5216007232666016:  92%|██████████▏| 139/151 [00:25<00:02,  5.20it/s]Epoch: 7, train for the 8-th batch, train loss: 0.48705804347991943:   5%|▋             | 7/146 [00:00<00:16,  8.40it/s]Epoch: 7, train for the 8-th batch, train loss: 0.48705804347991943:   5%|▊             | 8/146 [00:00<00:17,  7.75it/s]Epoch: 4, train for the 84-th batch, train loss: 0.5980247259140015:  34%|████▍        | 83/241 [00:12<00:24,  6.37it/s]Epoch: 4, train for the 84-th batch, train loss: 0.5980247259140015:  35%|████▌        | 84/241 [00:12<00:27,  5.72it/s]Epoch: 3, train for the 134-th batch, train loss: 0.5804784297943115:  56%|██████▏    | 133/237 [00:24<00:21,  4.73it/s]Epoch: 3, train for the 134-th batch, train loss: 0.5804784297943115:  57%|██████▏    | 134/237 [00:24<00:21,  4.73it/s]Epoch: 8, train for the 26-th batch, train loss: 0.37275955080986023:  21%|██▌         | 25/119 [00:03<00:14,  6.34it/s]Epoch: 2, train for the 5-th batch, train loss: 0.3353661000728607:   1%|▏              | 4/383 [00:00<01:01,  6.14it/s]Epoch: 8, train for the 26-th batch, train loss: 0.37275955080986023:  22%|██▌         | 26/119 [00:03<00:14,  6.62it/s]Epoch: 2, train for the 5-th batch, train loss: 0.3353661000728607:   1%|▏              | 5/383 [00:00<00:59,  6.37it/s]Epoch: 7, train for the 9-th batch, train loss: 0.4541870057582855:   5%|▊              | 8/146 [00:01<00:17,  7.75it/s]Epoch: 7, train for the 9-th batch, train loss: 0.4541870057582855:   6%|▉              | 9/146 [00:01<00:18,  7.56it/s]Epoch: 5, train for the 140-th batch, train loss: 0.46490493416786194:  92%|█████████▏| 139/151 [00:25<00:02,  5.20it/s]Epoch: 5, train for the 140-th batch, train loss: 0.46490493416786194:  93%|█████████▎| 140/151 [00:25<00:02,  5.12it/s]Epoch: 2, train for the 6-th batch, train loss: 0.3817363679409027:   1%|▏              | 5/383 [00:00<00:59,  6.37it/s]Epoch: 8, train for the 27-th batch, train loss: 0.46532902121543884:  22%|██▌         | 26/119 [00:03<00:14,  6.62it/s]Epoch: 8, train for the 27-th batch, train loss: 0.46532902121543884:  23%|██▋         | 27/119 [00:03<00:13,  6.77it/s]Epoch: 2, train for the 6-th batch, train loss: 0.3817363679409027:   2%|▏              | 6/383 [00:00<00:56,  6.65it/s]Epoch: 4, train for the 85-th batch, train loss: 0.4570779800415039:  35%|████▌        | 84/241 [00:12<00:27,  5.72it/s]Epoch: 7, train for the 10-th batch, train loss: 0.4839193522930145:   6%|▊             | 9/146 [00:01<00:18,  7.56it/s]Epoch: 7, train for the 10-th batch, train loss: 0.4839193522930145:   7%|▉            | 10/146 [00:01<00:18,  7.39it/s]Epoch: 4, train for the 85-th batch, train loss: 0.4570779800415039:  35%|████▌        | 85/241 [00:12<00:30,  5.08it/s]Epoch: 5, train for the 141-th batch, train loss: 0.504109799861908:  93%|███████████▏| 140/151 [00:25<00:02,  5.12it/s]Epoch: 5, train for the 141-th batch, train loss: 0.504109799861908:  93%|███████████▏| 141/151 [00:25<00:01,  5.38it/s]Epoch: 8, train for the 28-th batch, train loss: 0.4852133095264435:  23%|██▉          | 27/119 [00:03<00:13,  6.77it/s]Epoch: 8, train for the 28-th batch, train loss: 0.4852133095264435:  24%|███          | 28/119 [00:03<00:12,  7.07it/s]Epoch: 2, train for the 7-th batch, train loss: 0.4429435729980469:   2%|▏              | 6/383 [00:01<00:56,  6.65it/s]Epoch: 2, train for the 7-th batch, train loss: 0.4429435729980469:   2%|▎              | 7/383 [00:01<00:54,  6.89it/s]Epoch: 7, train for the 11-th batch, train loss: 0.5304566025733948:   7%|▉            | 10/146 [00:01<00:18,  7.39it/s]Epoch: 7, train for the 11-th batch, train loss: 0.5304566025733948:   8%|▉            | 11/146 [00:01<00:17,  7.53it/s]Epoch: 4, train for the 86-th batch, train loss: 0.4783058762550354:  35%|████▌        | 85/241 [00:12<00:30,  5.08it/s]Epoch: 3, train for the 135-th batch, train loss: 0.5803992748260498:  57%|██████▏    | 134/237 [00:25<00:21,  4.73it/s]Epoch: 4, train for the 86-th batch, train loss: 0.4783058762550354:  36%|████▋        | 86/241 [00:12<00:28,  5.39it/s]Epoch: 3, train for the 135-th batch, train loss: 0.5803992748260498:  57%|██████▎    | 135/237 [00:25<00:27,  3.77it/s]Epoch: 8, train for the 29-th batch, train loss: 0.450046569108963:  24%|███▎          | 28/119 [00:03<00:12,  7.07it/s]Epoch: 2, train for the 8-th batch, train loss: 0.2638495862483978:   2%|▎              | 7/383 [00:01<00:54,  6.89it/s]Epoch: 5, train for the 142-th batch, train loss: 0.46064695715904236:  93%|█████████▎| 141/151 [00:25<00:01,  5.38it/s]Epoch: 8, train for the 29-th batch, train loss: 0.450046569108963:  24%|███▍          | 29/119 [00:03<00:13,  6.92it/s]Epoch: 2, train for the 8-th batch, train loss: 0.2638495862483978:   2%|▎              | 8/383 [00:01<00:54,  6.85it/s]Epoch: 5, train for the 142-th batch, train loss: 0.46064695715904236:  94%|█████████▍| 142/151 [00:25<00:01,  5.33it/s]Epoch: 7, train for the 12-th batch, train loss: 0.5044945478439331:   8%|▉            | 11/146 [00:01<00:17,  7.53it/s]Epoch: 7, train for the 12-th batch, train loss: 0.5044945478439331:   8%|█            | 12/146 [00:01<00:18,  7.32it/s]Epoch: 4, train for the 87-th batch, train loss: 0.5391029715538025:  36%|████▋        | 86/241 [00:12<00:28,  5.39it/s]Epoch: 4, train for the 87-th batch, train loss: 0.5391029715538025:  36%|████▋        | 87/241 [00:13<00:26,  5.77it/s]Epoch: 3, train for the 136-th batch, train loss: 0.597563624382019:  57%|██████▊     | 135/237 [00:25<00:27,  3.77it/s]Epoch: 3, train for the 136-th batch, train loss: 0.597563624382019:  57%|██████▉     | 136/237 [00:25<00:24,  4.15it/s]Epoch: 2, train for the 9-th batch, train loss: 0.4148939847946167:   2%|▎              | 8/383 [00:01<00:54,  6.85it/s]Epoch: 2, train for the 9-th batch, train loss: 0.4148939847946167:   2%|▎              | 9/383 [00:01<00:54,  6.85it/s]Epoch: 8, train for the 30-th batch, train loss: 0.47073790431022644:  24%|██▉         | 29/119 [00:04<00:13,  6.92it/s]Epoch: 8, train for the 30-th batch, train loss: 0.47073790431022644:  25%|███         | 30/119 [00:04<00:13,  6.39it/s]Epoch: 5, train for the 143-th batch, train loss: 0.40663081407546997:  94%|█████████▍| 142/151 [00:26<00:01,  5.33it/s]Epoch: 7, train for the 13-th batch, train loss: 0.5126539468765259:   8%|█            | 12/146 [00:01<00:18,  7.32it/s]Epoch: 7, train for the 13-th batch, train loss: 0.5126539468765259:   9%|█▏           | 13/146 [00:01<00:19,  6.92it/s]Epoch: 5, train for the 143-th batch, train loss: 0.40663081407546997:  95%|█████████▍| 143/151 [00:26<00:01,  5.30it/s]Epoch: 4, train for the 88-th batch, train loss: 0.5083379745483398:  36%|████▋        | 87/241 [00:13<00:26,  5.77it/s]Epoch: 4, train for the 88-th batch, train loss: 0.5083379745483398:  37%|████▋        | 88/241 [00:13<00:26,  5.80it/s]Epoch: 2, train for the 10-th batch, train loss: 0.39830631017684937:   2%|▎            | 9/383 [00:01<00:54,  6.85it/s]Epoch: 2, train for the 10-th batch, train loss: 0.39830631017684937:   3%|▎           | 10/383 [00:01<00:55,  6.73it/s]Epoch: 7, train for the 14-th batch, train loss: 0.47625476121902466:   9%|█           | 13/146 [00:01<00:19,  6.92it/s]Epoch: 3, train for the 137-th batch, train loss: 0.5872565507888794:  57%|██████▎    | 136/237 [00:25<00:24,  4.15it/s]Epoch: 7, train for the 14-th batch, train loss: 0.47625476121902466:  10%|█▏          | 14/146 [00:01<00:19,  6.87it/s]Epoch: 8, train for the 31-th batch, train loss: 0.4414728879928589:  25%|███▎         | 30/119 [00:04<00:13,  6.39it/s]Epoch: 8, train for the 31-th batch, train loss: 0.4414728879928589:  26%|███▍         | 31/119 [00:04<00:14,  6.06it/s]Epoch: 5, train for the 144-th batch, train loss: 0.43342161178588867:  95%|█████████▍| 143/151 [00:26<00:01,  5.30it/s]Epoch: 3, train for the 137-th batch, train loss: 0.5872565507888794:  58%|██████▎    | 137/237 [00:25<00:24,  4.06it/s]Epoch: 4, train for the 89-th batch, train loss: 0.5136489272117615:  37%|████▋        | 88/241 [00:13<00:26,  5.80it/s]Epoch: 5, train for the 144-th batch, train loss: 0.43342161178588867:  95%|█████████▌| 144/151 [00:26<00:01,  5.32it/s]Epoch: 4, train for the 89-th batch, train loss: 0.5136489272117615:  37%|████▊        | 89/241 [00:13<00:26,  5.82it/s]Epoch: 2, train for the 11-th batch, train loss: 0.3346676826477051:   3%|▎            | 10/383 [00:01<00:55,  6.73it/s]Epoch: 2, train for the 11-th batch, train loss: 0.3346676826477051:   3%|▎            | 11/383 [00:01<00:53,  7.00it/s]Epoch: 7, train for the 15-th batch, train loss: 0.46960434317588806:  10%|█▏          | 14/146 [00:01<00:19,  6.87it/s]Epoch: 7, train for the 15-th batch, train loss: 0.46960434317588806:  10%|█▏          | 15/146 [00:01<00:17,  7.47it/s]Epoch: 4, train for the 90-th batch, train loss: 0.4845125377178192:  37%|████▊        | 89/241 [00:13<00:26,  5.82it/s]Epoch: 7, train for the 16-th batch, train loss: 0.4788599908351898:  10%|█▎           | 15/146 [00:02<00:17,  7.47it/s]Epoch: 4, train for the 90-th batch, train loss: 0.4845125377178192:  37%|████▊        | 90/241 [00:13<00:24,  6.15it/s]Epoch: 7, train for the 16-th batch, train loss: 0.4788599908351898:  11%|█▍           | 16/146 [00:02<00:16,  8.05it/s]Epoch: 5, train for the 145-th batch, train loss: 0.4193958044052124:  95%|██████████▍| 144/151 [00:26<00:01,  5.32it/s]Epoch: 5, train for the 145-th batch, train loss: 0.4193958044052124:  96%|██████████▌| 145/151 [00:26<00:01,  5.11it/s]Epoch: 8, train for the 32-th batch, train loss: 0.39137038588523865:  26%|███▏        | 31/119 [00:04<00:14,  6.06it/s]Epoch: 8, train for the 32-th batch, train loss: 0.39137038588523865:  27%|███▏        | 32/119 [00:04<00:16,  5.19it/s]Epoch: 3, train for the 138-th batch, train loss: 0.557673454284668:  58%|██████▉     | 137/237 [00:25<00:24,  4.06it/s]Epoch: 2, train for the 12-th batch, train loss: 0.34618422389030457:   3%|▎           | 11/383 [00:01<00:53,  7.00it/s]Epoch: 7, train for the 17-th batch, train loss: 0.44055190682411194:  11%|█▎          | 16/146 [00:02<00:16,  8.05it/s]Epoch: 2, train for the 12-th batch, train loss: 0.34618422389030457:   3%|▍           | 12/383 [00:01<01:02,  5.93it/s]Epoch: 7, train for the 17-th batch, train loss: 0.44055190682411194:  12%|█▍          | 17/146 [00:02<00:15,  8.32it/s]Epoch: 4, train for the 91-th batch, train loss: 0.4952271282672882:  37%|████▊        | 90/241 [00:13<00:24,  6.15it/s]Epoch: 3, train for the 138-th batch, train loss: 0.557673454284668:  58%|██████▉     | 138/237 [00:26<00:25,  3.88it/s]Epoch: 4, train for the 91-th batch, train loss: 0.4952271282672882:  38%|████▉        | 91/241 [00:13<00:22,  6.52it/s]Epoch: 8, train for the 33-th batch, train loss: 0.4428870975971222:  27%|███▍         | 32/119 [00:04<00:16,  5.19it/s]Epoch: 5, train for the 146-th batch, train loss: 0.4217459261417389:  96%|██████████▌| 145/151 [00:26<00:01,  5.11it/s]Epoch: 8, train for the 33-th batch, train loss: 0.4428870975971222:  28%|███▌         | 33/119 [00:04<00:15,  5.45it/s]Epoch: 7, train for the 18-th batch, train loss: 0.4441022276878357:  12%|█▌           | 17/146 [00:02<00:15,  8.32it/s]Epoch: 7, train for the 18-th batch, train loss: 0.4441022276878357:  12%|█▌           | 18/146 [00:02<00:15,  8.01it/s]Epoch: 4, train for the 92-th batch, train loss: 0.38586241006851196:  38%|████▌       | 91/241 [00:13<00:22,  6.52it/s]Epoch: 4, train for the 92-th batch, train loss: 0.38586241006851196:  38%|████▌       | 92/241 [00:13<00:22,  6.66it/s]Epoch: 5, train for the 146-th batch, train loss: 0.4217459261417389:  97%|██████████▋| 146/151 [00:26<00:01,  4.97it/s]Epoch: 3, train for the 139-th batch, train loss: 0.5443738698959351:  58%|██████▍    | 138/237 [00:26<00:25,  3.88it/s]Epoch: 3, train for the 139-th batch, train loss: 0.5443738698959351:  59%|██████▍    | 139/237 [00:26<00:23,  4.17it/s]Epoch: 7, train for the 19-th batch, train loss: 0.4929748773574829:  12%|█▌           | 18/146 [00:02<00:15,  8.01it/s]Epoch: 7, train for the 19-th batch, train loss: 0.4929748773574829:  13%|█▋           | 19/146 [00:02<00:15,  8.16it/s]Epoch: 4, train for the 93-th batch, train loss: 0.4169031083583832:  38%|████▉        | 92/241 [00:13<00:22,  6.66it/s]Epoch: 4, train for the 93-th batch, train loss: 0.4169031083583832:  39%|█████        | 93/241 [00:13<00:21,  6.86it/s]Epoch: 8, train for the 34-th batch, train loss: 0.41964203119277954:  28%|███▎        | 33/119 [00:04<00:15,  5.45it/s]Epoch: 2, train for the 13-th batch, train loss: 0.3606823682785034:   3%|▍            | 12/383 [00:02<01:02,  5.93it/s]Epoch: 8, train for the 34-th batch, train loss: 0.41964203119277954:  29%|███▍        | 34/119 [00:04<00:15,  5.41it/s]Epoch: 2, train for the 13-th batch, train loss: 0.3606823682785034:   3%|▍            | 13/383 [00:02<01:18,  4.70it/s]Epoch: 5, train for the 147-th batch, train loss: 0.42793890833854675:  97%|█████████▋| 146/151 [00:26<00:01,  4.97it/s]Epoch: 7, train for the 20-th batch, train loss: 0.5046144723892212:  13%|█▋           | 19/146 [00:02<00:15,  8.16it/s]Epoch: 3, train for the 140-th batch, train loss: 0.5528416037559509:  59%|██████▍    | 139/237 [00:26<00:23,  4.17it/s]Epoch: 7, train for the 20-th batch, train loss: 0.5046144723892212:  14%|█▊           | 20/146 [00:02<00:14,  8.42it/s]Epoch: 5, train for the 147-th batch, train loss: 0.42793890833854675:  97%|█████████▋| 147/151 [00:26<00:00,  4.92it/s]Epoch: 3, train for the 140-th batch, train loss: 0.5528416037559509:  59%|██████▍    | 140/237 [00:26<00:21,  4.50it/s]Epoch: 4, train for the 94-th batch, train loss: 0.3922291398048401:  39%|█████        | 93/241 [00:14<00:21,  6.86it/s]Epoch: 4, train for the 94-th batch, train loss: 0.3922291398048401:  39%|█████        | 94/241 [00:14<00:21,  6.85it/s]Epoch: 8, train for the 35-th batch, train loss: 0.44059446454048157:  29%|███▍        | 34/119 [00:05<00:15,  5.41it/s]Epoch: 2, train for the 14-th batch, train loss: 0.4338420331478119:   3%|▍            | 13/383 [00:02<01:18,  4.70it/s]Epoch: 8, train for the 35-th batch, train loss: 0.44059446454048157:  29%|███▌        | 35/119 [00:05<00:15,  5.51it/s]Epoch: 2, train for the 14-th batch, train loss: 0.4338420331478119:   4%|▍            | 14/383 [00:02<01:14,  4.96it/s]Epoch: 7, train for the 21-th batch, train loss: 0.4769655466079712:  14%|█▊           | 20/146 [00:02<00:14,  8.42it/s]Epoch: 7, train for the 21-th batch, train loss: 0.4769655466079712:  14%|█▊           | 21/146 [00:02<00:16,  7.72it/s]Epoch: 5, train for the 148-th batch, train loss: 0.45336753129959106:  97%|█████████▋| 147/151 [00:27<00:00,  4.92it/s]Epoch: 5, train for the 148-th batch, train loss: 0.45336753129959106:  98%|█████████▊| 148/151 [00:27<00:00,  5.14it/s]Epoch: 4, train for the 95-th batch, train loss: 0.40231186151504517:  39%|████▋       | 94/241 [00:14<00:21,  6.85it/s]Epoch: 3, train for the 141-th batch, train loss: 0.5826566815376282:  59%|██████▍    | 140/237 [00:26<00:21,  4.50it/s]Epoch: 4, train for the 95-th batch, train loss: 0.40231186151504517:  39%|████▋       | 95/241 [00:14<00:20,  7.18it/s]Epoch: 3, train for the 141-th batch, train loss: 0.5826566815376282:  59%|██████▌    | 141/237 [00:26<00:20,  4.70it/s]Epoch: 8, train for the 36-th batch, train loss: 0.37524569034576416:  29%|███▌        | 35/119 [00:05<00:15,  5.51it/s]Epoch: 8, train for the 36-th batch, train loss: 0.37524569034576416:  30%|███▋        | 36/119 [00:05<00:13,  5.99it/s]Epoch: 2, train for the 15-th batch, train loss: 0.3945555090904236:   4%|▍            | 14/383 [00:02<01:14,  4.96it/s]Epoch: 7, train for the 22-th batch, train loss: 0.47562360763549805:  14%|█▋          | 21/146 [00:02<00:16,  7.72it/s]Epoch: 2, train for the 15-th batch, train loss: 0.3945555090904236:   4%|▌            | 15/383 [00:02<01:10,  5.22it/s]Epoch: 7, train for the 22-th batch, train loss: 0.47562360763549805:  15%|█▊          | 22/146 [00:02<00:16,  7.53it/s]Epoch: 3, train for the 142-th batch, train loss: 0.57956463098526:  59%|███████▋     | 141/237 [00:26<00:20,  4.70it/s]Epoch: 5, train for the 149-th batch, train loss: 0.42292752861976624:  98%|█████████▊| 148/151 [00:27<00:00,  5.14it/s]Epoch: 4, train for the 96-th batch, train loss: 0.5231416821479797:  39%|█████        | 95/241 [00:14<00:20,  7.18it/s]Epoch: 8, train for the 37-th batch, train loss: 0.4382306635379791:  30%|███▉         | 36/119 [00:05<00:13,  5.99it/s]Epoch: 3, train for the 142-th batch, train loss: 0.57956463098526:  60%|███████▊     | 142/237 [00:26<00:19,  4.89it/s]Epoch: 8, train for the 37-th batch, train loss: 0.4382306635379791:  31%|████         | 37/119 [00:05<00:13,  6.16it/s]Epoch: 4, train for the 96-th batch, train loss: 0.5231416821479797:  40%|█████▏       | 96/241 [00:14<00:23,  6.27it/s]Epoch: 2, train for the 16-th batch, train loss: 0.46715182065963745:   4%|▍           | 15/383 [00:02<01:10,  5.22it/s]Epoch: 5, train for the 149-th batch, train loss: 0.42292752861976624:  99%|█████████▊| 149/151 [00:27<00:00,  4.80it/s]Epoch: 2, train for the 16-th batch, train loss: 0.46715182065963745:   4%|▌           | 16/383 [00:02<01:04,  5.72it/s]Epoch: 7, train for the 23-th batch, train loss: 0.5142872333526611:  15%|█▉           | 22/146 [00:02<00:16,  7.53it/s]Epoch: 7, train for the 23-th batch, train loss: 0.5142872333526611:  16%|██           | 23/146 [00:02<00:16,  7.29it/s]Epoch: 4, train for the 97-th batch, train loss: 0.3593794107437134:  40%|█████▏       | 96/241 [00:14<00:23,  6.27it/s]Epoch: 4, train for the 97-th batch, train loss: 0.3593794107437134:  40%|█████▏       | 97/241 [00:14<00:22,  6.36it/s]Epoch: 8, train for the 38-th batch, train loss: 0.3958747088909149:  31%|████         | 37/119 [00:05<00:13,  6.16it/s]Epoch: 8, train for the 38-th batch, train loss: 0.3958747088909149:  32%|████▏        | 38/119 [00:05<00:13,  6.01it/s]Epoch: 3, train for the 143-th batch, train loss: 0.5351293683052063:  60%|██████▌    | 142/237 [00:26<00:19,  4.89it/s]Epoch: 5, train for the 150-th batch, train loss: 0.4876302182674408:  99%|██████████▊| 149/151 [00:27<00:00,  4.80it/s]Epoch: 3, train for the 143-th batch, train loss: 0.5351293683052063:  60%|██████▋    | 143/237 [00:27<00:19,  4.73it/s]Epoch: 5, train for the 150-th batch, train loss: 0.4876302182674408:  99%|██████████▉| 150/151 [00:27<00:00,  4.73it/s]Epoch: 8, train for the 39-th batch, train loss: 0.4430474638938904:  32%|████▏        | 38/119 [00:05<00:13,  6.01it/s]Epoch: 4, train for the 98-th batch, train loss: 0.4752972424030304:  40%|█████▏       | 97/241 [00:14<00:22,  6.36it/s]Epoch: 4, train for the 98-th batch, train loss: 0.4752972424030304:  41%|█████▎       | 98/241 [00:14<00:21,  6.74it/s]Epoch: 7, train for the 24-th batch, train loss: 0.5062299370765686:  16%|██           | 23/146 [00:03<00:16,  7.29it/s]Epoch: 2, train for the 17-th batch, train loss: 0.4392037093639374:   4%|▌            | 16/383 [00:02<01:04,  5.72it/s]Epoch: 7, train for the 24-th batch, train loss: 0.5062299370765686:  16%|██▏          | 24/146 [00:03<00:22,  5.33it/s]Epoch: 2, train for the 17-th batch, train loss: 0.4392037093639374:   4%|▌            | 17/383 [00:02<01:20,  4.53it/s]Epoch: 5, train for the 151-th batch, train loss: 0.4939112961292267:  99%|██████████▉| 150/151 [00:27<00:00,  4.73it/s]Epoch: 5, train for the 151-th batch, train loss: 0.4939112961292267: 100%|███████████| 151/151 [00:27<00:00,  5.10it/s]Epoch: 8, train for the 40-th batch, train loss: 0.4016035497188568:  32%|████▏        | 38/119 [00:05<00:13,  6.01it/s]Epoch: 5, train for the 151-th batch, train loss: 0.4939112961292267: 100%|███████████| 151/151 [00:27<00:00,  5.45it/s]
Epoch: 8, train for the 40-th batch, train loss: 0.4016035497188568:  34%|████▎        | 40/119 [00:05<00:11,  7.07it/s]Epoch: 3, train for the 144-th batch, train loss: 0.5883880853652954:  60%|██████▋    | 143/237 [00:27<00:19,  4.73it/s]Epoch: 4, train for the 99-th batch, train loss: 0.3124156892299652:  41%|█████▎       | 98/241 [00:14<00:21,  6.74it/s]Epoch: 4, train for the 99-th batch, train loss: 0.3124156892299652:  41%|█████▎       | 99/241 [00:14<00:21,  6.71it/s]Epoch: 3, train for the 144-th batch, train loss: 0.5883880853652954:  61%|██████▋    | 144/237 [00:27<00:19,  4.70it/s]Epoch: 7, train for the 25-th batch, train loss: 0.5039904713630676:  16%|██▏          | 24/146 [00:03<00:22,  5.33it/s]Epoch: 7, train for the 25-th batch, train loss: 0.5039904713630676:  17%|██▏          | 25/146 [00:03<00:20,  5.94it/s]Epoch: 2, train for the 18-th batch, train loss: 0.46938830614089966:   4%|▌           | 17/383 [00:03<01:20,  4.53it/s]Epoch: 2, train for the 18-th batch, train loss: 0.46938830614089966:   5%|▌           | 18/383 [00:03<01:12,  5.00it/s]Epoch: 8, train for the 41-th batch, train loss: 0.41630157828330994:  34%|████        | 40/119 [00:05<00:11,  7.07it/s]Epoch: 8, train for the 41-th batch, train loss: 0.41630157828330994:  34%|████▏       | 41/119 [00:05<00:10,  7.20it/s]Epoch: 4, train for the 100-th batch, train loss: 0.5088422894477844:  41%|████▉       | 99/241 [00:14<00:21,  6.71it/s]Epoch: 7, train for the 26-th batch, train loss: 0.4252766966819763:  17%|██▏          | 25/146 [00:03<00:20,  5.94it/s]Epoch: 7, train for the 26-th batch, train loss: 0.4252766966819763:  18%|██▎          | 26/146 [00:03<00:18,  6.55it/s]Epoch: 4, train for the 100-th batch, train loss: 0.5088422894477844:  41%|████▌      | 100/241 [00:14<00:21,  6.71it/s]Epoch: 3, train for the 145-th batch, train loss: 0.6078124046325684:  61%|██████▋    | 144/237 [00:27<00:19,  4.70it/s]Epoch: 3, train for the 145-th batch, train loss: 0.6078124046325684:  61%|██████▋    | 145/237 [00:27<00:19,  4.83it/s]Epoch: 8, train for the 42-th batch, train loss: 0.4845760762691498:  34%|████▍        | 41/119 [00:06<00:10,  7.20it/s]Epoch: 8, train for the 42-th batch, train loss: 0.4845760762691498:  35%|████▌        | 42/119 [00:06<00:11,  6.83it/s]  0%|                                                                                            | 0/46 [00:00<?, ?it/s]Epoch: 4, train for the 101-th batch, train loss: 0.49570807814598083:  41%|████▏     | 100/241 [00:15<00:21,  6.71it/s]Epoch: 7, train for the 27-th batch, train loss: 0.4975932538509369:  18%|██▎          | 26/146 [00:03<00:18,  6.55it/s]Epoch: 4, train for the 101-th batch, train loss: 0.49570807814598083:  42%|████▏     | 101/241 [00:15<00:21,  6.55it/s]Epoch: 7, train for the 27-th batch, train loss: 0.4975932538509369:  18%|██▍          | 27/146 [00:03<00:18,  6.38it/s]Epoch: 2, train for the 19-th batch, train loss: 0.3329637348651886:   5%|▌            | 18/383 [00:03<01:12,  5.00it/s]Epoch: 3, train for the 146-th batch, train loss: 0.5607578158378601:  61%|██████▋    | 145/237 [00:27<00:19,  4.83it/s]evaluate for the 1-th batch, evaluate loss: 0.5660327672958374:   0%|                            | 0/46 [00:00<?, ?it/s]Epoch: 2, train for the 19-th batch, train loss: 0.3329637348651886:   5%|▋            | 19/383 [00:03<01:23,  4.38it/s]Epoch: 8, train for the 43-th batch, train loss: 0.45296645164489746:  35%|████▏       | 42/119 [00:06<00:11,  6.83it/s]Epoch: 3, train for the 146-th batch, train loss: 0.5607578158378601:  62%|██████▊    | 146/237 [00:27<00:18,  5.05it/s]Epoch: 8, train for the 43-th batch, train loss: 0.45296645164489746:  36%|████▎       | 43/119 [00:06<00:10,  7.17it/s]Epoch: 7, train for the 28-th batch, train loss: 0.47312819957733154:  18%|██▏         | 27/146 [00:03<00:18,  6.38it/s]Epoch: 7, train for the 28-th batch, train loss: 0.47312819957733154:  19%|██▎         | 28/146 [00:03<00:17,  6.64it/s]evaluate for the 2-th batch, evaluate loss: 0.5419389605522156:   0%|                            | 0/46 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5419389605522156:   4%|▊                   | 2/46 [00:00<00:04, 10.75it/s]Epoch: 4, train for the 102-th batch, train loss: 0.5262933969497681:  42%|████▌      | 101/241 [00:15<00:21,  6.55it/s]Epoch: 4, train for the 102-th batch, train loss: 0.5262933969497681:  42%|████▋      | 102/241 [00:15<00:22,  6.25it/s]Epoch: 2, train for the 20-th batch, train loss: 0.31461647152900696:   5%|▌           | 19/383 [00:03<01:23,  4.38it/s]Epoch: 8, train for the 44-th batch, train loss: 0.4201135039329529:  36%|████▋        | 43/119 [00:06<00:10,  7.17it/s]evaluate for the 3-th batch, evaluate loss: 0.5414378046989441:   4%|▊                   | 2/46 [00:00<00:04, 10.75it/s]Epoch: 8, train for the 44-th batch, train loss: 0.4201135039329529:  37%|████▊        | 44/119 [00:06<00:10,  7.18it/s]Epoch: 2, train for the 20-th batch, train loss: 0.31461647152900696:   5%|▋           | 20/383 [00:03<01:16,  4.74it/s]Epoch: 7, train for the 29-th batch, train loss: 0.4826379418373108:  19%|██▍          | 28/146 [00:03<00:17,  6.64it/s]Epoch: 3, train for the 147-th batch, train loss: 0.5546978116035461:  62%|██████▊    | 146/237 [00:27<00:18,  5.05it/s]Epoch: 7, train for the 29-th batch, train loss: 0.4826379418373108:  20%|██▌          | 29/146 [00:03<00:16,  7.13it/s]Epoch: 3, train for the 147-th batch, train loss: 0.5546978116035461:  62%|██████▊    | 147/237 [00:27<00:18,  4.83it/s]Epoch: 4, train for the 103-th batch, train loss: 0.5192493796348572:  42%|████▋      | 102/241 [00:15<00:22,  6.25it/s]Epoch: 8, train for the 45-th batch, train loss: 0.38633212447166443:  37%|████▍       | 44/119 [00:06<00:10,  7.18it/s]evaluate for the 4-th batch, evaluate loss: 0.5449721217155457:   4%|▊                   | 2/46 [00:00<00:04, 10.75it/s]evaluate for the 4-th batch, evaluate loss: 0.5449721217155457:   9%|█▋                  | 4/46 [00:00<00:03, 10.69it/s]Epoch: 8, train for the 45-th batch, train loss: 0.38633212447166443:  38%|████▌       | 45/119 [00:06<00:10,  7.31it/s]Epoch: 4, train for the 103-th batch, train loss: 0.5192493796348572:  43%|████▋      | 103/241 [00:15<00:22,  6.02it/s]Epoch: 7, train for the 30-th batch, train loss: 0.48198020458221436:  20%|██▍         | 29/146 [00:04<00:16,  7.13it/s]Epoch: 7, train for the 30-th batch, train loss: 0.48198020458221436:  21%|██▍         | 30/146 [00:04<00:15,  7.26it/s]evaluate for the 5-th batch, evaluate loss: 0.5349994897842407:   9%|█▋                  | 4/46 [00:00<00:03, 10.69it/s]Epoch: 3, train for the 148-th batch, train loss: 0.5395625233650208:  62%|██████▊    | 147/237 [00:27<00:18,  4.83it/s]Epoch: 8, train for the 46-th batch, train loss: 0.43429747223854065:  38%|████▌       | 45/119 [00:06<00:10,  7.31it/s]Epoch: 8, train for the 46-th batch, train loss: 0.43429747223854065:  39%|████▋       | 46/119 [00:06<00:09,  7.47it/s]Epoch: 3, train for the 148-th batch, train loss: 0.5395625233650208:  62%|██████▊    | 148/237 [00:28<00:17,  4.97it/s]Epoch: 2, train for the 21-th batch, train loss: 0.4202284812927246:   5%|▋            | 20/383 [00:03<01:16,  4.74it/s]Epoch: 2, train for the 21-th batch, train loss: 0.4202284812927246:   5%|▋            | 21/383 [00:03<01:23,  4.31it/s]Epoch: 7, train for the 31-th batch, train loss: 0.5018386840820312:  21%|██▋          | 30/146 [00:04<00:15,  7.26it/s]Epoch: 4, train for the 104-th batch, train loss: 0.5187525153160095:  43%|████▋      | 103/241 [00:15<00:22,  6.02it/s]Epoch: 7, train for the 31-th batch, train loss: 0.5018386840820312:  21%|██▊          | 31/146 [00:04<00:15,  7.47it/s]evaluate for the 6-th batch, evaluate loss: 0.542649507522583:   9%|█▊                   | 4/46 [00:00<00:03, 10.69it/s]evaluate for the 6-th batch, evaluate loss: 0.542649507522583:  13%|██▋                  | 6/46 [00:00<00:03, 10.92it/s]Epoch: 4, train for the 104-th batch, train loss: 0.5187525153160095:  43%|████▋      | 104/241 [00:15<00:23,  5.94it/s]Epoch: 8, train for the 47-th batch, train loss: 0.48540693521499634:  39%|████▋       | 46/119 [00:06<00:09,  7.47it/s]evaluate for the 7-th batch, evaluate loss: 0.5557990670204163:  13%|██▌                 | 6/46 [00:00<00:03, 10.92it/s]Epoch: 8, train for the 47-th batch, train loss: 0.48540693521499634:  39%|████▋       | 47/119 [00:06<00:09,  7.28it/s]Epoch: 7, train for the 32-th batch, train loss: 0.4736208915710449:  21%|██▊          | 31/146 [00:04<00:15,  7.47it/s]Epoch: 7, train for the 32-th batch, train loss: 0.4736208915710449:  22%|██▊          | 32/146 [00:04<00:15,  7.36it/s]Epoch: 4, train for the 105-th batch, train loss: 0.4236072897911072:  43%|████▋      | 104/241 [00:15<00:23,  5.94it/s]Epoch: 2, train for the 22-th batch, train loss: 0.4420401155948639:   5%|▋            | 21/383 [00:04<01:23,  4.31it/s]Epoch: 4, train for the 105-th batch, train loss: 0.4236072897911072:  44%|████▊      | 105/241 [00:15<00:22,  5.93it/s]evaluate for the 8-th batch, evaluate loss: 0.5745697617530823:  13%|██▌                 | 6/46 [00:00<00:03, 10.92it/s]evaluate for the 8-th batch, evaluate loss: 0.5745697617530823:  17%|███▍                | 8/46 [00:00<00:03, 10.93it/s]Epoch: 2, train for the 22-th batch, train loss: 0.4420401155948639:   6%|▋            | 22/383 [00:04<01:22,  4.39it/s]Epoch: 3, train for the 149-th batch, train loss: 0.5421676635742188:  62%|██████▊    | 148/237 [00:28<00:17,  4.97it/s]Epoch: 3, train for the 149-th batch, train loss: 0.5421676635742188:  63%|██████▉    | 149/237 [00:28<00:19,  4.47it/s]evaluate for the 9-th batch, evaluate loss: 0.535014271736145:  17%|███▋                 | 8/46 [00:00<00:03, 10.93it/s]Epoch: 8, train for the 48-th batch, train loss: 0.440102219581604:  39%|█████▌        | 47/119 [00:06<00:09,  7.28it/s]Epoch: 8, train for the 48-th batch, train loss: 0.440102219581604:  40%|█████▋        | 48/119 [00:06<00:10,  6.85it/s]Epoch: 7, train for the 33-th batch, train loss: 0.5025009512901306:  22%|██▊          | 32/146 [00:04<00:15,  7.36it/s]Epoch: 7, train for the 33-th batch, train loss: 0.5025009512901306:  23%|██▉          | 33/146 [00:04<00:16,  6.99it/s]Epoch: 4, train for the 106-th batch, train loss: 0.4948229491710663:  44%|████▊      | 105/241 [00:15<00:22,  5.93it/s]Epoch: 4, train for the 106-th batch, train loss: 0.4948229491710663:  44%|████▊      | 106/241 [00:15<00:21,  6.35it/s]evaluate for the 10-th batch, evaluate loss: 0.5149921178817749:  17%|███▎               | 8/46 [00:00<00:03, 10.93it/s]evaluate for the 10-th batch, evaluate loss: 0.5149921178817749:  22%|███▉              | 10/46 [00:00<00:03, 11.31it/s]Epoch: 2, train for the 23-th batch, train loss: 0.5020909905433655:   6%|▋            | 22/383 [00:04<01:22,  4.39it/s]Epoch: 2, train for the 23-th batch, train loss: 0.5020909905433655:   6%|▊            | 23/383 [00:04<01:15,  4.75it/s]Epoch: 3, train for the 150-th batch, train loss: 0.5173598527908325:  63%|██████▉    | 149/237 [00:28<00:19,  4.47it/s]Epoch: 8, train for the 49-th batch, train loss: 0.42518171668052673:  40%|████▊       | 48/119 [00:07<00:10,  6.85it/s]Epoch: 8, train for the 49-th batch, train loss: 0.42518171668052673:  41%|████▉       | 49/119 [00:07<00:10,  6.38it/s]evaluate for the 11-th batch, evaluate loss: 0.5627725720405579:  22%|███▉              | 10/46 [00:01<00:03, 11.31it/s]Epoch: 7, train for the 34-th batch, train loss: 0.4933091104030609:  23%|██▉          | 33/146 [00:04<00:16,  6.99it/s]Epoch: 7, train for the 34-th batch, train loss: 0.4933091104030609:  23%|███          | 34/146 [00:04<00:17,  6.53it/s]Epoch: 4, train for the 107-th batch, train loss: 0.5572456121444702:  44%|████▊      | 106/241 [00:16<00:21,  6.35it/s]Epoch: 3, train for the 150-th batch, train loss: 0.5173598527908325:  63%|██████▉    | 150/237 [00:28<00:19,  4.37it/s]Epoch: 2, train for the 24-th batch, train loss: 0.32784804701805115:   6%|▋           | 23/383 [00:04<01:15,  4.75it/s]Epoch: 4, train for the 107-th batch, train loss: 0.5572456121444702:  44%|████▉      | 107/241 [00:16<00:22,  5.96it/s]Epoch: 2, train for the 24-th batch, train loss: 0.32784804701805115:   6%|▊           | 24/383 [00:04<01:09,  5.13it/s]evaluate for the 12-th batch, evaluate loss: 0.5145792961120605:  22%|███▉              | 10/46 [00:01<00:03, 11.31it/s]evaluate for the 12-th batch, evaluate loss: 0.5145792961120605:  26%|████▋             | 12/46 [00:01<00:03, 10.88it/s]Epoch: 8, train for the 50-th batch, train loss: 0.43285050988197327:  41%|████▉       | 49/119 [00:07<00:10,  6.38it/s]Epoch: 7, train for the 35-th batch, train loss: 0.4825756847858429:  23%|███          | 34/146 [00:04<00:17,  6.53it/s]Epoch: 7, train for the 35-th batch, train loss: 0.4825756847858429:  24%|███          | 35/146 [00:04<00:17,  6.28it/s]Epoch: 8, train for the 50-th batch, train loss: 0.43285050988197327:  42%|█████       | 50/119 [00:07<00:11,  5.96it/s]Epoch: 4, train for the 108-th batch, train loss: 0.49701187014579773:  44%|████▍     | 107/241 [00:16<00:22,  5.96it/s]evaluate for the 13-th batch, evaluate loss: 0.5125277638435364:  26%|████▋             | 12/46 [00:01<00:03, 10.88it/s]Epoch: 3, train for the 151-th batch, train loss: 0.5665329098701477:  63%|██████▉    | 150/237 [00:28<00:19,  4.37it/s]Epoch: 4, train for the 108-th batch, train loss: 0.49701187014579773:  45%|████▍     | 108/241 [00:16<00:23,  5.69it/s]Epoch: 2, train for the 25-th batch, train loss: 0.4063508212566376:   6%|▊            | 24/383 [00:04<01:09,  5.13it/s]Epoch: 3, train for the 151-th batch, train loss: 0.5665329098701477:  64%|███████    | 151/237 [00:28<00:20,  4.26it/s]Epoch: 2, train for the 25-th batch, train loss: 0.4063508212566376:   7%|▊            | 25/383 [00:04<01:10,  5.09it/s]evaluate for the 14-th batch, evaluate loss: 0.5627772808074951:  26%|████▋             | 12/46 [00:01<00:03, 10.88it/s]evaluate for the 14-th batch, evaluate loss: 0.5627772808074951:  30%|█████▍            | 14/46 [00:01<00:03, 10.38it/s]Epoch: 7, train for the 36-th batch, train loss: 0.45428451895713806:  24%|██▉         | 35/146 [00:04<00:17,  6.28it/s]Epoch: 7, train for the 36-th batch, train loss: 0.45428451895713806:  25%|██▉         | 36/146 [00:04<00:17,  6.46it/s]Epoch: 8, train for the 51-th batch, train loss: 0.4806120991706848:  42%|█████▍       | 50/119 [00:07<00:11,  5.96it/s]Epoch: 8, train for the 51-th batch, train loss: 0.4806120991706848:  43%|█████▌       | 51/119 [00:07<00:11,  6.10it/s]Epoch: 4, train for the 109-th batch, train loss: 0.6636483073234558:  45%|████▉      | 108/241 [00:16<00:23,  5.69it/s]Epoch: 4, train for the 109-th batch, train loss: 0.6636483073234558:  45%|████▉      | 109/241 [00:16<00:21,  6.05it/s]evaluate for the 15-th batch, evaluate loss: 0.5337386727333069:  30%|█████▍            | 14/46 [00:01<00:03, 10.38it/s]Epoch: 7, train for the 37-th batch, train loss: 0.5378432273864746:  25%|███▏         | 36/146 [00:05<00:17,  6.46it/s]Epoch: 7, train for the 37-th batch, train loss: 0.5378432273864746:  25%|███▎         | 37/146 [00:05<00:15,  6.89it/s]evaluate for the 16-th batch, evaluate loss: 0.5215974450111389:  30%|█████▍            | 14/46 [00:01<00:03, 10.38it/s]evaluate for the 16-th batch, evaluate loss: 0.5215974450111389:  35%|██████▎           | 16/46 [00:01<00:02, 11.02it/s]Epoch: 8, train for the 52-th batch, train loss: 0.43569570779800415:  43%|█████▏      | 51/119 [00:07<00:11,  6.10it/s]Epoch: 3, train for the 152-th batch, train loss: 0.5067447423934937:  64%|███████    | 151/237 [00:29<00:20,  4.26it/s]Epoch: 8, train for the 52-th batch, train loss: 0.43569570779800415:  44%|█████▏      | 52/119 [00:07<00:11,  6.05it/s]Epoch: 2, train for the 26-th batch, train loss: 0.4189804792404175:   7%|▊            | 25/383 [00:04<01:10,  5.09it/s]Epoch: 3, train for the 152-th batch, train loss: 0.5067447423934937:  64%|███████    | 152/237 [00:29<00:20,  4.12it/s]Epoch: 2, train for the 26-th batch, train loss: 0.4189804792404175:   7%|▉            | 26/383 [00:04<01:16,  4.64it/s]Epoch: 4, train for the 110-th batch, train loss: 0.5479989647865295:  45%|████▉      | 109/241 [00:16<00:21,  6.05it/s]evaluate for the 17-th batch, evaluate loss: 0.44447237253189087:  35%|█████▉           | 16/46 [00:01<00:02, 11.02it/s]Epoch: 4, train for the 110-th batch, train loss: 0.5479989647865295:  46%|█████      | 110/241 [00:16<00:22,  5.73it/s]Epoch: 7, train for the 38-th batch, train loss: 0.5383094549179077:  25%|███▎         | 37/146 [00:05<00:15,  6.89it/s]Epoch: 7, train for the 38-th batch, train loss: 0.5383094549179077:  26%|███▍         | 38/146 [00:05<00:15,  6.76it/s]evaluate for the 18-th batch, evaluate loss: 0.5038029551506042:  35%|██████▎           | 16/46 [00:01<00:02, 11.02it/s]evaluate for the 18-th batch, evaluate loss: 0.5038029551506042:  39%|███████           | 18/46 [00:01<00:02, 10.79it/s]Epoch: 8, train for the 53-th batch, train loss: 0.4362282454967499:  44%|█████▋       | 52/119 [00:07<00:11,  6.05it/s]Epoch: 8, train for the 53-th batch, train loss: 0.4362282454967499:  45%|█████▊       | 53/119 [00:07<00:10,  6.12it/s]Epoch: 4, train for the 111-th batch, train loss: 0.22040873765945435:  46%|████▌     | 110/241 [00:16<00:22,  5.73it/s]Epoch: 7, train for the 39-th batch, train loss: 0.510778546333313:  26%|███▋          | 38/146 [00:05<00:15,  6.76it/s]evaluate for the 19-th batch, evaluate loss: 0.5774657130241394:  39%|███████           | 18/46 [00:01<00:02, 10.79it/s]Epoch: 7, train for the 39-th batch, train loss: 0.510778546333313:  27%|███▋          | 39/146 [00:05<00:15,  6.94it/s]Epoch: 4, train for the 111-th batch, train loss: 0.22040873765945435:  46%|████▌     | 111/241 [00:16<00:22,  5.72it/s]Epoch: 3, train for the 153-th batch, train loss: 0.5398386120796204:  64%|███████    | 152/237 [00:29<00:20,  4.12it/s]Epoch: 2, train for the 27-th batch, train loss: 0.36713021993637085:   7%|▊           | 26/383 [00:05<01:16,  4.64it/s]Epoch: 8, train for the 54-th batch, train loss: 0.3803393542766571:  45%|█████▊       | 53/119 [00:07<00:10,  6.12it/s]Epoch: 8, train for the 54-th batch, train loss: 0.3803393542766571:  45%|█████▉       | 54/119 [00:07<00:09,  6.53it/s]Epoch: 3, train for the 153-th batch, train loss: 0.5398386120796204:  65%|███████    | 153/237 [00:29<00:20,  4.01it/s]Epoch: 2, train for the 27-th batch, train loss: 0.36713021993637085:   7%|▊           | 27/383 [00:05<01:22,  4.32it/s]evaluate for the 20-th batch, evaluate loss: 0.5224164128303528:  39%|███████           | 18/46 [00:01<00:02, 10.79it/s]evaluate for the 20-th batch, evaluate loss: 0.5224164128303528:  43%|███████▊          | 20/46 [00:01<00:02, 11.28it/s]Epoch: 7, train for the 40-th batch, train loss: 0.4963606595993042:  27%|███▍         | 39/146 [00:05<00:15,  6.94it/s]Epoch: 7, train for the 40-th batch, train loss: 0.4963606595993042:  27%|███▌         | 40/146 [00:05<00:14,  7.35it/s]Epoch: 4, train for the 112-th batch, train loss: 0.3129917085170746:  46%|█████      | 111/241 [00:16<00:22,  5.72it/s]evaluate for the 21-th batch, evaluate loss: 0.55718994140625:  43%|████████▋           | 20/46 [00:01<00:02, 11.28it/s]Epoch: 4, train for the 112-th batch, train loss: 0.3129917085170746:  46%|█████      | 112/241 [00:17<00:22,  5.69it/s]Epoch: 8, train for the 55-th batch, train loss: 0.3699212670326233:  45%|█████▉       | 54/119 [00:08<00:09,  6.53it/s]evaluate for the 22-th batch, evaluate loss: 0.4919469356536865:  43%|███████▊          | 20/46 [00:01<00:02, 11.28it/s]evaluate for the 22-th batch, evaluate loss: 0.4919469356536865:  48%|████████▌         | 22/46 [00:01<00:02, 11.56it/s]Epoch: 8, train for the 55-th batch, train loss: 0.3699212670326233:  46%|██████       | 55/119 [00:08<00:10,  6.25it/s]Epoch: 7, train for the 41-th batch, train loss: 0.47884970903396606:  27%|███▎        | 40/146 [00:05<00:14,  7.35it/s]Epoch: 7, train for the 41-th batch, train loss: 0.47884970903396606:  28%|███▎        | 41/146 [00:05<00:14,  7.29it/s]Epoch: 2, train for the 28-th batch, train loss: 0.3874371349811554:   7%|▉            | 27/383 [00:05<01:22,  4.32it/s]Epoch: 3, train for the 154-th batch, train loss: 0.5393972992897034:  65%|███████    | 153/237 [00:29<00:20,  4.01it/s]Epoch: 2, train for the 28-th batch, train loss: 0.3874371349811554:   7%|▉            | 28/383 [00:05<01:28,  4.02it/s]evaluate for the 23-th batch, evaluate loss: 0.4705875813961029:  48%|████████▌         | 22/46 [00:02<00:02, 11.56it/s]Epoch: 4, train for the 113-th batch, train loss: 0.4032932221889496:  46%|█████      | 112/241 [00:17<00:22,  5.69it/s]Epoch: 3, train for the 154-th batch, train loss: 0.5393972992897034:  65%|███████▏   | 154/237 [00:29<00:22,  3.71it/s]Epoch: 4, train for the 113-th batch, train loss: 0.4032932221889496:  47%|█████▏     | 113/241 [00:17<00:22,  5.59it/s]Epoch: 8, train for the 56-th batch, train loss: 0.45180925726890564:  46%|█████▌      | 55/119 [00:08<00:10,  6.25it/s]Epoch: 7, train for the 42-th batch, train loss: 0.45124778151512146:  28%|███▎        | 41/146 [00:05<00:14,  7.29it/s]Epoch: 8, train for the 56-th batch, train loss: 0.45180925726890564:  47%|█████▋      | 56/119 [00:08<00:10,  5.95it/s]Epoch: 7, train for the 42-th batch, train loss: 0.45124778151512146:  29%|███▍        | 42/146 [00:05<00:15,  6.84it/s]evaluate for the 24-th batch, evaluate loss: 0.4823721945285797:  48%|████████▌         | 22/46 [00:02<00:02, 11.56it/s]evaluate for the 24-th batch, evaluate loss: 0.4823721945285797:  52%|█████████▍        | 24/46 [00:02<00:02, 10.40it/s]Epoch: 2, train for the 29-th batch, train loss: 0.39460647106170654:   7%|▉           | 28/383 [00:05<01:28,  4.02it/s]Epoch: 2, train for the 29-th batch, train loss: 0.39460647106170654:   8%|▉           | 29/383 [00:05<01:19,  4.44it/s]Epoch: 4, train for the 114-th batch, train loss: 0.36063873767852783:  47%|████▋     | 113/241 [00:17<00:22,  5.59it/s]evaluate for the 25-th batch, evaluate loss: 0.5132376551628113:  52%|█████████▍        | 24/46 [00:02<00:02, 10.40it/s]Epoch: 4, train for the 114-th batch, train loss: 0.36063873767852783:  47%|████▋     | 114/241 [00:17<00:22,  5.54it/s]Epoch: 7, train for the 43-th batch, train loss: 0.4471242427825928:  29%|███▋         | 42/146 [00:05<00:15,  6.84it/s]Epoch: 8, train for the 57-th batch, train loss: 0.4008517265319824:  47%|██████       | 56/119 [00:08<00:10,  5.95it/s]Epoch: 7, train for the 43-th batch, train loss: 0.4471242427825928:  29%|███▊         | 43/146 [00:05<00:15,  6.45it/s]Epoch: 8, train for the 57-th batch, train loss: 0.4008517265319824:  48%|██████▏      | 57/119 [00:08<00:10,  5.81it/s]Epoch: 3, train for the 155-th batch, train loss: 0.5541805028915405:  65%|███████▏   | 154/237 [00:29<00:22,  3.71it/s]evaluate for the 26-th batch, evaluate loss: 0.5465201735496521:  52%|█████████▍        | 24/46 [00:02<00:02, 10.40it/s]evaluate for the 26-th batch, evaluate loss: 0.5465201735496521:  57%|██████████▏       | 26/46 [00:02<00:01, 10.73it/s]Epoch: 2, train for the 30-th batch, train loss: 0.3540201485157013:   8%|▉            | 29/383 [00:05<01:19,  4.44it/s]Epoch: 3, train for the 155-th batch, train loss: 0.5541805028915405:  65%|███████▏   | 155/237 [00:29<00:22,  3.59it/s]Epoch: 2, train for the 30-th batch, train loss: 0.3540201485157013:   8%|█            | 30/383 [00:05<01:11,  4.91it/s]Epoch: 4, train for the 115-th batch, train loss: 0.40839824080467224:  47%|████▋     | 114/241 [00:17<00:22,  5.54it/s]Epoch: 4, train for the 115-th batch, train loss: 0.40839824080467224:  48%|████▊     | 115/241 [00:17<00:21,  5.82it/s]evaluate for the 27-th batch, evaluate loss: 0.5048878192901611:  57%|██████████▏       | 26/46 [00:02<00:01, 10.73it/s]Epoch: 7, train for the 44-th batch, train loss: 0.4473855495452881:  29%|███▊         | 43/146 [00:06<00:15,  6.45it/s]Epoch: 7, train for the 44-th batch, train loss: 0.4473855495452881:  30%|███▉         | 44/146 [00:06<00:16,  6.33it/s]Epoch: 8, train for the 58-th batch, train loss: 0.380966454744339:  48%|██████▋       | 57/119 [00:08<00:10,  5.81it/s]Epoch: 8, train for the 58-th batch, train loss: 0.380966454744339:  49%|██████▊       | 58/119 [00:08<00:10,  5.74it/s]Epoch: 2, train for the 31-th batch, train loss: 0.3559090793132782:   8%|█            | 30/383 [00:05<01:11,  4.91it/s]Epoch: 2, train for the 31-th batch, train loss: 0.3559090793132782:   8%|█            | 31/383 [00:05<01:05,  5.40it/s]evaluate for the 28-th batch, evaluate loss: 0.5201186537742615:  57%|██████████▏       | 26/46 [00:02<00:01, 10.73it/s]evaluate for the 28-th batch, evaluate loss: 0.5201186537742615:  61%|██████████▉       | 28/46 [00:02<00:01, 10.68it/s]Epoch: 4, train for the 116-th batch, train loss: 0.5178378224372864:  48%|█████▏     | 115/241 [00:17<00:21,  5.82it/s]Epoch: 3, train for the 156-th batch, train loss: 0.534500777721405:  65%|███████▊    | 155/237 [00:30<00:22,  3.59it/s]Epoch: 4, train for the 116-th batch, train loss: 0.5178378224372864:  48%|█████▎     | 116/241 [00:17<00:21,  5.81it/s]Epoch: 7, train for the 45-th batch, train loss: 0.49889740347862244:  30%|███▌        | 44/146 [00:06<00:16,  6.33it/s]Epoch: 7, train for the 45-th batch, train loss: 0.49889740347862244:  31%|███▋        | 45/146 [00:06<00:15,  6.44it/s]evaluate for the 29-th batch, evaluate loss: 0.4927125871181488:  61%|██████████▉       | 28/46 [00:02<00:01, 10.68it/s]Epoch: 8, train for the 59-th batch, train loss: 0.40859562158584595:  49%|█████▊      | 58/119 [00:08<00:10,  5.74it/s]Epoch: 3, train for the 156-th batch, train loss: 0.534500777721405:  66%|███████▉    | 156/237 [00:30<00:22,  3.67it/s]Epoch: 8, train for the 59-th batch, train loss: 0.40859562158584595:  50%|█████▉      | 59/119 [00:08<00:10,  5.89it/s]Epoch: 2, train for the 32-th batch, train loss: 0.3204343318939209:   8%|█            | 31/383 [00:06<01:05,  5.40it/s]Epoch: 2, train for the 32-th batch, train loss: 0.3204343318939209:   8%|█            | 32/383 [00:06<01:00,  5.78it/s]Epoch: 4, train for the 117-th batch, train loss: 0.5676426291465759:  48%|█████▎     | 116/241 [00:17<00:21,  5.81it/s]evaluate for the 30-th batch, evaluate loss: 0.4987698197364807:  61%|██████████▉       | 28/46 [00:02<00:01, 10.68it/s]evaluate for the 30-th batch, evaluate loss: 0.4987698197364807:  65%|███████████▋      | 30/46 [00:02<00:01, 10.75it/s]Epoch: 4, train for the 117-th batch, train loss: 0.5676426291465759:  49%|█████▎     | 117/241 [00:17<00:19,  6.27it/s]Epoch: 7, train for the 46-th batch, train loss: 0.41848963499069214:  31%|███▋        | 45/146 [00:06<00:15,  6.44it/s]Epoch: 7, train for the 46-th batch, train loss: 0.41848963499069214:  32%|███▊        | 46/146 [00:06<00:15,  6.59it/s]Epoch: 8, train for the 60-th batch, train loss: 0.4188638925552368:  50%|██████▍      | 59/119 [00:08<00:10,  5.89it/s]Epoch: 8, train for the 60-th batch, train loss: 0.4188638925552368:  50%|██████▌      | 60/119 [00:08<00:09,  6.06it/s]evaluate for the 31-th batch, evaluate loss: 0.4326554536819458:  65%|███████████▋      | 30/46 [00:02<00:01, 10.75it/s]Epoch: 4, train for the 118-th batch, train loss: 0.4569609761238098:  49%|█████▎     | 117/241 [00:17<00:19,  6.27it/s]Epoch: 2, train for the 33-th batch, train loss: 0.353774756193161:   8%|█▏            | 32/383 [00:06<01:00,  5.78it/s]Epoch: 4, train for the 118-th batch, train loss: 0.4569609761238098:  49%|█████▍     | 118/241 [00:18<00:19,  6.32it/s]evaluate for the 32-th batch, evaluate loss: 0.4670046865940094:  65%|███████████▋      | 30/46 [00:02<00:01, 10.75it/s]evaluate for the 32-th batch, evaluate loss: 0.4670046865940094:  70%|████████████▌     | 32/46 [00:02<00:01, 11.18it/s]Epoch: 2, train for the 33-th batch, train loss: 0.353774756193161:   9%|█▏            | 33/383 [00:06<01:05,  5.37it/s]Epoch: 3, train for the 157-th batch, train loss: 0.5701689720153809:  66%|███████▏   | 156/237 [00:30<00:22,  3.67it/s]Epoch: 7, train for the 47-th batch, train loss: 0.4928208887577057:  32%|████         | 46/146 [00:06<00:15,  6.59it/s]Epoch: 7, train for the 47-th batch, train loss: 0.4928208887577057:  32%|████▏        | 47/146 [00:06<00:15,  6.57it/s]Epoch: 8, train for the 61-th batch, train loss: 0.42909857630729675:  50%|██████      | 60/119 [00:09<00:09,  6.06it/s]Epoch: 3, train for the 157-th batch, train loss: 0.5701689720153809:  66%|███████▎   | 157/237 [00:30<00:22,  3.54it/s]Epoch: 8, train for the 61-th batch, train loss: 0.42909857630729675:  51%|██████▏     | 61/119 [00:09<00:09,  6.23it/s]evaluate for the 33-th batch, evaluate loss: 0.4734940230846405:  70%|████████████▌     | 32/46 [00:03<00:01, 11.18it/s]Epoch: 4, train for the 119-th batch, train loss: 0.38455504179000854:  49%|████▉     | 118/241 [00:18<00:19,  6.32it/s]Epoch: 4, train for the 119-th batch, train loss: 0.38455504179000854:  49%|████▉     | 119/241 [00:18<00:18,  6.53it/s]Epoch: 2, train for the 34-th batch, train loss: 0.35702842473983765:   9%|█           | 33/383 [00:06<01:05,  5.37it/s]evaluate for the 34-th batch, evaluate loss: 0.46474605798721313:  70%|███████████▊     | 32/46 [00:03<00:01, 11.18it/s]evaluate for the 34-th batch, evaluate loss: 0.46474605798721313:  74%|████████████▌    | 34/46 [00:03<00:01, 11.61it/s]Epoch: 7, train for the 48-th batch, train loss: 0.4923224449157715:  32%|████▏        | 47/146 [00:06<00:15,  6.57it/s]Epoch: 7, train for the 48-th batch, train loss: 0.4923224449157715:  33%|████▎        | 48/146 [00:06<00:14,  6.83it/s]Epoch: 2, train for the 34-th batch, train loss: 0.35702842473983765:   9%|█           | 34/383 [00:06<01:03,  5.52it/s]Epoch: 8, train for the 62-th batch, train loss: 0.4428779184818268:  51%|██████▋      | 61/119 [00:09<00:09,  6.23it/s]Epoch: 8, train for the 62-th batch, train loss: 0.4428779184818268:  52%|██████▊      | 62/119 [00:09<00:09,  6.26it/s]evaluate for the 35-th batch, evaluate loss: 0.500755786895752:  74%|██████████████     | 34/46 [00:03<00:01, 11.61it/s]Epoch: 3, train for the 158-th batch, train loss: 0.5086945295333862:  66%|███████▎   | 157/237 [00:30<00:22,  3.54it/s]Epoch: 4, train for the 120-th batch, train loss: 0.5875132083892822:  49%|█████▍     | 119/241 [00:18<00:18,  6.53it/s]evaluate for the 36-th batch, evaluate loss: 0.449688583612442:  74%|██████████████     | 34/46 [00:03<00:01, 11.61it/s]evaluate for the 36-th batch, evaluate loss: 0.449688583612442:  78%|██████████████▊    | 36/46 [00:03<00:00, 12.32it/s]Epoch: 4, train for the 120-th batch, train loss: 0.5875132083892822:  50%|█████▍     | 120/241 [00:18<00:18,  6.39it/s]Epoch: 3, train for the 158-th batch, train loss: 0.5086945295333862:  67%|███████▎   | 158/237 [00:30<00:21,  3.67it/s]Epoch: 2, train for the 35-th batch, train loss: 0.37982115149497986:   9%|█           | 34/383 [00:06<01:03,  5.52it/s]Epoch: 8, train for the 63-th batch, train loss: 0.4471227526664734:  52%|██████▊      | 62/119 [00:09<00:09,  6.26it/s]Epoch: 8, train for the 63-th batch, train loss: 0.4471227526664734:  53%|██████▉      | 63/119 [00:09<00:07,  7.01it/s]Epoch: 2, train for the 35-th batch, train loss: 0.37982115149497986:   9%|█           | 35/383 [00:06<01:01,  5.63it/s]evaluate for the 37-th batch, evaluate loss: 0.5143070220947266:  78%|██████████████    | 36/46 [00:03<00:00, 12.32it/s]Epoch: 7, train for the 49-th batch, train loss: 0.5007638931274414:  33%|████▎        | 48/146 [00:06<00:14,  6.83it/s]Epoch: 7, train for the 49-th batch, train loss: 0.5007638931274414:  34%|████▎        | 49/146 [00:06<00:17,  5.69it/s]Epoch: 4, train for the 121-th batch, train loss: 0.5303983092308044:  50%|█████▍     | 120/241 [00:18<00:18,  6.39it/s]evaluate for the 38-th batch, evaluate loss: 0.49137675762176514:  78%|█████████████▎   | 36/46 [00:03<00:00, 12.32it/s]evaluate for the 38-th batch, evaluate loss: 0.49137675762176514:  83%|██████████████   | 38/46 [00:03<00:00, 12.17it/s]Epoch: 4, train for the 121-th batch, train loss: 0.5303983092308044:  50%|█████▌     | 121/241 [00:18<00:19,  6.27it/s]Epoch: 8, train for the 64-th batch, train loss: 0.4369924068450928:  53%|██████▉      | 63/119 [00:09<00:07,  7.01it/s]Epoch: 8, train for the 64-th batch, train loss: 0.4369924068450928:  54%|██████▉      | 64/119 [00:09<00:08,  6.87it/s]Epoch: 3, train for the 159-th batch, train loss: 0.5574582815170288:  67%|███████▎   | 158/237 [00:30<00:21,  3.67it/s]Epoch: 7, train for the 50-th batch, train loss: 0.5369974374771118:  34%|████▎        | 49/146 [00:07<00:17,  5.69it/s]Epoch: 2, train for the 36-th batch, train loss: 0.3347476124763489:   9%|█▏           | 35/383 [00:06<01:01,  5.63it/s]Epoch: 7, train for the 50-th batch, train loss: 0.5369974374771118:  34%|████▍        | 50/146 [00:07<00:15,  6.25it/s]evaluate for the 39-th batch, evaluate loss: 0.4653564691543579:  83%|██████████████▊   | 38/46 [00:03<00:00, 12.17it/s]Epoch: 2, train for the 36-th batch, train loss: 0.3347476124763489:   9%|█▏           | 36/383 [00:06<01:04,  5.36it/s]Epoch: 3, train for the 159-th batch, train loss: 0.5574582815170288:  67%|███████▍   | 159/237 [00:30<00:20,  3.79it/s]Epoch: 4, train for the 122-th batch, train loss: 0.465969979763031:  50%|██████      | 121/241 [00:18<00:19,  6.27it/s]Epoch: 8, train for the 65-th batch, train loss: 0.43887677788734436:  54%|██████▍     | 64/119 [00:09<00:08,  6.87it/s]Epoch: 4, train for the 122-th batch, train loss: 0.465969979763031:  51%|██████      | 122/241 [00:18<00:18,  6.57it/s]Epoch: 8, train for the 65-th batch, train loss: 0.43887677788734436:  55%|██████▌     | 65/119 [00:09<00:07,  7.11it/s]evaluate for the 40-th batch, evaluate loss: 0.45931166410446167:  83%|██████████████   | 38/46 [00:03<00:00, 12.17it/s]evaluate for the 40-th batch, evaluate loss: 0.45931166410446167:  87%|██████████████▊  | 40/46 [00:03<00:00, 11.47it/s]Epoch: 7, train for the 51-th batch, train loss: 0.4820393919944763:  34%|████▍        | 50/146 [00:07<00:15,  6.25it/s]Epoch: 7, train for the 51-th batch, train loss: 0.4820393919944763:  35%|████▌        | 51/146 [00:07<00:14,  6.55it/s]Epoch: 8, train for the 66-th batch, train loss: 0.4623812437057495:  55%|███████      | 65/119 [00:09<00:07,  7.11it/s]Epoch: 8, train for the 66-th batch, train loss: 0.4623812437057495:  55%|███████▏     | 66/119 [00:09<00:07,  7.10it/s]Epoch: 2, train for the 37-th batch, train loss: 0.3280375599861145:   9%|█▏           | 36/383 [00:07<01:04,  5.36it/s]Epoch: 3, train for the 160-th batch, train loss: 0.5485782027244568:  67%|███████▍   | 159/237 [00:31<00:20,  3.79it/s]evaluate for the 41-th batch, evaluate loss: 0.480617880821228:  87%|████████████████▌  | 40/46 [00:03<00:00, 11.47it/s]Epoch: 2, train for the 37-th batch, train loss: 0.3280375599861145:  10%|█▎           | 37/383 [00:07<01:10,  4.90it/s]Epoch: 4, train for the 123-th batch, train loss: 0.7575684785842896:  51%|█████▌     | 122/241 [00:18<00:18,  6.57it/s]Epoch: 3, train for the 160-th batch, train loss: 0.5485782027244568:  68%|███████▍   | 160/237 [00:31<00:19,  3.87it/s]Epoch: 7, train for the 52-th batch, train loss: 0.5198309421539307:  35%|████▌        | 51/146 [00:07<00:14,  6.55it/s]Epoch: 7, train for the 52-th batch, train loss: 0.5198309421539307:  36%|████▋        | 52/146 [00:07<00:13,  6.73it/s]Epoch: 4, train for the 123-th batch, train loss: 0.7575684785842896:  51%|█████▌     | 123/241 [00:18<00:20,  5.79it/s]evaluate for the 42-th batch, evaluate loss: 0.441253125667572:  87%|████████████████▌  | 40/46 [00:03<00:00, 11.47it/s]evaluate for the 42-th batch, evaluate loss: 0.441253125667572:  91%|█████████████████▎ | 42/46 [00:03<00:00, 10.18it/s]Epoch: 4, train for the 124-th batch, train loss: 0.3675912320613861:  51%|█████▌     | 123/241 [00:18<00:20,  5.79it/s]Epoch: 7, train for the 53-th batch, train loss: 0.462717205286026:  36%|████▉         | 52/146 [00:07<00:13,  6.73it/s]Epoch: 8, train for the 67-th batch, train loss: 0.4658912122249603:  55%|███████▏     | 66/119 [00:10<00:07,  7.10it/s]Epoch: 7, train for the 53-th batch, train loss: 0.462717205286026:  36%|█████         | 53/146 [00:07<00:14,  6.61it/s]Epoch: 4, train for the 124-th batch, train loss: 0.3675912320613861:  51%|█████▋     | 124/241 [00:18<00:19,  6.02it/s]Epoch: 8, train for the 67-th batch, train loss: 0.4658912122249603:  56%|███████▎     | 67/119 [00:10<00:08,  6.02it/s]evaluate for the 43-th batch, evaluate loss: 0.5273743271827698:  91%|████████████████▍ | 42/46 [00:03<00:00, 10.18it/s]Epoch: 2, train for the 38-th batch, train loss: 0.3863075077533722:  10%|█▎           | 37/383 [00:07<01:10,  4.90it/s]Epoch: 2, train for the 38-th batch, train loss: 0.3863075077533722:  10%|█▎           | 38/383 [00:07<01:11,  4.81it/s]Epoch: 3, train for the 161-th batch, train loss: 0.570633590221405:  68%|████████    | 160/237 [00:31<00:19,  3.87it/s]evaluate for the 44-th batch, evaluate loss: 0.49306806921958923:  91%|███████████████▌ | 42/46 [00:03<00:00, 10.18it/s]evaluate for the 44-th batch, evaluate loss: 0.49306806921958923:  96%|████████████████▎| 44/46 [00:03<00:00, 10.84it/s]Epoch: 7, train for the 54-th batch, train loss: 0.4987120032310486:  36%|████▋        | 53/146 [00:07<00:14,  6.61it/s]Epoch: 3, train for the 161-th batch, train loss: 0.570633590221405:  68%|████████▏   | 161/237 [00:31<00:20,  3.73it/s]Epoch: 7, train for the 54-th batch, train loss: 0.4987120032310486:  37%|████▊        | 54/146 [00:07<00:13,  6.89it/s]Epoch: 8, train for the 68-th batch, train loss: 0.3784644901752472:  56%|███████▎     | 67/119 [00:10<00:08,  6.02it/s]Epoch: 4, train for the 125-th batch, train loss: 0.4655173718929291:  51%|█████▋     | 124/241 [00:19<00:19,  6.02it/s]Epoch: 2, train for the 39-th batch, train loss: 0.2647053599357605:  10%|█▎           | 38/383 [00:07<01:11,  4.81it/s]Epoch: 8, train for the 68-th batch, train loss: 0.3784644901752472:  57%|███████▍     | 68/119 [00:10<00:08,  5.91it/s]evaluate for the 45-th batch, evaluate loss: 0.45473891496658325:  96%|████████████████▎| 44/46 [00:04<00:00, 10.84it/s]Epoch: 2, train for the 39-th batch, train loss: 0.2647053599357605:  10%|█▎           | 39/383 [00:07<01:06,  5.20it/s]Epoch: 4, train for the 125-th batch, train loss: 0.4655173718929291:  52%|█████▋     | 125/241 [00:19<00:20,  5.65it/s]Epoch: 7, train for the 55-th batch, train loss: 0.4939565062522888:  37%|████▊        | 54/146 [00:07<00:13,  6.89it/s]Epoch: 7, train for the 55-th batch, train loss: 0.4939565062522888:  38%|████▉        | 55/146 [00:07<00:12,  7.15it/s]evaluate for the 46-th batch, evaluate loss: 0.44029632210731506:  96%|████████████████▎| 44/46 [00:04<00:00, 10.84it/s]evaluate for the 46-th batch, evaluate loss: 0.44029632210731506: 100%|█████████████████| 46/46 [00:04<00:00, 10.53it/s]evaluate for the 46-th batch, evaluate loss: 0.44029632210731506: 100%|█████████████████| 46/46 [00:04<00:00, 10.96it/s]
Epoch: 8, train for the 69-th batch, train loss: 0.3898543417453766:  57%|███████▍     | 68/119 [00:10<00:08,  5.91it/s]Epoch: 8, train for the 69-th batch, train loss: 0.3898543417453766:  58%|███████▌     | 69/119 [00:10<00:08,  5.96it/s]Epoch: 3, train for the 162-th batch, train loss: 0.5944066643714905:  68%|███████▍   | 161/237 [00:31<00:20,  3.73it/s]Epoch: 7, train for the 56-th batch, train loss: 0.5049504637718201:  38%|████▉        | 55/146 [00:07<00:12,  7.15it/s]Epoch: 2, train for the 40-th batch, train loss: 0.2826283872127533:  10%|█▎           | 39/383 [00:07<01:06,  5.20it/s]Epoch: 7, train for the 56-th batch, train loss: 0.5049504637718201:  38%|████▉        | 56/146 [00:07<00:12,  7.26it/s]Epoch: 3, train for the 162-th batch, train loss: 0.5944066643714905:  68%|███████▌   | 162/237 [00:31<00:20,  3.70it/s]Epoch: 2, train for the 40-th batch, train loss: 0.2826283872127533:  10%|█▎           | 40/383 [00:07<01:07,  5.08it/s]Epoch: 8, train for the 70-th batch, train loss: 0.36308127641677856:  58%|██████▉     | 69/119 [00:10<00:08,  5.96it/s]Epoch: 4, train for the 126-th batch, train loss: 0.40992575883865356:  52%|█████▏    | 125/241 [00:19<00:20,  5.65it/s]Epoch: 8, train for the 70-th batch, train loss: 0.36308127641677856:  59%|███████     | 70/119 [00:10<00:07,  6.26it/s]Epoch: 4, train for the 126-th batch, train loss: 0.40992575883865356:  52%|█████▏    | 126/241 [00:19<00:24,  4.72it/s]Epoch: 7, train for the 57-th batch, train loss: 0.5642737746238708:  38%|████▉        | 56/146 [00:08<00:12,  7.26it/s]Epoch: 7, train for the 57-th batch, train loss: 0.5642737746238708:  39%|█████        | 57/146 [00:08<00:11,  7.44it/s]Epoch: 2, train for the 41-th batch, train loss: 0.24420760571956635:  10%|█▎          | 40/383 [00:07<01:07,  5.08it/s]Epoch: 2, train for the 41-th batch, train loss: 0.24420760571956635:  11%|█▎          | 41/383 [00:07<01:03,  5.38it/s]Epoch: 3, train for the 163-th batch, train loss: 0.5621750354766846:  68%|███████▌   | 162/237 [00:31<00:20,  3.70it/s]Epoch: 3, train for the 163-th batch, train loss: 0.5621750354766846:  69%|███████▌   | 163/237 [00:32<00:19,  3.88it/s]Epoch: 4, train for the 127-th batch, train loss: 0.5265583992004395:  52%|█████▊     | 126/241 [00:19<00:24,  4.72it/s]Epoch: 8, train for the 71-th batch, train loss: 0.43573951721191406:  59%|███████     | 70/119 [00:10<00:07,  6.26it/s]Epoch: 4, train for the 127-th batch, train loss: 0.5265583992004395:  53%|█████▊     | 127/241 [00:19<00:22,  5.17it/s]Epoch: 8, train for the 71-th batch, train loss: 0.43573951721191406:  60%|███████▏    | 71/119 [00:10<00:07,  6.15it/s]Epoch: 7, train for the 58-th batch, train loss: 0.4541090428829193:  39%|█████        | 57/146 [00:08<00:11,  7.44it/s]Epoch: 7, train for the 58-th batch, train loss: 0.4541090428829193:  40%|█████▏       | 58/146 [00:08<00:12,  7.19it/s]Epoch: 2, train for the 42-th batch, train loss: 0.3146229386329651:  11%|█▍           | 41/383 [00:07<01:03,  5.38it/s]Epoch: 2, train for the 42-th batch, train loss: 0.3146229386329651:  11%|█▍           | 42/383 [00:07<01:00,  5.60it/s]  0%|                                                                                            | 0/25 [00:00<?, ?it/s]Epoch: 7, train for the 59-th batch, train loss: 0.4396418333053589:  40%|█████▏       | 58/146 [00:08<00:12,  7.19it/s]Epoch: 7, train for the 59-th batch, train loss: 0.4396418333053589:  40%|█████▎       | 59/146 [00:08<00:12,  7.07it/s]Epoch: 4, train for the 128-th batch, train loss: 0.5267942547798157:  53%|█████▊     | 127/241 [00:19<00:22,  5.17it/s]Epoch: 3, train for the 164-th batch, train loss: 0.5995071530342102:  69%|███████▌   | 163/237 [00:32<00:19,  3.88it/s]Epoch: 8, train for the 72-th batch, train loss: 0.43271464109420776:  60%|███████▏    | 71/119 [00:10<00:07,  6.15it/s]Epoch: 4, train for the 128-th batch, train loss: 0.5267942547798157:  53%|█████▊     | 128/241 [00:19<00:21,  5.25it/s]Epoch: 8, train for the 72-th batch, train loss: 0.43271464109420776:  61%|███████▎    | 72/119 [00:10<00:07,  5.99it/s]Epoch: 2, train for the 43-th batch, train loss: 0.30966854095458984:  11%|█▎          | 42/383 [00:08<01:00,  5.60it/s]Epoch: 3, train for the 164-th batch, train loss: 0.5995071530342102:  69%|███████▌   | 164/237 [00:32<00:18,  3.99it/s]evaluate for the 1-th batch, evaluate loss: 0.7780985832214355:   0%|                            | 0/25 [00:00<?, ?it/s]Epoch: 2, train for the 43-th batch, train loss: 0.30966854095458984:  11%|█▎          | 43/383 [00:08<00:57,  5.89it/s]Epoch: 7, train for the 60-th batch, train loss: 0.5045027136802673:  40%|█████▎       | 59/146 [00:08<00:12,  7.07it/s]Epoch: 7, train for the 60-th batch, train loss: 0.5045027136802673:  41%|█████▎       | 60/146 [00:08<00:11,  7.24it/s]evaluate for the 2-th batch, evaluate loss: 0.7815084457397461:   0%|                            | 0/25 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.7815084457397461:   8%|█▌                  | 2/25 [00:00<00:01, 12.30it/s]Epoch: 8, train for the 73-th batch, train loss: 0.4063032865524292:  61%|███████▊     | 72/119 [00:11<00:07,  5.99it/s]Epoch: 4, train for the 129-th batch, train loss: 0.4455142915248871:  53%|█████▊     | 128/241 [00:19<00:21,  5.25it/s]Epoch: 8, train for the 73-th batch, train loss: 0.4063032865524292:  61%|███████▉     | 73/119 [00:11<00:07,  5.94it/s]Epoch: 4, train for the 129-th batch, train loss: 0.4455142915248871:  54%|█████▉     | 129/241 [00:20<00:21,  5.26it/s]evaluate for the 3-th batch, evaluate loss: 0.8178746104240417:   8%|█▌                  | 2/25 [00:00<00:01, 12.30it/s]Epoch: 3, train for the 165-th batch, train loss: 0.5465642213821411:  69%|███████▌   | 164/237 [00:32<00:18,  3.99it/s]Epoch: 7, train for the 61-th batch, train loss: 0.4901385009288788:  41%|█████▎       | 60/146 [00:08<00:11,  7.24it/s]Epoch: 7, train for the 61-th batch, train loss: 0.4901385009288788:  42%|█████▍       | 61/146 [00:08<00:11,  7.39it/s]Epoch: 2, train for the 44-th batch, train loss: 0.2586566209793091:  11%|█▍           | 43/383 [00:08<00:57,  5.89it/s]Epoch: 3, train for the 165-th batch, train loss: 0.5465642213821411:  70%|███████▋   | 165/237 [00:32<00:17,  4.06it/s]evaluate for the 4-th batch, evaluate loss: 0.7645692825317383:   8%|█▌                  | 2/25 [00:00<00:01, 12.30it/s]evaluate for the 4-th batch, evaluate loss: 0.7645692825317383:  16%|███▏                | 4/25 [00:00<00:01, 12.88it/s]Epoch: 2, train for the 44-th batch, train loss: 0.2586566209793091:  11%|█▍           | 44/383 [00:08<01:03,  5.30it/s]Epoch: 8, train for the 74-th batch, train loss: 0.40380048751831055:  61%|███████▎    | 73/119 [00:11<00:07,  5.94it/s]Epoch: 8, train for the 74-th batch, train loss: 0.40380048751831055:  62%|███████▍    | 74/119 [00:11<00:07,  6.37it/s]Epoch: 4, train for the 130-th batch, train loss: 0.46669456362724304:  54%|█████▎    | 129/241 [00:20<00:21,  5.26it/s]Epoch: 4, train for the 130-th batch, train loss: 0.46669456362724304:  54%|█████▍    | 130/241 [00:20<00:19,  5.78it/s]evaluate for the 5-th batch, evaluate loss: 0.7625550627708435:  16%|███▏                | 4/25 [00:00<00:01, 12.88it/s]Epoch: 7, train for the 62-th batch, train loss: 0.4658794403076172:  42%|█████▍       | 61/146 [00:08<00:11,  7.39it/s]Epoch: 7, train for the 62-th batch, train loss: 0.4658794403076172:  42%|█████▌       | 62/146 [00:08<00:11,  7.56it/s]Epoch: 7, train for the 63-th batch, train loss: 0.501524806022644:  42%|█████▉        | 62/146 [00:08<00:11,  7.56it/s]Epoch: 7, train for the 63-th batch, train loss: 0.501524806022644:  43%|██████        | 63/146 [00:08<00:10,  7.81it/s]evaluate for the 6-th batch, evaluate loss: 0.769820511341095:  16%|███▎                 | 4/25 [00:00<00:01, 12.88it/s]evaluate for the 6-th batch, evaluate loss: 0.769820511341095:  24%|█████                | 6/25 [00:00<00:01, 10.98it/s]Epoch: 4, train for the 131-th batch, train loss: 0.5580753684043884:  54%|█████▉     | 130/241 [00:20<00:19,  5.78it/s]Epoch: 4, train for the 131-th batch, train loss: 0.5580753684043884:  54%|█████▉     | 131/241 [00:20<00:19,  5.63it/s]Epoch: 8, train for the 75-th batch, train loss: 0.3731016218662262:  62%|████████     | 74/119 [00:11<00:07,  6.37it/s]Epoch: 8, train for the 75-th batch, train loss: 0.3731016218662262:  63%|████████▏    | 75/119 [00:11<00:07,  5.55it/s]Epoch: 3, train for the 166-th batch, train loss: 0.5232316255569458:  70%|███████▋   | 165/237 [00:32<00:17,  4.06it/s]Epoch: 2, train for the 45-th batch, train loss: 0.31300175189971924:  11%|█▍          | 44/383 [00:08<01:03,  5.30it/s]evaluate for the 7-th batch, evaluate loss: 0.7637308835983276:  24%|████▊               | 6/25 [00:00<00:01, 10.98it/s]Epoch: 2, train for the 45-th batch, train loss: 0.31300175189971924:  12%|█▍          | 45/383 [00:08<01:13,  4.59it/s]Epoch: 3, train for the 166-th batch, train loss: 0.5232316255569458:  70%|███████▋   | 166/237 [00:32<00:19,  3.72it/s]Epoch: 7, train for the 64-th batch, train loss: 0.4972038269042969:  43%|█████▌       | 63/146 [00:08<00:10,  7.81it/s]Epoch: 7, train for the 64-th batch, train loss: 0.4972038269042969:  44%|█████▋       | 64/146 [00:08<00:10,  8.03it/s]Epoch: 4, train for the 132-th batch, train loss: 0.48186808824539185:  54%|█████▍    | 131/241 [00:20<00:19,  5.63it/s]Epoch: 8, train for the 76-th batch, train loss: 0.4529956579208374:  63%|████████▏    | 75/119 [00:11<00:07,  5.55it/s]Epoch: 8, train for the 76-th batch, train loss: 0.4529956579208374:  64%|████████▎    | 76/119 [00:11<00:07,  6.07it/s]evaluate for the 8-th batch, evaluate loss: 0.742695689201355:  24%|█████                | 6/25 [00:00<00:01, 10.98it/s]evaluate for the 8-th batch, evaluate loss: 0.742695689201355:  32%|██████▋              | 8/25 [00:00<00:01, 10.99it/s]Epoch: 4, train for the 132-th batch, train loss: 0.48186808824539185:  55%|█████▍    | 132/241 [00:20<00:18,  5.77it/s]Epoch: 7, train for the 65-th batch, train loss: 0.5146061182022095:  44%|█████▋       | 64/146 [00:09<00:10,  8.03it/s]Epoch: 7, train for the 65-th batch, train loss: 0.5146061182022095:  45%|█████▊       | 65/146 [00:09<00:10,  7.73it/s]Epoch: 2, train for the 46-th batch, train loss: 0.3286912739276886:  12%|█▌           | 45/383 [00:08<01:13,  4.59it/s]evaluate for the 9-th batch, evaluate loss: 0.7295820713043213:  32%|██████▍             | 8/25 [00:00<00:01, 10.99it/s]Epoch: 2, train for the 46-th batch, train loss: 0.3286912739276886:  12%|█▌           | 46/383 [00:08<01:11,  4.70it/s]Epoch: 3, train for the 167-th batch, train loss: 0.5857024192810059:  70%|███████▋   | 166/237 [00:33<00:19,  3.72it/s]Epoch: 4, train for the 133-th batch, train loss: 0.4164626896381378:  55%|██████     | 132/241 [00:20<00:18,  5.77it/s]Epoch: 8, train for the 77-th batch, train loss: 0.40619128942489624:  64%|███████▋    | 76/119 [00:11<00:07,  6.07it/s]Epoch: 8, train for the 77-th batch, train loss: 0.40619128942489624:  65%|███████▊    | 77/119 [00:11<00:06,  6.35it/s]Epoch: 4, train for the 133-th batch, train loss: 0.4164626896381378:  55%|██████     | 133/241 [00:20<00:17,  6.12it/s]evaluate for the 10-th batch, evaluate loss: 0.7710403203964233:  32%|██████             | 8/25 [00:00<00:01, 10.99it/s]Epoch: 3, train for the 167-th batch, train loss: 0.5857024192810059:  70%|███████▊   | 167/237 [00:33<00:18,  3.83it/s]evaluate for the 10-th batch, evaluate loss: 0.7710403203964233:  40%|███████▏          | 10/25 [00:00<00:01, 11.52it/s]Epoch: 7, train for the 66-th batch, train loss: 0.5361307263374329:  45%|█████▊       | 65/146 [00:09<00:10,  7.73it/s]Epoch: 7, train for the 66-th batch, train loss: 0.5361307263374329:  45%|█████▉       | 66/146 [00:09<00:10,  7.67it/s]Epoch: 2, train for the 47-th batch, train loss: 0.22797244787216187:  12%|█▍          | 46/383 [00:08<01:11,  4.70it/s]evaluate for the 11-th batch, evaluate loss: 0.7561445236206055:  40%|███████▏          | 10/25 [00:00<00:01, 11.52it/s]Epoch: 2, train for the 47-th batch, train loss: 0.22797244787216187:  12%|█▍          | 47/383 [00:08<01:06,  5.07it/s]Epoch: 8, train for the 78-th batch, train loss: 0.4677639603614807:  65%|████████▍    | 77/119 [00:11<00:06,  6.35it/s]Epoch: 8, train for the 78-th batch, train loss: 0.4677639603614807:  66%|████████▌    | 78/119 [00:11<00:06,  6.60it/s]Epoch: 4, train for the 134-th batch, train loss: 0.48912787437438965:  55%|█████▌    | 133/241 [00:20<00:17,  6.12it/s]Epoch: 4, train for the 134-th batch, train loss: 0.48912787437438965:  56%|█████▌    | 134/241 [00:20<00:17,  6.16it/s]evaluate for the 12-th batch, evaluate loss: 0.6677526831626892:  40%|███████▏          | 10/25 [00:01<00:01, 11.52it/s]evaluate for the 12-th batch, evaluate loss: 0.6677526831626892:  48%|████████▋         | 12/25 [00:01<00:01, 12.02it/s]Epoch: 7, train for the 67-th batch, train loss: 0.5107706189155579:  45%|█████▉       | 66/146 [00:09<00:10,  7.67it/s]Epoch: 7, train for the 67-th batch, train loss: 0.5107706189155579:  46%|█████▉       | 67/146 [00:09<00:10,  7.57it/s]Epoch: 3, train for the 168-th batch, train loss: 0.5881538987159729:  70%|███████▊   | 167/237 [00:33<00:18,  3.83it/s]evaluate for the 13-th batch, evaluate loss: 0.693264901638031:  48%|█████████          | 12/25 [00:01<00:01, 12.02it/s]Epoch: 3, train for the 168-th batch, train loss: 0.5881538987159729:  71%|███████▊   | 168/237 [00:33<00:17,  3.93it/s]Epoch: 8, train for the 79-th batch, train loss: 0.4064406454563141:  66%|████████▌    | 78/119 [00:11<00:06,  6.60it/s]Epoch: 2, train for the 48-th batch, train loss: 0.24095210433006287:  12%|█▍          | 47/383 [00:09<01:06,  5.07it/s]Epoch: 8, train for the 79-th batch, train loss: 0.4064406454563141:  66%|████████▋    | 79/119 [00:11<00:05,  6.73it/s]Epoch: 2, train for the 48-th batch, train loss: 0.24095210433006287:  13%|█▌          | 48/383 [00:09<01:04,  5.22it/s]Epoch: 4, train for the 135-th batch, train loss: 0.3901515007019043:  56%|██████     | 134/241 [00:20<00:17,  6.16it/s]Epoch: 7, train for the 68-th batch, train loss: 0.4870416224002838:  46%|█████▉       | 67/146 [00:09<00:10,  7.57it/s]Epoch: 7, train for the 68-th batch, train loss: 0.4870416224002838:  47%|██████       | 68/146 [00:09<00:10,  7.43it/s]evaluate for the 14-th batch, evaluate loss: 0.7217136025428772:  48%|████████▋         | 12/25 [00:01<00:01, 12.02it/s]evaluate for the 14-th batch, evaluate loss: 0.7217136025428772:  56%|██████████        | 14/25 [00:01<00:00, 11.95it/s]Epoch: 4, train for the 135-th batch, train loss: 0.3901515007019043:  56%|██████▏    | 135/241 [00:20<00:18,  5.88it/s]Epoch: 8, train for the 80-th batch, train loss: 0.44301214814186096:  66%|███████▉    | 79/119 [00:12<00:05,  6.73it/s]Epoch: 8, train for the 80-th batch, train loss: 0.44301214814186096:  67%|████████    | 80/119 [00:12<00:05,  6.53it/s]evaluate for the 15-th batch, evaluate loss: 0.778266429901123:  56%|██████████▋        | 14/25 [00:01<00:00, 11.95it/s]Epoch: 3, train for the 169-th batch, train loss: 0.5584765672683716:  71%|███████▊   | 168/237 [00:33<00:17,  3.93it/s]Epoch: 7, train for the 69-th batch, train loss: 0.5148773193359375:  47%|██████       | 68/146 [00:09<00:10,  7.43it/s]Epoch: 7, train for the 69-th batch, train loss: 0.5148773193359375:  47%|██████▏      | 69/146 [00:09<00:10,  7.36it/s]Epoch: 2, train for the 49-th batch, train loss: 0.3112366199493408:  13%|█▋           | 48/383 [00:09<01:04,  5.22it/s]Epoch: 3, train for the 169-th batch, train loss: 0.5584765672683716:  71%|███████▊   | 169/237 [00:33<00:17,  3.97it/s]Epoch: 4, train for the 136-th batch, train loss: 0.44642502069473267:  56%|█████▌    | 135/241 [00:21<00:18,  5.88it/s]Epoch: 2, train for the 49-th batch, train loss: 0.3112366199493408:  13%|█▋           | 49/383 [00:09<01:07,  4.92it/s]evaluate for the 16-th batch, evaluate loss: 0.6941784620285034:  56%|██████████        | 14/25 [00:01<00:00, 11.95it/s]evaluate for the 16-th batch, evaluate loss: 0.6941784620285034:  64%|███████████▌      | 16/25 [00:01<00:00, 11.46it/s]Epoch: 4, train for the 136-th batch, train loss: 0.44642502069473267:  56%|█████▋    | 136/241 [00:21<00:18,  5.75it/s]Epoch: 8, train for the 81-th batch, train loss: 0.4308279752731323:  67%|████████▋    | 80/119 [00:12<00:05,  6.53it/s]Epoch: 8, train for the 81-th batch, train loss: 0.4308279752731323:  68%|████████▊    | 81/119 [00:12<00:05,  6.43it/s]Epoch: 7, train for the 70-th batch, train loss: 0.49902015924453735:  47%|█████▋      | 69/146 [00:09<00:10,  7.36it/s]evaluate for the 17-th batch, evaluate loss: 0.6715191602706909:  64%|███████████▌      | 16/25 [00:01<00:00, 11.46it/s]Epoch: 7, train for the 70-th batch, train loss: 0.49902015924453735:  48%|█████▊      | 70/146 [00:09<00:10,  7.13it/s]Epoch: 4, train for the 137-th batch, train loss: 0.44135987758636475:  56%|█████▋    | 136/241 [00:21<00:18,  5.75it/s]Epoch: 4, train for the 137-th batch, train loss: 0.44135987758636475:  57%|█████▋    | 137/241 [00:21<00:17,  5.81it/s]evaluate for the 18-th batch, evaluate loss: 0.6507803201675415:  64%|███████████▌      | 16/25 [00:01<00:00, 11.46it/s]evaluate for the 18-th batch, evaluate loss: 0.6507803201675415:  72%|████████████▉     | 18/25 [00:01<00:00, 11.12it/s]Epoch: 3, train for the 170-th batch, train loss: 0.5898257493972778:  71%|███████▊   | 169/237 [00:33<00:17,  3.97it/s]Epoch: 2, train for the 50-th batch, train loss: 0.21407033503055573:  13%|█▌          | 49/383 [00:09<01:07,  4.92it/s]Epoch: 2, train for the 50-th batch, train loss: 0.21407033503055573:  13%|█▌          | 50/383 [00:09<01:11,  4.63it/s]Epoch: 3, train for the 170-th batch, train loss: 0.5898257493972778:  72%|███████▉   | 170/237 [00:33<00:17,  3.89it/s]Epoch: 8, train for the 82-th batch, train loss: 0.39359527826309204:  68%|████████▏   | 81/119 [00:12<00:05,  6.43it/s]Epoch: 7, train for the 71-th batch, train loss: 0.5268356800079346:  48%|██████▏      | 70/146 [00:09<00:10,  7.13it/s]evaluate for the 19-th batch, evaluate loss: 0.6168658137321472:  72%|████████████▉     | 18/25 [00:01<00:00, 11.12it/s]Epoch: 7, train for the 71-th batch, train loss: 0.5268356800079346:  49%|██████▎      | 71/146 [00:10<00:11,  6.48it/s]Epoch: 8, train for the 82-th batch, train loss: 0.39359527826309204:  69%|████████▎   | 82/119 [00:12<00:06,  5.77it/s]Epoch: 4, train for the 138-th batch, train loss: 0.38380447030067444:  57%|█████▋    | 137/241 [00:21<00:17,  5.81it/s]Epoch: 4, train for the 138-th batch, train loss: 0.38380447030067444:  57%|█████▋    | 138/241 [00:21<00:17,  5.90it/s]evaluate for the 20-th batch, evaluate loss: 0.6811527609825134:  72%|████████████▉     | 18/25 [00:01<00:00, 11.12it/s]evaluate for the 20-th batch, evaluate loss: 0.6811527609825134:  80%|██████████████▍   | 20/25 [00:01<00:00, 10.76it/s]Epoch: 7, train for the 72-th batch, train loss: 0.5221920013427734:  49%|██████▎      | 71/146 [00:10<00:11,  6.48it/s]Epoch: 7, train for the 72-th batch, train loss: 0.5221920013427734:  49%|██████▍      | 72/146 [00:10<00:11,  6.57it/s]Epoch: 3, train for the 171-th batch, train loss: 0.544538140296936:  72%|████████▌   | 170/237 [00:34<00:17,  3.89it/s]Epoch: 2, train for the 51-th batch, train loss: 0.2967946529388428:  13%|█▋           | 50/383 [00:09<01:11,  4.63it/s]Epoch: 4, train for the 139-th batch, train loss: 0.3300158679485321:  57%|██████▎    | 138/241 [00:21<00:17,  5.90it/s]Epoch: 2, train for the 51-th batch, train loss: 0.2967946529388428:  13%|█▋           | 51/383 [00:09<01:15,  4.42it/s]Epoch: 3, train for the 171-th batch, train loss: 0.544538140296936:  72%|████████▋   | 171/237 [00:34<00:16,  3.94it/s]evaluate for the 21-th batch, evaluate loss: 0.6949630379676819:  80%|██████████████▍   | 20/25 [00:01<00:00, 10.76it/s]Epoch: 4, train for the 139-th batch, train loss: 0.3300158679485321:  58%|██████▎    | 139/241 [00:21<00:17,  5.98it/s]Epoch: 8, train for the 83-th batch, train loss: 0.4764414131641388:  69%|████████▉    | 82/119 [00:12<00:06,  5.77it/s]Epoch: 8, train for the 83-th batch, train loss: 0.4764414131641388:  70%|█████████    | 83/119 [00:12<00:06,  5.15it/s]Epoch: 7, train for the 73-th batch, train loss: 0.47785118222236633:  49%|█████▉      | 72/146 [00:10<00:11,  6.57it/s]Epoch: 7, train for the 73-th batch, train loss: 0.47785118222236633:  50%|██████      | 73/146 [00:10<00:10,  7.20it/s]evaluate for the 22-th batch, evaluate loss: 0.6383159756660461:  80%|██████████████▍   | 20/25 [00:01<00:00, 10.76it/s]evaluate for the 22-th batch, evaluate loss: 0.6383159756660461:  88%|███████████████▊  | 22/25 [00:01<00:00, 10.24it/s]Epoch: 4, train for the 140-th batch, train loss: 0.38671454787254333:  58%|█████▊    | 139/241 [00:21<00:17,  5.98it/s]evaluate for the 23-th batch, evaluate loss: 0.6566632986068726:  88%|███████████████▊  | 22/25 [00:02<00:00, 10.24it/s]Epoch: 4, train for the 140-th batch, train loss: 0.38671454787254333:  58%|█████▊    | 140/241 [00:21<00:17,  5.69it/s]Epoch: 3, train for the 172-th batch, train loss: 0.5350255966186523:  72%|███████▉   | 171/237 [00:34<00:16,  3.94it/s]Epoch: 2, train for the 52-th batch, train loss: 0.242796391248703:  13%|█▊            | 51/383 [00:10<01:15,  4.42it/s]Epoch: 8, train for the 84-th batch, train loss: 0.4411598742008209:  70%|█████████    | 83/119 [00:12<00:06,  5.15it/s]Epoch: 3, train for the 172-th batch, train loss: 0.5350255966186523:  73%|███████▉   | 172/237 [00:34<00:16,  3.99it/s]Epoch: 7, train for the 74-th batch, train loss: 0.49969470500946045:  50%|██████      | 73/146 [00:10<00:10,  7.20it/s]Epoch: 8, train for the 84-th batch, train loss: 0.4411598742008209:  71%|█████████▏   | 84/119 [00:12<00:06,  5.04it/s]Epoch: 7, train for the 74-th batch, train loss: 0.49969470500946045:  51%|██████      | 74/146 [00:10<00:11,  6.28it/s]Epoch: 2, train for the 52-th batch, train loss: 0.242796391248703:  14%|█▉            | 52/383 [00:10<01:19,  4.16it/s]evaluate for the 24-th batch, evaluate loss: 0.6706336736679077:  88%|███████████████▊  | 22/25 [00:02<00:00, 10.24it/s]evaluate for the 24-th batch, evaluate loss: 0.6706336736679077:  96%|█████████████████▎| 24/25 [00:02<00:00, 10.82it/s]Epoch: 4, train for the 141-th batch, train loss: 0.4762805700302124:  58%|██████▍    | 140/241 [00:21<00:17,  5.69it/s]Epoch: 4, train for the 141-th batch, train loss: 0.4762805700302124:  59%|██████▍    | 141/241 [00:22<00:16,  5.95it/s]evaluate for the 25-th batch, evaluate loss: 0.6280504465103149:  96%|█████████████████▎| 24/25 [00:02<00:00, 10.82it/s]evaluate for the 25-th batch, evaluate loss: 0.6280504465103149: 100%|██████████████████| 25/25 [00:02<00:00, 11.06it/s]
Epoch: 8, train for the 85-th batch, train loss: 0.4044472873210907:  71%|█████████▏   | 84/119 [00:13<00:06,  5.04it/s]Epoch: 7, train for the 75-th batch, train loss: 0.5446478724479675:  51%|██████▌      | 74/146 [00:10<00:11,  6.28it/s]Epoch: 8, train for the 85-th batch, train loss: 0.4044472873210907:  71%|█████████▎   | 85/119 [00:13<00:06,  5.05it/s]Epoch: 7, train for the 75-th batch, train loss: 0.5446478724479675:  51%|██████▋      | 75/146 [00:10<00:11,  5.94it/s]Epoch: 3, train for the 173-th batch, train loss: 0.5105278491973877:  73%|███████▉   | 172/237 [00:34<00:16,  3.99it/s]Epoch: 4, train for the 142-th batch, train loss: 0.43143942952156067:  59%|█████▊    | 141/241 [00:22<00:16,  5.95it/s]Epoch: 2, train for the 53-th batch, train loss: 0.29271841049194336:  14%|█▋          | 52/383 [00:10<01:19,  4.16it/s]Epoch: 3, train for the 173-th batch, train loss: 0.5105278491973877:  73%|████████   | 173/237 [00:34<00:16,  3.99it/s]Epoch: 2, train for the 53-th batch, train loss: 0.29271841049194336:  14%|█▋          | 53/383 [00:10<01:18,  4.22it/s]Epoch: 4, train for the 142-th batch, train loss: 0.43143942952156067:  59%|█████▉    | 142/241 [00:22<00:16,  6.07it/s]Epoch: 8, train for the 86-th batch, train loss: 0.4628865718841553:  71%|█████████▎   | 85/119 [00:13<00:06,  5.05it/s]Epoch: 8, train for the 86-th batch, train loss: 0.4628865718841553:  72%|█████████▍   | 86/119 [00:13<00:05,  5.54it/s]Epoch: 7, train for the 76-th batch, train loss: 0.5289198756217957:  51%|██████▋      | 75/146 [00:10<00:11,  5.94it/s]Epoch: 7, train for the 76-th batch, train loss: 0.5289198756217957:  52%|██████▊      | 76/146 [00:10<00:11,  5.87it/s]INFO:root:Epoch: 5, learning rate: 0.0001, train loss: 0.5693
INFO:root:train average_precision, 0.7672
INFO:root:train roc_auc, 0.7488
INFO:root:validate loss: 0.5075
INFO:root:validate average_precision, 0.8421
INFO:root:validate roc_auc, 0.8372
INFO:root:new node validate loss: 0.7161
INFO:root:new node validate first_1_average_precision, 0.5488
INFO:root:new node validate first_1_roc_auc, 0.5541
INFO:root:new node validate first_3_average_precision, 0.5824
INFO:root:new node validate first_3_roc_auc, 0.5813
INFO:root:new node validate first_10_average_precision, 0.6336
INFO:root:new node validate first_10_roc_auc, 0.6358
INFO:root:new node validate average_precision, 0.6549
INFO:root:new node validate roc_auc, 0.6573
INFO:root:save model ./saved_models/TGN/ia-retweet-pol/TGN_seed0_tgn-ia-retweet-pol-lincorrect-time-linear/TGN_seed0_tgn-ia-retweet-pol-lincorrect-time-linear.pkl
Epoch: 4, train for the 143-th batch, train loss: 0.43874090909957886:  59%|█████▉    | 142/241 [00:22<00:16,  6.07it/s]Epoch: 2, train for the 54-th batch, train loss: 0.29936450719833374:  14%|█▋          | 53/383 [00:10<01:18,  4.22it/s]Epoch: 4, train for the 143-th batch, train loss: 0.43874090909957886:  59%|█████▉    | 143/241 [00:22<00:16,  5.89it/s]Epoch: 2, train for the 54-th batch, train loss: 0.29936450719833374:  14%|█▋          | 54/383 [00:10<01:14,  4.44it/s]Epoch: 3, train for the 174-th batch, train loss: 0.5285589694976807:  73%|████████   | 173/237 [00:34<00:16,  3.99it/s]Epoch: 8, train for the 87-th batch, train loss: 0.4211651384830475:  72%|█████████▍   | 86/119 [00:13<00:05,  5.54it/s]Epoch: 8, train for the 87-th batch, train loss: 0.4211651384830475:  73%|█████████▌   | 87/119 [00:13<00:05,  5.83it/s]Epoch: 3, train for the 174-th batch, train loss: 0.5285589694976807:  73%|████████   | 174/237 [00:34<00:15,  3.95it/s]Epoch: 7, train for the 77-th batch, train loss: 0.5425388216972351:  52%|██████▊      | 76/146 [00:10<00:11,  5.87it/s]Epoch: 7, train for the 77-th batch, train loss: 0.5425388216972351:  53%|██████▊      | 77/146 [00:10<00:11,  6.07it/s]Epoch: 4, train for the 144-th batch, train loss: 0.5779504776000977:  59%|██████▌    | 143/241 [00:22<00:16,  5.89it/s]Epoch: 4, train for the 144-th batch, train loss: 0.5779504776000977:  60%|██████▌    | 144/241 [00:22<00:15,  6.44it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5217181444168091:  14%|█▊           | 54/383 [00:10<01:14,  4.44it/s]Epoch: 8, train for the 88-th batch, train loss: 0.4461347460746765:  73%|█████████▌   | 87/119 [00:13<00:05,  5.83it/s]Epoch: 2, train for the 55-th batch, train loss: 0.5217181444168091:  14%|█▊           | 55/383 [00:10<01:08,  4.80it/s]Epoch: 8, train for the 88-th batch, train loss: 0.4461347460746765:  74%|█████████▌   | 88/119 [00:13<00:05,  6.17it/s]Epoch: 7, train for the 78-th batch, train loss: 0.5370425581932068:  53%|██████▊      | 77/146 [00:11<00:11,  6.07it/s]Epoch: 3, train for the 175-th batch, train loss: 0.597144603729248:  73%|████████▊   | 174/237 [00:34<00:15,  3.95it/s]Epoch: 4, train for the 145-th batch, train loss: 0.591776967048645:  60%|███████▏    | 144/241 [00:22<00:15,  6.44it/s]Epoch: 7, train for the 78-th batch, train loss: 0.5370425581932068:  53%|██████▉      | 78/146 [00:11<00:10,  6.24it/s]Epoch: 4, train for the 145-th batch, train loss: 0.591776967048645:  60%|███████▏    | 145/241 [00:22<00:14,  6.81it/s]Epoch: 3, train for the 175-th batch, train loss: 0.597144603729248:  74%|████████▊   | 175/237 [00:35<00:14,  4.15it/s]Epoch: 2, train for the 56-th batch, train loss: 0.3365209102630615:  14%|█▊           | 55/383 [00:10<01:08,  4.80it/s]Epoch: 8, train for the 89-th batch, train loss: 0.45005354285240173:  74%|████████▊   | 88/119 [00:13<00:05,  6.17it/s]Epoch: 4, train for the 146-th batch, train loss: 0.5588343739509583:  60%|██████▌    | 145/241 [00:22<00:14,  6.81it/s]Epoch: 8, train for the 89-th batch, train loss: 0.45005354285240173:  75%|████████▉   | 89/119 [00:13<00:05,  5.92it/s]Epoch: 2, train for the 56-th batch, train loss: 0.3365209102630615:  15%|█▉           | 56/383 [00:10<01:06,  4.90it/s]Epoch: 4, train for the 146-th batch, train loss: 0.5588343739509583:  61%|██████▋    | 146/241 [00:22<00:13,  7.13it/s]Epoch: 7, train for the 79-th batch, train loss: 0.5258813500404358:  53%|██████▉      | 78/146 [00:11<00:10,  6.24it/s]Epoch: 7, train for the 79-th batch, train loss: 0.5258813500404358:  54%|███████      | 79/146 [00:11<00:10,  6.16it/s]Epoch: 3, train for the 176-th batch, train loss: 0.5337803363800049:  74%|████████   | 175/237 [00:35<00:14,  4.15it/s]Epoch: 3, train for the 176-th batch, train loss: 0.5337803363800049:  74%|████████▏  | 176/237 [00:35<00:13,  4.42it/s]Epoch: 4, train for the 147-th batch, train loss: 0.6126989126205444:  61%|██████▋    | 146/241 [00:22<00:13,  7.13it/s]Epoch: 4, train for the 147-th batch, train loss: 0.6126989126205444:  61%|██████▋    | 147/241 [00:22<00:13,  6.84it/s]Epoch: 2, train for the 57-th batch, train loss: 0.26788270473480225:  15%|█▊          | 56/383 [00:11<01:06,  4.90it/s]Epoch: 2, train for the 57-th batch, train loss: 0.26788270473480225:  15%|█▊          | 57/383 [00:11<01:05,  4.96it/s]Epoch: 8, train for the 90-th batch, train loss: 0.4382679760456085:  75%|█████████▋   | 89/119 [00:13<00:05,  5.92it/s]Epoch: 7, train for the 80-th batch, train loss: 0.5294700264930725:  54%|███████      | 79/146 [00:11<00:10,  6.16it/s]Epoch: 8, train for the 90-th batch, train loss: 0.4382679760456085:  76%|█████████▊   | 90/119 [00:13<00:05,  5.51it/s]Epoch: 7, train for the 80-th batch, train loss: 0.5294700264930725:  55%|███████      | 80/146 [00:11<00:11,  5.95it/s]Epoch: 3, train for the 177-th batch, train loss: 0.5981148481369019:  74%|████████▏  | 176/237 [00:35<00:13,  4.42it/s]Epoch: 4, train for the 148-th batch, train loss: 0.46423375606536865:  61%|██████    | 147/241 [00:22<00:13,  6.84it/s]Epoch: 4, train for the 148-th batch, train loss: 0.46423375606536865:  61%|██████▏   | 148/241 [00:23<00:12,  7.18it/s]Epoch: 3, train for the 177-th batch, train loss: 0.5981148481369019:  75%|████████▏  | 177/237 [00:35<00:12,  4.62it/s]Epoch: 7, train for the 81-th batch, train loss: 0.4856124520301819:  55%|███████      | 80/146 [00:11<00:11,  5.95it/s]Epoch: 7, train for the 81-th batch, train loss: 0.4856124520301819:  55%|███████▏     | 81/146 [00:11<00:10,  6.32it/s]Epoch: 2, train for the 58-th batch, train loss: 0.3031150996685028:  15%|█▉           | 57/383 [00:11<01:05,  4.96it/s]Epoch: 8, train for the 91-th batch, train loss: 0.4115632474422455:  76%|█████████▊   | 90/119 [00:14<00:05,  5.51it/s]Epoch: 2, train for the 58-th batch, train loss: 0.3031150996685028:  15%|█▉           | 58/383 [00:11<01:03,  5.15it/s]Epoch: 8, train for the 91-th batch, train loss: 0.4115632474422455:  76%|█████████▉   | 91/119 [00:14<00:04,  5.64it/s]Epoch: 4, train for the 149-th batch, train loss: 0.3820878267288208:  61%|██████▊    | 148/241 [00:23<00:12,  7.18it/s]Epoch: 4, train for the 149-th batch, train loss: 0.3820878267288208:  62%|██████▊    | 149/241 [00:23<00:12,  7.56it/s]Epoch: 7, train for the 82-th batch, train loss: 0.5100216269493103:  55%|███████▏     | 81/146 [00:11<00:10,  6.32it/s]Epoch: 3, train for the 178-th batch, train loss: 0.5417999029159546:  75%|████████▏  | 177/237 [00:35<00:12,  4.62it/s]Epoch: 7, train for the 82-th batch, train loss: 0.5100216269493103:  56%|███████▎     | 82/146 [00:11<00:09,  6.81it/s]Epoch: 3, train for the 178-th batch, train loss: 0.5417999029159546:  75%|████████▎  | 178/237 [00:35<00:12,  4.73it/s]Epoch: 4, train for the 150-th batch, train loss: 0.34012943506240845:  62%|██████▏   | 149/241 [00:23<00:12,  7.56it/s]Epoch: 8, train for the 92-th batch, train loss: 0.4583722949028015:  76%|█████████▉   | 91/119 [00:14<00:04,  5.64it/s]Epoch: 4, train for the 150-th batch, train loss: 0.34012943506240845:  62%|██████▏   | 150/241 [00:23<00:11,  7.78it/s]Epoch: 2, train for the 59-th batch, train loss: 0.2535758912563324:  15%|█▉           | 58/383 [00:11<01:03,  5.15it/s]Epoch: 8, train for the 92-th batch, train loss: 0.4583722949028015:  77%|██████████   | 92/119 [00:14<00:04,  5.92it/s]Epoch: 2, train for the 59-th batch, train loss: 0.2535758912563324:  15%|██           | 59/383 [00:11<01:00,  5.34it/s]Epoch: 7, train for the 83-th batch, train loss: 0.5153400897979736:  56%|███████▎     | 82/146 [00:11<00:09,  6.81it/s]Epoch: 7, train for the 83-th batch, train loss: 0.5153400897979736:  57%|███████▍     | 83/146 [00:11<00:09,  6.92it/s]Epoch: 4, train for the 151-th batch, train loss: 0.31341856718063354:  62%|██████▏   | 150/241 [00:23<00:11,  7.78it/s]Epoch: 4, train for the 151-th batch, train loss: 0.31341856718063354:  63%|██████▎   | 151/241 [00:23<00:11,  7.84it/s]Epoch: 8, train for the 93-th batch, train loss: 0.37517234683036804:  77%|█████████▎  | 92/119 [00:14<00:04,  5.92it/s]Epoch: 8, train for the 93-th batch, train loss: 0.37517234683036804:  78%|█████████▍  | 93/119 [00:14<00:04,  6.23it/s]Epoch: 3, train for the 179-th batch, train loss: 0.5693315863609314:  75%|████████▎  | 178/237 [00:35<00:12,  4.73it/s]Epoch: 7, train for the 84-th batch, train loss: 0.495025634765625:  57%|███████▉      | 83/146 [00:11<00:09,  6.92it/s]Epoch: 2, train for the 60-th batch, train loss: 0.22761783003807068:  15%|█▊          | 59/383 [00:11<01:00,  5.34it/s]Epoch: 7, train for the 84-th batch, train loss: 0.495025634765625:  58%|████████      | 84/146 [00:11<00:08,  7.26it/s]Epoch: 2, train for the 60-th batch, train loss: 0.22761783003807068:  16%|█▉          | 60/383 [00:11<01:01,  5.24it/s]Epoch: 3, train for the 179-th batch, train loss: 0.5693315863609314:  76%|████████▎  | 179/237 [00:35<00:13,  4.44it/s]Epoch: 4, train for the 152-th batch, train loss: 0.251697301864624:  63%|███████▌    | 151/241 [00:23<00:11,  7.84it/s]Epoch: 4, train for the 152-th batch, train loss: 0.251697301864624:  63%|███████▌    | 152/241 [00:23<00:11,  7.89it/s]Epoch: 8, train for the 94-th batch, train loss: 0.3991911709308624:  78%|██████████▏  | 93/119 [00:14<00:04,  6.23it/s]Epoch: 8, train for the 94-th batch, train loss: 0.3991911709308624:  79%|██████████▎  | 94/119 [00:14<00:03,  6.61it/s]Epoch: 7, train for the 85-th batch, train loss: 0.509182870388031:  58%|████████      | 84/146 [00:12<00:08,  7.26it/s]Epoch: 7, train for the 85-th batch, train loss: 0.509182870388031:  58%|████████▏     | 85/146 [00:12<00:07,  7.66it/s]Epoch: 4, train for the 153-th batch, train loss: 0.34242185950279236:  63%|██████▎   | 152/241 [00:23<00:11,  7.89it/s]Epoch: 4, train for the 153-th batch, train loss: 0.34242185950279236:  63%|██████▎   | 153/241 [00:23<00:11,  7.91it/s]Epoch: 8, train for the 95-th batch, train loss: 0.32103773951530457:  79%|█████████▍  | 94/119 [00:14<00:03,  6.61it/s]Epoch: 2, train for the 61-th batch, train loss: 0.2905559241771698:  16%|██           | 60/383 [00:11<01:01,  5.24it/s]Epoch: 8, train for the 95-th batch, train loss: 0.32103773951530457:  80%|█████████▌  | 95/119 [00:14<00:03,  6.71it/s]Epoch: 2, train for the 61-th batch, train loss: 0.2905559241771698:  16%|██           | 61/383 [00:11<01:03,  5.09it/s]Epoch: 7, train for the 86-th batch, train loss: 0.4878312647342682:  58%|███████▌     | 85/146 [00:12<00:07,  7.66it/s]Epoch: 3, train for the 180-th batch, train loss: 0.5632940530776978:  76%|████████▎  | 179/237 [00:36<00:13,  4.44it/s]Epoch: 7, train for the 86-th batch, train loss: 0.4878312647342682:  59%|███████▋     | 86/146 [00:12<00:07,  7.55it/s]Epoch: 4, train for the 154-th batch, train loss: 0.2735963463783264:  63%|██████▉    | 153/241 [00:23<00:11,  7.91it/s]Epoch: 3, train for the 180-th batch, train loss: 0.5632940530776978:  76%|████████▎  | 180/237 [00:36<00:13,  4.22it/s]Epoch: 4, train for the 154-th batch, train loss: 0.2735963463783264:  64%|███████    | 154/241 [00:23<00:11,  7.81it/s]Epoch: 8, train for the 96-th batch, train loss: 0.37939730286598206:  80%|█████████▌  | 95/119 [00:14<00:03,  6.71it/s]Epoch: 8, train for the 96-th batch, train loss: 0.37939730286598206:  81%|█████████▋  | 96/119 [00:14<00:03,  6.72it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]Epoch: 2, train for the 62-th batch, train loss: 0.2470446079969406:  16%|██           | 61/383 [00:12<01:03,  5.09it/s]Epoch: 7, train for the 87-th batch, train loss: 0.5262243747711182:  59%|███████▋     | 86/146 [00:12<00:07,  7.55it/s]Epoch: 2, train for the 62-th batch, train loss: 0.2470446079969406:  16%|██           | 62/383 [00:12<01:00,  5.33it/s]Epoch: 7, train for the 87-th batch, train loss: 0.5262243747711182:  60%|███████▋     | 87/146 [00:12<00:08,  7.28it/s]Epoch: 4, train for the 155-th batch, train loss: 0.31825533509254456:  64%|██████▍   | 154/241 [00:23<00:11,  7.81it/s]Epoch: 4, train for the 155-th batch, train loss: 0.31825533509254456:  64%|██████▍   | 155/241 [00:23<00:11,  7.78it/s]Epoch: 3, train for the 181-th batch, train loss: 0.5626740455627441:  76%|████████▎  | 180/237 [00:36<00:13,  4.22it/s]Epoch: 3, train for the 181-th batch, train loss: 0.5626740455627441:  76%|████████▍  | 181/237 [00:36<00:12,  4.33it/s]Epoch: 8, train for the 97-th batch, train loss: 0.46155861020088196:  81%|█████████▋  | 96/119 [00:14<00:03,  6.72it/s]Epoch: 8, train for the 97-th batch, train loss: 0.46155861020088196:  82%|█████████▊  | 97/119 [00:15<00:03,  6.59it/s]Epoch: 7, train for the 88-th batch, train loss: 0.5707845687866211:  60%|███████▋     | 87/146 [00:12<00:08,  7.28it/s]Epoch: 7, train for the 88-th batch, train loss: 0.5707845687866211:  60%|███████▊     | 88/146 [00:12<00:08,  7.04it/s]Epoch: 4, train for the 156-th batch, train loss: 0.2893233299255371:  64%|███████    | 155/241 [00:23<00:11,  7.78it/s]Epoch: 2, train for the 63-th batch, train loss: 0.290741503238678:  16%|██▎           | 62/383 [00:12<01:00,  5.33it/s]Epoch: 2, train for the 63-th batch, train loss: 0.290741503238678:  16%|██▎           | 63/383 [00:12<00:59,  5.34it/s]Epoch: 4, train for the 156-th batch, train loss: 0.2893233299255371:  65%|███████    | 156/241 [00:24<00:11,  7.33it/s]Epoch: 6, train for the 1-th batch, train loss: 0.7337307333946228:   0%|                       | 0/151 [00:00<?, ?it/s]Epoch: 6, train for the 1-th batch, train loss: 0.7337307333946228:   1%|               | 1/151 [00:00<00:34,  4.39it/s]Epoch: 7, train for the 89-th batch, train loss: 0.5190600752830505:  60%|███████▊     | 88/146 [00:12<00:08,  7.04it/s]Epoch: 7, train for the 89-th batch, train loss: 0.5190600752830505:  61%|███████▉     | 89/146 [00:12<00:07,  7.34it/s]Epoch: 3, train for the 182-th batch, train loss: 0.5852442383766174:  76%|████████▍  | 181/237 [00:36<00:12,  4.33it/s]Epoch: 6, train for the 2-th batch, train loss: 0.736196756362915:   1%|                | 1/151 [00:00<00:34,  4.39it/s]Epoch: 6, train for the 2-th batch, train loss: 0.736196756362915:   1%|▏               | 2/151 [00:00<00:23,  6.27it/s]Epoch: 2, train for the 64-th batch, train loss: 0.2663431763648987:  16%|██▏          | 63/383 [00:12<00:59,  5.34it/s]Epoch: 3, train for the 182-th batch, train loss: 0.5852442383766174:  77%|████████▍  | 182/237 [00:36<00:12,  4.34it/s]Epoch: 2, train for the 64-th batch, train loss: 0.2663431763648987:  17%|██▏          | 64/383 [00:12<00:57,  5.52it/s]Epoch: 4, train for the 157-th batch, train loss: 0.19994017481803894:  65%|██████▍   | 156/241 [00:24<00:11,  7.33it/s]Epoch: 7, train for the 90-th batch, train loss: 0.5300124287605286:  61%|███████▉     | 89/146 [00:12<00:07,  7.34it/s]Epoch: 7, train for the 90-th batch, train loss: 0.5300124287605286:  62%|████████     | 90/146 [00:12<00:07,  7.84it/s]Epoch: 4, train for the 157-th batch, train loss: 0.19994017481803894:  65%|██████▌   | 157/241 [00:24<00:13,  6.46it/s]Epoch: 8, train for the 98-th batch, train loss: 0.37560293078422546:  82%|█████████▊  | 97/119 [00:15<00:03,  6.59it/s]Epoch: 6, train for the 3-th batch, train loss: 0.7284922003746033:   1%|▏              | 2/151 [00:00<00:23,  6.27it/s]Epoch: 6, train for the 3-th batch, train loss: 0.7284922003746033:   2%|▎              | 3/151 [00:00<00:19,  7.44it/s]Epoch: 8, train for the 98-th batch, train loss: 0.37560293078422546:  82%|█████████▉  | 98/119 [00:15<00:04,  5.07it/s]Epoch: 7, train for the 91-th batch, train loss: 0.509972333908081:  62%|████████▋     | 90/146 [00:12<00:07,  7.84it/s]Epoch: 7, train for the 91-th batch, train loss: 0.509972333908081:  62%|████████▋     | 91/146 [00:12<00:07,  7.12it/s]Epoch: 2, train for the 65-th batch, train loss: 0.3322475254535675:  17%|██▏          | 64/383 [00:12<00:57,  5.52it/s]Epoch: 8, train for the 99-th batch, train loss: 0.41790610551834106:  82%|█████████▉  | 98/119 [00:15<00:04,  5.07it/s]Epoch: 2, train for the 65-th batch, train loss: 0.3322475254535675:  17%|██▏          | 65/383 [00:12<01:03,  5.01it/s]Epoch: 8, train for the 99-th batch, train loss: 0.41790610551834106:  83%|█████████▉  | 99/119 [00:15<00:03,  5.30it/s]Epoch: 3, train for the 183-th batch, train loss: 0.549686074256897:  77%|█████████▏  | 182/237 [00:36<00:12,  4.34it/s]Epoch: 4, train for the 158-th batch, train loss: 0.22733677923679352:  65%|██████▌   | 157/241 [00:24<00:13,  6.46it/s]Epoch: 6, train for the 4-th batch, train loss: 0.7105849981307983:   2%|▎              | 3/151 [00:00<00:19,  7.44it/s]Epoch: 6, train for the 4-th batch, train loss: 0.7105849981307983:   3%|▍              | 4/151 [00:00<00:24,  6.02it/s]Epoch: 4, train for the 158-th batch, train loss: 0.22733677923679352:  66%|██████▌   | 158/241 [00:24<00:15,  5.50it/s]Epoch: 3, train for the 183-th batch, train loss: 0.549686074256897:  77%|█████████▎  | 183/237 [00:36<00:13,  3.93it/s]Epoch: 7, train for the 92-th batch, train loss: 0.5147668719291687:  62%|████████     | 91/146 [00:13<00:07,  7.12it/s]Epoch: 7, train for the 92-th batch, train loss: 0.5147668719291687:  63%|████████▏    | 92/146 [00:13<00:07,  7.09it/s]Epoch: 8, train for the 100-th batch, train loss: 0.4039696455001831:  83%|█████████▉  | 99/119 [00:15<00:03,  5.30it/s]Epoch: 8, train for the 100-th batch, train loss: 0.4039696455001831:  84%|█████████▏ | 100/119 [00:15<00:03,  5.63it/s]Epoch: 6, train for the 5-th batch, train loss: 0.7093110680580139:   3%|▍              | 4/151 [00:00<00:24,  6.02it/s]Epoch: 6, train for the 5-th batch, train loss: 0.7093110680580139:   3%|▍              | 5/151 [00:00<00:22,  6.57it/s]Epoch: 2, train for the 66-th batch, train loss: 0.30511361360549927:  17%|██          | 65/383 [00:12<01:03,  5.01it/s]Epoch: 4, train for the 159-th batch, train loss: 0.18310615420341492:  66%|██████▌   | 158/241 [00:24<00:15,  5.50it/s]Epoch: 2, train for the 66-th batch, train loss: 0.30511361360549927:  17%|██          | 66/383 [00:12<01:04,  4.91it/s]Epoch: 3, train for the 184-th batch, train loss: 0.5520232319831848:  77%|████████▍  | 183/237 [00:37<00:13,  3.93it/s]Epoch: 4, train for the 159-th batch, train loss: 0.18310615420341492:  66%|██████▌   | 159/241 [00:24<00:15,  5.42it/s]Epoch: 3, train for the 184-th batch, train loss: 0.5520232319831848:  78%|████████▌  | 184/237 [00:37<00:12,  4.21it/s]Epoch: 7, train for the 93-th batch, train loss: 0.5248512625694275:  63%|████████▏    | 92/146 [00:13<00:07,  7.09it/s]Epoch: 7, train for the 93-th batch, train loss: 0.5248512625694275:  64%|████████▎    | 93/146 [00:13<00:07,  6.87it/s]Epoch: 6, train for the 6-th batch, train loss: 0.7110640406608582:   3%|▍              | 5/151 [00:00<00:22,  6.57it/s]Epoch: 8, train for the 101-th batch, train loss: 0.40990957617759705:  84%|████████▍ | 100/119 [00:15<00:03,  5.63it/s]Epoch: 6, train for the 6-th batch, train loss: 0.7110640406608582:   4%|▌              | 6/151 [00:00<00:20,  7.01it/s]Epoch: 8, train for the 101-th batch, train loss: 0.40990957617759705:  85%|████████▍ | 101/119 [00:15<00:03,  5.97it/s]Epoch: 4, train for the 160-th batch, train loss: 0.522635281085968:  66%|███████▉    | 159/241 [00:24<00:15,  5.42it/s]Epoch: 4, train for the 160-th batch, train loss: 0.522635281085968:  66%|███████▉    | 160/241 [00:24<00:14,  5.56it/s]Epoch: 2, train for the 67-th batch, train loss: 0.3746757507324219:  17%|██▏          | 66/383 [00:13<01:04,  4.91it/s]Epoch: 6, train for the 7-th batch, train loss: 0.6814515590667725:   4%|▌              | 6/151 [00:01<00:20,  7.01it/s]Epoch: 7, train for the 94-th batch, train loss: 0.5070779919624329:  64%|████████▎    | 93/146 [00:13<00:07,  6.87it/s]Epoch: 6, train for the 7-th batch, train loss: 0.6814515590667725:   5%|▋              | 7/151 [00:01<00:20,  6.90it/s]Epoch: 7, train for the 94-th batch, train loss: 0.5070779919624329:  64%|████████▎    | 94/146 [00:13<00:08,  6.38it/s]Epoch: 2, train for the 67-th batch, train loss: 0.3746757507324219:  17%|██▎          | 67/383 [00:13<01:07,  4.67it/s]Epoch: 8, train for the 102-th batch, train loss: 0.42615339159965515:  85%|████████▍ | 101/119 [00:15<00:03,  5.97it/s]Epoch: 3, train for the 185-th batch, train loss: 0.5455663800239563:  78%|████████▌  | 184/237 [00:37<00:12,  4.21it/s]Epoch: 8, train for the 102-th batch, train loss: 0.42615339159965515:  86%|████████▌ | 102/119 [00:15<00:02,  5.95it/s]Epoch: 3, train for the 185-th batch, train loss: 0.5455663800239563:  78%|████████▌  | 185/237 [00:37<00:12,  4.14it/s]Epoch: 4, train for the 161-th batch, train loss: 0.3223206400871277:  66%|███████▎   | 160/241 [00:24<00:14,  5.56it/s]Epoch: 4, train for the 161-th batch, train loss: 0.3223206400871277:  67%|███████▎   | 161/241 [00:24<00:13,  5.85it/s]Epoch: 6, train for the 8-th batch, train loss: 0.6939406991004944:   5%|▋              | 7/151 [00:01<00:20,  6.90it/s]Epoch: 6, train for the 8-th batch, train loss: 0.6939406991004944:   5%|▊              | 8/151 [00:01<00:20,  6.87it/s]Epoch: 7, train for the 95-th batch, train loss: 0.49696600437164307:  64%|███████▋    | 94/146 [00:13<00:08,  6.38it/s]Epoch: 2, train for the 68-th batch, train loss: 0.29357439279556274:  17%|██          | 67/383 [00:13<01:07,  4.67it/s]Epoch: 7, train for the 95-th batch, train loss: 0.49696600437164307:  65%|███████▊    | 95/146 [00:13<00:08,  6.15it/s]Epoch: 8, train for the 103-th batch, train loss: 0.4097924828529358:  86%|█████████▍ | 102/119 [00:16<00:02,  5.95it/s]Epoch: 8, train for the 103-th batch, train loss: 0.4097924828529358:  87%|█████████▌ | 103/119 [00:16<00:02,  5.87it/s]Epoch: 2, train for the 68-th batch, train loss: 0.29357439279556274:  18%|██▏         | 68/383 [00:13<01:05,  4.79it/s]Epoch: 3, train for the 186-th batch, train loss: 0.5521438717842102:  78%|████████▌  | 185/237 [00:37<00:12,  4.14it/s]Epoch: 4, train for the 162-th batch, train loss: 0.1838907152414322:  67%|███████▎   | 161/241 [00:25<00:13,  5.85it/s]Epoch: 6, train for the 9-th batch, train loss: 0.6739814877510071:   5%|▊              | 8/151 [00:01<00:20,  6.87it/s]Epoch: 4, train for the 162-th batch, train loss: 0.1838907152414322:  67%|███████▍   | 162/241 [00:25<00:13,  5.75it/s]Epoch: 3, train for the 186-th batch, train loss: 0.5521438717842102:  78%|████████▋  | 186/237 [00:37<00:12,  4.17it/s]Epoch: 6, train for the 9-th batch, train loss: 0.6739814877510071:   6%|▉              | 9/151 [00:01<00:21,  6.76it/s]Epoch: 7, train for the 96-th batch, train loss: 0.5158479809761047:  65%|████████▍    | 95/146 [00:13<00:08,  6.15it/s]Epoch: 7, train for the 96-th batch, train loss: 0.5158479809761047:  66%|████████▌    | 96/146 [00:13<00:07,  6.32it/s]Epoch: 8, train for the 104-th batch, train loss: 0.4180598855018616:  87%|█████████▌ | 103/119 [00:16<00:02,  5.87it/s]Epoch: 8, train for the 104-th batch, train loss: 0.4180598855018616:  87%|█████████▌ | 104/119 [00:16<00:02,  6.02it/s]Epoch: 2, train for the 69-th batch, train loss: 0.17292076349258423:  18%|██▏         | 68/383 [00:13<01:05,  4.79it/s]Epoch: 2, train for the 69-th batch, train loss: 0.17292076349258423:  18%|██▏         | 69/383 [00:13<01:03,  4.91it/s]Epoch: 4, train for the 163-th batch, train loss: 0.16500434279441833:  67%|██████▋   | 162/241 [00:25<00:13,  5.75it/s]Epoch: 4, train for the 163-th batch, train loss: 0.16500434279441833:  68%|██████▊   | 163/241 [00:25<00:13,  5.92it/s]Epoch: 6, train for the 10-th batch, train loss: 0.6626309156417847:   6%|▊             | 9/151 [00:01<00:21,  6.76it/s]Epoch: 6, train for the 10-th batch, train loss: 0.6626309156417847:   7%|▊            | 10/151 [00:01<00:21,  6.58it/s]Epoch: 7, train for the 97-th batch, train loss: 0.5141377449035645:  66%|████████▌    | 96/146 [00:13<00:07,  6.32it/s]Epoch: 7, train for the 97-th batch, train loss: 0.5141377449035645:  66%|████████▋    | 97/146 [00:13<00:07,  6.39it/s]Epoch: 3, train for the 187-th batch, train loss: 0.5612594485282898:  78%|████████▋  | 186/237 [00:37<00:12,  4.17it/s]Epoch: 8, train for the 105-th batch, train loss: 0.38331732153892517:  87%|████████▋ | 104/119 [00:16<00:02,  6.02it/s]Epoch: 8, train for the 105-th batch, train loss: 0.38331732153892517:  88%|████████▊ | 105/119 [00:16<00:02,  6.15it/s]Epoch: 3, train for the 187-th batch, train loss: 0.5612594485282898:  79%|████████▋  | 187/237 [00:37<00:12,  4.16it/s]Epoch: 2, train for the 70-th batch, train loss: 0.31878530979156494:  18%|██▏         | 69/383 [00:13<01:03,  4.91it/s]Epoch: 2, train for the 70-th batch, train loss: 0.31878530979156494:  18%|██▏         | 70/383 [00:13<01:00,  5.18it/s]Epoch: 4, train for the 164-th batch, train loss: 0.11510839313268661:  68%|██████▊   | 163/241 [00:25<00:13,  5.92it/s]Epoch: 6, train for the 11-th batch, train loss: 0.6594849228858948:   7%|▊            | 10/151 [00:01<00:21,  6.58it/s]Epoch: 7, train for the 98-th batch, train loss: 0.4843200147151947:  66%|████████▋    | 97/146 [00:14<00:07,  6.39it/s]Epoch: 6, train for the 11-th batch, train loss: 0.6594849228858948:   7%|▉            | 11/151 [00:01<00:21,  6.40it/s]Epoch: 7, train for the 98-th batch, train loss: 0.4843200147151947:  67%|████████▋    | 98/146 [00:14<00:07,  6.50it/s]Epoch: 8, train for the 106-th batch, train loss: 0.36378923058509827:  88%|████████▊ | 105/119 [00:16<00:02,  6.15it/s]Epoch: 4, train for the 164-th batch, train loss: 0.11510839313268661:  68%|██████▊   | 164/241 [00:25<00:13,  5.68it/s]Epoch: 8, train for the 106-th batch, train loss: 0.36378923058509827:  89%|████████▉ | 106/119 [00:16<00:02,  6.46it/s]Epoch: 3, train for the 188-th batch, train loss: 0.5607660412788391:  79%|████████▋  | 187/237 [00:37<00:12,  4.16it/s]Epoch: 3, train for the 188-th batch, train loss: 0.5607660412788391:  79%|████████▋  | 188/237 [00:38<00:11,  4.32it/s]Epoch: 6, train for the 12-th batch, train loss: 0.6712827682495117:   7%|▉            | 11/151 [00:01<00:21,  6.40it/s]Epoch: 6, train for the 12-th batch, train loss: 0.6712827682495117:   8%|█            | 12/151 [00:01<00:20,  6.70it/s]Epoch: 2, train for the 71-th batch, train loss: 0.4147993326187134:  18%|██▍          | 70/383 [00:13<01:00,  5.18it/s]Epoch: 7, train for the 99-th batch, train loss: 0.5159582495689392:  67%|████████▋    | 98/146 [00:14<00:07,  6.50it/s]Epoch: 7, train for the 99-th batch, train loss: 0.5159582495689392:  68%|████████▊    | 99/146 [00:14<00:07,  6.60it/s]Epoch: 4, train for the 165-th batch, train loss: 0.18788497149944305:  68%|██████▊   | 164/241 [00:25<00:13,  5.68it/s]Epoch: 2, train for the 71-th batch, train loss: 0.4147993326187134:  19%|██▍          | 71/383 [00:13<01:03,  4.89it/s]Epoch: 8, train for the 107-th batch, train loss: 0.4122554659843445:  89%|█████████▊ | 106/119 [00:16<00:02,  6.46it/s]Epoch: 8, train for the 107-th batch, train loss: 0.4122554659843445:  90%|█████████▉ | 107/119 [00:16<00:01,  6.27it/s]Epoch: 4, train for the 165-th batch, train loss: 0.18788497149944305:  68%|██████▊   | 165/241 [00:25<00:13,  5.61it/s]Epoch: 6, train for the 13-th batch, train loss: 0.6654443740844727:   8%|█            | 12/151 [00:01<00:20,  6.70it/s]Epoch: 6, train for the 13-th batch, train loss: 0.6654443740844727:   9%|█            | 13/151 [00:01<00:19,  7.01it/s]Epoch: 7, train for the 100-th batch, train loss: 0.5538133978843689:  68%|████████▏   | 99/146 [00:14<00:07,  6.60it/s]Epoch: 7, train for the 100-th batch, train loss: 0.5538133978843689:  68%|███████▌   | 100/146 [00:14<00:07,  6.49it/s]Epoch: 3, train for the 189-th batch, train loss: 0.5831827521324158:  79%|████████▋  | 188/237 [00:38<00:11,  4.32it/s]Epoch: 4, train for the 166-th batch, train loss: 0.20838357508182526:  68%|██████▊   | 165/241 [00:25<00:13,  5.61it/s]Epoch: 8, train for the 108-th batch, train loss: 0.33437445759773254:  90%|████████▉ | 107/119 [00:16<00:01,  6.27it/s]Epoch: 3, train for the 189-th batch, train loss: 0.5831827521324158:  80%|████████▊  | 189/237 [00:38<00:11,  4.35it/s]Epoch: 8, train for the 108-th batch, train loss: 0.33437445759773254:  91%|█████████ | 108/119 [00:16<00:01,  6.25it/s]Epoch: 4, train for the 166-th batch, train loss: 0.20838357508182526:  69%|██████▉   | 166/241 [00:25<00:12,  5.78it/s]Epoch: 6, train for the 14-th batch, train loss: 0.6799390912055969:   9%|█            | 13/151 [00:02<00:19,  7.01it/s]Epoch: 2, train for the 72-th batch, train loss: 0.35475507378578186:  19%|██▏         | 71/383 [00:14<01:03,  4.89it/s]Epoch: 6, train for the 14-th batch, train loss: 0.6799390912055969:   9%|█▏           | 14/151 [00:02<00:19,  7.11it/s]Epoch: 2, train for the 72-th batch, train loss: 0.35475507378578186:  19%|██▎         | 72/383 [00:14<01:07,  4.57it/s]Epoch: 7, train for the 101-th batch, train loss: 0.4712410569190979:  68%|███████▌   | 100/146 [00:14<00:07,  6.49it/s]Epoch: 7, train for the 101-th batch, train loss: 0.4712410569190979:  69%|███████▌   | 101/146 [00:14<00:06,  6.59it/s]Epoch: 8, train for the 109-th batch, train loss: 0.39767104387283325:  91%|█████████ | 108/119 [00:17<00:01,  6.25it/s]Epoch: 8, train for the 109-th batch, train loss: 0.39767104387283325:  92%|█████████▏| 109/119 [00:17<00:01,  6.50it/s]Epoch: 3, train for the 190-th batch, train loss: 0.5463184118270874:  80%|████████▊  | 189/237 [00:38<00:11,  4.35it/s]Epoch: 4, train for the 167-th batch, train loss: 0.13722509145736694:  69%|██████▉   | 166/241 [00:26<00:12,  5.78it/s]Epoch: 6, train for the 15-th batch, train loss: 0.6679968237876892:   9%|█▏           | 14/151 [00:02<00:19,  7.11it/s]Epoch: 3, train for the 190-th batch, train loss: 0.5463184118270874:  80%|████████▊  | 190/237 [00:38<00:10,  4.41it/s]Epoch: 6, train for the 15-th batch, train loss: 0.6679968237876892:  10%|█▎           | 15/151 [00:02<00:20,  6.68it/s]Epoch: 7, train for the 102-th batch, train loss: 0.5258428454399109:  69%|███████▌   | 101/146 [00:14<00:06,  6.59it/s]Epoch: 4, train for the 167-th batch, train loss: 0.13722509145736694:  69%|██████▉   | 167/241 [00:26<00:13,  5.35it/s]Epoch: 7, train for the 102-th batch, train loss: 0.5258428454399109:  70%|███████▋   | 102/146 [00:14<00:06,  6.97it/s]Epoch: 2, train for the 73-th batch, train loss: 0.2764102816581726:  19%|██▍          | 72/383 [00:14<01:07,  4.57it/s]Epoch: 8, train for the 110-th batch, train loss: 0.36030295491218567:  92%|█████████▏| 109/119 [00:17<00:01,  6.50it/s]Epoch: 2, train for the 73-th batch, train loss: 0.2764102816581726:  19%|██▍          | 73/383 [00:14<01:07,  4.61it/s]Epoch: 8, train for the 110-th batch, train loss: 0.36030295491218567:  92%|█████████▏| 110/119 [00:17<00:01,  6.58it/s]Epoch: 6, train for the 16-th batch, train loss: 0.6699767708778381:  10%|█▎           | 15/151 [00:02<00:20,  6.68it/s]Epoch: 6, train for the 16-th batch, train loss: 0.6699767708778381:  11%|█▍           | 16/151 [00:02<00:19,  7.05it/s]Epoch: 7, train for the 103-th batch, train loss: 0.5051153302192688:  70%|███████▋   | 102/146 [00:14<00:06,  6.97it/s]Epoch: 7, train for the 103-th batch, train loss: 0.5051153302192688:  71%|███████▊   | 103/146 [00:14<00:06,  7.07it/s]Epoch: 4, train for the 168-th batch, train loss: 0.10188860446214676:  69%|██████▉   | 167/241 [00:26<00:13,  5.35it/s]Epoch: 3, train for the 191-th batch, train loss: 0.5456146597862244:  80%|████████▊  | 190/237 [00:38<00:10,  4.41it/s]Epoch: 4, train for the 168-th batch, train loss: 0.10188860446214676:  70%|██████▉   | 168/241 [00:26<00:13,  5.35it/s]Epoch: 8, train for the 111-th batch, train loss: 0.42931899428367615:  92%|█████████▏| 110/119 [00:17<00:01,  6.58it/s]Epoch: 8, train for the 111-th batch, train loss: 0.42931899428367615:  93%|█████████▎| 111/119 [00:17<00:01,  6.76it/s]Epoch: 6, train for the 17-th batch, train loss: 0.6665722727775574:  11%|█▍           | 16/151 [00:02<00:19,  7.05it/s]Epoch: 3, train for the 191-th batch, train loss: 0.5456146597862244:  81%|████████▊  | 191/237 [00:38<00:10,  4.38it/s]Epoch: 6, train for the 17-th batch, train loss: 0.6665722727775574:  11%|█▍           | 17/151 [00:02<00:17,  7.47it/s]Epoch: 2, train for the 74-th batch, train loss: 0.2885690927505493:  19%|██▍          | 73/383 [00:14<01:07,  4.61it/s]Epoch: 2, train for the 74-th batch, train loss: 0.2885690927505493:  19%|██▌          | 74/383 [00:14<01:06,  4.67it/s]Epoch: 7, train for the 104-th batch, train loss: 0.49067601561546326:  71%|███████   | 103/146 [00:14<00:06,  7.07it/s]Epoch: 7, train for the 104-th batch, train loss: 0.49067601561546326:  71%|███████   | 104/146 [00:14<00:05,  7.06it/s]Epoch: 4, train for the 169-th batch, train loss: 0.5935171246528625:  70%|███████▋   | 168/241 [00:26<00:13,  5.35it/s]Epoch: 8, train for the 112-th batch, train loss: 0.4109450876712799:  93%|██████████▎| 111/119 [00:17<00:01,  6.76it/s]Epoch: 8, train for the 112-th batch, train loss: 0.4109450876712799:  94%|██████████▎| 112/119 [00:17<00:01,  6.74it/s]Epoch: 4, train for the 169-th batch, train loss: 0.5935171246528625:  70%|███████▋   | 169/241 [00:26<00:13,  5.49it/s]Epoch: 6, train for the 18-th batch, train loss: 0.6731358766555786:  11%|█▍           | 17/151 [00:02<00:17,  7.47it/s]Epoch: 6, train for the 18-th batch, train loss: 0.6731358766555786:  12%|█▌           | 18/151 [00:02<00:18,  7.28it/s]Epoch: 7, train for the 105-th batch, train loss: 0.5070515871047974:  71%|███████▊   | 104/146 [00:15<00:05,  7.06it/s]Epoch: 7, train for the 105-th batch, train loss: 0.5070515871047974:  72%|███████▉   | 105/146 [00:15<00:05,  7.12it/s]Epoch: 8, train for the 113-th batch, train loss: 0.36342597007751465:  94%|█████████▍| 112/119 [00:17<00:01,  6.74it/s]Epoch: 8, train for the 113-th batch, train loss: 0.36342597007751465:  95%|█████████▍| 113/119 [00:17<00:00,  7.03it/s]Epoch: 3, train for the 192-th batch, train loss: 0.5586614608764648:  81%|████████▊  | 191/237 [00:38<00:10,  4.38it/s]Epoch: 2, train for the 75-th batch, train loss: 0.24055129289627075:  19%|██▎         | 74/383 [00:14<01:06,  4.67it/s]Epoch: 2, train for the 75-th batch, train loss: 0.24055129289627075:  20%|██▎         | 75/383 [00:14<01:09,  4.43it/s]Epoch: 3, train for the 192-th batch, train loss: 0.5586614608764648:  81%|████████▉  | 192/237 [00:39<00:11,  3.96it/s]Epoch: 7, train for the 106-th batch, train loss: 0.506953775882721:  72%|████████▋   | 105/146 [00:15<00:05,  7.12it/s]Epoch: 4, train for the 170-th batch, train loss: 0.5615319609642029:  70%|███████▋   | 169/241 [00:26<00:13,  5.49it/s]Epoch: 6, train for the 19-th batch, train loss: 0.6709350943565369:  12%|█▌           | 18/151 [00:02<00:18,  7.28it/s]Epoch: 7, train for the 106-th batch, train loss: 0.506953775882721:  73%|████████▋   | 106/146 [00:15<00:05,  7.34it/s]Epoch: 6, train for the 19-th batch, train loss: 0.6709350943565369:  13%|█▋           | 19/151 [00:02<00:19,  6.74it/s]Epoch: 4, train for the 170-th batch, train loss: 0.5615319609642029:  71%|███████▊   | 170/241 [00:26<00:13,  5.22it/s]Epoch: 8, train for the 114-th batch, train loss: 0.36620596051216125:  95%|█████████▍| 113/119 [00:17<00:00,  7.03it/s]Epoch: 8, train for the 114-th batch, train loss: 0.36620596051216125:  96%|█████████▌| 114/119 [00:17<00:00,  7.11it/s]Epoch: 7, train for the 107-th batch, train loss: 0.5237136483192444:  73%|███████▉   | 106/146 [00:15<00:05,  7.34it/s]Epoch: 2, train for the 76-th batch, train loss: 0.32576972246170044:  20%|██▎         | 75/383 [00:14<01:09,  4.43it/s]Epoch: 7, train for the 107-th batch, train loss: 0.5237136483192444:  73%|████████   | 107/146 [00:15<00:05,  7.29it/s]Epoch: 6, train for the 20-th batch, train loss: 0.6528775095939636:  13%|█▋           | 19/151 [00:02<00:19,  6.74it/s]Epoch: 2, train for the 76-th batch, train loss: 0.32576972246170044:  20%|██▍         | 76/383 [00:15<01:05,  4.70it/s]Epoch: 6, train for the 20-th batch, train loss: 0.6528775095939636:  13%|█▋           | 20/151 [00:02<00:19,  6.67it/s]Epoch: 4, train for the 171-th batch, train loss: 0.5051349401473999:  71%|███████▊   | 170/241 [00:26<00:13,  5.22it/s]Epoch: 4, train for the 171-th batch, train loss: 0.5051349401473999:  71%|███████▊   | 171/241 [00:26<00:12,  5.39it/s]Epoch: 8, train for the 115-th batch, train loss: 0.34915173053741455:  96%|█████████▌| 114/119 [00:17<00:00,  7.11it/s]Epoch: 3, train for the 193-th batch, train loss: 0.5452796220779419:  81%|████████▉  | 192/237 [00:39<00:11,  3.96it/s]Epoch: 8, train for the 115-th batch, train loss: 0.34915173053741455:  97%|█████████▋| 115/119 [00:17<00:00,  6.96it/s]Epoch: 3, train for the 193-th batch, train loss: 0.5452796220779419:  81%|████████▉  | 193/237 [00:39<00:11,  3.86it/s]Epoch: 7, train for the 108-th batch, train loss: 0.4880780279636383:  73%|████████   | 107/146 [00:15<00:05,  7.29it/s]Epoch: 6, train for the 21-th batch, train loss: 0.6723290085792542:  13%|█▋           | 20/151 [00:03<00:19,  6.67it/s]Epoch: 6, train for the 21-th batch, train loss: 0.6723290085792542:  14%|█▊           | 21/151 [00:03<00:18,  7.09it/s]Epoch: 7, train for the 108-th batch, train loss: 0.4880780279636383:  74%|████████▏  | 108/146 [00:15<00:05,  7.28it/s]Epoch: 2, train for the 77-th batch, train loss: 0.3169298768043518:  20%|██▌          | 76/383 [00:15<01:05,  4.70it/s]Epoch: 4, train for the 172-th batch, train loss: 0.38511383533477783:  71%|███████   | 171/241 [00:26<00:12,  5.39it/s]Epoch: 2, train for the 77-th batch, train loss: 0.3169298768043518:  20%|██▌          | 77/383 [00:15<01:01,  5.00it/s]Epoch: 4, train for the 172-th batch, train loss: 0.38511383533477783:  71%|███████▏  | 172/241 [00:26<00:11,  5.81it/s]Epoch: 8, train for the 116-th batch, train loss: 0.359474241733551:  97%|███████████▌| 115/119 [00:18<00:00,  6.96it/s]Epoch: 8, train for the 116-th batch, train loss: 0.359474241733551:  97%|███████████▋| 116/119 [00:18<00:00,  6.78it/s]Epoch: 6, train for the 22-th batch, train loss: 0.641605794429779:  14%|█▉            | 21/151 [00:03<00:18,  7.09it/s]Epoch: 7, train for the 109-th batch, train loss: 0.49062472581863403:  74%|███████▍  | 108/146 [00:15<00:05,  7.28it/s]Epoch: 7, train for the 109-th batch, train loss: 0.49062472581863403:  75%|███████▍  | 109/146 [00:15<00:05,  7.12it/s]Epoch: 6, train for the 22-th batch, train loss: 0.641605794429779:  15%|██            | 22/151 [00:03<00:18,  6.96it/s]Epoch: 3, train for the 194-th batch, train loss: 0.569286048412323:  81%|█████████▊  | 193/237 [00:39<00:11,  3.86it/s]Epoch: 4, train for the 173-th batch, train loss: 0.3134695291519165:  71%|███████▊   | 172/241 [00:27<00:11,  5.81it/s]Epoch: 4, train for the 173-th batch, train loss: 0.3134695291519165:  72%|███████▉   | 173/241 [00:27<00:11,  6.09it/s]Epoch: 2, train for the 78-th batch, train loss: 0.3264358341693878:  20%|██▌          | 77/383 [00:15<01:01,  5.00it/s]Epoch: 3, train for the 194-th batch, train loss: 0.569286048412323:  82%|█████████▊  | 194/237 [00:39<00:10,  3.92it/s]Epoch: 8, train for the 117-th batch, train loss: 0.3422638177871704:  97%|██████████▋| 116/119 [00:18<00:00,  6.78it/s]Epoch: 2, train for the 78-th batch, train loss: 0.3264358341693878:  20%|██▋          | 78/383 [00:15<01:01,  4.94it/s]Epoch: 8, train for the 117-th batch, train loss: 0.3422638177871704:  98%|██████████▊| 117/119 [00:18<00:00,  6.59it/s]Epoch: 6, train for the 23-th batch, train loss: 0.6505613327026367:  15%|█▉           | 22/151 [00:03<00:18,  6.96it/s]Epoch: 6, train for the 23-th batch, train loss: 0.6505613327026367:  15%|█▉           | 23/151 [00:03<00:18,  7.07it/s]Epoch: 7, train for the 110-th batch, train loss: 0.47452619671821594:  75%|███████▍  | 109/146 [00:15<00:05,  7.12it/s]Epoch: 7, train for the 110-th batch, train loss: 0.47452619671821594:  75%|███████▌  | 110/146 [00:15<00:05,  6.86it/s]Epoch: 4, train for the 174-th batch, train loss: 0.3614462912082672:  72%|███████▉   | 173/241 [00:27<00:11,  6.09it/s]Epoch: 4, train for the 174-th batch, train loss: 0.3614462912082672:  72%|███████▉   | 174/241 [00:27<00:10,  6.40it/s]Epoch: 3, train for the 195-th batch, train loss: 0.521870493888855:  82%|█████████▊  | 194/237 [00:39<00:10,  3.92it/s]Epoch: 6, train for the 24-th batch, train loss: 0.6222493052482605:  15%|█▉           | 23/151 [00:03<00:18,  7.07it/s]Epoch: 6, train for the 24-th batch, train loss: 0.6222493052482605:  16%|██           | 24/151 [00:03<00:18,  6.99it/s]Epoch: 3, train for the 195-th batch, train loss: 0.521870493888855:  82%|█████████▊  | 195/237 [00:39<00:10,  4.04it/s]Epoch: 8, train for the 118-th batch, train loss: 0.3315058946609497:  98%|██████████▊| 117/119 [00:18<00:00,  6.59it/s]Epoch: 7, train for the 111-th batch, train loss: 0.4703126847743988:  75%|████████▎  | 110/146 [00:15<00:05,  6.86it/s]Epoch: 8, train for the 118-th batch, train loss: 0.3315058946609497:  99%|██████████▉| 118/119 [00:18<00:00,  5.83it/s]Epoch: 4, train for the 175-th batch, train loss: 0.2383517473936081:  72%|███████▉   | 174/241 [00:27<00:10,  6.40it/s]Epoch: 7, train for the 111-th batch, train loss: 0.4703126847743988:  76%|████████▎  | 111/146 [00:15<00:05,  6.33it/s]Epoch: 4, train for the 175-th batch, train loss: 0.2383517473936081:  73%|███████▉   | 175/241 [00:27<00:10,  6.29it/s]Epoch: 6, train for the 25-th batch, train loss: 0.6632465720176697:  16%|██           | 24/151 [00:03<00:18,  6.99it/s]Epoch: 6, train for the 25-th batch, train loss: 0.6632465720176697:  17%|██▏          | 25/151 [00:03<00:17,  7.32it/s]Epoch: 8, train for the 119-th batch, train loss: 0.32471388578414917:  99%|█████████▉| 118/119 [00:18<00:00,  5.83it/s]Epoch: 8, train for the 119-th batch, train loss: 0.32471388578414917: 100%|██████████| 119/119 [00:18<00:00,  6.47it/s]Epoch: 8, train for the 119-th batch, train loss: 0.32471388578414917: 100%|██████████| 119/119 [00:18<00:00,  6.42it/s]
Epoch: 2, train for the 79-th batch, train loss: 0.3970735967159271:  20%|██▋          | 78/383 [00:15<01:01,  4.94it/s]Epoch: 2, train for the 79-th batch, train loss: 0.3970735967159271:  21%|██▋          | 79/383 [00:15<01:16,  3.96it/s]Epoch: 7, train for the 112-th batch, train loss: 0.5332399010658264:  76%|████████▎  | 111/146 [00:16<00:05,  6.33it/s]Epoch: 3, train for the 196-th batch, train loss: 0.5264004468917847:  82%|█████████  | 195/237 [00:39<00:10,  4.04it/s]  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 7, train for the 112-th batch, train loss: 0.5332399010658264:  77%|████████▍  | 112/146 [00:16<00:05,  6.20it/s]Epoch: 3, train for the 196-th batch, train loss: 0.5264004468917847:  83%|█████████  | 196/237 [00:39<00:10,  4.08it/s]evaluate for the 1-th batch, evaluate loss: 0.45698705315589905:   0%|                           | 0/40 [00:00<?, ?it/s]Epoch: 4, train for the 176-th batch, train loss: 0.4053063988685608:  73%|███████▉   | 175/241 [00:27<00:10,  6.29it/s]Epoch: 6, train for the 26-th batch, train loss: 0.6475509405136108:  17%|██▏          | 25/151 [00:03<00:17,  7.32it/s]Epoch: 6, train for the 26-th batch, train loss: 0.6475509405136108:  17%|██▏          | 26/151 [00:03<00:18,  6.75it/s]Epoch: 4, train for the 176-th batch, train loss: 0.4053063988685608:  73%|████████   | 176/241 [00:27<00:11,  5.57it/s]Epoch: 2, train for the 80-th batch, train loss: 0.2651698589324951:  21%|██▋          | 79/383 [00:15<01:16,  3.96it/s]Epoch: 7, train for the 113-th batch, train loss: 0.4572238028049469:  77%|████████▍  | 112/146 [00:16<00:05,  6.20it/s]evaluate for the 2-th batch, evaluate loss: 0.45775407552719116:   0%|                           | 0/40 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.45775407552719116:   5%|▉                  | 2/40 [00:00<00:02, 13.33it/s]Epoch: 7, train for the 113-th batch, train loss: 0.4572238028049469:  77%|████████▌  | 113/146 [00:16<00:05,  6.31it/s]Epoch: 2, train for the 80-th batch, train loss: 0.2651698589324951:  21%|██▋          | 80/383 [00:15<01:12,  4.20it/s]evaluate for the 3-th batch, evaluate loss: 0.4633525013923645:   5%|█                   | 2/40 [00:00<00:02, 13.33it/s]Epoch: 6, train for the 27-th batch, train loss: 0.6689213514328003:  17%|██▏          | 26/151 [00:03<00:18,  6.75it/s]Epoch: 6, train for the 27-th batch, train loss: 0.6689213514328003:  18%|██▎          | 27/151 [00:03<00:18,  6.76it/s]Epoch: 3, train for the 197-th batch, train loss: 0.5570851564407349:  83%|█████████  | 196/237 [00:40<00:10,  4.08it/s]Epoch: 3, train for the 197-th batch, train loss: 0.5570851564407349:  83%|█████████▏ | 197/237 [00:40<00:09,  4.14it/s]evaluate for the 4-th batch, evaluate loss: 0.5123183131217957:   5%|█                   | 2/40 [00:00<00:02, 13.33it/s]evaluate for the 4-th batch, evaluate loss: 0.5123183131217957:  10%|██                  | 4/40 [00:00<00:02, 14.05it/s]Epoch: 7, train for the 114-th batch, train loss: 0.485869437456131:  77%|█████████▎  | 113/146 [00:16<00:05,  6.31it/s]Epoch: 6, train for the 28-th batch, train loss: 0.6508975028991699:  18%|██▎          | 27/151 [00:04<00:18,  6.76it/s]Epoch: 7, train for the 114-th batch, train loss: 0.485869437456131:  78%|█████████▎  | 114/146 [00:16<00:05,  6.28it/s]Epoch: 6, train for the 28-th batch, train loss: 0.6508975028991699:  19%|██▍          | 28/151 [00:04<00:16,  7.24it/s]Epoch: 2, train for the 81-th batch, train loss: 0.24166736006736755:  21%|██▌         | 80/383 [00:16<01:12,  4.20it/s]evaluate for the 5-th batch, evaluate loss: 0.495491623878479:  10%|██                   | 4/40 [00:00<00:02, 14.05it/s]Epoch: 2, train for the 81-th batch, train loss: 0.24166736006736755:  21%|██▌         | 81/383 [00:16<01:08,  4.42it/s]Epoch: 4, train for the 177-th batch, train loss: 0.3117905855178833:  73%|████████   | 176/241 [00:27<00:11,  5.57it/s]evaluate for the 6-th batch, evaluate loss: 0.4585302174091339:  10%|██                  | 4/40 [00:00<00:02, 14.05it/s]evaluate for the 6-th batch, evaluate loss: 0.4585302174091339:  15%|███                 | 6/40 [00:00<00:02, 14.87it/s]Epoch: 4, train for the 177-th batch, train loss: 0.3117905855178833:  73%|████████   | 177/241 [00:27<00:14,  4.36it/s]Epoch: 7, train for the 115-th batch, train loss: 0.5112367868423462:  78%|████████▌  | 114/146 [00:16<00:05,  6.28it/s]Epoch: 7, train for the 115-th batch, train loss: 0.5112367868423462:  79%|████████▋  | 115/146 [00:16<00:04,  6.42it/s]Epoch: 6, train for the 29-th batch, train loss: 0.6588407754898071:  19%|██▍          | 28/151 [00:04<00:16,  7.24it/s]Epoch: 3, train for the 198-th batch, train loss: 0.5097450017929077:  83%|█████████▏ | 197/237 [00:40<00:09,  4.14it/s]evaluate for the 7-th batch, evaluate loss: 0.4661032259464264:  15%|███                 | 6/40 [00:00<00:02, 14.87it/s]Epoch: 6, train for the 29-th batch, train loss: 0.6588407754898071:  19%|██▍          | 29/151 [00:04<00:17,  7.12it/s]Epoch: 3, train for the 198-th batch, train loss: 0.5097450017929077:  84%|█████████▏ | 198/237 [00:40<00:09,  4.07it/s]evaluate for the 8-th batch, evaluate loss: 0.419906347990036:  15%|███▏                 | 6/40 [00:00<00:02, 14.87it/s]evaluate for the 8-th batch, evaluate loss: 0.419906347990036:  20%|████▏                | 8/40 [00:00<00:02, 14.82it/s]Epoch: 2, train for the 82-th batch, train loss: 0.27392321825027466:  21%|██▌         | 81/383 [00:16<01:08,  4.42it/s]Epoch: 2, train for the 82-th batch, train loss: 0.27392321825027466:  21%|██▌         | 82/383 [00:16<01:07,  4.44it/s]Epoch: 4, train for the 178-th batch, train loss: 0.4188377261161804:  73%|████████   | 177/241 [00:28<00:14,  4.36it/s]evaluate for the 9-th batch, evaluate loss: 0.4603402614593506:  20%|████                | 8/40 [00:00<00:02, 14.82it/s]Epoch: 4, train for the 178-th batch, train loss: 0.4188377261161804:  74%|████████   | 178/241 [00:28<00:13,  4.55it/s]Epoch: 6, train for the 30-th batch, train loss: 0.6566446423530579:  19%|██▍          | 29/151 [00:04<00:17,  7.12it/s]Epoch: 6, train for the 30-th batch, train loss: 0.6566446423530579:  20%|██▌          | 30/151 [00:04<00:17,  6.78it/s]Epoch: 3, train for the 199-th batch, train loss: 0.5442413687705994:  84%|█████████▏ | 198/237 [00:40<00:09,  4.07it/s]evaluate for the 10-th batch, evaluate loss: 0.4962221384048462:  20%|███▊               | 8/40 [00:00<00:02, 14.82it/s]evaluate for the 10-th batch, evaluate loss: 0.4962221384048462:  25%|████▌             | 10/40 [00:00<00:02, 12.70it/s]Epoch: 7, train for the 116-th batch, train loss: 0.5022791028022766:  79%|████████▋  | 115/146 [00:16<00:04,  6.42it/s]Epoch: 7, train for the 116-th batch, train loss: 0.5022791028022766:  79%|████████▋  | 116/146 [00:16<00:05,  5.07it/s]Epoch: 3, train for the 199-th batch, train loss: 0.5442413687705994:  84%|█████████▏ | 199/237 [00:40<00:09,  4.13it/s]Epoch: 2, train for the 83-th batch, train loss: 0.3055538237094879:  21%|██▊          | 82/383 [00:16<01:07,  4.44it/s]Epoch: 6, train for the 31-th batch, train loss: 0.6528772115707397:  20%|██▌          | 30/151 [00:04<00:17,  6.78it/s]Epoch: 2, train for the 83-th batch, train loss: 0.3055538237094879:  22%|██▊          | 83/383 [00:16<01:05,  4.55it/s]Epoch: 4, train for the 179-th batch, train loss: 0.3712705075740814:  74%|████████   | 178/241 [00:28<00:13,  4.55it/s]Epoch: 6, train for the 31-th batch, train loss: 0.6528772115707397:  21%|██▋          | 31/151 [00:04<00:18,  6.56it/s]evaluate for the 11-th batch, evaluate loss: 0.4214707016944885:  25%|████▌             | 10/40 [00:00<00:02, 12.70it/s]Epoch: 4, train for the 179-th batch, train loss: 0.3712705075740814:  74%|████████▏  | 179/241 [00:28<00:13,  4.61it/s]Epoch: 7, train for the 117-th batch, train loss: 0.535753607749939:  79%|█████████▌  | 116/146 [00:17<00:05,  5.07it/s]evaluate for the 12-th batch, evaluate loss: 0.44685378670692444:  25%|████▎            | 10/40 [00:00<00:02, 12.70it/s]evaluate for the 12-th batch, evaluate loss: 0.44685378670692444:  30%|█████            | 12/40 [00:00<00:02, 12.29it/s]Epoch: 7, train for the 117-th batch, train loss: 0.535753607749939:  80%|█████████▌  | 117/146 [00:17<00:05,  5.33it/s]Epoch: 6, train for the 32-th batch, train loss: 0.669201672077179:  21%|██▊           | 31/151 [00:04<00:18,  6.56it/s]evaluate for the 13-th batch, evaluate loss: 0.46652740240097046:  30%|█████            | 12/40 [00:00<00:02, 12.29it/s]Epoch: 6, train for the 32-th batch, train loss: 0.669201672077179:  21%|██▉           | 32/151 [00:04<00:19,  6.17it/s]Epoch: 4, train for the 180-th batch, train loss: 0.4958946406841278:  74%|████████▏  | 179/241 [00:28<00:13,  4.61it/s]Epoch: 4, train for the 180-th batch, train loss: 0.4958946406841278:  75%|████████▏  | 180/241 [00:28<00:12,  4.82it/s]evaluate for the 14-th batch, evaluate loss: 0.4485909938812256:  30%|█████▍            | 12/40 [00:01<00:02, 12.29it/s]evaluate for the 14-th batch, evaluate loss: 0.4485909938812256:  35%|██████▎           | 14/40 [00:01<00:02, 12.75it/s]Epoch: 7, train for the 118-th batch, train loss: 0.49064067006111145:  80%|████████  | 117/146 [00:17<00:05,  5.33it/s]Epoch: 7, train for the 118-th batch, train loss: 0.49064067006111145:  81%|████████  | 118/146 [00:17<00:04,  5.62it/s]Epoch: 2, train for the 84-th batch, train loss: 0.38385358452796936:  22%|██▌         | 83/383 [00:16<01:05,  4.55it/s]Epoch: 3, train for the 200-th batch, train loss: 0.5743293762207031:  84%|█████████▏ | 199/237 [00:41<00:09,  4.13it/s]evaluate for the 15-th batch, evaluate loss: 0.44860443472862244:  35%|█████▉           | 14/40 [00:01<00:02, 12.75it/s]Epoch: 2, train for the 84-th batch, train loss: 0.38385358452796936:  22%|██▋         | 84/383 [00:16<01:14,  4.01it/s]Epoch: 3, train for the 200-th batch, train loss: 0.5743293762207031:  84%|█████████▎ | 200/237 [00:41<00:10,  3.55it/s]Epoch: 6, train for the 33-th batch, train loss: 0.6486334204673767:  21%|██▊          | 32/151 [00:04<00:19,  6.17it/s]Epoch: 6, train for the 33-th batch, train loss: 0.6486334204673767:  22%|██▊          | 33/151 [00:04<00:19,  6.13it/s]evaluate for the 16-th batch, evaluate loss: 0.4590051472187042:  35%|██████▎           | 14/40 [00:01<00:02, 12.75it/s]evaluate for the 16-th batch, evaluate loss: 0.4590051472187042:  40%|███████▏          | 16/40 [00:01<00:01, 12.99it/s]Epoch: 7, train for the 119-th batch, train loss: 0.475972056388855:  81%|█████████▋  | 118/146 [00:17<00:04,  5.62it/s]Epoch: 7, train for the 119-th batch, train loss: 0.475972056388855:  82%|█████████▊  | 119/146 [00:17<00:04,  5.83it/s]evaluate for the 17-th batch, evaluate loss: 0.45946624875068665:  40%|██████▊          | 16/40 [00:01<00:01, 12.99it/s]Epoch: 2, train for the 85-th batch, train loss: 0.3634548783302307:  22%|██▊          | 84/383 [00:17<01:14,  4.01it/s]Epoch: 6, train for the 34-th batch, train loss: 0.6318844556808472:  22%|██▊          | 33/151 [00:05<00:19,  6.13it/s]Epoch: 6, train for the 34-th batch, train loss: 0.6318844556808472:  23%|██▉          | 34/151 [00:05<00:17,  6.52it/s]Epoch: 2, train for the 85-th batch, train loss: 0.3634548783302307:  22%|██▉          | 85/383 [00:17<01:07,  4.43it/s]Epoch: 4, train for the 181-th batch, train loss: 0.7325825095176697:  75%|████████▏  | 180/241 [00:28<00:12,  4.82it/s]Epoch: 4, train for the 181-th batch, train loss: 0.7325825095176697:  75%|████████▎  | 181/241 [00:28<00:14,  4.16it/s]evaluate for the 18-th batch, evaluate loss: 0.4212300479412079:  40%|███████▏          | 16/40 [00:01<00:01, 12.99it/s]evaluate for the 18-th batch, evaluate loss: 0.4212300479412079:  45%|████████          | 18/40 [00:01<00:01, 13.56it/s]Epoch: 7, train for the 120-th batch, train loss: 0.47009775042533875:  82%|████████▏ | 119/146 [00:17<00:04,  5.83it/s]Epoch: 7, train for the 120-th batch, train loss: 0.47009775042533875:  82%|████████▏ | 120/146 [00:17<00:04,  6.01it/s]evaluate for the 19-th batch, evaluate loss: 0.4699534475803375:  45%|████████          | 18/40 [00:01<00:01, 13.56it/s]Epoch: 6, train for the 35-th batch, train loss: 0.6407226324081421:  23%|██▉          | 34/151 [00:05<00:17,  6.52it/s]Epoch: 6, train for the 35-th batch, train loss: 0.6407226324081421:  23%|███          | 35/151 [00:05<00:17,  6.73it/s]evaluate for the 20-th batch, evaluate loss: 0.45612093806266785:  45%|███████▋         | 18/40 [00:01<00:01, 13.56it/s]evaluate for the 20-th batch, evaluate loss: 0.45612093806266785:  50%|████████▌        | 20/40 [00:01<00:01, 14.27it/s]Epoch: 4, train for the 182-th batch, train loss: 0.6413509845733643:  75%|████████▎  | 181/241 [00:29<00:14,  4.16it/s]Epoch: 2, train for the 86-th batch, train loss: 0.27901461720466614:  22%|██▋         | 85/383 [00:17<01:07,  4.43it/s]Epoch: 4, train for the 182-th batch, train loss: 0.6413509845733643:  76%|████████▎  | 182/241 [00:29<00:12,  4.65it/s]Epoch: 2, train for the 86-th batch, train loss: 0.27901461720466614:  22%|██▋         | 86/383 [00:17<01:08,  4.34it/s]Epoch: 3, train for the 201-th batch, train loss: 0.5363243222236633:  84%|█████████▎ | 200/237 [00:41<00:10,  3.55it/s]Epoch: 7, train for the 121-th batch, train loss: 0.47410067915916443:  82%|████████▏ | 120/146 [00:17<00:04,  6.01it/s]Epoch: 6, train for the 36-th batch, train loss: 0.6701517105102539:  23%|███          | 35/151 [00:05<00:17,  6.73it/s]evaluate for the 21-th batch, evaluate loss: 0.45002496242523193:  50%|████████▌        | 20/40 [00:01<00:01, 14.27it/s]Epoch: 6, train for the 36-th batch, train loss: 0.6701517105102539:  24%|███          | 36/151 [00:05<00:16,  6.85it/s]Epoch: 7, train for the 121-th batch, train loss: 0.47410067915916443:  83%|████████▎ | 121/146 [00:17<00:04,  5.89it/s]Epoch: 3, train for the 201-th batch, train loss: 0.5363243222236633:  85%|█████████▎ | 201/237 [00:41<00:11,  3.02it/s]evaluate for the 22-th batch, evaluate loss: 0.4103870987892151:  50%|█████████         | 20/40 [00:01<00:01, 14.27it/s]evaluate for the 22-th batch, evaluate loss: 0.4103870987892151:  55%|█████████▉        | 22/40 [00:01<00:01, 13.50it/s]Epoch: 4, train for the 183-th batch, train loss: 0.40365225076675415:  76%|███████▌  | 182/241 [00:29<00:12,  4.65it/s]Epoch: 4, train for the 183-th batch, train loss: 0.40365225076675415:  76%|███████▌  | 183/241 [00:29<00:11,  4.87it/s]Epoch: 6, train for the 37-th batch, train loss: 0.6426716446876526:  24%|███          | 36/151 [00:05<00:16,  6.85it/s]Epoch: 6, train for the 37-th batch, train loss: 0.6426716446876526:  25%|███▏         | 37/151 [00:05<00:17,  6.68it/s]Epoch: 7, train for the 122-th batch, train loss: 0.5204084515571594:  83%|█████████  | 121/146 [00:17<00:04,  5.89it/s]Epoch: 2, train for the 87-th batch, train loss: 0.31347304582595825:  22%|██▋         | 86/383 [00:17<01:08,  4.34it/s]evaluate for the 23-th batch, evaluate loss: 0.3936334252357483:  55%|█████████▉        | 22/40 [00:01<00:01, 13.50it/s]Epoch: 7, train for the 122-th batch, train loss: 0.5204084515571594:  84%|█████████▏ | 122/146 [00:17<00:04,  5.91it/s]Epoch: 2, train for the 87-th batch, train loss: 0.31347304582595825:  23%|██▋         | 87/383 [00:17<01:08,  4.35it/s]Epoch: 3, train for the 202-th batch, train loss: 0.5558474063873291:  85%|█████████▎ | 201/237 [00:41<00:11,  3.02it/s]Epoch: 4, train for the 184-th batch, train loss: 0.6174612045288086:  76%|████████▎  | 183/241 [00:29<00:11,  4.87it/s]Epoch: 4, train for the 184-th batch, train loss: 0.6174612045288086:  76%|████████▍  | 184/241 [00:29<00:10,  5.39it/s]Epoch: 3, train for the 202-th batch, train loss: 0.5558474063873291:  85%|█████████▍ | 202/237 [00:41<00:10,  3.28it/s]evaluate for the 24-th batch, evaluate loss: 0.4265894293785095:  55%|█████████▉        | 22/40 [00:01<00:01, 13.50it/s]evaluate for the 24-th batch, evaluate loss: 0.4265894293785095:  60%|██████████▊       | 24/40 [00:01<00:01, 12.08it/s]Epoch: 6, train for the 38-th batch, train loss: 0.6403860449790955:  25%|███▏         | 37/151 [00:05<00:17,  6.68it/s]Epoch: 6, train for the 38-th batch, train loss: 0.6403860449790955:  25%|███▎         | 38/151 [00:05<00:16,  6.84it/s]Epoch: 7, train for the 123-th batch, train loss: 0.5416221022605896:  84%|█████████▏ | 122/146 [00:17<00:04,  5.91it/s]Epoch: 7, train for the 123-th batch, train loss: 0.5416221022605896:  84%|█████████▎ | 123/146 [00:17<00:03,  6.05it/s]evaluate for the 25-th batch, evaluate loss: 0.4504454433917999:  60%|██████████▊       | 24/40 [00:01<00:01, 12.08it/s]Epoch: 2, train for the 88-th batch, train loss: 0.35290053486824036:  23%|██▋         | 87/383 [00:17<01:08,  4.35it/s]Epoch: 4, train for the 185-th batch, train loss: 0.6269834637641907:  76%|████████▍  | 184/241 [00:29<00:10,  5.39it/s]evaluate for the 26-th batch, evaluate loss: 0.4005613625049591:  60%|██████████▊       | 24/40 [00:01<00:01, 12.08it/s]evaluate for the 26-th batch, evaluate loss: 0.4005613625049591:  65%|███████████▋      | 26/40 [00:01<00:01, 12.94it/s]Epoch: 4, train for the 185-th batch, train loss: 0.6269834637641907:  77%|████████▍  | 185/241 [00:29<00:09,  5.65it/s]Epoch: 2, train for the 88-th batch, train loss: 0.35290053486824036:  23%|██▊         | 88/383 [00:17<01:06,  4.41it/s]Epoch: 6, train for the 39-th batch, train loss: 0.631422221660614:  25%|███▌          | 38/151 [00:05<00:16,  6.84it/s]evaluate for the 27-th batch, evaluate loss: 0.420021116733551:  65%|████████████▎      | 26/40 [00:02<00:01, 12.94it/s]Epoch: 6, train for the 39-th batch, train loss: 0.631422221660614:  26%|███▌          | 39/151 [00:05<00:16,  6.75it/s]Epoch: 3, train for the 203-th batch, train loss: 0.5735539793968201:  85%|█████████▍ | 202/237 [00:41<00:10,  3.28it/s]Epoch: 3, train for the 203-th batch, train loss: 0.5735539793968201:  86%|█████████▍ | 203/237 [00:42<00:09,  3.46it/s]evaluate for the 28-th batch, evaluate loss: 0.3920865058898926:  65%|███████████▋      | 26/40 [00:02<00:01, 12.94it/s]evaluate for the 28-th batch, evaluate loss: 0.3920865058898926:  70%|████████████▌     | 28/40 [00:02<00:00, 13.99it/s]evaluate for the 29-th batch, evaluate loss: 0.4161509871482849:  70%|████████████▌     | 28/40 [00:02<00:00, 13.99it/s]Epoch: 4, train for the 186-th batch, train loss: 0.541521430015564:  77%|█████████▏  | 185/241 [00:29<00:09,  5.65it/s]Epoch: 2, train for the 89-th batch, train loss: 0.32041388750076294:  23%|██▊         | 88/383 [00:17<01:06,  4.41it/s]Epoch: 2, train for the 89-th batch, train loss: 0.32041388750076294:  23%|██▊         | 89/383 [00:17<01:03,  4.66it/s]Epoch: 6, train for the 40-th batch, train loss: 0.6280638575553894:  26%|███▎         | 39/151 [00:05<00:16,  6.75it/s]evaluate for the 30-th batch, evaluate loss: 0.40076056122779846:  70%|███████████▉     | 28/40 [00:02<00:00, 13.99it/s]Epoch: 4, train for the 186-th batch, train loss: 0.541521430015564:  77%|█████████▎  | 186/241 [00:29<00:10,  5.47it/s]Epoch: 6, train for the 40-th batch, train loss: 0.6280638575553894:  26%|███▍         | 40/151 [00:05<00:16,  6.56it/s]evaluate for the 31-th batch, evaluate loss: 0.42488741874694824:  70%|███████████▉     | 28/40 [00:02<00:00, 13.99it/s]evaluate for the 31-th batch, evaluate loss: 0.42488741874694824:  78%|█████████████▏   | 31/40 [00:02<00:00, 16.10it/s]Epoch: 3, train for the 204-th batch, train loss: 0.5131364464759827:  86%|█████████▍ | 203/237 [00:42<00:09,  3.46it/s]Epoch: 7, train for the 124-th batch, train loss: 0.5100265145301819:  84%|█████████▎ | 123/146 [00:18<00:03,  6.05it/s]evaluate for the 32-th batch, evaluate loss: 0.4203850328922272:  78%|█████████████▉    | 31/40 [00:02<00:00, 16.10it/s]Epoch: 7, train for the 124-th batch, train loss: 0.5100265145301819:  85%|█████████▎ | 124/146 [00:18<00:05,  4.20it/s]Epoch: 2, train for the 90-th batch, train loss: 0.33345335721969604:  23%|██▊         | 89/383 [00:18<01:03,  4.66it/s]Epoch: 4, train for the 187-th batch, train loss: 0.5895023345947266:  77%|████████▍  | 186/241 [00:29<00:10,  5.47it/s]Epoch: 3, train for the 204-th batch, train loss: 0.5131364464759827:  86%|█████████▍ | 204/237 [00:42<00:09,  3.62it/s]Epoch: 4, train for the 187-th batch, train loss: 0.5895023345947266:  78%|████████▌  | 187/241 [00:29<00:09,  5.57it/s]Epoch: 2, train for the 90-th batch, train loss: 0.33345335721969604:  23%|██▊         | 90/383 [00:18<01:00,  4.83it/s]Epoch: 6, train for the 41-th batch, train loss: 0.6218467354774475:  26%|███▍         | 40/151 [00:06<00:16,  6.56it/s]evaluate for the 33-th batch, evaluate loss: 0.42803362011909485:  78%|█████████████▏   | 31/40 [00:02<00:00, 16.10it/s]evaluate for the 33-th batch, evaluate loss: 0.42803362011909485:  82%|██████████████   | 33/40 [00:02<00:00, 15.80it/s]Epoch: 6, train for the 41-th batch, train loss: 0.6218467354774475:  27%|███▌         | 41/151 [00:06<00:18,  6.06it/s]Epoch: 7, train for the 125-th batch, train loss: 0.4821394383907318:  85%|█████████▎ | 124/146 [00:18<00:05,  4.20it/s]Epoch: 7, train for the 125-th batch, train loss: 0.4821394383907318:  86%|█████████▍ | 125/146 [00:18<00:04,  4.90it/s]evaluate for the 34-th batch, evaluate loss: 0.41034796833992004:  82%|██████████████   | 33/40 [00:02<00:00, 15.80it/s]Epoch: 4, train for the 188-th batch, train loss: 0.4277077913284302:  78%|████████▌  | 187/241 [00:30<00:09,  5.57it/s]Epoch: 4, train for the 188-th batch, train loss: 0.4277077913284302:  78%|████████▌  | 188/241 [00:30<00:08,  5.97it/s]evaluate for the 35-th batch, evaluate loss: 0.4473367929458618:  82%|██████████████▊   | 33/40 [00:02<00:00, 15.80it/s]evaluate for the 35-th batch, evaluate loss: 0.4473367929458618:  88%|███████████████▊  | 35/40 [00:02<00:00, 15.21it/s]Epoch: 6, train for the 42-th batch, train loss: 0.5920901298522949:  27%|███▌         | 41/151 [00:06<00:18,  6.06it/s]Epoch: 6, train for the 42-th batch, train loss: 0.5920901298522949:  28%|███▌         | 42/151 [00:06<00:18,  6.04it/s]Epoch: 7, train for the 126-th batch, train loss: 0.48230624198913574:  86%|████████▌ | 125/146 [00:18<00:04,  4.90it/s]Epoch: 7, train for the 126-th batch, train loss: 0.48230624198913574:  86%|████████▋ | 126/146 [00:18<00:03,  5.40it/s]evaluate for the 36-th batch, evaluate loss: 0.4140140414237976:  88%|███████████████▊  | 35/40 [00:02<00:00, 15.21it/s]Epoch: 3, train for the 205-th batch, train loss: 0.5306428074836731:  86%|█████████▍ | 204/237 [00:42<00:09,  3.62it/s]Epoch: 4, train for the 189-th batch, train loss: 0.38379180431365967:  78%|███████▊  | 188/241 [00:30<00:08,  5.97it/s]Epoch: 2, train for the 91-th batch, train loss: 0.3080677390098572:  23%|███          | 90/383 [00:18<01:00,  4.83it/s]evaluate for the 37-th batch, evaluate loss: 0.45067036151885986:  88%|██████████████▉  | 35/40 [00:02<00:00, 15.21it/s]evaluate for the 37-th batch, evaluate loss: 0.45067036151885986:  92%|███████████████▋ | 37/40 [00:02<00:00, 15.39it/s]Epoch: 4, train for the 189-th batch, train loss: 0.38379180431365967:  78%|███████▊  | 189/241 [00:30<00:08,  6.18it/s]Epoch: 3, train for the 205-th batch, train loss: 0.5306428074836731:  86%|█████████▌ | 205/237 [00:42<00:09,  3.44it/s]Epoch: 2, train for the 91-th batch, train loss: 0.3080677390098572:  24%|███          | 91/383 [00:18<01:08,  4.29it/s]Epoch: 6, train for the 43-th batch, train loss: 0.6321623921394348:  28%|███▌         | 42/151 [00:06<00:18,  6.04it/s]Epoch: 7, train for the 127-th batch, train loss: 0.48452621698379517:  86%|████████▋ | 126/146 [00:18<00:03,  5.40it/s]evaluate for the 38-th batch, evaluate loss: 0.4414210319519043:  92%|████████████████▋ | 37/40 [00:02<00:00, 15.39it/s]Epoch: 7, train for the 127-th batch, train loss: 0.48452621698379517:  87%|████████▋ | 127/146 [00:18<00:03,  5.90it/s]Epoch: 6, train for the 43-th batch, train loss: 0.6321623921394348:  28%|███▋         | 43/151 [00:06<00:17,  6.09it/s]evaluate for the 39-th batch, evaluate loss: 0.47233846783638:  92%|██████████████████▌ | 37/40 [00:02<00:00, 15.39it/s]evaluate for the 39-th batch, evaluate loss: 0.47233846783638:  98%|███████████████████▌| 39/40 [00:02<00:00, 14.16it/s]Epoch: 4, train for the 190-th batch, train loss: 0.3536582589149475:  78%|████████▋  | 189/241 [00:30<00:08,  6.18it/s]Epoch: 4, train for the 190-th batch, train loss: 0.3536582589149475:  79%|████████▋  | 190/241 [00:30<00:08,  5.82it/s]evaluate for the 40-th batch, evaluate loss: 0.3472422659397125:  98%|█████████████████▌| 39/40 [00:02<00:00, 14.16it/s]evaluate for the 40-th batch, evaluate loss: 0.3472422659397125: 100%|██████████████████| 40/40 [00:02<00:00, 14.05it/s]
Epoch: 6, train for the 44-th batch, train loss: 0.5646955966949463:  28%|███▋         | 43/151 [00:06<00:17,  6.09it/s]Epoch: 6, train for the 44-th batch, train loss: 0.5646955966949463:  29%|███▊         | 44/151 [00:06<00:17,  6.09it/s]Epoch: 7, train for the 128-th batch, train loss: 0.4566361904144287:  87%|█████████▌ | 127/146 [00:18<00:03,  5.90it/s]Epoch: 7, train for the 128-th batch, train loss: 0.4566361904144287:  88%|█████████▋ | 128/146 [00:18<00:03,  5.78it/s]Epoch: 3, train for the 206-th batch, train loss: 0.5224593281745911:  86%|█████████▌ | 205/237 [00:42<00:09,  3.44it/s]Epoch: 2, train for the 92-th batch, train loss: 0.3462067246437073:  24%|███          | 91/383 [00:18<01:08,  4.29it/s]Epoch: 4, train for the 191-th batch, train loss: 0.33096688985824585:  79%|███████▉  | 190/241 [00:30<00:08,  5.82it/s]Epoch: 2, train for the 92-th batch, train loss: 0.3462067246437073:  24%|███          | 92/383 [00:18<01:14,  3.92it/s]Epoch: 3, train for the 206-th batch, train loss: 0.5224593281745911:  87%|█████████▌ | 206/237 [00:42<00:09,  3.38it/s]Epoch: 4, train for the 191-th batch, train loss: 0.33096688985824585:  79%|███████▉  | 191/241 [00:30<00:08,  6.21it/s]Epoch: 7, train for the 129-th batch, train loss: 0.4960654377937317:  88%|█████████▋ | 128/146 [00:19<00:03,  5.78it/s]Epoch: 7, train for the 129-th batch, train loss: 0.4960654377937317:  88%|█████████▋ | 129/146 [00:19<00:02,  6.46it/s]Epoch: 4, train for the 192-th batch, train loss: 0.2621449828147888:  79%|████████▋  | 191/241 [00:30<00:08,  6.21it/s]Epoch: 4, train for the 192-th batch, train loss: 0.2621449828147888:  80%|████████▊  | 192/241 [00:30<00:07,  6.66it/s]  0%|                                                                                            | 0/21 [00:00<?, ?it/s]Epoch: 7, train for the 130-th batch, train loss: 0.4967353045940399:  88%|█████████▋ | 129/146 [00:19<00:02,  6.46it/s]Epoch: 2, train for the 93-th batch, train loss: 0.3862769603729248:  24%|███          | 92/383 [00:18<01:14,  3.92it/s]Epoch: 7, train for the 130-th batch, train loss: 0.4967353045940399:  89%|█████████▊ | 130/146 [00:19<00:02,  6.23it/s]Epoch: 4, train for the 193-th batch, train loss: 0.32243698835372925:  80%|███████▉  | 192/241 [00:30<00:07,  6.66it/s]Epoch: 2, train for the 93-th batch, train loss: 0.3862769603729248:  24%|███▏         | 93/383 [00:18<01:11,  4.04it/s]evaluate for the 1-th batch, evaluate loss: 0.686316728591919:   0%|                             | 0/21 [00:00<?, ?it/s]Epoch: 4, train for the 193-th batch, train loss: 0.32243698835372925:  80%|████████  | 193/241 [00:30<00:06,  7.25it/s]Epoch: 3, train for the 207-th batch, train loss: 0.5684009194374084:  87%|█████████▌ | 206/237 [00:43<00:09,  3.38it/s]Epoch: 6, train for the 45-th batch, train loss: 0.5814660787582397:  29%|███▊         | 44/151 [00:06<00:17,  6.09it/s]Epoch: 6, train for the 45-th batch, train loss: 0.5814660787582397:  30%|███▊         | 45/151 [00:07<00:24,  4.30it/s]Epoch: 3, train for the 207-th batch, train loss: 0.5684009194374084:  87%|█████████▌ | 207/237 [00:43<00:08,  3.35it/s]evaluate for the 2-th batch, evaluate loss: 0.7395411729812622:   0%|                            | 0/21 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.7395411729812622:  10%|█▉                  | 2/21 [00:00<00:01, 15.83it/s]Epoch: 7, train for the 131-th batch, train loss: 0.5065006613731384:  89%|█████████▊ | 130/146 [00:19<00:02,  6.23it/s]Epoch: 7, train for the 131-th batch, train loss: 0.5065006613731384:  90%|█████████▊ | 131/146 [00:19<00:02,  6.27it/s]evaluate for the 3-th batch, evaluate loss: 0.6861405968666077:  10%|█▉                  | 2/21 [00:00<00:01, 15.83it/s]Epoch: 4, train for the 194-th batch, train loss: 0.30523115396499634:  80%|████████  | 193/241 [00:30<00:06,  7.25it/s]Epoch: 4, train for the 194-th batch, train loss: 0.30523115396499634:  80%|████████  | 194/241 [00:30<00:06,  6.91it/s]Epoch: 2, train for the 94-th batch, train loss: 0.2967318296432495:  24%|███▏         | 93/383 [00:19<01:11,  4.04it/s]Epoch: 6, train for the 46-th batch, train loss: 0.5832906365394592:  30%|███▊         | 45/151 [00:07<00:24,  4.30it/s]evaluate for the 4-th batch, evaluate loss: 0.6215430498123169:  10%|█▉                  | 2/21 [00:00<00:01, 15.83it/s]evaluate for the 4-th batch, evaluate loss: 0.6215430498123169:  19%|███▊                | 4/21 [00:00<00:01, 15.49it/s]Epoch: 6, train for the 46-th batch, train loss: 0.5832906365394592:  30%|███▉         | 46/151 [00:07<00:21,  4.81it/s]Epoch: 2, train for the 94-th batch, train loss: 0.2967318296432495:  25%|███▏         | 94/383 [00:19<01:10,  4.07it/s]Epoch: 3, train for the 208-th batch, train loss: 0.5699732303619385:  87%|█████████▌ | 207/237 [00:43<00:08,  3.35it/s]Epoch: 7, train for the 132-th batch, train loss: 0.4353359639644623:  90%|█████████▊ | 131/146 [00:19<00:02,  6.27it/s]evaluate for the 5-th batch, evaluate loss: 0.7350864410400391:  19%|███▊                | 4/21 [00:00<00:01, 15.49it/s]Epoch: 3, train for the 208-th batch, train loss: 0.5699732303619385:  88%|█████████▋ | 208/237 [00:43<00:08,  3.59it/s]Epoch: 4, train for the 195-th batch, train loss: 0.5177260637283325:  80%|████████▊  | 194/241 [00:31<00:06,  6.91it/s]Epoch: 7, train for the 132-th batch, train loss: 0.4353359639644623:  90%|█████████▉ | 132/146 [00:19<00:02,  6.03it/s]Epoch: 4, train for the 195-th batch, train loss: 0.5177260637283325:  81%|████████▉  | 195/241 [00:31<00:06,  6.58it/s]Epoch: 6, train for the 47-th batch, train loss: 0.6115199327468872:  30%|███▉         | 46/151 [00:07<00:21,  4.81it/s]Epoch: 6, train for the 47-th batch, train loss: 0.6115199327468872:  31%|████         | 47/151 [00:07<00:19,  5.32it/s]evaluate for the 6-th batch, evaluate loss: 0.6780518293380737:  19%|███▊                | 4/21 [00:00<00:01, 15.49it/s]evaluate for the 6-th batch, evaluate loss: 0.6780518293380737:  29%|█████▋              | 6/21 [00:00<00:01, 14.26it/s]Epoch: 2, train for the 95-th batch, train loss: 0.38919371366500854:  25%|██▉         | 94/383 [00:19<01:10,  4.07it/s]Epoch: 2, train for the 95-th batch, train loss: 0.38919371366500854:  25%|██▉         | 95/383 [00:19<01:06,  4.36it/s]evaluate for the 7-th batch, evaluate loss: 0.6396966576576233:  29%|█████▋              | 6/21 [00:00<00:01, 14.26it/s]Epoch: 7, train for the 133-th batch, train loss: 0.49825942516326904:  90%|█████████ | 132/146 [00:19<00:02,  6.03it/s]Epoch: 7, train for the 133-th batch, train loss: 0.49825942516326904:  91%|█████████ | 133/146 [00:19<00:02,  5.93it/s]Epoch: 4, train for the 196-th batch, train loss: 0.452859103679657:  81%|█████████▋  | 195/241 [00:31<00:06,  6.58it/s]evaluate for the 8-th batch, evaluate loss: 0.6338886022567749:  29%|█████▋              | 6/21 [00:00<00:01, 14.26it/s]evaluate for the 8-th batch, evaluate loss: 0.6338886022567749:  38%|███████▌            | 8/21 [00:00<00:00, 14.42it/s]Epoch: 3, train for the 209-th batch, train loss: 0.5376834273338318:  88%|█████████▋ | 208/237 [00:43<00:08,  3.59it/s]Epoch: 6, train for the 48-th batch, train loss: 0.5670216679573059:  31%|████         | 47/151 [00:07<00:19,  5.32it/s]Epoch: 4, train for the 196-th batch, train loss: 0.452859103679657:  81%|█████████▊  | 196/241 [00:31<00:07,  6.03it/s]Epoch: 6, train for the 48-th batch, train loss: 0.5670216679573059:  32%|████▏        | 48/151 [00:07<00:19,  5.36it/s]evaluate for the 9-th batch, evaluate loss: 0.6223750710487366:  38%|███████▌            | 8/21 [00:00<00:00, 14.42it/s]Epoch: 3, train for the 209-th batch, train loss: 0.5376834273338318:  88%|█████████▋ | 209/237 [00:43<00:07,  3.66it/s]Epoch: 2, train for the 96-th batch, train loss: 0.39951270818710327:  25%|██▉         | 95/383 [00:19<01:06,  4.36it/s]Epoch: 2, train for the 96-th batch, train loss: 0.39951270818710327:  25%|███         | 96/383 [00:19<01:04,  4.44it/s]Epoch: 7, train for the 134-th batch, train loss: 0.5067915320396423:  91%|██████████ | 133/146 [00:19<00:02,  5.93it/s]evaluate for the 10-th batch, evaluate loss: 0.6460239887237549:  38%|███████▏           | 8/21 [00:00<00:00, 14.42it/s]evaluate for the 10-th batch, evaluate loss: 0.6460239887237549:  48%|████████▌         | 10/21 [00:00<00:00, 13.75it/s]Epoch: 7, train for the 134-th batch, train loss: 0.5067915320396423:  92%|██████████ | 134/146 [00:19<00:02,  5.94it/s]Epoch: 4, train for the 197-th batch, train loss: 0.5220029354095459:  81%|████████▉  | 196/241 [00:31<00:07,  6.03it/s]Epoch: 6, train for the 49-th batch, train loss: 0.5793048143386841:  32%|████▏        | 48/151 [00:07<00:19,  5.36it/s]Epoch: 4, train for the 197-th batch, train loss: 0.5220029354095459:  82%|████████▉  | 197/241 [00:31<00:07,  5.79it/s]evaluate for the 11-th batch, evaluate loss: 0.6353691220283508:  48%|████████▌         | 10/21 [00:00<00:00, 13.75it/s]Epoch: 3, train for the 210-th batch, train loss: 0.5667338967323303:  88%|█████████▋ | 209/237 [00:43<00:07,  3.66it/s]Epoch: 6, train for the 49-th batch, train loss: 0.5793048143386841:  32%|████▏        | 49/151 [00:07<00:19,  5.31it/s]Epoch: 3, train for the 210-th batch, train loss: 0.5667338967323303:  89%|█████████▋ | 210/237 [00:43<00:06,  3.93it/s]Epoch: 7, train for the 135-th batch, train loss: 0.5005103349685669:  92%|██████████ | 134/146 [00:20<00:02,  5.94it/s]evaluate for the 12-th batch, evaluate loss: 0.648455023765564:  48%|█████████          | 10/21 [00:00<00:00, 13.75it/s]evaluate for the 12-th batch, evaluate loss: 0.648455023765564:  57%|██████████▊        | 12/21 [00:00<00:00, 12.87it/s]Epoch: 7, train for the 135-th batch, train loss: 0.5005103349685669:  92%|██████████▏| 135/146 [00:20<00:01,  5.90it/s]Epoch: 2, train for the 97-th batch, train loss: 0.1973908543586731:  25%|███▎         | 96/383 [00:19<01:04,  4.44it/s]Epoch: 2, train for the 97-th batch, train loss: 0.1973908543586731:  25%|███▎         | 97/383 [00:19<01:03,  4.50it/s]Epoch: 4, train for the 198-th batch, train loss: 0.48216715455055237:  82%|████████▏ | 197/241 [00:31<00:07,  5.79it/s]Epoch: 4, train for the 198-th batch, train loss: 0.48216715455055237:  82%|████████▏ | 198/241 [00:31<00:07,  5.83it/s]Epoch: 6, train for the 50-th batch, train loss: 0.5624690651893616:  32%|████▏        | 49/151 [00:07<00:19,  5.31it/s]evaluate for the 13-th batch, evaluate loss: 0.6141420006752014:  57%|██████████▎       | 12/21 [00:00<00:00, 12.87it/s]Epoch: 6, train for the 50-th batch, train loss: 0.5624690651893616:  33%|████▎        | 50/151 [00:07<00:18,  5.33it/s]Epoch: 7, train for the 136-th batch, train loss: 0.48443853855133057:  92%|█████████▏| 135/146 [00:20<00:01,  5.90it/s]Epoch: 3, train for the 211-th batch, train loss: 0.5268787145614624:  89%|█████████▋ | 210/237 [00:44<00:06,  3.93it/s]Epoch: 7, train for the 136-th batch, train loss: 0.48443853855133057:  93%|█████████▎| 136/146 [00:20<00:01,  6.11it/s]evaluate for the 14-th batch, evaluate loss: 0.6011697053909302:  57%|██████████▎       | 12/21 [00:01<00:00, 12.87it/s]evaluate for the 14-th batch, evaluate loss: 0.6011697053909302:  67%|████████████      | 14/21 [00:01<00:00, 12.25it/s]Epoch: 3, train for the 211-th batch, train loss: 0.5268787145614624:  89%|█████████▊ | 211/237 [00:44<00:06,  4.00it/s]Epoch: 2, train for the 98-th batch, train loss: 0.2232840657234192:  25%|███▎         | 97/383 [00:20<01:03,  4.50it/s]Epoch: 4, train for the 199-th batch, train loss: 0.329410582780838:  82%|█████████▊  | 198/241 [00:31<00:07,  5.83it/s]Epoch: 2, train for the 98-th batch, train loss: 0.2232840657234192:  26%|███▎         | 98/383 [00:20<01:00,  4.72it/s]Epoch: 4, train for the 199-th batch, train loss: 0.329410582780838:  83%|█████████▉  | 199/241 [00:31<00:07,  5.81it/s]Epoch: 6, train for the 51-th batch, train loss: 0.5911656022071838:  33%|████▎        | 50/151 [00:08<00:18,  5.33it/s]Epoch: 6, train for the 51-th batch, train loss: 0.5911656022071838:  34%|████▍        | 51/151 [00:08<00:18,  5.53it/s]evaluate for the 15-th batch, evaluate loss: 0.6292897462844849:  67%|████████████      | 14/21 [00:01<00:00, 12.25it/s]Epoch: 7, train for the 137-th batch, train loss: 0.4463804066181183:  93%|██████████▏| 136/146 [00:20<00:01,  6.11it/s]Epoch: 7, train for the 137-th batch, train loss: 0.4463804066181183:  94%|██████████▎| 137/146 [00:20<00:01,  6.07it/s]evaluate for the 16-th batch, evaluate loss: 0.5978273749351501:  67%|████████████      | 14/21 [00:01<00:00, 12.25it/s]evaluate for the 16-th batch, evaluate loss: 0.5978273749351501:  76%|█████████████▋    | 16/21 [00:01<00:00, 11.86it/s]Epoch: 4, train for the 200-th batch, train loss: 0.41571930050849915:  83%|████████▎ | 199/241 [00:31<00:07,  5.81it/s]Epoch: 4, train for the 200-th batch, train loss: 0.41571930050849915:  83%|████████▎ | 200/241 [00:31<00:07,  5.71it/s]Epoch: 3, train for the 212-th batch, train loss: 0.5309867858886719:  89%|█████████▊ | 211/237 [00:44<00:06,  4.00it/s]Epoch: 6, train for the 52-th batch, train loss: 0.6285373568534851:  34%|████▍        | 51/151 [00:08<00:18,  5.53it/s]Epoch: 6, train for the 52-th batch, train loss: 0.6285373568534851:  34%|████▍        | 52/151 [00:08<00:17,  5.52it/s]Epoch: 2, train for the 99-th batch, train loss: 0.2927660644054413:  26%|███▎         | 98/383 [00:20<01:00,  4.72it/s]evaluate for the 17-th batch, evaluate loss: 0.5158539414405823:  76%|█████████████▋    | 16/21 [00:01<00:00, 11.86it/s]Epoch: 3, train for the 212-th batch, train loss: 0.5309867858886719:  89%|█████████▊ | 212/237 [00:44<00:06,  3.88it/s]Epoch: 2, train for the 99-th batch, train loss: 0.2927660644054413:  26%|███▎         | 99/383 [00:20<01:03,  4.46it/s]evaluate for the 18-th batch, evaluate loss: 0.610569417476654:  76%|██████████████▍    | 16/21 [00:01<00:00, 11.86it/s]evaluate for the 18-th batch, evaluate loss: 0.610569417476654:  86%|████████████████▎  | 18/21 [00:01<00:00, 12.29it/s]Epoch: 7, train for the 138-th batch, train loss: 0.514052152633667:  94%|███████████▎| 137/146 [00:20<00:01,  6.07it/s]Epoch: 7, train for the 138-th batch, train loss: 0.514052152633667:  95%|███████████▎| 138/146 [00:20<00:01,  5.59it/s]Epoch: 4, train for the 201-th batch, train loss: 0.43382224440574646:  83%|████████▎ | 200/241 [00:32<00:07,  5.71it/s]Epoch: 6, train for the 53-th batch, train loss: 0.6054901480674744:  34%|████▍        | 52/151 [00:08<00:17,  5.52it/s]evaluate for the 19-th batch, evaluate loss: 0.6022761464118958:  86%|███████████████▍  | 18/21 [00:01<00:00, 12.29it/s]Epoch: 4, train for the 201-th batch, train loss: 0.43382224440574646:  83%|████████▎ | 201/241 [00:32<00:07,  5.59it/s]Epoch: 6, train for the 53-th batch, train loss: 0.6054901480674744:  35%|████▌        | 53/151 [00:08<00:17,  5.64it/s]Epoch: 2, train for the 100-th batch, train loss: 0.33991870284080505:  26%|██▊        | 99/383 [00:20<01:03,  4.46it/s]Epoch: 7, train for the 139-th batch, train loss: 0.5413289666175842:  95%|██████████▍| 138/146 [00:20<00:01,  5.59it/s]evaluate for the 20-th batch, evaluate loss: 0.5590639114379883:  86%|███████████████▍  | 18/21 [00:01<00:00, 12.29it/s]evaluate for the 20-th batch, evaluate loss: 0.5590639114379883:  95%|█████████████████▏| 20/21 [00:01<00:00, 11.80it/s]Epoch: 7, train for the 139-th batch, train loss: 0.5413289666175842:  95%|██████████▍| 139/146 [00:20<00:01,  5.64it/s]Epoch: 2, train for the 100-th batch, train loss: 0.33991870284080505:  26%|██▌       | 100/383 [00:20<01:04,  4.41it/s]Epoch: 3, train for the 213-th batch, train loss: 0.5366342663764954:  89%|█████████▊ | 212/237 [00:44<00:06,  3.88it/s]Epoch: 6, train for the 54-th batch, train loss: 0.6082593202590942:  35%|████▌        | 53/151 [00:08<00:17,  5.64it/s]evaluate for the 21-th batch, evaluate loss: 0.48984989523887634:  95%|████████████████▏| 20/21 [00:01<00:00, 11.80it/s]evaluate for the 21-th batch, evaluate loss: 0.48984989523887634: 100%|█████████████████| 21/21 [00:01<00:00, 13.01it/s]
Epoch: 6, train for the 54-th batch, train loss: 0.6082593202590942:  36%|████▋        | 54/151 [00:08<00:15,  6.11it/s]Epoch: 3, train for the 213-th batch, train loss: 0.5366342663764954:  90%|█████████▉ | 213/237 [00:44<00:06,  3.73it/s]Epoch: 6, train for the 55-th batch, train loss: 0.5814686417579651:  36%|████▋        | 54/151 [00:08<00:15,  6.11it/s]Epoch: 6, train for the 55-th batch, train loss: 0.5814686417579651:  36%|████▋        | 55/151 [00:08<00:15,  6.33it/s]Epoch: 7, train for the 140-th batch, train loss: 0.48957714438438416:  95%|█████████▌| 139/146 [00:21<00:01,  5.64it/s]Epoch: 4, train for the 202-th batch, train loss: 0.23558340966701508:  83%|████████▎ | 201/241 [00:32<00:07,  5.59it/s]Epoch: 2, train for the 101-th batch, train loss: 0.38365301489830017:  26%|██▌       | 100/383 [00:20<01:04,  4.41it/s]Epoch: 7, train for the 140-th batch, train loss: 0.48957714438438416:  96%|█████████▌| 140/146 [00:21<00:01,  5.50it/s]Epoch: 4, train for the 202-th batch, train loss: 0.23558340966701508:  84%|████████▍ | 202/241 [00:32<00:08,  4.63it/s]Epoch: 2, train for the 101-th batch, train loss: 0.38365301489830017:  26%|██▋       | 101/383 [00:20<01:03,  4.45it/s]INFO:root:Epoch: 8, learning rate: 0.0001, train loss: 0.4378
INFO:root:train average_precision, 0.8842
INFO:root:train roc_auc, 0.8775
INFO:root:validate loss: 0.4401
INFO:root:validate average_precision, 0.8748
INFO:root:validate roc_auc, 0.8717
INFO:root:new node validate loss: 0.6282
INFO:root:new node validate first_1_average_precision, 0.7233
INFO:root:new node validate first_1_roc_auc, 0.7124
INFO:root:new node validate first_3_average_precision, 0.7327
INFO:root:new node validate first_3_roc_auc, 0.7293
INFO:root:new node validate first_10_average_precision, 0.7290
INFO:root:new node validate first_10_roc_auc, 0.7398
INFO:root:new node validate average_precision, 0.7273
INFO:root:new node validate roc_auc, 0.7509
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]Epoch: 3, train for the 214-th batch, train loss: 0.585776150226593:  90%|██████████▊ | 213/237 [00:44<00:06,  3.73it/s]Epoch: 3, train for the 214-th batch, train loss: 0.585776150226593:  90%|██████████▊ | 214/237 [00:44<00:05,  3.93it/s]Epoch: 7, train for the 141-th batch, train loss: 0.4423370361328125:  96%|██████████▌| 140/146 [00:21<00:01,  5.50it/s]Epoch: 7, train for the 141-th batch, train loss: 0.4423370361328125:  97%|██████████▌| 141/146 [00:21<00:00,  6.17it/s]Epoch: 6, train for the 56-th batch, train loss: 0.4932914674282074:  36%|████▋        | 55/151 [00:08<00:15,  6.33it/s]Epoch: 6, train for the 56-th batch, train loss: 0.4932914674282074:  37%|████▊        | 56/151 [00:08<00:15,  6.32it/s]Epoch: 4, train for the 203-th batch, train loss: 0.5510911345481873:  84%|█████████▏ | 202/241 [00:32<00:08,  4.63it/s]Epoch: 4, train for the 203-th batch, train loss: 0.5510911345481873:  84%|█████████▎ | 203/241 [00:32<00:07,  4.99it/s]Epoch: 2, train for the 102-th batch, train loss: 0.36342334747314453:  26%|██▋       | 101/383 [00:20<01:03,  4.45it/s]Epoch: 7, train for the 142-th batch, train loss: 0.4399736821651459:  97%|██████████▌| 141/146 [00:21<00:00,  6.17it/s]Epoch: 7, train for the 142-th batch, train loss: 0.4399736821651459:  97%|██████████▋| 142/146 [00:21<00:00,  6.46it/s]Epoch: 2, train for the 102-th batch, train loss: 0.36342334747314453:  27%|██▋       | 102/383 [00:20<01:03,  4.41it/s]Epoch: 9, train for the 1-th batch, train loss: 0.6269662976264954:   0%|                       | 0/119 [00:00<?, ?it/s]Epoch: 3, train for the 215-th batch, train loss: 0.565882682800293:  90%|██████████▊ | 214/237 [00:45<00:05,  3.93it/s]Epoch: 9, train for the 1-th batch, train loss: 0.6269662976264954:   1%|▏              | 1/119 [00:00<00:27,  4.27it/s]Epoch: 6, train for the 57-th batch, train loss: 0.5300824046134949:  37%|████▊        | 56/151 [00:08<00:15,  6.32it/s]Epoch: 4, train for the 204-th batch, train loss: 0.40491124987602234:  84%|████████▍ | 203/241 [00:32<00:07,  4.99it/s]Epoch: 6, train for the 57-th batch, train loss: 0.5300824046134949:  38%|████▉        | 57/151 [00:08<00:15,  6.25it/s]Epoch: 3, train for the 215-th batch, train loss: 0.565882682800293:  91%|██████████▉ | 215/237 [00:45<00:05,  4.02it/s]Epoch: 4, train for the 204-th batch, train loss: 0.40491124987602234:  85%|████████▍ | 204/241 [00:32<00:07,  5.28it/s]Epoch: 7, train for the 143-th batch, train loss: 0.489672988653183:  97%|███████████▋| 142/146 [00:21<00:00,  6.46it/s]Epoch: 7, train for the 143-th batch, train loss: 0.489672988653183:  98%|███████████▊| 143/146 [00:21<00:00,  6.36it/s]Epoch: 9, train for the 2-th batch, train loss: 0.6586227416992188:   1%|▏              | 1/119 [00:00<00:27,  4.27it/s]Epoch: 9, train for the 2-th batch, train loss: 0.6586227416992188:   2%|▎              | 2/119 [00:00<00:21,  5.42it/s]Epoch: 6, train for the 58-th batch, train loss: 0.5371838212013245:  38%|████▉        | 57/151 [00:09<00:15,  6.25it/s]Epoch: 2, train for the 103-th batch, train loss: 0.3150186836719513:  27%|██▉        | 102/383 [00:21<01:03,  4.41it/s]Epoch: 6, train for the 58-th batch, train loss: 0.5371838212013245:  38%|████▉        | 58/151 [00:09<00:14,  6.24it/s]Epoch: 4, train for the 205-th batch, train loss: 0.45220810174942017:  85%|████████▍ | 204/241 [00:32<00:07,  5.28it/s]Epoch: 2, train for the 103-th batch, train loss: 0.3150186836719513:  27%|██▉        | 103/383 [00:21<01:04,  4.33it/s]Epoch: 4, train for the 205-th batch, train loss: 0.45220810174942017:  85%|████████▌ | 205/241 [00:32<00:06,  5.47it/s]Epoch: 3, train for the 216-th batch, train loss: 0.5483521223068237:  91%|█████████▉ | 215/237 [00:45<00:05,  4.02it/s]Epoch: 3, train for the 216-th batch, train loss: 0.5483521223068237:  91%|██████████ | 216/237 [00:45<00:05,  4.10it/s]Epoch: 7, train for the 144-th batch, train loss: 0.41693273186683655:  98%|█████████▊| 143/146 [00:21<00:00,  6.36it/s]Epoch: 9, train for the 3-th batch, train loss: 0.7446193695068359:   2%|▎              | 2/119 [00:00<00:21,  5.42it/s]Epoch: 9, train for the 3-th batch, train loss: 0.7446193695068359:   3%|▍              | 3/119 [00:00<00:20,  5.68it/s]Epoch: 7, train for the 144-th batch, train loss: 0.41693273186683655:  99%|█████████▊| 144/146 [00:21<00:00,  6.12it/s]Epoch: 6, train for the 59-th batch, train loss: 0.5155545473098755:  38%|████▉        | 58/151 [00:09<00:14,  6.24it/s]Epoch: 6, train for the 59-th batch, train loss: 0.5155545473098755:  39%|█████        | 59/151 [00:09<00:15,  6.09it/s]Epoch: 4, train for the 206-th batch, train loss: 0.5577964186668396:  85%|█████████▎ | 205/241 [00:33<00:06,  5.47it/s]Epoch: 4, train for the 206-th batch, train loss: 0.5577964186668396:  85%|█████████▍ | 206/241 [00:33<00:06,  5.66it/s]Epoch: 2, train for the 104-th batch, train loss: 0.3204396665096283:  27%|██▉        | 103/383 [00:21<01:04,  4.33it/s]Epoch: 9, train for the 4-th batch, train loss: 0.6671651601791382:   3%|▍              | 3/119 [00:00<00:20,  5.68it/s]Epoch: 9, train for the 4-th batch, train loss: 0.6671651601791382:   3%|▌              | 4/119 [00:00<00:17,  6.62it/s]Epoch: 2, train for the 104-th batch, train loss: 0.3204396665096283:  27%|██▉        | 104/383 [00:21<01:02,  4.43it/s]Epoch: 7, train for the 145-th batch, train loss: 0.4742830991744995:  99%|██████████▊| 144/146 [00:21<00:00,  6.12it/s]Epoch: 3, train for the 217-th batch, train loss: 0.5011364817619324:  91%|██████████ | 216/237 [00:45<00:05,  4.10it/s]Epoch: 7, train for the 145-th batch, train loss: 0.4742830991744995:  99%|██████████▉| 145/146 [00:21<00:00,  6.03it/s]Epoch: 6, train for the 60-th batch, train loss: 0.5193414688110352:  39%|█████        | 59/151 [00:09<00:15,  6.09it/s]Epoch: 3, train for the 217-th batch, train loss: 0.5011364817619324:  92%|██████████ | 217/237 [00:45<00:04,  4.07it/s]Epoch: 6, train for the 60-th batch, train loss: 0.5193414688110352:  40%|█████▏       | 60/151 [00:09<00:14,  6.19it/s]Epoch: 4, train for the 207-th batch, train loss: 0.5449187159538269:  85%|█████████▍ | 206/241 [00:33<00:06,  5.66it/s]Epoch: 9, train for the 5-th batch, train loss: 0.5986032485961914:   3%|▌              | 4/119 [00:00<00:17,  6.62it/s]Epoch: 4, train for the 207-th batch, train loss: 0.5449187159538269:  86%|█████████▍ | 207/241 [00:33<00:05,  5.70it/s]Epoch: 9, train for the 5-th batch, train loss: 0.5986032485961914:   4%|▋              | 5/119 [00:00<00:16,  6.77it/s]Epoch: 2, train for the 105-th batch, train loss: 0.24600011110305786:  27%|██▋       | 104/383 [00:21<01:02,  4.43it/s]Epoch: 7, train for the 146-th batch, train loss: 0.4776313602924347:  99%|██████████▉| 145/146 [00:21<00:00,  6.03it/s]Epoch: 7, train for the 146-th batch, train loss: 0.4776313602924347: 100%|███████████| 146/146 [00:21<00:00,  6.18it/s]Epoch: 7, train for the 146-th batch, train loss: 0.4776313602924347: 100%|███████████| 146/146 [00:21<00:00,  6.65it/s]
Epoch: 2, train for the 105-th batch, train loss: 0.24600011110305786:  27%|██▋       | 105/383 [00:21<01:01,  4.53it/s]Epoch: 6, train for the 61-th batch, train loss: 0.545671820640564:  40%|█████▌        | 60/151 [00:09<00:14,  6.19it/s]Epoch: 6, train for the 61-th batch, train loss: 0.545671820640564:  40%|█████▋        | 61/151 [00:09<00:14,  6.23it/s]Epoch: 4, train for the 208-th batch, train loss: 0.5267430543899536:  86%|█████████▍ | 207/241 [00:33<00:05,  5.70it/s]  0%|                                                                                            | 0/38 [00:00<?, ?it/s]Epoch: 3, train for the 218-th batch, train loss: 0.5069748759269714:  92%|██████████ | 217/237 [00:45<00:04,  4.07it/s]Epoch: 9, train for the 6-th batch, train loss: 0.5360105633735657:   4%|▋              | 5/119 [00:00<00:16,  6.77it/s]Epoch: 9, train for the 6-th batch, train loss: 0.5360105633735657:   5%|▊              | 6/119 [00:00<00:16,  6.65it/s]Epoch: 4, train for the 208-th batch, train loss: 0.5267430543899536:  86%|█████████▍ | 208/241 [00:33<00:05,  5.72it/s]Epoch: 3, train for the 218-th batch, train loss: 0.5069748759269714:  92%|██████████ | 218/237 [00:45<00:04,  4.07it/s]evaluate for the 1-th batch, evaluate loss: 0.4501997232437134:   0%|                            | 0/38 [00:00<?, ?it/s]Epoch: 6, train for the 62-th batch, train loss: 0.5721483826637268:  40%|█████▎       | 61/151 [00:09<00:14,  6.23it/s]Epoch: 2, train for the 106-th batch, train loss: 0.31014376878738403:  27%|██▋       | 105/383 [00:21<01:01,  4.53it/s]Epoch: 6, train for the 62-th batch, train loss: 0.5721483826637268:  41%|█████▎       | 62/151 [00:09<00:14,  6.23it/s]Epoch: 2, train for the 106-th batch, train loss: 0.31014376878738403:  28%|██▊       | 106/383 [00:21<01:00,  4.56it/s]Epoch: 9, train for the 7-th batch, train loss: 0.5176005959510803:   5%|▊              | 6/119 [00:01<00:16,  6.65it/s]Epoch: 4, train for the 209-th batch, train loss: 0.5072587132453918:  86%|█████████▍ | 208/241 [00:33<00:05,  5.72it/s]evaluate for the 2-th batch, evaluate loss: 0.4869345724582672:   0%|                            | 0/38 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.4869345724582672:   5%|█                   | 2/38 [00:00<00:03, 11.33it/s]Epoch: 9, train for the 7-th batch, train loss: 0.5176005959510803:   6%|▉              | 7/119 [00:01<00:17,  6.33it/s]Epoch: 4, train for the 209-th batch, train loss: 0.5072587132453918:  87%|█████████▌ | 209/241 [00:33<00:05,  5.62it/s]evaluate for the 3-th batch, evaluate loss: 0.4636758267879486:   5%|█                   | 2/38 [00:00<00:03, 11.33it/s]Epoch: 6, train for the 63-th batch, train loss: 0.5590089559555054:  41%|█████▎       | 62/151 [00:09<00:14,  6.23it/s]Epoch: 3, train for the 219-th batch, train loss: 0.5342143774032593:  92%|██████████ | 218/237 [00:46<00:04,  4.07it/s]Epoch: 6, train for the 63-th batch, train loss: 0.5590089559555054:  42%|█████▍       | 63/151 [00:09<00:13,  6.49it/s]evaluate for the 4-th batch, evaluate loss: 0.4698445796966553:   5%|█                   | 2/38 [00:00<00:03, 11.33it/s]evaluate for the 4-th batch, evaluate loss: 0.4698445796966553:  11%|██                  | 4/38 [00:00<00:02, 13.20it/s]Epoch: 3, train for the 219-th batch, train loss: 0.5342143774032593:  92%|██████████▏| 219/237 [00:46<00:04,  4.02it/s]Epoch: 2, train for the 107-th batch, train loss: 0.42744678258895874:  28%|██▊       | 106/383 [00:22<01:00,  4.56it/s]Epoch: 9, train for the 8-th batch, train loss: 0.4642295837402344:   6%|▉              | 7/119 [00:01<00:17,  6.33it/s]Epoch: 9, train for the 8-th batch, train loss: 0.4642295837402344:   7%|█              | 8/119 [00:01<00:17,  6.45it/s]Epoch: 2, train for the 107-th batch, train loss: 0.42744678258895874:  28%|██▊       | 107/383 [00:22<01:00,  4.58it/s]evaluate for the 5-th batch, evaluate loss: 0.5110360383987427:  11%|██                  | 4/38 [00:00<00:02, 13.20it/s]Epoch: 4, train for the 210-th batch, train loss: 0.40417373180389404:  87%|████████▋ | 209/241 [00:33<00:05,  5.62it/s]Epoch: 4, train for the 210-th batch, train loss: 0.40417373180389404:  87%|████████▋ | 210/241 [00:33<00:05,  5.34it/s]Epoch: 6, train for the 64-th batch, train loss: 0.5181100964546204:  42%|█████▍       | 63/151 [00:10<00:13,  6.49it/s]Epoch: 6, train for the 64-th batch, train loss: 0.5181100964546204:  42%|█████▌       | 64/151 [00:10<00:13,  6.29it/s]Epoch: 9, train for the 9-th batch, train loss: 0.4930397570133209:   7%|█              | 8/119 [00:01<00:17,  6.45it/s]Epoch: 9, train for the 9-th batch, train loss: 0.4930397570133209:   8%|█▏             | 9/119 [00:01<00:16,  6.87it/s]evaluate for the 6-th batch, evaluate loss: 0.4662112593650818:  11%|██                  | 4/38 [00:00<00:02, 13.20it/s]evaluate for the 6-th batch, evaluate loss: 0.4662112593650818:  16%|███▏                | 6/38 [00:00<00:02, 13.23it/s]Epoch: 3, train for the 220-th batch, train loss: 0.5618405938148499:  92%|██████████▏| 219/237 [00:46<00:04,  4.02it/s]evaluate for the 7-th batch, evaluate loss: 0.42901909351348877:  16%|███                | 6/38 [00:00<00:02, 13.23it/s]Epoch: 3, train for the 220-th batch, train loss: 0.5618405938148499:  93%|██████████▏| 220/237 [00:46<00:04,  4.03it/s]Epoch: 2, train for the 108-th batch, train loss: 0.46940353512763977:  28%|██▊       | 107/383 [00:22<01:00,  4.58it/s]Epoch: 9, train for the 10-th batch, train loss: 0.4485691487789154:   8%|█             | 9/119 [00:01<00:16,  6.87it/s]evaluate for the 8-th batch, evaluate loss: 0.44192737340927124:  16%|███                | 6/38 [00:00<00:02, 13.23it/s]evaluate for the 8-th batch, evaluate loss: 0.44192737340927124:  21%|████               | 8/38 [00:00<00:02, 12.85it/s]Epoch: 9, train for the 10-th batch, train loss: 0.4485691487789154:   8%|█            | 10/119 [00:01<00:16,  6.58it/s]Epoch: 2, train for the 108-th batch, train loss: 0.46940353512763977:  28%|██▊       | 108/383 [00:22<01:03,  4.33it/s]Epoch: 4, train for the 211-th batch, train loss: 0.35047706961631775:  87%|████████▋ | 210/241 [00:34<00:05,  5.34it/s]Epoch: 6, train for the 65-th batch, train loss: 0.5073788166046143:  42%|█████▌       | 64/151 [00:10<00:13,  6.29it/s]Epoch: 4, train for the 211-th batch, train loss: 0.35047706961631775:  88%|████████▊ | 211/241 [00:34<00:06,  4.90it/s]evaluate for the 9-th batch, evaluate loss: 0.4586981236934662:  21%|████▏               | 8/38 [00:00<00:02, 12.85it/s]Epoch: 6, train for the 65-th batch, train loss: 0.5073788166046143:  43%|█████▌       | 65/151 [00:10<00:15,  5.48it/s]Epoch: 3, train for the 221-th batch, train loss: 0.5796614289283752:  93%|██████████▏| 220/237 [00:46<00:04,  4.03it/s]Epoch: 9, train for the 11-th batch, train loss: 0.46731847524642944:   8%|█           | 10/119 [00:01<00:16,  6.58it/s]evaluate for the 10-th batch, evaluate loss: 0.48856320977211:  21%|████▍                | 8/38 [00:00<00:02, 12.85it/s]evaluate for the 10-th batch, evaluate loss: 0.48856320977211:  26%|█████▎              | 10/38 [00:00<00:02, 12.91it/s]Epoch: 9, train for the 11-th batch, train loss: 0.46731847524642944:   9%|█           | 11/119 [00:01<00:16,  6.53it/s]Epoch: 3, train for the 221-th batch, train loss: 0.5796614289283752:  93%|██████████▎| 221/237 [00:46<00:03,  4.11it/s]Epoch: 4, train for the 212-th batch, train loss: 0.48239317536354065:  88%|████████▊ | 211/241 [00:34<00:06,  4.90it/s]evaluate for the 11-th batch, evaluate loss: 0.4368550479412079:  26%|████▋             | 10/38 [00:00<00:02, 12.91it/s]Epoch: 6, train for the 66-th batch, train loss: 0.5929399132728577:  43%|█████▌       | 65/151 [00:10<00:15,  5.48it/s]Epoch: 4, train for the 212-th batch, train loss: 0.48239317536354065:  88%|████████▊ | 212/241 [00:34<00:05,  5.06it/s]Epoch: 2, train for the 109-th batch, train loss: 0.30212312936782837:  28%|██▊       | 108/383 [00:22<01:03,  4.33it/s]Epoch: 6, train for the 66-th batch, train loss: 0.5929399132728577:  44%|█████▋       | 66/151 [00:10<00:15,  5.43it/s]Epoch: 2, train for the 109-th batch, train loss: 0.30212312936782837:  28%|██▊       | 109/383 [00:22<01:06,  4.11it/s]Epoch: 9, train for the 12-th batch, train loss: 0.4387134313583374:   9%|█▏           | 11/119 [00:01<00:16,  6.53it/s]evaluate for the 12-th batch, evaluate loss: 0.5144702196121216:  26%|████▋             | 10/38 [00:00<00:02, 12.91it/s]evaluate for the 12-th batch, evaluate loss: 0.5144702196121216:  32%|█████▋            | 12/38 [00:00<00:02, 12.77it/s]Epoch: 9, train for the 12-th batch, train loss: 0.4387134313583374:  10%|█▎           | 12/119 [00:01<00:16,  6.43it/s]Epoch: 4, train for the 213-th batch, train loss: 0.4766617715358734:  88%|█████████▋ | 212/241 [00:34<00:05,  5.06it/s]Epoch: 3, train for the 222-th batch, train loss: 0.5780009627342224:  93%|██████████▎| 221/237 [00:46<00:03,  4.11it/s]Epoch: 4, train for the 213-th batch, train loss: 0.4766617715358734:  88%|█████████▋ | 213/241 [00:34<00:05,  5.55it/s]evaluate for the 13-th batch, evaluate loss: 0.483357310295105:  32%|██████             | 12/38 [00:00<00:02, 12.77it/s]Epoch: 3, train for the 222-th batch, train loss: 0.5780009627342224:  94%|██████████▎| 222/237 [00:46<00:03,  4.18it/s]Epoch: 9, train for the 13-th batch, train loss: 0.41806718707084656:  10%|█▏          | 12/119 [00:02<00:16,  6.43it/s]Epoch: 9, train for the 13-th batch, train loss: 0.41806718707084656:  11%|█▎          | 13/119 [00:02<00:15,  6.70it/s]evaluate for the 14-th batch, evaluate loss: 0.4242328405380249:  32%|█████▋            | 12/38 [00:01<00:02, 12.77it/s]evaluate for the 14-th batch, evaluate loss: 0.4242328405380249:  37%|██████▋           | 14/38 [00:01<00:01, 13.03it/s]Epoch: 2, train for the 110-th batch, train loss: 0.35119467973709106:  28%|██▊       | 109/383 [00:22<01:06,  4.11it/s]Epoch: 4, train for the 214-th batch, train loss: 0.45081740617752075:  88%|████████▊ | 213/241 [00:34<00:05,  5.55it/s]evaluate for the 15-th batch, evaluate loss: 0.43531662225723267:  37%|██████▎          | 14/38 [00:01<00:01, 13.03it/s]Epoch: 4, train for the 214-th batch, train loss: 0.45081740617752075:  89%|████████▉ | 214/241 [00:34<00:04,  5.76it/s]Epoch: 6, train for the 67-th batch, train loss: 0.5201724767684937:  44%|█████▋       | 66/151 [00:10<00:15,  5.43it/s]Epoch: 2, train for the 110-th batch, train loss: 0.35119467973709106:  29%|██▊       | 110/383 [00:22<01:08,  4.00it/s]Epoch: 6, train for the 67-th batch, train loss: 0.5201724767684937:  44%|█████▊       | 67/151 [00:10<00:18,  4.46it/s]Epoch: 9, train for the 14-th batch, train loss: 0.3937920331954956:  11%|█▍           | 13/119 [00:02<00:15,  6.70it/s]Epoch: 9, train for the 14-th batch, train loss: 0.3937920331954956:  12%|█▌           | 14/119 [00:02<00:14,  7.01it/s]evaluate for the 16-th batch, evaluate loss: 0.4884190559387207:  37%|██████▋           | 14/38 [00:01<00:01, 13.03it/s]evaluate for the 16-th batch, evaluate loss: 0.4884190559387207:  42%|███████▌          | 16/38 [00:01<00:01, 13.97it/s]Epoch: 3, train for the 223-th batch, train loss: 0.538448691368103:  94%|███████████▏| 222/237 [00:47<00:03,  4.18it/s]Epoch: 3, train for the 223-th batch, train loss: 0.538448691368103:  94%|███████████▎| 223/237 [00:47<00:03,  4.17it/s]Epoch: 4, train for the 215-th batch, train loss: 0.4765271544456482:  89%|█████████▊ | 214/241 [00:34<00:04,  5.76it/s]evaluate for the 17-th batch, evaluate loss: 0.4467712342739105:  42%|███████▌          | 16/38 [00:01<00:01, 13.97it/s]Epoch: 4, train for the 215-th batch, train loss: 0.4765271544456482:  89%|█████████▊ | 215/241 [00:34<00:04,  6.05it/s]Epoch: 6, train for the 68-th batch, train loss: 0.5259894728660583:  44%|█████▊       | 67/151 [00:10<00:18,  4.46it/s]Epoch: 9, train for the 15-th batch, train loss: 0.48169875144958496:  12%|█▍          | 14/119 [00:02<00:14,  7.01it/s]Epoch: 2, train for the 111-th batch, train loss: 0.43359556794166565:  29%|██▊       | 110/383 [00:23<01:08,  4.00it/s]Epoch: 9, train for the 15-th batch, train loss: 0.48169875144958496:  13%|█▌          | 15/119 [00:02<00:14,  7.02it/s]evaluate for the 18-th batch, evaluate loss: 0.5029048323631287:  42%|███████▌          | 16/38 [00:01<00:01, 13.97it/s]evaluate for the 18-th batch, evaluate loss: 0.5029048323631287:  47%|████████▌         | 18/38 [00:01<00:01, 14.01it/s]Epoch: 6, train for the 68-th batch, train loss: 0.5259894728660583:  45%|█████▊       | 68/151 [00:11<00:17,  4.77it/s]Epoch: 2, train for the 111-th batch, train loss: 0.43359556794166565:  29%|██▉       | 111/383 [00:23<01:05,  4.14it/s]evaluate for the 19-th batch, evaluate loss: 0.48517942428588867:  47%|████████         | 18/38 [00:01<00:01, 14.01it/s]Epoch: 4, train for the 216-th batch, train loss: 0.5608389973640442:  89%|█████████▊ | 215/241 [00:34<00:04,  6.05it/s]Epoch: 4, train for the 216-th batch, train loss: 0.5608389973640442:  90%|█████████▊ | 216/241 [00:34<00:04,  5.89it/s]Epoch: 9, train for the 16-th batch, train loss: 0.4250970482826233:  13%|█▋           | 15/119 [00:02<00:14,  7.02it/s]Epoch: 9, train for the 16-th batch, train loss: 0.4250970482826233:  13%|█▋           | 16/119 [00:02<00:14,  7.12it/s]evaluate for the 20-th batch, evaluate loss: 0.4005472958087921:  47%|████████▌         | 18/38 [00:01<00:01, 14.01it/s]evaluate for the 20-th batch, evaluate loss: 0.4005472958087921:  53%|█████████▍        | 20/38 [00:01<00:01, 14.15it/s]Epoch: 3, train for the 224-th batch, train loss: 0.5391259789466858:  94%|██████████▎| 223/237 [00:47<00:03,  4.17it/s]Epoch: 6, train for the 69-th batch, train loss: 0.532419741153717:  45%|██████▎       | 68/151 [00:11<00:17,  4.77it/s]Epoch: 6, train for the 69-th batch, train loss: 0.532419741153717:  46%|██████▍       | 69/151 [00:11<00:16,  5.08it/s]Epoch: 3, train for the 224-th batch, train loss: 0.5391259789466858:  95%|██████████▍| 224/237 [00:47<00:03,  3.99it/s]evaluate for the 21-th batch, evaluate loss: 0.43790382146835327:  53%|████████▉        | 20/38 [00:01<00:01, 14.15it/s]Epoch: 2, train for the 112-th batch, train loss: 0.40927061438560486:  29%|██▉       | 111/383 [00:23<01:05,  4.14it/s]Epoch: 4, train for the 217-th batch, train loss: 0.447845995426178:  90%|██████████▊ | 216/241 [00:35<00:04,  5.89it/s]Epoch: 9, train for the 17-th batch, train loss: 0.37562909722328186:  13%|█▌          | 16/119 [00:02<00:14,  7.12it/s]evaluate for the 22-th batch, evaluate loss: 0.45590075850486755:  53%|████████▉        | 20/38 [00:01<00:01, 14.15it/s]evaluate for the 22-th batch, evaluate loss: 0.45590075850486755:  58%|█████████▊       | 22/38 [00:01<00:01, 14.06it/s]Epoch: 9, train for the 17-th batch, train loss: 0.37562909722328186:  14%|█▋          | 17/119 [00:02<00:14,  7.00it/s]Epoch: 2, train for the 112-th batch, train loss: 0.40927061438560486:  29%|██▉       | 112/383 [00:23<01:06,  4.09it/s]Epoch: 6, train for the 70-th batch, train loss: 0.5598516464233398:  46%|█████▉       | 69/151 [00:11<00:16,  5.08it/s]Epoch: 4, train for the 217-th batch, train loss: 0.447845995426178:  90%|██████████▊ | 217/241 [00:35<00:04,  5.76it/s]Epoch: 6, train for the 70-th batch, train loss: 0.5598516464233398:  46%|██████       | 70/151 [00:11<00:14,  5.52it/s]evaluate for the 23-th batch, evaluate loss: 0.4518391191959381:  58%|██████████▍       | 22/38 [00:01<00:01, 14.06it/s]Epoch: 3, train for the 225-th batch, train loss: 0.5364259481430054:  95%|██████████▍| 224/237 [00:47<00:03,  3.99it/s]evaluate for the 24-th batch, evaluate loss: 0.4390406608581543:  58%|██████████▍       | 22/38 [00:01<00:01, 14.06it/s]evaluate for the 24-th batch, evaluate loss: 0.4390406608581543:  63%|███████████▎      | 24/38 [00:01<00:00, 14.45it/s]Epoch: 3, train for the 225-th batch, train loss: 0.5364259481430054:  95%|██████████▍| 225/237 [00:47<00:02,  4.07it/s]Epoch: 9, train for the 18-th batch, train loss: 0.3852897882461548:  14%|█▊           | 17/119 [00:02<00:14,  7.00it/s]Epoch: 9, train for the 18-th batch, train loss: 0.3852897882461548:  15%|█▉           | 18/119 [00:02<00:15,  6.50it/s]evaluate for the 25-th batch, evaluate loss: 0.47479715943336487:  63%|██████████▋      | 24/38 [00:01<00:00, 14.45it/s]Epoch: 2, train for the 113-th batch, train loss: 0.39722418785095215:  29%|██▉       | 112/383 [00:23<01:06,  4.09it/s]Epoch: 2, train for the 113-th batch, train loss: 0.39722418785095215:  30%|██▉       | 113/383 [00:23<01:04,  4.21it/s]Epoch: 4, train for the 218-th batch, train loss: 0.39136236906051636:  90%|█████████ | 217/241 [00:35<00:04,  5.76it/s]Epoch: 6, train for the 71-th batch, train loss: 0.5546314716339111:  46%|██████       | 70/151 [00:11<00:14,  5.52it/s]evaluate for the 26-th batch, evaluate loss: 0.45022153854370117:  63%|██████████▋      | 24/38 [00:01<00:00, 14.45it/s]evaluate for the 26-th batch, evaluate loss: 0.45022153854370117:  68%|███████████▋     | 26/38 [00:01<00:00, 14.77it/s]Epoch: 4, train for the 218-th batch, train loss: 0.39136236906051636:  90%|█████████ | 218/241 [00:35<00:04,  5.18it/s]Epoch: 6, train for the 71-th batch, train loss: 0.5546314716339111:  47%|██████       | 71/151 [00:11<00:15,  5.17it/s]Epoch: 9, train for the 19-th batch, train loss: 0.43904030323028564:  15%|█▊          | 18/119 [00:02<00:15,  6.50it/s]Epoch: 9, train for the 19-th batch, train loss: 0.43904030323028564:  16%|█▉          | 19/119 [00:02<00:14,  6.78it/s]evaluate for the 27-th batch, evaluate loss: 0.46842923760414124:  68%|███████████▋     | 26/38 [00:01<00:00, 14.77it/s]evaluate for the 28-th batch, evaluate loss: 0.4633961617946625:  68%|████████████▎     | 26/38 [00:02<00:00, 14.77it/s]evaluate for the 28-th batch, evaluate loss: 0.4633961617946625:  74%|█████████████▎    | 28/38 [00:02<00:00, 15.10it/s]Epoch: 2, train for the 114-th batch, train loss: 0.3674738109111786:  30%|███▏       | 113/383 [00:23<01:04,  4.21it/s]Epoch: 2, train for the 114-th batch, train loss: 0.3674738109111786:  30%|███▎       | 114/383 [00:23<00:59,  4.49it/s]Epoch: 3, train for the 226-th batch, train loss: 0.6066484451293945:  95%|██████████▍| 225/237 [00:47<00:02,  4.07it/s]Epoch: 6, train for the 72-th batch, train loss: 0.5128917098045349:  47%|██████       | 71/151 [00:11<00:15,  5.17it/s]Epoch: 4, train for the 219-th batch, train loss: 0.4379572868347168:  90%|█████████▉ | 218/241 [00:35<00:04,  5.18it/s]Epoch: 9, train for the 20-th batch, train loss: 0.4371652901172638:  16%|██           | 19/119 [00:03<00:14,  6.78it/s]evaluate for the 29-th batch, evaluate loss: 0.46910345554351807:  74%|████████████▌    | 28/38 [00:02<00:00, 15.10it/s]Epoch: 9, train for the 20-th batch, train loss: 0.4371652901172638:  17%|██▏          | 20/119 [00:03<00:15,  6.58it/s]Epoch: 6, train for the 72-th batch, train loss: 0.5128917098045349:  48%|██████▏      | 72/151 [00:11<00:15,  4.95it/s]Epoch: 4, train for the 219-th batch, train loss: 0.4379572868347168:  91%|█████████▉ | 219/241 [00:35<00:04,  4.84it/s]Epoch: 3, train for the 226-th batch, train loss: 0.6066484451293945:  95%|██████████▍| 226/237 [00:47<00:03,  3.55it/s]evaluate for the 30-th batch, evaluate loss: 0.4776872992515564:  74%|█████████████▎    | 28/38 [00:02<00:00, 15.10it/s]evaluate for the 30-th batch, evaluate loss: 0.4776872992515564:  79%|██████████████▏   | 30/38 [00:02<00:00, 13.80it/s]Epoch: 2, train for the 115-th batch, train loss: 0.3367205262184143:  30%|███▎       | 114/383 [00:23<00:59,  4.49it/s]Epoch: 9, train for the 21-th batch, train loss: 0.5434747934341431:  17%|██▏          | 20/119 [00:03<00:15,  6.58it/s]evaluate for the 31-th batch, evaluate loss: 0.4551648497581482:  79%|██████████████▏   | 30/38 [00:02<00:00, 13.80it/s]Epoch: 9, train for the 21-th batch, train loss: 0.5434747934341431:  18%|██▎          | 21/119 [00:03<00:15,  6.41it/s]Epoch: 6, train for the 73-th batch, train loss: 0.5153133869171143:  48%|██████▏      | 72/151 [00:11<00:15,  4.95it/s]Epoch: 2, train for the 115-th batch, train loss: 0.3367205262184143:  30%|███▎       | 115/383 [00:23<01:00,  4.45it/s]Epoch: 6, train for the 73-th batch, train loss: 0.5153133869171143:  48%|██████▎      | 73/151 [00:11<00:15,  5.14it/s]Epoch: 4, train for the 220-th batch, train loss: 0.5295107364654541:  91%|█████████▉ | 219/241 [00:35<00:04,  4.84it/s]evaluate for the 32-th batch, evaluate loss: 0.41950759291648865:  79%|█████████████▍   | 30/38 [00:02<00:00, 13.80it/s]evaluate for the 32-th batch, evaluate loss: 0.41950759291648865:  84%|██████████████▎  | 32/38 [00:02<00:00, 14.35it/s]Epoch: 3, train for the 227-th batch, train loss: 0.5533899068832397:  95%|██████████▍| 226/237 [00:48<00:03,  3.55it/s]Epoch: 4, train for the 220-th batch, train loss: 0.5295107364654541:  91%|██████████ | 220/241 [00:35<00:04,  4.81it/s]Epoch: 3, train for the 227-th batch, train loss: 0.5533899068832397:  96%|██████████▌| 227/237 [00:48<00:02,  3.70it/s]evaluate for the 33-th batch, evaluate loss: 0.480400413274765:  84%|████████████████   | 32/38 [00:02<00:00, 14.35it/s]Epoch: 9, train for the 22-th batch, train loss: 0.44705724716186523:  18%|██          | 21/119 [00:03<00:15,  6.41it/s]Epoch: 9, train for the 22-th batch, train loss: 0.44705724716186523:  18%|██▏         | 22/119 [00:03<00:15,  6.43it/s]Epoch: 6, train for the 74-th batch, train loss: 0.5263634324073792:  48%|██████▎      | 73/151 [00:12<00:15,  5.14it/s]Epoch: 6, train for the 74-th batch, train loss: 0.5263634324073792:  49%|██████▎      | 74/151 [00:12<00:14,  5.48it/s]evaluate for the 34-th batch, evaluate loss: 0.48245128989219666:  84%|██████████████▎  | 32/38 [00:02<00:00, 14.35it/s]evaluate for the 34-th batch, evaluate loss: 0.48245128989219666:  89%|███████████████▏ | 34/38 [00:02<00:00, 14.09it/s]Epoch: 4, train for the 221-th batch, train loss: 0.506670355796814:  91%|██████████▉ | 220/241 [00:35<00:04,  4.81it/s]Epoch: 2, train for the 116-th batch, train loss: 0.29553458094596863:  30%|███       | 115/383 [00:24<01:00,  4.45it/s]Epoch: 4, train for the 221-th batch, train loss: 0.506670355796814:  92%|███████████ | 221/241 [00:35<00:03,  5.07it/s]Epoch: 2, train for the 116-th batch, train loss: 0.29553458094596863:  30%|███       | 116/383 [00:24<01:01,  4.32it/s]evaluate for the 35-th batch, evaluate loss: 0.46663060784339905:  89%|███████████████▏ | 34/38 [00:02<00:00, 14.09it/s]Epoch: 9, train for the 23-th batch, train loss: 0.47135061025619507:  18%|██▏         | 22/119 [00:03<00:15,  6.43it/s]Epoch: 3, train for the 228-th batch, train loss: 0.5580916404724121:  96%|██████████▌| 227/237 [00:48<00:02,  3.70it/s]Epoch: 9, train for the 23-th batch, train loss: 0.47135061025619507:  19%|██▎         | 23/119 [00:03<00:14,  6.47it/s]Epoch: 6, train for the 75-th batch, train loss: 0.5220590233802795:  49%|██████▎      | 74/151 [00:12<00:14,  5.48it/s]Epoch: 6, train for the 75-th batch, train loss: 0.5220590233802795:  50%|██████▍      | 75/151 [00:12<00:13,  5.58it/s]evaluate for the 36-th batch, evaluate loss: 0.47792983055114746:  89%|███████████████▏ | 34/38 [00:02<00:00, 14.09it/s]evaluate for the 36-th batch, evaluate loss: 0.47792983055114746:  95%|████████████████ | 36/38 [00:02<00:00, 13.50it/s]Epoch: 3, train for the 228-th batch, train loss: 0.5580916404724121:  96%|██████████▌| 228/237 [00:48<00:02,  3.81it/s]Epoch: 4, train for the 222-th batch, train loss: 0.4863729774951935:  92%|██████████ | 221/241 [00:36<00:03,  5.07it/s]Epoch: 9, train for the 24-th batch, train loss: 0.44118016958236694:  19%|██▎         | 23/119 [00:03<00:14,  6.47it/s]Epoch: 9, train for the 24-th batch, train loss: 0.44118016958236694:  20%|██▍         | 24/119 [00:03<00:13,  6.86it/s]Epoch: 2, train for the 117-th batch, train loss: 0.3528815507888794:  30%|███▎       | 116/383 [00:24<01:01,  4.32it/s]Epoch: 4, train for the 222-th batch, train loss: 0.4863729774951935:  92%|██████████▏| 222/241 [00:36<00:03,  5.05it/s]Epoch: 2, train for the 117-th batch, train loss: 0.3528815507888794:  31%|███▎       | 117/383 [00:24<01:01,  4.32it/s]Epoch: 6, train for the 76-th batch, train loss: 0.5555354952812195:  50%|██████▍      | 75/151 [00:12<00:13,  5.58it/s]Epoch: 6, train for the 76-th batch, train loss: 0.5555354952812195:  50%|██████▌      | 76/151 [00:12<00:13,  5.58it/s]Epoch: 9, train for the 25-th batch, train loss: 0.4732113480567932:  20%|██▌          | 24/119 [00:03<00:13,  6.86it/s]evaluate for the 37-th batch, evaluate loss: 0.4339980185031891:  95%|█████████████████ | 36/38 [00:02<00:00, 13.50it/s]Epoch: 9, train for the 25-th batch, train loss: 0.4732113480567932:  21%|██▋          | 25/119 [00:03<00:12,  7.28it/s]Epoch: 4, train for the 223-th batch, train loss: 0.4961184561252594:  92%|██████████▏| 222/241 [00:36<00:03,  5.05it/s]Epoch: 4, train for the 223-th batch, train loss: 0.4961184561252594:  93%|██████████▏| 223/241 [00:36<00:03,  5.48it/s]evaluate for the 38-th batch, evaluate loss: 0.4575049579143524:  95%|█████████████████ | 36/38 [00:02<00:00, 13.50it/s]evaluate for the 38-th batch, evaluate loss: 0.4575049579143524: 100%|██████████████████| 38/38 [00:02<00:00, 10.79it/s]evaluate for the 38-th batch, evaluate loss: 0.4575049579143524: 100%|██████████████████| 38/38 [00:02<00:00, 13.12it/s]
Epoch: 2, train for the 118-th batch, train loss: 0.34419792890548706:  31%|███       | 117/383 [00:24<01:01,  4.32it/s]Epoch: 2, train for the 118-th batch, train loss: 0.34419792890548706:  31%|███       | 118/383 [00:24<00:58,  4.56it/s]Epoch: 6, train for the 77-th batch, train loss: 0.5358798503875732:  50%|██████▌      | 76/151 [00:12<00:13,  5.58it/s]Epoch: 9, train for the 26-th batch, train loss: 0.3484661877155304:  21%|██▋          | 25/119 [00:03<00:12,  7.28it/s]Epoch: 3, train for the 229-th batch, train loss: 0.6059793829917908:  96%|██████████▌| 228/237 [00:48<00:02,  3.81it/s]Epoch: 6, train for the 77-th batch, train loss: 0.5358798503875732:  51%|██████▋      | 77/151 [00:12<00:13,  5.46it/s]Epoch: 9, train for the 26-th batch, train loss: 0.3484661877155304:  22%|██▊          | 26/119 [00:03<00:13,  6.78it/s]Epoch: 4, train for the 224-th batch, train loss: 0.4890978932380676:  93%|██████████▏| 223/241 [00:36<00:03,  5.48it/s]Epoch: 4, train for the 224-th batch, train loss: 0.4890978932380676:  93%|██████████▏| 224/241 [00:36<00:02,  5.69it/s]Epoch: 3, train for the 229-th batch, train loss: 0.6059793829917908:  97%|██████████▋| 229/237 [00:48<00:02,  3.21it/s]  0%|                                                                                            | 0/20 [00:00<?, ?it/s]Epoch: 2, train for the 119-th batch, train loss: 0.39968734979629517:  31%|███       | 118/383 [00:24<00:58,  4.56it/s]Epoch: 6, train for the 78-th batch, train loss: 0.5038430094718933:  51%|██████▋      | 77/151 [00:12<00:13,  5.46it/s]Epoch: 9, train for the 27-th batch, train loss: 0.45488035678863525:  22%|██▌         | 26/119 [00:04<00:13,  6.78it/s]Epoch: 6, train for the 78-th batch, train loss: 0.5038430094718933:  52%|██████▋      | 78/151 [00:12<00:12,  5.71it/s]Epoch: 9, train for the 27-th batch, train loss: 0.45488035678863525:  23%|██▋         | 27/119 [00:04<00:13,  6.65it/s]Epoch: 2, train for the 119-th batch, train loss: 0.39968734979629517:  31%|███       | 119/383 [00:24<00:56,  4.67it/s]evaluate for the 1-th batch, evaluate loss: 0.7298976182937622:   0%|                            | 0/20 [00:00<?, ?it/s]Epoch: 4, train for the 225-th batch, train loss: 0.5160595178604126:  93%|██████████▏| 224/241 [00:36<00:02,  5.69it/s]Epoch: 4, train for the 225-th batch, train loss: 0.5160595178604126:  93%|██████████▎| 225/241 [00:36<00:02,  5.58it/s]evaluate for the 2-th batch, evaluate loss: 0.7566735148429871:   0%|                            | 0/20 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.7566735148429871:  10%|██                  | 2/20 [00:00<00:01, 12.94it/s]Epoch: 9, train for the 28-th batch, train loss: 0.4127381443977356:  23%|██▉          | 27/119 [00:04<00:13,  6.65it/s]Epoch: 3, train for the 230-th batch, train loss: 0.5450155735015869:  97%|██████████▋| 229/237 [00:49<00:02,  3.21it/s]Epoch: 9, train for the 28-th batch, train loss: 0.4127381443977356:  24%|███          | 28/119 [00:04<00:12,  7.13it/s]Epoch: 6, train for the 79-th batch, train loss: 0.5627571940422058:  52%|██████▋      | 78/151 [00:12<00:12,  5.71it/s]Epoch: 3, train for the 230-th batch, train loss: 0.5450155735015869:  97%|██████████▋| 230/237 [00:49<00:02,  3.37it/s]evaluate for the 3-th batch, evaluate loss: 0.625415027141571:  10%|██                   | 2/20 [00:00<00:01, 12.94it/s]Epoch: 6, train for the 79-th batch, train loss: 0.5627571940422058:  52%|██████▊      | 79/151 [00:12<00:12,  5.63it/s]Epoch: 4, train for the 226-th batch, train loss: 0.5213528871536255:  93%|██████████▎| 225/241 [00:36<00:02,  5.58it/s]Epoch: 2, train for the 120-th batch, train loss: 0.44714635610580444:  31%|███       | 119/383 [00:25<00:56,  4.67it/s]Epoch: 4, train for the 226-th batch, train loss: 0.5213528871536255:  94%|██████████▎| 226/241 [00:36<00:02,  5.58it/s]Epoch: 2, train for the 120-th batch, train loss: 0.44714635610580444:  31%|███▏      | 120/383 [00:25<01:00,  4.36it/s]Epoch: 9, train for the 29-th batch, train loss: 0.41479742527008057:  24%|██▊         | 28/119 [00:04<00:12,  7.13it/s]Epoch: 9, train for the 29-th batch, train loss: 0.41479742527008057:  24%|██▉         | 29/119 [00:04<00:13,  6.91it/s]evaluate for the 4-th batch, evaluate loss: 0.6719728112220764:  10%|██                  | 2/20 [00:00<00:01, 12.94it/s]evaluate for the 4-th batch, evaluate loss: 0.6719728112220764:  20%|████                | 4/20 [00:00<00:01, 11.24it/s]Epoch: 6, train for the 80-th batch, train loss: 0.51326584815979:  52%|███████▊       | 79/151 [00:13<00:12,  5.63it/s]Epoch: 6, train for the 80-th batch, train loss: 0.51326584815979:  53%|███████▉       | 80/151 [00:13<00:12,  5.79it/s]evaluate for the 5-th batch, evaluate loss: 0.6622483730316162:  20%|████                | 4/20 [00:00<00:01, 11.24it/s]Epoch: 3, train for the 231-th batch, train loss: 0.5783511400222778:  97%|██████████▋| 230/237 [00:49<00:02,  3.37it/s]Epoch: 9, train for the 30-th batch, train loss: 0.4333251118659973:  24%|███▏         | 29/119 [00:04<00:13,  6.91it/s]Epoch: 9, train for the 30-th batch, train loss: 0.4333251118659973:  25%|███▎         | 30/119 [00:04<00:12,  7.30it/s]Epoch: 4, train for the 227-th batch, train loss: 0.5571001172065735:  94%|██████████▎| 226/241 [00:37<00:02,  5.58it/s]evaluate for the 6-th batch, evaluate loss: 0.7403435111045837:  20%|████                | 4/20 [00:00<00:01, 11.24it/s]evaluate for the 6-th batch, evaluate loss: 0.7403435111045837:  30%|██████              | 6/20 [00:00<00:01, 12.37it/s]Epoch: 4, train for the 227-th batch, train loss: 0.5571001172065735:  94%|██████████▎| 227/241 [00:37<00:02,  5.44it/s]Epoch: 3, train for the 231-th batch, train loss: 0.5783511400222778:  97%|██████████▋| 231/237 [00:49<00:01,  3.43it/s]Epoch: 2, train for the 121-th batch, train loss: 0.44486746191978455:  31%|███▏      | 120/383 [00:25<01:00,  4.36it/s]Epoch: 6, train for the 81-th batch, train loss: 0.521274209022522:  53%|███████▍      | 80/151 [00:13<00:12,  5.79it/s]Epoch: 6, train for the 81-th batch, train loss: 0.521274209022522:  54%|███████▌      | 81/151 [00:13<00:11,  6.10it/s]evaluate for the 7-th batch, evaluate loss: 0.7414385080337524:  30%|██████              | 6/20 [00:00<00:01, 12.37it/s]Epoch: 2, train for the 121-th batch, train loss: 0.44486746191978455:  32%|███▏      | 121/383 [00:25<01:00,  4.30it/s]Epoch: 9, train for the 31-th batch, train loss: 0.3938462734222412:  25%|███▎         | 30/119 [00:04<00:12,  7.30it/s]Epoch: 9, train for the 31-th batch, train loss: 0.3938462734222412:  26%|███▍         | 31/119 [00:04<00:12,  7.16it/s]Epoch: 4, train for the 228-th batch, train loss: 0.48310744762420654:  94%|█████████▍| 227/241 [00:37<00:02,  5.44it/s]evaluate for the 8-th batch, evaluate loss: 0.6832897663116455:  30%|██████              | 6/20 [00:00<00:01, 12.37it/s]evaluate for the 8-th batch, evaluate loss: 0.6832897663116455:  40%|████████            | 8/20 [00:00<00:00, 12.32it/s]Epoch: 4, train for the 228-th batch, train loss: 0.48310744762420654:  95%|█████████▍| 228/241 [00:37<00:02,  5.61it/s]Epoch: 6, train for the 82-th batch, train loss: 0.5146985650062561:  54%|██████▉      | 81/151 [00:13<00:11,  6.10it/s]Epoch: 6, train for the 82-th batch, train loss: 0.5146985650062561:  54%|███████      | 82/151 [00:13<00:11,  5.99it/s]evaluate for the 9-th batch, evaluate loss: 0.6427134275436401:  40%|████████            | 8/20 [00:00<00:00, 12.32it/s]Epoch: 9, train for the 32-th batch, train loss: 0.4320637285709381:  26%|███▍         | 31/119 [00:04<00:12,  7.16it/s]Epoch: 3, train for the 232-th batch, train loss: 0.583347737789154:  97%|███████████▋| 231/237 [00:49<00:01,  3.43it/s]Epoch: 9, train for the 32-th batch, train loss: 0.4320637285709381:  27%|███▍         | 32/119 [00:04<00:12,  6.76it/s]Epoch: 2, train for the 122-th batch, train loss: 0.5311775207519531:  32%|███▍       | 121/383 [00:25<01:00,  4.30it/s]evaluate for the 10-th batch, evaluate loss: 0.6300311088562012:  40%|███████▌           | 8/20 [00:00<00:00, 12.32it/s]evaluate for the 10-th batch, evaluate loss: 0.6300311088562012:  50%|█████████         | 10/20 [00:00<00:00, 12.87it/s]Epoch: 4, train for the 229-th batch, train loss: 0.45776039361953735:  95%|█████████▍| 228/241 [00:37<00:02,  5.61it/s]Epoch: 2, train for the 122-th batch, train loss: 0.5311775207519531:  32%|███▌       | 122/383 [00:25<01:00,  4.29it/s]Epoch: 3, train for the 232-th batch, train loss: 0.583347737789154:  98%|███████████▋| 232/237 [00:49<00:01,  3.42it/s]Epoch: 4, train for the 229-th batch, train loss: 0.45776039361953735:  95%|█████████▌| 229/241 [00:37<00:02,  5.87it/s]evaluate for the 11-th batch, evaluate loss: 0.6348673105239868:  50%|█████████         | 10/20 [00:00<00:00, 12.87it/s]Epoch: 6, train for the 83-th batch, train loss: 0.512321412563324:  54%|███████▌      | 82/151 [00:13<00:11,  5.99it/s]Epoch: 6, train for the 83-th batch, train loss: 0.512321412563324:  55%|███████▋      | 83/151 [00:13<00:11,  5.76it/s]Epoch: 9, train for the 33-th batch, train loss: 0.42011940479278564:  27%|███▏        | 32/119 [00:04<00:12,  6.76it/s]Epoch: 9, train for the 33-th batch, train loss: 0.42011940479278564:  28%|███▎        | 33/119 [00:04<00:13,  6.51it/s]evaluate for the 12-th batch, evaluate loss: 0.6882447600364685:  50%|█████████         | 10/20 [00:00<00:00, 12.87it/s]evaluate for the 12-th batch, evaluate loss: 0.6882447600364685:  60%|██████████▊       | 12/20 [00:00<00:00, 13.02it/s]Epoch: 4, train for the 230-th batch, train loss: 0.42584291100502014:  95%|█████████▌| 229/241 [00:37<00:02,  5.87it/s]Epoch: 4, train for the 230-th batch, train loss: 0.42584291100502014:  95%|█████████▌| 230/241 [00:37<00:01,  6.03it/s]evaluate for the 13-th batch, evaluate loss: 0.7025912404060364:  60%|██████████▊       | 12/20 [00:01<00:00, 13.02it/s]Epoch: 3, train for the 233-th batch, train loss: 0.6051267385482788:  98%|██████████▊| 232/237 [00:49<00:01,  3.42it/s]Epoch: 9, train for the 34-th batch, train loss: 0.37934038043022156:  28%|███▎        | 33/119 [00:05<00:13,  6.51it/s]Epoch: 6, train for the 84-th batch, train loss: 0.5113206505775452:  55%|███████▏     | 83/151 [00:13<00:11,  5.76it/s]Epoch: 2, train for the 123-th batch, train loss: 0.38325420022010803:  32%|███▏      | 122/383 [00:25<01:00,  4.29it/s]Epoch: 9, train for the 34-th batch, train loss: 0.37934038043022156:  29%|███▍        | 34/119 [00:05<00:12,  6.90it/s]Epoch: 3, train for the 233-th batch, train loss: 0.6051267385482788:  98%|██████████▊| 233/237 [00:50<00:01,  3.53it/s]evaluate for the 14-th batch, evaluate loss: 0.7138122320175171:  60%|██████████▊       | 12/20 [00:01<00:00, 13.02it/s]evaluate for the 14-th batch, evaluate loss: 0.7138122320175171:  70%|████████████▌     | 14/20 [00:01<00:00, 13.58it/s]Epoch: 6, train for the 84-th batch, train loss: 0.5113206505775452:  56%|███████▏     | 84/151 [00:13<00:11,  5.85it/s]Epoch: 2, train for the 123-th batch, train loss: 0.38325420022010803:  32%|███▏      | 123/383 [00:25<01:05,  3.99it/s]Epoch: 4, train for the 231-th batch, train loss: 0.5050144791603088:  95%|██████████▍| 230/241 [00:37<00:01,  6.03it/s]Epoch: 4, train for the 231-th batch, train loss: 0.5050144791603088:  96%|██████████▌| 231/241 [00:37<00:01,  6.02it/s]evaluate for the 15-th batch, evaluate loss: 0.7150012254714966:  70%|████████████▌     | 14/20 [00:01<00:00, 13.58it/s]Epoch: 9, train for the 35-th batch, train loss: 0.3903759717941284:  29%|███▋         | 34/119 [00:05<00:12,  6.90it/s]Epoch: 9, train for the 35-th batch, train loss: 0.3903759717941284:  29%|███▊         | 35/119 [00:05<00:12,  6.68it/s]evaluate for the 16-th batch, evaluate loss: 0.6490432620048523:  70%|████████████▌     | 14/20 [00:01<00:00, 13.58it/s]evaluate for the 16-th batch, evaluate loss: 0.6490432620048523:  80%|██████████████▍   | 16/20 [00:01<00:00, 13.39it/s]Epoch: 6, train for the 85-th batch, train loss: 0.49221381545066833:  56%|██████▋     | 84/151 [00:13<00:11,  5.85it/s]Epoch: 6, train for the 85-th batch, train loss: 0.49221381545066833:  56%|██████▊     | 85/151 [00:13<00:11,  5.79it/s]Epoch: 3, train for the 234-th batch, train loss: 0.6022936701774597:  98%|██████████▊| 233/237 [00:50<00:01,  3.53it/s]evaluate for the 17-th batch, evaluate loss: 0.7136151194572449:  80%|██████████████▍   | 16/20 [00:01<00:00, 13.39it/s]Epoch: 4, train for the 232-th batch, train loss: 0.5438091158866882:  96%|██████████▌| 231/241 [00:37<00:01,  6.02it/s]Epoch: 2, train for the 124-th batch, train loss: 0.35612016916275024:  32%|███▏      | 123/383 [00:26<01:05,  3.99it/s]Epoch: 9, train for the 36-th batch, train loss: 0.39112159609794617:  29%|███▌        | 35/119 [00:05<00:12,  6.68it/s]Epoch: 3, train for the 234-th batch, train loss: 0.6022936701774597:  99%|██████████▊| 234/237 [00:50<00:00,  3.63it/s]Epoch: 9, train for the 36-th batch, train loss: 0.39112159609794617:  30%|███▋        | 36/119 [00:05<00:11,  7.27it/s]Epoch: 4, train for the 232-th batch, train loss: 0.5438091158866882:  96%|██████████▌| 232/241 [00:37<00:01,  5.64it/s]evaluate for the 18-th batch, evaluate loss: 0.6674813628196716:  80%|██████████████▍   | 16/20 [00:01<00:00, 13.39it/s]evaluate for the 18-th batch, evaluate loss: 0.6674813628196716:  90%|████████████████▏ | 18/20 [00:01<00:00, 14.35it/s]Epoch: 2, train for the 124-th batch, train loss: 0.35612016916275024:  32%|███▏      | 124/383 [00:26<01:05,  3.95it/s]Epoch: 6, train for the 86-th batch, train loss: 0.5204572081565857:  56%|███████▎     | 85/151 [00:14<00:11,  5.79it/s]Epoch: 6, train for the 86-th batch, train loss: 0.5204572081565857:  57%|███████▍     | 86/151 [00:14<00:10,  6.11it/s]evaluate for the 19-th batch, evaluate loss: 0.7293158769607544:  90%|████████████████▏ | 18/20 [00:01<00:00, 14.35it/s]evaluate for the 20-th batch, evaluate loss: 0.6958028078079224:  90%|████████████████▏ | 18/20 [00:01<00:00, 14.35it/s]evaluate for the 20-th batch, evaluate loss: 0.6958028078079224: 100%|██████████████████| 20/20 [00:01<00:00, 15.27it/s]evaluate for the 20-th batch, evaluate loss: 0.6958028078079224: 100%|██████████████████| 20/20 [00:01<00:00, 13.65it/s]
Epoch: 9, train for the 37-th batch, train loss: 0.434762567281723:  30%|████▏         | 36/119 [00:05<00:11,  7.27it/s]Epoch: 9, train for the 37-th batch, train loss: 0.434762567281723:  31%|████▎         | 37/119 [00:05<00:12,  6.67it/s]Epoch: 4, train for the 233-th batch, train loss: 0.49372702836990356:  96%|█████████▋| 232/241 [00:38<00:01,  5.64it/s]Epoch: 3, train for the 235-th batch, train loss: 0.5618758797645569:  99%|██████████▊| 234/237 [00:50<00:00,  3.63it/s]Epoch: 4, train for the 233-th batch, train loss: 0.49372702836990356:  97%|█████████▋| 233/241 [00:38<00:01,  5.31it/s]Epoch: 6, train for the 87-th batch, train loss: 0.48865944147109985:  57%|██████▊     | 86/151 [00:14<00:10,  6.11it/s]Epoch: 3, train for the 235-th batch, train loss: 0.5618758797645569:  99%|██████████▉| 235/237 [00:50<00:00,  3.73it/s]Epoch: 2, train for the 125-th batch, train loss: 0.4045264720916748:  32%|███▌       | 124/383 [00:26<01:05,  3.95it/s]Epoch: 6, train for the 87-th batch, train loss: 0.48865944147109985:  58%|██████▉     | 87/151 [00:14<00:10,  5.92it/s]Epoch: 2, train for the 125-th batch, train loss: 0.4045264720916748:  33%|███▌       | 125/383 [00:26<01:05,  3.93it/s]Epoch: 9, train for the 38-th batch, train loss: 0.38452181220054626:  31%|███▋        | 37/119 [00:05<00:12,  6.67it/s]Epoch: 9, train for the 38-th batch, train loss: 0.38452181220054626:  32%|███▊        | 38/119 [00:05<00:11,  6.80it/s]Epoch: 4, train for the 234-th batch, train loss: 0.5833612680435181:  97%|██████████▋| 233/241 [00:38<00:01,  5.31it/s]Epoch: 4, train for the 234-th batch, train loss: 0.5833612680435181:  97%|██████████▋| 234/241 [00:38<00:01,  5.51it/s]Epoch: 6, train for the 88-th batch, train loss: 0.5475940108299255:  58%|███████▍     | 87/151 [00:14<00:10,  5.92it/s]INFO:root:Epoch: 7, learning rate: 0.0001, train loss: 0.4978
INFO:root:train average_precision, 0.8511
INFO:root:train roc_auc, 0.8408
INFO:root:validate loss: 0.4617
INFO:root:validate average_precision, 0.8668
INFO:root:validate roc_auc, 0.8624
INFO:root:new node validate loss: 0.6897
INFO:root:new node validate first_1_average_precision, 0.5778
INFO:root:new node validate first_1_roc_auc, 0.5183
INFO:root:new node validate first_3_average_precision, 0.6131
INFO:root:new node validate first_3_roc_auc, 0.5697
INFO:root:new node validate first_10_average_precision, 0.6415
INFO:root:new node validate first_10_roc_auc, 0.6263
INFO:root:new node validate average_precision, 0.6901
INFO:root:new node validate roc_auc, 0.6816
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/146 [00:00<?, ?it/s]Epoch: 6, train for the 88-th batch, train loss: 0.5475940108299255:  58%|███████▌     | 88/151 [00:14<00:10,  5.95it/s]Epoch: 9, train for the 39-th batch, train loss: 0.41922658681869507:  32%|███▊        | 38/119 [00:05<00:11,  6.80it/s]Epoch: 9, train for the 39-th batch, train loss: 0.41922658681869507:  33%|███▉        | 39/119 [00:05<00:11,  7.16it/s]Epoch: 3, train for the 236-th batch, train loss: 0.5587341785430908:  99%|██████████▉| 235/237 [00:50<00:00,  3.73it/s]Epoch: 2, train for the 126-th batch, train loss: 0.3447669446468353:  33%|███▌       | 125/383 [00:26<01:05,  3.93it/s]Epoch: 3, train for the 236-th batch, train loss: 0.5587341785430908: 100%|██████████▉| 236/237 [00:50<00:00,  3.86it/s]Epoch: 8, train for the 1-th batch, train loss: 0.562046468257904:   0%|                        | 0/146 [00:00<?, ?it/s]Epoch: 2, train for the 126-th batch, train loss: 0.3447669446468353:  33%|███▌       | 126/383 [00:26<01:03,  4.04it/s]Epoch: 4, train for the 235-th batch, train loss: 0.6049840450286865:  97%|██████████▋| 234/241 [00:38<00:01,  5.51it/s]Epoch: 4, train for the 235-th batch, train loss: 0.6049840450286865:  98%|██████████▋| 235/241 [00:38<00:01,  5.70it/s]Epoch: 6, train for the 89-th batch, train loss: 0.5552275776863098:  58%|███████▌     | 88/151 [00:14<00:10,  5.95it/s]Epoch: 9, train for the 40-th batch, train loss: 0.4014798700809479:  33%|████▎        | 39/119 [00:05<00:11,  7.16it/s]Epoch: 6, train for the 89-th batch, train loss: 0.5552275776863098:  59%|███████▋     | 89/151 [00:14<00:10,  5.90it/s]Epoch: 9, train for the 40-th batch, train loss: 0.4014798700809479:  34%|████▎        | 40/119 [00:05<00:11,  6.96it/s]Epoch: 8, train for the 2-th batch, train loss: 0.46931105852127075:   0%|                      | 0/146 [00:00<?, ?it/s]Epoch: 8, train for the 2-th batch, train loss: 0.46931105852127075:   1%|▏             | 2/146 [00:00<00:15,  9.43it/s]Epoch: 3, train for the 237-th batch, train loss: 0.5886075496673584: 100%|██████████▉| 236/237 [00:50<00:00,  3.86it/s]Epoch: 2, train for the 127-th batch, train loss: 0.39229899644851685:  33%|███▎      | 126/383 [00:26<01:03,  4.04it/s]Epoch: 3, train for the 237-th batch, train loss: 0.5886075496673584: 100%|███████████| 237/237 [00:50<00:00,  3.99it/s]Epoch: 3, train for the 237-th batch, train loss: 0.5886075496673584: 100%|███████████| 237/237 [00:50<00:00,  4.65it/s]
Epoch: 4, train for the 236-th batch, train loss: 0.5542183518409729:  98%|██████████▋| 235/241 [00:38<00:01,  5.70it/s]Epoch: 2, train for the 127-th batch, train loss: 0.39229899644851685:  33%|███▎      | 127/383 [00:26<01:01,  4.18it/s]Epoch: 4, train for the 236-th batch, train loss: 0.5542183518409729:  98%|██████████▊| 236/241 [00:38<00:00,  5.59it/s]Epoch: 6, train for the 90-th batch, train loss: 0.49534517526626587:  59%|███████     | 89/151 [00:14<00:10,  5.90it/s]Epoch: 9, train for the 41-th batch, train loss: 0.46572959423065186:  34%|████        | 40/119 [00:06<00:11,  6.96it/s]Epoch: 9, train for the 41-th batch, train loss: 0.46572959423065186:  34%|████▏       | 41/119 [00:06<00:11,  6.62it/s]Epoch: 6, train for the 90-th batch, train loss: 0.49534517526626587:  60%|███████▏    | 90/151 [00:14<00:10,  5.82it/s]Epoch: 8, train for the 3-th batch, train loss: 0.4930025041103363:   1%|▏              | 2/146 [00:00<00:15,  9.43it/s]Epoch: 8, train for the 3-th batch, train loss: 0.4930025041103363:   2%|▎              | 3/146 [00:00<00:17,  8.23it/s]Epoch: 4, train for the 237-th batch, train loss: 0.5113430023193359:  98%|██████████▊| 236/241 [00:38<00:00,  5.59it/s]Epoch: 8, train for the 4-th batch, train loss: 0.48156583309173584:   2%|▎             | 3/146 [00:00<00:17,  8.23it/s]Epoch: 2, train for the 128-th batch, train loss: 0.456184983253479:  33%|███▉        | 127/383 [00:27<01:01,  4.18it/s]Epoch: 8, train for the 4-th batch, train loss: 0.48156583309173584:   3%|▍             | 4/146 [00:00<00:18,  7.80it/s]Epoch: 4, train for the 237-th batch, train loss: 0.5113430023193359:  98%|██████████▊| 237/241 [00:38<00:00,  5.64it/s]Epoch: 6, train for the 91-th batch, train loss: 0.45272940397262573:  60%|███████▏    | 90/151 [00:14<00:10,  5.82it/s]Epoch: 9, train for the 42-th batch, train loss: 0.40368834137916565:  34%|████▏       | 41/119 [00:06<00:11,  6.62it/s]Epoch: 6, train for the 91-th batch, train loss: 0.45272940397262573:  60%|███████▏    | 91/151 [00:15<00:10,  5.79it/s]Epoch: 2, train for the 128-th batch, train loss: 0.456184983253479:  33%|████        | 128/383 [00:27<00:58,  4.36it/s]Epoch: 9, train for the 42-th batch, train loss: 0.40368834137916565:  35%|████▏       | 42/119 [00:06<00:12,  6.25it/s]Epoch: 8, train for the 5-th batch, train loss: 0.5129069685935974:   3%|▍              | 4/146 [00:00<00:18,  7.80it/s]Epoch: 8, train for the 5-th batch, train loss: 0.5129069685935974:   3%|▌              | 5/146 [00:00<00:17,  8.10it/s]Epoch: 4, train for the 238-th batch, train loss: 0.5103425979614258:  98%|██████████▊| 237/241 [00:38<00:00,  5.64it/s]Epoch: 4, train for the 238-th batch, train loss: 0.5103425979614258:  99%|██████████▊| 238/241 [00:38<00:00,  5.81it/s]Epoch: 6, train for the 92-th batch, train loss: 0.5420716404914856:  60%|███████▊     | 91/151 [00:15<00:10,  5.79it/s]Epoch: 9, train for the 43-th batch, train loss: 0.4237269461154938:  35%|████▌        | 42/119 [00:06<00:12,  6.25it/s]Epoch: 6, train for the 92-th batch, train loss: 0.5420716404914856:  61%|███████▉     | 92/151 [00:15<00:09,  5.94it/s]Epoch: 9, train for the 43-th batch, train loss: 0.4237269461154938:  36%|████▋        | 43/119 [00:06<00:12,  6.24it/s]Epoch: 8, train for the 6-th batch, train loss: 0.5257319808006287:   3%|▌              | 5/146 [00:00<00:17,  8.10it/s]Epoch: 8, train for the 6-th batch, train loss: 0.5257319808006287:   4%|▌              | 6/146 [00:00<00:16,  8.38it/s]  0%|                                                                                            | 0/66 [00:00<?, ?it/s]Epoch: 2, train for the 129-th batch, train loss: 0.3136906027793884:  33%|███▋       | 128/383 [00:27<00:58,  4.36it/s]Epoch: 2, train for the 129-th batch, train loss: 0.3136906027793884:  34%|███▋       | 129/383 [00:27<00:59,  4.24it/s]Epoch: 4, train for the 239-th batch, train loss: 0.5825319290161133:  99%|██████████▊| 238/241 [00:39<00:00,  5.81it/s]Epoch: 9, train for the 44-th batch, train loss: 0.3829982578754425:  36%|████▋        | 43/119 [00:06<00:12,  6.24it/s]Epoch: 9, train for the 44-th batch, train loss: 0.3829982578754425:  37%|████▊        | 44/119 [00:06<00:11,  6.58it/s]evaluate for the 1-th batch, evaluate loss: 0.5702000856399536:   0%|                            | 0/66 [00:00<?, ?it/s]Epoch: 4, train for the 239-th batch, train loss: 0.5825319290161133:  99%|██████████▉| 239/241 [00:39<00:00,  5.77it/s]Epoch: 6, train for the 93-th batch, train loss: 0.5350514650344849:  61%|███████▉     | 92/151 [00:15<00:09,  5.94it/s]Epoch: 8, train for the 7-th batch, train loss: 0.4944418668746948:   4%|▌              | 6/146 [00:00<00:16,  8.38it/s]Epoch: 8, train for the 7-th batch, train loss: 0.4944418668746948:   5%|▋              | 7/146 [00:00<00:17,  7.87it/s]Epoch: 6, train for the 93-th batch, train loss: 0.5350514650344849:  62%|████████     | 93/151 [00:15<00:10,  5.73it/s]Epoch: 9, train for the 45-th batch, train loss: 0.3584960997104645:  37%|████▊        | 44/119 [00:06<00:11,  6.58it/s]Epoch: 9, train for the 45-th batch, train loss: 0.3584960997104645:  38%|████▉        | 45/119 [00:06<00:10,  6.74it/s]Epoch: 2, train for the 130-th batch, train loss: 0.27388182282447815:  34%|███▎      | 129/383 [00:27<00:59,  4.24it/s]evaluate for the 2-th batch, evaluate loss: 0.5115875005722046:   0%|                            | 0/66 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5115875005722046:   3%|▌                   | 2/66 [00:00<00:08,  7.87it/s]Epoch: 8, train for the 8-th batch, train loss: 0.44714176654815674:   5%|▋             | 7/146 [00:00<00:17,  7.87it/s]Epoch: 4, train for the 240-th batch, train loss: 0.4460238814353943:  99%|██████████▉| 239/241 [00:39<00:00,  5.77it/s]Epoch: 8, train for the 8-th batch, train loss: 0.44714176654815674:   5%|▊             | 8/146 [00:00<00:17,  7.75it/s]Epoch: 2, train for the 130-th batch, train loss: 0.27388182282447815:  34%|███▍      | 130/383 [00:27<00:59,  4.24it/s]Epoch: 4, train for the 240-th batch, train loss: 0.4460238814353943: 100%|██████████▉| 240/241 [00:39<00:00,  5.63it/s]Epoch: 6, train for the 94-th batch, train loss: 0.4858827590942383:  62%|████████     | 93/151 [00:15<00:10,  5.73it/s]Epoch: 6, train for the 94-th batch, train loss: 0.4858827590942383:  62%|████████     | 94/151 [00:15<00:09,  5.70it/s]evaluate for the 3-th batch, evaluate loss: 0.5528247952461243:   3%|▌                   | 2/66 [00:00<00:08,  7.87it/s]evaluate for the 3-th batch, evaluate loss: 0.5528247952461243:   5%|▉                   | 3/66 [00:00<00:07,  8.04it/s]Epoch: 9, train for the 46-th batch, train loss: 0.37746068835258484:  38%|████▌       | 45/119 [00:06<00:10,  6.74it/s]Epoch: 9, train for the 46-th batch, train loss: 0.37746068835258484:  39%|████▋       | 46/119 [00:06<00:11,  6.60it/s]Epoch: 8, train for the 9-th batch, train loss: 0.4307669699192047:   5%|▊              | 8/146 [00:01<00:17,  7.75it/s]Epoch: 8, train for the 9-th batch, train loss: 0.4307669699192047:   6%|▉              | 9/146 [00:01<00:18,  7.60it/s]Epoch: 4, train for the 241-th batch, train loss: 0.5048136711120605: 100%|██████████▉| 240/241 [00:39<00:00,  5.63it/s]Epoch: 2, train for the 131-th batch, train loss: 0.33516815304756165:  34%|███▍      | 130/383 [00:27<00:59,  4.24it/s]evaluate for the 4-th batch, evaluate loss: 0.5348517298698425:   5%|▉                   | 3/66 [00:00<00:07,  8.04it/s]evaluate for the 4-th batch, evaluate loss: 0.5348517298698425:   6%|█▏                  | 4/66 [00:00<00:07,  8.40it/s]Epoch: 4, train for the 241-th batch, train loss: 0.5048136711120605: 100%|███████████| 241/241 [00:39<00:00,  5.44it/s]Epoch: 4, train for the 241-th batch, train loss: 0.5048136711120605: 100%|███████████| 241/241 [00:39<00:00,  6.10it/s]
Epoch: 6, train for the 95-th batch, train loss: 0.498462975025177:  62%|████████▋     | 94/151 [00:15<00:09,  5.70it/s]Epoch: 2, train for the 131-th batch, train loss: 0.33516815304756165:  34%|███▍      | 131/383 [00:27<00:58,  4.28it/s]Epoch: 6, train for the 95-th batch, train loss: 0.498462975025177:  63%|████████▊     | 95/151 [00:15<00:10,  5.53it/s]Epoch: 8, train for the 10-th batch, train loss: 0.4337635338306427:   6%|▊             | 9/146 [00:01<00:18,  7.60it/s]Epoch: 8, train for the 10-th batch, train loss: 0.4337635338306427:   7%|▉            | 10/146 [00:01<00:18,  7.50it/s]Epoch: 9, train for the 47-th batch, train loss: 0.4531453549861908:  39%|█████        | 46/119 [00:07<00:11,  6.60it/s]Epoch: 9, train for the 47-th batch, train loss: 0.4531453549861908:  39%|█████▏       | 47/119 [00:07<00:11,  6.29it/s]evaluate for the 5-th batch, evaluate loss: 0.5487111806869507:   6%|█▏                  | 4/66 [00:00<00:07,  8.40it/s]evaluate for the 6-th batch, evaluate loss: 0.5745446085929871:   6%|█▏                  | 4/66 [00:00<00:07,  8.40it/s]evaluate for the 6-th batch, evaluate loss: 0.5745446085929871:   9%|█▊                  | 6/66 [00:00<00:06,  9.72it/s]Epoch: 6, train for the 96-th batch, train loss: 0.5033037066459656:  63%|████████▏    | 95/151 [00:15<00:10,  5.53it/s]Epoch: 6, train for the 96-th batch, train loss: 0.5033037066459656:  64%|████████▎    | 96/151 [00:15<00:09,  5.65it/s]Epoch: 8, train for the 11-th batch, train loss: 0.46220991015434265:   7%|▊           | 10/146 [00:01<00:18,  7.50it/s]Epoch: 8, train for the 11-th batch, train loss: 0.46220991015434265:   8%|▉           | 11/146 [00:01<00:18,  7.15it/s]Epoch: 2, train for the 132-th batch, train loss: 0.5504139065742493:  34%|███▊       | 131/383 [00:28<00:58,  4.28it/s]Epoch: 9, train for the 48-th batch, train loss: 0.4144956171512604:  39%|█████▏       | 47/119 [00:07<00:11,  6.29it/s]evaluate for the 7-th batch, evaluate loss: 0.544312596321106:   9%|█▉                   | 6/66 [00:00<00:06,  9.72it/s]evaluate for the 7-th batch, evaluate loss: 0.544312596321106:  11%|██▏                  | 7/66 [00:00<00:06,  8.65it/s]  0%|                                                                                            | 0/72 [00:00<?, ?it/s]Epoch: 6, train for the 97-th batch, train loss: 0.5414437651634216:  64%|████████▎    | 96/151 [00:16<00:09,  5.65it/s]Epoch: 9, train for the 48-th batch, train loss: 0.4144956171512604:  40%|█████▏       | 48/119 [00:07<00:13,  5.37it/s]Epoch: 8, train for the 12-th batch, train loss: 0.4480324387550354:   8%|▉            | 11/146 [00:01<00:18,  7.15it/s]Epoch: 8, train for the 12-th batch, train loss: 0.4480324387550354:   8%|█            | 12/146 [00:01<00:18,  7.16it/s]Epoch: 2, train for the 132-th batch, train loss: 0.5504139065742493:  34%|███▊       | 132/383 [00:28<01:06,  3.80it/s]Epoch: 6, train for the 97-th batch, train loss: 0.5414437651634216:  64%|████████▎    | 97/151 [00:16<00:09,  5.71it/s]evaluate for the 1-th batch, evaluate loss: 0.5159847736358643:   0%|                            | 0/72 [00:00<?, ?it/s]evaluate for the 8-th batch, evaluate loss: 0.52665776014328:  11%|██▎                   | 7/66 [00:00<00:06,  8.65it/s]evaluate for the 8-th batch, evaluate loss: 0.52665776014328:  12%|██▋                   | 8/66 [00:00<00:06,  8.64it/s]evaluate for the 2-th batch, evaluate loss: 0.5261129140853882:   0%|                            | 0/72 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5261129140853882:   3%|▌                   | 2/72 [00:00<00:04, 14.32it/s]Epoch: 9, train for the 49-th batch, train loss: 0.43847769498825073:  40%|████▊       | 48/119 [00:07<00:13,  5.37it/s]Epoch: 9, train for the 49-th batch, train loss: 0.43847769498825073:  41%|████▉       | 49/119 [00:07<00:13,  5.35it/s]Epoch: 8, train for the 13-th batch, train loss: 0.484671950340271:   8%|█▏            | 12/146 [00:01<00:18,  7.16it/s]Epoch: 8, train for the 13-th batch, train loss: 0.484671950340271:   9%|█▏            | 13/146 [00:01<00:20,  6.56it/s]Epoch: 6, train for the 98-th batch, train loss: 0.5068255662918091:  64%|████████▎    | 97/151 [00:16<00:09,  5.71it/s]evaluate for the 3-th batch, evaluate loss: 0.4912853240966797:   3%|▌                   | 2/72 [00:00<00:04, 14.32it/s]evaluate for the 9-th batch, evaluate loss: 0.4979647696018219:  12%|██▍                 | 8/66 [00:01<00:06,  8.64it/s]evaluate for the 9-th batch, evaluate loss: 0.4979647696018219:  14%|██▋                 | 9/66 [00:01<00:06,  8.48it/s]Epoch: 6, train for the 98-th batch, train loss: 0.5068255662918091:  65%|████████▍    | 98/151 [00:16<00:09,  5.46it/s]Epoch: 2, train for the 133-th batch, train loss: 0.512819230556488:  34%|████▏       | 132/383 [00:28<01:06,  3.80it/s]evaluate for the 4-th batch, evaluate loss: 0.5377306342124939:   3%|▌                   | 2/72 [00:00<00:04, 14.32it/s]evaluate for the 4-th batch, evaluate loss: 0.5377306342124939:   6%|█                   | 4/72 [00:00<00:05, 13.31it/s]Epoch: 2, train for the 133-th batch, train loss: 0.512819230556488:  35%|████▏       | 133/383 [00:28<01:08,  3.65it/s]evaluate for the 10-th batch, evaluate loss: 0.5198081135749817:  14%|██▌                | 9/66 [00:01<00:06,  8.48it/s]Epoch: 9, train for the 50-th batch, train loss: 0.38267782330513:  41%|██████▏        | 49/119 [00:07<00:13,  5.35it/s]Epoch: 8, train for the 14-th batch, train loss: 0.45258376002311707:   9%|█           | 13/146 [00:01<00:20,  6.56it/s]Epoch: 9, train for the 50-th batch, train loss: 0.38267782330513:  42%|██████▎        | 50/119 [00:07<00:12,  5.37it/s]Epoch: 8, train for the 14-th batch, train loss: 0.45258376002311707:  10%|█▏          | 14/146 [00:01<00:21,  6.22it/s]Epoch: 6, train for the 99-th batch, train loss: 0.5231050848960876:  65%|████████▍    | 98/151 [00:16<00:09,  5.46it/s]evaluate for the 5-th batch, evaluate loss: 0.5185238122940063:   6%|█                   | 4/72 [00:00<00:05, 13.31it/s]Epoch: 6, train for the 99-th batch, train loss: 0.5231050848960876:  66%|████████▌    | 99/151 [00:16<00:09,  5.57it/s]evaluate for the 11-th batch, evaluate loss: 0.510729968547821:  14%|██▋                 | 9/66 [00:01<00:06,  8.48it/s]evaluate for the 11-th batch, evaluate loss: 0.510729968547821:  17%|███▏               | 11/66 [00:01<00:06,  8.95it/s]evaluate for the 6-th batch, evaluate loss: 0.5470579266548157:   6%|█                   | 4/72 [00:00<00:05, 13.31it/s]evaluate for the 6-th batch, evaluate loss: 0.5470579266548157:   8%|█▋                  | 6/72 [00:00<00:05, 11.85it/s]Epoch: 8, train for the 15-th batch, train loss: 0.45865973830223083:  10%|█▏          | 14/146 [00:02<00:21,  6.22it/s]Epoch: 8, train for the 15-th batch, train loss: 0.45865973830223083:  10%|█▏          | 15/146 [00:02<00:21,  6.11it/s]Epoch: 9, train for the 51-th batch, train loss: 0.4335651993751526:  42%|█████▍       | 50/119 [00:07<00:12,  5.37it/s]evaluate for the 7-th batch, evaluate loss: 0.5591366291046143:   8%|█▋                  | 6/72 [00:00<00:05, 11.85it/s]Epoch: 2, train for the 134-th batch, train loss: 0.5235489010810852:  35%|███▊       | 133/383 [00:28<01:08,  3.65it/s]evaluate for the 12-th batch, evaluate loss: 0.5261338949203491:  17%|███               | 11/66 [00:01<00:06,  8.95it/s]evaluate for the 12-th batch, evaluate loss: 0.5261338949203491:  18%|███▎              | 12/66 [00:01<00:06,  8.79it/s]Epoch: 9, train for the 51-th batch, train loss: 0.4335651993751526:  43%|█████▌       | 51/119 [00:07<00:12,  5.35it/s]Epoch: 6, train for the 100-th batch, train loss: 0.5635468363761902:  66%|███████▊    | 99/151 [00:16<00:09,  5.57it/s]Epoch: 2, train for the 134-th batch, train loss: 0.5235489010810852:  35%|███▊       | 134/383 [00:28<01:08,  3.62it/s]Epoch: 6, train for the 100-th batch, train loss: 0.5635468363761902:  66%|███████▎   | 100/151 [00:16<00:09,  5.38it/s]evaluate for the 8-th batch, evaluate loss: 0.5458754897117615:   8%|█▋                  | 6/72 [00:00<00:05, 11.85it/s]evaluate for the 8-th batch, evaluate loss: 0.5458754897117615:  11%|██▏                 | 8/72 [00:00<00:04, 13.03it/s]Epoch: 8, train for the 16-th batch, train loss: 0.5040282607078552:  10%|█▎           | 15/146 [00:02<00:21,  6.11it/s]Epoch: 8, train for the 16-th batch, train loss: 0.5040282607078552:  11%|█▍           | 16/146 [00:02<00:19,  6.76it/s]evaluate for the 13-th batch, evaluate loss: 0.5316053032875061:  18%|███▎              | 12/66 [00:01<00:06,  8.79it/s]evaluate for the 13-th batch, evaluate loss: 0.5316053032875061:  20%|███▌              | 13/66 [00:01<00:05,  8.90it/s]evaluate for the 9-th batch, evaluate loss: 0.5166895985603333:  11%|██▏                 | 8/72 [00:00<00:04, 13.03it/s]Epoch: 9, train for the 52-th batch, train loss: 0.4415925443172455:  43%|█████▌       | 51/119 [00:08<00:12,  5.35it/s]Epoch: 9, train for the 52-th batch, train loss: 0.4415925443172455:  44%|█████▋       | 52/119 [00:08<00:12,  5.48it/s]Epoch: 6, train for the 101-th batch, train loss: 0.6091066002845764:  66%|███████▎   | 100/151 [00:16<00:09,  5.38it/s]evaluate for the 10-th batch, evaluate loss: 0.4931505024433136:  11%|██                 | 8/72 [00:00<00:04, 13.03it/s]evaluate for the 10-th batch, evaluate loss: 0.4931505024433136:  14%|██▌               | 10/72 [00:00<00:04, 12.97it/s]evaluate for the 14-th batch, evaluate loss: 0.5333717465400696:  20%|███▌              | 13/66 [00:01<00:05,  8.90it/s]evaluate for the 14-th batch, evaluate loss: 0.5333717465400696:  21%|███▊              | 14/66 [00:01<00:05,  8.97it/s]Epoch: 6, train for the 101-th batch, train loss: 0.6091066002845764:  67%|███████▎   | 101/151 [00:16<00:09,  5.47it/s]Epoch: 8, train for the 17-th batch, train loss: 0.5137867331504822:  11%|█▍           | 16/146 [00:02<00:19,  6.76it/s]Epoch: 8, train for the 17-th batch, train loss: 0.5137867331504822:  12%|█▌           | 17/146 [00:02<00:18,  6.85it/s]evaluate for the 11-th batch, evaluate loss: 0.5089499354362488:  14%|██▌               | 10/72 [00:00<00:04, 12.97it/s]Epoch: 2, train for the 135-th batch, train loss: 0.4330291152000427:  35%|███▊       | 134/383 [00:28<01:08,  3.62it/s]Epoch: 9, train for the 53-th batch, train loss: 0.36445215344429016:  44%|█████▏      | 52/119 [00:08<00:12,  5.48it/s]Epoch: 2, train for the 135-th batch, train loss: 0.4330291152000427:  35%|███▉       | 135/383 [00:28<01:08,  3.61it/s]evaluate for the 15-th batch, evaluate loss: 0.5239138007164001:  21%|███▊              | 14/66 [00:01<00:05,  8.97it/s]evaluate for the 15-th batch, evaluate loss: 0.5239138007164001:  23%|████              | 15/66 [00:01<00:05,  8.99it/s]Epoch: 9, train for the 53-th batch, train loss: 0.36445215344429016:  45%|█████▎      | 53/119 [00:08<00:11,  5.67it/s]evaluate for the 12-th batch, evaluate loss: 0.516447901725769:  14%|██▋                | 10/72 [00:00<00:04, 12.97it/s]evaluate for the 12-th batch, evaluate loss: 0.516447901725769:  17%|███▏               | 12/72 [00:00<00:04, 12.91it/s]Epoch: 8, train for the 18-th batch, train loss: 0.43303701281547546:  12%|█▍          | 17/146 [00:02<00:18,  6.85it/s]Epoch: 8, train for the 18-th batch, train loss: 0.43303701281547546:  12%|█▍          | 18/146 [00:02<00:18,  6.89it/s]Epoch: 6, train for the 102-th batch, train loss: 0.5052999258041382:  67%|███████▎   | 101/151 [00:17<00:09,  5.47it/s]evaluate for the 13-th batch, evaluate loss: 0.4485945701599121:  17%|███               | 12/72 [00:01<00:04, 12.91it/s]evaluate for the 16-th batch, evaluate loss: 0.49861282110214233:  23%|███▊             | 15/66 [00:01<00:05,  8.99it/s]evaluate for the 16-th batch, evaluate loss: 0.49861282110214233:  24%|████             | 16/66 [00:01<00:05,  8.96it/s]Epoch: 6, train for the 102-th batch, train loss: 0.5052999258041382:  68%|███████▍   | 102/151 [00:17<00:09,  5.08it/s]Epoch: 9, train for the 54-th batch, train loss: 0.4084535241127014:  45%|█████▊       | 53/119 [00:08<00:11,  5.67it/s]Epoch: 9, train for the 54-th batch, train loss: 0.4084535241127014:  45%|█████▉       | 54/119 [00:08<00:10,  6.17it/s]evaluate for the 14-th batch, evaluate loss: 0.43507978320121765:  17%|██▊              | 12/72 [00:01<00:04, 12.91it/s]evaluate for the 14-th batch, evaluate loss: 0.43507978320121765:  19%|███▎             | 14/72 [00:01<00:04, 13.20it/s]Epoch: 2, train for the 136-th batch, train loss: 0.392342746257782:  35%|████▏       | 135/383 [00:29<01:08,  3.61it/s]Epoch: 8, train for the 19-th batch, train loss: 0.42707327008247375:  12%|█▍          | 18/146 [00:02<00:18,  6.89it/s]Epoch: 8, train for the 19-th batch, train loss: 0.42707327008247375:  13%|█▌          | 19/146 [00:02<00:18,  7.04it/s]Epoch: 2, train for the 136-th batch, train loss: 0.392342746257782:  36%|████▎       | 136/383 [00:29<01:04,  3.82it/s]evaluate for the 17-th batch, evaluate loss: 0.5271745324134827:  24%|████▎             | 16/66 [00:01<00:05,  8.96it/s]evaluate for the 17-th batch, evaluate loss: 0.5271745324134827:  26%|████▋             | 17/66 [00:01<00:05,  8.62it/s]Epoch: 9, train for the 55-th batch, train loss: 0.3793153166770935:  45%|█████▉       | 54/119 [00:08<00:10,  6.17it/s]evaluate for the 15-th batch, evaluate loss: 0.46878454089164734:  19%|███▎             | 14/72 [00:01<00:04, 13.20it/s]Epoch: 6, train for the 103-th batch, train loss: 0.5603809356689453:  68%|███████▍   | 102/151 [00:17<00:09,  5.08it/s]Epoch: 9, train for the 55-th batch, train loss: 0.3793153166770935:  46%|██████       | 55/119 [00:08<00:10,  6.31it/s]Epoch: 6, train for the 103-th batch, train loss: 0.5603809356689453:  68%|███████▌   | 103/151 [00:17<00:09,  5.15it/s]Epoch: 8, train for the 20-th batch, train loss: 0.4706253707408905:  13%|█▋           | 19/146 [00:02<00:18,  7.04it/s]Epoch: 8, train for the 20-th batch, train loss: 0.4706253707408905:  14%|█▊           | 20/146 [00:02<00:17,  7.21it/s]evaluate for the 16-th batch, evaluate loss: 0.5037835836410522:  19%|███▌              | 14/72 [00:01<00:04, 13.20it/s]evaluate for the 16-th batch, evaluate loss: 0.5037835836410522:  22%|████              | 16/72 [00:01<00:04, 12.35it/s]evaluate for the 18-th batch, evaluate loss: 0.5352996587753296:  26%|████▋             | 17/66 [00:02<00:05,  8.62it/s]evaluate for the 18-th batch, evaluate loss: 0.5352996587753296:  27%|████▉             | 18/66 [00:02<00:06,  7.81it/s]evaluate for the 17-th batch, evaluate loss: 0.45702484250068665:  22%|███▊             | 16/72 [00:01<00:04, 12.35it/s]Epoch: 2, train for the 137-th batch, train loss: 0.39654120802879333:  36%|███▌      | 136/383 [00:29<01:04,  3.82it/s]Epoch: 9, train for the 56-th batch, train loss: 0.4225424528121948:  46%|██████       | 55/119 [00:08<00:10,  6.31it/s]Epoch: 2, train for the 137-th batch, train loss: 0.39654120802879333:  36%|███▌      | 137/383 [00:29<01:02,  3.91it/s]Epoch: 9, train for the 56-th batch, train loss: 0.4225424528121948:  47%|██████       | 56/119 [00:08<00:10,  6.02it/s]Epoch: 6, train for the 104-th batch, train loss: 0.5269551277160645:  68%|███████▌   | 103/151 [00:17<00:09,  5.15it/s]Epoch: 8, train for the 21-th batch, train loss: 0.46681803464889526:  14%|█▋          | 20/146 [00:02<00:17,  7.21it/s]evaluate for the 18-th batch, evaluate loss: 0.45157432556152344:  22%|███▊             | 16/72 [00:01<00:04, 12.35it/s]evaluate for the 18-th batch, evaluate loss: 0.45157432556152344:  25%|████▎            | 18/72 [00:01<00:03, 13.51it/s]Epoch: 8, train for the 21-th batch, train loss: 0.46681803464889526:  14%|█▋          | 21/146 [00:02<00:18,  6.86it/s]Epoch: 6, train for the 104-th batch, train loss: 0.5269551277160645:  69%|███████▌   | 104/151 [00:17<00:09,  5.12it/s]evaluate for the 19-th batch, evaluate loss: 0.4935552775859833:  27%|████▉             | 18/66 [00:02<00:06,  7.81it/s]evaluate for the 19-th batch, evaluate loss: 0.4935552775859833:  29%|█████▏            | 19/66 [00:02<00:06,  7.75it/s]evaluate for the 19-th batch, evaluate loss: 0.48823997378349304:  25%|████▎            | 18/72 [00:01<00:03, 13.51it/s]Epoch: 9, train for the 57-th batch, train loss: 0.3836755156517029:  47%|██████       | 56/119 [00:08<00:10,  6.02it/s]Epoch: 9, train for the 57-th batch, train loss: 0.3836755156517029:  48%|██████▏      | 57/119 [00:08<00:10,  6.13it/s]Epoch: 8, train for the 22-th batch, train loss: 0.4962712824344635:  14%|█▊           | 21/146 [00:03<00:18,  6.86it/s]Epoch: 8, train for the 22-th batch, train loss: 0.4962712824344635:  15%|█▉           | 22/146 [00:03<00:18,  6.69it/s]evaluate for the 20-th batch, evaluate loss: 0.5651031136512756:  29%|█████▏            | 19/66 [00:02<00:06,  7.75it/s]evaluate for the 20-th batch, evaluate loss: 0.5651031136512756:  30%|█████▍            | 20/66 [00:02<00:06,  7.37it/s]evaluate for the 20-th batch, evaluate loss: 0.49589186906814575:  25%|████▎            | 18/72 [00:01<00:03, 13.51it/s]evaluate for the 20-th batch, evaluate loss: 0.49589186906814575:  28%|████▋            | 20/72 [00:01<00:04, 12.11it/s]Epoch: 6, train for the 105-th batch, train loss: 0.5052095055580139:  69%|███████▌   | 104/151 [00:17<00:09,  5.12it/s]Epoch: 2, train for the 138-th batch, train loss: 0.285823792219162:  36%|████▎       | 137/383 [00:29<01:02,  3.91it/s]Epoch: 6, train for the 105-th batch, train loss: 0.5052095055580139:  70%|███████▋   | 105/151 [00:17<00:09,  5.02it/s]evaluate for the 21-th batch, evaluate loss: 0.5425251722335815:  28%|█████             | 20/72 [00:01<00:04, 12.11it/s]Epoch: 2, train for the 138-th batch, train loss: 0.285823792219162:  36%|████▎       | 138/383 [00:29<01:05,  3.77it/s]Epoch: 9, train for the 58-th batch, train loss: 0.40118247270584106:  48%|█████▋      | 57/119 [00:08<00:10,  6.13it/s]evaluate for the 21-th batch, evaluate loss: 0.5724971294403076:  30%|█████▍            | 20/66 [00:02<00:06,  7.37it/s]Epoch: 9, train for the 58-th batch, train loss: 0.40118247270584106:  49%|█████▊      | 58/119 [00:08<00:09,  6.22it/s]Epoch: 8, train for the 23-th batch, train loss: 0.5027444362640381:  15%|█▉           | 22/146 [00:03<00:18,  6.69it/s]Epoch: 8, train for the 23-th batch, train loss: 0.5027444362640381:  16%|██           | 23/146 [00:03<00:18,  6.55it/s]evaluate for the 22-th batch, evaluate loss: 0.4599759876728058:  28%|█████             | 20/72 [00:01<00:04, 12.11it/s]evaluate for the 22-th batch, evaluate loss: 0.4599759876728058:  31%|█████▌            | 22/72 [00:01<00:04, 12.47it/s]Epoch: 6, train for the 106-th batch, train loss: 0.5446612238883972:  70%|███████▋   | 105/151 [00:17<00:09,  5.02it/s]evaluate for the 23-th batch, evaluate loss: 0.5040687322616577:  31%|█████▌            | 22/72 [00:01<00:04, 12.47it/s]Epoch: 6, train for the 106-th batch, train loss: 0.5446612238883972:  70%|███████▋   | 106/151 [00:17<00:08,  5.14it/s]Epoch: 9, train for the 59-th batch, train loss: 0.4197140336036682:  49%|██████▎      | 58/119 [00:09<00:09,  6.22it/s]Epoch: 9, train for the 59-th batch, train loss: 0.4197140336036682:  50%|██████▍      | 59/119 [00:09<00:09,  6.36it/s]evaluate for the 22-th batch, evaluate loss: 0.5474456548690796:  30%|█████▍            | 20/66 [00:02<00:06,  7.37it/s]evaluate for the 22-th batch, evaluate loss: 0.5474456548690796:  33%|██████            | 22/66 [00:02<00:05,  7.53it/s]Epoch: 8, train for the 24-th batch, train loss: 0.47145411372184753:  16%|█▉          | 23/146 [00:03<00:18,  6.55it/s]evaluate for the 24-th batch, evaluate loss: 0.47547873854637146:  31%|█████▏           | 22/72 [00:01<00:04, 12.47it/s]evaluate for the 24-th batch, evaluate loss: 0.47547873854637146:  33%|█████▋           | 24/72 [00:01<00:03, 13.40it/s]Epoch: 8, train for the 24-th batch, train loss: 0.47145411372184753:  16%|█▉          | 24/146 [00:03<00:18,  6.46it/s]Epoch: 2, train for the 139-th batch, train loss: 0.3771108090877533:  36%|███▉       | 138/383 [00:29<01:05,  3.77it/s]Epoch: 2, train for the 139-th batch, train loss: 0.3771108090877533:  36%|███▉       | 139/383 [00:29<01:05,  3.73it/s]evaluate for the 23-th batch, evaluate loss: 0.5394771099090576:  33%|██████            | 22/66 [00:02<00:05,  7.53it/s]evaluate for the 23-th batch, evaluate loss: 0.5394771099090576:  35%|██████▎           | 23/66 [00:02<00:05,  7.87it/s]evaluate for the 25-th batch, evaluate loss: 0.5879643559455872:  33%|██████            | 24/72 [00:01<00:03, 13.40it/s]Epoch: 8, train for the 25-th batch, train loss: 0.4800907075405121:  16%|██▏          | 24/146 [00:03<00:18,  6.46it/s]Epoch: 8, train for the 25-th batch, train loss: 0.4800907075405121:  17%|██▏          | 25/146 [00:03<00:17,  7.12it/s]Epoch: 6, train for the 107-th batch, train loss: 0.5106624364852905:  70%|███████▋   | 106/151 [00:18<00:08,  5.14it/s]Epoch: 6, train for the 107-th batch, train loss: 0.5106624364852905:  71%|███████▊   | 107/151 [00:18<00:08,  4.94it/s]evaluate for the 26-th batch, evaluate loss: 0.42130225896835327:  33%|█████▋           | 24/72 [00:02<00:03, 13.40it/s]evaluate for the 26-th batch, evaluate loss: 0.42130225896835327:  36%|██████▏          | 26/72 [00:02<00:03, 12.75it/s]evaluate for the 24-th batch, evaluate loss: 0.5523853302001953:  35%|██████▎           | 23/66 [00:02<00:05,  7.87it/s]evaluate for the 24-th batch, evaluate loss: 0.5523853302001953:  36%|██████▌           | 24/66 [00:02<00:05,  7.74it/s]Epoch: 8, train for the 26-th batch, train loss: 0.4715016186237335:  17%|██▏          | 25/146 [00:03<00:17,  7.12it/s]Epoch: 8, train for the 26-th batch, train loss: 0.4715016186237335:  18%|██▎          | 26/146 [00:03<00:16,  7.40it/s]evaluate for the 27-th batch, evaluate loss: 0.3949756622314453:  36%|██████▌           | 26/72 [00:02<00:03, 12.75it/s]Epoch: 9, train for the 60-th batch, train loss: 0.3918024003505707:  50%|██████▍      | 59/119 [00:09<00:09,  6.36it/s]Epoch: 2, train for the 140-th batch, train loss: 0.37232115864753723:  36%|███▋      | 139/383 [00:30<01:05,  3.73it/s]Epoch: 9, train for the 60-th batch, train loss: 0.3918024003505707:  50%|██████▌      | 60/119 [00:09<00:11,  4.95it/s]evaluate for the 28-th batch, evaluate loss: 0.4600384533405304:  36%|██████▌           | 26/72 [00:02<00:03, 12.75it/s]evaluate for the 28-th batch, evaluate loss: 0.4600384533405304:  39%|███████           | 28/72 [00:02<00:03, 13.07it/s]Epoch: 2, train for the 140-th batch, train loss: 0.37232115864753723:  37%|███▋      | 140/383 [00:30<01:03,  3.80it/s]evaluate for the 25-th batch, evaluate loss: 0.5545616745948792:  36%|██████▌           | 24/66 [00:02<00:05,  7.74it/s]evaluate for the 25-th batch, evaluate loss: 0.5545616745948792:  38%|██████▊           | 25/66 [00:02<00:04,  8.22it/s]Epoch: 6, train for the 108-th batch, train loss: 0.49560895562171936:  71%|███████   | 107/151 [00:18<00:08,  4.94it/s]Epoch: 8, train for the 27-th batch, train loss: 0.45131075382232666:  18%|██▏         | 26/146 [00:03<00:16,  7.40it/s]Epoch: 8, train for the 27-th batch, train loss: 0.45131075382232666:  18%|██▏         | 27/146 [00:03<00:15,  7.63it/s]evaluate for the 29-th batch, evaluate loss: 0.4986269176006317:  39%|███████           | 28/72 [00:02<00:03, 13.07it/s]Epoch: 6, train for the 108-th batch, train loss: 0.49560895562171936:  72%|███████▏  | 108/151 [00:18<00:08,  4.92it/s]Epoch: 9, train for the 61-th batch, train loss: 0.43137529492378235:  50%|██████      | 60/119 [00:09<00:11,  4.95it/s]Epoch: 9, train for the 61-th batch, train loss: 0.43137529492378235:  51%|██████▏     | 61/119 [00:09<00:10,  5.38it/s]evaluate for the 30-th batch, evaluate loss: 0.4916425943374634:  39%|███████           | 28/72 [00:02<00:03, 13.07it/s]evaluate for the 30-th batch, evaluate loss: 0.4916425943374634:  42%|███████▌          | 30/72 [00:02<00:03, 13.03it/s]evaluate for the 26-th batch, evaluate loss: 0.5642703771591187:  38%|██████▊           | 25/66 [00:03<00:04,  8.22it/s]evaluate for the 26-th batch, evaluate loss: 0.5642703771591187:  39%|███████           | 26/66 [00:03<00:05,  7.76it/s]Epoch: 8, train for the 28-th batch, train loss: 0.45579564571380615:  18%|██▏         | 27/146 [00:03<00:15,  7.63it/s]Epoch: 8, train for the 28-th batch, train loss: 0.45579564571380615:  19%|██▎         | 28/146 [00:03<00:15,  7.59it/s]Epoch: 2, train for the 141-th batch, train loss: 0.41092467308044434:  37%|███▋      | 140/383 [00:30<01:03,  3.80it/s]Epoch: 6, train for the 109-th batch, train loss: 0.5017290711402893:  72%|███████▊   | 108/151 [00:18<00:08,  4.92it/s]Epoch: 9, train for the 62-th batch, train loss: 0.40699607133865356:  51%|██████▏     | 61/119 [00:09<00:10,  5.38it/s]evaluate for the 31-th batch, evaluate loss: 0.5463488698005676:  42%|███████▌          | 30/72 [00:02<00:03, 13.03it/s]Epoch: 9, train for the 62-th batch, train loss: 0.40699607133865356:  52%|██████▎     | 62/119 [00:09<00:09,  5.73it/s]Epoch: 2, train for the 141-th batch, train loss: 0.41092467308044434:  37%|███▋      | 141/383 [00:30<01:03,  3.83it/s]evaluate for the 27-th batch, evaluate loss: 0.5517869591712952:  39%|███████           | 26/66 [00:03<00:05,  7.76it/s]evaluate for the 27-th batch, evaluate loss: 0.5517869591712952:  41%|███████▎          | 27/66 [00:03<00:04,  8.22it/s]Epoch: 6, train for the 109-th batch, train loss: 0.5017290711402893:  72%|███████▉   | 109/151 [00:18<00:08,  4.83it/s]Epoch: 8, train for the 29-th batch, train loss: 0.41489073634147644:  19%|██▎         | 28/146 [00:04<00:15,  7.59it/s]Epoch: 8, train for the 29-th batch, train loss: 0.41489073634147644:  20%|██▍         | 29/146 [00:04<00:16,  7.18it/s]evaluate for the 32-th batch, evaluate loss: 0.5981575846672058:  42%|███████▌          | 30/72 [00:02<00:03, 13.03it/s]evaluate for the 32-th batch, evaluate loss: 0.5981575846672058:  44%|████████          | 32/72 [00:02<00:03, 12.26it/s]Epoch: 9, train for the 63-th batch, train loss: 0.4003772735595703:  52%|██████▊      | 62/119 [00:09<00:09,  5.73it/s]Epoch: 9, train for the 63-th batch, train loss: 0.4003772735595703:  53%|██████▉      | 63/119 [00:09<00:09,  6.12it/s]evaluate for the 28-th batch, evaluate loss: 0.5931965708732605:  41%|███████▎          | 27/66 [00:03<00:04,  8.22it/s]evaluate for the 28-th batch, evaluate loss: 0.5931965708732605:  42%|███████▋          | 28/66 [00:03<00:04,  7.85it/s]Epoch: 6, train for the 110-th batch, train loss: 0.49035733938217163:  72%|███████▏  | 109/151 [00:18<00:08,  4.83it/s]evaluate for the 33-th batch, evaluate loss: 0.5139721632003784:  44%|████████          | 32/72 [00:02<00:03, 12.26it/s]Epoch: 8, train for the 30-th batch, train loss: 0.43169301748275757:  20%|██▍         | 29/146 [00:04<00:16,  7.18it/s]Epoch: 8, train for the 30-th batch, train loss: 0.43169301748275757:  21%|██▍         | 30/146 [00:04<00:15,  7.34it/s]Epoch: 6, train for the 110-th batch, train loss: 0.49035733938217163:  73%|███████▎  | 110/151 [00:18<00:08,  4.93it/s]Epoch: 2, train for the 142-th batch, train loss: 0.4114561378955841:  37%|████       | 141/383 [00:30<01:03,  3.83it/s]evaluate for the 34-th batch, evaluate loss: 0.5577180981636047:  44%|████████          | 32/72 [00:02<00:03, 12.26it/s]evaluate for the 34-th batch, evaluate loss: 0.5577180981636047:  47%|████████▌         | 34/72 [00:02<00:03, 12.61it/s]Epoch: 9, train for the 64-th batch, train loss: 0.3960600197315216:  53%|██████▉      | 63/119 [00:10<00:09,  6.12it/s]evaluate for the 29-th batch, evaluate loss: 0.5108241438865662:  42%|███████▋          | 28/66 [00:03<00:04,  7.85it/s]evaluate for the 29-th batch, evaluate loss: 0.5108241438865662:  44%|███████▉          | 29/66 [00:03<00:04,  8.08it/s]Epoch: 2, train for the 142-th batch, train loss: 0.4114561378955841:  37%|████       | 142/383 [00:30<01:02,  3.84it/s]Epoch: 9, train for the 64-th batch, train loss: 0.3960600197315216:  54%|██████▉      | 64/119 [00:10<00:08,  6.50it/s]Epoch: 8, train for the 31-th batch, train loss: 0.5235791802406311:  21%|██▋          | 30/146 [00:04<00:15,  7.34it/s]evaluate for the 35-th batch, evaluate loss: 0.4982694089412689:  47%|████████▌         | 34/72 [00:02<00:03, 12.61it/s]Epoch: 8, train for the 31-th batch, train loss: 0.5235791802406311:  21%|██▊          | 31/146 [00:04<00:14,  7.79it/s]evaluate for the 30-th batch, evaluate loss: 0.5643548369407654:  44%|███████▉          | 29/66 [00:03<00:04,  8.08it/s]evaluate for the 30-th batch, evaluate loss: 0.5643548369407654:  45%|████████▏         | 30/66 [00:03<00:04,  8.34it/s]Epoch: 6, train for the 111-th batch, train loss: 0.5269900560379028:  73%|████████   | 110/151 [00:18<00:08,  4.93it/s]evaluate for the 36-th batch, evaluate loss: 0.5029021501541138:  47%|████████▌         | 34/72 [00:02<00:03, 12.61it/s]evaluate for the 36-th batch, evaluate loss: 0.5029021501541138:  50%|█████████         | 36/72 [00:02<00:02, 12.53it/s]Epoch: 9, train for the 65-th batch, train loss: 0.41306209564208984:  54%|██████▍     | 64/119 [00:10<00:08,  6.50it/s]Epoch: 6, train for the 111-th batch, train loss: 0.5269900560379028:  74%|████████   | 111/151 [00:18<00:08,  4.91it/s]Epoch: 9, train for the 65-th batch, train loss: 0.41306209564208984:  55%|██████▌     | 65/119 [00:10<00:08,  6.57it/s]Epoch: 8, train for the 32-th batch, train loss: 0.4777754545211792:  21%|██▊          | 31/146 [00:04<00:14,  7.79it/s]Epoch: 8, train for the 32-th batch, train loss: 0.4777754545211792:  22%|██▊          | 32/146 [00:04<00:15,  7.46it/s]evaluate for the 37-th batch, evaluate loss: 0.4937075078487396:  50%|█████████         | 36/72 [00:02<00:02, 12.53it/s]Epoch: 2, train for the 143-th batch, train loss: 0.3553837537765503:  37%|████       | 142/383 [00:30<01:02,  3.84it/s]evaluate for the 31-th batch, evaluate loss: 0.5383982062339783:  45%|████████▏         | 30/66 [00:03<00:04,  8.34it/s]evaluate for the 31-th batch, evaluate loss: 0.5383982062339783:  47%|████████▍         | 31/66 [00:03<00:04,  8.32it/s]Epoch: 2, train for the 143-th batch, train loss: 0.3553837537765503:  37%|████       | 143/383 [00:31<01:02,  3.81it/s]evaluate for the 38-th batch, evaluate loss: 0.3873629570007324:  50%|█████████         | 36/72 [00:02<00:02, 12.53it/s]evaluate for the 38-th batch, evaluate loss: 0.3873629570007324:  53%|█████████▌        | 38/72 [00:02<00:02, 12.14it/s]Epoch: 6, train for the 112-th batch, train loss: 0.4730365574359894:  74%|████████   | 111/151 [00:19<00:08,  4.91it/s]evaluate for the 32-th batch, evaluate loss: 0.5578464269638062:  47%|████████▍         | 31/66 [00:03<00:04,  8.32it/s]Epoch: 8, train for the 33-th batch, train loss: 0.5039153099060059:  22%|██▊          | 32/146 [00:04<00:15,  7.46it/s]Epoch: 6, train for the 112-th batch, train loss: 0.4730365574359894:  74%|████████▏  | 112/151 [00:19<00:07,  4.98it/s]Epoch: 8, train for the 33-th batch, train loss: 0.5039153099060059:  23%|██▉          | 33/146 [00:04<00:15,  7.12it/s]evaluate for the 39-th batch, evaluate loss: 0.43927156925201416:  53%|████████▉        | 38/72 [00:03<00:02, 12.14it/s]Epoch: 9, train for the 66-th batch, train loss: 0.4334162473678589:  55%|███████      | 65/119 [00:10<00:08,  6.57it/s]Epoch: 9, train for the 66-th batch, train loss: 0.4334162473678589:  55%|███████▏     | 66/119 [00:10<00:09,  5.72it/s]evaluate for the 40-th batch, evaluate loss: 0.4967431426048279:  53%|█████████▌        | 38/72 [00:03<00:02, 12.14it/s]evaluate for the 40-th batch, evaluate loss: 0.4967431426048279:  56%|██████████        | 40/72 [00:03<00:02, 12.58it/s]Epoch: 2, train for the 144-th batch, train loss: 0.31835809350013733:  37%|███▋      | 143/383 [00:31<01:02,  3.81it/s]Epoch: 8, train for the 34-th batch, train loss: 0.4598418176174164:  23%|██▉          | 33/146 [00:04<00:15,  7.12it/s]evaluate for the 33-th batch, evaluate loss: 0.5718795657157898:  47%|████████▍         | 31/66 [00:03<00:04,  8.32it/s]evaluate for the 33-th batch, evaluate loss: 0.5718795657157898:  50%|█████████         | 33/66 [00:03<00:04,  8.18it/s]Epoch: 8, train for the 34-th batch, train loss: 0.4598418176174164:  23%|███          | 34/146 [00:04<00:15,  7.25it/s]Epoch: 2, train for the 144-th batch, train loss: 0.31835809350013733:  38%|███▊      | 144/383 [00:31<01:00,  3.93it/s]Epoch: 6, train for the 113-th batch, train loss: 0.5146389007568359:  74%|████████▏  | 112/151 [00:19<00:07,  4.98it/s]Epoch: 9, train for the 67-th batch, train loss: 0.46836596727371216:  55%|██████▋     | 66/119 [00:10<00:09,  5.72it/s]evaluate for the 41-th batch, evaluate loss: 0.5065626502037048:  56%|██████████        | 40/72 [00:03<00:02, 12.58it/s]Epoch: 9, train for the 67-th batch, train loss: 0.46836596727371216:  56%|██████▊     | 67/119 [00:10<00:08,  5.84it/s]Epoch: 6, train for the 113-th batch, train loss: 0.5146389007568359:  75%|████████▏  | 113/151 [00:19<00:07,  4.90it/s]Epoch: 8, train for the 35-th batch, train loss: 0.4813762903213501:  23%|███          | 34/146 [00:04<00:15,  7.25it/s]Epoch: 8, train for the 35-th batch, train loss: 0.4813762903213501:  24%|███          | 35/146 [00:04<00:14,  7.49it/s]evaluate for the 42-th batch, evaluate loss: 0.49024268984794617:  56%|█████████▍       | 40/72 [00:03<00:02, 12.58it/s]evaluate for the 42-th batch, evaluate loss: 0.49024268984794617:  58%|█████████▉       | 42/72 [00:03<00:02, 12.75it/s]evaluate for the 34-th batch, evaluate loss: 0.5420037508010864:  50%|█████████         | 33/66 [00:04<00:04,  8.18it/s]evaluate for the 34-th batch, evaluate loss: 0.5420037508010864:  52%|█████████▎        | 34/66 [00:04<00:04,  7.63it/s]evaluate for the 43-th batch, evaluate loss: 0.4924977421760559:  58%|██████████▌       | 42/72 [00:03<00:02, 12.75it/s]Epoch: 9, train for the 68-th batch, train loss: 0.3851372301578522:  56%|███████▎     | 67/119 [00:10<00:08,  5.84it/s]Epoch: 9, train for the 68-th batch, train loss: 0.3851372301578522:  57%|███████▍     | 68/119 [00:10<00:08,  6.14it/s]Epoch: 6, train for the 114-th batch, train loss: 0.49035078287124634:  75%|███████▍  | 113/151 [00:19<00:07,  4.90it/s]Epoch: 8, train for the 36-th batch, train loss: 0.4783675968647003:  24%|███          | 35/146 [00:04<00:14,  7.49it/s]Epoch: 2, train for the 145-th batch, train loss: 0.3703147768974304:  38%|████▏      | 144/383 [00:31<01:00,  3.93it/s]Epoch: 8, train for the 36-th batch, train loss: 0.4783675968647003:  25%|███▏         | 36/146 [00:04<00:14,  7.45it/s]evaluate for the 35-th batch, evaluate loss: 0.5662787556648254:  52%|█████████▎        | 34/66 [00:04<00:04,  7.63it/s]evaluate for the 35-th batch, evaluate loss: 0.5662787556648254:  53%|█████████▌        | 35/66 [00:04<00:03,  7.84it/s]Epoch: 6, train for the 114-th batch, train loss: 0.49035078287124634:  75%|███████▌  | 114/151 [00:19<00:07,  4.91it/s]evaluate for the 44-th batch, evaluate loss: 0.4857652187347412:  58%|██████████▌       | 42/72 [00:03<00:02, 12.75it/s]evaluate for the 44-th batch, evaluate loss: 0.4857652187347412:  61%|███████████       | 44/72 [00:03<00:02, 12.90it/s]Epoch: 2, train for the 145-th batch, train loss: 0.3703147768974304:  38%|████▏      | 145/383 [00:31<01:01,  3.85it/s]evaluate for the 45-th batch, evaluate loss: 0.505605161190033:  61%|███████████▌       | 44/72 [00:03<00:02, 12.90it/s]Epoch: 9, train for the 69-th batch, train loss: 0.3885168433189392:  57%|███████▍     | 68/119 [00:10<00:08,  6.14it/s]Epoch: 9, train for the 69-th batch, train loss: 0.3885168433189392:  58%|███████▌     | 69/119 [00:10<00:08,  6.17it/s]Epoch: 8, train for the 37-th batch, train loss: 0.45782333612442017:  25%|██▉         | 36/146 [00:05<00:14,  7.45it/s]evaluate for the 36-th batch, evaluate loss: 0.5495414733886719:  53%|█████████▌        | 35/66 [00:04<00:03,  7.84it/s]evaluate for the 36-th batch, evaluate loss: 0.5495414733886719:  55%|█████████▊        | 36/66 [00:04<00:03,  7.78it/s]Epoch: 8, train for the 37-th batch, train loss: 0.45782333612442017:  25%|███         | 37/146 [00:05<00:15,  7.24it/s]evaluate for the 46-th batch, evaluate loss: 0.5561066269874573:  61%|███████████       | 44/72 [00:03<00:02, 12.90it/s]evaluate for the 46-th batch, evaluate loss: 0.5561066269874573:  64%|███████████▌      | 46/72 [00:03<00:02, 12.85it/s]Epoch: 6, train for the 115-th batch, train loss: 0.48352521657943726:  75%|███████▌  | 114/151 [00:19<00:07,  4.91it/s]Epoch: 6, train for the 115-th batch, train loss: 0.48352521657943726:  76%|███████▌  | 115/151 [00:19<00:07,  4.96it/s]evaluate for the 47-th batch, evaluate loss: 0.31672775745391846:  64%|██████████▊      | 46/72 [00:03<00:02, 12.85it/s]evaluate for the 37-th batch, evaluate loss: 0.5708388686180115:  55%|█████████▊        | 36/66 [00:04<00:03,  7.78it/s]Epoch: 9, train for the 70-th batch, train loss: 0.35602042078971863:  58%|██████▉     | 69/119 [00:10<00:08,  6.17it/s]Epoch: 9, train for the 70-th batch, train loss: 0.35602042078971863:  59%|███████     | 70/119 [00:11<00:07,  6.38it/s]evaluate for the 48-th batch, evaluate loss: 0.284545361995697:  64%|████████████▏      | 46/72 [00:03<00:02, 12.85it/s]Epoch: 8, train for the 38-th batch, train loss: 0.4872995615005493:  25%|███▎         | 37/146 [00:05<00:15,  7.24it/s]Epoch: 8, train for the 38-th batch, train loss: 0.4872995615005493:  26%|███▍         | 38/146 [00:05<00:15,  7.00it/s]evaluate for the 49-th batch, evaluate loss: 0.45593127608299255:  64%|██████████▊      | 46/72 [00:03<00:02, 12.85it/s]evaluate for the 49-th batch, evaluate loss: 0.45593127608299255:  68%|███████████▌     | 49/72 [00:03<00:01, 14.81it/s]evaluate for the 38-th batch, evaluate loss: 0.5117741823196411:  55%|█████████▊        | 36/66 [00:04<00:03,  7.78it/s]evaluate for the 38-th batch, evaluate loss: 0.5117741823196411:  58%|██████████▎       | 38/66 [00:04<00:03,  8.46it/s]Epoch: 6, train for the 116-th batch, train loss: 0.4576622247695923:  76%|████████▍  | 115/151 [00:19<00:07,  4.96it/s]evaluate for the 50-th batch, evaluate loss: 0.40341076254844666:  68%|███████████▌     | 49/72 [00:03<00:01, 14.81it/s]Epoch: 9, train for the 71-th batch, train loss: 0.3847026824951172:  59%|███████▋     | 70/119 [00:11<00:07,  6.38it/s]Epoch: 9, train for the 71-th batch, train loss: 0.3847026824951172:  60%|███████▊     | 71/119 [00:11<00:07,  6.43it/s]Epoch: 6, train for the 116-th batch, train loss: 0.4576622247695923:  77%|████████▍  | 116/151 [00:19<00:07,  4.95it/s]Epoch: 8, train for the 39-th batch, train loss: 0.5172458291053772:  26%|███▍         | 38/146 [00:05<00:15,  7.00it/s]Epoch: 8, train for the 39-th batch, train loss: 0.5172458291053772:  27%|███▍         | 39/146 [00:05<00:15,  6.91it/s]Epoch: 2, train for the 146-th batch, train loss: 0.3148888945579529:  38%|████▏      | 145/383 [00:31<01:01,  3.85it/s]evaluate for the 39-th batch, evaluate loss: 0.5727924704551697:  58%|██████████▎       | 38/66 [00:04<00:03,  8.46it/s]evaluate for the 39-th batch, evaluate loss: 0.5727924704551697:  59%|██████████▋       | 39/66 [00:04<00:03,  8.43it/s]evaluate for the 51-th batch, evaluate loss: 0.49470922350883484:  68%|███████████▌     | 49/72 [00:03<00:01, 14.81it/s]evaluate for the 51-th batch, evaluate loss: 0.49470922350883484:  71%|████████████     | 51/72 [00:03<00:01, 13.61it/s]Epoch: 2, train for the 146-th batch, train loss: 0.3148888945579529:  38%|████▏      | 146/383 [00:32<01:17,  3.07it/s]Epoch: 6, train for the 117-th batch, train loss: 0.46990782022476196:  77%|███████▋  | 116/151 [00:20<00:07,  4.95it/s]evaluate for the 52-th batch, evaluate loss: 0.4132416546344757:  71%|████████████▊     | 51/72 [00:04<00:01, 13.61it/s]evaluate for the 40-th batch, evaluate loss: 0.5373119711875916:  59%|██████████▋       | 39/66 [00:04<00:03,  8.43it/s]evaluate for the 40-th batch, evaluate loss: 0.5373119711875916:  61%|██████████▉       | 40/66 [00:04<00:02,  8.69it/s]Epoch: 6, train for the 117-th batch, train loss: 0.46990782022476196:  77%|███████▋  | 117/151 [00:20<00:06,  5.17it/s]Epoch: 9, train for the 72-th batch, train loss: 0.446039080619812:  60%|████████▎     | 71/119 [00:11<00:07,  6.43it/s]Epoch: 9, train for the 72-th batch, train loss: 0.446039080619812:  61%|████████▍     | 72/119 [00:11<00:07,  5.98it/s]Epoch: 8, train for the 40-th batch, train loss: 0.4986814856529236:  27%|███▍         | 39/146 [00:05<00:15,  6.91it/s]Epoch: 8, train for the 40-th batch, train loss: 0.4986814856529236:  27%|███▌         | 40/146 [00:05<00:16,  6.61it/s]evaluate for the 53-th batch, evaluate loss: 0.4903487265110016:  71%|████████████▊     | 51/72 [00:04<00:01, 13.61it/s]evaluate for the 53-th batch, evaluate loss: 0.4903487265110016:  74%|█████████████▎    | 53/72 [00:04<00:01, 13.49it/s]evaluate for the 41-th batch, evaluate loss: 0.4925776422023773:  61%|██████████▉       | 40/66 [00:04<00:02,  8.69it/s]evaluate for the 41-th batch, evaluate loss: 0.4925776422023773:  62%|███████████▏      | 41/66 [00:04<00:02,  8.50it/s]Epoch: 6, train for the 118-th batch, train loss: 0.4407642185688019:  77%|████████▌  | 117/151 [00:20<00:06,  5.17it/s]Epoch: 2, train for the 147-th batch, train loss: 0.2787422835826874:  38%|████▏      | 146/383 [00:32<01:17,  3.07it/s]evaluate for the 54-th batch, evaluate loss: 0.5243524312973022:  74%|█████████████▎    | 53/72 [00:04<00:01, 13.49it/s]Epoch: 9, train for the 73-th batch, train loss: 0.42503997683525085:  61%|███████▎    | 72/119 [00:11<00:07,  5.98it/s]Epoch: 9, train for the 73-th batch, train loss: 0.42503997683525085:  61%|███████▎    | 73/119 [00:11<00:07,  5.80it/s]Epoch: 6, train for the 118-th batch, train loss: 0.4407642185688019:  78%|████████▌  | 118/151 [00:20<00:06,  5.14it/s]evaluate for the 42-th batch, evaluate loss: 0.5403643846511841:  62%|███████████▏      | 41/66 [00:05<00:02,  8.50it/s]Epoch: 8, train for the 41-th batch, train loss: 0.4201739430427551:  27%|███▌         | 40/146 [00:05<00:16,  6.61it/s]Epoch: 8, train for the 41-th batch, train loss: 0.4201739430427551:  28%|███▋         | 41/146 [00:05<00:17,  6.03it/s]Epoch: 2, train for the 147-th batch, train loss: 0.2787422835826874:  38%|████▏      | 147/383 [00:32<01:16,  3.09it/s]evaluate for the 55-th batch, evaluate loss: 0.5017204880714417:  74%|█████████████▎    | 53/72 [00:04<00:01, 13.49it/s]evaluate for the 55-th batch, evaluate loss: 0.5017204880714417:  76%|█████████████▊    | 55/72 [00:04<00:01, 12.16it/s]evaluate for the 43-th batch, evaluate loss: 0.5068067908287048:  62%|███████████▏      | 41/66 [00:05<00:02,  8.50it/s]evaluate for the 43-th batch, evaluate loss: 0.5068067908287048:  65%|███████████▋      | 43/66 [00:05<00:02,  8.98it/s]Epoch: 9, train for the 74-th batch, train loss: 0.4135156571865082:  61%|███████▉     | 73/119 [00:11<00:07,  5.80it/s]Epoch: 9, train for the 74-th batch, train loss: 0.4135156571865082:  62%|████████     | 74/119 [00:11<00:07,  6.13it/s]Epoch: 8, train for the 42-th batch, train loss: 0.4764900803565979:  28%|███▋         | 41/146 [00:05<00:17,  6.03it/s]Epoch: 6, train for the 119-th batch, train loss: 0.526573121547699:  78%|█████████▍  | 118/151 [00:20<00:06,  5.14it/s]evaluate for the 56-th batch, evaluate loss: 0.5100073218345642:  76%|█████████████▊    | 55/72 [00:04<00:01, 12.16it/s]Epoch: 8, train for the 42-th batch, train loss: 0.4764900803565979:  29%|███▋         | 42/146 [00:05<00:16,  6.16it/s]Epoch: 6, train for the 119-th batch, train loss: 0.526573121547699:  79%|█████████▍  | 119/151 [00:20<00:06,  5.01it/s]evaluate for the 57-th batch, evaluate loss: 0.4561627209186554:  76%|█████████████▊    | 55/72 [00:04<00:01, 12.16it/s]evaluate for the 57-th batch, evaluate loss: 0.4561627209186554:  79%|██████████████▎   | 57/72 [00:04<00:01, 11.74it/s]evaluate for the 44-th batch, evaluate loss: 0.5449357628822327:  65%|███████████▋      | 43/66 [00:05<00:02,  8.98it/s]evaluate for the 44-th batch, evaluate loss: 0.5449357628822327:  67%|████████████      | 44/66 [00:05<00:02,  8.46it/s]Epoch: 9, train for the 75-th batch, train loss: 0.3839372992515564:  62%|████████     | 74/119 [00:11<00:07,  6.13it/s]Epoch: 2, train for the 148-th batch, train loss: 0.29176226258277893:  38%|███▊      | 147/383 [00:32<01:16,  3.09it/s]Epoch: 9, train for the 75-th batch, train loss: 0.3839372992515564:  63%|████████▏    | 75/119 [00:11<00:07,  6.07it/s]Epoch: 8, train for the 43-th batch, train loss: 0.45830416679382324:  29%|███▍        | 42/146 [00:06<00:16,  6.16it/s]Epoch: 8, train for the 43-th batch, train loss: 0.45830416679382324:  29%|███▌        | 43/146 [00:06<00:16,  6.24it/s]Epoch: 2, train for the 148-th batch, train loss: 0.29176226258277893:  39%|███▊      | 148/383 [00:32<01:14,  3.14it/s]evaluate for the 58-th batch, evaluate loss: 0.5177709460258484:  79%|██████████████▎   | 57/72 [00:04<00:01, 11.74it/s]evaluate for the 45-th batch, evaluate loss: 0.516502320766449:  67%|████████████▋      | 44/66 [00:05<00:02,  8.46it/s]Epoch: 6, train for the 120-th batch, train loss: 0.5054544806480408:  79%|████████▋  | 119/151 [00:20<00:06,  5.01it/s]evaluate for the 59-th batch, evaluate loss: 0.5145190954208374:  79%|██████████████▎   | 57/72 [00:04<00:01, 11.74it/s]Epoch: 6, train for the 120-th batch, train loss: 0.5054544806480408:  79%|████████▋  | 120/151 [00:20<00:06,  4.88it/s]evaluate for the 59-th batch, evaluate loss: 0.5145190954208374:  82%|██████████████▊   | 59/72 [00:04<00:01, 11.80it/s]evaluate for the 46-th batch, evaluate loss: 0.573415994644165:  67%|████████████▋      | 44/66 [00:05<00:02,  8.46it/s]evaluate for the 46-th batch, evaluate loss: 0.573415994644165:  70%|█████████████▏     | 46/66 [00:05<00:02,  9.12it/s]Epoch: 9, train for the 76-th batch, train loss: 0.39760062098503113:  63%|███████▌    | 75/119 [00:12<00:07,  6.07it/s]Epoch: 8, train for the 44-th batch, train loss: 0.4416775703430176:  29%|███▊         | 43/146 [00:06<00:16,  6.24it/s]Epoch: 8, train for the 44-th batch, train loss: 0.4416775703430176:  30%|███▉         | 44/146 [00:06<00:16,  6.21it/s]Epoch: 9, train for the 76-th batch, train loss: 0.39760062098503113:  64%|███████▋    | 76/119 [00:12<00:07,  5.74it/s]evaluate for the 60-th batch, evaluate loss: 0.4975334703922272:  82%|██████████████▊   | 59/72 [00:04<00:01, 11.80it/s]evaluate for the 47-th batch, evaluate loss: 0.5600208044052124:  70%|████████████▌     | 46/66 [00:05<00:02,  9.12it/s]evaluate for the 47-th batch, evaluate loss: 0.5600208044052124:  71%|████████████▊     | 47/66 [00:05<00:02,  9.24it/s]Epoch: 2, train for the 149-th batch, train loss: 0.34000974893569946:  39%|███▊      | 148/383 [00:32<01:14,  3.14it/s]Epoch: 2, train for the 149-th batch, train loss: 0.34000974893569946:  39%|███▉      | 149/383 [00:32<01:10,  3.32it/s]Epoch: 6, train for the 121-th batch, train loss: 0.4882315695285797:  79%|████████▋  | 120/151 [00:20<00:06,  4.88it/s]evaluate for the 61-th batch, evaluate loss: 0.46074873208999634:  82%|█████████████▉   | 59/72 [00:04<00:01, 11.80it/s]evaluate for the 61-th batch, evaluate loss: 0.46074873208999634:  85%|██████████████▍  | 61/72 [00:04<00:00, 11.36it/s]Epoch: 8, train for the 45-th batch, train loss: 0.435422420501709:  30%|████▏         | 44/146 [00:06<00:16,  6.21it/s]Epoch: 8, train for the 45-th batch, train loss: 0.435422420501709:  31%|████▎         | 45/146 [00:06<00:15,  6.55it/s]Epoch: 6, train for the 121-th batch, train loss: 0.4882315695285797:  80%|████████▊  | 121/151 [00:20<00:06,  4.75it/s]Epoch: 9, train for the 77-th batch, train loss: 0.3857051134109497:  64%|████████▎    | 76/119 [00:12<00:07,  5.74it/s]evaluate for the 48-th batch, evaluate loss: 0.5392035841941833:  71%|████████████▊     | 47/66 [00:05<00:02,  9.24it/s]Epoch: 9, train for the 77-th batch, train loss: 0.3857051134109497:  65%|████████▍    | 77/119 [00:12<00:07,  5.87it/s]evaluate for the 62-th batch, evaluate loss: 0.45783814787864685:  85%|██████████████▍  | 61/72 [00:04<00:00, 11.36it/s]Epoch: 8, train for the 46-th batch, train loss: 0.425211101770401:  31%|████▎         | 45/146 [00:06<00:15,  6.55it/s]Epoch: 8, train for the 46-th batch, train loss: 0.425211101770401:  32%|████▍         | 46/146 [00:06<00:15,  6.35it/s]evaluate for the 63-th batch, evaluate loss: 0.4251197576522827:  85%|███████████████▎  | 61/72 [00:05<00:00, 11.36it/s]evaluate for the 63-th batch, evaluate loss: 0.4251197576522827:  88%|███████████████▊  | 63/72 [00:05<00:00, 11.18it/s]Epoch: 9, train for the 78-th batch, train loss: 0.40078994631767273:  65%|███████▊    | 77/119 [00:12<00:07,  5.87it/s]Epoch: 6, train for the 122-th batch, train loss: 0.5144489407539368:  80%|████████▊  | 121/151 [00:21<00:06,  4.75it/s]Epoch: 9, train for the 78-th batch, train loss: 0.40078994631767273:  66%|███████▊    | 78/119 [00:12<00:06,  5.87it/s]Epoch: 6, train for the 122-th batch, train loss: 0.5144489407539368:  81%|████████▉  | 122/151 [00:21<00:06,  4.82it/s]Epoch: 2, train for the 150-th batch, train loss: 0.5277358293533325:  39%|████▎      | 149/383 [00:33<01:10,  3.32it/s]evaluate for the 49-th batch, evaluate loss: 0.5627649426460266:  71%|████████████▊     | 47/66 [00:05<00:02,  9.24it/s]evaluate for the 49-th batch, evaluate loss: 0.5627649426460266:  74%|█████████████▎    | 49/66 [00:05<00:02,  8.12it/s]evaluate for the 64-th batch, evaluate loss: 0.5554149150848389:  88%|███████████████▊  | 63/72 [00:05<00:00, 11.18it/s]Epoch: 2, train for the 150-th batch, train loss: 0.5277358293533325:  39%|████▎      | 150/383 [00:33<01:09,  3.35it/s]Epoch: 8, train for the 47-th batch, train loss: 0.4892764985561371:  32%|████         | 46/146 [00:06<00:15,  6.35it/s]evaluate for the 65-th batch, evaluate loss: 0.5271468162536621:  88%|███████████████▊  | 63/72 [00:05<00:00, 11.18it/s]evaluate for the 65-th batch, evaluate loss: 0.5271468162536621:  90%|████████████████▎ | 65/72 [00:05<00:00, 11.73it/s]Epoch: 8, train for the 47-th batch, train loss: 0.4892764985561371:  32%|████▏        | 47/146 [00:06<00:15,  6.39it/s]evaluate for the 50-th batch, evaluate loss: 0.5912541747093201:  74%|█████████████▎    | 49/66 [00:05<00:02,  8.12it/s]evaluate for the 50-th batch, evaluate loss: 0.5912541747093201:  76%|█████████████▋    | 50/66 [00:05<00:01,  8.28it/s]Epoch: 9, train for the 79-th batch, train loss: 0.4160865247249603:  66%|████████▌    | 78/119 [00:12<00:06,  5.87it/s]Epoch: 9, train for the 79-th batch, train loss: 0.4160865247249603:  66%|████████▋    | 79/119 [00:12<00:06,  6.12it/s]Epoch: 6, train for the 123-th batch, train loss: 0.5041021704673767:  81%|████████▉  | 122/151 [00:21<00:06,  4.82it/s]Epoch: 6, train for the 123-th batch, train loss: 0.5041021704673767:  81%|████████▉  | 123/151 [00:21<00:05,  4.94it/s]evaluate for the 66-th batch, evaluate loss: 0.4337605834007263:  90%|████████████████▎ | 65/72 [00:05<00:00, 11.73it/s]Epoch: 8, train for the 48-th batch, train loss: 0.4706605076789856:  32%|████▏        | 47/146 [00:06<00:15,  6.39it/s]Epoch: 8, train for the 48-th batch, train loss: 0.4706605076789856:  33%|████▎        | 48/146 [00:06<00:15,  6.44it/s]evaluate for the 67-th batch, evaluate loss: 0.49484118819236755:  90%|███████████████▎ | 65/72 [00:05<00:00, 11.73it/s]evaluate for the 67-th batch, evaluate loss: 0.49484118819236755:  93%|███████████████▊ | 67/72 [00:05<00:00, 12.00it/s]evaluate for the 51-th batch, evaluate loss: 0.5743550658226013:  76%|█████████████▋    | 50/66 [00:06<00:01,  8.28it/s]evaluate for the 51-th batch, evaluate loss: 0.5743550658226013:  77%|█████████████▉    | 51/66 [00:06<00:01,  7.72it/s]Epoch: 9, train for the 80-th batch, train loss: 0.40970754623413086:  66%|███████▉    | 79/119 [00:12<00:06,  6.12it/s]Epoch: 9, train for the 80-th batch, train loss: 0.40970754623413086:  67%|████████    | 80/119 [00:12<00:06,  6.25it/s]Epoch: 2, train for the 151-th batch, train loss: 0.5491251945495605:  39%|████▎      | 150/383 [00:33<01:09,  3.35it/s]Epoch: 6, train for the 124-th batch, train loss: 0.5254458785057068:  81%|████████▉  | 123/151 [00:21<00:05,  4.94it/s]evaluate for the 68-th batch, evaluate loss: 0.4602281153202057:  93%|████████████████▊ | 67/72 [00:05<00:00, 12.00it/s]evaluate for the 52-th batch, evaluate loss: 0.5411732196807861:  77%|█████████████▉    | 51/66 [00:06<00:01,  7.72it/s]Epoch: 2, train for the 151-th batch, train loss: 0.5491251945495605:  39%|████▎      | 151/383 [00:33<01:10,  3.29it/s]Epoch: 6, train for the 124-th batch, train loss: 0.5254458785057068:  82%|█████████  | 124/151 [00:21<00:05,  5.05it/s]evaluate for the 69-th batch, evaluate loss: 0.4839923083782196:  93%|████████████████▊ | 67/72 [00:05<00:00, 12.00it/s]evaluate for the 69-th batch, evaluate loss: 0.4839923083782196:  96%|█████████████████▎| 69/72 [00:05<00:00, 12.62it/s]Epoch: 8, train for the 49-th batch, train loss: 0.48254838585853577:  33%|███▉        | 48/146 [00:07<00:15,  6.44it/s]Epoch: 9, train for the 81-th batch, train loss: 0.39796873927116394:  67%|████████    | 80/119 [00:12<00:06,  6.25it/s]Epoch: 8, train for the 49-th batch, train loss: 0.48254838585853577:  34%|████        | 49/146 [00:07<00:15,  6.17it/s]Epoch: 9, train for the 81-th batch, train loss: 0.39796873927116394:  68%|████████▏   | 81/119 [00:12<00:06,  6.29it/s]evaluate for the 53-th batch, evaluate loss: 0.5013058185577393:  77%|█████████████▉    | 51/66 [00:06<00:01,  7.72it/s]evaluate for the 53-th batch, evaluate loss: 0.5013058185577393:  80%|██████████████▍   | 53/66 [00:06<00:01,  8.09it/s]evaluate for the 70-th batch, evaluate loss: 0.5139132738113403:  96%|█████████████████▎| 69/72 [00:05<00:00, 12.62it/s]Epoch: 6, train for the 125-th batch, train loss: 0.4675484299659729:  82%|█████████  | 124/151 [00:21<00:05,  5.05it/s]Epoch: 6, train for the 125-th batch, train loss: 0.4675484299659729:  83%|█████████  | 125/151 [00:21<00:05,  5.07it/s]evaluate for the 71-th batch, evaluate loss: 0.5430173873901367:  96%|█████████████████▎| 69/72 [00:05<00:00, 12.62it/s]evaluate for the 71-th batch, evaluate loss: 0.5430173873901367:  99%|█████████████████▊| 71/72 [00:05<00:00, 12.17it/s]Epoch: 2, train for the 152-th batch, train loss: 0.3596857488155365:  39%|████▎      | 151/383 [00:33<01:10,  3.29it/s]Epoch: 8, train for the 50-th batch, train loss: 0.543427050113678:  34%|████▋         | 49/146 [00:07<00:15,  6.17it/s]Epoch: 8, train for the 50-th batch, train loss: 0.543427050113678:  34%|████▊         | 50/146 [00:07<00:15,  6.15it/s]evaluate for the 54-th batch, evaluate loss: 0.5615584254264832:  80%|██████████████▍   | 53/66 [00:06<00:01,  8.09it/s]evaluate for the 54-th batch, evaluate loss: 0.5615584254264832:  82%|██████████████▋   | 54/66 [00:06<00:01,  8.28it/s]Epoch: 9, train for the 82-th batch, train loss: 0.40006399154663086:  68%|████████▏   | 81/119 [00:13<00:06,  6.29it/s]Epoch: 9, train for the 82-th batch, train loss: 0.40006399154663086:  69%|████████▎   | 82/119 [00:13<00:06,  5.90it/s]Epoch: 2, train for the 152-th batch, train loss: 0.3596857488155365:  40%|████▎      | 152/383 [00:33<01:07,  3.42it/s]evaluate for the 72-th batch, evaluate loss: 0.5568611025810242:  99%|█████████████████▊| 71/72 [00:05<00:00, 12.17it/s]evaluate for the 72-th batch, evaluate loss: 0.5568611025810242: 100%|██████████████████| 72/72 [00:05<00:00, 12.53it/s]
evaluate for the 55-th batch, evaluate loss: 0.5104689598083496:  82%|██████████████▋   | 54/66 [00:06<00:01,  8.28it/s]evaluate for the 55-th batch, evaluate loss: 0.5104689598083496:  83%|███████████████   | 55/66 [00:06<00:01,  8.01it/s]Epoch: 8, train for the 51-th batch, train loss: 0.45714834332466125:  34%|████        | 50/146 [00:07<00:15,  6.15it/s]Epoch: 8, train for the 51-th batch, train loss: 0.45714834332466125:  35%|████▏       | 51/146 [00:07<00:15,  6.05it/s]Epoch: 6, train for the 126-th batch, train loss: 0.4673387110233307:  83%|█████████  | 125/151 [00:21<00:05,  5.07it/s]Epoch: 9, train for the 83-th batch, train loss: 0.4431268274784088:  69%|████████▉    | 82/119 [00:13<00:06,  5.90it/s]Epoch: 9, train for the 83-th batch, train loss: 0.4431268274784088:  70%|█████████    | 83/119 [00:13<00:06,  5.99it/s]Epoch: 6, train for the 126-th batch, train loss: 0.4673387110233307:  83%|█████████▏ | 126/151 [00:21<00:05,  4.84it/s]Epoch: 2, train for the 153-th batch, train loss: 0.40361762046813965:  40%|███▉      | 152/383 [00:33<01:07,  3.42it/s]evaluate for the 56-th batch, evaluate loss: 0.5216271877288818:  83%|███████████████   | 55/66 [00:06<00:01,  8.01it/s]evaluate for the 56-th batch, evaluate loss: 0.5216271877288818:  85%|███████████████▎  | 56/66 [00:06<00:01,  8.41it/s]Epoch: 2, train for the 153-th batch, train loss: 0.40361762046813965:  40%|███▉      | 153/383 [00:34<01:04,  3.55it/s]Epoch: 8, train for the 52-th batch, train loss: 0.5227419137954712:  35%|████▌        | 51/146 [00:07<00:15,  6.05it/s]Epoch: 8, train for the 52-th batch, train loss: 0.5227419137954712:  36%|████▋        | 52/146 [00:07<00:15,  6.24it/s]Epoch: 9, train for the 84-th batch, train loss: 0.41176047921180725:  70%|████████▎   | 83/119 [00:13<00:06,  5.99it/s]Epoch: 9, train for the 84-th batch, train loss: 0.41176047921180725:  71%|████████▍   | 84/119 [00:13<00:05,  6.17it/s]evaluate for the 57-th batch, evaluate loss: 0.6066576838493347:  85%|███████████████▎  | 56/66 [00:06<00:01,  8.41it/s]evaluate for the 57-th batch, evaluate loss: 0.6066576838493347:  86%|███████████████▌  | 57/66 [00:06<00:01,  8.52it/s]Epoch: 6, train for the 127-th batch, train loss: 0.46637770533561707:  83%|████████▎ | 126/151 [00:22<00:05,  4.84it/s]Epoch: 6, train for the 127-th batch, train loss: 0.46637770533561707:  84%|████████▍ | 127/151 [00:22<00:04,  4.88it/s]Epoch: 2, train for the 154-th batch, train loss: 0.38556861877441406:  40%|███▉      | 153/383 [00:34<01:04,  3.55it/s]Epoch: 8, train for the 53-th batch, train loss: 0.48961615562438965:  36%|████▎       | 52/146 [00:07<00:15,  6.24it/s]evaluate for the 58-th batch, evaluate loss: 0.5703836679458618:  86%|███████████████▌  | 57/66 [00:06<00:01,  8.52it/s]evaluate for the 58-th batch, evaluate loss: 0.5703836679458618:  88%|███████████████▊  | 58/66 [00:06<00:00,  8.16it/s]Epoch: 9, train for the 85-th batch, train loss: 0.42077529430389404:  71%|████████▍   | 84/119 [00:13<00:05,  6.17it/s]Epoch: 8, train for the 53-th batch, train loss: 0.48961615562438965:  36%|████▎       | 53/146 [00:07<00:15,  5.95it/s]Epoch: 9, train for the 85-th batch, train loss: 0.42077529430389404:  71%|████████▌   | 85/119 [00:13<00:05,  6.14it/s]Epoch: 6, train for the 128-th batch, train loss: 0.4959101676940918:  84%|█████████▎ | 127/151 [00:22<00:04,  4.88it/s]Epoch: 2, train for the 154-th batch, train loss: 0.38556861877441406:  40%|████      | 154/383 [00:34<01:01,  3.73it/s]Epoch: 6, train for the 128-th batch, train loss: 0.4959101676940918:  85%|█████████▎ | 128/151 [00:22<00:04,  5.45it/s]  0%|                                                                                            | 0/34 [00:00<?, ?it/s]evaluate for the 59-th batch, evaluate loss: 0.5795506238937378:  88%|███████████████▊  | 58/66 [00:07<00:00,  8.16it/s]evaluate for the 59-th batch, evaluate loss: 0.5795506238937378:  89%|████████████████  | 59/66 [00:07<00:00,  8.55it/s]evaluate for the 1-th batch, evaluate loss: 0.6075774431228638:   0%|                            | 0/34 [00:00<?, ?it/s]Epoch: 8, train for the 54-th batch, train loss: 0.5313586592674255:  36%|████▋        | 53/146 [00:07<00:15,  5.95it/s]Epoch: 8, train for the 54-th batch, train loss: 0.5313586592674255:  37%|████▊        | 54/146 [00:07<00:15,  6.01it/s]Epoch: 9, train for the 86-th batch, train loss: 0.40458664298057556:  71%|████████▌   | 85/119 [00:13<00:05,  6.14it/s]Epoch: 9, train for the 86-th batch, train loss: 0.40458664298057556:  72%|████████▋   | 86/119 [00:13<00:05,  5.94it/s]Epoch: 6, train for the 129-th batch, train loss: 0.5160413980484009:  85%|█████████▎ | 128/151 [00:22<00:04,  5.45it/s]Epoch: 2, train for the 155-th batch, train loss: 0.29266685247421265:  40%|████      | 154/383 [00:34<01:01,  3.73it/s]evaluate for the 2-th batch, evaluate loss: 0.6791505217552185:   0%|                            | 0/34 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.6791505217552185:   6%|█▏                  | 2/34 [00:00<00:02, 12.75it/s]evaluate for the 60-th batch, evaluate loss: 0.5599507689476013:  89%|████████████████  | 59/66 [00:07<00:00,  8.55it/s]evaluate for the 60-th batch, evaluate loss: 0.5599507689476013:  91%|████████████████▎ | 60/66 [00:07<00:00,  8.12it/s]Epoch: 6, train for the 129-th batch, train loss: 0.5160413980484009:  85%|█████████▍ | 129/151 [00:22<00:04,  5.13it/s]Epoch: 2, train for the 155-th batch, train loss: 0.29266685247421265:  40%|████      | 155/383 [00:34<00:59,  3.84it/s]evaluate for the 3-th batch, evaluate loss: 0.6747077703475952:   6%|█▏                  | 2/34 [00:00<00:02, 12.75it/s]Epoch: 8, train for the 55-th batch, train loss: 0.5342890620231628:  37%|████▊        | 54/146 [00:08<00:15,  6.01it/s]Epoch: 8, train for the 55-th batch, train loss: 0.5342890620231628:  38%|████▉        | 55/146 [00:08<00:14,  6.19it/s]Epoch: 9, train for the 87-th batch, train loss: 0.476178914308548:  72%|██████████    | 86/119 [00:13<00:05,  5.94it/s]evaluate for the 61-th batch, evaluate loss: 0.5837028622627258:  91%|████████████████▎ | 60/66 [00:07<00:00,  8.12it/s]evaluate for the 61-th batch, evaluate loss: 0.5837028622627258:  92%|████████████████▋ | 61/66 [00:07<00:00,  8.22it/s]Epoch: 9, train for the 87-th batch, train loss: 0.476178914308548:  73%|██████████▏   | 87/119 [00:13<00:05,  5.93it/s]Epoch: 8, train for the 56-th batch, train loss: 0.5322380065917969:  38%|████▉        | 55/146 [00:08<00:14,  6.19it/s]Epoch: 6, train for the 130-th batch, train loss: 0.43720823526382446:  85%|████████▌ | 129/151 [00:22<00:04,  5.13it/s]Epoch: 8, train for the 56-th batch, train loss: 0.5322380065917969:  38%|████▉        | 56/146 [00:08<00:13,  6.68it/s]evaluate for the 4-th batch, evaluate loss: 0.6696328520774841:   6%|█▏                  | 2/34 [00:00<00:02, 12.75it/s]evaluate for the 4-th batch, evaluate loss: 0.6696328520774841:  12%|██▎                 | 4/34 [00:00<00:02, 10.73it/s]Epoch: 6, train for the 130-th batch, train loss: 0.43720823526382446:  86%|████████▌ | 130/151 [00:22<00:04,  5.01it/s]Epoch: 2, train for the 156-th batch, train loss: 0.27393755316734314:  40%|████      | 155/383 [00:34<00:59,  3.84it/s]Epoch: 9, train for the 88-th batch, train loss: 0.40333202481269836:  73%|████████▊   | 87/119 [00:13<00:05,  5.93it/s]evaluate for the 62-th batch, evaluate loss: 0.5891973972320557:  92%|████████████████▋ | 61/66 [00:07<00:00,  8.22it/s]evaluate for the 62-th batch, evaluate loss: 0.5891973972320557:  94%|████████████████▉ | 62/66 [00:07<00:00,  7.90it/s]evaluate for the 5-th batch, evaluate loss: 0.6480211019515991:  12%|██▎                 | 4/34 [00:00<00:02, 10.73it/s]Epoch: 9, train for the 88-th batch, train loss: 0.40333202481269836:  74%|████████▊   | 88/119 [00:14<00:05,  6.09it/s]Epoch: 2, train for the 156-th batch, train loss: 0.27393755316734314:  41%|████      | 156/383 [00:34<00:59,  3.80it/s]Epoch: 8, train for the 57-th batch, train loss: 0.5193778276443481:  38%|████▉        | 56/146 [00:08<00:13,  6.68it/s]Epoch: 8, train for the 57-th batch, train loss: 0.5193778276443481:  39%|█████        | 57/146 [00:08<00:12,  7.24it/s]evaluate for the 6-th batch, evaluate loss: 0.6187376379966736:  12%|██▎                 | 4/34 [00:00<00:02, 10.73it/s]evaluate for the 6-th batch, evaluate loss: 0.6187376379966736:  18%|███▌                | 6/34 [00:00<00:02, 11.79it/s]evaluate for the 63-th batch, evaluate loss: 0.5550675988197327:  94%|████████████████▉ | 62/66 [00:07<00:00,  7.90it/s]evaluate for the 63-th batch, evaluate loss: 0.5550675988197327:  95%|█████████████████▏| 63/66 [00:07<00:00,  7.63it/s]Epoch: 6, train for the 131-th batch, train loss: 0.5149576663970947:  86%|█████████▍ | 130/151 [00:22<00:04,  5.01it/s]evaluate for the 7-th batch, evaluate loss: 0.5858458876609802:  18%|███▌                | 6/34 [00:00<00:02, 11.79it/s]Epoch: 6, train for the 131-th batch, train loss: 0.5149576663970947:  87%|█████████▌ | 131/151 [00:22<00:04,  4.96it/s]Epoch: 9, train for the 89-th batch, train loss: 0.432539165019989:  74%|██████████▎   | 88/119 [00:14<00:05,  6.09it/s]Epoch: 9, train for the 89-th batch, train loss: 0.432539165019989:  75%|██████████▍   | 89/119 [00:14<00:05,  5.64it/s]Epoch: 8, train for the 58-th batch, train loss: 0.5027573108673096:  39%|█████        | 57/146 [00:08<00:12,  7.24it/s]Epoch: 8, train for the 58-th batch, train loss: 0.5027573108673096:  40%|█████▏       | 58/146 [00:08<00:13,  6.43it/s]evaluate for the 8-th batch, evaluate loss: 0.6736984252929688:  18%|███▌                | 6/34 [00:00<00:02, 11.79it/s]evaluate for the 8-th batch, evaluate loss: 0.6736984252929688:  24%|████▋               | 8/34 [00:00<00:02, 12.09it/s]evaluate for the 64-th batch, evaluate loss: 0.5398881435394287:  95%|█████████████████▏| 63/66 [00:07<00:00,  7.63it/s]evaluate for the 64-th batch, evaluate loss: 0.5398881435394287:  97%|█████████████████▍| 64/66 [00:07<00:00,  7.57it/s]Epoch: 2, train for the 157-th batch, train loss: 0.30500370264053345:  41%|████      | 156/383 [00:35<00:59,  3.80it/s]Epoch: 2, train for the 157-th batch, train loss: 0.30500370264053345:  41%|████      | 157/383 [00:35<01:02,  3.64it/s]Epoch: 9, train for the 90-th batch, train loss: 0.4174751937389374:  75%|█████████▋   | 89/119 [00:14<00:05,  5.64it/s]Epoch: 6, train for the 132-th batch, train loss: 0.4923660159111023:  87%|█████████▌ | 131/151 [00:23<00:04,  4.96it/s]evaluate for the 9-th batch, evaluate loss: 0.5857464671134949:  24%|████▋               | 8/34 [00:00<00:02, 12.09it/s]Epoch: 9, train for the 90-th batch, train loss: 0.4174751937389374:  76%|█████████▊   | 90/119 [00:14<00:04,  6.06it/s]Epoch: 6, train for the 132-th batch, train loss: 0.4923660159111023:  87%|█████████▌ | 132/151 [00:23<00:03,  4.96it/s]evaluate for the 65-th batch, evaluate loss: 0.5880508422851562:  97%|█████████████████▍| 64/66 [00:07<00:00,  7.57it/s]evaluate for the 65-th batch, evaluate loss: 0.5880508422851562:  98%|█████████████████▋| 65/66 [00:07<00:00,  7.86it/s]evaluate for the 10-th batch, evaluate loss: 0.6443337202072144:  24%|████▍              | 8/34 [00:00<00:02, 12.09it/s]evaluate for the 10-th batch, evaluate loss: 0.6443337202072144:  29%|█████▎            | 10/34 [00:00<00:02, 11.54it/s]evaluate for the 11-th batch, evaluate loss: 0.6652212142944336:  29%|█████▎            | 10/34 [00:00<00:02, 11.54it/s]evaluate for the 66-th batch, evaluate loss: 0.5511202216148376:  98%|█████████████████▋| 65/66 [00:07<00:00,  7.86it/s]evaluate for the 66-th batch, evaluate loss: 0.5511202216148376: 100%|██████████████████| 66/66 [00:07<00:00,  7.85it/s]evaluate for the 66-th batch, evaluate loss: 0.5511202216148376: 100%|██████████████████| 66/66 [00:07<00:00,  8.25it/s]
Epoch: 8, train for the 59-th batch, train loss: 0.43767333030700684:  40%|████▊       | 58/146 [00:08<00:13,  6.43it/s]Epoch: 9, train for the 91-th batch, train loss: 0.3780585825443268:  76%|█████████▊   | 90/119 [00:14<00:04,  6.06it/s]Epoch: 8, train for the 59-th batch, train loss: 0.43767333030700684:  40%|████▊       | 59/146 [00:08<00:17,  5.12it/s]Epoch: 9, train for the 91-th batch, train loss: 0.3780585825443268:  76%|█████████▉   | 91/119 [00:14<00:04,  5.78it/s]evaluate for the 12-th batch, evaluate loss: 0.6361162066459656:  29%|█████▎            | 10/34 [00:00<00:02, 11.54it/s]evaluate for the 12-th batch, evaluate loss: 0.6361162066459656:  35%|██████▎           | 12/34 [00:00<00:01, 12.83it/s]Epoch: 2, train for the 158-th batch, train loss: 0.39325690269470215:  41%|████      | 157/383 [00:35<01:02,  3.64it/s]Epoch: 2, train for the 158-th batch, train loss: 0.39325690269470215:  41%|████▏     | 158/383 [00:35<01:02,  3.59it/s]evaluate for the 13-th batch, evaluate loss: 0.5708400011062622:  35%|██████▎           | 12/34 [00:01<00:01, 12.83it/s]Epoch: 8, train for the 60-th batch, train loss: 0.5071771740913391:  40%|█████▎       | 59/146 [00:08<00:17,  5.12it/s]Epoch: 8, train for the 60-th batch, train loss: 0.5071771740913391:  41%|█████▎       | 60/146 [00:08<00:15,  5.72it/s]Epoch: 6, train for the 133-th batch, train loss: 0.4535042345523834:  87%|█████████▌ | 132/151 [00:23<00:03,  4.96it/s]Epoch: 9, train for the 92-th batch, train loss: 0.4201720058917999:  76%|█████████▉   | 91/119 [00:14<00:04,  5.78it/s]Epoch: 6, train for the 133-th batch, train loss: 0.4535042345523834:  88%|█████████▋ | 133/151 [00:23<00:04,  4.20it/s]Epoch: 9, train for the 92-th batch, train loss: 0.4201720058917999:  77%|██████████   | 92/119 [00:14<00:04,  5.92it/s]evaluate for the 14-th batch, evaluate loss: 0.5631383657455444:  35%|██████▎           | 12/34 [00:01<00:01, 12.83it/s]evaluate for the 14-th batch, evaluate loss: 0.5631383657455444:  41%|███████▍          | 14/34 [00:01<00:01, 12.37it/s]evaluate for the 15-th batch, evaluate loss: 0.6956130266189575:  41%|███████▍          | 14/34 [00:01<00:01, 12.37it/s]Epoch: 2, train for the 159-th batch, train loss: 0.43762168288230896:  41%|████▏     | 158/383 [00:35<01:02,  3.59it/s]Epoch: 8, train for the 61-th batch, train loss: 0.5657931566238403:  41%|█████▎       | 60/146 [00:09<00:15,  5.72it/s]Epoch: 8, train for the 61-th batch, train loss: 0.5657931566238403:  42%|█████▍       | 61/146 [00:09<00:14,  5.79it/s]Epoch: 2, train for the 159-th batch, train loss: 0.43762168288230896:  42%|████▏     | 159/383 [00:35<00:58,  3.83it/s]Epoch: 9, train for the 93-th batch, train loss: 0.35388171672821045:  77%|█████████▎  | 92/119 [00:14<00:04,  5.92it/s]Epoch: 9, train for the 93-th batch, train loss: 0.35388171672821045:  78%|█████████▍  | 93/119 [00:14<00:04,  6.10it/s]Epoch: 6, train for the 134-th batch, train loss: 0.4613141417503357:  88%|█████████▋ | 133/151 [00:23<00:04,  4.20it/s]evaluate for the 16-th batch, evaluate loss: 0.6160735487937927:  41%|███████▍          | 14/34 [00:01<00:01, 12.37it/s]evaluate for the 16-th batch, evaluate loss: 0.6160735487937927:  47%|████████▍         | 16/34 [00:01<00:01, 12.59it/s]Epoch: 6, train for the 134-th batch, train loss: 0.4613141417503357:  89%|█████████▊ | 134/151 [00:23<00:03,  4.28it/s]evaluate for the 17-th batch, evaluate loss: 0.6267069578170776:  47%|████████▍         | 16/34 [00:01<00:01, 12.59it/s]Epoch: 8, train for the 62-th batch, train loss: 0.44537994265556335:  42%|█████       | 61/146 [00:09<00:14,  5.79it/s]Epoch: 9, train for the 94-th batch, train loss: 0.34235450625419617:  78%|█████████▍  | 93/119 [00:14<00:04,  6.10it/s]Epoch: 8, train for the 62-th batch, train loss: 0.44537994265556335:  42%|█████       | 62/146 [00:09<00:14,  5.87it/s]Epoch: 9, train for the 94-th batch, train loss: 0.34235450625419617:  79%|█████████▍  | 94/119 [00:14<00:03,  6.39it/s]Epoch: 2, train for the 160-th batch, train loss: 0.43371260166168213:  42%|████▏     | 159/383 [00:35<00:58,  3.83it/s]Epoch: 2, train for the 160-th batch, train loss: 0.43371260166168213:  42%|████▏     | 160/383 [00:35<00:56,  3.93it/s]evaluate for the 18-th batch, evaluate loss: 0.5932460427284241:  47%|████████▍         | 16/34 [00:01<00:01, 12.59it/s]Epoch: 6, train for the 135-th batch, train loss: 0.47343936562538147:  89%|████████▊ | 134/151 [00:23<00:03,  4.28it/s]evaluate for the 18-th batch, evaluate loss: 0.5932460427284241:  53%|█████████▌        | 18/34 [00:01<00:01, 11.72it/s]Epoch: 6, train for the 135-th batch, train loss: 0.47343936562538147:  89%|████████▉ | 135/151 [00:23<00:03,  4.53it/s]Epoch: 8, train for the 63-th batch, train loss: 0.4713943302631378:  42%|█████▌       | 62/146 [00:09<00:14,  5.87it/s]Epoch: 8, train for the 63-th batch, train loss: 0.4713943302631378:  43%|█████▌       | 63/146 [00:09<00:13,  6.10it/s]evaluate for the 19-th batch, evaluate loss: 0.552838146686554:  53%|██████████         | 18/34 [00:01<00:01, 11.72it/s]Epoch: 9, train for the 95-th batch, train loss: 0.35400184988975525:  79%|█████████▍  | 94/119 [00:15<00:03,  6.39it/s]Epoch: 9, train for the 95-th batch, train loss: 0.35400184988975525:  80%|█████████▌  | 95/119 [00:15<00:04,  5.98it/s]evaluate for the 20-th batch, evaluate loss: 0.6370114088058472:  53%|█████████▌        | 18/34 [00:01<00:01, 11.72it/s]evaluate for the 20-th batch, evaluate loss: 0.6370114088058472:  59%|██████████▌       | 20/34 [00:01<00:01, 12.70it/s]Epoch: 8, train for the 64-th batch, train loss: 0.4980241358280182:  43%|█████▌       | 63/146 [00:09<00:13,  6.10it/s]Epoch: 6, train for the 136-th batch, train loss: 0.47846561670303345:  89%|████████▉ | 135/151 [00:23<00:03,  4.53it/s]Epoch: 8, train for the 64-th batch, train loss: 0.4980241358280182:  44%|█████▋       | 64/146 [00:09<00:12,  6.38it/s]evaluate for the 21-th batch, evaluate loss: 0.6361562013626099:  59%|██████████▌       | 20/34 [00:01<00:01, 12.70it/s]Epoch: 6, train for the 136-th batch, train loss: 0.47846561670303345:  90%|█████████ | 136/151 [00:24<00:03,  4.65it/s]  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 9, train for the 96-th batch, train loss: 0.36705663800239563:  80%|█████████▌  | 95/119 [00:15<00:04,  5.98it/s]Epoch: 2, train for the 161-th batch, train loss: 0.4682823717594147:  42%|████▌      | 160/383 [00:36<00:56,  3.93it/s]Epoch: 9, train for the 96-th batch, train loss: 0.36705663800239563:  81%|█████████▋  | 96/119 [00:15<00:03,  6.24it/s]evaluate for the 22-th batch, evaluate loss: 0.3598982095718384:  59%|██████████▌       | 20/34 [00:01<00:01, 12.70it/s]evaluate for the 22-th batch, evaluate loss: 0.3598982095718384:  65%|███████████▋      | 22/34 [00:01<00:00, 13.21it/s]Epoch: 2, train for the 161-th batch, train loss: 0.4682823717594147:  42%|████▌      | 161/383 [00:36<00:58,  3.80it/s]Epoch: 8, train for the 65-th batch, train loss: 0.5359954833984375:  44%|█████▋       | 64/146 [00:09<00:12,  6.38it/s]Epoch: 8, train for the 65-th batch, train loss: 0.5359954833984375:  45%|█████▊       | 65/146 [00:09<00:12,  6.61it/s]evaluate for the 23-th batch, evaluate loss: 0.33248594403266907:  65%|███████████      | 22/34 [00:01<00:00, 13.21it/s]evaluate for the 1-th batch, evaluate loss: 0.6622515916824341:   0%|                            | 0/40 [00:00<?, ?it/s]evaluate for the 1-th batch, evaluate loss: 0.6622515916824341:   2%|▌                   | 1/40 [00:00<00:04,  8.50it/s]Epoch: 9, train for the 97-th batch, train loss: 0.42570066452026367:  81%|█████████▋  | 96/119 [00:15<00:03,  6.24it/s]Epoch: 6, train for the 137-th batch, train loss: 0.4531294107437134:  90%|█████████▉ | 136/151 [00:24<00:03,  4.65it/s]Epoch: 9, train for the 97-th batch, train loss: 0.42570066452026367:  82%|█████████▊  | 97/119 [00:15<00:03,  6.34it/s]Epoch: 6, train for the 137-th batch, train loss: 0.4531294107437134:  91%|█████████▉ | 137/151 [00:24<00:02,  4.91it/s]evaluate for the 24-th batch, evaluate loss: 0.5090088844299316:  65%|███████████▋      | 22/34 [00:01<00:00, 13.21it/s]evaluate for the 24-th batch, evaluate loss: 0.5090088844299316:  71%|████████████▋     | 24/34 [00:01<00:00, 12.93it/s]Epoch: 8, train for the 66-th batch, train loss: 0.5308950543403625:  45%|█████▊       | 65/146 [00:09<00:12,  6.61it/s]Epoch: 8, train for the 66-th batch, train loss: 0.5308950543403625:  45%|█████▉       | 66/146 [00:09<00:12,  6.50it/s]evaluate for the 25-th batch, evaluate loss: 0.515593409538269:  71%|█████████████▍     | 24/34 [00:02<00:00, 12.93it/s]Epoch: 2, train for the 162-th batch, train loss: 0.41976919770240784:  42%|████▏     | 161/383 [00:36<00:58,  3.80it/s]evaluate for the 2-th batch, evaluate loss: 0.6597119569778442:   2%|▌                   | 1/40 [00:00<00:04,  8.50it/s]evaluate for the 2-th batch, evaluate loss: 0.6597119569778442:   5%|█                   | 2/40 [00:00<00:05,  6.53it/s]Epoch: 9, train for the 98-th batch, train loss: 0.3705129623413086:  82%|██████████▌  | 97/119 [00:15<00:03,  6.34it/s]Epoch: 9, train for the 98-th batch, train loss: 0.3705129623413086:  82%|██████████▋  | 98/119 [00:15<00:03,  6.45it/s]Epoch: 2, train for the 162-th batch, train loss: 0.41976919770240784:  42%|████▏     | 162/383 [00:36<00:59,  3.72it/s]Epoch: 6, train for the 138-th batch, train loss: 0.4834822714328766:  91%|█████████▉ | 137/151 [00:24<00:02,  4.91it/s]evaluate for the 26-th batch, evaluate loss: 0.6319231390953064:  71%|████████████▋     | 24/34 [00:02<00:00, 12.93it/s]evaluate for the 26-th batch, evaluate loss: 0.6319231390953064:  76%|█████████████▊    | 26/34 [00:02<00:00, 12.69it/s]Epoch: 6, train for the 138-th batch, train loss: 0.4834822714328766:  91%|██████████ | 138/151 [00:24<00:02,  4.84it/s]Epoch: 8, train for the 67-th batch, train loss: 0.5113918781280518:  45%|█████▉       | 66/146 [00:09<00:12,  6.50it/s]Epoch: 8, train for the 67-th batch, train loss: 0.5113918781280518:  46%|█████▉       | 67/146 [00:09<00:11,  6.64it/s]evaluate for the 3-th batch, evaluate loss: 0.6404531598091125:   5%|█                   | 2/40 [00:00<00:05,  6.53it/s]evaluate for the 3-th batch, evaluate loss: 0.6404531598091125:   8%|█▌                  | 3/40 [00:00<00:05,  6.72it/s]evaluate for the 27-th batch, evaluate loss: 0.6578269600868225:  76%|█████████████▊    | 26/34 [00:02<00:00, 12.69it/s]Epoch: 9, train for the 99-th batch, train loss: 0.3386664390563965:  82%|██████████▋  | 98/119 [00:15<00:03,  6.45it/s]Epoch: 9, train for the 99-th batch, train loss: 0.3386664390563965:  83%|██████████▊  | 99/119 [00:15<00:03,  6.17it/s]Epoch: 8, train for the 68-th batch, train loss: 0.46216174960136414:  46%|█████▌      | 67/146 [00:10<00:11,  6.64it/s]Epoch: 8, train for the 68-th batch, train loss: 0.46216174960136414:  47%|█████▌      | 68/146 [00:10<00:11,  7.00it/s]Epoch: 2, train for the 163-th batch, train loss: 0.3346998989582062:  42%|████▋      | 162/383 [00:36<00:59,  3.72it/s]evaluate for the 28-th batch, evaluate loss: 0.5776103734970093:  76%|█████████████▊    | 26/34 [00:02<00:00, 12.69it/s]evaluate for the 28-th batch, evaluate loss: 0.5776103734970093:  82%|██████████████▊   | 28/34 [00:02<00:00, 12.04it/s]evaluate for the 4-th batch, evaluate loss: 0.6717306971549988:   8%|█▌                  | 3/40 [00:00<00:05,  6.72it/s]evaluate for the 4-th batch, evaluate loss: 0.6717306971549988:  10%|██                  | 4/40 [00:00<00:04,  7.40it/s]Epoch: 2, train for the 163-th batch, train loss: 0.3346998989582062:  43%|████▋      | 163/383 [00:36<00:56,  3.86it/s]Epoch: 6, train for the 139-th batch, train loss: 0.5104284882545471:  91%|██████████ | 138/151 [00:24<00:02,  4.84it/s]evaluate for the 29-th batch, evaluate loss: 0.5793179869651794:  82%|██████████████▊   | 28/34 [00:02<00:00, 12.04it/s]Epoch: 6, train for the 139-th batch, train loss: 0.5104284882545471:  92%|██████████▏| 139/151 [00:24<00:02,  4.73it/s]Epoch: 9, train for the 100-th batch, train loss: 0.38462355732917786:  83%|█████████▏ | 99/119 [00:15<00:03,  6.17it/s]Epoch: 9, train for the 100-th batch, train loss: 0.38462355732917786:  84%|████████▍ | 100/119 [00:15<00:02,  6.44it/s]Epoch: 8, train for the 69-th batch, train loss: 0.5087337493896484:  47%|██████       | 68/146 [00:10<00:11,  7.00it/s]evaluate for the 5-th batch, evaluate loss: 0.6164191961288452:  10%|██                  | 4/40 [00:00<00:04,  7.40it/s]evaluate for the 5-th batch, evaluate loss: 0.6164191961288452:  12%|██▌                 | 5/40 [00:00<00:04,  7.76it/s]Epoch: 8, train for the 69-th batch, train loss: 0.5087337493896484:  47%|██████▏      | 69/146 [00:10<00:10,  7.01it/s]evaluate for the 30-th batch, evaluate loss: 0.5480273962020874:  82%|██████████████▊   | 28/34 [00:02<00:00, 12.04it/s]evaluate for the 30-th batch, evaluate loss: 0.5480273962020874:  88%|███████████████▉  | 30/34 [00:02<00:00, 12.40it/s]Epoch: 2, train for the 164-th batch, train loss: 0.34511321783065796:  43%|████▎     | 163/383 [00:36<00:56,  3.86it/s]Epoch: 9, train for the 101-th batch, train loss: 0.39481619000434875:  84%|████████▍ | 100/119 [00:16<00:02,  6.44it/s]Epoch: 9, train for the 101-th batch, train loss: 0.39481619000434875:  85%|████████▍ | 101/119 [00:16<00:02,  6.73it/s]evaluate for the 6-th batch, evaluate loss: 0.6215420365333557:  12%|██▌                 | 5/40 [00:00<00:04,  7.76it/s]evaluate for the 6-th batch, evaluate loss: 0.6215420365333557:  15%|███                 | 6/40 [00:00<00:04,  8.19it/s]Epoch: 2, train for the 164-th batch, train loss: 0.34511321783065796:  43%|████▎     | 164/383 [00:36<00:53,  4.08it/s]Epoch: 6, train for the 140-th batch, train loss: 0.4440239667892456:  92%|██████████▏| 139/151 [00:24<00:02,  4.73it/s]evaluate for the 31-th batch, evaluate loss: 0.6357266306877136:  88%|███████████████▉  | 30/34 [00:02<00:00, 12.40it/s]Epoch: 8, train for the 70-th batch, train loss: 0.5019109845161438:  47%|██████▏      | 69/146 [00:10<00:10,  7.01it/s]Epoch: 8, train for the 70-th batch, train loss: 0.5019109845161438:  48%|██████▏      | 70/146 [00:10<00:10,  6.92it/s]Epoch: 6, train for the 140-th batch, train loss: 0.4440239667892456:  93%|██████████▏| 140/151 [00:24<00:02,  4.64it/s]evaluate for the 32-th batch, evaluate loss: 0.6181859970092773:  88%|███████████████▉  | 30/34 [00:02<00:00, 12.40it/s]evaluate for the 32-th batch, evaluate loss: 0.6181859970092773:  94%|████████████████▉ | 32/34 [00:02<00:00, 12.03it/s]evaluate for the 7-th batch, evaluate loss: 0.6378365755081177:  15%|███                 | 6/40 [00:00<00:04,  8.19it/s]evaluate for the 7-th batch, evaluate loss: 0.6378365755081177:  18%|███▌                | 7/40 [00:00<00:03,  8.43it/s]Epoch: 9, train for the 102-th batch, train loss: 0.4213336706161499:  85%|█████████▎ | 101/119 [00:16<00:02,  6.73it/s]Epoch: 9, train for the 102-th batch, train loss: 0.4213336706161499:  86%|█████████▍ | 102/119 [00:16<00:02,  6.80it/s]evaluate for the 33-th batch, evaluate loss: 0.6356408596038818:  94%|████████████████▉ | 32/34 [00:02<00:00, 12.03it/s]Epoch: 8, train for the 71-th batch, train loss: 0.5564544796943665:  48%|██████▏      | 70/146 [00:10<00:10,  6.92it/s]Epoch: 8, train for the 71-th batch, train loss: 0.5564544796943665:  49%|██████▎      | 71/146 [00:10<00:11,  6.71it/s]Epoch: 6, train for the 141-th batch, train loss: 0.4990488290786743:  93%|██████████▏| 140/151 [00:25<00:02,  4.64it/s]Epoch: 2, train for the 165-th batch, train loss: 0.2572411596775055:  43%|████▋      | 164/383 [00:37<00:53,  4.08it/s]evaluate for the 34-th batch, evaluate loss: 0.6962860822677612:  94%|████████████████▉ | 32/34 [00:02<00:00, 12.03it/s]evaluate for the 34-th batch, evaluate loss: 0.6962860822677612: 100%|██████████████████| 34/34 [00:02<00:00, 12.37it/s]evaluate for the 34-th batch, evaluate loss: 0.6962860822677612: 100%|██████████████████| 34/34 [00:02<00:00, 12.31it/s]
evaluate for the 8-th batch, evaluate loss: 0.657464861869812:  18%|███▋                 | 7/40 [00:01<00:03,  8.43it/s]evaluate for the 8-th batch, evaluate loss: 0.657464861869812:  20%|████▏                | 8/40 [00:01<00:04,  7.85it/s]Epoch: 9, train for the 103-th batch, train loss: 0.35095489025115967:  86%|████████▌ | 102/119 [00:16<00:02,  6.80it/s]Epoch: 6, train for the 141-th batch, train loss: 0.4990488290786743:  93%|██████████▎| 141/151 [00:25<00:02,  4.70it/s]Epoch: 9, train for the 103-th batch, train loss: 0.35095489025115967:  87%|████████▋ | 103/119 [00:16<00:02,  6.97it/s]Epoch: 2, train for the 165-th batch, train loss: 0.2572411596775055:  43%|████▋      | 165/383 [00:37<00:55,  3.91it/s]Epoch: 8, train for the 72-th batch, train loss: 0.531612753868103:  49%|██████▊       | 71/146 [00:10<00:11,  6.71it/s]Epoch: 8, train for the 72-th batch, train loss: 0.531612753868103:  49%|██████▉       | 72/146 [00:10<00:10,  7.09it/s]evaluate for the 9-th batch, evaluate loss: 0.688270092010498:  20%|████▏                | 8/40 [00:01<00:04,  7.85it/s]evaluate for the 9-th batch, evaluate loss: 0.688270092010498:  22%|████▋                | 9/40 [00:01<00:03,  8.03it/s]Epoch: 9, train for the 104-th batch, train loss: 0.36796045303344727:  87%|████████▋ | 103/119 [00:16<00:02,  6.97it/s]Epoch: 9, train for the 104-th batch, train loss: 0.36796045303344727:  87%|████████▋ | 104/119 [00:16<00:02,  6.45it/s]Epoch: 8, train for the 73-th batch, train loss: 0.5385192036628723:  49%|██████▍      | 72/146 [00:10<00:10,  7.09it/s]Epoch: 8, train for the 73-th batch, train loss: 0.5385192036628723:  50%|██████▌      | 73/146 [00:10<00:10,  6.93it/s]INFO:root:Epoch: 4, learning rate: 0.0001, train loss: 0.4708
INFO:root:train average_precision, 0.8596
INFO:root:train roc_auc, 0.8300
INFO:root:validate loss: 0.4893
INFO:root:validate average_precision, 0.8635
INFO:root:validate roc_auc, 0.8474
INFO:root:new node validate loss: 0.6023
INFO:root:new node validate first_1_average_precision, 0.5185
INFO:root:new node validate first_1_roc_auc, 0.5255
INFO:root:new node validate first_3_average_precision, 0.5950
INFO:root:new node validate first_3_roc_auc, 0.5900
INFO:root:new node validate first_10_average_precision, 0.6825
INFO:root:new node validate first_10_roc_auc, 0.6683
INFO:root:new node validate average_precision, 0.7488
Epoch: 2, train for the 166-th batch, train loss: 0.4621749818325043:  43%|████▋      | 165/383 [00:37<00:55,  3.91it/s]INFO:root:new node validate roc_auc, 0.7232
Epoch: 6, train for the 142-th batch, train loss: 0.4965802729129791:  93%|██████████▎| 141/151 [00:25<00:02,  4.70it/s]evaluate for the 10-th batch, evaluate loss: 0.6360958814620972:  22%|████▎              | 9/40 [00:01<00:03,  8.03it/s]evaluate for the 10-th batch, evaluate loss: 0.6360958814620972:  25%|████▌             | 10/40 [00:01<00:03,  7.94it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/241 [00:00<?, ?it/s]Epoch: 6, train for the 142-th batch, train loss: 0.4965802729129791:  94%|██████████▎| 142/151 [00:25<00:02,  4.38it/s]Epoch: 2, train for the 166-th batch, train loss: 0.4621749818325043:  43%|████▊      | 166/383 [00:37<00:55,  3.90it/s]Epoch: 5, train for the 1-th batch, train loss: 0.617988109588623:   0%|                        | 0/241 [00:00<?, ?it/s]Epoch: 9, train for the 105-th batch, train loss: 0.3698479235172272:  87%|█████████▌ | 104/119 [00:16<00:02,  6.45it/s]Epoch: 8, train for the 74-th batch, train loss: 0.48526260256767273:  50%|██████      | 73/146 [00:10<00:10,  6.93it/s]Epoch: 9, train for the 105-th batch, train loss: 0.3698479235172272:  88%|█████████▋ | 105/119 [00:16<00:02,  6.17it/s]evaluate for the 11-th batch, evaluate loss: 0.6521310210227966:  25%|████▌             | 10/40 [00:01<00:03,  7.94it/s]evaluate for the 11-th batch, evaluate loss: 0.6521310210227966:  28%|████▉             | 11/40 [00:01<00:03,  7.92it/s]Epoch: 8, train for the 74-th batch, train loss: 0.48526260256767273:  51%|██████      | 74/146 [00:10<00:10,  6.80it/s]Epoch: 5, train for the 2-th batch, train loss: 0.44696903228759766:   0%|                      | 0/241 [00:00<?, ?it/s]Epoch: 5, train for the 2-th batch, train loss: 0.44696903228759766:   1%|              | 2/241 [00:00<00:26,  9.16it/s]Epoch: 6, train for the 143-th batch, train loss: 0.4325627088546753:  94%|██████████▎| 142/151 [00:25<00:02,  4.38it/s]Epoch: 6, train for the 143-th batch, train loss: 0.4325627088546753:  95%|██████████▍| 143/151 [00:25<00:01,  4.30it/s]Epoch: 2, train for the 167-th batch, train loss: 0.33468422293663025:  43%|████▎     | 166/383 [00:37<00:55,  3.90it/s]Epoch: 8, train for the 75-th batch, train loss: 0.5240195989608765:  51%|██████▌      | 74/146 [00:11<00:10,  6.80it/s]Epoch: 9, train for the 106-th batch, train loss: 0.3432633876800537:  88%|█████████▋ | 105/119 [00:16<00:02,  6.17it/s]evaluate for the 12-th batch, evaluate loss: 0.6997939944267273:  28%|████▉             | 11/40 [00:01<00:03,  7.92it/s]evaluate for the 12-th batch, evaluate loss: 0.6997939944267273:  30%|█████▍            | 12/40 [00:01<00:03,  7.06it/s]Epoch: 5, train for the 3-th batch, train loss: 0.48927831649780273:   1%|              | 2/241 [00:00<00:26,  9.16it/s]Epoch: 8, train for the 75-th batch, train loss: 0.5240195989608765:  51%|██████▋      | 75/146 [00:11<00:11,  6.36it/s]Epoch: 9, train for the 106-th batch, train loss: 0.3432633876800537:  89%|█████████▊ | 106/119 [00:16<00:02,  5.90it/s]Epoch: 2, train for the 167-th batch, train loss: 0.33468422293663025:  44%|████▎     | 167/383 [00:37<00:58,  3.70it/s]evaluate for the 13-th batch, evaluate loss: 0.6693368554115295:  30%|█████▍            | 12/40 [00:01<00:03,  7.06it/s]evaluate for the 13-th batch, evaluate loss: 0.6693368554115295:  32%|█████▊            | 13/40 [00:01<00:03,  7.41it/s]Epoch: 8, train for the 76-th batch, train loss: 0.5297842621803284:  51%|██████▋      | 75/146 [00:11<00:11,  6.36it/s]Epoch: 8, train for the 76-th batch, train loss: 0.5297842621803284:  52%|██████▊      | 76/146 [00:11<00:10,  6.65it/s]Epoch: 9, train for the 107-th batch, train loss: 0.40084588527679443:  89%|████████▉ | 106/119 [00:17<00:02,  5.90it/s]Epoch: 6, train for the 144-th batch, train loss: 0.4336189031600952:  95%|██████████▍| 143/151 [00:25<00:01,  4.30it/s]Epoch: 5, train for the 4-th batch, train loss: 0.438965380191803:   1%|▏               | 2/241 [00:00<00:26,  9.16it/s]Epoch: 5, train for the 4-th batch, train loss: 0.438965380191803:   2%|▎               | 4/241 [00:00<00:29,  8.15it/s]Epoch: 9, train for the 107-th batch, train loss: 0.40084588527679443:  90%|████████▉ | 107/119 [00:17<00:02,  5.80it/s]Epoch: 6, train for the 144-th batch, train loss: 0.4336189031600952:  95%|██████████▍| 144/151 [00:25<00:01,  4.17it/s]Epoch: 2, train for the 168-th batch, train loss: 0.2642267644405365:  44%|████▊      | 167/383 [00:37<00:58,  3.70it/s]Epoch: 8, train for the 77-th batch, train loss: 0.595816433429718:  52%|███████▎      | 76/146 [00:11<00:10,  6.65it/s]Epoch: 8, train for the 77-th batch, train loss: 0.595816433429718:  53%|███████▍      | 77/146 [00:11<00:10,  6.86it/s]evaluate for the 14-th batch, evaluate loss: 0.7269958257675171:  32%|█████▊            | 13/40 [00:01<00:03,  7.41it/s]evaluate for the 14-th batch, evaluate loss: 0.7269958257675171:  35%|██████▎           | 14/40 [00:01<00:03,  7.07it/s]Epoch: 5, train for the 5-th batch, train loss: 0.4778391420841217:   2%|▏              | 4/241 [00:00<00:29,  8.15it/s]Epoch: 5, train for the 5-th batch, train loss: 0.4778391420841217:   2%|▎              | 5/241 [00:00<00:27,  8.52it/s]Epoch: 2, train for the 168-th batch, train loss: 0.2642267644405365:  44%|████▊      | 168/383 [00:37<00:58,  3.66it/s]Epoch: 9, train for the 108-th batch, train loss: 0.3026553690433502:  90%|█████████▉ | 107/119 [00:17<00:02,  5.80it/s]Epoch: 9, train for the 108-th batch, train loss: 0.3026553690433502:  91%|█████████▉ | 108/119 [00:17<00:01,  6.01it/s]evaluate for the 15-th batch, evaluate loss: 0.6708301901817322:  35%|██████▎           | 14/40 [00:01<00:03,  7.07it/s]evaluate for the 15-th batch, evaluate loss: 0.6708301901817322:  38%|██████▊           | 15/40 [00:01<00:03,  7.68it/s]Epoch: 6, train for the 145-th batch, train loss: 0.44203487038612366:  95%|█████████▌| 144/151 [00:26<00:01,  4.17it/s]Epoch: 5, train for the 6-th batch, train loss: 0.6570954918861389:   2%|▎              | 5/241 [00:00<00:27,  8.52it/s]Epoch: 5, train for the 6-th batch, train loss: 0.6570954918861389:   2%|▎              | 6/241 [00:00<00:28,  8.20it/s]Epoch: 8, train for the 78-th batch, train loss: 0.5266565084457397:  53%|██████▊      | 77/146 [00:11<00:10,  6.86it/s]Epoch: 8, train for the 78-th batch, train loss: 0.5266565084457397:  53%|██████▉      | 78/146 [00:11<00:10,  6.67it/s]Epoch: 6, train for the 145-th batch, train loss: 0.44203487038612366:  96%|█████████▌| 145/151 [00:26<00:01,  4.27it/s]Epoch: 9, train for the 109-th batch, train loss: 0.3791264593601227:  91%|█████████▉ | 108/119 [00:17<00:01,  6.01it/s]Epoch: 9, train for the 109-th batch, train loss: 0.3791264593601227:  92%|██████████ | 109/119 [00:17<00:01,  6.31it/s]Epoch: 2, train for the 169-th batch, train loss: 0.28341248631477356:  44%|████▍     | 168/383 [00:38<00:58,  3.66it/s]Epoch: 5, train for the 7-th batch, train loss: 0.4136253595352173:   2%|▎              | 6/241 [00:00<00:28,  8.20it/s]Epoch: 5, train for the 7-th batch, train loss: 0.4136253595352173:   3%|▍              | 7/241 [00:00<00:27,  8.46it/s]evaluate for the 16-th batch, evaluate loss: 0.647404134273529:  38%|███████▏           | 15/40 [00:02<00:03,  7.68it/s]evaluate for the 16-th batch, evaluate loss: 0.647404134273529:  40%|███████▌           | 16/40 [00:02<00:03,  7.28it/s]Epoch: 2, train for the 169-th batch, train loss: 0.28341248631477356:  44%|████▍     | 169/383 [00:38<00:56,  3.78it/s]Epoch: 8, train for the 79-th batch, train loss: 0.5626014471054077:  53%|██████▉      | 78/146 [00:11<00:10,  6.67it/s]Epoch: 8, train for the 79-th batch, train loss: 0.5626014471054077:  54%|███████      | 79/146 [00:11<00:09,  6.82it/s]Epoch: 6, train for the 146-th batch, train loss: 0.45972537994384766:  96%|█████████▌| 145/151 [00:26<00:01,  4.27it/s]Epoch: 9, train for the 110-th batch, train loss: 0.35001447796821594:  92%|█████████▏| 109/119 [00:17<00:01,  6.31it/s]Epoch: 6, train for the 146-th batch, train loss: 0.45972537994384766:  97%|█████████▋| 146/151 [00:26<00:01,  4.61it/s]Epoch: 9, train for the 110-th batch, train loss: 0.35001447796821594:  92%|█████████▏| 110/119 [00:17<00:01,  6.45it/s]evaluate for the 17-th batch, evaluate loss: 0.6311867833137512:  40%|███████▏          | 16/40 [00:02<00:03,  7.28it/s]Epoch: 8, train for the 80-th batch, train loss: 0.5427812337875366:  54%|███████      | 79/146 [00:11<00:09,  6.82it/s]Epoch: 8, train for the 80-th batch, train loss: 0.5427812337875366:  55%|███████      | 80/146 [00:11<00:09,  7.06it/s]Epoch: 6, train for the 147-th batch, train loss: 0.4523845911026001:  97%|██████████▋| 146/151 [00:26<00:01,  4.61it/s]Epoch: 9, train for the 111-th batch, train loss: 0.4298429787158966:  92%|██████████▏| 110/119 [00:17<00:01,  6.45it/s]Epoch: 6, train for the 147-th batch, train loss: 0.4523845911026001:  97%|██████████▋| 147/151 [00:26<00:00,  5.12it/s]Epoch: 9, train for the 111-th batch, train loss: 0.4298429787158966:  93%|██████████▎| 111/119 [00:17<00:01,  6.65it/s]Epoch: 5, train for the 8-th batch, train loss: 0.700983464717865:   3%|▍               | 7/241 [00:01<00:27,  8.46it/s]Epoch: 5, train for the 8-th batch, train loss: 0.700983464717865:   3%|▌               | 8/241 [00:01<00:36,  6.44it/s]Epoch: 8, train for the 81-th batch, train loss: 0.5058503746986389:  55%|███████      | 80/146 [00:11<00:09,  7.06it/s]Epoch: 2, train for the 170-th batch, train loss: 0.31731781363487244:  44%|████▍     | 169/383 [00:38<00:56,  3.78it/s]Epoch: 8, train for the 81-th batch, train loss: 0.5058503746986389:  55%|███████▏     | 81/146 [00:11<00:08,  7.49it/s]evaluate for the 18-th batch, evaluate loss: 0.6525922417640686:  40%|███████▏          | 16/40 [00:02<00:03,  7.28it/s]evaluate for the 18-th batch, evaluate loss: 0.6525922417640686:  45%|████████          | 18/40 [00:02<00:03,  7.16it/s]Epoch: 2, train for the 170-th batch, train loss: 0.31731781363487244:  44%|████▍     | 170/383 [00:38<00:59,  3.61it/s]Epoch: 9, train for the 112-th batch, train loss: 0.331846684217453:  93%|███████████▏| 111/119 [00:17<00:01,  6.65it/s]Epoch: 5, train for the 9-th batch, train loss: 0.6349940299987793:   3%|▍              | 8/241 [00:01<00:36,  6.44it/s]Epoch: 9, train for the 112-th batch, train loss: 0.331846684217453:  94%|███████████▎| 112/119 [00:17<00:01,  6.60it/s]Epoch: 5, train for the 9-th batch, train loss: 0.6349940299987793:   4%|▌              | 9/241 [00:01<00:36,  6.43it/s]Epoch: 6, train for the 148-th batch, train loss: 0.42532554268836975:  97%|█████████▋| 147/151 [00:26<00:00,  5.12it/s]evaluate for the 19-th batch, evaluate loss: 0.71675044298172:  45%|█████████           | 18/40 [00:02<00:03,  7.16it/s]evaluate for the 19-th batch, evaluate loss: 0.71675044298172:  48%|█████████▌          | 19/40 [00:02<00:02,  7.34it/s]Epoch: 8, train for the 82-th batch, train loss: 0.48021018505096436:  55%|██████▋     | 81/146 [00:12<00:08,  7.49it/s]Epoch: 8, train for the 82-th batch, train loss: 0.48021018505096436:  56%|██████▋     | 82/146 [00:12<00:08,  7.37it/s]Epoch: 6, train for the 148-th batch, train loss: 0.42532554268836975:  98%|█████████▊| 148/151 [00:26<00:00,  5.05it/s]Epoch: 5, train for the 10-th batch, train loss: 0.5509964823722839:   4%|▌             | 9/241 [00:01<00:36,  6.43it/s]Epoch: 5, train for the 10-th batch, train loss: 0.5509964823722839:   4%|▌            | 10/241 [00:01<00:35,  6.60it/s]Epoch: 9, train for the 113-th batch, train loss: 0.3387579917907715:  94%|██████████▎| 112/119 [00:17<00:01,  6.60it/s]Epoch: 9, train for the 113-th batch, train loss: 0.3387579917907715:  95%|██████████▍| 113/119 [00:17<00:00,  6.30it/s]evaluate for the 20-th batch, evaluate loss: 0.6753794550895691:  48%|████████▌         | 19/40 [00:02<00:02,  7.34it/s]evaluate for the 20-th batch, evaluate loss: 0.6753794550895691:  50%|█████████         | 20/40 [00:02<00:02,  7.07it/s]Epoch: 8, train for the 83-th batch, train loss: 0.5173085927963257:  56%|███████▎     | 82/146 [00:12<00:08,  7.37it/s]Epoch: 8, train for the 83-th batch, train loss: 0.5173085927963257:  57%|███████▍     | 83/146 [00:12<00:08,  7.08it/s]Epoch: 2, train for the 171-th batch, train loss: 0.6298673152923584:  44%|████▉      | 170/383 [00:38<00:59,  3.61it/s]Epoch: 6, train for the 149-th batch, train loss: 0.41738373041152954:  98%|█████████▊| 148/151 [00:26<00:00,  5.05it/s]Epoch: 6, train for the 149-th batch, train loss: 0.41738373041152954:  99%|█████████▊| 149/151 [00:26<00:00,  5.09it/s]Epoch: 2, train for the 171-th batch, train loss: 0.6298673152923584:  45%|████▉      | 171/383 [00:38<01:00,  3.51it/s]Epoch: 5, train for the 11-th batch, train loss: 0.3742460310459137:   4%|▌            | 10/241 [00:01<00:35,  6.60it/s]Epoch: 5, train for the 11-th batch, train loss: 0.3742460310459137:   5%|▌            | 11/241 [00:01<00:32,  7.14it/s]evaluate for the 21-th batch, evaluate loss: 0.6886077523231506:  50%|█████████         | 20/40 [00:02<00:02,  7.07it/s]evaluate for the 21-th batch, evaluate loss: 0.6886077523231506:  52%|█████████▍        | 21/40 [00:02<00:02,  7.61it/s]Epoch: 9, train for the 114-th batch, train loss: 0.35871925950050354:  95%|█████████▍| 113/119 [00:18<00:00,  6.30it/s]Epoch: 9, train for the 114-th batch, train loss: 0.35871925950050354:  96%|█████████▌| 114/119 [00:18<00:00,  6.03it/s]Epoch: 8, train for the 84-th batch, train loss: 0.4779821038246155:  57%|███████▍     | 83/146 [00:12<00:08,  7.08it/s]Epoch: 8, train for the 84-th batch, train loss: 0.4779821038246155:  58%|███████▍     | 84/146 [00:12<00:09,  6.36it/s]evaluate for the 22-th batch, evaluate loss: 0.7035056948661804:  52%|█████████▍        | 21/40 [00:02<00:02,  7.61it/s]evaluate for the 22-th batch, evaluate loss: 0.7035056948661804:  55%|█████████▉        | 22/40 [00:02<00:02,  7.51it/s]Epoch: 5, train for the 12-th batch, train loss: 0.5830884575843811:   5%|▌            | 11/241 [00:01<00:32,  7.14it/s]Epoch: 5, train for the 12-th batch, train loss: 0.5830884575843811:   5%|▋            | 12/241 [00:01<00:33,  6.76it/s]Epoch: 6, train for the 150-th batch, train loss: 0.45681819319725037:  99%|█████████▊| 149/151 [00:26<00:00,  5.09it/s]Epoch: 2, train for the 172-th batch, train loss: 0.3070213496685028:  45%|████▉      | 171/383 [00:39<01:00,  3.51it/s]Epoch: 6, train for the 150-th batch, train loss: 0.45681819319725037:  99%|█████████▉| 150/151 [00:26<00:00,  4.88it/s]Epoch: 2, train for the 172-th batch, train loss: 0.3070213496685028:  45%|████▉      | 172/383 [00:39<00:57,  3.66it/s]evaluate for the 23-th batch, evaluate loss: 0.6691880822181702:  55%|█████████▉        | 22/40 [00:03<00:02,  7.51it/s]Epoch: 9, train for the 115-th batch, train loss: 0.3372254967689514:  96%|██████████▌| 114/119 [00:18<00:00,  6.03it/s]Epoch: 9, train for the 115-th batch, train loss: 0.3372254967689514:  97%|██████████▋| 115/119 [00:18<00:00,  6.01it/s]Epoch: 5, train for the 13-th batch, train loss: 0.527655303478241:   5%|▋             | 12/241 [00:01<00:33,  6.76it/s]Epoch: 8, train for the 85-th batch, train loss: 0.48324453830718994:  58%|██████▉     | 84/146 [00:12<00:09,  6.36it/s]Epoch: 5, train for the 13-th batch, train loss: 0.527655303478241:   5%|▊             | 13/241 [00:01<00:31,  7.23it/s]Epoch: 8, train for the 85-th batch, train loss: 0.48324453830718994:  58%|██████▉     | 85/146 [00:12<00:09,  6.29it/s]evaluate for the 24-th batch, evaluate loss: 0.7000529170036316:  55%|█████████▉        | 22/40 [00:03<00:02,  7.51it/s]evaluate for the 24-th batch, evaluate loss: 0.7000529170036316:  60%|██████████▊       | 24/40 [00:03<00:01,  8.44it/s]Epoch: 9, train for the 116-th batch, train loss: 0.34413325786590576:  97%|█████████▋| 115/119 [00:18<00:00,  6.01it/s]Epoch: 9, train for the 116-th batch, train loss: 0.34413325786590576:  97%|█████████▋| 116/119 [00:18<00:00,  6.56it/s]Epoch: 5, train for the 14-th batch, train loss: 0.5818964242935181:   5%|▋            | 13/241 [00:01<00:31,  7.23it/s]Epoch: 5, train for the 14-th batch, train loss: 0.5818964242935181:   6%|▊            | 14/241 [00:01<00:30,  7.37it/s]Epoch: 2, train for the 173-th batch, train loss: 0.31954509019851685:  45%|████▍     | 172/383 [00:39<00:57,  3.66it/s]evaluate for the 25-th batch, evaluate loss: 0.6668693423271179:  60%|██████████▊       | 24/40 [00:03<00:01,  8.44it/s]evaluate for the 25-th batch, evaluate loss: 0.6668693423271179:  62%|███████████▎      | 25/40 [00:03<00:01,  8.62it/s]Epoch: 6, train for the 151-th batch, train loss: 0.5305905342102051:  99%|██████████▉| 150/151 [00:27<00:00,  4.88it/s]Epoch: 9, train for the 117-th batch, train loss: 0.30148717761039734:  97%|█████████▋| 116/119 [00:18<00:00,  6.56it/s]Epoch: 2, train for the 173-th batch, train loss: 0.31954509019851685:  45%|████▌     | 173/383 [00:39<00:56,  3.71it/s]Epoch: 8, train for the 86-th batch, train loss: 0.4851697087287903:  58%|███████▌     | 85/146 [00:12<00:09,  6.29it/s]Epoch: 9, train for the 117-th batch, train loss: 0.30148717761039734:  98%|█████████▊| 117/119 [00:18<00:00,  7.02it/s]Epoch: 8, train for the 86-th batch, train loss: 0.4851697087287903:  59%|███████▋     | 86/146 [00:12<00:10,  5.65it/s]Epoch: 6, train for the 151-th batch, train loss: 0.5305905342102051: 100%|███████████| 151/151 [00:27<00:00,  4.19it/s]Epoch: 6, train for the 151-th batch, train loss: 0.5305905342102051: 100%|███████████| 151/151 [00:27<00:00,  5.53it/s]
Epoch: 5, train for the 15-th batch, train loss: 0.5787163972854614:   6%|▊            | 14/241 [00:02<00:30,  7.37it/s]Epoch: 5, train for the 15-th batch, train loss: 0.5787163972854614:   6%|▊            | 15/241 [00:02<00:29,  7.66it/s]evaluate for the 26-th batch, evaluate loss: 0.6675750017166138:  62%|███████████▎      | 25/40 [00:03<00:01,  8.62it/s]evaluate for the 26-th batch, evaluate loss: 0.6675750017166138:  65%|███████████▋      | 26/40 [00:03<00:01,  8.60it/s]Epoch: 8, train for the 87-th batch, train loss: 0.5184844136238098:  59%|███████▋     | 86/146 [00:12<00:10,  5.65it/s]  0%|                                                                                            | 0/46 [00:00<?, ?it/s]Epoch: 9, train for the 118-th batch, train loss: 0.3067099153995514:  98%|██████████▊| 117/119 [00:18<00:00,  7.02it/s]Epoch: 5, train for the 16-th batch, train loss: 0.49372249841690063:   6%|▋           | 15/241 [00:02<00:29,  7.66it/s]Epoch: 8, train for the 87-th batch, train loss: 0.5184844136238098:  60%|███████▋     | 87/146 [00:12<00:10,  5.70it/s]Epoch: 5, train for the 16-th batch, train loss: 0.49372249841690063:   7%|▊           | 16/241 [00:02<00:30,  7.28it/s]Epoch: 9, train for the 118-th batch, train loss: 0.3067099153995514:  99%|██████████▉| 118/119 [00:18<00:00,  6.40it/s]Epoch: 2, train for the 174-th batch, train loss: 0.4798257648944855:  45%|████▉      | 173/383 [00:39<00:56,  3.71it/s]evaluate for the 27-th batch, evaluate loss: 0.6603873372077942:  65%|███████████▋      | 26/40 [00:03<00:01,  8.60it/s]evaluate for the 27-th batch, evaluate loss: 0.6603873372077942:  68%|████████████▏     | 27/40 [00:03<00:01,  8.14it/s]evaluate for the 1-th batch, evaluate loss: 0.5601363778114319:   0%|                            | 0/46 [00:00<?, ?it/s]Epoch: 2, train for the 174-th batch, train loss: 0.4798257648944855:  45%|████▉      | 174/383 [00:39<00:55,  3.76it/s]Epoch: 5, train for the 17-th batch, train loss: 0.44217753410339355:   7%|▊           | 16/241 [00:02<00:30,  7.28it/s]Epoch: 9, train for the 119-th batch, train loss: 0.2588452398777008:  99%|██████████▉| 118/119 [00:18<00:00,  6.40it/s]evaluate for the 28-th batch, evaluate loss: 0.703312337398529:  68%|████████████▊      | 27/40 [00:03<00:01,  8.14it/s]evaluate for the 28-th batch, evaluate loss: 0.703312337398529:  70%|█████████████▎     | 28/40 [00:03<00:01,  8.26it/s]evaluate for the 2-th batch, evaluate loss: 0.5420283675193787:   0%|                            | 0/46 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.5420283675193787:   4%|▊                   | 2/46 [00:00<00:03, 11.32it/s]Epoch: 5, train for the 17-th batch, train loss: 0.44217753410339355:   7%|▊           | 17/241 [00:02<00:32,  6.92it/s]Epoch: 8, train for the 88-th batch, train loss: 0.5491933226585388:  60%|███████▋     | 87/146 [00:13<00:10,  5.70it/s]Epoch: 9, train for the 119-th batch, train loss: 0.2588452398777008: 100%|███████████| 119/119 [00:18<00:00,  6.28it/s]Epoch: 9, train for the 119-th batch, train loss: 0.2588452398777008: 100%|███████████| 119/119 [00:18<00:00,  6.28it/s]
Epoch: 8, train for the 88-th batch, train loss: 0.5491933226585388:  60%|███████▊     | 88/146 [00:13<00:10,  5.59it/s]evaluate for the 3-th batch, evaluate loss: 0.5563080310821533:   4%|▊                   | 2/46 [00:00<00:03, 11.32it/s]Epoch: 5, train for the 18-th batch, train loss: 0.46665361523628235:   7%|▊           | 17/241 [00:02<00:32,  6.92it/s]Epoch: 5, train for the 18-th batch, train loss: 0.46665361523628235:   7%|▉           | 18/241 [00:02<00:30,  7.26it/s]Epoch: 2, train for the 175-th batch, train loss: 0.4028189778327942:  45%|████▉      | 174/383 [00:39<00:55,  3.76it/s]Epoch: 8, train for the 89-th batch, train loss: 0.5235896706581116:  60%|███████▊     | 88/146 [00:13<00:10,  5.59it/s]evaluate for the 29-th batch, evaluate loss: 0.7329416275024414:  70%|████████████▌     | 28/40 [00:03<00:01,  8.26it/s]evaluate for the 29-th batch, evaluate loss: 0.7329416275024414:  72%|█████████████     | 29/40 [00:03<00:01,  7.72it/s]Epoch: 8, train for the 89-th batch, train loss: 0.5235896706581116:  61%|███████▉     | 89/146 [00:13<00:09,  6.03it/s]evaluate for the 4-th batch, evaluate loss: 0.5482842922210693:   4%|▊                   | 2/46 [00:00<00:03, 11.32it/s]evaluate for the 4-th batch, evaluate loss: 0.5482842922210693:   9%|█▋                  | 4/46 [00:00<00:03, 11.25it/s]Epoch: 2, train for the 175-th batch, train loss: 0.4028189778327942:  46%|█████      | 175/383 [00:39<00:56,  3.69it/s]  0%|                                                                                            | 0/40 [00:00<?, ?it/s]Epoch: 5, train for the 19-th batch, train loss: 0.4935177266597748:   7%|▉            | 18/241 [00:02<00:30,  7.26it/s]Epoch: 5, train for the 19-th batch, train loss: 0.4935177266597748:   8%|█            | 19/241 [00:02<00:29,  7.47it/s]Epoch: 8, train for the 90-th batch, train loss: 0.5289593935012817:  61%|███████▉     | 89/146 [00:13<00:09,  6.03it/s]evaluate for the 30-th batch, evaluate loss: 0.741484522819519:  72%|█████████████▊     | 29/40 [00:03<00:01,  7.72it/s]Epoch: 8, train for the 90-th batch, train loss: 0.5289593935012817:  62%|████████     | 90/146 [00:13<00:08,  6.51it/s]evaluate for the 30-th batch, evaluate loss: 0.741484522819519:  75%|██████████████▎    | 30/40 [00:03<00:01,  7.68it/s]evaluate for the 1-th batch, evaluate loss: 0.49492132663726807:   0%|                           | 0/40 [00:00<?, ?it/s]evaluate for the 5-th batch, evaluate loss: 0.5423775315284729:   9%|█▋                  | 4/46 [00:00<00:03, 11.25it/s]evaluate for the 2-th batch, evaluate loss: 0.458274245262146:   0%|                             | 0/40 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.458274245262146:   5%|█                    | 2/40 [00:00<00:02, 14.67it/s]Epoch: 5, train for the 20-th batch, train loss: 0.4382968842983246:   8%|█            | 19/241 [00:02<00:29,  7.47it/s]Epoch: 5, train for the 20-th batch, train loss: 0.4382968842983246:   8%|█            | 20/241 [00:02<00:29,  7.41it/s]evaluate for the 6-th batch, evaluate loss: 0.541663408279419:   9%|█▊                   | 4/46 [00:00<00:03, 11.25it/s]evaluate for the 6-th batch, evaluate loss: 0.541663408279419:  13%|██▋                  | 6/46 [00:00<00:04,  9.77it/s]evaluate for the 3-th batch, evaluate loss: 0.4841262400150299:   5%|█                   | 2/40 [00:00<00:02, 14.67it/s]evaluate for the 31-th batch, evaluate loss: 0.6591392159461975:  75%|█████████████▌    | 30/40 [00:04<00:01,  7.68it/s]evaluate for the 31-th batch, evaluate loss: 0.6591392159461975:  78%|█████████████▉    | 31/40 [00:04<00:01,  6.66it/s]Epoch: 8, train for the 91-th batch, train loss: 0.5135459899902344:  62%|████████     | 90/146 [00:13<00:08,  6.51it/s]Epoch: 2, train for the 176-th batch, train loss: 0.3852847218513489:  46%|█████      | 175/383 [00:40<00:56,  3.69it/s]Epoch: 8, train for the 91-th batch, train loss: 0.5135459899902344:  62%|████████     | 91/146 [00:13<00:09,  5.70it/s]evaluate for the 4-th batch, evaluate loss: 0.5129022002220154:   5%|█                   | 2/40 [00:00<00:02, 14.67it/s]evaluate for the 4-th batch, evaluate loss: 0.5129022002220154:  10%|██                  | 4/40 [00:00<00:02, 12.88it/s]Epoch: 5, train for the 21-th batch, train loss: 0.6272541880607605:   8%|█            | 20/241 [00:02<00:29,  7.41it/s]evaluate for the 7-th batch, evaluate loss: 0.5504981279373169:  13%|██▌                 | 6/46 [00:00<00:04,  9.77it/s]evaluate for the 7-th batch, evaluate loss: 0.5504981279373169:  15%|███                 | 7/46 [00:00<00:04,  9.22it/s]Epoch: 5, train for the 21-th batch, train loss: 0.6272541880607605:   9%|█▏           | 21/241 [00:02<00:31,  7.01it/s]Epoch: 2, train for the 176-th batch, train loss: 0.3852847218513489:  46%|█████      | 176/383 [00:40<01:01,  3.35it/s]evaluate for the 5-th batch, evaluate loss: 0.49154403805732727:  10%|█▉                 | 4/40 [00:00<00:02, 12.88it/s]evaluate for the 32-th batch, evaluate loss: 0.697974443435669:  78%|██████████████▋    | 31/40 [00:04<00:01,  6.66it/s]evaluate for the 32-th batch, evaluate loss: 0.697974443435669:  80%|███████████████▏   | 32/40 [00:04<00:01,  6.90it/s]evaluate for the 8-th batch, evaluate loss: 0.5588642954826355:  15%|███                 | 7/46 [00:00<00:04,  9.22it/s]Epoch: 8, train for the 92-th batch, train loss: 0.5148143172264099:  62%|████████     | 91/146 [00:13<00:09,  5.70it/s]evaluate for the 6-th batch, evaluate loss: 0.43378981947898865:  10%|█▉                 | 4/40 [00:00<00:02, 12.88it/s]evaluate for the 6-th batch, evaluate loss: 0.43378981947898865:  15%|██▊                | 6/40 [00:00<00:02, 12.84it/s]Epoch: 8, train for the 92-th batch, train loss: 0.5148143172264099:  63%|████████▏    | 92/146 [00:13<00:09,  5.80it/s]Epoch: 5, train for the 22-th batch, train loss: 0.5359057784080505:   9%|█▏           | 21/241 [00:03<00:31,  7.01it/s]evaluate for the 33-th batch, evaluate loss: 0.7029003500938416:  80%|██████████████▍   | 32/40 [00:04<00:01,  6.90it/s]evaluate for the 33-th batch, evaluate loss: 0.7029003500938416:  82%|██████████████▊   | 33/40 [00:04<00:00,  7.31it/s]evaluate for the 9-th batch, evaluate loss: 0.5472161769866943:  15%|███                 | 7/46 [00:00<00:04,  9.22it/s]evaluate for the 9-th batch, evaluate loss: 0.5472161769866943:  20%|███▉                | 9/46 [00:00<00:03,  9.76it/s]Epoch: 5, train for the 22-th batch, train loss: 0.5359057784080505:   9%|█▏           | 22/241 [00:03<00:34,  6.43it/s]evaluate for the 7-th batch, evaluate loss: 0.46384039521217346:  15%|██▊                | 6/40 [00:00<00:02, 12.84it/s]Epoch: 2, train for the 177-th batch, train loss: 0.3953092098236084:  46%|█████      | 176/383 [00:40<01:01,  3.35it/s]evaluate for the 10-th batch, evaluate loss: 0.5122890472412109:  20%|███▋               | 9/46 [00:01<00:03,  9.76it/s]evaluate for the 34-th batch, evaluate loss: 0.6600149273872375:  82%|██████████████▊   | 33/40 [00:04<00:00,  7.31it/s]evaluate for the 34-th batch, evaluate loss: 0.6600149273872375:  85%|███████████████▎  | 34/40 [00:04<00:00,  7.71it/s]Epoch: 8, train for the 93-th batch, train loss: 0.49107518792152405:  63%|███████▌    | 92/146 [00:13<00:09,  5.80it/s]evaluate for the 8-th batch, evaluate loss: 0.4010748565196991:  15%|███                 | 6/40 [00:00<00:02, 12.84it/s]evaluate for the 8-th batch, evaluate loss: 0.4010748565196991:  20%|████                | 8/40 [00:00<00:02, 11.94it/s]Epoch: 8, train for the 93-th batch, train loss: 0.49107518792152405:  64%|███████▋    | 93/146 [00:14<00:09,  5.66it/s]Epoch: 5, train for the 23-th batch, train loss: 0.5097121596336365:   9%|█▏           | 22/241 [00:03<00:34,  6.43it/s]Epoch: 2, train for the 177-th batch, train loss: 0.3953092098236084:  46%|█████      | 177/383 [00:40<01:02,  3.31it/s]Epoch: 5, train for the 23-th batch, train loss: 0.5097121596336365:  10%|█▏           | 23/241 [00:03<00:32,  6.67it/s]evaluate for the 11-th batch, evaluate loss: 0.5673301815986633:  20%|███▋               | 9/46 [00:01<00:03,  9.76it/s]evaluate for the 11-th batch, evaluate loss: 0.5673301815986633:  24%|████▎             | 11/46 [00:01<00:03, 10.66it/s]evaluate for the 35-th batch, evaluate loss: 0.7146435379981995:  85%|███████████████▎  | 34/40 [00:04<00:00,  7.71it/s]evaluate for the 9-th batch, evaluate loss: 0.4484371542930603:  20%|████                | 8/40 [00:00<00:02, 11.94it/s]evaluate for the 35-th batch, evaluate loss: 0.7146435379981995:  88%|███████████████▊  | 35/40 [00:04<00:00,  7.70it/s]evaluate for the 12-th batch, evaluate loss: 0.49288657307624817:  24%|████             | 11/46 [00:01<00:03, 10.66it/s]Epoch: 5, train for the 24-th batch, train loss: 0.553149938583374:  10%|█▎            | 23/241 [00:03<00:32,  6.67it/s]Epoch: 5, train for the 24-th batch, train loss: 0.553149938583374:  10%|█▍            | 24/241 [00:03<00:33,  6.40it/s]evaluate for the 10-th batch, evaluate loss: 0.48497089743614197:  20%|███▌              | 8/40 [00:00<00:02, 11.94it/s]evaluate for the 10-th batch, evaluate loss: 0.48497089743614197:  25%|████▎            | 10/40 [00:00<00:02, 11.27it/s]Epoch: 8, train for the 94-th batch, train loss: 0.5188634395599365:  64%|████████▎    | 93/146 [00:14<00:09,  5.66it/s]Epoch: 8, train for the 94-th batch, train loss: 0.5188634395599365:  64%|████████▎    | 94/146 [00:14<00:09,  5.26it/s]evaluate for the 36-th batch, evaluate loss: 0.7375895380973816:  88%|███████████████▊  | 35/40 [00:04<00:00,  7.70it/s]evaluate for the 36-th batch, evaluate loss: 0.7375895380973816:  90%|████████████████▏ | 36/40 [00:04<00:00,  7.50it/s]evaluate for the 11-th batch, evaluate loss: 0.42742598056793213:  25%|████▎            | 10/40 [00:00<00:02, 11.27it/s]evaluate for the 13-th batch, evaluate loss: 0.5177891254425049:  24%|████▎             | 11/46 [00:01<00:03, 10.66it/s]evaluate for the 13-th batch, evaluate loss: 0.5177891254425049:  28%|█████             | 13/46 [00:01<00:03,  9.74it/s]Epoch: 2, train for the 178-th batch, train loss: 0.5108336210250854:  46%|█████      | 177/383 [00:40<01:02,  3.31it/s]Epoch: 5, train for the 25-th batch, train loss: 0.5993643403053284:  10%|█▎           | 24/241 [00:03<00:33,  6.40it/s]Epoch: 2, train for the 178-th batch, train loss: 0.5108336210250854:  46%|█████      | 178/383 [00:40<01:02,  3.27it/s]Epoch: 5, train for the 25-th batch, train loss: 0.5993643403053284:  10%|█▎           | 25/241 [00:03<00:32,  6.57it/s]evaluate for the 12-th batch, evaluate loss: 0.41074687242507935:  25%|████▎            | 10/40 [00:01<00:02, 11.27it/s]evaluate for the 12-th batch, evaluate loss: 0.41074687242507935:  30%|█████            | 12/40 [00:01<00:02, 11.39it/s]evaluate for the 14-th batch, evaluate loss: 0.5586754679679871:  28%|█████             | 13/46 [00:01<00:03,  9.74it/s]Epoch: 8, train for the 95-th batch, train loss: 0.49634116888046265:  64%|███████▋    | 94/146 [00:14<00:09,  5.26it/s]Epoch: 8, train for the 95-th batch, train loss: 0.49634116888046265:  65%|███████▊    | 95/146 [00:14<00:09,  5.49it/s]evaluate for the 37-th batch, evaluate loss: 0.7208706736564636:  90%|████████████████▏ | 36/40 [00:04<00:00,  7.50it/s]evaluate for the 37-th batch, evaluate loss: 0.7208706736564636:  92%|████████████████▋ | 37/40 [00:04<00:00,  7.20it/s]evaluate for the 13-th batch, evaluate loss: 0.45263007283210754:  30%|█████            | 12/40 [00:01<00:02, 11.39it/s]Epoch: 5, train for the 26-th batch, train loss: 0.646662712097168:  10%|█▍            | 25/241 [00:03<00:32,  6.57it/s]Epoch: 5, train for the 26-th batch, train loss: 0.646662712097168:  11%|█▌            | 26/241 [00:03<00:31,  6.85it/s]evaluate for the 15-th batch, evaluate loss: 0.5380843877792358:  28%|█████             | 13/46 [00:01<00:03,  9.74it/s]evaluate for the 15-th batch, evaluate loss: 0.5380843877792358:  33%|█████▊            | 15/46 [00:01<00:03,  9.97it/s]Epoch: 8, train for the 96-th batch, train loss: 0.5398235321044922:  65%|████████▍    | 95/146 [00:14<00:09,  5.49it/s]evaluate for the 14-th batch, evaluate loss: 0.4282669723033905:  30%|█████▍            | 12/40 [00:01<00:02, 11.39it/s]evaluate for the 14-th batch, evaluate loss: 0.4282669723033905:  35%|██████▎           | 14/40 [00:01<00:02, 11.14it/s]Epoch: 8, train for the 96-th batch, train loss: 0.5398235321044922:  66%|████████▌    | 96/146 [00:14<00:08,  5.60it/s]evaluate for the 16-th batch, evaluate loss: 0.5213733911514282:  33%|█████▊            | 15/46 [00:01<00:03,  9.97it/s]Epoch: 2, train for the 179-th batch, train loss: 0.39168474078178406:  46%|████▋     | 178/383 [00:41<01:02,  3.27it/s]evaluate for the 38-th batch, evaluate loss: 0.6854403018951416:  92%|████████████████▋ | 37/40 [00:05<00:00,  7.20it/s]evaluate for the 38-th batch, evaluate loss: 0.6854403018951416:  95%|█████████████████ | 38/40 [00:05<00:00,  6.72it/s]Epoch: 5, train for the 27-th batch, train loss: 0.6771085858345032:  11%|█▍           | 26/241 [00:03<00:31,  6.85it/s]Epoch: 5, train for the 27-th batch, train loss: 0.6771085858345032:  11%|█▍           | 27/241 [00:03<00:31,  6.76it/s]evaluate for the 15-th batch, evaluate loss: 0.44412660598754883:  35%|█████▉           | 14/40 [00:01<00:02, 11.14it/s]Epoch: 2, train for the 179-th batch, train loss: 0.39168474078178406:  47%|████▋     | 179/383 [00:41<01:02,  3.29it/s]evaluate for the 17-th batch, evaluate loss: 0.45022428035736084:  33%|█████▌           | 15/46 [00:01<00:03,  9.97it/s]evaluate for the 17-th batch, evaluate loss: 0.45022428035736084:  37%|██████▎          | 17/46 [00:01<00:02,  9.80it/s]Epoch: 8, train for the 97-th batch, train loss: 0.4827321469783783:  66%|████████▌    | 96/146 [00:14<00:08,  5.60it/s]evaluate for the 16-th batch, evaluate loss: 0.45942291617393494:  35%|█████▉           | 14/40 [00:01<00:02, 11.14it/s]evaluate for the 16-th batch, evaluate loss: 0.45942291617393494:  40%|██████▊          | 16/40 [00:01<00:02, 11.82it/s]Epoch: 8, train for the 97-th batch, train loss: 0.4827321469783783:  66%|████████▋    | 97/146 [00:14<00:08,  5.98it/s]evaluate for the 39-th batch, evaluate loss: 0.6941085457801819:  95%|█████████████████ | 38/40 [00:05<00:00,  6.72it/s]evaluate for the 39-th batch, evaluate loss: 0.6941085457801819:  98%|█████████████████▌| 39/40 [00:05<00:00,  7.01it/s]Epoch: 5, train for the 28-th batch, train loss: 0.4840673804283142:  11%|█▍           | 27/241 [00:03<00:31,  6.76it/s]Epoch: 5, train for the 28-th batch, train loss: 0.4840673804283142:  12%|█▌           | 28/241 [00:03<00:29,  7.25it/s]evaluate for the 18-th batch, evaluate loss: 0.4992619454860687:  37%|██████▋           | 17/46 [00:01<00:02,  9.80it/s]evaluate for the 18-th batch, evaluate loss: 0.4992619454860687:  39%|███████           | 18/46 [00:01<00:02,  9.76it/s]evaluate for the 17-th batch, evaluate loss: 0.43900978565216064:  40%|██████▊          | 16/40 [00:01<00:02, 11.82it/s]evaluate for the 18-th batch, evaluate loss: 0.41484007239341736:  40%|██████▊          | 16/40 [00:01<00:02, 11.82it/s]evaluate for the 18-th batch, evaluate loss: 0.41484007239341736:  45%|███████▋         | 18/40 [00:01<00:01, 11.41it/s]evaluate for the 40-th batch, evaluate loss: 0.7060384154319763:  98%|█████████████████▌| 39/40 [00:05<00:00,  7.01it/s]evaluate for the 40-th batch, evaluate loss: 0.7060384154319763: 100%|██████████████████| 40/40 [00:05<00:00,  6.52it/s]evaluate for the 40-th batch, evaluate loss: 0.7060384154319763: 100%|██████████████████| 40/40 [00:05<00:00,  7.46it/s]
Epoch: 8, train for the 98-th batch, train loss: 0.49013641476631165:  66%|███████▉    | 97/146 [00:14<00:08,  5.98it/s]Epoch: 5, train for the 29-th batch, train loss: 0.579810619354248:  12%|█▋            | 28/241 [00:04<00:29,  7.25it/s]evaluate for the 19-th batch, evaluate loss: 0.5826970934867859:  39%|███████           | 18/46 [00:01<00:02,  9.76it/s]evaluate for the 19-th batch, evaluate loss: 0.5826970934867859:  41%|███████▍          | 19/46 [00:01<00:02,  9.30it/s]Epoch: 8, train for the 98-th batch, train loss: 0.49013641476631165:  67%|████████    | 98/146 [00:14<00:08,  5.60it/s]Epoch: 5, train for the 29-th batch, train loss: 0.579810619354248:  12%|█▋            | 29/241 [00:04<00:32,  6.58it/s]evaluate for the 19-th batch, evaluate loss: 0.45556142926216125:  45%|███████▋         | 18/40 [00:01<00:01, 11.41it/s]Epoch: 2, train for the 180-th batch, train loss: 0.42876380681991577:  47%|████▋     | 179/383 [00:41<01:02,  3.29it/s]Epoch: 2, train for the 180-th batch, train loss: 0.42876380681991577:  47%|████▋     | 180/383 [00:41<01:06,  3.06it/s]evaluate for the 20-th batch, evaluate loss: 0.5286449790000916:  41%|███████▍          | 19/46 [00:02<00:02,  9.30it/s]evaluate for the 20-th batch, evaluate loss: 0.5286449790000916:  43%|███████▊          | 20/46 [00:02<00:02,  9.42it/s]evaluate for the 20-th batch, evaluate loss: 0.44479072093963623:  45%|███████▋         | 18/40 [00:01<00:01, 11.41it/s]evaluate for the 20-th batch, evaluate loss: 0.44479072093963623:  50%|████████▌        | 20/40 [00:01<00:01, 12.28it/s]Epoch: 5, train for the 30-th batch, train loss: 0.6462863087654114:  12%|█▌           | 29/241 [00:04<00:32,  6.58it/s]Epoch: 8, train for the 99-th batch, train loss: 0.5346142649650574:  67%|████████▋    | 98/146 [00:15<00:08,  5.60it/s]Epoch: 5, train for the 30-th batch, train loss: 0.6462863087654114:  12%|█▌           | 30/241 [00:04<00:31,  6.63it/s]Epoch: 8, train for the 99-th batch, train loss: 0.5346142649650574:  68%|████████▊    | 99/146 [00:15<00:08,  5.79it/s]evaluate for the 21-th batch, evaluate loss: 0.42703354358673096:  50%|████████▌        | 20/40 [00:01<00:01, 12.28it/s]evaluate for the 21-th batch, evaluate loss: 0.560040295124054:  43%|████████▎          | 20/46 [00:02<00:02,  9.42it/s]evaluate for the 21-th batch, evaluate loss: 0.560040295124054:  46%|████████▋          | 21/46 [00:02<00:02,  9.39it/s]evaluate for the 22-th batch, evaluate loss: 0.4264005124568939:  50%|█████████         | 20/40 [00:01<00:01, 12.28it/s]evaluate for the 22-th batch, evaluate loss: 0.4264005124568939:  55%|█████████▉        | 22/40 [00:01<00:01, 13.03it/s]Epoch: 5, train for the 31-th batch, train loss: 0.44631072878837585:  12%|█▍          | 30/241 [00:04<00:31,  6.63it/s]Epoch: 5, train for the 31-th batch, train loss: 0.44631072878837585:  13%|█▌          | 31/241 [00:04<00:29,  7.02it/s]Epoch: 8, train for the 100-th batch, train loss: 0.5313664674758911:  68%|████████▏   | 99/146 [00:15<00:08,  5.79it/s]evaluate for the 22-th batch, evaluate loss: 0.5042333602905273:  46%|████████▏         | 21/46 [00:02<00:02,  9.39it/s]evaluate for the 23-th batch, evaluate loss: 0.381376177072525:  55%|██████████▍        | 22/40 [00:01<00:01, 13.03it/s]Epoch: 8, train for the 100-th batch, train loss: 0.5313664674758911:  68%|███████▌   | 100/146 [00:15<00:07,  6.02it/s]Epoch: 2, train for the 181-th batch, train loss: 0.34651321172714233:  47%|████▋     | 180/383 [00:41<01:06,  3.06it/s]Epoch: 2, train for the 181-th batch, train loss: 0.34651321172714233:  47%|████▋     | 181/383 [00:41<01:03,  3.20it/s]evaluate for the 24-th batch, evaluate loss: 0.3797938823699951:  55%|█████████▉        | 22/40 [00:01<00:01, 13.03it/s]evaluate for the 24-th batch, evaluate loss: 0.3797938823699951:  60%|██████████▊       | 24/40 [00:01<00:01, 13.45it/s]Epoch: 5, train for the 32-th batch, train loss: 0.42555373907089233:  13%|█▌          | 31/241 [00:04<00:29,  7.02it/s]evaluate for the 23-th batch, evaluate loss: 0.4742003083229065:  46%|████████▏         | 21/46 [00:02<00:02,  9.39it/s]evaluate for the 23-th batch, evaluate loss: 0.4742003083229065:  50%|█████████         | 23/46 [00:02<00:02,  9.66it/s]Epoch: 5, train for the 32-th batch, train loss: 0.42555373907089233:  13%|█▌          | 32/241 [00:04<00:28,  7.21it/s]Epoch: 8, train for the 101-th batch, train loss: 0.4996761083602905:  68%|███████▌   | 100/146 [00:15<00:07,  6.02it/s]evaluate for the 25-th batch, evaluate loss: 0.4439524710178375:  60%|██████████▊       | 24/40 [00:02<00:01, 13.45it/s]Epoch: 8, train for the 101-th batch, train loss: 0.4996761083602905:  69%|███████▌   | 101/146 [00:15<00:07,  5.94it/s]evaluate for the 24-th batch, evaluate loss: 0.49388015270233154:  50%|████████▌        | 23/46 [00:02<00:02,  9.66it/s]Epoch: 5, train for the 33-th batch, train loss: 0.6081845760345459:  13%|█▋           | 32/241 [00:04<00:28,  7.21it/s]evaluate for the 26-th batch, evaluate loss: 0.39851394295692444:  60%|██████████▏      | 24/40 [00:02<00:01, 13.45it/s]evaluate for the 26-th batch, evaluate loss: 0.39851394295692444:  65%|███████████      | 26/40 [00:02<00:01, 13.36it/s]INFO:root:Epoch: 3, learning rate: 0.0001, train loss: 0.5698
INFO:root:train average_precision, 0.7920
Epoch: 5, train for the 33-th batch, train loss: 0.6081845760345459:  14%|█▊           | 33/241 [00:04<00:28,  7.22it/s]INFO:root:train roc_auc, 0.7675
INFO:root:validate loss: 0.5463
INFO:root:validate average_precision, 0.8174
INFO:root:validate roc_auc, 0.7983
INFO:root:new node validate loss: 0.6787
INFO:root:new node validate first_1_average_precision, 0.6192
INFO:root:new node validate first_1_roc_auc, 0.5475
INFO:root:new node validate first_3_average_precision, 0.6446
INFO:root:new node validate first_3_roc_auc, 0.5744
INFO:root:new node validate first_10_average_precision, 0.6709
INFO:root:new node validate first_10_roc_auc, 0.6142
INFO:root:new node validate average_precision, 0.6895
INFO:root:new node validate roc_auc, 0.6450
evaluate for the 25-th batch, evaluate loss: 0.5113234519958496:  50%|█████████         | 23/46 [00:02<00:02,  9.66it/s]evaluate for the 25-th batch, evaluate loss: 0.5113234519958496:  54%|█████████▊        | 25/46 [00:02<00:01, 10.92it/s]Epoch: 2, train for the 182-th batch, train loss: 0.4060792326927185:  47%|█████▏     | 181/383 [00:42<01:03,  3.20it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/237 [00:00<?, ?it/s]Epoch: 8, train for the 102-th batch, train loss: 0.49281591176986694:  69%|██████▉   | 101/146 [00:15<00:07,  5.94it/s]evaluate for the 27-th batch, evaluate loss: 0.4235633611679077:  65%|███████████▋      | 26/40 [00:02<00:01, 13.36it/s]evaluate for the 26-th batch, evaluate loss: 0.544867753982544:  54%|██████████▎        | 25/46 [00:02<00:01, 10.92it/s]Epoch: 8, train for the 102-th batch, train loss: 0.49281591176986694:  70%|██████▉   | 102/146 [00:15<00:07,  6.16it/s]Epoch: 2, train for the 182-th batch, train loss: 0.4060792326927185:  48%|█████▏     | 182/383 [00:42<00:59,  3.38it/s]Epoch: 5, train for the 34-th batch, train loss: 0.5383126735687256:  14%|█▊           | 33/241 [00:04<00:28,  7.22it/s]evaluate for the 28-th batch, evaluate loss: 0.41194403171539307:  65%|███████████      | 26/40 [00:02<00:01, 13.36it/s]evaluate for the 28-th batch, evaluate loss: 0.41194403171539307:  70%|███████████▉     | 28/40 [00:02<00:00, 13.58it/s]Epoch: 5, train for the 34-th batch, train loss: 0.5383126735687256:  14%|█▊           | 34/241 [00:04<00:28,  7.29it/s]Epoch: 4, train for the 1-th batch, train loss: 0.7172062397003174:   0%|                       | 0/237 [00:00<?, ?it/s]Epoch: 4, train for the 1-th batch, train loss: 0.7172062397003174:   0%|               | 1/237 [00:00<00:25,  9.22it/s]evaluate for the 27-th batch, evaluate loss: 0.5212253332138062:  54%|█████████▊        | 25/46 [00:02<00:01, 10.92it/s]evaluate for the 27-th batch, evaluate loss: 0.5212253332138062:  59%|██████████▌       | 27/46 [00:02<00:01, 11.50it/s]evaluate for the 29-th batch, evaluate loss: 0.41756927967071533:  70%|███████████▉     | 28/40 [00:02<00:00, 13.58it/s]evaluate for the 28-th batch, evaluate loss: 0.518237829208374:  59%|███████████▏       | 27/46 [00:02<00:01, 11.50it/s]Epoch: 8, train for the 103-th batch, train loss: 0.5146791934967041:  70%|███████▋   | 102/146 [00:15<00:07,  6.16it/s]evaluate for the 30-th batch, evaluate loss: 0.41455936431884766:  70%|███████████▉     | 28/40 [00:02<00:00, 13.58it/s]evaluate for the 30-th batch, evaluate loss: 0.41455936431884766:  75%|████████████▊    | 30/40 [00:02<00:00, 13.41it/s]Epoch: 8, train for the 103-th batch, train loss: 0.5146791934967041:  71%|███████▊   | 103/146 [00:15<00:07,  5.73it/s]Epoch: 5, train for the 35-th batch, train loss: 0.575161337852478:  14%|█▉            | 34/241 [00:04<00:28,  7.29it/s]Epoch: 5, train for the 35-th batch, train loss: 0.575161337852478:  15%|██            | 35/241 [00:04<00:30,  6.85it/s]Epoch: 4, train for the 2-th batch, train loss: 0.688048243522644:   0%|                | 1/237 [00:00<00:25,  9.22it/s]Epoch: 4, train for the 2-th batch, train loss: 0.688048243522644:   1%|▏               | 2/237 [00:00<00:34,  6.78it/s]evaluate for the 31-th batch, evaluate loss: 0.4118245840072632:  75%|█████████████▌    | 30/40 [00:02<00:00, 13.41it/s]Epoch: 2, train for the 183-th batch, train loss: 0.3433670699596405:  48%|█████▏     | 182/383 [00:42<00:59,  3.38it/s]evaluate for the 29-th batch, evaluate loss: 0.49760299921035767:  59%|█████████▉       | 27/46 [00:02<00:01, 11.50it/s]evaluate for the 29-th batch, evaluate loss: 0.49760299921035767:  63%|██████████▋      | 29/46 [00:02<00:01, 10.92it/s]Epoch: 2, train for the 183-th batch, train loss: 0.3433670699596405:  48%|█████▎     | 183/383 [00:42<01:01,  3.25it/s]evaluate for the 30-th batch, evaluate loss: 0.49750572443008423:  63%|██████████▋      | 29/46 [00:02<00:01, 10.92it/s]Epoch: 8, train for the 104-th batch, train loss: 0.5058472156524658:  71%|███████▊   | 103/146 [00:15<00:07,  5.73it/s]evaluate for the 32-th batch, evaluate loss: 0.40651461482048035:  75%|████████████▊    | 30/40 [00:02<00:00, 13.41it/s]evaluate for the 32-th batch, evaluate loss: 0.40651461482048035:  80%|█████████████▌   | 32/40 [00:02<00:00, 12.73it/s]Epoch: 4, train for the 3-th batch, train loss: 0.6867101192474365:   1%|▏              | 2/237 [00:00<00:34,  6.78it/s]Epoch: 4, train for the 3-th batch, train loss: 0.6867101192474365:   1%|▏              | 3/237 [00:00<00:34,  6.81it/s]Epoch: 8, train for the 104-th batch, train loss: 0.5058472156524658:  71%|███████▊   | 104/146 [00:15<00:07,  5.48it/s]evaluate for the 31-th batch, evaluate loss: 0.4358974099159241:  63%|███████████▎      | 29/46 [00:02<00:01, 10.92it/s]evaluate for the 31-th batch, evaluate loss: 0.4358974099159241:  67%|████████████▏     | 31/46 [00:02<00:01, 11.57it/s]evaluate for the 33-th batch, evaluate loss: 0.41587355732917786:  80%|█████████████▌   | 32/40 [00:02<00:00, 12.73it/s]evaluate for the 32-th batch, evaluate loss: 0.4691255986690521:  67%|████████████▏     | 31/46 [00:03<00:01, 11.57it/s]Epoch: 4, train for the 4-th batch, train loss: 0.6333690881729126:   1%|▏              | 3/237 [00:00<00:34,  6.81it/s]Epoch: 4, train for the 4-th batch, train loss: 0.6333690881729126:   2%|▎              | 4/237 [00:00<00:33,  6.90it/s]evaluate for the 34-th batch, evaluate loss: 0.4120948016643524:  80%|██████████████▍   | 32/40 [00:02<00:00, 12.73it/s]evaluate for the 34-th batch, evaluate loss: 0.4120948016643524:  85%|███████████████▎  | 34/40 [00:02<00:00, 12.65it/s]Epoch: 8, train for the 105-th batch, train loss: 0.5101131796836853:  71%|███████▊   | 104/146 [00:16<00:07,  5.48it/s]Epoch: 2, train for the 184-th batch, train loss: 0.3800516426563263:  48%|█████▎     | 183/383 [00:42<01:01,  3.25it/s]Epoch: 8, train for the 105-th batch, train loss: 0.5101131796836853:  72%|███████▉   | 105/146 [00:16<00:07,  5.53it/s]evaluate for the 35-th batch, evaluate loss: 0.4549004137516022:  85%|███████████████▎  | 34/40 [00:02<00:00, 12.65it/s]evaluate for the 33-th batch, evaluate loss: 0.4797375500202179:  67%|████████████▏     | 31/46 [00:03<00:01, 11.57it/s]evaluate for the 33-th batch, evaluate loss: 0.4797375500202179:  72%|████████████▉     | 33/46 [00:03<00:01, 11.20it/s]Epoch: 2, train for the 184-th batch, train loss: 0.3800516426563263:  48%|█████▎     | 184/383 [00:42<00:59,  3.32it/s]Epoch: 5, train for the 36-th batch, train loss: 0.5772806406021118:  15%|█▉           | 35/241 [00:05<00:30,  6.85it/s]Epoch: 5, train for the 36-th batch, train loss: 0.5772806406021118:  15%|█▉           | 36/241 [00:05<00:47,  4.36it/s]evaluate for the 36-th batch, evaluate loss: 0.40343379974365234:  85%|██████████████▍  | 34/40 [00:02<00:00, 12.65it/s]evaluate for the 36-th batch, evaluate loss: 0.40343379974365234:  90%|███████████████▎ | 36/40 [00:02<00:00, 13.84it/s]Epoch: 4, train for the 5-th batch, train loss: 0.6740820407867432:   2%|▎              | 4/237 [00:00<00:33,  6.90it/s]Epoch: 4, train for the 5-th batch, train loss: 0.6740820407867432:   2%|▎              | 5/237 [00:00<00:32,  7.15it/s]Epoch: 8, train for the 106-th batch, train loss: 0.4948917329311371:  72%|███████▉   | 105/146 [00:16<00:07,  5.53it/s]evaluate for the 34-th batch, evaluate loss: 0.45257800817489624:  72%|████████████▏    | 33/46 [00:03<00:01, 11.20it/s]Epoch: 8, train for the 106-th batch, train loss: 0.4948917329311371:  73%|███████▉   | 106/146 [00:16<00:06,  5.93it/s]evaluate for the 37-th batch, evaluate loss: 0.44615036249160767:  90%|███████████████▎ | 36/40 [00:02<00:00, 13.84it/s]Epoch: 5, train for the 37-th batch, train loss: 0.501812219619751:  15%|██            | 36/241 [00:05<00:47,  4.36it/s]Epoch: 5, train for the 37-th batch, train loss: 0.501812219619751:  15%|██▏           | 37/241 [00:05<00:41,  4.93it/s]Epoch: 2, train for the 185-th batch, train loss: 0.3758460283279419:  48%|█████▎     | 184/383 [00:42<00:59,  3.32it/s]evaluate for the 38-th batch, evaluate loss: 0.4202709197998047:  90%|████████████████▏ | 36/40 [00:02<00:00, 13.84it/s]evaluate for the 38-th batch, evaluate loss: 0.4202709197998047:  95%|█████████████████ | 38/40 [00:02<00:00, 13.38it/s]Epoch: 2, train for the 185-th batch, train loss: 0.3758460283279419:  48%|█████▎     | 185/383 [00:42<00:55,  3.58it/s]Epoch: 8, train for the 107-th batch, train loss: 0.4816817343235016:  73%|███████▉   | 106/146 [00:16<00:06,  5.93it/s]Epoch: 8, train for the 107-th batch, train loss: 0.4816817343235016:  73%|████████   | 107/146 [00:16<00:06,  6.23it/s]Epoch: 5, train for the 38-th batch, train loss: 0.5686405301094055:  15%|█▉           | 37/241 [00:05<00:41,  4.93it/s]Epoch: 5, train for the 38-th batch, train loss: 0.5686405301094055:  16%|██           | 38/241 [00:05<00:35,  5.69it/s]evaluate for the 35-th batch, evaluate loss: 0.49595460295677185:  72%|████████████▏    | 33/46 [00:03<00:01, 11.20it/s]evaluate for the 35-th batch, evaluate loss: 0.49595460295677185:  76%|████████████▉    | 35/46 [00:03<00:01,  8.96it/s]Epoch: 8, train for the 108-th batch, train loss: 0.4768417179584503:  73%|████████   | 107/146 [00:16<00:06,  6.23it/s]Epoch: 4, train for the 6-th batch, train loss: 0.666015625:   2%|▍                     | 5/237 [00:01<00:32,  7.15it/s]Epoch: 8, train for the 108-th batch, train loss: 0.4768417179584503:  74%|████████▏  | 108/146 [00:16<00:05,  6.85it/s]Epoch: 4, train for the 6-th batch, train loss: 0.666015625:   3%|▌                     | 6/237 [00:01<00:46,  4.98it/s]evaluate for the 39-th batch, evaluate loss: 0.4785715341567993:  95%|█████████████████ | 38/40 [00:03<00:00, 13.38it/s]Epoch: 5, train for the 39-th batch, train loss: 0.6045911908149719:  16%|██           | 38/241 [00:05<00:35,  5.69it/s]Epoch: 5, train for the 39-th batch, train loss: 0.6045911908149719:  16%|██           | 39/241 [00:05<00:31,  6.41it/s]evaluate for the 36-th batch, evaluate loss: 0.46395719051361084:  76%|████████████▉    | 35/46 [00:03<00:01,  8.96it/s]evaluate for the 40-th batch, evaluate loss: 0.32676440477371216:  95%|████████████████▏| 38/40 [00:03<00:00, 13.38it/s]evaluate for the 40-th batch, evaluate loss: 0.32676440477371216: 100%|█████████████████| 40/40 [00:03<00:00, 11.06it/s]evaluate for the 40-th batch, evaluate loss: 0.32676440477371216: 100%|█████████████████| 40/40 [00:03<00:00, 12.29it/s]
evaluate for the 37-th batch, evaluate loss: 0.5055111050605774:  76%|█████████████▋    | 35/46 [00:03<00:01,  8.96it/s]evaluate for the 37-th batch, evaluate loss: 0.5055111050605774:  80%|██████████████▍   | 37/46 [00:03<00:00,  9.97it/s]Epoch: 2, train for the 186-th batch, train loss: 0.36419105529785156:  48%|████▊     | 185/383 [00:43<00:55,  3.58it/s]Epoch: 5, train for the 40-th batch, train loss: 0.6425544619560242:  16%|██           | 39/241 [00:05<00:31,  6.41it/s]Epoch: 5, train for the 40-th batch, train loss: 0.6425544619560242:  17%|██▏          | 40/241 [00:05<00:30,  6.66it/s]Epoch: 2, train for the 186-th batch, train loss: 0.36419105529785156:  49%|████▊     | 186/383 [00:43<00:55,  3.54it/s]Epoch: 8, train for the 109-th batch, train loss: 0.5149903297424316:  74%|████████▏  | 108/146 [00:16<00:05,  6.85it/s]Epoch: 8, train for the 109-th batch, train loss: 0.5149903297424316:  75%|████████▏  | 109/146 [00:16<00:05,  6.40it/s]Epoch: 4, train for the 7-th batch, train loss: 0.6333740949630737:   3%|▍              | 6/237 [00:01<00:46,  4.98it/s]Epoch: 4, train for the 7-th batch, train loss: 0.6333740949630737:   3%|▍              | 7/237 [00:01<00:44,  5.16it/s]evaluate for the 38-th batch, evaluate loss: 0.48225125670433044:  80%|█████████████▋   | 37/46 [00:03<00:00,  9.97it/s]  0%|                                                                                            | 0/21 [00:00<?, ?it/s]evaluate for the 39-th batch, evaluate loss: 0.46470412611961365:  80%|█████████████▋   | 37/46 [00:03<00:00,  9.97it/s]evaluate for the 39-th batch, evaluate loss: 0.46470412611961365:  85%|██████████████▍  | 39/46 [00:03<00:00,  9.98it/s]Epoch: 4, train for the 8-th batch, train loss: 0.6717908978462219:   3%|▍              | 7/237 [00:01<00:44,  5.16it/s]evaluate for the 1-th batch, evaluate loss: 0.7646723985671997:   0%|                            | 0/21 [00:00<?, ?it/s]Epoch: 8, train for the 110-th batch, train loss: 0.4479166269302368:  75%|████████▏  | 109/146 [00:16<00:05,  6.40it/s]Epoch: 4, train for the 8-th batch, train loss: 0.6717908978462219:   3%|▌              | 8/237 [00:01<00:41,  5.46it/s]Epoch: 8, train for the 110-th batch, train loss: 0.4479166269302368:  75%|████████▎  | 110/146 [00:16<00:05,  6.05it/s]evaluate for the 40-th batch, evaluate loss: 0.4615318477153778:  85%|███████████████▎  | 39/46 [00:03<00:00,  9.98it/s]Epoch: 2, train for the 187-th batch, train loss: 0.3149087429046631:  49%|█████▎     | 186/383 [00:43<00:55,  3.54it/s]evaluate for the 2-th batch, evaluate loss: 0.7581105828285217:   0%|                            | 0/21 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.7581105828285217:  10%|█▉                  | 2/21 [00:00<00:01, 12.54it/s]Epoch: 2, train for the 187-th batch, train loss: 0.3149087429046631:  49%|█████▎     | 187/383 [00:43<00:55,  3.53it/s]evaluate for the 41-th batch, evaluate loss: 0.4892372786998749:  85%|███████████████▎  | 39/46 [00:04<00:00,  9.98it/s]evaluate for the 41-th batch, evaluate loss: 0.4892372786998749:  89%|████████████████  | 41/46 [00:04<00:00, 10.91it/s]Epoch: 4, train for the 9-th batch, train loss: 0.5789004564285278:   3%|▌              | 8/237 [00:01<00:41,  5.46it/s]Epoch: 4, train for the 9-th batch, train loss: 0.5789004564285278:   4%|▌              | 9/237 [00:01<00:37,  6.08it/s]evaluate for the 3-th batch, evaluate loss: 0.6629336476325989:  10%|█▉                  | 2/21 [00:00<00:01, 12.54it/s]Epoch: 8, train for the 111-th batch, train loss: 0.47444847226142883:  75%|███████▌  | 110/146 [00:17<00:05,  6.05it/s]Epoch: 8, train for the 111-th batch, train loss: 0.47444847226142883:  76%|███████▌  | 111/146 [00:17<00:05,  6.16it/s]evaluate for the 42-th batch, evaluate loss: 0.43283116817474365:  89%|███████████████▏ | 41/46 [00:04<00:00, 10.91it/s]Epoch: 5, train for the 41-th batch, train loss: 0.547749400138855:  17%|██▎           | 40/241 [00:06<00:30,  6.66it/s]Epoch: 5, train for the 41-th batch, train loss: 0.547749400138855:  17%|██▍           | 41/241 [00:06<00:44,  4.48it/s]Epoch: 4, train for the 10-th batch, train loss: 0.6664766073226929:   4%|▌             | 9/237 [00:01<00:37,  6.08it/s]Epoch: 4, train for the 10-th batch, train loss: 0.6664766073226929:   4%|▌            | 10/237 [00:01<00:36,  6.20it/s]evaluate for the 43-th batch, evaluate loss: 0.5230852365493774:  89%|████████████████  | 41/46 [00:04<00:00, 10.91it/s]evaluate for the 43-th batch, evaluate loss: 0.5230852365493774:  93%|████████████████▊ | 43/46 [00:04<00:00, 11.08it/s]Epoch: 8, train for the 112-th batch, train loss: 0.5495336055755615:  76%|████████▎  | 111/146 [00:17<00:05,  6.16it/s]Epoch: 8, train for the 112-th batch, train loss: 0.5495336055755615:  77%|████████▍  | 112/146 [00:17<00:05,  6.55it/s]Epoch: 5, train for the 42-th batch, train loss: 0.5955862998962402:  17%|██▏          | 41/241 [00:06<00:44,  4.48it/s]evaluate for the 4-th batch, evaluate loss: 0.6093522906303406:  10%|█▉                  | 2/21 [00:00<00:01, 12.54it/s]evaluate for the 4-th batch, evaluate loss: 0.6093522906303406:  19%|███▊                | 4/21 [00:00<00:01,  8.66it/s]Epoch: 2, train for the 188-th batch, train loss: 0.43637529015541077:  49%|████▉     | 187/383 [00:43<00:55,  3.53it/s]Epoch: 5, train for the 42-th batch, train loss: 0.5955862998962402:  17%|██▎          | 42/241 [00:06<00:39,  5.03it/s]evaluate for the 44-th batch, evaluate loss: 0.47467362880706787:  93%|███████████████▉ | 43/46 [00:04<00:00, 11.08it/s]Epoch: 2, train for the 188-th batch, train loss: 0.43637529015541077:  49%|████▉     | 188/383 [00:43<00:56,  3.44it/s]Epoch: 4, train for the 11-th batch, train loss: 0.5967847108840942:   4%|▌            | 10/237 [00:01<00:36,  6.20it/s]Epoch: 4, train for the 11-th batch, train loss: 0.5967847108840942:   5%|▌            | 11/237 [00:01<00:35,  6.35it/s]evaluate for the 5-th batch, evaluate loss: 0.7167356014251709:  19%|███▊                | 4/21 [00:00<00:01,  8.66it/s]evaluate for the 5-th batch, evaluate loss: 0.7167356014251709:  24%|████▊               | 5/21 [00:00<00:01,  8.83it/s]Epoch: 8, train for the 113-th batch, train loss: 0.4571010172367096:  77%|████████▍  | 112/146 [00:17<00:05,  6.55it/s]evaluate for the 45-th batch, evaluate loss: 0.45628172159194946:  93%|███████████████▉ | 43/46 [00:04<00:00, 11.08it/s]evaluate for the 45-th batch, evaluate loss: 0.45628172159194946:  98%|████████████████▋| 45/46 [00:04<00:00, 10.68it/s]Epoch: 8, train for the 113-th batch, train loss: 0.4571010172367096:  77%|████████▌  | 113/146 [00:17<00:05,  6.11it/s]Epoch: 5, train for the 43-th batch, train loss: 0.6280030012130737:  17%|██▎          | 42/241 [00:06<00:39,  5.03it/s]Epoch: 5, train for the 43-th batch, train loss: 0.6280030012130737:  18%|██▎          | 43/241 [00:06<00:37,  5.26it/s]evaluate for the 6-th batch, evaluate loss: 0.6775028705596924:  24%|████▊               | 5/21 [00:00<00:01,  8.83it/s]Epoch: 4, train for the 12-th batch, train loss: 0.6688042879104614:   5%|▌            | 11/237 [00:01<00:35,  6.35it/s]Epoch: 4, train for the 12-th batch, train loss: 0.6688042879104614:   5%|▋            | 12/237 [00:01<00:34,  6.43it/s]evaluate for the 46-th batch, evaluate loss: 0.43531644344329834:  98%|████████████████▋| 45/46 [00:04<00:00, 10.68it/s]evaluate for the 46-th batch, evaluate loss: 0.43531644344329834: 100%|█████████████████| 46/46 [00:04<00:00, 10.27it/s]
evaluate for the 7-th batch, evaluate loss: 0.6082973480224609:  24%|████▊               | 5/21 [00:00<00:01,  8.83it/s]evaluate for the 7-th batch, evaluate loss: 0.6082973480224609:  33%|██████▋             | 7/21 [00:00<00:01, 10.10it/s]Epoch: 2, train for the 189-th batch, train loss: 0.47014757990837097:  49%|████▉     | 188/383 [00:44<00:56,  3.44it/s]Epoch: 8, train for the 114-th batch, train loss: 0.482857882976532:  77%|█████████▎  | 113/146 [00:17<00:05,  6.11it/s]Epoch: 8, train for the 114-th batch, train loss: 0.482857882976532:  78%|█████████▎  | 114/146 [00:17<00:05,  6.21it/s]Epoch: 2, train for the 189-th batch, train loss: 0.47014757990837097:  49%|████▉     | 189/383 [00:44<00:54,  3.55it/s]evaluate for the 8-th batch, evaluate loss: 0.62111496925354:  33%|███████▎              | 7/21 [00:00<00:01, 10.10it/s]Epoch: 4, train for the 13-th batch, train loss: 0.6697222590446472:   5%|▋            | 12/237 [00:02<00:34,  6.43it/s]Epoch: 4, train for the 13-th batch, train loss: 0.6697222590446472:   5%|▋            | 13/237 [00:02<00:33,  6.74it/s]Epoch: 5, train for the 44-th batch, train loss: 0.667486310005188:  18%|██▍           | 43/241 [00:06<00:37,  5.26it/s]Epoch: 5, train for the 44-th batch, train loss: 0.667486310005188:  18%|██▌           | 44/241 [00:06<00:39,  4.95it/s]evaluate for the 9-th batch, evaluate loss: 0.5962710380554199:  33%|██████▋             | 7/21 [00:00<00:01, 10.10it/s]evaluate for the 9-th batch, evaluate loss: 0.5962710380554199:  43%|████████▌           | 9/21 [00:00<00:01, 11.03it/s]Epoch: 8, train for the 115-th batch, train loss: 0.4834003448486328:  78%|████████▌  | 114/146 [00:17<00:05,  6.21it/s]Epoch: 8, train for the 115-th batch, train loss: 0.4834003448486328:  79%|████████▋  | 115/146 [00:17<00:04,  6.40it/s]evaluate for the 10-th batch, evaluate loss: 0.6057798862457275:  43%|████████▏          | 9/21 [00:00<00:01, 11.03it/s]Epoch: 2, train for the 190-th batch, train loss: 0.6245972514152527:  49%|█████▍     | 189/383 [00:44<00:54,  3.55it/s]Epoch: 4, train for the 14-th batch, train loss: 0.703824520111084:   5%|▊             | 13/237 [00:02<00:33,  6.74it/s]Epoch: 4, train for the 14-th batch, train loss: 0.703824520111084:   6%|▊             | 14/237 [00:02<00:36,  6.13it/s]  0%|                                                                                            | 0/25 [00:00<?, ?it/s]Epoch: 5, train for the 45-th batch, train loss: 0.523883044719696:  18%|██▌           | 44/241 [00:06<00:39,  4.95it/s]Epoch: 2, train for the 190-th batch, train loss: 0.6245972514152527:  50%|█████▍     | 190/383 [00:44<00:52,  3.65it/s]Epoch: 8, train for the 116-th batch, train loss: 0.5185929536819458:  79%|████████▋  | 115/146 [00:17<00:04,  6.40it/s]Epoch: 5, train for the 45-th batch, train loss: 0.523883044719696:  19%|██▌           | 45/241 [00:06<00:38,  5.11it/s]evaluate for the 11-th batch, evaluate loss: 0.6173003315925598:  43%|████████▏          | 9/21 [00:01<00:01, 11.03it/s]evaluate for the 11-th batch, evaluate loss: 0.6173003315925598:  52%|█████████▍        | 11/21 [00:01<00:00, 11.35it/s]Epoch: 8, train for the 116-th batch, train loss: 0.5185929536819458:  79%|████████▋  | 116/146 [00:17<00:04,  6.53it/s]evaluate for the 1-th batch, evaluate loss: 0.770741879940033:   0%|                             | 0/25 [00:00<?, ?it/s]evaluate for the 12-th batch, evaluate loss: 0.6155595779418945:  52%|█████████▍        | 11/21 [00:01<00:00, 11.35it/s]Epoch: 4, train for the 15-th batch, train loss: 0.6792502999305725:   6%|▊            | 14/237 [00:02<00:36,  6.13it/s]Epoch: 4, train for the 15-th batch, train loss: 0.6792502999305725:   6%|▊            | 15/237 [00:02<00:34,  6.39it/s]evaluate for the 2-th batch, evaluate loss: 0.7861190438270569:   0%|                            | 0/25 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.7861190438270569:   8%|█▌                  | 2/25 [00:00<00:02, 10.86it/s]Epoch: 8, train for the 117-th batch, train loss: 0.5681517720222473:  79%|████████▋  | 116/146 [00:17<00:04,  6.53it/s]evaluate for the 13-th batch, evaluate loss: 0.5993542075157166:  52%|█████████▍        | 11/21 [00:01<00:00, 11.35it/s]evaluate for the 13-th batch, evaluate loss: 0.5993542075157166:  62%|███████████▏      | 13/21 [00:01<00:00, 10.93it/s]Epoch: 8, train for the 117-th batch, train loss: 0.5681517720222473:  80%|████████▊  | 117/146 [00:17<00:04,  6.00it/s]Epoch: 5, train for the 46-th batch, train loss: 0.5246431827545166:  19%|██▍          | 45/241 [00:07<00:38,  5.11it/s]Epoch: 5, train for the 46-th batch, train loss: 0.5246431827545166:  19%|██▍          | 46/241 [00:07<00:40,  4.83it/s]evaluate for the 3-th batch, evaluate loss: 0.7938413619995117:   8%|█▌                  | 2/25 [00:00<00:02, 10.86it/s]Epoch: 2, train for the 191-th batch, train loss: 0.3826068341732025:  50%|█████▍     | 190/383 [00:44<00:52,  3.65it/s]evaluate for the 14-th batch, evaluate loss: 0.5714929103851318:  62%|███████████▏      | 13/21 [00:01<00:00, 10.93it/s]Epoch: 4, train for the 16-th batch, train loss: 0.6482513546943665:   6%|▊            | 15/237 [00:02<00:34,  6.39it/s]Epoch: 4, train for the 16-th batch, train loss: 0.6482513546943665:   7%|▉            | 16/237 [00:02<00:35,  6.14it/s]Epoch: 2, train for the 191-th batch, train loss: 0.3826068341732025:  50%|█████▍     | 191/383 [00:44<00:54,  3.50it/s]Epoch: 8, train for the 118-th batch, train loss: 0.48797228932380676:  80%|████████  | 117/146 [00:18<00:04,  6.00it/s]Epoch: 8, train for the 118-th batch, train loss: 0.48797228932380676:  81%|████████  | 118/146 [00:18<00:04,  6.43it/s]evaluate for the 4-th batch, evaluate loss: 0.7393370866775513:   8%|█▌                  | 2/25 [00:00<00:02, 10.86it/s]evaluate for the 4-th batch, evaluate loss: 0.7393370866775513:  16%|███▏                | 4/25 [00:00<00:01, 11.01it/s]evaluate for the 15-th batch, evaluate loss: 0.5919370651245117:  62%|███████████▏      | 13/21 [00:01<00:00, 10.93it/s]evaluate for the 15-th batch, evaluate loss: 0.5919370651245117:  71%|████████████▊     | 15/21 [00:01<00:00, 11.71it/s]Epoch: 5, train for the 47-th batch, train loss: 0.5923606157302856:  19%|██▍          | 46/241 [00:07<00:40,  4.83it/s]Epoch: 5, train for the 47-th batch, train loss: 0.5923606157302856:  20%|██▌          | 47/241 [00:07<00:36,  5.33it/s]Epoch: 4, train for the 17-th batch, train loss: 0.6414059400558472:   7%|▉            | 16/237 [00:02<00:35,  6.14it/s]Epoch: 4, train for the 17-th batch, train loss: 0.6414059400558472:   7%|▉            | 17/237 [00:02<00:35,  6.19it/s]evaluate for the 16-th batch, evaluate loss: 0.554493248462677:  71%|█████████████▌     | 15/21 [00:01<00:00, 11.71it/s]evaluate for the 5-th batch, evaluate loss: 0.7703361511230469:  16%|███▏                | 4/25 [00:00<00:01, 11.01it/s]Epoch: 8, train for the 119-th batch, train loss: 0.4681260287761688:  81%|████████▉  | 118/146 [00:18<00:04,  6.43it/s]Epoch: 8, train for the 119-th batch, train loss: 0.4681260287761688:  82%|████████▉  | 119/146 [00:18<00:04,  6.26it/s]Epoch: 5, train for the 48-th batch, train loss: 0.5661643147468567:  20%|██▌          | 47/241 [00:07<00:36,  5.33it/s]evaluate for the 17-th batch, evaluate loss: 0.4789678752422333:  71%|████████████▊     | 15/21 [00:01<00:00, 11.71it/s]evaluate for the 17-th batch, evaluate loss: 0.4789678752422333:  81%|██████████████▌   | 17/21 [00:01<00:00, 11.74it/s]Epoch: 5, train for the 48-th batch, train loss: 0.5661643147468567:  20%|██▌          | 48/241 [00:07<00:34,  5.65it/s]evaluate for the 6-th batch, evaluate loss: 0.7636139988899231:  16%|███▏                | 4/25 [00:00<00:01, 11.01it/s]evaluate for the 6-th batch, evaluate loss: 0.7636139988899231:  24%|████▊               | 6/25 [00:00<00:01, 10.72it/s]Epoch: 2, train for the 192-th batch, train loss: 0.3748285174369812:  50%|█████▍     | 191/383 [00:44<00:54,  3.50it/s]Epoch: 4, train for the 18-th batch, train loss: 0.5942712426185608:   7%|▉            | 17/237 [00:02<00:35,  6.19it/s]evaluate for the 18-th batch, evaluate loss: 0.5824453830718994:  81%|██████████████▌   | 17/21 [00:01<00:00, 11.74it/s]Epoch: 4, train for the 18-th batch, train loss: 0.5942712426185608:   8%|▉            | 18/237 [00:02<00:34,  6.39it/s]Epoch: 2, train for the 192-th batch, train loss: 0.3748285174369812:  50%|█████▌     | 192/383 [00:44<00:54,  3.48it/s]evaluate for the 7-th batch, evaluate loss: 0.7602465748786926:  24%|████▊               | 6/25 [00:00<00:01, 10.72it/s]Epoch: 8, train for the 120-th batch, train loss: 0.48001325130462646:  82%|████████▏ | 119/146 [00:18<00:04,  6.26it/s]Epoch: 8, train for the 120-th batch, train loss: 0.48001325130462646:  82%|████████▏ | 120/146 [00:18<00:04,  6.34it/s]Epoch: 5, train for the 49-th batch, train loss: 0.5665533542633057:  20%|██▌          | 48/241 [00:07<00:34,  5.65it/s]evaluate for the 19-th batch, evaluate loss: 0.5815547108650208:  81%|██████████████▌   | 17/21 [00:01<00:00, 11.74it/s]evaluate for the 19-th batch, evaluate loss: 0.5815547108650208:  90%|████████████████▎ | 19/21 [00:01<00:00, 12.27it/s]Epoch: 5, train for the 49-th batch, train loss: 0.5665533542633057:  20%|██▋          | 49/241 [00:07<00:32,  5.91it/s]evaluate for the 20-th batch, evaluate loss: 0.5136681795120239:  90%|████████████████▎ | 19/21 [00:01<00:00, 12.27it/s]evaluate for the 8-th batch, evaluate loss: 0.7463756203651428:  24%|████▊               | 6/25 [00:00<00:01, 10.72it/s]Epoch: 4, train for the 19-th batch, train loss: 0.6043435335159302:   8%|▉            | 18/237 [00:03<00:34,  6.39it/s]evaluate for the 8-th batch, evaluate loss: 0.7463756203651428:  32%|██████▍             | 8/25 [00:00<00:01, 10.27it/s]Epoch: 4, train for the 19-th batch, train loss: 0.6043435335159302:   8%|█            | 19/237 [00:03<00:34,  6.36it/s]evaluate for the 21-th batch, evaluate loss: 0.39118465781211853:  90%|███████████████▍ | 19/21 [00:01<00:00, 12.27it/s]evaluate for the 21-th batch, evaluate loss: 0.39118465781211853: 100%|█████████████████| 21/21 [00:01<00:00, 13.27it/s]evaluate for the 21-th batch, evaluate loss: 0.39118465781211853: 100%|█████████████████| 21/21 [00:01<00:00, 11.56it/s]
Epoch: 2, train for the 193-th batch, train loss: 0.4942132532596588:  50%|█████▌     | 192/383 [00:45<00:54,  3.48it/s]Epoch: 8, train for the 121-th batch, train loss: 0.4602231979370117:  82%|█████████  | 120/146 [00:18<00:04,  6.34it/s]Epoch: 2, train for the 193-th batch, train loss: 0.4942132532596588:  50%|█████▌     | 193/383 [00:45<00:51,  3.67it/s]Epoch: 5, train for the 50-th batch, train loss: 0.5673384666442871:  20%|██▋          | 49/241 [00:07<00:32,  5.91it/s]evaluate for the 9-th batch, evaluate loss: 0.7096136212348938:  32%|██████▍             | 8/25 [00:00<00:01, 10.27it/s]Epoch: 8, train for the 121-th batch, train loss: 0.4602231979370117:  83%|█████████  | 121/146 [00:18<00:04,  5.87it/s]Epoch: 5, train for the 50-th batch, train loss: 0.5673384666442871:  21%|██▋          | 50/241 [00:07<00:32,  5.80it/s]Epoch: 4, train for the 20-th batch, train loss: 0.5943378210067749:   8%|█            | 19/237 [00:03<00:34,  6.36it/s]Epoch: 4, train for the 20-th batch, train loss: 0.5943378210067749:   8%|█            | 20/237 [00:03<00:36,  5.96it/s]evaluate for the 10-th batch, evaluate loss: 0.7677918672561646:  32%|██████             | 8/25 [00:00<00:01, 10.27it/s]evaluate for the 10-th batch, evaluate loss: 0.7677918672561646:  40%|███████▏          | 10/25 [00:00<00:01, 10.03it/s]INFO:root:Epoch: 9, learning rate: 0.0001, train loss: 0.4153
INFO:root:train average_precision, 0.8977
INFO:root:train roc_auc, 0.8907
INFO:root:validate loss: 0.4338
INFO:root:validate average_precision, 0.8736
INFO:root:validate roc_auc, 0.8734
INFO:root:new node validate loss: 0.6057
INFO:root:new node validate first_1_average_precision, 0.7354
INFO:root:new node validate first_1_roc_auc, 0.7230
INFO:root:new node validate first_3_average_precision, 0.7395
INFO:root:new node validate first_3_roc_auc, 0.7321
INFO:root:new node validate first_10_average_precision, 0.7326
INFO:root:new node validate first_10_roc_auc, 0.7377
INFO:root:new node validate average_precision, 0.7377
INFO:root:new node validate roc_auc, 0.7576
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/119 [00:00<?, ?it/s]Epoch: 5, train for the 51-th batch, train loss: 0.5635132789611816:  21%|██▋          | 50/241 [00:07<00:32,  5.80it/s]Epoch: 8, train for the 122-th batch, train loss: 0.5500573515892029:  83%|█████████  | 121/146 [00:18<00:04,  5.87it/s]Epoch: 5, train for the 51-th batch, train loss: 0.5635132789611816:  21%|██▊          | 51/241 [00:07<00:31,  5.98it/s]Epoch: 8, train for the 122-th batch, train loss: 0.5500573515892029:  84%|█████████▏ | 122/146 [00:18<00:04,  5.98it/s]Epoch: 2, train for the 194-th batch, train loss: 0.441476047039032:  50%|██████      | 193/383 [00:45<00:51,  3.67it/s]Epoch: 10, train for the 1-th batch, train loss: 0.5946120023727417:   0%|                      | 0/119 [00:00<?, ?it/s]Epoch: 4, train for the 21-th batch, train loss: 0.5992476940155029:   8%|█            | 20/237 [00:03<00:36,  5.96it/s]Epoch: 10, train for the 1-th batch, train loss: 0.5946120023727417:   1%|              | 1/119 [00:00<00:13,  8.92it/s]Epoch: 4, train for the 21-th batch, train loss: 0.5992476940155029:   9%|█▏           | 21/237 [00:03<00:34,  6.26it/s]Epoch: 2, train for the 194-th batch, train loss: 0.441476047039032:  51%|██████      | 194/383 [00:45<00:50,  3.73it/s]Epoch: 5, train for the 52-th batch, train loss: 0.5998038649559021:  21%|██▊          | 51/241 [00:08<00:31,  5.98it/s]Epoch: 8, train for the 123-th batch, train loss: 0.550574541091919:  84%|██████████  | 122/146 [00:18<00:04,  5.98it/s]Epoch: 8, train for the 123-th batch, train loss: 0.550574541091919:  84%|██████████  | 123/146 [00:18<00:03,  6.39it/s]evaluate for the 11-th batch, evaluate loss: 0.7427117824554443:  40%|███████▏          | 10/25 [00:01<00:01, 10.03it/s]Epoch: 10, train for the 2-th batch, train loss: 0.6160858273506165:   1%|              | 1/119 [00:00<00:13,  8.92it/s]Epoch: 10, train for the 2-th batch, train loss: 0.6160858273506165:   2%|▏             | 2/119 [00:00<00:14,  8.29it/s]Epoch: 4, train for the 22-th batch, train loss: 0.6123424172401428:   9%|█▏           | 21/237 [00:03<00:34,  6.26it/s]Epoch: 4, train for the 22-th batch, train loss: 0.6123424172401428:   9%|█▏           | 22/237 [00:03<00:34,  6.19it/s]Epoch: 5, train for the 53-th batch, train loss: 0.439482182264328:  21%|██▉           | 51/241 [00:08<00:31,  5.98it/s]evaluate for the 12-th batch, evaluate loss: 0.6492219567298889:  40%|███████▏          | 10/25 [00:01<00:01, 10.03it/s]evaluate for the 12-th batch, evaluate loss: 0.6492219567298889:  48%|████████▋         | 12/25 [00:01<00:01,  8.23it/s]Epoch: 5, train for the 53-th batch, train loss: 0.439482182264328:  22%|███           | 53/241 [00:08<00:28,  6.61it/s]Epoch: 8, train for the 124-th batch, train loss: 0.5091239213943481:  84%|█████████▎ | 123/146 [00:19<00:03,  6.39it/s]Epoch: 8, train for the 124-th batch, train loss: 0.5091239213943481:  85%|█████████▎ | 124/146 [00:19<00:03,  6.40it/s]Epoch: 10, train for the 3-th batch, train loss: 0.6304882764816284:   2%|▏             | 2/119 [00:00<00:14,  8.29it/s]Epoch: 2, train for the 195-th batch, train loss: 0.4477698504924774:  51%|█████▌     | 194/383 [00:45<00:50,  3.73it/s]Epoch: 10, train for the 3-th batch, train loss: 0.6304882764816284:   3%|▎             | 3/119 [00:00<00:14,  8.14it/s]evaluate for the 13-th batch, evaluate loss: 0.6755180358886719:  48%|████████▋         | 12/25 [00:01<00:01,  8.23it/s]Epoch: 4, train for the 23-th batch, train loss: 0.5864731669425964:   9%|█▏           | 22/237 [00:03<00:34,  6.19it/s]Epoch: 2, train for the 195-th batch, train loss: 0.4477698504924774:  51%|█████▌     | 195/383 [00:45<00:52,  3.60it/s]Epoch: 4, train for the 23-th batch, train loss: 0.5864731669425964:  10%|█▎           | 23/237 [00:03<00:33,  6.40it/s]Epoch: 5, train for the 54-th batch, train loss: 0.38573580980300903:  22%|██▋         | 53/241 [00:08<00:28,  6.61it/s]Epoch: 10, train for the 4-th batch, train loss: 0.6324067711830139:   3%|▎             | 3/119 [00:00<00:14,  8.14it/s]Epoch: 10, train for the 4-th batch, train loss: 0.6324067711830139:   3%|▍             | 4/119 [00:00<00:13,  8.53it/s]Epoch: 5, train for the 54-th batch, train loss: 0.38573580980300903:  22%|██▋         | 54/241 [00:08<00:28,  6.46it/s]evaluate for the 14-th batch, evaluate loss: 0.7018925547599792:  48%|████████▋         | 12/25 [00:01<00:01,  8.23it/s]evaluate for the 14-th batch, evaluate loss: 0.7018925547599792:  56%|██████████        | 14/25 [00:01<00:01,  9.16it/s]Epoch: 4, train for the 24-th batch, train loss: 0.5890277028083801:  10%|█▎           | 23/237 [00:03<00:33,  6.40it/s]evaluate for the 15-th batch, evaluate loss: 0.7616206407546997:  56%|██████████        | 14/25 [00:01<00:01,  9.16it/s]Epoch: 4, train for the 24-th batch, train loss: 0.5890277028083801:  10%|█▎           | 24/237 [00:03<00:32,  6.56it/s]Epoch: 5, train for the 55-th batch, train loss: 0.3987831771373749:  22%|██▉          | 54/241 [00:08<00:28,  6.46it/s]Epoch: 5, train for the 55-th batch, train loss: 0.3987831771373749:  23%|██▉          | 55/241 [00:08<00:27,  6.70it/s]Epoch: 10, train for the 5-th batch, train loss: 0.5624637007713318:   3%|▍             | 4/119 [00:00<00:13,  8.53it/s]Epoch: 10, train for the 5-th batch, train loss: 0.5624637007713318:   4%|▌             | 5/119 [00:00<00:14,  7.80it/s]Epoch: 2, train for the 196-th batch, train loss: 0.47337058186531067:  51%|█████     | 195/383 [00:45<00:52,  3.60it/s]evaluate for the 16-th batch, evaluate loss: 0.6703741550445557:  56%|██████████        | 14/25 [00:01<00:01,  9.16it/s]evaluate for the 16-th batch, evaluate loss: 0.6703741550445557:  64%|███████████▌      | 16/25 [00:01<00:00,  9.74it/s]Epoch: 8, train for the 125-th batch, train loss: 0.4960887134075165:  85%|█████████▎ | 124/146 [00:19<00:03,  6.40it/s]Epoch: 8, train for the 125-th batch, train loss: 0.4960887134075165:  86%|█████████▍ | 125/146 [00:19<00:04,  4.68it/s]Epoch: 4, train for the 25-th batch, train loss: 0.5930357575416565:  10%|█▎           | 24/237 [00:03<00:32,  6.56it/s]Epoch: 4, train for the 25-th batch, train loss: 0.5930357575416565:  11%|█▎           | 25/237 [00:03<00:30,  6.86it/s]Epoch: 2, train for the 196-th batch, train loss: 0.47337058186531067:  51%|█████     | 196/383 [00:45<00:52,  3.59it/s]Epoch: 5, train for the 56-th batch, train loss: 0.38610732555389404:  23%|██▋         | 55/241 [00:08<00:27,  6.70it/s]Epoch: 5, train for the 56-th batch, train loss: 0.38610732555389404:  23%|██▊         | 56/241 [00:08<00:26,  6.91it/s]evaluate for the 17-th batch, evaluate loss: 0.6633398532867432:  64%|███████████▌      | 16/25 [00:01<00:00,  9.74it/s]Epoch: 10, train for the 6-th batch, train loss: 0.5039308667182922:   4%|▌             | 5/119 [00:00<00:14,  7.80it/s]Epoch: 10, train for the 6-th batch, train loss: 0.5039308667182922:   5%|▋             | 6/119 [00:00<00:14,  7.62it/s]Epoch: 8, train for the 126-th batch, train loss: 0.5205332636833191:  86%|█████████▍ | 125/146 [00:19<00:04,  4.68it/s]Epoch: 8, train for the 126-th batch, train loss: 0.5205332636833191:  86%|█████████▍ | 126/146 [00:19<00:03,  5.13it/s]Epoch: 4, train for the 26-th batch, train loss: 0.5526275634765625:  11%|█▎           | 25/237 [00:04<00:30,  6.86it/s]evaluate for the 18-th batch, evaluate loss: 0.6296548843383789:  64%|███████████▌      | 16/25 [00:01<00:00,  9.74it/s]evaluate for the 18-th batch, evaluate loss: 0.6296548843383789:  72%|████████████▉     | 18/25 [00:01<00:00,  9.88it/s]Epoch: 4, train for the 26-th batch, train loss: 0.5526275634765625:  11%|█▍           | 26/237 [00:04<00:30,  6.82it/s]Epoch: 5, train for the 57-th batch, train loss: 0.43379390239715576:  23%|██▊         | 56/241 [00:08<00:26,  6.91it/s]Epoch: 5, train for the 57-th batch, train loss: 0.43379390239715576:  24%|██▊         | 57/241 [00:08<00:26,  6.86it/s]Epoch: 10, train for the 7-th batch, train loss: 0.5137210488319397:   5%|▋             | 6/119 [00:00<00:14,  7.62it/s]Epoch: 10, train for the 7-th batch, train loss: 0.5137210488319397:   6%|▊             | 7/119 [00:00<00:15,  7.37it/s]Epoch: 2, train for the 197-th batch, train loss: 0.5259321331977844:  51%|█████▋     | 196/383 [00:46<00:52,  3.59it/s]evaluate for the 19-th batch, evaluate loss: 0.6108302474021912:  72%|████████████▉     | 18/25 [00:01<00:00,  9.88it/s]Epoch: 2, train for the 197-th batch, train loss: 0.5259321331977844:  51%|█████▋     | 197/383 [00:46<00:50,  3.69it/s]Epoch: 8, train for the 127-th batch, train loss: 0.5103713870048523:  86%|█████████▍ | 126/146 [00:19<00:03,  5.13it/s]Epoch: 8, train for the 127-th batch, train loss: 0.5103713870048523:  87%|█████████▌ | 127/146 [00:19<00:03,  5.39it/s]Epoch: 4, train for the 27-th batch, train loss: 0.5900990962982178:  11%|█▍           | 26/237 [00:04<00:30,  6.82it/s]Epoch: 4, train for the 27-th batch, train loss: 0.5900990962982178:  11%|█▍           | 27/237 [00:04<00:31,  6.62it/s]Epoch: 10, train for the 8-th batch, train loss: 0.4446636736392975:   6%|▊             | 7/119 [00:01<00:15,  7.37it/s]evaluate for the 20-th batch, evaluate loss: 0.6617546081542969:  72%|████████████▉     | 18/25 [00:02<00:00,  9.88it/s]evaluate for the 20-th batch, evaluate loss: 0.6617546081542969:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.92it/s]Epoch: 10, train for the 8-th batch, train loss: 0.4446636736392975:   7%|▉             | 8/119 [00:01<00:15,  7.15it/s]Epoch: 5, train for the 58-th batch, train loss: 0.32240408658981323:  24%|██▊         | 57/241 [00:08<00:26,  6.86it/s]Epoch: 5, train for the 58-th batch, train loss: 0.32240408658981323:  24%|██▉         | 58/241 [00:09<00:28,  6.34it/s]Epoch: 8, train for the 128-th batch, train loss: 0.5219869613647461:  87%|█████████▌ | 127/146 [00:19<00:03,  5.39it/s]Epoch: 8, train for the 128-th batch, train loss: 0.5219869613647461:  88%|█████████▋ | 128/146 [00:19<00:03,  5.79it/s]evaluate for the 21-th batch, evaluate loss: 0.6725090146064758:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.92it/s]Epoch: 10, train for the 9-th batch, train loss: 0.46627742052078247:   7%|▊            | 8/119 [00:01<00:15,  7.15it/s]Epoch: 10, train for the 9-th batch, train loss: 0.46627742052078247:   8%|▉            | 9/119 [00:01<00:14,  7.44it/s]Epoch: 4, train for the 28-th batch, train loss: 0.6026237607002258:  11%|█▍           | 27/237 [00:04<00:31,  6.62it/s]Epoch: 4, train for the 28-th batch, train loss: 0.6026237607002258:  12%|█▌           | 28/237 [00:04<00:33,  6.33it/s]Epoch: 5, train for the 59-th batch, train loss: 0.5901502370834351:  24%|███▏         | 58/241 [00:09<00:28,  6.34it/s]Epoch: 5, train for the 59-th batch, train loss: 0.5901502370834351:  24%|███▏         | 59/241 [00:09<00:27,  6.50it/s]Epoch: 2, train for the 198-th batch, train loss: 0.43660613894462585:  51%|█████▏    | 197/383 [00:46<00:50,  3.69it/s]evaluate for the 22-th batch, evaluate loss: 0.6142204999923706:  80%|██████████████▍   | 20/25 [00:02<00:00,  9.92it/s]evaluate for the 22-th batch, evaluate loss: 0.6142204999923706:  88%|███████████████▊  | 22/25 [00:02<00:00, 10.14it/s]Epoch: 8, train for the 129-th batch, train loss: 0.5050461292266846:  88%|█████████▋ | 128/146 [00:20<00:03,  5.79it/s]Epoch: 2, train for the 198-th batch, train loss: 0.43660613894462585:  52%|█████▏    | 198/383 [00:46<00:52,  3.51it/s]Epoch: 8, train for the 129-th batch, train loss: 0.5050461292266846:  88%|█████████▋ | 129/146 [00:20<00:02,  6.10it/s]Epoch: 10, train for the 10-th batch, train loss: 0.47929227352142334:   8%|▉           | 9/119 [00:01<00:14,  7.44it/s]Epoch: 4, train for the 29-th batch, train loss: 0.5841557383537292:  12%|█▌           | 28/237 [00:04<00:33,  6.33it/s]Epoch: 10, train for the 10-th batch, train loss: 0.47929227352142334:   8%|▉          | 10/119 [00:01<00:15,  7.15it/s]Epoch: 4, train for the 29-th batch, train loss: 0.5841557383537292:  12%|█▌           | 29/237 [00:04<00:32,  6.43it/s]evaluate for the 23-th batch, evaluate loss: 0.6341432929039001:  88%|███████████████▊  | 22/25 [00:02<00:00, 10.14it/s]Epoch: 5, train for the 60-th batch, train loss: 0.5791547894477844:  24%|███▏         | 59/241 [00:09<00:27,  6.50it/s]Epoch: 5, train for the 60-th batch, train loss: 0.5791547894477844:  25%|███▏         | 60/241 [00:09<00:28,  6.33it/s]Epoch: 8, train for the 130-th batch, train loss: 0.459001749753952:  88%|██████████▌ | 129/146 [00:20<00:02,  6.10it/s]Epoch: 8, train for the 130-th batch, train loss: 0.459001749753952:  89%|██████████▋ | 130/146 [00:20<00:02,  6.16it/s]evaluate for the 24-th batch, evaluate loss: 0.6458010673522949:  88%|███████████████▊  | 22/25 [00:02<00:00, 10.14it/s]evaluate for the 24-th batch, evaluate loss: 0.6458010673522949:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.75it/s]Epoch: 10, train for the 11-th batch, train loss: 0.43822944164276123:   8%|▉          | 10/119 [00:01<00:15,  7.15it/s]Epoch: 10, train for the 11-th batch, train loss: 0.43822944164276123:   9%|█          | 11/119 [00:01<00:15,  6.93it/s]Epoch: 5, train for the 61-th batch, train loss: 0.5627927780151367:  25%|███▏         | 60/241 [00:09<00:28,  6.33it/s]Epoch: 5, train for the 61-th batch, train loss: 0.5627927780151367:  25%|███▎         | 61/241 [00:09<00:26,  6.78it/s]Epoch: 2, train for the 199-th batch, train loss: 0.39444372057914734:  52%|█████▏    | 198/383 [00:46<00:52,  3.51it/s]Epoch: 4, train for the 30-th batch, train loss: 0.5703979134559631:  12%|█▌           | 29/237 [00:04<00:32,  6.43it/s]Epoch: 4, train for the 30-th batch, train loss: 0.5703979134559631:  13%|█▋           | 30/237 [00:04<00:35,  5.85it/s]evaluate for the 25-th batch, evaluate loss: 0.6172888875007629:  96%|█████████████████▎| 24/25 [00:02<00:00,  9.75it/s]evaluate for the 25-th batch, evaluate loss: 0.6172888875007629: 100%|██████████████████| 25/25 [00:02<00:00,  9.85it/s]
Epoch: 2, train for the 199-th batch, train loss: 0.39444372057914734:  52%|█████▏    | 199/383 [00:46<00:53,  3.47it/s]Epoch: 8, train for the 131-th batch, train loss: 0.5184372663497925:  89%|█████████▊ | 130/146 [00:20<00:02,  6.16it/s]Epoch: 8, train for the 131-th batch, train loss: 0.5184372663497925:  90%|█████████▊ | 131/146 [00:20<00:02,  6.24it/s]Epoch: 10, train for the 12-th batch, train loss: 0.46928372979164124:   9%|█          | 11/119 [00:01<00:15,  6.93it/s]Epoch: 10, train for the 12-th batch, train loss: 0.46928372979164124:  10%|█          | 12/119 [00:01<00:15,  6.94it/s]Epoch: 5, train for the 62-th batch, train loss: 0.5878577828407288:  25%|███▎         | 61/241 [00:09<00:26,  6.78it/s]Epoch: 5, train for the 62-th batch, train loss: 0.5878577828407288:  26%|███▎         | 62/241 [00:09<00:27,  6.61it/s]Epoch: 4, train for the 31-th batch, train loss: 0.5876936912536621:  13%|█▋           | 30/237 [00:04<00:35,  5.85it/s]Epoch: 4, train for the 31-th batch, train loss: 0.5876936912536621:  13%|█▋           | 31/237 [00:04<00:34,  5.95it/s]Epoch: 8, train for the 132-th batch, train loss: 0.47841888666152954:  90%|████████▉ | 131/146 [00:20<00:02,  6.24it/s]Epoch: 10, train for the 13-th batch, train loss: 0.4028984606266022:  10%|█▏          | 12/119 [00:01<00:15,  6.94it/s]Epoch: 8, train for the 132-th batch, train loss: 0.47841888666152954:  90%|█████████ | 132/146 [00:20<00:02,  6.26it/s]Epoch: 10, train for the 13-th batch, train loss: 0.4028984606266022:  11%|█▎          | 13/119 [00:01<00:15,  7.01it/s]INFO:root:Epoch: 6, learning rate: 0.0001, train loss: 0.5569
INFO:root:train average_precision, 0.7834
INFO:root:train roc_auc, 0.7628
INFO:root:validate loss: 0.5079
INFO:root:validate average_precision, 0.8407
INFO:root:validate roc_auc, 0.8344
INFO:root:new node validate loss: 0.7024
INFO:root:new node validate first_1_average_precision, 0.5605
INFO:root:new node validate first_1_roc_auc, 0.5683
INFO:root:new node validate first_3_average_precision, 0.5879
INFO:root:new node validate first_3_roc_auc, 0.5815
INFO:root:new node validate first_10_average_precision, 0.6395
INFO:root:new node validate first_10_roc_auc, 0.6335
INFO:root:new node validate average_precision, 0.6627
INFO:root:new node validate roc_auc, 0.6621
Epoch: 2, train for the 200-th batch, train loss: 0.5070539712905884:  52%|█████▋     | 199/383 [00:47<00:53,  3.47it/s]***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
***** CALCULATING HISTOGRAM *****
***** CALCULATED HISTOGRAM *****
  0%|                                                                                           | 0/151 [00:00<?, ?it/s]Epoch: 5, train for the 63-th batch, train loss: 0.7126086354255676:  26%|███▎         | 62/241 [00:09<00:27,  6.61it/s]Epoch: 5, train for the 63-th batch, train loss: 0.7126086354255676:  26%|███▍         | 63/241 [00:09<00:27,  6.59it/s]Epoch: 4, train for the 32-th batch, train loss: 0.6228777766227722:  13%|█▋           | 31/237 [00:05<00:34,  5.95it/s]Epoch: 4, train for the 32-th batch, train loss: 0.6228777766227722:  14%|█▊           | 32/237 [00:05<00:33,  6.14it/s]Epoch: 2, train for the 200-th batch, train loss: 0.5070539712905884:  52%|█████▋     | 200/383 [00:47<00:53,  3.45it/s]Epoch: 7, train for the 1-th batch, train loss: 0.7030550241470337:   0%|                       | 0/151 [00:00<?, ?it/s]Epoch: 7, train for the 1-th batch, train loss: 0.7030550241470337:   1%|               | 1/151 [00:00<00:15,  9.56it/s]Epoch: 10, train for the 14-th batch, train loss: 0.4181462824344635:  11%|█▎          | 13/119 [00:01<00:15,  7.01it/s]Epoch: 10, train for the 14-th batch, train loss: 0.4181462824344635:  12%|█▍          | 14/119 [00:01<00:14,  7.03it/s]Epoch: 8, train for the 133-th batch, train loss: 0.4794476330280304:  90%|█████████▉ | 132/146 [00:20<00:02,  6.26it/s]Epoch: 8, train for the 133-th batch, train loss: 0.4794476330280304:  91%|██████████ | 133/146 [00:20<00:02,  5.98it/s]Epoch: 5, train for the 64-th batch, train loss: 0.326246440410614:  26%|███▋          | 63/241 [00:09<00:27,  6.59it/s]Epoch: 5, train for the 64-th batch, train loss: 0.326246440410614:  27%|███▋          | 64/241 [00:09<00:26,  6.72it/s]Epoch: 4, train for the 33-th batch, train loss: 0.5672626495361328:  14%|█▊           | 32/237 [00:05<00:33,  6.14it/s]Epoch: 4, train for the 33-th batch, train loss: 0.5672626495361328:  14%|█▊           | 33/237 [00:05<00:32,  6.23it/s]Epoch: 7, train for the 2-th batch, train loss: 0.7102839350700378:   1%|               | 1/151 [00:00<00:15,  9.56it/s]Epoch: 7, train for the 2-th batch, train loss: 0.7102839350700378:   1%|▏              | 2/151 [00:00<00:17,  8.76it/s]Epoch: 10, train for the 15-th batch, train loss: 0.4270249903202057:  12%|█▍          | 14/119 [00:02<00:14,  7.03it/s]Epoch: 10, train for the 15-th batch, train loss: 0.4270249903202057:  13%|█▌          | 15/119 [00:02<00:15,  6.69it/s]Epoch: 8, train for the 134-th batch, train loss: 0.4674113988876343:  91%|██████████ | 133/146 [00:20<00:02,  5.98it/s]Epoch: 5, train for the 65-th batch, train loss: 0.37268683314323425:  27%|███▏        | 64/241 [00:10<00:26,  6.72it/s]Epoch: 8, train for the 134-th batch, train loss: 0.4674113988876343:  92%|██████████ | 134/146 [00:20<00:01,  6.06it/s]Epoch: 2, train for the 201-th batch, train loss: 0.5718338489532471:  52%|█████▋     | 200/383 [00:47<00:53,  3.45it/s]Epoch: 5, train for the 65-th batch, train loss: 0.37268683314323425:  27%|███▏        | 65/241 [00:10<00:25,  6.82it/s]Epoch: 7, train for the 3-th batch, train loss: 0.7106629610061646:   1%|▏              | 2/151 [00:00<00:17,  8.76it/s]Epoch: 7, train for the 3-th batch, train loss: 0.7106629610061646:   2%|▎              | 3/151 [00:00<00:17,  8.70it/s]Epoch: 4, train for the 34-th batch, train loss: 0.5686764121055603:  14%|█▊           | 33/237 [00:05<00:32,  6.23it/s]Epoch: 4, train for the 34-th batch, train loss: 0.5686764121055603:  14%|█▊           | 34/237 [00:05<00:32,  6.29it/s]Epoch: 2, train for the 201-th batch, train loss: 0.5718338489532471:  52%|█████▊     | 201/383 [00:47<00:53,  3.41it/s]Epoch: 10, train for the 16-th batch, train loss: 0.44022703170776367:  13%|█▍         | 15/119 [00:02<00:15,  6.69it/s]Epoch: 10, train for the 16-th batch, train loss: 0.44022703170776367:  13%|█▍         | 16/119 [00:02<00:15,  6.81it/s]Epoch: 8, train for the 135-th batch, train loss: 0.4746226370334625:  92%|██████████ | 134/146 [00:21<00:01,  6.06it/s]Epoch: 8, train for the 135-th batch, train loss: 0.4746226370334625:  92%|██████████▏| 135/146 [00:21<00:01,  6.12it/s]Epoch: 5, train for the 66-th batch, train loss: 0.27833133935928345:  27%|███▏        | 65/241 [00:10<00:25,  6.82it/s]Epoch: 7, train for the 4-th batch, train loss: 0.7009534239768982:   2%|▎              | 3/151 [00:00<00:17,  8.70it/s]Epoch: 7, train for the 4-th batch, train loss: 0.7009534239768982:   3%|▍              | 4/151 [00:00<00:19,  7.73it/s]Epoch: 5, train for the 66-th batch, train loss: 0.27833133935928345:  27%|███▎        | 66/241 [00:10<00:27,  6.31it/s]Epoch: 4, train for the 35-th batch, train loss: 0.5668032169342041:  14%|█▊           | 34/237 [00:05<00:32,  6.29it/s]Epoch: 4, train for the 35-th batch, train loss: 0.5668032169342041:  15%|█▉           | 35/237 [00:05<00:31,  6.43it/s]Epoch: 8, train for the 136-th batch, train loss: 0.46212413907051086:  92%|█████████▏| 135/146 [00:21<00:01,  6.12it/s]Epoch: 8, train for the 136-th batch, train loss: 0.46212413907051086:  93%|█████████▎| 136/146 [00:21<00:01,  6.76it/s]Epoch: 2, train for the 202-th batch, train loss: 0.4809163212776184:  52%|█████▊     | 201/383 [00:47<00:53,  3.41it/s]Epoch: 7, train for the 5-th batch, train loss: 0.7129412889480591:   3%|▍              | 4/151 [00:00<00:19,  7.73it/s]Epoch: 4, train for the 36-th batch, train loss: 0.5778031945228577:  15%|█▉           | 35/237 [00:05<00:31,  6.43it/s]Epoch: 7, train for the 5-th batch, train loss: 0.7129412889480591:   3%|▍              | 5/151 [00:00<00:20,  7.21it/s]Epoch: 5, train for the 67-th batch, train loss: 0.44062796235084534:  27%|███▎        | 66/241 [00:10<00:27,  6.31it/s]Epoch: 4, train for the 36-th batch, train loss: 0.5778031945228577:  15%|█▉           | 36/237 [00:05<00:30,  6.69it/s]Epoch: 2, train for the 202-th batch, train loss: 0.4809163212776184:  53%|█████▊     | 202/383 [00:47<00:52,  3.44it/s]Epoch: 5, train for the 67-th batch, train loss: 0.44062796235084534:  28%|███▎        | 67/241 [00:10<00:27,  6.26it/s]Epoch: 10, train for the 17-th batch, train loss: 0.3701961040496826:  13%|█▌          | 16/119 [00:02<00:15,  6.81it/s]Epoch: 10, train for the 17-th batch, train loss: 0.3701961040496826:  14%|█▋          | 17/119 [00:02<00:18,  5.49it/s]Epoch: 8, train for the 137-th batch, train loss: 0.4775041341781616:  93%|██████████▏| 136/146 [00:21<00:01,  6.76it/s]Epoch: 8, train for the 137-th batch, train loss: 0.4775041341781616:  94%|██████████▎| 137/146 [00:21<00:01,  6.73it/s]Epoch: 7, train for the 6-th batch, train loss: 0.7117350697517395:   3%|▍              | 5/151 [00:00<00:20,  7.21it/s]Epoch: 7, train for the 6-th batch, train loss: 0.7117350697517395:   4%|▌              | 6/151 [00:00<00:18,  7.75it/s]Epoch: 4, train for the 37-th batch, train loss: 0.6370885968208313:  15%|█▉           | 36/237 [00:05<00:30,  6.69it/s]Epoch: 10, train for the 18-th batch, train loss: 0.4386722147464752:  14%|█▋          | 17/119 [00:02<00:18,  5.49it/s]Epoch: 5, train for the 68-th batch, train loss: 0.5072914958000183:  28%|███▌         | 67/241 [00:10<00:27,  6.26it/s]Epoch: 10, train for the 18-th batch, train loss: 0.4386722147464752:  15%|█▊          | 18/119 [00:02<00:16,  6.06it/s]Epoch: 5, train for the 68-th batch, train loss: 0.5072914958000183:  28%|███▋         | 68/241 [00:10<00:26,  6.42it/s]Epoch: 4, train for the 37-th batch, train loss: 0.6370885968208313:  16%|██           | 37/237 [00:05<00:30,  6.54it/s]Epoch: 8, train for the 138-th batch, train loss: 0.5446906685829163:  94%|██████████▎| 137/146 [00:21<00:01,  6.73it/s]Epoch: 8, train for the 138-th batch, train loss: 0.5446906685829163:  95%|██████████▍| 138/146 [00:21<00:01,  6.58it/s]Epoch: 7, train for the 7-th batch, train loss: 0.6742286682128906:   4%|▌              | 6/151 [00:00<00:18,  7.75it/s]Epoch: 2, train for the 203-th batch, train loss: 0.43512633442878723:  53%|█████▎    | 202/383 [00:47<00:52,  3.44it/s]Epoch: 7, train for the 7-th batch, train loss: 0.6742286682128906:   5%|▋              | 7/151 [00:00<00:19,  7.21it/s]Epoch: 10, train for the 19-th batch, train loss: 0.4133435785770416:  15%|█▊          | 18/119 [00:02<00:16,  6.06it/s]Epoch: 10, train for the 19-th batch, train loss: 0.4133435785770416:  16%|█▉          | 19/119 [00:02<00:15,  6.49it/s]Epoch: 5, train for the 69-th batch, train loss: 0.6528641581535339:  28%|███▋         | 68/241 [00:10<00:26,  6.42it/s]Epoch: 4, train for the 38-th batch, train loss: 0.5699228644371033:  16%|██           | 37/237 [00:06<00:30,  6.54it/s]Epoch: 4, train for the 38-th batch, train loss: 0.5699228644371033:  16%|██           | 38/237 [00:06<00:29,  6.72it/s]Epoch: 5, train for the 69-th batch, train loss: 0.6528641581535339:  29%|███▋         | 69/241 [00:10<00:26,  6.52it/s]Epoch: 2, train for the 203-th batch, train loss: 0.43512633442878723:  53%|█████▎    | 203/383 [00:48<00:52,  3.40it/s]Epoch: 8, train for the 139-th batch, train loss: 0.5255093574523926:  95%|██████████▍| 138/146 [00:21<00:01,  6.58it/s]Epoch: 8, train for the 139-th batch, train loss: 0.5255093574523926:  95%|██████████▍| 139/146 [00:21<00:01,  6.86it/s]Epoch: 10, train for the 20-th batch, train loss: 0.4198729693889618:  16%|█▉          | 19/119 [00:02<00:15,  6.49it/s]Epoch: 10, train for the 20-th batch, train loss: 0.4198729693889618:  17%|██          | 20/119 [00:02<00:14,  6.99it/s]Epoch: 5, train for the 70-th batch, train loss: 0.5598808526992798:  29%|███▋         | 69/241 [00:10<00:26,  6.52it/s]Epoch: 5, train for the 70-th batch, train loss: 0.5598808526992798:  29%|███▊         | 70/241 [00:10<00:25,  6.77it/s]Epoch: 7, train for the 8-th batch, train loss: 0.7116789817810059:   5%|▋              | 7/151 [00:01<00:19,  7.21it/s]Epoch: 7, train for the 8-th batch, train loss: 0.7116789817810059:   5%|▊              | 8/151 [00:01<00:22,  6.41it/s]Epoch: 4, train for the 39-th batch, train loss: 0.6169631481170654:  16%|██           | 38/237 [00:06<00:29,  6.72it/s]Epoch: 4, train for the 39-th batch, train loss: 0.6169631481170654:  16%|██▏          | 39/237 [00:06<00:31,  6.25it/s]Epoch: 8, train for the 140-th batch, train loss: 0.4782942235469818:  95%|██████████▍| 139/146 [00:21<00:01,  6.86it/s]Epoch: 8, train for the 140-th batch, train loss: 0.4782942235469818:  96%|██████████▌| 140/146 [00:21<00:00,  6.57it/s]Epoch: 10, train for the 21-th batch, train loss: 0.4204234182834625:  17%|██          | 20/119 [00:02<00:14,  6.99it/s]Epoch: 10, train for the 21-th batch, train loss: 0.4204234182834625:  18%|██          | 21/119 [00:02<00:14,  6.92it/s]Epoch: 2, train for the 204-th batch, train loss: 0.29970550537109375:  53%|█████▎    | 203/383 [00:48<00:52,  3.40it/s]Epoch: 5, train for the 71-th batch, train loss: 0.6362009644508362:  29%|███▊         | 70/241 [00:10<00:25,  6.77it/s]Epoch: 5, train for the 71-th batch, train loss: 0.6362009644508362:  29%|███▊         | 71/241 [00:10<00:25,  6.75it/s]Epoch: 7, train for the 9-th batch, train loss: 0.6781434416770935:   5%|▊              | 8/151 [00:01<00:22,  6.41it/s]Epoch: 7, train for the 9-th batch, train loss: 0.6781434416770935:   6%|▉              | 9/151 [00:01<00:22,  6.31it/s]Epoch: 2, train for the 204-th batch, train loss: 0.29970550537109375:  53%|█████▎    | 204/383 [00:48<00:53,  3.34it/s]Epoch: 4, train for the 40-th batch, train loss: 0.6111984848976135:  16%|██▏          | 39/237 [00:06<00:31,  6.25it/s]Epoch: 4, train for the 40-th batch, train loss: 0.6111984848976135:  17%|██▏          | 40/237 [00:06<00:31,  6.22it/s]Epoch: 10, train for the 22-th batch, train loss: 0.4464457929134369:  18%|██          | 21/119 [00:03<00:14,  6.92it/s]Epoch: 10, train for the 22-th batch, train loss: 0.4464457929134369:  18%|██▏         | 22/119 [00:03<00:13,  7.06it/s]Epoch: 5, train for the 72-th batch, train loss: 0.5848607420921326:  29%|███▊         | 71/241 [00:11<00:25,  6.75it/s]Epoch: 8, train for the 141-th batch, train loss: 0.4782896935939789:  96%|██████████▌| 140/146 [00:21<00:00,  6.57it/s]Epoch: 5, train for the 72-th batch, train loss: 0.5848607420921326:  30%|███▉         | 72/241 [00:11<00:24,  6.92it/s]Epoch: 8, train for the 141-th batch, train loss: 0.4782896935939789:  97%|██████████▌| 141/146 [00:21<00:00,  5.96it/s]Epoch: 7, train for the 10-th batch, train loss: 0.6664581894874573:   6%|▊             | 9/151 [00:01<00:22,  6.31it/s]Epoch: 7, train for the 10-th batch, train loss: 0.6664581894874573:   7%|▊            | 10/151 [00:01<00:21,  6.42it/s]Epoch: 10, train for the 23-th batch, train loss: 0.4664584994316101:  18%|██▏         | 22/119 [00:03<00:13,  7.06it/s]Epoch: 10, train for the 23-th batch, train loss: 0.4664584994316101:  19%|██▎         | 23/119 [00:03<00:13,  7.25it/s]Epoch: 4, train for the 41-th batch, train loss: 0.5587524175643921:  17%|██▏          | 40/237 [00:06<00:31,  6.22it/s]Epoch: 5, train for the 73-th batch, train loss: 0.4880751967430115:  30%|███▉         | 72/241 [00:11<00:24,  6.92it/s]Epoch: 4, train for the 41-th batch, train loss: 0.5587524175643921:  17%|██▏          | 41/237 [00:06<00:34,  5.74it/s]Epoch: 5, train for the 73-th batch, train loss: 0.4880751967430115:  30%|███▉         | 73/241 [00:11<00:24,  6.93it/s]Epoch: 2, train for the 205-th batch, train loss: 0.35550111532211304:  53%|█████▎    | 204/383 [00:48<00:53,  3.34it/s]Epoch: 8, train for the 142-th batch, train loss: 0.4573206305503845:  97%|██████████▌| 141/146 [00:22<00:00,  5.96it/s]Epoch: 7, train for the 11-th batch, train loss: 0.6709110736846924:   7%|▊            | 10/151 [00:01<00:21,  6.42it/s]Epoch: 7, train for the 11-th batch, train loss: 0.6709110736846924:   7%|▉            | 11/151 [00:01<00:20,  6.75it/s]Epoch: 8, train for the 142-th batch, train loss: 0.4573206305503845:  97%|██████████▋| 142/146 [00:22<00:00,  6.04it/s]Epoch: 10, train for the 24-th batch, train loss: 0.3855734169483185:  19%|██▎         | 23/119 [00:03<00:13,  7.25it/s]Epoch: 10, train for the 24-th batch, train loss: 0.3855734169483185:  20%|██▍         | 24/119 [00:03<00:12,  7.35it/s]Epoch: 2, train for the 205-th batch, train loss: 0.35550111532211304:  54%|█████▎    | 205/383 [00:48<00:55,  3.22it/s]Epoch: 4, train for the 42-th batch, train loss: 0.6022477746009827:  17%|██▏          | 41/237 [00:06<00:34,  5.74it/s]Epoch: 4, train for the 42-th batch, train loss: 0.6022477746009827:  18%|██▎          | 42/237 [00:06<00:31,  6.17it/s]Epoch: 5, train for the 74-th batch, train loss: 0.44949203729629517:  30%|███▋        | 73/241 [00:11<00:24,  6.93it/s]Epoch: 5, train for the 74-th batch, train loss: 0.44949203729629517:  31%|███▋        | 74/241 [00:11<00:23,  7.00it/s]Epoch: 8, train for the 143-th batch, train loss: 0.49370867013931274:  97%|█████████▋| 142/146 [00:22<00:00,  6.04it/s]Epoch: 8, train for the 143-th batch, train loss: 0.49370867013931274:  98%|█████████▊| 143/146 [00:22<00:00,  5.96it/s]Epoch: 10, train for the 25-th batch, train loss: 0.41577351093292236:  20%|██▏        | 24/119 [00:03<00:12,  7.35it/s]Epoch: 10, train for the 25-th batch, train loss: 0.41577351093292236:  21%|██▎        | 25/119 [00:03<00:13,  7.12it/s]Epoch: 5, train for the 75-th batch, train loss: 0.35956287384033203:  31%|███▋        | 74/241 [00:11<00:23,  7.00it/s]Epoch: 4, train for the 43-th batch, train loss: 0.5994983315467834:  18%|██▎          | 42/237 [00:06<00:31,  6.17it/s]Epoch: 5, train for the 75-th batch, train loss: 0.35956287384033203:  31%|███▋        | 75/241 [00:11<00:22,  7.40it/s]Epoch: 4, train for the 43-th batch, train loss: 0.5994983315467834:  18%|██▎          | 43/237 [00:06<00:30,  6.34it/s]Epoch: 7, train for the 12-th batch, train loss: 0.690765380859375:   7%|█             | 11/151 [00:01<00:20,  6.75it/s]Epoch: 7, train for the 12-th batch, train loss: 0.690765380859375:   8%|█             | 12/151 [00:01<00:25,  5.48it/s]Epoch: 2, train for the 206-th batch, train loss: 0.5183742642402649:  54%|█████▉     | 205/383 [00:48<00:55,  3.22it/s]Epoch: 8, train for the 144-th batch, train loss: 0.4528922438621521:  98%|██████████▊| 143/146 [00:22<00:00,  5.96it/s]Epoch: 10, train for the 26-th batch, train loss: 0.3472011983394623:  21%|██▌         | 25/119 [00:03<00:13,  7.12it/s]Epoch: 10, train for the 26-th batch, train loss: 0.3472011983394623:  22%|██▌         | 26/119 [00:03<00:13,  6.88it/s]Epoch: 8, train for the 144-th batch, train loss: 0.4528922438621521:  99%|██████████▊| 144/146 [00:22<00:00,  5.78it/s]Epoch: 4, train for the 44-th batch, train loss: 0.6171135306358337:  18%|██▎          | 43/237 [00:06<00:30,  6.34it/s]Epoch: 4, train for the 44-th batch, train loss: 0.6171135306358337:  19%|██▍          | 44/237 [00:07<00:30,  6.41it/s]Epoch: 2, train for the 206-th batch, train loss: 0.5183742642402649:  54%|█████▉     | 206/383 [00:49<00:56,  3.14it/s]Epoch: 5, train for the 76-th batch, train loss: 0.28801754117012024:  31%|███▋        | 75/241 [00:11<00:22,  7.40it/s]Epoch: 7, train for the 13-th batch, train loss: 0.6746213436126709:   8%|█            | 12/151 [00:01<00:25,  5.48it/s]Epoch: 7, train for the 13-th batch, train loss: 0.6746213436126709:   9%|█            | 13/151 [00:01<00:24,  5.69it/s]Epoch: 5, train for the 76-th batch, train loss: 0.28801754117012024:  32%|███▊        | 76/241 [00:11<00:26,  6.31it/s]Epoch: 10, train for the 27-th batch, train loss: 0.38964247703552246:  22%|██▍        | 26/119 [00:03<00:13,  6.88it/s]Epoch: 10, train for the 27-th batch, train loss: 0.38964247703552246:  23%|██▍        | 27/119 [00:03<00:13,  6.91it/s]Epoch: 8, train for the 145-th batch, train loss: 0.4656086564064026:  99%|██████████▊| 144/146 [00:22<00:00,  5.78it/s]Epoch: 7, train for the 14-th batch, train loss: 0.6817551255226135:   9%|█            | 13/151 [00:02<00:24,  5.69it/s]Epoch: 8, train for the 145-th batch, train loss: 0.4656086564064026:  99%|██████████▉| 145/146 [00:22<00:00,  5.64it/s]Epoch: 7, train for the 14-th batch, train loss: 0.6817551255226135:   9%|█▏           | 14/151 [00:02<00:21,  6.24it/s]Epoch: 4, train for the 45-th batch, train loss: 0.5759776830673218:  19%|██▍          | 44/237 [00:07<00:30,  6.41it/s]Epoch: 4, train for the 45-th batch, train loss: 0.5759776830673218:  19%|██▍          | 45/237 [00:07<00:31,  6.01it/s]Epoch: 2, train for the 207-th batch, train loss: 0.4193495810031891:  54%|█████▉     | 206/383 [00:49<00:56,  3.14it/s]Epoch: 10, train for the 28-th batch, train loss: 0.4353741407394409:  23%|██▋         | 27/119 [00:03<00:13,  6.91it/s]Epoch: 10, train for the 28-th batch, train loss: 0.4353741407394409:  24%|██▊         | 28/119 [00:03<00:12,  7.33it/s]Epoch: 7, train for the 15-th batch, train loss: 0.6768655180931091:   9%|█▏           | 14/151 [00:02<00:21,  6.24it/s]Epoch: 7, train for the 15-th batch, train loss: 0.6768655180931091:  10%|█▎           | 15/151 [00:02<00:19,  6.99it/s]Epoch: 2, train for the 207-th batch, train loss: 0.4193495810031891:  54%|█████▉     | 207/383 [00:49<00:53,  3.28it/s]Epoch: 8, train for the 146-th batch, train loss: 0.41209161281585693:  99%|█████████▉| 145/146 [00:22<00:00,  5.64it/s]Epoch: 8, train for the 146-th batch, train loss: 0.41209161281585693: 100%|██████████| 146/146 [00:22<00:00,  5.81it/s]Epoch: 8, train for the 146-th batch, train loss: 0.41209161281585693: 100%|██████████| 146/146 [00:22<00:00,  6.40it/s]
Epoch: 5, train for the 77-th batch, train loss: 0.3646507263183594:  32%|████         | 76/241 [00:11<00:26,  6.31it/s]Epoch: 4, train for the 46-th batch, train loss: 0.6372127532958984:  19%|██▍          | 45/237 [00:07<00:31,  6.01it/s]Epoch: 5, train for the 77-th batch, train loss: 0.3646507263183594:  32%|████▏        | 77/241 [00:12<00:32,  5.01it/s]Epoch: 10, train for the 29-th batch, train loss: 0.38549408316612244:  24%|██▌        | 28/119 [00:04<00:12,  7.33it/s]Epoch: 10, train for the 29-th batch, train loss: 0.38549408316612244:  24%|██▋        | 29/119 [00:04<00:12,  7.38it/s]Epoch: 4, train for the 46-th batch, train loss: 0.6372127532958984:  19%|██▌          | 46/237 [00:07<00:31,  6.04it/s]Epoch: 7, train for the 16-th batch, train loss: 0.6680244207382202:  10%|█▎           | 15/151 [00:02<00:19,  6.99it/s]Epoch: 7, train for the 16-th batch, train loss: 0.6680244207382202:  11%|█▍           | 16/151 [00:02<00:18,  7.28it/s]  0%|                                                                                            | 0/38 [00:00<?, ?it/s]Epoch: 5, train for the 78-th batch, train loss: 0.5736330151557922:  32%|████▏        | 77/241 [00:12<00:32,  5.01it/s]Epoch: 10, train for the 30-th batch, train loss: 0.34986412525177:  24%|███▍          | 29/119 [00:04<00:12,  7.38it/s]evaluate for the 1-th batch, evaluate loss: 0.4596160054206848:   0%|                            | 0/38 [00:00<?, ?it/s]Epoch: 10, train for the 30-th batch, train loss: 0.34986412525177:  25%|███▌          | 30/119 [00:04<00:12,  6.85it/s]Epoch: 5, train for the 78-th batch, train loss: 0.5736330151557922:  32%|████▏        | 78/241 [00:12<00:31,  5.10it/s]Epoch: 7, train for the 17-th batch, train loss: 0.6813691854476929:  11%|█▍           | 16/151 [00:02<00:18,  7.28it/s]Epoch: 7, train for the 17-th batch, train loss: 0.6813691854476929:  11%|█▍           | 17/151 [00:02<00:20,  6.59it/s]Epoch: 2, train for the 208-th batch, train loss: 0.5123463273048401:  54%|█████▉     | 207/383 [00:49<00:53,  3.28it/s]evaluate for the 2-th batch, evaluate loss: 0.4700247347354889:   0%|                            | 0/38 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.4700247347354889:   5%|█                   | 2/38 [00:00<00:02, 13.06it/s]Epoch: 4, train for the 47-th batch, train loss: 0.5813308954238892:  19%|██▌          | 46/237 [00:07<00:31,  6.04it/s]Epoch: 4, train for the 47-th batch, train loss: 0.5813308954238892:  20%|██▌          | 47/237 [00:07<00:37,  5.12it/s]Epoch: 2, train for the 208-th batch, train loss: 0.5123463273048401:  54%|█████▉     | 208/383 [00:49<00:56,  3.12it/s]Epoch: 5, train for the 79-th batch, train loss: 0.4752480983734131:  32%|████▏        | 78/241 [00:12<00:31,  5.10it/s]Epoch: 5, train for the 79-th batch, train loss: 0.4752480983734131:  33%|████▎        | 79/241 [00:12<00:28,  5.70it/s]evaluate for the 3-th batch, evaluate loss: 0.4569588303565979:   5%|█                   | 2/38 [00:00<00:02, 13.06it/s]Epoch: 10, train for the 31-th batch, train loss: 0.42240190505981445:  25%|██▊        | 30/119 [00:04<00:12,  6.85it/s]Epoch: 10, train for the 31-th batch, train loss: 0.42240190505981445:  26%|██▊        | 31/119 [00:04<00:13,  6.48it/s]Epoch: 7, train for the 18-th batch, train loss: 0.6717086434364319:  11%|█▍           | 17/151 [00:02<00:20,  6.59it/s]Epoch: 7, train for the 18-th batch, train loss: 0.6717086434364319:  12%|█▌           | 18/151 [00:02<00:20,  6.60it/s]evaluate for the 4-th batch, evaluate loss: 0.4680134952068329:   5%|█                   | 2/38 [00:00<00:02, 13.06it/s]evaluate for the 4-th batch, evaluate loss: 0.4680134952068329:  11%|██                  | 4/38 [00:00<00:02, 12.76it/s]Epoch: 4, train for the 48-th batch, train loss: 0.5656811594963074:  20%|██▌          | 47/237 [00:07<00:37,  5.12it/s]Epoch: 5, train for the 80-th batch, train loss: 0.3672270178794861:  33%|████▎        | 79/241 [00:12<00:28,  5.70it/s]evaluate for the 5-th batch, evaluate loss: 0.5098114013671875:  11%|██                  | 4/38 [00:00<00:02, 12.76it/s]Epoch: 4, train for the 48-th batch, train loss: 0.5656811594963074:  20%|██▋          | 48/237 [00:07<00:36,  5.17it/s]Epoch: 5, train for the 80-th batch, train loss: 0.3672270178794861:  33%|████▎        | 80/241 [00:12<00:27,  5.93it/s]Epoch: 2, train for the 209-th batch, train loss: 0.3993869721889496:  54%|█████▉     | 208/383 [00:49<00:56,  3.12it/s]Epoch: 7, train for the 19-th batch, train loss: 0.65470290184021:  12%|█▊             | 18/151 [00:02<00:20,  6.60it/s]Epoch: 7, train for the 19-th batch, train loss: 0.65470290184021:  13%|█▉             | 19/151 [00:02<00:19,  6.67it/s]Epoch: 10, train for the 32-th batch, train loss: 0.3671219050884247:  26%|███▏        | 31/119 [00:04<00:13,  6.48it/s]evaluate for the 6-th batch, evaluate loss: 0.4663066267967224:  11%|██                  | 4/38 [00:00<00:02, 12.76it/s]evaluate for the 6-th batch, evaluate loss: 0.4663066267967224:  16%|███▏                | 6/38 [00:00<00:02, 13.94it/s]Epoch: 10, train for the 32-th batch, train loss: 0.3671219050884247:  27%|███▏        | 32/119 [00:04<00:14,  6.20it/s]Epoch: 2, train for the 209-th batch, train loss: 0.3993869721889496:  55%|██████     | 209/383 [00:49<00:51,  3.36it/s]Epoch: 5, train for the 81-th batch, train loss: 0.3926980197429657:  33%|████▎        | 80/241 [00:12<00:27,  5.93it/s]evaluate for the 7-th batch, evaluate loss: 0.4291135370731354:  16%|███▏                | 6/38 [00:00<00:02, 13.94it/s]Epoch: 5, train for the 81-th batch, train loss: 0.3926980197429657:  34%|████▎        | 81/241 [00:12<00:25,  6.21it/s]Epoch: 4, train for the 49-th batch, train loss: 0.5637762546539307:  20%|██▋          | 48/237 [00:07<00:36,  5.17it/s]Epoch: 4, train for the 49-th batch, train loss: 0.5637762546539307:  21%|██▋          | 49/237 [00:07<00:36,  5.21it/s]Epoch: 7, train for the 20-th batch, train loss: 0.6545270681381226:  13%|█▋           | 19/151 [00:02<00:19,  6.67it/s]Epoch: 7, train for the 20-th batch, train loss: 0.6545270681381226:  13%|█▋           | 20/151 [00:02<00:19,  6.81it/s]Epoch: 10, train for the 33-th batch, train loss: 0.40101882815361023:  27%|██▉        | 32/119 [00:04<00:14,  6.20it/s]evaluate for the 8-th batch, evaluate loss: 0.45649534463882446:  16%|███                | 6/38 [00:00<00:02, 13.94it/s]evaluate for the 8-th batch, evaluate loss: 0.45649534463882446:  21%|████               | 8/38 [00:00<00:02, 13.17it/s]Epoch: 10, train for the 33-th batch, train loss: 0.40101882815361023:  28%|███        | 33/119 [00:04<00:13,  6.15it/s]Epoch: 2, train for the 210-th batch, train loss: 0.45059001445770264:  55%|█████▍    | 209/383 [00:50<00:51,  3.36it/s]Epoch: 5, train for the 82-th batch, train loss: 0.44798627495765686:  34%|████        | 81/241 [00:12<00:25,  6.21it/s]Epoch: 5, train for the 82-th batch, train loss: 0.44798627495765686:  34%|████        | 82/241 [00:12<00:24,  6.46it/s]evaluate for the 9-th batch, evaluate loss: 0.4634888470172882:  21%|████▏               | 8/38 [00:00<00:02, 13.17it/s]Epoch: 2, train for the 210-th batch, train loss: 0.45059001445770264:  55%|█████▍    | 210/383 [00:50<00:49,  3.51it/s]Epoch: 7, train for the 21-th batch, train loss: 0.6591761112213135:  13%|█▋           | 20/151 [00:03<00:19,  6.81it/s]Epoch: 4, train for the 50-th batch, train loss: 0.5647799968719482:  21%|██▋          | 49/237 [00:08<00:36,  5.21it/s]Epoch: 7, train for the 21-th batch, train loss: 0.6591761112213135:  14%|█▊           | 21/151 [00:03<00:19,  6.84it/s]Epoch: 4, train for the 50-th batch, train loss: 0.5647799968719482:  21%|██▋          | 50/237 [00:08<00:34,  5.44it/s]Epoch: 10, train for the 34-th batch, train loss: 0.32481831312179565:  28%|███        | 33/119 [00:04<00:13,  6.15it/s]evaluate for the 10-th batch, evaluate loss: 0.4850558638572693:  21%|████               | 8/38 [00:00<00:02, 13.17it/s]evaluate for the 10-th batch, evaluate loss: 0.4850558638572693:  26%|████▋             | 10/38 [00:00<00:02, 12.82it/s]Epoch: 10, train for the 34-th batch, train loss: 0.32481831312179565:  29%|███▏       | 34/119 [00:04<00:13,  6.15it/s]Epoch: 5, train for the 83-th batch, train loss: 0.5141738653182983:  34%|████▍        | 82/241 [00:12<00:24,  6.46it/s]Epoch: 5, train for the 83-th batch, train loss: 0.5141738653182983:  34%|████▍        | 83/241 [00:12<00:23,  6.63it/s]Epoch: 7, train for the 22-th batch, train loss: 0.6489912867546082:  14%|█▊           | 21/151 [00:03<00:19,  6.84it/s]evaluate for the 11-th batch, evaluate loss: 0.4388471841812134:  26%|████▋             | 10/38 [00:00<00:02, 12.82it/s]Epoch: 7, train for the 22-th batch, train loss: 0.6489912867546082:  15%|█▉           | 22/151 [00:03<00:18,  6.88it/s]Epoch: 4, train for the 51-th batch, train loss: 0.5728809237480164:  21%|██▋          | 50/237 [00:08<00:34,  5.44it/s]Epoch: 4, train for the 51-th batch, train loss: 0.5728809237480164:  22%|██▊          | 51/237 [00:08<00:34,  5.41it/s]Epoch: 10, train for the 35-th batch, train loss: 0.3485587537288666:  29%|███▍        | 34/119 [00:05<00:13,  6.15it/s]evaluate for the 12-th batch, evaluate loss: 0.507187008857727:  26%|█████              | 10/38 [00:00<00:02, 12.82it/s]evaluate for the 12-th batch, evaluate loss: 0.507187008857727:  32%|██████             | 12/38 [00:00<00:02, 12.79it/s]Epoch: 10, train for the 35-th batch, train loss: 0.3485587537288666:  29%|███▌        | 35/119 [00:05<00:13,  6.25it/s]Epoch: 2, train for the 211-th batch, train loss: 0.3600812554359436:  55%|██████     | 210/383 [00:50<00:49,  3.51it/s]Epoch: 5, train for the 84-th batch, train loss: 0.6180689930915833:  34%|████▍        | 83/241 [00:13<00:23,  6.63it/s]evaluate for the 13-th batch, evaluate loss: 0.48825541138648987:  32%|█████▎           | 12/38 [00:00<00:02, 12.79it/s]Epoch: 5, train for the 84-th batch, train loss: 0.6180689930915833:  35%|████▌        | 84/241 [00:13<00:25,  6.09it/s]Epoch: 7, train for the 23-th batch, train loss: 0.6462486386299133:  15%|█▉           | 22/151 [00:03<00:18,  6.88it/s]Epoch: 2, train for the 211-th batch, train loss: 0.3600812554359436:  55%|██████     | 211/383 [00:50<00:50,  3.41it/s]Epoch: 7, train for the 23-th batch, train loss: 0.6462486386299133:  15%|█▉           | 23/151 [00:03<00:18,  6.74it/s]Epoch: 4, train for the 52-th batch, train loss: 0.5436471700668335:  22%|██▊          | 51/237 [00:08<00:34,  5.41it/s]Epoch: 10, train for the 36-th batch, train loss: 0.3527013957500458:  29%|███▌        | 35/119 [00:05<00:13,  6.25it/s]evaluate for the 14-th batch, evaluate loss: 0.42732498049736023:  32%|█████▎           | 12/38 [00:01<00:02, 12.79it/s]evaluate for the 14-th batch, evaluate loss: 0.42732498049736023:  37%|██████▎          | 14/38 [00:01<00:01, 13.05it/s]Epoch: 10, train for the 36-th batch, train loss: 0.3527013957500458:  30%|███▋        | 36/119 [00:05<00:13,  6.36it/s]Epoch: 4, train for the 52-th batch, train loss: 0.5436471700668335:  22%|██▊          | 52/237 [00:08<00:33,  5.50it/s]evaluate for the 15-th batch, evaluate loss: 0.4379609525203705:  37%|██████▋           | 14/38 [00:01<00:01, 13.05it/s]Epoch: 5, train for the 85-th batch, train loss: 0.44734740257263184:  35%|████▏       | 84/241 [00:13<00:25,  6.09it/s]Epoch: 7, train for the 24-th batch, train loss: 0.6486769914627075:  15%|█▉           | 23/151 [00:03<00:18,  6.74it/s]Epoch: 5, train for the 85-th batch, train loss: 0.44734740257263184:  35%|████▏       | 85/241 [00:13<00:27,  5.65it/s]Epoch: 7, train for the 24-th batch, train loss: 0.6486769914627075:  16%|██           | 24/151 [00:03<00:20,  6.15it/s]Epoch: 4, train for the 53-th batch, train loss: 0.5853692293167114:  22%|██▊          | 52/237 [00:08<00:33,  5.50it/s]Epoch: 10, train for the 37-th batch, train loss: 0.36114761233329773:  30%|███▎       | 36/119 [00:05<00:13,  6.36it/s]evaluate for the 16-th batch, evaluate loss: 0.4957219958305359:  37%|██████▋           | 14/38 [00:01<00:01, 13.05it/s]evaluate for the 16-th batch, evaluate loss: 0.4957219958305359:  42%|███████▌          | 16/38 [00:01<00:01, 12.50it/s]Epoch: 4, train for the 53-th batch, train loss: 0.5853692293167114:  22%|██▉          | 53/237 [00:08<00:32,  5.69it/s]Epoch: 10, train for the 37-th batch, train loss: 0.36114761233329773:  31%|███▍       | 37/119 [00:05<00:13,  6.10it/s]evaluate for the 17-th batch, evaluate loss: 0.4605879783630371:  42%|███████▌          | 16/38 [00:01<00:01, 12.50it/s]Epoch: 5, train for the 86-th batch, train loss: 0.5098499059677124:  35%|████▌        | 85/241 [00:13<00:27,  5.65it/s]Epoch: 5, train for the 86-th batch, train loss: 0.5098499059677124:  36%|████▋        | 86/241 [00:13<00:26,  5.93it/s]Epoch: 4, train for the 54-th batch, train loss: 0.5240974426269531:  22%|██▉          | 53/237 [00:08<00:32,  5.69it/s]Epoch: 7, train for the 25-th batch, train loss: 0.6836562752723694:  16%|██           | 24/151 [00:03<00:20,  6.15it/s]evaluate for the 18-th batch, evaluate loss: 0.49834156036376953:  42%|███████▏         | 16/38 [00:01<00:01, 12.50it/s]evaluate for the 18-th batch, evaluate loss: 0.49834156036376953:  47%|████████         | 18/38 [00:01<00:01, 13.02it/s]Epoch: 4, train for the 54-th batch, train loss: 0.5240974426269531:  23%|██▉          | 54/237 [00:08<00:29,  6.11it/s]Epoch: 7, train for the 25-th batch, train loss: 0.6836562752723694:  17%|██▏          | 25/151 [00:03<00:21,  5.94it/s]Epoch: 10, train for the 38-th batch, train loss: 0.3677692413330078:  31%|███▋        | 37/119 [00:05<00:13,  6.10it/s]Epoch: 10, train for the 38-th batch, train loss: 0.3677692413330078:  32%|███▊        | 38/119 [00:05<00:13,  6.00it/s]Epoch: 5, train for the 87-th batch, train loss: 0.5489295721054077:  36%|████▋        | 86/241 [00:13<00:26,  5.93it/s]evaluate for the 19-th batch, evaluate loss: 0.4865218997001648:  47%|████████▌         | 18/38 [00:01<00:01, 13.02it/s]Epoch: 2, train for the 212-th batch, train loss: 0.5040900707244873:  55%|██████     | 211/383 [00:50<00:50,  3.41it/s]Epoch: 5, train for the 87-th batch, train loss: 0.5489295721054077:  36%|████▋        | 87/241 [00:13<00:23,  6.48it/s]Epoch: 2, train for the 212-th batch, train loss: 0.5040900707244873:  55%|██████     | 212/383 [00:50<01:01,  2.78it/s]evaluate for the 20-th batch, evaluate loss: 0.3985675275325775:  47%|████████▌         | 18/38 [00:01<00:01, 13.02it/s]evaluate for the 20-th batch, evaluate loss: 0.3985675275325775:  53%|█████████▍        | 20/38 [00:01<00:01, 13.40it/s]Epoch: 7, train for the 26-th batch, train loss: 0.652267575263977:  17%|██▎           | 25/151 [00:03<00:21,  5.94it/s]Epoch: 4, train for the 55-th batch, train loss: 0.5534065365791321:  23%|██▉          | 54/237 [00:08<00:29,  6.11it/s]Epoch: 7, train for the 26-th batch, train loss: 0.652267575263977:  17%|██▍           | 26/151 [00:03<00:20,  6.09it/s]Epoch: 4, train for the 55-th batch, train loss: 0.5534065365791321:  23%|███          | 55/237 [00:08<00:30,  5.95it/s]evaluate for the 21-th batch, evaluate loss: 0.42733579874038696:  53%|████████▉        | 20/38 [00:01<00:01, 13.40it/s]Epoch: 5, train for the 88-th batch, train loss: 0.4787978231906891:  36%|████▋        | 87/241 [00:13<00:23,  6.48it/s]Epoch: 5, train for the 88-th batch, train loss: 0.4787978231906891:  37%|████▋        | 88/241 [00:13<00:24,  6.37it/s]Epoch: 7, train for the 27-th batch, train loss: 0.6480808854103088:  17%|██▏          | 26/151 [00:04<00:20,  6.09it/s]evaluate for the 22-th batch, evaluate loss: 0.46816307306289673:  53%|████████▉        | 20/38 [00:01<00:01, 13.40it/s]evaluate for the 22-th batch, evaluate loss: 0.46816307306289673:  58%|█████████▊       | 22/38 [00:01<00:01, 13.33it/s]Epoch: 7, train for the 27-th batch, train loss: 0.6480808854103088:  18%|██▎          | 27/151 [00:04<00:19,  6.39it/s]evaluate for the 23-th batch, evaluate loss: 0.447162389755249:  58%|███████████        | 22/38 [00:01<00:01, 13.33it/s]Epoch: 4, train for the 56-th batch, train loss: 0.5353090763092041:  23%|███          | 55/237 [00:09<00:30,  5.95it/s]Epoch: 4, train for the 56-th batch, train loss: 0.5353090763092041:  24%|███          | 56/237 [00:09<00:31,  5.74it/s]Epoch: 10, train for the 39-th batch, train loss: 0.38091662526130676:  32%|███▌       | 38/119 [00:05<00:13,  6.00it/s]Epoch: 10, train for the 39-th batch, train loss: 0.38091662526130676:  33%|███▌       | 39/119 [00:05<00:17,  4.58it/s]evaluate for the 24-th batch, evaluate loss: 0.44759178161621094:  58%|█████████▊       | 22/38 [00:01<00:01, 13.33it/s]Epoch: 2, train for the 213-th batch, train loss: 0.5152103900909424:  55%|██████     | 212/383 [00:51<01:01,  2.78it/s]Epoch: 5, train for the 89-th batch, train loss: 0.48656415939331055:  37%|████▍       | 88/241 [00:13<00:24,  6.37it/s]Epoch: 5, train for the 89-th batch, train loss: 0.48656415939331055:  37%|████▍       | 89/241 [00:13<00:23,  6.35it/s]Epoch: 7, train for the 28-th batch, train loss: 0.6680445671081543:  18%|██▎          | 27/151 [00:04<00:19,  6.39it/s]Epoch: 7, train for the 28-th batch, train loss: 0.6680445671081543:  19%|██▍          | 28/151 [00:04<00:19,  6.46it/s]evaluate for the 25-th batch, evaluate loss: 0.47749456763267517:  58%|█████████▊       | 22/38 [00:01<00:01, 13.33it/s]evaluate for the 25-th batch, evaluate loss: 0.47749456763267517:  66%|███████████▏     | 25/38 [00:01<00:00, 14.99it/s]Epoch: 2, train for the 213-th batch, train loss: 0.5152103900909424:  56%|██████     | 213/383 [00:51<00:58,  2.89it/s]Epoch: 10, train for the 40-th batch, train loss: 0.3958682119846344:  33%|███▉        | 39/119 [00:06<00:17,  4.58it/s]Epoch: 4, train for the 57-th batch, train loss: 0.5666791200637817:  24%|███          | 56/237 [00:09<00:31,  5.74it/s]Epoch: 10, train for the 40-th batch, train loss: 0.3958682119846344:  34%|████        | 40/119 [00:06<00:15,  5.21it/s]Epoch: 4, train for the 57-th batch, train loss: 0.5666791200637817:  24%|███▏         | 57/237 [00:09<00:30,  5.89it/s]evaluate for the 26-th batch, evaluate loss: 0.4584641456604004:  66%|███████████▊      | 25/38 [00:01<00:00, 14.99it/s]Epoch: 5, train for the 90-th batch, train loss: 0.46246495842933655:  37%|████▍       | 89/241 [00:14<00:23,  6.35it/s]Epoch: 5, train for the 90-th batch, train loss: 0.46246495842933655:  37%|████▍       | 90/241 [00:14<00:23,  6.49it/s]Epoch: 7, train for the 29-th batch, train loss: 0.6611998677253723:  19%|██▍          | 28/151 [00:04<00:19,  6.46it/s]Epoch: 7, train for the 29-th batch, train loss: 0.6611998677253723:  19%|██▍          | 29/151 [00:04<00:18,  6.55it/s]evaluate for the 27-th batch, evaluate loss: 0.47963544726371765:  66%|███████████▏     | 25/38 [00:01<00:00, 14.99it/s]evaluate for the 27-th batch, evaluate loss: 0.47963544726371765:  71%|████████████     | 27/38 [00:01<00:00, 14.59it/s]Epoch: 10, train for the 41-th batch, train loss: 0.39491286873817444:  34%|███▋       | 40/119 [00:06<00:15,  5.21it/s]Epoch: 10, train for the 41-th batch, train loss: 0.39491286873817444:  34%|███▊       | 41/119 [00:06<00:13,  5.69it/s]evaluate for the 28-th batch, evaluate loss: 0.4598667025566101:  71%|████████████▊     | 27/38 [00:02<00:00, 14.59it/s]Epoch: 2, train for the 214-th batch, train loss: 0.4589044451713562:  56%|██████     | 213/383 [00:51<00:58,  2.89it/s]evaluate for the 29-th batch, evaluate loss: 0.4751453697681427:  71%|████████████▊     | 27/38 [00:02<00:00, 14.59it/s]evaluate for the 29-th batch, evaluate loss: 0.4751453697681427:  76%|█████████████▋    | 29/38 [00:02<00:00, 15.12it/s]Epoch: 5, train for the 91-th batch, train loss: 0.4764023721218109:  37%|████▊        | 90/241 [00:14<00:23,  6.49it/s]Epoch: 4, train for the 58-th batch, train loss: 0.5852822661399841:  24%|███▏         | 57/237 [00:09<00:30,  5.89it/s]Epoch: 5, train for the 91-th batch, train loss: 0.4764023721218109:  38%|████▉        | 91/241 [00:14<00:24,  6.17it/s]Epoch: 7, train for the 30-th batch, train loss: 0.6782206296920776:  19%|██▍          | 29/151 [00:04<00:18,  6.55it/s]Epoch: 4, train for the 58-th batch, train loss: 0.5852822661399841:  24%|███▏         | 58/237 [00:09<00:33,  5.34it/s]Epoch: 7, train for the 30-th batch, train loss: 0.6782206296920776:  20%|██▌          | 30/151 [00:04<00:19,  6.36it/s]Epoch: 10, train for the 42-th batch, train loss: 0.4152379631996155:  34%|████▏       | 41/119 [00:06<00:13,  5.69it/s]Epoch: 10, train for the 42-th batch, train loss: 0.4152379631996155:  35%|████▏       | 42/119 [00:06<00:12,  6.18it/s]evaluate for the 30-th batch, evaluate loss: 0.47846829891204834:  76%|████████████▉    | 29/38 [00:02<00:00, 15.12it/s]Epoch: 2, train for the 214-th batch, train loss: 0.4589044451713562:  56%|██████▏    | 214/383 [00:51<00:57,  2.94it/s]Epoch: 5, train for the 92-th batch, train loss: 0.3964844048023224:  38%|████▉        | 91/241 [00:14<00:24,  6.17it/s]evaluate for the 31-th batch, evaluate loss: 0.46542418003082275:  76%|████████████▉    | 29/38 [00:02<00:00, 15.12it/s]evaluate for the 31-th batch, evaluate loss: 0.46542418003082275:  82%|█████████████▊   | 31/38 [00:02<00:00, 14.78it/s]Epoch: 5, train for the 92-th batch, train loss: 0.3964844048023224:  38%|████▉        | 92/241 [00:14<00:22,  6.62it/s]Epoch: 4, train for the 59-th batch, train loss: 0.6234717965126038:  24%|███▏         | 58/237 [00:09<00:33,  5.34it/s]Epoch: 7, train for the 31-th batch, train loss: 0.6621295213699341:  20%|██▌          | 30/151 [00:04<00:19,  6.36it/s]Epoch: 7, train for the 31-th batch, train loss: 0.6621295213699341:  21%|██▋          | 31/151 [00:04<00:18,  6.50it/s]Epoch: 4, train for the 59-th batch, train loss: 0.6234717965126038:  25%|███▏         | 59/237 [00:09<00:31,  5.59it/s]Epoch: 10, train for the 43-th batch, train loss: 0.3819900453090668:  35%|████▏       | 42/119 [00:06<00:12,  6.18it/s]Epoch: 10, train for the 43-th batch, train loss: 0.3819900453090668:  36%|████▎       | 43/119 [00:06<00:11,  6.37it/s]evaluate for the 32-th batch, evaluate loss: 0.4272592067718506:  82%|██████████████▋   | 31/38 [00:02<00:00, 14.78it/s]Epoch: 5, train for the 93-th batch, train loss: 0.3935478925704956:  38%|████▉        | 92/241 [00:14<00:22,  6.62it/s]evaluate for the 33-th batch, evaluate loss: 0.47806254029273987:  82%|█████████████▊   | 31/38 [00:02<00:00, 14.78it/s]evaluate for the 33-th batch, evaluate loss: 0.47806254029273987:  87%|██████████████▊  | 33/38 [00:02<00:00, 14.36it/s]Epoch: 5, train for the 93-th batch, train loss: 0.3935478925704956:  39%|█████        | 93/241 [00:14<00:22,  6.64it/s]Epoch: 7, train for the 32-th batch, train loss: 0.6624712944030762:  21%|██▋          | 31/151 [00:04<00:18,  6.50it/s]Epoch: 7, train for the 32-th batch, train loss: 0.6624712944030762:  21%|██▊          | 32/151 [00:04<00:19,  6.26it/s]Epoch: 10, train for the 44-th batch, train loss: 0.3571341335773468:  36%|████▎       | 43/119 [00:06<00:11,  6.37it/s]Epoch: 2, train for the 215-th batch, train loss: 0.35855185985565186:  56%|█████▌    | 214/383 [00:51<00:57,  2.94it/s]evaluate for the 34-th batch, evaluate loss: 0.48249784111976624:  87%|██████████████▊  | 33/38 [00:02<00:00, 14.36it/s]Epoch: 10, train for the 44-th batch, train loss: 0.3571341335773468:  37%|████▍       | 44/119 [00:06<00:12,  6.20it/s]Epoch: 4, train for the 60-th batch, train loss: 0.5647038817405701:  25%|███▏         | 59/237 [00:09<00:31,  5.59it/s]Epoch: 5, train for the 94-th batch, train loss: 0.35121428966522217:  39%|████▋       | 93/241 [00:14<00:22,  6.64it/s]Epoch: 4, train for the 60-th batch, train loss: 0.5647038817405701:  25%|███▎         | 60/237 [00:09<00:33,  5.21it/s]Epoch: 5, train for the 94-th batch, train loss: 0.35121428966522217:  39%|████▋       | 94/241 [00:14<00:21,  6.79it/s]Epoch: 2, train for the 215-th batch, train loss: 0.35855185985565186:  56%|█████▌    | 215/383 [00:52<00:58,  2.85it/s]evaluate for the 35-th batch, evaluate loss: 0.47344771027565:  87%|█████████████████▎  | 33/38 [00:02<00:00, 14.36it/s]evaluate for the 35-th batch, evaluate loss: 0.47344771027565:  92%|██████████████████▍ | 35/38 [00:02<00:00, 13.85it/s]Epoch: 10, train for the 45-th batch, train loss: 0.35654422640800476:  37%|████       | 44/119 [00:06<00:12,  6.20it/s]Epoch: 7, train for the 33-th batch, train loss: 0.6269205808639526:  21%|██▊          | 32/151 [00:04<00:19,  6.26it/s]evaluate for the 36-th batch, evaluate loss: 0.4669134318828583:  92%|████████████████▌ | 35/38 [00:02<00:00, 13.85it/s]Epoch: 7, train for the 33-th batch, train loss: 0.6269205808639526:  22%|██▊          | 33/151 [00:05<00:18,  6.29it/s]Epoch: 10, train for the 45-th batch, train loss: 0.35654422640800476:  38%|████▏      | 45/119 [00:06<00:11,  6.37it/s]Epoch: 4, train for the 61-th batch, train loss: 0.5588032603263855:  25%|███▎         | 60/237 [00:10<00:33,  5.21it/s]evaluate for the 37-th batch, evaluate loss: 0.44924017786979675:  92%|███████████████▋ | 35/38 [00:02<00:00, 13.85it/s]evaluate for the 37-th batch, evaluate loss: 0.44924017786979675:  97%|████████████████▌| 37/38 [00:02<00:00, 14.21it/s]Epoch: 5, train for the 95-th batch, train loss: 0.4248666763305664:  39%|█████        | 94/241 [00:14<00:21,  6.79it/s]Epoch: 4, train for the 61-th batch, train loss: 0.5588032603263855:  26%|███▎         | 61/237 [00:10<00:33,  5.32it/s]Epoch: 5, train for the 95-th batch, train loss: 0.4248666763305664:  39%|█████        | 95/241 [00:14<00:22,  6.46it/s]evaluate for the 38-th batch, evaluate loss: 0.46847742795944214:  97%|████████████████▌| 37/38 [00:02<00:00, 14.21it/s]evaluate for the 38-th batch, evaluate loss: 0.46847742795944214: 100%|█████████████████| 38/38 [00:02<00:00, 13.77it/s]
Epoch: 7, train for the 34-th batch, train loss: 0.6268309354782104:  22%|██▊          | 33/151 [00:05<00:18,  6.29it/s]Epoch: 2, train for the 216-th batch, train loss: 0.42321524024009705:  56%|█████▌    | 215/383 [00:52<00:58,  2.85it/s]Epoch: 7, train for the 34-th batch, train loss: 0.6268309354782104:  23%|██▉          | 34/151 [00:05<00:18,  6.43it/s]Epoch: 10, train for the 46-th batch, train loss: 0.3637588322162628:  38%|████▌       | 45/119 [00:06<00:11,  6.37it/s]Epoch: 10, train for the 46-th batch, train loss: 0.3637588322162628:  39%|████▋       | 46/119 [00:06<00:12,  6.00it/s]Epoch: 4, train for the 62-th batch, train loss: 0.5451071262359619:  26%|███▎         | 61/237 [00:10<00:33,  5.32it/s]Epoch: 2, train for the 216-th batch, train loss: 0.42321524024009705:  56%|█████▋    | 216/383 [00:52<00:55,  3.01it/s]Epoch: 5, train for the 96-th batch, train loss: 0.5591610670089722:  39%|█████        | 95/241 [00:14<00:22,  6.46it/s]Epoch: 4, train for the 62-th batch, train loss: 0.5451071262359619:  26%|███▍         | 62/237 [00:10<00:30,  5.70it/s]Epoch: 5, train for the 96-th batch, train loss: 0.5591610670089722:  40%|█████▏       | 96/241 [00:14<00:22,  6.55it/s]Epoch: 7, train for the 35-th batch, train loss: 0.6386316418647766:  23%|██▉          | 34/151 [00:05<00:18,  6.43it/s]Epoch: 7, train for the 35-th batch, train loss: 0.6386316418647766:  23%|███          | 35/151 [00:05<00:16,  6.90it/s]Epoch: 10, train for the 47-th batch, train loss: 0.4199363589286804:  39%|████▋       | 46/119 [00:07<00:12,  6.00it/s]Epoch: 10, train for the 47-th batch, train loss: 0.4199363589286804:  39%|████▋       | 47/119 [00:07<00:10,  6.59it/s]Epoch: 4, train for the 63-th batch, train loss: 0.6007084846496582:  26%|███▍         | 62/237 [00:10<00:30,  5.70it/s]Epoch: 5, train for the 97-th batch, train loss: 0.3707619309425354:  40%|█████▏       | 96/241 [00:15<00:22,  6.55it/s]Epoch: 4, train for the 63-th batch, train loss: 0.6007084846496582:  27%|███▍         | 63/237 [00:10<00:30,  5.70it/s]Epoch: 5, train for the 97-th batch, train loss: 0.3707619309425354:  40%|█████▏       | 97/241 [00:15<00:22,  6.37it/s]Epoch: 7, train for the 36-th batch, train loss: 0.6603266596794128:  23%|███          | 35/151 [00:05<00:16,  6.90it/s]  0%|                                                                                            | 0/20 [00:00<?, ?it/s]Epoch: 7, train for the 36-th batch, train loss: 0.6603266596794128:  24%|███          | 36/151 [00:05<00:17,  6.57it/s]Epoch: 10, train for the 48-th batch, train loss: 0.34609466791152954:  39%|████▎      | 47/119 [00:07<00:10,  6.59it/s]Epoch: 10, train for the 48-th batch, train loss: 0.34609466791152954:  40%|████▍      | 48/119 [00:07<00:10,  6.57it/s]evaluate for the 1-th batch, evaluate loss: 0.7353153824806213:   0%|                            | 0/20 [00:00<?, ?it/s]Epoch: 4, train for the 64-th batch, train loss: 0.5780040621757507:  27%|███▍         | 63/237 [00:10<00:30,  5.70it/s]Epoch: 4, train for the 64-th batch, train loss: 0.5780040621757507:  27%|███▌         | 64/237 [00:10<00:28,  6.18it/s]Epoch: 5, train for the 98-th batch, train loss: 0.4743172228336334:  40%|█████▏       | 97/241 [00:15<00:22,  6.37it/s]Epoch: 5, train for the 98-th batch, train loss: 0.4743172228336334:  41%|█████▎       | 98/241 [00:15<00:22,  6.26it/s]evaluate for the 2-th batch, evaluate loss: 0.7521554231643677:   0%|                            | 0/20 [00:00<?, ?it/s]evaluate for the 2-th batch, evaluate loss: 0.7521554231643677:  10%|██                  | 2/20 [00:00<00:01, 13.34it/s]Epoch: 7, train for the 37-th batch, train loss: 0.6695268154144287:  24%|███          | 36/151 [00:05<00:17,  6.57it/s]Epoch: 10, train for the 49-th batch, train loss: 0.4202154576778412:  40%|████▊       | 48/119 [00:07<00:10,  6.57it/s]Epoch: 10, train for the 49-th batch, train loss: 0.4202154576778412:  41%|████▉       | 49/119 [00:07<00:10,  6.68it/s]Epoch: 7, train for the 37-th batch, train loss: 0.6695268154144287:  25%|███▏         | 37/151 [00:05<00:17,  6.35it/s]Epoch: 2, train for the 217-th batch, train loss: 0.4893738329410553:  56%|██████▏    | 216/383 [00:52<00:55,  3.01it/s]evaluate for the 3-th batch, evaluate loss: 0.6373779773712158:  10%|██                  | 2/20 [00:00<00:01, 13.34it/s]Epoch: 4, train for the 65-th batch, train loss: 0.5356993079185486:  27%|███▌         | 64/237 [00:10<00:28,  6.18it/s]Epoch: 2, train for the 217-th batch, train loss: 0.4893738329410553:  57%|██████▏    | 217/383 [00:52<01:01,  2.68it/s]Epoch: 4, train for the 65-th batch, train loss: 0.5356993079185486:  27%|███▌         | 65/237 [00:10<00:27,  6.30it/s]Epoch: 5, train for the 99-th batch, train loss: 0.3441588878631592:  41%|█████▎       | 98/241 [00:15<00:22,  6.26it/s]Epoch: 5, train for the 99-th batch, train loss: 0.3441588878631592:  41%|█████▎       | 99/241 [00:15<00:22,  6.33it/s]evaluate for the 4-th batch, evaluate loss: 0.6693446040153503:  10%|██                  | 2/20 [00:00<00:01, 13.34it/s]evaluate for the 4-th batch, evaluate loss: 0.6693446040153503:  20%|████                | 4/20 [00:00<00:01, 13.24it/s]Epoch: 10, train for the 50-th batch, train loss: 0.3733609616756439:  41%|████▉       | 49/119 [00:07<00:10,  6.68it/s]Epoch: 10, train for the 50-th batch, train loss: 0.3733609616756439:  42%|█████       | 50/119 [00:07<00:10,  6.60it/s]Epoch: 7, train for the 38-th batch, train loss: 0.6371828317642212:  25%|███▏         | 37/151 [00:05<00:17,  6.35it/s]Epoch: 7, train for the 38-th batch, train loss: 0.6371828317642212:  25%|███▎         | 38/151 [00:05<00:18,  6.06it/s]evaluate for the 5-th batch, evaluate loss: 0.674442708492279:  20%|████▏                | 4/20 [00:00<00:01, 13.24it/s]Epoch: 4, train for the 66-th batch, train loss: 0.5615754127502441:  27%|███▌         | 65/237 [00:10<00:27,  6.30it/s]Epoch: 4, train for the 66-th batch, train loss: 0.5615754127502441:  28%|███▌         | 66/237 [00:10<00:27,  6.21it/s]evaluate for the 6-th batch, evaluate loss: 0.7374098300933838:  20%|████                | 4/20 [00:00<00:01, 13.24it/s]evaluate for the 6-th batch, evaluate loss: 0.7374098300933838:  30%|██████              | 6/20 [00:00<00:00, 14.03it/s]Epoch: 5, train for the 100-th batch, train loss: 0.5365915298461914:  41%|████▉       | 99/241 [00:15<00:22,  6.33it/s]Epoch: 5, train for the 100-th batch, train loss: 0.5365915298461914:  41%|████▌      | 100/241 [00:15<00:22,  6.23it/s]Epoch: 10, train for the 51-th batch, train loss: 0.45606863498687744:  42%|████▌      | 50/119 [00:07<00:10,  6.60it/s]Epoch: 2, train for the 218-th batch, train loss: 0.5014800429344177:  57%|██████▏    | 217/383 [00:52<01:01,  2.68it/s]Epoch: 7, train for the 39-th batch, train loss: 0.6285562515258789:  25%|███▎         | 38/151 [00:05<00:18,  6.06it/s]Epoch: 10, train for the 51-th batch, train loss: 0.45606863498687744:  43%|████▋      | 51/119 [00:07<00:10,  6.55it/s]evaluate for the 7-th batch, evaluate loss: 0.7424020767211914:  30%|██████              | 6/20 [00:00<00:00, 14.03it/s]Epoch: 7, train for the 39-th batch, train loss: 0.6285562515258789:  26%|███▎         | 39/151 [00:05<00:17,  6.37it/s]Epoch: 2, train for the 218-th batch, train loss: 0.5014800429344177:  57%|██████▎    | 218/383 [00:53<00:57,  2.85it/s]evaluate for the 8-th batch, evaluate loss: 0.6929700970649719:  30%|██████              | 6/20 [00:00<00:00, 14.03it/s]evaluate for the 8-th batch, evaluate loss: 0.6929700970649719:  40%|████████            | 8/20 [00:00<00:00, 14.70it/s]Epoch: 4, train for the 67-th batch, train loss: 0.5120824575424194:  28%|███▌         | 66/237 [00:11<00:27,  6.21it/s]Epoch: 4, train for the 67-th batch, train loss: 0.5120824575424194:  28%|███▋         | 67/237 [00:11<00:27,  6.19it/s]Epoch: 5, train for the 101-th batch, train loss: 0.5191132426261902:  41%|████▌      | 100/241 [00:15<00:22,  6.23it/s]Epoch: 5, train for the 101-th batch, train loss: 0.5191132426261902:  42%|████▌      | 101/241 [00:15<00:21,  6.40it/s]Epoch: 7, train for the 40-th batch, train loss: 0.6171771287918091:  26%|███▎         | 39/151 [00:06<00:17,  6.37it/s]Epoch: 10, train for the 52-th batch, train loss: 0.4223358929157257:  43%|█████▏      | 51/119 [00:07<00:10,  6.55it/s]Epoch: 7, train for the 40-th batch, train loss: 0.6171771287918091:  26%|███▍         | 40/151 [00:06<00:17,  6.41it/s]Epoch: 10, train for the 52-th batch, train loss: 0.4223358929157257:  44%|█████▏      | 52/119 [00:07<00:10,  6.31it/s]evaluate for the 9-th batch, evaluate loss: 0.6512858271598816:  40%|████████            | 8/20 [00:00<00:00, 14.70it/s]Epoch: 5, train for the 102-th batch, train loss: 0.5420992374420166:  42%|████▌      | 101/241 [00:15<00:21,  6.40it/s]evaluate for the 10-th batch, evaluate loss: 0.6387280225753784:  40%|███████▌           | 8/20 [00:00<00:00, 14.70it/s]evaluate for the 10-th batch, evaluate loss: 0.6387280225753784:  50%|█████████         | 10/20 [00:00<00:00, 13.22it/s]Epoch: 5, train for the 102-th batch, train loss: 0.5420992374420166:  42%|████▋      | 102/241 [00:15<00:21,  6.61it/s]Epoch: 4, train for the 68-th batch, train loss: 0.5324022769927979:  28%|███▋         | 67/237 [00:11<00:27,  6.19it/s]Epoch: 10, train for the 53-th batch, train loss: 0.4040279984474182:  44%|█████▏      | 52/119 [00:08<00:10,  6.31it/s]Epoch: 7, train for the 41-th batch, train loss: 0.5980135798454285:  26%|███▍         | 40/151 [00:06<00:17,  6.41it/s]Epoch: 10, train for the 53-th batch, train loss: 0.4040279984474182:  45%|█████▎      | 53/119 [00:08<00:10,  6.34it/s]evaluate for the 11-th batch, evaluate loss: 0.6501571536064148:  50%|█████████         | 10/20 [00:00<00:00, 13.22it/s]Epoch: 7, train for the 41-th batch, train loss: 0.5980135798454285:  27%|███▌         | 41/151 [00:06<00:17,  6.19it/s]Epoch: 4, train for the 68-th batch, train loss: 0.5324022769927979:  29%|███▋         | 68/237 [00:11<00:31,  5.30it/s]Epoch: 5, train for the 103-th batch, train loss: 0.5321794152259827:  42%|████▋      | 102/241 [00:16<00:21,  6.61it/s]evaluate for the 12-th batch, evaluate loss: 0.687704861164093:  50%|█████████▌         | 10/20 [00:00<00:00, 13.22it/s]evaluate for the 12-th batch, evaluate loss: 0.687704861164093:  60%|███████████▍       | 12/20 [00:00<00:00, 13.01it/s]Epoch: 5, train for the 103-th batch, train loss: 0.5321794152259827:  43%|████▋      | 103/241 [00:16<00:21,  6.39it/s]Epoch: 10, train for the 54-th batch, train loss: 0.344242662191391:  45%|█████▊       | 53/119 [00:08<00:10,  6.34it/s]Epoch: 10, train for the 54-th batch, train loss: 0.344242662191391:  45%|█████▉       | 54/119 [00:08<00:10,  6.46it/s]evaluate for the 13-th batch, evaluate loss: 0.7058724164962769:  60%|██████████▊       | 12/20 [00:00<00:00, 13.01it/s]Epoch: 7, train for the 42-th batch, train loss: 0.6118606925010681:  27%|███▌         | 41/151 [00:06<00:17,  6.19it/s]Epoch: 4, train for the 69-th batch, train loss: 0.5087798833847046:  29%|███▋         | 68/237 [00:11<00:31,  5.30it/s]Epoch: 7, train for the 42-th batch, train loss: 0.6118606925010681:  28%|███▌         | 42/151 [00:06<00:17,  6.18it/s]Epoch: 4, train for the 69-th batch, train loss: 0.5087798833847046:  29%|███▊         | 69/237 [00:11<00:30,  5.46it/s]evaluate for the 14-th batch, evaluate loss: 0.7024708390235901:  60%|██████████▊       | 12/20 [00:01<00:00, 13.01it/s]evaluate for the 14-th batch, evaluate loss: 0.7024708390235901:  70%|████████████▌     | 14/20 [00:01<00:00, 13.11it/s]Epoch: 5, train for the 104-th batch, train loss: 0.4989683926105499:  43%|████▋      | 103/241 [00:16<00:21,  6.39it/s]Epoch: 2, train for the 219-th batch, train loss: 0.3319867253303528:  57%|██████▎    | 218/383 [00:53<00:57,  2.85it/s]Epoch: 10, train for the 55-th batch, train loss: 0.377641886472702:  45%|█████▉       | 54/119 [00:08<00:10,  6.46it/s]Epoch: 5, train for the 104-th batch, train loss: 0.4989683926105499:  43%|████▋      | 104/241 [00:16<00:22,  6.22it/s]Epoch: 10, train for the 55-th batch, train loss: 0.377641886472702:  46%|██████       | 55/119 [00:08<00:09,  6.87it/s]evaluate for the 15-th batch, evaluate loss: 0.71604323387146:  70%|██████████████      | 14/20 [00:01<00:00, 13.11it/s]Epoch: 7, train for the 43-th batch, train loss: 0.6485040187835693:  28%|███▌         | 42/151 [00:06<00:17,  6.18it/s]Epoch: 2, train for the 219-th batch, train loss: 0.3319867253303528:  57%|██████▎    | 219/383 [00:53<01:08,  2.39it/s]Epoch: 4, train for the 70-th batch, train loss: 0.5471145510673523:  29%|███▊         | 69/237 [00:11<00:30,  5.46it/s]Epoch: 7, train for the 43-th batch, train loss: 0.6485040187835693:  28%|███▋         | 43/151 [00:06<00:16,  6.49it/s]Epoch: 4, train for the 70-th batch, train loss: 0.5471145510673523:  30%|███▊         | 70/237 [00:11<00:28,  5.87it/s]evaluate for the 16-th batch, evaluate loss: 0.6627358794212341:  70%|████████████▌     | 14/20 [00:01<00:00, 13.11it/s]evaluate for the 16-th batch, evaluate loss: 0.6627358794212341:  80%|██████████████▍   | 16/20 [00:01<00:00, 13.62it/s]Epoch: 10, train for the 56-th batch, train loss: 0.394002765417099:  46%|██████       | 55/119 [00:08<00:09,  6.87it/s]Epoch: 5, train for the 105-th batch, train loss: 0.4423515498638153:  43%|████▋      | 104/241 [00:16<00:22,  6.22it/s]Epoch: 10, train for the 56-th batch, train loss: 0.394002765417099:  47%|██████       | 56/119 [00:08<00:08,  7.02it/s]Epoch: 5, train for the 105-th batch, train loss: 0.4423515498638153:  44%|████▊      | 105/241 [00:16<00:22,  6.08it/s]evaluate for the 17-th batch, evaluate loss: 0.7243287563323975:  80%|██████████████▍   | 16/20 [00:01<00:00, 13.62it/s]Epoch: 7, train for the 44-th batch, train loss: 0.6055067777633667:  28%|███▋         | 43/151 [00:06<00:16,  6.49it/s]